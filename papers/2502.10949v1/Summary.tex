\section{Concluding Remarks}
\label{sec_summary}

% what have you done in this paper?
% what are the results?
% are they important?
% what are the key points?
% what are the limitations?
% what are the outstanding problems?


We have presented a method for learning the exact time integration
algorithm for non-autonomous and autonomous systems  based on ELM-type randomized
neural networks.
The trained NN  serves as a time-marching algorithm for solving
the initial value problems, for arbitrary initial data or step sizes
from some domain. 


For a given non-autonomous (including autonomous) system,
the exact time integration algorithm can be represented
by an algorithmic function in higher dimensions,
which satisfies an associated system of
partial differential equations  with corresponding boundary conditions.
We learn this high-dimensional algorithmic function based on a physics informed approach,
by solving the associated PDE system using ELM-type randomized NNs.
We have presented several explicit and implicit formulations for
the algorithmic function, which accordingly lead to explicit and implicit
time integration algorithms, and discussed how to train the NN  by
the nonlinear least squares method.
For more challenging problems,  we find that it is
crucial to incorporate 
domain decomposition into the algorithm learning. 
In this case the algorithmic function is represented by a collection
of local randomized NNs, one for each sub-domain.
Importantly, the local NNs  for different sub-domains
are not coupled, due to the nature of the system of equations
for the algorithmic function, and thus they can be trained individually or in parallel.

Choosing the domain for the input variables  for
training the neural network is crucial to the accuracy of
the resultant time integration algorithm and to the efficiency of
network training. The training domain should be sufficiently large,
in order to adequately cover the region of interest in the
phase space. An overly large training domain,
on the other hand, increases the difficulty in network training.
%
When the right hand side of the non-autonomous system exhibits a periodicity
with respect to any of its arguments, while the solution itself to
the problem is not periodic, we show that in this case the algorithmic function
is either periodic, or when it is not, shows a well-defined relation
for different periods.
This property about the algorithmic function
can greatly simply the choice of the training domain and
the algorithm learning.

% what are the results?
% comparison with scipy methods
% comparison between explicit and implicit NN methods
% near exponential convergence w.r.t. Q and M
% NN-Exp-S0, NN-Exp-S1 suitable for non-stiff and stiff problems
% NN methods are accurate and efficient

We have performed extensive numerical experiments with  benchmark problems
(non-stiff, stiff, or chaotic systems) to
evaluate the performance of the learned NN algorithms, and in particular
to compare them with the leading traditional algorithms
from the scipy library (DOP853, RK45, RK23, Radau, BDF).
We have made the following observations:
\begin{itemize}

\item
  The learned NN  algorithms produce highly accurate
  solutions with a high computational efficiency.
  Their solution accuracy  increases nearly exponentially,
  while their time-marching cost
  grows only quasi-linearly, as the
  number of degrees of freedom
  (training collocation points,  training parameters)
  in the ELM network increases.

\item
  The implicit NN algorithms are not as competitive as the explicit ones,
  in terms of both the accuracy and the time-marching cost.

\item
  Among the explicit NN algorithms, the accuracy increases from NN-Exp-S0
  to NN-Exp-S1, and to NN-Exp-S2, under comparable simulation
  parameters (training data points and training parameters).
  The reverse is true in terms of the time-marching cost.

\item
  The NN-Exp-S0 algorithm is observed to work well for both non-stiff and stiff
  problems. NN-Exp-S1 appears also able to work with both types of problems, but
  not as well as NN-Exp-S0 for stiff ones. On the other hand,  NN-Exp-S2 encounters
  difficulties for stiff problems (e.g.~failure to converge in NN training).

\item
  The computational performance of the learned NN algorithms is highly competitive
  compared with the traditional algorithms from scipy. The NN-Exp-S0 algorithm
  outperforms the best of the scipy methods in most of the tests,
  achieving superior accuracy under a comparable time-marching cost or the same
  accuracy under a lower time-marching cost.

\end{itemize}
The numerical results demonstrate that the learned NN  algorithms are
accurate, efficient, and computationally competitive.


% what are the limitations?
% what are the outstanding problems?
%
% accuracy remain same when reducing dt:
%   optimal distribution of collocation points wrt xi
%   how to increase accuracy leveraging small dt and allow larger dt simultaneously?
% how to do adaptive time-stepping?

% adaptive stepping

An outstanding problem with the learned NN algorithms is the current lack of an
effective adaptive-stepping strategy, which would be crucial for
efficiently solving stiff problems.
The strategy described in Section~\ref{sec_vdp} for
the stiff van der Pol oscillator problem takes advantage of
domain decomposition and employs different $h_{\max}$ on different sub-domains
for training the ELM  network. During time integration,
the time step size is adapted depending on the sub-domain that
the current solution approximation falls in. This is a useful strategy,
but it is only quasi-adaptive. That the NN algorithms fail to
outperform the scipy Radau (or BDF) method  for the stiff van der Pol
oscillator case is largely due to the lack of an effective adaptive-stepping
strategy. This is an important research problem and will be pursued
in a future endeavor.

% what else to discuss here?





