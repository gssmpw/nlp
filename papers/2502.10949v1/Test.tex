\section{Computational Examples}
\label{sec_tests}


In this section we evaluate the performance of the learned time integration algorithms 
from Section~\ref{sec_method} by simulating the dynamics of
several non-autonomous and autonomous systems.
Some of these systems are chaotic, or can become
stiff for a range of problem parameters.
In particular, we compare the learned NN algorithms with
the leading traditional time integration algorithms
as implemented in the Scipy library (scipy.integrate.solve\_ivp routine,
with methods ``DOP853'', ``RK45'', ``RK23'', ``Radau'' and ``BDF'')
in terms of the solution accuracy and the time marching cost.
All the scipy methods are adaptive in the step size and in the integration order,
with ``DOP853'', ``RK45'', and ``RK23'' being explicit and
``Radau'' and ``BDF'' being implicit schemes.
With the learned NN algorithms, we employ a constant time step size
for the majority of
simulations, and a quasi-adaptive time step for
certain stiff cases.


We define the maximum error $e_{\max}$ and the root-mean-squares (rms)
error $e_{\text{rms}}$ of a solution
$y(t)=(y_1(t),y_2(t),\dots,y_n(t))\in\mbb R^n$ for $t\in[t_0,t_f]$ as follows,
  \begin{equation}
    e_{\max} = \max\{\ \max\{\ |y_i^c(t_j) - y_i^{ex}(t_j)|\  \}_{j=1}^m  \ \}_{i=1}^n, \quad
    e_{\text{rms}} = \sqrt{\frac{1}{mn}\sum_{i=1}^n\sum_{j=1}^m|y_i^c(t_j) - y_i^{ex}(t_j)|^2 }.
  \end{equation}
Here $y^c$ is the numerical solution obtained by the NN algorithms (or
the scipy methods), $y^{ex}$ is the exact solution (if available)
or a reference solution, and $t_j\in[t_0,t_f]$ ($1\leqslant j\leqslant m$)
denotes the time instants resulting from the
time marching algorithm corresponding to a constant or a quasi-adaptive
time step. When the exact solution is unavailable, the reference solution
is computed using the scipy method ``DOP853'' (for non-stiff problems)
or ``Radau'' (for stiff problems) with absolute tolerance $10^{-16}$
and relative tolerance $10^{-13}$.  

% comment on time marching using psi(y0,t0,xi)
% customized NN evaluation:
%    model parameter extraction; evaluation using numpy;
%    scalar routines for time-stepping
%    determine which sub-domain to use for time-marching
%    provide example to compare timing: keras graph mode, keras predict routine,
%       and customized evaluation routine

For the learned NN algorithms, the time-marching cost lies primarily in
the forward evaluation of the trained NN (see~\eqref{eq_27})
at every time step. Therefore, efficient NN evaluation
is critical to the performance of the current NN algorithms.
As mentioned before, our implementation 
is based on the Tensorflow and Keras libraries.
We observe that the built-in NN evaluation methods from the Keras library, such as
the direct call against a Keras model (or the ``predict()'' method),
even in the graph mode,
induces a significant overhead, slowing down the NN time marching.

\begin{table}[tb]
  \centering
  \begin{tabular}{lcc}
    \hline
    $M$ & current-NN-evaluation (seconds) & keras-NN-evaluation (seconds) \\
    $400$ & $0.00138$ & $0.0258$ \\
    $600$ & $0.00148$ & $0.0270$ \\
    $800$ & $0.00155$ & $0.0271$ \\
    \hline
  \end{tabular}
  \caption{Comparison of the time-marching cost (wall time) of
    NN-Exp-S1 employing (i) the current customized NN evaluation method, and
    (ii) the Keras built-in NN evaluation method (in graph mode),
    for the test problem from Section~\ref{sec_lin_model} ($\lambda=100$).
    The tests correspond to those in Figure~\ref{fg_5}.
    $M$ is the number of training parameters.
    The built-in evaluation method calls the Keras model 
    directly with the input data.
  }
  \label{tab_1}
\end{table}


To reduce the NN evaluation overhead, we have employed a customized
evaluation method for the trained NN  using routines from
the numpy library. Here is the main idea for the customized evaluation.
After the neural network is trained,
we extract the weight and bias coefficients, as well as the activation functions,
for all layers from the trained Keras model. Then we form a routine in which
the extracted data are employed to implement the NN logic using
plain numpy functions. This routine is used as the customized NN evaluation
method for time marching.


% comment on scalar input in time marching
%
We would like to mention
another point in our implementation, regarding the computation of $f(y,t)$
for given $(y,t)$ ($y\in\mbb R^n$) in the customized NN evaluation method, which is needed
for computing the $F(y_0,t_0,\xi)$ term in~\eqref{eq_11}.
We have implemented a routine for computing $f(y,t)$, used
specifically for time marching, in which the input data $(y,t)$ is
a vector with shape $(n+1,)$ (in the Python notation). Note that the computation of
$f(y,t)$ is also required during NN training, where the input data is a matrix
with shape ($N_c,n+1$) that represents $(y,t)$ on $N_c$ collocation points.
In the version specifically for time marching,
the simpler vectorial input data of $(y,t)$ allows
more efficient implementation for computing $f(y,t)$.

The learned algorithms
employing the above customized NN evaluation method is significantly faster
than those based on the built-in evaluation methods from Keras.
This point is demonstrated by Table~\ref{tab_1}, which shows the time-marching time
of the NN-Exp-S1 algorithm (see Remark~\ref{rem_c26})
for the test problem of Section~\ref{sec_lin_model}
employing the customized NN evaluation method and the Keras built-in method.
The algorithm using the customized NN evaluation is faster by more
than an order of magnitude.


% need to comment on the normalization layer between input and the first hidden layer
% special normalization for xi variable

When implementing the $\varphi$-subnet for representing $\varphi(y_0,t_0,\xi)$,
we have added a normalization between the input layer
(representing $(y_0,t_0,\xi)$) and the first hidden layer. The normalization implements
an affine transform for each component of $(y_0,t_0,\xi)$, transforming
the input $(y_0,t_0)$ data
from $[a_1,b_1]\times\dots\times[a_n,b_n]\times[T_1\times T_2]\subset\mbb R^n\times\mbb R$ to
the standard domain $[-1,1]^n\times[-1,1]$ and
the input $\xi$ data from $[0,h_{\max}]$ to the interval
$[0,\frac{1}{\delta_m}]$. Here the prescribed constant $\delta_m>0$ will be referred
to as the $\xi$-domain map factor, with $\delta_m=1$ by default.
For some stiff problems, we find it more favorable to employ a smaller $\delta_m$
with the NN algorithms. These $\delta_m$ values will be provided when discussing
the specific test problems. The use of $\delta_m$ essentially stipulates
that the normalization should map the interval $[0,\delta_m h_{\max}]$
to the standard domain $[0,1]$ for $\xi$.
In the simulations  we will
employ $\delta_m=1$, unless otherwise specified.
The normalization procedure discussed here
is implemented using a ``lambda'' layer from the Keras library.
When domain decomposition is present, the above normalization is implemented
for the local NN  on each sub-domain.
In the simulations we employ no domain decomposition by default,
unless otherwise specified. 

When the training domain is decomposed along some direction,
in Remark~\ref{rem_26} we have discussed enlarging the sub-domains
by a factor $r$ along this direction during network training.
In the following numerical tests
we employ an enlargement factor $r=0$ (i.e.~no enlargement) by default,
unless otherwise specified.


\subsection{A Linear System: Non-Stiff and Stiff Cases}
\label{sec_lin_model}


\begin{table}[tb]
  \centering
  \begin{tabular}{l|l|l}
    \hline
   $\lambda=100$ & domain: $(y_{0},t_0,\xi)\in [-1.1,1.1]\times[-0.05,1.05]\times[0,h_{\max}]$
    & NN: $[3, M, 1]$ ($M$ varied) \\
    & no domain decomposition & activation function: Gaussian \\
   & $r$: $0.0$  & $\delta_m$: $1$  \\
   & $Q$: varied & $R_m$: to be specified \\
    & $\Delta t$: $0.02$ (for time marching) & time: $t\in[0,1]$  \\
    \hline
    $\lambda=10^6$ & domain: $(y_{0},t_0,\xi)\in [-1.1,1.1]\times[-0.05,1.05]\times[0,h_{\max}]$
    & NN: $[3, M, 1]$ ($M$ varied) \\
    & sub-domains: 2 along $t_0$ (uniform) & activation function: Gaussian \\
    & $r$: $0.05$ along $t_0$ & $\delta_m$: $0.02$ or $0.01$  \\
    & $Q$: $1000$ or $900$ (random) & $R_m$: to be specified \\
    & $\Delta t$: $0.02$ (for time marching) &  time: $t\in[0,1]$ \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the linear system
    (Section~\ref{sec_lin_model}).
    Values for some parameters are specified in the text.
  }
  \label{tab_a2}
\end{table}


In this test we consider a linear non-autonomous  problem,
\begin{subequations}\label{eq_53}
  \begin{align}
    &
    \frac{dy}{dt} = -\lambda[y - \cos(\pi t)], \quad t\in[0,1], \\
    &
    y(t_0) = y_0,
  \end{align}
\end{subequations}
where $\lambda>0$ is a constant, $y(t)$ is the unknown
to be computed, $t_0=0$, and $y_0=0$ is the initial condition.
This problem becomes very stiff with large $\lambda$ values.
It has the following exact solution,
\begin{equation}\label{eq_54}
  y(t) = \left[y_0-\frac{\lambda^2}{\lambda^2+\pi^2}\cos(\pi t_0)
     - \frac{\lambda\pi}{\lambda^2+\pi^2}\sin(\pi t_0)
     \right] e^{-\lambda(t-t_0)}
  + \frac{\lambda^2}{\lambda^2+\pi^2}\cos(\pi t)
  + \frac{\lambda\pi}{\lambda^2+\pi^2}\sin(\pi t).
\end{equation}

\begin{figure}
  \centerline{
    \includegraphics[width=2.0in]{Figures/Dahl/soln_profile_nn_exact_Lambda100_Rm0.5Q2500M800_dt0.02_A.pdf}(a)
    \includegraphics[width=2.0in]{Figures/Dahl/error_profile_nn_Lambda100_Rm0.5Q2500M800_dt0.02_A.pdf}(b)
    \includegraphics[width=2.0in]{Figures/Dahl/dahl_soln_hist_euler_dt0.02_lambda100_A.pdf}(c)
  }
  \caption{Linear model ($\lambda=100$):
    (a) Comparison of $y(t)$ between the NN-Exp-S1 solution and the exact solution.
    (b) Absolute error history of  the NN-Exp-S1 solution.
    (c) History of $y(t)$ obtained by the forward Euler method
    (i.e.~the $F(y_0,t_0,\xi)$ component in NN-Exp-S1, see~\eqref{eq_12b}).
    Training domain: $h_{\max}=0.03$,
    NN: [3, 800, 1], % Gaussian activation,
    $Q=2500$, and $R_m=0.5$. See Table~\ref{tab_a2} for the other parameter values.
  }
  \label{fg_3}
\end{figure}


% how to learn the algorithm?

We use an ELM network with architecture $[3,M,1]$
and the Gaussian activation function
$\sigma(x)=e^{-x^2}$ for the $\varphi$-subset
 to learn the  algorithmic function $\psi(y_0,t_0,\xi)$,
where $M$ is varied. The hidden-layer coefficients of the $\varphi$-subnet are
assigned to random values generated on the interval $[-R_m,R_m]$ drawn from
a uniform distribution, where the constant $R_m$ is specified below.
The NN is trained on a domain
$(y_0,t_0,\xi)\in[-1.1,1.1]\times[-0.05,1.05]\times[0,h_{\max}]$, where $h_{\max}$
is specified below. We employ $Q$ random collocation points,
where $Q$ is  varied,
uniformly drawn from the domain to train the NN  as discussed in
Section~\ref{sec_method}.
We employ the trained NN
to solve the problem~\eqref{eq_53}  for $t\in[0,1]$,
using a step size $\Delta t=0.02$ and the initial condition $(y_0,t_0)=(0,0)$.
The maximum and rms errors ($e_{\max}$, $e_{\text{rms}}$) of the numerical
solution against the exact solution~\eqref{eq_54} are then computed and
the time marching cost (wall time) of the NN algorithms is recorded for analysis.
Table~\ref{tab_a2} summarizes the simulation parameters related to the NN algorithms.
The same parameters will appear in the subsequent
test problems.


% overview of results

We first consider a non-stiff case, with $\lambda=100$ in~\eqref{eq_53}, and
illustrate the characteristics of the learned NN algorithms. 
Figures~\ref{fg_3}(a,b) provide an overview of the NN solution, comparing
the solution histories from the NN-Exp-S1 algorithm (see Remark~\ref{rem_c26})
and the exact solution~\eqref{eq_54} and showing the absolute-error history
of NN-Exp-S1. The parameter values for NN-Exp-S1 are listed in the figure caption
or in Table~\ref{tab_a2}.
The NN solution is highly accurate, with a maximum error on the order of $10^{-10}$
over $t\in[0,1]$.
Notice that the NN-Exp-S1 algorithm consists of two components (see~\eqref{eq_12b}),
$F(y_0,t_0,\xi)$ and $\varphi(y_0,t_0,\xi)$, with the former being the forward
Euler formula and the latter a neural network correction.
We find that the $\varphi(y_0,t_0,\xi)$ component is critical to
the NN-Exp-S1 accuracy. Without this component, the solution becomes very poor.
Figure~\ref{fg_3}(c) shows the history of $y(t)$ obtained by
the forward Euler method ($\Delta t=0.02$), i.e.~the $F(y_0,t_0,\xi)$ component
solely. The numerical solution is highly oscillatory, with no accuracy at all. 

\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Dahl/dahl_error_Q_Rm0.5M800_Dt0.02_A.pdf}(a)
    \includegraphics[width=2in]{Figures/Dahl/dahl_error_M_Rm0.5Q2500_Dt0.02_A.pdf}(b)
    \includegraphics[width=2in]{Figures/Dahl/dahl_error_hmax_Rm0.5M800Q2500_Dt0.02_A.pdf}(c)
  }
  \caption{Linear model ($\lambda=100$):
    Time marching errors ($e_{\max}$, $e_{\text{rms}}$) versus (a) the number of training
    collocation points ($Q$), (b) the number of training parameters ($M$)
    in NN, and (c) the domain size $h_{\max}$ along $\xi$, obtained
    with the NN-Exp-S1 algorithm.
    %Training domain: $[-1.1,1.1]\times[-0.05,1.05]\times[0,h_{\max}]$,
    %with $h_{\max}$ varied in (c) and $h_{\max}=0.03$ in (a,b);
    $h_{\max}=0.03$ in (a,b) and is varied in (c);
    %No domain decomposition, $\xi$-domain map factor $\delta_m=1$.
    %NN: $[3, M, 1]$, with $M$ varied in (b) and $M=800$ in (a,c),
    %Gaussian activation function.
    $M=800$ in (a,c) and is varied in (b);
    %$Q$ random collocation points in network training, with $Q$ varied in (a)
    %and $Q=2500$ in (b,c).
    $Q=2500$ in (b,c) and is varied in (a);
    $R_m=0.5$; Other simulation parameters are given in Table~\ref{tab_a2}.
    %for random hidden-layer coefficients.
    %$\Delta t=0.02$ in time integration for $t\in[0,1]$.
    %NN: $[3, M, 1]$, Gaussian activation. Training domain:
    %$(y_0,t_0,\xi)\in[-1.1,1.1]\times[-0.05,1.05]\times[0,0.03]$.
    %ELM 1st-order ERK formulation (NN-Exp-S1).
    %in (a), $M=800$ and $Q$ varied. in (b), $Q=2500$ and $M$ varied.
  }
  \label{fg_4}
\end{figure}



Figure~\ref{fg_4} illustrates the effects of several simulation parameters
on the attained NN algorithm.
It shows the maximum and rms time-marching errors of NN-Exp-S1
as a function of the number of collocation points ($Q$), the number of training parameters ($M$),
and the domain size $h_{\max}$ along $\xi$,
used for training the algorithm. The errors are
obtained on the points corresponding to a time step $\Delta t=0.02$ for $t\in[0,1]$.
The parameter values are specified in the figure caption or Table~\ref{tab_a2}.
Each plot represents a group of tests, in which one parameter is varied 
while the other parameters are fixed when training the NN-Exp-S1 network,
and the trained algorithm is employed in time marching to obtain the errors.
For example, in Figure~\ref{fg_4}(a) the number of training collocation points
$Q$ is varied systematically when training NN-Exp-S1, while the other parameters are fixed.
%
% what are the results?
We observe that the NN errors decrease 
nearly exponentially with increasing number of collocation points ($Q$) or
 training parameters ($M$). 
 Using a smaller domain along $\xi$ (i.e.~smaller $h_{\max}$)
 generally leads to improved accuracy in
the attained NN algorithm.
Numerical experiments suggest that the accuracy of the NN algorithm seems to
be determined at the time of training. Once the network is trained,
the accuracy of the resultant NN algorithm will be fixed.
Varying the step size $\Delta t\in[0,h_{\max}]$ in time marching
using the trained NN algorithm appears to have little influence on the
time-marching error. This characteristic is somewhat different from traditional
numerical algorithms, whose accuracy generally
improves with decreasing time step size.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Dahl/dahl_maxerr_M_lambda100_NN_compare_order.pdf}(a)
    \includegraphics[width=2in]{Figures/Dahl/dahl_rmserr_M_lambda100_NN_compare_order.pdf}(b)
    \includegraphics[width=2in]{Figures/Dahl/dahl_walltime_M_lambda100_NN_compare_order.pdf}(c)
  }
  \caption{Linear model ($\lambda=100$):
    Comparison of  (a) the maximum and (b) the rms time-marching errors,
    and (c) the time-marching cost (wall time)
     versus the number of training parameters ($M$)
     among the explicit and implicit NN algorithms.
     % (see~\eqref{eq_12} and~\eqref{eq_22}).
     %with explicit formulations from~\eqref{eq_12}
     %and implicit formulations from~\eqref{eq_22}.
     $h_{\max}=0.03$, and $M$ is varied in the tests.
     %NN: $[3, M, 1]$ with $M$ varied, Gaussian activation;
     %training domain: $[-0.05,1.05]\times[-1.1,1.1]\times[0,0.03]$;
    %$\Delta t=0.02$ in time marching for $t\in[0,1]$.
    NN-Exp-S0: $R_m=0.06$, $Q=2500$;
    NN-Exp-S1: $R_m=0.5$, $Q=2500$;
    NN-Exp-S2: $R_m=0.6$, $Q=3500$;
    NN-Imp-S1: $R_m=0.5$, $Q=2500$;
    NN-Imp-S2: $R_m=0.5$, $Q=3500$.
    Other simulation parameters are given in Table~\ref{tab_a2}.
  }
  \label{fg_5}
\end{figure}

A comparison of the accuracy and the time-marching cost (wall time)
of different NN algorithms with explicit (see~\eqref{eq_12}-\eqref{eq_12c})
and implicit (see~\eqref{eq_22})  formulations
is provided in Figure~\ref{fg_5}. These plots show the
maximum and rms errors and the time-marching time
as a function of the number of
training parameters ($M$) in the NN obtained by
NN-Exp-S0, NN-Exp-S1, NN-Exp-S2, NN-Imp-S1,
and NN-Imp-S2 (see Remark~\ref{rem_c26}).
The simulation parameters for this group of tests are specified in
the figure caption or in Table~\ref{tab_a2}.
Among the explicit NN algorithms, NN-Exp-S2 tends to be more
accurate than NN-Exp-S1, which in turn is generally more accurate
than NN-Exp-S0. In terms of time-marching cost 
NN-Exp-S0 is the fastest, followed by NN-Exp-S1 and then NN-Exp-S2.
The implicit NN algorithms 
are observed to be less accurate than the explicit ones.
For example, NN-Imp-S1 is the least accurate among these algorithms, with error levels
several orders of magnitude larger than those of the other algorithms,
and NN-Imp-S2 exhibits an accuracy comparable to NN-Exp-S0 for this problem.
The time marching cost of the implicit NN algorithms is notably higher than 
that of the explicit ones.
Since this test problem is linear, here the implicit NN algorithms do not actually
entail the solution of nonlinear algebraic equations during time marching.
For nonlinear problems in subsequent subsections
the implicit NN algorithms would be considerably slower compared with
the explicit ones, due to the need for solving nonlinear algebraic
systems during time integration.
Overall the implicit NN algorithms are not as competitive as the explicit ones.


% compare NN and scipy solvers

\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Dahl/dahl_maxerr_walltime_compare_NN_scipy_lambda100_A.pdf}(a)
    \includegraphics[width=2in]{Figures/Dahl/dahl_rmserr_walltime_compare_NN_scipy_lambda100_A.pdf}(b)
  }
  \caption{Linear model ($\lambda=100$):
    Comparison of
    (a) the maximum and (b) the rms time-marching errors 
    versus the time-marching cost (wall time)
    between the current NN algorithms (NN-Exp-S0 and NN-Exp-S1)
    and the scipy methods.
    The settings and parameters for NN-Exp-S0 and NN-Exp-S1
    follow those of Figure~\ref{fg_5}, with
    data points corresponding to different $M$ in the NN architecture.
    %$\Delta t=0.02$ in time marching for $t\in[0,1]$.
    Scipy methods: adaptive time step, adaptive order, absolute tolerance $10^{-16}$,
    data points corresponding to different relative tolerance values,
    dense output on points corresponding to $\Delta t=0.02$ for $t\in[0,1]$.
  }
  \label{fg_6}
\end{figure}


Figure~\ref{fg_6} shows a comparison of the computational performance (accuracy
versus cost)
between the NN algorithms and the traditional time integration algorithms from
the scipy library.
It depicts the maximum and rms time-marching errors
as a function of the time-marching cost (wall time) obtained
by the current NN-Exp-S0 and NN-Exp-S1 algorithms
and several scipy methods, including ``RK23''
(explicit Runge-Kutta method or order 3(2))~\cite{BogackiS1989},
``RK45'' (explicit Runge-Kutta method of order 5(4))~\cite{DormandP1980},
``DOP853'' (explicit Runge-Kutta
method of order 8)~\cite{HairerNW1993},
``Radau'' (implicit Runge-Kutta method of Radau IIA family
of order 5)~\cite{HairerW1996}, and ``BDF'' (implicit multi-step
variable-order (orders 1 to 5)
based on backward differentiation formulas enhanced with NDF modification)~\cite{ByrneH1975,ShampineR1997}.
The data for NN-Exp-S0 and NN-Exp-S1 correspond to those
in Figure~\ref{fg_5}.
The scipy results are obtained by employing
the routine ``scipy.integrate.solve\_ivp()'' from the scipy library
with different methods.
Different data points for the scipy methods correspond to
different relative tolerance values, while the absolute tolerance is
fixed at $10^{-16}$. Since the scipy methods are adaptive in time step and
order, we have used the dense output option to attain their solutions on
points corresponding to a time step size $\Delta t=0.02$ for $t\in[0,1]$
to compute the errors.
Among the scipy methods, DOP853 and RK45 show the best performance,
followed by Radau and BDF, and then by RK23.
The NN-Exp-S0 algorithm appears to perform slightly better than NN-Exp-S1
for this problem. 
Both NN-Exp-S0 and NN-Exp-S1 significantly outperform the scipy methods.
Here by ``outperform'' we refer to the ability to achieve superior accuracy
under the same time-marching cost or achieve the same accuracy under a lower
time-marching cost.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Dahl/dahl_nn_soln_lambda1e6_Rm0.4M1000Q1000_2Elem.pdf}(a)
    \includegraphics[width=2in]{Figures/Dahl/dahl_nn_error_lambda1e6_Rm0.4M1000Q1000_2Elem.pdf}(b)
    %\includegraphics[width=2in]{Figures/Dahl/dahl_nn_soln_lambda1e6_Rm0.6M1000Q2500_A.pdf}(a)
    %\includegraphics[width=2in]{Figures/Dahl/dahl_nn_error_lambda1e6_Rm0.6M1000Q2500_A.pdf}(b)
  }
  \caption{Linear model ($\lambda=10^6$, stiff):
    (a) Comparison between the NN-Exp-S0 solution and the exact solution.
    (b) Absolute-error history of the NN-Exp-S0 solution.
    %training domain:
    %$(y_0,t_0,\xi)\in[-1.1,1.1]\times[-0.05,1.05]\times[0,0.025]$,
    $h_{\max}=0.025$,
    2 uniform sub-domains along $t_0$
    with enlargement factor $r=0.05$; 
    $M=1000$, $Q=1000$, $R_m=0.4$, and $\delta_m=0.02$.
    %On each sub-domain, NN: $[3, 1000, 1]$, Gaussian activation function,
    %$Q=1000$ uniform random collocation points;
    %$R_m=0.4$ for random hidden-layer coefficients;
    %$\Delta t=0.02$ in time marching for $t\in[0,1]$.
    %$\xi$-domain map factor: $\delta_m=0.02$.
    See Table~\ref{tab_a2} for the other parameter values.
  }
  \label{fg_7}
\end{figure}

We next consider a stiff case, with $\lambda=10^6$ in problem~\eqref{eq_53}.
Among the explicit NN algorithms in~\eqref{eq_12}-\eqref{eq_12c}, we observe that
NN-Exp-S0 works well for the
stiff problem, while NN-Exp-S1
works not as well as NN-Exp-S0 and NN-Exp-S2 fails to work (NN training fails
to converge, with large loss values).
The implicit NN algorithms from~\eqref{eq_22}
also work well for this stiff problem, but is not as competitive
as the explicit NN-Exp-S0 algorithm in performance.

Figure~\ref{fg_7} illustrates the characteristics of
the NN-Exp-S0 solution for this stiff case. 
Figure~\ref{fg_7}(a) compares the NN-Exp-S0 solution with
the exact solution, and Figure~\ref{fg_7}(b) shows the absolute error
of the NN-Exp-S0 solution.
The training domain, the NN structure, and the other parameter values are provided
in the figure caption or in Table~\ref{tab_a2}. In particular, two uniform sub-domains are
employed along the $t_0$ direction, with an enlargement factor $r=0.05$
for network training (see Remark~\ref{rem_26}), and
we have employed a $\xi$-domain map factor $\delta_m=0.02$
when normalizing the input data on each sub-domain
(see discussions at the beginning of Section~\ref{sec_tests}).
A time step size $\Delta t=0.02$ is used in time marching for
$t\in[0,1]$. The NN-Exp-S0 method is highly accurate,
with a maximum error on the order of $10^{-8}$ over the domain.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Dahl/dahl_maxerr_walltime_compare_scipy_lambda1e6_A.pdf}(a)
    \includegraphics[width=2in]{Figures/Dahl/dahl_rmserr_walltime_compare_scipy_lambda1e6_A.pdf}(b)
  }
  \caption{Linear model ($\lambda=10^6$, stiff):
    Comparison of (a) the maximum and (b) the rms time-marching errors
    versus the time-marching cost (wall time) 
    between the NN algorithms (NN-Exp-S0, NN-Imp-S1)
    and the  scipy methods.
    %Both NN algorithms: training domain $(y_0,t_0,\xi)\in[-1.1,1.1]\times[-0.05,1.05]\times[0,h_{\max}]$,
    %2 uniform sub-domains along $t_0$, with an enlargement factor $r=0.05$ during
    %network training;
    %On each sub-domain,
    %NN: $[3,M,1]$ (with $M$ varied), Gaussian activation function;
    %$\Delta t=0.02$ in time marching for $t\in[0,1]$.
    NN-Exp-S0: $h_{\max}=0.025$, $Q=1000$, $R_m=0.4$, $\delta_m=0.02$;
    NN-Imp-S1: $h_{\max}=0.024$, $Q=900$, $R_m=0.35$, $\delta_m=0.01$;
    The other parameter values are provided in Table~\ref{tab_a2}.
    %$Q=1000$, sub-domain
    %safety factor in $t_0$: 0.05; $R_m=0.4$, $\Delta t=0.02$ in time marching.
    %NN-Imp-S1: training domain $(y_0,t_0,\xi)\in[-1.1,1.1]\times[-0.05,1.05]\times[0,0.024]$,
    %2 uniform sub-domains along $t_0$ direction; On each sub-domain,
    %NN architecture: [3,M,1] with $M$ varied, Gaussian activation, $Q=900$, sub-domain
    %safety factor in $t_0$: 0.05; $R_m=0.35$, $\Delta t=0.02$ in time marching.
    Scipy methods: absolute tolerance $10^{-16}$, 
    data points corresponding to different relative tolerance values,
    dense output on points corresponding to $\Delta t=0.02$ for $t\in[0,1]$.
    %is varied to attain results with
    %different accuracies.
    %$\xi$ direction normalization: for NN-Exp-S0 $0.02h_{\max}$ is normalized to unit
    %value; for NN-Imp-S1 $0.01h_{\max}$ is mapped to unit value.
  }
  \label{fg_8}
\end{figure}

In Figure~\ref{fg_8} we compare the maximum and rms solution errors
versus the time-marching cost (wall time) obtained
by the NN algorithms (NN-Exp-S0 and NN-Imp-S1)
and the scipy methods for this stiff problem. 
The parameter values employed in
these tests are provided in the figure caption or in Table~\ref{tab_a2}.
The explicit scipy methods (DOP853, RK45, and RK23)
are not competitive for this stiff problem,
as expected, inducing a large time marching cost.
In contrast, the implicit scipy methods (Radau and BDF) perform considerably better.
The NN-Exp-S0 algorithm is more competitive than NN-Imp-S1, but
appears slightly less accurate than the latter for this problem.
Both NN-Exp-S0 and NN-Imp-S1 significantly outperform
the implicit and explicit scipy methods for this stiff problem.


% convergence wrt collocation points, training parameters
% effect of different orders of formulation: 1st-order, 2nd-order, 3rd-order
% effect of delta-t, different y0
% long-time integration

% compare with ERK, IRK, RK45, RK23, DOP853, BDF, Radau IIA
%   in terms of accuracy, computational cost



\subsection{Pendulum Problem}

We test the learned NN algorithms using
the nonlinear pendulum problem in this section.
We study an autonomous system (free pendulum) first,
followed by a non-autonomous system (forced pendulum).



\subsubsection{Free Pendulum Oscillation}
\label{sec_321}

\begin{table}[tb]
  \centering
  \begin{tabular}{l|l}
    \hline
    domain: $(y_{01},y_{02},\xi)\in [-2,2]\times[-4,4]\times[0,0.25]$
    & sub-domains: 3, along $y_{01}$, uniform\\
    NN ($\varphi$-subnet) architecture: $[3, M, 2]$ ($M$ varied) & $r$: $0.1$ \\
    activation function: Gaussian & $\delta_m$: $1.0$  \\
    $Q$: $900$, $1000$ or $1500$ (random) & $R_m$: to be specified  \\
    $\Delta t$: $0.2$ (for time marching) & time: $t\in[0,200]$ (for time marching) \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the free pendulum problem (Section~\ref{sec_321}).
    %Values for some parameters are specified later.
  }
  \label{tab_a3}
\end{table}


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_y1_hist_nn_Rm0.6Q1000M800_3elem.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_y2_hist_nn_Rm0.6Q1000M800_3elem.pdf}(b)
    \includegraphics[width=2in]{Figures/Pend/pend_error_hist_nn_Rm0.6Q1000M800_3elem.pdf}(c)
  }
  \caption{Free pendulum: Comparison 
    of (a) $y_1(t)$ and (b) $y_2(t)$ between the NN-Exp-S1 solution and the reference
    solution. (c) Absolute-error histories of the NN-Exp-S1 solution for
    $y_1(t)$ and $y_2(t)$.
    NN-Exp-S1: $M=800$, $Q=1000$, $R_m=0.6$; See Table~\ref{tab_a3} for the other
    parameter values.
    %training domain $(y_{01},y_{02},\xi)\in[-2.0,2.0]\times[-4.0,4.0]\times[0,0.25]$,
    %three uniform sub-domains along the $y_{01}$ direction with an enlargement factor $r=0.1$;
    %On each sub-domain, NN: $[3,800,2]$, Gaussian activation,
    %$Q=1000$ uniform random collocation points, $R_m=0.6$ for the hidden-layer coefficients;
    %$\Delta t=0.2$ in time marching for $t\in[0,200]$.
    %Reference solution: computed by the scipy DOP853 method with relative tolerance
    %$10^{-13}$ and absolute tolerance $10^{-16}$.
  }
  \label{fg_9}
\end{figure}

Consider the initial value problem with the free pendulum equation,
\begin{subequations}\label{eq_56}
  \begin{align}
    &
    \frac{dy_1}{dt} = y_2, \quad
    \frac{dy_2}{dt} = -\alpha y_2 - \beta \sin y_1, \\
    &
    y_1(t_0) = y_{01}, \quad y_2(t_0) = y_{02},
  \end{align}
\end{subequations}
where $\alpha$ and $\beta$ are prescribed positive constants,
$y(t) = (y_1(t), y_2(t))\in\mbb R^2$ are the unknowns to be
computed, $t_0$ is the initial time, and
$y_0=(y_{10},y_{20})\in\mbb R^2$ are the initial conditions.
Physically $y_1(t)$ represents the pendulum angle, and $y_2(t)$
is the angular velocity.
In the following tests we employ $(\alpha,\beta)=(0.1,9.8)$, $t_0=0$, and
the initial condition $y_0=(y_{01},y_{02})=(1.0,-1.0)$, unless otherwise specified.


We employ an ELM network to learn the algorithmic function $\psi(y_{01},y_{02},\xi)$.
Table~\ref{tab_a3} summarizes the simulation parameters related to the NN algorithm.
In particular, we employ three uniform sub-domains along the $y_{01}$ direction,
with an enlargement factor $r=0.1$. The parameters $R_m$, $\Delta t$ and $\delta_m$
have the same meanings as in Section~\ref{sec_lin_model}, and $Q$ and $M$ refer to
the number of random collocation points and the hidden-layer width
of the $\varphi$-subnet on each sub-domain.
After the NN is trained, we employ the learned algorithm to solve
the problem~\eqref{eq_56} for $t\in[0,200]$ with a step size
$\Delta t=0.2$ and the initial condition $(y_{01},y_{02})=(1,-1)$.
The errors of the NN solution against a reference solution and the NN time-marching
time are recorded for analysis.
The reference solution is obtained by the using the scipy DOP853 method
with a sufficiently small tolerance (absolute tolerance $10^{-16}$, relative tolerance $10^{-13}$).


Figure~\ref{fg_9} provides 
an overview of the solution characteristics obtained by the NN algorithm.
It compares the NN-Exp-S1 solution for $y_1(t)$ and $y_2(t)$
with their reference solutions, and shows the absolute errors of the
NN-Exp-S1 solutions. The parameter values are given in
the figure caption or in Table~\ref{tab_a3}.
The NN solution is highly accurate, with a maximum error on the order of $10^{-9}$
for the time range.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_maxerr_M_compare_NN.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_rmserr_M_compare_NN.pdf}(b)
    \includegraphics[width=2in]{Figures/Pend/pend_walltime_M_compare_NN.pdf}(c)
  }
  \caption{Free pendulum: Comparison of (a) the maximum and (b) the rms time-marching errors,
    and (c) the time-marching cost (wall time) versus the hidden-layer width ($M$)
    in ELM network for different NN algorithms.
    %All algorithms:
    %training domain $(y_{01},y_{02},\xi)\in[-2.0,2.0]\times[-4.0,4.0]\times[0,0.25]$,
    %three uniform sub-domains along $y_{01}$ direction with enlargement factor $r=0.1$;
    %On each sub-domain, NN: $[3,M,2]$ ($M$ varied), Gaussian activation function;
    %$\Delta t=0.2$ in time marching for $t\in[0,200]$.
    NN-Exp-S0: $R_m=0.6$, $Q=1000$.
    NN-Exp-S1: $R_m=0.6$, $Q=1000$.
    NN-Exp-S2: $R_m=0.65$, $Q=900$.
    NN-imp-S1: $R_m=0.85$, $Q=1500$.
    NN-Imp-S2: $R_m=0.85$, $Q=1000$.
    Other parameter values are given in Table~\ref{tab_a3}.
  }
  \label{fg_10}
\end{figure}


Figure~\ref{fg_10} is a comparison of the accuracy and the time-marching cost
of different NN algorithms for solving this problem. 
Here we plot the maximum and rms time-marching errors ($e_{\max}$, $e_{\text{rms}}$),
as well as the time-marching time,
versus the hidden-layer width ($M$) of the $\varphi$-subnet for
the NN algorithms with explicit (NN-Exp-S0, NN-Exp-S1, and NN-Exp-S2) and
implicit (NN-Imp-S1, and NN-Imp-S2) formulations.
The parameter values are listed in the figure caption or in Table~\ref{tab_a3}.
We can make several observations.
First, the NN solution errors decrease nearly exponentially
with increasing number of hidden-layer nodes, while the time-marching
cost only grows quasi-linearly.
Second, the explicit NN algorithms tend to be more accurate than the implicit ones.
Third, among the explicit NN algorithms the accuracy generally increases
from NN-Exp-S0 to NN-Exp-S1, and to NN-Exp-S2. Between the implicit
NN algorithms, NN-Imp-S2 is significantly more accurate than NN-Imp-S1.
Fourth, in terms of time-marching cost the explicit NN algorithms
are much faster than the implicit ones, by an order of magnitude
for this problem.
Among the explicit NN algorithms, NN-Exp-S0 is the fastest, followed by NN-Exp-S1 and
NN-Exp-S2. Of the implicit ones, NN-Imp-S1 is notably faster than NN-Imp-S2.
Overall, the explicit NN algorithms are more competitive than the implicit ones.
This is similar to what has been observed for the test problem in Section~\ref{sec_lin_model}.

The observation that implicit NN algorithms are less competitive than the explicit ones
appears to be a common characteristic in the test problems we have considered, including
those in the subsequent subsections. Implicit NN algorithms
are computationally much more expensive, and
generally not as accurate as the explicit ones.
For this reason, the computation results using the implicit NN algorithms
will not be included for the simulations in the following subsections.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_maxerr_walltime_compare_nnexps0s1_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_rmserr_walltime_compare_nnexps0s1_scipy.pdf}(b)
  }
  \caption{Free pendulum:
    Comparison of (a) the maximum and (b) the rms time-marching errors versus the time marching
    cost (wall time) between the NN algorithms
    (NN-Exp-S0, NN-Exp-S1) and the scipy methods.
    Data for the NN algorithms correspond to
    those of NN-Exp-S0 and NN-Exp-S1 in Figure~\ref{fg_10}.
    %with the hidden-layer
    %width $M$ varied and the time step $\Delta t=0.2$ in time marching for $t\in[0,200]$. 
    %Both NN algorithms: 3 uniform sub-domains along $y_{01}$ direction, $[3,M,2]$ with $M$ varied,
    %Gaussian activation function; constant $\Delta t=0.2$ in time marching
    %for $t\in[0,200]$; $R_m=0.6$ for random hidden-layer coefficients,
    %$Q=1000$ random collocation points.
    Scipy methods: absolute tolerance $10^{-16}$, relative tolerance is varied,
    dense output on time instants
    corresponding to $\Delta t=0.2$ for $t\in[0,200]$.
  }
  \label{fg_11}
\end{figure}

Figure~\ref{fg_11} is a comparison of the computational performance
(accuracy versus cost) between the
NN algorithms (NN-Exp-S0 and NN-Exp-S1) and the scipy methods for the free pendulum
problem. It shows the maximum and rms errors of the NN and scipy solutions
versus their time-marching cost. The parameters and the settings for
the NN algorithms correspond to those in Figure~\ref{fg_10}, in which
the hidden-layer width of the $\varphi$-subnet is varied.
The scipy solutions are obtained by varying the relative tolerance, with
a fixed absolute tolerance $10^{-16}$ for different methods.
Among the scipy methods, DOP853 shows the best performance,
followed by RK45 and Radau. NN-Exp-S0 exhibits a slightly better performance than NN-Exp-S1.
Both NN-Exp-S0 and
NN-Exp-S1 significantly outperform the scipy methods. 


% show results for periodicity in f(t,y)?

\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_periy1_y1y2_hist_alpha0.02_Rm0.6M800Q1500_3elem.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_periy1_error_hist_alpha0.02_Rm0.6M800Q1500_3elem.pdf}(b)
    }
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_periy1_y1y2_hist_alpha0.005_Rm0.6M800Q1500_3elem.pdf}(c)
    \includegraphics[width=2in]{Figures/Pend/pend_periy1_error_hist_alpha0.005_Rm0.6M800Q1500_3elem.pdf}(d)
  }
  \caption{Free pendulum (exploiting periodicity of $f(y,t)$):
    NN-Exp-S1 solutions for $y_1$ and $y_2$ (left column) and their absolute errors (right column)
    for the problem~\eqref{eq_56} with the initial condition $(y_{01},y_{02})=(0,6.5)$ and
    the parameters $(\alpha,\beta)=(0.02,9.8)$ (top row)
    or $(\alpha,\beta)=(0.005,9.8)$ (bottom row).
    Training domain: $(y_{01},y_{02},\xi)\in[-3.2,3.2]\times[-6.7,6.7]\times[0,0.15]$,
    $3$ uniform sub-domains along $y_{01}$ with an enlargement factor $r=0.1$;
    On each sub-domain, NN: $[3,800,2]$,
    Gaussian activation function, $R_m=0.6$, $Q=1500$,
    $\Delta t=0.1$ in time marching for $t\in[0,100]$.
    Error computed relative to a reference solution (by scipy DOP853).
  }
  \label{fg_12}
\end{figure}

Since the RHS of system~\eqref{eq_56} is periodic in $y_1$ with a period $2\pi$,
i.e.~$f(y_1+2\pi,y_2,t)=f(y_1,y_2,t)$, the domain for the NN training
only needs to cover one period along the $y_{01}$ direction, in light of
Theorem~\ref{thm_a2}.
This will be crucial for computing the NN solutions corresponding to large
initial angular velocity ($y_{02}$) values, in which the pendulum angle ($y_1$)
can assume large magnitudes.
Figure~\ref{fg_12} shows the NN-Exp-S1 solutions for two such cases,
which exploit the periodicity of $f(y_1,y_2,t)$ during time-marching as
discussed in Remark~\ref{rem_210}.
These results correspond to an initial condition $(y_{01},y_{02})=(0,6.5)$ at $t_0=0$,
and the problem parameters $(\alpha,\beta)=(0.02,9.8)$ and $(\alpha,\beta)=(0.005,9.8)$,
respectively. Because the initial angular velocity $y_{02}=6.5$ is sufficiently
large, the pendulum spins around the axis for several periods before
settling down to oscillate around the equilibrium position (at an elevated angle $y_1$).
The simulation parameters for this group of tests are specified in the figure caption.
In particular, the NN is trained on the domain
$(y_{01},y_{02},\xi)\in[-3.2,3.2]\times[-6.7,6.7]\times[0,0.15]$, which
is slightly larger than one period $[-\pi,\pi]$ along $y_{01}$.
For time-marching using the learned $\psi(y_{01},y_{02},\xi)$, we exploit
the periodicity of $f(y_1,y_2,t)$ as follows (see also Remark~\ref{rem_210}).
Given $(y_{1k},y_{2k},t_k)$ and the step size $\Delta t$,
we compute $y_{1k}^*=\text{mod}(y_{1k}+\pi,2\pi)-\pi$,
$q=\lfloor\frac{y_{1k}+\pi}{2\pi} \rfloor$, and
$(y_{1,k+1},y_{2,k+1})=\psi(y_{1k}^*,y_{2k},\Delta t)+(2\pi q, 0)$.
The results in Figure~\ref{fg_12} confirm the effectiveness of
the method described in Remark~\ref{rem_210}
for taking advantage of the RHS $f(y,t)$ that is periodic with respect to $y$,
and show that the NN algorithm is accurate for this type of problems.
Without exploiting this periodicity, one would need to use a domain having a
large dimension along $y_{01}$ to train the NN,
which would pose significant challenges to network training for such problems.


\subsubsection{Forced Pendulum Oscillation}
\label{sec_322}

\begin{table}[tb]
  \centering
  \begin{tabular}{l|l}
    \hline
    domain: $(y_{01},y_{02},t_0,\xi)\in [-2,2]\times[-4,4]\times[0,2.01]\times[0,0.11]$
    & sub-domains: 3, along $y_{01}$, uniform\\
    NN ($\varphi$-subnet) architecture: $[4, M, 2]$ ($M$ varied) & $r$: $0.1$ \\
    activation function: Gaussian & $\delta_m$: $5.0$ or $7.0$  \\
    $Q$: $2000$ or $1500$, random & $R_m$: to be specified \\
    $\Delta t$: $0.1$ (for time marching) & time: $t\in[0,200]$ \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the forced
    pendulum problem (Section~\ref{sec_322}).
    %Values for some parameters are specified later.
  }
  \label{tab_2}
\end{table}


We next consider a non-autonomous system with the forced pendulum equation,
\begin{subequations}\label{eq_a56}
  \begin{align}
    &
    \frac{dy_1}{dt} = y_2, \quad
    \frac{dy_2}{dt} = -\alpha y_2 - \beta \sin y_1 + \gamma\cos(\pi t), \label{eq_a56a} \\
    &
    y_1(t_0) = y_{01}, \quad y_2(t_0) = y_{02},
  \end{align}
\end{subequations}
where $\gamma$ is an additional constant and the other variables are
the same as in Section~\ref{sec_321}.
We employ $(\alpha,\beta,\gamma)=(0.1,9.8,0.2)$, 
$y_0=(y_{01},y_{02})=(1.0,-1.0)$, and $t_0=0$ for this problem.

We train the NN using the parameter values from Table~\ref{tab_2}
to learn the algorithmic function $\psi(y_{01},y_{02},t_0,\xi)$.
Since the forcing term is periodic, the RHS of~\eqref{eq_a56a} satisfies
$f(y_1,y_2,t+2)=f(y_1,y_2,t)$.
Therefore, we only need to learn $\psi(y_{01},y_{02},t_0,\xi)$
on a domain that is not smaller than $2.0$ along $t_0$
(see Theorem~\ref{thm_a1}).
We will use $[0,2.01]$ as the training domain along $t_0$.

\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_forced_y1_hist_nn_exact_Rm0.4Q2000M1200_A.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_forced_y2_hist_nn_exact_Rm0.4Q2000M1200_A.pdf}(b)
    \includegraphics[width=2in]{Figures/Pend/pend_forced_error_hist_nn_Rm0.4Q2000M1200_A.pdf}(c)
  }
  \caption{Forced pendulum: Comparison 
    of (a) $y_1(t)$ and (b) $y_2(t)$ between the NN-Exp-S1 solution and
    the reference solution. (c) Absolute-error histories of
    the NN-Exp-S1 solution for $y_1(t)$ and $y_2(t)$.
    NN-Exp-S1: $M=1200$, $Q=2000$, $\delta_m=5.0$,
    and $R_m=0.4$; Other parameter values are given in Table~\ref{tab_2}.
    %training domain
    %$(y_{01},y_{02},t_0,\xi)\in[-2,2]\times[-4,4]\times[0,2.01]\times[0,0.11]$, three uniform
    %sub-domains along $y_{01}$ direction, overlap factor $0.1$, domain-t-map-factor $5.0$.
    %On each sub-domain, NN: [4, 1200, 2], Gaussian activation, $Q=2000$ uniform random
    %points, $R_m=0.4$ for ELM hidden-layer coefficients, $\Delta t=0.1$
    %in time marching for $t\in[0,200]$.
    Reference solution: obtained by the scipy DOP853 method with relative tolerance $10^{-13}$
    and absolute tolerance $10^{-16}$.
  }
  \label{fg_13}
\end{figure}

Figure~\ref{fg_13} illustrates the solutions for $y_1(t)$ and $y_2(t)$,
and their absolute errors, for $t\in[0,200]$ obtained by NN-Exp-S1.
The errors are computed against a reference solution attained by the scipy DOP853 method
with absolute tolerance $10^{-16}$ and  relative tolerance
$10^{-13}$. The reference solution is also shown in Figures~\ref{fg_13}(a,b) for comparison.
The parameter values  are provided in the figure
caption or in Table~\ref{tab_2}.
The NN solutions for $y_1(t)$ and $y_2(t)$  are highly accurate,
with a maximum error on the order
of $10^{-7}$ for a step size $\Delta t=0.1$.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_forced_maxerr_M_nn_expS0S1S2_A.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_forced_rmserr_M_nn_expS0S1S2_A.pdf}(b)
    \includegraphics[width=2in]{Figures/Pend/pend_forced_walltime_M_nn_expS0S1S2_A.pdf}(c)
  }
  \caption{Forced pendulum:
    Comparison of (a) the maximum and (b) the rms solution errors,
    and (c) the time marching cost (wall
    time) versus the hidden-layer width $M$ for
    the NN algorithms (NN-Exp-S0, NN-Exp-S1, and NN-Exp-S2).
    %Training domain:
    %$(y_{01},y_{02},t_0,\xi)\in[-2,2]\times[-4,4]\times[0,2.01]\times[0,0.11]$, three uniform
    %sub-domains along $y_{01}$ direction with an enlargement factor $r=0.1$.
    %On each sub-domain, NN: $[4, M, 2]$ ($M$ varied), Gaussian activation function,
    %$Q=2000$ uniform random collocation points,
    %$\Delta t=0.1$ in time marching for $t\in[0,200]$;
    %$R_m=0.4$ for ELM hidden-layer coefficients,
    NN-Exp-S0: $Q=2000$, $R_m=0.3$, $\delta_m=7.0$;
    NN-Exp-S1: $Q=2000$, $R_m=0.4$, $\delta_m=5.0$.
    NN-Exp-S2: $Q=1500$, $R_m=0.4$, $\delta_m=5.0$.
    Other parameter values are given in Table~\ref{tab_2}.
  }
  \label{fg_14}
\end{figure}

A comparison of the solution errors and the time-marching cost of different NN algorithms
is provided in Figure~\ref{fg_14}. Here we plot the maximum and rms solution errors
on $t\in[0,200]$, and the time-marching time,
versus the hidden-layer width $M$
obtained by NN-Exp-S0, NN-Exp-S1 and NN-Exp-S2.
The parameter values are provided in the figure caption or in
Table~\ref{tab_2}.
The solution errors decrease nearly exponentially
with increasing  $M$, while the time-marching cost grows
only quasi-linearly as $M$ increases.
NN-Exp-S2 is more accurate than NN-Exp-S1, which in turn
is more accurate than NN-Exp-S0.
In terms of the time-marching cost, NN-Exp-S2 is the most expensive
and NN-Exp-S0 is the least expensive among them.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Pend/pend_forced_maxwerr_walltime_compare_nn_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/Pend/pend_forced_rmswerr_walltime_compare_nn_scipy.pdf}(b)
  }
  \caption{Forced pendulum:
    Comparison of (a) the maximum and (b) the rms solution errors versus the time-marching
    cost (wall time) between the NN algorithms (NN-Exp-S0 and NN-Exp-S1)
    and the scipy methods.
    Data for NN-Exp-S0 and NN-Exp-S1 correspond to those from Figure~\ref{fg_14}.
    %NN-Exp-S1: Training domain
    %$(y_{01},y_{02},t_0,\xi)\in[-2,2]\times[-4,4]\times[0,2.01]\times[0,0.11]$, three uniform
    %sub-domains along $y_{01}$ direction, overlap factor $0.1$, domain-t-map-factor $5.0$.
    %On each sub-domain, NN: $[4, M, 2]$ ($M$ varied), Gaussian activation, $Q=2000$ random
    %collocation points in training,
    %$R_m=0.4$ for ELM hidden-layer coefficients, constant $\Delta t=0.1$
    %in time marching for $t\in[0,200]$.
    Scipy methods: absolute tolerance $10^{-16}$,
    data points corresponding to different relative tolerance values,
    dense output on points corresponding to $\Delta t=0.1$
    for $t\in[0,200]$.
  }
  \label{fg_15}
\end{figure}

A comparison of the computational performance (accuracy versus cost)
between the NN algorithms and the scipy methods is shown in Figure~\ref{fg_15}.
Here we plot the maximum and rms solution errors as a function of
the time-marching time (for $t\in[0,200]$) obtained by the NN-Exp-S0 and NN-Exp-S1
algorithms and the scipy methods.
The two NN algorithms exhibit essentially the same performance.
Among the scipy methods, DOP853 shows the best performance,  followed
by RK45 and the other methods. Both NN-Exp-S0 and NN-Exp-S1 markedly outperform DOP853 and the
other scipy methods.



%\subsection{Asymptotically Softening Nonlinear Spring}


\subsection{Van der Pol Oscillator}
\label{sec_vdp}


\begin{table}[tb]
  \centering
  \begin{tabular}{l|l|l}
    \hline
   $\mu=5$ & domain: $(y_{01},y_{02},\xi)\in [-2.05,2.05]\times[-8,8]\times[0,0.035]$
    & NN ($\varphi$-subnet): $[3, M, 2]$ \\
    & sub-domains: 3, along $y_{01}$, uniform & activation function: Gaussian \\
   & $r$: $0.1$  & $\delta_m$: $1$  \\
   & $Q$: $1500$, random & $R_m$: to be specified \\
    & $\Delta t$: $0.03$ (time-marching) & time: $t\in[0,120]$  \\
    \hline
    $\mu=100$ & domain: $(y_{01},y_{02},\xi)\in [-2.05,2.05]\times[-140,140]\times[0,h_{\max}]$
    & NN ($\varphi$-subnet): $[3, M, 2]$ \\
    & sub-domains: 3 or 5 along $y_{01}$ (uniform); & activation function: Gaussian \\
    & $\quad$ 5 along $y_{02}$ (non-uniform), sub-domain & $\delta_m$: $1$  \\
    & $\quad$ boundaries: $[-140, -0.5, -0.03, 0.03, 0.5, 140]$ & $R_m$: to be specified \\
    & $\quad$ $h_{\max}$ varied for different sub-domains & $\Delta t$: quasi-adaptive  \\
    & $r$: $0.1$ along $y_{01}$ and $0.05$ along $y_{02}$, or $r=0$ in $y_{01}$ and $y_{02}$
    & time: $t\in[0,300]$  \\
   & $Q$: $1000$ or $1400$, random &  \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the van der Pol oscillator
    (Section~\ref{sec_vdp}).
    %Values for some parameters are specified later.
  }
  \label{tab_3}
\end{table}



In this test we evaluate the learned NN algorithms
using the van der Pol oscillator problem:
\begin{subequations}\label{eq_57}
  \begin{align}
    &
    \frac{dy_1}{dt} = y_2, \quad
    \frac{dy_2}{dt} = \mu (1-y_1^2)y_2 - y_1, \\
    &
    y_1(t_0) = y_{01}, \quad y_2(t_0) = y_{02},
  \end{align}
\end{subequations}
where $y(t)=(y_1(t),y_2(t))$ are the unknowns,
$\mu>0$ is a constant parameter, $t_0$ is the initial time, and
$y_0=(y_{01},y_{02})$ are the initial data. We employ $t_0=0$
and  $(y_{01},y_{02})=(2,0)$  for time integration.
This problem becomes stiff when $\mu$ is large. We use
$\mu=5$ for a non-stiff case and $\mu=100$ for a stiff case to
test the performance of the learned NN algorithms.

Let's first consider the problem~\eqref{eq_57} with $\mu=5$.
The simulation parameters related to the NN algorithm
are listed in Table~\ref{tab_3}.
In particular, we partition the training domain into uniform sub-domains
along the $y_{01}$ direction, and employ an ELM network with
architecture $[3,M,2]$ and the Gaussian activation function
for the $\varphi$-subnet
to learn $\psi(y_{01},y_{02},\xi)$,
where the hidden-layer width $M$ is varied. In this table $R_m$ again
denotes the scope of random hidden-layer coefficients.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/VDP/vdp_y1_hist_nn_Rm0.5Q1500M1100_3elem.pdf}(a)
    \includegraphics[width=2in]{Figures/VDP/vdp_y2_hist_nn_Rm0.5Q1500M1100_3elem.pdf}(b)
    \includegraphics[width=2in]{Figures/VDP/vdp_error_hist_nn_Rm0.5Q1500M1100_3elem.pdf}(c)
  }
  \caption{Van der Pol oscillator ($\mu=5$):
    Comparison of  (a) $y_1(t)$ and (b) $y_2(t)$ between the
    NN-Exp-S1 solution and the reference solution. (c) Absolute-error histories of
    the NN-Exp-S1 solution for $y_1(t)$ and $y_2(t)$.
    NN-Exp-S1: $M=1100$, $Q=1500$,
    $R_m=0.5$; Other parameter values are
    given in Table~\ref{tab_3}.
    Reference solution: obtained by the scipy DOP853 method with absolute tolerance $10^{-16}$ and
    relative tolerance $10^{-13}$.
    %dense output on points corresponding to $\Delta t=0.03$.
  }
  \label{fg_16}
\end{figure}

Figure~\ref{fg_16} illustrates characteristics of the NN solutions
and their accuracy by comparing the  $y_1(t)$ and $y_2(t)$
obtained by NN-Exp-S1 and a reference solution computed using the scipy DOP853 method
with a sufficiently small tolerance. The absolute errors for $y_1(t)$ and
$y_2(t)$ are also shown. The parameter values for these results
are either specified in the caption or listed in Table~\ref{tab_3}.
The NN-Exp-S1 solution is observed to be highly accurate, with a maximum error
on the order of $10^{-7}$ for $t\in[0,120]$.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/VDP/vdp_mu5.0_maxerr_M_nn_s0s1s2.pdf}(a)
    \includegraphics[width=2in]{Figures/VDP/vdp_mu5.0_rmserr_M_nn_s0s1s2.pdf}(b)
    \includegraphics[width=2in]{Figures/VDP/vdp_mu5.0_walltime_M_nn_s0s1s2.pdf}(c)
  }
  \caption{Van der pol oscillator ($\mu=5$):
    Comparison of (a) the maximum and (b) the rms time-marching errors, and (c) the
    time-marching cost (wall time) versus the hidden-layer width $M$
    for the NN algorithms (NN-Exp-S0, NN-Exp-S1, NN-Exp-S2).
    %Three uniform sub-domains  along $y_{01}$.
    NN-Exp-S0: $R_m=0.45$;
    NN-Exp-S1: $R_m=0.5$; NN-Exp-S2: $R_m=0.55$.
    See Table~\ref{tab_3} for the other parameter values.
    %training domain $(y_{01},y_{02},\xi)\in[-2.05,2.05]\times[-8,8]\times[0,0.035]$,
    %with two (NN-Exp-S1 and NN-Exp-S2) or three (NN-Exp-S0) uniform sub-domains
    %along the $y_{01}$ direction, overlap safety factor $0.1$,
    %NN structure $[3, M, 2]$ (with $M$ varied), Gaussian activation,
    %$Q=1500$ uniform random collocation points,
    %$R_m=0.45$ in NN-Exp-S0, $R_m=0.8$ in NN-Exp-S1 and $R_m=0.7$ in NN-Exp-S2 for
    %hidden-layer coefficients,
    %$\Delta t=0.03$ in time marching
    %for $t\in[0,120]$.
  }
  \label{fg_17}
\end{figure}


The convergence behavior of the NN algorithms is exemplified by Figure~\ref{fg_17}.
Here we plot the maximum and rms time-marching errors, as well as the time-marching cost,
of NN-Exp-S0, NN-Exp-S1 and NN-Exp-S2 for $t\in[0,120]$ as a function of
the hidden-layer width $M$ in the $\varphi$-subnet architecture.
The figure caption and Table~\ref{tab_3} provide all the parameter values
corresponding to these results.
It is evident that the  NN solution errors decrease nearly exponentially
with increasing $M$ (before saturation), and that
the time-marching cost grows only quasi-linearly
as $M$ increases.




\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/VDP/vdp_mu5_maxerr_walltime_compare_nn_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/VDP/vdp_mu5_rmserr_walltime_compare_nn_scipy.pdf}(b)
  }
  \caption{Van der Pol oscillator ($\mu=5$):
    Comparison of (a) the maximum error and (b) the rms error versus the time marching
    cost (wall time) between the NN algorithms (NN-Exp-S0 and NN-Exp-S1)
    and the scipy methods.
    Data for the NN algorithms correspond to those of NN-Exp-S0 and NN-Exp-S1 in
    Figure~\ref{fg_17}.
    %NN-Exp-S0: training domain $(y_{01},y_{02},\xi)\in[-2.05,2.05]\times[-8,8]\times[0,0.035]$,
    %three uniform sub-domains along the $y_{01}$ direction, overlap safety factor $0.1$,
    %NN structure: $[3,M,2]$ ($M$ varied),
    %Gaussian activation, $Q=1500$ uniform random collocation points,
    %$R_m=0.45$ for hidden-layer coefficients, $\Delta t=0.03$ in time marching
    %for $t\in[0,120]$.
    %NN-Exp-S1: training domain $(y_{01},y_{02},\xi)\in[-2.05,2.05]\times[-8,8]\times[0,0.035]$,
    %two uniform sub-domains along the $y_{01}$ direction, overlap safety factor $0.1$,
    %NN structure: $[3,M,2]$ ($M$ varied),
    %Gaussian activation, $Q=1500$ uniform random collocation points,
    %$R_m=0.8$ for hidden-layer coefficients, $\Delta t=0.03$ in time marching
    %for $t\in[0,120]$.
    Scipy methods: absolute tolerance $10^{-16}$,
    relative tolerance varied for different data points,
    dense output on points corresponding to $\Delta t=0.03$ for $t\in[0,120]$.
  }
  \label{fg_18}
\end{figure}

A performance comparison (accuracy vs. cost) between the current NN algorithms
and the scipy methods is provided in Figure~\ref{fg_18}.
Here we plot the maximum and rms time-marching errors (for $t\in[0,120]$)
as a function of the time-marching time obtained by 
NN-Exp-S0 and NN-Exp-S1, and by the scipy methods.
The data for NN-Exp-S0 and NN-Exp-S1 correspond to those in Figure~\ref{fg_17},
while those for the scipy methods are attained by varying the relative tolerance values.
The performance of NN-Exp-S0 and NN-Exp-S1 is close,
with the former appearing slightly better.
The DOP853 method exhibits the best performance among the scipy methods,
followed by RK45 and Radau.
The NN-Exp-S0 and NN-Exp-S1 algorithms perform notably better than
the scipy methods for this non-stiff case  with the van der Pol oscillator.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/VDP/vdp_y1_hist_mu100_Rm0.75M800Q1400_compare.pdf}(a)
    \includegraphics[width=2in]{Figures/VDP/vdp_y2_hist_mu100_Rm0.75M800Q1400_compare.pdf}(b)
    \includegraphics[width=2in]{Figures/VDP/vdp_error_hist_mu100_Rm0.75M800Q1400_compare_A.pdf}(c)
  }
  \caption{Van der Pol oscillator ($\mu=100$):
    Comparison of (a) $y_1(t)$ and (b) $y_2(t)$ between
    the NN-Exp-S0 solution and the reference solution.
    (c) Absolute-error histories of the NN-Exp-S0 solution
    for $y_1(t)$ and $y_2(t)$.
    NN-Exp-S0: 
    %training domain $(y_{01},y_{02},\xi)\in[-2.05,2.05] \times [-140,140]
    %\times[0,h_{\max}]$;
    $3$ uniform sub-domains in $y_{01}$, with enlargement factor $r=0.1$;
    $5$ non-uniform
    sub-domains in $y_{02}$, with enlargement factor $r=0.05$,
    %and sub-domain boundaries given by $y_{02}=[-140, -0.5, -0.03, 0.03, 0.5, 140]$;
    NN architecture: $[3, 800, 2]$
    with Gaussian activation;
    %$h_{\max}$ varies for different sub-domains
    %along $y_{02}$, with
    %$h_{\max}=0.002, 0.011, 0.018, 0.011, 0.002$ for the 5 sub-domains in $y_{02}$;
    $R_m=0.75$, and $Q=1400$ on each sub-domain;
    See Table~\ref{tab_3} or the text for the other parameter values.
    %For time-marching $\Delta t$ is varied according to the sub-domains,
    %with $\Delta t=0.95h_{\max}$ for the sub-domain to which $(y_1^k,y_2^k)$ belongs.
    Reference solution is computed by the scipy Radau method, with absolute tolerance
    $10^{-16}$ and relative tolerance $10^{-13}$.
    The inset of plot (b) shows a magnified view around $t=244.02$.
  }
  \label{fg_19}
\end{figure}

We next consider the case $\mu=100$, with which the problem~\eqref{eq_57} becomes stiff.
Since the velocity of the oscillator (i.e.~$y_2$) is very small in the
majority of time
but can increase to large magnitudes in bursts, using a constant step size
for time integration becomes less efficient and it is necessary to incorporate
some adaptive strategy into the NN algorithm.

We employ the following quasi-adaptive strategy for network training
and for time integration when solving this problem. We first choose a training domain
for $(y_{01},y_{02})\in[A_1,A_2]\times[B_1,B_2]$, and then partition this domain
into $m$ ($m\geqslant 1$) sub-domains along $y_{01}$ and
$n$ ($n\geqslant 1$) sub-domains along $y_{02}$.
Let $\Omega_{ij}=[a_i,a_{i+1}]\times[b_j,b_{j+1}]$ ($0\leqslant i\leqslant m-1$,
$0\leqslant j\leqslant n-1$) denote a sub-domain, where $A_1=a_0<a_1<\cdots<a_m=A_2$
and $B_1=b_0<b_1<\cdots<b_n=B_2$.
On $\Omega_{ij}$ we train a local NN  to learn $\psi(y_{01},y_{02},\xi)$
for  $(y_{01},y_{02},\xi)\in[a_i,a_{i+1}]\times[b_j,b_{j+1}]\times[0,h_{\max}^{(ij)}]$, where
$h_{\max}^{(ij)}$ depends on  $\Omega_{ij}$ and
can differ in different sub-domains. This provides the opportunity, during
time marching, to vary the step size based on the current value of $(y_1,y_2)$.
Specifically for this problem, we choose $h_{\max}$ to depend only on $j$,
i.e.~$(y_{01},y_{02},\xi)\in[a_i,a_{i+1}]\times[b_j,b_{j+1}]\times[0,h_{\max}^{(j)}]$
for $\Omega_{ij}$, and we partition the domain non-uniformly along
the $y_{02}$ direction.
%
% how to do time-marching with the trained NN as above?
For time integration, given $(y_{1k},y_{2k})$ at time $t_k$,
we compute the next approximation as follows:
\begin{enumerate}[(i)]
\item Determine the sub-domain $\Omega_{ij}$ such that $(y_{1k},y_{2k})\in\Omega_{ij}$,
  and let $h=0.95h_{\max}^{(j)}$.
\item Compute
  $y_{k+1}=(y_{1,k+1},y_{2,k+1})=\psi(y_{1k},y_{2k},h)$ and $t_{k+1}=t_k+h$.
\end{enumerate}

We use a training domain $(y_{01},y_{02})\in[-2.05,2.05]\times[-140,140]$ for this stiff case,
as shown in Table~\ref{tab_3}, and partition the domain into $5$ non-uniform
sub-domains along $y_{02}$, with the sub-domain boundaries
given by $y_{02}=[-140, -0.5, -0.03, 0.03, 0.5, 140]$.
Along the $y_{01}$ direction we partition the domain into $3$ uniform
sub-domains (for NN-Exp-S0), or $5$ uniform sub-domains (for NN-Exp-S1).
We use
\begin{equation*}
  \begin{split}
    &\mbs h_{\max}=(h_{\max}^{(0)},h_{\max}^{(1)},\dots,h_{\max}^{(4)})
    =(0.002,0.011,0.018,0.011,0.002),
    \quad \text{(for NN-Exp-S0)}; \\
    & \mbs h_{\max}=(h_{\max}^{(0)},h_{\max}^{(1)},\dots,h_{\max}^{(4)})
    =(0.002,0.011,0.025,0.011,0.002),
    \quad \text{(for NN-Exp-S1)}.
  \end{split}
\end{equation*}
The NN-Exp-S2 algorithm does not work for this stiff case (training fails to
converge), and
the implicit NN algorithms (NN-Imp-S1, NN-Imp-S2) are
not as competitive as the explicit NN-Exp-S0 and NN-Exp-S1 algorithms.

Figure~\ref{fg_19} is a comparison of $y_1(t)$ and $y_2(t)$
between the NN-Exp-S0 solution and a reference solution obtained by
the scipy Radau method, and the absolute errors
of the NN-Exp-S0 solution.
The values for the simulation parameters are provided in the figure
caption or Table~\ref{tab_3}.
The NN solution agrees well with the reference solution,
with the maximum absolute error on the order of $10^{-3}$ for $y_1$
and $10^{-1}$ for $y_2$. The inset of Figure~\ref{fg_19}(b) shows that
even at the bursts the NN method has captured the solution
accurately. 


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/VDP/vdp_maxerr_walltime_mu100_nn_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/VDP/vdp_rmserr_walltime_mu100_nn_scipy.pdf}(a)
  }
  \caption{Van der Pol oscillator ($\mu=100$):
    Comparison of (a) the maximum and (b) the rms time-marching error 
    versus the time marching cost (wall time) between the NN algorithms (NN-Exp-S0,
    NN-Exp-S1) and the scipy methods for $t\in[0,300]$.
    %NN methods: architecture $[3, M, 2]$ ($M$ varied) with Gaussian activation.
    Simulation parameters for NN-Exp-S0 follow those of Figure~\ref{fg_19}.
    %NN-Exp-S0: training domain $(y_{01},y_{02},\xi)\in[-2.05,2.05] \times [-140,140]
    %\times[0,h_{\max}]$.
    %$3$ uniform sub-domains in $y_{01}$, with overlap factor $0.1$; $5$ non-uniform
    %sub-domains in $y_{02}$, with overlap factor $0.05$ and sub-domain boundaries
    %given by $[-140, -0.5, -0.03, 0.03, 0.5, 140]$; NN architecture: $[3, M, 2]$
    %($M$ varied),
    %Gaussian activation, on each sub-domain; $h_{\max}$ varied for different sub-domains
    %along the $y_{02}$ direction, with
    %$h_{\max}=0.002, 0.011, 0.018, 0.011, 0.002$ for the 5 sub-domains in $y_{02}$;
    %$Q=1400$ training collocation points on each sub-domain; $R_m=0.75$;
    %When time-marching $\Delta t$ is varied according to the sub-domains,
    %with $\Delta t=0.95h_{\max}$ for the sub-domain to which $(y_1^k,y_2^k)$ belongs.
    NN-Exp-S1:
    %training domain same as NN-Exp-S0;
    5 uniform sub-domains in $y_{01}$ with enlargement factor $r=0$;
    $5$ non-uniform sub-domains in $y_{02}$ with enlargement factor $r=0$;
    %and sub-domain boundaries
    %given by $[-140, -0.5, -0.03, 0.03, 0.5, 140]$; NN architecture: $[3, M, 2]$
    %($M$ varied),
    %Gaussian activation, on each sub-domain; $h_{\max}$ varied for different sub-domains
    %along the $y_{02}$ direction, with
    %$h_{\max}=0.002, 0.011, 0.025, 0.011, 0.002$ for the 5 sub-domains in $y_{02}$;
    $R_m=0.5$, and $Q=1000$ on each sub-domain;
    See Table~\ref{tab_3} and the text for the other parameter values.
    %When time-marching $\Delta t$ is varied according to the sub-domains,
    %with $\Delta t=0.95h_{\max}$ for the sub-domain to which $(y_1^k,y_2^k)$ belongs.
    Scipy methods:
    %adaptive time-stepping, adaptive order,
    absolute tolerance
    $10^{-16}$, data points corresponding to different relative tolerance values.
    Errors are computed with respect to a
    reference solution obtained by the scipy Radau method with absolute tolerance
    $10^{-16}$ and relative tolerance $10^{-13}$.
  }
  \label{fg_20}
\end{figure}

Figure~\ref{fg_20} compares the computational performance (accuracy versus cost)
of the current NN algorithms and the scipy methods for this stiff case.
It shows the maximum and the rms time-integration errors ($e_{\max}$ and $e_{\text{rms}}$)
on $t\in[0,300]$ as a function of the time-marching time for 
the NN-Exp-S0 and NN-Exp-S1 algorithms and the scipy methods.
The parameter values for the NN algorithms
are provided in the figure caption or in Table~\ref{tab_3}.
The scipy Radau method shows the best performance, followed by
the scipy BDF method. The NN-Exp-S0 and NN-Exp-S1 algorithms
are less competitive than the scipy Radau and BDF methods for this case.
The performance of the explicit scipy methods (DOP853, RK45, and RK23)
is significantly worse than the Radau/BDF and the NN-Exp-S0/NN-Exp-S1
algorithms.



\subsection{Lorenz63 Chaotic System}
\label{sec_lorenz63}


\begin{table}[tb]
  \centering
  \begin{tabular}{l|l}
    \hline
    domain: $(y_{01},y_{02},y_{03},\xi)\in [-20,20]\times[-25,25]\times[2,46]\times[0,0.012]$,
    & NN ($\varphi$-subnet): $[4, M, 3]$  \\
    $\quad$ or $(y_{01},y_{02},y_{03},\xi)\in [-20,15]\times[-27,16]\times[2,46]\times[0,0.012]$ & activation function: Gaussian  \\
    domain decomposition: none & $\delta_m$: $0.2$ \\
    % sub-domains: 3 along $y_{01}$, uniform & \\
    $r$: $0.0$  & $R_m$: to be specified  \\
    $Q$: $1000$ or $1200$, random & time: $t\in[0,t_f]$, $t_f=200$, $17$  \\
     $\Delta t$: $0.01$ (time-marching) &   \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the Lorenz63 model
    (Section~\ref{sec_lorenz63}).
    %Values for some parameters are specified later.
  }
  \label{tab_4}
\end{table}


\begin{figure}
  \centerline{
    \includegraphics[width=1.9in]{Figures/Lorenz63/lorenz63_phase_map_perspective.pdf}(a)
    \includegraphics[width=1.9in]{Figures/Lorenz63/lorenz63_phase_map_xzview.pdf}(b)
    \includegraphics[width=2.2in]{Figures/Lorenz63/lorenz63_y1_hist_long.pdf}(c)
  }
  \caption{Lorenz63 system: (a) Perspective view, and (b) projection to the
    $y_1$-$y_3$ plane
    of the phase space trajectories obtained by NN-Exp-S0.
    (c) $y_1(t)$ history.
    NN-Exp-S0: training domain
    $(y_{01},y_{02},y_{03},\xi)\in[-20,20]\times[-25,25]\times[2,46]\times[0,0.012]$,
    %t-map-factor=0.2, no domain decomposition,
    NN: $[4,900,3]$, $Q=1200$, $R_m=0.12$, time integration for $t\in[0,200]$.
    See Table~\ref{tab_4} for the other parameter values.
    %random collocation points $Q=1200$;  Gaussian activation;
    %$R_m=0.12$ for random hidden-layer coefficients; $\Delta t=0.01$ in time
    %marching for $t\in[0,200]$. 
  }
  \label{fg_21}
\end{figure}


In this subsection we test the NN algorithms using the Lorenz63 chaos model,
\begin{subequations}
\begin{align}
  & \frac{dy_1}{dt} = \sigma (y_2-y_1),\quad
   \frac{dy_2}{dt} = y_1(\rho-y_3) - y_2, \quad
   \frac{dy_3}{dt} = y_1y_2 - \beta y_3,\\
  & y_1(t_0)=y_{01}, \quad y_2(t_0) = y_{02}, \quad y_3(t_0) = y_{03},
\end{align}
\end{subequations}
where $y(t)=(y_1(t),y_2(t),y_3(t))$ are the unknowns,
$\sigma = 10$, $\rho=28$, $\beta=8/3$, and we employ the initial conditions
$y_0=(y_{01},y_{02},y_{03})=(-10,-10,25)$ with $t_0=0$ for time integration.


We learn the algorithmic function
$\psi(y_{01},y_{02},y_{03},\xi)$ using ELM with an
architecture $[4, M, 3]$ and Gaussian activation function
for the $\varphi$-subnet, where
$M$ is varied. Table~\ref{tab_4} provides values of the simulation
parameters related to the NN algorithms. No domain decomposition is used for
this problem.

Figure~\ref{fg_21} is an overview of the solution 
obtained by NN-Exp-S0. It shows the trajectory
in phase space with two different views, as well as the time history
of $y_1(t)$. The parameter values corresponding to these results
are listed in the figure caption or in Table~\ref{tab_4}.
The chaotic nature of the system is unmistakable.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_y1_hist_short_Rm0.12M900Q1200_1elem.pdf}(a)
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_y2_hist_short_Rm0.12M900Q1200_1elem.pdf}(b)
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_y3_hist_short_Rm0.12M900Q1200_1elem.pdf}(c)
  }
  \centerline{
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_y1error_hist_short_Rm0.12M900Q1200_1elem.pdf}(d)
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_y2error_hist_short_Rm0.12M900Q1200_1elem.pdf}(e)
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_y3error_hist_short_Rm0.12M900Q1200_1elem.pdf}(f)
  }
  \caption{Lorenz63 system ($t_f=17$): Comparison between the NN-Exp-S0 solutions and
    the reference solutions (top row) for $y_1$, $y_2$ and $y_3$, and
    the absolute errors (bottom row) 
     of the NN-Exp-S0 solutions.
     Parameter values for NN-Exp-S0 follow those of Figure~\ref{fg_21},
     except here for $t\in[0,17]$.
    % NN-Exp-S0: training domain $(y_{01},y_{02},y_{03},\xi)\in[-20,20]\times[-25,25]\times[2,46]\times[0,0.012]$,
    %domain-t map factor $0.2$, no domain composition; NN: $[4,900,3]$, Gaussian activation,
    %training collocation points $Q=1200$; $R_m=0.12$ for random hidden-layer coefficients;
    %$\Delta t=0.01$ in time marching for $t\in[0,17]$.
     Reference solution is obtained by the scipy ``DOP853'' method
     with absolute tolerance $10^{-16}$
     and relative tolerance $10^{-13}$, with
     dense output on points corresponding to $\Delta t=0.01$.
  }
  \label{fg_22}
\end{figure}

Because the system is chaotic, comparison of different methods to compute their errors
for long time integration becomes impractical. However,
if the time horizon is not very long, comparing two solutions to compute
the error is still physically meaningful.
We next concentrate on a shorter time horizon $t\in[0,17]$ and compare
the NN algorithms with the scipy methods to evaluate their performance.

A comparison between the NN-Exp-S0 solution and a reference solution
obtained by the scipy DOP853 method is provided in Figure~\ref{fg_22}.
It shows the histories of $y_1$, $y_2$ and $y_3$ ($t\in[0,17]$)
obtained by NN-Exp-S0 and DOP853,  as well
as the absolute errors between these two solutions.
The parameter values corresponding to these results are
provided in the caption or in Table~\ref{tab_4}.
The NN solution agrees very well with the scipy solution,
with the maximum error on the order of $10^{-5}$ in the domain.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_maxerr_walltime_compare_nn_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/Lorenz63/lorenz63_rmserr_walltime_compare_nn_scipy.pdf}(b)
  }
  \caption{Lorenz63 system ($t_f=17$): Comparison of (a) the maximum and
    (b) the rms solution errors
    versus the time marching cost (wall time)
    between the NN algorithms (NN-Exp-S0, NN-Exp-S1) and the scipy methods.
    Parameter values for NN-Exp-S0 follow those of Figure~\ref{fg_22}.
    %NN-Exp-S0: training domain $(y_{01},y_{02},y_{03},\xi)\in[-20,20]\times[-25,25]\times[2,46]\times[0,0.012]$,
    %domain-t map factor $0.2$, no domain composition; NN: $[4,M,3]$ ($M$ varied), Gaussian activation,
    %training collocation points $Q=1200$; $R_m=0.12$ for random hidden-layer coefficients,
    %$\Delta t=0.01$ in time marching.
    NN-Exp-S1: training domain $(y_{01},y_{02},y_{03},\xi)\in[-20,15]\times[-27,16]\times[2,46]\times[0,0.012]$,
    %domain-t map factor $0.2$, no domain composition; NN: $[4,M,3]$ ($M$ varied), Gaussian activation,
    $R_m=0.1$, $Q=1000$; Other parameter values are given in Table~\ref{tab_4}.
    %training collocation points $Q=1000$; $R_m=0.1$ for random hidden-layer coefficients,
    %$\Delta t=0.01$ in time marching.
    Scipy methods: %adaptive time-step, adaptive order, with
    absolute tolerance $10^{-16}$, 
    relative tolerance varied,  dense output
    on points corresponding to $\Delta t=0.01$.
    %Time marching for $t\in[0,17]$.
  }
  \label{fg_23}
\end{figure}

We compare the performance (accuracy versus cost) of the learned
NN algorithms and the scipy
methods  in Figure~\ref{fg_23}, which shows
the maximum and rms time-marching errors ($e_{\max}$, $r_{\text{rms}}$) obtained by
NN-Exp-S0/-S1 and the scipy methods as a function of the time-marching time
on $t\in[0,17]$. The figure caption and Table~\ref{tab_4} provide values
of the simulation parameters for these methods.
The data points for NN-Exp-S0 and NN-Exp-S1 correspond to different $M$
in the $\varphi$-subnet architecture $[4,M,3]$, and those data points for the scipy methods
correspond to different relative tolerance values.
The errors are computed against a reference solution attained by
the scipy DOP853 method with an absolute tolerance $10^{-16}$ and
a relative tolerance $10^{-13}$.
Among the scipy methods, DOP853 exhibits the best performance, followed by
RK45 and other methods. The NN-Exp-S0 and NN-Exp-S1 algorithms demonstrate
a similar performance, and both notably outperform the DOP853, RK45 and
the other scipy methods for this problem.



\subsection{Hindmarsh-Rose Neuronal Model}
\label{sec_hr}

\begin{table}[tb]
  \centering
  \begin{tabular}{l|l}
    \hline
    domain: $(y_{01},y_{02},y_{03},\xi)\in [-1.5,1.8]\times[-8,0.7]\times[2.7,3.3]\times[0,0.012]$,
    & NN ($\varphi$-subnet): $[4, M, 3]$  \\
    sub-domains: 4 along $y_{01}$ (non-uniform), sub-domain  & activation function: Gaussian  \\
    \quad\quad\quad boundaries: $[-1.5, -0.8, 0, 0.9, 1.8]$ & $\delta_m$: $1$ \\
    $r$: $0.0$  & $R_m$: to be specified  \\
    $Q$: $2000$, random & time: $t\in[0,t_f]$, $t_f=499.8$  \\
     $\Delta t$: $0.06$ (time-marching) &   \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the Hindmarsh-Rose model
    (Section~\ref{sec_hr}).
    %Values for some parameters are specified later.
  }
  \label{tab_5}
\end{table}

In the next example we test the NN algorithms using
the Hindmarsh-Rose model~\cite{HindmarshR1984},
which describes the spiking-bursting behavior of the membrane potential
in neurons. The model is given by,
\begin{subequations}
  \begin{align}
    & \frac{dy_1}{dt} = y_2 - y_1^3 + 3y_1 - y_3 + I, \quad
     \frac{dy_2}{dt} = 1-5y_1^2 - y_2, \quad
     \frac{dy_3}{dt} = 4\alpha\left(y_1 + \frac85 \right) - \alpha y_3, \\
    & y_1(t_0) = y_{01}, \quad y_2(t_0)=y_{02}, \quad y_3(t_0) = y_{03},
  \end{align}
\end{subequations}
where we employ $I=3.1$, $\alpha=0.006$, and the initial conditions
$(y_{01},y_{02},y_{03})=(-1,-3.5,3)$ with $t_0=0$.
Here $y_1$ represents the membrane potential, $y_2$ measures the
rate of ion transport,  and
$y_3$ is an adaption current that modulates the neuron firing rate.

Table~\ref{tab_5} lists the parameter values related to the
NN algorithms for learning $\psi(y_{01},y_{02},y_{03},\xi)$.
In particular, we employ $4$ non-uniform sub-domains
along the $y_{01}$ direction, with the sub-domain boundaries at
$y_{01}=-1.5$, $-0.8$, $0$, $0.9$, and $1.8$, with
an enlargement factor $r=0$ for network training.
The $\varphi$-subnet architecture is $[4,M,3]$ on each sub-domain, with
$M$ varied in the tests.


\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/HRN/hrn_y1_hist_Rm0.39M1200Q2000_4elem.pdf}(a)
    \includegraphics[width=2in]{Figures/HRN/hrn_y2_hist_Rm0.39M1200Q2000_4elem.pdf}(b)
    \includegraphics[width=2in]{Figures/HRN/hrn_y3_hist_Rm0.39M1200Q2000_4elem.pdf}(c)
  }
  \centerline{
    \includegraphics[width=2in]{Figures/HRN/hrn_y1error_hist_Rm0.39M1200Q2000_4elem.pdf}(d)
    \includegraphics[width=2in]{Figures/HRN/hrn_y2error_hist_Rm0.39M1200Q2000_4elem.pdf}(e)
    \includegraphics[width=2in]{Figures/HRN/hrn_y3error_hist_Rm0.39M1200Q2000_4elem.pdf}(f)
  }
  \caption{Hindmarsh-Rose neuron model:
    Comparison of  (a) $y_1(t)$, (b) $y_2(t)$, and (c) $y_3(t)$ between the NN-Exp-S1 solution
    and the reference solution. The absolute error for (d) $y_1(t)$, (e) $y_2(t)$,
    and (f) $y_3(t)$ of the NN-Exp-S1 solution.
    NN-Exp-S1:
    %training domain $(y_{01},y_{02},y_{03},\xi)\in[-1.5,1.8]\times[-8,0.7]\times[2.7,3.3]\times[0,0.062]$, 4 sub-domains in $y_{01}$ direction, sub-domain boundaries: $[-1.5,-0.8,0,0.9,1.8]$;
    %domain-t-map-factor=1;
    architecture [4,1200,3] on each sub-domain,
    %Gaussian activation, collocation points $Q=2000$;
    $R_m=0.39$; Other simulation parameters are given in Table~\ref{tab_5}.
    %for random hidden-layer coefficients, $\Delta t=0.06$ in time marching.
    Reference solution: computed by scipy DOP853, with absolute tolerance $10^{-16}$
    and relative tolerance $10^{-13}$, dense output on points corresponding to $\Delta t=0.06$.
    %for $t\in[0,499.8]$.
  }
  \label{fg_24}
\end{figure}


Figure~\ref{fg_24} illustrates the solution characteristics for $y_1(t)$, $y_2(t)$
and $y_3(t)$ obtained by NN-Exp-S1. 
A reference solution for these variables obtained by the scipy DOP853 method
is also shown in Figures~\ref{fg_24}(a,b,c) for comparison. The
absolute errors  are shown in the bottom row of the plots.
The  parameter values for these results
are provided in the figure caption or in Table~\ref{tab_5}.
The bursting behavior of the solution is evident from the history plots.
The NN algorithm has captured the dynamics of the system accurately.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/HRN/hrn_maxerr_walltime_compare_nn_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/HRN/hrn_rmserr_walltime_compare_nn_scipy.pdf}(b)
  }
  \caption{Hindmarsh-Rose neuron model:
    Comparison of (a) the maximum (b) the rms time-marching errors
    versus the time marching
    cost (wall time) between the NN algorithms (NN-Exp-S0, NN-Exp-S1)
    and the scipy methods.
    $R_m=0.12$ for NN-Exp-S0, $R_m=0.39$ for NN-Exp-S1; See Table~\ref{tab_5} for
    the other parameter values of the NN algorithms.
    %Current methods: training domain $(y_{01},y_{02},y_{03},\xi)\in[-1.5,1.8]\times[-8,0.7]\times[2.7,3.3]\times[0,0.062]$, 4 sub-domains in $y_{01}$ direction, sub-domain boundaries: $[-1.5,-0.8,0,0.9,1.8]$;
    %domain-t-map-factor=1; NN: $[4,M,3]$ ($M$ varied), Gaussian activation, collocation points $Q=2000$;
    %$R_m=0.12$ with NN-Exp-S0 and $R_m=0.39$ with NN-Exp-S1
    %for random hidden-layer coefficients;
    %$\Delta t=0.06$ in time marching
    %for $t\in[0,499.8]$.
    Scipy methods:
    %adaptive time-step, adaptive order,
    absolute tolerance $10^{-16}$,
    data points corresponding to different relative tolerance values.
    %relative tolerance varied to obtain solutions with different accuracy,
    %dense output on points corresponding to $\Delta t=0.06$ for $t\in[0,499.8]$.
  }
  \label{fg_25}
\end{figure}

In Figure~\ref{fg_25} we compare the computational performance (accuracy versus cost)
between the NN algorithms and the scipy methods for the Hindmarsh-Rose model.
It shows the maximum and rms time-marching errors
as a function of the time-marching cost (for $t\in[0,499.8]$) obtained
by the NN-Exp-S0/NN-Exp-S1 algorithms and the scipy methods.
The performance of NN-Exp-S0, NN-Exp-S1 and DOP853 is close to one another,
with NN-Exp-S0 being slightly better.
These three methods show a better performance than the Radau and RK45 methods.
All these methods perform significantly better than RK23 and BDF.


\subsection{Lorenz96 Chaotic System}
\label{sec_lorenz96}

\begin{table}[tb]
  \centering
  \begin{tabular}{l|l}
    \hline
    domain: $(y_{01},y_{02},y_{03},y_{04},y_{05},\xi)\in [-5,10]^5\times[0,0.011]$,
    & NN ($\varphi$-subnet): $[6, M, 5]$ \\
    \quad or $[-9,12]\times[-7,10]\times[-5.5,11]\times[-6,7]\times[-4,12]\times[0,0.011]$ &
    activation function: Gaussian\\
    sub-domains: 2 or 4 along $y_{01}$, uniform  & $\delta_m$: $1$  \\
    $r$: $0.0$ & $R_m$: to be specified   \\
    $Q$: $1500$ or $2000$, random  & time: $t\in[0,t_f]$, $t_f=5$ or $50$ \\
     $\Delta t$: $0.01$ (time-marching) &   \\
    \hline
  \end{tabular}
  \caption{NN simulation parameters for the Lorenz96 model
    (Section~\ref{sec_lorenz96}).
    %Values for some parameters are specified later.
  }
  \label{tab_6}
\end{table}



In the last example we employ the Lorenz96 chaotic system~\cite{Lorenz1996} to
evaluate the performance of the  NN  algorithms.
This model is given by (in $5$ dimensions),
\begin{subequations}
  \begin{align}
    & \frac{dy_1}{dt} = (y_2 - y_4)y_5 - y_1 + F, \quad
     \frac{dy_2}{dt} = (y_3 - y_5)y_1 - y_2 + F, \quad
     \frac{dy_3}{dt} = (y_4 - y_1)y_2 - y_3 + F, \\
    & \frac{dy_4}{dt} = (y_5 - y_2)y_3 - y_4 + F, \quad
     \frac{dy_5}{dt} = (y_1 - y_3)y_4 - y_5 + F, \\
    & y_1(t_0)=y_{01},\ y_2(t_0)=y_{02},\ y_3(t_0)=y_{03},\ y_4(t_0)=y_{04},\ y_5(t_0)=y_{05},
  \end{align}
\end{subequations}
where $F=8$, and the initial conditions are
$y_0=(y_{01},y_{02},y_{03},y_{04},y_{05})=(-0.99,-1,-1,-1,-1)$ with $t_0=0$.


We learn the algorithmic function
function $\psi(y_{01},y_{02},y_{03},y_{04},y_{05},\xi)$ using
an ELM network with the architecture $[6,M,5]$ and the Gaussian activation
function for the $\varphi$-subnet, where $M$ is varied. The simulation parameters
for the NN algorithms are summarized in Table~\ref{tab_6}.


\begin{figure}
  \centerline{
    \includegraphics[height=2.2in]{Figures/Lorenz96/lorenz96_y123_phase_long.pdf}(a)
    \includegraphics[height=2.5in]{Figures/Lorenz96/lorenz96_y1_hist_long.pdf}(b)
  }
  \caption{Lorenz96 system: (a) Trajectory in the $(y_1,y_2,y_3)$-phase space, and (b)
    the $y_1$ history, obtained by the NN-Exp-S1 algorithm.
    Training domain: $(y_{01},y_{02},y_{03},y_{04},y_{05},\xi)\in[-5,10]^5\times[0,0.011]$,
    with 2 uniform sub-domains along $y_{01}$;
    %t-map-factor = 1;
    NN: $[6,800,5]$ on each sub-domain;
    $R_m=0.15$, $Q=1500$,  time integration for $t\in[0,50]$;
    The Other parameter values
    are given in Table~\ref{tab_6}.
  }
  \label{fg_26}
\end{figure}

Figure~\ref{fg_26} provides an overview of the solution characteristics.
It shows the system trajectory  ($t\in[0,50]$) in
the $(y_1,y_2,y_3)$ phase space and the time history of $y_1$ obtained by
the NN-Exp-S1 algorithm. The figure caption and Table~\ref{tab_6} list
the parameter values for this set of tests. 


\begin{figure}
  \centerline{
    \includegraphics[width=1.8in]{Figures/Lorenz96/lorenz96_y1_hist_expS1_Rm0.15M1000Q1500_4elem.pdf}(a)
    \includegraphics[width=1.8in]{Figures/Lorenz96/lorenz96_y3_hist_expS1_Rm0.15M1000Q1500_4elem.pdf}(b)
    \includegraphics[width=1.8in]{Figures/Lorenz96/lorenz96_y4_hist_expS1_Rm0.15M1000Q1500_4elem.pdf}(c)
  }
  \centerline{
    \includegraphics[width=1.8in]{Figures/Lorenz96/lorenz96_y1_error_hist_expS1_Rm0.15M1000Q1500_4elem.pdf}(d)
    \includegraphics[width=1.8in]{Figures/Lorenz96/lorenz96_y3_error_hist_expS1_Rm0.15M1000Q1500_4elem.pdf}(e)
    \includegraphics[width=1.8in]{Figures/Lorenz96/lorenz96_y4_error_hist_expS1_Rm0.15M1000Q1500_4elem.pdf}(f)
  }
  \caption{Lorenz96 system ($t_f=5$): Comparison 
    of (a) $y_1$, (b) $y_3$, and (c) $y_4$ between the NN-Exp-S1 solution and
    the reference solution. Absolute errors for
    $y_1(t)$, $y_3(t)$ and $y_4(t)$ of the NN-Exp-S1 solution.
    NN-Exp-S1: training domain $(y_{01},y_{02},y_{03},y_{04},y_{05},\xi)\in[-9,12]\times[-7,10]\times[-5.5,11]\times[-6,7]\times[-4,12]\times[0,0.011]$, $4$ uniform sub-domains
    along $y_{01}$;
    %t-map-factor = 1;
    NN: $[6,1000,5]$ on each sub-domain,
    $R_m=0.15$, $Q=1500$, time integration for $t\in[0,5]$; See Table~\ref{tab_6}
    for the other parameter values.
    Reference solution: obtained by scipy DOP853, with
    absolute tolerance $10^{-16}$ and relative tolerance $10^{-13}$,
    dense output on points corresponding to $\Delta t=0.01$.
  }
  \label{fg_27}
\end{figure}

Due to the chaotic nature of the system, computing the error by comparing
the solution histories in long-term evolution becomes less meaningful.
We therefore consider a shorter history ($t\in[0,5]$) and compare
the current NN algorithms with the scipy methods.
Figure~\ref{fg_27} compares the solution histories for $y_1$, $y_3$ and $y_4$
from NN-Exp-S1 and  a reference solution computed by the scipy
DOP853 method (with a sufficiently small tolerance). It also shows
the absolute errors for these variables from the NN-Exp-S1 solution.
The NN algorithm has  captured the solution accurately.



\begin{figure}
  \centerline{
    \includegraphics[width=2in]{Figures/Lorenz96/lorenz96_maxerr_walltime_compare_nn_scipy.pdf}(a)
    \includegraphics[width=2in]{Figures/Lorenz96/lorenz96_rmserr_walltime_compare_nn_scipy.pdf}(b)
  }
  \caption{Lorenz96 system ($t_f=5$): Comparison of (a) the maximum
    and (b) the rms time-marching errors versus the time marching cost (wall time)
    between the NN algorithms (NN-Exp-S0, NN-Exp-S1) and the scipy methods.
    NN algorithms: training domain $(y_{01},y_{02},y_{03},y_{04},y_{05},\xi)\in[-9,12]\times[-7,10]\times[-5.5,11]\times[-6,7]\times[-4,12]\times[0,0.011]$, $4$ uniform sub-domains
    along $y_{01}$;
    NN: $[6,M,5]$ ($M$ varied);
    $R_m=0.05$ and $Q=2000$ for NN-Exp-S0, and $R_m=0.15$ and $Q=1500$ for NN-Exp-S1.
    See Table~\ref{tab_6} for the other parameter values.
    %Gaussian activation; $\Delta t=0.01$ in time marching for $t\in[0,5]$;
    %NN-Exp-S0: $R_m=0.05$, $Q=2000$ random collocation points on each sub-domain;
    %NN-Exp-S1: $R_m=0.15$, $Q=1500$ random collocation points on each sub-domain.
    Scipy methods: 
    absolute tolerance $10^{-16}$, data points corresponding to different relative tolerance
    values, dense output on points corresponding to
    $\Delta t=0.01$ for $t\in[0,5]$.
  }
  \label{fg_28}
\end{figure}

We next compare the computational performance (accuracy versus cost)
between the NN algorithms and
the scipy methods for the Lorenz96 system.
Figure~\ref{fg_28} shows the maximum and rms time-marching errors
($t\in[0,5]$) as a function of the time-marching time
obtained by the current NN-Exp-S0 and NN-Exp-S1 algorithms
and the scipy methods. The errors are computed with respect to a reference
solution obtained by the scipy DOP853 method with a sufficiently small
tolerance. The parameter values for these tests are given in
the figure caption or in Table~\ref{tab_6}.
The data points for NN-Exp-S0 and NN-Exp-S1 correspond to different
$M$ in the $\varphi$-subnet architecture $[6,M,5]$, while those for
the scipy methods correspond to different tolerance values.
Among the scipy methods DOP853 shows the best performance, followed by RK45,
Radau and the other methods.  The performance of NN-Exp-S1 and scipy
DOP853 is close to each other. NN-Exp-S0 exhibits a better performance
than NN-Exp-S1, DOP853, and the other scipy methods for this problem.


