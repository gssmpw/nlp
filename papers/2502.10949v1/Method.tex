\section{Learning the  Exact Time Integration Algorithm}
\label{sec_method}


\subsection{Exact Time Marching Scheme}

% what is exact time-stepping scheme?
% existence of exact time-stepping scheme?

Let $n$ be a positive integer, and we
consider the initial value problem on $t\in(a,b)$,
% system of differential equations,
\begin{subequations}\label{eq_1}
\begin{align}
  & \frac{dy}{dt} = f(y,t),  \label{eq_1a} \\
  & y(t_0) = y_0,  \label{eq_1b}
\end{align}
\end{subequations}
where $t$ denotes the time, $t_0\in(a,b)$ is
the initial time, 
$y: (a,b)\subset \mbb R\rightarrow \mbb R^n$
denotes the solution, $f: \mbb R^n\times\mbb R \rightarrow \mbb R^n$
is a prescribed function, and $y_0\in \mbb R^n$ denotes the initial data.

To numerically solve this system, a time integration algorithm
produces a sequence $\{ y_k\in\mbb R^n\ |\ \mathrm{integer}\ k\geqslant 0 \}$
as approximations to the solution $y(t)$  at
discrete instants, $\{y(t_k)\ |\ t_k=t_0+kh,\ k\geqslant 0 \}$,
where $h$ denotes the step size. Interestingly, there exist discrete schemes
that can produce  exact results to the system~\eqref{eq_1}.
The forms of such schemes for several problems
%cases of the system~\eqref{eq_1}
have been explicitly constructed  in~\cite{Mickens2021}.
Following~\cite{Mickens2021}, we define
an exact time integration (or time marching) algorithm as follows,

\begin{definition}
  A time marching algorithm is said to be exact if it produces results
  identical to the exact solution for arbitrary values of $h\in(0,h_{\max}]$
    ($h_{\max}>0$ denoting some constant),
    i.e.~$y_k=y(t_k)$ for $k\geqslant 0$.
\end{definition}

% the existence of exact time stepping scheme

Suppose $f(y,t)$ is continuous and satisfies the Lipschitz condition
$\|f(y,t)-f(z,t) \|\leqslant \lambda \|y-z \|$ for some constant $\lambda$.
Then the system~\eqref{eq_1} has a unique solution within a neighborhood
of $t_0$~\cite{HairerNW1993}. We re-write this solution as,
\begin{equation}\label{eq_2}
  y(t) = \psi(y_0,t_0,t), \quad t\in[t_0-\delta, t_0+\delta]
\end{equation}
for some $\delta>0$, where
$  %\begin{equation}
  \psi(y_0,t_0,t_0) = y_0
$  %\end{equation}
and the dependence of the solution on $y_0$ and $t_0$ has been made explicit.
The existence of the exact time marching scheme for~\eqref{eq_1}
is established in~\cite{StuartH1996,Mickens2021}, given by the following result
(see~pages 61-62 of~\cite{Mickens2021}).
\begin{theorem}(\cite{Mickens2021})\label{thm_1}
  The system~\eqref{eq_1} has an exact time integration scheme given by
  \begin{equation}\label{eq_4}
    y_{k+1} = \psi(y_k,t_k,t_k+h), \quad k\geqslant 0,
  \end{equation}
  where $\psi$ is given in~\eqref{eq_2} and $h$ is the step size.
\end{theorem}

This result indicates that by acquiring the function
$\psi(y_0,t_0,t)$, for $y_0\in\Omega\subset\mbb R^n$,
$t_0\in(a,b)\subset\mbb R$ and $t\in[t_0-\delta,t_0+\delta]$,
one can attain the exact time integration algorithm~\eqref{eq_4}
for solving~\eqref{eq_1}.
In the dynamical systems parlance, this function is often referred to
as the evolution map (or flow map, evolution semigroup)~\cite{StuartH1996}.
This is a high-dimensional vector-valued function, and
%Unfortunately, this function
is unknown for an arbitrary given
$f(y,t)$. Attaining $\psi$ is in general even more challenging than
solving the original system~\eqref{eq_1}.
In the current work we use randomized NNs
to learn the function $\psi(y_0,t_0,t)$, and the trained network
provides an approximation to the exact time integration algorithm.
%for solving system~\eqref{eq_1}.
The trained NN can be used as
a time marching algorithm for solving~\eqref{eq_1},
with arbitrary initial data $(y_0,t_0)\in\Omega\times(a,b)$ and arbitrary
step size $h\in(0,h_{\max}]$.
It is crucial to note that it is only necessary to learn
$\psi(y_0,t_0,t)$ for $t$ within a neighborhood of $t_0$.

Specifically, we will learn the function $\psi(y_0,t_0,t)$
by ELM-type randomized neural networks~\cite{DongL2021,DongY2022rm,NiD2023,WangD2024},
leveraging ELM's universal approximation capability~\cite{IgelnikP1995,HuangCS2006},
high accuracy, faster training,
and effectiveness for function approximation in high
dimensions~\cite{DongL2021,WangD2024}.
It is necessary to distinguish non-autonomous and autonomous systems
when learning the exact time integration algorithm.
We will first discuss %the ELM method for
general non-autonomous systems, and then restrict our attention
to autonomous systems.


\subsection{Learning Exact Time Marching Algorithm for Non-Autonomous Systems}

% what is autonomous system?
% what is ELM?
% how to learn psi(y0,t0,t)?
%  (i) formulations of different orders
%  (ii) meta-elements
%  (iii) random collocation points of power distributions
%  (iv) physics-informed nonlinear least squares training
%  (v) adaptive time-stepping
%
% how to use psi for time stepping?


Let us consider the general non-autonomous system~\eqref{eq_1} and
discuss how to learn the function $\psi$  by ELM.
Since it is only necessary to learn $\psi$ for $t$ within some neighborhood of $t_0$,
we define
\begin{equation}\label{eq_6}
  \xi = t-t_0, \quad y(t) = y(t_0+\xi) = Y(\xi),
\end{equation}
where $y(t)$ is the solution to~\eqref{eq_1}.
The system~\eqref{eq_1} can be reformulated  in terms of $Y(\xi)$ into,
\begin{subequations}\label{eq_7}
  \begin{align}
    & \frac{dY}{d\xi} = f(Y, t_0+\xi), \\
    & Y(0) = y_0.
  \end{align}
\end{subequations}
%
Since the solution to  system~\eqref{eq_7} depends on $y_0$, $t_0$ and $\xi$,
we re-write it as
\begin{equation}
  Y(\xi) = \psi(y_0,t_0,\xi),
\end{equation}
where  $\psi: \mbb R^n\times\mbb R\times \mbb R \rightarrow \mbb R^n$
is the function we are pursuing here
and provides the exact time marching algorithm.
Henceforth we will refer to $\psi(y_0,t_0,\xi)$ as the algorithmic
function for system~\eqref{eq_1}.


In light of~\eqref{eq_7}, the algorithmic function $\psi(y_0,t_0,\xi)$ is determined by the following
partial differential equations,
\begin{subequations}\label{eq_9}
  \begin{align}
    & \frac{\partial\psi}{\partial\xi} = f(\psi(y_0,t_0,\xi), t_0+\xi), \label{eq_9a} \\
    & \psi(y_0,t_0,0) = y_0. \label{eq_9b}
  \end{align}
\end{subequations}
Our objective  is to determine the function $\psi(y_0,t_0,\xi)$, for
$y_0\in\Omega\subset\mbb R^n$, $t_0\in[T_0,T_f]\subset\mbb R$,
and $\xi\in[0,h_{\max}]\subset\mbb R$
with prescribed $\Omega$, $T_0$, $T_f$ and $h_{\max}$, such that
the system~\eqref{eq_9} is satisfied.
$\psi(y_0,t_0,\xi)$ is an $n$-vector valued function of $(n+2)$ variables,
where $n$ denotes the dimension of system~\eqref{eq_1}.

\subsubsection{Property of Algorithmic Function $\psi(y_0,t_0,\xi)$ When $f(y,t)$
  Exhibits Some Periodicity}
\label{sec_a221}

We next discuss a useful property of the algorithmic
function $\psi(y_0,t_0,\xi)$
when the RHS of~\eqref{eq_1a}, $f(y,t)$,
exhibits a periodicity with respect to $t$ or  to
some components of $y$.
This property can be used to simplify the NN training
for learning  $\psi(y_0,t_0,\xi)$ when applicable.
In this subsection we assume that $(y,t)\in\mbb R^n\times\mbb R$ in
problem~\eqref{eq_1}
and $\xi\in[0,h_{\max}]$ for a prescribed constant $h_{\max}>0$.
We further make the following assumption:
\begin{assumption}\label{ass_1}
  The function $f(y,t)$, with $(y,t)\in\mbb R^n\times\mbb R$,
  is
  such that the problem~\eqref{eq_1}
  has a unique solution for all $t\in\mbb R$, with
  arbitrary $(y_0,t_0)\in\mbb R^n\times\mbb R$.
  
\end{assumption}
\noindent This assumption can be satisfied, e.g.~if $f(y,t)$ is globally
Lipschitz on $\mbb R^n\times\mbb R$ (see Theorem 2.1.3 in~\cite{StuartH1996}).
Under this assumption, it can be noted that problem~\eqref{eq_9} has a unique
solution $\psi(y_0,t_0,\xi)$ for all
$(y_0,t_0,\xi)\in\mbb R^n\times\mbb R\times[0,h_{\max}]$.

We look into the effect on $\psi(y_0,t_0,\xi)$ if $f(y,t)$
exhibits some periodicity, and consider the following two cases.
In the first case, 
$f(y,t)$ is periodic with respect to $t$, i.e.
\begin{equation}\label{eq_a9}
  f(y,t+T) = f(y,t), \quad \text{for all}\ (y,t)\in\mbb R^n\times\mbb R,
\end{equation}
where $T>0$ is the fundamental period.
In the second case, $f(y,t)=f(y_1,y_2,\dots,y_n,t)$
is periodic with respect to $y_i$ with the fundamental period $L_i>0$,
for some $1\leqslant i\leqslant n$, i.e.
\begin{equation}\label{eq_a10}
  f(y+L_i\mbs e_i,t) = f(y,t), \quad \text{for all}\ (y,t)\in\mbb R^n\times\mbb R,
\end{equation}
where $\mbs e_k=(0,\dots,0,1,0,\dots,0)\in\mbb R^n$ ($1\leqslant k\leqslant n$)
denote the standard basis vectors of $\mbb R^n$.

If $f(y,t)$ is periodic in $t$, the solution $y(t)$ to the initial value
problem~\eqref{eq_1} may or may not be a
periodic function. It depends on $y(t)$ for
$t\in[t_0,t_0+T]$, where $T$ is the period of $f(y,t)$ and
$t_0$ is the initial time.
The following lemma summarizes the result.
\begin{lemma}\label{lem_1}
  Suppose $f(y,t)$ is periodic in $t$ with the period $T$
  and $y(t)$ denotes the solution
  to problem~\eqref{eq_1}.
  If $y(t_0+T)=y(t_0)$, then $y(t)$ is periodic with a period $T$
  under the Assumption~\ref{ass_1},
  i.e.~$y(t+T)=y(t)$ for all $t\in\mbb R$.
\end{lemma}
\noindent A proof of this result is provided in the Appendix.
For a general periodic $f(y,t)$ with the period $T$, when
restricted to $t\in[t_0,t_0+T]$, the solution $y(t)$ to~\eqref{eq_1}
in general does not satisfy $y(t_0+T)=y(t_0)$, unless
$\int_{t_0}^{t_0+T}f(y(t),t)dt=0$.
This lemma therefore indicates that the solution to problem~\eqref{eq_1}
in general is not a periodic function, even though $f(y,t)$ is periodic in $t$.

The algorithmic function $\psi(y_0,t_0,\xi)$, on the other hand,
would be periodic with respect to $t_0$, if $f(y,t)$ is periodic in $t$.
This result is summarized in the following theorem.
\begin{theorem}\label{thm_a1}
  Suppose $f(y,t)$ is periodic in $t$ with the period $T$.
  Then $\psi(y_0,t_0,\xi)$ from the system~\eqref{eq_9} is
  periodic in $t_0$ with a period $T$ under the Assumption~\ref{ass_1},
  i.e.~$\psi(y_0,t_0+T,\xi)=\psi(y_0,t_0,\xi)$
  for all $(y_0,t_0,\xi)\in\mbb R^n\times\mbb R\times[0,h_{\max}]$.
\end{theorem}
\noindent A proof of this theorem is provided in the Appendix.
This result suggests that, if $f(y,t)$ is periodic in $t$,
we only need to learn the algorithmic function $\psi(y_0,t_0,\xi)$
over one period along the $t_0$ direction for non-autonomous systems.
The learned
algorithm can then be used to compute the solution $y(t)$
to problem~\eqref{eq_1} for all $t\in\mbb R$ (i.e.~arbitrarily long time horizons),
which, as noted above, is not periodic in general.

If $f(y,t)$ is periodic with respect to a component
of $y$, the algorithmic function $\psi(y_0,t_0,\xi)$
is not periodic with respect to the corresponding component of $y_0$.
Instead, along the direction of that component, $\psi$
satisfies a well-defined relation as given below.
\begin{theorem}\label{thm_a2}
  Suppose $f(y,t)$ is periodic in $y_i$ with the period $L_i$ (see~\eqref{eq_a10}),
  for some $1\leqslant i\leqslant n$.
  Then $\psi(y_0,t_0,\xi)$ from system~\eqref{eq_9} satisfies
  the following relation under the Assumption~\ref{ass_1},
  \begin{equation}\label{eq_a11}
    \psi(y_0+L_i\mbs e_i,t_0,\xi) = \psi(y_0,t_0,\xi) + L_i\mbs e_i,
    \quad \text{for all}\ (y_0,t_0,\xi)\in\mbb R^n\times \mbb R\times[0,h_{\max}].
  \end{equation}
\end{theorem}
\noindent A proof of this theorem is provided in the Appendix.
This result suggests that, if $f(y,t)$ is periodic with respect to some components
of $y$, we only need to learn $\psi(y_0,t_0,\xi)$ over one period
for the values of those  corresponding components of $y_0$.
The learned algorithm can then be used to compute the solution $y(t)$
to problem~\eqref{eq_1}, in which those components of $y(t)$
corresponding to the periodic directions of $f(y,t)$ can each take
arbitrary values on $\mbb R$.

When discussing the time integration using the learned
algorithmic function $\psi(y_0,t_0,\xi)$ later in Section~\ref{sec_224},
we will consider
how to exploit the above properties to simplify the computations.




\subsubsection{Representation of Algorithmic Function $\psi(y_0,t_0,\xi)$ by Randomized NNs}
\label{sec_221}


\begin{figure}
  \centerline{
    \includegraphics[width=2.8in]{Figures/nn_non_autonomous_B.pdf}(a)
    \includegraphics[width=3in]{Figures/nn_non_autonomous.pdf}(b)
  }
  \caption{Non-autonomous system: (a) base NN architecture; (b)
    NN structure adopted in this paper.
    $F(y_0,t_0,\xi)$ is an approximation of $\psi(y_0,t_0,\xi)$ of $s$-th order.
    Three hidden layers are shown as an example.
  }
  \label{fg_1}
\end{figure}


% represent psi(y0,xi) by ELM/NN
% present with single ELM, then discuss meta-elements in a remark

We will learn $\psi(y_0,t_0,\xi)$ by solving the system~\eqref{eq_9}
using ELM, for $(y_0,t_0,\xi)$ over some prescribed domain
$\Omega\times[T_0,T_f]\times[0,h_{\max}]$,  with  a physics informed approach.
For this purpose, we represent $\psi(y_0,t_0,\xi)$ by an ELM-type randomized
feed-forward NN; see Figure~\ref{fg_1} for
the NN architecture for representing $\psi(y_0,t_0,\xi)$.
%
% need a sketch of the NN here
% describe the NN structure
Figure~\ref{fg_1}(a) illustrates the base ELM  architecture.
Let $(L+1)$ denote the total number of layers in the network,
with $L\geqslant 2$ being an integer. We refer to the vector
\begin{equation}\label{eq_10}
  \mbs m = [m_0, m_1, \dots, m_L]
\end{equation}
as the architectural vector for this NN,
where $m_i$ ($0\leqslant i\leqslant L$) are positive integers
corresponding to the number of nodes in layer $i$.
Layer zero is the input layer, representing the input variables
$(y_0,t_0,\xi)$, with $m_0=n+2$. Layer
$L$ is the output layer, representing the function $\psi(y_0,t_0,\xi)$,
with $m_L=n$. The layers in between
are the hidden layers. From layer to layer the network logic
represents an affine transform, followed by a function composition
with an activation function
$\sigma: \mbb R\rightarrow \mbb R$~\cite{GoodfellowBC2016}.
The coefficients involved in the affine transforms (weights, biases) of different
layers are collectively referred to
as the network parameters, which may be adjustable (i.e.~trainable)
or fixed (non-trainable).

ELM distinguishes itself from
conventional feed-forward neural networks by the following additional
requirements~\cite{DongL2021,HuangZS2006}:
\begin{itemize}
\item The network parameters in all the hidden layers are pre-assigned to
  random values and fixed (non-trainable). Specifically, we
  set the hidden-layer parameters to uniform random values
  generated on the interval $[-R_m,R_m]$, where $R_m$ is a prescribed constant.
  Once assigned, these network parameters are fixed
  throughout the computation.
\item The network parameters in the output layer are trainable. They are
  the ELM training parameters.
\item The output layer should contain no activation (i.e.~$\sigma(x)=x$),
  with zero bias.
\end{itemize}
These ELM requirements  are imposed on the NNs  when representing
$\psi(y_0,t_0,\xi)$ in this paper.


% modified formulations for psi(y0,xi)

In practice we find it beneficial to incorporate an approximation
of $\psi$ explicitly into the NN structure
when representing $\psi(y_0,t_0,\xi)$.
%which can markedly improve the accuracy.
This is illustrated in Figure~\ref{fg_1}(b)
and is adopted in the current work.
The network structure in Figure~\ref{fg_1}(b)
implements the following representation
for $\psi(y_0,t_0,\xi)$,
\begin{equation}\label{eq_11}
  \psi(y_0,t_0,\xi) = F(y_0,t_0,\xi) + \xi^{s+1}\varphi(y_0,t_0,\xi),
\end{equation}
where $F(y_0,t_0,\xi)$ is a prescribed function, $s$ is a prescribed constant,
and $\varphi(y_0,t_0,\xi)=(\varphi_1,\dots,\varphi_n)\in\mbb R^n$
denotes a function to be determined and
is represented by an ELM-type randomized NN with its architectural
vector given by~\eqref{eq_10}.

In this paper we choose $F(y_0,t_0,\xi)$ to be an $s$-th order
approximation of $\psi(y_0,t_0,\xi)$ for some integer $s$. With this choice
the second term in~\eqref{eq_11}, $\xi^{s+1}\varphi$, effectively represents
the error of $F(y_0,t_0,\xi)$ in approximating $\psi(y_0,t_0,\xi)$.
Therefore, with the NN structure in Figure~\ref{fg_1}(b),
we are effectively learning the error function $\varphi(y_0,t_0,\xi)$,
and in turn the function $\psi(y_0,t_0,\xi)$, with ELM.
It is noted that by setting $F(y_0,t_0,\xi)=0$ and $s=-1$
the network in Figure~\ref{fg_1}(b) reduces to
the base NN structure of Figure~\ref{fg_1}(a).

% specific forms of F(y0,xi)

We  consider the following representations of
$\psi(y_0,t_0,\xi)$ in this work:
\begin{subequations}\label{eq_12}
  \begin{align}
    & \label{eq_12a}
  \begin{array}{lll}
    s=0: & F(y_0,t_0,\xi) = y_0, & \psi(y_0,t_0,\xi) = y_0 + \xi\varphi(y_0,t_0,\xi);
  \end{array}  \\
  & \label{eq_12b}
  \begin{array}{lll}
    s=1: & F(y_0,t_0,\xi) = y_0+\xi f(y_0,t_0),
    & \psi(y_0,t_0,\xi) = y_0 + \xi f(y_0,t_0) + \xi^2\varphi(y_0,t_0,\xi).
  \end{array}
\end{align}
\end{subequations}
%
For some problems we also consider the following representation,
\begin{align}
  & \label{eq_12c}
  \begin{array}{ll}
    s = 2: & F_1(y_0,t_0,\xi) = f(y_0,t_0), \quad
    F_2(y_0,t_0,\xi) = f(y_0+\frac{\xi}{2} F_1,t_0+\frac{\xi}{2}), \\
    & F(y_0,t_0,\xi) = y_0 + \xi F_2(y_0,t_0,\xi), \quad
     \psi(y_0,t_0,\xi) = y_0 + \xi F_2(y_0,t_0,\xi) + \xi^3\varphi(y_0,t_0,\xi).
  \end{array}
\end{align}
%
Notice that these representations automatically satisfy the condition in~\eqref{eq_9b}.
The $F(y_0,t_0,\xi)$ in~\eqref{eq_12b} corresponds to a
forward Euler approximation of~\eqref{eq_9a}, and that in~\eqref{eq_12c}
a mid-point approximation of~\eqref{eq_9a}.
In these forms, $\varphi(y_0,t_0,\xi)$ corresponds to
the error function of $F(y_0,t_0,\xi)$ and is to be learned by ELM.

\begin{remark}\label{rem_1}
  In our implementation, the forms of $F(y_0,t_0,\xi)$
  and $\psi(y_0,t_0,\xi)$ in~\eqref{eq_12}-\eqref{eq_12c}
  are realized using a
  lambda layer from the Keras library,
  in which $\varphi(y_0,t_0,\xi)$ is implemented by a
  feedforward NN  with randomly-assigned
  and fixed (non-trainable) hidden-layer coefficients.
  We would like to comment that other representations of $\psi(y_0,t_0,\xi)$
  can be formulated, e.g.~by choosing $F(y_0,t_0,\xi)$ to be explicit
  Runge-Kutta approximations of~\eqref{eq_9a} of higher orders.

\end{remark}




\subsubsection{Neural Network Training to Learn Algorithmic Function $\psi(y_0,t_0,\xi)$}
\label{sec_222}

% how to train NN/ELM?

We next consider how to train the neural network to learn $\psi(y_0,t_0,\xi)$.
Our training is based on a physics-informed approach, 
employing the nonlinear least squares method~\cite{Bjorck1996}.
%which is the Gauss-Newton method combined with a trust-region strategy.

We employ the NN structure from Figure~\ref{fg_1}(b), corresponding to
the representations given in~\eqref{eq_12}-\eqref{eq_12c} for $\psi(y_0,t_0,\xi)$.
We will refer to the randomized feedforward NN in
the lower portion of Figure~\ref{fg_1}(b),
which represents $\varphi(y_0,t_0,\xi)$, as the $\varphi$-subnet.
The $\varphi$-subnet has an architecture characterized by the vector~\eqref{eq_10}.

Let $\phi(y_0,t_0,\xi)=(\phi_1(y_0,t_0,\xi),\phi_2(y_0,t_0,\xi),\dots,\phi_M(y_0,t_0,\xi))\in\mbb R^M$,
where $M=m_{L-1}$, denote the output fields of the last hidden layer of
the $\varphi$-subnet. Then the network logic of
the output layer of the $\varphi$-subnet leads to
the following relation,
\begin{equation}\label{eq_13}
  \varphi_i(y_0,t_0,\xi) = \sum_{j=1}^M\beta_{ij}\phi_j(y_0,t_0,\xi) = \bm\beta_i\cdot\phi(y_0,t_0,\xi),
  \quad 1\leqslant i\leqslant n,
\end{equation}
where $\varphi = (\varphi_1,\varphi_2,\dots,\varphi_n)$,
$\beta_{ij}$ ($1\leqslant i\leqslant n$, $1\leqslant j\leqslant M$)
are the weights in the $\varphi$-subnet's output layer,
and $\bm\beta_i = (\beta_{i1},\dots,\beta_{iM})\in\mbb R^M$.
Note that $\beta_{ij}$ are the training parameters of the $\varphi$-subnet,
and of the overall network for $\psi$.

The objective here is to train the parameters $\beta_{ij}$ so that $\psi(y_0,t_0,\xi)$
satisfies~\eqref{eq_9a} on $y_0\in\Omega\subset\mbb R^n$, $t_0\in[T_0,T_f]\subset\mbb R$,
and $\xi\in[0,h_{\max}]\subset\mbb R$, for prescribed $\Omega$, $T_0$, $T_f$ and $h_{\max}$.
Note that~\eqref{eq_9b}
is automatically satisfied by the NN formulation for~$\psi(y_0,t_0,\xi)$.


Define the residual function $R=(R_1,\dots,R_n)\in\mbb R^{n}$ of the problem,
\begin{equation}\label{eq_14}
  R(\bm\beta,y_0,t_0,\xi) = \frac{\partial\psi}{\partial \xi} - f(\psi,t_0+\xi),
\end{equation}
where $\psi$ is given by~\eqref{eq_11} and~\eqref{eq_12}-\eqref{eq_12c},
$\bm\beta = (\bm\beta_1,\bm\beta_2,\dots,\bm\beta_n)
= (\beta_{11}\dots,\beta_{1M}, \beta_{21},\dots,\beta_{nM})\in\mbb R^{nM}$,
and the dependence of $R$ on $\bm\beta$ has been made explicit.
We choose a set of $Q$ points,
$(y_0^{(i)},t_0^{(i)},\xi^{(i)})\in\Omega\times[T_0,T_f]\times[0,h_{\max}]$ ($1\leqslant i\leqslant Q$),
from a uniform random distribution and refer to them
as the collocation points hereafter.

Enforcing $R(\bm\beta,y_0,t_0,\xi)$ to be zero on these collocation points
gives rise to the following system,
\begin{equation}\label{eq_15}
  r^{(i)}(\bm\beta) = R(\bm\beta, y_0^{(i)},t_0^{(i)},\xi^{(i)})
  = \left.\frac{\partial\psi}{\partial \xi}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}
  -f(\psi(y_0^{(i)},t_0^{(i)},\xi^{(i)}),t_0^{(i)}+\xi^{(i)}) = 0,
  \quad 1\leqslant i\leqslant Q,
\end{equation}
where $r^{(i)}\in\mbb R^n$.
This is a system of nonlinear algebraic equations about $\bm\beta$,
consisting of $nQ$ equations with $nM$ unknowns.
Note that, for any given $\bm\beta$,
the term $\psi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ can be computed by
an evaluation of the neural network, and
the term $\left.\frac{\partial\psi}{\partial \xi}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$
can be computed by a forward-mode automatic differentiation.


We seek a least squares solution  to the
algebraic system~\eqref{eq_15}, and determine $\bm\beta$
by the nonlinear least squares method (i.e.~Gauss-Newton
method)~\cite{Bjorck1996,Bjorck2015}.
Specifically, we compute $\bm\beta$ using the NLLSQ-perturb
(nonlinear least squares
with perturbations) algorithm developed in~\cite{DongL2021};
see also the Appendix A of~\cite{DongW2023} for a more detailed exposition
of NLLSQ-perturb. NLLSQ-perturb
employs the scipy implementation of the Gauss-Newton
method plus a trust-region strategy (scipy.optimize.least\_squares
routine) and additionally incorporates a perturbation scheme
to keep the method from being trapped to the worst local minima.
Upon attaining the least squares solution $\bm\beta$ by NLLSQ-perturb,
we set the output-layer parameters of the $\varphi$-subnet by this solution
to conclude the NN training.
For the NN training,
the input data to the network  consists of the set of collocation
points $(y_0^{(i)},t_0^{(i)},\xi^{(i)})\in \Omega\times[T_0,T_f]\times[0,h_{\max}]$
($1\leqslant i\leqslant Q$).

To elaborate on the training procedure,
the NLLSQ-perturb algorithm~\cite{DongL2021,DongW2023} requires two routines
for its input, one for computing the residual vector
$r(\bm\beta) = (\dots,r^{(i)}(\bm\beta),\dots)\in\mbb R^{nQ}$ and
the other for computing
the Jacobian matrix
$\frac{\partial r}{\partial\bm\beta}\in\mbb R^{nQ\times nM}$,
for an arbitrary given $\bm\beta$.
Computing the residual $r^{(i)}(\bm\beta)$ by~\eqref{eq_15} is straightforward,
noting that the terms $\psi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ and
$\left.\frac{\partial\psi}{\partial \xi}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$ therein can be
attained by forward evaluations of the NN  or by automatic
differentiations, as discussed above.
Computing the Jacobian matrix is more involved.
We next discuss its computation and 
related implementation issues.

% Jacobian matrix

To facilitate computation of the Jacobian matrix, we note from~\eqref{eq_13}
and~\eqref{eq_11} that,
\begin{equation}\label{eq_16}
  \frac{\partial\varphi_i}{\partial\bm\beta_j} = \delta_{ij}\phi(y_0,t_0,\xi)
  \in\mbb R^{1\times M},
  \quad
  \frac{\partial\psi_i}{\partial\bm\beta_j} =
  \xi^{s+1}\frac{\partial\varphi_i}{\partial\bm\beta_j}
  = \delta_{ij}\xi^{s+1}\phi(y_0,t_0,\xi) \in\mbb R^{1\times M},
  \quad 1\leqslant i,j\leqslant n,
\end{equation}
where $\psi = (\psi_1,\dots,\psi_n)$ and
$\delta_{ij}$ denotes the Kronecker delta.
In light of~\eqref{eq_14},
for $1\leqslant i,j\leqslant n$,
\begin{equation}\label{eq_17}
  \frac{\partial R_i}{\partial\bm\beta_j} =
  \frac{\partial}{\partial\xi}\left(\frac{\partial\psi_i}{\partial\bm\beta_j} \right)
  -\sum_{k=1}^n\frac{\partial f_i}{\partial\psi_k}\frac{\partial\psi_k}{\partial\bm\beta_j}
  = (s+1)\xi^s\phi\delta_{ij} + \xi^{s+1}\frac{\partial\phi}{\partial\xi}\delta_{ij}
   - \xi^{s+1}\frac{\partial f_i}{\partial\psi_j}\phi \ \in\mbb R^{1\times M},
\end{equation}
where $f(\psi,t_0+\xi) = (f_1,\dots,f_n)$ and we have used~\eqref{eq_16}.
So the Jacobian matrix is given by
\begin{equation}\label{eq_18}
  \frac{\partial r}{\partial\bm\beta} = \begin{bmatrix}
    \frac{\partial r^{(1)}}{\partial\bm\beta_1} & \dots & \frac{\partial r^{(1)}}{\partial\bm\beta_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial r^{(Q)}}{\partial\bm\beta_1} & \dots & \frac{\partial r^{(Q)}}{\partial\bm\beta_n}
  \end{bmatrix} \in\mbb R^{nQ\times nM},
  \quad
  \frac{\partial r^{(i)}}{\partial\bm\beta_j} =
  \left.\frac{\partial R}{\partial\bm\beta_j}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}=
  \begin{bmatrix}
    \left.\frac{\partial R_1}{\partial\bm\beta_j}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})} \\
    \vdots \\ \left.\frac{\partial R_n}{\partial\bm\beta_j}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}
  \end{bmatrix} \in\mbb R^{n\times M},
\end{equation}
where $\frac{\partial R_i}{\partial\bm\beta_j}$ is given in~\eqref{eq_17}.

\begin{remark}\label{rem_2}
  % how to compute phi(y_0,xi) and d-phi/d-xi
  The Jacobian matrix involves the terms like $\phi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ and
  $\left.\frac{\partial\phi}{\partial\xi}\right|_{(y_0^{(i)}),t_0^{(i)},\xi^{(i)})}$, which
  represent the output fields of the last hidden layer of
  the $\varphi$-subnet and their derivatives.
  To compute these terms,
  in our implementation we have created a Keras sub-model of the $\varphi$-subnet,
  which takes $(y_0,t_0,\xi)$ as its input
  and $\phi(y_0,t_0,\xi)$ (last hidden layer
  of the $\varphi$-subnet) as its output.
  The terms $\phi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ are then computed by a forward
  evaluation of this Keras sub-model,
  and the terms
  $\left.\frac{\partial\phi}{\partial\xi}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$
  are computed by a forward-mode automatic differentiation with this sub-model.
  
\end{remark}

\begin{remark}\label{rem_23}
  % stiff problems, meta elements
  When the problem~\eqref{eq_1} becomes more complicated (e.g.~being stiff),
  we find it necessary to incorporate a domain decomposition into the above
  algorithm for learning $\psi(y_0,t_0,\xi)$. The discussion below pertains to
  the use of domain decomposition in algorithm learning.
  Suppose the domain $\Omega\times[T_0,T_f]\subset\mbb R^{n+1}$ is
  partitioned into $N_e$ ($N_e\geqslant 1$)
  non-overlapping sub-domains,
  $\Omega\times[T_0,T_f]=\mathcal{D}_1\cup\mathcal{D}_2\cup\dots\cup\mathcal{D}_{N_e}$.
  On sub-domain $\mathcal{D}_i$ ($1\leqslant i\leqslant N_e$) we seek a
  function $\psi_i: \mathcal{D}_i\times[0,h_{\max}]\rightarrow\mbb R^n$ such that
  \begin{subequations}\label{eq_19}
    \begin{align}
      &
      \frac{\partial\psi_i}{\partial\xi} = f(\psi_i(y_0,t_0,\xi), t_0+\xi), \quad
      ((y_0,t_0),\xi)\in\mathcal{D}_i\times[0,h_{\max}];
      \\
      &
        \psi_i(y_0,t_0,0) = y_0, \quad\quad\quad\quad\quad\quad (y_0,t_0)\in\mathcal{D}_i.
  \end{align}
  \end{subequations}
  The algorithmic function
  $\psi(y_0,t_0,\xi)$ for $(y_0,t_0,\xi)\in\Omega\times[T_0,T_f]\times[0,h_{\max}]$
  is then given by
  \begin{equation}
    \psi(y_0,t_0,\xi) = \left\{
    \begin{array}{ll}
      \psi_1(y_0,\xi), & (y_0,t_0)\in\mathcal{D}_1; \\
      \psi_2(y_0,t_0,\xi), & (y_0,t_0)\in\mathcal{D}_2; \\
      \dots \\
      \psi_{N_e}(y_0,t_0,\xi), & (y_0,t_0)\in\mathcal{D}_{N_e}.
    \end{array}
    \right.
  \end{equation}
  %
  % local NNs, how to train local NNs
  We use a local NN with an architecture given by Figure~\ref{fg_1}(b)
  to learn each $\psi_i(y_0,t_0,\xi)$, by solving the system~\eqref{eq_19}
  on $\mathcal D_i\times[0,h_{\max}]$, for $1\leqslant i\leqslant N_e$.
  This local learning problem on $\mathcal{D}_i\times[0,h_{\max}]$ is essentially
  the same as~\eqref{eq_9}.
  Hence the same algorithm as
  presented above can be used to
  learn $\psi_i$
  ($1\leqslant i\leqslant N_e$), noting that here the random collocation points
  shall be taken from $\mathcal{D}_i\times[0,h_{\max}]$. 
  We would like to emphasize that the learning problems
  on different sub-domains are not coupled, and they
  can be computed individually or in parallel.
  In our implementation we have created $N_e$ local ELM-type randomized NNs,
  implemented as Keras sub-models,
  with each for one sub-domain. The local NNs  each
  assumes a structure as given by Figure~\ref{fg_1}(b). They are
  trained individually  in an un-coupled fashion,
  using the procedure discussed in this section.
  The learned time-marching algorithm is represented by
  the collection of these local NNs.
  
\end{remark}


\begin{remark}\label{rem_24}
  % will discuss implicit formulations in this remark
  The expressions~\eqref{eq_11} and~\eqref{eq_12}-\eqref{eq_12c} are explicit
  representations of $\psi(y_0,t_0,\xi)$.
  Given $(y_0,t_0,\xi)$, $\psi(y_0,t_0,\xi)$ can be explicitly computed
  by a forward NN evaluation.
  It is also possible to
  learn $\psi(y_0,t_0,\xi)$ by adopting an
  implicit representation, which would entail solving
  algebraic systems  during training and also during time marching,
  in addition to forward NN evaluations.
  Therefore, implicit representations of $\psi(y_0,t_0,\xi)$
  lead to implicit time integration algorithms.
  %which would be more favorable
  %for stiff problems and numerical stability.
  %
  There are several ways to represent $\psi(y_0,t_0,\xi)$ implicitly.
  One simple idea is to again adopt the
  representation~\eqref{eq_11} and~\eqref{eq_12}-\eqref{eq_12c}
  with the same NN as in Figure~\ref{fg_1}(b),
  but restrict $\xi$ to the domain $\xi\in[-h_{\max},0]$ during NN training.
  This amounts to an algorithm
  that marches backward in time, giving rise to an implicit time
  integration scheme. The procedure
  discussed in this section, with $\xi\in[-h_{\max},0]$,
  can train the NN for learning this implicit scheme.

  We next discuss another implicit representation
  of $\psi(y_0,t_0,\xi)$, by leveraging implicit Runge-Kutta type
  approximations. Let us consider
  \begin{equation}\label{eq_21}
    \psi(y_0,t_0,\xi) = G(y_0,t_0,\xi) + \xi^{s+1}\varphi(y_0,t_0,\xi),
  \end{equation}
  where $G(y_0,t_0,\xi)$ is an $s$-th order implicit 
  approximation of $\psi(y_0,t_0,\xi)$ in equation~\eqref{eq_9a}.
  We consider the following specific forms,
  \begin{subequations}\label{eq_22}
    \begin{align}
      & \label{eq_22b}
      \begin{array}{lll}
        s = 1: & K(y_0,t_0,\xi) = f(y_0+\xi K(y_0,t_0,\xi), t_0+\xi),\\
        & G(y_0,t_0,\xi) = y_0 + \xi K(y_0,t_0,\xi), \quad
         \psi(y_0,t_0,\xi) = y_0+\xi K(y_0,t_0,\xi) + \xi^2\varphi(y_0,t_0,\xi);
      \end{array} \\
      & \label{eq_22c}
      \begin{array}{lll}
        s = 2: & K_1(y_0,t_0,\xi) = f(y_0+\gamma\xi K_1(y_0,t_0,\xi), t_0+\gamma\xi),\\
        & K_2(y_0,t_0,\xi) = f(y_0+(1-\gamma)\xi K_1(y_0,t_0,\xi) + \gamma\xi K_2(y_0,t_0,\xi),t_0+\xi), \\
        & G(y_0,t_0,\xi) = y_0+(1-\gamma)\xi K_1(y_0,t_0,\xi) + \gamma\xi K_2(y_0,t_0,\xi), \\
        & \psi(y_0,t_0,\xi) = y_0+(1-\gamma)\xi K_1(y_0,t_0,\xi) + \gamma\xi K_2(y_0,t_0,\xi)
        + \xi^3\varphi(y_0,t_0,\xi);
      \end{array}
    \end{align} 
  \end{subequations}
  where $\gamma = 1-\frac{\sqrt{2}}{2}$.
  The form~\eqref{eq_22b}
  utilizes the first-order backward Euler approximation
  in the implicit Runge-Kutta form, and~\eqref{eq_22c} uses the 2nd-order
  diagonally implicit Runge-Kutta (DIRK) approximation~\cite{HairerW1996,KenedyC2016}.
  These representations automatically satisfy~\eqref{eq_9b}.
  %
  % how to train NN with implicit representation of psi?
  In~\eqref{eq_21}, $\varphi(y_0,t_0,\xi)$ is to be determined, and  is represented by
  an ELM-type randomized NN.
  This leads to the relation~\eqref{eq_13}, in which $\beta_{ij}$
  are the trainable parameters
  and $\phi(y_0,t_0,\xi)\in\mbb R^M$ are the output fields of
  the last hidden layer of the NN.
  To train this NN,
  we seek  $\beta_{ij}$
  such that the expression~\eqref{eq_21} for $\psi(y_0,t_0,\xi)$
  satisfies~\eqref{eq_9a}, in the least squares sense,
  on  $Q$
  random collocation points $(y_0^{(i)},t_0^{(i)},\xi^{(i)})\in\Omega\times[T_0,T_f]\times[0,h_{\max}]$
  ($1\leqslant i\leqslant Q$). We can similarly compute $\beta_{ij}$
  by the NLLSQ-perturb algorithm. In this case the residual function
  is defined by~\eqref{eq_14} and the residuals on
  the collocation points are given by~\eqref{eq_15}, where $\psi(y_0,t_0,\xi)$
  is given by~\eqref{eq_21}.

  Let us use the form~\eqref{eq_22b}
  to illustrate how to compute the residual
  vector $r(\bm\beta)\in\mbb R^{nQ}$ and the Jacobian matrix
  $\frac{\partial r}{\partial\bm\beta}\in\mbb R^{nQ\times nM}$
  for use by NLLSQ-perturb during training.
  The representation~\eqref{eq_22c} requires a procedure similar
  to what follows.
  With~\eqref{eq_22b}
  the residual~\eqref{eq_15} is reduced to
  \begin{equation}\label{eq_23}
    \begin{split}
    r^{(i)}(\bm\beta) =& \ K(y_0^{(i)},t_0^{(i)},\xi^{(i)})
    + \xi^{(i)}\left.\frac{\partial K}{\partial\xi} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}
    + 2\xi^{(i)}\varphi(y_0^{(i)},t_0^{(i)},\xi^{(i)})
    + (\xi^{(i)})^2\left.\frac{\partial\varphi}{\partial\xi} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}\\
    &- f(\psi(y_0^{(i)},t_0^{(i)},\xi^{(i)}),t_0^{(i)}+\xi^{(i)}),
    \quad 1\leqslant i\leqslant Q.
    \end{split}
  \end{equation}
  Here $K(y_0^{(i)},t_0^{(i)},\xi^{(i)})\in\mbb R^n$ is computed by solving
  the nonlinear equation (see the first equation of~\eqref{eq_22b})
  \begin{equation}\label{eq_24}
    K(y_0^{(i)},t_0^{(i)},\xi^{(i)}) = f(y_0^{(i)} + \xi^{(i)}K,t_0^{(i)}+\xi^{(i)}).
  \end{equation}
  In our implementation we solve this equation using the
  routine ``root'' in the scipy library (scipy.optimize.root).
  $\left.\frac{\partial K}{\partial\xi} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}\in\mbb R^n$
  in~\eqref{eq_23} is computed by solving the linear algebraic system
  \begin{equation}\label{eq_25}
    \left(\mbs I - \xi^{(i)}\left.\frac{\partial f}{\partial G} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})} \right)
    \left.\frac{\partial K}{\partial\xi} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}
    = \left.\frac{\partial f}{\partial G} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}
    K(y_0^{(i)},t_0^{(i)},\xi^{(i)})
    + \left.\frac{\partial f}{\partial t}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}, 
  \end{equation}
  where $\mbs I\in \mbb R^{n\times n}$ is the identity matrix,
  $G = y_0 + \xi K(y_0,t_0,\xi)$, $t = t_0+\xi$, and
  $f = f(G,t)=f(y_0+\xi K, t_0+\xi)$.
  This linear system results from the differentiation of the
  first equation in~\eqref{eq_22b} with respect to $\xi$. We solve this system
  using the routine ``solve'' from the scipy library
  (scipy.linalg.solve).
  The terms $\varphi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ and 
  $\left.\frac{\partial\varphi}{\partial\xi} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$
  in~\eqref{eq_23} are computed by a forward evaluation of the NN
  and by a forward-mode automatic differentiation.
  %
  % Jacobian matrix
  The Jacobian matrix $\frac{\partial r}{\partial\bm\beta}$ is given by~\eqref{eq_18},
  in which
  \begin{equation}\label{eq_26}
    \frac{\partial R_i}{\partial\bm\beta_j} =
    2\xi\phi(y_0,t_0,\xi)\delta_{ij} + \xi^2\frac{\partial\phi}{\partial\xi}\delta_{ij}
    -\xi^2\frac{\partial f_i}{\partial\psi_j}\phi(y_0,t_0,\xi),
  \end{equation}
  to be evaluated on the collocation points.
  In summary, given $(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ ($1\leqslant i\leqslant Q$)
  and an arbitrary $\bm\beta$, we take the following steps to compute
  $r(\bm\beta)$ and $\frac{\partial r}{\partial\bm\beta}$:
  \begin{enumerate}[(i), nosep]
  \item Compute $\varphi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$,
    $\left.\frac{\partial\varphi}{\partial\xi}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$,
    $\phi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ and
    $\left.\frac{\partial\phi}{\partial\xi}\right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$
    by forward valuations of the neural network and by automatic
    differentiations;

  \item Solve equation~\eqref{eq_24} for $K(y_0^{(i)},t_0^{(i)},\xi^{(i)})$;
  \item Solve the linear system~\eqref{eq_25} for
    $\left.\frac{\partial K}{\partial\xi} \right|_{(y_0^{(i)},t_0^{(i)},\xi^{(i)})}$;
  \item Compute $\psi(y_0^{(i)},t_0^{(i)},\xi^{(i)})$ by the second equation in~\eqref{eq_22b};
  \item Compute $r(\bm\beta)$ by~\eqref{eq_23};
  \item Compute $\frac{\partial r}{\partial\bm\beta}$ by~\eqref{eq_18}
    and~\eqref{eq_26}.
    
  \end{enumerate}

\end{remark}

\begin{remark}\label{rem_c26}
  Hereafter, we refer to the learned NN algorithms employing the explicit
  formulations~\eqref{eq_12a},
  \eqref{eq_12b}, and \eqref{eq_12c} as ``NN-Exp-S0'',
  ``NN-Exp-S1'', and ``NN-Exp-S2'', respectively.
  We refer to those based on the implicit formulations~\eqref{eq_22b}
  and~\eqref{eq_22c} as ``NN-Imp-S1'' and ``NN-Imp-S2'', respectively.
\end{remark}


% remark on stiff problems: how to deal with them?
% remark on domain decomposition and meta-elements --> crucial for stiff problems
%    and for more complex problems.



\subsubsection{Time Marching Based on Learned Algorithmic Function $\psi(y_0,t_0,\xi)$}
\label{sec_224}

% how to time-march with NN/ELM?
% no need to re-train NN for different y0 and step size h.
% highly accurate, compared with F(y0,h), under same h etc
% psi(y0,xi) is not learned perfectly. It is not the exact algorithm itself.
%   it is an approximation of the exact algorithm.
% can handle stiff problems.
%
% how to time-march with NN/ELM with domain decompositions?
%   meta-elements
% how to time-march with implicit psi representation?
%
% what is the computational cost of the learned time integration algorithm?
%
% how to do adaptive time-stepping with NN/ELM?


With appropriately chosen domain $\Omega\times[T_0,T_f]\times[0,h_{\max}]$
for network training,
the trained ELM network can be used as a time marching algorithm
for solving problem~\eqref{eq_1}, with arbitrary initial data
$(y_0,t_0)\in\Omega\times[T_0,T_f]$ and step size $h\in(0,h_{\max})$.

Let $\psi_{\bm\beta}(y_0,t_0,\xi)$ denote the learned algorithmic function
represented by the trained neural network.
Suppose the initial time and data
are $(t_0,y_0)$, $h$ is the step size, and $y_k$ is the approximation to
the solution at $t_k=t_0+kh$ ($k\geqslant 0$).
Given $(y_k,t_k)$, we compute the  solution at $t_{k+1}=t_k+h$
by
\begin{equation}\label{eq_27}
  y_{k+1} = \psi_{\bm\beta}(y_k,t_k,h), \quad k\geqslant 0.
\end{equation}
It can be noted that only forward NN evaluations 
are needed for solving IVPs if the learned algorithm is based on
an explicit representation of $\psi(y_0,t_0,\xi)$.
On the other hand, if the algorithm
is based on an implicit representation (see Remark~\ref{rem_24}),
solving an algebraic system
is required during time marching, apart from the forward NN evaluations.
We will elaborate on the implicit case in a remark (Remark~\ref{rem_27})
below.

% comment on proper choice of domain Omega and h_{\max}
%   for training NN
% comment on difference between psi_{\beta} and psi and
%   accuracy


  Since one is not able to  learn the algorithmic function $\psi(y_0,t_0,\xi)$ perfectly,
  due to practical constraints (such as space, time, and computational resource),
  it should be emphasized that the learned algorithm $\psi_{\bm\beta}(y_0,t_0,\xi)$
  is but an approximation of
  the exact time integration algorithm $\psi(y_0,t_0,\xi)$.
  Nonetheless, we observe that the learned algorithms are
  %highly accurate and computationally competitive. They are
  highly competitive, in terms of their accuracy and time marching
  cost, compared with the leading traditional time integration algorithms.
  This point will be demonstrated
  with numerical experiments in Section~\ref{sec_tests}.
  


\begin{remark}\label{rem_25}
  Choosing an appropriate domain $\Omega\times[T_0,T_f]\times[0,h_{\max}]$, from
  which the collocation points are drawn, for training
  the NN  is important to the accuracy of the learned
  algorithm $\psi_{\bm\beta}(y_0,t_0,\xi)$.
  In general the domain $\Omega$ and $[T_0,T_f]$ should be
  sufficiently large so that $y_k\in\Omega$ and $t_k\in[T_0,T_f]$
  for all $0\leqslant k\leqslant N$ ($N$ denoting the number of time steps one plans to perform),
  and the parameter $h_{\max}$ should be sufficiently large so that
  the step size $h$ of interest falls in $(0,h_{\max})$.
  On the other hand, an overly large $\Omega$, $(T_f-T_0)$, or $h_{\max}$
  can increase the difficulty in the network training.
  In this regard, some knowledge about the system to be simulated can be helpful
  to the choice of $\Omega$, $h_{\max}$, and $[T_0,T_f]$.
  In the absence of any knowledge about the system, preliminary simulations
  are useful for making a choice about $\Omega$, $h_{\max}$ and $[T_0,T_f]$.
  
\end{remark}

\begin{remark}\label{rem_027}
  % non-autonomous systems and block time marching
  Suppose one intends to solve the non-autonomous
  system~\eqref{eq_1} for $0\leqslant t\leqslant t_f$,
  with $t_0=0$ in~\eqref{eq_1b} and $t_f$ being large (long time integration).
  When training the NN, the domain $[T_0,T_f]$
  should in general be chosen to be at least $[0,t_f]$.
  Since $t_f$ is large, using a single NN  to accurately
  learn $\psi(y_0,t_0,\xi)$ 
  on $(y_0,t_0,\xi)\in\Omega\times[0,t_f]\times[0,h_{\max}]$
  can become very challenging.
  To alleviate this problem, one can incorporate a domain decomposition
  of $[0,t_f]$ and employ local NNs
  to learn $\psi$ on the sub-domains (see Remark~\ref{rem_23}).
  Let us suppose $[0,t_f]$ is partitioned into $m$ ($m\geqslant 1$) sub-domains with
  $0=T_0<T_1<\cdots<T_m=t_f$.
  One only
  needs to train $m$ local NNs, each for a sub-domain
  $t_0\in[T_{i-1},T_i]$ ($1\leqslant i\leqslant m$),
  individually in an un-coupled fashion.
  By using a moderate size for the sub-domains,
  training the local NNs would become significantly
  easier. By incorporating domain decomposition and local neural
  networks, one can effectively learn the time marching algorithm
  for solving non-autonomous systems over long time horizons.

\end{remark}


\begin{remark}\label{rem_26}
  % time marching with domain decomposition
  In the presence of domain decomposition (see Remark~\ref{rem_23}),
  the algorithm~\eqref{eq_27} needs to be modified accordingly for time
  integration. Let $\psi_{\bm\beta i}(y_0,t_0,\xi)$ 
  denote the learned $\psi_i(y_0,t_0,\xi)$ on
  the sub-domain $\mathcal{D}_i\times [0,h_{\max}]$
  for $1\leqslant i\leqslant N_e$. Then given
  $(y_k,t_k)$ we approximate the solution at 
  $t_{k+1}=t_k+h$ by
  \begin{equation}\label{eq_28}
    \begin{array}{l}
      (i)\ \text{determine}\ s\ (1\leqslant s\leqslant N_e)\
      \text{such that}\ (y_k,t_k)\in\mathcal{D}_s; \\
      (ii)\ y_{k+1} = \psi_{\bm\beta s}(y_k,t_k,h).
    \end{array}
  \end{equation}
  %
  In the event $(y_k,t_k)$ falls on the shared
  boundary of two or more sub-domains, one can choose
  the $\psi_{\bm\beta}$ corresponding to any of these sub-domains
  for time integration. In our implementation, we have
  used the sub-domain with the lowest ID 
  for time integration.
  %
  To reduce the influence of different choices of sub-domains
  in such cases, during NN training it is preferable, after
  the domain $\Omega\times[T_0,T_f]$ is partitioned into disjoint
  sub-domains $\mathcal{D}_i$,
  to enlarge each sub-domain slightly (e.g.~by a few percent)
  along the directions of domain decomposition.
  In this way, $\psi_i(y_0,t_0,\xi)$ will be learned/trained on
  a domain slightly larger than $\mathcal{D}_i$.
  To be more specific, suppose the sub-domain $\mathcal{D}_i$ has a dimension $[a,b]$
  along a direction with domain decomposition,
  and let $r\geqslant 0$ denote the enlargement
  factor. Then along this direction the enlarged sub-domain for NN training
  will have a dimension $\left[a-(b-a)\frac{r}{2}, b+(b-a)\frac{r}{2}\right]$.
  Note that the use of enlarged sub-domains is 
  for network training only. During time marching,
  the algorithm~\eqref{eq_28} will still choose the algorithmic
  function based on the disjoint partitions $\mathcal{D}_i$.
  

\end{remark}


\begin{remark}\label{rem_29}
  % periodic forcing
  If $f(y,t)$ in~\eqref{eq_1}
  is periodic in $t$ with the period $T$, as shown by Theorem~\ref{thm_a1},
  it would be sufficient to learn $\psi(y_0,t_0,\xi)$
  on $(y_0,t_0,\xi)\in\Omega\times[T_0,T_f]\times[0,h_{\max}]$,
  with $[T_0,T_f]$ covering  one period of $f(y,t)$,
  and the learned algorithm $\psi_{\bm\beta}(y_0,t_0,\xi)$ can be used to solve
  system~\eqref{eq_1} for all $t\in\mbb R$ (arbitrarily long
  time horizons). This property can significantly simplify
  the algorithm learning and the long-time integration of
  non-autonomous systems with a  periodic RHS.
  %
  % time marching for periodic f(y,t)
  In this case,
  time integration using the learned algorithm
  $\psi_{\bm\beta}(y_0,t_0,\xi)$, which is trained on
  $(y_0,t_0,\xi)\in\Omega\times[0,T]\times[0,h_{\max}]$, takes the following
  form. Given ($y_k,t_k$) with $t_k\in\mbb R$, we compute
  ($y_{k+1},t_{k+1}$) by,
  \begin{equation}\label{eq_46}
    y_{k+1} = \psi_{\bm\beta}(y_k,t_k^*,h), \quad
    t_k^* = \mathrm{mod}(t_k,T), \quad
    t_{k+1} = t_k+h,
  \end{equation}
  where $\mathrm{mod}$ denotes the modulo operation.
  
\end{remark}

\begin{remark}\label{rem_210}
  If $f(y,t)$ in~\eqref{eq_1} is periodic with respect to
  one or more components of $y\in\mbb R^n$,
  when learning $\psi(y_0,t_0,\xi)$ on
  $(y_0,t_0,\xi)\in\Omega\times[T_0,T_f]\times[0,h_{\max}]$,
  it would be sufficient to choose $\Omega$ to cover
  one period of $f(y,t)$ along those directions, as shown by Theorem~\ref{thm_a2}.
  The resultant algorithm can be used to solve the system~\eqref{eq_1}
  with those components of $y_0\in\mbb R^n$
  in the periodic directions taking arbitrary values.

  Define a constant vector $\mbs L=(L_1,\dots,L_n)\in\mbb R^n$
  with $L_i\geqslant 0$ ($1\leqslant i\leqslant n$) and assume $\mbs L\neq 0$.
  Suppose
  \begin{equation}\label{eq_48}
    f(y+L_i\mbs e_i, t) = f(y,t), \quad
    \text{for all}\ (y,t)\in\mbb R^n\times\mbb R, \quad
    1\leqslant i\leqslant n.
  \end{equation}
  If $L_i>0$ ($1\leqslant i\leqslant n$),
  equation~\eqref{eq_48} indicates that
  $f(y,t)$ is periodic with respect to $y_i$
  with a period $L_i$.
  If $L_i=0$, equation~\eqref{eq_48} is reduced to an identity and
  we use this to denote that $f(y,t)$ is
  not periodic with respect to $y_i$.
  We will refer to $\mbs L$ as the periodicity vector of $f(y,t)$ hereafter.


  % time marching with psi(y0,t0,xi)
  Given the periodicity vector $\mbs L=(L_1,\dots,L_n)$ for $f(y,t)$,
  we suppose the NN has been trained to learn
  $\psi(y_0,t_0,\xi)$ on $\Omega\times[T_0,T_f]\times[0,h_{\max}]$,
  where $\Omega$ covers one period of $f(y,t)$ along those
  periodic directions.  Solving
  system~\eqref{eq_1} based on the learned
  algorithm $\psi_{\bm\beta}(y_0,t_0,\xi)$ takes the following form.
  Given step size $h$ and ($y_k,t_k$), where $y_k=(y_{k1},\dots,y_{kn})$,
  \begin{enumerate}[(i),nosep] 
  \item for $i=1,\dots,n$:
    \begin{enumerate}[,nosep]
    \item if $L_i>0$, then set $y_{ki}^*=\mathrm{mod}(y_{ki},L_i)$
      and $q_i=\left\lfloor \frac{y_{ki}}{L_i}\right\rfloor$; \\
      else set $y_{ki}^*=y_{ki}$ and $q_i=0$.
    \end{enumerate}
    
  \item
    $y_{k+1} = \psi_{\bm\beta}(y_k^*,t_k,h) + \mathrm{mult}(q,\mbs L)$, where
    $y_k^*=(y_{k1}^*,\dots,y_{kn}^*)$,
    $q=(q_1,\dots,q_n)\in\mbb Z^n$, and $\mathrm{mult}(\cdot,\cdot)$ denotes
    the element-wise multiplication of two vectors and returns the resultant
    vector.

  \end{enumerate}
  In the above steps $\lfloor\cdot\rfloor$ denotes the floor function.
  %
  % what else to discuss here?
  

\end{remark}


\begin{remark}\label{rem_27}
  % time marching with implicit psi representation
  Implicit
  representations of $\psi(y_0,t_0,\xi)$ (see Remark~\ref{rem_24}) lead to
  implicit time integration algorithms
  for solving~\eqref{eq_1}.
  For an implicit NN algorithm $\psi_{\bm\beta}$
  trained on $(y_0,t_0,\xi)\in\Omega\times[T_0,T_f]\times[-h_{\max},0]$,
  given $(y_k,t_k)$
  and step size $h\in(0,h_{\max})$,
  the solution $y_{k+1}$ at $t_{k+1}=t_k+h$ is determined by
  \begin{equation}\label{eq_29}
    y_k = \psi_{\bm\beta}(y_{k+1},t_{k+1},-h).
  \end{equation}
  This is a nonlinear system of algebraic equations involving 
  the NN function $\psi_{\bm\beta}$, which needs to be solved for $y_{k+1}$.
  This system can be solved using the scipy routine ``root''
  (scipy.optimize.root) in the implementation,
  in which the Jacobian matrix can be computed
  by automatic differentiation.
  For NN algorithms based on the implicit
  representations~\eqref{eq_22b} and~\eqref{eq_22c}, time marching takes the following forms:
  given $(y_k,t_k)$ and $h$,
  %\begin{subequations}
    \begin{align}
      & K = f(y_k+hK, t_k+h), \label{eq_30a} \quad
      y_{k+1} = y_0 + hK + h^2\varphi_{\bm\beta}(y_k,t_k,h);
      %\label{eq_30b}
    \end{align}
  %\end{subequations}
  and 
  \begin{subequations}
    \begin{align}
      & K_1 = f(y_k + \gamma h K_1, t_k+\gamma h), \label{eq_31a} \quad
      K_2 = f(y_k + (1-\gamma)hK_1 + \gamma hK_2, t_k+h), %\label{eq_31b}
      \\
      & y_{k+1} = y_k + (1-\gamma)hK_1 + \gamma hK_2
      + h^3\varphi_{\bm\beta}(y_k,t_k,h). \label{eq_31c}
    \end{align}
  \end{subequations}
  Here $\varphi_{\bm\beta}$ denotes the
  function $\varphi(y_0,t_0,\xi)$ in~\eqref{eq_22b} and~\eqref{eq_22c}
  learned by the neural network.
  Nonlinear equations need to be solved for
  computing $K$, $K_1$,
  and $K_2$ during time marching. They can be solved
  based on the scipy routine ``root'' in the implementation.
  
\end{remark}



% what else to discuss here?



\subsection{Learning Exact Time Marching Algorithm for Autonomous Systems}

If system~\eqref{eq_1} is autonomous, the algorithm presented in
the previous subsection will be simplified.
We briefly discuss this case here.
Consider an autonomous system in~\eqref{eq_1}, i.e.
\begin{align}
  & \frac{dy}{dt} = f(y), \label{eq_5}
\end{align}
where $f: \mbb R^n\rightarrow \mbb R^n$ is a prescribed function,
together with the initial condition~\eqref{eq_1b}.


By again introducing the transformation~\eqref{eq_6}, we re-write the
system consisting of~\eqref{eq_5} and~\eqref{eq_1b} into
\begin{subequations}\label{eq_32}
  \begin{align}
    & \frac{dY}{d\xi} = f(Y), \\
    & Y(0) = y_0.
  \end{align}
\end{subequations}
The solution to problem~\eqref{eq_32} depends only on $y_0$ and
$\xi$, and we re-write it as
$  %\begin{equation}
  Y(\xi) = \psi(y_0,\xi)
$  %\end{equation}
to make the dependence on $y_0$ and $\xi$ explicit.
$\psi(y_0,\xi)$ is the algorithmic function for the system consisting of~\eqref{eq_5}
and~\eqref{eq_1b}. This function is determined by,
%The system~\eqref{eq_32} accordingly becomes, in terms of $\psi$,
\begin{subequations}\label{eq_34}
  \begin{align}
    &
    \frac{\partial\psi}{\partial\xi} = f(\psi(y_0,\xi)),
    \label{eq_34a} \\
    & \psi(y_0,0) = y_0. \label{eq_34b}
  \end{align}
\end{subequations}
We learn the function $\psi(y_0,\xi)$, for $(y_0,\xi)\in\Omega\times[0,h_{\max}]$,
by solving the system~\eqref{eq_34} with ELM,
for prescribed $\Omega$ and $h_{\max}$.
The trained NN can be used to solve the problem consisting of~\eqref{eq_5}
and~\eqref{eq_1b}.


% representations of psi(y0,t0,xi)

\begin{figure}
  \centerline{
    \includegraphics[width=3in]{Figures/nn_autonomous_B.pdf}
  }
  \caption{Autonomous system: neural network structure,
    with three hidden layers shown
    as an example.
  }
  \label{fg_2}
\end{figure}

Figure~\ref{fg_2} illustrates the NN structure 
employed to learn $\psi(y_0,\xi)$,
which implements the following representation,
\begin{equation}\label{eq_35}
  \psi(y_0,\xi) = F(y_0,\xi) + \xi^{s+1}\varphi(y_0,\xi),
\end{equation}
where $F(y_0,\xi)\in\mbb R^n$ is a prescribed
$s$-th order approximation of $\psi(y_0,\xi)$ (for some integer $s$),
and $\varphi(y_0,\xi)\in\mbb R^n$
is an unknown function represented by the ELM-type
randomized NN.
The NN input nodes  represent
$(y_0,\xi)\in\mbb R^n\times\mbb R$, and the output
nodes represent $\psi(y_0,\xi)\in\mbb R^n$.
We again refer to the portion of the network
representing $\varphi(y_0,\xi)$ as the
$\varphi$-subset.
The $\varphi$-subset has an architecture characterized by~\eqref{eq_10},
with $m_0=n+1$ and $m_L=n$ and its hidden-layer coefficients
randomly assigned and fixed.

Analogous to~\eqref{eq_12} and~\eqref{eq_12c}, we consider the following
representations for $\psi(y_0,\xi)$:
\begin{subequations}\label{eq_36}
  \begin{align}
    & \label{eq_36a}
  \begin{array}{lll}
    s=0: & F(y_0,\xi) = y_0, & \psi(y_0,\xi) = y_0 + \xi\varphi(y_0,\xi);
  \end{array}  \\
  & \label{eq_36b}
  \begin{array}{lll}
    s=1: & F(y_0,\xi) = y_0+\xi f(y_0),
    & \psi(y_0,\xi) = y_0 + \xi f(y_0) + \xi^2\varphi(y_0,\xi);
  \end{array}
\end{align}
\end{subequations}
%
and for some problems also
\begin{align}
  & \label{eq_36c}
  \begin{array}{ll}
    s = 2: & F_1(y_0,\xi) = f(y_0), \quad F_2(y_0,\xi) = f(y_0+\frac{\xi}{2} F_1), \\
    & F(y_0,\xi) = y_0 + \xi F_2(y_0,\xi), \quad
    \psi(y_0,\xi) = y_0 + \xi F_2(y_0,\xi) + \xi^3\varphi(y_0,\xi).
  \end{array}
\end{align}
%
These forms automatically satisfy~\eqref{eq_34b}.
Therefore, only~\eqref{eq_34a} needs to be considered for NN training.

% training NN

We train the NN such that $\psi(y_0,\xi)$
satisfies~\eqref{eq_34a}
for $y_0\in\Omega$ and $\xi\in[0,h_{\max}]$.
The procedure from Section~\ref{sec_222}  can be
adapted to train the NN here  for autonomous
systems.
The logic of the $\varphi$-subset's output layer gives rise to
a relation analogous to~\eqref{eq_13},
\begin{equation}\label{eq_37}
  \varphi_i(y_0,\xi) = \bm\beta_i\cdot\phi(y_0,\xi), \quad
  1\leqslant i\leqslant n,
\end{equation}
where $\varphi(y_0,\xi)=(\varphi_1,\dots,\varphi_n)\in\mbb R^n$,
$\phi(y_0,\xi)=(\phi_1,\dots,\phi_M)\in\mbb R^M$,
$M=m_{L-1}$ denoting the number of nodes in the last hidden layer
of the $\varphi$-subset, and
$\bm\beta_i = (\beta_{i1},\dots,\beta_{iM})\in\mbb R^M$
for $1\leqslant i\leqslant n$.
Note that $\varphi(y_0,\xi)$ is the output field of
the $\varphi$-subnet and $\phi(y_0,\xi)$ denotes
the output fields of the last hidden layer of the
$\varphi$-subnet. $\beta_{ij}$ ($1\leqslant i\leqslant n$,
$1\leqslant j\leqslant M$) are the training parameters of the network.

% residuals, collocation points, Jacobian matrix

We determine the training parameters
$
\bm\beta = (\bm\beta_1,\dots,\bm\beta_n)
=(\beta_{11},\dots,\beta_{1M},\beta_{21},\dots,\beta_{nM})
\in\mbb R^{nM}
$
by the nonlinear least squares method.
Let $(y_0^{(i)},\xi^{(i)})$ ($1\leqslant i\leqslant Q$)
denote $Q$ random collocation points on
$\Omega\times[0,h_{\max}]$ drawn from
a uniform distribution. 
Enforcing~\eqref{eq_34a} on these collocation points
leads to the following nonlinear algebraic system about $\bm\beta$,
\begin{equation}\label{eq_38}
  r^{(i)}(\bm\beta) =
  \left.\frac{\partial\psi}{\partial\xi}\right|_{(y_0^{(i)},\xi^{(i)})}
  - f(\psi(y_0^{(i)},\xi^{(i)})) = 0,
  \quad 1\leqslant i\leqslant Q,
\end{equation}
in which $\psi$ is given by~\eqref{eq_36}-\eqref{eq_36c} and~\eqref{eq_37}
and the dependence of the residual $r^{(i)}\in\mbb R^n$ on $\bm\beta$
is made explicit.
This is a system of $nQ$ nonlinear algebraic equations
about $nM$ unknowns. We seek a least squares
solution to this system and compute $\bm\beta$
by the NLLSQ-perturb algorithm~\cite{DongL2021,DongW2023}.
%
NLLSQ-perturb requires the computation of the residual vector
and the Jacobian matrix for arbitrary given $\bm\beta$,
as noted previously.
Computing the residual vector
$r(\bm\beta) = (r^{(1)},\dots,r^{(Q)})\in\mbb R^{nQ}$
for NLLSQ-perturb is straightforward in light of~\eqref{eq_38},
noting that $\psi(y_0^{(i)},\xi^{(i)})$ and
$\left.\frac{\partial\psi}{\partial\xi}\right|_{(y_0^{(i)},\xi^{(i)})}$
therein can be obtained by forward NN evaluations
and by automatic differentiation.
%
% Jacobian matrix
The Jacobian matrix is given by
\begin{equation}
  \frac{\partial r}{\partial\bm\beta} = \begin{bmatrix}
    \frac{\partial r^{(1)}}{\partial\bm\beta_1} & \dots & \frac{\partial r^{(1)}}{\partial\bm\beta_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial r^{(Q)}}{\partial\bm\beta_1} & \dots & \frac{\partial r^{(Q)}}{\partial\bm\beta_n}
  \end{bmatrix} \in\mbb R^{nQ\times nM},
  \quad
  \frac{\partial r^{(i)}}{\partial\bm\beta_j} =
  \begin{bmatrix}
    \frac{\partial r_1^{(i)}}{\partial\bm\beta_j} \\
    \vdots \\ \frac{\partial r_n^{(i)}}{\partial\bm\beta_j}
  \end{bmatrix} \in\mbb R^{n\times M},
\end{equation}
where
\begin{equation}
  \begin{split}
  \frac{\partial r^{(i)}_k}{\partial\beta_j}
  =&\ (s+1)(\xi^{(i)})^s\phi(y_0^{(i)},\xi^{(i)})\delta_{kj}
  + (\xi^{(i)})^{s+1}\left.\frac{\partial\phi}{\partial\xi} \right|_{(y_0^{(i)},\xi^{(i)})} \delta_{kj} \\
  & - (\xi^{(i)})^{s+1} \left.\frac{\partial f_k}{\partial\psi_j} \right|_{(y_0^{(i)},\xi^{(i)})} \phi(y_0^{(i)},\xi^{(i)})
  \ \ \in\mbb R^{1\times M},
  \quad 1\leqslant i\leqslant Q, \ 1\leqslant k, j\leqslant n.
  \end{split}
\end{equation}
%
% comment on implementing Jacobian matrix computations
The terms $\phi(y_0^{(i)},\xi^{(i)})$ and
$\left.\frac{\partial\phi}{\partial\xi} \right|_{(y_0^{(i)},\xi^{(i)})}$
involved in the Jacobian matrix can be computed
by forward evaluations of a network sub-model  and
by automatic differentiations (see Remark~\ref{rem_2}).

% time marching using psi(y0,t0,xi)

The trained NN  contains the learned algorithm for
solving the system consisting of~\eqref{eq_5} and~\eqref{eq_1b},
with any initial condition $y_0\in\Omega$, initial time $t_0\in\mbb R$, and
step size $h\in(0,h_{\max})$.
Suppose $(y_k,t_k)$ provide the solution and the time
at step $k$ ($k\geqslant 0$),
and that $\psi_{\bm\beta}(y_0,\xi)$ denotes the learned algorithmic function. 
The solution at the new time step is given by ($h$ denoting the step size),
\begin{equation}\label{eq_41}
  y_{k+1} = \psi_{\bm\beta}(y_k,h),
  \quad t_{k+1} = t_k+h,
\end{equation}
which involves the forward evaluation of the neural network.



\begin{remark}
  % comment on implicit representations of psi(y0,t0,xi)
  The discussion on implicit representations of $\psi$
  for non-autonomous systems in Remarks~\ref{rem_24} and~\ref{rem_27}
  can be adapted to autonomous
  systems for learning and using $\psi(y_0,\xi)$.
  The equations~\eqref{eq_21}
  and~\eqref{eq_22} and other related expressions
  need to be modified accordingly to exclude the $t_0$ effects.
  

\end{remark}

% what else to discuss here?




