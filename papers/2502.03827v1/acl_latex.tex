% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl} 
%\usepackage[review]{acl}




% \usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{comment}
\usepackage{latexsym}
\usepackage{afterpage}
\usepackage{hyperref}
\usepackage{svg}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{array}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{A comprehensive survey of contemporary Arabic sentiment analysis: Methods, Challenges, and Future Directions} 

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{Zhiqiang Shi \\
   University of Edinburgh\\
  % Affiliation / Address line 1 \\
  % Affiliation / Address line 2 \\
  % Affiliation / Address line 3 \\
  \texttt{shizhiqiang126@163.com} \\\And
  Ruchit Agrawal \\
  University of Birmingham % / Address line 1 \\
  % Affiliation / Address line 2 \\
  % Affiliation / Address line 3 \\
  \\ 
  \texttt{r.r.agrawal@bham.ac.uk} \\} 
%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}




\begin{document}
\maketitle
\begin{abstract}
\vspace{-0.1cm}
% Sentiment analysis is usually referred to as using computational methods to extract people's sentiments, opinions and other aspects. Because of the importance of understanding people's sentiments, lots of works have been conducted for sentiment analysis. However, much less works are done for Arabic compared with other high-resource languages, although Arabic is a widely used language. In this work, we give a systematic and up-to-date survey of Arabic sentiment analysis. The main contents of this paper are as follows: (1). We introduce a general classification framework for Arabic sentiment analysis which can cover most of the sentiment analysis tasks. (2). We provide a systemic survey of Arabic sentiment analysis, especially the works that use deep learning methods. (3). We put Arabic sentiment analysis in the context of general sentiment analysis so that researchers can easily identify the gap between them. (4). We outline the challenges and future directions for research. 
% This document is a supplement to the general instructions for *ACL authors. It contains instructions for using the \LaTeX{} style files for ACL conferences.
% The document itself conforms to its own specifications, and is therefore an example of what your manuscript should look like.
% These instructions should be used both for papers submitted for review and for final versions of accepted papers.\\
Sentiment Analysis, a popular subtask of Natural Language Processing, employs computational methods to extract sentiment, opinions, and other subjective aspects from linguistic data. Given its crucial role in understanding human sentiment, research in sentiment analysis has witnessed significant growth in the recent years. However, the majority of approaches are aimed at the English language, and research towards Arabic sentiment analysis remains relatively unexplored. This paper presents a comprehensive and contemporary survey of Arabic Sentiment Analysis, identifies the challenges and limitations of existing literature in this field and presents avenues for future research. %%%We propose a taxonomy encompassing various sentiment analysis tasks, followed by
We present a systematic review of Arabic sentiment analysis methods, focusing specifically on research utilizing deep learning. We then situate Arabic Sentiment Analysis within the broader context, highlighting research gaps in Arabic sentiment analysis as compared to general sentiment analysis. Finally, we outline the main challenges and promising future directions for research in Arabic sentiment analysis. 
\end{abstract}
\begin{comment}
\begin{IEEEkeywords}
Sentiment Analysis, Arabic NLP, Social Media Analysis, Data mining, Deep Learning, Transformers, Arabic social media analysis
\end{IEEEkeywords}
\end{comment} 
\section{Introduction}
\vspace{-0.1cm}
\begin{comment}
Sentiment analysis, a cornerstone of social media computing, has emerged as an indispensable tool within the realm of computational social sciences. By harnessing the capabilities of Natural Language Processing (NLP), researchers are empowered to extract and quantify sentiment expressed within the voluminous tapestry of social media content. This enables a systematic exploration of public opinion, brand perception, and broader societal trends. Through the application of sentiment analysis, computational social scientists can uncover hidden patterns, detect emerging issues, and measure the impact of events or campaigns. This methodology proves pivotal in understanding the dynamics of online discourse, informing public policy, and supporting evidence-based decision-making.

Moreover, by delving into the nuances of sentiment, researchers can develop a more comprehensive understanding of public attitudes and behaviors. Sentiment analysis facilitates the identification of emotional responses to various stimuli, enabling the tracking of sentiment evolution over time and across different demographics. As such, it serves as a catalyst for uncovering the complexities of social interactions and informing the development of computational models that accurately reflect human behavior in the digital age. Through the integration of sentiment analysis with other social media mining techniques, researchers can unlock deeper insights into the underlying structures and patterns within social media data.
\end{comment} 
Sentiment Analysis (SA), also referred to as opinion mining, leverages computational models to extract individuals' sentiments and opinions from data \cite{sentiment-analysis-book}. This field has garnered significant attention from both academic and industrial sectors, as evidenced by the multitude of studies conducted to comprehend human sentiment \cite{sentiment-analysis-book}, \cite{zeng-li-2022-survey}, \cite{https://doi.org/10.1002/asi.23716}. However, although Arabic is a widely popular language spoken by over 372 million people across the globe, the volume of research dedicated to Arabic Sentiment Analysis (ASA) remains considerably lower compared to high-resourced languages such as English and French. %The complexities of the Arabic language and the intricacies of the cultural context pose significant challenges to ASA. 
This study presents a systematic review of existing literature on Arabic sentiment analysis, with a particular focus on approaches that employ deep learning methodologies. 
\par Several prior surveys on Arabic sentiment analysis (ASA) exist \cite{Almurqren2024ArabicTS}, \cite{ALAYYOUB2019320}, \cite{arabic_survey_subjectivity}, \cite{arabic_survey_sc}, \cite{arabic_survey_asa_social_media}, \cite{arabic_survey_asa}, \cite{arabic_survey_sarcasm_detection}. However, the majority of these do not cover contemporary deep learning based methods. Additionally, %none of the existing surveys juxtapose the development of ASA with general methods, or 
these do not present a detailed analysis of the gaps, challenges and future directions for ASA.  %The work closest to ours is the recent study by \cite{asa_survey}, which provides a systematic survey for Arabic sentiment analysis. However, our work differs from this study in that in addition to presenting... 
This paper fills this gap and presents a comprehensive survey of contemporary methods for Arabic sentiment analysis, systematically organizing recent literature in the field and highlighting the key contributions and limitations of current SA methods. We also situate these approaches within the broader framework of general sentiment analysis and approaches for high-resource languages, facilitating an understanding of the developments as well as the gaps in ASA. The major contributions of this study are summarised below:

%\textbf{Comparative Context:} We situate Arabic sentiment analysis within a broader framework by introducing research conducted in the general sentiment analysis domain (i.e., sentiment analysis for any language). This comparative approach facilitates the identification of research gaps specific to Arabic sentiment analysis, particularly when compared to languages with more extensive resources.

%\textbf{Up-to-Date Deep Learning Focus:} We present a comprehensive and current survey of Arabic sentiment analysis, with a particular emphasis on studies that leverage deep learning methodologies. Deep learning has emerged as a dominant force in sentiment analysis, and this survey will provide valuable insights into its application within the Arabic context.
% \item \textbf{Multimodal Sentiment Analysis:} We acknowledge the growing importance of multimodal sentiment analysis, which is particularly relevant for Arabic due to the inherent dialectal variations within the language. This survey will also encompass research efforts that address the challenges posed by these dialectal variations in Arabic sentiment analysis. 

%\textbf{Arabic Specific Considerations and Challenges:} The complexities of Arabic itself and cultural context pose significant challenges to Arabic SA. This survey will also encompass research efforts that address these challenges such as dialectal variations in Arabic SA.  
% \par While a limited number of prior surveys on Arabic Sentiment Analysis (ASA) exist \cite{Almurqren2024ArabicTS}, \cite{ALAYYOUB2019320}, our survey differs from these studies in the following aspects: While previous studies only focus on Arabic, we situate ASA in a wider context, presenting the development of Arabic-centric sentiment analysis juxtaposed with general sentiment analysis \footnote{In this work, we refer to sentiment analysis in high-resourced languages such as English as general sentiment analysis.}, so that researchers can easily identify the gap between sentiment analysis for Arabic and other high-resource languages. Additionally, We provide an up-to-date survey for Arabic sentiment analysis with a focus on  deep learning techniques and also introduce methods for multimodal sentiment analysis. Lastly, we present a novel taxonomy for sentiment analysis tasks. 
%Sentiment analysis, also known as opinion mining, is usually referred to as using computational models to extract people's sentiments, opinions, etc . It has received great interest by both academia and industry, and lots of works have been conducted in order to understanding people's sentiments \cite{sentiment-analysis-book}, \cite{zeng-li-2022-survey}, \cite{https://doi.org/10.1002/asi.23716}. However, much fewer works are done for Arabic sentiment analysis compared to other high-resource languages, although Arabic is a widely used language. In this work, we aim to provide a systematic review of the existing works for Arabic sentiment analysis, especially works that use deep learning methods. 
%There are some existing surveys \cite{Almurqren2024ArabicTS}, \cite{ALAYYOUB2019320} for Arabic sentiment analysis, however, our survey differs from the existing surveys from the following aspects: (1). We put Arabic sentiment analysis in a wider context, more specifically, we introduce the works for both Arabic sentiment analysis and general sentiment analysis \footnote{In this work, we will refer sentiment analysis in any languages as general sentiment analysis.} so that researchers can easily identify the gap between sentiment analysis for Arabic and other high-resource languages. (2). We provide an up-to-date survey for Arabic sentiment analysis and mainly review the works that use deep learning methods. (3). We also introduce the works for multimodal sentiment analysis. Arabic has various dialects, which results in significant challenges for Arabic sentiment analysis. In this work, we will also cover the works that have been done in order to address this issue. 
% \textcolor{red}{Arabic has various versions/dialects: MSA (Modern Standard Arabic) is a popular one. Maybe you can include this as a factor in your taxonomy if you cover different dialects OR you can mention that you focus on MSA if most of the papers you review are for MSA}. 
%\vspace{-0.3cm}
\begin{itemize}
%%%\item We propose a novel taxonomy for sentiment analysis tasks, effectively categorizing existing approaches into well-defined categories, facilitating a structured analysis. 
\vspace{-0.2cm}
\item We present a systematic survey of contemporary research conducted in Arabic sentiment analysis, with a focus on deep learning methodologies. We present an analysis of the key contributions and limitations of state-of-the-art ASA methods across various dimensions such as modality (uni-modal, multi-modal), granularity (coarse-grained, fine-grained) and context (sentence-level, document-level, aspect-level). %%This in-depth survey offers valuable insights into the state-of-the-art methods for this task, and their within the Arabic context.
\vspace{-0.2cm}
\item We situate Arabic sentiment analysis within the broader framework of general sentiment analysis, identifying research gaps in Arabic sentiment analysis, and highlighting areas where advancements are needed to bridge the gap with high-resource languages.
\vspace{-0.2cm}
\item We outline the key challenges in developing robust sentiment analysis models for the Arabic language, and present promising directions to guide future research in this field. %By identifying these challenges and potential avenues for exploration, we can guide future research efforts and contribute to the ongoing development of the field. 
\end{itemize}

\begin{comment}
\begin{figure*}[t]
%\begin{figure} 
\centering
\includegraphics[scale=0.5]{sentiment taxonomy} 
\captionsetup{justification=centering}
  \caption{The proposed taxonomy for classification of existing sentiment analysis tasks using a three-level approach: modality level, granularity level and application level.}
  \label{figure: classification framework}
%\end{figure} 
\end{figure*}
\end{comment}

%%%\section{Related Work}
% Several prior surveys on Arabic sentiment analysis exist \cite{Almurqren2024ArabicTS}, \cite{ALAYYOUB2019320}, \cite{arabic_survey_subjectivity}, \cite{arabic_survey_sc}, \cite{arabic_survey_asa_social_media}, \cite{arabic_survey_asa}, \cite{arabic_survey_sarcasm_detection}, \cite{asa_survey}. \cite{arabic_survey_subjectivity} provides trends and challenges for subjectivity and sentiment analysis of Arabic, which covers methods before 2014 and therefore does not contain any deep learning based methods. \cite{arabic_survey_sc} organises Arabic sentiment classification methods which are published before 2016, they category the existing methods into lexicon based, machine learning based and hybrid approaches. Similarly, deep learning based methods are not covered. \cite{arabic_survey_asa_social_media} surveys ASA methods that are published before 2017 with a focus on social media. \cite{arabic_survey_asa} provides a survey for ASA which includes the works before 2019. Although deep learning has already been widely adopted in general SA, their survey still focuses on works utilising lexicon based methods and traditional machine learning methods. \cite{arabic_survey_sarcasm_detection} provides a systematic survey for Arabic sarcasm detection, while our work focuses on the broader field of ASA which includes Arabic sarcasm detection. The most related work is the survey by \cite{asa_survey} which provides systematic survey for ASA utilising different methods that includes lexicon based methods, machine learning based methods and deep learning based methods. However, our work distinguishes itself through the following contributions:




%The contributions of this paper are as follows: (1). We introduce a taxonomy for sentiment analysis which can cover most of the sentiment analysis tasks. (2). We provide a systematic and up-to-date survey for Arabic sentiment analysis, especially the works that use deep learning methods. (3). We put Arabic sentiment analysis in a wider context of general sentiment analysis and point out the gap between them. (4). We outline the challenges and future directions for research. 

%We present a detailed taxonomy for sentiment analysis, and categorize existing sentiment analysis approaches into specific, well-defined categories. We then present a comprehensive analysis of sentiment analysis methods proposed for each category according to this taxonomy.  


% \textcolor{red}{Is this the beginning of a new section?}
\begin{comment}
\begin{table*} 
  \centering
  % \captionsetup{justification=centering}
  % \caption{\label{table: datasets} Datasets for Arabic Sentiment Analysis, organised according to our proposed taxonomy in Section \ref{section: taxonomy}.\\ \textit{SC: Sentiment Classification, MAST: Multifacted Analysis of Subjective Text, ABSA: Aspect based Sentiment Analysis.}}
  \begin{tabular}{m{5cm} m{1.2cm} m{2cm} m{2.7cm} m{3.2cm}}
    \hline
    \textbf{Dataset} & \textbf{Modality} & \textbf{Granularity} & \textbf{Context} & \textbf{Dialect} \\
    \hline
    LABR \cite{aly-atiya-2013-labr} & text & SC & document-level & MSA  and various other dialects \\
    ASTD \cite{nabil-etal-2015-astd} & text & SC & document-level & MSA and Egyptian Arabic \\
    ArSentD-LEV \cite{DBLP:journals/corr/abs-1906-01830} & text & SC, ABSA, MAST & document-level & Levantine dialect \\
    ArSarcasm \cite{abu-farha-magdy-2020-arabic} & text & SC, MAST & document-level & various dialects\\
    ArSarcasm-v2 \cite{abufarha-etal-2021-arsarcasm-v2} & text & SC, MAST & document-level & various dialects \\
    Arsen-20 \cite{fang2024arsen} & text & SC & document-level & various dialects \\ 
    Arabic multimodal dataset \cite{Haouhat2023TowardsAM} & text, audio, video & SC & document-level (video segments) & various dialects \\ 
    \hline
  \end{tabular}
  \captionsetup{justification=centering}
  \caption{\label{table: datasets} Datasets for Arabic Sentiment Analysis, organised according to modality, granularity and context.\\ \textit{SC: Sentiment Classification, MAST: Multifacted Analysis of Subjective Text, ABSA: Aspect based Sentiment Analysis.}}
\end{table*}
\end{comment} 

\begin{comment}
\section{Taxonomy for Sentiment Analysis} \label{section: taxonomy}
%In this section, we introduce a taxonomy for sentiment analysis which can cover most of the sentiment analysis tasks. As shown in Figure \ref{figure: classification framework}, we classify the existing sentiment analysis tasks from three levels: modality level, granularity level and application level. 

%In modality level, we classify the sentiment analysis tasks according to the modality of the data they analyse. We can roughly divide them into two categories: Unimodal sentiment analysis (i.e. SA tasks that involve a single modality such as text), and multi-modal sentiment analysis (i.e. SA tasks that involve multiple modalities such as text+audio or text+video).

This section presents a novel taxonomy for sentiment analysis tasks, encompassing a broad range of applications. As illustrated in Figure \ref{figure: classification framework}, the taxonomy categorizes sentiment analysis tasks across three distinct levels: modality level, granularity level, and application level.
\subsection{Modality level}
The modality level categorizes sentiment analysis tasks based on the type of data they analyze. Here, we establish two primary categories: 
namely \textbf{Uni-modal SA} and \textbf{Multi-modal SA}. The first category comprises tasks 
%\textbf{Unimodal Sentiment Analysis:} This category encompasses tasks
that involve sentiment analysis on data from a single modality, such as text analysis for sentiment extraction from written reviews; while the second category comprises tasks 
%\textbf{Multimodal Sentiment Analysis:} This category encompasses tasks 
that leverage data from multiple modalities to extract sentiment. Examples include sentiment analysis combining text and audio data from social media videos, or sentiment analysis that combines text and visual data from online product reviews.
\subsection{Granularity level}
The next level of our taxonomy focuses on granularity, encompassing both task granularity and sentiment granularity. Within each dimension, we categorize tasks based on the level of detail involved, ranging from coarse-grained to fine-grained. 
% \subsubsection{Task Granularity}

\textbf{Sentiment Classification}: This foundational task involves assigning pre-defined sentiment labels (e.g., positive, negative) to a given data point (typically text data). Sentiment classification categorizes data into binary classes (positive/negative) or multiple classes (e.g., positive, neutral, negative). Consequently, both the task granularity and sentiment granularity of sentiment classification are generally considered coarse-grained. 
\subsubsection{Task Granularity} 
\textcolor{red}{this section is difficult to understand, are the following tasks fine-grained? how about 2.2.2 -- is ABSA fine grained? also, any other subsections in 2.2.2?}
\par \textbf{Multifaceted Analysis of Subjective Text (MAST):}
MAST extends beyond sentiment classification by focusing on more specialized tasks. These include but not limit to the following tasks: 

\textbf{Irony Detection:} Identifying the use of irony in text, which can be challenging due to its inherent ambiguity \cite{zeng-li-2022-survey}. 

\textbf{Comparative Opinion Mining:} Extracting and analyzing comparative opinions expressed within text data \cite{https://doi.org/10.1002/asi.23716}.

\subsubsection{Sentiment Granularity}
\textbf{Aspect-Based Sentiment Analysis (ABSA):} This approach extends sentiment classification by delving deeper into the sentiment dimension. ABSA analyzes sentiment expressed towards specific aspects or opinions within the data, offering a more nuanced understanding of user sentiment (e.g., positive sentiment towards the "battery life" aspect of a product review) \cite{Zhang2022ASO}. 
%\textbf{Examples from Other Direction (Task Granularity):}\\

%We then come to the next level, granularity level. In granularity level, we classify sentiment analysis tasks from two directions: task granularity and sentiment granularity. In each direction, we classify the tasks from coarse-grained to fine-grained. We first have sentiment classification \cite{sentiment-analysis-book} which assign a pre-defined label such as positive, negative to the given data. Sentiment classification classify the given data (usually text data) into binary classes (positive, negative) or multiple classes (such as positive, neutral, negative), so the task granularity and sentiment granularity of sentiment classification are usually coarse-grained. Aspect-based sentiment analysis (ABSA) \cite{Zhang2022ASO} extends sentiment classification from the sentiment granularity direction. Aspect-based sentiment analysis analyses people's sentiment from various sentiment elements such as aspects, opinions \cite{Zhang2022ASO}. From the other direction, multifaceted analysis of subjective text (MAST) \cite{sentiment-analysis-book} extends sentiment classification from task granularity direction. Multifaceted analysis of subjective text moves towards more specialised tasks, such as irony detection \cite{zeng-li-2022-survey}, comparative opinion mining \cite{https://doi.org/10.1002/asi.23716}. 



\subsection{Application level}
The final level of our taxonomy focuses on the application level, which categorizes sentiment analysis tasks based on the unit of analysis employed. Here, we distinguish tasks based on whether sentiment analysis is applied to documents, sentences, or aspects within text data. %As this level is often determined by the characteristics of the datasets used, we will briefly discuss some widely used examples across different application levels.

\textbf{Document-Level Sentiment Analysis}: In this application level, tasks involve classifying the overall sentiment expressed within an entire document. Datasets such as IMDB \cite{maas-etal-2011-learning} exemplify this level, where sentiment labels (e.g., positive, negative) are assigned to movie reviews as a whole.

\textbf{Sentence-Level Sentiment Analysis}: This application level focuses on analyzing sentiment expressed at the sentence level. Datasets like \cite{socher-etal-2013-recursive} are commonly used in this context. %%%These datasets require sentiment classification for individual sentences within a document.

\textbf{Aspect-Level Sentiment Analysis}: The most fine-grained level of our taxonomy is aspect-level sentiment analysis. Tasks in this category involve analyzing sentiment expressed towards specific aspects or entities within text data. Datasets such as Rest14 \cite{cai-etal-2021-aspect} cater to this application level, enabling the analysis of sentiment expressed towards various aspects of restaurant reviews (for example: \textit{food, service, ambience}).
\end{comment}
%%%%\par It must be noted that this level is often determined by the characteristics of the datasets used. Table \ref{table: datasets} provides an overview of datasets for ASA aligned with our taxonomy.

%The final level is application level, in this level, we classify sentiment analysis tasks according to whether they apply sentiment analysis in document level, sentence level or aspect level. Since this level is usually determined by the datasets, we will briefly mention some widely used datasets in different levels. In document level, datasets such as IMDB \cite{maas-etal-2011-learning} classify the whole document into different classes. Datasets such as SST2 \cite{socher-etal-2013-recursive} and SST5 \cite{socher-etal-2013-recursive} apply SA in sentence level, while datasets such as Rest14 \cite{cai-etal-2021-aspect} apply sentiment analysis in a more fine-grained level, aspect level. 
% \textcolor{red}{Previous paragraph is quite large - try to break it into 2-3 different paragraphs, such that each paragraph is a coherent unit.}



\begin{comment}
\begin{table*} 
  \centering
  % \captionsetup{justification=centering}
  % \caption{\label{table: datasets} Datasets for Arabic Sentiment Analysis, organised according to our proposed taxonomy in Section \ref{section: taxonomy}.\\ \textit{SC: Sentiment Classification, MAST: Multifacted Analysis of Subjective Text, ABSA: Aspect based Sentiment Analysis.}}
  \begin{tabular}{m{4cm} m{2cm} m{2cm} m{3cm} m{3cm}}
    \hline
    \textbf{Dataset} & \textbf{Modality} & \textbf{Granularity} & \textbf{Application} & \textbf{Dialect} \\
    \hline
    LABR \cite{aly-atiya-2013-labr} & text & SC & document-level & MSA  and different dialects \\
    ASTD \cite{nabil-etal-2015-astd} & text & SC & document-level & MSA and Egyptian dialect \\
    ArSentD-LEV \cite{DBLP:journals/corr/abs-1906-01830} & text & SC, ABSA, MAST & document-level & Levantine dialect \\
    ArSarcasm \cite{abu-farha-magdy-2020-arabic} & text & SC, MAST & document-level & various dialects\\
    ArSarcasm-v2 \cite{abufarha-etal-2021-arsarcasm-v2} & text & SC, MAST & document-level & various dialects \\
    Arsen-20 \cite{fang2024arsen} & text & SC & document-level & various dialects \\ 
    Arabic multimodal dataset \cite{Haouhat2023TowardsAM} & text, audio, video & SC & document-level (video segments) & various dialects \\ 
    \hline
  \end{tabular}
  \captionsetup{justification=centering}
  \caption{\label{table: datasets} Datasets for Arabic Sentiment Analysis, organised according to our proposed taxonomy in Section \ref{section: taxonomy}.\\ \textit{SC: Sentiment Classification, MAST: Multifacted Analysis of Subjective Text, ABSA: Aspect based Sentiment Analysis.}}
\end{table*}


\begin{table*} 
\centering 
% \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.} 
  \begin{center}
  \begin{tabular}[h]{m{4cm} m{9cm} m{3cm}} 
  \hline 
   \textbf{Methods} & \textbf{Contributions} & \textbf{Accuracy} \\
   \hline 
  \cite{dahou-etal-2016-word} & Build Arabic word embeddings and use CNN as classifier & LABR(89.6\%), ASTD(79.07\%) \\
  \cite{medhaffar-etal-2017-sentiment} & Annotate a Tunisian dialect corpus and evaluate models on different dialects & -  \\ 
  \cite{baly-etal-2017-characterization} & Perform a characterization study that analyses tweets from different Arab regions, compare traditional machine learning and deep learning based methods for Arabic SA & ASTD(SVM 51.7\%, RNTN 58.5\%)  \\ 
  \cite{Guellil2018SentiALGAC} & Automatically construct an Algerian dialect corpus & -  \\ 
  \cite{attia-etal-2018-multilingual} & Propose a language independent, multi-class model for SA using CNNs & ASTD(67.93\%)  \\ 
  \cite{Alyafeai2021EvaluatingVT} & Compare different tokenizers for different Arabic classification tasks & -  \\ 
  \cite{Atabuzzaman2023ArabicSA} & Propose an explainable sentiment classification framework for Arabic & LABR(88.0\%) \\ 
  \hline  
  \end{tabular}
  \end{center}
  \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.}
\end{table*}
\end{comment} 

\begin{comment}
\begin{table*} 
\centering 
% \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.} 
  \begin{center}
  \begin{tabular}[h]{m{2cm} m{5.5cm} m{5cm} m{2cm}}
  \hline 
   \textbf{Methods} & \textbf{Contributions} &\textbf{Limitations} & \textbf{Accuracy} \\
   \hline 
  \cite{dahou-etal-2016-word} & Build Arabic word embeddings and use CNN as classifier & Task-specific method, static word embeddings & LABR(89.6\%), ASTD(79.07\%) \\
  \cite{medhaffar-etal-2017-sentiment} & Annotate a Tunisian dialect corpus and evaluate models on different dialects & Only use traditional machine learning methods & -  \\ 
  \cite{baly-etal-2017-characterization} & Perform a characterization study that analyses tweets from different Arab regions, compare traditional machine learning and deep learning based methods for Arabic SA & Do not experiment on different dialects and topics & ASTD(SVM 51.7\%, RNTN 58.5\%)  \\ 
  \cite{Guellil2018SentiALGAC} & Automatically construct an Algerian dialect corpus & Only evaluate using traditional machine learning methods & -  \\ 
  \cite{attia-etal-2018-multilingual} & Propose a language independent, multi-class model for SA using CNNs & Task-specific, for Arabic, only evaluate on ASTD \cite{nabil-etal-2015-astd}, it is not clear whether their model generalises well to other Arabic datasets & ASTD(67.93\%)  \\ 
  \cite{Alyafeai2021EvaluatingVT} & Compare different tokenizers for different Arabic classification tasks & Do not evaluate on complex architecture like attention-based models -  \\ 
  \cite{Atabuzzaman2023ArabicSA} & Propose an explainable sentiment classification framework for Arabic & Do not experiment on transformer-based models & LABR(88.0\%) \\ 
  \hline  
  \end{tabular}
  \end{center}
  \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.}
\end{table*} 
\end{comment} 


% \textcolor{red}{Is this the beginning of a new section?}
%%%\section{Datasets and Lexicons for Arabic Sentiment Analysis}
%%%This section presents an overview of several established datasets commonly employed for Arabic Sentiment Analysis (ASA). \textbf{The selection provided here is not exhaustive, but rather focuses on widely used and representative datasets that align with the taxonomy introduced in Section \ref{section: taxonomy}}. A comprehensive overview of these datasets is provided in Table \ref{table: datasets}.

%In this section, we introduce some widely used datasets for Arabic sentiment analysis, LABR \cite{aly-atiya-2013-labr},ASTD \cite{nabil-etal-2015-astd},ArSentD-LEV \cite{DBLP:journals/corr/abs-1906-01830}, ArSarcasm \cite{abu-farha-magdy-2020-arabic}, ArSarcasm-v2 \cite{abufarha-etal-2021-arsarcasm-v2}, Arsen-20 \cite{fang2024arsen}, Multimodal Arabic Sentiment Analysis Dataset \cite{Haouhat2023TowardsAM}. The datasets we list here are not extensive, we select some widely used and representative datasets according to our taxonomy in Section \ref{section: taxonomy}. See Table \ref{table: datasets} for an overview of these datasets. 

\section{The Evolution of Arabic Sentiment Analysis} 
In this section, we describe the evolution of Arabic sentiment analysis, from lexicon based methods to deep learning based methods. To highlight the importance of these traditional methods, we conduct a case study using Arabic lexicons, in which we highlight how lexicons can be utilised to improve deep learning based methods. 
\vspace{-0.2cm}
\subsection{Lexicon Based Methods} \vspace{-0.1cm}
Lexicon based methods utilise a pre-defined lexicon to determine the sentiment of the given text. The words in the lexicon are annotated with polarity or sentiment scores. The overall sentiment of the text are calculated by summing up all the words' sentiment score. Given their crucial role in lexicon based methods, we briefly mention some widely used Arabic lexicons. 

\textbf{Arabic Senti-Lexicon} \cite{senti-lexicon}: Arabic Senti-Lexcion contains 3880 synsets that are annotated with part of speech, polarity scores and inflected forms. 

\textbf{ArsenL (Arabic Sentiment Lexicon)} \cite{badaro-etal-2014-large}: ArsenL are constructed from multiple resources, including English WordNet (EWN), Arabic WordNet (AWN), English SentiWordNet (ESWN), and SAMA (Standard Arabic Morphological Analyzer). It contains 157969 synsets and has positve, negative and neutral sentiment scores.  

\subsection{Machine learning based methods}
Lexicon based methods are simple and fast, but they heavily rely on the lexicons and the sentiment scores of the lexicons lack context. Machine learning based methods can help to overcome these limitations by learning the sentiment features from data rather than pre-defining them by humans. However, for traditional machine learning methods, feature engineering is required as a precursor to the ML algorithms. \par Some widely used feature engineering methods include bag-of-words \cite{bag-of-words}, TF-IDF \cite{tf-idf} and word embedding \cite{Almeida2019WordEA}. After the features have been extracted, machine learning methods such as naive Bayes \cite{AlHoraibi2016SentimentAO}, support vector machines \cite{svm_sa} and random forests based methods \cite{Altawaier2016ComparisonOM} can be used for sentiment analysis. In the following sections, we will systematically review deep learning based approaches. 



\subsection{The importance of traditional methods: a case study on sentiment lexicons} 
It is important to note that even with the rise of deep learning based methods for sentiment analysis, sentiment lexicons like ArSenL\cite{badaro-etal-2014-large} can still be valuable as they provide a foundational understanding of sentiment that can enhance model performance, especially in low-resource scenarios or when dealing with domain-specific language that may not be well-represented in training data. Some use cases of these lexicons include data preprocessing where the irrelevant terms are filtered out based on the sentiment lexicon, sentiment weighting \cite{Zhang2011CombiningLA} where the lexicon is employed to help the model weight sentiment-relevant features more effectively. In the following paragraphs, we will use some examples to illustrate how sentiment lexicons can be utilised to improve the sentiment analysis performance. 

\textbf{Feature Augmentation}: Lexicons can be utilised to augment the features. In \cite{Heikal2018SentimentAO}, a sentiment lexicon is integrated to augment the features for deep learning based modes. \cite{Xiang2021LexicalDA} explores part-of-speech-focused lexical substitution for data augmentation to enhance sentiment analysis performance. \cite{lexicon_augmentation} discusses how lexicon-based approaches assign sentiment polarities and scores to keywords, which can be used for feature augmentation in hybrid models. 

\textbf{Interpretability}: Arabic sentiment lexicons enhance interpretability in sentiment analysis by providing a clear framework for understanding how specific words and phrases contribute to sentiment assessments. By combining lexicons with advanced methods like attention-based LSTM and explainable AI techniques, such as LIME \cite{Ribeiro2016WhySI}, researchers can further clarify which features are most significant in determining sentiment, thereby enhancing the transparency of their findings \cite{Abdelwahab2022JustifyingAT}. These lexicons facilitate the identification of the sentiment polarity of individual terms, making it easier to trace the reasoning behind a sentiment classification \cite{lexical_interpretability}. Moreover, the integration of lexicon-based approaches with machine learning techniques can improve the interpretability of complex models, as researchers can analyze how lexicon entries influence the overall sentiment predictions \cite{Ambreen2024PredictingCS}.  

\begin{table*} 
  \centering
  % \captionsetup{justification=centering}
  % \caption{\label{table: datasets} Datasets for Arabic Sentiment Analysis, organised according to our proposed taxonomy in Section \ref{section: taxonomy}.\\ \textit{SC: Sentiment Classification, MAST: Multifacted Analysis of Subjective Text, ABSA: Aspect based Sentiment Analysis.}}
  \begin{tabular}{m{5cm} m{1.2cm} m{2cm} m{2.7cm} m{3cm}}
    \hline
    \textbf{Dataset} & \textbf{Modality} & \textbf{Granularity} & \textbf{Context} & \textbf{Dialect} \\
    \hline
    LABR \cite{aly-atiya-2013-labr} & text & SC & document-level & MSA  and various other dialects \\ 
    ASTD \cite{nabil-etal-2015-astd} & text & SC & document-level & MSA and Egyptian Arabic \\ 
    ArSentD-LEV \cite{DBLP:journals/corr/abs-1906-01830} & text & SC, ABSA, MAST & document-level & Levantine dialect \\ 
    ArSarcasm \cite{abu-farha-magdy-2020-arabic} & text & SC, MAST & document-level & various dialects\\ 
    ArSarcasm-v2 \cite{abu-farha-etal-2021-overview} & text & SC, MAST & document-level & various dialects \\
    Arsen-20 \cite{fang2024arsen} & text & SC & document-level & various dialects \\  
    Arabic multimodal dataset \cite{Haouhat2023TowardsAM} & text, audio, video & SC & document-level (video segments) & various dialects \\ 
    \hline
  \end{tabular}
  \captionsetup{justification=centering}
  \caption{\label{table: datasets} Datasets for Arabic Sentiment Analysis, organised according to modality, granularity and context.\\ \textit{SC: Sentiment Classification \\ MAST: Multifacted Analysis of Subjective Text \\ ABSA: Aspect based Sentiment Analysis.}}
\end{table*}
\begin{table*} 
\centering 
% \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.} 
  \begin{center}
  \begin{tabular}[h]{m{2cm} m{6cm} m{5.5cm} m{1.5cm}}
  \hline 
   \textbf{Methods} & \textbf{Contributions} &\textbf{Limitations} & \textbf{Accuracy} \\
   \hline 
  \cite{dahou-etal-2016-word} & Develops Arabic word embeddings and employs CNN as a classifier & Task-specific method, static word embeddings & LABR (89.6\%), ASTD (79.07\%) \\ 
  \cite{medhaffar-etal-2017-sentiment} & Annotates a Tunisian dialect corpus and evaluates models on different dialects & Only experiments with traditional machine learning methods & -  \\ 
  \cite{baly-etal-2017-characterization} & Performs a characterization study that analyses tweets from different Arab regions, and compares ML-based vs. deep-learning methods for Arabic SA & Does not experiment on different dialects and topics & ASTD (SVM 51.7\%, RNTN 58.5\%)  \\ 
  \cite{Guellil2018SentiALGAC} & Automatically constructs an Algerian dialect corpus & Evaluation is carried out only using traditional ML methods & -  \\  \\
  \cite{attia-etal-2018-multilingual} & Proposes a language independent, multi-class model for SA using CNNs & Evaluation for Arabic is only carried out on ASTD \cite{nabil-etal-2015-astd}, unclear whether the model generalises well to other datasets & ASTD (67.93\%)  \\  \\
  \cite{Alyafeai2021EvaluatingVT} & Compares different tokenizers for different Arabic classification tasks & Does not evaluate on complex architecture like attention-based models -  \\ \\
  \cite{Atabuzzaman2023ArabicSA} & Proposes an explainable sentiment classification framework for Arabic & Does not conduct experimentation on Transformer-based models & LABR (88.0\%) \\ 
  \hline  
  \end{tabular}
  \end{center}
\captionsetup{justification=centering}
  \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification. \\ \textit{LABR: Large Scale Arabic Book Reviews Dataset \\ ASTD: Arabic Sentiment Tweets Dataset}}
\end{table*} 




\begin{comment}
ArSenL: A Large Scale Arabic Sentiment Lexicon.
SentiWS 
Feature Augmentation	"Sentiment Analysis of Arabic Tweets Using Deep Learning"	This study integrates a sentiment lexicon to augment features for a deep learning model, improving classification accuracy.	Alharbi, A., et al. (2020). "Sentiment Analysis of Arabic Tweets Using Deep Learning." Journal of King Saud University - Computer and Information Sciences.
Preprocessing	"A Hybrid Approach for Arabic Sentiment Analysis"	The authors use a sentiment lexicon to preprocess data, filtering out irrelevant terms and enhancing model input.	Baly, R., et al. (2018). "A Hybrid Approach for Arabic Sentiment Analysis." Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.
Sentiment Weighting	"Combining Lexicon-Based and Machine Learning Approaches for Sentiment Analysis"	This work combines lexicon-based sentiment scores with deep learning models, allowing the model to weigh sentiment-relevant features more effectively.	Khaireddine, M., et al. (2019). "Combining Lexicon-Based and Machine Learning Approaches for Sentiment Analysis." International Journal of Computer Applications.
Domain Adaptation	"Domain-Specific Sentiment Analysis Using Lexicon and Deep Learning"	The study demonstrates how a domain-specific lexicon improves the performance of deep learning models in sentiment analysis for e-commerce reviews.	Alhindi, A., et al. (2021). "Domain-Specific Sentiment Analysis Using Lexicon and Deep Learning." Journal of Information Science.
Interpretability	"Interpretable Sentiment Analysis Using Lexicons and Deep Learning" This research focuses on enhancing model interpretability by using lexicons to explain sentiment predictions made by deep learning models.	Zahran, M., et al. (2022). "Interpretable Sentiment Analysis Using Lexicons and Deep Learning." Journal of Natural Language Engineering.


ABSA-AR	A dataset specifically designed for aspect-based sentiment analysis in Arabic, containing reviews annotated with aspects and their corresponding sentiments.	5,000 reviews	E-commerce	Alharbi, A., et al. (2021). "ABSA-AR: A Dataset for Aspect-Based Sentiment Analysis in Arabic." Proceedings of the 2021 International Conference on Data Science and Advanced Analytics.


\subsection{Large-Scale Arabic Book Reviews (LABR) Dataset}
 LABR \cite{aly-atiya-2013-labr} is a widely used dataset that comprises over 65,000 Arabic book reviews. Each review is labelled with a sentiment polarity (positive or negative) and additionally assigned a rating on a scale of one to five. This dual classification structure allows LABR to be employed for tasks involving both binary sentiment classification and multi-class sentiment classification. Notably, the dataset offers both balanced and unbalanced splits, catering to research focusing on the impact of class distribution in sentiment analysis.
%LABR (Large-Scale Arabic Book Reviews) \cite{aly-atiya-2013-labr} contains over 65,000 Arabic reviews. The dataset classifies each review as positive or negative and also rates it from one to five, which makes the dataset can be used for both binary sentiment classification and multi-class sentiment classification. The dataset also contains both balanced and unbalanced splits depending on whether each class has the same number of reviews. 

\subsection{Arabic Sentiment Tweet Dataset (ASTD)}
The Arabic Sentiment Tweet Dataset (ASTD) \cite{nabil-etal-2015-astd} caters to sentiment analysis of Arabic text data gathered from Tweets. Focusing on the text modality only, ASTD comprises around 10,000 tweets. The dataset employs a four-class classification scheme, categorizing tweets as subjective positive, subjective negative, subjective mixed (containing both positive and negative sentiment), or objective. \\
\par Notably, ASTD operates at the document level, assigning a single sentiment label to each entire tweet. However, the presence of "subjective mixed" tweets within the dataset highlights the inherent complexity of sentiment analysis in Arabic due to the potential for co-existence of positive and negative sentiment within a single utterance.  Through appropriate text processing techniques, ASTD has the potential to be adapted for more fine-grained sentiment analysis tasks, such as sentiment analysis at the sentence level.
%ASTD (Arabic Sentiment Tweet Dataset)  is an Arabic sentiment analysis dataset gathered from Tweet in text-only modality. It consists of around 10,000 tweets and classifies them as subjective positive, subjective negative, subjective mixed and objective. The dataset itself is in document level where the whole tweet is assigned a label. However, the dataset contains subjective mixed tweets where a tweet has both positive and negative contents. With appropriate processing, this dataset may be applied to more fine-grained tasks, such as sentence-level sentiment analysis. 

\subsection{ArSentD-LEV}
The ArSentD-LEV \cite{DBLP:journals/corr/abs-1906-01830} dataset presents a multi-topic corpus specifically designed for Arabic sentiment analysis. ArSentD-LEV comprises 4,000 annotated tweets, offering rich layers of information for sentiment analysis tasks. Annotations encompass the overall sentiment of the tweet (positive, negative, etc.), the target of the sentiment expression (e.g., product, service, event), the sentiment expression mode (explicit or implicit), and the tweet's topic. This comprehensive annotation schema makes ArSentD-LEV valuable for a range of sentiment analysis applications, including sentiment classification (SC), aspect-based sentiment analysis (ABSA), and multifaceted analysis of subjective text (MAST).
\subsection{ArSarcasm}
ArSarcasm \cite{abu-farha-magdy-2020-arabic}: This dataset caters specifically to the tasks of sarcasm detection and sentiment analysis in Arabic text.  ArSarcasm comprises 10,547 tweets, each extensively annotated with labels across three key aspects: sarcasm (sarcastic or non-sarcastic), sentiment (positive, neutral, and negative), and dialect (covering various Arabic dialects). The inclusion of both sarcasm and sentiment labels allows for the application of ArSarcasm in tasks such as multifaceted analysis of subjective text (MAST) and sentiment classification (SC).
%ArSarcasm  is a dataset for Arabic sarcasm detection and sentiment analysis. It contains 10,547 tweets, each tweet is labelled from sarcasm (sarcastic or non-sarcastic), sentiment(positive, neutral and negative) and dialect(different Arabic dialects) aspects. Because each tweet is labelled from both sarcasm and sentiment aspects, it can be used for both multifacted analysis of subjective text (MAST) and sentiment classification (SC). 
\subsection{ArSarcasm-v2}
ArSarcasm-v2 \cite{abufarha-etal-2021-arsarcasm-v2}: Building upon the foundation of ArSarcasm \cite{abu-farha-magdy-2020-arabic}, ArSarcasm-v2 represents an extended dataset specifically designed for Arabic sarcasm detection and sentiment analysis.  This dataset comprises 15,548 tweets, offering a larger corpus for training and evaluation purposes.
\par Notably, ArSarcasm-v2 was employed as the benchmark dataset for the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic, highlighting its relevance to the research community. The dataset inherits the same comprehensive annotation scheme as ArSarcasm, encompassing labels for sarcasm (sarcastic or non-sarcastic), sentiment (positive, neutral, and negative), and dialect (various Arabic dialects). This rich annotation structure allows ArSarcasm-v2 to be utilized for tasks such as multifaceted analysis of subjective text (MAST) and sentiment classification (SC).
%ArSarcasm-v2 \cite{abufarha-etal-2021-arsarcasm-v2} is an extension of ArSarcasm \cite{abu-farha-magdy-2020-arabic} which contains 15,548 tweets. The dataset is used in WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. It contains the same labels as in ArSarcasm, so it can be used for both multifacted analysis of subjective text (MAST) and sentiment classification (SC). 
\subsection{Arsen-20}
Arsen-20 \cite{fang2024arsen} is a large-scale dataset that caters to Arabic sentiment analysis tasks, encompassing 20,000 tweets. A distinctive feature of Arsen-20 is the inclusion of user metadata alongside the tweets and their sentiment labels. This user metadata holds potential for enriching sentiment analysis by providing contextual understanding about the tweet's author. Notably, Arsen-20 employs document-level sentiment labeling, assigning a single sentiment label to each entire tweet.

\subsection{Multimodal Arabic Sentiment Analysis Dataset}
Multimodal Arabic Sentiment Analysis Dataset \cite{Haouhat2023TowardsAM}: This dataset ventures into the realm of multimodal sentiment analysis for Arabic data. It comprises a collection of text, audio, and video data points, offering a richer and more comprehensive perspective on sentiment expression. The dataset incorporates annotations for both the video segments themselves and the corresponding text and speech elements within each segment. This granular annotation scheme allows researchers to explore the potential of leveraging multiple modalities (text, audio, and video) for enhanced sentiment analysis in Arabic.
\end{comment} 
%Arabic multimodal dataset  is a multimodal dataset for Arabic sentiment analysis which contains text, audio and videos. It contains annotations for video segments and their text and speech. 
% HARD (Hotel Arabic-Reviews Dataset) \cite{} is a large scale dataset for Arabic sentiment analysis which contains 93700 hotel reviews. Similar to LABR \cite{aly-atiya-2013-labr}, it classifies each review as positive, neutral and negative and also rates it from one to five. It also provides both balanced and unbalanced splits. 
% \textcolor{red}{That's a nice number of related methods you've collected. Try to organise them into tables/figures. You may refer to page 31 and/or page 49 in this thesis: https://www.eecs.qmul.ac.uk/~simond/phd/RuchitAgrawal-PhD-Thesis.pdf} \\



\begin{comment} 

\begin{table*} 
\centering 
% \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.} 
  \begin{center}
  \begin{tabular}[h]{m{4cm} m{7cm} m{2cm}} 
  \hline 
   \textbf{Methods} & \textbf{Contributions} & \textbf{Accuracy} \\
   \hline 
  \cite{dahou-etal-2016-word} & Build Arabic word embeddings and use CNN as classifier & LABR(89.6\%), ASTD(79.07\%) \\
  \cite{medhaffar-etal-2017-sentiment} & Annotate a Tunisian dialect corpus and evaluate models on different dialects & -  \\ 
  \cite{baly-etal-2017-characterization} & Perform a characterization study that analyses tweets from different Arab regions, compare traditional machine learning and deep learning based methods for Arabic SA & ASTD(SVM 51.7\%, RNTN 58.5\%)  \\ 
  \cite{Guellil2018SentiALGAC} & Automatically construct an Algerian dialect corpus & -  \\ 
  \cite{attia-etal-2018-multilingual} & Propose a language independent, multi-class model for SA using CNNs & ASTD(67.93\%)  \\ 
  \cite{Alyafeai2021EvaluatingVT} & Compare different tokenizers for different Arabic classification tasks & -  \\ 
  \cite{Atabuzzaman2023ArabicSA} & Propose an explainable sentiment classification framework for Arabic & LABR(88.0\%) \\ 
  \hline  
  \end{tabular}
  \end{center}
  \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.}
\end{table*} 

\begin{table*} 
\centering 
% \caption{\label{tabel: language model sc} \textbf{Pre-train language model (LM)} based methods for Arabic sentiment classification.} 
  \begin{center}
  \begin{tabular}{m{4cm} m{9cm} m{3cm}} 
  \hline 
  \textbf{Methods} & \textbf{Contributions} & \textbf{Accuracy} \\
  \hline 
   hULMonA \cite{eljundi-etal-2019-hulmona} & Develop a pre-trained LM for Arabic and fine-tune it for SA & ASTD (balanced 86.5\%, unbalanced 69.9\%), ArSenTD-LEV(52.4\%) \\ 
  AraBERT \cite{antoun-etal-2020-arabert} & Pre-train an Arabic LM AraBERT and evaluate on different tasks & LABR(86.7\%), ASTD(92.6\%), ArSenTD-Lev(59.4\%) \\ 
  \cite{alkaoud-syed-2020-importance} & Propose tokenization methods for static and contextual word embeddings and improve their performance & LABR(89.87\%) \\ 
  \cite{abdul-mageed-etal-2021-arbert} & Introduce ARBERT and MARBERT, pre-train models on dialectal Arabic, introduce ARLUE benchmark & LABR(92.51\%), ASTD(95.24\%), ArSenTD-Lev(61.38\%) \\ 
  \cite{alyafeai-ahmad-2021-arabic} & Use distillation and quantization to train compact Arabic language models & LABR(87.5\%), ASTD(86.2\%)  \\ 
  \cite{el-mekki-etal-2021-domain} & Introduce an unsupervised domain adaptation method for Arabic cross-domain and cross-dialect SA & -  \\ 
  \cite{abu-kwaik-etal-2022-pre} & Compare feature-based, deep learning and pre-trained LM based methods on dialectal Arabic SA & -  \\ 
  \cite{Refai2022DataAU} & Propose a data augmentation method for Arabic text classification using Transformer based models & -  \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{tabel: language model sc} Pre-train language model (LM) based methods for Arabic sentiment classification.} 
\end{table*}

\end{comment} 






\section{Situating ASA methods vis--vis general SA approaches}
% This section explores various sentiment analysis tasks, contrasting the approaches used in general sentiment analysis with those employed specifically for Arabic text.
\begin{comment}
\subsection{Sentiment Classification}

% General sentiment classification utilizes task-specific models, such as Support Vector Machines (SVMs) or deep learning architectures, to assign sentiment labels (positive, neutral, negative) or sentiment ratings to input data, which can be text or data from other modalities. Recent work has explored the effectiveness of large language models like BERT and GPT-3 in sentiment classification tasks (Zhang et al., 2023 \cite{Zhang2023SentimentAI}).



Sentiment classification assigns the given input (text or other modalities) a sentiment label (positive, neutral, negative) or the rate of the sentiment (such as one to five). Sentiment classification is one of the earliest sentiment analysis tasks \cite{sentiment-analysis-book}, lots of works has been done for sentiment classification.

The development of general sentiment classification reflects the ongoing paradigm shifts within the field of natural language processing \cite{Liu2021PretrainPA}. Early works in sentiment classification primarily relied on task-specific models, employing either traditional machine learning methods like Support Vector Machines (SVMs) or deep learning-based approaches. These models were trained on labeled data and limited to solving specific tasks. However, the emergence of pre-trained language models such as BERT \cite{devlin-etal-2019-bert} has revolutionized the field. These models are pre-trained on massive amounts of unlabeled data and subsequently fine-tuned for specific tasks, including sentiment classification. Large language models like GPT-3 \cite{Brown2020LanguageMA} further push the boundaries of model size, demonstrating the ability to acquire various emergent capabilities such as in-context learning when scaled sufficiently large \cite{Wei2022EmergentAO}. A systematic analysis of large language models' effectiveness in tackling various sentiment analysis tasks, including sentiment classification, is provided by Zhang et al. (2023) \cite{Zhang2023SentimentAI}.

The development of Arabic sentiment classification follows a similar trajectory. Early research primarily focused on task-specific models trained on sentiment classification datasets for Arabic text. However, the success of pre-trained transformer models like BERT has led to their increasing adoption for Arabic sentiment analysis. Models such as hULMonA, AraBERT, ARBERT, and MARBERT are being fine-tuned for Arabic sentiment classification tasks (Eljundi et al., 2019 \cite{eljundi-etal-2019-hulmona}; Abdul-Mageed et al., 2021 \cite{abdul-mageed-etal-2021-arbert}).

\subsection{Multifaceted Analysis of Subjective Text (MAST)}

General MAST delves deeper into sentiment analysis by focusing on specialized tasks beyond sentiment classification, such as irony detection and comparative opinion mining (see Zeng and Li, 2022 \cite{zeng-li-2022-survey} for a comprehensive overview of general MAST sub-tasks).

Research on Arabic MAST remains less developed compared to general MAST. The availability of datasets like ArSarcasm and ArSarcasm-v2 has driven research primarily towards Arabic sarcasm detection. Pre-trained language models combined with various techniques, such as those employed by Hengl et al. (2021) \cite{hengle-etal-2021-combining} and Faraj et al. (2021) \cite{faraj-etal-2021-sarcasmdet}, have shown promising results in this area.

\subsection{Aspect-Based Sentiment Analysis (ABSA)}

ABSA extends sentiment analysis by analyzing sentiment at a finer level, focusing on aspects within the text. General ABSA encompasses various sub-tasks, ranging from extracting aspect terms to identifying the sentiment associated with those aspects (Zhang et al., 2022 \cite{Zhang2022ASO}).

Arabic ABSA lags behind general ABSA, with most research concentrating on aspect sentiment classification, essentially sentiment classification applied at the aspect level. Feature-based approaches and traditional machine learning methods were initially dominant in this domain. However, deep learning methods like those proposed by AlSmadi et al. (2017) \cite{AlSmadi2017DeepRN} and Abdelgwad (2021) \cite{Abdelgwad2021ArabicAS} are gaining traction.
\subsection{Comparison: Arabic vs. General Sentiment Analysis}
Drawing upon the taxonomy established in Section \ref{section: taxonomy}, this section outlines key research gaps between Arabic sentiment analysis and general sentiment analysis across three dimensions: modality level, granularity level, and application level.
%\textbf{Comparison: Arabic vs. General Sentiment Analysis}\\
%Several key differences exist between Arabic sentiment analysis and general sentiment analysis across three dimensions: modality level, granularity level, and application level.
\begin{itemize}
    \item Modality Level: General sentiment analysis explores multimodality, incorporating text, audio, and video data. However, Arabic sentiment analysis is primarily limited to text data (\cite{Lai2023MultimodalSA} for recent advancements in multimodal sentiment analysis).
    \item Granularity Level: Arabic sentiment analysis focuses more on sentiment classification, with limited exploration of MAST and ABSA tasks. This disparity is likely due to the scarcity of fine-grained Arabic sentiment analysis datasets.
    \item Application Level: Datasets for general sentiment analysis cover various levels (document, sentence, aspect), whereas Arabic datasets are primarily document-level. Even some recently released datasets lack annotations at sentence and aspect levels.
\end{itemize}
\end{comment}
% \section{Different Sentiment Analysis Tasks}

% This section provides an overview of various sentiment analysis tasks. We will explore each task separately, beginning with a brief introduction to relevant research in general sentiment analysis. Subsequently, the focus will shift to research specifically dedicated to Arabic sentiment analysis. \textbf{It is important to acknowledge that the selection of works presented here is not exhaustive. Rather, we aim to showcase representative methods that effectively highlight the key differences between general and Arabic sentiment analysis, while also providing insights into current trends within the field of Arabic sentiment analysis}.

We situate the research in Arabic Sentiment Analysis (ASA) and juxtapose it with general trends in sentiment analysis (SA) in this section. We present an overview of several sentiment analysis tasks, and for each task we highlight the advancements in  general sentiment analysis research, followed by a focus on Arabic-specific sentiment analysis. While not exhaustive, the selected approaches illustrate key differences and current trends between general and Arabic sentiment analysis.
Table \ref{table: datasets} provides an overview of datasets for ASA organised according to the modality, granularity and context involved. We refer to these datasets in the subsequent subsections. %% overview of sentiment analysis tasks, starting with an introduction to general sentiment analysis research, followed by a focus on Arabic-specific sentiment analysis. While not exhaustive, the selected works illustrate key differences and current trends between general and Arabic sentiment analysis.

\vspace{-0.3cm}
\subsection{Sentiment Classification}
Sentiment classification involves assigning a sentiment label (positive, neutral, negative) or a sentiment rating (e.g., one to five) to a given input, which can be text or data from other modalities. As one of the earliest sentiment analysis tasks \cite{sentiment-analysis-book}, sentiment classification has attracted significant research interest.
\subsubsection{General Sentiment Classification} \label{section: general sc}

The development of general sentiment classification reflects the ongoing paradigm shifts within the field of natural language processing \cite{Liu2021PretrainPA}. Early works in sentiment classification primarily relied on task-specific models, employing either traditional machine learning methods like Support Vector Machines (SVMs) or deep learning-based approaches. These models were trained on labeled data and limited to solving specific tasks. 

\par However, the emergence of pre-trained language models such as BERT \cite{devlin-etal-2019-bert} has revolutionized the field. These models, typically based on components of the Transformer architecture, are pre-trained on massive amounts of unlabeled data and subsequently fine-tuned for specific tasks, including sentiment classification. Large language models like GPT-3 \cite{Brown2020LanguageMA} further push the boundaries of model size, demonstrating the ability to acquire various emergent capabilities such as in-context learning when scaled sufficiently large \cite{Wei2022EmergentAO}. A systematic analysis of large language models' effectiveness in tackling various sentiment analysis tasks, including sentiment classification, is provided by \cite{Zhang2023SentimentAI}.
% The evolution of sentiment classification highlights key paradigm shifts within natural language processing \cite{Liu2021PretrainPA}. Early works in sentiment classification relied on task-specific models using machine learning methods like Support Vector Machines (SVMs) or deep learning approaches, trained on labeled data for specific tasks. The advent of pre-trained language models, such as BERT \cite{devlin-etal-2019-bert}, marked a significant transformation, utilizing the Transformer architecture pre-trained on large amounts of unlabeled data and fine-tuned for specific tasks, including sentiment classification. 
\vspace{-0.3cm}
\subsubsection{Arabic Sentiment Classification} \label{section: arabic sentiment classification}
The development of Arabic sentiment classification follows a similar trajectory to that of general sentiment classification. Early research predominantly focused on task-specific models trained on sentiment classification datasets for Arabic text. \cite{dahou-etal-2016-word} constructed Arabic word embeddings and subsequently employed a Convolutional Neural Network (CNN) as a classifier. \cite{attia-etal-2018-multilingual} proposed a language-independent framework for text classification, evaluating its performance on Arabic sentiment classification tasks as well. Table \ref{tabel: task-specific sc} provides detailed descriptions of various task-specific methods for Arabic sentiment classification, along with their contributions and limitations. As highlighted in the table, the biggest limitation of such methods is that they are task-specific and do not generalise well to other tasks or dialects. 

\par With the remarkable success of pre-trained language models based on bidirectional transformers, such as BERT \cite{devlin-etal-2019-bert}, on diverse natural language understanding tasks, numerous studies have explored their utilization for Arabic sentiment classification. \cite{eljundi-etal-2019-hulmona} developed hULMonA, a pre-trained language model specifically for Arabic, and fine-tuned it for Arabic sentiment analysis. AraBERT \cite{antoun-etal-2020-arabert} builds upon this work by pre-training the model entirely on an Arabic corpus and evaluating its performance on various tasks. \cite{abdul-mageed-etal-2021-arbert} introduced ARBERT and MARBERT, language models pre-trained on dialectal Arabic. Table \ref{tabel: language model sc} offers detailed descriptions of different pre-trained language model-based methods for Arabic sentiment classification.
\begin{table*} 
\centering 
% \caption{\label{tabel: language model sc} \textbf{Pre-train language model (LM)} based methods for Arabic sentiment classification.} 
  \begin{center}
  \begin{tabular}{m{2cm} m{5cm} m{4.5cm} m{3cm}}
  \hline 
  \textbf{Methods} & \textbf{Contributions} &\textbf{Limitations} & \textbf{Accuracy} \\
  \hline 
   hULMonA \cite{eljundi-etal-2019-hulmona} & Develops a pre-trained LM for Arabic and fine-tunes it for SA & Does not employ an Arabic specific tokenizer and only evaluates on the SA task & ASTD (86.5\%), ArSenTD-LEV (52.4\%) \\ \\
  AraBERT \cite{antoun-etal-2020-arabert} & Pre-trains an Arabic LM AraBERT and evaluates performance on different tasks & Does not systematically evaluate the model on different dialects & LABR (86.7\%), ASTD (92.6\%), ArSenTD-Lev (59.4\%) \\ \\
  \cite{alkaoud-syed-2020-importance} & Proposes tokenization methods for static and contextual word embeddings and improves their performance & Does not study generalisation ability of the proposed method & LABR (89.87\%) \\ \\
  \cite{abdul-mageed-etal-2021-arbert} & Introduces ARBERT and MARBERT, pre-trains models on dialectal Arabic, introduces ARLUE benchmark & The models have a high memory requirement, thereby impeding computational efficiency & LABR (92.51\%), ASTD (95.24\%), ArSenTD-Lev (61.38\%) \\ \\
  \cite{alyafeai-ahmad-2021-arabic} & Employs distillation and quantization to train compact Arabic language models & The effects of hyperparameter tuning are not analysed & LABR (87.5\%), ASTD (86.2\%)  \\ \\
  \cite{el-mekki-etal-2021-domain} & Introduces an unsupervised domain adaptation method for Arabic cross-domain and cross-dialect SA & Does not study the effect of domain adaptation from high-resource languages to Arabic & -  \\ \\
  \cite{abu-kwaik-etal-2022-pre} & Compares feature-based, deep learning and pre-trained LM based methods on dialectal Arabic SA & Lacks an error analysis and a comparison of feature-based vs pre-trained LMs in different situations & -  \\ \\
  \cite{Refai2022DataAU} & Proposes a data augmentation method for Arabic text classification using Transformer based models & Does not study the generalisation ability of their method & -  \\ \\
  \hline 
  \end{tabular}
  \end{center}
  \captionsetup{justification=centering}
  \caption{\label{tabel: language model sc} Pre-trained language model (LM) based methods for Arabic sentiment classification. \\ \textit{hULMonA: The First Universal Language Model in Arabic \\ LABR: Large Scale Arabic Book Reviews Dataset \\ ASTD: Arabic Sentiment Tweets Dataset \\
  ArSenTD-Lev: Arabic Sentiment Twitter Dataset for the Levantine dialect}}
\end{table*} 


%\section{Different Sentiment Analysis Tasks}

%In this section, we introduce different sentiment analysis tasks separately. For each task, we will first briefly introduce the works in general sentiment analysis, and then introduce the works for Arabic sentiment analysis. The works we list in this section are not extensive, we select some representative methods which can reflect the difference between general sentiment analysis and Arabic sentiment analysis and the trend of Arabic sentiment analysis. 

%\subsection{Sentiment Classification}
%Sentiment classification assigns the given input (text or other modalities) a sentiment label (positive, neutral, negative) or the rate of the sentiment (such as one to five). Sentiment classification is one of the earliest sentiment analysis tasks \cite{sentiment-analysis-book}, lots of works has been done for sentiment classification. 

%\subsubsection{General Sentiment Classification} \label{section: general sc}
%The development of general sentiment classification is in line with the paradigm changes in natural language processing \cite{Liu2021PretrainPA}. The early works for sentiment classification usually use task-specific models, either traditional machine learning methods (such as SVM) or deep learning based methods. These models are trained on labeled data and can only solve one specific task. With the development of pre-trained language models such as BERT \cite{devlin-etal-2019-bert}, the model can be pre-trained on massive unlabeled data and fine-tuned on specific tasks including sentiment classification. Large language models such as GPT3 \cite{Brown2020LanguageMA} further scale the model size, it has been found that when these models are large enough, they can obtain various emergent abilities such as in-context learning \cite{Wei2022EmergentAO}. \cite{Zhang2023SentimentAI} gives a systematic analysis of large language models' ability to solve various sentiment analysis tasks including sentiment classification. 
%\subsubsection{Arabic Sentiment Classification} \label{section: arabic sentiment classification}
%The development of Arabic sentiment classification is similar to general sentiment classification. The early works for Arabic sentiment classification use a task-specific model which is trained on sentiment classification datasets. \cite{dahou-etal-2016-word} build Arabic word embeddings and then use CNN as a classifier. \cite{attia-etal-2018-multilingual} propose a language independent text classification framework and they also evaluate on Arabic sentiment classification. See Table \ref{table: task-specific sc} for detailed descriptions of different task-specific methods for Arabic sentiment classification and their contributions and limitations. With the great success of bidirectional transformer based pre-trained language models such as BERT \cite{devlin-etal-2019-bert} on various natural language understanding tasks, many works have been conducted to utilise them for Arabic sentiment classification. \cite{eljundi-etal-2019-hulmona} develop a pre-trained language model hULMonA for Arabic and fine-tune it for Arabic sentiment analysis. AraBERT\cite{antoun-etal-2020-arabert} further extends it by pre-training the model entirely on Arabic corpus and evaluating the model on different tasks. \cite{abdul-mageed-etal-2021-arbert} introduce ARBERT and MARBERT which are language models pre-trained on dialectal Arabic. See Table \ref{table: language model sc} for detailed descriptions of different pre-trained language model based methods and their contributions and limitations. 



\subsection{Multifaceted Analysis of Subjective Text (MAST)}

Multifaceted analysis of subjective text (MAST) represents an extension of sentiment classification that delves deeper into task granularity. It shifts the focus towards more specialized tasks, such as irony detection \cite{zeng-li-2022-survey} and comparative opinion mining \cite{https://doi.org/10.1002/asi.23716}.

\subsubsection{General MAST}

The development trajectory of general MAST mirrors that of general sentiment classification, as discussed in Section \ref{section: general sc}. Due to the focus on specialized tasks within MAST, it encompasses a multitude of sub-tasks. While these sub-tasks have been extensively explored in the field, a detailed description falls outside the scope of this survey. We encourage readers to refer to comprehensive surveys on specific sub-tasks, such as those by \cite{zeng-li-2022-survey} and another work referenced here \cite{https://doi.org/10.1002/asi.23716}.

\subsubsection{Arabic MAST}

Compared to general MAST, research on Arabic MAST remains less developed. This section will solely introduce research on Arabic sarcasm detection, as it has garnered a relatively larger body of work following the release of the ArSarcasm \cite{abu-farha-magdy-2020-arabic} and ArSarcasm-v2 \cite{abu-farha-etal-2021-overview} datasets, alongside a shared task organized by WANLP \cite{abu-farha-etal-2021-overview}. 

\par While various methods have been employed, including traditional machine learning approaches, task-specific deep learning methods, and pre-trained language model-based methods, the latter category combined with various optimizations has emerged as the most effective approach. \cite{hengle-etal-2021-combining} propose a hybrid model that leverages contextual representations from AraBERT \cite{antoun-etal-2020-arabert} alongside static word vectors. 

\par Additionally, recent research explores various machine learning techniques such as down-sampling and augmentation \cite{israeli-etal-2021-idc} for this task. \cite{faraj-etal-2021-sarcasmdet} employ an ensemble approach, combining different pre-trained language models with a hard voting technique. \cite{Talafha2021SarcasmDA} propose framing the problem as a regression task, predicting the level of sarcasm. Table \ref{tabel: sarcasm detection} provides detailed descriptions of methods for Arabic sarcasm detection, along with their contributions and limitations. 

%Because Arabic has rich morphology and different dialects, many works try to improve Arabic sentiment classification using Arabic specific methods from the following aspects. 


%\textbf{Different Arabic Dialects} Many works have been done to address the dialect issue, in both task-specific methods and pre-trained language model based methods. \cite{medhaffar-etal-2017-sentiment} annotate a Tunisian dialect corpus and then evaluate their models on different dialect. \cite{baly-etal-2017-characterization} perform a characterization study that analyses tweets from different Arab region. \cite{Guellil2018SentiALGAC} automatically construct an Algerian dialect corpus. For pre-trained language models, ARBERT and MARBERT \cite{abdul-mageed-etal-2021-arbert} are pre-trained on dialectal Arabic. 

%\textbf{Arabic specific tokenization} Some works also examine Arabic specific tokenization methods. \cite{Alyafeai2021EvaluatingVT} compare different tokenizers for different Arabic classification tasks. \cite{alkaoud-syed-2020-importance} propose tokenization strategies for both static and contextual Arabic word embeddings and greatly improve their performance. 


%\textbf{Domain Adaptation and Data Augmentation} Some works try to address the limited Arabic data issue by domain adaptation and data augmentation. \cite{el-mekki-etal-2021-domain} introduce an unsupervised domain adaptation methods for Arabic cross-domain and cross-dialect sentiment analysis. \cite{Refai2022DataAU} propose a data augmentation method for Arabic classification tasks using transformer based models. 



%\subsection{Multifaceted Analysis of Subjective Text} 
%Multifaceted analysis of subjective text (MAST) extends sentiment classification from task granularity direction. It moves towards more specialised tasks, such as irony detection \cite{zeng-li-2022-survey}, comparative opinion mining \cite{https://doi.org/10.1002/asi.23716}. 
%\subsubsection{General MAST}
%The development of general MAST is similar to general sentiment classification which we described in Section \ref{section: general sc}. Because MAST investigates more specialised tasks, it contained many sub-tasks. For general MAST, many sub-tasks have been well studied, the detailed descriptions of these tasks are beyond this survey. We refer the reader to the surveys for these sub-task, such as \cite{zeng-li-2022-survey}, \cite{https://doi.org/10.1002/asi.23716}. 
%\subsubsection{Arabic MAST}
%Compared to general MAST, much fewer works have been done for Arabic MAST. In this section, we will only introduce the works for Arabic sarcasm detection, as a relatively large number works have been done for it with the release of ArSarcasm \cite{abu-farha-magdy-2020-arabic} and ArSarcasm-v2 \cite{abufarha-etal-2021-arsarcasm-v2} datasets and a shared task organised by WANLP \cite{abu-farha-etal-2021-overview}. Although different methods including traditional machine learning methods, task-specific deep learning methods and pre-trained language model based methods, the most effective methods are pre-trained language model based methods combined with various tricks. \cite{hengle-etal-2021-combining} propose a hybrid model which combines contextual representations from AraBERT \cite{antoun-etal-2020-arabert} and static word vectors. \cite{israeli-etal-2021-idc} apply various machine learning techniques such as down-sampling and augmentation. \cite{faraj-etal-2021-sarcasmdet} ensemble different pre-trained language models and use hard voting technique. \cite{Talafha2021SarcasmDA} propose to treat the problem as a regression problem and predict the level of sarcasm. See Table \ref{tabel: sarcasm detection} for detailed descriptions of methods for Arabic sarcasm detection and their contributions and limitations. 

\begin{comment}
\begin{table*} 
\centering 
  % \caption{\label{tabel: sarcasm detection} Methods for Arabic sarcasm detection. We collect them according to their contributions and limitations. The accuracy is evaluated on ArSarcam-v2 dataset.} 
  \begin{center} 
  \begin{tabular}[h]{m{4cm} m{9cm} m{2cm}} 
  \hline 
  \textbf{Methods} & \textbf{Contributions} & \textbf{Accuracy}  \\
  \hline 
  \cite{hengle-etal-2021-combining} & Propose a hybrid model which combines contextual representations from AraBERT \cite{antoun-etal-2020-arabert} and static word vectors & 74.1\%  \\ 
  \cite{israeli-etal-2021-idc} & Use pre-trained transformer based models and various machine learning techniques such as down-sampling and augmentation & 76.7\%  \\
  \cite{abu-farha-magdy-2021-benchmarking} & Compare different transformer based models for Arabic sentiment classification and sarcasm detection & -  \\ 
  \cite{Talafha2021SarcasmDA} & Annotate an Arabic sarcasm detection dataset, train a model which treats the problem as a regression problem and predicts the level of sarcasm & -  \\ 
  \cite{khondaker-etal-2022-benchmark} & Apply contrastive learning to Arabic social meaning tasks & -  \\ 
  \cite{faraj-etal-2021-sarcasmdet} & Ensemble different pre-trained language models and use hard voting technique & 78.3\%  \\ 
  \cite{el-mahdaouy-etal-2021-deep} & Propose an end-to-end multi-task model for Arabic sentiment analysis and sarcasm detection & 76.8\%  \\ 
  \cite{kaseb-farouk-2022-saids} & Propose SAIDS which uses its prediction of sarcasm and dialect as known information to predict the sentiment & -  \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{tabel: sarcasm detection} Methods for Arabic sarcasm detection. The accuracy is evaluated on ArSarcam-v2 dataset.} 
\end{table*}
\end{comment} 
\begin{comment}
\begin{table*} 
\centering 
% \caption{\label{Table: arabic absa} Methods for Arabic aspect based sentiment analysis (ABSA). We collect them according their contributions and limitations.} 
\begin{center}
  \begin{tabular}[h]{m{4cm} m{11cm}} 
  \hline 
  \textbf{Methods} & \textbf{Contributions} \\ 
  \hline 
  \cite{inbook} & Use pre-trained word embeddings for Arabic ABSA \\ 
  \cite{AlSmadi2017DeepRN} & Compare RNN and SVM for Arabic ABSA \\ 
  \cite{Alshammari2020AspectbasedSA} & Compare CNN and traditional machine learning methods for Arabic ABSA \\ 
  \cite{AlDabet2021EnhancingAA} & Propose different network architecture for different Arabic ABSA tasks \\ 
  \cite{Abdelgwad2021ArabicAS} & Develop a BERT based model for Arabic ABSA \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{Table: arabic absa} Methods for Arabic aspect based sentiment analysis (ABSA).} 
\end{table*}
\end{comment} 


\begin{table*} 
\centering 
  % \caption{\label{tabel: sarcasm detection} Methods for Arabic sarcasm detection. We collect them according to their contributions and limitations. The accuracy is evaluated on ArSarcam-v2 dataset.} 
  \begin{center} 
  \begin{tabular}[h]{m{2cm} m{6.2cm} m{4.5cm} m{2cm}}
  \hline 
  \textbf{Methods} & \textbf{Contributions} & \textbf{Limitations} & \textbf{Accuracy}  \\
  \hline 
  \cite{hengle-etal-2021-combining} & Proposes a hybrid model which combines contextual representations from AraBERT \cite{antoun-etal-2020-arabert} and static word vectors & Hybrid model, computational efficiency is impeded owing to use of both contextual and static embeddings. & 74.1\%  \\ \\
  \cite{israeli-etal-2021-idc} & Employs pre-trained Transformer based models and various machine learning techniques such as down-sampling and augmentation & Does not explain the effects of these techniques & 76.7\%  \\ \\
  \cite{Talafha2021SarcasmDA} & Annotates an Arabic sarcasm detection dataset, trains a regression model and predicts the level of sarcasm & While the model can output the level of sarcasm, it is reliant on a binary classification dataset & -  \\ \\
  \cite{khondaker-etal-2022-benchmark} & Applies contrastive learning to Arabic social meaning tasks & Does not study the generalisation ability of their method & -  \\ \\
  \cite{faraj-etal-2021-sarcasmdet} & Ensembles different pre-trained language models and employs hard voting technique & The method is not efficient as it needs various pre-trained language models. & 78.3\%  \\ \\
  \cite{el-mahdaouy-etal-2021-deep} & Proposes an end-to-end multi-task model for Arabic sentiment analysis and sarcasm detection & Does not present experimentation on other tasks & 76.8\%  \\ \\
  \cite{kaseb-farouk-2022-saids} & Proposes SAIDS that uses its prediction of sarcasm and dialect as known information to predict the sentiment & Does not study the generalisation ability of their method & -  \\ 
  \hline 
  \end{tabular}
  \end{center}
  \vspace{-0.2cm}
  \caption{\label{tabel: sarcasm detection} Methods for Arabic sarcasm detection. The accuracy is evaluated on ArSarcam-v2 dataset.} 
  \vspace{0.2cm}
\end{table*}

\begin{table*} 
\centering 
% \caption{\label{Table: arabic absa} Methods for Arabic aspect based sentiment analysis (ABSA). We collect them according their contributions and limitations.} 
\begin{center}
  \begin{tabular}[h]{m{3cm} m{6cm} m{6cm}}
  \hline 
  \textbf{Methods} & \textbf{Contributions} & \textbf{Limitations} \\  
  \hline 
  \cite{inbook} & Employs pre-trained word embeddings for Arabic ABSA & Only uses traditional machine learning methods as classifier \\ 
  \cite{AlSmadi2017DeepRN} & Compares RNN and SVM for Arabic ABSA & The dataset is relatively small, does not use other deep learning models such as LSTM \\ 
  \cite{Alshammari2020AspectbasedSA} & Compares CNN and traditional machine learning methods for Arabic ABSA & Task-specific method, does not compare different deep learning methods \\ 
  \cite{AlDabet2021EnhancingAA} & Proposes different network architectures for various Arabic ABSA tasks & Task-specific method \\ 
  \cite{Abdelgwad2021ArabicAS} & Develops a BERT based model for Arabic ABSA & Does not present experimentation on other tasks \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{Table: arabic absa} Methods for Arabic aspect based sentiment analysis (ABSA).} 
\end{table*}

\subsection{Aspect-Based Sentiment Analysis (ABSA)}

Aspect-based sentiment analysis (ABSA) extends sentiment analysis by introducing a finer-grained level of task granularity. Unlike sentiment classification, where the output is typically a binary or multi-class label, ABSA delves deeper, focusing on aspects within the sentiment analysis process.

\subsubsection{General ABSA}

Similar to MAST, general ABSA encompasses various sub-tasks, ranging from simpler single ABSA tasks like aspect term extraction to more complex compound ABSA tasks such as aspect sentiment triplet extraction \cite{Zhang2022ASO}. \cite{Zhang2022ASO} provide a comprehensive survey on general ABSA, we recommend referring to their work for further details on general trends in this direction.

\subsubsection{Arabic ABSA}

Research on Arabic ABSA lags behind that of general ABSA. The majority of existing works in Arabic ABSA primarily address aspect sentiment classification, which essentially translates to sentiment classification applied at the aspect level. Additionally, many studies rely solely on feature-based approaches and traditional machine learning methods. This section will focus exclusively on deep learning-based methods for Arabic ABSA. 

The development of Arabic ABSA parallels that of Arabic sentiment classification, as discussed in Section \ref{section: arabic sentiment classification}. A growing number of studies are employing deep learning and pre-trained language model-based methods. \cite{AlSmadi2017DeepRN} and \cite{Alshammari2020AspectbasedSA} compare traditional machine learning and deep learning methods for Arabic ABSA. \cite{AlDabet2021EnhancingAA} propose different network architectures tailored to specific Arabic ABSA tasks. \cite{Abdelgwad2021ArabicAS} develops a BERT-based model for Arabic ABSA. Table \ref{Table: arabic absa} provides detailed descriptions of methods for Arabic ABSA. 
\subsection{Comparison within Arabic Sentiment Analysis Methods}
Although task-specific methods are dedicated to ASA tasks, the most effective methods are those that combine pre-trained language models and various optimisation techniques. hULMonA \cite{eljundi-etal-2019-hulmona} first demonstrate the effectiveness of pre-trained language models by developing a pre-trained LM for Arabic and fine-tuning it for ASA, which significantly improves the performance. Latter pre-trained Arabic LMs such as AraBERT \cite{antoun-etal-2020-arabert} and ARBERT \cite{abdul-mageed-etal-2021-arbert} further push the boundary of ASA by using Arabic specific tokenisation and pre-training models on dialectal Arabic.
\par Another research direction extends the existing methods using various approaches such as domain adaptation \cite{el-mekki-etal-2021-domain} and data augmentation \cite{Refai2022DataAU}. However, the models based on pre-trained LMs are not computationally efficient, and involve a significant computational overhead, whereas approaches such as \cite{alyafeai-ahmad-2021-arabic} maintain the balance between performance and efficiency by distillation and quantisation. 

\subsection{Gaps and challenges in Arabic Sentiment Analysis}\label{sec:gaps}
\subsubsection{Gaps}
This section outlines key research gaps between Arabic sentiment analysis and general sentiment analysis across three dimensions: 
\begin{itemize}
    \item \textbf{Modality:} Multimodality has recently garnered significant interest within general sentiment analysis, with a surge in research activity \cite{Lai2023MultimodalSA}. However, investigations into multi-modal Arabic sentiment analysis remain limited. Most datasets for Arabic sentiment analysis are restricted to the text modality. 
    \item \textbf{Granularity:} The majority of research in Arabic sentiment analysis focuses solely on Arabic sentiment classification. As evidenced in the previous sections, even studies exploring Arabic MAST and Arabic ABSA often target simpler tasks. %This disparity might be attributed to the scarcity of ASA datasets, as most datasets are designed for sentiment classification. 
    Consequently, Arabic sentiment analysis lags behind general sentiment analysis in terms of MAST and ABSA tasks. 
    \item \textbf{Context:} While datasets for general sentiment analysis encompass various levels ranging from document level to aspect level, most datasets for Arabic sentiment analysis remain at the document level. Even some recently released datasets lack annotations at sentence level and aspect level. 
\end{itemize}

\subsubsection{Challenges}
%The intrinsic linguistic complexities of the Arabic language, coupled with cultural contexts, present substantial challenges for sentiment analysis (SA) in Arabic. These challenges can be categorized as follows:
%\begin{itemize}
%\item Morphological complexity and dialectical variants: 
\par The Arabic language is characterized by its high morphological complexity, which entails intricate word formation processes that may obscure meaning \cite{habash2010introduction}. Additionally, the high degree of ambiguity and polysemy inherent in Arabic lexicon complicates semantic interpretation. The presence of negation and the extensive range of dialects further exacerbate these challenges, as they introduce variations that must be meticulously accounted for in linguistic models \cite{elbeltagy2013open}.
\par  Data scarcity and cultural contextualization present additional challenges for Arabic. There is a scarcity of large, labeled datasets for many dialects, making it difficult to train robust models. Moreover, sentiment expression can vary significantly based on cultural nuances, requiring models to understand context beyond mere text.  
\begin{comment}   
\subsection{Aspect Based Sentiment Analysis}
Aspect based sentiment analysis (ABSA) extends sentiment analysis from task granularity direction. In ABSA, the task can be more fine-grained, instead of just binary classification or multi-class classification.
\subsubsection{General ABSA}
Similar to MAST, general ABSA has been well studied with various sub-tasks, from some simple single ABSA tasks such as aspect term extraction to more complex compound ABSA tasks such as aspect sentiment triplet extraction \cite{Zhang2022ASO}. \cite{Zhang2022ASO} provide a systematic survey for general ABSA, we refer the reader to their paper. 
\subsubsection{Arabic ABSA}
Compared to general ABSA, much fewer works have been done for Arabic ABSA. Most of the existing works for Arabic ABSA only investigate aspect sentiment classification which can be seen as a sentiment classification task applied at aspect level. Also, many works only use feature based and traditional machine learning methods. In this section, we only introduce deep learning based methods for Arabic ABSA. The development of Arabic ABSA is similar to Arabic sentiment classification which we described in Section \ref{section: arabic sentiment classification}, with more works using deep learning methods and pre-trained language model based methods. \cite{AlSmadi2017DeepRN} and \cite{Alshammari2020AspectbasedSA} compare traditional machine learning and deep learning methods for Arabic ABSA. \cite{AlDabet2021EnhancingAA} propose different network architecture for different Arabic ABSA tasks. \cite{Abdelgwad2021ArabicAS} develop a BERT based model for Arabic ABSA. See Table \ref{Table: arabic absa} for detailed descriptions of the methods for Arabic ABSA and their contributions and limitations. 


\subsection{Comparison of Arabic Sentiment Analysis and General Sentiment Analysis}

In this section, we outline some key research gaps between Arabic sentiment analysis and general sentiment analysis from modality level, granularity level and application level according to our taxonomy in Section \ref{section: taxonomy}. 

\textbf{Modality Level} Multimodality has received great interest in general sentiment analysis recently with lots of research has been conducted \cite{Lai2023MultimodalSA}. However, there are only limited works investigate multimodal Arabic sentiment analysis. And most of the datasets for Arabic sentiment analysis are restricted to text-only modality.

\textbf{Granularity Level} Most of the works about Arabic sentiment analysis are only for Arabic sentiment classification. As we can see from the previous sections, even works that investigate Arabic MAST and Arabic ABSA, they only analyse some simple tasks. This is perhaps because of the lack of datasets, while most of the datasets for Arabic sentiment analysis are for sentiment classification. Arabic sentiment analysis is far behind general sentiment analysis on MAST and ABSA tasks. 

\textbf{Application Level} While the datasets for general sentiment analysis span from document level to aspect level, most of the datasets for Arabic sentiment analysis are still at document level, even some recently released datasets do not contain annotations at sentence level and aspect level. 
\end{comment}
\section{Recent trends in Arabic Sentiment Analysis}
% \section{Challenges and Considerations in Arabic Sentiment Classification}
%%%\textcolor{red}{Quite verbose section too, we can trim it to fit in one page - remove repetitions and extra subsections that are not very relevant (comment them so you can use them in the future)}
Several research efforts are ongoing to develop robust Arabic-specific methods and overcome the challenges presented in Section \ref{sec:gaps}. We organise and present these efforts below:
\subsection{Addressing Dialectal Variations}
The issue of dialectal variation has received significant attention in both task-specific and pre-trained language model-based approaches. \cite{baly-etal-2017-characterization} conducted a characterization study analyzing tweets from different Arab regions, highlighting the importance of addressing the dialectal problems in Arabic SA. The efforts that tackle this challenge are presented below, grouped into the broad approach employed:
\subsubsection{Dataset Creation, Domain Adaptation and Data Augmentation}
\cite{medhaffar-etal-2017-sentiment} addressed this challenge by annotating a corpus specifically for the Tunisian dialect and evaluating their models on data from various dialects. A similar approach to this method was the one proposed by \cite{Guellil2018SentiALGAC}. They presented a method for automatically constructing an Algerian dialect corpus. \cite{el-mekki-etal-2021-domain} introduced an unsupervised domain adaptation method for cross-domain and cross-dialect sentiment analysis in Arabic. \cite{Refai2022DataAU} proposed a data augmentation method specifically designed for Arabic classification tasks using transformer-based models.
\subsubsection{Increasing use of Deep learning} 
Researchers are increasingly using deep learning models, particularly transformer based models, to effectively capture the nuances of different Arabic dialects. For example, ARBERT and MARBERT \cite{abdul-mageed-etal-2021-arbert} were specifically pre-trained on dialectal Arabic to address these dialectal variations. 
\subsubsection{Transfer Learning and Multilingual Models}
Transfer learning approaches are being used to leverage knowledge from models trained on larger datasets in other languages or MSA, facilitating better performance on dialect data with limited resources. Multilingual transformer models like mBERT are also applied for handling multiple Arabic dialects \cite{devlin-etal-2019-bert}.

%%%There are some works that are not dedicated to Arabic SA, but their methods can be utilised to address the dialect problems in Arabic SA, these works include:

%%%\textbf{Crowdsourcing and Data Augmentation:} Due to the scarcity of annotated dialectal datasets, crowdsourcing annotation efforts \cite{snow2008cheap} and data augmentation strategies \cite{kobayashi-2018-contextual} are being employed to enrich training data, improving dialect representation in sentiment analysis tasks. 

%%%\textbf{Cross-linguistic Studies:} There is a growing interest in comparing sentiment analysis insights and methodologies between Arabic dialects and other languages, promoting the development of more robust, universal approaches to sentiment analysis \cite{Meyer2012WiktionaryAN}, \cite{cambria2017sentiment}. 

\subsection{Arabic-specific Tokenization}
Recent research has also explored the importance of developing tokenization methods specifically for the Arabic language. \cite{Alyafeai2021EvaluatingVT} compared the performance of different tokenizers for various Arabic classification tasks. \cite{alkaoud-syed-2020-importance} proposed tokenization strategies specifically tailored for both static and contextual Arabic word embeddings, demonstrating significant performance improvements.The efforts in this direction can be grouped into the following trends:

\textbf{Morphological Analysis} Implementation of advanced morphological analysis tools to accurately identify roots, prefixes, and suffixes, ensuring proper tokenization of complex words. Noteworthy contributions in this area include the MADAMIRA tool, which provides robust morphological analysis and disambiguation for modern written Arabic, showcasing significant improvement in processing complex Arabic morphological structures \cite{pasha2014madamira}. 

\textbf{Dialect-Specific Tokenizers:} Development of tokenization models tailored to specific Arabic dialects to handle regional vocabulary and expressions effectively. The CALIMA-Star project exemplifies efforts to create comprehensive morphological lexicons specific to different Arabic dialects, allowing more precise tokenization and analysis for dialectal data \cite{taji-etal-2018-arabic}. 

\textbf{Contextual Tokenization:} Use of context-aware tokenization methods to understand the meaning of words in context, assisting in disambiguating similar words. Contextual models like AraBERT and its advancements in tokenization strategies demonstrate the power of context-aware embeddings to capture nuanced language variations in sentiment analysis \cite{antoun-etal-2020-arabert}. 

\textbf{Incorporating Diacritics:} Desining tokenizers that handle diacritics appropriately, either by retaining them for analysis or by normalizing words without diacritics while preserving meaning. Research by \cite{alqahtani-etal-2020-multitask} highlights the role of diacritics in enhancing sentiment analysis, emphasizing the necessity for tokenizers that efficiently process diacritized text data without losing critical semantic information. 
\par While these trends have demonstrated improved performance for ASA, significant research efforts need to be directed in order to bridge the gap between ASA and general SA for high-resource languages.

%%%\subsection{Domain Adaptation and Data Augmentation}
%%%The limited availability of Arabic sentiment analysis data has motivated research into domain adaptation and data augmentation techniques.

\section{Future Directions}
To conclude, we present promising research directions to foster the development of robust models for Arabic sentiment analysis. \\
\begin{comment}
\textbf{Data Scarcity and Granularity:} The current landscape of Arabic sentiment analysis datasets is limited in several aspects. Most existing studies are restricted to:\\
\textbf{Text modality:} Research is hindered by the lack of datasets incorporating multimodal information (text, audio, video).\\ \textbf{Sentiment classification task:} The focus on sentiment classification tasks restricts exploration of more fine-grained sentiment analysis tasks. \textbf{Document level SA:} The absence of datasets with finer granularity (sentence level, aspect level) limits research progress for ASA. \\

\par To address these limitations, the creation of more comprehensive Arabic sentiment analysis datasets is crucial. 
\end{comment} 
\textbf{Creation of richer datasets:} Future efforts should prioritize the development of datasets that encompass richer annotations across the following dimensions: \vspace{-0.3cm}
\begin{itemize}
    \item \textbf{Multimodality:} Datasets that integrate various modalities (text, audio, video) to capture richer sentiment information.
    \vspace{-0.1cm}
    \item \textbf{Fine-grained tasks:} Datasets designed for exploring more intricate sentiment analysis tasks beyond sentiment classification.
    \vspace{-0.1cm}
    \item \textbf{Multi-context annotations:} Datasets with annotations at finer levels (sentence level, aspect level) to facilitate in-depth analysis. 
    \vspace{-0.1cm}
\end{itemize}
\textbf{Multimodal Sentiment Analysis:} While limited research has been conducted in multimodal ASA, leveraging information from multiple modalities holds significant potential for accurate sentiment analysis. Future research should explore effective techniques for incorporating multimodal data and develop robust models for this task. 

\textbf{Large Language Models (LLMs) for ASA:} Recent advancements in LLMs have yielded remarkable performance on various tasks. Arabic LLMs like AceGPT-LL \cite{Huang2023AceGPTLL} have also emerged. However, a systematic analysis of LLMs for sentiment analysis, particularly in the context of Arabic, is lacking. Future research should investigate how to best utilize LLMs for Arabic sentiment analysis. %%%, exploring techniques for effectively leveraging their capabilities. 

\textbf{Interpretable Sentiment Analysis:} Existing Arabic sentiment analysis methods primarily provide final sentiment labels without explanations for their outputs. %%%Interpretability is becoming increasingly important in sentiment analysis. 
Recent work on improving the interpretability of question answering by examining model reasoning \cite{huang-chang-2023-towards} suggests a promising approach that can be adapted to sentiment analysis. For example, models could be designed to output reasoning steps leading to their final sentiment polarity predictions. 

\textbf{Exploration of Fine-Grained Tasks:} General sentiment analysis research has shifted towards increasingly fine-grained tasks. However, most Arabic sentiment analysis studies remain focused on sentiment classification at the document level. A systematic exploration of other fine-grained tasks, particularly those within MAST and ABSA, would be beneficial for advancing the field. 

\section{Limitations}

This survey acknowledges some limitations. Firstly, it primarily focuses on works utilizing deep learning methods. As highlighted in \cite{abu-kwaik-etal-2022-pre}, feature-based methods can outperform pre-trained language model-based methods in some instances. Future surveys may benefit from including an exploration of feature-based approaches. Additionally, while this work compares Arabic sentiment analysis with general sentiment analysis, it would also be valuable to situate Arabic sentiment analysis within the broader context of Arabic classification tasks and Arabic natural language processing tasks in general. 
\begin{comment}
    

\section{Future Directions}
In this section, we outline some challenges and future directions for Arabic sentiment analysis. 

\textbf{Creation of more datasets} Most of the existing datasets for Arabic sentiment analysis are restricted to text-modality, sentiment classification task and document level, which greatly restricts the research in Arabic sentiment analysis. Therefore, it is important to create more Arabic sentiment analysis datasets, especially the datasets for multimodal sentiment analysis, more fine-grained datasets. 

\textbf{Multimodal sentiment analysis} There are very few works done for multimodal Arabic sentiment analysis. However, using the information from multimodality is important to accurately analyse the sentiment. 

\textbf{Use of large language models} Recently, large language models have shown remarkable performance on various tasks, and there are also some Arabic large language models have been proposed \cite{Huang2023AceGPTLL}. However, there lacks systematic analyses of large language models for sentiment analysis, especially for Arabic sentiment analysis, for example, how to better utilise them for sentiment analysis. 

\textbf{Interpretable sentiment analysis} Most of the existing methods for Arabic sentiment analysis can only output the final label. We do not know why the model gives us that output, and the interpretability is becoming more and more important. Some recent works try to improve the interpretability of question answering by examining the model's reasoning ability \cite{huang-chang-2023-towards}, and we argue that this method can also be used for sentiment analysis. For example, we can ask the model outputs its reasoning steps before it gives us the final sentiment polarity.

\textbf{More fine-grained tasks} The works in general sentiment analysis are moving towards more fine-grained tasks. However, most of the works for Arabic sentiment analysis still only investigate sentiment classification at document level. It will be helpful to also systematic analyse other more fine-grained tasks, especially MAST and ABSA tasks. 

\section{Limitations}
In this work, we only introduce works that uses deep learning methods. However, as pointed in \cite{abu-kwaik-etal-2022-pre}, feature based methods sometimes can even outperform pre-trained language model based methods. It might be helpful to also introduce works that uses feature based methods. In our survey, we compare Arabic sentiment analysis and general sentiment analysis. In might be helpful to also situate Arabic sentiment analysis within the broader field of Arabic classification tasks, and more broadly, Arabic natural language processing tasks. 


\end{comment}


\begin{comment}
These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
the \LaTeX{} style file used to format it (\texttt{acl.sty}),
an ACL bibliography style (\texttt{acl\_natbib.bst}),
an example bibliography (\texttt{custom.bib}),
and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

\section{Engines}

To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

\section{Preamble}

The first line of the file must be
\begin{quote}
\begin{verbatim}
\documentclass[11pt]{article}
\end{verbatim}
\end{quote}

To load the style file in the review version:
\begin{quote}
\begin{verbatim}
\usepackage[review]{acl}
\end{verbatim}
\end{quote}
For the final version, omit the \verb|review| option:
\begin{quote}
\begin{verbatim}
\usepackage{acl}
\end{verbatim}
\end{quote}

To use Times Roman, put the following in the preamble:
\begin{quote}
\begin{verbatim}
\usepackage{times}
\end{verbatim}
\end{quote}
(Alternatives like txfonts or newtx are also acceptable.)

Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
\begin{quote}
\begin{verbatim}
\setlength\titlebox{<dim>}
\end{verbatim}
\end{quote}
where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

\section{Document Body}

\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

See Table~\ref{tab:accents} for an example of a table and its caption.
\textbf{Do not override the default caption sizes.}

\begin{table}
  \centering
  \begin{tabular}{lc}
    \hline
    \textbf{Command} & \textbf{Output} \\
    \hline
    \verb|{\"a}|     & {\"a}           \\
    \verb|{\^e}|     & {\^e}           \\
    \verb|{\`i}|     & {\`i}           \\
    \verb|{\.I}|     & {\.I}           \\
    \verb|{\o}|      & {\o}            \\
    \verb|{\'u}|     & {\'u}           \\
    \verb|{\aa}|     & {\aa}           \\\hline
  \end{tabular}
  \begin{tabular}{lc}
    \hline
    \textbf{Command} & \textbf{Output} \\
    \hline
    \verb|{\c c}|    & {\c c}          \\
    \verb|{\u g}|    & {\u g}          \\
    \verb|{\l}|      & {\l}            \\
    \verb|{\~n}|     & {\~n}           \\
    \verb|{\H o}|    & {\H o}          \\
    \verb|{\v r}|    & {\v r}          \\
    \verb|{\ss}|     & {\ss}           \\
    \hline
  \end{tabular}
  \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
  \label{tab:accents}
\end{table}

As much as possible, fonts in figures should conform
to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

Using the \verb|graphicx| package graphics files can be included within figure
environment at an appropriate point within the text.
The \verb|graphicx| package supports various optional arguments to control the
appearance of the figure.
You must include it explicitly in the \LaTeX{} preamble (after the
\verb|\documentclass| declaration and before \verb|\begin{document}|) using
\verb|\usepackage{graphicx}|.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{example-image-golden}
  \caption{A figure with a caption that runs for more than one line.
    Example image is usually available through the \texttt{mwe} package
    without even mentioning it in the preamble.}
  \label{fig:experiments}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
  \includegraphics[width=0.48\linewidth]{example-image-b}
  \caption {A minimal working example to demonstrate how to place
    two images side-by-side.}
\end{figure*}

\subsection{Hyperlinks}

Users of older versions of \LaTeX{} may encounter the following error during compilation:
\begin{quote}
\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
\end{quote}
This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

\subsection{Citations}

\begin{table*}
  \centering
  \begin{tabular}{lll}
    \hline
    \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
    \hline
    \citep{Gusfield:97}       & \verb|\citep|           &                           \\
    \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
    \citet{Gusfield:97}       & \verb|\citet|           &                           \\
    \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
    \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
    \hline
  \end{tabular}
  \caption{\label{citation-guide}
    Citation commands supported by the style file.
    The style is based on the natbib package and supports all natbib citation commands.
    It also supports commands defined in previous ACL style files for compatibility.
  }
\end{table*}

Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

A possessive citation can be made with the command \verb|\citeposs|.
This is not a standard natbib command, so it is generally not compatible
with other style files.

\subsection{References}

\nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliography{custom}
\end{verbatim}
\end{quote}

You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}

Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Equations}

An example equation is shown below:
\begin{equation}
  \label{eq:example}
  A = \pi r^2
\end{equation}

Labels for equation numbers, sections, subsections, figures and tables
are all defined with the \verb|\label{label}| command and cross references
to them are made with the \verb|\ref{label}| command.

This an example cross-reference to Equation~\ref{eq:example}.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Acknowledgments}

This document has been adapted
by Steven Bethard, Ryan Cotterell and Rui Yan
from the instructions for earlier ACL and NAACL proceedings, including those for
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan,
NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.
\end{comment}
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
%\bibliographystyle{IEEEtran}
\bibliography{acl_latex}

%%\begin{appendices}
%%\appendix

\begin{comment}
\section{Appendix}\label{section: appendix} 

\begin{table*} 
\centering 
  \begin{center}
  \begin{tabular}[h]{m{4cm} m{6cm} m{5cm}}
  \hline 
   \textbf{Methods} & \textbf{Contributions} &\textbf{Limitations} \\
   \hline 
  \cite{dahou-etal-2016-word} & Builds Arabic word embeddings and employs a Convolutional Neural Network (CNN) as the classifier & Task-specific method, static word embeddings \\
  \cite{medhaffar-etal-2017-sentiment} & Annotates a Tunisian dialect corpus and evaluates models on different dialects & Only employs traditional machine learning methods \\ 
  \cite{baly-etal-2017-characterization} & Performs a characterization study that analyses tweets from different Arab regions, compares traditional machine learning methods and deep learning based methods for Arabic Sentiment Analysis & Does not experiment on different dialects and topics \\ 
  \cite{Guellil2018SentiALGAC} & Automatically constructs an Algerian dialect corpus & Evaluation is presented only using traditional machine learning methods \\ 
  \cite{attia-etal-2018-multilingual} & Proposes a language independent, multi-class model for Sentiment Analysis using Convolutional Neural Networks (CNNs) & Task-specific, for Arabic, evaluation is presented only on the Arabic Sentiment Tweets Dataset (ASTD) \cite{nabil-etal-2015-astd}, it is not clear whether their model generalises well to other Arabic datasets \\ 
  \cite{Alyafeai2021EvaluatingVT} & Compares different tokenizers for different Arabic sentiment classification tasks & Does not evaluate on complex architecture like attention-based models \\ 
  \cite{Atabuzzaman2023ArabicSA} & Proposes an explainable sentiment classification framework for Arabic & Does not experiment on transformer-based models \\ 
  \hline  
  \end{tabular}
  \end{center}
  \caption{\label{tabel: task-specific sc} Task-specific methods for Arabic sentiment classification.}
\end{table*}


\begin{table*} 
\centering 
  \begin{center}
  \begin{tabular}{m{4cm} m{6cm} m{5cm}}
  \hline 
  \textbf{Methods} & \textbf{Contributions} &\textbf{Limitations} \\
  \hline 
   hULMonA \cite{eljundi-etal-2019-hulmona} & Develops a pre-trained Language Model (LM) for Arabic and fine-tunes it for Sentiment Analysis & Does not use an Arabic specific tokenizer and only evaluates on the Sentiment Analysis task \\ 
  AraBERT \cite{antoun-etal-2020-arabert} & Pre-trains an Arabic Langage Model called AraBERT and evaluates on different tasks & Does not systematically evaluate on different dialects \\ 
  \cite{alkaoud-syed-2020-importance} & Proposes tokenization methods for static and contextual word embeddings and improves their performance & Does not study the generalisation ability of the proposed method \\ 
  \cite{abdul-mageed-etal-2021-arbert} & Introduces ARBERT and MARBERT, pre-trained models for dialectal Arabic, introduces the ARLUE benchmark & The models may not be computationally efficient owing to the high memory requirement\\ 
  \cite{alyafeai-ahmad-2021-arabic} & Uses distillation and quantization to train compact Arabic language models & Does not study the effects of hyper-parameter tuning \\ 
  \cite{el-mekki-etal-2021-domain} & Introduces an unsupervised domain adaptation method for Arabic cross-domain and cross-dialect Sentiment Analysis & Does not study the effect of domain adaptation from high-resource languages to Arabic \\ 
  \cite{abu-kwaik-etal-2022-pre} & Compares feature-based, deep learning and pre-trained LM based methods on dialectal Arabic Sentiment Analysis & Lacks a performance error analysis and a comparison of feature-based vs pre-trained LMs in different situations and an explanation of performance improvement obtained using a particular method\\ 
  \cite{Refai2022DataAU} & Proposes a data augmentation method for Arabic text classification using Transformer based models & Does not study the generalisation ability of their method \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{tabel: language model sc} Pre-trained language model (LM) based methods for Arabic sentiment classification.} 
\end{table*}

\begin{table*} 
\centering 
  \begin{center} 
  \begin{tabular}[h]{m{4cm} m{6cm} m{5cm}}
  \hline 
  \textbf{Methods} & \textbf{Contributions} & \textbf{Limitations} \\
  \hline 
  \cite{hengle-etal-2021-combining} & Proposes a hybrid model that combines contextual representations from AraBERT \cite{antoun-etal-2020-arabert} and static word vectors & Hybrid model, the model may not be efficient \\ 
  \cite{israeli-etal-2021-idc} & Uses pre-trained Transformer-based models and various machine learning techniques such as down-sampling and augmentation & Does not explain the effects of these techniques \\
  \cite{abu-farha-magdy-2021-benchmarking} & Compares different Transformer-based models for Arabic sentiment classification and sarcasm detection & Evaluation is not presented on other tasks \\ 
  \cite{Talafha2021SarcasmDA} & Annotates an Arabic sarcasm detection dataset, trains a model that treats the task as a regression problem and predicts the level of sarcasm & Their model can output the level of sarcasm, but their dataset only contains binary labels, as relevant for binary classification. \\ 
  \cite{khondaker-etal-2022-benchmark} & Applies contrastive learning to Arabic opinion mining tasks & Does not study the generalisation ability of their method \\ 
  \cite{faraj-etal-2021-sarcasmdet} & Ensembles different pre-trained language models and uses hard voting technique & The method is not efficient as it needs various pre-trained language models. \\ 
  \cite{el-mahdaouy-etal-2021-deep} & Proposes an end-to-end multi-task model for Arabic sentiment analysis and sarcasm detection & Does not experiment on other tasks \\ 
  \cite{kaseb-farouk-2022-saids} & Proposes SAIDS that employs the prediction of sarcasm and dialect as known information to predict the sentiment & Does not study the generalisation ability of their method \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{tabel: sarcasm detection} Methods for Arabic sarcasm detection.} 
\end{table*}
\begin{table*} 
\centering 
\begin{center}
  \begin{tabular}[h]{m{4cm} m{6cm} m{5cm}}
  \hline 
  \textbf{Methods} & \textbf{Contributions} & \textbf{Limitations} \\ 
  \hline
  \cite{inbook} & Employs pre-trained word embeddings for Arabic ABSA & Only uses traditional machine learning methods as classifiers \\ 
  \cite{AlSmadi2017DeepRN} & Compares RNN and SVM for Arabic ABSA & The dataset is relatively small, does not use other deep learning models such as LSTM \\ 
  \cite{Alshammari2020AspectbasedSA} & Compares CNN and traditional machine learning methods for Arabic ABSA & Task-specific method, does not compare different deep learning methods \\ 
  \cite{AlDabet2021EnhancingAA} & Proposes a novel network architecture for different Arabic ABSA tasks & Task-specific method \\ 
  \cite{Abdelgwad2021ArabicAS} & Develops a BERT based model for Arabic ABSA & Does not present experimentation on other tasks \\ 
  \hline 
  \end{tabular}
  \end{center}
  \caption{\label{Table: arabic absa} Methods for Arabic aspect based sentiment analysis (ABSA).} 
\end{table*}
\end{comment}
% This is an appendix.
%\end{appendices}
\end{document}
