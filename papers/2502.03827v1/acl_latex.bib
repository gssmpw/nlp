% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@inproceedings{nabil-etal-2015-astd,
    title = "{ASTD}: {A}rabic Sentiment Tweets Dataset",
    author = "Nabil, Mahmoud  and
      Aly, Mohamed  and
      Atiya, Amir",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1299",
    doi = "10.18653/v1/D15-1299",
    pages = "2515--2519",
}

@inproceedings{aly-atiya-2013-labr,
    title = "{LABR}: A Large Scale {A}rabic Book Reviews Dataset",
    author = "Aly, Mohamed  and
      Atiya, Amir",
    editor = "Schuetze, Hinrich  and
      Fung, Pascale  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-2088",
    pages = "494--498",
}

@inproceedings{abu-farha-magdy-2020-arabic,
    title = "From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset",
    author = "Abu Farha, Ibrahim  and Magdy, Walid",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://www.aclweb.org/anthology/2020.osact-1.5",
    pages = "32--39",
    language = "English",
    ISBN = "979-10-95546-51-1",
}



@inproceedings{fang2024arsen,
title={{ArSen}-20: A New Benchmark for {A}rabic Sentiment Detection},
author={Yang Fang and Cheng Xu},
booktitle={5th Workshop on African Natural Language Processing},
year={2024},
url={https://openreview.net/forum?id=GgsRUF5kJt}}

@article{Elnagar2018hotelreview,
author="Elnagar, Ashraf and Khalifa, Yasmin S. and Einea, Anas",
editor="Shaalan, Khaled and Hassanien, Aboul Ella and Tolba, Fahmy",
title="Hotel Arabic-Reviews Dataset Construction for Sentiment Analysis Applications",
journal="Intelligent Natural Language Processing: Trends and Applications",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="35--52",
abstract="Arabic language suffers from the lack of available large datasets for machine learning and sentiment analysis applications. This work adds to the recently reported large dataset BRAD, which is the largest Book Reviews in Arabic Dataset. In this paper, we introduce HARD (Hotel Arabic-Reviews Dataset), the largest Book Reviews in Arabic Dataset for subjective sentiment analysis and machine language applications. HARD comprises of 490587 hotel reviews collected from the Booking.com website. Each record contains the review text in the Arabic language, the reviewer's rating on a scale of 1 to 10 stars, and other attributes about the hotel/reviewer. We make available the full unbalanced dataset as well as a balanced subset. To examine the datasets, we implement six popular classifiers using Modern Standard Arabic (MSA) as well as Dialectal Arabic (DA). We test the sentiment analyzers for polarity and rating classifications. Furthermore, we implement a polarity lexicon-based sentiment analyzer. The findings confirm the effectiveness of the classifiers and the datasets. Our core contribution is to make this benchmark-dataset available and accessible to the research community on Arabic language.",
isbn="978-3-319-67056-0",
doi="10.1007/978-3-319-67056-0_3",
url="https://doi.org/10.1007/978-3-319-67056-0_3"
}

@article{DBLP:journals/corr/abs-1906-01830,
  author       = {Ramy Baly and
                  Alaa Khaddaj and
                  Hazem M. Hajj and
                  Wassim El{-}Hajj and
                  Khaled Bashir Shaban},
  title        = {{ArSentD-LEV}: {A} Multi-Topic Corpus for Target-based Sentiment Analysis
                  in {A}rabic {L}evantine Tweets},
  journal      = {CoRR},
  volume       = {abs/1906.01830},
  year         = {2019},
  url          = {http://arxiv.org/abs/1906.01830},
  eprinttype    = {arXiv},
  eprint       = {1906.01830},
  timestamp    = {Thu, 13 Jun 2019 13:36:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1906-01830.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{dahou-etal-2016-word,
    title = "Word Embeddings and Convolutional Neural Network for {A}rabic Sentiment Classification",
    author = "Dahou, Abdelghani  and
      Xiong, Shengwu  and
      Zhou, Junwei  and
      Haddoud, Mohamed Houcine  and
      Duan, Pengfei",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1228",
    pages = "2418--2427",
    abstract = "With the development and the advancement of social networks, forums, blogs and online sales, a growing number of Arabs are expressing their opinions on the web. In this paper, a scheme of Arabic sentiment classification, which evaluates and detects the sentiment polarity from Arabic reviews and Arabic social media, is studied. We investigated in several architectures to build a quality neural word embeddings using a 3.4 billion words corpus from a collected 10 billion words web-crawled corpus. Moreover, a convolutional neural network trained on top of pre-trained Arabic word embeddings is used for sentiment classification to evaluate the quality of these word embeddings. The simulation results show that the proposed scheme outperforms the existed methods on 4 out of 5 balanced and unbalanced datasets.",
}
@inproceedings{medhaffar-etal-2017-sentiment,
    title = "Sentiment Analysis of {T}unisian Dialects: Linguistic Ressources and Experiments",
    author = "Medhaffar, Salima  and
      Bougares, Fethi  and
      Est{\`e}ve, Yannick  and
      Hadrich-Belguith, Lamia",
    editor = "Habash, Nizar  and
      Diab, Mona  and
      Darwish, Kareem  and
      El-Hajj, Wassim  and
      Al-Khalifa, Hend  and
      Bouamor, Houda  and
      Tomeh, Nadi  and
      El-Haj, Mahmoud  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Third {A}rabic Natural Language Processing Workshop",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1307",
    doi = "10.18653/v1/W17-1307",
    pages = "55--61",
    abstract = "Dialectal Arabic (DA) is significantly different from the Arabic language taught in schools and used in written communication and formal speech (broadcast news, religion, politics, etc.). There are many existing researches in the field of Arabic language Sentiment Analysis (SA); however, they are generally restricted to Modern Standard Arabic (MSA) or some dialects of economic or political interest. In this paper we are interested in the SA of the Tunisian Dialect. We utilize Machine Learning techniques to determine the polarity of comments written in Tunisian Dialect. First, we evaluate the SA systems performances with models trained using freely available MSA and Multi-dialectal data sets. We then collect and annotate a Tunisian Dialect corpus of 17.000 comments from Facebook. This corpus allows us a significant accuracy improvement compared to the best model trained on other Arabic dialects or MSA data. We believe that this first freely available corpus will be valuable to researchers working in the field of Tunisian Sentiment Analysis and similar areas.",
}

@article{Guellil2018SentiALGAC,
  title={{SentiALG}: Automated Corpus Annotation for {A}lgerian Sentiment Analysis},
  author={Imane Guellil and Ahsan Adeel and Faical Azouaou and Amir Hussain},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.05079},
  url={https://api.semanticscholar.org/CorpusID:52008929}
}
@inproceedings{antoun-etal-2020-arabert,
    title = "{A}ra{BERT}: Transformer-based Model for {A}rabic Language Understanding",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    editor = "Al-Khalifa, Hend  and
      Magdy, Walid  and
      Darwish, Kareem  and
      Elsayed, Tamer  and
      Mubarak, Hamdy",
    booktitle = "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resource Association",
    url = "https://aclanthology.org/2020.osact-1.2",
    pages = "9--15",
    abstract = "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on \url{https://github.com/aub-mind/araBERT} hoping to encourage research and applications for Arabic NLP.",
    language = "English",
    ISBN = "979-10-95546-51-1",
}

@inproceedings{alkaoud-syed-2020-importance,
    title = "On the Importance of Tokenization in {A}rabic Embedding Models",
    author = "Alkaoud, Mohamed  and
      Syed, Mairaj",
    editor = "Zitouni, Imed  and
      Abdul-Mageed, Muhammad  and
      Bouamor, Houda  and
      Bougares, Fethi  and
      El-Haj, Mahmoud  and
      Tomeh, Nadi  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Fifth Arabic Natural Language Processing Workshop",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wanlp-1.11",
    pages = "119--129",
    abstract = "Arabic, like other highly inflected languages, encodes a large amount of information in its morphology and word structure. In this work, we propose two embedding strategies that modify the tokenization phase of traditional word embedding models (Word2Vec) and contextual word embedding models (BERT) to take into account Arabic{'}s relatively complex morphology. In Word2Vec, we segment words into subwords during training time and then compose word-level representations from the subwords during test time. We train our embeddings on Arabic Wikipedia and show that they perform better than a Word2Vec model on multiple Arabic natural language processing datasets while being approximately 60{\%} smaller in size. Moreover, we showcase our embeddings{'} ability to produce accurate representations of some out-of-vocabulary words that were not encountered before. In BERT, we modify the tokenization layer of Google{'}s pretrained multilingual BERT model by incorporating information on morphology. By doing so, we achieve state of the art performance on two Arabic NLP datasets without pretraining.",
}

@inproceedings{abdul-mageed-etal-2021-arbert,
    title = "{ARBERT} {\&} {MARBERT}: Deep Bidirectional Transformers for {A}rabic",
    author = "Abdul-Mageed, Muhammad  and
      Elmadany, AbdelRahim  and
      Nagoudi, El Moatez Billah",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.551",
    doi = "10.18653/v1/2021.acl-long.551",
    pages = "7088--7105",
    abstract = "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size). Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.",
}

@inproceedings{alyafeai-ahmad-2021-arabic,
    title = "{A}rabic Compact Language Modelling for Resource Limited Devices",
    author = "Alyafeai, Zaid  and
      Ahmad, Irfan",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.6",
    pages = "53--59",
    abstract = "Natural language modelling has gained a lot of interest recently. The current state-of-the-art results are achieved by first training a very large language model and then fine-tuning it on multiple tasks. However, there is little work on smaller more compact language models for resource-limited devices or applications. Not to mention, how to efficiently train such models for a low-resource language like Arabic. In this paper, we investigate how such models can be trained in a compact way for Arabic. We also show how distillation and quantization can be applied to create even smaller models. Our experiments show that our largest model which is 2x smaller than the baseline can achieve better results on multiple tasks with 2x less data for pretraining.",
}

@article{Alyafeai2021EvaluatingVT,
  title={Evaluating Various Tokenizers for {A}rabic Text Classification},
  author={Zaid Alyafeai and Maged S. Al-Shaibani and Mustafa Ghaleb and Irfan Ahmad},
  journal={Neural Processing Letters},
  year={2021},
  volume={55},
  pages={2911-2933},
  url={https://api.semanticscholar.org/CorpusID:235421725}
}
@inproceedings{Atabuzzaman2023ArabicSA,
  title={Arabic Sentiment Analysis with Noisy Deep Explainable Model},
  author={Md. Atabuzzaman and Md Shajalal and Maksuda Bilkis Baby and Alexander Boden},
  booktitle={International Conference on Natural Language Processing and Information Retrieval},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:262462294}
}
@inproceedings{baly-etal-2017-characterization,
    title = "A Characterization Study of {A}rabic {T}witter Data with a Benchmarking for State-of-the-Art Opinion Mining Models",
    author = "Baly, Ramy  and
      Badaro, Gilbert  and
      El-Khoury, Georges  and
      Moukalled, Rawan  and
      Aoun, Rita  and
      Hajj, Hazem  and
      El-Hajj, Wassim  and
      Habash, Nizar  and
      Shaban, Khaled",
    editor = "Habash, Nizar  and
      Diab, Mona  and
      Darwish, Kareem  and
      El-Hajj, Wassim  and
      Al-Khalifa, Hend  and
      Bouamor, Houda  and
      Tomeh, Nadi  and
      El-Haj, Mahmoud  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Third {A}rabic Natural Language Processing Workshop",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1314",
    doi = "10.18653/v1/W17-1314",
    pages = "110--118",
    abstract = "Opinion mining in Arabic is a challenging task given the rich morphology of the language. The task becomes more challenging when it is applied to Twitter data, which contains additional sources of noise, such as the use of unstandardized dialectal variations, the nonconformation to grammatical rules, the use of Arabizi and code-switching, and the use of non-text objects such as images and URLs to express opinion. In this paper, we perform an analytical study to observe how such linguistic phenomena vary across different Arab regions. This study of Arabic Twitter characterization aims at providing better understanding of Arabic Tweets, and fostering advanced research on the topic. Furthermore, we explore the performance of the two schools of machine learning on Arabic Twitter, namely the feature engineering approach and the deep learning approach. We consider models that have achieved state-of-the-art performance for opinion mining in English. Results highlight the advantages of using deep learning-based models, and confirm the importance of using morphological abstractions to address Arabic{'}s complex morphology.",
}

@inproceedings{attia-etal-2018-multilingual,
    title = "Multilingual Multi-class Sentiment Classification Using Convolutional Neural Networks",
    author = "Attia, Mohammed  and
      Samih, Younes  and
      Elkahky, Ali  and
      Kallmeyer, Laura",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1101",
}

@inproceedings{eljundi-etal-2019-hulmona,
    title = "h{ULM}on{A}: The Universal Language Model in {A}rabic",
    author = "ElJundi, Obeida  and
      Antoun, Wissam  and
      El Droubi, Nour  and
      Hajj, Hazem  and
      El-Hajj, Wassim  and
      Shaban, Khaled",
    editor = "El-Hajj, Wassim  and
      Belguith, Lamia Hadrich  and
      Bougares, Fethi  and
      Magdy, Walid  and
      Zitouni, Imed  and
      Tomeh, Nadi  and
      El-Haj, Mahmoud  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4608",
    doi = "10.18653/v1/W19-4608",
    pages = "68--77",
    abstract = "Arabic is a complex language with limited resources which makes it challenging to produce accurate text classification tasks such as sentiment analysis. The utilization of transfer learning (TL) has recently shown promising results for advancing accuracy of text classification in English. TL models are pre-trained on large corpora, and then fine-tuned on task-specific datasets. In particular, universal language models (ULMs), such as recently developed BERT, have achieved state-of-the-art results in various NLP tasks in English. In this paper, we hypothesize that similar success can be achieved for Arabic. The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hULMonA - حلمنا meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multi-lingual BERT can also be used for Arabic. We then conduct a benchmark study to evaluate both ULM successes with Arabic sentiment analysis. Experiment results show that the developed hULMonA and multi-lingual ULM are able to generalize well to multiple Arabic data sets and achieve new state of the art results in Arabic Sentiment Analysis for some of the tested sets.",
}

@inproceedings{el-mekki-etal-2021-domain,
    title = "Domain Adaptation for {A}rabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding",
    author = "El Mekki, Abdellah  and
      El Mahdaouy, Abdelkader  and
      Berrada, Ismail  and
      Khoumsi, Ahmed",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.226",
    doi = "10.18653/v1/2021.naacl-main.226",
    pages = "2824--2837",
    abstract = "Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and cross-dialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8{\%} over the zero-shot transfer learning from BERT.",
}

@inproceedings{abu-kwaik-etal-2022-pre,
    title = "Pre-trained Models or Feature Engineering: The Case of Dialectal {A}rabic",
    author = "Abu Kwaik, Kathrein  and
      Chatzikyriakidis, Stergios  and
      Dobnik, Simon",
    editor = "Al-Khalifa, Hend  and
      Elsayed, Tamer  and
      Mubarak, Hamdy  and
      Al-Thubaity, Abdulmohsen  and
      Magdy, Walid  and
      Darwish, Kareem",
    booktitle = "Proceedings of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Qur'an QA and Fine-Grained Hate Speech Detection",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.osact-1.5",
    pages = "41--50",
    abstract = "The usage of social media platforms has resulted in the proliferation of work on Arabic Natural Language Processing (ANLP), including the development of resources. There is also an increased interest in processing Arabic dialects and a number of models and algorithms have been utilised for the purpose of Dialectal Arabic Natural Language Processing (DANLP). In this paper, we conduct a comparison study between some of the most well-known and most commonly used methods in NLP in order to test their performance on different corpora and two NLP tasks: Dialect Identification and Sentiment Analysis. In particular, we compare three general classes of models: a) traditional Machine Learning models with features, b) classic Deep Learning architectures (LSTMs) with pre-trained word embeddings and lastly c) different Bidirectional Encoder Representations from Transformers (BERT) models such as (Multilingual-BERT, Ara-BERT, and Twitter-Arabic-BERT). The results of the comparison show that using feature-based classification can still compete with BERT models in these dialectal Arabic contexts. The use of transformer models have the ability to outperform traditional Machine Learning approaches, depending on the type of text they have been trained on, in contrast to classic Deep Learning models like LSTMs which do not perform well on the tasks",
}

@article{Refai2022DataAU,
  title={Data Augmentation Using Transformers and Similarity Measures for Improving {A}rabic Text Classification},
  author={Dania Refai and Saleh Abu-Soud and Mohammad J. Abdel-Rahman},
  journal={IEEE Access},
  year={2022},
  volume={11},
  pages={132516-132531},
  url={https://api.semanticscholar.org/CorpusID:255186403}
}
@inproceedings{hengle-etal-2021-combining,
    title = "Combining Context-Free and Contextualized Representations for {A}rabic Sarcasm Detection and Sentiment Identification",
    author = "Hengle, Amey  and
      Kshirsagar, Atharva  and
      Desai, Shaily  and
      Marathe, Manisha",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.46",
    pages = "357--363",
    abstract = "Since their inception, transformer-based language models have led to impressive performance gains across multiple natural language processing tasks. For Arabic, the current state-of-the-art results on most datasets are achieved by the AraBERT language model. Notwithstanding these recent advancements, sarcasm and sentiment detection persist to be challenging tasks in Arabic, given the language{'}s rich morphology, linguistic disparity and dialectal variations. This paper proffers team SPPU-AASM{'}s submission for the WANLP ArSarcasm shared-task 2021, which centers around the sarcasm and sentiment polarity detection of Arabic tweets. The study proposes a hybrid model, combining sentence representations from AraBERT with static word vectors trained on Arabic social media corpora. The proposed system achieves a F1-sarcastic score of 0.62 and a F-PN score of 0.715 for the sarcasm and sentiment detection tasks, respectively. Simulation results show that the proposed system outperforms multiple existing approaches for both the tasks, suggesting that the amalgamation of context-free and context-dependent text representations can help capture complementary facets of word meaning in Arabic. The system ranked second and tenth in the respective sub-tasks of sarcasm detection and sentiment identification.",
}

@inproceedings{israeli-etal-2021-idc,
    title = "The {IDC} System for Sentiment Classification and Sarcasm Detection in {A}rabic",
    author = "Israeli, Abraham  and
      Nahum, Yotam  and
      Fine, Shai  and
      Bar, Kfir",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.48",
    pages = "370--375",
    abstract = "Sentiment classification and sarcasm detection attract a lot of attention by the NLP research community. However, solving these two problems in Arabic and on the basis of social network data (i.e., Twitter) is still of lower interest. In this paper we present designated solutions for sentiment classification and sarcasm detection tasks that were introduced as part of a shared task by Abu Farha et al. (2021). We adjust the existing state-of-the-art transformer pretrained models for our needs. In addition, we use a variety of machine-learning techniques such as down-sampling, augmentation, bagging, and usage of meta-features to improve the models performance. We achieve an F1-score of 0.75 over the sentiment classification problem where the F1-score is calculated over the positive and negative classes (the neutral class is not taken into account). We achieve an F1-score of 0.66 over the sarcasm detection problem where the F1-score is calculated over the sarcastic class only. In both cases, the above reported results are evaluated over the ArSarcasm-v2{--}an extended dataset of the ArSarcasm (Farha and Magdy, 2020) that was introduced as part of the shared task. This reflects an improvement to the state-of-the-art results in both tasks.",
}


@inproceedings{abu-farha-magdy-2021-benchmarking,
    title = "Benchmarking {T}ransformer-based Language Models for {A}rabic Sentiment and Sarcasm Detection",
    author = "Abu Farha, Ibrahim  and
      Magdy, Walid",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.3",
    pages = "21--31",
    abstract = "The introduction of transformer-based language models has been a revolutionary step for natural language processing (NLP) research. These models, such as BERT, GPT and ELECTRA, led to state-of-the-art performance in many NLP tasks. Most of these models were initially developed for English and other languages followed later. Recently, several Arabic-specific models started emerging. However, there are limited direct comparisons between these models. In this paper, we evaluate the performance of 24 of these models on Arabic sentiment and sarcasm detection. Our results show that the models achieving the best performance are those that are trained on only Arabic data, including dialectal Arabic, and use a larger number of parameters, such as the recently released MARBERT. However, we noticed that AraELECTRA is one of the top performing models while being much more efficient in its computational cost. Finally, the experiments on AraGPT2 variants showed low performance compared to BERT models, which indicates that it might not be suitable for classification tasks.",
}

@inproceedings{khondaker-etal-2022-benchmark,
    title = "A Benchmark Study of Contrastive Learning for {A}rabic Social Meaning",
    author = "Khondaker, Md Tawkat Islam  and
      Nagoudi, El Moatez Billah  and
      Elmadany, AbdelRahim  and
      Abdul-Mageed, Muhammad  and
      Lakshmanan, V.S., Laks",
    editor = "Bouamor, Houda  and
      Al-Khalifa, Hend  and
      Darwish, Kareem  and
      Rambow, Owen  and
      Bougares, Fethi  and
      Abdelali, Ahmed  and
      Tomeh, Nadi  and
      Khalifa, Salam  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Seventh Arabic Natural Language Processing Workshop (WANLP)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wanlp-1.7",
    doi = "10.18653/v1/2022.wanlp-1.7",
    pages = "63--75",
    abstract = "Contrastive learning (CL) has brought significant progress to various NLP tasks. Despite such a progress, CL has not been applied to Arabic NLP. Nor is it clear how much benefits it could bring to particular classes of tasks such as social meaning (e.g., sentiment analysis, dialect identification, hate speech detection). In this work, we present a comprehensive benchmark study of state-of-the-art supervised CL methods on a wide array of Arabic social meaning tasks. Through an extensive empirical analysis, we show that CL methods outperform vanilla finetuning on most of the tasks. We also show that CL can be data efficient and quantify this efficiency, demonstrating the promise of these methods in low-resource settings vis-a-vis the particular downstream tasks (especially label granularity).",
}

@inproceedings{el-mahdaouy-etal-2021-deep,
    title = "Deep Multi-Task Model for Sarcasm Detection and Sentiment Analysis in {A}rabic Language",
    author = "El Mahdaouy, Abdelkader  and
      El Mekki, Abdellah  and
      Essefar, Kabil  and
      El Mamoun, Nabil  and
      Berrada, Ismail  and
      Khoumsi, Ahmed",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.42",
    pages = "334--339",
    abstract = "The prominence of figurative language devices, such as sarcasm and irony, poses serious challenges for Arabic Sentiment Analysis (SA). While previous research works tackle SA and sarcasm detection separately, this paper introduces an end-to-end deep Multi-Task Learning (MTL) model, allowing knowledge interaction between the two tasks. Our MTL model{'}s architecture consists of a Bidirectional Encoder Representation from Transformers (BERT) model, a multi-task attention interaction module, and two task classifiers. The overall obtained results show that our proposed model outperforms its single-task and MTL counterparts on both sarcasm and sentiment detection subtasks.",
}

@inproceedings{faraj-etal-2021-sarcasmdet,
    title = "{S}arcasm{D}et at Sarcasm Detection Task 2021 in {A}rabic using {A}ra{BERT} Pretrained Model",
    author = "Faraj, Dalya  and
      Faraj, Dalya  and
      Abdullah, Malak",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.44",
    pages = "345--350",
    abstract = "This paper presents one of the top five winning solutions for the Shared Task on Sarcasm and Sentiment Detection in Arabic (Subtask-1 Sarcasm Detection). The goal of the task is to identify whether a tweet is sarcastic or not. Our solution has been developed using ensemble technique with AraBERT pre-trained model. We describe the architecture of the submitted solution in the shared task. We also provide the experiments and the hyperparameter tuning that lead to this result. Besides, we discuss and analyze the results by comparing all the models that we trained or tested to achieve a better score in a table design. Our model is ranked fifth out of 27 teams with an F1 score of 0.5985. It is worth mentioning that our model achieved the highest accuracy score of 0.7830",
}

@inproceedings{kaseb-farouk-2022-saids,
    title = "{SAIDS}: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm",
    author = "Kaseb, Abdelrahman  and
      Farouk, Mona",
    editor = "Bouamor, Houda  and
      Al-Khalifa, Hend  and
      Darwish, Kareem  and
      Rambow, Owen  and
      Bougares, Fethi  and
      Abdelali, Ahmed  and
      Tomeh, Nadi  and
      Khalifa, Salam  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Seventh Arabic Natural Language Processing Workshop (WANLP)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wanlp-1.3",
    doi = "10.18653/v1/2022.wanlp-1.3",
    pages = "22--30",
    abstract = "Sentiment analysis becomes an essential part of every social network, as it enables decision-makers to know more about users{'} opinions in almost all life aspects. Despite its importance, there are multiple issues it encounters like the sentiment of the sarcastic text which is one of the main challenges of sentiment analysis. This paper tackles this challenge by introducing a novel system (SAIDS) that predicts the sentiment, sarcasm and dialect of Arabic tweets. SAIDS uses its prediction of sarcasm and dialect as known information to predict the sentiment. It uses MARBERT as a language model to generate sentence embedding, then passes it to the sarcasm and dialect models, and then the outputs of the three models are concatenated and passed to the sentiment analysis model. Multiple system design setups were experimented with and reported. SAIDS was applied to the ArSarcasm-v2 dataset where it outperforms the state-of-the-art model for the sentiment analysis task. By training all tasks together, SAIDS achieves results of 75.98 FPN, 59.09 F1-score and 71.13 F1-score for sentiment analysis, sarcasm detection, and dialect identification respectively. The system design can be used to enhance the performance of any task which is dependent on other tasks.",
}

@inbook{inbook,
author = {Ashi, Mohammed and Siddiqui, Muazzam and Nadeem, Farrukh},
year = {2019},
month = {01},
pages = {241-251},
title = {Pre-trained Word Embeddings for Arabic Aspect-Based Sentiment Analysis of Airline Tweets},
publisher = {Springer},
isbn = {978-3-319-99009-5},
doi = {10.1007/978-3-319-99010-1_22}
}
@article{AlSmadi2017DeepRN,
  title={Deep {Recurrent Neural Network vs. Support Vector Machine} for aspect-based sentiment analysis of {A}rabic hotels' reviews},
  author={Mohammad Al-Smadi and Omar Qawasmeh and Mahmoud Al-Ayyoub and Yaser Jararweh and Brij Bhooshan Gupta},
  journal={J. Comput. Sci.},
  year={2017},
  volume={27},
  pages={386-393},
  url={https://api.semanticscholar.org/CorpusID:51918752}
}

@article{Alshammari2020AspectbasedSA,
  title={Aspect-based Sentiment Analysis for {A}rabic Content in Social Media},
  author={Norah Fahad Alshammari and Amal Almansour},
  journal={2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)},
  year={2020},
  pages={1-6},
  url={https://api.semanticscholar.org/CorpusID:221388429}
}
@article{AlDabet2021EnhancingAA,
  title={Enhancing {A}rabic aspect-based sentiment analysis using deep learning models},
  author={Saja Al-Dabet and Sara Tedmori and Mohammad Al-Smadi},
  journal={Comput. Speech Lang.},
  year={2021},
  volume={69},
  pages={101224},
  url={https://api.semanticscholar.org/CorpusID:234800765}
}
@article{Abdelgwad2021ArabicAS,
  title={Arabic aspect sentiment polarity classification using {BERT}},
  author={Mohammed M. Abdelgwad and Taysir Hassan A. Soliman and Ahmed I. Taloba},
  journal={Journal of Big Data},
  year={2021},
  volume={9},
  pages={1-15},
  url={https://api.semanticscholar.org/CorpusID:254152261}
}
@article{Talafha2021SarcasmDA,
  title={Sarcasm Detection and Quantification in {A}rabic Tweets},
  author={Bashar Talafha and Muhy Eddin Za'ter and Samer Suleiman and Mahmoud Al-Ayyoub and Mohammed N. Al-Kabi},
  journal={2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)},
  year={2021},
  pages={1121-1125},
  url={https://api.semanticscholar.org/CorpusID:236881074}
}
@inproceedings{zeng-li-2022-survey,
    title = "A Survey in Automatic Irony Processing: Linguistic, Cognitive, and Multi-{X} Perspectives",
    author = "Zeng, Qingcheng  and
      Li, An-Ran",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.69",
    pages = "824--836",
    abstract = "Irony is a ubiquitous figurative language in daily communication. Previously, many researchers have approached irony from linguistic, cognitive science, and computational aspects. Recently, some progress have been witnessed in automatic irony processing due to the rapid development in deep neural models in natural language processing (NLP). In this paper, we will provide a comprehensive overview of computational irony, insights from linguisic theory and cognitive science, as well as its interactions with downstream NLP tasks and newly proposed multi-X irony processing perspectives.",
}

@article{https://doi.org/10.1002/asi.23716,
author = {Varathan, Kasturi Dewi and Giachanou, Anastasia and Crestani, Fabio},
title = {Comparative opinion mining: A review},
journal = {Journal of the Association for Information Science and Technology},
volume = {68},
number = {4},
pages = {811-829},
doi = {https://doi.org/10.1002/asi.23716},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/asi.23716},
eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/asi.23716},
abstract = {Opinion mining refers to the use of natural language processing, text analysis, and computational linguistics to identify and extract subjective information in textual material. Opinion mining, also known as sentiment analysis, has received a lot of attention in recent times, as it provides a number of tools to analyze public opinion on a number of different topics. Comparative opinion mining is a subfield of opinion mining which deals with identifying and extracting information that is expressed in a comparative form (e.g., “paper X is better than the Y”). Comparative opinion mining plays a very important role when one tries to evaluate something because it provides a reference point for the comparison. This paper provides a review of the area of comparative opinion mining. It is the first review that cover specifically this topic as all previous reviews dealt mostly with general opinion mining. This survey covers comparative opinion mining from two different angles. One from the perspective of techniques and the other from the perspective of comparative opinion elements. It also incorporates preprocessing tools as well as data set that were used by past researchers that can be useful to future researchers in the field of comparative opinion mining.},
year = {2017}
}
@book{sentiment-analysis-book,
author = {Liu, Bing},
year = {2015},
month = {06},
pages = {1-367},
title = {Sentiment Analysis: Mining Opinions, Sentiments, and Emotions},
publisher = {Cambridge University Press},
isbn = {9781107017894},
doi = {10.1017/CBO9781139084789}
}
@article{Zhang2023SentimentAI,
  title={Sentiment Analysis in the Era of Large Language Models: A Reality Check},
  author={Wenxuan Zhang and Yue Deng and Bing-Quan Liu and Sinno Jialin Pan and Lidong Bing},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.15005},
  url={https://api.semanticscholar.org/CorpusID:258866189}
}
@article{Zhang2022ASO,
  title={A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges},
  author={Wenxuan Zhang and Xin Li and Yang Deng and Lidong Bing and Wai Lam},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  volume={35},
  pages={11019-11038},
  url={https://api.semanticscholar.org/CorpusID:247218352}
}
@article{Almurqren2024ArabicTS,
  title={Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with Wider Topic Analysis},
  author={Latifah Almurqren and Ryan Hodgson and A Ioana Cristea},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.01921},
  url={https://api.semanticscholar.org/CorpusID:268248519}
}
@article{ALAYYOUB2019320,
title = {A comprehensive survey of {A}rabic sentiment analysis},
journal = {Information Processing and Management},
volume = {56},
number = {2},
pages = {320-342},
year = {2019},
note = {Advance Arabic Natural Language Processing (ANLP) and its Applications},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316306689},
author = {Mahmoud Al-Ayyoub and Abed Allah Khamaiseh and Yaser Jararweh and Mohammed N. Al-Kabi},
keywords = {Arabic text processing, Sentiment analysis (SA), Binary/ternary SA, Multi-Way SA, Aspect-based SA},
abstract = {Sentiment analysis (SA) is a continuing field of research that lies at the intersection of many fields such as data mining, natural language processing and machine learning. It is concerned with the automatic extraction of opinions conveyed in a certain text. Due to its vast applications, many studies have been conducted in the area of SA especially on English texts, while other languages such as Arabic received less attention. This survey presents a comprehensive overview of the works done so far on Arabic SA (ASA). The survey groups published papers based on the SA-related problems they address and tries to identify the gaps in the current literature laying foundation for future studies in this field.}
}
@inproceedings{maas-etal-2011-learning,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    editor = "Lin, Dekang  and
      Matsumoto, Yuji  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}


@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@inproceedings{cai-etal-2021-aspect,
    title = "Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions",
    author = "Cai, Hongjie  and
      Xia, Rui  and
      Yu, Jianfei",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.29",
    doi = "10.18653/v1/2021.acl-long.29",
    pages = "340--350",
    abstract = "Product reviews contain a large number of implicit aspects and implicit opinions. However, most of the existing studies in aspect-based sentiment analysis ignored this problem. In this work, we introduce a new task, named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions. We furthermore construct two new datasets, Restaurant-ACOS and Laptop-ACOS, for this new task, both of which contain the annotations of not only aspect-category-opinion-sentiment quadruples but also implicit aspects and opinions. The former is an extension of the SemEval Restaurant dataset; the latter is a newly collected and annotated Laptop dataset, twice the size of the SemEval Laptop dataset. We finally benchmark the task with four baseline systems. Experiments demonstrate the feasibility of the new task and its effectiveness in extracting and describing implicit aspects and implicit opinions. The two datasets and source code of four systems are publicly released at \url{https://github.com/NUSTM/ACOS}.",
}
@article{Liu2021PretrainPA,
  title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  journal={ACM Computing Surveys},
  year={2021},
  volume={55},
  pages={1 - 35},
  url={https://api.semanticscholar.org/CorpusID:236493269}
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}
@article{Wei2022EmergentAO,
  title={Emergent Abilities of Large Language Models},
  author={Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed Huai-hsin Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.07682},
  url={https://api.semanticscholar.org/CorpusID:249674500}
}
@inproceedings{huang-chang-2023-towards,
    title = "Towards Reasoning in Large Language Models: A Survey",
    author = "Huang, Jie  and
      Chang, Kevin Chen-Chuan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.67",
    doi = "10.18653/v1/2023.findings-acl.67",
    pages = "1049--1065",
    abstract = "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",
}

@article{Haouhat2023TowardsAM,
  title={Towards {A}rabic Multimodal Dataset for Sentiment Analysis},
  author={Abdelhamid Haouhat and Slimane Bellaouar and Attia Nehar and Hadda Cherroun},
  journal={2023 Fourth International Conference on Intelligent Data Science Technologies and Applications (IDSTA)},
  year={2023},
  pages={126-133},
  url={https://api.semanticscholar.org/CorpusID:259138827}
}
@inproceedings{abu-farha-etal-2021-overview,
    title = "Overview of the {WANLP} 2021 Shared Task on Sarcasm and Sentiment Detection in {A}rabic",
    author = "Abu Farha, Ibrahim  and
      Zaghouani, Wajdi  and
      Magdy, Walid",
    editor = "Habash, Nizar  and
      Bouamor, Houda  and
      Hajj, Hazem  and
      Magdy, Walid  and
      Zaghouani, Wajdi  and
      Bougares, Fethi  and
      Tomeh, Nadi  and
      Abu Farha, Ibrahim  and
      Touileb, Samia",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wanlp-1.36",
    pages = "296--305",
    abstract = "This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. The shared task has two subtasks: sarcasm detection (subtask 1) and sentiment analysis (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as sentiment analysis. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for sarcasm, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F1-PN respectively.",
}

@article{Lai2023MultimodalSA,
  title={Multimodal Sentiment Analysis: A Survey},
  author={Songning Lai and Hao Xu and Xifeng Hu and Zhaoxia Ren and Zhi Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.07611},
  url={https://api.semanticscholar.org/CorpusID:258676130}
}
@article{Huang2023AceGPTLL,
  title={{AceGPT}, Localizing Large Language Models in {A}rabic},
  author={Huang Huang and Fei Yu and Jianqing Zhu and Xuening Sun and Hao Cheng and Dingjie Song and Zhihong Chen and Abdulmohsen Alharthi and Bang An and Ziche Liu and Zhiyi Zhang and Junying Chen and Jianquan Li and Benyou Wang and Lian Zhang and Ruoyu Sun and Xiang Wan and Haizhou Li and Jinchao Xu},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.12053},
  url={https://api.semanticscholar.org/CorpusID:262084244}
}
@INPROCEEDINGS{arabic_survey_subjectivity,
  author={Al-Twairesh, Nora and Al-Khalifa, Hend and Al-Salman, AbdulMalik},
  booktitle={2014 IEEE/ACS 11th International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Subjectivity and sentiment analysis of {A}rabic: {T}rends and challenges}, 
  year={2014},
  volume={},
  number={},
  pages={148-155},
  keywords={Support vector machines;Accuracy;Niobium;Sentiment analysis;Feature extraction;Semantics;Twitter;Subjectivity and Sentiment Analysis;Arabic;Survey;Challenges},
  doi={10.1109/AICCSA.2014.7073192}}

@INPROCEEDINGS{arabic_survey_sc,
  author={Biltawi, Mariam and Etaiwi, Wael and Tedmori, Sara and Hudaib, Amjad and Awajan, Arafat},
  booktitle={2016 7th International Conference on Information and Communication Systems (ICICS)}, 
  title={Sentiment classification techniques for {A}rabic language: A survey}, 
  year={2016},
  volume={},
  number={},
  pages={339-346},
  keywords={Classification algorithms;Sentiment analysis;Dictionaries;Niobium;Support vector machines;Standards;Semantics;Sentiment;Opinion;Classification;Machine learning;Lexicon},
  doi={10.1109/IACS.2016.7476075}}

@INPROCEEDINGS{arabic_survey_asa_social_media,
  author={Abdelhameed, Huda Jamal and Muñoz-Hern'andez, Susana},
  booktitle={2017 Joint International Conference on Information and Communication Technologies for Education and Training and International Conference on Computing in Arabic (ICCA-TICET)}, 
  title={Emotion and opinion retrieval from social media in {A}rabic language: Survey}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  keywords={Tools;Sentiment analysis;Support vector machines;Feature extraction;Classification algorithms;Facebook;Arabic Classification;Opinion Mining;Sentiment Analysis;Social Media},
  doi={10.1109/ICCA-TICET.2017.8095291}}

@ARTICLE{arabic_survey_asa,
  author={Abo, Mohamed Elhag Mohamed and Raj, Ram Gopal and Qazi, Atika},
  journal={IEEE Access}, 
  title={A Review on {A}rabic Sentiment Analysis: State-of-the-Art, Taxonomy and Open Research Challenges}, 
  year={2019},
  volume={7},
  number={},
  pages={162008-162024},
  keywords={Sentiment analysis;Social networking (online);Machine learning;Machine learning algorithms;Supervised learning;Standards;Classification algorithms;Sentiment analysis;machine learning;supervised learning;support vector machines;modern standard Arabic;dialect Arabic},
  doi={10.1109/ACCESS.2019.2951530}}

@ARTICLE{arabic_survey_sarcasm_detection,
  author={Rahma, Alaa and Azab, Shahira Shaaban and Mohammed, Ammar},
  journal={IEEE Access}, 
  title={A Comprehensive Survey on {A}rabic Sarcasm Detection: Approaches, Challenges and Future Trends}, 
  year={2023},
  volume={11},
  number={},
  pages={18261-18280},
  keywords={Artificial intelligence;Sentiment analysis;Social networking (online);Feature extraction;Linguistics;Market research;Machine learning;Natural language processing;Deep learning;Artificial intelligence (AI);Arabic sarcasm detection;deep learning (DL);machine learning (ML);natural language processing (NLP);sentiment analysis (SA)},
  doi={10.1109/ACCESS.2023.3247427}}

@ARTICLE{asa_survey,
  author={Katat, Souha Al and Zaki, Chamseddine and Hazimeh, Hussein and Bitar, Ibrahim and Angarita, Rafael and Trojman, Lionel},
  journal={IEEE Transactions on Big Data}, 
  title={Natural Language Processing for Arabic Sentiment Analysis: A Systematic Literature Review}, 
  year={2024},
  volume={},
  number={},
  pages={1-18},
  keywords={Sentiment analysis;Feature extraction;Deep learning;Systematics;Electronic mail;Task analysis;Surveys;Arabic Sentiment Analysis;Deep Learning;Machine Learning;Transformers;Lexicon;Feature Extraction;Word Embedding},
  doi={10.1109/TBDATA.2024.3366083}}

@inproceedings{Badaro2014ALS,
  title={A Large Scale Arabic Sentiment Lexicon for Arabic Opinion Mining},
  author={Gilbert Badaro and R. Baly and Hazem M. Hajj and Nizar Habash and Wassim El-Hajj},
  booktitle={ANLP@EMNLP},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:16203230}
}
@inproceedings{Heikal2018SentimentAO,
  title={Sentiment Analysis of {A}rabic Tweets using Deep Learning},
  author={Maha Heikal and Marwan Torki and Nagwa M. El-Makky},
  booktitle={International Conference on Arabic Computational Linguistics},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:57171094}
}
@INPROCEEDINGS{combine_lexicon,
  author={Neshan, Seydeh Akram Saadat and Akbari, Reza},
  booktitle={2020 6th International Conference on Web Research (ICWR)}, 
  title={A Combination of Machine Learning and Lexicon Based Techniques for Sentiment Analysis}, 
  year={2020},
  volume={},
  number={},
  pages={8-14},
  keywords={Sentiment analysis;Dictionaries;Computational modeling;Machine learning;Information retrieval;sentiment analysis;classification;opinion mining},
  doi={10.1109/ICWR49608.2020.9122298}}
@inproceedings{arzouki2018arsenl,
  title={ArSenL: A Semantic Lexicon for Sentiment Analysis of Arabic Dialects},
  author={Arzouki, Abdullah and Raluca, Mihalcea},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={--},
  year={2018},
  organization={Association for Computational Linguistics}
}
@inproceedings{baly2018hybrid,
  author={Baly, R. and others},
  title={A Hybrid Approach for Arabic Sentiment Analysis},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2018},
  organization={Association for Computational Linguistics}
}
@article{alhindi2021domain,
  author={Alhindi, A. and others},
  title={Domain-Specific Sentiment Analysis Using Lexicon and Deep Learning},
  journal={Journal of Information Science},
  year={2021}
}
@article{zahran2022interpretable,
  author={Zahran, M. and others},
  title={Interpretable Sentiment Analysis Using Lexicons and Deep Learning},
  journal={Journal of Natural Language Engineering},
  year={2022}
}
@book{habash2010introduction,
  author={Habash, Nizar},
  title={Introduction to Arabic Natural Language Processing},
  publisher={Morgan \& Claypool Publishers},
  year={2010}
}
@inproceedings{elbeltagy2013open,
  author={El-Beltagy, Samhaa R. and Ali, Ahmed},
  title={Open issues in the sentiment analysis of {A}rabic social media: a case study},
  booktitle={2013 IEEE International Conference on Innovations in Information Technology (IIT)},
  pages={215--220},
  year={2013},
  organization={IEEE}
}
@article{al2018arabic,
  author={Al-Twairesh, Najla and others},
  title={Arabic Sentiment Analysis: A Survey},
  journal={Social Network Analysis and Mining},
  volume={8},
  number={1},
  pages={15},
  year={2018},
  publisher={Springer}
}
@inproceedings{snow2008cheap,
  author={Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng, Andrew Y.},
  title={Cheap and fast--but is it good?: evaluating non-expert annotations for natural language tasks},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={254--263},
  year={2008},
}
@inproceedings{kobayashi2018contextual,
  author={Kobayashi, Satoru},
  title={Contextual Data Augmentation for Natural Language Processing},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1008--1018},
  year={2018},
}
@inproceedings{meyer2014wiktionary,
  author={Meyer, Christian M. and Gurevych, Iryna},
  title={Wiktionary: A new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={9--16},
  year={2014},
}
@article{cambria2017sentiment,
  author={Cambria, Erik and others},
  title={Sentiment Analysis Is a Big Suitcase},
  journal={IEEE Intelligent Systems},
  volume={32},
  number={6},
  pages={74--80},
  year={2017},
}
@inproceedings{pasha2014madamira,
  author={Pasha, Arfath and others},
  title={MADAMIRA: A fast, comprehensive tool for morphological analysis and disambiguation of Arabic},
  booktitle={Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC)},
  year={2014},
}
@inproceedings{habash2012calima,
  author={Habash, Nizar and others},
  title={The CALIMA-Star Dialectal Arabic Morphological Analyzer/Generator},
  booktitle={Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT)},
  year={2012},
}
@inproceedings{alqahtani2019diacritics,
  author={Alqahtani, Manal and others},
  title={Diacritics Restoration in Arabic Text for Detecting Sentiment},
  booktitle={Proceedings of the 4th International Conference on Natural Language and Speech Processing (ICNLSP)},
  year={2019},
}
@inproceedings{kobayashi-2018-contextual,
    title = "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations",
    author = "Kobayashi, Sosuke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2072",
    doi = "10.18653/v1/N18-2072",
    pages = "452--457",
    abstract = "We propose a novel data augmentation for labeled sentences called contextual augmentation. We assume an invariance that sentences are natural even if the words in the sentences are replaced with other words with paradigmatic relations. We stochastically replace words with other words that are predicted by a bi-directional language model at the word positions. Words predicted according to a context are numerous but appropriate for the augmentation of the original words. Furthermore, we retrofit a language model with a label-conditional architecture, which allows the model to augment sentences without breaking the label-compatibility. Through the experiments for six various different text classification tasks, we demonstrate that the proposed method improves classifiers based on the convolutional or recurrent neural networks.",
}

@inproceedings{Meyer2012WiktionaryAN,
  title={Wiktionary: a new rival for expert-built lexicons? Exploring the possibilities of collaborative lexicography},
  author={Christian M. Meyer and Iryna Gurevych},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:34671014}
}
@inproceedings{taji-etal-2018-arabic,
    title = "An {A}rabic Morphological Analyzer and Generator with Copious Features",
    author = "Taji, Dima  and
      Khalifa, Salam  and
      Obeid, Ossama  and
      Eryani, Fadhl  and
      Habash, Nizar",
    editor = "Kuebler, Sandra  and
      Nicolai, Garrett",
    booktitle = "Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5816",
    doi = "10.18653/v1/W18-5816",
    pages = "140--150",
    abstract = "We introduce CALIMA-Star, a very rich Arabic morphological analyzer and generator that provides functional and form-based morphological features as well as built-in tokenization, phonological representation, lexical rationality and much more. This tool includes a fast engine that can be easily integrated into other systems, as well as an easy-to-use API and a web interface. CALIMA-Star also supports morphological reinflection. We evaluate CALIMA-Star against four commonly used analyzers for Arabic in terms of speed and morphological content.",
}

@inproceedings{alqahtani-etal-2020-multitask,
    title = "A Multitask Learning Approach for Diacritic Restoration",
    author = "Alqahtani, Sawsan  and
      Mishra, Ajay  and
      Diab, Mona",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.732",
    doi = "10.18653/v1/2020.acl-main.732",
    pages = "8238--8247",
    abstract = "In many languages like Arabic, diacritics are used to specify pronunciations as well as meanings. Such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word. This results in a more ambiguous text making computational processing on such text more difficult. Diacritic restoration is the task of restoring missing diacritics in the written text. Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data, but presumably lose useful information at the word level. Thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic restoration with related NLP problems namely word segmentation, part-of-speech tagging, and syntactic diacritization. We use Arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling. Our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and/or a lot more data (e.g. dialectal data).",
}

@inproceedings{Zhang2011CombiningLA,
  title={Combining lexicon-based and learning-based methods for twitter sentiment analysis},
  booktitle={HP Labs Technical Reports},
  author={Lei Zhang and Riddhiman Ghosh and Mohamed Dekhil and Meichun Hsu and B. Liu},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:16228540}
}

@inproceedings{badaro-etal-2014-large,
    title = "A Large Scale {A}rabic Sentiment Lexicon for {A}rabic Opinion Mining",
    author = "Badaro, Gilbert  and
      Baly, Ramy  and
      Hajj, Hazem  and
      Habash, Nizar  and
      El-Hajj, Wassim",
    editor = "Habash, Nizar  and
      Vogel, Stephan",
    booktitle = "Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3623",
    doi = "10.3115/v1/W14-3623",
    pages = "165--173",
}

@article{senti-lexicon,
author = {Tareq Al-Moslmi;Mohammed Albared;Adel Al-Shabi;Nazlia Omar;Salwani Abdullah;},
title = {Arabic senti-lexicon: Constructing publicly available language resources for Arabic sentiment analysis},
journal = {Journal of Information Science},
volume = {44},
number = {3},
pages = {345-362},
year = {2018},
doi = {10.1177/0165551516683908},
 URL = {
       http://dx.doi.org/10.1177/0165551516683908
},
 eprint = {
          http://dx.doi.org/10.1177/0165551516683908
},
abstract = {Sentiment analysis is held to be one of the highly dynamic recent research fields in Natural Language Processing, facilitated by the quickly growing volume of Web opinion data. Most of the approaches in this field are focused on English due to the lack of sentiment resources in other languages such as the Arabic language and its large variety of dialects. In most sentiment analysis applications, good sentiment resources play a critical role. Based on that, in this article, several publicly available sentiment analysis resources for Arabic are introduced. This article introduces the Arabic senti-lexicon, a list of 3880 positive and negative synsets annotated with their part of speech, polarity scores, dialects synsets and inflected forms. This article also presents a Multi-domain Arabic Sentiment Corpus (MASC) with a size of 8860 positive and negative reviews from different domains. In this article, an in-depth study has been conducted on five types of feature sets for exploiting effective features and investigating their effect on performance of Arabic sentiment analysis. The aim is to assess the quality of the developed language resources and to integrate different feature sets and classification algorithms to synthesise a more accurate sentiment analysis method. The Arabic senti-lexicon is used for generating feature vectors. Five well-known machine learning algorithms: na茂ve Bayes, k-nearest neighbours, support vector machines (SVMs), logistic linear regression and neural network are employed as base-classifiers for each of the feature sets. A wide range of comparative experiments on standard Arabic data sets were conducted, discussion is presented and conclusions are drawn. The experimental results show that the Arabic senti-lexicon is a very useful resource for Arabic sentiment analysis. Moreover, results show that classifiers which are trained on feature vectors derived from the corpus using the Arabic sentiment lexicon are more accurate than classifiers trained using the raw corpus.},
}
@inproceedings{AlHoraibi2016SentimentAO,
  title={Sentiment analysis of {A}rabic tweets using text mining techniques},
  author={Lamia Al-Horaibi and Muhammad Badruddin Khan},
  booktitle={International Workshop on Pattern Recognition, 2016},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:59094364}
}
@INPROCEEDINGS{svm_sa,
  author={Duwairi, Rehab M. and Alfaqeh, Mosab and Wardat, Mohammad and Alrabadi, Areen},
  booktitle={2016 7th International Conference on Information and Communication Systems (ICICS)}, 
  title={Sentiment analysis for {A}rabizi text}, 
  year={2016},
  volume={},
  number={},
  pages={127-132},
  keywords={Support vector machines;Niobium;Crowdsourcing;Filtering;Sentiment analysis;Supervised learning;Writing;Sentiment analysis;polarity classification;supervised learning;Arabic tweets;Arabizi},
  doi={10.1109/IACS.2016.7476098}}
@article{Altawaier2016ComparisonOM,
  title={Comparison of machine learning approaches on Arabic twitter sentiment analysis},
  author={Merfat.M. Altawaier and Sabrina Tiun},
  journal={International Journal on Advanced Science, Engineering and Information Technology},
  year={2016},
  volume={6},
  pages={1067-1073},
  url={https://api.semanticscholar.org/CorpusID:55801709}
}
@INPROCEEDINGS{bag-of-words,
  author={Qader, Wisam A. and Ameen, Musa M. and Ahmed, Bilal I.},
  booktitle={2019 International Engineering Conference (IEC)}, 
  title={An Overview of {Bag of Words}: Importance, Implementation, Applications, and Challenges}, 
  year={2019},
  volume={},
  number={},
  pages={200-204},
  keywords={Bag of Words;Image Classification;Text Classification;Visual Scene Classification},
  doi={10.1109/IEC47844.2019.8950616}}
@Inbook{tf-idf,
editor="Sammut, Claude
and Webb, Geoffrey I.",
title="TF--IDF",
bookTitle="Encyclopedia of Machine Learning",
year="2010",
publisher="Springer US",
address="Boston, MA",
pages="986--987",
isbn="978-0-387-30164-8",
doi="10.1007/978-0-387-30164-8_832",
url="https://doi.org/10.1007/978-0-387-30164-8_832"
}
@article{Almeida2019WordEA,
  title={Word Embeddings: A Survey},
  author={Felipe Almeida and Geraldo Bonorino Xex{\'e}o},
  journal={ArXiv},
  year={2019},
  volume={abs/1901.09069},
  url={https://api.semanticscholar.org/CorpusID:59316955}
}
@article{Xiang2021LexicalDA,
  title={Lexical data augmentation for sentiment analysis},
  author={Rong Xiang and Emmanuele Chersoni and Qin Lu and Chu-Ren Huang and Wenjie Li and Yunfei Long},
  journal={Journal of the Association for Information Science and Technology},
  year={2021},
  volume={72},
  pages={1432 - 1447},
  url={https://api.semanticscholar.org/CorpusID:237790029}
}
@article{lexicon_augmentation,
title = {Punctuation and lexicon aid representation: A hybrid model for short text sentiment analysis on social media platform},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {3},
pages = {102010},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102010},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824000995},
author = {Zhenyu Li and Zongfeng Zou},
keywords = {Sentiment analysis, Short text classification, BERT representation, Attention mechanism, Social media mining},
abstract = {Sentiment analysis measures user experience on social media. With the emergence of pre-trained models, text classification tasks have become homogeneous, without a significant improvement in accuracy. Therefore, we present a hybrid model called PLASA to classify the sentiment polarity of short comments, particularly barrages. PLASA introduces a collaborative attention module that integrates information about relative position and knowledge from summarized lexicons to better adjust the relationship between word representations. Our model is evaluated using three new curated sentiment analysis datasets: SentiTikTok-2023 (4613 items), SentiBilibili-2023 (7755 items), and SentiWeibo-2023 (5614 items). Although the comment length varies across datasets, all maintain a consistent punctuation percentage at approximately 13%. Consequently, PLASA with the optimal combination demonstrates notable performance improvements compared to both the baseline and commonly used models. It achieves micro-F1 scores of 93.94%, 90.34%, and 88.79% on the respective datasets. We also observed that the representation capacity of the pre-trained model decreases as the text length increases. Moreover, the proposed collaborative attention module effectively addresses this limitation, as confirmed by our ablation study.}
}
@article{lexical_interpretability,
    doi = {10.1371/journal.pone.0313092},
    author = {van der Veen, A. Maurits AND Bleich, Erik},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The advantages of lexicon-based sentiment analysis in an age of machine learning},
    year = {2025},
    month = {01},
    volume = {20},
    url = {https://doi.org/10.1371/journal.pone.0313092},
    pages = {1-19},
    abstract = {Assessing whether texts are positive or negative—sentiment analysis—has wide-ranging applications across many disciplines. Automated approaches make it possible to code near unlimited quantities of texts rapidly, replicably, and with high accuracy. Compared to machine learning and large language model (LLM) approaches, lexicon-based methods may sacrifice some in performance, but in exchange they provide generalizability and domain independence, while crucially offering the possibility of identifying gradations in sentiment. We demonstrate the strong performance of lexica using MultiLexScaled, an approach which averages valences across a number of widely-used general-purpose lexica. We validate it against benchmark datasets from a range of different domains, comparing performance against machine learning and LLM alternatives. In addition, we illustrate the value of identifying fine-grained sentiment levels by showing, in an analysis of pre- and post-9/11 British press coverage of Muslims, that binarized valence metrics give rise to different (and erroneous) conclusions about the nature of the post-9/11 shock as well as about differences between broadsheet and tabloid coverage. The code to apply MultiLexScaled is available online.},
    number = {1},

}
@article{Ambreen2024PredictingCS,
  title={Predicting customer sentiment: {T}he fusion of deep learning and a fuzzy system for sentiment analysis of Arabic text},
  author={Shela Ambreen and Muhammad Iqbal and Muhammad Zubair Asghar and Tehseen Mazhar and Umar Farooq Khattak and Muhammad Amir Khan and Habib Hamam},
  journal={Soc. Netw. Anal. Min.},
  year={2024},
  volume={14},
  pages={206},
  url={https://api.semanticscholar.org/CorpusID:273397549}
}
@article{Ribeiro2016WhySI,
  title={“{W}hy Should {I} Trust You?”: Explaining the Predictions of Any Classifier},
  author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  journal={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2016},
  url={https://api.semanticscholar.org/CorpusID:13029170}
}
@article{Abdelwahab2022JustifyingAT,
  title={Justifying {A}rabic Text Sentiment Analysis Using {E}xplainable {AI} {(XAI): LASIK} Surgeries Case Study},
  author={Youmna Abdelwahab and Mohamed Hamed Kholief and Ahmed Ahmed Hesham Sedky},
  journal={Inf.},
  year={2022},
  volume={13},
  pages={536},
  url={https://api.semanticscholar.org/CorpusID:253543269}
}


