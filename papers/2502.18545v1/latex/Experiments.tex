\section{Experiments}
\subsection{Overall Setup}
\noindent\textbf{Traditional Model Baselines:}
We implemented \textbf{BiLSTM-CRF} as a traditional sequence labeling baseline, following the architecture proposed by \citet{huang2015bidirectional}. We trained the model using Adam optimizer with a learning rate of 1e-3 and batch size of 32 for 50 epochs on the PII-Bench training set.

\noindent\textbf{LLM Baselines:}
The evaluation encompassed both API-based and open-source large language models.
API-based models included GPT-4o-2024-0806 (\textbf{GPT4o}) \citep{gpt-4o}, Claude-3.5-Sonnet (\textbf{Claude3.5}) \citep{claude}, and DeepSeek-Chat {\textbf{DeepseekV3}} \citep{deepseek}, accessed through their respective official APIs between January 1 and February 10, 2025.
Open-source alternatives comprised Llama-3.1-70B-Instruct (\textbf{Llama3.1}) \citep{dubey2024llama}, and Qwen-2.5-72B-Instruct (\textbf{Qwen2.5}) \citep{qwen25}.

\noindent\textbf{SLM Baselines:}
To investigate scaling effects, we included two small-scale language models: Llama-3.1-8B-Instruct (\textbf{Llama3.1-SLM}) and Qwen-2.5-7B-Instruct (\textbf{Qwen2.5-SLM}).
All experiments utilized default parameters with temperature set to 0 to ensure reproducibility.


\noindent\textbf{Prompt Baselines:}
The assessment incorporated multiple prompting strategies for query-related PII detection. \textbf{Naive} inputs the user description and query. 
\textbf{Naive /w Choice} includes a list of candidate PII entities to constrain the selection space.
\textbf{Self-CoT}~\citep{wei2022chain} incorporating step-by-step reasoning prompts. 
\textbf{Auto-CoT}~\cite{autocot}, which automates the generation of chain-of-thought demonstrations through three-shot setting. 
\textbf{Self-Consistency (SC)}~\cite{sc}, which  synthesizes multiple reasoning paths to derive the final output. 
\textbf{Plan-and-Solve CoT (PS-CoT})~\cite{wang2023plan} develops a strategic plan before executing the solution process. 
Appendix~\ref{sec:prompt} provides details of each prompts.

\noindent\textbf{Metrics:}
The PII detection task evaluates model performance through two sets of metrics: 
\textbf{Strict-F1} measures the accuracy of subject identification, entity span detection, and PII type classification simultaneously.
\textbf{Ent-F1} focuses on entity span detection independent of subject attribution and type classification. 
For query-related detection, model performance is measured through \textbf{Precision}, \textbf{Recall}, and \textbf{F1}. 
Considering the inherent variation in entity expressions and potential partial matches, \textbf{RougeL-F} is employed for both tasks to complement the exact matching metrics. 
Detailed computation procedures are provided in Appendix~\ref{sec:metrics}.

% \begin{table*}[htp]
% \centering
% \resizebox{2\columnwidth}{!}{
% \begin{tabular}{ccccccccccccc}
% \toprule
% \multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{PII-Single}} & \multicolumn{3}{c}{\textbf{PII-Multi}} & \multicolumn{3}{c}{\textbf{PII-Hard}} & \multicolumn{3}{c}{\textbf{PII-Distract}} \\
% \multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Baseline Models}}} & Strict-F1 & Ent-F1 & \multicolumn{1}{c}{RougeL-F} & Strict-F1 & Ent-F1 & \multicolumn{1}{c}{RougeL-F} & Strict-F1 & Ent-F1 & \multicolumn{1}{c}{RougeL-F} & Strict-F1 & Ent-F1 & RougeL-F \\ \hline
% \multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{Traditional Model}} \\ \hline
% \multicolumn{1}{c}{BiLSTM-CRF} & - & 0.851 & \multicolumn{1}{c}{-} & - & 0.828 & \multicolumn{1}{c}{-} & - & 0.684 & \multicolumn{1}{c}{-} & - & 0.787 & - \\ \hline
% \multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{API-based Large Language Model}} \\ \hline
% \multicolumn{1}{c}{GPT4o} & 0.893 & 0.914 & \multicolumn{1}{c}{0.895} & \textbf{0.891} & 0.923 & \multicolumn{1}{c}{\textbf{0.893}} & 0.817 & 0.869 & \multicolumn{1}{c}{0.819} & 0.715 & 0.868 & 0.716 \\
% \multicolumn{1}{c}{Claude3.5} & 0.858 & 0.891 & \multicolumn{1}{c}{0.862} & 0.890 & 0.920 & \multicolumn{1}{c}{0.892} & 0.813 & 0.857 & \multicolumn{1}{c}{0.818} & \textbf{0.910} & \textbf{0.948} & \textbf{0.911} \\
% \multicolumn{1}{c}{DeepSeekV3} & \textbf{0.903} & \textbf{0.921} & \multicolumn{1}{c}{\textbf{0.905}} & 0.884 & 0.927 & \multicolumn{1}{c}{0.886} & 0.838 & 0.893 & \multicolumn{1}{c}{0.838} & 0.658 & 0.945 & 0.658 \\ \hline
% \multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{Open-source Large Language Model}} \\ \hline
% \multicolumn{1}{c}{Llama3.1} & 0.881 & 0.913 & \multicolumn{1}{c}{0.883} & 0.883 & \textbf{0.942} & \multicolumn{1}{c}{0.884} & \textbf{0.840} & \textbf{0.893} & \multicolumn{1}{c}{\textbf{0.841}} & 0.834 & 0.946 & 0.835 \\
% \multicolumn{1}{c}{Qwen2.5} & 0.866 & 0.908 & \multicolumn{1}{c}{0.869} & 0.853 & 0.918 & \multicolumn{1}{c}{0.855} & 0.804 & 0.876 & \multicolumn{1}{c}{0.806} & 0.647 & 0.941 & 0.649 \\ \hline
% \multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{Open-source Small Language Model}} \\ \hline
% \multicolumn{1}{c}{Llama3.1-SLM} & 0.748 & 0.800 & \multicolumn{1}{c}{0.752} & 0.778 & 0.869 & \multicolumn{1}{c}{0.781} & 0.718 & 0.798 & \multicolumn{1}{c}{0.722} & 0.551 & 0.876 & 0.552 \\
% \multicolumn{1}{c}{Qwen2.5-SLM} & 0.787 & 0.846 & \multicolumn{1}{c}{0.792} & 0.451 & 0.806 & \multicolumn{1}{c}{0.453} & 0.591 & 0.810 & \multicolumn{1}{c}{0.594} & 0.454 & 0.815 & 0.456 \\ \bottomrule
% \end{tabular}}
% \caption{Performance of baseline models under the PII Detection task. Results in \textbf{bold} indicate the best performance for each dataset and metric category. }
% \label{tab:pii_recog}
% \vspace{-2mm}
% \end{table*}

\begin{table*}[htp]
\centering
\resizebox{2\columnwidth}{!}{
\begin{tabular}{ccccccccccc}
% \hline
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{GPT4o}} & \multicolumn{2}{c}{\textbf{Llama3.1}} & \multicolumn{2}{c}{\textbf{Qwen2.5}} & \multicolumn{2}{c}{\textbf{Llama3.1-SLM}} & \multicolumn{2}{c}{\textbf{Qwen2.5-SLM}} \\
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Method}}} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & RougeL-F \\ \hline
\multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Basic Method w/ PII Detection}} \\ \hline
\multicolumn{1}{l}{Naive} & 0.72 & \multicolumn{1}{c}{0.72} & 0.72 & \multicolumn{1}{c}{0.73} & 0.70 & \multicolumn{1}{c}{0.70} & 0.42 & \multicolumn{1}{c}{0.43} & 0.54 & 0.58 \\ \hline
\multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Advanced Method w/ PII Detection}} \\ \hline
\multicolumn{1}{l}{Self-CoT} & 0.76 & \multicolumn{1}{c}{0.77} & 0.75 & \multicolumn{1}{c}{0.75} & 0.73 & \multicolumn{1}{c}{0.73} & 0.53 & \multicolumn{1}{c}{0.54} & 0.54 & 0.58 \\ \hline
\multicolumn{1}{l}{Auto-CoT(3-shot)} & 0.75 & \multicolumn{1}{c}{0.75} & \textbf{0.76} & \multicolumn{1}{c}{0.77} & \textbf{0.76} & \multicolumn{1}{c}{0.76} & \textbf{0.57} & \multicolumn{1}{c}{0.58} & 0.54 & 0.58 \\ \hline
\multicolumn{1}{l}{Self-Consistency} & \textbf{0.77} & \multicolumn{1}{c}{0.77} & 0.71 & \multicolumn{1}{c}{0.72} & 0.71 & \multicolumn{1}{c}{0.72} & 0.49 & \multicolumn{1}{c}{0.50} & 0.49 & 0.53 \\ \hline
\multicolumn{1}{l}{PS-CoT} & 0.74 & \multicolumn{1}{c}{0.74} & 0.72 & \multicolumn{1}{c}{0.73} & 0.73 & \multicolumn{1}{c}{0.73} & 0.48 & \multicolumn{1}{c}{0.50} & \textbf{0.56} & 0.60 \\ \hline
\multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{w/ Extra Information w/ PII Detetcion}} \\ \hline
\multicolumn{1}{l}{Naive w/ Choice} & 0.82 & \multicolumn{1}{c}{0.82} & 0.77 & \multicolumn{1}{c}{0.78} & 0.79 & \multicolumn{1}{c}{0.79} & 0.46 & \multicolumn{1}{c}{0.48} & 0.67 & 0.71 \\ \bottomrule
\end{tabular}}
\caption{Performance comparison on the Query-Unrelated PII Masking task (PII-single and PII-multi datasets). The best performance for each model (excluding Naive w/ Choice) is in \textbf{bold}.}
\label{tab:query-unrelated}
\end{table*}

\begin{figure*}[t]
  \includegraphics[width=2\columnwidth]{fig/stats_analysis.pdf}
\vspace{-2mm}
  \caption{The performance of GPT-4o is correlated with the number of subject, the number of PII, decription length, and the number of query-related PII.}
  \label{fig:factor}
\vspace{-2mm}
\end{figure*}

\begin{figure}[t]
  \includegraphics[width=1\columnwidth]{fig/type_acc.pdf}
  \caption{Performance comparison across different models for seven main PII types.}
  \label{fig:type_acc}
\vspace{-2mm}
\end{figure}

% \begin{table}[t]
% \centering
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lcccc}
% \toprule
% \multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{Test-Hard}} & \multicolumn{2}{c}{\textbf{Test-Distract}} \\
% \multicolumn{1}{l}{\multirow{-2}{*}{\textbf{Method}}} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & RougeL-F \\ \hline
% \multicolumn{5}{l}{\cellcolor[HTML]{EFEFEF}\textit{Basic Method}} \\ \hline
% \multicolumn{1}{l}{Naive} & 0.361 & \multicolumn{1}{c}{0.361} & 0.568 & 0.573 \\ \hline
% \multicolumn{5}{l}{\cellcolor[HTML]{EFEFEF}\textit{Advanced Method}} \\ \hline
% \multicolumn{1}{l}{Self-CoT} & 0.449 & \multicolumn{1}{c}{0.454} & 0.661 & 0.667 \\ \hline
% \multicolumn{1}{l}{Auto-CoT(3-shot)} & 0.446 & \multicolumn{1}{c}{0.400} & 0.619 & 0.628 \\ \hline
% \multicolumn{1}{l}{Self-Consistency} & \textbf{0.463} & \multicolumn{1}{c}{0.463} & 0.621 & 0.625 \\ \hline
% \multicolumn{1}{l}{PS-CoT} & 0.380 & \multicolumn{1}{c}{0.380} & \textbf{0.665} & 0.669 \\ \hline
% \multicolumn{5}{l}{\cellcolor[HTML]{EFEFEF}\textit{w/ Extra Information}} \\ \hline
% \multicolumn{1}{l}{Naive w/ Choice} & 0.525 & \multicolumn{1}{c}{0.525} & 0.663 & 0.663 \\ \bottomrule
% \end{tabular}}
% \caption{Performance comparison on challenging test sets using GPT4o.}
% \label{tab:hard_distract}
% \vspace{-2mm}
% \end{table}

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{Test-Hard}} & \multicolumn{2}{c}{\textbf{Test-Distract}} \\
\multicolumn{1}{l}{\multirow{-2}{*}{\textbf{Method}}} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & RougeL-F \\ \hline
\multicolumn{5}{l}{\cellcolor[HTML]{EFEFEF}\textit{Basic Method}} \\ \hline
\multicolumn{1}{l}{Naive} & 0.36 & \multicolumn{1}{c}{0.36} & 0.57 & 0.57 \\ \hline
\multicolumn{5}{l}{\cellcolor[HTML]{EFEFEF}\textit{Advanced Method}} \\ \hline
\multicolumn{1}{l}{Self-CoT} & 0.45 & \multicolumn{1}{c}{0.45} & 0.66 & 0.67 \\ \hline
\multicolumn{1}{l}{Auto-CoT(3-shot)} & 0.45 & \multicolumn{1}{c}{0.40} & 0.62 & 0.63 \\ \hline
\multicolumn{1}{l}{Self-Consistency} & \textbf{0.46} & \multicolumn{1}{c}{0.46} & 0.62 & 0.63 \\ \hline
\multicolumn{1}{l}{PS-CoT} & 0.38 & \multicolumn{1}{c}{0.38} & \textbf{0.67} & 0.67 \\ \hline
\multicolumn{5}{l}{\cellcolor[HTML]{EFEFEF}\textit{w/ Extra Information}} \\ \hline
\multicolumn{1}{l}{Naive w/ Choice} & 0.53 & \multicolumn{1}{c}{0.53} & 0.66 & 0.66 \\ \bottomrule
\end{tabular}}
\caption{Performance comparison on challenging test sets using GPT4o.}
\label{tab:hard_distract}
\vspace{-2mm}
\end{table}

\subsection{Performance on Query-Unrelated PII Masking}
We evaluate models' performance on the query-unrelated PII masking task, which requires both accurate PII detection and relevance assessment. Table~\ref{tab:query-unrelated} presents our experimental results:

\noindent\textbf{Joint task yields improved performance.}
Notably, models achieve higher F1 scores in this combined task compared to individual query-relevance tasks. GPT4o reaches 0.77 F1 with Self-Consistency prompting, suggesting that the joint objective may provide complementary signals. Open-source models demonstrate comparable capabilities, with both Llama3.1 and Qwen2.5 achieving 0.76 F1 using Auto-CoT.

\subsection{Performance on PII Detection}
Experimental results on the PII detection task, presented in Table~\ref{tab:pii_recog}, reveal several key findings:

\noindent\textbf{Large Language Models demonstrate superior detection capabilities.}
API-based LLMs achieve strong performance across standard datasets, with DeepSeekV3 and GPT4o leading in Strict-F1 scores (0.903 and 0.891 on PII-Single and PII-Multi, respectively). The open-source Llama3.1 shows competitive performance, particularly in entity recognition (Ent-F1: 0.942 on PII-Multi), while traditional BiLSTM-CRF maintains reasonable entity detection capabilities despite its simpler architecture.

% \noindent\textbf{Model scale significantly impacts performance.}
% Our experiments reveal a substantial performance gap between large and small language models. Small-scale variants consistently underperform their larger counterparts, with performance degradation particularly evident in complex scenarios (PII-Multi and PII-Distract). For instance, Llama3.1-SLM's Strict-F1 score drops by 13.3\% compared to its larger counterpart on PII-Single.

\noindent\textbf{Entity type classification remains challenging.}
A consistent gap between Strict-F1 and Ent-F1 scores indicates that accurate PII type classification poses greater challenges than entity boundary detection. This disparity becomes more pronounced in the PII-Distract dataset, where models maintain relatively high Ent-F1 scores despite significant drops in Strict-F1, suggesting increased difficulty in precise PII categorization under complex scenarios.

\begin{table*}[htp]
\centering
\resizebox{2\columnwidth}{!}{
\begin{tabular}{ccccccccccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textbf{PII-Single}} & \multicolumn{3}{c}{\textbf{PII-Multi}} & \multicolumn{3}{c}{\textbf{PII-Hard}} & \multicolumn{3}{c}{\textbf{PII-Distract}} \\
\multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Baseline Models}}} & Strict-F1 & Ent-F1 & \multicolumn{1}{c}{RougeL-F} & Strict-F1 & Ent-F1 & \multicolumn{1}{c}{RougeL-F} & Strict-F1 & Ent-F1 & \multicolumn{1}{c}{RougeL-F} & Strict-F1 & Ent-F1 & RougeL-F \\ \hline
\multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{Traditional Model}} \\ \hline
\multicolumn{1}{c}{BiLSTM-CRF} & - & 0.851 & \multicolumn{1}{c}{-} & - & 0.828 & \multicolumn{1}{c}{-} & - & 0.684 & \multicolumn{1}{c}{-} & - & 0.787 & - \\ \hline
\multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{API-based Large Language Model}} \\ \hline
\multicolumn{1}{c}{GPT4o} & 0.893 & 0.914 & \multicolumn{1}{c}{0.895} & \textbf{0.891} & 0.923 & \multicolumn{1}{c}{\textbf{0.893}} & 0.817 & 0.869 & \multicolumn{1}{c}{0.819} & 0.715 & 0.868 & 0.716 \\
\multicolumn{1}{c}{Claude3.5} & 0.858 & 0.891 & \multicolumn{1}{c}{0.862} & 0.890 & 0.920 & \multicolumn{1}{c}{0.892} & 0.813 & 0.857 & \multicolumn{1}{c}{0.818} & \textbf{0.910} & \textbf{0.948} & \textbf{0.911} \\
\multicolumn{1}{c}{DeepSeekV3} & \textbf{0.903} & \textbf{0.921} & \multicolumn{1}{c}{\textbf{0.905}} & 0.884 & 0.927 & \multicolumn{1}{c}{0.886} & 0.838 & 0.893 & \multicolumn{1}{c}{0.838} & 0.658 & 0.945 & 0.658 \\ \hline
\multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{Open-source Large Language Model}} \\ \hline
\multicolumn{1}{c}{Llama3.1} & 0.881 & 0.913 & \multicolumn{1}{c}{0.883} & 0.883 & \textbf{0.942} & \multicolumn{1}{c}{0.884} & \textbf{0.840} & \textbf{0.893} & \multicolumn{1}{c}{\textbf{0.841}} & 0.834 & 0.946 & 0.835 \\
\multicolumn{1}{c}{Qwen2.5} & 0.866 & 0.908 & \multicolumn{1}{c}{0.869} & 0.853 & 0.918 & \multicolumn{1}{c}{0.855} & 0.804 & 0.876 & \multicolumn{1}{c}{0.806} & 0.647 & 0.941 & 0.649 \\ \hline
\multicolumn{13}{l}{\cellcolor[HTML]{EFEFEF}\textit{Open-source Small Language Model}} \\ \hline
\multicolumn{1}{c}{Llama3.1-SLM} & 0.748 & 0.800 & \multicolumn{1}{c}{0.752} & 0.778 & 0.869 & \multicolumn{1}{c}{0.781} & 0.718 & 0.798 & \multicolumn{1}{c}{0.722} & 0.551 & 0.876 & 0.552 \\
\multicolumn{1}{c}{Qwen2.5-SLM} & 0.787 & 0.846 & \multicolumn{1}{c}{0.792} & 0.451 & 0.806 & \multicolumn{1}{c}{0.453} & 0.591 & 0.810 & \multicolumn{1}{c}{0.594} & 0.454 & 0.815 & 0.456 \\ \bottomrule
\end{tabular}}
\caption{Performance of baseline models under the PII Detection task. Results in \textbf{bold} indicate the best performance for each dataset and metric category.}
\label{tab:pii_recog}
\vspace{-2mm}
\end{table*}

\begin{table*}[htp]
\centering
\resizebox{2\columnwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{GPT4o}} & \multicolumn{2}{c}{\textbf{Llama3.1}} & \multicolumn{2}{c}{\textbf{Qwen2.5}} & \multicolumn{2}{c}{\textbf{Llama3.1-SLM}} & \multicolumn{2}{c}{\textbf{Qwen2.5-SLM}} \\
\multicolumn{1}{l}{\multirow{-2}{*}{\textbf{Method}}} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & RougeL-F \\ \hline
\multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Basic Method}} \\ \hline
\multicolumn{1}{l}{Naive} & 0.63 & \multicolumn{1}{c}{0.63} & 0.63 & \multicolumn{1}{c}{0.63} & 0.62 & \multicolumn{1}{c}{0.62} & 0.33 & \multicolumn{1}{c}{0.33} & 0.41 & 0.41 \\ \hline
\multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Advanced Method}} \\ \hline
\multicolumn{1}{l}{Self-CoT} & 0.71 & \multicolumn{1}{c}{0.72} & 0.69 & \multicolumn{1}{c}{0.69} & 0.67 & \multicolumn{1}{c}{0.68} & 0.39 & \multicolumn{1}{c}{0.39} & 0.40 & 0.41 \\ \hline
\multicolumn{1}{l}{Auto-CoT(3-shot)} & 0.66 & \multicolumn{1}{c}{0.66} & \textbf{0.70} & \multicolumn{1}{c}{0.72} & \textbf{0.71} & \multicolumn{1}{c}{0.72} & \textbf{0.43} & \multicolumn{1}{c}{0.44} & 0.37 & 0.38 \\ \hline
\multicolumn{1}{l}{Self-Consistency} & \textbf{0.72} & \multicolumn{1}{c}{0.72} & 0.63 & \multicolumn{1}{c}{0.64} & 0.65 & \multicolumn{1}{c}{0.65} & 0.31 & \multicolumn{1}{c}{0.32} & 0.32 & 0.33 \\ \hline
\multicolumn{1}{l}{PS-CoT} & 0.65 & \multicolumn{1}{c}{0.65} & 0.65 & \multicolumn{1}{c}{0.66} & 0.67 & \multicolumn{1}{c}{0.67} & 0.35 & \multicolumn{1}{c}{0.36} & \textbf{0.45} & 0.46 \\ \hline
\multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{w/ Extra Information}} \\ \hline
\multicolumn{1}{l}{Naive w/ Choice} & 0.84 & \multicolumn{1}{c}{0.84} & 0.76 & \multicolumn{1}{c}{0.76} & 0.83 & \multicolumn{1}{c}{0.83} & 0.52 & \multicolumn{1}{c}{0.52} & 0.77 & 0.77 \\ \bottomrule
\end{tabular}}
\caption{Performance comparison on the Query-Related PII Detection task (PII-single dataset).}
\label{tab:single}
\end{table*}

% \begin{table*}[htp]
% \centering
% \resizebox{2\columnwidth}{!}{
% \begin{tabular}{ccccccccccc}
% % \hline
% \toprule
% \multicolumn{1}{c}{} & \multicolumn{2}{c}{\textbf{GPT4o}} & \multicolumn{2}{c}{\textbf{Llama3.1}} & \multicolumn{2}{c}{\textbf{Qwen2.5}} & \multicolumn{2}{c}{\textbf{Llama3.1-SLM}} & \multicolumn{2}{c}{\textbf{Qwen2.5-SLM}} \\
% \multicolumn{1}{c}{\multirow{-2}{*}{\textbf{Method}}} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & RougeL-F \\ \hline
% \multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Basic Method w/ PII Detection}} \\ \hline
% \multicolumn{1}{l}{Naive} & 0.72 & \multicolumn{1}{c}{0.72} & 0.72 & \multicolumn{1}{c}{0.73} & 0.70 & \multicolumn{1}{c}{0.70} & 0.42 & \multicolumn{1}{c}{0.43} & 0.54 & 0.58 \\ \hline
% \multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Advanced Method w/ PII Detection}} \\ \hline
% \multicolumn{1}{l}{Self-CoT} & 0.76 & \multicolumn{1}{c}{0.77} & 0.75 & \multicolumn{1}{c}{0.75} & 0.73 & \multicolumn{1}{c}{0.73} & 0.53 & \multicolumn{1}{c}{0.54} & 0.54 & 0.58 \\ \hline
% \multicolumn{1}{l}{Auto-CoT(3-shot)} & 0.75 & \multicolumn{1}{c}{0.75} & \textbf{0.76} & \multicolumn{1}{c}{0.77} & \textbf{0.76} & \multicolumn{1}{c}{0.76} & \textbf{0.57} & \multicolumn{1}{c}{0.58} & 0.54 & 0.58 \\ \hline
% \multicolumn{1}{l}{Self-Consistency} & \textbf{0.77} & \multicolumn{1}{c}{0.77} & 0.71 & \multicolumn{1}{c}{0.72} & 0.71 & \multicolumn{1}{c}{0.72} & 0.49 & \multicolumn{1}{c}{0.50} & 0.49 & 0.53 \\ \hline
% \multicolumn{1}{l}{PS-CoT} & 0.74 & \multicolumn{1}{c}{0.74} & 0.72 & \multicolumn{1}{c}{0.73} & 0.73 & \multicolumn{1}{c}{0.73} & 0.48 & \multicolumn{1}{c}{0.50} & \textbf{0.56} & 0.60 \\ \hline
% \multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{w/ Extra Information w/ PII Detetcion}} \\ \hline
% \multicolumn{1}{l}{Naive w/ Choice} & 0.82 & \multicolumn{1}{c}{0.82} & 0.77 & \multicolumn{1}{c}{0.78} & 0.79 & \multicolumn{1}{c}{0.79} & 0.46 & \multicolumn{1}{c}{0.48} & 0.67 & 0.71 \\ \bottomrule
% \end{tabular}}
% \caption{Performance comparison on the Query-Unrelated PII Masking task (PII-single and PII-multi datasets).}
% \label{tab:query-unrelated}
% \end{table*}

% \begin{table*}[htp]
% \centering
% \resizebox{2\columnwidth}{!}{
% \begin{tabular}{lcccccccccc}
% \toprule
% \multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{GPT4o}} & \multicolumn{2}{c}{\textbf{Llama3.1}} & \multicolumn{2}{c}{\textbf{Qwen2.5}} & \multicolumn{2}{c}{\textbf{Llama3.1-SLM}} & \multicolumn{2}{c}{\textbf{Qwen2.5-SLM}} \\
% \multicolumn{1}{l}{\multirow{-2}{*}{\textbf{Method}}} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & \multicolumn{1}{c}{RougeL-F} & F1 & RougeL-F \\ \hline
% \multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Basic Method}} \\ \hline
% \multicolumn{1}{l}{Naive} & 0.600 & \multicolumn{1}{c}{0.602} & 0.611 & \multicolumn{1}{c}{0.614} & 0.596 & \multicolumn{1}{c}{0.603} & 0.240 & \multicolumn{1}{c}{0.333} & 0.405 & 0.413 \\ \hline
% \multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{Advanced Method}} \\ \hline
% \multicolumn{1}{l}{Self-CoT} & 0.675 & \multicolumn{1}{c}{0.681} & 0.638 & \multicolumn{1}{c}{0.643} & 0.626 & \multicolumn{1}{c}{0.632} & 0.354 & \multicolumn{1}{c}{0.362} & 0.392 & 0.397 \\ \hline
% \multicolumn{1}{l}{Auto-CoT(3-shot)} & 0.629 & \multicolumn{1}{c}{0.640} & \textbf{0.650} & \multicolumn{1}{c}{0.662} & \textbf{0.657} & \multicolumn{1}{c}{0.665} & \textbf{0.393} & \multicolumn{1}{c}{0.402} & 0.391 & 0.394 \\ \hline
% \multicolumn{1}{l}{Self-Consistency} & \textbf{0.685} & \multicolumn{1}{c}{0.692} & 0.602 & \multicolumn{1}{c}{0.605} & 0.614 & \multicolumn{1}{c}{0.620} & 0.263 & \multicolumn{1}{c}{0.269} & 0.288 & 0.293 \\ \hline
% \multicolumn{1}{l}{PS-CoT} & 0.618 & \multicolumn{1}{c}{0.620} & 0.624 & \multicolumn{1}{c}{0.631} & 0.636 & \multicolumn{1}{c}{0.643} & 0.291 & \multicolumn{1}{c}{0.300} & \textbf{0.431} & 0.436 \\ \hline
% \multicolumn{11}{l}{\cellcolor[HTML]{EFEFEF}\textit{w/ Extra Information}} \\ \hline
% \multicolumn{1}{l}{Naive w/ Choice} & 0.846 & \multicolumn{1}{c}{0.846} & 0.775 & \multicolumn{1}{c}{0.775} & 0.804 & \multicolumn{1}{c}{0.804} & 0.387 & \multicolumn{1}{c}{0.388} & 0.743 & 0.743 \\ \bottomrule
% \end{tabular}}

% \caption{Performance comparison on the Query-Related PII Detection task (PII-multi dataset).}
% \label{tab:multi}
% \end{table*}


\subsection{Performance on Query-Related PII Detection}
% Tables \ref{tab:single} and \ref{tab:multi} present the results on PII-single and PII-multi datasets respectively across different model scales and prompting strategies.
Tables \ref{tab:single} presents the results on PII-single dataset across different model scales and prompting strategies. 
% Additional results are presented in Appendix \ref{sec:exp_res}.

\noindent\textbf{Limited Performance of Current LLMs.} State-of-the-art LLMs exhibit limited performance in this task, with GPT4o achieving only 0.627 F1 score with naive prompting—substantially below human performance (0.951 F1). Open-source alternatives demonstrate competitive performance, with Qwen2.5 reaching 0.615 F1.

\noindent\textbf{Impact of Advanced Prompting.} Chain-of-thought approaches generally improve performance, with Self-Consistency and Auto-CoT proving most effective for different models (0.716 F1 for GPT4o with Self-Consistency; 0.710 F1 for Qwen2.5 with Auto-CoT). However, these benefits are highly dependent on model scale—smaller models often show degraded performance with complex prompting strategies, indicating insufficient reasoning capabilities.

% \noindent\textbf{Scale-Dependent Performance Gap.} Model scale significantly impacts performance on this task. SLMs achieve substantially lower F1 scores (0.328-0.410) compared to LLMs (0.615-0.627) on PII-single. This performance gap widens in multi-subject scenarios, where Llama3.1-SLM's F1 score drops from 0.328 to 0.240.

\noindent\textbf{Effectiveness of Entity Candidates.} The provision of candidate PII entities (Naive w/ Choice) substantially improves performance across all models (e.g., GPT4o improves from 0.627 to 0.842 F1). However, the practical applicability of this approach is limited, as candidate entities are rarely available in real-world scenarios.

% \noindent\textbf{Prompting strategies show consistent benefits.}
% Advanced prompting techniques provide substantial improvements across all model scales. Chain-of-thought approaches consistently outperform naive prompting, with Self-Consistency and Auto-CoT showing the strongest results. The performance gain is particularly evident in larger models, where Self-CoT improves GPT4o's F1 score from 0.72 to 0.76, while Auto-CoT enhances Llama3.1-SLM's performance from 0.42 to 0.57.

% \noindent\textbf{Scale-dependent performance persists.}
% The performance gap between large and small models remains significant in this combined task. Small-scale variants achieve substantially lower F1 scores (0.42-0.54) compared to their larger counterparts (0.70-0.72), indicating that effective masking requires both robust PII detection and sophisticated query understanding capabilities.

\subsection{In-depth Performance Analysis}
% \noindent\textbf{Impact of Model Scale.}
% Analysis across all tasks reveals a consistent performance gap between large and small language models. In PII detection, small-scale variants show 13-33\% lower F1 scores than their larger counterparts. This gap widens in query-related tasks (Llama3.1-SLM: 0.328 F1 vs. GPT4o: 0.627 F1) and remains significant in the combined masking task (small models: 0.42-0.54 F1 vs. large models: 0.70-0.72 F1). The performance disparity becomes more pronounced in complex scenarios, particularly with multi-subject contexts and longer descriptions.

\noindent\textbf{Impact of Model Scale.}
Analysis reveals consistent performance gaps between large and small models across tasks. Small-scale variants show 13-33\% lower F1 scores in PII detection, with wider gaps in the query-related task (Llama3.1-SLM: 0.328 F1 vs. GPT4o: 0.627 F1) and query-unrelated masking task (0.42-0.54 F1 vs. 0.70-0.72 F1).

% \noindent\textbf{PII Type Performance:}
% Detailed analysis across PII categories reveals varying levels of model capability. As shown in Fig.~\ref{fig:type_acc}, while models demonstrate strong performance in recognizing personal identifiers (PER) and standardized codes (CODE) with accuracies above 0.95, they show significant weakness in handling location (LOC) and organizational (ORG) entities. This pattern persists across model scales, though more pronounced in smaller models, where LLama3.1-SLM's accuracy drops to 0.412 for location information. The performance disparity between structured (CODE) and unstructured (LOC, ORG) PII types indicates models' reliance on pattern matching over contextual understanding.

\noindent\textbf{PII Type Performance.} As shown in Fig.~\ref{fig:type_acc}, models perform better at recognizing structured information (PER, CODE) compared to contextual entities (LOC, ORG), suggesting models have stronger capability in identifying patterns with clear structural characteristics.

% \noindent\textbf{Factors Influencing Model Performance:}
% As shown in Fig.~\ref{fig:factor}, the number of subjects emerges as a critical factor affecting model accuracy. Performance maintains stability with 1-5 subjects but degrades sharply beyond this range, with F1 scores dropping from 0.85 to 0.52 for contexts with ten subjects. PII density shows similar impact patterns, where performance remains stable up to 33 entities before showing significant decline, particularly affecting type classification accuracy. Description length demonstrates a strong negative correlation with model performance, where accuracy deteriorates as text length increases beyond 3000 characters. Interestingly, while the number of query-related PII entities also influences performance, its impact appears less pronounced, with only modest accuracy decline as the count increases from 1 to 7 entities.

\noindent\textbf{Factors Influencing Performance.}
Fig.~\ref{fig:factor} reveals several critical factors affecting model accuracy: performance degrades sharply beyond 5 subjects (F1 drops from 0.85 to 0.52), 33 PII entities, or 3000 characters in text length. Query-related entity count shows modest impact, with gradual decline from 1 to 7 entities.

% \noindent\textbf{Performance on Challenging Scenarios:}
% As shown in Table~\ref{tab:hard_distract}, the Test-Hard dataset presents significant challenges, with conventional approaches showing limited effectiveness. While Self-Consistency achieves the highest F1 score (0.463), the improvement over basic methods remains marginal. Analysis of Test-Distract scenarios reveals similar limitations, though PS-CoT shows better adaptation (F1: 0.665). The provision of candidate entities (Naive w/ Choice) consistently improves performance across both datasets, suggesting the importance of constrained entity selection in complex scenarios.

\noindent\textbf{Performance on Challenging Scenarios.}
Results on specialized test sets (Table~\ref{tab:hard_distract}) reveal significant performance degradation in complex scenarios. On Test-Hard, featuring high PII density and long texts, even the best-performing Self-Consistency approach achieves only 0.463 F1. Test-Distract's multi-subject scenarios pose similar challenges.