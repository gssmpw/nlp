

\section{Introduction}

Recent years have witnessed the widespread adoption of Large Language Models (LLMs), with an increasing number of users directly interacting with these models through APIs for various tasks, ranging from daily conversations to complex analytical work (\citealp{sun2023textclassificationlargelanguage};~\citealp{yang2024zhongjing};~\citealp{WONG2023253}).
Despite the convenience these services offer, users often overlook a significant privacy risk: 
the prompts submitted to LLMs frequently contain substantial personally identifiable information (PII)~\citep{achiam2023gpt}.
Such information is vulnerable not only to interception by malicious actors during transmission~\citep{parast2022cloud} but also to potential misuse by unethical service providers who might collect and incorporate it into subsequent model training, leading to permanent privacy breaches~\citep{liu2023trustworthy}.

Current practices reveal that the vast majority of users adopt a zero-protection approach when utilizing LLM services, submitting original prompts containing PII directly to the LLMs.
While an obvious protection strategy would be to mask all PII (~\citealp{nakamura2020kart};~\citealp{biesner2022anonymization};\citealp{lukas2023analyzing}), as shown in Fig.~\ref{fig:intro}, this approach significantly compromises service quality.
An ideal Privacy Protection System should maintain LLMs' functionality while maximizing user privacy protection.
For instance, when a user inquires about a candidate's suitability for a senior researcher position, masking their educational background and work experience would render the LLM incapable of making an effective assessment.

This observation motivates our proposal of a query-unrelated PII masking strategy:
Masking only the PII irrelevant to user queries while retaining essential information.
In the aforementioned example, this approach would preserve the candidate's educational and professional information while masking unrelated personal details such as contact information.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/Intro_refine.pdf}
  \caption{The overall performance of three PII Masking strategies: No Masking, All PII Masking, and Query-unrelated PII Masking.
  Effective Privacy Protection Systems are required to maintain LLMs' functionality while protect user's privacy as much as possible.}
  \label{fig:intro}
  \vspace{-5mm}
\end{figure}


The implementation of query-unrelated PII masking stragety faces two-tier challenges.
The first involves accurate identification of all PII within the prompt, serving as foundational work. 
The second requires determining the relevance of identified PII to user queries. 
While existing research has made progress in basic PII detection, systematic studies considering query relevance remain scarce.

To advance the field of privacy-preserving language models, we present PII-Bench, a comprehensive evaluation framework designed to assess Privacy Protection Systems' efficacy in preserving Large Language Models' core functionalities while optimizing user privacy safeguards.
PII-Bench comprising 2,842 carefully designed test samples across 55 fine-grained PII categories, ranging from basic personal information to complex social relationship data. 
Each sample consists of three key components: 
(1) A user query simulating real-world information needs. 
(2) A context description containing diverse PII. 
(3) A standard answer indicating query-relevant PII and masking requirements.

Our experimental analysis reveals that while existing models, including  Bidirectional Long Short-Term Memory with Conditional Random Fields (BiLSTM-CRF), perform adequately in basic PII detection, they demonstrate notable limitations in determining PII query relevance.
Even state-of-the-art LLMs face challenges in this task, indicating substantial room for improvement in achieving intelligent PII masking. Despite the recent advances in model architecture and training techniques, smaller models (SLM) still show considerable performance gaps compared to larger LLMs, particularly in determining PII query relevance.

The primary contributions of this work include:
1. The first proposal of query-unrelated PII masking strategy, offering novel approaches to maintain LLM service quality while protecting privacy.
2. Development of PII-Bench evaluation framework, enabling systematic assessment of models' capabilities in PII identification and query relevance determination.
3. Experimental revelation of current model limitations in this task, providing direction for future research.
