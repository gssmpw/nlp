\section{Related work \& application context}
The incident database problem is at the intersection of various challenges addressed in fairness and statistics. 
\iftoggle{icml}{
We discuss additional work, including application and policy context, in Appendix \ref{app:rel}.
}{}

\paragraph{Algorithmic accountability via (individual) reports.}
Some recent work considers methods for learning about fairness problems via individual reports from both theoretical \citep{globus2022algorithmic} and practical \citep{agostini2024bayesian} perspectives. However, most discussion of individual experiences in machine learning fairness literature is limited to contexts where the objective is to assess, appeal, contest or seek recourse for that individual to change their \textit{individual} outcomes, rather than forming a \textit{collective} judgment about the system as a whole~\citep{sharifi2019average, ustun2019actionable, karimi2022survey}.

Work on identifying fairness-related issues via reporting data has typically focused on learning in batch contexts, e.g. via 
positive-unlabeled learning for handling disparate reporting rates across subgroups (e.g., \citet{shanmugam2024quantifying, wu2022fairness}). 
In other works, identifying disparate reporting rates is itself is the central challenge (e.g., \citet{liu2022equity, liu2024quantifying}).
On the other hand, an emerging body of literature from the human-computer interaction community develops the concept of \textit{contestability} (e.g., \citet{almada2019human, vaccaro2019contestability, landau2024challenging}); though contestability is still typically understood in terms of individual outcomes, we see our work as one possible path to implementing this ideal\iftoggle{icml}{.}{, with an eye towards empowering contestability at larger scale.}


\paragraph{Fairness auditing as hypothesis testing.} 
\iftoggle{icml}{Existing proposals to formalize fairness auditing via hypothesis testing mainly consider batch settings (i.e. post-hoc or pre-deployment) \citep{cenai, cherian2023statistical}.}{\citet{cenai} make a direct connection between legal AI fairness audit requirements and hypothesis testing, though they mainly consider a post-hoc setting. \citet{cherian2023statistical} take a multiple testing approach for handling a large number of groups, but this test is again post-hoc (or entirely pre-deployment).} Two more closely related works are that of \citet{chugg2024auditing} and \citet{feng2024monitoring}, who propose applying sequential hypothesis tests with the explicit goal of identifying problems in deployed systems in real time.
However, as neither of these works study a reporting model, we propose fundamentally different tests: they test equality of means across different groups, while we compare within groups. 

\iftoggle{icml}{}{
\input{a0-additional-related}
}