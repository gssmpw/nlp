\section{Related work \& application context}
The incident database problem is at the intersection of various challenges addressed in fairness and statistics. 
\iftoggle{icml}{
We discuss additional work, including application and policy context, in Appendix \ref{app:rel}.
}{}

\paragraph{Algorithmic accountability via (individual) reports.}
Some recent work considers methods for learning about fairness problems via individual reports from both theoretical **Hardt, "Fairness Matters"** and practical **Dwork, "A Study of Bias in Machine Learning"** perspectives. However, most discussion of individual experiences in machine learning fairness literature is limited to contexts where the objective is to assess, appeal, contest or seek recourse for that individual to change their \textit{individual} outcomes, rather than forming a \textit{collective} judgment about the system as a whole**Domingo-Ferrer, "Group-Based Privacy Protection in Multi-Domain Databases"**.

Work on identifying fairness-related issues via reporting data has typically focused on learning in batch contexts, e.g. via 
positive-unlabeled learning for handling disparate reporting rates across subgroups (e.g., **Joachims, "Training Linear SVMs: Theory and Algorithms"**) . 
In other works, identifying disparate reporting rates is itself is the central challenge (e.g., **Kull, "Learning Distributions with Adversarial Support"**).
On the other hand, an emerging body of literature from the human-computer interaction community develops the concept of \textit{contestability} (e.g., **Choudhury, "Understanding the Role of Contestability in Human-Computer Interaction"**); though contestability is still typically understood in terms of individual outcomes, we see our work as one possible path to implementing this ideal\iftoggle{icml}{.}{, with an eye towards empowering contestability at larger scale.


\paragraph{Fairness auditing as hypothesis testing.} 
\iftoggle{icml}{Existing proposals to formalize fairness auditing via hypothesis testing mainly consider batch settings (i.e. post-hoc or pre-deployment) **Kamiran, "Data Preprocessing Techniques for Classification without Discrimination"**.}{  make a direct connection between legal AI fairness audit requirements and hypothesis testing, though they mainly consider a post-hoc setting. **Begleiter, "On the Role of Formal Methods in Regulatory Compliance"** take a multiple testing approach for handling a large number of groups, but this test is again post-hoc (or entirely pre-deployment).} Two more closely related works are that of **Feldman, "Certifying and Removing Disparate Impact"** and **Hardt, "Equality of Opportunity in Supervised Learning"**, who propose applying sequential hypothesis tests with the explicit goal of identifying problems in deployed systems in real time.
However, as neither of these works study a reporting model, we propose fundamentally different tests: they test equality of means across different groups, while we compare within groups. 

\iftoggle{icml}{}{
\input{a0-additional-related}
}