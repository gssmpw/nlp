\section{Supplemental Material for Experimental Results}
\label{app:expts}

\subsection{Supplemental material for vaccine side effect experiments}
\textbf{Data sources.}
The Vaccine Adverse Event Reporting System (VAERS) is a national adverse event incident database for U.S.-licensed vaccines, co-managed by the Centers for Disease Control and Prevention (CDC) and the U.S. Food and Drug Administration (FDA)~\citep{chen1994vaccine,shimabukuro2015safety}. 
The database is a combination of voluntary reports from patients that have received the vaccine, as well as mandatory reports from vaccine manufacturers and healthcare professionals. 
For this case study, we filter the set of publicly-available reports from VAERS to reports about the COVID-19 vaccine with a complaint keyword including ``myocarditis.'' As for how a database administrator would have known to focus on myocarditis \textit{a priori}, one might imagine, for example, that the series of case studies found in early 2021 raised the alarm that more systematic analysis was warranted for myocarditis in particular.

To determine per-demographic base rates, i.e. to compute $\{\Basegroup\}_{G\in\Groups}$, we utilize 
VaxView, a government dataset tracking national vaccine coverage (publicly accessible \href{https://www.cdc.gov/vaccines/data-reporting/}{here}), managed by the CDC. \jessedit{VaxView does not track vaccination rates by granular subgroups, only providing coverage rates disaggregated by age, gender, and ethnicity separately. We thus impute the vaccination rates for intersections of subgroups (e.g., ``12-17, M'') by multiplying the known marginal rates (i.e., $\mu_{(12-17, M)}^0 := \mu_{(12-17)}^0 \cdot \mu_{(M)}^0$).}

\textbf{Additional results: Impact of Bonferroni correction.}
In  Figure \ref{fig:covid-beta2-bonf}, we show the same axes as in Figure \ref{fig:covid-all-beta2}---number of reports to first alarm on the x-axis, vs. number of permutations in which an alarm was triggered on the y-axis---for the three algorithms at $\beta=2$. Here, we show the impact of Bonferroni correction for multiple hypothesis testing on stopping time. As expected, the invalid version of the test, which has a lower threshold for rejecting each null, stops more quickly for all three algorithms (dashed, lighter). 
The difference between the invalid version and the valid version (solid, darker) is relatively minor, though the impact varies across algorithms. 
% For the betting-style test, recall that the stopping-time upper bound in Theorem \ref{thm:power_evals} suggested that the impact of adding the Bonferroni correction was an additive factor of $\mathcal{O}(\nicefrac{\log(|\Groups|)}{\Delta_{\mathrm{max}}^2})$. 
% In the plot for the betting-style test, we add a line (dotted, lightest) that adds exactly $\lceil\nicefrac{\log(|\Groups|)}{\Delta_{\mathrm{max}}^2}\rceil$  to the actual stopping time of the algorithm without the Bonferroni correction; note that the actual stopping time of the betting-style algorithm with the multiple testing correction is much faster than what is suggested by the theoretical upper-bound.

\begin{figure*}[h]
\hfill
\begin{minipage}[c]{0.32\textwidth}
    \includegraphics[width=0.97\linewidth]{plots/covid-bonf-asymp.png}
\end{minipage}
\hfill
\begin{minipage}[c]{0.32\textwidth}
    \includegraphics[width=0.97\linewidth]{plots/covid-bonf-finite.png}
\end{minipage}
\hfill
\begin{minipage}[c]{0.32\textwidth}
    \includegraphics[width=0.97\linewidth]{plots/covid-bonf.png}
\end{minipage}
\hfill
    \caption{\small Impact of multiple hypothesis correction on stopping time across algorithms. As in Figure \ref{fig:covid-all-beta2}, each point on the plot reflects the number of trials (out of 100) in which a rejection has occurred by time $t$. 
    In all plots, the lighter, dashed line reflects stopping time of the invalid test that does not correct for multiple testing; the dark, solid line reflects stopping time of the valid test including a Bonferroni correction. 
    % In the plot for the betting-style test (right), the lightest, dotted line plots Theorem \ref{thm:power_evals}'s upper bound on the expected impact of Bonferroni (i.e., $\nicefrac{\log(|\Groups|)}{\Delta_{\mathrm{max}}^2}$ additional steps from the stopping time of the invalid test).
    }
    \label{fig:covid-beta2-bonf}
\end{figure*}

\subsection{Supplemental material for mortgage allocation experiments}

\iftoggle{icml}{\textbf{Data sources.}
We use the data (and preprocessing code) of \citet{martinez2021secret}, which uses 2019 data from the HMDA.\footnote{The Consumer Financial Protection Bureau (CFPB) collects and publishes this data from financial institutions annually, with a two-year lag; the report (and our work) uses 2019 data which is finalized as of Dec. 31, 2022. The most recent year for which data is available is 2022, though it is available for edits through 2025.}
The analysis of \citet{martinez2021secret} used the full year of data from 2019; we reduce the dataset to applications for conventional loans at three of the largest lending institutions, from applicants who have positive income. We assume that reports will only come from applicants whose loans were denied; in all, there are 183k applicants which satisfy these criteria. 

\textbf{Reporting models.}
The dataset gives several levels of financial health as measured by DTI---in ascending order, are``Struggling'', ``Unmanageable,'' ``Manageable,'' and ``Healthy.'' 
Modeling the idea that reporting behavior may be related to financial health, we use these categories to simulate the following possible patterns of reporting.
\begin{enumerate}[(1)]
    \item \textit{Correlated:} The likelihood of reporting increases with financial health. That is, ``Healthy'' denials report with probability 0.9, ``Manageable'' with probability 0.5, ``Unmanageable'' with probability 0.3, and ``Struggling'' with probability 0.1. Under this reporting model, the 95th-percentile (among all groups) $\nicefrac{\rho_G}{\rho}$ is 1.2, and $\max_G \nicefrac{\rho_G}{\rho} = 1.4.$
    \item \textit{All Denials:} All denials submit reports regardless of financial health. Under this reporting model, the 95th-percentile $\nicefrac{\rho_G}{\rho}$ is 1.5, and $\max_G \nicefrac{\rho_G}{\rho} = 2.3.$
    \item \textit{Anti-Correlated:} The (unlikely) case where individuals with worse financial health are more likely to report, i.e. ``Healthy'' denials report with probability 0.1, ``Manageable'' with probability 0.5, ``Unmanageable'' with probability 0.7, and ``Struggling'' with probability 0.9. Under this reporting model, the 95th-percentile $\nicefrac{\rho_G}{\rho}$ is 1.8, and $\max_G \nicefrac{\rho_G}{\rho} = 2.7.$
\end{enumerate}
Note that the ground-truth ratios $\nicefrac{\rho_G}{\rho}$ would have been unknown at the time that a practitioner sets $\beta$; we are able to determine these only because we have full information about the dataset and control over the reporting model. However, these computations suggest that the assumptions on reporting rates implied by the settings of $\beta = \{1.2, 1.4, 1.6, 1.8\}$ are generally reasonable, especially after considering outliers---note the disparity between the 95th-percentile vs max ratios of $\nicefrac{\rho_G}{\rho}$, especially for the \textit{All Denials} and \textit{Anti-Correlated} models.

\textbf{Additional results: Full version of Table \ref{table:hmda}.} In Table \ref{table:hmda-all}, we report stopping times and relative risks for all algorithms, reporting models, and $\beta$.
While overall trends across algorithms and reporting models are consistent across values of $\beta$, seeing these results for different $\beta$ highlights an additional insight. 
While it is to be expected that stopping times (and the ground-truth relative risks) should increase with $\beta$, the increase in stopping time is dramatic---by sometimes by orders of magnitude---even for what appear to be relatively small changes in $\beta$. Moreover, the \textit{disparity} in stopping time across reporting models also increases dramatically with $\beta$. In fact, for $\beta=1.8$, some combinations of reporting and algorithm do not stop within 40,000 steps in at least one trial. 
Altogether, these results provide further evidence that in practice, the betting-style test balances stopping time with identifying highly-risky groups, and that the initial $\beta$ should be fairly small, and increased over time. 

\begin{table*}[ht]
\centering
\begin{tabular}{c|c|cc|cc|cc}
  \toprule
  \multirow{2}{*}{} & \multirow{2}{*}{\small{Reporting model}} &
  \multicolumn{2}{c|}{\textit{Asymptotic Z-test}} &
  \multicolumn{2}{c|}{\textit{Finite-sample Z-test}} &
  \multicolumn{2}{c}{\textit{Betting-style test}}~\\
  & & {\small{Stopping time}} & {\small{Rel. risk}} & {\small{Stopping time}} & {\small{Rel. risk}} & {\small{Stopping time}} & {\small{Rel. risk}} \\
  \hline\hline
  \multirow{3}{*}{$\beta=1.2$} & \textit{Correlated} & 85 & 1.62 & 2002 & 1.67 & 638 & 1.70 \\
  & \textit{All Denials} & 69 & 1.59 & 1546& 1.60 & 519 & 1.65 \\
  & \textit{Anti-Corr.} & 60 & 1.50 & 1065 & 1.53 & 403 & 1.65 \\
  \hline
    \multirow{3}{*}{$\beta=1.4$} & \textit{Correlated} & 316 & 1.69 & 4306 & 1.73 & 1542 & 1.77 \\
  & \textit{All Denials} & 163 & 1.62 & 3214 & 1.72 & 1073 & 1.72 \\
  & \textit{Anti-Corr.} & 95 & 1.47 & 2215 & 1.66 & 718 & 1.68 \\
  \hline
    \multirow{3}{*}{$\beta=1.6$} & \textit{Correlated} & 886 & 1.68 & 11755 & 1.73 & 4157 & 1.82 \\
  & \textit{All Denials} & 586 & 1.69 & 7410 & 1.72 & 2714 & 1.75 \\
  & \textit{Anti-Corr.} & 271 & 1.05 & 4668 & 1.72 & 1688 & 1.71 \\
  \hline
    \multirow{3}{*}{$\beta=1.8$} & \textit{Correlated} & 4959 & 1.74 & ---$^1$ & --- & 16425$^2$ & 1.98 \\
  & \textit{All Denials} & 2703 & 1.72 & 29751$^3$ & 1.73 & 9977 & 1.89 \\
  & \textit{Anti-Corr.} & 935 & 1.58 & 14072 & 1.73 & 4629 & 1.76 \\
  \hline
\end{tabular}
\vspace{1em}
\caption{Average stopping times (i.e. time to first alarm) and true relative risk (i.e., $\frac{\Pr[A_i = 0, Z_i = 1 \mid X_i \in G]}{\Pr[A_i = 0, Z_i = 1]}$) of first-identified group over 100 random permutations, for varying $\beta$, across algorithms and reporting models.
For $\beta=1.8$, some combinations of algorithm/reporting model failed to stop within 40,000 steps for some trials: 
$^1$stopped in 0/100 trials, $^2$stopped in 99/100 trials, $^3$stopped in 76/100 trials.}
\label{table:hmda-all}
\end{table*}
}{}