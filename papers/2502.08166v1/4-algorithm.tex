\section{Identifying Subgroups with High Reporting Overrepresentation}
\label{sec:algs}

How might the test proposed in Equation \eqref{eq:htest} be carried out in practice, with reports arriving over time, and what properties might we want for such a test? In this section, we provide two ways to instantiate this sequential hypothesis test. For each, we provide two types of guarantees. The first is (sequential) $\alpha$-validity, which, roughly speaking, guarantees correctness of groups identified in $\FlagG$. More formally, we say that a sequential test is valid for a single group $G$ at level $\alpha$ if $\Pr[\exists t: \NullH \; \mathrm{ rejected}] \leq \alpha$ when $\NullH$ is true. Because we are testing for all groups in $\Groups$ simultaneously, we say that a sequential test is valid with respect to all groups $\Groups$ if $\Pr[\exists t, \exists G: \NullH \; \mathrm{erroneously} \; \mathrm{rejected}] \leq \alpha$.

The second type of guarantee is power, which guarantees that the test will identify a harmed group, if one exists. In particular, we are interested in the \textit{stopping time} $T$ of the test, which is the number of samples required for the test to reject the first null, i.e. to raise an alarm for any group. 


At a high level, our algorithms for 
conducting this test
follow the protocol outlined in Algorithm \ref{alg:abstract}. 
\iftoggle{icml}{}{Every report $X_t$ can be considered a binary vector indexed by the groups in $\Groups$; the $G$ component of this vector is equal to $\1[X_t \in G]$. If there was only one group, we could run a sequential hypothesis test to determine whether $\mu_G$ was unacceptably large. With multiple groups, we can run $|\Groups|$ separate sequential hypothesis tests in parallel, one for each group, and correct the confidence levels for multiple hypothesis testing.}
For each group $G$, we maintain a test statistic $\Logwealth_t^G$ that is updated as reports $X_t$ are received over time. At each time $t$, each of these test statistics are compared to a threshold $\theta_t(\alpha)$, which depends on the test level $\alpha$; the null hypothesis $\NullH$ for group $G$ is rejected if $\Logwealth_t^G > \theta_t(\alpha)$. 
For ease of exposition, Algorithm \ref{alg:abstract} is written so that groups corresponding to rejected nulls are collected in a set $\FlagG$; in practice, a database administrator may choose to stop the test entirely as soon as one harmed group has been found.  

Correcting for multiple hypothesis testing across groups is handled by a simple Bonferroni correction---that is, given a particular test level $\alpha$, we test each individual group $G$ at level $\nicebonf$ rather than level $\alpha$.
% \footnote{The Bonferroni correction is also convenient for dealing with the correlations across hypotheses.} 
Though Bonferroni corrections often seem onerous in non-sequential settings, we show that, for sequential problems, the Bonferroni correction incurs only a modest increase in stopping time. 

In Section \ref{subsec:ztest}, we give a simple sequential Z-test-inspired approach which leverages a finite-time Law of the Iterated Logarithm. Section \ref{subsec:e-val-alg} presents a more complicated algorithm that leverages recent developments in anytime-valid inference.
The main differences in each algorithm lie in how they implement Lines 1 and 6 of Algorithm \ref{alg:abstract}---that is, how test statistics and thresholds are computed. 
For each instantiation of Algorithm \ref{alg:abstract}, we show validity and power guarantees.
Omitted proofs are given in Appendix \ref{app:seq-proofs}.


\begin{algorithm2e}
\caption{General protocol for testing overrepresentation}\label{alg:abstract}
\LinesNumbered
\KwIn{Set of groups $\Groups$; base preponderances $\{\Basegroup\}_{G \in \Groups}$; test level $\alpha$; relative strength $\beta$}
Initialize test statistic $\Logwealth_0^G$ for every $G \in \Groups$ and set threshold $\theta_0(\alpha)$\;
Initialize set of rejected nulls (flagged groups) $\FlagG := \emptyset$\;
\For{$t = 1, 2, \dots$}
{
See report $X_t$\;
\For{$G \in \Groups$}
{   Update test statistic $\Logwealth_t^G$ and compute threshold $\theta_t(\alpha)$\;
    \If{$\Logwealth_t^{G} \geq \theta_t(\alpha)$}{
    Add $G$ to $\FlagG$ and take requisite action for $G$, if applicable.}
    }
}
\end{algorithm2e}

\subsection{Sequential Z-test}
\label{subsec:ztest}
One simple observation that arises from the model presented in Section \ref{sec:model} is that if each report $X_t$ is drawn i.i.d. from some underlying distribution, then one might expect to be able to use concentration as a tool to conduct this test, since as time passes, the fraction of reports within the database from group $G$ should converge to the true mean $\mu_G$. We refer to this style of approach as a sequential Z-test, as it relies on measuring deviation from the mean. 

\paragraph{Updating the test statistic $\Logwealth_t^G$.}
Given this intuition, the test statistic is a simple count of the number of times a report from each group has been seen, i.e. (with $\Logwealth_0^G = 0$),
\begin{equation}
    \label{eq:ztest_update}
    \Logwealth_t^G \leftarrow \Logwealth_{t-1}^G +\1[X_t\in G]. 
\end{equation}
\paragraph{Setting the threshold $\theta_t(\alpha)$.}
Given the way that $\Logwealth_t^G$ accumulates evidence, one natural way to construct the threshold at each $t$ is to use the mean under the alternative, plus a correction term for both sample complexity and repeated testing over time. With $C$ set to either $\sqrt{\beta\Basegroup(1 - \beta\Basegroup)}$ or $\nicefrac12$, the threshold (including a Bonferroni correction) is 
\begin{equation}
    \label{eq:ztest_thresh}
    \theta_t(\alpha) := t \cdot \beta\Basegroup + C \sqrt{
    2.07 t
    \ln\left(|\Groups| \frac{(2 + \log_2(t))^2}{\alpha}\right)
    }.
\end{equation}

\paragraph{Theoretical guarantees.}
Our first guarantee is a bound on the probability that any group is incorrectly flagged. 
\begin{restatable}[Validity]{theorem}{ztestvalidity}
\label{thm:validity_ztest}
Running Algorithm~\ref{alg:abstract} with $\theta_t(\alpha)$ as in Equation \eqref{eq:ztest_thresh}, setting $C = \nicefrac
12$, and $\Logwealth_t^G$ updated as in Equation~\eqref{eq:ztest_update}, guarantees that 
the probability that $\FlagG$ will ever contain a group $G$ where $\NullH$ is true is at most $\alpha$, i.e. 
\[
\Pr\left[\exists t: \exists G \in \FlagG \text{ s.t. } \NullH \text{ holds}\right] \leq \alpha.
\]
\end{restatable}

The choice of $C$ affects the nature of the guarantee: the true, finite-sample anytime-validity guarantee requires $C = \nicefrac12$. If instead $C = \sqrt{\beta\Basegroup(1 - \beta\Basegroup)}$, then, strictly speaking, the guarantee holds only asymptotically. However, a higher value of $C$ affects stopping time unfavorably, so the asymptotic approximation can be useful practically. In this case, care must be taken to ensure that the algorithm does not erroneously reject too early due to noise; one way to implement this is to mandate a minimum stopping time. 

Finally, we give a stopping time guarantee for this test.
\begin{restatable}[Power]{theorem}{ztestpower}
\label{thm:power_ztest}
Let $T$ be the stopping time of 
Algorithm~\ref{alg:abstract} with $\theta_t(\alpha)$ as in Equation \eqref{eq:ztest_thresh}, $C = \nicefrac
12$, and $\Logwealth_t^G$ as in Equation~\eqref{eq:ztest_update}.
Let $\Delta_{\max} = \max_{G \in \Groups} \mu_G - \beta\Basegroup.$
If $\Delta_{\max} > 0$, then $\Pr[T < \infty] = 1$. 
Furthermore, with probability $1 - \nicebonf$, we have $T  \leq \widetilde{\mathcal{O}} \left( \frac{\ln(|\Groups|) + \ln(1/\alpha)}{\Delta_{\max}^2}\right)$, and for any $\delta \in (0, \nicebonf)$, we have with probability at least $1 - \delta$ that
$T  \leq \widetilde{\mathcal{O}} \left( \frac{\ln(1/\delta)}{\Delta_{\max}^2}\right)$.
\end{restatable}
\subsection{Betting-style approach}
\label{subsec:e-val-alg}
We refer to our second algorithm as a \textit{betting-style} approach, due to the way we construct our test statistics \citep{shafer2021testing, waudby2024estimating, vovk2021values, chugg2024auditing}; one way to interpret this approach is that the test ``bets against'' the null hypothesis $\NullH$ being true. We direct the reader to these references for more detailed technical exposition\iftoggle{icml}{.}{ and connection with literature on martingales, gambling, and finance. For us, these methods provide an adaptive algorithm which find a middle ground between the two approaches in the previous section: the betting-style approach achieves finite-sample validity but empirically terminates quickly when the null is false.}

\paragraph{Updating the test statistic $\Logwealth_t^G$.}
As in the previous approach, we let $\Logwealth_t^G$ represent some accumulated amount of evidence against the null hypothesis $\NullH$ by time $t$, with a higher value of $\Logwealth_t^G$ corresponding to greater level of evidence.\footnote{The quantity $\exp(\Logwealth_t^G)$ can also be referred to as an \textit{e-value} \citep{vovk2021values}, a measure of evidence against a null hypothesis similar to a p-value.} We initialize $\Logwealth_0^G = 0$, and use the update rule 
\begin{equation}\label{eq:wealth_update}
\Logwealth_t^G \leftarrow \Logwealth_{t-1}^G + \ln\left(1 + \lambda_t^G (\mathbf{1}_{X_t\in G} - \beta \mu_G^0)\right),
\end{equation}
with $\lambda_1^G, \ldots, \lambda_t^G \in [0, 1]$. 
\iftoggle{icml}{}{Note that this expression is similar to the running sum used in Section~\ref{subsec:ztest}.}
Here, the algorithm accumulates a nonlinear function, with an adaptive parameter $\lambda_t^G$ that weights the influence of each new sample. 
Our setting of $\lambda_t$ is motivated by the goal of minimizing stopping time under the alternative, and thus to maximize $\Logwealth_t^G$. 
\iftoggle{icml}{}{Taking $\lambda_{t} = 0$ means $\Logwealth_t^G$ remains the same regardless of what new information is received at time $t$. On the other hand, $\lambda_{t} = \nicefrac{1}{\beta\mu_G^0}$ means that if we receive evidence in accordance with $\NullH$ then $\Logwealth_t^G$ will decrease substantially; but, if we instead receive evidence \textit{against} the null, i.e. $X_{t}\in G$, we maximally increase $\Logwealth_{t}^G$.}
Drawing from the well-studied problem of portfolio optimization in the online learning literature \citep{cover1991universal,
zinkevich2003online, hazan2016introduction}, we use Online Newton Step \citep{hazan2007logarithmic, cutkosky2018black} to ensure that $\Logwealth_t^G$ is not too far from the best achievable in hindsight. This results in the following update for $\{\lambda_t\}_{t \geq 1}$:
\begin{equation}\label{eq:bet_update}
\lambda_{t+1}^G \gets \mathop{\text{Proj}}\limits_{\left[0, 1\right]}\left(\lambda_t^G + \tfrac{2}{2- \ln(3)}\cdot\tfrac{z_t}{1 + \sum_{s \in [t]}z_{s}^2}
   \right),
\end{equation}
where $z_t = \frac{\1[X_t\in G ]- \beta \Basegroup}{1 + \lambda_t^G(\1[X_t\in G]- \beta \Basegroup)}$, and $\lambda_0 = 0$.\footnote{The constant $\frac
{2}{2 - \ln(3)}$ is due to \citet{cutkosky2018black}, who give a tighter version of ONS than in \citet{hazan2007logarithmic}.}

\paragraph{Setting the threshold $\theta_t(\alpha)$.}
Unlike the sequential Z-test, we use the same threshold for all timesteps. Including a Bonferroni correction, we use $\theta_t(\alpha) :=   \ln(\nicefrac{|\Groups|}{\alpha})$ for all $t$; the motivation for this setting will become clear in our discussion of Theorem \ref{thm:validity_evals}.

\paragraph{Theoretical guarantees.}
We first give a validity guarantee that is essentially identical to the Sequential Z-test. 

\begin{restatable}[Validity]{theorem}{evalsvalidity}\label{thm:validity_evals} Running Algorithm~\ref{alg:abstract} with $\theta_t(\alpha) = \ln{(\nicefrac{|\Groups|}{\alpha})}$ and $\Logwealth_t^G$ updated as per Equations~\eqref{eq:wealth_update} and \eqref{eq:bet_update} guarantees that 
the probability that $\FlagG$ will ever contains a group $G$ where $\NullH$ is true is at most $\alpha$, i.e. 
\[
\Pr\left[\exists t: \exists G \in \FlagG \text{ s.t. } \NullH \text{ holds}\right] \leq \alpha.
\]
\end{restatable}
This result follows directly from the prior work referenced at the beginning of this section. At a high level, every sequence $\{\exp(\Logwealth_t^G)\}_{t \geq 1}$ is a non-negative super-martingale under $\NullH$; informally, this means that under the null hypothesis, the sequence $\{\exp(\Logwealth_t^G)\}_{t \geq 1}$ should be non-increasing, in expectation. 
This allows us to apply Ville's inequality, which guarantees that it is unlikely that $\exp(\Logwealth_t^G)$ ever becomes too large under $\NullH$. More specifically, for any $\alpha \in (0,1)$, under the null, $\Pr[\exists t: \exp(\Logwealth_t^G) > 1/\alpha] \leq \alpha $. Thus, maintaining a threshold of $\theta_t(\alpha) = \ln(\nicefrac{|\Groups|}{\alpha})$ is sufficient to provide a per-hypothesis $\nicebonf$-validity guarantee, and thus $\alpha$-validity overall. 

We also provide the following bound on stopping time; see Appendix \ref{app:eval} for additional discussion of the $\Logwealth_\star$ notion of gap. 
\begin{restatable}[Power]{theorem}{evalspower} 
\label{thm:power_evals}
Let $T$ be the stopping time of Algorithm~\ref{alg:abstract} with $\theta_t(\alpha) = \ln{(|\Groups|/\alpha)}$ and $\Logwealth_t^G$ updated as per Equations~\eqref{eq:wealth_update} and \eqref{eq:bet_update}. If $\max_{G \in \Groups} \mu_G - \beta\Basegroup > 0$, then, we have that $\Pr[T < \infty] = 1$. Furthermore, 
\[
\E[T] \leq \mathcal{O}\left(\frac{1}{\Logwealth_\star^2} + \frac{\ln(|\Groups|) + \ln(1/\alpha)}{\Logwealth_\star}\right)
\]
where $\Logwealth_\star := \max_{G\in\mathcal{G}, \lambda\in[0,1]}\E[\ln(1+ \lambda(\mathbf{1}_{X_t\in G} -\beta\Basegroup))]$ is the maximal expected one-step increase in $\omega_t^G$ over all groups and choices of $\lambda$.
\end{restatable}
We conclude this section with two further remarks on Theorems \ref{thm:power_ztest} and \ref{thm:power_evals} in the context of our test. First, our modeling in Section~\ref{sec:interpretation} measures severity of harm via a \textit{multiplicative} factor of overrepresentation.
However, both notions of gap in Theorems \ref{thm:power_ztest} and \ref{thm:power_evals} also on the absolute size of the group $\mu_G$.
Thus, for two groups $G$ and $G'$ with identical multiplicative gaps, i.e. $\nicefrac{\mu_G}{\Basegroup} = \nicefrac{\mu_{G'}}{\mu_{G'}^0}$, the test would stop faster in expectation for $G$ if and only if $\Basegroup > \mu_{G'}^0$. That is, if two groups are ``harmed'' to the same extent, both algorithms will identify the larger one first. 

Second, for both tests, the Bonferroni correction results in only an additive factor ($\nicefrac{\ln(|\Groups|)}{\Delta_{\max}^2}$ in Theorem \ref{thm:validity_ztest}, and $\nicefrac{\ln(|\Groups|)}{\omega_\star}$ in Theorem \ref{thm:validity_evals}) in stopping time over the scenario where we had only been testing the one group with the largest gap. 
This means that, in terms of worst-case guarantee on stopping time, the contribution of the Bonferroni correction is small relative to the contribution of the test level $\alpha$ and, especially, to the gap. In fact, the impact of Bonferroni on real-world data appears to be much smaller even than this additive term.
