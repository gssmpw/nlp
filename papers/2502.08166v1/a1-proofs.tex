\section{Omitted Proofs}
\label{app:seq-proofs}
\iftoggle{icml}{
\subsection{Omitted proofs from Section \ref{sec:model}}
We prove Proposition \ref{prop:relativerisk-conversion}, restated below. 
\propRR*
\begin{proof}[Proof of Proposition \ref{prop:relativerisk-conversion}]
First, note that by definition of $\rho$, $\rho_G$, and $\RR_G$, we have 
\[
\rho_G \leq b \cdot \rho \iff \frac{\Pr[R = 1 \mid G ]}{\Pr[Y = 1 \mid G]} \leq b \cdot \frac{\Pr[R = 1]}{\Pr[Y = 1]} \iff \RR_G \geq \frac{\Pr[R = 1 \mid G]}{\Pr[R = 1]} \cdot \frac{1}{b}. 
\]
By Bayes' rule, $\frac{\Pr[R = 1 \mid G]}{\Pr[R = 1]} = \frac{\Pr[ G \mid R = 1]}{\Pr[G]} = \frac{\mu_G}{\Basegroup}$; furthermore, by assumption, we have $\frac{\mu_G}{\Basegroup} \geq \beta$. 
The result follows from combining with the previous display. 
\end{proof}
We prove Proposition \ref{prop:reporting-conversion}, restated below. 
\propIR*
\begin{proof}[Proof of Proposition \ref{prop:reporting-conversion}]
Recall that we have defined $\mu_G = \Pr[G \mid R]$, and $\Basegroup = \Pr[G]$ is known by Assumption \ref{assn:ref}.
By Bayes' rule, we have
$    \mu_G = \Pr[G \mid R] = \frac{\Pr[G]\Pr[R \mid G]}{\Pr[R]} =  \Basegroup\frac{\Pr[R \mid G]}{r},$
Now, let us decompose $\Pr[R \mid G]$ by ``true'' reports ($\Badevent = 1$) and ``false'' reports ($\Badevent = 0$). 
By the law of total probability,
$
    \Pr[R \mid G] 
    = r \cdot \left(\Truereportrate \Actualgroup + \Falsereportrate(1-\Actualgroup )\right)
$; more precisely, 
\begin{align*}
    \frac1r\Pr[R \mid G] &= \Pr[R \mid G, \Badevent = 1]\Pr[\Badevent \mid G]  + \Pr[R \mid G,  \Badevent =0](1-\Pr[\Badevent \mid G] )
    \\&= \Truereportrate \Actualgroup + \Falsereportrate(1-\Actualgroup )
    \\&= \Falsereportrate + \Actualgroup (\Truereportrate-\Falsereportrate);
\end{align*} 
combining this with the Bayes' rule computation, cancelling the $\frac1r$ factor, gives us $
    \Actualgroup  = \frac{\frac{\mu_G}{\Basegroup} - \Falsereportrate}{\Truereportrate-\Falsereportrate}.
$
The result follows from the assumption that $\nicefrac{\mu_G}{\Basegroup} \geq \beta.$
\end{proof}
}{}

\subsection{Omitted proofs for Sequential Z-test}
We prove Theorem \ref{thm:validity_ztest}, restated below. 
\ztestvalidity*

To prove this result, we will use a foundational result known as Ville's inequality \citep{ville1939etude}.
\begin{theorem}[Ville's inequality]
\label{thm:ville}
    Let $\{M_t\}_{t \in \mathbb{N}^+}$ be a non-negative supermartingale, i.e. for all $t$, $M_t \geq 0$, and $\E[M_{t+1} \mid \mathcal{F}_t] \leq M_t$, where $\mathcal{F}_t$ is the filtration (history) of all realizations of randomness up to and including time $t$. Then, for any $x \in (0,1)$, we have $\Pr[\exists t: M_t > \nicefrac{\E[M_0]}{x}] \leq x$.
\end{theorem}

The central thrust of our proof of Theorem \ref{thm:validity_ztest} is due to \citet{koolen2017quick} (which itself draws from \citet{balsubramani2014sharp}, and is a refinement of \citet{jamieson2014lil}); we reproduce the argument in the context of our work below, though we emphasize that we do not claim the proof technique as ours.
\begin{proof}[Proof of Theorem \ref{thm:validity_ztest}]
It is sufficient to show that for any group $G$ where $\NullH$ holds, we have $\Pr[ \exists t: G \in\FlagG] \leq \nicefrac{\alpha}{|\Groups|}$; the statement of the theorem follows from the Bonferroni correction over all $|\Groups|$ hypotheses. 

Ville's inequality (Theorem \ref{thm:ville}) appears similar in form to the statement we hope to prove; we therefore seek to transform our test statistic $\Logwealth_t^G = \sum_{s \in [t]}\1[X_s \in G]$ into a quantity that can be interpreted as a (non-negative) supermartingale. Although $\{\Logwealth_t^G\}_{t \in \mathbb{N}^+}$ is by itself clearly not a non-negative supermartingale, each $\Logwealth_t^G$ is the sum of $t$ Bernoulli trials with mean $\mu_G$, and Bernoulli random variables are sub-Gaussian with variance parameter $\nicefrac14$. Each $\Logwealth_t^G$ therefore satisfies the property that $\E[\exp(\eta(\Logwealth_t^G - \E[\Logwealth_t^G])] \leq \exp(\eta^2/8)$. 

This holds for any $\eta$, so we will construct a distribution $\phi$ on $\eta$ and use it to construct a martingale $M_t$. In particular, note that under $\NullH$, $\E[\Logwealth_t^G] < t \cdot \beta\Basegroup$. Thus, we 
let $S_t \coloneqq \Logwealth_t^G - \E[\Logwealth_t^G] =  \Logwealth_t^G - t \beta\Basegroup$. 
We will let 
$M_t = \int \phi(\eta) \exp(\eta S_t - t\eta^2/8) d\eta$.
Then, for any distribution $\phi$, $\{M_t\}_{t \in \mathbb{N}^+}$ is a non-negative supermartingale with respect to the randomness in realizations of reports $X_t$.
To see this, we have 
\begin{align*}
    \E[M_{t+1} \mid \mathcal{F}_t] &= \E\left[\int \phi(\eta)\exp\left(\eta(S_t + \1[X_{t+1} \in G] - \beta\Basegroup) - \tfrac{(t+1)\eta^2}{8}\right) d\eta \bigmid \mathcal{F}_t\right]
    \\&= \int \phi(\eta) \exp\left(\eta S_t - \tfrac{t\eta^2}{8}\right) \E\left[\exp\left(\eta(\1[X_{t+1} \in G] - \beta\Basegroup) - \tfrac{(t+1)\eta^2}{8}\right) \bigmid \mathcal{F}_t\right] d\eta 
    \\&\leq \int \phi(\eta) \exp\left(\eta S_t - \tfrac{t\eta^2}{8}\right) d\eta 
    \\&= M_t,
\end{align*}
where the inequality is due to $\frac1t\E[\Logwealth_t^G] \leq \beta\Basegroup$ and subgaussianity. It thus remains to use this martingale to compute an appropriate threshold $\theta_t(\alpha)$ on $\Logwealth_t^G$. 

$M_t$ will satisfy the conditions of Theorem \ref{thm:ville} for any choice of $\phi$, including one which puts point mass of 1 on $\eta = \eta'$ and 0 elsewhere, i.e. $\phi(\eta') = 1$ and $\phi(\eta) = 0$ for any $\eta \neq \eta'$. One path towards establishing the threshold $\theta_t(\alpha)$ is to simply pick one value of $\eta$; however, such an $\eta$ cannot depend on $t$ and would thus result in a suboptimal threshold. 
Instead, we will construct $\phi$ such that it is a discrete distribution, indexed by $i \in \mathbb{N}^+$, so that $\eta$ takes values $\eta_1, \dots, \eta_i$ with probability $\phi_1, \dots, \phi_i$; this allows each $\eta_i$ to depend on $t$ and therefore more finer-grained optimization of the threshold. Before committing to the exact distribution $\phi$, we first illustrate how $\phi_i$ and $\eta_i$ will be used in the threshold. 

Note that $M_t = \sum_{i \in \mathbb{N}^+} \phi_i\exp(\eta_iS_t - t\eta_i^2/8) \geq \max_i  \phi_i\exp(\eta_iS_t - t\eta_i^2/8)$, so for any $\delta$, we have 
\[
\{M_t \geq 1/\delta\} \supseteq  \{\max_i  \phi_i\exp(\eta_iS_t - t\eta_i^2/8) > 1/\delta \} = \left\{S_t \geq \min_i \left(\frac{t \eta_i}{8} + \frac{1}{\eta_i}\ln \frac{1}{\phi_i\delta}\right)\right\}, 
\]
and thus, picking $\theta_t(\alpha) = t \beta\Basegroup + \min_i \left(\frac{t \eta_i}{8} + \frac{1}{\eta_i}\ln \frac{1}{\phi_i\nicebonf}\right)$ would guarantee that $\Pr[\exists t: \Logwealth_t^G >  \theta_t(\alpha)] \leq \nicebonf$. 

Finally, we must commit to $\phi_i$, $\eta_i$, then optimize for $i$. Let $\phi_i = \frac{1}{i(i+1)}$ (note that $\sum_i \phi_i = 1$, so this is a valid distribution), $\eta_i = 2\sqrt{\frac{2\ln(1/\phi_i(\nicebonf))}{2^i } }$, and $i = \lfloor \log_2(t) \rceil$. For $i = \log_2(t)$ (without rounding), this would have yielded $\eta_i = 2 \sqrt{\frac{2 \ln((\log_2(t) + 1)(\log_2(t))/(\nicebonf))}{t}}$ and $\theta_t(\alpha) = \frac12\sqrt{2t \ln((\log_2t)(\log_2t + 1)/\nicebonf)}$. 
The statement follows from handling the numerical impact of rounding. 
\end{proof}
\begin{remark}
    A key constant in the proof of the version of the algorithm that is valid in finite samples is the subgaussian variance parameter, for which we used $\nicefrac14$ (and which propagates to a multiplicative factor of $\sqrt{1/4} = 1/2$ on the threshold). This is because the variance \textit{any} Bernoulli is at most $\nicefrac14$; however, this also motivates the choice of constant for the asymptotically-valid version of the test, which instead uses the variance parameter $\beta\Basegroup(1-\beta\Basegroup)$. 
\end{remark}

We now prove the power result. 
\ztestpower*
\begin{proof}
Let $\Gstar \coloneqq \arg\max_{G \in\Groups} \mu_G - \beta\Basegroup$ and let $\Delta \coloneqq \mu_{\Gstar} - \beta\Basestar$. Without loss of generality, we can consider only the test corresponding to $\Gstar$ (while still testing at level $\nicefrac{\alpha}{|\Groups|}$). 
Recall that for this instantiation of Algorithm \ref{alg:abstract}, the test statistic $\Logwealth_t^\Gstar = \sum_{s \in [t]} \1[X_s \in \Gstar]$ is simply the number of all reports belonging to $\Gstar$ by time $t$, and that stopping time $T$ is the first time where $\Logwealth_t^\Gstar$ surpasses the threshold $\theta_t(\alpha)$, i.e., $T \coloneqq \inf_{t \in {\mathbb{N}}^+} \Logwealth_t^\Gstar > t \beta\Basestar + \tfrac12 \sqrt{2.06 t \ln \left(|\Groups| \frac{(2 + \log_2(t))^2}{\alpha}\right)}$. For ease of notation, we will denote $C_1 \coloneqq \tfrac12\sqrt{2.06} = 0.718$ within this proof. 

For the first claim, it is sufficient to show $\liminf_{t \to \infty} \Pr[T > t] = 0$.\footnote{For a simple proof of this fact, see the solution to Problem 1.13 in \citet{bertsekas2008introduction}.} 
Recall that, by our modeling, we can consider $\Logwealth_t^\Gstar$ to be the sum of $t$ i.i.d. Bernoulli trials with parameter $\mu_\Gstar$.
Applying Hoeffding's inequality to this sum yields for any $t$ that
\begin{align*}
\Pr[T > t] &= 
\Pr\left[\Logwealth_t^\Gstar < t\beta\Basestar + C_1 \sqrt{t \cdot \ln \left(|\Groups| \frac{(2 + \log_2(t))^2}{\alpha}\right)}\right]
% \\&= \Pr\left[\E[\Logwealth_t^\Gstar] - \Logwealth_t^\Gstar > \Delta t + C_1 \sqrt{t \cdot \ln \left(|\Groups| \frac{(2 + \log_2(t))^2}{\alpha}\right)} \right]
% \\&\leq \exp\left(-\frac{2(t^2\Delta^2 + t C_1^2\ln(\frac{(2 + \log_2(t))^2}{\alpha}) - 2t\Delta C_1\sqrt{t \cdot \ln\left(|\Groups| \frac{(2 + \log_2(t))^2}{\alpha}\right)})}{t} \right)
\\&\leq \exp\left(-2\left(\Delta^2t - 2\Delta C_1 \sqrt{t \cdot \ln \left(|\Groups| \frac{(2 + \log_2(t))^2}{\alpha}\right)}\right) \right).
\end{align*}
Note that $\frac{\sqrt{t}\ln(\log_2(t))}{t} \to 0$; it can thus be seen that $\lim_{t \to\infty} \Pr[T > t] = \lim_{t \to \infty} \exp(-t) = 1$.

For the second claim, we apply Hoeffding's inequality again to see that for all $t$,  
\[
\Pr\left[\Logwealth_t^\Gstar \leq \E[\Logwealth_t^\Gstar] - C_1  \sqrt{ t \ln\left(\frac{(2 + \log_2(t))^2}{\delta}\right)}\right] \leq \Pr\left[\Logwealth_t^\Gstar \leq \E[\Logwealth_t^\Gstar] - \sqrt{\frac t2 \ln\left(\frac{ 1}{\delta}\right)}\right] \leq \delta.
\]
Thus, with probability at least $1-\delta$, for all $t$ simultaneously, $\Logwealth_t^\Gstar > t\mu_\Gstar - C_1  \sqrt{ t \ln\left(\frac{(2 + \log_2(t))^2}{\delta}\right)}$. 
The algorithm stops at time $t$ if and only if 
\[
t \mu_\Gstar - C_1  \sqrt{ t \ln\left(\frac{(2 + \log_2(t))^2}{\delta}\right)} > t \beta\Basestar + C_1  \sqrt{ t \ln\left(\frac{(2 + \log_2(t))^2}{\nicebonf}\right)}. 
\]
Rearranging, we have 
\[
\frac{t}{\left(\sqrt{\ln\left(\frac{(2 + \log_2(t))^2}{\nicebonf}\right)} + \sqrt{\ln\left(\frac{(2 + \log_2(t))^2}{\delta}\right)}\right)^2} \geq \frac{C_1}{\Delta^2}.
\]
Note that we can can upper bound the denominator of the left hand side as 
% \begin{align*}
$    \left(\sqrt{\ln\left(\frac{(2 + \log_2(t))^2}{\nicebonf}\right)} + \sqrt{\ln\left(\frac{(2 + \log_2(t))^2}{\delta}\right)}\right)^2 \leq 4 \ln\left(\frac{(2 + \log_2(t))^2}{\min(\nicebonf, \delta)}\right). $
% \end{align*}
Setting $\frac{t}{4 \ln\left(\frac{(2 + \log_2(t))^2}{\min(\nicebonf, \delta)}\right)} \geq \frac{C_1}{\Delta^2}$ and rearranging gives 
\begin{align}
\label{eq:tildeT}
\frac{t}{1 + \ln((2 + \log_2(t))^2)} \geq \frac{4C_1\ln(\max(\nicebonf, 1/\delta)}{\Delta^2}  
\end{align}
Thus, with probability $1-\delta$, the algorithm terminates at the smallest $t$ satisfying Equation \eqref{eq:tildeT}. The statement of the theorem follows from separating the two cases for $\delta < \nicebonf$ and $\delta \geq \nicebonf$, and noting that $\widetilde{O}$ notation suppresses the (negligible) log-log factor. 
\end{proof}

\subsection{Omitted proofs for betting-style algorithm}
\label{app:eval}

We first prove Theorem~\ref{thm:validity_evals}, restated for the sake of presentation.
\evalsvalidity*

\begin{proof} First note that for any $G$ for which $\NullH$ holds, the sequence $\{\exp(\Logwealth_t^G)\}_{t\geq 0}$ is a non-negative super-martingale. The non-negative property follows directly from the quantity being an exponential of a real (albeit possibly negative) number, while the fact that it is a super-martingale follows from the computations below:
\begin{align*}
    \E[\exp(\Logwealth_t^G)|\mathcal{F}_t] 
    &= \E[\exp(\Logwealth_{t-1}^G + \ln(1+\lambda_t^G(\1_{X_t\in G} - \beta \Basegroup)))|\mathcal{F}_t]
    \\&= \exp(\Logwealth_{t-1}^G) \cdot (1+\lambda_t^G(\E[\1_{X_t\in G}|\mathcal{F}_t] - \beta \Basegroup)) 
    \\&= \exp(\Logwealth_{t-1}^G) \cdot (1+\lambda_t^G(\mu_G - \beta \Basegroup) )
    \\&\leq \exp(\Logwealth_{t-1}^G) \cdot (1+\lambda_t^G(\beta \Basegroup - \beta \Basegroup) )
    \\&= \exp(\Logwealth_{t-1}^G),
\end{align*}
where the first equality follows by Eq.~\ref{eq:wealth_update}, the second by re-arranging and noting that all quantities except $\mathbf{1}_{X_t\in G}$ are completely determined by $\mathcal{F}_t$\footnote{In particular, it is imposed that $\lambda_t^G$ be 'predictable' which precisely implies that it is fixed given $\mathcal{F}_t$.}, and the third by definition (see Section~\ref{sec:model}). Finally, the inequality follows because $\mu_G \leq \beta\Basegroup$ under $\NullH$ and $\lambda_t^G \geq 0$.

Next, for any group $G$ such that $\NullH$ holds, we can apply Ville's inequality (Theorem~\ref{thm:ville}), plugging in the super-martingale $\{\exp(\Logwealth_t^G)\}_{t\geq 0}$ and taking $x$ to be $\theta_t(\alpha) = \log{(|\Groups|/\alpha)}$. This yields the following guarantee: 
\begin{align*}
\Pr[\exists t: \Logwealth_t^{G} > \log(|\Groups|/\alpha)] &= \Pr[\exists t: \exp(\Logwealth_t^{G}) > |\Groups|/\alpha] 
\\&\leq \E[\exp(\Logwealth_0^{G})] \cdot \alpha/|\Groups| 
\\&= \alpha/|\Groups|,
\end{align*}
where the final line follows because $\Logwealth_0^{G}$ is initialized as $0$ and hence $\exp(\Logwealth_0^{G})$ is equal to $1$.

Finally, by union bound we get the desired guarantee:
\begin{align*}
\Pr[\exists t: \exists G \in \FlagG \text{ s.t. } \NullH \text{ holds}] &\leq \sum_{G \text{ s.t. }\NullH \text{ holds}} \Pr[\exists t: \Logwealth_t^G > \log{(|\Groups|/\alpha)}] \\
&\leq |G \text{ s.t. }\NullH \text{ holds}| \cdot \alpha / |\Groups|\\
&\leq \alpha.
\end{align*}
\end{proof}

Before proving Theorem~\ref{thm:power_evals}, we first state and prove some helper results. 

\begin{claim}
\label{claim:logwealth-regret} For any $T\geq 4$ and group $G$, we have that the expected value over the randomness in the realizations of each $X_t$ of $\Logwealth_T^G$  defined as per Equations~\eqref{eq:wealth_update} and \eqref{eq:bet_update} can be lower bounded as
\[
\E[\Logwealth_T^G] \geq \E\left[\max_{\lambda\in[0, 1]} \Logwealth_T(\lambda)\right] - 2 \ln T.
\]
where we define $\Logwealth_T^G(\lambda)$ to be the quantity obtained by applying Equation~\eqref{eq:wealth_update} with $\lambda_t^G \coloneqq \lambda$ for all $t\in[T]$.
\end{claim}
\begin{proof} By the definition of regret we have that 
$\max_{\lambda\in[0, 1]} \Logwealth_T^G(\lambda) - \Logwealth_T^G \leq R_T$.
Rearranging and taking expectations, we have 
\[ \E[\Logwealth_T^G] \geq \E\left[\max_{\lambda\in[0, 1]} \Logwealth_T^G(\lambda)\right] -  \E[R_T].\]
Next, it can be verified that Equation~\eqref{eq:bet_update} implements the Online Newton Step algorithm for $\ln(1 + \lambda_t^G(\1_{X_t \in G} - \beta\Basegroup))$ (see Appendix C of \citet{cutkosky2018black}). We therefore have that $R_T \leq \frac{1}{2-\ln(3)}\ln(T  + 1)$ in general, and $R_T \leq 2\ln(T)$ for $T \geq 4.$ 
The statement of the claim plugging this into the expression above.
\end{proof}

\begin{lemma}\label{lem:lambda_opt} 
For each group $G$, taking $\lambda^\star_G = \mathrm{Proj}_{[0,1]}\left[\dfrac{\mu_G - \beta \mu_G^0}{\beta\mu_G^0(1-\beta\mu_G^0)}\right]$ maximizes expected log-wealth (at every step $t$). The resulting expected log-wealth at time $T$ (had $\lambda_G^\star$ been used at every time $t$) is equal to
\[
\E\left[\Logwealth_T^G(\lambda^\star_G)\right] = T\cdot \Logwealth_\star^G 
\]
where we denote $\Logwealth_\star^G \coloneqq \E[\ln(1+\lambda_G^\star(\mathbf{1}_{X_t\in G} - \beta\Basegroup)]$ the expected one-step wealth change under the bet $\lambda^\star_G$.
\end{lemma}

\begin{proof}
For a fixed $\lambda$, the log-wealth at time $T$ is given by 
\begin{equation*}
\Logwealth_T^G(\lambda) = N_T \ln{(1+\lambda(1-\beta\mu_G^0))} + (T-N_T) \ln{(1-\lambda\beta\mu_G^0)},
\end{equation*}
where $N_T = \sum_{t=1}^T \mathbf{1}_{X_t\in G}$. Taking expectations, we have that $\E[N_T] = T\cdot \mu_G$ and therefore 
\begin{equation}\label{eq:expected_logwealth}
\E\left[\Logwealth_T^G(\lambda)\right] = T\cdot\left[\mu_G \ln{(1+\lambda(1-\beta\mu_G^0))} + (1-\mu_G) \ln{(1-\lambda\beta\mu_G^0)}\right]. 
\end{equation}
To maximize \eqref{eq:expected_logwealth}, we only need to find $\lambda_G^\star\in[0,1]$ that maximizes the expressions in the square brackets. Taking the derivative we see that the function is concave,
and, therefore, we can solve for $\lambda_G^\star$ by setting the derivative to $0$ and then projecting the resulting value to $[0,1]$. This yields
$\lambda^\star_G = \text{Proj}_{[0,1]}\left[\frac{\mu_G - \beta \mu_G^0}{\beta\mu_G^0(1-\beta\mu_G^0)}\right].$ Plugging this back into \eqref{eq:expected_logwealth} we get
\begin{align*}
\E\left[\Logwealth_T^G(\lambda_G^\star)\right] &= T\cdot \left[\mu_G \ln{(1+\lambda_G^\star(1-\beta\mu_G^0))} + (1-\mu_G) \ln{(1-\lambda_G^\star\beta\mu_G^0)}\right] \\
&= T\cdot \E[\ln(1+\lambda_G^\star(\mathbf{1}_{X_t\in G} - \beta\Basegroup)] \\
&\coloneqq T\cdot \Logwealth_\star^G.
\end{align*}
\end{proof}

\begin{remark}
    Note that we can explicitly compute 
\[\Logwealth_\star^G = \mu_G \ln\left(    1 + \tfrac{\Delta_G}{\beta\Basegroup(1 - \mu_G )}\right) + \ln\left(1 - \tfrac{\Delta_G}{1-\beta\Basegroup}\right),\]
where $\Delta_G = \mu_G - \beta\Basegroup$, but this quantity is difficult to analyze, and it is not clear that $\Logwealth_\star^G$ can be explicitly lower bounded as $O(\Delta_G)$. 
\end{remark}

We now prove Theorem~\ref{thm:power_evals}, which we restate below.

\evalspower*

\begin{proof}
Let $\Gstar \coloneqq \arg\max_G \Logwealth_\star^G$ and denote the corresponding one-step wealth change $\Logwealth_\star = \Logwealth_\star^\Gstar$. Note that under the alternative this will correspond to a strictly positive quantity and is equivalent to the definition in the theorem statement. We can analyze the likelihood that its null has not been rejected by time $t$ as follows:
\begin{align*}
\Pr\left[\Logwealth_t^{\Gstar} < \ln(\nicebonfinv)\right] 
&=  \Pr\left[\Logwealth_t^{\Gstar} - \E[\Logwealth_t^{\Gstar}]< \ln(\nicebonfinv) - \E[\Logwealth_t^{\Gstar}]\right] \\ 
 &\leq \Pr\left[\Logwealth_t^{\Gstar} - \E[\Logwealth_t^{\Gstar}]< \ln(\nicebonfinv) - (t\cdot \Logwealth_\star - 2\ln t)\right], 
 \end{align*}
 where the inequality follows by Claim \ref{claim:logwealth-regret} and Lemma~\ref{lem:lambda_opt}, and the fact that $\E[\max_{\lambda\in[0, 1]} \Logwealth_t^\Gstar(\lambda)] \geq \E[\Logwealth_t^\Gstar(\lambda_\star^\Gstar)] = t \cdot \Logwealth_\star$. Whenever $t$ is large enough such that $\frac{\ln(t)}{t} \leq \frac{\Logwealth_\star}{4}$, we have 
\begin{align}
\label{eq:prnostop}
    \Pr[\Logwealth_t^\Gstar < \ln(\nicebonfinv)] \leq \Pr\left[\Logwealth_t^{\Gstar} - \E[\Logwealth_t^{\Gstar}]< \ln(\nicebonfinv) - \tfrac{3}{4}(t\cdot \Logwealth_\star)\right].
\end{align}
Since $\sqrt{t} \geq \ln{t}$ for all $t\in \mathbb{N}^\star$, this is satisfied in particular by taking $t\geq \frac{2^4}{\Logwealth_\star^2}$. Further, note that $\ln(\nicebonfinv) \leq \frac{t\cdot \Logwealth_\star}{4}$ whenever $t\geq \frac{2^2\cdot \ln{(\nicebonfinv)}}{\Logwealth_\star}$. So, for $t\geq \max\{\frac{2^4}{\Logwealth_\star^2}, \frac{2^2\cdot \ln{(\nicebonfinv)}}{\Logwealth_\star}\}$, we have
\[\Pr[\Logwealth_t^\Gstar < \ln(\nicebonfinv)] \leq \Pr\left[\Logwealth_t^{\Gstar} - \E[\Logwealth_t^{\Gstar}]< - \tfrac{1}{2}(t\cdot \Logwealth_\star)\right].\]
Now, note that since $\lambda_t^G \in [0,1]$, we have that each $\ln(1 + \lambda_t^G(\1_{X_t \in G} - \beta\Basegroup))$ lies in $[\ln{(1-\beta\Basegroup)}, \ln{(2 - \beta\Basegroup)}]$ and is therefore sub-Gaussian with parameter $\sigma = \frac{1}{2}\ln{\left(1+ \frac{1}{1-\beta\Basegroup}\right)}$; then, Hoeffding's inequality gives 
\begin{align*}
\Pr\left[\Logwealth_t^{\Gstar} - \E[\Logwealth_t^{\Gstar}]< - \tfrac{1}{2}(t\cdot \Logwealth_\star)\right]
&= 
\Pr\left[\sum_{i \in [t]} \ln(1 + \lambda_t^\Gstar(\1_{X_t \in \Gstar} - \beta\mu_\Gstar^0)) - \E[\Logwealth_t^{\Gstar}] \leq -\frac{1}{2} t \cdot \Logwealth_\star \right] \\
&\leq \exp\left(-\frac{(\frac{1}{2} t \cdot \Logwealth_\star)^2}{\frac{1}{2}t \ln^2(1+ \frac{1}{1-\beta\mu_\Gstar^0})}\right)  
\\&= \exp\left(-\frac{\Logwealth_\star^2}{2\ln^2(1+ \frac{1}{1-\beta\mu_\Gstar^0})} \cdot t \right) \\
&\leq  \exp\left(- \frac{(1-\beta\mu_\Gstar^0)^2}{2} \cdot \Logwealth_\star^2 \cdot t \right). 
\end{align*}
where for the last inequality we used $\ln{(1+x)} \leq x$. Now we are ready to analyze the stopping time $T$ of Algorithm~\ref{alg:abstract}. 

\paragraph{Test of power one.} Let $E_t$ be the event that we stop at time $t$, i.e. $E_t = \{\exists G$ such that $\Logwealth_t^G \geq \nicebonfinv\}$. We have that
\begin{align*}
\Pr[T = \infty] &= \Pr\left[\lim_{t \to \infty} \cap_{s \leq t} \neg E_t\right] \\
&= \lim_{t \to \infty} \Pr[ \cap_{s \leq t} \neg E_t] \\
&\leq \lim_{t \to \infty} \Pr[ \neg E_t] \\
&= \lim_{t \to \infty} \Pr[\forall G, \, \Logwealth_t^G < \ln(\nicebonfinv)] \\
&\leq \lim_{t \to \infty}\Pr\left[\Logwealth_t^{\Gstar} < \ln(\nicebonfinv)\right] \\
&\leq \lim_{t \to \infty}\exp\left(- \frac{(1-\beta\Basegroup)^2}{2} \cdot \Logwealth_\star^2 \cdot t \right) \\
&= 0.
\end{align*}

\paragraph{Expected Stopping Time.} Since $T$ is a positive integer, we can express the expected stopping time as
\begin{align}
\E[T] &= \sum_{t=1}^\infty \Pr[T>t] \notag \\
&= \sum_{t=1}^\infty \Pr[\neg E_1 \land \ldots \land \neg E_t] \notag\\
&\leq \sum_{t=1}^\infty \Pr[\neg E_t] \notag\\
&= \sum_{t=1}^\infty \Pr[\forall G, \, \Logwealth_t^G < \ln(\nicebonfinv)]\notag \\
&\leq \sum_{t=1}^\infty \Pr\left[\Logwealth_t^{\Gstar} < \ln(\nicebonfinv)\right] \notag\\
&\leq \max\bigg\{\frac{2^4}{\Logwealth_\star^2}, \frac{2^2 \cdot \ln(\nicebonfinv)}{\Logwealth_\star}\bigg\} + \sum_{t=1}^\infty \exp\left(- \frac{(1-\beta\mu_\Gstar^0)^2}{2} \cdot \Logwealth_\star^2 \cdot t \right) \label{eq:our_up}\\
&= \max\bigg\{\frac{2^4}{\Logwealth_\star^2}, \frac{2^2 \cdot \ln(\nicebonfinv)}{\Logwealth_\star}\bigg\} + \frac{1}{\exp{((1-\beta\mu_\Gstar^0)^2 \Logwealth_\star^2 /2)} - 1} \notag\\
&\leq \max\bigg\{\frac{2^4}{\Logwealth_\star^2}, \frac{2^2 \cdot \ln(\nicebonfinv)}{\Logwealth_\star}\bigg\} + \frac{2}{(1-\beta\mu_\Gstar^0)^2 \Logwealth_\star^2} \label{eq:exp_ineq}\\
&\leq \mathcal{O}\left(\frac{1}{\Logwealth_\star^2} + \frac{\ln(\nicebonfinv)}{\Logwealth_\star}\right) \notag
\end{align}
where \eqref{eq:our_up} follows from the upper bound on $\Pr\left[\Logwealth_t^{\Gstar} < \ln(\nicebonfinv)\right]$ for $t \geq \max\left\{\frac{2^4}{\Logwealth_\star^2}, \frac{2^2 \cdot \ln(\nicebonfinv)}{\Logwealth_\star}\right\}$ derived in \eqref{eq:prnostop}, and \eqref{eq:exp_ineq} follows from $\exp(x) \geq 1+x$. 
\end{proof}
