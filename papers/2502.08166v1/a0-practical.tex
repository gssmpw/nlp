\paragraph{Choosing $\Groups$.}
In our experiments in Section \ref{sec:experiments}, we choose to define subgroups as all possible combinations of available demographic characteristics.
That said, a practitioner may seek to define $\Groups$ more carefully in accordance with their application. For instance, if the goal is to illustrate discrimination in a legal sense, $\Groups$ should be defined with respect to (protected) demographic features, rather than arbitrary combinations of covariates. On the other hand, groups need not be solely demographic, which allows our approach to test for safety rather than solely fairness. For example, $\Groups$ could include which batch of a medication an individual received; our tests could then help identify whether some batches were improperly manufactured.  

\paragraph{Baseline rates $\{\Basegroup\}_{G \in \Groups}$.} A natural question that arises from the modeling in this section is how $\{\Basegroup\}_{G \in \Groups}$ can be determined, or if Assumption \ref{assn:ref} is strictly necessary. 
Practically speaking, these base preponderances may be estimated, possibly with some amount of noise; however, the estimation problem can be addressed with standard techniques and is not core to our contribution. 
Similarly, in practice these baseline preponderances may change over time (e.g. if some subgroups increased uptake of a vaccine, or applied for loans more frequently, over time); however, such situations are relatively straightforward to handle under our algorithmic frameworks (see, e.g., the variants discussed in \citet{chugg2024auditing}). 
We therefore focus on the case where we have access to the true, underlying values of $\{\Basegroup\}_{G \in \Groups}$ for ease and clarity of exposition. 

Note that testing against base preponderances of the reference population (i.e., to compare $\mu_G$ to $\Basegroup$) is a new test proposed by this work, and the analysis in Sections \ref{subsec:rr} and \ref{subsec:ir} is specific to this test. Existing approaches to monitoring in incident databases compare to different baselines, most commonly the historical overall incidence rate for the specific symptom, sometimes by subgroup
\citep{shimabukuro2015safety, kulldorff2011maximized, oster2022myocarditis}. 
These baselines could, in principle, be plugged into the algorithms in Section \ref{sec:algs}, but new analysis for (possibly group-varying) reporting rates would be necessary to draw inferences about analogous quantities of interest (e.g., $\RR$ or $\mathrm{IR}$), as current approaches do not generally consider reporting behavior.
In contrast, our modeling allows us to identify what quantities may affect the true incidence rate even if they may be unmeasurable.

\paragraph{Setting $\beta$.} Finally, to run the test proposed in Equation \eqref{eq:htest}, it is necessary to determine how to set the value of $\beta$. As we will see in Section \ref{sec:algs}, when $\beta$ is set too high, then the test may never identify problematic groups, or identify them more slowly; on the other hand, as is clear from the previous subsections, rejecting the null hypothesis for a smaller $\beta$ requires more stringent assumptions on reporting behavior. Thus, we suggest a procedure to set $\beta$ as follows: (1) choose a relative risk or incidence rate threshold where it would be problematic for any group if $\RR_G$ or $\mathrm{IR}_G$ surpassed that threshold; (2) make the corresponding assumptions about reporting behavior; (3) use these quantities to compute a reasonable $\beta$.
We give some example computations in Section \ref{sec:experiments}. 
Due to an equivalence between hypothesis testing and confidence intervals, it is statistically valid to rerun tests with different $\beta$s once data collection has begun. Thus, it may be prudent to begin by setting the lowest $\beta$ that reporting assumptions would allow; then, if the tests appear to be stopping very quickly, to re-run them at higher $\beta$s, which would allow a practitioner to get a better sense of the severity of the harm.
