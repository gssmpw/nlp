\paragraph{Identifying and defining subgroups.}
One approach to subgroup definition, following the line of work in multicalibration \cite{hebert2018multicalibration}, is to simply enumerate over all possible combinations of covariates. 
For sequential problems, per-group guarantees can be provided for subgroups that are learned online \citep{dai2024learning}, though these guarantees are in terms of prediction quality rather than statistical validity. For sequential experiments, \citet{adam2024should}  propose an approach to early stopping that does not require the experimenter to pre-specify the group experiencing harm, but instead identifies those who appear to be harmed more frequently. Though this is in spirit similar to the idea of identifying groups that report more frequently, their algorithm (and application context) is substantially different.

\paragraph{Sequential and multiple testing with anytime guarantees.}
One of our proposed tests provides anytime-validity guarantees by adapting the analysis of \cite{jamieson2014lil} and \cite{balsubramani2014sharp}. 
Our second proposed test leverages the recent literature on e-values (e.g. \cite{waudby2024estimating,vovk2021values}), which can be used to construct sequential tests that have validity guarantees in finite samples. While existing literature suggests methods for global null testing that can aggregate e-processes (e.g., \citet{choPeekingPEAKSequential2024} or \citet{chi2022multiple}), such approaches are unable to provide per-hypothesis guarantees. 


\paragraph{Application \& policy context.} 
Sequential hypothesis tests have been used for real-world monitoring of adverse incidents in vaccines and medical devices (see, e.g., \cite{shimabukuro2015safety}).  Descriptive studies have identified disparate adverse impacts in pharmaceutical ~\citep{lee2023gender,whitley2009sex} and vaccine settings~\citep{oster2022myocarditis}. 
In AI policy, there have already been several calls to adopt a post-market surveillance regime for AI governance (e.g., \citet{raji2022outsider}).
% \jesscomment{Some of these are conflating the individual incident reporting model with the accident catalog model....}
% On March 21 2024, backed by 120 member states, 
The U.N. General Assembly's first AI Resolution (7 8/265 and 78/311) explicitly encourages ``the incorporation of feedback mechanisms to allow evidence-based discovery and reporting by end-users and third parties of 
% technical vulnerabilities and, as appropriate, 
[...]
misuses of artificial intelligence systems and artificial intelligence incidents'' 
% following their development, testing and deployment'' 
\citep{un-ai-res}. %https://documents.un.org/doc/undoc/ltd/n24/065/92/pdf/n2406592.pdf
% The interim report by the UN High-Level Advisory Body on Artificial Intelligence also directly names the act of ``monitoring risks, reporting incidents and coordinating emergency response''
%https://www.un.org/sites/un2.un.org/files/un_ai_advisory_body_governing_ai_for_humanity_interim_report.pdf
% as a key institutional function \citep{un_report_ai_interim}. similarly, OECD has already implemented an ``AI incidents Monitor''~\citep{OECD_AIM}, investing heavily in piloting the initiative across multiple languages and geographical contexts.
%https://oecd.ai/en/incidents?search_terms=%5B%5D&and_condition=false&from_date=2014-01-01&to_date=2024-12-19&properties_config=%7B%22principles%22:%5B%5D,%22industries%22:%5B%5D,%22harm_types%22:%5B%5D,%22harm_levels%22:%5B%5D,%22harmed_entities%22:%5B%5D%7D&only_threats=false&order_by=date&num_results=20
%https://www.oecd.org/en/publications/defining-ai-incidents-and-related-terms_d1a8d965-en.html
% As noted in the UN report, at the international level, reporting frameworks tend to be ``inspired by existing practices of the IAEA for mutual reassurance on nuclear safety and nuclear security, as well as the WHO on disease surveillance''\citep{un_report_ai_interim}. 
% Similarly, more localized initiatives seem to acknowledge the importance of incident tracking. 
% The E.U. AI Act Article 73 requires all providers to ``report any serious incident to the market surveillance authorities of the Member States where that incident occurred''~\citep{eu_ai_act_art73}, and Measure 18 on the General-Purpose AI Code of Practice focuses specifically on incident reporting~\citep{ai_code_of_practice}. 
In the U.S., Biden's (now repealed) AI Executive order explicitly directs the Department of Health and Human Services (HHS) to ``establish a [...]
% common framework for approaches to identifying and capturing clinical errors resulting from AI deployed in healthcare settings as well as specifications for a 
central tracking repository for associated incidents that cause harm, including through bias or discrimination''
% , to patients, caregivers, or other parties''
~\citep{biden2023executive}. In the E.U., Chapter IX of the 2024 EU AI Act focuses on post-market surveillance, with Articles 85 and 87 specifically highlighting individual reporting of harms.

\paragraph{Key definitions \& clarifications.}
Finally, we note that for AI systems, the term ``incident database'' has been used to describe systems for monitoring the adverse impact of algorithmic deployments, 
which often take the form of accident catalogs that focus on one-off, large-scale events (e.g.,~\citet{feffer2023ai,raji2022outsider,mcgregor2021preventing, ojewale2024towards, turri2023we}). 
However, in the context of our work, we are actively excluding these accident catalog databases. Instead, we focus on reporting databases that provide records of individual experiences of adverse events that are tied to specific systems. 