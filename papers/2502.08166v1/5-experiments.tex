\section{Real-World Examples}
\label{sec:experiments}

To demonstrate the applicability of our approach, we apply our framework to two real-world datasets. 
We begin by showing that our approach correctly and quickly identifies that young men experience myocarditis after the COVID-19 vaccine; then, on mortgage allocation data, we show that we identify known instances of discrimination under several reasonable reporting models. 
\iftoggle{icml}{\jessedit{Code for all experiments, including instructions for data download and pre-processing, is available in the supplemental materials; additional experimental details can be found in Appendix \ref{app:expts}.}}{
Code for all experiments, including instructions for data download and pre-processing, is available online \href{https://github.com/jessica-dai/reporting}{here}.
}

\subsection{Myocarditis from COVID-19 vaccines} 

\begin{table*}[t!]
\centering
  \begin{tabular}{l|ll|ll|ll}
    \toprule
    \multirow{2}{*}{} &
    \multicolumn{2}{c|}{\textit{Asymptotic Z-test}} &
    \multicolumn{2}{c|}{\textit{Finite-sample Z-test}} &
    \multicolumn{2}{c}{\textit{Betting-style test}}~\\
      & {\small{(M, 18-29)}} & {\small{(M, 12-17)}} &{\small{(M, 18-29)}} & {\small{(M, 12-17)}} & {\small{(M, 18-29)}} & {\small{(M, 12-17)}}  \\
      \hline\hline
    $\beta = 2.0$ & 34 (Feb. 22) & 256 (May 10) & 69 (Mar. 28) & 530 (May 30) & 61 (Mar. 23) &  241 (May 8) \\
    $\beta = 2.5$ & 49 (Mar. 10) & 302 (May 15) & 74 (Mar. 31) & 546 (Jun. 1) & 69 (Mar. 28) & 259 (May 11) \\
    $\beta = 3.0$ & 70 (Mar. 30) & 324 (May 18) & 111 (Apr. 20) & 612 (Jun. 6) & 80 (Apr. 5) & 302 (May 15) \\
    \bottomrule
  \end{tabular}
  \vspace{1em}
  \caption{On real historical sequence of myocarditis reports, time to identification of harmed groups. In each cell, we report the number of total reports to the rejection of the hypothesis corresponding to (M, 18-29) and the number of total reports corresponding to (M, 12-17). In all tests, the (M, 18-29) group is identified first---vaccines were authorized for the 12-15 age group only in May. }
  \label{table:covid}
\end{table*}

It is by now well-known that COVID-19 vaccines induce an elevated risk of myocarditis among young men. While initial suspicions of elevated myocarditis risk relied on case studies (e.g., \citet{mouch2021myocarditis, larson2021myocarditis,marshall2021symptomatic}), a more systematic understanding---including the pattern of disproportionate impact---was made possible by post-hoc analysis of reports from incident databases. \citet{barda2021safety} appears to be the first analysis based on a database of reports, but did not disaggregate by demographic subgroups; the confirmation of young men as the most drastically-impacted group came in later studies (e.g., \citet{witberg2021myocarditis, oster2022myocarditis}).

In the U.S., these reports are collected inthe  Vaccine Adverse Event Reporting System (VAERS).
If we had been able to run the hypothesis tests proposed in the preceding sections on the reports collected in VAERS, would we have correctly identified this problem---and if so, how quickly? 
Concretely, we let $Y_i$ be the event that individual $i$ experiences myocarditis after receiving a COVID-19 vaccine, and run the test with the end-goal of identifying elevated incidence rate $\Pr[Y_i \mid X_i \in G]$ for group(s) $G$ corresponding to adolescent men (ages 12-17 and 18-29). 

\iftoggle{icml}{}{
\paragraph{Data sources.}
The Vaccine Adverse Event Reporting System (VAERS) is a national adverse event incident database for U.S.-licensed vaccines, co-managed by the Centers for Disease Control and Prevention (CDC) and the U.S. Food and Drug Administration (FDA)~\citep{chen1994vaccine,shimabukuro2015safety}. 
The database is a combination of voluntary reports from patients that have received the vaccine, as well as mandatory reports from vaccine manufacturers and healthcare professionals. 
For this case study, we filter the set of publicly-available reports from VAERS to reports about the COVID-19 vaccine with a complaint keyword including ``myocarditis.'' As for how a database administrator would have known to focus on myocarditis \textit{a priori}, one might imagine, for example, that the series of case studies found in early 2021 raised the alarm that more systematic analysis was warranted for myocarditis in particular.

To determine per-demographic base rates, i.e. to compute $\{\Basegroup\}_{G\in\Groups}$, we utilize 
VaxView, a government dataset tracking national vaccine coverage, managed by the CDC. VaxView does not track vaccination rates by granular subgroups, only providing coverage rates disaggregated by age, gender, and ethnicity separately. We thus impute the vaccination rates for intersections of subgroups (e.g., ``12-17, M'') by multiplying the known marginal rates (i.e., $\mu_{(12-17, M)}^0 := \mu_{(12-17)}^0 \cdot \mu_{(M)}^0$).}

\paragraph{Defining $\Groups$.} 
We consider (intersections of) sex and age buckets to be the subgroups of interest.\footnote{While in principle it would have been interesting to also consider race/ethnicity, we are limited by the availability (and granularity) of the data given in VAERS, which does not include information on ethnicity/race in reports.}  Age buckets are discretized into 0-4, 5-11, 12-17, 18-29, 30-39, 40-49, 50-64, 65-74, and 75+; the sex categories represented in the data are (binary) male and female. After removing groups for which no vaccines were recorded, $\Groups$ contains 29 groups. 

\paragraph{Setting $\beta$.}
For this application, absolute incidence rate (that is, $\Pr[Y = 1 \mid G]$) is the quantity of interest to use for determining $\beta$.
As suggested by Proposition \ref{prop:reporting-conversion}, setting $\beta$ requires considering three quantities of interest: the threshold on an ``unacceptable'' incidence rate, the relative rates of true reporting $\Truereportrate$, and the relative rates of false reporting $\Falsereportrate$. Then, we can set $\beta = \max_G \left((\Truereportrate-\Falsereportrate)\cdot \mathrm{IR} + \Falsereportrate\right)$. 

We will choose 0 as the threshold on an ``unacceptable'' incidence rate.\iftoggle{icml}{}{\footnote{One might follow existing practice and use the per-group expected rate of myocarditis to benchmark an unacceptable incidence rate (e.g. as provided in Table 2 of \citet{oster2022myocarditis}, which suggests at most 2 cases per million doses). 
However, in addition to this expected incidence rate being very small (and, for any practical purposes, being vastly dominated by the other reporting terms), it also implicitly relies on reports so that the benchmark quantities are  $r \cdot \mathrm{IR}$, rather than just $\mathrm{IR}$, and thus depend on the unknown reporting rate $r$.}}
It is therefore sufficient to set $\beta = \max_G(\Falsereportrate)$. While this is quantity cannot be determined from report data alone, a conservative assumption could be that any group erroneously reports at most twice the average reporting rate  over the population, with $\Falsereportrate = 2.0$. 
If the algorithm is first run with $\beta=2.0$, stopping and flagging a group very quickly, the test may be re-run with increasing values of $\beta$, as a higher $\beta$ corresponds to a more severe true incidence rate; thus, we also show results for $\beta=2.5$ and $\beta=3$.\footnote{Re-using this data is statistically valid due to the equivalence between one-sided hypothesis testing and confidence sequences.}

\paragraph{Results.}  
We begin by running our algorithms on the actual sequence of reports in chronological order, as received in VAERS. In particular, we consider Algorithm \ref{alg:abstract} instantiated with $\Logwealth_t^G$ updated according to Equation \eqref{eq:ztest_update} and $\theta_t(\alpha)$ as in \eqref{eq:ztest_thresh} and $C = \nicefrac12$ (\textit{Finite-sample Z-test}); with $\Logwealth_t^G$ updated according to Equation \eqref{eq:ztest_update} and $\theta_t(\alpha)$ as in \eqref{eq:ztest_thresh} and $C = \sqrt{\beta\Basegroup(1 - \beta\Basegroup)}$ (\textit{Asymptotic Z-test}); and with $\Logwealth_t^G$ updated according to Equations \eqref{eq:wealth_update} and \eqref{eq:bet_update}, and $\theta_t(\alpha) = \ln (\nicefrac{|\Groups|}{\alpha})$ 
(\textit{Betting-style test}). 
For the asymptotically-valid Z-test, we require a minimum stopping time of $t = 25$, to prevent early rejections.
We run all experiments for $\alpha = 0.1$. 

In Table \ref{table:covid}, we report the stopping time---that is, the number of reports it takes for the first null hypothesis to be rejected---of each algorithm for various values of $\beta$, as well as the corresponding date by which an alarm would have been triggered. Note that, in all tests, the (M, 18-19) group is identified first. This is consistent with the timeline of regulatory approvals: vaccines were authorized for ages 12-15 only by May 10 \citep{lovelace2021fda}.

To explore the robustness of these results, we also run synthetic experiments, 
permuting the ordering of reports to get a sense of possible variance in the stopping time. We run 100 random permutations of the full set of reports. 
Figure \ref{fig:covid-all-beta2} tracks the number of reports it takes for each algorithm to reject the null hypothesis for any group---that is, a scenario when the test is stopped and an alarm is raised as soon as one harmed group is identified. Each point on these plots reflects the number of trials (out of 100) in which a rejection has occurred by time $t$, when tests are run at $\beta=2$. 

In Figure \ref{fig:covid-all-beta2}, we compare the performance of the three algorithms.\iftoggle{icml}{\footnote{To interpret the figure, by time $t=100$, the asymptotically-valid z-test had stopped (identified harm) in all 100 permutations; the betting-style test stopped in around 80 permutations; and the finite-sample z-test stopped in around 20 permutations.}}{ To interpret the figure, by time $t=100$, the asymptotically-valid z-test had already identified harm in all 100 permutations; the betting-style test identified harm in around 80 permutations; and the finite-sample z-test had only identified harm in around 20 permutations.}
Figure \ref{fig:covid-all-beta2} shows a clear ordering in terms of how quickly each algorithm tends to identify harm: the asymptotically-valid sequential z-test (dashed, red) is faster than the betting-style algorithm (solid, purple), which is faster than the finite-sample z-test (dotted, yellow). 

\iftoggle{icml}{
\begin{figure}[h]
\centering
\begin{minipage}[c]{0.45\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/covid-beta2.png}
\end{minipage}
    \caption{\small  Stopping time (i.e. first identification of harm) for each algorithm, over 100 random permutations of COVID-19 vaccine reports, with $\beta=2$. Each point on the plot reflects the number of trials (out of 100) in which a rejection has occured by time $t$. 
    }
    \label{fig:covid-all-beta2}
\end{figure}
}{
\begin{figure*}[t!]
\hfill
\begin{minipage}[c]{0.55\textwidth}
    \includegraphics[width=0.95\linewidth]{plots/covid-beta2.png}
\end{minipage}
    \hfill
\begin{minipage}[c]{0.35\textwidth}
    \caption{\small  Number of reports ($t$) it takes for each algorithm to reject the null hypothesis for any group (i.e. first identification of harm), over 100 random permutations of COVID-19 vaccine report database. Tests are run with $\beta=2$. Each point on the plot reflects the number of trials (out of 100) in which a rejection has occured by time $t$. 
    }
    \label{fig:covid-all-beta2}
\end{minipage}
\hfill
\end{figure*}
}

\iftoggle{icml}{}{
We also explore the impact of Bonferroni correction for multiple hypothesis testing on stopping time.
In  Figure \ref{fig:covid-beta2-bonf}, we show the same axes---number of reports to first alarm on the x-axis, vs. number of permutations in which an alarm was triggered on the y-axis---for the three algorithms at $\beta=2$. As expected, the invalid version of the test, which has a lower threshold for rejecting each null, stops more quickly for all three algorithms (dashed, lighter). 
The difference between the invalid version and the valid version (solid, darker) is relatively minor, though the impact varies across algorithms. 
% For the betting-style test, recall that the stopping-time upper bound in Theorem \ref{thm:power_evals} suggested that the impact of adding the Bonferroni correction was an additive factor of $\mathcal{O}(\nicefrac{\log(|\Groups|)}{\Delta_{\mathrm{max}}^2})$. 
% In the plot for the betting-style test, we add a line (dotted, lightest) that adds exactly $\lceil\nicefrac{\log(|\Groups|)}{\Delta_{\mathrm{max}}^2}\rceil$  to the actual stopping time of the algorithm without the Bonferroni correction; note that the actual stopping time of the betting-style algorithm with the multiple testing correction is much faster than what is suggested by the theoretical upper-bound.

\begin{figure*}[h]
\hfill
\begin{minipage}[c]{0.32\textwidth}
    \includegraphics[width=0.97\linewidth]{plots/covid-bonf-asymp.png}
\end{minipage}
\hfill
\begin{minipage}[c]{0.32\textwidth}
    \includegraphics[width=0.97\linewidth]{plots/covid-bonf-finite.png}
\end{minipage}
\hfill
\begin{minipage}[c]{0.32\textwidth}
    \includegraphics[width=0.97\linewidth]{plots/covid-bonf.png}
\end{minipage}
\hfill
    \caption{\small Impact of multiple hypothesis correction on stopping time across algorithms. As in Figure \ref{fig:covid-all-beta2}, each point on the plot reflects the number of trials (out of 100) in which a rejection has occurred by time $t$. 
    In all plots, the lighter, dashed line reflects stopping time of the invalid test that does not correct for multiple testing; the dark, solid line reflects stopping time of the valid test including a Bonferroni correction. 
    }
    \label{fig:covid-beta2-bonf}
\end{figure*}
}

Overall, our experimental results suggest that our proposed tests would in fact have been effective in determining that young men were disproportionately affected by myocarditis. Moreover, though it is difficult to determine exact timelines and the nature of clinical practice during early phases of the vaccine rollout, it is possible that such a test could have identified problems using less data---that is, more quickly---than was actually used for this finding. 


\subsection{Mortgage Allocations}

In 2021, \citet{martinez2021secret} found that, based on publicly-released data from the Home Mortgage Disclosure Act (HMDA), 
substantial racial disparities in 2019 loan approvals persisted even after controlling for financial status of applicants---most notably, healthy debt-to-income ratios (DTI). 
If loan applicants had been able to submit reports when they believed they had experienced unfavorable outcomes, could those reports have been used to identify this discrimination? If so, how accurately, and how quickly? 

We are interested primarily in disparity among applicants with healthy DTI, even though all loan applicants would have been eligible to submit reports. 
Concretely, we let $A_i = 0$ be the event that a loan is not made to applicant $i$, and $Z_i = 1$ be the event that applicant $i$ has a healthy debt-to-income ratio. Then, we let $Y_i = \{A_i = 0, Z_i = 1\}$ be the event that individual $i$ has a healthy DTI and did not receive a loan, and run the test with the end-goal of identifying groups that have relatively high rates of loan denials for applicants with healthy DTI, i.e. $\frac{\Pr[A_i = 0, Z_i = 1 \mid X_i \in G]}{\Pr[A_i = 0, Z_i = 1]}$. 

\iftoggle{icml}{}{
\paragraph{Data sources.}
We use the data (and preprocessing code) of \citet{martinez2021secret}, which uses 2019 data from the HMDA.\footnote{The Consumer Financial Protection Bureau (CFPB) collects and publishes this data from financial institutions annually, with a two-year lag; the report (and our work) uses 2019 data which is finalized as of Dec. 31, 2022. The most recent year for which data is available is 2022, though it is available for edits through 2025.}
The analysis of \citet{martinez2021secret} used the full year of data from 2019; we reduce the dataset to applications for conventional loans at three of the largest lending institutions, from applicants who have positive income. We assume that reports will only come from applicants whose loans were denied; in all, there are 183k applicants which satisfy these criteria. 
}

\paragraph{Defining $\Groups$.} 
While \citet{martinez2021secret} analyzed disparities with respect to race, we define groups as all possible intersections of demographic features across gender, race, and age. 
The available race categories include Native, Asian, Black, Pacific Islander, White, and Latino; sex categories include female, male, and unknown/nonbinary; and age categories include $<$25, 25-34, 35-44, 45-54, 55-64, and 65+. 
In total, after removing groups which comprise less than 0.1\% of all loan applicants, $\Groups$ contains 115 groups. 

\paragraph{Setting $\beta$.}
In this application, the quantity of interest is relative risk, so we draw on Proposition \ref{prop:relativerisk-conversion} to inform our setting of $\beta$. We will set our relative risk threshold to be 1.2---that is, we want our algorithm to raise an alarm when any group experiences event $Y$ 20\% more frequently than average over the population. 
Recall Definition \ref{def:rir} and Proposition \ref{prop:relativerisk-conversion}: to flag relative risk at 1.2, $\beta$ should be set to $1.2 \cdot b$ where $b = \max_G \nicefrac{\rho_G}{\rho}$, with $\rho_G = \frac{\Pr[R =1 \mid G]}{\Pr[Y = 1 \mid G]}$ and $\rho = \frac{\Pr[R = 1]}{\Pr[Y = 1]}$; that is, $b$ is the extent to which the group-conditional report-to-incidence ratio for any group deviates from the population average report-to-incidence ratio. 

As before, we can first test at $\beta = 1.2$ , then re-test for higher values of $\beta$; in this case, we will also test $\beta = \{1.4, 1.6, 1.8\}$. Setting $\beta = 1.2$ corresponds to assuming $b = 1$, i.e., no variance in report-to-incidence ratios across groups; the additional values of $\beta$ suggest possible values of $b = \nicefrac{7}{6}, \nicefrac{4}{3},$ and $\nicefrac{3}{2}$, respectively. 

\paragraph{Reporting models.}
The existence of verifiable disparities in this dataset allows us to evaluate the efficacy of our methods under varying models of reporting---that is, whether 
our algorithms identify groups that do in fact have high rates of healthy DTI denials, even if it is not the case that every report $X_i$ corresponds to $\Badevent_i$ actually occurring. 
\iftoggle{icml}{
Modeling the idea that reporting behavior may be related to financial health, we simulate the following possible patterns of reporting.
In the \textit{Correlated} model, the likelihood of reporting increases with financial health; in the \textit{All Denials} model, all denials submit reports regardless of financial health; in the \textit{Anti-Correlated} model, individuals with worse financial health are more likely to report (see Appendix \ref{app:expts}). 
}{
The dataset gives several levels of financial health as measured by DTI---in ascending order, are``Struggling'', ``Unmanageable,'' ``Manageable,'' and ``Healthy.'' 
Modeling the idea that reporting behavior may be related to financial health, we use these categories to simulate the following possible patterns of reporting.
\begin{enumerate}[(1)]
    \item \textit{Correlated:} The likelihood of reporting increases with financial health. That is, ``Healthy'' denials report with probability 0.9, ``Manageable'' with probability 0.5, ``Unmanageable'' with probability 0.3, and ``Struggling'' with probability 0.1. Under this reporting model, the 95th-percentile (among all groups) $\nicefrac{\rho_G}{\rho}$ is 1.2, and $\max_G \nicefrac{\rho_G}{\rho} = 1.4.$
    \item \textit{All Denials:} All denials submit reports regardless of financial health. Under this reporting model, the 95th-percentile $\nicefrac{\rho_G}{\rho}$ is 1.5, and $\max_G \nicefrac{\rho_G}{\rho} = 2.3.$
    \item \textit{Anti-Correlated:} The (unlikely) case where individuals with worse financial health are more likely to report, i.e. ``Healthy'' denials report with probability 0.1, ``Manageable'' with probability 0.5, ``Unmanageable'' with probability 0.7, and ``Struggling'' with probability 0.9. Under this reporting model, the 95th-percentile $\nicefrac{\rho_G}{\rho}$ is 1.8, and $\max_G \nicefrac{\rho_G}{\rho} = 2.7.$
\end{enumerate}
Note that the ground-truth ratios $\nicefrac{\rho_G}{\rho}$ would have been unknown at the time that a practitioner sets $\beta$; we are able to determine these only because we have full information about the dataset and control over the reporting model. However, these computations suggest that the assumptions on reporting rates implied by the settings of $\beta = \{1.2, 1.4, 1.6, 1.8\}$ are generally reasonable, especially after considering outliers---note the disparity between the 95th-percentile vs max ratios of $\nicefrac{\rho_G}{\rho}$, especially for the \textit{All Denials} and \textit{Anti-Correlated} models.
}


\iftoggle{icml}{
\begin{table*}[t!]
\centering
  \begin{tabular}{c|cc|cc|cc}
    \toprule
    \multirow{2}{*}{} &
    \multicolumn{2}{c|}{\textit{Asymptotic Z-test}} &
    \multicolumn{2}{c|}{\textit{Finite-sample Z-test}} &
    \multicolumn{2}{c}{\textit{Betting-style test}}~\\
      & {\small{Stopping time}} & {\small{Relative risk}} &{\small{Stopping time}} & {\small{Relative risk}} &{\small{Stopping time}} & {\small{Relative risk}}   \\
      \hline\hline
    \textit{Correlated} & 886 & 1.68 & 11755 & 1.73 & 4157 & 1.82 \\
    \textit{All Denials} &  586 & 1.69  & 7410 & 1.72 & 2714 & 1.75 \\
    \textit{Anti-Corr.} & 271 & 1.05 & 4668 &  1.72  & 1688 &  1.71 \\
    \bottomrule
  \end{tabular}
  \vspace{1em}
  \caption{Average stopping times (i.e. time to first alarm) and true relative risk (i.e., $\frac{\Pr[A_i = 0, Z_i = 1 \mid X_i \in G]}{\Pr[A_i = 0, Z_i = 1]}$) of first-identified group over 100 random permutations, for $\beta=1.6$, across algorithms and reporting models.}
  \label{table:hmda}
\end{table*}
}{
\begin{table*}[t!]
\centering
\begin{tabular}{c|c|cc|cc|cc}
  \toprule
  \multirow{2}{*}{} & \multirow{2}{*}{\small{Reporting model}} &
  \multicolumn{2}{c|}{\textit{Asymptotic Z-test}} &
  \multicolumn{2}{c|}{\textit{Finite-sample Z-test}} &
  \multicolumn{2}{c}{\textit{Betting-style test}}~\\
  & & {\small{Stopping time}} & {\small{Rel. risk}} & {\small{Stopping time}} & {\small{Rel. risk}} & {\small{Stopping time}} & {\small{Rel. risk}} \\
  \hline\hline
  \multirow{3}{*}{$\beta=1.2$} & \textit{Correlated} & 85 & 1.62 & 2002 & 1.67 & 638 & 1.70 \\
  & \textit{All Denials} & 69 & 1.59 & 1546& 1.60 & 519 & 1.65 \\
  & \textit{Anti-Corr.} & 60 & 1.50 & 1065 & 1.53 & 403 & 1.65 \\
  \hline
    \multirow{3}{*}{$\beta=1.4$} & \textit{Correlated} & 316 & 1.69 & 4306 & 1.73 & 1542 & 1.77 \\
  & \textit{All Denials} & 163 & 1.62 & 3214 & 1.72 & 1073 & 1.72 \\
  & \textit{Anti-Corr.} & 95 & 1.47 & 2215 & 1.66 & 718 & 1.68 \\
  \hline
    \multirow{3}{*}{$\beta=1.6$} & \textit{Correlated} & 886 & 1.68 & 11755 & 1.73 & 4157 & 1.82 \\
  & \textit{All Denials} & 586 & 1.69 & 7410 & 1.72 & 2714 & 1.75 \\
  & \textit{Anti-Corr.} & 271 & 1.05 & 4668 & 1.72 & 1688 & 1.71 \\
  \hline
    \multirow{3}{*}{$\beta=1.8$} & \textit{Correlated} & 4959 & 1.74 & ---$^1$ & --- & 16425$^2$ & 1.98 \\
  & \textit{All Denials} & 2703 & 1.72 & 29751$^3$ & 1.73 & 9977 & 1.89 \\
  & \textit{Anti-Corr.} & 935 & 1.58 & 14072 & 1.73 & 4629 & 1.76 \\
  \hline
\end{tabular}
\vspace{1em}
\caption{Average stopping times (i.e. time to first alarm) and true relative risk (i.e., $\frac{\Pr[A_i = 0, Z_i = 1 \mid X_i \in G]}{\Pr[A_i = 0, Z_i = 1]}$) of first-identified group over 100 random permutations, for varying $\beta$, across algorithms and reporting models.
For $\beta=1.8$, some combinations of algorithm/reporting model failed to stop within 40,000 steps for some trials: 
$^1$stopped in 0/100 trials, $^2$stopped in 99/100 trials, $^3$stopped in 76/100 trials.}
\label{table:hmda}
\end{table*}
}

\paragraph{Results.} We run all three algorithms discussed in Section \ref{sec:algs} at $\alpha=0.1$, for all four reporting models discussed above, and for $\beta=\{1.2, 1.4, 1.6, 1.8\}$. For the asymptotically-valid Z-test, we (heuristically) choose a higher minimum stopping time of 50, to reflect the more challenging problem instance compared to the vaccine reporting problem.
For each algorithm, reporting model, and $\beta$, we again run 100 random permutations.\iftoggle{icml}{\footnote{Since we are simulating reporting, there is no ``true'' historical sequence of reports to run an algorithm on, unlike in Table \ref{table:covid}.}}{ (Since we are simulating reporting, there is no ``true'' historical sequence of reports to run an algorithm on, unlike in Table \ref{table:covid}.)}

One important question for this application is the extent to which our tests identify the type of harm we are interested in, across various reporting models: while the algorithms guarantee statistical validity in terms of overrepresentation (i.e., in terms of whether $\mu_G \geq \beta\Basegroup$), they cannot intrinsically guarantee that reports themselves reflect true harm.
With the benefit of hindsight (and access to the full dataset), we are able to calculate ``ground truth'' relative risks; the hope for our algorithms is that they identify groups that actually do experience elevated relative risk.

Our results suggest that this is indeed generally the case, although the actual behavior varies by algorithm and reporting model. Table \ref{table:hmda} shows report the average stopping times and average true relative risks of the first-identified group for 100 permutations\iftoggle{icml}{, for $\beta = 1.6$ (see Appendix \ref{app:expts} for results for all $\beta$)}{}. 
Across all algorithms\iftoggle{icml}{}{ and values of $\beta$}, the stopping time under the \textit{Correlated} reporting model is the longest, followed by the \textit{All Denials} and \textit{Anti-Correlated} reporting models. On the other hand, the relative risk of the group that is first identified in each of these settings follows the same ordering, with the \textit{Correlated} model having the highest relative risk. That is, more ``favorable'' reporting behavior required a test to run longer, but the group identified is more severely harmed, whereas more ``adversarial'' reporting behavior raised an alarm sooner, but identified a less severely-harmed group. 

Similar tradeoffs arise when comparing algorithms: the asymptotically-valid Z-test stops far more quickly, but appears to identify less severely-harmed groups. On the other hand, while the betting-style test and the finite-sample Z-test tend to identify similarly-harmed groups, the latter stops much faster than the former; overall, it appears that the betting-style test is a reasonable approach to balancing fast identification with confidence in the severity of harm. 

While overall trends across algorithms and reporting models are consistent across values of $\beta$, seeing these results for different $\beta$ highlights an additional insight. 
While it is to be expected that stopping times (and the ground-truth relative risks) should increase with $\beta$, the increase in stopping time is dramatic---by sometimes by orders of magnitude---even for what appear to be relatively small changes in $\beta$. Moreover, the \textit{disparity} in stopping time across reporting models also increases dramatically with $\beta$. In fact, for $\beta=1.8$, some combinations of reporting and algorithm do not stop within 40,000 steps in at least one trial. 

\iftoggle{icml}{}{
\paragraph{Practical takeaways.}
Altogether, these results highlight several potentially non-obvious insights about conducting the type of tests that we propose.
While the algorithms appeared fairly similar in the vaccine case study, this mortgage allocation setting is 
more challenging in various ways: the presence of reporting behavior where some reports do not actually correspond to ``true'' harm; 115 total groups compared to 29; and much smaller numerical gaps, i.e. smaller group sizes, both for base preponderances $\Basegroup$ and reporting rates $\mu_G$. 
These additional challenges reveal some practical takeaways for conducting these tests. 

The first consideration is in the choice of algorithm. It appears that the betting-style test most effectively balances stopping time with identifying highly-risky groups---though it tends to stop more slowly than the asymptotically-valid Z-test, it also identifies groups that are more severely harmed (while also preserving true statistical validity). On the other hand, though the finite-sample Z-test appears to have similar theoretical guarantees as the betting-style test, it stops more slowly and in general appears to be much more likely to fail when gaps are smaller. 
This leads to the second consideration, which is the choice of $\beta$. Because it is statistically valid to retest with increasing values of $\beta$, these results suggest that the initial $\beta$ should be fairly small, and increased over time---especially as these tests tend to be fairly conservative. 
}