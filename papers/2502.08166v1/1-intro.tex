\section{Introduction}

\iftoggle{icml}{}{
\begin{figure*}
\centering
    \includegraphics[width=0.9\linewidth]{figs/overview_jess.pdf}
    \caption{Overview of incident database framework.}
    \label{fig:overview}
\end{figure*}
}
The impact of injustice is most acutely felt by the individual. But if an individual experiences harm, how can they know whether their experience is an isolated incident or part of a larger pattern of discrimination?

Fairness work has historically focused on model developers and third-party auditors as the main actors involved in creating fair mechanisms, motivating methods to construct models that are fair with respect to pre-defined subgroups at development time (e.g., as surveyed in \citet{pessach2022review})---or in identifying unfair ones, motivating post-hoc audits that occur after the entire decision-making process has completed (e.g., \citet{byun2024auditing, martinez2021secret}). 
However, in most applications where fairness is a concern, problems with the system may only emerge over time, and it is not necessarily obvious which subgroups might be important. Moreover, such approaches to fairness provide no mechanism for individuals to raise concerns. 
 
It is exactly this question of individual agency that drives our work. In addition to normative reasons, which suggest that individuals ought to have a voice in expressing concerns with their treatment (e.g., the literature on contestability of algorithmic decisions \citep{vaccaro2019contestability}), recent legislation has also highlighted individual reporting as a policy mandate for the governance of AI systems (e.g., the E.U. AI act \citep{EUAIAct2023}). While such legislation has yet to see full implementation,  mechanisms for individual incident reporting already exist in a variety of domains, including consumer finance, medical devices, and vaccines and pharmaceuticals. 
A key component of reporting databases in the latter settings is that information from individual reports are aggregated to build collective knowledge about specific vaccines or pharmaceuticals---and, when applicable, this aggregated information can drive downstream decisionmaking, such as updating vaccine guidelines or drug treatment protocols (e.g., \citet{oster2022myocarditis}). 

\iftoggle{icml}{
}{
Fairness is an especially salient application for incident reporting systems: while individuals bear the harm, commonly-accepted (and legally-legible) notions of fairness are understood at an aggregate level. In fact, existing examples of (algorithmic) discrimination lawsuits  (e.g., 
\citet{Gilbert_2023} in hiring, or \citet{pazanowski}
in housing) are often structured as class actions, even as they are initiated by individuals based on their personal experiences. Crucially, individuals themselves may not know whether their experience with the system was inherently problematic, and deserving of redress, until it is placed in context with the experiences of others. On the other hand, while existing incident databases do not typically analyze reporting behavior, it may be necessary to consider reporting more carefully in order for incident databases to be useful for fairness auditing in more general settings, such as for algorithms that make allocation decisions.
}

In this paper, we consider what a realistic approach to assessing fairness claims from an incident database might look like in practice. We are primarily interested in designing a framework for the general public to report and contest large-scale harms by leveraging reports of \emph{individual experience} to inform \emph{collective evidence} of discrimination. To this end, we propose \textit{incident databases}, which allow individuals to submit reports of negative interactions, as a new mechanism for post-deployment fairness auditing. In particular, we identify conditions on reporting behavior and show how they can be used to to make inferences about rates of true harm in Section \ref{sec:interpretation}.
Our formalization of the problem allows us to leverage known approaches to sequential hypothesis testing. In Section \ref{sec:algs} we show how to instantiate two reasonable algorithms for our test and provide theoretical guarantees for each. Finally, in Section \ref{sec:experiments}, we illustrate the usefulness of our approach using real-world datasets, for applications with known disparity in per-subgroup rates of harm. On both real vaccine incident reports and on mortgage allocation decisions, our algorithm correctly identifies groups that disproportionately experience harm---and does so using a comparatively small number of reports.

\subsection{Related work \& application context}

The incident database problem is at the intersection of various challenges addressed in fairness and statistics. 
\iftoggle{icml}{
We discuss additional work, including application and policy context, in Appendix \ref{app:rel}.
}{}

\paragraph{Algorithmic accountability via (individual) reports.}
Some recent work considers methods for learning about fairness problems via individual reports from both theoretical \citep{globus2022algorithmic} and practical \citep{agostini2024bayesian} perspectives. However, most discussion of individual experiences in machine learning fairness literature is limited to contexts where the objective is to assess, appeal, contest or seek recourse for that individual to change their \textit{individual} outcomes, rather than forming a \textit{collective} judgment about the system as a whole~\citep{sharifi2019average, ustun2019actionable, karimi2022survey}.

Work on identifying fairness-related issues via reporting data has typically focused on learning in batch contexts, e.g. via 
positive-unlabeled learning for handling disparate reporting rates across subgroups (e.g., \citet{shanmugam2024quantifying, wu2022fairness}). 
In other works, identifying disparate reporting rates is itself is the central challenge (e.g., \citet{liu2022equity, liu2024quantifying}).
On the other hand, an emerging body of literature from the human-computer interaction community develops the concept of \textit{contestability} (e.g., \citet{almada2019human, vaccaro2019contestability, landau2024challenging}); though contestability is still typically understood in terms of individual outcomes, we see our work as one possible path to implementing this ideal\iftoggle{icml}{.}{, with an eye towards empowering contestability at larger scale.}


\paragraph{Fairness auditing as hypothesis testing.} 
\iftoggle{icml}{Existing proposals to formalize fairness auditing via hypothesis testing mainly consider batch settings (i.e. post-hoc or pre-deployment) \citep{cenai, cherian2023statistical}.}{\citet{cenai} make a direct connection between legal AI fairness audit requirements and hypothesis testing, though they mainly consider a post-hoc setting. \citet{cherian2023statistical} take a multiple testing approach for handling a large number of groups, but this test is again post-hoc (or entirely pre-deployment).} Two more closely related works are that of \citet{chugg2024auditing} and \citet{feng2024monitoring}, who propose applying sequential hypothesis tests with the explicit goal of identifying problems in deployed systems in real time.
However, as neither of these works study a reporting model, we propose fundamentally different tests: they test equality of means across different groups, while we compare within groups. 

\iftoggle{icml}{}{
\input{a0-additional-related}
}