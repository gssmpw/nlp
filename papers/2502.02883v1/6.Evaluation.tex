%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-2mm}
\section{Evaluation on State-of-the-Art Dataset}
\label{sec:evaluation}

In this section, we thoroughly evaluate \Method on the state-of-the-art dataset focusing on quantitative questions. We further validate \Method in a real world study with open-ended, qualitative questions in Sec.~\ref{sec:deployment}.


\subsection{Dataset and Metrics}

In this evaluation section, we focus mainly on the \Dataset dataset~\citesensorqa and quantitative questions. A detailed introduction to \Dataset~\citesensorqa is provided in Sec.~\ref{sec:motivation}. 
To the best of our knowledge, \Dataset is the first and only available benchmarking dataset for QA interactions that use long-term timeseries sensor data and reflect practical user interests.
While we focus on \Dataset~\citesensorqa in this section, we emphasize that the motivation and design of \Method are broadly applicable and can be extended to other practical sensing applications.
To ensure the best alignment between the QA pairs and sensor information, we conduct offline encoders pretraining on the ExtraSensory multimodal sensor dataset~\cite{vaizman2017recognizing}, which servers as the sensor data source for \Dataset.
During pretraining, all sensor samples are aligned by a time window of 20 seconds.
%The experiments can be extended to other sensor application that has data readily available in the future.


%For all experiments, we randomly select 80\% of the QA pairs in \Dataset for training or finetuning and use the rest 20\% for testing. 
%\textcolor{red}{In Sec.XX, we further evaluate the gene}
%For all methods, we use the first 48 users' data for training and the remaining 12 for testing. 
%The training process for \Method includes training the sensor and label encoder with sensor data, followed by finetuning the LLaMA model using both the QA data and the stored sensor database.


\textbf{Dataset Variants and Metrics}
We evaluate three versions of \Dataset~\cite{sensorqa} using various metrics to assess both the quality and accuracy of the generated answers.
\begin{itemize}[topsep=0pt, itemsep=0pt]
    \item \textbf{Full answers} refer to the original full responses  in \Dataset. We evaluate the model's performance on the full answers dataset using Rouge-1, Rouge-2, and Rouge-L scores~\cite{eyal-etal-2019-question}. Rouge scores measure the overlap of n-grams between the machine-generated content and the ground-truth answers, expressed as F-1 scores. Higher Rouge scores indicate greater similarity between the generated and true answers.
    \item \textbf{Short answers} are the 1-2 key words extracted from the full answers by GPT-3.5-Turbo~\cite{gpt-3.5}, offered with the original \Dataset dataset~\citesensorqa. We use the exact match accuracy on the short answers to evaluate the precision of generated answers, as detailed in Sec.~\ref{sec:motivation}.
    \item \textbf{Multiple choices} are generated by prompting GPT-3.5-Turbo~\cite{gpt-3.5} to create three additional choices similar to the correct short answer. An example QA can be "Which day did I spend the most time with coworkers? A. Friday, B. Monday, C. Thursday, D. Wednesday", with the correct answer being "D" or "D. Wednesday." The models are expected to accurately select the correct answer from the four candidates. We evaluate the performance based on exact answer selection accuracy.
\end{itemize}
We create the multiple-choice version in addition to the full answers and short answers provided in the original \Dataset dataset~\citesensorqa, to further assess the model's ability in distinguishing similar facts based on sensor data. 
We use different dataset variants to assess various aspects of the models. The full answers dataset evaluates overall language quality, while the short answers and multiple-choice evaluations focus on the model's ability to learn underlying facts rather than relying solely on patterns in language token generation. Further discussion of the evaluation metrics can be found in Sec.~\ref{sec:future-work}.
%the original questions and answers collected from workers, a short-answer version, and a multiple-choice version. For the short-answer version, we use GPT-3.5 to extract 1-2 key words from the original answers. For the multiple-choice version, we use GPT-3.5 to generate three additional choices that are similar to the correct short answer, converting it into a multiple-choice question. The full-answer version is suitable for evaluating the language quality of the responses, while the short-answer and multiple-choice versions facilitate exact match accuracy evaluation, highlighting the precision of the answers.


\vspace{-2mm}
\subsection{State-of-the-Art Baselines}



We compare \Method against several state-of-the-art baselines that leverage different modalities combinations, including \textcolor{mygreen}{text}, \textcolor{myred}{vision+text}, and \textcolor{myblue}{sensor+text} data. These comparisons highlight the effectiveness of integrating multiple modalities to address the sensor-based QA tasks. 
We consider both closed-source and open-source baselines for a comprehensive analysis.

%For visual inputs, we use the activity graphs created when generating \Dataset.
 % to address natural QA interactions with sensors.

We first evaluate the state-of-the-art closed-source generative models using various modalities:
\begin{itemize}[topsep=0pt, itemsep=0pt]
    \item \textbf{GPT-3.5-Turbo~\cite{gpt-3.5} and GPT-4~\cite{gpt-4}} are \textcolor{mygreen}{text-only} baselines taking only the questions as inputs.
    \item \textbf{GPT-4-Turbo~\cite{gpt-4} and GPT-4o~\cite{gpt-4}} are \textcolor{myred}{vision+text} baselines taking the activity graphs and the questions as inputs. For all \textcolor{myred}{vision+text} baselines, we feed the activity graphs in \Dataset~\citesensorqa, similar to Fig.~\ref{fig:example_qas}, along with the questions into the model.
    \item \textbf{IMU2CLIP+GPT-4\footnote{\url{https://github.com/facebookresearch/imu2clip}}~\cite{moon-etal-2023-imu2clip}} is the state-of-the-art \textcolor{myblue}{sensor+text} GPT baseline as explained in Sec.~\ref{sec:motivation}.
\end{itemize}
For these closed-source generative models, we use few-shot learning (FSL). Specifically, we incorporate a set of QA examples from \Dataset~\citesensorqa into the prompt for each question input. We adopt $10$ samples per question based on a grid search of $\{2, 5, 10, 15\}$.





%The various combinations of modalities include \textcolor{mygreen}{text-only}, \textcolor{myred}{vision+text} and \textcolor{myblue}{sensor+text}.
%Given that the innovative aspect of our \Method QA model lies in its fine-tuning approach, we select baselines that either train a neural network or finetune a language model using the \Method training dataset and subsequently evaluate their performance on the \Method test dataset.

We further conduct comprehensive evaluation with the state-of-the-art open-source models using various modalities:
\begin{itemize}[topsep=0pt, itemsep=0pt]
    \item \textbf{T5~\cite{2020t5}} and \textbf{LLaMA\footnote{\url{https://www.llama.com/}}~\cite{touvron2023llama}} are popular \textcolor{mygreen}{text-only} language models.
    
    \item \textbf{LLaMA-Adapter\footnote{\url{https://github.com/OpenGVLab/LLaMA-Adapter}}~\cite{zhang2023llama}} is a recent \textcolor{myred}{vision+text} framework offering a lightweight method for fine-tuning instruction-following and multimodal LLaMA models. It integrates vision inputs (i.e., activity graphs from \Dataset~\citesensorqa) with LLMs using a transformer adapter. We utilize the latest LLaMA-Adapter V2 model.
    
    \item \textbf{LLaVA-1.5\footnote{\url{https://llava-vl.github.io/}}~\cite{liu2024improved}} represents state-of-the-art \textcolor{myred}{vision+text} model. LLaVA connects pre-trained CLIP ViT-L/14 visual encoder~\cite{radford2021learning} and large language model Vicuna~\cite{vicuna2023}, using a projection matrix. LLaVA-1.5~\cite{liu2024improved} achieves state-of-the-art performance on 11 benchmarks through simple modifications to the original LLaVA and the use of more extensive public datasets for finetuning.
    
    \item \textbf{DeepSQA\footnote{\url{https://github.com/nesl/DeepSQA}}~\cite{xing2021deepsqa}} trains a CNN-LSTM model with compositional attention to fuse \textcolor{myblue}{sensor+text} modalities and predict from a fixed and limited set of candidate answers given questions and IMU signals. We adapt their implementation to use the full-history timeseries data as input, to align with \Method's setup.
    
    \item \textbf{OneLLM\footnote{\url{https://github.com/csuhan/OneLLM}}~\cite{han2023onellm}} is a state-of-the-art multimodal LLM framework that processes \textcolor{myblue}{sensor+text} modalities using a universal pretrained CLIP encoder and a mixture of projection experts for modality alignment. We adapt their implementation and feed the full-history timeseries data from the IMU tokenizer.
\end{itemize}
All baselines based on LLaMA (that is, LLaMA, LLaMA-Adapter, and OneLLM) use LLaMA2-7B~\cite{touvron2023llama}.
For T5 and DeepSQA, we train the models directly on the \Dataset dataset~\citesensorqa.
For the LLM baselines, we apply LoRA fine-tuning (FT)~\cite{hu2021lora} using the samples from \Dataset~\citesensorqa. 
For all methods that require training, we randomly select 80\% of the QA pairs in \Dataset~\citesensorqa as training samples and reserve the remaining 20\% for testing. We explore alternative splitting schemes in Sec.~\ref{sec:generalizability} to demonstrate \Method's generalizability to unseen users.
All baselines adopt the same hyperparameters as those specified in their official codebases.
%
We did not compare with the latest works of Sensor2Text~\cite{chen2024sensor2text}, PrISM-Q\&A~\cite{arakawa2024prism}, and DrHouse~\cite{yang2024drhouse} due to limited access to their open-source code and models.



\begin{figure*}[tb]
  \centering
  \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/qual_example_1.png}
        \vspace{-6mm}
        \caption{Example of a time query question on a single-day duration.}
        \label{fig:qual-daily}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.82\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/qual_example_2.png}
        \vspace{-6mm}
        \caption{Example of a counting question on a multi-day duration.}
        \label{fig:qual-week}
    \end{subfigure}
  \vspace{-4mm}
  \caption{Qualitative results of \Method in comparison to state-of-the-art methods.}
  \vspace{-6mm}
  \label{fig:qual_results}
\end{figure*}




\subsection{Qualitative Performance}
\label{sec:qualitative}

We illustrate a qualitative comparison of \Method with the top-performing baselines in Fig.~\ref{fig:qual_results}. 
Specifically, Fig.~\ref{fig:qual_results} (a) focuses on time-related queries and (b) focuses on counting questions, which are the most challenging for SOTA methods given the long-duration time series sensor inputs (see Sec.~\ref{sec:motivation}).
Key phrases in the answers are highlighted in the green if they closely match the true answer, and in the red if they do not. The two presented examples are very challenging for state-of-the-art baselines. %, with GPT-4o in Fig.~\ref{fig:qual_results} (b) being the only correct instance.
\Method, on the other hand, consistently produces more accurate answers which can be attributed to its novel three-stage pipeline.

Answering the question in Fig.~\ref{fig:qual_results} (a) requires two major steps: (i) calculating the total time spent in school and in the main workplace, and (ii) computing the difference between them.
\Method accurately answers it by first decomposing and then querying the durations for ``at school'' and ``in the main workplace'' respectively, resulting in ``\textit{You spent 11 hours and 27 minutes at school Tuesday. You spent 9 hours and 3 minutes at main workplace}''. Finally, \Method integrates the above text and determines the time difference during the answering stage.
Although \Method's answer is 15 minutes off from the true value, possibly due to inaccuracies in the sensor encoder or LLM reasoning, \Method still approximates the ground truth with an accuracy unmatched by other baselines.


In Fig.~\ref{fig:qual_results} (b), counting the total days spent at home requires long-term reasoning where  LLaMA-Adapter~\cite{zhang2023llama} and  DeepSQA~\cite{xing2021deepsqa} usually fall short.
\Method decomposes this question and queries the duration of "at home" on "each day", then leaving the counting task to the answering stage.
In a nutshell, \Method relies on question decomposition and sensor data queries to extract relevant key sensor information, while the answer assembly stage handles reasoning and produces the final answer.
This collaboration across the three stages allows \Method to effectively manage a wide range of tasks, particularly those requiring multi-step reasoning and quantitative analyzes, which highlights \Method's advancements over existing works.
%The three-stage design in \Method and the intelligent work division between each stage enables \Method to properly handle such challenging tasks requiring multi-step reasoning.


\begin{table*}[t]
{
\footnotesize
\centering
\begin{tabular}{c|c|c|ccc|c|c}
\toprule
 \textbf{Modalities} & \textbf{Method} & \textbf{FSL/FT$^1$} & \multicolumn{3}{c|}{\textbf{Full Answers}} & \textbf{Short Answers} & \textbf{Multiple Choices} \\
 & & & Rouge-1 ($\uparrow$) & Rouge-2 ($\uparrow$) & Rouge-L ($\uparrow$) & Accuracy ($\uparrow$) & Accuracy ($\uparrow$) \\
\midrule
\textcolor{mygreen}{Text} & GPT-3.5-Turbo~\cite{gpt-3.5} & FSL & 0.35 & 0.23 & 0.32 & 0.03 & 0.33 \\
\textcolor{mygreen}{Text} & GPT-4~\cite{gpt-4} & FSL & 0.66 & 0.51 & 0.64 & 0.16 & 0.34 \\
\textcolor{mygreen}{Text} & T5-Base~\cite{2020t5} & FT & 0.71 & 0.55 & 0.69 & 0.25 & 0.52 \\
%\hline
\textcolor{mygreen}{Text} &  LLaMA2-7B~\cite{llama2} & FT & \underline{0.72} & \underline{0.62} & \underline{0.72} & 0.26 & \underline{0.56} \\
\hline
\textcolor{myred}{Vision+Text} & GPT-4-Turbo~\cite{gpt-4} & FSL & 0.38 & 0.28 & 0.36 & 0.14 & 0.24 \\
\textcolor{myred}{Vision+Text} & GPT-4o~\cite{gpt-4} & FSL & 0.39 & 0.28 & 0.37 & 0.20 & 0.07 \\
%\hline
\textcolor{myred}{Vision+Text} & LLaMA-Adapter~\cite{zhang2023llama} & FT & 0.73 & $0.57$ & $0.71$ & \underline{0.28} & 0.54 \\
%\hline
\textcolor{myred}{Vision+Text} & LLaVA-1.5~\cite{liu2024improved} & FT & $0.62$ &  $0.46$ & $0.60$ & 0.21 & 0.47 \\
\hline
\textcolor{myblue}{Sensor+Text} & IMU2CLIP-GPT4~\cite{moon-etal-2023-imu2clip} & FSL & 0.44 & 0.28 & 0.40 & 0.13 & 0.18 \\ 
\textcolor{myblue}{Sensor+Text} & DeepSQA~\cite{xing2021deepsqa} & FT & 0.34 & 0.05 & 0.34 & 0.27 & - \\
\textcolor{myblue}{Sensor+Text} & OneLLM~\cite{han2024onellm} & FT & 0.12 & 0.04 & 0.12 & 0.05 & 0.30 \\
\midrule
\textcolor{myblue}{Sensor+Text} & \textbf{\MethodC} & FT & \textbf{0.77}& \textbf{0.62} & \textbf{0.75} & \textbf{0.54} & \textbf{0.70} \\
\textcolor{myblue}{Sensor+Text} & \textbf{\MethodE} & FT & 0.76 & 0.60 & 0.74 & 0.49 & 0.67 \\
\bottomrule
\multicolumn{8}{l}{$^{1}$\small{FS: Few-Shot Learning. FT: Finetuning.}} \\
\end{tabular}
}
\vspace{-1mm}
\caption{Quantitative results of \Method compared against state-of-the-art methods. Bold and underlined values show the best results (all achieved by \Method) and the best among baselines.} % *DeepSQA only considers classification problem thus is only evaluated on the exact-match version of the dataset.}
\vspace{-7mm}
\label{tab:quant_results}
\end{table*}




\subsection{Quantitative Performance}

Table~\ref{tab:quant_results} presents the quantitative results of all methods on the three variants of \Dataset~\citesensorqa: full answers, short answers, and multiple-choice. Note that DeepSQA~\cite{xing2021deepsqa} was not evaluated on the multiple-choice version due to its inability to handle dynamic answer choices.
It is important to highlight that exact match accuracy for short and multiple-choice answers is a strict metric, as it requires the model to generate answers in the \textit{exact} same form as the correct ones. For instance, "4 hours" and "3 hours 50 min" would be considered different. Answering multiple-choice questions can also be particularly challenging, as candidate answers differ only slightly, such as "A. 10 min" vs. "B. 20 min," making them difficult to distinguish. In practical applications, however, a QA system does not necessarily need to achieve perfect exact match accuracy to be useful. We leave the exploration of more advanced metrics that better align with user satisfaction for future work, as discussed in Sec.~\ref{sec:future-work}.

As shown in Table~\ref{tab:quant_results}, our \Method outperforms the best state-of-the-art methods with the \textbf{highest Rouge scores on full answers}, \textbf{26\% higher accuracy on short answers}, and \textbf{14\% higher accuracy on multiple choices}. 
The top Rouge scores highlight \Method's ability to generate high-quality natural language that are the most similar with the ground-truth answers in \Dataset~\citesensorqa.
Even under the strict exact match evaluation, \Method achieves 54\% accuracy on short answers and 70\% on multiple-choice questions. These accuracy improvements demonstrate \Method's effectiveness in learning the underlying activity facts from the long-duration, multimodal time series sensor data. As explained in Sec.~\ref{sec:qualitative}, all three stages are crucial for achieving high accuracy, including correct question decomposition to invoke the right functions, precise sensor queries to accurately extract activity information from sensor data, and effective answer assembly for generating natural language responses.
\MethodC performs slightly better than \MethodE mainly due to the stronger model capability of GPTs compared to quantized LLaMA in question decomposition. However, \MethodE, as a pure edge solution, preserves better user privacy as discussed in Sec.~\ref{sec:system-implementation}.
%In future work, we plan to explore new metrics that allow small time drifts thus better fit practical use cases.

%The full answer dataset uses Rouge scores to measure the ``similarity'' between the model's generated answers and the true answers. Hence the full answer dataset focuses on the general text quality rather than precision. For example, an answer with similar wording but different key information, e.g., ``1 hour'' vs ``10 hours'', may still receive high Rouge scores.
%The short-answer and multiple-choice variants assess precision by comparing whether the exact key information is present in the answer.
%Together, these three datasets provide a comprehensive evaluation of the user-sensor QA task.


In contrast, all baselines struggle with quantitative accuracy, with an highest accuracy of merely 28\% on the short answers.
Despite GPT-4's strong reasoning capabilities and its multimodal variants (GPT-4-Turbo and GPT-4o), the models do not perform optimally for processing sensor data and daily life activities, as they are not specifically trained for these tasks.
The Rouge scores are lower because GPTs tend to generate longer text, resulting in less similarity with the ground-truth answers.
Interestingly, text-only baselines with finetuning, such as LLaMA2-7B~\cite{touvron2023llama} and T5~\cite{2020t5}, achieve some of the highest accuracy among the baselines, with 27\% accuracy on short answers and 56\% accuracy on multiple-choice datasets. These results suggest a sensor bias in the dataset, where some questions can be ``guessed'' correctly without sensor data.
Accuracy scores lower than these baselines indicate ineffective fusion of text and sensor data, as seen with models like Llava-1.5~\cite{liu2024improved} and OneLLM~\cite{han2024onellm}, whose poor performance stems from mismatches in pretraining and finetuning data formats. For example, OneLLM was designed for aligning raw IMU signals with fixed window sizes, making it challenging to adapt to long-duration sensor data in our task. Finally, DeepSQA~\cite{xing2021deepsqa} and LLaMA-Adapter~\cite{zhang2023llama} perform best among the baselines but struggle with accurate quantitative analysis on long-duration time series, as discussed in Sec.\ref{sec:motivation} and Sec.\ref{sec:qualitative}.


%through LLM-powered question decomposition and answer assembly stages. The accuracy gains demonstrate the effectiveness of sensor query and the fusion with text. 
%The GPT series are hindered by limited prompt length. If the narrative text does not fit in the length, then the prompt will be truncated and important information may be lost, leading to poor quality answers such as IMU2CLIP+GPT-4 in Fig~\ref{fig:qual_results} (b).
%The LLaMA-based models can adjust the answer according to the question and sensory input (if any), but struggles with accurate quantitative outputs. 

\iffalse
\subsection{Memory Requirements on Edge Devices}
\label{sec:memory}
\textcolor{red}{To be updated} Fig.~\ref{fig:memory} compares the model size requirements of all finetuning methods. The left plot shows unquantized LLM models while the right plot shows quantized LLMs by AWQ~\cite{lin2023awq} and small models such as DeepSQA~\cite{xing2021deepsqa} and T5~\cite{2020t5}.
Without quantization, all LLM-based methods require at least 13.5GB memory to accommodate model weights, making them unsuitable for edge deployment.
With AWQ quantization, \Method reduces its memory footprint to 3.8GB and fits into the 8GB RAM of a Jetson TX2. While non-LLM models, such as DeepSQA and T5, use less than 1GB of memory, they provide less natural and accurate answers as shown in the previous section. Multimodal LLMs require more memory to support adapter layers, where AWQ cannot be applied directly.
%
Notably, \Method consumes nearly the same memory as the text-only LLaMA2-7B. 
Such a negligible memory overhead can be attributed to the compression of encoding raw timeseries into logits, which reduces the total dataset size from 24G to 484K, achieving a compression ratio of approximately 50K times.
The sensor encoder and logits database enable \Method to run lightweight query search on edge devices.
\fi
%Compression ratio from raw data to logits.

\begin{figure}
\begin{center}
\vspace{-4mm}
\includegraphics[width=0.8\textwidth]{figs/efficiency2.png} 
\vspace{-4mm}
\caption{The answer accuracy and latency trade-offs of all methods measured on the cloud and edge platforms. 
We evaluate \MethodC on A100~\cite{a100} and \MethodE on Jetson Orin NX~\cite{jetsonorin}.
All LLM-based models are quantized to 4-bit weights with AWQ~\cite{lin2023awq} for Jetson Orin deployments.}
\label{fig:latency}
\end{center}
\vspace{-4mm}
\end{figure}


\subsection{End-to-End Answer Generation Latency}
To evaluate efficiency, we measure the end-to-end answer generation latency of \MethodC and \MethodE on their respective platforms - cloud server and NVIDIA Jetson Orin. For a fair comparison, we quantize all LLM-based baselines to 4-bit weights using AWQ~\cite{lin2023awq} for the Jetson Orin experiments, matching \MethodEâ€™s setup. \textbf{\Method takes an average generation latency of 2.3s on the cloud and 10.0s on the Jetson Orin}.  
Specifically, \MethodC takes an average of 1.2s for question decomposition, 0.5s for sensor data query, and 0.6s for answer assembly. \MethodE takes an average of 2.5s, 4.9s, and 2.5s for each stage, respectively.


The accuracy-latency trade-off of \Method and locally running baselines is shown in Fig.~\ref{fig:latency}. Despite higher latency due to its dual-LLM design, \Method outperforms other baselines in accuracy. \Method still achieves real-time responses on the cloud and can be deployed on resource-constrained devices with reasonable generation latency.
Notably, quantization causes negligible accuracy loss in \MethodE compared to the full-precision \MethodC. This is due to \Method's design, which converts sensor data into text, making the final answer assembly a purely language-based task. Techniques like AWQ~\cite{lin2023awq} minimize accuracy degradation in language tasks after quantization.
We plan to further optimize the latency of \Method on edge devices via algorithm-system co-design in future work.
%All quantized LLM-based methods achieve negligible accuracy degradation compared to the full-precision models as in Table~\ref{tab:baseline_results_unfiltered}.
%Methods closer to the top left corner are preferred for their higher accuracy and lower latency.
%This indicates that \Method achieves real-time interactions on desktop-level edge devices and acts a feasible solution on more resource-constrained devices.
%On Jetson TX2, DeepSQA~\cite{xing2021deepsqa} encounters OOM issues with the large multi-day timeseries input. T5-Base without TensorRT~\cite{tensorrt} optimization cannot run neither due to its large model size. While T5-Small~\cite{2020t5} is very lightweight, it lacks sensor data, leading to low accuracy.
%Remarkably, \Method's latency is nearly identical to the text-only LLaMA2-7B, indicating minimal overhead from query searches. It also reduces latency by 38\% compared to the complex OneLLM model.


%On the desktop, all methods are very efficient.

\iffalse
\begin{figure*}[tb]
  \centering
  \begin{subfigure}[b]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/memory.png} 
        \vspace{-4mm}
        \caption{Memory requirements of quantized and unquantized models from various methods. The red horizontal line shows the RAM size limit of 8GB on Jetson TX2~\cite{jetsontx2}.}
        \label{fig:memory}
    \end{subfigure} \hspace{0.02\textwidth} % Add horizontal space 
    \begin{subfigure}[b]{0.44\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/efficiency.png} 
        \vspace{-4mm}
        \caption{Efficiency of fine-tuning methods on desktop and Jetson TX2~\cite{jetsontx2}. The footnote $Q$ indicates quantized models.}
        \label{fig:efficiency}
    \end{subfigure}
  \vspace{-4mm}
  \caption{\textcolor{red}{To be updated.}}
  \vspace{-6mm}
  \label{fig:qual_results}
\end{figure*}
\fi





%\begin{itemize}
%    \item original AWQ (4bit)
%    \item personalized AWQ
%    \item original squeeze LLM (3bit)
%    \item personalized squeeze LLM
%\end{itemize}



\subsection{Ablation Studies}
\label{sec:ablation}

In this section, we comprehensively examine the impact of key design choices in \Method. Without loss of generality, we use \MethodC as the base model.
%major design variants in \Method by isolating one stage while keeping the other two fixed.
%This is because all three stages are essential for \Method and removing any one of them would disable the system entirely. 
%For example, without question decomposition, \Method would not know how to query the database. 
%We skip the answer assembly from discussion as it has few variants that are impactful to end-to-end performances.


\begin{table}[!t]
\small
\centering
%\vspace{-2mm}
\caption{Impact of three major stage in \MethodC. Bold values highlight the best results.}
\vspace{-4mm}
\label{tbl:ablation}
\begin{tabular}{c|ccc|c} 
\toprule
\small
\textbf{Setup} & \multicolumn{3}{c|}{\textbf{Full Answers}} & \textbf{Short Answers} \\ 
& Rouge-1 ($\uparrow$) & Rouge-2 ($\uparrow$) & Rouge-L ($\uparrow$) & Accuracy ($\uparrow$) \\
\midrule
w/o Question Decomposition & 0.73 & 0.57 & 0.71 & 0.35 \\
w/o Sensor Data Query & 0.72 & 0.62 & 0.72 & 0.26 \\ 
w/o Answer Assembly & 0.26 & 0.08 & 0.24 & 0.0 \\
\midrule
Full \Method & \textbf{0.77}& \textbf{0.62} & \textbf{0.75} & \textbf{0.54} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

\textbf{Impact of Each Stage in \Method} 
We first evaluate the individual contribution of each stage in \Method by removing one of them from the pipeline. 
By removing question decomposition, we use a fixed and general decomposition templates for all questions. Removing sensor data query reverts the model to a language-only approach. By removing answer assembly, we directly output the queried sensor context from the second stage.
Table~\ref{tbl:ablation} summarizes the results on full and short answers including both quality and quantitative accuracy.
As observed, removing any stage leads to a significant performance drop. 
In \Method, all three stages must work collaboratively to deliver high-quality, accurate answers across diverse question types in \Dataset.
Among the three stages, the answer assembly stage has the most significant impact, as it directly influences the final output. Removing it results in severely degraded performance, with near-zero accuracy on short answers. However, the question decomposition and sensor data query stages are equally crucial.


\begin{table}[!t]
\small
\centering
%\vspace{-2mm}
\caption{Impact of various designs for sensor-text pretraining in \MethodC. Bold values highlight the best results.}
\vspace{-4mm}
\label{tbl:ablation-sensor-feature}
\begin{tabular}{c|c|c|c} 
\toprule
\small
\textbf{Sensor Data} & \textbf{Training Loss} & \textbf{Online Querying} & \textbf{Multiple Choices} \\ 
& & Accuracy ($\uparrow$) & Answer Accuracy ($\uparrow$) \\
\midrule
Statistical features & Partial-Context Loss & 0.91 & 0.62 \\
Time series & IMU2CLIP~\cite{moon-etal-2023-imu2clip} & 0.90 & 0.61  \\ \midrule
Time series & Partial-Context Loss & \textbf{0.98} & \textbf{0.70} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}

% If use F1, statistical features: 0.58, CLIP: 0.56, timeseries & our loss: 0.83

\textbf{Impact of Sensor Features and Pretraining Loss Functions}
We next evaluate the impact of sensor features and loss functions during offline encoder pretraining.
Specifically, we compare using statistical features (e.g., mean acceleration) versus raw time series inputs, and IMU2CLIP loss~\cite{moon-etal-2023-imu2clip} versus our proposed contrastive sensor-text pretraining loss for partial context (see Sec.\ref{sec:pretraining}). 
For IMU2CLIP, text samples are generated by combining all valid labels into one sentence. We report the online querying accuracy and multiple-choice answer accuracy to assess the influence to sensor information extraction.
As shown in Table~\ref{tbl:ablation-sensor-feature}, statistical features result in low querying and answer accuracies. This validates our motivation to design \Method that high-dimensional time series sensor data are critical for fine-grained activity information. IMU2CLIP training, even with fine-grained data, yields poorer querying and answering accuracies, highlighting its limited ability associating sensor embeddings from partial text query. Our proposed loss function, which aligns sensor and text encoders for partial context queries, proves more effective. These findings emphasize the importance of selecting appropriate sensor features and loss functions during pretraining in order to achieve high-performance QA.


\textbf{Impact of LLM Design Choices}
%Question decomposition, as the first stage, is critical for \Method diving into the correct direction.
We finally evaluate the impact of various design choices for LLMs. 
In question decomposition, we assess the contribution of in-context learning (ICL), chain-of-thought (CoT) techniques, and different backbone LLMs.
The results are summarized in Table~\ref{tbl:ablation-detailed-design}. 
Both ICL and CoT are crucial for high-quality and accurate answers. This is because an effective question decomposition improves sensor data queries. The solution templates in ICL are more essential to \Method as removing ICL reduces answer accuracy by 11\%. CoT enhances reasoning and slightly boosts accuracy by 1-5\%. Interestingly, using a more advanced backbone (GPT-4 vs. GPT-3.5-Turbo) results in minimal improvement in answer quality, as GPT-4, while generating richer text, does not follow instructions as well as GPT-3.5-Turbo according to our observation.

For answer assembly, we evaluate the effectiveness of finetuning compared to zero-shot or few-shot learning, as well as different LLaMA backbones. As shown in Table~\ref{tbl:ablation-detailed-design}, zero-shot learning results in poor performance, while few-shot learning improves answer quality but still lags behind finetuning. This highlights that finetuning is the most effective approach when a dataset like \Dataset is available. Using a more advanced LLaMA backbone, such as LLaMA3-8B, has minimal impact. Finetuning and the dataset prove to be more important than the model architecture during answer assembly.

%shows the performance of various question decomposition designs, including different LLMs and the use of in-context learning (ICL) and chain-of-thought (CoT) techniques.
%The closed-source GPT models generally yield more accurate decompositions due to their larger parameter sets.
%GPT-4 queries take 1x longer, while the latencies for queries without ICL and without CoT appear to be uncorrelated with prompt length, possibly due to OpenAI's internal setup.
%Interestingly, the latencies of GPT decompositions may not be directly related to prompt length. 
%Surprisingly, GPT-3.5-Turbo without ICL demonstrates the longest average latency, despite having shorter prompts without solution templates. This may be due to OpenAI's internal setup. GPT-4 queries take approximately 1x longer time to return. 
%Notably, using GPTs here only requires sending the questions to cloud but not the raw sensor data, thus still protecting sensitive information.
%The quantized LLaMA3-8B~\cite{lin2023awq} offers a trade-off between privacy and performance. Using quantized LLaMA3-8B preserves perfect privacy by avoiding transmitting both questions and sensor data to cloud, but leads to 4-11\% lower final answer accuracy and a longer latency per query of 8.1 seconds on a desktop.


%\textbf{Impact of Finetuning Designs}
%Table~\ref{tbl:ablation-finetuning}



\begin{table}[!t]
\small
\centering
\caption{Impact of various design choices for LLMs in \MethodC. Bold values highlight the best results.}
\vspace{-4mm}
\label{tbl:ablation-detailed-design}
\begin{tabular}{c|c|c|ccc|c} 
\toprule
\small
\textbf{Stage} & \textbf{Setup} & \textbf{Model in Stage} & \multicolumn{3}{c|}{\textbf{Full Answers}} & \textbf{Short Answer} \\ 
& & & Rouge-1 ($\uparrow$) & Rouge-2 ($\uparrow$) & Rouge-L ($\uparrow$) & Accuracy ($\uparrow$) \\
\midrule
& w/o ICL & GPT-3.5-Turbo & 0.75 & 0.59 & 0.72 & 0.43 \\
Question & w/o CoT & GPT-3.5-Turbo & 0.76 & 0.61 & 0.74 & 0.50 \\ 
Decomposition & Full & GPT-4 & \textbf{0.77} & \textbf{0.62} & \textbf{0.75} & 0.49 \\
& Full & GPT-3.5-Turbo & \textbf{0.77} & \textbf{0.62} & \textbf{0.75} & \textbf{0.54} \\ \hline
& Zero-shot learning & LLaMA2-7B & 0.16 & 0.07 & 0.14 & 0.0 \\
Answer & Few-shot learning & LLaMA2-7B & 0.43 & 0.29 & 0.41 & 0.24 \\
Assembly & Finetuning & LLaMA3-8B & 0.76 & 0.61 & 0.74 & 0.53 \\
& Finetuning & LLaMA2-7B & \textbf{0.77} & \textbf{0.62} & \textbf{0.75} & \textbf{0.54} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
\end{table}








\iffalse
\begin{table}[!t]
\footnotesize
\centering
\vspace{-2mm}
\caption{Impact of designs in finetuning.}
\vspace{-2mm}
\label{tbl:ablation-finetuning}
\begin{tabular}{c|c|c|c} 
\toprule
\small
\textbf{Model} & \textbf{Method} & \textbf{Short Answer} & \textbf{Multiple Choices} \\ 
& & \textbf{Accuracy} & \textbf{Accuracy} \\ \hline
\multirow{2}{*}{LLaMA2-7B} & LoRA Finetuning & 0.54 & 0.69 \\
 & Full Finetuning & & \\
\multirow{2}{*}{LLaMA3-8B} & LoRA Finetuning & 0.53 & 0.70 \\
& Full Finetuning & \\
\bottomrule
\end{tabular}
%\vspace{-2mm}
\end{table}
\fi


\begin{figure}[t]
   \centering
    \setlength{\tabcolsep}{0.2pt}
\begin{tabular}{cccc}
        \vspace{-2mm}
        \includegraphics[width=0.22\textwidth, height=2.2cm]{figs/lr-gpt-shortened.png} &
        \includegraphics[width=0.22\textwidth, height=2.15cm]{figs/rank-gpt-shortened.png} &
        \includegraphics[width=0.22\textwidth, height=2.2cm]{figs/temperature-gpt-shortened.png} &
        \includegraphics[width=0.22\textwidth, height=2.15cm]{figs/query-threshold.png} \\ 
        %{\footnotesize (a) Working memory size} &
        %{\footnotesize (b) Novelty threshold} &
        %{\footnotesize (c) Merging sensitivity} \\
\end{tabular}
\vspace{-3mm}
    \caption{Sensitivity of key hyperparameters.}
    \label{fig:sensitivity}
    \vspace{-5mm}
\end{figure}



\subsection{Sensitivity Analyses}
%q, k, $\tau$, learning rate, lora rank
Fig.~\ref{fig:sensitivity} shows the sensitivity of key parameters in \Method.
while the less sensitive ones are omitted due to space limitation.
%We focus on short answer accuracy, as other metrics show similar trends.
The default parameter setting is the same as described in Sec.~\ref{sec:system-implementation}.
We mainly focus on evaluating the short answers to assess the parameters' impact on factual information extraction.

\textbf{Learning Rate in Answer Assembly} 
Fig.~\ref{fig:sensitivity} (leftmost) shows the short answers accuracy for learning rates of $\{1e-5, 5e-5, 1e-4, 2e-4, 5e-4\}$ during finetuning.
Larger learning rates result in bigger gradient steps during LoRA finetuning, with $1e-4$ providing the best performance for our task.

\textbf{LoRA Rank in Answer Assembly}
Fig.~\ref{fig:sensitivity} (middle left) shows the short answers accuracy for LoRA ranks of $\{8, 16, 32, 64\}$ during finetuning.
The rank affects the size of the LoRA adapter weights. Higher ranks mean more parameters and a larger weight space to optimize. 
For our task, varying ranks have little impact on final accuracy, with rank 32 achieving the best performance for short answers.

\textbf{Generating Temperature in Answer Assembly}
Fig.~\ref{fig:sensitivity} (middle right) shows the short answers accuracy for generating temperatures of $\{0.01, 0.1, 0.2, 0.5\}$ during the final answer generation.
Higher temperatures instruct the LLM to use more ``creativity''. For our task, varying temperatures have negligible impact on the short answers accuracy, indicating minimal impact to the sensor information extraction.

\textbf{Query Threshold in Sensor Data Query} Fig.~\ref{fig:sensitivity} (rightmost) shows the F1 scores of online querying at different query thresholds $h=\{0.2, 0.4, 0.5, 0.6, 0.8\}$. %As discussed in Sec.~\ref{sec:ablation}, the higher the BA, the more accurate the activity classification, and the better answer accuracy we get from \Method.
%While increasing the threshold reduces the BA, the reduction is minimal thus \Method is generally robust to various $h$.
Raising the threshold $h$ excludes less confident positive predictions, which can improve F1 scores. However, this may also overlook some detailed events, potentially reducing answer accuracy. Ideally, $h$ should be calibrated individually for each user to achieve the best results.

\iffalse
\textbf{Temperature during answer generation}
Fig.~\ref{fig:sensitivity} (right) shows the final accuracy for temperatures of $\{0.01, 0.1, 0.2, 0.5\}$.
Temperature balances LLaMA's predictability and creativity. A higher temperature encourages exploration and can be helpful in answering creative questions. However, on the short answer dataset, temperature has little impact on final performance.
\fi


\begin{figure}[t]
  \begin{subfigure}[b]{0.55\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/legend_split.png}
        \vspace{-5mm}
    \end{subfigure}

    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/exact_split.png}
        \vspace{-6mm}
        \caption{Average short-answer accuracy on different users.}
        \label{fig:exact_split}
    \end{subfigure} \hspace{0.02\textwidth} % Add horizontal space between subfigures if needed
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/acc_split.png}
        \vspace{-6mm}
        \caption{Average online querying accuracy on different users during sensor data query.}
        \label{fig:acc_split}
    \end{subfigure} 
    \vspace{-4mm}
    \caption{Generalization of key learning components in \Method.}
    \label{fig:generalization}
    \vspace{-4mm}
\end{figure}

\subsection{Generalization and Robustness}
\label{sec:generalizability}
We evaluate \Method's generalization and robustness across different users' sensor data and QA inputs. In addition to the standard 80/20 random split, we compare results with a split where training is performed on the first 48 out of 60 users and testing includes all users. To ensure a fair comparison, we equalize the training set size in both splits by duplicating samples in the smaller set.
Our evaluation focuses on two key learning processes in \Method: LLM fine-tuning in answer assembly and sensor-text encoder pretraining.

Fig.~\ref{fig:exact_split} presents the short-answer accuracy when fine-tuning on all users' QA data versus only the first 48 users. The results show similar accuracy for both seen and unseen users, demonstrating \Method's strong generalizability in answer assembly. This is likely due to \Method's design of treating answer assembly as a pure language task. Since all users' language tokens follow similar distributions in a sensor-based QA task, generalization remains robust across user variations.

Fig.\ref{fig:acc_split} presents the online querying accuracy when pretraining with all users' sensor data versus only the first 48 users. Unlike language fine-tuning, limiting sensor data to the first 48 users leads to accuracy degradation on unseen users due to variations in data distributions. Therefore, improving \Method's generalization to new users primarily depends on developing a robust sensor and text encoder, which is a well-studied problem in existing literature~\cite{xu2023practically}. We leave the investigation for combining with these techniques in future work.
%The performance of \Method mainly depends on two learning components: the sensor and text encoder obtained in pretraining, and the finetuned LLM in answer assembly. Therefore we assess the generalization of \Method 
