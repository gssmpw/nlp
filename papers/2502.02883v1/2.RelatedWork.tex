%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-work}




%\subsection{Question Answering using Sensor Data}
\textbf{Question Answering using Sensor Data}
The QA problem has been extensively studied across various domains, including text~\cite{rogers2023qa}, visual~\cite{schwenk2022okvqa}, medical~\cite{pal2022medmcqa}, and remote sensing~\cite{hu2023rsgpt}. In the sensor domain, early works \textit{AI Therapist}~\cite{nie2022conversational} and its successor CaiTI~\cite{nie2024llm} utilized smart home devices, such as Amazon Echo, to engage in conversations with users and assess mental well-being. 
DeepSQA~\cite{xing2021deepsqa} was the first to benchmark time-series sensor-based QA for human activity recognition. It introduced SQA-GEN, an automated QA generation tool that gathers 1-minute sensor readings and generates valid Q\&A pairs by exhaustively searching six pre-defined question templates. They mainly focused on quantitative questions including time query, counting and action compare.
The authors also evaluated traditional neural network models, including CNNs and LSTMs, finding that the ConvLSTM network with Compositional Attention achieved the highest QA accuracy.
%However, DeepSQA is limited by a restricted variety of questions and answers, formulating the problem as a classification task. In contrast, \Method supports natural question types and varied time lengths.

Recent contributions have pioneered the integration of LLMs for generating more intuitive answers and better interpreting sensor data. Englhardt \textit{et al.}~\cite{englhardt2024classification}, Health-LLM~\cite{kim2024health}, and DrHouse~\cite{yang2024drhouse} converted physiological data from wearable devices, such as heart rates and daily step counts, into text prompts for LLMs, enabling more sophisticated medical and healthcare diagnoses.
The latest Sensor2Text~\cite{chen2024sensor2text} and PrISM-Q\&A~\cite{arakawa2024prism} explored natural language interactions between users and wearable devices to understand and support daily activities using sensor data, such as advising on "What should I do next with this?" Both approaches utilized LLMs as their backbone and fed sensor embeddings into the models.

While promising, existing systems exhibit significant limitations in handling long duration and complex sensor data required for accurate answers. Table~\ref{tbl:related_works} highlights the key differences between \Method and existing QA systems. In summary, \Method greatly enhances capabilities of existing systems by (1) answering questions based on long-duration sensor data spanning weeks or months, compared to the short windows of seconds or minutes in previous systems~\cite{xing2021deepsqa,chen2024sensor2text,arakawa2024prism}, and (2) encoding high-dimensional time-series data to extract fine-grained activity details for reasoning, unlike prior systems that are limited to low-dimensional sensor data~\cite{englhardt2024classification,kim2024health,yang2024drhouse}.
%However, these works use coarse-grained sensor data which cannot be easily adapted to timeseries and detailed quantitative questions.
%\Method instead focuses on analyzing rich timeseries data.
%However, these works have two drawbacks: (1) they merely input text-formatted sensor data into LLM which may not work well for timeseries; (2) their usage of closed-source LLMs such as GPT-4 and external knowledge databases are challenging for edge devices. \Method addresses both drawbacks. 
%However, all above works focused on health-related aspects and used lower dimensional of data compared to raw sensor time series. Their approaches cannot be directly transferred to daily-life activity monitoring with raw time series sensors.

%fused low-dimensional sensors data such as from smart home sensors or wristband with LLMs to infer health status.
%infer mental health and assess the user's daily functioning using GPT-3. %Their approach integrated a GPT-3-based natural language processing core to interact with users as well as detect abnormal mental status, based on 37 dimensions suggested by the therapists.

%considering only the top 27 answers and 

%A major constraint lies in their language models, which predict outputs as a classification problem - such as a 0-2 mental health core in \textit{AI therapist} and only the top 27 answers considered as the ground-truth labels in DeepSQA. This design restricts their models to a highly constrained QA scenario that may not be applicable in real life.
%In contrast, the distinctiveness of \Method lies in its capacity to accommodate "arbitrary" and "unpredictable" questions, namely open-ended question answering.

%\subsection{LLMs for Multimodal Reasoning}
%\vspace{1mm}
\textbf{LLMs for Multimodal Reasoning}
%The surge of LLM has triggered a wave of innovations in text understanding and reasoning applications.
Recent works investigated multimodal LLMs that transform other data modalities into a sequence of tokens for LLM integration~\cite{zhang2023llama,lin2024vila}.
IMU2CLIP~\cite{moon-etal-2023-imu2clip} and TENT~\cite{zhou2023tent} employed contrastive pretraining to align text with various timeseries sensor signals.
%To enable this transformation, IMU2CLIP~\cite{moon-etal-2023-imu2clip} utilized contrastive pretraining to align IMU signals with text narration~\cite{grauman2022ego4d}. Similarly, TENT~\cite{zhou2023tent} employed contrastive pretraining to align text with a broader set of IoT sensor signals, including camera video, LiDAR, and mmWave data. Both approaches have limited sensor-specific reasoning capabilities as they used a frozen LLM without further tuning or did not utilize an LLM at all. 
Recent designs like AnyMAL~\cite{moon2023anymal} and OneLLM~\cite{han2024onellm} proposed fine-tuning multimodal LLMs to process up to eight different modalities, including IMU time series, for reasoning tasks.
However, direct integration of time series sensor data to LLMs is constrained by LLM's inherent weakness in handling long-context inputs~\cite{li2024long,gu2023mamba}, making them unsuitable for processing long-duration sensor signals. In contrast, \Method overcomes this limitation by incorporating a dedicated sensor data query stage, enabling it to handle long-term queries effectively.
%requires a large instruction dataset with well-aligned multimodal data, which is lacking in multimodal sensor-based QA. 
%that accept IMU signal inputs using carefully prepared instruction datasets.

%\begin{wraptable}{r}{0.55\textwidth}
%\vspace{-4mm}
\begin{table}
\small
\caption{Comparing {\Method} and existing QA systems.}
\label{tbl:related_works}
\vspace{-4mm}
\begin{center}
\begin{tabular}{ccc} % note: no vertical bars at all
\toprule
\textbf{Existing QA systems for sensor data} & \textbf{Long-duration} & \textbf{High-dimensional} \\
& \textbf{sensor data} & \textbf{time series sensors} \\
\midrule
DeepSQA~\cite{xing2021deepsqa}, Sensor2Text~\cite{chen2024sensor2text}, PrISM-Q\&A~\cite{arakawa2024prism} & \X & \Ch \\
Englhardt \textit{et al.}~\cite{englhardt2024classification}, Health-LLM~\cite{kim2024health}, DrHouse~\cite{yang2024drhouse}& \Ch & \X \\ \midrule
 \textbf{\Method (this work)} & \textbf{\Ch} & \textbf{\Ch} \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-4mm}
\end{table}
%\end{wraptable}


\iffalse
\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/frequency_query.png}
        \vspace{-6mm}
        \caption{Example question of frequency query.}
        \label{fig:daily-freq}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/time_query.png}
        \vspace{-6mm}
        \caption{Example question of time query.}
        \label{fig:daily-time}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/day_query.png}
        \vspace{-6mm}
        \caption{Example question of day query.}
        \label{fig:weekly-day}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/activity_query.png}
        \vspace{-6mm}
        \caption{Example question of activity query.}
        \label{fig:weekly-activity}
    \end{subfigure}
    \vspace{-4mm}
    \caption{Example QA pairs that we collect from AMT. (a) and (b) are generated from daily graph, while (c) and (d) are generated from weekly graph.}
    \label{fig:example_qas}
    \vspace{-4mm}
\end{figure*}
\fi





\iffalse
Limited prior research has studied finetuning LLMs for specific reasoning tasks. Based on IMU2CLIP, AnyMAL~\cite{moon2023anymal} further finetuned the LLM by training projection layers or applying Low-Rank Adaptation (LoRA).
Nevertheless, the general fine-tuning approach of AnyMAL struggles to accurately address sensor-specific queries, such as those pertaining to specific times and locations. These challenges are effectively tackled in \Method through the introduction of two novel fine-tuning techniques, setting \Method apart from existing methodologies.
\fi

%\subsection{Human Activity Monitoring}
%\vspace{1mm}
\textbf{Mobile Systems for Daily Life Monitoring}
Researchers have explored a variety of sensing technologies for monitoring human lives, including built-in sensors on smart phones~\cite{zhang2020pdlens}, cameras~\cite{radu2019vision2sensor}, Wi-Fi signals~\cite{yang2024mm} and mmWave radar~\cite{weng2024large}.
Although these efforts have resulted in numerous open-source datasets~\cite{misc_human_activity_recognition_using_smartphones_240,misc_mhealth_dataset_319,misc_opportunity_activity_recognition_226,vaizman2017recognizing} and powerful machine learning models~\cite{ma2019attnsense,xu2021limu,deldari2022cocoa,ouyang2022cosmo,ouyang2023harmony,xu2023practically}, they fail to handle queries in natural language, which are more creative and open-ended.
%the vast majority of the existing work has focused on human activity recognition. %, a \textit{passive} way of processing information where the user has no control. 
%How to further interpret the data beyond activity classification has remained less explored.
In this paper, we design \Method to facilitate long-term QA interactions based on time series sensor data.
The methodology of \Method can be further integrated with other sensing modalities and applications.