[
  {
    "index": 0,
    "papers": [
      {
        "key": "rogers2023qa",
        "author": "Rogers, Anna and Gardner, Matt and Augenstein, Isabelle",
        "title": "Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "schwenk2022okvqa",
        "author": "Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh",
        "title": "A-okvqa: A benchmark for visual question answering using world knowledge"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "pal2022medmcqa",
        "author": "Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan",
        "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hu2023rsgpt",
        "author": "Hu, Yuan and Yuan, Jianlong and Wen, Congcong and Lu, Xiaonan and Li, Xiang",
        "title": "RSGPT: A Remote Sensing Vision Language Model and Benchmark"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "nie2022conversational",
        "author": "Nie, Jingping and Shao, Hanya and Zhao, Minghui and Xia, Stephen and Preindl, Matthias and Jiang, Xiaofan",
        "title": "Conversational ai therapist for daily function screening in home environments"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "nie2024llm",
        "author": "Nie, Jingping and Shao, Hanya and Fan, Yuang and Shao, Qijia and You, Haoxuan and Preindl, Matthias and Jiang, Xiaofan",
        "title": "LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xing2021deepsqa",
        "author": "Xing, Tianwei and Garcia, Luis and Cerutti, Federico and Kaplan, Lance and Preece, Alun and Srivastava, Mani",
        "title": "DeepSQA: Understanding Sensor Data via Question Answering"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "englhardt2024classification",
        "author": "Englhardt, Zachary and Ma, Chengqian and Morris, Margaret E and Chang, Chun-Cheng and Xu, Xuhai\" Orson\" and Qin, Lianhui and McDuff, Daniel and Liu, Xin and Patel, Shwetak and Iyer, Vikram",
        "title": "From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "kim2024health",
        "author": "Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won",
        "title": "Health-llm: Large language models for health prediction via wearable sensor data"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "yang2024drhouse",
        "author": "Yang, Bufang and Jiang, Siyang and Xu, Lilin and Liu, Kaiwei and Li, Hai and Xing, Guoliang and Chen, Hongkai and Jiang, Xiaofan and Yan, Zhenyu",
        "title": "DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "chen2024sensor2text",
        "author": "Chen, Wenqiang and Cheng, Jiaxuan and Wang, Leyao and Zhao, Wei and Matusik, Wojciech",
        "title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "arakawa2024prism",
        "author": "Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank",
        "title": "PrISM-Q\\&A: Step-Aware Voice Assistant on a Smartwatch Enabled by Multimodal Procedure Tracking and Large Language Models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xing2021deepsqa",
        "author": "Xing, Tianwei and Garcia, Luis and Cerutti, Federico and Kaplan, Lance and Preece, Alun and Srivastava, Mani",
        "title": "DeepSQA: Understanding Sensor Data via Question Answering"
      },
      {
        "key": "chen2024sensor2text",
        "author": "Chen, Wenqiang and Cheng, Jiaxuan and Wang, Leyao and Zhao, Wei and Matusik, Wojciech",
        "title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors"
      },
      {
        "key": "arakawa2024prism",
        "author": "Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank",
        "title": "PrISM-Q\\&A: Step-Aware Voice Assistant on a Smartwatch Enabled by Multimodal Procedure Tracking and Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "englhardt2024classification",
        "author": "Englhardt, Zachary and Ma, Chengqian and Morris, Margaret E and Chang, Chun-Cheng and Xu, Xuhai\" Orson\" and Qin, Lianhui and McDuff, Daniel and Liu, Xin and Patel, Shwetak and Iyer, Vikram",
        "title": "From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models"
      },
      {
        "key": "kim2024health",
        "author": "Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won",
        "title": "Health-llm: Large language models for health prediction via wearable sensor data"
      },
      {
        "key": "yang2024drhouse",
        "author": "Yang, Bufang and Jiang, Siyang and Xu, Lilin and Liu, Kaiwei and Li, Hai and Xing, Guoliang and Chen, Hongkai and Jiang, Xiaofan and Yan, Zhenyu",
        "title": "DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhang2023llama",
        "author": "Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu",
        "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention"
      },
      {
        "key": "lin2024vila",
        "author": "Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song",
        "title": "Vila: On pre-training for visual language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "moon-etal-2023-imu2clip",
        "author": "Moon, Seungwhan  and\nMadotto, Andrea  and\nLin, Zhaojiang  and\nSaraf, Aparajita  and\nBearman, Amy  and\nDamavandi, Babak",
        "title": "{IMU}2{CLIP}: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhou2023tent",
        "author": "Zhou, Yunjiao and Yang, Jianfei and Zou, Han and Xie, Lihua",
        "title": "TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "moon-etal-2023-imu2clip",
        "author": "Moon, Seungwhan  and\nMadotto, Andrea  and\nLin, Zhaojiang  and\nSaraf, Aparajita  and\nBearman, Amy  and\nDamavandi, Babak",
        "title": "{IMU}2{CLIP}: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "grauman2022ego4d",
        "author": "Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others",
        "title": "Ego4d: Around the world in 3,000 hours of egocentric video"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "zhou2023tent",
        "author": "Zhou, Yunjiao and Yang, Jianfei and Zou, Han and Xie, Lihua",
        "title": "TENT: Connect Language Models with IoT Sensors for Zero-Shot Activity Recognition"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "moon2023anymal",
        "author": "Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and others",
        "title": "Anymal: An efficient and scalable any-modality augmented language model"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "han2024onellm",
        "author": "Han, Jiaming and Gong, Kaixiong and Zhang, Yiyuan and Wang, Jiaqi and Zhang, Kaipeng and Lin, Dahua and Qiao, Yu and Gao, Peng and Yue, Xiangyu",
        "title": "Onellm: One framework to align all modalities with language"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "li2024long",
        "author": "Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu",
        "title": "Long-context llms struggle with long in-context learning"
      },
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "xing2021deepsqa",
        "author": "Xing, Tianwei and Garcia, Luis and Cerutti, Federico and Kaplan, Lance and Preece, Alun and Srivastava, Mani",
        "title": "DeepSQA: Understanding Sensor Data via Question Answering"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "chen2024sensor2text",
        "author": "Chen, Wenqiang and Cheng, Jiaxuan and Wang, Leyao and Zhao, Wei and Matusik, Wojciech",
        "title": "Sensor2Text: Enabling Natural Language Interactions for Daily Activity Tracking Using Wearable Sensors"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "arakawa2024prism",
        "author": "Arakawa, Riku and Lehman, Jill Fain and Goel, Mayank",
        "title": "PrISM-Q\\&A: Step-Aware Voice Assistant on a Smartwatch Enabled by Multimodal Procedure Tracking and Large Language Models"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "englhardt2024classification",
        "author": "Englhardt, Zachary and Ma, Chengqian and Morris, Margaret E and Chang, Chun-Cheng and Xu, Xuhai\" Orson\" and Qin, Lianhui and McDuff, Daniel and Liu, Xin and Patel, Shwetak and Iyer, Vikram",
        "title": "From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "kim2024health",
        "author": "Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won",
        "title": "Health-llm: Large language models for health prediction via wearable sensor data"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "yang2024drhouse",
        "author": "Yang, Bufang and Jiang, Siyang and Xu, Lilin and Liu, Kaiwei and Li, Hai and Xing, Guoliang and Chen, Hongkai and Jiang, Xiaofan and Yan, Zhenyu",
        "title": "DrHouse: An LLM-empowered Diagnostic Reasoning System through Harnessing Outcomes from Sensor Data and Expert Knowledge"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "moon2023anymal",
        "author": "Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and others",
        "title": "Anymal: An efficient and scalable any-modality augmented language model"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "zhang2020pdlens",
        "author": "Zhang, Hanbin and Guo, Gabriel and Song, Chen and Xu, Chenhan and Cheung, Kevin and Alexis, Jasleen and Li, Huining and Li, Dongmei and Wang, Kun and Xu, Wenyao",
        "title": "PDLens: smartphone knows drug effectiveness among Parkinson's via daily-life activity fusion"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "radu2019vision2sensor",
        "author": "Radu, Valentin and Henne, Maximilian",
        "title": "Vision2sensor: Knowledge transfer across sensing modalities for human activity recognition"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "yang2024mm",
        "author": "Yang, Jianfei and Huang, He and Zhou, Yunjiao and Chen, Xinyan and Xu, Yuecong and Yuan, Shenghai and Zou, Han and Lu, Chris Xiaoxuan and Xie, Lihua",
        "title": "Mm-fi: Multi-modal non-intrusive 4d human dataset for versatile wireless sensing"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "weng2024large",
        "author": "Weng, Yuxuan and Wu, Guoquan and Zheng, Tianyue and Yang, Yanbing and Luo, Jun",
        "title": "Large Model for Small Data: Foundation Model for Cross-Modal RF Human Activity Recognition"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "misc_human_activity_recognition_using_smartphones_240",
        "author": "Reyes-Ortiz,Jorge and Anguita,Davide and Ghio,Alessandro and Oneto,Luca and and Parra,Xavier",
        "title": "{Human Activity Recognition Using Smartphones}"
      },
      {
        "key": "misc_mhealth_dataset_319",
        "author": "Banos,Oresti and Garcia,Rafael and and Saez,Alejandro",
        "title": "{MHEALTH Dataset}"
      },
      {
        "key": "misc_opportunity_activity_recognition_226",
        "author": "Roggen,Daniel and Calatroni,Alberto and Nguyen-Dinh,Long-Van and Chavarriaga,Ricardo and Sagha,Hesam",
        "title": "{OPPORTUNITY Activity Recognition}"
      },
      {
        "key": "vaizman2017recognizing",
        "author": "Vaizman, Yonatan and Ellis, Katherine and Lanckriet, Gert",
        "title": "Recognizing detailed human context in the wild from smartphones and smartwatches"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "ma2019attnsense",
        "author": "Ma, Haojie and Li, Wenzhong and Zhang, Xiao and Gao, Songcheng and Lu, Sanglu",
        "title": "AttnSense: Multi-level attention mechanism for multimodal human activity recognition."
      },
      {
        "key": "xu2021limu",
        "author": "Xu, Huatao and Zhou, Pengfei and Tan, Rui and Li, Mo and Shen, Guobin",
        "title": "Limu-bert: Unleashing the potential of unlabeled data for imu sensing applications"
      },
      {
        "key": "deldari2022cocoa",
        "author": "Deldari, Shohreh and Xue, Hao and Saeed, Aaqib and Smith, Daniel V and Salim, Flora D",
        "title": "Cocoa: Cross modality contrastive learning for sensor data"
      },
      {
        "key": "ouyang2022cosmo",
        "author": "Ouyang, Xiaomin and Shuai, Xian and Zhou, Jiayu and Shi, Ivy Wang and Xie, Zhiyuan and Xing, Guoliang and Huang, Jianwei",
        "title": "Cosmo: contrastive fusion learning with small data for multimodal human activity recognition"
      },
      {
        "key": "ouyang2023harmony",
        "author": "Ouyang, Xiaomin and Xie, Zhiyuan and Fu, Heming and Cheng, Sitong and Pan, Li and Ling, Neiwen and Xing, Guoliang and Zhou, Jiayu and Huang, Jianwei",
        "title": "Harmony: Heterogeneous Multi-Modal Federated Learning through Disentangled Model Training"
      },
      {
        "key": "xu2023practically",
        "author": "Xu, Huatao and Zhou, Pengfei and Tan, Rui and Li, Mo",
        "title": "Practically Adopting Human Activity Recognition"
      }
    ]
  }
]