%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

%%% What is the problem & why it is important
% Why natural language and QA: provide useful insights about wellness in daily life

% Challenges
% Accurate question answering given in-the-wlld sensor data
% Why accurate answer is needed: people need quantitative answers

%%% Missing from previous works
% sensor side: Long-duration sensor data - long context reasoning; language side: accurate answer to practical and arbitrary questions;

% Missing from previous works

% In this paper..
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Do I need to Motivate natural language interaction - Maybe No!!!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In recent years, the number of Internet of Things (IoT) devices in daily life has grown rapidly, including smartphones, smart home accessories, and smart city infrastructures~\cite{choi2021smart,khang2023smart}. These devices generate a massive amount of multimodal sensor data, with total production expected to reach 80 ZB by 2025~\cite{idc}. 
Traditional research has focused on developing classification models for tasks such as human activity recognition~\cite{ouyang2022cosmo, ouyang2023harmony, xu2023practically, xu2023mesen, cao2024mmclip, weng2024large}.
In contrast, latest studies have recognized the importance of natural language interactions, especially for making sensor data more accessible and useful to elderly people or patients with Alzheimer’s~\cite{xing2021deepsqa,nie2022ai,yang2024drhouse,arakawa2024prism,ji2024mindguard,chen2024sensor2text}.
These systems typically operate in a Question Answering (QA) framework: users ask questions, the sensor system processes both the query and the underlying sensor data, and finally delivers an adequate answer.
The questions can be \textit{qualitative}, requiring high-level reasoning about a user's lifestyle or well-being (e.g., “How is my work-life balance?”), or \textit{quantitative}, demanding quantitatively precise answers based on sensor data (e.g., “How long did I walk?”). 
QA offers a natural and flexible way for users to interact with and gain insights from sensors, going beyond simple classification models.



\begin{figure}[t]
  \begin{subfigure}[b]{0.38\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sensor2text_example.png}
        \vspace{-6mm}
        \caption{Example QA in Sensor2Text~\cite{chen2024sensor2text}.}
        \label{fig:sensor2text_example}
    \end{subfigure} \hspace{0.05\textwidth} % Add horizontal space between subfigures if needed
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/drhouse_example.png}
        \vspace{-6mm}
        \caption{Example QA in DrHouse~\cite{yang2024drhouse}.}
        \label{fig:drhouse_example}
    \end{subfigure}

    \begin{subfigure}[b]{0.9\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sensorchat_example2.png}
        \vspace{-6mm}
        \caption{Example quantitative (left) and qualitative (right) QA in SensorChat (our system).}
        \label{fig:sensorchat_example}
    \end{subfigure}
    \vspace{-4mm}
    \caption{A visual comparison of QA in state-of-the-art sensor systems and \Method. \Method excels in processing long-duration sensor data and generating quantitatively accurate answers and insightful qualitative analyses of well-being.}
    \label{fig:moti_intro}
    \vspace{-6mm}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Motivate our QA scenario: long-duration sensor, accurate answers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
However, existing QA systems for sensors face significant limitations, particularly with short time windows or low-dimensional sensors.
Most current systems can only process data over short periods, such as one minute in DeepSQA~\cite{xing2021deepsqa} or 16 seconds in Sensor2Text~\cite{chen2024sensor2text}, as illustrated in Fig.~\ref{fig:sensor2text_example}. While useful for both qualitative and quantitative activity analysis, such short time frames offer limited value for gaining deeper insights into users’ fitness and overall well-being.
Other QA systems focus on longer-term clinical diagnosis or health-related queries, but leverage low-dimensional physiological data like average heart rate or daily step counts~\cite{englhardt2024classification,kim2024health,yang2024drhouse}, as shown in Fig.~\ref{fig:drhouse_example}. While effective for certain use cases, they cannot handle high-dimensional time series data, such as IMU signals, and fail to provide accurate answers about fine-grained activities.
In practice, reasoning based on fine-grained physical activities, such as daily time spent exercising versus sitting, is critical for assessing a user’s health status~\cite{us2021increase}. A significant gap remains in developing a QA system capable of answering both qualitative and quantitative questions using (1) long-duration and (2) high-dimensional multimodal sensor data.


%These systems converted coarse-grained data, such as average heart rate and daily step counts, into text prompts for Large Language Models (LLMs) processing.
%Multimodal LLMs have emerged as a popular approach, combining short-term sensor data (e.g., a ten-second window) with LLMs using adapters~\cite{moon2023anymal,han2024onellm}.
%\textcolor{red}{Make the contrast with DeepSQA earlier, in the 2nd paragraph, change the argument (we are not first, but existing work has significant limitations)}

%\textbf{Lastly,} these designs demand significant computational resources due to the billions of parameters in LLMs, making them extremely challenging to run on edge systems. In summary, no existing solution offers natural QA interactions based on timeseries data that can operate efficiently at edge.
%a new system design is needed to enable natural interaction with users, query on full history of multimodal data, while running locally on the edge.
%no existing work has addressed natural QA interactions on sensor systems with limited resources.
%on closed-source LLMs which are not available for edge deployments~\cite{englhardt2024classification,kim2024health,yang2024drhouse}, or only deal with short and fixed sensor time window~\cite{moon2023anymal,han2024onellm}. 

%The key challenges are twofold: (i) how to effectively ``combine'' sensor data and natural language questions during reasoning, and (ii) how to compress the model for deployment on edge devices.
%The first challenge is exacerbated by the diverse queries in real life—ranging from work to relaxation and from recent days to months past—necessitating powerful language models for effective handling. 
%Meanwhile, powerful language models such as Large Language Models (LLMs) contain billions of parameters and are impractical for small devices~\cite{xu2024survey}.
%Addressing both of these challenges simultaneously is the most pressing obstacle towards QA using natural language.
%QA has been thoroughly explored in the text and vision domains~\cite{rogers2023qa,schwenk2022okvqa,reichman2023outside}, but less in the field of sensor systems.

\begin{comment}
Traditional sensor-based QA systems like \textit{AI therapist}~\cite{nie2022conversational} and DeepSQA~\cite{xing2021deepsqa} use template-based question generation and classification-based answer prediction, as shown in Fig.~\ref{fig:moti_intro}. 
Consequently, these systems are limited in the variety of questions and answers they can handle, failing to support natural user interactions.
The recent rise of LLMs offers a promising direction for handling arbitrary Q\&As. Multimodal LLMs have integrated sensor data with LLMs through adapters that can be fine-tuned for sensor-specific tasks~\cite{moon2023anymal,han2024onellm}.
However, the limitations of both DeepSQA~\cite{xing2021deepsqa} and multimodal LLMs~\cite{moon2023anymal,han2024onellm} stem from their reliance on short and fixed sensor time windows, such as 10 seconds of IMU signals. These systems are impractical for long-term queries about events occurring over days or weeks.
The latest contributions, such as Englhardt \textit{et al.}~\cite{englhardt2024classification}, Health-LLM~\cite{kim2024health}, and DrHouse~\cite{yang2024drhouse}, have converted physiological sensor data into text and applied pretrained LLMs to answer health-related questions.
Although these systems enable flexible user interactions, they rely on closed-source LLMs or external sophisticated knowledge databases, posing challenges for local deployment on resource-constrained edge devices.
\end{comment}



In this paper, we introduce \Method, the \textit{first} end-to-end system designed for answering both qualitative and quantitative queries in long-term sensor monitoring, using multimodal and high-dimensional time series sensors.
We consider the commonly used mobile devices for sensor data sources, i.e., smartphones and smartwatches.
In contrast to prior QA systems~\cite{xing2021deepsqa,nie2022ai,chen2024sensor2text,arakawa2024prism,yang2024drhouse}, \Method is capable of processing long-duration sensor data spanning weeks or even months. For example, Fig.~\ref{fig:sensorchat_example} shows a user's activities over a three-day period, with the x-axis representing wall-clock time from 00:00 AM to 11:59 PM, and the y-axis denoting separate days. Different activity labels are depicted by color-coded bars.
When asked a quantitative query like, "What day did I spend the longest eating last week?" \Method provides a precise answer based on a detailed search of the sensor database, such as, "You spent the most time eating on Saturday."
Furthermore, leveraging its detailed analysis of daily routines, \Method can also properly address open-ended qualitative queries related to overall health and lifestyle, such as, "How would you rate my lifestyle?"
To the best of the authors' knowledge, \Method is the \textit{first} QA system capable of processing such long-duration sensor data to provide both precise activity-based responses and insightful evaluations of lifestyle trends.

%\textcolor{red}{Should I change the writing flow? Maybe introducing pretraining the sensor-text encoder using the multi-label CLIP loss, then going with the three stage design of SensorChat?}
To achieve the goal, \Method introduces a novel three-stage design including question decomposition, sensor data query and answer assembly.
The question decomposition and answer assembly stages utilize the power of Large Language Models (LLMs) to correctly understand arbitrary questions and provide accurate answers.
\Method further enhances the LLMs in these stages via in-context learning and finetuning techniques to adapt LLMs for various QA scenarios.
The intermediate sensor data query stage is pivotal in encoding raw time series data from multimodal sensors into a pre-stored embedding database. 
During offline pretraining, \Method uses a novel contrastive sensor-text pretraining loss to align sensor and text encoders.
In the online stage, \Method accurately and exhaustively retrieves all relevant sensor embeddings by performing a similarity search against text embeddings generated from question decomposition. These queried embeddings are then processed by a set of carefully designed summarization functions to generate the sensor context in text.
Finally, in the answer assembly stage, the sensor context is integrated with the original question using another LLM, to generate the ultimate answer.
The explicit query mechanism and the fusion of textual sensor information with user questions distinguish \Method from emerging multimodal LLMs, which often rely on the "black-box" integration of natural language and sensory data~\cite{moon-etal-2023-imu2clip,zhang2023llama,liu2024improved,han2024onellm}. 
As a result, \Method excels in answering long-term qualitative and quantitative questions accurately, where state-of-the-art QA systems fall short~\cite{xing2021deepsqa,nie2022ai,chen2024sensor2text,arakawa2024prism,yang2024drhouse}.
%Unlike IMU2CLIP~\cite{moon-etal-2023-imu2clip}, which aligns the embeddings of a single sensor sample with a complete sentence of text, our loss function enables alignment between sensor data and arbitrary partial context. As a result, 

%Technically, \Method carefully designs each stage and the interfaces between them to maximize the strengths of each component.
%We utilize in-context learning and chain-of-thought prompting in the question decomposition stage to encourage accurate reasoning in LLMs. The sensor query stage focuses on training a sensor data encoder and performing efficient searches on the encoded database. \Method further integrates LLMs and sensor query stage with a compact set of well-designed query functions.
%dThe query function interfaces balance the generality and query accuracy.
%Finally, we optimize the efficiency of \Method via LLM quantization and offline sensor encoding. \Method is able to run locally on resource-constrained edge platforms without transmitting the raw sensor data to cloud.
%Specifically, given a question, question decomposition uses zero-shot prompting on LLMs to generate query function names and arguments for subsequent sensor queries.
%The answer assembly stage combines the extracted sensor information with the question, producing answers in the desired form using a finetuned LLM.

We implement \Method in two system variants to accommodate various user needs: \MethodC (``C'' stands for ``Cloud'') for deployment on a cloud server and \MethodE (``E'' stands for ``Edge'') for deployment on an edge device. \MethodC leverages OpenAI’s GPT APIs~\cite{gpt-3.5,gpt-4} for better interpretation of user queries, enabling real-time QA interactions on a server equipped with an NVIDIA A100 GPU~\cite{a100}. On the edge side, \MethodE operates entirely on an edge device, preserving user privacy albeit at the cost of increased latency. We test \MethodE on an NVIDIA Jetson Orin NX module~\cite{jetsonorin}.

We conduct comprehensive evaluations of \Method on a state-of-the-art QA dataset, mainly covering the quantitative queries. The dataset is carefully chosen to include long-term multimodal sensor data and practical QA samples aligned with users' interests.
Additionally, we recruited eight volunteers for a real-world user study, approved by the Institutional Review Board (IRB). Participants were instructed to carry a smartphone while following their daily routines for one to three days. At the end of the sensor data collection period, they were encouraged to interact with \Method, ask any questions they had, and provide their feedback. %This study confirms \Method’s ability to handle quantitative and open-ended questions in real life, such as "How would you rate my lifestyle?", with satisfactory quality.


In summary, the major contributions of our work are:
\begin{itemize}[nolistsep]
    \item We propose \Method, the \textit{first} end-to-end system for answering qualitative and quantitative questions in long-term monitoring using high-dimensional time series sensors. \Method is designed with a novel three stage pipeline, including question decomposition, sensor data query and answer assembly.
    \item We introduce a novel contrastive sensor-text pretraining loss for training the sensor and text encoders offline, enabling accurate sensor embedding queries given arbitrary partial text during the online sensor data query.
    \item We enhance the LLMs in \Method through in-context learning and fine-tuning, incorporating carefully designed prompts to improve the quality of question decomposition and answer assembly.
    \item Comprehensive evaluations show that \Method improves answer accuracy by up to 26\% on quantitative questions, compared to the best-performing state-of-the-art QA systems. A real-world user study confirms \Method’s ability to answer qualitative and open-ended questions to users' satisfaction. In terms of latency, \MethodC enables real-time user interactions from a cloud server, while \MethodE operates fully on local edge devices with higher latency.
\end{itemize}
%real-time responses on cloud and deployable on edge.
%\textcolor{red}{We focus on SensorQA in this section, and validate our system with more open-ended queries in a real-world user study in SEc.xxx}



%As substantiated in Fig.~\ref{fig:moti_intro}, \Method allows the loop of natural QA interactions between sensors and users, handling arbitrary user questions in natural language and a full history of multimodal sensor data. The whole system is implemented and tested on small edge devices such as Raspberry Pi~\cite{rpi4b} and Jetson TX2~\cite{jetsontx2}.
%In spite of resource constraints, \Method is still able to complete the answer with an average latency of \textcolor{red}{xxx}, enabling real-time interactions.
%\Method focuses on monitoring human daily activities due to the generality of this application and the abundance of time series sensor data. We emphasize that \Method can be extended to \textit{any} sensor application that has data available.


%The key technical novelty of \Method lies in combining the strong understanding and reasoning capabilities of LLMs, while precisely extracting relevant sensor information from a sensor database to form the answer.
%We first conduct a motivation study on a state-of-the-art QA dataset for sensors, \Dataset~\cite{}, which collects 5.6K natural question-answer pairs from human labors given daily life sensor data.
%Our findings reveal that the key barrier lies in previous designs' inability to accurately integrate sensor information into answers, leading to poor performance on time- and activity-specific queries.
%Inspired by these findings, we design a novel three-stage pipeline in \Method to ensure both effective integration of sensor data and natural answer generation, as shown in Fig.~\ref{fig:moti_intro}.
%The first stage of question decomposition uses a pretrained LLM for generating appropriate data query requests based on the question, enhanced by in-context learning and chain-of-thought reasoning.
%The second stage of sensor data query takes the decomposed query request, triggers the corresponding functions and returns sensor information in text.
%In the final stage, we finetune an open-sourced LLM for answering the question in appropriate style given the sensor information.
%All three stages work collaboratively to ensure high-quality and accurate answers.
%The complete \Method model is further boosted by question augmentations which enhance the generation quality of both the question decomposition and answer assembly stages.

%We also propose a new design of joint personalization and quantization to deploy \Method on resource-constrained edge devices for specific users. \textcolor{red}{adding more details later}
%to bridge the gap towards real-world deployment, we propose \textcolor{red}{quantization} which enables real-time system deployment on a resource-constrained edge device like the Raspberry Pi.
% Set the big picture, why time series!!
%\Method first processes multimodal data from real-world sensor deployments, and then provides answers to user queries by leveraging both collected data and external knowledge. 
%While our primary focus is on monitoring human daily lives with unobtrusive and easily accessible sensors, 
%We  finetune a Llama model to facilitate task-specific sensor as well as language processing and enhance interaction among users with diverse backgrounds. \textcolor{red}{More details to be added.}



%\noindent \textbf{(1)} We identify the key challenges faced by current sensor systems towards natural QA interactions with users.

%\noindent \textbf{(1)} We design \Method, the \textit{first} end-to-end QA system running on edge that enables natural QA interactions between users and multimodal sensors. %\Method fuses LLMs with accurate sensor data query to generate natural but precise answers.

%\noindent \textbf{(2)} We propose personalization and quantization techniques to deploy \Method on small edge systems and enable real-time user interaction.

%\noindent \textbf{(3)} We perform a comprehensive evaluation of \Method on a state-of-the-art \Dataset dataset as well as through a real-world user study. Results demonstrate that \textcolor{red}{xxxx}

%The rest of the paper is organized as follows: 
%Sec.~\ref{sec:related-work} reviews previous works in relevant fields. Sec.~\ref{sec:motivation} presents the motivating study. Sec.~\ref{sec:model} explains the detailed design of \Method. Afterward, Sec.~\ref{sec:reality} discusses personalization and quantization of \Method for edge deployments. Sec.~\ref{sec:evaluation} and~\ref{sec:deployment} show the comprehensive evaluation of \Method. Finally, Sec.~\ref{sec:discussion} discusses the limitation. The paper concludes in Sec.~\ref{sec:conclusion}.

%\begin{figure*}[tb]
%  \centering
%  \includegraphics[width=0.9\textwidth]{figs/data-compare.png}
%  \vspace{-3mm}
%  \caption{\small Visualizations of existing QA datasets using time series sensor data.}
%  \vspace{-3mm}
%  \label{fig:data-compare}
%\end{figure*}
