%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Motivation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Motivation}
\label{sec:motivation}

In this section, we formally define the problem we address and present a motivating study highlighting the limitations of state-of-the-art QA techniques.



%We formulate the interaction between users and multimodal sensors as a Question Answering problem.
%In one round of interaction, the natural language input and output of the problem are one pair of question and answer in text, denoted as $(q^i, a^i)$.
%$i \in \{1, ..., N\}$ is the index of the QA pair. 
%Another input to the problem is the user's historical sensor data $\mathbf{x}^i \in \mathbb{R}^{d \times T}$, associated with the $i$th QA.
%$\mathbf{x}^i$ is collected by $d$ multimodal sensors over a long time span $T^i$ determined by the user, up to the moment when the question is asked.
%we start with formally stating the QA problem to understand the difference between our problem and existing work. Then 



\subsection{Problem Statement}
\label{sec:problem-statement}
%For a total of $d$ multimodal sensors that generate raw time series, 


We target real-world QA scenarios in which users carry mobile sensing devices during their daily routines, with minimal attention to the sensing process. In this context, sensor data is collected continuously over an extended period in natural, uncontrolled environments. During this process, users may pose arbitrary, personalized questions about the sensor data, ranging from inquiries about a single day to those spanning several weeks, and expect precise answers derived from the collected data.

Formally, the problem is formulated as generating a proper answer given the question and sensory inputs, denoted as $\mathbf{x}$.
The questions can be arbitrary, including qualitative and quantitative queries, while the answers are expected to accurately address users' concerns.
The key distinction between our problem formulation and existing QA approaches lies in the format of $\mathbf{x}$, which consists of long-duration high-dimensional data collected up to the moment the question is asked. As shown in Table~\ref{tbl:related_works}, this makes our problem significantly more challenging.

\iffalse
In the $i$th round of interaction, the natural language input and output of the problem are one pair of question and answer in text, denoted as $(q^i, a^i)$.
$i \in \{1, ..., N\}$ is the index of the QA pair. 
%Another input to the problem is the user's historical sensor data $\mathbf{x}^i \in \mathbb{R}^{d \times T}$, associated with the $i$th QA.
For a specific user, we assume access to the user's historical sensor data $\mathbf{x} \in \mathbb{R}^{d \times T}$, where $d$ denotes the number of different multimodal sensors and $T$ represents the length of the time span. 
Note that $d$ and $T$ can vary across users due to differences in sensor availability and individual schedules. 
Formally, our objective is formed as training an autoregressive model, $\omega$, to predict the next token in the answer sequence: 
\begin{equation}
\max_\omega \sum_{i=1}^{N} \sum_{j=1}^{m} \log p_\omega \left ( a^i_j \: \mathlarger{|} \: \mathbf{x}, q^i, a^i_{1:j-1}\right ),\label{eq:problem}
\end{equation}
where $i$ iterates through all QA pairs, $j$ iterates through all tokens in the answer and $m$ is the maximal length for answers.
\fi

%As highlighted in Table~\ref{tbl:related_works}, our problem is significantly more challenging than prior systems, mainly in terms of the \textbf{duration and granularity} of sensor data $\mathbf{x}$.


%First, existing work such as DeepSQA~\cite{xing2021deepsqa}, AnyMAL~\cite{moon2023anymal} and OneLLM~\cite{han2024onellm} only dealt with a short and fixed sensor time window. 
%This limits the system to answering questions about events that occurred within the window, such as one minute.
%In contrast, \Method is designed to handle much longer durations, up to months or even years. 
%Recent LLM-based approaches~\cite{englhardt2024classification,kim2024health,yang2024drhouse} used processed sensor features for medical applications, such as daily steps counts. However, such processing can overlook activity details, such as the exact minutes the user spent running.
%Instead, \Method enables fine-grained activity sensing and reasoning by directly processing raw time series signals. %, for example, from a 40Hz IMU sensor.
%In summary, \Method allows natural questions about multimodal sensor data, from past months to now, from minute level to high level. This has not been achieved by existing work.

%The input and output of the QA problem are pairs of questions and answers in text, denoted as $(qs^i, ans^i)$. Here we focus on the QA of a single user for simplicity. Suppose we have a total of $N$ QA pairs for this user, thus $i \in {1, ..., N}$.
%We first convert the text question $q^i$ and answer $ans^i$ into a sequence of tokens $q^i = q^i_1, ..., q^i_l$ and $a^i = a^i_1, ..., a^i_m$ respectively. We pad the tokens to a maximal length $l$ and $m$ for questions and answers.
%$(q^i, a^i)$ deonte the tokenized Q\&A pairs.
%Our objective is to finetune an autoregressive model, $\theta$, to predict the next token in the answer sequence, taking into account the question tokens $q^i$, full-history of sensor readings $\mathbf{x}$, and previously decoded tokens $a^i_{1:j-1}$: 

%We use LLaMA~\cite{touvron2023llama} as the base architecture for $\theta$.
%To ease understanding, we summarize the important notations in Table~\ref{tbl:notation}.

%A \textbf{key distinction} of \Method from existing works~\cite{xing2021deepsqa,moon-etal-2023-imu2clip,moon2023anymal} is its use of the full history of raw sensor readings, rather than a fixed, short time window. This approach significantly increases the complexity of the QA problem. As shown in Eq.~\eqref{eq:problem}, when $\mathbf{x}$ contains weeks or days of raw signal data, correctly locating and extracting the relevant chunk becomes very challenging. This challenge is exacerbated by the varying time scales of sensor data across different users, who may ask diverse questions.
%We propose \Method to address these challenges through a novel, carefully designed pipeline powered by LLMs and sensor data queries.

\subsection{Limitations of SOTA Methods}
With a concrete problem setup, we conduct a motivating study to understand the limitations of state-of-the-art systems on this problem.
We then discuss the lessons learned which inspire the design of \Method.

%Understanding these limitations is crucial designing a practical QA system that cater users' needs.
%identify the key challenges in enabling natural language interactions using current systems.
%Understanding the gaps between current systems and the desired performance is crucial for next-generation designs.

\textbf{Dataset Setup}
We choose the \Dataset dataset\footnote{\url{https://github.com/benjamin-reichman/SensorQA}}~\citesensorqa to simulate long-term real-world QA interactions between humans and sensing devices. To our knowledge, \Dataset is the first and only existing QA dataset that matches our target scenario, i.e., in-the-wild, long-term, time series sensor data collection with practical questions that align with users' interests.

Specifically, \Dataset~\citesensorqa used the ExtraSensory dataset~\cite{vaizman2017recognizing,vaizman2018context} as its sensor data source, which contains raw sensor measurements from 60 subjects and has more than 300K minutes of data, annotated with 51 activity or context labels after cleaning. The data were collected using easily accessible smartphone and smartwatch sensors, including IMUs, audio (MFCC features), and phone state sensors (compass, location, Wi-Fi, light, etc.). Unlike other human activity recognition datasets~\cite{misc_human_activity_recognition_using_smartphones_240,misc_mhealth_dataset_319,misc_opportunity_activity_recognition_226,ouyang2023harmony}, ExtraSensory is notable for its natural and uncontrolled collection environment, with no restrictions on device placement.
%In contrast to the vast majority of sensor datasets that are collected in a heavily controlled environment~\cite{chavarriaga2013opportunity,ouyang2023harmony}, ExtraSensory encouraged participants to maintain their natural routines during the intermittent collection period of up to three months.
%Unlike datasets like Ego4D~\cite{grauman2022ego4d}, which rely on data captured by head-mounted cameras, ExtraSensory imposed no restrictions on device placement, allowing devices to be on desks, in pockets, or held in hand.
%Such natural collecting protocol aligns with \Method's target use case. 

The QA data in \Dataset~\citesensorqa were generated by real human workers as if they were interacting with a smart sensor device about their daily lives. %Sensor data is visualized and presented to workers in Gantt charts similar to Fig.~\ref{fig:moti_intro}. To create \Dataset, the authors considered 14 different aspects of life to be queried, covering topics from normal activities to work patterns to work-life balance.
As a result, \Dataset~\citesensorqa comprises diverse and practical questions that provide valuable insights for users. \Dataset covers a wide range of topics from normal activities to work patterns to work-life balance, and question types such as time queries, counting, and activity recognition.
In \Dataset~\citesensorqa, the authors identified six question categories and seven answer categories.
Table~\ref{tab:sensorqa_profile} details the categories along with example Q\&As and the corresponding number of instances. 
While \Dataset exhibits a skewed distribution favoring quantitative and time-related queries, this trend underscores users' strong interest in gaining temporal insights into their lives, highlighting the need for a QA system to address quantitative queries.

\begin{table}[pt]
\begin{subtable}[t]{0.6\textwidth}
\footnotesize
\centering
\begin{tabular}{c p{5.0cm} c} %{X >{\raggedleft\arraybackslash}X}
\toprule
\textbf{Question} & \textbf{Example Questions} & \textbf{\# of} \\
\textbf{Categories} & & \textbf{Questions} \\
\midrule
Time Compare & Did I spend more time sitting or standing? & 1,432 \\
Day Query & On which day did I spend the most time at home? & 1,277 \\
Time Query & How long was I in class and at school? & 1,119 \\
Counting & How often did I groom? & 725 \\
Existence & Did I have a meeting on Wednesday? & 668 \\
Action Query & What did I do after I left home on Tuesday? & 428 \\
\bottomrule
\end{tabular}
\caption{Question categories.}
\label{tab:question_profile}
\end{subtable}
%%%%%%%%%%%%%%%%%%%%%
\begin{subtable}[t]{0.35\textwidth}
\centering
\footnotesize
\begin{tabular}{c p{2.6cm} c} %{X >{\raggedleft\arraybackslash}X}
\toprule
\textbf{Answer} & \textbf{Example Shortened} & \textbf{\# of} \\
\textbf{Categories} & \textbf{Answers} & \textbf{Answers} \\
\midrule
Action & Doing computer work & 1,357 \\
Day/Days & Last Friday & 1,242 \\
Existence & Yes/No & 1,047 \\
Time Length & 40 Minutes & 1,018 \\
Location & At school & 792 \\
Count & Three times & 401\\
Timestamp & Around 11:00 am & 310 \\
\bottomrule
\end{tabular}
\caption{Answer categories.}
\vspace{-5mm}
\label{tab:answer_profile}
\end{subtable}
\caption{Q\&A categories in the \Dataset dataset~\cite{sensorqa}. The short answers are presented for simplicity.}
\label{tab:sensorqa_profile}
\vspace{-7mm}
\end{table}


In total, \Dataset~\citesensorqa contains 5.6K QA pairs reflecting practical human interests in daily life monitoring, including about 3K questions on daily activities and 2.6K on longer durations spanning weeks or months. We randomly split the QA data in \Dataset~\citesensorqa into an 80/20 train-test set. 
%
During the experiments, \textbf{the full sensor data history for each user and one question} are fed to the baselines.
%The specific format of sensor-based information can be different for various baselines, as detailed below.
The generated answer is compared with the ground-truth answer in \Dataset~\citesensorqa. 
%The input sensor data includes raw time series from smartphone and smartwatch IMUs, phone MFCC audio features, and discrete values from phone state sensors. 



\textbf{State-of-the-art (SOTA) Baselines Setup} 
In this motivating study, we select three state-of-the-art methods that have open-sourced code, each representing one mainstream category of approaches:
\begin{itemize}
 \item \textbf{DeepSQA}~\cite{xing2021deepsqa} represents the SOTA CNN-LSTM based model for fusing the question and time series sensor data. We specifically train and test the DeepSQA-CA model which uses Compositional Attention with CNN-LSTM architecture and achieves the highest accuracy as reported in the original paper. We use full-history IMU signals as sensor inputs, since the DeepSQA-CA model was designed to handle IMU time series data.
 \item \textbf{IMU2CLIP}+\textbf{GPT-4}~\cite{moon-etal-2023-imu2clip} is the SOTA GPT-based generative method. It uses IMU2CLIP~\cite{moon-etal-2023-imu2clip} to associate raw IMU signals with text and uses GPT-4 for reasoning and answer generation. For each timestamp, IMU2CLIP searches for a text description that best matches the IMU data. The complete narrative text from all timestamps is then fed into GPT-4~\cite{gpt-4} with the question to generate an answer.
 We train the IMU2CLIP model using the IMU sensor data in \Dataset~\citesensorqa. %The IMU2CLIP model is trained on the ExtraSensory~\cite{vaizman2017recognizing} dataset, which serves as the underlying sensor data for \Dataset. 
 \item \textbf{LLaMA-Adapter}~\cite{zhang2023llama} is the SOTA multimodal LLM model that integrates image or video inputs with the LLaMA backbone using a Transformer-based adapter. We use the activity Gantt charts as image inputs, representing the sensor information. We start with their pretrained weights on a LLaMA2-7B~\cite{touvron2023llama} backbone and then finetune it with LoRA~\cite{hu2021lora} on \Dataset~\citesensorqa. 
\end{itemize}
%These methods represent three mainstream approaches to human-sensors interactions. IMU2CLIP+GPT-4 uses a CLIP-like model to encode sensor data, then relies on state-of-the-art closed-source large models for zero-shot reasoning. OneLLM represents multimodal LLM, while DeepSQA highlights recent efforts in training neural networks to fuse sensor data with questions.

%DeepSQA and OneLLM were trained or finetuned on the training set and then evaluated on the test set.



\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/moti_example_1.png}
        \vspace{-6mm}
        \caption{Example of single-day query.}
        \label{fig:daily-time}
    \end{subfigure} %\hspace{0.05\textwidth} % Add horizontal space between subfigures if needed
    
    \begin{subfigure}[b]{0.85\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/moti_example_2.png}
        \vspace{-6mm}
        \caption{Example of multi-day query.}
        \label{fig:weekly-day}
    \end{subfigure}
    \vspace{-4mm}
    \caption{Ground-truth QA pairs in \Dataset~\cite{sensorqa} and SOTA methods' answers.}
    \label{fig:example_qas}
    \vspace{-4mm}
\end{figure}

%\textbf{Quanlitative Results and discussions.}

\textbf{Lesson 1: LLMs are necessary for natural interactions, but not enough}
We first visualize two examples of the question, true answer and the response generated by the three SOTA methods in Fig.~\ref{fig:example_qas} (a) and (b).
Similar as in Fig.~\ref{fig:moti_intro}, we visualize the user's activity in Gantt charts.
%For ease of understanding, we visualize the user's activity in Gantt charts, with the x-axis showing the hours in a day, the y-axis showing the date and the colors showing the occurrence of ground-truth labels. 
In Fig.~\ref{fig:example_qas} (a), we study a single-day query asking the activities after waking up on a specific day of Wednesday.
While the answers are obvious to humans from the chart, this question requires multi-step reasoning: the algorithm first needs to figure out the end time of "waking up", and then queries the activity right after this timestamp.
As a result, traditional methods such as DeepSQA cannot follow these non-trivial reasoning steps needed to process the full history of time series data. Additionally, DeepSQA can only generate short phrases because the CNN-LSTM is trained for classification, learning to select the most suitable answer from a limited set of candidates.
The other two methods based on LLMs are able to produce complete sentences, showing LLM's crucial role in natural interactions with humans.

%Simply fusing sensor signals with LLMs via an adapter can be ineffective in multi-step reasoning.

However, LLMs do not always succeed.
In Fig.~\ref{fig:example_qas} (a), LLaMA-Adapter ends up with an irrelevant answer of "went to school", likely a random guess rather than a result of multimodal fusion. Multimodal LLMs like LLaMA-Adapter may underperform without extensive high-quality data (e.g., over 1M samples) for finetuning.
In this example, only IMU2CLIP+GPT-4 proves its ability to generate the correct answer of "showering" and "in the toilet". This can be attributed to IMU2CLIP's success in associating the IMU signals with the correct text, and GPT-4 generating the correct reasoning from the full narrative text.
Nevertheless, IMU2CLIP+GPT-4 has a significant limitation due to GPT's restricted prompt length.
This leads to limited narrative text and a reduced sensor duration that can be processed.
Specifically, IMU2CLIP+GPT-4 struggles with multi-day queries, as shown in Fig.~\ref{fig:example_qas} (b).
The question asks for comparing the frequency of meals on multiple days, and then identifying the day with the fewest meals. All three methods fail in this challenging task. %, which requires multi-step reasoning across a multi-day span.
IMU2CLIP+GPT-4 in particular refuses to give any valid answer.

\textbf{Lesson 2: SOTA methods struggle with accurate quantitative answers over long-duration sensor data}
Although we recognize the critical role of LLMs in user interactions, it is well known that LLMs struggle with quantitative analyzes and numerical reasoning, especially for long-context inputs~\cite{li2024long,gu2023mamba}.
We study the precision of answers generated by SOTA methods on \Dataset~\citesensorqa.
To measure accuracy, we match the key phrases from the ground-truth answers with those in the generated answers.
Specifically, \Dataset offers a shortened version of each answer by distilling 1-2 key words from the full answers using GPT-3.5-Turbo~\cite{gpt-3.5}. A generated answer is considered correct if it includes the ground-truth short answer; otherwise, it is marked as incorrect.
%we profile the question and answer categories in \Dataset and evaluate the answer accuracy for each category.
%We manually label 200 Q\&As, identifying six distinct question categories and seven answer categories. 
%We then trained a BERT model~\cite{devlin2018bert} to classify these categories. The profiling results for \Dataset are presented in Table~\ref{tab:sensorqa_profile}, including example Q\&As and counts.

%\Dataset~\citesensorqa features a diverse range of questions and answers with an imbalanced distribution, posing significant challenges for baseline models in generating accurate responses in all scenarios.


%We further profile the question and answer categories in \Dataset to better understand the results.


\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\textwidth]{figs/motivation_res.png} 
\vspace{-4mm}
\caption{Answer accuracy of state-of-the-art methods and \Method by question category (left) and answer category (right).  Categories are arranged from easier questions/answers on the left to more challenging questions/answers on the right.}
\label{fig:variants}
\end{center}
\vspace{-4mm}
\end{figure}

Fig.~\ref{fig:variants} shows the accuracy results of SOTA methods by question category (left) and answer category (right).
One key observation is that different Q\&A types present varying levels of difficulty for the baselines.
For example, existence and time comparison questions are easier to answer due to the limited number of possible responses (e.g., Yes/No for existence questions).
Counting and time queries are more challenging as they require quantitative analysis.
Additionally, different methods exhibit varying sensitivity to question and answer types.
IMU2CLIP+GPT-4 completely fails in counting questions, as highlighted by the red arrow in Fig.~\ref{fig:variants} (left).
This can be credited to GPT-4's difficulty with counting in long narrative texts.
Similarly, as pointed by the red arrow in the right figure of Fig.~\ref{fig:variants}, all three baselines struggle to generate correct answers for timestamps, underscoring the weaknesses of existing methods in \textbf{quantitative reasoning over long-duration sensor data}.
This weakness stems from the essence of existing designs, where long-duration raw sensor signals are directly fed into large models for fusion with text. Our key insight is that sensor-text fusion requires a more carefully designed approach to handle the diverse queries and long-duration multimodal sensor data encountered in real-life scenarios. To address this, we propose \Method, a novel system with a three-stage pipeline that includes a dedicated sensor data query stage. As shown in Fig.~\ref{fig:variants}, \Method significantly improves answer accuracy across all categories, particularly for quantitative tasks such as day comparisons and time queries.


%The results of all three methods on \Dataset are summarized in Fig.~\ref{fig:variants}.
%\textcolor{red}{will rewrite later. Key takeaways: existing methods struggles with the diverse question types and long time spans, especially in the action and time queries.}
%We evaluate the exact match accuracy (i.e., the answer must be exactly the same as in the dataset) on our short-answer dataset after finetuning a LLaMA2 model~\cite{touvron2023llama} using only the question and answer text. Simpler questions, such as those regarding existence and time comparison, are easier to answer due to the limited number of possible responses (e.g., Yes/No for existence questions).
%The primary challenge of \Dataset lies in its \textbf{\textit{diversity of scenario types and time spans}}, especially in answering concrete time-related queries. Addressing these challenges is a key focus in the design of the \Method system.

%Table~\ref{tab:knowledge_added} summarizes the key statistics of two versions of the dataset for conversation and exact match scenarios respectively.