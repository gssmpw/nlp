\section{System Implementation}
\label{sec:system-implementation}
%We use GPT 3.5 Turbo~\cite{gpt-3.5} and LLaMA3-8B~\cite{touvron2023llama} in the question decomposition stage, while finetuning the open-source LLaMA2-7B and LLaMA3-8B~\cite{touvron2023llama} in the answer assembly stage.
%We focus on the small models because of the resource limitation of edge devices.


\iffalse
\begin{figure}[t]
   \centering
    \setlength{\tabcolsep}{0.2pt}
    \begin{tabular}{ccc}
        \vspace{-2mm}
        \includegraphics[width=0.35\textwidth, height=3.6cm]{figs/system.png} &
        \includegraphics[width=0.32\textwidth, height=3.4cm]{figs/sensorchat_cloud.png} &
        \includegraphics[width=0.27\textwidth, height=3.4cm]{figs/sensorchat_edge.png} \\
\end{tabular}
\vspace{-3mm}
    \caption{Sensitivity of key hyperparameters.}
    \label{fig:system}
    \vspace{-5mm}
\end{figure}
\fi



We implement \Method on real-world systems. We envision \Method as a personal assistant that provides accurate and timely answers to user questions, as outlined in our problem statement (Sec.~\ref{sec:problem-statement}).
Fig.~\ref{fig:system} visualizes the general system pipeline of \Method.
We employ smartphones and smartwatches to collect multimodal sensor data from users in daily lives (\textcircled{1} in Fig.~\ref{fig:system_diagram}).
Implemented based on the ExtraSensory App~\cite{vaizman2018extrasensory}, the mobile devices automatically gathers data for 20 seconds every minute, including 40Hz IMU signals, 13 MFCC audio features from a 22kHz microphone, and other phone state information including compass, GPS location, Wi-Fi status, light intensity, battery level, etc. The details can be found in the ExtraSensory App manual~\cite{vaizman2018extrasensory}.
These sensor data are then transmitted to a system running \Method (\textcircled{2} in Fig.~\ref{fig:system_diagram}). We offer two variants of \Method, designed for a cloud server and an edge environment respectively. Their detailed implementations and trade-offs are discussed below. Finally, users can interact with \Method directly through a chatting interface using natural language, shown as \textcircled{3} in Fig.~\ref{fig:system_diagram}.


\textbf{\MethodC and \MethodE} We offer two system variants of \Method, as shown in Fig.~\ref{fig:sensorchat_cloud} and \ref{fig:sensorchat_edge}.
\begin{itemize}
    \item \textbf{\MethodC}, designed for a cloud environment, uses GPT-3.5-Turbo~\cite{gpt-3.5} for question decomposition and a full-size finetuned LLaMA2-7B model~\cite{touvron2023llama} for answer assembly. We deploy and test \Method on a cloud server equipped with an NVIDIA A100 GPU~\cite{a100}.
    \item \textbf{\MethodE}, designed for an edge environment, uses quantized LLaMA model~\cite{touvron2023llama} for both question decomposition and answer assembly. The question decomposition model is quantized from the official LLaMA3-8B, while the answer assembly model is quantized from our fine-tuned version of LLaMA2-7B. 
    We use Activation-aware Weight Quantization (AWQ), a state-of-the-art quantization method for LLMs, known for its hardware efficiency. We deploy and test \MethodE on a NVIDIA Jetson Orin NX module~\cite{jetsonorin} with 16GB RAM.
\end{itemize}
\MethodC and \MethodE accommodate two typical use scenarios. \MethodC is expected to deliver superior QA performance with the full-precision LLMs in the cloud, at the cost of intensive resource consumption. Additionally, \MethodC requires the users to transmit the full sensor history to the cloud server.
On the other hand, \MethodE runs entirely on a local edge platform belonging to the user, eliminating the need to transmit user data to the cloud and thus preserving user privacy. However, its QA and latency performance degrade compared to \MethodC.  

%a Linux desktop and an NVIDIA Jetson Orin~\cite{}.
%RPi 5 enjoys a 2.4GHz quad-core Cortex-A76 CPU and 8GB RAM. 
%The desktop is equipped with an Intel Core i7-8700 CPU . The Jetson platform features a dual-core NVIDIA Denver 2 CPU, a quad-core ARM Cortex-A57 MPCore, an NVIDIA Pascal GPU with 256 CUDA cores, and 8GB of RAM. %We measure response generation speed on both platforms.

\begin{figure}[t]
  \begin{subfigure}[b]{0.38\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/system.png}
        \vspace{-5mm}
        \caption{Real system implementation.}
        \label{fig:system_diagram}
    \end{subfigure} %\hspace{0.05\textwidth} % Add horizontal space between subfigures if needed
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sensorchat_cloud.png}
        \vspace{-5mm}
        \caption{\MethodC diagram.}
        \label{fig:sensorchat_cloud}
    \end{subfigure}
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/sensorchat_edge.png}
        \vspace{-5mm}
        \caption{\MethodE diagram.}
        \label{fig:sensorchat_edge}
    \end{subfigure}
    \vspace{-4mm}
    \caption{System implementation details of \Method.}
    \label{fig:system}
    \vspace{-4mm}
\end{figure}


\textbf{Implementation Details}
The algorithm part of \Method is implemented with Python and PyTorch~\cite{paszke2019pytorch}.
For the offline sensor encoder, \Method uses a Transformer architecture~\cite{vaswani2017attention} with 6 encoder layers, 8 attention heads, and a feedforward network size of 2048 for time series data. For the low-dimensional phone status data, \Method employs a fully connected layer as the encoder. The fusion layer is also implemented as a fully connected layer. The label encoder is initialized from the pretrained CLIP ViT-B/32 label encoder~\cite{radford2021learning}. Both the sensor and label embedding spaces share a dimension size of 512. Offline pretraining is performed with the partial-context loss proposed in Sec.~\ref{sec:pretraining} using a temperature scalar of $\tau=0.1$. We use the Adam optimizer with a learning rate of $1e-5$ over 100 epochs.
%We test three candidate GPTs for question decomposition: GPT-3.5-Turbo~\cite{gpt-3.5}, GPT-4~\cite{gpt-4}, and quantized LLaMA3-8B~\cite{lin2023awq}, as shown in Fig.~\ref{fig:overview}. 
%In the answer assembly stage, \Method uses the open source LLaMA2-7B and LLaMA3-8B models~\cite{touvron2023llama}. 
%We focus on smaller models so that they can be deployed on edge devices after quantization.

For question decomposition, we design two solution templates for each question category. Limiting the number of templates to two helps balance prompt length and performance.
For online data queries, we initialize the classifier $f$ as a multilayer perceptron with one hidden layer and a ReLU activation function. The hidden layer size is set to 512 and the query threshold is configured to $h = 0.5$.
For finetuning the LLM in answer assembly, we apply low rank adaptation (LoRA) finetuning~\cite{hu2021lora} on an A100 GPU, using a batch size of 8, a learning rate of 0.0002 and 10 epochs.
The LoRA rank $r$ is 16, the LoRA scaling factor \textit{alpha} is 16, and the LoRA dropout is 0.1.
During answer generation, \Method uses a max sequence length of 1024 and a geberating temperature of $0.2$.
%
