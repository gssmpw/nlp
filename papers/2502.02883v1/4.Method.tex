%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Model
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\Method Design}
\label{sec:model}


%The collected QA dataset enables a concrete benchmark for the QA problem in daily-life activities monitoring. 
%Analyzing the dataset reveals unique challenges, such as handling heterogeneous question types and variable time spans. 
%In this section, we provide the details of \Method. 
%\Method is the first end-to-end QA system that enables natural QA interactions with users. 
%We envision \Method as a next-generation mobile system that assists in person's everyday tasks. \Method uses the most intuitive communication of natural language to ``chat'' with users, thus seamlessly embedding into users' lives.
%

%\Method advances SOTA systems from the following perspectives. \textbf{First,} \Method handles a wide range of natural language questions from existence to time queries, and provides clear, understandable answers through LLMs. This addresses the challenges in prior systems~\cite{xing2021deepsqa,nie2022conversational,englhardt2024classification} which restricted users to predefined question or answer sets.
%\textbf{Second,}  \Method can answer questions based on long durations of fine-grained timeseries sensor data, such as counting the total exercising time in a week, by integrating a sensor query stage. This differs from existing systems that use low-dimensional sensor inputs (e.g., daily step counts) or fixed windows~\cite{englhardt2024classification,kim2024health,yang2024drhouse,moon2023anymal,han2024onellm,moon-etal-2023-imu2clip}. %To do so, \Method designs an intermediate stage for accurate and efficient encoding and query of sensor data.
%\textbf{Third}, \Method generates accurate answers to quantitative questions where SOTA systems struggle with~\cite{xing2021deepsqa,zhang2023llama,moon-etal-2023-imu2clip,moon2023anymal}. This is achieved via a carefully designed pipeline that integrates LLMs and the sensor data query stage.
%\textbf{Last but not least,} \Method is optimized for edge systems, delivering real-time responses on the edge desktop. 
%\Method is the first real system that enables natural and real-time interactions with multimodal sensors on edge devices.






%%%%% Overall steps, why do you design in this way
%\vspace{-2mm}
\subsection{Overview of \Method} 

%This is critical for \Method to handle quantitative questions.
%, which process natural language, with a sensor data query stage, which processes raw sensor signals. 
%The integration must leverage the strengths of each module while mitigating their weaknesses. Specifically, LLMs excel at understanding diverse questions but can generate hallucinations that distort quantitative results. On the other hand, the sensor query stage provides accurate quantitative data but may struggle with unexpected queries. Additionally, the complexity of the query search must be carefully managed to ensure real-time responses.
%The challenge of creating a pipeline where these modules operate accurately and robustly in real-world scenarios remains unresolved.
%
To address the challenge of accurate QA over long-duration sensor data, we design a novel three-stage pipeline in \Method, consisting of \textit{question decomposition, sensor data query}, and \textit{answer assembly}.
Specifically, we use LLMs in question decomposition and answer assembly, recognizing their essential roles in correctly interpreting user queries and generating natural language answers.
The intermediate sensor query stage is the \textit{key component} of \Method.
First, the query stage uses a pretrained sensor encoder to effectively encode high-dimensional and long-duration multimodal sensor timeseries into meaningful embeddings. Second, the query stage performs a similarity search in the embedding space to retrieve all sensor information relevant to the original question. This ensures precise extraction of sensor context, making a significant contribution to the accuracy of the final answer.
To the best of the authors' knowledge, \Method is the first system to incorporate an explicit sensor data query stage for accurately handling sensor-based language tasks in long-term monitoring.
%We discuss the potential to further improve \Method with vector databases~\cite{zhou2024llm} or retrival augmented generation~\cite{zhao2024retrieval} in Sec.~\ref{sec:future-work}.
%We recognize the potential to further improve \Method with vector databases~\cite{zhou2024llm} or retrival augmented generation~\cite{zhao2024retrieval}, which we leave for future investigation and discuss 

%Each stage and its interfaces are carefully designed to leverage the strengths of each module while mitigating their weaknesses. 
%The \textbf{key technical challenge} for \Method lies in creating a pipeline where these modules operate \textit{accurately, robustly and efficiently} in real-world scenarios, a problem that remains unsolved before \Method.
%, such as the hallucinations in LLMs.
%The key innovation is to use the LLM as both the front- and back-end for processing questions and answers, with the sensor data query stage in the middle to ensure accurate sensor information extraction. E
%The key technical challenge for \Method lies in \textit{effectively integrating LLM with accurate sensory queries}, so that the whole pipeline works accurately and robustly in real-world scenarios.


The complete pipeline of \Method is illustrated in Fig.~\ref{fig:overview}. For example, given a question like, “\textit{How long did I exercise last week in the morning?}” (\textcircled{1} in Fig.~\ref{fig:overview}), \Method first decomposes the question and generates specific sensor data queries using LLMs. These queries include details such as the context of interest, date and time ranges, and the summarization function (\textcircled{3}). In this case, the query might specify a context of “\textit{do exercise},” a time span of “\textit{last week},” a time of day of “\textit{morning},” and a summarization function of \texttt{CalculateDuration}. To enhance the precision of question decomposition, \Method uses solution templates (\textcircled{2}) with carefully designed prompts. Next, the sensor query stage encodes the activity text “\textit{exercise}” into the embedding space (\textcircled{4}) and retrieves sensor embeddings that are sufficiently similar to the text embedding (\textcircled{5}). These sensor embeddings are encoded offline from the full-history raw sensor data. Both the sensor and text encoders are pretrained offline to align their outputs in the same embedding space, ensuring accurate sensor data retrieval.
Therefore, only text encoding and similarity-based query searches are performed online.
Additional properties, such as the date (“\textit{last week}”) and time of day (“\textit{morning}”), are used to constrain the sensor query range. The final step of the sensor data query stage involves summarizing the relevant sensor embeddings into a textual context with a summarization function (\textcircled{6}). For instance, with the \texttt{CalculateDuration} function, the summarization step calculates the total duration of the retrieved sensor embeddings. This results in a sensor context such as: “\textit{Among all days last week, you exercised for 35 minutes on Monday morning and 55 minutes on Thursday morning.}” The summarization function is identified during question decomposition. Finally, the original question and the sensor context are fed into the answer assembly stage, where a fine-tuned LLM generates the final answer (\textcircled{7}). With the precise sensor context, the model produces an accurate response, such as: “\textit{You exercised a total of 1 hour and 30 minutes last week.}” (\textcircled{8}).

\begin{figure*}[tb]
  \centering
  \includegraphics[width=0.95\textwidth]{figs/overview3.png}
  \vspace{-3mm}
  \caption{The system diagram of \Method including three stages.}
  \vspace{-4mm}
  \label{fig:overview}
\end{figure*}
%\textbf{First,} with the goal of handling heterogeneous questions and answers in the real world, \Method integrates LLMs in the first stage of question decomposition and the third stage of answer assembly. This design leverages the extensive knowledge of pretrained/finetuned LLMs to ensure accurate and natural interactions with users.
%\textbf{Second,} to achieve an exhaustive search across the entire sensor lifespan, \Method develops a sensor data query mechanism to improve answer accuracy.
%The sensor information (in text) and natural language questions are fused in the third stage.
%Without introducing a new modality, \Method avoids the intensive training required by multimodal LLMs, making it a practical solution for real-world applications.

%Inspired by the limitations of prior designs as discussed in Sec.~\ref{sec:analysis}, \Method focuses on handling the diverse interactions in the real world, especially the diverse scenarios and long sensor time spans.
%In \Method, we design a novel framework that fuses LLMs with sensor database query to leverage the strengths of both.
%\Method addresses the limited question and answer types in prior QA systems~\cite{xing2021deepsqa,nie2022ai} as well as the fixed sensor window in existing multimodal LLM works~\cite{moon2023anymal,han2024onellm}.


%leverages the exceptional reasoning capability of pretrained LLMs to handle arbitrary questions, while integrating with a carefully designed a sensor query database to effectively extract relevant multimodal sensor data in the full history.

%In this section, we first give an overview of \Method and then provide more details about its key designs.


%In \Method, we propose a novel framework to address two key limitations in previous designs: (i) heterogeneous question and answer types and (ii) variable time scales of the queries.
%These capabilities are crucial for real-world sensor applications but have not been sufficiently addressed in existing studies~\cite{xing2021deepsqa,han2024onellm,moon2023anymal}.


%The framework also needs to integrate with accurate sensor data queries to ensure correct answer generation, rather than relying on the LLM's potentially inaccurate predictions.
%\Method is the first design to effectively fuse sensor data with natural language. Although recent works in multimodal LLM~\cite{han2024onellm,moon2023anymal} provide a path for integrating data from other modalities into LLMs, they require vast amounts of multimodal data (e.g., over 1M samples) for finetuning. 
%\Dataset is insufficient to obtain satisfactory finetuning results, as shown in the previous section.

 
%Unfortunately, \Dataset is the only dataset currently available for our natural QA task with sensors, making multimodal LLM infeasible in our scenario.


%To address the limitations of (1) heterogeneous question and answer types and (2) fixed sensor scales, our approach leverages the extensive knowledge of pretrained LLMs to handle diverse questions while designing a sensor data query mechanism to adaptively search the entire database. 



%We use \Dataset to finetune the LLM in the last stage thus the model adapts to our sensor-specific task and is able to generate answers in the desired format.

%In \Method, all three stages work together to produce accurate, high-quality answers. The sensor query stage in the middle stage effectively integrates sensor data with natural language: its query search is guided by the first stage's decomposition results, while its extracted information (in text) is fused with the original question in the last stage, both powered by pretrained LLMs. This approach avoids the data-intensive training required by multimodal LLMs, making \Method a practical solution for real-world applications. 


In the following lines, we describe the key design elements of \Method: the offline contrastive sensor-text encoder pretraining (Sec.~\ref{sec:pretraining}) and the online three-stage pipeline (Sec.~\ref{sec:three-stage}).

%that enable accurate answers from long-duration raw sensor timeseries.
%We first detail the offline contrastive sensor-text pretraining in \Method (Sec.~\ref{sec:pretraining}), which is critical for training well-aligned sensor and text encoders and ensuring precise sensor data queries. To achieve this, we introduce a novel contrastive pretraining loss tailored for partial contexts. 
%Next, we detail the online three-stage pipeline of \Method in Sec.~\ref{sec:three-stage}, including how LLMs are utilized for question decomposition and answer assembly, and how the sensor data query stage enables effective fusion of sensor and text information through similarity-based searches.




\subsection{Contrastive Sensor-Text Pretraining for Partial Contexts}
\label{sec:pretraining}

The sensor and text encoders are crucial for effective sensor-text fusion, serving as a "bridge" between high-dimensional sensor timeseries and semantic text. A high-quality sensor encoder ensures that \Method can accurately and comprehensively retrieve relevant sensor embeddings during the sensor data query stage. Similarly, a robust text encoder is essential for handling the arbitrary contextual text generated during the question decomposition stage.

However, achieving this level of alignment poses significant challenges due to the lack of suitable techniques. Existing pretraining methods, such as CLIP and its variants~\cite{radford2021learning,moon-etal-2023-imu2clip}, are effective for pretraining encoders with paired inputs, i.e., one piece of sensor data and one sentence. Unfortunately, encoders trained in this manner struggle to accurately identify similar sensor embeddings when provided with partial or arbitrary context instead of complete sentences.
For instance, using CLIP, a piece of sensor data might align well with the full sentence "The person is sitting and working on computers at school." However, when a user is specifically interested in a partial context like "working on computers," the encoded text embedding may not closely match the original sensor embeddings, leading to reduced accuracy in sensor data query as we show in Sec.~\ref{sec:ablation}. To address this challenge, we introduce a novel contrastive sensor-text pretraining loss for partial contexts.



%tailored for multi-label contexts. 
We pretrain our model on a large-scale multimodal sensor dataset with annotations, where each sample $\{\mathbf{x}_t, w_t\}$ consists of raw time series sensor data $\mathbf{x}_t$ collected at time $t$ and an associated set of partial context labels $w_t$. $w_t$ can be extracted from label annotations. For instance, in a single-label human activity classification dataset,  $w_t$ may contain a single phrase (e.g., $\{\textrm{"standing"}\}$), whereas in a multi-label dataset, it may include multiple phrases (e.g., $\{ \textrm{"at school", "working on computers"}\}$).
We employ separate encoders for sensor and text inputs. 
Formally, let $\theta$ denote the complete sensor encoder model. The encoded sensor embedding is given by $\mathbf{z}^s_t = \theta(\mathbf{x}_t)$.
The text encoder, denoted by $\phi$, maps arbitrary phrases to a text embedding $\mathbf{z}^w_t = \phi(w_t)$.
Key notations used throughout our work are summarized in Table~\ref{tbl:notation}.
%
The details of the sensor encoder are shown in Fig.~\ref{fig:sensor_encoder}. We use distinct sensor encoder for each sensor modality (e.g., IMUs, audio and phone status) to accommodate the varying complexity of different sensor data types. For instance, a Transformer-based encoder is used for high-dimensional time series data while a simple linear layer is designed for encoding phone status. The outputs from these modality-specific encoders are concatenated and passed through a fusion layer to generate the final sensor embedding. Our framework allows for missing modalities by padding with mean values and is flexible for future expansion to additional modalities.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.75\textwidth]{figs/sensor_encoder.png} 
\vspace{-4mm}
\caption{Visualization of the contrastive sensor-text pretraining in \Method.}
\label{fig:sensor_encoder}
\end{center}
\vspace{-6mm}
\end{figure}


%which effectively aligns sensor embeddings with the semantic information in labels~\cite{zhang2023navigating}. 
%Important notations are listed in Table~\ref{tbl:notation}.
%We integrate the timeseries collected from $d$ multimodal sensors into a single sample $\mathbf{x}_t \in \mathbb{R}^{d \times \tau}$, where $t$ denotes the timestamp and $\tau$ denotes the time window length. 
%In the preprocessing stage, we normalize data based on complete readings from the same sensor.
%Missing modalities or readings are padded with zeros.
%The sensor encoder $\theta$ encodes $\mathbf{x}_t$, while the label encoder $\phi$ encodes text labels $w_c$, such as "at school". We then compute the logits by taking the dot product between the sensor and label embeddings:
%$g(\mathbf{x}_t; \theta, \phi) = \sigma \left ( \left [ f(\mathbf{x}_t;\theta) \circ f(w_c;\phi) \right ]_{w_c \in \mathcal{W}} \right )$,
%where $\sigma$ is a sigmoid function to obtain the predicted probabilities.
%
We introduce a new pretraining loss to align sensor embeddings $\mathbf{z}^s_t$ and partial text embeddings $\mathbf{z}^w_t$. Different from CLIP~\cite{radford2021learning}, our loss function enables alignment between sensor data and all partial phrases, defined as follows:
\begin{equation}
 \mathcal{L} = \sum_t {\frac{-1}{|w_t|} \sum_{w \in w_t} \log \frac{\exp(\mathbf{z}^s_t \cdot \mathbf{z}^w_t / \tau)}{\sum_{a \in A(w)} \exp (\mathbf{z}^s_t \cdot \mathbf{z}^a / \tau)}}, \label{eq:loss} 
\end{equation}
$|w_t|$ is the cardinality of set $w_t$, $\tau$ is a scalar temperature parameter. The term $A(w) \equiv A \setminus \{w\}$ represents the collection of negative contexts, defined as all possible phrases except the positive phrase $w$.
The intuition behind our loss function is illustrated in Fig.~\ref{fig:sensor_encoder}. Consider a sensor embedding $\mathbf{x}_t$ and a set of partial text labels $w_t = \{ \textrm{sitting, working on computers}\}$.
Our loss function encourages high similarity between the sensor embedding $\mathbf{z}^s_t$ and all positive text embeddings corresponding to $w_t$, namely "sitting" and "working on computers", while distinguishing them from other negative contexts such as "walking" or "at school". We draw inspiration from the supervised contrast learning loss~\cite{khosla2020supervised}. However, our loss function differs in that it explicitly models the similarity between sensor embeddings and multiple text phrases rather than between samples of the same modality.
%During encoder training, we minimize binary cross-entropy loss using logits $g(\mathbf{x}_t; \theta, \phi)$ and ground truth multi-class labels $y_t$. 
%In the final deployment of \Method, the pretrained encoders are fixed, and new sensor data are encoded into logits.
%The logits is a vector with the same size as $y_t$, indicating the probability of each activity.
After pretraining, we store all sensor embeddings in a database for online queries.
%We further optimize the efficiency in Sec.~\ref{sec:edge}.

\iffalse
\begin{wraptable}{r}{0.44\textwidth}
\footnotesize
\vspace{-5mm}
\caption{List of important notations.}
\label{tbl:notation}
\vspace{-5mm}
\begin{center}
\hspace{-2mm}\begin{tabular}{p{3em} p{22em}} 
 \toprule
 \hspace{-2mm}Symbol & Meaning \\
 \midrule
 %$(q^i, a^i)$ & The $i$th question-answer pair \\
 %$\mathbf{x}^i$ & The full-history sensor time series data w.r.t. $i$th QA pair \\
 $d$ & Number of different sensors \\
 %$T^i$ & Total duration of sensor data $\mathbf{x}^i$ \\
 $\tau$ & Window length of each sensor sample \\
 $\{\mathbf{x}_t, y_t\}$ & The sensor time series data and context labels at time $t$ \\
 %$c^i$ & The extracted sensor context w.r.t. $i$th QA pair \\
 %$T$ & Time window length of the raw timeseries sensor data \\
 $\theta, \phi$ & Pretrained sensor and label encoders \\
 $h$ & Threshold in query search \\
 %$\mathbf{t}$ & Global timestamps of sensor data \\
 %$N$ & Total number of QA pairs in our \Method dataset \\
 %$l, m$ & Maximum length of tokenized questions and answers \\
 %$\theta$ & Autoregressive Large Language Model to finetuning \\
 %$q$ & Number of provided templates during question decomposition \\
 %$k$ & Number of generated variants during dataset augmentation \\
 %$lr$ & Learning rate of during finetuning \\
 %$\tau$ & Temperature for LLM generation \\
 %$\phi$ & Pretrained transformer encoder for encoding raw sensor data \\
 %$b$ & The dimension of sensor embeddings \\
 \bottomrule
\end{tabular}
\end{center}
\vspace{-4mm}
\end{wraptable}
\fi

\begin{table}[t]
\small
\caption{List of important notations.}
\label{tbl:notation}
\vspace{-4mm}
\begin{center}
\begin{tabular}{p{3em} p{21em} | p{3em} p{21em} } 
\toprule
Symbol & Meaning & Symbol & Meaning \\
\midrule
 $\mathbf{x}_t$ & Multimodal sensor data sampled at $t$ &
 $w_t$ & Partial context labels at $t$ \\
 $d$ & Number of different sensors &
 $T$ & Total duration of sensor data \\
 $\theta$ & Sensor encoder &
 $\phi$ & Text encoder \\
 $\mathbf{z}^s_t$ & Sensor embedding of $\mathbf{x}_t$ &
 $\mathbf{z}^w_t$ & Text embedding of $w_t$ \\
 $A(w)$ & Set of negative contexts of $w$ &
 $\tau$ & Temperature scalar \\
 $f$ & Trained similarity function between embeddings & 
 $h$ & Threshold in query search \\
\bottomrule
\end{tabular}
\end{center}
\vspace{-4mm}
\end{table}


\subsection{Three Stages in \Method}
\label{sec:three-stage}

In this section, we explain the detailed designs in each stage of \Method, including question decomposition, sensor data query and answer assembly.
All stages need to work accurately, robustly and collaboratively to achieve natural and precise answer generation in the end.

%\Method uses LLMs in the question decomposition and answer assembly stages to interact freely with users using natural language.
%Our major goal is to utilize the extensive knowledge in LLMs for comprehension and summarizing with minimal hallucinations.
%The first and third stages of \Method use LLMs to to interact freely with users using natural language.
%correctly understand diverse types of questions and construct accurate answers. These stages allow \Method 
%More specifically, as shown in Fig.~\ref{fig:overview}, the question decomposition stage uses GPT~\cite{gpt-4} or LLaMA~\cite{touvron2023llama}, treating the decomposition as a zero-shot reasoning task due to the lack of finetuning data.
%The answer assembly stage uses a custom LLaMA~\cite{touvron2023llama} from fine-tuning on \Dataset. The finetuning process helps \Method produce answers in a desired format. 
%We introduce more details in the following.

%Due to the lack of data to this question decomposition, we formulate the task as zero-shot generation, enhanced by in-context learning and chain-of-thought reasoning to improve solution quality.
%As detailed in the left portion of Fig.~\ref{fig:overview}, we utilize both closed-source LLMs, such as GPT~\cite{gpt-4}, and open-source LLMs, like Llama~\cite{touvron2023llama}. 
\iffalse
\begin{figure}[t]
\begin{center}
\begin{tabular}{c}
    \includegraphics[width=0.95\textwidth]{figs/solution_template.png} \\
    {\small (a) Example solution template for ICL. \vspace{1mm}} \\
    
    \includegraphics[width=0.95\textwidth]{figs/question_decompose.png} \\
    {\small (b) The prompt design for the question decomposition stage includes ICL solution templates (in blue) and bolded text for CoT. \vspace{1mm}}  \\

    \includegraphics[width=0.5\textwidth]{figs/finetuning.png} \\
    {\small (c) Prompt design in the answer assembly stage for LLM finetuning.} \\
\end{tabular}
\vspace{-4mm}
\caption{Prompts design in \Method to better leverage the power of LLMs.}
\label{fig:prompts}
\end{center}
\vspace{-6mm}
\end{figure}
\fi


\subsubsection{Question Decomposition}
\label{sec:decomposition}

%Explicit question decomposition has not been utilized in existing sensor-based QA systems, which either used a limited question set~\cite{xing2021deepsqa,nie2022conversational,englhardt2024classification} or relied on a single LLM for end-to-end problem solving~\cite{englhardt2024classification,kim2024health,yang2024drhouse,zhang2023llama,moon-etal-2023-imu2clip,moon2023anymal}.
%In contrast, \Method integrates explicit question decomposition to guide accurate sensor data queries, thus empowering \Method to handle quantitative questions.

%Question decomposition stage is crucial for accurately interpreting various question types and determining the corresponding sensor query requests. 
The question decomposition stage processes the user question and identifies specific query triggers for the sensor data query stage, such as the context of interest, date and time ranges, and the summarization function, as illustrated in the leftmost box in Fig.~\ref{fig:overview}.
The primary challenge in designing this stage lies in generating \textit{accurate} decompositions for \textit{arbitrary} user inputs, including various questions types as discussed in Sec.~\ref{sec:motivation}.
We choose to rely on LLMs for this stage due to their outstanding capability in handling natural language inputs.
However, LLMs are not without limitations as they can produce erroneous outputs or hallucinations~\cite{huang2023survey}, especially in our case where a dedicated dataset for similar decomposition tasks is unavailable.


%%%Difference from SOTA, what makes this challenging


%However, a significant challenge is the lack of a dedicated dataset for this decomposition. Without such a dataset and finetuning, it is difficult to ensure that LLMs generate desired decompositions with minimal errors due to hallucinations.
%
%
%The \textit{primary challenge} in question decomposition is handling arbitrary user questions without a dedicated dataset. Existing datasets like DeepSQA~\cite{xing2021deepsqa} and SensorQA~\cite{} focus on end-to-end answers, not decomposition. While GPT models excel in general reasoning~\cite{gpt-4}, guiding them to generate precise query arguments in our specific format without pre-existing data is difficult. 
To address this challenge, we propose a \textit{few-shot learning} approach to prompt pre-trained LLMs, enhanced with in-context learning~\cite{alayrac2022flamingo,shao2023prompting} and chain of thought techniques~\cite{chuCoTReasoningSurvey2024,lu2022learn}.
An example prompt is illustrated in Fig.~\ref{fig:question-decompose}, where LLM is instructed to mark different arguments with distinct symbols, such as "<<" and ">>" for function names. This helps in accurately extracting various arguments from the LLM output. 
In \Method, we accommodate a variety of real-life scenarios by extracting contexts (such as activities), dates, times of day, and summarization functions during the query process, as shown in Fig.~\ref{fig:question-decompose}.
\Method supports multiple extracted phrases to handle complex questions such as "How long did I work at school on Monday and Tuesday?"
\Method is designed to be flexible, allowing for future expansion with additional decomposition terms.

We next explain more details on the in-context learning and chain-of-thought techniques used in \Method, both for ensuring high-quality question decompositions.



%We explain more details about each technique next.
%In in-context learning, we adaptively select solution templates and feed them to GPTs along with the question to decompose.
%The CoT design requires LLMs to perform detailed reasoning 
%We also introduce in-context learning and chain-of-thought prompting to improve zero-shot reasoning and avoid too ``creative'' generations.
%In this stage, we introduce in-context learning and chain-of-thought prompting to enhance the effectiveness of zero-shot reasoning, as detailed below.
%Due to the lack of data for this specific task, we opt for prompting GPTs in a zero-shot manner. 




\textbf{In-Context Learning (ICL)} integrates a few examples directly into the prompt during inference, enabling LLMs to adapt effectively to specific tasks without requiring fine-tuning~\cite{alayrac2022flamingo,shao2023prompting}. 
Building on this insight, we design a library of general solution templates covering diverse QA scenarios and incorporate them as in-context examples to improve decomposition accuracy (Fig.~\ref{fig:question-decompose}).
We observe that decomposition solutions for the same question type often share similarities. For instance, "how long" questions typically map to the \texttt{CalculateDuration} function. To utilize this question-specific property, in \Method, we first classify the question type using a BERT model~\cite{devlin2018bert}. Depending on the question type, we dynamically select templates that best match the question category, a strategy shown to enhance ICL performance~\cite{fu2023gpt4aigchip}.
Fig.~\ref{fig:solution-template} illustrates an example of a solution template for a time query. Including an explanation in the solution template is particularly helpful for complex reasoning tasks. %, as detailed with chain of thought.
%For example, a solution to ``\textit{How long did I work yesterday?}'' could be ``\textit{Query the database with the function <<CalculateDuration>> using the activity ((in the main workplace)), and the date [[yesterday]]}''.
%During decomposition, we adaptively select the two templates that solve the most similar questions, which has been shown to improve ICL performance~\cite{fu2023gpt4aigchip}. Using just two templates helps balance prompt length and performance.
%To ensure accurate decomposition across various question types, we create distinct templates tailored to each category.
%For example, the template in Fig.\ref{fig} (b) is used for time queries, while another template addresses action queries like ``What did I do on Tuesday?''
%This adaptive template selection offers specialized guidance based on the question category, thereby enhancing decomposition accuracy. 
%In \Method, we provide 2-3 templates for each question category listed in Table~\ref{tab:question_profile}.



%\textbf{In-context learning}
%includes task examples directly into the prompt during inference, which has shown superb performances in adapting LLM outputs to specific tasks without finetuning~\cite{alayrac2022flamingo,shao2023prompting}. Based on this insight, we design solution templates as in-context examples, which are embedded into prompts as illustrated in Fig.~\ref{fig:prompts} (a).
%An example of such a solution template is shown in Fig.\ref{fig:prompts} (b).
%For each question, we provide a corresponding solution and a detailed explanation. To ensure accurate decomposition across various question types, we create distinct templates tailored to different question categories.
%For instance, the template example in Fig.~\ref{fig:prompts} (b) is used for time queries, while a separate template with ``What did I do on Tuesday?'' is used for action queries. 
%This adaptive template selection offers specialized guidance based on the question category, thereby enhancing decomposition accuracy. In \Method, we provide 2-3 templates for each question category outlined in Table~\ref{tab:question_profile}.

%These templates serve two main purposes: first, they guide LLMs to highlight different properties with various symbols, enabling accurate extraction of query arguments from the generated text. Secondly, 
%We create multiple templates for each question category and use different templates based on the question to be decomposed.
%A concrete template example for the category of time query questions is shown in Fig.~\ref{fig:prompts} (b). We select a typical question within this category and provide a solution as well as an explanation. 

\textbf{Chain-of-Thought (CoT) Prompting} explicitly requests LLMs to generate their reasoning process, which improves accuracy in zero- or few-shot logical reasoning~\cite{chuCoTReasoningSurvey2024,lu2022learn}. In \Method, we include ``please generate step-by-step explanations'' in prompts to encourage CoT, as shown in Fig.~\ref{fig:question-decompose}.
CoT and detailed explanations in solution templates improve the quality of logical reasoning, especially for questions requiring multi-step thoughts.
For example, applying CoT to the question ``What did I do right after waking up on Wednesday?'' helps produce a step-by-step thinking process, such as: ``To find the activity right after waking up on Wednesday, we can query the start and end times of all activities on Wednesday, therefore we can use the function <<DetectFirstTime>> and <<DetectLastTime>> with ((all activities)) on [[Wednesday]]''.

\begin{figure}[t]
    \centering    
    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/question_decompose.png}
        \vspace{-6mm}
        \caption{The prompt design for the question decomposition stage includes ICL and CoT.}
        \label{fig:question-decompose}
    \end{subfigure}

    \begin{subfigure}[b]{0.95\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/solution_template.png}
        \vspace{-6mm}
        \caption{Example solution template for the time query question category.}
        \label{fig:solution-template}
    \end{subfigure} %\hspace{0.05\textwidth} % Add horizontal space between subfigures if needed

    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figs/finetuning.png}
        \vspace{-2mm}
        \caption{Prompt design in the answer assembly stage for LLM finetuning.}
        \label{fig:finetuning}
    \end{subfigure}
    \vspace{-4mm}
    \caption{Prompts design in \Method to better leverage the power of LLMs.}
    \label{fig:prompts}
    \vspace{-4mm}
\end{figure}


\subsubsection{Sensor Data Query}
\label{sec:sensor-data-query}

The sensor query stage is the core module of \Method, responsible for searching the sensor database and extracting relevant information, which directly impacts the accuracy of \Method's answers.
Existing sensor-based QA systems did not employ an explicit search module, restricting to low-dimensional sensor features~\cite{englhardt2024classification,kim2024health,yang2024drhouse} or fixed windows of sensor signals~\cite{xing2021deepsqa,moon-etal-2023-imu2clip,moon2023anymal,chen2024sensor2text,arakawa2024prism}.
In contrast, \Method introduces this explicit query stage to handle long-duration, fine-grained timeseries sensor signals and provide accurate information to queries, which is not possible with prior QA designs.

%In this stage, \Method invokes functions with arguments provided by the question decomposition stage and returns the extracted information as text
%For example, a query calling \texttt{CountingDays} using activity ``\textit{at home}'' and dates ``\textit{last week}'' may return one line of text ``\textit{You spent 4 days at home last week}'', which is then passed to the answer assembly stage.


%%%Difference from SOTA - what is missing from SOTA? what makes this challenging

%Sensor data query is the core component of \Method, which extracts relevant sensor data from raw timeseries in a long time span and composes the factual sensor information in text, , as shown in the middle part of Fig.~\ref{fig:overview}.

The sensor and label encoders are pretrained offline as explained in Sec.~\ref{sec:pretraining}. 
With the pretrained sensor encoder, the full-history raw sensor signals are converted into an embedding database beforehand.
During online user interactions, as shown in the middle part of Fig.~\ref{fig:overview}, the sensor query stage only needs to encode the context (e.g., activity) generated in the question decomposition stage using the text encoder (\textcircled{4} in Fig.~\ref{fig:overview}) , performs a similarity search among the sensor database given the text (\textcircled{5}), and summarizes the retrieved sensor embeddings into textual information for the final answer assembly (\textcircled{6}).
The design of the sensor query stage faces two primary challenges: (1) ensuring accurate and efficient searches within the sensor embedding database, and (2) constructing adequate output context based on users' queries. 
\Method addresses these challenges through an efficient similarity-based embedding search mechanism and a set of carefully designed summarization functions, as detailed below.
%(1) handling diverse queries from practical user questions, and (2) performing accurate and efficient searches on timeseries data.
%The sensor data encoder converts raw timeseries data to lightweight embeddings, enabling accurate and efficient searches within the embedding database.
%We next explain the detailed designs.



%We include the list of important notations in Table~\ref{tbl:notation}.



%While designing many functions can enable finer-grained sensor queries, it may also confuse the LLM during the decomposition stage. To balance these factors, we designed six functions in \Method, covering common question and answer types listed in Table~\ref{tab:sensorqa_profile}.
%\textcolor{red}{Note: we don't need to cover exhaustive functions, as stage 1 and stage 3 will project the scenarios to these questions.}
%%% What are the main challenge of this stage?
%The \textit{key challenge} lies in accurately extracting sensor data from an extensive duration. 
%This involves two major components: first, training a sensor encoder that accurately encodes raw data into embeddings for database storage, ensuring query \textit{precision}. Second, developing a simple yet comprehensive set of query functions to extract the most \textit{relevant} information. These components jointly determine \Method's ability to provide correct answers in the final stage.
%first, training a sensor encoder that accurately encodes raw sensor data into embeddings for database storage. A well-designed encoder ensures the \textit{precision} of sensor queries. Secondly, it is crucial to develop a simple yet comprehensive set of query functions to effectively extract the most \textit{relevant} sensor information from the long duration. Together, these two components determine whether \Method can provide correct answers in the final stage.

%Both sensor data encoder and query functions are crucial in ensuring the relevance and precision of the extracted sensor information, which 



\textbf{Similarity-based Embedding Search} 
%We design efficient query search in \Method based on the lightweight logits.
Our goal is to accurately identify all sensor samples relevant to the query context.
For example, if the query context is ``exercise'', \Method aims to retrieve all sensor samples associated with activities similar to exercise.
If multiple text are generated during question decomposition, \Method performs an embedding search for each query text individually.
The pretraining in Sec.~\ref{sec:pretraining} ensures that the sensor embedding space is well aligned with the text embedding space even using partial context. 
Therefore, the similarity comparison given any pair of sensor and text embeddings can be achieved by training a similarity function $f$ in the embedding space: 
\begin{equation}
f(\mathbf{z}^s_t, \mathbf{z}^w) = f \big(\theta(\mathbf{x}_t), \phi(w) \big) \in [0, 1]
\end{equation}
The output of $f$ is a scalar value between 0 and 1, representing the similarity between the sensor and text samples.
With $f$, the similarity search in the embedding space is significantly more efficient than searching directly through raw, high-dimensional time-series data. The efficiency can be further improved by narrowing the search scope based on the date and time-of-day arguments identified during question decomposition.

\textbf{Summarization Function Design}
We design a set of summarization functions to "summarize" the queried sensor samples and generate contextual information for the answer assembly stage. The specific summarization function to be used is determined by \Method during question decomposition.
For example, a question of "\textit{how long}" should be directed to the \texttt{CalculateDuration} function, while a question of "\textit{what did I do}" should be handled by the \texttt{DetectActivity} function.
Each summarization function uses a unique template to return text information.
The numerical value in the returned text is determined by the embedding search results.
For instance, when querying \texttt{CalculateDuration} with the activity ``\textit{cooking}'', date ``\textit{Sunday}'', and time ``\textit{morning}'', the text output can be: "\textit{You spent {$\gamma$} minutes cooking on Sunday morning}", where $\gamma$ is calculated as follows:
\begin{equation}
\gamma = \sum_{t \in T_{SundayMorning}} \Bigg[ f \big( \theta(\mathbf{x}_t), \phi( \textrm{"cooking"}) \big) > h \Bigg].
\end{equation}
%\begin{equation}
%    \gamma = \sum_{t \in T_{SundayMorning}} [ g(\mathbf{x}_t;\theta, \phi) > h ].
%\end{equation}
Here $T_{SundayMorning}$ represents all timestamps within Sunday morning. 
The notation $[Cond]$ gives $1$, when the inner condition {\it Cond} is met; otherwise $0$.
$h$ is a predetermined threshold.

In \Method, we carefully design a set of summarization functions to account for diverse scenarios in real life, including time quries, activity quries, counting, etc. The details of those functions are explained in Appendix~\ref{sec:query-function}.

\subsubsection{Answer Assembly}
\label{sec:answer-assembly}
As shown in the right box of Fig.~\ref{fig:overview}, the final stage of answer assembly integrates question and sensor information to generate the final answer.
State-of-the-art methods~\cite{xing2021deepsqa,zhang2023llama,moon-etal-2023-imu2clip} rely on ``black-box'' fusion of natural language and sensory data, often leading to ineffective fusion and inaccurate answers (see Sec.~\ref{sec:motivation}).
In contrast, \Method summarizes query results to text and directly fuses them with the question in the prompt, as illustrated in Fig.~\ref{fig:finetuning}.
Our intuition is that, in contrast to processing and fusing with other modalities, LLMs are the most professional in dealing with text.
\Method is capable of answering both qualitative and quantitative questions by combining the original question and the extracted fine-grained activity information from long-duration, high-dimensional sensors.
At this answer assembly stage, we finetune a LLM such as LLaMA~\cite{zhang2023llama} to adapt the model to the desired answer style. Fine-tuning is chosen over few-shot learning as it delivers better performance with the presence of high-quality datasets like \Dataset~\citesensorqa (see Sec.\ref{sec:ablation}). We use Low Rank Adaptation (LoRA)~\cite{hu2021lora} due to its parameter efficiency and comparable performance to full fine-tuning.




%the \texttt{CalculateDuration} function is widely used in time compare, time query and existence questions.
%\texttt{CalculateFrequency} and \texttt{ActivityDetection} are designed for the types of counting and activity query questions, respectively. The \texttt{CalculateDays} function is added to address day counting as a subset of the counting category.
%The \texttt{DetectFirstTime} and \texttt{DetectLastTime} are for counter the needs of concrete timestamp queries.
%Finally, the arguments \texttt{Activity, Date} and \texttt{Time\_of\_Day} are given as additional constraints to limit the search range within the sensor's duration.
%The above four functions are able to cover the majority of question and answer types.
%The sensor database comprises a full history of sensor data with timestamps and a pretrained label classifier, which we explain further in Sec.~\ref{sec:reality}.

%, since days counting differs from regular activity frequency counting.
%is the most commonly used, calculating the duration of a specified activity on a specified date for a certain period. \texttt{CalculateDuration} is widely used in handling time compare, time query and existence questions. Functions 

%We recognize a few potential directions to further improve the sensor query such as using vector databases~\cite{zhou2024llm} or retrival augmented generation~\cite{zhao2024retrieval}, which we leave for future investigation.
%We recognize the potential of enhancing \Method with vector database~\cite{zhou2024llm} or retrieval augmented generation techniques~\cite{zhao2024retrieval}. However, we emphasize that   We discuss the possible extensions to \Method in Sec.~\ref{sec:discussion}.
% 

%\subsection{Optimizing \Method for the Edge}
%\label{sec:edge}
%Deploying \Method on edge devices is critical in preserving user's privacy.
%As shown from Fig.~\ref{fig:overview}, the computational complexity of \Method mainly comes from two parts: (1) the LLMs in question decomposition and answer assembly, and (2) the sensor data encoder and and logits search.
%We use two strategies to optimize them.

%\textbf{Optimizing LLM deployments}
%We optimize the LLMs in \Method using quantization, which has achieved real-time token generation on desktop-level systems in recent works~\cite{lin2023awq,kim2023squeezellm}.
%Thanks to \Method's design of only using text inputs for both question decomposition and answer assembly, the LLMs in \Method can be integrated with any state-of-the-art quantization techniques, such as AWQ~\cite{lin2023awq}.
%In contrast, multimodal LLMs cannot be fully quantized by these techniques due to the additional adapter modules.
%The only thing needed is a calibration dataset, which can be created using our solution templates and finetuning data.
%We construct a calibration dataset from our data 

%\textbf{Optimizing the Encoding of Sensor Data}
%Although inferences on the sensor and label encoders can be inefficient on edge devices in real time, these encoding operations can be performed offline when the QA part is idle.
%We further optimize the encoding stage by performing the encoding inference offline, when the QA part is idle.
%During user interactions, \Method only needs to search the precomputed logits and ensure real-time responses without delay.
%Moreover, the sensor encoder ``compresses'' raw time series data into logits, thus significantly reducing storage requirements as shown in Sec.~\ref{sec:memory}.