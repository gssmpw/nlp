

\section{Additional Numerical Experiments}\label{app: experiments}


We give here an additional experiment made with the same synthetic data described in Section \ref{sec: experiments}. The first experiment shows the performance of the algorithm as a function of the multiplicative error. Instead of fixing $\error_{\min}$, we set a maximum error level $\eta_{\max}$ and sample $\pred$ uniformly at random from the interval $[\price - \eta_{\max}, \price + \eta_{\max}]$. Figure \ref{fig: additive experiment} presents the results in this setting.  

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/worstCase_ratio_vs_uniform_yscale_ADD.pdf}
    \caption{Performance of $\A^\rho_\rob$ with $\rho \in \{0,0.5,1\}$.}
    \label{fig: additive experiment}
\end{figure}

Similarly to the behaviour with respect to the multiplicative error, Figure \ref{fig: additive experiment} shows that $\rho = 0$ yields a significant performance degradation for an arbitrarily small error, which confirms the brittleness of \citet{sun_pareto-optimal_2021}'s algorithm. In contrast, $\rho=1$ achieves the best smoothness, having a performance that gracefully degrades with the prediction error.




\subsection{Experiments on real datasets}

We use the same experimental setting and Bitcoin data as in Section \ref{sec: experiments}, but we set different values of $\lambda \in \{0.2, 0.8\}$ instead of fixing $\lambda=0.5$ as in Figure \ref{fig:BTC-lmb0.5}. This yields different robustness levels, again expressed as $\rob = \ub^{-(1-\lambda/2)}$. The results are shown in Figures \ref{fig:BTC-lmb0.2} and \ref{fig:BTC-lmb0.8} for $\lambda = 0.2$ and $0.8$, respectively.


\begin{figure}[h!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/BTC_lmb01.pdf}
        \caption{Comparison of $\A^1_\rob$ and $\A^0_\rob$ on the Bitcoin price dataset with $\lambda = 0.2$.}
        \label{fig:BTC-lmb0.2}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/BTC_lmb08.pdf}
    \caption{Comparison of $\A^1_\rob$ and $\A^0_\rob$ on the Bitcoin price dataset with $\lambda = 0.8$.}
    \label{fig:BTC-lmb0.8}
    \end{minipage}
\end{figure}

For $\lambda = 0.2$, Figure \ref{fig:BTC-lmb0.2} shows that the performances of $\A^1_\rob$ and $\A^0_\rob$ are similar when $\lambda$ is small, i.e., when $\rob$ is close to $1/\theta$. This corresponds to a consistency of $1$, meaning that the algorithm fully trusts the prediction. Since both algorithms rely heavily on prediction in this setting, their behavior is naturally similar.

For larger $\lambda$, as seen in Figures \ref{fig:BTC-lmb0.5} and \ref{fig:BTC-lmb0.8}, the performance gap between the two algorithms increases. However, for $\lambda$ close to $1$, both consistency and robustness approach $1/\rob \ub$. While the performance of $\A^1_\rob$ degrades significantly more slowly than that of $\A^0_\rob$ for $\lambda = 0.8$ (Figure \ref{fig:BTC-lmb0.8}), the values of $\rob$ and $1/\rob \ub$ remain close. Figure \ref{fig:BTC-lmb0.5}, presented in Section \ref{sec: experiments}, is an intermediate setting between these two extremes, where $\A^1_\rob$ yields a better smoothness than $\A^0_\rob$, without having the values $\rob$ and $1/\rob$ close to each other.