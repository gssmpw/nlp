\section{Preliminaries}
\label{sec:preliminaries}

In the standard setting of the \OMS{} problem, the input consists of a sequence of prices
$\prices=(\prices_i)_{i=1}^n \in [1,\ub]^n$. At each step $i \in [n]$, the algorithm must decide irrevocably  whether to accept $\prices_i$, in which it terminates with a payoff equal to this price, or proceed 
to step $i+1$. If no price has been accepted by step $n$, then then payoff defaults to 1. 

The {\em competitive ratio} of an algorithm is defined as the worst-case ratio, over all sequences, between the algorithm's payoff and the maximum price in the sequence; we denote the latter by $\price$. We say that an algorithm has {\em threshold} $\thresh$ if it accepts the first price in the sequence that is at least $\Phi$. The optimal deterministic competitive ratio is $1/\ub$ and is achieved by an algorithm with threshold $\ub$~\cite{el-yaniv_competitive_1998}.

In the learning-augmented setting, the decision-maker receives a {\em prediction} $\pred$ of the maximum price in the input $\prices$. The payoff of an algorithm $\Alg$, in this setting, is denoted by $\Alg(\prices, \pred)$. We denote by $\con(\Alg)$ and by $\rob(\Alg)$ the consistency and the robustness of the algorithm, respectively, which are defined as
\[
\con(\Alg) = \inf_{\prices} \frac{\Alg(\prices, \price)}{\price} \;,
\;
\rob(\Alg) = \inf_{\prices,y} \frac{\Alg(\prices, y)}{\price} \;.
\]
\citet{sun_online_2024} established the Pareto front of the consistency-robustness tradeoff, albeit using a different convention for the competitive ratio, namely as the worst-case ratio between the maximum price and the algorithm's payoff~\footnote{We use the convention that the competitive ratio of the maximization problem is in $(0,1]$, since it allows for a more clean derivation of the performance as a function of the prediction error.}. Translated to the inverse convention used in this paper, their results show that the Pareto front is  the curve:
\begin{align}\label{eq:pareto_optimal_front}
    \left\{
    (\con, \rob) \in [\ub^{-1/2},1]\times [\ub^{-1},\ub^{-1/2}] \; : \;
    \con \rob \ub=1 \right\}\;,
\end{align}
In~\citet{sun_online_2024}, this front is reached by a family of algorithms parameterized by $\lambda \in [0,1]$. The consistency $\con(\lambda)$ and the robustness $\rob(\lambda)$ of this family satisfy
\[
\con(\lambda) \rob(\lambda) \ub = 1 
\quad, \quad
\frac{1}{\con(\lambda)} = \frac{\lambda}{\rob(\lambda)} + 1 - \lambda\;,
\]
and are achieved by an algorithm with a certain threshold $\thresh(\lambda, \pred)$. An equivalent expression of this threshold, as a function of the parameters $\rob$, $\ub$, and $\pred$ is
\[
\thresh^0_\rob(y) = \left\{
 \begin{array}{lll}
    \rob \ub & \text{if } y \in [1,\rob \ub) \\[5pt]
     \varphi_\rob(\pred) &\text{if } \pred \in [\rob \ub, 1/\rob)\\[8pt]
     1/\rob & \text{if } \pred \in [1/\rob, \ub],
\end{array}
\right. .
\]
where 
\[
\varphi_\rob : z \mapsto \frac{\rob \ub - 1}{1 - \rob} + \frac{1 - \rob^2 \ub}{1 - \rob} \cdot \frac{z}{\rob \ub}
\]
is the line passing through the points $(\rob\ub, \rob \ub)$ and $(\ub, 1/\rob)$.
%
The Pareto-optimal threshold function $\thresh^0_\rob$ of \cite{sun_online_2024}
is represented graphically on \cref{fig:thresh_rho} (with $\rho=0$).

Beyond the consistency-robustness tradeoffs, in this work we are interested in the performance of algorithms as a function of the prediction {\em error},  defined as $\eta(\price, \pred) = |\price - \pred|$. An algorithm is called {\em smooth} if the ratio 
$\Alg(\prices,\pred)/{\price}$ is a continuous, and monotone function of the error $\eta(\price,\pred)$.




\section{Pareto-optimal, and smooth algorithms for {\OMS{}}}\label{sec: deterministic predictions}



Despite its Pareto-optimality in terms of consistency and robustness, $\thresh^0_\rob$ suffers from a significant drawback: \textit{brittleness} \cite{elenter2024overcoming}. Specifically, \citet{benomar2025tradeoffs} showed that even an arbitrarily small positive error in the prediction can cause the ratio between the payoff of \citet{sun_pareto-optimal_2021}'s algorithm and the maximum price to shift abruptly from the consistency bound to the robustness bound. This limitation greatly reduces the algorithm's practicality, as predictions in real-world scenarios are inherently imperfect, because of the stochasticity of both the prices and the prediction.

To address this issue, \citet{benomar2025tradeoffs} proposed a randomized family of algorithms designed to mitigate brittleness and ensure smoothness. Precisely, the performance of their algorithm transitions continuously from the consistency to the robustness bound as a function of the prediction error, $\eta(\price, \pred) = |\price - \pred|$. However, the consistency and robustness levels of their algorithms deviate from the Pareto-optimal curve. Additionally, because these algorithms are randomized, they provide guarantees only in expectation, making them less suitable for risk-averse decision-makers who require guarantees for every realization of the problem.

In what follows, we present a family of deterministic algorithms that achieve Pareto-optimal consistency and robustness while also being smooth. Remarkably, these algorithms overcome the limitations of previous approaches, combining their advantages without compromising on performance.

For any mapping $\thresh: [1,\ub] \to [1,\ub]$, we define the algorithm $\A_\thresh$ for \OMS{}. Given a sequence $(\prices_i)_{i=1}^n$ and a prediction $\pred$ of $\price$, the algorithm selects the first price that exceeds the threshold $\thresh(\pred)$. 
Theorem \ref{thm:thresh-pareto-optimal} below provides a characterization of all thresholds that yield Pareto-optimal levels of consistency and robustness.

\begin{restatable}{theorem}{AllParetoOptimalThresholds}\label{thm:thresh-pareto-optimal}
For any fixed level of robustness $\rob$, the set of all thresholds $\thresh: [1,\ub] \to [1,\ub]$ such that $\A_\thresh$ has robustness $\rob$ and consistency $1/\rob\ub$ is 
\begin{align*}
\SPO := \{ \thresh : [1,\ub]& \to [1,\ub] \text{ satisfying }\\
&\forall z \in [1,\ub]: \rob \ub \leq \thresh(z) \leq \frac{1}{\rob}\\
&\forall z \in [\rob \ub,\ub]: \frac{z}{\rob \ub} \leq \thresh(z) \leq z \}
\end{align*}
\end{restatable}

The constraints defining Pareto-optimal algorithms, as stated in the previous theorem, are illustrated in Figure \ref{fig:thresh_pareto_optimal}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Execution_with_predictions/Submission/figures/thresh_PO.pdf}
    \caption{All the thresholds $\thresh$ yielding a robustness of $\rob$ and a consistency of $1/\rob \ub$.\lc{ A threshold $\thresh$ is $\rob$-robust and $1/\rob\ub$-consistent if and only if its graph lies in the shaded area.}}
    \label{fig:thresh_pareto_optimal}
\end{figure}


\subsection{Smooth and Pareto-optimal algorithms}
We now turn our attention to identifying smooth algorithms within the class proved in Theorem \ref{thm:thresh-pareto-optimal}.

The analysis of \citet{benomar2025tradeoffs} reveals that the brittleness of \citet{sun_pareto-optimal_2021}'s algorithm arises from the discontinuity of the threshold $\thresh^0_\rob$ at the point $1/\rob$. This observation suggests that the smoothness of an algorithm $\A_\thresh$ is influenced by the maximum slope of the function $z \mapsto \thresh(z)$. Guided by this idea, for fixed robustness $\rob$, we study the smoothness properties of the algorithm with threshold $\thresh^1_\rob: z \mapsto \max(\rob \ub, \varphi_\rob(z))$, which minimizes the maximum slope among thresholds in $\SPO$. To further substantiate this intuition, we analyze a family of algorithms $\{\algrho\}_{\rho \in [0,1]}$, that use thresholds interpolating $\thresh^0_\rob$ of \cite{sun_pareto-optimal_2021} and $\thresh^1_\rob$. For all $\rho \in [0,1]$, $\algrho$ selects the first price exceeding the threshold $\thresh^\rho_\rob(\pred)$ defined by
\[
\left\{
 \begin{array}{lll}
    \rob \ub & \text{if } y \in [1,\rob \ub) \\[5pt]
     \varphi_\rob(y) & \text{if } \pred \in [\rob \ub, \tfrac{1}{\rob})\\
     \varphi_\rob(\tfrac{1}{\rob}) + \dfrac{\tfrac{1}{\rob} - \varphi_\rob(\tfrac{1}{\rob})}{\rho (1-\rob)} \cdot \dfrac{\pred}{\rob \ub} & \text{if } \pred \in [\tfrac{1}{\rob}, \tfrac{1}{\rob} + \rho(\ub - \tfrac{1}{\rob}))\\[8pt]
     1/\rob & \text{if } \pred \in [\tfrac{1}{\rob} + \rho(\ub - \tfrac{1}{\rob}), \ub]
\end{array}
\right. .
\]

Figure~\ref{fig:thresh_rho} illustrates the threshold $\thresh^\rho_\rob(y)$. Notably, the case $\rho = 0$ corresponds to the algorithm of \citet{sun_pareto-optimal_2021}, while at the other extreme, $\rho = 1$, we obtain the threshold $\thresh^1_\rob: z \mapsto \max(\rob \ub, \varphi_\rob(z))$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Execution_with_predictions/Submission/figures/thresh_rho.pdf}
    \caption{The threshold functions $\thresh^\rho_r$ of $\algrho$}
    \label{fig:thresh_rho}
\end{figure}

To establish smoothness guarantees, we adopt a multiplicative prediction error measure:  
\begin{align}
    \error:(\price,\pred)\mapsto \min\left\{\frac{\price}{\pred}\,,\,\frac{\pred}{\price}\right\}\in [\ub^{-1},1]\;,
\end{align}
which better captures the nature of the 1-max search problem. A perfect prediction corresponds to $\error(\price, \pred) = 1$, while an arbitrary prediction places $\error(\price, \pred)$ within the interval $[1/\ub, 1]$.
Unlike the additive error $\additiveerror(\price, \pred) = |\price - \pred|$ used in previous works, this measure is scale-invariant and is more suitable for analyzing the competitive ratio.

\begin{restatable}{theorem}{ConsistencyRobustnessSmoothnessComplex}\label{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex}
The family $\{\algrho\}_{\rob, \rho}$ satisfies
\begin{equation}\label{eq: [deterministic Pareto-Optimal smooth Algorithm/ THM consistency-robustness-smoothness complex] PR bound}
\frac{\algrho(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} \error(\price,\pred)^{\smth}
\right)\;,
\end{equation}
with $\smth = \max\left(1, \frac{1}{\rho}\big(\tfrac{\ln \ub}{ \ln (\rob \ub)}-2\big)\right)$.
\end{restatable}
The smoothness described in the previous theorem is quantified by the exponent $\smth$ of $\error$. A smaller value of $\smth$ corresponds to a slower degradation of the bound, hence improved smoothness. In the limit $\rho \to 0$, the exponent $\smth$ becomes arbitrarily large, highlighting extreme sensitivity to prediction errors. In this case, even a slight imperfection in the prediction, despite being arbitrarily close to the true maximum price, results in degradation of the bound from $1/\rob \ub$ to $\rob$. On the other hand, the best smoothness is achieved when $\rho = 1$, yielding 
\[
\smth = \max\left(1, \tfrac{\ln \ub}{ \ln (\rob \ub)} - 2\right)\;.
\]

This naturally raises the question: is the smoothness guarantee in the previous result optimal, or can we design an algorithm that achieves the same levels of consistency and robustness while improving smoothness? To address this, we establish a lower bound on the smoothness achievable by any Pareto-optimal algorithm in terms of robustness and consistency.

\begin{restatable}{theorem}{LowerBoundSmoothness}\label{thm:lower-bound-smoothness}
Let $\A$ be an algorithm with robustness $\rob$ and consistency $1/\rob \ub$. Assume that $\A$ satisfies for all $\prices \in [1,\ub]^n$ and $\pred \in [1,\ub]$ that
\begin{equation}
\frac{\A(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} \error(\price,\pred)^{\smth}
\right)\;
\end{equation}
for some $\smth \in \mathbb{R}$, then necessarily $\smth \geq \tfrac{\ln \ub}{ \ln (\rob \ub)}-2$.
\end{restatable}

This lower bound shows the optimality of the exponent achieved by $\A^1_\rob$ for $\rob \leq \ub^{-2/3}$. Indeed, for a robustness level $\rob \in [\ub^{-1}, \ub^{-2/3}]$, it holds that $\tfrac{\ln \ub}{\ln (\rob \ub)} - 2 \geq 1$. Consequently, Theorem \ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} gives
\[
\frac{\A^1_\rob(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} \error(\price,\pred)^{\tfrac{\ln \ub}{ \ln (\rob \ub)}-2}
\right)\;,
\]
which achieves the best possible exponent for $\error$, as shown in Theorem \ref{thm:lower-bound-smoothness}. 

This implies that if we consider the exponent of $\error$ as the measure of smoothness, then for $\rob \in [\ub^{-1}, \ub^{-2/3}]$, Algorithm $\A^1_\rob$ attains the triple Pareto-optimal front for consistency, robustness, and smoothness among all deterministic algorithms for \OMS{}. For $\rob \in [\ub^{-2/3}, \ub^{-1/2}]$, $\A^1_\rob$ remains smooth and Pareto-optimal for the consistency-robustness tradeoff; however, its smoothness guarantee might admit further improvement.





\subsection{}
Learning-augmented algorithms are typically parameterized by a value $\lambda \in [0,1]$, which reflects the decision-maker's degree of trust in the prediction. A simple yet effective parametrization of $\A^1_\rob$ can be achieved by setting $\rob = \ub^{-(1-\lambda/2)}$. Noting that $1/\rob \ub = \ub^{-\lambda/2}$, and $\frac{\ln \ub}{\ln (\rob \ub)} = \frac{2}{\lambda}$, this parametrization can be used in writing the results from Theorem \ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} as
\[
\frac{\A^1_\rob(\prices, \pred)}{\price} \geq 
\max\left(
\ub^{-(1-\frac{\lambda}{2})}, \ub^{-\frac{\lambda}{2}} \error(\price,\pred)^{\max(1,\frac{2}{\lambda} - 2)}
\right)\;.
\]

An advantage of using the multiplicative error $\error$ is that the obtained bounds can be readily adapted to provide bounds on the inverse ratio $\price/\A^1_\rob(\prices,\pred)$, which is also commonly used as a definition of the competitive ratio in \OMS{} \cite{}\ltodo{missing citation}. By defining the inverse error as $\Bar{\error} = 1/\error = \max\left\{\frac{\price}{\pred}, \frac{\pred}{\price}\right\}$, we obtain
\[
\frac{\price}{\A^1_\rob(\prices, \pred)} \leq 
\min\left(
\ub^{1-\frac{\lambda}{2}}, \ub^{\frac{\lambda}{2}} \Bar{\error}(\price,\pred)^{\max(1,\frac{2}{\lambda} - 2)}
\right)\;.
\]

While the multiplicative error provides a more natural fit for the problem at hand, as discussed earlier, we also derive smoothness guarantees for $\A^1_\rob$ using the additive error $\additiveerror(\price,\pred) = |\price - \pred|$ in Appendix \ref{appendix:additive-error}.

