\section{Introduction}\label{sec:introduction}


% MOTIVATION
\ltodo{imo, a bit generic} The rapid advancement of machine learning (ML) and artificial intelligence (AI) in recent years has led to the development of powerful tools capable of learning complex patterns in data and time series, facilitating high-quality predictions. However, the accuracy of these predictions is strongly influenced by factors such as the choice of the ML model, data quality, stochasticity, and various other elements.\lc{What about the decision theoretic algorithm design question (how does the prediction change behaviour)?} As a result, leveraging these predictions in decision-making processes is nontrivial. This challenge has spurred the emergence of learning-augmented algorithms, a field that has witnessed significant growth since the seminal works~\cite{DBLP:journals/jacm/LykourisV21} and~\cite{NIPS2018_8174}.

Learning-augmented algorithms use machine-learned {\em predictions} about the unknown input, and must guarantee an enhanced performance when predictions are correct (consistency), while maintaining reasonable performance for arbitrarily inaccurate predictions (robustness). Furthermore, their performance must degrade gracefully as the prediction error increases (smoothness).\ltodo{imo this hasn't been sufficiently motivated at this point} This paradigm marks a significant departure from traditional worst-case analysis, which tends to be overly pessimistic.\ltodo{unclear imo} It allows for more nuanced guarantees that go beyond the worst-case scenario, evaluating algorithm performance as a function of the prediction error.

% ONE-MAX SEARCH
This framework has found successful application in the design of online algorithms, data structures, and statistical learning algorithms. Notably, many online selection problems have been studied within this context~\cite{}, including the \OMS{} problem, which serves as a simplified model of financial trading. In this model, a trader observes a sequence of prices $(\prices_i)_{i=1}^n$ revealed online and must decide which price to accept irrevocably. The performance of the algorithm is then assessed as the worst-case ratio between its payoff and the maximal price $\lc{\price:=}\max_i \prices_i$. 

%PROBLEM OF APPLICABILITY

\citet{sun_pareto-optimal_2021} studied \OMS{} assuming that the trader is given additionally a prediction of the maximum price in the sequence.\ltodo{\OMS with a prediction of $\price$.} They
introduced an algorithm achieving Pareto-optimal levels of consistency and robustnessâ€”meaning that no other algorithm can simultaneously outperform it in both aspects. However, a key limitation of their algorithm is its brittleness \lc{its lack of smoothness. In fact this algorithm is \emph{brittle}, as demonstrated in \cite{benomar2025tradeoffs}, meaning its performance may catastrophically fail from arbitrarily small errors in the prediction of $\price$.}\ltodo{we've been talking about smoothness before, brittleness isn't introduced yet.}\cite{}, as demonstrated in \cite{benomar2025tradeoffs}, \ie its performance degrades abruptly from the consistency to the robustness bound for an arbitrarily small prediction error. This significantly limits the applicability of this algorithm in real-world scenarios, as both the quantity to predict and the prediction itself are naturally stochastic, and assuming perfectly accurate predictions is not realistic. Consequently, the only guarantee that holds in practice is the robustness bound, which is no better than the worst-case bound without predictions. \lc{Consequently, this algorithm cannot be expected to perform consistently better than ignoring the prediction.}

To address this issue, \citet{benomar2025tradeoffs} proposed a randomization of the Pareto-optimal algorithm, which introduces smoother behaviour, albeit at the cost of deviating from the consistency-robustness Pareto front. A natural question that arises is whether this deviation from the Pareto-optimal front is necessary to achieve smoothness guarantees.

%Beyond \OMS{}, having smooth algorithms in financial trading problems is essential $\ldots$ (talk a little bit about financial trading to justify the last section?)


\subsection{Contributions}
In both previous works on learning-augmented \OMS{} \cite{sun_pareto-optimal_2021, benomar2025tradeoffs}, the proposed algorithms select the first price that exceeds a threshold $\thresh(\pred)$, which depends on the prediction $\pred$ of the maximum price in the sequence.
We revisit the problem by first characterizing the class $\SPO$ of all thresholds $\thresh: [1,\ub] \to [1,\ub]$ that achieve a Pareto-optimal tradeoff between consistency and robustness.\ltodo{Do we have a decent justification for why thresholds are optimal we could put?}

Next, we focus on a specific family of thresholds within this class, generalizing the algorithm of \cite{sun_pareto-optimal_2021}. Surprisingly, all algorithms in this family not only preserve the Pareto-optimal trade-off between consistency and robustness, but also exhibit smoothness guarantees. These algorithms combine the best properties of the algorithms in \citet{sun_pareto-optimal_2021} and \citet{benomar2025tradeoffs}, without the need for any compromises.
In particular, our analysis shows that the smoothness of these algorithms is inversely proportional to the maximal slope of the corresponding threshold. Guided by this insight, we prove the smoothness induced by a specific threshold $\thresh^1_\rob$, which minimizes the maximal slope among all thresholds in the class $\SPO$.\ltodo{In thm 2.2. we give smoothness for the whole class no?}

Furthermore, we establish a lower bound on the smoothness of any Pareto-optimal algorithm, based on a particular notion of smoothness that we define in this paper. Remarkably, the lower bound matches the performance of the algorithm using the threshold $\thresh^1_\rob$ in the regimes where the robustness $\rob$ is in $[\ub^{-1}, \ub^{-2/3}]$, while there remains a gap between both for $\rob \in [\ub^{-2/3}, \ub^{-1/2}]$. This demonstrates that, in the first regime, our algorithm achieves a triple Pareto-optimal front between consistency, robustness, and smoothness.\ltodo{I think we can massively trim down these last two paragraphs about the smoothness but just saying we characterise the smoothness of this Pareto optimal family of thresholds, and we show that one of them is Pareto-optimal in all 3. I think the contributions are too detailed and it dilutes them.}

Our work is the first to prove a large class of algorithms that achieve Pareto-optimal levels of consistency and robustness in a learning-augmented problem, and also the first to establish a triple Pareto-optimal front between consistency, robustness, and smoothness.

%\zb{Rewrite paragraph(s) below.\\}
%Additionally, to emphasize on the necessity of having a smooth algorithm, we discuss how smoothness allows the obtained bound to be used in stochastic settings. More precisely, it enables the derivation of bounds that account for the coupling between the distributions of the maximum price and the prediction. We demonstrate this with examples of different distributions. Furthermore, as these bounds capture the coupling between the random variables, they can be analyzed in the worst case over the coupling, leading to an optimal transport problem. We propose a formulation for this problem and discuss it, which may provide an interesting avenue for future research.

\lc{The smoothness of our family of thresholds has a direct benefit in handling uncertain predictions. When predictions and prices are both random, we show how to derive general form bounds in expectation as a function of the distributions of predictions and prices and their \emph{coupling}. We provide complexity metrics and detailed bounds on some examples and a general analysis framework based on optimal transport.}

%\lc{Unlike prior work, our algorithmic family and analysis method allows us to use inherently uncertain predictions, for which this is a simple model. Prior work requires ad-hoc modifications (such as using the worst-case prediction from $\dpred$), our algorithm functions as-is and its analysis reveals rich performance estimates directly depending on the parameters of $\dpred$. }
%\ltodo{this par didn't fit in the main text, it should fit better in the intro or ccl, so I moved it here}

Finally, we validate our theoretical results through numerical experiments, comparing our algorithm to those from previous works and testing it in various stochastic settings.



\subsection{Related work}
\paragraph{Learning-augmented algorithms.}
The framework of learning-augmented algorithms, also known as algorithms with predictions, was introduced in \cite{}. The objective is to leverage potentially inaccurate predictions about unknown problem parameters or optimal decisions to surpass worst-case performance guarantees. This paradigm has been applied to a wide range of online algorithms within competitive analysis, including ski rental, scheduling, caching, and matching problems, among many others. Additionally, it has been employed to enhance the runtime of classical algorithms, such as sorting and graph algorithms, as well as in the design of data structures like search trees, dictionaries, and priority queues.\ltodo{I think this paragraph has a lot of redundancy with the intro. It can be skimmed down 25\%. Also, not sure about the order of related work and contributions }


\paragraph{Pareto-optimal tradeoffs.}
A key challenge in the design of learning-augmented algorithms is achieving effective tradeoffs between consistency and robustness. Numerous studies have focused on establishing Pareto-optimal tradeoffs for these competing objectives in various problems \cite{}. However, reaching the Pareto-optimal frontier can sometimes compromise smoothness. For instance, \cite{} demonstrated in the online conversion problem that any Pareto-optimal algorithm is brittle: its performance improves only with perfectly accurate predictions, while even minimal prediction errors can cause the performance to be similar to the worst-case without predictions. One proposed solution to mitigate this brittleness is the use of randomization, which enhances smoothness at the cost of degrading consistency and robustness, as demonstrated in \cite{} and \cite{} for particular problems.


\paragraph{One-Max Search} 
The one-max search problem was initially studied within the context of competitive analysis \cite{}. Assuming that all the prices are within the interval $[1,\ub]$, the optimal competitive ratio\ltodo{Hasn't been introduced before} is $1/\sqrt{\theta}$, and it is achieved by the simple strategy of selecting the first price above the threshold $\sqrt{\theta}$. The problem was revisited in the context of algorithms with predictions, where \cite{} introduced a Pareto-optimal algorithm, while \cite{} proposed a variation that ensures smoothness at the cost of deviating from the Pareto-optimal front of consistency and robustness. The design of randomized algorithms for one-max search has also been studied in the literature, and it is shown that this is equivalent to designing deterministic algorithms for the fractional version of the problem, known as the online conversion problem. In this problem, at each step $i$, the trader may choose to sell a fraction $x_i \in [0,1]$ of the asset at the current price $p_i$, under the constraint that $\sum_{i=1}^n x_i = 1$.\ltodo{Is this necessary beyond the 'fractional formulation of OMS'?} In this setting, \cite{} proved an optimal algorithm achieving a competitive ratio of $O\left(1/\log \theta\right)$, and \cite{sun_pareto-optimal_2021} also derived a Pareto-optimal optimal algorithm when given a prediction of the maximum price. Both one-max search and online conversion are specific cases of broader online selection problems, which are extensively studied due to their strong relations with financial trading and mechanism design scenarios \cite{correa_recent_2019,cartea_optimal_2015}.\ltodo{again quite a lot has already been said in the intro one page prior. I think talking about markets is of little relevance here, it makes more sense to bring it out when talking about uncertain predictions in the intro.}


\paragraph{Distributional predictions.} 
As previously discussed, a smooth algorithm is crucial in stochastic settings where both the prediction and the predicted variable are random variables. Another interesting research direction in these settings involves providing the algorithm with a predicted probability distribution \( G \), intended to approximate the true distribution \( F \)\ltodo{ I don't think these should be introduced here.} of the problem variable. This framework has been explored in the design of data structures \cite{}, where the algorithm receives predictions about the access frequencies of the stored elements. \cite{angelopoulos_contract_2024} also studied the contract scheduling problem within this framework, and more recently, \cite{dinitz_binary_2024} extended the approach to the binary search problem.

\lc{While reliability under uncertain predictions is essential for practical use-cases, smoothness is not the only approach. An alternative line of work, \emph{distributional predictions}, modifies the predictions by considering predictions of the distribution of prices rather than prices themselves. Despite recent applications of this framework to problems such as data structures \citep{}, contract scheduling \citep{angelopoulos_contract_2024}, or binary search \citep{dinitz_binary_2024}, reconciling these two lines of work remains an open question.} 







