\section{Numerical Experiments}\label{sec: experiments}


\begin{figure}[t]
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/worstCase_ratio_vs_uniform_yscale_MUL.pdf}
    \caption{Performance of $\A^\rho_\rob$ with $\rho \in \{0,0.5,1\}$.}
    \label{fig:multiplicative experiment}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/BTC_lmb05.pdf}
    \caption{Comparison of $\A^1_\rob$ and $\A^0_\rob$ on the Bitcoin price dataset.}
    \label{fig:BTC-lmb0.5}
    \end{minipage}
\end{figure}

To complement our theoretical analysis and evaluate the performance of our algorithm in practice, we present experimental results in this section. We defer additional experimental results to Appendix \ref{app: experiments}.
%\ltodo{alt par}

\subsection{Experiments on synthetic data}
We fix $\ub = 5$, $\lambda = 0.5$, and $\rob = \ub^{-(1-\lambda/2)}$. We consider instances $\{\instance(\qmax)\}_{\qmax \in [1,\ub]}$, where $\instance(\qmax)$ is the sequence starting at $1$, and increasing by $\frac{\ub-1}{n-1}$ at each step until reaching $\qmax$, after which the prices drop to $1$. These instances model worst-case instances with maximum price $\qmax$.



We fix an error level $\error_{\min}$ and,
for each $\price \in [1,\ub]$, we generate the prediction $\pred$ by sampling uniformly at random in the interval $[\price \error_{\min}, \price / \error_{\min}] = \{z: \error(\price, z) \geq \error_{\min}\}$, then compute the ratio $\A^\rho_\rob (\instance(\price), \pred)/\price$. For $\error_{\min} \in (0,1]$, Figure \ref{fig:multiplicative experiment} illustrates the worst-case ratio $\inf_{\price \in [1,\ub]} \mathbb{E}_\pred[\A^\rho_\rob(\price,\pred)]/\price$, 
where the expectation is estimated empirically using $500$ independent trials.  

Figure~\ref{fig:multiplicative experiment} shows that for the different values of $\rho$, the worst-case ratio is $1/\rob \ub$ when the prediction is perfect, i.e. $\error_{\min} = 1$, 
%\stodo{Why the notation $\error_{\min}$? Not sure what the min signifies in this section}
 and degrades to $\rob$ when the prediction can be arbitrarily bad, which is consistent with Theorem~\ref{thm:thresh-pareto-optimal}. However, the ratio achieved for $\rho=0$ drops significantly even with a slight perturbation in the prediction, while the ratios with $\rho \in \{0.5, 1\}$ decrease much slower. This is again consistent with the smoothness of $\A^\rho_{\rob}$, as shown in Theorem~\ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex}.









\subsection{Experiments on real data}
To further validate our algorithm’s practicality, we evaluate it on the experimental setting of~\citep{sun_pareto-optimal_2021}. Specifically, we use real Bitcoin data (USD) recorded every minute from the beginning of 2020 to the end of 2024. The dataset’s prices range from $L = 3,858$ USD to $U = 108,946$ USD, yielding $\theta = U/L \approx 28$. 

We randomly sample a 10-week window of prices $W_0$, let $W_{-1}$ be the 10-week window of prices preceding $W_0$ and take the prediction  
$y = \alpha \max W_{-1} + (1-\alpha) \max W_0$. Here, $\alpha$ captures the prediction error: $\alpha = 0$ represents perfect foresight, while $\alpha = 1$ corresponds to a naive prediction equal to the maximum price in $W_{-1}$. To simulate worst-case scenarios, the last price in $W_0$ is changed to $L$ with a probability of 0.75.
For each value of $\alpha$, we sample $m = 100$ windows $(W^j_0)_{j=1}^m$ and compute the 
ratio $R_m = \min_j \{\A^\rho_\rob(W^j_0, \pred)/\max W^j_0\}$ for $\rho \in \{0,1\}$. We then empirically estimate $\Eb[R_m]$ by repeating this process 50 times. We choose the robustness $\rob$ of $\A^\rho_\rob$ by setting $\lambda \in [0,1]$ and $\rob = \ub^{-(1-\lambda/2)}$. 

Figure \ref{fig:BTC-lmb0.5} shows the obtained results with $\lambda = 0.5$, and compares our algorithm $\A^1_r$ to the algorithm of \cite{sun_pareto-optimal_2021}, which corresponds to $\A^0_r$. For $\alpha = 0$, i.e. perfect prediction, they both achieve the consistency $1/r\theta$. However, as the error increases, $\A^0_r$ quickly degrades to the robustness guarantee $r$, whereas $\A^1_r$ degrades more gradually. 








\section{Conclusion}

We provided an intuitive Pareto-optimal and smooth algorithm for a fundamental online decision problem, namely \OMS{}. We believe our methodology can be applied to generalizations such as the {\em $k$-search} problem~\cite{lorenz2009optimal}, \ie multi-unit \OMS{}, recently studied in a learning-augmented setting~\citep{DBLP:conf/eenergy/Lee0HL24}. More broadly, we believe our framework can help bring competitive analysis much closer to the analysis of real financial markets since it combines three essential aspects: worst-case analysis, adaptivity to stochastic settings, and smooth performance relative to the error. A broader research direction is thus to extend the study of competitive financial optimization (see, \eg, Chapter 14 in~\citep{borodin2005online}) to such realistic learning-augmented settings. 
This work also sheds light on connections between competitive analysis and optimal transport, suggesting 
the study of the geometry of OT problems induced by competitive analysis as a promising direction for both theories.



\section*{Acknowledgements}
This research was supported in part by the French National Research Agency (ANR) in the framework of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003) and through the grant DOOM ANR23-CE23-0002. It was also funded by the European Union (ERC, Ocean, 101071601). 
Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them.

This work was also partially funded by the project PREDICTIONS, grant ANR-23-CE48-0010 from the French National Research Agency (ANR).