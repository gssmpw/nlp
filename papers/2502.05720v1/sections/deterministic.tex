\section{Preliminaries}
\label{sec:preliminaries}

In the standard setting of the \OMS{} problem, the input consists of a (unknown in advance) sequence of prices
$\prices:=(\prices_i)_{i=1}^n \in [1,\ub]^n$, where the maximal range $\theta$ is known. At each step $i \in [n]$, the algorithm must decide irrevocably whether to accept $\prices_i$, terminating with a payoff of $\prices_i$, or to forfeit $\prices_i$ and proceed to step $i+1$. If no price has been accepted by step $n$, then the payoff defaults to 1. 

The {\em competitive ratio} of an algorithm is defined as the worst-case ratio (over all sequences $\prices$) between the algorithm's payoff and $\price$, the maximum price in the sequence. 
%We say that an algorithm has a {\em threshold} $\thresh$ if it accepts the first price in the sequence that is at least $\Phi$. 
A natural approach to this problem is to use threshold-based algorithms, which select the first price that exceeds a predetermined threshold $\thresh \in [1,\ub]$. We denote such an algorithm by $\A_\thresh$. In particular,
the optimal deterministic competitive ratio is $1/\sqrt{\ub}$ and it is achieved by $\A_{\sqrt{\ub}}$~\cite{el-yaniv_competitive_1998}. Focusing on this class of algorithms is not restrictive, as in worst-case instances any deterministic algorithm performs equivalently to a threshold algorithm (see Appendix \ref{app:sec2}).
%\VP{VP: Is it the natural approach or that any approach can be reduced to that one? Saying it's natural implies that you chose this for some obscure reasons (usually, you do not know how to handle other approaches)} \zb{ZB: On particular instances, which are used to prove impossibility results, any algorithm can be reduced to a threshold algorithm. But this is not true for any instance...}

In the learning-augmented setting, the decision-maker receives a {\em prediction} $\pred$ of the maximum price in the input $\prices$. The payoff of an algorithm $\Alg$ in this setting is denoted by $\Alg(\prices, \pred)$.
In this context, threshold rules are defined as mappings $\thresh: [1, \ub] \to [1, \ub]$ that depend on the prediction. We use again $\A_\thresh$ to denote the corresponding algorithm.
We denote by $\con(\Alg)$ and by $\rob(\Alg)$ the consistency and the robustness of the algorithm, respectively, defined as
\[
\con(\Alg) = \inf_{\prices} \frac{\Alg(\prices, \price)}{\price} \;,
\;
\rob(\Alg) = \inf_{\prices,y} \frac{\Alg(\prices, y)}{\price} \;.
\]
\citet{sun_pareto-optimal_2021} established the Pareto front of the consistency-robustness trade-off, albeit using a different convention (the inverse ratio $\price/\Alg$) for the competitive ratio\footnote{The convention that the competitive ratio of the maximization problem is in $(0,1]$ allows for cleaner bounds on the performance as a function of the prediction error.}. Under our convention, their results show that the Pareto front of consistency and robustness is  the curve:
\begin{align}\label{eq:pareto_optimal_front}
    \left\{\con \rob \ub=1 \text{ for }
    (\con, \rob) \in [\ub^{-1/2},1]\times [\ub^{-1},\ub^{-1/2}] \;
     \right\}\,.
\end{align}
In contrast, we are interested not only in consistency-robustness Pareto-optimality but also in smoothness, namely in the performance as a function of the prediction {\em error} $\eta(\price, \pred):= |\price - \pred|$. An algorithm is called {\em smooth} if the ratio 
$\Alg(\prices,\pred)/{\price}$ is lower bounded by a (non-constant) continuous and monotone function of the error $\eta(\price,\pred)$.



\section{Pareto-Optimal and Smooth Algorithms}\label{sec: deterministic predictions}


In this section, we present our main result in regards to deterministic learning augmented algorithms, namely a Pareto-optimal and smooth family of algorithms for \OMS{}.
Our approach is outlined as follows. We begin by characterising the class of all thresholds $\SPO$ which induce Pareto-optimal algorithms (Theorem~\ref{thm:thresh-pareto-optimal}). We then present a family of thresholds in $\SPO$, parametrised by a value $\rho\in [0,1]$ ( Eq.~\eqref{eq:rho.family}) that characterises their smoothness
%This parametrisation gives a quantification of the smoothness of these algorithms, as a function of $\rho$ (Theorem~\ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex}). 
and we show that $\rho = 1$ yields the best smoothness guarantees.
%Choosing $\rho=1$ maximises the smoothness. 
We complement this result with Theorem~\ref{thm:lower-bound-smoothness}, which shows that not only is our algorithm smooth, but any Pareto-optimal algorithm cannot improve on its smoothness.
%, in a strong sense.



%In what follows, we present a family of deterministic algorithms that achieve Pareto-optimal consistency and robustness while also being smooth. Remarkably, these algorithms overcome the limitations of previous approaches, combining their advantages without compromising on performance.

%\zb{ZB: I wrote below a quantification of the (negative) gap in consistency robustness from \cite{benomar2025tradeoffs}, but I  don't think this discussion is necessary, given the space constraint.}
%\sa{SA: I agree. It is mostly in case a reviewer criticizes us. We can move the discussion to the appendix. I propose a paragraph below. My understanding is that the gap in the previous work is significant}

Before we discuss our algorithms, we note that the randomized algorithm of~\cite{benomar2025tradeoffs} has a measurable and significant deviation from the Pareto front, even in comparison to deterministic algorithms; see Appendix~\ref{appx:comparison-with-prior-smooth} for the expression of the deviation. Furthermore, the guarantees of their algorithm hold in expectation only, whereas the results we obtain do not rely on randomisation.


We now proceed with the technical statements. 
%For any mapping $\thresh: [1,\ub] \to [1,\ub]$, we define the algorithm $\A_\thresh$ for \OMS{}.  Given a sequence $(\prices_i)_{i=1}^n$ and a prediction $\pred$ of $\price$, the algorithm selects the first price that exceeds the threshold $\thresh(\pred)$. 
Theorem~\ref{thm:thresh-pareto-optimal} below provides a characterization of all thresholds that yield Pareto-optimal levels of consistency and robustness.

%{\color{magenta}VP: what is $\A_\thresh$ below ?}
\begin{restatable}{theorem}{AllParetoOptimalThresholds}\label{thm:thresh-pareto-optimal}
For any fixed of robustness $\rob$, the set of all thresholds $\thresh: [1,\ub] \to [1,\ub]$ such that $\A_\thresh$ has robustness $\rob$ and consistency $1/\rob\ub$ is 
\begin{align*}
\SPO := \{ \thresh  \;:\;
&\forall z \in [1,\ub]: \rob \ub \leq \thresh(z) \leq \frac{1}{\rob}\\
&\forall z \in [\rob \ub,\ub]: \frac{z}{\rob \ub} \leq \thresh(z) \leq z \}.
\end{align*}
\end{restatable}
\Cref{fig:thresh_PO} illustrates the set $\SPO$ (shaded).
%\ltodo{ we can remove $\range\to\range$ here}







We now turn to identifying smooth algorithms within the class $\SPO$. Let us begin by giving the intuition behind our approach. Our starting observation is that the algorithm of~\citet{sun_pareto-optimal_2021} uses the threshold
\[
\thresh^0_\rob(y) := \left\{
 \begin{array}{lll}
    \rob \ub & \text{if } y \in [1,\rob \ub) \\[5pt]
     \varphi_\rob(\pred) &\text{if } \pred \in [\rob \ub, 1/\rob)\\[8pt]
     1/\rob & \text{if } \pred \in [1/\rob, \ub]
\end{array}
\right.\,,
\]
wherein 
\[
\varphi_\rob : z \mapsto \frac{\rob \ub - 1}{1 - \rob} + \frac{1 - \rob^2 \ub}{1 - \rob} \cdot \frac{z}{\rob \ub}
\]
is the line defined by $(\rob\ub, \rob \ub)$ and $(\ub, 1/\rob)$. The function $\thresh^0_\rob$ is illustrated in Figure \ref{fig:thresh_rho}, in dashed orange.

The analysis of \citet{benomar2025tradeoffs} revealed that the brittleness of this algorithm arises from the discontinuity of $\thresh^0_\rob$ at the point $1/\rob$, as illustrated in  \cref{fig:thresh_rho}. This observation suggests that the smoothness of an algorithm $\A_\thresh$ is influenced by the maximal slope of the function $z \mapsto \thresh(z)$. 
%Building on this intuition,\ltodo{I found this step quite jarring. Also we don't need to restrict ourselves to $\rho=1$ this early, we only do it after thm 3.3. 
%SA: Agree, but any discussion of the intuition should lead that $\rho$ has to be as high as 1. If this is not "obvious" but rather follows from analysis, so much the better, but we should rephrase this intuition}
%for fixed robustness $\rob$, we study the smoothness properties of the algorithm with threshold $\thresh^1_\rob: z \mapsto \max(\rob \ub, \varphi_\rob(z))$, which minimizes the maximum slope among thresholds in $\SPO$. 
%To further substantiate this insight,
To confirm this intuition,  we analyse a family of algorithms $\{\algrho\}_{\rho \in [0,1]}$, associated with the thresholds %
%that use thresholds interpolating $\thresh^0_\rob$ of \cite{sun_pareto-optimal_2021} and $\thresh^1_\rob$. For all $\rho \in [0,1]$, $\algrho$
$\{\thresh^\rho_\rob(\pred)\}_\rho$ defined by
\begin{equation}
\hspace{-1em}\left\{
 \begin{array}{lll}
    \hspace{-5pt}\rob \ub & \hspace{-4pt}\mbox{if } y \in [1,\rob \ub) \\[5pt]
     \hspace{-5pt}\varphi_\rob(y) & \hspace{-4pt}\mbox{if } \pred \in [\rob \ub, \tfrac{1}{\rob})\\
     \hspace{-5pt}\varphi_\rob(\tfrac{1}{\rob}) + \dfrac{\tfrac{1}{\rob} - \varphi_\rob(\tfrac{1}{\rob})}{\rho (1-\rob)} \cdot \dfrac{\pred}{\rob \ub} & \hspace{-4pt}\mbox{if } \pred \in [\tfrac{1}{\rob}, \tfrac{1}{\rob} + \rho(\ub - \tfrac{1}{\rob}))\\[8pt]
     \hspace{-5pt}1/\rob & \hspace{-4pt}\mbox{if } \pred \in [\tfrac{1}{\rob} + \rho(\ub - \tfrac{1}{\rob}), \ub]
\end{array}
\right. \hspace{-4pt}.
\label{eq:rho.family}
\end{equation}

Figure~\ref{fig:thresh_rho} illustrates the threshold functions  $(\thresh^\rho_\rob)_{\rho\in[0,1]}$. Notably, the case $\rho = 0$ corresponds to the algorithm of \citet{sun_pareto-optimal_2021}, while at the other extreme ($\rho = 1$) we obtain the threshold $\thresh^1_\rob: z \mapsto \max(\rob \ub, \varphi_\rob(z))$.

\begin{figure}
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/thresh_PO.pdf}
        \caption{
        The set $\SPO$ of Theorem~\ref{thm:thresh-pareto-optimal} is depicted shaded. A threshold $\thresh$ is $\rob$-robust and $1/\rob\ub$-consistent if and only if its graph lies in this shaded area.
        }
        \label{fig:thresh_PO}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/thresh_rho.pdf}
        \caption{
        The threshold functions $\thresh^\rho_r$ of $\algrho$, as defined by Eq.~\eqref{eq:rho.family}. Note that all are equal on $[1,\rob^{-1})$.
        }
        \label{fig:thresh_rho}
        \end{minipage}
\end{figure}

%Also, I would reorder this to argue first about the quality of the measure then introduce it.}
%To establish smoothness guarantees, we adopt a multiplicative prediction error measure, defined as 

The standard definition of smoothness involves demonstrating a continuous degradation of algorithm performance as a function of the prediction error $\eta(\price, \pred) = |\price - \pred|$. A key limitation of $\eta$ is its sensitivity to rescaling---multiplication of the instance
by a constant factor---which makes it less suitable in the context of competitive analysis.  
A common solution is to express bounds in terms of the relative error $\eta(\price, \pred) / \price = |1 - \frac{\pred}{\price}|$, as is often done in prior work on learning-augmented algorithms. However, this introduces another complication: the asymmetry between $\pred$ and $\price$.  

To overcome these issues, we use a multiplicative error measure that retains the desirable properties of scale invariance and symmetry:  
\begin{align}
    \error:(\price,\pred)\mapsto \min\left\{\frac{\price}{\pred}\,,\,\frac{\pred}{\price}\right\}\in [\ub^{-1},1]\;.
\end{align}
%which better captures the nature of the \OMS{} search problem. 
%Unlike the additive error $\additiveerror(\price, \pred) = |\price - \pred|$, this measure is scale-invariant, making it more appropriate for analyzing competitive ratios. 
Note that a perfect prediction corresponds to $\error(\price, \pred) = 1$.
We primarily focus on the multiplicative error $\error$ in the remainder of the paper. However, we also provide smoothness results using the additive error measure $\eta$ for completeness, in Section~\ref{sec:additive-error-smooth}.

%Unlike the additive error $\additiveerror(\price, \pred) = |\price - \pred|$, this measure is scale-invariant and thus more suitable for analysing competitive ratios. Nevertheless, our smoothness guarantees extend to the additive definition of the error, as discussed in \cref{appendix:additive-error}. Using $\error$, the following theorem quantifies the smooth performance of the family of thresholds $(\thresh_\rob^\rho)_{\rho\in[0,1]}$.

\subsection{Multiplicative error \texorpdfstring{$\error$}{}}
This first theorem establishes smoothness guarantees of the family of algorithms $\{\A^\rho_\rob\}_{\rob,\rho}$ with respect to the multiplicative error measure $\error$.

\begin{restatable}{theorem}{ConsistencyRobustnessSmoothnessComplex}\label{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex}
The family $\{\algrho\}_{\rob, \rho}$ satisfies
\begin{equation}\label{eq: [deterministic Pareto-Optimal smooth Algorithm/ THM consistency-robustness-smoothness complex] PR bound}
\frac{\algrho(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} {\error(\price,\pred)}^{\smthr}
\right)\;,
\end{equation}
%\ltodo{Will it be clear to readers that this bound holds pointwise in $p,y$ or should we add a $\forall(p,y)\in[1,\theta]^{n+1}$ to \eqref{eq: [deterministic Pareto-Optimal smooth Algorithm/ THM consistency-robustness-smoothness complex] PR bound}? SA: I think it is fine as is.}
with $\smthr := \max\left(1, \frac{1}{\rho}\big(\tfrac{\ln \ub}{ \ln (\rob \ub)}-2\big)\right)$, for  $\rho\in[0,1]$.
\end{restatable}

%Theorem~\ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} shows that the smoothness of this class of algorithms is \zb{The smoothness is quantified by the exponent because of the error measure. It is not something proper to the class of algorithms we considered, and not really a result of the theorem, as the sentence suggests.}
The smoothness in Theorem~\ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} is quantified by the exponent $\smthr$ of $\error$.
A smaller value of $\smthr$ results in slower degradation of the bound as a function of prediction error,  \ie improved smoothness. In the limit $\rho \to 0$, the exponent $\smthr$ becomes arbitrarily large, which results in extreme sensitivity to prediction errors, \ie brittleness. 
%In this case, even a slight imperfection in the prediction, despite being arbitrarily close to the true maximum price, results in degradation of the bound from $1/\rob \ub$ to $\rob$. 
In contrast, the best smoothness is achieved by $\rho = 1$, yielding an exponent
%\VP{yielding an almost optimal exponent of}
%\stodo{The near-optimality becomes clear only later, in particular the "near" part.}
\[
\smth:=s_1=\max\left(1, \tfrac{\ln \ub}{ \ln (\rob \ub)} - 2\right)\;.
\]
The above positive result naturally raises the question: is the smoothness guarantee of Theorem~\ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} optimal among Pareto-optimal algorithms?
This question is addressed by \cref{thm:lower-bound-smoothness} which gives a lower bound on the smoothness achievable by any Pareto-optimal algorithm. 


\begin{restatable}{theorem}{LowerBoundSmoothness}\label{thm:lower-bound-smoothness}
Let $\A$ be any algorithm with robustness $\rob$ and consistency $1/\rob \ub$. Suppose that $\A$ satisfies for all $\prices \in [1,\ub]^n$ and $\pred \in [1,\ub]$ that
\begin{equation}
\frac{\A(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} \error(\price,\pred)^{\smthu}
\right)\;
\end{equation}
for some $\smthu \in \mathbb{R}$, then necessarily $\smthu \geq \tfrac{\ln \ub}{ \ln (\rob \ub)}-2$.
\end{restatable}
%\ltodo{In this thm and only this theorem, we have for all $\price$ and for all $\pred$. SA: Again I believe it is fine. It is implied in the positive results, and it is required here}


This lower bound shows the optimality of the exponent achieved by $\A^1_\rob$ for $\rob \leq \ub^{-2/3}$. 
%Indeed, for a robustness level $\rob \in [\ub^{-1}, \ub^{-2/3}]$, it holds that $\tfrac{\ln \ub}{\ln (\rob \ub)} - 2 \geq 1$. 
%VP: I do not think those explicit computations are necessary
Indeed, if this condition is satisfied, then Theorem \ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} gives
\[
\frac{\A^1_\rob(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} \error(\price,\pred)^{\tfrac{\ln \ub}{ \ln (\rob \ub)}-2}
\right)\;.
\]
%which is the best possible exponent for $\error$, as shown in Theorem \ref{thm:lower-bound-smoothness}. 

This implies that, for $\rob \in [\ub^{-1}, \ub^{-2/3}]$, Algorithm $\A^1_\rob$ attains the triple Pareto-optimal front for consistency, robustness, and smoothness among all deterministic algorithms for \OMS{}. For $\rob \in [\ub^{-2/3}, \ub^{-1/2}]$, $\A^1_\rob$ remains smooth and Pareto-optimal; however, its smoothness guarantee might admit further improvement.


We conclude this section with some observations. First, note that many learning-augmented algorithms in the literature express consistency and robustness in terms of a parameter $\lambda \in [0,1]$, which reflects the decision-maker's trust in the prediction. 
%For instance, in~\citet{sun_online_2024}, the consistency $\con(\lambda)$ and the robustness $\rob(\lambda)$ of this family satisfy
%\[
%\con(\lambda) \rob(\lambda) \ub = 1 
%\quad, \quad
%\frac{1}{\con(\lambda)} = \frac{\lambda}{\rob(\lambda)} + 1 - \lambda\;.
%\]
A simple yet effective parametrisation of $\A^1_\rob$ can be achieved by setting $\rob = \ub^{-(1-\lambda/2)}$. Noting that $1/\rob \ub = \ub^{-\lambda/2}$, and $\frac{\ln \ub}{\ln (\rob \ub)} = \frac{2}{\lambda}$, the result of Theorem \ref{thm: [deterministic Pareto-Optimal smooth Algorithm] consistency-robustness-smoothness complex} can be restated, with this parametrization, as 
\[
\frac{\A^1_\rob(\prices, \pred)}{\price} \geq 
\max\left(
\ub^{-(1-\frac{\lambda}{2})}, \ub^{-\frac{\lambda}{2}} \error(\price,\pred)^{\max(1,\frac{2}{\lambda} - 2)}
\right)\;.
\]
A second observation is that the obtained bounds can be readily adapted to the inverse ratio $\price/\A^1_\rob(\prices,\pred)$, which is also commonly used to define the competitive ratio in \OMS{}~\cite{el-yaniv_competitive_1998}.
Specifically, by defining the inverse error as $\Bar{\error} = 1/\error = \max\left\{\frac{\price}{\pred}, \frac{\pred}{\price}\right\}$, we obtain 
\[
\frac{\price}{\A^1_\rob(\prices, \pred)} \leq 
\min\left(
\ub^{1-\frac{\lambda}{2}}, \ub^{\frac{\lambda}{2}} \Bar{\error}(\price,\pred)^{\max(1,\frac{2}{\lambda} - 2)}
\right)\;.
\]

\subsection{Extension to the additive error \texorpdfstring{$\eta$}{}}\label{sec:additive-error-smooth}
%Last, we note that 
While the multiplicative error provides a more natural fit for the problem at hand, we also derive smoothness guarantees for $\A^1_\rob$ using the additive error $\additiveerror(\price,\pred) = |\price - \pred|$. Moreover, we prove that the smoothness it achieves is optimal for $\eta$, for all possible values of $\rob \in [\ub^{-1}, \ub^{-1/2}]$.
%while it was only optimal when $\rob \leq \ub^{-2/3}$ for $\error$.


\begin{restatable}{theorem}{LowerBoundSmoothnessAdditive}\label{thm: [deterministic Pareto-Optimal smooth Algorithm] additive smoothness}
Let $\A$ be any algorithm with robustness $\rob$ and consistency $1/\rob \ub$. Suppose that $\A$ satisfies for all $\prices \in [1,\ub]^n$ and $\pred \in [1,\ub]$ that
\begin{equation}\label{eq:A-smooth-additive}
\frac{\A(\prices, \pred)}{\price} \geq 
\max\left(
\rob, \frac{1}{\rob \ub} - \beta \frac{\eta(\price, \pred)}{\price}
\right)\;
\end{equation}
for some $\beta \geq 0$, then necessarily $\beta \geq \cstbeta^*$, where
\[
\cstbeta^* := \frac{1-\rob^2\ub}{\rob \ub} \max\left( \frac{1}{1-\rob}, \frac{1}{\rob \ub - 1} \right).
\]
Moreover, Algorithm $\A^1_\rob$ satisfies  \eqref{eq:A-smooth-additive} with $\beta = \beta^*$, which shows its optimality.
%\VP{can we add again something like "yielding its optimality" ?}
\end{restatable}

The above theorem establishes that  $\A^1_\rob$ has the best possible smoothness guarantee amongst all Pareto-optimal algorithms.
%having a robustness $\rob$ and consistency $1/\rob \ub$. 
Consequently, it achieves a triple Pareto-optimal trade-off between consistency, robustness, and smoothness.

