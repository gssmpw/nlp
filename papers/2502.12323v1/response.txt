\section{Related Work}
\label{litreview}
This paper contributes to a growing literature that has begun to document the problem of non-random measurement error in machine-learning models (see \textbf{Kohavi, "Scaling Up Relief for Unbalanced Data Sets"} __**Freund, "Efficiently Reducing Impact of the Majority Classâ€”Passthrough Sample"}**__ __**Donmez, "Robust Statistical Scales: Measuring and Managing Uncertainty in Predictive Models"**) . A number of recent papers propose econometric estimators that can correct for the non-classical measurement error in some cases. \textbf{Hardt, "Equality of Opportunity in Supervised Learning"} proposes a misclassification model that requires users to specify the variables that may induce measurement error (e.g. cloud cover, satellite angle). \textbf{Kallus, "Balancing Neural Network Weights via Variance Reduced Stochastic Gradient Descent"} suggest a multiple imputation approach that may be sensitive to functional form specifications. \textbf{Zhang, "Correcting for Measurement Error with Machine Learning Models"} address an analagous problem in the NLP realm, developing a method similar to \textbf{Hardt, "Equality of Opportunity in Supervised Learning"} for using gold-standard labeled data to adjust labels provided by an LLM. 
% and  \textbf{Feldman, "Certifying and Removing Disparate Impact"} propose analytical bias correction techniques and MLE estimators to handle attenuation bias when classifier predictions are used as outcomes, but assume non-differential measurement error with respect to regressors. \textbf{Zhou, "Correcting for Measurement Error in Supervised Learning Models"} provide an approach based on hidden Markov models to correct for misclassification bias that does not require ground-truth data but requires a stronger set of structural assumptions about data generating processes. In contrast, bias estimation and correction methods  \textbf{Kallus, "Balancing Neural Network Weights via Variance Reduced Stochastic Gradient Descent"} and our proposed adversarial debiasing method make i.i.d sampling assumptions but do not require assumptions of functional forms or specific sources of measurement error, and are designed to target differential measurement error.
% We show that these methods can succeed in cases where multiple imputation fails. 

% add in something about PPI approach

%
        === INPUT TEXT ENDS ABOVE THIS LINE (DO NOT INCLUDE THIS LINE IN OUTPUT) ===