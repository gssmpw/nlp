\subsection{Error Gap Between Aligned and Misaligned Data}\label{subsec:proof-align-misalign}







\thmalignment*

\begin{proof}

For the aligned case, we can derive the mean squared error (MSE) as follows:
\begin{equation}\label{eq:mse_aligned}
    \mathrm{MSE}_\mathrm{aligned} = \inf_{\boldsymbol{\alpha} \in R^{m^P}, \boldsymbol{\beta} \in R^{m^S}} \|\mathbf{y} - \mathbf{X}^P \boldsymbol{\alpha} - \mathbf{X}^S \boldsymbol{\beta}\|
\end{equation}
The ordinary least squares (OLS) estimator of $\boldsymbol{\alpha}$ is given by:
\begin{equation}
    \hat{\boldsymbol{\alpha}} := (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P (\mathbf{y} - \mathbb{E}[\mathbf{R}] \mathbf{X}^S \boldsymbol{\beta}) 
\end{equation}
For a permutation matrix $\mathbf{R}$ under uniform distribution, we have $\mathbb{E}[\mathbf{R}] = \frac{1}{n}\mathds{1}^\top \mathds{1}$. Therefore:
\begin{equation}\label{eq:alpha_hat}
    \hat{\boldsymbol{\alpha}} = (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P (\mathbf{y} - \frac{1}{n} \mathds{1}^\top \mathds{1} \mathbf{X}^S \boldsymbol{\beta}) 
\end{equation}
The MSE for the misaligned case can be expressed as:
\begin{align}
    \mathrm{MSE}_{\mathrm{misaligned}} 
    & = \inf_{\boldsymbol{\beta}} \inf_{\boldsymbol{\alpha}} \mathbb{E}_\mathbf{R} \|\mathbf{y} - \mathbf{X}^P \boldsymbol{\alpha} - \mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\|_2^2 \\
    & = \inf_{\boldsymbol{\beta}} \mathbb{E}_\mathbf{R} \|\mathbf{y} - \mathbf{X}^P \hat{\boldsymbol{\alpha}} - \mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\|_2^2 \\
\end{align}
Substituting $\hat{\boldsymbol{\alpha}}$ from equation~\ref{eq:alpha_hat}, we obtain:
\begin{align}
    \mathrm{MSE}_{\mathrm{misaligned}} 
    & = \inf_{\boldsymbol{\beta}} \mathbb{E}_\mathbf{R} \left\|\mathbf{y} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} (\mathbf{X}^P \mathbf{y} - \mathbf{X}^P \frac{1}{n} 1^\top 1 \mathbf{X}^S \boldsymbol{\beta}) - \mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 \\
    & = \inf_{\boldsymbol{\beta}} \mathbb{E}_\mathbf{R} \left\| (\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y} + (\mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P \frac{1}{n} \mathds{1}^\top \mathds{1} \mathbf{X}^S \boldsymbol{\beta}) - \mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 
\end{align}
Since $\mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P$ is a projection matrix that projects any vector onto the column space of $\mathbf{X}^P$, and $\mathbf{X}^S \boldsymbol{\beta}$ is orthogonal to the column space of $\mathbf{X}^P$, the term $\mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P \frac{1}{n} \mathds{1}^\top \mathds{1} \mathbf{X}^S \boldsymbol{\beta} = 0$. Thus:
\begin{align}
    \mathrm{MSE}_{\mathrm{misaligned}}
    & = \inf_{\boldsymbol{\beta}} \mathbb{E}_\mathbf{R} \left\| (\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y} - \mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 \\
    & = \inf_{\boldsymbol{\beta}} \mathbb{E}_\mathbf{R} \left[\left\|\mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 - 2\left[(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right]^\top \mathbf{R} \mathbf{X}^S \boldsymbol{\beta} + \left\|(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right\|_2^2\right]
\end{align}
By properties of permutation matrices:
\begin{equation}
    \mathbb{E}_\mathbf{R}\| \mathbf{R} \mathbf{X}^S \boldsymbol{\beta}\|_2^2 = \|\mathbf{X}^S \boldsymbol{\beta}\|_2^2; \; \mathbb{E}_\mathbf{R} [\mathbf{R}]= \frac{1}{n}\mathds{1}^\top \mathds{1}
\end{equation}
Therefore:
\begin{align}
    \mathrm{MSE}_{\mathrm{misaligned}}
    & = \inf_{\boldsymbol{\beta}} \left[\left\|\mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 - 2\left[(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right]^\top \frac{1}{n}\mathds{1}^\top \mathds{1} \mathbf{X}^S \boldsymbol{\beta} + \left\|(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right\|_2^2\right]
\end{align}
Since $\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P$ projects any vector onto the orthogonal complement of the column space of $\mathbf{X}^P$, the term $\left[(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right]^\top \frac{1}{n}\mathds{1}^\top \mathds{1} \mathbf{X}^S \boldsymbol{\beta} = 0$. Hence:
\begin{align}
    \mathrm{MSE}_{\mathrm{misaligned}}
    & = \inf_{\boldsymbol{\beta}} \left[\left\|\mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 + \left\|(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right\|_2^2\right] \\
    & = \inf_{\boldsymbol{\beta}} \left\|\mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 + \left\|(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right\|_2^2 \\
\end{align}
The minimum is attained at $\boldsymbol{\beta} = \mathbf{0}$, yielding:
\begin{align}
    \mathrm{MSE}_{\mathrm{misaligned}}
    & = \left\|(\mathbf{I} - \mathbf{X}^P (\mathbf{X}^{P \top} \mathbf{X}^P)^{-1} \mathbf{X}^P)\mathbf{y}\right\|_2^2 \\
    & = \inf_{\boldsymbol{\alpha} \in \mathbb{R}^{m^P}, \boldsymbol{\beta} = \mathbf{0}} \left\|\mathbf{y} - \mathbf{X}^P \boldsymbol{\alpha} - \mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 \\
\end{align}
Comparing with Equation~\ref{eq:mse_aligned}, we conclude:
\begin{equation}
    \mathrm{MSE}_{\mathrm{misaligned}} \geq \inf_{\boldsymbol{\alpha} \in \mathbb{R}^{m^P}, \boldsymbol{\beta} \in \mathbb{R}^{m^S}} \left\|\mathbf{y} - \mathbf{X}^P \boldsymbol{\alpha} - \mathbf{X}^S \boldsymbol{\beta}\right\|_2^2 = \mathrm{MSE}_{\mathrm{aligned}}
\end{equation}
\end{proof}





















\subsection{Approximation Capacity of Cluster Sampler}\label{subsec:proof-cluster-sampler}

\begin{definition}[Definition of optimal cluster sampler]
    Assume the inputs are uniformly bounded by some constant $B$. 
    The optimal cluster sampler is defined by the uniform equi-continuous cluster sampler function which achieves the minimal optimization loss for the prediction task in \cref{fig:leal-framework}.
    \begin{equation}
        \textrm{Optimal cluster sampler} := \arginf_{\textrm{Uniform equi-continuous cluster sampler}} \textrm{Loss}(\textrm{cluster sampler})
    \end{equation}
    The cluster sampler is defined over bounded inputs ($|X^P|_{\infty} \leq B, |X^S|_{\infty} \leq B$) from $\mathbb{R}^{m^P} \times \mathbb{R}^{n^S \times m^S}$ and output in $\mathbb{R}^{n^S}$.
\end{definition}

\begin{remark}
    The existence of such optimal cluster sampler is guaranteed by the boundedness and uniform equi-continuity of the set of cluster sampler functions. 
\end{remark}


\thmclustersampler*

\begin{proof}
    We just need to prove the statement for small $\epsilon \leq 6$.

    The input of cluster sampler is $1 \times m^P$ and output is $n^S \times m^S$, the final prediction is to generate a sample probabilities:
    \begin{equation}
        (n^S * m^S, 1 * m^P) \to (n^S * d, 1 * C) \to (n^S * C, 1 * C) \to n^S * 1. 
    \end{equation}

    Also, since there is no weight depends on dimension $n_2$, we can reduce the approximation statement to that there exists trainable weight such that the continuous function $h$ can be approximated:
    \begin{equation}
        (1 * m^S, 1 * m^P) \to (n^S * d, 1 * C) \to (n^S * C, 1 * C) \to 1 * 1. 
    \end{equation}

    Notice that the layer operation of secondary embedding and trainable centroids weights $(C \times d)$ is continuous and the pretrained encoder as a neural network (which is a universal approximator) can approximates any continuous function $f$ composited with inverse embedding. 
    For simplicity, we will consider $m^P = m^S = 1$. 
    For any continuous function $h(p, s) \in [0, 1]$,
    we just need to show there exists trainable weight $\theta_1$, $\theta_2$ such that 
    \begin{equation}
        f(p; \theta_1) \odot g(s; \theta_2) = \sum_{i=1}^C f_i(p; \theta_1) \odot g_i(s; \theta_2). 
    \end{equation}
    Here $f(p; \theta_1) \in \mathbb{R}^C$ is a function of $p$ parameterized by $\theta_1$ and $g(s; \theta_1) \in \mathbb{R}^C$ is a function of $s$ parameterized by $\theta_2$.  
    As any continuous function $f(p, s)$ has a corresponding Taylor series expansion, it means for any $\epsilon > 0$, there exists $C$ which depends on error $\epsilon$ such that
    \begin{equation}
        \sup_p \sup_s |h(p, s) -\sum_{i=1}^C pol_{1,i}(p) pol_{2,i}(s)| \leq \frac{\epsilon}{2}. 
    \end{equation}
    Furthermore, as polynomial functions are continuous function, therefore $f_i$ can be used to approximate the polynomial function $pol_{1, i}$ and $g$ can be used to approximate the polynomial function $pol_{2, i}$.
    \begin{align}
        \sup_p |pol_{1,i}(p) - f_i(p; \theta_1)| & \leq \frac{\epsilon}{6B} \\ 
        \sup_s |pol_{2,i}(s) - g_i(s; \theta_2)| & \leq \frac{\epsilon}{6B}. 
    \end{align}
    Here $B := \max(1, \sup_p \max_{i} |pol_{1, i}(p)|, \sup_s \max_{i} |pol_{2, i}(s)|).$ 
    We show that the cluster sampler is capable to approximate any desirable continuous cluster sampler. 
    \begin{equation}
        \sup_p \sup_s |h(p, s) -\sum_{i=1}^C f_i(p; \theta_1) g_i(s; \theta_2)| \leq \frac{\epsilon}{2} + \frac{\epsilon}{6B} * B + \frac{\epsilon}{6B} (B + \frac{\epsilon}{6B}) = \frac{5}{6} \epsilon + \frac{\epsilon^2}{36B^2} < \epsilon. 
    \end{equation}
    The last inequality comes from $\epsilon < 6$. 
    The universal approximation capacity of the cluster sampler is proved. 
\end{proof}

\begin{remark}
    Since we are working with a cluster sampler with specific manually designed structure, it mainly comes from the fact the student's t-kernel introduce a suitable implicit bias to more efficiently learn the cluster sample probability $(n_2 \times 1)$. 
\end{remark}
