In this section, we present the overall framework design of \textbf{L}atent \textbf{E}ntity \textbf{A}lignment \textbf{L}earning (Leal), with the model structure illustrated in Figure~\ref{fig:leal-framework}. Section~\ref{subsec:soft-alignment} introduces the soft alignment mechanism, which maps primary and secondary data records to a shared latent space. In Section~\ref{subsec:cluster-sampler}, we describe the cluster sampler, which dynamically selects the most relevant secondary records during training. Finally, Section~\ref{subsec:train-infer} outlines the training and inference processes.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/Leal-Framework.pdf}
    \caption{Overall model structure of Leal}
    \label{fig:leal-framework}
\end{figure*}


\subsection{Soft Alignment}\label{subsec:soft-alignment}

The soft alignment mechanism is motivated by the observation that \textit{properly aligned data result in smaller loss compared to misaligned data}. We provide a formal expression for linear regression in Theorem~\ref{thm:alignment}, with the proof detailed in Appendix~\ref{subsec:proof-align-misalign}.
\begin{restatable}{theorem}{thmalignment}\label{thm:alignment}
    Let $\mathbf{X}^P\in \mathbb{R}^{n \times m^P}$ and $\mathbf{X}^S\in \mathbb{R}^{n \times m^S}$ be normalized primary and secondary feature matrices, respectively, $\mathbf{y}\in \mathbb{R}^{n}$ be the target variable, and $\mathbf{R}$ be a permutation matrix. For the linear regression model $\mathbf{y} = \mathbf{X}^P \boldsymbol{\alpha} + \mathbf{R}\mathbf{X}^S \boldsymbol{\beta}$, it holds that $\mathrm{MSE}_{\mathrm{aligned}} \leq \mathrm{MSE}_{\mathrm{misaligned}}$, where $\mathrm{MSE}_{\mathrm{aligned}}$ and $\mathrm{MSE}_{\mathrm{misaligned}}$ are the mean squared errors under correct alignment and random alignment.
\end{restatable}

Theorem~\ref{thm:alignment} suggests that, for linear regression tasks with sufficient samples, properly aligned data consistently results in lower loss. While extending this theoretical result to deep learning remains an open challenge, our experimental results in Figure~\ref{fig:motivation} demonstrate that when \(n\) is sufficiently large that prevents the model from memorizing the data, even complex deep learning models struggle to achieve low loss when the data is misaligned.

To perform soft alignment between a primary data record \(x^P_i\) and \(K\) secondary data records \(\mathbf{x}^S_i = \{x^S_j\}_{j=1}^K\) with distinct features, we first map them into a shared latent space:
\begin{equation}
z^P_i = f^P(x^P_i), \quad \mathbf{z}^S = f^S(\mathbf{x}^S)
\end{equation}
In this latent space, the distance between \(z^P_i \in \mathbb{R}^{1 \times d}\) and each row vector of \(\mathbf{z}^S \in \mathbb{R}^{K \times d}\) represents the relationship between \(x^P_i\) and each record in \(\mathbf{x}^S\), where \(d\) is the dimensionality of the latent space, and \(K\) is the number of candidate records from the secondary table. The scaled inner product between \(z^P_i\) and \(\mathbf{z}^S\) is computed to measure their similarities. The similarity scores for all secondary records are normalized using a softmax function to obtain the soft alignment weights \(\boldsymbol{\lambda}_i\):
\begin{equation}
\boldsymbol{\lambda}_i = \text{SoftMax}\left(\frac{z^P_i (\mathbf{z}^S)^T}{\sqrt{d}}\right)
\end{equation}
The alignment weights \(\boldsymbol{\lambda}_i\) indicate the importance of each record pair and are applied to the latent vectors. Weighted latent vectors are then aggregated for further processing:
\begin{equation}
\tilde{z}^P_i = z^P_i + \sum_{j=1}^K \lambda_{ij} z^S_j
\end{equation}
This weighted alignment module is functionally equivalent to the widely used attention mechanism. Consequently, we directly adopt multi-head attention \cite{vaswani2017attention} in our implementation. Specifically, both primary and secondary representations are processed through a self-attention module followed by a feed-forward network. Their intermediate representations are then aggregated using an attention layer, followed by another feed-forward network. This structure can be stacked to form a deep neural network.


\subsection{Cluster Sampler}\label{subsec:cluster-sampler}

The cluster sampler is designed to efficiently select \(K\) candidate secondary records to feed into the model. For small secondary tables, such as those with hundreds of records, directly feeding the entire table into the model is feasible and demonstrates promising performance in our experiments. However, for large secondary tables, this approach becomes inefficient and may lead to overfitting.

To address this, we propose a trainable cluster sampler to dynamically sample the most relevant records during each training step. The cluster sampler selects \(K\) records based on two criteria: cluster weights and in-cluster probabilities.
The cluster weight is generated by a trainable cluster weight generator, implemented as a feed-forward network that takes the primary record \(x^P_i\) as input and outputs a \(C\)-dimensional vector, where \(C\) is the number of clusters. The cluster weight is normalized using a softmax function to produce the cluster weight vector \(\mathbf{w}_i \in \mathbb{R}^{1 \times C}\):
\begin{equation}
\mathbf{w}_i = \text{SoftMax}(\text{FeedForward}(x^P_i))
\end{equation}
The in-cluster probability is generated using a trainable soft deep K-Means module, inspired by \citet{xie2016unsupervised,ye2024ptarl}. A pretrained encoder maps the secondary table \(\mathbf{x}^S\) into a latent space, resulting in \(\mathbf{h}^S = g^S(\mathbf{x}^S) \in \mathbb{R}^{n_2 \times d}\). Cluster centroids \(\mathbf{C} \in \mathbb{R}^{C \times d}\) are initialized via K-Means clustering on \(\mathbf{h}^S\). Following \citet{xie2016unsupervised}, each record is assigned to a cluster with a probability matrix \(\mathbf{q} \in \mathbb{R}^{n_2 \times C}\), computed using the Student's t-distribution kernel:
\begin{equation}
    q_{ij} = \frac{(1 + ||h^S_j - c_i||^2/\gamma)^{-(\gamma+1)/2}}{\sum_{t=1}^C (1 + ||h^S_j - c_t||^2/\gamma)^{-(\gamma+1)/2}}
\end{equation}
where \(\gamma\) is the degree of freedom, and \(c_i\) and \(h^S_j\) represent the \(i\)-th cluster centroid and the \(j\)-th latent vector, respectively.
The final sampling probability for \(x^P_i\) is computed as the product of cluster weight and in-cluster probability, followed by a multi-layer perceptron (MLP):
\begin{equation}
    \mathbf{p}_i = \mathrm{MLP}(\mathbf{q} \mathbf{w}_i^T) \in \mathbb{R}^{n_2 \times 1} 
\end{equation}
The \(K\) secondary records are sampled based on probabilities \(\mathbf{p}_i\) during training and selected from the top \(\mathbf{p}_i\) during inference. The embeddings of these secondary records are then fed into the soft alignment module.

Furthermore, we theoretically demonstrate that the trainable weights in the cluster sampler design are capable of approximating any target sampling function in Theorem~\ref{thm:cluster-sampler}, which is formally proven in Appendix~\ref{sec:proof}.
\begin{restatable}{theorem}{thmclustersampler}\label{thm:cluster-sampler}
    For any uniform equi-continuous fixed optimal cluster sampler $h^*$ and any $\epsilon > 0$, there exists $d, C$ 
    and a corresponding weight $\theta$ for cluster sampler $h_{CS}$ such that 
    \begin{align}
        \sup_{x \in \mathbf{X}^P} |h^*(x) - h_{CS}(x; \theta)| \leq \epsilon. 
    \end{align}
\end{restatable}


\subsection{Training and Inference}\label{subsec:train-infer}

The training process comprises two stages: an unsupervised stage and a supervised stage. The unsupervised stage focuses on learning an initial representation to facilitate cluster initialization, while the supervised stage is designed to train the model to predict the primary target.

\paragraph{Training.} 
In the unsupervised stage, the pretrained encoder is trained using an autoencoder \cite{zhai2018autoencoder}, where both the encoder \(g^S\) and decoder \(\phi^S\) are simple multi-layer perceptrons (MLPs) with LayerNorm \cite{ba2016layer}:
\begin{equation}
\min_{g^S,\phi^S} \|\mathbf{X}^S - \phi^S(g^S(\mathbf{X}^S))\|_2^2
\end{equation}
In the supervised stage, during each epoch, each batch of data records from the primary table, referred to as the \textit{primary records}, are fed into the cluster sampler alongside the entire secondary table. The sampling probabilities are then calculated (lines 6-8), and the indices of the selected records from the secondary table, referred to as \textit{secondary records}, are output (line 9). The primary record and secondary records are subsequently passed into an attention-based soft alignment module (lines 10-11) for further training to derive the final label (line 12) and loss (line 13), as illustrated in Figure~\ref{fig:leal-framework}. The pretrained encoder, cluster centroids, feed-forward layers, and attention layers are optimized jointly (line 14). The detailed training process is presented in Algorithm \ref{alg:training}.

\begin{algorithm}[t]
\caption{Training Process of Leal}
\label{alg:training}

\begin{algorithmic}[1]
\REQUIRE Primary table \(\mathbf{X}^P\), secondary table \(\mathbf{X}^S\), labels \(\mathbf{y}\), batch size \(B\), number of clusters \(C\), number of candidate records \(K\), learning rate \(\eta\).
\OUTPUT Trained model parameters \(\theta\)
\STATE Initialize model parameters \(\theta\) randomly
\STATE Train encoder-decoder \((g^S, \phi^S)\) by minimizing \(\|\mathbf{X}^S - \phi^S(g^S(\mathbf{X}^S))\|_2^2\)
\STATE Initialize cluster centroids \(\mathbf{C}\) via K-means on \(g^S(\mathbf{X}^S)\)
\FOR{each epoch}
    \FOR{each batch \(\{x^P_b, y_b\}_{b=1}^B\) from \((\mathbf{X}^P, \mathbf{y})\)}
        \STATE Calculate cluster weights: \(\mathbf{w}_b \leftarrow \text{SoftMax}(\text{FeedForward}(x^P_b))\)
        \STATE Calculate in-cluster probabilities \(\mathbf{q}\) using Student's t-distribution
        \STATE Obtain sampling probabilities \(\mathbf{p}_b \leftarrow \mathrm{MLP}(\mathbf{q} \mathbf{w}_i^T)\)
        \STATE Sample \(K\) secondary records \(\mathbf{x}^S_b\) according to \(\mathbf{p}_b\)
        \STATE Encoding: \(z^P_b \leftarrow f^P(x^P_b)\), \(\mathbf{z}^S_b \leftarrow f^S(\mathbf{x}^S_b)\)
        \STATE Compute attention weights \(\boldsymbol{\lambda}_b\) and embedding \(\tilde{z}^P_b\)
        \STATE Predict labels: \(\hat{y}_b \leftarrow f(\theta; \tilde{z}^P_b)\)
        \STATE Calculate loss: \(\mathcal{L}_b \leftarrow \mathcal{L}(\hat{y}_b, y_b)\)
        \STATE Update parameters \(\theta \leftarrow \theta - \eta \nabla_{\theta} \mathcal{L}_b\)
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\paragraph{Inference.} During inference, the pretrained encoder, cluster centroids, and feed-forward layer remain fixed. The procedure follows the forward pass of the supervised stage, with the only variation being in the cluster sampler. To ensure determinism, the top records with the highest sampling probabilities $\mathbf{p}_i$ are selected.


\paragraph{Scalability.} Suppose that training an individual table requires $M_0$ memory and $T_0$ time. Let $K$ represent the number of candidate secondary records. The current memory overhead of Leal is $K M_0$, as for each primary record, $K$ neighboring records are fed into the training process. The time complexity of Leal is $O(K^2 T_0)$, which arises from the quadratic complexity of the attention mechanism. However, with the cluster sampler, $K$ is typically less than 100, making the training time and memory overhead of Leal manageable.
