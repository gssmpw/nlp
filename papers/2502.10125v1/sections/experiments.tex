\subsection{Experimental Setup}

We conduct experiments on five real-world and five synthetic datasets, covering classification and regression tasks. Each real-world dataset comprises two tables from different sources, with shared features removed. The \texttt{house} dataset combines data from Lianjia \cite{qiu2017} and Airbnb \cite{airbnb2019} for house price prediction. The \texttt{bike} dataset uses Citibike \cite{citibike2016} and New York City Taxi routes \cite{nytlc2016} for travel time prediction. The \texttt{hdb} dataset integrates HDB resale prices \cite{hdb2018} and school rankings \cite{salary2020} in Singapore for resale price prediction. The \texttt{accidents} dataset, derived from Slovenian police traffic records \cite{slovenian_police_traffic_safety}, consists of individual and accident-level tables for accident type classification. The \texttt{hepatitis} dataset, sourced from the PKDD'02 Discovery Challenge database \cite{berka2002hepatitis}, consists of medical check tables for distinguishing Hepatitis B and C cases. Any shared features, if present, are removed from secondary tables. Dataset statistics are summarized in Table~\ref{tab:real_world_dataset_stats}, where ``cls'' means classification and ``reg'' means regression.

The synthetic datasets are generated from the UCI repository and include \texttt{breast} \cite{zwitter1988breast}, \texttt{covertype} \cite{blackard1998covertype}, \texttt{gisette} \cite{guyon2004gisette}, \texttt{letter} \cite{slate1991letter}, and \texttt{superconduct} \cite{hamidieh2018superconductivity}. Each synthetic dataset is randomly divided by features into two tables, serving as primary and secondary tables, without any overlapped feature. Detailed statistics of the synthetic datasets are provided in Table~\ref{tab:synthetic_dataset_stats}.

We apply one-hot encoding to categorical features, and normalize all features of real-world datasets to mitigate the impact of extreme values. Therefore, all features are treated as numerical in our experiments.



\begin{table}[ht]
  \centering
  \caption{Details of real-world datasets}
  \vskip 0.15in
  \small
  \scalebox{0.82}{
  \begin{tabular}{lcccccc}
  \toprule
  \multirow{2}{*}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{Primary}} & \multicolumn{2}{c}{\textbf{Secondary}} & \multirow{2}{*}{\textbf{\#Class}} & \multirow{2}{*}{\textbf{Task}} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-5}
  & \textbf{\#Inst.} & \textbf{\#Feat.} & \textbf{\#Inst.} & \textbf{\#Feat.} & & \\
  \midrule
accidents & 237337 & 21 & 419818 & 16 & 6 & cls \\
bike & 200000 & 966 & 100000 & 4 & 1 & reg \\
hdb & 92065 & 70 & 165 & 9 & 1 & reg \\
hepatitis & 621 & 6 & 5691 & 10 & 2 & cls \\
house & 141049 & 54 & 27827 & 23 & 1 & reg \\
  \bottomrule
  \end{tabular}}
  \label{tab:real_world_dataset_stats}
\vspace{-5pt}
\end{table}


\begin{table}[ht]
  \centering
  \caption{Details of synthetic datasets}
  \vskip 0.15in
  \small
  \begin{tabular}{lcccc}
  \toprule
  \textbf{Dataset} & \textbf{\#Inst.} & \textbf{\#Feat.} & \textbf{Classes} & \textbf{Task} \\
  \midrule
breast & 286 & 43 & 2 & cls \\
covertype & 581012 & 54 & 7 & cls \\
gisette & 6000 & 5000 & 2 & cls \\
letter & 20000 & 15 & 26 & cls \\
superconduct & 21263 & 81 & 1 & reg \\
  \bottomrule
  \end{tabular}
  \label{tab:synthetic_dataset_stats}
\end{table}




\paragraph{Training.} All models are trained using the AdamW~\cite{loshchilov2017decoupled} optimizer with early stopping and a learning rate of 0.001. A batch size of 128 is employed, and training is conducted for up to 150 epochs, with early stopping determined by validation loss. The hyperparameters \(K\) and \(C\) are selected from the set \(\{1, 5, 10, 20, 100\}\), while the layer depth is chosen from \(\{1, 3, 6\}\). In the unsupervised stage, the depth of the encoder and decoder is set to 2, except for the \texttt{hdb} dataset, where a depth of 1 is used to prevent overfitting. All embedding dimensions are fixed at 100.


\paragraph{Evaluation.} For all datasets, the primary table is divided into training, validation, and test sets in a 7:1:2 ratio. The secondary table is utilized during both training and inference. Evaluation metrics include accuracy for classification tasks and root mean squared error (RMSE) for regression tasks. The mean and standard deviation of test scores are reported over five seeds. 



\paragraph{Baselines.} We compare the proposed method with state-of-the-art deep tabular learning methods. "Solo" denotes training on the primary table only. The baselines are:
\begin{itemize}
    \item \textbf{Solo-MLP:} A three-layer MLP with hidden sizes of (800, 400, 400) and ReLU activation.
    \item \textbf{Solo-ResNet~\cite{gorishniy2021revisiting}:} An MLP model incorporating skip connection, ReLU activation and batch normalization.
    \item \textbf{Solo-FTTrans~\cite{gorishniy2021revisiting}:} A transformer-based model that embeds each feature as a token.
    \item \textbf{Solo-TabM~\cite{gorishniy2024tabm}:} A state-of-the-art multi-layer deep ensemble-based model.
\end{itemize}


\paragraph{Environement.} Each task is executed on a single NVIDIA V100 GPU with 32GB memory and an Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz. The system is equipped with 1.48TB of memory, which is not fully utilized. The code is implemented using PyTorch 2.5 and Python 3.10.




\subsection{Performance}

\begin{table*}[ht]
  \centering
  \caption{Performance of Leal and baselines on real-world datasets}\label{tab:real-perf}
  \vskip 0.15in
  \small
  \begin{tabular}{lcccccc}
  \toprule
  
  \multirow{2}{*}{\textbf{Methods}} & \multicolumn{5}{c}{\textbf{Real-world Dataset}} \\
  \cmidrule{2-6}
   & \textbf{accidents} (↑) & \textbf{bike} (↓) & \textbf{hdb} (↓) & \textbf{hepatitis} (↑) & \textbf{house} (↓)\\
  \midrule
  
Solo-MLP & 73.65\% \textpm 0.08\% & 541.4278 \textpm 1.5190 & 298.3065 \textpm 3.2910 & 65.74\% \textpm 11.74\% & 215.8363 \textpm 0.4579 \\
Solo-ResNet & \underline{73.89\%} \textpm 0.08\% & \textbf{242.7551} \textpm 0.9368 & 35.2518 \textpm 0.1684 & \underline{76.80\%} \textpm 4.02\% & 72.2510 \textpm 0.3008 \\
Solo-TabM & 73.69\% \textpm 0.05\% & 253.4289 \textpm 0.6411 & \underline{35.2030} \textpm 0.1276 & 74.40\% \textpm 3.39\% & 75.5523 \textpm 0.4723 \\
Solo-FTTrans & 73.47\% \textpm 0.10\% & 301.9098 \textpm 26.4545 & 35.4862 \textpm 0.2513 & 74.24\% \textpm 4.39\% & \underline{70.6538} \textpm 0.6250 \\
\midrule
Leal & \textbf{73.92\%} \textpm 0.16\% & \underline{248.0001} \textpm 1.5935 & \textbf{34.8088} \textpm 0.3468 & \textbf{79.03\%} \textpm 2.39\% & \textbf{51.7545} \textpm 0.2352 \\
  \bottomrule
\end{tabular}
\vspace{-5pt}
\end{table*}




\begin{table*}[ht]
  \centering
  \caption{Performance of Leal and baselines on synthetic datasets (OOM: Out of memory)}\label{tab:syn-perf}
  \vskip 0.15in
  \small
  \begin{tabular}{lcccccccccccc}
  \toprule
  \multirow{2}{*}{\textbf{Methods}} & \multicolumn{5}{c}{\textbf{Synthetic Dataset}} \\
  \cmidrule{2-6}
   & \textbf{breast} (↑) & \textbf{covertype} (↑) & \textbf{gisette} (↑) & \textbf{letter} (↑) & \textbf{superconduct} (↓)\\
  \midrule

Solo-MLP & 86.55\% \textpm 3.34\% & 58.60\% \textpm 13.30\% & 96.52\% \textpm 0.83\% & 44.88\% \textpm 5.01\% & 11.5641 \textpm 5.4981 \\
Solo-ResNet & \underline{89.65\%} \textpm 4.50\% & 73.75\% \textpm 0.12\% & \underline{97.30\%} \textpm 0.39\% & \textbf{89.55\%} \textpm 0.50\% & 0.1563 \textpm 0.1459 \\
Solo-TabM & 86.55\% \textpm 3.34\% & \underline{73.81\%} \textpm 0.19\% & 96.73\% \textpm 0.44\% & 83.97\% \textpm 0.49\% & 0.3011 \textpm 0.0739 \\
Solo-FTTrans & 87.59\% \textpm 3.68\% & 60.86\% \textpm 1.83\% & OOM & \underline{89.48\%} \textpm 0.46\% & \underline{0.1534} \textpm 0.1460 \\
\midrule
Leal & \textbf{93.11\%} \textpm 3.08\% & \textbf{76.46\%} \textpm 4.68\% & \textbf{97.57\%} \textpm 0.44\% & 87.57\% \textpm 1.28\% & \textbf{0.1468} \textpm 0.1505 \\
  \bottomrule
\end{tabular}%
\vspace{-5pt}
\end{table*}



The performance of Leal and the baseline methods across various datasets is summarized in Table~\ref{tab:real-perf} for real-world datasets and Table~\ref{tab:syn-perf} for synthetic datasets. Two key findings emerge from these results. First, Leal outperforms state-of-the-art approaches trained solely on the primary table for the majority of datasets. For instance, on the \texttt{house} dataset, Leal reduces RMSE by at least 26.8\% compared to all baselines. Second, Leal demonstrates significantly better performance on real-world datasets compared to synthetic datasets. This is attributed to the presence of fuzzy and many-to-many alignments in real-world datasets, which amplify the effectiveness of soft alignment by better capturing complex relationships and dependencies between data records, as also observed in \cite{wu2024federated}. In contrast, synthetic datasets typically involve only one-to-one alignments, limiting the benefits of soft alignment. Notably, FT-Transformer (FTTrans) fails to handle the high-dimensional \texttt{gisette} dataset due to its encoding approach, which treats each feature as a separate token. This results in an out-of-memory error on all tested GPUs.







\subsection{Efficiency}\label{subsec:efficiency}

Table~\ref{tab:train-time} presents the average training time per epoch for Leal and baseline methods, with $K=20$ and $C=20$, a typical hyperparameter setting for Leal in our experiments. Despite addressing the complex alignment challenges inherent in its design, Leal maintains competitive training efficiency compared to baselines trained on a single table. On average, Leal incurs a computational overhead ranging from 1.27\(\times\) to 22.82\(\times\), even with the quadratic growth in the number of record pairs introduced by incorporating the secondary table. This efficiency is largely attributed to Leal’s cluster sampler module, which significantly reduces the number of candidate pairs in the soft alignment process, thereby mitigating the computational burden. 

The overhead of Leal on large datasets is reasonable. For instance, on the \texttt{covertype} dataset with 581K instances in both tables, the alignment-training process for Leal requires approximately 692 minutes ($415 \times 100 / 60$ min). For comparison, adopting state-of-the-art approximate nearest neighbor search methods \textit{without learning}, such as FAISS-IVF \cite{douze2024faiss}, with a recall $\leq 10^{-2}$, would take around 312 minutes for a the same table scale according to a recent benchmark \cite{aumuller2020ann}. This highlights that while Leal introduces additional computational costs, its efficiency remains reasonable for large-scale datasets.



\begin{table*}[t!]
    \centering
    \caption{Average training time per epoch in seconds (OOM: out of memory)}\label{tab:train-time}
    \small
    \vskip 0.15in
    \begin{tabular}{lcccccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{10}{c}{\textbf{Dataset}} \\
    \cmidrule{2-11}
    & accidents & bike & breast & covertype & gisette & hdb & hepatitis & house & letter & superconduct\\
    \midrule
    Solo-MLP & 3.41 & 9.12 & 1.00 & 18.21 & 1.26 & 4.88 & 1.00 & 7.23 & 2.14 & 2.22 \\
    Solo-ResNet & 16.59 & 18.59 & 1.00 & 46.39 & 1.39 & 9.08 & 1.00 & 14.59 & 3.03 & 3.18 \\
    Solo-TabM & 13.09 & 14.76 & 1.00 & 29.06 & 1.22 & 7.83 & 1.00 & 10.78 & 2.51 & 2.62 \\
    Solo-FTT & 11.80 & 244.07 & 1.00 & 28.83 & OOM & 20.15 & 1.00 & 15.95 & 1.36 & 1.40 \\
    \midrule
    Leal & 69.64 & 50.04 & 1.03 & 415.43 & 1.85 & 20.17 & 1.27 & 34.85 & 8.52 & 3.62 \\
    Mean Overhead & 20.40$\times$ & 5.49$\times$ & 1.03$\times$ & 22.82$\times$ & 1.51$\times$ & 4.13$\times$ & 1.27$\times$ & 4.82$\times$ & 6.27$\times$ & 2.59$\times$ \\
    \bottomrule
    \end{tabular}
\end{table*}



\subsection{Ablation Studies}

\paragraph{Effect of Soft Alignment.}
We evaluate the effectiveness of soft alignment through a controlled experiment. To eliminate the influence of sampling techniques, we select $K$ candidates, including one ground-truth correctly aligned secondary record and $K-1$ randomly sampled secondary records. This setup allows us to assess the attention mechanism's ability to identify the correct secondary record. Each experiment is conducted under five seeds, and we report the mean and standard deviation of the accuracy in Figure~\ref{fig:lealtop}.

Two key findings emerge from Figure~\ref{fig:lealtop}. First, soft alignment effectively identifies the correct alignment at a modest $K$ value, despite the noise introduced by random sampling. For example, to outperform Solo-ResNet, the \texttt{covertype} dataset supports $K \leq 320$, while the \texttt{letter} dataset supports $K \leq 10$. Second, a comparison of Figure~\ref{fig:lealtop} and Table~\ref{tab:syn-perf} reveals that the performance of Leal in Figure~\ref{fig:lealtop} with small $K$ significantly exceeds its performance in Table~\ref{tab:syn-perf}. This indicates that the primary bottleneck for Leal lies in the clustering process. Enhancing the clustering process to improve the probability of including the correct pair in the candidate set would further boost Leal’s performance.


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.49\linewidth]{fig/lealtop_k_covertype_resnet.png}
    \includegraphics[width=0.49\linewidth]{fig/lealtop_k_letter_resnet.png}
    \caption{Capacity of attention mechanism in soft alignment}
    \label{fig:lealtop}
\end{figure}


\paragraph{Effect of Hyperparameters.}
We conduct ablation studies to examine the impact of the number of clusters ($C$) and the number of neighbors ($K$) on Leal's performance. The results are illustrated in Figure~\ref{fig:ablation-C} and Figure~\ref{fig:ablation-K}, respectively. From these results, we derive three key findings: First, setting $K$ too large typically has a negative effect, consistent with the observations in Figure~\ref{fig:lealtop}. Second, the effect of $C$ is dataset-size-dependent. For instance, the large \texttt{bike} dataset benefits from a larger $C$ value, whereas for the small \texttt{hepatitis} dataset, a small $C$ value is sufficient. Third, the effect of $C$ also interacts with the number of neighbors ($K$). For example, in the \texttt{hepatitis} dataset (Figure~\ref{fig:ablation-C}), when $K$ is small, a larger $C$ value is advantageous; however, when $K$ is large, a smaller $C$ value proves to be more effective. These findings suggest that soft alignment and cluster sampling can complement each other during the training.


\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{fig/cluster-ablation-bike.pdf}
  \includegraphics[width=0.48\linewidth]{fig/cluster-ablation-hepatitis.pdf}
  \caption{Effect of cluster size $C$ on performance}
  \label{fig:ablation-C}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.48\linewidth]{fig/k-ablation-bike.pdf}
  \includegraphics[width=0.48\linewidth]{fig/k-ablation-hepatitis.pdf}
  \caption{Effect of number of neighbors $K$ on performance}
  \label{fig:ablation-K}
\end{figure}
