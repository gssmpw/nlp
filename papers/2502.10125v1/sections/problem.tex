Consider two tables, \(\mathbf{X}^P \in \mathbb{R}^{n^P \times m^P}\) and \(\mathbf{X}^S \in \mathbb{R}^{n^S \times m^S}\), where \(n^P\) and \(n^S\) represent the number of records in the primary table \(\mathbf{X}^P\) and the secondary table \(\mathbf{X}^S\), respectively, and \(m^P\) and \(m^S\) denote the number of features in these tables. We focus on a supervised learning task aimed at predicting a target variable \(\mathbf{y} \in \mathbb{R}^{n^P}\). Without loss of generality, we assume that \(\mathbf{y}\) is associated with \(\mathbf{X}^P\), referred to as the \textit{primary table}, while \(\mathbf{X}^S\) serves as the \textit{secondary table}.


We address a scenario where there are no shared features between \(\mathbf{X}^P\) and \(\mathbf{X}^S\), formally stated as \(a^P \cap a^S = \emptyset\), where \(a^P\) and \(a^S\) represent the feature sets of \(\mathbf{X}^P\) and \(\mathbf{X}^S\), respectively. Despite this, it is assumed that an approximate functional dependency \cite{mandros2017discovering} exists between the primary and secondary tables. This dependency is quantified using the information fraction (IF) metric \cite{reimherr2013quantifying}, defined as:

\[
\text{IF}(Y; X^S \mid X^P) = \frac{I(\mathbf{y}; \mathbf{X}^S \mid \mathbf{X}^P)}{H(\mathbf{y} \mid \mathbf{X}^P)}
\]

where \(I(\mathbf{y}; \mathbf{X}^S \mid \mathbf{X}^P) = H(\mathbf{y} \mid \mathbf{X}^P) - H(\mathbf{y} \mid \mathbf{X}^P, \mathbf{X}^S)\) is the conditional mutual information between \(\mathbf{y}\) and \(\mathbf{X}^S\) given \(\mathbf{X}^P\), and \(H(\mathbf{y} \mid \mathbf{X}^P)\) and \(H(\mathbf{y} \mid \mathbf{X}^P, \mathbf{X}^S)\) denote the conditional entropies of \(\mathbf{y}\) given \(\mathbf{X}^P\) and both \(\mathbf{X}^P\) and \(\mathbf{X}^S\), respectively. The IF measures the additional information provided by \(\mathbf{X}^S\) about \(\mathbf{y}\) beyond what is captured by \(\mathbf{X}^P\). Specifically, \(\text{IF}(Y; X^S \mid X^P) = 1\) implies a functional dependency, while \(\text{IF}(Y; X^S \mid X^P) = 0\) indicates statistical independence. We consider the case where \(|\text{IF}(Y; X^S \mid X^P) - 1|<\delta\), where \(\delta\) is a small positive constant, indicating a strong functional dependency between the primary and secondary tables.

The objective is to learn a predictive model by minimizing the following loss function:

\[
\min_{\theta} \frac{1}{n^P} \sum_{i=1}^{n^P} \mathcal{L}\left(f(\theta; x_i^P, \mathbf{X}^S), y_i\right)
\]

where \(f(\theta; x_i^P, \mathbf{X}^S)\) is a model parameterized by \(\theta\), \(\mathcal{L}\) is a loss function, and \(y_i\) is the target for the \(i\)-th record in \(\mathbf{X}^P\). The challenge lies in effectively utilizing the secondary table \(\mathbf{X}^S\) to enhance the predictive performance for \(\mathbf{y}\), despite the lack of common features between the two tables.
