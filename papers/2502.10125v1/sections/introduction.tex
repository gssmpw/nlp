Tabular data is a prevalent structured data format, especially in real-world databases. Training models on such data has numerous applications across domains, including medical and financial fields. However, real-world tabular data is often highly heterogeneous, with each table containing a distinct set of features and unique data distributions. This challenge is commonly referred to as the data lake problem~\cite{nargesian2019data} or the data silo problem~\cite{patel2019bridging}. Existing machine learning approaches~\cite{gorishniy2021revisiting, gorishniy2024tabm, chen2016xgboost, prokhorenkova2018catboost} primarily focus on learning from individual tables in isolation. In practice, however, tables are often correlated, and leveraging information across tables can significantly enhance predictive performance. For instance, in the financial domain, data from a transaction table can improve predictions in a loan table by joining the tables on shared features - referred to as \textit{keys} in relational databases - and subsequently training models on the joined table.


The join-learn paradigm is effective in ideal relational databases but faces significant challenges with heterogeneous tabular data widely existed in practice. The primary limitation is the absence of shared features. For example, in an anonymous Bitcoin transaction table and a bank transaction table (Figure~\ref{fig:app-example}), no shared features exist to facilitate a join. Even when tables share features, identifying them by schema or column names is difficult, with exact match accuracy reaching only 18.4\% according to a previous study \cite{vogel2024wikidbs}. While some methods address partially \cite{kang2022fedcvt, sun2023communication} or fuzzily \cite{wu2022coupled, wu2024federated} aligned keys, they can not handle scenarios with no shared features, a scenario we refer to as \textit{latent alignment learning}. 

The second limitation pertains to efficiency, particularly in scenarios involving many-to-many key relationships. Such cases often leads to significant time cost and high memory consumption. In our study, even joining two tables with 200k and 400k rows without optimization takes approximately four hours, with a memory cost 22$\times$ higher than that of the individual tables. These challenges hinder the scalability and practical adoption of join-based learning methods for heterogeneous tabular data in real applications.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/leal-app.pdf}
    \caption{Example of latent alignment learning in a financial application: (left) bank transaction table and (right) Bitcoin transaction table. The tables lack shared features, and the label “Bitcoin-related” can be inferred if bank withdrawals are proportional to Bitcoin deposits (or vice versa).}
    \label{fig:app-example}
\end{figure}


A significant challenge arises to enable the training of relational tabular data without shared features. The absence of keys prevents the evaluation of match probabilities based on key similarities, as traditionally done in relational databases \cite{mishra1992join}. Therefore, identifying new criteria to estimate match probabilities between records across tables becomes crucial. Moreover, designing a model that can effectively learn based on such criteria introduces additional complexity.

To address the above challenges, we propose using training loss decay to estimate match probabilities, based on the insight that \textit{properly aligned data result in smaller loss than misaligned data}. This is formally established in Theorem~\ref{thm:alignment} and validated empirically in Figure~\ref{fig:motivation}. Matched record pairs are probabilistically identified as those yielding the greatest loss decay. Candidate records are embedded and compared via an attention mechanism to assess alignment. Experimental results show that the soft alignment mechanism achieves performance comparable to perfectly aligned data when candidate records contain the ground truth.




\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/motivation.png}
    \caption{Relationship between data alignment and model training loss. Consider the training of a model on two relational tables: the first table contains a single feature \(x_1\) and a binary label \(y\), while the second table contains a single feature \(x_2\). The left subfigure illustrates the distribution of data points \((x_1, x_2)\) when \(x_1\) and \(x_2\) are properly aligned, while the right subfigure depicts the case of misaligned data. Both subfigures present the converged mean-squared loss and the decision boundary of linear regression.}
    \label{fig:motivation}
\end{figure}

Soft alignment, despite its advantages, remains insufficient for achieving practical latent alignment learning due to scalability challenges. As the number of candidate pairs increases, it becomes prone to overfitting and incurs significant computational costs. To overcome this challenge, we propose a novel module, the \textit{cluster sampler}, which selects a small subset of data records from the table. This module organizes data records into clusters using a soft, differentiable clustering approach and samples records from each cluster. The cluster assignments are dynamically updated during training through gradients propagated from subsequent modules. By limiting the number of candidate pairs, the \textit{cluster sampler} significantly enhances the model's scalability, enabling its effective application to larger tables.

In summary, we propose Latent Entity Alignment Learning (Leal), a coupled model that integrates alignment into supervised learning. Leal addresses the challenge of missing shared features by proposing soft alignment, which learns alignment through training loss. To further improve efficiency, Leal introduces the \textit{cluster sampler}, designed to mitigate overfitting and reduce computational costs when applied to larger tables. To the best of our knowledge, Leal is the first model to enable machine learning across relational tabular data without shared features and pre-aligned samples. The contributions of this paper can be summarized as follows:

\begin{itemize}
    \item We propose a novel approach, Leal, to enable machine learning across relational tabular data without shared features or pre-aligned samples.
    \item We provide theoretical demonstrations to the advantages of aligned data over misaligned data in terms of loss and the approximation capacity of the cluster sampler design.
    \item We evaluate Leal on five real-world and five synthetic datasets, demonstrating its effectiveness in learning relational tabular data without shared features. Our experiments show that Leal reduces the error by up to 26.8\% over state-of-the-art methods trained on individual tables with reasonable computational overhead.
\end{itemize}
