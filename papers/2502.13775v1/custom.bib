
@misc{gpt4,
	author = {OpenAI},
	title = {},
	howpublished = {\url{https://openai.com/index/gpt-4-research/}},
	year = {2024},
	note = {[Accessed 05-02-2025]},
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@inproceedings{positionpluralistic,
  title={Position: A Roadmap to Pluralistic Alignment},
  author={Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell L and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and others},
  booktitle={Forty-first International Conference on Machine Learning},
    year={2024}
}

@inproceedings{sorensen2024value,
  title={Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties},
  author={Sorensen, Taylor and Jiang, Liwei and Hwang, Jena D and Levine, Sydney and Pyatkin, Valentina and West, Peter and Dziri, Nouha and Lu, Ximing and Rao, Kavel and Bhagavatula, Chandra and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={18},
  pages={19937--19947},
  year={2024}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{feng2024modular,
  title={Modular pluralism: Pluralistic alignment via multi-llm collaboration},
  author={Feng, Shangbin and Sorensen, Taylor and Liu, Yuhan and Fisher, Jillian and Park, Chan Young and Choi, Yejin and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2406.15951},
  year={2024}
}

@inproceedings{
liu2023trustworthy,
title={Trustworthy {LLM}s: a Survey and Guideline for Evaluating Large Language Models' Alignment},
author={Yang Liu and Yuanshun Yao and Jean-Francois Ton and Xiaoying Zhang and Ruocheng Guo and Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hang Li},
booktitle={Socially Responsible Language Modelling Research},
year={2023},
url={https://openreview.net/forum?id=oss9uaPFfB}
}

@article{wang2023ethical,
  title={Ethical considerations of using ChatGPT in health care},
  author={Wang, Changyu and Liu, Siru and Yang, Hao and Guo, Jiulin and Wu, Yuxuan and Liu, Jialin},
  journal={Journal of Medical Internet Research},
  volume={25},
  pages={e48009},
  year={2023},
  publisher={JMIR Publications Toronto, Canada}
}

@article{krugel2023chatgpt,
  title={ChatGPT’s inconsistent moral advice influences users’ judgment},
  author={Kr{\"u}gel, Sebastian and Ostermaier, Andreas and Uhl, Matthias},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={4569},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{hartmann2023political,
  title={The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation},
  author={Hartmann, Jochen and Schwenzow, Jasper and Witte, Maximilian},
  journal={arXiv preprint arXiv:2301.01768},
  year={2023}
}

@inproceedings{santurkar2023whose,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  booktitle={International Conference on Machine Learning},
  pages={29971--30004},
  year={2023},
  organization={PMLR}
}

@article{sharma2023human,
  title={Human--AI collaboration enables more empathic conversations in text-based peer-to-peer mental health support},
  author={Sharma, Ashish and Lin, Inna W and Miner, Adam S and Atkins, David C and Althoff, Tim},
  journal={Nature Machine Intelligence},
  volume={5},
  number={1},
  pages={46--57},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{song2024typing,
  title={The typing cure: Experiences with large language model chatbots for mental health support},
  author={Song, Inhwa and Pendse, Sachin R and Kumar, Neha and De Choudhury, Munmun},
  journal={arXiv preprint arXiv:2401.14362},
  year={2024}
}

@article{wang2023chatgpt,
  title={ChatGPT: promise and challenges for deployment in low-and middle-income countries},
  author={Wang, Xiaofei and Sanders, Hayley M and Liu, Yuchen and Seang, Kennarey and Tran, Bach Xuan and Atanasov, Atanas G and Qiu, Yue and Tang, Shenglan and Car, Josip and Wang, Ya Xing and others},
  journal={The Lancet Regional Health--Western Pacific},
  volume={41},
  year={2023},
  publisher={Elsevier}
}

@article{li2023ethics,
  title={Ethics of large language models in medicine and medical research},
  author={Li, Hanzhou and Moon, John T and Purkayastha, Saptarshi and Celi, Leo Anthony and Trivedi, Hari and Gichoya, Judy W},
  journal={The Lancet Digital Health},
  volume={5},
  number={6},
  pages={e333--e335},
  year={2023},
  publisher={Elsevier}
}

@article{argyle2023out,
  title={Out of one, many: Using language models to simulate human samples},
  author={Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and Gubler, Joshua R and Rytting, Christopher and Wingate, David},
  journal={Political Analysis},
  volume={31},
  number={3},
  pages={337--351},
  year={2023},
  publisher={Cambridge University Press}
}

@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@inproceedings{gordon2022jury,
  title={Jury learning: Integrating dissenting voices into machine learning models},
  author={Gordon, Mitchell L and Lam, Michelle S and Park, Joon Sung and Patel, Kayur and Hancock, Jeff and Hashimoto, Tatsunori and Bernstein, Michael S},
  booktitle={Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@misc{OED-overton-window,
	title        = {{Oxford English Dictionary, s.v. “Overton window (n.)”}},
	year         = 2025,
	publisher    = {Oxford University Press},
}


@article{globalopinionQA,
  title={Towards measuring the representation of subjective global opinions in language models},
  author={Durmus, Esin and Nyugen, Karina and Liao, Thomas I and Schiefer, Nicholas and Askell, Amanda and Bakhtin, Anton and Chen, Carol and Hatfield-Dodds, Zac and Hernandez, Danny and Joseph, Nicholas and others},
  journal={arXiv preprint arXiv:2306.16388},
  year={2023}
}

@article{chen2024combating,
  title={Combating misinformation in the age of llms: Opportunities and challenges},
  author={Chen, Canyu and Shu, Kai},
  journal={AI Magazine},
  volume={45},
  number={3},
  pages={354--368},
  year={2024},
  publisher={Wiley Online Library}
}

@article{menz2024current,
  title={Current safeguards, risk mitigation, and transparency measures of large language models against the generation of health disinformation: repeated cross sectional analysis},
  author={Menz, Bradley D and Kuderer, Nicole M and Bacchi, Stephen and Modi, Natansh D and Chin-Yee, Benjamin and Hu, Tiancheng and Rickard, Ceara and Haseloff, Mark and Vitry, Agnes and McKinnon, Ross A and others},
  journal={bmj},
  volume={384},
  year={2024},
  publisher={British Medical Journal Publishing Group}
}

@article{suarez2021prevalence,
  title={Prevalence of health misinformation on social media: systematic review},
  author={Suarez-Lledo, Victor and Alvarez-Galvez, Javier},
  journal={Journal of medical Internet research},
  volume={23},
  number={1},
  pages={e17187},
  year={2021},
  publisher={JMIR Publications Toronto, Canada}
}

@article{porter2010value,
  title={What is value in health care?},
  author={Porter, Michael E},
  journal={New England Journal of Medicine},
  volume={363},
  number={26},
  pages={2477--2481},
  year={2010},
  publisher={Mass Medical Soc}
}

@article{kreuter2004role,
  title={The role of culture in health communication},
  author={Kreuter, Matthew W and McClure, Stephanie M},
  journal={Annu. Rev. Public Health},
  volume={25},
  number={1},
  pages={439--455},
  year={2004},
  publisher={Annual Reviews}
}

@article{klessig1992effect,
  title={The effect of values and culture on life-support decisions.},
  author={Klessig, Jill},
  journal={Western Journal of Medicine},
  volume={157},
  number={3},
  pages={316},
  year={1992},
  publisher={BMJ Publishing Group}
}

@article{de2000sensitivity,
  title={Sensitivity and perspective in the valuation of health status: whose values count?},
  author={De Wit, G Ardine and Busschbach, Jan JV and De Charro, Frank Th},
  journal={Health economics},
  volume={9},
  number={2},
  pages={109--126},
  year={2000},
  publisher={Wiley Online Library}
}

@misc{thomas2004health,
  title={Health disparities: the importance of culture and health communication},
  author={Thomas, Stephen B and Fine, Michael J and Ibrahim, Said A},
  journal={American journal of public health},
  volume={94},
  number={12},
  pages={2050--2050},
  year={2004},
  publisher={American Public Health Association}
}

@article{yang2023large,
  title={Large language models in health care: Development, applications, and challenges},
  author={Yang, Rui and Tan, Ting Fang and Lu, Wei and Thirunavukarasu, Arun James and Ting, Daniel Shu Wei and Liu, Nan},
  journal={Health Care Science},
  volume={2},
  number={4},
  pages={255--263},
  year={2023},
  publisher={Wiley Online Library}
}

@article{elmahjub2023artificial,
  title={Artificial Intelligence (AI) in Islamic Ethics: Towards Pluralist Ethical Benchmarking for AI},
  author={Elmahjub, Ezieddin},
  journal={Philosophy \& Technology},
  volume={36},
  number={4},
  pages={73},
  year={2023},
  publisher={Springer}
}

@article{tseng2024two,
  title={Two tales of persona in llms: A survey of role-playing and personalization},
  author={Tseng, Yu-Min and Huang, Yu-Chao and Hsiao, Teng-Yun and Hsu, Yu-Ching and Foo, Jia-Yin and Huang, Chao-Wei and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:2406.01171},
  year={2024}
}

@article{chen2024persona,
  title={From persona to personalization: A survey on role-playing language agents},
  author={Chen, Jiangjie and Wang, Xintao and Xu, Rui and Yuan, Siyu and Zhang, Yikai and Shi, Wei and Xie, Jian and Li, Shuang and Yang, Ruihan and Zhu, Tinghui and others},
  journal={arXiv preprint arXiv:2404.18231},
  year={2024}
}

@inproceedings{
tang2024medagents,
title={MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning},
author={Xiangru Tang and Anni Zou and Zhuosheng Zhang and Ziming Li and Yilun Zhao and Xingyao Zhang and Arman Cohan and Mark Gerstein},
booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
year={2024},
url={https://openreview.net/forum?id=DrCAyzMDmt}
}

@article{
liang2023holistic,
title={Holistic Evaluation of Language Models},
author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Alexander Cosgrove and Christopher D Manning and Christopher Re and Diana Acosta-Navas and Drew Arad Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue WANG and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Andrew Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=iO4LZibEqW},
note={Featured Certification, Expert Certification}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@inproceedings{perez-etal-2022-red,
    title = "Red Teaming Language Models with Language Models",
    author = "Perez, Ethan  and
      Huang, Saffron  and
      Song, Francis  and
      Cai, Trevor  and
      Ring, Roman  and
      Aslanides, John  and
      Glaese, Amelia  and
      McAleese, Nat  and
      Irving, Geoffrey",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.225/",
    doi = "10.18653/v1/2022.emnlp-main.225",
    pages = "3419--3448",
    abstract = "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ({\textquotedblleft}red teaming{\textquotedblright}) using another LM. We evaluate the target LM`s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot`s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users."
}

@inproceedings{
lake2024from,
title={From Distributional to Overton Pluralism: Investigating Large Language Model Alignment},
author={Thom Lake and Eunsol Choi and Greg Durrett},
booktitle={Pluralistic Alignment Workshop at NeurIPS 2024},
year={2024},
url={https://openreview.net/forum?id=roe8GMahZL}
}

@inproceedings{liu-etal-2024-evaluating-moral,
    title = "Evaluating Moral Beliefs across {LLM}s through a Pluralistic Framework",
    author = "Liu, Xuelin  and
      Zhu, Yanfei  and
      Zhu, Shucheng  and
      Liu, Pengyuan  and
      Liu, Ying  and
      Yu, Dong",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.272/",
    doi = "10.18653/v1/2024.findings-emnlp.272",
    pages = "4740--4760",
    abstract = "Proper moral beliefs are fundamental for language models, yet assessing these beliefs poses a significant challenge. This study introduces a novel three-module framework to evaluate the moral beliefs of four prominent large language models. Initially, we constructed a dataset containing 472 moral choice scenarios in Chinese, derived from moral words. The decision-making process of the models in these scenarios reveals their moral principle preferences. By ranking these moral choices, we discern the varying moral beliefs held by different language models. Additionally, through moral debates, we investigate the firmness of these models to their moral choices. Our findings indicate that English language models, namely ChatGPT and Gemini, closely mirror moral decisions of the sample of Chinese university students, demonstrating strong adherence to their choices and a preference for individualistic moral beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their moral choices and debates. This study also uncovers gender bias embedded within the moral beliefs of all examined language models. Our methodology offers an innovative means to assess moral beliefs in both artificial and human intelligence, facilitating a comparison of moral values across different cultures."
}

@article{meister2024benchmarking,
  title={Benchmarking Distributional Alignment of Large Language Models},
  author={Meister, Nicole and Guestrin, Carlos and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2411.05403},
  year={2024}
}

@article{benkler2023assessing,
  title={Assessing llms for moral value pluralism},
  author={Benkler, Noam and Mosaphir, Drisana and Friedman, Scott and Smart, Andrew and Schmer-Galunder, Sonja},
  journal={arXiv preprint arXiv:2312.10075},
  year={2023}
}

@inproceedings{huang2024flames,
  title={Flames: Benchmarking Value Alignment of LLMs in Chinese},
  author={Huang, Kexin and Liu, Xiangyang and Guo, Qianyu and Sun, Tianxiang and Sun, Jiawei and Wang, Yaru and Zhou, Zeyang and Wang, Yixu and Teng, Yan and Qiu, Xipeng and others},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={4551--4591},
  year={2024}
}


@inproceedings{weidinger2022taxonomy,
  title={Taxonomy of risks posed by language models},
  author={Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and others},
  booktitle={Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
  pages={214--229},
  year={2022}
}

@inproceedings{parikh2023exploring,
  title={Exploring Zero and Few-shot Techniques for Intent Classification},
  author={Parikh, Soham and Tiwari, Mitul and Tumbade, Prashil and Vohra, Quaizar},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)},
  pages={744--751},
  year={2023}
}

@inproceedings{carpenter2024assessing,
  title={Assessing student explanations with large language models using fine-tuning and few-shot learning},
  author={Carpenter, Dan and Min, Wookhee and Lee, Seung and Ozogul, Gamze and Zheng, Xiaoying and Lester, James},
  booktitle={Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)},
  pages={403--413},
  year={2024}
}

@article{lu2024llm,
  title={LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play},
  author={Lu, Li-Chun and Chen, Shou-Jen and Pai, Tsung-Min and Yu, Chan-Hung and Lee, Hung-yi and Sun, Shao-Hua},
  journal={arXiv preprint arXiv:2405.06373},
  year={2024}
}

@inproceedings{schuster2021get,
  title={Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence},
  author={Schuster, Tal and Fisch, Adam and Barzilay, Regina},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={624--643},
  year={2021}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6/",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}."
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{xia-etal-2024-aligning,
    title = "Aligning as Debiasing: Causality-Aware Alignment via Reinforcement Learning with Interventional Feedback",
    author = "Xia, Yu  and
      Yu, Tong  and
      He, Zhankui  and
      Zhao, Handong  and
      McAuley, Julian  and
      Li, Shuai",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.262/",
    doi = "10.18653/v1/2024.naacl-long.262",
    pages = "4684--4695",
    abstract = "Large language models (LLMs) often generate biased outputs containing offensive, toxic, or stereotypical text. Existing LLM alignment methods such as reinforcement learning from human feedback (RLHF) alleviate biases primarily based on reward signals from current model outputs without considering the source of biases. In this work, to explore how biases are formed, we revisit LLMs' text generation from a causal perspective. We identify pretraining data and input prompts, which contain semantic correlations of textual phrases, as two confounders between LLMs and model outputs causing biases. Inspired by our causal view, we leverage the reward model in RL alignment as an instrumental variable to perform causal intervention on LLMs. Utilizing the reward difference between an initial LLM and intervened LLM as interventional feedback to guide RL finetuning, we propose Causality-Aware Alignment (CAA) for LLM debiasing. Experiments on two text generation tasks with three different alignment objectives demonstrate the advantages of our method in aligning LLMs to generate less biased and safer outputs."
}

@article{scherrer2024evaluating,
  title={Evaluating the moral beliefs encoded in llms},
  author={Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{thirunavukarasu2023large,
  title={Large language models in medicine},
  author={Thirunavukarasu, Arun James and Ting, Darren Shu Jeng and Elangovan, Kabilan and Gutierrez, Laura and Tan, Ting Fang and Ting, Daniel Shu Wei},
  journal={Nature medicine},
  volume={29},
  number={8},
  pages={1930--1940},
  year={2023},
  publisher={Nature Publishing Group US New York}
}
@article{yang_large_nodate,
  title={Large language models in health care: Development, applications, and challenges},
  author={Yang, Rui and Tan, Ting Fang and Lu, Wei and Thirunavukarasu, Arun James and Ting, Daniel Shu Wei and Liu, Nan},
  journal={Health Care Science},
  volume={2},
  number={4},
  pages={255--263},
  year={2023},
  publisher={Wiley Online Library}
}

@inproceedings{yang_mentallama_2024,
    title = {{MentaLLaMA}: {Interpretable} {Mental} {Health} {Analysis} on {Social} {Media} with {Large} {Language} {Models}},
    shorttitle = {{MentaLLaMA}},
    url = {http://arxiv.org/abs/2309.13567},
    doi = {10.1145/3589334.3648137},
    abstract = {With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks. We use expert-written few-shot prompts and collected labels to prompt ChatGPT and obtain explanations from its responses. To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data. Based on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA, the first open-source LLM series for interpretable mental health analysis with instruction-following capability. We also evaluate the performance of MentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where their correctness for making predictions and the quality of explanations are examined. The results show that MentalLLaMA approaches state-of-the-art discriminative methods in correctness and generates high-quality explanations.},
    urldate = {2024-09-15},
    booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
    author = {Yang, Kailai and Zhang, Tianlin and Kuang, Ziyan and Xie, Qianqian and Huang, Jimin and Ananiadou, Sophia},
    month = may,
    year = {2024},
    note = {arXiv:2309.13567 [cs]},
    keywords = {Computer Science - Computation and Language},
    pages = {4489--4500},
}
@misc{kim_health-llm_2024,
    title = {Health-{LLM}: {Large} {Language} {Models} for {Health} {Prediction} via {Wearable} {Sensor} {Data}},
    shorttitle = {Health-{LLM}},
    url = {http://arxiv.org/abs/2401.06866},
    abstract = {Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and nonlinguistic data is crucial. This paper investigates the capacity of LLMs to make inferences about health based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of 12 state-of-the-art LLMs with prompting and fine-tuning techniques on four public health datasets (PMData, LifeSnaps, GLOBEM and AW FB). Our experiments cover 10 consumer health prediction tasks in mental health, activity, metabolic, and sleep assessment. Our fine-tuned model, HealthAlpaca exhibits comparable performance to much larger models (GPT-3.5, GPT-4 and GeminiPro), achieving the best performance in 8 out of 10 tasks. Ablation studies highlight the effectiveness of context enhancement strategies. Notably, we observe that our context enhancement can yield up to 23.8\% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance.},
    language = {en},
    urldate = {2024-09-13},
    publisher = {arXiv},
    author = {Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
    month = apr,
    year = {2024},
    note = {arXiv:2401.06866 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{west2022symbolic,
  title={Symbolic Knowledge Distillation: from General Language Models to Commonsense Models},
  author={West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena and Jiang, Liwei and Le Bras, Ronan and Lu, Ximing and Welleck, Sean and Choi, Yejin},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4602--4625},
  year={2022}
}

@article{gilardi2023chatgpt,
  title={ChatGPT outperforms crowd workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={30},
  pages={e2305016120},
  year={2023},
  publisher={National Acad Sciences}
}

@inproceedings{loshchilovdecoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations}
}

@article{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.03771},
  year={2020}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}
@misc{wang_comprehensive_2024,
    title = {A {Comprehensive} {Survey} of {LLM} {Alignment} {Techniques}: {RLHF}, {RLAIF}, {PPO}, {DPO} and {More}},
    shorttitle = {A {Comprehensive} {Survey} of {LLM} {Alignment} {Techniques}},
    url = {http://arxiv.org/abs/2407.16216},
    abstract = {With advancements in self-supervised learning, the availability of trillions tokens in a pre-training corpus, instruction fine-tuning, and the development of large Transformers with billions of parameters, large language models (LLMs) are now capable of generating factual and coherent responses to human queries. However, the mixed quality of training data can lead to the generation of undesired responses, presenting a significant challenge. Over the past two years, various methods have been proposed from different perspectives to enhance LLMs, particularly in aligning them with human expectation. Despite these efforts, there has not been a comprehensive survey paper that categorizes and details these approaches. In this work, we aim to address this gap by categorizing these papers into distinct topics and providing detailed explanations of each alignment method, thereby helping readers gain a thorough understanding of the current state of the field.},
    language = {en},
    urldate = {2024-08-23},
    publisher = {arXiv},
    author = {Wang, Zhichao and Bi, Bin and Pentyala, Shiva Kumar and Ramnath, Kiran and Chaudhuri, Sougata and Mehrotra, Shubham and Zixu and Zhu and Mao, Xiang-Bo and Asur, Sitaram and Na and Cheng},
    month = jul,
    year = {2024},
    note = {arXiv:2407.16216 [cs]},
    keywords = {Computer Science - Computation and Language},
}

@article{shen2023large,
  title={Large language model alignment: A survey},
  author={Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  journal={arXiv preprint arXiv:2309.15025},
  year={2023}
}

@article{jiang2024evaluating,
  title={Evaluating and inducing personality in pre-trained language models},
  author={Jiang, Guangyuan and Xu, Manjie and Zhu, Song-Chun and Han, Wenjuan and Zhang, Chi and Zhu, Yixin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wan2024evidence,
  title={What Evidence Do Language Models Find Convincing?},
  author={Wan, Alexander and Wallace, Eric and Klein, Dan},
  journal={arXiv preprint arXiv:2402.11782},
  year={2024}
}

@article{jiang2025investigating,
  title={Investigating machine moral judgement through the Delphi experiment},
  author={Jiang, Liwei and Hwang, Jena D and Bhagavatula, Chandra and Bras, Ronan Le and Liang, Jenny T and Levine, Sydney and Dodge, Jesse and Sakaguchi, Keisuke and Forbes, Maxwell and Hessel, Jack and others},
  journal={Nature Machine Intelligence},
  pages={1--16},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

@article{xu2024debateqa,
  title={Debateqa: Evaluating question answering on debatable knowledge},
  author={Xu, Rongwu and Qi, Xuan and Qi, Zehan and Xu, Wei and Guo, Zhijiang},
  journal={arXiv preprint arXiv:2408.01419},
  year={2024}
}

@article{pistilli2024civics,
  title={CIVICS: Building a Dataset for Examining Culturally-Informed Values in Large Language Models},
  author={Pistilli, Giada and Leidinger, Alina and Jernite, Yacine and Kasirzadeh, Atoosa and Luccioni, Alexandra Sasha and Mitchell, Margaret},
  journal={arXiv preprint arXiv:2405.13974},
  year={2024}
}

@article{banerjee2024navigating,
  title={Navigating the Cultural Kaleidoscope: A Hitchhiker's Guide to Sensitivity in Large Language Models},
  author={Banerjee, Somnath and Layek, Sayan and Shrawgi, Hari and Mandal, Rajarshi and Halder, Avik and Kumar, Shanu and Basu, Sagnik and Agrawal, Parag and Hazra, Rima and Mukherjee, Animesh},
  journal={arXiv preprint arXiv:2410.12880},
  year={2024}
}

@article{yuan2024cultural,
  title={Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette},
  author={Yuan, Jiahao and Di, Zixiang and Zhao, Shangzixin and Naseem, Usman},
  journal={arXiv preprint arXiv:2412.11167},
  year={2024}
}

@article{qwen2.5,
  publtype={informal},
  author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
  title={Qwen2 Technical Report},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2407.10671},
  url={https://doi.org/10.48550/arXiv.2407.10671}
}