\section{Introduction}



The advent of Large Language Models (LLMs) has revolutionised Natural Language Processing (NLP) \citep{zhao2023survey}. While these models, trained on massive datasets, have shown remarkable capabilities, initial versions exhibited concerning issues like toxicity, hallucinations, and biases \citep{liang2023holistic,perez-etal-2022-red,ganguli2022red,weidinger2021ethical,liu2023trustworthy}. Consequently, aligning LLMs with human values has become a central research focus \citep{ouyang2022training,bai2022training,christiano2017deep,gabriel2020artificial}. The impact of alignment is evident in the success of ChatGPT \citep{gpt4}, highlighting its importance for safety, reliability, and broader applicability \citep{shen2023large,liu2023trustworthy}.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95\linewidth]{asset/imgs/v2-vital-overview.pdf}
    \vspace{-0.50cm}
    \caption{A pluralistic alignment example from \ourdataset dataset. More multi-opinionated health scenarios can be found in \refapptab{table:health-scenarios}.}
    \label{fig:vital-alignment-overview}
    \vspace{-0.4cm}
\end{figure}

Despite progress in alignment \citep{wang2023aligning,ouyang2022training,stiennon2020learning,christiano2017deep,rafailov2024direct,schulman2017proximal}, current methods often model \textit{average} human values, neglecting the diversity of preferences across different groups \citep{positionpluralistic,sorensen2024value,feng2024modular}. As AI systems become increasingly prevalent, they must reflect this plurality \citep{positionpluralistic}. Recent work has begun to address pluralistic alignment \citep{bai2022constitutional,gordon2022jury,sorensen2024value} (as illustrated in \reffig{fig:vital-alignment-overview}), recognising the risks of overlooking diverse opinions, particularly in sensitive domains like health where misinformation can have severe consequences \citep{chen2024combating,menz2024current,suarez2021prevalence}.

LLMs are increasingly deployed in open-ended health applications like chatbots \citep{yang2023large,thirunavukarasu2023large}, where their responses to subjective questions are critical. In this domain, LLM outputs can significantly influence user beliefs \citep{santurkar2023whose}, potentially leading to undesirable outcomes such as the promotion of specific viewpoints or homogenization of beliefs \citep{weidinger2021ethical,weidinger2022taxonomy,gabriel2020artificial}. Therefore, evaluating the \textit{representativeness} of health-related LLM responses is crucial before deployment.











Although alignment datasets are available \citep{santurkar2023whose,sorensen2024value}, none focus primarily on health to the best of our knowledge. We argue that existing datasets lack the specificity needed to address diverse cultural and ethical norms within healthcare, which is paramount when incorporating AI into this field. A health-specific dataset will better capture these nuances and improve AI (pluralistic) alignment with varied health beliefs, addressing limitations in current pluralistic approaches. Hence, we build a comprehensive dataset for \textbf{V}al\textbf{I}dating pluralis\textbf{T}ic \textbf{A}lignment for hea\textbf{L}th, \ourdataset, consisting of 13.1K value-laden situations and 5.4K multiple-choice questions across surveys, polls, and moral scenarios focusing on the health domain. We focus on health scenarios \citep{porter2010value}, which present many conflicting opinions from different cultures \citep{thomas2004health,kreuter2004role}, religions \citep{elmahjub2023artificial}, values \citep{klessig1992effect,de2000sensitivity}, and others.

In this paper, we study how alignment techniques, particularly recent pluralistic alignment methods \citep{feng2024modular} in LLMs, for health-specific scenarios. We benchmark these against vanilla LLMs, existing alignment proceduresâ€”prompting, Mixture of Experts (\moe), and Modular Pluralism (\modplural). Our investigation includes eight LLMs (a combination of open-source and black-box models) across three modes of pluralistic alignment. We also experiment with some solutions to improve alignment and discuss the future scope of research. 

The contributions of this work are as follows:
\begin{itemize}
    \item To the best of our knowledge, this work is the first to explore the pluralistic alignment of LLMs, specifically within the health domain.
    \item We construct and introduce a comprehensive benchmark dataset, \ourdataset, concentrating on the health domain for various pluralistic alignment methodologies.
    \item Using this dataset, we benchmark and evaluate the current state-of-the-art (SOTA) pluralistic alignment techniques through detailed analyses and ablation studies. Our findings demonstrate that current leading models exhibit limited performance on \ourdataset. 
    
\end{itemize}



