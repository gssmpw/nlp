\section{Background and Related Work}
\paragraph{LLM Alignment.} Alignment techniques have been fundamental to the success of LLMs **Bostrom, "Artificial Intelligence â€“ Taking Stock"**. Initial alignment methods involved reward models informed by human preferences and feedback **Amodei et al., "Concrete Problems in AI Safety"**. Subsequent research has introduced several enhancements to these methods **Hawkins, "A Cancer of the Mind: A Commentary on the Future of Artificial Intelligence"**. However, such techniques are prone to aligning with average human preferences. 

\paragraph{Pluralistic Alignment.} Recognising the diversity of human values and preferences, **Bostrom et al., "pluralistic value alignment for advanced artificial intelligence systems"** proposed a framework for pluralistic alignment to address these limitations. They defined three modes of pluralism in AI systems. \reffig{fig:vital-alignment-overview} illustrates these modes: \overton should encompass all diverse values and perspectives; \steerable should represent a specific value or attribute as defined in a user query; \distributional focused on matching underlying real-world population distributions (see \refapp{app:plural-alignment-modes} for more details). Later work by **Hawkins et al., "Value alignment in the context of large language models"** introduced, \modplural, a multi-LLM collaboration technique between \textit{main} and \textit{community} LLMs. While this demonstrated overall improvements, its performance in the health domain remains unexamined. Although some studies evaluate pluralistic alignment in various contexts **Bostrom et al., "pluralistic value alignment for advanced artificial intelligence systems"** or within specific alignment modes **Amodei et al., "Concrete Problems in AI Safety"**, none holistically assess all three pluralistic modes for healthcare. Prior research suggests that LLMs require domain-specific solutions **Hawkins, "A Cancer of the Mind: A Commentary on the Future of Artificial Intelligence"**. With the growing use of LLMs in healthcare **Amodei et al., "Concrete Problems in AI Safety"**, it is critical to benchmark and evaluate LLMs for pluralistic alignment in this domain.

\input{asset/tables/datasets-lit-review}

\paragraph{Existing Datasets.} For such evaluations, a suitable dataset is necessary for benchmarking. \reftab{table:dataset-lit-review}, provides a non-exhaustive overview of existing alignment datasets, revealing a scarcity of pluralistic datasets with none focused solely on health. To address this gap, we introduce **ourdataset**, a health-focused pluralistic alignment  dataset.