
\section{Benchmarking}

Using our proposed \ourdataset dataset, we extensively benchmark the current alignment techniques across a suite of models, investigating pluralistic alignment within healthcare.

\subsection{Models} 
We evaluate the alignment techniques on a mix of eight proprietary and open-source models: \llamaSeven, \llamaThirteen, \llamaSeventy \citep{touvron2023llama}, \gemmaSeven \citep{team2024gemma}, \llamaEight \citep{dubey2024llama}, \qwenSeven, \qwenFourteen \citep{qwen2.5}, and \chatgpt \citep{achiam2023gpt}. Furthermore, both unaligned and aligned model variants are also evaluated. We utilise perspective and culture community LLMs from \citet{feng2024modular} for the \moe and \modplural alignment techniques. 

\subsection{Current Alignment Techniques}
\begin{itemize}[noitemsep,leftmargin=*]
\item \textbf{Vanilla:} LLM is prompted directly with no special instruction. This helps us evaluate how the off-the-shelf model fares for pluralistic tasks.
\item \textbf{Prompting:} Pluralism is added to the prompt by adding the instructions (detailed in \refappfig{app:prompting-plural-alignment}).
\item \textbf{\moe:} Here, the main LLM acts as a router and selects the most appropriate community LLM for a given task. Then, the response from this community LLM is provided to the main LLM, using which the main LLM populates the final response \citep{feng2024modular}.
\item \textbf{\modplural:} The main LLM outputs the final response in collaboration with other specialised community LLMs depending on the pluralistic alignment mode \citep{feng2024modular}. For \overton, the community LLM messages are concatenated along with the query and passed to the main LLM, which functions as a multi-document summariser to synthesise a coherent response reflecting diverse viewpoints. For \steerable, the main LLM selects the most relevant community LLM and generates the final response conditioned on the selected community LLM message. For \distributional, multiple response probability distributions are generated for each community LLM and then aggregated using the community priors.
\end{itemize}


\subsection{Metrics}
Our evaluation metrics align with those used in prior research \cite{positionpluralistic,feng2024modular} for each mode. For {\overton}, we use an NLI model \citep{schuster2021get} to calculate the percentage of values covered in various situations. Additionally, we incorporate human evaluations and LLM-as-a-Judge evaluations to assess responses. Specifically, we compare \modplural responses against baseline responses to determine their reflection of pluralistic values. For \steerable, we check whether the final response maintains the steer attribute, quantified by calculating accuracy. For \distributional, we compare expected and actual distributions by measuring the Jensen-Shannon (JS) distance, where a lower value indicates a better alignment.

More experimental setting details can be found in \refapp{app:exp-details}.

\subsection{Results}
\label{sec:results}


\paragraph{\overton Alignment.}







From \reftab{table:overton-vk}, we find that prompting consistently outperforms \modplural across models. Interestingly, simple pluralism achieved through prompting yields superior results for health-related tasks compared to the more complex multi-LLM collaboration, \modplural, as illustrated in \reftab{table:reduced-kidney-response-overton}. Of the models assessed, \gemmaSeven aligns most closely with the \overton framework for health applications, delivering superior performance across all methodologies. It is observed that both prompting and vanilla LLMs surpass \modplural across all eight models for both aligned and unaligned variants. Alarmingly, the coverage disparity between \modplural and the best-performing method reaches a maximum of 55.5\%. It indicates that community LLMs' messages fall short of fully covering the \overton windows within health contexts. Nevertheless, the effectiveness of health-specific \overton alignment remains below that of original \modplural work as detailed in \citet{feng2024modular}. We perform further qualitative analysis and discern missing points in community messages in \refapp{app:qualitative-analysis-overton}.

\input{asset/tables/example-reduced-kidney}

We also evaluate \overton alignment using both human annotators and \texttt{GPT}-as-a-Judge. We sample 100 queries and present a pair of answers for each (one from \modplural and another from one of three methodologies). We calculate the \textcolor{green!80!black!100}{win rate}, \textcolor{blue!100!black!95}{tie rate}, and \textcolor{red!100!black!75}{loss rate} for these answer pairs, as displayed in \reffig{fig:annotation-overton}. We observe a consistent trend where \modplural does not exhibit a clear winning rate over the other baselines. Similar to the NLI coverage results, prompting achieves the highest win rate against \modplural across both evaluation settings, followed by vanilla LLMs.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=.9\linewidth]{asset/imgs/vk-gpt-human-eval-plot.pdf}
    \vspace{-0.5cm}
    \caption{Results of the \emph{\overton} mode in \ourdataset, evaluated using human and GPT-4 assessments with \chatgpt as the main LLM. \modplural is found to have a low win rate against the other alignment techniques. All values are in \%. }
    \label{fig:annotation-overton}
    \vspace{-0.3cm}
\end{figure}

\input{asset/tables/overton-health-VK}

\paragraph{\steerable Alignment.}
\begin{figure*}[!htp]
    \centering
    \includegraphics[width=0.98\linewidth]{asset/imgs/steerable-main-results-plot.pdf}
    \vspace{-0.3cm}
    \caption{Results of LLMs for \steerable mode in \ourdataset in accuracy. All values in \%, with $\uparrow$ values denoting better steerability.}
    \label{fig:steerable-main}
    \vspace{-0.3cm}
\end{figure*}

In \reffig{fig:steerable-main}, we highlight the steerability performance of LLMs. Although results vary, prompting and vanilla techniques are the top 2 performers for all LLMs and alignment methods. As in \overton, the performance of \modplural lags significantly, particularly in value-laden situations (see Appendix~Tables~\ref{table:steerable-vk}~and~\ref{table:steerable-opinionQA} for more).


\paragraph{\distributional Alignment.}
\begin{figure*}[!htp]
    \centering
    \includegraphics[width=0.98\linewidth]{asset/imgs/distributional-main-results-plot.pdf}
    \vspace{-0.3cm}
    \caption{Results of LLMs for \distributional mode in \ourdataset in JS distance, with $\downarrow$ values better denoting higher similarity with the expected distribution.}
    \label{fig:distributional-main}
    \vspace{-0.3cm}
\end{figure*}
\reffig{fig:distributional-main} presents the benchmark results for the \distributional mode in \ourdataset. Compared to results from earlier alignment modes, \modplural performs relatively better and SOTA in some scenarios. Additionally, the performance gap is narrower than observed in other alignment modes. Nonetheless, unaligned vanilla LLMs appear more adeptly aligned distributionally for health-related contexts. Results are comparable for moral and poll multiple-choice questions in the \distributional mode. Detailed results are available in Appendix~Tables~\ref{table:distributional-moralchoice} and~\ref{table:distributional-globalopinionqa}.

\paragraph{Findings.}
Relative to other alignment techniques, prompting provides superior alignment for health-related tasks. \texttt{GPT-4} and human evaluations support this, suggesting that prompting responses are more representative. We attribute this to constant improvements in these LLMs.  LLMs inherently seem to represent population distributions best compared to other complex pluralistic techniques for health. However, considering overall poor performance, it might merely represent baseline capabilities. As discovered in this paper, \modplural also does not excel in model steerability. Additionally, our extensive benchmarking reveals no performance gains with increases in model size. We conclude that \modplural serves as a general solution but faces challenges in domain-specific applications like health, necessitating the development of specialised solutions.

\subsection{Analysis}
\label{sec:analysis}
\subsubsection*{Is \overton evaluation biased by the number of sentences?}
    \input{asset/tables/avg-num-sentences}
The NLI evaluation seems biased towards the number of sentences in the final answer, as illustrated in \reftab{tab:avg-sentences}. Given that the NLI evaluation is conducted on a sentence-by-sentence basis, having a higher number of sentences can increase the likelihood of entailing a value. Furthermore, due to the summarisation in \modplural, we have observed that the main LLM might encapsulate multiple arguments within a single sentence. However, this may result in lower entailment scores. This trend is also evident in the \texttt{GPT}-as-a-Judge evaluations, where there are notably low win rates against prompting; nevertheless, human annotations indicate a higher win rate. We encourage further research into \overton coverage evaluation.


\subsubsection*{Could we leverage modularity and patch health gaps in LLMs?}
In this paper, we primarily focus on perspective community LLMs. However, we did a preliminary analysis using cultural community LLMs since there have been works considering alignment from multi-cultural views. We found performance to be similar with slight improvements; \modplural, \llamaSeven: 15.15 $\rightarrow$ 17.61, \llamaEight: 23.82 $\rightarrow$ 25.11, \gemmaSeven: 22.37 $\rightarrow$ 22.45. 

Similarly, we leveraged health-specialised LLM \citep{yang_mentallama_2024,kim_health-llm_2024} as the main LLMs. 
For a fair comparison, we used \texttt{mental-llama2-7b} and compared against \llamaSeven. We observe no significant performance gains; vanilla: 20.62 $\rightarrow$ 23.48, prompting: 23.69 $\rightarrow$ 24.88, \moe: 19.51 $\rightarrow$ 20.90, \modplural: 15.15 $\rightarrow$ 12.00. This suggests that straightforward patching with specialised LLMs might not be an effective solution for specialised domains like health.


\subsubsection*{How does \distributional pluralism affect the LLM entropy?}
\input{asset/tables/distributional-entropy}
Existing literature \citep{santurkar2023whose,globalopinionQA,positionpluralistic} has found that alignment reduces the entropy of the LLMs of output token distributions. For \distributional alignment, eventual low JS distance could be due to poor alignment and entropy decrease. For the latter, in this subsection, we measure the entropy values of different LLMs for \distributional mode of the \ourdataset in \reftab{table:distributional-entropy}. Expectedly, the aligned variant has lower entropy than the unaligned model for each technique and model. However, unaligned models seem to have entropy similar to vanilla variants. Likewise, the \modplural aligned models show significant improvement compared to other alignment techniques. Interestingly, entropy values are higher for smaller models compared to bigger LLMs. It suggests larger LLMs might be susceptible to shortcuts instead of better-aligned responses. In conclusion, we see a consistent pattern of reduced entropy post-alignment for the health domain.











\subsubsection*{Can specialised community LLMs be replaced by LLM agents?}




Considering the recent success of LLM agents \citep{tseng2024two,chen2024persona,tang2024medagents}, we investigate if on-the-fly role-assigned LLM agents could replace these specialised, fine-tuned community LLMs. 

We first construct a pool of health-specific agents, following \citep{lu2024llm}. Then, we ask \gptFour to select the most relevant six agents (mirroring the number of community LLMs used in the main experiments) for the given situation. These agents' messages substitute the community LLM messages. To ensure a fair comparison, we employ the same backbone model, \mistral, used in community LLMs, as the backbone of these agents. More details about the agents are in \refapp{app:agents-community-LLM}.

In \overton mode, one can conceptualise \modplural as consisting of two LLM tiers: community and main LLMs. The community LLMs aim to encompass various values and perspectives, while the main LLM acts as a summariser. We compute the NLI score for all values in all community LLM messages. A high score is desirable for the main LLM to summarise and cover all the values. For example, culture community LLMs have approximately the same coverage as perspective community LLMs, akin to what we observed with overall \overton performance. Thus, we use these scores for role-assigned community LLMs (\aka LLM Agents) to evaluate: \emph{Do lightweight agents outperform or surpass the current fine-tuned community LLMs used?}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{asset/imgs/diff-num-agents-nli-coverage.pdf}
    \vspace{-0.3cm}
    \caption{Impact of different numbers of agents on \overton NLI coverage. The \textcolor{red}{red} horizontal dashed line denotes the NLI coverage using the original community LLMs.}
    \label{fig:diff-agents-nli-coverage}
    \vspace{-0.3cm}
\end{figure}

We calculate the NLI coverage for varying numbers of agents, as depicted in \reffig{fig:diff-agents-nli-coverage}. Notably, with six agents (matching the number of community LLms), the coverage is similar at 44.16\%, compared to the original 47.84\%. Interestingly, if we use ten agents, the coverage improves to 49.37\%. Given the lightweight nature of these agents, using ten agents or more appears viable. Nonetheless, further research is necessary in this direction. Our findings indicate an overall suboptimal performance, primarily due to the main LLM's bias towards the position of community messages. Additionally, enhancing the summarisation ability of the main LLM to encompass all agent messages is paramount. Finally, there is also scope for improving this collection of agents and role settings. 

All points considered, this direction is worthwhile, given the noted improvement in NLI coverage. We posit that the benefit of agents, which do not necessitate resource-intensive fine-tuning and allow for the dynamic integration of new agents alongside active research, might be an interesting avenue for pluralistic alignment.

