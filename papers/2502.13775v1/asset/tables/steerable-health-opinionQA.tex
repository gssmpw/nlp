\begin{table*}[!htp]\centering
\scriptsize
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{\hspace{5pt}}c@{}}\toprule[1.5pt]
& \textbf{\texttt{LLaMA2}} & \textbf{\texttt{Gemma}} & \textbf{\texttt{Qwen2.5}} & \textbf{\texttt{LLaMA3}} & \textbf{\texttt{LLaMA2}} & \textbf{\texttt{Qwen2.5}} & \textbf{\texttt{LLaMA2}} & \multirow{2}{*}{\textbf{\texttt{ChatGPT}}} \\
& \textbf{\texttt{7B}} & \textbf{\texttt{7B}} & \textbf{\texttt{7B}} & \textbf{\texttt{8B}} & \textbf{\texttt{13B}} & \textbf{\texttt{14B}} & \textbf{\texttt{70B}} & {} \\
\midrule

Unaligned LLM & 37.31 & 42.50 & 56.67 & \underline{56.23} & 37.90 & 43.74 & 37.51 & 40.26 \\
\ \ w/ Prompting & 37.07 & 38.78 & 57.91 & 39.46 & 34.92 & 48.29 & 36.42 & 41.06 \\
\ \ w/ \moe & 39.37 & 43.30 & 48.61 & 44.04 & 40.08 & 47.52 & 41.77 & 44.78 \\
\ \ w/ \modplural & 37.43 & 41.09 & 46.96 & 43.06 & 38.40 & 46.07 & \underline{43.71} & 39.67 \\
\midrule
Aligned LLM & \underline{48.91} & \textbf{57.70} & \textbf{61.13} & \textbf{57.59} & \textbf{47.23} & \textbf{49.85} & \textbf{45.16} & \underline{54.46} \\
\ \ w/ Prompting & \textbf{51.83} & \underline{57.44} & \underline{60.57} & 52.89 & \underline{42.03} & 44.57 & 40.79 & \textbf{58.62} \\
\ \ w/ \moe & 36.36 & 46.72 & 50.32 & 51.95 & 38.08 & 48.47 & 43.39 & 48.52 \\
\ \ w/ \modplural & 41.56 & 47.34 & 48.47 & 46.28 & 40.64 & \underline{49.47} & 42.81 & 48.70 \\
\bottomrule[1.5pt]
\end{tabular}
}
\caption{Results of LLMs for \steerable mode in \ourdataset specifically for opinion questions, in accuracy ($\uparrow$ better). Refer \reffig{fig:steerable-main} for overall \steerable results.}
\label{table:steerable-opinionQA}
\end{table*}
