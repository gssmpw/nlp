\begin{table*}[!htp]\centering
\begin{tabular}{lll}\toprule[1.5pt]
\textbf{Model} & \textbf{Checkpoint} & \textbf{Type} \\
\midrule
\multirow{2}{*}{\llamaSeven \citep{touvron2023llama}} & {\textit{meta-llama/Llama-2-7b-hf}} & {Unaligned} \\
     & {\textit{meta-llama/Llama-2-7b-chat-hf}} & {Aligned} \\
\midrule

\multirow{2}{*}{\gemmaSeven \citep{team2024gemma}} & {\textit{google/gemma-7b}} & {Unaligned} \\
    & {\textit{google/gemma-7b-it}} & {Aligned} \\
\midrule

\multirow{2}{*}{\qwenSeven \citep{qwen2.5}} & {\textit{Qwen/Qwen2.5-7B}} & {Unaligned} \\
    & {\textit{Qwen/Qwen2.5-7B-Instruct}} & {Aligned} \\
\midrule

\multirow{2}{*}{\llamaEight \citep{dubey2024llama}} & {\textit{meta-llama/Meta-Llama-3-8B}} & {Unaligned} \\
     & {\textit{metallama/Meta-Llama-3-8B-Instruct}} & {Aligned} \\
\midrule
\multirow{2}{*}{\llamaThirteen \citep{touvron2023llama}} & {\textit{meta-llama/Llama2-13b-hf}} & {Unaligned} \\
    & {\textit{meta-llama/Llama-2-13b-chat-hf}} & {Aligned} \\
\midrule
\multirow{2}{*}{\qwenFourteen \citep{qwen2.5}} & {\textit{Qwen/Qwen2.5-14B}} & {Unaligned} \\
    & {\textit{Qwen/Qwen2.5-14B-Instruct}} & {Aligned} \\
\midrule
\multirow{2}{*}{\llamaSeventy \citep{touvron2023llama}} & {\textit{meta-llama/Llama-2-70b-hf}} & {Unaligned} \\
    & {\textit{llama/Llama-2-70b-chat-hf}} & {Aligned} \\
\midrule
\multirow{2}{*}{\chatgpt \citep{achiam2023gpt}} & {\textit{davinci-002}} & {Unaligned} \\
    & {\textit{GPT3.5-turbo}} & {Aligned} \\
\midrule
{\mistral \citep{jiang2023mistral}} & {\textit{mistralai/Mistral-7B-Instruct-v0.3}} & {Aligned} \\
\bottomrule[1.5pt]
\end{tabular}
\caption{A list of models used in the experiments. We enlist the HuggingFace \citep{wolf-etal-2020-transformers} model checkpoints for the open-source model and API names for the black-box models; additionally, whether the model is aligned or unaligned. We make assumptions as in \citep{positionpluralistic,feng2024modular} regarding aligned and unaligned versions for OpenAI models.}
\label{table:model-details}
\end{table*}
