\section*{Appendix}

\section{\ourdataset Dataset}
\label{app:dataset-details}

\begin{figure}[!htp]
    \
    \begin{subfigure}{0.99\linewidth}
        \includegraphics[width=\textwidth]{asset/imgs/overall-word-cloud-plot.pdf}
        \caption{\textbf{Overall}}
    \end{subfigure}
    \begin{subfigure}{0.99\linewidth}
        \includegraphics[width=\textwidth]{asset/imgs/vk-word-cloud-plot.pdf}
        \caption{\overton}
    \end{subfigure}
    \begin{subfigure}{0.99\linewidth}
        \includegraphics[width=\textwidth]{asset/imgs/steerable-word-cloud-plot.pdf}
        \caption{\steerable}
    \end{subfigure}
    \begin{subfigure}{0.99\linewidth}
        \includegraphics[width=\textwidth]{asset/imgs/distributional-word-cloud-plot.pdf}
        \caption{\distributional}
    \end{subfigure}
    \caption{Word Cloud visualisation of the \ourdataset dataset (overall and per alignment modes), dominated by health-related terms.}
    \label{fig:vital-world-clouds}
\end{figure}

\input{asset/tables/health-multiple-opinions-example}

\input{asset/tables/more-top-clusters-dataset}

\subsection{\ourdataset Preparation Prompt}
\label{app:filter-prompt}
We construct few-shot classification prompts \citep{sorensen2024value} with a mix of \texttt{Yes} and \texttt{No} examples. These examples are selected by the authors from the original datasets.

\begin{center}
    \begin{tcolorbox}[width=0.98\linewidth,colback=white, title=A few-shot classification using \flan model.]
    {\slshape 
    \textbf{Instruction}\\
    Classify the following situations as either ``Yes'' if they are health scenarios having multiple often conflicting opinions, or ``No'' if they are not.

    \vspace{1em}
    \textbf{Examples:}
    
    \vspace{0.5em}
    Example 1:
    
    Situation: ``Washing hands regularly to prevent infection"
    
    Label: No

    \vspace{0.5em}
    Example 2:
    
    Situation: ``Chestfeeding an infant child."
    
    Label: Yes

    \vspace{0.5em}
    Example 3:
    
    Situation: ``Donating money to Palestinian refugees"
    
    Label: No

    \vspace{0.5em}
    Example 4:
    
    Situation: ``Abstain from vaccination"
    
    Label: Yes

    \vspace{0.5em}
    Example 5:
    
    Situation: ``Making your employees work over the weekend"
    
    Label: No

    \vspace{1em}
    \textbf{Task:}
    
    Situation: ``\{situation\}''
    
    Label:
    }
    \end{tcolorbox}
    \noindent\begin{minipage}{\linewidth}
    \captionof{figure}{An example prompt for filtering health related situations in \ourdataset dataset.}\label{flanT5-filter}
    \vspace{-0.5cm}
    \end{minipage}
\end{center} 



\subsection{Pluralistic Alignment Modes}
\label{app:plural-alignment-modes}

\paragraph{\overton} In this, there might be no single correct answer for a given query, and the LLM should cover all the reasonable values (or \overton\footnote{``\textit{The spectrum of ideas on public policy and social issues considered acceptable or viable by the general public at a given time.}'' \citep{OED-overton-window}} window) from \VK. Outputting a single reasonable answer might lead to biased responses, which might be unsafe in health-based applications. Furthermore, most applications of AI in health are for advice-giving, and current LLM responses are inconsistent but confident and opinionated \citep{krugel2023chatgpt,wang2023ethical}. 

\paragraph{\steerable} This ensures different user demographics, values, or frameworks are respected by the LLM. Otherwise, there is evidence of WEIRD bias in current LLMs for White, American, Wealthy, and Male perspectives \citep{hartmann2023political,santurkar2023whose}. A crucial application of \steerable mode in healthcare is customisation. Patients interacting with a chatbot would benefit from personalised experiences, an approach actively used in mental health therapy \citep{sharma2023human,song2024typing}. We leverage \opinionQA along with \VK to evaluate whether the model steers to represent a specific demographic (from survey questions) and specific value respectively.

\paragraph{\distributional} This has applications in the cases where views of the population should be respected \citep{argyle2023out}. For example, localised country health LLM responds as per the underlying population distributions \citep{wang2023chatgpt,li2023ethics}. The model output token probabilities should be [1, 0] for low ambiguity and [0.5, 0.5] for high ambiguity moral scenarios. Similarly, should correspond to the country in question for the global survey questions.

\begin{center}
    \begin{tcolorbox}[width=0.98\linewidth,colback=white, title=Pluralistic Alignment via Prompting] 
    {\slshape 
    \textbf{\overton:}\\
    \textsf{\textit{Make sure your response reflects diverse values and perspectives for the following instruction.}}
    
    \vspace{1em}
    \textbf{\steerable:}\\
    \textsf{\textit{Make sure your response reflects the specified value, right, or duty in the following instruction.}}
    
    \vspace{1em}
    \textbf{\distributional:}\\
    \textsf{\textit{Make sure your confidence scores reflect the real-world distributions of responses for the following instruction.}}

    }
    \end{tcolorbox}
    \noindent\begin{minipage}{\linewidth}
    \captionof{figure}{Prompts for adding different modes of pluralism to LLM via prompting.}\label{app:prompting-plural-alignment}
    \vspace{-0.5cm}
    \end{minipage}
\end{center} 

\input{asset/tables/more-vital-examples}



















\clearpage
\section{Experiment Details}
\label{app:exp-details}

We make extensive use of the Huggingface Transformers \citep{wolf2020transformers} 
 framework and AdamW \citep{loshchilovdecoupled} for model development. 
The exact model and its checkpoint details are documented in \reftab{table:model-details}.
Likewise, we use scikitlearn \citep{pedregosa2011scikit} for clustering algorithms and other utility calculations. We perform all the experiments on a single A100 GPU with CUDA 11.7 and pytorch 2.1.2. Finally, we implement alignment techniques and other experiments following their default configurations and settings. Due to the computation limit, we run \llamaSeventy for 20\% sampled datasets. Most experiments use six perspective community LLMs covering left, centre, and right-leaning for news and social media. There are culture community LLMs focused on North America, South America, Asia, Europe, and Oceania continents. We mean perspective community LLMs whenever referring to community LLMs unless stated otherwise.

\input{asset/tables/model-details}








\section{Further Analysis}


\input{asset/tables/steerable-health-VK}

\input{asset/tables/steerable-health-opinionQA}

\input{asset/tables/distributional-health-moralchoice}

\input{asset/tables/distributional-health-globalopinionQA}

\paragraph{Qualitative Analysis}
\label{app:qualitative-analysis-overton}

\input{asset/tables/example-kidney}
\input{asset/tables/example-drugs}

We conduct further analysis for LLM responses by manually examining some of the different responses for the same query in Tables~\ref{table:kidney-response-overton}~and~\ref{table:narcotics-response-overton}. For the situation, ``donating a kidney when i have only one'' (\reftab{table:kidney-response-overton}), we note that prompting response has more coverage and \modplural response is missing autonomy value and religious beliefs (highlighted in pink). One must note that this is not the case where the main LLM ignored these points while summarising. We checked that none of the community LLMs cover these points. Hence, we performed more extensive experiments on NLI coverage of the community LLMs in \refsec{sec:analysis}. Similarly, in \reftab{table:narcotics-response-overton}, for ``Use narcotic drugs for heavy pain.'', cultural perspective is missed as the use of narcotics would be unacceptable in some religions and cultures. In summary, the qualitative analysis seconds the quantitative findings from \refsec{sec:results} that \modplural is subpar for the health domain and prompting is more pluralistic. It provides further insights into the issue which might be due to the poor community LLM message coverage.










\section{LLM Agents as Community LLMs}
\label{app:agents-community-LLM}
We create a pool of 60 health-related agents (role assigned \mistral) along the axis of communities, cultures, demographics, ideologies, perspectives, and religions. We adapt from the prompts from \citep{lu2024llm}, a few such agents and their descriptions are mentioned in \reffig{fig:few-agents}. Due to brevity, we will release the complete agent configurations and prompts in the repository. For now, we use \gptFour for selecting a few agents for a given situation; in future, we could train a specialised lightweight router for selecting the agents.

\begin{figure}[!htp]
    \centering
    \begin{minipage}{0.95\linewidth}
    \begin{lstlisting}[language=json]
[
    {
        "agent_role":"Community Health Worker",
        "agent_speciality":"Community Engagement and Cultural Competency",
        "agent_role_prompt":"Acts as a vital link between healthcare systems and communities, helping navigate cultural nuances and build trust among patients and healthcare providers."
      },
      {
        "agent_role":"Patient/Individual",
        "agent_speciality":"Personal Health Experience",
        "agent_role_prompt":"Provides firsthand insights into symptoms, health concerns, and personal preferences that influence healthcare decisions."
      },
      {
        "agent_role":"Environmental Health Activist",
        "agent_speciality":"Sustainability and Public Health",
        "agent_role_prompt":"Highlights the links between environmental sustainability and public health, advocating for policies that protect natural and human health."
      },
      ...
    ]
    \end{lstlisting}
    \end{minipage}
    \caption{Few examples of LLM agents used in place of community LLMs.}
    \label{fig:few-agents}
\end{figure}


