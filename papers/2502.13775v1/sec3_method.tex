
\section{\ourdataset Dataset}










We present the first health-focused benchmark dataset specifically tailored for three modes of pluralistic alignment. It includes 13.1K value-laden situations and 5.4K multiple-choice questions (see \reftab{table:vital-dataset-stats}). We undertake a meticulous and thorough benchmark construction process, including data collection, filtering, expert review, and analysis. 

\input{asset/tables/vital-stats}

\subsection{Dataset Construction}





We begin by constructing a large-scale question bank, sourcing multiple-choice questions from a variety of survey and moral datasets \citep{liu-etal-2024-evaluating-moral,globalopinionQA,santurkar2023whose,sorensen2024value}. 
We concentrate on collecting diverse health scenarios—some listed in \refapptab{table:health-scenarios}—characterised by their multiple perspectives and subjectivity, where we anticipate the most cross-value and perspective disagreement. Ultimately, we curate \ourdataset by filtering out questions and scenarios unrelated to health, lack diverse multiple opinions, or do not require action. It is accomplished through few-shot classification using the \flan model (see prompts in \refapp{app:filter-prompt}) \citep{carpenter2024assessing,parikh2023exploring}. 

We transform these multiple-choice questions into benchmarks for evaluating pluralistic alignment in LLMs. Demographic information from surveys, alongside situational values, is used to investigate the \textit{steerability} of LLMs. Similarly, country information from polls is leveraged to construct the underlying real-world distributions needed to evaluate the \textit{distributionality} of the models. The ambiguous nature of moral scenarios provides an ideal basis for comparing the LLM’s response distributions across various perspectives. 

While previous benchmarks and datasets have primarily focused on QA, we broaden the scope and enhance complexity by incorporating value-laden situations. We assess the \textit{overtness} of models by ensuring they cover all human values. This blend of general text and questions within \ourdataset makes it a challenging and ideal benchmark for pluralistic alignment. Further details regarding the construction of \ourdataset are available in \refapp{app:dataset-details}.


\subsection{Dataset Analysis}


\input{asset/tables/ngram-dataset-analysis}
\input{asset/tables/vital-examples}


\paragraph{Lexical Analysis.}

We investigate lexical diversity within \ourdataset, aiming for diversity in both questions and situations to be diverse. This diversity is assessed by calculating the number and percentage of unique samples and n-grams as detailed in \reftab{tab:ngram-analysis}. The dataset exhibits high lexical diversity across both overall and alignment modes. Additionally, we visualise the entire dataset in \refappfig{fig:vital-world-clouds}. Our analysis reveals that the curated dataset is predominantly composed of health-related terms.



\paragraph{Topic Analysis.}
We conduct clustering on the samples to identify the range of themes captured. By employing agglomerative clustering, we summarise the samples within each cluster using \gptFour. \reftab{tab:top-clusters-dataset} presents summaries of the top clusters containing the most samples. These summaries illustrate a variety of health topics. We observe that clusters encompass a combination of situations and multiple-choice questions. Conflicting samples within the same cluster and theme further underscore the diversity and complexity of \ourdataset as a health pluralistic alignment benchmark.

\input{asset/tables/top-clusters-dataset}

\paragraph{Relevance Analysis.}
Despite LLMs demonstrating annotation performance comparable to human workers \citep{gilardi2023chatgpt}, we cautiously undertake human validation. In this context, we carry out a study where 10\% of \ourdataset is labelled by humans to verify their health-related relevance. Human annotators identified samples in \ourdataset as health-related 80\% of the time, with moderate agreement (Fleiss' Kappa: 0.49). The relevance of data in specific alignment modes within \ourdataset are similar: \overton at 80.5\%, \steerable at 75.6\%, and \distributional at 83.32\%. Previous studies indicate that potential noise introduced by LLMs as annotators is mitigated by their ability for large-scale synthesis \citep{west2022symbolic}. Moreover, the multi-opinionated scenarios addressed pose challenges for human interpretation. Several samples initially marked as non-health-related—such as instances like \textit{``Smoking weed as an adult''} or \textit{``Spanking my children''}—could be argued as health-related due to their potential indirect implications.


    




