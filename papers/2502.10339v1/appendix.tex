\newpage
\cleardoublepage
\appendix

\section{Appendix}
\subsection{Bounding $\|Bx\|$}
\label{subsec:bound}
Let $r^A$ and $r^B$ be the original ranks of $A$ and $B$, $B=\sum_{i=1}^{r^B}\sigma_i^B u_i^B (v_i^B)^T$, $x=\sum_{j=1}^{r^A} \alpha_j v_j^A$, and $\{v_i^A\}_{i=1}^{r^A}$ and $\{v_i^B\}_{i=1}^{r^B}$ are orthonormal vectors, then we have
\begin{align}
    \|Bx\|&=\|\sum_i\sigma_i^B u_i^B (v_i^B)^T \sum_j \alpha_j v_j^A\| \nonumber\\
    &\leq \sum_i \|u_i^B\| \cdot |\sum_j \sigma_i^B\alpha_j  (v_i^B)^T   v_j^A| \nonumber\\
    &\leq \sum_i \beta\cdot |\sum_j (v_i^B)^T   v_j^A| \nonumber\\
    &\leq \sum_{i=1}^{r^B} \beta \sqrt{r^A} \left(\sum_{j=1}^{r^A} \left(\left(v_i^B\right)^T   v_j^A\right)^2\right)^{1/2} \label{ieq:cauthy}\\
    &= \sum_{i=1}^{r^B} \beta \sqrt{r^A} \left(\sum_{j=1}^{r^A} \left<v_i^B,   v_j^A\right>^2\right)^{1/2},\label{eq:first}
    % \\
    % &\leq r^B \beta \sqrt{r^A},
\end{align}
where $\beta=\max_{i,j}|\sigma_i^B\alpha_j|$, and inequality~\eqref{ieq:cauthy} uses Cauchy-Schwarz inequality. Then we show that
\begin{align}
    1 &= \|v_i^B\|^2 \nonumber\\
    &=\|\sum_{j=1}^{r^A}\left<v_i^B,   v_j^A\right> v_j^A + v_i^{B\perp A}\|^2 \label{eq:decompose}\\
    &=\sum_{j=1}^{r^A} \|\left<v_i^B,   v_j^A\right> v_j^A\|^2 + \|v_i^{B\perp A}\|^2 \label{eq:ptg}\\
    &= \sum_{j=1}^{r^A} \left<v_i^B,   v_j^A\right>^2 + \|v_i^{B\perp A}\|^2 \nonumber\\
    &\geq \sum_{j=1}^{r^A} \left<v_i^B,   v_j^A\right>^2, \label{ieq:second}
\end{align}
where equation~\eqref{eq:decompose} expresses $v_i^B$ by $\{v_i^A\}_{i=1}^{r^A}$, and $v_i^{B\perp A}$ denotes the part of $v_i^B$ that is orthogonal to the span of $\{v_i^A\}_{i=1}^{r^A}$. Equation~\eqref{eq:ptg} follows Pythagorean identity since $v_1^A, v_2^A,\ldots,v_{r_A}^A, v_i^{B\perp A}$ are pairwise-orthogonal vectors. Finally, with Equation~\eqref{eq:first} and~\eqref{ieq:second}, we have
\[
\|Bx\|\leq r^B\beta \sqrt{r^A} .
\]

\newpage
\subsection{Algorithm}

\begin{algorithm}[h]
\caption{Model merging by STAR}\label{alg:STAR}
\begin{algorithmic}
\State \textbf{Input:} \(\bm{\theta}_{\text{pre}}\), \(\{\bm{\theta}_{\text{ft}, i}\}_{i=1}^{T}\), \(\eta\) 
\State \textbf{Output:} \(\bm{\theta}_{\text{merged}}\)
\For{$i = 1$ \textbf{to} $T$} 
    \State $\triangleright$ Get task vector
    \State $\bm{\delta}_{i} \gets \bm{\theta}_{\text{ft}, i} - \bm{\theta}_{\text{pre}}$
    
    \For{$l = 1$ \textbf{to} $L$}
        \State $\triangleright$ SVD
        \State $\bm{u}_{k}, \sigma_{k}, \bm{v}_{k} \gets \textbf{SVD}(\bm{\delta}_{i}^{l})$
        \State $r \gets \textbf{rank\_keep}(\bm{\sigma}, \eta, p)$\label{derived_r}
        \State  $\triangleright$ Rescale Singular Values
        \For{$k = 1$ \textbf{to} $r$}
        \State $\sigma_k^{'} \gets \frac{\|\bm{\sigma}\|_{1}}{\|\bm{\sigma}_{1:r}\|_{1}} \cdot \sigma_k$
        \EndFor
        \State  $\triangleright$ Reconstruct
        \State $\bm{\delta}_{i, \text{out}} \gets \sum_{k=1}^{r} \bm{u}_{k} \sigma_{k}^{'} \bm{v}_{k}$
    \EndFor
\EndFor
\State  $\triangleright$  Simple Averaging
\State $\bm{\delta}_{\text{merged}} \gets \frac{1}{T} \sum_{i=1}^{T} \bm{\delta}_{i, \text{out}}$
\State \textbf{return} $\bm{\theta}_{\text{merged}} \gets \bm{\theta}_{\text{pre}} + \bm{\delta}_{\text{merged}}$
\end{algorithmic}
\end{algorithm}

% \subsection{Ablation studies on retraining the nuclear norm}






% \subsection{Sensitivity Analysis}
% \label{subsec:sensitivity}
% In this section, we allow STAR to choose $\eta$ from $\{10, 20, \dots, 70\}$ and TIES to choose $K$ from $K\in\{1, 5, 10, 20, \dots, 70\}$.
% In Sec.~\ref{subsubsec:oneshot_star_is_better}, we will show that grid search TIES's hyperparameter $K$ helps to improve the performance, but it still performs worse than STAR without grid search over $\eta$. In Sec.~\ref{subsubsec:star_insensitive}, we will further show that STAR is not sensitive to $\eta$, sparing users' need to fine-tune $\eta$ during the deployment.
\subsection{Discussion on EMR-Merging}\label{discuss_EMR_Merging}
EMR-Merging~\cite{huang2024emr} is a recent data-free model merging method that reports outstanding performance with minimal additional storage. It first constructs a unified merged task vector, \( \tau_{\text{uni}} \), which retains the maximum amplitude and sign information shared by all task vectors (\( \tau_i \)). Then, task-specific masks (\( M_i \)) and rescalers (\( \lambda_i \)) are derived based on sign agreement and parameter magnitude alignment between \( \tau_i \) and \( \tau_{\text{uni}} \). Finally, during inference, EMR-Merging dynamically adapts \( \tau_{\text{uni}} \) for each task using 
\[
\hat{W_t} = W_{\text{pre}} + \hat{\tau}_t,
\]
where 
\[
\hat{\tau}_t = \lambda_t \cdot M_t \odot \tau_{\text{uni}}.
\]

In other words, EMR-Merging adjusts model weights at run-time, whereas our approach, along with the included baselines (i.e., TIES, MetaGPT, and TALL-masks), operates statically. This makes direct comparison infeasible; therefore, we do not include EMR-Merging as one of the baselines.

\subsection{Discussion on DARE}\label{discuss_DARE}

STAR follows a similar protocol to DARE~\cite{yu2024language}, as both methods involve two steps: dropping certain components and rescaling. However, there are key differences between them.

On one hand, DARE randomly drops entries of task vectors in parameter space, following:
\[
\mathbf{m}^t \sim \text{Bernoulli}(p),
\]
\[
\tilde{\delta}^t = (1 - \mathbf{m}^t) \odot \delta^t.
\]
In contrast, STAR selectively removes redundant dimensions in spectral space.

On the other hand, DARE's rescaling scheme is based on:
\[
\hat{\delta}^t = \frac{\tilde{\delta}^t}{1 - p},
\]
aiming at approximating the original embeddings, while STAR's rescaling focus on restore the spectral-truncated weight matrices to their original scale.

Unlike STAR, which can function as a standalone model merging method, DARE primarily serves as a plug-in to enhance other merging techniques. For comparison, we follow DARE's protocol and report the results of DARE+TA (Task Arithmetic) and DARE+TIES in Table~\ref{tab:dare_comparison}. Specifically, we vary DARE's drop rate \( p \) from \{0.1, 0.2, \dots, 0.9\}, and the results suggest that even when DARE is applied on top of TA and TIES, STAR still achieves superior performance.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.3} % 增加行距
\resizebox{\linewidth}{!}{ % 自動縮放表格以適應雙欄寬度
\LARGE
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Hyperparameter} & \textbf{Avg. Normalized} \\
\midrule
TA & \( \alpha = 0.125 \) & 91.67 \\
TA+\textbf{DARE} & \( \alpha = 0.125, p^* = 0.7 \) & 91.78 \\
TIES & \( k = 20 \) & 93.83 \\
TIES+\textbf{DARE} & \( k = 20, p^* = 0.2 \) & 93.71 \\
STAR & \( \eta = 40 \) & \textbf{95.30} \\
\bottomrule
\end{tabular}
}
\caption{Results from merging eight fine-tuned Flan-T5-large models. TA is fixed with a scaling factor of \( \alpha = 0.125 \), and TIES is set with \( k = 20 \), using the best-performing DARE drop rate (\( p^* \)).}
\label{tab:dare_comparison}
\end{table}



\subsection{One-shot STAR performs even better than grid-search TIES}
\label{subsec:oneshot_star_is_better}
\begin{figure}[h]
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=0.98\linewidth]{image/main_results/tuning/flan_t5_base_tune.png} 
\caption{Flan-T5-base}
\label{fig:enter-label}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=0.98\linewidth]{image/main_results/tuning/flan_t5_large_tune.png}
\caption{Flan-T5-large}
\end{subfigure}
\caption{The model merging results on Flan-T5-base and Flan-T5-large with both pre-determined hyperparameter (one-shot, solid lines) and grid-searched hyperparameter (dashed Lines). The performance of each sampled combinations is represented by shaded dots.
% \IKB{1. Change the marker to distinguish STAR and TIES; 2. Use the same style for one-shot results as in Fig 3, and change grid search lines to dash lines}
}
\label{fig:main_grid}
\end{figure}

Recall that in Fig.~\ref{fig:main_datafree}, we have shown the one-shot performance with pre-determined $K=20$ and $\eta=40$ for TIES and STAR, respectively. In Fig.~\ref{fig:main_grid}, we further show their best possible results over the grids we searched for. Specifically, from Fig.~\ref{fig:main_grid}, we see that the grid search does not improve the performance much on Flan-T5-base for both TIES and STAR. Even after performing grid search for TIES, it still fails to surpass the one-shot performance of STAR, further emphasizing the practicality of our method in real-world applications. On Flan-T5-large, the gain from grid search on TIES becomes obvious especially when we are merging more models. With STAR, grid search over $\eta$ also helps but the results are relatively consistent.

% \newpage


% To take a closer look at the impact of $\eta$ to STAR's performance, we calculate the average performance of STAR with $\eta\in\{10, 20, \dots, 70\}$ and include the standard deviation in Fig.~\ref{fig:eta_analysis1}. On both Flan-T5-base and Flan-T5-large, the standard deviation on STAR's average normalized performance is small, implying STAR is not sensitive to $\eta$ and hence does not require active hyperparameter tuning to perform well.



% \subsection{Single Task Generalization Ability}
% % like model breadcrumbs, do single task performance gain by merging with others!
% % Would be easy(? just pick  out the combination that have avg normalized perf. > 1

\subsection{Details about the fine-tuned models considered in the experiments}\label{app:detail}
\label{subsec:experiment_details}
For Flan-T5-base, we selected 7 LoRA-16 finetuned models from FusionBench\footnote{\url{https://huggingface.co/collections/tanganke}}~\cite{tang2024fusionbench}, which is a benchmark targeted for model merging (excluding only CoLA as it tends to output the same answer), and finetuned 5 additional models ourselves on the Finance, IMDB, AG News, HellaSwag, and BoolQ datasets. We applied the same rank (16) and scaling factor (32) as in FusionBench, with the learning rate and number of epochs tuned on the validation set. Following a similar approach, we selected 7 Flan-T5-large models from FusionBench and finetuned 6 additional models ourselves, including Finance, IMDB, AG News, HellaSwag, and BoolQ, and PIQA.

For Mistral-Instruct, 20 models are selected from the Lots of LoRA collection~\footnote{\url{https://huggingface.co/Lots-of-LoRAs}}~\cite{bruel2024compress}, which encompasses up to 500 diverse task types, making it an ideal environment for evaluating model merging methods.
% , with the selection criteria being ease of evaluation while covering a diverse range of types. 
The considered task IDs are: 039, 190, 247, 280, 290, 298, 330, 357, 363, 391, 513, 564, 587, 834, 846, 1198, 1341, 1391, 1448, 1605.
% \TP{specify the datasets on which these models were trained on. Also include the model ids for both lots of loras and this in the appendix so that others can reproduce it.}\AR{Noted}


% \subsection{An example of the automatic rank and scale determination in STAR}
% \label{subsec:automatic_rank}
% \begin{figure*}[b!]
%     \centering\hspace{8cm}
%     \includegraphics[width=0.90\linewidth]{image/adaptively/piqa_flan_large.png}
%     \caption{An example of the rank automatically determined by STAR using PIQA's task vector on Flan-T5-large. }
%     \label{fig:rank_viz}
% % \end{figure*}

% % \begin{figure*}
%     \centering\hspace{8cm}
%     \includegraphics[width=0.90\linewidth]{image/adaptively/piqa_flan_large_rescale_factor_3.png}
%     \caption{An example of the rescaling factor automatically determined by STAR using PIQA's task vector on Flan-T5-large. 
%     % \textbf{Rescale factor} when \(\eta\) is set to 40\% for PIQA's task vector on Flan-T5-large. Each matrix requires different rescaling factors to restore the nuclear norm, even though the threshold \(\eta\) is applied uniformly. This demonstrates how STAR leverages heterogeneity in sigular values across matrice to accomplish weighted autonomous weighting.
% }

%     \label{fig:rescale_viz}
% \end{figure*}
% By definition, STAR can adaptively determine the number of dimensions needed (rank) to express each weight matrices as well as the scaling factor for each matrices. We give an example of the ranks and scaling factors using PIQA's task vector on Flan-T5-large in Fig.~\ref{fig:rank_viz} and Fig.~\ref{fig:rescale_viz}.


