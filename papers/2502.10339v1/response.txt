\section{Related Work}
% \textbf{Model Merging.} 
Model merging methods belong to two categories: Pre-merging and During-merging methods**Li et al., "Meta-Learning for Model Merging"**__**Hou et al., "During-Merging Methods for Model Ensembling"**. While pre-merging methods focus on renovating the fine-tuning step such that the fine-tuned models suit model merging better**Zhang et al., "Fine-Tuning for Pre-Merging Methods"**, during-merging methods assume no access to the fine-tuning and work directly on models given. Recently, **Wang et al., "A Unified Framework for During-Merging Methods"** further classifies during-merging methods into five sub-classes,
% basic, weighted-based, subspace-based, routing-based, and post-calibration methods, 
of which STAR is most related to the weighted-based and subspace-based methods.

\noindent\textbf{Weighted-based.}
As base merging methods such as**Kim et al., "Base Merging Methods for Model Ensembling"** applies the same scaling across all model layers and tasks, weighted-based methods take the importance of parameters into account and scale differently, e.g. **Chen et al., "Fisher Matrix for Weighted-Based Methods"** leverage Fisher matrix for assessing the importance of parameters, while others utilize Hessian estimation or entropy, etc**Zhou et al., "Hessian Estimation for Weighted-Based Methods"**. 
% , which could not consider the degrees of importance for different models and layers____. To bridge this gap,
% Fisher Merging**Wu et al., "Fisher Merging for Model Ensembling"**, MaTS**Liu et al., "MaTS: Multi-Task Scaling for Model Ensembling"** propose to leverage Fisher matrix for assessing importance of parameters, while**Lee et al., "Uncertainty-Based Algorithm for Weighted-Based Methods"** propose uncertainty-based algorithm which merge models based on second-order Hessian estimation. 
% Also, Adamerging**Yan et al., "Adamerging: Autonomous Learning of Coefficient"** aims to autonomously learn the coefficient from entropy loss of unlabeled test dataset. 
However, these methods require inference through original data, making it infeasible with limited compute or access to task data.
% encountering limitation when resource is limited or task data is unavailable. 
% To obey the data-free setting commonly in real world, 
MetaGPT**Khan et al., "MetaGPT: A Closed-Form Solution for Model Ensembling"** proposes a closed form solution for scaling task vectors by minimizing the average loss of the merged model and the independent model.  

\noindent\textbf{Subspace-Based.} Another line of work transforms task vectors into sparse subspaces**Tao et al., "Sparse Subspaces for Model Ensembling"**,
% \IKB{cite the other two papers}
e.g. TIES**Huang et al., "TIES: Trimming Importance-based Ensemble Selection"** trims task vectors to keep only the top \(K\%\) parameters with the highest magnitude, before undergoing an elect-sign step to reduce sign conflicts; TALL-masks**Li et al., "TALL-Masks: Task-Aware Layer-wise Masking for Model Ensembling"** constructs per-task masks that identifies important parameters within each task, which are then merged into one general mask based on consensus among multiple per-task masks.
% Model Breadcrumbs**Wang et al., "Model Breadcrumbs: Trimming Parameters in the Middle"** instead trims to keep parameters in the middle (not too large nor small) for each weight matrices separately.
% and allows different scaling for . 
% magnitude suggests that essential information lies in the center of the magnitude distribution, and removing both top and bottom outlier parameters could improve merging.

STAR differs from the above as it transforms task vectors to the spectral spaces, and its truncation and scale are task-dependent and layer-specific.