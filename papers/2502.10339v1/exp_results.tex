\subsection{Performance Comparison}\label{exp_results}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{image/optimal_eta.png}
    \caption{The mean and standard deviation of the optimal \( \eta \), which yields the best merged model performance, decrease as the number of merged models increases.} 
    \vspace{-1em}
    \label{fig:optimal_eta}
\end{figure}
We compare STAR to other data-free approaches, including TIES~\cite{yadav2024ties}, TALL-masks~\cite{wang2024localizing}, which we apply on top of Task Arithmetic~\cite{ilharco2022editing}, i.e., Consensus Task Arithmetic (without tuning the data-dependent hyperparameter \( \lambda_{t} \)), and MetaGPT~\cite{zhou2024metagpt}. Due to the page limit, we defer the discussion around EMR-Merging~~\cite{huang2024emr} and DARE~\cite{yu2024language} to appendix Sec.~\ref{discuss_EMR_Merging} and Sec.~\ref{discuss_DARE}.

The results on Flan-T5-large and Mistral-7B-Instruct are shown in Fig.~\ref{fig:main_datafree} and Flan-T5-base in Fig.~\ref{fig:numerial_evidence}. We note that similar trends as Fig.~\ref{fig:numerial_evidence} can be seen in Fig.~\ref{fig:main_datafree} where the averaged normalized performance decreases as the number of models merged increases, with STAR's performance decay being the slowest across models. On Flan-T5-base, MetaGPT tends to fail quickly, echoing with the findings in~\cite{zhou2024metagpt} - 
% As discussed by~\citet{zhou2024metagpt}, 
MetaGPT may face limitations when merging models of smaller sizes (e.g. Flan-T5-base has only 0.25B parameters) due to its reliance on NTK linearization. 
% We observe similar results here, where MetaGPT tends to fail quickly on the Flan-series models. STAR, however, significantly outperforms the baselines across different model size settings. 
% Up to 20 models were merged in this case, with MetaGPT and STAR show potential to merge even more models without knowledge collapse.
To examine the full potential of each algorithm, we also perform grid search for TIES and STAR and report the best result in Appendix Sec.~\ref{subsec:oneshot_star_is_better}.