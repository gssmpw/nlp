
\section{Background and Related Work}
\label{sec:background}

\subsection{Notations and Problem Definition}\label{subsec:problem_def}

% \textbf{Task Vectors.} 
We denote the weight matrices of a pretrained LM by \(\bm{\theta}_{\text{pre}}^{l}\) for \(l = \{ 1, \ldots, L\} \), where \(L\) is the total number of such matrices. Let $\bm{\theta}_{\text{pre}}$ denote the concatenation of all vectorized weight matrices and $\bm{\theta}_{\text{ft}}$ denote the updated model parameters after fine-tuning on task $\mathcal{T}$. 
A task vector \(\bm{\delta}\) is then defined as the difference between \(\bm{\theta}_{\text{ft}}\) and \(\bm{\theta}_{\text{pre}}\), i.e., \(\bm{\delta} = \bm{\theta}_{\text{ft}} - \bm{\theta}_{\text{pre}}\)~\cite{ilharco2022editing}.
% \noindent\textbf{Model Merging.} 
Given $T$ fine-tuned models, model merging fuses $\{\bm{\delta}_1,\ldots,\bm{\delta}_T\}$ into a merged $\bm{\delta}_{\text{merged}}$ such that $\bm{\theta}_{\text{pre}}+\bm{\delta}_{\text{merged}}$ still performs well on $T$ tasks simultaneously.




\input{related_work}