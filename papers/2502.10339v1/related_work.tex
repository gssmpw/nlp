

\subsection{Related Work}

% \textbf{Model Merging.} 
Model merging methods belong to two categories: Pre-merging and During-merging methods~\cite{yang2024model}. While pre-merging methods focus on renovating the fine-tuning step such that the fine-tuned models suit model merging better~\cite{ortiz2024task,imfeld2023transformer,guerrero2022re}, during-merging methods assume no access to the fine-tuning and work directly on models given. Recently, \citet{yang2024model} further classifies during-merging methods into five sub-classes,
% basic, weighted-based, subspace-based, routing-based, and post-calibration methods, 
of which STAR is most related to the weighted-based and subspace-based methods.

\noindent\textbf{Weighted-based.}
As base merging methods such as~\citet{ilharco2022editing} applies the same scaling across all model layers and tasks, weighted-based methods take the importance of parameters into account and scale differently, e.g. \citet{matena2022merging,tam2024merging} leverage Fisher matrix for assessing the importance of parameters, while others utilize Hessian estimation or entropy, etc~\cite{daheim2023model,yang2023adamerging}. 
% , which could not consider the degrees of importance for different models and layers~\cite{yang2024model}. To bridge this gap,
% Fisher Merging~\cite{matena2022merging}, MaTS~\cite{tam2024merging} propose to leverage Fisher matrix for assessing importance of parameters, while~\citet{daheim2023model} propose uncertainty-based algorithm which merge models based on second-order Hessian estimation. 
% Also, Adamerging~\cite{yang2023adamerging} aims to autonomously learn the coefficient from entropy loss of unlabeled test dataset. 
However, these methods require inference through original data, making it infeasible with limited compute or access to task data.
% encountering limitation when resource is limited or task data is unavailable. 
% To obey the data-free setting commonly in real world, 
MetaGPT~\cite{zhou2024metagpt} proposes a closed form solution for scaling task vectors by minimizing the average loss of the merged model and the independent model.  

\noindent\textbf{Subspace-Based.} Another line of work transforms task vectors into sparse subspaces~\cite{davari2023model,yadav2024ties,wang2024localizing, huang2024emr},
% \IKB{cite the other two papers}
e.g. TIES~\cite{yadav2024ties} trims task vectors to keep only the top \(K\%\) parameters with the highest magnitude, before undergoing an elect-sign step to reduce sign conflicts; TALL-masks~\cite{wang2024localizing} constructs per-task masks that identifies important parameters within each task, which are then merged into one general mask based on consensus among multiple per-task masks.
% Model Breadcrumbs~\cite{davari2023model} instead trims to keep parameters in the middle (not too large nor small) for each weight matrices separately.
% and allows different scaling for . 
% magnitude suggests that essential information lies in the center of the magnitude distribution, and removing both top and bottom outlier parameters could improve merging.

STAR differs from the above as it transforms task vectors to the spectral spaces, and its truncation and scale are task-dependent and layer-specific. 