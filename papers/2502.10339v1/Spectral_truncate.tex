
\section{Methodology}\label{sec:spectral_truncate}
Sec. \ref{subsec:truncate} provides the rationale behind performing truncations in the spectral space. Sec. \ref{subsec:rescale} defines the rescaling step for restoring the nuclear norm. Sec. \ref{subsec:STAR} gives the complete STAR algorithm.


\subsection{Spectral Truncation}
\label{subsec:truncate}
Let $\mathcal{T}_1,\mathcal{T}_2$ be two fine-tuning tasks that yield task vectors $\delta_{T_1}$ and $\delta_{T_2}$. Take the entries correspond to a weight matrix and reconstruct them into $A,B$ from $\delta_{T_1}$ and $\delta_{T_2}$, respectively. Suppose $A$ and $B$ admit SVD into $\sum_i\sigma_i^A u_i^A (v_i^A)^T$ and $\sum_i\sigma_i^B u_i^B (v_i^B)^T$, one can obtain the matrix rank by the number of nonzero singular values. By selecting only the top few singular values and vectors (i.e. truncated SVD), we naturally find the principal components and remove the redundant dimensions, effectively reducing the rank of the matrix. As small singular values often correlate with noise or fine details, low-rank prior is also widely used in compressed sensing and denoising applications in signal processing~\cite{dabov2007image,candes2010matrix,cai2010singular,candes2012exact}.

% When applying SVD on a matrix \(\bm{W}\), it can be decomposed as 
% \[ \bm{W} = \sum_k \sigma_k \bm{u}_k \bm{v}_k^\top, \]
% where \(\sigma_k\) denotes the \(k\)-th singular value, and \(\bm{u}_k\) and \(\bm{v}_k\) are the corresponding left and right singular vectors, respectively. \(\bm{c}_k = \sigma_k \bm{u}_k \bm{v}_k^\top\) represents the \(k\)-th orthogonal component.

% The advantage of transforming task vectors into spectral domain using SVD is illustrated in two main folds as follows.

% \begin{enumerate}
%     \item \textbf{Orthogonality.} SVD helps decompose \(\delta\) into a set of orthogonal components. Leveraging this, when spectral truncate is applied to eliminate insignificant components, the top components—representing important task-related weight changes—remain unaffected by any distortion, preserving complete orthogonal information. Note that this is impossible to accomplished by parameter space magnitude-based methods, such as TIES~\cite{yadav2024ties} and Model Breadcrumbs (Fig. ~\ref{fig:produce_more_ranks}). Further evidences are provided in to validate that low rank extracted knowledge is of more importance and robustness.

% \item \textbf{Ranking Mechanism.} The spectral decomposition not only results in orthogonal components but also yields singular values \(\bm{\sigma}\). These values are not only useful for ranking importance but can also be utilized in two specific ways. (1) Adaptively deciding how many top ranks should be extracted from matrices across layers and task vectors (Sec.~\ref{subsec:STAR}); (2) modeling an information budget for automatic rescaling (Sec. ~\ref{subsec:rescale}).
% \end{enumerate}

Besides extracting principal components, we also give a high-level illustration of why using truncated SVD on $A$ and $B$ separately can help reduce conflicts during model merging. Assume $\mathcal{T}_1$ is associated with data manifold $\mathcal{D_A}$. For $x\in\mathcal{D_A}$, we essentially hope $(A\oplus B)x$ to be close to $Ax$ while excelling at $\mathcal{T}_2$ after merging, where $\oplus$ denotes the merging operation. Let us consider the merging operation to be plainly $A+B$, then the level of conflicts can be measured by $\|Bx\|$. By expressing $x\in\mathcal{D_A}$ via the right singular vectors of $A$, $x=\sum_j \alpha_j v_j^A$, we prove in Sec.~\ref{subsec:bound} that we have $\|Bx\|\leq r^B\beta \sqrt{r^A}$,
% write out $\|Bx\|$ as  
% % \begin{align*}
% %     &\|\sum_i\sigma_i^B u_i^B (v_i^B)^T \sum_j \alpha_j v_j^A\| \\
% %     \leq &\sum_i\sum_j |\sigma_i^B\alpha_j|\cdot |(v_i^B)^Tv_j^A|\leq \sum_i\sum_j |\sigma_i^B\alpha_j|.
% % \end{align*}
% % \IK{
% {\small
% \begin{align*}
%     &\|\sum_i\sigma_i^B u_i^B (v_i^B)^T \sum_j \alpha_j v_j^A\| \\
%     \leq &\sum_i \|u_i^B\| \cdot |\sum_j \sigma_i^B\alpha_j  (v_i^B)^T   v_j^A| 
%     \leq \sum_i \beta\cdot |\sum_j (v_i^B)^T   v_j^A| \\
%     \leq & \sum_i \beta \sqrt{D^A}\cdot \left(\sum_j \left(\left(v_i^B\right)^T   v_j^A\right)^2\right)^{1/2} \leq r^B \beta \sqrt{r^A},
% \end{align*}}%
where $\beta=\max_{i,j}|\sigma_i^B\alpha_j|$, and $r^A$ and $r^B$ are the original ranks of $A$ and $B$.
% }
By truncating $B$ to rank-$r$, this upper bound is lowered by $(r^B-r)\beta \sqrt{r^A}$
% $\sum_{i=r+1}\sum_j |\sigma_i^B\alpha_j|$
, implying potentially less conflicts in model merging. 
% We denote the task vectors after spectral truncations as $\tilde{\delta}$.
% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{image/svd_truncate/produce_more_ranks_3.png}
%     \caption{Performing knowledge extraction on \(\delta^{l}\) which is originally with rank \(16\). STAR (\(r=4\)) naturally reduce the total ranks, while TIES (\(K=20\%\)) and Model Breadcrumbs (\(\beta=90\%, \gamma=99\%\)) distort the original basis and introduce more spectral components, which do not resolve task conflicts in spectral perspective.}
%     \label{fig:produce_more_ranks}
% \end{figure}




\input{retain_sv.tex}






\subsection{STAR: Spectral Truncate And Rescale}\label{subsec:STAR}

Now that we have elaborated on the two key components of STAR, we explain the complete workflow in the following. With $T$ task vectors, we transform them into respective spectral spaces via SVD, and their ranks are determined by $r = \mathop{\arg\min}_{k} \left( \frac{\sum_{i=1}^k \sigma_i}{\sum_i \sigma_i} \geq \eta\% \right)$, where $\eta$ is a tunable parameter. Then, we follow Section~\ref{subsec:rescale} to rescale back to their original nuclear norm. Finally, STAR reconstructs $T$ task vectors from their decompositions and perform simple averaging to obtain $\bm{\delta}_{\text{merged}}$. We give the full STAR model merging algorithm in Alg.~\ref{alg:STAR} in appendix.




We note that as the distribution of singular values varies both within and across task vectors, truncating components adaptively allows different ranks across not only tasks and even layers (e.g. Fig.~\ref{fig:rank_viz}). 

\begin{figure}[t]    
    \centering
    \includegraphics[width=0.52\textwidth, height=3.8cm]{image/adaptively/piqa_flan_large.png}
    \caption{An example of the automatic rank determination by STAR ($\eta=40$) on PIQA's task vector with Flan-T5-large.}
    \label{fig:rank_viz}
\end{figure}


