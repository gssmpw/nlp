\subsection{Additional Results}

\noindent\textbf{Ablation studies on restoring the nuclear norm}
%In appendix Sec.~\ref{subsec:ablation_rescale}, 
In Table~\ref{tab:rescale_help}, we give an example of merging 4 fine-tuned Flan-T5-large models with and without rescale to restore the matrix nuclear norm.
%as introduced in Sec.~\ref{subsec:rescale}. 
%From the table, 
We see that rescale is crucial especially when we use low-rank approximations (e.g. rank-2). 
%1
\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{ 
\ra{1.8}
\LARGE
\begin{tabular}{@{}ccccccc@{}}\toprule
Rank Kept & Rescale & MRPC & Finance & HellaSwag & PIQA & Avg. Normalized \\ \midrule
r=2 & No & 73.36 & 91.19 & 77.75 & \textbf{80.75} & 97.17 \\
    & Yes & \textbf{74.05} & \textbf{96.04} & \textbf{79.40} & 80.25 & \textbf{99.01} \\ \cmidrule(lr){1-7}
r=4 & No & 73.27 & 94.71 & 78.35 & \textbf{81.00} & 98.32 \\
    & Yes & \textbf{73.79} & \textbf{96.04} & \textbf{79.20} & 80.75 & \textbf{99.02} \\ \cmidrule(lr){1-7}
r=8 & No & 73.44 & 94.71 & 78.70 & \textbf{81.00} & 98.48 \\
    & Yes & 73.44 & \textbf{95.59} & \textbf{78.80} & 80.50 & \textbf{98.58} \\ \cmidrule(lr){1-7}
r=12 & No & 73.44 & 94.71 & 78.55 & \textbf{81.00} & 98.44 \\
    & Yes & 73.44 & \textbf{95.15} & \textbf{78.85} & \textbf{81.25} & \textbf{98.72} \\ \bottomrule
\end{tabular}
}
\caption{The ablation study of the rescaling step to restore nuclear norms (i.e. Sec.~\ref{subsec:rescale}).}
\label{tab:rescale_help}
\end{table}


\noindent\textbf{Sensitivity analysis of $\eta$.} As \(\eta\) is the only tunable hyperparameter in STAR, we further show in 
%appendix 
Fig.~\ref{fig:eta_analysis1} that \(\eta\) is robust across different model merging combinations and numbers of models merged, compared to the baseline (e.g. TIES).
%In this section, 
Specifically, we allow STAR to choose $\eta$ from $\{10, 20, \dots, 70\}$ and TIES to choose $K$ from $\{1, 5, 10, 20, \dots, 70\}$. From the standard deviation in Fig.~\ref{fig:eta_analysis1}, it can indeed be seen that STAR is not sensitive to $\eta$, sparing users' need to fine-tune $\eta$ during the deployment.
\begin{figure}[t]
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=0.99\linewidth]{image/main_results/sensitivity/sensitivity_flan_base.png} 
\caption{Flan-T5-base}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=0.99\linewidth]{image/main_results/sensitivity/sensitivity_flan_large.png}
\caption{Flan-T5-large}
\end{subfigure}
\caption{The average model merging results on Flan-T5-base and Flan-T5-large over a range of possible hyperparameter choices.}
\label{fig:eta_analysis1}
\end{figure}

\noindent\textbf{Optimal $\eta$ varies as number of models merged.} Following~\citet{ilharco2022editing}, we report the optimal $\eta$ when merging different number of models in Fig.~\ref{fig:optimal_eta}. By searching for \( \eta \) within \(\{10, 20, \dots, 70\}\) across all sampled model merging combinations, we observed an interesting trend: as the number of merged models increases, the optimal \( \eta \) gradually decreases, indicating that higher truncation for each task vector is necessary. 
% This trend is intuitiveâ€”task conflicts become more pronounced with more merged models, likely necessitating higher truncation for each task vector. Conversely, when merging fewer models, excessive truncation may be unnecessary, as it could result in a loss of task-specific information.






% \noindent\textbf{More discussions.} Due to the page limit, we put ... in appendix.
% we defer additional analysis on our automatic rank determination schemes in appendix Sec.~\ref{subsec:automatic_rank}. 
% \IKB{Add more references to the appendix once its done.}






