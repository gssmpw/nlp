\subsection{Rescale to Restore Matrix Nuclear Norm}
\label{subsec:rescale}
As model merging favors spectral truncation as discussed in Sec.~\ref{subsec:truncate}, a caveat is the resulting change in the ratio between the pretrained model and the task vector. Roughly, one sees that $\|Ax\|=\|\sum_i\sigma_i^A u_i^A (v_i^A)^T \sum_j \alpha_j v_j^A\|=\|\sum_i\sigma_i^A\alpha_i u_i^A\|$ and can at most be $\sum_{i=r+1}\|\sigma_i^A\alpha_i\|$ smaller with the truncated $A$. Therefore, the performance on the fine-tuning task $\mathcal{T}_1$ might be compromised. 
On that account, it is crucial to include a step where we rescale the spectral-truncated weight matrices back to their original ``size'', similar to the compensation operation in dropout. We propose to retain matrix nuclear norm (aka Schatten $1$-norm or trace norm) as it is a proper measure of matrix ``size'', especially in low-rank approximation contexts as nuclear norm is a convex relaxation of the rank function~\cite{candes2012exact}.
Specifically, we rescale the remaining singular values by
\[
\sigma_k' = \frac{\sum_i \sigma_i}{\sum_{i=1}^r \sigma_i} \cdot \sigma_k, \quad \forall k \in [1, r].
\]

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}