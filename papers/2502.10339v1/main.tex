% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{float}

\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{bm} 

%Algorithm related packages
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

% table
\usepackage{booktabs}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{color}
\newcommand{\PY}[1]{{\color{blue}PY: #1}}
\newcommand{\PYB}[1]{{\color{blue}[PY: #1]}}
\newcommand{\IK}[1]{{\color{brown} #1}}
\newcommand{\IKB}[1]{{\color{brown}[IK: #1]}}
\newcommand{\AR}[1]{{\color{orange}AR: #1}}
\newcommand{\TP}[1]{{\color{orange}[TP: #1]}}


\title{STAR: Spectral Truncation and Rescale for Model Merging}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\author{\textbf{Yu-Ang Lee\textsuperscript{1, 2}}\thanks{This work was done while Yu-Ang Lee was a visiting
researcher at IBM Thomas J. Watson Research Center.},
 \textbf{Ching-Yun Ko\textsuperscript{2}},
 \textbf{Tejaswini Pedapati\textsuperscript{2}},
 \\
 \textbf{I-Hsin Chung\textsuperscript{2}},
 \textbf{Mi-Yen Yeh\textsuperscript{3}},
 \textbf{Pin-Yu Chen\textsuperscript{2}}
\\
\\
\textsuperscript{1}National Taiwan University,
\textsuperscript{2}IBM Research,
\textsuperscript{3}Academia Sinica
\\
\texttt{r12946015@ntu.edu.tw, cyko@ibm.com, tejaswinip@us.ibm.com} \\
\texttt{ihchung@us.ibm.com, miyen@iis.sinica.edu.tw, pin-yu.chen@ibm.com}
}


\begin{document}
\maketitle
\begin{abstract}
Model merging is an efficient way of obtaining a multi-task model from several pretrained models without further fine-tuning, and it has gained attention in various domains, including natural language processing (NLP). Despite the efficiency, a key challenge in model merging is the seemingly inevitable decrease in task performance as the number of models increases. In this paper, we propose \textbf{S}pectral \textbf{T}runcation \textbf{A}nd \textbf{R}escale (STAR) that aims at mitigating ``merging conflicts'' by truncating small components in the respective spectral spaces, which is followed by an automatic parameter rescaling scheme to retain the nuclear norm of the original matrix. STAR requires no additional inference on original training data and is robust to hyperparamater choice. We demonstrate the effectiveness of STAR through extensive model merging cases on diverse NLP tasks. Specifically, STAR works robustly across varying model sizes, and can outperform baselines by 4.2\% when merging 12 models on Flan-T5. Our code is publicly available at \href{https://github.com/IBM/STAR}{https://github.com/IBM/STAR}.

\end{abstract}

\section{Introduction}
With the popularity of pretrained models on large neural networks, the same architecture is often deployed to fine-tune individual natural language processing (NLP) tasks. A natural question then arises about whether it is possible to merge these same-architecture fine-tuned models into one multi-task model. For example, researchers are interested in understanding if we can empower a fine-tuned conversational large language model (LLM) with reasoning capabilities by merging with an LLM specializing in solving math problems.  Specifically, ~\citet{ilharco2022editing} has formally defined a \emph{task vector} as \(\theta_{\text{ft}} - \theta_{\text{pre}}\), where $\theta_{\text{pre}}$ and $\theta_{\text{ft}}$ denote the vectorized parameters of the pre-trained model and the fine-tuned model, respectively. Thus, task vectors mark the updates made to the pretrained model's weights when fine-tuned on specific tasks. Then, \textit{model merging} essentially studies ways of fusing different task vectors that are trained separately and merging them with the pretrained model. However, as the number of fine-tuned models increases, the multi-task performance of their merged model also decreases drastically. Fig.~\ref{fig:numerial_evidence}
shows the averaged normalized performance (y-axis) v.s.  the number of models merged (x-axis).
%the x-axis denotes the number of models merged and the y-axis denotes the averaged normalized performance.
% (i.e. average normalized accuracy). 
Furthermore, we point out that when the number of models exceeds a certain threshold, the multi-task performance of the merged model could be even worse than that of the original pretrained model, diminishing the fundamental goal of model merging. For example, TIES~\cite{yadav2024ties}, MetaGPT~\cite{zhou2024metagpt}, and TALL-masks~\cite{wang2024localizing} merged models drop below 0.82 when we merge 6, 5, and 7 fine-tuned models, respectively, in Fig.~\ref{fig:numerial_evidence}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth, height=4cm]{image/main_results/No_tuning/flan_t5_base_correct_no_tune_tall.png}
    \caption{The averaged normalized performance of Flan-T5-base merged models by TIES~\cite{yadav2024ties}, MetaGPT~\cite{zhou2024metagpt}, TALL-masks~\cite{wang2024localizing}, and STAR (this paper).} 
    \vspace{-1em}
    \label{fig:numerial_evidence}
\end{figure}
\begin{figure*}[t]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.85\linewidth, height=5.cm]{image/figure1/final_3.png}
    \caption{An overview of the \textbf{STAR workflow}. When merging two task vectors, \(\delta_{1}\) and \(\delta_{2}\), (1) STAR transforms both task vectors into their spectral spaces with their singular vectors being the orthogonal basis using singular value decomposition (SVD) (singular values are represented by the length of the arrows), (2) STAR removes redundant dimensions by truncating singular vectors with small singular values, (3) STAR restores the original nuclear norm by rescaling the truncated SVD, and (4) STAR reconstructs the parameters by multiplying components back to form the weight matrices and then perform simple averaging.}
    \vspace{-1em}
    \label{fig:fig1}
\end{figure*}
The complexity of existing model merging methods varies largely depending on whether they require fine-tuning or inference on training data~\cite{yang2024model}. In this paper, we study the ``data-free'' setting when we are not authorized to change the fine-tuning protocol nor do we have access to the training data. In this work, we propose to use spectral decomposition (e.g. singular value decomposition, SVD) to remove noisy components on model merging. We will also motivate the potential gain of our spectral space merging scheme by comparing the upper bounds of the task conflicts. A rescaling step is then followed to restore the original nuclear norm. We give the overview of the proposed method in Fig.~\ref{fig:fig1}. Our proposed merging scheme, \textbf{S}pectral \textbf{T}runcation \textbf{A}nd \textbf{R}escale (STAR), is effective and efficient as it requires no additional inference on original training data and is not sensitive to hyperparameters.
Our extensive experimental results show that STAR is superior across various model size settings and can effectively merge up to 20 models while achieving positive performance gains, compared to the pretrained model before merging.


\input{background}

\input{Spectral_truncate}

\input{experiment}

\input{discussion}

\input{conclusion}


\section*{Limitation}

While STAR demonstrates strong potential for practical model merging use cases across domains, its performance has been tested primarily on parameter-efficient fine-tuned (PEFT) models in NLP. 
Additionally, STAR requires SVD to orthogonalize task vectors, which may introduce additional computational cost. However, users can mitigate this by leveraging fast SVD algorithms in the implementation.


\section*{Acknowledgement}
This work was primarily done during Yu-Ang Lee's visit to IBM Research, and was supported in part by the National Science and Technology Council, Taiwan, under grant NSTC 113-2628-E-001 -003 -MY4.



\bibliography{custom}

\input{appendix}

\end{document}
