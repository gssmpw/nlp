\section{Experiments}


\begin{figure*}[t]
\vspace{-0.5em}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=0.99\linewidth]{image/main_results/No_tuning/flan_t5_large_no_tune_tall.png} 
\caption{Flan-T5-large}
\end{subfigure}
% \begin{subfigure}{0.33\textwidth}
% \includegraphics[width=0.99\linewidth]{image/main_results/No_tuning/flan_t5_large_no_tune_small_spot.png}
% \caption{Flan-T5-large}
% \end{subfigure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=0.99\linewidth]{image/main_results/No_tuning/mistral_no_tune_small_dots.png}
\caption{Mistral-7B-Instruct}
\label{fig:main_mist}
\end{subfigure}
\caption{Model merging results on Flan-T5-large and Mistral-7B-Instruct. For all numbers of models merged, we sampled 5 task combinations for Flan-T5 and 3 for Mistral, with the sampled combinations represented by shaded dots and the average depicted by solid lines. While STAR remains a strong model merging method, TIES, TALL-masks and MetaGPT can be more sensitive to model architecture choice.}
%\PYB{If all methods are data-free; no need to mention; we can just say in Sec. 2/3 that we consider data-free model merging settings throughout this paper}
\label{fig:main_datafree}
\vspace{-0.5em}
\end{figure*}




% \begin{figure}
%     \centering
%     \includegraphics[width=0.98\linewidth]{image/main_results/No_tuning/mistral_no_tune_small_dots.png}
%     \caption{\textbf{One Shot Merging on Mistral-Instruct.} Up to 20 models were merged in this case, with MetaGPT and STAR show potential to merge even more models without knowledge collapse.}
%     \label{fig:main_mist}
% \end{figure}


\subsection{Experimental Setup}

% \textbf{Pre-Trained Backbone Selection.} 
\noindent\textbf{Models.} We consider both encoder-decoder models (e.g. Flan-T5-base/large)~\cite{chung2024scaling} and decoder-only model (e.g. Mistral-7B-Instruct-v0.2)~\cite{jiang2023mistral}.
For Flan-T5-base/large, we use finetuned models on GLUE from FusionBench~\cite{tang2024fusionbench}, 
% \TP{specify the datasets on which these models were trained on. Also include the model ids for both lots of loras and this in the appendix so that others can reproduce it.} 
together with additional fine-tuned models on Finance~\cite{Malo2014GoodDO}, IMDB~\cite{maas-EtAl:2011:ACL-HLT2011}, AG News~\cite{zhang2015character}, BoolQ~\cite{clark2019boolq}, PIQA~\cite{Bisk2020}, and HellaSwag~\cite{zellers2019hellaswag} by ourselves, bringing the total number of task vectors to \(13\). For Mistral-Instruct, we randomly select \(20\) models directly from the Lots of LoRAs collection~\cite{bruel2024compress}, which covers a range of NLI tasks.
% , which offers a large set of models finetuned on various NLI tasks.
% \(12\) and 
All models considered herein are LoRA finetuned~\cite{hu2021lora} with rank \(16\) and scaling factor (alpha) set to \(32\). Details about the models are in Appendix Sec.~\ref{subsec:experiment_details}.
To understand how each merging method performs on $n$ models, we randomly sample $n$ tasks and report their average results. 

\noindent\textbf{Hyperparameters.} Without otherwise specified, we let $K=20$ for TIES (the default parameter in~\cite{yadav2024ties}), \(\lambda_{t}=0.4\) for TALL-masks (the middle value searched by~\cite{wang2024localizing}), and \(\eta=40\) for STAR. 
% We defer the sensitivity analysis of $K$ and \(\eta\) to the appendix.

\noindent\textbf{Evaluation metric.} Following~\citet{tang2024fusionbench,bruel2024compress}, performances on QASC~\cite{allenai:qasc} and STSB~\cite{cer2017semeval} are evaluated by F1 score and Spearman's coefficient, respectively,
% When evaluating the performance of merged models on individual tasks, F1 score is applied for QASC, Spearman's rank correlation for STSB, 
and accuracy for all other tasks. 
% Despite all three LMs being instruction-tuned, the merged model sometimes outputs answers mixed with random tokens, especially when task conflicts are severe. To account for this, i
If the correct output appears within the first 10 tokens generated by the merged model, the response is deemed correct.
For a model merged on \(t\) tasks, we report the normalized average performance~\cite{ilharco2022editing,yadav2024ties} defined by $\frac{1}{t} \sum_{i}^{t} \frac{\text{(Merged Model Perf.)}_{i}}{\text{(Finetuned Model Perf.)}_{i}}$. We further measure the performance of the pretrained model by $\frac{1}{T} \sum_{i=1}^{T} \frac{\text{Pretrained Model Perf.}_{i}}{\text{Finetuned Model Perf.}_{i}}$. If the merged model performs worse than the pretrained model, then model merging loses its purpose. 
% Normalized average performance is utilized to quantify merged model efficacy~\cite{ilharco2022editing,yadav2024ties}. Specifically, for a merged case \(H_{t}^{j}\), we calculate its \emph{normalized average performance} by
% \[
% \frac{1}{t} \sum_{i}^{t} \frac{\text{(Merged Model Perf.)}_{i}}{\text{(Finetuned Model Perf.)}_{i}}
% \]
% Notably, we additionally introduce
% \[
% \text{\emph{lower bound}} = \frac{1}{T} \sum_{i=1}^{T} \frac{\text{Pretrained Model Perf.}_{i}}{\text{Finetuned Model Perf.}_{i}},
% \]
% which serves as a baseline to determine at which point a given model merging method starts to lose value. This would occur when task conflicts are not well-handled, and using the pretrained model actually becomes more favorable. In our case, the \emph{lower bound} is 82.30\%, 88.03\%, and 66.47\% for Flan-T5-base, Flan-T5-large, and Mistral-Instruct, respectively.





\input{exp_results}