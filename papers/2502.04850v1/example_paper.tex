%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{multirow}
\usepackage{colortbl}
\usepackage{siunitx}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{algpseudocode}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{subcaption}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption} 
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\newcommand{\blue}{\color{blue}}
\newcommand{\red}{\color{red}}
\definecolor{lightred}{rgb}{1,0.8,0.8}
\definecolor{lightgreen}{rgb}{0.8,1,0.8}
\definecolor{lightblue}{rgb}{0.88,0.96,1} 
\definecolor{lightgray}{rgb}{0.9,0.9,0.9}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks} 

%Provably Fair Federated Learning using Slimmable Networks}

\begin{document}

\twocolumn[
% \icmltitle{Submission and Formatting Instructions for \\
%            International Conference on Machine Learning (ICML 2025)}
% \icmltitle{Proper Fairness in Federated Learning} 
\icmltitle{Aequa: Fair Model Rewards in Collaborative Learning via Slimmable Networks} 
%Provably Fair Federated Learning using Slimmable Networks} 

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Nurbek Tastan}{mbzuai}
\icmlauthor{Samuel Horv\'ath}{mbzuai}
\icmlauthor{Karthik Nandakumar}{mbzuai,msu}
\end{icmlauthorlist}

% \icmlaffiliation{mbzuai}{Department of Machine Learning, MBZUAI, UAE}
\icmlaffiliation{mbzuai}{Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), UAE}
% \icmlaffiliation{msu}{Department of Computer Science and Electrical Engineering, MSU, USA}
\icmlaffiliation{msu}{Michigan State University (MSU), USA}

\icmlcorrespondingauthor{Nurbek Tastan}{nurbek.tastan@mbzuai.ac.ae}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Federated Learning, Collaborative Fairness, Fair Federated Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
    Collaborative learning enables multiple participants to learn a single global model by exchanging focused updates instead of sharing data. One of the core challenges in collaborative learning is ensuring that participants are rewarded fairly for their contributions, which entails two key sub-problems: \textit{contribution assessment} and \textit{reward allocation}. This work focuses on fair reward allocation, where the participants are incentivized through \textit{model rewards} - differentiated final models whose performance is commensurate with the contribution. In this work, we leverage the concept of \textbf{slimmable neural networks} to collaboratively learn a shared global model whose performance degrades gracefully with a reduction in model width. We also propose a post-training \textbf{fair allocation algorithm} that determines the model width for each participant based on their contributions. We theoretically study the convergence of our proposed approach and empirically validate it using extensive experiments on different datasets and architectures. We also extend our approach to enable training-time model reward allocation. 
\end{abstract}


\section{Introduction}
\label{section: intro}

Collaborative learning (CL) has emerged as a transformative paradigm for training machine learning models across data silos while preserving data privacy. Unlike centralized approaches, CL enables participants (e.g., hospitals, financial institutions, etc.) to jointly train a shared global model by exchanging only focused updates rather than raw data \cite{mcmahan2017communication}. CL mitigates privacy risks and complies with regulations such as GDPR, making it particularly useful in domains where data sensitivity is paramount. A special case of CL is federated learning (FL), where a central server orchestrates the collaboration. However, CL/FL faces many challenges such as communication inefficiencies due to frequent exchanges of updates, system heterogeneity resulting from different participant capabilities, and statistical heterogeneity caused by non-i.i.d. data distributions across participants \cite{zhu2021flonnoniiddataSurvey}. These issues often degrade model performance, scalability, and practical adoption. 


A critical yet less explored challenge in CL lies in ensuring \textbf{collaborative fairness} among participants. Traditional CL frameworks assume uniform contributions from all parties, but real-world scenarios involve disparities in data quality, quantity, and computational resources. For instance, participants with high-quality data may receive disproportionately less rewards despite their critical role in model generalization. The interplay between fairness and incentivization is essential to sustaining long-term collaboration. Without equitable incentives, participants may withhold resources or disengage entirely, leading to the ``free-rider problem" where some entities benefit without contributing meaningfully. To address this issue, recent research has explored incentivization mechanisms such as Shapley value-based reward allocation \cite{xu2021gradient, tastan2024redefining}, reputation systems \cite{xu2020reputation}, and game-theoretic frameworks \cite{wu2024incentive}. These methods aim to quantify and reward participant contributions transparently. 

Achieving collaborative fairness in CL fundamentally hinges on two key sub-problems: (1) contribution assessment and (2) reward allocation mechanism. Contribution assessment evaluates the marginal impact of each participant's data or computational resources on the global model's performance. Recent efforts \cite{xu2021gradient, jia2019towards, shi2022fedfaim, jiang2023fair} attempt to quantify the quality of the local model updates as a proxy for measuring their contribution. Once the marginal contribution of each participant is determined, a reward allocation mechanism is necessary for incentivizing the participants to collaborate. Rewards can be in the form of financial compensation (monetary rewards) or differentiated final models (model rewards). To ensure fairness, rewards must be commensurate with the contribution. \textbf{This work focuses exclusively on fair distribution of model rewards in CL}. 


A critical question arises: \textit{How can participants be rewarded with different models whose performance (accuracy) faithfully reflects their heterogeneous contributions?} While previous work has explored this problem \cite{xu2021gradient, wu2024incentive}, they typically rely on sharing partial updates with the participants, which often lacks rigorous convergence guarantees. While \cite{wu2024incentive} provide convergence analysis, their approach suffers from the following limitations: (1) The introduction of a stochastic recovery mechanism necessitates occasional broadcasting of the full global model to all participants, thereby aiding free-riding; (2) Since participants begin training from divergent starting points in each round (except the initial broadcast), it leads to potential instability in model convergence; (3) Finally, sampling a subset of updates from a pool of gradients does not inherently ensure that low-contribution participants receive low-quality models; they may still receive high-quality gradients originating from high-contribution participants. 

To circumvent these problems, we draw inspiration from the concept of slimmable networks \cite{yu2019slimmable, yu2019universally}, which were originally proposed to dynamically adjust the width of neural network models for efficient inference with a graceful degradation in model performance. By extending this concept to the collaborative/federated setting, we obtain a global model with a nested structure, where subnetworks of varying widths (e.g. $0.25 \times$, $0.5\times$, $1.0\times$) are embedded. Participants are then assigned subnetworks corresponding to their contribution levels -- higher contributors receive wider, higher-performing subnetworks, while lower contributors obtain narrower ones. Our approach ensures that model rewards are proportional to client contributions, achieving both high performance and collaborative fairness simultaneously. However, there is one significant obstacle that needs to be surmounted. Collaborative learning in the plaintext domain exposes the intermediate models to the participants, thereby re-introducing the free-rider problem. To overcome this limitation, we assume that each participant has access to a trusted execution environment (TEE) and local training happens confidentially within a TEE. While the use of TEEs has been considered in the FL literature \cite{huba2022papaya, eichner2024confidential,daly2024federated}, they are most used on the server side for secure aggregation and minimization of privacy risks. This work uses TEEs on the client side to enhance collaborative fairness.

The main contributions of this work are as follows: (1) We introduce a CL framework called \textbf{Aequa} (\textit{Latin: fair}) that leverages slimmable networks to dynamically adapt model performance to client contributions while being agnostic to any contribution measure. (2) We propose a fair allocation mechanism for the post-training distribution of model rewards and then extend this approach to training-time rewards. (3) We provide convergence analysis demonstrating that our framework retains optimality guarantees. (4) We also empirically validate the efficacy of our framework through experiments on benchmark datasets, highlighting balanced model performance and fairness across diverse scenarios. 




\section{Related Work}
\label{section: related-work}

\textbf{Fairness in FL.} Fairness in federated learning has been extensively studied through two primary lenses: performance fairness \cite{jiang2023fair}, which emphasizes uniform model performance across all participants, and collaborative fairness \cite{lyu2020collaborative}, which advocates proportionality between client contributions and rewards. Our work focuses on collaborative fairness, where clients receive model rewards commensurate with their contributions. The foundational work of \citet{lyu2020collaborative} operationalizes collaborative fairness by assigning only the allocated aggregated updates based on their reputations. 
Other studies consider fairness by quantifying the impact of clients on the global model -- the naive choice being the self-reported dataset sizes (self-reported information) \cite{donahue2021optimality, zhang2020hierarchicallyfairfederatedlearning}, and similarly, \citet{kang2019incentivedesignforefl} employ such self-reported information to build a fair scheme based on contract theory. Various approaches also assess client importance through Shapley values \cite{shapley_book1952, ghorbani2019datashapley}, utility games \cite{gollapudi2017profitsharing, nishio2009estimationIFL} and empirical methods \cite{kuk2021fedccea}. For a complete taxonomy of fairness in FL, we refer the reader to check \cite{Shi_2024FairnessAwareFL}. 

\textbf{Contribution assessment.} A substantial body of work has addressed the problem of evaluating individual client contributions in federated learning. As mentioned earlier, an initial approach to collaborative fairness \cite{lyu2020collaborative} employed a global validation set, applying a function $\sinh$ to the validation accuracy of each client as a penalty mechanism to approximate their contribution or reputation. Subsequently, \citet{xu2021gradient} removed the need for a global validation set by approximating game-theoretic Shapley values with the cosine similarity of shared parameter updates -- thereby capturing each client's marginal contribution. A range of follow-up studies \cite{shi2022fedfaim, jiang2023fair, Lin2023fairyetasymp, tastan2024redefining} further expanded and refined these strategies for contribution assessment. 

\textbf{Reward mechanisms.} Broadly, existing incentive mechanisms in FL fall into two categories: post-training monetary rewards and training time model rewards. The former employs frameworks such as Stackelberg games \cite{zhan2020learning}, auctions \cite{zhang2021incentive, cong2020game}, and contract theory \cite{liu2022contract, yang2024asynchronous} to distribute monetary compensation post hoc based on client contributions. The latter focuses on model-based rewards during training, incentivizing participation by dynamically adjusting access to the modelâ€™s capabilities. For example, CGSV \cite{xu2021gradient} allocates sparsified model outputs to clients proportionate to their contributions, while achieving fairness. Similarly, IAFL \cite{wu2024incentive} shares aggregated gradients based on each client's contribution through probabilistic sampling, thus restricting highly performing models from under-contributing clients. However, while CGSV relies on a heuristic approach and lacks a formal convergence analysis, IAFL includes a convergence proof but exhibits its own limitations. Specifically, its stochastic recovery mechanism shares the full model updates with all participants based on a certain probability: setting this probability to zero yields higher fairness at the expense of performance, and increasing it boosts performance at the cost of fairness -- yet still falls short of the performance achieved by FedAvg \cite{mcmahan2017communication}. 

\textbf{Slimmable networks.} The seminal work by \cite{yu2019slimmable} introduced the idea of training a single neural network that can operate at multiple widths, enabling dynamic trade-offs between model size and performance. This innovation led to numerous follow-up studies and applications in federated learning, predominantly focused on resource efficiency \cite{mei2022resource, horvath2021fjord}, communication and computational efficiency \cite{wang2022progfed}, and neural architecture search \cite{yu2019autoslim}. To the best of our knowledge, we are the first to leverage slimmable networks in the context of fair federated learning. 



\section{Preliminaries}
\label{section: prob-statement}


We consider a federated learning (FL) setup with $N$ participants collaboratively learning the parameters $\vx \in \mathbb{R}^d$ of a machine learning model. Each client $i$ possesses a local dataset $\mathcal{D}_i$, and the overall objective is to minimize a sum-structured FL optimization problem, given by 
\begin{equation}
    \vx^* \gets \arg\min_{\vx \in \mathbb{R}^d} \left[ F(\vx) \coloneqq \frac{1}{N} \sum_{i=1}^{N} F_i(\vx) \right], 
\end{equation}
where the local loss components $F_i : \mathbb{R}^d \rightarrow \mathbb{R}$ are distributed among $N$ participants and are expressed in a stochastic format as $F_i(\vx) \coloneqq \mathbb{E}_{\xi \sim \mathcal{D}_i} \left[ F_i(\vx, \xi) \right]$. 

Our specific task is supervised classification, where we define a classifier $\mathcal{C}_{\vx}$ parameterized by $\vx$, mapping input samples to class labels $\mathcal{C}_{\vx}: \mathcal{Z} \to \mathcal{Y}$, where $\mathcal{Z} \subseteq \mathbb{R}^D$ represents the input space, $\mathcal{Y} = \{1, 2, \ldots, C\}$ denotes the label space, $D$ is the input dimensionality, and $C$ is the number of classes. The empirical loss function for each client is defined as $F_i(\vx, \xi) = \mathcal{L}(\mathcal{C}_{\vx}(z), y)$ where $\mathcal{L}$ is the loss function and $\xi \coloneqq (z, y)$ is a training sample drawn from the local dataset of participant $i$. Additionally, the model parameters $\vx$ must be \textit{slimmable}, i.e., it should be possible to obtain a sub-network $\vx(p) \subseteq \vx$ by dynamically adjusting the width parameter $p$, such that the model performance is proportional to the width (which in turn is set proportional to the participant contribution).  



\section{Proposed Solution} 
\label{section: prop-solution} 

In federated learning, ensuring fairness in model allocation is a fundamental challenge, as clients contribute to training with varying levels of data quality, quantity, and computational resources. To address this, we propose an allocation mechanism based on slimmable networks, which ensures fair model rewards by adjusting the model width assigned to each client commensurate to their contributions. This section details our approach to federated optimization (Section \ref{subsection: fed-opt}), the allocation algorithm (Section \ref{subsection: alloc-alg}), and the extension to training-time model rewards (Section \ref{subsection: extension-to-traintime}). 



\subsection{Federated Optimization} 
\label{subsection: fed-opt}


\begin{algorithm}[t]
    \caption{Aequa: Federated optimization} 
    \label{alg:cap}
    \begin{algorithmic}[1]
        \Statex \textbf{Input:} minimum width $p_{\min}$, maximum width $p_{\max}$, comm. rounds $T$, number of participants $N$, randomly initialized parameters $\vx^0$, number of local iterations $E$ 
        \For{\text{each round} $t \gets 0, 1, \ldots, T-1$} 
            \State Server broadcasts $\vx^t$ to each client $i, \forall ~ i \in [N]$ 
            \For{each participant $i \in [N]$}
                \For{$k \gets 0, 1, \ldots, E$} \Comment{\textit{Local iterations}}
                    \State Sample width $p_{(i,k)} \gets \mathcal{U}([p_{\min}, p_{\max}])$, where $\mathcal{U}([a,b])$ represents uniform distribution in $[a,b]$.
                    \State Update parameters of the model corresponding to $p_{\max}$ and $p_{(i,k)}$ 
                \EndFor
                    \State Send the updated parameters $\vx_{(i,E)}^{t}$ to the server 
            \EndFor
            \State Server updates $\vx^{t+1} \gets \frac{1}{N} \sum_{i=1}^N \vx_{(i,E)}^t$ 
        \EndFor 
    \end{algorithmic}
\end{algorithm} 

Slimmable networks are a class of deep learning architectures that allow dynamic adjustments of model width, ensuring that different clients can operate with models of varying capacities without training separate networks. Initially introduced for efficient model scaling \citep{rippel2014learning, yu2019slimmable, yu2019universally, horvath2021hyperparameter, kusupati2022matryoshka, horvath2023maestro}, we repurpose slimmable networks for fairness in federated learning, ensuring that model allocation reflects each client's contribution to training. Each slimmable network can switch between different widths $p \in [p_{\min}, p_{\max}]$, where $p_{\max}$ represents the full model and $p_{\min}$ is the smallest subnetwork. Intermediate widths allow for a smooth transition based on contribution levels. 

Incorporating slimmable networks into FL requires adapting the optimization process to ensure that all subnetworks contribute effectively to the learning process. In our approach, clients train with varying model widths at each local iteration, ensuring that all width configurations are updated. The training process follows a random-width sampling strategy, where each client trains on different subnetworks in each iteration (one forward-backward pass), promoting a balanced optimization process across all model sizes. 

The federated optimization process begins with the server initializing the global model and setting the minimum and maximum possible widths. In each communication round, the server broadcasts the full model to all clients, who then train locally using a uniformly sampled width from the allowed range $[p_{\min}, p_{\max}]$. Clients update both the full model parameters and the parameters corresponding to their sampled width, following a strategy similar to the sandwich rule in \cite{yu2019universally}, but with improved efficiency. Upon completing local training, clients transmit their updated weights to the server, which then aggregates them to update the global model. 




\subsection{Fair Allocation Algorithm}
\label{subsection: alloc-alg}



\textbf{The allocation problem.}  We consider a setting with $N$ clients, indexed by $i = 1, 2, \ldots, N$, each with a contribution (standalone accuracy as a surrogate) $c_i$. We assume, without loss of generality, that $c_1 \leq c_2 \leq \cdots \leq c_N$. We have a (sufficiently large) family of models whose performances $\{a_k\}$ lie in the continuous interval $[\ell, u]$, where $u$ is the maximum possible accuracy ($u \geq c_N$) and $\ell$ is the minimum possible accuracy ($\ell \leq c_1 + (u - c_N)$). An allocation $\va = (a_1, \ldots, a_N) \in \mathcal{A} \equiv \{a_k\}^N$ assigns exactly one model with a performance level $a_i \in [\ell, u]$ to each client $i$. The gain (utility) of client $i$ under allocation $\va$ is 
\begin{equation}
    u_i(\va) = a_i - c_i. 
\end{equation}

An ideal allocation algorithm must satisfy the following three objectives: 
\begin{enumerate}
    \vspace{-0.1in}
    \item The gain of every client must be nonnegative, satisfying individual rationality (Definition \ref{definition: IR}) and ensuring the participation of all rational agents. \vspace{-0.05in}
    \item The variability of $\{u_i(\va)\}$ should be kept low so that no client's benefit is disproportionately high or low compared to others. \vspace{-0.05in}
    \item The average gain $\dfrac{1}{N} \sum_{i=1}^{N} u_i(\va)$ should be as large as possible, balancing utility with fairness. 
\end{enumerate}

\begin{definition}[Individual Rationality (IR)]
    \label{definition: IR}
    An allocation satisfies IR if $u_i(\va) \geq 0$ for all $i$. This ensures that no client is worse off than their standalone performance. 
\end{definition}

The above objectives can be achieved by maximizing the average gain $\mathbb{E}[u(\va)]$ while simultaneously minimizing the variance $\text{Var}[u(\va)]$ and respecting the IR constraint. Formally, we (equivalently) minimize the cost 
\begin{equation}
    f(\va) = - \frac{\mathbb{E}[u(\va)]}{\text{Var}[u(\va)] + \epsilon}, 
\end{equation}
where $f: \mathcal{A} \rightarrow \mathbb{R}$ is a cost function, and $\epsilon>0$ is a small constant that avoids division by zero. To solve this constrained optimization problem, we employ the simulated annealing algorithm \cite{granville1994simulated, bouttier2019convergence} and adapt it to our specific setting. 





\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/cifar10-cifar100-width-to-acc.pdf}
    \caption{Performance vs. network width $(p)$ using CIFAR-10 and CIFAR-100 datasets on ResNet-18 model.} 
    \label{fig: c10-c100-width-to-acc}
\end{figure}






\subsection{Extension to Training-time Rewards} 
\label{subsection: extension-to-traintime}
So far, our focus has been on post-training model rewards. However, our approach can be seamlessly extended to incorporate training-time rewards as well. The primary modification involves sending a slimmed-down network (a subnetwork) to each client instead of the full model during training and dynamically updating their contributions based on the quality of their shared updates. A well-established method for the evaluation of contributions is CGSV \cite{xu2021gradient}, and our approach can leverage CGSV to evaluate the contribution of each participant. In addition, other contribution assessment methods (\texttt{CA}), such as FedSV \cite{wang2020principled}, GTG-Shapley \cite{liu2022gtg}, ComFedSV \cite{fan2022improvingcomfedsv}, FedFAIM \cite{shi2022fedfaim}, ShapFed \cite{tastan2024redefining}, and others, can also be utilized within this framework. Since some clients (low-performing clients) only train on subnetworks and do not share the full gradients, they instead transmit their trained subnetwork updates. During the contribution assessment phase, we evaluate the gradients corresponding to the minimum-width subnetwork selected by the algorithm (e.g. $0.25\times$ as in Figure \ref{fig: c10-c100-width-to-acc}), ensuring consistency as all clients train on this common subnetwork. 

Furthermore, any incentive mechanism can be employed to dynamically adjust client contributions. 
% For example, the incentivization mechanism proposed in IAFL can be integrated. 
In our case, since the maximum width of each participant is determined based on estimated contributions, the approach involves directly mapping the normalized contributions to network widths, ensuring that the highest contributor or set of contributors receives the full model. The update rule of contributions is: 
\begin{equation}
    \label{eq: contribution-update}
    c_i^t = \gamma c_i^{t-1} + (1-\gamma) \tilde{c}_i, 
\end{equation}
where $\gamma$ is a momentum parameter and $\tilde{c}_i$ is obtained using \texttt{CA} method. Then, we define the reward mechanism as: 
\begin{equation}
    \label{eq: reward-training-time}
    [\mathcal{M}_{\nu} (\vc)]_i = \nu (c_i / \max_k c_k ), 
\end{equation}
where $\nu$ is a utility function and it directly maps the contributions to network widths $(p_i)$. 
We present the results of this approach in Figures \ref{fig: cifar10-homogeneous-combined-plots} and \ref{fig: cifar10-imbalanced-combined-plots}. Additionally, further details and a pseudo-algorithm outlining this extension are provided in Appendix \ref{appendix: extension-to-trainingtime}. 




\section{Theoretical Analysis} 
\label{section: theoretical-analysis} 

We now present the convergence analysis of the main algorithm described in Section \ref{subsection: fed-opt}, along with the convergence and fairness analysis of the allocation algorithm in Section \ref{subsection: alloc-alg}. 

\subsection{Convergence Analysis} 

Formally, we establish the following assumptions for the convergence analysis, which pertain to the properties of local objectives -- namely, $L$-smoothness (Assumption \ref{assumption: l-smoothness}), convexity (Assumption \ref{assumption: convexity-lsmoothness}), and bounded variance (Assumption \ref{assumption: bounded-variance}, Appendix \ref{subsection: deferred-lemmas-assumptions}). Additionally, we account for the data similarity across participants through bounded dissimilarity (Assumption \ref{assumption: bounded-dissimilarity}, Appendix \ref{subsection: deferred-lemmas-assumptions}). 

\begin{assumption}[$L$-smoothness]
    \label{assumption: l-smoothness}
    The local objective $F_i(\vx), \forall i \in [N]$ is $L$-smooth, then for all $\vx, \vy \in \mathbb{R}^d$, 
    \begin{eqnarray}
        F_i(\vx) \leq F_i(\vy) + \langle \nabla F_i(\vx), \vy - \vx \rangle + \frac{L}{2} \|\vx-\vy\|^2, \nonumber \\ 
        \forall i \in [N]. \nonumber
    \end{eqnarray}
\end{assumption}

\begin{assumption}[Convexity]
    \label{assumption: convexity-lsmoothness}
    The local objective $F_i(\vx), \forall i \in [N]$ is both convex and $L$-smooth, then for all $\vx, \vy, \vz \in \mathbb{R}^d$, 
    \begin{eqnarray}
        \frac{1}{L} \left\| \nabla F_i(\vx) - \nabla F_i(\vy) \right\|^2 \leq \left\langle \nabla F_i(\vx) - F_i(\vy), \vx-\vy \right\rangle, \nonumber\\
        F_i(\vx) \leq F_i(\vy) + \langle \nabla F_i(\vz), \vx-\vy \rangle + \frac{L}{2} \|\vx -\vz\|^2, \nonumber \\ 
        \forall i \in [N]. \nonumber 
    \end{eqnarray} 
\end{assumption} 

Next, we present the lemmas that establish the L-smoothness and convexity properties of the slimmed models. 
\begin{lemma}
\label{lemma: l3}


% Clearer version 
Let $F: \mathbb{R}^d \rightarrow \mathbb{R}$ be an $L$-smooth function over $\mathbb{R}^d$. Consider a selection of $d_1 \leq d$ coordinates from $\vx$, which we denote by $\widetilde{\vx} \in \mathbb{R}^{d_1}$. Define 
\begin{equation}
    F_{\vr}(\widetilde{\vx}) = F(\widetilde{\vx}, \vr),
\end{equation} 
where $\vr \in \mathbb{R}^{d-d_1}$ contains the fixed (zeros) coordinates of $\vx$. Then $F_{\vr}$ is $L$-smooth in the subspace $\mathbb{R}^{d_1}$ with a constant $\widetilde{L} \leq L$. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma: l3}]
    See Appendix \ref{subsection: proof-l-smoothness} 
\end{proof}


\begin{lemma}
    \label{lemma: convexity}
    Let $F: \mathbb{R}^d \rightarrow \mathbb{R}$ be a convex function, as per Assumption \ref{assumption: convexity-lsmoothness}. Consider a subset of coordinates $\widetilde{\vx} \in \mathbb{R}^{d_1}$ (with $d_1 \leq d$) of the vector $\vx \in \mathbb{R}^d$, and let $\vr \in \mathbb{R}^{d-d_1}$ denote the remaining fixed coordinates. Define 
    \begin{equation}
        F_{\vr}(\widetilde{\vx}) = F(\widetilde{\vx}, \vr). \nonumber 
    \end{equation}
    Then $F_{\vr}$ is convex in $\mathbb{R}^{d_1}$. 
\end{lemma}


\begin{proof}[Proof of Lemma \ref{lemma: convexity}]
    See Appendix \ref{subsection: proof-convexity} 
\end{proof}

With the established $L$-smoothness and convexity parameters of the slimmed models, we can now directly derive the convergence guarantee for Local SGD. For completeness, we present Theorem \ref{theorem: convergence}, which provides a performance guarantee under the given assumptions \cite{stich2018local, wang2021field, khaled2020tighter, woodworth2020local}. 




\begin{theorem}[Performance guarantee]
    \label{theorem: convergence}
    Under the assumptions \ref{assumption: l-smoothness}, \ref{assumption: convexity-lsmoothness}, \ref{assumption: bounded-variance}, \ref{assumption: bounded-dissimilarity}, if the participant learning rate satisfies $\eta \leq \frac{1}{4L}$, then the performance is bounded by 
    \begin{eqnarray}
        \mathbb{E}\left[ \frac{1}{\tau T} \sum_{t=0}^{T-1} \sum_{k=1}^{\tau} F(\overline{\vx}^{t,k}) - F(\vx^{\star}) \right] \nonumber \\ 
        \leq \frac{D^2}{2\eta\tau T} + \frac{\eta\sigma^2}{N} + 4\tau\eta^2L\sigma^2 + 18\tau^2\eta^2L\zeta^2, \nonumber 
    \end{eqnarray}
    where $D \coloneq \|\vx^{0,0} - \vx^{\star}\|$. 
\end{theorem}

\begin{proof}[Proof of Theorem \ref{theorem: convergence}]
    Taking into account Lemmas \ref{lemma: l3}, \ref{lemma: convexity}, \ref{lemma: l1}, and \ref{lemma: l2}, combining them, and telescoping $t$ over $T$ communication rounds gives us the main theorem. 
\end{proof}


\subsection{Fairness Analysis} 
\label{section: fairness-analysis}


\begin{lemma}
    \label{lemma: highest-allocation}
    Let $\va^{\star} = (a_1, a_2, \ldots, a_N)$ be an optimal allocation that minimizes $f(\va)$. Then $\max_{i \in [N]} a_i = u$. In other words, the client with the largest contribution must receive the model with the highest accuracy $u$. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma: highest-allocation}]
    See Appendix \ref{subsection: proof-of-lemma1}. 
\end{proof}


From Lemma \ref{lemma: highest-allocation}, it follows that an optimal allocation $\va^{\star}$ ensures that the highest-performing model is assigned to the highest contributor. 
Furthermore, under the assumption that the model performances are continuous within the interval $[\ell, u]$, we establish that the method achieves a perfect Pearson correlation coefficient (Lemma \ref{lemma: perfect-correlation}). 

\begin{lemma}
    \label{lemma: perfect-correlation}
    Consider $\va^{\star} = (a_1, a_2, \ldots, a_N)$ is the optimal solution that minimizes $f(\va^{\star})$. Then $\rho(\va^{\star}, \vc)=1$ (under the continuous model-performance assumption). 
    
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma: perfect-correlation}]
    See Appendix \ref{subsection: proof-of-lemma2}. 
\end{proof}

This result indicates that the method satisfies the criterion of collaborative fairness \cite{lyu2020collaborative, tastan2025cycle}. 
Although this is theoretically confirmed, in practice, the assumption of continuity is not strictly applicable. Since the width of the model is inherently limited, we must discretize the accuracy range, leading to minor deviations. As a result, we observe that Pearson correlation coefficients are close to 1 if not exactly 1 (Table \ref{table: pearson-correlation-short}). 





\subsection{Convergence of the Allocation Algorithm}
As described in Section \ref{subsection: alloc-alg}, we define a cost function 
\begin{equation}
    f: \mathcal{A} \rightarrow \mathbb{R} 
\end{equation}
which we aim to minimize. Next, we present Theorem \ref{theorem: fair-alloc-convergence}, which states that under assumptions \ref{assumption: finite-state-space}-\ref{assumption: annealing-schedule} (Appendix \ref{subsubsection: alloc-assumptions}), the allocation algorithm asymptotically converges to the global minimizer of $f$ with probability $1$. 


\begin{theorem}[Asymptotic convergence] 
    \label{theorem: fair-alloc-convergence}
    Under Assumptions \ref{assumption: finite-state-space}, \ref{assumption: bounded-function}, \ref{assumption: irreducibility}, \ref{assumption: aperiodicity}, \ref{assumption: annealing-schedule}, define the time-inhomogeneous Markov chain $\{A_k\}$ on $\mathcal{A}$ with transition probabilities 
    \begin{equation}
        P_k(\va \rightarrow \va^{\prime}) = \begin{cases}
            \exp{\left( - \dfrac{f(\va^{\prime}) - f(\va)}{T_k} \right)}, & \hfill f(\va^{\prime}) > f(\va), \nonumber \\ 
            1, & \hfill f(\va^{\prime}) \leq f(\va). 
        \end{cases}
    \end{equation}
    Then for any \textbf{global minimizer} $\va^{\star} \in \mathcal{A}$ (i.e., $f(\va^{\star}) = \min_{\va\in\mathcal{A}} f(\va)$), 
    \begin{equation}
        \lim_{k \rightarrow \infty} P(A_k = \va^{\star}) = 1. \nonumber 
    \end{equation}
    Equivalently, $\{A_k\}$ converges (with probability 1) to the set of global minimizers of $f$. 
\end{theorem}

\begin{proof}[Proof of Theorem \ref{theorem: fair-alloc-convergence}]
    See Appendix \ref{subsection: appendix-fair-alloc}. 
\end{proof}






\section{Experiments and Results} 
\label{section: experiments} 

\subsection{Experimental details} 
\textbf{Datasets and partition settings.} We use the following datasets to carry out our experiments (following \cite{li2020federated}): MNIST \cite{lecun1998mnist}, Fashion-MNIST (FMNIST) \cite{xiao2017fashion}, SVHN \cite{netzer2011reading}, CIFAR-10 \& CIFAR-100 \cite{krizhevsky2009learning}, and Stanford Sentiment Treebank (SST) \cite{socher2013recursive}. The datasets are partitioned using the following strategies: (i) \textbf{homogeneous}, where each participant gets an equal number of data points per class; (ii) \textbf{heterogeneous}, where each client gets a varying number of data points per class based on a Dirichlet($\alpha$) distribution (concentration parameter $\alpha$ reflects the degree of non-i.i.d. characteristics within the dataset); (iii) \textbf{quantity skew} allocates $\kappa$ proportion of total data points to each of the $m$ selected participants and the remaining $N-m$ participants split the remaining data equally; (iv) \textbf{label skew}, denoted by $\#C=m$, creates a label imbalance by sampling $m$ classes for each client and then randomly distributing samples from class $m$ among selected participants. 

\textbf{Baseline approaches.} We compare our approach to a FedAvg algorithm \cite{mcmahan2017communication} followed by an additional epoch of local training to obtain a differentiated model for each participant, CGSV \cite{xu2021gradient} and IAFL \cite{wu2024incentive}. We also evaluate standalone accuracy (SA), where each client trains its ML model on its local dataset without any collaboration with others. For a detailed explanation of the baseline approaches, evaluation metrics, and implementation details, we refer to Appendix \ref{appendix: implementation-details}.



\begin{table*}[t]
    \caption{The predictive performance of our method and other baselines using different dataset partitions. } % The results are averaged over five independent evaluations. 
    \vskip -0.1in
    \label{table: predictive-performance-short}
    \begin{center}
        \begin{small}
            \begin{sc}
                \resizebox{0.875\linewidth}{!}{%
                \begin{tabular}{llrrrr}
\toprule
\rowcolor{lightgray} Partition & Dataset & FedAvg & CGSV & IAFL & Aequa  \\ \midrule % Ours (Appr. I) 
\multirow{6}{*}{Homogeneous} 
& MNIST     & $\mathbf{98.67} \pm 0.07$ & $90.62 \pm 2.32$ & $98.40 \pm 0.13\phantom{0}$ & $\underline{98.60} \pm 0.11$ \\
& FMNIST    & $\underline{89.45} \pm 0.33$ & $77.01 \pm 2.51$ & $88.54 \pm 0.25\phantom{0}$ & $\mathbf{89.63} \pm 0.19$ \\
& SVHN      & $\mathbf{90.54} \pm 0.18$ & $77.61 \pm 3.16$ & $89.64 \pm 0.13\phantom{0}$ & $\underline{90.18} \pm 0.15$ \\
& CIFAR-10  & $\underline{89.99} \pm 0.23$ & $61.29 \pm 2.92$ & $88.42 \pm 0.07\phantom{0}$ & $\mathbf{90.84} \pm 0.26$ \\
& CIFAR-100 & $\underline{65.92} \pm 0.22$ & $35.36 \pm 0.77$ & $63.23 \pm 0.30\phantom{0}$ & $\mathbf{67.83} \pm 0.32$ \\
& SST       & $\underline{34.44} \pm 1.33$ & $30.12 \pm 1.03$ & $34.02 \pm 0.51\phantom{0}$ & $\mathbf{34.44} \pm 1.19$ \\ \midrule 
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.1$)}} 
& MNIST     & $\mathbf{97.38} \pm 0.62$ & $94.06 \pm 1.79$ & $88.45 \pm 7.87\phantom{0}$ & $\underline{97.30} \pm 0.58$ \\
& FMNIST    & $\underline{83.32} \pm 1.78$ & $71.51 \pm 8.20$ & $66.46 \pm 4.72\phantom{0}$ & $\mathbf{84.60} \pm 1.32$ \\
& SVHN      & $\mathbf{86.38} \pm 0.87$ & $72.48 \pm 4.81$ & $68.71 \pm 7.85\phantom{0}$ & $\underline{86.33} \pm 1.00$ \\
& CIFAR-10  & $\underline{74.73} \pm 3.65$ & $48.77 \pm 5.02$ & $46.36 \pm 9.31\phantom{0}$ & $\mathbf{75.97} \pm 3.36$ \\
& CIFAR-100 & $\underline{61.16} \pm 0.25$ & $34.16 \pm 1.63$ & $43.38 \pm 4.53\phantom{0}$ & $\mathbf{63.42} \pm 0.54$ \\
& SST       & $\underline{32.17} \pm 1.60$ & $21.54 \pm 1.89$ & $27.28 \pm 3.06\phantom{0}$ & $\mathbf{33.54} \pm 1.48$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.15, 6)$}} 
& MNIST     & $\mathbf{98.69} \pm 0.10$ & $93.22 \pm 0.99$ & $98.40 \pm 0.12\phantom{0}$ & $\underline{98.62} \pm 0.09$ \\
& FMNIST    & $\underline{89.53} \pm 0.24$ & $78.73 \pm 2.14$ & $88.53 \pm 0.26\phantom{0}$ & $\mathbf{89.72} \pm 0.17$ \\
& SVHN      & $\mathbf{90.59} \pm 0.17$ & $77.54 \pm 2.34$ & $89.51 \pm 0.13\phantom{0}$ & $\underline{90.26} \pm 0.16$ \\
& CIFAR-10  & $\underline{90.00} \pm 0.13$ & $66.89 \pm 1.85$ & $89.51 \pm 0.15\phantom{0}$ & $\mathbf{90.71} \pm 0.14$ \\
& CIFAR-100 & $\underline{65.88} \pm 0.38$ & $39.62 \pm 0.74$ & $64.60 \pm 0.25\phantom{0}$ & $\mathbf{68.24} \pm 0.11$ \\
& SST       & $\underline{34.26} \pm 0.98$ & $29.53 \pm 1.02$ & $33.70 \pm 0.70\phantom{0}$ & $\mathbf{34.64} \pm 1.01$ \\ \midrule
\multirow{6}{*}{Label Skew: \#OC=\{3, 30\}} 
& MNIST     & $\underline{94.37} \pm 3.43$ & $79.19 \pm 7.94$ & $73.10 \pm 15.00$ & $\mathbf{95.37} \pm 1.15$ \\
& FMNIST    & $\underline{79.73} \pm 3.80$ & $61.54 \pm 8.03$ & $60.10 \pm \phantom{0}8.03$  & $\mathbf{80.51} \pm 3.27$ \\
& SVHN      & $\underline{79.73} \pm 5.89$ & $64.07 \pm 7.65$ & $55.83 \pm 11.89$ & $\mathbf{80.69} \pm 6.05$ \\
& CIFAR-10  & $\underline{71.88} \pm 3.28$ & $48.02 \pm 3.88$ & $44.12 \pm 21.15$ & $\mathbf{72.40} \pm 3.17$ \\
& CIFAR-100 & $\underline{60.95} \pm 1.18$ & $35.09 \pm 0.42$ & $55.26 \pm \phantom{0}3.85$  & $\mathbf{62.84} \pm 1.18$ \\
& SST       & $\mathbf{33.96} \pm 0.35$ & $24.88 \pm 2.04$ & $30.33 \pm \phantom{0}1.68$  & $\underline{33.01} \pm 0.90$ \\ \midrule 
\multicolumn{2}{c}{Number of times that performs the best } & $7/24$ & $0/24$ & $0/24$ & $\mathbf{17/24}$ \\ \bottomrule 
                
                \end{tabular}
                }
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*}



\subsection{Predictive performance} 
We begin by benchmarking existing algorithms across all six datasets listed above, using the partitioning strategies detailed in Table \ref{table: predictive-performance-short}. Due to space constraints, we present a summarized version of the results, while the full details can be found in Table \ref{table: predictive-performance}, Appendix \ref{appendix: predict-performance}. We report the accuracy (balanced accuracy) of the global model as the evaluation metric. 
As observed in the table, our approach outperforms all other methods, achieving the best results in $17$ out of $24$ cases. The FedAvg algorithm performs slightly better than our method in 7 out of 24 cases. Meanwhile, the fairness-based approaches, CGSV and IAFL, exhibit lower performance, particularly in heterogeneous settings such as Heterogeneous (Dirichlet $(\alpha=0.1)$) and Label skew ($\#C=3$). This is due to their emphasis on balancing model performance with fairness, which limits their effectiveness in these scenarios. However, in our case, our algorithm is capable of handling all the data splits, consistently performing better or on par with FedAvg, thereby validating the performance guarantee established in our analysis. We also provide per-participant performance analysis on the CIFAR-100 dataset in Appendix \ref{appendix: per-participant-performance}. 


\begin{table}[t]
    \caption{Comparison to baseline methods when the contribution measure corresponds to the participation rate.} 
    \vskip -0.1in 
    \label{table: contribution-participation-rate} 
    \begin{center}
        \begin{small}
            \begin{sc}
                \resizebox{\linewidth}{!}{%
                    \begin{tabular}{lcccc}
\toprule
\multirow{2.5}{*}{Method} & \multicolumn{2}{c}{MNIST}                          & \multicolumn{2}{c}{CIFAR-10}                       \\ \cmidrule{2-5}
                          & $\rho$                  & Acc.                     & $\rho$                  & Acc.                     \\ \midrule 
FedAvg-FT                 & $0.12 \pm 0.1$          & $97.65 \pm 0.3$          & $0.11 \pm 0.1$          & $83.73 \pm 0.3$          \\
CGSV                      & $0.50 \pm 0.2$          & $96.18 \pm 0.7$          & $0.55 \pm 0.1$          & $55.97 \pm 0.3$          \\
IAFL                      & $0.79 \pm 0.0$          & $98.04 \pm 0.2$          & $0.69 \pm 0.2$          & $82.58 \pm 0.1$          \\ \midrule 
\rowcolor{lightblue} Aequa & $\mathbf{0.98 \pm 0.0}$ & $\mathbf{98.19 \pm 0.1}$ & $\mathbf{0.99 \pm 0.0}$ & $\mathbf{84.59 \pm 0.2}$ \\ 
\bottomrule 
    
                    \end{tabular}
                }
            \end{sc}
        \end{small}
    \end{center}
    \vskip -0.25in
\end{table}


\subsection{Incentivization performance} 
\label{subsection: fairness-results}
\textbf{Correlation to contribution.} As noted in \cite{wu2024incentive}, one way to quantify incentivization performance is by measuring the Pearson correlation coefficient $(\rho)$ between the client model accuracies achieved after $T$ communication rounds and their standalone accuracies. This metric also serves as a measure of fairness, as highlighted in \cite{xu2021gradient, lyu2020collaborative}. Following IAFL \cite{wu2024incentive}, we use standalone accuracies as a surrogate for client contributions. Thus, by analyzing the correlation values, we can directly compare the incentivization effectiveness of different algorithms. From the results presented in Table \ref{table: pearson-correlation-short}, we observe that FedAvg-FT struggles to establish a strong correlation, often yielding negative values, particularly under the Quantity skew. Meanwhile, CGSV and IAFL underperform in a homogeneous setting, where client standalone accuracies exhibit minimal variation. While IAFL demonstrates good performance under Quantity skew, our approach achieves more precise and consistently superior performance across all settings, excelling in $24/24$ cases. For a comprehensive analysis, we refer the reader to Figure \ref{table: pearson-correlation}, Appendix \ref{subsection: appendix-pearson}, where we present results across $54$ different scenarios. Notably, our method outperforms all other approaches in all $54$ cases, achieving a perfect score of $54/54$. 

\textbf{Collaboration gain spread.} 
We evaluate the collaboration gain spread (CGS) achieved by Aequa in comparison to other baseline methods. As shown in Table \ref{table: mcg-cgs} in Appendix \ref{appendix: mcg-cgs}, our approach consistently outperforms other methods in most of the settings, in $42/54$ cases. 
One scenario where Aequa underperforms compared to IAFL is in the quantity skew setting. This is due to the choice of $p_{\min}$, which determines the lowest accuracy $\ell$ assigned to low-contributing clients. To address this, we conduct an additional set of experiments, adjusting $p_{\min}$ to $0.1$, as presented in Table \ref{table: mcg-cgs-2}. Under this configuration, Aequa outperforms other methods in $11/12$ cases, overall making it $48/54$ cases. 



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/cifar10-mnist-pp.pdf}
    \caption{Visualization of the correlation when contribution measure corresponds to the participation rate on CIFAR-10 and MNIST datasets with $N=50$ participants.}
    \label{fig: cifar10-mnist-pp}
    \vskip -0.185in 
\end{figure}





\begin{table*}[t]
    \caption{Incentivization performance comparison of our method and baseline approaches across different dataset partitions, evaluated using the Pearson correlation coefficient between the accuracies of the final model and the accuracies of the standalone model. The results are averaged over five independent evaluations. For complete results, refer to Appendix \ref{subsection: appendix-pearson}.} % , Table \ref{table: pearson-correlation}. 
    \vskip -0.1in
    \label{table: pearson-correlation-short}
    \begin{center}
        \begin{small}
            \begin{sc}
                \resizebox{0.87\linewidth}{!}{%
                \begin{tabular}{llrrrr}
\toprule
\rowcolor{lightgray} Partition & Dataset & \multicolumn{1}{c}{FedAvg-FT} & \multicolumn{1}{c}{CGSV} & \multicolumn{1}{c}{IAFL} & Aequa  \\ \midrule
\multirow{6}{*}{Homogeneous} 
& MNIST     & $0.07  \pm 0.24$ & $-0.30 \pm 0.16$ & $0.16  \pm 0.20$ & $\mathbf{0.97 \pm 0.02}$ \\
& FMNIST    & $0.21  \pm 0.07$ & $-0.09 \pm 0.35$ & $0.27  \pm 0.24$ & $\mathbf{0.98 \pm 0.02}$ \\
& SVHN      & $-0.12 \pm 0.36$ & $0.02  \pm 0.17$ & $0.07  \pm 0.28$ & $\mathbf{0.98 \pm 0.02}$ \\
& CIFAR-10  & $0.04  \pm 0.27$ & $0.05  \pm 0.42$ & $-0.01 \pm 0.15$ & $\mathbf{0.99 \pm 0.01}$ \\
& CIFAR-100 & $-0.07 \pm 0.37$ & $-0.19 \pm 0.36$ & $0.02  \pm 0.31$ & $\mathbf{0.96 \pm 0.01}$ \\
& SST       & $0.06  \pm 0.25$ & $-0.07 \pm 0.32$ & $-0.03 \pm 0.26$ & $\mathbf{0.98 \pm 0.01}$ \\ \midrule 
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.1$)}} 
& MNIST     & $0.39  \pm 0.37$ & $0.56  \pm 0.18$ & $0.61  \pm 0.23$ & $\mathbf{0.85 \pm 0.03}$ \\
& FMNIST    & $0.21  \pm 0.41$ & $0.56  \pm 0.28$ & $0.61  \pm 0.25$ & $\mathbf{0.89 \pm 0.06}$ \\
& SVHN      & $0.66  \pm 0.24$ & $0.21  \pm 0.46$ & $0.80  \pm 0.18$ & $\mathbf{0.92 \pm 0.02}$ \\
& CIFAR-10  & $-0.18 \pm 0.35$ & $0.67  \pm 0.14$ & $0.84  \pm 0.15$ & $\mathbf{0.94 \pm 0.02}$ \\
& CIFAR-100 & $-0.17 \pm 0.46$ & $0.30  \pm 0.52$ & $0.89  \pm 0.07$ & $\mathbf{0.99 \pm 0.01}$ \\
& SST       & $-0.20 \pm 0.35$ & $0.14  \pm 0.50$ & $0.91  \pm 0.09$ & $\mathbf{0.98 \pm 0.01}$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.15, 6)$}} 
% \multirow{6}{*}{Imbalanced (0.15, 6)} 
& MNIST     & $-0.63 \pm 0.18$ & $0.34  \pm 0.80$ & $0.95  \pm 0.04$ & $\mathbf{0.98 \pm 0.01}$ \\
& FMNIST    & $-0.45 \pm 0.28$ & $0.49  \pm 0.71$ & $0.93  \pm 0.02$ & $\mathbf{0.98 \pm 0.01}$ \\
& SVHN      & $-0.76 \pm 0.12$ & $0.42  \pm 0.75$ & $0.99  \pm 0.01$ & $\mathbf{1.00 \pm 0.00}$ \\
& CIFAR-10  & $-0.37 \pm 0.16$ & $0.98  \pm 0.02$ & $0.99  \pm 0.00$ & $\mathbf{1.00 \pm 0.00}$ \\
& CIFAR-100 & $0.06  \pm 0.40$ & $0.97  \pm 0.03$ & $\mathbf{1.00  \pm 0.00}$ & $\mathbf{1.00 \pm 0.00}$ \\
& SST       & $-0.07 \pm 0.48$ & $-0.23 \pm 0.54$ & $0.90  \pm 0.02$ & $\mathbf{0.94 \pm 0.04}$ \\ \midrule
\multirow{6}{*}{Label Skew: \#OC=\{3, 30\}} 
& MNIST     & $0.03  \pm 0.41$ & $-0.27 \pm 0.27$ & $0.23  \pm 0.24$ & $\mathbf{0.81 \pm 0.13}$ \\
& FMNIST    & $-0.44 \pm 0.27$ & $0.11  \pm 0.45$ & $0.08  \pm 0.26$ & $\mathbf{0.99 \pm 0.01}$ \\
& SVHN      & $0.43  \pm 0.25$ & $-0.43 \pm 0.42$ & $0.01  \pm 0.25$ & $\mathbf{0.98 \pm 0.00}$ \\
& CIFAR-10  & $0.19  \pm 0.32$ & $0.12  \pm 0.32$ & $0.22  \pm 0.38$ & $\mathbf{0.97 \pm 0.02}$ \\
& CIFAR-100 & $-0.38 \pm 0.22$ & $0.00 \pm 0.24$  & $0.31  \pm 0.22$ & $\mathbf{0.98 \pm 0.02}$ \\
& SST       & $0.45  \pm 0.38$ & $-0.19 \pm 0.48$ & $0.48  \pm 0.37$ & $\mathbf{0.97 \pm 0.03}$ \\ \midrule 
\multicolumn{2}{c}{Number of times that performs the best } & $0/24$ & $0/24$ & $1/24$ & $\mathbf{24/24}$   \\ \bottomrule 


\end{tabular}
}
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*} 


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/cifar10-homogeneous-combined-plots.pdf}
    \caption{Performance comparison of our proposed methods on CIFAR-10 under a homogeneous partitioning strategy. }
    \label{fig: cifar10-homogeneous-combined-plots}
    \vskip -0.16in
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/cifar10-imbalanced-combined-plots.pdf}
    \caption{Performance comparison of our proposed methods on CIFAR-10 under a quantity skew. } 
    \label{fig: cifar10-imbalanced-combined-plots}
    \vskip -0.155in
\end{figure}




\subsection{Alternative contribution measures.} 
Aequa is designed to be flexible in incorporating various contribution measures. To demonstrate its effectiveness under partial participation, we use the client participation rate as the contribution measure. Following the experimental setup studied in \cite{wu2024incentive}, we define the participation rate as $r_i = 0.5 \times (1 + i/N)$, as illustrated in Figure \ref{fig: cifar10-mnist-pp}. To evaluate our approach, we conduct experiments on two datasets using two different architectures. The results indicate that Aequa achieves a very high  Pearson correlation coefficient, demonstrating a strong alignment between participation rates and model performance (see Figure \ref{fig: cifar10-mnist-pp}). Table \ref{table: contribution-participation-rate} presents a comparative analysis against other baseline algorithms, confirming that Aequa consistently outperforms the baselines in both correlation and overall performance. The second-best performing algorithm in this setting is IAFL. 


\subsection{Training-time model rewards.} 
We evaluate our method in combination with existing contribution assessment algorithms for training-time model rewards, comparing against baseline methods. The results, presented in Figures \ref{fig: cifar10-homogeneous-combined-plots} and \ref{fig: cifar10-imbalanced-combined-plots}, illustrate the performance on the CIFAR-10 dataset under homogeneous and quantity skew partitioning strategies. 
The plot presents all evaluation metrics, and the results clearly demonstrate that Aequa consistently outperforms other approaches across all cases. 


\section{Conclusion and Limitation} 
\label{section: conclusion} 
Using the concept of slimmable networks, we have presented a framework for achieving fair model rewards in collaborative learning. While the proposed approach achieves collaborative fairness, it has one key limitation, which is the need for TEEs for confidential computation of local updates. 




\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Mathematical proofs} 



\subsection{Proof of Lemma \ref{lemma: l3}} 
\label{subsection: proof-l-smoothness} 


\begin{proof}[Proof of Lemma \ref{lemma: l3}] 
    Since $F$ is $L$-smooth over $\mathbb{R}^d$, for any $\vx, \vx^{\prime} \in \mathbb{R}^d$ we have 
    \begin{eqnarray}
        \| \nabla F(\vx) - \nabla F(\vx^{\prime}) \| \leq L \| \vx - \vx^{\prime} \|.
    \end{eqnarray}

    Let $\widetilde{\vx}, \widetilde{\vx}^{\prime} \in \mathbb{R}^{d_1}$ be two points in the reduced space of selected coordinates. We embed each into $\mathbb{R}^d$ by defining 
    \begin{equation}
        \vx = \left( \widetilde{\vx}, \vr \right), \quad \vx^{\prime} = \left( \widetilde{\vx}^{\prime}, \vr \right), 
    \end{equation}
    where $\vr$ is held fixed in both cases (e.g. zeros in our case). Note that 
    \begin{equation}
        \| \vx - \vx^{\prime} \| = \| (\widetilde{\vx}, \vr) - (\widetilde{\vx}^{\prime}, \vr) \| = \| \widetilde{\vx} - \widetilde{\vx}^{\prime} \|. 
    \end{equation}

    Next, write the gradient of $F$ in coordinates:  
    \begin{eqnarray}
        \nabla F(\vx) = (\nabla_{\widetilde{\vx}} F(\vx), \nabla_{\vr} F(\vx)), \quad \nabla F(\vx^{\prime}) = (\nabla_{\widetilde{\vx}^{\prime}} F(\vx^{\prime}), \nabla_{\vr} F(\vx^{\prime})), 
    \end{eqnarray} 

    Because $\vx$ and $\vx^{\prime}$ differ only in the $\widetilde{\vx}$-coordinates (the $\vr$-part is the same), it follows that 
    \begin{equation}
        \| \nabla F_{\vr}(\widetilde{\vx}) - \nabla F_{\vr}(\widetilde{\vx}^{\prime}) \|^2 = \left\|\nabla_{\widetilde{\vx}} F(\vx) - \nabla_{\widetilde{\vx}^{\prime}} F(\vx^{\prime})) \right\|^2 \leq \| \nabla F(\vx) - \nabla F(\vx^{\prime}) \|^2.
    \end{equation}

    By the $L$-smoothness of $F$ in $\mathbb{R}^d$ and putting all together, we have 
    \begin{equation}
        \|\nabla F_{\vr}(\widetilde{\vx}) - \nabla F_{\vr}(\widetilde{\vx}^{\prime})\| = \|\nabla_{\widetilde{\vx}} F(\widetilde{\vx}) - \nabla_{\widetilde{\vx}^{\prime}} F(\widetilde{\vx}^{\prime})\| \leq \| \nabla F(\vx) - \nabla F(\vx^{\prime}) \| \leq L \| \vx - \vx^{\prime} \| = L \| \widetilde{\vx} - \widetilde{\vx}^{\prime} \|.  
    \end{equation}

    Hence $F_{\vr}$ (the restriction of $F$ to the subset $\widetilde{\vx}$) is also $L$-smooth with respect to $\widetilde{\vx}$. In particular, its smoothness constant $\widetilde{L}$ satisfies $\widetilde{L} \leq L$. 

    This concludes the proof. 
    
\end{proof} 



\subsection{Proof of Lemma \ref{lemma: convexity}} 
\label{subsection: proof-convexity} 


\begin{proof}[Proof of Lemma \ref{lemma: convexity}]
    Since $F$ is convex over $\mathbb{R}^d$, for any two points $\vx, \vy \in \mathbb{R}^d$ and any $\lambda \in [0,1]$, 
    \begin{equation}
        F(\lambda \vx + (1-\lambda) \vy) \leq \lambda F(\vx) + (1-\lambda) F(\vy). 
    \end{equation}
    Now fix $\vr \in \mathbb{R}^{d-d_1}$. 

    Let $\widetilde{\vx}, \widetilde{\vx}^{\prime} \in \mathbb{R}^{d_1}$ and $\lambda \in [0,1]$. Then, 
    \begin{equation}
        F_{\vr} (\lambda \widetilde{\vx} + (1-\lambda) \widetilde{\vx}^{\prime}) = F(\lambda (\widetilde{\vx}, \vr) + (1-\lambda) (\widetilde{\vx}^{\prime}, \vr)). 
    \end{equation}

    Since $\vr$ is fixed, the interpolation in $\mathbb{R}^d$ satisfies 
    \begin{equation}
        \lambda (\widetilde{\vx}, \vr) + (1-\lambda) (\widetilde{\vx}^{\prime}, \vr) = \left( \lambda \widetilde{\vx} + (1-\lambda) \widetilde{\vx}^{\prime}, \vr \right). 
    \end{equation}

    By convexity of $F$ in $\mathbb{R}^d$, 
    \begin{equation}
        F \left( \lambda (\widetilde{\vx}, \vr) + (1-\lambda) (\widetilde{\vx}^{\prime}, \vr) \right) \leq \lambda F(\widetilde{\vx}, \vr) + (1-\lambda) F(\widetilde{\vx}^{\prime}, \vr). 
    \end{equation} 

    Rewriting in terms of $F_{\vr}$, 
    \begin{equation}
        F_{\vr} (\lambda \widetilde{\vx} + (1-\lambda) \widetilde{\vx}^{\prime}) \leq \lambda F_{\vr}(\widetilde{\vx}) + (1-\lambda) F_{\vr}(\widetilde{\vx}^{\prime}). 
    \end{equation}

    Hence $F_{\vr}$ is convex in $\mathbb{R}^{d_1}$. This completes the proof. 
    
\end{proof}



\subsection{Proof of Lemma \ref{lemma: highest-allocation}}
\label{subsection: proof-of-lemma1}

\begin{proof}
    Suppose, for contradiction, that in the optimal allocation $\va^{\star}$, we have $a^{\star}_N < u$. Let $\alpha = u - a_N^{\star} > 0$. Because the set of achievable accuracies is assumed to be \textit{continuous} on $[\ell, u]$, we can attempt to shift all the assigned accuracies in $\va^{\star}$ upward by $\alpha$. Define the new allocation $\widetilde{\va} = (\widetilde{a}_1, \widetilde{a}_2, \ldots, \widetilde{a}_N)$ by $\widetilde{\va} = \va^{\star} + \alpha \mathbf{1} \in \mathcal{A}$. 

    Then, this gives us 
    \begin{eqnarray}
        f(\widetilde{\va}) &=& f(\va^{\star} + \alpha \mathbf{1}) = - \frac{\mathbb{E}[u(\va^{\star})] + \alpha}{\text{Var}[u(\va^{\star})] + \epsilon} \nonumber \\ 
        &<& - \frac{\mathbb{E}[u(\va^{\star})]}{\text{Var}[u(\va^{\star})] + \epsilon} = f(\va^{\star}), \nonumber 
    \end{eqnarray}

    contradicting the optimality of $\va^{\star}$. Thus, we must have $\alpha=0$ and therefore $\va^{\star}_N = u$. 
    
\end{proof}


\subsection{Proof of Lemma \ref{lemma: perfect-correlation}}
\label{subsection: proof-of-lemma2}

\begin{proof} 
    Since model performances are continuous on the interval $[\ell, u]$ and that $a^{\star}_N = u$, we define $\alpha = u - c_N > 0$. By construction, for every client $i$, the allocation satisfies $a^{\star}_i = c_i + \alpha$. 
    Consequently, the Pearson correlation coefficient is computed as: 
    \begin{equation}
        \rho(\va^{\star}, \vc) = \rho(\vc + \alpha \mathbf{1}, \vc) = \rho(\vc, \vc) = 1.
    \end{equation}

    This proves the lemma. 
    
\end{proof}



\subsection{Deferred Lemmas} 

\subsubsection{Assumptions} 
\label{subsection: deferred-lemmas-assumptions}

\begin{assumption}[Expected stochastic gradient variance] 
    \label{assumption: bounded-variance}
    The variance of an unbiased stochastic gradient in participant is $\sigma^2$-uniformly bounded in $L_2$ norm, $\forall i\in [N], \forall k \in [\tau], \forall t \in [T]$, 
    \begin{eqnarray}
        \mathbb{E}\left[ g_i(\vx_i^{t,k}) \mid \vx_i^{t,k} \right] = \nabla F_i(\vx_i^{t,k}), \nonumber \\ 
        \mathbb{E} \left[ \Big\| g_i(\vx_i^{t,k}) - \nabla F_i(\vx_i^{t,k}) \Big\|^2 \middle| \vx_i^{t,k} \right] \leq \sigma^2. \nonumber
    \end{eqnarray}
\end{assumption}


\begin{assumption}[Gradient dissimilarity]
    \label{assumption: bounded-dissimilarity}
    The difference of local gradient $\nabla F_i(\vx)$ and the global gradient $\nabla F(\vx)$ is $\zeta$-uniformly bounded in $L_2$ norm, $\forall i\in [N], \forall k \in [\tau], \forall t \in [T]$, 
    \begin{eqnarray}
        \max_{i} \sup_{\vx} \Big\| \nabla F_i (\vx_i^{t,k}) - \nabla F(\vx_i^{t,k}) \Big\| \leq \zeta. \nonumber
    \end{eqnarray}
\end{assumption}


\subsubsection{Lemmas} 

\begin{lemma}
    \label{lemma: l1}
    Assuming the participant learning rate satisfies $\eta \leq \frac{1}{4L}$, then 
    \begin{eqnarray}
        \mathbb{E} \left[ \frac{1}{\tau} \sum_{k=1}^{\tau} F(\overline{\vx}^{t,k}) - F(\vx^{\star}) \middle| \mathcal{F}^{t,0} \right] \leq \frac{1}{2 \eta \tau} \left( \Big\| \overline{\vx}^{t,0} - \vx^{\star} \Big\|^2 - \mathbb{E} \left[ \Big\| \overline{\vx}^{t,\tau} - \vx^{\star}\Big\|^2 \middle| \mathcal{F}^{t,0} \right] \right) \nonumber \\ 
        + \frac{\eta \sigma^2}{N} + \frac{L}{N \tau} \sum_{i=1}^{N} \sum_{k=0}^{\tau-1} \mathbb{E} \left[ \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 \middle| \mathcal{F}^{t,0} \right], \nonumber 
    \end{eqnarray} 
    where $\mathcal{F}^{t,0}$ is the $\sigma$-field representing all the historical information up to the start of the $t$-th round. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma: l1}]
    \begin{eqnarray}
        \frac{1}{N} \sum_{i=1}^N \Big\langle g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle = \Big\langle -\frac{1}{\eta} \left( \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \right), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle \label{eq: l1-update} \\ 
        = \frac{1}{2\eta} \left( \Big\| \overline{\vx}^{t,k} - \vx^{\star} \Big\|^2 - \Big\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\|^2 - \Big\| \overline{\vx}^{t,k+1} - \vx^{\star} \Big\|^2 \right). \label{eq: parallelogram-law}
    \end{eqnarray}
    where (\ref{eq: l1-update}) uses the update rule $\overline{\vx}^{t,k+1} = \overline{\vx}^{t,k} - \eta \dfrac{1}{N} \sum_{i=1}^N g_i(\vx_i^{t,k})$, (\ref{eq: parallelogram-law}) uses the parallelogram law, which is $\langle \vu, \vv \rangle = \dfrac{1}{2} \left( \|\vu\|^2 + \|\vv\|^2 - \|\vu-\vv\|^2 \right), \forall \vu, \vv \in \mathbb{R}^d$.

    By Assumptions \ref{assumption: l-smoothness} and \ref{assumption: convexity-lsmoothness}, we have: 
    \begin{eqnarray}
        F_i(\overline{\vx}^{t,k+1}) \leq F_i(\vx_i^{t,k}) + \Big\langle \nabla F_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx_i^{t,k} \Big\rangle + \frac{L}{2} \Big\| \overline{\vx}^{t,k+1} - \vx_i^{t,k} \Big\|^2 \label{eq: l-smoothness} \\ 
        \leq F_i(\vx^{\star}) + \Big\langle \nabla F_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle +\frac{L}{2} \Big\| \overline{\vx}^{t,k+1} - \vx_i^{t,k} \Big\|^2 \label{eq: 3-points-descent} \\ 
        = F_i(\vx^{\star}) + \Big\langle \nabla F_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle +\frac{L}{2} \Big\| (\overline{\vx}^{t,k+1} - \overline{\vx}^{t,k}) - (\vx_i^{t,k} - \overline{\vx}^{t,k}) \Big\|^2 \label{eq: rearrange-5} \\ 
        \leq F_i(\vx^{\star}) + \Big\langle \nabla F_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle + L \Big\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\|^2 + L \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 \label{eq: ineq-sqr}
    \end{eqnarray}
    where (\ref{eq: l-smoothness}) uses the $L$-smoothness property (see Assumption \ref{assumption: l-smoothness}), (\ref{eq: 3-points-descent}) uses the three points descent lemma, which holds true when $F_i(\vx)$ is both convex and $L$-smooth (see Assumption \ref{assumption: convexity-lsmoothness}), (\ref{eq: rearrange-5}) includes the addition and subtraction of $\overline{\vx}^{t,k}$ to the third term, and (\ref{eq: ineq-sqr}) applies this inequality $\|\vu+\vv\|^2 \leq 2 (\|\vu\|^2 + \|\vv\|^2), \forall \vu, \vv \in \mathbb{R}^d$. 

    By combining (\ref{eq: parallelogram-law}) and (\ref{eq: ineq-sqr}), we get: 
    \begin{eqnarray}
        F(\overline{\vx}^{t,k+1}) - F(\vx^{\star}) = \frac{1}{N} \sum_{i=1}^N \Big( F_i(\overline{\vx}^{t,k+1}) - F(\vx^{\star}) \Big) \\ 
        \leq \frac{1}{N} \sum_{i=1}^N \Big\langle \nabla F_i(\vx_i^{t,k}) - g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle + L \Big\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\|^2 \nonumber \\ 
        + \frac{L}{N} \sum_{i=1}^N \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 + \frac{1}{N} \sum_{i=1}^N \Big\langle g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle \label{eq: add-subtract-gi} \\
        = \frac{1}{N} \sum_{i=1}^N \Big\langle \nabla F_i(\vx_i^{t,k}) - g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle + L \Big\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\|^2 \nonumber \\ 
        + \frac{L}{N} \sum_{i=1}^N \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 + \frac{1}{2\eta} \left( \Big\| \overline{\vx}^{t,k} - \vx^{\star} \Big\|^2 - \Big\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\|^2 - \Big\| \overline{\vx}^{t,k+1} - \vx^{\star} \Big\|^2 \right) \label{eq: subs-eq6}
    \end{eqnarray}
    where (\ref{eq: add-subtract-gi}) adds and subtracts $\dfrac{1}{N} \sum_{i=1}^N \Big\langle g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle$, (\ref{eq: subs-eq6}) replaces the last term with (\ref{eq: parallelogram-law}). 

    Since $\mathbb{E} \left[ \nabla F_i(\vx_i^{t,k}) - g_i(\vx_i^{t,k}) \middle| \mathcal{F}^{t,k} \right] = 0$ we have 
    \begin{eqnarray}
        \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^N \Big\langle \nabla F_i(\vx_i^{t,k}) - g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \vx^{\star} \Big\rangle \middle| \mathcal{F}^{t,k} \right] \\ 
        = \mathbb{E} \left[ \frac{1}{N} \sum_{i=1}^N \Big\langle \nabla F_i(\vx_i^{t,k}) - g_i(\vx_i^{t,k}), \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\rangle \middle| \mathcal{F}^{t,k} \right] \\ 
        \leq \eta \mathbb{E} \left[ \left\| \frac{1}{N} \sum_{i=1}^N \left( \nabla F_i(\vx_i^{t,k}) - g_i(\vx_i^{t,k}) \right) \right\|^2 \middle| \mathcal{F}^{t,k} \right] + \frac{1}{4\eta} \mathbb{E} \left[ \left\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \right\|^2 \middle| \mathcal{F}^{t,k} \right] \label{eq: young-ineq} \\ 
        \leq \frac{\eta\sigma^2}{N} + \frac{1}{4\eta} \mathbb{E} \left[ \left\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \right\|^2 \middle| \mathcal{F}^{t,k} \right] \label{eq: bounded-covariance}, 
    \end{eqnarray}
    where (\ref{eq: young-ineq}) uses Young's inequality, which is $\langle \vu, \vv \rangle \leq \epsilon \|\vu\|^2 + \dfrac{1}{4\epsilon} \|\vv\|^2, \forall \epsilon>0$ and $\forall \vu, \vv \in \mathbb{R}^d$, and (\ref{eq: bounded-covariance}) uses bounded covariance assumption (see Assumption \ref{assumption: bounded-variance}) and independence across clients. 

    By plugging (\ref{eq: bounded-covariance}) back to the conditional expectation of (\ref{eq: subs-eq6}) with $\eta \leq \frac{1}{4L}$, we get: 
    \begin{eqnarray}
        \mathbb{E} \left[ F(\overline{\vx}^{t,k+1}) - F(\vx^{\star}) \right] + \frac{1}{2\eta} \left( \mathbb{E} \left[ \Big\| \overline{\vx}^{t,k+1} - \vx^{\star} \Big\|^2 \middle| \mathcal{F}^{t,k} \right] - \Big\| \overline{\vx}^{t,k} - \vx^{\star} \Big\|^2 \right) \nonumber \\ 
        \leq \frac{\eta\sigma^2}{N} - \left( \frac{1}{4\eta} - L \right) \mathbb{E} \left[ \Big\| \overline{\vx}^{t,k+1} - \overline{\vx}^{t,k} \Big\|^2 \middle| \mathcal{F}^{t,k} \right] + \frac{L}{N} \sum_{i=1}^N \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 \\ 
        \leq \frac{\eta\sigma^2}{N} + \frac{L}{N} \sum_{i=1}^N \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 \label{eq: lr-condition}
    \end{eqnarray}
    where (\ref{eq: lr-condition}) holds true since $\eta \leq \frac{1}{4L}$. 

    Telescoping $k$ from $0$ to $\tau$ gives us: 
    \begin{eqnarray}
        \mathbb{E} \left[ \frac{1}{\tau} \sum_{k=1}^{\tau} F(\overline{\vx}^{t,k}) - F(\vx^{\star}) \middle| \mathcal{F}^{t,0} \right] \leq \frac{1}{2 \eta \tau} \left( \Big\| \overline{\vx}^{t,0} - \vx^{\star} \Big\|^2 - \mathbb{E} \left[ \Big\| \overline{\vx}^{t,\tau} - \vx^{\star}\Big\|^2 \middle| \mathcal{F}^{t,0} \right] \right) \nonumber \\ 
        + \frac{\eta \sigma^2}{N} + \frac{L}{N \tau} \sum_{i=1}^{N} \sum_{k=0}^{\tau-1} \mathbb{E} \left[ \Big\| \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\|^2 \middle| \mathcal{F}^{t,0} \right], \nonumber 
    \end{eqnarray} 
    which completes the proof. 
    
\end{proof} 

\begin{lemma}
    \label{lemma: l2}
    Assuming the client learning rate satisfies $\eta \leq \frac{1}{4L}$, then 
    \begin{eqnarray}
        \mathbb{E} \left[ \Big\lVert \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\rVert^2 \middle| \mathcal{F}^{t,0} \right] \leq 18\tau^2 \eta^2 \zeta^2 + 4\tau\eta^2\sigma^2, \nonumber
    \end{eqnarray}
    where $\mathcal{F}^{t,0}$ is the $\sigma$-field representing all the historical information up to the start of the $t$-th round. 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lemma: l2}]
    \begin{eqnarray}
        \mathbb{E} \left[ \left\| \vx_1^{t,k+1} - {\vx_2^{t,k+1}} \right\|^2 \middle| \mathcal{F}^{t,k} \right] = \mathbb{E} \left[ \left\| \vx_1^{t,k} - {\vx_2^{t,k}} - \eta \left( g_1(\vx_1^{t,k}) - g_2(\vx_2^{t,k})  \right) \right\|^2 \middle| \mathcal{F}^{t,k} \right] \\ 
        \leq \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\|^2 
        - 2 \eta  \left\langle  \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k})  , \vx_1^{t,k} - {\vx_2^{t,k}} \right\rangle \nonumber \\ 
        + \eta^2  \left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k})  \right\|^2 + 2 \eta^2 \sigma^2 \label{eq: l2-2}
    \end{eqnarray}
    where the last term in (\ref{eq: l2-2}) is from Assumption \ref{assumption: bounded-variance} for both $g_1(\vx_1^{t,k})$ and $g_2(\vx_2^{t,k})$. 

    Following Assumption \ref{assumption: bounded-dissimilarity}, the second term of (\ref{eq: l2-2}) is bounded as 
    \begin{eqnarray}
        - \left\langle  \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k})  , \vx_1^{t,k} - {\vx_2^{t,k}} \right\rangle \leq - \left\langle  \nabla F(\vx_1^{t,k}) - \nabla F(\vx_2^{t,k})  , \vx_1^{t,k} - {\vx_2^{t,k}}   \right\rangle \nonumber \\ 
        + \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\| \left( \left\|  \nabla F_1(\vx_1^{t,k}) - \nabla F(\vx_1^{t,k}) \right\| + \left\|  \nabla F_2(\vx_2^{t,k}) - \nabla F(\vx_2^{t,k}) \right\|  \right) \\ 
        \leq   - \left\langle  \nabla F(\vx_1^{t,k}) - \nabla F(\vx_2^{t,k})  , \vx_1^{t,k} - {\vx_2^{t,k}}   \right\rangle + 2 \zeta \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\| \\ 
        \leq - \frac{1}{L} \left\| \nabla F(\vx_1^{t,k}) - \nabla F(\vx_2^{t,k}) \right\|^2 + 2 \zeta \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\| \label{eq: l2-cvx-smth} \\ 
        \leq - \frac{1}{L} \left\| \nabla F(\vx_1^{t,k}) - \nabla F(\vx_2^{t,k}) \right\|^2 + \frac{1}{2\eta\tau} \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\|^2 + 2 \eta \tau \zeta^2 \label{eq: am-gm}
    \end{eqnarray}
    where (\ref{eq: l2-cvx-smth}) uses smoothness and convexity properties as per Assumption \ref{assumption: convexity-lsmoothness}, (\ref{eq: am-gm}) uses AM-GM inequality (weighted sum version), which is $uv \leq \dfrac{u^2}{2\epsilon} + \dfrac{\epsilon v^2}{2}$, with $u=\left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\|, v=2\zeta$, and $\epsilon=\eta\tau$. 

    Similarly, the third term of (\ref{eq: l2-2}) is bounded as
    \begin{eqnarray}
        \left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k})  \right\|^2 \leq \left( 2\zeta + \left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k}) \right\| \right)^2 \label{eq: l2-22} \\ 
        = 4\zeta^2 + \left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k}) \right\|^2 + 4\zeta \left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k}) \right\| \label{eq: l2-23} \\ 
        \leq 3 \left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k}) \right\|^2 + 6\zeta^2, \label{eq: l2-24}
    \end{eqnarray}
    where (\ref{eq: l2-22}) uses Assumption \ref{assumption: bounded-dissimilarity}, (\ref{eq: l2-24}) is obtained using AM-GM inequality on the last term of (\ref{eq: l2-23}), which is $uv \leq \dfrac{u^2}{2\epsilon} + \dfrac{\epsilon v^2}{2}$, with $u=\left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k}) \right\|, v=4\zeta$, and $\epsilon=\dfrac{1}{4}$. 

    Putting all these results together gives us 
    \begin{eqnarray}
        \mathbb{E} \left[ \left\| \vx_1^{t,k+1} - {\vx_2^{t,k+1}} \right\|^2 \middle| \mathcal{F}^{t,k} \right] \leq \left( 1 + \dfrac{1}{\tau} \right) \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\|^2 + 4 \tau \eta^2 \zeta^2 + 6 \eta^2 \zeta^2 + 2 \eta^2 \sigma^2 \label{eq: l2-25} \\ 
        \leq \left( 1 + \dfrac{1}{\tau} \right) \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\|^2 + 10 \tau \eta^2 \zeta^2 + 2 \eta^2 \sigma^2. 
    \end{eqnarray}
    where (\ref{eq: l2-25}) drops $\left\| \nabla F_1(\vx_1^{t,k}) - \nabla F_2(\vx_2^{t,k}) \right\|^2$ term, since the resulting term is always negative given that $\eta \leq \frac{1}{4L}$. 

    Telescoping gives us 
    \begin{eqnarray}
        \mathbb{E} \left[ \left\| \vx_1^{t,k} - {\vx_2^{t,k}} \right\|^2 \middle| \mathcal{F}^{t,0} \right] 
        &\leq& \frac{\left( 1 + \frac{1}{\tau} \right)^k - 1}{\frac{1}{\tau}} \cdot \left( 10 \tau \eta^2 \zeta^2 + 2 \eta^2 \sigma^2 \right) \label{eq: l2-27} \\ 
        &\leq& 18 \tau^2 \eta^2 \zeta^2 + 4 \tau \eta^2 \sigma^2, 
    \end{eqnarray}
    where the multiplier in (\ref{eq: l2-27}) is obtained from $\sum_{j=0}^{k-1} \left(1+\frac{1}{\tau}\right)^{j}$ and its numerator is upper bounded by a scalar value of $1.8$. 

    Then, by convexity, we have 
    \begin{eqnarray}
        \mathbb{E} \left[ \Big\lVert \vx_i^{t,k} - \overline{\vx}^{t,k} \Big\rVert^2 \middle| \mathcal{F}^{t,0} \right] \leq 18 \tau^2 \eta^2 \zeta^2 + 4 \tau \eta^2 \sigma^2, \quad \forall i \in [N], 
    \end{eqnarray}
    which completes the proof. 
    
\end{proof} 






\subsection{Convergence of the Allocation Algorithm} 
\label{subsection: appendix-fair-alloc}


\subsubsection{Assumptions}
\label{subsubsection: alloc-assumptions}

\begin{assumption}[Finite state space]
    \label{assumption: finite-state-space}
    The state/allocation space $\mathcal{A}$ is finite: $|\mathcal{A}| = M^N < \infty$. 
\end{assumption}

\begin{assumption}[Bounded cost function]
    \label{assumption: bounded-function}
    The objective function $f: \mathcal{A} \rightarrow \mathbb{R}$ is bounded: $\exists \xi > 0$ such that $|f(\va)| \leq \xi$ for all $\va \in \mathcal{A}$. 
\end{assumption}

\begin{assumption}[Irreducibility]
    \label{assumption: irreducibility}
    For each fixed $T>0$, the induced Markov chain (with acceptance probabilities depending on $T$) is irreducible on $\mathcal{A}$. That is, for any $\va, \va^{\prime} \in \mathcal{A}$, we can reach $\va^{\prime}$ from $\va$ with a positive probability in a finite number of steps. 
\end{assumption}

\begin{assumption}[Aperiodicity]
    \label{assumption: aperiodicity}
    For each fixed $T>0$, the chain is aperiodic: there is no integer $d>1$ such that transitions occur only in multipliers of $d$. Equivalently, for each $\va \in \mathcal{A}, \text{gcd}\{m\ |\ P_T^m(\va \rightarrow \va) > 0\} = 1$. 
\end{assumption}

\begin{assumption}[Annealing schedule]
    \label{assumption: annealing-schedule}
    The temperature $T_k$ satisfies: 
    \begin{enumerate} \vspace{-1em}
        \item $T_k \rightarrow 0$ as $k \rightarrow \infty$. \vspace{-0.5em}
        \item $\sum_{k=1}^{\infty} \exp{(-\Delta f / T_k) = \infty}$, where $\Delta f = \min_{\va \neq \va^{\prime}} |f(\va) - f(\va^{\prime})| > 0$ is the smallest nonzero gap of $f$. 
    \end{enumerate}
    A classic example is $T_k = \dfrac{1}{\log(k+k_0)}$ with $k_0 > 1$. 
    
\end{assumption}



\subsubsection{Proof of Theorem \ref{theorem: fair-alloc-convergence}} 

\begin{proof}
    Let $\{A_k\}$ be our Markov chain on $\mathcal{A}$. Denote the transition probability at iteration $k$ by $P_k(\va \rightarrow \va^{\prime})$. We show that, almost surely, the chain eventually remains (or keeps returning) to a global minimizer. 

    First, fix a temperature $T>0$. By Assumptions \ref{assumption: irreducibility} and \ref{assumption: aperiodicity}, the \textit{homogeneous} Markov chain with transitions 
    \begin{equation}
        P_T(\va \rightarrow \va^{\prime}) = \begin{cases}
            \exp{\left( - \dfrac{f(\va^{\prime}) - f(\va)}{T} \right)}, & \hfill f(\va^{\prime}) > f(\va), \\ 
            1, & \hfill f(\va^{\prime}) \leq f(\va), 
        \end{cases} 
    \end{equation}
    
    is irreducible and aperiodic on the finite state space $\mathcal{A}$. Therefore, it has a unique stationary distribution $\pi_T$. 

    Standard Metropolis-Hastings arguments show that 
    \begin{equation}
        \pi_T(\va) \propto \exp{\left( -\frac{f(\va)}{T} \right)}. 
    \end{equation}
    
    As $T \rightarrow 0$, $\exp{\left( -f(\va)/T \right)}$ is maximized by allocations/states $\va \in \mathcal{A}$ that minimize  $f$. In fact, if $\va^{\star}$ is a global minimizer (with $f(\va^{\star}) = f_{\min}$), then for any $\va$ with $f(\va) > f_{\min}$, 
    \begin{equation}
        \frac{\pi_T(\va^{\star})}{\pi_T(\va)} = \exp{\left( -\dfrac{f(\va^{\star}) - f(\va)}{T} \right)} \underset{T \rightarrow 0}{\relbar\joinrel\relbar\joinrel\rightarrow} 0. 
    \end{equation}
    
    Hence, as $T \rightarrow 0$, all stationary mass concentrates on the set of global minima. 

    However, in our algorithm, $T$ is not fixed but varies with iteration $k$. Thus $\{A_k\}$ is a \textit{time-inhomogeneous} Markov chain whose transition matrix $P_k$ depends on $T_k$. The chain does not, in general, admit a single stationary distribution. 

    If $\{T_k\}$ decreases slowly enough, the chain nearly equilibrates around each temperature. This ensures we do not remain trapped in a suboptimal local minimum. 

    Consider a suboptimal state $\va$ where $f(\va) > f_{\min}$. Because $\mathcal{A}$ is finite, there is a finite path from $\va$ to some global minimizer $\va^{\star}$ along which $\max\{f(\cdot)\}$ is well-defined. Let $\Delta (\va \rightarrow \va^{\star})$ be the ``energy barrier" above $\max\{f(\va), f(\va^{\star})\}$ along that path -- i.e., the minimal extra cost one must pay to move from $\va$ eventually down to $\va^{\star}$. Formally, 
    \begin{equation}
        \Delta (\va \rightarrow \va^{\star}) = \min_{\gamma: \va \rightarrow \va^{\star}} \max_{x \in \gamma} \Big[ f(x) - \min\{f(\va), f(\va^{\star})\} \Big]. 
    \end{equation}
    
    Because $f(\va^{\star}) < f(\va)$, we have $\Delta(\va \rightarrow \va^{\star})>0$. A single uphill step in cost $\delta$ has acceptance probability of $\exp{(-\delta/T_k)}$. 

    \paragraph{Why we need $\sum_k \exp{[-(\Delta f) / T_k]} = \infty$:} 
    Let $\Delta_{\max}$ be the maximum barrier needed to reach any global minimizer from any suboptimal state: 
    
    \begin{equation}
        \Delta_{\max} = \max_{\va: f(\va) > f_{\min}, \va^{\star}: f(\va^{\star}) = f_{\min}} \Delta(\va \rightarrow \va^{\star}). 
    \end{equation}
    
    Then any upward move $\delta \leq \Delta_{\max}$ is accepted with probability at least $\exp{(-\Delta_{\max} / T_k)}$. If 
    
    \begin{equation}
        \sum_{k=1}^{\infty} \exp{\left( -\dfrac{\Delta_{\max}}{T_k} \right) = \infty}, 
    \end{equation}

    we get infinitely many chances (with positive probability) to surmount each barrier. By the Borel-Cantelli lemma, almost surely, the chain eventually does surmount every finite barrier and thus can move from any suboptimal state to a strictly better region. Repeatedly, the chain escapes local minima with probability $1$. 

    Once the chain hits a global minimum $\va^{\star}$ at sufficiently small $T_k$, any transition to a higher-cost state is exponentially unlikely (with probability $\exp{\left( - \left[ f(\va^{\prime}) - f(\va^{\star}) \right] / T_k \right)}$). As $T_k \rightarrow 0$, these moves become negligible, causing the chain to remain in (or return quickly to) a global minimizer. Consequently, 
    \begin{equation}
        \lim_{k \rightarrow \infty} P(A_k \in \{\va: f(\va) = f_{\min}\}) = 1, 
    \end{equation} 
    proving convergence to a global minimizer with probability $1$. 
    
    
    This completes the proof. 
    
\end{proof}



\section{Implementation Details}
\label{appendix: implementation-details}

\paragraph{Evaluation metrics.} We evaluate our approach and the baseline methods in terms of both predictive performance and fairness/incentivization. For predictive performance, we use balanced accuracy. Fairness, on the other hand, can be assessed using several metrics, including the incentivized participation rate (IPR) proposed by \cite{cho2022federate}, the Pearson correlation coefficient, and the collaboration gain spread (CGS) proposed by \cite{tastan2025cycle}. However, we exclude IPR from our analysis, as our allocation algorithm inherently guarantees a perfect IPR score by design. Instead, we primarily benchmark our results using Pearson correlation and CGS. 


\paragraph{Implementation details.} We use cross-entropy loss for all image and language classification tasks and maintain consistent training hyperparameters across all experiments. The optimizer of choice is SGD with momentum, with a default initial learning rate of $0.01$. A learning rate scheduler is applied, reducing the learning rate by a factor of $0.1$ at rounds $50$ and $75$, when the total number of communication rounds is set to $100$. The total number of communications is set as follows: 
\begin{itemize}
    \item CIFAR-10, CIFAR-100, and SST: $T=100$, 
    \item MNIST, FMNIST, and SVHN: $T=50$. 
\end{itemize}
In each round, clients perform one local epoch of training. The batch size is fixed at $128$ across all experiments. Additionally, we specify one parameter: the minimum width of the slimmable network, which is set to $p_{\min}=0.25$ unless stated otherwise. The maximum width is always kept at $p_{\max}=1.0$. All experiments were carried out on NVIDIA A100-SXM4-40GB GPUs, with each run utilizing a single GPU. 



\paragraph{Architecture details.} To ensure the reproducibility of our experiments, we used easily implementable model architectures. Specifically, we employed four different architectures: 

\begin{enumerate}
    \item Convolutional neural networks (CNN) -- a lightweight CNN with one convolutional layer followed by two fully connected layers, totaling $0.0606$M parameters. This model is used for the MNIST and FMNIST datasets. 
    \item Enhanced CNN model -- A more complex architecture with two convolutional layers followed by two fully connected layers, comprising $2.0566$M parameters. This model is used for the SVHN dataset. 
    \item ResNet18 \cite{he2016deepresnet} -- a deeper model with $10.7527$M parameters, used for the CIFAR-10 and CIFAR-100 datasets. 
    \item Long short-term memory network (LSTM) -- this model includes an embedding layer of dimension $300$, an LSTM layer, and three fully connected layers, comprising $6.1461$M parameters. It is used for the SST dataset. 
\end{enumerate}

For slimmable model implementation, we apply width slimming to the following layers: convolutional layers, fully connected (FC) layers, batch normalization (BatchNorm), and LSTM layers. Slimming convolutional, fully connected, and LSTM layers is straightforward; however, BatchNorm layers require special treatment due to the inconsistencies between training and testing. To address this, we implement switchable batch normalization, which maintains multiple sets of batch normalization statistics corresponding to different model widths. For further details, refer to \cite{yu2019slimmable}. 


\begin{algorithm}[t]
    \caption{Aequa (with training-time model rewards)} 
    \label{alg: appr2}
    \begin{algorithmic}[1]
        \Statex \textbf{Input:} minimum width $p_{\min}$, maximum width $p_{\max}$, number of communication rounds $T$, number of participants $N$, randomly initialized weights $\vx^0$, number of local iterations $E$, momentum factor $\gamma=0.5$, \texttt{CA} - contribution assessment algorithm (e.g. CGSV, ShapFed, FedFAIM, etc.) 
        \For{\text{each round} $t \gets 0, 1, \ldots, T$} \Comment{\textit{Communication rounds}} 
            \If{$t=0$}
                \State $p_{\max}^{(i,t)} \gets p_{\max}, \forall i \in [N]$
                \State Server broadcasts the full-sized model ($p_{\max}$-model) to each client $i \in [N]$ 
            \Else
                \State Server broadcasts $p_{\max}^{(i,t)}$-submodel to each client $i \in [N]$ 
            \EndIf 
            \For{each participant $i \in [N]$}
                \For{$k \gets 0, 1, \ldots, E$} \Comment{\textit{Local iterations}} 
                    \State Sample width $p_{(i,k)} \gets \mathcal{U}(p_{\min}, p_{\max}^{(i,t)})$ uniformly
                    \State Update the weights of the model corresponding to $p_{\max}^{(i,t)}$ and $p_{(i,k)}$ widths 
                    \State Send the updated weights of the $p_{\max}^{(i,t)}$-submodel 
                \EndFor
            \EndFor
            \State Server computes the contribution of each client $\tilde{c}_i$ using gradients according to $p_{\min}$-submodel via the \texttt{CA} algorithm 
            \State Server updates the contribution of each client using Eq. \ref{eq: contribution-update}, $c_i^{t} \gets \gamma c_i^{t-1} + (1-\gamma) \tilde{c}_i$ if $t > 0$ else $c_i^t \gets \tilde{c}_i$ 
            \State Server updates the participant widths using the reward mechanism in Eq. \ref{eq: reward-training-time} \Comment{\textit{Any reward mechanism}} 
            \State Server updates $\vx^{t+1}$ using the masked averaging \cite{mohtashami2022masked, tastan2024fedpews} 
        \EndFor 
    \end{algorithmic}
\end{algorithm} 



\paragraph{Baseline approaches.} We describe the baseline methods used for comparison. 

\textbf{FedAvg-FT.} The standard FedAvg algorithm \cite{mcmahan2017communication} returns the same model to all clients at each FL iteration, making the computation of the Pearson correlation coefficient undefined. To address this, following IAFL \cite{wu2024incentive}, we introduce a fine-tuning step at the end of collaboration, where each client trains the global model locally for an additional round without sharing the updates with the server. This approach enables each client to receive a personalized model, allowing for a valid estimation of the Pearson correlation coefficient. 

\textbf{CGSV.} In cosine-gradient Shapley value (CGSV) \cite{xu2021gradient}, the server estimates each participant's contribution using gradient alignment, computed as the cosine similarity between an individual gradient and the aggregated gradient. During the broadcasting phase, updates are sparsified based on these estimated contributions before being sent back to clients. 

\textbf{IAFL.} For incentive-aware federated learning (IAFL), we adopt the same setting described in \cite{wu2024incentive} under the most fair scenario, which corresponds to setting the hyperparameters $\kappa=0$ (sharing parameter) and $q=0$ (stochastic recovery probability). Additionally, we use standalone accuracies as the contribution measure, which is consistently used in \cite{wu2024incentive}. 

\textbf{ShapFed.} We incorporate Shapley-driven federated learning (ShapFed) \cite{tastan2024redefining} and CGSV as contribution assessment algorithms within our method. ShapFed estimates contributions using last-layer gradients instead of full-model parameters, making it an efficient and accurate approach in certain scenarios. To better align with our objective, we modify the ShapFed algorithm to use the last $m$ layers instead of only the classification layer, as our method does not require class-specific contribution values. We set $m=10$ in the CIFAR-10 experiment with the ResNet-18 architecture. 




\section{Extension to Training-time Model Rewards}
\label{appendix: extension-to-trainingtime}

In this section, we provide Algorithm \ref{alg: appr2}, which extends our proposed Aequa framework to incorporate training-time model rewards. While our primary allocation mechanism focuses on post-training model distribution, this extension enables dynamic model adaptation during training, where clients receive real-time adjustments to their assigned model widths based on their contributions. However, our approach remains flexible and can seamlessly integrate with the primary allocation mechanism. We showcase this extension to demonstrate that our method is compatible with various reward allocation algorithms, emphasizing its adaptability. 

Algorithm \ref{alg: appr2} outlines the federated training process with contribution-aware model scaling. The algorithm initializes all clients with the full-sized model. As training progresses, client contributions are continuously assessed using a contribution assessment (\texttt{CA}) algorithm, such as CGSV \cite{xu2021gradient}, ShapFed \cite{tastan2024redefining}, or FedFAIM \cite{shi2022fedfaim}. These contributions are then updated iteratively using a momentum-based update rule, ensuring a fair and stable estimation over multiple communication rounds. 

Based on the updated contributions, the server dynamically adjusts the model width allocated to each client, applying a reward mechanism that incentivizes higher-performing participants. This ensures that clients who contribute more to the global model benefit from larger subnetworks, while maintaining fairness in model distribution. The global model update is performed using masked averaging \cite{mohtashami2022masked, tastan2024fedpews}, a robust aggregation technique that ensures stability across varying model widths. The full details of Algorithm \ref{alg: appr2} are presented below. 


\begin{figure}[b!] 
    \centering
    \includegraphics[width=\linewidth]{images/rem-four-width-to-acc.pdf}
    \caption{Performance vs. network width $(p \in [p_{\min}=0.25, p_{\max}=1.0])$ on MNIST, FMNIST, SVHN, and SST datasets using CNN and LSTM models. This figure extends the results presented in Figure \ref{fig: c10-c100-width-to-acc}.} 
    \label{fig: remaining-four-perf-vs-width}
\end{figure}


\section{Additional Experiments}
\subsection{Performance vs model width}
\label{appendix: performance-vs-width}
\paragraph{Remaining results of Figure \ref{fig: c10-c100-width-to-acc}.} In this section, we present the remaining plots corresponding to Figure \ref{fig: c10-c100-width-to-acc} from the main paper, which analyzed CIFAR-10 and CIFAR-100 on the ResNet-18 architecture. Here, we extend the analysis by including Figure \ref{fig: remaining-four-perf-vs-width}, which illustrates the same performance vs. model width relationship for four additional datasets: MNIST, FMNIST, SVHN, and SST, and under a homogeneous partitioning strategy. 

As described in Appendix \ref{appendix: implementation-details}, these datasets are trained using different model architectures: MNIST and FMNIST utilize a lightweight CNN, SVHN is trained on a more complex CNN, and SST is trained using an LSTM network. Despite the differences in model complexity, the width-accuracy relationship remains monotonic, demonstrating that even small-sized network architectures exhibit a clear relation between model width and predictive performance. 

An important observation is that as the model complexity increases, the range between minimum and maximum accuracy widens, making the trade-off between model width and accuracy more pronounced. 





\subsection{Predictive performance}
\label{appendix: predict-performance}

\paragraph{Remaining results of Table \ref{table: predictive-performance-short}.} For completeness, we present the full results of our approach alongside the baseline methods across all partitioning strategies. Table \ref{table: predictive-performance-short} previously reported results for a subset of partitions, while the complete results are provided in Table \ref{table: predictive-performance}. 

From the overall results presented in Table \ref{table: predictive-performance}, it is evident that our method outperforms all other approaches, including FedAvg, and significantly surpasses fairness-based methods. Specifically, across 9 partitioning strategies and 6 datasets per partition, our approach achieves the best performance in 36 cases. In the remaining 18 cases, it ranks as the second-best, performing on par with the FedAvg algorithm. 

\begin{table*}[ht] 
    \caption{The predictive performance of our method and other baselines using different dataset partitions. The results are averaged over five independent evaluations.}
    \vskip -0.1in
    \label{table: predictive-performance} 
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{llrrrr}
\toprule
\rowcolor{lightgray} Partition & Dataset & \multicolumn{1}{c}{FedAvg} & \multicolumn{1}{c}{CGSV} & \multicolumn{1}{c}{IAFL} & Aequa  \\ \midrule 
\multirow{6}{*}{Homogeneous} 
& MNIST     & $\mathbf{98.67} \pm 0.07$ & $90.62 \pm 2.32$ & $98.40 \pm 0.13\phantom{0}$ & $\underline{98.60} \pm 0.11$ \\
& FMNIST    & $\underline{89.45} \pm 0.33$ & $77.01 \pm 2.51$ & $88.54 \pm 0.25\phantom{0}$ & $\mathbf{89.63} \pm 0.19$ \\
& SVHN      & $\mathbf{90.54} \pm 0.18$ & $77.61 \pm 3.16$ & $89.64 \pm 0.13\phantom{0}$ & $\underline{90.18} \pm 0.15$ \\
& CIFAR-10  & $\underline{89.99} \pm 0.23$ & $61.29 \pm 2.92$ & $88.42 \pm 0.07\phantom{0}$ & $\mathbf{90.84} \pm 0.26$ \\
& CIFAR-100 & $\underline{65.92} \pm 0.22$ & $35.36 \pm 0.77$ & $63.23 \pm 0.30\phantom{0}$ & $\mathbf{67.83} \pm 0.32$ \\
& SST       & $\underline{34.44} \pm 1.33$ & $30.12 \pm 1.03$ & $34.02 \pm 0.51\phantom{0}$ & $\mathbf{34.44} \pm 1.19$ \\ \midrule 
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.1$)}} 
& MNIST     & $\mathbf{97.38} \pm 0.62$ & $94.06 \pm 1.79$ & $88.45 \pm 7.87\phantom{0}$ & $\underline{97.30} \pm 0.58$ \\
& FMNIST    & $\underline{83.32} \pm 1.78$ & $71.51 \pm 8.20$ & $66.46 \pm 4.72\phantom{0}$ & $\mathbf{84.60} \pm 1.32$ \\
& SVHN      & $\mathbf{86.38} \pm 0.87$ & $72.48 \pm 4.81$ & $68.71 \pm 7.85\phantom{0}$ & $\underline{86.33} \pm 1.00$ \\
& CIFAR-10  & $\underline{74.73} \pm 3.65$ & $48.77 \pm 5.02$ & $46.36 \pm 9.31\phantom{0}$ & $\mathbf{75.97} \pm 3.36$ \\
& CIFAR-100 & $\underline{61.16} \pm 0.25$ & $34.16 \pm 1.63$ & $43.38 \pm 4.53\phantom{0}$ & $\mathbf{63.42} \pm 0.54$ \\
& SST       & $\underline{32.17} \pm 1.60$ & $21.54 \pm 1.89$ & $27.28 \pm 3.06\phantom{0}$ & $\mathbf{33.54} \pm 1.48$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.5$)}} 
& MNIST     & $\mathbf{98.45} \pm 0.13$ & $93.18 \pm 1.70$ & $97.69 \pm 0.44\phantom{0}$ & $\underline{98.29} \pm 0.16$ \\
& FMNIST    & $\underline{87.86} \pm 0.42$ & $79.24 \pm 3.28$ & $85.42 \pm 1.67\phantom{0}$ & $\mathbf{88.20} \pm 0.49$ \\
& SVHN      & $\mathbf{89.33} \pm 0.25$ & $80.20 \pm 2.10$ & $87.08 \pm 0.79\phantom{0}$ & $\underline{89.14} \pm 0.28$ \\
& CIFAR-10  & $\underline{87.74} \pm 0.23$ & $65.12 \pm 1.82$ & $81.99 \pm 2.46\phantom{0}$ & $\mathbf{88.75} \pm 0.39$ \\
& CIFAR-100 & $\underline{64.54} \pm 0.30$ & $32.19 \pm 0.96$ & $62.37 \pm 1.35\phantom{0}$ & $\mathbf{66.39} \pm 0.41$ \\
& SST       & $\underline{33.52} \pm 0.80$ & $23.82 \pm 2.03$ & $29.64 \pm 2.15\phantom{0}$ & $\mathbf{34.21} \pm 1.08$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=1.0$)}} 
& MNIST     & $\mathbf{98.52} \pm 0.14$ & $92.71 \pm 1.53$ & $98.17 \pm 0.15\phantom{0}$ & $\underline{98.48} \pm 0.13$ \\
& FMNIST    & $\underline{88.72} \pm 0.36$ & $78.76 \pm 1.40$ & $87.67 \pm 0.22\phantom{0}$ & $\mathbf{89.09} \pm 0.23$ \\
& SVHN      & $\mathbf{89.96} \pm 0.30$ & $79.53 \pm 2.50$ & $88.51 \pm 0.26\phantom{0}$ & $\underline{89.75} \pm 0.22$ \\
& CIFAR-10  & $\underline{88.76} \pm 0.26$ & $65.88 \pm 5.02$ & $84.02 \pm 1.12\phantom{0}$ & $\mathbf{89.67} \pm 0.28$ \\
& CIFAR-100 & $\underline{65.06} \pm 0.29$ & $33.92 \pm 1.32$ & $64.37 \pm 0.68\phantom{0}$ & $\mathbf{67.06} \pm 0.44$ \\
& SST       & $\underline{34.00} \pm 0.87$ & $25.24 \pm 2.55$ & $31.26 \pm 1.22\phantom{0}$ & $\mathbf{34.15} \pm 1.07$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=2.0$)}} 
& MNIST     & $\mathbf{98.58} \pm 0.15$ & $92.56 \pm 1.54$ & $98.22 \pm 0.13\phantom{0}$ & $\underline{98.50} \pm 0.09$ \\
& FMNIST    & $\underline{89.12} \pm 0.36$ & $79.42 \pm 2.17$ & $88.27 \pm 0.28\phantom{0}$ & $\mathbf{89.46} \pm 0.18$ \\
& SVHN      & $\mathbf{90.19} \pm 0.10$ & $78.16 \pm 2.46$ & $88.91 \pm 0.14\phantom{0}$ & $\underline{89.94} \pm 0.13$ \\
& CIFAR-10  & $\underline{89.35} \pm 0.28$ & $68.60 \pm 3.36$ & $88.09 \pm 0.33\phantom{0}$ & $\mathbf{90.30} \pm 0.18$ \\
& CIFAR-100 & $\underline{65.56} \pm 0.22$ & $36.14 \pm 1.66$ & $64.71 \pm 0.70\phantom{0}$ & $\mathbf{67.51} \pm 0.29$ \\
& SST       & $\underline{33.45} \pm 1.32$ & $27.55 \pm 0.62$ & $33.25 \pm 0.27\phantom{0}$ & $\mathbf{34.58} \pm 0.49$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=5.0$)}} 
& MNIST     & $\mathbf{98.64} \pm 0.12$ & $91.73 \pm 2.95$ & $98.31 \pm 0.13\phantom{0}$ & $\underline{98.54} \pm 0.11$ \\
& FMNIST    & $\underline{89.35} \pm 0.31$ & $77.03 \pm 2.66$ & $88.44 \pm 0.24\phantom{0}$ & $\mathbf{89.60} \pm 0.15$ \\
& SVHN      & $\mathbf{90.48} \pm 0.16$ & $80.52 \pm 1.94$ & $89.32 \pm 0.15\phantom{0}$ & $\underline{90.08} \pm 0.29$ \\
& CIFAR-10  & $\underline{89.56} \pm 0.29$ & $68.09 \pm 5.05$ & $88.47 \pm 0.50\phantom{0}$ & $\mathbf{90.35} \pm 0.14$ \\
& CIFAR-100 & $\underline{65.79} \pm 0.29$ & $37.28 \pm 0.66$ & $65.61 \pm 0.39\phantom{0}$ & $\mathbf{67.77} \pm 0.16$ \\
& SST       & $\underline{34.26} \pm 1.01$ & $28.75 \pm 0.31$ & $33.41 \pm 1.17\phantom{0}$ & $\mathbf{34.47} \pm 0.55$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.15, 6)$}} 
& MNIST     & $\mathbf{98.69} \pm 0.10$ & $93.22 \pm 0.99$ & $98.40 \pm 0.12\phantom{0}$ & $\underline{98.62} \pm 0.09$ \\
& FMNIST    & $\underline{89.53} \pm 0.24$ & $78.73 \pm 2.14$ & $88.53 \pm 0.26\phantom{0}$ & $\mathbf{89.72} \pm 0.17$ \\
& SVHN      & $\mathbf{90.59} \pm 0.17$ & $77.54 \pm 2.34$ & $89.51 \pm 0.13\phantom{0}$ & $\underline{90.26} \pm 0.16$ \\
& CIFAR-10  & $\underline{90.00} \pm 0.13$ & $66.89 \pm 1.85$ & $89.51 \pm 0.15\phantom{0}$ & $\mathbf{90.71} \pm 0.14$ \\
& CIFAR-100 & $\underline{65.88} \pm 0.38$ & $39.62 \pm 0.74$ & $64.60 \pm 0.25\phantom{0}$ & $\mathbf{68.24} \pm 0.11$ \\
& SST       & $\underline{34.26} \pm 0.98$ & $29.53 \pm 1.02$ & $33.70 \pm 0.70\phantom{0}$ & $\mathbf{34.64} \pm 1.01$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.4, 2)$}} 
& MNIST     & $\mathbf{98.69} \pm 0.09$ & $92.73 \pm 1.34$ & $98.42 \pm 0.16\phantom{0}$ & $\underline{98.63} \pm 0.12$ \\
& FMNIST    & $\underline{89.58} \pm 0.29$ & $76.14 \pm 4.79$ & $88.51 \pm 0.25\phantom{0}$ & $\mathbf{89.78} \pm 0.19$ \\
& SVHN      & $\mathbf{90.69} \pm 0.15$ & $79.11 \pm 1.35$ & $89.49 \pm 0.07\phantom{0}$ & $\underline{90.35} \pm 0.17$ \\
& CIFAR-10  & $\underline{89.78} \pm 0.14$ & $69.13 \pm 3.26$ & $88.50 \pm 0.13\phantom{0}$ & $\mathbf{90.86} \pm 0.15$ \\
& CIFAR-100 & $\underline{66.30} \pm 0.23$ & $38.35 \pm 1.01$ & $63.67 \pm 0.10\phantom{0}$ & $\mathbf{68.80} \pm 0.16$ \\
& SST       & $\mathbf{35.45} \pm 1.11$ & $26.10 \pm 0.65$ & $34.65 \pm 0.56\phantom{0}$ & $\underline{35.29} \pm 0.76$ \\ \midrule
\multirow{6}{*}{Label Skew: \#OC=\{3, 30\}} 
& MNIST     & $\underline{94.37} \pm 3.43$ & $79.19 \pm 7.94$ & $73.10 \pm 15.00$ & $\mathbf{95.37} \pm 1.15$ \\
& FMNIST    & $\underline{79.73} \pm 3.80$ & $61.54 \pm 8.03$ & $60.10 \pm 8.03\phantom{0}$  & $\mathbf{80.51} \pm 3.27$ \\
& SVHN      & $\underline{79.73} \pm 5.89$ & $64.07 \pm 7.65$ & $55.83 \pm 11.89$ & $\mathbf{80.69} \pm 6.05$ \\
& CIFAR-10  & $\underline{71.88} \pm 3.28$ & $48.02 \pm 3.88$ & $44.12 \pm 21.15$ & $\mathbf{72.40} \pm 3.17$ \\
& CIFAR-100 & $\underline{60.95} \pm 1.18$ & $35.09 \pm 0.42$ & $55.26 \pm 3.85\phantom{0}$  & $\mathbf{62.84} \pm 1.18$ \\
& SST       & $\mathbf{33.96} \pm 0.35$ & $24.88 \pm 2.04$ & $30.33 \pm 1.68\phantom{0}$  & $\underline{33.01} \pm 0.90$ \\ \midrule 
\multicolumn{2}{c}{Number of times that performs the best } & $18/54$ & $0/54$ & $0/54$ & $\mathbf{36/54}$ \\ \bottomrule 

                \end{tabular}
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*}



\subsection{Pearson correlation}
\label{subsection: appendix-pearson}


\paragraph{Remaining results of Table \ref{table: pearson-correlation-short}.} 
In Table \ref{table: pearson-correlation}, we present the complete results of the experiments on Pearson correlation, extending the findings of Table \ref{table: pearson-correlation-short}, which reported results for only a subset of partitions. The results demonstrate that our algorithm outperforms all other methods in all $54$ cases. 

As discussed in the fairness analysis (Section \ref{section: fairness-analysis}), our method consistently achieves near-perfect correlation coefficients, typically in the range of $0.98$ to $0.99$. While the theoretical analysis guarantees a perfect correlation, in practice, the continuity assumption is slightly violated due to the need for discretizing the accuracy range $[\ell, u]$. 

Additionally, in experiments using the ResNet-18 architecture, we introduce discretization by modifying the uniform sampling strategy. Instead of continuous sampling, we define a set of model widths sampled from a bucket of predefined values, starting from $0.25, 0.3, 0.35$, and increasing in increments of $0.05$, up to $1.0$. 




\begin{table*}[t]
    \caption{The incentivization performance of our method and other baselines under different dataset partitions, measured using the Pearson correlation coefficient between the final model accuracies and standalone accuracies. The results are averaged over five independent evaluations. } 
    \vskip -0.1in
    \label{table: pearson-correlation}
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{llrrrr}
\toprule
\rowcolor{lightgray} Partition & Dataset & \multicolumn{1}{c}{FedAvg-FT} & \multicolumn{1}{c}{CGSV} & \multicolumn{1}{c}{IAFL} & \multicolumn{1}{c}{Aequa} \\ \midrule
\multirow{6}{*}{Homogeneous} 
& MNIST     & $0.07  \pm 0.24$ & $-0.30 \pm 0.16$ & $0.16  \pm 0.20$ & $\mathbf{0.97 \pm 0.02}$ \\
& FMNIST    & $0.21  \pm 0.07$ & $-0.09 \pm 0.35$ & $0.27  \pm 0.24$ & $\mathbf{0.98 \pm 0.02}$ \\
& SVHN      & $-0.12 \pm 0.36$ & $0.02  \pm 0.17$ & $0.07  \pm 0.28$ & $\mathbf{0.98 \pm 0.02}$ \\
& CIFAR-10  & $0.04  \pm 0.27$ & $0.05  \pm 0.42$ & $-0.01 \pm 0.15$ & $\mathbf{0.99 \pm 0.01}$ \\
& CIFAR-100 & $-0.07 \pm 0.37$ & $-0.19 \pm 0.36$ & $0.02  \pm 0.31$ & $\mathbf{0.96 \pm 0.01}$ \\
& SST       & $0.06  \pm 0.25$ & $-0.07 \pm 0.32$ & $-0.03 \pm 0.26$ & $\mathbf{0.98 \pm 0.01}$ \\ \midrule 
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.1$)}} 
& MNIST     & $0.39  \pm 0.37$ & $0.56  \pm 0.18$ & $0.61  \pm 0.23$ & $\mathbf{0.85 \pm 0.03}$ \\
& FMNIST    & $0.21  \pm 0.41$ & $0.56  \pm 0.28$ & $0.61  \pm 0.25$ & $\mathbf{0.89 \pm 0.06}$ \\
& SVHN      & $0.66  \pm 0.24$ & $0.21  \pm 0.46$ & $0.80  \pm 0.18$ & $\mathbf{0.92 \pm 0.02}$ \\
& CIFAR-10  & $-0.18 \pm 0.35$ & $0.67  \pm 0.14$ & $0.84  \pm 0.15$ & $\mathbf{0.94 \pm 0.02}$ \\
& CIFAR-100 & $-0.17 \pm 0.46$ & $0.30  \pm 0.52$ & $0.89  \pm 0.07$ & $\mathbf{0.99 \pm 0.01}$ \\
& SST       & $-0.20 \pm 0.35$ & $0.14  \pm 0.50$ & $0.91  \pm 0.09$ & $\mathbf{0.98 \pm 0.01}$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.5$)}} 
& MNIST     & $0.32  \pm 0.40$ & $-0.22 \pm 0.48$ & $0.67  \pm 0.16$ & $\mathbf{0.87 \pm 0.03}$ \\
& FMNIST    & $0.67  \pm 0.19$ & $0.34  \pm 0.48$ & $0.50  \pm 0.10$ & $\mathbf{0.88 \pm 0.05}$ \\
& SVHN      & $0.80  \pm 0.09$ & $-0.18 \pm 0.54$ & $0.59  \pm 0.41$ & $\mathbf{0.90 \pm 0.04}$ \\
& CIFAR-10  & $-0.16 \pm 0.21$ & $0.44  \pm 0.38$ & $0.93  \pm 0.03$ & $\mathbf{0.94 \pm 0.03}$ \\
& CIFAR-100 & $0.19  \pm 0.29$ & $0.38  \pm 0.14$ & $0.58  \pm 0.23$ & $\mathbf{0.99 \pm 0.00}$ \\
& SST       & $-0.15 \pm 0.07$ & $-0.10 \pm 0.34$ & $0.88  \pm 0.05$ & $\mathbf{0.94 \pm 0.04}$ \\ \midrule 
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=1.0$)}} 
& MNIST     & $0.55  \pm 0.20$ & $0.13  \pm 0.48$ & $0.48  \pm 0.40$ & $\mathbf{0.91 \pm 0.03}$ \\
& FMNIST    & $0.42  \pm 0.51$ & $0.27  \pm 0.30$ & $0.46  \pm 0.37$ & $\mathbf{0.95 \pm 0.03}$ \\
& SVHN      & $0.52  \pm 0.28$ & $0.08  \pm 0.19$ & $0.69  \pm 0.08$ & $\mathbf{0.94 \pm 0.03}$ \\
& CIFAR-10  & $-0.05 \pm 0.15$ & $0.50  \pm 0.41$ & $0.89  \pm 0.04$ & $\mathbf{0.94 \pm 0.02}$ \\
& CIFAR-100 & $-0.08 \pm 0.35$ & $-0.23 \pm 0.51$ & $0.65  \pm 0.37$ & $\mathbf{0.99 \pm 0.00}$ \\
& SST       & $0.09  \pm 0.29$ & $-0.16 \pm 0.33$ & $0.71  \pm 0.12$ & $\mathbf{0.94 \pm 0.04}$ \\ \midrule 
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=2.0$)}} 
& MNIST     & $0.49  \pm 0.32$ & $-0.07 \pm 0.57$ & $0.37  \pm 0.41$ & $\mathbf{0.96 \pm 0.03}$ \\
& FMNIST    & $0.50  \pm 0.24$ & $0.03  \pm 0.52$ & $0.51  \pm 0.27$ & $\mathbf{0.96 \pm 0.03}$ \\
& SVHN      & $0.67  \pm 0.10$ & $0.09  \pm 0.06$ & $0.72  \pm 0.16$ & $\mathbf{0.95 \pm 0.02}$ \\
& CIFAR-10  & $0.17  \pm 0.28$ & $0.58  \pm 0.21$ & $0.83  \pm 0.15$ & $\mathbf{0.97 \pm 0.03}$ \\
& CIFAR-100 & $-0.26 \pm 0.25$ & $0.05  \pm 0.46$ & $0.65  \pm 0.26$ & $\mathbf{0.99 \pm 0.00}$ \\
& SST       & $-0.19 \pm 0.36$ & $0.08  \pm 0.36$ & $0.59  \pm 0.28$ & $\mathbf{0.95 \pm 0.04}$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=5.0$)}} 
& MNIST     & $0.26  \pm 0.32$ & $-0.10 \pm 0.40$ & $0.39  \pm 0.46$ & $\mathbf{0.96 \pm 0.03}$ \\
& FMNIST    & $0.26  \pm 0.32$ & $-0.32 \pm 0.22$ & $0.12  \pm 0.29$ & $\mathbf{0.97 \pm 0.02}$ \\
& SVHN      & $0.09  \pm 0.18$ & $0.14  \pm 0.37$ & $0.62  \pm 0.05$ & $\mathbf{0.98 \pm 0.01}$ \\
& CIFAR-10  & $0.01  \pm 0.16$ & $0.22  \pm 0.54$ & $0.80  \pm 0.08$ & $\mathbf{0.98 \pm 0.02}$ \\
& CIFAR-100 & $0.11  \pm 0.25$ & $0.04  \pm 0.26$ & $0.18  \pm 0.50$ & $\mathbf{0.99 \pm 0.01}$ \\
& SST       & $-0.04 \pm 0.21$ & $0.21  \pm 0.32$ & $0.37  \pm 0.36$ & $\mathbf{0.98 \pm 0.01}$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.15, 6)$}} 
% \multirow{6}{*}{Imbalanced (0.15, 6)} 
& MNIST     & $-0.63 \pm 0.18$ & $0.34  \pm 0.80$ & $0.95  \pm 0.04$ & $\mathbf{0.98 \pm 0.01}$ \\
& FMNIST    & $-0.45 \pm 0.28$ & $0.49  \pm 0.71$ & $0.93  \pm 0.02$ & $\mathbf{0.98 \pm 0.01}$ \\
& SVHN      & $-0.76 \pm 0.12$ & $0.42  \pm 0.75$ & $0.99  \pm 0.01$ & $\mathbf{1.00 \pm 0.00}$ \\
& CIFAR-10  & $-0.37 \pm 0.16$ & $0.98  \pm 0.02$ & $0.99  \pm 0.00$ & $\mathbf{1.00 \pm 0.00}$ \\
& CIFAR-100 & $0.06  \pm 0.40$ & $0.97  \pm 0.03$ & $\mathbf{1.00  \pm 0.00}$ & $\mathbf{1.00 \pm 0.00}$ \\
& SST       & $-0.07 \pm 0.48$ & $-0.23 \pm 0.54$ & $0.90  \pm 0.02$ & $\mathbf{0.94 \pm 0.04}$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.4, 2)$}} 
% \multirow{6}{*}{Imbalanced (0.4, 2)} 
& MNIST     & $-0.61 \pm 0.13$ & $-0.33 \pm 0.76$ & $0.76  \pm 0.04$ & $\mathbf{0.96 \pm 0.03}$ \\
& FMNIST    & $-0.21 \pm 0.26$ & $0.51  \pm 0.38$ & $0.90  \pm 0.04$ & $\mathbf{0.99 \pm 0.01}$ \\
& SVHN      & $-0.54 \pm 0.20$ & $-0.06 \pm 0.79$ & $0.93  \pm 0.01$ & $\mathbf{0.98 \pm 0.01}$ \\
& CIFAR-10  & $-0.41 \pm 0.35$ & $0.20  \pm 0.96$ & $0.92  \pm 0.04$ & $\mathbf{1.00 \pm 0.00}$ \\
& CIFAR-100 & $0.05  \pm 0.25$ & $0.47  \pm 0.75$ & $0.98  \pm 0.01$ & $\mathbf{1.00 \pm 0.00}$ \\
& SST       & $-0.05 \pm 0.61$ & $-0.97 \pm 0.01$ & $0.90  \pm 0.07$ & $\mathbf{0.97 \pm 0.01}$ \\ \midrule
\multirow{6}{*}{Label Skew: \#OC=\{3, 30\}} 
& MNIST     & $0.03  \pm 0.41$ & $-0.27 \pm 0.27$ & $0.23  \pm 0.24$ & $\mathbf{0.81 \pm 0.13}$ \\
& FMNIST    & $-0.44 \pm 0.27$ & $0.11  \pm 0.45$ & $0.08  \pm 0.26$ & $\mathbf{0.99 \pm 0.01}$ \\
& SVHN      & $0.43  \pm 0.25$ & $-0.43 \pm 0.42$ & $0.01  \pm 0.25$ & $\mathbf{0.98 \pm 0.00}$ \\
& CIFAR-10  & $0.19  \pm 0.32$ & $0.12  \pm 0.32$ & $0.22  \pm 0.38$ & $\mathbf{0.97 \pm 0.02}$ \\
& CIFAR-100 & $-0.38 \pm 0.22$ & $0.00 \pm 0.24$  & $0.31  \pm 0.22$ & $\mathbf{0.98 \pm 0.02}$ \\
& SST       & $0.45  \pm 0.38$ & $-0.19 \pm 0.48$ & $0.48  \pm 0.37$ & $\mathbf{0.97 \pm 0.03}$ \\ \midrule 
\multicolumn{2}{c}{Number of times that performs the best } & $0/54$ & $0/54$ & $1/54$ & $\mathbf{54/54}$   \\ \bottomrule 


\end{tabular}
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*}




\subsection{MCG and CGS results}
\label{appendix: mcg-cgs} 

In this section, we provide the detailed results for mean collaboration gain (MCG) and collaboration gain spread (CGS), expanding upon the summary presented in Section \ref{subsection: fairness-results}. Table \ref{table: mcg-cgs} presents a comprehensive comparison of Aequa against baseline methods across all experimental settings. The results confirm that Aequa consistently achieves superior performance, outperforming other methods in $42$ out of $54$ cases. 

For cases where Aequa does not achieve the lowest CGS, we highlight the MCG values in green, demonstrating that Aequa still significantly outperforms other methods in MCG (by $\sim \times 3$). This suggests that even when another method achieves a comparable CGS, it does so at the expense of lower mean collaboration gain, indicating a weaker overall incentivization effect. Such cases occur in extremely heterogeneous settings. 

As explained earlier, another scenario where Aequa underperforms compared to IAFL is in the quantity skew setting. This performance gap is primarily due to the selected value of $p_{\min}$, which determines the minimum accuracy $\ell$ assigned to low-contributing clients. To examine the impact of this parameter, we conduct an additional set of experiments where $p_{\min}$ is adjusted to $0.1$, specifically for the quantity skew partition. The results, presented in Table \ref{table: mcg-cgs-2}, show that under this revised configuration, Aequa outperforms other baseline methods in $11$ out of $12$ cases, bringing the overall success rate of $48$ out of $54$ cases. 

The partitioning strategies where Aequa consistently outperforms all methods include homogeneous, Dirichlet $(\alpha = \{1.0, 2.0, 5.0\})$, quantity skew $(0.15, 6)$, and label skew $(C=3)$. In contrast, CGSV performs well in extremely heterogeneous settings, achieving positive MCG values, but it tends to yield negative values in other settings, limiting its generalizability. FedAvg-FT, on the other hand, consistently achieves positive MCG values but exhibits higher CGS values, indicating a lack of fairness. 




\begin{table*}[t]
    \caption{The average collaboration gain (MCG) $(\uparrow)$ and the collaboration gain spread (CGS) $(\downarrow)$ results of our method and other baselines under different dataset partitions. The results are averaged over five independent evaluations. } %  Utilized from \cite{tastan2025cycle}.
    \vskip -0.125in
    \label{table: mcg-cgs}
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{llrrrrrrrr}
\toprule
\multirow{2.5}{*}{Partition} & \multirow{2.5}{*}{Dataset} & \multicolumn{2}{c}{FedAvg-FT} & \multicolumn{2}{c}{CGSV} & \multicolumn{2}{c}{IAFL} & \multicolumn{2}{c}{Aequa}  \\ \cmidrule{3-10} 
& & MCG & CGS & MCG & CGS & MCG & CGS & MCG & CGS \\ \midrule 
\multirow{6}{*}{Homogeneous} 
& MNIST     & $1.58$   & $0.29$  & $-6.63$  & $0.18$  & $1.64$  & $0.18$  & $1.64$  & $\mathbf{0.04}$ \\
& FMNIST    & $3.05$   & $0.77$  & $-7.90$  & $0.68$  & $3.13$  & $0.68$  & $3.43$  & $\mathbf{0.17}$ \\
& SVHN      & $5.08$   & $0.81$  & $-5.26$  & $0.39$  & $6.57$  & $0.39$  & $6.68$  & $\mathbf{0.08}$ \\
& CIFAR-10  & $13.62$  & $0.65$  & $-15.08$ & $0.48$  & $13.62$ & $0.59$  & $13.31$ & $\mathbf{0.09}$ \\
& CIFAR-100 & $31.54$  & $0.64$  & $-2.02$  & $0.43$  & $31.54$ & $0.56$  & $31.14$ & $\mathbf{0.14}$ \\
& SST       & $8.77$   & $1.48$  & $4.74$   & $0.81$  & $8.96$  & $0.81$  & $8.11$  & $\mathbf{0.22}$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.1$)}} 
& MNIST     & $29.27$  & $14.60$ & $43.72$  & $12.46$ & \cellcolor{lightred} $16.41$ & $\mathbf{11.77}$ & \cellcolor{lightgreen} $47.63$ & $12.50$ \\
& FMNIST    & $15.56$  & $14.75$ & $31.43$  & $9.80$  & $8.48$  & $10.21$ & $40.89$ & $\mathbf{9.07}$  \\
& SVHN      & $11.97$  & $12.15$ & $36.77$  & $11.81$ & \cellcolor{lightred} $9.47$  & $\mathbf{9.63}$  & \cellcolor{lightgreen} $49.85$ & $9.92$  \\
& CIFAR-10  & $25.51$  & $16.13$ & $7.63$   & $7.00$  & $3.77$  & $6.87$  & $39.91$ & $\mathbf{4.73}$  \\
& CIFAR-100 & $41.13$  & $3.02$  & $8.09$   & $1.91$  & $10.79$ & $4.99$  & $40.65$ & $\mathbf{0.39}$  \\
& SST       & $9.19$   & $3.80$  & $-0.89$  & $2.24$  & $1.26$  & $1.01$  & $8.65$  & $\mathbf{0.88}$  \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=0.5$)}} 
& MNIST     & $9.48$   & $7.37$  & $7.27$   & $6.00$  & $8.93$  & $\mathbf{4.59}$  & \cellcolor{lightgreen} $11.56$ & $5.23$ \\
& FMNIST    & $6.81$   & $4.60$  & $6.24$   & $5.92$  & $5.86$  & $6.05$  & $14.34$ & $\mathbf{4.41}$ \\
& SVHN      & $12.90$  & $\mathbf{5.85}$  & $14.37$  & $7.75$  & $12.85$ & $6.11$  & \cellcolor{lightgreen} $21.79$ & $6.33$ \\
& CIFAR-10  & $28.88$  & $9.52$  & $6.34$   & $6.56$  & $13.97$ & $\mathbf{3.26}$  & $28.25$ & $5.73$ \\
& CIFAR-100 & $37.35$  & $1.74$  & $4.99$   & $1.23$  & $27.71$ & $7.99$  & $37.00$ & $\mathbf{0.23}$ \\
& SST       & $9.81$   & $2.45$  & $-0.15$  & $2.53$  & $3.52$  & $1.48$  & $9.49$  & $\mathbf{1.15}$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=1.0$)}} 
& MNIST     & $4.69$   & $2.84$  & $-0.38$  & $2.75$  & $4.55$  & $2.53$  & $5.19$  & $\mathbf{2.10}$ \\
& FMNIST    & $4.20$   & $2.55$  & $-1.17$  & $2.74$  & $4.35$  & $2.39$  & $7.06$  & $\mathbf{1.22}$ \\
& SVHN      & $8.10$   & $7.11$  & $4.08$   & $4.53$  & $9.72$  & $3.38$  & $13.36$ & $\mathbf{3.13}$ \\
& CIFAR-10  & $22.69$  & $6.54$  & $-2.38$  & $4.75$  & $14.94$ & $3.50$  & $20.39$ & $\mathbf{3.50}$ \\
& CIFAR-100 & $35.46$  & $2.00$  & $3.37$   & $1.22$  & $31.08$ & $2.94$  & $34.60$ & $\mathbf{0.22}$ \\
& SST       & $9.48$   & $1.97$  & $0.77$   & $2.12$  & $4.74$  & $1.73$  & $8.50$  & $\mathbf{0.88}$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=2.0$)}} 
& MNIST     & $2.64$   & $1.10$  & $-3.39$  & $1.13$  & $2.54$  & $1.04$  & $2.59$  & $\mathbf{0.52}$ \\
& FMNIST    & $2.54$   & $2.13$  & $-3.53$  & $1.58$  & $3.84$  & $1.33$  & $4.63$  & $\mathbf{0.52}$ \\
& SVHN      & $6.97$   & $1.94$  & $-1.63$  & $2.48$  & $7.36$  & $1.72$  & $8.79$  & $\mathbf{1.15}$ \\
& CIFAR-10  & $17.99$  & $4.31$  & $-7.89$  & $3.06$  & $14.08$ & $2.30$  & $15.70$ & $\mathbf{1.47}$ \\
& CIFAR-100 & $33.68$  & $1.92$  & $1.12$   & $1.24$  & $29.58$ & $5.11$  & $32.89$ & $\mathbf{0.22}$ \\
& SST       & $8.32$   & $2.14$  & $2.87$   & $1.55$  & $6.02$  & $1.49$  & $8.53$  & $\mathbf{0.76}$ \\ \midrule
\multirow{6}{*}{\parbox{3cm}{Heterogeneous: \\ Dirichlet ($\alpha=5.0$)}} 
& MNIST     & $1.84$   & $0.40$  & $-4.23$  & $0.37$  & $1.78$  & $0.32$  & $1.65$  & $\mathbf{0.09}$ \\
& FMNIST    & $3.05$   & $2.58$  & $-6.74$  & $1.62$  & $3.74$  & $1.64$  & $4.45$  & $\mathbf{0.67}$ \\
& SVHN      & $5.26$   & $4.35$  & $-1.12$  & $1.16$  & $6.78$  & $0.93$  & $7.06$  & $\mathbf{0.21}$ \\
& CIFAR-10  & $15.38$  & $3.03$  & $-11.04$ & $2.34$  & $9.29$  & $7.61$  & $13.04$ & $\mathbf{0.78}$ \\
& CIFAR-100 & $32.47$  & $1.04$  & $0.58$   & $0.66$  & $31.92$ & $1.33$  & $32.01$ & $\mathbf{0.17}$ \\
& SST       & $8.73$   & $1.17$  & $3.66$   & $1.02$  & $7.36$  & $1.20$  & $8.28$  & $\mathbf{0.27}$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.15, 6)$}} 
& MNIST     & $2.83$   & $2.50$  & $-2.23$  & $2.09$  & $2.33$  & $1.73$  & $2.43$  & $\mathbf{1.56}$  \\
& FMNIST    & $3.96$   & $2.85$  & $-4.98$  & $2.17$  & $3.59$  & $1.42$  & $3.72$  & $\mathbf{0.49}$  \\
& SVHN      & $7.91$   & $5.00$  & $-3.56$  & $4.47$  & $6.25$  & $\mathbf{1.73}$  & $7.98$  & $2.84$  \\
& CIFAR-10  & $19.88$  & $14.02$ & $-5.85$  & $8.96$  & $13.67$ & $\mathbf{7.01}$  & $17.62$ & $10.31$ \\
& CIFAR-100 & $34.13$  & $13.63$ & $2.87$   & $7.93$  & $21.23$ & $\mathbf{1.26}$   & $29.56$ & $5.07$ \\
& SST       & $9.16$   & $2.46$  & $4.78$   & $2.40$  & $6.85$  & $1.75$  & $8.62$  & $\mathbf{1.21}$  \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.4, 2)$}} 
& MNIST     & $4.60$   & $2.38$  & $-1.13$  & $2.33$  & $2.58$  & $1.58$  & $3.44$  & $\mathbf{1.58}$  \\
& FMNIST    & $5.71$   & $3.00$  & $-5.93$  & $2.72$  & $3.28$  & $1.73$  & $3.76$  & $\mathbf{1.25}$  \\
& SVHN      & $11.80$  & $5.04$  & $1.30$   & $4.77$  & $5.94$  & $\mathbf{2.21}$  & $10.08$ & $3.49$  \\
& CIFAR-10  & $29.65$  & $14.70$ & $-2.20$  & $10.60$ & $13.88$ & $\mathbf{8.03}$  & $24.88$ & $11.76$ \\
& CIFAR-100 & $41.66$  & $19.02$ & $6.92$   & $11.84$ & $10.63$ & $\mathbf{5.59}$  & $30.96$ & $12.42$ \\
& SST       & $11.08$  & $3.56$  & $3.63$   & $4.16$  & $3.85$  & $\mathbf{1.69}$  & $9.64$  & $2.39$  \\ \midrule
\multirow{6}{*}{Label Skew: \#OC=\{3, 30\}} 
& MNIST     & $18.99$  & $10.86$ & $50.04$  & $0.71$  & $17.88$ & $11.98$ & $64.34$ & $\mathbf{0.06}$  \\
& FMNIST    & $11.61$  & $9.64$  & $34.63$  & $2.35$  & $13.22$ & $9.58$  & $51.80$ & $\mathbf{0.20}$  \\
& SVHN      & $1.13$   & $1.07$  & $38.59$  & $1.13$  & $9.21$  & $8.47$  & $51.45$ & $\mathbf{0.09}$  \\
& CIFAR-10  & $24.11$  & $6.38$  & $5.79$   & $4.86$  & $5.57$  & $11.68$ & $39.42$ & $\mathbf{0.18}$  \\
& CIFAR-100 & $40.28$  & $1.40$  & $5.25$   & $0.84$  & $23.16$ & $9.93$  & $40.75$ & $\mathbf{0.16}$  \\
& SST       & $8.86$   & $1.70$  & $-0.89$  & $1.97$  & $5.55$  & $1.92$  & $7.60$  & $\mathbf{0.65}$  \\ \midrule 
\multicolumn{2}{c}{Number of times that performs the best } & \multicolumn{2}{c}{$1/54$} & \multicolumn{2}{c}{$0/54$} & \multicolumn{2}{c}{$11/54$} & \multicolumn{2}{c}{$\mathbf{42/54}$} \\ \bottomrule 

\end{tabular}
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*}



\begin{table*}[t]
    \caption{The average collaboration gain (MCG) $(\uparrow)$ and the collaboration gain spread (CGS) $(\downarrow)$ results of our method and other baselines under different dataset partitions. The results are averaged over five independent evaluations. Continuation of Table \ref{table: mcg-cgs} with different minimum width ($p_{\min}=0.1$).} 
    \vskip -0.1in 
    \label{table: mcg-cgs-2}
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{llrrrrrrrr}
\toprule
\multirow{2.5}{*}{Partition} & \multirow{2.5}{*}{Dataset} & \multicolumn{2}{c}{FedAvg-FT} & \multicolumn{2}{c}{CGSV} & \multicolumn{2}{c}{IAFL} & \multicolumn{2}{c}{Aequa}  \\ \cmidrule{3-10} 
& & MCG & CGS & MCG & CGS & MCG & CGS & MCG & CGS \\ \midrule 
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.15, 6)$}} 
& MNIST     & $2.83$   & $2.50$  & $-2.23$  & $2.09$  & $2.33$  & $1.73$          & $1.70$  & $\mathbf{1.02}$ \\
& FMNIST    & $3.96$   & $2.85$  & $-4.98$  & $2.17$  & $3.59$  & $1.42$          & $2.82$  & $\mathbf{0.45}$ \\
& SVHN      & $7.91$   & $5.00$  & $-3.56$  & $4.47$  & $6.25$  & $1.73$          & $5.38$  & $\mathbf{0.75}$ \\
& CIFAR-10  & $19.88$  & $14.02$ & $-5.85$  & $8.96$  & $13.67$ & $7.01$          & $13.46$ & $\mathbf{3.04}$ \\
& CIFAR-100 & $34.13$  & $13.63$ & $2.87$   & $7.93$  & $21.23$ & $1.26$          & $24.72$ & $\mathbf{1.16}$ \\
& SST       & $9.16$   & $2.46$  & $4.78$   & $2.40$  & $6.85$  & $1.75$          & $7.71$  & $\mathbf{0.58}$ \\ \midrule
\multirow{6}{*}{\parbox{3.5cm}{Quantity Skew: \\ Imbalanced $(0.4, 2)$}} 
& MNIST     & $4.60$   & $2.38$  & $-1.13$  & $2.33$  & $2.58$  & $1.58$          & $2.59$  & $\mathbf{1.28}$ \\
& FMNIST    & $5.71$   & $3.00$  & $-5.93$  & $2.72$  & $3.28$  & $1.73$          & $2.69$  & $\mathbf{1.09}$ \\
& SVHN      & $11.80$  & $5.04$  & $1.30$   & $4.77$  & $5.94$  & $2.21$          & $5.89$  & $\mathbf{1.71}$ \\
& CIFAR-10  & $29.65$  & $14.70$ & $-2.20$  & $10.60$ & $13.88$ & $8.03$          & $14.20$ & $\mathbf{6.62}$ \\
& CIFAR-100 & $41.66$  & $19.02$ & $6.92$   & $11.84$ & $10.63$ & $\mathbf{5.59}$ & $21.08$ & $7.64$          \\
& SST       & $11.08$  & $3.56$  & $3.63$   & $4.16$  & $3.85$  & $1.69$          & $8.80$  & $\mathbf{1.66}$ \\ \midrule
\multicolumn{2}{c}{Number of times that performs the best } & \multicolumn{2}{c}{$0/12$} & \multicolumn{2}{c}{$0/12$} & \multicolumn{2}{c}{$1/12$} & \multicolumn{2}{c}{$\mathbf{11/12}$} \\ \bottomrule 
\end{tabular}
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*}



\subsection{Per-participant performance}
\label{appendix: per-participant-performance} 

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/cifar100-radar-plots.pdf}
    \caption{Per-participant performance comparison of Aequa and other baseline methods on CIFAR-100 dataset under Dirichlet ($\alpha=0.5$), label skew $(0.15, 6)$ and quantity skew $(0.4, 2)$ partitions. } 
    \label{fig: per-client-performance}
\end{figure}

In this section, we present a per-participant performance analysis of Aequa and other baseline methods on the CIFAR-100 dataset under the following partitioning strategies: Dirichlet ($\alpha=0.5$), label skew $(0.15, 6)$ and quantity skew $(0.4, 2)$. 

From the results, we observe that IAFL exhibits high variability in the first two partitioning strategies, and in some random seeds, IAFL fails to achieve a positive collaboration gain. On the other hand, CGSV remains very close to standalone accuracy, significantly limiting the overall collaboration gain of the participants. Meanwhile, Aequa achieves performance comparable to FedAvg-FT, while precisely capturing the correlation between final model accuracies and client contributions, reinforcing its fairness properties. 

In the quantity skew partition, we observe that Clients 1 and 2 are the highest contributors, as each holds 40\% of the dataset. The goal of a fair allocation algorithm is to match this distribution while expanding the total collaboration gain. Both IAFL and Aequa produce similar-shaped performance plots to the standalone accuracy, consistently ensuring positive collaboration gains. However, Aequa provides even better performance to Clients 1 and 2 compared to FedAvg-FT, while also maximizing the mean collaboration gain (MCG) across all participants, demonstrating its effectiveness in fair model allocation. 



\subsection{Model width as a reward}
In this section, we explore the transferability of Aequa when the server lacks prior knowledge of each sub-model's performance (does not possess a validation set). In this scenario, participants are rewarded directly based on the model width $(\vp)$, rather than using explicit performance-based allocations $(\va)$. 

To implement this approach, we use standalone accuracies as the contribution measure, and the allocation problem becomes mapping model widths to participant contributions. We employ the same allocation algorithm described in Section \ref{subsection: alloc-alg}, but modify the utility measure to be computed using normalized contributions, i.e. $c_i / \max_k c_k$, ensuring that both vectors share the same range of values before executing the allocation process. 

Table \ref{table: model-width-as-a-reward} presents the results of this experiment, covering three datasets across all partitioning strategies. The results demonstrate that the correlation coefficient between participant contributions (input) and assigned model widths (output) is perfectly aligned, consistently achieving a correlation coefficient of 1.0. 

Additionally, we report the correlation coefficient between the contribution measure and the assigned sub-modelâ€™s corresponding accuracy. The findings indicate that even when the server lacks knowledge of each sub-modelâ€™s accuracy, high fairness performance is still achievable. Compared to the results presented in the main paper, this method still outperforms all baseline approaches, demonstrating that Aequa maintains strong fairness properties. 



\begin{table*}[t]
    \caption{Incentivization performance of Aequa on CIFAR-10, CIFAR-100, and MNIST datasets under various partitioning strategies, evaluated using the Pearson correlation coefficient between (1) assigned model widths and contributions and (2) final model accuracies and contributions. Results are averaged over five independent runs.} 
    \vskip -0.1in
    \label{table: model-width-as-a-reward}
    \begin{center}
        \begin{small}
            \begin{sc}
                \begin{tabular}{llcc}
\toprule
\rowcolor{lightgray} Dataset & Partition & \multicolumn{1}{c}{$\rho(\vc, \vp)$ (contribution, width)} & \multicolumn{1}{c}{$\rho(\vc, \va)$ (contribution, acc.)} \\ \midrule
\multirow{9}{*}{CIFAR-10} 
& Homogeneous                   & $1.00 \pm 0.00$ & $0.94 \pm 0.01$  \\
& Dirichlet $(\alpha=0.1)$      & $1.00 \pm 0.00$ & $0.91 \pm 0.04$  \\
& Dirichlet $(\alpha=0.5)$      & $1.00 \pm 0.00$ & $0.95 \pm 0.02$  \\
& Dirichlet $(\alpha=1.0)$      & $1.00 \pm 0.00$ & $0.95 \pm 0.02$  \\
& Dirichlet $(\alpha=2.0)$      & $1.00 \pm 0.00$ & $0.96 \pm 0.02$  \\
& Dirichlet $(\alpha=5.0)$      & $1.00 \pm 0.00$ & $0.92 \pm 0.04$  \\ 
& Quantity Skew $(0.15, 6)$     & $1.00 \pm 0.00$ & $1.00 \pm 0.00$  \\
& Quantity Skew $(0.4, 2)$      & $1.00 \pm 0.00$ & $0.99 \pm 0.01$  \\
& Label Skew $(\#C=\{3, 30\})$  & $1.00 \pm 0.00$ & $0.96 \pm 0.02$  \\ \midrule
\multirow{9}{*}{CIFAR-100} 
& Homogeneous                   & $1.00 \pm 0.00$ & $0.95 \pm 0.01$  \\
& Dirichlet $(\alpha=0.1)$      & $1.00 \pm 0.00$ & $0.96 \pm 0.02$  \\
& Dirichlet $(\alpha=0.5)$      & $1.00 \pm 0.00$ & $0.92 \pm 0.02$  \\
& Dirichlet $(\alpha=1.0)$      & $1.00 \pm 0.00$ & $0.94 \pm 0.03$  \\
& Dirichlet $(\alpha=2.0)$      & $1.00 \pm 0.00$ & $0.95 \pm 0.03$  \\
& Dirichlet $(\alpha=5.0)$      & $1.00 \pm 0.00$ & $0.96 \pm 0.02$  \\ 
& Quantity Skew $(0.15, 6)$     & $1.00 \pm 0.00$ & $1.00 \pm 0.00$  \\
& Quantity Skew $(0.4, 2)$      & $1.00 \pm 0.00$ & $1.00 \pm 0.00$  \\
& Label Skew $(\#C=\{3, 30\})$  & $1.00 \pm 0.00$ & $0.95 \pm 0.02$  \\ \midrule
\multirow{9}{*}{MNIST} 
& Homogeneous                   & $1.00 \pm 0.00$ & $0.91 \pm 0.04$  \\
& Dirichlet $(\alpha=0.1)$      & $1.00 \pm 0.00$ & $0.90 \pm 0.03$  \\
& Dirichlet $(\alpha=0.5)$      & $1.00 \pm 0.00$ & $0.92 \pm 0.04$  \\
& Dirichlet $(\alpha=1.0)$      & $1.00 \pm 0.00$ & $0.92 \pm 0.03$  \\
& Dirichlet $(\alpha=2.0)$      & $1.00 \pm 0.00$ & $0.94 \pm 0.04$  \\
& Dirichlet $(\alpha=5.0)$      & $1.00 \pm 0.00$ & $0.88 \pm 0.03$  \\ 
& Quantity Skew $(0.15, 6)$     & $1.00 \pm 0.00$ & $0.95 \pm 0.04$  \\
& Quantity Skew $(0.4, 2)$      & $1.00 \pm 0.00$ & $0.94 \pm 0.04$  \\
& Label Skew $(\#C=\{3, 30\})$  & $1.00 \pm 0.00$ & $0.90 \pm 0.04$  \\ \bottomrule 
\end{tabular}
            \end{sc}
        \end{small}
    \end{center}
\vskip -0.1in
\end{table*}







\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
