\subsection{Experimental Setup}
\label{sec:setup}

\subsubsection{System used}
\label{sec:system}

We utilize a server featuring two Intel Xeon Gold 6226R processors, each with 16 cores clocked at 2.90 GHz, along with 512 GB of RAM. Each core has a 1 MB L1 cache, a 16 MB L2 cache, and a 22 MB shared L3 cache. This system runs CentOS Stream 8. For GPU-based evaluations of cuGraph, we use a separate server equipped with an NVIDIA A100 GPU, which has 108 SMs with 64 CUDA cores each, 80 GB of global memory, a bandwidth of 1935 GB/s, and 164 KB of shared memory per SM. This server has an AMD EPYC-7742 processor with 64 cores running at 2.25 GHz, 512 GB of DDR4 RAM, and Ubuntu 20.04.


\subsubsection{Configuration}
\label{sec:configuration}

We use 32-bit integers for vertex IDs. For compilation, we employ GCC 13.2 and OpenMP 5.0 on the CPU-only system, while on the GPU system, we use GCC 9.4, OpenMP 5.0, and CUDA 11.4. Our CP2AA allocator, used in the DiGraph, has a pool size of $512$ KB and handles allocations between $16$ and $8192$ bytes, with other sizes routed to \texttt{new[]}/\texttt{delete[]}. When reading graphs, each thread processes text in $256$ KB blocks and uses four partitions when converting per-thread Edgelist to CSR. Most parallel operations leverage OpenMP dynamic scheduling with chunk sizes of $512$, $1024$, or $2048$. For DiGraph updates (see \texttt{update()} in Algorithm \ref{alg:digraph2}), we employ task-based parallelism in OpenMP for vertices with a degree greater than $2048$.


\subsubsection{Dataset}
\label{sec:dataset}

The graphs used in our experiments are listed in Table \ref{tab:dataset}, and they are sourced from the SuiteSparse Matrix Collection \cite{suite19}. The graphs vary in size, with the number of vertices ranging from $3.07$ million to $214$ million, and the number of edges ranging from $25.4$ million to $3.80$ billion. We ensure that the edges are undirected and weighted, with a default weight of $1$.

\input{src/tab-dataset}




\subsection{Performance Comparison}
\label{sec:performance-comparison}  

\subsubsection{Loading a Graph from Disk}

We now compare the performance of PetGraph \cite{sverdrup2025petgraph}, SNAP \cite{leskovec2016snap}, SuiteSparse:GraphBLAS \cite{davis2023algorithm, davis2019algorithm}, cuGraph \cite{kang2023cugraph}, Aspen \cite{dhulipala2019low}, and our DiGraph in loading graphs from files into memory. PetGraph is a sequential Rust implementation, cuGraph is a parallel GPU-based implementation, and SNAP, SuiteSparse:GraphBLAS, Aspen, and our DiGraph are multicore implementations. For PetGraph, we use \texttt{MtxData::from\_file()} to read a Matrix Market (MTX) file, extracting the matrix shape, non-zero indices, and symmetry information. We initialize a directed graph (\texttt{DiGraphMap}) and add vertices corresponding to each matrix row. We then iterate over index pairs, adding the edges with a weight of $1$, inserting an additional reverse edge if the matrix is symmetric. We measure runtime using \texttt{Instant::now()} before and after loading. For SNAP, we first convert the MTX file to an EdgeList format, ensuring isolated vertices are included. We then load the EdgeList file as a \texttt{Graph} using \texttt{TSnap::LoadEdgeList()} and explicitly add any isolated vertices. We measure runtime using \texttt{clock()} before and after loading, excluding the conversion time from MTX to EdgeList. With SuiteSparse:GraphBLAS, we use \texttt{LAGraph\_MMRead()} to read the MTX file into a \texttt{GrB\_Matrix}, then construct a graph using the \texttt{LAGraph\_New()} function. We record the runtime using \texttt{LAGraph\_WallClockTime()}. Note that the LAGraph library is a collection of high-level graph algorithms built on the GraphBLAS C API, using linear algebra for graph computations.

For cuGraph, we first convert the MTX file to a CSV format. We then read the CSV into a dataframe using \texttt{cudf.read\_csv()}, setting edge weights to $1$. We create an empty graph $G$ with \texttt{cugraph.Gra} \texttt{ph()} and add edges using \texttt{G.from\_cudf\_edgelist()}. Runtime is measured using \texttt{time.time()}, before and after loading, and does not include the MTX-to-CSV conversion time. With Aspen, we first converting the MTX file to an Adjacency graph format, which is supported by Aspen, and is equivalent to the CSR format in plaintext. We then use \texttt{initialize\_graph()} to read the file into a versioned graph. Runtime is measured using \texttt{std::chrono::high\_resoluti} \texttt{on\_clock::now()} before and after loading, excluding the conversion time. However, as the Adjacency graph format is equivalent to CSR, this gives Aspen an unfair advantage. For our DiGraph, we use Algorithm \ref{alg:load} to load the MTX file into a CSR graph. We then measure runtime using \texttt{std::chrono::high\_resolution\_clock::now()} before and after execution of the algorithm. In all cases, runtime is averaged over five runs to minimize measurement noise.

\input{src/fig-static-read}

Figure \ref{fig:static-read--runtime} presents the runtime for loading graphs from files into memory for each graph in the dataset (Table \ref{tab:dataset}) across PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, Aspen, and Our DiGraph, while Figure \ref{fig:static-read--speedup} shows the speedup of Our DiGraph relative to the other implementations. Due to out-of-memory issues, cuGraph fails to load the \textit{uk-2005}, \textit{webbase-2001}, \textit{it-2004}, and \textit{sk-2005} graphs, so these results are omitted. Our DiGraph achieves a mean speedup of $177\times$, $106\times$, $76\times$, $17\times$, and $3.3\times$ compared to PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, and Aspen, respectively. On the \textit{sk-2005} graph, it loads data in just $5.1$ seconds, reaching a graph loading rate of $379$ million edges per second. Notably, as discussed above, Aspen loads graphs\ignore{from disk} in the Adjacency graph format, equivalent to CSR, which likely gives it an unfair advantage over other frameworks. These results highlight the potential for significant improvements in graph loading times for state-of-the-art graph processing frameworks, and Algorithm \ref{alg:load} can help achieve this.


\subsubsection{Cloning a Graph}

Next, we compare the performance of PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, Aspen, and our DiGraph in cloning or obtaining a snapshot of a graph $G$. In PetGraph, cloning is performed using $G.clone()$. For SNAP, we find that using \texttt{TSnap::ConvertGraph()} is slower than reloading the graph from disk, so we opt for the latter. With SuiteSparse:GraphBLAS, we clone a graph by duplicating its adjacency matrix with \texttt{GrB\_Matrix\_dup()} and converting it to a graph via \texttt{LAGraph\_New()}. In cuGraph, we copy the dataframe containing the graph's edge list and convert it into a graph using \texttt{cugraph.Graph()} and $G'.from\_cudf\_edgelist()$, where $G'$ is the new graph. For Aspen, we take a snapshot of the graph with $G.acquire\_version()$. Finally, Our DiGraph uses Algorithm \ref{alg:clone} for cloning. Runtime is measured as discussed earlier, averaged over five runs.

\input{src/fig-static-clone}

Figure \ref{fig:static-clone--runtime} presents the runtime for cloning graphs, for each graph in the dataset, across PetGraph, SNAP, SuiteSparse:Graph-BLAS, cuGraph, Aspen, and our DiGraph, while Figure \ref{fig:static-clone--speedup} shows the speedup of our DiGraph relative to the other implementations. Due to out-of-memory issues, results of cuGraph are omitted for the \textit{uk-2005}, \textit{webbase-2001}, \textit{it-2004}, and \textit{sk-2005} graphs. Our DiGraph achieves a mean speedup of $20\times$, $235\times$, $0.24\times$, $1.3\times$, and $0\times$ compared to PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, and Aspen, respectively. Aspen’s graph cloning is effectively zero-cost since we take a snapshot with its $G.acquire\_version()$, which only requires pointing to the root node of the vertex tree. In addition, SuiteSparse:GraphBLAS likely performs a shallow copy of the adjacency matrix, copying individual rows/columns only when modified. In contrast, PetGraph, SNAP, cuGraph, and our DiGraph perform full deep copies. On the \textit{sk-2005} graph, our DiGraph deep copies the graph in only $2.1$ seconds, thus achieving a rate of $908M$ edges/s. These results highlight that our DiGraph performs efficient deep copies (using Algorithm \ref{alg:clone}), while also demonstrating the significant advantages of Aspen’s C-trees and the lazy copy behavior of SuiteSparse:GraphBLAS.

\input{src/fig-split-runtime}


\subsubsection{Performing Edge Deletions}
\label{sec:perform-edge-deletions}

We now evaluate performance of the graph implementations --- PetGraph, SNAP, SuiteSparse:Graph-BLAS, cuGraph, Aspen, and our DiGraph --- in performing a batch of edge deletions, both in-place and by creating a new graph instance without the specified edges. For PetGraph, edge deletions are performed using the method $G.remove\_edge()$ in a loop over the batch of edges to be deleted. To perform edge deletions onto a new graph instance, we simply clone the original graph and remove the edges using the same method. In SNAP, edges are removed by using $G.DelEdge()$ iteratively, while for new graph instances, we reload the graph from disk and remove the edges accordingly. With SuiteSparse:GraphBLAS, we remove edges by deleting entries from its adjacency matrix via \texttt{GrB\_Matrix\_removeElement()}. For a new graph instance, we duplicate the adjacency matrix with \texttt{GrB\_Matrix\_dup()}, create a new graph using \texttt{LAGraph\_New()}, and remove edges in the same way. In cuGraph, we construct a new edge list through a left merge on the original, followed by drop and rename operations, then build a new graph $G'$ using $cugraph.Graph()$ and $G'.from\_cudf\_edgelist()$. For new-instance deletions, the original edge list is copied before applying the same process. For Aspen, we delete edges by calling $G.delete\_edges\_batch()$, which returns a new graph snapshot with the specified edges removed. There is no in-place deletion mechanism in Aspen. Finally, our DiGraph removes edges using \texttt{subtractGraphInplace()} in Algorithm \ref{alg:sub}. To perform edge deletions on a new graph instance, we use the \texttt{subtractGraph()} function instead, which is more efficient than cloning the original graph and then removing the edges in-place.

\input{src/fig-dynamic-del-runtime}

We test the performance of these implementations\ignore{(in removing a subset of edges, both in-place and by creating a new graph instance without the specified edges)} on large graphs, from Table \ref{tab:dataset}, with random batch updates, consisting of edge deletions. Edges are uniformly deleted, with batch sizes ranging from $10^{-7} |E|$ to $0.1|E|$ (batch size is measured as a fraction of edges in the original graph), and multiple random batch updates per batch size are generated for averaging. Figure \ref{fig:dynamic-del-runtime--mean} shows the overall runtime of in-place deletions at each batch size across all implementations, while Figure \ref{fig:dynamic-del-runtime--all} details runtimes for each framework and batch size on individual graphs. Due to out-of-memory issues, results for cuGraph are omitted for \textit{uk-2005}, \textit{webbase-2001}, \textit{it-2004}, and the \textit{sk-2005} graphs. Additionally, Figure \ref{fig:dynamic-delnew-runtime--mean} illustrates the overall runtimes for each framework when deleting edges into a new graph instance, with Figure \ref{fig:dynamic-delnew-runtime--all} providing per-graph results.

\input{src/fig-dynamic-delnew-runtime}

Figure \ref{fig:dynamic-del-runtime--mean} shows that, on batch updates of size $10^{-4}|E|$ to $0.1|E|$, our DiGraph offers a mean speedup of $141\times$, $44\times$, $13\times$, $28\times$, and $3.5\times$ over PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, and Aspen, respectively, for in-place edge deletions. This is due to efficient parallel deletion of edges using $\textit{setDifference()}$ upon independent vertices (in Algorithms \ref{alg:digraph2} and \ref{alg:sub}). However, for small batch updates, the overhead of iterating over all vertices, leads to lower performance. When deleting edges into a new graph instance, our DiGraph achieves a mean speedup of $87\times$, $6.3\times$, $2.4\times$, $2.3\times$, and $0.29\times$ over PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, and Aspen, respectively. These results underscore the limitations of current state-of-the-art graph processing frameworks in handling large batch updates (both in-place and when creating a new graph instance) and highlight the advantages of C-trees, as used by Aspen, which enable lightweight graph snapshots and superior performance in batch updates involving new graph instances. They also suggest that our DiGraph could benefit from further optimizations, particularly for small batch sizes.


\subsubsection{Performing Edge Insertions}

Next, we evaluate the performance of the graph implementations in performing a batch of edge insertions, both in-place and by creating a new graph instance that includes the newly added edges. For PetGraph, edges are inserted using the method $G.add\_edge()$ in a loop over the batch of edges to be inserted. To insert edges into a new graph instance, we first clone the original graph and then add the edges using the same method. In SNAP, edges are inserted iteratively using $G.AddEdge()$, while for new graph instances, we reload the graph from disk and then perform insertions. In SuiteSparse:GraphBLAS, edges are inserted by updating entries in its adjacency matrix using \texttt{GrB\_Matrix\_setElement()}. For a new graph instance, we duplicate the adjacency matrix with \texttt{GrB\_Matrix\_dup()}, create a new graph using \texttt{LAGraph\_New()}, and then insert edges using the same method. In cuGraph, we construct a new edge list by performing a left merge operation with the original, dropping the duplicate entries, renaming the column name in the dataframe, and concatenating the insertions, before building a new graph $G'$ using $cugraph.Graph()$ and $G'.from\_cudf\_edgelist()$. For new-instance insertions, the original edge list is copied before undergoing the same process. In Aspen, edges are inserted using $G.insert\_edges\_b$ $atch()$, which produces a new graph snapshot containing the added edges, as Aspen does not support in-place insertions. Finally, our DiGraph inserts edges using \texttt{addGraphInplace()} as described in Algorithm \ref{alg:add}. To perform insertions into a new graph instance, we use \texttt{addGraph()}, which is more efficient than cloning the original graph and then inserting edges in-place.

\input{src/fig-dynamic-ins-runtime}

We test the performance of these implementations on large graphs from Table \ref{tab:dataset}, with random batch updates consisting of edge insertions. To prepare the set of edges for insertion, we select vertex pairs with equal probability, with batch sizes ranging from $10^{-7} |E|$ to $0.1|E|$ (batch size measured as a fraction of edges in the original graph). Multiple random batch updates per batch size are generated for averaging, as earlier. Figure \ref{fig:dynamic-ins-runtime--mean} presents the overall runtime of in-place insertions at each batch size across all implementations, while Figure \ref{fig:dynamic-ins-runtime--all} details runtimes for each framework and batch size on individual graphs.\ignore{Results for cuGraph are omitted for \textit{uk-2005}, \textit{webbase-2001}, \textit{it-2004}, and \textit{sk-2005} due to out-of-memory issues.} Additionally, Figure \ref{fig:dynamic-insnew-runtime--mean} illustrates the overall runtimes when inserting edges into a new graph instance, with Figure \ref{fig:dynamic-insnew-runtime--all} providing per-graph results.

\input{src/fig-dynamic-insnew-runtime}

Figure \ref{fig:dynamic-ins-runtime--mean} shows that for batch updates ranging from $10^{-4}|E|$ to $0.1|E|$, our DiGraph achieves a mean speedup of $45\times$, $25\times$, $11\times$, $34\times$, and $2.2\times$ over PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, and Aspen, respectively, for in-place edge insertions. This performance gain is due to efficient parallel insertion using $\textit{setUnion()}$ on independent vertices, as described in Algorithms \ref{alg:digraph2} and \ref{alg:add}. However, for small batch sizes, the overhead of iterating over all vertices reduces performance (as with edge deletions). When inserting edges into a new graph instance, our DiGraph attains a mean speedup of $58\times$, $4.7\times$, $2.9\times$, $4.1\times$, and $0.26\times$ over the same frameworks. These findings align with the conclusions discussed in Section \ref{sec:perform-edge-deletions}.


\subsubsection{Performance on $k$-step Reverse Walks}
\label{sec:perform-reverse-walks}

Finally, we evaluate the performance of various graph representations on a representative algorithm by measuring the efficiency of $k$-step reverse walks from each vertex in the given input graph, and counting the number of walks ending at each vertex. This corresponds to computing $A_T^k \cdot \vec{1}$, where $A_T$ is the transposed adjacency matrix and $\vec{1}$ is a ones vector. Reverse walks are preferred as they can be executed directly on the input graph, whereas forward walks require its transpose. The purpose of this evaluation is to assess the efficiency of different frameworks in processing graph algorithms on updated graphs, ensuring they prioritize algorithm execution alongside fast data structure updates. For PetGraph, SNAP, Aspen, and our DiGraph, we implement this using their respective APIs, following a similar approach to Algorithm \ref{alg:visit}. In SuiteSparse:GraphBLAS, we use the \texttt{GrB\_vxm()} function repeatedly to compute $A_T^k \cdot \vec{1}$, while in cuGraph, we utilize \texttt{cupyx.scatter\_add()}. Note however that PetGraph is executed sequentially, while the others are parallelized.

\input{src/fig-dynamic-visdel-runtime}

We evaluate the efficiency of the updated graphs in each graph processing framework by testing their performance on the dataset, using random batch updates consisting of either edge deletions or insertions. Edges are uniformly deleted, while insertion edges are selected by choosing vertex pairs with equal probability. Batch sizes range from $10^{-7} |E|$ to $0.1|E|$, as earlier, and multiple random batch updates per batch size are generated for averaging. Figure \ref{fig:dynamic-visdel-runtime--mean} presents the overall runtime of performing $42$-step reverse walks on graphs with edge deletions at each batch size across all implementations, while Figure \ref{fig:dynamic-visdel-runtime--all} details runtimes per framework and graph. Due to out-of-memory issues, cuGraph results are omitted for \textit{uk-2005}, \textit{webbase-2001}, \textit{it-2004}, and \textit{sk-2005}. Similarly, Figure \ref{fig:dynamic-visins-runtime--mean} shows overall runtimes for $42$-step reverse walks with edge insertions, with Figure \ref{fig:dynamic-visins-runtime--all} providing per-graph results.

\input{src/fig-dynamic-visins-runtime}

Figure \ref{fig:dynamic-visdel-runtime--mean} illustrates that for batch updates ranging from $10^{-7}|E|$ to $0.1|E|$, our DiGraph achieves an average speedup of $67\times$, $86\times$, $2.5\times$, $0.25\times$, and $1.3\times$ over PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, and Aspen, respectively, when performing $k$-step reverse walks on graphs with edge deletions. This performance gain is largely due to the fact that all edges are stored contiguously, even after a batch update, and our DiGraph is designed in a Struct-of-Arrays (SoA) format, both of which enable high cache locality. cuGraph outperforms our DiGraph because it is being executed on the GPU. We also note that the performance of Aspen starts to slightly degrade as the batch size increases, which is likely due to fragmentation in the C-tree. When executing reverse walks on graphs with newly inserted edges, our DiGraph achieves mean speedups of $63\times$, $86\times$, $2.6\times$, $0.24\times$, and $1.3\times$ over the same frameworks. These results point to the inefficiencies in representations of existing graph processing frameworks, but also highlight the advantages of C-trees, as utilized by Aspen, which offer performance similar to our DiGraph, with the added advantage of fast graph snapshots.








% We now compare the performance of PetGraph \cite{sverdrup2025petgraph}, SNAP \cite{leskovec2016snap}, SuiteSparse:GraphBLAS \cite{davis2023algorithm, davis2019algorithm}, cuGraph \cite{kang2023cugraph}, Aspen \cite{dhulipala2019low}, and Our DiGraph in loading graphs from files into memory.
% With PetGraph, we use \texttt{MtxData::from_file()} to read the contents of Matrix Market (MTX) files, extracting matrix shape, indices of non-zero entries, and symmetry information. We then initialize a directed graph, \texttt{DiGraphMap}, and add vertices corresponding to each row of the matrix. We iterate over the list of index pairs, adding directed edges between nodes \texttt{a} and \texttt{b} with a weight of $1$. If the matrix is symmetric, we add an additional edge in the reverse direction. Runtime is measured using \texttt{Instant::now()} before and after the operation, and the difference is recorded.
% With SNAP, we convert the MTX file to an EdgeList format file, while also keeping track of the number of vertices and edges (this is important, since the original graph may have some isolated vertices, which would not be included in an edgelist). Next, we load the Edgelist file as a \texttt{Graph} object using \texttt{TSnap::LoadEdgeList()}. After that, we ensure that even the isolated vertices are included in the graph by adding them with no edges. We then measure the runtime using \texttt{clock()} before and after the operation. The runtime does not include the time taken to convert the MTX file to an EdgeList file.
% With SuiteSparse:GraphBLAS, we use the \texttt{LAGraph_MMRead()} function to read the contents of a Matrix Market (MTX) file into a \texttt{GrB\_Matrix}. We then use the \texttt{GrB\_Matrix} to contruct a graph using \texttt{LAGraph_New()}. Runtime is measured using \texttt{LAGraph_WallClockTime()} before and after the operation, and the difference is recorded.
% With cuGraph, similar to SNAP, we first convert the MTX file to a CSV file. We then use the \texttt{cudf.read\_csv()} function to read the contents of the CSV file into a dataframe $gdf$, and set the edge weights to $1$. We then create an empty graph $G$ using \texttt{cugraph.Graph()} and add edges in the dataframe to the graph using \texttt{G.from\_cudf\_edgelist()}. Runtime is measured using \texttt{time.time()} before and after the operation, and the difference is recorded. As earlier, the runtime does not include the time taken to convert the MTX file to a CSV file.
% With Aspen, we first convert the MTX file to an Adjacency graph format which is used by Aspen, and is equivalent to the CSR format in plaintext. We then use the \texttt{initialize\_graph()} function to read the contents of the file into a versioned graph (\texttt{versioned\_graph<treeplus\_graph>}). Runtime is measured using \texttt{std::chrono::high\_resolution\_clock::now()} before and after the operation, and the difference is recorded. The runtime does not include the time taken to convert the MTX file to an Adjacency graph file. However, as the Adjacency graph format is equivalent to the CSR format, this gives a unfair advantage to Aspen.
% With Our DiGraph, we use Algorithm \ref{alg:load} to read the contents of the MTX file into a directed graph. Runtime is measured using \texttt{std::chrono::high\_resolution\_clock::now()} before and after the operation, and the difference is recorded.

% Next, we compare the performance of PetGraph, SNAP, SuiteSparse:GraphBLAS, cuGraph, Aspen, and Our DiGraph in cloning (or obtaining the snapshot of) a graph. With PetGraph, we clone a graph $G$ by calling $G.clone()$. With SNAP, we observe that cloning a graph using \texttt{TSnap::ConvertGraph()} is slower than simply reloading the graph from the disk, and thus we use the latter method. With SuiteSparse:GraphBLAS, we clone a graph by calling \texttt{GrB\_Matrix\_dup()} on the adjacency matrix of the graph, and then converting the matrix to a graph using \texttt{LAGraph\_New()}. With cuGraph, we clone a graph by calling making a copy of the dataframe containing the edge list of the graph, and then converting the dataframe to a graph using \texttt{cugraph.Graph()} and $G'.from\_cudf\_edgelist()$, where $G'$ is the new graph. With Aspen, we clone a graph $G$ by simply taking a snapshot of the graph with $G.acquire\_version()$. With Our DiGraph, we clone a graph with Algorithm \ref{alg:clone}.
