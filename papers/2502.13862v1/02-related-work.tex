PetGraph \cite{sverdrup2025petgraph} is a general-purpose graph library written in Rust. It offers several graph implementations, each with its own tradeoffs. We find its \texttt{GraphMap} structure to offer the best performance for the workload we are testing, and hence we focus on it in this report. A \texttt{GraphMap} uses an associative array (i.e. a hashmap) to represent vertices, and an inner hashmap for each vertex to represent its neighbors. This allows it to achieve quick lookup ($O(1)$ average per neighbor check), and testing whether an edge exists is simply a matter of doing two hashmap lookups. PetGraph provides support for single-edge operations, and these can be invoked in a loop for batch updates. When an edge is added (using \texttt{add\_edge()}), PetGraph first locates the source node in its hashmap and then inserts the new destination ID into the node's inner hashmap. Similarly, deleting an edge (using \texttt{remove\_edge()}) involves locating the appropriate neighbor entry in the inner hashmap and then removing it.

Stanford Network Analysis Platform (SNAP) \cite{leskovec2016snap} is an open-source C++ library (with a Python interface, Snap.py) for analyzing and manipulating large-scale networks. It supports undirected graphs (TUNGraph), directed graphs (TNGraph), and multigraphs (TNEGraph). Nodes are stored in hash tables, keyed by unique integer IDs. Each node maintains one (for undirected graphs) or two sorted vectors (outgoing and incoming edges, for directed graphs) listing its neighbors. Sorting these vectors facilitates fast lookup via binary search while keeping the memory footprint compact. SNAP builds on top of vectors and hash tables (similar to the STL but optimized for SNAP's use cases) that are implemented in the GLib library developed at the Jozef Stefan Institute. It also provides support for single-edge operations, and these can be invoked in a loop for batch updates. When an edge is added, SNAP first locates the source node in its hash table ($O(1)$ average time) and then inserts the new destination ID into the node's sorted neighbor vector. Similarly, deleting an edge involves locating the appropriate neighbor entry in the sorted vector and then removing it.

SuiteSparse:GraphBLAS \cite{davis2023algorithm, davis2019algorithm} is an open-source C library providing an efficient parallel implementation of the GraphBLAS standard for (sparse) matrix-based graph algorithms. GraphBLAS objects (matrices and vectors) are opaque and stored in standard compressed formats: Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC). In these formats, a matrix comprises \textbf{(1)} a pointer array indicating the start of each column/row, \textbf{(2)} an index array storing row indices (CSC) or column indices (CSR), and \textbf{(3)} a value array holding nonzero entries. Dynamic graph updates are handled via \textit{zombies} and \textit{pending tuples}: deleted edges are marked as zombies by modifying indices instead of immediate removal, preventing costly restructuring, while new edge insertions are temporarily stored as pending tuples in an unsorted list, enabling fast incremental updates in $O(\log k)$ time (for $k$ entries in a column). A subsequent assembly phase consolidates these pending updates by removing zombie entries, and flushing pending tuples by sorting and merging them into the data arrays. This lazy consolidation amortizes the cost of incremental updates\ignore{over multiple operations}, and is only done when an operation is invoked that requires the matrix to be in a fully assembled state.

cuGraph \cite{kang2023cugraph} is NVIDIA's GPU-accelerated graph analytics library that is part of the RAPIDS suite. cuGraph accepts graph data in various forms --- edge lists or adjacency lists --- and can ingest data stored in GPU DataFrames provided by cuDF. This makes it easy to integrate with RAPIDS workflows, where data preparation and graph processing can be done entirely on the GPU. Internally, cuGraph represents graphs in a CSR-like format, where row offsets mark the start of each vertex's edge list, column indices store neighbor vertex IDs, and optional edge attributes hold weights for weighted graphs. Building the CSR representation typically requires sorting the edges, performing parallel scan to compute the row offsets, and copying the neighbor indices. Performing a batch update of edge insertions involves a parallel merge with the existing sorted edge list and removing duplicates before creating a new graph. For deletions, edges marked for removal are filtered out and a new graph is built from the remaining edges.

Aspen \cite{dhulipala2019low} is a low-latency graph streaming framework designed to support concurrent graph queries and dynamic updates on large graphs, and has been shown to outperform other dynamic graph streaming systems (e.g., Stinger and LLAMA) in both throughput and memory usage. Its core idea is to represent the graph using compressed purely-functional search trees, called C-trees. Both the vertex set and the adjacency lists are represented using C-trees. The vertex set is maintained in a persistent (immutable) vertex-tree, and each vertex's adjacency list is stored in an edge-tree that is itself a C-tree. In a C-tree, instead of storing one element per node, elements are ``chunked" into arrays. This not only reduces the number of nodes (thus saving memory) but also improves cache locality. For integer data (typical for vertex IDs and edges), Aspen applies difference encoding within these chunks to further compress the representation. Because the C-trees are purely functional (immutable) \cite{okasaki1998purely}, every update creates a new version by copying only the nodes along the modified path (copy-on-write). This design enables lightweight snapshots and concurrent operations: readers can acquire a consistent snapshot of the graph instantly (this is simply a pointer, or handle, to the graph's root), while a single writer can apply updates without blocking queries.

We now describe how Aspen loads a graph, clones it to create snapshots, and applies batch updates of edge insertions and deletions. Aspen expects input graphs in a variant of the “adjacency graph” format (similar to what Ligra uses). The file typically starts with metadata (e.g., number of vertices and edges) followed by offsets and edge lists. The loader reads the graph file and constructs a persistent vertex-tree (using a framework such as PAM for augmented maps). Each vertex is inserted along with its metadata. For each vertex, the corresponding list of out-edges is read and converted into an edge-tree (a C-tree). This process involves “chunking” the sorted edge list and applying difference encoding for compression. For applying batch updates of edge insertions/deletions, Apsen provides functions such as \texttt{insert\_edges\_batch()} and \texttt{delete\_edges\_batch()}. Both functions first preprocess the batch of updates to group them by source vertex and aggregate the new edges (or edges to delete) for each vertex. Then, for each source vertex, the corresponding edge-tree is updated using a \textit{MultiInsert} or \textit{MultiDelete} operation. These operations are efficient because they operate on chunks and only require updating the nodes along the modified paths. While only one writer is allowed to update at a time, the batch processing is designed to be highly parallel internally, allowing different vertex updates to be processed concurrently in the union/difference steps if they affect different parts of the tree. Aspen's API also includes functions to manage graph versions and handle garbage collection (reclaiming old versions when unused).

Finally, we discuss how Aspen handles memory management. Aspen employs a parallel reference counting garbage collector and a custom pool-based allocator to manage memory efficiently. It uses a concurrent memory allocator, \texttt{list\_allocator}, which minimizes contention by maintaining per-thread local memory pools. Each thread primarily operates on its local pool, fetching batches from a global stack as needed and returning excess memory when the pool surpasses a threshold. Memory is structured in a linked-list format and aligned to cache line boundaries to prevent false sharing.








% Aspen is a graph-streaming system, based on compressed purely-functional trees. It is designed for maintaining a dynamic graph subject to updates by a single writer, while supporting multiple concurrent readers. Due to the fact that the graph is purely-functional, all operations in Aspen are strictly serializable.
% An initial static graph can be generated using the \texttt{initialize\_graph} function, which takes as input several optional arguments (including if \texttt{mmap()} should be used, and if the graph is symmetric) and returns a \texttt{versioned\_graph}. A \texttt{versioned\_graph} is a single-writer, multi-reader interface, i.e., it can be read by multiple concurrent readers, which can acquire a version of the graph to perform queries or graph analyses, and a single writer, which can perform updates. It provides functions for atomically acquiring a snapshot of the graph, inserting a batch of edges (single writer), and deleting a batch of edges (single writer). The returned \texttt{snapshot\_graph} also provides functions for creating a flat graph snapshot.
% % HOW GRAPH READ WORKS IN ASPEN?
% Aspen supports the adjacency graph format, which is used by the Problem Based Benchmark Suite (PBBS) and Ligra. The adjacency format starts with a sequence of offsets, one for each vertex, followed by a sequence of directed edges ordered by their source vertex. The offset of vertex $i$ refers to the location of the start of a contiguous block of out edges for vertex $i$ in the sequence of edges. The block continues until the offset of the next vertex, or the end, if $i$ is the last vertex. All vertices and offsets are $0$-based, and represented in decimal. The file itself is represented as plain text.
% We have not measured memory usage of Aspen. However, it can be measured using a tool called \texttt{memory\_footprint}, which loads a graph using the C-tree data structure and outputs the number of bytes used by the C-tree representation. In our experiments, we set the probability of a node being selected as a head to $p = 1/256$, so the expected number of nodes in the edges tree is $p*m$.
% NOTE: Aspen uses the adjacency graph format, which is easier to load.
% Here is how the \texttt{read\_unweighted\_graph()} function works. First, the contents of the file are read into a \texttt{pbbs::sequence}, either by mapping the file to memory, or by reading the contents of the file using standard IO functions. Next, pbbs::tokenize() is used to split the file contents into tokens (by replacing whitespace with nulls). From there, the memory for offsets and edges are allocated, and they are populated in parallel using the atol() method. This is then used to build a versioned\_graph<treeplus\_graph>.
% To handle memory management, Aspen uses a parallel reference counting garbage collector along with a custom pool-based memory allocator. The pool-allocation is critical for achieving good performance due to the large number of small memory allocations in the functional setting.
% Aspen uses a `list\_allocator`, which is a concurrent memory allocator designed for efficiently allocating and deallocating memory blocks for a fixed type `T`. It is optimized for parallel execution by maintaining separate local pools of memory per thread, reducing contention on a global memory pool.
% Here’s how it works: Each thread has a local pool of memory blocks to reduce contention when allocating and freeing memory. If a thread’s local pool runs out of memory, it fetches a batch of memory blocks from a global stack. When a local pool exceeds a threshold, excess memory blocks are returned to the global pool. It supports preallocating memory for efficiency and optionally randomizing the memory layout for testing. The number of allocated memory blocks is tracked using atomic operations. The allocator works with a linked-list structure `block`. Each thread maintains a local list of allocated memory blocks - and prevents false sharing by aligning memory to cache line boundaries.
% The allocator maintains: (1) Global stack (global\_stack): Stores memory blocks available for reuse. (2) Per-thread lists (local\_lists): Each thread has its own memory list. (3) Atomic counter (blocks\_allocated): Keeps track of allocated blocks.
