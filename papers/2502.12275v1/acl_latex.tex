% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage{array}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Integrating Expert Knowledge into Logical Programs via LLMs}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Franciszek Górski*+\\
%   Gdansk University of Technology / Address line 1 \\
%   \texttt{email@domain} \\\And
%   Oskar Wysocki* \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\\
%   \texttt{email@domain} \\\And
%   Marco Valentino \\
%     Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\\
%   \texttt{email@domain} \\\And
%   Andre Freitas \\
%     Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\\
%   \texttt{email@domain}}

\author{
 \textbf{Franciszek Górski\textsuperscript{*,+,1}}, 
 \textbf{Oskar Wysocki\textsuperscript{*2,4}}, 
 \textbf{Marco Valentino\textsuperscript{4}}, 
 \textbf{Andre Freitas\textsuperscript{2,3,4}}
\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
\\
 \textsuperscript{*} Eqaul contribution,
 \textsuperscript{+} Corresponding author \\
 \textsuperscript{1}Multimedia Systems Department, Gdansk University of Technology, Poland, \\
 \textsuperscript{2}Department of Computer Science, University of Manchester, United Kingdom, \\
 \textsuperscript{3}National Biomarker Centre (NBC), CRUK Manchester Institute, United Kingdom, \\
 \textsuperscript{4}Idiap Research Institute, Martigny, Switzerland
\\
 \small{
   \textbf{Correspondence:} \href{mailto:email@domain}{firstname.lastname@pg.edu.pl}\textsuperscript{1} 
   \href{mailto:email@domain}{firstname.lastname@manchester.ac.uk}\textsuperscript{2}
   \href{mailto:email@domain}{firstname.lastname@idiap.ch}\textsuperscript{4}
 }
}

\begin{document}
\maketitle
\begin{abstract}
This paper introduces \textbf{ExKLoP}, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems.
This capability is especially valuable in engineering, where expert knowledge—such as manufacturer-recommended operational ranges—can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks.
We also explore the models' capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen.  Results reveal that while models generate nearly perfect syntactically correct code, they frequently exhibit logical errors in translating expert knowledge. Furthermore, iterative self-correction yields only marginal improvements (up to 3\%). Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered. The complete implementation, along with all relevant data, is available at \href{https://github.com/Franek18/ExKLoP}{GitHub}\footnote{\url{https://github.com/Franek18/ExKLoP}}.
%Future work should explore fine-tuned code-generation models, symbolic solvers, and reasoning-enhanced prompting techniques to further enhance LLM-driven constraint validation.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable abilities in translating natural language into executable code, opening new possibilities for integrating expert knowledge into Expert Systems. This capability is particularly promising in domains such as engineering, where technical manuals, guidelines, and service reports could serve as inputs for building expert-driven reasoning systems. Such systems would enable a new class of knowledge-informed machine learning models, seamlessly combining data-driven approaches with domain expertise.

However, for this vision to become a reality, we must first assess how effectively LLMs can translate domain-specific facts and logical rules into executable code. For instance, consider an industrial machine with multiple operational parameters and manufacturer-recommended ranges for normal operation. Ideally, an LLM should be able to encode these recommendations into a program that can automatically monitor the machine’s performance. Yet, the accuracy, reliability, and robustness of such model-generated code remain open questions.

To address these challenges, we require well-defined frameworks, evaluation datasets, and benchmarks to systematically assess LLMs' ability to generate executable reasoning rules. Expert systems demand precise logical rules, making it essential to investigate whether LLMs, even when initially incorrect, can self-correct (also via agentic systems) by leveraging feedback from external symbolic models (e.g., a Python interpreter) \cite{jiang2024leanreasoner, pan2023logic, kalyanpur2024llm, bi2024iterative, xu2024faithful}. The envisioned future of systems capable of producing fully functional reasoning modules through iterative refinement hinges on key architectural decisions—such as model selection, prompt formulation, and feedback mechanism design. 

In this paper, we introduce \textbf{ExKLoP}, a framework designed to assess LLMs’ capability to integrate expert knowledge into logical reasoning systems while evaluating their potential for self-correction. Our framework provides a structured approach to exploring the reliability of LLM-generated code via syntax, runtime and logic validation, before and after self-correction based on the output errors. We offer insights into both model performance and iterative improvement strategies.

To guide our investigation, we formulate the following key research questions:\newline
\textbf{RQ1}: Can large language models (LLMs) facilitate the integration of expert knowledge with AI and knowledge-informed machine learning models in engineering?\newline
\textbf{RQ2}: How efficiently can LLMs convert expert knowledge expressed in natural language into valid Python logic rules?\newline
\textbf{RQ3}: To what extent can LLMs self-correct their mistakes using feedback from external program interpreter?

To address these questions, we present the following contributions:\newline
$\bullet$ We introduce \textbf{ExKLoP}, an experimental framework for evaluating how effectively LLMs integrate expert knowledge from engineering domain into logical rules and executable code by measuring Formalization Success Rate and Logical Consistency Rate. Additionally, our framework assesses the models' capacity for self-correction by leveraging feedback from the execution environment.\newline
$\bullet$ ExKLoP provides a simple yet powerful approach to exploring the potential of agentic systems in knowledge-based reasoning. Specifically, we investigate whether iterative self-correction enhances the accuracy and reliability of model-generated outputs.\newline
$\bullet$ We introduce a scalable and extensible dataset generation methodology for engineering premises and validation points. ExKLoP, comprising 130 premises and 950 prompts, enables efficient benchmarking while allowing researchers to control task complexity and seamlessly expand experiments by integrating additional premises within a single prompt.\newline
$\bullet$ We demonstrate significant performance variations among similarly sized LLMs in generating correct logical rules. While most produce nearly flawless syntactic output, except for Llama 3, they often misrepresent expert knowledge, with iterative self-correction yielding only marginal improvements (up to 3\%).

\section{ExKLoP}
ExKLoP follows a structured evaluation process consisting of three key phases: (1) Translation of natural language premises into executable functions, (2) Syntax, runtime, and logic validation, and (3) Iterative refinement of incorrect outputs using an LLM-driven self-correction loop. This end-to-end workflow is illustrated in Figure \ref{fig:python_rules_gen_diagram}.

\begin{figure*}[h!]
    \centering
  \includegraphics[width=\textwidth]{latex/figures/framework_diagram.pdf}
  \caption {ExKLoP framework for evaluating expert knowledge integration in Python-based logical rules}
  \label{fig:python_rules_gen_diagram}
\end{figure*}

\subsection{Translation of Natural Language to Python Logic}
The primary task assigned to the LLM is to generate functions that accurately encode logical constraints specified in natural language. These premises define acceptable ranges for given physical parameters of an object, ensuring compliance with expected operational conditions. In engineering applications, such constraints often describe the normal operating ranges of mechanical systems. For example, an industrial machine may have predefined thresholds for temperature, pressure, and vibration that indicate safe operation. The prompt contains instruction, in-context examples and premises to be translated into code (for details see Section \ref{prompt_construction}).

\subsection{Syntax, runtime, and logic validation}
Once the Python functions are generated, they undergo a multi-stage validation process to ensure correctness across three dimensions:
\newline
\textbf{Syntax Validation:} The Python interpreter checks for syntactical correctness detecting any parsing errors, such as missing colons or incorrect indentation.
\newline
\textbf{Runtime Validation:} The generated code is executed using the validation dataset as a source of input data points to identify runtime errors, such as mismatched argument counts or undefined variables. 
%The generated functions are tested using a validation dataset containing measured values from real-world truck operations. This dataset includes both in-range and out-of-range values for each parameter. The functions are executed with these inputs to detect runtime errors, such as mismatched argument counts or undefined variables.
\newline
\textbf{Logical Verification}: This step determines whether the logic implemented by the model accurately represents the intended constraints. The functions take data points from the validation dataset and evaluate their outputs against ground truth labels.
\newline
Any errors are flagged for correction and used as added part of the prompt in the following step (see examples in Fig.\ref{fig:Errors_examples}).


\subsection{Iterative Refinement via LLM Self-Correction}
If a function fails any validation step, it undergoes an iterative refinement process. This involves re-prompting the LLM with structured feedback to improve the function. Each re-prompt includes a slightly modified task instruction, the previously generated code, and a detailed error description to guide correction. The refinement process consists of three key stages:

\textbf{Syntax Correction:} Syntax errors are provided to the LLM with explicit error messages detailing the cause of failure. The model is then prompted to regenerate the function while ensuring that the identified syntax issue is corrected.
\newline
\textbf{Execution Error Correction:} For runtime errors, the corresponding error message is provided as feedback to the LLM (see Figure \ref{fig:runtime_errors} for examples such as an incorrect number of arguments or undefined variables). The model then attempts to revise the function to ensure successful execution.
\newline
\textbf{Logical Correction:} For incorrect outputs during logical verification, the LLM is tasked with refining the conditional logic. It receives feedback on the specific functions that are part of the overall logic, allowing it to correct these functions accordingly.

After each refinement step, the updated function is re-evaluated using the same validation process (see Fig. \ref{fig:python_rules_gen_diagram}). 

\begin{figure}
\centering
  \includegraphics[width=.9\linewidth]{latex/figures/Errors_examples.pdf} 
  \caption {Examples of syntax, runtime and logic errors that are used in the self-correction. }
  \label{fig:Errors_examples}
\end{figure}

\subsection{Metrics}
The framework uses two metrics: %Formalization Accuracy (FA), Correctness Accuracy (CA), and Overall Accuracy (OA).

\textbf{Formalization Success Rate (FSR)} measures the proportion of natural language inputs correctly translated into syntax error-free Python rules, assessing the LLM's ability to generate valid code.
\begin{equation}
    \label{eq:fa}
    FSR = \frac{X_f}{X}
\end{equation}

% 2. \textbf{Correctness Accuracy (CA)} measures the ratio of inputs that are not only correctly translated into Python syntax but also correctly implement rules for determining whether a data point is an outlier.
% \begin{equation}
%     \label{eq:ca}
%     CA = \frac{X_c}{X_f}
% \end{equation}

\textbf{Logical Consistency Rate (LCR)} measures the proportion of natural language inputs accurately translated into Python rules that not only execute without errors but also correctly determine whether a data point is an outlier, ensuring logical validity.
\begin{equation}
    \label{eq:oa}
    LCR = \frac{X_c}{X}
\end{equation}

where:  \(X\) is the total set of natural language inputs (prompts) containing rules; \(X_f\) is the subset of \(X\) that is successfully formalized into valid, syntax error-free Python rules; \(X_c\) is the subset of \(X_f\) where the generated rules correctly identify out-of-range values.  


\subsection{Tasks}
\textbf{Task 1: Range Checking} The initial task aims to verify whether a single parameter's value lies within a predefined range, e.g., `A vehicle operates for between 2 and 10 hours per day'. %This serves as a foundational experiment to assess the ability of LLMs to generate functions that encode such constraints. 
Additionally, a single prompt contains multiple premises, each defining different parameter ranges. As the number of premises increases, so does the complexity of the task, requiring the model to correctly handle multiple constraints simultaneously. The objective is to evaluate the model’s ability to process and encode multiple premises within a single prompt, rather than just a single constraint.  

Formally, given multiple parameters $x$ with their respective valid ranges \([x_{\min}, x_{\max}]\), the generated function should determine whether all parameters satisfy their constraints:
\small
\[
F(x_1, x_2, \dots, x_n) =
\begin{cases} 
1, & \text{if } \forall i \in \{1, \dots, n\}, \\ & \quad x_i \in [x_{i,\min}, x_{i,\max}] \\
0, & \text{otherwise}
\end{cases}
\]
\normalsize

\textbf{Task 2: Constraint Validation} 
The second task extends the complexity by introducing interdependencies between parameters. Instead of evaluating each parameter independently, the constraints now define relational conditions between them. For example, in a vehicle load distribution scenario, a constraint may state: `The load on the first axle cannot be greater than the load on the second axle.'
Formally, let \( R \) be a set of relational constraints defined over the parameters:
\small
\[
R = \{ (x_i, x_j) \mid x_i \ \mathcal{O}_{ij} \ x_j, \quad \forall (i, j) \}
\]
\normalsize

where \( \mathcal{O}_{ij} \) represents a relational operator such as \( \leq, \geq \). The function \( F(x_1, x_2, \dots, x_n) \) should evaluate whether all given constraints hold simultaneously:
\small
\[
F(x_1, x_2, \dots, x_n) =
\begin{cases} 
1, & \text{if } \forall (x_i, x_j) \in R, \quad x_i \ \mathcal{O}_{ij} \ x_j \\
0, & \text{otherwise}
\end{cases}
\]
\normalsize


\subsection{Premises Dataset}
The dataset consists of statements defining the normal operating ranges of various parameters based on industry standards. To enhance linguistic diversity, each statement is rephrased into five distinct textual variations while preserving logical consistency. This approach increases robustness and ensures variability in how constraints are expressed. Each premise is crafted to resemble expert descriptions, providing realistic inputs for LLMs.

A subset of the dataset is shown in Tables \ref{tab:task1_premises} and \ref{tab:task2_premises}. The dataset is easily extendable, with the current version covering: 17 parameters corresponding to real-world vehicle operational variables such as speed, distance, fuel consumption, and axle loads; 130 engineering premises, 950 prompts, and corresponding validation points.

\subsection{Prompt Construction}
\label{prompt_construction}
Each prompt follows a structured format to guide the LLM in code generation and iterative refinement (Fig. \ref{fig:prompt_structure}):\newline
\textbf{Task Instruction:} A system message that clearly defines the model’s role and objective in translating natural language constraints into executable logic. \newline
\textbf{In-Context Examples:} Demonstrations of correctly formatted Python functions, with parameter names and units replaced by placeholders. This ensures the model understands the expected output format while preventing memorization of specific values. The number of examples corresponds to the number of premises in the task. \newline
\textbf{Input Premises:} A set of natural language statements defining constraints on the operational parameters of an object. These premises serve as the basis for generating Python functions and are derived from \textit{Premises Dataset}. \newline
\textbf{Error Messages (During Self-Correction):} If the generated function fails syntax, runtime, or logic validation, error messages (Fig.\ref{fig:Errors_examples}) are appended to the prompt. These messages provide explicit feedback on the failure, enabling the model to iteratively refine and correct its output.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{latex/figures/Prompt_structure.pdf} 
  \caption {Examples of prompts in Task 1. Range Checking (left) and Task 2. Constraint Validation (right).}
  \label{fig:prompt_structure}
\end{figure*}

As illustrated in Figure \ref{fig:python_rules_gen_diagram} and \ref{fig:prompt_structure}, the prompt is dynamically augmented during refinement steps, allowing the model to improve incorrect functions based on structured feedback. This iterative approach ensures better alignment between the generated code and the intended logical constraints.


\subsection{Evaluation Data Points}
As part of the ExKLoP framework, we generate a structured evaluation dataset to systematically assess whether LLM-generated functions correctly classify in-range and out-of-range values. This dataset is tailored to each task, ensuring comprehensive validation of the model’s ability to enforce predefined constraints.

\textbf{Range Checking Task:} 
For each parameter \(x_i\) (\(i=1,\dots,n\)), we generate four test points: $\{x_{i1}, x_{i2}, x_{i3}, x_{i4}\}$,with the following properties:
\[
\begin{aligned}
x_{i1} &< x_{i,\min}, \\
x_{i2},\, x_{i3} &\in [x_{i,\min}, x_{i,\max}], \\
x_{i4} &> x_{i,\max}.
\end{aligned}
\]

To minimize the total number of evaluations—which would otherwise require testing all \(n!\) combinations—we adopt the following strategy that requires only \(4n\) evaluations: for a given parameter \(x_i\), we evaluate \(F\) by varying \(x_i\) over its four test points while keeping all other parameters fixed at a nominal value \(x_{j0}\) (with \(x_{j0} \in [x_{j,\min}, x_{j,\max}]\) for \(j\neq i\)). In this way, the vector of evaluation points for parameter \(x_i\) is constructed as:
\[
\mathbf{x}^{(i,\ell)} = \bigl( x_{10}, \dots, x_{i-1,0}, x_{i\ell}, x_{i+1,0}, \dots, x_{n0} \bigr)
\]
where $\ell=1,2,3,4$. By design, for each test vector \(\mathbf{x}^{(i,\ell)}\) the function evaluation simplifies to:
\small
\[
F\bigl(\mathbf{x}^{(i,\ell)}\bigr) =
\begin{cases}
1, & \text{if } x_{i\ell} \in [x_{i,\min}, x_{i,\max}], \\
0, & \text{if } x_{i\ell} \notin [x_{i,\min}, x_{i,\max}],
\end{cases}
\]
\normalsize
since \(F(x_{j0})=1\) for all \(j\neq i\).

%This approach efficiently verifies the behavior of \(F\) with respect to the individual parameter bounds.








%  shows that, each parameter is tested with four validation points:
% \begin{itemize}
%   \item Two points fall within the defined range.
%   \item Two points fall outside the range (one below the minimum, one above the maximum).
%   \item These validation points ensure that functions correctly implement both upper and lower thresholds.
% \end{itemize}

\textbf{Constraint Validation Task}: 
For each relational constraint \(x_i \,\mathcal{O}_{ij}\, x_j\) (with \(\mathcal{O}_{ij}\) a relational operator such as \(\geq\) or \(\leq\)), we generate three test points for the parameter \(x_i\): $\{x_{i1},\, x_{i2},\, x_{i3}\}$ with the following properties:
$x_{i1},\, x_{i2}$ satisfy $x_i \,\mathcal{O}_{ij}\, x_{j,0}$ and $x_{i3}$ violates $x_i \,\mathcal{O}_{ij}\, x_{j,0}$,
where \(x_{j,0}\) is a nominal valid value for \(x_j\) (i.e., one that complies with its range and any applicable constraints).

For a given constraint \(x_i \,\mathcal{O}_{ij}\, x_j\), we evaluate \(F\) by varying \(x_i\) over its three test points while keeping all other parameters fixed at nominal values \(x_{k,0}\) (with \(x_{k,0}\) chosen so that \(F\) evaluates to 1 when no constraint is violated). In this way, the evaluation vector is constructed as:
\[
\mathbf{x}^{(i,\ell)} = \bigl( x_{10}, \dots, x_{i-1,0}, x_{i\ell}, x_{i+1,0}, \dots, x_{n0} \bigr)
\]
where $\ell=1,2,3$. 
By design, for each test vector \(\mathbf{x}^{(i,\ell)}\) the function evaluation is:
\small
\[
F\bigl(\mathbf{x}^{(i,\ell)}\bigr) =
\begin{cases}
1, & \text{if } x_{i\ell} \,\mathcal{O}_{ij}\, x_{j,0} \quad (\ell=1,2), \\
0, & \text{if } x_{i\ell} \notin \{\,x \mid x \,\mathcal{O}_{ij}\, x_{j,0}\,\} \quad (\ell=3),
\end{cases}
\]
\normalsize
since the nominal values for the other parameters ensure that all other constraints are satisfied.

Assuming each constraint is associated with a distinct parameter being varied, the total number of evaluation scenarios for \(F(x_1, x_2, \dots, x_n)\) is \(3n\).

\section{Experiments}
To evaluate the effectiveness of the proposed ExKLoP, we conducted experiments on two tasks: Range Checking and Constraint Validation.

For the Range Checking task, each prompt comprises between 2 and 12 premises. We generated 50 prompts for each set size, resulting in a total of 550 unique prompts. Varying the number of premises per prompt allows us to assess whether performance degrades as the input complexity increases. Each prompt is assigned a unique, randomly selected set of premises to ensure diversity in parameter selection.

For the Constraint Validation task, each prompt contains between 2 and 9 conditions. With 50 prompts generated for each set size, this task includes a total of 400 unique prompts. Each prompt randomly selects a distinct set of conditions, ensuring a broad range of logical interdependencies.

In total, we evaluated 5,700 prompts, as each of the 6 LLMs was tested on 550 Range Checking prompts and 400 Constraint Validation prompts.



\subsection{Experimental setting}

To ensure a comprehensive evaluation, we tested six open-sourced LLMs: \textbf{Llama3-70B}, \textbf{Llama3-8B}\citep{grattafiori2024llama3herdmodels}, \textbf{Gemma-7B}\citep{gemmateam2024gemmaopenmodelsbased}, \textbf{Mistral-7B}\citep{jiang2023mistral7b}, \textbf{Mixtral-8x7B}\citep{jiang2024mixtralexperts}, and \textbf{Qwen2-7B}\citep{yang2024qwen2technicalreport}. All models are used in their Instruct versions, optimized for task-following. For consistency and reproducibility, inference is conducted with zero temperature and no sampling, minimizing randomness in responses. These settings ensure that performance differences arise from model capabilities rather than stochastic variation.

All experiments were conducted using 4 NVIDIA L40 GPUs, each with 48GB of VRAM, and an AMD EPYC 75F3 32-Core Processor. The downloaded models required 360GB of free disk space. All scripts were executed with Python 3.10. The pretrained LLM weights were utilized through Hugging Face’s \textit{transformers} library (version 4.33.1) and \textit{accelerate} (version 0.33.0), with \textit{CUDA} 12.1. The models were loaded using the \textit{AutoModelForCausalLM} and \textit{AutoTokenizer} classes with the options \textit{device\_map="auto"} and \textit{torch\_dtype=torch.bfloat16}, enabling multi-GPU inference and reduced GPU memory usage. Inference was performed with the \textit{temperature} set to \textit{0} and \textit{sampling=False}.

% \subsection{Evaluated tasks}
% In this work we focus on evaluation of our methodology for two types of tasks:

% {\large \textbf{Task 1.} \textit{range checking}.} This experiment is designed to evaluate the potential of proposed method and act as a starting point for further research. The aim of range checking is to evaluate the models performance for generating Python functions for premises including one parameter. This is an easier task and serves as starting point for further evaluation. In this task we include 2 to 12 premises in a prompt, generating 50 prompts for each number of premises, which gives us 11 groups of prompts and 550 inputs. For each number of premises \textbf{P} in a prompt we randomly select 50 sets of parameters, so that each prompt within group contains different set of parameters.

% {\large \textbf{Task 2.} \textit{constraint validation}.} This task is more complicated, it involves including 2 variables inside a premise and creating a condition sentence by making their values dependent on each other. For each task we generate a separate set of premises and separate dataset of prompts. It shares the same set of parameters with range checking which is defined in the Appendix \ref{sec:appendix_1}, but have different set of validation data points. In this task we evaluate the models performance for more complex problem. LLMs are told to generate Python functions for conditions including two parameters, dependent on each other. This is more sophisticated problem comparing to range checking. In this task we include 2 to 9 conditions in a prompt, generating 50 prompts for each number of conditions, which gives us 8 groups of prompts and 400 inputs. For each number of conditions \textbf{C} in a prompt we randomly select 50 sets of conditions, so that each prompt within group contains different set of conditions, which means different set of parameters.

% \subsection{Premises dataset}
% To create the premises dataset, we began by defining a set of 17 parameters that correspond to real operational physical variables associated with the exploitation of vehicles. For each parameter, we established an expected, normal range of values. Next, we prepared five different versions of a free-text statement that describe the normal range of each parameter in a manner consistent with how an expert might articulate it. Each of these five versions for a given parameter varies syntactically but conveys the same logical meaning. This approach ensures diversity in expression while maintaining consistency in the logical content across the dataset. The full premises dataset is available at Github, and examples are shown in Table \ref{tab:premises}.

% \subsection{Prompt dataset}
% From the diagram in Figure \ref{fig:prompt_structure} we can see that our prompts consist of 3 main components - 1) instruction for model, for each task we have four different instructions, one for each inference step, pictured in Fig. \ref{fig:python_rules_gen_diagram}, 2) anonymized In-context examples, presenting to the model the task completion and desired output format and 3) given input. During the refinement steps, prompt also includes 4) element which is an error message or a list of incorrectly implemented functions. The diagram from Figure \ref{fig:corr_step_prompt_structure} show the details. For In-context examples we use an 'adaptive' approach that adjusts the number of statements in the example to match exactly the number of statements provided in the task. The rationale behind the 'adaptive' type is to evaluate how aligning the instructions and examples more closely with the specific task impacts the performance of large language models.




% \begin{figure*}
%   \includegraphics[width=\textwidth]{latex/figures/Prompt_structure_correction_tasks.pdf}
%   \caption {Visualization of prompt structure for correction steps.}
%   \label{fig:corr_step_prompt_structure}
% \end{figure*}

% \subsection{Evaluation data points}\label{r:data_points}
% Evaluation data points are used to validate the correctness of logic implemented in Python functions. The number of points generated depends on the task being evaluated. In range checking task, each parameter is constrained by 2 values, defining its minimum and maximum value. For each parameter, 4 points are generated, two of which are used to evaluate the function's response to detecting values below and above the lower range, and another two for the same evaluation for the upper range. The details are pictured in Figure \ref{fig:points_task1}. With this approach, we can be sure of the correct validation of the implementation of the logic in this task.

% In constraint validation task, we have conditions described by 2 parameters, dependent on each other. This dependence is expressed by one of two mathematical conditions: >= or <=. This means that we need 3 data points to correctly evaluate the realization of the condition. Two of them will evaluate the fulfillment of the relationship, which means that the parameter values are correctly unequal or equal. The third data point will verify that the inverse inequality is correctly detected. This ensures the correct validation of the implementation of the logic in Task 2.



% \begin{figure*}
%   \includegraphics[width=\textwidth]{latex/figures/Task2_points_generation.pdf}
%   \caption {Visualization of the process of generation validation data points in Task 2.}
%   \label{fig:points_task2}
% \end{figure*}



% \subsection{Evaluated models}
% We used the following LLMs: \textbf{Llama3-70B}, \textbf{Llama3-8B}, \textbf{Gemma-7B}, \textbf{Mistral-7B}, \textbf{Mixtral-8x7B} and \textbf{Qwen2-7B}. For all models we used their \textit{Instruct} versions for better following of the prompted instructions. We inference all models with zero temperature and without sampling to ensure repeatability of results and minimize results that deviate from the In-context examples.

\section{Results and discussion}
Our evaluation of LLM-generated Python functions for range checking and constraint validation tasks revealed several key findings.

\textbf{High Fluency in Syntactically Correct Code}:
Most models achieved near-perfect Formalization Success Rates (FSR between 0.97 and 1.0), demonstrating a strong capability in producing syntactically correct Python code. Although generating code that passes syntax checks is relatively straightforward, ensuring logical correctness is considerably more challenging. This highlights the need for robust evaluation frameworks to systematically assess not only syntax but also the underlying logic.

\textbf{Translation of Expert Knowledge into Code}:
We observed notable issues in translating expert knowledge into code. Runtime errors, such as mismatched function arguments and undefined variables, were common in constraint validation tasks. Even when the generated code executed without errors, logical discrepancies were evident. For example, a frequent mistake involved reversing the order of arguments (e.g., generating X2 <= X1 instead of X1 >= X2). In models like Mixtral, inconsistent indexing further compounded these issues, leading to the loss of inter-argument dependencies. Other errors included improper argument definitions or omitting key physical parameters of an object.

\textbf{Task 1: Logical Correctness in Range Checking}:
In the range checking task, the Llama3 models (70b and 8b) excelled with Logical Consistency Rates (LCR) of 0.99 and 0.96, respectively. Qwen also showed significant improvement through self-correction, with its LCR rising from 0.37 to 0.78. In contrast, Gemma and Mistral scored 0.64 and 0.61 respectively, with only modest improvements (0.13 and 0.08) after three self-correction iterations. While Mistral and Gemma’s performance declined as the task complexity increased (with more premises to translate), Mixtral and Qwen improved with complexity, and the Llama3 models maintained high LCR regardless of the number of relations.

\textbf{Task 2: Logical Correctness in Constraint Validation}:
For constraint validation, the largest model, Llama3-70b, led with an LCR of 0.96, followed by Llama3-8 at 0.83—showing only a marginal improvement of 0.01 after self-correction. Gemma achieved a mediocre LCR of 0.59 with a similar minimal gain, while the remaining models scored below 0.30 with negligible improvements. Most models, except for the Llama3 variants, struggled as task complexity increased; notably, Llama3-8 performed better with the highest number of relations, and Gemma only matched its performance when four premises were provided.

\textbf{Impact of Self-Correction and Model Comparison}:
Self-correction had a limited impact on overall performance, likely because the initial syntax was already correct and the experimental setting (with a temperature set to 0) constrained further refinement. The results suggest that iterative self-correction, as currently implemented, offers only marginal benefits. An alternative approach might be to employ one model to correct another. Additionally, while model size did not decisively affect performance—with both Llama3-70b and Llama3-8 outperforming others—Llama3-8 presents a more cost-effective option by requiring significantly less GPU resources.

Key numerical results are summarized in Table \ref{tab:tasks_results}, while Figure \ref{fig:OA_before_after_steps_comp} shows changes in LCR in successive stages of iterative refinement for both tasks. Figures \ref{fig:task1_OA_before_after_comp}, \ref{fig:task2_OA_before_after_comp} shows the same changes but with detailed results for each parameter. 
Figures \ref{fig:FA_changes} and \ref{fig:OA_changes} show in detail the changes in all accuracy metrics in successive stages of refinement.

% To assess the effectiveness of the proposed method, we performed an evaluation of the models on the datasets for both tasks, along with all the steps for improving their answers. The main results are presented in Table \ref{tab:task1_res} and Table \ref{tab:task2_res}. Figure \ref{fig:OA_before_after_comp} shows the change in the value of the Overall Accuracy metric as a result of the corrections made, for all models in both tasks. Figures \ref{fig:FA_changes}, \ref{fig:CA_changes} and \ref{fig:OA_changes} show detailed results for all models in both tasks in all correction steps.

% The main insights from the results of the experiments are as follows:
% \begin{itemize}
%   \item The large language models evaluated mostly generate Python code with correct syntax.
%   \item Validation of correct syntax does not guarantee the absence of errors during code execution.
%   \item Most of the LLMs tested can achieve proper results in Task 1, but have difficulty with Task 2.
%   \item In Task 2 most of the models generate code execution errors, usually caused by implementing too few parameters that are used during evaluation.
%   \item Llama-70 has the best performance in both tasks.
%   \item In Task 1, Qwen achieved the greatest improvement in Overall Accuracy values, more than 40 percentage points, despite a poor initial result.
%   \item In Task 2, Mistral, Mixtral and Llama-8 achieved the greatest improvement in Overall Accuracy values.
%   \item Mixtral initially has very poor results in Task1 and Task2.
%   \item The suggested set of code improvement steps allowed, for most models, except Llama-70, to achieve higher scores in both Task 1 and Task 2.
% \end{itemize}

\begin{table*}[h]
  \centering
  \resizebox{0.8\textwidth}{!}{
  \small
  \begin{tabular}{ccccccccccc}
    \hline
    \multicolumn{11}{c}{Evaluation steps} \\
    \multicolumn{1}{c}{Model} & \multicolumn{2}{c}{First iteration} & \multicolumn{2}{c}{Syntax correction} & \multicolumn{2}{c}{Code correction} & \multicolumn{2}{c}{Logic correction} & \multicolumn{2}{c}{Improvement} \\
      & FSR & LCR & FSR & LCR & FSR & LCR & FSR & LCR & $\Delta$ FSR & $\Delta$ LCR \\
    \hline
    Llama-70 & \textbf{1.0} & \textbf{0.99} & \textbf{1.0} & \textbf{0.99} & \textbf{1.0} & \textbf{0.99} & \textbf{1.0} & \textbf{0.99} & 0.00 & 0.00 \\
    Llama-8 & 1.0 & 0.95 & 1.0 & 0.95 & 1.0 & 0.95 & 1.0 & 0.96 & 0.00 & 0.01 \\
    Gemma & 1.0 & 0.51 & 1.0 & 0.51 & 1.0 & 0.58 & 1.0 & 0.64 & 0.00 & 0.13 \\
    Mistral & 0.99 & 0.53 & 0.99 & 0.53 & 0.99 & 0.57 & 0.99 & 0.61 & 0.00 & 0.08 \\
    Mixtral & 0.97 & 0.19 & 0.97 & 0.19 & 0.97 & 0.2 & 0.97 & 0.2 & 0.00 & 0.01 \\
    Qwen & 1.0 & 0.37 & 1.0 & 0.37 & 1.0 & 0.77 & 1.0 & 0.78 & 0.00 & \textbf{0.41} \\    
    \hline
  \end{tabular}
  }
  %\caption{Formalization Success Rates (FSR) and Logical Consistency Rate (LCR) in Range Checking task. $\Delta$ corresponds to the change between the \textit{first iteration} and after \textit{logic correction}.}
  \label{tab:task1_res}
\end{table*}

\begin{table*}[h]
  \centering
  \resizebox{0.8\textwidth}{!}{
  \small
  \begin{tabular}{ccccccccccc}
    \hline
    \multicolumn{11}{c}{Evaluation steps} \\
    \multicolumn{1}{c}{Model} & \multicolumn{2}{c}{First iteration} & \multicolumn{2}{c}{Syntax correction} & \multicolumn{2}{c}{Code correction} & \multicolumn{2}{c}{Logic correction} & \multicolumn{2}{c}{Improvement} \\
      & FSR & LCR & FSR & LCR & FSR & LCR & FSR & LCR & $\Delta$ FSR & $\Delta$ LCR \\
    \hline
    Llama-70 & \textbf{1.0} & \textbf{0.96} & \textbf{1.0} & \textbf{0.96} & \textbf{1.0} & \textbf{0.96} & \textbf{1.0} & \textbf{0.96} & 0.00 & 0.00 \\
    Llama-8 & 0.99 & 0.82 & 0.99 & 0.82 & 0.99 & 0.83 & 0.99 & 0.83 & 0.00 & 0.01 \\
    Gemma & 1.0 & 0.58 & 1.0 & 0.58 & 1.0 & 0.58 & 1.0 & 0.59 & 0.00 & 0.01 \\
    Mistral & 1.0 & 0.06 & 1.0 & 0.06 & 1.0 & 0.09 & 1.0 & 0.09 & 0.00 & \textbf{0.03} \\
    Mixtral & 1.0 & 0.14 & 1.0 & 0.14 & 1.0 & 0.16 & 1.0 & 0.16 & 0.00 & 0.02 \\
    Qwen & 0.99 & 0.29 & 0.99 & 0.29 & 0.99 & 0.29 & 0.99 & 0.29 & 0.00 & 0.00 \\    
    \hline
  \end{tabular}
  }
  \caption{Formalization Success Rates (FSR) and Logical Consistency Rate (LCR) in Range Checking (top) and Constraint Validation (bottom) tasks. $\Delta$ corresponds to the change between the \textit{first iteration} and after \textit{logic correction}.}
  \label{tab:tasks_results}
\end{table*}

\begin{figure}
    \centering
  \includegraphics[width=0.5\textwidth]{latex/figures/oa_metric_changes_through_steps.pdf} 
  \caption {Logical Consistency Rate (LCR) at each step of the evaluation.}
  \label{fig:OA_before_after_steps_comp}
\end{figure}

\begin{figure}[t]
    \centering

  \includegraphics[width=0.5\textwidth]{latex/figures/task1_OA_before_after_comp.pdf}
  \caption {Logical Consistency Rate relation to task complexity, before (left) and after (right) all self-corrections.}
  \label{fig:task1_OA_before_after_comp}
\end{figure}

\begin{figure}[t]
    \centering

  \includegraphics[width=0.5\textwidth]{latex/figures/task2_OA_before_after_comp.pdf}
  \caption {Logical Consistency Rate relation to task complexity, before (left) and after (right) all self-corrections.}
  \label{fig:task2_OA_before_after_comp}
\end{figure}

\section{Related work}
Our work intersects several research streams including neurosymbolic reasoning, code generation from natural language, and iterative self‐correction. Several recent studies have explored neurosymbolic approaches that translate natural language problems into formal logical representations. For instance, LeanReasoner \cite{jiang2024leanreasoner} and Logic-LM \cite{pan2023logic} formalize reasoning tasks as theorems to be proven by external solvers, while LINC \cite{olausson2023linc} and Symbolic Chain-of-Thought \cite{xu2024faithful} convert natural language inputs into first-order logic for symbolic deduction. Similarly, works such as Logic-of-Thought \cite{liu2024logic} and Logic Agent \cite{liu2024logic} inject explicit logical rules into the reasoning process, enhancing the coherence and validity of generated outputs.

Parallel to these efforts, iterative refinement strategies have emerged to address error accumulation in LLM outputs. For example, LLM-ARC \cite{kalyanpur2024llm} and Determlr \cite{sun2024determlr} leverage automated feedback to guide the self-correction of reasoning chains, while SIP-BART \cite{enstrom2024reasoning} and ChatLogic \cite{wang2024chatlogic} integrate symbolic checkers or logic programming into the inference process. In the realm of code generation, methods such as AutoSpec \cite{wen2024enchanting} and CoCoGen \cite{bi2024iterative} employ compiler feedback and static analysis to iteratively refine generated code. Yet, despite these promising approaches, challenges remain in reliably translating domain-specific expert knowledge into executable code.

Our framework, ExKLoP, specifically addresses the underexplored challenge of converting expert engineering guidelines—expressed as natural language premises—into Python logic rules that can be executed and validated. While previous studies have focused on symbolic reasoning \cite{jiang2024leanreasoner, pan2023logic, olausson2023linc, xu2024faithful, liu2024logic, wysocka2024syllobio, dalal-etal-2024-inference,quan-etal-2024-verification,quan-etal-2024-enhancing, meadows-etal-2024-symbolic, quan-etal-2024-verification,meadows2025controllingequationalreasoninglarge} or on general code generation benchmarks \cite{wang2023review, yadav2024pythonsaga}, our work uniquely investigates whether LLMs can integrate and preserve the nuanced constraints of domain-specific recommendations, as encountered in industrial and engineering settings.

Additionally, research on integrating expert intuition into machine learning pipelines—such as the work by \cite{bogatu2023meta}, \cite{jing2024translating} and \cite{boicu2001automatic}—has largely focused on feature extraction for predictive analytics. In contrast, ExKLoP evaluates the transformation of expert knowledge into operational logic, ensuring that the resulting code not only meets syntactic standards but also adheres to strict logical constraints. This is crucial when the generated code is intended to monitor and control critical systems, as even minor logical errors can have significant real-world consequences.

Our investigation also builds upon prior research in iterative self-correction and refinement. While frameworks like LLM-ARC \cite{kalyanpur2024llm} and Determlr \cite{sun2024determlr} demonstrate that feedback loops can enhance reasoning accuracy, our experiments reveal that—despite near-perfect syntactic outputs from many LLMs—logical inconsistencies persist in the translation of expert rules. This observation aligns with findings in studies on code verification and reasoning \cite{wu2023lemur, feng2023language, feng2024language, chen2022program}, suggesting that additional strategies, such as cross-model correction \cite{li2024leveraging} or more robust feedback mechanisms \cite{yang2024enhancing}, may be necessary.

Beyond these core areas, our work resonates with broader efforts in the LLM community to enhance code quality and logical reasoning. Reviews on code generation \cite{wang2023review} and advancements in efficient syntactic decoding \cite{ugare2024improving} provide context for our challenges, while studies on program synthesis and refactoring \cite{zhang2024refactoring, lyu2024automatic} underscore the ongoing need for improved integration of formal methods into LLM outputs. Moreover, research that bridges natural language and formal reasoning—such as CoRE \cite{xu2024core} and work on automated specification synthesis \cite{wen2024enchanting}—further highlights the diversity of approaches aimed at enhancing the interpretability and reliability of LLM-generated code.

Finally, our work complements recent advances in large-scale LLMs and their applications in diverse domains \cite{clark2020transformers, wang2019logic, yang2024if, nejjar2025llms, chu2024think, wysocka2024large, wysocki2024llm, delmas-etal-2024-relation, wysocki2023transformers}, and aligns with emerging trends in model scaling and multimodal integration \cite{grattafiori2024llama3herdmodels, jiang2024mixtralexperts, jiang2023mistral7b, yang2024qwen2technicalreport, gemmateam2024gemmaopenmodelsbased}. By focusing on the specific challenge of encoding domain-specific expert knowledge into executable logic and systematically assessing self-correction, ExKLoP offers a valuable benchmark and a novel perspective that bridges the gap between expert systems and automated reasoning.

In summary, while prior work has made significant strides in symbolic reasoning, iterative self-correction, and expert knowledge integration, our framework uniquely contributes to the field by targeting the reliable conversion of expert domain guidelines into validated Python logic, thereby laying the groundwork for more robust, knowledge-informed AI systems.





% Integrating Large Language Models (LLMs) with expert knowledge expressed in natural language provides a promising method for evaluating parameter values across domains \citep{tan2024struct}, \citep{sun2024determlr}, \citep{pan2023logic}, \citep{nejjar2025llms}, \citep{selby2024quantitative}. LLMs process complex, domain-specific information, converting expert insights into quantifiable features for improved predictive analytics. A study showed how LLMs convert investigator insights into structured features, enhancing risk assessment and decision-making \citep{wen2024enchanting}. However, challenges remain in ensuring the accuracy and reliability of LLM outputs, particularly in specialized fields like biomedicine. A proposed framework streamlines expert evaluation of LLM-generated content to maintain scientific factuality \citep{wysocka2024large}.

% Some studies also explore LLMs as evaluators of their own outputs \citep{chu2024think}. \citep{liu2024logic} found that LLM evaluations aligned with human experts in specific tasks, suggesting LLMs could complement human evaluation. However, LLMs may not fully capture the nuanced judgments of experts, especially in complex tasks. Therefore, while LLMs are useful for evaluating parameter values from expert knowledge, addressing factual accuracy and depth of understanding is crucial. Combining LLMs with human expertise can lead to more reliable evaluations across various domains \citep{chu2024think}.

% Recent advancements combine LLMs with symbolic solvers and programming languages to improve value range checking and constraint validation from natural language premises \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{quan2024verification}, \citep{enstrom2024reasoning}, \citep{soroco2025pde}, \citep{chen2022program}, \citep{chen2022large}, \citep{wang2023meta}. This integration merges LLM flexibility with formal methods' rigor, enhancing logical reasoning and program verification. Logic-LM \citep{pan2023logic} translates natural language into symbolic logic, refining errors through solver feedback for better logical accuracy. The three-stage pipeline processes logical premises, ensuring consistency and correctness. LINC \citep{olausson2023linc} uses LLMs as semantic parsers to convert premises into first-order logic, outperforming traditional prompting in verifying consistency in datasets like FOLIO and ProofWriter. Lemur \citep{wu2023lemur} combines LLMs with automated reasoners for program verification, improving performance on benchmarks. LoGiPT \citep{feng2023language}, \citep{feng2024language} emulates logical solvers with strict syntax adherence, enhancing logical reasoning. AutoSpec \citep{wen2024enchanting} generates and validates program specifications with LLMs, ensuring alignment with program behavior across multiple benchmarks.

% These studies show that integrating LLMs with symbolic solvers enhances logical consistency, inference reliability, and program verification precision, offering scalable solutions for automating constraint validation and value range checking.


% The advent of Large Language Models (LLMs) has significantly transformed the landscape of code generation, enabling the translation of natural language descriptions into executable code \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{wang2023review}, \citep{li2024leveraging}, \citep{xu2024core}, \citep{lyu2024automatic}, \citep{yang2024if}, \citep{nejjar2025llms}, \citep{ugare2024improving}. These models have demonstrated remarkable proficiency in understanding and generating programming constructs, thereby facilitating tasks such as range checking and constraint validation. Range checking involves verifying whether a given value falls within a specified parameter range, while constraint validation assesses if the values of multiple parameters satisfy predefined conditions. Recent advancements have also explored the integration of LLMs with symbolic solvers and programming languages to enhance the precision of value range checking and constraint validation derived from natural language premises \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{xu2024faithful}, \citep{wang2023meta}, \citep{xu2023symbol}, \citep{chen2022program}. This fusion combines the adaptability of LLMs with the rigor of formal methods, thereby improving logical reasoning and program verification \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{quan2024verification}, \citep{xu2024faithful}, \citep{wang2023meta}, \citep{xu2023symbol}, \citep{chen2022program}. For instance, Logic-LM \citep{pan2023logic} translates natural language into symbolic logic, refining errors through solver feedback to achieve better logical accuracy. Similarly, LINC \citep{olausson2023linc} employs LLMs as semantic parsers to convert premises into first-order logic, outperforming traditional prompting methods in verifying consistency across various datasets. These studies underscore the potential of integrating LLMs with symbolic solvers to enhance logical consistency, inference reliability, and program verification precision, offering scalable solutions for automating constraint validation and value range checking \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{quan2024verification}, \citep{xu2024faithful}, \citep{wang2023meta}, \citep{xu2023symbol}, \citep{chen2022program}.

% Moreover, combining LLMs with expert knowledge expressed in natural language presents a promising approach for evaluating parameter values across diverse domains \citep{selby2024quantitative}, \citep{han2024large}, \citep{jing2024translating}. LLMs can process complex, domain-specific information, converting expert insights into quantifiable features for improved predictive analytics \citep{boicu2001automatic}, \citep{nejjar2025llms}. For example, \citep{jing2024translating} demonstrated how LLMs could transform investigator insights into structured features, thereby enhancing risk assessment and decision-making. However, challenges persist in ensuring the accuracy and reliability of LLM outputs, particularly in specialized fields like biomedicine. To address this, framework have been proposed to streamline expert evaluation of LLM-generated content, maintaining scientific factuality \citep{wysocka2024large}.

% In addition to leveraging external expert knowledge, some studies have explored the potential of LLMs to evaluate their own outputs \citep{szymanski2024limitations}. \citep{szymanski2024limitations} found that LLM evaluations aligned with human experts in specific tasks, suggesting that LLMs could complement human evaluation \citep{selby2024quantitative}, \citep{mcinerney2023chill}. Nevertheless, LLMs may not fully capture the nuanced judgments of experts, especially in complex tasks. Therefore, while LLMs are useful for evaluating parameter values from expert knowledge, addressing factual accuracy and depth of understanding remains crucial \citep{boicu2001automatic}. Combining LLMs with human expertise can lead to more reliable evaluations across various domains.

%While previous studies have explored LLMs for logic formalization, there remains a gap in automating and iteratively refining their outputs with minimal human intervention. This research proposes a structured methodology to improve LLM-generated logic through a systematic refinement process, enhancing reliability in real-world applications.


\section{Conclusions}

We introduced \href{https://github.com/Franek18/ExKLoP}{ExKLoP}\footnote{\url{https://github.com/Franek18/ExKLoP}}, a framework designed to evaluate LLMs’ ability to integrate expert knowledge into logical reasoning and self-correction. Leveraging an extensible dataset of engineering premises and validation points, our approach systematically assesses both the syntactic fluency and logical correctness of Python code for critical tasks like range checking and constraint validation.

Our experiments reveal that although most models generate nearly perfect syntactically correct code, they frequently struggle to accurately translate expert knowledge, leading to logical errors. Notably, Llama3 emerged as the best-performing model, with the 7B variant showing simialar performance compared to the 70B version ($LCR_{Task1}=$.99 vs .96; $LCR_{Task2}=$.96 vs .83). Additionally, iterative self-correction produced only marginal improvements (up to 3\%), suggesting that alternative strategies—such as cross-model correction—may be necessary.

Overall, ExKLoP offers a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating error types. This framework establishes a valuable benchmark and lays the groundwork for future research aimed at enhancing logical consistency and the integration of expert knowledge in AI-based Expert systems.

% This work proposes a novel approach to the problem of detecting deviations in data based on expert knowledge expressed in natural language. Using LLMs and the Python programming language, we developed a generic solution, allowing it to be used in any field with any language models. In addition, we proposed a solution based on the LLMs agent system to improve the obtained results. The obtained results show that the large language models do well in generating syntactically correct Python code, but have a clear problem with the correct implementation of functions with at least 2 parameters. This may be due to the fact that more complex implementation tasks require greater reasoning capabilities through LLMs. The models we studied were not explicitly taught to reason or trained strictly on implementation tasks, nor did they use prompting methods to force reasoning. The proposed method of improving results proved to be effective and allowed the models to improve their performance in almost every case. The results of our work set the stage for further development, with the goal of achieving better results for problems using more parameters needed in implementation.

% \begin{figure*}[t]
%   % \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   % \includegraphics[width=0.48\linewidth]{example-image-b}
%   \includegraphics[width=\textwidth]{latex/figures/LLM_logic_main_diagram.pdf}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}
\newpage
\section{Limitations}
While our study demonstrates the potential of LLMs for expert knowledge integration and constraint validation, several limitations must be acknowledged:
\newline
\textbf{Dataset Constraints:} Our experiments were conducted on a domain-specific dataset (vehicle operation parameters). This limits the generalizability of our findings to other fields, such as biomedicine or finance. The dataset includes synthetic premises and validation points. %While designed to reflect real-world scenarios, performance may differ when applied to noisy or ambiguous expert inputs.
\newline
\textbf{Model Selection and Training Considerations:} We evaluated general-purpose LLMs (Llama, Mistral, Qwen, etc.) rather than specialized code-generation models like Codestral or CodeLlama. These dedicated models might perform better in Python logic translation.
The models were used in their default instruction-tuned versions, without fine-tuning on domain-specific logical constraints, which may limit their effectiveness in complex reasoning tasks. However, the proposed framework can be used to evaluate the aforementioned.

\textbf{Prompting Limitations:} Our approach relied on In-Context Learning (ICL) without advanced prompting strategies like Chain-of-Thought (CoT) reasoning. These techniques might improve logical accuracy. The iterative refinement process improved performance but still relied on model-generated self-corrections, which may introduce biases or reinforce incorrect patterns.
\newline
\textbf{Computational Constraints:} Due to resource limitations, we only tested a single round of iterative correction per failure. Allowing multiple iterations with adaptive feedback could further enhance accuracy but would require more computational resources.
\newline
%Addressing these limitations in future research—through dataset expansion, fine-tuning on logical tasks, and improved reasoning strategies will be crucial for advancing LLM-driven constraint validation.
% \noindent
% \textbf{Evaluation dataset from one specific field:} We tested our approach on a dataset from a specific field, related to the operation of trucks. For this reason we can't compare the efficiency of our method between many fields to see if and how much our method depends on the data used? This could be the approach for further research, to compare the effectiveness of our method for many, different expert fields i.e. not only truck operational data but also medicine and biology.

% \noindent
% \textbf{Evaluated models:} The task we order to large language models is primarily to generate code in Python and we evaluate a default instruction-tuned models, not specifically designed for this task. An interesting idea to explore in the future seems to be the approach of using LLMs designed specifically for the task of generating code like Codestral, Codex or CodeLlama and seeing if this will improve performance in more complex tasks such as Task 2. 

% \noindent
% \textbf{Struggling with examples that require more reasoning:} As a result of our research, it emerged that the evaluated LLMs couldn't cope with the two examples used \textbf{[Do przedstawienia w Appendixie?]}, which required the model to deduce a mathematical formula describing the dependence of the two metrics on a specific value of a constant derived from the content of the condition. This therefore points to the low properties of these models in reasoning tasks and the need for techniques to enhance these properties.

% \noindent
% \textbf{Prompting techniques:} In our work, we did not use many advanced techniques for prompting language models, focusing instead on a simple In-context learning approach. This could be another research thread in future work, which would focus more on using prompting techniques geared more toward forcing LLMs to reason more during the task execution. These techniques could be Chain-of-Thought, Tree-of-Thought or even Graph-of-Thought which are designed to strengthen the models' ability to reason.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\nocite{bi2024iterative,borazjanizadeh2024reliable,clark2020transformers,enstrom2024reasoning,jiang2024leanreasoner,jing2024translating,li2024leveraging,liu2024logic,ma2024think,olausson2023linc,pan2023logic,quan2024verification,sun2024determlr,tan2024struct,wang2019logic,wang2024chatlogic,xu2024faithful,yang2024enhancing,feng2023language,feng2024language,wen2024enchanting,wu2023lemur,kalyanpur2024llm,wysocka2024large,lin2024llm,szymanski2024limitations,wang2023review,boicu2001automatic,han2024large,mcinerney2023chill,selby2024quantitative,chen2022large,chen2022program,wang2023meta,xu2023symbol,guo2024stop,lyu2024automatic,nejjar2025llms,soroco2025pde,ugare2024improving,wang2024enhancing,xu2024core,yadav2024pythonsaga,yang2024if,zhang2024refactoring,chu2024think,gemmateam2024gemmaopenmodelsbased,grattafiori2024llama3herdmodels,jiang2023mistral7b,jiang2024mixtralexperts,yang2024qwen2technicalreport}

\clearpage
\appendix
\section{Appendix}
% \renewcommand{\thetable}{\Alph{section}\arabic{table}}
\setcounter{table}{0}
\renewcommand{\thetable}{A.\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A.\arabic{figure}}

\subsection{Supplementary Figures}
\label{sec:appendix_figures}

\begin{figure}[h]
\centering
  \includegraphics[width=\textwidth]{latex/figures/Runtime_errors.pdf}
  \caption {Examples of the runtime errors.}
  \label{fig:runtime_errors}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{latex/figures/FSR_changes_comp.pdf}
    \caption {Changes in Formalization Success Rate (FSR) values per parameter on the next stages of improvement for both tasks.}
    \label{fig:FA_changes}
\end{figure}

\clearpage

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{latex/figures/LCR_changes_comp.pdf}
    \caption {Changes in Logical Consistency Rate (LCR) values per parameter on the next stages of improvement for both tasks.}
    \label{fig:OA_changes}
\end{figure}

\clearpage
\subsection{Supplementary Tables}
\label{sec:appendix_tables}

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \hline
     Parameter name & Min value & Max value & Unit \\
    \hline
    Distance & 20 & 110 & km \\
    Total operation time & 2 & 10 & h \\
    PTO operation time & 0.2 & 5 & h \\
    Idle time & 1 & 5 & h \\
    Driving time & 1 & 5 & h \\
    Stops & 20 & 300 & - \\ 
    Stops with PTO engaged & 20 & 250 & - \\
    Sehicle speed & 0 & 80 & km/h \\
    Engine speed & 550 & 1600 & rpm \\
    Axle 1 load & 2000 & 8000 & kg \\
    Axle 2 load & 7000 & 11500 & kg \\
    Axle 3 load & 4500 & 7500 & kg \\
    Payload & 0 & 11000 & kg \\
    Total fuel consumption & 10 & 100 & dm3 \\
    Compaction cycles & 50 & 300 & - \\
    Lifter cycles & 50 & 300 & - \\
    Refulling time & 0.08 & 0.2 & h \\  
    \hline
  \end{tabular}
  \caption{List of parameters used in both tasks.}
  \label{tab:parameters}
\end{table}

\clearpage

\begin{table}
  % \centering
  \begin{tabular}{ | m{5cm} | m{9cm} |}
        \hline
        \multicolumn{2}{| c |}{\textbf{1st Example premise}} \\
        \hline
        \textbf{Parameter} & Distance \\
        \hline 
        \textbf{Logical expression} & \[20 <= D <= 110\]\\
        \hline
       \textbf{ Premise version }1 & In a typical day, a vehicle travels between 20 and 110 km. \\
        \hline
        \textbf{Premise version 2} & During a typical day, a vehicle covers a distance of 20 to 110 km. \\
        \hline
        \textbf{Premise version 3} & On a usual day, a vehicle travels between 20 and 110 km. \\
        \hline
        \textbf{Premise version 4} & It is uncommon for a vehicle to travel less than 20 km or more than 110 km in a typical day. \\
        \hline
        \textbf{Premise version 5} & It is rare for a vehicle to cover less than 20 km or more than 110 km in a usual day. \\
        \hline
        \multicolumn{2}{| c |}{\textbf{2nd Example premise}} \\
        \hline
        \textbf{Parameter} & Total operation time \\
        \hline 
        \textbf{Logical expression} & \[2 <= T_{TOTAL} <= 10\]\\
        \hline
        \textbf{Premise version 1} & In a typical day, a vehicle's total operational time ranges from 2 to 10 hours. \\
        \hline
        \textbf{Premise version 2} & A vehicle's operational hours range from 2 to 10 in a typical day. \\
        \hline
        \textbf{Premise version 3} & A vehicle typically operates for 2 to 10 hours each day. \\
        \hline
        \textbf{Premise version 4} & It is unlikely for a vehicle to operate for less than 2 hours or more than 10 hours in a typical day. \\
        \hline
        \textbf{Premise version 5} & It is infrequent for a vehicle to operate for fewer than 2 hours or more than 10 hours in a typical day. \\
        \hline
        \multicolumn{2}{| c |}{\textbf{3rd Example premise}} \\
        \hline
        \textbf{Parameter} & PTO operation time \\
        \hline 
        \textbf{Logical expression} & \[0.2 <= T_{PTO} <= 5\]\\
        \hline
        \textbf{Premise version 1} & In a typical day, a vehicle's PTO is engaged for 0.2 to 5 hours. \\
        \hline
        \textbf{Premise version 2} & A vehicle's PTO operates for 0.2 to 5 hours in a typical day. \\
        \hline
        \textbf{Premise version 3} & On a normal day, the PTO operates for 0.2 to 5 hours. \\
        \hline
        \textbf{Premise version 4} & It is unusual for the PTO to be engaged for less than 0.2 hours or more than 5 hours in a day. \\
        \hline
        \textbf{Premise version 5} & It is not typical for the PTO to be engaged for under 0.2 hours or over 5 hours in a day. \\
        \hline
  \end{tabular}
  \caption{Task 1 - examples of premises, in all 5 lexical versions and logical expression.}
  \label{tab:task1_premises}
\end{table}

\clearpage

\begin{table}
  % \centering
  \begin{tabular}{ | m{5cm} | m{9cm} |}
    \hline
        \multicolumn{2}{| c |}{\textbf{1st Example premise}} \\
        \hline
        \textbf{Parameters} & [Total operation time, PTO operation time] \\
        \hline 
        \textbf{Logical expression} & \[T_{PTO} <= T_{TOTAL}\]\\
        \hline
        \textbf{Premise version 1} & The total operation duration must not be lower than the PTO operation time. \\
        \hline
        \textbf{Premise version 2} & PTO operation time cannot be greater than the total operation time. \\
        \hline
        \textbf{Premise version 3} & PTO operation time should be less than or equal to the total operation time. \\
        \hline
        \textbf{Premise version 4} & Total operation time shouldn’t be lower than PTO operation time. \\
        \hline
        \textbf{Premise version 5} & PTO operation time must be less than or equal to the total operation time. \\
        \hline
        \multicolumn{2}{| c |}{\textbf{2nd Example premise}} \\
        \hline
        \textbf{Parameters} & [Total operation time, Idle time] \\
        \hline 
        L\textbf{ogical expression} & \[T_{IDLE} <= T_{TOTAL}\]\\
        \hline
        \textbf{Premise version 1} & The total operation duration must not be lower than the idle time. \\
        \hline
        \textbf{Premise version 2} & Idle time cannot be greater than the total operation time. \\
        \hline
        \textbf{Premise version 3 }& Idle time should be less than or equal to the total operation time. \\
        \hline
        \textbf{Premise version 4 }& Total operation time shouldn’t be lower than idle time. \\
        \hline
        \textbf{Premise version 5} & Idle time must always be less than or equal to the total operation time. \\
        \hline
        \multicolumn{2}{| c |}{\textbf{3rd Example premise}} \\
        \hline
        \textbf{Parameters} & [Total operation time, PTO operation time, Idle time, Driving time] \\
        \hline 
        \textbf{Logical expression} & \[T_{TOTAL} >= T_{PTO} + T_{IDLE} + T_{DRIVING}\]\\
        \hline
        \textbf{Premise version 1} & The total operation time must always be not lower than the combined duration of PTO operation, idle time, and driving time. \\
        \hline
        \textbf{Premise version 2} & The combined time for PTO operation, idle time, and driving time must not surpass the total operation time. \\
        \hline
        \textbf{Premise version 3} & Total operation time must be greater than or equal to the sum of PTO operation, idle, and driving times. \\
        \hline
        \textbf{Premise version 4} & Total operation time should be at least the sum of PTO, idle, and driving times. \\
        \hline
        \textbf{Premise version 5} & The total operation duration must always be greater than or equal to the combined time for PTO, idle, and driving. \\ 
    \hline
  \end{tabular}
  \caption{Task 2 - examples of premises, in all 5 lexical versions and logical expression.}
  \label{tab:task2_premises}
\end{table}

    % \hline
    %   Parameters & Logical expression & Version 1 & Version 2 & Version 3 & Version 4 & Version 5  \\
    % \hline
    % distance & 1.0 & In a typical day, a vehicle travels between 20 and 110 km. & During a typical day, a vehicle covers a distance of 20 to 110 km. & On a usual day, a vehicle travels between 20 and 110 km. & It is uncommon for a vehicle to travel less than 20 km or more than 110 km in a typical day. & It is rare for a vehicle to cover less than 20 km or more than 110 km in a usual day. \\
    % \hline
    % total operation time & 1.0 & In a typical day, a vehicle's total operational time ranges from 2 to 10 hours. & A vehicle's operational hours range from 2 to 10 in a typical day. & A vehicle typically operates for 2 to 10 hours each day. & It is unlikely for a vehicle to operate for less than 2 hours or more than 10 hours in a typical day. & It is infrequent for a vehicle to operate for fewer than 2 hours or more than 10 hours in a typical day. \\ 
    % \hline
    % PTO operation time & 1.0 & In a typical day, a vehicle's PTO is engaged for 0.2 to 5 hours. & A vehicle's PTO operates for 0.2 to 5 hours in a typical day. & On a normal day, the PTO operates for 0.2 to 5 hours. & It is unusual for the PTO to be engaged for less than 0.2 hours or more than 5 hours in a day. & It is not typical for the PTO to be engaged for under 0.2 hours or over 5 hours in a day. \\ 
    % \hline
    % idle time & 1.0 & In a typical day, a vehicle idles for 1 to 5 hours. & In a typical day, a vehicle idles for between 1 and 5 hours. & Typically, a vehicle idles for 1 to 5 hours daily. & A vehicle idling for less than 1 hour or more than 5 hours in a day is not common. & It is rare for a vehicle to idle for less than 1 hour or more than 5 hours in a day. \\
    % \hline
    % driving time & 1.0 & In a typical day, a vehicle spends 1 to 5 hours driving. & A vehicle spends 1 to 5 hours driving each typical day. & During a normal day, a vehicle is driven for 1 to 5 hours. & It is uncommon for a vehicle to spend less than 1 hour or more than 5 hours driving in a day. & It is not common for a vehicle to drive for less than 1 hour or more than 5 hours in a day. \\ 
    % \hline



% \begin{figure*}
%   \includegraphics[width=0.48\linewidth]{latex/figures/Task1_points_generation.pdf} \hfill
%   \includegraphics[width=0.48\linewidth]{latex/figures/Task2_points_generation.pdf}
%   % \includegraphics[width=\textwidth]{latex/figures/Task1_points_generation.pdf}
%   \caption {Visualization of the process of generation validation data points for both tasks.}
%   \label{fig:val_points}
% \end{figure*}


% \begin{figure*}
  
%   \includegraphics[width=0.48\linewidth]{latex/figures/Prompt_structure_correction_tasks.pdf}
%   % \includegraphics[width=\textwidth]{latex/figures/Prompt_structure.pdf}
%   \caption {Visualization of prompt structure for both tasks, for initial inference and refinement steps.}
%   \label{fig:prompt_structure}
% \end{figure*}
\end{document}
