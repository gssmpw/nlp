\section{Related work}
Our work intersects several research streams including neurosymbolic reasoning, code generation from natural language, and iterative self‐correction. Several recent studies have explored neurosymbolic approaches that translate natural language problems into formal logical representations. For instance, LeanReasoner \cite{jiang2024leanreasoner} and Logic-LM \cite{pan2023logic} formalize reasoning tasks as theorems to be proven by external solvers, while LINC \cite{olausson2023linc} and Symbolic Chain-of-Thought \cite{xu2024faithful} convert natural language inputs into first-order logic for symbolic deduction. Similarly, works such as Logic-of-Thought \cite{liu2024logic} and Logic Agent \cite{liu2024logic} inject explicit logical rules into the reasoning process, enhancing the coherence and validity of generated outputs.

Parallel to these efforts, iterative refinement strategies have emerged to address error accumulation in LLM outputs. For example, LLM-ARC \cite{kalyanpur2024llm} and Determlr \cite{sun2024determlr} leverage automated feedback to guide the self-correction of reasoning chains, while SIP-BART \cite{enstrom2024reasoning} and ChatLogic \cite{wang2024chatlogic} integrate symbolic checkers or logic programming into the inference process. In the realm of code generation, methods such as AutoSpec \cite{wen2024enchanting} and CoCoGen \cite{bi2024iterative} employ compiler feedback and static analysis to iteratively refine generated code. Yet, despite these promising approaches, challenges remain in reliably translating domain-specific expert knowledge into executable code.

Our framework, ExKLoP, specifically addresses the underexplored challenge of converting expert engineering guidelines—expressed as natural language premises—into Python logic rules that can be executed and validated. While previous studies have focused on symbolic reasoning \cite{jiang2024leanreasoner, pan2023logic, olausson2023linc, xu2024faithful, liu2024logic, wysocka2024syllobio, dalal-etal-2024-inference,quan-etal-2024-verification,quan-etal-2024-enhancing, meadows-etal-2024-symbolic, quan-etal-2024-verification,meadows2025controllingequationalreasoninglarge} or on general code generation benchmarks \cite{wang2023review, yadav2024pythonsaga}, our work uniquely investigates whether LLMs can integrate and preserve the nuanced constraints of domain-specific recommendations, as encountered in industrial and engineering settings.

Additionally, research on integrating expert intuition into machine learning pipelines—such as the work by \cite{bogatu2023meta}, \cite{jing2024translating} and \cite{boicu2001automatic}—has largely focused on feature extraction for predictive analytics. In contrast, ExKLoP evaluates the transformation of expert knowledge into operational logic, ensuring that the resulting code not only meets syntactic standards but also adheres to strict logical constraints. This is crucial when the generated code is intended to monitor and control critical systems, as even minor logical errors can have significant real-world consequences.

Our investigation also builds upon prior research in iterative self-correction and refinement. While frameworks like LLM-ARC \cite{kalyanpur2024llm} and Determlr \cite{sun2024determlr} demonstrate that feedback loops can enhance reasoning accuracy, our experiments reveal that—despite near-perfect syntactic outputs from many LLMs—logical inconsistencies persist in the translation of expert rules. This observation aligns with findings in studies on code verification and reasoning \cite{wu2023lemur, feng2023language, feng2024language, chen2022program}, suggesting that additional strategies, such as cross-model correction \cite{li2024leveraging} or more robust feedback mechanisms \cite{yang2024enhancing}, may be necessary.

Beyond these core areas, our work resonates with broader efforts in the LLM community to enhance code quality and logical reasoning. Reviews on code generation \cite{wang2023review} and advancements in efficient syntactic decoding \cite{ugare2024improving} provide context for our challenges, while studies on program synthesis and refactoring \cite{zhang2024refactoring, lyu2024automatic} underscore the ongoing need for improved integration of formal methods into LLM outputs. Moreover, research that bridges natural language and formal reasoning—such as CoRE \cite{xu2024core} and work on automated specification synthesis \cite{wen2024enchanting}—further highlights the diversity of approaches aimed at enhancing the interpretability and reliability of LLM-generated code.

Finally, our work complements recent advances in large-scale LLMs and their applications in diverse domains \cite{clark2020transformers, wang2019logic, yang2024if, nejjar2025llms, chu2024think, wysocka2024large, wysocki2024llm, delmas-etal-2024-relation, wysocki2023transformers}, and aligns with emerging trends in model scaling and multimodal integration \cite{grattafiori2024llama3herdmodels, jiang2024mixtralexperts, jiang2023mistral7b, yang2024qwen2technicalreport, gemmateam2024gemmaopenmodelsbased}. By focusing on the specific challenge of encoding domain-specific expert knowledge into executable logic and systematically assessing self-correction, ExKLoP offers a valuable benchmark and a novel perspective that bridges the gap between expert systems and automated reasoning.

In summary, while prior work has made significant strides in symbolic reasoning, iterative self-correction, and expert knowledge integration, our framework uniquely contributes to the field by targeting the reliable conversion of expert domain guidelines into validated Python logic, thereby laying the groundwork for more robust, knowledge-informed AI systems.





% Integrating Large Language Models (LLMs) with expert knowledge expressed in natural language provides a promising method for evaluating parameter values across domains \citep{tan2024struct}, \citep{sun2024determlr}, \citep{pan2023logic}, \citep{nejjar2025llms}, \citep{selby2024quantitative}. LLMs process complex, domain-specific information, converting expert insights into quantifiable features for improved predictive analytics. A study showed how LLMs convert investigator insights into structured features, enhancing risk assessment and decision-making \citep{wen2024enchanting}. However, challenges remain in ensuring the accuracy and reliability of LLM outputs, particularly in specialized fields like biomedicine. A proposed framework streamlines expert evaluation of LLM-generated content to maintain scientific factuality \citep{wysocka2024large}.

% Some studies also explore LLMs as evaluators of their own outputs \citep{chu2024think}. \citep{liu2024logic} found that LLM evaluations aligned with human experts in specific tasks, suggesting LLMs could complement human evaluation. However, LLMs may not fully capture the nuanced judgments of experts, especially in complex tasks. Therefore, while LLMs are useful for evaluating parameter values from expert knowledge, addressing factual accuracy and depth of understanding is crucial. Combining LLMs with human expertise can lead to more reliable evaluations across various domains \citep{chu2024think}.

% Recent advancements combine LLMs with symbolic solvers and programming languages to improve value range checking and constraint validation from natural language premises \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{quan2024verification}, \citep{enstrom2024reasoning}, \citep{soroco2025pde}, \citep{chen2022program}, \citep{chen2022large}, \citep{wang2023meta}. This integration merges LLM flexibility with formal methods' rigor, enhancing logical reasoning and program verification. Logic-LM \citep{pan2023logic} translates natural language into symbolic logic, refining errors through solver feedback for better logical accuracy. The three-stage pipeline processes logical premises, ensuring consistency and correctness. LINC \citep{olausson2023linc} uses LLMs as semantic parsers to convert premises into first-order logic, outperforming traditional prompting in verifying consistency in datasets like FOLIO and ProofWriter. Lemur \citep{wu2023lemur} combines LLMs with automated reasoners for program verification, improving performance on benchmarks. LoGiPT \citep{feng2023language}, \citep{feng2024language} emulates logical solvers with strict syntax adherence, enhancing logical reasoning. AutoSpec \citep{wen2024enchanting} generates and validates program specifications with LLMs, ensuring alignment with program behavior across multiple benchmarks.

% These studies show that integrating LLMs with symbolic solvers enhances logical consistency, inference reliability, and program verification precision, offering scalable solutions for automating constraint validation and value range checking.


% The advent of Large Language Models (LLMs) has significantly transformed the landscape of code generation, enabling the translation of natural language descriptions into executable code \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{wang2023review}, \citep{li2024leveraging}, \citep{xu2024core}, \citep{lyu2024automatic}, \citep{yang2024if}, \citep{nejjar2025llms}, \citep{ugare2024improving}. These models have demonstrated remarkable proficiency in understanding and generating programming constructs, thereby facilitating tasks such as range checking and constraint validation. Range checking involves verifying whether a given value falls within a specified parameter range, while constraint validation assesses if the values of multiple parameters satisfy predefined conditions. Recent advancements have also explored the integration of LLMs with symbolic solvers and programming languages to enhance the precision of value range checking and constraint validation derived from natural language premises \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{xu2024faithful}, \citep{wang2023meta}, \citep{xu2023symbol}, \citep{chen2022program}. This fusion combines the adaptability of LLMs with the rigor of formal methods, thereby improving logical reasoning and program verification \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{quan2024verification}, \citep{xu2024faithful}, \citep{wang2023meta}, \citep{xu2023symbol}, \citep{chen2022program}. For instance, Logic-LM \citep{pan2023logic} translates natural language into symbolic logic, refining errors through solver feedback to achieve better logical accuracy. Similarly, LINC \citep{olausson2023linc} employs LLMs as semantic parsers to convert premises into first-order logic, outperforming traditional prompting methods in verifying consistency across various datasets. These studies underscore the potential of integrating LLMs with symbolic solvers to enhance logical consistency, inference reliability, and program verification precision, offering scalable solutions for automating constraint validation and value range checking \citep{jiang2024leanreasoner}, \citep{pan2023logic}, \citep{quan2024verification}, \citep{xu2024faithful}, \citep{wang2023meta}, \citep{xu2023symbol}, \citep{chen2022program}.

% Moreover, combining LLMs with expert knowledge expressed in natural language presents a promising approach for evaluating parameter values across diverse domains \citep{selby2024quantitative}, \citep{han2024large}, \citep{jing2024translating}. LLMs can process complex, domain-specific information, converting expert insights into quantifiable features for improved predictive analytics \citep{boicu2001automatic}, \citep{nejjar2025llms}. For example, \citep{jing2024translating} demonstrated how LLMs could transform investigator insights into structured features, thereby enhancing risk assessment and decision-making. However, challenges persist in ensuring the accuracy and reliability of LLM outputs, particularly in specialized fields like biomedicine. To address this, framework have been proposed to streamline expert evaluation of LLM-generated content, maintaining scientific factuality \citep{wysocka2024large}.

% In addition to leveraging external expert knowledge, some studies have explored the potential of LLMs to evaluate their own outputs \citep{szymanski2024limitations}. \citep{szymanski2024limitations} found that LLM evaluations aligned with human experts in specific tasks, suggesting that LLMs could complement human evaluation \citep{selby2024quantitative}, \citep{mcinerney2023chill}. Nevertheless, LLMs may not fully capture the nuanced judgments of experts, especially in complex tasks. Therefore, while LLMs are useful for evaluating parameter values from expert knowledge, addressing factual accuracy and depth of understanding remains crucial \citep{boicu2001automatic}. Combining LLMs with human expertise can lead to more reliable evaluations across various domains.

%While previous studies have explored LLMs for logic formalization, there remains a gap in automating and iteratively refining their outputs with minimal human intervention. This research proposes a structured methodology to improve LLM-generated logic through a systematic refinement process, enhancing reliability in real-world applications.