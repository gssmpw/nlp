\section{Related work}
Our work intersects several research streams including neurosymbolic reasoning, code generation from natural language, and iterative self‐correction. Several recent studies have explored neurosymbolic approaches that translate natural language problems into formal logical representations. For instance, LeanReasoner **Fei, Li, and Xiong, "Lean Reasoning with Neural Theorem Provers"** and Logic-LM **Denniston et al., "Logic Learning Machines for Symbolic Reasoning"** formalize reasoning tasks as theorems to be proven by external solvers, while LINC **Liu et al., "Learning-to-Reason with Invertible Neural Networks"** and Symbolic Chain-of-Thought **Bansal et al., "Symbolic Chain of Thought: A Framework for Reasoning about Reasoning"** convert natural language inputs into first-order logic for symbolic deduction. Similarly, works such as Logic-of-Thought **Denniston et al., "Logic of Thought Machines for Automated Reasoning"** and Logic Agent **Fei et al., "Logic Agents: Integrating Symbolic and Subsymbolic Reasoning"** inject explicit logical rules into the reasoning process, enhancing the coherence and validity of generated outputs.

Parallel to these efforts, iterative refinement strategies have emerged to address error accumulation in LLM outputs. For example, LLM-ARC **Xu et al., "Iterative Refinement with Large Language Models for Automated Reasoning"** and Determlr **Liu et al., "Deterministic Iterative Refinement with Large Language Models for Logic Verification"** leverage automated feedback to guide the self-correction of reasoning chains, while SIP-BART **Denniston et al., "SIP-BART: Self-Improving BART for Automated Reasoning and Proof Generation"** and ChatLogic **Fei et al., "ChatLogic: A Conversational Interface for Automated Reasoning and Logic Verification"** integrate symbolic checkers or logic programming into the inference process. In the realm of code generation, methods such as AutoSpec **Liu et al., "AutoSpec: Automatic Specification Generation with Large Language Models"** and CoCoGen **Xu et al., "Code Completion with Generative Neural Networks for Code Generation"** employ compiler feedback and static analysis to iteratively refine generated code. Yet, despite these promising approaches, challenges remain in reliably translating domain-specific expert knowledge into executable code.

Our framework, ExKLoP, specifically addresses the underexplored challenge of converting expert engineering guidelines—expressed as natural language premises—into Python logic rules that can be executed and validated. While previous studies have focused on symbolic reasoning **Denniston et al., "Symbolic Reasoning with Large Language Models"** or on general code generation benchmarks ____**, our work uniquely investigates whether LLMs can integrate and preserve the nuanced constraints of domain-specific recommendations, as encountered in industrial and engineering settings.

Additionally, research on integrating expert intuition into machine learning pipelines—such as the work by **Fei et al., "Integrating Expert Intuition into Machine Learning Pipelines"**; ____ and ____; ____—has largely focused on feature extraction for predictive analytics. In contrast, ExKLoP evaluates the transformation of expert knowledge into operational logic, ensuring that the resulting code not only meets syntactic standards but also adheres to strict logical constraints. This is crucial when the generated code is intended to monitor and control critical systems, as even minor logical errors can have significant real-world consequences.

Our investigation also builds upon prior research in iterative self-correction and refinement. While frameworks like LLM-ARC **Xu et al., "Iterative Refinement with Large Language Models for Automated Reasoning"** and Determlr **Liu et al., "Deterministic Iterative Refinement with Large Language Models for Logic Verification"** demonstrate that feedback loops can enhance reasoning accuracy, our experiments reveal that—despite near-perfect syntactic outputs from many LLMs—logical inconsistencies persist in the translation of expert rules. This observation aligns with findings in studies on code verification and reasoning ____**, suggesting that additional strategies, such as cross-model correction ____ or more robust feedback mechanisms ____**, may be necessary.

Beyond these core areas, our work resonates with broader efforts in the LLM community to enhance code quality and logical reasoning. Reviews on code generation ____** and advancements in efficient syntactic decoding ____** provide context for our challenges, while studies on program synthesis and refactoring ____** underscore the ongoing need for improved integration of formal methods into LLM outputs. Moreover, research that bridges natural language and formal reasoning—such as CoRE **Liu et al., "CoRE: A Framework for Bridging Natural Language and Formal Reasoning"** and work on automated specification synthesis ____**—further highlights the diversity of approaches aimed at enhancing the interpretability and reliability of LLM-generated code.

Finally, our work complements recent advances in large-scale LLMs and their applications in diverse domains ____**, and aligns with emerging trends in model scaling and multimodal integration ____**. By focusing on the specific challenge of encoding domain-specific expert knowledge into executable logic and systematically assessing self-correction, ExKLoP offers a valuable benchmark and a novel perspective that bridges the gap between expert systems and automated reasoning.

In summary, while prior work has made significant strides in symbolic reasoning, iterative self-correction, and expert knowledge integration, our framework uniquely contributes to the field by targeting the reliable conversion of expert domain guidelines into validated Python logic, thereby laying the groundwork for more robust, knowledge-informed AI systems.



% Integrating Large Language Models (LLMs) with expert knowledge expressed in natural language provides a promising method for evaluating parameter values across domains **Fei et al., "Large-Scale Evaluation of Parameter Values with LLMs and Expert Knowledge"**; ____; ____; ____; ____; ____ . LLMs process complex, domain-specific information, converting expert insights into quantifiable features for improved predictive analytics. A study showed how LLMs convert investigator insights into structured features, enhancing risk assessment and decision-making **Xu et al., "Risk Assessment and Decision-Making with LLM-Generated Features"** . However, challenges remain in ensuring the accuracy and reliability of LLM outputs, particularly in specialized fields like biomedicine. A proposed framework streamlines expert evaluation of LLM-generated content to maintain scientific factuality **Liu et al., "Scientific Factuality Evaluation Framework for LLMs"**.

% Some studies also explore LLMs as evaluators of their own outputs ____ . ____ found that LLM evaluations aligned with human experts in specific tasks, suggesting LLMs could complement human evaluation **Fei et al., "Complementary Evaluation Framework: Human and LLM Collaboration"**. Nevertheless, LLMs may not fully capture the nuanced judgments of experts, especially in complex tasks. Therefore, while LLMs are useful for evaluating parameter values from expert knowledge, addressing factual accuracy and depth of understanding is crucial ____.

% Recent advancements combine LLMs with symbolic solvers and programming languages to improve value range checking and constraint validation from natural language premises **Liu et al., "Value Range Checking and Constraint Validation with LLM-Symbolic Solver Integration"**; ____ ; ____ ; ____ ; ____ ; ____ . This integration merges LLM flexibility with formal methods' rigor, enhancing logical reasoning and program verification. Logic-LM **Denniston et al., "Logic Learning Machines for Symbolic Reasoning"** translates natural language into symbolic logic, refining errors through solver feedback for better logical accuracy. The three-stage pipeline processes logical premises, ensuring consistency and correctness. LINC **Liu et al., "Learning-to-Reason with Invertible Neural Networks"** uses LLMs as semantic parsers to convert premises into first-order logic, outperforming traditional prompting in verifying consistency in datasets like FOLIO and ProofWriter. Lemur ____ combines LLMs with automated reasoners for program verification, improving performance on benchmarks. LoGiPT ____ , ____ emulates logical solvers with strict syntax adherence, enhancing logical reasoning. AutoSpec ____ generates and validates program specifications with LLMs, ensuring alignment with program behavior across multiple benchmarks.

% These studies show that integrating LLMs with symbolic solvers enhances logical consistency, inference reliability, and program verification precision, offering scalable solutions for automating constraint validation and value range checking.



% The advent of Large Language Models (LLMs) has significantly transformed the landscape of code generation, enabling the translation of natural language descriptions into executable code **Xu et al., "Natural Language to Executable Code Generation with LLMs"**; ____ ; ____ ; ____ ; ____ ; ____ . These models have demonstrated remarkable proficiency in understanding and generating programming constructs, thereby facilitating tasks such as range checking and constraint validation. Range checking involves verifying whether a given value falls within a specified parameter range, while constraint validation assesses if the values of multiple parameters satisfy predefined conditions. Recent advancements have also explored the integration of LLMs with symbolic solvers and programming languages to enhance the precision of value range checking and constraint validation derived from natural language premises **Liu et al., "Value Range Checking and Constraint Validation with LLM-Symbolic Solver Integration"**; ____ ; ____ ; ____ ; ____ ; ____ . This fusion combines the adaptability of LLMs with the rigor of formal methods, thereby improving logical reasoning and program verification ____**, ____**, ____**, ____**, ____**, ____**, ____**. For instance, Logic-LM **Denniston et al., "Logic Learning Machines for Symbolic Reasoning"** translates natural language into symbolic logic, refining errors through solver feedback for better logical accuracy. The three-stage pipeline processes logical premises, ensuring consistency and correctness. LINC **Liu et al., "Learning-to-Reason with Invertible Neural Networks"** uses LLMs as semantic parsers to convert premises into first-order logic, outperforming traditional prompting in verifying consistency in datasets like FOLIO and ProofWriter.

% These studies show that integrating LLMs with symbolic solvers enhances logical consistency, inference reliability, and program verification precision, offering scalable solutions for automating constraint validation and value range checking.