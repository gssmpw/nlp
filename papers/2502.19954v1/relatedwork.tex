\section{Related work}
% \vspace{-6pt}
% Our CoVer uses the collaboration of SLM and LLM to handle two key challenges on stance detection: information insufficiency and unclear classification criteria.

\subsubsection{Stance Detection via Knowledge-Augmentation}
To enhance the understanding and classification of a stance in a given text \cite{li2023stance,liu2021enhancing}, many studies leverage external knowledge sources, such as knowledge graphs \cite{liu2021enhancing}, structured databases \cite{auer2007dbpedia}, and external textual information \cite{zhu2022enhancing} for knowledge augmentation. By incorporating external knowledge such as DBpedia \cite{auer2007dbpedia} or ConceptNet \cite{speer2017conceptnet}, models can gain a deeper contextual understanding, particularly useful for identifying implicit stances or understanding domain-specific terminology.
Additionally, recent studies \cite{wang2023boosting} indicate that integrating factual and contextual knowledge can significantly enhance the modelâ€™s ability to detect subtle or implicit stances, especially in scenarios with limited or biased training data.

In summary, knowledge augmentation has been proven by existing studies to be an effective strategy for enhancing stance classification. It addresses information insufficiency by providing context, resolving ambiguities, and identifying subtle relationships between the text and the target, which is especially effective in complex scenarios where direct textual information is limited.

\vspace{-5pt}
\subsubsection{Stance Detection via Reasoning}
Many studies \cite{liang2022zero,liu2021enhancing,zhang2023investigating} emphasize identifying stances in text through logical reasoning. 
These methods focus on analyzing arguments, causal relations, and implicit cues within the text to determine the stance, making them particularly effective in few-shot and zero-shot scenarios with complex arguments.
Recently, some studies have combined LLMs with such strategies to generate reasoning chains for stance detection. Specifically, the Logically Consistent Chain-of-Thought (LC-CoT) \cite{zhang2023investigating} enhances zero-shot stance detection by evaluating external knowledge requirements, invoking APIs to retrieve background knowledge, and employing if-then logic templates to generate reasoning chains. The Collaborative Role-Infused LLM-based Agents (COLA) \cite{lan2024stance} sets up multi-role LLM agents (e.g., linguistic experts, domain specialists, social media experts) for multi-view analysis.

In summary, stance detection via reasoning effectively handles implicit meanings and multi-step reasoning contexts by logical reasoning, demonstrating significant advantages in few-shot and zero-shot scenarios.




% In this work, we propose the method named Hierarchical Text Classification via Fusing the Structural and Semantic  (S\textsuperscript{2}-HTC), which ensures consistent hierarchical classification by integrating explicit structural relation information and implicit label semantic information. 
% Firstly, we use the Label Semantic-Aware Hierarchical Adjacency Matrix (LSA-HAM) to record both structural and semantic relations among labels. By applying the LSA-HAM, we execute a hierarchical consistency alignment process to incorporate hierarchical information features into our model.
% To more effectively capture the features of minor classes, we introduce the Class Balanced Negative Tolerance Regulation (CB-NTR) loss function.
% Moreover, we explore the effectiveness of Simple Contrastive Learning of Sentence Embeddings (SimCSE) and pre-trained methods, which contribute to the enhancement of our model. 
% Our experiments on the WOS, BGC, and NST5 datasets illustrate the state-of-the-art performance of S\textsuperscript{2}-HTC.

% combines the strengths of LLM and SLM to achieve balanced computational consumption and model performance