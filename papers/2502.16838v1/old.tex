\section{Introduction}
Event extraction (EE) is an essential NLP task for event-centric information extraction. It transforms information into actionable structured knowledge. EE comprises two tasks: event detection (ED) and event argument extraction (EAE). ED identifies events in the text, while EAE extracts event-specific details or arguments according to predefined event roles \cite{}. EAE is core to event extraction, as it benefits a wide range of applications, including document understanding \cite{tong-etal-2022-docee}, misinformation detection \cite{wu-etal-2022-cross}, discourse understanding \cite{sharif-etal-2024-explicit}, pharmacovigilance \cite{sun-etal-2022-phee}. With the rise of generative models (e.g., LLMs), EAE has gained significant attention in recent years. \cite{zhang-etal-2024-ultra, sun-etal-2024-leveraging, zhang-etal-2024-moka}. Consequently, the accurate evaluation of these generative models has become increasingly important.




%By eliminating obvious matches, it reduces computation cost for a later stage. RM stage aims to capture arguments that are semantically similar with minor syntactic variations. Complex match helps to capture sematically similar arguments based on context but structurally different phrases. This stage requires strong LLM for judging as it requires nuance and complex understanding of the context. Finally, in the judgment alignment stage, we account the scores in each level for potential misjudgments based on human evaluation. 

% \textcolor{red}{[Points for intro]},
% Advantage of multi-level matching and JAM score,
% \begin{itemize}
%     \item Multi-level framework is more reliable than only using GPT-4 as a judge because we systematically reduce the possibility of error. There will be no error in exact matching and minimal error in high-threshold relaxed matching. 

%     \item It also reduces the burden of human evaluation because we will need only to check the complex match pairs where we use a judge model as a human proxy. Minium human evaluation is required for high-threshold relaxed matching. 

%      \item The traditional way of calculating precision, recall, and f1-score with GPT-4 as judge overestimate model performance. Our proposed JAM score provides an improved correlation with human judgments on a dataset. This helps us to get more realistic score if use an LLM as judge model to proxy a human. 

%     \item Reduce the overall evaluation cost by reducing the number of inference calls. 
% \end{itemize}

\textcolor{red}{[Points for 2nd part of the intro]}
Our multi-level evaluation framework has carefully designed four steps.
\begin{itemize}
    \item \textbf{Exact match} helps to filter the phrases that match exactly. By eliminating obvious matches, it will reduce computation cost for a later stage. More reliable human judgment is not required.
    
    \item \textbf{High threshold relaxed matching} help to capture phrases that are semantically very similar with minor syntactic variation (e.g. “prescribed medication” vs. “administered medication”). [predictions missing redundant words or coreference]

    \item \textbf{Complex matching} to find semantically similar but structurally different phrases.  
    
    [Reserving LLMs or fine-tuned models for the most challenging cases where complex, nuanced contextual understanding is required.]
    \item \textbf{Judgment alignment}:
\end{itemize}



\textcolor{red}{[Points for intro]},
Advantage of multi-level matching and JAM score,
\begin{itemize}
    \item Multi-level framework is more reliable than only using GPT-4 as a judge because we systematically reduce the possibility of error. There will be no error in exact matching and minimal error in high-threshold relaxed matching. 

    \item It also reduces the burden of human evaluation because we will need only to check the complex match pairs where we use a judge model as a human proxy. Minium human evaluation is required for high-threshold relaxed matching. 

     \item The traditional way of calculating precision, recall, and f1-score with GPT-4 as judge overestimate model performance. Our proposed JAM score provides an improved correlation with human judgments on a dataset. This helps us to get more realistic score if use an LLM as judge model to proxy a human. 

    \item Reduce the overall evaluation cost by reducing the number of inference calls. 
\end{itemize}

\subsection{Level-1: Exact Match}
We get an exact match pair when \( p_i = g_j \). So, the list of exact match (EM) pairs are $[(p_1, g_2), (p_3, g_5), \ldots, (p_x, g_y)]$. We calculate the precision, recall, and f1-score from the list of EM-level as follows.
\[
\text{EM}_{p} = \frac{\text{NP}_e }{|P|}, \quad
\text{EM}_{r} = \frac{\text{NG}_e }{|G|}
\]
\[
\text{EM}_{f1} = \frac{2 * \text{EM}_{p} * \text{EM}_{r}}{\text{EM}_{p}+\text{EM}_{r}}
\]
Here, $\text{NP}_e$ and $\text{NG}_e$ indicate the total number of arguments predicted correctly from the predicted ($P$) and ground-truth ($G$) arguments list. $\text{EM}_{p}$, $\text{EM}_{r}$, and $\text{EM}_{f1}$ denotes precision, recall, and f1-score under exact match. For the exact match level the value of $\text{NP}_e$ and $\text{NG}_e$ will be the same.

\subsection{Level 2: High-threshold Relaxed-match}


After the exact match, we get an updated list of ground truth and predictions for the relaxed match. We remove the arguments from both lists where we find a match in level-1. The updated predicted and ground-truth arguments lists are as follows. 

\[
P'_{rm} = [p'_1, p'_2, \ldots, p'_{l_{x2}}]
\]
\[
G'_{rm} = [g'_1, g'_2, \ldots, g'_{l_{y2}}] 
\]

We calculate the semantic similarity between all possible pairs: $[(p'_1, g'_1), (p'_1, g'_2), \ldots (p'_1, g'_{l_{y2}}), \dots (p'_{l_{x2}}, a'_{l_{y2}})]$. If the similarity score of a pair $(p'_x, a'_y) > T_r$, then this is matched under relaxed match settings. $T_r$ denotes the predefined threshold selected for relaxed matching. So, the list of relaxed match (RM) pairs is a subset of all possible pairs.
\[
\text{RM pairs} = [(p'_1, a'_3), (p'_3, a'_4), \ldots, (p'_x, a'_y)]
\]

\noindent
We calculate the precision, recall, and f1-score from the list of relaxed match pairs as follows.
\[
\text{RM}_{p} = \frac{\text{NP}_e + \text{NP}_r}{|P|}, \quad
\text{RM}_{r} = \frac{\text{NA}_e + \text{NA}_r}{|A|}
\]
\[
\text{RM}_{f1} = \frac{2 * \text{RM}_{p} * \text{RM}_{r}}{\text{RM}_{p}+\text{RM}_{r}}
\]
Here, $\text{NP}_r$ and $\text{NA}_r$ indicate the total number of arguments predicted correctly from the predicted ($P$) and ground-truth ($A$) arguments list under relaxed match. $\text{NP}_e$ and $\text{NA}_e$ values are calculated in previous level. $\text{RM}_{p}$, $\text{RM}_{r}$, and $\text{RM}_{f1}$ denotes precision, recall, and f1-score for relaed match. 

Here, \(\text{NP}_r\) and \(\text{NA}_r\) represent the arguments matched under relaxed conditions form $P'_{rm}$ and $G'_{rm}$, while \(\text{NP}_e\) and \(\text{NA}_e\) are taken from Level 1.  $\text{NP}_r$ and $\text{NG}_r$ may not be similar under relaxed match because $p'_x$ or $g'_x$ can appear in multiple pairs where the similarity is higher than threshold. To ensure to do not over count the values we keep a separate count for how many of the arguments correctly match for the prediction list and from the ground truth list. 


\subsection{Level 3: Complex Match}
%\textcolor{blue}{I wonder if we should call this a robust match instead of a complex match: it's not exactly a complex process, rather we're trying to make the matching more robust to similar arguments with different surface forms. }

After exact and relaxed matching, unmatched arguments are carried forward for complex matching. Arguments matched in previous levels are removed from the updated predicted and ground-truth lists:
\[
P''_{cm} = [p''_1, p''_2, \ldots, p''_{l_{x3}}]
\]
\[
G''_{cm} = [g''_1, g''_2, \ldots, g''_{l_{y3}}] 
\]



After the exact and relaxed match, we get an updated list of ground truth and predictions for complex matching. Where we find a match at level 1 or level 2, all the arguments are removed from both lists. The updated predicted and ground-truth arguments list for complex matching is as follows. 

\[
P''_{cm} = [p''_1, p''_2, \ldots, p''_{l3}]
\]
\[
A''_{cm} = [a''_1, a''_2, \ldots, a''_{l3}] 
\]

From all possible pairs in both lists, we use a judge model to determine whether a pair $(p''_x, a''_y)$ is similar. If it is, we add it to the complex match pair list. 

\[
\text{CM pairs} = [(p''_1, a''_2), (p''_4, a''_6), \ldots, (p''_x, a''_y)]
\]

\noindent
We calculate the precision, recall, and f1-score from the list of complex match pairs as follows.
\[
\text{CM}_{p} = \frac{\text{NP}_e + \text{NP}_r + \text{NP}_c}{|P|}
\]
\[
\text{CM}_{r} = \frac{\text{NA}_e + \text{NA}_r + \text{NA}_c}{|A|}
\]
\[
\text{CM}_{f1} = \frac{2 * \text{CM}_{p} * \text{CM}_{r}}{\text{CM}_{p}+\text{CM}_{r}}
\]
Here, $\text{NP}_c$ and $\text{NA}_c$ indicate the total number of arguments predicted correctly from the predicted ($P$) and ground-truth ($A$) arguments list under complex match. $\text{CM}_{p}$, $\text{CM}_{r}$, and $\text{CM}_{f1}$ denotes precision, recall, and f1-score for complex match.

\subsection{Generic equation:} We have three levels for matching L = [EM, RM, CM]. $\text{NP}_{em}$, $\text{NP}_{rm}$, $\text{NP}_{cm}$ denotes the number of arguments predicted correctly from the predictions list ($P$) for each level. $\text{NA}_{em}$, $\text{NA}_{rm}$, $\text{NA}_{cm}$ denotes the number of arguments predicted correctly from the ground-truth arguments list ($A$) for each level. Now, precision, recall and f1-score for each level calculated using equations \ref{eq: precision-recall}-\ref{eq: f1}.

\begin{equation}
   \text{P}_l = \frac{\sum_{i=1}^{l} \text{NP}_{li}}{|P|},  \quad
   \text{R}_l = \frac{\sum_{i=1}^{l} \text{NA}_{li}}{|A|}
   \label{eq: precision-recall}
\end{equation}
\begin{equation}
   \text{F1}_l = \frac{2 * \text{P}_l * \text{R}_l}{\text{P}_l + \text{R}_l}
   \label{eq: f1}
\end{equation}

Similar to previous approaches \cite{}, the calculated performance under level-2 (high-threshold relaxed match) and level-3 (complex match) can be overestimated. The relaxed matching model or the complex matching judge can incorrectly consider a non-match pair as a match. To get an estimate how much level-2 and level-3 scores deviate from the original performance, we conduct a human evaluation. 

We calculate the error rate of a model from the number of disagreement between the model and human. The error rate of a model (m) on dataset (d) is calculated using equation \ref{eq: error-rate}. 

\begin{equation}
    \text{E}_{(m,d)} = \frac{\text{N}_d}{\text{N}_p}
    \label{eq: error-rate}
\end{equation}

Here, $\text{N}_d$ denotes number of disagreements between model and human expert. $\text{N}_p$ indicates the total number of observations. 

After human evaluation we will get a table like this table \ref{judgement-error-rate}. 

To account for the overestimated performance of the judge models we calculated a novel score. \textbf{Judgment Aligned Matching (JAM) Score}. This idea is to penalize the score in each level by the amount is deviate from the human judgment. This is more reliable estimate of the overall performance even if we use GPT-4 as a human proxy. 

For a dataset (D), we calculate JAMS as the following, 

\[
\text{JAM}_{p} = \frac{\text{NP}_e +(1-\text{E}_{rm})* \text{NP}_r + (1-\text{E}_{cm})* \text{NP}_c}{|P|}
\]
\[
\text{JAM}_{r} = \frac{\text{NA}_e +(1-\text{E}_{rm})* \text{NA}_r + (1-\text{E}_{cm})* \text{NA}_c}{|P|}
\]
\[
\text{JAMS}_{f1} = \frac{2 * \text{JAM}_{p} * \text{JAM}_{r}}{\text{JAM}_{p}+\text{JAM}_{r}}
\]


Generic equation for calculating \textbf{JAM score} on a dataset is, 

\begin{equation}
   \text{JAM}_p = \frac{\sum_{i=1}^{L} (1-\text{E}_{li}) * \text{NP}_{li}}{|P|}
      \label{eq: jam-precision}
\end{equation}

\begin{equation}
   \text{JAM}_r = \frac{\sum_{i=1}^{L} (1-\text{E}_{li}) * \text{NA}_{li}}{|A|}
   \label{eq: jam-recall}
\end{equation}

\begin{equation}
   \text{JAM}_{f1} = \frac{2 * \text{JAM}_p * \text{JAM}_r}{\text{JAM}_p + \text{JAM}_r}
   \label{eq: jam-f1}
\end{equation}
The JAM score provides improved correlation with human judgment for a specific dataset.

Advantage of multi-level matching and JAM score,
\begin{itemize}
    \item Multi-level framework is more reliable than only using GPT-4 as a judge because we systematically reduce the possibility of error. There will be no error in exact-match and minimal error in high-threshold relaxed matching. 

    \item It also reduces the burden of human evaluation because we will need only to check the complex match pairs where we use a judge model as a human proxy. Minium human evaluation is required for high-threshold relaxed matching. 

     \item The traditional way of calculating precision, recall, and f1-score with GPT-4 as judge overestimate model performance. Our proposed JAM score provides an improved correlation with human judgments on a dataset. This helps us to get more realistic score if use an LLM as judge model to proxy a human. 

    \item Reduce the overall evaluation cost by reducing the number of inference calls. 
\end{itemize}

 \subsection{Threshold Selection for Relaxed Match}
 %A good portion of the predicted arguments from the models contain the core word of the ground-truth arguments \cite{sharif-etal-2024-explicit, lu2024exactmatchsemanticallyreassessing}. The predictions only misses or have redundant words that do not affect the overall semantics. High threshold relaxed-match identify this variation and help to achieve accurate evaluation more reliably. We consider to arguments to be similar if their semantic similarity score exceeds 0.85 and we compute the score based on SBERT embedding \cite{}. The similarity score is determined based on the human judgments. We experimented with three threshold values which are 0.95, 0.85, and 0.75 for 500 argument pairs. The disagreement (error) rate for the arguments matcher under 0.95, 0.85, and 0.75 are 0.0, 1.78, and 8.33 respectively. Although 0.95 threshold has perfect agreement with human we can only filter a small number of arguments. On the other hand, if we select 0.75 as the threshold a lot of the arguments matched incorrectly. Therefore we selected 0.85 as threshold. Our proposed JAM score ensures that the reported scores are not overestimated because of the misjudgment.  



% \begin{table}[h!]
% \centering
% \renewcommand*{\arraystretch}{1}
% \small
% \begin{tabular}{l|ccc}
% \toprule
% \multirow{6}{*}{GPT-4o} & \textbf{DiscourseEE} & \textbf{PHEE} & \textbf{RAMS} \\ 
% \cmidrule{2-4}
% & 15.23 & 21.48 & 9.45 \\ 
% \cmidrule{2-4}
%  & \textbf{GENEVA} & \textbf{DocEE} & \textbf{WikiEvents} \\ 
% \cmidrule{2-4}
% & 16.54 & 11.97 & 8.26 \\ 
% \bottomrule
% \end{tabular}
% \caption{Judgment deviation rate of the GPT-4o judge model from humans on the evaluated EAE datasets. \textcolor{red}{[We will get updated scores when we will do error analysis on the model outputs. It will be updated after getting all results and experiments ]}}

% \end{table}



% \noindent 
% \textbf{Generative Event Argument Extraction:}
% \textit{This approach leverages generative models such as LLMs to extract arguments from the source document $D$. Given the source document, along with information about events and roles, the model generates a structured list of associated arguments $(A = [a_1, a_2, \ldots, a_{n}])$.}

% \subsection{Level-1: Exact Match}
% \label{subsection:exact-match}
% Let's assume we have a list of predicted and ground-truth argument strings for each role $R_i$ in a $D$. 
% \[
% P = [p_1, \ldots, p_{l_{x1}}], \quad
% G = [g_1, \ldots, g_{l_{y1}}] 
% \]

% An exact match (EM) pair is defined when $p_i = g_j$, forming a list of EM pairs such as \( [(p_1, g_2), (p_3, g_5), \ldots, (p_x, g_y)] \). Precision, recall, and F1-score for the EM level are computed as:

% \[
% \text{EM}_p = \frac{\text{NP}_e}{|P|}, \quad
% \text{EM}_r = \frac{\text{NG}_e}{|G|}
% \]
% \[
% \text{EM}_{f1} = \frac{2 * \text{EM}_{p} * \text{EM}_{r}}{\text{EM}_{p}+\text{EM}_{r}}
% \]

% Here, \(\text{NP}_e\) and \(\text{NG}_e\) represent the number of correctly predicted arguments from the predicted (\(P\)) and ground-truth (\(G\)) argument lists, respectively. Note that \(\text{NP}_e = \text{NG}_e\) under exact match. 


% \subsection{Level 2: Relaxed Match}
% \label{subsection:relaxed-match}


% We update the predicted and ground-truth argument lists by removing arguments matched in the previous level. 
% \[
% P'_{rm} = [p'_1, \ldots, p'_{l_{x2}}], \quad
% G'_{rm} = [g'_1, \ldots, g'_{l_{y2}}] 
% \]

% We compute the semantic similarity for all possible argument pairs $[(p'_1, g'_1), \ldots (p'_1, g'_{l_{y2}}), \dots (p'_{l_{x2}}, g'_{l_{y2}})]$. A pair \((p'_x, g'_y)\) is considered a relaxed match (RM) if its embedding-based similarity score exceeds the predefined threshold \(T_r\). The threshold selected method is described in section \ref{threshold-selection-process}.  The resulting list of RM pairs ($[(p'_1, a'_3), (p'_3, a'_4), \ldots, (p'_x, a'_y)]$) is a subset from all possible pairs. Precision, recall, and F1-score for relaxed matching are computed as:
% \[
% \text{RM}_{p} = \frac{\text{NP}_e + \text{NP}_r}{|P|}, \quad
% \text{RM}_{r} = \frac{\text{NG}_e + \text{NG}_r}{|G|}
% \]
% % \[
% % \text{RM}_{f1} = \frac{2 * \text{RM}_{p} * \text{RM}_{r}}{\text{RM}_{p}+\text{RM}_{r}}
% % \]

% Here, \(\text{NP}_r\) and \(\text{NG}_r\) represent the arguments matched under relaxed conditions form $P'_{rm}$ and $G'_{rm}$, while \(\text{NP}_e\) and \(\text{NA}_e\) are taken from Level 1. Relaxed matching allows an argument (\(p'_x\) or \(g'_y\)) to appear in multiple pairs where the similarity exceeds \(T_r\). To avoid overcounting, separate counts ($\text{NP}_r$, $\text{NG}_r$) are maintained for the number of arguments correctly matched from the prediction list and the ground-truth list.

% \subsection{Level 3: Complex Match}
% \label{subsection:complex-match}
% After exact and relaxed matching, unmatched arguments are carried forward for complex matching. Arguments matched in previous levels are removed from the updated predicted and ground-truth lists:
% \[
% P''_{cm} = [p''_1, \ldots, p''_{l_{x3}}], \quad
% G''_{cm} = [g''_1, \ldots, g''_{l_{y3}}] 
% \]

% For all possible pairs from these lists, a preselected judge model determines similarity based on context. Details on how a judge model is selected for complex matching are discussed in section \ref{section: judge-selection}. If a pair  \((p''_x, g''_y)\) predicted similar it is added to the complex match (CM) pair list: $[(p''_1, g''_2), \ldots, (p''_x, g''_y)]$. Precision, recall, and F1-score for complex matching are computed as:
% \[
% \text{CM}_{p} = \frac{\text{NP}_e + \text{NP}_r + \text{NP}_c}{|P|}
% \]
% \[
% \text{CM}_{r} = \frac{\text{NG}_e + \text{NG}_r + \text{NG}_c}{|G|}
% \]
% \[
% \text{CM}_{f1} = \frac{2 * \text{CM}_{p} * \text{CM}_{r}}{\text{CM}_{p}+\text{CM}_{r}}
% \]

% Here, \(\text{NP}_c\) and \(\text{NG}_c\) represent arguments correctly matched by the judge model. Similar to relaxed matching, separate counts ensure that arguments are not overcounted when they appear in multiple matches. \(\text{NP}_e\), \(\text{NG}_e\), \(\text{NP}_r\), and \(\text{NG}_r\) are precomputed values from previous levels. 

% \noindent
% \textbf{Generic Equation:}
% We define three levels of matching \(L = [\text{EM}, \text{RM}, \text{CM}]\). At each level, \(\text{NP}_{e}\), \(\text{NP}_{r}\), and \(\text{NP}_{c}\) represent the number of correctly predicted arguments from the prediction list (\(P\)), while \(\text{NG}_{e}\), \(\text{NG}_{r}\), and \(\text{NG}_{c}\) represent the number of correctly matched arguments from the ground-truth list (\(G\)). The precision, recall, and F1-score for a level are calculated using the equations \ref{eq: precision-recall}-\ref{eq: f1}.

% \begin{equation}
%    \text{P}_l = \frac{\sum_{i=1}^{l} \text{NP}_{li}}{|P|},  \quad
%    \text{R}_l = \frac{\sum_{i=1}^{l} \text{NG}_{li}}{|G|}
%    \label{eq: precision-recall}
% \end{equation}
% \begin{equation}
%    \text{F1}_l = \frac{2 * \text{P}_l * \text{R}_l}{\text{P}_l + \text{R}_l}
%    \label{eq: f1}
% \end{equation}

% \subsection{Alignment with Human Judgments}
% \label{subsection:judgement-alignment}

% In Levels 2 (Relaxed Match) and 3 (Complex Match), the performance can be overestimated if the relaxed matching model or the complex match judge incorrectly classifies a non-match pair as a match. To account for this overestimation, we introduce a novel \textbf{Judgment Aligned Match (JAM)} Score, which penalizes the counts on each level based on the deviation from human judgment. This adjustment offers a more reliable estimate of the model’s performance when we use LLMs as a judge model.

% We first calculate the \textit{deviation rate} of a model $(M)$ on a dataset $(DT)$ by measuring the number of disagreements between the judge model and the human evaluator. The deviation rate is computed using equation \ref{eq: deviation-rate}. 

% \begin{equation}
%     \text{E}_{(M, DT)} = \frac{\text{N}_d}{\text{N}_o}
%     \label{eq: deviation-rate}
% \end{equation}

% Here, $\text{N}_d$ and $\text{N}_o$ denote the number of disagreements and the total number of observations, respectively. We calculate the \textbf{JAM Score} for a dataset factoring the model’s score at each matching level (EM, RM, CM) by the deviation rate:

% \begin{equation}
%    \text{JAM}_p = \frac{\sum_{i=1}^{L} (1-\text{E}_{li}) * \text{NP}_{li}}{|P|}
%       \label{eq: jam-precision}
% \end{equation}

% \begin{equation}
%    \text{JAM}_r = \frac{\sum_{i=1}^{L} (1-\text{E}_{li}) * \text{NG}_{li}}{|G|}
%    \label{eq: jam-recall}
% \end{equation}

% \begin{equation}
%    \text{JAM}_{f1} = \frac{2 * \text{JAM}_p * \text{JAM}_r}{\text{JAM}_p + \text{JAM}_r}
%    \label{eq: jam-f1}
% \end{equation}

% The \textbf{JAM Score} improves the alignment with human judgment, providing a more accurate reflection of the model’s true performance. 
