\section{REGen Implementation Details}
\subsection{Threshold Selection for Relaxed Match}
\label{threshold-selection-process}
Usually, the model-predicted arguments contain the core words from the corresponding ground-truth arguments \cite{sharif-etal-2024-explicit, lu2024exactmatchsemanticallyreassessing}. While these predictions may have redundant words or miss some surrounding words, such discrepancies do not alter the overall semantics. We can identify these variations by using a high threshold relaxed match for accurate evaluation. We consider two arguments similar if their semantic similarity score exceeds 0.85, calculated using SBERT embeddings \cite{reimers-gurevych-2019-sentence}. 

This threshold is determined as follows. We tested three thresholds: 0.95, 0.85, and 0.75, across 500 argument pairs sourced from the six EAE datasets evaluated. The disagreement (error) rates were 0.0\%, 1.78\%, and 8.33\% for these thresholds, respectively. Although the 0.95 threshold yielded perfect agreement with human assessments, it allowed us to filter only a limited number of arguments. Conversely, the 0.75 threshold led to many incorrect matches. Therefore, we selected 0.85 as the optimal threshold. Our judgment alignment step ensures that our results are reliable and not inflated due to misjudgments.
 
\subsection{Judge Selection for Complex Match}
\label{section: judge-selection}
Studies show that LLMs achieve a strong correlation with human judgment across various tasks \cite{fu-etal-2024-gptscore,liu-etal-2023-g}. We also used LLMs to determine whether the ground truth and predicted arguments match. This approach makes the evaluation scalable across datasets and models. 

\textbf{Judge data annotation:} We construct a \textit{judge dataset comprising 900 argument pairs} to select the best judge model. Specifically, we randomly select 150 pairs not matched under exact or relaxed criteria from each of the six datasets evaluated. Each pair is annotated as \textit{`match'} or \textit{`non-match'} by a human annotator. A second human verifies the labels, and disagreements are resolved through discussion to finalize the annotations.


\textbf{Judge LLM selection:} We evaluate both open-source (Llama3.1-70B) and closed-source (GPT-4o, GPT-4o-mini, GPT-3.5) models as potential judges, assessing their performance in \textit{zero-shot} and \textit{chain-of-thought} settings. GPT-4o, with a zero-shot prompt, achieves the highest agreement with human judgments, scoring 86.17. Therefore, we selected GPT-4o as the judge model for complex match evaluation. Note that the choice of judge is orthogonal to our proposed framework. The selected judge model can easily be swapped with newer or better alternatives without further modifications. Appendix \ref{appendix: judge-selection} provides additional details on judge selection and relevant prompts. 

\subsection{Judgment Alignment}
\begin{table}[h!]
\centering
\renewcommand*{\arraystretch}{0.9}
\small
\begin{tabular}{l|ccc|C{2.1cm}}
& \multicolumn{3}{c}{Deviation Rate (\%)}& Alignment (\%)\\

Datasets & EM & RM & CM & (1-$\sum$ deviation)\\
\midrule
DiscourseEE & 0.0 & 2.67 & 13.33 & 84.0 \\
PHEE &0.0 & 0.0 & 7.33 & 92.67\\
RAMS & 0.0 & 1.33 & 8.66 & 90.0\\
GENEVA & 0.0 & 2.0 & 8.0 & 90.0\\
DocEE & 0.0 & 3.33 & 16.0 & 80.67\\
WikiEvents &0.0 & 0.0 & 11.33 &  88.67\\
\midrule
&\multicolumn{3}{c}{Avg. Alignment (\%) } & 87.67\\
\bottomrule
\end{tabular}
\caption{Alignment and judgment deviation rate from humans at different matching levels on the evaluated EAE datasets.}
\label{judgement-error-rate}
\end{table}

We manually evaluated a subset of predictions from each matching level to determine the alignment with human judgments. In total, we analyzed 2,700 arguments (900 for each level) to quantify the frequency of disagreements with humans. To ensure unbiased judgments, we randomly selected 150 outputs from each level for each evaluated dataset. Table \ref{judgement-error-rate} presents the alignment and deviation rates. The EM consistently showed perfect alignment with human judgments, while RM exhibited minimal disagreement. However, CM demonstrated the highest deviation rates. 

Among the datasets, the PHEE dataset showed the highest alignment (92.67\%) with human judgments, while DocEE had the lowest (80.67\%). On average, the \textbf{REGen framework achieved 87.67\% alignment with human evaluators} across all six datasets and matching levels. Our analysis reveals primary reasons for judgment disagreements are (1) \textit{Numerical nuances:} the model often failed to distinguish numerical differences. Such as for a role `\texttt{drug-dosage},' it incorrectly treated \texttt{`14 mg'} and \texttt{`6 mg'} as equivalent. (2) \textit{Temporal variations:} dates such as \texttt{`18 April'} versus \texttt{`20 April'} or days like \texttt{`Thursday'} versus \texttt{`Friday'} were incorrectly judged as similar. (3) \textit{Coreference handling:} datasets like RAMS and WikiEvents frequently used pronouns (e.g., `he', `they') in the ground truth, while models predicted specific names (e.g., `John'). This mismatch led to judgment errors, especially when documents contained multiple names, confusing the model.


The proposed JAM score accounts for these judgment errors. The score for each dataset is calculated based on alignment, providing a more reliable estimate of a model's true performance when using relaxed matching and LLM as judge models instead of human evaluators.


%Selecting a judge model that perfectly aligns with human judgments is challenging. Nonetheless, human evaluation remains the most reliable standard. The judge model's reported performance becomes less reliable due to this potential disagreement. Performance may be inflated when the judge model incorrectly classifies a \textit{`non-match'} argument pair as a \textit{match}. This overestimation can also occur in relaxed matching, where a \textit{`non-match'} pair has a higher similarity than the set threshold. The proposed JAM score accounts for this overestimation incorporating the deviation from human judgments. 


% \begin{table}[h!]
% \centering
% \renewcommand*{\arraystretch}{0.95}
% \small
% \begin{tabular}{l|cccccc}
% Datasets & Dis & P & R & G & Doc & W \\
% \toprule
% EM & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\

% \end{tabular}
% \caption{Judgment deviation rate from human evaluators at different matching levels on the evaluated EAE datasets. \textcolor{red}{[We will get updated scores when we will do error analysis on the model outputs. It will be updated after getting all results and experiments ]}}
% \label{judgement-error-rate}
% \end{table}

% \begin{table}[h!]
% \centering
% \renewcommand*{\arraystretch}{0.95}
% \small
% \begin{tabular}{l|cccccc}
% Datasets & Di & Ph & R & G & Do & W \\
% \toprule
% EM & 0.0 & 0.0 & 0.0 & 0.0 &0.0  & 0.0  \\
% RM &2.67 & 0.0 & 1.33 & 2.0 & 3.33 & 0.0 \\
% CM & 13.33  & 7.33 & 8.66 & 8.0 & 16.0 & 11.33 \\
% \bottomrule
% \end{tabular}
% \caption{Judgment deviation rate from humans at different matching levels on the evaluated EAE datasets. Here, Di, P, R, G, Do, W indicates DiscourseEE, PHEE, RAMS, GENEVA, DocEE, and Wikievents datasets, rexpectively.}
% \label{judgement-error-rate}
% \end{table}