\section{Limitations}
One limitation of this work is that we did not conduct statistical significance testing on the reported results.  We chose not to conduct statistical testing for two reasons. First, our goal is not to conclude which model is best but to highlight performance gaps and show how existing evaluation approaches underestimate model performance. The results clearly demonstrate a significant performance gap with exact match evaluation, which is not diminished by the lack of statistical testing. Second, performing statistical tests across all datasets and models with multiple runs is time-consuming and prohibitively expensive. For example, averaging over 3 runs would require an additional 320k inferences.

Another limitation is that we did not optimize prompts for each model. Performance could be improved with dataset- and model-specific prompting. However, we chose to focus on benchmarking a wide range of datasets using model-agnostic prompting. Conducting a thorough, prompt engineering for every model and dataset in a single study is not feasible. Our results show significant performance gains and future work can explore dataset- and model-specific prompts to further enhance performance. Additionally, future work can explore few-shot experiments and find optimal prompting strategies for different datasets, such as self-consistency \cite{wang2023selfconsistencyimproveschainthought} or plan-and-solve \cite{wang-etal-2023-plan}. We exclude few-shot experiments as they require selecting demonstration examples through trial and error, finding the optimal order of demonstration, and running multiple iterations, which significantly increases the experimental cost and complexity.

% In future work, we will explore these techniques and evaluate their effectiveness under the REGen framework.



%\textcolor{blue}{talk about few shot as future work, as we do zero-shot and CoT.}


%Since LLM predictions can vary across runs, the reported scores may fluctuate slightly.

% \section{Limitations / Potential Reviewer Questions}

% \textbf{Q:} Why the authors have not performed statistical analysis in this work.  

% \textbf{A:} Regarding statistical significance, we acknowledge its value but believe it is not a top priority in our evaluation for the following reasons: Our primary goal is not to conclude which model is best. The emphasis of this work is on revealing performance gaps and showing how existing evaluation approaches underestimate model performance, which is not diminished by the lack of statistical testing. Moreover, performing statistical tests for all the datasets across the models (through multiple runs) is infeasible. For example, if we take an average of 3 runs, we have to make 200k more extra inferences, which is prohibitively expensive and time-consuming. Therefore, reporting statistical significance is not critical, and we believe that this point should not be a decisive issue for evaluating our contribution. 

% \textbf{Q:} Why have authors not tried few-shot or chain-of-thought approaches with the LLMs in this work? 

% \textbf{A:} Thanks for your question. In this work, we focus on systematically showing that the existing evaluation approach (e.g., exact-match) underestimates generative models' performance. Therefore, we focus of benchmarking a wide range of generative model than get the best results from the models. For LLMs, we select a zero-shot prompt and evaluate all the models with the same prompt to be fair with all the models. For example, for few-shot or chain-of-thought experiments, we have to find the optimal prompt, demonstration example, and explanations via a trial-and-error approach. Conducting thorough, prompt engineering and reporting results for every model and dataset we benchmarked in a single study is not feasible. %Moreover, not doing chain-of-thought or few-shot prompting do not reduce the reliability or applicability of our framework. 
% What are the problems with few-shot and chain-of-thought.


% \textbf{Q:} How have you selected the LLM models for evaluation?  

% \textbf{A:}  In selecting the models for evaluation, we considered a diverse range of both open-source and closed-source models, as they have been widely utilized in previous studies related to genrative event argument extraction \cite{sharif-etal-2024-explicit,lu2024exactmatchsemanticallyreassessing}. To thoroughly investigate the feasibility of our multi-level evaluation framework, we experimented with models having parameter sizes ranging from 4 billion to 70 billion. This variety allows us to assess performance across different scales. 

% \textbf{Q: Why not the authors compared the extractive models under this framework. } 

% \textbf{Q: Why not talk about event detection part. }