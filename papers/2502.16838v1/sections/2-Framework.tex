\section{REGen Framework}
\label{mlm-eval-framework}
\begin{figure*}[t!]
  \centering
  \includegraphics[width =0.94\linewidth]{figures/EAE-Eval-Framework-with-example.png}
 \caption{Proposed REGen evaluation framework for event argument extraction. \textbf{\textit{Left:}} An example of getting role-specific arguments from documents using generative models. Different colors indicate arguments for different roles. Semicolons separate multiple arguments for a role.  \textbf{\textit{Right:}} Illustration of the REGen's sequential evaluation process: Exact Match (\ref{subsection:exact-match}), Relaxed Match (\ref{subsection:relaxed-match}), and Complex Match (\ref{subsection:complex-match}) and Alignment with Human Judgments (\ref{subsection:judgement-alignment}). Only the arguments that do not match at the previous level are carried forward to the next level. Due to space constraints, the mathematical illustration of the framework is provided in the Appendix Figure \ref{eval-framework}.}
 \label{eval-framework-example}
\end{figure*}
% with corresponding ground-truth arguments
\subsection{Preliminaries}
\label{dataset-formatting}
\textbf{Document:} A document $D$ is a piece of text, which can be a sentence, a paragraph, or a full document.

\noindent
\textbf{Events, Roles and Arguments:} Events $(E)$ refer to occurrences or actions described in document $D$. A document can have multiple events. Each event is characterized by its roles $(R)$, which define the participants or entities involved. Arguments $(A)$ are specific details or attributes associated with these roles, providing context such as specific time, location, and other information. For example, consider the sentence: \textit{`Alice sent a package to Bob on Monday'}. The event here is \texttt{`Send'}, with potential roles such as sender, recipient, and time. The corresponding arguments for these roles are Alice, Bob, and Monday, respectively. 

\noindent 
\textbf{Generative Event Argument Extraction:}
This approach leverages generative models such as LLMs to extract arguments from the source document $D$. Given the source document, along with information about events and roles, the model generates a structured list of associated arguments.% $(A = [a_1, a_2, \ldots, a_{n}])$.
\subsection{Level-1: Exact Match}
\label{subsection:exact-match}
Let's assume we have a list of predicted and ground-truth argument strings for each role $R_i$ in a $D$. 
\[
P = [p_1, \ldots, p_{x1}], \quad
G = [g_1, \ldots, g_{y1}] 
\]
An exact match (EM) pair is defined when $p_i = g_j$, forming a list of EM pairs such as \( [(p_1, g_2), (p_3, g_5), \ldots, (p_x, g_y)] \). Precision and recall for the EM level are computed as:

{
\small
\[
\text{EM}_p = \frac{\text{NP}_e}{|P|}, \quad
\text{EM}_r = \frac{\text{NG}_e}{|G|}
\]
}
% \[
% \text{EM}_{f1} = \frac{2 * \text{EM}_{p} * \text{EM}_{r}}{\text{EM}_{p}+\text{EM}_{r}}
% \]
Here, \(\text{NP}_e\) and \(\text{NG}_e\) represent the number of correctly predicted arguments from the predicted (\(P\)) and ground-truth (\(G\)) argument lists, respectively. Note that \(\text{NP}_e = \text{NG}_e\) under exact match. 

\subsection{Level 2: Relaxed Match}
\label{subsection:relaxed-match}
Predicted and ground-truth argument lists are updated by removing arguments matched in Level-1. 
\[
P_{rm} = [p_1, \ldots, p_{x2}], \quad
G_{rm} = [g_1, \ldots, g_{y2}] 
\]
%$[(p'_1, g'_1), \dots (p'_{l_{x2}}, g'_{l_{y2}})]$
We compute the embedding-based similarity for all possible argument pairs of $P_{rm}$ and $G_{rm}$. A pair is considered a relaxed match (RM) if its similarity score exceeds the predefined threshold \(T_r\). The threshold selection method is described in section \ref{threshold-selection-process}.  The resulting list of RM pairs is a subset of all possible pairs. Precision and recall for relaxed matching are computed as:

{\small
\[
\text{RM}_{p} = \frac{\text{NP}_e + \text{NP}_r}{|P|}, \quad
\text{RM}_{r} = \frac{\text{NG}_e + \text{NG}_r}{|G|}
\]
}
% \[
% \text{RM}_{f1} = \frac{2 * \text{RM}_{p} * \text{RM}_{r}}{\text{RM}_{p}+\text{RM}_{r}}
% \]
Here, \(\text{NP}_r\) and \(\text{NG}_r\) represent the arguments matched under relaxed conditions form $P_{rm}$ and $G_{rm}$, while \(\text{NP}_e\) and \(\text{NA}_e\) are taken from Level 1. Relaxed matching allows an argument (\(p_x\) or \(g_y\)) to appear in multiple pairs where the similarity exceeds \(T_r\). To avoid overcounting, separate counts ($\text{NP}_r$, $\text{NG}_r$) are maintained for the number of arguments correctly matched from the prediction list and the ground-truth list.

\subsection{Level 3: Complex Match}
\label{subsection:complex-match}
After exact and relaxed matching, unmatched arguments are carried forward for complex matching. 
\[
P_{cm} = [p_1, \ldots, p_{x3}], \quad
G_{cm} = [g_1, \ldots, g_{y3}] 
\]
For the possible pairs from these lists, a preselected judge model determines similarity based on context. Details on how a judge model is selected for complex matching are discussed in section \ref{section: judge-selection}. If a pair is predicted as similar, it is added to the complex match (CM) pair list. Precision and recall for complex matching are computed as:


{\small
\[
\text{CM}_{p} = \frac{\text{NP}_e + \text{NP}_r + \text{NP}_c}{|P|}, \quad
\text{CM}_{r} = \frac{\text{NG}_e + \text{NG}_r + \text{NG}_c}{|G|}
\]
}
% \[
% \text{CM}_{f1} = \frac{2 * \text{CM}_{p} * \text{CM}_{r}}{\text{CM}_{p}+\text{CM}_{r}}
% \]
Here, \(\text{NP}_c\) and \(\text{NG}_c\) represent arguments correctly matched by the judge model. Similar to relaxed match, separate counts ensure that arguments are not overcounted when they appear in multiple matches. \(\text{NP}_e\), \(\text{NG}_e\), \(\text{NP}_r\), and \(\text{NG}_r\) are precomputed values from previous levels. 

\textbf{Generic Equation:}
We define three match levels \(L = [\text{EM}, \text{RM}, \text{CM}]\). \(\text{NP}_{e}\), \(\text{NP}_{r}\), and \(\text{NP}_{c}\) denote the number of correctly predicted arguments from the prediction list (\(P\)), while \(\text{NG}_{e}\), \(\text{NG}_{r}\), and \(\text{NG}_{c}\) represent the number of correctly matched arguments from the ground-truth list (\(G\)) at each level. Finally, precision, recall, and F1-score for a given level are calculated using the Equations \ref{eq: precision-recall}-\ref{eq: f1}.

{
\small
\begin{equation}
   \text{P}_l = \frac{\sum_{i=1}^{l} \text{NP}_{i}}{|P|},  \quad
   \text{R}_l = \frac{\sum_{i=1}^{l} \text{NG}_{i}}{|G|}
   \label{eq: precision-recall}
\end{equation}
\begin{equation}
   \text{F1}_l = \frac{2 * \text{P}_l * \text{R}_l}{\text{P}_l + \text{R}_l}
   \label{eq: f1}
\end{equation}
}
\subsection{Alignment with Human Judgments}
\label{subsection:judgement-alignment}

In Levels 2 (Relaxed Match) and 3 (Complex Match), the performance can be overestimated if the relaxed matching model or the complex match judge incorrectly classifies a non-match pair as a match. To account for this overestimation, we introduced a novel \textbf{Judgment Aligned Match (JAM)} Score, which penalizes the counts on each level based on the deviation from human judgment. %This adjustment offers a more reliable estimate of the model’s performance when we use LLMs as a judge model.

We first calculate the \textit{deviation rate} of a matching model $(M)$ on a dataset $(DT)$ by measuring the number of disagreements between the model and the human evaluator. The deviation rate is computed using equation \ref{eq: deviation-rate}. 

\begin{equation}
    \text{E}_{(M, DT)} = \frac{\text{N}_d}{\text{N}_o}
    \label{eq: deviation-rate}
\end{equation}

Here, $\text{N}_d$ and $\text{N}_o$ denote the number of disagreements and the total number of observations, respectively. We calculate the \textbf{JAM Score} for a dataset factoring the model’s score at each matching level (EM, RM, CM) by the deviation rate of that level following equations \ref{eq: jam-precision}-\ref{eq: jam-f1}.

{\small
\begin{equation}
   \text{JAM}_p = \frac{\sum_{i=1}^{L} ((1-\text{E}_{i}) * \text{NP}_{i})}{|P|}
      \label{eq: jam-precision}
\end{equation}

\begin{equation}
   \text{JAM}_r = \frac{\sum_{i=1}^{L} ((1-\text{E}_{i}) * \text{NG}_{i})}{|G|}
   \label{eq: jam-recall}
\end{equation}

\begin{equation}
   \text{JAM}_{f1} = \frac{2 * \text{JAM}_p * \text{JAM}_r}{\text{JAM}_p + \text{JAM}_r}
   \label{eq: jam-f1}
\end{equation}
}

The JAM Score improves the alignment with human judgment, providing a more reliable reflection of the model’s true performance. 
