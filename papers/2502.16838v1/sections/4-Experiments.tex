\section{Experiments}
\subsection{Datasets and Experimental Setup}
We used six standard EAE datasets from diverse domains to evaluate REGen. These datasets include: \textbf{RAMS} \cite{ebner-etal-2020-multi} (news), \textbf{GENEVA} \cite{parekh-etal-2023-geneva} (book, news, journal articles), \textbf{DocEE} \cite{tong-etal-2022-docee} (long news documents), \textbf{WikiEvents} \cite{li-etal-2021-document} (Wikipedia texts), \textbf{DiscourseEE} \cite{sharif-etal-2024-explicit} (online health discourse), and \textbf{PHEE} \cite{sun-etal-2022-phee} (pharmacovigilance texts). Prior works, such as \citet{huang-etal-2024-textee} and \citet{lu2024exactmatchsemanticallyreassessing} have evaluated LLMs using small test subsets sampled and merged from multiple datasets. Thus not reflecting actual performance of LLMs on these datasets. We conduct evaluations using the complete official test sets of the selected datasets to provide a more reliable assessment of LLMs' performance on these benchmarks. Detailed statistics for these test datasets are presented in Table \ref{data-statistics}. Appendix \ref{appendix: data-info} contains additional details on the data preparation steps.
%The same test set is used across all the models to ensure unbiased evaluation. 

\noindent
\textbf{Performance Metrics:} We report the precision, recall, and F1-score at each evaluation phase: exact match, relaxed match, complex match, and post-judgment alignment. Scores are computed following prior works \cite{peng-etal-2023-devil} and calculation details are discussed in Section \ref{mlm-eval-framework}.

\subsection{EAE Models}
\textbf{Baselines:} Following prior works \cite{sharif-etal-2024-explicit, lu-etal-2023-event}, we implement question-answering-based baselines. We use two models: BERT and FLAN-T5. Both models are fine-tuned on SQuAD \cite{rajpurkar-etal-2016-squad} data to extract \texttt{arguments} from \texttt{context} based on the \texttt{question}.  

\noindent
\textbf{LLM Based Models:} We perform comprehensive experiments using open-source and closed-source LLMs from different model families of various parameter sizes, including Phi-3.5 (3.8B), Gemma-1.1 (7B), Mixtral (8x7B), Llama-3.1 (70B), and GPT-4o. We evaluate all the models in two prompt settings: \textit{zero-shot} and \textit{chain-of-thought}. We employed question-guided prompting as previous works achieved SOTA performance using this approach \cite{lu-etal-2023-event,hsu-etal-2022-degree,du-cardie-2020-event}. Specifically, models are prompted with \texttt{(Instruction, Document, Question)} to generate  $\rightarrow$ \texttt{(Arguments)}, where each question is tailored to extract specific role\footnote{Role-specific questions for each dataset can be found here: \href{https://tinyurl.com/48y2wus3}{https://tinyurl.com/48y2wus3}}. Sample questions for the datasets are presented in Table \ref{role-specific-question-details}. 

Different LLMs require prompts and in-context samples tailored to each model and dataset. In practice, users select the optimal prompt using a trial-and-error approach \cite{ziems-etal-2024-large, 10.1145/3544548.3581388}. However, in our experiments, iterating over various prompts to find the optimal prompt for each model and dataset is impractical. Instead, we opted to use a consistent prompt across all models and datasets to (i) ensure a fair comparison among the models and (ii) eliminate the confounding factors related to prompt optimization.  Generic templates for zero-shot and chain-of-thought prompts for argument extraction are illustrated in Figures \ref{zs-EAE-prompt} and \ref{cot-EAE-prompt}, respectively. Additional descriptions of the models are provided in Appendix \ref{appendix: models}.


%We exclude few-shots and other prompt settings to reduce experimental complexity, noting that this can be explored in future work.
%\textcolor{blue}{[mention few shot as immediate future work as we could leverage insights from zero and CoT to design the few-shot experiments?]} {added in the limitations}