\section{Related Work}
\textbf{Generative Event Argument Extraction:} 
Early studies on event argument extraction (EAE) treated it as an extractive or token-level classification task \cite{doddington-etal-2004-automatic, du-cardie-2020-event}. These efforts primarily focused on identifying argument spans directly found in the text \cite{sun-etal-2022-phee}. Recently, EAE has been formulated as a generative task where pre-trained language models are guided with natural language to fill templates or generate arguments \cite{hsu-etal-2022-degree}. \citet{sharif-etal-2024-explicit} argue that this generative formulation better suits real-world applications as it can capture implicit and scattered arguments better. With the emergence of LLMs,  generative model-based argument extraction gained more traction \cite{sun-etal-2024-leveraging, he-etal-2024-demonstration}. So, we focus on generative extraction covering diverse models and datasets. 

%where the predicted argument must exactly match the ground-truth span
\noindent
\textbf{Evaluations for Generative EAE:} Existing works for generative EAE primarily rely on \textit{exact matching} for evaluation \cite{huang-etal-2024-textee}. This strict approach unfairly penalizes models, even when the generated output is correct. To address this, \citet{han2024empiricalstudyinformationextraction} adopt a relaxed matching approach, considering arguments similar if their embedding-based similarly exceeds a threshold of 0.5. Similarly, \citet{sharif-etal-2024-explicit} used a threshold of 0.75. However, this approach has limitations. It fails to capture semantically similar arguments with different lexical or syntactic forms and wrongly classifies arguments with high token overlap as similar. Thus, performance reported solely on relaxed matching is unreliable. More recently, 
\citet{lu2024exactmatchsemanticallyreassessing} employed LLMs to determine argument similarity. Nonetheless, this approach incurs significant computational overhead and demands extensive human validation. Our REGen framework combines the strengths of exact, relaxed, and LLM-based matching. It systematically reduces misjudgments, computational costs, and the need for human validation. 
% Finally, the judgment alignment component ensures reliable performance evaluation. 