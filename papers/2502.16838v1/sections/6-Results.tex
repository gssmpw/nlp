\section{Results}
\begin{table}[t!]
\centering
\renewcommand*{\arraystretch}{0.9}
\setlength{\tabcolsep}{4pt}
\small
\begin{tabular}{l|C{1cm}C{1.5cm}cC{1.2cm}}
Datasets& Avg. EM-F1 & Avg. REGen-F1 & $\Delta$ F1  &Gain (\%) \\
\toprule
DiscourseEE & 10.74 & 37.45 & +26.71 & +248.69 \\
PHEE       & 39.26 & 62.34 & +23.08 & +58.79 \\
RAMS       & 13.38 & 28.27 & +14.89 & +111.25 \\
GENEVA     & 13.62 & 46.12 & +32.49 & +238.52 \\
DocEE      & 17.33 & 41.65 & +24.32 & +140.36 \\
WikiEvents & 8.93  & 31.02 & +22.08 & +247.18 \\
\midrule
&\multicolumn{2}{c}{Avg. $\Delta$ F1}& +23.93 &\\
\bottomrule
\end{tabular}
\caption{Comparison of average F1-scores of the LLMs between Exact Match (EM) and REGen evaluation frameworks.} 
\label{average-improvement-rate}
\end{table}

\begin{table*}[t!]
\centering
\renewcommand*{\arraystretch}{1}
\small
\begin{tabular}{l|ccccC{1.5cm}C{1.3cm}c}
Datasets& \#Events & \#Roles & \#Docs  &  \#Arguments &  Doc-length (words) & Argument Density & Domain \\
\toprule
DiscourseEE & 3 & 34 & 98  & 997 & 121.21 & 10.17 &  Online health discourse\\
PHEE & 2 & 14 & 968  & 4952 & 20.12 & 5.11 & Pharmacovigilance \\
RAMS & 129 & 63 & 754  & 2023 & 133.70 & 2.68 & News \\
GENEVA & 115 & 196 & 899  & 3078 & 29.74 & 3.42 & General (book, news, journal)\\
DocEE & 57 & 266 & 500  & 3453 & 635.60 & 6.90 & News\\
WikiEvents & 33 & 44 & 19 & 473 & 653.87 & 24.89 &  Wikipedia\\
\bottomrule
\end{tabular}
\caption{Test set statistics of the six datasets used for evaluation show broad variability among these datasets. The columns \#Events, \#Roles, \#Docs, and \#Args represent the number of unique event types, unique role types, unique documents, and number of arguments, respectively. The average document length is measured in words, and argument density reflects the average number of arguments per document. } 
\label{data-statistics}
\end{table*}


\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{l|cccc|cccc|cccc}
& \multicolumn{4}{c}{\textbf{DiscourseEE}} & \multicolumn{4}{c}{\textbf{PHEE}} & \multicolumn{4}{c}{\textbf{RAMS}} \\
\midrule
\textbf{Model}& EM & RM & CM & JAM & EM & RM & CM & JAM & EM & RM & CM & JAM \\
\midrule
\multicolumn{13}{c}{\textit{Baselines }}\\
\midrule
BERT & 5.88 & 8.66 & 33.56 & 30.18 & 27.78 & 34.98 & 52.61 & 51.33& 14.63 & 18.14 & 33.61 & 32.24 \\
Flan-T5 & 6.74 & 10.16 & 36.46 & 32.87 & 42.34 & 50.44 & 66.98 & 65.77 & 12.61 & 15.13 & 28.62 & 27.43\\
\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5  & 3.40 & 5.00 & 14.73 & 13.39 & 43.03 & 50.46 & 67.67 & 66.42 & 15.34 & 17.92 & 34.19 & 32.76 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 11.87 & 15.86 & \cellcolor{green!50} 50.14 & \cellcolor{green!15} 45.48 & 45.00 & 54.34 & \cellcolor{green!15} 76.93 & \cellcolor{green!15} 75.28 & 14.87 & 17.50 & 32.43 & 31.11 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 13.10 & 17.74 & 48.59 & 44.38 & 36.58 & 42.55 & 59.19 & 57.98 & 12.97 & 15.46 & 29.93 & 28.65\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1  & 13.38 & 18.73 & 43.57 & 40.13 & 39.17 & 46.95 & 63.96 & 62.72 & 11.95 & 14.56 & 25.44 & 24.47  \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o  & \cellcolor{green!50} 16.82 & \cellcolor{green!50} 23.08 & \cellcolor{green!15} 49.87 & \cellcolor{green!50} 46.16 & \cellcolor{green!50} 53.67 & \cellcolor{green!50} 61.92 & \cellcolor{green!50}78.96 &\cellcolor{green!50} 77.72 & \cellcolor{green!50}19.44 & \cellcolor{green!50} 23.15 & \cellcolor{green!50} 37.42 & \cellcolor{green!50} 36.15 \\


\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5  & 7.08 & 12.42 & 41.41 & 37.43 & 32.09 & 38.01 & 54.03 & 52.86 & \cellcolor{green!15} 15.53 & 18.65 & \cellcolor{green!15} 34.54 & \cellcolor{green!15} 33.13 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 9.35 & 13.06 & 43.27 & 39.16 & 34.14 & 42.28 & 61.46 & 60.06 & 10.71 & 13.57 & 26.28 & 25.15\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 4.99 & 7.00 & 26.39 & 23.76 & 29.46 & 37.28 & 50.75 & 49.77 & 6.85 & 8.28 & 17.00 & 16.23 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1  & 12.65 & 17.31 & 44.75 & 40.98 & 31.29 & 39.93 & 52.51 & 51.59 & 10.66 & 12.76 & 23.72 & 22.75\\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o  & \cellcolor{green!15} 14.77 & \cellcolor{green!15} 20.86 & 47.33 & 43.66 & \cellcolor{green!15} 48.14 & \cellcolor{green!15} 55.66 & 70.01 & 68.96 & 15.50 & \cellcolor{green!15} 19.58 & 33.56 & 32.30 \\

\midrule
\midrule

& \multicolumn{4}{c}{\textbf{GENEVA}} & \multicolumn{4}{c}{\textbf{DocEE}} & \multicolumn{4}{c}{\textbf{WikiEvents}} \\
\midrule
\multicolumn{13}{c}{\textit{Baselines }}\\
\midrule
BERT & 15.24 & 26.58 & 53.09 & 50.74 & 18.66 & 25.74 & 47.81 & 44.05& 6.46 & 9.55 & 29.44 & 27.2 \\
Flan-T5 & \cellcolor{green!15} 18.34 & \cellcolor{green!15} 30.85 & \cellcolor{green!15} 57.76 & \cellcolor{green!15} 55.36 & 18.55 & 24.97 & 45.4 & 41.92 & 9.27 & 11.8 & 29.4 & 27.41 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5  & 13.20 & 25.46 & 49.80 & 47.61 & 14.26 & 19.95 & 38.39 & 35.25 & 9.08 & 10.90 & 34.53 & 31.86 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 11.69 & 24.40 & 50.73 & 48.37 & 17.99 & 26.78 & 46.77 & 43.28 & 6.22 & 7.31 & 34.37 & 31.32 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 13.31 & 24.86 & 48.44 & 46.32 & \cellcolor{green!50} 22.91 & \cellcolor{green!50} 32.55 & \cellcolor{green!50} 58.16 & \cellcolor{green!50} 53.74 & 9.89 & 12.36 & 38.24 &  35.31\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1  & 16.36 &  29.62 &  55.09 &  52.79 & 17.56 & 25.14 & 46.44 & 42.80 & \cellcolor{green!15} 12.81 & \cellcolor{green!15} 15.48 & 38.94 & 36.29 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o  & \cellcolor{green!50} 19.16 & \cellcolor{green!50} 33.35 & \cellcolor{green!50} 58.30 & \cellcolor{green!50} 56.02 & 21.91 & 31.65 & 56.40 & 52.14 & \cellcolor{green!50} 13.80 & \cellcolor{green!50} 17.00 & \cellcolor{green!15} 41.85 & \cellcolor{green!50}39.04\\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5  & 10.76 & 21.76 & 46.31 & 44.12 & 19.84 & 27.79 & 48.51 &  44.93 & 6.87 & 8.53 & 31.97 & 29.32 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 9.38 & 21.21 & 45.51 & 43.33 & 9.87 & 14.71 & 26.02 & 24.05 & 3.25 & 4.61 & 17.63 & 16.16 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 16.37 & 26.77 & 47.07 & 45.24 & 6.90 & 9.63 & 19.28 & 17.65 & 4.49 & 6.07 & 19.58 & 18.06\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1  & 9.46 & 17.29 & 32.05 & 30.72 & 19.79 & 29.68 & 53.99 & 49.78 & 10.78 & 13.11 & 36.56 & 33.91 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o  &  16.54 & 28.50 & 48.48 & 46.65 & \cellcolor{green!15} 22.26 & \cellcolor{green!15} 31.92 & \cellcolor{green!15} 57.27 & \cellcolor{green!15} 52.90 & 12.15 & 15.10 & \cellcolor{green!50} 41.93 & \cellcolor{green!15} 38.90\\

\bottomrule
\end{tabular}

\caption{Evaluation results using the REGen framework for event argument extraction across the six datasets. The table reports F1-scores for models assessed at different evaluation levels: Exact Match (EM), Relaxed Match (RM), Complex Match (CM), and Judgment-Aligned Match (JAM). Due to space constraints, detailed precision, recall, and F1-scores are provided in Appendix Tables \ref{DiscourseEE-all-results}-\ref{WikiEvents-all-results}. The highest and the second-highest values in a column are highlighted using a dark shade and light shade, respectively.}
\label{EAE-all-results}
\end{table*}

\textbf{Significant improvement in F1-score across all datasets:} Table \ref{EAE-all-results} illustrates performance of various models using REGen framework. We observed a notable performance boost when models transitioned from the EM to the JAM score. For instance, the F1-score for the top-performing GPT-4o model increased from 16.82 with EM to 46.16 with JAM in the DiscourseEE dataset. Additionally, the average F1-scores of the LLMs shown in Table \ref{average-improvement-rate} exhibit that all evaluated datasets achieved considerable performance gains, averaging 23.93 points. The increase in F1 score for the GENEVA dataset was 32.49, representing a 238.52\% improvement over the standard EM evaluation. Similar substantial gains were noted in other datasets, such as 26.71 for DiscourseEE and 24.32 for DocEE. 


\textbf{On average, 41.20\% of inferences are reduced under the REGen framework:} Our results in Figure \ref{inference-count-and-reduction-comparison} and Table \ref{inference-reduction-full-stat} demonstrate that the REGen framework significantly lowers the number of inferences needed for evaluation compared to solely using the LLMs-as-judge approach \cite{lu2024exactmatchsemanticallyreassessing}. For example, in the PHEE dataset, the inference count drops dramatically from 12,206 to 4,436, resulting in a reduction of 63.6\%. Similarly, the DocEE dataset sees a decrease from 24,166 to 12,624, corresponding to a 47.7\% reduction. These results highlight the efficiency of the REGen framework in streamlining the inference process. It enables effective evaluation by significantly decreasing the computational burden. Moreover, the systematic reduction in judgment errors through the REGen framework lessens the need for human validation without compromising reliability.


\textbf{REGen framework is more reliable (87.67\% alignment)}: REGen shows no/minimal errors in the performance under exact and relaxed match scoring. While there is some overestimation due to misjudgments in the complex match step, our extensive validation indicates an 87.67\% alignment with human judgments (see Table \ref{judgement-error-rate}). The JAM score incorporates this human alignment, ensuring the overall reliability of the framework. Additionally, the reported scores are more explainable, as they include a clear breakdown of performance gains at each level (EM, RM, CM, and JAM). 


\begin{figure}[t!]
  \centering
  \includegraphics[width =1\linewidth]{figures/inference-count-and-reduction-comparison.png}
 \caption{Comparison of required inference counts and reduction in inferences when using LLM-as-Judge versus the REGen framework for the GPT-4o prediction model. Additional statistics are presented in Table \ref{inference-reduction-full-stat}.}
 \label{inference-count-and-reduction-comparison}
\end{figure}

\textbf{Recall is on average higher than precision in all settings:} Our fine-grained analysis (see Tables \ref{DiscourseEE-all-results} to \ref{WikiEvents-all-results}) reveals LLMs achieve higher recall than precision. Such as the GPT-4o model in the DocEE dataset achieved a JAM recall of 68.41 compared to a precision of only 42.12. This indicates while the models are effective in identifying ground-truth arguments, they tend to over-predict, impacting the overall F1-score. In this work, we used a single prompt for all the models and datasets, which might have contributed to this overprediction. Future research should focus on pushing the performance through dataset- and model-specific prompting to enhance precision without sacrificing recall.
