\section{Dataset Details}

\label{appendix: data-info}
We transform all datasets into a unified format as explained in Section \ref{dataset-formatting}. Each document includes a set of predefined roles based on the event, with each role having a list of ground-truth argument strings. In this work, we use a trigger-free approach for argument extraction \cite{tong-etal-2022-docee}. We adopt this formulation because many datasets lack trigger annotations or include implicit or scattered arguments that can not be tied to trigger phrases \cite{sharif-etal-2024-explicit}. We use the official test split of all the datasets. Table \ref{data-statistics} exhibits the detailed statistics of the datasets. We will release our processed datasets and associated scripts upon acceptance of the paper. Detailed descriptions of each dataset are provided in the following.

\begin{itemize}
    \item  \textbf{DiscourseEE} \cite{sharif-etal-2024-explicit} dataset is annotated from online health discussions and includes explicit, implicit, and scattered arguments. This dataset is hierarchical, with each role further classified into four types: core, type-specific, subject-specific, and effect-specific arguments. We sourced the test set from the official repository 
\href{https://github.com/omar-sharif03/DiscourseEE/tree/main/Data}{https://github.com/omar-sharif03/DiscourseEE}. It features 34 unique roles across 3 event types, with all arguments annotated as strings. In this work, we do not use the argument types or hierarchical structure, as they are not essential. We process the dataset using the author's provided code.

\item  \textbf{PHEE} \cite{sun-etal-2022-phee} is an event extraction dataset sourced from the pharmacovigilance domain. It contains 14 unique roles across 2 event types. We obtain the dataset from \href{https://github.com/ZhaoyueSun/PHEE}{https://github.com/ ZhaoyueSun/PHEE}. The dataset includes annotations for both trigger and argument spans. Following our formulation, we discard the trigger and only take the argument strings. We combine multiple arguments under the same role into a single argument list, separating them with semicolons.

\item \textbf{RAMS} \cite{ebner-etal-2020-multi} is an event extraction dataset from the news domain. We downloaded the dataset from \href{https://nlp.jhu.edu/rams/}{https://nlp.jhu.edu/rams/} and processed it leveraging the script provided by TextEE \cite{huang-etal-2024-textee}. We ignored the trigger annotation and used the argument string to map the dataset into our formulation. The test set contains 129 unique events and 63 roles.

\item \textbf{GENEVA} \cite{parekh-etal-2023-geneva} is a general-domain event extraction dataset developed using FrameNet. This dataset includes samples from books, articles, journals, and Wikipedia. We used the provided test set from \href{https://github.com/PlusLabNLP/GENEVA/tree/main/data}{https://github.com/PlusLabNLP/GENEVA}. The test set includes 115 events and 196 unique roles. We applied the preprocessing script from TextEE \cite{huang-etal-2024-textee} to convert the dataset to our format.

\item \textbf{DocEE} \cite{tong-etal-2022-docee} is a trigger-free document-level event extraction dataset with very long documents. We obtained the test set from the official GitHub repository \href{https://github.com/tongmeihan1995/DocEE}{https://github.com/tongmeihan1995/DocEE}. The official test set contains 2,771 documents. Due to high inference time, we selected 500 random samples to reduce complexity. Our test set includes 57 unique events and 266 unique roles.


\item \textbf{WikiEvents}: \cite{li-etal-2021-document} is a document-level event extraction dataset based on Wikipedia texts. We sourced the dataset from \href{https://github.com/raspberryice/gen-arg}{https://github.com/raspberryice/gen-arg} and processed it using the TextEE \cite{huang-etal-2024-textee} preprocessing script. We retained only the argument annotations and discarded the rest. The test set includes 33 event types and 44 roles. WikiEvents is highly argument-dense compared to other datasets, with a density of 24.89. It also has longer documents, averaging 654 words per document.

\end{itemize}

