\section{Model Details}
\label{appendix: models}
\textbf{Baselines:} As baselines, we used transformer-based BERT (110M parameters) and instruction-fine-tuned FLAN-T5 (250M parameters) models. Both models were implemented using the HuggingFace pipeline and fine-tuned on the SQuAD \cite{rajpurkar-etal-2016-squad} dataset. BERT was fine-tuned with a learning rate of $2 \times 10^{-5}$, a batch size of 8, and trained for 3 epochs. FLAN-T5 was fine-tuned for 4 epochs with a batch size of 16 and the same learning rate. During prediction, we provide a role-specific question and the associated document as context. The input is formatted as \texttt{[CLS] Question [SEP] Document [SEP]}. The output span is then decoded as the argument for the specific role. Arguments for each role are extracted independently.

\paragraph{LLMs:} To investigate the feasibility of our proposed evaluation framework, we experimented with various LLMs used in previous studies on event argument extraction \cite{sharif-etal-2024-explicit,lu2024exactmatchsemanticallyreassessing}. We evaluated open-source models ranging from 4B to 70B parameters and the closed-source GPT-4o model. This diverse selection allowed us to assess performance across different scales. We used five LLMs for the experimentation.
\begin{itemize}
    \item \textbf{Phi-3.5} : 
    We used the Phi-3.5-mini, a 3.8 billion parameter model trained on 3.3 trillion tokens \cite{phi-3.5}. It achieves comparable performance to Mixtral  8x7B and GPT-3.5 models on academic benchmarks despite being a very small model. 
    
    \item \textbf{Gemma-1.1} model trained on 6T tokens with novel RLHF method, based on the architecture and training recipe of Gemini models \cite{gemmateam2024gemma}. It performs better than similar open-source models in 11 out of 18 text tasks. Gemma is available in two versions, with 2 billion and 7 billion parameters. We use the 7 billion parameter version for our experiments.
    
    \item \textbf{Mixtral (8x7B)} is a sparse mixture of expert language designed with an architecture similar to Mistral 7B \cite{jiang2024mixtral}. It has a total of 47 billion parameters, with only 13 billion being active at a time. These architectural changes allow Mixtral to outperform models with more parameters (e.g., Llama-2, GPT-3.5) across several benchmarks. 
    
    \item \textbf{Llama-3.1} is a state-of-the-art open-source language model pretrained and instruction-fined with 8B, 70B, and 405B parameters. It builds upon the Llama-3 model \cite{llama-3.1}, incorporating grouped query attention (GQA) and RLHF. We use the 70B version of the model. 

 
    \item \textbf{GPT-4o} \cite{openai2024gpt4} is one of the best-performing models that can reason across audio, vision, and text. It achieved state-of-the-art performance across most benchmarks\footnote{https://lmarena.ai/}. 
\end{itemize}

We utilized the instruction-tuned versions of all the models. The HuggingFace inference strings for the open-source LLMs are Phi-3.5 \texttt{(microsoft/ Phi-3.5-mini-instruct)}, Gemma-1.1 \texttt{(google/ gemma-1.1-7b-it)}, Mixtral \texttt{(mistralai/ Mixtral-8x7B-Instruct- v0.1)}, and Llama-3.1 \texttt{(meta-llama/ Llama-3.1-70B-Instruct)}. We assess the performance of the GPT-4o model through API calls, using version \texttt{(gpt-4o-2024-11-20)}.