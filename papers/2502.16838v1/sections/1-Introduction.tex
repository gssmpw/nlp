\section{Introduction}
Information extraction is a key area in natural language processing \cite{gaizauskas-wilks-1998-information}. Event argument extraction (EAE) is a core information extraction task that transforms text into structured information. As EAE identifies and extracts event-specific arguments from texts, it is essential for a wide range of applications such as document understanding \cite{tong-etal-2022-docee}, misinformation detection \cite{wu-etal-2022-cross}, discourse understanding \cite{sharif-etal-2024-explicit}, pharmacovigilance \cite{sun-etal-2022-phee}. With the emergence of generative models (e.g., LLMs), EAE has gained significant attention in recent years \cite{zhang-etal-2024-ultra,zhang-etal-2025-survey}. However, previous studies \cite{gao2023exploringfeasibilitychatgptevent, sun-etal-2024-leveraging} indicate that LLMs perform poorly on EAE tasks. This is largely due to the disconnect between the nature of generative predictions and the exact span-based evaluation method commonly used for EAE \cite{huang-etal-2024-textee}.

\begin{figure}[h!]
  \centering
  \includegraphics[width =\linewidth]{figures/performance-comparison.png}
 \caption{Performance comparison of the best-performing EAE model across six datasets under Exact Match (EM) and the REGen framework. The results highlight that, on average, EM underestimates model performance by 54.8\%, which is captured by REGen.}
 \label{intro-example-post}
\end{figure}

Span-based exact matching (EM) significantly underestimates the performance of LLMs as they often predict accurate arguments in surface forms that differ from the ground truth. For example, if the ground truth annotation for a role is \texttt{`pain relief'}, the model might output terms like \texttt{[alleviates pain, reducing discomfort, analgesia]}. Depending on the context, all or multiple of these outputs are correct, but none would be accepted by EM. Even minor variations would result in no match. Authors in \cite{sharif-etal-2024-explicit} highlighted that this problem is even more pronounced when evaluating the arguments composed of information from different parts of the text (scattered arguments) or the arguments that are not directly mentioned (implicit arguments).


Previous works have attempted to address these issues using embedding-based relaxed matching, which considers two arguments similar if they have high embedding similarity \cite{han2024empiricalstudyinformationextraction}. However, this approach fails to capture semantically similar arguments with different lexical forms and wrongly classifies arguments with high token overlap as similar \cite{sharif-etal-2024-explicit}. For example, in Figure \ref{eval-framework-example} for the role \textit{`patient concerns'}, the ground-truth argument \texttt{`limited insurance coverage'} and the predicted argument \texttt{`coverage limitations for FLA and cryotherapy'} refers to the same issue. Due to lexical variation, relaxed matching fails to capture this. In contrast, consider a role \textit{`date'} for which ground-truth and predicted arguments are \texttt{`18 April 2024'} and \texttt{`20 April 2024'}, respectively. These two arguments are different, but relaxed matching considers them the same due to high token overlap. Context is needed when evaluating these arguments. Recent work by  \citet{lu2024exactmatchsemanticallyreassessing} used LLMs as judges to identify similar arguments. This approach requires a large number of inferences, adding significant computational costs. Additionally, without human validation, LLM-based judgments can produce unreliable results.  Relying solely on relaxed match or judge-based approaches can overestimate performance by incorrectly classifying non-matching arguments as matches, leading to inflated and unreliable model assessments.

To address these limitations, we introduce \textbf{REGen}, a reliable evaluation framework for event argument extraction. REGen systematically combines the strengths of exact, relaxed, and LLM-based matching by \textbf{maximizing the evaluation reliability while minimizing the computation costs}. Figure \ref{eval-framework-example} illustrates the framework, and it is structured into four sequential phases: \textit{Exact Match (EM)}, \textit{Relaxed Match (RM)}, \textit{Complex Match (CM)}, and \textit{Alignment with Human Judgments}. 

The EM level filters arguments that match exactly, reducing computational costs for subsequent stages by eliminating obvious matches. This level does not require human evaluation as exact matches indicate perfect agreement with humans. The RM stage identifies arguments that are semantically similar, making evaluation robust to minor syntactic variations. This matching is performed based on the contextual embedding of the arguments. Setting up a high embedding similarity threshold ensures higher reliability and minimizes human evaluation. 

After filtering out exact and relaxed matches, unmatched arguments are carried forward for complex matching. The CM stage captures semantically similar arguments based on context despite lexical and/or syntactic differences. We leverage LLM as a judge \cite{NEURIPS2023_91f18a12} at this stage for determining argument similarity. Finally, in the judgment alignment stage, we propose a novel \textbf{Judgment Aligned Match (JAM)} score to factor in the scores from each level to account for misjudgments based on extensive human validation. This framework ensures evaluation accuracy, cost-effectiveness, and better alignment with human judgments. 

To the best of our knowledge, this is the first systematic evaluation of LLMs on popular EAE datasets. Unlike prior studies \cite{lu2024exactmatchsemanticallyreassessing, huang-etal-2024-textee} that experimented on small test subsets sampled and merged from multiple datasets, we evaluate the complete test sets of the original datasets. This provides a more reliable assessment of LLMs' performances on these benchmarks and highlights their potential in solving the EAE task, which has been previously underestimated. Our key contributions are as follows.

\begin{itemize}
   
\item We present \textbf{REGen}, a \textbf{R}eliable \textbf{E}valuation framework for \textbf{Gen}erative event
argument extraction, minimizing inference costs and the need for human validation. REGen yields 87.67\% alignment with humans thus ensuring higher reliability. We also introduce a scoring mechanism to systematically measure how well REGen's evaluation aligns with human judgments. Finally, we curate a novel, human-annotated dataset with 900 samples to select LLM models as judges for EAE evaluation. % which we will release to enable future research in this direction.

\item We demonstrate the generalizability of REGen through extensive evaluation using multiple LLMs on six widely-used EAE datasets, including DiscourseEE \cite{sharif-etal-2024-explicit}, PHEE \cite{sun-etal-2022-phee}, RAMS \cite{ebner-etal-2020-multi}, GENEVA \cite{parekh-etal-2023-geneva}, DocEE \cite{tong-etal-2022-docee}, and WikiEvents \cite{li-etal-2021-document}. The results show an average improvement of 23.93 F1 points across all datasets while reducing inference costs by 41.2\% than the LLM-as-judge-only approach \cite{lu2024exactmatchsemanticallyreassessing}.

    % \item \textcolor{blue}{This seems a bit out-of-place: not sure if this is a separate contribution.}
    % To the best of our knowledge, this is the first systematic evaluation of generative models on popular EAE datasets. Unlike prior studies \cite{lu2024exactmatchsemanticallyreassessing, huang-etal-2024-textee} that experimented on small, combined test subsets, we evaluate the complete official test sets of the datasets. This provides a more accurate assessment of LLMs' performances on these benchmarks and highlights their potential in solving the EAE task, which has been previously underestimated.

\end{itemize}

\noindent
\textbf{Reproducibility:} Our code, evaluation framework, the judge and alignment datasets, and other relevant resources are available at \href{https://github.com/omar-sharif03/EAE-Eval}{https://github.com/omar-sharif03/EAE-Eval}.
% \textbf{Reproducibility:}  We will release the code, evaluation framework, the judge and alignment datasets, and other relevant resources upon the acceptance of the paper to enable future research in this direction.


%\textcolor{blue}{[mention other strengths of REGen: saving manual effort, computational inference across multiple datasets and domains]}

% \begin{itemize}
%     \item We investigate the limitations for the current evaluation approach for generative models in argument extraction through a comprehensive empirical study.
    
%     % \textcolor{red}{}
%     \item We introduce \textbf{MLM-Eval}, a reliable evaluation framework for argument extraction. It systematically reduces the possibility of error in evaluation and the burden of human assessment. As part of this framework, we propose a novel JAM score for improved alignment with human judgment. We also developed a human-annotated judge dataset and will release it to facilitate future research.
%     \item To the best of our knowledge, this is the first systematic evaluation of generative models on six argument extraction datasets.  Unlike prior studies that experimented on small, combined test subsets, we conduct evaluations on the full official test sets of the datasets, providing a more accurate assessment of generative models' performance on these benchmarks.
%     \item \textcolor{red}{I dont know if it would make this list, but the 900 samples for judge eval is a pretty valuable resource. }

% \textcolor{blue}{Note: [Will add numbers in the contribution list, such as how much 90\% alignment with humans, captures 50\% more of the true performance]}
% \end{itemize}

%Figure \ref{eval-framework} outlines the proposed framework.


%Consequently, the accurate evaluation of these models for argument extraction has become increasingly important.

%Most existing works evaluate EAE models using exact match (EM), where predicted arguments must match the gold-label annotated span \cite{huang-etal-2024-textee, hsu-etal-2022-degree}.

% Furthermore, it is not possible to evaluate the scattered arguments composed of information throughout the text or implicit arguments with no direct mention in the text \cite{sharif-etal-2024-explicit}. As a result, the scores of models reported under EM do not accurately reflect their true performance.

% Information extraction is a challenging natural language processing task, transforming text into structured information. Within this domain, we focus on event argument extraction (EAE), a core task, transforms text into structured information. As EAE extracts event-specific arguments from texts, it can serve a wide range of applications, such as document understanding \cite{tong-etal-2022-docee}, misinformation detection \cite{wu-etal-2022-cross}, discourse understanding \cite{sharif-etal-2024-explicit}, pharmacovigilance \cite{sun-etal-2022-phee}. With the emergence of generative models (e.g., LLMs), EAE has gained significant attention in recent years. \cite{zhang-etal-2024-ultra,zhang-etal-2025-survey}. However, previous studies \cite{gao2023exploringfeasibilitychatgptevent, sun-etal-2024-leveraging} indicate that LLMs perform poorly on EAE tasks. This is largely due to the disconnect between the nature of generative predictions and the exact span-based evaluation method commonly used for EAE \cite{huang-etal-2024-textee}.

% Authors in \cite{sharif-etal-2024-explicit} highlighted this problem is even more pronounced when evaluating scattered arguments composed of information from different parts of the document, or implicit arguments that are not directly mentioned. As a result, scores reported under EM fail to capture the true capabilities of the models.