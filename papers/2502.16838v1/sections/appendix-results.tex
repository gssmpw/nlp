\section{Additional Results}

\begin{table*}[t!]
\centering
\renewcommand*{\arraystretch}{1}
\small
\begin{tabular}{l|cccccc|C{1.9cm}}
Datasets & DiscourseEE & PHEE & RAMS  &  GENEVA &  DocEE & WikiEvents & \\
\toprule
\multicolumn{7}{c}{Inference count and reduction in inference only for zero-shot approach}& Avg. Reduction (\%)  \\
\midrule
\#Inference (LLM as Judge) & 1822 & 6215 & 3718 & 3897 & 12655 & 1852 &  \\
\#Inference (REGen) & 1201 & 2077 & 2318 & 2465 & 6513 & 1158 & \\
Reduction count  & 621 & 4138 & 1400 & 1432 & 6142 & 694 & \\
\cmidrule{8-8}
Reduction (\%) & 34.08 & 66.58 & 37.65 & 36.74 & 48.53 & 37.47 & \textbf{43.51} \\
\midrule
\multicolumn{7}{c}{Inference count and reduction in inference only for only for chain-of-thought approach}&  \\
\midrule
\#Inference (LLM as Judge) & 1740 & 5991 & 3124 & 3549 & 11511 & 1588 &  \\
\#Inference (REGen) & 1186 & 2359 & 2200 & 2435 & 6111 & 1089 & \\
Reduction count  & 554 & 3632 & 924 & 1114 & 5400 & 499 & \\
\cmidrule{8-8}
Reduction (\%) & 31.83 & 60.62 & 29.57 & 31.38 & 46.91 & 31.42 & \textbf{38.62} \\
\midrule
\multicolumn{7}{c}{Total inference count and reduction in inference (zero-shot + chain-of-thought)}&  \\
\midrule
\#Inference (LLM as Judge) & 3562 & 12206 & 6842 & 7446 & 24166 & 3440 &  \\
\#Inference (REGen) & 2387 & 4436 & 4518 & 4900 & 12624 & 2247 & \\
Reduction count  & 1175 & 7770 & 2324 & 2546 & 11542 & 1193 & \\
\cmidrule{8-8}
Reduction (\%) & 32.98 & 63.65 & 33.96 & 34.19 & 47.76 & 34.68 & \textbf{41.20}\\
\bottomrule
\end{tabular}
\caption{Detailed comparison of inference counts and reductions when using LLMs as Judge versus the proposed REGen framework for the GPT-4o prediction model. Results for both zero-shot and chain-of-thought approaches are presented, illustrating total inference counts, achieved reductions, and corresponding percentage reductions across the evaluated datasets.} 
\label{inference-reduction-full-stat}
\end{table*}

%% DiscourseEE Results
\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1}

\begin{tabular}{l|ccc|ccc|ccc|ccc}
 & \multicolumn{3}{c}{\textbf{Exact-Match}}& \multicolumn{3}{c}{\textbf{Relaxed-Match}} & \multicolumn{3}{c}{\textbf{Complex-Match}}& \multicolumn{3}{c}{\textbf{JAM-Score}} \\
\midrule
\textbf{Model}& P & R & F1 & P & R & F1  & P & R & F1 & P & R & F1\\
\midrule

\multicolumn{13}{c}{\textit{Baselines}}\\
\midrule
BERT & 6.3	& 5.52	& 5.88	& 9.28	& 8.12	& 8.66 &	35.17 & 32.1 & 33.56 & 31.65 & 28.84& 30.18 \\

Flan-T5 & 7.22 & 6.32 & 6.74 & 10.88 & 9.53 & 10.16 & 37.8 & 35.21 & 36.46 & 34.13 & 31.71 & 32.87 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 3.21 & 3.61 & 3.40 & 4.73 & 5.32 & 5.00 & 14.81 & 14.64 & 14.73 & 13.43 & 13.36 & 13.39 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 10.45 & 13.74 & 11.87 & 13.96 & 18.36 & 15.86 & 45.61 & 55.67 & 50.14 & 41.31 & 50.58 & 45.48 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 10.59 & 17.15 & 13.10 & 14.37 & 23.17 & 17.74 & 41.82 & 57.97 & 48.59 & 38.07 & 53.19 & 44.38 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 11.05 & 16.95 & 13.38 & 15.49 & 23.67 & 18.73 & 37.52 & 51.96 & 43.57 & 34.47 & 48.02 & 40.13 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 14.14 & 20.76 & 16.82 & 19.40 & 28.49 & 23.08 & 43.99 & 57.57 & 49.87 & 40.58 & 53.50 & 46.16\\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 5.53 & 9.83 & 7.08 & 9.76 & 17.05 & 12.42 & 34.26 & 52.36 & 41.41 & 30.89 & 47.47 & 37.43 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 7.60 & 12.14 & 9.35 & 10.62 & 16.95 & 13.06 & 37.44 & 51.25 & 43.27 & 33.79 & 46.57 & 39.16 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 4.78 & 5.22 & 4.99 & 6.71 & 7.32 & 7.00 & 26.01 & 26.78 & 26.39 & 23.39 & 24.14 & 23.76 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 10.81 & 15.25 & 12.65 & 14.79 & 20.86 & 17.31 & 39.83 & 51.05 & 44.75 & 36.40 & 46.89 & 40.98 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 12.64 & 17.75 & 14.77 & 17.86 & 25.08 & 20.86 & 42.71 & 53.06 & 47.33 & 39.27 & 49.15 & 43.66 \\

\bottomrule 
\end{tabular}
\caption{DiscourseEE evaluation results using REGen framework.}
\label{DiscourseEE-all-results}
\end{table*}

%%PHEE dataset results
\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1}

\begin{tabular}{l|ccc|ccc|ccc|ccc}
 & \multicolumn{3}{c}{\textbf{Exact-Match}}& \multicolumn{3}{c}{\textbf{Relaxed-Match}} & \multicolumn{3}{c}{\textbf{Complex-Match}}& \multicolumn{3}{c}{\textbf{JAM-Score}} \\
\midrule
\textbf{Model}& P & R & F1 & P & R & F1  & P & R & F1 & P & R & F1\\
\midrule

\multicolumn{13}{c}{\textit{Baselines }}\\
\midrule
BERT & 29.09 & 26.6 & 27.78 & 36.62 & 33.48 & 34.98 & 54.88 & 50.53 & 52.61 & 53.55 & 49.28 & 51.33 \\

Flan-T5 & 44.32 & 40.53 & 42.34 & 52.8 & 48.28 & 50.44 & 69.96 & 64.24 & 66.98 & 68.71 & 63.07 & 65.77 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 39.07 & 47.88 & 43.03 & 45.84 & 56.12 & 50.46 & 62.61 & 73.63 & 67.67 & 61.39 & 72.35 & 66.42\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 44.10 & 45.94 & 45.00 & 53.23 & 55.49 & 54.34 & 75.91 & 77.99 & 76.93 & 74.25 & 76.35 & 75.28\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 33.68 & 40.02 & 36.58 & 39.16 & 46.59 & 42.55 & 55.39 & 63.55 & 59.19 & 54.20 & 62.31 & 57.98 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 36.49 & 42.29 & 39.17 & 43.77 & 50.63 & 46.95 & 61.53 & 66.60 & 63.96 & 60.23 & 65.43 & 62.72 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 51.38 & 56.18 & 53.67 & 59.34 & 64.74 & 61.92 & 77.36 & 80.63 & 78.96 & 76.04 & 79.47 & 77.72 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 30.57 & 33.76 & 32.09 & 36.24 & 39.96 & 38.01 & 52.68 & 55.45 & 54.03 & 51.48 & 54.32 & 52.86 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 33.39 & 34.92 & 34.14 & 41.35 & 43.26 & 42.28 & 60.99 & 61.93 & 61.46 & 59.56 & 60.57 & 60.06 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 29.12 & 29.81 & 29.46 & 36.87 & 37.70 & 37.28 & 50.90 & 50.61 & 50.75 & 49.87 & 49.66 & 49.77 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 30.44 & 32.19 & 31.29 & 38.87 & 41.05 & 39.93 & 52.20 & 52.83 & 52.51 & 51.22 & 51.97 & 51.59\\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 46.91 & 49.43 & 48.14 & 54.27 & 57.13 & 55.66 & 69.68 & 70.34 & 70.01 & 68.56 & 69.37 & 68.96 \\

\bottomrule 
\end{tabular}
\caption{PHEE evaluation results using REGen framework.}
\label{PHEE-all-results}
\end{table*}


%% RAMS Results
\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1.1}

\begin{tabular}{l|ccc|ccc|ccc|ccc}
 & \multicolumn{3}{c}{\textbf{Exact-Match}}& \multicolumn{3}{c}{\textbf{Relaxed-Match}} & \multicolumn{3}{c}{\textbf{Complex-Match}}& \multicolumn{3}{c}{\textbf{JAM-Score}} \\
\midrule
\textbf{Model}& P & R & F1 & P & R & F1  & P & R & F1 & P & R & F1\\
\midrule

\multicolumn{13}{c}{\textit{Baselines }}\\
\midrule
BERT & 14.63 & 14.63 & 14.63 & 18.14 & 18.14 & 18.14 & 33.61 & 33.61 & 33.61 & 32.24 & 32.24 & 32.24\\
Flan-T5 & 12.61 & 12.61 & 12.61 & 15.13 & 15.13 & 15.13 & 28.62 & 28.62 & 28.62 & 27.43 & 27.43 & 27.43 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 13.75 & 17.35 & 15.34 & 16.07 & 20.27 & 17.92 & 31.23 & 37.77 & 34.19 & 29.90 & 36.22 & 32.76 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 14.40 & 15.37 & 14.87 & 16.95 & 18.09 & 17.50 & 31.45 & 33.47 & 32.43 & 30.17 & 32.11 & 31.11 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 11.30 & 15.22 & 12.97 & 13.47 & 18.14 & 15.46 & 26.42 & 34.50 & 29.93 & 25.28 & 33.06 & 28.65 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 9.98 & 14.88 & 11.95 & 12.20 & 18.04 & 14.56 & 21.82 & 30.50 & 25.44 &  20.96 & 29.39 & 24.47 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 15.01 & 27.58 & 19.44 & 17.89 & 32.82 & 23.15 & 29.91 & 49.98 & 37.42 & 28.84 & 48.43 & 36.15 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 14.15 & 17.20 & 15.53 & 17.00 & 20.66 & 18.65 & 31.96 & 37.57 & 34.54 & 30.64 & 36.07 & 33.13\\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 10.17 & 11.32 & 10.71 & 12.88 & 14.34 & 13.57 & 25.13 & 27.53 & 26.28 & 24.04 & 26.36 & 25.15 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 6.08 & 7.86 & 6.85 & 7.34 & 9.49 & 8.28 & 15.33 & 19.08 & 17.00 & 14.63 & 18.23 & 16.23 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 9.28 & 12.51 & 10.66 & 11.12 & 14.98 & 12.76 & 21.21 & 26.89 & 23.72 & 20.32 & 25.83 & 22.75 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 12.77 & 19.72 & 15.50 & 16.13 & 24.91 & 19.58 & 28.55 & 40.68 & 33.56 & 27.44 & 39.26 & 32.30 \\

\bottomrule 
\end{tabular}
\caption{RAMS evaluation results using REGen framework.}
\label{RAMS-all-results}
\end{table*}

%GENEVA Results
\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1.1}

\begin{tabular}{l|ccc|ccc|ccc|ccc}
 & \multicolumn{3}{c}{\textbf{Exact-Match}}& \multicolumn{3}{c}{\textbf{Relaxed-Match}} & \multicolumn{3}{c}{\textbf{Complex-Match}}& \multicolumn{3}{c}{\textbf{JAM-Score}} \\
\midrule
\textbf{Model}& P & R & F1 & P & R & F1  & P & R & F1 & P & R & F1\\
\midrule

\multicolumn{13}{c}{\textit{Baselines}}\\
\midrule
BERT & 15.81 & 14.72 & 15.24 & 27.56 & 25.67 & 26.58 & 54.64 & 51.62 & 53.09 & 52.24 & 49.33 & 50.74\\

Flan-T5 & 19.02 & 17.71 & 18.34 & 32.0 & 29.79 & 30.85 & 59.63 & 56.01 & 57.76 & 57.16 & 53.67 & 55.36\\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 12.21 & 14.36 & 13.20 & 23.56 & 27.68 & 25.46 & 47.65 & 52.14 & 49.80 & 45.50 & 49.92 & 47.61 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 11.75 & 11.63 & 11.69 & 24.52 & 24.27 & 24.40 & 51.41 & 50.06 & 50.73 & 49.01 & 47.75 & 48.37 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 12.20 & 14.65 & 13.31 & 22.80 & 27.32 & 24.86 & 45.98 & 51.17 & 48.44 & 43.92 & 49.01 & 46.32 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 15.04 & 17.93 & 16.36 & 27.24 & 32.46 & 29.62 & 52.74 & 57.67 & 55.09 & 50.45 & 55.36 & 52.79 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 17.72 & 20.86 & 19.16 & 30.86 & 36.29 & 33.35 & 56.25 & 60.49 & 58.30 & 53.96 & 58.25 & 56.02 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 9.98 & 11.66 & 10.76 & 20.22 & 23.55 & 21.76 & 44.22 & 48.60 & 46.31 & 42.09 & 46.36 & 44.12 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 9.21 & 9.55 & 9.38 & 20.83 & 21.60 & 21.21 & 45.69 & 45.32 & 45.51 & 43.47 & 43.18 & 43.33 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 15.39 & 17.48 & 16.37 & 25.17 & 28.59 & 26.77 & 45.08 & 49.25 & 47.07 & 43.29 & 47.38 & 45.24 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 9.29 & 9.65 & 9.46 & 16.98 & 17.61 & 17.29 & 33.15 & 31.03 & 32.05 & 31.70 & 29.79 & 30.72\\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 16.00 & 17.12 & 16.54 & 27.57 & 29.50 & 28.50 & 48.50 & 48.47 & 48.48 & 46.59 & 46.71 & 46.65 \\

\bottomrule 
\end{tabular}
\caption{GENEVA evaluation results using REGen framework.}
\label{GENEVA-all-results}
\end{table*}

%DocEE Results
\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1}
\begin{tabular}{l|ccc|ccc|ccc|ccc}
 & \multicolumn{3}{c}{\textbf{Exact-Match}}& \multicolumn{3}{c}{\textbf{Relaxed-Match}} & \multicolumn{3}{c}{\textbf{Complex-Match}}& \multicolumn{3}{c}{\textbf{JAM-Score}} \\
\midrule
\textbf{Model}& P & R & F1 & P & R & F1  & P & R & F1 & P & R & F1\\
\midrule

\multicolumn{13}{c}{\textit{Baselines }}\\
\midrule
BERT & 22.93 & 15.73 & 18.66 & 31.59 & 21.72 & 25.74 & 57.09 & 41.12 & 47.81 & 52.73 & 37.82 & 44.05 \\

Flan-T5 & 22.8 & 15.64 & 18.55 & 30.62 & 21.08 & 24.97 & 54.35 & 38.98 & 45.4 & 50.29 & 35.94 & 41.92 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 14.16 & 14.36 & 14.26 & 19.86 & 20.04 & 19.95 & 39.18 & 37.62 & 38.39 & 35.90 & 34.62 & 35.25 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 20.47 & 16.04 & 17.99 & 30.38 & 23.95 & 26.78 & 51.92 & 42.54 & 46.77 & 48.15 & 39.31 & 43.28 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 21.84 & 24.09 & 22.91 & 31.06 & 34.20 & 32.55 & 56.50 & 59.92 & 58.16 & 52.12 & 55.47 & 53.74 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 15.03 & 21.11 & 17.56 & 21.65 & 29.97 & 25.14 & 42.51 & 51.17 & 46.44 & 38.95 & 47.49 & 42.80 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 16.82 & 31.42 & 21.91 & 24.45 & 44.89 & 31.65 & 45.79 & 73.41 & 56.40 & 42.12 & 68.41 & 52.14 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 18.95 & 20.82 & 19.84 & 26.57 & 29.13 & 27.79 & 46.94 & 50.19 & 48.51 & 43.43 & 46.55 & 44.93 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 10.90 & 9.01 & 9.87 & 16.20 & 13.47 & 14.71 & 28.44 & 23.98 & 26.02 & 26.30 & 22.15 & 24.05 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 7.84 & 6.17 & 6.90 & 10.93 & 8.60 & 9.63 & 22.38 & 16.94 & 19.28 & 20.44 & 15.53 & 17.65 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 16.81 & 24.07 & 19.79 & 25.26 & 35.97 & 29.68 & 48.02 & 61.66 & 53.99 & 44.10 & 57.15  & 49.78 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 17.78 & 29.77 & 22.26 & 25.56 & 42.48 & 31.92 & 47.90 & 71.18 & 57.27 & 44.07 & 66.17 & 52.90\\

\bottomrule 
\end{tabular}
\caption{DocEE evaluation results using REGen framework.}
\label{DocEE-all-results}
\end{table*}

%WikiEvents Results
\begin{table*}[h!]
\small
\centering
\renewcommand*{\arraystretch}{1}

\begin{tabular}{l|ccc|ccc|ccc|ccc}
 & \multicolumn{3}{c}{\textbf{Exact-Match}}& \multicolumn{3}{c}{\textbf{Relaxed-Match}} & \multicolumn{3}{c}{\textbf{Complex-Match}}& \multicolumn{3}{c}{\textbf{JAM-Score}} \\
\midrule
\textbf{Model}& P & R & F1 & P & R & F1  & P & R & F1 & P & R & F1\\
\midrule

\multicolumn{13}{c}{\textit{Baselines}}\\
\midrule
BERT & 9.62 & 4.86 & 6.46 & 14.23 & 7.19 & 9.55 & 38.91 & 23.68 & 29.44 & 36.12 & 21.82 & 27.2 \\

Flan-T5 & 13.81 & 6.98 & 9.27 & 17.57 & 8.88 & 11.8 & 39.33 & 23.47 & 29.4 & 36.87 & 21.82 & 27.41 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Zero-Shot Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 9.80 & 8.46 & 9.08 & 11.76 & 10.15 & 10.90 & 36.76 & 32.56 & 34.53 & 33.94 & 30.03 & 31.86 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 8.65 & 4.86 & 6.22 & 10.15 & 5.71 & 7.31 & 40.98 & 29.60 & 34.37 & 37.49 & 26.90 & 31.32 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 10.55 & 9.30 & 9.89 & 13.19 & 11.63 & 12.36 & 39.09 & 37.42 & 38.24 & 36.16 & 34.51 & 35.31 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 11.06 & 15.22 & 12.81 & 13.36 & 18.39 & 15.48 & 33.95 & 45.67 & 38.94 & 31.62 & 42.58 & 36.29 \\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 11.47 & 17.34 & 13.80 & 14.13 & 21.35 & 17.00 & 35.10 & 51.80 & 41.85 & 32.73 & 48.36 & 39.04 \\

\midrule
\multicolumn{13}{c}{\textit{LLMs with Chain-of-thought Prompt}}\\
\midrule

\includegraphics[width=0.30cm, height=0.30cm]{icons/ms-icon.png} Phi-3.5 & 7.82 & 6.13 & 6.87 & 9.70 & 7.61 & 8.53 & 35.04 & 29.39 & 31.97 & 32.18 & 26.93 & 29.32 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/google-icon.png} Gemma-1.1 & 4.53 & 2.54 & 3.25 & 6.42 & 3.59 & 4.61 & 22.26 & 14.59 & 17.63 & 20.47 & 13.35 & 16.16 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/mistral-ai-icon.png} Mixtral & 5.96 & 3.59 & 4.49 & 8.07 & 4.86 & 6.07 & 24.56 & 16.28 & 19.58 & 22.70  & 14.99 & 18.06 \\
\includegraphics[width=0.30cm, height=0.30cm]{icons/meta-icon.png} Llama-3.1 & 10.78 & 10.78 & 10.78 & 13.11 & 13.11 & 13.11 & 35.94 & 37.21 & 36.56 & 33.36 & 34.49 & 33.91\\
\includegraphics[width=0.32cm, height=0.32cm]{icons/openai-icon.png} GPT-4o & 10.77 & 13.95 & 12.15 & 13.38 & 17.34 & 15.10 & 37.36 & 47.78 & 41.93 & 34.65 & 44.34 & 38.90 \\

\bottomrule 
\end{tabular}
\caption{WikiEvents evaluation results using REGen framework.}
\label{WikiEvents-all-results}
\end{table*}


\begin{figure}[h!]
  \centering
  \includegraphics[width =1\linewidth]{figures/EAE-Eval-zs-judge-prompt.png}
 \caption{Zero-shot judge selection prompt}
 \label{zs-judge-prompt}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width =1\linewidth]{figures/EAE-Eval-cot-judge-prompt.png}
 \caption{Chain-of-thought judge selection prompt}
 \label{cot-judge-prompt}
\end{figure}


\begin{figure}[t!]
  \centering
  \includegraphics[width =1\linewidth]{figures/EAE-Eval-zs-EAE-prompt.png}
 \caption{Zero-shot event argument extraction prompt}
 \label{zs-EAE-prompt}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width =1\linewidth]{figures/EAE-Eval-cot-EAE-prompt.png}
 \caption{Chain-of-thought event argument extraction prompt}
 \label{cot-EAE-prompt}
\end{figure}
