\section{Judge Selection Process}
\label{appendix: judge-selection}
There is a growing trend to leverage LLM as a judge to reduce the high cost of human evaluation \cite{NEURIPS2023_91f18a12, GPTEval, gu2025surveyllmasajudge}. Following this approach, we employed LLMs to mimic human evaluation and automatically determine whether the ground truth and predicted arguments match. Figure \ref{judge-selection-process} shows the schematic diagram of the judge selection process.

\begin{figure}[h!]
  \centering
  \includegraphics[width =1\linewidth]{figures/EAE-Eval-judge-selection-process.png}
 \caption{Schematic diagram of the judge selection process. }
 \label{judge-selection-process}
\end{figure}

First, we create a judge dataset through manual annotation. We then use this dataset to experiment with multiple models and select the most suitable judge. Specifically, we evaluate four models: Llama3.1-70B, GPT-3.5, GPT-4o-mini, and GPT-4o. Each model is tested using both \textit{zero-shot} and \textit{chain-of-thought} prompts, with a single prompt uniformly applied across all the evaluated datasets and models. Figures \ref{zs-judge-prompt} and \ref{cot-judge-prompt} show the prompts for zero-shot and chain-of-thought, respectively. Table \ref{LLM-Human-Aggrement-Rate} presents the agreement rate between the judge models and human evaluations. 

\begin{table}[h!]
\centering
\renewcommand*{\arraystretch}{1}
\small
\begin{tabular}{l|C{1cm}C{1.2cm}C{1.2cm}C{1.2cm}c}
& GPT-3.5 & GPT-4o-mini &  Llama3-70B & GPT-4o  \\
\toprule
ZS & 73.05 & 84.27 & 51.52 & \cellcolor{green!40} 86.17 \\
COT & 79.91 & 68.11 & 73.95 & 78.39\\
\bottomrule
\end{tabular}
\caption{Agreement percentage of different LLMs with human judgments. ZS and COT indicate zero-shot and chain-of-thought prompting approaches, respectively.} 
\label{LLM-Human-Aggrement-Rate}
\end{table}

While prompt optimization and alternative techniques (e.g., self-consistency) could further improve agreement, we refrain from such experiments due to the high cost and time requirements. Iterating to find optimal prompts for each model and dataset is impractical. These aspects, along with exploring the applicability of small fine-tuned judge models, are better suited for a separate study. 
