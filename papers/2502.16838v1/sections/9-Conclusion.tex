\section{Conclusion}
This paper presents REGen, a novel evaluation framework for EAE. Our extensive experiments and human validation demonstrate its effectiveness, with a 23.93 points improvement in average F1 score across six EAE datasets and reliability, with 87.67\% alignment with human judgments. We highlight the limitations of current evaluation approaches and illustrate how REGen addresses these issues. Furthermore, our analysis reveals that previous studies have underestimated the true performance of LLMs. We believe that REGen fills a critical gap in EAE research and motivates future work to explore the generative model's capability in solving other information extraction tasks, e.g. relation extraction, entity extraction, and beyond.
