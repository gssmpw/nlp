\section{Methodology} \label{sec:method}

This section introduces DropConnect as the background knowledge, followed by our methodology to assign dynamic dropping rates.

\subsection{Preliminary: DropConnect}

Let $l \in \left\{1, \ldots ,L\right\}$ be the $L$ hidden layers in a neural network and $\boldsymbol{y}^{(l)}$ be the output of layer $l$ (and therefore the input of layer $l+1$), each layer of a neural network transforms $\boldsymbol{y}^{(l-1)}$ to $\boldsymbol{y}^{(l)}$ by Equation~\ref{eq:normal-forward}.

\begin{equation} \label{eq:normal-forward}
\boldsymbol{y}^{(l)} = f_l\left(\boldsymbol{z}^{(l)}\right) = f_l\left(\boldsymbol{W}^{(l)} \boldsymbol{y}^{(l-1)}\right),
\end{equation}
where $\boldsymbol{W}^{(l)}$ is the set of parameters in layer $l$, and $f_l(\cdot)$ is the activation function of the same layer.

DropConnect assigns a universal dropping rate $p$ such that each edge has a probability $p$ of being turned off during training. Therefore, a neural network, with DropConnect, transforms $\boldsymbol{y}^{(l-1)}$ to $\boldsymbol{y}^{(l)}$ by the equation below during training.

\begin{equation} \label{eq:dropconnect-forward}
\boldsymbol{y}^{(l)} = f_l\left(\boldsymbol{z}^{(l)}\right) =
f_l\left( \left( (1-\boldsymbol{M}^{(l)}) \odot \boldsymbol{W}^{(l)}\right) \boldsymbol{y}^{(l-1)}\right),
\end{equation}
where $\boldsymbol{M}^{(l)} = \left[m_{i,j}\right]$ is a $(0,1)$-matrix whose shape is the same as $\boldsymbol{W}^{(l)}$; each entry $m_{i,j}$ is sampled from a Bernoulli distribution with a fixed hyper-parameter $p$, and $\odot$ performs the Hadamard product (i.e., element-wise product) of matrices.  Thus, $\boldsymbol{M}^{(l)}$ is a mask matrix that decides which parameters in $\boldsymbol{W}^{(l)}$ (edges) to omit. 

\subsection{DDC -- Mask Generation} \label{sec:gen-mask}

\RestyleAlgo{ruled}
\begin{algorithm}[tb]
\caption{Generating a mask for $\boldsymbol{W}^{(l)}$}\label{alg:ddc-gen-mask}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{
gradients $\boldsymbol{G}^{(l)} = \left[g_{i,j}^{(l)}\right] \in \mathcal{R}^{n_1 \times n_2}$,
\\ hyperparameters $p$, $p_g$, and $\tau$}
\Output{mask $\widetilde{\boldsymbol{M}}^{(l)} = \left[\widetilde{m}_{i,j}^{(l)}\right] \in \mathcal{R}^{n_1 \times n_2}$}
\For{$i \gets 1$ \KwTo $n_1$}{
    \For{$j \gets 1$ \KwTo $n_2$}{
        $v_{i,j}^{(l)} \gets \left\lvert g_{i,j}^{(l)}\right\rvert$\;
    }
}
$\mu^{(l)} \gets \dfrac{\sum_{i=1}^{n_1} \sum_{j=1}^{n_2} v_{i,j}^{(l)}}{n_1 \times n_2}$\;
$\sigma^{(l)} \gets \dfrac{\sum_{i=1}^{n_1} \sum_{j=1}^{n_2} (v_{i,j}^{(l)} - \mu^{(l)})^2}{n_1 \times n_2}$\;
\For{$i \gets 1$ \KwTo $n_1$}{
    \For{$j \gets 1$ \KwTo $n_2$}{
        $z_{i,j}^{(l)} \gets \dfrac{v_{i,j}^{(l)} - \mu^{(l)}}{\sigma^{(l)}}$\;
        Update $q_{i,j}^{(l)}$ by Equation~\ref{eq:candidate-drop-prob}\;
        Update $p_{i,j}^{(l)}$ by Equation~\ref{eq:drop-rate}\;
        $\widetilde{m}_{i,j}^{(l)} = \text{Bernoulli}(p_{i,j}^{(l)})$\;
    }
}
\end{algorithm}

The proposed DDC advances DropConnect by allowing variable dropping probabilities for distinct neural network edges. 

We create a mask matrix, $\widetilde{\boldsymbol{M}}^{(l)}$ to drop edges. Each entry $\widetilde{m}_{i,j}^{(l)}$ in $\widetilde{\boldsymbol{M}}^{(l)}$ is sampled from a Bernoulli distribution with a unique parameter, $p_{i,j}^{(l)}$. An edge is omitted if $\widetilde{m}_{i,j}^{(l)} = 1$.

Algorithm~\ref{alg:ddc-gen-mask} shows a pseudocode to generate $\widetilde{\boldsymbol{M}}^{(l)}$ during training. For every edge $e_{i,j}^{(l)}$ that bridges neuron $i$ at layer $l-1$ and neuron $j$ at layer $l$, the algorithm decides the value of the corresponding mask cell $\widetilde{m}_{i,j}^{(l)}$ based on its current gradient $g_{i,j}^{(l)}$ at every training iteration. The algorithm first computes the absolute gradient value of $g_{i,j}^{(l)}$, represented as $v_{i,j}^{(l)}$ (line 1 to line 5). DDC needs gradient normalization to harmonize the gradient magnitudes across distinct layers. Therefore, the mean and standard deviation of the set of $v_{i,j}^{(l)}$ values for layer $l$ are calculated (line 6 and line 7). Subsequently, a z-score normalization is performed on $v_{i,j}^{(l)}$, yielding normalized gradient magnitudes $z_{i,j}^{(l)}$-s.
DDC then calculates $q_{i,j}^{(l)}$ for each edge based on the normalized gradient magnitude $z_{i,j}^{(l)}$.  This candidate dropping probability positively correlates with the edge's final dropping probability. Eventually, we define the candidate dropping probability based on Equation~\ref{eq:candidate-drop-prob}, so the model tends to keep the edge with a larger gradient magnitude.

\begin{equation} \label{eq:candidate-drop-prob}
q_{i,j}^{(l)} \gets \begin{cases}
            1 - \sigma\left(z_{i,j}^{(l)}\right) & \text{if } 1 - \sigma\left(z_{i,j}^{(l)}\right) \geq \tau \\
            0 & \text{otherwise},
        \end{cases}
\end{equation}
where $\sigma()$ is the sigmoid function.  Since $\sigma()$ is monotonically increasing and the output is always between 0 and 1, a larger $z_{i,j}^{(l)}$ results in a smaller candidate dropping probability. We set hyperparameter $\tau$ to $0.5$. So, half of the $q_{i,j}^{(l)}$-s are zero on average because the expected value of $\sigma\left(z_{i,j}^{(l)}\right)$ is 0.5.

The final dropping rate $p_{i,j}^{(l)}$ for each edge $e_{i,j}^{(l)}$ is a function of three variables: the candidate dropping probability $q_{i,j}^{(l)}$, a base dropping probability $p$ (a hyper-parameter), and a gradient unit dropping rate $p_g$ (another hyper-parameter). We limit the value of $p_{i,j}^{(l)}$ to be no greater than 1 to ensure $p_{i,j}^{(l)}$ as a proper probability value. The computation of $p_{i,j}^{(l)}$ is shown in Equation~\ref{eq:drop-rate}.

\begin{equation} \label{eq:drop-rate}
p_{i,j}^{(l)} \gets \begin{cases}
            p + p_g \times q_{i,j}^{(l)} & \text{if } p + p_g \times q_{i,j}^{(l)} \le 1 \\
            1 & \text{otherwise}.
        \end{cases}
\end{equation}

Finally, the DDC generates each $\widetilde{m}_{i,j}^{(l)}$ in $\widetilde{\boldsymbol{M}}^{(l)}$ by sampling from a Bernoulli distribution with the parameter $p_{i,j}^{(l)}$ (line 13).

\subsection{DDC -- Training Re-calibration and Testing} \label{sec:ddc-forward}

\begin{algorithm}[tb]
\caption{One-layer forward re-calibration during training}\label{alg:one-layer-forward}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{weights $\boldsymbol{W}^{(l)} = \left[w_{i,j}^{(l)}\right] \in \mathcal{R}^{n_1 \times n_2}$,\\
gradients $\boldsymbol{G}^{(l)} = \left[g_{i,j}^{(l)}\right] \in \mathcal{R}^{n_1 \times n_2}$,\\ 
outputs from previous layer $\boldsymbol{y}^{(l-1)} \in \mathcal{R}^{n_2 \times 1}$}
\Output{$\boldsymbol{y}^{(l)} \in \mathcal{R}^{n_1 \times 1}$}
Compute $\widetilde{\boldsymbol{M}}^{(l)}$ by Algorithm~\ref{alg:ddc-gen-mask} using $\boldsymbol{W}^{(l)}$ and $\boldsymbol{G}^{(l)}$\;
$\widetilde{\boldsymbol{W}}^{(l)} \gets \left(1-\widetilde{\boldsymbol{M}}^{(l)} \right) \odot \boldsymbol{W}^{(l)}$\;
Compute $r^{(l)}$ by Equation~\ref{eq:para-dropping-rate}\;
Compute $\boldsymbol{y}^{(l)}$ by Equation~\ref{eq:compensated-output}\;
\end{algorithm}

Since $\widetilde{\boldsymbol{M}}^{(l)}$ is the mask for the parameters in layer $l$, the effective parameters in this layer go from $\boldsymbol{W}^{(l)}$ to $(1-\widetilde{\boldsymbol{M}}^{(l)}) \odot \boldsymbol{W}^{(l)}$. The dropping mechanism is only applied in training but not in prediction. However, if we set the value of $\boldsymbol{y}^{(l)}$ as $f_l\left((1-\widetilde{\boldsymbol{M}}^{(l)}) \odot \boldsymbol{W}^{(l)} \boldsymbol{y}^{(l-1)}\right)$ in training but predict $\boldsymbol{y}^{(l)}$ as $f_l\left(\boldsymbol{W}^{(l)} \boldsymbol{y}^{(l-1)}\right)$ in inference, the learned $\boldsymbol{W}^{(l)}$ from training cannot be used directly in the tests.  To fix the inconsistency, we need to recalibrate $(1-\widetilde{\boldsymbol{M}}^{(l)}) \odot \boldsymbol{W}^{(l)}$ during training, so that the learned $\boldsymbol{W}^{(l)}$ in training can be used directly during inference by $f_l\left(\boldsymbol{W}^{(l)} \boldsymbol{y}^{(l-1)}\right)$.

Since $\widetilde{\boldsymbol{M}}^{(l)}$ is a $(0,1)$-matrix, $\sum_{i=1}^{n_1} \sum_{j=1}^{n_2} \widetilde{m}_{i,j}^{(l)}$ is the number of 1-s in $\widetilde{\boldsymbol{M}}^{(l)}$.  So, we can compute the expected value of the dropping rate of layer $l$ by Equation~\ref{eq:para-dropping-rate}.

\begin{equation}\label{eq:para-dropping-rate}
r^{(l)} = \frac{\sum_{i=1}^{n_1}\sum_{j=1}^{n_2} \widetilde{m}_{i,j}^{(l)}}{n_1 \times n_2},
\end{equation}
where $n_1$ and $n_2$ are the number of rows and the number of columns of the masking matrix $\widetilde{\boldsymbol{M}}^{(l)}$.  

We re-calibrate the output of layer $l$ during training such that the learned weights $\boldsymbol{W}$ can be used during inference. The re-calibration is achieved by dividing the output by the keep rate:

\begin{equation} \label{eq:compensated-output}
\boldsymbol{y}^{(l)} \gets f_l\left(\widetilde{\boldsymbol{W}}^{(l)} \boldsymbol{y}^{(l-1)} \times \dfrac{1}{1 - r^{(l)}}\right).
\end{equation}

The mask matrix $\widetilde{\boldsymbol{M}}^{(l)}$ and the masked parameter matrix $\widetilde{\boldsymbol{W}}^{(l)}$ are only used in training.  At inference, we use only the unmasked parameter matrix $\boldsymbol{W}^{(l)}$ for forwarding: $\boldsymbol{y}^{(l)} \gets f_l\left(\boldsymbol{W}^{(l)} \boldsymbol{y}^{(l-1)}\right)$.

Algorithm ~\ref{alg:one-layer-forward} shows one-layer forward re-calibration in training.
