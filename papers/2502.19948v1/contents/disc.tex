\section{Conclusion} \label{sec:conc}

This paper presents a straightforward yet effective methodology for dynamically adjusting the dropping rate of the edges in a neural network. This approach avoids the complexity of introducing additional learning parameters and simplifies the implementation process. We demonstrate the parameter updating process through a series of experiments using a synthetic dataset and multiple open datasets and compare our proposed methodology against Dropout and its various adaptations. The results of these experiments indicate that our method surpasses traditional approaches in nearly all scenarios.

The efficacy of gradient magnitude as a reliable indicator for setting the dropping rate has been validated through our results, suggesting a promising direction for future enhancements. Building on this foundation, we propose to develop a model that algorithmically uses gradients as features to determine optimal dropping rates. We hypothesize that this advanced strategy could potentially elevate the model's predictive accuracy even further. However, it's important to note that this approach would introduce additional parameters that require learning, which could extend the training duration and increase computational demands.

Furthermore, we are interested in delving into a more rigorous theoretical analysis of our methodology. Previous studies, such as those referenced in \cite{gal2016dropout}, have explored the connection between Dropout techniques and Bayesian learning. Inspired by this, we are interested in investigating potential theoretical links between our dynamic edge dropping approach and Bayesian inference principles. This exploration could offer deeper insights into the probabilistic foundations of our method and possibly reveal new theoretical reasoning that could explain why our Dynamic DropConnect method enhances model robustness and generalizability more effectively than traditional Dropout techniques.

In summary, our current methodology provides a significant improvement over traditional techniques. Our future efforts will focus on refining this approach by incorporating more sophisticated learning strategies and gaining deeper theoretical insight. Our goal is to further boost the robustness and accuracy of neural network training.