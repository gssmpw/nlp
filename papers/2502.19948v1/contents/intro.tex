\section{Introduction}

Dropout~\cite{hinton2012improving,srivastava2014dropout} and DropConnect~\cite{wan2013regularization} are prominent techniques designed to mitigate overfitting in deep neural networks. Dropout functions by independently zeroing each neuron within a layer with a predetermined fixed probability $p$. In contrast, DropConnect takes a slightly different approach by randomly eliminating an edge within the network with a fixed probability $p$. This makes DropConnect a generalization of Dropout; specifically, removing a single neuron as performed in Dropout equates to eliminating all incoming and outgoing edges associated with that neuron, which DropConnect facilitates.

Both Dropout and DropConnect uniformly apply a fixed dropping rate across all neurons or edges within a layer. However, this universal dropping rate might not always represent the optimal strategy; ideally, a model should utilize available data-driven insights to tailor the dropping rate for each individual edge or neuron based on their specific characteristics.

To address this, we introduce a novel methodology termed DynamicDropConnect (DDC). This approach dynamically assigns a drop probability to each edge based on the magnitude of the gradient associated with that edge. The underlying principle is that edges with larger gradients are crucial for learning and should be retained, whereas it might be acceptable to omit those with minimal impact on the model's output occasionally.

\figccwidth{linreg-contour}{The contour plot of the loss and the parameter update process. The two rows represent two sets of initial values and their updating process. The blue pluses denote the initial values. The red stars, triangles, and diamonds represent the values of $w_1$ and $w_2$ after training for 7, 13, and 19 epochs. The method that tends to drop the edges with small gradients (the second column) reaches a small error faster.}{fig:linreg-contours}{1.\textwidth}

DDC offers several advantages over other methods such as Standout~\cite{ba2013adaptive}, which also employs a dynamic dropping rate. Firstly, DDC does not require any additional learning parameters, which simplifies the model architecture and reduces memory requirements during training. Secondly, DDC provides a more deterministic and transparent approach to deciding the dropping rate, as opposed to the potentially opaque and unpredictable learning processes used by methods like Standout. Moreover, empirical evidence from our experiments demonstrates that DDC achieves superior accuracy compared to Standout.

Our experimental analysis extends beyond theoretical discussions. We conduct rigorous experiments on both synthetic and real open datasets to validate the efficacy of our methodology. Figure~\ref{fig:linreg-contours} illustrates the trajectory of parameter values and their corresponding losses using a synthetic dataset. The first column in the figure depicts a model without any regularization (labelled as ``No regularization'' in Figure~\ref{fig:linreg-contours}, whereas the second column shows the impact of training the model by preferentially dropping edges associated with smaller gradients (``Drop Small Gradient''). For comparative purposes, we also trained the same model using different variants (``Drop Big Gradient'', ``Drop Small Parameter'', and ``Drop Big Parameter''). The plots clearly demonstrate that training a linear regressor using our methodology, which preferentially drops edges with smaller gradients, reaches minimal losses more quickly than other compared methods. This encouraging result has motivated us to further explore the performance of our method when applied to more complex networks on open datasets.

The rest of the paper is organized as follows: Section~\ref{sec:rel-work} reviews related studies. Section~\ref{sec:method} details the proposed method. Section~\ref{sec:exp} evaluates various dropping strategies using different network architectures on diverse datasets. Finally, Section~\ref{sec:conc} concludes our work and outlines potential future research directions.