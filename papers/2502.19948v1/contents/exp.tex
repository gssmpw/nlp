\section{Experiments} \label{sec:exp}

This section introduces the experiment settings and results. All the models are implemented by PyTorch and trained on the NVIDIA GTX 3090. We conducted experiments based on one synthetic dataset and four open datasets: MNIST, CIFAR-10, CIFAR-100, and NORB.  
We split the labeled instances for each open dataset into the training set, the validation set, and the test set.  
Detailed settings, such as parameter initialization, learning rate, and batch size, are included in the experimental code.

\subsection{Experiments on the Synthetic Dataset} \label{sec:syn-data}

\begin{figure}[tb]
     \centering
     \begin{subfigure}{0.48\textwidth}
         \centering         \includegraphics[width=\textwidth]{fig/syn-loss-vs-iter-1.pdf}
         \caption{$w_1 = 11.5, w_2 = -10$}
         \label{fig:syn-loss-vs-iter-1}
     \end{subfigure}
     \begin{subfigure}{0.48\textwidth}
         \centering         \includegraphics[width=\textwidth]{fig/syn-loss-vs-iter-2.pdf}
         \caption{$w_1 = 9, w_2 = -10.5$}
         \label{fig:syn-loss-vs-iter-2}
     \end{subfigure}
        \caption{Loss vs.~epochs of different dropping strategies.}
        \label{fig:syn-loss-vs-iter}
\end{figure}

We generate a synthetic dataset to analyze the learning process of various edge-dropping strategies. For each instance $i$, we sample two independent features $x_1^{(i)}$ and $x_2^{(i)}$, each $x_j^{(i)} \sim N(0,1)$.  We fix the values of $w_1$ and $w_2$ and let the target $y^{(i)}$ be $w_1 x_1^{(i)} + w_2 x_2^{(i)} + \xi$, where $\xi \sim N(0,1)$.  We only use two parameters, $w_1$ and $w_2$,  because it is easier to visualize the parameter updating/learning process in a 2-dimensional contour plot.

Figure~\ref{fig:linreg-contours} shows the parameter learning process of various edge-dropping strategies: the first column (labeled ``No regularization'') represents the scenario of the no-dropping mechanism. The second column (labeled ``Drop Small Gradient'') corresponds to the DDC method. As for the other methods, ``Drop Big Gradient'' tends to drop the edges with more significant gradients; ``Drop Small Parameter'' favors dropping edges with small weights; ``Drop Big Parameter'' tends to drop edges with large weights. The horizontal and vertical axes denote the values of the estimated $w_1$ and $w_2$ at different epochs, and the contour lines represent the loss. We highlight the initial values of $w_1$ and $w_2$ and their values at the 7th, 13th, and 19th epochs by blue pluses, red stars, red triangles, and red diamonds, respectively. As shown, updating the parameters with DDC reaches small losses faster than the compared baselines. We randomly initialize the values of $w_1$ and $w_2$ and repeat the process several times.   Figure~\ref{fig:linreg-contours} shows two cases with different initializations.

We show the relationship between iteration and the losses of the compared methods in Figure~\ref{fig:syn-loss-vs-iter}. The results show that DDC gets the lowest loss when giving a fixed iteration count. The methodologies ``Drop Small Parameter'', ``Drop Big Parameter'', and ``No Dropping'' are in the middle, and ``Drop Big Gradient'' yields the largest loss given a fixed iteration count. We demonstrate the results using two sets of initial values for $w_1$ and $w_2$. Other sets of initial values give similar patterns. It appears that the parameter values per se are unlikely to be influential factors in deciding the dropping rate; the gradients are more influential.

\subsection{Experiments on Open Datasets}
\label{sec:open-data}

\begin{table}[tb]
\begin{center}  
    \caption{The test accuracies (\%) of applying SimpleCNN on MNIST.  We repeat each experiment 5 times and report the mean $\pm$ standard deviation. Let the test accuracies of ``No Dropping'' and a dropping method $m$ be $a \pm b$ and $c \pm d$, respectively, if $a+b < c-d$, we label the method $m$ with symbol $\upuparrows$ (much better). If $a < c$ but $a + b \nless c - d$, we denote $m$ with $\uparrow$ (better).  If $a-b > c+d$, we denote $\downdownarrows$ (much worse). Finally, if $a > c$ but $a - b \ngtr c + d$, we label $\downarrow$ (worse). We also highlight the method with the largest average accuracy with a boldface.} 
\label{tab:mnist-simple-cnn} 
\begin{tabular}{cl}
\toprule
Method & SimpleCNN \\
\midrule
No Dropping & $99.08 \pm 0.04$ \\
DDC               & $\boldsymbol{99.25 \pm 0.01} (\upuparrows)$ \\
Dropout           & $99.03 \pm 0.08 (\downarrow)$ \\
DropConnect       & $99.22 \pm 0.03 (\upuparrows)$ \\
Standout & $99.01 \pm 0.02 (\downdownarrows)$\\
Drop Small Parameter        & $99.24 \pm 0.01 (\upuparrows)$ \\
Drop Big Parameter          & $99.10 \pm 0.01 (\uparrow)$ \\
Drop Big Gradient         & $99.12 \pm 0.01 (\uparrow)$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The previous section shows that DDC helps linear models learn faster. This section explores DDC's capacity for more complicated deep learning models using four open datasets: MNIST~\cite{lecun2010mnist}, CIFAR-10, CIFAR-100~\cite{krizhevsky2009learning}, and NORB~\cite{lecun2004learning}.

In addition to the baselines introduced in Section~\ref{sec:syn-data}, we include Dropout, DropConnect, and Standout for comparison. We adjust the dropping rate $r^{(l)}$ in Algorithm~\ref{alg:one-layer-forward} to be close to the dropping probability of the compared baselines whenever possible.

We test the convolutional neural network used in the DropConnect paper~\cite{wan2013regularization} (called SimpleCNN below). In addition, we add two more complicated networks --AlexNet~\cite{krizhevsky2017imagenet} and VGG~\cite{simonyan2014very} -- for comparison.

MNIST contains grayscale images; the size of each image is $28 \times 28$. The dataset is simple, so we tested only different dropping strategies on SimpleCNN, which includes three convolutional layers and two fully connected layers. The detailed structure is given in~\cite{wan2013regularization}; the hyperparameters and detailed settings are included in the experimental code.

The results are presented in Table~\ref{tab:mnist-simple-cnn}. After repeating each experiment five times, we report the average accuracy $\pm$ standard deviation. As demonstrated, most test accuracies improve when dropping strategies are applied, and the proposed DDC method achieves the best accuracy among all compared methods. Although all methods exhibit accuracies above $99\%$, DDC is significantly better because it consistently shows very small standard deviations across all repeated trials.

\begin{table}[tb]
\begin{center}
\caption{The test accuracies (\%) of applying AlexNet and VGG on the CIFAR-10 dataset, the symbols $(\upuparrows)$, $(\uparrow)$, $(\downarrow)$, and $(\downdownarrows)$ are the same as in Table~\ref{tab:mnist-simple-cnn}.} 
\label{tab:cifar10-cnn-alexnet-vgg} 
\begin{tabular}{@{}ccc@{}}
\toprule
                     & \multicolumn{2}{c}{Network Structure} \\ \midrule
Method      & AlexNet     & VGG     \\ \midrule
No Dropping         & $81.04 \pm 0.04$ & $90.46 \pm 0.12$ \\
DDC      & $\boldsymbol{84.25 \pm 0.09} (\upuparrows)$ & $\boldsymbol{90.94 \pm 0.11} (\upuparrows)$ \\
Dropout & $83.86 \pm 0.09 (\upuparrows)$ & $90.66 \pm 0.07 (\upuparrows)$ \\
DropConnect  & $83.52 \pm 0.17 (\upuparrows)$ & $90.68 \pm 0.08 (\upuparrows)$ \\
Standout & $83.75 \pm 0.04 (\upuparrows)$ & $90.64\pm 0.08 (\uparrow)$ \\
Drop Small Parameter & $83.00 \pm 0.10 (\upuparrows)$ & $89.88 \pm 0.01 (\downdownarrows)$ \\
Drop Big Parameter & $83.81 \pm 0.06 (\uparrow)$ & $90.45 \pm 0.02 (\downarrow)$ \\
Drop Big Gradient & $83.19 \pm 0.16 (\uparrow)$ & $90.88 \pm 0.03 (\upuparrows)$ \\ \bottomrule
\end{tabular}
\end{center}
\end{table}

CIFAR-10 is a more challenging dataset: it contains color images; each image's size is $32 \times 32$. We use AlexNet~\cite{krizhevsky2017imagenet} and VGG~\cite{simonyan2014very} network structures for the experiments because CIFAR-10 is more complex than MNIST.

Table~\ref{tab:cifar10-cnn-alexnet-vgg} shows the results of applying AlexNet and VGG using the CIFAR-10 dataset. For both the AlexNet and VGG networks, the dropping methods mostly increase test accuracies, and our DDC performs the best among all methods in both networks.

We also test VGG and AlexNet on CIFAR-100 by changing the output softmax layer to 100 categories; all other settings are identical to the previous setup.

Table~\ref{tab:cifar100-alexnet-vgg} shows the results of applying the AlexNet and VGG on the CIFAR-100 dataset. The results are similar to those reported earlier: methods with dropping rates usually perform better than without dropping. Our DDC performs best on the VGG network and second on AlexNet.

Finally, we apply the AlexNet and the VGG network to the NORB dataset. The outcomes of these experiments are displayed in Table~\ref{tab:norb-alexnet-vgg}. Our DDC method still consistently outperforms the other techniques on both datasets. It is noteworthy that, in some cases when using the AlexNet, methodologies that involve dropping rates occasionally yield lower performance compared to methods that do not employ any dropping strategy on the NORB dataset. However, the difference is not statistically significant. By examining the mean accuracies plus or minus one standard deviation, we observe that the performance of the poorer performing methods with dropping rates overlaps with the ``No Dropping'' method.

\begin{table}[tb]
\centering
\caption{The test accuracies (\%) of applying AlexNet and VGG on the CIFAR-100 dataset, the symbols $(\upuparrows)$, $(\uparrow)$, $(\downarrow)$, and $(\downdownarrows)$ are the same as in Table~\ref{tab:mnist-simple-cnn}.}
\label{tab:cifar100-alexnet-vgg} 
\begin{tabular}{@{}ccc@{}}
\toprule
                     & \multicolumn{2}{c}{Network Structure} \\ \midrule
Method               & AlexNet              & VGG             \\ \midrule
No Dropping & $62.53 \pm 0.50$ & $71.38 \pm 0.03$ \\
DDC & $64.06 \pm 0.37 (\upuparrows)$ & $\boldsymbol{72.09 \pm 0.04} (\upuparrows)$ \\
Dropout & $ \boldsymbol{66.53 \pm 0.27} (\upuparrows)$ & $71.69 \pm 0.06 (\upuparrows)$ \\
DropConnect & $63.72 \pm 0.11 (\upuparrows)$ & $71.60 \pm 0.21 (\uparrow)$ \\
Standout & $63.97 \pm 0.38 (\upuparrows)$ & $71.53 \pm 0.31 (\uparrow)$ \\
Drop Small Parameter & $63.10 \pm 0.13 (\uparrow)$ & $71.71 \pm 0.10 (\upuparrows)$ \\
Drop Big Parameter & $63.02 \pm 0.47 (\uparrow)$ & $71.82 \pm 0.03 (\upuparrows)$ \\
Drop Big Gradient & $62.92 \pm 0.22 (\uparrow)$ & $71.13 \pm 0.07 (\downdownarrows)$ \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[tb]
\centering
\caption{The test accuracies (\%) of applying AlexNet and VGG on the NORB dataset, the symbols $(\upuparrows)$, $(\uparrow)$, $(\downarrow)$, and $(\downdownarrows)$ are the same as in Table~\ref{tab:mnist-simple-cnn}.}
\label{tab:norb-alexnet-vgg}
\begin{tabular}{@{}ccc@{}}
\toprule
                     & \multicolumn{2}{c}{Network Structure} \\ \midrule
Method               & AlexNet             & VGG             \\ \midrule
No Dropping & $92.08 \pm 0.11$ & $93.36 \pm 0.03$ \\
DDC & $\boldsymbol{92.93 \pm 0.09} (\upuparrows)$ & $\boldsymbol{94.20 \pm 0.16} (\upuparrows)$ \\
Dropout & $92.04 \pm 0.07 (\downarrow)$ & $93.79 \pm 0.25 (\upuparrows)$ \\
DropConnect & $92.09 \pm 0.14 (\uparrow)$ & $93.94 \pm 0.19 (\upuparrows)$ \\
Standout & $92.08 \pm 0.07$ & $93.81 \pm 0.08 (\upuparrows)$ \\
Drop Small Parameter & $92.42 \pm 0.10 (\upuparrows)$ & $94.02 \pm 0.05 (\upuparrows)$ \\
Drop Big Parameter & $92.02 \pm 0.04 (\downarrow)$ & $93.82 \pm 0.15 (\upuparrows)$ \\
Drop Big Gradient & $91.85 \pm 0.13 (\downarrow)$ & $94.17 \pm 0.07 (\upuparrows)$ \\ \bottomrule
\end{tabular}
\end{table}

% \subsection{DDC vs. DropConnect with Different Dropping Rates}

% We compare DDC and DropConnect using different hyper-parameters in this section. We used VGG as the network structure and trained on the NORB dataset. For DropConnect, we 

% \begin{enumerate}
% 	\item {DropConnect$(p=0.4)$}
%         \item {DropConnect$(p=0.6)$}
%         \item {GradientDropConnect\\$($Init\_Prob$=0.3, Gradient_Prob=0.25)$}
%         \item {GradientDropConnect\\$($Init\_Prob$=0.3, Gradient_Prob=0.5)$}
% \end{enumerate}

% Table~\ref{tab:Table10} shows the result of \textbf{SimpleCNN2}. In the DropConnect, we can set the parameter p. As for Gradient DropConnect, we can set the parameters $Init\_Prob$ and $Gradient\_Prob$ which correspond to the $\boldsymbol{P^{(l)}}$ matrix and $gp$ hyperparameters in Algorithm 14$\sim$17 (\ref{alg:ALGTrain}), respectively. When we increase the dropping rate of DropConnect and Gradient DropConnect, both improve in Test Accuracy. Because when Gradient DropConnect is 0 at the $Gradient\_Prob$ ratio, it will not specifically increase the dropping rates of weights with smaller gradients and will essentially become the general DropConnect method. Therefore, it is worth paying attention to Gradient DropConnect can also make the overall model perform better in the test set data by increasing the probability of discarding weights with smaller gradients under the same dropping rates of $Init\_Prob$. Also, it is not inferior to the DropConnect, increasing the dropping rate by 20\%. It is useful we increase the probability of dropping a small gradient.\\

% Table~\ref{tab:Table11} shows the result of \textbf{VGG}. In the DropConnect, we can set the parameter p. As for Gradient DropConnect, we can set the parameters $Init\_Prob$ and $Gradient\_Prob$ which correspond to the $\boldsymbol{P^{(l)}}$ matrix and $gp$ hyperparameters in Algorithm 14$\sim$17 (\ref{alg:ALGTrain}), respectively. When we increase the dropping rate of DropConnect and Gradient DropConnect, both improve in Test Accuracy. It is worth seeing that Gradient DropConnect has the same dropping probability as $Init\_Prob$. Moreover, we raise the dropping probability of weights with smaller gradients can also make the overall model perform better on the test set data and is not inferior to the DropConnect method increasing the overall dropping probability by 20\%. It also represents that it is useful that the more gradients we raise, the smaller the weights are discarded.

% \begin{table*}[tbh]
% \begin{center}  
% \caption{SimpleCNN2 in Different Drop Ratio} 
% \label{tab:Table10}  
% %\begin{adjustbox}{width={0.5\textwidth}}%
% \begin{tabular}{lllll}
% \textbf{Model} & \textbf{Train Acc(\%)} & \textbf{Test Acc(\%)} & \textbf{Train Loss} & \textbf{Test Loss} \vspace{2pt}\\
% \hline\\

% \begin{tabular}{@{}l@{}}\textbf{DropConnect}($p=0.4$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$98.90$\\ $\pm0.17$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$94.06$\\ $\pm0.28$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.046$\\ $\pm0.004$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.172$\\ $\pm0.008$\end{tabular} \\\\
% \hline\\
% \begin{tabular}{@{}l@{}}\textbf{DropConnect}($p=0.6$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$98.82$\\ $\pm0.17$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$94.36$\\ $\pm0.02$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.048$\\ $\pm0.004$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.163$\\ $\pm0.002$\end{tabular} \\\\
% \hline\\
% \begin{tabular}{@{}l@{}}\textbf{Gradient DropConnect}\\($Init\_Prob=0.3, Gradient\_Prob=0.25$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$99.11$\\ $\pm0.16$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$94.17$\\ $\pm0.14$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.042$\\ $\pm0.003$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.167$\\ $\pm0.005$\end{tabular} \\\\
% \hline\\
% \begin{tabular}{@{}l@{}}\textbf{Gradient DropConnect}\\($Init\_Prob=0.3, Gradient\_Prob=0.50$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$98.92$\\ $\pm0.10$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$\boldsymbol{94.61}$\\ $\pm\boldsymbol{0.02}$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.044$\\ $\pm0.002$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.172$\\ $\pm0.003$\end{tabular} \\\\

% \hline
% \end{tabular} 
% %\end{adjustbox}
% \end{center}  
% \end{table*}

% \begin{table*}[tbh]
% \begin{center}  
% \caption{VGG in Different Drop Ratio} 
% \label{tab:Table11}  
% %\begin{adjustbox}{width={0.5\textwidth}}%
% \begin{tabular}{lllll}
% \textbf{Model} & \textbf{Train Acc(\%)} & \textbf{Test Acc(\%)} & \textbf{Train Loss} & \textbf{Test Loss} \vspace{2pt}\\
% \hline\\

% \begin{tabular}{@{}l@{}}\textbf{DropConnect}($p=0.4$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$100.00$\\ $\pm0.00$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$97.22$\\ $\pm0.11$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.005$\\ $\pm0.000$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.084$\\ $\pm0.003$\end{tabular} \\\\
% \hline\\
% \begin{tabular}{@{}l@{}}\textbf{DropConnect}($p=0.6$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$100.00$\\ $\pm0.00$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$97.05$\\ $\pm0.18$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.005$\\ $\pm0.000$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.089$\\ $\pm0.005$\end{tabular} \\\\
% \hline\\
% \begin{tabular}{@{}l@{}}\textbf{Gradient DropConnect}\\($Init\_Prob=0.3, Gradient\_Prob=0.25$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$100.00$\\ $\pm0.00$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$97.02$\\ $\pm0.15$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.005$\\ $\pm0.000$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.090$\\ $\pm0.005$\end{tabular} \\\\
% \hline\\
% \begin{tabular}{@{}l@{}}\textbf{Gradient DropConnect}\\($Init\_Prob=0.3, Gradient\_Prob=0.50$)\end{tabular} &
%   \begin{tabular}{@{}l@{}}$99.99$\\ $\pm0.00$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$\boldsymbol{97.34}$\\ $\pm\boldsymbol{0.09}$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.005$\\ $\pm0.000$\end{tabular} &
%   \begin{tabular}{@{}l@{}}$0.217$\\ $\pm0.003$\end{tabular} \\\\

% \hline
% \end{tabular} 
% %\end{adjustbox}
% \end{center}  
% \end{table*}

% \subsection{Studying the Distribution of the Learned Parameter Values}

% \begin{figure*}[tb]
%      \centering
%      \begin{subfigure}{0.4\textwidth}
%          \centering         \includegraphics[width=\textwidth]{fig/mnist-cnn-distr-diff.pdf}
%          \caption{MNIST}
%          \label{fig:mnist-cnn-distr-diff}
%      \end{subfigure}
%      \begin{subfigure}{0.4\textwidth}
%          \centering         \includegraphics[width=\textwidth]{fig/cifar10-cnn-distr-diff.pdf}
%          \caption{CIFAR-10}
%          \label{fig:cifar10-cnn-distr-diff}
%      \end{subfigure}
%         \vspace{.15in}
%         \caption{The distribution of the learned parameter values with different dropping strategies. DropConnect and our proposed DDC result in the strongest regularization effect.}
%         \vspace{.15in}
%         \label{fig:distr-diff}
% \end{figure*}

% This section compares the distribution of the learned parameters of the non-dropping method, DDC, Dropout, and DropConnect. We use SimpleCNN as the experimental network structure. We use MNIST and CIFAR-10 as experimental datasets.Â 

% Figure~\ref{fig:mnist-cnn-distr-diff} shows the result of applying SimpleCNN to MNIST using the no-dropping method, Dropout, DropConnect, and DDC. As expected, the final parameter values generated by all the dropping techniques are closer to zero when compared with the no-dropping approach. If we define the close-to-zero numbers as those in the $[-0.1, 0.1]$ range, the no-dropping method outputs approximately 400 close-to-zero parameters. On the other hand, Dropout with dropping rates of 0.5 and 0.6 generate 550 and 450 close-to-zero parameters, respectively. In other words, a higher dropout rate does not lead to a more significant number of parameters whose values are close to zero. For DropConnect and DDC, the number of close-to-zero parameters is more than 550.

% We observe a similar trend on the CIFAR-10 dataset. As shown in Figure~\ref{fig:cifar10-cnn-distr-diff}, all dropping methods generate more close-to-zero parameters. Particularly, the no-dropping method generates approximately 1,500 close-to-zero parameters, Dropout yields 1,700 close-to-zero parameters, and DropConnect and DDC generate roughly 2,000 such parameters.

% The experimental results are consistent on different datasets: Dropout has a stronger regularization strength than the no-dropping technique, and DropConnect and DDC have a considerably greater regularization strength than Dropout. DropConnect and DDC have stronger regularization strength than Dropout, likely because DropConnect and DDC omit the edges directly during training, while Dropout discards neurons, not edges. However, the fundamental reasons still require further study.

