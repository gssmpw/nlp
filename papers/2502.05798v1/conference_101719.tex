\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
%\usepackage{algorithmic}

\usepackage{threeparttable}
\usepackage{multirow}

\usepackage{url}
\usepackage{subfigure}
\usepackage{rotating}
\usepackage{soul}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{verbatim}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{utfsym}
\usepackage{makecell}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{pifont}

\usepackage{upgreek}
\usepackage{textcomp}

\usepackage{enumitem}

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

%qst add begin
\usepackage{marvosym}
%\usepackage[misc,geometry]{ifsym}
%qst add end

\begin{document}

%\title{Towards Efficient Dynamic Scheduling in Dataflow Architectures via Architecting the Task Flow Plane\\
%\title{StreamDCIM: Streaming Digital CIM Architecture for LLM Training through 3D Parallelism\\
    %\title{StreamDCIM: A Tile-based Streaming Digital Computing-in-Memory Accelerator for Sparse Transformer via Hardware-Software Co-design\\
    %\title{StreamDCIM: A Tile-based Streaming Digital CIM Accelerator for Multimodal Transformer via Hybrid Sparse Attention and Diagonal Sparse Compression \\
    \title{StreamDCIM: A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer\\
\vspace{-0.1cm}
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{Shantian Qin, Ziqing Qiang, Zhihua Fan\textsuperscript{\Letter}, Wenming Li\textsuperscript{\Letter}, Xuejun An, Xiaochun Ye, Dongrui Fan\\
\textit{State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China}\\
\textit{School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China}\\
Email: \{qinshantian23s, qiangziqing23s, fanzhihua, liwenming, axj, yexiaochun, fandr\}@ict.ac.cn
\vspace{-0.3cm}
}

\maketitle

\begin{abstract}
%Dataflow architectures are considered promising architecture, offering a commendable balance of performance, efficiency, and flexibility. 
%Multimodal Transformers are emerging artificial intelligence (AI) models that comprehend a mixture of signals from different modalities like vision, natural language, and speech. 
%Digital computing-in-memory (CIM) architecture is considered a promising architecture with high efficiency and accuracy.
Multimodal Transformers are emerging artificial intelligence (AI) models designed to process a mixture of signals from diverse modalities. 
%Digital computing-in-memory (CIM) architecture is  as a potential hardware solution to reduce data movement with high accuracy.
Digital computing-in-memory (CIM) architectures are considered promising for achieving high efficiency while maintaining high accuracy. 
%are considered promising for achieving high efficiency and accuracy.
%However, current digital CIM-based accelerators face challenges due to inflexibility in microarchitecture, dataflow, and pipeline, limiting their ability to efficiently accelerate multimodal Transformers. 
However, current digital CIM-based accelerators exhibit inflexibility in microarchitecture, dataflow, and pipeline to effectively accelerate multimodal Transformer.
%to effectively accelerate multimodal Transformer. 
%In this paper, we propose GEMINI, a dataflow architecture with decoupled task flow and data flow planes, dedicated to efficient dynamic scheduling through hardware-software co-optimizations. 
%In this paper, we introduce GEMINI, a novel dataflow architecture that enhances dynamic scheduling through decoupled task flow and data flow planes, leveraging hardware-software co-optimizations.
In this paper, we propose StreamDCIM, a tile-based streaming digital CIM accelerator for multimodal Transformers. 
%StreamDIM introduces three levels of flexibility to overcome microarchitecture, dataflow, and pipeline challenges
It overcomes the above challenges with three features: 
First, we present a tile-based reconfigurable CIM macro microarchitecture with normal and hybrid reconfigurable modes to improve intra-macro CIM utilization. 
Second, we implement a mixed-stationary cross-forwarding dataflow with tile-based execution decoupling to exploit tile-level computation parallelism. 
Third, we introduce a ping-pong-like fine-grained compute-rewriting pipeline to overlap high-latency on-chip CIM rewriting. 
Experimental results show that StreamDCIM outperforms non-streaming and layer-based streaming CIM-based solutions by geomean 2.63$\times$ and 1.28$\times$ on typical multimodal Transformer models.
%We implement PANDA in RTL design and %demonstrate 
%Experimental results show that in a wide range of real-world applications,
%including scientific computing, artificial intelligence, digital signal processing, and graph processing algorithms, 
%GEMINI achieves up to 1.89$\times$ performance improvement and 1.67$\times$ energy efficiency improvement over the state-of-the-art dataflow architectures.
%compared to the state-of-the-art architectures, PANDA outperforms REVEL, Plasticine, DFU, and MTDE by geomean 2.53$\times$, 1.90$\times$, 1.38$\times$, and 1.19$\times$.
\end{abstract}

\begin{IEEEkeywords}
digital computing-in-memory (CIM), dataflow, multimodal transformer, reconfigurable architecture.
\end{IEEEkeywords}

\vspace{-0.15cm}
\section{Introduction}
\vspace{-0.05cm}

\begin{comment}
Advances in integrated circuit technology have propelled progress in conventional processors over the past decades. 
However, this approach is losing effectiveness with the slowdown of Moore’s Law \cite{Moore} and the termination of Dennard Scaling \cite{Dennard}. 
Fortunately, dataflow architectures exhibit great promise owing to their inherent flexibility and immense potential for high computational parallelism \cite{Marionette,TaskStreaming}.
Dataflow architectures represent a category of architectures that harness the immense potential of high computational parallelism through direct intercommunication among an array of processing elements (PEs) \cite{TACO2024,EURO}. 
A dataflow program is delineated by a dataflow graph (DFG), which is a graph that depicts operations as nodes and data dependencies as edges.
The fundamental principle of the dataflow execution model is that any DFG node can be executed as soon as all its required operands are available \cite{Dataflow}.
\end{comment}

%Advances in integrated circuit technology have propelled progress in conventional processors for decades, but this approach is losing effectiveness with the slowdown of Moore’s Law \cite{Moore} and the termination of Dennard Scaling \cite{Dennard}. Fortunately, dataflow architectures exhibit great promise owing to their inherent flexibility and immense potential for high computational parallelism \cite{Marionette,TaskStreaming}. These architectures harness the immense potential of high computational parallelism through direct intercommunication among an array of processing elements (PEs) \cite{TACO2024,EURO}. A dataflow program is delineated by a dataflow graph (DFG), which is a graph that depicts operations as nodes and data dependencies as edges. The core principle of the dataflow execution model is that any DFG node can be executed as soon as all its required operands are available \cite{Dataflow}.

%Remarkable success has been witnessed recently in the development of Transformer architecture \cite{Attention}, for both natural language processing (NLP) [2]–[10] and computer vision (CV) tasks [11]–[19]. The impressive capabilities of Transformers greatly stems from their self-attention module, which excels at extracting global context information [20]. %Typically, self-attention modules take three matrices as their inputs: namely, Q (query), K (key) and V (value). First, an attention matrix A RS×S is obtained by multiplying Q and K, where S is sequence length. Next, A goes through the softmax function for normalization, then is multiplied by V for the final output.
%Artificial intelligence (AI) has one important goal of approaching human’s multimodal perception, which is able to comprehend a mixture of vision, natural language, speech, and so on. In recent algorithm advances [12], [14], [16], [17], multimodal Transformers are promising models that can learn from different modalities as human perception, achieving excellent results on multimodal AI tasks such as video question answering, multilingual image retrieval, and action prediction.
Transformers, a type of neural network (NN) model, have achieved remarkable success across a wide range of artificial intelligence (AI) tasks, outperforming recurrent  neural networks (RNNs) and traditional convolution neural networks (CNNs) in both natural language processing (NLP) \cite{NLP} and computer vision (CV) \cite{CV}. Their exceptional performance is largely due to the attention mechanism, which effectively capture contextual knowledge from the entire input sequence. 
Additionally, a key goal in AI is to emulate human multimodal perception, enabling systems to comprehend information across various modalities, such as language and vision. Recent algorithmic advancements have highlighted the potential of multimodal Transformers in learning from diverse inputs, delivering impressive results in tasks like multilingual image retrieval and action prediction \cite{Multimodal21,Multimodal22}.

\begin{figure}[h]
\vspace{0.2cm}
\centering
  %\includegraphics[width=0.85\textwidth]{fig/predictor-new.pdf}
  \includegraphics[width=0.485\textwidth]{fig/Abstract.pdf}
  \setlength{\abovecaptionskip}{-0.35cm}
  \setlength{\belowcaptionskip}{-0.35cm}
  \caption{Overview of Different NN Accelerator Architectures.}
  \label{abstract}
  \vspace{-0.6cm}
\end{figure}

%Abundant prior works have been proposed to accelerate the NN through architectural optimization. 
%Fig. \ref{abstract} provides an overview of different NN accelerator architectures. 
%Conventional Von Neumann architectures have discrete compute and memory units. 
% 一些工作通过存算一体架构以消除存储墙瓶颈。
% 而另一些工作通过引入可重构的数字逻辑以实现弹性的计算和互联。
% 此外，很多工作也通过精心的硬件数据流设计与映射以实现计算-访存高效的片上调度。
% 数字CIM不仅比模拟CIM的精度高，而且可以更加灵活的数字逻辑，因此可以很好地兼容其他架构的优势。% 通过结合In-memory computing、reconfigurable computing和dataflow-driven computing三种范式，streaming digital CIM architecture有望实现高效的访存、计算和调度。
% TranCIM和MulTCIM的pipeline/parallel reconfigurable modes就是一种layer-based streaming, which .
% 一些工作通过存算一体架构以消除存储墙瓶颈,而另一些工作通过引入可重构的数字逻辑以实现弹性的计算和互联。 此外，很多工作也通过精心的硬件数据流设计与映射以实现计算-访存高效的片上调度。数字CIM不仅比模拟CIM的精度高，而且可以更加灵活的数字逻辑，因此可以很好地兼容其他架构的优势。Digital CIM combines the benefits of digital architectures and CIM: high efficiency is achieved by ntegrating compute into memory, while digital in-memory logic guarantees high accuracy by avoiding analog non-ideality. 而且数字存算一体架构不同于模拟存算一体架构，其数字逻辑具备极强的可定制性，通过对其数字存内逻辑进行优化，就有望融合In-memory computing、reconfigurable computing和dataflow-driven computing三种范式的优点，实现一种streaming digital CIM architecture，which is a promising architecture that可以兼顾高效的访存、计算和调度。TranCIM和MulTCIM的pipeline/parallel reconfigurable modes就是一种layer-based streaming。
%（下面的就不一定加了）
% 但这种架构受限于low hardware utilization due to single-sationary dataflow和on-chip writing latency due to coarse-grained pipeline。
% 因此，本文提出了一种tile-based streaming digital CIM accelerator with mixed-stationary cross-forwarding dataflow for multimodal Transformer。
%With the increasing size and computation of NN models, the massive data movements between the MAC units and memory will affect the overall energy efficiency. 
%Digital compute-in-memory (CIM) has emerged as a potential hardware solution to reduce data movement with high accuracy.
%Digital computing-in-memory (CIM) has proven to be an efficient matrix multiplication (MM) computation architecture with high precision support. 

Abundant prior works have been proposed to accelerate NNs through architectural optimizations. Fig. \ref{abstract} provides an overview of different NN accelerator architectures. Conventional von Neumann architectures have separate computation and memory units. Some works eliminate the memory access bottleneck by integrating multiply-and-accumulate operations directly into memory \cite{TSMC21,TSMC22}, while others leverage reconfigurable digital logic to enable flexible computation \cite{Plasticine,Thinker}. Additionally, several solutions enhance the efficiency of on-chip scheduling for both computation and memory access through well-designed hardware dataflow \cite{Sanger,SALO}.

\begin{figure*}[h]
\centering
  %\includegraphics[width=0.85\textwidth]{fig/predictor-new.pdf}
  \includegraphics[width=\textwidth]{fig/Challenges.pdf}
  \setlength{\abovecaptionskip}{-0.2cm}
  \setlength{\belowcaptionskip}{-0.2cm}
  \caption{Challenges for CIM-based Multimodal Transformer Acceleration: 1) Microarchitecture Inflexibility. 2) Dataflow Inflexibility. 3) Pipeline Inflexibility.}
  \label{challenges}
  \vspace{-0.6cm}
\end{figure*}

Digital computing-in-memory (CIM) architecture combines the advantages of both CIM and digital architectures, achieving high efficiency by embedding computation directly within memory while ensuring high accuracy by eliminating analog non-ideality through its digital in-memory logic \cite{ReDCIM,TranCIM,MulTCIM,SparseTrans}. Furthermore, unlike analog CIM, digital CIM offers greater customization, allowing for optimization of the in-memory logic. This flexibility makes digital CIM a promising solution to integrate the benefits of in-memory computing, reconfigurable computing, and dataflow-driven computing paradigms, ultimately leading to a streaming digital CIM architecture. Such an architecture holds promise to accommodate efficient memory access, computation and scheduling. The pipeline and parallel reconfigurable modes in TranCIM \cite{TranCIM} illustrate layer-based streaming, enhancing overall performance.

However, the multimodal Transformer introduces new challenges for existing digital CIM-based accelerators.
%, as it also has substantial MM computation demands and introduces a cross-modal attention mechanism to vanilla Transformers that enables joint information learning from various modalities. 
% (?不确定要不要加)哪怕是上述的layer-based streaming也受限于low hardware utilization due to single-sationary dataflow和on-chip writing latency due to coarse-grained pipeline。
Our observations indicate that current digital CIM-based accelerators exhibit \textit{\textbf{inflexibility in microarchitecture, dataflow, and pipeline}} to effectively accelerate multimodal Transformer (see Fig. \ref{challenges}).%, as illustrated in Fig. \ref{challenges}.
%Previous Transformer accelerators usually focus on reducing computation in the attention mechanism [7], [8], [13], [15], [20], [22], but fully exploiting the hybrid sparsity can bring higher end-to-end efficiency. In this article, we will discuss how to overcome the challenges of utilizing hybrid sparsity on the promising digital CIM network architecture proposed in MulTCIM [?]. 
%However, the transformer’s attention mechanism raises new challenges for CIM-based accelerator design in both memory access and computation spects (see Fig. 2).
%Detailed challenge analysis is illustrated in the rest part of Fig. 2.

%This approach typically employs a fixed attention pattern for a given task, as it is impossible to predetermine which tokens should attend to one another【14】【15】【16】. While static sparse attention is well-suited for NLP applications with long token sequences【17】, it often results in irregular patterns in both vision and multimodal Transformers due to the complex spatial relationships in images and dynamic token interactions across different modalities. These irregularities hinder achieving high throughput, even with advanced scheduling techniques, particularly for CIM-based Transformer accelerators that rely on rigid weight-stationary dataflows.
%In contrast, dynamic token pruning provides a more adaptive solution, optimizing tasks in NLP and computer vision【18】【19】【20】. Multimodal inputs, such as language and images, contain tokens of varying significance. Token pruning selectively retains more important tokens while gradually pruning less relevant ones across layers. Experimental results demonstrate that token pruning significantly reduces the computational and memory overhead of Transformers while maintaining comparable accuracy【20】.
\textit{Challenge 1 (Microarchitecture Inflexibility):} Previous digital CIM-based Transformer accelerators mainly rely on static sparse attention, overlooking the benefits of dynamic token pruning, or they lack the flexible microarchitecture for efficient token pruning. 
Static sparse attention limits attention computations to a predefined set of token pairs, reducing the computational and memory complexity in Transformer models, and is widely used in CIM-based Transformer accelerators \cite{TranCIM,MulTCIM,SparseTrans}.
This approach typically employs a fixed attention pattern for a given task, as it is impossible to predetermine which tokens should be attended to each other \cite{Longformer,BigBird,ETC}. 
While static sparse attention is well-suited for NLP applications with long token sequences \cite{LongSparse}, 
%For CV tasks, the image is divided into independent patches, and then mapped as token sequences in order. 
%These tokens have special spatial characteristics corresponding to the image itself, making the static sparse attention more complicated \cite{Evo-ViT,DynamicViT}. 
%This results in lower throughput in vision transformer even with harder scheduling, especially for CIM-based accelerator normally with rigid weight-stationary dataflow. 
it often results in irregular patterns in vision and multimodal Transformers due to the complex spatial relationships in images and dynamic token interactions across different modalities. 
These irregularities hinder achieving high throughput, even with complex scheduling method, particularly for CIM-based Transformer accelerators using rigid weight-stationary dataflows \cite{DynamicViT,Evo-ViT}.
In contrast, dynamic token pruning offers a more adaptive solution, optimizing tasks in NLP and CV \cite{SpAtten,Evo-ViT}.
%In short, these limitations create a difficult tradeoff between throughput and accuracy in previous works, and render them unsuitable for vision Transformers.
Multimodal inputs, such as language and images, contain tokens of varying significance \cite{SpAtten,DynamicViT,Evo-ViT}. 
%token pruning is a dynamic sparsity technology unique to Transformer models that is suitable for optimizing NLP and CV tasks \cite{SpAtten,Evo-ViT,DynamicViT}. 
%Plenty of unessential tokens exist in human languages, which can be pruned away to boost efficiency. Many image patches contribute very little to the final prediction in vision Transformers. 
Token pruning selectively retains attentive tokens and gradually prune inattentive ones across layers. 
Experimental results show that pruning the redundancy in image tokens can lead to over 1.6$\times$ speedup with negligible accuracy loss \cite{Evo-ViT}.
%layer-based inter-macro reconfigurable architecture
%从token pruning会对QK矩阵的规模会在运行时动态变化，导致之前的固定fixed intra-macro CIM microarchitecture 和 compute mapping strategy 会导致CIM的资源利用率低
However, token pruning introduces dynamic workloads like \textit{Matrix} $Q$, $V$, $K$ generation during runtime. In this context, the fixed intra-macro CIM microarchitecture and static workload mapping can result in low CIM utilization.
% 视觉transformer和多模态transformer的静态稀疏注意力pattern往往是irregular的，对于这种pattern即使通过复杂调度，其吞吐量依旧不高

\textit{Challenge 2 (Dataflow Inflexibility):} 
Previous digital CIM-based Transformer accelerators often employ fixed single-stationary dataflows, such as weight-stationary dataflow \cite{TranCIM,MulTCIM}, which are sub-optimal \cite{MobiLattice,MorphableCIM} and limited by layer-level computation parallelism and rigid multi-macro CIM scheduling. This limitation restricts support for tile-based execution decoupling and tile-level computation parallelism, ultimately hindering overall performance.
% fixed single-stationary dataflow
% Previous CIM-based Transformer accelerators have primarily relied on static sparse attention, overlooking the benefits of dynamic token pruning, or they lack the flexible microarchitecture for efficient token pruning. 


%Challenge 3 (Pipeline Inflexibility): Attention layers introduce dynamic matrix multiplications (QKT and PV), where both weights and inputs are generated at runtime. This leads to redundant off-chip memory access for intermediate data in traditional CIM-based Transformer accelerators using non-streaming parallel methods [5], [24], [25]. While TranCIM [12] reduces redundant off-chip accesses through a coarse-grained pipeline, it still faces significant pipeline bubbles and incurs additional on-chip CIM rewriting, resulting in high latency and energy consumption. For example, the benchmark involves QKT with INT8 precision and a K matrix size of 2048×512. With a 512-bit memory access bandwidth, TranCIM experiences over 57% latency in rewriting the K matrix in CIM macros during QKT computation. Furthermore, considering the generation of Q and K, QKT accounts for 66.7% of computations, with CIM rewriting contributing to 88.9% of the latency.

\textit{Challenge 3 (Pipeline Inflexibility):} 
Attention layers introduce dynamic matrix multiplications (such as $QK^{T}$), where both weights and inputs are generated at runtime. This leads to redundant off-chip memory access for intermediate data in traditional CIM-based Transformer accelerators using non-streaming parallel methods \cite{TSMC21,Non-streaming-2,Non-streaming-3}. 
%TranCIM \cite{TranCIM} reduces the redundant off-chip memory access through the coarse-grained pipeline but typically exists a significant number of pipeline bubbles and introduces additional on-chip CIM rewriting with high latency and energy consumption.
Although TranCIM \cite{TranCIM} mitigates redundant off-chip accesses through pipeline and parallel reconfigurable modes, it still encounters significant pipeline bubbles and incurs additional on-chip CIM rewriting due to its coarse-grained pipeline, resulting in high latency and energy consumption. 
For example, assuming a 512-bit memory access bandwidth and a benchmark involving $QK^{T}$ with INT8 precision and a $K$ matrix size of 2048$\times$512, TranCIM incurs over 57\% latency to rewrite the $K$ matrix in CIM macros during $QK^{T}$computation. When considering $Q$ and $K$ generation, $QK^{T}$comprises 66.7\% of computations, with CIM rewriting accounting for 88.9\% of the latency \cite{CIMRing}.
%Assuming a 512-bit memory access bandwidth, TranCIM incurs over 57\% latency to rewriting $K$ matrix in CIM macros during the computation of $QK^{T}$. The benchmark involves $QK^{T}$ with INT8 precision and a $K$ matrix size of 2048$\times$512. When considering $Q$ and $K$ generation, $QK^{T}$ constitutes 66.7\% of computations, with CIM rewriting accounting for 88.9\% of the latency. 
%TranCIM \cite{TranCIM} incurs over 63.8\% latency to load Matrix K in CIM macros during the computation of $QK^{T}$. When considering $Q$ and $K$ generation, $QK^{T}$ constitutes 41.6\% of computations, with data loading accounting for 43.3\% of the latency.
%coarse-grained access-execute pipeline
%self-attention是transformer效果好的关键，但对计算和内存的要求是序列长度的平方，尤其是其引入了dynamic MM，这对于采用non-streaming parallel的CIM-based加速器会导致大量非必要的片外访存；哪怕是对于采用coarse-grained pipeline的TranCIM，虽然避免了片外访存，但是引入了同样high-latency的片上rewriting。如何掩盖rewritiing的latency成为加速的关键。

\begin{comment}
\begin{table}[h]
	\centering
	\caption{Comparison of Dynamic Scheduling Methods.}
        \vspace{-0.1cm}
        \renewcommand\arraystretch{1.2}
	\resizebox{0.5\textwidth}{!}{
		\begin{tabular}{|c|c|c|c|c|}
            \hline
            %\textbf{Features}&          \makecell[l]{\textbf{Cache}\\ \cite{Manic_C, ASCELLA_C, OuterSPACE_C, Gamma_C}}& \makecell[l]{\textbf{SPM}\\ \cite{StreamDataflow,VIA,EURO,DianNao,Eyeriss,UCSR,Graph,TPDS2023}}&  \textbf{ROMA} \\
            \multicolumn{2}{|c|}{\textbf{Dynamic Scheduling}}&          \textbf{Centralized}& \textbf{Decentralized}&  \textbf{Ours} \\
            \hline
            %\multirow{2}{*}{\textbf{Dynamic Scheduling}}&          \multicolumn{2}{c|}{\textbf{Existing Method}}&  \multirow{2}{*}{\textbf{Ours}} \\
            %\cline{2-3}
            %& \textbf{Centralized}& \textbf{Decentralized} & \\
            %\hline
            \multirow{2}{*}{\textbf{Triggering}}& \textbf{Driver}& \makecell{Controller-driven} & \makecell{PE-driven} & \makecell{Dual-driven} \\   
            \cline{2-5}
            & \textbf{Feature}& \makecell{Periodic} & \makecell{On-demand} & \makecell{On-demand} \\
            %\cline{2-3}
            %Scheduling Trigger& \makecell{Controller-driven\\(Periodic)} & \makecell{PE-driven\\(On-demand)} & \makecell{Dual-driven\\(Periodic \& On-demand)} \\
            \hline
            %\multicolumn{2}{|l|}{\textbf{Workload Detection}}& \makecell{Global} & \makecell{Local} & \makecell{Global} \\
            \multicolumn{2}{|c|}{\textbf{Workload Sensing}}& \makecell{Global} & \makecell{Local} & \makecell{Global} \\
            \hline
            %On-demand Fine-grained Load
            %\multicolumn{2}{|l|}{\textbf{Decision (Trade-off)}}& \makecell{Global Optimum} & \makecell{Local Optimum} & \makecell{Global Optimum} \\
            \multicolumn{2}{|c|}{\textbf{Planning}}& \makecell{Global Optimum} & \makecell{Local Optimum} & \makecell{Global Optimum} \\
            \hline
            %Rapid and Simple Prefetching
            \multicolumn{2}{|c|}{\textbf{Task Remapping}}& \makecell{High Latency} & \makecell{High Latency} & \makecell{Low Latency} \\
            \hline
	    \end{tabular}
        }
    \label{comparison_dynamic}
    \vspace{-0.6cm}
\end{table}
\end{comment}

%Contemporary applications impose demands on the dataflow architecture’s capacity for \textbf{dynamic scheduling}. First, modern applications across pivotal domains often exhibit complex control flow patterns, such as branches or nested loops \cite{Marionette}. These control-flow constraints can hinder the partitioning of applications into program segments with approximately equal computation times. Consequently, to fully utilize multiple processing elements (PEs), more sophisticated coordination is required to dynamically feed PEs with new tasks as they complete their previous assignments \cite{picos-2019MICRO,picos-2024TC}. Second, the workload for certain tasks can only be determined at runtime, such as the number of elements matched in a join, necessitating dynamic scheduling for effective load balancing \cite{TaskStreaming,MTDE}.

\begin{comment}
%(like mesh network and reactive PEs) % moderately centralized controller
%(high-latency remapping and untimely triggering)
Our insights are that: 
%\textit{1)} Centralized dynamic scheduling is constrained by the hardware platform tailored for data flow and reactive PEs, while decentralized dynamic scheduling is limited by the absence of global sensing and planning capabilities. 
%\textit{2)} Centralized dynamic scheduling overly relies on the centralized controller, whereas decentralized dynamic scheduling depends excessively on decentralized PEs. 
\textit{1)} centralized dynamic scheduling relies heavily on the centralized controller and is constrained by the hardware tailored for data flow and reactive PEs.
\textit{2)} decentralized dynamic scheduling depends excessively on decentralized proactive PEs and lacks the global sensing and planning capabilities. 
Consequently, we advocate that an ideal dataflow architecture for dynamic scheduling should incorporate: 
\textit{1)} relatively proactive PEs with the ability to trigger scheduling on demand, enabling real-time scheduling (\textit{semi-proactive}), 
\textit{2)} a relatively simple controller responsible for global sensing and global optimal planning (\textit{global-monitoring}), and
\textit{3)} a low-latency peer-to-peer task flow to reduce communication and remapping overhead. 
\end{comment}

\begin{comment}
\begin{table*}[h]
	\centering
	\caption{Comparison of ROMA and prior works.}
        \renewcommand\arraystretch{1.2}
	\resizebox{0.96\textwidth}{!}{
		\begin{tabular}{|l|c|c|c|c|}
            \hline
            \textbf{Features}& \makecell[c] {\textbf{Static Scheduling}\cite{Hotpads}}& \makecell[c] {\textbf{Dynamic Scheduling}\\(Centralized)\cite{DeSC}}& \makecell[c] {\textbf{Dynamic Scheduling}\\(Decentralized)\cite{PDAE}}& \makecell[c] {\textbf{Dynamic Scheduling}\\(Semi-centralized)} \\
            \hline
            Scheduling Trigger& 
            \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & 
            \textcolor{teal}{\usym{2713}} (Non-pre.) \\
            %\hline
            %Dynamic Addressing& \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{teal}{\usym{2713}}} & & & & & & \textcolor{teal}{\usym{2713}} (Non-pre.) \\
            \hline
           Workload Detection&
           \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & 
           \textcolor{teal}{\usym{2713}} (Non-pre.) \\
            \hline
            %On-demand Fine-grained Load
            Decision / Trade-off&
            \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & 
            \textcolor{teal}{\usym{2713}} (Non-pre.) \\
            \hline
            %Rapid and Simple Prefetching
            Task Scheduling& 
            \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{red}{\usym{2717}}} & 
            \textcolor{teal}{\usym{2713}} (Prefetch.) \\
            %\hline
            %Direct Addressing&
            %\makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{teal}{\usym{2713}}} & 
            %\textcolor{teal}{\usym{2713}} (Prefetch.) \\
            %\hline
            %Explicit Data Orchestration& 
            %\makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{red}{\usym{2717}}} & 
            %\textcolor{teal}{\usym{2713}} (Prefetch.) \\
            %\hline
            %Precise Control& \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{teal}{\usym{2713}}} & & & & & & & \textcolor{teal}{\usym{2713}} (Prefetch.) \\
            %\hline
            %Compact Storage& \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{teal}{\usym{2713}}} & & & & & & & \makecell{\textcolor{teal}{\usym{2713}}} \\
            %\hline
            %Reconfigurable Physical Storage& 
            %\makecell{\textcolor{teal}{\usym{2713}}} & \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{red}{\usym{2717}}} & \makecell{\textcolor{teal}{\usym{2713}}} \\
            \hline
	    \end{tabular}
        }
    \label{comparison_t}
    \vspace{-0.3cm}
\end{table*}
\end{comment}

%To achieve the above objectives, we introduce GEMINI, a novel dataflow architecture with decoupled task flow plane and data flow plane dedicated to semi-centralized dynamic scheduling. Specifically, we architect a task flow plane for existing dataflow architectures, incorporating peer-to-peer task flow, along with semi-proactive PEs and a global-monitoring controller that collaboratively manage dynamic scheduling.

\begin{comment}
%To achieve the above objectives,
%we introduce GEMINI, a novel dataflow architecture with co-designed hardware and software techniques.
To achieve the above objectives,
we introduce GEMINI, a novel dataflow architecture with decoupled task flow and data flow planes, 
specifically designed for efficient dynamic scheduling through hardware-software co-optimizations.
On the hardware side, GEMINI establishes a task flow plane dedicated to semi-centralized dynamic scheduling, incorporating peer-to-peer task flow, along with semi-proactive PEs and a global-monitoring controller that collaboratively manage dynamic scheduling. 
On the software side, GEMINI adopts an adaptive load balancing scheme, combining cost-minimizing dynamic task stealing with throughput-aware dynamic migration mechanisms. 
A key insight is that the scheduling mechanisms are meticulously co-designed with the above task flow plane. 
This scheme leverages the hardware flexibility to not only reduce the dynamic scheduling overhead by considering both the dynamic reconfiguration cost and the load imbalance impact, but also to improve the dynamic scheduling efficiency by predicting throughput bottlenecks and facilitating task migration.
\end{comment}

In this paper, we propose StreamDCIM, a tile-based streaming digital CIM accelerator for multimodal Transformers. 
%It overcomes the above challenges with three key features.
StreamDCIM introduces three levels of flexibility to overcome the above challenges.
%StreamDIM introduces three levels of flexibility to overcome challenges in microarchitecture, dataflow, and pipeline: \textit{1)} a tile-based reconfigurable CIM macro \textbf{microarchitecture} with normal and hybrid reconfigurable modes to improve intra-macro utilization, \textit{2)} a mixed-stationary cross-forwarding \textbf{dataflow} to exploit tile-level computation parallelism, and \textit{3)} a ping-pong-like fine-grained compute-rewriting \textbf{pipeline} to overlap high-latency CIM rewriting.

\begin{enumerate}[label=\arabic*)]
\setlength{\itemindent}{\parindent}
% 这一部分明天再看看之前和强姐怎么写的

%We describe an ISA and its programmability, supporting the prefetch/postback and load/store for prefetchable and non-prefetchable data. It fosters the decoupling of the prefetchable data access from the application execution.
\item For \textbf{Challenge 1}, we present a tile-based reconfigurable CIM (TBR-CIM) macro \textbf{\textit{microarchitecture}} with normal and hrbrid reconfigurable modes to improve the intra-macro CIM utilization.
% 计算并行性不够 - 解决方案：从同时算一行到同时算一行加一列
% 提出了 Intra-macro reconfigurable Digital CIM Marco Arichitecture，支持tile-based的Ring状streaming与computing，可以更细粒度地充分利用计算资源

%We introduce an on-chip memory microarchitecture tailored for multi-core accelerators based on the ROMA ISA. It leverages reconfigurable physical storage shared by SPM and cache to reduce on-chip area overhead.
\item For \textbf{Challenge 2}, we implement a mixed-statinary cross-forwarding \textbf{\textit{dataflow}} with tile-based execution decoupling and elastic single-macro scheduling to exploit the tile-level computation parallelism.
%enhancing tile-based data reuse efficiency and computation parallelism.
% reloading时间很长 - 解决方案，用计算时间掩盖reloading时间
% 提出了 mixed-stationary（感觉这里不太合适？）的dataflow，既减少了一部分的reloading（不确定？），同时也通过设计高效的computing-reloading流水线对不可避免的reloading进行了overlapping

%\item We propose an adaptive semi-centralized dynamic task scheduling policy for GEMINI architecture.
%\item We propose an adaptive load balancing scheme that integrates dataflow-driven dynamic issuing and throughput-aware dynamic balancing mechanisms.
\item For \textbf{Challenge 3}, we introduce a ping-pong-like fine-grained compute-rewriting \textbf{\textit{pipeline}} to overlap the high latency of on-chip CIM rewriting.
% 多模态的输入长度不同 - 解决问题，对于cross-mode的SA计算，两个一起算，就不用区分X和Y了
% 提出了一个调度器，用于协调单模态和多模态的self-attention计算的输入和调度

%\item (Overall Workflow) 
%We present a mechanism for data mapping and on-chip memory partitioning in applications targeting ROMA-based architectures. It facilitates the organized separation of data into prefetchable and non-prefetchable categories.

%\item We implement all modules of StreamDCIM using Verilog. 
%Experimental results show that StreamDCIM outperforms state-of-the-art dataflow architectures, including Plasticine, ParallelXL, and MTDE, by geomean 1.89$\times$, 1.47$\times$, and 1.21$\times$ in a wide range of real-world applications.
%Experimental results show that SALO achieves 17.66x and 89.33x speedup on average compared to GPU and CPU implementations, respectively, on typical workloads like Longformer and ViL.

\end{enumerate}

%We implement all modules of StreamDCIM using Verilog. 
Experimental results show that StreamDCIM outperforms non-streaming and layer-based streaming CIM-based solutions by geomean 2.63$\times$ and 1.28$\times$ on typical multimodal Transformer models. 
%It reduces fine-tuning energy by 4.27× and offers 3.57× speedup for GPT-2. 
%Experimental results show that StreamDCIM outperforms state-of-the-art dataflow architectures, including Plasticine, ParallelXL, and MTDE, by geomean 1.89$\times$, 1.47$\times$, and 1.21$\times$ in a wide range of real-world applications.

\vspace{-0.05cm}
\section{StreamDCIM Designs}
\vspace{-0.05cm}

\begin{figure*}[h]
\centering
  %\includegraphics[width=0.85\textwidth]{fig/predictor-new.pdf}
  \includegraphics[width=0.97\textwidth]{fig/StreamDCIM.pdf}
  \setlength{\abovecaptionskip}{-0.05cm}
  \setlength{\belowcaptionskip}{-0.05cm}
  \caption{StreamDCIM: (a) Overall Architecture. (b) TBR-CIM Macro Microarchitecture.}
  \label{StreamDCIM}
  \vspace{-0.6cm}
\end{figure*}

%In this section, we first present the GEMINI execution model for semi-centralized dynamic scheduling, designed to achieve optimal performance in triggering, sensing, planning, and remapping. Moreover, we construct a microarchitecture for GEMINI, establishing the relationship between the semi-centralized dynamic scheduling execution model and its hardware implementation.
%Finally, we present an adaptive load balancing scheme targeting GEMINI-based architectures to extract more parallelism with fewer reconfigurations. 
%Next, we will describe these designs in detail.

%In this section, we introduce the GEMINI execution model for semi-centralized dynamic scheduling, aimed at optimizing performance across four key phases: triggering, sensing, planning, and remapping. 
%Following this, we outline the GEMINI microarchitecture, demonstrating the relationship between the execution model and its corresponding hardware implementation. 
%Finally, we propose an adaptive load balancing scheme tailored to GEMINI architecture, designed to extract greater parallelism with fewer reconfigurations. 
%Next, we will describe these designs in detail.

%To better support decentralized dataflow-driven task stealing and migration, a decentralized PE microarchitecture is constructed, as illustrated in Fig. \ref{TaskModel} (right). Detailed elaboration on this microarchitecture will follow in the next section.

%However, an essential challenge remains unresolved - how to effectively categorize the data necessitating access during application execution into the two aforementioned categories: prefetchable and non-prefetchable. To address this, we present a mechanism for data mapping and on-chip memory partitioning in applications targeting ROMA-based architectures. 

\textbf{\textit{Multimodal Transformer \& Attention Mechanism:}} 
% attention mechanism
The input to an attention layer in vanilla Transformers is typically a sequence of $N$ tokens ($I$). 
By multiplying $I$ with weight matrices $W_{Q}$, $W_{K}$, $W_{V}$, we obtain the query ($Q$), key ($K$), and value ($V$) matrices. 
Next, the attention matrix ($A$) is obtained by multiplying $Q$ and the transpose of $K$ ($K_{T}$). $A$ is then normalized using the softmax function to yield a probability matrix ($P$), which is finally multiplied by $V$ to generate the output. 
% multimodal transformer
%In addition, a typical multimodal Transformer model has a stacking structure of single-modal and cross-modal encoders. 
In multimodal Transformers, the structure often consists of stacked single-modal and cross-modal encoders to process different modalities, such as vision and language. To process two modalities, the encoders can be divided into two streams for modal $X$ and modal $Y$. 
%For example, $X$ is for vision, and $Y$ is for language. 
Single-modal attention layers are similar to those in vanilla Transformers. 
Cross-modal attention layers are introduced to facilitate information exchange between the two modalities. 
For instance, in the stream for Modal $X$, 
$Q_{X}$ come from modal $X$ ($I_{X}W_{Q}$), while $K_{Y}$ and $V_{Y}$ are from modal $Y$ ($I_{Y}W_{K}$ and $I_{Y}W_{V}$). 
A similar process occurs for modal $Y$.
%Similarly, cross-modal attention layers in the stream of Modal $Y$ generate queries from Modal $Y$ with keys and values from Modal $X$. The rest attention computation is similar to the conventional attention mechanism. 
%Similarly, in the stream for Modal $Y$, the queries come from Modal $Y$ while the keys and values come from Modal $X$.
%This cross-modal attention mechanism produces features for one modality based on the other.%, such as vision-conditioned language attention and language-conditioned vision attention. The joint information enables multimodal Transformers to comprehend deeper knowledge behind different modalities.
%In addition, attention layers represent the attention mechanism that makes transformer models different from previous NNs and helps achieve state-of-the-art accuracy. 
%The input of an attention layer is a sequence of $N$ tokens ($I$). By multiplying $I$ with the corresponding weight matrices $W_{Q}$, $W_{K}$, $W_{V}$, we obtain the query ($Q$), key ($K$), and value ($V$) matrices with $N$$\times$$d_{k}$ elements. \textit{Matrix} $Q$ and the transpose of Matrix $K$ are then multiplied to compute the attention matrix $A = Q \cdot K_{T}$, which represents each input token’s importance to the outputs. Matrix A is normalized with a softmax function to generate Matrix $P =$ softmax(($A/\sqrt{d_{k}}$)), which is finally multiplied by Matrix V to compute the attention layer’s output matrix Att = A ·V.

%The CIMFormer accelerator’s overall architecture is shown in Fig. 4, with three features highlighted in different colors. It consists of a tile-based streaming network with threee , a quantization unit, a top controller, and a 192-kB SRAM buffer. Each attention core comprises an X|W-CIM core, a PPGSS, a bidirectional write-supported CIM (BiW-CIM) array, and a systolic input broadcast unit (SIBU). The X|W-CIM core has 16 4-kb X|W-CIM macros, a CIM-output distribution network, and three accumulator arrays outside CIM macros. Previous CIM macros usually store weights and perform matrix multiplications in a weight-stationary mode. Different from previous works, each X|W-CIM macro is virtually divided into multiple columnwise sub-arrays that store weights and tokens, respectively. To reduce the CIM reloads for K matrix, the X|W-CIM core performs the cascaded matrix ultiplications that involve weights and tokens. Specifically, the computation of Q × KT is reformulated and executed as Q × WT K × XT to improve compute efficiency and reduce CIM access.
%\textbf{\textit{Overall Architecture:}}
The overall architecture of the StreamDCIM accelerator is illustrated in Fig. \ref{StreamDCIM} (a). It comprises a tile-based streaming network (TBSN) with a tile-based systolic input scheduler and three CIM cores: Q-CIM, K-CIM, and TBR-CIM. Additionally, it includes a 64-KB input buffer, a 64-KB weight buffer, a 64-KB output buffer, a dynamic token pruning unit (DTPU), a special function unit (SFU), and a global controller. The CIM cores are interconnected through the TBSN’s pipeline bus, with each core containing eight CIM macros.% (TBR-CIM array size: 4$\times$16b$\times$128, Q/K-CIM array size: 4$\times$16b$\times$64). 
%The overall architecture of the StreamDCIM accelerator, shown in Fig. 3(a), consists of a tile-based streaming network (TBSN) with a tile-based systolic input scheduler and three CIM cores: Q-CIM, K-CIM, and TBR-CIM. Additionally, it includes a 64-KB input buffer, a 64-KB weight buffer, a 64-KB output buffer, a dynamic token pruning unit (DTPU), a special function unit (SFU), and a global controller. The CIM cores are interconnected through the TBSN's pipeline bus, with each core containing eight CIM macros. The TBR-CIM macro has an array size of 4×16b×128, while the Q-CIM and K-CIM macros have an array size of 4×16b×64.

%application-daptive data prefetching and decentralized dataflow-driven dynamic scheduling. We will describe these innovations in detail in the following sections.
%Previous CIM macros usually store weights and perform matrix multiplications in a weight-stationary mode. Different from previous works, each X|W-CIM macro is virtually divided into multiple columnwise sub-arrays that store weights and tokens, respectively. To reduce the CIM reloads for K matrix, the X|W-CIM core performs the cascaded matrix multiplications that involve weights and tokens. Specifically, the computation of Q × KT is reformulated and executed as Q × WT K × XT to improve compute efficiency and reduce CIM access.
%The dashed arrows indicate three types of configurations in MulTCIM: Mode configuration for pipeline/parallel modes, layer parameters, and a sparse attention pattern; Hybrid sparsity configuration for attention, token, bit sparsity-related dynamic scheduling; Cross-modal configuration for modality switch scheduling in Pipeline StageS. 

%MulTCIM’s computing resources are organized in a digital CIM network with econfigurable modes for different layers in a multimodal Transformer. For FC layers, MulTCIM works in the parallel mode with all CIM cores storing the current layer weights and producing outputs. For attention layers (QKT-MM and A‘V-MM), CIM cores work in the pipeline for less off-chip access of intermediate data [20]. The first pipeline stage is StageS for Static MM of Q, K,V generation, and the second pipeline stage is StageD for Dynamic MM of attention computation. Take QKT-MM, for example. RTP first prunes insignificant input tokens. LRES then produces configurations of sparse token and attention scheduling for the input buffer and MACN’s StageD. The CIM cores in StageS store weight matrices (WQ, WK) and load inputs from the input buffer to generate matrixes Q and K. The output q,k vectors are merged in the StageS adder and sent to StageD through the pipeline bus. LRES controls the StageS output’s destination core in StageD for weight writing or input feeding, so StageD can compute the attention matrix A = Q · KT. Specifically for cross-modal attention layers, MACN utilizes the modal workload llocator (MWA) to adjust the StageS workload when switching streams from Modal X to Modal Y. MACN’s outputs are finally stored in the global buffer with activation functions performed in the SIMD core.

\vspace{-0.05cm}
\subsection{Tile-based Reconfigurable CIM Macro Microarchitecture}
\vspace{-0.05cm}

\begin{figure}[h]
%\vspace{0.2cm}
\centering
  %\includegraphics[width=0.85\textwidth]{fig/Dataflow.pdf}
  \includegraphics[width=0.45\textwidth]{fig/Dataflow.pdf}
  \setlength{\abovecaptionskip}{-0.15cm}
  \setlength{\belowcaptionskip}{-0.15cm}
  \caption{(a) Mixed-stationary Cross-forwarding Dataflow \& (b) Fine-grained Compute-Rewriting Pipeline (Example of the Stream for Modal X).}
  \label{Dataflow}
  \vspace{-0.7cm}
\end{figure}

Token pruning reduces computational complexity by eliminating redundant token. 
This process generally includes token ranking and selection, which can leverage attention probabilities, as the probability can indicate a token's relevance to all other tokens \cite{SpAtten,Evo-ViT}.
Each token’s importance rank can be computed by taking the column mean of its attention probability. 
Once a token is pruned, its $Q$, $K$, $V$ will be excluded from further computations. 

\begin{comment}
The TPAR aims to determine the most efficient approach for generating the Q matrix from different reformulation techniques, taking into account the token-pruning ratio. 
Similar to Evo-ViT\cite{Evo-ViT} and SpAtten\cite{SpAtten}, StreamDCIM uses the attention possibility as a natural indicator for identifying the significance of tokens in vision and language Transformers, removing redundant tokens. 
We observe that more redundant tokens can be safely removed in deeper layers, and fewer in shallower layers. It implies that the MHSA aggregates different tokens layer by layer, and a large number of similar tokens are produced in the process. 
While progressively pruning tokens, the CIM macros that previously store X can be reconfigured as WQ-CIM. In this configuration, the total memory capacity of WQ-CIM varies with the pruning ratio, which requires different types of reformulation.
\end{comment}

%Fig. 11 demonstrates the hierarchical and reconfigurable in-memory accumulators in a BM2-CIM macro. The 32 × 48 macro is divided into four 8 × 48 subarrays, so each macro has four rows of subarray adders and one macro accumulator. The minimum weight recision in ReDCIM is 8 b, so the macro’s 48 columns are divided into six groups

To facilitate efficient dynamic token pruning while maintaining high CIM utilization, the DTPU and TBR-CIM macro with normal and hybird reconfigurable modes are proposed, as illustrated in Fig. \ref{StreamDCIM}. %The TBR-CIM macro consists of three key modules: the \textit{Task Management Unit}, the \textit{Memory Unit}, and the \textit{Function Unit}.
% hybrid模式适合在token较多的前几层，normal 模式适合多次pruning之后的token较少时。这种可重构可以更好地适配动态token剪枝带来的workload减少at runtime，提高CIM的资源利用率。
Similar to Evo-ViT\cite{Evo-ViT} and SpAtten\cite{SpAtten}, StreamDCIM uses the attention possibility to identify the significance of tokens in various modalities, removing redundant tokens under the control of DTPU.
Each TBR-CIM macro consists of eight 4$\times$16b$\times$128 SRAM-CIM arrays and each array has four rows of dual-mode reconfigurable subarray adder trees and one macro accumulator. 
%We observe that more redundant tokens can be safely removed in deeper layers, and fewer in shallower layers. 
%It implies that the MHSA aggregates different tokens layer by layer, and a large number of similar tokens are produced in the process. 
%While progressively pruning tokens, the CIM macros that previously store $I$ and $W$ with the hybrid mode can be reconfigured as the CIM macros that only store $W$ with the normal mode. %In this configuration, the total memory capacity of WQ-CIM varies with the pruning ratio, which requires different types of reformulation.
As token pruning progresses, TBR-CIM macros that initially store both $I$ and $W$ in hybrid mode ($mode\_config = 0$) can be reconfigured to operate in normal mode ($mode\_config = 1$), storing only $W$. 
In normal mode, the TBR-CIM macro functions as a weight-stationary CIM macro, accelerating the $Q$, $K$, $V$ generation.
%This dynamic reconfiguration optimizes resource allocation, allowing for reduced memory usage and increased CIM efficiency.

%TBR-CIM has reconfigurable hybrid and normal modes for high-workload attention layers before token pruning and low-workload attention layers after token pruning. 
%The parallel mode is similar to the work mode of previous CIM-based accelerators [6], [15], [23]. All three CIM engines store the weights of an FC layer and work in parallel. The pipeline mode works for the two dynamic MMs in attention layers (QKT and A V). In the QKT-pipeline, TranCIM configures SEngine0 and SEngine1 as the 1st stage to compute Matrix K and Q, and DEngine as the 2nd stage to compute Matrix A = Q·KT. 
%MulTCIM’s computing resources are organized in a digital CIM network with reconfigurable modes for different layers in a multimodal Transformer. For FC layers, MulTCIM works in the parallel mode with all CIM cores storing the current layer weights and producing outputs. For attention layers (QKT-MM and AV-MM), CIM cores work in the pipeline for less off-chip access of intermediate data [20]. The first pipeline stage is StageS for Static MM of Q, K,V generation, and the second pipeline stage is StageD for Dynamic MM of attention computation. Take QKT-MM, for example. RTP first prunes insignificant input tokens. LRES then produces configurations of sparse token and attention scheduling for the input buffer and MACN’s StageD. The CIM cores in StageS store weight matrices (WQ, WK) and load inputs from the input buffer to generate matrixes Q and K. The output q,k vectors are merged in the StageS adder and sent to StageD through the pipeline bus. LRES controls the StageS output’s destination core in StageD for weight writing or input feeding, so StageD can compute the attention matrix A = Q  KT. Specifically for cross-modal attention layers, MACN utilizes the modal workload allocator (MWA) to adjust the StageS workload when switching streams from Modal X to Modal Y. MACN’s outputs are finally stored in the global buffer with activation functions performed in the SIMD core.

\vspace{-0.05cm}
\subsection{Mixed-stationary Cross-forwarding Dataflow}
\vspace{-0.05cm}

\begin{comment}
%(like mesh network and reactive PEs) % moderately centralized controller
%(high-latency remapping and untimely triggering)
Our insights are that: 
%\textit{1)} Centralized dynamic scheduling is constrained by the hardware platform tailored for data flow and reactive PEs, while decentralized dynamic scheduling is limited by the absence of global sensing and planning capabilities. 
%\textit{2)} Centralized dynamic scheduling overly relies on the centralized controller, whereas decentralized dynamic scheduling depends excessively on decentralized PEs. 
\textit{1)} Centralized dynamic scheduling relies heavily on the centralized controller and is constrained by the hardware platform tailored for data flow and reactive PEs.
\textit{2)} Decentralized dynamic scheduling depends excessively on decentralized proactive PEs and is limited by the absence of global sensing and planning capabilities. 
Consequently, we advocate that an ideal dataflow architecture for dynamic scheduling should incorporate: 
\textit{1)} relatively proactive PEs with the ability to trigger scheduling on demand, enabling real-time scheduling (\textit{semi-proactive}), 
\textit{2)} a relatively simple controller responsible for global sensing and planning (\textit{global-monitoring}), and
\textit{3)} a network for low-latency peer-to-peer task flow to reduce communication overhead. 
\end{comment}

%To better support decentralized dataflow-driven task stealing and migration, a decentralized PE microarchitecture is constructed, as illustrated in Fig. \ref{TaskModel} (right). Detailed elaboration on this microarchitecture will follow in the next section.

%Overall, we integrate semi-proactive PEs to enable on-demand real-time dynamic scheduling triggering, a global-monitoring controller for global sensing and global optimal planning, and a low-latency peer-to-peer task flow to reduce communication overhead, as illustrated in Fig. \ref{GEMINI} (c).

\begin{figure}%[htbp]
  \vspace{-0.3cm}
\centering
  \includegraphics[width=0.43\textwidth]{fig/AreaPower.pdf}
  %\setlength{\abovecaptionskip}{-0.33cm}
  %\setlength{\belowcaptionskip}{-0.33cm}
  \vspace{-0.2cm}
  \caption{StreamDCIM: (a) Area Breakdown. (b) Power Breakdown.}
  \label{A&P}
  \vspace{-0.6cm}
\end{figure}

To support the tile-based execution decoupling, the mixed-stationary cross-forwarding dataflow is introduced, as shown in Fig. \ref{Dataflow} (a) for an example of modal $X$. 
%This dataflow strategy manages data movement efficiently by enabling simultaneous row-wise and column-wise data propagation across CIM macros. 
For instance, in the $1^{st}$ cycle, TBR-CIM $\#0$ provides row-wise $I_Y$ and column-wise $W_V$ as inputs to other CIM macros.  
Each row from $(I_Y)_{0}$ is sent to the $W_V$ part of TBR-CIM $\#0$-$7$, generating a full row in $V$. 
Concurrently, each column from $(W_V)_{0}$ is sent to the $I_Y$ part of TBR-CIM $\#1$-$7$, producing a portion of a column in $V$. 
%This cross-forwarding approach leverages the mixed-stationary configuration, where data is kept stationary in optimal segments to facilitate efficient, parallel updates across the CIM array, maximizing throughput in the tile-based execution model. 
This cross-forwarding computation approach leverages the mixed-stationary TBR-CIM configuration, facilitating more frequent reuse of stored data and utilizing tile-level computation parallelism. 
%facilitating tile-based execution decoupling and utilizing tile-level computation parallelism. 
Additionally, the calculation of $Q_{X}K_{Y}^{T}$ operates as the inverse of the process for $I_{Y}W_{V}$, which also exploits the mixed-stationary cross-forwarding dataflow. Conversely, the calculation of $I_{X}W_{Q}$ and $I_{Y}W_{K}$ follows the fixed weight-stationary dataflow.

%Weight-stationary $Q$ and $K$ generation

%Mixed-stationary cross-forwarding dataflow: $I_{Y}$$\times$$W_{V}$ and $Q_{X}$$\times$$K_{Y}^{T}$ 

%\subsection{Overall Workflow}
\subsection{Ping-pong-like Fine-grained Compute-Rewriting Pipeline}

To overlap the high latency associated with on-chip CIM rewriting, the ping-pong-like fine-grained compute-rewriting pipeline is proposed, as shown in Fig. \ref{Dataflow} (b). 
%In Pipeline StageD, the CIM macros have to hold infrequently used weights for a long time, leading to rigid multi-macro CIM scheduling.
%Thus, inputs and weights stored in CIM can be reused more frequently to improve CIM utilization. 
The above mixed-stationary cross-forwarding dataflow enables tile-based computation parallelism and elastic single-macro CIM scheduling, providing the possibility for a more finer-grained pipeline. 
In this pipeline, specific resources can be dynamically released and reallocated. 
For instance, in the $2^{nd}$ cycle, the inputs and weights stored in TBR-CIM $\#0$ are no longer required for ongoing calculations, freeing TBR-CIM $\#0$ for immediate rewriting. 
Meanwhile, $(Q_X)_0$ and $(K_Y)_0$ computations have been completed in Q-CIM $\#0$-$7$ and K-CIM $\#0$-$7$, respectively. 
%In this case, the rewriting of $(Q_X)_0$ and $(K_Y)_0$ and the computation of $(Q_X)_1$, $(K_Y)_1$, and $(V_Y)_1$ can be executed in parallel, overlapping the rewriting latency and enhancing the overall throughput. 
This timing permits the parallel execution of rewriting $(Q_X)_0$ and $(K_Y)_0$ alongside new computations of $(Q_X)_1$, $(K_Y)_1$, and $(V_Y)_1$.
Such parallelism effectively overlaps rewriting latency, enhancing throughput by ensuring continuous resource utilization across cycles.
%讲下QKV生成和QKT计算的掩盖，以及QK生成的rewriting

%Ping-pong-like Rewriting Overlapping:

%The SFU performs scaling and softmax on A to generate Matrix A = softmax((A/dk)). The A V-pipeline has a similar dataflow, except that the 2nd stage’s input A is loaded from the global buffer instead of the 1st stage. SEngines are combined to compute Matrix V, while DEngine computes the attention layer’s output matrix Att = AV. In this article, we will mainly use the QKT MMas example to explain the technical features of TranCIM.

\section{Experimental Results}

\subsection{Experiment Settings}

% non-streaming 的描述去 tranCIM的第三页和第四页抄
% layer-based streaming 的描述 去TranCIM和 MulTCIM的challenge 前面那一阵去抄

\begin{comment}
\begin{table}[htbp]
%\vspace{-0.15cm}
\caption{Baseline Architecture Configuration.}
%\vspace{0.1cm}
%\setlength{\abovecaptionskip}{-0.5cm}
%\setlength{\belowcaptionskip}{-0.7cm}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|c|c|l|c|l|}
\hline
\multicolumn{2}{|c|}{\textbf{Components}}& \makecell[c]{\textbf{Parameters}}&\textbf{Components}&\makecell[c]{\textbf{Parameters}} \\
\hline
\multirow{3}{*}{\textbf{PE}} & \textbf{Conf. Buffer} & 16KB total size &\textbf{Network on Chip}& 2D mesh network \\
\cline{2-5}
& \textbf{Inst. Buffer}& 4KB total size& \textbf{On-chip Memory}&  
64KB total size  \\
\cline{2-5}
& \textbf{Func. Units}& Load, Store, etc &\textbf{Off-chip Memory} & 2 channels, DDR4 \\
\hline
\multicolumn{2}{|c|}{\textbf{PE Array}}& 4 $\times$ 4 &\textbf{System Data BUS} & 16B width \\
\hline
\end{tabular}}
\label{setup}
\end{center}
\vspace{-0.45cm}
\end{table}
\end{comment}

\begin{comment}
\begin{table}
\scriptsize
%\footnotesize
%\small
%\tiny
%\vspace{-0.2cm}
\centering
  \caption{Design Parameters and Area and Power Breakdown (28nm).}
  \vspace{-0.1cm}
  \label{hardware}
  \renewcommand{\arraystretch}{1.2}
  \resizebox{0.46\textwidth}{!}{
  %\renewcommand\tabcolsep{3.0pt}
  \begin{tabular}{|c|l|l|l|}
  \hline
  \multicolumn{2}{|c|}{\textbf{Component (Parameter)}} & \textbf{Area (}\textit{mm$^2$}\textbf{)} & \textbf{Power (}\textit{mW}\textbf{)} \\
  \hline
  \multirow{3}{*}{\textbf{PE}} & \textbf{Function (LD/ST/CAL/FLOW)} &0.014 (0.62\%)  & 10.9 (0.73\%) \\
  \cline{2-4}
  & \textbf{Control (Task Management)} &0.011 (0.49\%)  & 11.8 (0.79\%) \\
  \cline{2-4}
  & \textbf{Memory (4KB Inst/Data RAM)} &0.017 (0.76\%)  &12.6 (0.85\%)  \\
  %\cline{2-4}
  \hline
  \multicolumn{2}{|l|}{\textbf{PE Array (4$\times$4 Semi-proactive, SIMD)}} & 0.672 (29.9\%)  &564.8 (37.9\%) \\
  \hline
  \multicolumn{2}{|l|}{\textbf{Controller (Global Config. \& Monitor.)}} & 0.206 (9.2\%)  &174.3 (11.7\%) \\
  \hline
  \multicolumn{2}{|l|}{\textbf{Network (16 Routers, 2D Mesh)}} & 0.301 (13.4\%)  &191.2 (12.8\%) \\
  \hline
  \multicolumn{2}{|l|}{\textbf{Data Buffer (256KB Total Size)}} & 0.795 (35.3\%)  &408.2 (27.4\%) \\
  \hline
  \multicolumn{2}{|l|}{\textbf{Instruction Buffer (64KB Total Size)}} & 0.275 (12.2\%)  &152.5 (10.2\%) \\
  %\hline
  %\multicolumn{2}{|l|}{\textbf{Configuration Buffer (8KB total size)}} & 0.063 (5.5\%)  &57.2 (4.7\%) \\
  \hline
  \multicolumn{2}{|l|}{\textbf{\textit{GEMINI (Total)}}} & \textbf{\textit{2.249}}  & \textbf{\textit{1491}} \\
  \hline
\end{tabular}}
\vspace{-0.4cm}
\end{table}
\end{comment}

\begin{comment}
\begin{table}[htbp]
\caption{Profile of Applications.}
\begin{center}
\vspace{-0.1cm}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.46\textwidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Application} & \textbf{Domain} & \textbf{Data Sizes} & \textbf{Feature}\\
\hline
  \textbf{GEMM} & \multirow{3}{*}{\makecell[c]{Scientific\\Computing}} & 128$\times$128& \textit{Regular} \\
  \cline{1-1} \cline{3-4}
  \textbf{Stencil-3d7p} & & 16$\times$32$\times$25K & \textit{Regular} \\
  \cline{1-1} \cline{3-4}
  \textbf{SHA-256} & & 8K Points & \textit{Regular} \\
\hline
  \textbf{SVD} & \multirow{2}{*}{\makecell[c]{Artificial\\Intelligence}} & 128$\times$128 & \textit{Near-irregular} \\
  \cline{1-1} \cline{3-4}
  %\textbf{ReLU-3d} & & 8$\times$8-32 & 32$\times$32-32 & NR \\
  %\cline{1-1} \cline{3-5}
  \textbf{Convolution-3d} & & 32$\times$32-32 & \textit{Regular} \\
\hline
  \textbf{FFT} & \multirow{3}{*}{\makecell[c]{Signal\\Processing}} & 1K Points& \textit{Near-irregular} \\
  \cline{1-1} \cline{3-4}
  \textbf{Cholesky} & & 128$\times$128& \textit{Near-irregular} \\
  \cline{1-1} \cline{3-4}
  \textbf{Lagrange} & & 4K Points & \textit{Near-irregular} \\
  %\cline{1-1} \cline{3-5}
  %\textbf{Lagrange} & & 128 Points & 4K Points& NIR \\
\hline
  \textbf{BFS} & \multirow{2}{*}{\makecell[c]{Graph\\Processing}} & $|$V$|$=1K $|$E$|$=128K & \textit{Irregular} \\
  \cline{1-1} \cline{3-4}
  %\textbf{PageRank} & & 100 iters 7680 pages & 100 iters 7680 pages & IR \\
  \textbf{PageRank} & & $|$V$|$=1K $|$E$|$=128K & \textit{Irregular} \\
  %\textbf{BFS} & Graph & $|$V$|$=9K $|$E$|$=5M & $|$V$|$=9K $|$E$|$=5M & IR \\
\hline
  %\multirow{2}{*}{\textbf{BFS}} & \multirow{2}{*}{\makecell[l]{Breadth First\\Search}} & 1& & &  \\
  %\cline{3-6}
  %& & 2& & & \\
%\hline
\end{tabular}}
\begin{tablenotes}
\item[] \textit{In the table, all data types are floating-point 32-bit.}
\end{tablenotes}
\label{benchmarks}
\end{center}
\vspace{-0.88cm}
\end{table}
\end{comment}

% hardware comparison
\begin{comment}
\begin{table*}[h]
\begin{center}
	\centering
        \setlength{\abovecaptionskip}{-0.1cm}
        \setlength{\belowcaptionskip}{-0.1cm}
	\caption{Hardware Comparison with State-of-the-art Reconfigurable Dataflow Architectures}
        \renewcommand\arraystretch{1.2}
	\resizebox{1.0\textwidth}{!}{
		\begin{tabular}{|l|c|c|c|c|c|c|c|}
			\hline 
                    %\textbf{Architectures} & \textbf{Tech. (}\textit{nm}\textbf{)} & \textbf{Area (}\textit{\textmu m$^2$}\textbf{)} & \textbf{Max. Power(}\textit{W}\textbf{)} & \textbf{Frequency (}\textit{GHz}\textbf{)} & \textbf{Peak Perf. (}\textit{GFLOPS}\textbf{)} & \textbf{Energy Effi. (}\textit{GFLOPS/W}\textbf{)}  \\
                    \textbf{Architecture} & \textbf{Scheduling Approach} & \textbf{Tech.} & \textbf{Area} & \textbf{Max. Power} & \textbf{Frequency} & \textbf{Peak Performance} & \textbf{Energy Efficiency}  \\
			%\hline
                   %\textbf{REVEL} & Static + Hybrid & 128KB (Shared SPM) & 28 nm & 2.709 mm$^2$ & 2.282 W & 1.25 GHz & 320 GFLOPS & 25.99$\sim$137.29 GFLOPS/W  \\
                \hline
                    \textbf{Plasticine \cite{Plasticine}} & Static Scheduling & 28 nm & 3.760 mm$^2$ & 1.933 W & 1 GHz & 410 GFLOPS & 62.31$\sim$157.93 GFLOPS/W  \\
                %\hline    
                    %\textbf{DFU} & Static + Cen. Dataflow. & 512KB(D)+128KB(C) & 28 nm & 10.28 mm$^2$ & 2.040 W & 1.25 GHz & 320 GFLOPS & 88.06$\sim$154.25 GFLOPS/W  \\
			\hline
                    \textbf{MTDE \cite{MTDE}} & Central. Dynamic Sched. & 28 nm & 2.816 mm$^2$ & 1.282 W & 0.8 GHz & 410 GFLOPS & 89.15$\sim$197.67 GFLOPS/W  \\
			\hline
                    \textbf{ParallelXL \cite{ParallelXL}} & Decentral. Dynamic Sched. & 28 nm & 2.544 mm$^2$ & 1.276 W & 0.8 GHz & 410 GFLOPS & 73.68$\sim$161.19 GFLOPS/W  \\
			\hline
                    \textbf{GEMINI} & Semi-central. Dynamic Sch. & 28 nm & 2.249 mm$^2$ & 1.491 W & 1.25 GHz & 320 GFLOPS & 92.66$\sim$223.65 GFLOPS/W  \\
			\hline
	\end{tabular}}
    \vspace{-0.5cm}
    %\begin{tablenotes}
    %\item[] \textit{In the table, H/S/D is short for hybrid static/dynamic, static, and dynamic scheduling. NDF/CDF/DDF is short for non-dataflow-driven, centralized dataflow-driven, and decentralized dataflow-driven scheduling.}
    %\end{tablenotes}
     \label{HC_SOTA}
\end{center}
\end{table*}
\end{comment}

%\textbf{Setup.} 
\textbf{Methodology.} 
%We implement ROMA microarchitecture in Verilog. We synthesize the design with Synopsys Design Compiler with a commercial 16nm technology with a 1GHz frequency. We use Synopsys PrimeTime PX for accurate power analysis. Table \ref{setup} shows our baseline architecture. For a detailed justification of the design choices see\cite{LRP}. 
%We develop a cycle-accurate micro-architecture simulator for hardware utilization and performance evaluation. The simulator is developed in C language based on SimICT framework [16] and can simulate behaviors such as memory access, data transfer, scheduling, etc. We calibrate the error to within ±7\% using RTL environment. We also implement our architecture using Verilog. We use Synopsys Design Compiler and a TSMC 28nm GP standard VT library to synthesize it and obtain area, delay and energy consumption, which meets timing at 1GHz. Table \ref{setup} shows the hardware parameters.
We implement all modules of StreamDCIM using Verilog. We synthesize the design using Synopsys Design Compiler with a commercial 28nm technology, meeting timing at a 200 MHz frequency. We use Synopsys PrimeTime PX for accurate power analysis. %Table \ref{hardware} shows the hardware design parameters, as well as the evaluated area and power breakdown of essential modules in StreamDCIM. %The DFGC simulator was also developed and was calibrated to cycle-level accuracy.
%Evaluation shows that GEMINI has an area footprint of 2.249 mm$^2$ in a 28nm process, and consumes a maximum power of 1.491 W.
%\textbf{Benchmark.} 
To evaluate our design, we select two typical multimodal Transformer models, ViLBERT-base and ViLBERT-large \cite{ViLBERT}, with the visual question answering (VQA) v2.0 dataset. To maintain high accuracy, the attention layers use INT16 precision.  In our configuration, \textit{X} is the vision modality, while \textit{Y} is the language modality, each with a token count of $N_X = N_Y = 4096$.
%These applications are crucial in pivotal domains such as scientific computing, artificial intelligence, signal processing, and graph processing.
%For our experiments, we classify the applications based on their control structures, computation, and memory access patterns. 
%\textit{1) Regular Applications:} GEMM, STE, SHA, and CONV exhibit highly regular and fixed computation and memory access patterns, making them well-suited for balanced task partitioning. 
%\textit{2) Near-irregular Applications:} FFT, CHO, LAG, and SVD feature complex control flows with imperfect loops and nested branches, which complicates efficient task partitioning. 
%\textit{3) Irregular Applications:} BFS and PR involve both complex control flows and dynamic workloads that can only be determined at runtime. 
%These applications suffer from imbalanced workloads, large amounts of random memory accesses across diverse memory regions, and redundant computations resulting from the traversal irregularity of edges and the update irregularity of vertex property \cite{GraphDynS}.
%Table \ref{benchmarks} provides a summary of the applications.

\begin{comment}
Table \ref{benchmarks} shows a list of applications and their characteristics.
To evaluate our design, we select several real-world applications from Plasticine [9], MTDE [13], and ParallelXL [16]. These applications are classified based on their memory access and computation patterns:
Regular Applications: GEMM, STE, CONV, and FIR — These have highly regular, fixed memory access and computation patterns.
Near-regular Applications: SHA, ReLU, and SVD — These exhibit relatively regular patterns with some small-scale, discrete parameter accesses and imperfect loops.
Near-irregular Applications: FFT, LAG, and CHO — These involve complex computations with large spans of memory access addresses.
Irregular Applications: BFS and PR — These feature imbalanced workloads, frequent random memory accesses across different regions, and redundant computations due to edge traversal and vertex property update irregularities [18].
Table III provides an overview of these applications and their characteristics
\end{comment}

% old definition of Near-regular: hese applications contain a small amount of non-prefetchable data and involve cross-stride computations.

% HPCC Benchmark Profile
\begin{comment} 
\begin{table}[htbp]
\caption{Profile of Benchmarks.}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{|c|l|c|c|c|c|}
\hline
\begin{tabular}[c]{@{}l@{}}\textbf{Bench.}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Description}\end{tabular} &\begin{tabular}[c]{@{}l@{}}\textbf{Scale}\end{tabular}  & \begin{tabular}[c]{@{}l@{}}\textbf{Scalars}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Arrays}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{Tot. Size (B)}\end{tabular}\\
\hline
  \multirow{2}{*}{\textbf{GEMM}} & \multirow{2}{*}{\makecell[l]{General Matrix\\Multiplication}} & 64$\times$64& 0& 3& 49152 \\
  \cline{3-6}
  & & 128$\times$128& 0& 3& 196608\\
\hline
  \multirow{2}{*}{\textbf{SVD}} & \multirow{2}{*}{\makecell[l]{Singular Value\\Decomposition}} & 32$\times$32& 0& 2& 8192 \\
  \cline{3-6}
  & & 128$\times$128& 0& 2& 131072\\
\hline
  \multirow{2}{*}{\textbf{ReLU}} & \multirow{2}{*}{\makecell[l]{Rectified Linear\\Unit Activation}} & 8$\times$8-32 & 1& 64& 16388 \\
  \cline{3-6}
  & & 32$\times$32-32& 1& 64& 262148\\
\hline
  \multirow{2}{*}{\textbf{CONV}} & \multirow{2}{*}{\makecell[l]{Convolution}} & 8$\times$8-32& 288& 64& 17536 \\
  \cline{3-6}
  & & 32$\times$32-32& 288& 64& 263296\\
\hline
  \multirow{2}{*}{\textbf{FFT}} & \multirow{2}{*}{\makecell[l]{Fast Fourier\\Transform}} & 1K$\times$1K& 512& 2& 20480 \\
  \cline{3-6}
  & & 8K$\times$8K& 4096& 2& 163840\\
\hline
  \multirow{2}{*}{\textbf{LAG}} & \multirow{2}{*}{\makecell[l]{Lagrange\\Interpolation}} & 128$\times128$& 128& 3& 11264 \\
  \cline{3-6}
  & & 4K$\times$4K& 4096& 3& 360448 \\
\hline
  %\multirow{2}{*}{\textbf{BFS}} & \multirow{2}{*}{\makecell[l]{Breadth First\\Search}} & 1& & &  \\
  %\cline{3-6}
  %& & 2& & & \\
%\hline
\end{tabular}}
\label{benchmarks}
\end{center}
%\vspace{-0.4cm}
\end{table}
\end{comment}

\begin{comment}
\textbf{Comparison Methodology.} To quantify the performance of our design, we compare it to three state-of-the-art reconfigurable dataflow designs: Plasticine \cite{Plasticine}, MTDE \cite{MTDE}, and ParallelXL \cite{ParallelXL}.
%REVEL comprises simple PEs in a systolic form and dataflow PEs capable of performing complex computations. It features hybrid static-dynamic task issuing and static task mapping.
Plasticine decouples dedicated pattern compute units and memory units. It features static scheduling. 
%We leverage the open source implementations of REVEL and Plasticine for each design for performance evaluation.
%DFU is a reconfigurable dataflow architecture for multi-batch data processing. It employs dynamic task issuing and static task mapping.
%We use the experimental data from the original paper.
MTDE is a multi-task dataflow engine that supports the elastic task scheduling scheme, featuring centralized dynamic scheduling. 
For fairness, these designs are extended to achieve similar peak performance and process. The clock frequencies are configured based on the specifications from their original papers.
\end{comment}

\textbf{Comparison.} %Our comparison is between semi-centralized dynamic scheduling (GEMINI), static scheduling (Plasticine \cite{Plasticine}), centralized dynamic scheduling (MTDE \cite{MTDE}), and decentralized dynamic scheduling (work-stealing). For work-stealing, our implementation is based on the description in ParallelXL \cite{Plasticine}.
We compare StreamDCIM using tile-based streaming solution (\textbf{Tile-stream}) with two digital CIM-based accelerators: non-streaming solution and layer-based streaming solution. 
The non-streaming solution (\textbf{Non-stream}) operates similarly to the conventional work mode of previous CIM accelerators \cite{TSMC21,Non-streaming-2,Non-streaming-3}. 
In Non-stream, dynamic matrix multiplication in the attention layers lead to redundant off-chip memory access for intermediate data, which negatively impacts performance and efficiency. 
The layer-based streaming solution (\textbf{Layer-stream}) aligns with the parallel and pipeline reconfigurable mode of TranCIM \cite{TranCIM}. 
In Layer-stream, on-chip CIM rewriting during execution due to layer-based streaming introduces high latency.
%In Layer-stream, dynamic matrix multiplications computation in attention layers will cause redundant off-chip memory access for intermediate data. 
%
%Plasticine \cite{Plasticine} employs static scheduling and decouples dedicated compute and memory units. 
%We leverage the open source implementations of REVEL and Plasticine for each design for performance evaluation.
%DFU is a reconfigurable dataflow architecture for multi-batch data processing. It employs dynamic task issuing and static task mapping.
%We use the experimental data from the original paper.
%MTDE \cite{MTDE} is a multi-task dataflow engine that supports the elastic task scheduling scheme, featuring centralized dynamic scheduling. 
%ParallelXL \cite{ParallelXL} is an architectural framework for accelerating both static and dynamic parallel algorithms on reconfigurable hardware, utilizing decentralized dynamic scheduling through work stealing.
%For fairness, these designs are extended to achieve similar peak performance. The clock frequencies are configured based on the specifications from their original papers.

%We compare GEMINI with three reconfigurable dataflow architectures: Plasticine [9], MTDE [13], and ParallelXL [16]. Plasticine features static scheduling and decouples dedicated compute and memory units. MTDE, a multi-task dataflow engine, supports an elastic task scheduling scheme with centralized dynamic scheduling. ParallelXL accelerates both static and dynamic parallel algorithms on reconfigurable hardware, utilizing decentralized dynamic scheduling through work stealing. For a fair comparison, these architectures are extended to match similar peak performance and process, with clock frequencies configured according to their original specifications."

%Our comparison is between elastic scheduling (MTDE), static scheduling (TIA), and work-stealing. For work-stealing, our implementation is based on the description in ParallelXL[36]. We remove the TSU and add a task stealing queue in the TCU, thereby each TCU can steal tasks from others. Besides, several instructions are added to PEs for dynamic task generation. For a fair comparison, all of the three architectures have the same computation substrate provisioned with the same number of 4 4 PEs and a 4-bank scratchpad memory, as evaluated in the literature[55]. Our choice of 2 2 subarray and 4 TCUs is mainly to balance the degree communications. In this scenario, one TCU communicates with 4 TCUs and each TCU communicates with 4 PEs. In addition, an array should be able to execute at least 2 tasks for combination. In our experiment, a 2 2 TIA subarray accommodates up to 32 instructions (8 insts per PE), typically suitable for 2 simultaneous tasks.

%We leverage the open source implementations of them and develop a simulator for each design for performance evaluation. We also implement their main components using Verilog to obtain area and power consumption. 

%We extend these architectures to have similar peak performance and process technology, using the configurations for clock frequencies from their original papers.
%Our baseline architecture has SPM and DMA as described above. To evaluate the ROMA, we replaced the DMA-assisted SPM with ROMA, following the design in Section \Rmnum{4}. We also compared ROMA to a cache-only configuration. Overall, we evaluate the following configurations:

\begin{comment}
\begin{itemize}

\item First, we evaluate the overall performance of PANDA architecture compared to the traditional dataflow architecture.

\item Second, we evaluate the dataflow architecture of adaptive prefetching. In order to verify the optimization effect on Branch Divergence, for a fair comparison, we do not consider the decentralized scheduling.
%Besides, we conduct a DC synthesis experiment on the control network delay under different frequencies and network stages. Moreover, we compare the network normalized area with the state-of-the-art architecture.

\item Subsequently, we evaluate the dataflow architecture of decentralized scheduling.
%Furthermore, we compute the utilization improvement of the PE that originally executed the outer BB and pipeline utilization, and analyze the relationship between the result of the peer-to-peer control network and the Agile PE Assignment.

\item In addition, we built the performance models of REVEL, DFU, and PANDA with the simulator and normalized the computing fabric to the same size to compare the performance.
%, TIA [49], REVEL [70] (15 systolic PEs, 1 tagged-dataflow PE), Riptide [28] (16 fully functional PEs and 25 control flow operators inside network) and Marionette with the simulator and normalized the computing fabric to the same size to compare the performance.

\item Finally, we evaluate the hardware and software overhead of the adaptive prefetching and decentralized scheduling.

\end{itemize}
\end{comment}

%Table \ref{metric} shows the SPM, cache, and HDSC configurations used for the experiments. 

\subsection{Area and Power Breakdown}

%\vspace{-0.3cm}
%\subsection{GEMINI Outperforms State-of-the-art Architectures}
%\vspace{-0.1cm}

%In this section, we compare GEMINI with state-of-the-art reconfigurable dataflow architectures.
%For comparison, we use three typical reconfigurable dataflow architectures: Plasticine \cite{Plasticine}, MTDE \cite{MTDE}, and ParallelXL \cite{ParallelXL}.
%We extend these architectures to have similar peak performance and process technology, using the configurations for clock frequencies from their original papers.
%Due to different design objectives and target applications, there is a significant disparity in peak performance among these designs. We made efforts to mitigate the influence of peak performance and process technology disparity during the evaluation. 
%Table \ref{HC_SOTA} shows the hardware comparisons of our architecture with other designs.

Fig. \ref{A&P} shows the evaluated area and power breakdown of the essential modules in StreamDCIM.
Evaluation shows that StreamDCIM has a chip area of 12.10 mm$^2$ in a 28nm process, and consumes a maximum power of 122.77 mW.
%The highparallelismof the hardware helps toamortize thecontrol overhead.Adetailedsynthesis of the internal structureof PEshows that thehardware for dataflowcontrolon16PEsoccupies3.2\% ofthetotalarea,and thedesignemployed for TimeStamp scheduling incurs little overhead,with0.03\%perPEintotalarea.

\subsection{Performance and Energy Comparison}

Fig. \ref{Speedup} illustrates the performance comparison between StreamDCIM and both non-streaming and layer-based streaming solutions on the ViLBERT-base and ViLBERT-large models. 
%On ViLBERT-base, our work achieves 2.86$\times$ and 1.25$\times$ speedup over Non-stream and Layer-stream. On ViLBERT-large, our work achieves 2.42$\times$ and 1.31$\times$ speedup  over Non-stream and Layer-stream. 
Specifically, StreamDCIM achieves a speedup of 2.86$\times$ and 1.25$\times$ over Non-stream and Layer-stream, respectively, on ViLBERT-base model. For ViLBERT-large model, StreamDCIM provides a speedup of 2.42$\times$ compared to Non-stream and 1.31$\times$ over Layer-stream.
%Plasticine, which employs static scheduling, relies on a high-width SIMD design and decouples compute and memory units to enhance performance by overlapping computation execution time and memory access latency. \textit{Regular} applications like GEMM and STE can benefit from efficient memory access by decoupled-access-execution mechanism. However, its lack of dynamic scheduling limits computational resource utilization in \textit{irregular} applications such as BFS and PR, leading to diminished performance.
%MTDE and ParallelXL enhance performance through centralized and decentralized dynamic scheduling, respectively. Dynamic scheduling allows bottleneck tasks to be distributed across the entire array, accelerating computation, particularly in \textit{irregular} or \textit{near-irregular} applications where bottleneck tasks dominate. 
%MTDE generally outperforms ParallelXL across applications due to its frequent periodic global workload sensing. Although this sensing is not on-demand, it operates frequently enough to enable timely dynamic scheduling. While this approach enhances performance, it also incurs higher energy overhead.
%In contrast, ParallelXL's localized sensing and real-time on-demand scheduling triggering, though less effective at identifying bottlenecks for efficient task migration and performance optimization, leads to lower energy consumption. 
%GEMINI's semi-centralized dynamic scheduling strikes a balance between the centralized MTDE and decentralized ParallelXL. It leverages on-demand scheduling triggering, global sensing and planning, and low-latency remapping to enhance utilization and performance while conserving energy. 
%Compared to MTDE, GEMINI reduces the need for frequent global sensing and enhances communication and remapping efficiency, resulting in energy savings and performance improvements.
%Compared to ParallelXL, GEMINI's global sensing and planning allow for better identification of bottleneck tasks, enabling global optimal planning and resulting in notable performance gains.

%Figure 4 presents the performance comparison, normalized to Plasticine. Our design demonstrates an average performance improvement of 1.89× over Plasticine, 1.47× over ParallelXL, and 1.21× over MTDE.
%Plasticine, which employs static scheduling, relies on a high-width SIMD design and decouples compute and memory units to enhance performance by overlapping computation execution time and memory access latency. \textit{regular} applications like GEMM and STE can benefit from efficient memory access by decoupled-access-execution mechanism. However, its lack of dynamic scheduling limits computational resource utilization in \textit{irregular} applications such as BFS and PR, leading to diminished performance.
%MTDE and ParallelXL enhance performance through centralized and decentralize dynamic scheduling, respectively. Dynamic scheduling allows bottleneck tasks to be distributed across the entire array, accelerating computation, particularly in irregular or near-irregular applications where bottleneck tasks dominate. 
%MTDE generally outperforms ParallelXL due to its periodic global workload sensing, enabling more timely and responsive scheduling. However, this frequent sensing increases energy overhead. 
%In contrast, ParallelXL's localized sensing and real-time on-demand scheduling, while less effective in identifying bottlenecks, results in lower energy consumption.
%GEMINI, employing a semi-centralized dynamic scheduling approach, strikes a balance between the centralized MTDE and decentralized ParallelXL. It uses on-demand scheduling triggers, global sensing \& planning and low-latency remapping, which improves resource utilization and performance while saving the energy. Unlike MTDE, GEMINI reduces the need for frequent global sensing, leading to significant energy savings. Compared to ParallelXL, GEMINI's global sensing and planning capabilities allow it to better identify bottleneck tasks, enabling global optimal planning and resulting in notable performance gains.



%\subsection{Energy Efficiency Comparison}

\begin{comment}
\begin{table}
\scriptsize
%\footnotesize
%\small
%\tiny
\vspace{-0.25cm}
\centering
  \caption{Hardware Comparison of SPM, Cache, and PANDA MEM.}
  \label{SPR}
  \renewcommand{\arraystretch}{1.2}
  \resizebox{0.45\textwidth}{!}{
  %\renewcommand\tabcolsep{3.0pt}
  \begin{tabular}{|c|l|l|l|}
  \hline
  \multicolumn{2}{|c|}{\textbf{Component}} & \textbf{Area (}\textit{mm$^2$}\textbf{)} & \textbf{Power (}\textit{mW}\textbf{)} \\
  \hline
  \multirow{3}{*}{\textbf{SPM}} & \textbf{RAM Banks (64KB)} &0.212 (80.5\%)  & 163.9 (81.1\%) \\
  \cline{2-4}
  & \textbf{DMA \& Control Logic} &0.042 (16.5\%)  & 39.6 (19.5\%) \\
  \cline{2-4}
  & \textbf{Total} &0.254  &203.5  \\
  \cline{2-4}
  \hline
  \multirow{3}{*}{\textbf{Cache}} & \textbf{Data Arrays (64KB)} &0.212 (68.6\%)  & 163.9 (64.2\%) \\
  \cline{2-4}
  & \textbf{Tag \& Control Logic} &0.097 (31.4\%)  & 91.3 (35.8\%) \\
  \cline{2-4}
  & \textbf{Total} &0.309  &255.2  \\
  \cline{2-4}
  \hline  
  \multirow{3}{*}{\textbf{\makecell[c]{PANDA\\MEM}}} & \textbf{Shared RAM (64KB)} &0.212 (60.7\%)  & 163.9 (57.8\%) \\
  \cline{2-4}
  & \textbf{Control Logic} &0.137 (39.3\%)  & 119.7 (41.8\%) \\
  \cline{2-4}
  & \textbf{Total} &0.349  &283.6  \\
  \cline{2-4}
  \hline
\end{tabular}}
\vspace{-0.35cm}
\end{table}
\end{comment}

\begin{comment}
\begin{table}
\scriptsize
%\footnotesize
%\small
%\tiny
%\vspace{-0.2cm}
\centering
  \caption{Hardware Comparison of Base PE and PANDA PE.}
  \label{CD}
  \renewcommand{\arraystretch}{1.2}
  \resizebox{0.45\textwidth}{!}{
  %\renewcommand\tabcolsep{3.0pt}
  \begin{tabular}{|c|l|l|l|}
  \hline
  \multicolumn{2}{|c|}{\textbf{Component}} & \textbf{Area (}\textit{mm$^2$}\textbf{)} & \textbf{Power (}\textit{mW}\textbf{)} \\
  \hline
  \multirow{3}{*}{\textbf{\makecell[c]{Base\\PE}}} & \textbf{PEs (16 ordinary)} &0.478 (88.7\%)  & 486.6 (74.7\%) \\
  \cline{2-4}
  & \textbf{Controller} &0.061 (11.3\%)  & 165.1 (25.3\%) \\
  \cline{2-4}
  & \textbf{Total} &0.539  &651.7  \\
  \cline{2-4}
  \hline
  \multirow{3}{*}{\textbf{\makecell[c]{PANDA\\PE}}} & \textbf{PEs (16 decentralized)} &0.574 (100\%)  & 690.0 (100\%) \\
  \cline{2-4}
  & \textbf{Controller (removed)} &0 (0\%)  & 0 (0\%) \\
  \cline{2-4}
  & \textbf{Total} &0.574  &690.0  \\
  \cline{2-4}
  \hline  
\end{tabular}}
\vspace{0.2cm}
\end{table}
\end{comment}

%\subsubsection{Energy Efficiency} ~

%Figure 6 presents the performance gains of StreamDCIM, normalized to a non-streaming CIM solution, across the ViLBERT-base and ViLBERT-large models. Specifically, StreamDCIM achieves a speedup of 2.86× and 1.25× over the non-stream and layer-streaming solutions, respectively, on ViLBERT-base. For ViLBERT-large, the accelerator provides a speedup of 2.42× compared to non-streaming and 1.31× over layer-streaming.
%Figure 7 displays the energy savings achieved, also normalized to the non-streaming solution. On ViLBERT-base, StreamDCIM reduces energy consumption by 2.64× over non-streaming and 1.27× over layer-streaming. On ViLBERT-large, it provides energy savings of 1.94× compared to non-streaming and 1.19× over layer-streaming.

Fig. \ref{Energy} shows the energy comparison, normalized to the non-streaming solution, across the  ViLBERT-base and ViLBERT-large models. 
%On average, our design achieves 1.67$\times$ energy saving over Non-stream and 1.31$\times$ over Layer-stream. 
On ViLBERT-base model, StreamDCIM reduces energy consumption by 2.64$\times$ over Non-stream and 1.27$\times$ over Layer-stream. 
On ViLBERT-large model, StreamDCIM achieves energy saving of 1.94$\times$ compared to Non-stream and 1.19$\times$ over Layer-stream. 
%Plasticine’s static scheduling avoids the runtime overhead of dynamic scheduling and reconfiguration in other dynamic scheduled architectures. However, its lower utilization in most applications results in poor energy efficiency. 
%MTDE’s centralized dynamic scheduling, driven by frequent global load sensing, boosts performance but introduces considerable energy overhead. Despite its performance advantage over ParallelXL, the two have similar energy efficiency. 
%Compared with MTDE, which sacrifices energy for performance, and ParallelXL, which saves energy but also has poorer performance, GEMINI combines on-demand triggering, global sensing \& planning, and low-latency peer-to-peer remapping to maintain high performance without introducing excessive energy overheads, resulting in superior energy efficiency. 

\begin{figure}%[htbp]
  \vspace{-0.3cm}
\centering
  \includegraphics[width=0.489\textwidth]{fig/Speedup.pdf}
  %\setlength{\abovecaptionskip}{-0.33cm}
  %\setlength{\belowcaptionskip}{-0.33cm}
  \vspace{-0.5cm}
  \caption{Performance Comparison on ViLBERT-base and ViLBERT-large.}
  \label{Speedup}
  \vspace{-0.1cm}
\end{figure}

\begin{figure}%[htbp]
\vspace{-0.1cm}
\centering
  \includegraphics[width=0.489\textwidth]{fig/Energy.pdf}
  %\setlength{\abovecaptionskip}{-0.33cm}
  %\setlength{\belowcaptionskip}{-0.33cm}
  \vspace{-0.6cm}
  \caption{Energy Comparison on ViLBERT-base and ViLBERT-large.}
  \label{Energy}
  \vspace{-0.5cm}
\end{figure}

\begin{comment}
%\vspace{-0.2cm}
\subsection{Overhead}
%\subsection{Cost and Discussions}
%\vspace{-0.1cm}

Finally, we evaluate the hardware overhead of the adaptive prefetching and decentralized scheduling. First is the adaptive prefetching. The hardware overhead of PANDA MEM is shown in Table \ref{SPR}, and PANDA MEM exhibits a total area overhead of 37.4\% and 12.9\% increase over SPM with DMA and cache of the same size, respectively. 
Next is the decentralized scheduling. The hardware overhead of PANDA PE is shown in Table \ref{CD}, and PANDA PE exhibits a total area overhead of 6.5\% increase over the Base PE of the same computing fabric. 
\end{comment}

\section{Conclusion}

%In this paper, we describe GEMINI, a novel dataflow architecture with decoupled task flow and data flow planes, dedicated to efficient dynamic scheduling through hardware-software co-optimizatios. 
In this paper, we describe StreamDCIM, a tile-based streaming digital CIM accelerator with mixed-stationary cross-forwarding dataflow for multimodal Transformers.
%We implement GEMINI using Verilog and demonstrate that in a wide range of real-world applications, including scientific computing, artificial intelligence, signal processing, and graph processing algorithms, GEMINI attains up to 1.89$\times$ performance and 1.67$\times$ energy efficiency improvement over the state-of-the-art dataflow architectures.
We implement StreamDCIM using Verilog and demonstrate that comparede with non-streaming and layer-based streaming CIM-based solutions, StreamDCIM attains by geomean 2.63$\times$, 1.28$\times$ speedup and 2.26$\times$, 1.23$\times$ energy saving on typical multimodal Transformer models.%, including scientific computing, artificial intelligence, signal processing, and graph processing algorithms.

%\begin{comment}
\section*{Acknowledgment}

This work was supported by Beijing Natural Science Foundation (Grant No.L234078) and Beijing Nova Program (Grant No. 20220484054 and No. 20230484420).
%\end{comment}

\begin{thebibliography}{00}
\bibitem{NLP} Ashish Vaswani, Noam Shazeer, Niki Parmaret al., “Attention Is All You Need," in \textit{Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS)}, 2017, pp. 6000–6010. 
\bibitem{CV} A. Vaswani et al., “Scaling local self-attention for parameter efficient visual backbones,” in \textit{Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021, pp. 12894–12904.
\bibitem{Multimodal21} P. H. Seo, A. Nagrani, and C. Schmid, “Look before you speak: Visually contextualized utterances,” in \textit{Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021, pp. 16872–16882.
\bibitem{Multimodal22} P. H. Seo et al., “End-to-end generative pretraining for multimodal video captioning,” in \textit{Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 17938–17947.
\bibitem{TSMC21} Y. -D. Chih et al., “16.4 An 89TOPS/W and 16.3TOPS/mm2 All-Digital SRAM-Based Full-Precision Compute-In Memory Macro in 22nm for Machine-Learning Edge Applications,” in \textit{Proceedings of the 2021 IEEE International Solid-State Circuits Conference (ISSCC)}, 2021, pp. 252-254.
\bibitem{TSMC22} H. Fujiwara et al., “A 5-nm 254-TOPS/W 221-TOPS/mm2 Fully-Digital Computing-in-Memory Macro Supporting Wide-Range Dynamic-Voltage-Frequency Scaling and Simultaneous MAC and Write Operations,” in \textit{Proceedings of the 2022 IEEE International Solid-State Circuits Conference (ISSCC)}, 2022, pp. 1-3.
\bibitem{Plasticine} R. Prabhakar, Y. Zhang, D. Koeplinger et al., “Plasticine: A reconfigurable architecture for parallel patterns,” in \textit{Proceedings of the 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)}, 2017, pp. 389–402.
\bibitem{Thinker} S. Yin et al., “A High Energy Efficient Reconfigurable Hybrid Neural Network Processor for Deep Learning Applications,” in \textit{IEEE Journal of Solid-State Circuits (JSSC)}, vol. 53, no. 4, pp. 968-982, 2018.
\bibitem{Sanger} L. Lu, Y. Jin, H. Bi et al., “Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture,” in \textit{Proceedings of the 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 2021, pp. 977–991.
\bibitem{SALO} G. Shen, J. Zhao, Q. Chen et al., “SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences,” in \textit{Proceedings of the 59th ACM/IEEE Design Automation Conference (DAC)}, 2022, pp. 571–576.
\bibitem{ReDCIM} F. Tu et al., “A 28nm 29.2TFLOPS/W BF16 and 36.5TOPS/W INT8 Reconfigurable Digital CIM Processor with Unified FP/INT Pipeline and Bitwise In-Memory Booth Multiplication for Cloud Deep Learning Acceleration,” in \textit{Proceedings of the 2022 IEEE International Solid-State Circuits Conference (ISSCC)}, 2022, pp. 1-3.
\bibitem{TranCIM} F. Tu et al., “A 28nm 15.59µJ/Token full-digital bitline-transpose CIM-based sparse transformer accelerator with pipeline/parallel reconfigurable modes,” in \textit{Proceedings of the 2022 IEEE International Solid-State Circuits Conference (ISSCC)}, 2022, pp. 466–468.
\bibitem{MulTCIM} F. Tu et al., “16.1 MuITCIM: A 28nm $2.24 \mu\mathrm{J}$/Token Attention-Token-Bit Hybrid Sparse Digital CIM-Based Accelerator for Multimodal Transformers,” in \textit{Proceedings of the 2023 IEEE International Solid-State Circuits Conference (ISSCC)}, 2023, pp. 248–250.
\bibitem{SparseTrans} S. Liu, P. Li, J. Zhang et al., "16.2 A 28nm 53.8TOPS/W 8b Sparse Transformer Accelerator with In-Memory Butterfly Zero Skipper for Unstructured-Pruned NN and CIM-Based Local-Attention-Reusable Engine," in \textit{Proceedings of the 2023 IEEE International Solid-State Circuits Conference (ISSCC)}, 2023, pp. 250-252.
\bibitem{CIMRing} R. Guo et al., "A 28-nm 28.8-TOPS/W Attention-Based NN Processor With Correlative CIM Ring Architecture and Dataflow-Reshaped Digital-Assisted CIM Array," in \textit{IEEE Journal of Solid-State Circuits (JSSC)}, 2024.
\bibitem{ETC} J. Ainslie et al., “ETC: Encoding long and structured inputs in transformers,” in \textit{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2020, pp. 268–284. 
\bibitem{Longformer} I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document transformer,” 2020, \textit{arXiv:2004.05150}.
\bibitem{BigBird} M. Zaheer, G. Guruganesh, A. Dubey et al., "Big bird: transformers for longer sequences," in \textit{Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS)}, 2020, pp. 17283–17297.
\bibitem{LongSparse} R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long sequences with sparse transformers,” 2019, \textit{arXiv:1904.10509}.
\bibitem{DynamicViT} Y. Rao et al., “DynamicViT: efficient vision transformers with dynamic token sparsification," in \textit{Proceedings of the 35th International Conference on Neural Information Processing Systems (NeurIPS)}, 2021, pp. 13937–13949.
\bibitem{Evo-ViT} Y. Xu et al., “Evo-ViT: Slow-fast token evolution for dynamic vision
 transformer,” in \textit{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}, 2022, vol. 36, no. 3, 2022, pp. 2964–2972.
\bibitem{SpAtten} H. Wang et al., “SpAtten: Efficient sparse attention
 architecture with cascade token and head pruning,” in \textit{Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, 2021, pp. 97–110.
\bibitem{MobiLattice} Q. Zheng et al., "MobiLattice: A Depth-wise DCNN Accelerator with Hybrid Digital/Analog Nonvolatile Processing-In-Memory Block," in \textit{Proceedings of the 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)}, 2020, pp. 1-9.
\bibitem{MorphableCIM} Y. -C. Lo and R. -S. Liu, "Morphable CIM: Improving Operation Intensity and Depthwise Capability for SRAM-CIM Architecture," in \textit{Proceedings of the 2023 60th ACM/IEEE Design Automation Conference (DAC)}, 2023, pp. 1-6.
\bibitem{Non-streaming-2} J. Yue et al., “A 2.75-to-75.9 TOPS/W computing-in-memory NN processor supporting set-associate block-wise zero skipping and ping-pong CIM with simultaneous computation and weight updating,” in \textit{Proceedings of the 2021 IEEE International Solid-State Circuits Conference (ISSCC)}, 2021, pp. 238–240.
\bibitem{Non-streaming-3} J.-W. Su et al., “A 28 nm 384kb 6T-SRAM computation-in-memory macro with 8b precision for AI edge chips,” in \textit{Proceedings of the 2021 IEEE International Solid-State Circuits Conference (ISSCC)}, 2021, pp. 250–252. 
\bibitem{ViLBERT} J. Lu et al., “ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,” in \textit{Proceedings of the 33rd International Conference on Neural Information Processing Systems (NeurIPS)}, 2019, pp. 13–23.

\begin{comment}
\bibitem{Moore} G. E. Moore, "Cramming more components onto integrated circuits," in \textit{Proceedings of the IEEE}, vol. 86, no. 1, pp. 82-85, 1998.
\bibitem{Dennard} R. H. Dennard, F. H. Gaensslen, H. -N. Yu et al., "Design of ion-implanted MOSFET's with very small physical dimensions," in \textit{IEEE JSSC}, vol. 9, no. 5, pp. 256-268, 1974.
\bibitem{Marionette} J. Deng, X. Tang, J. Zhang et al., “Towards Efficient Control Flow Handling in Spatial Architecture via Architecting the Control Flow Plane,” in \textit{MICRO}, 2023, pp. 1395-1408.
\bibitem{TaskStreaming} V. Dadu and T. Nowatzki, “TaskStream: accelerating task-parallel workloads by recovering program structure,” in \textit{ASPLOS}, 2022, pp. 1–13.
\bibitem{TACO2024} Z. Fan et al., "Improving Utilization of Dataflow Unit for Multi-Batch Processing," in \textit{ACM TACO}, vol. 21, no. 1, pp. 1-26, 2024.
\bibitem{EURO} Z. Fan et al., "Improving Utilization of Dataflow Architectures Through Software and Hardware Co-Design," in \textit{Euro-Par}, 2023, pp. 245-259.
% Dataflow Principle
\bibitem{Dataflow} J. B. Dennis, "First version of a data flow procedure language," in \textit{Programming Symposium: Proceedings Colloque sur la Programmation}, 1974, pp. 362–376.
\bibitem{DFGC} T. Liu et al., "DFGC: DFG-aware NoC Control based on Time Stamp Prediction for Dataflow Architecture," in \textit{ICCD}, 2023, pp. 432-439.
\bibitem{Plasticine} R. Prabhakar, Y. Zhang, D. Koeplinger et al., “Plasticine: A reconfigurable architecture for parallel patterns,” in \textit{ISCA}, 2017, pp. 389–402.
%\bibitem{TIA} A. Parashar et al., “Triggered instructions: A control paradigm for spatially-programmed architectures,” in \textit{ISCA}, 2013, pp. 142–153.
\bibitem{TIA} T. J. Repetti, J. P. Cerqueira, M. A. Kim, and M. Seok, “Pipelining a
 triggered processing element,” in \textit{MICRO}, 2017, pp. 96–108.
%\bibitem{REVEL} J. Weng, S. Liu, Z. Wang et al., "A Hybrid Systolic-Dataflow Architecture for Inductive Matrix Algorithms," in \textit{HPCA}, 2020, pp. 703-716.
\bibitem{picos-2019MICRO} L. Morais et al., "Adding tightly-integrated task scheduling acceleration to a RISC-V multi-core processor," in \textit{MICRO}, 2019, pp. 861–872.
\bibitem{picos-2024TC} L. Morais et al., "Enabling HW-Based Task Scheduling in Large Multicore Architectures," in \textit{IEEE TC}, vol. 73, no. 1, pp. 138-151, 2024.
\bibitem{MTDE} L. Chen, J. Zhu, Y. Deng et al., “An Elastic Task Scheduling Scheme on Coarse Grained Reconfigurable Architectures,” in \textit{IEEE TPDS}, vol. 32, no. 12, pp. 3066–3080, 2021. 
\bibitem{DRIPS} C. Tan et al., "DRIPS: Dynamic Rebalancing of Pipelined Streaming Applications on CGRAs,” in \textit{HPCA}, 2022, pp. 304-316.
%\bibitem{WorkStealing} R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul et al., “Cilk: an efficient multithreaded runtime system," in \textit{PPoPP}, 1995, pp. 207–216.
\bibitem{WorkStealing} R. D. Blumofe et al., “Scheduling multithreaded computations by work stealing" in \textit{Journal of the ACM}, vol. 46, no. 5, pp. 720–748, 1999.
\bibitem{ParallelXL} T. Chen, S. Srinath, C. Batten and G. E. Suh, “An Architectural Framework for Accelerating Dynamic Parallel Algorithms on Reconfigurable Hardware,” in \textit{MICRO}, 2018, pp. 55-67.
\bibitem{SMART} T. Krishna, C. -H. O. Chen, W. C. Kwon and L. -S. Peh, “Breaking the on-chip latency barrier using SMART,” in \textit{HPCA}, 2013, pp. 378-389.
% GraphDynS - irregular application def
\bibitem{GraphDynS} M. Yan et al., “Alleviating irregularity in graph analytics acceleration: A hardware/software co-design approach,” in \textit{MICRO}, 2019, pp. 615-628.
\end{comment}

\end{thebibliography}
\vspace{12pt}
%\color{red}
%IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
