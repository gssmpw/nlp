\section{Experiments}

\subsection{Experimental Setup}

%\textbf{Models.} 
We perform multi-task fine-tuning on two widely used decoder-only pretrained language models: LLaMA-2-7B \citep{touvron2023llama} and Mistral-7B \citep{jiang2023mistral}.% both of which are decoder-only architectures. %We use their base versions for all experiments.

\textbf{Tasks.} We evaluate our approach on a diverse set of natural language understanding (NLU) and natural language generation (NLG) tasks from the GLUE \citep{wang-etal-2018-glue} and SuperGLUE \citep{wang2019superglue} benchmarks. Specifically, for NLU, we include SST-2, BoolQ, RTE, WSC, WiC, MultiRC, and COPA, covering various classification and reasoning tasks. For NLG, we use SQuAD for question answering. To ensure computational feasibility, we sample a subset of each dataset with fixed training, validation, and test splits. Details on datasets and evaluation metrics are in Appendix~\ref{appendix:task_details}.



\textbf{Baselines.} We compare MaZO with several baselines. First, we include vanilla ZO optimization combined with traditional multi-task learning (MTL-ZO) techniques as a direct comparison to MaZO in the ZO setting. Second, we evaluate single-task learning (STL-ZO), where models are trained individually on each task to provide an {\bf upper bound} for task-specific performance without multi-task conflicts, as well as a single-task transfer baseline, where the model is trained on a single task (SST-2) using vanilla ZO optimization and evaluated across all tasks to highlight the limitations of single-task training in multi-task scenarios. Third, we include LoRA fine-tuning \citep{hu2021lora}, a parameter-efficient approach, and extend MaZO to update LoRA matrices under ZO settings, demonstrating its flexibility. Finally, we compare MaZO against state-of-the-art first-order (FO) multi-task learning methods, including CoBa \citep{gong2024coba}, FAMO \citep{liu2024famo}, and MTL-LoRA \citep{yang2024mtl}, to its compatibility with ZO optimization. These baselines provide a comprehensive comparison for assessing MaZO's effectiveness and robustness in addressing the challenges of ZO-based multi-task learning.


% \zz{Better to itemize these baselines and provide very short descriptions for each baseline method.}




\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Task} & \textbf{SST-2} & \textbf{BoolQ} & \textbf{RTE} & \textbf{WSC} & \textbf{WiC} & \textbf{MultiRC} & \textbf{COPA} & \textbf{SQuAD} & \multirow{2}{*}{\textbf{Avg}} \\ 
\textbf{Task Type} & \multicolumn{6}{c}{---------------------- \textit{Classification} ----------------------} & \multicolumn{1}{c}{-- \textit{Multiple Choice} --} & \multicolumn{1}{c}{-- \textit{Generation} --}\\
\midrule
$\text{STL-ZO}$ (one model per task)   &  93.8           & 83.0           & 73.5         & 51.3         & 62.1         & 61.0            & 86.0          & 79.6          & 73.8         \\
Zero-Shot         &  83.8           & 75.8           & 57.0         & 37.5         & 52.6         & 46.6            & 79.0          & 56.4          & 61.1         \\
ICL               &  93.7           & 78.7           & 61.2         & 47.2         & 59.9         & 54.3            & 80          & 57.7          & 66.6         \\
$\text{STL-ZO} (\text{shared model}) $    & 93.8           & 75.8           & 58.8         & 43.3         & 50.6         & 46.0            & 75.0          & 52.2          & 61.9         \\
$\text{MTL-ZO}$     & 80.6           & 74.2           & 55.6         & 58.6         & 51.0         & 53.8            & \textbf{80.0}          & 62.3          & 64.5         \\
$\text{MTL-ZO}_{\text{LoRA}}$           & 90.2           & \textbf{80.0}           & 61.0         & 54.8         & 56.6         & 58.0            & 76.0          & 74.8          & 68.9         \\
\midrule
$\text{MTL-ZO}_{\text{MTL-LoRA}}$        & 88.4           & 76.8           & 60.2         & 60.6         & \textbf{57.6}         & \textbf{61.6}            & 79.0          & 59.4          & 68.0         \\
$\text{MTL-ZO}_{\text{CoBa}}$            & 82.8           & 75.3           & 56.8         & 60.6         & 53.4         & 55.8            & 77.0          & 57.6          & 64.9         \\
$\text{MTL-ZO}_{\text{FAMO}}$            & 91.0           & 77.6           & 59.4         & 56.5         & 53.4         & 50.6            & 78.0          & 53.9          & 65.1         \\
\midrule
\textbf{MaZO}     &  90.6  & 76  & \textbf{62.8} & 56.7 & 52.6 & 58.6   & \textbf{82.0} & 55.5 & 66.9 \\ 
\textbf{$\text{MaZO}_{\text{LoRA}}$}     &  \textbf{91.2}  & \textbf{80.0}  & \textbf{62.8} & \textbf{61.5} & 56.6 & 60.4   & 80.0 & \textbf{77.7} & \textbf{71.3} \\ 
\bottomrule
\end{tabular}}
\caption{Performance comparison across tasks using different methods on LLaMA-2-7B. The average score (Avg) is computed across all tasks. Metrics for these tasks are consistent with MeZO \citep{malladi2023mezo}. \textit{Shared model} indicates training on a single task (SST-2) and testing on all tasks. \textit{ICL} refers to in-context learning. \textit{STL} represents single-task learning and \textit{MTL} represents multi-task learning.}

\label{tab:performance_comparison}
\end{table*}



\begin{figure}[t]
    \centering
    
    \includegraphics[width=0.48\textwidth]{img/convergenceloss.pdf} 
    % \vspace{-20pt} 
    \caption{The convergence curve of (1) vanilla multi-task ZO fine-tuning with LoRA, (2) MaZO with LoRA. }
    % \vspace{-10pt} 
    \vspace{-10pt}
    \label{fig:convergence}
\end{figure} 

\subsection{Results on LLaMA-2-7B}
\paragraph{MaZO Outperforms Competitors.} 
The results for LLaMA-2-7B are presented in Table~\ref{tab:performance_comparison}. Vanilla multi-task ZO optimization shows only slight improvements over the zero-shot baseline, highlighting its inability to effectively address multi-task conflicts under ZO settings. Similarly, vanilla single-task ZO optimization with a shared model fails to generalize effectively across multiple tasks, underscoring the inherent challenges of ZO optimization in multi-task scenarios.

In contrast, our proposed MaZO framework achieves the {\bf highest average performance} across all tasks and demonstrates a {\bf balanced} performance profile. These results validate MaZO's ability to mitigate inter-task conflicts and optimize multi-task learning by selectively focusing on critical parameters. The effectiveness of MaZO is further evident in its superior performance in both full-model ZO fine-tuning and LoRA-based fine-tuning, with particularly pronounced gains in the latter. This underscores MaZO's flexibility and its compatibility with parameter-efficient fine-tuning techniques.

\paragraph{Dimensionality Reduction Enhances Multi-Task Learning.} 
The application of LoRA to ZO fine-tuning significantly improves the performance of multi-task learning. This improvement can be attributed to LoRA's ability to reduce the dimensionality of the parameter space, thereby lowering the variance of gradient estimates. These findings reinforce the validity of MaZO's masking strategy, which optimizes multi-task learning by focusing on a reduced set of critical parameters.

\paragraph{FO multi-task learning methods do not apply to ZO.} Multi-task learning methods originally developed for first-order (FO) optimization, such as CoBa and FAMO, do not achieve effective performance in the ZO setting. This can be attributed to their inability to resolve multi-task conflicts due to the collinearity problem in ZO gradient estimates. Under the ZO framework, FO methods can only adjust the magnitude of the approximated gradient, but not its direction, resulting in  performance degradation. 
Additionally, MTL-LoRA, the multi-task version of LoRA fine-tuning, does not significantly enhance performance in the ZO setting. This may be due to the sensitivity of task-specific weights and the diagonal transformation matrix to noise. Perturbation-based optimization, as used in ZO, introduces excessive variance, which undermines the effectiveness of these FO-based methods.


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Task} & \textbf{SST-2} & \textbf{BoolQ} & \textbf{RTE} & \textbf{WSC} & \textbf{WiC} & \textbf{MultiRC} & \textbf{COPA} & \textbf{SQuAD} & \multirow{2}{*}{\textbf{Avg}} \\ 
\textbf{Task Type} & \multicolumn{6}{c}{---------------------- \textit{Classification} ----------------------} & \multicolumn{1}{c}{-- \textit{Multiple Choice} --} & \multicolumn{1}{c}{-- \textit{Generation} --}\\
\midrule
$\text{STL-ZO}$ (one model per task)       & 93.6           & 77.8           & 74.2         & 55.3         & 62.1         & 62.7            & 88.0          & 76.5          & 73.8         \\
Zero-Shot         & 56.7           & 42.4           & 50.5         & 52.8         & 50.3         & 43.6            & 79.0          & 57.2          & 54.1         \\
ICL               & 62.3           & 46.1           & 56.0         & 53.2         & 61.4         & 53.4            & 79.0          & 62.3          & 59.2         \\
$\text{MTL-ZO}$      &  58.7           & 47.2           & 55.0         & 53.2         & 59.8         & 54.4            & 79.0          & 56.3          & 58.0         \\
$\text{MTL-ZO}_{\text{LoRA}}$           & 89.3           & \textbf{73.2}           & 71.5         & 51.3         & 58.1         & 53.4            & 80.0          & \textbf{73.5}          & 68.7         \\
\midrule
\textbf{MaZO}     & 83.4  & 56.3  & 60.2 & 54.8 & 58.1 & 55.8 & 79 & 59.4 & 63.4 \\ 
\textbf{$\text{MaZO}_{\text{LoRA}}$}    & \textbf{90.2}  & 72.4  & \textbf{74.2} & \textbf{54.8} & \textbf{62.1} & \textbf{57.3}   & \textbf{82.0} & \textbf{73.5} & \textbf{70.8} \\ 
\bottomrule
\end{tabular}}
\caption{Performance comparison across tasks using different methods on Mistral-7B. The setting and notation are the same as Table~\ref{tab:performance_comparison}. We exclude the FO MTL methods as they do not have significant improvement.}
\label{tab:mistral_performance_comparison}
% \vspace{-10pt}
\end{table*}
% \vskip 8em

\subsection{Results on Mistral-7B}  
The results for Mistral-7B in Table~\ref{tab:mistral_performance_comparison} reveal trends similar to those observed with LLaMA-2-7B. Despite the relatively low zero-shot performance of Mistral-7B, vanilla multi-task learning ZO fails to deliver substantial improvements. This underscores the inherent challenges of ZO-based multi-task learning.
In contrast, MaZO consistently outperforms all other methods. Its ability to mitigate ZO-specific challenges is evident in its superior performance, further validating MaZO as a state-of-the-art solution for ZO-based multi-task learning.


\subsection{Computational Performance}
  
% \zz{The training diverged in the later stage, which is very bad.. We may need some variance reduction approaches in the later stage to improve the convergence. I'm not sure if this doable before the ACL deadline. If not, we may remove the convergence curve, keep working on this and show an improved convergence curve in the rebuttal.}
Figure~\ref{fig:convergence} shows that MaZO converges faster and achieves a significantly lower loss compared to traditional multi-task ZO fine-tuning methods. This holds true both with and without LoRA. This improvement can be attributed to the mask mechanism in MaZO, which focuses on optimizing the most critical parameters, thereby reducing gradient noise, balancing the inter-task conflicts, and accelerating convergence.

The significantly better convergence and accuracy of MaZO is obtained at marginal computing and memory overhead. Specifically, the mask search time introduced by MaZO is negligible compared to the overall training time. When evaluated in the LlaMMA-7B model, MaZO incurs a slight increase in memory usage (approximately $10\%$) compared to baseline multi-task learning ZO methods. This is primarily due to the additional storage required for the weight update mask. However, this increase does not significantly impact the overall memory efficiency, especially when combined with LoRA, where the parameter space is already reduced. Details are provided in Appendix~\ref{sec:memory usage}.
%While MaZO introduces a small memory overhead, its benefits in terms of faster convergence and reduced gradient variance outweigh this cost, making it an effective and practical solution for multi-task fine-tuning under ZO optimization.

\subsection{Various Weight Importance Metrics}  

% \zz{Compare with standard sparse ZO fine-tuning.}
\begin{table}[t]
    \centering
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{l|ccccc}
        \toprule
        \textbf{Task} & \textbf{SST-2} & \textbf{BoolQ} & \textbf{Copa} & \textbf{SQuAD} & \textbf{Avg}\\ 
        \midrule
        No Mask    &  85.4 & 72.2 & 80.0 & 66.0 & 75.9 \\
        Random     &  86.6 & 73.0 & 80.0 & 63.4 & 75.8 \\
        Magnitude  &  87.4 & 75.6 & 79.0 & 65.6 & 76.9 \\
        Wanda      &  88.4 & 77.8 & 80.0 & 62.4 & 77.2 \\
        MaZO       &  90.2 & 78.0 & 81.0 & 72.3 & 80.4 \\
        \bottomrule
    \end{tabular}}
    \caption{Comparison of different weight importance metrics. The sparsity is set to 50\% except for \textit{No Mask}. Random and Magnitude are done weight-wise while Wanda and MaZO are selected row-wise.}
    \label{tab:maskmetric}
    \vspace{-8pt} 
\end{table}

To further validate the effectiveness of MaZO, we compare its performance with three alternative weight scoring methods: random selection, magnitude-based scoring, and Wanda scoring. Detailed implementation of these methods is described in Appendix~\ref{app:metrics}. For a fair comparison, we fix the sparsity level at 50\%, consistent with the sparsity used in the Wanda score. Table~\ref{tab:maskmetric} summarizes the results of this comparison.

The findings indicate that while both the magnitude-based and Wanda-based scoring can improve average performance, their improvements are less pronounced and less balanced across tasks compared to MaZO. This is because these methods evaluate the weight importance statically, without considering training dynamics or perturbation-based insights. In contrast, MaZO dynamically identifies critical parameters during training, enabling more effective optimization and better multi-task balance under the ZO framework. These results underscore the superiority of MaZO in leveraging weight importance to achieve state-of-the-art performance in multi-task fine-tuning.





% The vanilla ZO multi-task learning method in Table~\ref{tab:performance_comparison} shows only marginal improvements compared to the zero-shot baseline. This indicates that traditional multi-task learning under the ZO setting struggles to effectively balance task-specific objectives and suffers from interference between tasks. The lack of parameter-efficient mechanisms, such as LoRA, further limits its effectiveness.

% The introduction of LoRA in the ZO setting (ZO-LoRA and ZO-MTL-LoRA) significantly improves performance compared to vanilla ZO methods. By enabling parameter-efficient fine-tuning, LoRA reduces task interference and allows the model to better adapt to individual tasks. Among these, ZO-MTL-LoRA achieves the best balance between performance and efficiency, highlighting the importance of task-specific parameterization in the ZO setting.



% \subsection{FO Multi-task Learning}
% State-of-the-art FO multi-task learning methods, such as COBA, FAMO, and MTLoRA, fail to provide significant improvements under the ZO setting. These methods are designed for FO optimization and do not fully leverage the sparse and task-specific nature of ZO optimization. Consequently, their performance is suboptimal in this context.


% \subsection{Our Method}
% Our proposed method outperforms all baselines, achieving the highest average score across tasks (71.3). Notably, it strikes a balance between task-specific optimization and multi-task generalization. Key observations include:

% Significant improvements on challenging tasks like WSC and SQuAD compared to other ZO methods.
% Comparable or better performance on classification tasks such as SST-2 and RTE.
% Effective handling of multiple task types (classification, multiple choice, and generation) under a unified framework.


% \subsection{Computational Performance}
% To be done.

\subsection{Ablation Study}  
%\zz{Reduce this subsection by about a half.}
%\subsubsection{Hyperparameters}  



Finally, we explore the optimal hyperparameter settings for MaZO, includeing $\alpha$, $\beta$, sparsity level, and the LoRA rank. To streamline the process, we perform grid searches for each hyperparameter while keeping the others constant. For most experiments, we fine-tune the model on SST-2, BoolQ, COPA, and SQuAD, encompassing binary classification, multiple-choice, and generation tasks, providing diverse evaluation scenarios. However, for the LoRA rank, we evaluate performance across all tasks.




\textbf{$\alpha$ and $\beta$.}  
To optimize $\alpha$ and $\beta$, we fix the sparsity level at 50\% and perform full-model fine-tuning (without LoRA). The search is conducted in two stages. First, $\beta$ is set to zero, and $\alpha$ is tuned, resulting in an optimal value of $\alpha = 10$. Next, with $\alpha$ fixed, $\beta$ is tuned, yielding an optimal value of $\beta = 1$. These values strike a balance between the global and greedy weight importance metrics, ensuring effective parameter selection.

\textbf{LoRA rank.} 
We examine the impact of LoRA rank and provide detailed results in Appendix~\ref{app:lorarank}. In summary, the results reveal a U-shaped relationship between rank and performance, reflecting a trade-off between model capacity and dimensionality. The optimal rank of $16$ minimizes loss and is used as the default setting for LoRA-based baseline.

\textbf{Sparsity.}  
We perform a grid search of the sparsity level $\rho$ from $0.1$ to $0.99$.
For full-model fine-tuning, the performance first improves with increasing sparsity and then sharply declines. The peak performance is achieved at $\rho = 0.9$. For LoRA fine-tuning, we jointly optimize sparsity levels and LoRA ranks. %The LoRA rank is searched from 16 to 512. 
The optimal result is found at a LoRA rank of 64 and a sparsity level of $0.8$. Notably, the effective number of parameters is equivalent to $64 \times (1-0.8) = 12.8$, which is less than the best-performing rank of LoRA baseline. This highlights that MaZO can further reduce the dimension while maintaining the model capacity.








