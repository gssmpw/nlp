





\section{Preliminaries}
\label{sec:preliminaries}
\subsection{Zeroth-order optimization}
Zeroth-order (ZO) optimization is a backpropagation-free method that estimates gradients using forward passes only. A common approach for gradient estimation in ZO optimization is the Simultaneous Perturbation Stochastic Approximation (SPSA) \citep{spall1992multivariate}, which serves as a randomized gradient estimator (RGE).

Consider a model with parameters $\theta \in \mathbb{R}^d$ and a loss function $\mathcal{L}(\theta)$. The Taylor expansion of the loss function around $\theta$ is given by:
\begin{equation}
\mathcal{L}(\theta + \delta \theta) = \left( \frac{\partial \mathcal{L}}{\partial \theta} \right)^\top \cdot \delta \theta + \frac{1}{2} \delta \theta^\top \cdot \mathbf{H} \cdot \delta \theta + \mathcal{O}(|\delta \theta|^3),
\end{equation}
where $H$ is the Hessian matrix.

To estimate the gradient, Zeroth-order optimization  replace $\delta \theta$ with $\epsilon \mathbf{z}$ and $-\epsilon \mathbf{z}$, where $\epsilon$ is a small scalar and $\mathbf{z} \sim \mathcal{N}(0, \bm{I}_d)$ is a random perturbation sampled from a normal distribution. Ignoring terms of third degree and higher, the estimated gradient is computed as:
\begin{align}
\widehat{\nabla} \mathcal{L}(\theta)
&= \frac{\mathcal{L}(\theta + \epsilon \mathbf{z}) - \mathcal{L}(\theta - \epsilon \mathbf{z})}{2 \epsilon} \mathbf{z} \\
&\approx \mathbf{z}\mathbf{z}^\top \nabla \mathcal{L}(\theta).
\end{align}

The expectation of the estimated gradient matches the true gradient:
\begin{align}
\mathbb{E}[\widehat{\nabla} \mathcal{L}(\theta)]
&= \mathbb{E}[\mathbf{z}\mathbf{z}^\top \nabla \mathcal{L}(\theta)] \\
&= \mathbb{E}[\mathbf{z}\mathbf{z}^\top] \nabla \mathcal{L}(\theta) \\
&= I_d \nabla \mathcal{L}(\theta) = \nabla \mathcal{L}(\theta),
\end{align}
where $I_d$ is the identity matrix.

During the training process, the random perturbation $\mathbf{z}$ is first sampled. Using forward and reverse perturbations, the losses $\mathcal{L}(\theta + \epsilon \mathbf{z})$ and $\mathcal{L}(\theta - \epsilon \mathbf{z})$ are computed, and the gradient estimate $\widehat{\nabla} \mathcal{L}(\theta)$ is obtained. Finally, the parameters are updated using zeroth-order stochastic gradient descent (ZO-SGD) as follows:
\begin{equation}
\theta = \theta - \eta \widehat{\nabla} \mathcal{L}(\theta),
\end{equation}
where $\eta$ is the learning rate.
\citet{malladi2023mezo} propose MeZO to regenerate the random perturbation $\mathbf{z}$ by random seed instead of storing $\mathbf{z}$ itself, which significantly reduce the memory consumption.


\subsection{Formulating Multi-task Learning}
\label{sec:reformmtl}

Multi-task learning (MTL) aims to improve generalization performance by jointly learning $N$ related tasks $\mathcal{T} = \{T_1, \dots, T_N\}$ through shared parameterization. Consider each task $T_i$ with dataset $\mathcal{D}_i = \{(x_i^{(j)}, y_i^{(j)})\}_{j=1}^{M_i}$, where $M_i$ denotes the sample size. The classical MTL objective minimizes the convex combination of task-specific losses:

\begin{align}
    & \mathcal{L}_{\text{MTL}}(\theta) = \sum_{t=1}^T w_t \mathcal{L}^t_{\text{STL}}(\theta),\\
    & \text{s.t.}\ \sum_{t=1}^T w_t = 1,\ w_t \geq 0
\end{align}

where $\mathcal{L}^t_{\text{STL}}(\theta) = \mathbb{E}_{(x,y)\sim\mathcal{D}_t}[\ell(f_\theta(x), y)]$ is the empirical risk for task $t$, and $\ell(\cdot)$ the loss function. Parameter updates follow:

\begin{equation}
    \theta \leftarrow \theta - \eta \nabla\mathcal{L}_{\text{MTL}}(\theta)
\end{equation}

\noindent\textbf{Challenge of Gradient Collinearity in ZO-MTL.} Under zeroth-order optimization, the task-specific gradient estimates exhibit fundamental collinearity. For task $t$, the ZO gradient estimator is:

\begin{equation}
    g^t_{\text{STL}} = \underbrace{\frac{\mathcal{L}^t_{\text{STL}}(\theta+\epsilon z) - \mathcal{L}^t_{\text{STL}}(\theta-\epsilon z)}{2\epsilon}}_{\text{scalar coefficient}} \cdot z
\end{equation}

where $z \sim \mathcal{N}(0,I_d)$ is the shared random perturbation. The MTL gradient aggregates these estimates:

\begin{align}
    g_{\text{MTL}} &= \sum_{t=1}^T w_t g^t_{\text{STL}} \\
    &= \underbrace{\left(\sum_{t=1}^T w_t \frac{\mathcal{L}^t_{\text{STL}}(\theta+\epsilon z) - \mathcal{L}^t_{\text{STL}}(\theta-\epsilon z)}{2\epsilon}\right)}_{\alpha(z)} \cdot z,
\end{align}

which is just a scaler times $z$.
This reveals the limitation: all task gradients align with $z$, losing directional diversity

Existing dynamic weighting schemes that manipulate $\{w_t\}$ only scale the scalar coefficient $\alpha(z)$, but cannot resolve directional conflicts. This necessitates a paradigm shift from loss weighting to parameter space interventions.

To overcome this limitation, we propose a novel multi-task learning approach that operates at the weight level rather than the loss scale or the gradient.