\section{Related Work}

\zz{We need to merge Section 2 and Section 3.}
\subsection{Zeroth-Order Optimization}
Zeroth-order (ZO) optimizers utilize just two forward passes to estimate gradients without backpropagation, significantly lowering the GPU hours and memory usage compared to first-order (FO) optimizers \citep{tang2024private}. This offers a considerable advantage for fine-tuning large language models (LLMs), where the memory-intensive backpropagation process becomes a bottleneck as model sizes keep growing \citep{yang2024adazeta}.
Researchers have focused on improving the convergence rates and reducing gradient estimation variance of ZO optimization for LLM fine-tuning. Increasing batch size can diminish noise in ZO gradient estimation \citep{gautam2024variance}. Perturbing a subset of model parameters, either through random and sparse pruning masks or block-coordinate perturbations, also lowers gradient variance \citep{guo2024zeroth}. Additionally, some approaches try to reduce the number of trainable parameters through parameter-efficient fine-tuning (PEFT) techniques like tensorized adapters \citep{yang2024adazeta}.
Convergence theories for ZO optimization have been elaborated in both convex and non-convex settings. However, these convergence rates typically increase linearly with the number of trainable parameters 1. More recent work has focused on developing specialized ZO optimization methods with improved theoretical guarantees, such as the stagewise DP zeroth-order method (DP-ZOSO) \citep{liu2024differentially} and the Hessian-informed zeroth-order optimizer (HiZOO) \citep{zhao2024second}.

\subsection{Multi-task Learning under First-Order Setting}

Multi-task learning (MTL) has been widely studied in the context of deep learning, where the goal is to leverage shared representations across multiple related tasks to improve the overall performance. Several works have explored first-order optimization methods for MTL.
One line of research focuses on using scalarization techniques to combine the losses of multiple tasks into a single objective function, which can then be optimized using standard first-order methods like stochastic gradient descent (SGD) \citep{hu2024revisiting}. However, these approaches have been shown to struggle in reaching the Pareto frontier, which represents the optimal trade-off between the different task objectives .
To address this issue, some studies have proposed using uncertainty weighting to balance the losses of different tasks during training \citep{xin2022current}. This approach aims to automatically adjust the importance of each task based on the model's confidence, rather than relying on manual task weighting.
Another direction is to leverage the task relationships or affinities to guide the optimization process. For example, \citet{li2024scalable} introduced a gradient-based estimation method to approximate the fine-tuning performance of different task subsets, allowing for efficient subset selection during multi-task learning \citep{samant2022framework}. This approach can be particularly useful for large-scale language model fine-tuning, where choosing the right auxiliary tasks is crucial.
