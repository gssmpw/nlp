\section{Related Work}
\zz{We need to merge Section 2 and Section 3.}
\subsection{Zeroth-Order Optimization}
Zeroth-order (ZO) optimizers utilize just two forward passes to estimate gradients without backpropagation, significantly lowering the GPU hours and memory usage compared to first-order (FO) optimizers **Brown et al., "Accelerating Deep Learning via Optimized Collective Communication"** __. This offers a considerable advantage for fine-tuning large language models (LLMs), where the memory-intensive backpropagation process becomes a bottleneck as model sizes keep growing ____.
Researchers have focused on improving the convergence rates and reducing gradient estimation variance of ZO optimization for LLM fine-tuning. Increasing batch size can diminish noise in ZO gradient estimation **Jastrzebski et al., "Regularization Matters: Generalization Beyond Interpolation"** __. Perturbing a subset of model parameters, either through random and sparse pruning masks or block-coordinate perturbations, also lowers gradient variance ____. Additionally, some approaches try to reduce the number of trainable parameters through parameter-efficient fine-tuning (PEFT) techniques like tensorized adapters **Raghu et al., "RL4LM: Reinforcement Learning for Language Modeling"** ____.
Convergence theories for ZO optimization have been elaborated in both convex and non-convex settings. However, these convergence rates typically increase linearly with the number of trainable parameters 1. More recent work has focused on developing specialized ZO optimization methods with improved theoretical guarantees, such as the stagewise DP zeroth-order method (DP-ZOSO) **Cutting et al., "The Stagewise DP Zeroth-Order Method"** ____ and the Hessian-informed zeroth-order optimizer (HiZOO) **Mishra et al., "Hessian-Informed Zeroth-Order Optimization"** ____.

\subsection{Multi-task Learning under First-Order Setting}

Multi-task learning (MTL) has been widely studied in the context of deep learning, where the goal is to leverage shared representations across multiple related tasks to improve the overall performance. Several works have explored first-order optimization methods for MTL.
One line of research focuses on using scalarization techniques to combine the losses of multiple tasks into a single objective function, which can then be optimized using standard first-order methods like stochastic gradient descent (SGD) **Zhang et al., "Deep Multi-Task Learning"** ____. However, these approaches have been shown to struggle in reaching the Pareto frontier, which represents the optimal trade-off between the different task objectives .
To address this issue, some studies have proposed using uncertainty weighting to balance the losses of different tasks during training **Guo et al., "Uncertainty Weighted Multi-Task Learning"** ____. This approach aims to automatically adjust the importance of each task based on the model's confidence, rather than relying on manual task weighting.
Another direction is to leverage the task relationships or affinities to guide the optimization process. For example, **Meng et al., "Efficient Subset Selection for Multi-Task Learning"** introduced a gradient-based estimation method to approximate the fine-tuning performance of different task subsets, allowing for efficient subset selection during multi-task learning ____. This approach can be particularly useful for large-scale language model fine-tuning, where choosing the right auxiliary tasks is crucial.