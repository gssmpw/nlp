\section{Additional Explanation on Hessian and Gradient Approximation}
\label{app:approxiation}

Consider a linear layer in an LLM that computes:
\begin{equation}
\mathbf{y} = \mathbf{W} \mathbf{x},
\end{equation}
where \(\mathbf{W} \in \mathbb{R}^{m \times n}\), \(\mathbf{x} \in \mathbb{R}^{n}\), and \(\mathbf{y} \in \mathbb{R}^{m}\). Focusing on one particular linear component, let us analyze a single row \(\mathbf{w}_i \in \mathbb{R}^{n}\) of \(\mathbf{W}\). The corresponding output is given by:
\begin{equation}
\mathbf{y}_i = \mathbf{w}_i \, \mathbf{x},
\end{equation}
which is a scalar.

To analyze the sensitivity of the loss \(\mathcal{L}\) with respect to \(\mathbf{w}_i\), we perform a second-order Taylor expansion of \(\mathcal{L}\) with respect to \(y_i\):
\begin{align}
\mathcal{L}(\mathbf{y}_i + \Delta \mathbf{y}_i) &\approx \mathcal{L}(\mathbf{y}_i) + \nabla \mathcal{L}(\mathbf{y}_i)\,\Delta \mathbf{y}_i \notag \\
&\quad + \frac{1}{2}\nabla^2 \mathcal{L}(\mathbf{y}_i)\,(\Delta \mathbf{y}_i)^2.
\end{align}
Since \(\mathbf{y}_i\) is a scalar, its second derivative \(\nabla^2 \mathcal{L}(\mathbf{y}_i)\) is also a scalar.

Now, the change in \(\mathbf{y}_i\) due to a change in the weights is
\begin{equation}
\Delta \mathbf{y}_i = \Delta \mathbf{w}_i\, \mathbf{x}.
\end{equation}
Substituting this into the second-order term yields:
\begin{equation}
\frac{\partial^2 \mathcal{L}}{\partial \mathbf{w}_i^2} \approx \nabla^2 \mathcal{L}(\mathbf{y}_i)\, (\mathbf{x} \mathbf{x}^\top).
\end{equation}

Since we are primarily interested in comparing weight importance along the row direction, the absolute scale of the Hessian is not crucial. In practice, we can drop the multiplicative factor \(\nabla^2 \mathcal{L}(y_i)\) (or, equivalently, assume it to be a constant) and write:
\begin{equation}
\frac{\partial^2 \mathcal{L}}{\partial \mathbf{w}_i^2} \propto \mathbf{x} \mathbf{x}^\top.
\end{equation}

Similarly, one can derive a first-order approximation for the gradient. By retaining only the first-order term of the Taylor expansion, we have:
\begin{equation}
\mathcal{L}(\mathbf{y}_i + \Delta \mathbf{y}_i) \approx \mathcal{L}(\mathbf{y}_i) + \nabla \mathcal{L}(\mathbf{y}_i)\, \Delta \mathbf{y}_i.
\end{equation}
With \(\Delta \mathbf{y}_i = \Delta \mathbf{w}_i \, \mathbf{x}\), the gradient with respect to \(\mathbf{w}_i\) becomes:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mathbf{w}_i} \approx \nabla \mathcal{L}(\mathbf{y}_i)\, \mathbf{x}.
\end{equation}
Similarly, since we are only interested in the relative value, the factor is dropped:
\begin{equation}
\mathbf{g} \propto \mathbf{x}.
\end{equation}

This derivation shows that, by considering each row independently (row-wise), we avoid the immense complexity involved in computing the full Hessian matrix (which is high-dimensional and difficult to characterize even under diagonalization assumptions). In other words, computing the Hessian row-wise allows us to circumvent the problem of determining the eigenvalues or even a reliable diagonal approximation of the full Hessian.



%\label{mazopseudocode}
\begin{algorithm}[t!]
\caption{MaZO LLM Fine-Tuning Framework}
\label{alg:mazopseudocode}
\begin{algorithmic}[t]
\State \textbf{Input:} 
     Pre-trained LLM parameters $\theta$, training data, tasks $t = 1, \dots, T$, 
     sparsity level $\rho$, hyperparameters $\alpha$, $\beta$, learning rate $\eta$
\State \textbf{Output:} Updated parameters $\theta^*$

\For{each task $t=1$ to $T$}
    \State Collect evaluation data
    \State \textbf{Compute} $\mathbf{S}^t_{\text{global}}$ with eq. (\ref{eq:globalscore})
    \State \textbf{Compute} $\mathbf{S}^t_{\text{greedy}}$ with eq. (\ref{eq:greedyscore})
    \State \textbf{Combine} scores:
        \[
        \mathbf{S}^t = \mathbf{S}^t_{\text{global}} + \alpha\,\mathbf{S}^t_{\text{greedy}} + \beta|\mathbf{W}|
        \]
    \State \textbf{Normalize} $\mathbf{S}^t$ and get $\hat{\mathbf{S}}^t$ with eq. (\ref{eq:rownorm})
\EndFor

\State \textbf{Aggregate:} Sum row-wise normalized scores across tasks:
    \[
    \mathbf{S} = \sum_{t=1}^{T}\hat{\mathbf{S}}^t
    \]
\For{each weight $\mathbf{W}$ in LLM}
\For{each row $i$ in $\mathbf{W}$}
    \State Select top $k$ parameters in each row according to corresponding $\mathbf{S}$ and construct weight update mask $\mathbf{M}$ 
\EndFor
\EndFor
\For{each training step}
\State \textbf{Compute} weight update $\Delta \mathbf{W}$ using ZO optimization
\State \textbf{Apply mask:} 
    \[
    \Delta \mathbf{W}_{\text{masked}} = \Delta \mathbf{W} \odot \mathbf{M}
    \]
\State \textbf{Update} parameters: 
    \[
    \theta \leftarrow \theta + \eta \Delta \mathbf{W}_{\text{masked}}
    \]
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Pseudo-code of MaZO}
\label{app:pseudocode}
The pseudo-code of the whole MaZO LLM fine-tuning framework is shown as Algorithm~\ref{alg:mazopseudocode}.

\section{Baseline}
\label{app:baseline}

\subsection{CoBa: Convergence Balancer for Multitask Finetuning}
CoBa (Convergence Balancer) \citep{gong2024coba} is a novel multi-task learning (MTL) method designed for large language models (LLMs). It dynamically adjusts task weights during training to ensure balanced convergence across tasks, while maintaining computational efficiency. 

Consider an LLM parameterized by $\theta \in \mathbb{R}^m$, trained on $T \geq 2$ tasks. The loss function for task $t$ at iteration $i$ is denoted as $\mathcal{L}^t(\theta; i): \mathbb{R}^m \to \mathbb{R}_{\geq 0}$. The overall optimization objective is:
\begin{equation}
\min_{\theta \in \mathbb{R}^m} \mathcal{L}(\theta; i) = \sum_{t=1}^T \omega_t(i) \mathcal{L}^t(\theta; i),
\end{equation}
where $\omega_t(i)$ is the weight of task $t$ at iteration $i$. A uniform weight assignment $\omega_t(i) = \frac{1}{T}$ ensures equal attention to all tasks but often leads to varying convergence rates. CoBa dynamically adjusts $\omega_t(i)$ to balance these rates, prioritizing generalization by deriving weights from validation losses instead of training losses.
CoBa is built upon three main components:

\textbf{Relative Convergence Score (RCS)} dynamically allocates smaller weights to tasks that converge faster and larger weights to slower-converging tasks. It is computed as:
\begin{equation}
\label{eq:rcs}
\text{RCS}_t(i) = \text{softmax}_t \left( T \frac{\alpha_t(i)}{\sum_{t'=1}^T |\alpha_{t'}(i)|} \right),
\end{equation}
where $\alpha_t(i)$ is the convergence slope of task $t$, derived from the normalized validation loss ratio over a sliding window of $N$ iterations. The softmax operation ensures differentiation across tasks, with faster-converging tasks receiving lower weights.

\textbf{Absolute Convergence Score (ACS)} addresses task divergence by reducing weights for diverging tasks and increasing weights for converging tasks. It is computed as:
\begin{equation}
\label{eq:acs}
\text{ACS}_t(i) = \text{softmax}_t \left( -N \frac{\alpha_t(i)}{\sum_{j=i-N+1}^i |\alpha_t(j)|} \right),
\end{equation}
where normalization is performed along the historical iteration dimension, isolating a task's own trajectory. ACS ensures tasks with consistent convergence receive higher weights while diverging tasks are penalized.

\textbf{Divergence Factor (DF)} determines the relative influence of RCS and ACS on the final task weights. It is defined as:
\begin{equation}
\label{eq:df}
\text{DF}(i) = \min \left( \text{softmax}_i \left( \frac{i \cdot \alpha_{\text{max}}(i)}{\sum_{j=1}^i \alpha_{\text{max}}(j)} \right), 1 \right),
\end{equation}
where $\alpha_{\text{max}}(i)$ is the largest convergence slope across all tasks at iteration $i$. DF ensures RCS dominates when all tasks are converging, while ACS takes precedence when divergence is detected.

\textbf{The final task weights} $\omega_t(i)$ are computed as:
\begin{equation}
\label{eq:cobaw}
\omega_t(i) = \text{DF}(i) \cdot \text{RCS}_t(i) + (1 - \text{DF}(i)) \cdot \text{ACS}_t(i),
\end{equation}
allowing a seamless transition between RCS and ACS dominance based on task convergence trends.


\textbf{The convergence slope} $\alpha_t(i)$ for task $t$ is calculated based on the normalized validation loss ratio $\bar{\mathcal{L}}_t^{\text{val}}(\theta; i)$. Specifically, we fit a linear model to the validation loss ratios over a sliding window of $N$ iterations. The observations are defined as:
\begin{equation}
\mathbf{x}_t(i) = [i, 1]^\top, \quad \mathbf{X}_t(N; i) = [\mathbf{x}_t(s_0), \dots, \mathbf{x}_t(i)]^\top,
\end{equation}
\begin{equation}
\mathbf{y}_t(N; i) = [\bar{\mathcal{L}}_t^{\text{val}}(\theta; s_0), \dots, \bar{\mathcal{L}}_t^{\text{val}}(\theta; i)]^\top,
\end{equation}
where $s_0 = \max(0, i - N + 1)$ is the starting step of the sliding window. The goal is to compute the coefficient vector $c_t(N; i) = [\alpha_t(N; i), \beta_t(N; i)]^\top$ that minimizes the mean squared error (MSE) between the predicted and actual validation loss ratios:
\begin{equation}
c_t = \arg \min_{c_t} \frac{1}{2} (\mathbf{X}_t c_t - \mathbf{y}_t)^\top (\mathbf{X}_t c_t - \mathbf{y}_t).
\end{equation}

The closed-form solution for $c_t$ is given by:
\begin{equation}
c_t = (\mathbf{X}_t^\top \mathbf{X}_t)^{-1} \mathbf{X}_t^\top \mathbf{y}_t.
\end{equation}


\paragraph{Algorithm}
The CoBa algorithm is summarized in Algorithm~\ref{alg:coba}, We use $M = 4$ with $\text{batchsize}=16$

\begin{algorithm}[t]
\caption{CoBa Algorithm}
\label{alg:coba}
\begin{algorithmic}[1]
\Require Initial parameters $\theta_0$, $M$ batches of validation set, history window length $N = 5M$, warm-up steps $W = M$, number of tasks $T$, initial weights $\omega_i(0) = \frac{1}{T}$.
\Ensure Trained parameters $\theta$.
\For{$t = 0$ to $T$}
    \State Compute $\mathcal{L}(\theta; i)$ with training batch $x_i$.
    \State Compute $\bar{\mathcal{L}}_t^{\text{val}}(\theta; t)$ with validation batch $v_i$.
    \State Update validation loss history $\mathbf{y}_t(N; i)$.
    \State Compute $\alpha_t(i)$.
    \If{$i > W$}
        \State Compute $\text{RCS}(i)$, $\text{ACS}(i)$, and $\text{DF}(i)$ using Eqs. (\ref{eq:rcs}), (\ref{eq:acs}), and (\ref{eq:df}).
        \State Update task weights $\omega_t(i)$ using Eq. (\ref{eq:cobaw}).
    \Else
        \State Set $\omega_t(i) = \frac{1}{T}$.
    \EndIf
\State Update model parameters $\theta$ using weighted loss $\mathcal{L}(\theta; i)$.
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{FAMO: Fast Adaptive Multitask Optimization}

Fast Adaptive Multitask Optimization (FAMO) is a dynamic weighting method designed to address the challenges of multitask learning (MTL), where directly optimizing the average loss across tasks often leads to under-optimization of certain tasks. FAMO ensures balanced task loss reduction using only $O(1)$ space and time per iteration, making it computationally efficient and scalable.

The complete FAMO algorithm is summarized in Algorithm~\ref{alg:famo}.

\begin{algorithm}[t]
\caption{PyTorch Implementation of Wanda}
\label{alg:wanda}
\begin{algorithmic}
\State \textbf{Input:} Weight matrix $\mathbf{W} \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}}}$, input activations $\mathbf{X} \in \mathbb{R}^{(N \cdot L) \times C_{\text{in}}}$, sparsity ratio $s \in [0, 1]$
\State \textbf{Output:} Pruned weight matrix $\mathbf{W}$
\State Compute importance scores: $\text{metric} = \mathbf{W}.\text{abs()} \cdot \mathbf{X}.\text{norm}(p=2, \text{dim}=0)$
\State Sort scores \textbf{within each row}: $\_, \text{sorted\_idx} = \text{torch.sort}(\text{metric}, \text{dim}=1)$
\State Identify indices to prune: $\text{pruned\_idx} = \text{sorted\_idx}[:, :\lfloor C_{\text{in}} \cdot s \rfloor]$
\State Set pruned weights to zero: $\mathbf{W}.\text{scatter\_}(\text{dim}=1, \text{index}=\text{pruned\_idx}, \text{src}=0)$
\State \textbf{Return} $\mathbf{W}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Fast Adaptive Multitask Optimization (FAMO)}
\label{alg:famo}
\begin{algorithmic}[1]
\Require Initial model parameters $\theta_0$, task losses $\{\mathcal{L}_{t,i}\}_{t=1}^T$, learning rates $\alpha$ and $\beta$, decay factor $\gamma$.
\State Initialize logits: $\mathbf{\xi}_1 \gets \mathbf{0}$.
\For{$i = 1$ to $T$}
    \State Compute task weights: 
    \[
    \mathbf{z}_i = \operatorname{Softmax}(\mathbf{\xi}_t),
    \]
    where for each $i$, 
    \[
    \mathbf{z}_{t,i} = \frac{\exp(\mathbf{\xi}_{t,i})}{\sum_{t'=1}^T \exp(\mathbf{\xi}_{t',i})}.
    \]
    \State Update model parameters:
    \[
    \theta_{t+1} = \theta_t - \alpha\,  \sum_{t=1}^T \left( c_t \frac{\mathbf{z}_{t,i}}{\mathcal{L}_{t,i}}\,\right) \nabla \mathcal{L}_{t,i},
    \]
    \[
    \text{with} \quad
    c_i = \left(\sum_{i=1}^k \frac{\mathbf{z}_{t,i}}{\mathcal{L}_{t,i}} \right)^{-1}.
    \]
    \State Compute the vector of log-loss differences:
    \[
    d_i = \begin{bmatrix}
      \log \mathcal{L}_{1,i} - \log \mathcal{L}_{1,i+1} \\
      \vdots \\
      \log \mathcal{L}_{T,i} - \log \mathcal{L}_{T,i+1}
    \end{bmatrix}.
    \]
    \State Compute the Jacobian of the softmax function:
    \[
    (J_i)_{tt'} = \frac{\partial z_{t,i}}{\partial \xi_{t',i}} = z_{t,i} (\delta_{tt'} - z_{t',i}).
    \]
    \State Aggregate the gradient by the chain rule:
    \[
    \delta_i = J_i^\top\, d_i.
    \]
    \State Update logits:
    \[
    \xi_{i+1} = \xi_i - \beta \bigl( \delta_i + \gamma\, \xi_i \bigr).
    \]
\EndFor
\end{algorithmic}
\end{algorithm}



\subsection{MTL-LoRA}
MTL-LoRA (Multi-Task Learning LoRA) is a parameter-efficient fine-tuning method designed to enhance the multi-task learning (MTL) capabilities of large language models (LLMs). It builds upon the Low-Rank Adaptation (LoRA) framework by addressing the challenges of task interference and suboptimal information sharing in multi-task scenarios.

LoRA is a parameter-efficient fine-tuning method that freezes the majority of a pre-trained model's parameters and introduces trainable low-rank matrices to approximate gradient updates. For a weight matrix $\mathbf{W} \in \mathbb{R}^{d \times k}$ in the original model, LoRA decomposes the gradient update $\Delta W$ into two low-rank matrices $\mathbf{B} \in \mathbb{R}^{d \times r}$ and $\mathbf{A} \in \mathbb{R}^{r \times k}$, where $r \ll \min(d, k)$. The updated weight matrix is expressed as:
\[
\mathbf{W}' = \mathbf{W} + \Delta \mathbf{W} = \mathbf{W} + \mathbf{BA}.
\]

The output of the updated layer for an input $x$ is:
\[
\mathbf{h} = (\mathbf{W} + \mathbf{BA})\mathbf{x}.
\]

MTL-LoRA enhances LoRA by introducing task-specific transformations and dynamic information-sharing strategies.

\paragraph{Task-Specific Transformation.} 
MTL-LoRA introduces a learnable task-specific transformation matrix $\mathbf{\Lambda}_t \in \mathbb{R}^{r \times r}$ for each task $t$. For an input $\mathbf{x}_t$ corresponding to task $t$, the low-rank projection is modified as:
\[
\mathbf{z}_t = \mathbf{\Lambda}_t \mathbf{A} \mathbf{x}_t,
\]
where $\mathbf{A} \in \mathbb{R}^{r \times k}$ is the shared low-rank matrix.

\paragraph{Dynamic Information Sharing.}
To improve cross-task information sharing, MTL-LoRA employs multiple up-projection matrices $\mathbf{B}_i \in \mathbb{R}^{d \times r}$ ($i = 1, \dots, n$) and combines their outputs using a weighted averaging strategy. The final output for task $t$ is computed as:
\[
\mathbf{h}_t = \mathbf{W} \mathbf{x}_t + \sum_{i=1}^{n} \frac{\exp(w_i^t / \tau)}{\sum_{j=1}^{n} \exp(w_j^t / \tau)} \mathbf{B}_i \mathbf{z}_t,
\]
where $w_i^t$ are learnable weights for task $t$, and $\tau$ is a temperature hyperparameter controlling the sharpness of the weight distribution.

We set number of up-projection matrices $n$ to 3, rank to 16 and temperature $\tau$ to 0.5

\section{LoRA Rank}
\label{app:lorarank}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{img/rank_loss.pdf} 
    \caption{The loss curve with different LoRA rank. }
    \label{fig:rankloss}
\end{figure}  
% \zz{This convergence curve will also cause lots of troubles. Instead, please use a table to show the accuracy or evaluation loss under various ranks.}
We investigate the influence of LoRA rank on the model's final performance. Initially, we exclude weight masking and fine-tune the model with different LoRA ranks. The evaluation loss curves for ranks ranging from $1$ to $512$ are plotted in Figure~\ref{fig:rankloss}. As the rank increases, the loss forms a U-shaped curve, with the lowest point occurring at a rank of $16$. Ideally, the trend of the lowest point in first-order (FO) optimization should follow the green dashed line in Figure~\ref{fig:rankloss}. However, in the zeroth-order (ZO) setting, the larger parameter optimization space as rank increases leads to a deviation from this ideal trend. 

This U-shaped curve highlights a critical trade-off: while increasing the rank improves the model's capacity, it simultaneously introduces challenges in optimizing a larger parameter space under ZO settings. This observation directly motivates our exploration of sparsity and mask selection strategies, which aim to reduce the number of parameters being optimized while retaining the most important ones. By identifying and focusing on the most critical parameters, we can mitigate the challenges posed by ZO optimization and achieve better performance, as demonstrated by our MaZO approach.

\section{Memory Usage and Search Time}
\label{sec:memory usage}

To evaluate the efficiency of MaZO, we compare its memory usage, search time, and training time against baseline vanilla multi-task learning ZO methods, both with and without LoRA. Table~\ref{tab:memory_time_comparison} summarizes the results.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|c|c|c}
\hline
\textbf{Method}             & \textbf{Memory (GB)} & \textbf{Search Time (min)} & \textbf{Training Time (h)} \\ \hline
MTL-ZO        &            29.0              &             -              &                  14.3           \\ 
MaZO          &            33.3               &             42              &                 16.6            \\ 
$\text{MTL-ZO}_{\text{LoRA}}$             &            31.2               &            -               &      13.7                       \\ 
$\text{MaZO}_{\text{LoRA}}$                   &            33.9               &        8.5                   &           14.1                  \\ \hline
\end{tabular}}
\caption{Comparison of memory usage, search time, and training time between MTL-ZO and MaZO, with and without LoRA. \textit{MTL} refers to multi-task learning. While MaZO introduces marginal memory and runtime overhead due to the mask storage and search process, it achieves significantly better accuracy as shown in Tables 1 and 2, demonstrating its effectiveness and practicality. Note that the memory requirement exceeds the model size (7B) because we use a batch size of 16 and a maximum token length of 600. }
\vspace{-10pt} 
\label{tab:memory_time_comparison}
\end{table}
% \zz{This table can mislead the reviewers, and they may think that our method does not have benefit. Consider two options: (1) delete this table and explain in the body text that our method achieves much better convergence with negligible memory and run-time overhead. (2) keep this table but emphasize in the caption that our method has marginal memory and run-time overhead, while achieving much better accuracy.}

The search time introduced by MaZO is negligible compared to the overall training time.
MaZO incurs a slight increase in memory usage (approximately $10\%$) compared to baseline multi-task learning ZO methods. This is primarily due to the additional storage required for the weight update mask. However, this increase is marginal and does not significantly impact the overall memory efficiency, especially when combined with LoRA, where the parameter space is already reduced.
While MaZO introduces a small memory overhead, its benefits in terms of faster convergence and reduced gradient variance outweigh this cost, making it an effective and practical solution for multi-task fine-tuning under ZO optimization.

% \section{Hyperparameter Search for Sparsity}
% \label{app:sparsity}


\section{Details of Different Weight Score Metrics}
\label{app:metrics}

\subsection{Wanda: Pruning by Weights and Activations}

In this section, we introduce \textbf{Wanda} (Pruning by Weights and Activations), a simple yet effective method for pruning large language models (LLMs). Wanda can induce high sparsity in pretrained LLMs without requiring retraining or weight updates, making it computationally efficient and easy to implement.

The key idea of Wanda is to evaluate the importance of each weight based on both its magnitude and the corresponding input activation. Specifically, for a linear layer with weight matrix $\mathbf{W} \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}}}$ and input activations $\mathbf{X} \in \mathbb{R}^{(N \cdot L) \times C_{\text{in}}}$, the importance score $\mathbf{S}_{ij}$ of weight $\mathbf{W}_{ij}$ is defined as:
\begin{equation}
    \mathbf{S}_{ij} = |\mathbf{W}_{ij}| \cdot \|\mathbf{X}_j\|_2,
\end{equation}
where $|\mathbf{W}_{ij}|$ is the absolute value of the weight, and $\|\mathbf{X}_j\|_2$ is the $\mathcal{L}_2$ norm of the $j$-th column of $\mathbf{X}$, aggregated across all tokens in the batch and sequence dimensions. This metric effectively combines weight magnitude and input activation information to determine the importance of each weight.

Unlike traditional pruning methods that compare weights globally or layer-wise, Wanda adopts a per-output comparison strategy (the same as our row-wise comparison). For a weight $\mathbf{W}_{ij}$ connecting input $j$ to output $i$, its comparison group is defined as all weights connected to the same output $i$:
\begin{equation}
    \mathbf{G}_{ij} = \{\mathbf{W}_{uv} \,|\, u = i\}.
\end{equation}
Within each comparison group, weights are ranked by their importance scores $\mathbf{S}_{ij}$, and a predefined sparsity ratio $s\%$ is applied to prune the lowest-ranked weights. 



\subsection{Other Metrics}

In this section, we introduce two additional heuristic weight importance metrics: random and magnitude.

For the random metric, we randomly select $50\%$ of the weights. It is important to note that the comparison group is the entire set of weights, rather than a single row.

For the magnitude metric, we select weights with the smallest values in a weight, following the approach described by \citet{liu2024sparse}.



% \section{More Experiments on Mistral-7B}
% \label{app:mistral}
% The results for Mistral-7B are shown in Table \ref{tab:mistral_performance_comparison}.

% \begin{table*}[t!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{lcccccccccc}
% \toprule
% Task & \multirow{2}{*}{Multi-task} & \textbf{SST-2} & \textbf{BoolQ} & \textbf{RTE} & \textbf{WSC} & \textbf{WiC} & \textbf{MultiRC} & \textbf{COPA} & \textbf{SQuAD} & \multirow{2}{*}{\textbf{Avg}} \\ 
% Task Type & & \multicolumn{6}{c}{------------------------ classification ------------------------} & \multicolumn{1}{c}{-- multiple choice --} & \multicolumn{1}{c}{--- generation ---}\\
% \midrule
% $\text{ZO}_{\text{STL}}$        & \ding{55} & 93.6           & 77.8           & 74.2         & 55.3         & 62.1         & 62.7            & 88.0          & 76.5          & 73.8         \\
% Zero-Shot         & \ding{51} & 56.7           & 42.4           & 50.5         & 52.8         & 50.3         & 43.6            & 79.0          & 57.2          & 54.1         \\
% ICL               & \ding{51} & 62.3           & 46.1           & 56.0         & 53.2         & 61.4         & 53.4            & 79.0          & 62.3          & 59.2         \\
% $\text{ZO}_{\text{MTL}}$      & \ding{51} & 58.7           & 47.2           & 55.0         & 53.2         & 59.8         & 54.4            & 79.0          & 56.3          & 58.0         \\
% $\text{ZO}_{\text{LoRA}}$           & \ding{51} & 89.3           & \textbf{73.2}           & 71.5         & 51.3         & 58.1         & 53.4            & 79.0          & \textbf{73.5}          & 68.7         \\
% \midrule
% \textbf{MaZO}     & \ding{51} & 83.4  & 56.3  & 60.2 & 54.8 & 58.1 & 55.8 & 79 & 59.4 & 63.4 \\ 
% \textbf{$\text{MaZO}_{\text{LoRA}}$}    & \ding{51} & \textbf{90.2}  & 72.4  & \textbf{74.2} & \textbf{54.8} & \textbf{62.1} & \textbf{57.3}   & \textbf{82.0} & \textbf{73.5} & \textbf{70.8} \\ 
% \bottomrule
% \end{tabular}}
% \caption{Performance comparison across tasks using different methods on Mistral-7B. The setting and notation are the same as Table~\ref{tab:performance_comparison}. We exclude the FO MTL methods as they do not have significant improvement.}
% \label{tab:mistral_performance_comparison}
% \end{table*}


\section{Task Details}
\label{appendix:task_details}

We consider a diverse set of natural language understanding (NLU) and natural language generation (NLG) tasks. 

\subsection{Natural Language Understanding Tasks}
We select tasks from the GLUE \citep{wang-etal-2018-glue} and SuperGLUE \citep{wang2019superglue} benchmarks:
\begin{itemize}
    \item \textbf{SST-2} (Stanford Sentiment Treebank): A binary sentiment classification task.
    \item \textbf{BoolQ}: A yes/no question-answering task.
    \item \textbf{RTE} (Recognizing Textual Entailment): A binary classification task for textual entailment.
    \item \textbf{WSC} (Winograd Schema Challenge): A pronoun resolution task.
    \item \textbf{WiC} (Word-in-Context): A word sense disambiguation task.
    \item \textbf{MultiRC} (Multi-Sentence Reading Comprehension): A question-answering task where each question has multiple correct answers.
    \item \textbf{COPA} (Choice of Plausible Alternatives): A multiple-choice task for causal reasoning.
\end{itemize}

\subsection{Natural Language Generation Task}
For natural language generation, we include:
\begin{itemize}
    \item \textbf{SQuAD} \citep{rajpurkar2016squad}: A question-answering dataset where the model generates text-based answers from a given passage.
\end{itemize}

\subsection{Dataset Splits and Evaluation Metrics}
To ensure computational feasibility, we randomly sample 500 instances for training, 250 for validation, and 500 for testing for each task. Performance is measured using F1 score or accuracy, depending on the task.


\section{Discussion of Collinearity}
\label{app:collinear}
\textbf{Task-specific ZO gradients:} For each task \( t \in \{1,\dots,T\} \), the zeroth-order gradient estimate is given by
\begin{equation}
    \mathbf{g}^t = \frac{\mathcal{L}^t(\theta+\epsilon \mathbf{z}) - \mathcal{L}^t(\theta-\epsilon \mathbf{z})}{2\epsilon} \, \mathbf{z} \equiv \alpha_t \mathbf{z},
\end{equation}
where \(\alpha_t\) is a scalar. Thus, every \(\mathbf{g}^t\) is a scalar multiple of the same random direction \(\mathbf{z}\).

\textbf{Span of all task gradients:} The space spanned by the set of all task gradients is
\begin{equation}
    \operatorname{span}\{\mathbf{g}^1, \mathbf{g}^2, \dots, \mathbf{g}^T\} = \operatorname{span}\{\mathbf{z}\}.
\end{equation}
Therefore, the dimension of this span is
\begin{equation}
    \dim\left(\operatorname{span}\{\mathbf{g}^1, \mathbf{g}^2, \dots, \mathbf{g}^T\}\right) = 1.
\end{equation}

\textbf{Aggregated gradient:} The combined gradient used for the update is
\begin{equation}
    \mathbf{g} = \sum_{t=1}^T w_t \mathbf{g}^t = \left(\sum_{t=1}^T w_t \alpha_t \right) \mathbf{z},
\end{equation}
which clearly lies in the one-dimensional subspace spanned by \(\mathbf{z}\).

\textbf{Gradient covariance matrix:} Define the covariance matrix of the task gradients as
\begin{equation}
    \mathbf{C} = \sum_{t=1}^T \pi_t \left( \mathbf{g}^t - \bar{\mathbf{g}} \right)\left( \mathbf{g}^t - \bar{\mathbf{g}} \right)^\top,
\end{equation}
where \(\pi_t\) are probability weights (or simply \(1/T\) for uniform weighting) and the mean gradient is
\begin{equation}
    \bar{\mathbf{g}} = \sum_{t=1}^T \pi_t \mathbf{g}^t.
\end{equation}
Since \(\mathbf{g}^t = \alpha_t \mathbf{z}\), we have
\begin{equation}
    \mathbf{g}^t - \bar{\mathbf{g}} = (\alpha_t - \bar{\alpha}) \mathbf{z}, \quad \text{with } \bar{\alpha} = \sum_{t=1}^T \pi_t \alpha_t.
\end{equation}
Thus, the covariance matrix becomes
\begin{equation}
    \mathbf{C} = \left(\sum_{t=1}^T \pi_t (\alpha_t - \bar{\alpha})^2 \right) \mathbf{z} \mathbf{z}^\top.
\end{equation}
Since \(\mathbf{z}\mathbf{z}^\top\) is an outer product of a vector with itself, it has rank 1. Hence,
\begin{equation}
    \operatorname{rank}(\mathbf{C}) = 1.
\end{equation}

\textbf{Conclusion:} The lack of \emph{directional diversity} in the task gradients is mathematically captured by the fact that all task-specific gradients lie in a one-dimensional subspace, and the covariance matrix of these gradients has rank 1. This indicates that no matter how many tasks are aggregated, the update direction remains confined to a single direction \(\mathbf{z}\) in the parameter space.
