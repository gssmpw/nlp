\section{Introduction}
\begin{figure*}[t]
    \centering
    \vspace{-8pt} 
    \includegraphics[width=0.8\textwidth]{img/radar_plots.pdf}
    \vspace{-8pt} 
    \caption{Radar chart comparing the performance of our MaZO method with other methods on LLaMA-2-7B and Mistral-7B. Larger is better. Shared model means we train the model on one task and test it on all tasks.}
    \vspace{-10pt} 
    \label{fig:radar}
\end{figure*}

Large language models (LLMs) have revolutionized natural language processing, enabling breakthroughs in various applications \citep{claude35sonnet,gemini20,gpt4o,bai2023qwen}. However, the large sizes of LLMs pose significant memory challenges during training. Traditional first-order (FO) optimization uses backpropagation, which requires substantial memory to store intermediate activations and gradients \citep{rostam2024achieving, kundu2024performance}. This issue is especially pronounced in fine-tuning tasks on resource-constrained platforms (e.g. low-end GPUs or edge devices) \citep{10.1145/3666025.3699355}. Moreover, certain hardware platforms lack software support (e.g. automatic differentiation) for backpropagation \citep{bergholm2018pennylane}, further restricting FO methods. Although parameter-efficient fine-tuning methods have alleviated some of these challenges, they still require multiple times the memory of inference \citep{bai2024beyond,zhang2024revisiting}.

Zeroth-order (ZO) optimization provides a memory-efficient alternative by estimating gradients via forward passes only. Recent advances, such as MeZO \citep{malladi2023mezo}, have reduced memory usage to inference levels while achieving strong performance in LLM fine-tuning. However, the gradient variance in ZO methods is proportional to the number of perturbed parameters, which makes ZO methods struggle with high-dimensional parameter spaces, leading to slower convergence, increased gradient estimation variance, and hard to scale up \citep{chen2024enhancing}. Although recent work~\citep{liu2024sparse, yang2024adazeta,chen2023deepzero, liu2024sparse, yu2024subzero} has addressed some of these issues, most ZO methods focus on single-task learning, leaving their application to multi-task learning largely unexplored.


Multi-task learning is a key paradigm in LLMs to enable shared representations across diverse downstream tasks. This approach improves generalization, reduces the need for task-specific models, and improves performance in a wide range of applications \citep{zhang-etal-2023-survey, radford2019language}. Despite its advantages, multi-task learning also introduces inherent challenges, particularly when tasks exhibit conflicting objectives. These conflicts arise when the optimization signals from different tasks are misaligned, leading to competing gradients that prevent the model from learning effectively across all tasks~\citep{sener2018multi,Mahapatra2020MultiTaskLW,crawshaw2020multi,9382101, shi2023recon}.

The issue of conflicting gradients is further exacerbated in scenarios involving ZO optimization~\cite{liu2020primer,malladi2023mezo}. The high gradient variance in ZO methods can amplify inter-task conflicts and make it even more difficult to balance competing objectives~\citep{zhang2024convergence}. Furthermore, ZO methods suffer from collinearity in gradient estimates (see Section~\ref{sec:mtlfozo}), where aggregated gradient directions lack diversity, and higher rank in Hessian matrix (see Section~\ref{sec:challengesmtlzo}), where slower decay of eigenvalues in multi-task learning makes the convergence slow. A primary experiment demonstrated in Figure~\ref{fig:radar} shows that vanilla multi-task ZO optimization is only slightly better than zero-shot on average and is even worse on many tasks.





 % \zz{We should emphasize on the conflicts among different tasks rather than non-convexity. Single-task LLM fine-tuning is also a highly non-convex optimization problem. There is no evidence or theory to show that the proposed masking method can handle non-convexity better. }


To address these challenges, we propose Masked Zeroth-Order Optimization (MaZO), a novel framework designed for multi-task fine-tuning under ZO settings. MaZO tackles the problem at parameter level, which introduces two key innovations: (1) a weight importance metric that identifies critical parameters for each task, and (2) a multi-task weight update mask that selectively updates these parameters while freezing others. By focusing on the most important parameters, MaZO reduces the dimension of parameter space, mitigating the high variance of ZO fine-tuning while preserving the model capacity. Moreover, unlike traditional approaches dynamic weighting \citep{chen2018gradnorm,liu2024famo,AGHAJANZADEH2023109587}, which are trivial in ZO settings because of collinearity, MaZO balances multi-task learning conflicts from the perspective of weight. It activates distinct parameter subsets for different tasks based on their importance scores, allowing MaZO to allocate more capacity to tasks that require more updates.


\paragraph{Paper Contributions.} This paper makes the following novel contributions:
\begin{itemize}[leftmargin=*]
\vspace{-5pt}
    \item \textbf{First ZO-based multi-task fine-tuning framework}: We propose Masked Zeroth-Order Optimization (MaZO), the first framework specifically designed for multi-task LLM fine-tuning under ZO optimization. 
    % MaZO introduces a novel combination of weight importance metrics and a multi-task weight update mask, reducing the optimization dimensionality to lower variance while maintaining model capacity.
    \vspace{-5pt}
    \item \textbf{Task conflict resolution at the parameter level}: MaZO addresses inter-task conflicts by selectively activating critical parameters for each task. This parameter-level approach ensures balanced optimization across tasks under ZO settings.
    \vspace{-5pt}
    \item \textbf{State-of-the-art performance}: Comprehensive experiments on LLaMA-2-7B and Mistral-7B demonstrate that MaZO achieves state-of-the-art results in multi-task fine-tuning under ZO settings, outperforming multi-task learning methods designed for first-order (FO) optimization.
\end{itemize}





