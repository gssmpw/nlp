% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").


@article{duan2024efficient,
  title={Efficient training of large language models on distributed infrastructures: a survey},
  author={Duan, Jiangfei and Zhang, Shuo and Wang, Zerui and Jiang, Lijuan and Qu, Wenwen and Hu, Qinghao and Wang, Guoteng and Weng, Qizhen and Yan, Hang and Zhang, Xingcheng and others},
  journal={arXiv preprint arXiv:2407.20018},
  year={2024}
}

@article{zhao2023tensor,
  title={Tensor-compressed back-propagation-free training for (physics-informed) neural networks},
  author={Zhao, Yequan and Yu, Xinling and Chen, Zhixiong and Liu, Ziyue and Liu, Sijia and Zhang, Zheng},
  journal={arXiv preprint arXiv:2308.09858},
  year={2023}
}

@article{rostam2024achieving,
  title={Achieving Peak Performance for Large Language Models: A Systematic Review},
  author={Rostam, Zhyar Rzgar K and Sz{\'e}n{\'a}si, S{\'a}ndor and Kert{\'e}sz, G{\'a}bor},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@inproceedings{kundu2024performance,
  title={Performance modeling and workload analysis of distributed large language model training and inference},
  author={Kundu, Joyjit and Guo, Wenzhe and BanaGozar, Ali and De Alwis, Udari and Sengupta, Sourav and Gupta, Puneet and Mallik, Arindam},
  booktitle={2024 IEEE International Symposium on Workload Characterization (IISWC)},
  pages={57--67},
  year={2024},
  organization={IEEE}
}

@article{wolters2024memory,
  title={Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference},
  author={Wolters, Christopher and Yang, Xiaoxuan and Schlichtmann, Ulf and Suzumura, Toyotaro},
  journal={arXiv preprint arXiv:2406.08413},
  year={2024}
}

@article{bai2024beyond,
  title={Beyond efficiency: A systematic survey of resource-efficient large language models},
  author={Bai, Guangji and Chai, Zheng and Ling, Chen and Wang, Shiyu and Lu, Jiaying and Zhang, Nan and Shi, Tingwei and Yu, Ziyang and Zhu, Mengdan and Zhang, Yifei and others},
  journal={arXiv preprint arXiv:2401.00625},
  year={2024}
}

@article{wang2024simultaneous,
  title={Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models},
  author={Wang, Fei and Shen, Li and Ding, Liang and Xue, Chao and Liu, Ye and Ding, Changxing},
  journal={arXiv preprint arXiv:2410.09823},
  year={2024}
}

@article{malladi2023mezo,
   title={Fine-Tuning Large Language Models with Just Forward Passes},
   author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
   year={2023}
}

@article{chen2024enhancing,
  title={Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures},
  author={Chen, Yiming and Zhang, Yuan and Cao, Liyuan and Yuan, Kun and Wen, Zaiwen},
  journal={arXiv preprint arXiv:2410.07698},
  year={2024}
}

@article{liu2024sparse,
  title={Sparse mezo: Less parameters for better performance in zeroth-order llm fine-tuning},
  author={Liu, Yong and Zhu, Zirui and Gong, Chaoyu and Cheng, Minhao and Hsieh, Cho-Jui and You, Yang},
  journal={arXiv preprint arXiv:2402.15751},
  year={2024}
}

@article{yang2024adazeta,
  title={AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning},
  author={Yang, Yifan and Zhen, Kai and Banijamal, Ershad and Mouchtaris, Athanasios and Zhang, Zheng},
  journal={arXiv preprint arXiv:2406.18060},
  year={2024}
}

@inproceedings{zhan2024unlocking,
  title={Unlocking Black-Box Prompt Tuning Efficiency via Zeroth-Order Optimization},
  author={Zhan, Heshen and Chen, Congliang and Ding, Tian and Li, Ziniu and Sun, Ruoyu},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={14825--14838},
  year={2024}
}


@article{gautam2024variance,
  title={Variance-reduced zeroth-order methods for fine-tuning language models},
  author={Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok},
  journal={arXiv preprint arXiv:2404.08080},
  year={2024}
}

@article{zhang2024revisiting,
  title={Revisiting zeroth-order optimization for memory-efficient llm fine-tuning: A benchmark},
  author={Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D and Yin, Wotao and Hong, Mingyi and others},
  journal={arXiv preprint arXiv:2402.11592},
  year={2024}
}

@article{tang2024private,
  title={Private fine-tuning of large language models with zeroth-order optimization},
  author={Tang, Xinyu and Panda, Ashwinee and Nasr, Milad and Mahloujifar, Saeed and Mittal, Prateek},
  journal={arXiv preprint arXiv:2401.04343},
  year={2024}
}



@article{guo2024zeroth,
  title={Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity},
  author={Guo, Wentao and Long, Jikai and Zeng, Yimeng and Liu, Zirui and Yang, Xinyu and Ran, Yide and Gardner, Jacob R and Bastani, Osbert and De Sa, Christopher and Yu, Xiaodong and others},
  journal={arXiv preprint arXiv:2406.02913},
  year={2024}
}

@article{liu2024differentially,
  title={Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning},
  author={Liu, Zhihao and Lou, Jian and Bao, Wenjie and Qin, Zhan and Ren, Kui},
  journal={arXiv preprint arXiv:2402.07818},
  year={2024}
}

@article{zhao2024second,
  title={Second-order fine-tuning without pain for llms: A hessian informed zeroth-order optimizer},
  author={Zhao, Yanjun and Dang, Sizhe and Ye, Haishan and Dai, Guang and Qian, Yi and Tsang, Ivor W},
  journal={arXiv preprint arXiv:2402.15173},
  year={2024}
}

@article{hu2024revisiting,
  title={Revisiting scalarization in multi-task learning: A theoretical perspective},
  author={Hu, Yuzheng and Xian, Ruicheng and Wu, Qilong and Fan, Qiuling and Yin, Lang and Zhao, Han},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xin2022current,
  title={Do current multi-task optimization methods in deep learning even help?},
  author={Xin, Derrick and Ghorbani, Behrooz and Gilmer, Justin and Garg, Ankush and Firat, Orhan},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={13597--13609},
  year={2022}
}

@inproceedings{li2024scalable,
  title={Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity},
  author={Li, Dongyue and Sharma, Aneesh and Zhang, Hongyang R},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={1542--1553},
  year={2024}
}

@article{samant2022framework,
  title={Framework for deep learning-based language models using multi-task learning in natural language understanding: A systematic literature review and future directions},
  author={Samant, Rahul Manohar and Bachute, Mrinal R and Gite, Shilpa and Kotecha, Ketan},
  journal={IEEE Access},
  volume={10},
  pages={17078--17097},
  year={2022},
  publisher={IEEE}
}

@article{spall1992multivariate,
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  author={Spall, James C},
  journal={IEEE transactions on automatic control},
  volume={37},
  number={3},
  pages={332--341},
  year={1992},
  publisher={IEEE}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446/",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions."
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{gong2024coba,
  title={CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models},
  author={Gong, Zi and Yu, Hang and Liao, Cong and Liu, Bingchang and Chen, Chaoyu and Li, Jianguo},
  journal={arXiv preprint arXiv:2410.06741},
  year={2024}
}

@article{liu2024famo,
  title={Famo: Fast adaptive multitask optimization},
  author={Liu, Bo and Feng, Yihao and Stone, Peter and Liu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yang2024mtl,
  title={MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning},
  author={Yang, Yaming and Muhtar, Dilxat and Shen, Yelong and Zhan, Yuefeng and Liu, Jianfeng and Wang, Yujing and Sun, Hao and Deng, Denvy and Sun, Feng and Zhang, Qi and others},
  journal={arXiv preprint arXiv:2410.09437},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}


@article{das2023beyond,
  title={Beyond size: How gradients shape pruning decisions in large language models},
  author={Das, Rocktim Jyoti and Sun, Mingjie and Ma, Liqun and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2311.04902},
  year={2023}
}


@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{hu2024revisiting,
  title={Revisiting scalarization in multi-task learning: A theoretical perspective},
  author={Hu, Yuzheng and Xian, Ruicheng and Wu, Qilong and Fan, Qiuling and Yin, Lang and Zhao, Han},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{royer2024scalarization,
  title={Scalarization for multi-task and multi-domain learning at scale},
  author={Royer, Amelie and Blankevoort, Tijmen and Ehteshami Bejnordi, Babak},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{sener2018multi,
  title={Multi-task learning as multi-objective optimization},
  author={Sener, Ozan and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{claude35sonnet,
  author = {Anthropic},
  title = {Claude 3.5 Sonnet},
  year = {2024},
  note = {Available at \url{https://www.anthropic.com/claude/sonnet}}
}

@misc{gemini20,
  author = {Google DeepMind},
  title = {Gemini 2.0},
  year = {2024},
  note = {Available at \url{https://deepmind.google/technologies/gemini/}}
}



@misc{deepseekv3,
  author = {DeepSeek AI},
  title = {DeepSeek V3},
  year = {2024},
  note = {Available at \url{https://www.deepseek.ai/deepseek-v3}}
}

@misc{gpt4o,
  author = {OpenAI},
  title = {GPT-4o},
  year = {2024},
  note = {Available at \url{https://www.openai.com/gpt-4o}}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}


@inproceedings{10.1145/3666025.3699355,
author = {Zhuang, Yan and Zheng, Zhenzhe and Wu, Fan and Chen, Guihai},
title = {LiteMoE: Customizing On-device LLM Serving via Proxy Submodel Tuning},
year = {2024},
isbn = {9798400706974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666025.3699355},
doi = {10.1145/3666025.3699355},
abstract = {Considering limited on-device resources, current practices are attempting to deploy a system-level mixture-of-experts (MoE)-based foundation LLM shared by multiple mobile apps on a device to support mobile intelligence. However, mobile apps are hard to customize their services that require tuning adapters associated with the LLM using private in-app data. The difficulty arises due to both the limited on-device resources and the restricted control that apps have over the foundation LLM. To address this issue, in this work, we propose LiteMoE, a novel proxy submodel tuning framework that supports mobile apps to efficiently fine-tune customized adapters on devices using proxy submodels. The key technique behind LiteMoE is a post-training submodel extraction method, whereby without additional re-training, we can identify and reserve critical experts, match and merge moderate experts, to extract a lightweight and effective proxy submodel from the foundation LLM for a certain app. We implemented a prototype of LiteMoE and evaluated it over various MoE-based LLMs and mobile computing tasks. The results show that with LiteMoE, mobile apps are able to fine-tune customized adapters on resource-limited devices, achieving 12.7\% accuracy improvement and 6.6\texttimes{} memory reduction compared with operating the original foundation LLM.},
booktitle = {Proceedings of the 22nd ACM Conference on Embedded Networked Sensor Systems},
pages = {521–534},
numpages = {14},
keywords = {customized LLM serving, on-device LLM fine-tuning, mixture of experts},
location = {Hangzhou, China},
series = {SenSys '24}
}

@article{bergholm2018pennylane,
  title={Pennylane: Automatic differentiation of hybrid quantum-classical computations},
  author={Bergholm, Ville and Izaac, Josh and Schuld, Maria and Gogolin, Christian and Ahmed, Shahnawaz and Ajith, Vishnu and Alam, M Sohaib and Alonso-Linaje, Guillermo and AkashNarayanan, B and Asadi, Ali and others},
  journal={arXiv preprint arXiv:1811.04968},
  year={2018}
}

@article{chen2023deepzero,
  title={Deepzero: Scaling up zeroth-order optimization for deep model training},
  author={Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2310.02025},
  year={2023}
}

@article{yu2024subzero,
  title={SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning},
  author={Yu, Ziming and Zhou, Pan and Wang, Sike and Li, Jia and Huang, Hua},
  journal={arXiv preprint arXiv:2410.08989},
  year={2024}
}

@inproceedings{zhang-etal-2023-survey,
    title = "A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods",
    author = "Zhang, Zhihan  and
      Yu, Wenhao  and
      Yu, Mengxia  and
      Guo, Zhichun  and
      Jiang, Meng",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.66/",
    doi = "10.18653/v1/2023.eacl-main.66",
    pages = "943--956",
    abstract = "Multi-task learning (MTL) has become increasingly popular in natural language processing (NLP) because it improves the performance of related tasks by exploiting their commonalities and differences. Nevertheless, it is still not understood very well how multi-task learning can be implemented based on the relatedness of training tasks. In this survey, we review recent advances of multi-task learning methods in NLP, with the aim of summarizing them into two general multi-task training methods based on their task relatedness: (i) joint training and (ii) multi-step training. We present examples in various NLP downstream applications, summarize the task relationships and discuss future directions of this promising topic."
}


@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}


@ARTICLE{9382101,
  author={Zhou, Xiaojun and Gao, Yuan and Li, Chaojie and Huang, Zhaoke},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={A Multiple Gradient Descent Design for Multi-Task Learning on Edge Computing: Multi-Objective Machine Learning Approach}, 
  year={2022},
  volume={9},
  number={1},
  pages={121-133},
  keywords={Multitasking;Optimization;Machine learning algorithms;Learning systems;Edge computing;Deep learning;Surgery;License plate recognition;Deep neural network;edge computing;multi-objective machine learning;multi-task learning;multiple gradient descent},
  doi={10.1109/TNSE.2021.3067454}}


@inproceedings{Mahapatra2020MultiTaskLW,
  title={Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization},
  author={Debabrata Mahapatra and Vaibhav Rajan},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:221088926}
}

@article{shi2023recon,
  title={Recon: Reducing conflicting gradients from the root for multi-task learning},
  author={Shi, Guangyuan and Li, Qimai and Zhang, Wenlong and Chen, Jiaxin and Wu, Xiao-Ming},
  journal={arXiv preprint arXiv:2302.11289},
  year={2023}
}
@article{crawshaw2020multi,
  title={Multi-task learning with deep neural networks: A survey},
  author={Crawshaw, Michael},
  journal={arXiv preprint arXiv:2009.09796},
  year={2020}
}

@article{liu2020primer,
  title={A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications},
  author={Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={43--54},
  year={2020},
  publisher={IEEE}
}

@article{zhang2024convergence,
  title={On the Convergence of Multi-objective Optimization under Generalized Smoothness},
  author={Zhang, Qi and Xiao, Peiyao and Ji, Kaiyi and Zou, Shaofeng},
  journal={arXiv preprint arXiv:2405.19440},
  year={2024}
}

@article{10.1145/3663363,
author = {Chen, Shijie and Zhang, Yu and Yang, Qiang},
title = {Multi-Task Learning in Natural Language Processing: An Overview},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3663363},
doi = {10.1145/3663363},
abstract = {Deep learning approaches have achieved great success in the field of Natural Language Processing (NLP). However, directly training deep neural models often suffer from overfitting and data scarcity problems that are pervasive in NLP tasks. In recent years, Multi-Task Learning (MTL), which can leverage useful information of related tasks to achieve simultaneous performance improvement on these tasks, has been used to handle these problems. In this article, we give an overview of the use of MTL in NLP tasks. We first review MTL architectures used in NLP tasks and categorize them into four classes, including parallel architecture, hierarchical architecture, modular architecture, and generative adversarial architecture. Then we present optimization techniques on loss construction, gradient regularization, data sampling, and task scheduling to properly train a multi-task model. After presenting applications of MTL in a variety of NLP tasks, we introduce some benchmark datasets. Finally, we make a conclusion and discuss several possible research directions in this field.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {295},
numpages = {32},
keywords = {Multi-task learning}
}

@inproceedings{kongyoung-etal-2020-multi,
    title = "Multi-Task Learning using Dynamic Task Weighting for Conversational Question Answering",
    author = "Kongyoung, Sarawoot  and
      Macdonald, Craig  and
      Ounis, Iadh",
    editor = "Dalton, Jeff  and
      Chuklin, Aleksandr  and
      Kiseleva, Julia  and
      Burtsev, Mikhail",
    booktitle = "Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.scai-1.3/",
    doi = "10.18653/v1/2020.scai-1.3",
    pages = "17--26",
    abstract = "Conversational Question Answering (ConvQA) is a Conversational Search task in a simplified setting, where an answer must be extracted from a given passage. Neural language models, such as BERT, fine-tuned on large-scale ConvQA datasets such as CoQA and QuAC have been used to address this task. Recently, Multi-Task Learning (MTL) has emerged as a particularly interesting approach for developing ConvQA models, where the objective is to enhance the performance of a primary task by sharing the learned structure across several related auxiliary tasks. However, existing ConvQA models that leverage MTL have not investigated the dynamic adjustment of the relative importance of the different tasks during learning, nor the resulting impact on the performance of the learned models. In this paper, we first study the effectiveness and efficiency of dynamic MTL methods including Evolving Weighting, Uncertainty Weighting, and Loss-Balanced Task Weighting, compared to static MTL methods such as the uniform weighting of tasks. Furthermore, we propose a novel hybrid dynamic method combining Abridged Linear for the main task with a Loss-Balanced Task Weighting (LBTW) for the auxiliary tasks, so as to automatically fine-tune task weighting during learning, ensuring that each of the task`s weights is adjusted by the relative importance of the different tasks. We conduct experiments using QuAC, a large-scale ConvQA dataset. Our results demonstrate the effectiveness of our proposed method, which significantly outperforms both the single-task learning and static task weighting methods with improvements ranging from +2.72{\%} to +3.20{\%} in F1 scores. Finally, our findings show that the performance of using MTL in developing ConvQA model is sensitive to the correct selection of the auxiliary tasks as well as to an adequate balancing of the loss rates of these tasks during training by using LBTW."
}

@inproceedings{mao-etal-2022-metaweighting,
    title = "{M}eta{W}eighting: Learning to Weight Tasks in Multi-Task Learning",
    author = "Mao, Yuren  and
      Wang, Zekai  and
      Liu, Weiwei  and
      Lin, Xuemin  and
      Xie, Pengtao",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.271/",
    doi = "10.18653/v1/2022.findings-acl.271",
    pages = "3436--3448",
    abstract = "Task weighting, which assigns weights on the including tasks during training, significantly matters the performance of Multi-task Learning (MTL); thus, recently, there has been an explosive interest in it. However, existing task weighting methods assign weights only based on the training loss, while ignoring the gap between the training loss and generalization loss. It degenerates MTL`s performance. To address this issue, the present paper proposes a novel task weighting algorithm, which automatically weights the tasks via a learning-to-learn paradigm, referred to as MetaWeighting. Extensive experiments are conducted to validate the superiority of our proposed method in multi-task text classification."
}

@article{AGHAJANZADEH2023109587,
title = {Task weighting based on particle filter in deep multi-task learning with a view to uncertainty and performance},
journal = {Pattern Recognition},
volume = {140},
pages = {109587},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109587},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323002881},
author = {Emad Aghajanzadeh and Tahereh Bahraini and Amir Hossein Mehrizi and Hadi Sadoghi Yazdi},
keywords = {Multi task learning, Uncertainty, Hyper-parameter tuning, Deep learning, Particle filter, Bayesian estimation},
abstract = {Recently multi-task learning (MTL) has been widely used in different applications to build more robust models by sharing knowledge across several related tasks. However, one challenge that arises is the variability in the learning pace of different tasks causing the inefficiency of naively training all tasks. Therefore, it is of great importance to consider some coefficients to balance tasks in the process of learning, but, due to the large search space and the significance of setting them properly, conventional search methods such as grid or random search are no longer effective. In this paper, we propose a learning mechanism for these coefficients based on the high efficiency of the particle filter (PF) algorithm to deal with nonlinear search problems. PF considers each state of the tasks’ coefficients as a particle and recursively converges coefficients to an optimum point. While in most previous works coefficients were evaluated to only increase performance, to address the recent concerns related to applying AI in real-world applications, we also incorporate uncertainty alongside our method to prevent learning coefficients leading to unstable outcomes. This mechanism is independent of the models main learning process and can be easily added to every learning system without changing its training algorithm. Extensive experiments on real-world data sets demonstrate the superiority of the proposed method over the state-of-the-art methods on both performance and uncertainty. We also proved the acceptable performance of the method using Cramer Rao lower bound theory.}
}

@article{bai2024survey,
  title={A survey of multimodal large language model from a data-centric perspective},
  author={Bai, Tianyi and Liang, Hao and Wan, Binwang and Xu, Yanran and Li, Xi and Li, Shiyu and Yang, Ling and Li, Bozhou and Wang, Yifan and Cui, Bin and others},
  journal={arXiv preprint arXiv:2405.16640},
  year={2024}
}


@inproceedings{chen2018gradnorm,
  title={Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
  author={Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
  booktitle={International conference on machine learning},
  pages={794--803},
  year={2018},
  organization={PMLR}
}

@article{liu2019loss,
    title={Loss-Balanced Task Weighting to Reduce Negative Transfer in Multi-Task Learning},
    author={Liu, Shengchao and Liang, Yingyu and Gitter, Anthony},
    booktitle={Association for the Advancement of Artificial Intelligence (Student Abstract)},
    year={2019}
}

@article{ahmadian2024mix,
  title={Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning},
  author={Ahmadian, Arash and Goldfarb-Tarrant, Seraphina and Ermis, Beyza and Fadaee, Marzieh and Hooker, Sara and others},
  journal={arXiv preprint arXiv:2410.10801},
  year={2024}
}



@article{feng2024mixture,
  title={Mixture-of-loras: An efficient multitask tuning for large language models},
  author={Feng, Wenfeng and Hao, Chuzhan and Zhang, Yuewei and Han, Yu and Wang, Hao},
  journal={arXiv preprint arXiv:2403.03432},
  year={2024}
}

@article{liu2023moelora,
  title={Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications},
  author={Liu, Qidong and Wu, Xian and Zhao, Xiangyu and Zhu, Yuanshao and Xu, Derong and Tian, Feng and Zheng, Yefeng},
  journal={arXiv preprint arXiv:2310.18339},
  year={2023}
}


@article{wang2023multilora,
  title={Multilora: Democratizing lora for better multi-task learning},
  author={Wang, Yiming and Lin, Yu and Zeng, Xiaodong and Zhang, Guannan},
  journal={arXiv preprint arXiv:2311.11501},
  year={2023}
}

@article{gupta2022sparsely,
  title={Sparsely activated mixture-of-experts are robust multi-task learners},
  author={Gupta, Shashank and Mukherjee, Subhabrata and Subudhi, Krishan and Gonzalez, Eduardo and Jose, Damien and Awadallah, Ahmed H and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2204.07689},
  year={2022}
}




@article{desideri2012multiple,
  title={Multiple-gradient descent algorithm (MGDA) for multiobjective optimization},
  author={D{\'e}sid{\'e}ri, Jean-Antoine},
  journal={Comptes Rendus Mathematique},
  volume={350},
  number={5-6},
  pages={313--318},
  year={2012},
  publisher={Elsevier}
}

@article{liu2021conflict,
  title={Conflict-averse gradient descent for multi-task learning},
  author={Liu, Bo and Liu, Xingchao and Jin, Xiaojie and Stone, Peter and Liu, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18878--18890},
  year={2021}
}

@article{yu2020gradient,
  title={Gradient surgery for multi-task learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={arXiv preprint arXiv:2001.06782},
  year={2020}
}

@article{yang2023adamerging,
  title={Adamerging: Adaptive model merging for multi-task learning},
  author={Yang, Enneng and Wang, Zhenyi and Shen, Li and Liu, Shiwei and Guo, Guibing and Wang, Xingwei and Tao, Dacheng},
  journal={arXiv preprint arXiv:2310.02575},
  year={2023}
}

@inproceedings{jiang2024zo,
  title={ZO-AdaMU Optimizer: Adapting Perturbation by the Momentum and Uncertainty in Zeroth-Order Optimization},
  author={Jiang, Shuoran and Chen, Qingcai and Pan, Youcheng and Xiang, Yang and Lin, Yukang and Wu, Xiangping and Liu, Chuanyi and Song, Xiaobao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18363--18371},
  year={2024}
}

@article{chen2019zo,
  title={Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization},
  author={Chen, Xiangyi and Liu, Sijia and Xu, Kaidi and Li, Xingguo and Lin, Xue and Hong, Mingyi and Cox, David},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}



@article{hu2024localized,
  title={Localized zeroth-order prompt optimization},
  author={Hu, Wenyang and Shu, Yao and Yu, Zongmin and Wu, Zhaoxuan and Lin, Xiangqiang and Dai, Zhongxiang and Ng, See-Kiong and Low, Bryan Kian Hsiang},
  journal={arXiv preprint arXiv:2403.02993},
  year={2024}
}


@article{li2024addax,
  title={Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models},
  author={Li, Zeman and Zhang, Xinwei and Zhong, Peilin and Deng, Yuan and Razaviyayn, Meisam and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2410.06441},
  year={2024}
}


@article{kozak2023zeroth,
  title={Zeroth-order optimization with orthogonal random directions},
  author={Kozak, David and Molinari, Cesare and Rosasco, Lorenzo and Tenorio, Luis and Villa, Silvia},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={1179--1219},
  year={2023},
  publisher={Springer}
}