% \section{Discussion}
\vspace{-1pt}
\section{Conclusion}
\vspace{-1pt}
In this work, we have presented MaZO, a novel framework that harnesses masked zeroth-order optimization for the multi-task fine-tuning of LLMs. By incorporating weight importance score alongside a multi-task weight update mask, MaZO effectively reduces gradient variance and mitigates conflicts among tasks. Our experimental results demonstrate that MaZO not only surpasses current zeroth-order optimization methods but also outperforms leading multi-task learning methods designed for first-order optimization across a range of NLP tasks. Furthermore, our parameter-level approach is not limited solely to zeroth-order optimization, offering potential integrations with a variety of other optimization strategies.

\section{Limitations}
While MaZO demonstrates strong empirical performance, several limitations warrant discussion. First, the computation of weight importance introduces additional computational overhead compared to vanilla ZO methods. However, this cost remains negligible relative to the memory and computational demands of model weights and activations. Second, the effectiveness of MaZO is partially contingent on the quality of gradient and Hessian approximations. While our current approximations are effective, they could be further refined through more sophisticated estimation techniques to enhance performance. Third, our experiments are limited to medium-scale models (7B parameters) due to computational constraints. Although the method is theoretically applicable to larger models, the interplay between mask sparsity and model scale has not been systematically studied and represents an avenue for future research. Finally, we do not provide a theoretical convergence analysis for the masking approach. However, Sparse MeZO~\citep{liu2024sparse} has already conducted a comprehensive and rigorous analysis of general masking scenarios in zeroth-order optimization. We refer interested readers to their work for detailed theoretical insights, and therefore do not duplicate these efforts here.