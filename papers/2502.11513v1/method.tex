\section{The MaZO Framework}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{img/weight_importance.pdf} 
    \caption{Diagram of our MaZO method. The weight importance scoring and weight update mask is calculated row-wise. The weight importance for each task is calculated independently, and only from the input and weight. }
    \label{fig:mask}
    \vspace{-10pt}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\textwidth]{img/eigenvalues.pdf} 
    \vspace{-20pt} 
    \caption{Top-K eigenvalue distribution of the Hessian matrices in multi-task learning and single-task learning. These eigenvalue are normalized by dividing by the maximum value. The slower decay of eigenvalues in multi-task learning suggests a higher effective rank, which contributes to the slower convergence of ZO fine-tuning in multi-task scenarios.}
    \vspace{-20pt} 
    \label{fig:eigenvalue_distribution}
\end{figure}  


\subsection{Challenges in ZO Multi-Task Fine Tuning}
% \zz{Add a subsection to show the higher rank of the Hessian matrix in MTL. }
\label{sec:challengesmtlzo}

Under ZO optimization, multi-task learning faces unique challenges. Specifically, task-specific ZO gradient estimates exhibit fundamental collinearity, as the aggregated multi-task learning gradient aligns with the shared random perturbation $\mathbf{z}$:
\vspace{-6pt} 
\begin{align}
    \mat{g} &= \sum_{t=1}^T w_t \mat{g}^t \notag \\
    &= \left(\sum_{t=1}^T w_t \frac{\mathcal{L}^t (\theta+\epsilon \mat{z}) - \mathcal{L}^t(\theta-\epsilon \mat{z})}{2\epsilon}\right)  \mat{z}.
\end{align}
\vskip -8pt
\noindent Here $\mat{g}$ and $\mat{g}^t$ are gradients of multi-task learning and of task $t$, respectively.
This collinearity results in a lack of directional diversity, limiting optimization efficacy. Further discussion can be found in Appendix~\ref{app:collinear}.
% \zz{I'm not sure what "directional diversity" means in the mathematical sense, and why it can limit optimization efficiency.}

As explained in \citep{malladi2023mezo}, the surprising success of ZO optimization in LLM fine-tuning is due to the low-rank property of the Hessian matrix. Based on \eqref{eq:mtl_objective}, the Hessian matrix in multi-task fine-tuning can be written as
\begin{equation}
\mat{H}=\sum\limits_{t=1}^T w_t \mat{H}^t,
\end{equation}
where $\mat{H}^t$ is the Hessian associated with single-task learning loss ${\cal L}^t$. Although $\mat{H}^t$ has a low rank in the fine-tuning process, $\mat{H}$ can have a much higher rank due to the weighted sum of $T$ task-specific Hessian matrices. Figure~\ref{fig:eigenvalue_distribution} empirically verifies our theoretical claim: the Hessian in multi-task learning exhibits a broader eigenvalue spectrum than single-task learning, leading to a higher effective rank. This further slows down the convergence of ZO in multi-task LLM fine-tuning. 


To address the above challenges, we propose \textbf{Masked Zeroth-Order Optimization (MaZO)}. Our approach introduces a novel framework that solves multi-task learning at parameter level. MaZO combines weight importance metrics and a multi-task weight update mask. The weight importance is derived using two complementary metrics: (1) a global score which evaluates the theoretical minimum loss when freezing a parameter, (2) a greedy score which quantifies the immediate loss change during a single optimization step. Using these scores, we construct a weight update mask that identifies a subset of critical parameters, enabling effective optimization by reducing dimensionality and variance while balancing the performance among potentially conflicting tasks. 

\subsection{Multi-Task Weight Update Mask}

We first introduce the multi-task weight update mask, assuming the weight importance scores are precomputed. We defer the computation of weight importance scores to the next subsection. In ZO optimization, the variance of an estimated gradient increases with the number of training parameters. Therefore, it is crucial to identify and focus on critical parameters for effective optimization while freezing others \citep{liu2024sparse,guo2024zeroth}.

Suppose that we have a weight importance score matrix $\mathbf{S}^t$ for each task $t$ and a sparsity level $\rho$. We unfreeze the top $k = \lceil (1 - \rho) \cdot N \rceil$ parameters in each row, where $N$ is the total number of parameters in that row. The importance scores are compared row-wise due to the approximations involved in gradient and Hessian estimation following \citet{sun2023simple}, which will be detailed in Section~\ref{sec:gradapproximation}.

Since importance scores across tasks are not directly comparable due to differing scales, we normalize the scores row-wise for each task:
\vspace{-5pt} 
\begin{equation}
\label{eq:rownorm}
\hat{\mathbf{S}}^t_{ij} = \frac{\mathbf{S}^t_{ij} - \min(\mathbf{S}^t_{i})}{\max(\mathbf{S}^t_{i}) - \min(\mathbf{S}^t_{i}) },
\end{equation}
where $\mat{S}_i^t$ denotes the $i$-th row of $\mat{S}^t$; $\hat{\mathbf{S}}^t_{ij}$ is the normalized score for parameter $j$ in row $i$ for task $t$. The overall score across tasks is computed as:
\vspace{-5pt} 
\begin{equation}
\mathbf{S} = \sum_{t=1}^T{\hat{\mathbf{S}}^t}.
\end{equation}

We select the top $k$ parameters based on $\mathbf{S}$ in {\it each row} to fine-tune, while freezing the others. This selection is represented by a binary mask matrix $\mathbf{M}$, where $\mathbf{M}_{ij} = 1$ indicates that parameter $j$ in row $i$ is unfrozen. The final parameter update is computed as:
\vspace{-8pt} 
\begin{equation}
\mathbf{\Delta W}_{\text{masked}} = \mathbf{\Delta W} \odot \mathbf{M},
\end{equation}
where $\odot$ denotes element-wise multiplication. When applied to LoRA \citep{hu2021lora}, this becomes:
\vspace{-8pt} 
\begin{equation}
\mathbf{\Delta W}_{\text{masked}} =  (\mat{A} \cdot \mat{B}) \odot \mathbf{M},
\end{equation}
where $\mat{A}$ and $\mat{B}$ are the decomposed matrices of LoRA. 

% \zz{Better to derive and use the masks for $\mat{A}$ and $\mat{B}$ respectively. It will be memory consuming if we have to reconstruct $\mathbf{\Delta W}_{\text{masked}}$ or $\mathbf{\Delta W}$ in LoRA.}


\subsection{Weight Importance}

The overall importance score for task $t$ combines the normalized global and greedy scores with a weight regularization term:
\vspace{-4pt} 
\begin{equation}
\mathbf{S}^t = \mathbf{S}^t_\text{global} +  \alpha \mathbf{S}^t_\text{greedy} + \beta |\mathbf{W}|,
\end{equation}
where $\alpha$ and $\beta$ are hyperparameters controlling the contributions of each component and $|\mathbf{W}|$ is the absolute value of weight. We now describe the computation of two complementary metrics: the \textbf{global score} and the \textbf{greedy score}.

% \zz{The weight importance scores are computed based on a scalar loss ${\cal L}$, but in multi-task learning the loss should be a vector with multiple elements. Are you talking about how to compute $\mat{S}^t$?}


\subsubsection{Global Score}

The global score is inspired by the Optimal Brain Surgeon method  \citep{frantar2023sparsegpt,sun2023simple,das2023beyond}. Unlike pruning, which sets the parameters to zero, our approach freezes certain parameters while updating others via perturbation.
Consider the Taylor expansion of the loss function of task $t$:
\begin{equation}
    \delta \mathcal{L}^t = (\mathbf{g}^t)^\top \cdot \delta \theta + \frac{1}{2} \delta \theta^\top \cdot \mathbf{H}^t \cdot \delta \theta + \mathcal{O}(\|\delta \mathbf{\theta}\|^3), \nonumber
\end{equation}
where $\mathbf{H}^t$ is the Hessian matrix of task $t$ and $\mathbf{g}^t = \frac{\partial \mathcal{L}^t_{\text{STL}}}{\partial \theta}$ . Freezing a parameter at position $m$ imposes the constraint $\mathbf{I}_m^\top \delta \theta = 0$, where $\mathbf{I}_m$ is an indicator function. The optimization problem becomes:
\begin{equation}
\begin{adjustbox}{max width=\columnwidth}
$\begin{aligned}
\min_{m} \bigg\{ 
    &\min_{\delta \theta} \bigg( 
        \left( \mathbf{g}^t \right)^\top \cdot \delta \theta 
        + \frac{1}{2} \delta \theta^\top \cdot \mathbf{H}^t \cdot \delta \theta
    \bigg) \\
    &\bigg| \mathbf{I}_m^\top \cdot \delta \mathbf{w} = 0 
\bigg\}.
\end{aligned}$
\end{adjustbox}
\end{equation}
This formulation seeks to find the parameter position $m$ that, when frozen, results in the maximal decrease in the loss function while allowing other parameters to adjust optimally. The inner optimization determines the best possible parameter updates given the constraint, while the outer optimization identifies the least impactful parameter to fix.

Using Lagrange multipliers, the optimal loss change (global score) is derived as:
\begin{align}
\label{eq:globalscore}
    (\mathbf{S}^t_\text{global})_m &=
    \delta \mathcal{L}^t_m = \frac{\left( \mathbf{I}_m^\top \cdot \left(\mathbf{H}^{t}\right)^{-1} \cdot \mathbf{g}^t \right)^2}{2 \left( \left(\mathbf{H}^t \right)^{-1} \right)_{mm}},
\end{align}
% \zz{I didn't see the dependence on $t$ on the right-hand side of the above equation.}
This expression quantifies the theoretical maximum decrease in loss when parameter $m$ is fixed, providing a measure of its importance to the overall optimization process. Smaller values indicate less important parameters, which should be frozen.

\subsubsection{Greedy Score}

Although the global score provides a theoretical measure of parameter importance, it may not suffice because the model may not converge to the optimal situation due to the large variance in the ZO gradient. Therefore, we also introduce a greedy score as a practical complement, which considers the immediate impact of freezing a parameter in a {\it single optimization step}. %This approach provides a practical complement to the global score.

For a gradient descent update with learning rate $\eta$ and random direction $\mathbf{z}$, the parameter update of task $t$ is approximated as:

\begin{equation}
\mathbf{\delta \theta} \approx - \eta \mathbf{z} \mathbf{z}^T \mathbf{g}^t.
\end{equation}
% The change in loss can be approximated using a Taylor expansion:
% \begin{align}
% \delta \mathcal{L}^t_{STL}
% &= (\mathbf{g}^t)^T \mathbf{u} + \frac{1}{2} \mathbf{u}^T \mathbf{H} \mathbf{u} + o(|\mathbf{u}|^3) \notag \\
% &= - (\mathbf{g}^t)^T \mathbf{z} \mathbf{z}^T (\mathbf{g}^t) \eta \notag \\
% &\quad + \frac{1}{2} (\mathbf{g}^t)^T \mathbf{z} \mathbf{z}^T (\mathbf{H}^t) \mathbf{z} \mathbf{z}^T (\mathbf{g}^t) \eta^2 + o(\eta^3).
% \end{align}
Substituting $\delta \theta$ and taking the expectation over random directions $z$, we obtain the expected change in loss:
\begin{align}
\mathbb{E}(\delta \mathcal{L}^t )
&= - (\mathbf{g}^t)^T \mathbf{g}^t \cdot \eta \notag \\
&+ \left( \sum_{i=0}^M (\mathbf{g}^t_i)^2 \mathbf{H}^t_{ii} + 2 (\mathbf{g}_i^t)^T \mathbf{H}^t \mathbf{g}^t \right) \eta^2 \nonumber
\end{align}
where $M$ is the number of parameters in a LLM.

When we freeze a parameter at position $m$, the change of loss (greedy score) will increase by:
\begin{align}
\label{eq:greedyscore}
(\mathbf{S}^t_\text{greedy})_m &= \delta \mathcal{L}^t_m \notag \\
&= (\mathbf{g}^t_m)^2 \eta + \mathbf{H}^t_{mm} (\mathbf{g}^t_m)^2 \eta^2 \notag \\
&\quad - 4 \sum_{j=0}^M \mathbf{H}^t_{mj} (\mathbf{g}^t_m) (\mathbf{g}^t_j) \eta^2
\end{align}
% \zz{I didn't see the dependence on $t$ on the right-hand side of the above equation.} 
Parameters with lower $\mathbf{S}^t_\text{greedy}$ values are considered less important for the current optimization step and are better candidates for freezing during multi-task learning.

% \subsection{Implementation.} 
% \label{sec:gradapproximation}

% \paragraph{Approximation of Gradient and Hessian.} In the ZO setting, directly computing the full gradient and Hessian is computationally expensive. To alleviate this, we adopt a row-wise approximation strategy following \citet{frantar2023sparsegpt}. Specifically, when considering a single row of the weight matrix, the corresponding output is a scalar, and thus the first- and second-order derivatives of the loss with respect to that output are also scalars. This observation implies that the Taylor expansion of the loss for that row leads to a gradient proportional to the input, and a Hessian proportional to the outer product of the input with itself. By dropping constant scalar factors, we approximate the gradient as $\mathbf{g}\propto \mathbf{x}$ and the Hessian as $\mathbf{H}\propto \mathbf{x}^\top\mathbf{x}$. Such an approximation not only simplifies the computation but also restricts the weight importance comparison to the row direction. Detailed derivations are provided in Appendix~\ref{app:approxiation}. \zz{Need to write this paragraph more rigorously with a few key equations in the text. }


\subsection{Implementation}  
\label{sec:gradapproximation}


To avoid the huge cost of computing the full gradient and Hessian, we adopt a row-wise approximation strategy. For a linear layer $\mathbf{y} = \mathbf{W} \mathbf{x}$, focusing on a single row \(\mathbf{w}_i\), the output is $\mathbf{y}_i = \mathbf{w}_i \mathbf{x}\). Performing a Taylor expansion of the loss \(\mathcal{L}\) with respect to \(y_i\), we find that both the first-order gradient \(\nabla \mathcal{L}(\mathbf{y}_i)\) and second-order derivative \(\nabla^2 \mathcal{L}(\mathbf{y}_i)\) are scalars. Substituting \(\Delta \mathbf{y}_i = \Delta \mathbf{w}_i \mathbf{x}\), the gradient and Hessian with respect to \(\mathbf{w}_i\) are:
\begin{align}
\mathbf{g}^t &= \frac{\partial \mathcal{L}}{\partial \mathbf{w}_i} = \nabla \mathcal{L}(\mathbf{y}_i) \mathbf{x}, \label{eq:gradient_approx} \\
\mathbf{H}^t &= \frac{\partial^2 \mathcal{L}}{\partial \mathbf{w}_i^2} = \nabla^2 \mathcal{L}(\mathbf{y}_i) (\mathbf{x} \mathbf{x}^\top). \label{eq:hessian_approx}
\end{align}
\vskip -5pt
\noindent Here, we replace the gradient with \(\mathbf{x}\), and the Hessian with \(\mathbf{x} \mathbf{x}^\top\) since we only care about the relative value {\it in a row}. This row-wise approximation significantly reduces computational cost, while still capturing the relative importance of parameters within each row. However, it also restricts the weight-importance comparison to the row direction.


\paragraph{Overall Algorithm Flow.} The pseudo-code of the whole MaZO fine-tuning framework is shown as Algorithm~\ref{alg:mazopseudocode} in Appendix~\ref{app:pseudocode}.








