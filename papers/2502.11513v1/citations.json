[
  {
    "index": 0,
    "papers": [
      {
        "key": "tang2024private",
        "author": "Tang, Xinyu and Panda, Ashwinee and Nasr, Milad and Mahloujifar, Saeed and Mittal, Prateek",
        "title": "Private fine-tuning of large language models with zeroth-order optimization"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yang2024adazeta",
        "author": "Yang, Yifan and Zhen, Kai and Banijamal, Ershad and Mouchtaris, Athanasios and Zhang, Zheng",
        "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gautam2024variance",
        "author": "Gautam, Tanmay and Park, Youngsuk and Zhou, Hao and Raman, Parameswaran and Ha, Wooseok",
        "title": "Variance-reduced zeroth-order methods for fine-tuning language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "guo2024zeroth",
        "author": "Guo, Wentao and Long, Jikai and Zeng, Yimeng and Liu, Zirui and Yang, Xinyu and Ran, Yide and Gardner, Jacob R and Bastani, Osbert and De Sa, Christopher and Yu, Xiaodong and others",
        "title": "Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "yang2024adazeta",
        "author": "Yang, Yifan and Zhen, Kai and Banijamal, Ershad and Mouchtaris, Athanasios and Zhang, Zheng",
        "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2024differentially",
        "author": "Liu, Zhihao and Lou, Jian and Bao, Wenjie and Qin, Zhan and Ren, Kui",
        "title": "Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhao2024second",
        "author": "Zhao, Yanjun and Dang, Sizhe and Ye, Haishan and Dai, Guang and Qian, Yi and Tsang, Ivor W",
        "title": "Second-order fine-tuning without pain for llms: A hessian informed zeroth-order optimizer"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hu2024revisiting",
        "author": "Hu, Yuzheng and Xian, Ruicheng and Wu, Qilong and Fan, Qiuling and Yin, Lang and Zhao, Han",
        "title": "Revisiting scalarization in multi-task learning: A theoretical perspective"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xin2022current",
        "author": "Xin, Derrick and Ghorbani, Behrooz and Gilmer, Justin and Garg, Ankush and Firat, Orhan",
        "title": "Do current multi-task optimization methods in deep learning even help?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "li2024scalable",
        "author": "Li, Dongyue and Sharma, Aneesh and Zhang, Hongyang R",
        "title": "Scalable Multitask Learning Using Gradient-based Estimation of Task Affinity"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "samant2022framework",
        "author": "Samant, Rahul Manohar and Bachute, Mrinal R and Gite, Shilpa and Kotecha, Ketan",
        "title": "Framework for deep learning-based language models using multi-task learning in natural language understanding: A systematic literature review and future directions"
      }
    ]
  }
]