\section{Background and Related works}
\label{sec:background}

\subsection{Diffusion Models}
\label{ssec:DM}
Diffusion models operate by gradually introducing Gaussian noise to real data $x_0$ in a forward process and then learning a reverse process to denoise and generate high-quality images. For denoising diffusion probabilistic models (DDPMs) **Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"**, the forward process is defined as a Markov chain, represented by the following equation:

\begin{equation}
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, \beta_t I),
\end{equation}
where $\alpha_t$ and $\beta_t$ are hyperparameters, with $\beta_t = 1 - \alpha_t$. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figure/Fig3.pdf}
    \caption{Maximum channel magnitudes after softmax are depicted for various timesteps during inference, revealing large variance across timesteps. This shows the necessity of handling timestep-dependent values effectively.
    } \label{fig3}
\end{figure}

In the reverse process, since directly modeling the true distribution $q(x_{t-1}|x_t)$ is infeasible, diffusion models employ variational inference to approximate it as a Gaussian distribution:

\begin{equation}
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)).
\end{equation}
The mean of the Gaussian can further be reparameterized using a noise prediction network $\epsilon_\theta(x_t, t)$ as follows.

\begin{equation}
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right),
\end{equation}
where $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$. The variance $\Sigma_\theta(x_t, t)$ can either be reparameterized or set to a fixed schedule $\sigma_t$. Under the fixed variance schedule, the distribution of $x_{t-1}$ is given by

\begin{equation}
x_{t-1} \sim \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I).
\end{equation}

In diffusion models, the noise at each time step $t$ is predicted from $x_t$ using a noise estimation model, which typically shares the same weights across all time steps.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figure/Fig4_2.pdf}
    \caption{Illustration of the diffusion transformer (DiT) **Ho et al., "DenoiSer: Diffusion-based Image Synthesis and Editing with Discrete Noise"** with stacked transformer-based DiT blocks. Each block includes MHSA layers with softmax and PF layers with GELU activations, conditioned on class and timestep inputs.
    } \label{fig4}
\end{figure}

\subsection{Diffusion Transformers}
\label{ssec:DiT}
Despite considerable impact of the U-Net architecture on image generation models **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, recent studies have shifted toward transformer-based approaches **Vaswani et al., "Attention Is All You Need"**. Diffusion transformers (DiTs) **Song et al., "Denoising Diffusion Implicit Processes"** show state-of-the-art performance in image generation, while they can scale effectively in terms of data representation and model size.

DiTs are structured with $N$ transformer-based blocks that form the backbone of the denoising process, as depicted in Fig. \ref{fig4}. Each block includes two fundamental components: multi-head self-attention (MHSA) and pointwise feedforward (PF) layers. Both components are conditioned on class information and timestep inputs, ensuring the model effectively captures time-dependent features throughout the denoising process. 

MHSA mechanism primarily relies on linear projections and matrix multiplications (MatMul) of the query, key, and value matrices, allowing the model to capture contextual relationships among image patches. Each DiT block employs a softmax layer in the MHSA to normalize attention scores and effectively capture relative importance among tokens. This normalization is critical for the self-attention mechanism to function properly. For PF layers, two sequential linear transformations are applied, separated by a Gaussian Error Linear Unit (GELU) activation layer.

Although DiTs have shown remarkable efficiency in generating high-fidelity images, their significant computational demands present challenges for practical applications. To address this limitation, we propose a quantization framework designed for DiTs, substantially reducing memory usage and inference time. Notably, our approach achieves this efficiency without requiring re-training of the original model, making it a practical and scalable solution for deploying DiTs in resource-constrained environments.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figure/Fig6_2.pdf}
    \caption{Illustration of the proposed TQ-DiT. Multi-Region Quantization (MRQ) handles skewed distributions in post-softmax and post-GELU layers within MHSA and PF. Hessian-guided Optimization (HO) with Time-Grouping Quantization(TGQ) addresses timestep-dependent activation variability in post-softmax layers.
    } \label{fig6}
\end{figure*}

\subsection{Model Quantization}
\label{ssec:DiT}

Quantization is employed for model compression to enhance the inference efficiency of deep learning models by converting full-precision tensors into $k$-bit integer representations **Gupta et al., "Deep Learning with Limited Numerical Precision"**. This conversion leads to significant improvements in computational efficiency and reductions in memory usage **Courbariaux et al., "Training Deep Neural Networks with Low Precision Multiplication"**.

For uniform quantization, the process can be mathematically expressed as

\begin{equation}
\hat{\mathbf{x}} = s \cdot \text{clip} \left( \lfloor\frac{\mathbf{x}}{s} \rceil + z, 0, 2^k-1 \right) - z,
\end{equation}
where $\lfloor \cdot \rceil$ denotes the rounding operation, $s = \frac{\max(\mathbf{x}) - \min(\mathbf{x})}{2^k-1}$ is the step size, and $z = -\lfloor \frac{\min(\mathbf{x})}{s} \rceil$ is the zero-point. Here, $k$ represents the bit-width of the quantization. This formula essentially maps floating-point values to a predefined set of fixed points (or grids).

For $k$-bit uniform asymmetric quantization, the set of quantization grids can be expressed as

\begin{equation}
\mathcal{Q}_k^\text{u} = s \times \{0, \dots, 2^k-1\} - z.
\end{equation}

The quantization function, denoted as $Q_k(\cdot \,;\Delta):\mathbb{R} \to \mathcal{Q}_k^\text{u}$, is often optimized to minimize the quantization error, defined by the deviation between the original and quantized grids. The optimization is formulated as

\begin{equation}
\min_{s, z} ||\hat{\mathbf{w}} - \mathbf{w||}_F^2 \quad \text{s.t.} \quad \hat{\mathbf{w}} \in \mathcal{Q}_k^\text{u},
\end{equation}
where $\mathbf{w}$ is the original parameter, and $\hat{\mathbf{w}}$ is the quantized parameter.

A recent study extended PTQ to DiTs, introducing a technique that redistributes activations and weights based on their salience to mitigate quantization errors caused by outlier magnitudes **Chen et al., "Learning Efficient Quantization for Deep Neural Networks"**. However, this approach is limited by its reliance on salience-based redistribution, which requires extensive calibration time and a large-scale calibration dataset, imposing significant computational and resource burdens. Such inefficiencies are particularly problematic in real-world applications with limited computational resources, such as edge servers in distributed systems **Farabet et al., "Large-Scale Distributed Neural Network Training through Adversarial Parameterization"**, where efficient quantization strategies are critical for deployment. In comparison, our work proposes an alternative PTQ strategy that directly targets quantization errors across DiT, significantly reducing calibration overhead while maintaining generation quality. By addressing these inefficiencies, our study introduces a streamlined approach for effectively quantizing DiTs, enabling their deployment in resource-constrained environments without compromising performance in high-quality image generation tasks.