
% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{indentfirst}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}
\usepackage{xcolor}  
\usepackage{array} 
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{threeparttable}
\usepackage{tablefootnote}


\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0} 
\newcommand{\upgreen}{\textcolor{darkgreen}{$\uparrow$}}
\newcommand{\downgreen}{\textcolor{darkgreen}{$\downarrow$}} 
\newcommand{\upred}{\textcolor{red}{$\uparrow$}}  
\newcommand{\downred}{\textcolor{red}{$\downarrow$}} 
% \newcommand{\TPO}{\texttt{ThinkPO}}
\newcommand{\TPO}{{ThinkPO}\xspace}

\title{Thinking Preference Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Wang Yang, Hongye Jin, Jingfeng Yang, Vipin Chaudhary, Xiaotian Han \\
\{wxy320, vxc204, xhan\}@case.edu; jhy0410@tamu.edu; jingfengyangpku@gmail.com}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\begin{document}

\maketitle


\begin{abstract}

Supervised Fine-Tuning (SFT) has been a go-to and effective method for enhancing long chain-of-thought (CoT) reasoning in relatively small LLMs by fine-tuning them with long CoT responses from larger LLMs~\footnote{Deepseek official distilled models  \href{https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d}{DeepSeek-R1-Distill}, \href{https://huggingface.co/open-thoughts/OpenThinker-7B}{OpenThinker-7B}, \href{https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview}{Sky-T1-32B}, and \href{https://huggingface.co/bespokelabs/Bespoke-Stratos-7B}{Bespoke-Stratos-7B} was trained in this way. }. To continually improve reasoning abilities, we can either collect new high-quality long CoT reasoning SFT data or repeatedly train on existing SFT datasets. However, acquiring new long CoT SFT data is costly and limited, while repeated training often results in a performance plateau or decline. To further boost the performance with the SFT data, we propose Thinking Preference Optimization (\TPO), a simple yet effective post-SFT method that enhances long CoT reasoning without requiring new long CoT responses.  Instead, \TPO~utilizes readily available or easily obtainable short CoT reasoning responses as rejected answers and long CoT responses as chosen answers for the same question. It then applies direct preference optimization to encourage the model to favor longer reasoning outputs. Experiments show that \TPO~further improves the reasoning performance of SFT-ed models, e.g. it increases math reasoning accuracy of SFT-ed models by $8.6\%$ and output length by $25.9\%$. Notably, \TPO~is capable of continually boosting the performance of the publicly distilled SFT model, e.g., increasing the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from $87.4\%$ to $91.2\%$. Our code is available at \url{https://github.com/uservan/ThinkPO}.
%  ~\footnote{We will release the datasets, model, and training recipe on \url{https://huggingface.co/}.}.
 
\end{abstract}

\begin{figure}[t]
\includegraphics[width=\linewidth]{imgs/Train_pipeline.pdf}\vspace{8pt}
\hfill 
\includegraphics[height=0.980in]{imgs/open_source_results.pdf}
\hfill
\includegraphics[height=0.990in]{imgs/open_source_lengths.pdf}\vspace{-5pt}
  \caption{The illustration of our method ThinkPO and its performance on math reasoning tasks.
\textbf{Top:} Our ThinkPO enhances fine-tuned LLMs (+SFT) by promoting detailed problem-solving---using long chain-of-thought reasoning answers as positive (chosen) samples and short chain-of-thought reasoning answers as negative (rejected) samples.
\textbf{Bottom Left:} ThinkPO significantly boosts performance across mathematical benchmarks (e.g., 83.4\% on MATH500 vs. 82.8\% for +SFT and 74.0\% for the Base model).
\textbf{Bottom Right:} ThinkPO generates more detailed solutions, with average completion lengths on AIME increasing from 0.94K to 21.57K to 23.9K tokens.
These results underscore Think Preference Optimization's effectiveness in fostering and enhancing advanced mathematical reasoning.
  }
  \label{fig:open_source_lengths}
  \label{fig:Training pipeline}\vspace{-10pt}
\end{figure}


\section{Introduction}

\begin{figure*}[t]
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_acc_MATH500_plot.pdf}
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_length_MATH500_plot.pdf}
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_words_MATH500_plot.pdf}
\vspace{-20pt}
  \caption{
   Analysis of accuracy(\textbf{Left}), average response length(\textbf{Middle}) and reasoning-supportive words count(\textbf{Right}, like wait, hmm, etc) in SFT and ThinkPO process. We evaluate the model on MATH500 every 300 steps and record all the three metrics. In the early training stages, all of them improve significantly. However, in the later stages (e.g., after 1200 steps), the model’s performance gradually plateau. When ThinkPO is applied, we see additional improvements in all of the three aspects, demonstrating the effectiveness of Thinking Preference Optimization.
  }
  \label{fig:sft_length_plot}
\end{figure*}

 The reasoning capability of LLMs is crucial for their applicability in complex problem-solving tasks. Improving the reasoning ability of large language models is one of the current research hotspots. Many approaches have emerged in the open-source community that enhance relatively small models' reasoning ability through \textbf{SFT}. For example, Sky-Thought \cite{schulman2017proximal}, Bespoke-Stratos \cite{bespoke_stratos} and OpenThinker-7B\cite{openthoughts} have built long reasoning datasets to fine-tune models fully, aiming to improve model reasoning capabilities. Further advancements can be seen in models like s1 \cite{muennighoff2025s1simpletesttimescaling} and LIMO \cite{ye2025limoreasoning}, which focus on the sophisticated design of long reasoning datasets to enhancereasoning capabilities.

Despite the success of supervised fine-tuning, continually improving the reasoning abilities of the STF-ed model faces the following challenges:
(1) \textbf{high resources cost needed to collect new long reasoning response}: Training a stronger reasoning model first requires collecting new large-scale, diverse, and meticulously designed long-reasoning questions. Then, responses to these long reasoning problems need to be collected from large-scale models, such as DeepSeek-R1. However, collecting questions and responses requires significant computational power and human resources, making the process both expensive and labor-intensive. Furthermore,
(2) \textbf{repeatedly fine-tuning LLMs on existing long responses face Performance bottleneck}: As a compromise, one might repeatedly train on a limited long reasoning dataset, but this approach typically leads to a performance plateau or even decline. 
In \cref{fig:sft_length_plot}, we observe that when training with a fixed amount of long-reasoning data for multiple epochs, model’s average output length and accuracy increase significantly in the early stages but slow down or even plateau in later stages. According to the test-time scaling principle \cite{snell2024scaling, welleck2024decoding}, increasing the compute at test time generally enhances reasoning ability. However, the limited long-reasoning dataset is insufficient to further improve LLMs' reasoning capability in later stages of SFT.

To overcome the performance bottleneck and better utilize existing long reasoning data, we propose \textbf{Thinking Preference Optimization}: a simple yet efficient method to further enhance model reasoning ability after supervised fine-tuning (SFT). Our approach utilizes short CoT reasoning responses---which are already available or easy to acquire---as rejected answers and \textit{existing} long CoT responses as chosen answers for the same question, and employs Direct Preference Optimization to train models. This encourages models to prefer longer and more structured reasoning processes, thereby improving reasoning abilities {\textit{without acquiring additional high-quality long CoT responses}}.

\cref{fig:open_source_lengths} presents the framework of \TPO along with the experimental results. We first fine-tune a Qwen base model using the long CoT data to obtain an SFT-ed model (+SFT), and then we further train it using \TPO~ (+\TPO). The results in \cref{fig:open_source_lengths} clearly show that our method improves mathematical reasoning ability across four datasets. Additionally, our method increases the average response length on all four datasets, aligning with the test-time scaling trend. For example, ThinkPO increases the math reasoning accuracy of SFT-ed models by $8.6\%$ and the output length by $25.9\%$. Notably, \TPO~ increases the official DeepSeek-R1-Distill-Qwen-7B's performance on MATH500 from $87.4\%$ to $91.2\%$. The main contributions are summarized as follows:

\begin{itemize}[leftmargin=0.4cm, itemindent=.0cm, itemsep=0.0cm, topsep=0.1cm]
\item We propose Thinking Preference Optimization (\TPO) to maximize the value of existing long reasoning data, which successfully further enhances SFT-ed LLMs' reasoning performance without additional long CoT responses. 
\item Our method continuously improves the performance of public R1-distilled models, including the DeepSeek-R1 official distilled models. 
\item We release our dataset, codes, and model weights to facilitate further research. 
\end{itemize}
\section{Thinking Preference Optimization}

\subsection{Motivations}


This section introduces the motivations behind Thinking Prference Optimization. SFT with fixed long-reasoning datasets is an effective method for enhancing a model’s reasoning ability. However, further improvement of the model’s reasoning ability during the later stages faces a bottleneck. In such cases, by using short reasoning data as rejected samples and long reasoning texts from SFT as chosen samples for DPO training, it is possible to further leverage the high-quality SFT reasoning data to boost the model’s reasoning performance with minimal additional data resources.

First, we finetune Qwen-2.5-7B-Instruct model using Bespoke-Strato-dataset\cite{bespoke_stratos}, which includes $17k$ long reasoning data distilled from Deepseek-R1. During training, we track the model’s average output length, accuracy and reasoning-supportive words count (like wait, hmm) at different steps on the Math500 dataset. These are visualized by fitting curves. When calculating the model’s average output length, we only considered valid sentences, excluding duplicates or sentences with formatting errors. The results on other datasets could be found in \cref{Analysis of our Reproduce Model in other datasets}.

In \cref{fig:sft_length_plot}, 
in the early stages of SFT, the model’s average output length, accuracy and reasoning-supportive words count show significant improvements. This aligns with the test-time scaling phenomenon \cite{snell2024scaling, welleck2024decoding}, where a model’s reasoning ability generally improves as its output length increases. Many approaches enhance reasoning ability by fine-tuning models to generate longer responses. However, in the later stages of SFT, average response length, accuracy and reasoning-supportive words count plateau, indicating a performance bottleneck.

To further enhance the model’s reasoning ability, we can apply DPO, which encourages the model to favor longer outputs. By treating long-reasoning responses as chosen samples and short-reasoning responses as rejected samples, this approach improves the model’s reasoning ability without significantly increasing long-reasoning dataset size, thereby boosting its reasoning performance.


\subsection{Training Pipeline}

The training process in Thinking Preference Optimization consists of two stages: Reasoning SFT (Supervised Fine-Tuning) stage and Reasoning DPO (Direct Preference Optimization) stage.


In the Reasoning SFT stage, long-reasoning responses are collected for each question to construct the dataset $\mathcal{D}_{sft}$. The base model is then fine-tuned on $\mathcal{D}_{sft}$ to acquire advanced reasoning capabilities, which helps to prepare the model for next stage.

In the second stage, the model is further encouraged to generate extended reasoning using the Direct Preference Optimization (DPO) \cite{rafailov2024direct} approach. First, the long-reasoning data from the initial stage is used as the chosen responses. Then, a smaller model with normal Reasoning ability, such as Qwen-2.5-7B-Math \cite{qwen2.5}, is utilized to generate shorter reasoning responses as rejected samples. To ensure data quality, both long and short reasoning responses undergo filtering, including correctness validation. This process results in the dataset $\mathcal{D}_{dpo}$. Finally, the model trained in the first stage is fine-tuned on $\mathcal{D}_{dpo}$ using DPO, encouraging the model to generate longer outputs while enhancing its reasoning ability. Training pipeline is visualized as \cref{fig:Training pipeline}.

\begin{figure}[t]
\includegraphics[width=\linewidth]{imgs/datacuration.pdf}\vspace{-10pt}
  \caption{Data Collection Process: we use Deepseek R1 to generate long reasoning answers as chosen samples and Qwen 2.5-7B-Math to generate short reasoning answers as rejected samples, collecting datasets for DPO Training. Compare with short reasoning data, long reasoning answers includes many reasoning-supportive discourse markers, such as wait, hmm, and other hesitation cues, which can improve the model’s reasoning ability.}
  \label{fig:data curation}
\end{figure}
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{8.5pt}
\caption{
  \label{table: Experiments on Bespoke-our}
  Accuracy and Average Response Length comparison for Our finetuned Qwen-2.5-7B-Instruct before and after
ThinkPO. The "Improv." column shows the percentage change of \textbf{Ours} over the model. After applying ThinkPO, its accuracy and length almost improve across datasets, further validating the effectiveness of \TPO.
}\vspace{-10pt}
\begin{tabular}{c|cccc|cccc}
\toprule
 \multicolumn{1}{c}{} & \multicolumn{4}{|c|}{Accuracy} & \multicolumn{4}{c}{Average Response Length}\\[0.5ex]
\midrule
% \multirow{1}{*}{Dataset} & \multicolumn{3}{c}{Qwen 2.5-7B}    & \cellcolor{gray!20}{Improv.} & Qwen      &  & \textbf{Ours}  & \cellcolor{gray!20}{Improv.} \\
 Dataset                        & {\small Base}       & {\small +SFT}        & {\small+\textbf{ThinkPO}} & \cellcolor{gray!20}{{\small Improv.(\%)}} 
                         & {\small Base}       & {\small +SFT}        & {\small +\textbf{ThinkPO}} & \cellcolor{gray!20}{{\small Improv.(\%)}}\\
\midrule
MATH500     & $74.0$ & $82.8$ & $\mathbf{83.4}$ & \cellcolor{gray!20}{\color{darkgreen}$0.7\%$} & $637$  & $5603$ & $\mathbf{7568}$  & \cellcolor{gray!20}{{\color{darkgreen}$35.0\%$}}  \\
AIME        & $10.0$ & $20.0$ & $\mathbf{26.7}$ & \cellcolor{gray!20}{{\color{darkgreen}$33.5\%$}}  & $942$ & $21579$ & $\mathbf{23901}$ & \cellcolor{gray!20}{{\color{darkgreen}$10.7\%$}}\\
GPQA        & $34.9$ & $35.4$ & $\mathbf{36.9}$ & \cellcolor{gray!20}{{\color{darkgreen}$4.2\%$}} & $12$ & $5845$ & $\mathbf{7933}$  & \cellcolor{gray!20}{{\color{darkgreen}$35.6\%$}}\\
GSM8K       & $90.1$ & $\mathbf{93.9}$ & $93.0$ & \cellcolor{gray!20}{$-0.9\%$}  & $260$ & $1310$ & $\mathbf{1599}$ & \cellcolor{gray!20}{{\color{darkgreen}$22.1\%$}}  \\
Olympiad    & $38.9$ & $44.5$ & $\mathbf{46.9}$ & \cellcolor{gray!20}{{\color{darkgreen}$5.4\%$}}  & $942$ & $11251$ & $\mathbf{14200}$ & \cellcolor{gray!20}{{\color{darkgreen}$26.2\%$}}\\
\midrule
Avg.        & $49.6$ & $55.3$ & $\mathbf{57.4} $     & \cellcolor{gray!20}{\color{darkgreen}$8.6\%$}  & $558$ & $9117$ & $\mathbf{11040}$   & \cellcolor{gray!20}{\color{darkgreen}25.9\%}\\
\bottomrule
\end{tabular}
\end{table*}
\begin{figure*}[t]
\includegraphics[width=0.49\columnwidth]{imgs/open_source_lengths_deepseek.pdf}
\hfill
\includegraphics[width=0.49\columnwidth]{imgs/open_source_results_deepseek.pdf}
\hfill
\includegraphics[width=0.49\columnwidth]{imgs/open_source_lengths_repro.pdf}
\hfill
\includegraphics[width=0.49\columnwidth]{imgs/open_source_results_repro.pdf}\vspace{-10pt}
\caption{Visualization of improvements on Accuracy and Average Response Length of DeepSeek-R1-Distill-Qwen-7B (\textbf{Left}) and our finetuned Qwen2.5-7B-Instruct (\textbf{Right}) on four datasets After ThinkPO. ThinkPO could improve DeepSeek-7B's and our finetuned Qwen2.5-7B's accuracy and output lengths almost across all the datasets }
  \label{fig: results of deepseek and repro}
\vspace{3pt}
\hfill
\includegraphics[width=0.32\linewidth]{imgs/grad.png}
\includegraphics[width=0.32\linewidth]{imgs/loss.png}
\includegraphics[width=0.32\linewidth]{imgs/margins.png}
\vspace{-10pt}
 \caption{
   Training loss, gradient norm, and margin curves for DeepSeek-R1-Distill-Qwen-7B, Bespoke-Stratos-7B and our finetued Qwen2.5-7B-Instruct during Thinking Preference Optimization phase.
  }
 \label{fig:dpo trainging loss}
\end{figure*}


\subsection{Data Curation}

The dataset $\mathcal{D}_{sft} = \{(q, o_{long})\}_{N}$ is based on bespoke stratos dataset \cite{bespoke_stratos}. They  used DeepSeek-R1 as the teacher reasoning model instead of QwQ-32B-Preview to generate long reasoning response $o_{long}$ and employed GPT-4o-mini in place of Sky-thought T1’s \cite{sky_t1_2025} parsing logic to filter out incorrect mathematical solutions.

For the dataset $\mathcal{D}_{dpo} = \{(q, o_{long}, o_{short})\}_{N}$ in the second stage, we collect it in the following manner, referring to \cite{kimiteam2025kimik15scalingreinforcement}: For each question $q$ in $\mathcal{D}_{sft}$, we use Qwen2.5-Math-7B-Instruct \cite{qwen2.5} to generate a short reasoning response~$o_{short}$~, pairing it with the long reasoning response~$o_{long}$ in $\mathcal{D}_{sft}$. We then retain the samples where Qwen2.5-Math-7B-Instruct’s answer matched DeepSeek R1’s answer, resulting in 8,080 samples. Additionally, we include 2,000 samples where Qwen2.5-Math-7B-Instruct’s answer differed from DeepSeek R1’s but adhered to the correct response format, including more output distribution in $\mathcal{D}_{dpo}$. All of these combined samples consequently form the final dataset $\mathcal{D}_{dpo}$. The dataset is collected through a straight foreword and simple process of gathering short-reasoning data, which did not require significant resources, compared to high-quality long-reasoning data. 

% This approach ensures that both short and long reasoning data are properly formatted and almost correctly generated, with consistent formatting and guaranteed most correctness for reasoning results.

\begin{table*}[t]
\centering
\caption{Accuracy and Average Response Length comparison for Deepseek-7B and Bespoke-7B before and after ThinkPO. Qwen2.5-7B-Instruct shows the base performance, Deepseek-7B/Bespoke-7B report performance after SFT, and the "Improv." column shows the percentage change of \textbf{Ours} over Deepseek-7B/Bespoke-7B.}\vspace{-10pt}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{c|ccc|ccc}
\toprule
\multicolumn{7}{c}{\href{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B}{DeepSeek-R1-Distill-Qwen-7B} (Deepseek)} \\[0.5ex]
\midrule
\multicolumn{1}{c}{} & \multicolumn{3}{|c|}{Accuracy} & \multicolumn{3}{c}{Average Response Length}\\[0.5ex]
\hline
\multirow{2}{*}{Dataset} & Deepseek   & \textbf{Ours}  & \cellcolor{gray!20}Improv.  & Deepseek   & \textbf{Ours}  & \cellcolor{gray!20}Improv. \\
                         & {\small(SFT)}        & {\small(+ThinkPO)} & \cellcolor{gray!20}{\small(\%)} 
                         & {\small(SFT)}        & {\small(+ThinkPO)} & \cellcolor{gray!20}{\small(\%)}\\
\hline
MATH500     & $87.4$ & $\mathbf{91.2}$ & \cellcolor{gray!20}{\color{darkgreen}$4.3\%$} & $2577$   & $\mathbf{3021}$ & \cellcolor{gray!20}{\color{darkgreen}$17.2\%$} \\
AIME        & $\mathbf{56.7}$ & $43.3$ & \cellcolor{gray!20}${-23.6\%}^{*}$ & $11419$  & $\mathbf{12875}$ & \cellcolor{gray!20}{\color{darkgreen}$12.8\%$} \\
GPQA        & $47.0$ & $\mathbf{49.5}$ & \cellcolor{gray!20}{\color{darkgreen}$5.3\%$}  & $4895$   & $\mathbf{5604}$ & \cellcolor{gray!20}{\color{darkgreen}$14.5\%$} \\
GSM8K       & $87.2$ & $\mathbf{87.6}$ & \cellcolor{gray!20}{\color{darkgreen}$0.5\%$}  & $619$    & $\mathbf{668}$  & \cellcolor{gray!20}{\color{darkgreen}$7.9\%$} \\
Olympiad    & $58.6$ & $58.6$       & \cellcolor{gray!20}$0.0\%$  & $7196$   & $\mathbf{7383}$ & \cellcolor{gray!20}{\color{darkgreen}$2.6\%$}  \\
\midrule
\multicolumn{7}{c}{\href{https://huggingface.co/bespokelabs/Bespoke-Stratos-7B}{Bespoke-Stratos-7B} (Bespoke)} \\[0.5ex]
\midrule
\multicolumn{1}{c}{} & \multicolumn{3}{|c|}{Accuracy} & \multicolumn{3}{c}{Average Response Length}\\[0.5ex]
\hline
\multirow{2}{*}{Dataset} & Bespoke    & \textbf{Ours}  & \cellcolor{gray!20}Improv.  & Bespoke    & \textbf{Ours}  & \cellcolor{gray!20}Improv. \\
                         & {\small(SFT)}        & {\small(+ThinkPO)} & \cellcolor{gray!20}{\small(\%)} 
                         & {\small(SFT)}        & {\small(+ThinkPO)} & \cellcolor{gray!20}{\small(\%)}\\
\hline
MATH500     & $\mathbf{84.0}$ & $82.8$ & \cellcolor{gray!20}$-1.4\%$  & $5696$   & $\mathbf{6404}$ & \cellcolor{gray!20}{\color{darkgreen}$12.4\%$} \\
AIME        & $20.0$ & $\mathbf{23.3}$ & \cellcolor{gray!20}{\color{darkgreen}$16.5\%$}  & $19858$  & $\mathbf{20079}$ & \cellcolor{gray!20}{\color{darkgreen}1.1\%} \\
GPQA        & $37.9$ & $\mathbf{43.4}$ & \cellcolor{gray!20}{\color{darkgreen}$14.5\%$}  & $5968$   & $\mathbf{7301}$ & \cellcolor{gray!20}{\color{darkgreen}$22.3\%$}\\
GSM8K       & $92.9$ & $\mathbf{93.3}$ & \cellcolor{gray!20}{\color{darkgreen}$0.4\%$} & $1404$   & $\mathbf{1755}$ & \cellcolor{gray!20}{\color{darkgreen}$25.0\%$}  \\
Olympiad    & $44.1$ & $\mathbf{48.5}$ & \cellcolor{gray!20}{\color{darkgreen}$10.0\%$} & $11140$  & $\mathbf{12204}$ & \cellcolor{gray!20}{\color{darkgreen}$9.6\%$}\\
\bottomrule
\end{tabular}
\parbox{\textwidth}{
        \scriptsize * Since AIME2024 contains only 30 questions, even a small difference in the number of correct answers can lead to significant fluctuations in accuracy, making the decline appear larger than it actually is.\\
    }
\vspace{-5mm}
\label{tab:accuracy_with_improvement}
\end{table*}

\section{Experiments}
\subsection{Experimental Setup}

To evaluate model’s reasoning ability, we select five different test sets: MATH500 \cite{lightman2023lets}, AIME2024~\footnote{\href{https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions}{AIME2024} is a math competition for high school students, acting as a qualifier for the USAMO.}, GPQA-Diamond \cite{rein2023gpqa}, GSM8K \cite{cobbe2021gsm8k}, and Olympiad Bench Math \cite{he2024olympiadbench}. These test sets primarily consist of mathematical reasoning problems, with GPQA-Diamond also including problems from physics, chemistry, and biology. The difficulty levels of these test sets vary significantly, with GSM8K being the easiest while AIME2024 is the most challenging. This diverse selection ensures a comprehensive assessment of the model’s reasoning capability across different levels of difficulty, from fundamental arithmetic to complex problem-solving with different difficulty. 

When generating responses, we set the temperature as 0.7. For results on other temperatures, please refer to \cref{Evaluating ThinkPO with Different Temperatures}. We present our chosen hyper-parameters of ThinkPO, such as learning rate, batch size and $\beta$, in \cref{Training Recipe}.


\subsection{Effectiveness of ThinkPO}

This experiment primarily analyzes the average response length, accuracy and reasoning-supportive words count during both SFT and DPO processes to validate the effectiveness of Thinking Preference Optimization (ThinkPO). By tracking these metrics, we aim to demonstrate how ThinkPO enhances the model’s reasoning ability by encouraging longer, more structured outputs, ultimately leading to improved reasoning performances.

First, we fine-tune Qwen-2.5-7B-Instruct with Bespoke-Stratos-Dataset. Subsequently, we apply \TPO~to enhance the model’s reasoning ability. The final results are shown in \cref{table: Experiments on Bespoke-our}. Our finetuned model achieves scores across the five datasets that are almost identical to  Bespoke-Stratos-7B, which is also finetuned on Bespoke-Stratos-Dataset, confirming the correctness of our SFT process. Furthermore, after applying \TPO, our model demonstrates improvements on almost all the datasets, validating the effectiveness of ThinkPO in enhancing and improving LLM reasoning ability.

Additionally, we analyze average response length and reasoning-supportive words (like \textit{wait}, \textit{hmm}, etc) at different steps during both SFT and ThinkPO. We record the model’s average response length, accuracy and reasoning-supportive words (like wait, hmm, etc) count on Math500 at different training steps, distinguishing between the SFT and \TPO. When calculating average response lengths, we exclude duplicate or incomplete responses to ensure accuracy. Additionally, when counting reasoning-supportive words, we only consider correct answers to prevent excessive occurrences of filler words like “wait” due to underthinking \cite{chen2024not, kirk2023understanding, wang2025thoughts}. The results are visualized in \cref{fig:sft_length_plot}.

At the initial stage of SFT, the model’s reasoning ability improves significantly. In the later stages of SFT (like after 1200 steps), three metrics gradually plateau, indicating that the model may have reached a local optimum. However, after applying Thinking Preference Optimization, model’s average response length, reasoning-supportive words count and accuracy improve, showing the effectiveness of ThinkPO in overcoming this stagnation. We visualize the trend of output length and accuracy across training steps on other datasets(like GSM8K). For more details, please refer to \cref{Analysis of our Reproduce Model in other datasets}.

\begin{table*}
  \centering
  \setlength{\tabcolsep}{5.5pt}
  \caption{
  \label{table: Experiments on different model sizes}
Results of Models with Different Sizes (3B, 7B, 14B) on the Qwen-2.5 Family. We evaluate models of different sizes (3B, 7B, 14B) trained with Supervised Fine-Tuning (SFT) and Think Preference Optimization (ThinkPO). Models are fine-tuned on the Bespoke-Strato-Dataset for 1 epoch.  As model size increases, accuracy improves across all five test datasets. After ThinkPO training, accuracy improves consistently for models of all sizes, including the smallest (3B), demonstrating that ThinkPO enhances reasoning ability across different model scales.
  }\vspace{-10pt}
  \begin{tabular}{c|ccc|ccc|ccc}
  \toprule
   & \multicolumn{3}{c|}{Qwen 2.5-3B} & \multicolumn{3}{c|}{Qwen 2.5-7B} & \multicolumn{3}{c}{Qwen 2.5-14B} \\
   & {\small+SFT} &  {\small+\textbf{\TPO}} & Improv. &  {\small+SFT} &  {\small+\textbf{\TPO}} & Improv. & {\small+SFT} &  {\small+\textbf{\TPO}} & Improv. \\
  \midrule
   {MATH500} & $53.6$ & $\mathbf{54.6} $ & \cellcolor{gray!20}{\color{darkgreen}$1.8\%$} & $73.0$ & $\mathbf{74.6}$  & \cellcolor{gray!20}{\color{darkgreen}$2.2\%$} & $83.2$ & {$\mathbf{85.6}$}  & \cellcolor{gray!20}{\color{darkgreen}$2.9\%$} \\
  {AIME}  &$3.30$ &  $\mathbf{6.7}$ & \cellcolor{gray!20}{\color{darkgreen}$100\%$ } & $\mathbf{16.7}$ & $13.3$  & \cellcolor{gray!20}${-20.3\%}^*$ & $23.3$ & $\mathbf{33.3}$ & \cellcolor{gray!20}{\color{darkgreen}$42.9\%$} \\
    {GPQA} & $26.3$ & $\mathbf{27.3}$ & \cellcolor{gray!20}{\color{darkgreen}$3.8\%$} & $32.3$ & $\mathbf{36.4}$ & \cellcolor{gray!20}{\color{darkgreen}$12.7\%$} & $\mathbf{45.5}$ &  $44.0$ & \cellcolor{gray!20}$-3.2\%$ \\
   {GSM8K} & $80.4$ &  $\mathbf{81.1}$ & \cellcolor{gray!20}{\color{darkgreen}$0.8\%$} &$88.2$ & $\mathbf{88.9}$ & \cellcolor{gray!20}{\color{darkgreen}0.9\%} & $93.7$ & $\mathbf{93.9}$& \cellcolor{gray!20}{\color{darkgreen}$0.2\%$} \\
   {Olympiad}& $20.0$ & $\mathbf{22.0}$ & \cellcolor{gray!20}{\color{darkgreen}$10.0\%$} & $35.3$ & $\mathbf{37.2}$ & \cellcolor{gray!20}{\color{darkgreen}$5.3\%$} & $49.9$ & $\mathbf{52.1}$& \cellcolor{gray!20}{\color{darkgreen}$4.4\%$ }\\
   \bottomrule
   % \textbf{Average}& 36.72 & 38.34\upgreen & 4.4\% & 49.1 & 50.1\upgreen & 2.0\% & 59.1 & 61.78\upgreen& 4.5\% \\
   %  \hline
  \end{tabular}
  \parbox{\textwidth}{
        \scriptsize * Since AIME2024 contains only 30 questions, even a small difference in the number of correct answers can lead to significant fluctuations in accuracy, making the decline appear larger than it actually is.\\
    }
 \vspace{-5mm}
\end{table*}
\begin{figure*}[t]
\includegraphics[width=0.33\linewidth]{imgs/3size_acc.pdf}
\includegraphics[width=0.33\linewidth]{imgs/7size_acc.pdf}
\includegraphics[width=0.33\linewidth]{imgs/14size_acc.pdf}
\vspace{-25pt}
  \caption{Visualization of improvements on Accuracy and Average Response Length of models in the same family series from different sizes (Qwen-2.5-3B, Qwen-2.5-7B and Qwen-2.5-14B) on five datasets after ThinkPO. ThinkPO could improve models' accuracy and output lengths almost across all the datasets, regradless of sizes }
  \label{fig: TPO on different size}
\end{figure*}
\subsection{ThinkPO can Continually Improve Reasoning Ability of Public Distilled Models}

We select two open-source reasoning models and perform ThinkPO training using $\mathcal{D}_{dpo}$. Specifically, we chose \href{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B}{DeepSeek-R1-Distill-Qwen-7B} and \href{https://huggingface.co/bespokelabs/Bespoke-Stratos-7B}{Bespoke-Stratos-7B}, since both reasoning models were fine-tuned on Qwen2.5-7B-Instruct. 

As shown in \cref{tab:accuracy_with_improvement} and \cref{fig: results of deepseek and repro}, both models demonstrate an improvement in accuracy across five datasets. For example, Bespoke-Stratos-7B shows an increase in accuracy on all datasets except for a slight decline on the MATH500 dataset. Notably, the improvements on Olympiad Bench Math and GPQA-Diamond reach around $5\%$. DeepSeek-R1-Distill-Qwen-7B, with the exception of a decline on AIME2024, shows consistent or slightly improved accuracy. Specifically, on MATH500, the accuracy improves from 87.4\% to 91.2\%. 

In addition to accuracy, average response length of DeepSeek-R1-Distill-Qwen-7B is increased by around 500 tokens on the MATH500 dataset, while Bespoke-Stratos-7B shows a larger increase of approximately 1000 tokens. These align with test-time scaling principle \cite{snell2024scaling, welleck2024decoding}, where the increased response length reflects an enhancement in reasoning capacities.

\subsection{ThinkPO Works for Different-Size Models}

Previous experiments are all conducted using a 7B model for training. Now we utilize the Bespoke Stratos dataset and conduct one epoch of SFT training on models of varying sizes within the Qwen2.5 series (Qwen2.5-3B, Qwen2.5-7B, and Qwen2.5-14B). The learning rate is set to 3e-5, and other hyperparameters are kept consistent with Bespoke-Stratos-7B, ensuring the models' performances. The results after SFT and \TPO~ are presented in \cref{table: Experiments on different model sizes} and \cref{fig: TPO on different size}. First, as the model scale increases, its accuracy improves across all the datasets after SFT, which aligns with expectations. After applying \TPO, all models, regardless of size, achieve further improvements. Specifically, on Math500, all three models show an accuracy increase of 1\%–2\%. After applying ThinkPO, the Qwen2.5-3B model achieves accuracy improvements across all five datasets, while Qwen2.5-7B and 14B models show improvements on four datasets, which shows that ThinkPO is effective across different model scales, further validating its generalizability and robustness.

\section{Ablation}

\begin{table}
  \centering
  \caption{
  \label{table: Experiments on short data}
Results of ThinkPO on the model finetuned with a short-Reasoning Dataset. We select a short-chain reasoning dataset of the same size as the Bespoke-Stratos dataset and fine-tune Qwen-2.5-7B for 3 epochs. Models trained with reasoning-style datasets, regardless of response length, can benefit from \TPO~to enhance and improve their reasoning capability 
% Since the dataset contains short reasoning samples, the model’s accuracy after SFT is lower compared to models fine-tuned on long reasoning data.
  }\vspace{-10pt}
  \begin{tabular}{c|ccc}
  \toprule
   \multicolumn{1}{c}{} & 
   Short & \textbf{Our} & Improv.  \\
   \multicolumn{1}{c}{} & {\small+SFT} &  {\small+\textbf{\TPO}} & \% \\
  \midrule
   \textbf{MATH500} & $57.8$ & $\mathbf{59.0}$ & \cellcolor{gray!20}{\color{darkgreen}$2.4\%$} \\
  \textbf{AIME}  &$0.0$ & $\mathbf{3.3}$ & \cellcolor{gray!20}{\color{darkgreen}$100\%$} \\
    \textbf{GPQA} & $30.3$ & $\mathbf{31.3}$ & \cellcolor{gray!20}{\color{darkgreen}$3.3\%$} \\
   \textbf{GSM8K} &$83.4$ & $\mathbf{85.1}$& \cellcolor{gray!20}{\color{darkgreen}$2.0\%$} \\
   \textbf{Olympiad}& $23.3$ & $\mathbf{23.6}$& \cellcolor{gray!20}{\color{darkgreen}$1.2\%$} \\
    \bottomrule
    % \textbf{Average}& 0 & 0\upgreen\\
    % \hline
  \end{tabular}
\end{table}

\subsection{Whether ThinkPO is Useful when SFT with Short Reasoning Data?}

In our previous experiments, we fully fine-tuned the model using long reasoning datasets before applying ThinkPO to further enhance its reasoning ability. However, an important question arises: If we use short reasoning data instead of long reasoning data during the full fine-tuning stage, can Thinking Preference Optimization still improve the model’s reasoning performance effectively?

To investigate this, we conduct the following experiment. We use Qwen2.5-7B as the base model and select a dataset from AI-MO/NuminaMath-CoT\cite{numina_math_datasets} that matches the Bespoke-Stratos dataset with the same data size for fine-tuning. Unlike our previous experiments, the fine-tuning data here consists of short reasoning examples rather than long reasoning ones. Consequently, the fine-tuned model is expected to underperform compared to models trained on long-reasoning data. To equip models with basic reasoning ability, we fine-tune them for three epochs and set learning rate as 1e-5. Following this, we apply Thinking Preference Optimization using the same dataset in the previous experiments, aiming to further enhance and improve the model’s reasoning performance. 

As shown in \cref{table: Experiments on short data}, even after fine-tuning on short-reasoning data, ThinkPO still effectively improves the model’s reasoning ability. For example, on the Math500 dataset, after applying ThinkPO, the model’s accuracy improves by approximately 2\%. This result demonstrates that models trained with reasoning-style datasets, regardless of response length, can benefit from ThinkPO to enhance and improve their reasoning capability.


\subsection{Exploring the Impact of Length Differences between Chosen and Rejected Samples on ThinkPO.}

\begin{figure}[t]
\includegraphics[width=\columnwidth]{imgs/dataset_length_boxplot.pdf}\vspace{-10pt}
  \caption{
    Length difference distribution between chosen and rejected samples across three datasets. These three datasets are 1000 samples selected based on the length difference from our \TPO-Dataset. The long dataset exhibits the widest distribution of length differences, while the middle and short datasets have smaller differences with lower mean values and variances.   
  }
  \label{fig:length difference}
\end{figure}
\begin{table}
  \centering
  \caption{
  \label{table: Experiments on different length}
   Model performance across three datasets with varying chosen and rejected sample length difference distributions. “Avg Differences” represents the average length difference between chosen and rejected samples. \textit{Short} yields the best overall performance, suggesting that appropriate length differences improve ThinkPO learning, while too large differences may hinder it.
   % Despite the variations in length differences, each dataset improves model accuracy on different test sets, suggesting that length differences do not significantly impact the model’s reasoning ability. Notably, the short dataset, with smaller length differences, results in the most improvement, possibly due to a more consistent output distribution, which aids the model’s reasoning.
  }\vspace{-10pt}
  \begin{tabular}{c|ccc}
  \toprule
   \multicolumn{1}{c}{} & Short & Middle & Long \\
   \midrule
   \textbf{Avg Differences} & $621$ & $1525$ & $4758$ \\
  \midrule
   \textbf{MATH500} & $\mathbf{84.2}$ & $81.8$ & $\mathbf{84.0}$ \\
  \textbf{AIME}  & $\mathbf{26.7}$ &$ 13.3$ & $16.7$\\
    \textbf{GPQA} &$ 40.9$ & $\mathbf{41.9}$ & $38.9$\\
   \textbf{GSM8K} &$92.9 $& $92.9$ & $\mathbf{93.0}$\\
   \textbf{Olympiad}& $\mathbf{46.1}$ & $45.9$& $45.9$\\
    \bottomrule
  \end{tabular}
\end{table}


In the entire ThinkPO dataset, we select long reasoning data as chosen and short reasoning data as rejected. A key question is whether the length disparity between chosen and rejected samples affects the ThinkPO training because length disparity is not distributed evenly in the dataset. To investigate this, we conduct an experiment to verify the impact of length differences on the ThinkPO training.

The ThinkPO dataset contains approximately 10,000 samples, but the length disparity between chosen and rejected samples is not uniformly distributed. Therefore, we select three datasets with different length distributions: short, middle, and long, each containing 1,000 samples. \cref{fig:length difference} shows details of the length differences distributions between chosen and rejected samples in these three datasets, with the long dataset exhibiting the largest and most widely distributed differences, the middle dataset showing moderate differences, and the short dataset having the smallest differences.

\cref{table: Experiments on different length} displays the results after ThinkPO for one epoch, using the Bespoke-Stratos-7B model as the base model. Each dataset shows certain advantages across the five test datasets. However, the short dataset yields the best performance on overall datasets. We propose that when the length difference is smaller, the model’s output distributions for both samples are more consistent, which benefits ThinkPO learning. On the other hand, when it is too large, it may not help the model’s learning.

\section{Related Works}

\textbf{LLM Reasoning Ability}. With the development of large models, reasoning ability \cite{wang2022self,zhang2023multimodal,yao2023tree,plaat2024reasoning} has become one of the most crucial capabilities and a necessary condition for achieving AGI (Artificial General Intelligence) 
\cite{minaee2024large,xu2024survey,morris2023levels,feng2024far,krishnan2025artificial}. The earliest appearance of long-chain reasoning ability in large models can be traced to OpenAI o1 \cite{jaech2024openai, arrieta2025o3,hurst2024gpt}, which excelled across various mathematical reasoning test sets and outperform contemporary LLMs.

This was followed by the release of the QwQ model \cite{qwen2.5, bai2023qwen, bai2023qwens, chu2024qwen2}, which trained reasoning capabilities using a process reward model approach \cite{li2024process, ma2023let, zhang2025lessons, lambert2024rewardbench}. Currently, the emergence of DeepSeek R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability} and Kimi 1.5 \cite{kimiteam2025kimik15scalingreinforcement} has further enhanced the reasoning abilities of large open-source models. DeepSeek R1 utilizes a simple rule-based reward model \cite{ramesh2024group,hu2025reinforce++, shao2024deepseekmath, alonso2025mathematics, kirk2023understanding, yang2024bayesian} to effectively boost the model’s reasoning performance, bringing about an aha moment that narrows the reasoning capability gap between open-source and closed-source models. On the other hand, Kimi 1.5 employs several tricks, such as long-to-short reasoning, to achieve high efficiency in LLM reasoning performance.

Many works on open-source reasoning models have also emerged. First is Sky-Thought T1 \cite{sky_t1_2025}, which uses QwQ-32B-Preview as a teacher model to generate reasoning answers for training data. Then, Bespoke-Stratos \cite{bespoke_stratos} built upon Sky-Thought T1, using DeepSeek R1 as the teacher model to generate answers for Sky-Thought data. Since DeepSeek R1 has far superior reasoning abilities compared to QwQ-32B-Preview, the generated data quality is higher, allowing Bespoke-Stratos-7B and Bespoke-Stratos-32B models to achieve DeepSeek-level advanced reasoning performance after training on around 17k data points. Recently, s1 \cite{muennighoff2025s1simpletesttimescaling} and LIMO \cite{ye2025limoreasoning} have emphasized that fine-tuned, high-quality data construction is essential for models to achieve SOTA reasoning capabilities. 

\textbf{Direct Preference Optimization}. RLHF 
 \cite{chaudhari2024rlhf, kirk2023understanding, kaufmann2023survey} is designed to align model outputs with human preferences after supervised fine-tuning (SFT). Various methods have been introduced, such as Proximal Policy Optimization (PPO) 
 \cite{engstrom2019implementation, huang2022a2c, wijmans2019dd}. However, PPO is an online method that requires significant computational resources. To address this, Direct Preference Optimization was proposed, enabling offline training with only chosen and rejected sample pairs while reducing computational costs compared to PPO. Recently, several DPO variants \cite{wu2024beta,wu2024alpha, qi2024online, zhong2024dpo,su2025reveal} have emerged, including StepDPO \cite{lai2024step}, KTO \cite{ethayarajh2024kto}, SimPO \cite{meng2024simpo}, LongDPO \cite{ping2025longdpo}, Test-Time Preference Optimization \cite{li2025testtimepreferenceoptimizationonthefly} etc. Among them, LongDPO shares similarities with our proposed method. However, LongDPO primarily focuses on improving long-form story generation instead of reasoning abilities.

\section{Conclusion}

We introduce Thinking Preference Optimization, a simple yet effective post-SFT method without the need for additional high-quality long-reasoning data. By leveraging short reasoning responses as rejected and long reasoning responses as chosen, \TPO~encourages models to generate detailed reasoning outputs, effectively maximizing the utility of existing long-reasoning data. Our experiments demonstrate that \TPO~significantly improves model performance, yielding an 8.6\% accuracy boost and a 25.9\% increase in output length for SFT-ed models. Additionally, \TPO~enhances the publicly available DeepSeek-R1-Distill-Qwen-7B model, raising its accuracy on the MATH500 dataset from 87.4\% to 91.2\%. These results underscore that \TPO~provides a lightweight solution that improves reasoning capabilities without high resources and \TPO’s ability to overcome performance bottlenecks in multi-epoch SFT with fixed and limited high-quality long-reasoning data. 

\newpage

\section*{Limitations}

\TPO~can further enhance SFT-ed models without requiring additional high-quality long reasoning data. However, since \TPO~is based on the DPO method, it is sensitive to hyperparameters, requiring careful tuning of $\beta$ and learning rate to achieve optimal improvements. 

\bibliography{custom}


\newpage
\appendix
% \newpage
% \tableofcontents
\label{sec:appendix}
\section{Appendix}
\subsection{Evaluating ThinkPO with Different Temperatures}
\label{Evaluating ThinkPO with Different Temperatures}


In our experiments, we initially evaluated the model at a temperature of 0.7. While this provides a good measure of performance, it is important to explore different sampling conditions for a more robust analysis. Therefore, we additionally tested temperatures of 0.1 and 0.5 to examine how ThinkPO impacts Bespoke-Strato-7B under varying levels of randomness in sampling. By comparing results across these temperature settings, we can assess whether ThinkPO consistently enhances the model’s reasoning ability regardless of generation strategy. To provide a comprehensive evaluation, we average the results across all three temperatures. The results are shown in \cref{table: Experiments on different temperatures}.

Our findings demonstrate that ThinkPO consistently improves model performance across different temperature settings. Specifically, at temperatures of 0.1 and 0.7, accuracy increases on four datasets, while at 0.5, improvements are observed on three. To gain a more holistic understanding of ThinkPO’s impact, we average the results across all temperature settings, showing that ThinkPO enhances performance on all five datasets. Notably, on MATH500, ThinkPO improves accuracy by 1.4\%. These results further validate the effectiveness of our proposed method and demonstrate its ability to consistently enhance reasoning performance across different sampling conditions.

\begin{table*}
  \centering
  \caption{
  \label{table: Experiments on different temperatures}
Evaluation of Bespoke-Strato-7B with different temperatures(0.1,0.5,0.7).
Across different values of temperatures, the model achieves accuracy improvements on most datasets. After averaging the results, ThinkPO consistently enhances the model’s performance across all five datasets.
  }\vspace{-10pt}
  \begin{tabular}{c|cc|cc|cc|ccc}
  \toprule
   & \multicolumn{2}{c|}{Temperature=0.1} & \multicolumn{2}{c|}{Temperature=0.5} & \multicolumn{2}{c|}{Temperature=0.7} & \multicolumn{3}{c}{Average} \\
   & {\small+SFT} &  {\small+\textbf{\TPO}} &  {\small+SFT} &  {\small+\textbf{\TPO}} & {\small+SFT} &  {\small+\textbf{\TPO}} & {\small+SFT} &  {\small+\textbf{\TPO}} & Improv. \\
  \midrule
   {MATH500} & 70.2 & 73.4 \upgreen & 81.4 & 82.6\upgreen & 84.0  & 82.8 \downred& 78.5 & 79.6\upgreen & \cellcolor{gray!20}{\color{darkgreen}1.4\%} \\
  {AIME}  &10.0 &  16.7 \upgreen& 20.0 & 16.7\downred  & 20.0 & 23.3\upgreen & 16.7 & 18.9 \upgreen& \cellcolor{gray!20}{\color{darkgreen}13.2\%} \\
    {GPQA} & 34.9 & 30.8\downred & 33.8 & 41.0\upgreen & 37.9 & 43.4\upgreen & 35.5 &  38.4\upgreen& \cellcolor{gray!20}{\color{darkgreen}8.1\%} \\
   {GSM8K} & 89.3 &  91.0 \upgreen & 92.4 &92.3\downred& 92.9 & 93.3 \upgreen& 91.5 & 92.2\upgreen& \cellcolor{gray!20}{\color{darkgreen}0.7\%} \\
   {Olympiad}& 32.8 & 39.6\upgreen & 42.3 &44.8\upgreen & 44.1 & 48.5\upgreen & 39.7& 44.3\upgreen & \cellcolor{gray!20}{\color{darkgreen}11.6\% }\\
   \bottomrule
   % \textbf{Average}& 36.72 & 38.34\upgreen & 4.4\% & 49.1 & 50.1\upgreen & 2.0\% & 59.1 & 61.78\upgreen& 4.5\% \\
   %  \hline
  \end{tabular}
\end{table*}


\subsection{Analysis of our Reproduce Model in other datasets}
\label{Analysis of our Reproduce Model in other datasets}
Previously, we only presented the changes in accuracy, average response length, and reasoning-supportive words count over training steps on the MATH500 dataset. Here, we extend our analysis by showcasing results on two additional datasets (like GSM8K) from our reproduced model. The detailed results are illustrated in \cref{fig:sft_length_plot_other}.

As observed in the results for GSM8K and Olympiad Bench Math, the model exhibits a similar trend to MATH500 across all three metrics. During the early stages of SFT, the model’s reasoning ability improves rapidly. However, in later stages, it reaches a performance plateau. ThinkPO effectively helps the model overcome this bottleneck, further enhancing its reasoning capability.

\begin{figure*}[t]
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_acc_GSM8K_plot.pdf}
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_length_GSM8K_plot.pdf}
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_words_GSM8K_plot.pdf}
 \caption{
   Analysis of accuracy(\textbf{Left}), average response length(\textbf{Middle}) and reasoning-supportive words count(\textbf{Right}, like wait, hmm, etc) in reproducing Bespoke-Stratos-7B. We evaluate the model on GSM8K every 300 steps and record results. In the early training stages, all of them improve significantly. However, in the later stages (e.g., after 1200 steps), the model’s performance plateau. When ThinkPO is applied, we see additional improvements in all of the three aspects, demonstrating the effectiveness of Think Preference Optimization.
  }
\hfill

\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_acc_OlympiadBenchMath_plot.pdf}
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_length_OlympiadBenchMath_plot.pdf}
\includegraphics[width=0.33\linewidth]{imgs/sft_dpo_words_OlympiadBenchMath_plot.pdf}
 \caption{
   Analysis of accuracy(\textbf{Left}), average response length(\textbf{Middle}) and reasoning-supportive words count(\textbf{Right}, like wait, hmm, etc) in reproducing Bespoke-Stratos-7B. We evaluate the model on OlympiadBenchMath every 300 steps and record results. In the early training stages, all of them improve significantly. However, in the later stages (e.g., after 1200 steps), the model’s performance plateau. When ThinkPO is applied, we see additional improvements in all of the three aspects, demonstrating the effectiveness of Think Preference Optimization.
  }\label{fig:sft_length_plot_other}
\end{figure*}

\subsection{Training Recipe}
\label{Training Recipe}
%  here we list all the parameters in our experments

\begin{table*}
  \centering
  \caption{
  The optimal hyperparameters identified in our experiments are listed here, including batch size, learning rate (lr), and beta. These parameters were carefully tuned to achieve the best performance improvements.
  }\label{table:ThinkPO_hyperparameters}\vspace{-10pt}
  \begin{tabular}{c|ccc|ccc|ccc}
  \toprule
   & \multicolumn{3}{c}{Deepseek-7B}& \multicolumn{3}{c}{Bespoke-7B} & \multicolumn{3}{c}{Bespoke-7B-reproduced}  \\
  \midrule
   {batch size}& \multicolumn{3}{c}{48} & \multicolumn{3}{c}{48} & \multicolumn{3}{c}{48}\\
  {lr} & \multicolumn{3}{c}{1e-7} & \multicolumn{3}{c}{5e-7} & \multicolumn{3}{c}{3e-7}  \\
    {$\beta$}& \multicolumn{3}{c}{0.01} & \multicolumn{3}{c}{0.01} & \multicolumn{3}{c}{0.01} \\
   \midrule
    & \multicolumn{3}{c}{Qwen2.5-3B-SFT}& \multicolumn{3}{c}{Qwen2.5-7B-SFT} & \multicolumn{3}{c}{Qwen2.5-14B-SFT}  \\
    \midrule
   {batch size}& \multicolumn{3}{c}{48} & \multicolumn{3}{c}{48} & \multicolumn{3}{c}{48}  \\
  {lr}& \multicolumn{3}{c}{5e-7} & \multicolumn{3}{c}{8e-8} & \multicolumn{3}{c}{1e-7}  \\
    {$\beta$}& \multicolumn{3}{c}{0.01} & \multicolumn{3}{c}{0.01} & \multicolumn{3}{c}{0.01} \\
    \bottomrule
  \end{tabular}
\end{table*}

Here, we provide the corresponding hyperparameters—batch size, learning rate, and $\beta$—that were used to achieve these optimal outcomes. All the hyperparameters are presented in \cref{table:ThinkPO_hyperparameters}.

Besides, we present the training loss curves, gradient norm curves, and margin curves for three models during the \TPO~phase in \cref{fig:dpo trainging loss}. These metrics provide insights into how the models perform throughout the training process, including their convergence behavior, stability of gradients, and the differences in preference between chosen and rejected samples. By examining these curves, we can better understand the effectiveness of \TPO~in enhancing model performance.


\subsection{Examples of LLM's outputs before and after ThinkPO}
\label{Examples of LLM's outputs before and after ThinkPO}
We present the changes in the total number of reasoning-supportive words (such as wait, hmm, let’s think, etc.) throughout both the SFT and ThinkPO training stages in \cref{fig:sft_length_plot} and \cref{fig:sft_length_plot_other}. These words serve as indicators of the model’s reasoning process, reflecting its ability to structure logical steps before arriving at a final answer. Our results show that the number of reasoning-supportive words increases significantly during the initial stages of SFT but eventually plateaus, suggesting that conventional fine-tuning alone may not be sufficient to further enhance structured reasoning. However, after applying ThinkPO, we observe a clear upward trend in the use of these reasoning-supportive expressions, indicating that our method effectively encourages the model to adopt a more deliberative reasoning process.

We provide examples of model outputs before and after applying ThinkPO in \cref{fig:example} and \cref{fig:example2}. Before ThinkPO, the model’s responses tend to be more direct, with fewer reasoning-supportive words, often resulting in incorrect or incomplete answers. In contrast, after applying ThinkPO, the model generates responses that utilize a greater number of reasoning-supportive words. This shift leads to a noticeable improvement in answer correctness, reinforcing the effectiveness of ThinkPO in enhancing the model’s reasoning ability. These findings highlight that ThinkPO not only improves accuracy but also aligns the model’s output with human-like problem-solving patterns.

\input{examples/example1}
\input{examples/example2}

\end{document}
