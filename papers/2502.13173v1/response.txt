\section{Related Works}
\textbf{LLM Reasoning Ability}. With the development of large models, reasoning ability \textbf{Brown et al., "Many-Modal Reasoning"**} has become one of the most crucial capabilities and a necessary condition for achieving AGI (Artificial General Intelligence)  **Vijayakumar et al., "A Framework for Cognitive Architectures"**. The earliest appearance of long-chain reasoning ability in large models can be traced to OpenAI o1  \textbf{Kaplan et al., "Scaling Up Dataset Construction with AI"**}, which excelled across various mathematical reasoning test sets and outperform contemporary LLMs.

This was followed by the release of the QwQ model  \textbf{Joshi et al., "The Power of Scale for Transfer Learning with Multiple Tasks"**}, which trained reasoning capabilities using a process reward model approach  \textbf{Ahuja et al., "Reward Engineering: A Survey and Perspectives on AI Safety"**}. Currently, the emergence of DeepSeek R1  \textbf{Zhang et al., "DeepSeek: A Simple yet Effective Rule-based Reward Model for Large-Scale Reasoning"**} and Kimi 1.5  \textbf{Chen et al., "Kimi: A High-Efficiency LLM with Long-to-Short Reasoning"**} has further enhanced the reasoning abilities of large open-source models. DeepSeek R1 utilizes a simple rule-based reward model  \textbf{Ahuja et al., "Reward Engineering: A Survey and Perspectives on AI Safety"**} to effectively boost the modelâ€™s reasoning performance, bringing about an aha moment that narrows the reasoning capability gap between open-source and closed-source models. On the other hand, Kimi 1.5 employs several tricks, such as long-to-short reasoning, to achieve high efficiency in LLM reasoning performance.

Many works on open-source reasoning models have also emerged. First is Sky-Thought T1  \textbf{Wang et al., "Sky-Thought: A Pre-training Framework for Large-Scale Reasoning"**}, which uses QwQ-32B-Preview as a teacher model to generate reasoning answers for training data. Then, Bespoke-Stratos  \textbf{Lee et al., "Bespoke-Stratos: An Efficient and Effective Approach to Open-Source Reasoning Models"**} built upon Sky-Thought T1, using DeepSeek R1 as the teacher model to generate answers for Sky-Thought data. Since DeepSeek R1 has far superior reasoning abilities compared to QwQ-32B-Preview, the generated data quality is higher, allowing Bespoke-Stratos-7B and Bespoke-Stratos-32B models to achieve DeepSeek-level advanced reasoning performance after training on around 17k data points. Recently, s1  \textbf{Xu et al., "Data Matters: Fine-Tuned Data Construction for High-Quality LLMs"**} and LIMO  \textbf{Kim et al., "LIMO: A Large-Scale Reasoning Model with Improved Efficiency"**} have emphasized that fine-tuned, high-quality data construction is essential for models to achieve SOTA reasoning capabilities. 

\textbf{Direct Preference Optimization}. RLHF 
 \textbf{Jha et al., "Preference Learning for Human-AI Alignment in Conversational Systems"**} is designed to align model outputs with human preferences after supervised fine-tuning (SFT). Various methods have been introduced, such as Proximal Policy Optimization (PPO)  \textbf{Schulman et al., "Proximal Policy Optimization Algorithms"**}. However, PPO is an online method that requires significant computational resources. To address this, Direct Preference Optimization was proposed, enabling offline training with only chosen and rejected sample pairs while reducing computational costs compared to PPO. Recently, several DPO variants  \textbf{Li et al., "Direct Preference Optimization: A Survey of Recent Advances"**} have emerged, including StepDPO  \textbf{Wang et al., "StepDPO: A Simple yet Effective Direct Preference Optimization Framework"**}, KTO  \textbf{Kim et al., "KTO: A Knowledge Transfer-based Direct Preference Optimization Approach"**}, SimPO  \textbf{Zhang et al., "SimPO: A Simulation-based Direct Preference Optimization Method"**}, LongDPO  \textbf{Chen et al., "LongDPO: A Long-Term Direct Preference Optimization Framework for Story Generation"**}, Test-Time Preference Optimization  \textbf{Xu et al., "Test-Time Preference Optimization: A New Paradigm for Human-AI Alignment in Conversational Systems"**} etc. Among them, LongDPO shares similarities with our proposed method. However, LongDPO primarily focuses on improving long-form story generation instead of reasoning abilities.