\section{Related Works}
\textbf{LLM Reasoning Ability}. With the development of large models, reasoning ability \cite{wang2022self,zhang2023multimodal,yao2023tree,plaat2024reasoning} has become one of the most crucial capabilities and a necessary condition for achieving AGI (Artificial General Intelligence) 
\cite{minaee2024large,xu2024survey,morris2023levels,feng2024far,krishnan2025artificial}. The earliest appearance of long-chain reasoning ability in large models can be traced to OpenAI o1 \cite{jaech2024openai, arrieta2025o3,hurst2024gpt}, which excelled across various mathematical reasoning test sets and outperform contemporary LLMs.

This was followed by the release of the QwQ model \cite{qwen2.5, bai2023qwen, bai2023qwens, chu2024qwen2}, which trained reasoning capabilities using a process reward model approach \cite{li2024process, ma2023let, zhang2025lessons, lambert2024rewardbench}. Currently, the emergence of DeepSeek R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability} and Kimi 1.5 \cite{kimiteam2025kimik15scalingreinforcement} has further enhanced the reasoning abilities of large open-source models. DeepSeek R1 utilizes a simple rule-based reward model \cite{ramesh2024group,hu2025reinforce++, shao2024deepseekmath, alonso2025mathematics, kirk2023understanding, yang2024bayesian} to effectively boost the modelâ€™s reasoning performance, bringing about an aha moment that narrows the reasoning capability gap between open-source and closed-source models. On the other hand, Kimi 1.5 employs several tricks, such as long-to-short reasoning, to achieve high efficiency in LLM reasoning performance.

Many works on open-source reasoning models have also emerged. First is Sky-Thought T1 \cite{sky_t1_2025}, which uses QwQ-32B-Preview as a teacher model to generate reasoning answers for training data. Then, Bespoke-Stratos \cite{bespoke_stratos} built upon Sky-Thought T1, using DeepSeek R1 as the teacher model to generate answers for Sky-Thought data. Since DeepSeek R1 has far superior reasoning abilities compared to QwQ-32B-Preview, the generated data quality is higher, allowing Bespoke-Stratos-7B and Bespoke-Stratos-32B models to achieve DeepSeek-level advanced reasoning performance after training on around 17k data points. Recently, s1 \cite{muennighoff2025s1simpletesttimescaling} and LIMO \cite{ye2025limoreasoning} have emphasized that fine-tuned, high-quality data construction is essential for models to achieve SOTA reasoning capabilities. 

\textbf{Direct Preference Optimization}. RLHF 
 \cite{chaudhari2024rlhf, kirk2023understanding, kaufmann2023survey} is designed to align model outputs with human preferences after supervised fine-tuning (SFT). Various methods have been introduced, such as Proximal Policy Optimization (PPO) 
 \cite{engstrom2019implementation, huang2022a2c, wijmans2019dd}. However, PPO is an online method that requires significant computational resources. To address this, Direct Preference Optimization was proposed, enabling offline training with only chosen and rejected sample pairs while reducing computational costs compared to PPO. Recently, several DPO variants \cite{wu2024beta,wu2024alpha, qi2024online, zhong2024dpo,su2025reveal} have emerged, including StepDPO \cite{lai2024step}, KTO \cite{ethayarajh2024kto}, SimPO \cite{meng2024simpo}, LongDPO \cite{ping2025longdpo}, Test-Time Preference Optimization \cite{li2025testtimepreferenceoptimizationonthefly} etc. Among them, LongDPO shares similarities with our proposed method. However, LongDPO primarily focuses on improving long-form story generation instead of reasoning abilities.