\section{Related Works}
\textbf{LLM Reasoning Ability}. With the development of large models, reasoning ability ____ has become one of the most crucial capabilities and a necessary condition for achieving AGI (Artificial General Intelligence) 
____. The earliest appearance of long-chain reasoning ability in large models can be traced to OpenAI o1 ____, which excelled across various mathematical reasoning test sets and outperform contemporary LLMs.

This was followed by the release of the QwQ model ____, which trained reasoning capabilities using a process reward model approach ____. Currently, the emergence of DeepSeek R1 ____ and Kimi 1.5 ____ has further enhanced the reasoning abilities of large open-source models. DeepSeek R1 utilizes a simple rule-based reward model ____ to effectively boost the modelâ€™s reasoning performance, bringing about an aha moment that narrows the reasoning capability gap between open-source and closed-source models. On the other hand, Kimi 1.5 employs several tricks, such as long-to-short reasoning, to achieve high efficiency in LLM reasoning performance.

Many works on open-source reasoning models have also emerged. First is Sky-Thought T1 ____, which uses QwQ-32B-Preview as a teacher model to generate reasoning answers for training data. Then, Bespoke-Stratos ____ built upon Sky-Thought T1, using DeepSeek R1 as the teacher model to generate answers for Sky-Thought data. Since DeepSeek R1 has far superior reasoning abilities compared to QwQ-32B-Preview, the generated data quality is higher, allowing Bespoke-Stratos-7B and Bespoke-Stratos-32B models to achieve DeepSeek-level advanced reasoning performance after training on around 17k data points. Recently, s1 ____ and LIMO ____ have emphasized that fine-tuned, high-quality data construction is essential for models to achieve SOTA reasoning capabilities. 

\textbf{Direct Preference Optimization}. RLHF 
 ____ is designed to align model outputs with human preferences after supervised fine-tuning (SFT). Various methods have been introduced, such as Proximal Policy Optimization (PPO) 
 ____. However, PPO is an online method that requires significant computational resources. To address this, Direct Preference Optimization was proposed, enabling offline training with only chosen and rejected sample pairs while reducing computational costs compared to PPO. Recently, several DPO variants ____ have emerged, including StepDPO ____, KTO ____, SimPO ____, LongDPO ____, Test-Time Preference Optimization ____ etc. Among them, LongDPO shares similarities with our proposed method. However, LongDPO primarily focuses on improving long-form story generation instead of reasoning abilities.