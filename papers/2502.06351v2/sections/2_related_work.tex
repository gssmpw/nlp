\section{Related work}

\textbf{EDL:} EDL leverages models to predict the Dirichlet prior distribution, with training typically done using NLL~\citep{haussmann2023bayesian}, $\ell_p$-loss~\citep{tsiligkaridis2021information}, or MSE~\citep{sensoy2018evidential}. \citet{chen2018variational,joo2020being,shen2023post} derive loss functions from a variational Bayesian perspective, while Posterior Networks~\citep{charpentier2020posterior,charpentier2022natural} optimize the posterior via Normalizing Flows. EDL methods often incorporate two main types of regularization: (i) encouraging uniformity in non-target Dirichlet parameters~\citep{malinin2018predictive,sensoy2018evidential,chen2018variational,tsiligkaridis2021information}, or (ii) modifying assumptions in the EDL formulation~\citep{deng2023uncertainty,chen2024redl}. IB-EDL differs by not requiring (i) and taking an alternative approach to (ii), without altering EDL assumptions. Some EDL methods also incorporate OOD data during training~\citep{malinin2018predictive,malinin2019reverse}. Beyond classification, EDL has been extended to regression~\citep{amini2020deep} and other applications~\citep{gao2024evidential,liu2024weakly}. Recent works~\citep{shen2024are,juergens2024is} analyze EDLâ€™s effectiveness and limitations.

\textbf{Other uncertainty-aware methods:} Besides EDL, there are other methods for uncertainty estimation and calibration, including Bayesian Neural Networks via Variational Inference~\citep{graves2011practical,blundell2015weight}, MC-Dropout~\citep{gal2016dropout}, stochastic gradient MCMC~\citep{welling2011bayesian,ma2015complete}, and Laplace approximations~\citep{ritter2018scalable,kristiadi2021learnable}, recently extended to LoRA fine-tuned LLMs~\citep{yang2024lalora,wang2024blob, li2024mixlora}.

\textbf{IB:} The Information Bottleneck (IB) was introduced by \citet{tishby99information} and later applied in neural networks for learning generalized representations~\citep{tishby2015deep,alemi2017deep,sun2022graph}, and as a feature attribution method~\citep{schulz2020restricting,zhang2021fine,wang2023visual}. Other works focusing on theory studied different Markov chains in IB~\citep{wieczorek2020difference} and the impact of IB on generalization errors~\citep{kawaguchi2023does}.