\section{Background}
\subsection{MLE fine-tuning of LLMs}
Let $\xv \in \Vspace^S$ represent the input sequence for an LLM, where $\Vspace$ represents the set of tokens (vocabulary) and $S$ is the sequence length.
The target space is denoted by $\Yspace$, which may be identical to $\Vspace$ (e.g., in next-token prediction) or a different set (e.g., in sentiment analysis). We generally assume that $|\Yspace| = C$. 
As we focus on the context of LLMs, \emph{we will use the term ``tokens'' instead of ``classes''} throughout this paper. 
In tasks like next-token prediction, the target can also be a sequence of tokens. For clarity in our theoretical analysis, we focus on a single-token target, with the understanding that a token sequence can be treated as multiple single-token targets. 

Let $f$ be the LLM. The output logits of the LLM, $f(\xv) \in \mathbb{R}^{C}$, are passed through a Softmax function, yielding a vector $\piv$ with entries $\pi_j = \exp(f(\xv)_j) / \sum_{j^\prime = 1}^C \exp(f(\xv)_{j^\prime})$, which represent the probability for each token. Let $\yv \in \{0, 1\}^C$ be the one-hot encoded target. Then, $\yv | \xv$ conforms to a categorical distribution $p(\yv|\xv) = \Cat{\yv ; \piv} = \prod_{j=1}^C \pi_j^{y_j}$. Fine-tuning LLMs on downstream tasks typically involves minimizing $- \log p(\yv|\xv)$ which corresponds to maximum likelihood estimation (MLE). However, fine-tuning LLMs on small downstream datasets can result in overfitting and overconfident predictions. 
Additionally, MLE yields a \emph{deterministic} model that cannot express uncertainty in the predicted $\piv$.  

\subsection{Uncertainty-aware modeling via EDL}
While conventional uncertainty-aware methods like a Deep Ensemble can alleviate overconfidence and improve calibration, they require multiple forward passes during inference. This can be particularly challenging for LLMs due to their already substantial computational demands. EDL provides a more efficient alternative by capturing uncertainty in \emph{a single forward pass}, making it especially suitable for large models. EDL builds on the principles of Subjective Logic~\citep{josang1997artificial,josang2016subjective}, which is derived from Dempster-Shafer Theory (DST)~\citep{dempster1968generalization,shafer1976mathematical}. 

% To illustrate, consider the task of predicting whether it will rain tomorrow. In traditional probability theory, a model might predict a $70\%$ chance of ``rain'' and a $30\%$ chance of ``no rain'' based on available data. However, if there is uncertainty about the reliability of the weather model or the data, SL allows this uncertainty to be expressed. For example, the model might allocate $50\%$ belief to ``rain'' and $20\%$ to ``no rain'', leaving the remaining $30\%$ unallocated, reflecting the uncertainty in the model or the data itself.

\textbf{EDL inference pipeline:} Instead of directly predicting the token probabilities $\piv$, EDL uses the model to predict a \emph{Dirichlet prior} on $\piv$. Specifically, the model's output is interpreted as \emph{pre-evidence} $\etv = f(\xv) \in \mathbb{R}^C$, which is converted into a non-negative \emph{evidence} vector $\ev = \text{SoftPlus}(\etv)$ using the SoftPlus activation. Each element $e_j$ of the evidence vector represents the amount of support for token $j$ being the correct prediction. Once the evidence is obtained, we can proceed to predict the Dirichlet prior $\Dir{\piv; \alv}$ over the simplex of possible token probabilities $\piv = [\pi_1, \ldots, \pi_C]^\top$ by computing the Dirichlet parameters $\alj = \ej + 1, \forall j \in [C]$. More formally,
\begin{equation}
    \begin{aligned}
        p(\piv | \alv) = \frac{\Gamma(\alpha_0)}{\prod_{j=1}^C \Gamma(\alj)} \prod_{j=1}^C \pij^{\alj - 1}, \quad \text{with }\alpha_0 = \sum_{j=1}^C \alj,
    \end{aligned}
\end{equation}
where $\Gamma(\cdot)$ is the \emph{gamma} function. 
The expected probabilities $\hat{\pi}_j$ and the final predicted token $\hat{y}$ are: 
\begin{equation}
    \begin{aligned}
        \hat{\pi}_j = \mathbb{E}_{\piv \sim \Dir{\piv ; \alv}} \left[\pij | \alv \right] = \frac{\alj}{\alpha_0} = \frac{\ej + 1}{\sum_{j=1}^C (\ej + 1)}, \quad \hat{y} = {\arg \max}_{j} \  \hat{\pi}_j.
    \end{aligned}
\end{equation}
In summary, the EDL pipeline can be symbolized as: $\xv \rightarrow f(\xv) \rightarrow \etv \rightarrow \ev \rightarrow \alv \rightarrow \piv \rightarrow \yv$.

\textbf{Uncertainty estimate:} EDL also enables quantifying uncertainty in the model's prediction. This is done through the concepts of \emph{belief mass} $b_j$ and \emph{uncertainty mass} $u$ in the Subjective Logic:
\begin{equation}
    \begin{aligned}
        \bj = (\alj - 1) / \alpha_0, \quad u = C / \alpha_0.
    \end{aligned}
\end{equation}
Similar to the evidence, the belief mass $\bj$ indicates the support for token $j$ being the correct prediction, while the uncertainty mass $u$ captures the model's overall uncertainty about the prediction. The sum of all belief masses and the uncertainty mass is normalized: $\sum_{j=1}^C \bj + u = 1$.

\subsection{Training of EDL networks}
In EDL, models are usually trained by minimizing the Bayes risk, which involves the expected loss under the Dirichlet distribution. Given the predicted $\alv$ from an input  $\xv$, and the target $\yv \in \{0, 1\}^C$, the Bayes risk for the cross-entropy loss is defined as $\lce = \mathbb{E}_{\piv \sim \Dir{\piv; \alv}}\left[ - \sum_{j=1}^C y_j \log(\pij)\right] = \sum_{j=1}^C y_j (\psi(\alz) - \psi(\alj))$,
where $\thetav$ denotes the trainable parameters of the model, and $\psi$ is the \emph{digamma} function. To stabilize the training, \citet{sensoy2018evidential} introduce the MSE loss as an alternative objective, which can be analytically computed using $\alv$:
\begin{equation}
    \begin{aligned}
        \lmse = \mathbb{E}_{\piv \sim \Dir{\piv; \alv}} \|\yv - \piv\|^2_2 = \sum_{j=1}^C \left(y_j - \frac{\alj}{\alz}\right)^2 + \frac{\alj (\alz - \alj)}{\alz^2 (\alj + 1)}.
    \end{aligned}
    \label{eq:loss_mse}
\end{equation}
For detailed derivations, we refer to~\citet{sensoy2018evidential}. Furthermore, they introduce a regularization term to \emph{suppress evidence for non-target tokens}, i.e., the tokens labeled as $\bm{0}$ in $\yv$. This is achieved by first ``removing'' the evidence associated with the target token, using $\altv = \yv + (\onev - \yv) \odot \alv$, where $\onev = [1, \ldots, 1]^\top$. The regularization term is then defined as
\begin{equation}
    \begin{aligned}
        \lreg &= \dkl{\Dir{\piv; \altv} \ \| \ \Dir{\piv; \onev}},
        % &= \log \left(\frac{\Gamma(\sum_{j=1}^C \altj)}{\Gamma(C) \prod_{j=1}^C \Gamma(\altj)}\right) + \sum_{j=1}^C (\altj - 1) \left[ \psi(\altj) - \psi\left(\sum_{j=1}^C \altj \right)\right],
    \end{aligned}
    \label{eq:loss_reg}
\end{equation}
where $D_{\text{KL}}$ denotes the Kullback-Leibler (KL) divergence. The total loss for training is given by: 
\begin{equation}
    \begin{aligned}
        \ledl = \lmse + \lambda \cdot \lreg,
    \end{aligned}
    \label{eq:loss_edl}
\end{equation}
where $\lambda > 0$ is a hyper-parameter. Note that $\lmse$ can be replaced with $\lce$. 

However, as the model is trained to minimize the empirical risk, it remains susceptible to overfitting the data and producing overconfident predictions. The objectives in $\lce$ and $\lmse$ drive the learned Dirichlet distribution towards a Dirac delta distribution. Consequently, the trained model may produce $\alpha_j$ with extreme magnitudes for the target token $j$~\citep{chen2024redl}. \cref{eq:loss_reg} also does not fully address this issue, as it only suppresses the evidence of non-target tokens. 

Recent efforts have sought to mitigate the overconfidence issue in EDL. For instance, I-EDL~\citep{deng2023uncertainty} incorporates the Fisher Information matrix into the distribution of $\yv$. R-EDL~\citep{chen2024redl} alleviates overconfidence by relaxing $\alj = \ej + 1, \forall j$ to $\alj = \ej + \eta, \forall j$ with a hyper-parameter $\eta \in \mathbb{R}_+$. Orthogonal to these approaches, we do not alter the assumptions on $\yv$ or $\alv$ in the EDL formulation. Instead, we impose regularization on the model using an information bottleneck (IB), which \emph{discourages} the model from relying on irrelevant or spurious correlations that could lead to an overly concentrated Dirichlet distribution, thereby preventing overconfident predictions.


