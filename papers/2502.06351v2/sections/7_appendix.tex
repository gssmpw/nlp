\section{Derivation of the variational bounds} \label{sec:app:derivation_of_bounds}

\textbf{Upper bound of $I(Z, X)$:} We reproduce the derivation steps from \citet{alemi2017deep} as follows:
\begin{equation}
    \begin{aligned}
         I(Z, X) &= \int p(\xv, \zv) \log \frac{p(\zv | \xv)}{p(\zv)} \dd \zv \dd \xv = \mathbb{E}_{p(\zv| \xv)} \mathbb{E}_{p(\xv)} [\log p(\zv | \xv)] - \mathbb{E}_{p(\zv)}[\log p(\zv)].
    \end{aligned}
    \label{eq:app:izx_1}
\end{equation}

Given a prior $r(\zv)$, we have $\dkl{p(\zv) \| r(\zv)} \geq 0$. This indicates that:
\begin{equation*}
    \begin{aligned}
        \dkl{p(\zv) \| r(\zv)} &= \int p(\zv) \log \frac{p(\zv)}{r(\zv)} \dd \zv \\
        &= \mathbb{E}_{p(\zv)} [\log p(\zv)] - \mathbb{E}_{p(\zv)} [\log r(\zv)] \geq 0. \\
    \end{aligned}
\end{equation*}
Therefore, 
\begin{equation}
    \begin{aligned}
        \mathbb{E}_{p(\zv)} [\log p(\zv)] \geq \mathbb{E}_{p(\zv)} [\log r(\zv)].
    \end{aligned}
    \label{eq:app:izx_2}
\end{equation}
Plugging \cref{eq:app:izx_2} into \cref{eq:app:izx_1}, we obtain
\begin{equation}
    \begin{aligned}
        I(Z, X) &= \mathbb{E}_{p(\zv| \xv)} \mathbb{E}_{p(\xv)} [\log p(\zv | \xv)] - \mathbb{E}_{p(\zv)}[\log p(\zv)] \\
        &\leq \mathbb{E}_{p(\zv| \xv)} \mathbb{E}_{p(\xv)} [\log p(\zv | \xv)] - \mathbb{E}_{p(\zv)}[\log r(\zv)] \\
        &\leq \mathbb{E}_{p(\zv| \xv)} \mathbb{E}_{p(\xv)} [\log p(\zv | \xv)] - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\zv |\xv)} \left[ \log r(\zv)\right] \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\zv | \xv)} \left[ \log \frac{p(\zv | \xv)}{r(\zv)} \right] \\
        &= \mathbb{E}_{p(\xv)} \left[ \dkl{p(\zv | \xv) \| r(\zv)} \right].
    \end{aligned}
    \label{eq:app:izx_3}
\end{equation}



\textbf{Lower bound of $I(Z, Y)$:} For clarity, we reproduce the derivation steps from \citet{wieczorek2020difference} here. Unlike \citet{alemi2017deep}, who assume the Markov chain $Z - X - Y$, \citet{wieczorek2020difference} assume the Markov chain $X - Z - Y$, implying the conditional independence $p(\yv | \zv) = p(\yv | \zv, \xv)$.
The detailed steps are as follows:

\begin{equation}
    \begin{aligned}
        I(Z,Y) & = \mathbb{E}_{p(\xv, \yv)}\mathbb{E}_{p(\zv | \xv , \yv)} [\log p(\yv | \zv)] + H(Y) \\
        & = \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \xv)} \mathbb{E}_{p(\zv | \xv, \yv)}[\log p (\yv | \zv, \xv)] + H(Y) \\
    \end{aligned}
    \label{eq:app:izy_1}
\end{equation}

Now, we derive a lower bound on the term $\mathbb{E}_{p(\yv | \xv)} \mathbb{E}_{p(\zv | \xv, \yv)} [\log p (\yv | \zv, \xv)]$ in \cref{eq:app:izy_1} as follows:

\begin{equation}
    \begin{aligned}
        &\mathbb{E}_{p(\yv | \xv)} \mathbb{E}_{p(\zv | \xv, \yv)} [\log p (\yv | \zv, \xv)] \\
        & = \int \int p(\zv, \yv | \xv) \log p (\zv, \yv | \xv) \dd \zv \dd \yv \\
        & = \int \int p(\zv, \yv | \xv) \log \frac{p(\yv | \xv) p(\zv, \yv | \xv)}{p(\yv | \xv) p(\zv | \xv)} \dd \yv \dd \zv \\
        &= \dkl{p(\yv, \zv | \xv) \| p(\yv | \xv) p(\zv | \xv)} + \int \int p(\zv, \yv | \xv) \log p(\yv | \xv) \dd \zv \dd \yv \\
        &= \dkl{p(\yv, \zv | \xv) \| p(\yv | \xv) p(\zv | \xv)} + \int p(\yv |\xv) \log p(\yv | \xv) \dd \yv \\
        &= \dkl{p(\yv, \zv | \xv) \| p(\yv | \xv) p(\zv | \xv)} + \int \int p(\yv | \xv) p (\zv | \xv) \log p(\zv | \xv) \dd \zv \dd \yv \\
        &= \dkl{p(\yv, \zv | \xv) \| p(\yv | \xv) p(\zv | \xv)} + \int \int p(\yv | \xv) p(\zv | \xv) \log \frac{p(\zv |\xv) p(\yv | \xv) p(\zv, \yv | \xv)}{p(\zv, \yv | \xv) p(\zv | \xv)} \dd \zv \dd \yv \\
        &= \dkl{p(\yv, \zv | \xv) \| p(\yv | \xv) p(\zv | \xv)} + \dkl{p(\yv|\xv) p(\zv | \xv) \| p(\yv, \zv|\xv)} \\
        & \qquad + \mathbb{E}_{p(\zv |\xv) p(\yv | \xv)} [\log p(\yv | \zv, \xv)] \\
        &\geq \mathbb{E}_{p(\zv | \xv) p(\yv | \xv)} [\log p(\yv | \zv, \xv)]. 
    \end{aligned}
    \label{eq:app:izy_2}
\end{equation}

Plugging \cref{eq:app:izy_2} into \cref{eq:app:izy_1} and using $p(\yv| \zv ) = p(\yv |\zv, \xv)$ again, we obtain: 

\begin{equation}
    \begin{aligned}
        I(Z, Y) &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \xv)} \mathbb{E}_{p(\zv | \xv, \yv)} [\log p (\yv | \zv, \xv)] + H(Y) \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \zv)} \mathbb{E}_{p(\zv | \xv)} [\log p(\yv | \zv) ]\\
        & \qquad + \mathbb{E}_{p(\xv)}\left[ \dkl{p(\yv, \zv | \xv) \| p(\yv | \xv) p(\zv| \xv)} \right] \\
        & \qquad + \mathbb{E}_{p(\xv)} \left[ \dkl{p(\yv | \xv) p(\zv | \xv) \| p(\yv, \zv | \xv)} \right] + H(Y) \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \zv)} \mathbb{E}_{p(\zv | \xv)} [\log p(\yv | \zv)]   \\
        & \qquad + I(Y, Z | X) + L(Y, Z | X) + H(Y) \\
        &\geq \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \xv)} \mathbb{E}_{p(\zv | \xv)} [\log p(\yv | \zv)]  + H(Y) \\
        &\geq \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \xv)} \mathbb{E}_{p(\zv | \xv)} [\log p(\yv | \zv)],
    \end{aligned}
\end{equation}
where $I(Y, Z | X)$ is the conditional mutual information, and $L(Y, Z | X)$ is the conditional lautum information~\citep{palomar2008lautum}, respectively.
\newpage
\section{Proof of Proposition 1} \label{sec:app:proof_proposition}

\textbf{A brief introduction to VB-based EDL methods:} Some previous EDL methods~\citep{chen2018variational, joo2020being} derive the optimization objective from the variational Bayes (VB) perspective, where the goal is to optimize the model's parameters $\thetav$ such that the \emph{posterior} distribution $p(\piv | \xv; \thetav)$ aligns with the \emph{true} posterior distribution $p(\piv | \yv)$.

For notation simplicity, we omit the model's parameters $\thetav$ in $p(\piv | \xv; \thetav)$ (or $p(\zv | \xv; \thetav)$) and use $p(\piv | \xv)$ (or $p(\zv | \xv)$) instead.

\begin{remark}
The condition in \cref{prop:vb_as_ib_edl} is that the latent variable $\zv = \piv$, and the prior $r(\zv) = r(\piv) = \Dir{\piv; \yv \odot \alv + (\onev - \yv)}$. In fact, the choice of the prior $r(\piv)$ is not unique. For example, \citet{chen2018variational} present three options. The correctness of \cref{prop:vb_as_ib_edl} remains unaffected by the choice of prior. \cref{prop:vb_as_ib_edl} uses one exemplary prior $r(\piv) = \Dir{\piv; \yv \odot \alv + (\onev - \yv)}$ suggested by \citet{chen2018variational}. 
\end{remark}

\begin{proof}
The target of VB-based EDL methods is:
\begin{equation}
    \begin{aligned}
        \min_{\thetav} \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \xv)} \left[ \dkl{p(\piv | \xv) \| p(\piv | \yv) }\right].
    \end{aligned}
    \label{eq:app:vb_objective_1}
\end{equation}
Next, we show that the IB objective in \cref{eq:loss_ib} is an upper bound of \cref{eq:app:vb_objective_1}  when $\zv = \piv$ and $r(\zv) = r(\piv)$ for a given prior $r(\piv)$, e.g., $r(\piv) = \Dir{\piv; \yv \odot \alv + (\onev - \yv)}$.
Minimizing the IB objective in \cref{eq:loss_ib} therefore provides a tractable way to approximate the minimization of \cref{eq:app:vb_objective_1}.

If we choose $\beta=1$, $\zv = \piv$ and a prior $r(\zv) = r(\piv)$, then the IB objective in \cref{eq:loss_ib} becomes
\begin{equation} \label{eq:temp_ib_obj}
    \begin{aligned}
        \min_{\thetav} - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \mathbb{E}_{p(\piv | \xv)} \left[ \log p(\yv | \piv) \right] + \mathbb{E}_{p(\xv)}\left[\dkl{p(\piv | \xv) \| r(\piv) }\right].
    \end{aligned}
\end{equation}

Expanding \cref{eq:temp_ib_obj}, we have:
\begin{equation}
    \begin{aligned}
        &\mathbb{E}_{p(\xv)} \left[\dkl{p(\piv | \xv) \| r(\piv) }\right] - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)}\mathbb{E}_{p(\piv | \xv)} \left[ \log p(\yv | \piv) \right] \\
        &= \mathbb{E}_{p(\xv, \yv)} \left[\dkl{p(\piv | \xv) \| r(\piv) }\right] - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)}\mathbb{E}_{p(\piv | \xv)}  \left[ \log p(\yv | \piv) \right] \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv)}{r(\piv)} \dd \piv \right] - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[\int p(\piv | \xv) \log p(\yv | \piv) \dd \piv \right] \\
        &\geq \underbrace{\mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv)}{p(\piv)} \dd \piv \right]}_{\text{Use} \ \dkl{p(\piv) \| r(\piv)} \geq 0 \ \text{(Similar to \cref{eq:app:izx_3})}} - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[\int p(\piv | \xv) \log p(\yv | \piv) \dd \piv \right] \\
        &\geq \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv)}{p(\piv)} \dd \piv \right] - \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[\int p(\piv | \xv) \log p(\yv | \piv) \dd \piv \right] \underbrace{- H(Y)}_{\leq 0} \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv)}{p(\yv | \piv) p(\piv) }\dd \piv\right] + \mathbb{E}_{p(\yv)} [\log p(\yv)] \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv)}{p(\yv | \piv) p(\piv) }\dd \piv\right] + \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[\log p(\yv) \right] \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv) p(\yv)}{p(\piv, \yv)} \dd \piv \right] \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv| \xv)} \left[ \int p(\piv | \xv) \log \frac{p(\piv | \xv)}{p(\piv | \yv)} \dd \piv \right] \\
        &= \mathbb{E}_{p(\xv)} \mathbb{E}_{p(\yv | \xv)} \left[ \dkl{p(\piv | \xv) \| p(\piv | \yv) }\right],
    \end{aligned}
    \label{eq:app:vb_objective_2}
\end{equation}

which is the target of VB-based EDL methods.
\end{proof}


\newpage

\section{Detailed implementation}\label{sec:app:implementation}

\textbf{Models:} We fine-tuned three models: Llama2-7B~\citep{touvron2023llama}, Llama3-8B~\citep{dubey2024llama3herdmodels}, and Mistral-7B-v0.1~\citep{jiang2023mistral7b}.

\textbf{LoRA hyperparameters:} We applied LoRA~\citep{hu2022lora} finetuning using the PEFT~\citep{mangrulkar2022peft} library. For all models, LoRA adaptors were applied to the \texttt{q\_proj}, \texttt{v\_proj}, and \texttt{lm\_head} modules. Additionally, we used Dropout with a dropout rate of $p = 0.1$, LoRA $\alpha = 16$, rank $r = 8$, and set \texttt{bias = "lora\_only"}.

\textbf{Training details:} For the MAP baseline and all EDL methods, all models were trained for $30000$ steps on the CSQA dataset and $10080$ steps on the other datasets. The learning rate was set to $0.00005$ and annealed using a cosine schedule. The maximum token length was set to $300$ for the RACE dataset and $256$ for all other datasets. Training was conducted with \texttt{bfloat16} precision. For MCD~\citep{gal2016dropout}, we performed $10$ forward passes. For Ens~\citep{lakshminarayanan2017simple,fort2019deep}, we used predictions from 3 models. For LA~\citep{yang2024lalora}, I-EDL~\citep{deng2023uncertainty}, and R-EDL~\citep{chen2024redl}, we used the original implementations and hyperparameters; please refer to the respective official codebases. For EDL methods, we follow the previous works to apply gradient clipping with maximal gradient norm of $20$ to stabilize the training.

\textbf{IB-EDL implementation details:} Both linear prediction heads (for $\muv$ and $\sigmav$) are initialized using the linear head from the pretrained LLM. Additionally, both heads are equipped with LoRA adapters and are jointly trained alongside the LoRA weights of the transformer encoder layers. By default, we used $K=20$ for sampling pre-evidences from the predicted Gaussian distribution during both training and inference. \cref{tab:beta_values} lists the $\beta$ values used in different experiments. Note that for OOD detection experiments, the parameters are identical to those used for OBQA, as OBQA is the training set for these experiments. We optimized $\beta$ values using grid search. We suggest a guideline for selecting $\beta$: if the MAP baseline shows lower accuracy and higher ECE, or in the presence of label noise, a larger $\beta$ may be chosen to encourage stronger compression and forgetting of the input. Otherwise, a smaller $\beta$ can be selected to allow more information from the input follow through the pre-evidences.

\begin{table}[t]
    \centering
    \caption{Loss weight $\beta$ for IB-EDL.}
    \label{tab:beta_values}
    \begin{tabular}{c | c c c c c c}
        \toprule
        \multirow{2}{*}{Model} & \multicolumn{6}{c}{ID Calibration} \\
        \cmidrule{2-7}
         & ARC-C & ARC-E & OBQA & CSQA & SciQ & RACE \\
        \midrule
        Llama2-7B & $1\times10^{-3}$ & $1\times10^{-4}$ & $1.5 \times 10^{-4}$ & $5 \times 10^{-5}$ & $1\times10^{-6}$ & $1\times10^{-6}$ \\ 
        Llama3-8B & $9\times 10^{-5}$ & $2\times 10^{-6}$ & $1 \times 10^{-5}$ & $3 \times 10^{-5}$ & $5\times 10^{-7}$ & $1\times10^{-6}$ \\
        Mistral-7B & $9\times10^{-5}$ & $1\times10^{-6}$ & $1\times10^{-5}$ & $5\times 10^{-5}$ & $4\times10^{-6}$ & $2\times10^{-6}$ \\
        \midrule
        \multirow{2}{*}{Model} & \multicolumn{6}{c}{Learning with Label Noise} \\
        \cmidrule{2-7}
         & ARC-C & ARC-E & OBQA & CSQA & SciQ & RACE \\
        \midrule
        Llama2-7B & $2\times 10^{-3}$ & $1\times10^{-4}$ & $5\times10^{-4}$ & $1.6\times10^{-4}$ & $5\times10^{-6}$ & $1\times10^{-6}$ \\
        Llama3-8B & $5\times10^{-6}$ & $2\times10^{-5}$ & $6\times10^{-5}$ & $3\times10^{-5}$ & $5\times10^{-6}$ & $2\times10^{-6}$ \\
        Mistral-7B & $1\times10^{-5}$ & $2\times10^{-5}$ & $9\times10^{-5}$ & $1\times10^{-5}$ & $1\times10^{-6}$ & $7\times10^{-6}$ \\
        \bottomrule
    \end{tabular}
\end{table}

\section{A post-hoc calibration technique for IB-EDL}
In this section, we introduce an additional \emph{post-hoc} calibration technique aimed at further refining the calibration of the IB-EDL finetuned model.

\textbf{Intuition:} The key motivation behind this calibration method is to account for an additional source of uncertainty in IB-EDL: the variance \(\sigma_j^2\) of the pre-evidences. In IB-EDL, the uncertainty mass and belief mass are normalized by the constraint:
$\sum_j b_j + u = 1$, where $\quad b_j = \frac{\alpha_j - 1}{\sum_j \alpha_j}$. To incorporate the additional uncertainty arising from the variance of pre-evidences, we consider the effect of increasing $u$, which leads to a reduction in $\sum_j b_j$. This suggests the need to adjust $\alpha_j$ by subtracting a term that is proportional to the standard deviation $\sigma_j$, which is predicted by IB-EDL. We parameterize this adjustment as $\zeta \cdot \sigma_j$, where $\zeta$ is a scalar hyperparameter. Consequently, we update $\alpha_j$ as follows: $\alpha_j \leftarrow \alpha_j - \zeta \cdot \sigma_j$. The intuition behind this update is that if the model exhibits high uncertainty in predicting $\alpha_j$ (i.e., predicting pre-evidences $\tilde{e}_j$), we enforce a more conservative belief representation by reducing $\alpha_j$ accordingly. This adjustment prevents excessively large values of $\alpha_j$ and ensures a more calibrated uncertainty estimation. As a result, the updated uncertainty mass is given by: $u = \frac{C}{\sum_j \alpha_j - \zeta \cdot \sum_j \sigma_j}$. This approach effectively integrates the variance of pre-evidences into the calibration process, leading to improved uncertainty quantification in IB-EDL.

\textbf{Determining the optimal value of $\zeta$:} Theoretically, the EDL model can still be overconfident or underconfident after training, leading to an overestimation or underestimation of the uncertainty mass $u$. Since $u$ may require adjustment in either direction, we allow the hyperparameter $\zeta \in \mathbb{R}$ to be either positive or negative, enabling calibration in both cases. To determine the optimal $\zeta$, we recommend to analyze the calibration curve~\citep{guo2017calibration} on the training or validation set, which plots accuracy against confidence for binned samples. If the calibration curve lies below the optimal diagonal line, it indicates that the model is overconfident. In this case, we set $\zeta > 0$ to encourage greater uncertainty and improve calibration. Conversely, if the calibration curve lies above the diagonal, the model is underconfident, and we set $\zeta < 0$ to increase certainty and correct for underconfidence.

\textbf{Values of $\zeta$ used in additional experiments:} It is important to note that this post-hoc calibration technique is an optional component of IB-EDL and is not required in all cases. In the experiments presented in the main text, we omit this technique. In the calibration experiment on OOD test sets presented in \cref{tab:ood_calibration_llama3_8b}, we use $\zeta = -1.0$ for OBQA $\rightarrow$ ARC-C, $\zeta = 3.0$ for OBQA $\rightarrow$ ARC-E, and $\zeta = -5.0$ for OBQA $\rightarrow$ CSQA.

% \input{tables/id_mistral_7b}
\begin{table}[t]
    \centering
    \caption{Calibration performance of uncertainty-aware methods on fine-tuned Mistral-7B.}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c| c | c c c c c c}
        \toprule
         Metrics & Method & ARC-C & ARC-E & OBQA & CSQA & SciQ & RACE\\
         \midrule 
         \multirow{9}{*}{Acc $\uparrow$}
            & MAP    & \ms{80.19}{0.68} & \ms{92.07}{0.12} & \ms{88.06}{0.61} & \ms{83.24}{1.47} & \ms{94.73}{0.06}[u] & \ms{86.61}{0.28} \\
            & MCD    & \ms{80.26}{0.52} & \ms{92.13}{0.15} & \ms{88.07}{0.61} & \ms{83.24}{1.47} & \ms{94.73}{0.06}[u] & \ms{86.64}{0.22}[u] \\ 
            & Ens    & \ms{80.48}{0.45} & \ms{92.41}{0.12}[u] & \ms{88.51}{0.18}[u] & \ms{83.41}{1.44}[u] & \ms{94.80}{0.10}[b] & \ms{86.81}{0.21}[b] \\
            & LA     & \ms{78.47}{1.10} & \ms{92.12}{0.11} & \ms{88.19}{0.69} & \ms{83.21}{1.01} & \ms{94.66}{0.16} & \ms{86.34}{0.39} \\
            & EDL    & \ms{80.60}{0.47}[b] & \ms{92.27}{0.44} & \ms{87.23}{1.42} & \ms{83.31}{0.34} & \ms{94.27}{0.23} & \ms{85.70}{0.41} \\
            & VID    & \ms{80.37}{0.39} & \ms{91.85}{0.13} & \ms{88.33}{0.81} & \ms{82.52}{0.54} & \ms{94.17}{0.12} & \ms{86.40}{0.21} \\
            & I-EDL  & \ms{80.46}{1.63} & \ms{92.28}{0.15} & \ms{88.06}{0.31} & \ms{82.91}{0.20} & \ms{94.03}{0.32} & \ms{85.11}{0.61} \\
            & R-EDL  & \ms{80.12}{0.97} & \ms{92.20}{0.15} & \ms{88.33}{0.91} & \ms{83.07}{0.83} & \ms{94.03}{0.31} & \ms{86.07}{0.28} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{80.57}{0.68}[u] & \ms{92.47}{0.12}[b] & \ms{88.73}{0.70}[b] & \ms{83.65}{0.45}[b] & \ms{94.40}{0.26} & \ms{86.40}{0.32} \\
        \midrule
        \multirow{9}{*}{ECE $\downarrow$}
            & MAP    & \ms{19.42}{0.68} & \ms{7.63}{0.16} & \ms{11.29}{0.31} & \ms{15.72}{1.46} & \ms{4.98}{0.06} & \ms{8.43}{0.46} \\
            & MCD    & \ms{19.26}{0.52} & \ms{7.62}{0.16} & \ms{11.22}{0.40} & \ms{15.72}{1.46} & \ms{4.98}{0.06} & \ms{8.42}{0.47} \\ 
            & Ens    & \ms{17.04}{1.58} & \ms{7.03}{0.79} & \ms{8.87}{0.90}  & \ms{14.18}{2.52} & \ms{4.78}{0.13} & \ms{8.21}{0.62} \\
            & LA     & \ms{20.04}{0.62} & \ms{1.57}{0.39}[b] & \ms{4.49}{0.17}[u]  & \ms{15.11}{2.95} & \ms{1.91}{0.46}[b] & \ms{2.94}{0.25}[b] \\
            & EDL    & \ms{6.21}{1.18}  & \ms{6.25}{0.46} & \ms{6.23}{0.56}  & \ms{7.71}{0.86}  & \ms{7.64}{0.85} & \ms{7.75}{0.86} \\
            & VID    & \ms{9.38}{0.46}  & \ms{2.44}{0.16}[u] & \ms{4.88}{0.77}  & \ms{7.32}{0.74}  & \ms{5.62}{0.52} & \ms{4.20}{0.13} \\
            & I-EDL  & \ms{4.70}{2.15}[u]  & \ms{10.94}{1.27} & \ms{9.63}{0.75} & \ms{8.85}{0.44}  & \ms{11.00}{0.30} & \ms{13.16}{0.87} \\
            & R-EDL  & \ms{11.26}{0.18} & \ms{2.86}{1.07} & \ms{5.43}{0.65}  & \ms{6.30}{0.65}[u]  & \ms{4.48}{0.23}[u] & \ms{3.69}{0.52} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{3.60}{1.10}[b]  & \ms{3.49}{1.24}  & \ms{2.27}{0.67}[b]  & \ms{6.02}{0.08}[b]  & \ms{4.99}{1.22} & \ms{3.58}{0.22}[u] \\
        \midrule
        \multirow{9}{*}{NLL $\downarrow$}
            & MAP    & \ms{2.18}{0.14}  & \ms{0.85}{0.03}  & \ms{0.85}{0.02}  & \ms{1.18}{0.06}  & \ms{0.31}{0.01} & \ms{0.52}{0.01} \\
            & MCD    & \ms{2.18}{0.13}  & \ms{0.84}{0.04}  & \ms{0.84}{0.03}  & \ms{1.18}{0.07}  & \ms{0.31}{0.01} & \ms{0.52}{0.02} \\ 
            & Ens    & \ms{1.82}{0.17}  & \ms{0.78}{0.10}  & \ms{0.67}{0.04}  & \ms{1.01}{0.13}  & \ms{0.28}{0.01} & \ms{0.51}{0.03} \\
            & LA     & \ms{0.78}{0.02}  & \ms{0.29}{0.01}[b]  & \ms{0.38}{0.02}[b]  & \ms{0.59}{0.03}[b]  & \ms{0.17}{0.01}[b] & \ms{0.45}{0.03} \\
            & EDL    & \ms{0.71}{0.04}[u]  & \ms{0.35}{0.02}  & \ms{0.47}{0.03}  & \ms{0.63}{0.01}  & \ms{0.27}{0.01} & \ms{0.47}{0.02} \\
            & VID    & \ms{0.76}{0.03}  & \ms{0.36}{0.01}  & \ms{0.46}{0.01}  & \ms{0.68}{0.02}  & \ms{0.24}{0.01} & \ms{0.41}{0.01}[u] \\
            & I-EDL  & \ms{0.71}{0.05}[u]  & \ms{0.38}{0.02}  & \ms{0.45}{0.01}  & \ms{0.65}{0.01}  & \ms{0.29}{0.01} & \ms{0.50}{0.01} \\
            & R-EDL  & \ms{0.77}{0.02}  & \ms{0.33}{0.01}  & \ms{0.41}{0.03}[u]  & \ms{0.64}{0.01}  & \ms{0.24}{0.01} & \ms{0.45}{0.01} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{0.70}{0.01} [b] & \ms{0.32}{0.02}[u]  & \ms{0.41}{0.01}[u]  & \ms{0.61}{0.01}[u]  & \ms{0.23}{0.01}[u] & \ms{0.40}{0.01}[b] \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:id_mistral_7b}
\end{table}


% \input{tables/ood_mistral_7b}
\begin{table}[t]
    \centering
    \caption{OOD detection performance on fine-tuned Mistral-7B. $A \rightarrow B$ indicates $A$ as the ID training set and $B$ as the OOD test set. MP and UM are two scores for measuring AUROC.}
    \resizebox{0.95\textwidth}{!}{
    \begin{tabular}{c | c | c c | c c | c c}
        \toprule
        \multirow{3}{*}{Model} & \multirow{3}{*}{Method} & \multicolumn{2}{c|}{OBQA $\rightarrow$ ARC-C} & \multicolumn{2}{c|}{OBQA $\rightarrow$ ARC-E} & \multicolumn{2}{c}{OBQA $\rightarrow$ CSQA} \\
        \cmidrule{3-8} 
         & & \multicolumn{2}{c|}{AUROC $\uparrow$} & \multicolumn{2}{c|}{AUROC $\uparrow$} & \multicolumn{2}{c}{AUROC $\uparrow$} \\
        \cmidrule{3-8}
         & & MP & UM & MP & UM & MP & UM \\
        \midrule
        \multirow{9}{*}{Mistral-7B} & MAP    & \ms{60.40}{1.36} & $-$ & \ms{53.30}{1.16} & $-$ & \ms{63.70}{1.10} & $-$ \\
        & MCD    & \ms{60.39}{1.37} & $-$ & \ms{53.30}{1.16} & $-$ & \ms{63.70}{1.10} & $-$ \\
        & Ens    & \ms{60.67}{0.87} & $-$ & \ms{54.05}{0.88} & $-$ & \ms{63.80}{1.09} & $-$ \\
        & LA     & \ms{61.61}{1.05} & $-$ & \ms{53.14}{0.93} & $-$ & \ms{68.31}{1.11} & $-$ \\
        & EDL    & \ms{84.17}{1.87} & \ms{77.34}{6.97} & \ms{81.81}{2.31} & \ms{74.18}{6.38} & \ms{83.55}{2.67} & \ms{78.28}{5.81} \\
        & VID    & \ms{86.30}{3.85}[u] & \ms{84.74}{0.89}[u] & \ms{88.16}{1.87}[u] & \ms{90.68}{0.17}[u] & \ms{88.93}{1.39}[u] & \ms{81.45}{2.04} \\
        & I-EDL  & \ms{85.02}{0.90} & \ms{82.28}{1.44} & \ms{82.67}{1.34} & \ms{79.42}{2.33} & \ms{85.08}{0.87} & \ms{82.07}{1.43}[u] \\
        & R-EDL  & \ms{76.26}{1.05} & \ms{72.85}{0.61} & \ms{71.95}{0.78} & \ms{67.56}{0.72} & \ms{75.59}{1.32} & \ms{71.93}{2.05} \\
        \cmidrule{2-8}
        & IB-EDL & \ms{90.28}{1.22}[b] & \ms{88.53}{1.08}[b] & \ms{89.54}{1.16}[b] & \ms{94.29}{0.49}[b] & \ms{90.45}{1.13}[b] & \ms{83.85}{2.55}[b] \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:ood_mistral_7b}
\end{table}

\section{Additional Experimental Results}

\subsection{ID calibration and OOD detection results of Mistral-7B}~\label{sec:app:mistral_7b_results}

In this subsection, we present the ID calibration and OOD detection performance of the fine-tuned Mistral-7B model. \cref{tab:id_mistral_7b} provides an overview of the calibration performance on ID datasets, while \cref{tab:ood_mistral_7b} reports the OOD detection results.

\subsection{Additional results of fine-tuning models on noisy datasets}~\label{sec:app:full_noisy_ft_results}

\cref{tab:full_noisy_llama2_7b} and \cref{tab:full_noisy_llama3_8b}, and \cref{tab:full_noisy_mistral_7b} show the results of fine-tuning Llama2-7B, Llama3-8B, and Mistral-7B on noisy datasets, respectively.

\textbf{Results:} In the context of noisy data, the primary objective is to preserve accuracy. Therefore, in \cref{sec:exp_ft_noisy_labels} of the main text, we focus primarily on accuracy results. As highlighted in \cref{sec:exp_id_calibration}, a good uncertainty-aware method should achieve comparable accuracy while maintaining a low calibration error, a position shared by many other current research efforts \citep[see, e.g.,][]{chen2024redl}. %express a similar view in their work. 
Here, we thus provide additional results for the ECE and NLL metrics.
In the presence of label noise, Non-EDL methods tend to underperform EDL methods significantly in terms of accuracy. In the comparisons of \cref{tab:full_noisy_llama2_7b,tab:full_noisy_llama3_8b,tab:full_noisy_mistral_7b}, we therefore focus on the comparison of EDL methods, which manage to strike the right balance of good accuracy and uncertainty quantification. Among these, IB-EDL demonstrates the highest accuracy and achieves the lowest ECE and NLL compared to other EDL approaches, highlighting its robustness against label noise.

% \input{tables/full_noisy_llama2_7b}
\begin{table}[t]
    \centering
    \caption{Fine-tuning Llama2-7B on noisy datasets. In each training dataset, the labels of $30\%$ samples are randomly perturbed. A robust uncertainty-aware method should not only maintain accuracy but also exhibit low calibration error. Non-EDL methods tend to significantly underperform EDL methods in terms of Accuracy. Therefore, the comparison for ECE and NLL is limited to EDL methods, with the best and second-best values among them highlighted.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c| c | c c c c c c}
        \toprule
         Metrics & Method & ARC-C & ARC-E & OBQA & CSQA & SciQ & RACE\\
         \midrule 
         \multirow{9}{*}{Acc $\uparrow$}
            & MAP    & \ms{51.08}{2.96} & \ms{71.39}{2.73} & \ms{73.93}{2.21} & \ms{74.56}{0.31} & \ms{88.63}{0.06} & \ms{76.83}{0.39} \\
            & MCD    & \ms{51.08}{2.96} & \ms{71.40}{2.72} & \ms{73.93}{2.20} & \ms{74.56}{0.32} & \ms{88.64}{0.06} & \ms{76.95}{0.19} \\ 
            & Ens    & \ms{56.10}{3.11} & \ms{76.03}{1.99} & \ms{75.23}{1.06} & \ms{74.59}{0.23} & \ms{88.64}{0.06} & \ms{76.94}{0.22} \\
            & LA     & \ms{53.38}{2.17} & \ms{74.90}{1.76} & \ms{74.57}{2.22} & \ms{74.59}{0.20} & \ms{88.47}{0.21} & \ms{77.04}{0.39} \\
            \cmidrule{2-8}
            & EDL    & \ms{57.16}{3.33} & \ms{76.16}{1.12} & \ms{74.46}{1.33} & \ms{74.65}{0.05} & \ms{89.03}{0.25} & \ms{77.69}{0.44} \\
            & VID    & \ms{57.56}{0.58}[u] & \ms{76.57}{1.79} & \ms{76.67}{1.42} & \ms{75.97}{0.66}[u] & \ms{89.06}{0.21}[u] & \ms{78.75}{0.25}[u] \\
            & I-EDL  & \ms{53.86}{1.25} & \ms{76.08}{0.71} & \ms{77.00}{0.88}[u] & \ms{74.01}{1.28} & \ms{89.26}{0.32}[b] & \ms{76.48}{0.87} \\
            & R-EDL  & \ms{57.00}{0.76} & \ms{76.96}{1.07}[u] & \ms{73.20}{1.44} & \ms{74.36}{0.49} & \ms{87.33}{0.49} & \ms{78.01}{0.53} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{59.06}{2.07}[b] & \ms{80.27}{0.48}[b] & \ms{78.13}{0.99}[b] & \ms{76.35}{0.66}[b] & \ms{89.06}{0.50}[u] & \ms{79.41}{0.59}[b] \\
        \midrule
        \multirow{9}{*}{ECE $\downarrow$}
            & MAP    & \ms{21.89}{0.11} & \ms{8.95}{1.04} & \ms{8.21}{0.78} & \ms{4.45}{1.51} & \ms{19.05}{0.17} & \ms{15.87}{0.30} \\
            & MCD    & \ms{21.87}{0.12} & \ms{8.91}{1.10} & \ms{8.23}{0.82} & \ms{4.47}{1.55} & \ms{19.04}{0.16} & \ms{15.84}{0.26} \\ 
            & Ens    & \ms{8.24}{1.18}  & \ms{7.83}{0.28} & \ms{11.63}{1.12} & \ms{4.42}{1.52} & \ms{18.91}{0.12} & \ms{15.90}{0.94} \\
            & LA     & \ms{7.12}{1.06}  & \ms{27.02}{1.39} & \ms{22.06}{2.11} & \ms{22.57}{0.09} & \ms{26.28}{0.04} & \ms{24.70}{0.24} \\
            \cmidrule{2-8}
            & EDL    & \ms{8.38}{1.91}[b]  & \ms{26.16}{4.44} & \ms{35.64}{2.97} & \ms{44.29}{2.42} & \ms{46.47}{0.24} & \ms{30.59}{1.14} \\
            & VID    & \ms{19.14}{0.99} & \ms{13.28}{1.84}[b] & \ms{15.00}{0.98}[u] & \ms{13.76}{0.90}[u] & \ms{23.49}{0.34}[u] & \ms{21.22}{0.63}[u] \\
            & I-EDL  & \ms{13.97}{9.04} & \ms{35.30}{1.74} & \ms{37.56}{0.53} & \ms{38.77}{1.59} & \ms{48.60}{0.37} & \ms{36.34}{1.09} \\
            & R-EDL  & \ms{12.25}{1.18} & \ms{20.67}{2.30} & \ms{28.02}{2.63} & \ms{33.64}{0.29} & \ms{39.58}{0.56} & \ms{30.42}{0.89} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{11.21}{2.27}[u] & \ms{13.48}{1.61}[u] & \ms{12.30}{1.65}[b] & \ms{10.48}{0.82}[b] & \ms{23.47}{0.31}[b] & \ms{21.15}{0.36}[b] \\
        \midrule
        \multirow{9}{*}{NLL $\downarrow$}
            & MAP    & \ms{1.94}{0.06} & \ms{1.05}{0.09} & \ms{0.74}{0.03} & \ms{0.84}{0.02} & \ms{0.50}{0.01} & \ms{0.73}{0.00} \\
            & MCD    & \ms{1.94}{0.05} & \ms{1.05}{0.09} & \ms{0.74}{0.03} & \ms{0.81}{0.02} & \ms{0.49}{0.01} & \ms{0.72}{0.02} \\ 
            & Ens    & \ms{1.37}{0.21} & \ms{0.73}{0.06} & \ms{0.73}{0.03} & \ms{0.83}{0.01}[u] & \ms{0.49}{0.01} & \ms{0.73}{0.01} \\
            & LA     & \ms{1.32}{0.06} & \ms{1.03}{0.03} & \ms{0.85}{0.00}[u] & \ms{0.92}{0.01} & \ms{0.56}{0.00} & \ms{0.81}{0.00} \\
            \cmidrule{2-8}
            & EDL    & \ms{1.19}{0.03}[u] & \ms{0.97}{0.03} & \ms{1.05}{0.02} & \ms{1.35}{0.11} & \ms{0.92}{0.01} & \ms{0.90}{0.03} \\
            & VID    & \ms{1.27}{0.01} & \ms{0.83}{0.03} & \ms{0.76}{0.03}[u] & \ms{0.85}{0.01} & \ms{0.54}{0.01}[u] & \ms{0.71}{0.01}[b] \\
            & I-EDL  & \ms{1.29}{0.12} & \ms{1.09}{0.02} & \ms{1.03}{0.01} & \ms{1.20}{0.01} & \ms{0.95}{0.00} & \ms{1.02}{0.01} \\
            & R-EDL  & \ms{1.24}{0.01} & \ms{0.92}{0.01} & \ms{0.95}{0.03} & \ms{1.13}{0.01} & \ms{0.83}{0.01} & \ms{0.93}{0.01} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{1.18}{0.03}[b] & \ms{0.74}{0.03}[b] & \ms{0.71}{0.03}[b] & \ms{0.81}{0.01}[b] & \ms{0.52}{0.01}[b] & \ms{0.71}{0.01}[b] \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:full_noisy_llama2_7b}
\end{table}

% \input{tables/full_noisy_llama3_8b}
\begin{table}[t]
    \centering
    \caption{Fine-tuning Llama3-8B on noisy datasets. In each training dataset, the labels of $30\%$ samples are randomly perturbed. A robust uncertainty-aware method should not only maintain accuracy but also exhibit low calibration error. Non-EDL methods tend to significantly underperform EDL methods in terms of Accuracy. Therefore, the comparison for ECE and NLL is limited to EDL methods, with the best and second-best values among them highlighted.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c| c | c c c c c c}
        \toprule
         Metrics & Method & ARC-C & ARC-E & OBQA & CSQA & SciQ & RACE\\
         \midrule 
         \multirow{9}{*}{Acc $\uparrow$}
            & MAP    & \ms{57.71}{0.42} & \ms{80.34}{1.47} & \ms{78.78}{1.00} & \ms{77.04}{0.41} & \ms{92.60}{0.53} & \ms{86.93}{0.11} \\
            & MCD    & \ms{57.71}{0.42} & \ms{80.37}{1.46} & \ms{79.00}{0.92} & \ms{77.05}{0.41} & \ms{92.87}{0.12} & \ms{86.93}{0.12} \\ 
            & Ens    & \ms{63.39}{1.09} & \ms{84.63}{0.53} & \ms{80.61}{0.53} & \ms{77.62}{0.41} & \ms{92.97}{0.06} & \ms{86.93}{0.07} \\
            & LA     & \ms{66.62}{1.08} & \ms{82.64}{0.50} & \ms{79.59}{0.72} & \ms{77.80}{0.49} & \ms{92.93}{0.21} & \ms{87.00}{0.01} \\
            \cmidrule{2-8}
            & EDL    & \ms{69.43}{0.98}[b] & \ms{87.57}{0.13} & \ms{85.60}{0.72} & \ms{79.14}{0.41} & \ms{92.90}{0.40} & \ms{86.26}{0.76} \\
            & VID    & \ms{66.58}{1.92} & \ms{87.67}{0.99}[u] & \ms{84.86}{1.01} & \ms{79.66}{1.26}[b] & \ms{93.23}{0.25} & \ms{86.86}{0.26}[u] \\
            & I-EDL  & \ms{63.87}{2.65} & \ms{84.06}{2.80} & \ms{84.26}{0.42} & \ms{78.02}{0.87} & \ms{92.53}{0.37} & \ms{86.01}{0.51} \\
            & R-EDL  & \ms{71.25}{1.20} & \ms{84.91}{3.91} & \ms{85.73}{0.70}[u] & \ms{78.57}{0.21} & \ms{93.56}{0.05}[b] & \ms{86.16}{0.42} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{68.53}{0.25}[u] & \ms{88.05}{0.43}[b] & \ms{86.13}{0.51}[b] & \ms{79.59}{0.79}[u] & \ms{93.46}{0.38}[u] & \ms{87.01}{0.20}[b] \\
        \midrule
        \multirow{9}{*}{ECE $\downarrow$}
            & MAP    & \ms{13.26}{0.85} & \ms{8.52}{0.63} & \ms{3.72}{1.06} & \ms{8.43}{1.04} & \ms{16.29}{0.43} & \ms{19.61}{0.25} \\
            & MCD    & \ms{12.96}{1.15} & \ms{8.51}{0.62} & \ms{4.17}{1.45} & \ms{8.40}{1.07} & \ms{16.29}{0.43} & \ms{19.64}{0.26} \\ 
            & Ens    & \ms{10.41}{0.74} & \ms{11.09}{0.59} & \ms{4.24}{1.91} & \ms{8.21}{0.21} & \ms{16.15}{0.16} & \ms{19.69}{0.23} \\
            & LA     & \ms{19.02}{0.78} & \ms{23.63}{1.74} & \ms{11.61}{0.29} & \ms{18.05}{0.86} & \ms{18.36}{0.37} & \ms{20.99}{0.18} \\
            \cmidrule{2-8}
            & EDL    & \ms{11.01}{0.79} & \ms{24.29}{0.81} & \ms{41.29}{0.62} & \ms{34.19}{1.93} & \ms{48.36}{0.43} & \ms{34.92}{0.46} \\
            & VID    & \ms{5.90}{0.97}[b]  & \ms{19.58}{0.29}[u] & \ms{19.53}{0.77}[u] & \ms{15.22}{1.15}[u] & \ms{26.30}{0.22}[u] & \ms{22.25}{0.25}[b] \\
            & I-EDL  & \ms{9.57}{2.10}  & \ms{33.57}{6.96} & \ms{42.64}{0.36} & \ms{41.89}{1.18} & \ms{50.13}{0.31} & \ms{42.49}{0.20} \\
            & R-EDL  & \ms{14.50}{2.55} & \ms{23.91}{2.93} & \ms{35.09}{0.84} & \ms{30.95}{2.78} & \ms{41.89}{0.15} & \ms{30.74}{0.94} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{6.73}{1.30}[u]  & \ms{19.49}{0.38}[b] & \ms{17.38}{0.52}[b] & \ms{11.73}{1.11}[b] & \ms{25.00}{0.33}[b] & \ms{23.43}{0.32}[u] \\
        \midrule
        \multirow{9}{*}{NLL $\downarrow$}
            & MAP    & \ms{1.39}{0.05}  & \ms{0.78}{0.03}  & \ms{0.64}{0.01}  & \ms{0.83}{0.01}  & \ms{0.39}{0.01} & \ms{0.56}{0.01} \\
            & MCD    & \ms{1.38}{0.02}  & \ms{0.78}{0.03}  & \ms{0.64}{0.03}  & \ms{0.82}{0.02}  & \ms{0.40}{0.01} & \ms{0.57}{0.01} \\ 
            & Ens    & \ms{1.15}{0.05}  & \ms{0.64}{0.03}  & \ms{0.61}{0.01}  & \ms{0.78}{0.02}  & \ms{0.39}{0.01} & \ms{0.57}{0.02} \\
            & LA     & \ms{1.11}{0.01}  & \ms{0.80}{0.02}  & \ms{0.63}{0.02}  & \ms{0.85}{0.01}  & \ms{0.39}{0.00} & \ms{0.57}{0.01} \\
            \cmidrule{2-8}
            & EDL    & \ms{0.94}{0.02}[b]  & \ms{0.70}{0.01}  & \ms{0.92}{0.01}  & \ms{1.04}{0.03}  & \ms{0.85}{0.00} & \ms{0.78}{0.01} \\
            & VID    & \ms{0.98}{0.03}  & \ms{0.62}{0.02}[u]  & \ms{0.63}{0.01}[u]  & \ms{0.80}{0.02}[u]  & \ms{0.49}{0.01}[u] & \ms{0.57}{0.01}[b] \\
            & I-EDL  & \ms{1.03}{0.03}  & \ms{0.87}{0.06}  & \ms{0.96}{0.01}  & \ms{1.16}{0.00}  & \ms{0.89}{0.01} & \ms{0.91}{0.00} \\
            & R-EDL  & \ms{1.00}{0.02}  & \ms{0.76}{0.05}  & \ms{0.83}{0.02}  & \ms{0.99}{0.02}  & \ms{0.72}{0.00} & \ms{0.73}{0.01} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{0.99}{0.02}[u]  & \ms{0.60}{0.01}[b]  & \ms{0.57}{0.01}[b]  & \ms{0.74}{0.02}[b]  & \ms{0.46}{0.01}[b] & \ms{0.57}{0.01}[b] \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:full_noisy_llama3_8b}
\end{table}

% \input{tables/full_noisy_mistral_7b}
\begin{table}[t]
    \centering
    \caption{Fine-tuning Mistral-7B on noisy datasets. In each training dataset, the labels of $30\%$ samples are randomly perturbed. A robust uncertainty-aware method should not only maintain accuracy but also exhibit low calibration error. Non-EDL methods tend to significantly underperform EDL methods in terms of Accuracy. Therefore, the comparison for ECE and NLL is limited to EDL methods, with the best and second-best values among them highlighted.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c| c | c c c c c c}
        \toprule
         Metrics & Method & ARC-C & ARC-E & OBQA & CSQA & SciQ & RACE\\
         \midrule 
         \multirow{9}{*}{Acc $\uparrow$}
            & MAP    & \ms{50.65}{0.30} & \ms{66.30}{2.16} & \ms{75.72}{0.46} & \ms{69.81}{0.55} & \ms{92.96}{0.32} & \ms{85.47}{0.35} \\
            & MCD    & \ms{50.65}{0.30} & \ms{66.21}{2.04} & \ms{75.46}{0.24} & \ms{69.77}{0.51} & \ms{92.93}{0.31} & \ms{85.46}{0.35} \\ 
            & Ens    & \ms{58.16}{0.47} & \ms{76.10}{0.86} & \ms{76.93}{2.14} & \ms{74.18}{1.50} & \ms{92.93}{0.39} & \ms{85.62}{0.28} \\
            & LA     & \ms{66.35}{0.63} & \ms{75.48}{1.88} & \ms{78.20}{1.25} & \ms{74.24}{1.01} & \ms{93.02}{0.29} & \ms{85.57}{0.06} \\
            \cmidrule{2-8}
            & EDL    & \ms{61.63}{1.80} & \ms{85.01}{0.23} & \ms{83.20}{1.25}[u] & \ms{76.63}{0.98} & \ms{93.33}{0.06}[u] & \ms{84.91}{0.10} \\
            & VID    & \ms{62.27}{2.18} & \ms{76.98}{1.13} & \ms{82.06}{1.51} & \ms{77.31}{1.01} & \ms{93.20}{0.17} & \ms{85.59}{0.06}[u] \\
            & I-EDL  & \ms{68.28}{1.19}[u] & \ms{79.73}{0.57} & \ms{81.06}{0.81} & \ms{77.41}{0.09}[u] & \ms{93.33}{0.31}[u] & \ms{84.44}{0.28} \\
            & R-EDL  & \ms{76.39}{0.60}[b] & \ms{86.85}{0.69}[b] & \ms{82.76}{1.73} & \ms{77.10}{1.10} & \ms{93.09}{0.78} & \ms{85.07}{0.34} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{66.44}{0.80}[u] & \ms{85.17}{1.39}[u] & \ms{84.33}{0.51}[b] & \ms{77.44}{0.39}[b] & \ms{93.63}{0.21}[b] & \ms{85.68}{0.20}[b] \\
        \midrule
        \multirow{9}{*}{ECE $\downarrow$}
            & MAP    & \ms{18.08}{0.45} & \ms{9.95}{1.18} & \ms{7.83}{0.62} & \ms{9.61}{0.98} & \ms{15.11}{0.28} & \ms{17.80}{0.33} \\
            & MCD    & \ms{18.24}{0.65} & \ms{9.94}{1.18} & \ms{7.82}{0.61} & \ms{9.60}{0.99} & \ms{15.10}{0.27} & \ms{17.80}{0.33} \\ 
            & Ens    & \ms{10.94}{0.21} & \ms{12.40}{0.34} & \ms{7.02}{0.61} & \ms{8.72}{0.29} & \ms{15.09}{0.17} & \ms{18.58}{0.46} \\
            & LA     & \ms{18.50}{1.46} & \ms{17.31}{0.23} & \ms{13.92}{1.45} & \ms{15.04}{0.41} & \ms{18.13}{0.19} & \ms{18.32}{0.59} \\
            \cmidrule{2-8}
            & EDL    & \ms{12.76}{0.41} & \ms{19.84}{0.95} & \ms{32.57}{1.89} & \ms{25.26}{5.63} & \ms{48.55}{0.33} & \ms{34.06}{0.14} \\
            & VID    & \ms{6.27}{0.66}[b]  & \ms{13.17}{0.76}[b] & \ms{13.92}{0.97}[b] & \ms{11.66}{1.17}[u] & \ms{26.45}{0.16}[u] & \ms{22.01}{0.08}[b] \\
            & I-EDL  & \ms{13.14}{1.17} & \ms{21.44}{1.54} & \ms{35.10}{1.07} & \ms{33.29}{1.04} & \ms{50.90}{0.44} & \ms{41.19}{0.27} \\
            & R-EDL  & \ms{17.63}{2.83} & \ms{17.57}{0.42}[u] & \ms{18.55}{1.68} & \ms{21.35}{1.24} & \ms{41.48}{0.26} & \ms{29.40}{0.31} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{7.46}{1.58}[u]  & \ms{19.08}{3.68} & \ms{14.90}{0.50}[u] & \ms{11.58}{0.43}[b] & \ms{25.08}{0.27}[b] & \ms{22.04}{0.06}[u] \\
        \midrule
        \multirow{9}{*}{NLL $\downarrow$}
            & MAP    & \ms{1.67}{0.03}  & \ms{0.98}{0.06}  & \ms{0.73}{0.05}  & \ms{0.98}{0.03}  & \ms{0.38}{0.01} & \ms{0.56}{0.00} \\
            & MCD    & \ms{1.67}{0.03}  & \ms{0.98}{0.06}  & \ms{0.70}{0.03}  & \ms{0.98}{0.03}  & \ms{0.38}{0.01} & \ms{0.56}{0.01} \\ 
            & Ens    & \ms{1.32}{0.01}  & \ms{0.73}{0.03}  & \ms{0.64}{0.04}  & \ms{0.81}{0.02}  & \ms{0.39}{0.01} & \ms{0.57}{0.02} \\
            & LA     & \ms{1.08}{0.03}  & \ms{0.83}{0.03}  & \ms{0.70}{0.03}  & \ms{0.93}{0.01}  & \ms{0.40}{0.01} & \ms{0.56}{0.00} \\
            \cmidrule{2-8}
            & EDL    & \ms{0.93}{0.05}[b]  & \ms{0.67}{0.00}[u]  & \ms{0.82}{0.03}  & \ms{0.99}{0.04}  & \ms{0.85}{0.00} & \ms{0.78}{0.01} \\
            & VID    & \ms{1.04}{0.06}  & \ms{0.74}{0.02}  & \ms{0.63}{0.01}[u]  & \ms{0.84}{0.01}[u]  & \ms{0.50}{0.01}[u] & \ms{0.60}{0.01}[b] \\
            & I-EDL  & \ms{0.96}{0.04}  & \ms{0.76}{0.02}  & \ms{0.90}{0.01}  & \ms{1.05}{0.02}  & \ms{0.89}{0.00} & \ms{0.91}{0.01} \\
            & R-EDL  & \ms{0.93}{0.03}[b]  & \ms{0.64}{0.02}[b]  & \ms{0.64}{0.03}  & \ms{0.97}{0.04}  & \ms{0.72}{0.01} & \ms{0.72}{0.01} \\
            \cmidrule{2-8}
            & IB-EDL & \ms{0.98}{0.02}  & \ms{0.69}{0.03}  & \ms{0.57}{0.01}[b]  & \ms{0.81}{0.01}[b]  & \ms{0.45}{0.01}[b] & \ms{0.60}{0.01}[b] \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:full_noisy_mistral_7b}
\end{table}


\subsection{Additional OOD detection results on MMLU-Math dataset}\label{sec:app:ood_mmlu_math}
We conduct additional OOD detection experiments using Llama2-7B in a setting characterized by a significant distribution shift. For this purpose, we select a subset of the MMLU dataset~\citep{hendrycks2021ethics,hendryckstest2021} as the OOD test set, which includes data samples from the math-related topics \texttt{college\_mathematics}, \texttt{high\_school\_mathematics}, and \texttt{abstract\_algebra}. We refer to this subset as the MMLU-Math dataset. In addition, we use the OBQA dataset as the ID dataset. While OBQA focuses on common-sense reasoning, the MMLU-Math dataset requires advanced mathematical knowledge to solve its questions. As a result, the distribution shift in this setting is substantially larger compared to settings like OBQA $\rightarrow$ ARC-C.

As shown in \cref{tab:ood_mmlu_math_llama2_7b}, IB-EDL consistently achieves the best OOD detection performance when using either MP or UM as the detection score. Additionally, a general trend is observed: the AUROCs of IB-EDL and other baselines improve as the distribution shift increases compared to the setting OBQA $\rightarrow$ ARC-C.

% \input{tables/ood_mmlu_math_llama2_7b}
\begin{table}[t]
    \centering
    \caption{OOD Detection AUROC on Llama3-8B in the setting OBQA $\rightarrow$ MMLU-Math. IB-EDL achieves the best performance even under significant distribution shifts, such as transitioning from a common-sense reasoning dataset like OBQA to a math-focused OOD dataset.}
    \begin{tabular}{c|cc}
        \toprule
        \multirow{3}{*}{Method} & \multicolumn{2}{c}{OBQA $\rightarrow$ MMLU-Math} \\
        \cmidrule(lr){2-3}
        & \multicolumn{2}{c}{AUROC $\uparrow$} \\
        \cmidrule(lr){2-3}
        & MP & UM \\
        \midrule
        MAP & \ms{91.36}{0.57} & - \\
        MCD & \ms{90.85}{0.33} & - \\
        Ens & \ms{90.68}{0.80} & - \\
        LA & \ms{91.09}{0.41} & - \\
        EDL & \ms{92.78}{0.26} & \ms{92.86}{0.21} \\
        VID & \ms{91.64}{0.79} & \ms{66.61}{4.98} \\
        I-EDL & \ms{91.48}{0.72} & \ms{90.67}{0.88} \\
        R-EDL & \ms{88.44}{2.11} & \ms{88.22}{1.70} \\
        \cmidrule(lr){1-3}
        IB-EDL & \ms{93.63}{0.66}[b] & \ms{93.64}{0.56}[b] \\
        \bottomrule
    \end{tabular}
    \label{tab:ood_mmlu_math_llama2_7b}
\end{table}

\subsection{Addition calibration results on OOD test sets}\label{sec:app:ood_calibration}

In this section, we evaluate the calibration performance on OOD test sets to assess whether the uncertainty-aware methods can generalize effectively to OOD datasets. Specifically, we fine-tune Llama3-8B on the OBQA dataset and evaluate it on three OOD test sets. As shown in \cref{tab:ood_calibration_llama3_8b}, IB-EDL achieves the best ECE and NLL on two out of the three OOD test sets, demonstrating that its calibration performance generalizes well to OOD datasets.

% \input{tables/ood_calibration_llama3_8b}
\begin{table}[t]
    \centering
    \caption{Calibration performance of uncertainty-aware methods on fine-tuned Llama3-8B in the OOD setting. The model is trained on OBQA and tested on three different OOD test sets.}
    \resizebox{0.75\textwidth}{!}{
    \begin{tabular}{c| c | c c c}
        \toprule
         Metrics & Method & OBQA $\rightarrow$ ARC-C & OBQA $\rightarrow$ ARC-E & OBQA $\rightarrow$ CSQA \\
         \midrule 
         \multirow{9}{*}{Acc $\uparrow$}
            & MAP & \ms{79.18}{0.45} & \ms{88.06}{0.20} & \ms{69.37}{0.67} \\
            & MCD & \ms{79.16}{0.43} & \ms{88.05}{0.20} & \ms{69.38}{0.68} \\ 
            & Ens & \ms{79.27}{0.25}[u] & \ms{88.15}{0.02}[u] & \ms{69.14}{0.47} \\
            & LA  & \ms{79.38}{0.40}[b] & \ms{88.36}{0.24}[b] & \ms{69.34}{0.58} \\
            & EDL & \ms{78.27}{0.79} & \ms{86.38}{0.87} & \ms{69.34}{0.88} \\
            & VID & \ms{78.27}{0.57} & \ms{87.41}{0.82} & \ms{69.99}{1.07} \\
            & I-EDL & \ms{78.55}{0.30} & \ms{87.55}{0.20} & \ms{70.49}{0.56} \\
            & R-EDL & \ms{78.32}{1.24} & \ms{87.31}{0.93} & \ms{70.62}{1.28}[u] \\
            \cmidrule{2-5}
            & IB-EDL & \ms{78.31}{1.14} & \ms{87.94}{0.22} & \ms{71.29}{0.96}[b] \\
        \midrule
        \multirow{9}{*}{ECE $\downarrow$}
            & MAP & \ms{18.04}{0.38} & \ms{9.63}{0.47} & \ms{27.85}{0.53} \\
            & MCD & \ms{18.06}{0.36} & \ms{9.63}{0.46} & \ms{27.86}{0.46} \\ 
            & Ens & \ms{16.74}{0.13} & \ms{9.19}{0.65} & \ms{27.07}{1.32} \\
            & LA  & \ms{6.61}{0.41} & \ms{3.02}{0.15}[b] & \ms{11.99}{0.72} \\
            & EDL & \ms{7.65}{0.47} & \ms{10.11}{0.85} & \ms{8.32}{1.33} \\
            & VID & \ms{8.74}{1.19} & \ms{3.19}{0.19}[u] & \ms{16.66}{0.73} \\
            & I-EDL & \ms{7.68}{1.19} & \ms{11.38}{0.65} & \ms{5.96}{0.71}[u] \\
            & R-EDL & \ms{5.03}{0.62}[u] & \ms{5.32}{0.46} & \ms{12.84}{1.73} \\
            \cmidrule{2-5}
            & IB-EDL & \ms{4.67}{1.09}[b] & \ms{5.03}{0.15} & \ms{4.51}{0.15}[b] \\
        \midrule
        \multirow{9}{*}{NLL $\downarrow$}
            & MAP & \ms{1.30}{0.02} & \ms{0.71}{0.03} & \ms{2.06}{0.04} \\
            & MCD & \ms{1.33}{0.06} & \ms{0.70}{0.04} & \ms{2.13}{0.13} \\ 
            & Ens & \ms{1.19}{0.03} & \ms{0.68}{0.05} & \ms{2.01}{0.10} \\
            & LA  & \ms{0.73}{0.02} & \ms{0.42}{0.02}[b] & \ms{1.15}{0.01} \\
            & EDL & \ms{0.74}{0.01} & \ms{0.52}{0.01} & \ms{0.98}{0.03} \\
            & VID & \ms{0.78}{0.01} & \ms{0.46}{0.02} & \ms{1.06}{0.02} \\
            & I-EDL & \ms{0.73}{0.02} & \ms{0.53}{0.01} & \ms{0.94}{0.01}[u] \\
            & R-EDL & \ms{0.73}{0.04}[u] & \ms{0.46}{0.01} & \ms{1.00}{0.05} \\
            \cmidrule{2-5}
            & IB-EDL & \ms{0.72}{0.02}[b] & \ms{0.44}{0.02}[u] & \ms{0.93}{0.02}[b] \\
         \bottomrule
    \end{tabular}
    }
    \label{tab:ood_calibration_llama3_8b}
\end{table}

\subsection{Analysis on training and inference speed, and memory consumption}\label{sec:app:train_test_consumption}

In this section, we evaluate the complexity of uncertainty-aware methods using three key metrics: (1) the number of samples processed per second during training; (2) the number of samples processed per second during inference; and (3) GPU memory consumption during training. For this experiment, we use Llama3-8B in mixed precision as the model and OBQA as both the training and test dataset. All experiments are conducted on a single NVIDIA H100 GPU.
As shown in \cref{tab:train_test_consumption}, IB-EDL demonstrates comparable training and inference speeds as well as similar memory consumption to MAP and other EDL baselines. Moreover, IB-EDL attains faster inference speeds compared to methods such as MCD and Ens, which require multiple forward passes, and LA, which involves gradient computation during inference. This substantial improvement in speed further highlights the computational efficiency of IB-EDL.

% \input{tables/train_test_consumption}
\begin{table}[t]
    \centering
    \caption{Comparison of computational efficiency for various methods using Llama3-8B on the OBQA. IB-EDL demonstrates comparable training and inference speeds as well as memory consumption compared to MAP and other EDL methods, confirming its computational efficiency.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
        Method & Test Samples/s $\uparrow$ & Training Samples/s $\uparrow$ & Memory (GB) at Training$\downarrow$ \\
        \midrule
        MAP & \ms{69.55}{2.86} & \ms{26.57}{1.96} & \ms{21.21}{0.35} \\
        MCD (10 forwards) & \ms{9.79}{1.21} & - & - \\
        Ens (3 models) & \ms{25.77}{3.54} & - & - \\
        LA & \ms{5.95}{0.49} & - & - \\
        EDL & \ms{68.99}{1.59} & \ms{26.44}{1.11} & \ms{21.23}{0.15} \\
        VID & \ms{69.17}{0.99} & \ms{26.69}{1.37} & \ms{21.29}{0.37} \\
        I-EDL & \ms{68.94}{2.18} & \ms{26.02}{0.71} & \ms{21.33}{0.11} \\
        R-EDL & \ms{68.84}{1.09} & \ms{26.47}{1.09} & \ms{21.27}{0.21} \\
        \midrule
        IB-EDL & \ms{68.08}{1.75} & \ms{26.41}{1.04} & \ms{21.88}{0.66} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:train_test_consumption}
\end{table}

\subsection{Sensitivity analysis on the number of bins of ECE}\label{sec:app:ablation_ece_bins}
Following \cite{yang2024lalora}, we use 15 bins by default when measuring ECE. To assess the impact of this hyperparameter, we conduct a sensitivity analysis by varying the number of bins across $\{10, 15, 25, 35\}$. For this experiment, we train and test Llama3-8B on the OBQA dataset and calculate the ECE for each bin setting. As shown in \cref{tab:ablation_ece_bins}, although ECE values increase slightly with a higher number of bins, the relative rankings of the methods remain largely consistent.


% \input{tables/ablation_ece_bins}
\begin{table}[t]
    \centering
    \caption{Sensitivity analysis of ECE with respect to the number of bins. We train and test Llama3-8B on the OBQA dataset. The results show that while the ECE values increase slightly as the number of bins increases, the relative rankings of the methods remain consistent.}
    \begin{tabular}{c|cccc}
        \toprule
        Method & Bins = 10 & Bins = 15 & Bins = 25 & Bins = 35 \\
        \midrule
        MAP & \ms{10.45}{0.52} & \ms{10.52}{0.87} & \ms{10.89}{0.67} & \ms{10.99}{1.01} \\
        MCD & \ms{10.31}{0.37} & \ms{10.48}{0.86} & \ms{10.69}{0.59} & \ms{10.83}{0.76} \\
        Ens & \ms{10.11}{0.13} & \ms{10.08}{0.90} & \ms{10.91}{0.40} & \ms{10.92}{0.84} \\
        LA & \ms{5.20}{1.29} & \ms{5.26}{1.30} & \ms{6.33}{1.11} & \ms{6.42}{0.96} \\
        EDL & \ms{8.16}{1.25} & \ms{8.28}{1.62} & \ms{8.78}{1.27} & \ms{9.44}{1.79} \\
        VID & \ms{5.29}{0.50} & \ms{5.99}{1.41} & \ms{7.16}{1.59} & \ms{7.34}{1.17} \\
        I-EDL & \ms{7.31}{0.33} & \ms{7.57}{0.52} & \ms{8.20}{0.46} & \ms{9.01}{0.50} \\
        R-EDL & \ms{4.64}{0.87} & \ms{4.68}{1.35} & \ms{4.81}{1.09} & \ms{5.47}{0.82} \\
        \midrule
        IB-EDL & \ms{2.77}{0.61}[b] & \ms{2.34}{0.61}[b]\ & \ms{3.91}{0.77}[b] & \ms{4.54}{0.52}[b] \\
        \bottomrule
    \end{tabular}
    \label{tab:ablation_ece_bins}
\end{table}

