\section{Experiments}
% In this section, we conduct a comparative study of IB-EDL and other baselines for fine-tuning LLMs. We begin by detailing the experimental setups in \cref{sec:exp_setups}, followed by an evaluation of in-distribution calibration performance in \cref{sec:exp_id_calibration} and out-of-distribution detection in \cref{sec:exp_ood_detection}. We then assess our method under the more challenging condition of label noise, as discussed in \cref{sec:exp_ft_noisy_labels}. Finally, we provide the ablation study in \cref{sec:ablation_study}.

\subsection{Experimental setups} \label{sec:exp_setups}

\textbf{Models:} We fine-tune Llama2-7B~\citep{touvron2023llama}, Llama3-8B~\citep{dubey2024llama3herdmodels}, and Mistral-7B~\citep{jiang2023mistral7b} using LoRA~\citep{hu2022lora} implemented via PEFT~\citep{mangrulkar2022peft} and Transformers~\citep{wolf2020transformers}. Details on training configurations and $\beta$ values are provided in \cref{sec:app:implementation}. Due to space constraints, we primarily present results for Llama2-7B and Llama3-8B, with Mistral-7B results provided in \cref{sec:app:mistral_7b_results}.

\textbf{Datasets:} We compare methods on six multiple-choice classification datasets, including five for commonsense reasoning, ARC-C and ARC-E~\citep{clark2018arc}, OpenbookQA (OBQA)~\citep{mihaylov2018obqa}, CommonsenseQA (CSQA)~\citep{talmor2019commonsenseqa}, and SciQ~\citep{welbl2017sciq}, alongside a dataset for reading comprehension, RACE~\citep{lai2017race}. For these datasets, we define the target space $\Yspace$ as the tokens corresponding to the possible options (A/B/C/D). When fine-tuning LLMs, we select next-token logits (pre-evidences) corresponding to these options.

\textbf{Baselines:} We compare our method against a variety of baselines, including standard MAP training, two conventional uncertainty-aware approaches: Deep Ensemble (\textbf{Ens})~\citep{lakshminarayanan2017simple,fort2019deep} and MC-Dropout (\textbf{MCD})~\citep{gal2016dropout}, and Laplace-LoRA (\textbf{LA})~\citep{yang2024lalora}, a recent calibration method tailored for fine-tuned LLMs. Additionally, we include four baselines from the EDL family: vanilla \textbf{EDL}~\citep{sensoy2018evidential}, \textbf{VID}~\citep{chen2018variational} from VB-based EDL, and two SOTA methods: \textbf{I-EDL}~\citep{deng2023uncertainty} and \textbf{R-EDL}~\citep{chen2024redl}. We use their original implementations and hyperparameters where available. Additionally, although PostNet~\citep{charpentier2020posterior} is also a well-known EDL method, it is omitted here because it requires a specialized Normalizing Flow design to be compatible with Transformers.


\subsection{In-distribution calibration} \label{sec:exp_id_calibration}

\input{tables/id_llama2_7b}

\input{tables/id_llama3_8b}

An effective uncertainty-aware method should 1) significantly improve model calibration and 2) show accuracy comparable to standard MAP training. We therefore use Accuracy (Acc), expected calibration error (ECE), and negative log-likelihood (NLL) as metrics for evaluating the fine-tuned LLMs on the six aforementioned datasets. \cref{tab:id_llama2_7b} and \cref{tab:id_llama3_8b} present the results for Llama2-7B and Llama3-8B, respectively. The accuracy of IB-EDL and other uncertainty-aware methods is on par with, or even higher than, the MAP baseline, so we focus primarily on analyzing ECE and NLL. MAP exhibits substantially higher ECE and NLL than other methods, suggesting that fine-tuning LLMs on small datasets using MAP (or MLE) leads to significant overconfidence.  
Overall, IB-EDL shows the lowest ECE and NLL, reducing ECE by several factors compared to MAP, MCD, and Ens. This highlights IB-EDL's ability to effectively mitigate overconfidence in fine-tuned models. The superior performance of IB-EDL, compared to other EDL methods, can be attributed to the $\ell_2$ regularization in \cref{eq:loss_ib_info}, which discourages the model from generating excessively large evidences that lead to over-concentrated Dirichlet distributions. Furthermore, the advantages of IB-EDL extend to other architectures, such as Mistral-7B, as demonstrated in \cref{sec:app:mistral_7b_results}.

\subsection{Out-of-distribution detection} \label{sec:exp_ood_detection}
\input{tables/ood_llama2_7b_llama3_8b}
In addition to in-distribution (ID) calibration, out-of-distribution (OOD) detection serves as a key benchmark for assessing the performance of uncertainty-aware methods. An effective approach should reliably assign higher uncertainties to OOD samples compared to ID samples. This can be evaluated by labeling ID samples as class 1 and OOD samples as class 0, and measuring the \textbf{AUROC} based on OOD detection scores derived from the fine-tuned model. A higher AUROC indicates better OOD detection performance. Similar to \citet{chen2024redl}, we use two OOD detection scores: \emph{max probability} (\textbf{MP}) and the \emph{reciprocal of uncertainty mass} (\textbf{UM}). We fine-tune the LLMs on OBQA (as the ID dataset) and test them on ARC-C, ARC-E, and CSQA (as OOD dataset). Note that non-EDL methods, such as LA, do not provide UM, so we evaluate them using MP only. 
As shown in \cref{tab:ood_llama2_7b_llama3_8b}, IB-EDL achieves the highest AUROC across all datasets using both scores, surpassing both non-EDL and EDL competitors. Furthermore, IB-EDL also demonstrates superior OOD detection performance under large distribution shifts (see \cref{sec:app:ood_mmlu_math}). Its calibration performance also generalizes well to OOD datasets (see \cref{sec:app:ood_calibration}).
% In addition, a key difference is that while other EDL methods consistently show higher AUROC with MP than UM, IB-EDL shows higher AUROC with UM than MP on OOD datasets ARC-C and ARC-E. This suggests that IB-EDL substantially enhances the reliability of UM as an OOD detection score. Since UM is computed from the Dirichlet parameters $\alv$, it also indicates that IB-EDL effectively calibrates the predicted evidences and $\alv$ scores.


\subsection{Fine-tuning with noisy labels} \label{sec:exp_ft_noisy_labels}

Datasets used for fine-tuning LLMs can contain a significant portion of mislabeled samples~\citep{wang2024mmlu,havrilla2024understanding}, and label verification is often expert-knowledge demanding. Therefore, it is crucial for fine-tuning algorithms to be robust to label noise~\citep{wang2023noise}. To assess this robustness, we perturb the A/B/C/D options for $30\%$ of the samples in each training set, fine-tune the models using the aforementioned methods on the noisy datasets, and evaluate them on clean test sets. In this adversarial setting, the primary goal is to maintain accuracy despite label noise, so we primarily evaluate accuracy, with additional metrics in the \cref{sec:app:full_noisy_ft_results}. As shown in \cref{tab:acc_noisy_llama2_7b_llama3_8b}, IB-EDL achieves the highest accuracy overall, demonstrating strong robustness to label noise. This suggests that the information bottleneck effectively filters out spurious signals and retains predictive information in the generated evidence.

\input{tables/acc_noisy_llama2_7b_llama3_8b}
\begin{figure}[t]
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/ablation_accuracy.eps}
        \caption{Accuracy}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/ablation_ece.eps}
        \caption{ECE}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/ablation_nll.eps}
        \caption{NLL}
    \end{subfigure}
    \caption{Ablation study. IB-EDL reduces ECE and NLL compared to MAP across a broad range of $\beta$ and $K$ values. $\beta$ controls the regularization strength and balances the calibration and accuracy.}
    \label{fig:ablation_study}
\end{figure}

\subsection{Ablation study} \label{sec:ablation_study}
\textbf{Hyperparameters:} We study the weight $\beta$ and sample size $K$ (used for drawing $\etv$), using ID calibration for Llama2-7B on ARC-C as the evaluation task. \cref{fig:ablation_study} shows that IB-EDL consistently outperforms MAP across a broad range of values, demonstrating its robustness in reducing overconfidence. While increasing $K$ slightly improves accuracy, it does not necessarily improve ECE or NLL. $\beta$ plays a key role in balancing regularization and predictive performance.
% In extreme cases, a small $\beta$ weakens regularization, resulting in higher overconfidence and ECE, whereas a large $\beta$ overly suppresses predictive features (i.e.\ evidences), lowering accuracy. 
Nonetheless, all $\beta$ values reduce ECE by at least $40\%$ and NLL by at least $50\%$ compared to MAP. Additionally, we also present a sensitivity analysis on the number bins of ECE in \cref{sec:app:ablation_ece_bins}.

\textbf{Complexity analysis:} Here, we consider Llama2-7B and assume the target space is the vocabulary $\Yspace = \Vspace$. Compared to the pretrained LLM, IB-EDL introduces an additional linear head $h^\sigma$, adding only $1.95\%$ more parameters. The computational overhead stems from $h^\sigma$ and the evidence averaging operation (see \cref{algo:ib_edl}), which amount to only $1.98\%$ of the pretrained model's GFLOPs. In \cref{sec:app:train_test_consumption}, we provide detailed tests of training and inference time as well as memory usage.