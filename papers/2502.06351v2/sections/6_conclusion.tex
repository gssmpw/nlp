\section{Conclusion}

\textbf{Summary:} We focused on a key challenge in fine-tuning LLMs: mitigating overconfidence and improving calibration. We introduced an information-theoretic regularization to conventional EDL to prevent over-concentrated distributions in predictions. Our method, IB-EDL, introduces minimal computational overhead while significantly improving calibration in fine-tuned LLMs. Additionally, IB-EDL maintains model performance even in the presence of substantial label noise. These results highlight IB-EDL as a promising method for fostering more trustworthy LLMs.

\textbf{Limitations and future work:} Despite IB-EDL's efficiency, there is room for improvement. To reduce the complexity of covariance matrix prediction, we assume the pre-evidences are uncorrelated, but this assumption can be relaxed. Additionally, our evaluations primarily focused on conventional classification tasks, where well-established metrics for calibration and uncertainty estimation are available. In future work, it would be interesting to test IB-EDL on generative tasks. A great challenge is that uncertainty estimation metrics for generative tasks are still an ongoing research topic~\citep{yadkori2024believe,jesson2024estimating}. 
% Lastly, future work could also explore dynamically adjusting the weight $\beta$ during fine-tuning, rather than keeping it fixed throughout training.