\section{Related Work}
\label{sec:formatting}

\subsection{Vision-Language Models}
Vision-Language Models (VLMs) represent significant progress in multimodal learning~\cite{wei2021meta,9454290,wei2020universal} in recent years. These models are pre-trained on large-scale datasets to effectively learn joint representations from both images and text. Recent studies~\cite{radford2021learning,jia2021scaling,yao2021filip,yuan2021florence,liu2023learning} have demonstrated VLMs' superior performance in tasks such as zero/few-shot image recognition. Notably, the pioneering work CLIP ~\cite{radford2021learning} is renowned for its simplicity and effectiveness, leveraging large-scale image-text pair training and contrastive learning techniques. Other works like ALIGN~\cite{jia2021scaling}, FILIP~\cite{yao2021filip}, Florence~\cite{yuan2021florence}, and REACT~\cite{liu2023learning} further highlight VLMs' strong open vocabulary understanding. To adapt pre-trained VLMs to specific downstream tasks, numerous task-specific methods have been proposed including segmentation~\cite{wysoczanska2024clip,zhang2024exploring,zhou2022extract,wang2024cm}, image recognition~\cite{zhang2021tip,alayrac2022flamingo,yao2023visual,yang2024mma,wei2024runge, wu2024fine} and object detection~\cite{fang2024simple,zang2022open,zhou2022detecting,zhong2022regionclip,ke2024vldadaptor}. In this work, we propose a novel prompt-tuning technique that enhances VLMsâ€™ generalization performance across various visual recognition tasks.

\subsection{Prompt Learning for Vision-Language Models}
Fine-tuning Vision-Language Models (VLMs) for downstream tasks or datasets while preserving their original generalization capabilities poses significant challenges. Retraining the entire model is often impractical due to the vast number of parameters, and it risks overfitting, which could diminish the generalization benefits obtained during pre-training. To tackle this, prompt-tuning methods have emerged, with CoOp ~\cite{zhou2022learning} being a pioneering approach. CoOp adapts VLMs to downstream tasks by introducing learnable vectors in place of hand-crafted prompts. Extensions of this approach, multimodal prompt tuning~\cite{khattak2023maple,liu2024multi,cho2023distribution,zhang2024unleash} has been applied to fine-tune both text and visual branches simultaneously. To prevent overfitting, a line of works\cite {khattak2023self,yao2023visual,yao2024tcp,zhu2023prompt} have introduced regularization constraints to reduce the loss of general information. Other approaches, such as UNIGRAM~\cite{li2023gradient} and ProMetaR~\cite{park2024prompt}, leverage meta-learning to enhance generalization by initializing prompts or applying regularization in the meta-learning framework. More recently, some methods~\cite{lee2024coapt,menon2022visual,wang2024learning,zhang2024concept,kan2023knowledge} have introduced external knowledge to enrich textual representations and better capture complex visual semantics. Our approach goes beyond using learnable tokens by infusing them into class-specific prior knowledge to provide class-relevant preferences, resulting in improved generalization performance on downstream tasks.

%-------------------------------------------------------------------------