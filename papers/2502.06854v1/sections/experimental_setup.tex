\section{Study Design}
\label{sec:experimental_setup}
We systematically evaluate whether LLMs can effectively handle IRs without explicit IR-specific training. As shown in Fig.~\ref{fig:overview}, we investigate their ability to capture \emph{structural}, \emph{syntactic}, and \emph{semantic} properties of IRs through four tasks: 
\begin{itemize}[nolistsep,leftmargin=*]
\renewcommand{\labelitemi}{$\triangleright$}
    \item \emph{Task 1: CFG Reconstruction} measures an LLM’s ability to infer structural relationships in IRs by accurately reconstructing CFGs.
    \item \emph{Task 2: IR Decompilation} evaluates syntactic comprehension by recovering high-level source code from IR.
    \item \emph{Task 3: Code summarization} assesses semantic understanding by generating high-level descriptions of IR functions, including purpose, inputs, and outputs.
    \item \emph{Task 4: Execution Reasoning} tests whether LLMs can infer program behavior by reasoning about control flow, variable state changes, and assertion outcomes.
\end{itemize}

\subsection{Evaluation Tasks}


\subsubsection{CFG Reconstruction (Task 1)}


\label{subsec:cfg_evaluation}

Reconstructing a CFG from IR is critical for various software engineering tasks such as bug detection~\cite{zhang2023bugdetecting}, vulnerability detection~\cite{li2018vuldeepecker}, and malware analysis~\cite{anju2010malware}.
Although CFG recovery from binaries has been extensivly studied binaries~\cite{andriesse2016depth,pang2021sok,shoshitaishvili2016sok}, the application of LLMs to IR\_based CFG reconstruction is relatively unexplored. 


\mypara{Approach}

We prompt LLMs to generate CFGs directly from IR snippets. Outputs are compared against compiler-generated ground truth using an expert meta-template prompt (detailed in Section~\ref{subsec:prompt_design}), which provides structured guidance and iterative refinements.


\mypara{Evaluation Metrics}
Following~\cite{maunveiling}, we classify LLM-generated CFGs based on structural correctness:
\textbf{Node Construction Accuracy} measures the correctness of identified basic blocks, while \textbf{Edge Construction Accuracy} assesses control-flow edges, further categorized into \textbf{Full CFG Accuracy} (both nodes and edges match the ground truth) and \textbf{Partial CFG Accuracy} (nodes are correct, but edges contain errors). Within partial accuracy, \textbf{Loop Edge Accuracy} evaluates loops construction (e.g., \texttt{for}, \texttt{while}), and \textbf{Conditional Branch Accuracy} verifies branching structures (\texttt{if-else}). 

\subsubsection{IR Decompilation (Task 2)}
\label{subsec:decompilation_summarization}
\textit{Decompilation}—converting IR back to source code—is a fundamental task in reverse engineering, vulnerability assessments, malware detection, and software modernization~\cite{brumley2013native,jiang2023nova,hu2024degpt}.
Unlike CFG reconstruction, decompilation focuses on syntactic comprehension, testing whether LLMs can recover human-readable, high-level representations from IRs.

\mypara{Approach}
We prompt LLMs to decompile IR into C++ source and compare the generated output to the original code. Since compiler optimizations (O0–O3) significantly alter IR structure, we evaluate LLMs' ability to handle these transformations.

\mypara{Evaluation Metrics}
Following prior work~\cite{tan2024llm4decompile,armengol2024slade,wong2023refining}, we evaluate \emph{re-executability} of decompiled code by categorizing outcomes into \textbf{Re-execution Error} (compilation failure or crash), \textbf{Re-execution Completed} (successful execution), and,within the latter, \textbf{Re-execution Mismatch} (at least one assertion fails) and \textbf{Re-execution Success} (all assertions pass, ensuring correct functionality).

\subsubsection{Code Summarization (Task 3)}
\label{subsec:code_summarization}

While CFG reconstruction and decompilation emphasize structure and syntax, code summarization assesses semantic understanding. In this task, LLMs are required to generate natural language descriptions of IR functions, capturing their purpose, inputs, and outputs~\cite{codesummaryEvaluate,codesummarization2024}.

\mypara{Approach}
We prompt LLMs with IR snippets and request concise, human-readable summaries. To enhance clarity, we employ expert meta-prompts, few-shot examples, and chain-of-thought guidance. The generated summaries are compared to ground truth or human-curated references for evaluation.

\mypara{Evaluation Metrics}

The quality of summary is measured using the following metrics: \textbf{BLEU}~\cite{bleu} for token overlap, \textbf{ROUGE-L}~\cite{rouge} for sequence alignment based on the longest common subsequence, and \textbf{METEOR}~\cite{meteor} for a comprehensive assessment that combines precision, recall, and synonym matching.

\subsubsection{Execution Reasoning  (Task 4)}
\textit{Execution reasoning (ER)} evaluates whether LLMs can track variable states and control flow in IR-level "execution" without actually running the program. Unlike specification reasoning (SR)~\cite{min2023beyond,deshpande2021rect,wu2023reasoning}, which assesses compliance with expected behavior, ER focuses on inferring program behavior from static IR code.


\mypara{Approach}
We provide LLMs with IR code snippets and assertion statements, prompting them them to predict which assertions hold ``\textit{true}''. This requires reasoning over control flow, variable changes, and function interactions.

\mypara{Evaluation Metrics}
Execution reasoning is assessed based on assertion correctness: \textbf{Pass} (all assertions correct), \textbf{Partial Pass} (some assertions correct), and \textbf{Failure} (no assertions correct). Additionally, we compute the \textbf{Overall Pass Rate}, which quantifies the ratio of correct assertions to the total, reflecting the LLM's ability to reason about IR-level behaviors.

\subsection{Datasets}
To evaluate LLMs’ ability to comprehend IRs, we build on prior work~\cite{zheng2023codegeex, tan2024llm4decompile} and use the widely recognized \textbf{HumanEval} benchmark. Developed by OpenAI, HumanEval~\cite{HuamanEval2021} is designed to assess multilingual code generation and consists of 164 carefully handcrafted programming challenges. Each challenge contains a function signature, a natural language description, a function body, and an average of 7.7 assertion statements per challenge. In our experiments, these 164 \texttt{C++} programs serve as the source code for generating IRs to be analyzed.



\subsection{Prompt Design}
\label{subsec:prompt_design}

Effective prompt design is crucial for optimizing LLM performance~\cite{liu22csur}. Traditional single-turn prompts, which combine role instructions and task descriptions into a single input, often lack clarity and fail to provide structured guidance. Recent studies~\cite{prompt, experprompt} show that LLMs perform more effectively when treated as \emph{expert agents} with detailed, domain-specific instructions.

To improve response accuracy, we adopt an Expert Meta-Template Prompt:
\begin{prompt}
    \textit{You are an expert in [relevant field].} \\
    \textbf{Input Data:} [IR snippet or relevant context] \\  
    \textbf{Task:} [Description of tasks] \\  
    \textbf{Output Format:} [Structured format expectations]
\end{prompt}

This template ensures that LLMs receive explicit role definitions, domain knowledge, task specifications, and output formatting requirements, enabling more precise responses.

For each experiment (Tasks 1–4), we iteratively refine prompts using strategies such as few-shot learning and chain-of-thought (CoT) prompting~\cite{chain_of_thought_prompt}, evaluating multiple versions to maximize accuracy and relevance.
