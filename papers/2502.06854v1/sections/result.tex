\section{Experimental Results and Analysis}
\label{sec:results}

We evaluate the performance of LLMs across four core IR-related tasks: CFG construction, decompilation, code summarization, and execution reasoning, assessing their effectiveness without fine-tuning. This section presents quantitative results and key observations. We categorized tasks based on their completion status: \textbf{Task Completed}, where the model successfully produced an output, and \textbf{Task Failed}, where it encountered a hang or crash, primarily due to the IR length exceeding the LLM’s context window.
\input{tables/CFG_results}

\subsection{CFG Reconstruction (Task 1)}
Table~\ref{tab:cfg_results} presents the number of applications successfully processed by each LLM in Task 1, highlighting significant challenges in CFG reconstruction. GPT-4 is the only model to complete all 164 applications, whereas GPT-3 and Gemma 2 complete 133 and 98, respectively. LLaMA 3.1 and Code Llama perform the worst, with only 76 and 67 completed applications. These differences likely stem from token limitations, suggesting the need for expanded context windows or token-efficient IR representations.

In terms of accuracy, GPT-4 outperforms all other models, achieving full CFG accuracy in 39 applications, while GPT-3, Gemma 2, and LLaMA 3.1 achieve only 14, 16, and 15, respectively. Code Llama fails to produce a single fully accurate CFG, despite being code-focused, highlighting its struggles with control flow reconstruction. These results underscore a clear gap in LLMs’ ability to comprehend and construct CFGs, emphasizing the need for stronger structural reasoning in IR processing.



\finding{LLMs struggle with accurately identifying basic blocks and constructing control flow edges.}

Despite their ability to process IRs, LLMs face significant challenges in basic block recognition and loop structure reconstruction, both essential for CFG construction. As shown in Table~\ref{tab:cfg_results}, node construction errors remain high across all models, with GPT-4 at 114 errors, GPT-3 and Gemma 2 exceeding 100 errors, and LLaMA 3.1 and Code Llama at 60 and 40 errors, respectively. These errors stem from misinterpreted instruction boundaries, leading to incorrect basic block formation and disrupted CFGs.

LLMs also struggle with control flow edge construction, particularly in loops and conditional branches. Even GPT-4 reconstructs only 9 out of 125 loop edges and 2 conditional branches, while other models perform worse, often failing to recognize branching behavior. Code Llama’s complete failure to produce an accurate CFG underscores LLMs’ limited structural reasoning. These findings highlight the need for improved control flow reasoning, particularly in instruction grouping, edge formation, and loop handling, to enhance CFG reconstruction.

\finding{LLMs' performance varies by CFG density, guiding model selection.}
Although LLMs generally struggle with CFG reconstruction, their success or failure varies by graph density.
Upon further analysis, we observe that the applications in these groups are distinguished by their graph density metrics, suggesting that CFG density influences which model can successfully or unsuccessfully construct them.
\begin{figure}[!t]
\centering
\includegraphics[width=.96\linewidth]{images/density_comparison_reordered_with_rectangles.pdf}
\vspace{-1em}
\caption{Density metrics comparing Complete CFG Accuracy vs.\ Node Construction Error.}%, highlighting differences in model performance.}
\label{fig:veen_CFG}
\vspace{-1em}
\end{figure}\\
\mypara{Graph Density} Graph density measures the proportion of present edges to the maximum possible edges in a graph. For directed graphs, it is calculated as:
\begin{equation}
    \text{Density} = \frac{\text{Number of Edges}}{n \times (n - 1)}
\end{equation}
where \( n \) is the number of nodes.

Fig.~\ref{fig:veen_CFG} compares Complete CFG Accuracy (blue bars) and Node Construction Error (orange bars) across LLMs. The x-axis represents the combinations of LLMs with identical results for the same set of applications (e.g., "GPT-4 only" refers to applications where only GPT-4 succeeded, while "GPT-4 × Gemma 2" refers to applications where GPT-4 and Gemma 2 produced identical results). The y-axis represents CFG density, with error bars illustrate the density range for each category. The distinct density patterns between correctly constructed CFGs and those with node construction errors highlight the role of CFG density in LLM performance. 


This suggests that analyzing density can guide model selection for CFG tasks, increasing the likelihood of correctly reconstructing a complete CFG by choosing models suited for specific density characteristics. For example:\\
\noindent
\emph{\colorbox{red!35}{Denser CFGs (0.11):}} GPT-4 or Gemma 2 are recommended, as both successfully completed the tasks at this density, while LLaMA 3.1 often fails.\\
\noindent
\emph{\colorbox{green!35}{Denser CFGs (0.02)}} LLaMA 3.1 is the better option, as it handled these tasks successfully, while GPT-4 and Gemma 2 struggle.



\subsection{IR Decompilation (Task 2)}
\label{subsec:decompilation_results}

This task evaluates LLMs' ability to decompile IR into C++ by comparing generated code with the original source. We assess re-executability~\cite{tan2024llm4decompile, armengol2024slade, wong2023refining}, ensuring correctness via assertion-based checks. Key metrics include task completion, re-execution success, and failure rates.

\input{tables/decompilation_result}

Table~\ref{tab:decompilation_result} shows that most failures occur at O0, where GPT-4 has the fewest failures (3), while Code Llama struggles (99 failures). This is due to O0 generating the longest IRs, exceeding the context windows of many models.

It is interesting to see that re-execution success is highest at O1 and O2, despite similar IR lengths at O1–O3. This is likely because O1 and O2 retain structured control flow, while O3 applies aggressive optimizations (e.g., loop unrolling, instruction reordering) that obscure execution logic, reducing LLM accuracy. Overall, O1 and O2 provide the best balance between optimization and interpretability, leading to more reliable decompilation outcomes.

\finding{LLMs occasionally skip parts of the IR, deviating from traditional decompilers.}
In several tasks, LLMs omit portions of the IR during decompilation, such as entire if statements within loops, leading to incomplete or simplified outputs. This suggests that LLMs do not strictly follow traditional decompilation methods but instead reconstruct code by recognizing patterns and inferring missing details. Due to their limited ability to process fine-grained IR semantics, LLMs often guess loop behavior, resulting in missing control flow elements. A similar issue is observed in the other two tasks: Code Summarization (Section~\ref{subsec:code_summarization_result}) and Execution Reasoning task (Section~\ref{subsec:execution_reasoning}), where LLMs struggle to fully capture execution semantics, further reinforcing the need for more structured IR comprehension.
Figure~\ref{list:decompilation_result} (Appendix~\ref{subsec:appendix:decompilation_example}) shows a re-execution mismatch from GPT-4. 

Overall, these omissions confirm that LLMs do not always follow a strict decompilation pipeline. Instead, they generate code that is \emph{syntactically valid} but may simplify or skip certain IR instructions, reflecting a more heuristic approach to IR reconstruction.


\subsection{Code Summarization (Task 3)}
\label{subsec:code_summarization_result}

To assess LLMs' ability to capture the broader semantics of IRs, we prompt them to generate natural language descriptions of code snippets derived from IRs. This task evaluates how well LLMs understand and articulate the purpose, functionality, inputs, and outputs of the code. The generated summaries are compared against predefined golden summaries using similarity metrics such as BLEU and ROUGE. Higher similarity scores indicate a stronger grasp of IR semantics, reflecting the LLMs' ability to infer and convey meaningful program behavior.

Table~\ref{tab:codesummary_result} shows the summarization performance of LLMs across BLEU, ROUGE-L, and METEOR metrics. GPT-4 outperforms the others in task completion, successfully completing all 164 tasks, while GPT-3, Gemma 2, LLaMA 3.1, and Code Llama fail 31, 76, 83, and 41 tasks, respectively. High-quality summaries (scores above 0.8) are rare, with each model achieving just one BLEU score above 0.8. However, GPT-4 leads in ROUGE-L with 9 high-quality summaries, followed by LLaMA 3.1 (5), Gemma 2 (4), and GPT-3 and Code Llama (1 each). GPT-4 also excels in METEOR with 21 high-quality summaries, while LLaMA 3.1, Gemma 2, GPT-3, and Code Llama have 11, 7, and 1 each, respectively. These results highlight GPT-4’s advantage in capturing and summarizing code semantics.
In terms of average scores, LLaMA 3.1 leads in BLEU (0.39), followed by GPT-4 and Gemma 2 (both 0.35). LLaMA 3.1 also leads in ROUGE-L (0.61), followed by GPT-4 (0.56), Gemma 2 (0.52), GPT-3 (0.47), and Code Llama (0.43). In METEOR, LLaMA 3.1 scores the highest (0.67), followed by GPT-4 and Gemma 2 (0.63 each). GPT-3 and Code Llama trail with 0.56 and 0.55, respectively, indicating weaker performance in summarizing code semantics.


\finding{LLMs can capture function signatures and input-output relationships but fail to retain full semantic details.}

Our results show that LLMs accurately identify the number of functions, inputs, and outputs, as well as extract basic function structures, input types, and arithmetic operations. However, their summaries often omit critical semantic details, such as iteration logic and conditional dependencies. This suggests that LLMs generate descriptions based on high-level patterns rather than truly reconstructing the underlying semantics. A detailed case study is provided in Appendix~\ref{subsec:appendix:summarization_example}.


\begin{table}[t]
\centering
\setlength\tabcolsep{2pt} % Adjust column spacing
\renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
\caption{Performance comparison of LLMs in the code summarization task, including task completion and evaluation scores (BLEU, ROUGE-L, METEOR).}
\label{tab:codesummary_result}

\resizebox{\columnwidth}{!}{ % Resize table to fit single column
\begin{tabular}{l|c|ccc|ccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{LLMs}}} &
  \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Task\\ Comp.\end{tabular}}} &
  \multicolumn{3}{c|}{\textbf{High-Quality (Score $>$ 0.8)}} &
  \multicolumn{3}{c}{\textbf{Avg. Scores}} \\ \cline{3-8} 
\multicolumn{1}{c|}{} &
   &
  \textbf{BLEU} &
  \textbf{ROUGE} &
  \textbf{METEOR} &
  \textbf{BLEU} &
  \textbf{ROUGE} &
  \textbf{METEOR} \\ \hline
GPT-4      & \cellcolor{pink!25}{\textbf{164}} & 1 & \cellcolor{pink!25}\textbf{9} & \cellcolor{pink!25}\textbf{21} & 0.35 & 0.56 & 0.63 \\
GPT-3      & 131 & \cellcolor{pink!25}\textbf{1} & 1 & 1  & 0.24 & 0.47 & 0.56 \\
Gemma 2    & 88  & \cellcolor{pink!25}\textbf{1} & 4 & 7  & 0.35 & 0.52 & 0.63 \\
LLaMA 3.1  & 81  & \cellcolor{pink!25}\textbf{1} & 5 & 11 & \cellcolor{pink!25}\textbf{0.39} & \cellcolor{pink!25}\textbf{0.61} & \cellcolor{pink!25}\textbf{0.67} \\
Code Llama & 123 & \cellcolor{pink!25}\textbf{1} & 1 & 1  & 0.25 & 0.43 & 0.55 \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Execution Reasoning (Task 4)}
\label{subsec:execution_reasoning}

\input{tables/execution_simulation_result}

We prompt LLMs to simulate execution using assertion statements from the HumanEval benchmark, generating detailed execution logs to assess whether they correctly follow input-driven control flow and produce expected outputs. To further investigate their reasoning abilities, we extend this task to a counterfactual study, comparing execution behavior on IRs and their corresponding source code (C++). Execution reasoning requires models to track variable evolution, manage control flow transitions, and handle memory operations, testing their ability to infer program behavior accurately.

Table~\ref{tab:dynamic_results} summarizes execution simulation results across LLMs for both IRs and source code.
All models successfully complete the execution tasks, except for Code Llama, which fails in only one IR-based case. Unlike failures in other tasks caused by token limitations, this suggests that code reasoning processes IR incrementally, interpreting execution step by step rather than consuming the entire IR at once. This distinction highlights a different way LLMs interact with IRs in execution reasoning compared to structural and syntactic tasks.

Pass rates in Table~\ref{tab:dynamic_results} indicate a significant performance gap between source code and IRs, with LLMs performing better on high-level source code. GPT-4 leads in IR execution, achieving 59 successful cases, while Code Llama demonstrates the highest pass rate on source code with 131 passes.
Notably, Code Llama’s strong performance on source code suggests that code-specific training enhances LLMs' understanding of programming languages. This implies that if we aim to improve LLMs' comprehension of IR, IR-specific training may be necessary to bridge the gap between source-level and IR-level reasoning.

In this task, we also observe that LLMs struggle with complex control flows, leading to inconsistencies between IR and source code execution, which is consistent to Finding 2. While LLMs correctly predict execution results for simple control flows, assertion failures increase with more iterations or interdependencies. For instance, in "CPP\_93," an encoding function to replaces all vowels is tested with:

\texttt{assert (encode("TEST") == "tgst")} \quad (Pass) \\
\texttt{assert (encode("I DoNt KnOw WhAt tO WrItE") == "k dQnT kNqW wHcT Tq wRkTg")} \quad (Fail)

The failure stems from the LLM skipping intermediate transformation steps, causing an incomplete encoding process.


\finding{LLMs approximate execution in large semantic steps rather than strictly following control flow.}
Instead of interpreting IRs instruction by instruction, LLMs generate abstracted execution steps based on their understanding of function semantics. The reasoning logs reveal that LLMs typically break execution into 5-7 steps, including: function comprehension, input analysis, loading elements, performing operations, logical evaluation, assertion comparison, and final conclusions (detailed example in Section~\ref{subsec:appendix:example_exe_reasoning}). In each step, they attempt to summarize the semantic meaning of different parts rather than executing them in a sequential, instruction-level manner.

As discussed in Section~\ref{subsec:code_summarization_result}, LLMs already struggle with semantic understanding, and their execution reasoning further reflects this limitation. The reasoning log provides deeper insights into how LLMs approximate rather than simulate execution, often relying on pattern recognition rather than faithfully following control dependencies.

\mypara{Granularity Issues:} While LLMs recognize high-level algorithms (e.g., sorting), they fail to apply rules consistently—e.g., identifying a sorting algorithm but not realizing it only sorts odd numbers. LLMs sometimes omit execution steps, leading to assertion failures. 

For example, in ``CPP\_149'', which is
\begin{tcolorbox}[colframe=black!50, colback=gray!10, boxrule=0.5mm, arc=3mm, sharp corners, left=2mm, right=2mm, top=2mm, bottom=2mm]
% \footnotesize
\textit{Function Description:}\\
% \footnotesize
\texttt{
A function that accepts a vector of strings as a parameter
deletes the strings that have odd lengths from it,
and returns the resulting vector in sorted order.
}
\end{tcolorbox}
However, the LLM skipped an ``odd number removal'' step, altering execution behavior. Here is a reasoning log snippet:
\begin{tcolorbox}[colframe=black!50, colback=gray!10, boxrule=0.5mm, arc=3mm, sharp corners, left=2mm, right=2mm, top=2mm, bottom=2mm]
\footnotesize\texttt{Step 3:}\\
\footnotesize\texttt{Analyzing the LLVM IR code:}\\
\footnotesize\texttt{ - The LLVM IR code is complex and involves multiple operations, including comparisons, memory allocations, and calls to other functions.}\\
\footnotesize\texttt{ - Key parts of the code involve loading strings from the vector, comparing them, and potentially invoking sorting or filtering operations.}
\end{tcolorbox}
This suggests that LLMs need a more structured approach to execution reasoning, focusing on fine-grained step tracking instead of high-level approximations. Overcoming this limitation may involve fine-tuning on execution traces, using explicit reasoning prompts, or incorporating reinforcement-based feedback to improve control flow reasoning.



\finding{LLMs exhibit lower confidence in IR execution compared to source code, leading them to rely on pattern-based inference rather than explicit reasoning.}
Unlike source code, IRs lack explicit variable names and high-level semantics, making execution reasoning more ambiguous. 
When LLMs encounter uncertainties in IR execution, instead of acknowledging gaps in understanding, they often default to heuristic strategies, such as inferring behavior from function names or patterns in assertions. This guesswork approach frequently results in incorrect outputs, as LLMs prioritize familiar structures over accurate execution semantics. We have observed 35 cases out of 164 in GPT-4 that guess based function names or result patterns. There is one case detailed in Appendix~\ref{subsubsec:appendix:example_pattern_Guess}.

The lack of explicit uncertainty handling highlights a fundamental limitation in their IR comprehension, further reinforcing the need for IR-specific training to improve execution reasoning.
LLMs compensate by:
\begin{itemize}[nolistsep, leftmargin=*]
    \item \emph{Guessing semantics} based on function names rather than pure execution logic. For example, in ``CPP\_120'', LLM might assume that a function named \texttt{maximum} sorts a vector and selects the largest $k$ elements based solely on its name. While a naive LLM can recognize syntax patterns, it fails at step-by-step execution reasoning, which is necessary for accurately understanding the function's intent. 
    \item \emph{Leaning on assertion results} to infer missing information rather than deducing behavior strictly from IR operations. In ``CPP\_109'', where the function \texttt{move\_one\_ball} determines whether a vector can be sorted through any number of right shifts, the model should simulate the IR operations to validate correctness. However, instead of reasoning through the shifting process, LLMs make conclusions ``pass'' based on the expected behavior of the function.
\end{itemize}

\mypara{Summary} Our evaluation reveals significant challenges in LLMs' capacity to handle IRs, particularly in control flow reasoning, execution semantics, and loop analysis. One key issue is token limitations, which become more severe for IRs than for natural language or source code, as IRs contain longer sequences with more tokens per function. This exacerbates the difficulty of processing structural dependencies and capturing execution semantics.

Even within the constraints of token length, LLMs struggle with tasks requiring control flow graph reconstruction, execution behavior inference, and iteration analysis. While they can effectively identify syntactic patterns and high-level structures, their performance drops significantly when deeper reasoning is required. Among the evaluated models, GPT-4 consistently outperforms the others, aligning with its demonstrated proficiency in source code comprehension, as explored in~\cite{ma2023lms}.

Addressing these limitations requires targeted improvements in three areas:
\begin{itemize}[nolistsep,leftmargin=*]
    \item \emph{Control Flow Comprehension}: LLMs frequently misinterpret branching and loop structures, leading to inaccuracies in CFG reconstruction. Enhancing their ability to track execution paths could improve structural reasoning.
    \item \emph{Granular Semantic Understanding}: These models rely heavily on heuristics when reasoning about IRs, often skipping fine-grained execution details. Future study should focus on refining instruction-level comprehension.
    \item \emph{Loop Reasoning}: The inability to accurately predict loop behavior and termination conditions remains a significant challenge. A deeper understanding of iterative constructs is necessary to enhance execution reasoning capabilities.
\end{itemize}

These findings suggest that improving control flow awareness, refining multi-level semantic analysis, and strengthening loop handling mechanisms could significantly enhance LLMs' effectiveness in IR-related tasks.

