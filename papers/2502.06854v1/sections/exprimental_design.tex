% \section{Task-Based Evaluation of LLMs on IRs}
%\section{Study Design}
%\label{sec:Experimental_setup}

%\section{Detailed Overview of Experimental Design}
\label{appendix:sec:experimental_design}
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.95\linewidth]{images/LLMinIR-overview5.0_icml.drawio.pdf}
%     \caption{Overview of the study design}%\textbf{Study I:} (A) Compile Original Source Codes to IRs by the compiler; (B) Static Analysis: prompt LLMs to construct CFGs/CGs/DDGs and assess the construction results against the Golden CFG/CG/DDG constructed by the compiler (in Section~\ref{subsec:RQ1}: RQ1); C) Syntax Analysis: prompt LLMs to decompile IR to New Source Code and assess the New Source Code against the Original Source Code (in Section~\ref{subsec:RQ2}: RQ2). D) Semantic Analysis: Prompt LLMs to summarize IR (in Section~\ref{subsec:RQ3}).\textbf{Study II:} E) Prompt LLMS to execute the IR with given assertion statement to see Pass or Fail (in Section~\ref{subsec:RQ4})}
%     \label{fig:overview}
% \end{figure*}

%Overall, these four tasks—CFG reconstruction, decompilation, code summarization, and execution reasoning—enable a broad evaluation of how well LLMs handle IR structure, syntax, semantics, and dynamic behaviors. In the next sections, we detail our experimental setup, report quantitative results, and analyze the capabilities and limitations of various LLMs on each task.