\section{Related Work}
\label{sec:related_work} 
\mypara{LLMs for High-level Programming Languages}  
Recent advancements in LLMs for NLP have extended to code understanding, enabling models to comprehend high-level languages like Python, C++, and Java. Models such as GPT-3 \cite{brown2020gpt3}, GPT-4 \cite{openai2023gpt4}, LLaMA \cite{touvron2023llama}, and Claude 3 \cite{claude3} excel at tasks like code generation, translating natural language into executable code. Specialized models, including StarCoder \cite{li2023starcoder}, Code Llama \cite{rozi√®re2024codellamaopenfoundation}, and Code Gemma \cite{team2024codegemma}, enhance these capabilities, tackling more complex coding tasks and driving advances in software engineering applications \cite{zhao2023survey}.

\mypara{IR Representation Learning}  
IR representation learning integrates structural and flow-based features, such as token sequences~\cite{peng2021could}, control flow graphs (CFGs)\cite{2020ir2vec, yu2020codecmr}, and control-data flow graphs (CDFGs)\cite{ben2018neural, brauckmann2020compiler, cummins2021programl}.
In terms of model architectures, graph neural networks (GNNs) have been widely used to encode the structures of CFGs and CDFGs via message-passing techniques~\cite{brauckmann2020compiler, cummins2021programl, yu2020codecmr}. Other approaches include skip-gram embeddings, such as inst2vec~\cite{ben2018neural}, and relation-based embeddings, like TransE~\cite{bordes2013transE}, which are trained on CDFGs and CFGs to produce instruction-level embeddings. However, these models lack task-agnostic pre-trained embeddings, limiting their ability to capture essential contextual information for downstream tasks.
Approaches like IR2Vec~\cite{2020ir2vec} address this limitation by introducing hierarchical vector representations that enhance the semantic understanding of IRs.
Meta's LLM Compiler~\cite{llmscompiler} aligns with these efforts, offering pre-trained models for code optimization tasks. 
While previous work has explored IR representation for code optimization, no study has systematically examined how LLMs understand IR syntax, CFGs, execution behavior, and semantics. This study provides the first empirical evaluation of LLMs' IR comprehension across these dimensions.
