\section{Related Works}
\textbf{Neural Optimal Transport. }Neural OT solvers employ continuous methods to estimate transportation plans or dual potentials through neural networks, avoiding the discrete formulations typically used by traditional approaches. Popular methods **Genevay, "Stabilizing Neural Transport"** often reparameterize the transportation plan or dual potentials and leverage ICNN  **Cuturi, "Improved Divergence Estimation with Sliced Wasserstein Distances"** for approximation. In addition, **Genevay et al., "Learning to Measure Distributions by Maximizing and Minimizing Divergence"** provides a benchmark for evaluating the performance of current neural OT solvers, and **Kroemer et al., "Relaxing Wasserstein Constraints on Individual Data Points"** proposes a framework to solve neural OT problems with general cost functions.

In line with CellOT**, we utilize the minimax formulation of dual potentials from **Cuturi and Blondel, "Smoothed Low-Rank Regularization for Large-Scale Optimal Transport"** to introduce displacement sparsity. Comparing to other neural OT solvers, this formulation was chosen because it provides a straightforward approach to recovering displacement vectors through dual potentials. 

Unlike Sinkhorn OT solvers, which relies on entropic regularization and fixed cost functions, neural OT solvers can dynamically learn transport maps for unfixed measures and generalize across tasks with dynamic loss functions. This ability to adapt makes neural OT solvers highly versatile in a variety of optimal transport applications. It is also worth clarifying that approaches based on the Wasserstein Generative Adversarial Network (W-GAN)** **Arjovsky et al., "Wasserstein GAN"** do not compute OT mappings or transportation plans directly; rather, they use the Wasserstein distance as a loss function during training.

\textbf{Displacement-sparse Optimal Transport. }**Genevay and Cuturi, "Stabilizing Duality by Variable Step Regularization of the Dual Variables"** laid the groundwork for incorporating sparsity into displacement vectors in OT by formulating a framework using proximal operators of the sparsity penalty \(\tau\) in the elastic cost to recover the OT map \(T\). Building on this, **Cuturi and Chonavel, "A Non-Stationary Sinkhorn Algorithm with Application to Transport-based Word Embeddings"** proposed a framework to recover the OT map under any elastic cost without relying on proximal operators. Under the framework of neural OT, our work generalizes the formulation to allow the use of any sparsity penalties, even when their proximal operators are not well-defined.

\textbf{Sparsity-related Optimal Transport. }Apart from displacement sparsity, other works have explored different aspects of sparsity in the context of OT. **Genevay et al., "Stabilizing Neural Transport"** introduced a regularization method to control the column cardinality of the transportation plan matrix, enhancing both storage efficiency and computational complexity. Similarly, **Cuturi and Blondel, "Smoothed Low-Rank Regularization for Large-Scale Optimal Transport"** proposed a regularization approach using the \(\ell^2_2\)-norm to address the dense and complex transportation plans typically produced by the entropic-regularized Sinkhorn solution.