\section{Related Works}
\textbf{Neural Optimal Transport. }Neural OT solvers employ continuous methods to estimate transportation plans or dual potentials through neural networks, avoiding the discrete formulations typically used by traditional approaches. Popular methods~\cite{taghvaei2019,Makkuva20,pmlr-v139-fan21d,korotin2021wasserstein,mokrov2021largescale,Bunne22,alvarez-melis2022optimizing} often reparameterize the transportation plan or dual potentials and leverage ICNN \cite{Amos17} for approximation. In addition, \citet{Korotin21} provides a benchmark for evaluating the performance of current neural OT solvers, and \citet{asadulaev2024neural} proposes a framework to solve neural OT problems with general cost functions.

In line with CellOT~\cite{Bunne23}, we utilize the minimax formulation of dual potentials from \citet{Makkuva20} to introduce displacement sparsity. Comparing to other neural OT solvers, this formulation was chosen because it provides a straightforward approach to recovering displacement vectors through dual potentials. 

Unlike Sinkhorn OT solvers, which relies on entropic regularization and fixed cost functions, neural OT solvers can dynamically learn transport maps for unfixed measures and generalize across tasks with dynamic loss functions. This ability to adapt makes neural OT solvers highly versatile in a variety of optimal transport applications. It is also worth clarifying that approaches based on the Wasserstein Generative Adversarial Network (W-GAN)~\cite{wgan,gulrajani2017improved,wei2018improving} do not compute OT mappings or transportation plans directly; rather, they use the Wasserstein distance as a loss function during training.

\textbf{Displacement-sparse Optimal Transport. }\citet{Cuturi23} laid the groundwork for incorporating sparsity into displacement vectors in OT by formulating a framework using proximal operators of the sparsity penalty \(\tau\) in the elastic cost to recover the OT map \(T\). Building on this, \citet{klein2024learning} proposed a framework to recover the OT map under any elastic cost without relying on proximal operators. Under the framework of neural OT, our work generalizes the formulation to allow the use of any sparsity penalties, even when their proximal operators are not well-defined.

\textbf{Sparsity-related Optimal Transport. }Apart from displacement sparsity, other works have explored different aspects of sparsity in the context of OT. \citet{liu2023sparsity} introduced a regularization method to control the column cardinality of the transportation plan matrix, enhancing both storage efficiency and computational complexity. Similarly, \citet{blondel18} proposed a regularization approach using the \(\ell^2_2\)-norm to address the dense and complex transportation plans typically produced by the entropic-regularized Sinkhorn solution.