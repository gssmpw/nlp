\section{Theoretical Results}\label{sect3}
This section presents our theoretical results and the idea of taking each step in the proof. Our proof constructs a particular instance of the transformers and shows that the forward propagation on our constructed instance approximates the Power Method. We also carefully design the contextual matrix $\bfa P$, explained as follows.

\paragraph{The Design of Auxillary Matrix.} Our design of the matrix $\bfa P$ consists of three parts:
\begin{enumerate}
    \item \emph{Place Holder.} For $\ell\in\{1\}\cup[4:k+3]$ and $i\in[N]$, we let $\tda p_{\ell, i}=\bfa 0\in\bb R^{d\times 1}$. The place holders in $\bfa P$ record the intermediate results in the forward propagation.
    \item \emph{Identity Matrix.} We let $ \begin{bmatrix}
        \tda p_{2,1}&\ldots&\tda p_{2,N}
    \end{bmatrix}=\begin{bmatrix}
        \bfa I_d&\bfa 0_{d\times(N-d)}
    \end{bmatrix}$. The identity matrix in $\bfa P$ helps us screen out all the covariates $\bfa X$ in the forward propagation.
    \item \emph{Random Samples on the Hypersphere.} We let $\tda p_{3,1},\ldots\tda p_{3,k}$ be the i.i.d. samples uniformly distributed on $\bb S^{d-1}$. The random samples on the sphere correspond to the initial vectors $\bfa v_{0,\ell}$ for $\ell\in[k]$ in algorithm \ref{alg:almoexactrecov}.
\end{enumerate}
Given the above construction on the auxiliary matrix $\bfa P$, we are ready to state the existence theorem in this work, given as follows.

\begin{algorithm}[htbp] 
 \caption{Spectral Clustering}
\label{alg:spectralclustering}
\KwData{$\bfa X\in\bb R^{D\times N}$ with $\bfa X_i\in\bb R^D$ for all $i\in[N]$}
 Compute the SVD of the data matrix $\bfa X=UDV^\top$. Let $U_k$ be the first $k$ columns of $U$\;

 Project $\bfa x_1,\ldots,\bfa x_N$ onto $U_k$, i.e. let $\wha x_i\gets U_kU_k^\top \bfa x_i$ \;
 Run an \emph{Transformer k-means++\cite{arthur2006k}} algorithm on the columns of projected matrix $\wha X=\begin{bmatrix}
     \wha x_1,\ldots,\wha x_N
 \end{bmatrix}$\;
\end{algorithm}
\begin{assumption}\label{asumpt1}
    We assume that $\bfa X_1,\ldots,\bfa X_N$ are i.i.d. samples from $\sum_{\ell=1}^k\pi_\ell\bb P(X|\mu)$. Minimal separation is denoted by $\Delta:=\inf_{i\neq j}\Vert\bfa\mu_i-\bfa\mu_j\Vert_2$.
\end{assumption}

And the kmeans++ algorithm is given by the following procedure.
\begin{algorithm}[htbp] 
 \caption{k-means++}
\label{alg:kmeans++}
\KwData{$\bfa X\in\bb R^{k\times N}$ with $\bfa X_i\in\bb R^k$ for all $i\in[N]$}
Choose an initial center $\bfa c_1$ uniformly at random from $\ca X:=\{\bfa X_1,\ldots,\bfa X_N\}$ and let $\ca C=\{\bfa c_1\}$\;
\While{$|\ca C|<k$}{Choose the next center $\bfa c$, selecting $\bfa c=\bfa x\in\ca X$ with probability $\frac{D(\bfa x)}{\sum_{x\in\ca X}D(\bfa x)}$\\
where $D(\bfa x):=\argmin_{\bfa c\in\ca C}\Vert \bfa c-\bfa x\Vert_2$\;
If $\bfa c\notin\ca C$, let $\ca C\gets\ca C\cup\{\bfa c\}$\;
}
Run the Lloyd's algorithm with covariates $\bfa X$ and initial centroids $\ca C$\;
\end{algorithm}
\begin{algorithm}[htbp] 
 \caption{Lloyd's Algorithm}
\label{alg:lloyd}
\KwData{$\bfa X\in\bb R^{k\times N}$ and initial clusters $\ca C=\lef\{\wha\mu_\ell^{(0)}\rig\}_{\ell\in[k]}$}
Let $t\gets 1$\;
Initialize with the starting membership $\wh z_i^{(0)}\gets\argmin_{\ell\in[k]}\Vert \bfa x_i-\wha\mu_\ell^{(0)}\Vert_{\Sigma^{-1}}$\;
\While{$\exists j\in[n]$ such that $\wh z^{(t+1)}_j\neq \wh z^{(t)}_j$}{Update the centroids and memberships through $\wha\mu_{\ell}^{(t)}\gets\frac{\sum_{i=1}^n\bfa x_{i}\mbbm 1_{\wh z_i^{(t-1)}=\ell}}{\sum_{i=1}^n\mbbm 1_{\wh z_i^{(t-1)}=\ell}},\enspace\forall \ell\in[k],\qquad \wh z_i^{(t)}\gets\argmin_{\ell\in[k]}\Vert \bfa x_i-\wha\mu_{\ell}^{(t)}\Vert^2_{\Sigma^{-1}},\enspace\forall i\in[n].$
}
Run the Lloyd's algorithm with $\bfa X$ and $\ca C$\;
\end{algorithm}

In particular, take as input the context given by $\bfa H=\begin{bmatrix}
    \bfa x_1,\ldots,\bfa x_N\\
    \bfa p_1,\ldots,\bfa p_N
\end{bmatrix}\in\bb R^{D\times N}$, we show that the Transformer network can approximate the spectral clustering algorithm in the initialization phase, given by \ref{alg:spectralclustering}.
\subsection{The Power Iteration Method}
In the first step of the power method 
we use the Transformer model to approximate the singular vectors of the matrix $\bfa X^\top\bfa X$. A typical implementation to achieve this is given by the power method.


\begin{theorem}[Transformer Approximation of the Power Iteration]\label{thm3.1} Denote the eigenvalues of $\bfa X\bfa X^\top$ to be $\lambda_1>\lambda_2>\ldots>\lambda_k>\ldots$. Let $\Delta:=\min_{1\leq i<j\leq k}|\lambda_i-\lambda_j|$. Assume that the eigenvalues of $\bfa X$ satisfy $\Vert\bfa X\Vert_2\leq B_X$. Assume that the initialized vectors $\tda p_{3,1},\ldots\tda p_{3,N}$ 
    satisfy  $\tda p_{3,i}^\top\bfa v_{i}\geq\delta$ for all $i\in[k]$ and make the rest of the vectors $\bfa 0$. 
    Then, there exists a transformer model with number of layers $L=2\tau+4k+1$ and number of heads $M\leq \lambda_1^d\frac{C}{\epsilon^2}$ with $\tau\leq\frac{\log(1/\epsilon_0\delta)}{\epsilon_0}$ such that for all $\epsilon_0,\epsilon>0$,
    the final output $\wha v_1,\ldots,\wha v_{k}$ given by the transformer model achieve
    \begin{align*}
        \lef\Vert\wha v_{\eta+1}-\bfa v_{\eta+1}\rig\Vert_2\leq C\tau \epsilon\lambda_1^2+\frac{C\lambda_1\sqrt{\epsilon_0}}{\Delta}\prod_{i=1}^{\eta}\frac{5\lambda_{i+1}}{\Delta}.
    \end{align*} 
    Moreover, consider the accuracy of multiple $\bfa v$s as a whole. There exists $\bfa\theta$ such that 
    \begin{align*}
        L\lef(TF_{\wha\theta}(\bfa H),\bfa V\rig)\leq C\tau\epsilon k\lambda_1^2+C\bl\frac{\epsilon_0\lambda_1^2}{\Delta^2}\sum_{\eta=1}^{k-1}\prod_{i=1}^{\eta}\frac{25\lambda_{i+1}^2}{\Delta^2}\br^{1/2}.
    \end{align*}
\end{theorem}
\begin{remark}
\label{remark3}
    The approximation error consists of two terms. The first term comes from the approximation of the Power Method iterations by transformers. The second term comes from the error caused by finite iteration $\tau$. To acquire a more direct account of the error terms and its order of magnitude, we consider a special case where the eigenvalues $\lambda_1\asymp\lambda_2\asymp\ldots\asymp\lambda_k\asymp\Delta$. Then our results boil down to 
    \begin{align*}
        \lef\Vert TF_{\bfa\theta}(\bfa H)-\begin{bmatrix}
            \bfa v_1^\top,\bfa v_2^\top,\ldots,\bfa v_k^\top
        \end{bmatrix}^\top\rig\Vert_2\leq C\tau\epsilon k \lambda_1^2+ C\frac{\lambda_1}{\Delta}\sqrt{k\epsilon_0}.
    \end{align*}
    These results hide dimension $d$ in the universal constant. Hence the dimension significantly affects the approximation properties of transformers. Our experimental results in section \ref{sect4} also indicate that learning high dimensional principal eigenvectors is challenging.
\end{remark}
We show that the conditions on $\tda p_{3,1},\ldots,\tda p_{3,N}$ can be achieved through sampling from isotropic Gaussians, given by the following lemma.
    \begin{lemma}\label{lm3.1.1}\label{lm3.1}
        Let $\bfa y\in\bb R^d$ be a random vector with isotropic Gaussian as its probability density. Consider $\bfa x=\frac{\bfa y}{\Vert\bfa y\Vert_2}$. Let $\bfa v$ be any unit length vector, then we have for all $\delta<\frac{1}{2}d^{-1}$,
           $ \bb P\lef(|\bfa v^\top\bfa x|\leq\delta\rig)\leq  \frac{1}{\sqrt{\pi}}\sqrt\delta+\exp\lef(-C\delta^{-\frac{1}{2}}\rig)$.
        Therefore, for all $\delta<\frac{1}{2}d^{-1}$, the event in theorem \ref{thm3.1} is achieved with
        \begin{align*}
             \bb P\bl\exists i\in[k]\text{ such that }&\bfa x_i^\top\bfa v_i\leq\frac{\delta}{\sqrt d}\br\leq \frac{k\sqrt{\delta}}{\sqrt{\pi}}+k\exp(-C\delta^{-1}).
        \end{align*}
    \end{lemma}
Given the approximation error provided by theorem \ref{thm3.1}, we further provide the generalization error bound for the ERM defined by \eqref{ERM}. This requires us to consider the following regularity conditions on the underlying distribution of $\bfa X\bfa X^\top$ (which also translates to the distribution of $\bfa X$).
\begin{assumption}\label{assump1}
    The distribution of $\bfa X\bfa X^\top$ supports on $$\bb X:=\lef\{A:A\in\bfa S^d_{++},  B_X\geq\lambda_1(A)>\lambda_2(A)>\ldots>\lambda_k(A),\inf_{1\leq i<j\leq k}\lambda_i(A)-\lambda_j(A)\geq\Delta\rig\}.$$
\end{assumption}
\begin{remark}
    The above assumption can be easily generalized to distribution that supports on $\bb X$ with high probability. Examples of such distribution include the Wishart distribution under the Gaussian design. In this work, we stick to the simplest case where the maximum eigenvalue is bounded from above.
\end{remark}
Given the above assumption, we are ready to state the generalization bound.
\begin{proposition}\label{genbound}
   With probability at least $1-\xi$, the ERM solution $\wha\theta$ satisfies
    \begin{align*}
        \bb E\lef[L\lef(TF_{\wha\theta}(\bfa H),\bfa V\rig)|\wha\theta\rig]&\leq\inf_{\bfa\theta\in\Theta(B_{\bfa\theta},B_M)}\bb E\lef[L\lef(TF_{\bfa\theta}(\bfa H),\bfa V\rig)\rig]\\
        &+ C \sqrt{\frac{k^3LB_Md^2\log(B_{\theta}+B_X+k)+\log(1/\xi)}{n}}.
    \end{align*}
\end{proposition}
   Together with the bound given by theorem \ref{thm3.1} and lemma \ref{lm3.1.1}, which essentially give a high probability upper bound on $\inf_{\bfa\theta\in\Theta(B_{\bfa\theta},B_M)}$ we can derive a general upper bound on the generalization error, given as follows.
    \begin{corollary}
        Under assumption \ref{assump1}, with probability at least $1-\xi-\frac{k\sqrt\delta}{\sqrt\pi}-k\exp\lef(-C\delta^{-1/2}\rig)$ for all $\delta<d^{-1}$ we have for all $\epsilon,\epsilon_0>0$,
        \begin{align*}
            \bb E\lef[L\lef(TF_{\wha\theta}(\bfa H),\bfa V\rig)|\wha\theta\rig]&\leq \bb E\lef[C\tau\epsilon k\lambda_1^2+C\bl\frac{\epsilon_0\lambda_1^2}{\Delta^2}\sum_{\eta=1}^{k-1}\prod_{i=1}^{\eta}\frac{25\lambda_{i+1}^2}{\Delta^2}\br^{1/2}\rig]\\
            &+C\sqrt{\frac{k^3\log(\delta/
            \epsilon_0)\lambda_1^d d^2\log(B_{\theta}+B_X+k)+\log(1/\xi)}{n\epsilon_0\epsilon^2}}.
        \end{align*}
    \end{corollary}
    \begin{remark}
        If we consider optimizing the bound w.r.t. $\epsilon_0$ and $\epsilon$, we obtain that $\bb E\lef[L(TF_{\wha\theta}(\bfa H),\bfa V)|\wha\theta\rig]\lesssim n^{-1/5}$ given that the rest of the parameters are of constant scales. It is not known if the results are improvable or not and the authors believe this question worth future explorations.
    \end{remark}
The next theorem demonstrates that there exists a Transformer that simulates the expectation maximization algorithm on Gaussian mixture model.
\begin{theorem}
    
\end{theorem}
\begin{proof}
We first consider the input matrix to be 
\begin{align*}
    \bfa H_1:=\bfa H=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_n\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(n)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa p_{3,1}&\bfa p_{3,2}&\ldots&\bfa p_{3,N}
    \end{bmatrix}\in\bb R^{D\times N}, 
\end{align*}
where $\wh z^{(0)}:[n]\to[k]$ is the assignment function, $\wha\mu_i\in\bb R^{d}$ is the initially estimated centroid for the $i$-th cluster. $\bfa p_{1,i}\in\bb R^k$ satisfies $\bfa p_{1,i,j}=\mbbm 1_{\wh z^{(0)}(i)=j}$ for all $j\in[k]$. And for $\bfa p_{2,i}$ we have $\bfa p_{2,i,j} = \mbbm 1_{j=i}$ for $i\leq d$ and $\bfa p_{2,i,j}=0$ for $i\leq N$ and $j\leq d$. We let $\bfa p_{3,1}=\bfa p_{3,2}=\ldots=\bfa p_{3,N} = \bfa 0\in\bb R^k$.
We note that algorithm \ref{alg:lloyd} consists of two iterative steps: (1) The expectation step where we take the averages to get an initial estimate $\wha \mu_{\ell}^{(t)}$. (2) The maximization step where we assign each individual their labels. Our following discussions treat the two steps separately. 
\begin{center}
    \textbf{1. The Expectation Step.}
\end{center}
To achieve the first step, we construct our transformer weights as follows:
\begin{align*}
    \bfa V_1^{(1)}=\begin{bmatrix}
        \tda V_{1,1}^{(1)}&\tda V_{1,2}^{(1)}&\tda V_{1,3}^{(1)}
    \end{bmatrix},\quad\bfa Q_1^{(1)}=\begin{bmatrix}
        \bfa 0_{1\times(3d+k)}&1&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa K_1^{(1)}=\begin{bmatrix}
        \bfa 0_{1\times(3d+k)}&1&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix},
\end{align*}
where $\tda V_{1,1}^{(1)}\in\bb R^{2d\times D}=\bfa 0$, $\tda V_{1,2}^{(1)}=\begin{bmatrix}
        \bfa 0_{3d+k}\\
        I_k\\
        \bfa 0
    \end{bmatrix}\in\bb R^{k\times D}$. Then we can show that
\begin{align*}
   (\bfa K_1^{(1)}\bfa H_1)^\top=(\bfa Q_1^{(1)}\bfa H)^\top = \begin{bmatrix}
        1&\bfa 0\\
        \vdots &\bfa 0\\
        1&\bfa 0
    \end{bmatrix}.
\end{align*}
Then we can show that
\begin{align*}
   (\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H) = \begin{bmatrix}
       \bfa v_1&\ldots&\bfa v_1
   \end{bmatrix},\quad \bfa v_{1,i} = 1\quad\forall i\in[N].
\end{align*}
Hence, we can obtain that
\begin{align*}
    \bfa V_1^{(1)}\bfa H&\times\sigma((\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H))=\bfa V_1\bfa H\times\begin{bmatrix}
        \bfa v_1 &\ldots&\bfa v_1
    \end{bmatrix}=\bfa V_1\times \begin{bmatrix}
        &\bfa A_0&\\
        \bfa v_k &\ldots&\bfa v_k\\
        &\bfa A_1&
    \end{bmatrix}\\
    &=\begin{bmatrix}
        \tda V_{1,1}^{(1)}&\tda V_{1,2}^{(1)}&\tda V_{1,3}^{(1)}
    \end{bmatrix}\begin{bmatrix}
        &\bfa A_0&\\
        \bfa v_k&\ldots&\bfa v_k\\
        &\bfa A_1&
    \end{bmatrix}=\tda V_{1,2}^{(1)}\begin{bmatrix}
        \bfa v_k &\ldots&\bfa v_k
    \end{bmatrix}\\
    &=\begin{bmatrix}
        \bfa 0_{3d+k}\\
        I_k\\
        \bfa 0
    \end{bmatrix}\begin{bmatrix}
        \bfa v_k&\ldots&\bfa v_k
    \end{bmatrix}=\begin{bmatrix}
        &\bfa 0_{3d+k}&\\
        \bfa v_k&\ldots&\bfa v_k\\
        &\bfa 0&
    \end{bmatrix},
\end{align*}
 where $\bfa A_0\in\bb R^{2d\times N}$ and $
    \bfa v_{k,\ell}=\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=\ell}$.
Then it is checked that
\begin{align*}
   \bfa H_2 =  \bfa H_1+ \bfa V_1^{(1)}\bfa H_1\times\sigma\lef((\bfa Q_1^{(1)}\bfa H_1)^\top(\bfa K_1^{(1)}\bfa H_1)\rig)=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \bfa p_{4,1}& \bfa p_{4,2}&\ldots &\bfa p_{4,N}
    \end{bmatrix}.
\end{align*}
Therefore, we further construct the following multi-head layer to remove the off-diagonal elements in $\begin{bmatrix}
    \bfa v_k&\bfa v_k&\ldots&\bfa v_k
\end{bmatrix}$, given by 
\begin{align*}
    \bfa V_i^{(2)} = \begin{bmatrix}
        &\bfa 0_{(3d+2k+i)\times D}&\\
        \bfa 0_{1\times(3d+2k+i)}&1&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad \bfa Q_1^{(2)} = \begin{bmatrix}
        b
    \end{bmatrix},\quad\bfa K_1^{(2)} = \begin{bmatrix}
        c
    \end{bmatrix},\qquad\text{ for }i\in[k].
\end{align*}
Given this formulation, we can show that
\begin{align*}
    \sigma((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)) = \begin{bmatrix}
        &\bfa 0_{(i-1)\times N}&\\
        \bfa 0_{1\times(i-1)}&1&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
\end{align*}
Hence, we can further show that
\begin{align*}
    \bfa V_{i}^{(2)}\bfa H_2\sigma((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2))=\begin{bmatrix}
        &\bfa 0_{(3d+2k+i)\times D}&\\
        \bfa 0_{(i-1)}&\bfa v_{k,i}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},
\end{align*}
which immediately implies that
\begin{align*}
    \sum_{i=1}^k\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
        \bfa 0_{(3d+2k)\times N}&\\
        \diag(\bfa v_k)&\bfa 0\\
        \bfa 0
    \end{bmatrix}.
\end{align*}
Given the above design, we can show that
\begin{align*}
    \bfa H_{3,1}=\bfa H_2+\sum_{i=1}^k\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
Then, we construct the MLP layer to remove the $\bfa v_k$ part, which is designed by
\begin{align*}
    \bfa W_1^{(2)}=I_D,\quad \bfa W_2^{(2)}= \begin{bmatrix}
        &\bfa 0_{(3d+k)\times D}&&\\
        \bfa 0_{k\times (3d+k)}&-I_k&I_k&\bfa 0_{}\\
       \bfa 0 &-I_k&\bfa 0&\bfa 0\\
       &\bfa 0&&
    \end{bmatrix}.
\end{align*}
Given this formulation, we can show that
\begin{align*}
    \bfa H_3 := \bfa H_{3,1}+\bfa W_1^{(2)}\sigma\lef(\bfa W_2^{(2)}\bfa H_{3,1}\rig)=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}

% Then we consider the following attention head which essentially remove the $\bfa v_k$ part in $\bfa H_2$. We note that the rank of $\sigma((\bfa Q_i\bfa H_2)^\top(\bfa K_i\bfa H_2))$ is at most $D$. To form the 
% \begin{align*}
%     \bfa V_{k+1}^{(2)} = \begin{bmatrix}
%         &\bfa 0_{(3d+k)\times D}&\\
%         \bfa 0_{k\times (3d+k)}&-I_k&\bfa 0_{}\\
%         &\bfa 0&
%     \end{bmatrix},\quad \bfa Q_{k+1}^{(2)}=,\quad \bfa K_{k+1}^{(2)}=
% \end{align*}
% Given the above design, we can show that
% \begin{align*}
%     \bfa V_{k+1}^{(2)}\bfa H_2\sigma\lef((\bfa Q_{k+1}^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
%         &\bfa 0_{(3d+k)\times N}&\\
%         -\bfa v_k&\ldots&-\bfa v_k\\
%         &\bfa 0&
%     \end{bmatrix}.
% \end{align*}
% Therefore, collecting pieces, we can show that
% \begin{align*}
%    \bfa H_3:= \bfa H_2 + \sum_{i=1}^{k+1}\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \diag(\bfa v_k)&&\bfa 0&\\
%         \bfa p_{4,1}& \bfa p_{4,2}&\ldots &\bfa p_{4,N}
%     \end{bmatrix}.
% \end{align*}
The following layer converts the term $\diag(\bfa v_k)$ to $\diag(\bfa v_k^\prime)$ where $\bfa v_{k,i}^\prime = 1/\bfa v_{k,i}$. The design is given as follows
\begin{align*}
    \bfa V_{i}^{(3)} &= \begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k\times(2d+2k)}&\diag(c_i)_{k\times k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa Q_{i}^{(3)}=\begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k\times(2d+2k)} &I_{k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa K_{i}^{(3)}&=\begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k \times (3d+3k+1)}&\diag(a_i)_{k\times k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}\begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
       \bfa 0_{k\times(3d+2k+1)}&I_{k} &\bfa 0\\
       &\bfa 0&
    \end{bmatrix},
\end{align*}
where we show in lemma \ref{reluapprox} that for $M>\frac{1}{\epsilon^2}\log(1+C/\epsilon)$, there exists $\{a_i\}_{i\in[M]}$ such that for $x>1$, we have 
\begin{align*}
    \bigg\Vert\sum_{i=1}^Mc_i\sigma(a_ix) - \frac{1}{x}\bigg\Vert_2\leq\epsilon.
\end{align*}
And when $x=0$, we automatically obtain that 
$   \sum_{i=1}^Mc_i\sigma(0) =0$. We then immediately obtain that
\begin{align*}
    \bfa H_{4,1}:&=\bfa H_3 + \sum_{i=1}^M\bfa V_i^{(3)}\sigma\lef((\bfa Q_i^{(3)}\bfa H_3)^\top(\bfa K_i^{(3)}\bfa H_3)\rig)= \begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k)&&\bfa 0&\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}+O_{2}(\epsilon),
\end{align*}
where $\bfa v_{k,i}^\prime = \bfa v_{k,i}^{-1}$. Then we apply the MLP again with the following design 
\begin{align*}
    \bfa W_1^{(4)}= I_D,\quad\bfa W_2^{(4)}=\begin{bmatrix}
        &\bfa 0_{(3d+k)\times D}&&\\
        \bfa 0_{k\times(3d+k)}&-I_k&I_k&\bfa 0\\
        \bfa 0&-I_k&\bfa 0&\bfa 0\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
The above construction implies that
\begin{align*}
    \bfa H_4 =\bfa W_2^{(3)}\sigma\lef(\bfa W_1^{(3)}\bfa H_3\rig) = \begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
We construct the following layer to perform the normalization, given by 
\begin{align*}
    \bfa V_2^{(4)}&=-\bfa V_{1}^{(4)}= \begin{bmatrix}
    &\bfa 0_{(3d+2k+1)\times D}& \\
    \bfa 0_{k\times(3d+k+1)}&I_{k}&\bfa 0\\
    &\bfa 0&
    \end{bmatrix},\quad \bfa Q_1^{(4)} =-\bfa Q_2^{(4)}= \begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa 0_{k\times 3d}&I_{k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa K_1^{(4)}&=\bfa K_2^{(4)}=\begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa 0_{k\times (3d+1)}&I_k&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
\end{align*}
Then we can show that
\begin{align*}
    \sigma\lef((\bfa Q_1^{(4)}\bfa H_4)^\top(\bfa K_1^{(4)}\bfa H_4)\rig)-\sigma\lef((\bfa Q_2^{(4)}\bfa H_4)^\top(\bfa K_2^{(4)}\bfa H_4)\rig) = \begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa p_{1,1}&\ldots&\bfa p_{1,N}\\
        &\bfa 0
    \end{bmatrix}.
\end{align*}
And we also have
\begin{align*}
    \bfa V_2^{(4)}\bfa H_4 = \begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k\times(3d+2k+1)}&\diag(\bfa v_k^\prime) &\bfa 0\\
        &\bfa 0&
\end{bmatrix}+O_2(\epsilon),
\end{align*}
which implies that
\begin{align*}
    \bfa H_{4,1} = \bfa H_3+\sum_{i=1}^2\bfa V_i^{(4)}\bfa H_3\times\sigma\lef((\bfa Q_i^{(4)}\bfa H_3)^\top(\bfa K_i^{(3)}\bfa H_3)\rig)=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        \bfa p_{1,1}^\prime&\bfa p_{1,2}^\prime&\ldots&\bfa p_{1,N}^\prime
    \end{bmatrix}+O_{2}(\epsilon),
\end{align*}
where $\bfa p_{1,i}^\prime = \diag(\bfa v_k^\prime)\bfa p_{1,i}$ for all $i\in[N]$. We therefore construct an MLP layer to replace the $\bfa p_{1}$ part using the following design
\begin{align*}
    \bfa W_1^{(4)}=I_D,\qquad\bfa W_2^{(4)} = \begin{bmatrix}
        &\bfa 0_{2d\times D}&&\\
        \bfa 0_{k\times 2d}&-I_k&\bfa 0&I_k&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0&\bfa 0&\bfa 0\\
        \bfa 0_{k\times 2d}&-I_k&\bfa 0&\bfa 0 &\bfa 0\\
    \end{bmatrix},
\end{align*}
which ultimately leads to 
\begin{align*}
    \bfa H_4 = \bfa W_1^{(3)}\sigma\lef(\bfa W_2^{(3)}\bfa H_3\rig) = \begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
        \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
We can further perform the expectation step by the following layer
\begin{align*}
    \bfa V_i^{(4)}=\begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa 0_{k\times2d}& I_k&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa Q_i^{(4)}=\begin{bmatrix}
        \bfa 0&\bfa 0&\bfa 0\\
        \bfa 0_{k\times2d}& I_k&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa K_i^{(4)}=\begin{bmatrix}
        \bfa 0&\bfa 0&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix}
\end{align*}
Then it is further noted that
\begin{align*}
    \bfa H_5 =\bfa H_4+\sum_{i=1}^2\bfa V_i^{(4)}
\end{align*}
\end{proof}
% The rest of this section presents a proof sketch on theorem \ref{thm3.1}.
% \paragraph{Proof Sketch of Theorem \ref{thm3.1}.}
%     The proof sketch for theorem \ref{thm3.1} goes by two important steps:



%     To obtain $\tda p_{3,i}$ for $i\in[k]$, we generate through the probablistic method, which gives the following lemma.
%     % \begin{lemma}
%     %     Let $\bfa y\in\bb R^d$ be a random vector with isotropic Gaussian as its probability density. Consider $\bfa x=\frac{\bfa y}{\Vert\bfa y\Vert_2}$. Let $\bfa v$ be any unit length vector, then we have
%     %        $$ \bb P\bl|\bfa v^\top\bfa x|\leq\frac{\epsilon}{\sqrt d}\br\leq  \frac{1}{\sqrt{\pi}}\epsilon+\exp\lef(-Cd\rig).$$
%     %     Therefore, the event in theorem \ref{thm3.1} can be estimated as
%     %     \begin{align*}
%     %          \bb P\bl\exists i\text{ such that }&\bfa x_i^\top\bfa v_i\leq\frac{\epsilon}{\sqrt d}\br\leq 1-\bl1-\frac{\epsilon}{\sqrt{\pi}}-\exp(-Cd)\br^k\leq \frac{k\epsilon}{\sqrt{\pi}}+k\exp(-Cd).
%     %     \end{align*}
%     % \end{lemma}

%     % \begin{align*}
%     %    \Bigg\Vert \tda H^{rpe,2}-\begin{bmatrix}
%     %         \bfa X\\
%     %         \tda y\\
%     %         \bfa X\bfa X^\top,\bfa 0\\
%     %         \tda p_{2,1},\ldots,\tda p_{2,N}\\
%     %         \tda p_{3,1},\ldots,\tda p_{3,N}\\
%     %         \tda p_{3,1}^{(\tau)},\bfa 0\\
%     %         \Vert\bfa X\bfa X^\top\tda p_{3,N}^{(\tau)}\Vert_2,\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2\leq\epsilon \Vert\bfa X\bfa X^\top\tda p_{3,1}\Vert_2.
%     % \end{align*}
%     % Then we can show that
%     % \begin{align*}
%     %     &\Bigg\Vert\bfa K\tda H^{pow,2\tau+2}-\begin{bmatrix}
%     %         \bfa 0_{(5d+1)\times(4d+1)}&\bfa 0_{(5d+1)\times 1}&\bfa 0\\
%     %         \bfa 0_{d\times(4d+1)}&\tda p_{3,1}^{(\tau)}&\bfa 0\\
%     %         \bfa 0&\bfa 0&\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2\leq \tau\epsilon\Vert\bfa X\bfa X^\top\Vert_2,\\ 
%     %     &\Bigg\Vert\bfa Q\tda H^{pow,2\tau+2}-\begin{bmatrix}
%     %         \bfa 0_{(5d+1)\times 1}&\bfa 0\\
%     %        \Vert\bfa X\bfa X^\top\tda p_{3,1}^{(\tau)}\Vert_2 \tda p_{3,1}^{(\tau)}&\bfa 0\\
%     %         \bfa 0&\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2\leq\tau\epsilon\Vert\bfa X\bfa X^\top\Vert_2.
%     % \end{align*}
%     % which immediately implies that
%     % \begin{align*}
%     %     \Bigg\Vert(\bfa K\tda H^{pow,2\tau+2})^\top(\bfa Q\tda H^{pow,2\tau+2})-\begin{bmatrix}
%     %         \bfa 0&\bfa 0\\
%     %         \Vert\bfa X\bfa X^\top\tda p_{3,1}\Vert_2\tda p_{3,1}^{(\tau)}\tda p_{3,1}^{(\tau),\top}&\bfa 0\\
%     %         \bfa 0&\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2
%     % \end{align*}
% \begin{lemma}\label{lm3.2}
% Assume that the correlation matrix $\bfa X\bfa X^\top$ has eigenvalues $\lambda_1>\lambda_2>\ldots>\lambda_{k}$. Assume that the eigenvectors are given by $\bfa v_1,\bfa v_2,\ldots,\bfa v_n$ and the eigenvalues satisfy $\inf_{i\neq j}|\lambda_i-\lambda_j|=\Delta$. Then, given that the estimate for the first $\tau$ eigenvectors satisfy
% $    \bfa v_i^\top\wha v_i\geq 1-\epsilon_i $ and the eigenvalues satisfy $|\lambda_i-\wh\lambda_i|\leq\delta_i$, the principle eigenvector of $\bfa X\bfa X^\top-\sum_{i=1}^{\tau}\wh\lambda_i\wha v_i\wha v_i^\top$ denoted by $\tda v_{\tau+1}$ satisfies
% \begin{align*}
%     \Vert\tda v_{\tau+1}-\bfa v_{\tau+1}\Vert_2 \leq\frac{\max_{i\in[\tau]}\delta_i+\sum_{i=1}^\tau\sqrt8\lambda_i\sqrt{\epsilon_i}}{\Delta}.
% \end{align*}
% Alternatively, we can also show that the eigenvector $\wha v_{\tau+1}$ returned by power method with $k= \frac{\log(1/\epsilon_0\delta)}{2\epsilon_0}$ that is initialized by  satisfies
% \begin{align*}
%     \wha v_{\tau+1}^\top\bfa v_{\tau+1} \geq 1-\epsilon_{\tau+1}:=  1-\frac{1}{2}\Big(\frac{\max_{i\in[\tau]}\delta_i+\sum_{i=1}^{\tau}\sqrt{8}\lambda_i\sqrt{\epsilon_i}}{\Delta}+\sqrt{2\epsilon_0}\Big)^2,
% \end{align*}
% and alternatively we have $\Vert\wha v_{\tau+1}-\bfa v_{\tau+1}\Vert_2\leq\sqrt{2\epsilon_{\tau+1}}$. Furthermore, consider the eigenvalue estimate, we show that
% \begin{align*}
%    \Big|\Big\Vert\Big(\bfa X\bfa X^\top-\sum_{i=1}^{\tau}\wh\lambda_i\wha v_i\wha v_i^\top\Big)\wha v_{\tau+1}\Big\Vert_2-\lambda_{\tau+1}\Big|\leq \frac{2\lambda_{\tau+1}}{\Delta}\Big(\max_{i\in[\tau]}\delta_i+\sum_{i=1}^{\tau}\sqrt{8}\lambda_i\sqrt{\epsilon_i}\Big)+\lambda_{\tau+1}\sqrt{2\epsilon_0}
% \end{align*}
% \end{lemma}

% \begin{lemma}[Approximation of norm by sum of Relu activations by Transformer networks]\label{reluapprox}
%     Assume that there exists a constant $C$ with $\Vert\bfa v\Vert_2\leq C$. There exists a multihead Relu attention layer with number of heads $M<\lef(\frac{\overline R}{\underline R}\rig)^d\frac{C(d)}{\epsilon^2}\log(1+C/\epsilon)$ such that there exsits $\{\bfa a_m\}_{m\in[M]}\subset\bb S^{N-1}$ and $\{c_m\}_{m\in[M]}\subset\bb R$ where for all $\bfa v$ with $\overline R\geq\Vert\bfa v\Vert_2\geq \underline R$, we have 
%     \begin{align*}
%         \bigg|\sum_{m=1}^Mc_m\sigma(\bfa a_m^\top\bfa v)-\frac{1}{\Vert\bfa v\Vert_2}+1\bigg|\leq\epsilon.
%     \end{align*}
%     Similarly, there exists a multihead Relu attention layer with number of heads $M\leq\overline R^{\frac{d}{2}}\frac{C(d)}{\epsilon^2}\log\lef(1+C/\epsilon\rig)$, a set of vectors $\{\bfa b_m\}_{m\in[M]}\subset\bb S^{N-1}$ and $\{d_m\}_{m\in[M]}\subset\bb R$ such that 
%     \begin{align*}
%         \Big|\sum_{m=1}^Md_m\sigma(\bfa b_m^\top\bfa v)-\Vert\bfa v\Vert_2^{1/2}+1\Big|\leq\epsilon.
%     \end{align*}
% \end{lemma}



