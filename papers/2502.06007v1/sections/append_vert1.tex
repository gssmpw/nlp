\section{Theoretical Background}
This section provides approximation results of the Softmax function in the form of $f(\bfa x):=\sum_{i=1}^M a_i\sigmoid\lef(\bfa x^\top\bfa v_i\rig)+a_0$ where $\{a_i\}_{i\in[M]}\subset \bb R$ and $\{\bfa v_i\}\subset \bb R^{d+1}$. We consider the class of $(R, C_{\ell})$-smooth functions \citep{bach2017breaking,bai2024transformers} defined as follows.
\begin{definition}[\citet{bai2024transformers}]\label{rcsmoothfunc}
    A function $g:\bb R^d\to\bb R$ is $(R, C_{\ell})$ smooth if for $s= \lceil(k-1)/2\rceil+2$, $g$ is a $C^s$ function supported on $[-R, R]^k$ such that
    \begin{align*}
        \sup_{\bfa x\in [-R,R]^k}\Vert\nabla^ig(\bfa x)\Vert_{\infty}\leq L_i,
    \end{align*}
    for all $i\in\{0,1,\ldots,s\}$, with $\max_{0\leq i\leq s}L_iR^i\leq C_{\ell}$.
\end{definition}
We then consider the following class of functions
\begin{align*}
    &\ca F_d = \bigg\{f:f(\bfa x)=\int_{\ca W:\Vert\bfa v\Vert=1}\sigmoid\lef([\bfa x^\top,1]\bfa v\rig)d\mu(\bfa v)\bigg\},\text{ with } TV(\mu)=\int_{\ca W:\Vert\bfa v\Vert_2=1}d|\mu(\bfa v)|<C_{\ell}C(f)^d ,
\end{align*}
where $C(f)$ is a constant that depends only on $f$. 
It is further noted that by \citet{bach2017breaking}, we can write that the class of $(R, C_{\ell})$ smooth functions belongs to the above class. Then we prove the following approximation lemma for the sigmoid function, which provides explicit dependence on $B$ and $C$.
 \begin{lemma}\label{sigmoidapprox}
     Suppose $f$ is $(R,C_{\ell})$ smooth. Then there exists a set of points $(\bfa v_1, a_1),\ldots,(\bfa v_M, a_M)\in \ca B(\Vert\cdot\Vert_2, 1)$ that makes the following hold
     \begin{align*}
         \sup_{\bfa x\in \ca B\lef(\Vert\cdot\Vert_\infty,R\rig)}\Big|\frac{1}{C_{\ell}}f(\bfa x)-\frac{1}{M}\sum_{i=1}^Ma_i\sigmoid\lef([\bfa x^\top,1]^\top\bfa v_i\rig)\Big|&\leq  C(f)^d\inf_{\epsilon>0}\Big(2\epsilon+\sqrt{\frac{\log(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)/\delta)}{M}}\Big)\\
         &\lesssim C(f)^d\sqrt{\frac{d}{M}\log\Big(\frac{MR}{d}\Big)},
     \end{align*}
     where we also have $\sum_{i=1}^M|a_i|\leq C(f)^d$.
 \end{lemma}
 \begin{proof}
     The proof goes by the probabilistic method, where we can first use \citet{pisier1981remarques} to show that when we sample from the distribution given by $f(\bfa v)=\frac{|\mu(\bfa v)|}{\int_{\bb S^d}d\mu(\bfa v)}=\frac{|\mu(\bfa v)|}{TV(\mu)}$. Then, under this probability measure, we sample independently in a total of $M$ samples $(\bfa V_1, \ldots, \bfa V_M)$ to obtain that for any $\bfa x\in\ca V$, pointwise
     \begin{align*}
         \int_{\bb R^{d+1}}&\softmaxx\lef([\bfa x^\top, 1]\bfa v\rig)d\mu(\bfa v)- \frac{TV(\mu)}{M}\sum_{i=1}^M\softmaxx\lef([\bfa x^\top, 1]\bfa V_i\rig)\\
         &= TV(\mu)\bl\bb E\Big[\sign(\mu(\bfa v)) \softmaxx\lef([\bfa x^\top,1]\bfa V\rig)\Big]-\frac{1}{M}\sum_{i=1}^M\sign\lef(\mu(\bfa v)\rig)\softmaxx\lef([\bfa x^\top,1]\bfa V_i\rig)\br.
     \end{align*}
     And we have by Hoeffding's inequality,
     \begin{align*}
         \bb P\bl\Big|\bb E\lef[\sign(\mu(\bfa v))\softmaxx\lef([\bfa x^\top, 1]\bfa V\rig)\rig]-\frac{1}{M}\sum_{i=1}^M\sign(\sigma_f(\bfa V_i))\softmaxx\lef([\bfa x^\top, 1]\bfa V_i\rig)\Big|\geq \frac{t}{TV(\mu)}\br\leq\exp\lef(-\frac{CMt^2}{TV(\mu)^2}\rig).
     \end{align*}
     And, by the union bound, we can show that for points in the $\epsilon$-cover of $\ca B(\Vert\cdot\Vert_{\infty},R)$, the following holds
     \begin{align*}
         \bb P\bl\sup_{\bfa x\in \ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)}&\Big|\bb E\Big[\ub{\sign(\sigma_f (\bfa V))\softmaxx\lef([\bfa x^\top, 1]\bfa V\rig)}_{=:F(\bfa V,\bfa x)}\Big]-\frac{1}{M}\sum_{i=1}^M\sign(\sigma_f(\bfa V_i))\softmaxx\lef([\bfa x^\top,1]\bfa V_i\rig)\Big|\geq\frac{t}{TV(\mu)}\br\\
         &\leq |\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)|\exp\lef(-\frac{CMt^2}{TV(\mu)^2}\rig).
     \end{align*}
     Alternatively, we can show the following holds with probability at least $1-\delta$,
     \begin{align*}
         \sup_{\bfa x\in\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)}&\Big|\bb E\lef[\sign(\mu(\bfa v))\softmaxx\lef([\bfa x^\top,1]\bfa V\rig)\rig]-\frac{1}{M}\sum_{i=1}^M\sign(\sigma_f)\softmaxx\lef([\bfa x^\top,1]\bfa V_i\rig)\Big|\\
         &\leq \sqrt{\frac{\log\lef(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)/\delta\rig)}{M}}.
     \end{align*}

     Then we consider generalizing these results to uniform convergence. For $\bfa x$, we denote $\pi(\bfa x)$ as the closest point in the $\epsilon$-cover of $\ca B(\Vert\cdot\Vert_{\infty},R)$ denoted by $\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)$. For function $F$, we can show that for all $\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)$,
     \begin{align*}
         \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\lef|\bb E[F(\bfa V,\bfa x)] - \bb E[F(\bfa V,\pi(\bfa x))]\rig|\leq \lef|\bfa V^\top(\bfa X_1-\bfa X_2)\rig|\leq\Vert\bb E[\bfa V^\top]\Vert_2\Vert\bfa X_1-\bfa X_2\Vert_2\leq \epsilon.
     \end{align*}
     Then we consider the error given by
     \begin{align*}
         \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\bfa x)-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\pi(\bfa x))\Big|\leq\frac{1}{M}\sum_{i=1}^M\epsilon\Vert\bfa V_i\Vert_2\leq\epsilon.
     \end{align*}
    Then the following holds with probability at least $1-\delta$,
     \begin{align*}
         &\sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\bb E\lef[F(\bfa V, \bfa x)\rig]-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\bfa x)\Big|\\
         &\leq \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\bb E\lef[F(\bfa V, \bfa x)\rig]-\bb E\lef[F(\bfa V, \pi(\bfa x))\rig]\Big|+\sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\bfa x)-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i, \pi(\bfa x))\Big|\\
         &+\sup_{\pi(\bfa x)\in\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)}\Big|\bb E\lef[F(\bfa V,\pi(\bfa x))\rig]-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\pi(\bfa x))\Big|\leq\epsilon+\epsilon + \sqrt{\frac{\log(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)/\delta)}{M}}.
     \end{align*}
     Given these results, we show that there exists a set of parameters $\{(\bfa V_i, a_i=TV(\mu)\sign_f(\bfa V_i))\}_{i\in[m]}$ where the following holds
     \begin{align*}
         \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\frac{1}{C_{\ell}}f(\bfa x)-\frac{1}{M}\sum_{i=1}^M a_i\softmaxx\lef([\bfa x^\top, 1]\bfa V_i\rig)\Big|&\lesssim \inf_{\epsilon}\bigg\{\epsilon+\sqrt{\frac{\log(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon))}{M}}\bigg\}\\
         &\lesssim \inf_{\epsilon}\bigg\{\epsilon +\sqrt{\frac{d\log\frac{R}{\epsilon}}{M}}\bigg\}\lesssim\sqrt{\frac{d}{M}\log\lef(\frac{MR}{d}\rig)},
     \end{align*}
     where we already utilize the estimate given by \citet{wu2020information} on the covering number of $L_2$ balls.
 \end{proof}
 \begin{lemma}[Approximating $d$ Dimensional $(R, C_{\ell})$ smooth functions by Softmax Neural Networks]\label{softmaxapprox}
        Consider an element-wise $(R, C_{\ell})$ smooth mapping $\bfa f(\bfa x)=(f_1(\bfa x),\ldots f_k(\bfa x))^\top$ where $\bfa x\in[-R,R]^d$. There exists a set of points $\{(\bfa A_{i}, a_{i})\}_{i\in[M]}$ with $\sup_{i\in[M]}\Vert\bfa A\Vert_2\leq C$ such that the following holds
        \begin{align*}
            \sup_{\bfa x\in\ca B\lef(\Vert\cdot\Vert_\infty, R\rig)}\frac{1}{C_{\ell}}\Big\Vert\bfa f(\bfa x)-\sum_{i=1}^{Md} a_i\softmaxx\lef(\bfa A_i\begin{bmatrix}
                \bfa x\\
1\end{bmatrix}\rig)\Big\Vert_{\infty}\leq C(f)^d\sqrt{\frac{d^2}{M}\log\lef(\frac{MR}{d^2}\rig)}.
        \end{align*}
 \end{lemma}
 \begin{proof}
     Our proof goes by connecting the $\softmax$ activation with the $\sigmoid$ functions. Note that by lemma \ref{sigmoidapprox} we can show that for all $\ell\in[k]$, there exists a set $\{(\bfa v_i^{(\ell)}, a_i^{(\ell)})\}_{\ell\in[M^\prime]}$
     \begin{align*}
         \sup_{\bfa x\in\ca B\lef(\Vert\cdot\Vert_{\infty}, R\rig)}\frac{1}{C_{\ell}}\Big|f_{\ell}(\bfa x)-\sum_{i=1}^{M^\prime}a_i^{(\ell)}\sigmoid\lef(\bfa v_i^{(\ell),\top}\begin{bmatrix}
             \bfa x\\
             1
         \end{bmatrix}\rig)\Big|\leq C(f)^d\sqrt{\frac{d}{M^\prime}\log\lef(\frac{M^\prime R}{d}\rig)}.
     \end{align*}
     Consider the following matrices construction of $\{\bfa B_{i}^{(\ell)}\}_{i\in[d], \ell\in[M]}\subset\bb R^{k\times d}$, given by 
            $\bfa B_{i}^{(\ell)} = \begin{bmatrix}
            \bfa 0_{(\ell-1)\times (d-1)}& 0\\
             \bfa v_{i,[d-1]}^{(\ell),\top}&\log d+\bfa v_{i,d}^{(\ell)}\\
             \bfa 0&\bfa 0
         \end{bmatrix}$. Then we can show that
         \begin{align*}
             \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty})}\bigg\Vert\begin{bmatrix}
                 \bfa 0_{(\ell-1)\times 1}\\
                 f_{\ell}(\bfa x)\\
                 \bfa 0
             \end{bmatrix}-\sum_{i=1}^{M^\prime}a_i^{(\ell)}\softmaxx\lef(\bfa B_i^{(\ell)}\begin{bmatrix}
                 \bfa x\\
                 1
             \end{bmatrix}\rig)\bigg\Vert_{\infty}\leq C(f)^d\sqrt{\frac{d}{M^\prime}\log\Big(\frac{M^\prime R}{d}\Big)},
         \end{align*}
         which completes the proof through noticing that $M^\prime d=M$.
 \end{proof}
    \begin{lemma}[Norm Approximation by Sigmoid Functions]\label{Softmaxapprox1/x}
    Consider the vector $\bfa v\in\bb R^d$. Assume that there exists a constant $C$ with $\Vert\bfa v\Vert_2\leq C$. For $M<\lef(C\frac{\overline R}{\underline R}\rig)^d\frac{1}{\epsilon^2}\log(1+C/\epsilon)$ such that there exists $\{\bfa a_m\}_{m\in[M]}\subset\bb S^{d}$ and $\{c_m\}_{m\in[M]}\subset\bb R$ where for all $\bfa v$ with $\overline R\geq\Vert\bfa v\Vert_2\geq \underline R$, we have 
    \begin{align*}
        \bigg|\sum_{m=1}^Mc_m\sigmoid\lef(\bfa a_m^\top\begin{bmatrix}
            \bfa v\\
            1
        \end{bmatrix}\rig)-\frac{1}{\Vert\bfa v\Vert_2}\bigg|\leq \lef(\frac{C\overline R}{\underline R}\rig)^d\sqrt{\frac{d^2}{M}\log\lef(\frac{MR}{d^2}\rig)}.
    \end{align*}
    % Similarly, there exists a multihead ReLU attention layer with number of heads $M\leq\overline R^{\frac{d}{2}}\frac{C(d)}{\epsilon^2}\log\lef(1+C/\epsilon\rig)$, a set of vectors $\{\bfa b_m\}_{m\in[M]}\subset\bb S^{N-1}$ and $\{d_m\}_{m\in[M]}\subset\bb R$ such that 
    % \begin{align*}
    %     \Big|\sum_{m=1}^Md_m\sigma(\bfa b_m^\top\bfa v)-\Vert\bfa v\Vert_2^{1/2}+1\Big|\leq\epsilon.
    % \end{align*}
\end{lemma}
\begin{proof}
    Consider a set $\ca C^d(\overline R) :=\ca B^d_{\infty}(\overline R)\setminus\ca B_{2}^d(\underline R)$, we note that
    \begin{align*}
        \sup_{\bfa v\in\ca C^d(\overline R)}\pta_{v_{j_1},\ldots,v_{j_i}\in[d]}\bl\frac{1}{\Vert\bfa v\Vert_2}\br\leq\frac{C^d}{\Vert\bfa v\Vert_2^d}\leq\frac{C^d}{\underline R^d}.
    \end{align*}
    Therefore, consider the definition \ref{rcsmoothfunc}, we have $C_{\ell}=\lef(\frac{\overline R}{\underline R}\rig)^d C^d$. Then the result is concluded by lemma \ref{Softmaxapprox1/x}.
\end{proof}

%  After a careful evaluation of the proof of the above lemma, we come up with the following corollary, which provides explicit dependence on the maximum value. 
% \begin{lemma}\label{lm:a1}
%     Suppose $f\in\ca F_d^{2m}$ with $|f|<B_f$, then for all bounded sets $U\subset\bb R^d$, there exists $B(d,f,U)<\infty$
%  and a point $((\bfa v_1,a_1),\ldots(\bfa v_M,a_M))\in [\bb R^{d+1}\times[-B,B]]^{M}$ such that the following holds
%  \begin{align*}
%      \sup_{0\leq \alpha\leq m}\sup_{\bfa x\in U}\Big|D^{\alpha}f(\bfa x)-\frac{1}{M}\sum_{i=1}^Ma_iD^{\alpha}\sigmoid\lef([\bfa x^\top,1]\bfa v_i\rig)\Big|\leq B_f C(d,f,U)M^{-1/2}.
%  \end{align*}
%  \end{lemma}
% where $\mu$ is the Radon measure and $\ca V$ is a compact space and
% \begin{align*}
%     \gamma_2(f) = \inf_{p}\int_{\ca V}|p(\bfa v)|^2d\tau(\bfa v),\quad\text{ s.t. }f(\bfa x)=\int_{\ca V}p(\bfa v)\sigmoid(\bfa v^\top\bfa x)d\tau(\bfa v),
% \end{align*}
% with $\tau$ being a fixed probability measure. Then, we employ the result coming from \citet{bach2017breaking,bai2024transformers} and consider the following smooth function class.
% \begin{definition}[Sufficiently smooth $d$-variate function] We say a function $g:\bb R^d\to\bb R$ is $(R,C_{\ell})$-smooth if for $s=\lceil (d-1)/2\rceil+2$, $g$ is a $C^s$ function on $\ca B_{\infty}^d(R)$, and
% \begin{align*}
%     \sup_{z\in\ca B_{\infty}^d(R)}\Vert\nabla^i g(\bfa z)\Vert_{\infty}=\sup_{\bfa z\in\ca B_{\infty}^d(R)}\max_{j_1,\ldots, j_i\in[d]}\lef|\partial_{x_{j_1}\ldots x_{j_i}}g(\bfa x)\rig|\leq L_i
% \end{align*}
% for all $i\in\{0,1,\ldots, s\}$ with $\max_{0\leq i\leq s}L_iR^i\leq C_{\ell}$. 
% \end{definition}
% Then, using \citep{bach2017breaking}, Proposition 5 we can show that $(R, C_{\ell})$-smooth function is representable as follows.
% \begin{lemma}[Lemma A.5 in \citet{bai2024transformers}]
%     Suppose function $g:\bb R^d\to\bb R$ is $(R, C_{\ell})$ smooth. Then there exists a signed measure $\mu$ over $\ca V:=\{\bfa w\in\bb R^{d+1},\Vert\bfa w\Vert_2=1\}$ such that
%     \begin{align*}
%         g(\bfa x)=\int_{\ca V}\frac{}{}
%     \end{align*}
% \end{lemma}


\section{Omitted Proofs}

\subsection{Proof of Theorem \ref{thm_main1}}
The proof of the Maximization step follows directly from that of the proof of theorem \ref{thm_main2}. We only change the proof of the Expectation Step for the Transformer layers.
    \begin{center}
    \textbf{1. The Expectation Step.}
\end{center}
For the expectation step, our network is designed by
\begin{align*}
    \bfa V^{(1)}_1&=\begin{bmatrix}
        \bfa 0_{(3d+k+1)\times (D-d)}&\bfa 0\\
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa Q^{(1)}_1=\begin{bmatrix}
    \bfa 0_{k\times 2d}&I_k&\bfa 0\\       
        &\bfa 0&
    \end{bmatrix},\quad\bfa K^{(1)}_1=\begin{bmatrix}
        \bfa 0_{d\times(2d+k)}& I_{d}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa V^{(1)}_2&=\begin{bmatrix}
        \bfa 0_{(3d+k+1)\times (D-d)}&\bfa 0\\
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa Q^{(1)}_2=\bfa 0,\quad\bfa K^{(1)}_1=\begin{bmatrix}
        \bfa 0_{d\times(2d+k)}& I_{d}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
    % \bfa V_2^{(1)} &= \begin{bmatrix}
    %     \bfa 0_{d\times d}&\bfa 0&\bfa 0\\
    %     \bfa 0_{d\times d}&-I_d &\bfa 0\\
    %     &\bfa 0&
    % \end{bmatrix},\quad\bfa Q_2^{(1)}=\bfa K_2^{(1)}=\begin{bmatrix}
    %     \bfa 0_{d\times d}&\bfa 0&\bfa 0\\
    %     \bfa 0_{d\times d}&-I_d &\bfa 0\\
    %     &\bfa 0&
    % \end{bmatrix}.
\end{align*}
Then we can show that
\begin{align*}
    \bfa V_1^{(1)}\bfa H=\begin{bmatrix}
        &\bfa 0_{d\times N}&\\
        \bfa X_1&\ldots&\bfa X_N\\
        &\bfa 0&
    \end{bmatrix}, \quad\bfa Q_1^{(1)}\bfa H_1=\begin{bmatrix}
        (2\log N)\bfa p_{1,1}^{(0)}&\ldots&(2\log N)\bfa p_{1,N}^{(0)}\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa K_1^{(1)}\bfa H_1=\begin{bmatrix}
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix}.
\end{align*}
Hence we can show that
\begin{align*}
   \softmax\Big( (\bfa Q_1^{(1)}\bfa H_1)^\top(\bfa K_1^{(1)}\bfa H_1)\Big)=\softmax\lef(\begin{bmatrix}
       C(\log N)\bfa p_{1,1}^{(0),\top}&\bfa 0\\
       \vdots&\vdots\\
       C(\log N)\bfa p_{1,N}^{(0),\top}&\bfa 0
   \end{bmatrix}\rig).
\end{align*}
We further note that
\begin{align*}
    \bigg|\frac{\exp(C\log N)}{\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=j}\exp(C\log N)+\mbbm 1_{\wh z_i^{(0)}\neq j}}-\frac{1}{\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=j}}\bigg|\leq N^{-C}.
\end{align*}
Similarly we have
\begin{align*}
    \bigg|\frac{1}{\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=j}\exp(C\log N)+\mbbm 1_{\wh z_i^{(0)}\neq j}}\bigg|\leq N^{-C}.
\end{align*}
We further note that
\begin{align*}
    \softmax\Big( (\bfa Q_1^{(1)}\bfa H_1)^\top(\bfa K_1^{(1)}\bfa H_1)\Big)=\begin{bmatrix}
        \frac{1}{N}&\ldots&\frac{1}{N}\\
        \vdots &\ddots&\vdots\\
        \frac{1}{N}&\ldots&\frac{1}{N}
    \end{bmatrix}.
\end{align*}
Hence one can show that
\begin{align*}
    \lef\Vert\bfa H\sum_{i=1}^2\softmax\lef(\bfa H^\top\bfa Q_i^{(1),\top}\bfa K_i^{(1)}\bfa H\rig)-\begin{bmatrix}
        \wha\mu_1^{(1)}&\ldots&\wha\mu_k^{(1)}&\bfa 0\\
        &\bfa 0
    \end{bmatrix}\rig\Vert_2\leq N^{-1}.
\end{align*}
Therefore, applying $\bfa V_1$ we have
\begin{align*}
   \lef\Vert\sum_{i=1}^2\bfa V_i^{(1)}\bfa H \cdot\softmax\lef(\bfa H^\top\bfa Q_i^{(1),\top}\bfa K_i^{(1)}\bfa H\rig)- \begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\wha \mu_{k}^{(0)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        &&\bfa 0&&&
    \end{bmatrix}\rig\Vert_2\leq N^{-1}.
\end{align*}
% And we can show that under this construction,
% \begin{align*}
%    \bfa V^{(5)}_1\bfa H_4 (\bfa Q^{(5)}_1\bfa H_4)^\top&=\begin{bmatrix}
%        &&\bfa 0_{d}\\\wha\mu_1^{(1)}&\ldots&\wha\mu_k^{(1)}&\bfa 0\\
%        &&\bfa 0&
%    \end{bmatrix},\quad \bfa K^{(5)}\bfa H_4=\begin{bmatrix}
%         I_d&\bfa 0\\
%         \bfa 0&\bfa 0
%     \end{bmatrix},\\
%     \bfa V^{(5)}_2\bfa H_4(\bfa Q_2^{(5)}\bfa H_4)^\top(\bfa K_2^{(5)}\bfa H_4)&=\begin{bmatrix}
%         &&\bfa 0_{d}&\\
%         \wha\mu_1^{(0)}&\ldots&\wha\mu_k^{(0)}&\bfa 0\\
%         &&\bfa 0&
%     \end{bmatrix}.
% \end{align*}
% And, we can show that
% \begin{align*}
%     \bfa H_{5,1} = \bfa H_4 +\bfa V^{(5)}\bfa H_4(\bfa Q^{(5)}\bfa H_4)^\top(\bfa K^{(5)}\bfa H_4)=\begin{bmatrix}
%         \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
%         \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
%         \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1&\ldots&1\\
%         &&\bfa 0&&&
%     \end{bmatrix}.
% \end{align*}
And the Expectation Step is concluded as we updates the centroids from $\{\bfa\mu_i^{(0)}\}_{i\in[k]}$ to $\{\bfa\mu_i^{(1)}\}_{i\in[k]}$. The next step is to update the assignment $\bfa p_{1,i}^\prime$ to $\bfa p_{1,i}^{(1)}$ for $i\in[N]$.
\subsection{Proof of Theorem \ref{thm_main2}}
We first consider the input matrix to be 
\begin{align*}
    \bfa H_1:=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_n\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa p_{3,1}&\bfa p_{3,2}&\ldots&\bfa p_{3,N}
    \end{bmatrix}\in\bb R^{D\times N}, 
\end{align*}
where $\wh z^{(0)}:[n]\to[k]$ is the assignment function, $\wha\mu_i\in\bb R^{d}$ is the initially estimated centroid for the $i$-th cluster. $\bfa p_{1,i}\in\bb R^k$ satisfies $\bfa p_{1,i,j}=\mbbm 1_{\wh z^{(0)}(i)=j}$ for all $j\in[k]$. And for $\bfa p_{2,i}$ we have $\bfa p_{2,i,j} = \mbbm 1_{j=i}$ for $i\leq d$ and $\bfa p_{2,i,j}=0$ for $i\leq N$ and $j\leq d$. We let $\bfa p_{3,1}=\bfa p_{3,2}=\ldots=\bfa p_{3,N} = \bfa 0\in\bb R^k$.
We note that algorithm \ref{alg:loyld} consists of two iterative steps: (1) The expectation step where we take the averages to get an initial estimate $\wha \mu_{\ell}^{(t)}$. (2) The maximization step where we assign each individual sample their labels. Our following discussions simulate the two steps separately as follows.
\begin{center}
    \textbf{1. The Expectation Step.}
\end{center}
To achieve the first step, we construct our transformer weights as follows:
\begin{align*}
    \bfa V_1^{(1)}=\begin{bmatrix}
        \tda V_{1,1}^{(1)}&\tda V_{1,2}^{(1)}&\tda V_{1,3}^{(1)}
    \end{bmatrix},\quad\bfa Q_1^{(1)}=\begin{bmatrix}
        \bfa 0_{1\times(3d+k)}&1&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa K_1^{(1)}=\begin{bmatrix}
        \bfa 0_{1\times(3d+k)}&1&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix},
\end{align*}
where $\tda V_{1,1}^{(1)}\in\bb R^{2d\times D}=\bfa 0$, $\tda V_{1,2}^{(1)}=\begin{bmatrix}
        \bfa 0_{3d+k}\\
        I_k\\
        \bfa 0
    \end{bmatrix}\in\bb R^{k\times D}$. Then we can show that
\begin{align*}
   \lef(\bfa K_1^{(1)}\bfa H_1\rig)^\top=\lef(\bfa Q_1^{(1)}\bfa H\rig)^\top = \begin{bmatrix}
        1&\bfa 0\\
        \vdots &\bfa 0\\
        1&\bfa 0
    \end{bmatrix}.
\end{align*}
Then we can show that
\begin{align*}
   (\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H) = \begin{bmatrix}
       \bfa v_1&\ldots&\bfa v_1
   \end{bmatrix},\quad \bfa v_{1,i} = 1\quad\forall i\in[N].
\end{align*}
And after the Softmax function we obtain that
\begin{align*}
    \softmax\lef((\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H)\rig)=\begin{bmatrix}
        \frac{1}{D}\bfa v_1&\ldots&\frac{1}{D}\bfa v_1
    \end{bmatrix}.
\end{align*}
Hence, we further obtain that
\begin{align*}
    \bfa V_1^{(1)}\bfa H&\times\softmax((\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H))=\bfa V_1\bfa H\times\begin{bmatrix}
        \frac{1}{D}\bfa v_1 &\ldots&\frac{1}{D}\bfa v_1
    \end{bmatrix}=\bfa V_1\times \begin{bmatrix}
        &\bfa A_0&\\
        \frac{1}{D}\bfa v_k &\ldots&\frac{1}{D}\bfa v_k\\
        &\bfa A_1&
    \end{bmatrix}\\
    &=\begin{bmatrix}
        \tda V_{1,1}^{(1)}&\tda V_{1,2}^{(1)}&\tda V_{1,3}^{(1)}
    \end{bmatrix}\begin{bmatrix}
        &\bfa A_0&\\
        \frac{1}{D}\bfa v_k&\ldots&\frac{1}{D}\bfa v_k\\
        &\bfa A_1&
    \end{bmatrix}=\tda V_{1,2}^{(1)}\begin{bmatrix}
        \frac{1}{D}\bfa v_k &\ldots&\frac{1}{D}\bfa v_k
    \end{bmatrix}\\
    &=\begin{bmatrix}
        \bfa 0_{3d+k}\\
        I_k\\
        \bfa 0
    \end{bmatrix}\begin{bmatrix}
        \frac{1}{D}\bfa v_k&\ldots&\frac{1}{D}\bfa v_k
    \end{bmatrix}=\begin{bmatrix}
        &\bfa 0_{3d+k}&\\
        \frac{1}{D}\bfa v_k&\ldots&\frac{1}{D}\bfa v_k\\
        &\bfa 0&
    \end{bmatrix},
\end{align*}
 where $\bfa A_0\in\bb R^{2d\times N}$ and $
    \bfa v_{k,\ell}=\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=\ell}$.
Then it is checked that
\begin{align*}
   \bfa H_2 =  \bfa H_1+ \bfa V_1^{(1)}\bfa H_1\times\softmax\lef((\bfa Q_1^{(1)}\bfa H_1)^\top(\bfa K_1^{(1)}\bfa H_1)\rig)=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \frac{1}{D}\bfa v_k&\frac{1}{D}\bfa v_k&\ldots&\frac{1}{D}\bfa v_k\\
        &&\bfa 0&
    \end{bmatrix}.
\end{align*}
Therefore, we construct the following multi-head layer to remove the off-diagonal elements in $\begin{bmatrix}
    \frac{1}{D}\bfa v_k&\frac{1}{D}\bfa v_k&\ldots&\frac{1}{D}\bfa v_k
\end{bmatrix}$, given by 
\begin{align*}
    \bfa V_{2i}^{(2)} &= -\bfa V_{2i+1}^{(2)}=\frac{N}{N-1}\begin{bmatrix}
        &\bfa 0_{(3d+2k+i)\times D}&\\
        \bfa 0_{1\times(3d+2k+i)}&1&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\ 
    {\bfa Q_{2i}^{(2)}}=\bfa K_{2i}^{(2)} &= \begin{bmatrix}
    &\bfa 0_{i-1}&
        \bfa 0_{1\times(2d+k)}&1&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa K_{2i+1}^{(2)} =\bfa 0,\quad\text{ for }i\in[k].
\end{align*}
Given this formulation, we can show that the mapping $f(\bfa x)= 1$ is $(1,1)$ smooth
\begin{align*}
    \lef(\bfa Q_{2i}^{(2)}\bfa H\rig)^\top\lef(\bfa K_{2i}^{(2)}\bfa H\rig) =\begin{bmatrix}
        \bfa 0_{(i-1)\times 1} &\bfa 0_{1\times (D-1)}\\
        N&\bfa 0_{1\times (D-1)}\\
        0&\bfa 0
    \end{bmatrix}\begin{bmatrix}
        \bfa 0_{1\times (i-1)} &N &\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix}=\begin{bmatrix}
        &\bfa 0_{(i-1)\times N}&\\
        \bfa 0_{1\times(i-1)}&N^2&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
\end{align*}
After the Softmax function, we have
\begin{align*}
   \softmax \lef((\bfa Q_{2i}^{(2)}\bfa H)^\top(\bfa K_{2i}^{(2)}\bfa H)\rig)=\begin{bmatrix}
       \frac{1}{N}&\ldots &\frac{1}{\exp(N^2)+N-1}&\cdots&\frac{1}{N}\\
       \vdots&\ddots&\vdots&\cdots&\vdots\\
       \frac{1}{N}&\cdots &\frac{\exp(N^2)}{\exp(N^2)+N-1}&\cdots&\frac{1}{N}\\
       \vdots&\ddots&\vdots&\ddots&\vdots\\
       \frac{1}{N}&\cdots&\frac{1}{\exp(N^2)+N-1}&\cdots&\frac{1}{N}
   \end{bmatrix}.
\end{align*}
And similarly we can show that $(\bfa Q_{2i+1}^{(2)}\bfa H)^\top(\bfa K_{2i+1}^{(2)}\bfa H)=\bfa 0$, which implies that
\begin{align*}
    \softmax\lef((\bfa Q_{2i+1}^{(2)}\bfa H)^\top(\bfa K_{2i+1}^{(2)}\bfa H)\rig)=\begin{bmatrix}
        \frac{1}{N}&\ldots&\frac{1}{N}\\
        \vdots&\ddots&\vdots\\
        \frac{1}{N}&\ldots&\frac{1}{N}
    \end{bmatrix}.
\end{align*}
We notice that after the softmax, only the $i$-th column remains nonzero, where the value for its $i$-th row is given by
\begin{align*}
    \Big|\frac{\exp(N^2)-1}{\exp(N^2)+D-1}-1\Big|\lesssim D\exp(-N^2).
\end{align*}
Hence, the following holds
\begin{align*}
    \softmax&\lef((\bfa Q_{2i}^{(2)}\bfa H)^\top(\bfa K_{2i}^{(2)}\bfa H)\rig)-\softmax\lef((\bfa Q_{2i+1}^{(2)}\bfa H)^\top(\bfa K_{2i+1}^{(2)}\bfa H)\rig)\\
    &=\begin{bmatrix}
        \bfa 0_{(i-1)\times (i-1)}&\bfa 0_{(i-1)\times 1}&\bfa 0_{(i-1)\times(N-1)}\\
        \bfa 0_{1\times(i-1)}&\frac{N-1}{N}+O\lef( D\exp\lef(-N^2\rig)\rig)&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix}\\
    &=\frac{N-1}{N}\begin{bmatrix}
        &\bfa 0_{(3d+2k+i)\times D}&\\
        \bfa 0_{(i-1)}&\bfa v_{k,i} +O(D\exp(-N^2))&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},
\end{align*}
% Hence, we can further show that
% \begin{align*}
%     \bfa V_{i}^{(2)}\bfa H_2\sigma((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)),
% \end{align*}
which immediately implies that
\begin{align*}
    \sum_{i=1}^{2k}\bfa V_i^{(2)}\bfa H_2\softmax\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
        \bfa 0_{(3d+2k)\times N}&\\
        \diag(\bfa v_k) +O(D\exp(-N^2))&\bfa 0\\
        \bfa 0
    \end{bmatrix}.
\end{align*}
Given the above design, we can show that
\begin{align*}
    \bfa H_{3,1}&=\bfa H_2+\sum_{i=1}^k\bfa V_i^{(2)}\bfa H_2\softmax\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)\\
    &=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k)+O(D\exp(-N^2))&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
Then, we construct the MLP layer to remove the $\bfa v_k$ part, which is designed by
\begin{align*}
    \bfa W_1^{(2)}=I_D,\quad \bfa W_2^{(2)}= \begin{bmatrix}
        &\bfa 0_{(3d+k)\times D}&&\\
        \bfa 0_{k\times (3d+k)}&-I_k&I_k&\bfa 0_{}\\
       \bfa 0 &-I_k&\bfa 0&\bfa 0\\
       &\bfa 0&&
    \end{bmatrix}.
\end{align*}
Given this formulation, we can show that
\begin{align*}
    \bfa H_3 := \bfa H_{3,1}+\bfa W_1^{(2)}\sigma\lef(\bfa W_2^{(2)}\bfa H_{3,1}\rig)=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k)+O(D\exp(-N^2))&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}

% Then we consider the following attention head which essentially remove the $\bfa v_k$ part in $\bfa H_2$. We note that the rank of $\sigma((\bfa Q_i\bfa H_2)^\top(\bfa K_i\bfa H_2))$ is at most $D$. To form the 
% \begin{align*}
%     \bfa V_{k+1}^{(2)} = \begin{bmatrix}
%         &\bfa 0_{(3d+k)\times D}&\\
%         \bfa 0_{k\times (3d+k)}&-I_k&\bfa 0_{}\\
%         &\bfa 0&
%     \end{bmatrix},\quad \bfa Q_{k+1}^{(2)}=,\quad \bfa K_{k+1}^{(2)}=
% \end{align*}
% Given the above design, we can show that
% \begin{align*}
%     \bfa V_{k+1}^{(2)}\bfa H_2\sigma\lef((\bfa Q_{k+1}^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
%         &\bfa 0_{(3d+k)\times N}&\\
%         -\bfa v_k&\ldots&-\bfa v_k\\
%         &\bfa 0&
%     \end{bmatrix}.
% \end{align*}
% Therefore, collecting pieces, we can show that
% \begin{align*}
%    \bfa H_3:= \bfa H_2 + \sum_{i=1}^{k+1}\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
%         \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \diag(\bfa v_k)&&\bfa 0&\\
%         \bfa p_{4,1}& \bfa p_{4,2}&\ldots &\bfa p_{4,N}
%     \end{bmatrix}.
% \end{align*}
The following layer converts the term $\diag(\bfa v_k)$ to $\diag(\bfa v_k^\prime)$ where $\bfa v_{k,i}^\prime = 1/\bfa v_{k,i}$. The design is given as follows for all $i\in[M]$,
\begin{align*}
    \bfa V_{i}^{(3)} &= \begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k\times(2d+2k)}&\diag(c_i)_{k\times k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa Q_{i}^{(3)}=\begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k\times(2d+2k)} &I_{k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa K_{i}^{(3)}&=\begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k \times (3d+3k+1)}&\diag(a_i)_{k\times k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}\begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
       \bfa 0_{k\times(3d+2k+1)}&I_{k} &\bfa 0\\
       &\bfa 0&
    \end{bmatrix},
\end{align*}
where we show in lemma \ref{sigmoidapprox} that there exists set of values $\{(c_i,a_i)\}_{i\in[M]}$ such that the following holds for all $\underline R\leq x\leq \overline R$,
\begin{align*}
    \bigg\Vert\sum_{i=1}^Mc_i\sigmoid(a_ix) - \frac{1}{x}\bigg\Vert_2\leq\lef(\frac{C\overline R}{\underline R}\rig)\sqrt{\frac{\log \lef(M \overline R\rig)}{M}}.
\end{align*}
Using the above result, we immediately obtain that
\begin{align*}
    \bfa H_{4,1}:&=\bfa H_3 + \sum_{i=1}^M\bfa V_i^{(3)}\bfa H_3\softmax\lef((\bfa Q_i^{(3)}\bfa H_3)^\top(\bfa K_i^{(3)}\bfa H_3)\rig)\\
    &= \begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k)&&\bfa 0&\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}+C\sqrt{\frac{\log(M D)}{M}},
\end{align*}
where $\bfa v_{k,i}^\prime = \bfa v_{k,i}^{-1}$. Then we apply the MLP again with the following design 
\begin{align*}
    \bfa W_1^{(4)}= I_D,\quad\bfa W_2^{(4)}=\begin{bmatrix}
        &\bfa 0_{(3d+k)\times D}&&\\
        \bfa 0_{k\times(3d+k)}&-I_k&I_k&\bfa 0\\
        \bfa 0&-I_k&\bfa 0&\bfa 0\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
The above construction implies that
\begin{align*}
    \bfa H_4 =\bfa W_2^{(3)}\sigma\lef(\bfa W_1^{(3)}\bfa H_3\rig) = \begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        &\bfa 0&&
    \end{bmatrix}.
\end{align*}
We construct the following layer to perform the normalization, given by 
\begin{align*}
    \bfa V_i^{(4)}&= \begin{bmatrix}
    &\bfa 0_{(3d+2k+1)\times D}& \\
    \bfa 0_{k\times(3d+k+1)}&I_{k}&\bfa 0\\
    &\bfa 0&
    \end{bmatrix},\quad \bfa Q_i^{(4)} = \begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa 0_{k\times 3d}&I_{k}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa K_i^{(4)}&=\begin{bmatrix}
        &\bfa 0_{(3d+1)\times D}&\\
        \bfa 0_{d\times (3d+2k+1)}&\bfa A_i&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}\begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa 0_{k\times (3d+1)}&I_k&\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
\end{align*}
Then, using the approximation bound for the Softmax mapping in lemma \ref{Softmaxapprox1/x}, we can show that there exists $\{(\bfa A_i,c_i)\}_{i\in[M]}$ such that
\begin{align*}
    \sum_{i=1}^Mc_i\softmax\lef((\bfa Q_i^{(4)}\bfa H_4)^\top(\bfa K_i^{(4)}\bfa H_4)\rig) = \begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa p_{1,1}&\ldots&\bfa p_{1,N}\\
        &\bfa 0
    \end{bmatrix}+O\bigg(C^d\sqrt{\frac{d}{M}\log\lef(\frac{M}{d^2}\rig)}\bigg).
\end{align*}
And we also have
\begin{align*}
    \bfa V_2^{(4)}\bfa H_4 = \begin{bmatrix}
        &\bfa 0_{(3d+3k+1)\times D}&\\
        \bfa 0_{k\times(3d+2k+1)}&\diag(\bfa v_k^\prime) +C\sqrt{\frac{\log(MD)}{M}}&\bfa 0\\
        &\bfa 0&
\end{bmatrix},
\end{align*}
which implies that
\begin{align*}
    \bfa H_{4,1} &= \bfa H_3+\sum_{i=1}^M\bfa V_i^{(4)}\bfa H_3\times\softmax\lef((\bfa Q_i^{(4)}\bfa H_3)^\top(\bfa K_i^{(3)}\bfa H_3)\rig)\\
    &=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_N\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        \diag(\bfa v_k^\prime)&&\bfa 0&\\
        \bfa p_{1,1}^\prime&\bfa p_{1,2}^\prime&\ldots&\bfa p_{1,N}^\prime
    \end{bmatrix}+O\bigg(C^d\sqrt{\frac{d}{M}\log\lef(\frac{M}{d^2}\rig)}\bigg),
\end{align*}
where $\bfa p_{1,i}^\prime = \diag(\bfa v_k^\prime)\bfa p_{1,i}$ for all $i\in[N]$. We therefore construct an MLP layer to replace the $\bfa p_{1}$ part using the following design
\begin{align*}
    \bfa W_1^{(4)}=I_D,\qquad\bfa W_2^{(4)} = \begin{bmatrix}
        &&\bfa 0_{2d\times D}&&\\
        \bfa 0_{k\times 2d}&-I_k&\bfa 0_{d\times d}&I_k&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0&\bfa 0&\bfa 0\\
        \bfa 0_{k\times 2d}&-I_k&\bfa 0&\bfa 0 &\bfa 0\\
        &&\bfa 0_{(d+1)\times D}
    \end{bmatrix},
\end{align*}
which immediately leads to 
\begin{align*}
    \bfa H_4 = \bfa W_1^{(4)}\sigma\lef(\bfa W_2^{(4)}\bfa H_3\rig) +\bfa H_3= \begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\wha \mu_{k}^{(0)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        &&\bfa 0&
    \end{bmatrix}+O\bigg(C^d\sqrt{\frac{d}{M}\log\lef(\frac{M}{d^2}\rig)}\bigg).
\end{align*}
Then the next layer is designed as follows
\begin{align*}
    \bfa V^{(5)}_1&=\begin{bmatrix}
        \bfa 0_{d\times (D-d)}&\bfa 0\\
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa Q^{(5)}_1=\begin{bmatrix}
    \bfa 0_{k\times 2d}&I_k&\bfa 0\\       
        &\bfa 0&
    \end{bmatrix},\quad\bfa K^{(5)}_1=\begin{bmatrix}
        \bfa 0_{d\times(2d+k)}& I_{d}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa V_2^{(5)} &= \begin{bmatrix}
        \bfa 0_{d\times d}&\bfa 0&\bfa 0\\
        \bfa 0_{d\times d}&-I_d &\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa Q_2^{(5)}=\bfa K_2^{(5)}=\begin{bmatrix}
        \bfa 0_{d\times d}&\bfa 0&\bfa 0\\
        \bfa 0_{d\times d}&-I_d &\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
\end{align*}
And we can show that under this construction,
\begin{align*}
   \bfa V^{(5)}_1\bfa H_4 (\bfa Q^{(5)}_1\bfa H_4)^\top&=\begin{bmatrix}
       &&\bfa 0_{d}\\\wha\mu_1^{(1)}&\ldots&\wha\mu_k^{(1)}&\bfa 0\\
       &&\bfa 0&
   \end{bmatrix} +O\bigg(C^d\sqrt{\frac{d}{M}\log\lef(\frac{M}{d^2}\rig)}\bigg),\quad \bfa K^{(5)}\bfa H_4=\begin{bmatrix}
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\\
    \bfa V^{(5)}_2\bfa H_4(\bfa Q_2^{(5)}\bfa H_4)^\top(\bfa K_2^{(5)}\bfa H_4)&=\begin{bmatrix}
        &&\bfa 0_{d}&\\
        -\wha\mu_1^{(0)}&\ldots&\wha\mu_k^{(0)}&\bfa 0\\
        &&\bfa 0&
    \end{bmatrix}.
\end{align*}
And, we can show that
\begin{align*}
    \bfa H_{5,1} = \bfa H_4 +\bfa V^{(5)}\bfa H_4(\bfa Q^{(5)}\bfa H_4)^\top(\bfa K^{(5)}\bfa H_4)=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        &&\bfa 0&&&
    \end{bmatrix}.
\end{align*}
And the Expectation Step is concluded as we update the centroids from $\{\bfa\mu_i^{(0)}\}_{i\in[k]}$ to $\{\bfa\mu_i^{(1)}\}_{i\in[k]}$. The next step is to update the assignment $\bfa p_{1,i}^\prime$ to $\bfa p_{1,i}^{(1)}$ for $i\in[N]$.
\begin{center}
    \textbf{2. The Maximization Step.}
\end{center}
The maximization step involves two sub-networks:
\textbf{Step 1:} Copy the $\bfa x$ below the $1$s, yielding $$\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        \bfa x_{1,1}&\bfa x_{2,1}&\ldots&\bfa x_{k,1}&\ldots&\bfa x_{N,1}\\
        &&\vdots\\
        \bfa x_{1,k}&\bfa x_{2,k}&\ldots&\bfa x_{k,k}&\ldots&\bfa x_{N,k}\\
        &&\bfa 0
    \end{bmatrix};$$
\textbf{Step 2:} Move $\{\wha\mu_i^{(1)}\}_{i\in[M]}$ to $\{\bfa x_{j,i}\}_{j\in[N]}$, yielding
$$\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        \bfa x_{1,1}-\wha\mu_1^{(1)}&\bfa x_{2,1}-\wha \mu_1^{(1)}&\ldots&\bfa x_{k,1}-\wha\mu_1^{(1)}&\ldots&\bfa x_{N,1}-\wha\mu_1^{(1)}\\
        &&\vdots\\
        \bfa x_{1,k}-\wha\mu_k^{(1)}&\bfa x_{2,k}-\wha\mu_k^{(1)}&\ldots&\bfa x_{k,k}-\wha\mu_k^{(1)}&\ldots&\bfa x_{N,k}-\wha\mu_k^{(1)}\\
        &&\bfa 0
    \end{bmatrix};$$ \textbf{Step 3:} One-by-one, compute the norm and obtain the following matrix
    $$\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        &&\bfa 0_{k\times N}
        \\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
         \Vert\bfa x_{1,1}-\wha\mu_1^{(1)}\Vert_2&\Vert\bfa x_{2,1}-\wha \mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa x_{k,1}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa x_{N,1}-\wha\mu_1^{(1)}\Vert_2\\
        &&\vdots\\
        \Vert\bfa x_{1,k}-\wha\mu_k^{(1)}\Vert_2&\Vert\bfa x_{2,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa x_{k,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa x_{N,k}-\wha\mu_k^{(1)}\Vert_2\\
        &&\bfa 0
    \end{bmatrix};$$ \textbf{Step 4:} And finally, we apply the Softmax and recover approximates to $\{\tda p_{1,i}^{(2)}\}_{i\in[N]}$. Then we move it back to the original places belonging to $\{\bfa p_{1,i}^\prime\}_{i\in[k]}$. Then we give the following construction for each step:
    \begin{center}
        \textbf{Step 1.}
    \end{center}
    In step 1, we construct the following parameters
    \begin{align*}
        \bfa W^{(5)}_1=\begin{bmatrix}
            \bfa 0_{(3d+k+1)\times N}\\
            I_d&\bfa 0\\
            \bfa 0
        \end{bmatrix},\quad\bfa W_2^{(5)} = I_D.
    \end{align*}
    And the Attention layer of the $6$-th layer makes an identity mapping.
    Then we consider 
    \begin{align*}
        \bfa W^{(6)}_1=\begin{bmatrix}
            \bfa 0_{(3d+k+1)\times N}\\
            I_d&\bfa 0\\
            \bfa 0
        \end{bmatrix},\quad \bfa W_2^{(5)}=-I_D.
    \end{align*}
    Similarly we define $\lef\{\bfa W_1^{(i)},\bfa W_2^{(i)}\rig\}_{i\in[7:6+2(k-1)]}$, and the task is achieved. Hence we show that
    \begin{align*}
        \bfa H_{4+2k}=\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        \bfa x_{1,1}&\bfa x_{2,1}&\ldots&\bfa x_{k,1}&\ldots&\bfa x_{N,1}\\
        &&\vdots\\
        \bfa x_{1,k}&\bfa x_{2,k}&\ldots&\bfa x_{k,k}&\ldots&\bfa x_{N,k}\\
        &&\bfa 0
    \end{bmatrix}.
    \end{align*}
    \begin{center}
        \textbf{Step 2.}
    \end{center}
    To achieve step 2, we note that the following holds
    \begin{align*}
    \tda V^{(j)}\bfa H_{4+2k}\tda G^{(j)}= \begin{bmatrix}
        &&\bfa 0_{k+3d+j}\\
        -\wha\mu_j^{(1)}&-\wha \mu_j^{(1)}&\ldots&-\wha\mu_j^{(1)}\\
        &&\bfa 0\\
    \end{bmatrix}=\Delta\bfa H_{0},
    \end{align*}
    where 
    \begin{align*}
        \tda V^{(j)} =\begin{bmatrix}
            &\bfa 0_{(3d+k+j)\times D}&\\
            0_{d\times 1}&I_d&\bfa 0\\
            &\bfa 0&
        \end{bmatrix},\quad\tda G=\begin{bmatrix}
            1&\ldots&1\\
            &\bfa 0&
        \end{bmatrix}.
    \end{align*}
    Hence, using lemma \ref{softmaxapprox} to show that there exists a set of parameter $\lef\{\bfa V_i^{(5+2k)},\bfa Q_i^{(5+2k)},\bfa K_i^{(5+2k)}\rig\}_{i\in[M}$ given as follows for $i\in[M]$,
    \begin{align*}
        \bfa V_i^{(5+2k)}=c_i\tda V,\quad \bfa Q_i^{(5+2k)} = \begin{bmatrix}
            \bfa 0_{(2d+k)\times d}&I_d&\bfa 0\\
            &\bfa 0&
        \end{bmatrix},\quad \bfa K_i^{(5k+2)} =\begin{bmatrix}
            \bfa A_i&\bfa 0\\
            \bfa 0&\bfa 0
        \end{bmatrix}\times\begin{bmatrix}
            \bfa 0_{1\times (3d+k+1)}&1&\bfa 0\\
            &\bfa 0&
        \end{bmatrix}.
    \end{align*}
    And one can show that
    \begin{align*}
        \bigg\Vert\Delta \bfa H_1- \sum_{i=1}^M\bfa V_i\bfa H^{(4+2k)}\softmax\lef(\bfa H_i^{(2k+4),\top}\bfa Q_i^\top\bfa K_i^{(2k+4)}\bfa H^{(4+2k)}_i\rig)\bigg\Vert_{2}\leq \Vert\wha\mu_1^{(1)}\Vert_2\sqrt{\frac{d}{M}\log(M)}.
    \end{align*}
    Hence, we design accordingly the next $(k-1)M$ heads $\lef\{\bfa V_i^{(2k+4)},\bfa Q_i^{(2k+4)},\bfa K_i^{(2k+4)}\rig\}_{j\in[2:k]}$ similarly. We also let all the FC layer preserve their identity maps. The above construction implies that
    \begin{align*}
        \Big\Vert\sum_{i=1}^k\Delta \bfa H_i-\sum_{i=1}^{kM}\bfa V_i\bfa H^{(4+2k)}\softmax\lef(\bfa H_i^{(2k+4),\top}\bfa Q_i^\top\bfa K_i^{(2k+4)}\bfa H_i\rig)\Big\Vert_2\leq \sqrt k\sup_{j\in[k]}\lef\Vert\wha\mu_j^{(1)}\rig\Vert_2\sqrt{\frac{d\log (M)}{M}},
    \end{align*}
    which alternatively implies that
    \begin{align*}
       \lef\Vert \bfa H_{4+3k}-\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        \bfa x_{1,1}-\wha\mu_1^{(1)}&\bfa x_{2,1}-\wha\mu_1^{(1)}&\ldots&\bfa x_{k,1}-\wha\mu_1^{(1)}&\ldots&\bfa x_{N,1}-\wha\mu_1^{(1)}\\
        &&\vdots\\
        \bfa x_{1,k}-\wha\mu_k^{(1)}&\bfa x_{2,k}-\wha\mu_k^{(1)}&\ldots&\bfa x_{k,k}-\wha\mu_k^{(1)}&\ldots&\bfa x_{N,k}-\wha\mu_k^{(1)}\\
        &&\bfa 0
    \end{bmatrix} \rig\Vert_{2}\lesssim \sqrt k\sup_{j\in[k]}\lef\Vert\wha\mu_j^{(1)}\rig\Vert_2\sqrt{\frac{d\log(M)}{M}}.
    \end{align*}
    \begin{center}
        \textbf{Step 3.}
    \end{center}
    To achieve step 3, we first divide the heads into $k$-blocks where the $j$-th block achieves the task of approximating the norm function $\Vert\bfa x_{1,j}-\wha\mu_j^{(1)}\Vert_2$ taking $\bfa x_{1,j}-\wha\mu_j^{(1)}$ as input. We design the parameters for the first block as follows, as an example
    \begin{align*}
        \bfa V_i^{(5+3k)}&= \begin{bmatrix}
            &\bfa 0_{(k+3)d+k+1}&\\
            \bfa 0_{d\times((k+3)d+k+1)}&c_iI_d&\bfa 0\\
            &\bfa 0&
        \end{bmatrix},\quad \bfa Q_i^{(5+3k)}=\begin{bmatrix}
            &\bfa 0_{(2d+k)\times N}&\\
            \bfa 0_{2d+k}& I_d&\bfa 0\\
            &\bfa 0&
            \end{bmatrix},\\
        \bfa K_i^{(5+3k)}&=\begin{bmatrix}
            &\bfa 0_{(3d+k+1)\times D}&\\
            \bfa 0_{d\times(3d+k+1)}&\bfa A_i&\bfa 0\\
            &\bfa 0&
        \end{bmatrix}\cdot\begin{bmatrix}
            &\bfa 0_{(3d+k+1)\times D}&\\
            \bfa 0_{d\times(3d+k+1)}&I_d&\bfa 0\\
            &\bfa 0&
        \end{bmatrix}.
    \end{align*}
      $\{c_i,\bfa A_i\}_{i\in[M]}$ satifies $\sup_{i\in[M]}\Vert\bfa A\Vert_2\leq C$, whose existence is guaranteed by lemma \ref{softmaxapprox}. Moreover, it is not hard to show that one can utilize 2 FC layers to remove the vector part and the $\{\bfa p_{1,i}^\prime\}_{i\in[N]}$ Under this design, our final output satisfies
      \begin{align*}
       &\lef\Vert \bfa H_{6+3k}-\begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        &&\bfa 0_{k\times N}
        \\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
         \Vert\bfa x_{1,1}-\wha\mu_1^{(1)}\Vert_2&\Vert\bfa x_{2,1}-\wha \mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa x_{k,1}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa x_{N,1}-\wha\mu_1^{(1)}\Vert_2\\
        &&\vdots\\
        \Vert\bfa x_{1,k}-\wha\mu_k^{(1)}\Vert_2&\Vert\bfa x_{2,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa x_{k,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa x_{N,k}-\wha\mu_k^{(1)}\Vert_2\\
        &&\bfa 0
    \end{bmatrix} \rig\Vert_{2}\\
    &\lesssim \sqrt k\sup_{j\in[k]}\lef\Vert\wha\mu_j^{(1)}\rig\Vert_2\sqrt{\frac{\log(M)}{M}}.
    \end{align*}
    \begin{center}
        \textbf{Step 4.}
    \end{center}
    In step 4, we utilize the property of the Softmax function to approximate the hard max
    and replace the $\bfa p_{1,1}^\prime$ with our new estimate $\bfa p_{1,k}^{(1)}$. We construct our layer weights by
    \begin{align*}
        \bfa V^{(7+3k)}&= \begin{bmatrix}
            &\bfa 0_{(2d)\times N}&\\
            \bfa 0_{k\times (2d+k)}&I_k&\bfa 0\\
            &\bfa 0&
        \end{bmatrix},\quad\bfa Q^{(7+3k)} = \begin{bmatrix}
            &\bfa 0_{(2d+k)}&
            \bfa 0_{(2d+k)\times k}&I_k&\bfa 0\\
             &\bfa 0&
        \end{bmatrix},\\
        \bfa K^{(7+3k)}&=C\log N\begin{bmatrix}
            &\bfa 0_{(3d+k+1)\times N}&\\
            \bfa 0_{k\times(2d)}&I_k&\bfa 0\\
            &\bfa 0&
        \end{bmatrix}.
    \end{align*}
    Note that the result given by \ref{approxhardmax} implies that
    \begin{align*}
        \lef\Vert\softmax\lef(\begin{bmatrix}
         \Vert\bfa x_{1,1}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa x_{N,1}-\wha\mu_1^{(1)}\Vert_2\\
        &\ddots&\\
        \Vert\bfa x_{1,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa x_{N,k}-\wha\mu_k^{(1)}\Vert_2
        \end{bmatrix}\rig)-\begin{bmatrix}
            \bfa p_{1,1}^{(1)}&\bfa p_{1,2}^{(1)}&\ldots&\bfa p_{1,N}^{(1)}
        \end{bmatrix}\rig\Vert_2\lesssim N\exp(-C\log N) = dN^{-C}.
    \end{align*}
     Under this construction and let the FC layer retain the identity of the first $3d+k+1$ columns and remove the rest, our approximation results in lemma \ref{approxhardmax} implies that
    \begin{align*}
        \lef\Vert\bfa H_{7+3k}- \begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa x_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(1)}&\bfa p_{1,2}^{(1)}&\ldots&\bfa p_{1,k}^{(1)}&\ldots&\bfa p_{1,N}^{(1)}
        \\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        &&\bfa 0
    \end{bmatrix}\rig\Vert_2\lesssim dN^{-C}+\sqrt k\sup\Vert\wha\mu_j^{(1)}\Vert_2\sqrt{\frac{\log M}{M}}+C^d\sqrt{\frac{d^2\log M}{M}}.
    \end{align*}
    Hence, we construct in total of $\tau$ sets of subnetwork, and use the final layer to extract the set of output $\{\bfa p_{1,i}^{(\tau)}\}_{i\in[N]}$. By subadditivity of the $L_2$ norm we finalize our results by
    \begin{align*}
        \Big\Vert\tfp(\bfa H)&-\begin{bmatrix}
            \bfa p_{1,1}^{(1)}&\bfa p_{1,2}^{(1)}&\ldots&\bfa p_{1,N}^{(1)}
        \end{bmatrix}\Big\Vert_2\lesssim \tau \Big(dN^{-C}+\sqrt k\sup\Vert\wha\mu_j^{(1)}\Vert_2\sqrt{\frac{\log M}{M}}+C^d\sqrt{\frac{d^2\log M}{M}}\Big).
    \end{align*}
% \begin{lemma}[Identity Mappings of the Softmax Function]
%     There exists a set of weights $((\bfa v_1, a_1),\ldots,(\bfa v_M,a_M))\in[\bb R^{d+1}\times[-B,B]]^M$. The mapping $f_i(\bfa x):=x_i$ can be approximated by a sum of the Softmax functions with the number of heads $M<$ such that
%     \begin{align*}
%         \sup_{0\leq\alpha\leq m}\Big|f(\bfa x)-\frac{1}{M}\Big|\leq\epsilon
%     \end{align*}
% \end{lemma}
% \begin{proof}
%     By lemma \ref{lm:a1} we can show that for $M\gtrsim C^2(d)B_f^2/\epsilon^2$ there exists $((\bfa v_1, a_1),\ldots,(\bfa v_M,a_M))\in[\bb R^{d+1}\times[-B,B]]^M$ such that the function $f(\bfa x)=x_i$ can be approximated with
%     \begin{align*}
%         \sup_{\bfa x\in U}\Big|f(\bfa x)-\frac{1}{M}\sum_{i=1}^Ma_i\sigmoid\lef([\bfa x^\top, 1]\bfa v_i\rig)\Big|\leq\epsilon.
%     \end{align*}
% \end{proof}
\begin{lemma}[Approximating the Hardmax Function by the Softmax Function]\label{approxhardmax} Consider a vector $\bfa x\in\bb R^d$. Let $x^*=\max_{i}\{x_i\}_{i\in[d]}$. Define $x^*=\max_{i\in[d]}x_i$, $\ca N_2=\{i:x_i=x^*\}$, $\Delta = x^*-\max_{i\in\ca N_2}x_i$. Define the Hardmax function as $\text{Hardmax}(\bfa x)=\begin{bmatrix}
    \frac{\mbbm 1_{x_1=\max_{i\in[d]}\{x_i\}}}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}}\{x_i\}}&\ldots&\frac{\mbbm 1_{x_d=\max_{i\in[d]}\{x_i\}}}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}}\{x_i\}}
\end{bmatrix}$, we subsequently show that 
\begin{align*}
    \Big|\softmax(\beta\bfa v)-\text{Hardmax}(\bfa v)\Big|\leq\bigg((d-|\ca N_2|)+\frac{(d-|\ca N_2|)^2}{|\ca N_2|^3}\bigg)^{\frac{1}{2}}\exp(-\beta\Delta).
\end{align*}
\end{lemma}
\begin{proof}
 We can show that the difference is given by
    \begin{align*}
        \Big\Vert\softmax(&\beta \bfa v)-\text{Hardmax}(\bfa v)\Big\Vert_{2} =\bigg(\sum_{i=1}^d\bigg(\frac{\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\}}}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\}}}-\frac{\exp(\beta x_i)}{\sum_{i=1}^d\exp(\beta x_i)}\bigg)^2\bigg)^{\frac{1}{2}} \\
        &=\bigg(\sum_{i=1}^d\bigg(\frac{\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\} }}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\} }}-\frac{\exp(\beta(x_i-x^*))}{\sum_{i=1}^d\exp(\beta(x_i-x^*)) }\bigg)\bigg)^{\frac{1}{2}}\\
        &\leq\bigg(\sum_{i=1}^d\mbbm 1_{x_i\neq x^*}\exp\lef(2\beta(x_i-x^*)\rig)+\mbbm 1_{x_i=x^*}\bl\frac{\exp\lef(\beta(x_i-x^*)\rig)}{\sum_{i=1}^d\exp\lef(\beta(x_i-x^*)
        \rig)}-\frac{1}{|\{i:x_i=x^*\}|}\br^2\bigg)^{\frac{1}{2}}\\
        &\leq \Big(|\ca N_1|\exp(-2\beta\Delta)+ |\ca N_2|\Big(\frac{1}{|\ca N_2|+|\ca N_1|\exp(-\beta \Delta)}-\frac{1}{|\ca N_2|}\Big)^2\Big)^{\frac{1}{
        2}}\\
        &\leq \bigg((d-|\ca N_2|)\exp(-2\beta\Delta) +\frac{(d-|\ca N_2|)^2\exp(-2\beta\Delta)}{|\ca N_2|\lef(|\ca N_2|+(d-|\ca N_2|)\exp(-\beta\Delta)\rig)^2}\bigg)^{\frac{1}{2}}\\
        &=\bigg((d-|\ca N_2|)+\frac{\lef(d-|\ca N_2|\rig)^2}{|\ca N_2|\lef(|\ca N_2|+(d-|\ca N_2|)\exp(-\beta\Delta)\rig)^2}\bigg)^{\frac{1}{2}}\exp\lef(-\beta\Delta\rig)\\
        &\leq \bigg((d-|\ca N_2|)+\frac{(d-|\ca N_2|)^2}{|\ca N_2|^3}\bigg)^{\frac{1}{2}}\exp(-\beta\Delta).
    \end{align*}
\end{proof}



\subsection{Proof of Proposition \ref{genbd}}
    Before we start the proof, we first consider the event of $\ca E=\lef\{\lef\Vert\bfa X_i^{(j)}-\bfa\mu_{\bfa z_i}^{(j)}\rig\Vert_2\leq \sigma\sqrt{d\log(nN/\delta)},\forall i\in[N],j\in[n]\rig\}$, it is not hard to check that by sub-Gaussian tail bound, $\bb P(E)\geq 1-\delta$.
    Then, it is not hard to check that under $E$, $\Vert \bfa H\Vert_2\lesssim \sigma\sqrt{d\log(nN/\delta)}\times N$.    
    Then we obtain generalization bound under the event $\ca E$ using the proof machine created by \citet{bai2024transformers} J.2 \citet{he2025learning} Proposition 1, where we note that the multi-layered Transformer satisfies the following conditions
    \begin{enumerate}
        \item The metric entropy of the operator norm ball satisfies $\log\lef(\delta, \ca B_{\vertiii \cdot}, \vertiii\cdot\rig)\leq C B_LB_MD^2\log(1+(B_{\bfa\theta}+B_X+k)/\delta)$.
        \item $L(TF_{\bfa\theta}(\bfa H), \bfa P_1(\bfa z))\leq 2$ and is $2$ sub-Gaussian.
        \item The Lipschitz condition of the Transformer satisfies for $\bfa\theta_1,\bfa\theta_2\in\Theta_{B_M,B_L}(B_{\bfa\theta})$,
        \begin{align*}
         L(TF_{\bfa\theta_1}(\bfa H), \bfa P_1(\bfa z))- L(TF_{\bfa\theta_2}(\bfa H), \bfa P_1(\bfa z))\leq CLB_{\bfa\theta}^{4L}B_X^{3L}\vertiii{\bfa\theta_1-\bfa\theta_2}.
        \end{align*}
    \end{enumerate}
    where the last condition is obtained through noticing that the Softmax function is Lipschitz with constant $C$. And for bounded input $\bfa H$ the Lipschitz constant of the non-activated Attention layer is proportional to $B_{\bfa\theta}^3$. Then, using proposition A.4 in \citet{bai2024transformers}, one can show by union bound that with probability at least $1-\delta$,
    \begin{align*}
        L\lef(A_{\wha\theta}(\bfa H), \bfa P_1(\bfa z)\rig)& \leq \inf_{\bfa\theta\in\Theta_{B_M,B_L}(B_{\bfa\theta})}\bb E[L\lef(A_{\wha\theta}(\bfa H), \bfa P_1(\bfa z)\rig)|\ca E]\\
        &+C\sqrt{\frac{D^2B_LB_M\log(NB_{\bfa\theta}B_{M}D\sigma\cdot\sup_{i\in[N],j\in[n]}\Vert\bfa\mu_i^{(j)}\Vert_2)+\log(2/\delta)}{n}},
    \end{align*}
    where $A\in\{TF,TF^+\}$.


\subsection{Proof of Theorem \ref{ultimate}}
    We prove the above theorem through upper bounding the term $\inf_{\bfa\theta\in\Theta_{B_M,B_L(B_{\bfa\theta})}}\bb E[L(A_{\wha\theta}(\bfa H),\bfa P_1(\bfa z))|\ca A\cap\ca E]$ where $\ca A$ appears to be the event in Corollary 3 in \citet{lu2016statistical}. Note that $\bb P(\ca A)\geq 1-5n^{-1}-2\exp(-\Delta/\sigma)$. Since the proof for the Transformer and the Transformer+ shares similar idea, we only prove the case with the Transformer model where $\tda\theta$ is the construction. Denote the estimate given by Lloyd's algorithm as $
    \wha P(\bfa z):=\begin{bmatrix}
        \bfa p_{1,1}^{(\tau)}&\ldots&\bfa p_{1,N}^{(\tau)}
    \end{bmatrix}$ Then we can see that
    \begin{align*}
\inf_{\bfa\theta\in\Theta_{B_M,B_L(B_{\bfa\theta})}}\bb E&[L(TF_{\wha\theta}(\bfa H),\bfa P_1(\bfa z))|\ca A\cap\ca E]\leq \bb E\lef[L(TF_{\tda\theta}(\bfa H),\bfa P_1(\bfa z))|\ca A\cap\ca E\rig]\\
&\leq \bb E\lef[L(\wha P_1(\bfa z), \bfa P_1(\bfa z))|\ca A\cap\ca E\rig]+\frac{1}{N}\Vert\wha P_1-TF_{\tda\theta}(\bfa H)\Vert_1.
    \end{align*}
    Note that the first term can be controlled by \citet{yu2015useful} Corollary 3.1 as follows
    \begin{align*}
        \bb E\lef[L(\wha P_1(\bfa z),\bfa P_1(\bfa z))\Big|\ca E\rig]&\leq 2\bb E\Big[L(\wha P_1(\bfa z),\bfa P_1(\bfa z))|\ca A\cap \ca E\Big]\bb P(\ca A)+2\bb P(\ca A)\\
        &\leq \exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2}\rig) +\frac{10}{n}+2\exp\lef(-\frac{\Delta}{\sigma}\rig).
    \end{align*}
    For the second term, we note that
    \begin{align*}
        \frac{1}{N}\lef\Vert\wha P_1(\bfa z)-TF_{\tda\theta}(\bfa H)\rig\Vert_1\leq\frac{1}{\sqrt N}\lef\Vert\wha P_1(\bfa z)-TF_{\tda\theta}(\bfa H)\rig\Vert_2.
    \end{align*}
Then we can show that, with probability at least $1-\delta - 5n^{-1} - 2\exp(-\Delta/\sigma)$, given $\tau\geq 4\log n+1$, $B_L= k\tau+C\tau$, $B_M\geq M$, $D=Cdk$,
\begin{align*}
    L\lef(TF_{\wha\theta}(\bfa H),\bfa P_1(\bfa z)\rig) &\lesssim \exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2}\rig)\\
&+\sqrt{\frac{D^2B_LB_M\log(NB_{\bfa\theta}B_{M}D\sigma\cdot\sup_{i\in[N],j\in[n]}\Vert\bfa\mu_i^{(j)}\Vert_2)+\log(2/\delta)}{n}}\\
&+\tau \sqrt N C^d\Big(\sqrt k\sup_{j\in[k],\ell\in[\tau]}\Vert\wha\mu_j^{(1)}\Vert_2\sqrt{\frac{\log M}{M}}+N^{-1}\Big).
\end{align*}
We further let $M=n^{\frac{1}{4}}, L\asymp k\log n$ and obtain that with probability at least $1-\delta - 5n^{-1} - 2\exp(-\Delta/\sigma)$.
\begin{align*}
    L(TF_{\wha\theta}(\bfa H),\bfa P_1(\bfa z))\lesssim \exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2}\rig)+\sqrt kn^{-1/4}C^d\sqrt{Polylog(n)+\log(2/\delta)}+N^{-3/2}.
\end{align*}
And similarly we have with probability at least $1-\delta - 5n^{-1} - 2\exp(-\Delta/\sigma)$
\begin{align*}
    L(TF_{\wha\theta}^+(\bfa H),\bfa P_1(\bfa z))\lesssim \exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2}\rig)+d\sqrt kn^{-1/4}C^d\sqrt{Polylog(n)+\log(2/\delta)}+N^{-100.5} .
\end{align*}
% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

\section{Additional Experiments}

\begin{figure}[htbp]
    \centering
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_dist_metrics_attn.pdf}
        % \subcaption{Top-10 Eigenvalues}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_min_dist_loss_attn.pdf}
        % \subcaption{RMSE with Varying Layers}
    \endminipage\vspace{-0.1em}
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_N_metrics_attn.pdf}
        % \subcaption{Cosine Similarity with Varying d}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_N_loss_attn.pdf}
        % \subcaption{Fourth Chart}
    \endminipage\vspace{-0.1em}
    \vspace{-1em}
    \caption{\textbf{Comparison of Concatenated Attention and Averaged Attention on Synthetic Dataset.}
    \emph{Top: Performance Comparision on Minimum Distance Task.}
    \emph{Bottom: Performance Comparision on Number of Data Task.}
    We observe similar trend of performance between concatenated multihead attention a averaged multihead attention across three tasks, two evaluation metrics, and the converged loss.
    All the experiment settings are the same as the experiments in \cref{sect4}.}
    \label{fig:attn_1}
\end{figure}


\begin{figure}[htbp]
    \centering
\minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_num_classes_metrics_attn.pdf}
        % \subcaption{Top-10 Eigenvalues}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_num_classes_loss_attn.pdf}
        % \subcaption{RMSE with Varying Layers}
    \endminipage\vspace{-0.1em}
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_ratio_metrics_attn.pdf}
        % \subcaption{Cosine Similarity with Varying d}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_ratio_loss_attn.pdf}
        % \subcaption{Fourth Chart}
    \endminipage
    \vspace{-1em}
    \caption{{\textbf{Comparison of Concatenated Attention and Averaged Attention on Synthetic Dataset.}
    \emph{Top: Performance Comparision on Number of Classes Task.}
    \emph{Bottom: Performance Comparision on Inbalance Ratio Task.}
    }
    Again, we observe a similar performance trend between concatenated multihead attention and averaged multihead attention across both tasks.
    All experimental settings remain the same as those in \cref{sect4}.
    }
    % \label{fig:softmax_loss}
\end{figure}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
