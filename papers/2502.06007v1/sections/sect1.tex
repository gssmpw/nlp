\section{Introduction}
Large Language Models (LLMs) have demonstrated significant success in learning and performing inference on real world high dimensional datasets. Most modern LLMs use the Transformer model \citep{vaswani2017attention} as their backbone. 

Many existing works have considered the theoretical guarantees of the in-context-learning setup of Transformers \citep{bai2024transformers, akyurek2022learning}. However, in practice, LLMs require a significant amount of pretraining data to achieve their empirical advantage. And, little is known about the unsupervised learning guarantees of Transformers, especially after a sufficient number of problem instances are observed by the Transformer model in the pre-training phase. Motivated by the strong empirical performance of Transformers, we provide theoretical analysis of the Transformers on a standard unsupervised learning problem of clustering a mixture of Gaussians in the multi-class setup. Our results suggest that Transformers, like human brains, can benefit from experienced problem instances and learn the way to solve the problem (algorithms). Then, when fed with a new problem instance, Transformers can solve it through the learned algorithms naturally.

The problem of clustering a mixture of multivariate Gaussian is one of the most standard unsupervised learning problems \citep{bishop2006pattern} that can be solved by the EM algorithm or Lloyd's algorithm \citep{lloyd1982least}. The EM algorithm contains both the \emph{Expectation} and the \emph{Maximization} sub-procedures where the \emph{Expectation Step} creates a function for \textbf{the expectation} of the log-likelihood evaluated using the current estimate for the parameters and the \emph{Maximization Step} computes parameters maximizing the expected log-likelihood given by the \emph{Expectation Step}. We draw connections between the Softmax Attention in Transformers and the EM algorithms through the following:
\begin{enumerate}
    \item \emph{The Expectation Step} involves a normalized sum in \textbf{the expectation} whose weight vector is naturally given by the output of the softmax function as 
    $\begin{bmatrix}
        \frac{\exp(z_1)}{\sum_{i=1}^D\exp(z_i)},\ldots,\frac{\exp(z_D)}{\sum_{i=1}^D\exp(z_i)}
    \end{bmatrix}$.
    \item \emph{The Maximization Step} involves finding the index with the maximum value in a vector (the Hardmax Function). This is naturally approximated by the Softmax function as its name suggests.
\end{enumerate}
Given the strong connections of the two steps to the Softmax function, we build an approximation theory for Lloyd's algorithm in a constructive manner. We also note that existing works only build approximation bound for multihead ReLU neural networks \citep{bach2017breaking} while the Softmax approximation of multivariate to multivariate mapping remains a myth. We resolve this obstacle by proving an approximation bound for multi-head Transformers on a class of $\bb R^{d_1}\to\bb R^{d_2}$ mappings that might be of independent interests.



\paragraph{Contributions.} We summarize our major contributions as follows:
\begin{enumerate}
    \item We rigorously show that a pre-trained Transformer can perform multi-class clustering by drawing its connection to Lloyd's algorithm, which is used as a proof machine. We provide constructive proof and error bound for the approximation;
    \item We further consider the setup where the Transformer model is trained with independent instances from a class of clustering problems whose labels are used as supervision. We show that Transformers are able to generalize the mapping on new clustering problem instances. We provide upper bounds on the generalization error for the empirical risk minimizer in the pre-training task. Moreover, we show that given a sufficient number of training instances and proper initialization, pre-trained Transformers reach the fundamental limit of the problem;
    \item We systematically evaluate the performance of Transformers through extensive simulations. These empirical results demonstrate that Transformers perform well in the multi-class clustering task even when the assumptions leading to the theoretical results no longer hold.
\end{enumerate}

\subsection{Related Works}
\paragraph{Transformers are algorithm approximators.} Recently, the capacity of Transformers to automatically performing certain algorithms has drawn great attention from researchers. In particular, a rich line of recent works studied the expressive power of Transformers to perform in-context learning (ICL) \citep{akyurek2022learning,bai2024transformers,abernethy2024mechanism,li2023transformers2,jeon2024informationtheoretic}. Specifically, \citet{akyurek2022learning,bai2024transformers,abernethy2024mechanism} studied how Transformers perform gradient descent based training to perform ordinary or sparse linear regression on the context. \citet{chen2024transformers} further showed how Transformers utilize the multi-head structure to perform in-context sparse linear regression.  \citet{li2023transformers2} studied the generalization and stability of transformers in ICL tasks. \citet{jeon2024informationtheoretic} studies the information-theoretical lower bound of in-context learning. Another closely related line of works studied how Transformers can be pretrained by gradient descent to perform certain tasks. Specifically, \citet{zhang2023trained,huang2023context,chen2024training} studied the pretraining optimization dynamics of Transformers to learn in-context linear prediction rules. \citet{li2024one} showed that one-layer Transformers can be trained to perform one-nearest neighbor classification in context. \citet{ahn2024transformers,giannou2024well} studied the training of Transformers in learning various optimization methods. \citet{li2023transformers}  studied how Transformers can be trained to learn topic models. \citet{jelassi2022vision} proved that Vision Transformers can learn a class of image-like data whose patches follow certain spatial structures. \citet{zhang2025transformer} studied how Transformers can learn to perform variable selection in ``group-sparse'' linear regression.
% \paragraph{In-Context Learning with Transformers.} Recent studies have explored the in-context learning (ICL) capabilities of Transformers \citep{garg2022can, bai2024transformers}. Specifically, \citet{bai2024transformers} examined the approximation and generalization properties of Transformers in ICL tasks, including various linear and logistic regression scenarios. \citet{akyurek2022learning, von2023transformers} investigated how Transformers approximate gradient descent during ICL. In our work, we leverage the Power Method to provide theoretical guarantees for the approximation capabilities of Transformers. Notably, obtaining eigenvectors through gradient descent is challenging due to the lack of an explicit functional form. To our knowledge, this is the first study to offer theoretical guarantees for Transformers' approximation of the Power Method. For further practical insights into ICL, see \citet{dong2022survey} and the references therein.

\paragraph{Other theoretical studies on Transformers.} Various efforts have been made to gain a theoretical understanding of Transformers. \citet{yun2019transformers} analyzed the universal approximation properties of Transformers for sequence-to-sequence functions. \citet{li2023transformers} studied the mean-filed limit of large-scale Transformers and proved global convergence in regression tasks. 
\citet{perez2021attention} showed that Transformers with hard-attention are Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. \citet{bhattamishra2020computational}
 further provided an alternate and simpler proof to show that vanilla Transformers are Turing-complete, and then proved that Transformers with only positional masking and without any positional encoding are also Turing-complete. \citet{liu2022transformers} showed that a low-depth Transformer can represent the computations of any finite-state automaton by hierarchically reparameterizing its recurrent dynamics. \citet{yao2021self} demonstrated that Transformers can efficiently process bounded hierarchical languages, offering better space complexity compared to recurrent neural networks.

 

% The computational power of Transformers has been investigated by \citet{perez2021attention}, \citet{bhattamishra2020computational}, and \citet{liu2022transformers}. \citet{hron2020infinite} studied the behavior of infinitely wide multi-head and single-head attention mechanisms. Additionally, \citet{yao2021self} demonstrated that Transformers can efficiently process bounded hierarchical languages, offering better space complexity compared to recurrent neural networks.




















% This work is related to a few different branches in the literature.
% \paragraph{In Context Learning of Transformers.} Some recent works studied the in-context learning (ICL) capacities of Transformers \citep{garg2022can, bai2024transformers}. In particular, \citep{bai2024transformers} considered the approximation and generalization properties of transformers on the ICL tasks, including many linear regression and logistic regression setups. The problem of PCA is a standard unsupervised learning problem. Hence, it differs from ICL in that there is no individual label that the model needs to learn. \citet{akyurek2022learning, von2023transformers} considered the approximation of transformers on gradient descent when performing ICL. In this work, the proof machine utilizes the Power Method. We also notice that performing gradient descent is difficult to obtain the eigenvectors as no explicit functional form is given. To the best of the authors' knowledge, this is the first work that provides theoretical guarantees for the transformers' approximation of the Power method in the literature. Other related works on the more practical side of ICL can be found in \citet{dong2022survey} and references therein.
% \paragraph{Other Theoretical Works on Transformers.}
% Many other attempts are made to theoretically understand transformers. \citet{yun2019transformers} studied the universal approximation properties of transformers on sequence-to-sequence functions. \citet{perez2021attention,bhattamishra2020computational,liu2022transformers} studied the computational power of transformers. \citet{hron2020infinite} studied the limit of infinite width multi/single head attentions. \citet{yao2021self} showed that transformers can process bounded hierarchical languages and demonstrate better space complexity than recurrent neural networks.

\paragraph{Notations}
In this work we follow the following notation conventions. The vector-valued variable is given by boldfaced characters. We denote $[n]:=\{1,\ldots,n\}$ and $[i:j]:=\{i,i+1,\ldots, j\}$ for $i<j$. The universal constants are given by $C$ and are ad hoc. For a vector $\bfa v$ we denote $\Vert\bfa v\Vert_2$ as its $L_2$ norm. For a matrix $\bfa A\in\bb R^{m\times n}$ we denote its operator norm as $\Vert\bfa A\Vert_2:=\sup_{\bfa v\in\bb S^{n-1}}\Vert\bfa A\bfa v\Vert_2$. Given two sequences $a_n$ and $b_n$, we denote $a_n\lesssim b_n$ or $a_n = O(b_n)$ if $\limsup_{n\to\infty}|\frac{a_n}{b_n}|<\infty$ and $a_n=o(b_n)$ if $\limsup_{n\to\infty}|\frac{a_n}{b_n}|=0$. We denote $\mbbm 1_A$ as the indicator function for event $A$. The universal constants are denoted by $C$ in this work and are ad hoc. We use $\ca B(\Vert\cdot\Vert, r)$ to denote a ball with radius $r$ under the norm $\Vert\cdot\Vert$.  

\paragraph{Organizations} The rest of the paper is organized as follows: Section \ref{sect2} reviews standard contexts and describes the learning problem; Section \ref{sect3} provides rigorous theoretical results and sketches of proof; Section \ref{sect4} provides extensive experimental details and results; Section \ref{sect5} discusses the limitations and potential future works. The detailed proofs and additional figures in experiments are delayed to the appendix. The supplementary materials include the code for the experiments.
% We consider the problem of learning the EM algorithm using Transformers. We consider the unsupervised learning setup, given our input sequence given by
% $
%      X=\begin{bmatrix}
%          X_1,\ldots, X_{N}
%     \end{bmatrix}$. Then we consider the following problems under the class of EM algorithms.
%     \subsection{Clustering Mixture of Gaussians}
%     Moreover, we assume that $\{X_i\}_{i\in[N]}$ are i.i.d. samples generated from the Gaussian mixture model, defined by
%     \begin{align*}
%         p( \bfa x|\bfa\mu,z=\ell)=(2\pi)^{-\frac{m}{2}}|\Sigma|^{-\frac{1}{2}}\exp\lef(-\frac{1}{2}(\bfa x-\bfa\mu_{\ell})^\top\Sigma^{-1}(\bfa x-\bfa \mu_{\ell})\rig),\quad z\in[k],
%     \end{align*}
%     where we use $z$ to denote the index of the group from which the sample is generated from.
    
%     And consider the simple EM algorithm that is standard in the literature of clustering \cite{bishop2006pattern}. We use $\bfa z$. Then it is not hard to check that the Maximum Likelihood estimator is given by 
%     \begin{align*}
%        \lef(\{\wha \mu_{\ell}\}_{\ell\in[k]},\wha z\rig)&=\argmax_{\{\bfa\mu_{\ell}\}_{\ell\in[k]},\bfa z}\log p(\bfa X|\bfa\mu,\bfa z)=\argmin_{\{\bfa\mu_{\ell}\}_{\ell\in[k]},\bfa z}\sum_{i\in[n]}(X_i-\bfa\mu_{z_i})^\top\Sigma^{-1}(X_i-\bfa \mu_{z_{i}}) \\
%        &=\argmin_{\{\bfa\mu_{\ell}\}_{\ell\in[k]},\bfa z}\sum_{i\in[n]}\min_{\ell\in[k]}\Vert\bfa\mu_{z_i}-X_i\Vert_{\Sigma^{-1}}^2
%     \end{align*}
%     Note that the Lloyd's algorithm is used to solve the above $k$-means problem, given by
%     \begin{align*}
%         \wha\mu_{\ell}^{(t)}=\frac{\sum_{i=1}^nX_{\ell}^{(t-1)}\mbbm 1_{\wh z_i^{(t-1)}=\ell}}{\sum_{i=1}^n\mbbm 1_{\wh z_i^{(t-1)}=\ell}},\enspace\forall \ell\in[k],\qquad \wh z_i^{(t)}=\argmin_{\ell\in[k]}\Vert X_i-\wha\mu_{\ell}^{(t)}\Vert^2_{\Sigma^{-1}},\enspace\forall i\in[n].
%     \end{align*}
%     Our goal is to study the algorithmic approximation theory on transformers for problems that can be solved using EM algorithms.
%     \begin{assumption}
%         We assume that the distribution of the samples are given by $$\{\bfa X_i\}_{i\in[d]}\overset{i.i.d.}{\sim}(2\pi)^{-\frac{m}{2}}|\Sigma|^{-\frac{1}{2}}\exp\lef(-\frac{1}{2}(\bfa x-\bfa\mu_{\ell})^\top\Sigma^{-1}(\bfa x-\bfa\mu_{\ell})\rig),\quad z\in[k].$$ And the minimal separation condition is given by $\min_{i\neq j}\Vert\bfa\mu_i-\bfa\mu_j \Vert_2=\Delta$.
%     \end{assumption}
% \subsection{Community Detection in the Stochastic Block Model}
% Then we consider the community detection problem under the sparse network, under the following model specification:
% \begin{align*}
%     X_{ij}\sim\begin{cases}
%         Bern(\frac{p}{n})&\text{ if }z_i=z_j\\
%         Bern(\frac{q}{n})&\text{ otherwise, }
%     \end{cases}\quad z_i\in[k]\text{ for all }i\in[n],\quad\bb P(z_i=\ell)=\pi_{\ell}\text{ for all }\ell\in[k].
% \end{align*}
% Then we study the following algorithm
% \begin{align*}
%     \wh A_{i\ell}^{(t)}=\frac{\sum_{j\neq i}^nX_{ij}\mbbm 1_{\wh z_j^{(t)}=\ell}}{\sum_{j\neq i}^n\mbbm 1_{\wh z_i^{(t)}=\ell}},\quad\forall i\in[n],\ell\in[k],\quad \wh z_i^{(t)}=\argmax_{\ell\in[k]}\wh A_{i,\ell}^{(t-1)}\quad\forall i\in[n],
% \end{align*}
% which approximately minimizes the following objective
% \begin{align*}
%     (\wh A,\wh z)=\argmax_{(A,z)}\sum_{i=1}^n\inf_{\ell\in[k]}\sum_{j\neq i}(A_{i,\ell}-X_{jz(j)})^2
% \end{align*}

