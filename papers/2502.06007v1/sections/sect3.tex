\section{Theoretical Results}\label{sect3}
This section presents our theoretical results and a proof sketch of our results. This section is divided into three parts: section \ref{approx1} provides the approximation bound to EM algorithms by Transformers; section \ref{approx2} provides the generalization bound; section \ref{approx3} provides a short proof sketch over the main theorem.

% \paragraph{The Design of Auxillary Matrix.} Our design of the matrix $\bfa P$ consists of three parts:
% \begin{enumerate}
%     \item \emph{Place Holder.} For $\ell\in\{1\}\cup[4:k+3]$ and $i\in[N]$, we let $\tda p_{\ell, i}=\bfa 0\in\bb R^{d\times 1}$. The place holders in $\bfa P$ records the intermediate results in the forward propagation.
%     \item \emph{Identity Matrix.} We let $ \begin{bmatrix}
%         \tda p_{2,1}&\ldots&\tda p_{2,N}
%     \end{bmatrix}=\begin{bmatrix}
%         \bfa I_d&\bfa 0_{d\times(N-d)}
%     \end{bmatrix}$. The identity matrix in $\bfa P$ helps us screen out all the covariates $\bfa X$ in the forward propagation.
%     \item \emph{Random Samples on the Hypersphere.} We let $\tda p_{3,1},\ldots\tda p_{3,k}$ be the i.i.d. samples uniformly distributed on $\bb S^{d-1}$. The random samples on the sphere corresponds to the initial vectors $\bfa v_{0,\ell}$ for $\ell\in[k]$ in algorithm \ref{alg:almoexactrecov}.
% \end{enumerate}
% Given the above construction on the auxillary matrix $\bfa P$, we are ready to state the existence theorem in this work, given as follows.

% % \begin{algorithm}[htbp] 
% %  \caption{Spectral Clustering}
% % \label{alg:spectralclustering}
% % \KwData{$\bfa X\in\bb R^{D\times N}$ with $\bfa X_i\in\bb R^D$ for all $i\in[N]$}
% %  Compute the SVD of the data matrix $\bfa X=UDV^\top$. Let $U_k$ be the first $k$ columns of $U$\;

% %  Project $\bfa x_1,\ldots,\bfa x_N$ onto $U_k$, i.e. let $\wha x_i\gets U_kU_k^\top \bfa x_i$ \;
% %  Run an \emph{Transformer k-means++\cite{arthur2006k}} algorithm on the columns of projected matrix $\wha X=\begin{bmatrix}
% %      \wha x_1,\ldots,\wha x_N
% %  \end{bmatrix}$\;
% % \end{algorithm}
% \begin{assumption}\label{asumpt1}
%     We assume that $\bfa X_1,\ldots,\bfa X_N$ are i.i.d. samples from $\sum_{\ell=1}^k\pi_\ell\bb P(X|\mu)$. Minimal separation is denoted by $\Delta:=\inf_{i\neq j}\Vert\bfa\mu_i-\bfa\mu_j\Vert_2$.
% \end{assumption}

% And the kmeans++ algorithm is given by the following procedure.
% % \begin{algorithm}[htbp] 
% %  \caption{k-means++}
% % \label{alg:kmeans++}
% % \KwData{$\bfa X\in\bb R^{k\times N}$ with $\bfa X_i\in\bb R^k$ for all $i\in[N]$}
% % Choose an initial center $\bfa c_1$ uniformly at random from $\ca X:=\{\bfa X_1,\ldots,\bfa X_N\}$ and let $\ca C=\{\bfa c_1\}$\;
% % \While{$|\ca C|<k$}{Choose the next center $\bfa c$, selecting $\bfa c=\bfa x\in\ca X$ with probability $\frac{D(\bfa x)}{\sum_{x\in\ca X}D(\bfa x)}$\\
% % where $D(\bfa x):=\argmin_{\bfa c\in\ca C}\Vert \bfa c-\bfa x\Vert_2$\;
% % If $\bfa c\notin\ca C$, let $\ca C\gets\ca C\cup\{\bfa c\}$\;
% % }
% % Run the Lloyd's algorithm with covariates $\bfa X$ and initial centroids $\ca C$\;
% % \end{algorithm}
% % \begin{algorithm}[htbp] 
% %  \caption{Lloyd's Algorithm}
% % \label{alg:lloyd}
% % \KwData{$\bfa X\in\bb R^{k\times N}$ and initial clusters $\ca C=\lef\{\wha\mu_\ell^{(0)}\rig\}_{\ell\in[k]}$}
% % Let $t\gets 1$\;
% % Initialize with the starting membership $\wh z_i^{(0)}\gets\argmin_{\ell\in[k]}\Vert \bfa x_i-\wha\mu_\ell^{(0)}\Vert_{\Sigma^{-1}}$\;
% % \While{$\exists j\in[n]$ such that $\wh z^{(t+1)}_j\neq \wh z^{(t)}_j$}{Update the centroids and memberships through $\wha\mu_{\ell}^{(t)}\gets\frac{\sum_{i=1}^n\bfa x_{i}\mbbm 1_{\wh z_i^{(t-1)}=\ell}}{\sum_{i=1}^n\mbbm 1_{\wh z_i^{(t-1)}=\ell}},\enspace\forall \ell\in[k],\qquad \wh z_i^{(t)}\gets\argmin_{\ell\in[k]}\Vert \bfa x_i-\wha\mu_{\ell}^{(t)}\Vert^2_{\Sigma^{-1}},\enspace\forall i\in[n].$
% % }
% % Run the Lloyd's algorithm with $\bfa X$ and $\ca C$\;
% % \end{algorithm}

% In particular, take as input the context given by $\bfa H=\begin{bmatrix}
%     \bfa x_1,\ldots,\bfa x_N\\
%     \bfa p_1,\ldots,\bfa p_N
% \end{bmatrix}\in\bb R^{D\times N}$, we show that the Transformer network can approximate the spectral clustering algorithm in the initialization phase, given by \ref{alg:spectralclustering}.
\subsection{The Approximation Bound}\label{approx1}
We first present an approximation bound for Lloyd's algorithm by both the Transformer and the Transformer+.
\begin{theorem}\label{thm_main1}
   Assume that $d=o(N)$, $k<d$. There exists a Transformer with number of layers $L=\tau(3+3k)$ and norm $\vertiii{\bfa\theta}\lesssim C^d(\log N +M)$ with the number of heads $M$ such that
    \begin{align*}
        \Big\Vert TF_{\bfa\theta}&(\bfa H)-\begin{bmatrix}
            \bfa p_{1,1}^{(\tau)}&\bfa p_{1,2}^{(\tau)}&\ldots&\bfa p_{1,N}^{(\tau)}
        \end{bmatrix}\Big\Vert_2\\
        &\lesssim \tau C^d\Big(\sqrt k\sup_{j\in[k],\ell\in[\tau]}\Vert\wha\mu_j^{(\tau)}\Vert_2\sqrt{\frac{\log M}{M}}+N^{-1}\Big)
    \end{align*}
where $\tau$ is the number of iterations in the Lloyd's algorithm, and $p_{1,i}^{(\tau)}$ the one-hot coding of the membership $\wha z_i^{(\tau)}$ there.
\end{theorem}

\begin{remark}
The two terms in the bound come from the expectation and the maximization steps, respectively. The $N^{-1}$ term comes from the expectation step, where we relate the weighted average with the weights given by the Softmax function. The second term related to multi-head attention comes from a few multi-head approximation layers for some general functions. To achieve this bound, we prove a new result on the universal approximation of the Softmax function, which brings in the approximation term discussed in section \ref{approx3}. We also note that the $N^{-1}$ term can be further improved by introducing the non-activated Attention layer, given by the next theorem for Transformer+ architecture. 
\end{remark} 
\begin{theorem}\label{thm_main2}
    Under the same condition as theorem \ref{thm_main1}, there exists a \tfp with the number of layers $L=\tau(7+3k)$ and norm $\vertiii{\bfa\theta}\lesssim C^dM\log N$ with the number of heads $M$ such that
    \sm{\begin{align*}
        &\Big\Vert TF^+_{\bfa\theta}(\bfa H)-\begin{bmatrix}
            \bfa p_{1,1}^{(\tau)}&\bfa p_{1,2}^{(\tau)}&\ldots&\bfa p_{1,N}^{(\tau)}
        \end{bmatrix}\Big\Vert_2\lesssim \tau \Big(dN^{-100}\\
        &+C^d\sqrt k\sup_{j\in[k],\ell\in[\tau]}\Vert\wha\mu_j^{(\ell)}\Vert_2\sqrt{\frac{\log M}{M}}+C^d\sqrt{\frac{d^2\log M}{M}}\Big).
    \end{align*}}
\end{theorem}
\begin{remark}
    The key difference between Theorems \ref{thm_main1} and \ref{thm_main2} comes from different designs of the Expectation Step. Through the introduction of the non-activated Attention layer, we manage to approximate a wider range of functions, including the polynomials on $\bfa H$ with degree $3$. We also believe that the non-activated Attention layer is unnecessary if a stronger \emph{right-product} universal approximation result can be proved for the Softmax functions, which is discussed in section \ref{approx3}.
\end{remark}
\subsection{The Generalization Bounds}\label{approx2}
Given the approximation error provided by Theorems \ref{thm_main1} and \ref{thm_main2}, we further provide the generalization error bound for the ERM defined by \eqref{ERM}. We consider the problem instances $\{\bfa X^{(i)},\bfa z^{(i)} \}_{i\in[n]}$ to be sampled i.i.d. from a distribution supported on $\Theta_{GM}$. Then, we can show the following generalization bound for the Transformer network in the space of $\Theta(B_{\bfa\theta},B_{M},B_{L})$.
\begin{proposition}[Generalization Bounds]\label{genbd}
    With probability at least $1-\delta$,
    \sm{\begin{align*}
        L&\lef(A_{\wha\theta}(\bfa H), \bfa P_1(\bfa z)\rig) \leq \inf_{\bfa\theta\in\Theta_{B_M,B_L}(B_{\bfa\theta})}\bb E[L\lef(A_{\bfa \theta}(\bfa H), \bfa P_1(\bfa z)\rig)]\\
        &+C\sqrt{\frac{D^2B_LB_M\log(NB_{\bfa\theta}B_{M}D\sigma m_0)+\log(2/\delta)}{n}},
    \end{align*}}
    where $m_0=\sup_{i\in[N],j\in[n]}\Vert\bfa\mu_i^{(j)}\Vert_2$.
\end{proposition}
\begin{remark}
    The above proposition implies that, given the sufficiently large number of samples $n$, the ERM solution generalizes to new samples as we can use the approximation results given by Theorem \ref{thm_main1} and \ref{thm_main2} to upper bound the first term on the R.H.S. of the inequality. The following results ultimately provide an ultimate bound for the error of the ERM estimator on the unseen instance.
\end{remark}
\begin{theorem}[The Matching Upper Bound]\label{ultimate}
    Let $r_k=\frac{\Delta}{\sigma}\sqrt{\frac{\alpha}{1+kd/N}}$, $k\log N = o(N\alpha^2)$, $M\asymp n^{1/2}$, $L\asymp k\log n$, $\sqrt k=o(r_k)$ as $n\to\infty$. Assume that we use Algorithm \ref{alg:example} as initialization and let $\tau > 4\log n+1$. Then, with probability at least $1-\delta - 5n^{-1} - 2\exp(-\Delta/\sigma)$, the ERM estimator given by the Transformer satisfies
    \begin{align*}
    L(&TF_{\wha\theta}(\bfa H),\bfa P_1(\bfa z))\lesssim \exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2}\rig)\\
    &+\sqrt kn^{-1/4}C^d\sqrt{Polylog(n)+\log(1/\delta)}+N^{-3/2}.
\end{align*}
And with the same parameter setup and initialization, with probability at least $1-\delta - 5n^{-1} - 2\exp(-\Delta/\sigma)$, the ERM estimator given by the Transformer+ satisfies
\begin{align*}
    L(&TF_{\wha\theta}^+(\bfa H),\bfa P_1(\bfa z))\lesssim \exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2}\rig)\\
    &+d\sqrt kn^{-1/4}C^d\sqrt{Polylog(n)+\log(1/\delta)}+N^{-100.5}.
\end{align*}
\end{theorem}
\begin{remark}
    Our results in the above theorem imply that, given the number of samples $n\asymp \exp\lef(\frac{\Delta^2}{2\sigma^2}\rig)$, Transformers can reach the fundamental limits given by Lemma \ref{minimaxlb} and achieve the minimax optimal rate of the clustering problem with high probability. Moreover, the introduction of the non-activated Attention layer in Softmax+ significantly improves the upper bound in the exponent of $N$. In particular, the $N^{-100.5}$ can even be improved with arbitrarily large universal constants. We discuss in section \ref{approx3} that solving a potential open problem on the universal approximation of the Transformer might lead to the removal of the non-activated Attention layer in the proof. 
\end{remark}
\subsection{The Proof Ideas}\label{approx3}
This section discusses some new results we obtained on the midway of proving Theorems \ref{thm_main1} and \ref{thm_main2}. We then present a proof sketch for the more complicated proof of Theorem \ref{thm_main1}.
\subsubsection{An Approximation Bound for the Softmax Function} We provide a new approximation bound for the sum of Softmax functions to mappings from $\bb R^{d_1}\to\bb R^{d_2}$. We first introduce the class of $(R,C_{\ell})$ smooth functions. The $(R, C_{\ell})$ smooth function class contains a wide range of functions.
\begin{definition}[\citep{bach2017breaking}\citep{bai2024transformers}]\label{rcsmoothfunc}
    A function $g:\bb R^d\to\bb R$ is $(R, C_{\ell})$ smooth if for $s= \lceil(k-1)/2\rceil+2$, $g$ is a $C^s$ function supported on $[-R, R]^k$ such that
    \begin{align*}
        \sup_{\bfa x\in [-R,R]^k}\Vert\nabla^ig(\bfa x)\Vert_{\infty}\leq L_i,
    \end{align*}
    for all $i\in\{0,1,\ldots,s\}$, with $\max_{0\leq i\leq s}L_iR^i\leq C_{\ell}$.
\end{definition}
Then, we are ready to present our results on the approximation error of Softmax functions.
 \begin{lemma}[Approximating $d$ Dimensional $(R, C_{\ell})$ Smooth Mappings by Softmax Neural Networks]\label{softmaxapprox}
        Consider an element-wise $(R, C_{\ell})$ smooth mapping $\bfa f(\bfa x)=(f_1(\bfa x),\ldots f_{d_1}(\bfa x))^\top$ where $\bfa x\in[-R,R]^d$. There exists a set of points $\{(\bfa A_{i}, a_{i})\}_{i\in[M]}$ with $\sup_{i\in[M]}\Vert\bfa A\Vert_2\leq C$ such that the following holds
        \begin{align*}
            \sup_{\bfa x\in\ca B\lef(\Vert\cdot\Vert_\infty, R\rig)}&\frac{1}{C_{\ell}}\Big\Vert\bfa f(\bfa x)-\sum_{i=1}^{Md} a_i\softmaxx\lef(\bfa A_i\begin{bmatrix}
                \bfa x\\
            1\end{bmatrix}\rig)\Big\Vert_{\infty}\\
            &\leq C(f)^d\sqrt{\frac{dd_1}{M}\log\lef(\frac{MR}{dd_1}\rig)}.
        \end{align*}
 \end{lemma}
\begin{remark}
    The above bound demonstrates the universal approximation of Softmax functions to smooth mappings. Our proof idea utilizes a preliminary result on the sigmoid function and dissects the softmax function into multiple sigmoid functions. For each of the sigmoid functions, we use the probabilistic method to construct $L_{\infty}$ approximation bound.
\end{remark}
In Lemma \ref{softmaxapprox}, our proof applies to the left product of $\bfa A$. It is then of general interest to know whether there exists a universal approximation bound for the right product form $\softmax\lef(\begin{bmatrix}
        \bfa x&1
\end{bmatrix}\bfa A\rig)$. Solving this fundamental problem helps us to achieve the rate of the Transformer+ using the Transformer model.
\subsubsection{The Proof Sketches of Theorem \ref{thm_main1}}
We here provide the proof sketch of the Theorem \ref{thm_main1}. The proof of Theorem \ref{thm_main2} is more involved in the Expectation step and is delayed to the appendix. Our proof idea is to manually construct $\bfa\theta$ for the network and estimate the error caused by each layer constructed. 

\paragraph{The Expectation Step.} In the expectation step, we notice the following relationship holds
\sm{\begin{align*}
   \begin{bmatrix}
       \bfa X_1&\ldots&\bfa X_N
   \end{bmatrix} \softmax\lef(\begin{bmatrix}
        \bfa p_{1,1}^{(0)}\\\vdots\\\bfa p_{1,N}^{(0)}
    \end{bmatrix}\rig)\approx \begin{bmatrix}
        \wha\mu_1^{(1)}&\ldots&\wha\mu_k^{(1)}
    \end{bmatrix}.
\end{align*}}
However, we have \tny{$\begin{bmatrix}
    \bfa p_{1,1}^{(0)}&\ldots &\bfa p_{1,N}^{(0)}\\
    \bfa 0&\ldots&\bfa 0
\end{bmatrix}^\top$} and the $\bfa 0$ part needs to be cancelled. We then construct another head with $\bfa 0$ matrix in the SoftMax function to cancel out the $\bfa 0$ part in the first head. The two cancellations result in an approximation error of $O(1/N)$. 

\paragraph{The Maximization Step} In the maximization step, our proof involves a total of 4 steps. Our initial matrix is given by 
\tny{\begin{align*}
    \begin{bmatrix}
        \bfa X_1&\bfa X_2&\ldots&\bfa X_{k}&\ldots&\bfa X_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^{(0)}_{1,1}&\bfa p^{(0)}_{1,2}&\ldots&\bfa p_{1,k}^{(0)}&\ldots&\bfa p^{(0)}_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        &&\bfa 0&&&
    \end{bmatrix}.
\end{align*}}
Then, in the \textbf{Step 1}, we copy in a total of $k$ times the first row and move them to the $\bfa 0$ part using two FC layers, with one moving the negative part and one moving the positive part, providing
\tny{\begin{align*}
    \begin{bmatrix}
        \vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
        1&1&\ldots&1&\ldots&1\\
        \bfa X_{1,1}&\bfa X_{2,1}&\ldots&\bfa X_{k,1}&\ldots&\bfa X_{N,1}\\
        &&\vdots\\
        \bfa X_{1,k}&\bfa X_{2,k}&\ldots&\bfa X_{k,k}&\ldots&\bfa X_{N,k}\\
        &&\bfa 0
    \end{bmatrix}.
\end{align*}}
Then, in the \textbf{Step 2}, we move $\{\wha\mu_i^{(1)}\}_{i\in[M]}$ to $\{\bfa x_{j,i}\}_{j\in[N]}$, yielding
\tny{$$\begin{bmatrix}
        \vdots&\vdots&\vdots&\vdots&\vdots\\
        1&\ldots&1&\ldots&1\\
        \bfa X_{1,1}-\wha\mu_1^{(1)}&\ldots&\bfa X_{k,1}-\wha\mu_1^{(1)}&\ldots&\bfa X_{N,1}-\wha\mu_1^{(1)}\\
        &&\vdots\\
        \bfa X_{1,k}-\wha\mu_k^{(1)}&\ldots&\bfa X_{k,k}-\wha\mu_k^{(1)}&\ldots&\bfa X_{N,k}-\wha\mu_k^{(1)}\\
        &&\bfa 0
    \end{bmatrix},$$}
    This step utilizes the approximation bound given by Lemma \ref{softmaxapprox} to approximate the function of $f(\bfa x)=\bfa x_i$.
    
    Then, in the \textbf{Step 3}, we apply the approximation bound again to construct Softmax networks that approximate the mapping from a vector to its norm, providing us with the following matrix.
    \ttny{$$\begin{bmatrix}
        \vdots&\vdots&\vdots&\vdots&\vdots\\
        1&\ldots&1&\ldots&1\\
        \Vert\bfa X_{1,1}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa X_{k,1}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa X_{N,1}-\wha\mu_1^{(1)}\Vert_2\\
        &&\vdots\\
        \Vert\bfa X_{1,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa X_{k,k}-\wha\mu_k^{(1)}\Vert_2&\ldots&\Vert\bfa X_{N,k}-\wha\mu_k^{(1)}\Vert_2\\
        &&\bfa 0
    \end{bmatrix},$$}
    Finally, in the \textbf{Step 4}, we obtain approximate vectors to $\{\bfa p_{1,i}^{(1)}\}_{i\in[N]}$ through applying the softmax function to the submatrix
    \tny{\begin{align*}
        \begin{bmatrix}
        \Vert\bfa X_{1,1}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa X_{N,1}-\wha\mu_1^{(1)}\Vert_2\\
        \vdots&\vdots&\vdots\\
        \Vert\bfa X_{1,k}-\wha\mu_1^{(1)}\Vert_2&\ldots&\Vert\bfa X_{N,k}-\wha\mu_1^{(1)}\Vert_2
    \end{bmatrix}.
    \end{align*}} Using another approximation bound showing the difference between the Softmax and the Hardmax function we accomplish the Maximization step.
% This requires us to consider the following regularity conditions on the underlying distribution of $\bfa X\bfa X^\top$ (which also translates to the distribution of $\bfa X$).
% \begin{assumption}\label{assump1}
%     The distribution of $\bfa X\bfa X^\top$ supports on $$\bb X:=\lef\{A:A\in\bfa S^d_{++},  B_X\geq\lambda_1(A)>\lambda_2(A)>\ldots>\lambda_k(A),\inf_{1\leq i<j\leq k}\lambda_i(A)-\lambda_j(A)\geq\Delta\rig\}.$$
% \end{assumption}
% \begin{remark}
%     The above assumption can be easily generalized to distribution that supports on $\bb X$ with high probability. Examples of such distribution include the Wishart distribution under the Gaussian design. In this work, we stick to the simplest case where the maximum eigenvalue is bounded from above.
% \end{remark}
% Given the above assumption, we are ready to state the generalization bound.
% \begin{proposition}\label{genbound}
%    With probablity at least $1-\xi$, the ERM solution $\wha\theta$ satisfies
%     \begin{align*}
%         \bb E\lef[L\lef(TF_{\wha\theta}(\bfa H),\bfa V\rig)|\wha\theta\rig]&\leq\inf_{\bfa\theta\in\Theta(B_{\bfa\theta},B_M)}\bb E\lef[L\lef(TF_{\bfa\theta}(\bfa H),\bfa V\rig)\rig]\\
%         &+ C \sqrt{\frac{k^3LB_Md^2\log(B_{\theta}+B_X+k)+\log(1/\xi)}{n}}.
%     \end{align*}
% \end{proposition}
%    Together with the bound given by theorem \ref{thm3.1} and lemma \ref{lm3.1.1}, which essentially give a high probability upper bound on $\inf_{\bfa\theta\in\Theta(B_{\bfa\theta},B_M)}$ we can derive a general upper bound on the generalization error, given as follows.
%     \begin{corollary}
%         Under assumption \ref{assump1}, with probability at least $1-\xi-\frac{k\sqrt\delta}{\sqrt\pi}-k\exp\lef(-C\delta^{-1/2}\rig)$ for all $\delta<d^{-1}$ we have for all $\epsilon,\epsilon_0>0$,
%         \begin{align*}
%             \bb E\lef[L\lef(TF_{\wha\theta}(\bfa H),\bfa V\rig)|\wha\theta\rig]&\leq \bb E\lef[C\tau\epsilon k\lambda_1^2+C\bl\frac{\epsilon_0\lambda_1^2}{\Delta^2}\sum_{\eta=1}^{k-1}\prod_{i=1}^{\eta}\frac{25\lambda_{i+1}^2}{\Delta^2}\br^{1/2}\rig]\\
%             &+C\sqrt{\frac{k^3\log(\delta/
%             \epsilon_0)\lambda_1^d d^2\log(B_{\theta}+B_X+k)+\log(1/\xi)}{n\epsilon_0\epsilon^2}}.
%         \end{align*}
%     \end{corollary}
%     \begin{remark}
%         If we consider optimizing the bound w.r.t. $\epsilon_0$ and $\epsilon$, we obtain that $\bb E\lef[L(TF_{\wha\theta}(\bfa H),\bfa V)|\wha\theta\rig]\lesssim n^{-1/5}$ given that the rest of the parameters are of constant scales. It is not known if the results are improvable or not and the authors believe this question worth future explorations.
%     \end{remark}
% The next theorem demonstrates that there exists a Transformer that simulates the expectation maximization algorithm on Gaussian mixture model.
% \begin{theorem}
    
% \end{theorem}
% \begin{proof}
% We first consider the input matrix to be 
% \begin{align*}
%     \bfa H_1:=\bfa H=\begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_n\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(n)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \bfa p_{3,1}&\bfa p_{3,2}&\ldots&\bfa p_{3,N}
%     \end{bmatrix}\in\bb R^{D\times N}, 
% \end{align*}
% where $\wh z^{(0)}:[n]\to[k]$ is the assignment function, $\wha\mu_i\in\bb R^{d}$ is the initially estimated centroid for the $i$-th cluster. $\bfa p_{1,i}\in\bb R^k$ satisfies $\bfa p_{1,i,j}=\mbbm 1_{\wh z^{(0)}(i)=j}$ for all $j\in[k]$. And for $\bfa p_{2,i}$ we have $\bfa p_{2,i,j} = \mbbm 1_{j=i}$ for $i\leq d$ and $\bfa p_{2,i,j}=0$ for $i\leq N$ and $j\leq d$. We let $\bfa p_{3,1}=\bfa p_{3,2}=\ldots=\bfa p_{3,N} = \bfa 0\in\bb R^k$.
% We note that algorithm \ref{alg:lloyd} consists of two iterative steps: (1) The expectation step where we take the averages to get an initial estimate $\wha \mu_{\ell}^{(t)}$. (2) The maximization step where we assign each individuals their labels. Our following discussions treat the two steps separately. 
% \begin{center}
%     \textbf{1. The Expectation Step.}
% \end{center}
% To achieve the first step, we construct our transformer weights as follows:
% \begin{align*}
%     \bfa V_1^{(1)}=\begin{bmatrix}
%         \tda V_{1,1}^{(1)}&\tda V_{1,2}^{(1)}&\tda V_{1,3}^{(1)}
%     \end{bmatrix},\quad\bfa Q_1^{(1)}=\begin{bmatrix}
%         \bfa 0_{1\times(3d+k)}&1&\bfa 0\\
%         \bfa 0&\bfa 0&\bfa 0
%     \end{bmatrix},\quad\bfa K_1^{(1)}=\begin{bmatrix}
%         \bfa 0_{1\times(3d+k)}&1&\bfa 0\\
%         \bfa 0&\bfa 0&\bfa 0
%     \end{bmatrix},
% \end{align*}
% where $\tda V_{1,1}^{(1)}\in\bb R^{2d\times D}=\bfa 0$, $\tda V_{1,2}^{(1)}=\begin{bmatrix}
%         \bfa 0_{3d+k}\\
%         I_k\\
%         \bfa 0
%     \end{bmatrix}\in\bb R^{k\times D}$. Then we can show that
% \begin{align*}
%    (\bfa K_1^{(1)}\bfa H_1)^\top=(\bfa Q_1^{(1)}\bfa H)^\top = \begin{bmatrix}
%         1&\bfa 0\\
%         \vdots &\bfa 0\\
%         1&\bfa 0
%     \end{bmatrix}.
% \end{align*}
% Then we can show that
% \begin{align*}
%    (\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H) = \begin{bmatrix}
%        \bfa v_1&\ldots&\bfa v_1
%    \end{bmatrix},\quad \bfa v_{1,i} = 1\quad\forall i\in[N].
% \end{align*}
% Hence, we can obtain that
% \begin{align*}
%     \bfa V_1^{(1)}\bfa H&\times\sigma((\bfa Q_1^{(1)}\bfa H)^\top(\bfa K_1^{(1)}\bfa H))=\bfa V_1\bfa H\times\begin{bmatrix}
%         \bfa v_1 &\ldots&\bfa v_1
%     \end{bmatrix}=\bfa V_1\times \begin{bmatrix}
%         &\bfa A_0&\\
%         \bfa v_k &\ldots&\bfa v_k\\
%         &\bfa A_1&
%     \end{bmatrix}\\
%     &=\begin{bmatrix}
%         \tda V_{1,1}^{(1)}&\tda V_{1,2}^{(1)}&\tda V_{1,3}^{(1)}
%     \end{bmatrix}\begin{bmatrix}
%         &\bfa A_0&\\
%         \bfa v_k&\ldots&\bfa v_k\\
%         &\bfa A_1&
%     \end{bmatrix}=\tda V_{1,2}^{(1)}\begin{bmatrix}
%         \bfa v_k &\ldots&\bfa v_k
%     \end{bmatrix}\\
%     &=\begin{bmatrix}
%         \bfa 0_{3d+k}\\
%         I_k\\
%         \bfa 0
%     \end{bmatrix}\begin{bmatrix}
%         \bfa v_k&\ldots&\bfa v_k
%     \end{bmatrix}=\begin{bmatrix}
%         &\bfa 0_{3d+k}&\\
%         \bfa v_k&\ldots&\bfa v_k\\
%         &\bfa 0&
%     \end{bmatrix},
% \end{align*}
%  where $\bfa A_0\in\bb R^{2d\times N}$ and $
%     \bfa v_{k,\ell}=\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=\ell}$.
% Then it is checked that
% \begin{align*}
%    \bfa H_2 =  \bfa H_1+ \bfa V_1^{(1)}\bfa H_1\times\sigma\lef((\bfa Q_1^{(1)}\bfa H_1)^\top(\bfa K_1^{(1)}\bfa H_1)\rig)=\begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
%         \bfa p_{4,1}& \bfa p_{4,2}&\ldots &\bfa p_{4,N}
%     \end{bmatrix}.
% \end{align*}
% Therefore, we further construct the following multi-head layer to remove the off-diagonal elements in $\begin{bmatrix}
%     \bfa v_k&\bfa v_k&\ldots&\bfa v_k
% \end{bmatrix}$, given by 
% \begin{align*}
%     \bfa V_i^{(2)} = \begin{bmatrix}
%         &\bfa 0_{(3d+2k+i)\times D}&\\
%         \bfa 0_{1\times(3d+2k+i)}&1&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix},\quad \bfa Q_1^{(2)} = \begin{bmatrix}
%         b
%     \end{bmatrix},\quad\bfa K_1^{(2)} = \begin{bmatrix}
%         c
%     \end{bmatrix},\qquad\text{ for }i\in[k].
% \end{align*}
% Given this formulation, we can show that
% \begin{align*}
%     \sigma((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)) = \begin{bmatrix}
%         &\bfa 0_{(i-1)\times N}&\\
%         \bfa 0_{1\times(i-1)}&1&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix}.
% \end{align*}
% Hence, we can further show that
% \begin{align*}
%     \bfa V_{i}^{(2)}\bfa H_2\sigma((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2))=\begin{bmatrix}
%         &\bfa 0_{(3d+2k+i)\times D}&\\
%         \bfa 0_{(i-1)}&\bfa v_{k,i}&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix},
% \end{align*}
% which immediately implies that
% \begin{align*}
%     \sum_{i=1}^k\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
%         \bfa 0_{(3d+2k)\times N}&\\
%         \diag(\bfa v_k)&\bfa 0\\
%         \bfa 0
%     \end{bmatrix}.
% \end{align*}
% Given the above design, we can show that
% \begin{align*}
%     \bfa H_{3,1}=\bfa H_2+\sum_{i=1}^k\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
%         \diag(\bfa v_k)&&\bfa 0&\\
%         &\bfa 0&&
%     \end{bmatrix}.
% \end{align*}
% Then, we construct the MLP layer to remove the $\bfa v_k$ part, which is designed by
% \begin{align*}
%     \bfa W_1^{(2)}=I_D,\quad \bfa W_2^{(2)}= \begin{bmatrix}
%         &\bfa 0_{(3d+k)\times D}&&\\
%         \bfa 0_{k\times (3d+k)}&-I_k&I_k&\bfa 0_{}\\
%        \bfa 0 &-I_k&\bfa 0&\bfa 0\\
%        &\bfa 0&&
%     \end{bmatrix}.
% \end{align*}
% Given this formulation, we can show that
% \begin{align*}
%     \bfa H_3 := \bfa H_{3,1}+\bfa W_1^{(2)}\sigma\lef(\bfa W_2^{(2)}\bfa H_{3,1}\rig)=\begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
%         \diag(\bfa v_k)&&\bfa 0&\\
%         &\bfa 0&&
%     \end{bmatrix}.
% \end{align*}

% % Then we consider the following attention head which essentially remove the $\bfa v_k$ part in $\bfa H_2$. We note that the rank of $\sigma((\bfa Q_i\bfa H_2)^\top(\bfa K_i\bfa H_2))$ is at most $D$. To form the 
% % \begin{align*}
% %     \bfa V_{k+1}^{(2)} = \begin{bmatrix}
% %         &\bfa 0_{(3d+k)\times D}&\\
% %         \bfa 0_{k\times (3d+k)}&-I_k&\bfa 0_{}\\
% %         &\bfa 0&
% %     \end{bmatrix},\quad \bfa Q_{k+1}^{(2)}=,\quad \bfa K_{k+1}^{(2)}=
% % \end{align*}
% % Given the above design, we can show that
% % \begin{align*}
% %     \bfa V_{k+1}^{(2)}\bfa H_2\sigma\lef((\bfa Q_{k+1}^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
% %         &\bfa 0_{(3d+k)\times N}&\\
% %         -\bfa v_k&\ldots&-\bfa v_k\\
% %         &\bfa 0&
% %     \end{bmatrix}.
% % \end{align*}
% % Therefore, collecting pieces, we can show that
% % \begin{align*}
% %    \bfa H_3:= \bfa H_2 + \sum_{i=1}^{k+1}\bfa V_i^{(2)}\bfa H_2\sigma\lef((\bfa Q_i^{(2)}\bfa H_2)^\top(\bfa K_i^{(2)}\bfa H_2)\rig)=\begin{bmatrix}
% %         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
% %         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
% %         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
% %         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
% %         1&1&\ldots&1\\
% %         \diag(\bfa v_k)&&\bfa 0&\\
% %         \bfa p_{4,1}& \bfa p_{4,2}&\ldots &\bfa p_{4,N}
% %     \end{bmatrix}.
% % \end{align*}
% The following layer converts the term $\diag(\bfa v_k)$ to $\diag(\bfa v_k^\prime)$ where $\bfa v_{k,i}^\prime = 1/\bfa v_{k,i}$. The design is given as follows
% \begin{align*}
%     \bfa V_{i}^{(3)} &= \begin{bmatrix}
%         &\bfa 0_{(3d+3k+1)\times D}&\\
%         \bfa 0_{k\times(2d+2k)}&\diag(c_i)_{k\times k}&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix},\quad\bfa Q_{i}^{(3)}=\begin{bmatrix}
%         &\bfa 0_{(3d+3k+1)\times D}&\\
%         \bfa 0_{k\times(2d+2k)} &I_{k}&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix},\\
%     \bfa K_{i}^{(3)}&=\begin{bmatrix}
%         &\bfa 0_{(3d+3k+1)\times D}&\\
%         \bfa 0_{k \times (3d+3k+1)}&\diag(a_i)_{k\times k}&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix}\begin{bmatrix}
%         &\bfa 0_{(3d+3k+1)\times D}&\\
%        \bfa 0_{k\times(3d+2k+1)}&I_{k} &\bfa 0\\
%        &\bfa 0&
%     \end{bmatrix},
% \end{align*}
% where we show in lemma \ref{reluapprox} that for $M>\frac{1}{\epsilon^2}\log(1+C/\epsilon)$, there exists $\{a_i\}_{i\in[M]}$ such that for $x>1$, we have 
% \begin{align*}
%     \bigg\Vert\sum_{i=1}^Mc_i\sigma(a_ix) - \frac{1}{x}\bigg\Vert_2\leq\epsilon.
% \end{align*}
% And when $x=0$, we automatically obtain that 
% $   \sum_{i=1}^Mc_i\sigma(0) =0$. We then immediately obtain that
% \begin{align*}
%     \bfa H_{4,1}:&=\bfa H_3 + \sum_{i=1}^M\bfa V_i^{(3)}\sigma\lef((\bfa Q_i^{(3)}\bfa H_3)^\top(\bfa K_i^{(3)}\bfa H_3)\rig)= \begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
%         \diag(\bfa v_k)&&\bfa 0&\\
%         \diag(\bfa v_k^\prime)&&\bfa 0&\\
%         &\bfa 0&&
%     \end{bmatrix}+O_{2}(\epsilon),
% \end{align*}
% where $\bfa v_{k,i}^\prime = \bfa v_{k,i}^{-1}$. Then we apply the MLP again with the following design 
% \begin{align*}
%     \bfa W_1^{(4)}= I_D,\quad\bfa W_2^{(4)}=\begin{bmatrix}
%         &\bfa 0_{(3d+k)\times D}&&\\
%         \bfa 0_{k\times(3d+k)}&-I_k&I_k&\bfa 0\\
%         \bfa 0&-I_k&\bfa 0&\bfa 0\\
%         &\bfa 0&&
%     \end{bmatrix}.
% \end{align*}
% The above construction implies that
% \begin{align*}
%     \bfa H_4 =\bfa W_2^{(3)}\sigma\lef(\bfa W_1^{(3)}\bfa H_3\rig) = \begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \bfa v_k&\bfa v_k&\ldots&\bfa v_k\\
%         \diag(\bfa v_k^\prime)&&\bfa 0&\\
%         &\bfa 0&&
%     \end{bmatrix}.
% \end{align*}
% We construct the following layer to perform the normalization, given by 
% \begin{align*}
%     \bfa V_2^{(4)}&=-\bfa V_{1}^{(4)}= \begin{bmatrix}
%     &\bfa 0_{(3d+2k+1)\times D}& \\
%     \bfa 0_{k\times(3d+k+1)}&I_{k}&\bfa 0\\
%     &\bfa 0&
%     \end{bmatrix},\quad \bfa Q_1^{(4)} =-\bfa Q_2^{(4)}= \begin{bmatrix}
%         &\bfa 0_{(3d+2k+1)\times D}&\\
%         \bfa 0_{k\times 3d}&I_{k}&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix},\\
%     \bfa K_1^{(4)}&=\bfa K_2^{(4)}=\begin{bmatrix}
%         &\bfa 0_{(3d+2k+1)\times D}&\\
%         \bfa 0_{k\times (3d+1)}&I_k&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix}.
% \end{align*}
% Then we can show that
% \begin{align*}
%     \sigma\lef((\bfa Q_1^{(4)}\bfa H_4)^\top(\bfa K_1^{(4)}\bfa H_4)\rig)-\sigma\lef((\bfa Q_2^{(4)}\bfa H_4)^\top(\bfa K_2^{(4)}\bfa H_4)\rig) = \begin{bmatrix}
%         &\bfa 0_{(3d+2k+1)\times D}&\\
%         \bfa p_{1,1}&\ldots&\bfa p_{1,N}\\
%         &\bfa 0
%     \end{bmatrix}.
% \end{align*}
% And we also have
% \begin{align*}
%     \bfa V_2^{(4)}\bfa H_4 = \begin{bmatrix}
%         &\bfa 0_{(3d+3k+1)\times D}&\\
%         \bfa 0_{k\times(3d+2k+1)}&\diag(\bfa v_k^\prime) &\bfa 0\\
%         &\bfa 0&
% \end{bmatrix}+O_2(\epsilon),
% \end{align*}
% which implies that
% \begin{align*}
%     \bfa H_{4,1} = \bfa H_3+\sum_{i=1}^2\bfa V_i^{(4)}\bfa H_3\times\sigma\lef((\bfa Q_i^{(4)}\bfa H_3)^\top(\bfa K_i^{(3)}\bfa H_3)\rig)=\begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \diag(\bfa v_k^\prime)&&\bfa 0&\\
%         \bfa p_{1,1}^\prime&\bfa p_{1,2}^\prime&\ldots&\bfa p_{1,N}^\prime
%     \end{bmatrix}+O_{2}(\epsilon),
% \end{align*}
% where $\bfa p_{1,i}^\prime = \diag(\bfa v_k^\prime)\bfa p_{1,i}$ for all $i\in[N]$. We therefore construct an MLP layer to replace the $\bfa p_{1}$ part using the following design
% \begin{align*}
%     \bfa W_1^{(4)}=I_D,\qquad\bfa W_2^{(4)} = \begin{bmatrix}
%         &\bfa 0_{2d\times D}&&\\
%         \bfa 0_{k\times 2d}&-I_k&\bfa 0&I_k&\bfa 0\\
%         \bfa 0&\bfa 0&\bfa 0&\bfa 0&\bfa 0\\
%         \bfa 0_{k\times 2d}&-I_k&\bfa 0&\bfa 0 &\bfa 0\\
%     \end{bmatrix},
% \end{align*}
% which ultimately leads to 
% \begin{align*}
%     \bfa H_4 = \bfa W_1^{(3)}\sigma\lef(\bfa W_2^{(3)}\bfa H_3\rig) = \begin{bmatrix}
%         \bfa x_1&\bfa x_2&\ldots&\bfa x_N\\
%         \wha \mu_{\wh z^{(0)}(1)}^{(0)}&\wha \mu_{\wh z^{(0)}(2)}^{(0)}&\ldots&\wha \mu_{\wh z^{(0)}(N)}^{(0)}\\
%         \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p^\prime_{1,N}\\
%         \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
%         1&1&\ldots&1\\
%         \diag(\bfa v_k^\prime)&&\bfa 0&\\
%         &\bfa 0&&
%     \end{bmatrix}.
% \end{align*}
% We can further perform the expectation step by the following layer
% \begin{align*}
%     \bfa V_i^{(4)}=\begin{bmatrix}
%         &\bfa 0_{(3d+2k+1)\times D}&\\
%         \bfa 0_{k\times2d}& I_k&\bfa 0\\
%         &\bfa 0&
%     \end{bmatrix},\quad\bfa Q_i^{(4)}=\begin{bmatrix}
%         \bfa 0&\bfa 0&\bfa 0\\
%         \bfa 0_{k\times2d}& I_k&\bfa 0\\
%         \bfa 0&\bfa 0&\bfa 0
%     \end{bmatrix},\quad\bfa K_i^{(4)}=\begin{bmatrix}
%         \bfa 0&\bfa 0&\bfa 0\\
%         \bfa 0&\bfa 0&\bfa 0\\
%         \bfa 0&\bfa 0&\bfa 0
%     \end{bmatrix}
% \end{align*}
% Then it is further noted that
% \begin{align*}
%     \bfa H_5 =\bfa H_4+\sum_{i=1}^2\bfa V_i^{(4)}
% \end{align*}
% \end{proof}
% The rest of this section presents a proof sketch on theorem \ref{thm3.1}.
% \paragraph{Proof Sketch of Theorem \ref{thm3.1}.}
%     The proof sketch for theorem \ref{thm3.1} goes by two important steps:



%     To obtain $\tda p_{3,i}$ for $i\in[k]$, we generate through the probablistic method, which gives the following lemma.
%     % \begin{lemma}
%     %     Let $\bfa y\in\bb R^d$ be a random vector with isotropic Gaussian as its probability density. Consider $\bfa x=\frac{\bfa y}{\Vert\bfa y\Vert_2}$. Let $\bfa v$ be any unit length vector, then we have
%     %        $$ \bb P\bl|\bfa v^\top\bfa x|\leq\frac{\epsilon}{\sqrt d}\br\leq  \frac{1}{\sqrt{\pi}}\epsilon+\exp\lef(-Cd\rig).$$
%     %     Therefore, the event in theorem \ref{thm3.1} can be estimated as
%     %     \begin{align*}
%     %          \bb P\bl\exists i\text{ such that }&\bfa x_i^\top\bfa v_i\leq\frac{\epsilon}{\sqrt d}\br\leq 1-\bl1-\frac{\epsilon}{\sqrt{\pi}}-\exp(-Cd)\br^k\leq \frac{k\epsilon}{\sqrt{\pi}}+k\exp(-Cd).
%     %     \end{align*}
%     % \end{lemma}

%     % \begin{align*}
%     %    \Bigg\Vert \tda H^{rpe,2}-\begin{bmatrix}
%     %         \bfa X\\
%     %         \tda y\\
%     %         \bfa X\bfa X^\top,\bfa 0\\
%     %         \tda p_{2,1},\ldots,\tda p_{2,N}\\
%     %         \tda p_{3,1},\ldots,\tda p_{3,N}\\
%     %         \tda p_{3,1}^{(\tau)},\bfa 0\\
%     %         \Vert\bfa X\bfa X^\top\tda p_{3,N}^{(\tau)}\Vert_2,\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2\leq\epsilon \Vert\bfa X\bfa X^\top\tda p_{3,1}\Vert_2.
%     % \end{align*}
%     % Then we can show that
%     % \begin{align*}
%     %     &\Bigg\Vert\bfa K\tda H^{pow,2\tau+2}-\begin{bmatrix}
%     %         \bfa 0_{(5d+1)\times(4d+1)}&\bfa 0_{(5d+1)\times 1}&\bfa 0\\
%     %         \bfa 0_{d\times(4d+1)}&\tda p_{3,1}^{(\tau)}&\bfa 0\\
%     %         \bfa 0&\bfa 0&\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2\leq \tau\epsilon\Vert\bfa X\bfa X^\top\Vert_2,\\ 
%     %     &\Bigg\Vert\bfa Q\tda H^{pow,2\tau+2}-\begin{bmatrix}
%     %         \bfa 0_{(5d+1)\times 1}&\bfa 0\\
%     %        \Vert\bfa X\bfa X^\top\tda p_{3,1}^{(\tau)}\Vert_2 \tda p_{3,1}^{(\tau)}&\bfa 0\\
%     %         \bfa 0&\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2\leq\tau\epsilon\Vert\bfa X\bfa X^\top\Vert_2.
%     % \end{align*}
%     % which immediately implies that
%     % \begin{align*}
%     %     \Bigg\Vert(\bfa K\tda H^{pow,2\tau+2})^\top(\bfa Q\tda H^{pow,2\tau+2})-\begin{bmatrix}
%     %         \bfa 0&\bfa 0\\
%     %         \Vert\bfa X\bfa X^\top\tda p_{3,1}\Vert_2\tda p_{3,1}^{(\tau)}\tda p_{3,1}^{(\tau),\top}&\bfa 0\\
%     %         \bfa 0&\bfa 0
%     %     \end{bmatrix}\Bigg\Vert_2
%     % \end{align*}
% \begin{lemma}\label{lm3.2}
% Assume that the correlation matrix $\bfa X\bfa X^\top$ has eigenvalues $\lambda_1>\lambda_2>\ldots>\lambda_{k}$. Assume that the eigenvectors are given by $\bfa v_1,\bfa v_2,\ldots,\bfa v_n$ and the eigenvalues satisfy $\inf_{i\neq j}|\lambda_i-\lambda_j|=\Delta$. Then, given that the estimate for the first $\tau$ eigenvectors satisfy
% $    \bfa v_i^\top\wha v_i\geq 1-\epsilon_i $ and the eigenvalues satisfy $|\lambda_i-\wh\lambda_i|\leq\delta_i$, the principle eigenvector of $\bfa X\bfa X^\top-\sum_{i=1}^{\tau}\wh\lambda_i\wha v_i\wha v_i^\top$ denoted by $\tda v_{\tau+1}$ satisfies
% \begin{align*}
%     \Vert\tda v_{\tau+1}-\bfa v_{\tau+1}\Vert_2 \leq\frac{\max_{i\in[\tau]}\delta_i+\sum_{i=1}^\tau\sqrt8\lambda_i\sqrt{\epsilon_i}}{\Delta}.
% \end{align*}
% Alternatively, we can also show that the eigenvector $\wha v_{\tau+1}$ returned by power method with $k= \frac{\log(1/\epsilon_0\delta)}{2\epsilon_0}$ that is initialized by  satisfies
% \begin{align*}
%     \wha v_{\tau+1}^\top\bfa v_{\tau+1} \geq 1-\epsilon_{\tau+1}:=  1-\frac{1}{2}\Big(\frac{\max_{i\in[\tau]}\delta_i+\sum_{i=1}^{\tau}\sqrt{8}\lambda_i\sqrt{\epsilon_i}}{\Delta}+\sqrt{2\epsilon_0}\Big)^2,
% \end{align*}
% and alternatively we have $\Vert\wha v_{\tau+1}-\bfa v_{\tau+1}\Vert_2\leq\sqrt{2\epsilon_{\tau+1}}$. Furthermore, consider the eigenvalue estimate, we show that
% \begin{align*}
%    \Big|\Big\Vert\Big(\bfa X\bfa X^\top-\sum_{i=1}^{\tau}\wh\lambda_i\wha v_i\wha v_i^\top\Big)\wha v_{\tau+1}\Big\Vert_2-\lambda_{\tau+1}\Big|\leq \frac{2\lambda_{\tau+1}}{\Delta}\Big(\max_{i\in[\tau]}\delta_i+\sum_{i=1}^{\tau}\sqrt{8}\lambda_i\sqrt{\epsilon_i}\Big)+\lambda_{\tau+1}\sqrt{2\epsilon_0}
% \end{align*}
% \end{lemma}

% \begin{lemma}[Approximation of norm by sum of Relu activations by Transformer networks]\label{reluapprox}
%     Assume that there exists a constant $C$ with $\Vert\bfa v\Vert_2\leq C$. There exists a multihead Relu attention layer with number of heads $M<\lef(\frac{\overline R}{\underline R}\rig)^d\frac{C(d)}{\epsilon^2}\log(1+C/\epsilon)$ such that there exsits $\{\bfa a_m\}_{m\in[M]}\subset\bb S^{N-1}$ and $\{c_m\}_{m\in[M]}\subset\bb R$ where for all $\bfa v$ with $\overline R\geq\Vert\bfa v\Vert_2\geq \underline R$, we have 
%     \begin{align*}
%         \bigg|\sum_{m=1}^Mc_m\sigma(\bfa a_m^\top\bfa v)-\frac{1}{\Vert\bfa v\Vert_2}+1\bigg|\leq\epsilon.
%     \end{align*}
%     Similarly, there exists a multihead Relu attention layer with number of heads $M\leq\overline R^{\frac{d}{2}}\frac{C(d)}{\epsilon^2}\log\lef(1+C/\epsilon\rig)$, a set of vectors $\{\bfa b_m\}_{m\in[M]}\subset\bb S^{N-1}$ and $\{d_m\}_{m\in[M]}\subset\bb R$ such that 
%     \begin{align*}
%         \Big|\sum_{m=1}^Md_m\sigma(\bfa b_m^\top\bfa v)-\Vert\bfa v\Vert_2^{1/2}+1\Big|\leq\epsilon.
%     \end{align*}
% \end{lemma}



