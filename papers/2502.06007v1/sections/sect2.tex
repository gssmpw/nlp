\section{Connecting Transformers with EM}\label{sect2}
This section discusses the connections between the EM algorithm and the Transformer architecture. 
Our discussion is split into $2$ separate subsections: In \ref{Transformers}, we review the mathematical definitions of the Softmax-based Transformer model; In \ref{emintro}, we review the EM algorithm and connect it with the multiphase Transformer design. In section \ref{pretrain}, we discuss the pretraining procedure of the Transformers.
\subsection{The Transformer Architecture}\label{Transformers}
We consider the Softmax Attention Layer, which is defined as follows:
\begin{definition}[Softmax Attention]\label{def:attention}
  The Softmax Attention layer is defined as a self-attention layer with $M$ heads denoted as $Attn_{\bfa\theta_1}(\cdot)$ with parameters $\bfa\theta_1=\{(\bfa V_m,\bfa Q_m,\bfa K_m)\}_{m\in[M]}\subset\bb R^{D\times D}$. On input sequence $\bfa H\in\bb R^{D\times N}$,
    {\begin{align*}
       Attn_{\bfa\theta_1}&(\bfa H)=
            \bfa H\\
            &+\sum_{m=1}^M(\bfa V_m\bfa H)\softmax\Big((\bfa Q_m\bfa H)^\top(\bfa K_m\bfa H)\Big),
    \end{align*}}
    where $\softmax$ is the activation function defined by
    \begin{align*}
        \softmax(\bfa x) = \begin{bmatrix}
            \frac{\exp(x_1)}{\sum_{i=1}^d\exp(x_i)}&\ldots&\frac{\exp(x_d)}{\sum_{i=1}^d\exp(x_d)}
        \end{bmatrix}^\top,
    \end{align*}
    for all $\bfa x\in\bb R^d$.
\end{definition}
In addition to the Softmax Attention layer, we also consider an un-normalized Attention layer, given by
\begin{definition}[Un-normalized Attention]\label{def:mattention}
  The un-normalized Attention layer is defined as a self-attention layer with $M$ heads and denoted as $nAttn_{\bfa\theta_1}(\cdot)$ with parameters $\bfa\theta_1=\{(\bfa V_m,\bfa Q_m,\bfa K_m)\}_{m\in[M]}\subset\bb R^{D\times D}$. On input sequence $\bfa H\in\bb R^{D\times N}$,
    {\begin{align*}
       nAttn_{\bfa\theta_1}&(\bfa H)=
            \bfa H+\sum_{m=1}^M(\bfa V_m\bfa H)(\bfa Q_m\bfa H)^\top(\bfa K_m\bfa H).
    \end{align*}}
\end{definition}
\begin{remark}
    The un-normalized Attention layer is the Attention layer without the non-linear activation function. This layer is studied mainly for technical reasons. We also provide results not using the un-normalized Attention layer, despite having weaker rates. 
\end{remark}
% \begin{remark}
%     We note that in the literature \citep{bai2024transformers} only considers the approximation guarantees yield by the ReLU Transformers. However, existing LLMs predominantely utilize the Softmax-based Attention layers. Our approximation guarantee for the Softmax Attention layers is new in the literature, relying on our new approximation result for the Softmax function, discussed in section \ref{sect31}. 

%     We also replace concatenation of multihead attentions by their average. Despite adaptations are employed for the technical convinience, in the simulation section we carefully evaluate the effect of these modifications on the performance of the model.
% \end{remark}
The following defines the classical Fully-Connected (FC) layers with residual connections.
\begin{definition}[FC Layer] A FC layer with hidden dimension $D^\prime$ is denoted as $FC_{\bfa\theta}(\cdot)$ with parameter $\bfa\theta_2\in(\bfa W_1,\bfa W_2)\in\bb R^{D^\prime\times D}\times\bb R^{D\times D^\prime}$. On any input sequence $\bfa H\in\bb R^{D\times N}$, we define
$$   FC_{\bfa\theta_2}(\bfa H):=\bfa H+\bfa W_2\sigma(\bfa W_1\bfa H).$$
\end{definition}
Then, we use the above definitions on the FC and the Attn/nAttn layers to define the Transformer model and the Transformer+ model.
\begin{definition}[Transformer]\label{transform}
    We define the Transformer $TF_{\bfa\theta}(\cdot)$ as a composition of the self-attention layers with the FC layers. Consider the output dimension to be $\tilde D$, a $L$-layered Transformer is defined by  \sm{\begin{align*}
        TF_{\bfa\theta}&(\bfa H) :=\\
        &\tda W_0\times FC_{\bfa\theta_{2}^L}(Attn_{\bfa\theta_{1}^L}(\cdots FC_{\bfa\theta_{2}^1}(Attn_{\bfa\theta_{1}^1}(\bfa H)))\times\tda W_1,
    \end{align*}}
    where $\tda W_0\in\bb R^{d_1\times D}$ and $\tda W_1\in\bb R^{N\times d_2}$.
\end{definition}
The two additional matrices $\tda W_0$ and $\tda W_1$ serve for the dimension adjustment purpose such that the output of $TF_{\bfa\theta}(\bfa H)$ or $TF^+_{\bfa\theta}(\bfa H)$ will be of dimension $\bb R^{d_1\times d_2}$.

Then, we introduce a class of models called the Transformer+, which includes the un-normalized Attention layer.
\begin{definition}[Transformer+]
    Under the same notations as definition \ref{transform}. We define the Transformer+ model $TF_{\bfa\theta}^+(\cdot)$ as
    \sm{\begin{align*}
        TF_{\bfa\theta}^+&(\bfa H) :=\tda W_0\times FC_{\bfa\theta_{2}^L}(A_{\bfa\theta_{1}^L}(\cdots FC_{\bfa\theta_{2}^1}(A_{\bfa\theta_{1}^1}(\bfa H)))\times\tda W_1,
    \end{align*}}
    where $A\in\{Attn,nAttn\}$ is either the Attn layer defined in definition 2.1 or the nAttn layer defined in definition 2.2.
\end{definition}

We use $\bfa \theta$ to denote all the parameters in the Transformer and the super-index $\ell$ to denote the parameter matrix corresponding to the $\ell$-th layer.
    Under such definition, the parameter $\bfa\theta$ is given by 
    \sm{\begin{align*}
        \bfa \theta = \{\{(\{\bfa Q_m^{(\ell)},\bfa K_m^{(\ell)},\bfa V_m^{(\ell)}\}_{m\in[M]}, \bfa W_{1}^{(\ell)},\bfa W_{2}^{(\ell)})\}_{\ell\in[L]},\tda W_{0},\tda W_{1}\}.
    \end{align*}}
Following the notations in \citep{bai2024transformers}, we define the operator norm of the parameter $\bfa\theta$ as follows.
    \begin{align*}
&\vertiii{\bfa\theta}:=\max_{\ell\in[L]}\Big\{\max_{m\in[M^{(\ell)}]}\lef\{\Vert \bfa Q_m^{(\ell)}\Vert_2,\Vert\bfa K_m^{(\ell)}\Vert_2\rig\}\\
&+\Vert\tda W_0\Vert_2+\Vert\tda W_1\Vert_2+\sum_{m=1}^{M^{(\ell)}}\Vert\bfa V_m^{(\ell)}\Vert_2+\Vert\bfa W_1^{(\ell)}\Vert_2+\Vert\bfa W_2^{(\ell)}\Vert_2\Big\}, 
    \end{align*}
    where $M^{(\ell)}$ is the number of heads of the $\ell$-th attention layer.
    It is also shown in \citep{bai2024transformers} that such a norm relates to the Lipschitz constant of Transformers, which controls the model complexity and leads to the generalization bound. Hence, in this work, we consider the following space of the model
    \begin{align*}
        \Theta(B_{\bfa\theta},B_{M},B_L) &= \bigg\{(\bfa\theta, \{M^{(\ell)}\}_{\ell\in[L]}, L): \vertiii{\bfa\theta}\leq B_{\bfa\theta},\\
        &\quad\quad  \sup_{\ell\in[L]}M^{(\ell)}\leq B_{M}, L\leq B_L \bigg\}.
    \end{align*}
    And for the subspace of $\bfa\theta$ given $M$ and $L$ as hyperparamaters, we denote by
    $\Theta_{B_M,B_L}(B_{\bfa\theta})$.
    

\subsection{The Learning Problem and EM}\label{emintro}
In this section, we first provide notations for the sub-Gaussian mixture models and the clustering problem. Then, we provide the literature on the EM Algorithm and Lloyd's algorithm.
\subsubsection{Clustering Mixture of Gaussians}
 We take samples $\{\bfa X_i\}_{i\in[N]}$ from a sub-Gaussian mixture model with in total of $k$ centers $\{\bfa\mu_i\}_{i\in[k]}$. In particular, we let
 \begin{align*}
  \bfa X:=\begin{bmatrix}
      \bfa X_1\ldots\bfa X_N
  \end{bmatrix},\quad   \bfa X_i := \bfa\mu_{z_i}+\bfa\omega_i\text{ for all }i\in[N],
 \end{align*}
 where $z_i:\in[k]$ corresponds to the membership of $i$-th index. We assume the following condition to hold for $\bfa\omega_i$.
 \begin{assumption}
     $\{\bfa\omega_i\}_{i\in[N]}$ are i.i.d. zero mean random variables from sub-Gaussian distribution that satisfies 
     $\bb E[\exp(\bfa a^\top\bfa\omega)]\leq\exp\lef(\frac{1}{2}\sigma^2\Vert\bfa a\Vert_2^2\rig)$ for all $\bfa a\in\bb R^d$.
 \end{assumption}
 We consider the mapping from $z$ to a set of one-hot vectors
\sm{\begin{align}\label{defp}
    \bfa P_1(z):=\begin{bmatrix}
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}
    \end{bmatrix}\in\bb R^{k\times N},\bfa p_{1,i,j}:=\mbbm 1_{j=z_i}.
\end{align}}

 
Define $\ca S_k:[k]\to[k]$ as the set of permutations of $[k]$. We consider the following loss function for the Transformer output. 
\begin{align*}
    L(A_{\bfa\theta}(\bfa H),\bfa P_1(\bfa z)):=\inf_{\pi\in\ca S_k}\frac{1}{N}\Vert\bfa P_1(\pi(\bfa z_i))-A_{\bfa\theta}(\bfa H)\Vert_{1,1},
\end{align*}
where $A\in\{TF,TF^+\}$. 
\paragraph{The Parameter Space} This work considers the following space of parameters of the generative model. Here, we denote $F_{\omega}$ as the distribution of the random variable $\omega$.
\begin{align*}
    \Theta_{GM}&=\Big\{(\bfa\mu,\bfa z,F_{\bfa \omega}),\bfa\mu\in\bb R^{d\times k}, \Delta\leq \min_{i\neq j}\Vert\bfa\mu_i-\bfa\mu_j\Vert_2,\\
    &\bfa z:[N]\to[k], |\{i\in[N],\bfa z_i=u\}|\geq\alpha n, \forall u\in[k], \\
    &\omega_i\text{ is i.i.d. }\sigma\text{ sub-Gaussian random variable } \forall i\in[N]\Big\}.
\end{align*}
We further consider the solution space $\Theta_{\bfa A}=\{\bfa A:\sum_{j=1}^N\bfa A_{ij}=1,\forall i\in[k],\bfa A\in[0,1]^{k\times N}\}$.
Then, the fundamental limit of the problem class $\Theta_{GM}$ is given by the following lemma.
\begin{lemma}[Lower Bound \citep{yu2015useful}]\label{minimaxlb} For model class $\Theta_{GM}$, given $\frac{\Delta}{\sigma \log(k/\alpha)}\to \infty$, 
\begin{align*}
    \inf_{\wha A\in\Theta_{\bfa A}}\sup_{(z,\theta,F_{\omega})}\bb E[L(\wha A,\bfa P_1)]\geq\exp\lef(-(1+o(1))\frac{\Delta^2}{8\sigma^2} \rig).
\end{align*}
\end{lemma}
\begin{remark}
    The above result implies that the difficulty of this problem is governed by the Signal-to-Noise ratio $\frac{\Delta}{\sigma}$. In particular, the above results imply that the minimax rate of this problem is largely dependent on the distance between the two closest centroids.
    We also note that the original result is instead on the $0-1$ loss between $\wha z$ and $\bfa z$. However, it is also not difficult to show the same results hold for the solution space $\Theta_{\bfa A}$ and our defined loss $L$.
\end{remark}

\subsubsection{The EM (Lloyd's) Algorithm}
Lloyd's algorithm is a special case of EM algorithm on the Gaussian mixture model, which is formally stated by Algorithm \ref{alg:loyld}. The Lloyd's algorithm iteratively updates: \textbf{(1)} The centroid of each cluster; \textbf{(2)} The membership of each sample. Since Lloyd's algorithm requires an initial input $\{\wha\mu_i^{(0)}\}$, \citet{lu2016statistical} has shown that given a proper initialization algorithm \ref{alg:example}, Lloyd's algorithm provably achieves good performance. An example initialization algorithm is given by algorithm \ref{alg:example} where the spectral algorithm and k-means++ algorithm \citep{kumar2004simple} are first called to obtain approximate solutions.
\begin{algorithm}[h]
   \caption{Lloyd's Algorithm}
   \label{alg:loyld}
\begin{algorithmic}
   \STATE {\bfseries Input:} A sample matrix from Mixture of Gaussians $\bfa X\in\bb R^{d\times N}$, number of iterations $\tau$, and initial centroids $\{\wha\mu^{(0)}_i\}_{i\in[k]}$.
   \STATE Compute the Initial Clusters 
   \begin{align}\label{z0}
       \wha z^{(0)}_i = \argmin_{i\in[k]}\Vert \bfa X_j - \wha\mu_i^{(0)}\Vert_2\quad\text{for all }j\in[N].
   \end{align}
   \FOR{$\ell=1$ {\bfseries to} $\tau$}
   \STATE \textbf{(1) The Expectation Step:} Update the centroid by 
   \sm{\begin{align*}
       \wha \mu_i^{(\ell)}=\frac{\sum_{j=1}^N\mbbm 1_{\wha z_j^{(\ell-1)}=i}\bfa X_j}{\sum_{j=1}^N\mbbm 1_{\wha z_j^{(\ell-1)}=i}}.
   \end{align*}}
   \STATE \textbf{(2) The Maximization Step:} Update the cluster assignment by
   \begin{align*}
       \wha z^{(\ell)}_j=\argmin_{i\in[k]}\Vert \bfa X_j-\wha\mu_i^{(\ell)}\Vert_2\quad\text{for all }j\in[N].
   \end{align*}
   \ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection{Pretraining with Supervised Learning}\label{pretrain}
The clustering problem is unsupervised where no labels are given. Transformers are usually used in the supervised learning setup. To let Transformers learn the algorithms, we perform supervised pre-training. 

In this setup, we are first given in a total of $n$ pretraining instances $\{\bfa X^{(i)}\}_{i\in[n]}$ and $\{\bfa z^{(i)}\}_{i\in[n]}$. We also form the pretraining instances by feeding the Transformer with the initialization given by \ref{alg:example}, encoded in $\{\bfa H^{(i)}\}_{i\in[n]}$. Then, we train the Transformer using the standard supervised learning on this set. Since the optimization of Transformers is non-convex and difficult to analyze, we consider the empirical risk minimizer of the Transformer given by 
\begin{align}\label{ERM}
    \wha\theta :=\argmin_{\bfa\theta\in\Theta_{(B_M,B_L)}(B_{\bfa\theta})}\sum_{i=1}^nL\lef(A_{\bfa\theta}(\bfa H^{(i)}),\bfa P_1(\bfa z^{(i)})\rig),
\end{align}
where $A\in\{TF,TF^+\}$.

In our theoretical analysis, we construct the input of the Transformer as a \emph{context-augmented matrix} given by the following
\sm{\begin{align}\label{aux}
    \bfa H=\begin{bmatrix}
        \bfa X\\
        \bfa P
    \end{bmatrix}, \bfa P=\begin{bmatrix}
        \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\wha \mu_{k}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}^{(0)}&\bfa p_{1,2}^{(0)}&\ldots&\bfa p_{1,k}^{(0)}&\ldots&\bfa p_{1,N}^{(0)}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        &&\bfa 0&&&
    \end{bmatrix},
\end{align}}
where $\bfa H\in\bb R^{D\times N}$ and $\bfa P^{(D-d)\times N}$. We let the input dimension $D\leq Ckd$ for some universal constant $C$.
The matrix $\bfa P$ contains contextual information. For the first row, $\{\wha\mu_i^{(0)}\}_{i\in[k]}\subset\bb R^{d}$ are the initial centroid estimates given by the initialization algorithm \ref{alg:example}. Then the next row $\{\bfa p_{1,i}\}_{i\in[N]}\subset[0,1]^{k}$ is given by $\bfa P_1(\wha z^{(0)})$ as in \eqref{defp} where $\wha z^{(0)}$ corresponds to the initialized membership in \eqref{z0}. Then the row $\{\bfa p_{2,i}\}_{i\in[N]}\subset\bb R^d$ satisfies
\begin{align*}
    \bfa p_{2,i,j}=\mbbm 1_{i=j}\text{ if }j\in[d].
\end{align*}
And the last row is set to all $1$ for the technical purpose of introducing constants into the Softmax function. 
\begin{algorithm}[tb]
   \caption{Initialization by Spectral Clustering}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} Matrix $\bfa X\in\bb R^{d\times N}$.
   \STATE Perform PCA on $\bfa X\bfa X^\top$ and obtain its top-k eigenvectors $\{\bfa V_i\}_{i\in[k]}$.
   \STATE Project the input matrix by $\tda X=\bfa V^\top \bfa X$.
   \STATE Solve the $k$-means program given by 
   \begin{align*}
       \tda z:=\argmin_{\wh z:[N]\to [k]}\min_{\{\bfa\mu_i\}_{i\in[N]}}\sum_{i=1}^k\Vert\bfa \mu_{\wh z_i}- \tda X_i\Vert_2
   \end{align*}
   by the $k$-means++ algorithm \citep{kumar2004simple}.
   \STATE{\bfseries Return:} Initial Cluster Assignment $\tda z$.
\end{algorithmic}
\end{algorithm}

% \paragraph{The Learning Problem.} Consider a set of samples $\{\bfa X_i\}_{i\in[u]}$ i.i.d. sampled from some distribution $p_{\bfa X}$, we construct their oracle top-$k$ principle components as $\bfa V_i=\begin{bmatrix}
%     \bfa v^{i,\top}_1&\ldots&\bfa v_k^{i,\top}
% \end{bmatrix}^\top$
% and the context-augmented input matrix as $\bfa H_i$ for each $\bfa X_i$. Then, the pretraining procedure is given by minimizing the following objective for some convex loss function $L(\cdot,\cdot):\bb R^{dk}\times \bb R^{dk}\to\bb R$,
% \begin{align}\label{ERM}
%  \wha\theta =\argmin_{\bfa\theta\in\Theta(B_{\bfa\theta}, B_M)}\sum_{i=1}^uL(TF_{\bfa\theta}(\bfa H_i), \bfa V_i).
% \end{align}
% Here we consider $\Theta(B_{\bfa\theta}):=\{\bfa\theta:\Vert\bfa\theta\Vert\leq B_{\bfa\theta}, \max_{\ell}M^{(\ell)}\leq B_M\}$ to be the space of parameters. We also consider guarantees in the $L_2$ norm which states that $L(\bfa x_1,\bfa x_2):=\Vert\bfa x_1-\bfa x_2\Vert_2$ in the theoretical part.
% Since $\wha\theta$ given by minimizing the empirical risk is not obtainable in practice, our theory only gives guarantee on the empirical risk minimizer. We further show that the local minimizers obtained through stochastic gradient optimization achieve good empirical performance in section \ref{sect4}.

