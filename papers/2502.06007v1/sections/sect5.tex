\section{Simulations}\label{sect4}

\begin{figure}[!h]
% \vspace{-1em}
\minipage{0.48\textwidth}
    \minipage{0.48\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_dist_metric_softmax.pdf}
    \endminipage\hfill
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_dist_loss_softmax.pdf}
    \endminipage\hfill
\endminipage\hfill
\minipage{0.48\textwidth}
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_dim_metric_new_softmax.pdf}
    \endminipage\hfill
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_dim_loss_new_softmax.pdf}
    \endminipage\hfill
\endminipage\hfill
\minipage{0.48\textwidth}
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_n_step_metric_softmax.pdf}
    \endminipage\hfill
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_n_steps_loss_softmax.pdf}
    \endminipage\hfill
\endminipage
\vspace{-1em}
\caption{\textbf{$4$-Class Clustering with Different Minimum Distance, Data Dimension, and Number of Training Data.} 
% \red{Can move detials into appendix.}
We train a small Transformer (layer $=3$, head $=2$, embedding $= 64$) and iterate for $300$ steps for each different setting. Each point in the figure is evaluated on $512$ testing data. 
We report the $10$ runs averaged result with a shaded region representing the standard deviation.
Each training sample is generated according to isotropic Gaussian with covariances $\sigma^2 \bfa I$.
\emph{(1) First Row: Minimum Distance.} We set $\sigma^2 \sim \mathrm{Uniform}[10,40]$.
\emph{(2) Second Row: Data Dimension.} We set $\sigma^2 \sim \mathrm{Uniform}[10,20]$, minimum distance $=5$.
\emph{(3) Three Row: Number of Training Data.} 
We set $\sigma^2 \sim \mathrm{Uniform}[0.5,5]$, minimum distance $=5$.}
\label{fig:dist_dim_step}
% \vspace{-1em}
\end{figure}

\begin{figure}[!h]
% \vspace{-1em}
\minipage{0.48\textwidth}
    \minipage{0.48\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_num_classes_metric_new_softmax.pdf}
    \endminipage\hfill
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_num_classes_loss_softmax.pdf}
    \endminipage\hfill
\endminipage\hfill
\minipage{0.48\textwidth}
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_ratio_metric_softmax.pdf}
    \endminipage\hfill
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_ratio_loss_softmax.pdf}
    \endminipage\hfill
\endminipage
\vspace{-1em}
\caption{\textbf{$4$-Class Clustering with Different Number of Class and Inbalance Ratio.}
% \red{Can move detials into appendix.}
We train a small Transformer (layer $=3$, head $=2$, embedding $= 64$) and train for $300$ steps for each different setting. Each point in the figure is evaluated on $512$ testing data.
We report the $10$ runs averaged result with a shaded region representing the standard deviation.
Each training sample is generated according to isotropic Gaussian with covariances $\sigma^2 \bfa I$.
\emph{(1) First Row: Number of Class.} We set $\sigma^2 \sim \mathrm{Uniform}[10,20]$, minimum distance $=5$.
\emph{(2) Second Row: Inbalance Ratio.} Two clusters each contain 50 data points, while the other two contain $50 \times \mathrm{ratio}$ and $50 \times \mathrm{1-ratio}$ respectively. We set $\sigma^2 \sim \mathrm{Uniform}[10,20]$, minimum distance $=5$.}
\label{fig:inbalance}
% \vspace{-1em}
\end{figure}

% We set $\sigma^2 \sim \mathrm{Uniform}[10,20]$, minimum distance $=5$.}

\begin{figure}[!h]
% \vspace{-1em}
\minipage{0.48\textwidth}
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_num_classes_metric_lloyd_more_layer.pdf}
    \endminipage\hfill
    \minipage{0.48\textwidth}
        \includegraphics[width=\textwidth]{charts/multiclass/multiclass_synthetic_num_classes_metric_lloyd_iter.pdf}
    \endminipage\hfill
\endminipage
\vspace{-1em}
\caption{\textbf{Comparision between Transformer and Lloyd's Algorithm.}
% \red{Can move detials into appendix.}
We compare the effect of the number of layers in Transformers with the number of iterations $\tau$ in Lloyd's algorithm under the same dataset configuration.  
We use a $6$-class dataset, where each cluster contains $50$ data points in a $d=10$ dimensional space.  
Each training sample is generated according to isotropic Gaussian with covariances $\sigma^2 \bfa I$, where $\sigma^2 \sim \mathrm{Uniform}[20,30]$, and the minimum cluster separation is set to $1$.  
\emph{(1) Left: Transformer.} 
We train Transformers with fixed head $=2$, embedding $= 64$, but vary the number of layers from $3$ to $20$.
Each model is trained for $500$ steps per layer.  
\emph{(2) Right: Lloyd's Algorithm.}
% We use ``sklearn`` to run the Lloyd algorithm, and vary the maximum iteration from $1$ to $6$.
We use \texttt{sklearn}\citep{pedregosa2011scikit} to run the Lloyd's algorithm, varying the maximum iteration count from $1$ to $6$.  
Early convergence is declared when the Frobenius norm of the difference between cluster centers in consecutive iterations falls below $10^{-4}$. 
Each point in the figure represents an evaluation of $512$ test samples.  
Results are averaged over $10$ runs, with the shaded region indicating the standard deviation.}
\label{fig:com_lloyd}
% \vspace{-1em}
\end{figure}

% \red{TODOs: Comparision of Lloyd baseline and Transformer.}
In this section, we verify our theoretical results on the multi-class clustering problem and examine its interplay with five key factors: the minimum distance between centroids $\Delta$, the data dimension $d$, the training sample size $N$, the total number of classes, and an imbalance ratio $\alpha$. These results are presented in  \cref{fig:dist_dim_step} and \cref{fig:inbalance}.
Furthermore, we compare the impact of a number of layers in the Transformer with the number of iterations $\tau$ in \cref{fig:com_lloyd}.

\paragraph{Experimental Setup}
We use a small Transformer with $3$ layers, $2$ heads, and $64$-dimensional embedding size.
All simulations are conducted on NVIDIA A100 80G GPUs. We run each experiment for 300 iterations, initialize the model with $10$ different random seeds, and report the mean and standard deviation of the resulting metrics. The model is trained using the Adam optimizer with a learning rate of $0.0005$ and an exponential decay factor of $0.995$ for each step.
After training, each configuration is evaluated on 512 synthetic and random test samples. Note that our empirical evaluation slightly differs from the theoretical part through removing the auxiliary matrix $\bfa P$ given by \eqref{aux} from the input.

\paragraph{Metrics.}
We compute cross entropy among every permutation of the label and choose the minimum as the loss function since clustering tasks are permutation invariant.
We evaluate the clustering performance using two widely adopted permutation-invariant metrics: Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI) \citep{ma2019learning, huang2020partially, monnier2020deep, sun2024lsenet, li2024image}.


\paragraph{Preparation for the Synthetic Data.}
{We generate our synthetic data as follows: For each input $\bfa X \in \R^{d \times N}$, we sample $50$ data points from every cluster. 
Each sample is generated according to isotropic Gaussian with covariances $\sigma^2 \bfa I$.
The variance $\sigma$ differs from task to task; we specify more details in the caption of figures.}

\paragraph{Results}
Our results suggest that the theoretical threshold given by the minimax rate matches with the trend given in the experiments. Moreover, we also showcase that the pre-trained Transformers can be a strong alternative to Lloyd's algorithm, verifying the strong inference capacities of Transformers on this problem.



% In this section, we verify the theoretical result in section~\ref{sect3} on synthetic and real-world datasets. Our experiments include both prediction of eigenvalues and eigenvectors. 
% % Note that our goal is to quickly understand the influence of different parameters via experiments. 
% % We conduct our experiments on both synthetic and realworld datasets.
% For synthetic datasets, we generate samples according to normal distributions. We focus on evaluating the effects of three major parameters:
%     \textbf{(1)} The Impact of $D$;
%     \textbf{(2)} The Impact of Number of Layers;
%     \textbf{(3)} The Impact of $k_{\text{train}}$\footnote{We denote $k_{\text{train}}$ as the value of $k$ used in training}. For real-world datasets, we perform experiments on MNIST \citep{lecun1998gradient} and Fashion-MNIST \citep{xiao2017fashion}. \emph{All the results presented in this section are errors on the testing set}.


% % the experiments in this section don't aim to demonstrate the optimal performance of our proposed model can achieve, but rather to run various experiments to quickly understand the influence of different parameters.
% % To speed up the training process, we set the number of samples $N \leq 50$ for each input $X$ across all settings.
% % In the first two sections, we use a synthetic dataset to (1) demonstrate transformer's ability to perform PCA to get top-$k$ eigenvalues and eigenvectors, and (2) investigate the influence of input dimension $D$, number of layers and attention heads of transformer, and the number of $k$ principal component to obtain.
% % In the third section, we evaluate transformer's ability on perfoming PCA on 2 real-world datasets: MNIST \citep{lecun1998gradient} and Fashion-MNIST \citep{xiao2017fashion}.  
% % Six figures in the main text?
% % \begin{itemize}
% %     \item A graph with loss v.s. step, predict top-1 eigenvector, varying d.
% %     \item varying k.
% %     \item varying layer
% % \end{itemize}

% \paragraph{Data Preparation.}
% For synthetic data $\bfa X \in \R^{D \times N}$, we generate each column with a randomly initialized multivariate Gaussian distribution $\ca N(\mu, \Sigma)$.
% We then generate the labels as the top-$k$ eigenvalues $\lambda$ and eigenvectors $V$ of the empirical covariance matrix $\bfa X^\top \bfa X/(N-1)$ via \texttt{numpy.linalg.eigh}.
% For real world dataset, we apply SVD to reduce the dimensionality of both datasets and evaluate whether the transformer, previously trained on multivariate Gaussian data, are capable of performing PCA on those real-world datasets.
% If the transformer successfully learns to perform PCA, we expect comparable performance on both synthetic and real-world data. 
% % Note that we set $N < D$ in all our experiment setting to speed up the training process.
% For more details on data generation and configuration, please refer to table~\ref{table:data-param} in appendix~\ref{appendix:data}.
 
% \begin{table}[t]
% \centering
% \caption{\textbf{Cosine Similarity for Different $k$.}
% We dentoe $k_{\text{train}}$ as the number of eigenvectors to predict during training.
% For example, for $k_{\text{train}}=4$, the model is trained to predict 4 eigenvectors.
% }
% \begin{tabular}{lcccc}
% \toprule
% \textbf{k-th eigenvec.} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} \\ \hline
% $k_{\text{train}}=4$    & $0.891 (0.006)$ & $0.616 (0.038)$ & $0.282 (0.047)$ & $0.120 (0.022)$ \\
% $k_{\text{train}}=3$    & $0.908 (0.011)$ & $0.706 (0.023)$ & $0.366 (0.018)$ & - \\
% $k_{\text{train}}=2$   & $0.903 (0.006)$ & $0.647 (0.019)$ & - & - \\
% $k_{\text{train}}=1$   & $0.894 (0.009)$ & - & - & - \\
% \bottomrule
% \end{tabular}
% \label{tab:mul_k_vector}
% \end{table}

% \begin{figure}[t]
%     \centering
%     \minipage{0.33\textwidth}
%         \includegraphics[width=\linewidth]{charts/eigenvalue_multiple_k.pdf}
%         % \subcaption{Top-10 Eigenvalues}
%     \endminipage\hfill
%     \minipage{0.33\textwidth}
%         \includegraphics[width=\linewidth]{charts/eigenvalue_vary_d_rmse.pdf}
%         % \subcaption{Cosine Similarity with Varying d}
%     \endminipage\hfill
%     \minipage{0.33\textwidth}
%         \includegraphics[width=\linewidth]{charts/eigenvalue_vary_layer_rmse.pdf}
%         % \subcaption{RMSE with Varying Layers}
%     \endminipage
%     \vspace{-1em}
%     \caption{\textbf{Comparisons of Eigenvalue Prediction on Synthetic Data.} 
%     \emph{(1) Left: Evidence of Transformer's Ability to Predict Multiple Eigenvalues.} 
%     We use a small transformer (layer $=3$, head $=2$, embedding $= 64$) to predict top $10$ eigenvalues with $D=20$ and $N=50$.
%     All of the (Relative MSE) of $10$ eigenvalues are below $2\%$, verifying that the transformer can predict eigenvalue very well.
%     Additionally, the error of prediction grows slightly with $k$.
%     We note that: (i) 
%     % While in theoretical analysis we show transformer can perform power iteration, 
%     Higher-order eigenvalues require additional iterations and models with more layers. 
%      (ii) Smaller eigenvalues are more sensitive to the fluctuations in the predicted values under the relative MSE metric.  
%     \emph{(2) Middle: Predictions of eigenvalues with different input dimension $D$.} We use a small transformer and use $N=10$ in this experiment. We show that prediction errors increase significantly as dimension scales up, corroborating our theoretical remark \ref{remark3}.
%     \emph{(3) Right: Predictions of eigenvalues with different number of layers.} We use the same input as the previous multiple eigenvalues predictions experiment, and use a small transformer to predict top-$3$ eigenvalues. As the number of layers grow, the model performs better on eigenvalue prediction, which aligns with the result in theorem \ref{thm3.1}.
%     }
%     \label{fig:eigenvalues}
% \end{figure}



% % We generate each training sample $\bfa X \in \R^{D \times N}$ from a different multivariate Gaussian distribution $\mathcal{N}(\mu, \Sigma)$, and use the classical method via the NumPy function\footnote{We use \texttt{numpy.linalg.eig} to compute eigenvalues and eigenvectors.} to compute the top-$k$ eigenvalues $\lambda \in \mathbb{R}^k$ and eigenvectors $V \in \mathbb{R}^{D \times k}$ of the empirical covariance matrix $\bfa X^\top \bfa X/(N-1)$ as our ground truth.


% % Specifically, for each $\bfa X_i \in \R^D$, we sample $Z_i \sim N(0, I) \in \R^D$.
% % We then form $Z = [Z_1, \cdots, Z_N]$ and transform it using an invertible matrix $L \sim N(0, I) \in \R ^{D \times D}$, yielding the desired training sample $\bfa X$.



% % \begin{table}[h]
% %         \centering
% %         \caption{Hyperparameters for Multiple Eigenvector Prediction.
% %         }
% %         % \resizebox{ \textwidth}{!}{  
% %         \begin{tabular}{l*{2}{c}}
% %         \toprule
% %             \bf{parameter} & $N=10$  \\ 
% %             \midrule
% %             top-$k$ & $1$ & $5$ & $?$  \\
% %              steps (eigenvector) & 60k & 20k & 60k \\
% %             learning rate  & $1\text{e-}3$ & $5\text{e-}3$ & $5\text{e-}3$ \\
% %             Optimizer  & Adam & Adam & Adam \\
% %             batch size  & $64$ & $64$ & $64$  \\
% %             number of layers & $3$ & $3$ & $3$  \\
% %             hidden dimension & $256$ & $256$& $256$ \\
% %             hidden dimension & $256$ & $256$& $256$ \\
% %             number of heads  & $8$ & $8$& $8$ \\
% %             number of examples (Train) $N$ & $100$ & $100$ & $100$  \\
% %             \bottomrule
% %         \end{tabular}
% %         \label{table:hyperparam}
% %     \end{table} 


% % Use the theorem "Generate $Z \sim N(\mu, I)$, then apply linear transformation we get $X = LZ + \mu$, where $\Sigma = LL^{T}$ and $L$ is a invertible matrix, we get $X \sim N(\mu, Σ)$". 
% % Hence starting from building an random invertible matrix $L$ then we are done.
% % That is, to generate the $k$th training data $X_k$, the outline of procedure is:
% % Generate $L_k$ use `np.random.randn(D,D)`-> get $\Sigma_k = L_k L_k^T$ ->
% % use `np.linalg.eigh(Σ_k)` get the eigenvalue ground truth.
% % use $\Sigma_k$ to generate training data, $X_{k,i} ∈ ℝ^D \sim N(\mu_k, Σ_k)$, where $X_k = [X_{k,1}, X_{k,2},...,X_{k,N}]$.
% \paragraph{Model.}
% We use the GPT2-architecture transformer \citep{radford2019language} as our backbone model.
% We follow most settings in \citet{garg2022can}, but replace the Softmax attention with ReLU attention as constructed in definition~\ref{def:attention}.
% We also provide a empirical comparison between Softmax and ReLU attention in Figure \ref{fig:relu_softmax} in the Appendix.
% We use a slighly differernt architectures to predict eigenvalues and eigenvectors.
% For eigenvalues prediction, we flatten the transformer output $TF_{\bfa\theta}(\bfa H) \in \R^{N \times D}$ and use a linear layer $W_\lambda \in \R^{(N\cdot D) \times k}$ to readout the top $k$ eigenvalues.
% As for eigenvectors, we use one more linear layer $W_v \in \R^{(N\cdot D) \times (k\cdot D)}$ to readout $k$ eigenvectors concatenated in a $1$-dimension vector.
% We use a transformer with layer $=3$, head $=2$, and embedding size $=64$ to speed up the training process for most settings and find that it is sufficient to predict multiple eigenvalues and top-$1$ eigenvector well, see below sections for detailed discussion.
% % We start from a very small transformer with 64 embedding dimensions, 2 heads, and 3 layers only.
% % As the dimension of feature input data increases, we further test on a deeper model with 6, 9, and 12 layers respectively.

% \paragraph{Metrics.}
% For eigenvalues, we use relative mean squared error (RMSE) as loss function $\mathcal{L}_{\text{RMSE}}$ and evaluation metric.
% For the loss of predicting top-$K$ eigenvalue, the loss function is defined as following
% \begin{align*}
%     \mathcal{L}_{\text{RMSE}}( \lambda_i, \hat{\lambda}_i )
%     \coloneqq
%     \frac{1}{K} \sum_{i=1}^{K} \frac{\lambda_{i} - \hat{\lambda}_{i}}{\lambda_{i} + \epsilon}.
% \end{align*}
% For eigenvectors, we use cosine similarity as loss function and evaluation metric.
% For predicting $k$ eigenvectors, the loss function is defined as
% \begin{align*}
%     \mathcal{L}_{\text{cos}}(v_i, \hat{v}_i)
%     \coloneqq
%     \frac{1}{K} \sum_{i=1}^{K} 1 - \frac{v_{i} \cdot \hat{v}_i}{\max (\Vert v_{i}\Vert_2 \cdot \Vert \hat{v}_{i}\Vert_2, \epsilon)},
% \end{align*}
% where $v_{i}$ represent the $i$-th eigenvector.
% The design of these loss functions not only matches the intuition of eigenvectors and eigenvalues, but also stablize training by normalizing the loss values.
% % We use two different metrics for predicting eigenvalues and eigenvectors.
% % For eigenvalues, we use mean relative squared error as the training loss and evaluation metric.
% % To predict top-$K$ eigenvalue, defined as follows:
% % \begin{align*}
% %     \frac{1}{K} \sum_{i=1}^{K} \frac{\lambda_{i} - \hat{\lambda}_{i}}{\lambda_{i} + \epsilon}.
% % \end{align*}
% % For eigenvectors, we use cosine similarity as the training loss and evaluation metric as follows:
% % \begin{align*}
% %     \frac{1}{K} \sum_{i=1}^{K} 1 - \frac{v_{i} \cdot \hat{v}_i}{\max (\Vert v_{i}\Vert_2 \cdot \Vert \hat{v}_{i}\Vert_2, \epsilon)},
% % \end{align*}
% % where $v_{i}$ represent the $i$-th eigenvector.


% \begin{figure}[tbp]
%     \centering
%     \minipage{0.33\textwidth}
%         \includegraphics[width=\linewidth]{charts/eigenvector_vary_d_cos_sim.pdf}
%         % \subcaption{Top-10 Eigenvalues}
%     \endminipage\hfill
%     \minipage{0.33\textwidth}
%         \includegraphics[width=\linewidth]{charts/eigenvector_vary_layer_cos_sim.pdf}
%         % \subcaption{Cosine Similarity with Varying d}
%     \endminipage
%     \minipage{0.33\textwidth}
%         \includegraphics[width=\linewidth]{charts/eigenvector_vary_k_cos_sim.pdf}
%         % \subcaption{RMSE with Varying Layers}
%     \endminipage\hfill
%     \vspace{-1em}
%     \caption{\textbf{Comparison of Eigenvector Prediction on Synthetic Data.}
%     \emph{(1) Left: Prediction of top-$1$ eigenvector with different input dimension $D$.} We use a small transformer and $N=10$. As the dimension $D$ scales up the eigenvector prediction suffers significantly. 
%     \emph{ (2) Middle: Prediction of top=$1$ eigenvector with varying number of layers.} 
%     % We start with a small transformer and predict the top $1$ eigenvector with an increasing number of layers.
%     % We also run on different dimensions $d$ to see the effect number of layers on different dimensions.
%     We start from small transformer and use $N=5$. The result demonstrates an `elbow effect', where we show that the increase of $L$ significantly boost the performance when $L$ is small but halt to progress for larger $L$. We believe this can be explained by the bias-variance tradeoff.
%     % shows that model performance increases with the number of layers, but the rate of improvement slows as the number of layers grows.
%     % Hence we further increase the number of heads from $2$ to $8$, and further increase the performance, suggesting increasing model depth alone is not sufficient to keep enhancing eigenvector prediction.
%     % The result also verifies theorem \ref{thm3.1} that we need at least 2 layers of the transformer to perform one iteration of the power method, as there is a sharper decrease between layer$=1$ and layer$=2$.
%     \emph{(3) 
%     Right: Predictions of eigenvectors with different numbers of $k$} We use $N=10$ and $D=10$ in this experiment.
%     We use a larger transformer with layer$=12$, heads$=8$, and an embedding size$=256$ in this experiment.
%     The result demonstrates a decreasing prediction accuracy and increasing standard deviation as $k$ increases.
%     We list the individual cosine similarities of the predicted k-th eigenvectors in table \ref{tab:mul_k_vector}.
%     All the evaluations in the above three figures are averaged on $10$ runs with different random seed.
%     }
%     \label{fig:eigenvector}
% \end{figure}



% \subsection{Synthetic Data vs Real World Data} 


% \begin{figure}[t]
%     \centering
%     \minipage{0.24\textwidth}
%         \includegraphics[width=\linewidth]{charts/mnist_eigenvalue_multiple_k.pdf}
%         % \subcaption{Top-10 Eigenvalues}
%     \endminipage\hfill
%     \minipage{0.24\textwidth}
%         \includegraphics[width=\linewidth]{charts/mnist_3_eigenvector.pdf}
%         % \subcaption{Cosine Similarity with Varying d}
%     \endminipage\hfill
%     \minipage{0.24\textwidth}
%         \includegraphics[width=\linewidth]{charts/fashion_mnist_eigenvalue_multiple_k.pdf}
%         % \subcaption{RMSE with Varying Layers}
%     \endminipage\hfill
%     \minipage{0.24\textwidth}
%         \includegraphics[width=\linewidth]{charts/fashion_mnist_3_eigenvector.pdf}
%         % \subcaption{Fourth Chart}
%     \endminipage
%     \vspace{-1em}
%     \caption{\textbf{Comparison of Eigenvalues and Eigenvectors Prediction on Real World Data.} 
%     \emph{(1) Left: Predicting Top-$10$ Eigenvalues on MNIST}  
%     \emph{(2) Second Left: Predicting Top-$3$ Eigenvectors on MNIST}
%     \emph{(3) Second Right: Predicting Top-$10$ Eigenvalues on FMNIST} 
%     \emph{(4) Right: Predicting Top-$3$ Eigenvectors on 
%     FMNIST}.
%     % We the transformer's performance on predicting eigenvalues and eigenvectors on real world datasets.
%     % Note that transformers are trained on synthetic samples, whose columns are generated by Gaussians.
%     % This leads to a large gap between training and test data distributions.
%     % However, as we see in the figure, transformers are still capable of prediction eigenvalues well, especially on MNIST.
%     % Further, for eigenvectors, transformers also show similar performance as they do on synthetic datasets, indicating their capability to predict principle eigenvectors accuractly.
%     We show that on real world datasets, Transformers perform similarly to the synthetic datasets. The experimental setup for real world data is analogous to the ones performed for synthetic data.
%     }
%     \label{fig:eigenvalues-realworld}
% \end{figure}


% \paragraph{Synthetic Dataset.}
% The results on synthetic data are in figure~\ref{fig:eigenvalues}.
% We first observe that transformers are capable of predicting top-$10$ eigenvalues with small error ($< 2\%$ error).
% The result also corresponds to theorem \ref{thm3.1}, indicating transformers are able to perform the power iteration method and generate eigenvalues with small error.
% For the impact of $D$, we observe the second subfigure in figure~\ref{fig:eigenvalues}.
% In general, we discover an increasing trend of RMSE when $D$ increases, this coincides with the theoretical findings stated in remark \ref{remark3}.
% We also observe that the error of prediction slightly increases with $k$, which is natural as the prediction dimension grows larger.
% For the impact of layers (right subfigure of figure~\ref{fig:eigenvalues}), we observe that as the number of layer increases, RMSE shows significant reduction.
% This matches our theoretical construction as we show the iteration of power methods correspond to the number of layers, see figure~\ref{fig:diagram} for the visualization of our transformer model.
% % transformer can approximate power iteration methods, and the additional iterations in our transformer can be seen as requiring a deeper model, see figure \ref{fig:diagram} for the visualization of our transformer model.
% One thing to highlight is higher-order eigenvalues (larger $k$) have smaller magnitudes, which are are more sensitive to fluctuations in the predicted values when using the relative MSE metric.
% This explaines the higher variance/error of higher order eigenvalues.
% For eigenvectors, we also observe that transformers are capable of predicting principle eigenvectors.
% In particular, the cosine similarity between predicted eigenvector and ground truth is close to 1 when $D$ is small.

% % We demonstrate that transformer can predict eigenvalue by predicting top-$1$ eigenvalues for $D = 10, 20, 30, 40$.
% % and (ii) Predict top-$10$ eigenvalues for $D=20$.
% % Note that in this experiment we want the scale of eigenvalue across different $D$ to remain the same.
% % Since the total variance of the covariance matrix scales with dimension $D$ and the sum of all eigenvalues equal to the total variance, the eigenvalues also scale with $D$.
% % Therefore we divide the generated $\bfa X^T \bfa X$ by $D$ in this experiment.
% % We set $N=10$ and use a small transformer for this experiment.


% \paragraph{Real World Dataset.}
% The results on real world dataset are in figure~\ref{fig:eigenvalues-realworld}.
% We observe that transformers are also capable of predicting top-$k$ eigenvalues well on both MNIST and FMNIST.
% Despite the difference in data distribution on training and test data, transformers are able to produce small error on predicting eigenvalues.
% Overall, we show that pretrained transformers learn PCA, and is able to generalize to other datasets as well.
% For eigenvectors, we can see that trained transformers show similar behavior on real world datasets when comparing to the synthetic ones.
% Indicating that our model actually learn to perform PCA instead of learn certain inductive bias.

% % To evaluate the impact of $D$, we evaluated transformers' performance with different values of $D \in \{10, 20, 30, 40\}$.


% % \paragraph{Multiple top-$k$ eigenvalues prediction.}
% % \red{Do we need to say that we want the variance of top-$10$ to be small hence using larger $N = 50$ ?}
% % We begin by demonstrating that a small transformer is able to predict top $10$ eigenvalues.

% % In this experiment we set $D = 20$ and $N = 50$ for our input data
% % The result is shown in the left figure in \ref{fig:eigenvalues}, all the relative mean squared errors of top $10$ eigenvalues are below $2\%$.
% % This confirms the transformer's ability to predict eigenvalue very well. 
% % We observe that the error of prediction grows slightly with $k$.
% % We attribute this to two main factors. 
% % First, power iteration methods are optimized for computing the largest eigenvalues, higher-order eigenvalues require additional iteration.
% % Our theoretical construction shows transformer can approximate power iteration methods, and the additional iterations in our transformer can be seen as requiring a deeper model, see figure \ref{fig:diagram} for the visualization of our transformer model.
% % Second, higher-order eigenvalues (larger $k$) have smaller magnitudes. Smaller values are more sensitive to fluctuations in the predicted values when using the relative MSE metric.
% % We include more evidence in section \ref{sec:add_exp}.
% % % Predict top-$10$ eigenvalues for $D=20$.
% % % We want the variance of top-$10$ to be small.
% % % We achieve this by using larger $N = 50$.

% % \paragraph{Vary $D$ to predict eigenvalue.} 
% % Results are Figure~\ref{fig:d_eigenvalues}.
% % We observe that transformers predict top-$1$ eigenvalue well for $D \in \{ 10, 20, 30, 40 \}$.



% % For (ii), we want the variance of top-$10$ to be small.
% % We achieve this by using larger $N = 40$
% % We derive the ground truth eigenvalues from $X^\top X$ where $X$ is generated as $X = LZ$.
% % As $D$ increase, the norm row/column vector in matrix grow with $\sqrt{D}$.
% % When we do matrix multiplication like $X = LZ$, the variance of the  increase when the 
% % And when we 
% % we generate training data by $X = LZ$ and get the ground truth of eigenvalue 


% % \paragraph{Vary layers to predict eigenvalue.}
% % The result is in figure \ref{fig:layer_eigenvalue}.
% % In this experiment, we investigate the relation between the number of layers and the predicted eigenvalue error.
% % In the previous paragraph we see that the transformer predicts the first eigenvalue very well, so we increase the data 
% % We use a larger input data the same as in the multiple top $k$ eigenvalue prediction experiment ($D = 20$ and $N = 50$) and train a small transformer to predict top $3$ eigenvalues.



% \subsection{Prediction with Different Parameter Combination.}\label{sec:eigvec_pred}
% \paragraph{Prediction with different $D$.} 
% The results are given in figure~\ref{fig:eigenvector}.
% In this experiment, we test the influence of increasing feature dimension $D$ affects the ability of a Transformer model to predict the principal eigenvector of a data matrix.
% We use the simplest setting with a small transformer and test on $D = 5,10,20,30,40$, $N=10$ and predict top-$1$ eigenvector.
% As the feature dimension 
% $D$ increases, we observe a clear trend of performance degradation of the Transformer's ability to predict the principal eigenvector accurately.
% This confirms our theoretical results stated in remark~\ref{remark3} that the feature dimension $D$ affects the approximation properties of Transformers significantly.
% % The challenges observed with increasing 
% % $d$ suggests that Transformers require additional capacity or architectural modifications to handle high-dimensional data effectively.






% \paragraph{Prediction with different $L$.}
% The results are given in figure~\ref{fig:eigenvector}.
% In this experiment, we change the number of layers of transformer with head $= 2$ and embedding $=64$, and set $N = 5$ to speed up the experiment.
% We observe that as the number of layers increases, the testing error also decreases, but the decreasing scale is less obvious when the number of layers becomes larger.
% However, the rate of improvement diminishes as the number of layers becomes larger. 
% This suggests that increasing model depth alone is not sufficient for significantly enhancing eigenvector prediction.
% To verify our guess, we increase the number of heads from $2$ to $8$ and find that the cosine similarity increases further and with a slightly steeper incline.
% Note that there is a sharper decrease between layer$=1$ and layer$=2$ across different $d$.
% This finding supports theorem \ref{thm3.1} that we need at least 2 layers of the transformer to perform one iteration of the power method.

% \paragraph{Prediction with different $k$.}
% The results are given in table \ref{tab:mul_k_vector}, and the right subfigure in figure \ref{fig:eigenvector} where the cosine similarity in y-axis is averaged over $k$ eigenvectors. 
% % In this experiments, we verify transformer's ability to predict top $k$ principal components.
% We use $N=10$ and $D=10$ in this experiment.
% We use a larger transformer with layer$=12$, heads$=8$, and an embedding size$=256$ in this experiment.
% As shown in figure~\ref{fig:eigenvector}, the model's ability to predict top $k$ eigenvectors decreases as more eigenvectors are predicted, with increasing standard deviation. 
% Table \ref{tab:mul_k_vector} lists the individual cosine similarities of the predicted $k$-th eigenvectors.
% The results show that most errors come from high-order eigenvectors.
% When trained to predict $k_{\text{train}} = 4$ eigenvectors, the model performs as well at predicting the top $1$ eigenvector as when trained on $k_{\text{train}} = 1$.
% The result shows that most prediction errors come from high-order eigenvectors. 
% This suggests that the pivotal difficulty is in the prediction of higher order eigenvectors.
% % This suggests that the pivotal of the task lies not in predicting more vectors, but in the transformer's performance on high-order eigenvectors. 
% % \red{This aligns with our theoretical result that to find high-order eigenvectors, the model must revisit earlier layers (from block 3 to block 2 in figure \ref{fig:diagram}) during forward propagation, which is equivalent to increasing the depth of the forward pass in the actual experiment.
% % Hence under the fixed number layers, we observe a worse performance for predicting high-order eigenvectors.}





% % \subsubsection{Real-world Datasets}
% % Finally, we test our framwork on two real-world dataset instead of the sythetic data generate from multivariate gaussian distribution.
% % We train the model to predict top-$5$ eigenvalues and top-$1 \sim 5$ eigenvectors, with former using small model and latter using large model.
% % For both dataset we use $64000$ data as training data and $6000$ testing data.
% % We set number of samples $N=10$, this lead to $6400 $ training data and $600$ testing data.


% % \begin{figure}[t]
% %     \centering
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvalue_multiple_k.pdf}
% %         % \subcaption{Top-10 Eigenvalues}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvalue_vary_d_rmse.pdf}
% %         % \subcaption{Cosine Similarity with Varying d}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvalue_vary_layer_rmse.pdf}
% %         % \subcaption{RMSE with Varying Layers}
% %     \endminipage
% %     \vspace{-1em}
% %     \caption{\textbf{Comparison of eigenvalue metrics.} \textcolor{red}{Make the font bigger for clarity.}
% %     \emph{1. Left:} Evidence of Transformer Ability to Predict Multiple Eigenvalues.
% %     We use a small transformer (layer $=3$, head $=2$, embedding $= 64$) to predict the top $10$ eigenvalues of input data with $D=20$ and $N=50$.
% %     All the Relative Mean Squared Error (Relative MSE) of $10$ eigenvalues are below $2\%$, verifying that the transformer indeed can predict the eigenvalue very well.
% %     Additionally, the error of prediction grows slightly with $k$.
% %     This may be due to: (i) 
% %     % While in theoretical analysis we show transformer can perform power iteration, 
% %     Higher-order eigenvalues require additional iterations in the power iteration method. 
% %     This corresponds to needing a deeper model, which contrasts with our small model. (ii) Larger $k$ eigenvalues have smaller magnitudes, making them more sensitive to fluctuations in the predicted values under the relative MSE metric.  
% %     \emph{Middle:} xxx.
% %     \emph{Right:} xxx.
% %     }
% %     \label{fig:eigenvalues}
% % \end{figure}

% % \begin{table}[t]
% % \centering
% % \caption{\textbf{Cosine Similarity.}
% % We dentoe $k_{\text{train}}$ as the number of eigenvectors to predict during training.
% % For example, for $k_{\text{train}}=4$, the model is trained to predict 4 eigenvectors.
% % }
% % \begin{tabular}{lcccc}
% % \toprule
% % \textbf{k-th eigenvec.} & \textbf{k=1} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} \\ \hline
% % $k_{\text{train}}=4$    & $0.891 (0.006)$ & $0.616 (0.038)$ & $0.282 (0.047)$ & $0.120 (0.022)$ \\
% % $k_{\text{train}}=3$    & $0.908 (0.011)$ & $0.706 (0.023)$ & $0.366 (0.018)$ & - \\
% % $k_{\text{train}}=2$   & $0.903 (0.006)$ & $0.647 (0.019)$ & - & - \\
% % $k_{\text{train}}=1$   & $0.894 (0.009)$ & - & - & - \\
% % \bottomrule
% % \end{tabular}
% % \label{tab:mul_k_vector}
% % \end{table}

% % \begin{figure}[htbp]
% %     \centering
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvector_vary_d_cos_sim.pdf}
% %         % \subcaption{Top-10 Eigenvalues}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvector_vary_layer_cos_sim.pdf}
% %         % \subcaption{Cosine Similarity with Varying d}
% %     \endminipage
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvector_vary_k_cos_sim.pdf}
% %         % \subcaption{RMSE with Varying Layers}
% %     \endminipage\hfill
% %     \vspace{-1em}
% %     \caption{\textbf{Comparison of Eigenvector Metrics.}
% %     \emph{(1) Left: Varing $d$ to predict top $1$ eigenvector.} We run this experiment on a small transformer to verify our theoretical result in remark \ref{remark3}. We clearly find that the model predicts worse when feature dimension  $d$ increases.
% %     \emph{ (2) Middle: Varying number of layer to predict top $1$ eigenvector.} 
% %     % We start with a small transformer and predict the top $1$ eigenvector with an increasing number of layers.
% %     % We also run on different dimensions $d$ to see the effect number of layers on different dimensions.
% %     The result shows that model performance increases with the number of layers, but the rate of improvement slows as the number of layers grows.
% %     Hence we further increase the number of heads from $2$ to $8$, and further increase the performance, suggesting increasing model depth alone is not sufficient to keep enhancing eigenvector prediction.
% %     The result also verifies theorem \ref{thm3.1} that we need at least 2 layers of the transformer to perform one iteration of the power method, as there is a sharper decrease between layer$=1$ and layer$=2$.
% %     \emph{(3) 
% %     Right:} xxx.
% %     All the testing error in above three figure xxx
% %     }
% %     \label{fig:eigenvector}
% % \end{figure}



% % \begin{figure}[htb]
% %     \centering
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/mnist_eigenvalue_multiple_k.pdf}
% %         % \subcaption{Top-10 Eigenvalues}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/fashion_mnist_eigenvalue_multiple_k.pdf}
% %         % \subcaption{Cosine Similarity with Varying d}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/cifar_eigenvalue_multiple_k.pdf}
% %         % \subcaption{RMSE with Varying Layers}
% %     \endminipage
% %     \vspace{-1em}
% %     \caption{\textbf{Eigenvalue Prediction on 3 Real-World Datasets.}
% %     \emph{Left:} xxx.  
% %     \emph{Middle:} xxx.
% %     \emph{Right:} xxx.
% %     }
% %     \label{fig:real_world_eigenvalue}
% % \end{figure}


% % \begin{figure}[htb]
% %     \centering
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigenvector_vary_d_cos_sim.pdf}
% %         % \subcaption{Top-10 Eigenvalues}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/N_LAYER_EIGENVECTOR.png}
% %         % \subcaption{Cosine Similarity with Varying d}
% %     \endminipage\hfill
% %     \minipage{0.33\textwidth}
% %         \includegraphics[width=\linewidth]{charts/eigen.pdf}
% %         % \subcaption{RMSE with Varying Layers}
% %     \endminipage
% %     \vspace{-1em}
% %     \caption{\textbf{Comparison of eigenvalue metrics.} \textcolor{red}{Make the font bigger for clarity.}
% %     \emph{Left:} Evidence of Transformer Ability to Predict Multiple Eigenvalues.
% %     We use a small transformer (layer $=3$, head $=2$, embedding $= 64$) to predict the top $10$ eigenvalues of input data with $D=20$ and $N=50$.
% %     All the Relative Mean Squared Error (Relative MSE) of $10$ eigenvalues are below $2\%$, verifying that the transformer indeed can predict the eigenvalue very well.
% %     Additionally, the error of prediction grows slightly with $k$.
% %     This may be due to: (i) 
% %     % While in theoretical analysis we show transformer can perform power iteration, 
% %     Higher-order eigenvalues require additional iterations in the power iteration method. 
% %     This corresponds to needing a deeper model, which contrasts with our small model. (ii) Larger $k$ eigenvalues have smaller magnitudes, making them more sensitive to fluctuations in the predicted values under the relative MSE metric.  
% %     \emph{Middle:} xxx.
% %     \emph{Right:} xxx.
% %     }
% %     \label{fig:same_layer}
% % \end{figure}




% % \begin{figure}[htb]
% % % \vspace{-1em}
% % \minipage{0.5\textwidth}
% % \minipage{0.5\textwidth}
% % \includegraphics[width=\linewidth]{charts/top-10 eigenvalue.png}
% % \endminipage\hfill
% % \minipage{0.5\textwidth}
% % \includegraphics[width=\linewidth]{charts/eigenvalue_vary_d_cos_sim.pdf}
% % \endminipage
% % \endminipage\hfill
% % \minipage{0.5\textwidth}
% % \minipage{0.5\textwidth}
% % \includegraphics[width=\linewidth]{charts/eigenvalue_vary_layer_rmse.pdf}
% % \endminipage\hfill
% % % \minipage{0.5\textwidth}
% % % \includegraphics[width=\linewidth]{charts/eigenvalue_vary_layer_train_loss.pdf}
% % % \endminipage\hfill
% % \endminipage
% % \vspace{-1em}
% % \caption{\red{Bigger font.}
% % % Maximum infinity norm $\| \mathbf{x} \|_{\infty}$ for different tensor components within layer 10 of BERT. Our work is analysed using two softmax variations: $\mathtt{OutEffHop}$ (represented in red) and vanilla $\Softmax$ (in grey). We find $\mathtt{OutEffHop}$ suppresses the outliers growing in both FFN layers.
% % }
% % \label{fig:same_layer}
% % % \vspace{-1.8em}
% % \end{figure}




% % \begin{figure*}[htb]
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=0.9\linewidth]{charts/eigenvalue_vary_d_rmse.pdf}
% % \end{minipage}%
% % \hfill
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=0.9\linewidth]{charts/eigenvalue_vary_d_train_loss.pdf}
% % \end{minipage}%
% % \vspace{-1.5em}
% % \caption{
% % \textbf{Influence of Different Feature Fimension $D$ on Top-$1$ Eigenvalue Prediction.}
% % We use 
% % % \emph{Left}
% % }
% % \label{fig:d_eigenvalues}
% % \end{figure*}

% % \begin{figure*}[htb]
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvalue_vary_layer_rmse.pdf}
% % \end{minipage}%
% % \hfill
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvalue_vary_layer_train_loss.pdf}
% % \end{minipage}%
% % \vspace{-1.5em}
% % \caption{
% % % \textbf{Noise-Robustness.}
% % % Our extensive evaluation of noise robustness across various Hopfield Networks, including Vanilla Modern Hopfield, Sparse Hopfield, 10th Order Hopfield, and our $\mathtt{OutEffHop}$, is conducted on two image datasets: MNIST and CIFAR10. The results show that as the noise level rises, the impact of $\mathtt{OutEffHop}$ on the error rate is minimal.
% % }
% % \label{fig:layer_eigenvalue}
% % \end{figure*}


% % \begin{figure*}[htb]
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvector_vary_d_cos_sim.pdf}
% % \end{minipage}%
% % \hfill
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvector_vary_d_train_loss.pdf}
% % \end{minipage}%
% % \vspace{-1.5em}
% % \caption{
% % % \textbf{Noise-Robustness.}
% % % Our extensive evaluation of noise robustness across various Hopfield Networks, including Vanilla Modern Hopfield, Sparse Hopfield, 10th Order Hopfield, and our $\mathtt{OutEffHop}$, is conducted on two image datasets: MNIST and CIFAR10. The results show that as the noise level rises, the impact of $\mathtt{OutEffHop}$ on the error rate is minimal.
% % }
% % \label{fig:d_eigenvector}
% % \end{figure*}





% % \begin{figure*}[htb]
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvector_vary_layer_cos_sim.pdf}
% % \end{minipage}%
% % \hfill
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/training_loss_layers_8x8.pdf}
% % \end{minipage}%
% % \vspace{-1.5em}
% % \caption{
% % % \textbf{Noise-Robustness.}
% % % Our extensive evaluation of noise robustness across various Hopfield Networks, including Vanilla Modern Hopfield, Sparse Hopfield, 10th Order Hopfield, and our $\mathtt{OutEffHop}$, is conducted on two image datasets: MNIST and CIFAR10. The results show that as the noise level rises, the impact of $\mathtt{OutEffHop}$ on the error rate is minimal.
% % }
% % \label{fig:layers_eigenvector}
% % \end{figure*}

% % \begin{figure}[H]
% % \vspace{-1em}\includegraphics[width=\linewidth]{}
% %     \vspace{-2em}
% %     \caption{
% %     }
% %     \label{fig:energy-land}
% %     \vspace{-1em}
% % \end{figure}

% % \begin{figure*}[htb]
% % % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/top-10 eigenvalue.png}
% % % \end{minipage}%
% % % \hfill
% % % \begin{minipage}[t]{0.5\textwidth}
% % %   \includegraphics[width=\linewidth]{charts/top-10 eigenvalue.png}
% % % \end{minipage}%
% % % \vspace{-1.5em}
% % \caption{
% % \textbf{Evidence of Transformer Ability to Predict Multiple Eigenvalues.}
% % We use a small transformer (layer $=3$, head $=2$, embedding $= 64$) to predict the top $10$ eigenvalues of input data with $D=20$ and $N=50$.
% % The result shows that all the Relative Mean Squared Error (Relative MSE) of $10$ eigenvalues are below $2\%$, verifying that the transformer indeed can predict the eigenvalue very well.
% % Additionally, the error of prediction grows slightly with $k$.
% % This may be due to: (i) While in theoretical analysis we show transformer can perform power iteration, higher-order eigenvalues require additional iterations in the power iteration method. This corresponds to needing a deeper model, which contrasts with our small model. (ii) Larger $k$ eigenvalues have smaller magnitudes, making them more sensitive to fluctuations in the predicted values under the relative MSE metric.
% % }
% % \label{fig:d_20_top_10}
% % \end{figure*}

% % \begin{figure*}[htb]
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvector_vary_k_cos_sim.pdf}
% % \end{minipage}%
% % \hfill
% % \begin{minipage}[t]{0.5\textwidth}
% %   \includegraphics[width=\linewidth]{charts/eigenvector_vary_k_train_loss.pdf}
% % \end{minipage}%
% % \vspace{-1.5em}
% % \caption{
% % % \textbf{Noise-Robustness.}
% % % Our extensive evaluation of noise robustness across various Hopfield Networks, including Vanilla Modern Hopfield, Sparse Hopfield, 10th Order Hopfield, and our $\mathtt{OutEffHop}$, is conducted on two image datasets: MNIST and CIFAR10. The results show that as the noise level rises, the impact of $\mathtt{OutEffHop}$ on the error rate is minimal.
% % }
% % \label{fig:k_eigenvector}
% % \end{figure*}







% % \clearpage