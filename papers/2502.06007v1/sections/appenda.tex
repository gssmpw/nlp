\section{Theoretical Background}
This section provides approximation results of the Softmax function in the form of $f(\bfa x):=\sum_{i=1}^M a_i\sigmoid\lef(\bfa x^\top\bfa v_i\rig)+a_0$ where $\{a_i\}_{i\in[M]}\subset \bb R$ and $\{\bfa v_i\}\subset \bb R^{d+1}$. We consider the class of $(R, C_{\ell})$-smooth functions \citep{bach2017breaking,bai2024transformers} defined as follows.
\begin{definition}[\citet{bai2024transformers}]\label{rcsmoothfunc}
    A function $g:\bb R^d\to\bb R$ is $(R, C_{\ell})$ smooth if for $s= \lceil(k-1)/2\rceil+2$, $g$ is a $C^s$ function supported on $[-R, R]^k$ such that
    \begin{align*}
        \sup_{\bfa x\in [-R,R]^k}\Vert\nabla^ig(\bfa x)\Vert_{\infty}\leq L_i,
    \end{align*}
    for all $i\in\{0,1,\ldots,s\}$, with $\max_{0\leq i\leq s}L_iR^i\leq C_{\ell}$.
\end{definition}
We then consider the following class of functions
\begin{align*}
    &\ca F_d = \bigg\{f:f(\bfa x)=\int_{\ca W:\Vert\bfa v\Vert=1}\sigmoid\lef([\bfa x^\top,1]\bfa v\rig)d\mu(\bfa v)\bigg\},\text{ with } TV(\mu)=\int_{\ca W:\Vert\bfa v\Vert_2=1}d|\mu(\bfa v)|<C_{\ell}C(f)^d ,
\end{align*}
where $C(f)$ is a constant that depends only on $f$. 
It is further noted that by \citet{bach2017breaking}, we can write that the class of $(R, C_{\ell})$ smooth functions belongs to the above class. Then we prove the following approximation lemma for the sigmoid function, which provides explicit dependence on $B$ and $C$.
 \begin{lemma}\label{sigmoidapprox}
     Suppose $f$ is $(R,C_{\ell})$ smooth. Then there exists a set of points $(\bfa v_1, a_1),\ldots,(\bfa v_M, a_M)\in \ca B(\Vert\cdot\Vert_2, 1)$ that makes the following hold
     \begin{align*}
         \sup_{\bfa x\in \ca B\lef(\Vert\cdot\Vert_\infty,R\rig)}\Big|\frac{1}{C_{\ell}}f(\bfa x)-\frac{1}{M}\sum_{i=1}^Ma_i\sigmoid\lef([\bfa x^\top,1]^\top\bfa v_i\rig)\Big|&\leq  C(f)^d\inf_{\epsilon>0}\Big(2\epsilon+\sqrt{\frac{\log(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)/\delta)}{M}}\Big)\\
         &\lesssim C(f)^d\sqrt{\frac{d}{M}\log\Big(\frac{MR}{d}\Big)},
     \end{align*}
     where we also have $\sum_{i=1}^M|a_i|\leq C(f)^d$.
 \end{lemma}
 \begin{proof}
     The proof goes by the probabilistic method, where we can first use \citet{pisier1981remarques} to show that when we sample from the distribution given by $f(\bfa v)=\frac{|\mu(\bfa v)|}{\int_{\bb S^d}d\mu(\bfa v)}=\frac{|\mu(\bfa v)|}{TV(\mu)}$. Then, under this probability measure, we sample independently in a total of $M$ samples $(\bfa V_1, \ldots, \bfa V_M)$ to obtain that for any $\bfa x\in\ca V$, pointwise
     \begin{align*}
         \int_{\bb R^{d+1}}&\softmaxx\lef([\bfa x^\top, 1]\bfa v\rig)d\mu(\bfa v)- \frac{TV(\mu)}{M}\sum_{i=1}^M\softmaxx\lef([\bfa x^\top, 1]\bfa V_i\rig)\\
         &= TV(\mu)\bl\bb E\Big[\sign(\mu(\bfa v)) \softmaxx\lef([\bfa x^\top,1]\bfa V\rig)\Big]-\frac{1}{M}\sum_{i=1}^M\sign\lef(\mu(\bfa v)\rig)\softmaxx\lef([\bfa x^\top,1]\bfa V_i\rig)\br.
     \end{align*}
     And we have by Hoeffding's inequality,
     \begin{align*}
         \bb P\bl\Big|\bb E\lef[\sign(\mu(\bfa v))\softmaxx\lef([\bfa x^\top, 1]\bfa V\rig)\rig]-\frac{1}{M}\sum_{i=1}^M\sign(\sigma_f(\bfa V_i))\softmaxx\lef([\bfa x^\top, 1]\bfa V_i\rig)\Big|\geq \frac{t}{TV(\mu)}\br\leq\exp\lef(-\frac{CMt^2}{TV(\mu)^2}\rig).
     \end{align*}
     And, by the union bound, we can show that for points in the $\epsilon$-cover of $\ca B(\Vert\cdot\Vert_{\infty},R)$, the following holds
     \begin{align*}
         \bb P\bl\sup_{\bfa x\in \ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)}&\Big|\bb E\Big[\ub{\sign(\sigma_f (\bfa V))\softmaxx\lef([\bfa x^\top, 1]\bfa V\rig)}_{=:F(\bfa V,\bfa x)}\Big]-\frac{1}{M}\sum_{i=1}^M\sign(\sigma_f(\bfa V_i))\softmaxx\lef([\bfa x^\top,1]\bfa V_i\rig)\Big|\geq\frac{t}{TV(\mu)}\br\\
         &\leq |\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)|\exp\lef(-\frac{CMt^2}{TV(\mu)^2}\rig).
     \end{align*}
     Alternatively, we can show the following holds with probability at least $1-\delta$,
     \begin{align*}
         \sup_{\bfa x\in\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)}&\Big|\bb E\lef[\sign(\mu(\bfa v))\softmaxx\lef([\bfa x^\top,1]\bfa V\rig)\rig]-\frac{1}{M}\sum_{i=1}^M\sign(\sigma_f)\softmaxx\lef([\bfa x^\top,1]\bfa V_i\rig)\Big|\\
         &\leq \sqrt{\frac{\log\lef(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)/\delta\rig)}{M}}.
     \end{align*}

     Then we consider generalizing these results to uniform convergence. For $\bfa x$, we denote $\pi(\bfa x)$ as the closest point in the $\epsilon$-cover of $\ca B(\Vert\cdot\Vert_{\infty},R)$ denoted by $\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)$. For function $F$, we can show that for all $\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)$,
     \begin{align*}
         \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\lef|\bb E[F(\bfa V,\bfa x)] - \bb E[F(\bfa V,\pi(\bfa x))]\rig|\leq \lef|\bfa V^\top(\bfa x_1-\bfa x_2)\rig|\leq\Vert\bb E[\bfa V^\top]\Vert_2\Vert\bfa x_1-\bfa x_2\Vert_2\leq \epsilon.
     \end{align*}
     Then we consider the error given by
     \begin{align*}
         \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\bfa x)-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\pi(\bfa x))\Big|\leq\frac{1}{M}\sum_{i=1}^M\epsilon\Vert\bfa V_i\Vert_2\leq\epsilon.
     \end{align*}
    Then the following holds with probability at least $1-\delta$,
     \begin{align*}
         &\sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\bb E\lef[F(\bfa V, \bfa x)\rig]-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\bfa x)\Big|\\
         &\leq \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\bb E\lef[F(\bfa V, \bfa x)\rig]-\bb E\lef[F(\bfa V, \pi(\bfa x))\rig]\Big|+\sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\bfa x)-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i, \pi(\bfa x))\Big|\\
         &+\sup_{\pi(\bfa x)\in\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)}\Big|\bb E\lef[F(\bfa V,\pi(\bfa x))\rig]-\frac{1}{M}\sum_{i=1}^MF(\bfa V_i,\pi(\bfa x))\Big|\leq\epsilon+\epsilon + \sqrt{\frac{\log(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon)/\delta)}{M}}.
     \end{align*}
     Given these results, we show that there exists a set of parameters $\{(\bfa V_i, a_i=TV(\mu)\sign_f(\bfa V_i))\}_{i\in[m]}$ where the following holds
     \begin{align*}
         \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty},R)}\Big|\frac{1}{C_{\ell}}f(\bfa x)-\frac{1}{M}\sum_{i=1}^M a_i\softmaxx\lef([\bfa x^\top, 1]\bfa V_i\rig)\Big|&\lesssim \inf_{\epsilon}\bigg\{\epsilon+\sqrt{\frac{\log(\ca N(\ca B(\Vert\cdot\Vert_{\infty},R),\epsilon))}{M}}\bigg\}\\
         &\lesssim \inf_{\epsilon}\bigg\{\epsilon +\sqrt{\frac{d\log\frac{R}{\epsilon}}{M}}\bigg\}\lesssim\sqrt{\frac{d}{M}\log\lef(\frac{MR}{d}\rig)},
     \end{align*}
     where we already utilize the estimate given by \citet{wu2020information} on the covering number of $L_2$ balls.
 \end{proof}
 \begin{lemma}[Approximating $d$ Dimensional $(R, C_{\ell})$ smooth functions by Softmax Neural Networks]\label{softmaxapprox}
        Consider an element-wise $(R, C_{\ell})$ smooth mapping $\bfa f(\bfa x)=(f_1(\bfa x),\ldots f_k(\bfa x))^\top$ where $\bfa x\in[-R,R]^d$. There exists a set of points $\{(\bfa A_{i}, a_{i})\}_{i\in[M]}$ such that the following hold
        \begin{align*}
            \sup_{\bfa x\in\ca B\lef(\Vert\cdot\Vert_\infty, R\rig)}\frac{1}{C_{\ell}}\Big\Vert\bfa f(\bfa x)-\sum_{i=1}^{Md} a_i\softmaxx\lef(\bfa A_i\begin{bmatrix}
                \bfa x\\
1\end{bmatrix}\rig)\Big\Vert_{\infty}\leq C(f)^d\sqrt{\frac{d^2}{M}\log\lef(\frac{MR}{d^2}\rig)}.
        \end{align*}
 \end{lemma}
 \begin{proof}
     Our proof goes by connecting the $\softmax$ activation with the $\sigmoid$ functions. Note that by lemma \ref{sigmoidapprox} we can show that for all $\ell\in[k]$, there exists a set $\{(\bfa v_i^{(\ell)}, a_i^{(\ell)})\}_{\ell\in[M^\prime]}$
     \begin{align*}
         \sup_{\bfa x\in\ca B\lef(\Vert\cdot\Vert_{\infty}, R\rig)}\frac{1}{C_{\ell}}\Big|f_{\ell}(\bfa x)-\sum_{i=1}^{M^\prime}a_i^{(\ell)}\sigmoid\lef(\bfa v_i^{(\ell),\top}\begin{bmatrix}
             \bfa x\\
             1
         \end{bmatrix}\rig)\Big|\leq C(f)^d\sqrt{\frac{d}{M^\prime}\log\lef(\frac{M^\prime R}{d}\rig)}.
     \end{align*}
     Consider the following matrices construction of $\{\bfa B_{i}^{(\ell)}\}_{i\in[d], \ell\in[M]}\subset\bb R^{k\times d}$, given by 
            $\bfa B_{i}^{(\ell)} = \begin{bmatrix}
            \bfa 0_{(\ell-1)\times (d-1)}& 0\\
             \bfa v_{i,[d-1]}^{(\ell),\top}&\log d+\bfa v_{i,d}^{(\ell)}\\
             \bfa 0&\bfa 0
         \end{bmatrix}$. Then we can show that
         \begin{align*}
             \sup_{\bfa x\in\ca B(\Vert\cdot\Vert_{\infty})}\bigg\Vert\begin{bmatrix}
                 \bfa 0_{(\ell-1)\times 1}\\
                 f_{\ell}(\bfa x)\\
                 \bfa 0
             \end{bmatrix}-\sum_{i=1}^{M^\prime}a_i^{(\ell)}\softmaxx\lef(\bfa B_i^{(\ell)}\begin{bmatrix}
                 \bfa x\\
                 1
             \end{bmatrix}\rig)\bigg\Vert_{\infty}\leq C(f)^d\sqrt{\frac{d}{M^\prime}\log\Big(\frac{M^\prime R}{d}\Big)},
         \end{align*}
         which completes the proof through noticing that $M^\prime d=M$.
 \end{proof}
    \begin{lemma}[Norm Approximation by Sigmoid Functions]\label{Softmaxapprox1/x}
    Consider the vector $\bfa v\in\bb R^d$. Assume that there exists a constant $C$ with $\Vert\bfa v\Vert_2\leq C$. For $M<\lef(C\frac{\overline R}{\underline R}\rig)^d\frac{1}{\epsilon^2}\log(1+C/\epsilon)$ such that there exists $\{\bfa a_m\}_{m\in[M]}\subset\bb S^{d}$ and $\{c_m\}_{m\in[M]}\subset\bb R$ where for all $\bfa v$ with $\overline R\geq\Vert\bfa v\Vert_2\geq \underline R$, we have 
    \begin{align*}
        \bigg|\sum_{m=1}^Mc_m\sigmoid\lef(\bfa a_m^\top\begin{bmatrix}
            \bfa v\\
            1
        \end{bmatrix}\rig)-\frac{1}{\Vert\bfa v\Vert_2}\bigg|\leq \lef(\frac{C\overline R}{\underline R}\rig)^d\sqrt{\frac{d}{M}\log\lef(\frac{MR}{d^2}\rig)}.
    \end{align*}
    % Similarly, there exists a multihead ReLU attention layer with number of heads $M\leq\overline R^{\frac{d}{2}}\frac{C(d)}{\epsilon^2}\log\lef(1+C/\epsilon\rig)$, a set of vectors $\{\bfa b_m\}_{m\in[M]}\subset\bb S^{N-1}$ and $\{d_m\}_{m\in[M]}\subset\bb R$ such that 
    % \begin{align*}
    %     \Big|\sum_{m=1}^Md_m\sigma(\bfa b_m^\top\bfa v)-\Vert\bfa v\Vert_2^{1/2}+1\Big|\leq\epsilon.
    % \end{align*}
\end{lemma}
\begin{proof}
    Consider a set $\ca C^d(\overline R) :=\ca B^d_{\infty}(\overline R)\setminus\ca B_{2}^d(\underline R)$, we note that
    \begin{align*}
        \sup_{\bfa v\in\ca C^d(\overline R)}\pta_{v_{j_1},\ldots,v_{j_i}\in[d]}\bl\frac{1}{\Vert\bfa v\Vert_2}\br\leq\frac{C^d}{\Vert\bfa v\Vert_2^d}\leq\frac{C^d}{\underline R^d}.
    \end{align*}
    Therefore, consider the definition \ref{rcsmoothfunc}, we have $C_{\ell}=\lef(\frac{\overline R}{\underline R}\rig)^d C^d$. Then the result is concluded by lemma \ref{Softmaxapprox1/x}.
\end{proof}

%  After a careful evaluation of the proof of the above lemma, we come up with the following corollary, which provides explicit dependence on the maximum value. 
% \begin{lemma}\label{lm:a1}
%     Suppose $f\in\ca F_d^{2m}$ with $|f|<B_f$, then for all bounded sets $U\subset\bb R^d$, there exists $B(d,f,U)<\infty$
%  and a point $((\bfa v_1,a_1),\ldots(\bfa v_M,a_M))\in [\bb R^{d+1}\times[-B,B]]^{M}$ such that the following holds
%  \begin{align*}
%      \sup_{0\leq \alpha\leq m}\sup_{\bfa x\in U}\Big|D^{\alpha}f(\bfa x)-\frac{1}{M}\sum_{i=1}^Ma_iD^{\alpha}\sigmoid\lef([\bfa x^\top,1]\bfa v_i\rig)\Big|\leq B_f C(d,f,U)M^{-1/2}.
%  \end{align*}
%  \end{lemma}
% where $\mu$ is the Radon measure and $\ca V$ is a compact space and
% \begin{align*}
%     \gamma_2(f) = \inf_{p}\int_{\ca V}|p(\bfa v)|^2d\tau(\bfa v),\quad\text{ s.t. }f(\bfa x)=\int_{\ca V}p(\bfa v)\sigmoid(\bfa v^\top\bfa x)d\tau(\bfa v),
% \end{align*}
% with $\tau$ being a fixed probability measure. Then, we employ the result coming from \citet{bach2017breaking,bai2024transformers} and consider the following smooth function class.
% \begin{definition}[Sufficiently smooth $d$-variate function] We say a function $g:\bb R^d\to\bb R$ is $(R,C_{\ell})$-smooth if for $s=\lceil (d-1)/2\rceil+2$, $g$ is a $C^s$ function on $\ca B_{\infty}^d(R)$, and
% \begin{align*}
%     \sup_{z\in\ca B_{\infty}^d(R)}\Vert\nabla^i g(\bfa z)\Vert_{\infty}=\sup_{\bfa z\in\ca B_{\infty}^d(R)}\max_{j_1,\ldots, j_i\in[d]}\lef|\partial_{x_{j_1}\ldots x_{j_i}}g(\bfa x)\rig|\leq L_i
% \end{align*}
% for all $i\in\{0,1,\ldots, s\}$ with $\max_{0\leq i\leq s}L_iR^i\leq C_{\ell}$. 
% \end{definition}
% Then, using \citep{bach2017breaking}, Proposition 5 we can show that $(R, C_{\ell})$-smooth function is representable as follows.
% \begin{lemma}[Lemma A.5 in \citet{bai2024transformers}]
%     Suppose function $g:\bb R^d\to\bb R$ is $(R, C_{\ell})$ smooth. Then there exists a signed measure $\mu$ over $\ca V:=\{\bfa w\in\bb R^{d+1},\Vert\bfa w\Vert_2=1\}$ such that
%     \begin{align*}
%         g(\bfa x)=\int_{\ca V}\frac{}{}
%     \end{align*}
% \end{lemma}


\section{Omitted Proofs}
\begin{theorem}
    
\end{theorem}
\begin{proof}
We first consider the input matrix to be 
\begin{align*}
    \bfa H_1:=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_n\\
         \wha \mu_{1}^{(0)}&\wha \mu_{2}^{(0)}&\ldots&\bfa 0\\
        \bfa p_{1,1}&\bfa p_{1,2}&\ldots&\bfa p_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1\\
        &&\bfa 0&
    \end{bmatrix}\in\bb R^{D\times N}, 
\end{align*}
where $\wh z^{(0)}:[n]\to[k]$ is the assignment function, $\wha\mu_i\in\bb R^{d}$ is the initially estimated centroid for the $i$-th cluster. $\bfa p_{1,i}\in\bb R^k$ satisfies $\bfa p_{1,i,j}=\mbbm 1_{\wh z^{(0)}(i)=j}$ for all $j\in[k]$. And for $\bfa p_{2,i}$ we have $\bfa p_{2,i,j} = \mbbm 1_{j=i}$ for $i\leq d$ and $\bfa p_{2,i,j}=0$ for $i\leq N$ and $j\leq d$. We let $\bfa p_{3,1}=\bfa p_{3,2}=\ldots=\bfa p_{3,N} = \bfa 0\in\bb R^k$.
We note that algorithm \ref{alg:loyld} consists of two iterative steps: (1) The expectation step where we take the averages to get an initial estimate $\wha \mu_{\ell}^{(t)}$. (2) The maximization step where we assign each individual sample their labels. Our following discussions simulate the two steps separately as follows.
\begin{center}
    \textbf{1. The Expectation Step.}
\end{center}
For the expectation step, our network is designed by
\begin{align*}
    \bfa V^{(1)}_1&=\begin{bmatrix}
        \bfa 0_{d\times (D-d)}&\bfa 0\\
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa Q^{(1)}_1=\begin{bmatrix}
    \bfa 0_{k\times 2d}&I_k&\bfa 0\\       
        &\bfa 0&
    \end{bmatrix},\quad\bfa K^{(1)}_1=\begin{bmatrix}
        \bfa 0_{d\times(2d+k)}& I_{d}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \begin{bmatrix}
        \bfa 0_{d\times (D-d)}&\bfa 0\\
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa Q^{(1)}_1=\begin{bmatrix}
    \bfa 0_{k\times 2d}&I_k&\bfa 0\\       
        &\bfa 0&
    \end{bmatrix},\quad\bfa K^{(1)}_1=\begin{bmatrix}
        \bfa 0_{d\times(2d+k)}& I_{d}&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\\
    \bfa V_2^{(1)} &= \begin{bmatrix}
        \bfa 0_{d\times d}&\bfa 0&\bfa 0\\
        \bfa 0_{d\times d}&-I_d &\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa Q_2^{(1)}=\bfa K_2^{(1)}=\begin{bmatrix}
        \bfa 0_{d\times d}&\bfa 0&\bfa 0\\
        \bfa 0_{d\times d}&-I_d &\bfa 0\\
        &\bfa 0&
    \end{bmatrix}.
\end{align*}
Then we can show that
\begin{align*}
    \bfa V_1^{(1)}\bfa H=\begin{bmatrix}
        &\bfa 0_{d\times N}&\\
        \bfa x_1&\ldots&\bfa x_N\\
        &\bfa 0&
    \end{bmatrix}, \quad\bfa Q_1^{(1)}\bfa H_1=\begin{bmatrix}
        (2\log n)\bfa p_{1,1}^{(0)}&\ldots&(2\log n)\bfa p_{1,N}^{(0)}\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa K_1^{(1)}\bfa H_1=\begin{bmatrix}
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix}.
\end{align*}
Hence we can show that
\begin{align*}
   \softmax\Big( \lef(\bfa Q_1^{(1)}\bfa H_1\rig)^\top\lef(\bfa K_1^{(1)}\bfa H_1\rig)\Big)=\softmax\lef(\begin{bmatrix}
       C\log n\tda p_{1,1}^{(0),\top}&\bfa 0\\
       \vdots&\vdots\\
       C\log n\tda p_{1,N}^{(0),\top}&\bfa 0
   \end{bmatrix}\rig).
\end{align*}
We further note that
\begin{align*}
    \bigg|\frac{\exp(C\log N)}{\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=j}\exp(C\log N)+\mbbm 1_{\wh z_i^{(0)}\neq j}}-\frac{1}{\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=j}}\bigg|\leq N^{-C}.
\end{align*}
Similarly we have
\begin{align*}
    \bigg|\frac{1}{\sum_{i=1}^N\mbbm 1_{\wh z_i^{(0)}=j}\exp(C\log N)+\mbbm 1_{\wh z_i^{(0)}\neq j}}\bigg|\leq N^{-C}.
\end{align*}
And we can show that under this construction,
\begin{align*}
   \bfa V^{(5)}_1\bfa H_4 (\bfa Q^{(5)}_1\bfa H_4)^\top&=\begin{bmatrix}
       &&\bfa 0_{d}\\\wha\mu_1^{(1)}&\ldots&\wha\mu_k^{(1)}&\bfa 0\\
       &&\bfa 0&
   \end{bmatrix},\quad \bfa K^{(5)}\bfa H_4=\begin{bmatrix}
        I_d&\bfa 0\\
        \bfa 0&\bfa 0
    \end{bmatrix},\\
    \bfa V^{(5)}_2\bfa H_4(\bfa Q_2^{(5)}\bfa H_4)^\top(\bfa K_2^{(5)}\bfa H_4)&=\begin{bmatrix}
        &&\bfa 0_{d}&\\
        \wha\mu_1^{(0)}&\ldots&\wha\mu_k^{(0)}&\bfa 0\\
        &&\bfa 0&
    \end{bmatrix}
\end{align*}
And, we can show that
\begin{align*}
    \bfa H_{5,1} = \bfa H_4 +\bfa V^{(5)}\bfa H_4(\bfa Q^{(5)}\bfa H_4)^\top(\bfa K^{(5)}\bfa H_4)=\begin{bmatrix}
        \bfa x_1&\bfa x_2&\ldots&\bfa x_{k}&\ldots&\bfa x_N\\
        \wha \mu_{1}^{(1)}&\wha \mu_{2}^{(1)}&\ldots&\wha \mu_{k}^{(1)}&\ldots&\bfa 0\\
        \bfa p^\prime_{1,1}&\bfa p^\prime_{1,2}&\ldots&\bfa p_{1,k}^\prime&\ldots&\bfa p^\prime_{1,N}\\
        \bfa p_{2,1}&\bfa p_{2,2}&\ldots&\bfa p_{2,k}&\ldots&\bfa p_{2,N}\\
        1&1&\ldots&1&\ldots&1\\
        &&\bfa 0&&&
    \end{bmatrix}.
\end{align*}
And the Expectation Step is concluded as we update the centroids from $\{\bfa\mu_i^{(0)}\}_{i\in[k]}$ to $\{\bfa\mu_i^{(1)}\}_{i\in[k]}$. The next step is to update the assignment $\bfa p_{1,i}^\prime$ to $\bfa p_{1,i}^{(1)}$ for $i\in[N]$.

\begin{center}
    \textbf{2. The Maximization Step.}
\end{center}
We further perform the expectation step by designing the following layers.
\begin{align*}
    \bfa V_i^{(4)}=\begin{bmatrix}
        &\bfa 0_{(3d+2k+1)\times D}&\\
        \bfa 0_{k\times2d}& I_k&\bfa 0\\
        &\bfa 0&
    \end{bmatrix},\quad\bfa Q_i^{(4)}=\begin{bmatrix}
        \bfa 0&\bfa 0&\bfa 0\\
        \bfa 0_{k\times2d}& I_k&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix},\quad\bfa K_i^{(4)}=\begin{bmatrix}
        \bfa 0&\bfa 0&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0\\
        \bfa 0&\bfa 0&\bfa 0
    \end{bmatrix}
\end{align*}
Then we further note that
\begin{align*}
    \bfa H_5 =\bfa H_4+\sum_{i=1}^2\bfa V_i^{(4)}\bfa H_4\softmax\lef(\rig)
\end{align*}
\end{proof}
% \begin{lemma}[Identity Mappings of the Softmax Function]
%     There exists a set of weights $((\bfa v_1, a_1),\ldots,(\bfa v_M,a_M))\in[\bb R^{d+1}\times[-B,B]]^M$. The mapping $f_i(\bfa x):=x_i$ can be approximated by a sum of the Softmax functions with the number of heads $M<$ such that
%     \begin{align*}
%         \sup_{0\leq\alpha\leq m}\Big|f(\bfa x)-\frac{1}{M}\Big|\leq\epsilon
%     \end{align*}
% \end{lemma}
% \begin{proof}
%     By lemma \ref{lm:a1} we can show that for $M\gtrsim C^2(d)B_f^2/\epsilon^2$ there exists $((\bfa v_1, a_1),\ldots,(\bfa v_M,a_M))\in[\bb R^{d+1}\times[-B,B]]^M$ such that the function $f(\bfa x)=x_i$ can be approximated with
%     \begin{align*}
%         \sup_{\bfa x\in U}\Big|f(\bfa x)-\frac{1}{M}\sum_{i=1}^Ma_i\sigmoid\lef([\bfa x^\top, 1]\bfa v_i\rig)\Big|\leq\epsilon.
%     \end{align*}
% \end{proof}
\begin{lemma}[Approximating the Hardmax Function by the Softmax Function] Consider a vector $\bfa x\in\bb R^d$. Let $x^*=\max_{i}\{x_i\}_{i\in[d]}$. Define $x^*=\max_{i\in[d]}x_i$, $\ca N_2=\{i:x_i=x^*\}$, $\Delta = x^*-\max_{i\in\ca N_2}x_i$. Define the Hardmax function as $\text{Hardmax}(\bfa x)=\begin{bmatrix}
    \frac{\mbbm 1_{x_1=\max_{i\in[d]}\{x_i\}}}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}}\{x_i\}}&\ldots&\frac{\mbbm 1_{x_d=\max_{i\in[d]}\{x_i\}}}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}}\{x_i\}}
\end{bmatrix}$, we subsequently show that 
\begin{align*}
    \Big|\softmax(\beta\bfa v)-\text{Hardmax}(\bfa v)\Big|\leq\bigg((d-|\ca N_2|)+\frac{(d-|\ca N_2|)^2}{|\ca N_2|^3}\bigg)^{\frac{1}{2}}\exp(-\beta\Delta).
\end{align*}
\end{lemma}
\begin{proof}
 We can show that the difference is given by
    \begin{align*}
        \Big\Vert\softmax(&\beta \bfa v)-\text{Hardmax}(\bfa v)\Big\Vert_{2} =\bigg(\sum_{i=1}^d\bigg(\frac{\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\}}}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\}}}-\frac{\exp(\beta x_i)}{\sum_{i=1}^d\exp(\beta x_i)}\bigg)^2\bigg)^{\frac{1}{2}} \\
        &=\bigg(\sum_{i=1}^d\bigg(\frac{\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\} }}{\sum_{i=1}^d\mbbm 1_{x_i=\max_{i\in[d]}\{x_i\} }}-\frac{\exp(\beta(x_i-x^*))}{\sum_{i=1}^d\exp(\beta(x_i-x^*)) }\bigg)\bigg)^{\frac{1}{2}}\\
        &\leq\bigg(\sum_{i=1}^d\mbbm 1_{x_i\neq x^*}\exp\lef(2\beta(x_i-x^*)\rig)+\mbbm 1_{x_i=x^*}\bl\frac{\exp\lef(\beta(x_i-x^*)\rig)}{\sum_{i=1}^d\exp\lef(\beta(x_i-x^*)
        \rig)}-\frac{1}{|\{i:x_i=x^*\}|}\br^2\bigg)^{\frac{1}{2}}\\
        &\leq \Big(|\ca N_1|\exp(-2\beta\Delta)+ |\ca N_2|\Big(\frac{1}{|\ca N_2|+|\ca N_1|\exp(-\beta \Delta)}-\frac{1}{|\ca N_2|}\Big)^2\Big)^{\frac{1}{
        2}}\\
        &\leq \bigg((d-|\ca N_2|)\exp(-2\beta\Delta) +\frac{(d-|\ca N_2|)^2\exp(-2\beta\Delta)}{|\ca N_2|\lef(|\ca N_2|+(d-|\ca N_2|)\exp(-\beta\Delta)\rig)^2}\bigg)^{\frac{1}{2}}\\
        &=\bigg((d-|\ca N_2|)+\frac{\lef(d-|\ca N_2|\rig)^2}{|\ca N_2|\lef(|\ca N_2|+(d-|\ca N_2|)\exp(-\beta\Delta)\rig)^2}\bigg)^{\frac{1}{2}}\exp\lef(-\beta\Delta\rig)\\
        &\leq \bigg((d-|\ca N_2|)+\frac{(d-|\ca N_2|)^2}{|\ca N_2|^3}\bigg)^{\frac{1}{2}}\exp(-\beta\Delta).
    \end{align*}
\end{proof}
\subsection{Algorithms}

If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
environments to format pseudocode. These require
the corresponding stylefiles, algorithm.sty and
algorithmic.sty, which are supplied with this package.
\cref{alg:example} shows an example.

\begin{algorithm}[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Tables}

You may also want to include tables that summarize material. Like
figures, these should be centered, legible, and numbered consecutively.
However, place the title \emph{above} the table with at least
0.1~inches of space before the title and the same after it, as in
\cref{sample-table}. The table title should be set in 9~point
type and centered unless it runs two or more lines, in which case it
should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\begin{table}[t]
\caption{Classification accuracies for naive Bayes and flexible
Bayes on various data sets.}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Data set & Naive & Flexible & Better? \\
\midrule
Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Tables contain textual material, whereas figures contain graphical material.
Specify the contents of each row and column in the table's topmost
row. Again, you may float tables to a column's top or bottom, and set
wide tables across both columns. Place two-column tables at the
top or bottom of the page.

\section{Additional Experiments}

\begin{figure}[htbp]
    \centering
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_dist_metrics_attn.pdf}
        % \subcaption{Top-10 Eigenvalues}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_min_dist_loss_attn.pdf}
        % \subcaption{RMSE with Varying Layers}
    \endminipage\vspace{-0.1em}
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_N_metrics_attn.pdf}
        % \subcaption{Cosine Similarity with Varying d}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_N_loss_attn.pdf}
        % \subcaption{Fourth Chart}
    \endminipage\vspace{-0.1em}
    \vspace{-1em}
    \caption{{\textbf{xxx.}
    We use a $3$-layer, $2$ head, $64$ hidden dimension Transformer to xxx}}
    % \label{fig:softmax_loss}
\end{figure}


\begin{figure}[htbp]
    \centering
\minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_num_classes_metrics_attn.pdf}
        % \subcaption{Top-10 Eigenvalues}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_num_classes_loss_attn.pdf}
        % \subcaption{RMSE with Varying Layers}
    \endminipage\vspace{-0.1em}
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_num_classes_metrics_attn.pdf}
        % \subcaption{Cosine Similarity with Varying d}
    \endminipage
    \minipage{0.45\textwidth}
        \includegraphics[width=\linewidth]{charts/multiclass/multiclass_synthetic_ratio_loss_attn.pdf}
        % \subcaption{Fourth Chart}
    \endminipage
    \vspace{-1em}
    \caption{{\textbf{Comparison of Concatenated Attention and Averaged Attention on the Real World Dataset.}
    \emph{Left: Performance Comparision on ARI and NMI.}
    \emph{Right: Performance Comparision on Converged Loss.}
    }
    }
    % \label{fig:softmax_loss}
\end{figure}




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
