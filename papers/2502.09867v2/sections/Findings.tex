\section{FINDINGS}
The findings section overviews participants’ interactions with \toolname{}, highlighting how dimensional scaffolding influenced their design process and outcomes. Overall, users engaged deeply with the tool, leveraging its scaffolding features to explore design dimensions, craft detailed prompts, and iteratively refine their ideas. One notable example involved a participant curating an image with “sustainability” and “minimalist” tags, which led them to discover an overlooked product dimension—ergonomics (P13). Incorporating this dimension into their prompt generated a design that aligned with their original intent and improved functionality and client appeal. These results illustrate how \toolname{} fosters both creative exploration and the emergence of nuanced design strategies, laying the foundation for the following detailed discussion.

\subsection{Impact of Dimensional Scaffolding on Prompt Behavior}
\label{finding6.1:naunced_prompt}
\subsubsection{\toolname{} Participants Wrote Longer Prompts with Richer Design Vocabulary}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/findings_prompt_length.png}
    \caption{DesignWeaver participants issued longer prompts.}
    \Description{This plot illustrates the difference in prompt length between the two groups, highlighting the effectiveness of \toolname{} in encouraging more detailed prompts.}
    \label{fig:findings_prompt_length}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/findings_prompt_length_over_time.png}
    \caption{DesignWeaver participants consistently had higher median prompt lengths over time.}
    \Description{This graph tracks how prompt length evolved for both groups during the session.}
    \label{fig:findings_prompt_length_over_time}
\end{figure}

Prompts generated in the \toolname{} condition were more developed compared to those from the Baseline condition. These prompts reflected a deeper consideration of design dimensions such as aesthetics, functionality, and sustainability. The Baseline prompt described a dining chair with basic features like material, color, and comfort. In contrast, the \toolname{} condition prompted a more nuanced design, incorporating contemporary aesthetics, eco-friendly materials, ergonomic considerations, and playful geometry, showcasing a richer understanding of design dimensions (as shown in Appendix~\ref{appD:pc}).

Participants using \toolname{} produced significantly longer prompts (M = 48.22, SD = 15.03) compared to the Baseline group (M = 23.73, SD = 19.12), with a statistically significant difference ($U = 97.0, p < 0.001$), as illustrated in \autoref{fig:findings_prompt_length}. Analysis of prompt length over time showed that participants in the DesignWeaver group consistently expanded their prompts, with a steady increase in median prompt length throughout the session (see \autoref{fig:findings_prompt_length_over_time}). In contrast, participants in the Baseline group reached a point of stabilization earlier, suggesting a more constrained approach to prompt development and limited iterative exploration. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/findings_avg_unique_design_terms.png}
    \caption{DesignWeaver participants averaged more unique design terms per prompt than the Baseline.}
    \Description{A violin plot showing the distribution of the average number of unique design terms per prompt for Baseline and DesignWeaver groups. The DesignWeaver group has a higher median and a higher range overall.}
    \label{fig:findings_avg_unique_design_terms}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/findings_total_unique_design_terms.png}
    \caption{DesignWeaver participants had a higher total number of unique design terms per session than the Baseline.}
    \Description{A violin plot showing the distribution of the total unique tags per session for Baseline and DesignWeaver groups. Again, the DesignWeaver group exhibits a higher median.}
    \label{fig:findings_total_unique_design_terms}
\end{figure}

In addition to generating longer prompts, participants in the DesignWeaver group employed a broader range of unique design terms. \autoref{fig:findings_avg_unique_design_terms} shows, on average, participants in the \toolname{} group used 24.48 unique terms per prompt (SD = 7.47), compared to 10.59 unique terms in the Baseline group (SD = 6.72). Similarly, \autoref{fig:findings_total_unique_design_terms} shows that the total number of unique tags per session was also higher in the \toolname{} group (M = 44.44, SD = 14.07) than in the Baseline group (M = 22.72, SD = 7.50), with significant differences confirmed by the analysis ($U = 61.0, p < 0.001$ \& $U = 56.5, p < 0.001$).

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/findings_prompt_design_term_violin_plots_unique.png}
    \caption{DesignWeaver participants come up with more distinct design terms than the Baseline.}
    \label{fig:findings_prompt_design_term_violin_plots_unique}
    \Description{A violin plot comparing the distribution of distinct design terms for Baseline vs. DesignWeaver. The DesignWeaver group has a higher median and a broader range.}
\end{figure}

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/findings_prompt_design_term_violin_plots_common.png}
    \caption{DesignWeaver participants adapted more common design terms than the Baseline.}
    \label{fig:findings_prompt_design_term_violin_plots_common}
    \Description{A violin plot comparing the distribution of common design terms for Baseline vs. DesignWeaver. Again, the DesignWeaver group shows a higher median usage.}
\end{figure}

To assess the impact of dimensional scaffolding on design vocabulary development, we compared, between the two groups, how many of the design terms were directly adapted from the Design document and how many were developed by the participants themselves while using the tool. Participants using \toolname{} both thought of more distinct design terms ($M = 34.6, SD = 11.7$) and common design terms ($M = 9.4, SD = 2.5$) from the Design document than those in the Baseline group, who had distinct terms ($M = 16.2, SD = 5.3$) and common design terms ($M = 5.4, SD = 3.0$), as shown in \autoref{fig:findings_prompt_design_term_violin_plots_unique} and \autoref{fig:findings_prompt_design_term_violin_plots_common}. Statistically significant differences were observed in the adoption rates for distinct terms ($U = 635.0, p < 0.001$) and common design terms ($U = 566.0, p < 0.001$), demonstrating that dimensional scaffolding facilitated not only the use of terms from the Design document but also encouraged participants to learn and adopt new design terms.

\subsubsection{Participants Adapted Diverse Prompt Strategies Afforded by \toolname{}}
\label{finding6.1.2:prompt_strategy}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/findings/finding_survey_average_ratings_comparison.png}
    \caption{Participants rated DesignWeaver higher than the Baseline on ease of idea-to-prompt conversion, design space exploration, prompt generation, concept refinement, and iterative design improvement.}
    \label{fig:finding_survey_average_ratings_comparison}
    \Description{This figure compares the average ratings given by participants in the Baseline (n=25) and DesignWeaver (n=27) groups across multiple survey questions. The p-values above the bars show statistical significance for some of the questions. The x-axis lists the survey questions, and the y-axis represents the average ratings on a Likert scale. Statistically significant results are marked, with differences favoring the DesignWeaver condition in several categories.}
\end{figure*}

Participants using \toolname{} employed multiple strategies during prompt crafting, integrating dimension scaffolding to guide their design process. Initially, all participants (27/27) started with automated prompts generated from selecting tags based on the Design document. Throughout the process, they employed three main strategies: 18 out of 27 used default tags from the discussion panel, 5 out of 27 created custom dimensions and tags, and 12 out of 27 manually edited their prompts. Additionally, 6 out of 27 participants used the information button to map images back to dimension tags, gaining insights into how specific elements influenced the design. Notably, 10 out of 27 participants switched between these strategies. For example, P24 began with default tags such as ``Minimalist,'' ``Eco-friendly,'' `` Ergonomic,'' and ``Scratch-resistant.'' After reviewing the initial images, they added new dimensions and tags from the recommended list. As they continued, P24 customized tags using terms from the design document, like ``stain-resistant,'' and further refined their prompts by manually adding specific details such as ``Width is 18 inches and depth is 16 inches.'' Adopting this expanded range of approaches, participants using \toolname{} found it significantly easier ($U = 236.5, p = 0.030$) to convert ideas into prompts, as reflected in their higher ratings ($M = 5.63, SD = 1.11$) compared to the Baseline group ($M = 4.88, SD = 1.51$) shown in \autoref{fig:finding_survey_average_ratings_comparison}. 

In contrast, all participants (25/25) in the Baseline group relied heavily on vocabulary from design documents and adjusted their prompts iteratively based on visual feedback from generated images. For straightforward requirements like ``Solid oak'', participants easily incorporated terms directly from the design documents. For example, P2 stated, ``\textit{I mainly just copied and pasted from the document and... included those into the prompt.}" When dealing with more complex requirements, such as ensuring comfort in long conversations, 24 out of 25 participants started with general terms and refined them based on feedback. As P21 explained, they began with ``\textit{neutral colored with a cushion seat,}'' then added features such as wood material and color accents based on how images aligned with their vision, iteratively refining to achieve a closer match. Such reliance on image feedback was reported to lead to participants' frustration when images failed to improve; 9 of the 25 participants reported this experience. For instance, P46 noted, \begin{quote}``\textit{while I'm … adding more information or … keywords to it, but sometimes … some keywords later, it doesn't change that much. It just gives me some new pictures, but I can't really … tell where… there's a big change.}''\end{quote}


\subsection{Impact of Dimensional Scaffolding on Generated Artifacts}
\label{finding6.2:image_novelty}
Using the ViT-B/32 CLIP model \cite{radford2021learning}, we calculated both image and prompt similarities for the Baseline and \toolname{} conditions. Results show that:

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/finding_image_similarity_scores_distribution.png}
    \caption{DesignWeaver participants created semantically more diverse images than the Baseline.}
    \label{fig:finding_image_similarity_scores_distribution}
    \Description{Histogram comparing the mean image similarity scores for the Baseline (blue) and DesignWeaver (orange) groups. The p-values from the Mann-Whitney U Test indicate that DesignWeaver has significantly higher image similarity scores.}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/finding_prompt_similarity_scores_distribution.png}
    \caption{DesignWeaver participants created semantically more similar prompts than the Baseline.}
    \label{fig:finding_prompt_similarity_scores_distribution}
    \Description{Histogram comparing the mean prompt similarity scores for the Baseline (blue) and DesignWeaver (orange) groups. The p-values from the Mann-Whitney U Test indicate that DesignWeaver has significantly higher prompt similarity scores.}
\end{figure}

% \begin{figure*}[htbp]
%     \centering
%     \includegraphics[width=\textwidth]{figures/findings/finding_similarity_scores_distribution.png}
%     \caption{DesignWeaver increases image diversity by promoting broader semantic exploration (left) while fostering refined and consistent prompt construction (right), outperforming the baseline.}
%     \label{fig:finding_similarity_scores_distribution}
%     \Description{This figure presents two histograms with fitted density curves: the left plot compares the mean image similarity scores for the Baseline and DesignWeaver groups, while the right plot compares the mean prompt similarity scores. The Baseline group is represented in blue, and the DesignWeaver group is represented in orange. The p-values from the Mann-Whitney U Test confirm significant differences between the two groups, with DesignWeaver achieving generally higher prompt and image similarity scores.}
% \end{figure*}

Both image and prompt similarity scores were significantly different from each other group (p<0.001). In image similarity, Baseline has a mean similarity of 0.903 (SD=0.015), and \toolname{} has a similarity of 0.863 (SD=0.024), which yields a statistically significant difference ($U=521.0, p<0.001$) (see \autoref{fig:finding_image_similarity_scores_distribution}). In prompt similarity, Baseline has a mean similarity of 0.916 (SD=0.035), and \toolname{} has a similarity of 0.964 (SD=0.022), which also yields a statistically significant difference ($U=78.0, p<0.001$) (see \autoref{fig:finding_prompt_similarity_scores_distribution}). We included more detailed analysis in Appendix~\ref{appD:sd}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/finding_levenshtein_distances_distribution.png}
    \caption{DesignWeaver group has larger Levenshtein text edit distance in prompts across iterations than the Baseline.}
    \label{fig:finding_levenshtein_distances_distribution}
    \Description{This figure presents the distribution of average Levenshtein distances for Baseline in blue and DesignWeaver in orange. The x-axis represents the average Levenshtein distance (a measure of textual difference between prompts), and the y-axis represents the frequency of occurrences. The density curves highlight that DesignWeaver tends to have higher average Levenshtein distances than the Baseline, indicating more variation in the textual evolution of prompts in the DesignWeaver group.}
\end{figure}

To further explore why that semantically similar prompts in \toolname{} yield more semantically diverse image outcomes, we performed a simpler prompt difference comparison using the Levenshtein edit distance to address potential concerns. The results in \autoref{fig:finding_levenshtein_distances_distribution} indicate that users in the \toolname{} condition ($M=174.49, SD=74.83$) made significantly more prompt modifications compared to the Baseline Condition ($M=68.21, SD=44.94$). A Mann-Whitney U Test produced a U statistic of 76.0 and a p-value less than 0.001, confirming the robustness of our findings.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/findings/findings_expert_ratings.png}
    \caption{Average expert ratings on the novelty and alignment with the client's design brief of the participants' final design. DesignWeaver participants created designs with significantly higher ratings on novelty but not alignment compared to baseline participants.}
    \Description{Two side-by-side violin plots comparing expert ratings on Novelty and Value Alignment between Baseline and DesignWeaver groups. The left plot displays a significant difference, while the right plot does not.}
    \label{fig:findings_expert_ratings}
\end{figure*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/findings/finding_novelty_gallery.pdf}
    \caption{Top 3 expert rated chair on innovation}
    \label{fig:finding_novelty_gallery}
    \Description{The figure presents two sets of designs: the baseline group (top) and the DesignWeaver group (bottom). The baseline group focuses on minimalist designs with moderate creative elements, whereas the DesignWeaver group showcases more distinctive and novel designs that integrate sustainability, creative material choices, and innovative forms. Each design is labeled with its unique identifier.}
\end{figure}

Lastly, we compared the experts' rating data we gathered on novelty and value alignment for the final images picked by all the participants. We found that \toolname{} condition was rated significantly higher on image novelty ($M=4.09, SD=1.63$) compared to the Baseline group ($M=3.54, SD=1.60$), with a statistically significant difference ($U=9899.0, p=0.002$), as illustrated in \autoref{fig:findings_expert_ratings} (left). Analysis of the alignment with design requirements didn't show a significant difference between \toolname{} condition ($M=4.5, SD=1.656$) and Baseline condition ($M=4.2, SD=1.67$) with a statistical significance at 0.059 ($U = 10925.5$), shown in \autoref{fig:findings_expert_ratings} (right). \autoref{fig:finding_novelty_gallery} showcases the top 3 chairs rated by experts for their novelty.



\subsection{Users Perceived Support and Challenges}
\label{finding6.3:perception}

\subsubsection{ \toolname{} Participants Adapted New Design Terms}
One of the primary advantages of \toolname{} reported by participants was its ability to help participants adopt new design terminology. 
\toolname{} was perceived to significantly aid in adapting new design terms, as evidenced by significantly higher ($U = 225.0, p = 0.0139$) ratings from participants using \toolname{} ($M = 6.15, SD = 0.77$) compared to the Baseline group ($M = 5.48, SD = 1.16$) for exploring different design dimensions (see \autoref{fig:finding_survey_average_ratings_comparison}). This adaptability was noticeable from the initial stages of the design process, where \toolname{} generated images that closely aligned with participants’ early visions. Participants found \toolname{} highly effective in helping create initial prompts ($M = 6.19, SD = 0.74$), significantly outperforming ($U = 115.0, p < 0.001$) the Baseline group ($M = 3.92, SD = 1.91$). For instance, P7 remarked, “\textit{it immediately generated something very similar to the mood board,}” while P11 noted that it “\textit{helped generate a sort of Baseline on what chairs I could generate.}” Throughout the design process, the tags listed on the panel were instrumental in broadening participants' design exploration, with 8 participants appreciating how these tags introduced new dimensions and terminology they hadn't previously considered. P20 highlighted, “\textit{there were lots of different dimensions that I wouldn’t typically think about for chairs. So that was like new information.}” Similarly, P13 valued the exposure to terms like “ergonomic” and “sustainability,” which allowed them to refine and expand their design ideas, enhancing their creative versatility.

\subsubsection{\toolname{} Participants Gained Greater Control Over the Design Process}
Participants also felt that \toolname{} enhanced their control over the design process, enabling them to manage their designs more effectively through tag selection. 8 of the 27 participants mentioned that tag selection helped them efficiently navigate and refine their designs. P1 described their experience: ``\textit{I was able to change the chair and select different [options] that fit the design... and go back if I wanted to add or change... tags.}'' Among these 8 participants, six highlighted the information button as crucial for staying informed about their current design state and understanding how each modification impacted their design within specific dimensions. P30 explained, ``\textit{Whenever I got info from the chairs that I liked, it was helpful to see what wording or characteristics I should remember.}'' Participants using \toolname{} reported a greater sense of control, reflected in significantly higher ratings for refining their design concepts ($M = 6.19, SD = 0.74$) compared to the Baseline group ($M = 5.72, SD = 1.06$; $U = 251.5, p = 0.047$) as shown in \autoref{fig:finding_survey_average_ratings_comparison}. They also felt their designs improved more throughout the iterative process ($M = 6.19, SD = 0.79$) than those in the Baseline group ($M = 5.48, SD = 1.23$; $U = 226.5, p = 0.017$), further supporting the tool’s effectiveness.


\subsubsection{\toolname{} Participants Faced Challenges with Over-Tagging and Generating Specific Designs}
During interviews, participants also reported challenges while using \toolname{}. A notable issue was that choosing too many tags (over-tagging) led to confusion over the design process. Several participants (5 out of 27) reported that over-tagging led to confusion and a sense of losing baseline over the design process. P11 mentioned, ``\textit{if you choose too many options, it'll get a bit too constricted,}'' reflecting a perception that too many tags can make the system overly restrictive. P1 similarly felt that ``\textit{at the end, it was kind of already having too many tags, and it was kind of just showing me like a little off to what I needed.}'' Participants also expressed concerns about the tool's responsiveness to tag changes. For example, P17 stated, ``\textit{I feel like when I change the culture design tab, not much changed,}'' suggesting that tag adjustments did not always produce the expected variations.

Furthermore, participants reported difficulties in generating designs with specific or detailed characteristics. P47 shared their experience: ``\textit{There were many times when I was clicking around trying to get more variety in an image… but it didn't really do that. Towards the end, I started getting closer, but it wasn't what the client wanted. The client wanted a curved back [on the chairs], but instead of giving me curved backs as in the beginning, it started giving me straight backs.}'' Similarly, P18 found it challenging to generate specific colors, saying, ``\textit{I wanted the colors not to be just tan and white, but they were only giving me tan and white. I didn't know how specific I could be or if I could just click certain colors.}'' P5 also described the increased effort needed as designs became more complex, stating, ``\textit{as we got further on into the more nitty, gritty aspects, then it was more on me,}'' reflecting the perceived effort needed to refine the design as complexity increased.

With perceived challenges and support from \toolname{}, participants using the tool reported similar satisfaction levels with the design images compared to the Baseline group, both in terms of quality and alignment with their expectations. There was no significant statistical difference between the Baseline and \toolname{} conditions for satisfaction with the generated images ($M = 5.56, SD = 1.04; M = 5.59, SD = 0.63; p = 0.5790$) and for alignment of the images with expectations ($M = 5.44, SD = 0.87; M = 5.63, SD = 0.84; p = 0.1314$).
