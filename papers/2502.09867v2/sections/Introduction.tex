\section{INTRODUCTION}

Recent advancements in Generative AI (GenAI) models \cite{goodfellow2014generative, vaswani2017attention, radford2021learning, dhariwal2021diffusion} have transformed creative processes, opening new approaches to ideation and content generation \cite{davis2017quantifying, karimi2020creative}. These models show significant potential across multiple modalities, including text, images, 3D models, and video \cite{ramesh2021zero, poole2022dreamfusion, singer2022make}. In the Human-Computer Interaction (HCI) community, efforts to improve interactivity with GenAI have focused on refining prompt engineering interfaces, such as structuring prompts through familiar templates \cite{xu2024jamplate, wu2022ai, chung2023promptpaint}, studying how designers craft prompts \cite{chong2024cad, palani2024evolving}, visually rearrange AI-generated images \cite{zhang2023adding, brade2023promptify}, as well as iteratively refining Large Language Model (LLM) prompts using chaining techniques \cite{liu2022opal, di2022idea, wu2022promptchainer}. However, studies also show that supporting prompt engineering through bespoke UIs, such as through multi-modal interactions, can introduce unpredictability and hinder practical use for its increased complexity \cite{peng2024designprompt}.

Researchers have explored using text-to-image (T2I) models to generate renderings to support product design conceptualization \cite{jeon2021fashionq, liu20233dall}. T2I models can quickly produce high-fidelity visualizations that, prior to Gen AI, typically require detailed specifications about the materials and other dimensions (e.g., sizing, color, etc.). Expert designers develop domain-specific knowledge through experiences navigating diverse design scenarios, managing constraints, and mastering design principles, technical details, and available options within a given domain \cite{chong2024cad, tollestrup2023design, lawson2013design, mckenna2014adaptive}. This expertise allows designers to effectively balance high-level creative goals with technical constraints \cite{winston1970learning}. However, novice designers, even if experienced with AI prompting, often lack the domain-specific vocabulary and understanding needed to express their ideas clearly \cite{palani2021conotate, zamfirescu2023johnny}. Experts also struggle to convey tacit knowledge, and even if they express those personal and intuitive insights, GenAI may be fickle with underrepresented language in training data \cite{liu2023wants}. Experts may also benefit from structured, step-by-step prompt interactions to help wrangle unpredictable models. Current T2I models have the potential to aid both experts and novices in product design communication, but they need support to understand key terminology understood by the models. 

To understand how experts explore design spaces and communicate with their clients who generally lack domain knowledge and design expertise, we conducted a formative study and interviewed twelve experienced designers (2-20 years in professional practice) from diverse fields on how they communicate about key dimensions and options with clients. We found that designers often presented multiple alternatives to give their clients choices while maintaining creative control. Both experts and clients rely on visual representations to articulate and negotiate a vision. Based on these insights, we distill design goals for tools to help novices grapple with product design concepts, despite a lack of domain language, by emphasizing visual representations and surfacing key design dimensions and terminology.

Based on these insights, we developed \toolname{}, an interface powered by GPT-4 and DALL-E 3, designed to support novice designers through a technique we call \textit{dimensional scaffolding}. \toolname{} enables rapid iteration on text prompts by surfacing key design dimensions of user-curated images and allowing users to toggle on/off language from the "dimension palette." Key dimensions, such as geometry, style, color, and material, emerge organically through bidirectional interaction between text prompts and generated images. We hypothesize this technique helps users discover and adjust key dimensions, compensating for their lack of domain-specific knowledge, leading to more effective prompts while avoiding cognitive overload from advanced AI features.

To evaluate the effectiveness of \textit{\toolname{}}, we focused on three research questions:

\begin{enumerate} 
    \item How does dimensional scaffolding affect the \textbf{quality, length, and use of domain-specific language in text prompts} compared to a standard text-based prompting interface?
    \item How do differences in the text prompts enabled by dimensional scaffolding affect \textbf{the diversity and quality of generated product designs} compared to a standard text-based prompting interface?
    \item \textbf{How do participants engage with dimensional scaffolding} and what are their overall impressions using text-to-image models for product design?
\end{enumerate}

To investigate these research questions, we conducted a between-subjects study (n=52), where participants created a chair design based on a design brief using either \toolname{} or a baseline interface --- a standard text-based prompting interface (like ChatGPT). We collected user-generated design artifacts, system logs, survey ratings, and semi-structured interviews to gather quantitative and qualitative insights.

The study finds that \toolname{}'s \textit{dimensional scaffolding} enabled users to explore various design combinations and uncover dimensions they might not have considered with a text-based prompting interface. Participants using \toolname{} produced longer prompts with more nuance (unique domain vocabulary) from the outset. \toolname{} created a complementary interaction between text and images, leading to more nuanced prompts and better design outcomes. \toolname{} participants produced images with greater diversity as measured by CLIP-based similarity \cite{radford2021learning} and higher overall novelty as rated by blind-to-condition experts. On average, participants using \toolname{} added 3.1 new dimensions and 19.1 new tags to the palette. While this led to longer, more nuanced prompts, the \toolname{} participants issued fewer prompts overall and seemed to have greater expectations for the T2I outputs. This sometimes led to frustration (as measured by their satisfaction level with both the process and the result). We discuss how generative AI platforms might want to tamper with expectations as the underlying T2I models continue to evolve.

This paper contributes: 
\begin{itemize} 
    \item A novel system, \toolname{}, introducing dimensional scaffolding for iteratively authoring prompts and discovering key design dimensions through output inspection.
    \item An empirical study shows that \toolname{} enhances iterative prompt refinement, improves creative exploration through novel design dimensions, and fosters design innovation, with AI-based image diversity aligning closely with expert ratings on novelty.
    \item An insight that helping users author longer, more precise text prompts for GenAI may lead to frustration if not met with equally accurate image outputs.
\end{itemize}