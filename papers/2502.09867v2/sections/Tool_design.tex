\section{DIMENSIONAL SCAFFOLDING SYSTEM}
%incorporate citations without calling out
\subsection{Design Goals}
The Formative Study revealed a significant challenge in client-designer interactions: clients often struggle to articulate what they like or dislike about their preferred product concepts, relying on images from the web or social media rather than using technical language. This leaves designers to interpret these visuals, decipher client preferences, and explain design decisions, all while helping clients navigate trade-offs in the design process. To address this, we aim to equip designers—especially novices—with domain-specific language that enhances their ability to construct precise prompts for generating accurate reference images through large language models (LLMs). Beyond simply enabling image rendering based on less concrete or imprecise input, this approach supports a principled exploration of key design dimensions and options within the design space. By structuring these explorations and capturing critical trade-offs, designers are better positioned to systematically navigate the design space, fostering informed decision-making and thoughtful engagement with design challenges.

Based on these findings and insights from theory and practice, we aim to create a tool with the following design goals:
\begin{itemize}
    \item \textbf{\textit{Goal 1: Allow novices to absorb known preferences, requirements, and constraints.}} Establishing a context-aware foundation requires a thorough understanding of the client’s identity, vision, and specific needs, which shape and inform the design process \cite{palani2022interweave}. Gaining deep insight into the client, their value, and their project goals, requirements, and constraints allows the designer to make decisions about form, materials, or style that align with the client’s core intentions. By integrating these elements from the beginning, the designer ensures that every step of the process resonates with the client’s objectives. Collecting tangible examples and personal inspirations of what is already known is crucial to developing a clear sense of the client’s aesthetic preferences and functional requirements and effectively guiding the creative direction for the rest of the design process. Providing a framework for testing different design concepts based on these known constraints helps designers quickly explore trade-offs and refine their decisions, making it easier to compare and communicate these variations with clients.
    \item \textbf{\textit{Goal 2: Surface specific dimensions from the product design space.}} Current prompt-generation tools often lack the precision and control designers need to tailor outputs effectively, especially when clients struggle to articulate their preferences clearly. This gap highlights the need for designers to surface and refine design dimensions such as form, texture, color, and style—key elements that often emerge only through visual feedback. Sometimes, realizing too late that a different approach would have simplified the manufacturing process or prevented client dissatisfaction can be avoided by anticipating the effects of design decisions early on. By allowing designers to adjust dimensions based on visual cues or client input, the system enables flexible exploration of trade-offs, alignment with client expectations, and real-time testing of design scenarios, minimizing revisions and unforeseen issues. This flexibility is particularly valuable for novice designers, who may lack the vocabulary or experience to navigate all possible variations without such structured experimentation.
    \item \textbf{\textit{Goal 3: Enable comparison through multiple visual representations.}} Presenting multiple alternative options simultaneously allows designers to explore a broader range of possibilities and engage clients early in the design process \cite{tohidi2006getting, dow2010parallel}. By showcasing distinct visual variations that emphasize different elements, designers can demonstrate how each choice impacts the final product, facilitating comparing and contrasting options quickly and easily. Additionally, this method allows designers to justify their design choices through visual evidence, ensuring client alignment before moving into advanced stages like prototyping or full-scale modeling. Maintaining transparency throughout this process can clarify how decisions were made and ensure that both parties can easily compare and discuss the implications of each option.
    \item \textbf{\textit{Goal 4: Facilitate dimensional reasoning through trial and error.}} Novice designers often struggle to navigate complex design spaces due to a lack of domain knowledge and the language needed to communicate ideas and trade-offs. Despite access to large language models (LLMs), this gap can hinder their ability to create effective prompts or engage meaningfully with clients. An iterative feedback loop, focused on trial-and-error experimentation, provides a solution by allowing novices to refine their understanding of design-specific language and principles. Through repeated practice guided by visual references, they can explore alternatives in materials, form, and aesthetics while learning to articulate the trade-offs involved. This process builds the confidence necessary for clear client communication and fosters a deeper understanding of balancing creative vision with practical constraints. 
\end{itemize}

\subsection{DesignWeaver User Experience}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/tool_design/tool_design_system_diagram.png}
    \caption{Overview of the iterative design process using \toolname{}. The process involves four main stages: (1) Ingest the design document to extract initial dimensions and tags, (2) Refine and recommend dimensions to generate prompts, (3) Use prompts to render and refine images, and (4) Iterate based on new dimensions and tags inspired by the generated images.}
    \label{fig:design_workflow}
    \Description{The figure shows the iterative workflow in \toolname{}, starting from the ingestion of the design document (Step 1), where initial dimensions and tags are extracted. In Step 2, dimension tags are suggested and updated based on feedback from generated images. In Step 3, prompts are generated and updated using new tags and dimensions, leading to the rendering of images (Step 4). These images further inspire new dimensions and tags, completing the cycle.}
\end{figure*}

To address the aforementioned design goals, we developed \toolname{} with the following workflow shown in \autoref{fig:design_workflow}. \autoref{fig:designweaver-ui} overviews the key features.

In \toolname{}, users interact mainly through two primary panels: the Design Panel on the left and the Image Panel on the right. Before starting, an initial Design Document is uploaded. This could detail the client’s persona, vision, specifications, budget, and timeline. This document is a crucial guide throughout the design process, helping users make informed decisions while using the tool.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/tool_design/tool_design_all_features.png}
    \caption{User Interface of DesignWeaver. The UI facilitates structured dimensional tagging and interactive exploration of AI-generated designs. Key features include a design document for guidance, a prompt box for input, a dimension palette for organizing and modifying design aspects, and an image panel displaying generated outputs. Users can add or delete dimensions, tag designs, view detailed image information, and curate favorite designs for final selection. This workflow supports iterative refinement and creativity.}
    \label{fig:designweaver-ui}
    
    \Description{
    The figure shows the user interface of the DesignWeaver tool, which facilitates structured dimensional tagging for enhanced exploration of design spaces. 

    \textbf{1. Design Document:} Displays the design requirements guiding the creative process.

    \textbf{2. Design Panel:} Provides an overview where users can write prompts to generate designs.

    \textbf{3. Prompt Box:} The area where users input prompt text for generating product designs.

    \textbf{4. Dimension Palette:} Displays the list of dimensions (e.g., aesthetic, functionality, comfort) that users can modify and explore.

    \textbf{5. Add Dimension:} Allows users to add new design dimensions to guide the generative process.

    \textbf{6. Add Tag:} Enables users to add specific tags under each dimension.

    \textbf{7. Delete Dimension:} Provides the option to remove unnecessary dimensions.

    \textbf{8. Image Panel:} Displays generated images based on the input prompts and selected dimensions.

    \textbf{9. Info Button:} Displays detailed information about each design or image.

    \textbf{10. Like Button:} Allows users to mark designs they prefer or like.

    \textbf{11. Favorite Folder:} Keeps track of liked or favorited designs.

    \textbf{12. Favorite Selection:} Shows designs or images selected as favorites for final image selection.

    \textbf{13. Image Preview:} Provides a closer look at the selected design image from the gallery.
    }
\end{figure*}

\subsubsection {Design Panel}
The Design Panel is divided into two sections: the Prompt Box at the top and the Dimension Palette at the bottom.
\newline
\textbf{Prompt Box.} The Prompt Box is a standard text input area that allows users to type prompts for image generation. Users could describe their design vision in their own words or use keywords as inputs for the DALL-E 3 API to function. It includes a scroll bar for longer inputs and a Send Button on the right side to submit the prompt. Prompt Box introduces two additional features. The first is an auto-complete feature, automatically completing any partially written input when activated. The second is a merging feature that combines user-written content with newly generated dimension tags (which will be explained in the next section). 
\newline
\textbf{Dimension Palette.} The Dimension Palette is the most essential feature that distinguishes \toolname{} from other tools. It helps users discover and refine design dimensions by providing categorized style tags based on keywords extracted from the uploaded Design Document. In this context, the term dimension refers to a broad design concept or aspect, such as aesthetics, sustainability, or functionality, displayed as the title of each row in the Dimension Palette. For each dimension, such as “Aesthetics,” there are associated subcategories like “modern,” “classical,” or “minimalism,” which are located within the same row. These subcategories appear as clickable style tags. Users can add new tags to any row that fits the existing dimension, delete tags, introduce new dimensions (which will create new rows), remove entire rows, and reorder them as needed. 

Users can select from a list of LLM-generated recommendations when adding new tags or creating custom tags. The exact process applies when adding new dimensions. Single clicking on a style tag within a dimension changes its color from default white to bright blue and simultaneously updates the prompt in the Prompt Box. For instance, selecting the tag “Minimalist” will automatically generate the phrase “The design embraces a minimalist aesthetic” in the Prompt Box. Users can continue to add more tags to refine the prompt further. Once satisfied, they can click the Send Button to generate images. After a brief moment, three images will appear on the right-hand side in the Image Panel, displayed in a row.

\subsubsection{Image Panel}
Users start with an empty Image Panel. Pressing the Send Button in the Prompt Box generates three images, each with a heart icon and an info icon at the bottom right. Users can click an image to zoom, flip, or expand it for a detailed preview. After generating the first set, they can modify the prompt and generate new images, repeating the process for continuous exploration.
\newline
\textbf{Like Button \& Favorite Folder.} The heart icon lets users like an image, which is then stored in the Favorite Folder at the top right for quick access. Clicking the Favorite Folder shows only liked images, helping users filter and compare them. All images remain visible in the Image Panel by default, but users can toggle between the full gallery and favorites by clicking the Favorite Folder again.
\newline
\textbf{Info Button.} Clicking the Info Icon reveals style tags on the Dimension Palette, including those used for image generation and additional ones discovered by an image understanding model (GPT-4o-mini). Tags are highlighted in yellow with dashed outlines for temporary visibility—existing tags appear bolded in yellow, while new ones have dashed outlines. A single click adds a tag and a double click activates it, turning it blue and adding it to the Prompt Box as part of the evolving prompt. The prompt updates only when a tag is activated. Users can click the Info Button again to hide all tags.
\newline
Users exploring the design space with \toolname{} switch between the Design Panel and Image Panel, forming a natural design iteration loop. The Dimension Palette displays design dimensions refined through previous prompts, helping users improve future ones. The Image Panel shows generated results, enabling visual exploration and interaction. This iterative process deepens users' understanding of the design space, revealing new design dimensions and prompting language, allowing them to craft more effective prompts and generate images that better match their design vision.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/user_study/user_study_participant_experience.png}
    \caption{Number of user study participants with diverse experience in design, large language models, and text-to-image models.}
    \Description{
    The plot consists of three side-by-side bar charts showing participant experience with design, large-language models (LLM), and text-to-image (T2I) models. Each chart shares the same axes: the x-axis represents experience levels, and the y-axis shows the number of participants. 

    In the "Design Experience" chart:
    - 29 participants have no prior experience.
    - 20 participants are beginners with 0–1 year of experience.
    - 2 participants are intermediate with 2–5 years of experience.
    - 1 participant is advanced with over 5 years of experience.

    In the "LLM Experience" chart:
    - 6 participants have never used large-language models.
    - 24 participants have tried them a few times.
    - 15 participants use them a few times per month.
    - 6 participants use them weekly.
    - 1 participant uses them daily.

    In the "T2I Experience" chart:
    - 41 participants have never used text-to-image models.
    - 7 participants have tried them a few times.
    - 2 participants use them a few times per month.
    - 2 participants use them weekly.

    The bars are light blue, and numbers are displayed above or inside them for clarity. The charts include light gray horizontal grid lines for reference.
    }
    \label{fig:user_study_participant_experience}
\end{figure*}

\subsection{Implementation Details}
\toolname{} is a web application built with React and powered by a Python backend. All logged data, including prompts and tags used, are stored in Firebase, while generated images are downloaded via the Python backend and uploaded to Google FireStore. Image generation is handled using OpenAI's DALL-E 3 API. 

\subsubsection{Design Palette Initialization}
Three dimensions with corresponding design tags are extracted from the provided design document (see Appendix~\ref{appB:dd}) using GPT-4o to initialize the Dimension Palette. We instructed the model to pick the three most relevant dimensions to form the initial dimensions and design tags (see Appendix~\ref{appA1:prompts_doc_digest}). We hope the appropriate information from the design document could help users with the cold-start problem.

\subsubsection{Prompt Generation \& Update}
GPT-4o converts tags into prompt text for accurate and high-quality formatting. When tags are added or removed, GPT-4o generates and updates prompts as described in Appendix~\ref{appA2:prompts_prompt_gen_and_update}. The model compares old and new tags, ensuring tags with zero weight are removed, tags with weight one are included, and the original prompt structure is preserved as much as possible.

\subsubsection{Image Generation \& Tag Extraction}
Each time we use DALL-E 3 to generate three new images per iteration, we save their corresponding tags and prompts in Firebase (see Appendix~\ref{appA3:prompts_img_gen}). To enhance these tags, we leverage the fast processing speed of GPT-4o-mini to extract additional style tags and design dimensions from the generated images (see Appendix~\ref{appA4:prompts_tag_extraction}).

\subsubsection{New Tag \& Dimension Recommendation}
The model’s prompt incorporates existing style tags and dimensions as constraints to generate relevant recommendations. It builds on these tags to discover new dimensions by analyzing the current image, ensuring the outputs stay contextually relevant while avoiding redundant tags or dimensions (see Appendix~\ref{appA5:prompts_tag_recommendation} and \ref{appA6:prompts_dimension_extraction}).

