\section{Performance Against an i.i.d.\@ Adversary}\label{sec: iid}

In this section, we study an i.i.d.\@ adversary. In this model, we show that online envy minimization is easier than online multicolor discrepancy.
We first prove a super-constant lower bound for the online vector balancing problem (\Cref{thm: lower bound for iid vector balancing}), which, naturally, implies a super-constant lower bound for the online multicolor discrepancy problem. In~\Cref{subsec: iid envy n agents} we give a simple algorithm for online envy minimization and $n$ agents. All missing proofs can be found in~\Cref{app:missing proofs from iid}.

\subsection{Lower bounds for online vector balancing}

In the following lower bound, we show that if for all $t \in [T]$, each coordinate of all the vectors $v_t$ are i.i.d.\@ drawn from the distribution $\mathcal{U}([-1,1])$, then the discrepancy at time $T$ of any online algorithm must be $\Omega\left(\sqrt{\frac{\log{T}}{\log{\log{T}}}}\right)$. Note that a drawn vector might not be a member of $\mathcal{B}_2^d$. However, the same lower bound will hold up to a factor of $\sqrt{d}$ if each coordinate is drawn from $\mathcal{U}([-1/\sqrt{d},1/\sqrt{d}])$ which ensures $v_t \in \mathcal{B}_2^d$; we use $\mathcal{U}([-1,1])$ for the ease of exposition.

% \alex{Can we make it $v_t \in \Ball^d_2$ like the other results?}\paritosh{ We have had this conversation before. The reason of \emph{not} going for $\mathcal{B}_2^d$ is that it introduces correlation between the various coordinates, whereas, our iid positive result that follows is for iid wrt items and iid wrt agents, and hence, to present a valid comparison (aka separation) we need to have independence between the coordinates. This leads to the choice of the distribution $\mathcal{U}([-1,1])^d$, as defined below.} \daniel{Could we just scale it down by $\sqrt{d}$ so they all are in the ball?} \paritosh{We can, but why is this scaling needed? Using $\mathcal{U}([1, 1])$ or $v_{i,j} \sim \mathcal{U}([1/\sqrt{d}, 1/\sqrt{d}])$ will not affect the final conclusion, however, $\mathcal{U}([1, 1])$ looks cleaner.}
% \alex{$v_t \in \Ball^d_2$ and $v_{i,j} \sim \mathcal{U}([-1/\sqrt{d}, 1/\sqrt{d}])$ would raise fewer eyebrows. I'm scared that with the $\mathcal{U}([-1, 1])$ reviewer 2 will think that we're comparing apples to oranges}\paritosh{My point is the following: this negative result is to complement the iid result that follows, and also is for the same setting as the iid positive result. Just like the choice of $\mathcal{D}$ being supported on $[0,1]$ or $[0,B]$ doesn't matter in the iid positive result, similarly, here it doesn't matter. Added a line to explain this point above. Additionally, the choice of $\mathcal{U}([-1/\sqrt{d}, 1/\sqrt{d}])$ will make the argument messy, because we need to consider an inscribed ball within the hypercube we define.} \daniel{Why don't we just have a comment after the theorem that, although this isn't in the ball, we could easiliy scale down by $\sqrt{d}$ and then they would all be in the ball and the bound is only affected by $\sqrt{d}$ so the lower bound still holds.}\paritosh{I added this clarification at the top (before this comment thread). Please take a read and make any necessary edits.}

\begin{theorem}\label{thm: lower bound for iid vector balancing}
    Even for $n=2$ colors, for any $T \in \mathbb{N}$, any online algorithm $\mathcal{A}$, and any $d > 2$, when $\mathcal{A}$ is presented with a sequence of vectors $v_1, \dots, v_T \in \mathbb{R}^d$, where $v_{t,i} \sim \mathcal{U}([-1,1])$ in an i.i.d.\@ fashion, the discrepancy of $\mathcal{A}$ is $\Omega\left(\sqrt{\frac{\log{T}}{\log{\log{T}}}}\right)$, with probability at least $1 - 1/T^{\Theta(1)}$. %\alex{dfn whp}
\end{theorem}
\begin{proof}
    Let $\dist = \mathcal{U}([-1,1])$. We use the notation $v \sim \dist^d$ to denote a random vector $v \in \mathbb{R}^d$ each of whose coordinates are drawn independently from $\dist$. 
    The key observation is that, with sufficiently high probability, there is a long enough sequence of input vectors that are orthogonal to the current discrepancy vector; this leads to a large discrepancy at the end of this sequence. The following claim will be used to formalize this idea. 

    \begin{claim}\label{claim:hypercube_prob}
        There exists a constant $c>0$ such that, for all $\delta > 0$, and $u \in \mathbb{R}^d$, we have $\Pr_{v \sim \dist^d}[|\langle v, u\rangle| \leq \delta \norm{u}_2 \text{ and } \norm{v}_2 \in [1/2,1]] \geq c \delta$, where the constant $c$ depends on $d$. 
    \end{claim}
    \begin{proof} We can rewrite the probability of interest as
    \begin{align*}
        & \phantom{{}={}} \Pr_{v \sim \dist^d}[|\langle v, u\rangle| \leq \delta \norm{u}_2 \text{ and } \norm{v}_2 \in [1/2,1]]\\
        & = \Pr_v[\norm{v}_2 \in [1/2,1]] \cdot \Pr_v[|\langle v, u\rangle| \leq \delta \norm{u}_2 \mid \norm{v}_2 \in [1/2,1]] \numberthis \label{equation:obv-iid-lower-bound}
    \end{align*}

    We will show that $\Pr_v[\norm{v}_2 \in [1/2,1]] \geq c_1$ and $\Pr_v[|\langle v, u\rangle| \leq \delta \norm{u}_2 \mid \norm{v}_2 \in [1/2,1]] \geq c_2 \delta$ where $c_1$ and $c_2$ are constants that depends on $d$. These two inequalities, along with \Cref{equation:obv-iid-lower-bound}, imply that $\Pr_{v \sim \dist^d}[|\langle v, u\rangle| \leq \delta \norm{u}_2 \text{ and } \norm{v}_2 \in [1/2,1]] \geq c_1c_2 \delta = c \delta$, where $c = c_1 c_2$.

    To prove that $\Pr_v[\norm{v}_2 \in [1/2,1]] \geq c_1$ we use the fact that the volume of the unit Euclidean ball is given by $\textrm{vol}(\mathcal{B}^d_2) = \frac{\pi^{d/2}}{\Gamma(d/2+1)}$ where $\Gamma$ represents the gamma function~\cite{smith1989small}:
    $\Pr[\norm{v}_2 \in [1/2,1]] = \frac{\textrm{vol}(\mathcal{B}^d_2) - \textrm{vol}(\mathcal{B}^d_2)/2^d}{2^d} \geq \frac{\textrm{vol}(\mathcal{B}^d_2)}{2^{d+1}} = c_1,$
    where $c_1$ only depends on $d$. 
    
    It remains to prove that $\Pr_v[|\langle v, u\rangle| \leq \delta \norm{u}_2 \mid \norm{v}_2 \in [1/2,1]] \geq c_2 \delta$ for a constant $c_2$ that depends only on $d$. Conditioning on the event $\norm{u}_2 \in [1/2,1]$, the distribution of the random vector $v \sim \mathcal{D}^d$ is centrally symmetric, i.e., the probability density of $v$ only depends on $\norm{v}_2$ and not the direction of $v$. Define $\theta$ to be the random angle between $u$ and $v$. All possible angles $\theta \in [0, 2\pi]$ that $u$ can make with $v \sim \dist^d$ are equally likely. Using this fact, we get
    %For a vector $v \in [0,1]^d$ whose coordinates are drawn independently from $\mathcal{U}([0,1])$, we have that 
        \begin{align*}
 &\phantom{{}={}}\Pr_{v \sim \dist^d}[|\langle v, u\rangle| \leq \delta \norm{u}_2 \mid \norm{v}_2 \in [1/2,1]] \\
 & =  \Pr_{v \sim \dist^d}[ | \sqrt{d} \cos{\theta} | \leq \delta  \mid \norm{v}_2 \in [1/2,1]]\\
  & =  \Pr_{v \sim \dist^d}[ | \cos{\theta} | \leq \frac{\delta}{\sqrt{d}}  \mid \norm{v}_2 \in [1/2,1]]\\
  & =  \frac{(\pi/2 - \arccos(\delta/\sqrt{d}))}{\pi/2} \tag{$\theta \in [0, 2\pi]$ is uniformly distributed} \\
  & \geq 1 - \frac{\arccos(\delta/\sqrt{d})}{\pi/2} = \frac{2}{\pi\sqrt{d}} \cdot \delta,
        \end{align*}
        the penultimate inequality here follows from the Taylor expansion of $\arccos$, which implies that $\arccos(x) \leq \pi/2 - x$ for $x \geq 0$. Setting $c_2 = \frac{2}{\pi\sqrt{d}}$ completes the proof of the claim.
    \end{proof}
    Denote by $d_t \coloneqq \sum_{i=1}^t \chi_i v_i$, where $\chi_i \in \{ -1, 1 \}$ is the sign the algorithm picks, the discrepancy at time $t$. We know that, $\norm{d_t}_2^2 \geq \norm{d_{t-1}}_2^2 + \norm{v_t}_2^2 - 2|\langle d_{t-1}, v_t \rangle|$. For the case when $\norm{d_{t-1}}_2 \leq \frac{1}{8 \delta}$, from \Cref{claim:hypercube_prob}, we have that, with probability at least $c \delta$, $|\langle d_{t-1}, v_t\rangle| \leq 1/8$ and $\norm{v_t}_2 \in [1/2,1]$. Both these events imply that $\norm{d_t}_2^2 \geq \norm{d_{t-1}}_2^2 + 1/2 - 2\cdot 1/8 = \norm{d_{t-1}}_2^2 + 1/4.$
    
    We now divide the time horizon from $1,\ldots, T$ into $T/\tau$ contiguous chunks having $\tau$ timesteps each. Consider a contiguous chunk spanning timesteps $t_s, \ldots, t_e$ where $t_e - t_s = \tau$. Note that with probability at least $(c\delta)^\tau$ all the incoming vectors in this chunk will satisfy the condition in \Cref{claim:hypercube_prob}, thereby implying that $\norm{d_{t_e}}_2^2 - \norm{d_{t_s}}_2^2 \geq \tau/4$, which in turn will imply that $\norm{d_{t_e}}_2 \geq \sqrt{\tau/4}$.
    
    We now set $\delta = 1/(c \log{T})$ and $\tau = \log{T}/(2\log{\log{T}})$. Either at some point we have $\norm{d_{t-1}}_2 > \frac{1}{8 \delta} = (c\log{T})/8$, in which case the lower bound holds. Otherwise $\norm{d_{t-1}}_2 < \frac{1}{8 \delta}$ for all the timesteps, and with probability at least $1- \left(1 - (c\delta)^\tau\right)^{T/\tau} = 1- \left(1 - (1/\log{T})^{\log{T}/(2\log{\log{T}})}\right)^{(2T\log{\log{T}})/\log{T}} = 1- 1/T^{\Theta(1)}$ at least one of the chunks will have all its vectors almost orthogonal to the current discrepancy vector (i.e., all vectors will satisfy the condition in \Cref{claim:hypercube_prob}), leading to a discrepancy of at least $\sqrt{\tau/4} = O\left(\sqrt{\frac{\log{T}}{\log{\log{T}}}}\right)$. This concludes the proof of~\Cref{thm: lower bound for iid vector balancing}.
\end{proof}

% \subsection{Online Envy Minimization Warm-Up: The Case of Two Agents}\label{subsec: iid envy two agents}

% In this section, we give an algorithm for online envy minimization, against an i.i.d.\@ adversary, for the case of $n=2$ agents. We extend this algorithm to the $n$ agent case in the next section.

% \begin{algorithm}[t]
% \caption{Two-Phase Envy Minimization Algorithm for Two Agents}\label{algo:online envy for two agents}
% \SetAlgoLined
% \DontPrintSemicolon

% Set $T^{(1)} \gets T - \lceil \log T \sqrt{T} \rceil$, and $T^{(2)} \gets \lceil \log T \sqrt{T} \rceil$\;
% Run welfare maximization (i.e., allocate item $t$ to $\argmax_{i \in [n]} v_{i,t}$ breaking ties randomly)  for $T^{(1)}$ steps \;
% \For{$t \gets T^{(1)} + 1$ \KwTo $T$}{
%     Allocate item $t$ to the most envious agent, i.e., $\argmax_{i \in [n]} \max_{j \in [n]} \envy^t_{i,j}$. \;
% }
% \end{algorithm}

% \begin{theorem}\label{thm:two agent upper bound iid fair division}
% Algorithm~\ref{algo:online envy for two agents} has envy at most $c + 1$ with probability at least $1 - O(1 / T^{c/2})$, for all integers $c \ge 1$. 
% \end{theorem}

% \begin{proof}
% Fix a value distribution $\dist$. For two independent random variables $X, Y \sim \dist$, let $\dist^{gap}$ and $\dist^{|gap|}$ be the distribution of $X - Y$ and $|X - Y|$, respectively (i.e., $\dist^{gap}$ the difference of two independent draws, and  $\dist^{|gap|}$ is the absolute difference of two independent draws).  The following lemma implies that, with high probability, Algorithm~\ref{algo:online envy for two agents} cannot have heavy envy cycles.
% \begin{lemma}\label{lem:envy-cycle}
%     Fix a positive integer $c$. For $T \geq C_0$, for a universal constant $C_0$ (independent of $\dist$), with probability at least $1 - 2 \left(\frac{2eT^{(2)}}{T} \right)^c$, it holds that $\envy^t_{1,2} + \envy^t_{2,1} \le 2c$ for all $t$.
% \end{lemma}

% \begin{proof}[Proof of \Cref{lem:envy-cycle}]
% Fix $i \ne j$. We will first prove that $v_i(A_i^t) + c \ge v_j(A_i^t)$ for all $t$, with probability at least $1 - 2 \left(\frac{2eT^{(2)}}{T} \right)^c$. Without loss of generality, consider agents $i = 1$ and $j = 2$.  Let $S_t := v_1(A_1^t) - v_2(A_1^t)$ be the difference in values at time $t$. Our goal is to show that $S_t \ge -c$ with high probability at every step. Let $X_t$ be contribution of good $g_t$ to $S_t$, i.e., \[
% X_t = \begin{cases}
%     v_{1,t} - v_{2,t} & \text{ if } g_t \in A_1^t\\
%     0 & \text{ otherwise.}
% \end{cases}
% \]

% It will be convenient to think of $X_t$ as a draw from $W_t \cdot I_t$, where $W_t \sim D^{gap}$, and $I_t$ is an indicator for the event that agent $1$ gets the item. Importantly, $W_t$ and $I_t$ are independent. 

% We can write $S_t = \sum_{\ell = 1}^t X_{\ell}$. Let $\dist^{gap0}$ be the distribution of $\max(X, 0)$ for $X \sim \dist^{gap}$, i.e., the distribution which takes a value of zero half the time, and samples from $|\dist^{gap}|$ half the time ($\dist^{gap}$ is a symmetric distribution).
% For $t \le T^{(1)}$, $X_t \sim \dist^{gap0}$, independent of all previous timesteps, and therefore $X_t \ge 0$ with probability $1$. Thus, for all $t \le T^{(1)}$, $S_t \ge 0$ with probability $1$.

% For $t > T^{(1)}$, $X_t$ depends on the history. There is some probability that $g_t$ is given to agent $i=1$, in which case $X_t \sim \dist^{gap}$, and otherwise it is not, in which case $X_t = 0$. We will couple this with an alternate draw $X'_t$ sampled independently from $-\dist^{gap0} = \min(W_t, 0)$. Clearly, $X_t$ (drawn from $W_t \cdot I_t$) first-order stochastically dominates $X'_t$ (drawn from $\min(W_t, 0)$). It also holds that $S_t = \sum_{\ell = 1}^t X_{\ell}$ first-order stochastically dominates $S'_t = S_{T^{(1)}} + \sum_{t' = T^{(1)}}^{t} X'_{t'}$. Therefore, it suffices to show that $S'_t \ge -c$ for all $t > T^{(1)}$, with probability at least $1 - 2 \left(\frac{2eT^{(2)}}{T} \right)^c$.
% Next, note that $X'_t \le 0$ for all $t > T^{(1)}$. Thus, $S'_t$ is nonincreasing for such $t$, and it is sufficient to show that $S'_T \geq -c$ with probability at least $1 - 2 \left(\frac{2eT^{(2)}}{T} \right)^c$. 
% $S'_T$ is the sum of $T^{(1)}$ independent draws from $\dist^{gap0}$ and $T^{(2)}$ independent draws from $-\dist^{gap0}$. From \Cref{lem:concentration}, plugging in $K = T$ and $L = T^{(2)}$ (as long as $T^{(2)} = \lceil \log T \sqrt{T} \rceil < T/8e$), we have that $v_i(A_i^t) + c \ge v_j(A_i^t)$ for all $t$.
% Conditioned on this event, we have that, $\envy^t_{1,2} + \envy^t_{2,1} = (v_1(A_2^t) - v_1(A_1^t)) + (v_2(A_1^t) - v_2(A_2^t)) = (v_2(A_1^t) - v_1(A_1^t)) + (v_1(A_2^t) - v_2(A_2^t)) \le 2c$, as needed.
% \end{proof}


% The next lemma shows that, with high probability, a variant of Algorithm~\ref{algo:online envy for two agents} that allocates the last $T^{(2)}$ items to a single agent, guarantees small envy to that agent.

% \begin{lemma}\label{lem:high-value}For all $c > 1$ and $T^{(2)} \ge \log T \sqrt{T}$, the algorithm that runs welfare maximization for $T^{(1)} = T - T^{(2)}$ and then allocates all remaining $T^{(2)}$ items to a single agent must leave that agent with envy as most $c$ with probability at least $1 - O(1/T^{c})$. 
% \end{lemma}

% Finally, the next lemma shows that when there are no heavy envy cycles and the aforementioned variant of Algorithm~\ref{algo:online envy for two agents} succeeds (guarantees small envy to the agent that got the last $T^{(2)}$ items), then Algorithm~\ref{algo:online envy for two agents} guarantees small envy to both agents.

% \begin{lemma}\label{lem:small envy for two agents}
%     Conditioned on the events of \Cref{lem:envy-cycle,lem:high-value}, 
%     $\envy^T_{ij} \le c + 1$ for all $i, j$.
% \end{lemma}
% \begin{proof}
% First, we will prove that there exists a time $t^* \geq T^{(1)}$ such that $\envy^t_{ij} \le c + 1$ for all $i, j$. If not, $\envy^t_{ij} > c$ for all $t$. We've conditioned on $\envy^t_{12} + \envy^t_{21} \le 2c$ for all $t$ (\Cref{lem:envy-cycle}), therefore $\envy^t_{ji} \leq c$, and therefore $i$ must have been the most envious agent in all final $T^{(2)}$ steps. Therefore, $i$ received the last $T^{(2)}$ items. By~\Cref{lem:high-value}, $\envy^t_{ij} \leq c$; a contradiction.

% Second, we will prove that $\envy^{t'}_{ij} \le c + 1$ for all $i,j$ and all $t' \ge  t^*$, which implies the lemma. As argued earlier, in the last $T^{(2)}$ steps of the algorithm the envy of an agent can increase only if they are \emph{not} the most envious agent. We've conditioned on $\envy^t_{12} + \envy^t_{21} \le 2c$ for all $t$ (\Cref{lem:envy-cycle}), therefore, the envy of such an agent is at most $c$, and it can increase to at most $c+1$. Therefore, starting at step $t^*$, no agent can have envy strictly greater than $c+1$.
% \end{proof}

% The events of~\Cref{lem:envy-cycle,lem:high-value} occur with probability at least $1 - O(1/T^{c})$; therefore,~\Cref{lem:small envy for two agents} implies the theorem.
% \end{proof}

\subsection{Online envy minimization}\label{subsec: iid envy n agents}

In this section, we give an algorithm,    ~\Cref{algo:online envy for n agents}, for online envy minimization, against an i.i.d.\@ adversary. \Cref{algo:online envy for n agents} works in two phases. In phase 1, which lasts $T^{(1)}$ steps, it makes allocations using the welfare maximization algorithm (``item $j$ is allocated to the agent with the largest value''). In Phase 2, at every step $t$ the algorithm singles out the set of agents who have not received a large number of items (within phase 2, up until $t$); among this set, it allocates item $t$ to the agent who is envied the least by agents in this set.


\begin{theorem}\label{thm:n agent upper bound iid fair division}
For all positive integers $c$, \Cref{algo:online envy for n agents} has envy at most $c + 1$ with probability at least $1 - O(T^{-c/2})$.
\end{theorem}

Note that the $O(\cdot)$ hides constants that depend on the number of agents, but is independent of the value distribution.

\begin{proof}
    Fix a positive integer $c$, an arbitrary distribution $\calD$ supported on $[0, 1]$, and a time horizon $T$. Throughout, we assume that $T$ is sufficiently large, i.e., larger than some number $T_0$ that depends only on $n$ and $c$, not on the distribution $\calD$. Let $F$ denote the CDF of $\calD$. 

    

    A key observation is that we can analyze the algorithm using an equivalent, but more structured method of sampling item values. Normally, at each time step $t$, the item values revealed to the algorithm are sampled i.i.d.\@ from $\mathcal{D}$, independent of every decision made so far. Instead, we define an equivalent experiment as follows. Let $G^{welf} = \{g^{welf}_1, \ldots, g^{welf}_{T^{(1)}}\}$ be a set of $T^{(1)}$ goods and, for each agent $i \in [n]$,  let $G^{i} = \{g^i_1, \ldots, g^i_{T^{(2)}}\}$ be a set of $T^{(2)}$ goods.
    \begin{enumerate}[leftmargin=*]
        \item Before the algorithm begins, nature samples values $(V^g_1, \ldots, V^g_n)$ for each $g \in G^{welf} \cup \bigcup_i G^i$, where each $V^g_i \stackrel{i.i.d.}{\sim} \calD$. 
        \item During \emph{Phase 1} of the algorithm (welfare maximization), when the $t^{th}$ item arrives, it is revealed to be  item $g^{welf}_t$, with pre-sampled values $(V^{g^{welf}_t}_1, \ldots, V^{g^{welf}_t}_n)$.
        \item During \emph{Phase 2} (lines 3-7 in~\Cref{algo:online envy for n agents}), suppose item $t$ will be assigned to agent $i$ who, at this point, has received $k$ items during phase $2$ ($|A^t_i \setminus G^{welf}| = k$). Then, item $t$ is revealed to be $g^i_{k + 1}$ with pre-sampled values $(V^{g^i_{k + 1}}_1, \ldots, V^{g^i_{k + 1}}_n)$.
    \end{enumerate}
    % \alex{why is this something important to note right here?}\daniel{happy to delete} Note that in the final allocation, $A^T_i \subseteq G^{welf} \cup G^i$, and furthermore, $A^T_i \cap G^i = \set{g^i_1, \ldots, g^i_k}$ for some $k$.

    \begin{algorithm}[t]
\caption{Two-Phase Envy Minimization Algorithm}\label{algo:online envy for n agents}
\SetAlgoLined
\DontPrintSemicolon

Set $T^{(1)} \gets T - \frac{n(n - 1)}{2}\lceil \log T \sqrt{T} \rceil$, and $T^{(2)} \gets \frac{n(n - 1)}{2} \lceil \log T \sqrt{T} \rceil$\;
Run welfare maximization (i.e., allocate item $t$ to $\argmax_{i \in [n]} v_{i,t}$ breaking ties randomly)  for $T^{(1)}$ steps \;
\For{$t \gets T^{(1)} + 1$ \KwTo $T$}{
    Let $w^t_i$ be the number of items agent $i$ has received in steps $t' >T^{(1)}$.\;
    Let $S$ be the smallest (in terms of cardinality) subset of agents, such that $\forall i \in S, j \notin S$ $w^t_i \leq w^t_j - \lceil \log T \sqrt{T} \rceil$. \;
    Allocate item $t$ to an agent $i \in S$ who is envied the least, i.e., $\argmin_{i \in S} \max_{j \in S} \envy^t_{j,i}$. \;
}
\end{algorithm}

    Importantly, the allocation decision for item $t$ does not depend on agents' values for this item. This ensures that the value vector $(V^{g^i_{k + 1}}_1, \ldots, V^{g^i_{k + 1}}_n)$ is independent of all decisions made by the algorithm. Consequently, this modified experiment is statistically identical to the original setup in terms of the envy of the final allocation.

    A second useful modification is to work with item \emph{quantiles} instead of item values. More formally, instead of directly sampling $V^g_i$, we will first sample a quantile $Q^g_i \sim \mathcal{U}[0, 1]$ and then set $V^g_i = F^{-1}(Q^g_i)$ where $F^{-1}$ is the generalized inverse of $F$. Throughout the remainder of this proof, we condition on the probability $1$ event that all $Q^g_i$s are distinct.
    Note that for $g \in G^{welf}$, allocating item $g$ to an agent with the highest quantile, $i \in \argmax_{j} Q^g_j$, is equivalent to welfare maximization with random tie-breaking.\footnote{We make this point, since unequal quantiles does not imply unequal values.} Thus, we will assume these are coupled. Since all quantiles are distinct by assumption, ties never occur, and this allocation is always well-defined. 

    \textbf{No heavy envy-cycles.} Our first high-level step will be to show that, with high probability, no envy cycles with large weight exist during the execution of the algorithm.

    \begin{lemma}\label{lem:no-cycle}
        With probability $1 - O ( T^{-c/2}  )$, at every time $t \ge T^{(1)}$, there does not exist a cycle of agents $i_1, \ldots, i_k, i_{k+1}=i_1$ such that $\envy_{i_j, i_{j + 1}} > c$ for all $j = 1, \ldots, k$. 
    \end{lemma}

    The proof of~\Cref{lem:no-cycle} crucially relies on the following concentration inequality (which, to the best of our knowledge, is not known), that might be of independent interest.

    \begin{lemma}\label{lem:concentration}
    Fix positive integers $L, K$, and $c$, with $L < \frac{K}{4e}$. Let $Y_1, \ldots, Y_K$ be i.i.d.\@ draws from a distribution supported on $[0, 1]$. Then, $\Pr\left[\sum_{i \le K - L} Y_i - \sum_{i > K - L} Y_i < -c \right] \leq 4 \cdot \left(\frac{2 e L}{K} \right)^{c + 1}$.
    \end{lemma}

    The proof of~\Cref{lem:no-cycle}  also relies on two (relatively more straightforward) facts,~\Cref{lem:phase-2-items,lem:halls}. 
    The first lemma shows that the items allocated in phase 2 are relatively balanced among the agents, up to additive $\ceil{\log T \sqrt{T}}$ factors.
    
    
    %the way the algorithm is defined, there is a lot of structure in the number of items agents receive during phase $2$. 
    \begin{lemma}\label{lem:phase-2-items}
        Fix a time $t$. Let $w^t_i|$ be the number of items agent $i$ has received in phase 2, i.e., $w^t_i = |A^T_i \setminus G^{welf}|$. Let $(w^t_{i_1}, \ldots, w^t_{i_n})$ be these numbers sorted from smallest to largest; so, $w^t_{i_j} \le w^t_{i_{j + 1}}$. Then, for all $j \le n - 1$, $w^t_{i_{j + 1}} \le w^t_{i_j} + \ceil{\log T \sqrt{T}}$. Furthermore, $w^t_{i_n} \le (n - 1) \ceil{\log T \sqrt{T}}$, i.e., no agent ever receives more than $(n - 1) \ceil{\log T \sqrt{T}}$ phase 2 items. 
    \end{lemma}
    The second lemma is a sufficient condition for bounding the envy between two sets of values and is reminiscent of approximate \emph{stochastic-dominance envy-freeness} (SD-EF). The proof is based on a generalization of Hall's theorem.
    \begin{lemma}\label{lem:halls}
        Given two sequences of values $a_1, \ldots, a_k$ and $b_1, \ldots, b_{\ell}$ where each $a_i, b_i \in [0, 1]$. Suppose that, for each $a_i$, $|\set{i' | a_{i'} \ge a_i}| \le |\set{i' | b_{i'} \ge a_i}| + c$. Then, $\sum_i a_i \le \sum_i b_i + c$. 
    \end{lemma}

    \textbf{Long phase 2 eliminates envy.} Our second high-level step will be to show that if phase 2 is sufficiently long, then with high probability, envy can be eliminated.

    % \begin{lemma}\label{lem:high-value-n} Fix any two agents $i, j \in \agents$ and an integer $L \le (n - 2) \ceil{\log T \sqrt{T}}$. Let $A_j = A^{T^{(1)}}_j \cup \{g^j_1, \ldots, g^j_L\}$ and  $A_i = A^{T^{(1)}}_i \cup \{g^i_1, \ldots,  g^i_{L + \ceil{\log T \sqrt{T}}}\}$. That is, suppose we run phase 1 as usual, then in phase 2, give $L$ phase 2 items to agent $j$, and $L + \ceil{\log T \sqrt{T}}$ phase 2 items to agent $i$. Then, $\envy_{ij} \le c$ with probability \alex{TODO}.   
    % \end{lemma}

    \begin{lemma}\label{lem:high-value-n} With probability $1 - O(T^{-c/2})$ it is the case that for all agents $i, j \in \agents$ and all time steps $t \ge T^{(1)}$, if $|A^t_i \setminus G^{welf}| \ge |A^t_j \setminus G^{welf}| + \ceil{\log T \sqrt{T}}$, then $\envy^t_{i, j} \le c$. 
    \end{lemma}


    \textbf{Putting it all together.}
    With~\Cref{lem:no-cycle,lem:high-value-n} in hand, we are ready to prove the theorem.
    
    Let $H^t$ be a graph with nodes $[n]$ where there is an edge $(i, j)$ if $\envy^t_{i, j} > c$. Condition on the events in \Cref{lem:no-cycle} and \Cref{lem:high-value-n}. %\daniel{for all L maybe if we don't have this in the lemma statement}. 
    These happen with probability $1 - O(T^{-c/2})$. Then,~\Cref{lem:no-cycle} ensures that $H^t$ is acyclic for all $t$, while~\Cref{lem:high-value-n} ensures that if $|A^t_i \setminus G^{welf}| \le  |A^t_j \setminus G^{welf}| + \ceil{\log T \sqrt{T}}$, then $(j, i) \notin H^t$.%\alex{need to confirm this part} \daniel{I think any explanation of this should go inside of the lemma, so we can change the statement to match what is written here. High level though, the argument  is that if agent $i$ has received $L^i$ items and $j$ has received $L^j$ with $L^j \ge L^i + \ceil{\log T \sqrt{T}}$, then the envy at most as much had $j$ received only $ L^i + \ceil{\log T \sqrt{T}}$ exactly (i.e., we remove their final $L^j - (L^i + \ceil{\log T \sqrt{T}})$). The lemma guarantees that $j$ doesn't envy $i$ by more than $c$ in this case. } \alex{so, shouldn't it be $(j,i) \notin H^t$?}


    First, we prove that if an agent $i$ received an item at some point during phase 2, then $\envy^T_{j, i} \le c + 1$, for all $j \in \agents$. To this end, suppose that $i$ does indeed receive an item at some point during phase 2, and let $t$ be the \emph{last} time step for which $i$ received an item. 

    We first claim that $\envy^{t - 1}_{j, i} \le c$ for all $j \ne i$, i.e., $i$ is a source node in $H^{t - 1}$. Let $S$ be the set defined in~\Cref{algo:online envy for n agents} for time step $t$, i.e., for all $j \in S$ and $j' \notin S$, $j$ has received at least $\ceil{\log T \sqrt{T}}$ fewer items in phase 2, or  $|A^{t - 1}_j \setminus G^{welf}| \le |A^{t - 1}_{j'} \setminus G^{welf}| - \ceil{\log T \sqrt{T}}$. Note that $i \in S$ as they received item $t$, and for all $j' \notin S$, $(j', i) \notin H^{t - 1}$, by~\Cref{lem:high-value-n}. Then, consider $H^{t - 1}[S]$, the subgraph of $H^{t - 1}$ that only containing the nodes $S$. Note that $H^{t - 1}[S]$ is acyclic, since $H^{t - 1}$ is acyclic. Furthermore, by definition, source nodes in $H^{t - 1}[S]$ are envied by $\le c$ by all agents in $S$, while non-source nodes are envied by $>c$ by at least one agent in $S$. Hence, $i$ must be a source node in $H^{t - 1}[S]$. Together with the fact that there are no $(j, i)$ edges for $j \notin S$, we have that $i$ is a source node in $H^{t - 1}$.

    Now, since $\envy^{t - 1}_{j, i} \le c$, giving an item to $i$ can increase envy by at most $1$. Hence, $\envy^{t}_{j, i} \le c + 1$. Furthermore, as $i$ never received any more items after this time ($t$ was defined as the last item $i$ received), envy toward $i$ cannot increase. Hence, $\envy^T_{j, i} \le c + 1$, as needed. 

    Finally, let $w^t_i = |A^T_i \setminus G^{welf}|$ be the number of items allocated to agent $i$ in phase $2$. Note that if $w^t_i > 0$, by our previous argument, $\envy_{j, i} \le c + 1$ for all $j \ne i$. So, if $w^t_i > 0$ for all $i \in \agents$, we are done. Suppose this is not the case. So, there exists an agent $i$ such that  $w^t_i = 0$. Let $i_1, \ldots, i_n$ be an ordering of the agents sorted by $w^t_i$, i.e., $w^t_{i_1} \le \cdots \le w^t_{i_n}$. The assumption that $w^t_i = 0$ implies that $w^t_{i_1} = 0$. We claim that $w^t_{i_2} \ge \ceil{\log T \sqrt{T}}$. Indeed, by induction,~\Cref{lem:phase-2-items} ensures that for all $j \ge 2$, $w^t_{i_j} \le w^t_{i_2} + (j - 2) \ceil{\log T \sqrt{T}}$. Hence, 
    $\sum_{j=1}^n w^t_{i_j} \le (n - 1) \cdot w^t_{i_2} + \frac{(n - 2)(n - 1)}{2} \, \ceil{\log T \sqrt{T}}$. 
    However, since this is time $T$,  $\sum_j w^t_{i_j} = T^{(2)} = \frac{n(n - 1)}{2} \cdot \ceil{\log T \sqrt{T}}$. Together, these imply that $w^t_{i_2} \geq \ceil{\log T \sqrt{T}}$. Therefore, all agents other than $i_1$ received at least one item during phase 2, and hence are not envied by more than $c + 1$. On the other hand, all agents $j \ne i_1$ received at least $\ceil{\log T \sqrt{T}}$ items more than $i_1$ in phase $2$, and therefore, $\envy_{j, {i_1}} \le c \le c + 1$ as needed.
\end{proof}
