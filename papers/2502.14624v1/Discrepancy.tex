\section{Performance against an oblivious adversary}\label{sec: oblivious}
 
In this section, we prove that there exists an algorithm for the online multicolor discrepancy problem that guarantees, with high probability, a maximum discrepancy of $O(\sqrt{\log T})$ against an oblivious adversary (\Cref{subsec: optimal multicolor discrepancy}); there is a matching lower bound for the online vector balancing problem~\cite{kulkarni2024optimal}, therefore our algorithm for the online multicolor discrepancy problem is optimal. As a corollary, we get an algorithm that guarantees, with high probability, an envy of $O(\sqrt{\log T})$, against an oblivious adversary, for the online envy minimization problem; we give a matching lower bound in~\Cref{subsec: envy for oblivious}. Missing proofs are deferred to~\Cref{app: missing from section 3}.

\subsection{Balancing sets of vectors}\label{subsec: balance sets of vectors}

% \alex{Define $\gamma_d(K)$, $\conv(.)$, $\mathbf{0}$. }


Similar to the result of \citet{kulkarni2024optimal}, we think of a discretized adversary whose choices correspond to the edges of a (massive) rooted tree. \citeauthor{kulkarni2024optimal} prove the following:


\begin{theorem}[\cite{kulkarni2024optimal}]\label{theorem:tree-kulkarni} Let $\calT = (V,E)$ be a rooted tree, where every edge $e \in E$ has a corresponding vector $v_e \in \Ball^d_2$. Let $K \subseteq \mathbb{R}^d$ be a convex body with $\gamma_d(K) \geq 1 - \frac{1}{2|E|}$. Then there exists $z \in \{-1,1\}^{|E|}$ such that, for all $u \in V$, the vector sum $\sum_{e \in P_u} z_e v_e \in 5K$, where $P_u$ is the set of edges of the path from the root to the node $u$.
\end{theorem}

For our result, we think of the adversary as assigning a set of vectors $S_e$ to each edge $e$, and prove that we can choose a vector from each set $S_e$, so that all paths from the root are balanced.


\begin{theorem}\label{theorem:tree-reduction}
 Let $\calT = (V,E)$, $|E| \geq 2$, be a rooted tree such that: (1) every $e \in E$ has an associated set of vectors $S_e \subseteq \Ball^d_2$ satisfying $\mathbf{0} \in \conv(S_e)$, and (2) there exists an $\ell \in \mathbb{N}$, $\ell \geq 2$, such that, for all $e \in E$, $\mathbf{0}$ is a convex combination of at most $\ell$ vectors in $S_e$. Let $K \subseteq \mathbb{R}^d$ be a symmetric convex body with $\gamma_d(K) \geq 1 - \frac{1}{\ell\, |E|}$. Then, for every edge $e \in E$, there exists a vector $v_e \in S_e$, such that for all $u \in V$, $\sum_{e \in P_u} v_e \in 11 \, K$, where $P_u$ is the set of edges of the path from the root to the node $u$.
\end{theorem}
\begin{proof}
    Our goal is to select a vector $v_e \in S_e$ from each set $S_e$ to satisfy the desired property. At a high level, we start with a \emph{fractional} selection of vectors from each set $S_e$ such that, for every node $u \in V$, the fractional vector sum of the edges in the path from the root to $u$ is $\mathbf{0}$ (so, the desired property of being contained in $11 K$ is clearly satisfied). We iteratively round this fractional selection to get a single vector from each set $S_e$, in a way that every rounding step does not increase the vector sums we are interested in by too much.


    For all $e \in E$, by definition, $\mathbf{0} \in \conv(S_e)$ and $\mathbf{0}$ is a convex combination of at most $\ell$ vectors in $S_e$. %\footnote{Via Caratheodory's theorem, $\ell$ must be at most $n+1$. \alex{why is this being pointed out? where is it used?}} 
    Therefore there exists a fractional selection of vectors $X^e = \{(v^e_1,x^e_1), (v^e_2,x^e_2), \ldots, (v^e_{\ell},x^e_{\ell})\}$, with $(v^e_i,x^e_i) \in S_e \times [0,1]$, such that $\sum_{i=1}^{\ell} x^e_{i} \cdot v^e_i = \mathbf{0}$, and $X^e$ is \emph{feasible}, i.e., $\sum_{i=1}^{\ell} x^e_{i} = 1$ and $x^e_i \in [0,1]$, for all $i \in [\ell]$. In the subsequent proof, we show the following two claims:
    \begin{enumerate}
        \item [(a)] 
            We can round each $x^e_i$ to $\hat{x}^e_i$ such that (i) for every $e \in E$, $\sum_{i=1}^{\ell} \hat{x}^e_i = 1$, and $\hat{x}^e_i \in [0,1]$ for all $i\in [\ell]$, (ii) the fractional part of each $\hat{x}^e_i$ is at most $\log(\frac{2 \, \ell \, h}{\varepsilon})$ bits long, where $h$ is the height of the tree $\calT$ and $\varepsilon = \frac{1}{2}-\frac{1}{\ell |E|}  > 0$, and (iii) for all $u \in V$, we have $\sum_{e \in P_u} \sum_{i=1}^{\ell} \hat{x}^e_i \cdot v^e_i \in K$.
        
        %Note that $\ell \geq 2$, and without loss of generality $|E| \geq 2$ (otherwise the theorem is trivial), so $\frac{1}{2}-\frac{1}{\ell |E|} > 0$. \alex{fix} 
        \item [(b)] 
            Let $Y^e = \{(v^e_1,y^e_1), \ldots, (v^e_{\ell},y^e_{\ell})\}$ be a feasible ($\sum_{i=1}^{\ell} y^e_i = 1$ and $y^e_i \in [0,1]$ for all $i \in [\ell]$), fractional selection of vectors. If, for every $e \in E$ and $i \in [\ell]$, the fractional part of $y^e_i$ is $k \geq 1$ bits long, then we can round the $k^{th}$-bit of all $\{y^e_i\}_{i,e}$ to get $\{\hat{y}^e_i\}_{i,e}$, whose fractional parts are at most $k-1$ bits long, and, for every $u \in V$, $\left( \sum_{e \in P_u} \sum_{i=1}^{\ell} \hat{y}^e_i v^e_i - \sum_{e \in P_u} \sum_{i=1}^{\ell} y^e_i v^e_i  \right) \in 2^{-k} \cdot 10 K$.
    \end{enumerate}

Starting from the fractional selection $X^e$, applying the rounding process of (a) gives us a feasible fractional selection $\hat{X}^e$ such that $\sum_{e \in P_u} \sum_{i=1}^{\ell} \hat{x}^e_i \cdot v^e_i \in K$. Then, repeatedly applying the rounding process of (b), starting from the fractional selection $\hat{X}^e$, results in an integral selection, after at most $\log(\frac{2 \, \ell \, h}{\varepsilon})$ steps. Let $A^e = \{ a^e_i \}_{i \in [\ell]}$, where $a^e_i \in \{ 0 , 1 \}$ and $\sum_{i=1}^\ell a^e_i = 1$, be the integral selection obtained at the end of the process. We have that $\left( \sum_{e \in P_u} \sum_{i=1}^{\ell} a^e_i v^e_i - \sum_{e \in P_u} \sum_{i=1}^{\ell} \hat{x}^e_i v^e_i  \right) \in \left( 2^{-\log(\frac{2 \, \ell \, h}{\varepsilon})} + 2^{-\log(\frac{2 \, \ell \, h}{\varepsilon})+1} + \dots + 2^{-1} \right) \cdot 10 K \subseteq 10 K$. And, since $\sum_{e \in P_u} \sum_{i=1}^{\ell} \hat{x}^e_i \cdot v^e_i \in K$, we have that $\sum_{e \in P_u} \sum_{i=1}^{\ell} a^e_i v^e_i \in 11 K$.

\paragraph{Proving $(a)$.} 
Let $\varepsilon = \frac{1}{2}-\frac{1}{\ell |E|}  > 0$ be a constant, and let $b = \log(\frac{2 \, \ell \, h}{\varepsilon})$. Construct $z^e_i$, for every $e \in E$ and $i \in [\ell]$, by taking $x^e_i$ and setting to zero all the bits in the fractional part of $x^e_i$ after the $b^{th}$ bit. 
Then, $\hat{x}^e_1 = z^e_1 + \left( 1 - \sum_{i=1}^{\ell} z^e_i \right)$, and $\hat{x}^e_i = z^e_i$ for all $i = 2, \dots, \ell$.
Clearly, for all $i \geq 2$, $\hat{x}^e_i \in [0,1]$, and the fractional part of $\hat{x}^e_i$ is at most $b$ bits long. 
By definition, $\sum_{i=1}^\ell \hat{x}^e_i = \hat{x}^e_1 + \sum_{i=2}^{\ell} \hat{x}^e_i = z^e_1 + \left( 1 - \sum_{i=1}^{\ell} z^e_i \right) + \sum_{i=2}^{\ell} z^e_i = 1$.
Furthermore, since $\sum_{i=2}^{\ell} \hat{x}^e_i \leq 1$ and $\sum_{i=1}^\ell \hat{x}^e_i = 1$, we have that $\hat{x}^e_1 \in [0,1]$. By the construction of the $z^e_i$s, the fractional part of $1 - \sum_{i=1}^{\ell} z^e_i = \sum_{i=1}^{\ell} x^e_i - \sum_{i=1}^{\ell} z^e_i$ is at most $b$ bits long, and therefore, the fractional part of $\hat{x}^e_1$ is at most $b$ bits long. Finally, for all $u \in V$,
\begin{align*}
\norm{ \sum_{e \in P_u} \sum_{i=1}^\ell \hat{x}^e_i \cdot v^e_i}_2 &= \norm{ \sum_{e \in P_u} \sum_{i=1}^\ell \hat{x}^e_i \cdot v^e_i - \sum_{e \in P_u} \sum_{i=1}^\ell x^e_i \cdot v^e_i}_2 \\
&= \norm{ \sum_{e \in P_u} \sum_{i=1}^\ell (\hat{x}^e_i - x^e_i) \cdot v^e_i }_2 \\
&\leq h \cdot \left( \ell \cdot 2^{-b} + (\ell - 1) \cdot 2^{-b} \right) \\
&\leq \varepsilon,
\end{align*}
where in the first equality we used the fact that $\sum_{e \in P_u} \sum_{i=1}^\ell x^e_i \cdot v^e_i = \mathbf{0}$ and in the first inequality we used that $\hat{x}^e_1 - x^e_1 \leq \ell \cdot 2^{-b}$. Therefore, $\sum_{e \in P_u} \sum_{i=1}^\ell \hat{x}^e_i \cdot v^e_i \in \varepsilon \, \Ball^d_2$. Since $\gamma_d(K) \geq 1 - \frac{1}{\ell|E|} \geq \frac{1}{2} + \varepsilon$, we have $\varepsilon \, \Ball^d_2 \subseteq K$ (\Cref{proposition:large-body-ball}). So, overall, $\sum_{e \in P_u} \sum_{i=1}^\ell \hat{x}^e_i \cdot v^e_i \in K$.
    
\paragraph{Proving $(b)$.} Assume that for all $e \in E$ and $i\in [\ell]$, the fractional part of $y^e_i$ is at most $k$ bits long. To round the $k^{th}$-bits of $\{y^e_i\}_{i,e}$, we first construct a new tree $\calT' = (V',E')$ that has vectors (instead of sets) associated to each edge, and then invoke~\Cref{theorem:tree-kulkarni} with $\mathcal{T}'$ and $K$. Intuitively, the signs from the guarantee of~\Cref{theorem:tree-kulkarni} will tell us how to round (up or down) the $k^{th}$-bit of $\{y^e_i\}_{i,e}$, so that the resulting $\{\hat{y}^e_i\}_{i,e}$ (after rounding) have at most $k-1$ bits in the fractional part, and this rounding process doesn't incur too much cost. 

For each $e \in E$, let $I^e = \{i_1, i_2, \ldots, i_{2q}\} \subseteq [\ell]$ be the set of indices such that, for every $j \in I^e$, the $k^{th}$ bit of the fractional part of $y^e_{j}$ is $1$; the set $I^e$ may be empty. Since $\sum_{i=1}^{\ell} y^e_i = 1$, $|I^e|$ must be even. For every $e \in E$, we pair up consecutive indices in $I^e$, and corresponding to each pair we define a vector. Formally, if $I^e = \emptyset$, set $\widetilde{S}^e = \{\mathbf{0}\}$; otherwise, if $I^e \neq \emptyset$, define the set of vectors 
\[
\widetilde{S}^e = \left\{ \frac{1}{2}\left(v^e_{i_{2p}} - v^e_{i_{2p+1}}\right) : \text{ for every } i_{2p}, i_{2p+1} \in I^e \right\}.
\]
For each $e \in E$, we have $\widetilde{S}^e \subseteq \Ball^d_2$ since $\norm{\frac{1}{2}\left(v^e_{i_{2p}} - v^e_{i_{2p+1}}\right)}_2 \leq \frac{1}{2}\left( \norm{v^e_{i_{2p}}}_2 + \norm{v^e_{i_{2p+1}}}_2\right) \leq 1$.  Also, by definition, $|\widetilde{S}^e| \leq \frac{|I^e|}{2} \leq \lfloor \frac{\ell}{2} \rfloor$. 


To construct $\calT'$, we start from $\calT$ and replace every edge $e \in E$ by a path of $|\widetilde{S}^e|$ edges $e^{(1)}, e^{(2)}, \ldots, e^{(|\widetilde{S}^e|)}$. For every edge $e^{(i)} \in E'$, associate a unique vector $u^{(i)}_e \in \widetilde{S}^e$ (so, there's a bijection between $\widetilde{S}^e$ and $\{e^{(1)}, e^{(2)}, \ldots, e^{(|\widetilde{S}^e|)}\}$). Note that, since  $|\widetilde{S}^e| \leq \lfloor \frac{\ell}{2} \rfloor$, we have $|E'| \leq |E| \cdot \lfloor \frac{\ell}{2}\rfloor$. $\calT'$ satisfies the conditions of~\Cref{theorem:tree-kulkarni}: vectors associated with edges belong to the sets $\widetilde{S}^e$, where $\widetilde{S}^e \subseteq \Ball^d_2$. Applying~\Cref{theorem:tree-kulkarni} for $\calT'$ and $K$ (which also satisfies the conditions of~\Cref{theorem:tree-kulkarni}), there exists a sign $z_e^{(i)} \in \{-1,1\}$ for every $e \in E$ and $i \in [|\widetilde{S}^e|]$ (i.e., a sign for every edge in $E'$), such that, for every $u \in V$, $\sum_{e \in P_u} \sum_{i=1}^{|\widetilde{S}^e|} z_e^{(i)}u_e^{(i)} \in 5 K$ where $P_u$ is the set of edges on the path from the root node of $\calT$ to the node $u$ in $\calT$.

Consider an edge $e \in E$. We round every $y^e_i$ to $\hat{y}^e_i$ as follows: If $I^e = \emptyset$, then $\hat{y}^e_i = y^e_i$ for every $i \in [\ell]$. Otherwise, if $I^e \neq \emptyset$, then for every vector $u^{(i)}_e = \frac{1}{2}( v^e_{i_{2p}} - v^e_{i_{2p+1}} ) \in \widetilde{S}^e$, whose corresponding sign is $z^{(i)}_e$, we set $\hat{y}^e_{i_{2p}} =  y^e_{i_{2p}} + 2^{-k}\cdot z^{(i)}_e$ and $\hat{y}^e_{i_{2p+1}} = y^e_{i_{2p+1}} - 2^{-k}\cdot z^{(i)}_e$; all other $j \in [\ell] \setminus I^e$ are left unupdated, $\hat{y}^e_j = y^e_j$. This specifies a way to round each $y^e_{j}$ to $\hat{y}^e_{j}$.

It remains to show that our rounding procedure $(i)$ preserves feasibility, $(ii)$ sets the $k^{th}$-bit of the fractional part of $\hat{y}^e_i$ to $0$ for all $e$ and $i$, and $(iii)$ does not increase the vector sums of interest by too much: for all $u \in V$, 
$\left( \sum_{e \in P_u} \sum_{i=1}^{\ell} \hat{y}^e_i v^e_i - \sum_{e \in P_u} \sum_{i=1}^{\ell} y^e_i v^e_i  \right) \in 2^{-k} \cdot 10 K$.

Recall that $I^e$ is the set of all indices $i$ where the $k^{th}$ bit of the fractional part of $y^e_{i}$ is $1$. As per the aforementioned rounding process, for all $e \in E$ such that $I^e = \emptyset$, we have $\hat{y}^e_{i} = y^e_{i}$ for all $i \in [\ell]$, hence $(ii)$ holds.
Otherwise, if $I^e \neq \emptyset$, during the rounding we add or subtract $2^{-k}$ from every $y^e_{i_{2p}}$ and $y^e_{i_{2p+1}}$ respectively, where $i_{2p},i_{2p+1} \in I^e$. This addition and subtraction ensures that the $k^{th}$-bits of $y^e_{i_{2p}}$ and $y^e_{i_{2p+1}}$ are zero, additionally, for all $j \in [\ell] \setminus I^e$, $\hat{y}^e_j = y^e_j$, i.e., the $k^{th}$ bit remains zero; $(ii)$ follows. %(and thus, for both $y^e_{i_{2p}}$ $y^e_{i_{2p+1}}$ the $k^{th}$ bit of their fractional part is equal to $1$); therefore $(ii)$ is guaranteed for all such $\hat{y}^e_{j}$. 
Since $\hat{y}^e_{i_{2p}} + \hat{y}^e_{i_{2p+1}} = y^e_{i_{2p}} + y^e_{i_{2p+1}}$, the equality $\sum_{i=1}^{\ell} \hat{y}^e_i = \sum_{i=1}^{\ell} y^e_i = 1$ is maintained. Additionally, $\hat{y}^e_j \geq 0$ for all $e, j$, the feasibility condition $(i)$, also holds.
Finally, for $(iii)$, recall that for all $u \in V$, $\sum_{e \in P_u} \sum_{i=1}^{|\widetilde{S}^e|} z_e^{(i)} u_e^{(i)} = \sum_{e \in P_u} \sum_{i=1}^{|\widetilde{S}^e|} z_e^{(i)} \frac{1}{2} \left(v^e_{i_{2p}} - v^e_{i_{2p+1}}\right) \in 5 K$, by~\Cref{theorem:tree-kulkarni}. Therefore, by our rounding process: $\sum_{e \in P_u} \sum_{i=1}^{\ell} (\hat{y}^e_i -  y^e_i ) \cdot v^e_i \leq \sum_{e \in P_u} \sum_{i=1}^{|\widetilde{S}^e|} 2^{-k} z^{(i)}_e \left(v^e_{i_{2p}} - v^e_{i_{2p+1}}\right) \in 2^{-k} \cdot 10 K$.
\end{proof}








% ======


% We will show that each $x^e_i \in [0,1]$ that we start off with can be assumed to have at most $\log(\frac{\ell\cdot d}{\varepsilon})$ decimal digits where $d$ is the depth of the tree $\calT$ and $\varepsilon>0$ is any constant such that $\gamma_d(K) \geq 1 - \frac{1}{\ell|E|} \geq \frac{1}{2} + \varepsilon$; such an $\varepsilon$ must exist for a large enough tree with $\ell|E| > 2$. Starting from $x^e_i$ if we drop all the decimal digits of $x^e_i$ after the $\log(\frac{\ell \cdot d}{\varepsilon})^{th}$ decimal bit, then the vector sum $\norm{\sum_{e \in P_u} \sum_{i=1}^\ell x^e_i \cdot v^e_i}_2$ will increase by at most $\ell \cdot d 2^{-\log(\frac{\ell\cdot d}{\varepsilon})} = \varepsilon$. Additionally, since $\gamma_d(\widetilde{K}) \geq \frac{1}{2} + \varepsilon$, we know that $\varepsilon \cdot \Ball^d_2 \subseteq \widetilde{K}$. Therefore assuming that $\{x^e_i\}_{i,e}$ has at most $\log(\frac{\ell \cdot d}{\varepsilon})$ decimal digits increases the concerned vectors sums by a vector that is contained in $\widetilde{K}$. 

Given~\Cref{theorem:tree-reduction}, our next task is to show that, given a rooted tree $\calT$ as above, there exists a distribution $\calD$ over vectors (one from each edge set) such that for $x \sim \calD$, $\sum_{e \in P_u} x_e$ is subgaussian, for every node $u \in V$. 


\begin{theorem}\label{theorem:tree-subgaussianity}
    Let $\calT = (V,E)$ be a rooted tree, where every $e \in E$ has an associated set of vectors $S_e \subseteq \Ball^d_2$ satisfying $\mathbf{0} \in \conv(S_e)$. Then there exists a distribution $\calD$ supported on $\bigtimes_{e \in E} S_e$ such that for $x \sim \calD$, $\sum_{e \in P_u} x_e$ is $22.11$-subgaussian for every $u \in V$, where $P_u$ is the set of edges of the path from the root to the node $u$.
\end{theorem}





    
    
    
    %However, this further implies that the following distribution $\calD$, supported over $\bigtimes_{e \in E} S_e$, is $22.11$-subgaussian: first select an element $j \in \{1,2,\ldots, N\}$ uniformly at random, and then get $\{\check{s}_e^{(i)}\}_{e \in E} \in \bigtimes_{e \in E} S_e$, where each $\check{s}_e^{(i)}$ is obtained by projecting ${s}_e^{(i)} = (\mathbf{0}, \mathbf{0}, \ldots, v, \ldots, \mathbf{0}) \in \mathbb{R}^{Nd}$ to $\check{s}_e^{(i)} \coloneqq v \in S_e$. \alex{Not sure what's happening here. Define ``project.'' Also, $j$ is never used. Is $i$ supposed to be $j$??} This concludes the proof.
    %defined as the uniform distribution over the set $\{ \{s^{(1)}_e\}_{e \in E}, \{s^{(2)}_e\}_{e \in E}, \ldots, \{s^{(N)}_e\}_{e \in E}\}$ \alex{I know what you want to say here, but the notation is wrong. $s^{(i)}_e \in \mathbb{R}^{Nd}$. $\calD$ needs to be supported on $\bigtimes_{e \in E} S_e$} is such that, for $x \sim \calD$, $\sum_{e \in P_u} x_e$ is $22.11$-subgaussian, for any node $u \in V$. 


Finally, we prove that there exists an algorithm that, given sets of vectors one at a time, selects a vector from each set such that the vector sum is $O(1)$ subgaussian.

\begin{theorem}\label{theorem:subgauss-algo}
    For every $T, k \in \mathbb{N}$, there exists an online algorithm that, given sets $S_1, S_2, \ldots, S_T \subseteq \Ball^d_2$ satisfying $1 \leq |S_i| \leq k$ and $\mathbf{0} \in \conv(S_i)$, chosen by an oblivious adversary and arriving one at a time, selects a vector $s_i \in S_i$ from each arriving set $S_i$ such that, for every $t \in [T]$, the $\sum_{i=1}^t s_i$ is $23$-subgaussian. %The time and space complexity of this algorithm is $O(T^{CdT})$ and $O(k^{\left(\frac{T}{\delta}\right)^{CdT}})$, respectively, for some constant $C > 0$.
\end{theorem}

\begin{proof}
    Let $\beta = 22.11 = 23-\delta$ be the subgaussianity parameter in the guarantee of~\Cref{theorem:tree-subgaussianity}. Additionally, let $\calW$ be the smallest $\varepsilon$-net of the set $\calS = \cup_{i=1}^k \left( \bigtimes_{j=1}^i \Ball^d_2 \right)$ for $\varepsilon = \frac{\delta}{2T}$; here, $\calS$ represents the set of all subsets $A \subseteq \Ball^d_2$ satisfying $1 \leq |A| \leq k$. From~\Cref{proposition:size-of-net}, we know that $\calW$ has size at most $\sum_{i=1}^k \left(\frac{3}{\varepsilon}\right)^{di} \leq \left(\frac{3}{\varepsilon}\right)^{d(k+1)}$. We consider a complete and full $|\calW|$-ary tree $\calT = (V,E)$ of height $T$, where every internal node $u \in V$ of $\calT$ has $|\calW|$ children, where each edge to a child-node is associated with an element of (or, set in) $\calW$. Let $A_e$ be the set that corresponds to edge $e \in E$ in our construction, where, by the definition of $\calW$, $A_e \subseteq \Ball^d_2$ and $1 \leq |A_e| \leq k$. ~\Cref{theorem:tree-subgaussianity} implies the existence of distribution $\calD$ over $\bigtimes_{e \in E} A_e$  such that for any node $u \in V$, $\sum_{e \in P_u} y_e$ is $\beta$-subgaussian for $y \sim \mathcal{D}$.
    At time $t=0$, we sample an $y \sim \calD$ and start at the root node of $\calT$. We will keep track of a location $p_t \in V$, which at the beginning of time $t$ will be a node at depth $t-1$. A time $t$, when the set $S_t$ arrives, we map it to a set $Y_t \in \argmin_{Z \in \calW \cap \Ball_2^{|S_t|}} \norm{Z - S_t}_2$, i.e., $Y_t$ is an element of $\calW$, with the same number of elements as $S_t$, closest to $S_t$. $Y_t$ corresponds to some edge $e_t$ incident to the current node $p_t$ (that is $A_{e_{t}} = Y_t$). Let $y_{t} \in Y_t$ be the vector corresponding to edge $e_t$ in the sample $y$ from $\calD$ (from time $0$). Given $y_{e_t}$, our algorithm selects vector $x_{t} \in \argmin_{v \in S_t}\norm{v - y_{e_t}}_2$. Overall, at time $t$ we: (i) map set $S_t$ to a set $Y_t$ (or, equivalently, edge $e_t$), (ii) use the sample $y$ (the same across all times) to identify a vector $y_{e_t} \in Y_t$, and finally (iii) map $y_{e_t}$ to a vector in $x_{e_t} \in S_t$; $x_{t}$ is our output in time $t$.
    Finally, we update $p_{t+1}$ to be the child of $p_t$ along edge $e_t$.

    Next, we prove that for all $t \in [T]$, $\sum_{i=1}^t x_t$ is $23$-subgaussian. Since $\calW$ is an $\varepsilon$-net of $\calS = \cup_{i=1}^k \left( \bigtimes_{j=1}^i \Ball^d_2 \right)$, $x_t \in S_t$ (and therefore, $x_t \in \calS$) and $y_{e_t} \in Y_t$ (and therefore $y_{e_t} \in \calW$), we have that that $\norm{x_t - y_{e_t}}_2 \leq \varepsilon$. Furthermore, $y \sim \calD$, and therefore $\sum_{e \in P_u} y_e$ is $\beta$-subgaussian for all $u \in V$. Noticing that $e_{1}, e_{2}, \dots, e_t$ form a path from the root of $\calT$ to some node $u$, we have that $\sum_{i=1}^t y_{e_i}$ is $\beta$-subgaussian, or, equivalently, $\norm{\sum_{i=1}^t y_{e_i}}_{\psi_2, \infty} \leq \beta = 23 - \delta$.
    
    Towards proving subgaussianity for $\sum_{i=1}^t x_i$ we have
    \begin{align*}
        &\norm{\sum_{i=1}^t x_i}_{\psi_2, \infty} \leq \norm{\sum_{i=1}^t y_{e_i}}_{\psi_2, \infty} + \norm{\sum_{i=1}^t x_i - y_{e_i}}_{\psi_2, \infty} \tag{triangle inequality}\\
        & \leq (23 - \delta) + \sum_{i=1}^t \norm{x_i - y_{e_i}}_{\psi_2, \infty} \tag{subgaussianity of $\sum_{i=1}^t y_{e_i}$ and triangle inequality}\\
        & \leq 23 - \delta + T \cdot \sup_{d \in \Sph^{d-1}} \norm{\langle x_i - y_{e_i}, d \rangle}_{\psi_2} \tag{definition of $\norm{.}_{\psi_2, \infty}$}\\
        &= 23 - \delta + T \cdot \sup_{d \in \Sph^{d-1}} \norm{ \, \norm{x_i - y_{e_i}}_2 \cdot \norm{d}_2 \cdot \cos(\theta) }_{\psi_2},
    \end{align*}
    where $\theta$ is the random angle between the vectors $x_i - y_{e_i}$ and $d$.  $\cos(\theta)$ is random variable supported on $[-1,1]$, $\norm{x_i - y_{e_i}}_2 \in [0,\varepsilon]$ and $\norm{d}_2 = 1$. Therefore, $\norm{x_i - y_{e_i}}_2 \cdot \norm{d}_2 \cdot \cos(\theta) \in [-\varepsilon,\varepsilon]$. From the definition of $\norm{.}_{\psi_2}$ norm we therefore have that $\norm{ \, \norm{x_i - y_{e_i}}_2 \cdot \norm{d}_2 \cdot \cos(\theta) }_{\psi_2} \leq \inf\{ t > 0: \E[exp(\varepsilon^2/t^2)] \leq 2 \} \leq 2 \varepsilon$. Our upper bound on $\norm{\sum_{i=1}^t x_i}_{\psi_2, \infty}$ becomes $23 - \delta + T \cdot 2 \varepsilon = 23$. %\alex{@Paritosh: double check the argument. Yours was different}\paritosh{which part in particular should I review? } \alex{Everything after ``Towards proving subgaussianity for...''}\paritosh{This looks good; what I had in mind was pretty much the same argument.}
    %The size of $\calT$ is the number of its edges, which is $|\calW| + |\calW|^2 + \ldots + |\calW|^{T} \leq (2|\calW|)^{T} \leq ( 2 (\frac{6 T}{\delta})^{d(k+1)})^{T} \in O((\frac{2T}{\delta})^{dT})$\alex{why?}, hence, the space complexity is at least this much. The time complexity of the algorithm is dominated by the time required to find the distribution $\calD$ that is guaranteed to exist by~\Cref{theorem:tree-subgaussianity}. Through~\Cref{theorem:tree-subgaussianity}, we know that $\calD$ is supported on $N = \left\lfloor \left((d+1)c_\delta^d|E|\right)^{1/\delta}\right\rfloor$ elements, where each element belongs to $\varprod_{e \in E} S_e$. Therefere, the number of such distributions are at most $k^{N |E|} = O(k^{\left(\frac{T}{\delta}\right)^{dT}})$.
\end{proof}



\subsection{Optimal online multicolor discrepancy}\label{subsec: optimal multicolor discrepancy}

Here, we prove our main result for this section, an optimal algorithm for online multicolor discrepancy. We start by giving an algorithm for \emph{weighted} online vector balancing.

\begin{lemma}\label{lemma:weighted-vector-balancing}
     For every $\alpha \in \left[\frac{1}{2}, \frac{2}{3}\right]$ and $T \in \mathbb{N}$, there exists an online algorithm that, given vectors $v_1, v_2, \ldots, v_T \in \Ball^d_2$ chosen by an oblivious adversary and arriving one at a time, assigns to each vector $v_i$ a weight $w_i \in \{1-\alpha, -\alpha\}$, such that, with probability at least $1 - \delta$, for any $\delta \in (0,1/2]$, for all $t \in [T]$, $\norm{\sum_{i=1}^t w_iv_i}_\infty \leq \sqrt{\log(T)} + \sqrt{\log(2/ \delta)}$.
\end{lemma}

Given an algorithm for weighted online vector balancing, we give an algorithm for online multicolor discrepancy: we construct a binary tree, where the leaves correspond to colors, and the internal nodes execute the weighted online vector balancing algorithm. We note that this trick has been used in the same context in previous work~\cite{alweiss2021discrepancy,bansal2021online}.

\begin{theorem}\label{thm: multi color main upper bound}
    For every $T \in \mathbb{N}$, there exists an online algorithm that, given vectors $v_1, v_2, \ldots, v_T \in \Ball^d_2$ chosen by an oblivious adversary and arriving one at a time, assigns each arriving vector $v_i$ to one of $n$ colors such that, with probability at least $1-\delta$, for any $\delta \in (0,1/2]$, for all $t \in [T]$,
    \[\max_{i,j \in [n]} \norm{\sum_{v \in \calC^t_i} v - \sum_{v \in \calC^t_j} v}_\infty \leq 6 \left( \sqrt{\log(T)} + \sqrt{\log(2/\delta)} \right) \]
 where $\calC^t_i$ is the set of all vectors that got assigned color $i \in [n]$ up to time $t \in [T]$.
\end{theorem}
\begin{proof}
    Let $\calA_\alpha$ be the algorithm of~\Cref{lemma:weighted-vector-balancing}, for an $\alpha \in [1/2, 2/3]$. We recursively construct a binary tree with $n$ leaves, corresponding to the $n$ colors. For a tree with $k > 1$ leaves we add a root node, as its left subtree recursively construct a tree with $\lceil k/2\rceil$ leaves, and as its right subtree recursively construct a tree with $\lfloor k/2\rfloor$ leaves; for $k=1$, we simply have a leaf. 
    
    Given vectors, one at a time, our algorithm for the online multicolor discrepancy problem decides which set/color a vector gets by repeatedly running $\calA_\alpha$ for the online vector balancing problem at each internal node of the tree. Specifically, at an internal node with $k$ descendent leaves, we will run a copy of $\calA_\alpha$ by setting $\alpha = \lceil k/2\rceil/k$, and by recursively passing the vectors that are assigned $1-\alpha$ (resp. $-\alpha$) to the left (resp. right) subtree (until they reach the leaves). Note that for any $k \geq 1$, we have $\alpha = \lceil k/2\rceil/k \in [1/2, 2/3]$. Vectors are assigned the color of the leaf they reach.

    Let $p_e$ be a weight for each edge $e$: the edge between the left (resp. right) child of an internal node with $k$ children has a weight $p_e = \alpha = \lceil k/2\rceil/k$ (resp. $p_e = 1 - \alpha$). The weights on the edges of an internal node is the ``opposite'' with respect to the weight of its children in the execution of $\calA_\alpha$. Intuitively, $p_e$ for an edge $(u,v)$ is the expected fraction of vectors that go to node $v$, out of the vectors that arrive at the parent node $u$.

    There are $n-1$ internal nodes in our tree. Let $\mathcal{E}$ be the event that all $n-1$ executions of $\calA_\alpha$ have maintained the discrepancy at most $\sqrt{\log(T)} + \sqrt{\log(2/ \delta)}$ between the corresponding two children nodes; $\mathcal{E}$ occurs with probability at least $1 - (n-1)\delta$. Let $S^{sum} = \sum_{i=1}^t v_i$ be the sum of all vectors until time $t$, and let $S_u$ be the sum of all vectors that have passed through node $u$ until time $t$ (so, $S_r = S^{sum}$ for the root node $r$). Also, let $\pi_u = \Pi_{e \in P_u} p_e$, for a node $u$; intuitively, $\pi_u$ is the (expected) fraction of vectors (out of $\{ v_1, \dots, v_t \}$) that arrive at node $u$.    

    We will prove, via induction on $\ell$, that conditioned on $\mathcal{E}$, for all nodes $u$ on level $\ell \leq 0$ we have
\[
\norm{ S_u - \pi_u S^{sum}}_{\infty} \leq 3 \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right).
\]

For $\ell = 0$ the statement trivially holds: for the root $r$ at level zero we have $\norm{ S_r - \pi_r S^{sum}}_{\infty} = 0 \leq 3 \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right)$. Suppose that the statement holds for level $\ell$, and let $u$ be a node in level $\ell + 1$, with parent node $p$ (on level $\ell$) and sibling node $v$ (on level $\ell + 1$). Assume that $u$ is the left child of $p$ (the other case is identical). We have that $S_p = S_u + S_v$, and $\pi_u = \pi_p \cdot \alpha$. Also, conditioned on $\mathcal{E}$ we have $\norm{ (1-\alpha)S_u - \alpha S_v}_{\infty} \leq \sqrt{\log(T)} + \sqrt{\log(2/ \delta)}$. So, overall:
\begin{align*}
    &\norm{ S_u - \pi_u S^{sum}}_{\infty} = \norm{ (1-\alpha)S_u + \alpha S_u - \alpha \pi_p S^{sum}}_{\infty} \\
    &\leq \norm{ (1-\alpha)S_u - \alpha S_v }_{\infty} + \norm{ \alpha S_v + \alpha S_u - \alpha \pi_p S^{sum}}_{\infty} \tag{triangle inequality}\\
    &= \norm{ (1-\alpha)S_u - \alpha S_v }_{\infty} + \alpha \norm{ S_p - \pi_p S^{sum}}_{\infty} \\
    &\leq \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right) + \frac{2}{3} \, 3 \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right) \\
    &\leq 3 \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right).
\end{align*}

We will also prove, via induction on $k$, that for a node $u$ that is the root of a subtree with $n-k+1$ leaves, $\pi_u = (n-k+1) \cdot \frac{1}{n}$. For the root $r$ (whose subtree has $n = n-1+1$ leaves) we have $\pi_r = 1 = n \cdot \frac{1}{n}$. Consider a node $u$ that is the left child of a node $p$, such that $p$ is the root of a subtree with $k$ leaves. Then $u$ is the root of a subtree with $\lceil k/2\rceil$ leaves, and $\pi_u = \pi_p \cdot \alpha = k \, \frac{1}{n} \cdot \lceil k/2\rceil/k = \lceil k/2\rceil \cdot \frac{1}{n}$; the case that $u$ is the right child of $p$
 is identical.

Finally, consider two arbitrary leaves $v_1$ and $v_2$. From the previous arguments we have that $\pi_{v_1} = \pi_{v_2} = \frac{1}{n}$, and $\norm{ S_{v_i} - \frac{1}{n} \, S^{sum}}_{\infty} \leq 3 \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right) $. Therefore, $\norm{ S_{v_1} - S_{v_2}}_{\infty} \leq 6 \left( \sqrt{\log(T)} + \sqrt{\log(2/ \delta)} \right)$.
\end{proof}

The lower bound for the online envy minimization problem in the next section implies that the bound of~\Cref{thm: multi color main upper bound} is optimal.

% \begin{theorem}[\cite{kulkarni2024optimal}]
%     For any $d \geq 2$, there is a strategy for an oblivious adversary that yields a sequence of vectors $v_1, \dots, v_T \in \mathbb{R}^d$ so that for any online algorithm that picks $x_t \in \{ -1 , 1 \}$ at every step $t$, with probability at least $1 - 2^{-T^{\Omega(1)}}$, one has $\max_{t \in [T]} \norm{\sum_{i=1}^t x_i v_i}_{\infty} \in \Omega(\sqrt{ \log T})$.
% \end{theorem}
    
    
    % ======


    % Define for any internal node $u$, $\pi_u = \Pi_{e \in P_u} p_e$ and for the root node $r$, $\pi_r = 1$. For the subsequent analysis, we 
    % will condition on the ``good event," i.e., the event happening with probability $1 - (n-1) \delta$ where $\calA$ running at each internal node given the guarantee mentioned in~\Cref{corollary:weighted-vector-balancing}.
    % Conditioned on this event, we consider a time $t \in [T]$ during the execution of the algorithm. Let $S^* = \sum_{i=1}^t v_i$ denote the sum of all vectors till time $t$ and for any node $u \in V$, denote by $S_u$ the sum of all vectors that pass through that node during the execution of the algorithm till time $t$; indeed, we have $S^* = S_r$ where $r$ is the root node. To prove~\Cref{equation:multicolor-reduction-guarantee}, we will prove the following two claims.
    % \begin{enumerate}
    %     \item [(a)] With probability at least $1-(n-1)\delta$, for any node $u$ in the tree, we have $\norm{S_u - \pi_u \cdot S^*}_2 \leq 3 f(T,\delta)$.
    %     \item [(b)] For all leaves $\ell$, we have $\pi_\ell = \frac{1}{n}$.
    % \end{enumerate}
    % \Cref{equation:multicolor-reduction-guarantee} follows from the above two claims as with probability at least $1-(n-1)\delta$ (and for any $t \in [T]$) for any two leaves $\ell_1$ and $\ell_2$, we have $\norm{S_{\ell_1} - S_{\ell_2}}_2 = \norm{(S_{\ell_1} - 1/n \cdot S^*) - (S_{\ell_2} - 1/n \cdot S^*)}_2 \leq^{(b)} \norm{S_{\ell_1} - p_{\ell_1} S^*}_2  + \norm{S_{\ell_2} - p_{\ell_2} S^*}_2 \leq^{(a)} 6 \cdot f(T,\delta)$. The obtained inequality $\norm{S_{\ell_1} - S_{\ell_2}}_2 \leq 6 \cdot f(T,\delta)$ is precisely same as~\Cref{equation:multicolor-reduction-guarantee} and thus plugging in $f(T,\delta) = \sqrt{\log(T)} + \sqrt{\log(1/ \delta)}$ from~\Cref{corollary:weighted-vector-balancing} will complete the proof. Henceforth, we will prove $(1)$ and $(2)$. The claim $(2)$ directly follows from the definition of $\pi_u$s and the structure of the tree: the edge $e$ going from an internal node having $k$ leaves to its left subtree having $\lceil k/2 \rceil$ leaves has weight equal to $p_e = \lceil k/2 \rceil / k$ and thus for any internal node $u$, $\pi_u$ equals to the number of its descendent leaves (including itself) times $1/n$, i.e., for a leaf $\ell$, $\pi_\ell = 1/n$. 
    
    % To prove $(2)$, we will consider an inductive argument. As the base case, for the root $r$, we trivially have $\norm{S_r - \pi_r \cdot S^*}_2 =  \norm{S^* - 1 \cdot S^*}_2  = 0 \leq 3 \cdot d(t)$. For the induction step, assume that for an internal node $u$, we have $\norm{S_u - \pi_u \cdot S^*}_2 \leq 3 \cdot f(T,\delta)$ and the left (right) child of $u$ is node $l$ ($r$). Since we are operating a copy of $\calA$ with parameter $\alpha$ (equal to the proportion of leaves in the left subtree), we have the guarantee $\norm{(1-\alpha) S_l - \alpha S_r}_2 \leq f(T,\delta)$. Along with the fact that $S_u = S_l + S_r$, we get that $\norm{S_l - \alpha S_u}_2 = \norm{S_l - \alpha (S_r + S_l)}_2 = \norm{(1-\alpha) S_l - \alpha S_r}_2 \leq f(T,\delta)$. Adding this inequality with the inequality $\norm{S_u - \pi_u \cdot S^*}_2 \leq 3 \cdot f(T,\delta)$ with $\alpha$ multiplied by $\alpha$ gives us,

    % \begin{align*}
    %     3 \cdot f(T, \delta) & \geq \alpha \cdot 3f(T, \delta) + f(T,\delta) \tag{$2/3 \geq \alpha$}\\
    %     & \geq \alpha \norm{S_u - p_u \cdot S^*}_2 + \norm{S_l - \alpha \cdot S_u}_2\\
    %     & \geq \norm{S_l - \alpha p_u \cdot S^*}_2 \\
    %     & = \norm{S_l - p_l \cdot S^*}_2,
    % \end{align*}
    % where the last equality follows from the fact that $\pi_l = \alpha p_u$. This proves the claim for node $l$ and a similar argument establishes the same for $r$, i.e., $3 \cdot f(T, \delta) \geq \norm{S_r - p_r \cdot S^*}_2$. This concludes the induction step and hence the proof





% \subsection{Alternative Reduction}

% \alex{what is this?? a proof for the corollary above?}\paritosh{From Theorem 4 we get our main results via Lemma 1 and Theorem 5; this is an alternative that we can use to get Theorem 5 from Theorem 4.}\alex{You mean our main result from Thm 4? If this is a proof of Thm 5 I don't see why it's phrased in the fair division language.}
% Each edge corresponds to an item that is valued as $(v_1, \ldots, v_n)$. We must choose one of $n$ ``vectors'' $\{\mathbf{u}^1, \ldots, \mathbf{u}^n\}$ where $\mathbf{u}^i$ that corresponds to placing the item in bundle $i$ (although think of these as anonymous). Our goal is to create an almost perfect partition where all agents value all bundles about the same. To track this, each $\mathbf{u}^i$ vector has $n \cdot (n - 1)$ dimensions, i.e., there is a value $u^i_{jk}$ for all $k \ne i$ and $i, j, k \in [n]$. The coordinate $u^i_{jk}$ tracks $V_j(A_k) - V_j(A_j)$. Therefore,
% \[
% u^i_{jk} = 
%     \begin{cases}
%         v_j &\text{ if } k = i, j\ne i\\
%         -v_j & \text{ if } j = i, k \ne i\\
%         0 & \text{ otherwise}.
%     \end{cases}
% \]
% Note that $\mathbf{0}$ is in the convex hull because $\sum_{i = 1}^n \mathbf{u}^i/n  = 0$, positive for exactly one vector and negative for exactly one.

% If we've chosen a sequence of vectors such that all coordinates are small, this means that we have found an almost perfect partition. 





\subsection{Optimal online envy minimization}\label{subsec: envy for oblivious}


\Cref{thm: multi color main upper bound} immediately implies, for the online envy minimization problem, a $O_n(\sqrt{\log{T}})$ upper bound against an oblivious adversary.

\begin{corollary}\label{cor: main result for oblivious and fair division}
    For any $n \geq 2, T \geq 1$ and $\delta \in (0,1/2]$, there exists an online algorithm that, given a sequence of $T$ items with $v_{i,t} \in [0,1]$ for all $i \in [n]$ and $t \in [T]$ selected by an oblivious adversary and arriving one at a time, allocates each item to an agent such that the envy between any pair of agents $i,j \in [n]$ satisfies, $\envy^t_{i,j} \in O_n(\sqrt{\log{T}})$ with probability at least $1 - \frac{1}{T^c}$, for any constant $c$.
\end{corollary}


Here, we prove a lower bound of $\Omega_n((\log(T))^{r/2})$, for all $r < 1$, for the online envy minimization problem, against an oblivious adversary. Our proof crucially uses the construction in the lower bound of~ \cite{benade2024fair} for the online envy minimization problem, against an adaptive adversary; for completeness, we include a proof of this result in~\Cref{app:proof from OR paper}.


\begin{theorem}[Theorem 2 of \cite{benade2024fair} restated]\label{theorem:adaptive-lb}
    For any $n \geq 2$, $r < 1$ and $T \geq 1$, there exists a set $S_T$ of instances with $|S_T|  \leq 2^T$ such that for any online algorithm $\mathcal{A}$, there exists an instance $I \in [0,1]^{n\cdot m}$ in $S_T$ such that running algorithm $\mathcal{A}$ on the sequence of items $1,2 \ldots, T$ described by $I$ results in a maximum envy of at least $\envy^T \in \Omega_n(T^{r/2})$ at time $T$.
\end{theorem}

Our result is stated as follows.



\begin{theorem}\label{thm: envy lower bound for oblivious}
    Fix any $n \geq 2$, $T \geq 1$, and $r \in (0,1)$. Let $\mathcal{A}$ be a (possibly randomized) online algorithm. There exists an oblivious adversary that can select a sequence of $T$ items such that the allocation $A^T$ constructed by $\mathcal{A}$ has $\envy^T \in \Omega_n((\log(T))^{r/2})$ with probability at least $1/T$.
\end{theorem}
\begin{proof}
    By Yao's minimax principle, we can, without loss of generality, focus on deterministic algorithms $\mathcal{A}$ and an adversary that selects distributions over instances. We will construct a distribution $\mathcal{D}$ over instances, such that any deterministic algorithm $\mathcal{A}$ has $\envy^T \in \Omega_n((\log(T))^{r/2})$ with probability at least $1/T$, where the randomness is over instances drawn from $\mathcal{D}$. $\mathcal{D}$ is defined as follows: for a fixed $T$, consider the set of instances $S_{\log{T}}$ described in~\Cref{theorem:adaptive-lb}, and select an instance uniformly at random from this set. This gives us a sequence of $\log{T}$ items; to get to $T$ items, include $T - \log{T}$ items zero value for all the agents. Note that, by definition, $S_{\log{T}}$ contains an instance $I^*$ for which algorithm $\mathcal{A}$ incurs an maximum envy of $\Omega_n((\log(T))^{r/2})$ at time $\log(T)$, and therefore at time $T$ as well, since all items after step $\log{T}$ have zero value. $\mathcal{D}$ samples $I^*$ with probability exactly $1/|S_{\log{T}}|$, which is at least $1/2^{\log{T}} = 1/T$, by~\Cref{theorem:adaptive-lb}.
\end{proof}
