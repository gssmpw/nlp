\section{Preliminaries}\label{sec:prelims}

Throughout the paper we will use $\Sph^{d-1} \defeq \{ x \in \mathbb{R}^d : \norm{x}_2 = 1 \}$ to denote the Euclidean sphere in $\mathbb{R}^d$ and $\Ball^d_2 \vcentcolon= \{ x \in \mathbb{R}^d : \norm{x}_2 \leq 1 \}$ for the Euclidean ball in $\mathbb{R}^d$. 

\paragraph{Online multicolor discrepancy.} 

In the online multicolor discrepancy problem, there is a set of $T$ vectors, $v_1, v_2, \dots, v_T \in \Ball^d_2$. At each time step $t \in [T]$, vector $v_t$ arrives and must be immediately and irrevocably assigned to one of $n$ colors. Our algorithms learn $v_t$ at step $t$. Let $S^t_i$ be the set of vectors that are assigned color $i$ up until time $t$. The \emph{discrepancy} at time $t$ is defined as $\max_{i,j \in [n]} \norm{ \sum_{v \in S^t_i} v - \sum_{v \in S^t_j} }_{\infty}$.
Our goal in the online multicolor discrepancy problem is to minimize the maximum discrepancy over all time steps, that is, minimize $$\max_{t \in [T]} \max_{i,j \in [n]} \norm{ \sum_{v \in S^t_i} v - \sum_{v \in S^t_j} v }_{\infty}.$$ The online vector balancing problem is the special case of the multicolor discrepancy problem where $n=2$.

\paragraph{Online envy minimization.} 

In the online envy minimization problem, there is a set of $T$ indivisible items (also referred to as goods) to be allocated to a set of $n$ agents $\agents$. At each time step $t \in [T]$, item $g_t$ arrives and must be immediately and irrevocably allocated to one of the agents in $\agents$. Item $g_t$ has an associated vector $v_t \in [0,1]^n$ such that $v_t = (v_{1,t}, v_{2,t}, \ldots, v_{n,t})$ where $v_{i,t}$ denotes agent $i$'s value for item $g_t$. Our algorithms learn $v_t$ at step $t$. Let $A^t = (A^t_1, A^t_2, \ldots, A^t_n)$ be the allocation at the end of time $t$ (i.e., after $g_t$ has been allocated) such that for each $i \in \agents$, we have $A^t_i \subseteq \{g_1, g_2, \ldots, g_t\}$, $\cup_{i\in \agents} A^t_i = 
\{g_1, g_2, \ldots, g_t\}$, and $A^t_i \cap A^t_j = \emptyset$ for each $i \neq j$. The value of agent $i \in \agents$ for any set of items (also referred as a \emph{bundle}) $S \subseteq \{g_1, g_2, \ldots, g_T\}$ is denoted by $v_i(S) \coloneqq \sum_{g_k \in S} v_{i,k}$, i.e., the preferences of agents are \emph{additive}.

At each timestep $t$, and for any pair of agents $i,j \in \agents$, we define $\envy^t_{i,j}(A^t) \coloneqq v_i(A^t_j) - v_i(A^t_i)$ to be the difference in value, with respect to agent $i$'s preferences, between the bundle of agent $j$ at time $t$ and the bundle of agent $i$ at time $t$. The maximum envy at time $t \leq T$ is denoted by $\envy^t(A^t) = \max_{i,j \in [n]} \ \envy^t_{i,j}(A^t)$. Our goal in the online envy minimization problem is to minimize the maximum envy at time $T$, that is, minimize $\envy^T = \envy^T(A^T)$. 

\paragraph{Adversary models.} 
For both problems, our results crucially depend on how the input (vectors/agents' values) are generated. It will be convenient to think of a game between an adversary, who picks the input, and the designer, who picks an online algorithm. 


First, we consider an \textbf{oblivious adversary}.
In the online multicolor discrepancy problem, given a number of colors $n$, a number of vectors $T$, and a dimension $d$, the designer picks a (possibly randomized) algorithm. Then, an oblivious adversary, who knows the algorithm's ``code,'' but does not have access to any of the randomness the algorithm uses, selects all $T$ vectors, which are presented to the algorithm one at a time, in the order the oblivious adversary selected at the start of the game.
In the online envy minimization problem, given a number of agents $n$ and a number of items $T$, the designer picks a (possibly randomized) algorithm. Then, an oblivious adversary, who knows the algorithm's ``code,'' but does not have access to any of the randomness the algorithm uses, selects the agents' values for all $T$ items, which are presented to the algorithm one at a time, in the order the oblivious adversary selected at the start of the game.
 

Second, we consider an \textbf{i.i.d.\@ adversary}.
In the online multicolor discrepancy problem, given a number of colors $n$, a number of vectors $T$, and a dimension $d$, a stochastic adversary specifies a distribution $\dist$, supported on $[-1,1]$. The designer, who knows this distribution, then selects a (possibly randomized) algorithm. In every round $t$, the vector $v_t$ is generated by sampling each of its coordinates (independently) from $\dist$, i.e., $v_{t,j} \sim \dist$ for all $j \in [d]$. In the online envy minimization problem, given a number of agents $n$ and a number of items $T$, the stochastic adversary specifies a distribution $\dist$, supported on $[0,1]$. The designer, who knows this distribution, then selects a (possibly randomized) algorithm. In every round $t$, the value of item $t$ for each agent $i$ is drawn independently from $\dist$, i.e., $v_{i,t} \sim \dist$.



% \alex{I already have text for this from my survey. no need to do this part now
% }\paritosh{what about discrepancy prelims? $\gamma_d(\cdot), Conv(\cdot)$, etc.?}


\subsection{Other technical preliminaries}

\paragraph{Geometry definitions.} 


A body $K \subseteq \mathbb{R}^d$ is \emph{convex} if and only if $x, y \in K$ implies that $\alpha x + (1-\alpha) y \in K$ for any $\alpha \in [0,1]$. A body $K\subseteq \mathbb{R}^d$ is \emph{symmetric} if and only if $x \in K$ implies that $-x \in K$ as well. We use $\mathbf{0} \in \mathbb{R}^d$ to denote the origin. The \emph{Gaussian measure} $\gamma_d(K)$ for a body $K \subseteq \mathbb{R}^d$ is defined as $\gamma_d(K) \coloneqq \Pr_{v \sim \mathcal{N}(\mathbf{0}, \mathbf{I}^d)}[v \in K]$ and denotes the probability that a random vector $v$, drawn from the standard Gaussian $\mathcal{N}(\mathbf{0}, \mathbf{I}^d)$ in $\mathbb{R}^d$, is in the body $K$.  We use the following result about convex bodies \cite{ball1997elementary}; the proof is included in~\Cref{app: missing from prelims} for completeness.

\begin{proposition}\label{proposition:large-body-ball}
    If $\gamma_d(K) \geq 1/2 + \varepsilon$ for a convex body $K \subseteq \mathbb{R}^d$, for some $\varepsilon > 0$, then $\varepsilon \, \Ball^d_2 \subseteq K$.
\end{proposition}

Given a set $S \subseteq \mathbb{R}^d$, we will use $\conv(S) \coloneqq \{v : v = \sum_{u \in S} u \cdot x_u \text{, } \sum_{u \in S} x_u = 1 \text{, and } x_u \geq 0 \text{ for all } u \in S\}$ to denote the convex hull of $S$. Given a set $A \subseteq \mathbb{R}^d$, a set $W \subseteq A$ is called an $\varepsilon$-net of $S$ if for all $x \in A$ there is a $y \in W$ such that $\norm{x - y}_2 \leq \varepsilon$. For any $\varepsilon \in (0,1]$, there exists an $\varepsilon$-net of $\Ball^d_2$ of size at most $\left( \frac{3}{\varepsilon} \right)^{d}$; see \cite{kulkarni2024optimal} for a proof. We will use the following analogous statement for $\bigtimes_{i=1}^k \Ball^d_2$.

\begin{proposition}\label{proposition:size-of-net}
    For any $\varepsilon \in (0,1]$, there exists an $\varepsilon$-net of $\bigtimes_{i=1}^k \Ball^d_2$ that is of size at most $\left(\frac{3}{\varepsilon}\right)^{dk}$.
\end{proposition}
\begin{proof}
    There is a natural bijection $f:\Ball^{dk}_2 \rightarrow  \bigtimes_{i=1}^k \Ball^d_2 $ where each $z = (z_{1,1}, \ldots, z_{1,d}, z_{2,1}, \ldots, z_{2,d} \allowbreak \ldots, z_{k,1}, \ldots, z_{k,d})\in \Ball^{dk}_2$ corresponds to $f(z) = ((z_{1,1}, \ldots, z_{1,d}), (z_{2,1}, \ldots, z_{2,d}), \ldots,\allowbreak (z_{k,1}, \ldots, z_{k,d})) \in \bigtimes_{i=1}^k \Ball^d_2$. This bijection preserves distances: for any $z,z' \in \Ball^{dk}_2$, we have $\norm{z - z'}_2 = k$ if and only if $\norm{f(z) - f(z')}_2 = k$.\footnote{With a slight abuse in notation we use  $\norm{f(z) - f(z')}_2$ to denote $\sum_{i=1}^k \norm{f(z)_i - f(z')_i}_2$ where $f(z) = (f(z)_1, f(z)_2, \ldots, f(z)_k) \in \bigtimes_{i=1}^k \Ball^d_2$ and $f(z') = (f(z')_1, f(z')_2, \ldots, f(z')_k) \in \bigtimes_{i=1}^k \Ball^d_2$.} Hence an $\varepsilon$-net of $\Ball^{dk}_2$, via $f$, corresponds to an $\varepsilon$-net of $\bigtimes_{i=1}^k \Ball^d_2$. Since $\Ball^{dk}_2$ has an $\varepsilon$-net of size at most $\left(\frac{3}{\varepsilon}\right)^{dk}$ (by the result of~\cite{kulkarni2024optimal}).
\end{proof}

\paragraph{Subgaussian distributions.}



\begin{definition}[Subgaussian norm]
    For a real-valued random variable $X$, the subgaussian norm is defined as $\norm{X}_{\psi_2} \defeq \inf\{ t > 0: \E[\exp(X^2/t^2)] \leq 2 \}$. For a random vector $Y$ taking values in $\mathbb{R}^d$, $\norm{Y}_{\psi_2, \infty} \defeq \sup_{w \in \Sph^{d-1}} \norm{ \langle\, Y , w \rangle }_{\psi_2}$.
\end{definition}

We say that a random vector $Y$ taking values in $\mathbb{R}^d$ 
is $\beta$-subgaussian if $\norm{Y}_{\psi_2, \infty} \leq \beta$. We use the following two properties of norms: (i) for a random vector $Y$ taking values in $\mathbb{R}^d$ and any $k \geq 0$, we have $\norm{k Y}_{\psi_2, \infty} = k \norm{Y}_{\psi_2, \infty}$ (homogeneity); (ii) for random vectors $Y$ and $Z$ in $\mathbb{R}^d$, we have $\norm{ X+ Y }_{\psi_2, \infty} \leq \norm{X}_{\psi_2, \infty} + \norm{Y}_{\psi_2, \infty}$ (triangle inequality). We also use the following proposition.

\begin{proposition}\label{prop: subgaussian means small tail}
    For a random vector $X$ taking values in $\mathbb{R}^d$, if $\norm{X}_{\psi_2, \infty} \leq C$, then, for all $\ell > 0$, $\Pr[ \norm{X}_2 \geq C \sqrt{ \log(\ell) } ] \leq 2/\ell$.
\end{proposition}


\begin{proof}
    By the definition of $\norm{.}_{\psi_2, \infty}$ we have that, for all $w \in \Sph^{d-1}$,  $\norm{ \langle\, X , w \rangle}_{\psi_2} \leq C$.  By the definition of $\norm{.}_{\psi_2}$, this in turn implies that for all $w \in \Sph^{d-1}$, $\E[\exp(\left( \langle\, X , w \rangle \right)^2/C^2)] \leq 2$. Using Markov's inequality we therefore have that for all $w \in \Sph^{d-1}$ and $\ell > 0$, $\Pr[ e^{\left( \langle\, X , w \rangle \right)^2/C^2} \geq \ell ] \cdot \ell \leq 2$, or, equivalently, $\Pr[ | \langle\, X , w \rangle | \geq C \sqrt{ \ln(\ell) } ] \leq \frac{2}{\ell}$, or
    \[
    \Pr\left[ \norm{X}_2 | \cos(\theta) | \geq C \sqrt{ \ln(\ell) } \right] \leq \frac{2}{\ell},
    \]
    where $\theta$ is the (random) angle between $X$ and $w$. Since $| \cos(\theta) | \leq 1$, the proposition follows.
\end{proof}

% \begin{proposition}[cite cite cite]
% Dfn subgaussian random variable
% \end{proposition}

% \alex{add all the necessary definitions/context to get to the following}\paritosh{I am writing these along with the norm properties we use later.}

% For a random vector $X$ taking values in $\mathbb{R}^d$ we write that $X$ is $t$-subgaussian if $\norm{X}_{\psi_2, \infty} \leq t$.