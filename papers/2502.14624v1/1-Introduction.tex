\section{Introduction}

We consider the fundamental problem of fairly allocating indivisible items that arrive sequentially over time to agents with additive preferences. At each time step $t$, an item $g_t$ arrives and must be allocated immediately and irrevocably to one of $n$ agents. Each agent $i$ has value $v_{i,t} \in [0,1]$ for $g_t$, which is revealed only upon the item's arrival. Our objective is to ensure that the final allocation is fair, which we measure through the notion of \emph{envy-freeness}. Concretely, our goal is to minimize $\max_{i,j \in [n]} \left( \sum_{t \in A_j^T} v_{i,t} - \sum_{t \in A_i^T} v_{i,t} \right)$, where $A_{\ell}^T$ is the allocation of agent $\ell$ after all $T$ items have been allocated. 
The canonical motivation for this problem is that of food banks~\cite{aleksandrov2015online,lee2019webuildai,mertzanidis2024}, which receive food donations and allocate them to non-profits (e.g., food pantries). Ensuring that no organization is significantly disadvantaged relative to others is a natural challenge in such settings.


A crucial question in this problem is what assumptions can we reasonably make about the items' values? Different assumptions lead to vastly different fairness guarantees.

At one extreme, many works assume that the vector of values for $g_t$, $(v_{1, t}, \ldots, v_{n, t})$, is drawn independently from a fixed distribution $\mathcal{D}$. This assumption allows for strong positive results: If $T$ is sufficiently large, with high probability, we can find allocations that are completely envy-free, achieving a maximum of at most $0$. However, this setting has notable downsides. First, in many practical scenarios, $T$ may need to be quite large before these guarantees become meaningful. Furthermore, the distributional assumption rules out natural hard instances---such as a single high-value item that agents agree is better than all others combined---where envy-freeness is fundamentally unattainable.

At the other extreme, we can consider fully adversarial settings, where item values are chosen \emph{adaptively}: an adversary, observing the allocation algorithm and all allocation decisions made so far, selects each item's values in order to maximize envy at the end. Prior work has shown that in such a setting, any online algorithm must incur envy at least $\Omega(\sqrt{T})$, even when there are only $n=2$ agents~\cite{benade2024fair}. While this model captures the absolute worst-case scenario, it may be overly pessimistic for many real-world applications. 

In this paper, we explore intermediate adversaries. One particularly natural, yet robust, assumption is that of an \emph{oblivious adversary}, which selects worst-case item values in advance, knowing only the algorithm, and not depending on the specific allocation choices made in previous rounds. To better understand this problem, it is useful to consider the closely related \emph{online vector balancing} problem, introduced by Spencer nearly half a century ago~\cite{spencer1977balancing}, and its generalization, \emph{online multicolor discrepancy}.

In online vector balancing, at each time step $t$, a vector $v_t$ with $\norm{v_t}_2 \leq 1$ arrives and must be assigned immediately and irrevocably to one of two bundles. The key object of study is the \emph{discrepancy vector at time $t$}: $d_t := \sum_{v \in S_1^t} v - \sum_{v \in S_2^t} v$ where $S_1^t$ and $S_2^t$ be the set of vectors assigned to each bundle after $t$ steps. The goal is to keep the $\ell_{\infty}$-norm of all $d_t$s as small as possible.\footnote{The classic problem is more frequently formalized as choosing a sign $\chi_t \in \{-1, 1\}$, and setting $d_t := \sum_{i = 1}^t \chi_i v_i$. Our formulation is equivalent, using notation more consistent with envy-minimization.} That is, find the smallest $B$ for which $\max_{t \in [T]} \norm{ d_t }_{\infty} \leq B$ (or, if randomness is involved, given a $\delta$, the smallest $B$ for which this holds with probability $1 - \delta$).

In online multicolor discrepancy, instead of \emph{two} bundles, there are $n$ bundles~\cite{bansal2021online}. At each time step $t$, a vector $v_t \in \mathbb{R}^d$ with $\norm{v_t}_2 \le 1$ arrives and must be assigned one of $n$ bundles (often framed as assigning the vector one of $n$ colors). The goal is to minimize the maximum discrepancy between any pair of bundles, defined as $\max_{t \in [T], i,j \in [n]} \norm{ \sum_{z \in S^t_{i}} z - \sum_{z \in S^t_{j}} z }_{\infty}$ where $S_i^t$ denotes the set of vectors assigned to bundle $i$ after $t$ steps. Online vector balancing corresponds to the special case of $n = 2$.

These problems are particularly useful because algorithms for online multicolor discrepancy can be directly applied to envy-minimization: the item values $(v_{1, t}, \ldots, v_{n, t})$ can be treated as input vectors to a discrepancy algorithm, and the resulting envy is upper bounded by the discrepancy.\footnote{Since envy-minimization values are only bounded in $[0, 1]$, the $\ell_2$-norm may be as large as $\sqrt{n}$, increasing the bounds by a factor of $\sqrt{n}$. However, since we primarily focus on bounds as a function of $T$, this detail is less critical.}  Additionally, algorithms for online vector balancing can be used for envy-minimization when $n = 2$.
However, discrepancy problems are generally more challenging than envy-minimization for four key reasons: (i) the discrepancy must remain small at all time steps $t$, not just at time $T$, (ii) input vectors in discrepancy problems may have negative entries, whereas item values in envy minimization are nonnegative, (iii) discrepancy requires bounding the $\ell_{\infty}$ norm, whereas in envy minimization, very negative envy is allowed,\footnote{In fact, a consequence of this is that positive results for online multicolor discrepancy give online algorithms for computing a near-perfect allocation (a perfect allocation $A$ satisfies $v_i(A_j) = v_{i}([m])/n$ for all agents $i,j$), a problem harder than computing a small-envy allocation.} and (iv) in envy-minimization the vectors always have dimension $d=n$, while in discrepancy we need to handle arbitrary combinations of $n$ and $d$.

We now summarize the best-known bounds for these problems.
For online vector balancing, a simple greedy algorithm achieves a bound of $O(\sqrt{T})$ against an adaptive adversary, which was shown to be tight by \citet{spencer1977balancing,spencer1994ten}. Against a weaker, oblivious adversary, \citet{alweiss2021discrepancy} proposed an elegant algorithm that guarantees an $O(\log T)$ bound with high probability. In a recent breakthrough, \citet{kulkarni2024optimal} give an algorithm that guarantees an $O(\sqrt{\log T})$ bound with high probability, as well as a matching lower bound, thus resolving the vector balancing problem against an oblivious adversary.
An immediate implication of this result is a $O(\sqrt{\log T})$ bound, with high probability, for online envy minimization with $n=2$ agents against an oblivious adversary; however, no corresponding lower bound for envy minimization was known.\footnote{It is worth mentioning that \cite{benade2024fair} prove that sublinear envy is incompatible with non-trivial efficiency guarantees against an oblivious adversary. This result, however, has no implications for envy minimization.} For online multicolor discrepancy against an adaptive adversary, a $O(\sqrt{T})$ bound is possible, while against an oblivious adversary, the best known result was an $O(\log T)$ bound with high probability~\cite{alweiss2021discrepancy}. This directly implies an $O(\log T)$ with high probability bound for online envy minimization problem with $n$ agents.





Overall, the state-of-the-art can be summarized as follows. Online envy minimization is exactly as hard as online multicolor discrepancy against an adaptive adversary. And, for all we know, this could be the case for an oblivious adversary as well: there are no known lower bounds for online envy minimization, and the best known upper bounds for $n=2$ and $n>2$ agents are implications of online vector balancing and online multicolor discrepancy, respectively. There is a gap between the best known bound for online vector balancing ($O(\sqrt{\log T})$, which is known to be optimal) and online multicolor discrepancy ($O(\log T)$, which is not known to be optimal).
Finally, for all we know, online envy minimization could be as hard as online multicolor discrepancy for stochastic adversaries, weaker than an oblivious adversary, where we only know lower bounds for online multicolor discrepancy.\footnote{As we explain in detail later in the paper, known positive results for online envy minimization against stochastic adversaries, e.g.~\cite{benade2024fair}, rely on certain technical assumptions, rendering them incompatible with the online vector balancing literature.}
Simply put, the goal of this paper is to resolve these gaps in our understanding of online envy minimization and online multicolor discrepancy.


\subsection{Our results}



In~\Cref{thm: multi color main upper bound} we prove the existence of an algorithm for the online multicolor discrepancy problem that achieves a bound of $O(\sqrt{\log T})$, with high probability, against an oblivious adversary. This result directly implies the same bound for online envy minimization with $n$ agents (\Cref{cor: main result for oblivious and fair division}). We also give a matching lower bound in \Cref{thm: envy lower bound for oblivious}: for any $r \in (0,1)$, an oblivious adversary can guarantee envy at least $\Omega((\log(T))^{r/2})$ with probability at least $1/T$. Thus, we resolve both problems, online multicolor discrepancy and envy minimization, against an oblivious adversary. Overall, these results show that, similar to the case of an adaptive adversary, online envy minimization is exactly as hard as online multicolor discrepancy against an oblivious adversary.

Next, we analyze a stochastic, i.i.d.\@ adversary that selects a distribution $\calD$ such that, in online multicolor discrepancy (respectively, envy minimization), $v_{t,i} \sim \calD$, for all rounds $t$ and $i \in [d]$ (respectively, $v_{i,t} \sim \calD$, for all agents $i$ and items $g_t$). In the discrepancy minimization literature, \citet{bansal2020line} consider a similar model, where each coordinate of the arriving vectors is sampled uniformly at random from the set $\{ -1, 1\}$, achieving an $O(\sqrt{n} \log T)$ bound with high probability. Other works in the discrepancy minimization world use the term ``i.i.d.'' to refer to settings where dependence over the coordinates is allowed; e.g., in~\cite{bansal2020online} vectors $v_1, \dots v_T$ are sampled i.i.d.\@ from a distribution over $[-1,1]^d$; here, a lower bound of $\Omega\left( \sqrt{\frac{\log T}{\log \log T}} \right)$, with constant probability, is known. In~\Cref{thm: lower bound for iid vector balancing} we prove that there exists a distribution for which every algorithm must have discrepancy at least $\Omega\left( \sqrt{\frac{\log T}{\log \log T}} \right)$ with high probability, even in our ``easier'' i.i.d.\@ model. This result implies that the upper bound in~\Cref{thm: multi color main upper bound} against an oblivious adversary cannot be improved, up to $\log\log$ factors, even against a much weaker i.i.d.\@ adversary.



In the fair division literature, many works provide guarantees in stochastic models. However, to the best of our knowledge, all previous results are asymptotic with respect to the number of items. Specifically, as observed by \citet{bansal2020online}, an innocuous-looking (and prevalent) assumption in stochastic fair division is that the adversary's distribution does not depend on the number of items $T$ (e.g., ruling out a variance of $1/T$). The setup is typically as follows: given a number of agents $n$, the adversary specifies a distribution $\dist$. The designer, who knows this distribution, then selects a (possibly randomized) algorithm. Nature then selects a number of items $T$. In every round $t$, the value of item $t$ for each agent $i$ is drawn independently from $\dist$, i.e., $v_{i,t} \sim \dist$. Under such a ``constant distribution'' assumption, envy-freeness with ``high probability'' means ``probability at least $1 - O(1/\text{poly}(T))$,'' where $\dist$ is treated as a constant. In this easier setup, envy-freeness with high probability is known to be compatible with Pareto Efficiency ex-post, even online~\cite{benade2024fair}. Removing the ``constant distribution'' assumption introduces numerous technical obstacles; see~\Cref{subsec: why random is hard} for a discussion. In this paper, we give an online algorithm that guarantees envy of at most $c+1$ with probability at least $1 - O(T^{-c/2})$ regardless of the distribution.







See~\Cref{tab:summary_results} for a summary of our results.

\begin{table}[t]
    \centering
    \caption{Our results for Online Multicolor Discrepancy (OMD) and Envy Minimization (OEM)}\label{tab:summary_results}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{p{1.1cm}cc|cc}
    \toprule
    \multirow{2}{*}{} & \multicolumn{2}{c|}{\textbf{Oblivious}} & \multicolumn{2}{c}{\textbf{i.i.d.}} \\
    \cline{2-5}
    & \textbf{Upper Bound} & \textbf{Lower Bound} & \textbf{Upper Bound} & \textbf{Lower Bound} \\
    \hline
    \tikzmark{z1}{OMD}
    & \tikzmark{a1}{$O(\sqrt{\log T})$ [Thm~\ref{thm: multi color main upper bound}]}
    & \tikzmark{b1}{$\Omega((\log T)^{r/2})$} 
    & \tikzmark{a2}{$O(\sqrt{\log T})$ [Thm~\ref{thm: multi color main upper bound}]} 
    & \tikzmark{b2}{$\Omega\left(\sqrt{\frac{\log T}{\log \log T}}\right)$ [Thm~\ref{thm: lower bound for iid vector balancing}]} \\
    \hline
    \tikzmark{z2}{OEM} 
    & \tikzmark{a3}{$O(\sqrt{\log T})$ [Cor.~\ref{cor: main result for oblivious and fair division}]} 
    & \tikzmark{b3}{$\Omega((\log T)^{r/2})$ [Thm.~\ref{thm: envy lower bound for oblivious}]} 
    & \tikzmark{a4}{$O(1)$ [Thm.~\ref{thm:n agent upper bound iid fair division}]} 
    &  - \\
    \hline
    \end{tabular}%
    } % End of resizebox
    % Implication Links
    \link{a1}{a3}
    \link{b3}{b1}
\end{table}


























\subsection{Technical overview: multicolor discrepancy against an oblivious adversary}\label{subsec: technical overview of multicolor}

% \alex{
% Overall, the result is achieved as follows:
% \begin{itemize}
%     \item First, show that for every tree with sets of vectors on edges and every large $K$, one can pick a vector per set so that every path is in $11 K$.
%     \item Then, give a tree with sets of vectors on edges, how to get a subgaussian distribution.
%     \item Then algorithm for keeping the prefix sum subgaussian
%     \item Then online weighted vector balancing.
%     \item Then multi-color discrepancy.
% \end{itemize}
% We want to stress why the first one is hard.
% }


To obtain an optimal algorithm for online vector balancing, \citet{kulkarni2024optimal} think of their problem as picking random signs for the edges of a (massive) rooted tree $\calT = (V,E)$. Each edge $e$ of the tree corresponds to a vector $v_e$, a possible choice for the adversary (after some discretization of the adversary's available options). The adversary picks a path from the root to a leaf, which is revealed one edge at a time, and, after learning an edge, the algorithm must assign a (random) sign $x_e \in \{-1,1 \}$. The first and most challenging step in \citeauthor{kulkarni2024optimal}'s proof is to show that, for any convex body $K$ that is sufficiently large, and any rooted tree $\calT = (V,E)$ (whose edges have associated vectors), there exist signs $x \in \{-1,1 \}^E$ such that, for some constant $\alpha < 5$, $\sum_{e \in P} x_e v_e \in \alpha K$, for any root-node path $P$. This result generalizes a well-known result of \citet{banaszczyk2012series} from paths to trees. \citeauthor{kulkarni2024optimal} then show how to go from a single choice of signs $x$ to a distribution $\calD$ over signs, so that for $y \sim \calD$, $\sum_{e \in P} y_e v_e$ is $\gamma$-subgaussian for some constant $\gamma$. Finally, they show that by taking a fine enough $\varepsilon$-net of the unit ball and constructing an appropriate rooted tree $\calT$, whose edges correspond to vectors of the $\varepsilon$-net, the distribution $\calD$ over signs implies an optimal algorithm for online vector balancing.

Our approach for online multicolor discrepancy follows the same high-level steps. Intuitively, in \citet{kulkarni2024optimal}'s blueprint for online vector balancing, we could think of the adversary as picking a set $S_e = \{ v_e, - v_e \}$, and the algorithm picking one of the two vectors from $S_e$ for each edge $e$. The first and most challenging step of our proof is to extend the result of~\cite{kulkarni2024optimal} from trees where edges have associated vectors (or sets of size two) to trees where edges have associated sets of arbitrary size. Concretely, in \Cref{theorem:tree-reduction} we prove that, given any rooted tree $\calT = (V,E)$ where every edge $e \in E$ has an associated set of vectors $S_e \subseteq \Ball^d_2$ (satisfying a couple of technical assumptions, for example, $\mathbf{0} \in \conv(S_e)$), and a sufficiently large symmetric convex body $K$, there exists a vector $v_e \in S_e$, such that for all $u \in V$, $\sum_{e \in P_u} v_e \in 11 K$, where $P_u$ is the set of edges of the path from the root to the node $u$. To prove \Cref{theorem:tree-reduction} we start with a fractional selection $x^e = (x^e_1, \dots, x^e_{|S_e|})$ of vectors for each edge $e$, such that the desired property is satisfied. We iteratively round this fractional selection, so that each step does not increase $\sum_{e \in P_u} x^e v_e$ by too much, so that the final, integral selection has the desired property. Our rounding is \emph{bit-by-bit}, inspired by similar procedures by~\cite{bansal2022prefix,lovasz1986discrepancy}. Our process has two steps. First, we prove that each $x^e_i$ can be rounded so that its fractional part is at most $k$ bits long (for some small $k$). Then, we iteratively reduce the number of bits in the fractional part by rounding each remaining bit, one at a time, until all values become integers. The rounding decisions---which bits get rounded down to $0$ and which bits are rounded up to $1$---are guided by a black-box call to~\cite{kulkarni2024optimal}'s extension of Banaszczyk's result to a certain blown-up tree (where the $\{-1,1\}$ signs tell us whether to round up or down).

With~\Cref{theorem:tree-reduction} in hand, we prove there exists a distribution $\calD$ over vectors (one from each edge set) such that for $x \sim \calD$, $\sum_{e \in P_u} x_e$ is subgaussian, for every node $u \in V$ (\Cref{theorem:tree-subgaussianity}). Finally, we prove that there exists an algorithm that, given sets of vectors one at a time, selects a vector from each set such that the vector sum is $O(1)$-subgaussian (\Cref{theorem:subgauss-algo}). This algorithm can be used to get an algorithm for \emph{weighted} online vector balancing (\Cref{lemma:weighted-vector-balancing}), which can, in turn, be used to give an algorithm for online multicolor discrepancy (\Cref{thm: multi color main upper bound}).

\subsection{Technical overview: online item allocation against a random adversary}\label{subsec: why random is hard}

For the case of an i.i.d.\@ adversary, where each $v_{i,t} \sim \mathcal{D}$, we establish even tighter bounds: the maximum envy is a \emph{constant} with all but polynomially small probability in $T$. Strikingly, the algorithms achieving this guarantee are distribution-independent.

Prior work~\cite{dickerson2014computational} shows that, for a fixed distribution $\calD$, assuming $T$ is sufficiently large, the simple \emph{welfare maximization} algorithm---``allocate each item $t$ to $\argmax_{i \in [n]} v_{i,t}$''---achieves envy-freeness with high probability. Here, we seek bounds that depend only on $T$, and are independent of $\calD$. Unfortunately, for \emph{any} choice of $T$, there exist distributions $\mathcal{D}$ where welfare maximization results in $\Theta(\sqrt{T})$ envy.

Our algorithm works in two phases. The first phase is welfare maximization; this phase might generate significant envy. The second phase, consisting of the final $\Theta(\log T \sqrt{T})$ items, mitigates this envy. At every step $t$, we single out a set of agents who have not received a large
number of items (within this phase). Among this set, we allocate item $t$ to the agent who is envied the least by agents in this set. The key challenges are (i) ensuring that phase two is sufficiently long so that the envy generated during phase one is eliminated, and (ii) preventing endless cycles of envy, where eliminating the envy of agent $i$ inadvertently causes another agent $j$ to envy $i$. As we show, running welfare maximization for longer, i.e., having a longer phase one, makes envy cycles lighter (but increases the maximum envy). Therefore, properties (i) and (ii) are, in fact, in tension: the length of the two phases must be carefully chosen to simultaneously satisfy these competing requirements.

Let $H^t$ be a graph with agents as nodes and an edge $(i,j)$ if, at step $t$, agent $i$ envies agent $j$ by at least a constant $c$. Our goal is to ensure that $H^t$ is empty after all items have been allocated. Intuitively, allocating an item to an agent $i$ decreases the prevalence of outgoing edges and increases the prevalence of incoming edges of node $i$. Moreover, after running welfare maximization phase, $H^t$ is guaranteed to be acyclic. To prevent new edges from forming, during phase 2, we allocate arriving items to sources in $H^t$.

Our first major technical hurdle (\Cref{lem:no-cycle}) is ensuring that no cycles form in $H^t$, for any $1 \leq t \leq T$, which would make allocating to source nodes impossible (and seemingly make eliminating envy extremely challenging). The second major hurdle (\Cref{lem:high-value-n}) is showing that giving agent $i$ a moderate number of items more than agent $j$ during phase 2---specifically $\lceil \log T \sqrt{T} \rceil$---suffices to remove the edge $(i, j)$. Crucially, both of these are statements about \emph{all} time steps $t$. However, phase 2 induces correlations between $H^t$ and $H^{t+1}$, making it difficult to maintain an analytical handle on this graph.

To address this, we introduce an alternate sampling method that, from the algorithm's perspective, is identical to the standard sampling process. We pre-sample a large pool of items (in quantile space) and choose which item arrives next based on which agent will receive it. This is only possible because phase 2 decisions are agnostic to the arriving item's values. Importantly, this approach makes $H^t$ depend only on the \emph{number} of times each agent received an item rather than the specific time steps at which they were received, giving traction toward the lemmas. 

A final challenge is ensuring that our results hold for \emph{any} distribution without additional assumptions. Crucially, we cannot apply standard concentration inequalities, as those typically require a large number of items or specific distributional properties (e.g., a lower bound on variance). To address this, we derive a new distribution-agnostic concentration inequality (tailored to our specific task of bounding the envy), which might be of independent interest.



%Specifically, if welfare maximization is run for sufficiently long and all remaining items are given to a specific agent $i$, the probability that $i$ envies another agent $j$ by more than $c$ decreases exponentially fast in $c$. To establish these results, we leverage a notion of ``approximate'' stochastic dominance: we prove that there exists an almost perfect matching between $i$'s items and $j$'s items such that $i$ prefers her own item to the one matched in $j$'s bundle. We show that a matching that includes all but $c$ of $i$'s items exists, with all but exponentially small (in $c$) probability. This matching readily implies the existence of an approximately envy-free allocation.


\subsection{Related work}

\paragraph{Online vector balancing.}
    When the incoming vectors satisfy $\norm{v_i}_2 \leq 1$, then randomly coloring the vectors $\{-1,+1\}$ achieves a discrepancy of $\sqrt{T \log{d}}$, even for the case of an adaptive adversary. A  matching lower bound of $\Omega(\sqrt{T})$ was proved by \citet{spencer1977balancing,spencer1994ten}. The stochastic setting for online vector balancing was first studied in~\cite{bansal2020line}. They considered a setting where the incoming vectors are chosen i.i.d.\@ from the uniform distribution over all vectors in $\{-1,1\}^n$ and gave an algorithm for achieving a $O(\sqrt{d}\log{T})$ bound on discrepancy at all time steps till $T$. This was later improved by \citet{bansal2020online} who showed a $O(d^2 \log{nT})$ upper bound for any distribution supported on $[-1,1]^n$. The work of \citet{bansal2021online} improved the dependence on $d$ by showing that $O(\sqrt{d}\log^4{dT})$ discrepancy can be achieved. The dependence on $T$ was further improved to $O_d(\sqrt{\log(T)})$ by \citet{aru2016balancing}, where the implicit dependence on $d$ was super-exponential. 
    
    The setting of an oblivious adversary remained relatively under-explored until the recent work of \citet{alweiss2021discrepancy}. They design an extremely simple and elegant algorithm that achieves a bound of $O(\log{nT})$ for both online vector balancing and online multicolor discrepancy. Their algorithm is based on a self-balancing random walk, which for vector balancing, ensures that the discrepancy prefix vector is $O(\sqrt{\log{nT}})$-subgaussian. A tight bound of $\Theta(\sqrt{\log{T}})$ for online vector balancing was achieved by \citet{kulkarni2024optimal}, who proved the existence of an algorithm that maintains $O(1)$-subgaussian prefix vectors.


\paragraph{Stochastic fair division.}

Stochastic fair division, introduced by \citet{dickerson2014computational}, studies the existence of fair allocations when valuations are drawn from a distribution. \citeauthor{dickerson2014computational} show that maximizing utilitarian welfare produces an envy-free allocation with high probability when the number of items $T \in \Omega(n \log n)$ and items values are drawn i.i.d.\@ from a fixed ``constant distribution'' (i.e., the distribution does not depend on the number of items). \citet{manurangsi2020envy,manurangsi2021closing} establish tight bounds for the existence of envy-free allocations in the ``constant distribution'' i.i.d.\@ model: $T \in \Omega (n \log n / \log \log n )$ is both a necessary and sufficient condition. \citet{bai2021envy} extend the result to the case of independent but non-identical additive agents.

Beyond envy-freeness, weaker fairness notions such as maximin share fair~\cite{kurokawa2016can,amanatidis2017approximation,farhadi2019fair} and proportional~\cite{suksompong2016asymptotic} allocations exist with high probability. Finally, the existence of fair allocations for agents with non-additive stochastic valuations are studied in~\cite{manurangsi2021closing,gan2019envy,benade2024existence}.

%\alex{this paper has discrepancy in the title: ~\cite{manurangsi2022almost}; need to check}

\paragraph{Online fair division.}
A rich literature studies online or dynamic fair division. Numerous works study the problem where divisible or indivisible items arrive over time, with the goal of optimizing utilitarian welfare~\cite{gkatzelis2021fair,bogomolnaia2022fair}, egalitarian welfare~\cite{springer2022online,kawase2022online}, Nash welfare~\cite{gao2021online,banerjee2022online,liao2022nonstationary,huang2023online,yang2024online}, the generalized mean of the agents' utilities~\cite{barman2022universal}, and approximation of the maximin share guarantee~\cite{zhou2023multi}.

Closer to our work, the aforementioned works~\cite{dickerson2014computational,bai2021envy} prove that maximizing welfare and weighted welfare---algorithms that can be implemented online---achieve envy-freeness with high probability, in addition to Pareto efficiency, when valuations are drawn i.i.d.\@ from ``constant'' distributions for identical and non-identical agents. \citet{benade2024fair} prove that even when correlation is allowed between agents (but items are independent), Pareto efficiency and fairness are compatible in the online setting---the specific fairness guarantee for a pair of agents $i,j$ is ``either $i$ envies $j$ by at most 1 item, or $i$ does not envy $j$ with high probability''; again, distributions are treated as constants. To the best of our knowledge, we are the first to study the stochastic setting (online or offline) without the ``constant distribution'' assumption.



There are several variations of the standard models, e.g., revocable allocations~\cite{he2019achieving,yang2023fairly}, only having access to pairwise comparisons~\cite{benade2025dynamic}, two-sided matching~\cite{mertzanidis2024}, items arriving in batches~\cite{benade2018make}, and repeated allocations~\cite{igarashi2024repeated}. Further afield, many works study the ``inverse'' problem of allocating static resources to agents that arrive/depart over time~\cite{kash2014no,li2018dynamic,sinclair2022sequential,vardi2022dynamic,banerjee2023online}.
