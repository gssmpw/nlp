\section{Related Work}
\subsection{Data and AI in Public Health}

Given the growing availability of technological systems to track health at scale (a task often termed \textit{public health surveillance})~\cite{Fisher2022,Zeng2020}, public health has seen increased interest in AI. 
HCI literature has contributed both applications and critiques of public health tracking, including novel public health interventions \cite{Balaam2015}, models to predict disease spread based on social media signals \cite{Matero2023,Shahid2020}, and critical discourse around how personal data can or should be used for public health purposes \cite{Thakkar2022,Watson2021}. 
Furthermore, considerable research in data visualization has focused on how visual analytics can support public health and epidemiology applications, as reviewed by \citet{carroll_visualization_2014} and \citet{preim_survey_2020}.
Recently, the COVID-19 pandemic prompted a wave of new tools for tracking and predicting disease spread using geographic visualization \cite{Ghimire2021,Zhang2021,Chande2020}, building on previous work using mapping to track other public health conditions \cite{Maciejewski2011,Allen2016,Miranda2002}. The present work takes inspiration from these geographic risk assessment tools in developing visualizations for overdose risk; however, in this work we focus on the added challenges of communicating an ML-driven risk score.

Despite a large number of technological systems focused on public health, the perspectives of public health decision-makers and administrators themselves have largely been absent thus far from the literature, particularly at the local level. 
For example, many prior data visualization systems for epidemiology demonstrate usability and value through evaluations with domain-expert researchers~\cite{driedger_correction_2007,robinson_combining_2005}, which may yield valuable feedback from a normative perspective yet obscure practical challenges that arise later around adoption and deployment~\cite{carroll_visualization_2014,Fisher2022}.
In a qualitative study by \citeauthor{Morgenstern2021}, public health experts were cautiously optimistic about AI as a tool to help them leverage new data sources without the lag of traditional data collection, but they were also concerned about the risks of selection bias and inequity \cite{Morgenstern2021}. 
Most closely related to our work, \citet{mccurdy_framework_2019} reported a case study on designing a Zika outbreak dashboard for global health workers at a national scale, while \citet{backonja_supporting_2022} developed a dashboard in collaboration with rural health departments to highlight health disparities in their areas.
Our work builds on these findings in a context where an AI system built for a large scale is being considered for deployment into local settings, yielding additional implications for use cases in which priorities at the two levels may not always be aligned.

\subsection{Human-AI Collaboration in the Public Sector}

AI-driven systems have recently been introduced to support decision-makers in a variety of areas in local government, including recidivism prediction \cite{Angwin2016}, child welfare \cite{Vaithianathan2019}, job placement \cite{Scott2022}, and housing allocation \cite{Kuo2023}. These developments have prompted and in turn been influenced by work in CSCW and HCI examining how these systems are used by decision-makers in practice. Many studies consider these systems through the lens of ``street-level algorithms''~\cite{alkhatib_street-level_2019,ammitzboll_flugge_street-level_2021}, which draws a contrast between algorithmic rigidity and the discretion often associated with human-decision makers.
For example, studies with child welfare agencies that use ML-based tools to predict future removal of a child from their home have found that risk predictions measured from administrative data do not align with the notions of risk humans expect them to approximate \cite{Saxena2023}, a finding which workers in these organizations were aware of and compensated for \cite{Kawakami2022partnerships}. Similarly, works by Kuo and Shen et al. \cite{Kuo2023} and \citeauthor{Scott2022} \cite{Scott2022}, on algorithms used in homeless services and in unemployment support, respectively, highlight the tendency of AI-based decision support tools to implicitly view caseworkers' job as accepting some people and denying others (as opposed to advocating for clients and matching people with services). This literature provides a useful starting point for understanding how administrative practices in public health may influence predictive algorithm use at the local level. In another sense, however, algorithms for regional public health are not necessarily best envisioned as ``street-level algorithms'' because they do not purport to replace what is already a highly data-driven decision process~\cite{hoeyer_datafication_2019}. Rather, AI in public health typically aims to provide additional signals to support expert decision-making, creating different challenges for human-AI collaboration that we explore in our study.

Much of the work discussed above has focused on understanding frontline decision-makers' experiences with algorithmic tools \textit{after} they have been deployed and used for some time \cite{Kawakami2022partnerships,Kuo2023,Beede2020}. While this has the advantage of ensuring that decision-makers are familiar with the algorithm and its behavior, it is often difficult to incorporate structural changes suggested by workers into the system's design. A few studies have used early-stage design probes \cite{Kawakami2022} or co-design sessions \cite{HoltenMoller2020} to better understand stakeholders' values alongside algorithm development. Others have developed prototype user interfaces to explore decision-makers' responses to visualizations or algorithmic insights, such as improving the interpretability \cite{Zytek2021} or fairness \cite{Cheng2021} of child maltreatment risk assessments. Our work follows an approach similar to the latter studies by constructing an interactive system based on real data, which we use to understand experts' perceived opportunities for algorithm and interface design \textit{if} such a system were to be deployed.

\subsection{Prototyping and Evaluating Explainable AI Systems}

The design of explainable AI-based systems for decision support has been extensively explored through empirical research in CSCW and HCI literature (reviewed in \cite{lai_towards_2023,rong_towards_2024}).
These studies largely suggest that there is no one-size-fits-all explainability solution for every task.
For example, studies with expert decision-makers have found that feature explanations such as Shapley Additive Explanations (SHAP)~\cite{Lundberg2017} help users understand what data a model is using~\cite{Zytek2021,Cheng2019}, and lead them to perceive the AI as more useful~\cite{Sivaraman2023}.
Yet familiarity with the underlying data and model appears to modulate whether users will gain useful insight from feature explanations~\cite{Wang2020}.
For example, data scientists using feature explanations as a model debugging tool tended to over-rely on misleading explanations in the absence of an in-depth understanding of the data they were working with~\cite{kaur_interpreting_2020}.
For other types of domain experts, feature-based explainability may be less helpful than the simple knowledge that a system has been rigorously validated for their use~\cite{Ghassemi2021,amann_explain_2022}.
Given these often-conflicting findings, there is no clear consensus on how model predictions should be communicated and explained, particularly in under-explored areas such as local public health.
We explored this question through the design and evaluation of our prototype system.

As our evaluation was situated within a model development effort, it forms part of an ongoing, practice-oriented discussion in the literature about how data science teams can most effectively prototype and evaluate possible human-AI interaction designs.
\citeauthor{yang_re-examining_2020} point out several challenges when prototyping and designing AI-driven systems, including helping domain experts understand AI capabilities, rapidly testing human-AI interactions, and combining expertise between AI engineers and end users~\cite{yang_re-examining_2020}. 
Because of these obstacles, previous efforts to design AI-based support systems for experts have often used design concepts without any working system~\cite{yildirim_sketching_2024,Kawakami2022} or evaluations using preliminary but functional systems~\cite{gu_lessons_2020,Cai2019,Sivaraman2023}.
While the former approach is core to human-centered design processes, the latter approach could be described as technology-centered design~\cite{yang_re-examining_2020,bly_design_1999,ries_lean_2011}, in which developers create technological interventions first, then solicit user feedback on them to determine what to build next.
We worked within this technology-centered paradigm primarily because local health officials' workload~\cite{brownson_building_2018,leider_state_2020} made it difficult to engage users in co-design activities without a catalyst for buy-in like a working system. 
However, it also yielded feedback that was more grounded in realistic AI capabilities, and it allowed stakeholders to directly assess the characteristics and limitations of the data.
Since these challenges are likely to exist in other highly-constrained domains, we aimed to identify strategies that future human-AI collaboration projects taking a technology-centered approach can use to more easily adapt to expert feedback when it is available.