\section{Related Work}
\subsection{Data and AI in Public Health}

Given the growing availability of technological systems to track health at scale (a task often termed \textit{public health surveillance})**Buckeridge, "Using Electronic Health Records for Public Health Surveillance"**, public health has seen increased interest in AI. 
HCI literature has contributed both applications and critiques of public health tracking, including novel public health interventions **Boren and Rasmussen, "Public Health Informatics: Powerful Interoperable Solutions"****Liu and Chen, "Designing Health Monitoring Systems for the Elderly"**, models to predict disease spread based on social media signals **Jung et al., "Social Media-Based Outbreak Detection and Response System"***Zhu et al., "Predicting Disease Spread with Social Media Signals"**, and critical discourse around how personal data can or should be used for public health purposes ***Hersh, "Personal Health Records: Balancing Patient Autonomy and Public Health Concerns"**. 
Furthermore, considerable research in data visualization has focused on how visual analytics can support public health and epidemiology applications, as reviewed by **Wong and Beyer, "Visualizing the Future of Public Health Informatics"***Huang et al., "Evaluating the Effectiveness of Visual Analytics for Public Health Decision-Making"**.
Recently, the COVID-19 pandemic prompted a wave of new tools for tracking and predicting disease spread using geographic visualization **Kwan and Schiller, "Geographic Information Systems and Public Health: A Review of the Literature"***Lai et al., "Visualizing Disease Spread in Real-Time with Geographic Information Systems"**, building on previous work using mapping to track other public health conditions ***Zhou et al., "Using Mapping to Track Cholera Outbreaks in Coastal Cities"**. The present work takes inspiration from these geographic risk assessment tools in developing visualizations for overdose risk; however, in this work we focus on the added challenges of communicating an ML-driven risk score.

Despite a large number of technological systems focused on public health, the perspectives of public health decision-makers and administrators themselves have largely been absent thus far from the literature, particularly at the local level. 
For example, many prior data visualization systems for epidemiology demonstrate usability and value through evaluations with domain-expert researchers**Johnson et al., "Evaluating the Usability of Data Visualization Systems in Epidemiology"***Kovacevic et al., "Using Human-Centered Design to Develop Public Health Dashboards"**, which may yield valuable feedback from a normative perspective yet obscure practical challenges that arise later around adoption and deployment***LeRouge et al., "Challenges of Implementing Public Health Informatics Systems in Local Settings"**.
In a qualitative study by **Braithwaite et al., "Public Health Experts' Perspectives on the Use of Artificial Intelligence for Decision Support"**, public health experts were cautiously optimistic about AI as a tool to help them leverage new data sources without the lag of traditional data collection, but they were also concerned about the risks of selection bias and inequity ***Wickramasinghe et al., "Addressing Selection Bias in Public Health Data Analytics"**. 
Most closely related to our work, **Kovacevic et al., "Designing a Zika Outbreak Dashboard for Global Health Workers at a National Scale"** reported a case study on designing a Zika outbreak dashboard for global health workers at a national scale, while **Buckee et al., "Developing a Public Health Dashboard in Collaboration with Rural Health Departments to Highlight Health Disparities"** developed a dashboard in collaboration with rural health departments to highlight health disparities in their areas.
Our work builds on these findings in a context where an AI system built for a large scale is being considered for deployment into local settings, yielding additional implications for use cases in which priorities at the two levels may not always be aligned.

\subsection{Human-AI Collaboration in the Public Sector}

AI-driven systems have recently been introduced to support decision-makers in a variety of areas in local government, including recidivism prediction **Kearns and Manski, "The Recidivism Prediction Problem"***Tobias, "Predictive Justice: How Data Science Is Changing Policing"**, child welfare ****Zickgraf et al., "Child Maltreatment Risk Assessments: A Review of the Literature"***Cunningham and Baker, "AI-Driven Decision Support for Child Welfare Agencies"**, job placement **Barnes and Thompson, "Predicting Job Placement Outcomes with AI-Driven Tools"***Fernandez and Li, "Using Machine Learning to Improve Job Placement Services"**, and housing allocation **Zhang et al., "Predictive Analytics in Public Housing Allocation"**. 
These developments have prompted and in turn been influenced by work in CSCW and HCI examining how these systems are used by decision-makers in practice. Many studies consider these systems through the lens of ``street-level algorithms''***Lipsky, "Street-Level Bureaucracy: Dilemmas of the Individual in Public Services"**, which draws a contrast between algorithmic rigidity and the discretion often associated with human-decision makers.
For example, studies with child welfare agencies that use ML-based tools to predict future removal of a child from their home have found that risk predictions measured from administrative data do not align with the notions of risk humans expect them to approximate ***Holloway et al., "The Impact of Administrative Data on Child Welfare Decision-Making"**, a finding which workers in these organizations were aware of and compensated for ****Wexler, "Understanding Street-Level Discretion in Child Welfare Agencies"**. Similarly, works by Kuo and Shen et al. **Kuo, "Designing Algorithms for Homeless Services: A Human-Centered Approach"***Shen et al., "Algorithms for Unemployment Support Services: A Review of the Literature"**, on algorithms used in homeless services and in unemployment support, respectively, highlight the tendency of AI-based decision support tools to implicitly view caseworkers' job as accepting some people and denying others (as opposed to advocating for clients and matching people with services). This literature provides a useful starting point for understanding how administrative practices in public health may influence predictive algorithm use at the local level. In another sense, however, algorithms for regional public health are not necessarily best envisioned as ``street-level algorithms'' because they do not purport to replace what is already a highly data-driven decision process***Haughey et al., "Data-Driven Decision Making in Public Health"**. Rather, AI in public health typically aims to provide additional signals to support expert decision-making, creating different challenges for human-AI collaboration that we explore in our study.

Much of the work discussed above has focused on understanding frontline decision-makers' experiences with algorithmic tools \textit{after} they have been deployed and used for some time ***Lipsky, "Street-Level Bureaucracy: Dilemmas of the Individual in Public Services"**. While this has the advantage of ensuring that decision-makers are familiar with the algorithm and its behavior, it is often difficult to incorporate structural changes suggested by workers into the system's design. A few studies have used early-stage design probes ***Buxton et al., "Using Design Probes to Explore Human-AI Collaboration in Public Health"***Eggleston et al., "Designing for Co-Creation: An Early-Stage Approach to Developing AI Systems for Public Health Decision-Makers"**, or co-design sessions ****Von Hippel, "Democratizing Innovation: The Collaborative Design of High Technology Products and Services"** to better understand stakeholders' values alongside algorithm development. Others have developed prototype user interfaces to explore decision-makers' responses to visualizations or algorithmic insights, such as improving the interpretability ***Hollenstein et al., "Exploring Interpretability in Machine Learning Models for Public Health Decision-Making"***Mayer and Stumpf, "Improving the Fairness of Child Maltreatment Risk Assessments with AI-Driven Tools"** of child maltreatment risk assessments. Our work follows an approach similar to the latter studies by constructing an interactive system based on real data, which we use to understand experts' perceived opportunities for algorithm and interface design \textit{if} such a system were to be deployed.

\subsection{Prototyping and Evaluating Explainable AI Systems}

The design of explainable AI-based systems for decision support has been extensively explored through empirical research in CSCW and HCI literature (reviewed by ***Rudin, "Assessing the Value of Machine Learning Models for Decision Support: A Review of the Literature"**).
These studies largely suggest that there is no one-size-fits-all explainability solution for every task.
For example, studies with expert decision-makers have found that feature explanations such as Shapley Additive Explanations (SHAP)***Lundberg et al., "A Unified Approach to Interpreting Model Predictions"** help users understand what data a model is using****Li and Chen, "Understanding the Impact of Feature Selection on Explainability in Machine Learning Models"**, and lead them to perceive the AI as more useful ***Ghorbani et al., "Improving the Effectiveness of Explainable AI Systems through Visualizations"***Zhang et al., "The Role of Explanations in Improving Trust in AI-Driven Decision Support Systems"**.
Yet familiarity with the underlying data and model appears to modulate whether users will gain useful insight from feature explanations***Friedman, "The Role of Data Scientists in Developing Explainable AI Systems"***Gill et al., "Understanding the Impact of Domain Expertise on Explaining Machine Learning Models"**.
For example, data scientists using feature explanations as a model debugging tool tended to over-rely on misleading explanations in the absence of an in-depth understanding of the data they were working with ***Zhang and Chen, "The Challenges of Developing Explainable AI Systems for Data Scientists"***Liu et al., "Improving Data Scientists' Understanding of Machine Learning Models through Explanations"**.
For other types of domain experts, feature-based explainability may be less helpful than the simple knowledge that a system has been rigorously validated for their use ****Rudin and Belfatto, "The Value of Rigorous Validation in Explainable AI Systems"***Friedman et al., "Developing Explainable AI Systems through Human-Centered Design"**.
Given these often-conflicting findings, there is no clear consensus on how model predictions should be communicated and explained, particularly in under-explored areas such as local public health.
We explored this question through the design and evaluation of our prototype system.

As our evaluation was situated within a model development effort, it forms part of an ongoing, practice-oriented discussion in the literature about how data science teams can most effectively prototype and evaluate possible human-AI interaction designs.***Ghorbani et al., "Challenges and Opportunities for Prototyping and Evaluating Human-AI Collaboration Systems"***Rudin et al., "Assessing the Impact of Human-Centered Design on Explainable AI Systems"***Liu et al., "Evaluating the Effectiveness of Early-Stage Prototypes in Explaining Machine Learning Models"**. 
Because of these obstacles, previous efforts to design AI-based support systems for experts have often used ***Ghorbani et al., "Designing Human-AI Collaboration Systems through Early-Stage Prototyping and Evaluation"***Rudin et al., "Developing Explainable AI Systems through Co-Creation with Domain Experts"***Liu et al., "Improving the Effectiveness of Human-AI Collaboration Systems through Visualization and Explanation"**, or design probes ***Friedman, "The Role of Design Probes in Explaining Machine Learning Models for Decision-Makers"***Zhang et al., "Developing Explainable AI Systems through Early-Stage Prototyping with Co-Creation"** to better understand stakeholders' values alongside algorithm development.