\documentclass[12pt]{article}

\usepackage[numbers,sort]{natbib}
\usepackage[margin=1.1in]{geometry}
\usepackage{macros}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{pgfmath}
\usepackage{amsmath}
\usepackage{authblk}



\title{Analyzing Cost-Sensitive Surrogate Losses via $\H$-calibration}

\author[1]{Sanket Shah\thanks{Corresponding Author: sanketshah@g.harvard.edu}}
\author[1]{Milind Tambe\thanks{milind\_tambe@harvard.edu}}
\author[2]{Jessie Finocchiaro\thanks{Corresponding Author: finocch@bc.edu}}

\affil[1]{Department of Computer Science, Harvard University}
\affil[2]{Department of Computer Science, Boston College}


\begin{document}

\maketitle









\begin{abstract}
This paper aims to understand whether machine learning models should be trained using cost-sensitive surrogates or cost-agnostic ones (e.g., cross-entropy). 
Analyzing this question through the lens of $\H$-calibration, we find that cost-sensitive surrogates can strictly outperform their cost-agnostic counterparts when learning small models under common distributional assumptions. 
Since these distributional assumptions are hard to verify in practice, we also show that cost-sensitive surrogates consistently outperform cost-agnostic surrogates on classification datasets from the UCI repository. 
Together, these make a strong case for using cost-sensitive surrogates in practice.
\end{abstract}

\section{Introduction}

In training machine learning models for classification-like tasks, the practitioner faces a plethora of design decisions, one of which the loss function: should loss functions be hand-crafted for the task at hand, or does it suffice to use a generic loss combined with post-processing?
The former camp argues that, when faced with the inevitability of imperfect prediction, optimizing a custom surrogate at least will yield better predictions for the task at hand~\citep{ramaswamy_consistent_2016,scott_calibrated_2012,vapnik_nature_2000,elmachtoub_smart_2022,wilder_melding_2019}.
In contrast, the other camp argues that one can always adapt the model for the task at hand with clever post-processing~\citep{dawid_well-calibrated_1982,seidenfeld_calibration_1985,buja_loss_2005,reid_information_2011,reid_surrogate_2009}.
For example, if the cost of a false positive is much higher than the cost of a false negative in a binary classification problem, one might optimize cross-entropy to obtain a predictive model, then set a threshold on decisions, say $\tau = \frac 4 5$, to establish the decision boundary.
\footnote{Formally, we denote this decision boundary through the link function $\psi_\tau(x) := \sign(x - \tau)$, where, e.g., $\tau = \frac 4 5$.}


We speak to this debate by examining the problem of cost-sensitive classification---where ``costs'' for different types of errors (e.g., false positives and negatives) can be written as a cost matrix $\ell$. We then use limited model expressiveness as a case study, asking:
\begin{quote}
    Do small ML models trained by optimizing a cost-sensitive surrogate necessarily outperform models trained by a cost-agnostic surrogate combined with ``clever'' post-processing in cost-sensitive classification?
\end{quote}

We answer this question through the lens of $\H$-consistency, which examines whether learning via a surrogate loss (over the model class $\H$) implies learning the best decision for a cost-sensitive target problem.
We aim to see if, for ``small'' model classes $\H$  (e.g., linear models), a cost-sensitive surrogate $\Ltargetaware$ is $\H$-consistent with respect to the target problem $\ell$ while a canonical cost-agnostic surrogate $\Ltargetagnostic$ is not $\H$-consistent, \emph{regardless of post-processing choices}.

In \Cref{sec:XE-not-H-consistent}, we first show that, even in a ``nice'' setting ($P$-minimizability), a cost-agnostic surrogate (cross-entropy) plus post-processing using thresholding fails to perform well for the cost-sensitive target.
In contrast, we show in~\cref{sec:embeddings-H-consistent} that cost-sensitive surrogates provided by the Embeddings framework of \citet{finocchiaro_embedding_2024} guarantee $\H$-consistency in the same setting. 
Our work establishes a strict gap between the performance of small ML models trained on cost-sensitive surrogates and those trained on cost-agnostic ones, even when the cost-agnostic losses are equipped with a threshold search to post-process.

We then proceed to empirically validate our theorems in~\Cref{sec:experiments} when distributional assumptions cannot be verified. 
We find that cost-sensitive surrogates continue to outperform their cost-agnostic counterparts on three realistic datasets on lending, student performance, and diabetes diagnosis from the UCI repository~\cite{dua17uci}. Our theoretical results combined with empirical evidence move towards a theoretical explanation for the recent success of research directions like decision-focused learning~\citep{elmachtoub_smart_2022,wilder_melding_2019} and structured prediction~\citep{yu_lovasz_2018} which advocate for cost-sensitive surrogates. 



\section{Background and Related Work}
Our main question of interest is if \hlc[myYellow]{small} ML models trained using a \hlc[myBlue]{cost-sensitive surrogate} loss functions outperform models trained on cost-agnostic surrogates combined with some \hlc[myPurple]{``clever'' post-processing}.

To understand this question, we must formalize the three emphasized pieces.
First, we formalize \hlc[myYellow]{small} through the optimized model class $\H$, which enables us to restrict ourselves to model classes like the set of linear models $\Hlin := \{h : \exists \vec w,b \text{ s.t. } h(x) = \sigma(\langle w,x \rangle + b)\}$ or two-layer neural networks.
\hlc[myBlue]{Cost-sensitive surrogates} refer to continuous loss functions $L : \reals^d \times \Y \to \reals_+$ designed with the intention of emulating a cost-sensitive classification problem $\ell^\alpha : \R \times \Y \to \reals_+$.
We designate a surrogate as cost-sensitive if it is $\H$-consistent, where approaching the optimal surrogate loss implies approaching the optimal cost-sensitive loss. 
This notion is a prerequisite for establishing any PAC-learning bounds.
Finally, \hlc[myPurple]{clever post-processing} involves optimizing over the choice of a function $\psi : \reals^d \to \R$ mapping surrogate predictions, such as risk scores, to discrete decisions, such as positive or negative classifications.
The most common example of this clever post-processing is through a threshold search to establish decision boundaries. However, this threshold search is often difficult to scale beyond binary classification as the threshold search space grows in the number of classes.

Our approach to this problem merges the perspectives of two fields: the work on surrogate loss design and the work on $\H$-consistency.
The surrogate loss design literature generally assumes that the model class $\H$ is rich enough for just about any hypothesis to be learned, ignoring the relationship between model class and learned hypothesis (e.g., \citet{reid_information_2011,reid_surrogate_2009,buja_loss_2005,bao_proper_2023}).
On the other hand, the $\H$-consistency literature has yet to provide results informing cost-sensitive surrogate design, simply providing bounds for a variety of standard surrogates for tasks such as adversarial learning~\citep{awasthi2021calibration,bao_calibrated_2021,awasthi2021finer}, regression~\citep{mao_h-consistency_2024}, learning with deferral~\citep{mao_realizable_2024}, and multi-label learning~\citep{mao_multi-label_2024}.

\paragraph{Cost-sensitive classification}
The surrogate losses we design are proxies for some other loss $\ell$, which might not be tractable to optimize directly.
One such loss function, which our work focuses on, is a generalization of the 0-1 loss called \emph{cost-sensitive classification}, denoted $\ell : \R \times \Y \to \reals_+$, where both the report set $\R$ (i.e., the predicted classes) and label set $\Y$ are finite.
In general, we consider any loss $\ell$ that can be written in this form to be a cost-sensitive classification problem, though this approach encapsulates broader classes of problems such as ranking~\citep{ramaswamy_convex_2016}.
\Cref{tab:csc-costs} gives one example of such a cost-sensitive classification loss $\ell^\alpha$ for the binary report and label setting.
In this instance, we normalize the loss so it can be represented with a single parameter, as done by the foundational works of \citet{elkan_foundations_2001,scott_calibrated_2012}.
In general, we refer to cost-agnostic classification as the target loss $\ell = (\ones \ones^T) - \mathrm{Id}$, i.e., the 0-1 loss.




\paragraph{$\H$-consistency}
Agnostic learning aims to find a model $h \in \H$ such that the expected loss of $h$ is close to the best expected target loss possible in $\H$.
However, given the computational hardness of directly optimizing discontinuous functions, such as the cost-sensitive classification loss~\citep{ben2003difficulty}, we instead aim to optimize a \emph{surrogate} loss and apply a \emph{link} function $\psi$ that yields our desired agnostic learning guarantee, recovered by $\H$-consistency.

\begin{definition}[$\H$-consistency]\label{def:H-consistent}
    A surrogate $L : \reals^d \times \Y \to \reals_+$ and link $\psi : \reals^d \to \R$ pair $(L, \psi)$ are $\H$-consistent with respect to a target loss matrix $\ell$ over a set of distributions $\P \subseteq \Delta(\X \times \Y)$ if, for all sequences $\{h_n\} \in \H$ and $P \in \P$, we have
    \begin{multline}
        \resizebox{0.9\textwidth}{!}{        $\underset{\text{approaching the best-in-class surrogate prediction}}{\underbrace{\E_{(X,Y) \sim P}L(h_n(X), Y) \to \inf_{h^* \in \H} \E_{(X,Y) \sim P}L(h^*(X), Y)}} \implies
        \underset{\text{approaching the best-in-class utility by discretizing surrogate predictions}}{\underbrace{\E_{(X,Y) \sim P}\ell(\psi(h_n(X)), Y) \to \inf_{h^* \in \H} \E_{(X,Y) \sim P}\ell(\psi(h^*(X)), Y)}}$ }    \end{multline}
    
\end{definition}

$\H$-consistency has clear connections to PAC learning: minimizing the surrogate implies a low target loss, which is the goal of PAC learning.
However, analyzing for which surrogates $\H$-consistency holds can be technically challenging, which has historically led researchers to study $\H$-calibration as a ``point-wise'' proxy.
To understand when $\H$-calibration and $\H$-consistency are equivalent, we must first discuss some nuance around the distributional assumptions made in the literature.

\paragraph{Distributional assumptions}
In the literature, two common distributional assumptions are made: realizability and minimizability.
If a loss $L$ is $P$-realizable under $\H$, then there exists a hypothesis $h^* \in \H$ such that $\E_{X,Y}L(h^*(X),Y) = 0$.
Notably, this means that $h^*$ yields a deterministic labeling over outcomes, and that there is no outcome uncertainty.
Under even these relatively strict distributional assumptions, \citet{long_consistency_2013,kuznetsov2014multi} observe a gap between $\H$-consistency and $\Hall$-consistency, where (a) the logit loss, which is $\Hall$-consistent, is not $\Hlin$-consistent for multiclass classification, and (b) the Crammer-Singer surrogate, which is not $\Hall$-consistent, is realizable $\H$-consistent for multiclass classification.
Notably, their second result does not extend beyond the realizable setting.

Less stringent is the assumption that a loss $L$ is $P$-minimizable over $\H$, meaning that a Bayes optimal classifier is in class (but its error may not be 0).
\begin{definition}[$P$-minimizability]\label{def:P-minimizable}
    For a given distribution $P \in \Delta(\X \times \Y)$, a loss $L$ is $P$-minimizable over $\H$ if there exists an $h^* \in \H$ such that $h^*(x) = \argmin_{r} \E_{Y\mid X=x}L(r, Y)$ for all $x \in \X$.
    Moreover, let $\Q_{L,\H} := \{P \in \Delta(\X \times \Y) : L $ is $P$-minimizable over $\H\}$ denote the set of distributions $P$ such that the loss $L$ is $P$-minimizable over $\H$.
\end{definition}

$P$-minimizability tends to be the most common distributional assumption~\citep{bao_calibrated_2021}, as it often simplifies analysis to a ``point-wise optimality.'' 
In this paper, we leverage this structure and primarily work under minimizability assumptions.

We refrain from studying the most general distributions because it is known that even in the simplest case of linear classification $\H = \Hlin$ with the 0-1 loss $\ell = \ell_{0-1}$ is NP-Hard~\citep{ben2003difficulty,guruswami2009hardness}.
As a result, there are no $\H$-consistent surrogates for the most general set of distributions. If there were such easy-to-minimize $\H$-consistent surrogates for general distributions, one could reduce direct cost minimization to a tractable problem, implying $P=NP$. 


\paragraph{$\H$-calibration}
While $\H$-consistency yields agnostic learning guarantees, studying the expected loss over distributions on labels \emph{and features} often poses practical difficulties.
To circumvent these, the notion of $\H$-calibration, a ``point-wise consistency'' for each input $x \in \X$, is often used to analyze these surrogates more tractably.
To understand $\H$-calibration, we use the conditional risk, which is the expected loss conditioned on input $x$.

\begin{definition}[Conditional risk]\label{def:inner-risk}
    For a loss function $L : \R \times \Y \to \reals_+$ and hypothesis function $h : \X \to \R$, the conditional risk of the hypothesis $h$ is given
    \begin{align*}
        \condrisk L (h, x, p) &:= \sum_{y} p_y L(h(x), y) 
        = \E_{Y \sim p} L(h(x), Y)
    \end{align*}
    Moreover, we denote the \emph{minimal conditional risk} by
    \begin{align*}
        \condriskstar L \H(x, p) := \inf_{h' \in \H} \condrisk L (h', x, p)
    \end{align*}
\end{definition}

\begin{definition}[$\H$-calibration]\label{def:H-calibration}
    A loss function $L : \reals^d \times \Y \to \reals_+$ and link $\psi : \reals^d \to \R$ pair are $\H$-calibrated with respect to a target loss $\ell : \R \times \Y \to \reals_+$ over the set $\P \subseteq \simplex$ if, for any $\epsilon > 0$, $p \in \P$, and $x \in \X$, there exists some $\delta > 0$ such that
    \begin{align*}
        \condrisk L(h,x, p) - \condriskstar L \H (x,p) &< \delta \implies  \condrisk \ell(\psi \circ h,x, p) - \condriskstar \ell {\psi \circ \H} (x,p) < \epsilon
    \end{align*}
    for all $h \in \H$.
\end{definition}





Early works of \citet{bartlett_convexity_2006,ramaswamy_convex_2016} show that $\Hall$-consistency and $\Hall$-calibration are equivalent in finite outcome settings, where $\Hall$ is the set of all measurable hypotheses.
However, when $\H \neq \Hall$, two gaps emerge: first, the gap between $\H$-calibration and $\H$-consistency, and second, the gap between $\H$-consistency and $\Hall$-consistency.
The following Theorem shows the equivalence between $\H$-calibration and $\H$-consistency under $P$-minimizability, closing the first gap.

\begin{theorem}[{\citet[Theorem 2.8]{steinwart_how_2007}}]\label{thm:steinwart}
    Given a surrogate loss $L : \reals^d \times \Y \to \reals_+$, link function $\psi: \reals^d \to \R$, and target loss $\ell : \R \times \Y\to \reals_+$, assume that $P \in \Q_{L, \H} \cap \Q_{\ell,\H}$.
    Furthermore, assume there exists a function $b \in \L^1(P_X)$ and measurable function $\delta(\epsilon, \cdot) : \X \to \reals_{>0}, \epsilon > 0$ such that 
    \begin{align}
    \begin{aligned}
        \C_{\ell}(h,x,p) &\leq \C^*_{\ell}(x,p) + b(x) \label{eq:ptwise-min}
    \end{aligned}\\
        \begin{aligned}
        \C_{L}(h,x,p) &\leq \C^*_{L}(x,p) + \delta(\epsilon, x)\\
        \implies \C_{\ell}(\psi \circ h,x,p) &\leq \C^*_{\ell}(x,p) + \epsilon
        \label{eq:H-calibration}
        \end{aligned}
    \end{align}
    for all $x \in \X$, $\epsilon > 0$, and $h \in \H$.
    Then for all $\epsilon > 0$, there exists a $\delta > 0$ such that for all $h \in \H$, we have
    \begin{align}
    \begin{aligned}
        \E_{P} L(h(X), Y) &< \inf_{h^* \in \H} \E_{P} L(h^*(X), Y) + \delta \\
          \implies \E_{P} \ell(\psi \circ h(X), Y) &< \inf_{h^* \in \H} \E_{P} \ell(\psi \circ h^*(X), Y) + \epsilon
        \label{eq:H-consistency}
        \end{aligned}
    \end{align}
\end{theorem}

The preceding result gives a sufficient condition for $\H$-calibration to imply $\H$-consistency when the losses are $P$-minimizable over $\H$.
Equation~\eqref{eq:ptwise-min} is simply a condition that the conditional target risk is not infinite, which is generally satisfied in our work based on the restriction to cost-sensitive classification tasks for $\ell$ that can be written in matrix form (and are always finite).
Equation~\eqref{eq:H-calibration} is then our $\H$-calibration condition, and equivalence follows in this setting where the surrogate and discrete problems are $P$-minimizable.

\paragraph{Proper Scoring Losses}
One special class of cost-agnostic surrogates to consider is that of \emph{proper scoring rules}~\citep{savage_elicitation_1971,gneiting_strictly_2007,brier_verification_1950}.
These are generally regarded as loss functions that lead to a predictor $h^*(x) = \Pr_{P}[Y\mid X=x]$ and satisfy the assumption of Equation \eqref{eq:H-calibration}, thus implying $\H$-calibration.
However, something interesting happens when the model class $\H$ does not contain a function $h^*$ predicting the conditional distribution for all $x \in \X$: while the assumption of Equation~\eqref{eq:H-calibration} may still hold, the implication may not.
In fact, we show in~\cref{sec:XE-not-H-consistent} that there are instances where the implication does not hold and that this gap is precisely the one we can close with cost-sensitive surrogate design.

This observation highlights the tensions between minimizability and the translation from $\H$-calibration to $\H$-consistency.
Namely, minimizability means that the Bayes optimal classifier is in $\H$.
However, in $\H$-calibration, we only require that some model $h \in \H$ is individually optimal \emph{for each $x \in \X$}. Importantly, $\H$-calibration does \emph{not} mean that $h$ minimizing the conditional risk for one $x \in \X$ also minimizes conditional risk for some other $x' \neq x$. As a result, the Bayes optimal classifier may not satisfy the assumption of Equation~\eqref{eq:H-calibration} for cost-agnostic classifiers.














\section{Cross-entropy is not $\H$-consistent for Cost-Sensitive Classification}\label{sec:XE-not-H-consistent}

In this section, we give a counterexample of the non-$\H$-consistency of cost-agnostic surrogates for cost-sensitive classification, even when the surrogate and target are both $P$-minimizable (i.e., $P \in \Q_{L, \H} \cap \Q_{\ell, \H}$).
To develop an intuition for the inconsistency of cost-agnostic surrogates, we turn to~\Cref{ex:binary-csc}, which gives a cost-sensitive binary classification problem $\ell^\alpha$, with costs in~\Cref{tab:csc-costs} and data distribution akin to~\Cref{fig:csc-full}.

\begin{example}\label{ex:binary-csc}
    Consider features $\X \subseteq \reals^2$ and binary labels $\Y = \{-1,1\}$ drawn from the distribution $P$ such that 
    \begin{align*}
    Y ~\sim \mathbf{Bern}\left(\frac 1 2\right)\\
    x_2 \sim U[0,1]\\
    x_1 \sim \mathcal{N}(y \cdot x_2, x_2^2).
    \end{align*}
    The optimal cost-agnostic classifier over $P$ is simply $h^*(x) = \sign(x_1)$ because of the symmetry of labels and the labels' incorporation into $x_1$.
\end{example}


\begin{propositionE}[][all end]\label{prop:example-optimal-classifiers-linear}
The Bayes-optimal cost-sensitive and cost-agnostic classifiers for Example~\ref{ex:binary-csc} are linear. 
\end{propositionE}
\begin{proofE}
Then we have 
\begin{align*}
    \Pr[Y = 1 \mid X = x] &= \frac{\Pr[X = x \mid Y = 1] \Pr[Y = 1]}{\Pr[X = x]} \\
    &=\frac{f(\frac{x_1 - x_2}{x_2}) \frac 1 2}{\frac{1}{2}\left(f(\frac{x_1 - x_2}{x_2}) + f(\frac{x_1 + x_2}{x_2})\right)} \\
    &=\frac{f(\frac{x_1 - x_2}{x_2})}{\left(f(\frac{x_1 - x_2}{x_2}) + f(\frac{x_1 + x_2}{x_2})\right)} ~.~
\end{align*}
where $f(\cdot)$ is the pdf of the unit Gaussian.

Therefore, with cost matrix $C(r,y) = \begin{bmatrix} 0 & 1-\alpha \\ \alpha & 0 \end{bmatrix}$, given any $x$, we can ask when the expected \emph{cost} of predicting when $r = 1$ is better than $r = -1$


\begin{align*}
    \sum_{y \in \{-1,1\}} \Pr[Y = y \mid X = x] C(1, y) &\leq \sum_{y \in \{-1,1\}} \Pr[Y = y \mid X = x] C(-1, y) \\
    \alpha \Pr[Y = -1 \mid X = x] &\leq (1- \alpha) \Pr[Y = 1 \mid X = x] \\
    \alpha (1 - \Pr[Y = 1 \mid X = x]) &\leq (1- \alpha) \Pr[Y = 1 \mid X = x] \\
    \alpha &\leq \Pr[Y = 1 \mid X = x] \\
    \alpha &\leq \frac{f(\frac{x_1 - x_2}{x_2})}{\left(f(\frac{x_1 - x_2}{x_2}) + f(\frac{x_1 + x_2}{x_2})\right)} \\
    \alpha &\leq  \frac{\exp(-\frac 1 2(\frac{x_1 - x_2}{x_2})^2)}{\exp(-\frac 1 2(\frac{x_1 - x_2}{x_2})^2) + \exp(-\frac 1 2(\frac{x_1 + x_2}{x_2})^2)} \\
    \alpha &\leq \frac{\exp(-\frac 1 2(\frac{x_1 - x_2}{x_2})^2)}{\exp(-\frac 1 2(\frac{x_1 - x_2}{x_2})^2) + \exp(-\frac 1 2(\frac{x_1 + x_2}{x_2})^2)} \\
    \frac{1}{\alpha} &\geq \frac{\exp(A) + \exp(B)}{\exp(A)} \qquad A = -\frac 1 2(\frac{x_1 - x_2}{x_2})^2, B = -\frac 1 2(\frac{x_1 + x_2}{x_2})^2 \\
    \frac{1}{\alpha} &\geq 1 + \frac{\exp(B)}{\exp(A)} \\
    \frac{1}{\alpha} &\geq 1 + \exp(B-A) \\
    \frac{1}{\alpha} &\geq 1 + \exp(\frac{-2x_1}{x_2}) \\
    \log\left(\frac{1}{\alpha}-1\right) &\geq \frac{-2x_1}{x_2}  \\
    x_2 \log\left(\frac{1}{\alpha}-1\right) &\geq -2x_1 \\
    \frac{x_2}{2} \log\left(\frac{\alpha}{1-\alpha}\right) &\leq x_1
\end{align*}
\end{proofE}

Example \ref{ex:binary-csc} has a few nice properties: first, both the optimal cost-agnostic and cost-sensitive classifiers are linear (Proposition~\ref{prop:example-optimal-classifiers-linear}).
Importantly, however, these optimal classifiers are \emph{different} from each other.
Namely, the optimal cost-agnostic classifier is $h^{ag}(x)$ is vertical, while the optimal cost-sensitive classifier is $h^{aw}(x)$ has slope $\frac{1}{2} \log\left(\frac{\alpha}{1-\alpha}\right)$.

Now, we can incorporate ``post-hoc thresholding'' via the hyperparameter $\tau$ as the link $\psi_\tau$, and modify these thresholded classifiers by $\psi_\tau \circ \htargetagnostic(x) := \sign(x_1 + \tau)$ 
to enable a threshold search, as is commonplace in practice. However, the threshold $\tau$ only permits shifting of the bias --- not the slope. Then, because the optimal classifiers $h^{ag}(\cdot)$ and $h^{aw}(\cdot)$ have different slopes, thresholding can't recover the non-vertical slope of $\htargetaware$, and some cost-sensitive decisions will be made erroneously.

\begin{table}
\centering
    \centering
    \resizebox{0.3\linewidth}{!}{    \begin{tabular}{cr|cc}
     \multicolumn{2}{c|}{\multirow{2}{*}{\makecell{Cost\\Matrix}}} & \multicolumn{2}{c}{\color{black!50} True} \\
     &  & -1 & 1 \\
    \midrule
    \multirow{2}{*}{\rotatebox[origin=c]{90}{\color{black!50} Pred}} & -1 & 0 & $1-\alpha$ \\
     & 1 & $\alpha$ & 0
    \end{tabular}
    }    \caption{Cost-sensitive classification loss $\ell^\alpha$ for binary outcomes}
    \label{tab:csc-costs}
\end{table}

\begin{figure}[h]
    \centering
    
    \includegraphics[width=0.6\linewidth]{figures/csc-example-fig.pdf}
    \label{fig:csc-example}
    \caption{A simple cost-sensitive binary classification task in which neither the cost-agnostic classifier (magenta dashed line $\psi \circ \htargetagnostic$) nor its post-processed counterpart (purple dot-dashed classifier $\psi_{\tau^*} \circ \htargetagnostic$) yield the optimal model. Instead, we need to incorporate information about the cost matrix into your training algorithm, e.g., by using a cost-sensitive loss function (orange dotted classifier $\htargetaware$ for $\alpha = \frac 1 4$). Data distribution given in Example~\ref{ex:binary-csc}. 
    }\label{fig:csc-full}
\end{figure}

Consequently, this example leads us to our first main result: even with a threshold search, cost-agnostic surrogates are not $\H$-consistent for cost-sensitive classification problems.
\begin{theoremE}[][end]\label{thm:cost-agnostic-not-H-consistent}
    For the binary cost-sensitive classification task $\ell^\alpha$ parameterized by $\alpha \neq \frac 1 2$ and linear hypothesis class $\Hlin$, the pair $(\Ltargetagnostic, \sign(\cdot + \tau))$ is not $\H$-consistent with respect to $\ell^\alpha$ over $\Q_{\Ltargetagnostic, \H} \cap \Q_{\ell, \H}$ for any threshold $\tau \in \reals$.
\end{theoremE}
\begin{proofE}
    Since $\Hlin \subset \H$, we leverage the distribution $P$ in Example~\ref{ex:binary-csc}, which is contained in $\Q_{\Ltargetagnostic, \H} \cap \Q_{\ell^{\alpha},\H}$.
    Since there is no $\tau \in \reals$ such that $\psi_\tau \circ h^{ag} = \psi \circ h^{aw}$, let the region $S := \{x \in \X : \psi_\tau \circ h^{ag}(x) \neq \psi \circ h^{aw}(x)\} \subset \X$ denote disagreement between the models, and observe that $P_X(S) > 0$.
    For all $x \in S$, $\psi_\tau \circ h^{aw}(x)$ is strictly larger than the Bayes optimal for the cost sensitive problem, and therefore $\psi_\tau \circ h^{aw}(x)$ does not optimize $\E_{(X,Y) \sim P}\ell^{\alpha}(h(X), Y)$ over $h : \X \to \{-1,1\}$ measurable, contradicting $\H$-consistency.
\end{proofE}
Theorem~\ref{thm:cost-agnostic-not-H-consistent} is similar to, but stronger than the canonical result of \citet{long_consistency_2013}.
First, our result changes settings from (cost-agnostic) multiclass classification to cost-sensitive binary classification (and can be generalized to cost-sensitive multiclass classification).
Second, \citeauthor{long_consistency_2013} focuses on \emph{realizable} $\H$-consistency, whereas we give results under $P$-minimizability assumptions; this nuance is discussed further by \citet{zhang_bayes_2020}. 


The choice of parameterization, i.e., the class $\H$, is important. Specifically, even though we say $\Hlin$ is the class of ``linear'' classifiers, we define it as $\Hlin := \{h : h(x) = \sigma(\langle w,x \rangle + b)\}$.
If it were instead defined as $\Hlin' := \{h : \langle w,x \rangle + b\}$, the distribution in \Cref{ex:binary-csc} may not be in $\Q_{L, \Hlin'}$ for certain surrogates $L$, even though the optimal decision boundary is linear. 
As an example, consider a cost-sensitive surrogate $\Ltargetaware$ that is piecewise linear and convex (analyzed in more detail in~\Cref{sec:embeddings-H-consistent}).
To minimize $\Ltargetaware$, the linear model must then output a single value (e.g., -1) for every point left of the optimal decision boundary, but a function of the form  $\langle w,x \rangle + b$ can yield unbounded values in $\reals$.

\section{Embeddings are $\H$-consistent}\label{sec:embeddings-H-consistent}
In the previous section, we used cross-entropy to show that $\Hall$-consistent surrogate link pairs for a target loss are not necessarily $\H$-consistent.
This result even holds though the cost-sensitive classification and cost-agnostic surrogate losses are both $P$-minimizable. 
That is, both the $\ell$-Bayes optimal, and $\Ltargetagnostic$-Bayes optimal classifiers are in $\H$. In contrast, we proceed to show that there exists a class of surrogates tailored to cost-sensitive classification that is $\H$-consistent under the same distributional assumptions.

Specifically, we focus on a class of cost-sensitive surrogate loss functions called \emph{Embeddings}~\citep{finocchiaro_embedding_2024}.
We show that, for a cost-sensitive Embedding $\Ltargetaware$, when the $\ell$-Bayes optimal and $\Ltargetaware$-Bayes optimal classifiers are in $\H$, then there exists a link $\psi$ such that $(\Ltargetaware, \psi)$ are $\H$-calibrated with respect to the target loss $\ell$, which implies $\H$-consistency via \Cref{thm:steinwart}. 
One feature of Embeddings losses that simplifies this analysis is that these losses allow us to use \emph{property elicitation}, i.e., we only have to examine direct minimizers of these losses (argmins) instead of their approximate minimizers (limits of sequences approaching the infimum).



The Embeddings framework was first introduced by \citet{finocchiaro_embedding_2024} as a method of constructing cost-sensitive surrogate losses for the class of target losses that can be written as a cost matrix, which includes cost-sensitive classification.
In essence, an Embedding is a convex relaxation of a cost-sensitive target mapped into $\reals^d$ such that the Bayes risks of the surrogate and cost-sensitive loss match for all distributions over labels.
As with all all surrogates, Embeddings enable prediction in $\reals^d$ rather than the discrete set of reports $\R$.
Examples of Embeddings include the hinge loss embedding the cost-agnostic classification task via the $\varphi$ function being the identity, and a weighted hinge embedding the cost-sensitive binary classification $\ell^\alpha$.

\begin{definE}[Representative set][all end]\label{def:representative-set}
We say $\Sc \subseteq \R$ is representative for a loss $\ell: \R \times \Y \to \reals_+$ if we have $\argmin_{r \in \R} \E_{Y\sim p}\ell(r, Y) \cap \Sc \neq \emptyset$ for all $p \in \simplex$.
\end{definE}

\begin{definition}[{\citet[Theorem 10]{finocchiaro_embedding_2024}}]\label{def:embeddings}
A polyhedral (piecewise linear and convex) loss $L: \reals^d \times \Y \to \reals_+$ \emph{embeds} a target loss $\ell: \R \times \Y \to \reals_+$ if there exists a representative set $\Sc$ (Definition~\ref{def:representative-set}) for $\ell$ and injective embedding $\varphi : \Sc \to \reals^d$ such that (i) for all $r \in \Sc$ and $y \in \Y$, we have $\ell(r,y) = L(\varphi(r), y)$, and (ii) for all $p \in \simplex, r \in \Sc$, we have
\begin{align}
    r \in \argmin_{r^* \in \R} \E_{Y \sim p}\ell(r^*, Y) &\iff \varphi(r) \in \argmin_{u \in \reals^d} \E_{Y \sim p}L(u, Y)~.\label{eq:embedding}
\end{align}
\end{definition}

One of the arguments against designing cost-sensitive surrogates has historically been that the process is labor-intensive and rarely yields consistency guarantees.
However, the Embeddings framework provides a surrogate construction yielding $\Hall$-consistency guarantees, mitigating some of these concerns.
Moreover, the Embeddings framework also yields a link function $\psi:\reals^d \to \R$ such that $(\Ltargetaware, \psi)$ is $\Hall$-calibrated with respect to $\ell$~\citep[Construction 1, Theorem 2]{finocchiaro_embedding_2024}.
Moreover, we show this pair $(\Ltargetaware, \psi)$ is additionally $\H$-calibrated with respect to $\ell$ under the assumption that the $\Ltargetaware$-Bayes optimal classifier is in $\H$.

\begin{definE}[Hoffman constant][all end]\label{def:hoffman-constant}
    Given a matrix $A \in \reals^{m \times n}$, there exists some smallest $H(A) := \alpha \geq 0$ called the \emph{Hoffman constant} (with respect to $\| \cdot \|_q$) such that, for all $b \in \reals^m$ and $x \in \reals^n$
    \begin{align*}
        d_q(x, S(A,b)) \leq H(A) \|(Ax-b)_+\|_q~,
    \end{align*}
    where $S(A,b) := \{x \in \reals^n \mid Ax \leq b\}$.
\end{definE}

\begin{lemmaE}[{\citep{finocchiaro_embedding_2024}} Lemma 44][all end]\label{lem:hoffman-constant}
    Let $L : \reals^d \times \Y \to \reals_+$ be a polyhedral loss with $\Gamma : p \mapsto \argmin_{u \in \reals^d} \E_{Y\sim p} L(u,Y)$.
    Then for any fixed $x$ and $p \in \simplex$ such that $p = P(\cdot \mid x)$, there exists some smallest constant $H_{L,p} \geq 0$ such that $H_{L,p}(\C_{L}(h,x,p) - \C^*{L,\H}(p)) \geq d_\infty(h(x), \Gamma(p))$ for all $h \in \H$.
\end{lemmaE}

In order to prove that Embeddings are $\H$-calibrated in~\Cref{thm:embeddings-H-calibrated} below, we first introduce the notion of an $\alpha$-separated link that we will use in the proof.
The idea of an $\alpha$-separated link is to ensure that any surrogate report $u \in \reals^d$ with distance (in the $\infty$-norm) $< \alpha$ to the minimizer of the surrogate embedding should link to the correct decision.


\begin{definE}[$\alpha$-separated link][normal]\label{def:alpha-sep-link}
    Fix surrogate loss $L : \reals^d \times \Y \to \reals_+$ and cost-sensitive loss $\ell : \R \times \Y \to \reals_+$, and denote $\Gamma(p) := \argmin_{u \in \reals^d} \E_{Y \sim p} L(u,Y)$ and $\gamma(p) := \argmin_{r \in \R} \E_{Y\sim p} \ell(r,Y)$.
    A link $\psi : \reals^d \to \R$ is \emph{$\alpha$-separated} (for $\alpha > 0)$ with respect to $\Gamma$ and $\gamma$ (or $L$ and $\ell$) if, for all $u \in \reals^d$ and $p \in \simplex$ with $\psi(u) \not \in \gamma(p)$, $d_\infty(u, \Gamma(p)) \geq \alpha$, where $d_\infty (u,A) = \inf_{a \in A} \|u-a\|_\infty$.
\end{definE}

Now, the proof of $\H$-calibration of the Embeddings loss below proves the contrapositive of the $\H$-calibration definition by showing that a non-zero error in the target loss $\ell$ transfers to a non-zero error in the cost-sensitive Embeddings loss $\L$:

\begin{theoremE}[Embeddings are $\H$-calibrated][normal]\label{thm:embeddings-H-calibrated}
    Let cost-sensitive polyhedral surrogate $L$ embed $\ell$.
    Moreover, let $\psi$ be an $\alpha$-separated link function for a small $\alpha > 0$\footnote{\citep[Theorem 18]{finocchiaro_embedding_2024} shows such an $\alpha$-separated link exists.}.
    Then the pair $(L,\psi)$ is $\H$-calibrated with respect to $\ell$ over $\Q_{L, \H} \cap \Q_{\ell, \H}$.
\end{theoremE}
\begin{proofEnd}
    

    Fix any $x \in \X$, $p \in \simplex$, $\epsilon > 0$, and $P \in \Q_{L,\H} \cap \Q_{\ell, \H}$.
    
    Now consider any $h$ such that $\C_\ell(\psi \circ h,x,p) - \C_\ell^*(x,p) > \epsilon$.
    
    Then, to prove $\H$-calibration using~\Cref{eq:H-calibration}, it suffices to show that $\C_L(h,x,p) - \C^*_L(x,p) > 0$.
    
    
    To show this, we begin by noting that since the target regret is non-zero, i.e., $\C_\ell(\psi \circ h, x,p) - \C^*_\ell(x,p) > \epsilon$, we know that we are not making the optimal prediction in the target loss space, i.e., $\psi \circ h(x) \notin \gamma(p)$, because $\C_{\ell}(\gamma(p), x, p) = \condriskstar{\ell}{\psi \circ \H}$.
    
    We can now use the fact that $\psi$ is $\alpha$-separated from~\Cref{def:alpha-sep-link} to show that $\psi \circ h(x) \notin \gamma(p)$ implies that there is a gap between the current prediction and the optimal prediction in the surrogate space, i.e., $d_{\infty}(h(x), \Gamma(p)) \geq \alpha > 0$.

    
    

    Moreover, since $L$ is polyhedral, by its convexity, we can use Lemma~\ref{lem:hoffman-constant} in the Appendix to show that there is some constant (the Hoffman constant) $H_{L,p} \geq 0$ that allows us to show that this separation in the surrogate space translates to a non-zero surrogate loss regret: 
    \begin{align*}
        H_{L,p} \cdot \left(\C_L(f, x, p) - \C_L^*(x,p) \right) &\geq d_\infty(f(x), \Gamma(p)) \\ 
        &\geq \alpha > 0
    \end{align*}
    Since $H_{L,p} \geq 0$, we must have $\C_L(f, x, p) - \C_L^*(x,p) > 0$, and $\H$-calibration follows.
\end{proofEnd}


Finally, we now apply Theorem~\ref{thm:steinwart} to translate the $\H$-calibration of polyhedral Embeddings in minimizable settings to $\H$-consistency.




\begin{corollary}[Embeddings are $\H$-consistent]\label{thm:embeddings-H-consistent}
    Let a polyhedral $\Ltargetaware$ embed $\ell$ via an $\alpha$-separated link function $\psi$. Then the pair $(\Ltargetaware, \psi)$ is $\H$-consistent with respect to $\ell$ over $\Q_{\Ltargetaware, \H} \cap \Q_{\ell, \H}$.
\end{corollary}
\paragraph{Limitations} Above, we have shown that for distributions $P \in \Q_{\ell, \H} \cap \Q_{L,\H}$, minimizing our surrogate loss $\Ltargetaware$ leads to minimizing our target loss $\ell$. However, we would ideally like to show something stronger---for any distribution $P \in \Q_{\ell,\H}$, minimizing some surrogate loss $\Ltargetaware$ will lead to the optimal classifier $h^*$ for the target problem. 
Concretely, this is equivalent to either proving consistency for distributions $P \in \Q_{\ell, \H} \setminus \Q_{L,\H}$, or showing that $\Q_{\ell, \H} \setminus \Q_{L,\H} = \emptyset$.
\btw{CAREFUL... note that for this example, $P \in (\Q_{\Ltargetagnostic,\Hlin} \cap \Q_{\ell^\alpha, \Hlin})$ but $P \not \in \Q_{\Ltargetaware, \Hlin}$}


\begin{table*}[h]
    \centering
    \begin{tabular}{clcc}
    \toprule
    Dataset & Loss & Cost-Sensitive Loss ($\downarrow$) & 0-1 Accuracy ($\uparrow$)\\
    \midrule
    \multirow{5}{*}{\makecell{Synthetic\\\textcolor{black!40}{(2-Class)}}} & Cross-entropy & 0.412 ± 0.016 & \textbf{0.815 ± 0.006} \\
    & Cross-entropy + Post-processing & 0.395 ± 0.015 & 0.801 ± 0.008 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & 0.428 ± 0.009 & 0.596 ± 0.010 \\
    & Embeddings + Softmax & \textbf{0.353 ± 0.008} & 0.711 ± 0.008 \\
    & Scaled Cross-entropy & 0.364 ± 0.009 & 0.678 ± 0.010 \\
    \midrule
    \multirow{5}{*}{\makecell{German\\Credit\\\textcolor{black!40}{(2-Class)}}} & Cross-entropy & 0.796 ± 0.020 & \textbf{0.692 ± 0.006} \\
    & Cross-entropy + Post-processing & 0.685 ± 0.019 & 0.602 ± 0.011 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.589 ± 0.013} & 0.607 ± 0.006 \\
    & Embeddings + Softmax & 0.625 ± 0.013 & 0.582 ± 0.006 \\
    & Scaled Cross-entropy & 0.627 ± 0.012 & 0.503 ± 0.007 \\
    \midrule
    \multirow{4}{*}{\makecell{German\\Credit\\w/ Deferral}} & Cross-entropy & 0.699 ± 0.018 & -\\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.490 ± 0.008} & -\\
    & Embeddings + Softmax & 0.538 ± 0.011 & -\\
    & Scaled Cross-entropy & 0.581 ± 0.016 & -\\
    \midrule
    \multirow{4}{*}{\makecell{Student\\Performance\\\textcolor{black!40}{(3-Class)}}} & Cross-entropy & 0.785 ± 0.018 & \textbf{0.683 ± 0.006} \\
    & Cross-entropy + Post-processing & 0.841 ± 0.020 & 0.668 ± 0.007 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.679 ± 0.016} & 0.678 ± 0.006 \\
    & Embeddings + Softmax & 0.792 ± 0.017 & 0.624 ± 0.007 \\
    & Scaled Cross-entropy & 0.782 ± 0.022 & 0.674 ± 0.007 \\
    \midrule
    \multirow{4}{*}{\makecell{Diabetes\\\textcolor{black!40}{(3-Class)}}} & Cross-entropy & 0.726 ± 0.031 & 0.828 ± 0.007 \\
    & Cross-entropy + Post-processing & 0.741 ± 0.036 & \textbf{0.832 ± 0.010} \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.709 ± 0.028} & 0.788 ± 0.009 \\
    & Embeddings + Softmax & 0.716 ± 0.033 & 0.807 ± 0.011 \\
    & Scaled Cross-entropy & 0.746 ± 0.037 & 0.797 ± 0.010 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Main Results}. The table above summarizes the performance of linear models trained on different loss functions, datasets (500 samples), and metrics. The results are presented in the form of (Expected Value $\pm$ Standard Error of the Mean). We find that cost-sensitive surrogates consistently outperform their cost-agnostic counterparts.}
    \label{tab:results}
\end{table*}

\section{Experiments}\label{sec:experiments}
We have shown how cost-sensitive surrogate loss functions $\Ltargetaware$ outperform cost-agnostic losses $\Ltargetagnostic$ under the assumption that both target and surrogate are $P$-minimizable over $\H$. 
However, this assumption cannot typically be verified in practice, so a natural question is to ask how well cost-sensitive surrogates perform on realistic data. In this section, we train linear models on different loss functions and classification tasks to answer this question and document the results below.

\noindent\textbf{Loss Functions}
\begin{itemize}[itemsep=2pt,parsep=2pt,partopsep=0pt,topsep=0pt,leftmargin=1.2em]
    \item \textbf{Cross-entropy (CE):} This is the most common loss function used for classification tasks and we use it as our baseline. In addition to simply training our models on cross-entropy and using the $\argmax$ operation to predict the class with the highest score, we also to emulate ``clever'' post-processing, computing a weighted $\argmax$ where the weights are randomly sampled, and the weight which leads to the best performance on the validation set is chosen. This is equivalent to ``post-hoc thresholding''(\cref{sec:XE-not-H-consistent}) for binary classification but also extends to multiclass classification.
    \item \textbf{Scaled Cross-entropy:} This is a common cost-sensitive modification of the cross-entropy loss used for classification in imbalanced data and cost-sensitive settings. The class scores are re-weighted in this loss based on the average misclassification cost from the cost matrix for that outcome. 
    For example, for the cost matrix $\ell = \begin{bmatrix}0&3&5\\1& 0& 3\\ 3& 1& 0\end{bmatrix}$, the weights for each of the classes are $\frac{4}{3}$, $\frac{4}{3}$, and $\frac{8}{3}$.
    \item \textbf{Embeddings (\citet{finocchiaro_embedding_2024}):} This is the cost-sensitive surrogate loss function we analyze in~\cref{sec:embeddings-H-consistent}. It uses the cost matrix to create a piecewise linear and convex loss function that generalizes the hinge loss to cost-sensitive classification. In addition to the standard version of the loss, we also use a `softmax' version of the loss function in which the scores of different classes are not directly predicted. Instead, the predicted point is parameterized as a convex combination of ``embedding points,'' one for each class. In this version, we predict the coefficients of the convex combination. We use this version of the loss function to make the linear model $h \in \Hlin'$ behave like a logistic model $h \in \Hlin$.
\end{itemize}

\noindent\textbf{Datasets}
We evaluate the loss functions above on our example in~\cref{sec:XE-not-H-consistent} and three cost-sensitive tasks from the UCI repository~\cite{dua17uci}.
\begin{itemize}[itemsep=2pt,topsep=0pt,partopsep=0pt,parsep=2pt,leftmargin=1.2em]
    \item \textbf{Synthetic:} This is the task from \Cref{sec:XE-not-H-consistent} with $\alpha = \frac{1}{6}$.
    \item \textbf{German Credit~\cite{statlog_144}:} The goal is to differentiate between those with ``good'' and ``bad'' credit risks based on 20 observed attributes like credit history and requested loan amount. They also specify a cost matrix of $\ell = \begin{bmatrix}0&5\\1& 0\end{bmatrix}$. Additionally, we consider a variant in which you can choose to defer by paying a constant cost of $\frac{1}{2}$ units.
    \item \textbf{Student Performance~\cite{valentim21predict}:} The goal is to predict whether a student is going to either (a) drop out, (b) continued enrollment (e.g., > 4 years), or (c) graduate from their undergraduate degree at the end of the expected duration of the course. There are 36 features, such as program choice and past academic performance. We use the cost matrix $\ell = \begin{bmatrix}0&3& 5\\1& 0& 3\\ 3& 1& 0\end{bmatrix}$.
    \item \textbf{Diabetes~\cite{teboul22diabetes}:} The goal is to predict whether an individual either (a) has no diabetes, (b) is pre-diabetic, or (c) has diabetes based on 21 lifestyle features like physical activity level and history of certain medical conditions like high blood pressure. We use the same cost matrix as student performance.
\end{itemize}

\noindent\textbf{Metrics}
\begin{itemize}[itemsep=2pt,topsep=0pt,partopsep=0pt,parsep=2pt,leftmargin=1.2em]
    \item \textbf{Cost-Sensitive Loss (CSL):} This is our metric of interest. To compute this value, we first take the element-wise product of the confusion matrix of the predictions and cost matrix of the task. We then add up all the values of the resultant matrix.
    \item \textbf{0-1 Accuracy} This is a secondary metric of interest. It is equivalent to the cost-sensitive loss when any type of misclassification has a cost of 1 unit.
\end{itemize}

\subsection{Main Results}
Our experiments use a randomly selected set of 500 samples from each dataset. The models are trained on each loss function using gradient descent, and model selection is performed based on the validation loss. The results are averaged over 100 random seeds that control the choice of samples from the dataset and initial model weights. We summarize our main results in~\Cref{tab:results}. 
\begin{itemize}[itemsep=2pt,topsep=0pt,partopsep=0pt,parsep=2pt,leftmargin=1.2em]
    \item \textbf{Cost-Sensitive Losses Outperform Cost-Agnostic Losses + Post-Processing:}
    We see that CE maximizes the accuracy but not the CSL. Moreover, we see that post-processing improves the CSL but does not close the gap between CE and cost-sensitive losses (Scaled CE and Embeddings).  
    \item \textbf{Embeddings Outperforms Scaled CE:} From among the cost-sensitive losses, we see that training on the Embeddings loss leads to better performance than the Scaled CE loss. For datasets with three or more classes, this may be because of the fact that Embeddings is $\H$-consistent, while Scaled CE is not. However, it is interesting to see that the Embeddings loss significantly outperforms Scaled CE even in the case of binary classification on the German Credit dataset.
    \item \textbf{Embeddings Parameterization Is Important:} As highlighted at the end of~\cref{sec:XE-not-H-consistent}, the distribution $P$ in~\Cref{ex:binary-csc} is not minimizable for the Embeddings loss and a linear model $\Hlin' \coloneqq \{h : \langle w,x \rangle + b\}$. On the other hand, it is minimizable for the Scaled CE and Embeddings + Softmax losses.
    This observe is salient in our empirical results on Synthetic data.
\end{itemize}

\subsection{Ablations}
In addition to the experiments above, we run additional experiments to test the sensitivity of our conclusions to two important confounders:
\begin{itemize}[itemsep=2pt,topsep=0pt,partopsep=0pt,parsep=2pt,leftmargin=1.2em]
    \item \textbf{More Samples:} In~\Cref{tab:results_more_data}, we see that the loss values (and their variance) decrease across the board when we increase the number of samples used to train our linear model. However, except for the Diabetes dataset, the performance order does not change. For the diabetes dataset, all the cross-entropy-based models seem to do better than Embeddings. While it is unclear why this is the case, we hypothesize that it has something to do with the smoothness of CE loss, but leave further exploration to future work. 
    \item \textbf{Bigger Models} While our theoretical results are motivated by smaller models, we want to understand whether these trends extend to larger models. To test this, we train a 4-layer fully-connected ReLU network with 100-dimensional hidden layers.
    In~\Cref{tab:results_nn}, we see that the Embeddings loss remains the best (or close to the best for German Credit) even for these more expressive models. We hypothesize that this results from a limited effective model capacity due to the small size of the datasets we consider in our experiments. This suggests that, in addition to small model settings, cost-sensitive losses may also be useful in small data settings.
    
\end{itemize}

\section{Discussion}
We study the suitability of cost-sensitive surrogate losses for cost-sensitive classification with small models through the lens of $\H$-consistency.
We show that, even under moderately weak assumptions about the expressiveness of $\H$, there is a strict gap in the cost-sensitive performance of cost-sensitive surrogates and cost-agnostic surrogates aided with threshold search in post-processing.
In order to supplement our theoretical findings, we additionally contrast model performance on a handful of binary and multiclass cost-sensitive classification problems, generally finding that cost-sensitive surrogates outperform cost-agnostic losses with small models.
\paragraph{Future work}
Many directions of future work are opened by this initial study: first, we only proved $\H$-consistency results for the Embeddings framework of \citet{finocchiaro_embedding_2024}.
Characterizing which cost-sensitive surrogates these results extend to, notably which smooth surrogates, remains an interesting open problem that involves extending their results leveraging polyhedral surrogate structure.
We conjecture this might be shown via indirect property elicitation and refinement.
Moreover, this work seems to indicate connections to the predict-then-optimize literature~\citep{elmachtoub_smart_2022,wilder_melding_2019}, but showing these connections again requires careful consideration.
Finally, further exploring the limits and necessity of distributional assumptions poses an interesting line of inquiry.

\newpage

\section*{Acknowledgments}
We would like to thank Han Bao in helpful discussions shaping the early stages of this work.
This material is based upon work supported by the National Science Foundation under Award No. 2202898.
Sanket Shah was supported through the Siebel Scholars program.

\bibliographystyle{plainnat}
\bibliography{refs,zotero-jessie}

\clearpage
\newpage

\appendix
\onecolumn
\section{Deferred Proofs}
\printProofs

\section{Additional Empirical Results}

\begin{table*}[h]
    \centering
    \begin{tabular}{clcc}
    \toprule
    Dataset & Loss & Cost-Sensitive Loss & 0-1 Accuracy\\
    \midrule
    \multirow{5}{*}{\makecell{Synthetic\\\textcolor{black!40}{(10000 Samples)}}} & Cross-Entropy & 0.427 ± 0.004 & \textbf{0.829 ± 0.001} \\
    & Cross-Entropy + Post-processing & 0.391 ± 0.004 & 0.807 ± 0.002 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & 0.389 ± 0.002 & 0.641 ± 0.002 \\
    & Embeddings + Softmax & \textbf{0.304 ± 0.001} & 0.767 ± 0.001 \\
    & Scaled Cross-Entropy & 0.313 ± 0.002 & 0.741 ± 0.002 \\
    \midrule
    \multirow{5}{*}{\makecell{German\\Credit\\\textcolor{black!40}{(1000 Samples)}}} & Cross-Entropy & 0.756 ± 0.016 & \textbf{0.720 ± 0.004} \\
    & Cross-Entropy + Post-processing & 0.626 ± 0.012 & 0.619 ± 0.009 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.556 ± 0.006} & 0.625 ± 0.003 \\
    & Embeddings + Softmax & 0.572 ± 0.007 & 0.592 ± 0.004 \\
    & Scaled Cross-Entropy & 0.573 ± 0.005 & 0.515 ± 0.007 \\
    \midrule
    \multirow{4}{*}{\makecell{German\\Credit\\w/ Deferral}} & Cross-Entropy & 0.682 ± 0.015 & - \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.470 ± 0.004} & - \\
    & Embeddings + Softmax & 0.492 ± 0.006 & - \\
    & Scaled Cross-Entropy & 0.513 ± 0.006 & - \\
    \midrule
    \multirow{4}{*}{\makecell{Student\\Performance\\\textcolor{black!40}{(4424 Samples)}}} & Cross-Entropy & 0.639 ± 0.006 & \textbf{0.752 ± 0.002} \\
    & Cross-Entropy + Post-processing & 0.624 ± 0.006 & 0.730 ± 0.002 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.539 ± 0.003} & 0.725 ± 0.001 \\
    & Embeddings + Softmax & 0.584 ± 0.004 & 0.699 ± 0.001 \\
    & Scaled Cross-Entropy & 0.637 ± 0.006 & 0.737 ± 0.002 \\
    \midrule
    \multirow{4}{*}{\makecell{Diabetes\\\textcolor{black!40}{(253680 Samples)}}} & Cross-Entropy & 0.673 ± 0.002 & \textbf{0.844 ± 0.000} \\
    & Cross-Entropy + Post-processing & \textbf{0.655 ± 0.002} & 0.839 ± 0.000 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & 0.688 ± 0.001 & 0.775 ± 0.001 \\
    & Embeddings + Softmax & 0.675 ± 0.002 & 0.783 ± 0.004 \\
    & Scaled Cross-Entropy & 0.660 ± 0.002 & 0.817 ± 0.001 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablations: More Samples}. The table above summarizes the performance of linear models trained on different loss functions, datasets and metrics. The results are presented in the form of (Expected Value $\pm$ Standard Error of the Mean).}
    \label{tab:results_more_data}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{clcc}
    \toprule
    Dataset & Loss & Cost-Sensitive Loss & 0-1 Accuracy\\
    \midrule
    \multirow{5}{*}{\makecell{Synthetic\\\textcolor{black!40}{(10000 Samples)}}} & Cross-Entropy & 0.370 ± 0.003 & \textbf{0.828 ± 0.001} \\
    & Cross-Entropy + Post-processing & 0.331 ± 0.003 & 0.811 ± 0.001 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.303 ± 0.002} & 0.771 ± 0.001 \\
    & Embeddings + Softmax & \textbf{0.303 ± 0.002} & 0.775 ± 0.001 \\
    & Scaled Cross-Entropy & 0.317 ± 0.002 & 0.722 ± 0.002 \\
    \midrule
    \multirow{5}{*}{\makecell{German\\Credit\\\textcolor{black!40}{(1000 Samples)}}} & Cross-Entropy & 0.747 ± 0.012 & \textbf{0.710 ± 0.004} \\
    & Cross-Entropy + Post-processing & 0.614 ± 0.011 & 0.615 ± 0.008 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & 0.639 ± 0.008 & 0.471 ± 0.017 \\
    & Embeddings + Softmax & 0.608 ± 0.008 & 0.552 ± 0.014 \\
    & Scaled Cross-Entropy & \textbf{0.602 ± 0.008} & 0.465 ± 0.012 \\
    \midrule
    \multirow{4}{*}{\makecell{German\\Credit\\w/ Deferral}} & Cross-Entropy & 0.655 ± 0.013 & - \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.504 ± 0.005} & - \\
    & Embeddings + Softmax & 0.493 ± 0.004 & - \\
    & Scaled Cross-Entropy & 0.507 ± 0.008 & - \\
    \midrule
    \multirow{4}{*}{\makecell{Student\\Performance\\\textcolor{black!40}{(4424 Samples)}}} & Cross-Entropy & 0.622 ± 0.004 & \textbf{0.746 ± 0.001} \\
    & Cross-Entropy + Post-processing & 0.589 ± 0.004 & 0.717 ± 0.003 \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.567 ± 0.004} & 0.717 ± 0.001 \\
    & Embeddings + Softmax & 0.574 ± 0.004 & 0.718 ± 0.002 \\
    & Scaled Cross-Entropy & 0.677 ± 0.004 & 0.732 ± 0.001 \\
    \midrule
    \multirow{4}{*}{\makecell{Diabetes\\\textcolor{black!40}{(253680 Samples)}}} & Cross-Entropy & 0.754 ± 0.001 & \textbf{0.842 ± 0.000} \\
    & Cross-Entropy + Post-processing & 0.754 ± 0.001 & \textbf{0.842 ± 0.000} \\
    \arrayrulecolor{black!40} \cmidrule{2-4} \arrayrulecolor{black}
    & Embeddings & \textbf{0.645 ± 0.001} & 0.824 ± 0.000 \\
    & Embeddings + Softmax & 0.652 ± 0.002 & 0.837 ± 0.000 \\
    & Scaled Cross-Entropy & 0.657 ± 0.001 & 0.817 ± 0.000 \\
    \bottomrule
    \end{tabular}
    \caption{\textbf{Ablations: Larger Model}. The table above summarizes the performance of 2-layer neural  trained on different loss functions, datasets and metrics. The results are presented in the form of (Expected Value $\pm$ Standard Error of the Mean).}
    \label{tab:results_nn}
\end{table*}


\begin{table*}[h]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{clcccccc}
    \toprule
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Loss} & \multicolumn{3}{c}{Linear} & \multicolumn{3}{c}{NN} \\
    \cmidrule(lr){3-5} \cmidrule(lr){6-8}
    & & Train & Val & Test & Train & Val & Test
    \\
    \midrule
    \multirow{5}{*}{\makecell{Synthetic\\\textcolor{black!40}{(Low Overfitting)}}} & Cross-Entropy & 0.430 ± 0.003 & 0.429 ± 0.004 & 0.427 ± 0.004 & 0.362 ± 0.002 & 0.366 ± 0.004 & 0.370 ± 0.003 \\
    & Cross-Entropy + Post-processing & 0.396 ± 0.003 & 0.391 ± 0.004 & 0.391 ± 0.004 & 0.331 ± 0.002 & 0.327 ± 0.003 & 0.331 ± 0.003 \\
    \arrayrulecolor{black!40} \cmidrule{2-8} \arrayrulecolor{black}
    & Embeddings & 0.388 ± 0.001 & 0.391 ± 0.002 & 0.389 ± 0.002 & 0.297 ± 0.001 & 0.304 ± 0.002 & 0.303 ± 0.002 \\
    & Embeddings + Softmax & 0.302 ± 0.001 & 0.309 ± 0.002 & 0.304 ± 0.001 & 0.295 ± 0.001 & 0.303 ± 0.002 & 0.303 ± 0.002 \\
    & Scaled Cross-Entropy & 0.314 ± 0.002 & 0.316 ± 0.002 & 0.313 ± 0.002 & 0.310 ± 0.001 & 0.318 ± 0.002 & 0.317 ± 0.002 \\
    \midrule
    \multirow{5}{*}{\makecell{German\\Credit\\\textcolor{black!40}{(High Overfitting)}}} & Cross-Entropy & 0.590 ± 0.005 & 0.734 ± 0.016 & 0.756 ± 0.016 & 0.427 ± 0.011 & 0.739 ± 0.013 & 0.747 ± 0.012 \\
    & Cross-Entropy + Post-processing & 0.579 ± 0.004 & 0.570 ± 0.009 & 0.626 ± 0.012 & 0.470 ± 0.008 & 0.549 ± 0.008 & 0.614 ± 0.011 \\
    \arrayrulecolor{black!40} \cmidrule{2-8} \arrayrulecolor{black}
    & Embeddings & 0.476 ± 0.003 & 0.574 ± 0.006 & 0.556 ± 0.006 & 0.505 ± 0.019 & 0.607 ± 0.010 & 0.639 ± 0.008 \\
    & Embeddings + Softmax & 0.381 ± 0.003 & 0.574 ± 0.007 & 0.572 ± 0.007 & 0.383 ± 0.017 & 0.573 ± 0.008 & 0.608 ± 0.008 \\
    & Scaled Cross-Entropy & 0.473 ± 0.006 & 0.555 ± 0.011 & 0.573 ± 0.005 & 0.499 ± 0.014 & 0.587 ± 0.009 & 0.602 ± 0.008 \\
    \midrule
    \multirow{4}{*}{\makecell{German\\Credit\\w/ Deferral}} & Cross-Entropy & 0.543 ± 0.005 & 0.676 ± 0.015 & 0.682 ± 0.015 & 0.382 ± 0.008 & 0.642 ± 0.012 & 0.655 ± 0.013 \\
    \arrayrulecolor{black!40} \cmidrule{2-8} \arrayrulecolor{black}
    & Embeddings & 0.432 ± 0.004 & 0.466 ± 0.004 & 0.470 ± 0.004 & 0.473 ± 0.008 & 0.497 ± 0.006 & 0.504 ± 0.005 \\
    & Embeddings + Softmax & 0.333 ± 0.003 & 0.488 ± 0.005 & 0.492 ± 0.006 & 0.427 ± 0.009 & 0.469 ± 0.004 & 0.493 ± 0.004 \\
    & Scaled Cross-Entropy & 0.411 ± 0.002 & 0.509 ± 0.007 & 0.513 ± 0.006 & 0.372 ± 0.010 & 0.501 ± 0.007 & 0.507 ± 0.008 \\
    \midrule
    \multirow{4}{*}{\makecell{Student\\Performance\\\textcolor{black!40}{(High Overfitting)}}} & Cross-Entropy & 0.546 ± 0.002 & 0.614 ± 0.006 & 0.639 ± 0.006 & 0.436 ± 0.004 & 0.613 ± 0.006 & 0.622 ± 0.004 \\
    & Cross-Entropy + Post-processing & 0.565 ± 0.003 & 0.575 ± 0.006 & 0.624 ± 0.006 & 0.458 ± 0.003 & 0.566 ± 0.005 & 0.589 ± 0.004 \\
    \arrayrulecolor{black!40} \cmidrule{2-8} \arrayrulecolor{black}
    & Embeddings & 0.519 ± 0.001 & 0.530 ± 0.004 & 0.539 ± 0.003 & 0.409 ± 0.003 & 0.556 ± 0.005 & 0.567 ± 0.004 \\
    & Embeddings + Softmax & 0.461 ± 0.001 & 0.569 ± 0.004 & 0.584 ± 0.004 & 0.341 ± 0.002 & 0.566 ± 0.005 & 0.574 ± 0.004 \\
    & Scaled Cross-Entropy & 0.542 ± 0.002 & 0.615 ± 0.006 & 0.637 ± 0.006 & 0.522 ± 0.005 & 0.661 ± 0.006 & 0.677 ± 0.004 \\
    \midrule
    \multirow{4}{*}{\makecell{Diabetes\\\textcolor{black!40}{(Low Overfitting)}}} & Cross-Entropy & 0.668 ± 0.001 & 0.672 ± 0.002 & 0.673 ± 0.002 & 0.752 ± 0.000 & 0.751 ± 0.001 & 0.754 ± 0.001 \\
    & Cross-Entropy + Post-processing & 0.652 ± 0.001 & 0.652 ± 0.001 & 0.655 ± 0.002 & 0.752 ± 0.000 & 0.751 ± 0.001 & 0.754 ± 0.001 \\
    \arrayrulecolor{black!40} \cmidrule{2-8} \arrayrulecolor{black}
    & Embeddings & 0.684 ± 0.001 & 0.684 ± 0.001 & 0.688 ± 0.001 & 0.607 ± 0.001 & 0.642 ± 0.001 & 0.645 ± 0.001 \\
    & Embeddings + Softmax & 0.664 ± 0.002 & 0.672 ± 0.003 & 0.675 ± 0.002 & 0.614 ± 0.002 & 0.649 ± 0.002 & 0.652 ± 0.002 \\
    & Scaled Cross-Entropy & 0.656 ± 0.001 & 0.658 ± 0.001 & 0.660 ± 0.002 & 0.639 ± 0.000 & 0.654 ± 0.001 & 0.657 ± 0.001 \\
    \bottomrule
    \end{tabular}    }
    \caption{Overfitting in Linear Models vs. 2-Layer Neural Networks.}
    \label{tab:results_overfitting}
\end{table*}

\end{document}
