\section{Background and Related Work}
Our main question of interest is if \hlc[myYellow]{small} ML models trained using a \hlc[myBlue]{cost-sensitive surrogate} loss functions outperform models trained on cost-agnostic surrogates combined with some \hlc[myPurple]{``clever'' post-processing}.

To understand this question, we must formalize the three emphasized pieces.
First, we formalize \hlc[myYellow]{small} through the optimized model class $\H$, which enables us to restrict ourselves to model classes like the set of linear models $\Hlin := \{h : \exists \vec w,b \text{ s.t. } h(x) = \sigma(\langle w,x \rangle + b)\}$ or two-layer neural networks.
\hlc[myBlue]{Cost-sensitive surrogates} refer to continuous loss functions $L : \reals^d \times \Y \to \reals_+$ designed with the intention of emulating a cost-sensitive classification problem $\ell^\alpha : \R \times \Y \to \reals_+$.
We designate a surrogate as cost-sensitive if it is $\H$-consistent, where approaching the optimal surrogate loss implies approaching the optimal cost-sensitive loss. 
This notion is a prerequisite for establishing any PAC-learning bounds.
\hlc[myPurple]{Mason and Reid (2007)} show that using proper scoring rules can be beneficial in certain situations.

To understand $\H$-calibration, we use the conditional risk, which is the expected loss conditioned on input $x$.

\begin{definition}[Conditional risk]\label{def:inner-risk}
    For a loss function $L : \R \times \Y \to \reals_+$ and hypothesis function $h : \X \to \R$, the conditional risk of the hypothesis $h$ is given
    \begin{align*}
        \condrisk L (h, x, p) &:= \sum_{y} p_y L(h(x), y) 
        = \E_{Y \sim p} L(h(x), Y)
    \end{align*}
    Moreover, we denote the \emph{minimal conditional risk} by
    \begin{align*}
        \condriskstar L \H(x, p) := \inf_{h' \in \H} \condrisk L (h', x, p)
    \end{align*}
\end{definition}

\begin{definition}[$\H$-calibration]\label{def:H-calibration}
    A loss function $L : \reals^d \times \Y \to \reals_+$ and link $\psi : \reals^d \to \R$ pair are $\H$-calibrated with respect to a target loss $\ell : \R \times \Y \to \reals_+$ over the set $\P \subseteq \simplex$ if, for any $\epsilon > 0$, $p \in \P$, and $x \in \X$, there exists some $\delta > 0$ such that
    \begin{align*}
        \condrisk L(h,x, p) - \condriskstar L \H (x,p) &< \delta \implies  \condrisk \ell(\psi \circ h,x, p) - \condriskstar \ell {\psi \circ \H} (x,p) < \epsilon
    \end{align*}
    for all $h \in \H$.
\end{definition}

Early works of \citet{Steinwart2009} show that $\Hall$-consistency and $\Hall$-calibration are equivalent in finite outcome settings, where $\Hall$ is the set of all measurable hypotheses.

The following Theorem shows the equivalence between $\H$-calibration and $\H$-consistency under $P$-minimizability, closing the first gap.

\begin{theorem}[{\citet{Steinwart2009}}]\label{thm:steinwart}
    Given a surrogate loss $L : \reals^d \times \Y \to \reals_+$, link function $\psi: \reals^d \to \R$, and target loss $\ell : \R \times \Y\to \reals_+$, assume that $P \in \Q_{L, \H} \cap \Q_{\ell,\H}$.
    Furthermore, assume there exists a function $b \in \L^1(P_X)$ and measurable function $\delta(\epsilon, \cdot) : \X \to \reals_{>0}, \epsilon > 0$ such that 
    \begin{align}
    \begin{aligned}
        \C_{\ell}(h,x,p) &\leq \C^*_{\ell}(x,p) + b(x) \label{eq:ptwise-min}
    \end{aligned}\\
        \begin{aligned}
        \C_{L}(h,x,p) &\leq \C^*_{L}(x,p) + \delta(\epsilon, x)\\
        \implies \C_{\ell}(\psi \circ h,x,p) &\leq \C^*_{\ell}(x,p) + \epsilon
        \label{eq:H-calibration}
        \end{aligned}
    \end{align}
    for all $x \in \X$, $\epsilon > 0$, and $h \in \H$.
    Then for all $\epsilon > 0$, there exists a $\delta > 0$ such that for all $h \in \H$, we have
    \begin{align}
    \begin{aligned}
        \E_{P} L(h(X), Y) &< \inf_{h^* \in \H} \E_{P} L(h^*(X), Y) + \delta \\
          \implies \E_{P} \ell(\psi \circ h(X), Y) &< \inf_{h^* \in \H} \E_{P} \ell(\psi \circ h^*(X), Y) + \epsilon
        \label{eq:H-consistency}
        \end{aligned}
    \end{align}
\end{theorem}

Proper Scoring Losses, as discussed by \citet{Gneiting2007}, are a class of loss functions that satisfy the assumption of Equation~\eqref{eq:H-calibration}.