@misc{teboul22diabetes,
  author       = {Teboul, Alex},
  title        = {{Diabetes Health Indicators Dataset
}},
  year         = {2022},
  howpublished = {Kaggle}
}

@misc{valentim21predict,
  author       = {Realinho, Valentim and Vieira Martins, Mónica and Machado, Jorge and Baptista, Luís},
  title        = {{Predict Students' Dropout and Academic Success}},
  year         = {2021},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5MC89}
}


@misc{dua17uci,
  author = {Dua, Dheeru and Graff, Casey},
  year = {2017},
  title = {UCI Machine Learning Repository},
  url = {http://archive.ics.uci.edu/ml},
  institution = {University of California, Irvine, School of Information and Computer Sciences}
}


@article{guruswami2009hardness,
  title={Hardness of learning halfspaces with noise},
  author={Guruswami, Venkatesan and Raghavendra, Prasad},
  journal={SIAM Journal on Computing},
  volume={39},
  number={2},
  pages={742--765},
  year={2009},
  publisher={SIAM}
}

@article{ben2003difficulty,
  title={On the difficulty of approximately maximizing agreements},
  author={Ben-David, Shai and Eiron, Nadav and Long, Philip M},
  journal={Journal of Computer and System Sciences},
  volume={66},
  number={3},
  pages={496--514},
  year={2003},
  publisher={Elsevier}
}

@misc{statlog_144,
  author       = {Hofmann, Hans},
  title        = {{Statlog (German Credit Data)}},
  year         = {1994},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5NC77}
}

@inproceedings{long2013consistency,
  title={Consistency versus realizable H-consistency for multiclass classification},
  author={Long, Phil and Servedio, Rocco},
  booktitle={International conference on machine learning},
  pages={801--809},
  year={2013},
  organization={PMLR}
}
,
@article{steinwart2007compare,
  title={How to compare different loss functions and their risks},
  author={Steinwart, Ingo},
  journal={Constructive Approximation},
  volume={26},
  number={2},
  pages={225--287},
  year={2007},
  publisher={Springer}
}
,
@article{kuznetsov2014multi,
  title={Multi-class deep boosting},
  author={Kuznetsov, Vitaly and Mohri, Mehryar and Syed, Umar},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014},
url={https://papers.nips.cc/paper_files/paper/2014/hash/7bb060764a818184ebb1cc0d43d382aa-Abstract.html}
}
,
@ARTICLE{Błasiok_2023,title={A Unifying Theory of Distance from Calibration},year={2023},author={Jarosław Błasiok and Parikshit Gopalan and Linlin Hu and Venkatesan Guruswami},doi={10.1145/3564246.3585182},pmid={null},pmcid={null},mag_id={4376639572},journal={Symposium on the Theory of Computing}}
@ARTICLE{Hébert-Johnson_2018,title={Multicalibration: Calibration for the (Computationally-Identifiable) Masses},year={2018},author={Úrsula Hébert-Johnson and Ursula Hebert-Johnson and Michael P. Kim and Michael P. Kim and Omer Reingold and Omer Reingold and Guy N. Rothblum and Guy N. Rothblum},doi={null},pmid={null},pmcid={null},mag_id={2803648878},journal={International Conference on Machine Learning}}
@ARTICLE{Błasiok_2023,title={When Does Optimizing a Proper Loss Yield Calibration?},year={2023},author={Jarosław Błasiok and Parikshit Gopalan and Linlin Hu and Venkatesan Guruswami},doi={10.48550/arxiv.2305.18764},pmid={null},pmcid={null},mag_id={4378942775},journal={arXiv.org}}
@ARTICLE{Guo_2017,title={On Calibration of Modern Neural Networks},year={2017},author={Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},doi={null},pmid={null},pmcid={null},mag_id={2950953798},journal={International Conference on Machine Learning}}
@ARTICLE{Ho-Nguyen_2020,title={Risk Guarantees for End-to-End Prediction and Optimization Processes},year={2020},author={Nam Ho-Nguyen and F. Kılınç-Karzan},doi={10.1287/mnsc.2022.4321},pmid={null},pmcid={null},mag_id={null},journal={Management Sciences}}
@ARTICLE{Gopalan_2024,title={On Computationally Efficient Multi-Class Calibration},year={2024},author={Parikshit Gopalan and Lunjia Hu and G. Rothblum},doi={10.48550/arxiv.2402.07821},pmid={null},pmcid={null},mag_id={null},journal={arXiv.org}}
@ARTICLE{Kull_2019,title={Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration},year={2019},author={Meelis Kull and Meelis Kull and Miquel Perelló Nieto and Miquel Perello Nieto and Markus Kängsepp and Markus Kängsepp and Telmo M. Silva Filho and Telmo de Menezes e Silva Filho and Telmo Silva Filho and Hao Song and Hao Song and Peter A. Flach and Peter A. Flach},doi={null},pmid={null},pmcid={null},mag_id={2971118045},journal={Neural Information Processing Systems}}
@ARTICLE{Zhao_2021,title={Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration},year={2021},author={Shengjia Zhao and Michael P. Kim and Roshni Sahoo and Tengyu Ma and Stefano Ermon},doi={null},pmid={null},pmcid={null},mag_id={null},journal={Neural Information Processing Systems}}
@ARTICLE{Gupta_2021,title={Top-label calibration and multiclass-to-binary reductions},year={2021},author={Chirag Gupta and Aaditya Ramdas},doi={null},pmid={null},pmcid={null},mag_id={null},journal={International Conference on Learning Representations}}
@ARTICLE{Michael_2022,title={The Calibration Generalization Gap},year={2022},author={Carrell, A. Michael and A. Michael Carrell and Mallinar, Neil and Neil Mallinar and Lucas, James and James M. Lucas and Nakkiran, Preetum and Venkatesan Guruswami},doi={10.48550/arxiv.2210.01964},pmid={null},pmcid={null},mag_id={4303648016},journal={arXiv.org}}
@ARTICLE{Huang_2020,title={An Experimental Investigation of Calibration Techniques for Imbalanced Data},year={2020},author={Lanlan Huang and Lanlan Huang and Junkai Zhao and Junkai Zhao and Bing Zhu and Bing Zhu and Bing Zhu and Hao Chen and Hao Chen and Seppe vanden Broucke and Seppe Vanden Broucke and Seppe vanden Broucke},doi={10.1109/access.2020.3008150},pmid={null},pmcid={null},mag_id={3042124509},journal={IEEE Access}}
@ARTICLE{Dwork_2022,title={Beyond Bernoulli: Generating Random Outcomes that cannot be Distinguished from Nature},year={2022},author={C. Dwork and Michael P. Kim and Omer Reingold and G. Rothblum and G. Yona},doi={null},pmid={null},pmcid={null},mag_id={null},journal={International Conference on Algorithmic Learning Theory}}
@ARTICLE{Noarov_2023,title={The Statistical Scope of Multicalibration},year={2023},author={Georgy Noarov and Aaron Roth},doi={null},pmid={null},pmcid={null},mag_id={null},journal={International Conference on Machine Learning}}
@ARTICLE{Derr_2024,title={Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret},year={2024},author={Rabanus Derr and Robert C. Williamson},doi={10.48550/arxiv.2401.14483},pmid={null},pmcid={null},mag_id={null},journal={arXiv.org}}
@ARTICLE{Globus-Harris_2023,title={Multicalibration as Boosting for Regression},year={2023},author={Ira Globus-Harris and Declan Harrison and Michael Kearns and Aaron Roth and Jessica Sorrell},doi={10.48550/arxiv.2301.13767},pmid={null},pmcid={null},mag_id={4318908720},journal={International Conference on Machine Learning}}
@ARTICLE{Elkan_2001,title={The foundations of cost-sensitive learning},year={2001},author={Charles Elkan and Charles Elkan},doi={null},pmid={null},pmcid={null},mag_id={167016754},journal={International Joint Conference on Artificial Intelligence}}
@ARTICLE{He_2009,title={Learning from Imbalanced Data},year={2009},author={Haibo He and Haibo He and E.A. Garcia and E.A. Garcia},doi={10.1109/tkde.2008.239},pmid={null},pmcid={null},mag_id={2118978333},journal={IEEE Transactions on Knowledge and Data Engineering}}
@ARTICLE{Dmochowski_2010,title={Maximum Likelihood in Cost-Sensitive Learning: Model Specification, Approximations, and Upper Bounds},year={2010},author={Jacek Dmochowski and Jacek Dmochowski and Paul Sajda and Paul Sajda and Lucas C. Parra and Lucas C. Parra},doi={10.5555/1756006.1953037},pmid={null},pmcid={null},mag_id={2121946739},journal={Journal of Machine Learning Research}}
@ARTICLE{Maloof_2003,title={Learning When Data Sets are Imbalanced and When Costs are Unequal and Unknown},year={2003},author={Marcus A. Maloof and Marcus A. Maloof},doi={null},pmid={null},pmcid={null},mag_id={2123977051},journal={null}}
@ARTICLE{Mao_2023,title={H-Consistency Bounds: Characterization and Extensions},year={2023},author={Anqi Mao and M. Mohri and Yutao Zhong},doi={null},pmid={null},pmcid={null},mag_id={null},journal={Neural Information Processing Systems}}
@ARTICLE{Long_2013,title={Consistency versus Realizable H-Consistency for Multiclass Classification},year={2013},author={Philip M. Long and Phil Long and Rocco A. Servedio and Rocco A. Servedio},doi={null},pmid={null},pmcid={null},mag_id={107701773},journal={International Conference on Machine Learning},abstract={A consistent loss function for multiclass classification is one such that for any source of labeled examples, any tuple of scoring functions that minimizes the expected loss will have classification accuracy close to that of the Bayes optimal classifier. While consistency has been proposed as a desirable property for multiclass loss functions, we give experimental and theoretical results exhibiting a sequence of linearly separable data sources with the following property: a multiclass classification algorithm which optimizes a loss function due to Crammer and Singer (which is known not to be consistent) produces classifiers whose expected error goes to 0, while the expected error of an algorithm which optimizes a generalization of the loss function used by LogitBoost (a loss function which is known to be consistent) is bounded below by a positive constant.

We identify a property of a loss function, realizable consistency with respect to a restricted class of scoring functions, that accounts for this difference. As our main technical results we show that the Crammer-Singer loss function is realizable consistent for the class of linear scoring functions, while the generalization of LogitBoost is not. Our result for LogitBoost is a special case of a more general theorem that applies to several other loss functions that have been proposed for multiclass classification.}}
@ARTICLE{Zhang_2020,title={Bayes Consistency vs. H -Consistency: The Interplay between Surrogate Loss Functions and the Scoring Function Class.},year={2020},author={Mingyuan Zhang and Shivani Agarwal and Shivani Agarwal},doi={null},pmid={34305367},pmcid={null},mag_id={3099777040},journal={Neural Information Processing Systems}}
@ARTICLE{Awasthi_2022,title={H-Consistency Bounds for Surrogate Loss Minimizers},year={2022},author={Pranjal Awasthi and Anqi Mao and M. Mohri and Yutao Zhong},doi={null},pmid={null},pmcid={null},mag_id={null},journal={International Conference on Machine Learning}}
@ARTICLE{Domingos_1999,title={MetaCost: a general method for making classifiers cost-sensitive},year={1999},author={Pedro Domingos and Pedro Domingos},doi={10.1145/312129.312220},pmid={null},pmcid={null},mag_id={2058732827},journal={Knowledge Discovery and Data Mining}}
@ARTICLE{Branco_2015,title={A Survey of Predictive Modelling under Imbalanced Distributions},year={2015},author={Paula Branco and Paula Branco and Luı́s Torgo and Luís Torgo and Rita P. Ribeiro and Rita P. Ribeiro},doi={null},pmid={null},pmcid={null},mag_id={2101156862},journal={arXiv: Learning}}
@ARTICLE{Steinwart_2007,title={How to Compare Different Loss Functions and Their Risks},year={2007},author={Ingo Steinwart},doi={10.1007/s00365-006-0662-3},pmid={null},pmcid={null},mag_id={2052448032},journal={Constructive Approximation}}
@ARTICLE{Wang_2023,title={Unified Binary and Multiclass Margin-Based Classification},year={2023},author={Yutong Wang and Clayton Scott},doi={10.48550/arxiv.2311.17778},pmid={null},pmcid={null},mag_id={null},journal={arXiv.org}}
@ARTICLE{Scott_2012,title={Calibrated asymmetric surrogate losses},year={2012},author={Clayton Scott and Clayton Scott},doi={10.1214/12-ejs699},pmid={null},pmcid={null},mag_id={1969623397},journal={Electronic Journal of Statistics}}
@ARTICLE{Frongillo_2021,title={Surrogate Regret Bounds for Polyhedral Losses},year={2021},author={Rafael M. Frongillo and Bo Waggoner},doi={null},pmid={null},pmcid={null},mag_id={null},journal={Neural Information Processing Systems}}
@ARTICLE{Mahdavi_2014,title={Binary Excess Risk for Smooth Convex Surrogates.},year={2014},author={Mehrdad Mahdavi and Mehrdad Mahdavi and Lijun Zhang and Lijun Zhang and Lijun Zhang and Lijun Zhang and Rong Jin and Rong Jin},doi={null},pmid={null},pmcid={null},mag_id={1483788985},journal={arXiv: Learning}}
,
@article{awasthi2021calibration,
  title={Calibration and consistency of adversarial surrogate losses},
  author={Awasthi, Pranjal and Frank, Natalie and Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9804--9815},
  year={2021}
}
,
@article{awasthi2021finer,
  title={A finer calibration analysis for adversarial robustness},
  author={Awasthi, Pranjal and Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  journal={arXiv preprint arXiv:2105.01550},
  year={2021}
}