\section{Additional Real world Experiment Details}
\subsection{Implementation Details}
\label{app:real_world}
In real world experiments, we investigate how the optimal CoT length varies between these two models and with different question difficulties. To create solutions with varying step lengths, we follow \citep{complex-prompt} by using in-context examples (8-shots) with three different levels of complexity to guide the model in generating solutions with different step counts. For each set of in-context examples, we sample 20 times, resulting in a total of 60 samples per question. 

When calculating the number of steps, we separate the full reasoning chain using \texttt{"\textbackslash n"}\citep{complex-prompt} and remove empty lines caused by \texttt{"\textbackslash n\textbackslash n"}. Then we consider the total number of lines as the CoT length. Since the MATH dataset questions are challenging, leading to high variability in final CoT lengths, we scale the CoT length using \texttt{length = length // 5}. As we are primarily observing trends, this scaling is considered acceptable.

When evaluating the results, questions with accuracy $<0.01$ or $>0.99$ (indicating all incorrect or all correct responses) are excluded, as their accuracy does not vary with step length changes.
\subsection{Task Difficulty \textit{v.s.} Optimal CoT Lengths}
\label{app:difficulty}
To further investigate the relationship between task difficulty and optimal CoT lengths on real world datasets, we conduct experiments on different models. The results (Figure \ref{fig:qwen} and \ref{fig:llama}) are impressive that results on all models show a significant correlation between the task difficulties and optimal lengths.

\begin{figure*}[ht]
\centering

\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Realworld/step_vs_acc_qwen7b.pdf}
    \caption{Qwen2.5 7B Instruct}
    \label{fig:step_vs_acc_qwen7b}
\end{subfigure}%
% \hspace{0.03\textwidth}  % Adjusted spacing between figures
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Realworld/step_vs_acc_qwen14b.pdf}
    \caption{Qwen2.5 14B Instruct}
    \label{fig:step_vs_acc_qwen14b}
\end{subfigure}
\caption{Evaluation between task difficulties and optimal CoT lengths on MATH datasets with Qwen2.5 Series Instruct models. }
\label{fig:qwen}
\end{figure*}


\begin{figure*}[ht]
\centering

\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Realworld/step_vs_acc_llama8b.pdf}
    \caption{Llama 3.1 8B Instruct}
    \label{fig:step_vs_acc_llama8b}
\end{subfigure}%
% \hspace{0.03\textwidth}  % Adjusted spacing between figures
\begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Realworld/step_vs_acc_llama70b.pdf}
    \caption{Llama 3.1 70B Instruct}
    \label{fig:step_vs_acc_llama70b}
\end{subfigure}
\caption{Evaluation between task difficulties and optimal CoT lengths on MATH datasets with LLama3.1 Series Instruct models. 
}
\label{fig:llama}
\end{figure*}
\section{Synthetic Arithmetic Problem Discussions}
\label{app:task}
\subsection{Contrast to vanilla arithmetic problem}
\textbf{Why pruning?}
Initially, we intended to create a synthetic dataset for regular arithmetic tasks, but we quickly realized that the computation tree for such tasks is uncontrollable. For example, consider the task $1*2 + 3*4$. We hoped to compute $2$ operators in one step, but found it impossible because the addition needs to be computed after the two multiplications, and we cannot aggregate two multiplications in one subtask. Therefore, pruning the computation tree becomes essential.

\textbf{Why only focusing on addition?}
There are two reasons why we focus on arithmetic tasks involving only addition: first, it simplifies pruning, as the order of operations can be controlled solely by parentheses; second, it facilitates the computation of sub-tasks, since parentheses do not affect the final result, and the model only needs to compute the sum of all the numbers when solving a sub-task. We aim for the model to handle longer sub-tasks, thereby allowing a broader study of the impact of CoT length.

\textbf{Will the simplified synthetic dataset impact the diversity of the data?}
We need to clarify that even with pruning, the structure of the expressions will still vary because swapping the left and right child nodes of each non-leaf node in the computation tree results in different expressions. When $T > 30$, the number of possible variations exceeds $1 \times 10^9$.
% \item \textbf{Q:} 
%  \item \textbf{A:} Yes! like name capital and switch bubble, but lack of variation
\subsection{Same Subtask Difficulty}
\label{app:same_length}

A Chain-of-Thought (CoT) containing sub-tasks of varying difficulty can be seen as a mixture of CoTs with the same difficulty level. On the other hand, allowing CoTs to generate sub-tasks of different lengths increases the overall difficulty of CoTs of the same length, making them harder to study. Third, under Assumption \ref{ass:error_e}, convexity analysis shows that the final accuracy function (Proposition~\ref{pro: general_acc}) is concave. Therefore, to maximize accuracy, all sub-tasks should have the same difficulty level.




\section{Subtask Loss}
\label{app:subtask}
As we observed in training losses, the loss of subtask generation tokens (e.g. $1+2$) for the easiest subtask($t=1$) is about 3 times larger than the hardest subtask ($t=12$), while the loss ratio for subtask answer tokens is $1e4$. Therefore, it is acceptable for taking the subtask error rate constant with $t$.

Besides, there is no obvious pattern showing the model sizes affect the subtask loss. Moreover, the smallest model and the largest model have almost the same subtask loss. Therefore, in our settings, we take model size as irrelevant with the subtask error rate.





\section{Synthetic Experiment Details}
\label{app:syn}
In default, we train different models(layers ranging from 5 to 9) on the same dataset, which included mixed questions with total operators $T \in [12,80]$ and random sampled CoT solutions with each step operators $t \in [1,12]$. All other parameters are kept the same with the huggingface GPT-2 model. During the training process, the CoT indicator token \texttt{<t>} is also trained, so that during test-time, we can let the model decide which type of CoT it will use by only prompting the model with the question. For each model, we train $25000$ iterations with batch size that equals 256. During test-time, we test 100 questions for each $T$ and $t$. All experiments can be conducted on one NVIDIA A800 80G GPU. 




\section{General Error Functions}
\label{app:assumption}
\begin{assumption}
     $E(N,M,T)$ satisfies the following reasonable conditions:
\begin{itemize}
\label{ass:error_e}
    \item $0 < E(N=1,M,T) < 1$
    \item $\lim_{N \rightarrow +\infty}E(N,M,T) = 0$
    \item $E(N,M,T)$ is monotonically deceasing with $N$, since more detailed decomposition leads to easier subtask.
    \item $E(N,M,T)$ is convex with $N$, since the benefits of further decomposing an already fine-grained problem($N$ is large) are less than the benefits of decomposing a problem that has not yet been fully broken down($N$ is small).
    \item $E(N,M,T)$ is monotonically deceasing with $M$, since stronger models have less subtask error rate.
    \item $E(N,M,T)$ is monotonically increasing with $T$, since harder total task leads to harder subtask while $N,M$ are the same.
\end{itemize}
\end{assumption}

\begin{assumption}
\label{ass:error_sigma}
     $\sigma(T)$ is monotonically increasing with $T$
\end{assumption}

\section{Proof}
\label{app:proof}
In this section, we provide the proofs for all theorems.

\subsection{Proof of Proposition~\ref{pro: general_acc}}
\finalacc*
\begin{proof}
In each subtask \( t_i \), which contains \( t \) operators, there are \( 2t + 1 \) tokens (as the number of numerical tokens is one more than the number of operators). Therefore, the accuracy of each subtask is given by
\begin{equation}
\label{eq:task_acc}
    P(t_i= t_i^* | H_{i-1}, q, \theta) = \left(1 - \sigma(T)\right)^{2t + 1}. 
\end{equation}

In our theoretical analysis, for simplicity, we allow \( t \) to be a fraction, defined as \( t = \frac{T}{N} \), and assume that each subtask has the same level of difficulty given \( T \) and \( N \). Under this assumption, we have the final accuracy:
     \begin{align}
        A(N) &= P(a_N=a_N^*|q,\theta) \\
        &= \prod^N_{i=1}P(t_i=t_i^*|H_{i-1},q,\theta)P(a_i=a_i^*|t_i,H_{i-1},q,\theta)\\
        &= \prod^N_{i=1}\left(1 - \sigma(T)\right)^{2t + 1}\left(1-E(N,M,T)\right)\\
        &= \left(1 - \sigma(T)\right)^{N(2t + 1)}\left(1-E(N,M,T)\right)^N\\
        &= \left(1 - \sigma(T)\right)^{2T}\left((1-E(N,M,T))(1-\sigma(T))\right)^N\\
        &= \alpha\left((1-E(N,M,T))(1-\sigma(T))\right)^N
    \end{align}
\end{proof}
\subsection{Proof of Theorem~\ref{thm:simple_optimal_N}}
\OptimalN*
\begin{proof}
Given Eq.~(\ref{eq:final_acc}) that
\begin{align}
    A(N) =  \alpha\left(\left(1-\frac{T}{C}\right)\left(1-\frac{T}{NM}\right)\right)^N
\end{align}

We consider function

\begin{equation}
    f(x) \;=\; \Bigl[\bigl(1 - \tfrac{T}{Mx}\bigr)\,\bigl(1 - \tfrac{T}{C}\bigr)\Bigr]^{x}.
    \label{eq:f}
\end{equation}



For convenience, define

\[
g(x) \;=\; \ln\bigl(f(x)\bigr) \;=\; x \;\ln\!\Bigl[\bigl(1 - \tfrac{T}{Mx}\bigr)\,\bigl(1 - \tfrac{T}{C}\bigr)\Bigr].
\]

Thus,
\[
g'(x) 
= \Bigl[\ln\bigl(1 - \tfrac{T}{Mx}\bigr) \;+\; \frac{T}{Mx\,\bigl(1 - \tfrac{T}{Mx}\bigr)}\Bigr]
\;+\;
\ln\bigl(1 - \tfrac{T}{C}\bigr).
\]
Set \(g'(x) = 0\):

\[
\ln\Bigl[\bigl(1 - \tfrac{T}{Mx}\bigr)\,\bigl(1 - \tfrac{T}{C}\bigr)\Bigr]
\;+\;
\frac{T}{Mx\bigl(1 - \tfrac{T}{Mx}\bigr)} 
= 0.
\]

Let $A \;=\; \frac{1}{1 - \frac{T}{Mx}},$
then we have

\[
\ln\Bigl[\,\bigl(1 - \tfrac{T}{C}\bigr)\Bigr]
\;+\;
A-1
= \ln(A).
\]

Let \(z := 1 - T/C\). (Since \(T/C < 1\), \(z = 1 - T/C > 0\).) By moving terms, we have:

\[
-\frac{z}{e}\;=\; -A\exp(-A).
\]

Therefore, 
\[
A = -W^{-1}(-\frac{z}{e}) = -Z,
\]
Finally, we have

$$N(M,T) = x = \frac{TZ}{M(Z+1)}$$


Here \(W(\cdot)\) is the \textbf{Lambert W function}, and for \(0 < 1 - \tfrac{T}{C} < 1\), the argument \(\alpha = -\frac{1 - T/C}{e}\) lies in the interval \(\bigl(-\tfrac{1}{e}, 0\bigr)\). This means there are two real branches \(W_0\) and \(W_{-1}\) in that domain, but since $\frac{Z}{Z+1}>0$,we have $Z<-1$. Therefore, we only take the solution on branch \(W_{-1}\).
\label{pr:4.2}
\end{proof}

\subsection{Proof of Corollary~\ref{cor:NwithT}}
\NwithT*

\begin{proof}
    We begin the proof by incorporating the notation from \ref{pr:4.2}.
    We have \[
g'(x) 
= \Bigl[\ln\bigl(1 - \tfrac{T}{Mx}\bigr) \;+\; \frac{T}{Mx\,\bigl(1 - \tfrac{T}{Mx}\bigr)}\Bigr]
\;+\;
\ln\bigl(1 - \tfrac{T}{C}\bigr),
\] and $x^*(T)$ such that $g'(x^*(T))=0$.

Let $F(x^*(T),T) = g'(x^*(T)) = 0 ,\forall T. $
We want to see how \(x^*(T)\) changes as \(T\) changes, therefore we take total derivative w.r.t.\ \(T\).  By the chain rule,
\[
0 
\;=\; 
\frac{d}{dT}\,F\bigl(x^*(T),\,T\bigr)
\;=\;
\underbrace{\frac{\partial F}{\partial x}\bigl(x^*(T),T\bigr)}_{\text{call this }F_x}\;\cdot\;\frac{\partial x^*}{\partial T}\bigl(T\bigl)
\;\;+\;\;
\underbrace{\frac{\partial F}{\partial T}\bigl(x^*(T),T\bigr)}_{\text{call this }F_T}.
\] 
Hence
\[
\frac{\partial x^*}{\partial T}\bigl(T\bigl)
\;=\;
-\;\frac{F_T\Bigl(x^*(T),\,T\Bigr)}{\,F_x\,\Bigl(x^*(T),\,T\Bigr)}.
\]
So the sign of \(x'^*(T)\) is the opposite of the sign of \(F_T\), provided \(F_x\neq 0\).  

Since
\begin{equation}
    \,F_x\,\Bigl(x,\,T\Bigr) = -\frac{T^2}{x(Mx-T)^2} < 0, \forall x > 0,
\end{equation}

all we need to prove is 

\begin{equation}
    \,F_T\,\Bigl(x^*(T),\,T\Bigr) = \frac{T}{(Mx^*(T)-T)^2}-\frac{1}{C-T}>0.
\end{equation}

That is 

\begin{equation}
    \frac{\sqrt{T(C-T)}+T}{M}>x^*(T).
\end{equation}

Let $x_0(T) = \frac{\sqrt{T(C-T)}+T}{M}$ be the test point.

According to Lemma~\ref{lm:test_point}, $F(x_0(T),T)<0$. Since $F(x^*(T),T) = 0, \text{ and } \,F_x\,\Bigl(x^*(T),\,T\Bigr)<0, $ we have $x_0(T)>x^*(T)$.

Thus, $\,F_T\,\Bigl(x^*(T),\,T\Bigr)>0$ holds and we have proved our corollary with $\frac{\partial x^*}{\partial T}\bigl(T\bigl) > 0$.

\end{proof}

\subsection{Proof of Theorem~\ref{thm:general}}
\general*

\begin{proof}
    (1) Since $0<A(N)<(1-\sigma(T))^N$, and $\lim_{N\rightarrow +\infty} (1-\sigma(T))^N = 0$, $\lim_{N\rightarrow +\infty} A(N,M,T) = 0$

    (2) Let $g(x)$ denote $E(x,M,T)$ and define $f(x) = \ln A(x)$. Then,  
\begin{align}
    f'(x) 
    &= \ln (1-\sigma(T)(1-g(x)))  - \frac{xE'(x)}{1-E(x)}\\
    &< \ln (1-\sigma(T)(1-g(x)))  + 2, \quad \text{(since $E$ is convex and $x = N \geq 1$)}
\end{align}
If $A(N)$ attains its maximum at some point $N^*>1$, then $\ln (1-\sigma(T))  + 2 > 0$. Otherwise, we would have $f'(x)<\ln (1-\sigma(T))  + 2 \leq 0 \;\forall x>1$, leading to a contradiction. 

Thus, it follows that $e^2(1-\sigma(T))>1$. 

Now, define $N(M,T) = E^{-1}\left(1-\frac{1}{e^2(1-\sigma(T))}\right)$, which satisfies  
\[
\ln (1-\sigma(T)(1-g(N(M,T))))  + 2 = 0.
\]
If there exists $x^*<N(M,T)$ such that $f'(x^*) = 0$, then we obtain  
\[
0 = f'(x^*)< \ln (1-\sigma(T)(1-E(x)))  + 2<0,
\]
which is a contradiction. Hence, the assumption that $x^*<N(M,T)$ must be false. 

Therefore, we conclude that $x^* = N^* > N(M,T)$.

\end{proof}

\subsection{Technical Lemmas}
\begin{lemma}
\label{lm:test_point}
    Let \( F(x) \) be defined as  
    \[
    F(x) = \ln\left(1 - \frac{T}{Mx}\right) + \frac{T}{Mx\left(1 - \frac{T}{Mx}\right)} + \ln\left(1 - \frac{T}{C}\right),
    \]
    where \( T, M, C \in \mathbb{R}^+ \) satisfy the conditions:
    \begin{itemize}
        \item \( 0 < \frac{T}{C} < 0.9 \),
        \item \( 0 < \frac{T}{Mx} < 1 \).
    \end{itemize}
    
    Define \( x_0 \) as
    \[
    x_0 = \frac{\sqrt{T(C-T)}+T}{M}.
    \]
    Then, we have
    \[
    F(x_0) < 0.
    \]
\end{lemma}

\begin{proof}
    At \( x = x_0 \), note that
\[
Mx_0 = \sqrt{T(C-T)} + T.
\]

Thus,
\[
1 - \frac{T}{Mx_0} 
= 1 - \frac{T}{T+\sqrt{T(C-T)}}
= \frac{\sqrt{T(C-T)}}{T+\sqrt{T(C-T)}}.
\]

Therefore,
\[
\ln\Bigl(1-\frac{T}{Mx_0}\Bigr)
=\ln\left(\frac{\sqrt{T(C-T)}}{T+\sqrt{T(C-T)}}\right)
=\ln\sqrt{T(C-T)} - \ln\bigl(T+\sqrt{T(C-T)}\bigr).
\]

Also, observe that
\[
\frac{T}{Mx_0\Bigl(1-\frac{T}{Mx_0}\Bigr)}
=\frac{T}{(T+\sqrt{T(C-T)})\Bigl(\frac{\sqrt{T(C-T)}}{T+\sqrt{T(C-T)}}\Bigr)}
=\frac{T}{\sqrt{T(C-T)}}
=\sqrt{\frac{T}{C-T}}.
\]

It is convenient to introduce the change of variable
\[
u = \sqrt{\frac{T}{C-T}},
\]
so that
\[
T = u^2 (C-T),\quad \sqrt{T(C-T)} = u(C-T). 
\]
Then we have
$$T+\sqrt{T(C-T)} = u^2 (C-T) + u(C-T) = u(C-T)(u+1).$$


In these terms we have:
\[
\ln\sqrt{T(C-T)} = \ln\bigl[u(C-T)\bigr] = \ln u + \ln(C-T),
\]
\[
\ln\bigl(T+\sqrt{T(C-T)}\bigr) = \ln\bigl[u(C-T)(u+1)\bigr] = \ln u + \ln(C-T) + \ln(u+1),
\]
and
\[
\sqrt{\frac{T}{C-T}} = u.
\]

Finally, we have
\[
\ln\Bigl(1-\frac{T}{C}\Bigr) = -\ln(\frac{C}{C-T}) = -\ln(u^2 +1)
\]

Thus, the function \( F(x_0) \) becomes
\begin{align}
F(x_0)
&=\ln u + \ln(C-T) - \bigl(\ln u + \ln(C-T) + \ln(u+1)\bigr) + u -\ln (u^2+1)\\
&= -\ln(u+1) + u - \ln(u^2 +1)\, ,
\end{align}
where $u = \sqrt{\frac{T}{C-T}} \in \left(0,3\right).$
It is easy to show $F(x_0)<0$ when $u \in \left(0,3\right)$.
\end{proof}