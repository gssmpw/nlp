[
  {
    "index": 0,
    "papers": [
      {
        "key": "icl:Brown",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario",
        "title": "Language Models are Few-Shot Learners"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cot:Wei",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "least-to-most",
        "author": "Denny Zhou and Nathanael Sch{\\\"a}rli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Claire Cui and Olivier Bousquet and Quoc V Le and Ed H. Chi",
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "tree-of-thoughts",
        "author": "Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik R Narasimhan",
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Divide-and-Conquer",
        "author": "Yizhou Zhang and Lun Du and Defu Cao and Qiang Fu and Yan Liu",
        "title": "An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models"
      },
      {
        "key": "dacMultichoice",
        "author": "Zijie Meng and Yan Zhang and Zhaopeng Feng and Zuozhu Liu",
        "title": "DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with LLMs"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "cot-theory:wlw",
        "author": "Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang",
        "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective"
      },
      {
        "key": "cot-theory:tcs",
        "author": "Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma",
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "cot-theory:icl",
        "author": "Yingqian Cui and Pengfei He and Xianfeng Tang and Qi He and Chen Luo and Jiliang Tang and Yue Xing",
        "title": "A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "cot-theory:infor-theory",
        "author": "Jean-Francois Ton and Muhammad Faaiz Taufiq and Yang Liu",
        "title": "Understanding Chain-of-Thought in LLMs through Information Theory"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "cot-theory:gradient",
        "author": "Ming Li and Yanhong Li and Tianyi Zhou",
        "title": "What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cot-theory:allen-zhu",
        "author": "Tian Ye and Zicheng Xu and Yuanzhi Li and Zeyuan Allen-Zhu",
        "title": "Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "test-time:0",
        "author": "Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar",
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"
      },
      {
        "key": "test-time:1",
        "author": "Yanxi Chen and Xuchen Pan and Yaliang Li and Bolin Ding and Jingren Zhou",
        "title": "A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models"
      },
      {
        "key": "test-time:2",
        "author": "Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang",
        "title": "Scaling Inference Computation: Compute-Optimal Inference for Problem-Solving with Language Models"
      },
      {
        "key": "test-time:3",
        "author": "Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher R\u00e9 and Azalia Mirhoseini",
        "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "are-more-calls-you-need",
        "author": "Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis and Ion Stoica and Matei Zaharia and James Zou",
        "title": "Are More {LLM} Calls All You Need? Towards the Scaling Properties of Compound {AI} Systems"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "2+3",
        "author": "Xingyu Chen and Jiahao Xu and Tian Liang and Zhiwei He and Jianhui Pang and Dian Yu and Linfeng Song and Qiuzhi Liu and Mengfei Zhou and Zhuosheng Zhang and Rui Wang and Zhaopeng Tu and Haitao Mi and Dong Yu",
        "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
      }
    ]
  }
]