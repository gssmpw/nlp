@misc{2+3,
      title={Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs}, 
      author={Xingyu Chen and Jiahao Xu and Tian Liang and Zhiwei He and Jianhui Pang and Dian Yu and Linfeng Song and Qiuzhi Liu and Mengfei Zhou and Zhuosheng Zhang and Rui Wang and Zhaopeng Tu and Haitao Mi and Dong Yu},
      year={2024},
      eprint={2412.21187},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.21187}, 
}

@misc{Divide-and-Conquer,
      title={An Examination on the Effectiveness of Divide-and-Conquer Prompting in Large Language Models}, 
      author={Yizhou Zhang and Lun Du and Defu Cao and Qiang Fu and Yan Liu},
      year={2024},
      eprint={2402.05359},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.05359}, 
}

@misc{cot-theory:allen-zhu,
      title={Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process}, 
      author={Tian Ye and Zicheng Xu and Yuanzhi Li and Zeyuan Allen-Zhu},
      year={2024},
      eprint={2407.20311},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.20311}, 
}

@misc{cot-theory:gradient,
      title={What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective}, 
      author={Ming Li and Yanhong Li and Tianyi Zhou},
      year={2024},
      eprint={2410.23743},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23743}, 
}

@misc{cot-theory:icl,
      title={A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration}, 
      author={Yingqian Cui and Pengfei He and Xianfeng Tang and Qi He and Chen Luo and Jiliang Tang and Yue Xing},
      year={2024},
      eprint={2410.16540},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.16540}, 
}

@misc{cot-theory:infor-theory,
      title={Understanding Chain-of-Thought in LLMs through Information Theory}, 
      author={Jean-Francois Ton and Muhammad Faaiz Taufiq and Yang Liu},
      year={2024},
      eprint={2411.11984},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.11984}, 
}

@misc{cot-theory:tcs,
      title={Chain of Thought Empowers Transformers to Solve Inherently Serial Problems}, 
      author={Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
      year={2024},
      eprint={2402.12875},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12875}, 
}

@inproceedings{cot:Wei,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and ichter, brian and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {24824--24837},
 publisher = {Curran Associates, Inc.},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 volume = {35},
 year = {2022}

}

@misc{dacMultichoice,
      title={DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with LLMs}, 
      author={Zijie Meng and Yan Zhang and Zhaopeng Feng and Zuozhu Liu},
      year={2024},
      eprint={2401.05190},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.05190}, 
}

@inproceedings{icl:Brown,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{test-time:0,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@misc{test-time:1,
      title={A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models}, 
      author={Yanxi Chen and Xuchen Pan and Yaliang Li and Bolin Ding and Jingren Zhou},
      year={2024},
      eprint={2411.19477},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.19477}, 
}

@misc{test-time:3,
      title={Large Language Monkeys: Scaling Inference Compute with Repeated Sampling}, 
      author={Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher RÃ© and Azalia Mirhoseini},
      year={2024},
      eprint={2407.21787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.21787}, 
}

