\section{Related Work}
% \wyy{
% \begin{itemize}
%     \item \textbf{cot method}: All method is satisfied a decomposition and subtask solving framework and self-reflection is also kind of CoT.
%     \item \textbf{cot theory:} fancy but no one provide a guidance on how long is the best CoT
%     \item \textbf{overthinking and long cot}: some ones do researches on overthinking but not related to CoT, while others research long CoT but think CoT is the long the better. Well our work ...
% \end{itemize}
% }


\textbf{Chain of Thought}
Large Language Models (LLMs) ____ have demonstrated remarkable abilities in complex reasoning tasks by breaking down challenging problems into intermediate steps before arriving at the final answer ____. Numerous researchers have proposed various approaches to enhance the CoT reasoning capabilities of LLMs. Least-to-most prompting ____ decomposes  a complex problem into a series of simpler sub-problems, solving them sequentially, where the solution to each subproblem builds upon the answers to previously solved sub-problems. Tree of thoughts ____ enables LLMs to engage in deliberate decision-making by exploring multiple reasoning paths, self-evaluating options, and dynamically adjusting the reasoning process through backtracking or look-ahead strategies to make globally optimal choices. Similarly, Divide-and-Conquer methods ____ divide the input sequence into multiple sub-inputs, which can significantly improve LLM performance in specific tasks. Despite their differences, these methods share a common characteristic: they all treat the CoT process as a framework for decomposition and subtask-solving. Similarly, our study adopts this perspective. 


\textbf{CoT Understanding}
In addition to the methods mentioned above, many works aim to formalize the CoT process and explore why it is effective. Circuit complexity theory has been used to analyze the computational complexity of problems that transformers can solve with and without CoT, providing a theoretical understanding of CoT's effectiveness ____. ____ theoretically demonstrate that, compared to Stepwise ICL, integrating reasoning from earlier steps (Coherent CoT) enhances transformers' error correction capabilities and prediction accuracy. ____ quantify the information gain at each reasoning step in an information-theoretic perspective to understand the CoT process. Furthermore, ____ show that fast thinking without CoT results in larger gradients and greater gradient differences across layers compared to slow thinking with detailed CoT, highlighting the improved learning stability provided by the latter. ____ investigate CoT in a controlled setting by training GPT-2 models on a synthetic GSM dataset, revealing hidden mechanisms through which language models solve mathematical problems. Unlike these theoretical studies, our work focuses on the impact of different lengths of CoT on final performance and tries to understand CoT from task decomposition and error accumulation perspective. 

\textbf{Overthinking}
With the remarkable success of OpenAI's o1 model, test-time computation scaling has become increasingly important. More and more works ____ have explored the scaling laws during inference using various methods, such as greedy search, majority voting, best-of-n, and their combinations. They concluded that with a compute-optimal strategy, a smaller base model can achieve non-trivial success rates, and test-time compute can outperform larger models. This highlights the importance of designing optimal inference strategies. 

However, ____ hold a contrastive opinion that in some cases, the performance of the Best-of-N method may decline as \( N \) increases. Similarly, the \textit{overthinking} phenomenon ____ becomes more and more important as o1-like reasoning models allocate excessive computational resources to simple problems (e.g., \( 2+3=5 \)) with minimal gains. These findings indicate the need to balance computation based on model capabilities and task difficulty. In our study, we focus on different types of CoT reasoning, categorized by CoT length. Moreover, we theoretically identify a balanced CoT strategy that adapts to model size and task difficulty, optimizing performance under these constraints.