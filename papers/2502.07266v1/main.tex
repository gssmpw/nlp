
\documentclass{article} % For LaTeX2e
\usepackage{arxiv}
\usepackage{natbib}
\setlength{\parindent}{0pt}
\pdfoutput=1

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{bbm}
\usepackage{wrapfig}
% \usepackage{enumitem}

\usepackage{comment}

\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}

%##############My Packages #############
\usepackage{hyperref}
\usepackage{url}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
% \usepackage{algorithm}
% \usepackage{algorithmic}
% \usepackage{algorithm2e}
% \RestyleAlgo{boxed} % 让算法环境带有边框
\usepackage{algorithm}
\usepackage{algpseudocode}

% \RestyleAlgo{ruled}



% \usepackage{newtxtext}

%##############My Packages #############


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%##############My CMDs #############
\newcommand{\wyy}[1]{\textcolor{orange}{\#Yuyang: \textit{#1}}}
%##############My CMDS #############

\title{When More is Less: Understanding Chain-of-Thought Length in LLMs}


\author{
Yuyang Wu\thanks{Equal Contribution} \\
Peking University\\
\and
Yifei Wang\textsuperscript{*} \\ 
MIT CSAIL\\
\and
Tianqi Du \\
Peking University\\
% \and\and
\AND
Stefanie Jegelka  \\ 
TU Munich and MIT CSAIL \and
Yisen Wang \\ 
Peking University 
  % \texttt{stefje@mit.edu} 
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Chain-of-thought (CoT) reasoning enhances the multi-step reasoning capabilities of large language models (LLMs) by breaking complex tasks into smaller, manageable sub-tasks. Researchers have been exploring ways to guide models to generate more complex CoT processes to improve the reasoning ability of LLMs, such as long CoT and the test-time scaling law. However, for most models and tasks, does an increase in CoT length consistently lead to improved reasoning accuracy?
In this paper, we observe a nuanced relationship: as the number of reasoning steps increases, performance initially improves but eventually decreases. To understand this phenomenon, we provide a piece of evidence that \textit{longer reasoning processes are increasingly susceptible to noise.} We theoretically prove the existence of an optimal CoT length and derive a scaling law for this optimal length based on model capability and task difficulty. Inspired by our theory, we conduct experiments on both synthetic and real world datasets and propose Length-filtered Vote to alleviate the effects of excessively long or short CoTs. Our findings highlight the critical need to calibrate CoT length to align with model capabilities and task demands, offering a principled framework for optimizing multi-step reasoning in LLMs. 
\end{abstract}




\section{Introduction}
Large language models (LLMs) have demonstrated impressive capabilities in solving complex reasoning tasks \citep{icl:Brown,touvron2023llama}. One approach to enhance their performance on such tasks is Chain of Thought (CoT) reasoning \citep{cot:Wei}, where the model generates explicit intermediate reasoning steps before arriving at the final answer. The CoT process can be seen as a divide-and-conquer strategy \citep{Divide-and-Conquer}, where the model breaks a complex problem into simpler sub-problems, solves each one individually, and then combines the results to reach a final conclusion. It is commonly believed that longer CoT generally improves the performance, especially on more difficult tasks \citep{complex-prompt,long-step}. On the other hand, a concise CoT \citep{ConciseCoT} has been shown to incur a decreased performance penalty on math problems. But does performance consistently improve as the length of the CoT increases?

In this paper, we conduct comprehensive and rigorous experiments on synthetic arithmetic datasets and find that for \textbf{CoT length, longer is not always better} (Figure~\ref{fig:U-curve}). Specifically, by controlling task difficulty, we observe that as the reasoning path lengthens, the model's performance initially improves but eventually deteriorates, indicating the existence of an optimal length. This phenomenon can be understood by modeling the CoT process as a task decomposition and subtask-solving structure. While early-stage decomposition helps break down the problem, excessively long reasoning paths lead to error accumulation—where a single mistake can mislead the entire chain of thought.

\begin{figure*}[ht]
\centering
% 1st
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{{Figures/Synthetic/U_curve_small_model.pdf}}
    \caption{Small model with 6 layers}
    \label{fig:U_curve_small_model}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{{Figures/Synthetic/U_curve_middle_model}}
    \caption{Large model with 9 layers}
    \label{fig:U_curve_middle_model}
\end{subfigure}
\caption{Evaluation of the relationship between CoT length and final performance on synthetic arithmetic datasets. The models used are different layers of GPT-2, with all other hyperparameters unchanged. The results show that accuracy initially increases and then decreases as the number of operations per step grows (while the total number of steps decreases), indicating that both overthinking and underthinking can harm LLM reasoning abilities.}
\label{fig:U-curve}
\end{figure*}

Our experimental results also show that \textbf{the optimal CoT length is directly influenced by both model capability and task complexity}. Intuitively, for more challenging tasks, a deeper decomposition is required. Meanwhile, a more capable model can handle moderately complex subtasks in a single step, reducing the need for excessive reasoning steps. Furthermore, \textbf{as task difficulty increases, the optimal length of each reasoning step tends to grow} to prevent the total number of steps from becoming excessively long.

Then in Section~\ref{sec:theory}, we provide a formal and rigorous theoretical explanation for these findings. Under a simplified setting, we show that each of the above conclusions can be described mathematically. We also present a more general version of our theory that applies to a broader range of conditions.


Following the theoretical analyses of optimal CoT length, we further validate these insights by examining their effects on real-world LLM behavior at test-time and their influence on CoT training. In Section~\ref{sec:real}, we conduct experiments on real-world math problems using various sizes of open-source LLMs. The results further confirm that a longer CoT is not always better, but should adapt to model size and task difficulty. Furthermore, in Section~\ref{sec:solu_tr}, we train models using data with the optimal CoT length rather than randomly selected lengths. The results are striking: \textbf{a small model trained on optimal-length CoT can even outperform larger models trained on randomly chosen CoT lengths. }This highlights the significant impact of CoT length in training data on model performance.  

Inspired by both theoretical and experimental findings, we propose methods to leverage the optimal CoT length during inference. Specifically, we enhance the standard majority voting approach by introducing \textbf{Length-filtered Vote}. This adaptive method selects answers with the optimal CoT length while filtering out those that are either too short or too long.



\section{Related Work}
% \wyy{
% \begin{itemize}
%     \item \textbf{cot method}: All method is satisfied a decomposition and subtask solving framework and self-reflection is also kind of CoT.
%     \item \textbf{cot theory:} fancy but no one provide a guidance on how long is the best CoT
%     \item \textbf{overthinking and long cot}: some ones do researches on overthinking but not related to CoT, while others research long CoT but think CoT is the long the better. Well our work ...
% \end{itemize}
% }


\textbf{Chain of Thought}
Large Language Models (LLMs) \citep{icl:Brown} have demonstrated remarkable abilities in complex reasoning tasks by breaking down challenging problems into intermediate steps before arriving at the final answer \citep{cot:Wei}. Numerous researchers have proposed various approaches to enhance the CoT reasoning capabilities of LLMs. Least-to-most prompting \citep{least-to-most} decomposes  a complex problem into a series of simpler sub-problems, solving them sequentially, where the solution to each subproblem builds upon the answers to previously solved sub-problems. Tree of thoughts \citep{tree-of-thoughts} enables LLMs to engage in deliberate decision-making by exploring multiple reasoning paths, self-evaluating options, and dynamically adjusting the reasoning process through backtracking or look-ahead strategies to make globally optimal choices. Similarly, Divide-and-Conquer methods \citep{Divide-and-Conquer,dacMultichoice} divide the input sequence into multiple sub-inputs, which can significantly improve LLM performance in specific tasks. Despite their differences, these methods share a common characteristic: they all treat the CoT process as a framework for decomposition and subtask-solving. Similarly, our study adopts this perspective. 


\textbf{CoT Understanding}
In addition to the methods mentioned above, many works aim to formalize the CoT process and explore why it is effective. Circuit complexity theory has been used to analyze the computational complexity of problems that transformers can solve with and without CoT, providing a theoretical understanding of CoT's effectiveness \citep{cot-theory:wlw,cot-theory:tcs}. \citet{cot-theory:icl} theoretically demonstrate that, compared to Stepwise ICL, integrating reasoning from earlier steps (Coherent CoT) enhances transformers' error correction capabilities and prediction accuracy. \citet{cot-theory:infor-theory} quantify the information gain at each reasoning step in an information-theoretic perspective to understand the CoT process. Furthermore, \citet{cot-theory:gradient} show that fast thinking without CoT results in larger gradients and greater gradient differences across layers compared to slow thinking with detailed CoT, highlighting the improved learning stability provided by the latter. \citet{cot-theory:allen-zhu} investigate CoT in a controlled setting by training GPT-2 models on a synthetic GSM dataset, revealing hidden mechanisms through which language models solve mathematical problems. Unlike these theoretical studies, our work focuses on the impact of different lengths of CoT on final performance and tries to understand CoT from task decomposition and error accumulation perspective. 

\textbf{Overthinking}
With the remarkable success of OpenAI's o1 model, test-time computation scaling has become increasingly important. More and more works \citep{test-time:0,test-time:1,test-time:2,test-time:3} have explored the scaling laws during inference using various methods, such as greedy search, majority voting, best-of-n, and their combinations. They concluded that with a compute-optimal strategy, a smaller base model can achieve non-trivial success rates, and test-time compute can outperform larger models. This highlights the importance of designing optimal inference strategies. 

However, \citet{are-more-calls-you-need} hold a contrastive opinion that in some cases, the performance of the Best-of-N method may decline as \( N \) increases. Similarly, the \textit{overthinking} phenomenon \citep{2+3} becomes more and more important as o1-like reasoning models allocate excessive computational resources to simple problems (e.g., \( 2+3=5 \)) with minimal gains. These findings indicate the need to balance computation based on model capabilities and task difficulty. In our study, we focus on different types of CoT reasoning, categorized by CoT length. Moreover, we theoretically identify a balanced CoT strategy that adapts to model size and task difficulty, optimizing performance under these constraints.



\section{Influence of Chain-of-Thought Length on Arithmetic Tasks} 
\label{sec:arithmetic}

\begin{wrapfigure}{r}{0.5\linewidth}
  \centering
  \begin{tikzpicture}
    [scale=1, level distance=0.75cm, sibling distance=1.5cm,
     every node/.style={draw, circle, minimum size=0.5cm, inner sep=1.5pt}]
     
    \node {+}
      child { node {5} }
      child { node {+}
        child { node {4} }
        child { node {+} 
          child { node {+} 
            child { node {2} }
            child { node {1} }
          }
          child { node {3} }
        }
      };
  \end{tikzpicture}
  \caption{Computation tree of arithmetic expression $5+(4+((2+1)+3))$.}
  \label{fig:tree}
\end{wrapfigure}
To begin, we aim to empirically investigate the relationship between reasoning performance and CoT length. Therefore, we need to control a given model to generate reasoning chains of varying lengths for a specific task. Unfortunately, no existing real-world dataset or model fully meets these strict requirements. Real-world reasoning tasks, such as GSM8K or MATH \citep{gsm8k, math}, do not provide multiple solution paths of different lengths, and manually constructing such variations is challenging. Moreover, it is difficult to enforce a real-world model to generate a diverse range of reasoning paths for a given question.  

Given these limitations, we begin our study with experiments on synthetic datasets. Notably, even when working with real-world datasets, we observe behavioral patterns that align well with the findings derived from synthetic data (Section~\ref{sec:real}).


\subsection{Problem Formulation}

To investigate the effect of CoT length in a controlled manner, we design a synthetic dataset of simplified arithmetic tasks with varying numbers of reasoning steps in the CoT solutions. The \textbf{necessity} and \textbf{rationale} for simplified arithmetic tasks will be further discussed in the Appendix~\ref{app:task}.
\begin{definition}[Problem]
\label{def:task}
    In a simplified setting, an arithmetic task $q$ is defined as a binary tree of depth \( T \). The root and all non-leaf nodes are labeled with the \( + \) operator, while each leaf node contains a numerical value (mod 10). In addition, we impose a constraint that every non-leaf node must have at least one numerical leaf as a child. 
\end{definition}

The bidirectional conversion method between arithmetic expressions and computation trees is as follows: \textit{keeping the left-to-right order of numbers unchanged, the computation order of each "+" or tree node is represented by tree structure or bracket structures.}
For example, consider the task \( 5+(4+((2+1)+3)) \) with $T=4$. The corresponding computation tree is defined as Figure~\ref{fig:tree}.


To ensure that CoT solutions of the same length have equal difficulty for a specific problem, we assume that each reasoning step performs the same operations within a single CoT process. A more rigorous discussion will be conducted in Appendix~\ref{app:same_length}.
\begin{definition}[Solution]
\label{def:solu}
    We define a $t$-hop CoT with a fixed each step length of \( t \) as a process that executes \( t \) operations starting from the deepest level and moving upward recursively. 
\end{definition}

According to this definition, the execution sequence is uniquely determined. For example, one way to solve expression in Figure~\ref{fig:tree} is by performing one addition at a time:
\begin{align}
    &5+(4+((2+1)+3)) = \texttt{<1>}\\ \label{eq:subtask}
    &2 + 1 = 3\\ \notag
    &3 + 3 = 6\\ \notag
    &4 + 6 = 0\\ \notag
    &5 + 0 = 5 \texttt{<END>.} \notag
\end{align}
%
Another approach is to perform two additions at a time:
%
\begin{align}
    &5+(4+((2+1)+3)) = \texttt{<2>}\\ \notag
    &(2 + 1 ) + 3  = 6\\ \notag
    &5 + (4 + 6) = 5\texttt{<END>.} \notag
\end{align}
%
The latter approach is half as long as the former, but each reasoning step is more complex\footnote{This is because performing two operations at once requires the model to either memorize all combinations of numbers in a two-operator equation and their answers, apply techniques like commutativity  to reduce memory requirements, or use its mental reasoning abilities to perform the two operations without relying on CoT.}. This illustrates a clear trade-off between the difficulty of each subtask and the total number of reasoning steps. 

In practice, when \( t \) does not evenly divide \( T \), the final step performs \( T \mod t \) operations. To guide the model in generating the desired CoT length, we insert the control token \texttt{<t>} after the question and before the beginning of the solution. To preserve the parentheses that indicate the order of operations, we construct expressions in Polish notation. However, for readability, we present each problem in its conventional form throughout the article.

\subsection{Experiment Setup}
\begin{figure*}[ht]
\centering
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Synthetic/envelope_m_6_t_9.pdf}
    \caption{Small model with 6 layers}
    \label{fig:envelope_m_6_t_9}
\end{subfigure}
\begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Synthetic/envelope_m_9_t_9.pdf}
    \caption{Large model with 9 layers}
    \label{fig:envelope_m_9_t_9}
\end{subfigure}
\caption{The envelope curve shows how the performance of optimal operators in a single step ($t$) changes as the task becomes more difficult. The different colors of the envelope curve correspond to the best-performing operators in one step ($t$) for the given set of total operators. 
%As shown in this figure, the optimal step length increases as the task becomes more challenging.
}
\label{fig:envolop}
\end{figure*}

\textbf{Dataset}  
The datasets used for training different models are identical and are constructed from tasks with varying total operators \( T \) and solutions with different \( t \).

\textbf{Model}
Since model depth plays a critical role in mathematical reasoning \citep{cot-theory:allen-zhu}, we trained GPT-2 models with varying numbers of layers, keeping all other parameters (e.g., heads, dimensions) constant, to assess the impact of model capacity. Our experimental results provide evidence that models with more layers are capable of performing more operations in a single step, without the need for CoT. 

\textbf{Train and Test}  
For each problem, we train the model to start generating from the control token \texttt{<t>} to ensure that it can independently determine which CoT solution to use. During testing, we guide the model to produce the required \( t \)-hop CoT by inserting the control token \texttt{<t>} right before the question. More details can be found in Appendix~\ref{app:syn}.


\subsection{Experimental Results}

\textbf{U-curve}
For convenience, we present how the final accuracy changes as the number of operators performed per step $t$ increases, which corresponds to a decrease in the number of reasoning steps, for tasks of varying difficulty (total operators). Figures \ref{fig:U_curve_small_model} and \ref{fig:U_curve_middle_model} show the performance of small and large models on easy tasks (16, 24, 32 operators) and hard tasks (48, 56, 64 operators) respectively.

The results (Figure~\ref{fig:U-curve}) align well with our theory, demonstrating that \textit{as the number of reasoning steps increases, the final performance initially improves and then declines}. Subsequent experiments will explore how the optimal CoT length changes with model capability and task difficulty.


\textbf{Envelope Curve}
Figure~\ref{fig:envelope_m_6_t_9} and \ref{fig:envelope_m_9_t_9} illustrate the envelope curve, where different colors represent the optimal single-step length. The results indicate that \textit{as the task becomes more challenging, CoT with a larger single-step reasoning length \( t \) achieves the best performance}. This can be interpreted as a mechanism to regulate the total number of CoT steps—shorter single-step lengths require more CoT steps to complete the task.

\begin{wrapfigure}{r}{0.5\textwidth} 
\vspace{-10pt}
    \centering
    \includegraphics[width=\linewidth]{Figures/Synthetic/heatmap_alot.pdf}
    \caption{Heat map of optimal CoT length in different model sizes and task difficulties.}
    \label{fig:heatmap-stepnum}
\vspace{-10pt}
\end{wrapfigure}


\textbf{Optimal CoT length shifts}
To further investigate how the optimal CoT length changes with model capability and task difficulty, we trained models of varying sizes (ranging from 5 to 9 layers) on tasks of different difficulties (from 16 to 64 operators). For each combination of model size and task difficulty, we recorded the optimal number of reasoning steps.




Figure \ref{fig:heatmap-stepnum} illustrates two key findings. First, an increase in the number of reasoning steps is beneficial for solving more challenging problems, indicating that \textit{harder tasks require more steps to achieve optimal performance}. Second, the optimal number of reasoning steps decreases as the model size increases, suggesting that \textit{stronger models can handle more complex reasoning within fewer steps}.

\section{Theoretical Analysis}

\label{sec:theory}
In this section, we provide a theoretical analysis of the CoT process for the simplified arithmetic tasks defined above and explain the empirical results observed in synthetic datasets. All proofs of the paper are deferred to Appendix~\ref{app:proof}.

\subsection{Setup}
Let \( N \in \mathbb{N^+} \) represent the total number of steps in the CoT process. As defined earlier, \( T \) denotes the total number of operators in the given question, and \( t=\left\lceil \frac{T}{N} \right\rceil \) represents the number of operators processed in each reasoning step. We use \( t_i \) to denote the subtask in the \( i \)-th reasoning step (e.g., \( 2+1 \) in Eq.~(\ref{eq:subtask})), and \( a_i \) to represent the corresponding answer (e.g., \( 3 \) in Eq.~(\ref{eq:subtask})). 

\begin{definition}
Given task $q$ with total operators $T$ (Definition~\ref{def:task}) and model $\theta$, to a specific $N$, we define an $N$ step ($t$-hop in Definition \ref{def:solu}) CoT process as 
$$P(a_N|q,\theta) = \prod^N_{i=1}P(t_i|H_{i-1},q,\theta)P(a_i|t_i,H_{i-1},q,\theta),$$

where $H_k=[t_1,a_1,\cdots,t_k,a_k]$ collects the first $k$ steps in the $N$-step CoT process, and $a_N$ is not only the answer of the final subtask $t_i$, but the answer to the whole task $q$ as well.
\end{definition}

Let \( a_i^* \) denote the correct answer to subtask \( t_i \), and \( t_i^* \) the correct next subtask given the history reasoning steps \( H_{i-1} \), total task \( q \), and total step number \( N \). To estimate the final accuracy \( A(N) = P(a_N = a_N^*|q, \theta) \), we need to separately estimate the sub-question accuracy \( P(t_i = t_i^*|H_{i-1}, q, \theta) \) and the sub-answer accuracy \( P(a_i = a^*|t_i, H_{i-1}, q, \theta) \).

For the \textbf{sub-question accuracy}, as observed in our experiments, the loss for tokens appearing in \( t_i \) remains almost constant across different values of \( N \) and model sizes (see Appendix~\ref{app:subtask}). We assume that the noise affecting tokens in each subtask \( t_i \), denoted as \( \sigma(T) \in [0,1)\), is positively correlated with the total number of operators \( T \). Intuitively, as the number of operators increases, extracting the correct subtask becomes more challenging. 

For the \textbf{sub-answer accuracy}, it is clear that when given subtask \( t_i \), \( P(a_i = a^*|t_i, H_{i-1}, q, \theta) \) is independent of the history reasoning steps \( H_{i-1} \) and is only influenced by the model \( \theta \) and the difficulty of the subtask \( t_i \). For each model, we define its capability \( M \) based on the reasoning boundary \citep{reasoningboundary}. Specifically, we consider \( M \) as the maximum number of operators the model can compute in a single step without CoT. 
\begin{equation}
\label{eq:M}
    M = M(\theta)=\max_{t}\left\{P(a_i=a^*|t_i,\theta ) > 0 ,  |t_i|=t\right\},
\end{equation}
where $|t_i|$ refers to the number of operators in subtask $t_i$. In the following discussion, we focus only on CoT chains that do not exceed the model's capability, which is $t < M.$ Hence, we define the error rate of each subtask answer as \( E(N, M, T) \in [0,1)\).
% and thus we have 
% \begin{equation}
% \label{eq:task_ans_acc}
%     P(a_i = a^*|t_i, H_{i-1}, q, \theta) = 1 - E(N, M, T).
% \end{equation}
\begin{restatable}{proposition}{finalacc}
The total accuracy of $N$-step reasoning is 
\begin{equation}
    A(N) =  \alpha\left((1-E(N,M,T))(1-\sigma(T))\right)^N,
\end{equation}
where \(\alpha\) denotes a constant value independent of \(N\).
\label{pro: general_acc}
\end{restatable}

Proposition~\ref{pro: general_acc} establishes a differentiable functional relationship between total accuracy and CoT length \(N\). Once we obtain estimates for \(E(N,M,T)\) and \(\sigma(T)\), we can determine the optimal \(N(M,T)\). For simplicity, in the following discussion, we allow \( N \), \( M \), and \( T \) to be real numbers. In Section~\ref{sec:linear}, we analyze the case of a linear error rate, while in Section~\ref{sec:general}, we explore more general error functions.

\subsection{A Simple Case with Linear Error}
\label{sec:linear}
\textbf{An estimation of \(\sigma(T)\).}
To simplify the setting, we assume \(\sigma(T) = \frac{T}{C}\), where \(C\) is a hyperparameter representing the maximum number of operators the model can handle, which is solely influenced by the training data. To ensure that the model is capable of generating a reasonable subtask (even if it is incorrect), we consider a finite range \(T \in [0, 0.9C]\), ensuring that the subtask accuracy rate \(1 - \sigma(T)\) remains within \([0.1,1]\).

\textbf{An estimation of  $E(N, M, T)$.}
 We define the model answer's error rate $E(N, M, T) = \frac{T}{N}/{M}$ as the ratio between the number of subtask operators and the model's capacity $M$ (Eq.~(\ref{eq:M})) that the maximum number of operators the
model can compute in a single step. Therefore, $1-\frac{T}{NM}>0$.

According to Proposition~\ref{pro: general_acc}, a simplified total accuracy of $N$-step reasoning is 
\begin{align}
\label{eq:final_acc}
A(N) = \alpha\left(\left(1-\frac{T}{C} \right)\left(1-\frac{T}{NM}\right) \right)^N.
\end{align}

\begin{restatable}{theorem}{OptimalN}
    \label{thm:simple_optimal_N}
    In a simplified setting, for a given model capability $M$ and task difficulty $T$, the final accuracy $A(N)$ (Eq.~(\ref{eq:final_acc})) increases initially and then decreases as the number of reasoning steps $N$ increases. Besides, there exists an optimal 
    \begin{equation}
        N(M,T) = \frac{TZ}{M(Z+1)}
    \end{equation}
    
    that maximizes the final accuracy, where \( Z = W_{-1} \left(-\frac{1 - \frac{T}{C}}{e} \right) \), and \( W_{-1}(x) \) denotes the smaller branch of the Lambert function, which satisfies the equation \( w e^w = x \).
\end{restatable}


Theorem \ref{thm:simple_optimal_N} shows the optimal $N(M,T)$ is only affected by two factors $M$ and $T$. Through the expression of $N$, we can naturally derive the following three observations about the trend of $N$ with respect to changes in $M$ or $T$:

\begin{corollary}
    $\frac{T}{N(T,M)} = M\left(1+\frac{1}{Z}\right) $ increases monotonically with $T$, which aligns the envelope curve result well.
\end{corollary}

\begin{restatable}{corollary}{NwithT}
    $N(M, T)$ increases monotonically with $T$, which shows that as the total task becomes more difficult, the optimal number of reasoning steps increases.
    \label{cor:NwithT}
\end{restatable}

\begin{corollary}
    $N(M, T)$ decreases monotonically with $M$, which shows that as the capability of the model becomes stronger, it is easier for the model to solve one-step subtask reasoning, which leads to the decrease of the total task decomposition number.
\end{corollary}

\subsection{Extension to General Error Functions}
\label{sec:general}
% \wyy{Inference Result 1}
% \begin{definition}
%     $N^*(T,M)$ is the point of the minimum value of $E(N|T,M)$, that is:
%     $$  f(N|T,M) = \frac{\partial E(N|T,M)}{\partial N} \rightarrow f(N^*(T,M)|T,M)=0$$
% \end{definition}

% \begin{theorem}
% \wyy{draft}
%     We provide the scaling behavior between task difficulty, model size, and CoT length. Overall, an optimal CoT length strategy $N^*(T,M)$ is both determined by \textbf{model size} and \textbf{task difficulty}.

%     \begin{itemize}
%         \item $N^*(T|M)$ increases when T goes more difficult. \wyy{Inference Result 2}
        
%         \textbf{Message:} Tasks with higher difficulties need finer-grained decomposition.
%         \item $N^*(T|M)$ decreases when M goes stronger. \wyy{Inference Result 3}
        
%         \textbf{Messages:} Stronger models are capable for difficult one step reasoning, so they do not need to decompose the original task that much like weak model.
%     \end{itemize}

% \end{theorem}

In the simple case above, we discussed the trend of overall accuracy with respect to $N$ and the variation of optimal $N$ with $M$ and $T$, assuming the subtask error rate is a linear function. In the following discussion, we aim to derive conclusions corresponding to more general error rate functions. We find that as long as the error function satisfies some basic assumptions, the above conclusions still hold. A detailed discussion of basic assumptions will be conducted in Appendix~\ref{app:assumption}.

\begin{restatable}{theorem}{general}
\label{thm:general}
    For any noise function $0<\sigma(T)<1$ and a subtask error rate function $0<E(N,M,T)<1$ satisfying Assumption~\ref{ass:error_e} and \ref{ass:error_sigma}, a general final accuracy function $A(N)$ (Proposition~\ref{pro: general_acc}) has the following properties:
    \begin{itemize}
        \item $\lim_{N\rightarrow +\infty} A(N) \rightarrow 0$
        \item If $A(N)$ has the point of maximum value $N^*>1$, then $N^*$ has a lower bound
        \begin{align}
            \label{eq:lower_bound_N}
            N^*\geq N(M,T) 
            = E^{-1}\left(1-\frac{1}{e^2(1-\sigma(T))}\right),
        \end{align}
    \end{itemize}
\end{restatable}
% \wyy{$\sigma$ needs discussion}
where $E^{-1}$ is the inverse function of $E(N,M,T)$ with respect to $N$, which has the same monotonicity as $E$ with respect to $M$ and $T$. Therefore, we have similar corollaries as in the linear case.


\begin{corollary}
    As the model becomes stronger, $E^{-1}$ decreases monotonically with respect to $M$, which leads to decrease of $N(M,T)$.
\end{corollary}

\begin{corollary}
    As the task becomes harder, $E^{-1}$ is monotonically increasing with respect to $T$, which leads to an increase of $N(M,T)$.
\end{corollary}

\section{Empirical Examination of Optimal CoT Length}
Following the theoretical analyses of optimal CoT length in Section \ref{sec:theory}, we further validate these insights on both real-world LLM behaviors at test time and its influence on CoT training.

\subsection{Optimal CoT Length of LLMs on MATH}
\label{sec:real}
\begin{figure*}[ht]
\centering
\begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=.92\linewidth]{Figures/Realworld/accuracy_comparison.pdf}
    \caption{Optimal length \textit{v.s.} Longest length}
    \label{fig:accuracy_comparison.5b}
\end{subfigure}%
% \hspace{0.03\textwidth}  % Adjusted spacing between figures
\begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Realworld/optimal_step.pdf}
    \caption{Optimal length \textit{v.s.} Model size}
    \label{fig:optimal_step}
\end{subfigure}%
% \hspace{0.03\textwidth}  % Adjusted spacing between figures
\begin{subfigure}{0.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Figures/Realworld/step_vs_acc_qwen1.5b.pdf}
    \caption{Optimal length \textit{v.s.} Task difficulty}
    \label{fig:step_vs_acc_qwen1.5b}
\end{subfigure}
\caption{Evaluation on MATH datasets with Qwen2.5 Series Instruct models. 
%\ref{fig:accuracy_comparison.5b} shows the comparison between optimal step's accuracy and the longest step's accuracy. \ref{fig:optimal_step} shows the trend that larger models have fewer optimal steps. \ref{fig:step_vs_acc_qwen1.5b} shows a significant ($p=1e-8<<0.05$) correlation between task difficulties and optimal steps. 
}
\end{figure*}

To validate our theory on real-world tasks with LLMs, we consider the MATH \citep{math} algebra dataset (Level 5), which includes challenging competition-level mathematics problems requiring multi-step reasoning. We select Qwen2.5 series Instruct models \citep{qwen} to investigate behaviors among models with different capabilities. More details can be found in Appendix~\ref{app:real_world}.

\textbf{Optimal Length and Model Capability}
In this section, we randomly select 30 questions from the dataset, generating 60 samples for each question, resulting in a total of 1,800 samples for each model. 
 The results, shown in Figure \ref{fig:accuracy_comparison.5b}, demonstrate that for each model, the longest number of steps is not the best one. Additionally, the optimal CoT length decreases as the model size increases, from 14 steps for the 1.5B model to 4 steps for the 72B model (Figure~\ref{fig:optimal_step}). This trend aligns with our theory that larger models perform better with fewer reasoning steps, as they are more capable of effectively handling single-step reasoning.

\textbf{Impact of Task Difficulty}
In this part, we evaluate how the optimal CoT length changes with task difficulty. To achieve this, we randomly select 100 questions from the dataset, generating 60 samples for each question to ensure the number of reasoning steps is calculated accurately. We assume that a question with lower accuracy among the 60 samples is more difficult for the model, using 1-accuracy as a proxy for the relative difficulty of each question(the larger the harder).



 
We then plot a scatter plot of accuracy versus optimal CoT length and calculate the correlation. As shown in Figure~\ref{fig:step_vs_acc_qwen1.5b}, a significant correlation ($p = 1e-8 << 0.05$) between task difficulty and optimal CoT length. Results on different models are shown in Appendix~\ref{app:difficulty}. This finding supports our theory that harder tasks require more steps to solve effectively.

\subsection{Implications for Training with CoT Data}
\label{sec:opt}

\label{sec:solu_tr}
In the previous synthetic dataset experiments (Section \ref{sec:arithmetic}), we generated solutions with varying step lengths by training our model on data sampled with random CoT lengths. Now that we have identified the optimal CoT length, an important question arises:  
\begin{quote}
\textit{Can we construct a dataset that contains only CoT solutions with optimal lengths, tailored to the current model size and task difficulty?}      
\end{quote}

To explore this, we conduct experiments on a synthetic dataset. The baseline model is trained on CoT step lengths following a uniform distribution, while the optimal model is trained exclusively on the optimal CoT lengths identified in Figure~\ref{fig:heatmap-stepnum}.

\begin{figure}
% {r}{0.5\textwidth}
\vspace{-20pt}
    \centering
    \includegraphics[width=.5\linewidth]{Figures/Synthetic/rand_base_vs_opt.pdf}
    \caption{Comparison of training on CoTs with optimal lengths versus random lengths: \textit{Base }refers to a model trained on data with random CoT lengths, while \textit{Opt} refers to a model trained on data with optimal CoT lengths. 
    % The small model consists of 6 layers, while the large model has 9 layers. The results are impressive, showing that a small model trained on a carefully designed dataset can outperform a larger model.
    }
    \label{fig:rand_base_vs_opt}
% \vspace{-20pt}
\end{figure}







During testing, we allow the model to choose the CoT types on its own. The results shown in Figure~\ref{fig:rand_base_vs_opt} demonstrate that with a carefully designed training dataset, a smaller model can achieve significantly better performance—nearly 100\% accuracy—even surpassing a larger model trained on base dataset.  

However, in real-world datasets, the same problem may be associated with reasoning chains of varying lengths, none of which necessarily correspond to the optimal one. This experiment highlights the importance of carefully selecting CoT length when training models for chain-of-thought reasoning.

\begin{algorithm}[t]
\caption{Length-filtered Vote}
\label{alg:la-vote-top-k}
\begin{algorithmic}[1]
\State \textbf{Input:} Model $f_\theta$, Question $q$, Space of All Possible Answers $A$, Number of Total Groups $M$, Number of Selected Groups $K$, Group Width $D$
\State \textbf{Output:} Final Answer $\hat{a}$

\State Sample candidates $c_1, \dots, c_n \stackrel{i.i.d.}{\sim} f_\theta(q)$
\State \textbf{Define} $\mathcal{A}(c)$ as the corresponding answer of candidates $c$.
\State \textbf{Define} $p_j \in [0,1]^{|\mathcal{A}|}$ as the frequency of each answer in length group $L_j$.
\For{$j = 1$ to $m$}

    $L_j = \{c_i \mid \ell(c_i) \in \left[D*(j-1),D*j\right) , i = 1,\cdots,n \}$

    \For{$a \in \mathcal{A}$}
        \[
        p_j[a] = \frac{\sum_{c \in L_j} \mathbb{I}(\mathcal{A}(c) = a)}{|L_j|}
        \]
    \EndFor
\EndFor

\State$
\{s_1, \dots, s_K\} = \arg\min_{S \subseteq \{1, \dots, M\}, |S|=K} \sum_{s \in S} H(p_s)
$

\State$
\hat{a} = \arg\max_{a \in A} \sum_{c \in L_{s_1} \cup \dots \cup L_{s_K}} \mathbb{I}(\mathcal{A}(c)=a)
$

\State \Return $\hat{a}$
\end{algorithmic}
\end{algorithm}



\section{Length-filtered Vote}
\label{sec:solu_te}
Section~\ref{sec:opt} highlights the importance of aligning the CoT length with the model's capabilities and the task's difficulty. However, achieving this alignment requires an accurate estimation of both the task and the model. Moreover, given a pretrained model, how can we leverage the optimal CoT length without any prior estimation of the task or even the model itself?

In this section, we propose a length-aware variant of majority vote, \textbf{Length-filtered vote}, where we use prediction uncertainty as a proxy to filter reliable CoT lengths.
% to demonstrate how we can benefit from selecting the optimal CoT length during the inference stage. 
% In detail, it refines answer selection by filtering answers with too long and too short CoT length among sampled responses.
As in majority vote, given a model \( f_\theta \), a question \( q \), a ground truth answer \( a^* \), we first sample a set of answer candidates independently $c_1, \dots, c_n \stackrel{i.i.d.}{\sim} f_\theta(q)$. After that, instead of direct vote, we group the answers based on their corresponding CoT length $\ell(c_i)$ (discussed in Appendix~\ref{app:real_world}) into groups with equal bandwidth $D$ (by default, we set $D=2$), denoted as $\{L_j\}_{j=1}^m$. As our theory suggests that the prediction accuracy of CoT paths is peaked around a certain range of CoT length, we identify such groups through the prediction uncertainty of the answers within each group, based on the intuition that lower uncertainty implies better predictions. Specifically we calculate the Shannon entropy of the final answers given by the CoT chains in each group $L_i$, denoted as $H(L_i)$. We then select $K$ (by default, we set $K=3$) out of $M$ groups that has the smallest entropy and then perform majority vote only on these selected groups. We summarize it in Algorithm \ref{alg:la-vote-top-k}.



% To identify the most reliable length groups, the algorithm selects three groups \( \{L_i, L_j, L_k\} \) with the lowest three entropy, representing the optimal range of CoT length. Finally, the final answer \( \hat{a} \) is determined by majority voting across these selected groups, maximizing the occurrence of \( a^* \) among them. 

\begin{table}[h]
    \centering
    \caption{Performance of different models with varying sample numbers for Direct Vote on all candidates and Length-filtered Vote.}
    \label{tab:1}
    \begin{tabular}{lccccc}  % 第一列模型，第二列方法，接下来是样本数
        \toprule
        \multirow{2}{*}{Model} & \multirow{2}{*}{Method} & \multicolumn{4}{c}{Number of Samples} \\  
        \cmidrule(lr){3-6}
        & & 20 & 30 & 40 & 50 \\
        \midrule
        \multirow{2}{*}{Llama3.1-8B-Ins} 
        & Direct Vote    & 35\% & 38\% & 39\% & 38\% \\
        & Length-filtered Vote  & \textbf{36\%} & \textbf{42\%} & \textbf{42\%} & \textbf{41\%} \\
        \midrule  
        \multirow{2}{*}{Qwen2.5-7B-Ins}  
        & Vote    & 34\% & 35\% & 36\% & 34\%\\
        & Length-filtered Vote  & \textbf{36\% }& \textbf{40\%} & \textbf{38\%} & \textbf{40\%}\\
        % \midrule  
        % \multirow{2}{*}{Qwen2.5-14B-Ins} 
        % & SC    & 44\% & 45\% & 44\% &44\%\\
        % & SFSC  & 49\% & 48\% & 44\% &44\%\\
        % \midrule 
        % \multirow{2}{*}{Qwen2.5-32B-Ins} 
        % & SC    & 43\% & 43\% & 42\% & 43\% \\
        % & SFSC  & 46\% & 48\% & 43\% & 43\% \\
        % \midrule 
        % \multirow{2}{*}{Qwen2.5-72B-Ins} 
        % & SC    & 52\% & 52\% & 54\% & 51\% \\
        % & SFSC  & 51\% & 54\% & 54\% & 53\% \\
        \bottomrule
    \end{tabular}

    \label{tab:model_performance}
\end{table}


We evaluate the propose method against vanilla majority vote (i.e., self-consistency) \citep{self-consistency}  on a randomly chosen subset of 100 questions from the GPQA dataset, a more challenging collection of multiple-choice questions. The results in Table \ref{tab:1} show that our filtered vote consistently outperforming vanilla majority vote at different sample numbers and have little performance degradation as the sample number increases. This indicates the importance of considering the influence CoT length in the reasoning process.
% selecting answers with the optimal CoT lengths improves final performance.

\section{Conclusion}
In this paper, we conduct experiments on a simplified synthetic dataset, drawing clear conclusions on how the CoT length affects the final performance. Our study also provides valuable insights, showing that the optimal CoT length should adapt to both model size and task difficulty. Furthermore, we present a rigorous theoretical framework demonstrating the non-monotonic scaling behavior of CoT length and how it is influenced by model size and task difficulty. Additionally, we conduct experiments on real-world datasets, yielding similar results. We also propose methods that can benefit from the optimal CoT length during both training and test phases. In this way, our analysis offers concrete theoretical and empirical insights into developing LLMs that adaptively select the appropriate reasoning length, avoiding either overthinking or underthinking.










% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocitep
% \nocitep{}
% \newpage
% \section{Impact Statements}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.
\bibliography{ref}
\bibliographystyle{plainnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\begin{center}
\LARGE \textbf{Appendix}
\end{center}

\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\tableofcontents

\input{Appendix}

\end{document}