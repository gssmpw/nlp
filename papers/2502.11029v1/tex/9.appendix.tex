

\section{Communication Cost Configurations of MPL Frameworks}~\label{appendix:protocol_config}
\wqruan{In this section, we introduce how we configure the communication costs for basic operations of ten MPL frameworks (i.e., \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2}, \texttt{CrypTen}~\cite{crypten2020}, \texttt{ABY}~\cite{aby}, \texttt{SPDZ-2k}~\cite{spdz2k}, \texttt{ABY3}~\cite{mohassel2018aby3}, and \texttt{Falcon}~\cite{wagh2020falcon}, \texttt{Delphi}~\cite{mishra2020delphi}, \texttt{Cheetah}~\cite{Cheetah}, \texttt{Deep-MPC}~\cite{pmlr-v162-keller22a}, \texttt{SecretFLow-SEMI2K}~\cite{secretflow}) involved in our experiments. We first introduce the extra parameter for some basic operations. Secure multiplication (`muls') has three extra parameters: the degree of HE polynomial $\mathit{deg}$, HE prime coefficients modulus $\mathit{mod}$, and $\mathit{size}$ that indicates the number of secure multiplications executed in parallel. Matrix multiplication (`matmuls') has five extra parameters: the row number of the first input matrix $p$, the column number of the first input matrix $q$, the column number of the second input matrix $r$, the degree of HE polynomial $\mathit{deg}$, HE prime coefficients modulus $\mathit{mod}$. Secure truncation (`TruncPr') has one extra parameter: $\mathit{knownmsb}$ that indicates whether the most significant bit (msb) of the input is known.} 

\texttt{CrypTFlow2}~\cite{rathee2020cryptflow2}: We configure the communication costs for basic operations of \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} by analyzing Tables 1 and 2, Section 4.2, and Appendix F of \cite{rathee2020cryptflow2} and running its source codes. \wqruan{\texttt{CrypTFlow2} integrates an optimization for truncation proposed by Rathee et al.~\cite{sirnn}. Therefore, we configure two versions of the communication cost of `TruncPr' for \texttt{CrypTFlow2}. Besides, for the communication round of secure matrix multiplication, we find that \texttt{CrypTFlow2} would partition the transferred messages for large input messages. Therefore, we set the communication round of `matmuls' according to the partition strategy defined in lines 239-248 of `linear-ot.cpp' in the source codes of \texttt{CrypTFlow2}.} Because \texttt{CrypTFlow2} finishes all computations in the online phase, we set the communication cost of its offline phases as 0. The communication cost configuration of \texttt{CrypTFlow2} is shown in Listing~\ref{list:cryptflow2_cost}. Note that we use ring-based \texttt{CrypTFlow2} following Ganesan et al.~\cite{ganesan2022efficient}.

\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of $\texttt{CrypTFlow2}$~\cite{rathee2020cryptflow2}.}, columns=fullflexible, label = {list:cryptflow2_cost}]
class CrypTFlow2(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (0, 0, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (2*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$, $\mathit{deg}$, $\mathit{mod}$: 
    ($\mathit{size}$*$k$*($\lceil$($k$+1)/2$\rceil$+$\kappa$), 
    2, 0, 0),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$, $q$, $r$, $\mathit{deg}$, $\mathit{mod}$: 
    ($q$*$r$*$k$*($p$*$\lceil$($k$+1)/2$\rceil$+$\kappa$), 2*$k$/ ($\lceil$ $2^{24}$/($p$*$q$*$r$)$\rceil$),
    0, 0) ,
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: 
    (($\kappa$+14)*$f$+ 2*$\kappa$ + 4*$k$ if $\mathit{knownmsb}$ else $\kappa$*($k$+2)+19*$k$
    +($\kappa$+14)*$f$, 2 if $\mathit{knownmsb}$ else 2*log($k$)+2, 0, 0) ,    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (($\kappa$+18)*$k$, log($k$), 0, 0) 
    }
\end{lstlisting}


\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of $\texttt{CrypTen}$~\cite{crypten2020}.}, columns=fullflexible, label = {list:crypten_cost}]
class CrypTen(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (0, 0, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (2*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$, $\mathit{deg}$, $\mathit{mod}$: 
    (2*$k$*$\mathit{size}$, 1, $k$*$\mathit{size}$, 3),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, $r$, $\mathit{deg}$, $\mathit{mod}$: 
    (($p$*$q$+$q$*$r$)*$k$*2, 1, $p$*$r$*$k$, 3) ,
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: (0, 0, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (54*$k$, log($k$)+2, 14*$k$, (log($k$)+2)*3), 
    "exp_fx": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  (16*$k$, 8, 8*$k$, 24), 
    "EQZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  (26*$k$, log($k$), 7*$k$, 21), 
    "Reciprocal" : lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (138*$k$, 38, 44*$k$, 114)
    }
\end{lstlisting}

\texttt{CrypTen}~\cite{crypten2020}:\wqruan{We configure the communication costs for basic operations of the two-party backend of \texttt{CrypTen}~\cite{crypten2020} by analyzing the protocol description in Appendices C.1, C.2 of \cite{crypten2020}.}  Furthermore, we update the communication cost configuration by running these basic operations with open-source codes of \texttt{CrypTen}. The communication cost configuration of \texttt{CrypTen} is shown in Listing~\ref{list:crypten_cost}. \wqruan{For its offline communication costs, we configure them by running each basic operation under the setting of one trusted dealer, i.e., trusted third party (TTP).}



\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{ABY}~\cite{aby}.}, columns=fullflexible, label = {list:aby_cost}]
class ABY(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (0, 0, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (2*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$, $\mathit{deg}$, $\mathit{mod}$: 
    (4*$k$*$\mathit{size}$, 1, 
    (2*$\kappa$+$k$+1)*$k$*$\mathit{size}$, 2),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, $r$, $\mathit{deg}$, $\mathit{mod}$: 
    ($p$*$q$*$r$*$k$*4, 1, $p$*$q$*$r$*(2*$\kappa$+$k$+1)*$k$, 2),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, $r$, $\mathit{knownmsb}$: 
    (0, 0, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, $r$:  
    ($\kappa$*$k$*7+($$k$^2$+$k$)/2, 4, 5*$\kappa$*$k$, 2) 
    }
\end{lstlisting}

\texttt{ABY}~\cite{aby}: \wqruan{We configure the communication costs for basic operations of \texttt{ABY} according to Section III-A, Table1 of \cite{aby}, and Algorithm 4 of \cite{pmlr-v162-keller22a}. We configure the communication cost for `reveal' and `muls' of \texttt{ABY}~\cite{aby} according to the protocol description and complexity analysis of Section III-A of \cite{aby}. For the communication cost of `matmuls', we directly use $p$*$q$*$r$ multiplications to implement it because \texttt{ABY} does not have special optimization for matrix multiplication. For the communication cost of `TruncPr', we use the local truncation of \texttt{SecureML}. For `LTZ', we implement it in \texttt{ABY} by calling one A2Y, one Y2B, and one B2A in \texttt{ABY}. Then we configure the communication cost of `LTZ' according to Table 1 of \cite{aby}.  Besides, the communication cost of `share' is from the optimized protocol in Algorithm 4 of \cite{pmlr-v162-keller22a}.} The communication cost configuration of \texttt{ABY} is shown in Listing~\ref{list:aby_cost}.

 
\texttt{SPDZ-2k}~\cite{spdz2k}: We configure the communication costs for basic operations of \texttt{SPDZ-2k}~\cite{spdz2k} according to Figures 6, 9, 11, 12 and 13, and Section 7 of \cite{spdz2k}. 

\wqruan{Firstly, we analyze the communication costs of two basic protocols: authentication and triple generation. According to Figure 11 and the complexity analysis in Section 7 of \cite{spdz2k}, the authentication protocol requires $\mathit{\kappa_s*max(k+\kappa_s, 2*\kappa_s)*m*(m-1)}$ bits and 3 rounds (2 for authentication and 1 for consistency check). For simplicity, we assume that $k$ is always larger than $\kappa_s$. According to Figures 12, 13 and the complexity analysis in Section 7 of \cite{spdz2k}, the triple generation protocol requires $\mathit{18*\kappa_s^2+4*k^2+17*\kappa*k)*m*(m-1)}$ bits and 8 rounds (2 for multiplication and 6 for authentication and sacrifice). we omit the communication costs for batch check of triples that are negligible compared to those of triple generation. }

\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{SPDZ-2k}~\cite{spdz2k}.}, columns=fullflexible, label = {list:spdz_cost}]
class SPDZ-2k(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: 
    (($\kappa_s$+$k$)*($m$-1), 1, ($\kappa_s$*($k$+$\kappa_s$)*$m$*($m$-1), 3),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$:
    (($\kappa_s$+$k$)*$m$*($m$-1), 1,  ($\kappa_s$*($k$+$\kappa_s$)*$m$*($m$-1), 3),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{deg}$, $\mathit{mod}$: 
    (($k$ + $\kappa_s$)*$m$*($m$-1)*2, 1, 
    (18*$\kappa_s^2$+4*$$k$^2$+17*$\kappa_s$*$k$)*$m$*($m$-1), 8),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, $r$, $\mathit{deg}$, $\mathit{mod}$: 
    (($k$ + $\kappa_s$)*$m$*($m$-1)*2*$p$*$q$*$r$, 1,
    ((18*$\kappa_s^2$+4*$$k$^2$+17*$\kappa_s$*$k$))*$m$*($m$-1)*$p$*$q$*$r$, 8),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:
    (($k$ + $\kappa_s$)*$m$*($m$-1), 1, 
    $k$*(($\kappa_s$+$k$)*(3m+1)*($m$-1)+$\kappa_s$*($k$+$\kappa_s$)*$m$*($m$-1)*2
    +(18*$\kappa_s^2$+4*$$k$^2$+17*$\kappa_s$*$k$)*$m$*($m$-1)), 11) 
    }
\end{lstlisting}


\wqruan{Secondly, we configure the communication costs for basic operations as follows. For the communication costs of `share' and `reveal', according to Figures 6 and 9 of \cite{spdz2k}, their online phases need to broadcast one message, and the offline phases require one call of the protocol for authentication. Therefore, their online communication cost is $\mathit{(\kappa_s+k)*(m-1)}$ and $\mathit{(\kappa_s+k)*m*(m-1)}$ respectively, and offline communication cost is the communication cost of the authentication protocol. For the communication cost of `muls', according to Figures 6 and 9 of \cite{spdz2k}, its online phase requires two calls of reveal protocol to reveal masked input, and the offline phase generates a beaver triple.  Therefore, its online communication cost is $\mathit{(\kappa_s+k)*m*(m-1)*2}$, and offline communication cost is the communication cost of the triple generation protocol. The communication cost of `matmuls' is obtained by simply multiplying the communication cost of `muls' by $p*q*r$. Finally, for the communication cost of TruncPr operation, we configure its communication cost according to the description of Appendix IX.C in the study~\cite{8835310}. Its online phase requires one call of the reveal protocol, and the offline phase requires $k$ calls of the random bit generation protocol that includes one share, one reveal, and one multiplication. The communication cost configuration of \texttt{SPDZ-2k} is shown in Listing~\ref{list:spdz_cost}.}

\texttt{ABY3}~\cite{mohassel2018aby3}: We configure the communication costs for basic operations of \texttt{ABY3}~\cite{mohassel2018aby3} according to Tables 2, Figure 2, and Sections 3.2.1 and 5.2 of \cite{mohassel2018aby3}. \wqruan{For the communication costs of `share', `reveal' and `muls' operations, we obtain their communication costs by analyzing the protocol description in Section 3.2.1 of \cite{mohassel2018aby3}. For the communication cost of `matmuls', we configure it by analyzing the protocol description in Section 5.2 of \cite{mohassel2018aby3}.  We configure the communication cost of `TruncPr' by analyzing the protocol description in Figure 2 of \cite{mohassel2018aby3}. Finally, for the communication cost of `LTZ', \texttt{ABY3} implements it by calling a bit extraction and a B2A. We then set the communication cost of `LTZ' according to Table 2 of \cite{mohassel2018aby3}.} The communication cost configuration of \texttt{ABY3} is shown in Listing~\ref{list:aby3_cost}. 

\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{ABY3}~\cite{mohassel2018aby3}.}, columns=fullflexible, label = {list:aby3_cost}]
class ABY3(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (3*$k$, 1, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (3*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$, $\mathit{deg}$, $\mathit{mod}$: 
    (3*$k$*$\mathit{size}$, 1, 0, 0),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, $r$, $\mathit{deg}$, $\mathit{mod}$: 
    (3*$p$*$r$*$k$, 1, 0, 0),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$:
    ($k$, 1, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  (9*$k$, log($k$)+2, 0, 0) 
    }
\end{lstlisting}

\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{Falcon}~\cite{wagh2020falcon}.}, columns=fullflexible, label = {list:falcon_cost}]
class Falcon(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (3*$k$, 1, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (6*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$, $\mathit{deg}$, $\mathit{mod}$: 
    (6*$k$*$\mathit{size}$, 1, 0, 0),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, r, $\mathit{deg}$, $\mathit{mod}$: 
    (6*$p$*r*$k$, 1, 0, 0),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: 
    (2*$k$, 1, (6+log($k$))*$k$+(6+$\lceil$log($k$-$f$)$\rceil$)*($k$-$f$), log($k$)+2),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (24*$k$, log($k$)+5,(k+8+log($k$))*$k$*3, 4+2*log($k$)),
    "Pow2":  lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (24*$k$*$k$, (log($k$)+5)*$k$, 
    ((k+8+log($k$))*$k$*3))*$k$)*$k$, 4+2*log($k$)),
    "Reciprocal":  lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (24*$k$*$k$+36*$k$, (log($k$)+5)*$k$+5, 
    ((k+8+log($k$))*$k$*3)*$k$, 4+2*log($k$))
    }
\end{lstlisting}

\texttt{Falcon}~\cite{wagh2020falcon}: We configure the communication costs for basic operations of the malicious version of \texttt{Falcon}~\cite{wagh2020falcon} according to Algorithms 5 and 6, Section 3.2, Table 9 and Figure 6 of \cite{wagh2020falcon}. \wqruan{For the communication cost of `share', \texttt{Falcon} uses the same construction as \texttt{ABY3}. For the communication cost of `reveal' according to the description in Section 3.2 of \cite{wagh2020falcon}, each party needs to transfer two messages to the other parties. For the online communication cost of `matmuls', `LTZ', and `Pow2', we directly collect the number of Table 9 of \cite{wagh2020falcon} to configure them. Besides, we configure the communication cost of `muls' operation by setting $p$, $q$, and $r$ of `matmuls' as 1, 1, and 1. For their offline communication costs, `matmuls' does not have the offline phase. Meanwhile, we configure the offline communication cost of `LTZ' according to the description in Figure 6. According to Algorithm 5 of \cite{wagh2020falcon}, `Pow2' is implemented by calling $k$ `LTZ'. Therefore, the offline communication cost of `Pow2' is $k$ times of `LTZ'. For the communication cost of `Reciprocal', according to Algorithm 6 of \cite{wagh2020falcon}, \texttt{Falcon} implements it by calling one `Pow2' and five `muls'. Therefore, we configure the communication cost of `Reciprocal' using a simple calculation. For the communication cost of `TruncPr', \texttt{Falcon} uses the same construction as \texttt{ABY3}. Therefore, we configure its communication cost according to Figure 8 of \cite{mohassel2018aby3}. The communication cost configuration of \texttt{Falcon} is shown in Listing~\ref{list:falcon_cost}.}

\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{Delphi}~\cite{mishra2020delphi}.}, columns=fullflexible, label = {list:delphi_cost}]
class Delphi(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (0, 0, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (2*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$: ($k$*$\mathit{size}$, 1, 
    $\lceil$ $\mathit{size}$/$\mathit{deg}$$\rceil$ *$\mathit{deg}$*sum($\mathit{mod}$)*4, 2),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, r, $\mathit{deg}$, $\mathit{mod}$: 
    ($p$*$q$*$k$, 1, ($\lceil$$p$*r/$\mathit{deg}$$\rceil$+$\lceil$$p$*$q$/$\mathit{deg}$$\rceil$) *$\mathit{deg}$*sum($\mathit{mod}$)*2, 2),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: 
    (0, 0, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (148*$k$, 1, 1470*$k$, 3),
    "conv2d" lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{batch\_size}$, $\mathit{in\_channel}$, 
    $\mathit{out\_channel}$, $\mathit{inw}$, $\mathit{inh}$, $\mathit{outw}$, $\mathit{outh}$, $\mathit{kw}$, $\mathit{kh}$, $\mathit{deg}$, $\mathit{mod}$: 
    ($\mathit{batch\_size}$*$\mathit{in\_channel}$*$\mathit{inw}$*$\mathit{inh}$*$k$, 1,
    $\mathit{batch\_size}$*$\lceil$$\mathit{in\_channel}$*$\mathit{inw}$*$\mathit{inh}$/$\mathit{deg}$$\rceil$*$\mathit{kw}$*$\mathit{kh}$*$\mathit{deg}$
    *sum($\mathit{mod}$)+$\lceil$$\mathit{batch\_size}$*$\mathit{out\_channel}$*$\mathit{outw}$*$\mathit{outh}$/$\mathit{deg}$$\rceil$
    *$\mathit{deg}$*sum($\mathit{mod}$), 2)
    }
\end{lstlisting}
 

\wqruan{\texttt{Delphi}~\cite{mishra2020delphi}: We configure the communication cost for basic operations of \texttt{Delphi}~\cite{mishra2020delphi} by analyzing the descriptions in Section 6, Figures 3 and 4 of \cite{mishra2020delphi} and running source codes. Because \texttt{Delphi} has special optimizations for convolution operations, it has a `conv2d'  operation with eight extra parameters: the input channel number $\mathit{in\_channel}$, the output channel number $\mathit{out\_channel}$, the input width $\mathit{inw}$, the input height $\mathit{inh}$, the output width $\mathit{outw}$, the output height $\mathit{outh}$, the kernel width $\mathit{kw}$, the kernel height $\mathit{kh}$. Meanwhile, because \texttt{Delphi} uses additive secret sharing, the communication costs of its `share' and `reveal' operations are consistent with those of \texttt{ABY}.  For the online phase of all linear operations (`muls', `matmuls', `conv2d'), according to Figure 4 of \cite{mishra2020delphi}, their communication costs are the size of input data multiplied by bit length. For the offline phase of linear operations, \texttt{Delphi} directly uses \texttt{GAZELLE}~\cite{juvekar2018gazelle}'s algorithms for linear layers according to the description in Section 6. Therefore, we configure the offline communication costs of \texttt{Delphi} according to the description in Tables II and IV of \texttt{GAZELLE}~\cite{juvekar2018gazelle}. For `LTZ', we configure its online and offline communication cost by testing the source codes of \texttt{Delphi}. The communication cost configuration of \texttt{Delphi} is shown in Listing~\ref{list:delphi_cost}.} 



\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{Cheetah}~\cite{Cheetah}. compute\_ct\_num represents the matrix multiplication ciphertext count algorithm defined in Algorithm~\ref{alg:cheetah}}, columns=fullflexible, label = {list:cheetah_cost}]
class Cheetah(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (0, 0, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (2*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$: ($\lceil$$\mathit{size}$/$\mathit{deg}$$\rceil$*
    ($\mathit{deg}$*sum[0:-1]+$\mathit{deg}$*sum[0:-2]), 2, 0, 0),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, r, $\mathit{deg}$, $\mathit{mod}$: 
    (2*(comput_ct_num($p$,$q$,r,$\mathit{deg}$)[0]*$\mathit{deg}$*sum($\mathit{mod}$)[0:-1]+
    comput_ct_num($p$,$q$,r,$\mathit{deg}$)[1]*$\mathit{deg}$*sum($\mathit{mod}$)[0:-2]),
    4, 0, 0),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: 
    ($f$+4, 2, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (13*$k$+1, log($k$), 0, 0)
    }
\end{lstlisting}

\begin{algorithm}[ht]
\small
\caption{Matrix multiplication ciphertext count algorithm of \texttt{Cheetah}~\cite{Cheetah}. }
\label{alg:cheetah}
 \begin{algorithmic}[1]
    \REQUIRE The row number of the first matrix $p$, the column number of the first matrix $q$, the column number of the second matrix r, the degree of HE polynomial $\mathit{deg}$, local computation price $\mathit{lp}$, bandwidth price $\mathit{bp}$
    \ENSURE The number of sending ciphertext $\mathit{s\_ct}$, the number of response ciphertext $\mathit{r\_ct}$
    \STATE \textbf{Initialization}: initialize min\_cost as Integer.MAX, $\mathit{s\_ct}$ = $\mathit{r\_ct}$ =0
    \FOR{$\mathit{d_1}$:$\mathit{min(deg, p+1)}$}
    \STATE $\mathit{block\_num_1}$ = $\lceil $p$/d_1 \rceil$, $\mathit{d_2 = 1}$
    \WHILE{$\mathit{d_2 \leq q}$ and $\mathit{d_1 *d_2} \leq deg$}
    \STATE $\mathit{block\_num_2}$ = $\lceil q/d_2 \rceil$
    \STATE $\mathit{d_3 = min(r, \lceil deg / d_1 / d_2 \rceil)}$
    \STATE $\mathit{block\_num_3}$ = $\lceil r/d_3 \rceil$
    \STATE $\mathit{s\_ct' = min(block\_num_1, block\_num_3)* block\_num_2}$
    \STATE $\mathit{r\_ct' = \lceil block\_num_1*block\_num_3/ d_2 \rceil}$
    \STATE num\_1 = $\mathit{\lceil block\_num_1 * block\_num_3 / deg \rceil*d_2}$
    \STATE num\_2 = $\mathit{block\_num_1}$*$\mathit{block\_num_2}$*$\mathit{block\_num_3}$
    \STATE cost = $(\mathit{s\_ct'}$+$\mathit{r\_ct'}$)*$\mathit{bp}$+ num\_1*$\mathit{lp}$+ num\_2*$\mathit{lp}/10$
    \IF{cost $\leq$ min\_cost}
    \STATE min\_cost = cos
    \STATE $\mathit{s\_ct = s\_ct', r\_ct = r\_ct'}$
    \ENDIF
    \STATE $\mathit{d\_2 = d\_2 *2}$
    \ENDWHILE
    \ENDFOR
    \RETURN $\mathit{s\_ct}$, $\mathit{r\_ct}$
\end{algorithmic}
\end{algorithm}

\wqruan{\texttt{Cheetah}~\cite{Cheetah}:  We configure the communication costs for basic operations of \texttt{Cheetah}~\cite{Cheetah} according to Sections 3.1, 3.2.2, 3.3 and Tables 2, 3, and 4 of \cite{Cheetah}. Besides the above analysis, we further adjust our configuration by the discussion with the developer of the developers of \texttt{SecretFlow}. Because \texttt{Cheetah} uses additive secret sharing,  the communication costs of its `share' and `reveal' operations are consistent with those of \texttt{ABY}. For communication costs of linear operations (`muls', `matmuls'), \texttt{Cheetah} implements them using the CKKS HE mechanism. \texttt{Cheetah} has two optimizations for the usage of the CKKS HE mechanisms. Firstly, the sender who holds the private key of HE can use the symmetric version of CKKS with only half of the ciphertext size. Secondly, according to Section 5.2 of \cite{Cheetah}, the response ciphertext can truncate the low-end parts of the ciphertext. In addition, \texttt{Cheetah} in \texttt{SecretFlow} uses a matrix partition strategy to balance the costs of computation and communication. We show the strategy that is defined in lines 180-223 of `matmat\_prot.cc' of the source codes of \texttt{SecretFlow} in Algorithm~\ref{alg:cheetah}. Combining the above information, we obtain the communication costs of linear operations. For the `LTZ' operation, \texttt{Cheetah} implements it by combining its millionaires' protocol and VOLE-style OT. Therefore, we obtain the communication cost of `LTZ' by collecting the numbers from Tables 2, 3 of \cite{Cheetah}. For the communication cost of `TruncPr', because \texttt{Cheetah} in \texttt{SecretFlow} uses the optimization proposed by Dalskov et al.~\cite{DBLP:journals/popets/Dalskov0K20}, msbs of its truncation input data are always 0. Therefore, we can configure the communication cost of the `TruncPr' according to Table 4 of ~\cite{Cheetah}. The communication cost configuration of \texttt{Cheetah} is shown in Listing~\ref{list:cheetah_cost}.}



\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{Deep-MPC}~\cite{pmlr-v162-keller22a}.}, columns=fullflexible, label = {list:Deep_MPC_cost}]
class Deep-MPC(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: ($k$, 1, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: (3*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$: (3*$k$, 1, 0, 0),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $p$ ,$q$, r, $\mathit{deg}$, $\mathit{mod}$: 
    (3*$p$*r*$k$, 1, 0, 0),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: 
    (8*$k$, 3, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    (7.425*$k$, log($k$)+2, 3*$k$, 2)
    }
\end{lstlisting}



\wqruan{\texttt{Deep-MPC}~\cite{pmlr-v162-keller22a}: We configure the communication costs for basic operations of the three-party protocol of \texttt{Deep-MPC}~\cite{pmlr-v162-keller22a} by analyzing Appendix A, Table 8 of \cite{pmlr-v162-keller22a} and running its source codes in \texttt{MP-SPDZ}. Keller and Sun listed communication costs of `reveal', `muls', `TruncPr', `LTZ' in Table 8 of \cite{pmlr-v162-keller22a} of \texttt{Deep-MPC}. Therefore, we directly collect numbers listed in Table 8 of \cite{pmlr-v162-keller22a} to configure the communication costs of the three basic operations. Meanwhile, we run the source codes of \texttt{MP-SPDZ} and align the communication cost of `TruncPr' to the newest implementation of \texttt{Deep-MPC}. Besides, as stated in Appendix A of \cite{pmlr-v162-keller22a}, \texttt{Deep-MPC} uses dabits~\cite{10.1007/978-3-030-35423-7_12} to convert boolean sharing to arithmetic sharing. Therefore, the cost for `LTZ' in Table 8 of \cite{pmlr-v162-keller22a} is split into online and offline communication costs (7.425k bits for online and 3k bits for offline). For the `matmuls' operation, as stated in Appendix A of \cite{pmlr-v162-keller22a}, \texttt{Deep-MPC} uses the same construction as \texttt{ABY3}, the communication cost of `matmuls' is the same as that of \texttt{ABY3}. For the `reveal' operation, as stated in Appendix A of \cite{pmlr-v162-keller22a}, \texttt{Deep-MPC} employs the optimization proposed by Eerikson et al.~\cite{DBLP:conf/icits/EeriksonKOPP020}. Therefore, the `reveal' operation of \texttt{Deep-MPC} only requires one message transfer. The communication cost configuration of \texttt{Deep-MPC} is shown in Listing~\ref{list:Deep_MPC_cost}.}

\begin{lstlisting}[mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {Communication cost configuration for basic operations of \texttt{SecretFlow-SEMI2K}~\cite{secretflow}. compute\_mmul\_size represents the matrix multiplication message count algorithm defined in Algorithm~\ref{alg:secretFlow-SEMI2K}}, columns=fullflexible, label = {list:secretflow_semi2k_cost}]
class SecretFlow-SEMI2K(Cost):
  cost_func_dict = {
    "share": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$: (0, 0, 0, 0),
    "reveal": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$: ($m$*($m$-1)*$k$, 1, 0, 0),
    "muls": lambda $k$, $\kappa_s$,$\kappa$, $f$, $m$, $\mathit{size}$: (2*$m$*($m$-1)*$k$*$\mathit{size}$, 1, 0, 0),
    "matmuls": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{mod}$ ,$q$, r, $\mathit{deg}$, $\mathit{mod}$: 
    ($m$*$k$*compute_mmul_size($\mathit{mod}$, $q$, r, $k$), 1,0, 0),
    "TruncPr": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$, $\mathit{knownmsb}$: 
    ($m$*($m$-1)*$k$, 1, 0, 0),    
    "LTZ": lambda $k$, $\kappa_s$, $\kappa$, $f$, $m$:  
    ($m$*(2*$k$ + 2*($m$-1)*(2*$k$+32)), math.log2($k$)+1
    , 0, 0)
    }
\end{lstlisting}


\begin{algorithm}[ht]
\small
\caption{Matrix multiplication message count algorithm of \texttt{SecretFlow-SEMI2K}~\cite{secretflow}. }
\label{alg:secretFlow-SEMI2K}
 \begin{algorithmic}[1]
    \REQUIRE The row number of the first matrix $p$, the column number of the first matrix $q$, the column number of the second matrix r, bit length $k$
    \ENSURE The number of ring elements required for the input matrices multiplication $num\_e$
    \STATE \textbf{Initialization}: initialize $\mathit{mem\_limit}$ as $2^{31}$, $\mathit{res} =0$
    \IF{ $p=0 \; or \; q=0 \; or\; r=0 \; or$ $\mathit{(p*q+q*r)*k <}$ $\mathit{mem\_limit}$}
    \RETURN $\mathit{p*q+q*r}$
    \ENDIF
    \STATE  q\_step = expected\_pr\_step =0
    \IF{$\mathit{q > (p+r)*8}$}
    \STATE expected\_pr\_step = $\mathit{p+r}$
    \STATE q\_step = max(1, $\lceil$ $\mathit{mem\_limit}$/$k$/ expected\_pr\_step$\rceil$)
    \ELSIF{$\mathit{(p+r) > q*8}$}
    \STATE  q\_step = $q$
    \STATE expected\_pr\_step = max(1, $\lceil$ $\mathit{mem\_limit}/k/q\_step \rceil$)
    \ELSE
    \STATE pr\_step = $\sqrt{\mathit{(p+r)*mem\_limit}/(q*k)}$
    \STATE q\_step = max(1, $\lceil \mathit{mem\_limit/(k)}$/pr\_step $\rceil$)
    \STATE expected\_pr\_step = max(1, $\lceil$ pr\_step $\rceil$)
    \ENDIF
    \STATE p\_step = max(1, $\lceil$ expected\_pr\_step*$\mathit{mod}$/($\mathit{mod}$+r) $\rceil$)
    \STATE r\_step = max(1, $\lceil$ expected\_pr\_step*r/($\mathit{mod}$+r) $\rceil$)
    \FOR{$\mathit{i}$:$\mathit{\lceil p/}$ p\_step $\rceil$}
    \FOR{$\mathit{j}$:$\mathit{\lceil q/}$ q\_step $\rceil$}
    \FOR{$\mathit{k}$:$\mathit{\lceil r/}$ r\_step $\rceil$}
    \STATE p\_sub = min($\mathit{mod}$ - p\_step*$i$, p\_step)
    \STATE q\_sub = min(q - q\_step*$i$, q\_step)
    \STATE r\_sub = min(r - r\_step*$i$, r\_step)
    \STATE $\mathit{res}$ += p\_sub*q\_sub + q\_sub*r\_sub
    \ENDFOR
    \ENDFOR
    \ENDFOR
    \RETURN $\mathit{res}$
\end{algorithmic}
\end{algorithm}


\wqruan{\texttt{SecretFlow-SEMI2K}~\cite{secretflow}: We configure the communication costs for basic operations of \texttt{SecretFlow-SEMI2K}~\cite{secretflow} according to communication costs of linear operations (`share', `reveal', `muls', `matmuls', `TruncPr') and the comparison operation (`LTZ') listed in `arithmetic.h' and `conversion.h' of `libspu/mpc/semi2k/'. Besides, \texttt{SecretFlow-SEMI2K} partitions the input matrices of matrix multiplication to avoid memory overflow. We show the message count algorithm for matrix multiplication in Algorithm~\ref{alg:secretFlow-SEMI2K} according to the matrix partition strategy defined in lines 249-289 of `ring.cc' of `libs/kernel/hal/'. The communication cost configuration of \texttt{SecretFlow-SEMI2K} is shown in Listing~\ref{list:secretflow_semi2k_cost}. Note that we configure the communication costs of \texttt{SecretFlow-SEMI2K} for its trusted first-party (TFP) setting. Therefore, the offline communication costs of basic operations listed in Listing~\ref{list:secretflow_semi2k_cost} are all zero.}

\begin{table*}[ht]
\caption{\wqruan{The online communication size profiling results of two secure CNN model inference processes outputted by \texttt{Delphi}~\cite{mishra2020delphi} and \hawkeye.} }
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|ccc|c}
\hline
Model                      & Framework                        & \multicolumn{3}{c|}{\% (MB) of linear operators}                                                                                                                                                                                                                                  & \% (MB) of non-linear operators                                                \\ \hline
\multicolumn{1}{l|}{}      &                                  & \multicolumn{1}{c|}{\% (MB) of Conv2d}                                                           & \multicolumn{1}{c|}{\% (MB) of other linear operators}                                           & \% (MB) of all                                                              & \multicolumn{1}{l}{}                                                           \\ \hline
\multirow{3}{*}{MiniONN}   & \texttt{Delphi}~\cite{mishra2020delphi} & \multicolumn{1}{c|}{\wqruan{0.50\% (0.98MB)}}                                                             & \multicolumn{1}{c|}{\wqruan{0.01\% (0.01MB)}}                                                             & \wqruan{0.51\% (0.99MB)}                                                             & \wqruan{99.49\% (195.41MB)}                                                             \\ \cline{2-6} 
                           & \hawkeye          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.44\% (0.87MB)}\\ \wqruan{-0.06\% (-0.11MB)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.01\% (0.01MB)}\\ \wqruan{-0.00\% (-0.00MB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{0.45\% (0.88MB)}\\ \wqruan{-0.06\% (-0.11MB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{99.55\% (195.41MB)}\\ \wqruan{+0.06\% (+0.00MB)}\end{tabular}      \\ \hline
\multirow{3}{*}{ResNet-32} & \texttt{Delphi}~\cite{mishra2020delphi} & \multicolumn{1}{c|}{\wqruan{0.75\% (2.59MB)}}                                                             & \multicolumn{1}{c|}{\wqruan{0.00\% (0.01MB)}}                                                             & \wqruan{0.75\% (2.60MB)}                                                             & \wqruan{99.25\% (342.25MB)}                                                             \\ \cline{2-6} 
                           & \hawkeye          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.73\% (2.52MB)}\\ \wqruan{-0.02\% (-0.07MB)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.00\% (0.01MB)}\\ \wqruan{+0.00\% (-0.00MB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{0.73\% (2.53MB)}\\ \wqruan{-0.02\% (-0.07MB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{99.27\% (342.25MB)}\\ \wqruan{+0.02\% (+0.00MB)}\end{tabular} \\ \hline
\end{tabular}}
\label{tab:delphi_online}

\end{table*}

\begin{table*}[ht]
\caption{\wqruan{The offline communication size profiling results of two secure CNN model inference processes outputted by \texttt{Delphi}~\cite{mishra2020delphi} and \hawkeye.}}
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|ccc|c}
\hline
Model                      & Framework                        & \multicolumn{3}{c|}{\% (MB) of linear operators}                                                                                                                                                                                                                                        & \% (MB) of non-linear operators                                                 \\ \hline
\multicolumn{1}{l|}{}      &                                  & \multicolumn{1}{c|}{\% (MB) of Conv2d}                                                              & \multicolumn{1}{c|}{\% (MB) of other linear operators}                                           & \% (MB) of all                                                                 & \multicolumn{1}{l}{}                                                            \\ \hline
\multirow{3}{*}{MiniONN}   & \texttt{Delphi}~\cite{mishra2020delphi} & \multicolumn{1}{c|}{\wqruan{2.34\% (75.52MB)}}                                                               & \multicolumn{1}{c|}{\wqruan{0.03\% (1.00MB)}}                                                             & \wqruan{2.37\% (76.52MB)}                                                               & \wqruan{97.63\% (3150.27MB)}                                                             \\ \cline{2-6} 
                           & \hawkeye          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{2.00\% (64.29MB)}\\ \wqruan{-0.34\% (-11.23MB)}\end{tabular}}  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.03\% (0.85MB)}\\ \wqruan{+0.00\% (-0.15MB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{2.03\% (65.14MB)}\\ \wqruan{-0.34\% (-11.38MB)}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}\wqruan{97.97\% (3148.73MB)}\\ \wqruan{+0.34\% (-1.54MB)}\end{tabular} \\ \hline
\multirow{3}{*}{ResNet-32} & \texttt{Delphi}~\cite{mishra2020delphi} & \multicolumn{1}{c|}{\wqruan{3.70\% (212.04MB)}}                                                              & \multicolumn{1}{c|}{\wqruan{0.02\% (1.00MB)}}                                                             & \wqruan{3.72\% (213.04MB)}                                                              & \wqruan{96.28\% (5517.63MB)}                                                             \\ \cline{2-6} 
                           & \hawkeye          & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{3.43\% (195.86MB)}\\ \wqruan{-0.27\% (-16.18MB)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.01\% (0.85MB)}\\ \wqruan{-0.01\% (-0.15MB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{3.44\% (196.71MB)}\\ \wqruan{-0.28\% (-16.33MB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{96.56\% (5516.39MB)}\\ \wqruan{+0.28\% (-1.24MB)}\end{tabular} \\ \hline
\end{tabular}}
\label{tab:delphi_offline}
\end{table*}



\section{Accuracy of \hawkeye on Mixed-protocol Frameworks}~\label{appendix:mixed-protocol}



\wqruan{\noindent \textbf{Experiment Setup.} In order to further show that \hawkeye works well with mixed-protocol frameworks that rely on HE or GC, we compare the communication cost profiling results outputted by \hawkeye and two MPL frameworks (i.e. \texttt{Delphi}~\cite{mishra2020delphi} and \texttt{Cheetah}~\cite{Cheetah}) that rely on HE and GC respectively. We run two secure CNN models (MiniONN~\cite{minionn} and ResNet-32~\cite{7780459}) inference processes on the open-source codes~\footnote{The code repository address of \texttt{Delphi} is \url{https://github.com/mc2-project/delphi}. We use codes at commit \textit{92bc007}}~\footnote{The code repository address of \texttt{Cheetah} is \url{https://github.com/secretflow/spu}. We use \texttt{SecretFlow-SPU0.9.3b0}} of these two MPL frameworks to obtain the dynamic profiling results. We choose MiniONN~\cite{minionn} and ResNet-32~\cite{7780459} because these two models are tested in the paper of \texttt{Delphi} and \texttt{Cheetah} simultaneously. Because \texttt{Delphi} merges the batch normalization layer and the convolution layer into a single convolution operation, we omit the batch normalization layer of these two models.  For parameter setting, we set the bit length as 64 and the bit length of fixed numbers' fractional part as 18. We set the statistical security parameter and computation security parameter of these two MPL frameworks as $40$ and $128$, respectively. Besides, we set the polynomial degree and modulus coefficients of BFV HE used by \texttt{Delphi} as 8192 and \{43, 43, 44, 44, 44\} respectively. For the CKKS HE used by \texttt{Cheetah}, we set the polynomial degree and modulus coefficients as 8192 and \{59, 55, 49, 49\} respectively. For the input data, following the setting of \texttt{Delphi}, we set the input data size as $1 \times 3 \times 32 \times 32$.}

\wqruan{To obtain the static model communication cost profiling results from \hawkeye, we configure the model communication cost of \texttt{Delphi} and \texttt{Cheetah} in \hawkeye and implement the above models. Finally, we run \hawkeye under the same parameter setting with \texttt{Delphi} and \texttt{Cheetah} to obtain the static profiling results. We also show that communication cost configuration of \texttt{Delphi} and \texttt{Cheetah} in Appendix~\ref{appendix:protocol_config}.}

\wqruan{\textbf{Experimental results.} As is shown in Table~\ref{tab:delphi_online}, Table~\ref{tab:delphi_offline}, and Table~\ref{tab:cheetah}, \hawkeye can accurately profile the model communication cost on \texttt{Delphi} and \texttt{Cheetah}. Concretely, the proportions of operators' online/offline communication size to the total online/offline communication size outputted by \hawkeye and baselines are very close. The differences between the communication size profiling results outputted by \hawkeye and baselines could be caused by the following two reasons: (1) The HE packing is usually not compact in the implementation. \texttt{Delphi} and \texttt{Cheetah} both use HE to compute linear operators. HE packs multiple plaintext data into one ciphertext to reduce the communication overhead. However, due to implementation issues, the HE packing usually cannot be as compact as the theoretical. Therefore, the communication sizes of linear operators outputted by \texttt{Delphi} and \texttt{Cheetah} are slightly larger than those outputted by \hawkeye (2) The complexity of the comparison operations is asymptotic rather than actual. Similar to \texttt{CrypTFlow2}, \texttt{Cheetah} only provides an upper bound of the communication complexity for its comparison operators. Therefore, the communication sizes of non-linear operators outputted by \hawkeye are slightly larger than those outputted by \texttt{Cheetah}. }

\begin{table*}[ht]
\caption{\wqruan{The online communication size profiling results of outputted by \texttt{Cheetah}~\cite{Cheetah} and \hawkeye for two secure CNN model inference processes.}}
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|ccc|c}
\hline
Model                      & Framework                         & \multicolumn{3}{c|}{\% (MB) of linear operators}                                                                                                                                                                                                                                      & \% (MB) of non-linear operators                                               \\ \hline
\multicolumn{1}{l|}{}      &                                   & \multicolumn{1}{c|}{\% (MB) of Conv2d}                                                             & \multicolumn{1}{c|}{\% (MB) of other linear operators}                                           & \% (MB) of all                                                                & \multicolumn{1}{l}{}                                                          \\ \hline
\multirow{3}{*}{MiniONN}   & \texttt{Cheetah}~\cite{Cheetah} & \multicolumn{1}{c|}{\wqruan{53.99\% (20.14MB)}}                                                             & \multicolumn{1}{c|}{\wqruan{2.41\% (0.90MB)}}                                                             & \wqruan{56.40\% (21.04MB)}                                                             & \wqruan{43.60\% (16.26MB)}                                                             \\ \cline{2-6} 
                           & \hawkeye           & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{50.47\% (18.34MB)}\\ \wqruan{-3.52\% (-1.80MB)}\end{tabular}}  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{2.26\% (0.82MB)}\\ \wqruan{-0.15\% (-0.08MB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{52.73\% (19.16MB)}\\ \wqruan{-3.67\% (-1.88MB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{47.27\% (17.18MB)}\\ \wqruan{+3.67\% (+0.92MB)}\end{tabular} \\ \hline
\multirow{3}{*}{ResNet-32} & \texttt{Cheetah}~\cite{Cheetah} & \multicolumn{1}{c|}{\wqruan{64.40\% (53.03MB)}}                                                             & \multicolumn{1}{c|}{\wqruan{1.02\% (0.84MB)}}                                                             & \wqruan{65.42\% (53.87MB)}                                                             & \wqruan{34.58\% (28.47MB)}                                                             \\ \cline{2-6} 
                           & \hawkeye           & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{63.14\% (52.87MB)}\\ \wqruan{-1.26\% (-0.16MB)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{0.92\% (0.77MB)}\\ \wqruan{-0.10\% (-0.07MB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{64.06\% (53.64MB)}\\ \wqruan{-1.36\% (-0.23MB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{35.94\% (30.10MB)}\\ \wqruan{+1.36\% (+1.63MB)}\end{tabular} \\ \hline
\end{tabular}}
\label{tab:cheetah}
\end{table*}
\begin{table*}[ht]
\caption{\wqruan{The online communication size profiling results of two secure CNN model training processes outputted by \texttt{SecreFLow-SEMI2K}~\cite{secretflow} and \hawkeye.}}
\centering
\scalebox{0.7}{
\begin{tabular}{c|c|c|c|c}
\hline
Model                      & Framework                                                                           & \% (GB) of Model Forward                                                             & \% (GB) of Model Backward                                                            & \% (GB) of Model Update                                                           \\ \hline
\multirow{3}{*}{ResNet-50}    & \texttt{SecretFlow-SEMI2K}~\cite{secretflow} & \wqruan{45.59\% (120.75GB)}                                                             & \wqruan{54.28\% (143.78GB)}                                                             & \wqruan{0.13\% (0.35GB)}                                                             \\ \cline{2-5} 
                           & \hawkeye                                                             & \begin{tabular}[c]{@{}c@{}}\wqruan{44.02\% (120.04GB)}\\ \wqruan{-1.57\% (-0.71GB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{55.85\% (152.29GB)}\\ \wqruan{+1.57\% (+8.51GB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{0.13\% (0.35GB)}\\ \wqruan{+0.00\% (+0.00GB)}\end{tabular} \\ \hline
\multirow{3}{*}{VGG-16} & \texttt{SecretFlow-SEMI2K}~\cite{secretflow} & \wqruan{47.49\% (16.92GB)}                                                              & \wqruan{51.89\% (18.49GB)}                                                              & \wqruan{0.62\% (0.22GB)}                                                             \\ \cline{2-5} 
                           & \hawkeye                                                             & \begin{tabular}[c]{@{}c@{}}\wqruan{45.69\% (16.77GB)}\\ \wqruan{-1.80\% (-0.15GB)}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}\wqruan{53.71\% (19.71GB)}\\ \wqruan{+1.82\% (+1.22GB)}\end{tabular}  & \begin{tabular}[c]{@{}c@{}}\wqruan{0.60\% (0.22GB)}\\ \wqruan{-0.02\% (+0.00GB)}\end{tabular} \\ \hline
\end{tabular}}
\label{tab:semi2k_training}
\end{table*}

\section{Accuracy of \hawkeye on Secure Model Training Scenarios}~\label{appdendix:secure_training}

\wqruan{\textbf{Experiment Setup.} To show that \hawkeye works well in secure model training scenarios, we compare the communication cost profiling results outputted by \hawkeye and \texttt{SecretFLow-SEMI2K}, the two-party backend of \texttt{SecretFlow-SPU}~\cite{secretflow}. We use classical VGG-16 and ResNet-50 models as our target models. To obtain the dynamic communication cost profiling results, we run secure VGG-16 and ResNet-50 model training processes on \texttt{SecretFLow-SEMI2K}~\footnote{The code repository address of \texttt{SecretFLow-SEMI2K} is \url{https://github.com/secretflow/spu}. We use \texttt{SecretFlow-SPU0.9.3b0}}.  For parameter setting, we set the bit length as 64 and the bit length of fixed numbers' fractional part as 18. We set the statistical security parameter and computation security parameter of these two MPL frameworks as $40$ and $128$, respectively. Meanwhile, We use SGD as the optimizer. The input data size is $128\times3\times32\times32$, where 128 is the batch size.}

\wqruan{To obtain the static model communication cost profiling results from \hawkeye, we configure the model communication cost of \texttt{SecretFLow-SEMI2K} in \hawkeye and implement the above models. Finally, we run \hawkeye under the same parameter setting with \texttt{SecretFLow-SEMI2K} to obtain the static profiling results. We show the communication cost configuration of \texttt{SecretFLow-SEMI2K} in Appendix~\ref{appendix:protocol_config}.}

\wqruan{\textbf{Experimental results.} As is shown in Table~\ref{tab:semi2k_training}, \hawkeye can accurately profile the communication cost of the secure model training process. The proportions of model training phases' online communication size to the total online communication size outputted by \texttt{SecretFlow-SEMI2K} and \hawkeye are very close. Concretely, the proportion differences between baselines and \hawkeye are all smaller than 1.82\%. The main difference lies in the model backward process. The potential reason behind it is that \texttt{SecretFlow-SEMI2K} has optimizations for a few special circuit structures, such as consecutive multiplications used in the model backward process. Therefore, the online communication sizes of the model backward process outputted by \texttt{SecretFlow-SEMI2K} are slightly smaller than those outputted by \hawkeye.}


