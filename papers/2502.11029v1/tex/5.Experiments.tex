


\begin{table*}[ht]
\centering
\caption{\wqruan{The online communication size profiling results of four secure CNN model inference processes outputted by \texttt{CryptFlow2}~\cite{rathee2020cryptflow2} and \hawkeye.} We report the proportion of each operator's online communication size to the total online communication size and the online communication size of each operator. Following Ganesan et al.~\cite{ganesan2022efficient}, we list the online communication size across linear and non-linear operators. }
\scalebox{0.7}{
\begin{tabular}{c|c|cccc}
\hline
Model                          & Framework                                                                              & \multicolumn{3}{c}{\% (GB) of linear operators}                                                                                                                                                                                                                                                      & \% (GB) of non-linear operators                                                       \\ \hline
\multicolumn{1}{l|}{}         &                                                                                        & \multicolumn{1}{c}{\% (GB) of Conv2d}                                                               & \multicolumn{1}{c}{\% (GB) of other linear operators}                                                       & \% (GB) of all                                                                     & \multicolumn{1}{l}{}                                                        \\ \hline
\multirow{3}{*}{DenseNet-121} & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c}{\wqruan{94.14\% (646.05GB)}}                                                              & \multicolumn{1}{c}{\wqruan{3.60\% (24.72GB)}}                                                              & \wqruan{97.74\% (670.78GB)}                                                              & \wqruan{2.26\% (15.52GB)}                    \\ \cline{2-6} 
                 & \hawkeye                                                                & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{93.92\% (657.06GB)}\\ \wqruan{-0.22\% (+11.01GB)}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{3.77\% (26.33GB)}\\ \wqruan{+0.17\% (+1.61GB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}} \wqruan{97.69\% (683.39GB)}\\ \wqruan{-0.05\% (+12.61GB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{2.31\% (16.17GB)}\\ \wqruan{+0.05\% (+0.65GB)}\end{tabular}                       \\ \hline
\multirow{3}{*}{ResNet-50}     & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c}{\wqruan{96.17\% (851.76GB)}}                                                              & \multicolumn{1}{c}{\wqruan{2.04\% (18.05GB)}}                                                              & \wqruan{98.21\% (869.81GB)}                                                              & \wqruan{1.79\% (15.83GB)}                        \\ \cline{2-6} 
    & \hawkeye                                                                & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{96.14\% (863.11GB)}\\ \wqruan{-0.03\% (+11.35GB)}\end{tabular}} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{2.05\% (18.43GB)}\\ \wqruan{+0.01\% (+0.38GB)}\end{tabular}}  & \begin{tabular}[c]{@{}c@{}}\wqruan{98.19\% (881.54GB)}\\ \wqruan{-0.02\% (+11.73GB)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{1.81\% (16.25GB)}\\ \wqruan{+0.02\% (+0.42GB)}\end{tabular}                                                                 \\ \hline
\multirow{3}{*}{MobileNet-V3}  & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c}{\wqruan{75.14\% (73.30GB)}}                                                               & \multicolumn{1}{c}{\wqruan{18.62\% (18.16GB)}}                                                             & \wqruan{93.76\% (91.46GB)}                                                               & \wqruan{6.24\% (6.09GB)}                          \\ \cline{2-6} 
  & \hawkeye                                                                & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{74.98\% (73.33GB)}\\ \wqruan{-0.16\% (+0.03GB)}\end{tabular}}   & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{18.57\% (18.16GB)}\\ \wqruan{-0.05\% (+0.00GB)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{93.55\% (91.49GB)}\\ \wqruan{-0.21\% (+0.03GB)}\end{tabular}   & \begin{tabular}[c]{@{}c@{}}\wqruan{6.45\% \wqruan{(6.31GB)}}\\ \wqruan{+0.21\% (+0.22GB)}\end{tabular}                                                                   \\ \hline
\multirow{3}{*}{ShuffleNet-V2}   & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c}{\wqruan{86.18\% (38.42GB)}}                                                               & \multicolumn{1}{c}{\wqruan{9.20\% (4.10GB)}}                                                              & \wqruan{95.38\% (42.52GB)}                                                               & \wqruan{4.62\% (2.06GB)} \\ \cline{2-6} 
& \hawkeye                                                                & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{86.14\% (38.87GB)}\\ \wqruan{-0.04\% (+0.45GB)}\end{tabular}}   & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\wqruan{9.20\% (4.15GB)}\\ \wqruan{+0.00\% (+0.05GB)}\end{tabular}}  & \begin{tabular}[c]{@{}c@{}}\wqruan{95.34\% (43.02GB)}\\ \wqruan{-0.04\% (+0.50GB)}\end{tabular}   & \begin{tabular}[c]{@{}c@{}}\wqruan{4.66\% (2.10GB)}\\ \wqruan{+0.04\% (+0.04GB)}\end{tabular}                                                                                            \\ \hline
\end{tabular}
}
\label{tab:cryptflow2}
\end{table*}


\section{Performance Evaluation}~\label{sec:exp}
\vspace{-6mm}
\subsection{\wqruan{Implementation}} 
We implement \hawkeye by supplementing about 12k lines of code of Python (including test codes) to the \mpspdz compiler. At first, we modify the instruction generation process of the \mpspdz compiler to implement our proposed block labeling method. Then, we modify the aggregate functions of ReqNode and ReqChild in the \mpspdz compiler to implement our proposed block analysis method. 

\wqruan{For the Autograd library of \hawkeye, we implement all of its operators and functions according to the description of \texttt{PyTorch} API document~\footnote{\url{https://pytorch.org/docs/stable/torch.html}}. Concretely, we implement functions of five \texttt{PyTorch}-related modules (Tensor module, Functional module, NN module, Optimizers, and Dataloader) described in Section~\ref{subsec:autograd_architecture} based on the secure fixed-point numbers computation functions provided by the \mpspdz compiler. The above Autograd library modules are integrated into the \mpspdz compiler as one of its standard libraries. Thus, in \hawkeye, when model designers construct models they want to profile, they can directly import the above modules to construct models in \texttt{PyTorch}.}

% \begin{table*}[ht]
% \centering
% \caption{The online communication round profiling results of four secure CNN model inference processes outputted by \texttt{CryptFlow2}~\cite{rathee2020cryptflow2} and \hawkeye. }
% \scalebox{0.7}{
% \begin{tabular}{c|c|ccc|c}
% \hline
% Model                          & Framework                                                                              & \multicolumn{3}{c|}{\% (Rounds) of linear operators}                                                                                                                                                                                                                       & \% (Rounds) of non-linear operators                                      \\ \hline
% \multicolumn{1}{l|}{}          &                                                                                        & \multicolumn{1}{c|}{\% (Rounds) of Conv2d}                                                     & \multicolumn{1}{c|}{\% (Rounds) of other linear operators}                                    & \% (Rounds) of all                                                        & \multicolumn{1}{l}{}                                                 \\ \hline
% \multirow{3}{*}{DenseNet-121}  & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c|}{\wqruan{83.35\% (10,317)}}                                                           & \multicolumn{1}{c|}{\wqruan{10.83\% (1,340)}}                                                           & \wqruan{94.18\% (11,657)}                                                           & \wqruan{5.82\% (720)}                                                         \\ \cline{2-6} 
%                                & \hawkeye                                                                & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{83.49\% (10,278)}\\ \wqruan{+0.14\% (-39)}\end{tabular}}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{10.67\% (1,313)}\\ \wqruan{-0.17\% (-27)}\end{tabular}}   & \begin{tabular}[c]{@{}c@{}}\wqruan{94.15\% (11,591)}\\ \wqruan{-0.03\% (-66)}\end{tabular}   & \begin{tabular}[c]{@{}c@{}}\wqruan{5.85\% (720)}\\ \wqruan{+0.03\% (+0)}\end{tabular}  \\ \hline
% \multirow{3}{*}{ResNet-50}     & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c|}{\wqruan{87.52\% (7,155)}}                                                            & \multicolumn{1}{c|}{\wqruan{7.12\% (582)}}                                                             & \wqruan{94.64\% (7,737)}                                                           & \wqruan{5.36\% (438)}                                                         \\ \cline{2-6} 
%                                & \hawkeye                                                                & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{87.84\% (7,148)}\\ \wqruan{+0.32\% (-7)}\end{tabular}}     & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{7.07\% (575)}\\ \wqruan{-0.05\% (-7)}\end{tabular}}      & \begin{tabular}[c]{@{}c@{}}\wqruan{94.91\% (7,723)}\\ \wqruan{+0.27\% (-14)}\end{tabular}    & \begin{tabular}[c]{@{}c@{}}\wqruan{5.09\% (414)}\\ \wqruan{-0.27\% (-24)}\end{tabular} \\ \hline
% \multirow{3}{*}{MobileNet-V3}  & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c|}{\wqruan{80.90\% (12,131)} }                                                          & \multicolumn{1}{c|}{\wqruan{15.02\% (2,252)}}                                                           & \wqruan{95.92\% (14,383)}                                                           & \wqruan{4.08\% (612)}                                                         \\ \cline{2-6} 
%                                & \hawkeye                                                                & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{44.94\% (2,234)}\\ \wqruan{-35.96\% (-9,897)}\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{42.75\% (2,125)}\\ \wqruan{+27.73\% (-127)}\end{tabular}} & \begin{tabular}[c]{@{}c@{}}\wqruan{87.69\% (4,359)}\\ \wqruan{-8.23\% (-10,024)}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\wqruan{12.31\% (612)}\\ \wqruan{+8.23\% (+0)}\end{tabular} \\ \hline
% \multirow{3}{*}{ShuffleNet-V2} & \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2} & \multicolumn{1}{c|}{\wqruan{88.61\% (6,827)} }                                                           & \multicolumn{1}{c|}{\wqruan{7.89\% (608)}}                                                             & \wqruan{96.50\% (7,435)}                                                            & \wqruan{3.50\% (270)}                                                         \\ \cline{2-6} 
%                                & \hawkeye                                                                & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{67.37\% (1,745)}\\ \wqruan{-21.24\% (-5,082)}\end{tabular}}  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\wqruan{23.13\% (599)}\\ \wqruan{+15.24\% (-9)}\end{tabular}}    & \begin{tabular}[c]{@{}c@{}}\wqruan{90.50\% (2,344)}\\ \wqruan{-6.00\% (-5091)}\end{tabular}   & \begin{tabular}[c]{@{}c@{}}\wqruan{9.50\% (246)}\\ \wqruan{+6.00\% (-24)}\end{tabular} \\ \hline
% \end{tabular}
% }
% \label{tab:cryptflow2_round}
% \end{table*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/round_crypTFlow.pdf}
    \caption{\wqruan{The online communication round profiling results of four secure CNN model inference processes outputted by \texttt{CryptFlow2}~\cite{rathee2020cryptflow2} and \hawkeye.}}
    \label{fig:round_crypTFlow}
\end{figure*}

\subsection{Accuracy of \hawkeye}~\label{subsec:accuracy_garnet}
\begin{table}[ht]
\centering
\caption{\wqruan{The online/offline communication size profiling results of secure BERT$_{\textsc{base}}$ model inference process outputted by \texttt{CrypTen}~\cite{crypten2020} and \hawkeye.} We report the proportion of each operator's communication size to the total communication size and the communication size of each operator. Following Li et al.~\cite{li2023mpcformer}, we list the communication sizes of MatMul, Gelu, and Softmax operators. }
\scalebox{0.7}{
\begin{tabular}{c|c|c|c}
\hline
Operator                 & Framework                                                                  & \% (GB) of online phase           & \% (GB) of offline phase                                                                  \\ \hline
\multirow{3}{*}{MatMul}  & \texttt{CrypTen}~\cite{crypten2020} & \wqruan{4.75\% (3.22GB)}                                         & \wqruan{7.05\% (1.37GB)}                                                                                                                                     \\ \cline{2-4} 
                         & \hawkeye                                                    & \begin{tabular}[c]{@{}c@{}}\wqruan{4.74\% (3.22GB)} \\ \wqruan{-0.01\% (+0.00GB)}\end{tabular} 
                         & \begin{tabular}[c]{@{}c@{}} \wqruan{7.09\% (1.37GB)}\\ \wqruan{+0.04\% (+0.00GB)}\end{tabular}  
                           \\ \hline
\multirow{3}{*}{Gelu}    & \texttt{CrypTen}~\cite{crypten2020} & \wqruan{26.15\% (17.72GB)}                                       & \wqruan{23.87\% (4.64GB)}                                                                                                                                    \\ \cline{2-4} 
                         & \hawkeye                                                    & \begin{tabular}[c]{@{}c@{}}\wqruan{26.49\% (18.00GB)} \\ \wqruan{+0.34\% (+0.28GB)}\end{tabular} 
                         & \begin{tabular}[c]{@{}c@{}} \wqruan{24.12\% (4.64GB)}\\ \wqruan{+0.25\%(+0.00GB)}\end{tabular}        \\ \hline
\multirow{3}{*}{Softmax} & \texttt{CrypTen}~\cite{crypten2020} & \wqruan{69.10\% (46.83GB)}                                            & \wqruan{69.08\% (13.43GB)}                                                                                                                       \\ \cline{2-4} 
                         & \hawkeye                                                    & \begin{tabular}[c]{@{}c@{}}\wqruan{68.77\% (46.73GB)} \\ \wqruan{-0.33\% (-0.10GB)}\end{tabular} 
                         & \begin{tabular}[c]{@{}c@{}} \wqruan{68.79\% (13.23GB)}\\ \wqruan{-0.29\% (-0.20GB)}\end{tabular} \\
                         \hline
\end{tabular}
}
\label{tab:mpcformer}
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/round_crypTen.pdf}
    \caption{\wqruan{The online communication round profiling results of secure BERT$_{\textsc{base}}$ model inference process outputted by \texttt{CrypTen}~\cite{crypten2020} and \hawkeye.}}
    \label{fig:round_crypTen}
\end{figure}
\noindent\textbf{Experiment Setup.} We verify the accuracy of \hawkeye by comparing the static profiling results outputted by \hawkeye with dynamic profiling results obtained from two MPL frameworks, \wqruanother{i.e.,} \texttt{CypTFlow2}~\cite{rathee2020cryptflow2} and \texttt{CrypTen}~\cite{crypten2020}. We first describe two dynamic profiling processes we compare with.  Firstly, Ganesan et al.~\cite{ganesan2022efficient} dynamically profile secure inference processes of four popular CNN models (\wqruanother{i.e.,} DenseNet-121~\cite{Huang_2017_CVPR}, ResNet-50~\cite{7780459}, MobileNetV3~\cite{howard2019searching}, ShuffleNetV2~\cite{Ma_2018_ECCV}) on \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2}. Secondly, Li et al.~\cite{li2023mpcformer} dynamically profile the secure BERT$_{\textsc{base}}$ model inference process on the two-party backend of \texttt{CrypTen}~\cite{crypten2020}.  We run the open-source codes of the two MPL frameworks~\footnote{The code repository address of \texttt{CrypTFlow2} is \url{https://github.com/mpc-msri/EzPC}. We use the codes at commit \textit{4bae530}.}~\footnote{The code repository address of \texttt{CrypTen} is \url{https://github.com/facebookresearch/CrypTen}. We use \texttt{CrypTen0.4.1}.} to obtain the dynamic profiling results. For dynamic profiling on \texttt{CrypTFlow2}, following Ganesan et al.~\cite{ganesan2022efficient}, we set the bit length as $60$ and the bit length of fixed-point numbers' fractional part as $23$. For dynamic profiling on \texttt{CrypTen}, we set the bit length as $64$ and the bit length of fixed-point numbers' fractional part as $16$. We set the statistical security parameter and computation security parameter of these two MPL frameworks as $40$ and $128$. For the input data of CNN models, we set their sizes as $1 \times 3 \times 224 \times 224$. For the input data of BERT$_{\textsc{base}}$, following Li et al.~\cite{li2023mpcformer}, we set their sizes as $1 \times 512 \times 28996$. Because \texttt{CrypTen} does not support the tanh function, we replace the tanh function with the hardtanh function following Li et al.~\cite{li2023mpcformer}. \wqruan{Note that because \texttt{CrypTFlow2} does not have the offline phase, we only show the offline communication cost profiling results of \texttt{CrypTen}. }
% Besides, because the offline preparation of \texttt{CrypTen} does not depend on circuit structures, it can be finished in the constant round, and we do not consider its offline communication round here.

For static profiling on \hawkeye, we first configure the communication cost of \texttt{CryptoFlow2}~\cite{rathee2020cryptflow2} and \texttt{CrypTen}~\cite{crypten2020}. After that, we implement the models profiled by Ganesan et al.~\cite{ganesan2022efficient} and Li et al.~\cite{li2023mpcformer} in \hawkeye. Finally, we run \hawkeye under the same parameter setting with \texttt{CypTFlow2} and \texttt{CrypTen} to obtain the static profiling results. 

\smallskip
\noindent\textbf{Experimental results.} \wqruan{As is shown in Table~\ref{tab:cryptflow2}, Figure~\ref{fig:round_crypTFlow}, Table~\ref{tab:mpcformer}, and Figure~\ref{fig:round_crypTen}, \hawkeye can accurately profile model communication cost on different MPL frameworks. For communication size, the proportions of operators' online/offline communication size to the total online/offline communication size outputted by \hawkeye, \texttt{CrypTFlow2}, and \texttt{CrypTen} are almost the same, i.e., the proportion differences between baselines and \hawkeye are all smaller than 0.87\%.  The slight differences between the communication size profiling results outputted by \hawkeye and dynamic profiling results could be caused by the following two reasons: (1) The communication complexity of basic operations is asymptotic rather than actual. For example, \texttt{CrypTFlow2} analyzes the communication complexity upper bound of truncation and comparison operations. Because we configure the communication cost for truncation and comparison operations of \texttt{CrypTFlow2} with the upper bound rather than the actual complexity, the communication sizes outputted by \hawkeye are slightly larger than those outputted by \texttt{CrypTFlow2}  (2) The implementations of high-level operations are different. For example, \texttt{CrypTen} implements the max operation by mixing the tree-based and pairwise comparison methods, while \hawkeye purely uses the tree-based method. Thus, \texttt{CrypTen} has a larger communication size on the softmax function than \hawkeye.}

\wqruan{For communication rounds, as is shown in Figure~\ref{fig:round_crypTFlow} and Figure~\ref{fig:round_crypTen}, \hawkeye can accurately profile the communication rounds of most CNN operators on \texttt{CrypTFlow2} and all transformer operators on \texttt{CrypTen}. The main difference between the results outputted by \texttt{CrypTFlow2} and \hawkeye lies in the communication rounds of the Conv2d operators in MobileNet-V3 and ShuffleNet-V2 models. The main reason is that the implementation of the group convolution operator, which is used by MobileNet-V3 and ShuffleNet-V2, in \texttt{CrypTFlow2} is not parallel. Therefore, the group convolution operator in \texttt{CrypTFlow2} would require the group number times of communication rounds than the parallel implementation in \hawkeye. The above results show that besides helping model designers analyze model communication costs, \hawkeye could also help MPL framework developers find the performance issues in their implementation. }


% Concretely, for secure CNN model inference processes, although the online communication sizes of operators outputted by \texttt{CrypTFlow2} are slightly lower than those outputted by \hawkeye, the proportions of operators' online communication size to the total online communication size outputted by \texttt{CrypTFlow2} and \hawkeye are almost the same, \wqruanother{i.e.,} the proportion differences between \texttt{CrypTFlow2} and \hawkeye are all smaller than 1.25\%. For the secure BERT$_{\textsc{base}}$ model inference process, the proportions and online communication sizes of operators outputted by \texttt{CrypTen} and \hawkeye are both almost the same. The slight differences between the profiling results outputted by \hawkeye and dynamic profiling results could be caused by the following two reasons: (1) The communication complexity of basic operations is asymptotic rather than actual. For example, \texttt{CrypTFlow2} analyzes the communication complexity upper bound of truncation and comparison operations. Because we configure the communication cost for truncation and comparison operations of \texttt{CrypTFlow2} with the upper bound rather than the actual complexity, the online communication sizes outputted by \hawkeye are slightly larger than those outputted by \texttt{CrypTFlow2}  (2) The implementations of high-level operations are different. For example, \texttt{CrypTen} implements the max operation by mixing the tree-based and pairwise comparison methods, while \hawkeye purely uses the tree-based method. Thus, \texttt{CrypTen} has a larger communication size on the softmax function than \hawkeye. In summary, even though the asymptotic communication complexity of basic operations and different implementations of high-level operations cause slight bias, the bias has negligible impacts on finding the communication bottleneck of models. 

\wqruan{Besides \texttt{CrypTFlow2} and \texttt{CrypTen}, we compare \hawkeye with two mixed-protocol MPL frameworks (\texttt{Delphi}~\cite{mishra2020delphi} and \texttt{Cheetah}~\cite{Cheetah}) that rely on garbled circuit (GC) and HE. Due to the space limitation, we show the experimental results in Appendix~\ref{appendix:mixed-protocol}. Furthermore, to show the accuracy of \hawkeye in secure model training scenarios, we compare \hawkeye with the two-party backend of \texttt{SecretFlow-SPU}, i.e., \texttt{SecretFlow-SEMI2K}, in Appendix~\ref{appdendix:secure_training}.}


\subsection{Efficiency of \hawkeye}
To demonstrate the efficiency of \hawkeye, \wqruan{we compare the runtimes of \hawkeye and the runtimes of \texttt{CrypTFlow2} and \texttt{CrypTen} in the experiments of Section~\ref{subsec:accuracy_garnet}.} All experiments are run on a Linux server equipped with two 32-core 2.30 GHz Intel Xeon CPUs and 512GB of RAM.

\begin{table}[ht]
\centering
\caption{\wqruan{Runtimes of static and dynamic profiling for five secure model inference processes in Section~\ref{subsec:accuracy_garnet}}.  We report the average results of five runs and show the standard deviations in brackets.}
\scalebox{0.7}{
\begin{tabular}{c|cc}
\hline
 Network                        & \multicolumn{1}{c|}{ \wqruan{Static profiling (s)}}   & \wqruan{Dynamic profiling (s)} \\ \hline
Densenet-121           & \multicolumn{1}{c|}{\wqruan{34.38 ($\pm$ 0.62)}}   & \wqruan{4,429.01 ($\pm$ 65.74)}  \\ 
Resnet-50              & \multicolumn{1}{c|}{\wqruan{39.24 ($\pm$ 0.13)}}   & \wqruan{5,698.77 ($\pm$ 80.06)}  \\ 
\wqruanother{Mobilenet-V3}           & \multicolumn{1}{c|}{\wqruan{31.47 ($\pm$ 0.31}}   & \wqruan{811.92 ($\pm$ 1.81)}  \\ 
Shufflenet-V2          & \multicolumn{1}{c|}{ \wqruan{15.50 ($\pm$ 0.12)}}   & \wqruan{319.34 ($\pm$ 3.44)}   \\ 
BERT$_{\textsc{base}}$ & \multicolumn{1}{c|}{\wqruan{471.54 ($\pm$ 4.98)}} & \wqruan{618.90 ($\pm$ 2.11)}  \\ \hline
\end{tabular}}
\label{tab:compilation_time}
\end{table}
\wqruan{As is shown in Table~\ref{tab:compilation_time}, the efficiency of \hawkeye is promising for model communication cost profiling. Concretely, \hawkeye can profile all CNN models within one minute. The profiling time is about eight minutes even for large BERT$_{\textsc{base}}$ model. In contrast, dynamic profiling the models using \texttt{CrypTFlow2} or \texttt{CrypTen} requires much more time than \hawkeye ($1.31\times \sim 145.23\times$). As a result, \hawkeye enables model designers to efficiently profile the model communication cost and design MPC-friendly models agilely.}

\subsection{Ease of Use}~\label{subsec:ease_use}
We then report on the ease of using \hawkeye to profile the communication cost of models in \texttt{PyTorch} by showing an example of modifying a \texttt{PyTorch}-based logistics regression model training codes to be \hawkeye-based.

\begin{lstlisting}[escapechar=!,mathescape,xleftmargin=2em,framexleftmargin=2em, language=python, columns=fullflexible, caption= {An example of modifying a \texttt{PyTorch}-based logistics regression model training codes to be \hawkeye-based. The removed \texttt{PyTorch} codes are highlighted on a red background and labeled with a minus sign at the beginning of the line. The newly added \hawkeye codes are highlighted on a green background and labeled with a plus sign at the beginning of the line.}, label={listing:code_example}]
class LogisticRegression(nn.Module):
    def __init__(self, n_inputs, n_outputs):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(n_inputs, n_outputs)
    def forward(self,x):
        out = F.sigmoid(self.linear(x))
        return out
!\colorbox[RGB]{255,235,233}{$-$ mnist = datasets.MNIST(root='./data', train=True)}!
!\colorbox[RGB]{230,255,236}{$+$ x = Tensor(60000, 784).get\_input\_from(0)}!
!\colorbox[RGB]{230,255,236}{$+$ y = Tensor(60000, 10).get\_input\_from(0)}!
!\colorbox[RGB]{255,235,233}{$-$ dataloader = DataLoader(mnist, batch\_size=128)}!
!\colorbox[RGB]{230,255,236}{$+$ dataloader = DataLoader(x, y, batch\_size = 128)}!
model = LogisticRegression(784, 10)
optimizer = optim.SGD(model.parameters(), lr = 0.01)
criterion = nn.CrossEntropyLoss()
model.train()
!\colorbox[RGB]{255,235,233}{$-$ for i in range(10):}!
!\colorbox[RGB]{230,255,236}{$+$ @for\_range(10)}!
!\colorbox[RGB]{230,255,236}{$+$ def \_(i):}!
    x, labels = dataloader[i]
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, labels)
    loss.backward()
    optimizer.step()
\end{lstlisting}


As is shown in Listing~\ref{listing:code_example}, the model construction process, model forward process, model backward process, and model optimization process of \hawkeye-based codes are fully consistent with those of \texttt{PyTorch}-based codes. The main differences fall on the data loading and the definition of the loop: (1) Rather than loading local data, in \hawkeye, model designers need to specify the data source and then use the input data to initialize the dataloader (Line 8-12). (2) The loop interface of \hawkeye slightly differs from that of \texttt{PyTorch} (Line 17-19). The differences should be subtle. Meanwhile, because the Autograd library of \hawkeye integrates the communication cost profiling method, model designers can profile the communication cost of the secure logistics regression model training process by directly running \hawkeye to analyze the program shown in Listing~\ref{listing:code_example} without manually inserting test instruments. As a result, model designers can use \hawkeye to profile the communication cost of complex models (\wqruanother{e.g.,} transformers) in \texttt{PyTorch} by modifying less than ten lines of code. In contrast, Li et al.~\cite{li2023mpcformer} have to manually insert about 100 lines of codes to dynamically profile the communication cost of secure transformer inference processes on \texttt{CrypTen}. More examples of implementing complex models in \hawkeye can be found in our source codes.





\subsection{Case Studies}~\label{subsec:case_studies}
In this section, we conduct three case studies to show the practical applications of \hawkeye. Firstly, to show that \hawkeye can help model designers find communication bottlenecks of models on MPL frameworks with different security models, we apply \hawkeye to profile model communication cost on four MPL frameworks whose security models are different. Secondly, to show that \hawkeye can help model designers choose a proper optimizer for secure model training, we apply \hawkeye to profile the communication cost of secure model training processes with two different optimizers. Finally, we apply \hawkeye in model computational graph optimization to improve the efficiency of secure model inference.

\subsubsection{Communication bottlenecks of models on MPL frameworks with different security models}
\noindent\textbf{Security models of MPL frameworks.} We first introduce security models of MPL frameworks. The security model of an MPL framework refers to assumptions the MPL framework makes about parties. In scenarios with different security requirements, model designers usually need to employ MPL frameworks with the corresponding security models. One security model generally compromises two dimensions: the behavior of the parties and the number of colluded parties. Depending on whether parties follow the protocols, the security models of MPL frameworks can be classified as semi-honest and malicious. Depending on whether the number of colluded parties is strictly below half of the total number of parties or not, the security models of MPL frameworks can be classified as honest-majority and dishonest-majority.

We apply \hawkeye to profile the secure inference processes of Resnet-50 and BERT$_{\textsc{base}}$ models on four MPL frameworks, \wqruanother{i.e.,}  \texttt{ABY}~\cite{aby}, \texttt{SPDZ-2k}~\cite{spdz2k}, \texttt{ABY3}~\cite{mohassel2018aby3}, and \texttt{Falcon}~\cite{wagh2020falcon}, whose security models are (semi-honest, dishonest-majority), (malicious, dishonest-majority), (semi-honest, honest-majority),  (malicious, honest-majority) respectively.
For parameter settings, we set the bit length as $64$, the statistical security parameter as $40$, the computational security parameter as $128$, the bit length of fixed-point numbers' fractional part as $16$, and the number of parties as two for \texttt{ABY} and \texttt{SPDZ-2k}, three for \texttt{ABY3} and \texttt{Falcon}. The input data sizes are consistent with those used in Section~\ref{subsec:accuracy_garnet}.
% We show communication cost configurations of the four MPL frameworks in Appendix~\ref{appendix:protocol_config}.

As is shown in Figure~\ref{fig:protocol_profiling}, two dimensions of the security model have significantly different impacts on communication bottlenecks of models: (1) Under the same assumption on the number of colluded parties, switching the assumption on the behavior of parties from semi-honest to malicious slightly changes the profiling results. Therefore, MPC-friendly models optimized for semi-honest MPL frameworks could remain effective for malicious MPL frameworks.  (2) When underlying MPL frameworks are designed for a dishonest majority, the communication bottlenecks are linear operators, \wqruanother{i.e.,} Matmul or Conv2d. In contrast, the communication bottlenecks become non-linear operators (\wqruanother{i.e.,} Softmax, Relu, Gelu, MaxPool) when underlying MPL frameworks are designed for an honest majority. Therefore, model designers would need to tailor MPC-friendly models for different scenarios where the assumptions on the number of colluded parties are different. 
The above results show that \hawkeye can help model designers efficiently find communication bottlenecks of models on MPL frameworks with different security models. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/protocol_profiling.pdf}
    \caption{The proportion of each operator's online communication size to the total online communication size on four MPL frameworks with different security models.}
    \label{fig:protocol_profiling}
\end{figure}

\subsubsection{Choice of optimizers in secure model training}  We apply \hawkeye to profile the communication cost of secure model training processes with two optimizers. A few studies~\cite{10.1145/3411501.3419427,DBLP:journals/popets/AttrapadungHIKM22} show that in secure model training, Adam~\cite{adam} could be a better optimizer than SGD because Adam can significantly improve the convergence speed. However, Adam incurs much more communication overhead than SGD. To quantitatively analyze the communication cost of optimizers, we apply \hawkeye to profile the communication cost of three secure CNN model training processes (LeNet, AlexNet, VGG-16) on two MPL frameworks (\texttt{ABY}, \texttt{ABY3}). Meanwhile, following previous studies~\cite{watson22piranha,cryptGPU}, we replace MaxPool in the CNN models with AvgPool. Finally, we set the input data size as $128 \times 3 \times 28 \times 28$ for LeNet, $128 \times 3 \times 224 \times 224$ for AlexNet and VGG-16, where $128$ is the batch size. Other parameter settings are the same as those of the first case study. Note that we run the secure model training processes on one batch of data.  Because the model training process is the same for each batch of data, the proportion of each part's online communication size to the total online communication size remains unchanged as the batch number changes.

\begin{table}[ht]
\centering
\caption{The online communication size of gradient computation and optimization of three secure CNN model training processes. We report the proportion of each part's online communication size to the total online communication size and the online communication size of each part.}
\scalebox{0.7}{
\begin{tabular}{c|c|cc}
\hline
Framework              & Model        & Grad Computation & Optimization \\ \hline
\multirow{6}{*}{\texttt{ABY}~\cite{aby}}  & Lenet-SGD    & 100.00\% (10.47GB)               & 0.00\% (0GB)                       \\ 
                      & Lenet-Adam   & 92.17\% (10.47GB)                      & 7.83\% (0.89GB)                  \\ 
                      & AlexNet-SGD  & 100.00\% (12,526.23GB)                   & 0.00\% (0GB)                       \\ 
                      & AlexNet-Adam & 97.06\% (12,526.23GB)                   & 2.94\% (379.67GB)                \\ 
                      & VGG-16-SGD   &  100.00\% (189,079.51GB)                  & 0.00\% (0GB)                       \\  
                      & VGG-16-Adam  & 99.55\% (189,079.51GB)                  & 0.45\% (859.72GB)                \\ \hline
\multirow{6}{*}{\texttt{ABY3}~\cite{mohassel2018aby3}} & Lenet-SGD    & 98.94\% (0.10GB)                   & 1.06\% ($<0.01$GB)         \\ 
                      & Lenet-Adam   & 32.26\% (0.10GB)                      & 67.74\% (0.21GB)                  \\ 
                      & AlexNet-SGD  & 96.34\% (12.12GB)                     & 3.66\% (0.46GB)                  \\ 
                      & AlexNet-Adam & 3.99\% (12.12GB)                     & 96.01\% (291.35GB)                \\ 
                      & VGG-16-SGD   & 99.80\% (521.60GB)                    & 0.20\% (1.03GB)                 \\ 
                      & VGG-16-Adam  & 44.15\% (521.60GB)                    & 55.85\% (659.74GB)                \\ \hline
\end{tabular}
}
\label{tab:choice_optimizer}
\end{table}

As is shown in Table~\ref{tab:choice_optimizer}, the extra communication cost incurred by Adam significantly differs among different models and MPL frameworks. When training models on \texttt{ABY}, the online communication size of Adam only accounts for $0.45\% \sim 7.83\%$ of the total online communication size. In this case, replacing SGD with Adam would significantly improve the training efficiency because the extra communication cost incurred by Adam is minor compared with the communication cost of gradient computation. In contrast, when training models on \texttt{ABY3}, replacing SGD with Adam would increase the total online communication size by $2.26 \sim 24.12$ times. Especially when securely training the AlexNet model on \texttt{ABY3}, replacing SGD with Adam would cause the total online communication size to increase by $24.12$ times. In this case, SGD should be a better optimizer than Adam because the convergence speed improvement brought by Adam cannot cover its extra communication cost. 

\subsubsection{\wqruan{Computational graph optimization}}
\wqruan{To further show the practical application of \hawkeye, we combine \hawkeye with TASO~\cite{10.1145/3341301.3359630}, a classical computational graph optimization method for deep learning models, to effectively reduce the communication overhead of secure model inference. TASO improves the efficiency of secure model inference by changing the structure of the computational graphs that represent the model inference process. Meanwhile, TASO ensures that the optimized computational graph is equivalent to the original computational graph. In this experiment, we use the online communication size outputted by \hawkeye as the cost model of TASO. For the underlying MPL framework, we use the three-party protocol of \texttt{Deep-MPC} proposed by Keller and Sun~\cite{pmlr-v162-keller22a}, whose source codes are included in \texttt{MP-SPDZ}, as our target MPL framework. We show the communication cost configuration of \texttt{Deep-MPC} in Appendix~\ref{appendix:protocol_config}. Meanwhile, following Jia et al.~\cite{10.1145/3341301.3359630}, we choose the backbone network (i.e., model components excluding the stem component and the final classifier) of ResNet-18 and ResNet-50 models as target models.}

\begin{table}[ht]
\centering
\caption{\wqruan{The communication size and communication time of two CNN models that are optimized by original TASO and \hawkeye-enhanced TASO (TASO-\hawkeye). The communication time is obtained under the WAN setting where round-trip time is 72ms, and bandwidth is 9 MBps. We report the average results of five runs and show the standard deviations in brackets.}}
\scalebox{0.7}{
\begin{tabular}{c|c|c|c}
\hline
Model                      & Method       & Comm Size (MB) & Comm Time (s)    \\ \hline
\multirow{3}{*}{ResNet-18} & Original     & \wqruan{286.10MB}       & \wqruan{41.30s ($pm$ 0.68s)}   \\ \cline{2-4} 
                           & TASO   &  \wqruan{247.49MB }      &  \wqruan{37.00s ($pm$ 0.74s)}   \\ \cline{2-4} 
                           & TASO-\hawkeye &  \wqruan{210.92MB}       &  \wqruan{30.54s ($pm$ 1.26s) }  \\ \hline
\multirow{3}{*}{ResNet-50} & Original     &  \wqruan{ 1758.85MB }     &  \wqruan{207.71s ($pm$ 8.17s)} \\ \cline{2-4} 
                           & TASO   &  \wqruan{1647.08MB }     &  \wqruan{193.54s ($pm$ 8.72s)}  \\ \cline{2-4} 
                           & TASO-\hawkeye &  \wqruan{1492.64MB  }    &  \wqruan{174.27s ($pm$ 5.48s)}  \\ \hline
\end{tabular}}
\label{tab:taso}
\end{table}
\wqruan{As is shown in Table~\ref{tab:taso}, \hawkeye-enhanced TASO significantly outperforms the original TASO. Concretely, \hawkeye-enhanced TASO can reduce the communication overhead by 15.14\% $\sim$ 26.28\% and the communication time by 16.10\% $\sim$ 26.05\%, which is 1.95 $\sim$ 2.38 times and 2.36 $\sim$ 2.50 times that of the original TASO, respectively. The above results show that \hawkeye can effectively help the efficiency optimization of secure model inference.}
% \wqruan{We apply HawkEye to redesign the cost model in the computation graph optimization for MPL. We just simply extend TASO, which is a graph optimization framework for DNN models. TASO uses three plain machine learning metrics (i.e. FLOPs, memory usage, and number of kernel launches) as the cost model. Its relaxed graph substitution algorithm uses a priority queue to select the lowest-cost graph in the queue as the source and use those subustution rules matching the graph to produce new graphs from the source. The new graphs whose cost are exceed $\alpha$ higer than the best graph are dropped, and the left are added to the searching queue for next iteration. $\alpha$ is a searching parameter designed manually. }

% \wqruan{We compare the optimizing graph's online commucation between the HawkEye-based cost model and the TASO-based cost model, using the above graph substitution algorithm searching for 600 graphs for two DNN models (ResNet18 and ResNet50). For parameter setting, we set the bit length as 64 and the bottom MPC protocol as ABY3, the searching parameter $\alpha$ as 1.05. The results are shown in the following table. the input data size is 1 × 3 × 224 × 224, the batch size is 1 for secure inference scenario.}


% \wqruan{As is shown in Table, Hawkeye help improving the graph optimizing process for MPL. For resnet18, the hawkeye-optimized online communication is 42.29\% of the origin model while the TASO-optimized online communication is 68.64\% of the origin model. For resnet50, the hawkeye-optimized online communication is 85.05\% of the origin model while the TASO-optimized online communication is 69.28\% of the origin model. At the same searching graphs amount, Haweye get in the less online communication cost and the faster end-to-end model inference lantency. Compared with the base TASO cost model, the Hawkeye cost model can reduce the searching time to find the nearly best graph for MPL and achieve less commuication result owing to the accurate profiling model for MPL.}