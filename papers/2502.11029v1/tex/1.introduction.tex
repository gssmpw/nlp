\section{Introduction}~\label{sec:intro}
As more and more countries publish privacy protection regulations, such as GDPR from the EU, multi-party computation (MPC) based machine learning, referred to as multi-party learning (MPL)~\cite{song2020sok,10.1145/3548606.3560697}, has become an important technology for utilizing data from multiple parties with privacy preservation. On the one hand, in \wqruanother{industry}, many IT giants have released MPL frameworks, such as \texttt{CrypTen}~\cite{crypten2020} released by Facebook, \texttt{TF-Encrypted}~\cite{TF-Encrypted} released by Google, and \texttt{SecretFlow}~\cite{secretflow} released by Ant group et al., to meet their data sharing requirements with high-security restrictions, where raw data must be stored distributively. For example, Ant group applies \texttt{SecretFlow} to deliver personalized policies for different customers of insurers. On the other hand, in \wqruanother{academia}, researchers have proposed dozens of MPL frameworks, such as \texttt{Cheetah}~\cite{Cheetah}, \texttt{Delphi}~\cite{244032},  and \texttt{CRYPTGPU}~\cite{cryptGPU}, in recent years. However, although MPL has \wqruanother{started to be used broadly}, the huge communication overhead still seriously limits its practical applications.

In recent years, \wqruanother{to reduce the huge communication overhead of MPL, many MPC-friendly models~\cite{ganesan2022efficient, MPCViT, li2023mpcformer, liu2023llms, Peng_2023_ICCV, dhyani2023privit} have been proposed.} \wqruanother{Within} the optimization of MPC-friendly models, a critical element to \wqruanother{tackle} the challenge is profiling the communication cost of models. For example, through profiling the communication cost of the secure DenseNet-121~\cite{Huang_2017_CVPR} inference process, Ganesan et al.~\cite{ganesan2022efficient} \wqruanother{found} that the convolution layer is the communication bottleneck. Then, they \wqruanother{designed} an optimized convolution operator to reduce its communication overhead. \wqruanother{In} the above process, profiling the model communication cost is a key step, which is also a popular methodology of efficiency optimization in the machine learning field~\cite{10.1145/3379337.3415890, NEURIPS2022_4fc81f4c}. 


Even though model communication cost profiling is a key step in the design of MPC-friendly models, an effective model communication cost framework is still absent. Therefore, model designers have to manually establish the profiles to find communication bottlenecks of models, often involving burdensome human efforts in a monotonous procedure.
Intuitively, model designers can profile model communication cost by inserting test instruments between each layer and dynamically running the secure model training or inference processes on a specific MPL framework~\cite{ganesan2022efficient, MPCViT, li2023mpcformer, dhyani2023privit}. We refer to this method as dynamic profiling. Although dynamic profiling can accurately find the communication bottleneck of models, it requires specific designs and implementation for different models. As a result, dynamic profiling usually takes burdensome human efforts and computing resources, thus being less efficient. On the other hand, a few MPL frameworks, such as \texttt{SecretFlow-SPU}~\cite{secretflow}, support model communication cost profiling at the operation level, \wqruanother{i.e.,} outputting the communication cost of each basic operation (\wqruanother{e.g.,} multiplication, comparison). \wqruanother{However, because \texttt{SecretFlow-SPU} uses Google's XLA~\cite{50530} as a black box to compile the input program, it requires model designers to manually construct many models with a single layer (\wqruanother{e.g.,} convolution) to test the communication cost of different model components.} Note that it is the key to analyzing the communication cost of each layer when model designers try optimizing MPC-friendly model~\cite{ghodsi2020cryptonas}.

\wqruanother{In this paper, we propose \hawkeye, a static model communication cost profiling framework to accurately profile model communication cost without dynamic profiling.} Firstly, to profile the communication cost of models with complex structures, we propose a static communication cost profiling method based on a prefix structure that records the function calling chain. We describe the details of the prefix structure in Section~\ref{subsec:block_label_analysis}. With this method, the communication cost of models can be automatically profiled without manually inserting test instruments and dynamically running the secure model training or inference process. Because the program execution flow must be input-independent to ensure security in MPL frameworks, we can accurately know the computations required by a secure model training or inference process through such static analysis. Therefore, the static profiling results from \hawkeye are consistent with those obtained by the dynamic profiling. 

Secondly, \hawkeye employs an automatic differentiation (\wqruanother{Autograd}) library, which integrates our proposed static communication cost profiling method, to assist model designers in profiling the communication cost of models in \texttt{PyTorch}. In particular, to avoid the bias brought by the extra communication overhead of the broadcast operator, which is commonly used to construct machine learning models, we design an optimized broadcast operator that is communication-efficient for various MPL frameworks. Based on our proposed static communication cost profiling method and the Autograd library, \hawkeye can receive \texttt{PyTorch}-based model training or inference codes, then automatically profile model communication cost. As a result, with \hawkeye, model designers can focus on optimizing the structure of MPC-friendly models without spending much time on profiling model communication costs.

Finally, we compare the static profiling results outputted by \hawkeye with the dynamic profiling results obtained from \wqruanother{five MPL frameworks,  \texttt{CryptFlow2}~\cite{rathee2020cryptflow2}, \texttt{CrypTen}~\cite{crypten2020}, \texttt{Delphi}~\cite{mishra2020delphi}, \texttt{Cheetah}~\cite{Cheetah}, and \texttt{SecretFlow-SEMI2K}~\cite{secretflow}}. The experimental results show that \hawkeye can accurately profile the communication cost of models in various MPL frameworks without dynamic profiling. Furthermore, we conduct \wqruanother{three} case studies to show the practical applications of \hawkeye (Section~\ref{subsec:case_studies}). For example, by applying \hawkeye to profile the communication cost of models in four MPL frameworks with different security models, we find that under the same assumption on the number of colluded parties, MPC-friendly models optimized for MPL frameworks with semi-honest security models remain effective when model designers transfer the same optimization for MPL frameworks with malicious security models. The above results show that  \hawkeye can effectively help model designers optimize the structures of MPC-friendly models without dynamic profiling. 

To summarize, we highlight our contributions as follows.
\begin{itemize}
    \item  We propose a static model communication cost profiling method to accurately analyze the communication cost of models in MPL frameworks without dynamic profiling.
    
    \item  We design and implement \hawkeye with an Autograd library to assist model designers in profiling the communication cost of models in \texttt{PyTorch}. 
    % \wqruanother{Therefore, with \hawkeye, model designers can quickly find the communication cost bottleneck of models run on different MPL frameworks and optimize MPC-friendly models agilely.} 
\end{itemize}

% \wqruanother{We organize the rest of the paper as follows. In Section~\ref{sec:pre} includes the necessary background knowledge to understand the paper.}