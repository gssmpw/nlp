\section{Autograd Library of \hawkeye}~\label{sec:autograd}
% \vspace{-6mm}
\subsection{Architecture}~\label{subsec:autograd_architecture}
As is shown in Figure~\ref{fig:overview-autograd}, the Autograd library of \hawkeye contains six modules: Secure matrix computation module, Tensor module, Functional module, NN module, Optimizers, and Dataloader. The Secure matrix computation module is the basis of the other five modules and would not be exposed to model designers. The interfaces of the other five modules are fully consistent with those of \texttt{PyTorch}. We then describe the above six modules as follows.

    \noindent \textbf{(1) Secure matrix computation module.} This module contains secure matrix computation functions that are necessary for MPL. These functions are either our supplemented computation functions (\wqruanother{e.g.,} permutation) or provided by the original \mpspdz compiler (\wqruanother{e.g.,} matrix multiplication). The module is the basis of the \hawkeye Autograd library. All of the other five modules are built on this module.
    
    \noindent \textbf{(2) Tensor module.} This module contains tensor computation operators, such as element-wise operators and batch matrix-matrix products. As is mentioned in Section~\ref{subsec:pre_autograd}, each tensor computation operator of the Tensor module is overloaded to contain a forward function and a derivative computation function, such that the gradients of parameters would be automatically derived. We label the forward and derivative computation functions of each operator of this module to support model communication cost profiling.
    
    \noindent \textbf{(3) Functional module.} This module contains activation functions (\wqruanother{e.g.,} Relu) and functions that depend on model parameters (\wqruanother{e.g.,} BatchNorm). Like the Tensor module, each operator of this module contains a labeled forward function and a labeled derivative computation function.
    
    \noindent  \textbf{(4) NN module.} This module contains operators (\wqruanother{e.g.,} Conv2d) and containers (\wqruanother{e.g.,} Sequential) for model construction. The operators of the NN module are implemented using functions from the Tensor module and the Functional module. Like \texttt{PyTorch}, model designers can also define new operators based on operators of the NN module, the Tensor module, and the Functional module.
    
    \noindent  \textbf{(5) Optimizers.} Optimizers contain popular optimizers in machine learning, \wqruanother{e.g.,} stochastic gradient descent (SGD) and Adam~\cite{adam}. Their step functions that require communication are also labeled to profile the secure model training processes.
    
    \noindent \textbf{(6) Dataloader.} Dataloader shuffles the input data and converts the input data into tensors.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.42\textwidth]{figures/overview-autograd.pdf}
    \caption{The architecture of the \hawkeye Autograd library.  }
    \label{fig:overview-autograd}
\end{figure}

\subsection{Optimization of Broadcast Operator}
\noindent\textbf{Strawman implementation of the broadcast operator.} We first describe the strawman implementation of the broadcast operator and show that the implementation would bring extra communication overhead. Taking broadcast multiplication as an example, as shown in Figure~\ref{fig:broadcast_forward}, given two inputs $A$, $B$ with shapes (1, 2) and (2, 2), broadcast multiplication performs element-wise multiplication between each column of $B$ and $A$ to output $C$. During the backward phase, to compute the derivative of $A$, as is shown in Figure~\ref{fig:broadcast_strawmanbackward}, a straightforward method is first performing element-wise multiplication between $\Delta C$ and $B$ to obtain an intermediate result $\Delta A_{inter}$. Then, we can sum each column of $\Delta A_{inter}$ to reduce $\Delta A_{inter}$ as $\Delta A$. The main drawback of the strawman implementation is that it produces the intermediate result with a larger shape than the final result, which causes extra communication overhead. The main reason is that the communication costs of some MPL frameworks, \wqruanother{e.g.,} \texttt{ABY3}, \wqruanother{only depend on the size of the computation results and are independent of the size of input data and intermediate results.} Therefore, the intermediate result $\Delta A_{inter}$ whose size is two times the size of the final result $\Delta A$ would bring significantly more communication overhead, causing bias in the static profiling results from \hawkeye.

\begin{figure}[htbp]
    \centering
    \subfloat[Forward phase 
    \label{fig:broadcast_forward}]
    {
        \includegraphics[width=1.9cm]{figures/broadcast1.pdf}
    }\hfill
    \subfloat[Strawman backward phase
    \label{fig:broadcast_strawmanbackward}]
    {
        \includegraphics[width=2.2cm]{figures/broadcast2.pdf}
    }\hfill
    \subfloat[Optimized backward phase \label{fig:broadcast_optimizaedbackward}]
    {\includegraphics[width=2.9cm]{figures/broadcast3.pdf}}
    \caption{The strawman implementation of broadcast multiplication and optimized broadcast multiplication.}
    \label{fig:broadcast}
\end{figure}

\noindent\textbf{Optimized broadcast operator.} To address the drawback of the strawman implementation, we fuse the broadcast computation and intermediate result reduction as a parallel vector computation to compute the final result directly. Concretely, as shown in Figure~\ref{fig:broadcast_optimizaedbackward}, the backward process of the optimized broadcast operator is completed by performing the dot product between each row of $\Delta C$ and $B$ in parallel. In this way, we can avoid the intermediate result with a large size, thus avoiding extra communication overhead. For more complex broadcast cases, we can transform the input data into the two-dimensional case shown in Figure~\ref{fig:broadcast} through reshaping and permuting. For example, for two inputs with shapes (5, 3, 2) and (3, 2), we can reshape them as (5, 6) and (1, 6) and finish the broadcast computation in the optimized way.

\subsection{Integration of Model Communication Cost Profiling Method}~\label{subsec:autograd_profiling}
We then describe how the Autograd library of \hawkeye integrates the static model communication cost profiling method introduced in Section~\ref{sec:analysis}. We first discuss two requirements that the Autograd library of \hawkeye should meet to enable model designers to profile the communication cost of models in \texttt{PyTorch}. Firstly, the communication cost of the model forward and backward processes should be profiled automatically. Manually inserting test instruments in secure model training or inference codes could be time-consuming for model designers. Meanwhile, even though a few MPL frameworks (\wqruanother{e.g.,} \texttt{CrypTen}~\cite{crypten2020}) provide user-friendly model construction interfaces to assist model designers in building a secure model training or inference process, they hide the derivative computation functions of operators. Thus, model designers have to modify the source codes of these MPL frameworks to profile the communication cost of model backward processes. The code modification process would require burdensome human efforts. Therefore, the Autograd library of \hawkeye should automatically profile the communication cost of the model forward and backward processes.  

\begin{lstlisting}[escapechar=!,mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {The labeled exp function in the Tensor module of the Autograd library of \hawkeye. The newly added labeling codes are highlighted on a green background and labeled with a plus sign at the beginning of the line.},columns=fullflexible, label = {list:autograd_labeling}]
!\colorbox[RGB]{230,255,236}{$+$ @buildingblock("exp-forward")}!
def exp(self):
!\colorbox[RGB]{230,255,236}{$+ \;$ @buildingblock("exp-backward")}!
    def propagate(dl_doutputs, operation):
        ...... 
        #The derivative computation process of exp
        return dl_dinputs
    ..... #The forward process of exp
    return result
\end{lstlisting}


Secondly, the granularity of model communication cost profiling can be adjusted without manually inserting test instruments. To simplify the model construction process, model designers usually construct models in a hierarchical manner. For example, one Densenet-121 model~\cite{Huang_2017_CVPR} contains multiple Bottlenecklayers and TransitionLayers. These two types of sub-modules both contain Conv2d operators, pooling operators, Relu functions, etc. To comprehensively profile the model communication cost, model designers usually need to test the communication cost of operators at different granularity. 

\begin{lstlisting}[escapechar=!,mathescape,xleftmargin=2em,framexleftmargin=2em, caption = {An example of labeling the forward functions of sub-modules to adjust the granularity of model communication cost profiling. The newly added labeling codes are highlighted on a green background and labeled with a plus sign at the beginning of the line.},columns=fullflexible, label = {list:component_labeling}]
class TransitionLayer(nn.Module):
    def __init__(self, c_in, c_out):
        super().__init__()
        self.layers = nn.Sequential(
            #label: transitionlayer-batchnorm
            nn.BatchNorm2d(num_features=c_in),
            #label: transitionlayer-relu
            nn.ReLU(),
            #label: transitionlayer-conv2d
            nn.Conv2d(in_channels=c_in, 
                    out_channels=c_out, kernel_size=1),
            #label: transitionlayer-avgpool2d
            nn.AvgPool2d(kernel_size=2))
!\colorbox[RGB]{230,255,236}{$+ \;$  @buildblock("transitionlayer")}! #labeling transitionlayer
    def forward(self, x):
        out = self.layers(x)
        return out
\end{lstlisting}


We meet the first requirement by labeling the forward and derivative computation functions of each operator and function that requires communication. For example, as is shown in Listing~\ref{list:autograd_labeling}, the exp function in the Tensor module contains a propagate function that defines the derivative computation process of the exponentiation operator, and the rest part defines its forward process. To profile the communication cost of the forward and backward processes of the exponentiation operator, we label the exp function and its propagate function with the function labeling interface described in Section~\ref{subsec:label_interface}. When model designers use the exponentiation operator to construct models and profile them with \hawkeye, the communication cost of the exponentiation operator can be obtained by summing the items whose keys contain ``\textit{exp-forward}'' or ``\textit{exp-backward}'' in the profiling results. Furthermore, the communication cost of the exponentiation operator forward process can be separately computed by summing the items whose labels contain ``\textit{exp-forward}''. We apply the above labeling process to each operator and function that requires communication in the Autograd library of \hawkeye. As a result, \hawkeye can automatically profile the communication cost of the model forward and backward processes.



For the second requirement, \hawkeye meets it based on the prefix structure of our proposed static communication cost profiling method. Concretely, model designers can adjust the granularity of model communication cost profiling by labeling the forward functions of sub-modules with the function labeling interface described in Section~\ref{subsec:label_interface}. Taking the Densenet-121 model as an example, as mentioned above, two sub-modules of the Densenet-121 model, \wqruanother{i.e.,} Bottlenecklayers and TransitionLayers, both contain basic operators (\wqruanother{e.g.,} Conv2d) of the \hawkeye Autograd library. If model designers do not label forward functions of these two sub-modules, \hawkeye would output the total communication cost of basic operators without distinguishing the sub-modules these basic operators belong to. In contrast, as is shown in Listing~\ref{list:component_labeling}, if model designers label the forward function of the TransitionLayer with the label ``\textit{transitionlayer}'', the communication cost of basic operators in TransitionLayer would be outputted separately with the prefix ``\textit{transitionlayer}''. Because composing sub-modules is a common way to construct complex models, labeling the forward functions of sub-modules should be a promising way for model designers to adjust the granularity of model communication cost profiling.


