\section{Preliminaries}~\label{sec:pre}
\vspace{-6mm}
\subsection{Multi-Party Learning}~\label{subsec:mpl}
MPL enables multiple parties to collaboratively perform model training or inference on their private data with privacy preservation. Since Mohassel and Zhang published \texttt{SecureML}~\cite{DBLP:conf/sp/MohasselZ17}, the pioneer study of MPL, it has become a significant topic in both \wqruanother{industry and academia}. Currently, There are mainly two technical routes to implement MPL: secret sharing~\cite{ref_damgard} and homomorphic encryption (HE)~\cite{yi2014homomorphic}. Because secret sharing-based MPC protocols usually have higher efficiency in arithmetic operations, MPL frameworks mainly take secret sharing-based MPC protocols as their underlying protocols to implement secure model training or inference.  \wqruanother{HE is typically used to implement secure model inference because it can significantly reduce the computation and communication overhead of clients who hold data.}
% In \hawkeye, we focus on model communication cost profiling on secret sharing-based MPL frameworks~\cite{aby, mohassel2018aby3}, whose performance bottleneck is usually the communication cost.

\smallskip
\noindent\textbf{Fixed-point number representation and computation.} Fixed-point number is a widely used data representation in MPL. A fixed-point number $\widetilde{x}$ with $f$-bit precision is encoded by mapping it to an integer $\overline{x}$, \wqruanother{i.e.,} $\overline{x} = \widetilde{x} * 2^f$, where $\overline{x}$ is an element of ring or field. Since Multiplication would double the fractional part of the output, \wqruanother{i.e.,} $\overline{z} = \overline{x}*\overline{y} = \widetilde{x} * \widetilde{y} * 2^{2f}$, we need to perform truncation on the output to restore the bit length of its fractional part as $f$. 

We then introduce the basic operations that are necessary for a secret sharing-based MPL framework as follows: 


   \noindent \textbf{Share}: Given an input $x$, it generates $m$ shares $\share{x}_1, \cdots$, $\share{x}_m$ and distributes them to corresponding parties.
   
   \noindent \textbf{Reveal}: Given $m$ shares $\share{x}_1, \cdots, \share{x}_m$, it reconstructs the original value $x$.
   
    % \item Addition: Given two shares $\share{x}_i, \share{y}_i$, Addition outputs $\share{z}_i$ to $P_i$ such that $z = x+y$.
   \noindent \textbf{Multiplication}: Given two shares $\share{x}_i, \share{y}_i$, it outputs $\share{z}_i$ to $P_i$ such that $z = x*y$. 
    % \item Matrix\_Multiplication: Given two secret shared matrix $\share{\mathbb{X}}_i$, $\share{\mathbb{Y}}_i$, it outputs $\share{\mathbb{Z}}_i$ to $P_i$ such that $\mathbb{Z} = \mathbb{X}*\mathbb{Y}$. 
% \end{itemize}

The more complicated operations, such as truncation or exponentiation, can be implemented by composing the above basic operations~\cite{10.1007/978-3-642-15317-4_13, div2mp}. Besides combining basic operations, some MPL frameworks~\cite{mohassel2018aby3, aby,wagh2020falcon} provide special optimizations for complicated operations. For example, Mohassel et al.~\cite{mohassel2018aby3} design an efficient comparison protocol in \texttt{ABY3}. In this case, the complicated operations with the special optimizations can be viewed as basic operations of the corresponding MPL framework.
 \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.33\textwidth]{figures/tree_of_mpspdz.pdf}
    \caption{An example of the \mpspdz compiler's block tree. Rectangles and ovals represent ReqNodes and ReqChilds.}
    \label{fig:overview_mpspdz}
\end{figure}


\subsection{\mpspdz Compiler}~\label{subsec:mpspdz_compiler}
We build upon the \mpspdz compiler to construct \hawkeye. The \mpspdz compiler translates \textit{mpc} programs written in Python into a sequence of instructions that represent basic operations of MPL frameworks and store instructions in instruction blocks that contain instructions without branching. Through organizing instruction blocks generated from an \textit{mpc} program as a block tree, the \mpspdz compiler can output the number of basic operations required by the \textit{mpc} program.

As is shown in Figure~\ref{fig:overview_mpspdz}, the block tree of the \mpspdz compiler contains two types of nodes: ReqNode and ReqChild. ReqNode stores a list of instruction blocks. ReqChild records the control flow information. The root node of a block tree is a ReqNode. Concretely, a ReqNode instance contains an aggregate function, a list of instruction blocks, and a list of children, which could be ReqNode or ReqChild. The aggregate function of a ReqNode instance is used to count the number of basic operations in its instruction blocks and children. A ReqChild instance contains an aggregator function, an aggregate function, and a list of children, which must be ReqNode. The aggregator function of a ReqChild instance is an anonymous function that contains the control flow information. For example, in Figure~\ref{fig:overview_mpspdz}, the aggregator function of ReqChild2 is an anonymous function that multiplies input data (\wqruanother{i.e.,} the operation statistics from its children) fifty times because the instructions in ReqNode4 represent a loop body in a loop whose size is $50$. The aggregate function of a ReqChild is used to sum the operation statistics outputted by its aggregator function. After the \mpspdz compiler generates the block tree from an \textit{mpc} program, it recursively calls the aggregate functions of nodes in the block tree to compute the number of operations required by the \textit{mpc} program. 

\subsection{Automatic Differentiation}~\label{subsec:pre_autograd}
Autograd automatically computes the derivative (gradients) of parameters to simplify model construction. It automatically computes the partial derivative of a function by expressing the function as a sequence of basic operators. The partial derivatives of output to the intermediate values and inputs are computed by applying the chain rule to these basic operators. Therefore, with autograd technology, model designers only need to define the forward process of models, and the gradients of model parameters can be automatically computed in the backward process. Following \texttt{PyTorch}, we design and implement the Autograd library of \hawkeye based on operator overloading-based autograd technology. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/autograd.pdf}
    \caption{An example of the operator overloading-based autograd. The solid lines represent the forward process, and the dashed lines represent the backward process.}
    \label{fig:augograd}
\end{figure}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.98\textwidth]{figures/workflow.pdf}
    \caption{The workflow of our proposed static communication cost profiling method. }
    \label{fig:workflow}
\end{figure*}
The main idea of operator overloading-based autograd technology is to overload basic operators so that each basic operator contains a forward function and a derivative computation function. When model designers use one overloaded basic operator in the forward phase, the basic operator is recorded in an operator list for derivative computations. After the forward process is finished and the backward process starts, the derivative computation function of each overloaded operator is sequentially called from the end of the operator list to compute the derivative of outputs to the intermediate values and inputs. In this way, model designers can use overloaded basic operators to construct complex functions and automatically compute the gradients of the target function. For example, as is shown in Figure~\ref{fig:augograd}, in the forward phase, to compute the output $z$, model designers first obtain $v$ by multiplying $x$ and $y$. Then, model designers obtain $z$ by computing the logarithm of $v$. The above two operators are recorded in a list during the forward phase. In the backward phase, the derivative of $\frac{\partial z}{\partial v}$ is computed by calling the derivative computation function of the logarithm operator with $z$ and $v$ as inputs. Then, $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ are computed by calling the derivative computation function of the multiplication operator with $\frac{\partial z}{\partial v}$, $x$ and $y$ as inputs. 
 
