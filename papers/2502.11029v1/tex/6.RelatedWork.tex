\section{Related Work}~\label{sec:related}
\noindent\textbf{Design and optimization of MPC-friendly models.} Many studies~\cite{ganesan2022efficient, Peng_2023_ICCV, li2023mpcformer, liu2023llms, MPCViT, dhyani2023privit,10179422} try designing and optimizing MPC-friendly models by dynamically profiling model communication cost on one or two specific MPL frameworks~\cite{9444564}. Ganesan et al.~\cite{ganesan2022efficient} design an MPC-friendly convolution operator to optimize the efficiency of secure CNN model inference on \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2}. Subsequently, Peng et al.~\cite{Peng_2023_ICCV} propose AutoReP to automatically replace part of Relu functions of CNN models with polynomial functions to speed up secure CNN model inference on \texttt{CrypTen}~\cite{crypten2020}. In addition to CNN models, Liu et al.~\cite{liu2023llms} design an MPC-friendly transformer model by profiling the communication cost of secure transformer inference on \texttt{Cheetah}~\cite{Cheetah} and \texttt{CrypTFlow2}~\cite{rathee2020cryptflow2}. At the same time, Li et al.~\cite{li2023mpcformer} design an MPC-friendly variant of the BERT model, namely MPCFormer, by profiling the communication cost of secure BERT$_{\textsc{base}}$ model inference process on \texttt{CrypTen}~\cite{crypten2020}. For the image classification task based on the transformer model, Zeng et al.~\cite{MPCViT} propose an MPC-friendly vision transformer (ViT)~\cite{dosovitskiy2021an} model, namely MPCViT, by profiling the communication cost of secure ViT inference process on \texttt{SecretFlow-SPU}~\cite{secretflow}. In addition, Dhyani et al.~\cite{dhyani2023privit} design another MPC-friendly ViT model by profiling the communication cost of the secure ViT inference process on \texttt{DELPHI}~\cite{244032}. The above studies optimize the structure of MPC-friendly models by dynamically profiling model communication cost on one or two specific MPL frameworks. Therefore, their optimizations may only be effective in limited scenarios. With \hawkeye, model designers can statically profile model communication cost on multiple MPL frameworks without implementing specific models on these MPL frameworks without dynamic profiling, thus significantly improving the automation of the optimization of MPC-friendly models.

% \smallskip
\noindent\textbf{Cost models for hybrid protocol compilers.}  Several studies~\cite{costco, HyCC, OPA, CheapSMC,10179397} try modeling the cost of different MPC protocols to select an MPC protocol combination for efficient MPC. \texttt{CheapSMC}~\cite{CheapSMC} models the cost of an MPC protocol by running every basic operation of the MPC protocol to obtain the cost of each basic operation. Then, they estimate the cost of an input program by counting the number of operations required by the input program. \texttt{OPA}~\cite{OPA} and \texttt{HyCC}~\cite{HyCC} further improve \texttt{CheapSMC} by considering the circuit structure in their cost model. Concretely, in addition to testing the runtime of every single operation, they test circuits with different levels of parallelism, \wqruanother{i.e.,} computing multiple operations in one communication round. Beyond its previous studies, \texttt{CostCO}~\cite{costco} automatically generates and tests thousands of circuits with different structures to estimate the cost of MPC protocols. These studies aim to enhance the existing hybrid protocol compilers to find an efficient combination of MPC protocols for a given secure computation task. Unlike the above studies, \hawkeye aims to help model designers optimize the structures of MPC-friendly models for MPL frameworks, thus improving the efficiency of MPL from the machine learning perspective. Therefore, \hawkeye and existing studies on cost models for hybrid protocol compilers are orthogonal.


