\section{Methodology}
%This section provides a comprehensive description of our model merging approach. We begin by formalizing the problem and then detail our low-rank based merging method.
\subsection{Problem Setting}

We denotes $\mathcal{M}_i$ as the candidate models to be merged, where each $\mathcal{M}_i$ is parameterized by $\vtheta_i$. 
In this work, we focus on the homogeneous model merging \citep{wortsman_2022_model, ilharco_2022_editing, yadav_2024_ties}, suggesting that the base models share the same model architecture. Specifically, these models can be obtained from the training process, such as checkpoints, or fine-tuned from the same pre-trained model, referred to as task-specific models.
The primary objective of model merging is to construct a new model, $\mathcal{M}^*$, having better performance on the target single or multiple tasks.

\subsection{Implicit Low-Rank Estimation for Model Merging}
As demonstrated in \citet{yu_2024_language, yu2024extend}, model pairs would exhibit limited mergeability, especially when comprehensive fine-tuning or extended pre-training procedures are employed and result in substantial parameter shifts.
Under such conditions, existing task-vector based merging methods struggle to work due to the significant representational divergence between the base model and its fine-tuned derivative. To address this challenge, we propose an implicit low-rank estimation model merging method, named \textsc{LoRE-Merging}, which not only employs the robustness of low-rank estimation against the interference but also eliminates the need for access to the base model.
%In this section, we focus on scenarios such as checkpoint merging \citep{liu_2024_checkpoint, MoonshotAI}, where only the task models are available for merging. To address this challenge, we propose an implicit low-rank merging method to merge task-specific models effectively.

The core idea of \textsc{LoRE-Merging} is straightforward: instead of using the original base model, we first construct an approximate base model and subsequently integrate the task-specific vectors via a low-rank approximation technique.
Formally, denote the approximate base model as $\vtheta_0$ and the low-rank task vectors $\{ \vdelta_i \}_{i=1}^n$ where $n$ is the number of FT models, our objective is to minimize the discrepancy between each FT model and its corresponding integrated version derived from the constructed base model, expressed as $\vtheta_0 + \vdelta_i \approx \vtheta_i$.

%in the absence of the base model, we first construct an approximate base model and then use a low-rank approximation method to integrate task-specific information into this base model. The key lies in constructing the base model by deriving low-rank task vectors from the fine-tuned models. Following the principle of low-rank merging, we aim to identify a base model $\vtheta_0$ and low-rank task vectors $\{ \vdelta_i \}_{i=1}^n$ such that $\vtheta_0 + \vdelta_i \approx \vtheta_i$ for $i = 1, 2, \dots, n$.

To ensure the low-rank structure of $\vdelta$, we apply a nuclear norm penalty, as suggested in \citet{cai_2008_singular}. Then, we formulate the merging problem as the following optimization problem:

\begin{equation} \label{equ:2}
\min_{\vtheta_0, \vdelta_1, \dots, \vdelta_n} f := \sum_{i=1}^n \left( \| \vtheta_0 + \vdelta_i - \vtheta_i \|_F^2 + \mu \| \vdelta_i \|_*^2 \right),
\end{equation}

where $\| \cdot \|_*$ represents the nuclear norm, and $\mu > 0$ is a hyperparameter.
In Equation (\ref{equ:2}), the first term minimizes the difference between $\vtheta_0 + \vdelta_i$ and $\vtheta_i$, ensuring reconstruction accuracy. The second term acts as a penalty that encourages the task vectors $\vdelta_i$ to exhibit low-rank properties.

This problem is a standard multi-variable convex optimization problem. To solve it efficiently, we employ the coordinate descent method \citep{wright_2015_coordinate}.
Starting from an initial point $\{ \vtheta_0^0, \vdelta_1^0, \dots, \vdelta_n^0 \}$, each iteration (round $k+1$) updates the variables by iteratively solving the following single-variable minimization problem:
\begin{equation} \label{equ:3}
\begin{dcases}
\vtheta_0^{k+1} = \mathop{\arg\min}_{\vtheta} f(\vtheta, \vdelta_1^k ,\cdots, \vdelta_n^k) \\ 
\vdelta_i^{k+1} = \mathop{\arg\min}_{\vdelta} f(\cdots, \vdelta_{i-1}^k, \vdelta, \vdelta_{i+1}^k, \cdots), ~ \forall i
\end{dcases}
\end{equation}
%Once Problem (\ref{equ:2}) is solved, the models can be merged following the low-rank merging framework. 

The update for $\vtheta_0^*$ is trivial, while the update for $\vdelta$ is less straightforward due to the presence of the nuclear norm.
Fortunately, as shown in \citet{cai_2010_singular}, closed-form solutions for the coordinate descent method iterations in Problem (\ref{equ:2}) can be obtained using the Singular Value Thresholding (SVT) technique.
Recall that for a given matrix $\vdelta$ with the Singular Value Decomposition (SVD) $\vdelta = U \Sigma V^\top$, and a hyperparameter $\mu$, the SVT operator is defined as follows.
Let $\Sigma^+(\mu) := \text{diag}((\sigma_i - \mu)^+)$, where $(\cdot)^+$ denotes the positive part function. The SVT($\vdelta; \mu$) operator with hyperparameter $\mu$ is then defined as SVT($\vdelta; \mu$) := $U \Sigma^+ (\mu) V^\top$.
Using the SVT operator, the update for $\vdelta_i$ can be expressed as:
$\vdelta_i^{k+1} = $ SVT($\vtheta_i - \vtheta_0^{k+1}; \mu$).

Once the optimization problem is solved, we can obtain the approximate base model and a set of low-rank task vectors. Then, existing task-vector based approaches, such as Average Merging and Ties-Merging, can be applied to combine the task vectors and the base model. In this work, we directly adopt Average Merging as our post-calculation merging methods for simplicity, as as it demonstrated comparable performance to Ties-Merging in our preliminary experiments. The overall process is outlined in Algorithm \ref{algo:2}.

%Now we propose Algorithm \ref{algo:2} to merge the models.
%In Step 1: We solve Problem (\ref{equ:2}) using the closed-form solution of the coordinate descent method to obtain an approximation of the base model and the task vectors. In Step 2 and 3: We merge the task vectors into the base model.

\begin{algorithm}[t!]

\caption{Implicit low-rank merging method}
\label{algo:2}

Input: fine-tuned models $\{ \vtheta_i \}_{i=1}^n$, parameter dimension $d$, and hyperparameter $\lambda, \mu$.

Output: merged model $\vtheta^*$.

\begin{algorithmic}
\STATE $\rhd$ Step 1: Coordinate descent method to solve problem (\ref{equ:2}).
\STATE Set $\vdelta_i = 0$ for $i = 1, 2, \dots, n$.
\WHILE{iteration NOT converges}
\STATE $\vtheta_0 = \frac{1}{n} \sum_{i=1}^n (\vtheta_i - \vdelta_i)$
\FOR{$i=1, \dots, n$}
\STATE $\vdelta_i = \mathrm{SVT} (\vtheta_i - \vtheta_0; \mu)$;
\ENDFOR 
\ENDWHILE
\STATE
\STATE $\rhd$ Step 2 (Optional 1): Direct sum.
\STATE $\vtau = \sum_{i=1}^n \vdelta_i$.
\STATE
\STATE $\rhd$ Step 2 (Optional 2): TIES selection \citep{yadav_2024_ties}.
\STATE $\vgamma = sgn(\sum_{i=1}^n \vdelta_i)$.
\FOR{$p = 1, 2, \dots, d$}
\STATE $\mathcal{A}^p = \{i:  \vgamma_i^p = \vgamma^p \}$
\STATE $\vtau^p = \frac{1}{|\mathcal{A}^p|} \sum_{i \in \mathcal{A}^p} \vtau^p$
\ENDFOR
\STATE
\STATE $\rhd$ Step 3: Obtain merged checkpoint.
\STATE $\vtheta^* = \vtheta_0 + \lambda \vtau$.
\RETURN $\vtheta^*$
\end{algorithmic}

\end{algorithm}


% We evaluate the performance of Algorithm \ref{algo:2} for the checkpoint merging problem, as introduced in Section \ref{sec:exp}. It is important to note that the checkpoints hold equal status in model merging, with no single checkpoint being more important than the others. Therefore, when merging only the checkpoints, the implicit low-rank merging method must be utilized.
% In this evaluation, we merge two checkpoints using the implicit low-rank merging method and assess its accuracy on the MAMO-ComplexLP and MAMO-EasyLP datasets. The results are summarized in Table \ref{table:2}.

% \begin{table}[htbp]
% \begin{center}
% \begin{tabular}{c|c|c}
% \toprule
% {\textbf{Method}} & {\textbf{EasyLP (\%)}} & {\textbf{ComplexLP (\%)}}  \\
% \midrule
% low-rank & $83.4$ & $47.4$ \\
% implicit low-rank & $83.6$ & $46.5$ \\
% \bottomrule
% \end{tabular}
% \caption{The accuracies (higher is better) of the standard low-rank merging method and the implicit low-rank merging method.} 
% \label{table:2}
% \end{center}
% \end{table}

% As shown in Table \ref{table:2}, despite the implicit low-rank merging method lacking access to the base model, it achieves an accuracy level comparable to that of the standard low-rank merging method. This demonstrates the efficacy of the implicit approach.
