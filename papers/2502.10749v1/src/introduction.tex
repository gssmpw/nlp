\section{Introduction} \label{sec:intro}

Large Language Models (LLMs) have become ubiquitous in numerous real-world applications \citep{bommasani_2021_opportunities, zhuang_2020_comprehensive}. The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging \citep{garipov_2018_loss, wortsman_2022_model,yu_2024_language}, which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. 

%Existing research on model merging has predominantly concentrated on combining model weights using various weighting schemes. These schemes span from simple averaging techniques \citep{choshen_2022_fusing, ilharco_2022_patching, wortsman_2022_robust} to more advanced methods that incorporate parameter sampling based on importance \citep{ilharco_2022_editing, matena_2022_merging, yadav_2024_ties, yu2024extend}.
Building on recent studies \citep{ilharco_2022_editing, yadav_2024_ties, yu_2024_language}, task vector-based merging approaches have demonstrated significant effectiveness, where task vectors are defined as the parameter differences between fine-tuned models and the base LLM.
Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, \citet{yu_2024_language} applied random dropping with probability $p$ to obtain a sparse representation of task vectors, while \citet{yadav_2024_ties} retained only the top-$k$ elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference.

Nevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations \citep{yu_2024_language}. As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness \citep{yu2024extend}.
Second, empirical evidence from \citet{yadav_2024_ties} reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques. On the other hand, the sparsification process risks inadvertently eliminating essential task-specific features, thereby compromising the efficacy of the resultant merged model. These inherent constraints of sparse approximation methodologies underscore the necessity for developing alternative frameworks to estimate higher-fidelity low-rank task vector representations.

%Second, interference between task vectors can persist even when sparsified representations are employed. \citet{yadav_2024_ties} demonstrated that conflicting task vectors may still arise, even when sparse estimations are used. Furthermore, sparsification can inadvertently discard critical task-specific information, limiting the effectiveness of the merged model. These limitations of sparse estimation methods also motivate the exploration of alternative approaches for estimating more high-quality low-rank task vectors.

\begin{figure}[t!]
\centering
\includegraphics[width=0.23\textwidth]{imgs/1_0.png}
\includegraphics[width=0.23\textwidth]{imgs/1_1.png} \\
\includegraphics[width=0.23\textwidth]{imgs/1_2.png} 
\includegraphics[width=0.23\textwidth]{imgs/1_3.png} \\
\includegraphics[width=0.23\textwidth]{imgs/1_4.png} 
\includegraphics[width=0.23\textwidth]{imgs/1_5.png}
\caption{Singular value distributions for the task vector in layer $1$. We show the top-100 singular values, out of 4096 within the full rank.}
\label{fig:1}
\end{figure}

To this end, we first empirically validate that task vectors exhibit a small number of dominant singular values, with the remaining singular values being significantly smaller in magnitude, as shown in Figure \ref{fig:1}. Additionally, the dimension of the intersection of the images of two matrices is bounded by the minimum of their ranks. Therefore, we propose \textsc{LoRE-Merging}, a unified framework for model merging based on \textbf{Lo}w-\textbf{R}ank \textbf{E}stimation of task vectors, which eliminates the need for access to the base model.
Specifically, given a set of FT models, we formulate the merging problem as an optimization problem whose goal is to simultaneously identify an approximate base model integrated with a set of low-rank task vectors that collectively approximate the behavior of the FT models.
By leveraging low-rank estimations, task vectors become inherently less susceptible to interference, effectively addressing a fundamental challenge in model merging.
We conduct extensive experiments on optimization modeling problems and math word problems to confirm the effectiveness of our method.

%Some previous works \citep{hu_2021_lora,liu_2023_deja} have pointed out that the features of LLMs are generally low-rank. Moreover, our methodology is inspired by a key observation: in many fine-tuned models, task vectors exhibit a small number of dominant singular values, with the remaining singular values being significantly smaller in magnitude. This phenomenon is empirically validated, as illustrated in Figure \ref{fig:1}. Additionally, the dimension of the intersection of the images of two matrices is bounded by the minimum of their ranks. Consequently, low-rank estimations of task vectors are inherently less prone to interference, effectively addressing a fundamental challenge in model merging.

% Building on the concept of low-rank estimation, we develop a unified framework for model merging and rigorously evaluate its performance through extensive experimentation. Our contributions are summarized as follows:

% \begin{itemize}
% \item We propose an implicit low-rank merging method for model merging, which eliminates the need for an explicit base model.
% \item For the special case where an explicit base model is available, we adapt the implicit method and introduce an explicit low-rank merging method to address this scenario.
% \item We conduct comprehensive empirical experiments, demonstrating the efficiency and efficacy of the proposed framework across a variety of tasks and settings.
% \end{itemize}

