\section{Low-Rank Based Merging}

In previous works, we discussed methods for merging LLMs without access to the base model.
In this section, we address a special case where the base model is available and can be utilized in the model merging process.

Before detailing our algorithm, we first describe how to obtain a low-rank approximation $A_r$ of rank $r$ for a given matrix $A \in \mathbb{R}^{m \times n}$. Formally, $A_r$ is the solution to the following optimization problem:

\begin{equation} \label{equ:1}
\textrm{low-rank}(A;r) = \mathop{\arg\min}_{B: \text{rank}(B) \leq r} \| B - A \|_F^2,
\end{equation}

where $\| \cdot \|_F$ denotes the Frobenius norm. A closed-form solution to Equation \ref{equ:1} exists \citep{householder_2013_theory}. Specifically, let $U \Sigma V^\top$ be the singular value decomposition (SVD) of $A$, where $\Sigma$ is a diagonal matrix containing the singular values $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_{\min (m, n)}$. Let $\Sigma_r$ be the matrix obtained by retaining the top $r$ singular values in $\Sigma$ and setting the rest to zero. Then, the low-rank approximation is given by $A_r = U \Sigma_r V^\top$.

Building upon prior work, we present our low-rank merging method in Algorithm \ref{algo:1}. 

\begin{algorithm}[ht]

\caption{Low-Rank Merging}
\label{algo:1}

Input: fine-tuned models $\{ \vtheta_i \}_{i=1}^n$, base model $\vtheta_0$, rank $r$, parameter dimension $d$, and hyperparameter $\lambda$.
Output: merged model $\vtheta^*$.

\begin{algorithmic}
\STATE $\rhd$ Step 1: Create low-rank task vectors.
\FOR{$i = 1, 2, \dots, n$}
\STATE $\vdelta_i = \vtheta_i - \vtheta_0$.
\STATE $\tilde{\vdelta}_i$ = low-rank($\vdelta_i$; rank=$r$).
\STATE $\vgamma_i = sgn(\tilde{\vdelta}_i)$.
% \STATE $\vmu_i = |\tilde{\vdelta}_i|$
\ENDFOR
\STATE
\STATE $\rhd$ Step 2 (Optional 1): Direct sum.
\STATE $\vtau = \sum_{i=1}^n \tilde{\vdelta}_i$.
\STATE
\STATE $\rhd$ Step 2 (Optional 2): TIES selection \citep{yadav_2024_ties}.
\STATE $\vgamma = sgn(\sum_{i=1}^n \tilde{\vdelta}_i)$.
\FOR{$p = 1, 2, \dots, d$}
\STATE $\mathcal{A}^p = \{i:  \vgamma_i^p = \vgamma^p \}$
\STATE $\vtau^p = \frac{1}{|\mathcal{A}^p|} \sum_{i \in \mathcal{A}^p} \vtau^p$
\ENDFOR
\STATE
\STATE $\rhd$ Step 3: Obtain merged checkpoint.
\STATE $\vtheta^* = \vtheta_0 + \lambda \vtau$.
\RETURN $\vtheta^*$
\end{algorithmic}

\end{algorithm}

We provide a brief explanation of Algorithm \ref{algo:1}.
Compared with other merging methods, such as TIES and DARE, our approach differs in that we utilize the low-rank estimation of the task vectors to capture the importance coefficients. These coefficients are then used to identify the most critical components of the task vectors.
Subsequently, we add the important components back to the original model. This step can be performed using either simple summation, the TIES summation method, or any other summing methods to obtain the final merged model.

\section{Task Vector Rank Validation}

In this subsection, we validate the low-rank properties underlying the low-rank assumption. Specifically, we focus on the checkpoint merging problem and compute the rank of the task vectors. As previously discussed, we set the rank $r$ as $r = 0.2 \times \min \{ m, n \}$ for any given task vector $\vdelta$.

The distribution of the largest 100 singular values for Layer $1$ is presented in Figure \ref{fig:1}. Our experimental results reveal that $\sigma_r \leq 0.05 \times \sigma_1$, indicating that the singular values set to $0$ in low-rank estimation are significantly smaller than the largest singular value across all linear layers. This finding supports the validity of adopting a low-rank approximation for task vectors, as it reflects the inherent structure of the data.
