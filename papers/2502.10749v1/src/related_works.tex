\section{Related Works}

% {\textbf{SFT and landscape between models}}.
% Supervised Fine-tuning (SFT) of LLMs aims to endow pre-trained models with task-specific abilities by optimizing them on task-specific datasets. This approach has become the standard paradigm in NLP. Generally, SFT can be categorized into two types: full fine-tuning \citep{devlin_2018_bert, houlsby_2019_parameter} and parameter-efficient fine-tuning \citep{lester_2021_power, hu_2021_lora}.
% The effects of SFT are captured in task vectors, which are defined as the difference between the parameters of LLMs before and after fine-tuning. These task vectors provide insights into how fine-tuning influences model parameters and are central to methods like model merging.
% Model merging builds on the phenomenon of mode-connectivity, which suggests that the parameter values of independently trained LLMs can be interpolated without increasing the test loss, as demonstrated in \citep{frankle_2018_lottery, garipov_2018_loss, simsek_2021_geometry}. However, \citet{neyshabur_2020_being} showed that naively interpolating two LLMs with completely disjoint optimization trajectories can result in a catastrophic drop in accuracy.
% To address this, several studies have proposed techniques for better aligning the parameters of models trained from different initializations. For instance, \citet{ainsworth_2022_git}, \citet{singh_2020_model}, and \citet{wang_2020_federated} introduced methods to find permutations that better align independently trained LLMs, enabling interpolation without increasing the test loss.

% {\textbf{Model merging methods}}.
%With the rapidly increasing number of task-specific LLM models, model merging techniques have been widely applied to combine different models into a single model to achieve superior performance. A comprehensive survey of model merging methods can be found in \citep{yu_2024_language}.
%In particular, task-specific models initialized from the same pre-trained model can often be merged without accounting for permutation symmetry \citep{ilharco_2022_editing, wortsman_2022_model, wortsman_2022_robust}.
Merging fine-tuned models has been shown to offer several benefits, such as improving performance on a single target task \citep{gupta_2020_stochastic, choshen_2022_fusing, wortsman_2022_model}, enhancing out-of-domain generalization \citep{cha_2021_swad, arpit_2022_ensemble, ilharco_2022_editing, rame_2023_model}, creating multi-task models from different tasks \citep{jin_2022_dataless, li_2022_branch, yadav_2024_ties}, supporting continual learning \citep{yadav_2022_exclusive, yadav_2023_exploring}, and addressing other challenges \citep{don_2022_cold, li_2022_branch}.
%To go beyond traditional averaging methods for model merging, numerous advanced techniques have been proposed. For example, \citet{jin_2022_dataless} introduced RegMean, a data-less merging method that merges models in parameter space by solving a linear system constructed from data and model parameters. \citet{matena_2022_merging} suggested a probability-space approach, which uses the Fisher information matrix to identify the importance of model parameters and proposed Fisher Merging. However, both RegMean and Fisher Merging suffer from high computational costs and dependence on activation information from the data.
Among these methods, task-vector-based merging approaches play an important role. Task Arithmetic \citep{ilharco_2022_editing} first introduced the concept of task vectors and shows that simple arithmetic operations can be performed to obtain the merged models. Building on this idea, methods like DARE \citep{yu_2024_language} and Ties \citep{yadav_2024_ties} adopt pruning-then-scaling techniques to merge task vectors, based on the assumption that not all parameters equally contribute to the final performance. However, these methods based on sparsity estimation consistently suffer from the interference among task vectors and require access to the base model, thus limiting their overall effectiveness.

%\citet{yu_2024_language} proposed DARE, which applies random dropping and scaled addition to merge models. Furthermore, the DARE method can be easily integrated with other merging techniques. Task Arithmetic \citep{ilharco_2022_editing} introduced a method for merging models by generating task vectors and performing arithmetic operations to obtain the merged model. In addition, \citet{yadav_2024_ties} proposed TIES Merging, which calculates the importance of task vectors and selectively averages them to improve task merging.
%Beyond these works, other researchers have explored model merging in more specialized contexts. For instance, \citet{shen_2024_efficient} and \citet{he_2023_merging} studied model merging in the context of Mixture-of-Experts (MoE) models, addressing unique challenges associated with merging these architectures.
