\section{Experiments} \label{sec:exp}

\begin{table*}[htbp]
\fontsize{8.5}{9} \selectfont
\begin{center}
\begin{tabular}{c | c c| c c |  c c c | c}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{Deepseek \& NuminaMath}} & \multicolumn{2}{c|}{\textbf{WizardLM \& WizardMath}} & \multicolumn{3}{c|}{\textbf{Checkpoints}} & \multirow{2}{*}{\textbf{Avg.}} \\
 \cline{2-3} \cline{4-5} \cline{6-8}
 & {GSM8K} & {MATH} & {GSM8K} & {MATH} & {EasyLP} & {ComplexLP} & {NL4OPT}  \\
\hline
Baseline & 76.3 & \textbf{55.8} & 54.8 & 12.4 & 81.9 & 39.3 & 94.0 & 59.21 \\ 
Average & 75.0 & 45.8 & 58.8 & 12.6 & 75.9 & 40.3 & 91.6 & 57.14  \\ 
DARE & 81.0 &  54.2 & 14.9 & 3.7 & 80.7 & 35.1 & $\textbf{95.1}$ & 52.10 \\ 
Ties-Merging & 80.8 & 51.6 & 58.5 & 11.8 & 82.4 & 42.7 & 94.8 & 60.37\\
\hdashline
{\textsc{LoRE-Merging}} & $\textbf{81.0}$ & 52.7 & $\textbf{60.3}$ & $\textbf{13.0}$ & $\textbf{83.4}$ & $\textbf{47.4}$ & 94.8 & \textbf{61.80} \\ 
\bottomrule
\end{tabular}
\caption{The evaluation results on math word problems and optimization modeling problems. We use the best performance of base models as the baseline.}
\label{table:1}
\end{center}
\vspace{-3mm}
\end{table*}

\paragraph{Baselines \& Settings}
We compare \textsc{LoRE-Merging} with following popular merging methods.
{\textbf{Average Merging}} \citep{choshen_2022_fusing}: This method computes the element-wise mean of all the individual models.
{\textbf{DARE}} \citep{yu_2024_language}: This approach randomly drops task-specific vectors and rescales the remaining vectors back to the base model. We set the hyperparameter for the random probability to $0.5$.
{\textbf{Ties-Merging}} \citep{yadav_2024_ties}: In this method, task-specific vectors are randomly dropped, and only the parameters aligned with the final agreed-upon sign are merged. For Ties-merging, we set the top-$k$ value to $20 \%$, and the hyperparameter $\lambda$ is fixed at $1$.
For \textsc{LoRE-Merging}, the rank $r$ is determined dynamically. 
For a given task vector $\vdelta \in \mathbf{R}^{m \times n}$, we set the rank $r = 0.2 \times \min \{ m, n \}$ to get a low-rank estimation.
%Empirical experiments validate the efficacy of this setting.

\paragraph{Evaluation}
We first evaluate \textsc{LoRE-Merging} on math word problems using the popular benchmarks GSM8K \citep{cobbe_2021_training} and MATH \citep{hendrycks2measuring}.
For comprehensive evaluation, we test both DeepSeek-series models (NuminaMath-7B \citep{beeching_2024_numinamath} and DeepSeek-Math-7B-Base \citep{shao_2024_deepseekmath}) and LLaMA-series models (WizardLM-13B \citep{xu2023wizardlm} and WizardMath-13B \citep{luo2023wizardmath}).
Additionally, we also evaluate the effectiveness of \textsc{LoRE-Merging} on another advanced task, i.e. mathematical optimization modeling problems \citep{ramamonjison_2023_nl4opt, huang_2024_mamo, huang_2025_orlm}. This task aims to generate solvable mathematical models given an optimization problem in natural language. As the lack of public models on this task, we first fine-tune Qwen-2.5-Coder-7B-Instruct model \citep{hui_2024_qwen2} with the datasets provided by \citet{huang_2025_orlm} and merge the checkpoints in the training process. The evaluations are conducted on MAMO dataset \citep{huang_2024_mamo} which includes two subsets EasyLP and ComplexLP, and NL4OPT dataset \citep{ramamonjison_2023_nl4opt}.

\paragraph{Main Results}
As shown in Table \ref{table:1}, \textsc{LoRE-Merging} achieves superior performance across most metrics, as well as the highest overall score. For the math word problem evaluation, our method demonstrates consistently superior performance compared to the baselines, except for the evaluation on the MATH dataset when merging DeepSeek-Math with NuminaMath. due to the significant performance gap between the base models, where DeepSeek-Math achieves only a score of 36.2 on the MATH dataset, while NuminaMath reaches 55.8. As indicated in \citet{yao2024determine}, a large performance gap can significantly impact the effectiveness of model merging. Another worthy-noting observation is that DARE demonstrates significantly poorer performance when merging WizardLM and WizardMath. This can likely be attributed to the substantial parameter divergence between these models, which results in the failure of calculating the task vector derived from the base model.
In contrast, our \textsc{LoRE-Merging} with the approximate base model and low-rank task vectors demonstrates superior robustness and effectiveness in solving math word problems.
For the evaluations on optimization modeling with checkpoints merging, we can see existing task-vector based merging methods consistently improve the performance because of the marginal gap between the checkpoints. Therefore, we believe that checkpoint merging can serve as a highly effective technique complementary to training methods, particularly our \textsc{LoRE-Merging} method. We also conduct a detailed analysis how our method enhance the modeling capacity on ComplexLP dataset. We found that the earlier checkpoint is more good at identifying the variables and parameters in the questions while the later one focuses on more complex components, such as formulating variables and the constraints. With the merging of task vectors, the merged model exhibits superior overall performance on the task.

%In this section, we conduct an additional experiment on math problem, focusing on merging Supervised Fine-Tuning (SFT) models of base LLMs. The experiments are performed on two different LLM structures: DeepSeek-Math and Wizard.

%First Experiment:
%We merge the NuminaMath-7B \citep{beeching_2024_numinamath} and DeepSeek-Math-7B-Base \citep{shao_2024_deepseekmath} models, and evaluate the merged models on the GSM8K dataset \citep{cobbe_2021_training} and MATH dataset \citep{saxton_2019_analysing}.
%Second Experiment:
%We merge the WizardLM-13B and WizardMath-13B models, and then evaluate the merged models.
%The test accuracies of the original DeepSeek-Math-7B-Base and WizardMath-13B models on the GSM8K and MATH dataset are used as baselines for comparison. The results are presented in Table \ref{table:1}.

%As shown in Table \ref{table:1}, all the listed merging methods achieve higher accuracy compared to the original models. Notably, the low-rank merging method demonstrates superior performance in this experiment, further validating its efficacy.

%\paragraph{Optimization Modeling Evaluation}
%We conduct checkpoint merging, which merges the checkpoints obtained during the SFT (Supervised Fine-Tuning) process \citep{liu_2024_checkpoint} for the task of Natural Language for Optimization \citep{ramamonjison_2023_nl4opt}. This task aims to investigate methods for extracting the meaning and formulation of an optimization problem from its textual description using large language models (LLMs) \citep{huang_2025_orlm}.
%For this study, we use the Qwen-2.5-Coder-7B-Instruct model \citep{hui_2024_qwen2} as the base model. The base model is fine-tuned on the dataset, resulting in the generation of two checkpoints. The goal of checkpoint merging is to combine these two checkpoints with the base model to enhance its performance on the validation dataset.
%We evaluate the effectiveness of the fine-tuned model using the MAMO dataset \citep{huang_2024_mamo}, which consists of two distinct types of linear programming problems: MAMO-EasyLP and MAMO-ComplexLP and the NL4OPT dataset \citep{ramamonjison_2023_nl4opt}. The baseline for comparison is the accuracy achieved by the fine-tuned model without merging. The performance of various merging methods is reported in Table \ref{table:1}.

%As shown in Table \ref{table:1}, the introduction of merging methods to integrate checkpoints into the base model results in higher accuracies on the validation datasets. Notably, our proposed method, low-rank merging, achieves the highest accuracies on both the MAMO and ML4OPT datasets, demonstrating the effectiveness of our approach.
