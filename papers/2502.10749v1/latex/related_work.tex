\section{Related Work}
\label{rel_work}

\textbf{Emergent Large Features of LLMs} 
Emergent large magnitude and massive activation features have been observed in Transformer-based large language models \citep{kovaleva2021bert, puccetti2022outliers, wei2022outlier, dettmers2022gpt3, sun2024massive}. 
% Emergent large magnitudes is a subset of hidden state features had magnitudes approximately 100 times larger than the majority of features \citep{\citep{dettmers2022gpt3}}. Similarly, massive activation refers to a small number of activations that had values notably larger, up to 100,000 times than the remaining activations \citep{sun2024massive}. 
The occurrence of these hidden state features and input activations with large magnitudes is relatively rare, indicating the outlier patterns within model internal representations. However, these outlier features are shown to have essential importance in representing information, as zeroing out these outlier features during inference leads to a significant degradation in model performance \citep{dettmers2022gpt3, sun2024massive}. 
Recent development of quantization schemes \citep{lin2023awq, dettmers2023spqr, xiao2023smoothquant} and model pruning methods \citep{sun2023simple, zhangplug} for LLMs have been influenced by the presence of these outlier features. Our research expands on this insight by demonstrating that the relationship between these weights and input activation outlier features should also act as key indicators for selecting which weights to prune in LLMs.

\textbf{Post-Training Pruning}  
Post training pruning (PTP) has emerged as a popular technique for reducing the size and computational complexity of models without the need for extensive retraining \citep{hubara2021accelerated, kwon2022fast, frantar2023sparsegpt}. 
% These methods focus on preserving performance during the pruning process, assuming access to a small amount of data, also referred to as calibration data. 
% allows for an efficient and simplified pruning process when scaling to LLMs with billions of parameters, as it eliminates the need for multiple iterations of pruning and any retraining or fine-tuning. 
% Pruning methods in the pre-LLM era focus on employing hessian matrix for weight pruning \citep{lecun1989optimal, hassibi1993optimal}. However, calculating hessian matrix inverse incurs substantial computational for LLMs. 
Recent PTP methods for LLMs aim to evaluate the importance of weights using specific pruning metrics and remove less important weights by setting them to zero.
Magnitude pruning \citep{han2015learning} directly removes weights based on their magnitude, offering simplicity but often resulting in unsatisfied performance for LLMs. To improve accuracy, SparseGPT \citep{frantar2023sparsegpt} solves layer-wise reconstruction problem, which significantly boosts performance but adds computational costs due to weight updates. 
Wanda \citep{sun2023simple} simplifies SparseGPT by considering only the product of weight magnitude and the norm of input activations. Building on Wanda, RIA \citep{zhangplug} introduces a relative importance coefficient to enhance weight importance evaluation. These one-shot pruning metrics now stand out as strong baselines for LLM pruning.