\section{In-depth Analysis}
\label{sec:analysis}
% In this section, we take an in-depth analysis towards the effectiveness of OptiShear.

\subsection{Pruning Metrics Generalizability}
% A potential limitation of \textsc{OptiShear} might be the relatively high cost of the search process, which is conventionally necessitated for every model across all datasets. To alleviate this problem, we explore the generalizability of our searched pruning metrics. We also outline the search costs for finding optimal metrics and layerwise sparsity ratios for LLaMA-1/2/3 and Mistral models in Appendix \ref{sec:appendix_cost}. 
One potential limitation of \textsc{OptiShear} is the computational cost of searching for optimal metrics and ratios across different models and datasets. We address this by investigating the transferability of our searched pruning metrics. 
% The detailed computational costs for optimizing metrics and layerwise sparsity ratios across LLaMA-1/2/3 and Mistral models are documented in Appendix \ref{sec:appendix_cost}.
\vspace{-2mm}
\paragraph{Cross-task Generalization.}
%First, we evaluate the cross-task generalization of the pruning metrics, i.e. applying the metrics searched on the original task to another. Here we attempt to
% We evaluate the generalizability of the pruning metrics identified from the complex arithmetic reasoning task (e.g. GSM8K) to the easier tasks, such as language modeling (e.g. WikiText) and zero-shot reasoning (e.g. LM Harness).
% This evaluation is partially inspired by the work \citep{fu2022complexity} of multi-step reasoning which finds that complex demonstrations provide more valuable information.
% For instance, on the evaluation of LLaMA-1 7B on WikiText, we directly leverage the metrics searched with LLaMA-1 7B on GSM8K.
% As shown in Table \ref{tab:transfer_gsm8k_1}, GSM8K metric, derived from the arithmetic reasoning task, consistently achieves the better performance than SparseGPT and comparable scores against the task-specific pruning metrics across LLaMA-1/2/3 and Mistral models on the WikiText and LM Harness benchmarks. We also conduct the counter experiment -- performing the metrics derived from WikiText on the harder tasks, like LM Harness and GSM8K. We regrettably observe consistent performance declines on the target tasks. These findings suggest that the generalization works from the complex tasks to those easier.
We investigate the transferability of pruning metrics from complex tasks (GSM8K arithmetic reasoning) to simpler ones (WikiText language modeling and LM Harness zero-shot reasoning), inspired by research showing complex demonstrations provide richer information \citep{fu2022complexity}. As shown in Table \ref{tab:transfer_gsm8k_1}, metrics derived from GSM8K outperform SparseGPT and match task-specific metrics across LLaMA-1/2/3 and Mistral models on simpler tasks. 
% However, the reverse transfer—applying WikiText-derived metrics to more complex tasks like GSM8K—leads to performance degradation, indicating that metric transfer is effective only from complex to simpler tasks.

\vspace{-2mm}
\paragraph{Cross-model Generalization.}

% For cross-model evaluation, we select models notable for their superior performance on arithmetic reasoning, specifically the LLaMA-2 7B and LLaMA-3 8B models. Metrics derived from these models, termed the LLaMA-2 and LLaMA-3 metrics, are applied across different model families to assess their effectiveness. 
% The LLaMA-3 metric is tested on Mistral models, while the LLaMA-2 metric is evaluated with the LLaMA-1 models. We also tried the cross-model evaluations of transferring the metrics of LLaMA-3 to LLaMA-1/2 models. But we found the results are insignificant and we think the reason is that their weight distributions largely differ.
% As shown in Table \ref{tab:transfer_gsm8k_1}, we find that the metrics from the superior model can consistently surpass the established baselines across the board.
% More excitingly, it even exceeds the original \textsc{OptiShear}.
% Further details of the optimal metrics found for each LLM, using both C4 and GSM8K calibration data, are provided in Appendix \ref{sec:appendix_coe}.

We examine cross-model transferability using metrics derived from high-performing arithmetic reasoning models (LLaMA-2 7B and LLaMA-3 8B). The LLaMA-3 metric is applied to Mistral models, while the LLaMA-2 metric is tested on LLaMA-1 models. Attempts to transfer LLaMA-3 metrics to LLaMA-1/2 models proved ineffective, likely due to substantial differences in weight distributions. Table \ref{tab:transfer_gsm8k_1} shows metrics from superior models not only outperform baselines but also surpass the original \textsc{OptiShear}. Therefore, although we still claim the necessity of adaptive pruning for different models, we also provide a cost-effective alternative to mitigate the search process, which is adopting the pruning metric identified on the challenging task with the strongest model in your candidate pool.
This metric has demonstrated a capacity for generalization, proving transferrable and reusable across less complex tasks or the less-performing models.

% \begin{figure*}[]
%     \centering
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/sparsity.png}
%         \label{fig:sparsity}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/nsamples.png}
%         \label{fig:nsample}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/cot_steps.png}
%         \label{fig:cot_steps}
%     \end{subfigure}
%     \vspace{-2em}
%     \caption{Sensitivity evaluation on sparsity, number of calibration samples (samples), and the reasoning steps in calibration samples for arithmetic reasoning. }
%     \label{fig:sensi_eval}
%     \vspace{-1em}
% \end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/samples.pdf}
    \vspace{-1em}
    \caption{Sensitivity evaluation on sparsity, number of calibration samples (samples), and the reasoning steps in calibration samples for arithmetic reasoning. }
    \label{fig:sensi_eval}
    \vspace{-1em}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.92\linewidth]{figures/distance.pdf}
    \vspace{-1em}
    \caption{Layerwise absolute distance between transformed weights and transformed activations for Wanda, RIA, and OptiShear metrics on LLaMA-2 13B models.}
    \label{fig:w_x}
    \vspace{-1em}
\end{figure*}



\subsection{Sparsity \& Calibration Samples}
% In Figure \ref{fig:sensi_eval}(a), we investigate the impact of different sparsity ratios, which range from 0.1 to 0.6, on the performance of the LLaMA-2 13B model. 
% The perplexity curve demonstrates that \textsc{OptiShear} (red curve) consistently surpasses SparseGPT, Wanda, and RIA across all tested sparsity levels.
% Especially at the sparsity ratio of 60\%, \textsc{OptiShear} outperforms the baselines by a large margin, achieving a 10.52\% relative improvement compared to RIA.

% In Figure \ref{fig:sensi_eval}(a), we examine how different sparsity ratios (0.1 to 0.6) affect LLaMA-2 13B model performance. The perplexity results show that \textsc{OptiShear} (red curve) consistently outperforms SparseGPT, Wanda, and RIA across all sparsity levels. The advantage is particularly pronounced at 60\% sparsity, where \textsc{OptiShear} achieves a 10.52\% relative improvement over RIA.
% Using LLaMA2-7B, we first analyze the impact of calibration sample size on Wikitext evaluation. Figure \ref{fig:sensi_eval}(b) shows distinct performance patterns as samples vary from 2 to 128. While SparseGPT requires more calibration samples, Wanda, RIA, and \textsc{OptiShear} maintain robust performance even with limited samples. \textsc{OptiShear} achieves superior results across all sample sizes.
% We then evaluate arithmetic reasoning performance using calibration samples with varying complexity (2-10 reasoning steps). Figure \ref{fig:sensi_eval}(c) reveals that increasing reasoning steps in calibration samples improves accuracy, with \textsc{OptiShear} consistently outperforming baselines. Additional analyses of search algorithms, robustness, and search space exploration are detailed in Appendices \ref{sec:robustness} and \ref{sec:space}.

In Figure \ref{fig:sensi_eval}(a), we evaluate LLaMA-2 13B under different sparsity ratios (0.1-0.6). \textsc{OptiShear} consistently outperforms all baselines, achieving a 10.52\% relative improvement over RIA at 60% sparsity.
Figure \ref{fig:sensi_eval}(b) examines calibration sample size impact (2-128) on LLaMA2-7B. While SparseGPT's performance varies with sample size, \textsc{OptiShear} maintains superior results even with limited samples.
For arithmetic reasoning (Figure \ref{fig:sensi_eval}(c)), increasing reasoning steps in calibration samples improves accuracy across all methods, with \textsc{OptiShear} showing consistent advantages. Further analyses of search algorithms, robustness, and search space are provided in Appendices \ref{sec:robustness} and \ref{sec:space}.


\subsection{Insights from Optimal Pruning Metrics}

% In our analysis of the optimal searched pruning metrics, We find that the differences between transformed weights and transformed activations may affect the effectiveness of different pruning metrics. 
% Figure \ref{fig:w_x} shows that the RIA pruning metric reduces the absolute difference compared to Wanda, while the OptiShear searched metric further minimizes this difference, bringing it close to zero. 
% The weighted transformation operation in the OptiShear pruning metric effectively scales both weights and activations into a similar numerical range, facilitating a balanced evaluation of each weight relative to its corresponding activation.
% Coupled with the performance results of each pruning metric presented in Table \ref{tab:all_res},  suggests that pruning metrics with smaller absolute differences between transformed weights and activations are more likely to achieve effective pruning. 
% We provide more detailed analysis in Appendix \ref{sec:w_x}.

Our analysis reveals that the effectiveness of pruning metrics is influenced by the gap between transformed weights and activations. Figure \ref{fig:w_x} demonstrates that while RIA reduces this gap compared to Wanda, \textsc{OptiShear}'s searched metric nearly eliminates it through weighted transformation that normalizes both components into a comparable range.
The superior performance shown in Table \ref{tab:all_res} suggests that minimizing this transformation gap leads to more effective pruning. Detailed analysis on other LLMs is provided in Appendix \ref{sec:w_x}.