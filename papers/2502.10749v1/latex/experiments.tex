\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

\paragraph{Models and Evaluations.}
We adopt four prominent open-sourcing LLMs as our foundation model, including LLaMA-1 \citep{touvron2023llama} and LLaMA-2 \citep{touvron2023llama2} with sizes ranging from 7B to 70B, LLaMA-3 8B \citep{llama3} and Mistral 7B \citep{jiang2023mistral} with the base models and their instruction-tuned variants.
Following previous works \citep{sun2023simple,xia2023sheared}, we first evaluate on seven tasks from the EleutherAI LM Harness \citep{eval-harness}\footnote{Referred as \textit{LM Harness} in remaining parts.}, and the language modeling task based on the held-out WikiText \citep{merity2016pointer} validation set. Furthermore, we also evaluate two more challenging tasks, arithmetic reasoning on GSM8K \citep{cobbe2021training} and the language understanding benchmark MMLU \citep{hendrycks2020measuring}.
For the comparison group settings, 
we follow Wanda \citep{sun2023simple} to compare and remove weights on a per-output basis, where weight importance scores are compared locally within each output neuron. 
We evaluate three sparsity types as defined in previous research \citep{sun2023simple, zhangplug}: unstructured sparsity, semi-structured 4:8 and 2:4 sparsity.
We conduct each search process with 350 trials. On a single NVIDIA RTX A6000 GPU, each trial takes less than 16 seconds for 8B models, making the search process computationally efficient. Detailed ablation studies on the number of trials and comprehensive search time analyses are provided in Appendix \ref{appendix:hyper} and Appendix \ref{sec:appendix_cost}, respectively.


\vspace{-5mm}
\paragraph{Baselines.}
We compare \textsc{OptiShear} with five existing baselines:
Magnitude pruning \citep{han2015learning} which discards weights based on their magnitudes. SparseGPT \citep{frantar2023sparsegpt} solves the layer-wise reconstruction problem to identify redundant weights and prune them accordingly. Wanda \citep{sun2023simple} utilizes large-magnitude features and input activation to induce sparsity. RIA \citep{zhangplug} further improves Wanda pruning by introducing the relative importance and channel permutation. Pruner-Zero \citep{dongpruner} automatically searches for the optimal pruning metric based on weights and gradients, using perplexity as the evaluation measure.


\begin{table*}[t!]
\resizebox{\textwidth}{!}{%
\fontsize{9}{11} \selectfont
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{2}{*}{Method} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Weight\\ Update\end{tabular}} &
  \multirow{2}{*}{Sparsity} &
  \multicolumn{2}{c}{LLaMA-1} & &
  \multicolumn{2}{c}{LLaMA-2} & &
  \multicolumn{2}{c}{LLaMA-3} & &
  \multicolumn{2}{c}{Mistral} \\
\cline{4-5} \cline{7-8} \cline{10-11} \cline{13-14}
  &  &  & 7B & 13B & & 7B & 13B & & 8B & 8B-Inst & & 7B & 7B-Inst \\ \hline
\rowcolor{gray!15}
\multicolumn{14}{c}{WiKiText Perplexity} \\
Magnitude & \ding{55} & 4:8 & 17.48 & 16.80  & & 16.10 & 7.23 & & 2.5E2  &  5.6E2   &  & 8.78  &  8.67   \\
SparseGPT &  $\checkmark$  & 4:8 & 8.16  & 7.05   & & 7.89  &  6.54   &  & \textbf{15.57}  &  \textbf{16.62}   &  & 7.71  &   8.15  \\
Wanda & \ding{55} & 4:8 & 8.19  & 6.95 & & 8.01  & 6.60 & &  16.82  &  21.52   &  & 8.95  & 8.42   \\
RIA & \ding{55} & 4:8 & 8.18  & 6.97 & & 8.04  & 6.62 &  & 17.28  &  21.15   & &  8.91  &  8.51   \\
\hdashline
\textsc{OptiShear} & \ding{55} & 4:8 & \textbf{7.93}  & \textbf{6.65} & & \textbf{7.72}  & \textbf{6.34} & &  17.24  &  21.15    &  & \textbf{7.57}  & \textbf{7.66}   \\
\hline
Magnitude & \ding{55} & 2:4 & 49.06 & 19.33  & & 38.50 & 9.04 & &  5.3E3  &  5.3E3  &   &  13.18  &  11.83  \\
SparseGPT &  $\checkmark$  & 2:4 & 10.58 & 8.53 & & 10.38 & 8.26  && \textbf{23.43}  &  \textbf{26.68}  &  & 10.17  &  9.84    \\
Wanda & \ding{55} & 2:4 & 11.04 & 9.06 & & 11.31 & 8.46 & &  31.89  &   59.12  &  &  13.54   &  11.08  \\
RIA & \ding{55} & 2:4 & 11.10 & 9.24 & & 11.40 & 8.57 & &  31.79  &   38.00 &   &  13.61  &  11.21  \\
\hdashline
\textsc{OptiShear} & \ding{55} & 2:4 & \textbf{10.54} & \textbf{8.21}  &  & \textbf{10.34} & \textbf{7.97} & &  31.71  &  37.98  &  &  \textbf{10.13}  & \textbf{9.23}   \\
%GSM8K Metric & \ding{55} & 4:8 & \textbf{7.68}  & \textbf{6.59} & & \textbf{7.68}  & \textbf{6.28}  &  & \textbf{14.81}  &  21.33  &   &  7.59  & 7.73   \\
\hline
\rowcolor{gray!15}
\multicolumn{14}{c}{GSM8K} \\
Magnitude & \ding{55} & 4:8 & 1.53 & 3.48  & & 1.59 & 4.70 & &  4.16  &  7.81  &   &  9.60  &  14.15  \\
SparseGPT &  $\checkmark$  & 4:8 & 3.54 & 8.78 & & 4.84 & 8.20  && \textbf{9.23}  &  \textbf{18.35}  &  & 21.46  &  29.82    \\
Wanda & \ding{55} & 4:8 & 2.65 & 7.40 & & 3.10 & 8.13 & &  6.60  &   10.84  &  &  12.87   &  20.92  \\
RIA & \ding{55} & 4:8 & 3.17 & 8.74 & & 2.93 & 7.75 & &  8.12  &   17.59 &   &  17.36  &  27.18  \\
\hdashline
\textsc{OptiShear} & \ding{55} & 4:8 & \textbf{3.71}  & \textbf{9.29} & & \textbf{4.95}  & \textbf{8.53} & &  8.38  &  17.59    &  & \textbf{21.80}  & \textbf{30.39}   \\
\hline
Magnitude & \ding{55} & 2:4 & 0.74 & 2.29  & & 0.98 & 3.60 & &  0.24  &  3.12  &   &  3.80  &  9.26  \\
SparseGPT &  $\checkmark$  & 2:4 & 3.28 & \textbf{6.27} & & 3.10 & 6.53  && 1.71  &  \textbf{8.21}  &  & 7.52  &  19.45    \\
Wanda & \ding{55} & 2:4 & 2.75 & 6.12 & & 2.75 & 6.48 & &  2.27  &   3.51  &  &  4.93   &  10.79  \\
RIA & \ding{55} & 2:4 & 2.56 & 4.73 & & 2.79 & 5.65 & &  1.98  &   6.74 &   &  6.49  &  17.22  \\
\hdashline
\textsc{OptiShear} & \ding{55} & 2:4 & \textbf{3.34} & \textbf{6.27}  &  & \textbf{3.41} & \textbf{6.72} & &  \textbf{2.52}  &  6.74  &  &  \textbf{7.91}  & \textbf{20.33}   \\
%GSM8K Metric & \ding{55} & 2:4 & \textbf{10.29} & \textbf{8.02} &   & \textbf{10.29} & \textbf{7.60} &    &  36.86  &  38.00 &   &  10.19  & 9.61   \\
\bottomrule
\end{tabular}%
}
\caption{Evaluations of semi-structured N:M sparsity on WikiText and GSM8K datasets.}
\label{tab:N:M}
\vspace{-1.5em}
\end{table*}

\paragraph{Calibration Data.}
Calibration data is used to estimate input statistics from a small set of samples. For a fair comparison, we use the exact same calibration data as Wanda and SparseGPT when evaluating on LM Harness and WikiText, which includes 128 sequences sampled from the C4 training set \citep{raffel2020exploring}. For evaluations on GSM8K and MMLU, we randomly select 10 samples from the training dataset, each truncated to a sequence length of 512, as our calibration samples.



\subsection{Main Results}
\paragraph{LM Harness \& Language Modeling.}
% Table \ref{tab:all_res} presents the performance of LM Harness and the WikiText perplexity on the language modeling task. We refer the reader to Appendix \ref{appendix:lm-harness} for task-wise performance.
% %the mean zero-shot accuracies on 7 zero-shot commonsense reasoning tasks and WikiText perplexity in language modeling task, for pruned LLaMA-1/2/3, and Mistral models. 
% The results indicate that our method consistently outperforms all established baselines across the board.
% More intriguingly, a notable performance gap between SparseGPT and the other two baselines, i.e. Wanda and RIA, is observed on the LLaMA-3 and Mistral models, while the performance remains comparable on the LLaMA-1 and LLaMA-2 models. This observation is also aligned with the results in previous work \citep{sun2023simple}.
% However, our \textsc{OptiShear} framework further significantly improves the state-of-the-art performance on all types of models.
% %while SparseGPT, Wanda, and RIA exhibit comparable results in LLaMA-1 and LLaMA-2 models, SparseGPT significantly outperforms them in the LLaMA-3 and Mistral models.
% %Our OptiShear-derived metric further enhances SparseGPT results.
% We think this is attributed to the different weight distributions of LLaMA-1/2 models with the LLaMA-3 model, as depicted in Figure \ref{fig:weight_distribution}, highlighting the importance of adaptive pruning on different models.
Table \ref{tab:all_res} shows our evaluation results on LM Harness and WikiText perplexity (detailed task-wise results in Appendix \ref{appendix:lm-harness}). \textsc{OptiShear} consistently outperforms all baseline methods across different models. Notably, while SparseGPT shows comparable performance to Wanda and RIA on LLaMA-1/2 models \citep{sun2023simple}, it exhibits a significant performance gap on LLaMA-3 and Mistral models. Our \textsc{OptiShear} framework achieves new state-of-the-art results across all model types, which we attribute to its adaptive pruning approach that accounts for different weight distributions between LLaMA-1/2 and LLaMA-3 models, as shown in Figure \ref{fig:weight_distribution}.
% We also explore the effectiveness of \textsc{OptiShear} when scaling up the model size, such as LLaMA-30B and LLaMA2-70B. As shown in Table \ref{tab:70b}, \textsc{OptiShear} consistently achieves the best performance on both WikiText perplexity and LM Harness benchmarks. Besides, for the particularly larger models like LLaMA2-70B, \textsc{OptiShear} can even outperform the dense model without the need for weight updating.
Our evaluation extends to larger models including LLaMA-30B and LLaMA2-70B, where \textsc{OptiShear} maintains superior performance on both WikiText perplexity and LM Harness benchmarks. (in Appendix \ref{sec:70b} Table \ref{tab:70b}). Remarkably, when applied to LLaMA2-70B, \textsc{OptiShear} achieves performance surpassing the dense model even without weight adjustment. We provide the 



%In Table \ref{tab:70b}, we scale to larger models and present the language modeling and zero-shot performance of models pruned to 50\% unstructured sparsity using Magnitude, Wanda, SparseGPT, RIA, and OptiShear on the LLaMA1-30B and LLaMA2-70B models. The last column reports the average performance across the datasets from LM Harness. As shown in the table, OptiShear achieves the best performance on WikiText perplexity and also achieves the best average performance across the 7 datasets.

\vspace{-1em}


% \begin{table}[htbp]
% \caption{GSM8K accuracies(\%) of pruned LLaMA-1/2/3 and Mistral models. }
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{lccccccccccccc}
% \toprule
% \multirow{2}{*}{Method} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Weight\\ Update\end{tabular}} &
%   \multirow{2}{*}{Sparsity} &
%   \multicolumn{2}{c}{LLaMA-1} & &
%   \multicolumn{2}{c}{LLaMA-2} & &
%   \multicolumn{2}{c}{LLaMA-3} & &
%   \multicolumn{2}{c}{Mistral} \\
% \cline{4-5} \cline{7-8} \cline{10-11} \cline{13-14}
%   &  &  & 7B & 13B & & 7B & 13B & & 8B & 8B-Inst & & 7B & 7B-Inst \\ \hline
% \rowcolor{gray!30}
% Dense & - & 0\%  & 11.07 & 17.82 & & 14.59 & 19.86 & & 52.39 & 74.45 & & 40.11  &  47.76  \\
% \hdashline
% Magnitude  & \ding{55} & 50\% & 1.52 & 5.99 && 2.05 & 6.22  & & 1.97 & 1.29 &  & 15.53  &  27.37  \\
% SparseGPT  & $\checkmark$ & 50\% & 8.19 & 15.60 & & 8.11 & 13.42  & & 21.46 & 49.20 & & 25.40 &  33.97  \\
% Wanda & \ding{55} & 50\% & 7.96 & 11.52  & & 7.43  & 9.10   & & 10.16 & 32.68 & & 22.74 &  33.59  \\
% RIA & \ding{55} & 50\% & 8.04 & 11.14 & & 7.96 & 9.25  & & 15.85 & 52.39 & & 24.18 &  32.15  \\
% Pruner-Zero & \ding{55} & 50\% & 6.41 & 9.22 & & 7.32 & 8.58  & & 17.25 & 43.63 & & 21.16 &  32.24  \\
% \hline
% OptiShear & \ding{55} & 50\% & 8.14 & 15.37 && 8.13 & 13.79 &  & 41.17 & \textbf{52.39} & & 25.31 & \textbf{35.25}   \\
% % Incr vs. non-update  & - & - & +0.10 & +3.85 & &  +0.17  & +4.54 &  &  &  +1.13 & +1.66  \\
% \quad w/ eval. & \ding{55} & 50\% & \textbf{8.22} & \textbf{15.62}  & & \textbf{8.47} & \textbf{15.03} & & \textbf{43.07}  & 52.15 & & \textbf{25.78} &   35.14   \\
% % Incr vs. non-update  & - & - & +0.18 & +4.09 & &  +0.51  & +5.78 &  &  &  +1.60 & +1.55  \\
% \bottomrule
% \end{tabular}%
% }
% \label{tab:gsm8k}
% \vspace{-1em}
% \end{table}

% \begin{table*}[t!]
% \resizebox{\textwidth}{!}{
% \fontsize{6.5}{8} \selectfont
% \begin{tabular}{lccccccccccccc}
% \toprule
% \multirow{2}{*}{Method} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Weight\\ Update\end{tabular}} &
%   \multirow{2}{*}{Sparsity} &
%   \multicolumn{2}{c}{LLaMA-1} & &
%   \multicolumn{2}{c}{LLaMA-2} & &
%   \multicolumn{2}{c}{LLaMA-3} & &
%   \multicolumn{2}{c}{Mistral} \\
%   \cline{4-5} \cline{7-8} \cline{10-11} \cline{13-14}
%   &  &  & 7B & 13B & & 7B & 13B & & 8B & 8B-Inst & & 7B & 7B-Inst \\ \hline
%   \rowcolor{gray!15}
% \multicolumn{14}{c}{GSM8K} \\
% Dense & - & 0\%  & 11.07 & 17.82 & & 14.59 & 19.86 & & 52.39 & 74.45 & & 40.11  &  47.76  \\
% \hdashline
% Magnitude  & \ding{55} & 50\% & 1.52 & 5.99 && 2.05 & 6.22  & & 1.97 & 1.29 &  & 15.53  &  27.37  \\
% SparseGPT  & $\checkmark$ & 50\% & 8.19 & 15.60 & & 8.11 & 13.42  & & 21.46 & 49.20 & & 25.40 &  33.97  \\
% Wanda & \ding{55} & 50\% & 7.96 & 11.52  & & 7.43  & 9.10   & & 10.16 & 32.68 & & 22.74 &  33.59  \\
% RIA & \ding{55} & 50\% & 8.04 & 11.14 & & 7.96 & 9.25  & & 15.85 & 52.39 & & 24.18 &  32.15  \\
% Pruner-Zero & \ding{55} & 50\% & 6.41 & 9.22 & & 7.32 & 8.58  & & 17.25 & 43.63 & & 21.16 &  32.24  \\
% \hline
% \textsc{OptiShear} & \ding{55} & 50\% & 8.14 & 15.37 && 8.13 & 13.79 &  & 41.17 & \textbf{52.39} & & 25.31 & \textbf{35.25}   \\
% % Incr vs. non-update  & - & - & +0.10 & +3.85 & &  +0.17  & +4.54 &  &  &  +1.13 & +1.66  \\
% \quad w/ eval. & \ding{55} & 50\% & \textbf{8.22} & \textbf{15.62}  & & \textbf{8.47} & \textbf{15.03} & & \textbf{43.07}  & 52.15 & & \textbf{25.78} &   35.14   \\
% \hline
% \rowcolor{gray!15}
% \multicolumn{14}{c}{MMLU} \\
% Dense & - & 0\%  & 35.28 & 46.98 & & 41.97 & 51.47 & & 65.23 & 66.35 & & 58.92 & 62.54   \\
% \hdashline
% Magnitude  & \ding{55} & 50\% & 26.24 & 30.12 & & 26.04 & 43.83 & & 4.36 & 12.03 & & 50.83 & 49.52   \\
% SparseGPT  & $\checkmark$ & 50\% & 29.48 & 38.29  & & 33.03 & 47.14 & & 49.50 & 52.27 & & 50.95 & 52.04   \\
% Wanda & \ding{55} & 50\% & 29.81 & 37.84 & & 32.09 & 48.06 & & 49.05 & 53.15 & & 53.05 & 53.62   \\
% RIA & \ding{55} & 50\% & 30.37 & 37.79 & & 31.46 & 47.39 & & 48.99 & 54.02 & & 52.67 & 53.14   \\
% Pruner-Zero & \ding{55} & 50\% & 28.57 & 35.51 & & 30.26 & 45.24 & & 41.39 & 46.32 & & 51.75 & 53.15   \\
% \hline
% \textsc{OptiShear} & \ding{55} & 50\% & 30.93 & 38.80  & & 32.24 & 48.15 & & 50.65 & 55.11 & & 53.10 & 53.77  \\
% \quad w/ eval. & \ding{55} & 50\% & \textbf{31.05} & \textbf{39.76} & & \textbf{33.06} & \textbf{48.38} & & \textbf{51.22} & \textbf{55.60} &  & \textbf{53.87} &   \textbf{54.36}  \\
% \bottomrule
% \end{tabular}
% }
% \caption{GSM8K and MMLU accuracies(\%) of pruned LLaMA-1/2/3 and Mistral models.}
% \label{tab:gsm8k}
% \vspace{-1em}
% \end{table*}

\paragraph{Arithmetic \& Knowledge Reasoning.}
% % In Tables \ref{tab:gsm8k} and \ref{tab:mmlu}, we present the accuracies achieved on the GSM8K and MMLU datasets of the pruned LLaMA-1/2/3, and Mistral models. 
% % Notably, our search-derived pruning metric consistently surpasses other weight update-free pruning metrics and performs on par with SparseGPT across various models in arithmetic reasoning. Moreover, it consistently outperforms all baseline methods in knowledge reasoning tasks. 
% In Table \ref{tab:all_res}, we report the performance of pruned LLaMA-1/2/3 and Mistral models on the GSM8K and MMLU dataset. 
% We can see that \textsc{OptiShear} consistently outperforms all baselines on the reasoning tasks. 
% % As the findings on the MMLU dataset are similar to those on GSM8K, we provide detailed results on the MMLU dataset in Appendix \ref{appendix:mmlu}.
% We highlight that we make remarkable improvements on the GSM8K dataset. For instance, on the LLaMA-3 8B model, \textsc{OptiShear} achieves an accuracy of 41.17, significantly better than the previous best performance of 21.46.
% This result also suggests that existing pruning methods are sensitive to the models.
% %Notably, previous post-training pruning metrics retain only 41\% of the GSM8K accuracies on the LLaMA-3 8B model, while our approach preserves 78\%. 
% Additionally, since the optimal pruning strategies differ across the tasks but \textsc{OptiShear} is task-agnostic, we also attempt to align the search process of \textsc{OptiShear} with the target task objective. We implement it by introducing the evaluation accuracy on the validation set as an additional search objective. We find that with the aid of evaluation accuracy, further improvements are achieved over the standard \textsc{OptiShear}.

For GSM8K and MMLU datasets in Table \ref{tab:all_res}, \textsc{OptiShear} consistently outperforms all baselines. The improvements are particularly pronounced on GSM8K, where \textsc{OptiShear} achieves an accuracy of 41.17 on LLaMA-3 8B, nearly doubling the previous best result of 21.46. This substantial gap highlights the sensitivity of existing pruning methods to different model architectures. While \textsc{OptiShear} is inherently task-agnostic, we found that incorporating validation set accuracy as an additional search objective further enhances performance beyond the standard \textsc{OptiShear} approach. We provide detailed optimal metrics for each LLM  in Appendix \ref{sec:appendix_coe}.

\paragraph{Comparison to Pruner-Zero.}
As demonstrated in Table \ref{tab:all_res}, \textsc{OptiShear} significantly outperforms Pruner-Zero, while offering two key advantages: it achieves superior results without requiring computationally expensive gradient calculations, and it accelerates the pruning process 7-fold for LLaMA2-7B by employing model-wise reconstruction error instead of WikiText perplexity for evaluation.

% Pruner-Zero \citep{dongpruner} is also an adaptation-based pruning method, which searches symbolic pruning metrics using genetic programming.
% Notably, \textsc{OptiShear} differs from Pruner-Zero in two key aspects: 1) Search Space: Pruner-Zero's search space involves weights, activations, and gradients, while \textsc{OptiShear} deliberately omits gradient computations. Despite this omission, as shown in Tables \ref{tab:all_res}, Pruner-Zero even underperforms when compared to the baselines like Wanda and RIA, which rely on static metrics derived from weights and activations. Moreover, the calculation of gradients also introduces additional computational overhead.
% %Pruner-Zero’s weight- and gradient-based metric underperforms weight- and activation-based metrics like Wanda and RIA on LLaMA-1/2 models, but adds extra pruning time due to gradient calculations.
% 2) Search Evaluation: Pruner-Zero uses perplexity on WikiText as search evaluation, whereas \textsc{OptiShear} relies on model-wise reconstruction error, thus substantially decreasing the evaluation duration. For instance, pruning LLaMA2-7B takes less than 10 seconds per trial with \textsc{OptiShear}, compared to over 70 seconds with Pruner-Zero.
\vspace{-0.2cm}
\paragraph{N:M Semi-Structured Pruning.}
% While \textsc{OptiShear} is designed for unstructured sparsity, it can be easily extended to semi-structured N:M sparsity \citep{mishra2021accelerating}, which can leverage NVIDIA’s sparse tensor cores to accelerate matrix multiplication in practice. 
% In Table \ref{tab:N:M}, we report the performance of 4:8 and 2:4 sparsity constraints on the WikiText and GMS8K datasets. We find that \textsc{OptiShear} consistently achieves superior performance than baselines, except LLaMA-3 models. We think this is because the LLaMA-3 model, trained on a larger amount of data, exhibits higher knowledge density \citep{llama3}. Thus, pruning a continuous block of parameters in semi-structured pruning leads to a significant performance drop, necessitating weight updates in SparseGPT for recovery.
\textsc{OptiShear}'s framework extends naturally to semi-structured N:M sparsity \citep{mishra2021accelerating}, enabling hardware acceleration through NVIDIA's sparse tensor cores. Table \ref{tab:N:M} shows that under 4:8 and 2:4 sparsity constraints, \textsc{OptiShear} outperforms baselines on both WikiText and GSM8K datasets, with one notable exception: LLaMA-3 models. This exception likely stems from LLaMA-3's higher knowledge density due to its larger training dataset \citep{llama3}, making it more sensitive to the continuous parameter blocks removed in semi-structured pruning—a limitation that requires SparseGPT's weight updates for compensation.

%comparing baseline metrics with our searched pruning metrics from language modeling and arithmetic reasoning tasks. Our metrics consistently outperform the baselines on LLaMA-1/2 and Mistral models under N:M semi-structured pruning conditions, though SparseGPT leads on LLaMA-3 models.

\subsection{Searched Layerwise Sparsity Ratios}
% Motivated by the distinct importance of parameters across different layers \citep{wang2020rethinking, zhang2021moefication}, another component of \textsc{OptiShear} involves setting specific pruning ratios for each layer while maintaining an overall 50\% reduction in parameters. 
% Table \ref{tab:ratio} demonstrates that these layer-specific sparsity ratios, optimized through our pruning metric, not only enhance our model's performance but also significantly improve other baseline metrics, such as Wanda and RIA.
% Notably, these searched sparsity ratios lead to an average relative improvement of 4.68\% in perplexity reduction on the WikiText set. Furthermore, in LLaMA-3 models, these ratios enhance the performance of Wanda and RIA by an impressive score of 13.68\%. 
% Details of the layerwise sparsity ratios for each LLM are provided in Appendix \ref{sec:appendix_coe}, and the results generally show that the upper layers tend to have more redundant parameters than the lower layers, aligning with findings from previous studies.

Building on research showing varying parameter importance across layers \citep{wang2020rethinking, zhang2021moefication}, \textsc{OptiShear} implements layer-specific pruning ratios while maintaining 50\% overall sparsity. Table \ref{tab:ratio} shows these optimized ratios enhance not only our model's performance but also improve baseline methods like Wanda and RIA, achieving 4.68\% better perplexity on WikiText and a 13.68\% improvement for LLaMA-3 models. The detailed ratios in Appendix \ref{sec:appendix_coe} reveal higher redundancy in upper layers compared to lower layers, consistent with previous research findings.

\begin{table*}[hbtp]
\resizebox{\textwidth}{!}{%
\fontsize{10}{11} \selectfont
\begin{tabular}{lcccccccccccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Uniform} & 
\multicolumn{2}{c}{LLaMA-1} & & \multicolumn{2}{c}{LLaMA-2} & &\multicolumn{2}{c}{LLaMA-3} & & \multicolumn{2}{c}{Mistral} \\
\cline{3-4} \cline{6-7} \cline{9-10} \cline{12-13}
& & 7B & 13B & & 7B & 13B && 8B & 8B-Inst & &7B & 7B-Inst \\
\hline
Wanda & $\checkmark$   &  6.90  &  5.82 &  &  6.47  &  5.64 & & 10.57 & 16.37 & & 7.24  &   7.22      \\
\multirow{2}{*}{Wanda w/ Ratio } & \multirow{2}{*}{\ding{55}} &  6.72  &  5.64  & & 6.28 &   5.52 &  & 9.45  &  13.67 & & 6.97 & 6.98  \\
& & (\textbf{+2.61}) & (\textbf{+3.09}) & &  (\textbf{+2.94}) & (\textbf{+2.13}) & & (+10.60) & (\textbf{+16.49}) & & (+3.73) & (+3.32) \\
\hdashline
RIA & $\checkmark$ & 6.81  &  5.83  &  &  6.43  & 5.63 & & 12.56 & 15.57 & & 7.27  &   7.21      \\
\multirow{2}{*}{RIA w/ Ratio}  & \multirow{2}{*}{\ding{55}}   & 6.65  &  5.67  &   &  6.26  &  5.54 & &  10.98  & 13.23  &  & 6.89  &   6.96   \\
 &  & (+2.35) & (+2.74) & & (+2.64) & (+1.60) & & (\textbf{+12.58}) & +(15.03) & & (\textbf{+5.23}) & (\textbf{+3.47}) \\
\hdashline
{\textsc{OptiShear} wo/ Ratio} & $\checkmark$ &  6.75  &  5.75   & &  6.32  & 5.52 & & 9.23 & 11.37 &  & 6.22 & 6.55 \\
\multirow{2}{*}{\textsc{OptiShear}} & \multirow{2}{*}{\ding{55}}  &  6.61  &  5.60  &   &  6.19  & 5.44  &   & 8.95 &  10.73 &  &  6.08  &  6.39  \\
 &  & (+2.07) & (+2.61) & & (+2.06) & (+1.45) & & (+3.03) & (+5.63)& & (+2.25) & (+2.62) \\
\bottomrule
\end{tabular}%
}
\vspace{-3mm}
\caption{Our searched layerwise sparsity ratios are effective for both Wanda and RIA metrics. The number (\%) in ($\cdot$) denotes the relative improvement (RI). For instance, \text{Wanda RI} = (Wanda w/ Ratio - Wanda) / Wanda.}
\label{tab:ratio}
\vspace{-3mm}
\end{table*}

\begin{table*}[t!]
\resizebox{\textwidth}{!}{%
\fontsize{5.5}{6} \selectfont
\begin{tabular}{lccccccccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c}{WiKiText} & & \multicolumn{4}{c}{LM Harness} \\
 \cline{2-5} \cline{7-10}
 & L1-7B & L2-7B & L3-8B & M-7B & & L1-7B & L2-7B & L3-8B & M-7B\\
% & \multicolumn{2}{c}{LLaMA-1} & \multicolumn{2}{c}{LLaMA-2} & \multicolumn{2}{c}{LLaMA-1} & & \multicolumn{2}{c}{LLaMA-2} \\
 %\cline{2-3} \cline{5-6} \cline{8-9} \cline{11-12}
 %& 7B & 13B  & 7B & 13B & & 7B & 13B & & 7B & 13B \\
% & ppl & com & ppl & com & ppl & com & ppl & com & ppl & com & ppl & com & ppl & com & ppl & com \\
\cline{1-10}
%Dense &- & 0\%  &  5.37   &   59.70        &  4.80   &     62.58      &   5.04  &      59.72     &  4.56   &     63.03      \\
%Magnitude &\ding{55} & 50\% &  13.27   &   46.89  &  13.55   &     47.34      &  11.96   &    52.40       &  6.16   &  52.90         \\
SparseGPT & 6.92 & 6.59 & 10.89 & 6.42 & & 54.86 & 55.90 & 53.87 & 57.49 \\
%Wanda  &\ding{55} & 50\%   &  6.90   &     54.08      &  5.82   &  59.18         &  6.47   &     55.89      &  5.64   &     60.88      \\
%RIA  &\ding{55} & 50\%   &  6.81   &     55.10      &    5.83 &    59.45       &  6.43   &     55.67      &  5.63   &    61.03       \\
\textsc{OptiShear} & 6.78 & \textbf{6.35} & \textbf{9.23} & 6.22 & & 55.10 & \textbf{57.47} & 55.50 & \textbf{59.33} \\
\hdashline
\quad GSM8K Metric  &  6.78 & 6.39 & 12.78 & 6.23 & & 55.15 & 56.05 & \textbf{55.59}  & 57.66 \\ 
\hline
% LLaMA-2 Metric & \textbf{6.76} & - & - & - & & \textbf{55.24}  & - &  -  & - \\
% LLaMA-3 Metric & - & - & - & \textbf{6.16} & & - & - & - & 58.30\\
Transferred Metric & \textbf{6.76} & \textbf{6.35} & \textbf{9.23} & \textbf{6.16} & & \textbf{55.24}  & \textbf{57.47} &  55.50  & 58.30 \\
\bottomrule
\end{tabular}%
}
\vspace{-3mm}
\caption{WikiText perplexity and zero-shot reasoning accuracy (\%) with different pruning metrics.}
\label{tab:transfer_gsm8k_1}
\vspace{-5mm}
\end{table*}



\subsection{Speedup}
% The theoretical computational complexity of SparseGPT is $O(d_{hidden}^3)$, while our meta pruning metric has a lower complexity of $O(d_{hidden}^2)$. We compare their empirical pruning speed on NVIDIA RTX A6000 GPUs by measuring the total time required to prune the model to 50\% sparsity using each metric. Calibration data from the C4 training dataset is used to estimate activation magnitudes for the language modeling task. As shown in Table \ref{tab:prune-speed}, our meta pruning metric results in negligible time overhead compared to SparseGPT. We further evaluate the inference speedup for semi-structured 4:8 and 2:4 sparsity on NVIDIA RTX A6000 GPUs.
% Our simulations utilize the high-performance GEMM kernel from the NVIDIA CUTLASS library. According to the results presented in Table \ref{tab:infer-speed}, when compared with dense models, we observe an average speedup of 1.20$\times$ in end-to-end latency.

While SparseGPT has a computational complexity of $O(d_{hidden}^3)$, our meta pruning metric achieves a lower complexity of $O(d_{hidden}^2)$. Empirical measurements on NVIDIA RTX A6000 GPUs, using C4 dataset for calibration, confirm negligible pruning overhead when reducing models to 50\% sparsity (Table \ref{tab:prune-speed}). Furthermore, leveraging NVIDIA CUTLASS library's GEMM kernel for semi-structured 4:8 and 2:4 sparsity implementations, we achieve an average 1.20× inference speedup compared to dense models (Table \ref{tab:infer-speed}).

\begin{table}[h!]
\centering
    \fontsize{9.5}{11} \selectfont
    \begin{tabular}{lcccc}
    \toprule
        Method & L2-7B & L2-13B & L3-8B & M-7B \\
     \hline
        SparseGPT  & 370.03 & 464.77 & 457.71 &  450.76   \\
        % \rowcolor{gray!30}
        \textsc{OptiShear} & \textbf{56.16} & \textbf{107.11} & \textbf{60.11} &\textbf{59.80}  \\
    \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Pruning speed for pruning LLaMA-2/3 and Mistral models to 50\% sparsity.}
    \label{tab:prune-speed}
    \vspace{-3mm}
\end{table}

\begin{table}[h!]
\centering
    \fontsize{9.5}{11} \selectfont
    \begin{tabular}{lcccc}
    \toprule
    Sparsity & L2-7B & L2-13B & L3-8B & M-7B \\
    \hline
    % 50\%  & {1.15}$\times$ & \textbf{1.14}$\times$ & \textbf{1.17}$\times$ & \textbf{1.18}$\times$ \\
    4:8 & {1.11}$\times$ & 1.04$\times$ & 1.15$\times$ & \textbf{1.17}$\times$ \\
    2:4 & \textbf{1.35}$\times$ & \textbf{1.14}$\times$ & \textbf{1.15}$\times$ & 1.16$\times$ \\
    \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Inference speedup of different sparsity patterns for LLaMA-2/3 and Mistral models.}
    \label{tab:infer-speed}
    \vspace{-5mm}
\end{table}

