%
%
%

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%

\documentclass{article}
\usepackage[accepted]{icml2025}
\bibliographystyle{icml2025}

\icmltitlerunning{Optimal prompts for sequence predictors}


%
\usepackage[hang,flushmargin]{footmisc} %

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{xfrac}    %
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\Crefname{equation}{Eq.}{Eqs.}
\Crefname{figure}{Fig.}{Figs.}
\Crefname{tabular}{Tab.}{Tabs.}
\Crefname{section}{Sec.}{Secs.}
\usepackage{bbm}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
%
\usepackage{xspace}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}


\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\DeclareMathOperator{\RNN}{\mathrm{RNN}}
\DeclareMathOperator{\Beta}{\mathrm{Beta}}
\DeclareMathOperator{\Bernoulli}{\mathrm{Bernoulli}}
\DeclareMathOperator{\BetaBern}{\mathrm{BetaBern}}
\DeclareMathOperator{\BernMix}{\mathrm{BernMix}}
\DeclareMathOperator{\Bern}{\mathrm{Bern}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\KL}{\mathrm{KL}}
\DeclareMathOperator{\MI}{\mathrm{MI}}
\DeclareMathOperator{\MAMI}{\mathrm{MAMI}}
\newcommand{\Lmax}{{L_{\text{max}}}}
\def\cX{{\cal X}}
\def\vs{{\mathbf{s}}}
\def\ud{{\mathrm{d}}}
\def\vx{{\mathbf{x}}}
\def\eps{\varepsilon} %
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\Eqref}[1]{Eqn.~\eqref{#1}}
\newcommand{\DqN}{{\mathcal{D}_{N}}}
\newcommand{\ZERO}{{\textsc{zero}}\xspace}
\newcommand{\ZEROs}{{\textsc{zero}s}\xspace}
\newcommand{\ONE}{{\textsc{one}}\xspace}
\newcommand{\ONEs}{{\textsc{one}s}\xspace}
\newcommand{\pbayes}{{p_{\textrm{B}}}}
\newcommand{\vL}{{v_{\textrm{L}}}}
\newcommand{\vR}{{v_{\textrm{R}}}}
\newcommand{\pdot}{{p_{(\cdot)}}}



%
%

%

%
%

%
%
%

%
%

\def\theabstract {
%

Large language models (LLMs) can be prompted to do many tasks, but finding good prompts is not always easy, nor is understanding some performant prompts.
%
%
We explore these issues
%
by viewing prompting as conditioning a near-optimal sequence predictor (LLM)
pretrained on diverse data sources.
%
%
Through numerous prompt search experiments,
we show that the unintuitive patterns in optimal prompts can be better understood given the pretraining distribution, which is often unavailable in practice.
%
Moreover, even using exhaustive search, reliably identifying optimal prompts from practical neural predictors can be difficult.
%
Further, we demonstrate that common prompting methods, such as using intuitive prompts or samples from the targeted task, are in fact suboptimal.
%
%
%
Thus, this work takes an initial step towards understanding the difficulties in finding and understanding optimal prompts from a statistical and empirical perspective.
%
%

%
%
%
}

%
%


%
%
%
%
\begin{document}

\twocolumn[
%
\icmltitle{Why is prompting hard? Understanding prompts on binary sequence predictors}

%
%
%
%

%
%
%
%

%
%
%

\begin{icmlauthorlist}
\icmlauthor{Li Kevin Wenliang}{gdm}
\icmlauthor{Anian Ruoss}{gdm}
\icmlauthor{Jordi Grau-Moya}{gdm}
\icmlauthor{Marcus Hutter}{gdm}
\icmlauthor{Tim Genewein}{gdm}
\end{icmlauthorlist}

\icmlaffiliation{gdm}{Google DeepMind, London, UK}

\icmlcorrespondingauthor{Li Kevin Wenliang}{kevinliw@google.com}

%
%
%
\icmlkeywords{prompt optimization, interpretability, sequence prediction}

\vskip 0.3in
]

%

%
%
%
%
%

\printAffiliationsAndNotice{}  %
%


\begin{abstract}
    \theabstract
\end{abstract}

%
%

%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%


%
\section{Introduction}
%
%
LLMs are pretrained over large text corpora, produced by many writers with diverse personalities, writing styles, sentiments, and topics. The distribution over these high-level \emph{latent factors}, together with the text distribution of each writer, implicitly define a hierarchical generative process, inducing a meta-distribution of tokens over which LLMs are pretrained. Minimizing next-token prediction error on sufficient training data from the meta-distribution theoretically leads to a
%
Bayes-optimal predictor for the meta-distribution \citep{ortega2019meta}.
One hallmark feature of such a predictor is that it can be steered using human-readable prompts to perform various tasks (prompting and in-context learning). In the case of LLMs, this behavior is very familiar \citep{brown2020language}, where the LLM takes a suitable prompt to implicitly \emph{infer}
%
the latent factors desirable for a particular behavior  \citep{xieexplanation,wang2023large,jiang2023latent,wies2023learnability,arora2024bayesian}.
%
%
%
Nonetheless, heuristic and handcrafted prompts often underperform our expectations, 
while some known powerful prompts appear eccentric and beg for better explanations. 


%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%

To better understand why prompting can be hard on LLMs, we leverage simpler models pretrained on binary sequences. Suppose that, given a model pretrained on sequences of i.i.d.\ coin flips, with the coin bias randomly chosen for each sequence, the task is to generate $70\%$ heads---what is the best prompt?
%
%
Without further information, a heuristic prompt is one with $70\%$ heads, matching the desired ratio in the task. However, as we will show, this is not always the best prompt: it depends on the pretraining meta-distribution which is often unknown.
%
%
Likewise, we can investigate fundamental properties of optimal prompts: Are longer prompts better? Does more data help identify optimal prompts? Can we readily tell features of the task from the optimal prompt? Perhaps unintuitively, the answers to all these are ``not always'', as they strongly depend on the often-overlooked pretraining distribution. We thus reveal and explain unexpected challenges in finding and interpreting optimal prompts, shedding light on the difficulties of prompting LLM.
%
%
More concretely, our main observations are:
\vspace{-0.5em}
\begin{enumerate}
    \itemsep 0.0em 
    \item Optimal prompt can be \textbf{atypical} for the task: biases in the pretraining distribution can pull the prompt away from the task distribution, obfuscating its connection to the task.
    %
    Trying to make sense of optimal prompts given only the task but not the pretraining distribution is hard or even futile.
    %
    %
    %
    \item Identifying optimal prompts can be \textbf{unreliable} when evaluating performance on a finite dataset;
    even an exhaustive search can yield only near- but sub-optimal prompts, leading to inconsistent interpretations.
    %
    \item Using approximate neural predictors, the reliability of finding the optimal prompt can follow \textbf{unintuitive trends}; for example, increasing the task dataset size does not always improve reliability.
    %
    \item On performance, the common practice of prompting by \textbf{samples or demonstrations} from the desired task for in-context learning is \textbf{less effective} compared to unintuitive prompts.
\end{enumerate}

%



%


%
\section{Related work}
%
%
A large set of works propose methods to more reliably find better prompts. Prompt engineering, e.g.\  \citet{liu2023pre,chen2023unleashing,marvin2023prompt,sahoo2024systematic,song2024communication}, can improve performance on many tasks by handcrafting prompts using intuitions from system design, search and planning, and empirical trial and error. Prompt optimization, e.g.\ \citet{pryzant2023automatic,wang2023promptagent,fernando2023promptbreeder,guo2023connecting}, instead employs variants of discrete optimization and search techniques.
%
Another approach for prompt optimization, particularly in agentic or robotic tasks, is to find the best set of demonstrations for in-context learning~\citep{dong2022survey}.

While much focus has been put on the performance of the resulting prompts, we do not know whether they are actually optimal. It has been challenging to \emph{interpret} the prompts by relating to the desired tasks, and to understand concretely why some prompts work better than others~\citep{webson2022prompt,daras2022discovering,murr2023testing,he2024does,chen2023many,battle2024unreasonable,anagnostidis2024susceptible}. These difficulties are exacerbated by the variations across architectures, pretraining mixtures, fine-tuning processes, and nuanced parameters that vary across publicly available LLMs~\citep{chen2023chatgpt}
%
This has led to the question why some prompts work better than others, despite no obvious difference to humans~\citep{kojima2022large,mizrahi2024state}, and why some prompts that are intuitively expected to work may disappoint~\citep{zamfirescu2023johnny,khurana2024and}. The unreliability and unintuitive nature of prompting have caused safety and ethical concerns~\citep{wei2024jailbroken,wu2023jailbreaking,xu2024comprehensive}.


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
\begin{figure*}[ht!]
%
    \centering
    \includegraphics[page=1,width=\textwidth]{figs/OptimalPromptsFigures.pdf}
    \vspace{-1em}
    \caption{Experimental design to obtain optimal prompts under pretraining and task data generators.
    %
    %
    }
    \label{fig:experiment_figure}
    \vspace{-1em}
\end{figure*}

%
\section{Background}
%
See \cref{fig:experiment_figure} for an overview of our approach. %
%
We design synthetic data generators (DGs) with latent factors (akin to \citet{xieexplanation,jiang2023latent}) to define pretraining distributions and (downstream) tasks. Each DG induces a (meta-)distribution over binary token sequences. These DGs, from Bernoulli sequences in \cref{sec:CIB-DG_methods} to Bandit problems in \cref{sec:further_experiments}, have simple semantic meanings that aid interpretation of our results,
and have tractable Bayes-optimal predictors that serve as optimal baselines to practical neural predictors.
%

\subsection{Data generators} \label{sec:data_generators}

A DG produces each sequence hierarchically as follows:
First, draw a random latent value $\tau$ from a distribution $p_\tau(\tau)$; such as sampling the bias of a coin from a distribution on (0, 1).
%
%
Then, sample a sequence of length $T\in\mathbb{N}_+$ from a conditional distribution $p_{x|\tau}(x_{1:T}|\tau)$, e.g., sequences of coin flips from a coin with bias $\tau$.
The distribution induced by this DG is then the marginal (mixture)
\begin{gather}\label{eq:joint}
    p_x(x_{1:T}) = \int p_{x|\tau}(x_{1:T}|\tau) \ud p_\tau(\tau)\,.
\end{gather}
The sequence $x_{1:T}$ is composed of binary tokens from the alphabet $\mathcal{A}:=\{0,1\}$. 
%
%
Subscripts are omitted when no confusion should arise.
%
%
We consider (piecewise) conditionally independent DGs where the conditional $p_{x|\tau}$ satisfies that,
for some lengths $T, L\in\mathbb{N}_+$, we have $p_{x|\tau}(s_{1:L}x_{1:T} | \tau)=p_{x|\tau}(s_{1:L}|\tau)p_{x|\tau}(x_{1:T} | \tau)$ for all sequences $s_{1:L}\in\mathcal{A}^L$ and $x_{1:T}\in\mathcal{A}^T$.
%
%
In this case, when only $s_{1:L}$ is given to predict an unknown $x_{1:T}$, the Bayes-optimal predictor (or Bayes predictor) under the sequence distribution $p_x$ in \cref{eq:joint} is
\begin{equation}\label{eq:analytical_predictive}
  \pbayes(x_{1:T}|s_{1:L}):=\int p_{x|\tau}(x_{1:T}|\tau) \ud p_{\tau|x}(\tau|s_{1:L}) \,,
\end{equation}
where $p_{\tau|x}(\tau|s_{1:L})=p_\tau(\tau)p_{x|\tau}(s_{1:L}|\tau) / p_x(s_{1:L})$ is the posterior over the latent factors given $s_{1:T}$. We regard $s_{1:T}$ as the \emph{prompt} for the sequence(-to-predict) $x_{1:T}$.
%
%
Conditional independence helps interpret $s$, and 
%
is a common assumption \citep{xieexplanation,jiang2023latent,wang2023large}.

%
   
%
%
%
%

%
\subsection{Conditionally independent Bernoulli DGs}
%

Our first set of experiments in \cref{sec:CIB-DG_methods} uses DGs from the family
of conditionally independent Bernoulli DGs (CIB-DGs) where
$p_\tau$ has support on the unit interval, and 
$p_{x|\tau}(x_{1:T} | \tau)=\prod_{t} \mathrm{Bernoulli}(x_t; \tau)$.
%
The Bayes predictor of a CIB-DGs has the property that 
%
two prompts $s_{1:L}$ and $s'_{1:L}$ are equivalent in the sense that $\pbayes(\cdot | s_{1:L})\equiv \pbayes(\cdot| s'_{1:L})$ if they have the same counts of zeros and ones, defined as 
\begin{equation}\label{eq:counts}
\begin{aligned}
S_0(s_{1:L})\!:=\!\sum_{t=1}^L\mathbbm{1}[s_t\!=\!0],~~
S_1(s_{1:L})\!:=\!\sum_{t=1}^L\mathbbm{1}[s_t\!=\!1].
\end{aligned}
\end{equation}
%
%
This permutation invariance reduces the cost of prompt search on the Bayes predictor, but is unlikely to hold for neural networks with standard architectures \citep{mikulik2020meta}.
%
%
%
%
We define a few concrete CIB-DGs and their Bayes predictors used in the first part of our experiments:
\begin{definition}[Bernoulli] 
$\Bern(\tau)$ is the CIB-DG 
%
%
with $p_\tau=\delta_{\tau}$, where $\delta_\tau$ is Dirac delta.
The Bayes predictor evaluated on $x_{1:T}$ is trivially $\Bern(\tau)$ itself, that is,
$$
  \pbayes(x_{1:T}|s_{1:L})=p_x(x_{1:T})=\tau^{S_1(x_{1:T})}(1-\tau)^{S_0(x_{1:T})}
$$
\end{definition}
\begin{definition}[Bernoulli mixture]\label{thm:bernoulli-mixture}
$\BernMix(w, \tau_1, \tau_2)$ 
%
$p_\tau=(1-w)\delta_{\tau_1} + w\delta_{\tau_2}$ for $0<\tau_1<\tau_2<1$. 
In our experiments we use equal mixture weights, and write: $\BernMix(\tau_1,\tau_2):=\BernMix(0.5, \tau_1,\tau_2)$.
Its Bayes predictor is $\pbayes(\cdot|s_{1:L})=\BernMix(w_L(s_{1:L}), \tau_1,\tau_2)$, where
%
%
\begin{gather}\label{eq:mixture_weight_posterior}
w_L(s_{1:L})^{-1}=1+\left(\tfrac{\tau_1}{\tau_2}\right)^{S_1(s_{1:L})}\left(\tfrac{1-\tau_1}{1-\tau_2}\right)^{S_0(s_{1:L})}.
\end{gather}
%
\end{definition}
\begin{definition}[Beta-Bernoulli]\label{thm:beta-bernoulli}
$\BetaBern(\alpha,\beta)$ is the CIB-DG with $p_\tau=\mathrm{Beta}(\alpha, \beta)$.
The Bayes predictor is
%
%
$\pbayes(\cdot|s_{1:L}) = \BetaBern(\alpha+S_1(s_{1:L}), \beta + S_0(s_{1:L})).$
\end{definition}
%

\subsection{Neural predictors and meta-learning}\label{sec:neural_predictor}
%
In our experiments, we train neural predictors on samples from meta-distributions.
Denote such a predictor by $p_\theta(\cdot)$ where $\theta$ are parameters. The objective for a single sequence 
$x_{1:T}$ is
$
   - \log p_\theta(x_{1:T}) =  -\sum_{t=1}^T \log p_\theta(x_t|x_{1:t-1}).
$
The latent $\tau$ is resampled for each $x_{1:T}$.
%
%
Under realizability and convergence \citep{ortega2019meta}, neural predictors can \emph{meta-learn} to predict sequences by adapting to different latent factors, giving predictions that are indistinguishable from the Bayes predictor over $x\sim p_x$
%
\citep{mikulik2020meta,genewein2023memory,grau2024learning}.
However, how these predictors compare in prompt optimization has not been specifically studied before.
We thus address this question empirically in the following experiments. 

%

%
\section{Experiments on CIB-DGs}\label{sec:CIB-DG_methods}
%

%
We now describe our methodology (illustrated in \cref{fig:experiment_figure}), and first apply it to conceptually simple CIB-DGs, before moving to more complex DGs in \cref{sec:further_experiments}.
%
For each pretraining DG $p$, we obtain the ideal Bayes predictor $\pbayes$ by \cref{eq:analytical_predictive} and practical neural predictors $p_\theta$ of recurrent and Transformer-based architectures, detailed in \cref{sec:neural_predictor_detail}.
%
%
%
%
%


%
%

%
\begin{table*}[t]
%
\small
\centering
%
\caption{Summary of results on conditionally independent Bernoulli data generators. 
%
%
\label{tab:opt_prompt_summary}
}\vspace{-1em}
\begin{tabular}{lll|ccc}
\hline
%
\multirow{2}{*}{\textbf{Pretraining $p$}} & \multirow{2}{*}{\textbf{Task $q$}} & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Sec.}}}            & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{$s^*$ matches $\tau$ in $q$?}}}& \multicolumn{2}{c}{\textbf{$\hat s = s^*$ reliably as $N,T$ increases?}} \\
                                             &                                             &                                                                & \multicolumn{1}{c}{}                                              & \textbf{Bayes}    & \textbf{Neural}    \\ \hline
\multirow{2}{*}{$\BernMix(0.2, 0.7)$}        & $\Bern(0.7)\in\mathcal{M}_p$                                & \ref{sec:sensitivity_to_sampling}                                  &  No, extreme counts                           & No                & No                                     \\
                                             & $\Bern(0.6)\notin\mathcal{M}_p$                                & \ref{sec:ood_prompting}                                         & {No, unintuitive optimal length}              & No                & No                                     \\\hline
\multirow{2}{*}{$\BetaBern(1, 1)$}           & $\Bern(\tau)\in\mathcal{M}_p$                               & \ref{sec:continuous_latent}                                        & {Yes if $p(\tau)$ uniform, $N, T$ large }     & Yes               & Yes?                                    \\
                                             & $\BernMix(\tau_1, \tau_2)\notin\mathcal{M}_p$                  & \ref{sec:meta_learning}                                         & {Yes, for mean bias}                          & Yes               & Yes?                                   \\ \hline
\end{tabular}
\end{table*}

%
\begin{figure*}[t!] %
%
    \centering
    \includegraphics[height=0.2\textheight]{figs/sensitivity_correct.pdf}\hfill
    \includegraphics[height=0.195\textheight]{figs/sensitivity.png}
    \caption{
    Results for a pretraining DG $p=\BernMix(0.2, 0.7)$ and a task DG $q=\Bern(0.7)$.
    Left, the proportion correct for Bayes predictor ($10^3$ seeds per data point)
    and two neural predictors (30 seeds per data point). Error bar show 1 SEM.
    The black dotted line is the theoretical value for $T=1$ (see \Cref{sec:bernmix_bern_0.7_nonmonotonic}).
    %
    %
    Additional results are in \cref{sec:bernmix_bern_0.7_correct}.
    %
    Right, empirically optimal prompts at $\Lmax=5$ for the Bayes predictor for different values of $T$ (colors) and $N$ (panels); 100 repetitions per setting. The counts of zeros and ones are jittered. The cyan cross 
    shows the all-\ONE theoretical $s^*$.
    %
    %
    %
    }
    \label{fig:sensitivity}
    \vspace{-1em}
\end{figure*}

%
\subsection{Prompt setup and optimization}
%


%
%
\paragraph{Theoretically optimal prompt $s^*$.} Given the Bayes predictor $\pbayes$ of a pretraining DG $p$, the performance of a prompt $s_{1:L}$ towards a task DG $q$ is
%
\begin{equation}\label{eq:log_loss_L}
   \mathcal{L}(\pbayes, q, s_{1:L}) := -\sum_{x\in\mathcal{A}^L}q(x)\log \pbayes(x|s_{1:L})\,.
\end{equation}
%
%
The best prompt of length $L$ is then 
%
%
\begin{equation}\label{eq:optimal_prompt_L}
    s^*_{1:L}(\pbayes, q) := \argmin_{s_{1:L}} \mathcal{L}(\pbayes, q, s_{1:L})
\end{equation}
%
We also define the optimal prompt of length up to $\Lmax$ as 
\begin{equation}
\begin{aligned}\label{eq:optimal_prompt_Lmax}
    s^*_\Lmax(\pbayes,q) := \argmin_{\ell\in\{1,...,\Lmax\},s_{1:\ell}} \mathcal{L}(\pbayes, q, s_{1:\ell}).
\end{aligned}
\end{equation}
Both $s^*_{1:L}$ and $s^*_\Lmax$ are \emph{theoretically} optimal under different constraints, both requiring full knowledge of the pretraining and task DGs. In this work, we optimize prompts by exhaustive search; see \cref{sec:prompt_search}.

%
\vspace{-1em}
\paragraph{Empirically optimal prompt $\hat s\,$.}
Given a dataset $\DqN:=\{x_{1:T}^{i}\}_{i=1}^N$ of $N$ sequences from $q$, we optimize the prompt for a predictor $\pdot\in\{\pbayes, p_\theta\}$ under the empirical version of the loss \cref{eq:log_loss_L}:
\vspace{-0.3em}
\begin{equation}\label{eq:log_loss_L_est}
\vspace{-0.3em}
   \mathcal{\hat{L}}(\pdot, \DqN, s_{1:L}) = -\frac{1}{N}\sum_{i=1}^N \log \pdot(x_{1:T}^{i}|s_{1:L}).
\end{equation}
The resulting \emph{empirically} optimal prompts are denoted by $\hat{s}_{1:L}$ and $\hat{s}_{\Lmax}$ under the two constraints before. Each exhaustive search uses a \emph{fixed} $\DqN$, and we repeat for different draws of $\DqN$ to give a distribution of $\hat s$.
%
%
%
%
While the theoretical $s^*$ depends on the pretraining and task DGs, the sequence length $T$, and the prompt length $L$ or $\Lmax$; the empirical $\hat{s}$ depends additionally on the predictor $\pdot$ and dataset size $N$. Each configuration forms a \emph{prompt setup}, which we vary to gain insights on prompt optimization. 

\vspace{-1em}
\paragraph{Interpretating prompts.}
The prompt acts as both a parameter and a conditioning data. As in prompt optimization for LLMs, we hope to understand the semantics of (\emph{interpret}) the prompt, under the purpose set by the task $q$. In \cref{sec:info}, we show that, minimizing \cref{eq:log_loss_L} for certain family of tasks amounts to minimizing a mutual information-like objective between $s_{1:L}$ and $\tau$. 
Thus, after finding the theoretical $s^*$, we try to interpret it by recovering the latent $\tau$ in task DG.
Meanwhile, the empirical $\hat{s}$ is stochastic due to randomness in $\DqN$ and pretraining. To interpret $\hat{s}$, it is crucial to ensure that it matches $s^*$ reliably. As we will see from the distribution of $\hat s$ presented later, suboptimal prompts are not necessarily close to the theoretically optimal, and they may offer inconsistent interpretations with each other and with $s^*$. We will thus empirically estimate the probability that $\hat{s}$ matches $s^*$, or \emph{proportion correct}, averaged over random datasets and network instances; see \cref{sec:proportion_correct} for the definition of $\hat s$ being correct.  
%
%

Further, to explain any unreliability in identifying the theoretical $s^*$, we evaluate the loss ``landscape'' (define in \cref{sec:loss_landscape}) of each prompt setup. A shaper loss ``landscape'' around $s^*$ can improve reliability, and \emph{vice versa}.
%
%

\vspace{-1em}
\paragraph{In-meta-distribution.}
Previous works found that whether or not the task is within the pretraining mixture affects the performance of prompting \citep{wei2021pretrained,krishna2023downstream,wang2023large}.
To examine if this affects optimal prompts, we choose task DGs $q$ to be either \emph{in-meta-distribution} (IMD) w.r.t.\ $p$, where $q\in\mathcal{M}_p\!:=\!\{p_{x|\tau}(\cdot|\tau_0)| p_\tau(\tau_0) \!>\! 0\}$, or \emph{out-of-meta-distribution} (OOMD) w.r.t.\ $p$ ($q\notin\mathcal{M}_p$).
See \cref{tab:opt_prompt_summary} for examples.
%
%
%


%
\begin{figure*}%
%
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/length_dependence.pdf}\hfill
    \includegraphics[width=1.05\columnwidth]{figs/sensitivity_0.6_correct.pdf}
    \vspace{-0.5em}
    \caption{Results for $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.6)$. 
    Left, each circle represents the heads/tails count of the theoretically optimal prompts. 
    %
    %
    The orange dotted line indicates the maximum prompt length $\Lmax$.
    The green dashed line marks 60\% \ONEs in $s^*$.
    Right, the proportion correct of $\hat{s}=s^*$. \Cref{fig:sensitivity_0.6_supp} shows additional results.
    %
    %
    }
    \label{fig:length_dependence}
\end{figure*}
%

%
\begin{figure*}[t]
%
    \centering
    \includegraphics[height=0.22\textheight]{figs/beta_categorical_optimal_prompts_twobeta.pdf}\hspace{1cm}
    \includegraphics[height=0.22\textheight]{figs/beta_categorical_correct.pdf}
    \vspace{-0.5em}
    \caption{
    %
    Results for $p=\BetaBern(1, \beta)$ with $\beta\in\{1, 2\}$, and $q=\Bern(\tau)$ with $\tau\in\{0.7, 0.9\}$.
    Left, the ratio of \ONEs in the theoretical $s^*$. Red dotted line shows true bias of $q$.
    Right, proportion correct of $\hat s$ for $\beta=1$. \Cref{fig:beta_categorical_supp} has more results.
    }
    \label{fig:beta_categorical}
    \vspace{-0.5em}
\end{figure*}


%
\subsection{Results on CIB-DGs}\label{sec:CIB-DG_results}
%
%
We take four pairs of pretraining and task CIB-DGs, listed in \cref{tab:opt_prompt_summary}, covering discrete/continuous pretraining latent factors, and IMD/OOMD tasks.
%
Using CIB-DGs, we can easily visualize the optimal prompts, and see how they vary as we sweep the prompt setups.
%
For each pair, to find $s^*$, we sweep sequence length $T\in\{1,3,10,30,100\}$, and maximum prompt length $\Lmax\in\{5, 10, 15\}$; to find $\hat s$, we additionally sweep optimization datasets consisting of $N\in\{10^1,10^2,10^3,10^4\}$ sequences, and 7 predictor types (3 in main text).
For the Bayes predictor, we draw $\DqN$ $10^3$ times with different seeds; 
for each neural predictor type, we pretrain 30 instances and find $\hat s$ on a sampled $\DqN$, both using different seeds. Pretraining log-losses in \cref{sec:additional_results} show that LSTM and (softmax) Transformer are near-optimal.


%
%

%
\subsubsection{Sensitivity to random sampling}\label{sec:sensitivity_to_sampling}
%
%
%
Consider a pretraining DG $p=\BernMix(0.2, 0.7)$ and task DG $q=\mathrm{Bern}(0.7)$ ($q\in\mathcal{M}_p$).
The theoretical $s^*$ is always a sequence of all \ONEs, since it causes the posterior $p_{\tau|x}$ to collapse on 0.7 most rapidly. However, such a sequence is atypical to the task DG, and would be unintuitive without knowing $p$.
Can this prompt be identified? As \cref{fig:sensitivity}(left) shows, $\hat s$ is mostly correct for longer sequences when $\Lmax=5$. At $\Lmax=10$, the Transformer predictor has much lower proportion correct, even using very large optimization sets and long sequences. 
In addition, on the Bayes predictor, increasing sequence length $T$ can \emph{decrease} the odds of finding $s^*$, even for the Bayes predictor, which is counter-intuitive; see \Cref{sec:bernmix_bern_0.7_nonmonotonic} for an explanation.
In addition, \Cref{sec:bernmix_bern_0.7_landscape} shows a flat loss ``landscape': there are many suboptimal prompts with similar losses \eqref{eq:log_loss_L} as $s^*$. Thus, 
when minimizing the empirical version \eqref{eq:log_loss_L_est}, randomness in a particular draw of $\DqN$ can easily shift the optimum to a prompt different to the theoretical $s^*$.

Why does the proportion correct decrease? \Cref{fig:sensitivity}(right) shows example empirical $\hat{s}$ for $\Lmax=5$. As $N$ increases, the \emph{support} of the distribution of $\hat{s}$ approaches $s^*$, but the \emph{probability} of finding a suboptimal prompt may increase (e.g., yellow dots). \Cref{sec:sensitivity_emp_dist} presents the distribution of $\hat s$ from all predictors, showing non-convergence to $s^*$ with inconsistent counts and interpretations.

%
%
%
Note that the prompting strategy of using samples from the task $q\in\mathcal{M}_p$ is most likely suboptimal: sampling the all-\ONE sequence from the task DG is increasingly unlikely as T increases.
Knowledge of the pretraining DG is crucial to come up with and make sense of $s^*$.

%
%
%
%
%
%


%
\begin{figure*}[t]
%
    \centering
    \includegraphics[width=\textwidth]{figs/switching.pdf}
    \vspace{-1em}
    \caption{Results for a random switching pretraining DG and four switching downstream DGs with different causes $(\eps, \lambda)$ (rows). Left two columns, example theoretical prompts $s^*$'s (dots), the true switching latent bias $y$ (orange solid), and heuristic estimates of the latent $y$ based on $s^*$. 
    Middle three columns: the proportion correct of $\hat s$; \Cref{fig:switching_supp} shows additional results.
    Right two columns, estimated log-loss of prompting the Bayes predictor using typical prompts from $q$ of increasing length (blue line), compared to using the theoretical $s^*$ with length $15$ (red cross and dotted line)
    %
    %
    }
    \vspace{-1em}
    \label{fig:switching}
\end{figure*}


%
\subsubsection{Prompt length matters}\label{sec:ood_prompting}
%
In the previous example, the optimal prompt length was always maximal at $\Lmax$. Intuitively, a longer prompt provides more information about the task; is this true?
We take an OOMD case to show that, contrary to this intuition, longer prompts are not necessarily better. Let $p=\BernMix(0.2, 0.7)$ as before, and now $q=\Bern(0.6)$ ($q\notin\mathcal{M}_p$).
\Cref{fig:length_dependence}(left) shows that, as $T$ increases, $s^*$ can shorten or lengthen with no obvious pattern;
the ratio of \ONE appears irrelevant to $\tau=0.6$ in $q$.
To explain this, we examine the pretraining DG (\cref{thm:bernoulli-mixture}): its Bayes predictor needs to interpolate over an unevenly spaced values between $\tau_1=0.2$ and $\tau_2=0.7$ to match $0.6$, by adjusting the mixture weight $w_{L}$ \eqref{eq:mixture_weight_posterior}. However, the prompt's effect on $w_L$ is short-lived as samples from $q$ continues to update $w_L$.  
%
This causes a complex interplay between $T$ and the optimal prompt length. Hence, knowing the pretraining DG helps explain this pattern.
%
%
%
\Cref{fig:length_dependence}(right) shows that, for $\Lmax=5$, finding the theoretical $s^*$ on both the Bayes and LSTM predictor get more reliable as $N$ and $T$ increase, but not for $\Lmax=10$. This can be explained by the flatter loss ``landscape'' when $\Lmax\ge10$ (\cref{sec:bernmix_bern_0.6_landscape}); in addition, the distribution of $\hat s$ does \emph{not} concentrate around $\hat s$, with large discrepancies across predictors, as shown in \cref{fig:sensitivity_0.6_emp_dist}.
In this case, knowing the pretraining DG helps understand some patterns in $s^*$, but the unreliability of $\hat s$ is still unintuitive.
%



%
\subsubsection{Continuous latent factors}\label{sec:continuous_latent}
%
Some of the complexities of the previous example may result from the discrete $p_\tau$ that does not have full support over all coin biases.
%
We now consider a pretraining $p=\BetaBern(1, 1)$ (uniform over $(0, 1)$) and two tasks $q=\Bern(\tau)$ with $\tau\in\{0.7, 0.9\}$ ($q\in\mathcal{M}_p$). Intuitively, the theoretical $s^*_\Lmax$ should have roughly $\tau\Lmax$ \ONEs and $(1-\tau)\Lmax$ \ZEROs.
As \cref{fig:beta_categorical}(left) shows, this only holds for large enough $T$ and $\Lmax$.
%
%
The detailed mismatch for shorter $T$ can be explained if we know the pretraining $p$; see \cref{sec:betabern_bern_shorterT}.
However, the ratio of \ONE in $s^*$ deviates slightly from the true bias in $q$ if the $p_\tau$ is not uniform, 
such as when $p=\BetaBern(1, 2)$; see \cref{fig:beta_categorical}(left) and \cref{sec:betabern_bern_supp}.
%
%
For $p=\BetaBern(1,1)$, \Cref{fig:beta_categorical}(right) shows a close match in the proportion correct between all predictors, in constrast to previous cases. Overall, increasing $T$ and $N$ helps identify the theoretical $s^*$ more reliably, consistent with the sharper loss ``landscape'' in \cref{sec:betabern_bern_landscape}.
%
%

%
\subsubsection{Adaptive task}\label{sec:meta_learning}
%
In all previous examples, the task DG was a coin with fixed bias ($\Bern(\tau)$).
%
In \cref{sec:betabern_bernmix}, we present an example where $p=\BetaBern(1, 1)$ as before but now $q=\BernMix(\tau_1, \tau_2)$.
This task requires adaptation to one of the two components after the prompt. In short, the optimal prompt length varies depending on $q$, and knowing the pretraining DG helps explain overall patters of $s^*$ but not the details, similar to results in \cref{sec:ood_prompting}.
Also, the proportion correct increases as $N$ and $T$ increase (\cref{fig:beta_bernmix}), with slightly worse match between Bayes and neural predictors.
%


%
\section{Switching DGs and bandit task}\label{sec:further_experiments}
%
The summary \Cref{tab:opt_prompt_summary} suggests that IMD prompting tends to produce more intuitive prompts. We thus move to two additional IMD prompting scenarios, using DGs that are conditionally independent over \emph{segments} of tokens: a periodically switching source (thus non-i.i.d.), and a bandit task where we maximize the total reward.
Besides showing properties of optimal prompts, we also highlight their advantages over \emph{statistically typical} prompts: random samples or demonstrations from the task
%
%
\citep{luo2024context,xu2023expertprompting,WangPQLZWGGN00024,ruoss2024lmact,dai2024context}. 

%
\begin{figure*}[t]
%
    \centering
    \includegraphics[width=0.9\textwidth]{figs/OptimalPromptsBanditFigures.pdf}\\
    \vspace{1mm}
    \includegraphics[width=0.9\textwidth]{figs/bandit_results.pdf}
    \caption{Upper, experiment design for the bandit task. 
    %
    Lower, 4 equivalent theoretical $s^*$, 3 heuristic prompts, and performance metrics on the bottom panels: Left two panels, the proportion of correct
    %
    %
    empirical $\hat s$ for the Bayes and LSTM predictors; errorbars show 1 SEM over 100 pretraining runs with different weight initialization. Right two panels, the mean return and mean instantaneous regret of different prompts. We use the $\hat s$ found on $N=10^5$ rollouts. Grey dots show a subset of 30 pretraining runs.
    %
    %
    %
    }
    \label{fig:bandit}
\end{figure*}
%
\subsection{Switching DGs}\label{sec:switching}
%
The DG below switches periodically between two coins with fixed biases.
\begin{definition}[Switching Process]\label{thm:switching_process}
$\textrm{SwitchProc}(\eps, \lambda)$ for $\eps\in[0, 1]$ and $\lambda \in \mathbb{N}_+$ generates sequences by
\begin{equation*}
\begin{aligned}
     x_t   &\sim \Bernoulli(y_t) ~~ \forall t \in \{1,\ldots, T\}, \text{~~where}\\
    y_{1:T} &= [\underbrace{\eps, \ldots, \eps}_{\lambda ~ \eps\text{'s}},
                \underbrace{1-\eps, \ldots, 1-\eps}_{\lambda ~ (1-\eps)\text{'s} },
                \underbrace{\eps,\ldots,\eps}_{\lambda ~\eps\text{'s}},\ldots]
\end{aligned}
\end{equation*}
%
%
Here, the latent factor $\tau:=(\eps, \lambda)$. The pretraining DG $p$ is:
%
%
\end{definition}
\begin{definition}[Random Switching Process] This DG generates sequences by
first sampling 
$\eps \sim \textrm{Uniform}([0, 1])$ and
$ \lambda  \sim \text{Uniform}(\{3, 4, 5\})$,
then
$x_{1:T} \sim \text{SwitchProc}(\eps, \lambda)$.
\end{definition}
We take $\textrm{SwitchProc}(\eps, \lambda)$ with fixed values of $\eps$
and $\lambda$ as a task DG $q$, such that $q\in\mathcal{M}_p$. Examples of $\tau$ and $y$ are shown in \cref{fig:switching}(left). Prompting here is harder; for instance, a prompt that alternates between $\lambda$ \ZEROs and $\lambda$ \ONEs is very informative of $\lambda$, but not for $\eps$ if $\eps\notin\{0, 1\}$.
We set $\Lmax=15$ and search through $x_{1:15}\in\mathcal{A}^{15}$ and $x_{1:T}\in\mathcal{A}^T$.
%
%

For each of the four tasks in \cref{fig:switching}(left), we show the biases $y$ under the true $\tau$ (orange solid), and one example theoretical optimal prompts $s^*$'s (blue dots).
All $s^*$'s of each $q$ induce the same posterior over $\tau$,
%
%
so we show the latent $y$ corresponding to the mode of $p(\tau|s^*_{15})$ (the maximum a posteriori). This estimate for $\lambda$ is correct for all four $q$'s, and the estimate for $\eps$ is close to the ground-truth in $q$.
%
%
Thus, in this case, the true $\lambda$ in $q$ can be recovered from the theoretical $s^*$ if we know the pretraining $p$ and its Bayes predictor.
%
\Cref{fig:switching}(left) also presents a heuristic method given incomplete knowledge of $p$, producing a close estimate of the true bias sequence for $T=30$, but can make mistakes for $T=10$; this suggests again the importance of knowing $p$ fully to recover task latent factors. %
%
%
\Cref{fig:switching}(middle) shows the proportion correct of empirical $\hat{s}$. Under the Bayes predictor, increasing $N$ and $T$ leads to higher chances of finding $s^*$, more so for $T=30$ compared to $T=10$. %
This trend is weaker on the neural predictors.
%
%

Given the high cost of obtaining optimal prompts, how much do we gain compared to just using typical prompts? 
We compare them to statistically typical prompts (samples) drawn from $q$, which has expected log-loss 
$
    \mathbb{E}_{s_{1:L}\sim q}[\mathcal{L}(q, p, s_{1:L})].
$
This is nearly always higher than the log-loss \eqref{eq:log_loss_L} under the optimal prompt for the same prompt length, but can be lower if we use longer typical prompts.
\Cref{fig:switching}(right) shows the log-losses of $s^*_\Lmax$ at $\Lmax=15$ and typical prompts with lengths that are multiples of the true $\lambda$ in $q$. Typical prompts require 3-8 times the length of the optimal prompt to reach the same log-loss, and is thus much less efficient in conveying task information in terms of the number of tokens. Optimal prompts tend to be short making them suitable for context-efficient LLM-scale applications.
%


%
\subsection{Bandit decision-maker}
%

%

%
One of the most common approaches to in-context learning for solving a class of problems (e.g.\ chess games) is to prompt a pretrained LLM with expert demonstrations  of a few problems in this class \citep{ruoss2024lmact,dai2024context,laskincontext}, hoping that the LLM mimics the expert in new problems of the same class (a new chess game).
%
How effective is expert demonstration compared to the optimal prompt for a given problem class? What does the optimal prompt look like?
We empirically answer these questions using a two-arm Bernoulli bandit as the problem class, where the objective for the agent in this environment is to collect maximal expected total return.
\Cref{fig:bandit} shows the experiment design. The reward probabilities of the arms $\vL$ and $\vR$ are drawn i.i.d.\ from $\text{Uniform}([0, 1])$ at the beginning of each episode. These two reward probabilities specify the problem in the bandit class, but are unknown to the agent.
%
At time $t$, the agent chooses an action $a_t\in\{\text{L},\text{R}\}$ given past actions and rewards, and obtains reward $r_t\sim\Bernoulli(v_{a_t})$. This is repeated for a finite number of steps in each episode.
%
%

\vspace{-1em}
\paragraph{Policy mixture.} We design a distribution of agents with different skill levels $\tau$ (hidden from predictors), simulating the diverse skills in the text corpora of LLMs.
At time $t$, each agent maintains the counts of rewarded $S_{b,1,t}$ and unrewarded $S_{b, 0,t}$ actions on each arm ($b\in\{\text{L},\text{R}\}$) up to time $t$, and takes the action $a_{t+1}=\argmax_{b\in\{\text{L},\text{R}\}}\{v_{b,t,\tau}\}$,
%
where 
$
   v_{b,t,\tau} \sim \textrm{Beta}(\tau S_{b,1,t}+1, \tau S_{b,0,t}+1) \text{~for~} b\in\{\text{L},\text{R}\}.
$
Here, $\tau\in[0,1]$ is the latent skill, 
with $\tau=0$ giving uniformly random actions and $\tau=1$ corresponding to the optimal Thompson sampling (TS)
\citep{thompson1933likelihood,agrawal2012analysis}, see \cref{sec:bandit_details_skills} for details of the agent.
%
%
Agents with such mixture policy act in the bandit environment to generate trajectories of various skills.

\vspace{-1em}
\paragraph{Pretraining trajectories.}
Given a trajectory predictor, the purpose of a prompt is to induce a \emph{skill} level, but not to reveal specific reward probabilities $v$'s, to solve new problems of the bandit class where the $v$'s are unknown. 
%
So, we design each pretraining trajectory by concatenating a prompt segment and a rollout segment, separated by a special token (\cref{fig:bandit} and \cref{alg:bandit_pretrain}). The prompt segment consists of 8 actions and rewards by an agent of a randomly sampled skill. The rollout segment consists of 300 actions and rewards with the same skill but $v$'s redrawn.
%
The prompt is thus informative about the skill only. 
%
%
%
Samples of such trajectories are used to train neural predictor, with log-loss measured only on the action tokens. 
%
See \cref{sec:bandit_details_bayes} for details on our Bayes predictor. 
The predictor then acts according to the predicted probabilities at each action step.

%

%

\vspace{-1em}
\paragraph{Prompting methods.}
We prompt each predictor to maximize the return $\mathbb{E}[\sum_t{r_t}]$ in the rollout segment, from which we also estimate the instantaneous regret ($\mathbb{E}[\max_{a}v_a - v_{a_t}]$). 
%
We compare three prompting approaches.
First, we exhaustively search through all prompts on a predictor, running $N$ rollouts to estimate the return, giving $\hat{s}$.
%
Using a large $N$, we found four equivalent theoretically optimal prompts $s^*$ shown in \cref{fig:bandit}. Surprisingly, they show little exploration but strong exploitation to the rewarding arm. We address this unintuitive pattern in \cref{sec:bandit_details_optimal_prompts}, using knowledge of the pretraining distribution. 
%
%
%
Second, we prompt the model by demonstrations from the expert TS agent ($\tau=1$).
%
Finally, we handcraft several intuitive prompts: a ``maximally exploring'', a ``maximally exploiting'', and a ``heuristic'' prompt handcrafted by an author (\cref{fig:bandit}).
%
The latter two approaches cover common methods of in-context learning on LLMs.

%
%
\vspace{-1em}
\paragraph{Results.} \Cref{fig:bandit}(bottom left) shows the proportion correct of the empirical $\hat s$ found on the Bayes and LSTM predictors. It increases very slowly as $N$ increases, reaching arond $0.8$ when using 100k rollout sequences for each prompt evaluation. 
In
%
%
\Cref{sec:bandit_interp} show that the suboptimal prompts lead to different values of behavioral metrics, thus offering inconsistent interpretations.
%
The other panels of \cref{fig:bandit}(bottom) compare the performance of the three prompt types. Typical prompts from the TS expert and the ``heuristic'' prompt both yield lower return than the theoretically and empirically optimal prompts, and is even worse than the ``max. exploiting'' prompt. The ``max. exploring'' prompt gives the lowest return. These results show that the common practice of prompting by expert demonstrations do not induce expert behavior as well as the optimal prompt, or even the ``maximally exploiting'', which is surprising given that it exhibits little exploration.
%
%
%
The LSTM predictor largely resembles the Bayes predictor in both the return and regret, with only small discrepancies. Finally, \cref{sec:bandit_typical_prompt} shows that of the optimal prompts are on par in expected return with expert demonstrations of roughly 4 times the length. 


%
\section{Discussion and limitations}
%

%
%
%
%
%
%

Our experiments, while minimal, have revealed surprising patterns of optimal prompts even in conceptually simple settings.
Optimal prompts may be atypical sequences under the task distribution, vary with erratic patterns depending on the intended task, and be hard to find reliably in practice, resulting in discrepant interpretations of the optimal prompts. %
%
In addition, samples from the task distribution are less efficient at inducing the intended behavior than some unintuitive (but optimal) prompts.
%
Some of these issues occur even in the theoretically idealized Bayes predictor, and will thus persist at any scale, including that of LLMs. Pretrained neural predictors can exacerbate these issue and introduce additional complications.
%
%
%
Overall, these difficulties seem to stem from the opaque mechanism by which prompts influence the latent factors (the Bayes predictor); and the inaccessiblity of the pretraining distribution that shapes the predictor, except for its samples.
%
We elaborate on how our results relate to previous findings on prompting LLMs in \cref{sec:related_findings}. 


%
As with all empirical works, our results are limited by the settings we investigate.
%
We provided understanding through the tractable Bayes predictors enabled by the simplicity of the generative processes, which constitutes a drastic simplification from real text tokens.
%
Also, we interpreted prompts in terms of known latent features in the task, which may be obscure in real texts.
%
Moreover, the impact of post-pretraining fine-tuning remains unknown.
%
Therefore, our current empirical results thus do not allow us to make definitive claims about real LLMs. 
%
Nonetheless, we have presented a first systematic exploration towards understanding prompts in controlled settings.
%
We discuss detailed limitations and potential future work in \cref{sec:limitations}.



%

\clearpage
%
\section*{Impact Statement}
%

We have found that effective prompts can be very unintuitive, raising the possibilities that ill intentions of optimized prompts are inherently difficult to decipher by humans, causing safety concerns. Using exhaustive prompt search is tractable here, and the optimized prompt can be more efficient in token usage and thus reduce energy consumption, but it will be considerably more costly to find optimal prompts if applied to real language models. There are many additional potential societal consequences of our work, none which we feel must be specifically highlighted here. 
%
%
%

\bibliography{ref}


\clearpage
\onecolumn
\appendix

%
\section{Method details}
%

%
\subsection{Training neural predictors}\label{sec:neural_predictor_detail}
%

The neural predictors we build share the same overall structure involving the following stages:
\begin{enumerate}
    \itemsep0em
    \item Map the binary tokens $x_t\in\{0,1\}$ to embeddings $e_t\in\mathbb{R}^h$ through $e_t = W_\text{emb} x_t$ where $W_\text{emb}\in\mathbb{R}^{h\times 2}$, and $h\in\mathbb{N}_+$ is the hidden size.
    \item Sequentially map $h_{1:T}$ through some neural architecture, called the \emph{torso}, such as LSTM, multi-head attention, etc, to obtain some hidden activations $u_t\in\mathbb{R}^h$.
    \item For each $t$, map $u_t$ through the fully connected MLP to $v_t\in\mathbb{R}^h$ that is usually found after the attention layer in a transformer block \citep{vaswani2017attention}.
    \item For each $t$, map $v_t$ to output logits through a linear map.
\end{enumerate}
There is also a residual connection from step 2 to 3 and from 3 to 4.
The different neural architectures differ only by the torso. This maintains a flexible enough architecture for different tasks while controlling for the model complexity between different architectures.

For the torso, we use the following variants of recurrent networks and transformers:
\begin{enumerate}
    \itemsep0em
    \item Vanilla recurrent neural networks \citep{elman1990finding};
    \item Long-short term memory (LSTM) \citep{hochreiter1997long}, reported in main text;
    \item sLSTM \citep{beck2024xlstm}
    \item Softmax-attention transformer (Transformer) \citep{vaswani2017attention}, reported in main text;
    \item Linear transformer \citep{katharopoulos2020transformers}
    \item Another variant of Linear transformer we refer to as Inner-product transformer (IP transformer) \citep{li2020linear,shen2021efficient}
\end{enumerate}

We found that step 3 above is crucial for transformer architectures to perform some of the tasks, although this is not essential for LSTMs to perform well, so we leave this stage in for all model architectures. We did use normalization or dropout layers for simplicity. Transformer architecture requires more heads and layers for some tasks. These hyperparameters for all networks and pretraining DGs are listed in \cref{tab:hypers}
During pretraining, we make sure that the sequence length is long enough to avoid bad generalization over unseen sequence lengths \citep{deletang2022neural,anil2022exploring} when used for tasks.

Previous empirical work have shown that well-trained neural predictors behave almost identical to the Bayes predictor in terms of their predictions given data from the pretraining distribution $p$ \citep{mikulik2020meta,genewein2023memory,grau2024learning}. However, these results do not necessarily imply agreements between the optimal prompts given an intended task. Specifically, even though we have the optimality bound from positivity of the KL divergence:
$$
    \mathbb{E}_{s\sim p} [\log p_\theta(s)] \le \mathbb{E}_{s\sim p} [\log p(s)] \,,
$$
and also, for a sequence from $p$ that can be separated into segments $s$ and $x$,
$$
    \mathbb{E}_{sx\sim p} [\log p_\theta(sx)] \le \mathbb{E}_{sx\sim p} [\log p(sx)]\, ,
$$
these do \emph{not} imply that the prompted distribution would agree, i.e.\
$$
    \mathbb{E}_{[sx]\sim p} [\log p_\theta(x|s)] \not\le \mathbb{E}_{[sx]\sim p} [\log p(x|s)] = \mathbb{E}_{[sx]\sim p} [\log \pbayes(x|s)] \,.
$$
This is trivially due to the fact that inequalities are not preserved under subtraction.
%
In fact, it is possible that the neural predictor produces a \emph{higher} conditional log-likelihood than the Bayes predictor. This can happen if $p_\theta$ is more suboptimal on the shorter conditioning sequence $s$ than on the whole sequence $sx$. For example, we observe this on the periodic switching dataset.

In addition, even if the inequalities hold between the conditional log-likelihoods, we do not expect that neural predictors would produce the same optimal prompts at all as the Bayes predictor. This is because the gap between the conditionaly likelihoods may not be consistent under different prompts. The optimal prompt under one predictor may not be optimal under another. Therefore, an empirical approach to studying prompting is valuable.



%
\begin{table}
%
\centering
\caption{Hyperparameters for neural predictors. $^*$For vanilla RNN, we train 5 m steps. \label{tab:hypers}}
\begin{tabular}{l|ccc|c|ccc}
               & \multicolumn{3}{c|}{\textbf{Common parameters}} & \textbf{Recurrent}              & \multicolumn{3}{c}{\textbf{Transformer}}        \\
\textbf{Pretraining DG} & hidden size $h$             & \# steps     &   seq length $T$    & learning rate     & \# head & \# layer & learning rate     \\
\hline
BernMix        & 128             & 300 k      &  180    & $1\times 10^{-4}$         & 1       & 1        & $1\times 10^{-4}$         \\
BetaBern       & 128             & 500 k$^*$  &  180    & $1\times 10^{-4}$         & 1       & 1        & $1\times 10^{-4}$         \\
Switching      & 128             & 3 m        &  180    & $3\times 10^{-5}$         & 8       & 1        & $3\times 10^{-5}$ \\
Bandit         & 256             & 1 m        &  630 (300 actions)    & $1\times 10^{-4}$         & 8       & 2        & $3\times 10^{-5}$
\end{tabular}
\end{table}


%
\subsection{Prompt search method}\label{sec:prompt_search}
%

On the CIB-DG experiments \cref{sec:CIB-DG_methods}, both the prompt $s$ and the sequence(-to-predict) $x$ are permutation invariant under the Bayes predictor $\pbayes$, which means that the counts of \ZERO and \ONE \cref{eq:counts} form sufficient summary of both objects under $\pbayes$. This drastically reduces the search space of the theoretically optimal prompt on the Bayes predictor. Also, by using the equivalent distribution defined over the counts based on the binomial distribution, the summation in \cref{eq:log_loss_L} can also be reduced for the Bayes predictor. This makes searching for $s^*(\pbayes, \cdot)$ and $\hat s(\pbayes, \cdot, \cdot)$ very efficient. However, these savings do not apply to neural predictors, so we search through all possible prompts and sequences.

On the other DGs in \cref{sec:further_experiments}, there is no trivial symmetry in the space of all prompts and all sequences/rollouts, so again we have to exhaustively search through the spaces of all possible prompts and sequences/rollouts to find $s^*$. We also search through all possible prompts on the given task dataset to find $\hat s$.

%
\subsection{Information-theoretic justification}\label{sec:info}
%
We show that prompt optimization over the objective in \cref{eq:log_loss_L} is equivalent to maximizing an information-theoretic objective defined over a pretrained predictor and a prompting policy, for the case $q\in\mathbb{M}_p$.
In this subsection only, to reduce notational clutter and avoid specifying $T$ and $L$, we temporarily define $\vx$ as the sequence and $\vs$ as the prompt. The latent variable $\tau$ is not restricted to a scalar. We \emph{do} still assume that $\vx$ is conditionally independent of $\vs$ given $\tau$.
The pretraining DG $p(\vx):=\int p_\tau(\tau)P(\vx|\tau) \ud \tau$ is as defined in \cref{eq:joint}.
Denote the pretrained predictor by $\bar{p}(\vx|\vs)$, which could be the Bayes predictor $p(\vx|\vs)$ or a pretrained neural predictor.
In addition, define the \emph{prompting strategy} has $\nu(\vs|\tau)$, which maps a given $\tau$ to a distribution of prompts. Together with the pretraining DG, we can define a joint distribution that relates $\vs$ and $\vx$ by 
\begin{equation}\label{eq:joint_nu_vec}
p^\nu(\vs, \vx):= \int p_\tau(\tau)\nu(\vs|\tau)P(\vx|\tau)\ud \tau.
\end{equation}
Note that the prompt distribution $\nu$ need not be the same as $P(\vx|\tau)$. By construction, the unprompted distribution under $p^\nu$ is identical to the pretraining data distribution, i.e.\ $p^\nu(\vx)=p(\vx)$.

What would be a best prompting strategy? First, there should a high mutual information between $\vx$ and $\vs$ under $p^\nu$, so that varying $\vs$ effectively manipulates the distribution over $\vx$. Second, the prompt must condition the pretrained predictor in a similar way as to how it conditions the pretrained DG, i.e.\ $\bar{p}(\vx|\vs)$ must be close to $p(\vx|\vs)$. Combine these ideas together, we propose the following objective.
\begin{definition}
The model-aware mutual information (MAMI) is defined as
\begin{equation}\label{eq:mami}
\MAMI(\vs, \vx):= \MI_{p^\nu}(\vs; \vx) - \mathbb{E}_{\vs\sim p^\nu}[\KL[p^\nu(\vx|\vs)\|\bar{p}(\vx|\vs)]]\,.
\end{equation}
\end{definition}
This objective trades off the identifiability of $\vx$ through $\vs$ under $p^\nu$ and the alignment between conditioning on $p^\nu$ and on $\bar{p}$. MAMI can be rewritten into a form similar to the conventional mutual information:
\begin{align*}
\MAMI(\vs, \vx) &=\sum_{\vs, \vx} p^\nu(\vs,\vx)\frac{p^\nu(\vx|\vs)}{p^\nu(\vx)} - \sum_{\vs,\vx}p^\nu(\vs,\vx)\log\frac{p^\nu(\vx|\vs)}{\bar{p}(\vx|\vs)} =\sum_{\vs, \vx} p^\nu(\vs,\vx)\log\frac{p(\vx|\vs)}{p^\nu(\vx)} \\
&= H[p(\vx)] + \sum_{\vs, \vx}  p^\nu(\vs,\vx)\log p(\vx|\vs).\numberthis\label{eq:mami_mi_form}
\end{align*}
The last equality uses the fact that $p^\nu(\vx)=p(\vx)$. The second term in \cref{eq:mami_mi_form} is not a conditional entropy as in a conventional mutual information.
\begin{proposition}
The deterministic prompt distribution $\nu(\vs|\tau)=\delta_{\vs^*(\tau)}$ centered at 
$$
\vs^*(\tau):=\argmax_\vs \sum_\vx p(\vx|\tau)\log \bar{p}(\vx|\vs)
$$
for all $\tau$ maximizes $\MAMI(\vs;\vx)$.
\end{proposition}
\begin{proof}
The first term in \cref{eq:mami_mi_form} is independent of $\nu$, so we only need to show that $\delta_{\vx^*(\tau)}$ maximizes the second term. This term expands to
$$
\int p_\tau(\tau)\sum_\vs \nu(\vs|\tau)\sum_\vx p(\vx|\tau) \log \bar{p}(\vx|\vs) \ud \tau\,.
$$
The Dirac delta measure $\delta_{\vx^*(\tau)}$ assigns all mass to $\vs^*(\tau)$ that maximizes $\sum_\vx p(\vx|\tau)\log p(\vx|\vs)$. This holds for all $\tau$, and thus $\delta_{\vx^*(\tau)}$ maximizes the second term of \cref{eq:mami_mi_form}.
\end{proof}


%
\subsection{Non-uniquenss of optimal prompt and criterion for an empirically optimal prompt to be correct}\label{sec:proportion_correct}
%
For each prompt setup in our experiment, we found multiple theoretical $s^*$'s that give the same expected log-loss \eqref{eq:log_loss_L}.
For CIB-DGs, we found empirically that the counts in the theoretical $s^*$ are unique.
As such, an empirically optimal prompt for CIB-DG experiments, particularly those found on neural predictors, is deemed correct if it has the same counts \cref{eq:counts} as the counts in the theoretically optimal prompt. 
%
For other DGs, correctness requires exact match between $\hat s$ and any one of the theoretical $s^*$'s.

To estimate the proportion correct, we train a large number of networks with different random seeds, find the empirically optimal prompt $\hat s$ for each network using $\DqN$ drawn with another different seed, and then calculate the empirical ratio of correct $\hat s$ out of all prompts found on all networks.


%
\subsection{The loss ``landscape'' of Bayes predictor}\label{sec:loss_landscape}
%
Why is it difficult to identify the optimal prompt with a finite task dataset? We hypothesize that this is because there are many suboptimal prompts that are only slightly worse compared to the optimal one in terms of the expected log-loss \eqref{eq:log_loss_L}. To test this, we take the Bayes predictor for each prompt setup, compute expected log-loss given all possible prompts with length $L\le\Lmax$. We also subtract this log-loss with the best possible log-loss, giving the Kullback--Leibler divergence
\begin{equation}\label{eq:kl_divergence}
\text{KL}[q(\cdot)\| p(\cdot|s_{1:L})] := \mathcal{L}(p_{(\cdot)}, q, s_{1:L}) - \mathcal{L}(q, q, s_{1:L}).
\end{equation}
If many prompts yield KL divergences close to the optimal KL divergence (under $s^*$), then it would be less likely that the order of these prompts are preserved when evaluated under finite dataset, and even less so on approximate neural predictors. 

Since the prompts are discrete with no obvious order, we show the KL divergences of all prompts, sorted in increasing order. Each prompt is than associated with a rank. To see if the optimal prompt has a distinctively smaller KL divergence than other prompts, we plot the KL divergences against the rank of the prompt. 

For CIB-DGs, the prompts expressed in the counts $(S_0(s_{1:L}), S_1(s_{1:L}))$ are ordered, so we show the prompt rank by their counts, and also plot the loss ``landscape'': the KL divergence as the color of each dot representing the counts. We show these results for each of the four CIB-DG experiments below.

\clearpage
%
\section{Additional Experimental Results}\label{sec:additional_results}
%

For each pretraining DG except the bandit DG, we train 30 networks with different random seeds for the weight initialization and minibatch sampling. 
For the bandit DG, we train 100 networks. Below, we report, for each pretraining DG, the estimated KL divergence between $p_x$ and $p_\theta$ using samples from $p_x$, computed as 

\begin{equation}\label{eq:pretrain_kl}
\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \log \left(\frac{\pbayes(x_t|x_{1:t-1})}{p_\theta(x_t|x_{1:t-1})} \right).
\end{equation}
The training sequence length and minibatch size for each pretraining DG $p$ are as shown in \cref{tab:hypers}. In most cases, the network achieved near-zero KL divergence, consistent with previous findings \citep{mikulik2020meta,genewein2023memory,grau2024learning}.
Under the bandit DG, we use an approximate Bayes predictor described in \cref{sec:bandit_details_bayes}. We see that the LSTM and Transformer outperform this predictor by a small but statistically significant amount.

\begin{figure}[ht]
    \centering
    \includegraphics[height=0.193\textheight]{figs/pretrain_loss_bernmix.pdf}\hfill
    \includegraphics[height=0.19\textheight]{figs/pretrain_loss_betabern.pdf}\hfill
    \includegraphics[height=0.19\textheight]{figs/pretrain_loss_switching.pdf}\hfill
    \includegraphics[height=0.19\textheight]{figs/pretrain_loss_bandit.pdf}
    \caption{Final pretraining loss under various DGs $p$ and neural predictors. Each dot is an Monte Carlo estiamte of the KL divergence through \cref{eq:pretrain_kl} in the last training step.}
    \label{fig:my_label}
\end{figure}



%
\subsection{Pretraining on \texorpdfstring{$\BernMix(0.2, 0.7)$}{Bern(0.2, 0.7)}, prompting towards \texorpdfstring{$\Bern(0.7)$}{Bern(0.7)} (IMD)}\label{sec:bernmix_bern_0.7_supp}
%

\subsubsection{Proportion correct}\label{sec:bernmix_bern_0.7_correct}

\Cref{fig:sensitivity_supp} shows the proportion correct results for the task DG $q=\Bern(0.7)$, so $q\in\mathcal{M}_p$. The recurrent networks show similar behaviors to the Bayes predictor, and the Transformers had worse proportion correct at $T=100$ even for a short prompt with length up to $\Lmax$. The proportion correct is lower for longer prompts for all predictors, likely because the prompt of all \ONEs requires perfect match on each token and is more difficult to identify exactly when the maximum prompt length is longer. 

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=\textwidth]{figs/sensitivity_correct_supp.pdf}
    \caption{
    The proportion correct of the empirically optimal prompt for 
    $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.7)$.
    Same as \cref{fig:sensitivity} but with more prompting setups.}
    \label{fig:sensitivity_supp}
\end{figure}

%
\begin{table}[ht] %
%
    \centering
    \caption{Table of prompts, of length up to $\Lmax=5$, the has the top 10 values in the posterior belief $p(x_1|s)$ under the pretraining DG $\BernMix(0.2, 0.7)$. \label{tab:tau_hat}}
    \begin{tabular}{l|*{11}{c}}
        \multicolumn{1}{c|}{} & \multicolumn{11}{c}{Prompt counts} \\
        \hline
        $S_0(s)$ & 0 & 0 & 1 & 0 & 1 & 0 & 2 & 1 & 0 & 2 & 1 \\
        $S_1(s)$ & 5 & 4 & 4 & 3 & 3 & 2 & 3 & 2 & 1 & 2 & 1 \\
        $p(x_1=1|s)$ & 0.699 & 0.697 & 0.691 & 0.689 & 0.671 & 0.662 & 0.629 & 0.611 & 0.589 & 0.516 & 0.484 \\
    \end{tabular}
\end{table}

%
\subsubsection{Non-monotonicity of proportion correct versus \texorpdfstring{$N$}{N}}\label{sec:bernmix_bern_0.7_nonmonotonic}
%

Let us gain some insight to this phenomenon by considering the case of $T=1$. Here, we can compute theoretically the probability of an empirical prompt being correct and predict this trend as the black dotted line. The short reason is that the task dataset may have an \emph{empirical} ratio of \ONEs below 0.7, which happens with nonzero probability even though the task DG is $\Bern(0.7)$.
When this happens, this dataset may be better explained by a mixture of Bernoulli that has nonzero weight on the component with $\tau_1=0.2$, giving a prediction that is close to the empirical ratio. Such a mixing weight can be induced by a prompt other than the all-\ONE prompt; see \cref{tab:tau_hat} for example prompts with lengths up to $\Lmax=5$. Note that some of the prompt counts in \cref{tab:tau_hat} appear in the clusters shown in \cref{fig:sensitivity}(right, blue dots). At $N=10$, if the empirical mean in the task dataset takes on values of 0.6 or 0.5, the best prompts to induce such biases have counts, respectively, (1 \ZERO \& 2 \ONEs) or (1  \ZERO \& 1 \ONE). Such datasets are responsible for the clusters of empirical prompts. 


More precisely, for $T=1$ we can compute the probability that $\hat{s}(\pbayes, \DqN)= s^*(\pbayes,q)$ given a randomly drawn task dataset $\DqN$ from $q=\Bern(0.7)$. 
Recall that the log-loss on the first token following a prompt $s$ is 
\begin{equation}\label{eq:length_1_seq_likelihood}
\begin{gathered}
\hat{\mathcal{L}}(\pbayes, \DqN, s) = -N\hat\tau(\DqN) \log(\bar\tau(s)) - N(1-\hat\tau(\DqN)\log(1-\bar\tau(s))\,,~\text{where}\\
\hat\tau(\DqN) := \frac{1}{N}\sum_{n=1}^N x_1^n\,,  \quad \bar\tau(s):=\pbayes(x_1=1|s)=(1-w_L(s))\tau_1 + w_L(s) \tau_2\,,
\end{gathered}
\end{equation}
and $w_L(s)$ is given in \cref{thm:bernoulli-mixture}.
%
For prompts of length up to $\Lmax$, it is easy to see that $\hat{s}(\pbayes, \DqN)=s^*(\pbayes,q)$ if and only if, under the Bayes predictor $\pbayes$, the optimal prompt $s^*$ with $\Lmax$ \ONEs gives a lower log-loss than the prompt $s^+$ that has $0$ \ZERO and $(\Lmax-1)$ \ONEs (see \cref{tab:tau_hat} for an example when $\Lmax=5$). Then we have, 
$$
\mathbb{P}(\hat{s}=s^*) = \mathbb{P}( \hat{\mathcal{L}}(\pbayes, \DqN, s^*)< \hat{\mathcal{L}}(\pbayes, \DqN, s^+)).
$$ 
Substituting in \cref{eq:length_1_seq_likelihood} and after some manipulation, we get
\begin{equation*}
\begin{gathered}
\mathbb{P}(\hat{s}=s^*) =
    \mathbb{P}\left(N\hat\tau(\DqN) > \kappa\right)\,,~\text{where}~
    \kappa := N\log\left(\frac{1-\tau^+}{1-\tau^*}\right)
        \bigg/
        \log\left(\frac{\tau^*(1-\tau^+)}{\tau^+(1-\tau^*)}
    \right)
\end{gathered}
\end{equation*}
Noting that $N\hat\tau(\DqN)$ is a binomial distribution $\text{Binom}(0.7, N)$, we can easily find $N\hat\tau(\DqN)$ using its cumulative distribution function. For sequence length $T>1$, the loss becomes more complicated, so is its dependence on $\bar\tau$.

\def\landscapelegend{The leftmost column shows the KL divergence of each prompt sorted in increasing order against the prompt rank (sort indices, or argsort), with lower rank meaning lower KL divergence \eqref{eq:kl_divergence}. The other columns show KL divergence of each individual prompt. The prompts here are all expressed by their counts. See \cref{sec:loss_landscape} for a detailed explanation.
}
%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[height=0.33\textheight]{figs/bernmix_bern_0.7_prompt_rank.pdf}
    \includegraphics[height=0.33\textheight]{figs/bernmix_bern_0.7_prompt_tree.pdf}
    \caption{
    The loss ``landscape'' of $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.7)$. \landscapelegend
    \label{fig:sensitivity_landscape}
    }
\end{figure}


%
\subsubsection{Loss ``landscape''}\label{sec:bernmix_bern_0.7_landscape}
%

We show the KL divergence of all possible prompts in two ways in \cref{fig:sensitivity_landscape}. On the leftmost column, we order the prompts (expressed in counts) according to their values of the KL divergence, producing a rank plot of in ascending order. The best prompt is at rank 0, second prompt at rank 1, etc.
We see that there is a relative flat region next to the best prompt with $rank 0$, suggesting that it would be more difficult to distinguish the best few prompts.
As $\Lmax$ increases and $T$ decreases, the flat region expands towards the right, meaning that there are more close-to-optimal prompts. This trend is consistent with the results shown in \cref{fig:sensitivity_supp}. The other columns of \cref{fig:sensitivity_landscape} show KL divergence of all possible prompts with length less than $\Lmax$. It is evident that the many prompts around the optimal one that have very similar KL divergences close to zero. 

%
\subsubsection{Distribution of empirically optimal prompts}\label{sec:sensitivity_emp_dist}
%

\Cref{fig:sensitivity_emp_dist} shows the distribution of empirically optimal prompts for different neural predictors. We show the setting of $N=10\,000$ and $T=100$, which is the most reliable setting. At the shortest $\Lmax=5$, the empirical $\hat s$ is mostly correct. At larger $Lmax$, the empirical $\hat s$ gets further away from $s^*$ with inconsistent ratio of \ONE, leading to unreliable identification worse interpretability of the scattered empirical $\hat s$'s.

\def\theempdistcaption{Red cross shows the theoretical $s^*$. The black dashed line shows the boundary set by $\Lmax$.}

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=\textwidth]{figs/bernmix_bern_0.7_empirical_prompt_dist.pdf}
    \caption{
    The distribution of the prompt counts for $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.7)$ for different neural predictors. We pick the most reliable prompt setup of $N=10\,000$ and $T=100$.
    \theempdistcaption
    }
    \label{fig:sensitivity_emp_dist}
\end{figure}


\clearpage

%
\subsection{Pretraining on \texorpdfstring{$\BernMix(0.2, 0.7)$}{BernMix(0.2, 0.7)}, prompting towards \texorpdfstring{$\Bern(0.6)$}{Bern(0.6)} (OOMD)}\label{sec:bernmix_bern_0.6_supp}
%

%
\subsubsection{Proportion correct}
%

\Cref{fig:sensitivity_0.6_supp} extends the results in \cref{fig:sensitivity}. For $Lmax=5$, the Transformer predictors have lower proportion correct than other predictors when $T=100$. At $\Lmax=10$ and $\Lmax=15$, all neural predictors got worse than the Bayes predictor when $T$ reaches $30$; and all predictors, including the Bayes predictor, failed to identify $s^*$ when $T=100$.

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=\textwidth]{figs/sensitivity_0.6_correct_supp.pdf}
    \caption{
    The proportion correct of empirically optimal prompt for 
    $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.6)$.
    Same as \cref{fig:length_dependence} but with more prompting setups.}
    \label{fig:sensitivity_0.6_supp}
\end{figure}

%
\subsubsection{Distribution of empirically optimal prompts}
%
\Cref{fig:sensitivity_0.6_emp_dist} shows the distribution of empirically optimal prompts for different neural predictors. 
Here, when the maximum prompt length $\Lmax=5$, the empirical $\hat s$'s are mostly correct. At larger $Lmax$, some empirical $\hat s$ are close to the $s^*$, but there is a cluster of empirical $\hat s$ at around (5 \ZEROs and 10 \ONEs for $\Lmax=15$), which is quite far away from the theoretical $s^*$. In this case, even knowing the pretraining distribution does not explain the existence of this cluster, as the Bayes predictor only has a cluster at the all-\ONE prompt.


%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=\textwidth]{figs/bernmix_bern_0.6_empirical_prompt_dist.pdf}
    \caption{
    The distribution of the prompt counts for $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.6)$ for different neural predictors. We pick the most reliable prompt setup of $N=10\,000$ and $T=100$.
    \theempdistcaption
    }
    \label{fig:sensitivity_0.6_emp_dist}
\end{figure}



%
\subsubsection{Loss ``landscape''}\label{sec:bernmix_bern_0.6_landscape}
%
\Cref{fig:sensitivity_0.6_landscape} shows the ``loss landscape'' of prompts on the Bayes predictor. In addition to the flat landscape similar to \cref{fig:sensitivity_landscape}, the KL divergence does not go towards zero, which is a sign of OOMD prompting. Thus, the optimal prompts are harder to identify, and the best prompt does not improve the performance much.

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[height=0.33\textheight]{figs/bernmix_bern_0.6_prompt_rank.pdf}
    \includegraphics[height=0.33\textheight]{figs/bernmix_bern_0.6_prompt_tree.pdf}
    \caption{The loss ``landscape'' of $p=\BernMix(0.2, 0.7)$ and $q=\Bern(0.6)$.\landscapelegend
    \label{fig:sensitivity_0.6_landscape}}
\end{figure}


\clearpage
%
\subsection{Pretraining on \texorpdfstring{$\BetaBern$}{BetaBern}, prompting towards \texorpdfstring{$\Bern$}{Bern} (IMD)}\label{sec:betabern_bern_supp}
%

\subsubsection{Proportion correct}

\Cref{fig:beta_categorical_supp} extends the results in \cref{fig:beta_categorical}. In almost all prompt setups, the all neural predictors show very similar trends compared to the Bayes predictor. 
%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=\textwidth]{figs/beta_categorical_correct_supp_0.7.pdf}\\
    \vspace{0.5cm}
    \includegraphics[width=\textwidth]{figs/beta_categorical_correct_supp_0.9.pdf}\\
    \caption{Proportion correct of $\hat s$. Same as \cref{fig:beta_categorical} but with more prompting setups.}
    \label{fig:beta_categorical_supp}
\end{figure}

\subsubsection{Distribution of empirically optimal prompts}
As \cref{fig:beta_categorical_emp_dist} shows, the empirically optimal prompts are highly likely correct, suggesting that the distribution is very concentrated at $s^*$. To show how the empirical $\hat s$ differ when they do, we show the distribution of $\hat s$ for the setting $q=\Bern(0.7)$, $N=100$ and $T=3$ in \cref{fig:beta_categorical_emp_dist}. Unlike in previous case, the prompt distribution is still very close to $s^*$, giving more or less the same ratio of \ONE and hence consistent interpretation.


%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=\textwidth]{figs/betabern_bern_0.7_empirical_prompt_dist.pdf}
    \caption{
    The distribution of empirically optimal prompt for $p=\BetaBern(1,1)$, $q=\Bern(0.7)$, $N=100$, $T=3$ for different neural predictors. \theempdistcaption
    \label{fig:beta_categorical_emp_dist}
    }
\end{figure}





\subsubsection{Effect of the prior at shorter sequence length}\label{sec:betabern_bern_shorterT}

The mismatch at shorter $T$ is due to the uniform prior $\Beta(1,1)$ having pseudocounts of 1 \ONE and 1 \ZERO, which regularizes the predicted bias towards 0.5 by adding 1 to each counter. For shorter $T$, say $T=1$, it is important to get the ratio correct after this regularization, since the Bayes optimal predictor gives $p(x_1=1|s)=(S_1(s) + 1) / (S_0(s) + S_1(s) + 2)$. If we added 1 to each count, then the regularized ratio is closer to the true bias. To reduce the gap between the regularized ratio, the optimal prompt length under the upper limit $\Lmax$ may be less than $\Lmax$. For example, for $\Lmax=10$ and $q=\Bern(0.7)$, the optimal prompt is 2 \ZEROs and 6 \ONEs and has length 8 (see \cref{tab:betabern_seq_len}); the regularized ratio is then $(6+1) / (2 + 6 + 2) = 0.7$, which is optimal for predicting the next $x_1$.

However, as more tokens arrives, the regularized ratio fluctuates due to the randomness in the draws from $\Bern(0.7)$, while ideally it should be fixed at 0.7. Hence, there is additional benefit from being certain about the bias (with larger counts), which can be gained by using more tokens in the prompt. For the same example of $\Lmax=10$ and $q=\Bern(0.7)$, but now $T=10$, the optimal prompt becomes 3 \ZEROs and 7 \ONEs \cref{tab:betabern_seq_len}. There will be a small cost for predicting earlier $x_t$'s, as the regularized ratio is initially not exactly 0.7.

\subsubsection{Non-uniform latent factor}


%
\begin{table}%
%
\centering
\caption{Example optimal prompts $s^*_\Lmax$ for $p=\BetaBern(1, \beta)$ and $q=\Bern(0.7)$ of various values of 
$\beta$, $T$ and $\Lmax$.}%
\label{tab:betabern_seq_len}
\begin{tabular}{lccccc}
\hline
\multicolumn{2}{c}{SDs $p$ and $q$} & \multicolumn{4}{c}{Optimal prompt ${s^*_\Lmax}$} \\
$\beta$    & $T$              & $L_{\text{max}}$         & ($S_0$, $S_1$)   &  $S_0$+ $S_1$ & $S_1/(S_0+S_1)$      \\
\hline
1           & 1                & 3                        & (0, 1)          &  1            &  1.00    \\
1           & 3                & 3                        & (0, 2)          &  2            & 1.00   \\
1           & 5                & 3                        & (1, 2)          &  3            & 0.67   \\
1           & 1                & 10                       & (2, 6)          &  8            & 0.75  \\
1           & 10               & 10                       & (3, 7)          &  10            & 0.70  \\
1           & 3                & 12                       & (3, 8)          &  11           & 0.72 \\
1           & 5                & 12                       & (3, 8)          &  11           & 0.72 \\
\hline
2           & 10                & 10                       & (2, 8)         &  10           & 0.8  \\
2           & 100               & 20                      & (5, 15)         &  20           & 0.75  \\
2           & 100               & 50                       & (14, 36)       &  50           & 0.72     \\
\hline
\end{tabular}
\end{table}

In \cref{tab:betabern_seq_len}, we show the optimal prompts on a few prompt setups. When $\beta=1$ and $p_\tau$ is uniform, the optimal prompt contains roughtly the correct proportion of \ZEROs and \ONEs even for a short task sequence length $T$, converging to the true bias in $q$ as $\Lmax$ increases. When $\beta=2$ and thus $p_\tau$ is biased towards zero, the optimal prompts have to debias the prior, and thus the empirical ratio of \ONEs is further away from the ground truth bias 0.7, and requires larger $T$ and $\Lmax$ to converge to 0.7.  


\subsubsection{Loss ``landscape''}\label{sec:betabern_bern_landscape}

\Cref{fig:betabern_bern_landscape}(left) shows that the theoretical optimal prompt has a distinctively lower KL divergence compared to other prompts, wit hthe exception that $T=1$ still has a very flat landscape. The other columns show a clearer optimal region, especially for $\tau=0.7$. These are in stark contract to \cref{fig:sensitivity_landscape,fig:sensitivity_0.6_landscape} where the optimal points do not standout among other suboptimal prompts.

%
\begin{figure}[!th]
%
    \centering
    \includegraphics[height=0.33\textheight]{figs/betabern_bern_0.7_prompt_rank.pdf}
    \includegraphics[height=0.33\textheight]{figs/betabern_bern_0.7_prompt_tree.pdf}\\
    \vspace{1cm}
    \includegraphics[height=0.33\textheight]{figs/betabern_bern_0.9_prompt_rank.pdf}
    \includegraphics[height=0.33\textheight]{figs/betabern_bern_0.9_prompt_tree.pdf}\\
    \caption{The loss ``landscape'' of $p=\BetaBern(1, 1)$ and $q=\Bern(0.7)$ or $q=\Bern(0.9)$.\landscapelegend %
    \label{fig:betabern_bern_landscape}}
\end{figure}



\clearpage



%
\subsection{Pretraining on \texorpdfstring{$\BetaBern$}{}, prompting towards \texorpdfstring{$\BernMix$}{} (OOMD)}\label{sec:betabern_bernmix}
%

%
\begin{figure*}[ht] %
%
\centering
\begin{minipage}{0.38\textwidth}
\centering
\begin{tabular}{cccc|cc}
\hline
\multicolumn{4}{c|}{\textbf{$q=\BernMix(\tau_1,\tau_2)$}} & \multicolumn{2}{c}{\textbf{$s^*_{100}$}} \\
$\tau_1$    & $\tau_2$  & $\Delta\tau$  & $\bar{\tau}$   & ($S_0$, $S_1$)       & ratio  \\
\hline
1/2         & 1/3       & 1/6           & 0.42                  & (20, 14)      & 0.41      \\
2/5         & 3/5       & 1/5           & 0.50                  & (11, 11)      & 0.50      \\
1/5         & 2/5       & 1/5           & 0.30                  & (13, 5)       & 0.28      \\
1/4         & 1/2       & 1/4           & 0.38                  & (9, 5)        & 0.36      \\                  
1/3         & 2/3       & 1/3           & 0.50                  & (3, 3)        & 0.50      \\
1/4         & 3/4       & 1/2           & 0.50                  & (1, 1)        & 0.50      \\
1/5         & 4/5       & 3/5           & 0.50                  & (0, 0)        & -         \\
2/5         & 1         & 3/5           & 0.70                  & (0, 1)        & 0.70\\
\hline
\end{tabular}
\captionsetup{type=table}
\caption{Empirically optimal prompts for ${p=\BetaBern(1,1)}$ and several task DGs ${q=\BernMix(\tau_1, \tau_2)}$. For each case, the optimal $s_\Lmax^*$ is found for large ${\Lmax=100}$ and ${T=100}$. ${\Delta\tau:=(\tau_2-\tau_1)}$, and ${\bar\tau:=(\tau_1+\tau_2)/2}$.
\label{tab:beta_bernmix_optimal}
 }
\end{minipage}\hfill
\begin{minipage}{0.58\textwidth}
    \includegraphics[width=\textwidth]{figs/beta_mixture.pdf}
    \caption{Proprotion of empirical prompts that match the theoretically optimal prompt for two of the task DGs in \cref{tab:beta_bernmix_optimal}. The prompt length is capped at $\Lmax=15$, which is sufficient for the empirically optimal prompt in both cases. \Cref{fig:betabern_bernmix_supp} shows additional results. %
    \label{fig:beta_bernmix}
    }
\end{minipage}
\end{figure*}

The difficulty with this mixture task $q$ is that it is impossible to provide a single input sequence (the prompt) that leads to a bimodal $p_{\tau|x}$. Thus, while each component of the task mixture is IMD, the mixture itself is OOMD.
The theoretically optimal prompts $s^*_\Lmax$ for different instance of such $q$, under a large prompt length limit $\Lmax=100$, are shown in \cref{tab:beta_bernmix_optimal}. Although the empirical ratio reflects the mean bias of $q$ well, it is still unclear why some prompts are longer and some shorter. Given just the information about $q$, it is difficult to make sense of the varying optimal prompt length

If we know $p$ fully, then this pattern makes more sense from a posterior concentration perspective. If $\tau_1$ and $\tau_2$ are close together, a helpful prompt should loosely concentrates the posterior around these values. The closer the two values, the longer thus the optimal prompt (with a ratio of \ONEs close to the mean of the two mixture components). On the other hand, if the two values are far apart, a helpful prompt leaves the posterior as broad as possible, leading to very short prompts or no prompt at all.

The empirical prompts found on neural predictors are more likely to be correct for larger $T$ and larger $N$ (\cref{fig:beta_bernmix}), similar to the in-meta-distribution case in \cref{sec:continuous_latent}. The overall trend of proportion correct also generally agrees with empirical prompts found on Bayes predictors, except for $T=100$ for the case of $\tau=(1/3, 2/3)$. \Cref{fig:betabern_bernmix_supp} extends \cref{fig:beta_bernmix}.
Compare across all prompt setups, the overall agreement between Bayes and neural predictors is worse compared to the previous IMD case shown in \cref{fig:beta_categorical_supp}.

Overall, although the optimal prompts cannot reveal the bimodality nature and the bias in each component of the task DG, they still reveal overall statistical properties. Compared to the OOMD case in \cref{sec:ood_prompting}, here we are able to make sense of the ratio of \ONE in the optimal prompts by matching them to the mean bias of $q$, but a detailed interpretation of the prompt length relies on knowing $p$.


\cref{fig:betabern_bernmix_emp_dist} shows the distribution of empirical prompts. To show how they are different to the theoretical $s^*$, we pick the prompt setup $N=100$ and $T=30$. For the least reliable case $q=\BernMix(1/4, 1/2)$, the suboptimal prompts turn out to have consistent ratio of \ONEs, giving consistent interpretation of the mean bias in $q$. This also holds for the suboptimal prompts in the case $q=\BernMix(1/3, 2/3)$.

\Cref{fig:betabern_bernmix_landscape} shows the loss ``landscape''. The optimal prompt produces a more distinctive optimal KL divergence compared to the two cases with $\tau$ supported on a finite mixture.
In particular, for the case of $q=\BernMix(1/4, 3/4)$, there is a sharp dip around the optimal prompt, which should imply more reliable identification.
This is consistent with the reliable identifiability under the Bayes predictor \cref{fig:betabern_bernmix_supp}.

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=0.9\textwidth]{figs/beta_mixture_supp.pdf}
    \caption{Proportion correct of $\hat s$. Same as \cref{fig:beta_bernmix} but with more prompting setups, using $\Lmax=15$.}
    \label{fig:betabern_bernmix_supp}
\end{figure}

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[width=0.9\textwidth]{figs/betabern_bernmix_0_empirical_prompt_dist.pdf}
    \caption{Distribution of empirically optimal prompts. Here, we set the $\Lmax=15$ for all predictors, which is greater than the length of the theoretical $s^*$ in all cases. \theempdistcaption
    \label{fig:betabern_bernmix_emp_dist}
    }
\end{figure}

%
\begin{figure}[ht] %
%
    \centering
    \includegraphics[height=0.3\textheight]{figs/betabern_bernmix_prompt_rank.pdf}
    \includegraphics[height=0.3\textheight]{figs/betabern_bernmix_prompt_tree.pdf}
    \caption{The loss ``landscape'' of $p=\BetaBern(1.0, 1.0)$ and $q=\BernMix(\tau_1, \tau_2)$.\landscapelegend
    \label{fig:betabern_bernmix_landscape}}
\end{figure}

\clearpage
%
\subsection{Switching DG pretraining}
%

%
\subsubsection{Heuristic method for interpreting prompts of the switching tasks}\label{sec:switching_details}
%


Given a prompt $s_{1:L}$, we want to ``guess'' the corresponding latent causes $\eps$ and $\lambda$. The heuristic method assumes that we know the sequences are generated from the switching process in \cref{thm:switching_process}, and that  
\begin{enumerate}
    \item $\eps\in[0, 1]$;
    \item $\lambda\in\{3,4,5\}$;
    \item The first bias $y_1$ associated with $s_1$ may not be the first $\eps$ appearing in \cref{thm:switching_process}. In other words, the ``phase'' of the $y$ is unknown and $y_{1:L}$. Taking $\lambda=3$ for example, $y$ can start with any of the following:  
    \begin{equation}\label{eq:all_phases}
\iftrue
    \begin{alignedat}{7}
    [&\eps, &~&  \eps, &~&\eps, &~& 1\!-\!\eps, &~& 1\!-\!\eps, &~& 1\!-\!\eps, &~& \ldots]\\
    [&\eps, &&  \eps, &&1\!-\!\eps, & & 1\!-\!\eps, & & 1\!-\!\eps, & &\eps, & & \ldots]\\
    [&\eps,     &&1\!-\!\eps,&& 1\!-\!\eps, & & 1\!-\!\eps, & &\eps, && \eps, & & \ldots]\\
    [&1\!-\!\eps,  && 1\!-\!\eps, & & 1\!-\!\eps, & &\eps, && \eps, & &\eps, && \ldots]\\
    [& 1\!-\!\eps, & & 1\!-\!\eps, & &\eps, && \eps, & &\eps, &&1\!-\!\eps,  && \ldots]\\
    [& 1\!-\!\eps, & &\eps, && \eps, & &\eps, &&1\!-\!\eps,  && 1\!-\!\eps, & & \ldots]\\
    \end{alignedat}
\else
    \begin{alignedat}{7}
    [&\epsilon, &~&  \epsilon, &~&\epsilon, &~& (1-\epsilon), &~& (1-\epsilon), &~& (1-\epsilon), &~& \ldots]\\
    [&\epsilon, &&  \epsilon, &&(1-\epsilon), & & (1-\epsilon), & & (1-\epsilon), & &\epsilon, & & \ldots]\\
    [&\epsilon,     &&(1-\epsilon),&& (1-\epsilon), & & (1-\epsilon), & &\epsilon, && \epsilon, & & \ldots]\\
    [&(1-\epsilon),  && (1-\epsilon), & & (1-\epsilon), & &\epsilon, && \epsilon, & &\epsilon, && \ldots]\\
    [& (1-\epsilon), & & (1-\epsilon), & &\epsilon, && \epsilon, & &\epsilon, &&(1-\epsilon),  && \ldots]\\
    [& (1-\epsilon), & &\epsilon, && \epsilon, & &\epsilon, &&(1-\epsilon),  && (1-\epsilon), & & \ldots]\\
    \end{alignedat}
\fi
    \end{equation}
\end{enumerate}
We first try to match $\lambda$ to the prompt. Take $\epsilon=0$ so that $y$ is binary, and enumerate all possible $y$'s of length $L$ with different phases (as in \cref{eq:all_phases}) and different values of $\lambda$. Pick the $y$ that has the fewest mismatch (smallest Hamming distance) with the binary prompt, note the best match by $y^*$ and the corresponding $\lambda$. Effectively, this $\lambda$ produces the smallest ``mismatch''  between the lower bias and a \ZERO token in the prompt, and between the higher bias and a \ONE token in the prompt.

Given the best matching binary $y^*$, we estimate $\eps$ as the proportion of incorrect match with the prompt: $$\frac{1}{L}\sum_{i=1}^L (1-y^*_i)(s_i) + y^*_i(1-s_i)$$.

\subsubsection{Proportion correct}

\Cref{fig:switching_supp} extends the results of \cref{fig:switching}(middle). Note that in this experiment we search through all possibly binary prompts of length $\Lmax$, and for each prompt we compute the expectation \eqref{eq:log_loss_L} by enumerating all possible sequences of length $T$. For the Bayes predictor this can be done quite efficiently, but for neural predictors this is still quite computationally intensive. As such, we only sweep $T\in\{10, 30\}$. We observe a robust increasing pattern only on the Bayes predictor. For the neural predictors, there is a slight increasing trend only for $\eps=0.3$.

%
\begin{figure}[ht!]
%
    \centering
    \includegraphics[width=\textwidth]{figs/switching_correct_supp.pdf}
    \caption{Same as \cref{fig:switching}(middle) but with more prompting setups, using $\Lmax=15$.}
    \label{fig:switching_supp}
\end{figure}


\clearpage
%
\subsection{Decision maker on bandits}
%

%

%
\begin{figure}[ht!]
%
    \centering
    \includegraphics[width=\textwidth]{figs/bandit_correct_supp.pdf}
    \caption{Same as \cref{fig:bandit}(lower left) but with more prompting setups, using $\Lmax=15$.}
    \label{fig:bandit_correct_supp}
\end{figure}


%
\section{Bandit decision-maker}\label{sec:bandit_details}
%

%
\subsection{Skill levels}\label{sec:bandit_details_skills}
%
In the main text, we specified that the skill parameter $\tau$ scales  the agent's counts of  the outcomes. Here, we detail how different skill levels are sampled to create the mixture of agents. 

To obtain agents with various skill levels, we modify the beliefs of the Thompons sampling (TS) agent by scaling the pseudocounts in the Beta posteriors of the reward probabilities by a skill level $\tau$. For skill level $\tau\in[0,1]$, for each arm $b\in\{\text{L}, \text{R}\}$, the posterior of the reward probability given past trajectories of $a_{1:t}\in\{\text{L},\text{R}\}^t$ and $r_{1:t}\in\{0, 1\}^t$ is

\begin{equation}\label{eq:bandit_arm_post}
v_{b,t,\tau}|a_{1:t}, r_{1:t},\tau = \Beta(1+\tau S_{b,1,t}, 1+\tau S_{b,0,t}),
\end{equation}
where
\begin{equation}\label{eq:bandit_counts}
\begin{aligned}
    S_{b,1,t}(a_{1:t}, r_{1:t}) &= \sum_{t'=1}^t \mathbbm{1}[a_{t'}=b] r_{t'}\,,\\
    S_{b,0,t}(a_{1:t}, r_{1:t}) &= \sum_{t'=1}^t \mathbbm{1}[a_{t'}=b] (1-r_{t'})\,.\\
\end{aligned}
\end{equation}
For each action, the agent first samples the reward from the posterior, and choose the action of the more rewarding arm. 

\cref{fig:TS_returns}(left) shows that the skill level affects the return most for lower values. As such, if we uniformly sample the skill level between 0 and 1, then there will be a lot of agents performing close to the optimal TS agent.


To avoid this, we define the skill $\tau=u^k$ where $u\sim \text{Uniform}([0,1])$ and $k>0$. We simulate the agent for different values of $k$ and $u$ for 300 actions repeated for 100k different random seeds, and show the returns as a function of $u$ in \cref{fig:TS_returns}(middle).
Through change of variable, we numerically estimate the distribution the return for a given value of $k$ \cref{fig:TS_returns}(right). We pick $k=4$ throughout all bandit experiments, so that there is a mixture of agents across all levels.

There is still a significant proportion of performant agents, which should make prompting easier. We could have designed the transformation from $u$ to $\tau$ more complicated to induce a more uniform return distribution, but this is not essential to demonstrate our points.
%
\begin{figure}
%
    \centering
    \includegraphics[width=\textwidth]{figs/TS_returns.pdf}
    \caption{Relationship between the return and the uniform random variable for different values of the power index $k$.}
    \label{fig:TS_returns}
\end{figure}



%
\begin{algorithm}
%
\caption{Pretraining Trajectory Generation for the bandit task.\label{alg:bandit_pretrain}}
\begin{algorithmic}
\INPUT: $p(a_{t+1}|h_t,\tau)$, the TS-like agent that acts according to posterior reward probability samples with scaled pseudocounts \eqref{eq:bandit_arm_post} , given history $h_t:=(a_i,r_i)_{i=1}^t$.
\INPUT A Bernoulli two-arm bandit environment with a uniform distribution of reward probabilities on each arm.

\STATE Sample a skill level $\tau \sim \text{Uniform}(0, 1)$.

\COMMENT{Generate the prompt segment}
\STATE Sample bandit reward probabilities $r_\text{L}$ and $r_\text{R}$.
\STATE Let $x_1$ be an empty sequence.
\FOR{$i = 1$ to $8$}
    \STATE Sample agent action $a_i \sim p(a|h_i, \tau)$.
    \STATE Observe reward $r_i$ from the bandit given $a_i$, $r_\text{L}$ and $r_\text{R}$.
    \STATE Append $(a_i, r_i)$ to $x_1$.
\ENDFOR

\COMMENT{Generate the rollout segment}
\STATE Sample bandit reward probabilities $r_L$ and $r_R$.
\STATE Empty history $h$
\FOR{$i = 1$ to $300$}
    \STATE $a_i \sim p(a|h_i, \tau)$ where $h_i$ is the history up to step $i$.
    \STATE Observe reward $r_i$ from the bandit given $a_i$, $r_L$ and $r_R$.
    \STATE Append $(a_i, r_i)$ to $x_2$.
\ENDFOR

\STATE Concatenate $x = [x_1, \texttt{/}, x_2]$, where $\texttt{/}$ is a separator token.

\OUTPUT $x$.
\end{algorithmic}
\end{algorithm}


%
\subsection{Approximate Bayes predictor}\label{sec:bandit_details_bayes}
%
The key quantity required for predicting the action at each time step, marginalizing over all skill levels, is to compute the probability
\begin{align*}
\mathbb{P}(a_{t+1}=\text{L} | a_{1:t}, r_{1:t})
&= \int_0^1\int_0^1\mathbbm{1}[\vL > \vR]p(\vL, \vR| a_{1:t}, r_{1:t}) \ud \vL \ud \vR\\
&= \int_0^1\int_0^1\int_0^1
    \mathbbm{1}[\vL > \vR]p(\vL, \vR| a_{1:t}, r_{1:t}, \tau)
    p(\tau|a_{1:t}, r_{1:t}) \ud \vL \ud \vR \ud \tau\\
&= \int_0^1\int_0^1\int_0^1
    \mathbbm{1}[\vL > \vR]p(\vR| a_{1:t}, r_{1:t}, \tau)p(\vL| a_{1:t}, r_{1:t}, \tau)
    p(\tau|a_{1:t}, r_{1:t}) \ud \vL \ud \vR \ud \tau\\
&= \int_0^1\mathbb{P}(v_{\text{L},t,\tau} > v_{\text{R},t,\tau})p(\tau|a_{1:t}, r_{1:t})  \ud \tau,
\end{align*}
where the third equality uses conditional independence between reward probabilities given history and $\tau$. To approximate the last integral, we discretize the support at 1000 evenly spaced grid points. For each value of $\tau$ on the grid, we now need to compute the probability that one Beta random variable is greater than another. This does not have a closed  form solution, but we found the technique by \citet{cook2012fast} to be fast and accurate compared to a Monte Carlo approximation. Finally, to compute $p(\tau|a_{1:t}, r_{1:t})$, we use the following recursion.
\begin{align}\label{eq:bandit_post_tau}
    p(\tau|a_{1:t}, r_{1:t}) \propto  p(\tau|a_{1:t-1}, r_{1:t-1}) p(a_t | a_{1:t-1}, r_{1:t-1}, \tau),
\end{align}
which is also approximated on the evenly spaced grid for $\tau$. 

%
\subsection{Theoretically optimal prompts}\label{sec:bandit_details_optimal_prompts}
%

We find the theoretically optimal prompt on the Bayes predictor above using the following steps. We first estimate the return using $10^5$ Monte Carlo rollouts for each of the $2^16$, using the same sequence of random seeds for actions selection and reward outcomes.
We then take the best 20 prompts with top 20 estimated return, and re-evaluate using $10^7$ Monte Carlo rollouts, using the same seed sequence for actions and rewards as above.
We sort the prompts according to the return evaluated on $10^5$, and plot the return against the prompt rank in \cref{fig:bandit_prompt_index}. In this case, we can see the loss ``landscape'' is very sharp, indicating that the optimal prompts should be reliably identified. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/bandit_prompt_index.pdf}
    \caption{
    Estimated return of each prompt by Monte Carlo against the prompt rank. The prompts are sorted using estimated return under $N=10^5$ rollout trajectories. 
    \label{fig:bandit_prompt_index}
    }
\end{figure}

These theoretically optimal prompts shown in \cref{fig:bandit} share the same pattern: try one arm and get no reward, then stick to the other arm and always get rewarded, except that the last reward may be missing. It may be striking to see that sticking to the first rewarding arm is the optimal strategy. 
The explanation of such persistence relies on how different skill levels are generated by the pretraining distribution: the reward pattern indicates that the second chosen arm is highly rewarding, and persistence to the more rewarding arm implies that the skill factor $\tau$ is likely large, which is desirable as it promotes more TS-like decisions. In addition, the first unrewarded outcome induces posterior Beta$(1, 1+\tau)$, which is biased towards 0. Together with the constantly reward streak from the other arm, creating a posterior of Beta$(1+7\tau,1)$, choosing the rewarding arm is more likely if $\tau$ is large. Essentially, a large reward gap between the two Beta distributions helps the predictor identify $\tau$. 

Another large reward gap can be induced by other reward patterns, such as Beta$(1, 1+4\tau)$
and Beta$(1+4\tau, 1)$, which is brought by 4 unrewarded outcomes from one arm, and 4 rewarded outcomes from the other. However, the choosing a previously unrewarded arm is unlikely to happen for an agent with large $\tau$, so such reward gap is not as effective as the one above in shifting the posterior of $\tau$ towards 1. 

The four equivalent optimal prompt is because of a simple symmetry between the left and right arm, and the fact that the posterior \eqref{eq:bandit_post_tau} does not depend on the last reward.

\subsection{Empirically optimal prompts}\label{sec:bandit_interp}

For each empirically optimial prompt, we estimate the return in the rollout segment using 300 actions and $N=10^6$ sequences. The results in \cref{fig:bandit_return} suggest that the performances of these prompts are very close to the ceiling on the Bayes predictor, even when optimizing using  $1000$  rollout trajectories with $50$ actions and rewards.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/bandit_return.pdf}
    \caption{The estimated rollout return for empirically optimized prompts. Errorbars show 1 SEM from 100 seeds.}
    \label{fig:bandit_return}
\end{figure}

However, the prompts may differ and cause inconsistent interpretations.
In order to interpret and visualize the empirically optimal prompts, we map each prompt to the Win-Stay/Lose-Shift (WSLS) probabilities which have been used in psychology to analyze human behavior on playing bandits \citep{bruner1957perceptual,nowak1993strategy}. Specifically respectively, WS is the probability that the previous action is repeated when receiving a reward, and LS is the probability of shifting to the other arm after an unrewarded outcome. 
For the theoretically optimal prompts, these probabilities are both 1.0. 
%
We compute the WSLS of the empirically optimal prompts from all predictors and show the distribution in \cref{fig:bandit_interp}. It shows that the empirical prompts may support multiple likely values for WSLS unless $N=100000$, win which case the WSLS is more concentrated at (1.0, 1.0). Therefore, the suboptimal prompts can lead to different interpretations, under the WSLS metric.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/bandit_interp.pdf}
    \caption{
    The distribution of Win-Stay/Lose-Shift for all empirical prompts for $300$ rollout actions and rewards.
    }
    \label{fig:bandit_interp}
\end{figure}


\subsection{Comparing optimal and typical prompts}\label{sec:bandit_typical_prompt}

As in the switching problem in \cref{sec:switching}, we report the performance of statistically typical prompts in \cref{fig:bandit_typical_prompt}. In terms of expected total return, the optimal prompt is equivalent to typical prompts of length 60, roughly 4 times the length of the optimal prompt. This is consistent with the instantaneous regret. Note that, in the context of the bandit problem class, using longer expert demonstrations not only takes up more context window in a model, but also induces higher costs to the expert.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figs/bandit_typical_prompt.pdf}
    \caption{
    Total reward and regret of typical prompts, compared with Thompson sampling agent and the optimal prompt applied to the Bayes predictor. Estimated using $100\,000$ rollouts and typical prompts. Errorbars on the left panel show 1 SEM.
    \label{fig:bandit_typical_prompt}
    }
\end{figure}




\clearpage


\section{Detailed discussions}

\subsection{Relation to previous finds on LLM}\label{sec:related_findings}

First, the most effective prompt may not necessarily be typical samples from either the pretraining or the task distribution. 
This is because the optimal prompts serve to overcome and exploit the biases built into the predictor at pretraining to perform a task.
%
The fact that the pretrained distribution is usually unknown makes it very difficult to identify or to interpret optimal prompts based only on the task.
%
Previous work on LLMs found that the examples that make in-context learning work well can be counter-intuitive, and may even include ``wrong'' examples \citep{min2022rethinking,yang2024prompts}. The influence of word frequencies on task performance has also been illustrated by \citet{razeghi2022impact} and \citet{wei2021frequency}. In our work, we additionally analyzed the effects of having discrete and continuous latent causes in the data distribution \citep{xieexplanation,jiang2023latent}, and whether or not the pretraining distribution includes the task in terms of the latent cause distribution (IMD \emph{versus} OOMD). In particular, we showed that prompts under OOMD tasks can be harder to interpret.

Second, typical and thus interpretable samples from the task distribution may require a longer length to reach a good performance than optimal prompt. This finding is consistent with \citet{bhargava2023s,wu2023many,renze2024benefits} who discovered that shorter/fewer \emph{selected} (not necessarily typical) demonstrations can work surprisingly well compared to longer prompts or more demonstrations.
%
%
On the bandit decision-maker experiment, heuristic prompts can also lead to unexpectedly good or bad results compared to sample trajectories from an expert agent, which is reminiscent of common experience interacting with LLMs \citep{zamfirescu2023johnny,khurana2024and}. In our bandit experiment, again, knowing the pretraining distribution may help explain this mystery.
%

%

Third, with the optimal prompts obtained on Bayes predictors as ground-truth, we have shown that the empirically optimized prompts based on a finite dataset may or may not converge to the optimal prompts, even when using a large task dataset with long sequences. The unreliability of finding an optimal prompt means that the empirically optimized prompts on LLMs may vary substantially and unpredictably across different runs, predictors, prompt lengths, batch sizes, even for a fixed pretrained model, making interpretation more challenging. Previous works on prompt optimization methods (e.g.\ \citep{deng2022rlprompt,pryzant2023automatic,fernando2023promptbreeder}) typically do not compare or report how their method perform as these seemingly irrelevant hyperparameters vary. In particular, the batch size plays a significant role on proportion correct of empirically optimized prompts, and quite often it needs to be very large to ensure high chances of reaching the optimal prompt. 

\subsection{Limitations and future work}\label{sec:limitations}

\paragraph{Binary tokens vs text tokens.}

While previous work on different data domains tries to understand the implications of different alphabet sizes \citep{rajaraman2024toward,ieremie2024protein,gagie2012note,heurtel2024compression}, they do not directly predict how our results on optimal prompts are affected by alphabet size.
%
Using sequences over binary tokens does not change the nature of sequence prediction task, but the token space is much more limited than text tokens. A single token contains much more information, richer semantics, and reflect more complex structure. Although generalizing our data generators from Bernoulli to categorical distribution is straightforward, the latent factor also becomes complex, i.e. the number of biases grows with the alphabet size. Our results on optimal prompt do not explicitly depend on dimensionality and should still hold qualitatively in those regimes, although the interpretations may change, especially when there are additional hierarchical structures behind text tokens. Future work could consider progressively more complex hierarhical models, such as variants of the Latent Dirichlet Allocation model \citep{blei2003latent} and the GINC dataset \citep{xieexplanation}, while ensuring interpretability of the results.

\paragraph{Non-statistical aspects of language prediction.}

We have investigated specifically the mechanism of sequence prediction, from a Bayesian meta-learning perspective. In particular, each data generator in this work have Bayes predictors that can be expressed  in terms of a fixed-dimensional sufficient statistics. They fully summarizes the prompt history of any length as a posterior distribution over a fixed-dimensional task variable. This makes it natural to take the Bayesian view to explain many interesting phenomena shown in this work.

Previous work on synthetic datasets considered more ``retrieval''-like mechanisms \citep{xieexplanation, jiang2023latent}. 
For example, \citet{allen2023physics} showed the observation that permuting the sentences can improves question answering or information retrieval .
%
The Bayesian perspective, though certainly applicable in this and all other sequence prediction problems, 
may not provide the most intuitive explanation to such phenomena.

Further, the Bayesian view also does not account for any post-pretraining stages of real LLMs, such as supervised finetuning, human preference learning, and those that promote search and reasoning capabilities. For example, it is possible to ask a language model to produce 70\% \ONEs with the prompt ``Give me a sequence of 70\% \ONEs.'', which is clearly out-of-scope for the Bayesian meta-learning theory. It is nonetheless possible that supervized finetuning makes it easy to prompt by effectively making the latent factor distributions more uniform over certain semantic domains. Understanding these post-pretraining effects by probing the latent factor distribution could be an interesting future direction.

\paragraph{Typical prompt.}

When comparing the advantage of optimal prompts to typical prompts, we computed the \emph{expected} performance of samples or demonstrations from the task distribution. However, the performance of prompting with individual prompts can vary drastically. In practice, people often perform \emph{some} form of prompt selection within allowable budget. While hitting the optimal prompt by random chance is small, it is possible that for the task has a very flat ``loss landscape'', in which case finding a performant prompt by chance can be quite high. The advantage of optimal prompt is likely diminished in this case.

\paragraph{Importance of pretraining distribution.}

We attributed the cause of unintuitivess to unknown pretraining distribution. It is often believed that the \emph{coverage} of pretraining data affects the range of capabilities in downstream applications, leading to the notion that if some behavior is included in the dataset, then the capability can be induced by prompting. Our results on in-meta-distribution experiments indicate further that the \emph{distribution} of the pretraining data affects the \emph{difficulty} of intuitively discover an understand optimal prompts. In other words, the existence of some behavior is the pretraining dataset does not imply that this behavior can be intuitively prompted, but rather the distribution of all behavior matter. To verify these hypotheses, one would need to pretrain different language models on datasets with controllable latent factors. Alternatively, there are techniques to debias the generated content by building a model of the generative behavior \citep{gagne2023inner}.

\paragraph{Interpreting prompts.}

We have chosen to interpret rather simple semantics in binary sequences: the bias of a hypothetical coin, or actions and rewards in Bernoulli bandits. There are also simple symmetry in the prompts that would correspond roughly to synonyms or paraphrases of the same meaning. In real language models, however, the nature of the semantics may be quite different to a real-valued coin bias. Nonetheless, the simplicity of interpreting coin biases induces minimal subjectiveness, which may be an issue with interpreting natural languages.  

\paragraph{Prompting under a trivial problem} 

The optimal prompts on the bandit problem suggests that the most effective way to control a latent factor, such as the skill level, may appear in a hypothetical situation that is extremely simple: two arms with huge reward gap. Future prompt engineering endeavors could consider choosing demonstrations corresponding to a variety of settings in other latent factors of the problem class, including cases that would be considered simple or trivial.




\clearpage

%
\section{List of Notation}\label{sec:notation}
%
%
%
%

%
%
%
\begin{tabbing}
  \hspace{0.13\textwidth} \= \hspace{0.73\textwidth} \= \kill
  {\bf Symbol }     \> {\bf Explanation}                                                    \\[0.5ex]
  DG                \> data generator                                                \\[0.5ex]
  CIB-DG            \> conditionally independent Bernoulli data generator  \\[0.5ex]
  $\tau$            \> hidden factor (e.g., $\in[0,1]$ for Bernoulli bias parameter and bandit skill level) \\[0.5ex]
  $\cX\ni x_t$      \> binary token alphabet                                                \\[0.5ex]
  $T\in\mathbb{N}_+$\> length of a task sequence to be predicted  \\[0.5ex]
  $x_{1:T}\in\cX^T$ \> a sequence (to be predicted) of length $T$  \\[0.5ex]
  $p_{x|\tau}(x_{1:T}|\tau)$ \> distribution of sequence with bias $\tau$                   \\[0.5ex]
  $p_\tau(\tau)$    \> prior distribution over $\tau$                                            \\[0.5ex]
  $p_x(x_{1:T})$      \> = $\int p_{x|\tau}(x_{1:T}|\tau)\ud p_\tau(\tau)$, pretraining distribution \\[0.5ex]
  $p$      \> $p_x$ above, reference to a previously mentioned pretraining distribution/DG \\[0.5ex]
  $L\in\mathbb{N}_+$\> context length                                                       \\[0.5ex]
  $s_{1:L}\in\cX^L$ \> prompt of length $L$ to condition on                                                \\[0.5ex]
  $p_\text{B}(x_{1:T}|s_{1:L})$ \> Bayes predictor under DG $p$ (= $\int p_{x|\tau}(x_{1:T}|\tau)p_{\tau|x}(\tau|s_{1:T})$ for piecewise conditionally independent DG)  \\[0.5ex]
  $p_\theta(x_{1:T}|s_{1:L})$ \> Neural predictor $s_{1:L}$ trained on data from $p$ given $s_{1:L}$   \\[0.5ex]
  $S_x(s_{1:L})$    \> number of time $x$ appears in $s_{1:L}$                                           \\[0.5ex]
  $\Lmax\in\mathbb{N}_+$           \> maximal prompt length                                                \\[0.5ex]
  %
  $q(x_{1:T})$      \> task distribution                                         \\[0.5ex]
  $q$      \> reference to a previously mentioned task distribution                                         \\[0.5ex]
  $N\in\mathbb{N}_+$               \> task dataset size                                                        \\[0.5ex]
  $\DqN$          \> $= \{x_{1:T}^n\}_{i=n}^N$ data set                                   \\[0.5ex]
  $\beta$  \> parameter in the Beta prior distribution \\[0.5ex]
  $\epsilon$ \> Bernoulli parameter in the switching process \\[0.5ex]
  $\lambda$ \> half period of the switching process \\[0.5ex]
  $w$               \> probability of $\tau_2$ in $\BernMix{\tau_1, \tau_2}$ when $p_\tau=(1-w)\delta_{\tau_1} + w\delta_{\tau_2}$                                             \\[0.5ex]
  $w_L(s_{1:T})$    \> posterior weight of $\tau_2$                                         \\[0.5ex]
  %
  %
  %
  %
  %
  %
  %
  %
  %
  ${\cal L}(p,q,s_{1:L})$ \> Log-loss of predictor $p(x_{1:T}|s_{1:L})$ under prompt $s_{1:L}$ for data from $q(x_{1:T})$              \\[0.5ex]
  $\hat{\cal L}(p,{\DqN}, s_{1:L})$ \> empirical log-loss given dataset $\DqN$ \\[0.5ex]
  $s^*_{1:L}(p,q)$  \> theoretically optimal ($\mathcal{L}(p,q,\cdot)$-maximizing) prompt of length $L$  \\[0.5ex]
  $s^*_\Lmax(p,q)$  \> theoretically optimal ($\mathcal{L}(p,q,\cdot)$-maximizing) prompt of length $\leq\Lmax$  \\[0.5ex]
  $\hat s_{1:L}(p,q)$\> empirically optimal ($\hat{\cal L}(p, \DqN, \cdot)$-maximizing) prompt of length $L$ \\[0.5ex]  
  $\hat s_\Lmax(p,q)$  \> empirically optimal ($\hat{\cal L}(p, \DqN, \cdot)$-maximizing) prompt of length $\leq\Lmax$ \\[0.5ex]  
  %
  %
  %
  %
  %
  %
  %
  %
  %
\end{tabbing}
%
\end{document}
%
