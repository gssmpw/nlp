\section{Related Work}
\label{sec:related_work}

\subsection{Federated Learning with LLMs}

\paragraph{Federated Learning.}
FL is a privacy-preserving collaborative training paradigm that allows multiple parties to build a shared global model without the need to exchange raw data. One of the main challenges in FL is data heterogeneity, as client datasets often stem from different distributions~\citep{mcmahan2017communication}. To address this issue, various techniques have been proposed, including regularization~\citep{li2020federated}, gradient correction~\citep{niu2022federated}, feature alignment~\citep{yu2021fed2}, adaptive aggregation weights~\citep{wu2021fast}, momentum introduction~\citep{liu2020accelerating}, and leveraging pre-trained models~\citep{huang2023federated}.

\paragraph{Fedeated Learning with LLMs.}
As LLMs have achieved significant success in centralized learning, there is a growing interest in adapting FL to accommodate the fine-tuning of pre-trained LLMs~\citep{Ren-et-al:2024}, particularly to supplement the publicly available data with privately owned datasets~\citep{jin2023backdoor}. In response, several frameworks have emerged recently, including OpenFedLLM~\citep{ye2024openfedllm} and FederatedScope-LLM~\citep{kuang2024federatedscope}. Moreover, advanced methods such as FedbiOT~\citep{wu2024fedbiot} which safeguards model ownership, and FFA-LoRA~\citep{sun2024improving} which enhances performance under differential privacy constraints, are being developed to optimize LLM training in federated environments. 

\paragraph{Privacy in LLM Prompting.}
LLM prompting have revolutionized natural language processing but face significant challenges when handling privacy-sensitive text data, a topic that remains relatively underexplored. \cite{chong2024casper} propose “Casper,” a browser extension that sanitizes user prompts by removing sensitive information before submission to LLMs. \cite{edemacu2024privacy} survey privacy-preserving prompt engineering techniques, emphasizing approaches such as differential privacy and data obfuscation. \cite{gim2024confidential} introduce “Confidential Prompting,” which leverages confidential computing to secure user prompts during LLM inference. \cite{yu2024privacy} tackle privacy concerns in instruction tuning by generating synthetic instructions under differential privacy guarantees, reducing data exposure risks. Finally, \cite{li2024llm} present “LLM-PBE,” a toolkit designed to evaluate privacy risks and mitigation strategies in LLMs. These studies highlight the pressing need for robust privacy-preserving mechanisms in LLM prompting applications. Building on these advancements, future work can explore integrating privacy-preserving techniques, such as differential privacy, data obfuscation, or confidential computing, into FedTextGrad to secure prompts or textual gradients and mitigate privacy risks in federated learning scenarios.

\subsection{LLMs as Optimizers}

\paragraph{Prompt Optimization.}
Prompt optimization has attracted significant attention, with various strategies proving effective in enhancing the performance of LLMs. Techniques such as selecting optimal few-shot examples~\citep{pryzant2023automatic}, in-context learning~\citep{dong2022survey}, chain of thought reasoning~\citep{wei2022chain}, and model ensembles~\citep{jiang2023llm} have shown promise. Furthermore, several strategies have been developed to automate this process. White-box approaches, which rely on numerical gradients, offer a useful solution. However, they are limited by the need to access model parameters, restricting their applicability to only open-source LLMs.

\paragraph{LLMs as Optimizier.}
Recent research has turned towards leveraging \textit{LLMs as optimizers} in black-box settings~\citep{yang2023large}. The foundation of this concept stems from the ability of LLMs to simulate human decision-making. \cite{zheng2023judging} benchmarked the behavior of LLMs and human decisions, finding that modern LLMs align closely with human judgment. Building on this, \cite{yang2024large} proposed \textit{optimization by prompting}, where LLMs generate new solutions based on a prompt that includes previously generated solutions. \cite{ma2024large} further investigated whether LLMs are effective prompt optimizers. Tools like DSPy~\cite{khattab2023dspy} and ProTeGi~\cite{pryzant2023automatic} introduced programmatic frameworks for optimizing LLM-based APIs, achieving performance gains across tasks such as question answering and prompt refinement. 
All LLM-as-optimizer approaches require the LLM to be as powerful (large-scale) as possible, as smaller LLMs currently lack the capability to serve as effective optimizers. However, it is feasible to reuse prompts optimized by large-scale models for smaller ones~\cite{vu2022spot}, enabling the adaptation of the LLM-as-optimizer paradigm in resource-constrained settings

\paragraph{TextGrad.}
Recently, TextGrad~\citep{yuksekgonul2024textgrad} presents a more generalized approach of using LLM as optimizers by adapting the above ideas to broader domains, such as optimizing instances like molecular structures or code snippets, using a \textit{textual-backpropagation-based} framework. These methods highlight the versatility of LLMs in enhancing their own outputs across diverse applications, thus opening up opportunities for prompt optimization in closed-source LLMs within centralized learning settings by circumventing the need for access to model parameters. However, the research question of how to achieve similar advances in FL settings remains unresolved. This paper seeks to address this question.
Nevertheless, adapting FedTextGrad to resource-constrained settings with smaller LLMs remains a challenging and unresolved research question, as smaller LLMs often lack the capacity to serve effectively as LLM-as-optimizers for self-refinement.
