\section{Experimental Investigation}

\subsection{Experiment Settings}

\textbf{Data and Tasks.} We evaluate \ours{} on prompt optimization across three key tasks from the \textit{BBH} benchmark~\citep{srivastava2022beyond}: 1) \textit{BBH Object Counting}, 2) \textit{BBH Multistep Arithmetic}, and 3) \textit{GSM8k Math Problem}~\citep{cobbe2021training}. They are well-suited for assessing the effectiveness of prompt optimization in complex reasoning scenarios. For each dataset, we split it into \textit{training}, \textit{validation}, and \textit{test} sets. 
We adopt the dataset preprocessing methodology outlined in~\citep{yuksekgonul2024textgrad}. The training set is used for prompt optimization. The validation set is used for prompt selection and hyper-parameter tuning. The test set is used for reporting the final performance, thereby ensuring fair and rigorous evaluation.

\textbf{Model and Setup.} For our experiments, we use the Llama-3.1-8B model~\citep{dubey2024llama} for prompt optimization, serving as both the inference engine and the optimizer within our framework. Unless otherwise specified, we use a default batch size of $3$ with $3$ local steps for tuned hyper-parameters, with batches sampled randomly with replacement. After each iteration, the same batch is evaluated in a loop. The prompt is updated only if the performance does not drop compared to the previous non-updated version. Under homogeneous FL settings, each dataset is randomly split into $3$ clients, each having an equal number of training and validation samples. 

\subsection{Empirical Study on Key Hyper-Parameter Choices}

This section investigates the impact of key hyper-parameters, including local steps, number of clients and batch size, on \ours{} through ablation studies.

\textbf{Local steps}: Previous FL research~\citep{mcmahan2017communication} has frequently conducted ablation studies on local steps to understand the balance between local model updates and global model synchronization. In traditional FL, increasing local steps is expected to reduce communication costs by allowing more local updates before synchronization with the server~\citep{mcmahan2017communication, li2020federated}. However, this often comes at the cost of performance degradation due to local overfitting and divergence from the global model. As observed in Fig.~\ref{fig:local_step}, increasing local steps in our setting leads to a significant performance drop, confirming that too many local updates without frequent synchronization exacerbate model misalignment across clients.

\textbf{Number of clients}: Previous work~\cite{mcmahan2017communication, li2020federated} has explored the effect of increasing the number of clients in FL to evaluate the model’s robustness to client heterogeneity and communication bottlenecks. In Fig.~\ref{fig:num_client}, we examine this effect by splitting a single-task dataset into multiple subsets, each representing a client. With the increase in the number of clients, the performance drops dramatically. This can be attributed to communication overhead and misaligned prompt updates between the server and clients. Furthermore, in tasks like Object Counting, increasing the number of clients consistently degraded performance, likely due to the model’s sensitivity to the heightened heterogeneity and divergent data distributions.

\textbf{Batch size}: Ablation studies on batch size in FL typically explore its impact on convergence and communication efficiency. Larger batch sizes are expected to stabilize training by reducing gradient variance, but they might also slow the convergence due to the reduced frequency of updates~\citep{mcmahan2017communication}. In Fig.~\ref{fig:batch_size}, it can be observed that increasing the batch size initially improves performance by smoothing the optimization process. After a certain threshold, performance declines. This is likely due to less frequent updates, which reduce the model’s ability to adapt quickly to new data distributions, especially under distribution shifts.

\textbf{In summary}, our ablation studies reveal that while increasing local steps and batch size can initially stabilize and improve optimization, they ultimately introduce significant challenges related to communication efficiency and global model alignment. Similarly, increasing the number of clients improves performance up to a point, but leads to degradation due to communication and synchronization issues, particularly in data heterogeneous environments.

\input{figure/trend_ablation}

\subsection{Evaluation on Heterogeneous Settings}

\input{table/noniid}

\textbf{Heterogeneous Experimental Setup.}
We evaluate the Heterogeneous \ours{} framework using three distinct datasets: Object Counting, Multistep Arithmetic, and GSM8K, with each dataset representing a client in the federated learning setting. The experiments focus on two critical hyperparameters: the number of local steps (\(E\)) and batch size (\(B\)). Local steps (\(E\)) refer to the number of client-specific updates performed before global model synchronization, while the batch size (\(B\)) determines the number of samples processed in each local update. To investigate the interaction between these hyperparameters, we conduct evaluations with \(E \in \{3, 5, 10\}\) and \(B \in \{1, 3, 10\}\). Each dataset is split evenly among clients, and the performance is assessed based on the model's ability to adapt under varying hyperparameter configurations.

\textbf{Results and Observations.}
The results presented in Table~\ref{table:noniid} reveal notable patterns in the Heterogeneous \ours{} framework's performance across different local steps and batch sizes. Increasing local steps (\(E\)) from 3 to 5 improves performance across all batch sizes, indicating that more local updates allow clients to better capture their specific data distributions. However, further increasing \(E\) to 10 leads to a slight performance degradation, suggesting that excessive local updates before synchronization may result in overfitting to client-specific data. Similarly, batch size (\(B\)) exhibits an optimal value at \(B = 3\), which consistently delivers the best results. Larger batch sizes, such as \(B = 10\), show diminishing returns or even performance drops, possibly due to the reduced frequency of updates, which hampers the model's ability to adapt effectively to local distributions. These findings underscore the importance of careful tuning of local steps and batch size to balance local adaptation with global model convergence.

\subsection{Performance with Various LLM APIs.}

\textbf{Experimental Setup.}
We evaluate the performance of various LLM application programming interfaces (APIs) on the BBH Object Counting dataset, considering both centralized and federated learning settings. In the federated learning setup, we use the homogeneous \ours{} configuration with the following default hyperparameters: local steps (\(E = 3\)), batch size (\(B = 3\)), and three clients, each receiving an evenly split portion of the dataset. For the centralized setting, the split datasets are grouped and trained as a single dataset, enabling a direct comparison between centralized learning and federated learning.

\textbf{Results.}
The results, illustrated in Figure~\ref{fig:two_images}, highlight several notable trends. In the centralized TextGrad setting (Figure 3a), GPT-4~\citep{achiam2023gpt} and DeepSeek R1 distill Llama 70B~\citep{guo2025deepseek} model achieve the highest accuracy ($0.99$), closely followed by LLaMA-3.1-405B ($0.96$) and LLaMA-3.1-70B ($0.95$). In the federated learning setting (Fig.~\ref{fig:image1}), while GPT-4 continues to perform best ($0.98$), there is a slight performance drop across all models when transitioning from centralized to federated learning. The performance gap is more pronounced in smaller models, such as Gemma-2-9B~\citep{team2024gemma} and Qwen-2-7B~\citep{yang2024qwen2}, which experience sharper declines in accuracy (see Fig.~\ref{fig:image2}). These findings suggest that while federated learning has a marginal effect on more capable models like GPT-4 and LLaMA, the impact is more substantial for less powerful LLMs, underscoring the challenges of federated learning in heterogeneous environments.



\input{figure/main_result}








\section{Enhanced \ours{} Prompt Aggregation Method} %

In the following section, we first highlight the limitations of directly applying prompt concatenation for prompt aggregation in \ours{}, demonstrating that this approach is impractical due to the excessive token length it generates, which often exceeds the context window of LLM API and leads to processing errors. Second, we explore summarization as an alternative method to mitigate this issue; however, we observe that it consistently underperforms compared to concatenation. Finally, inspired by principles of human communication, we introduce an enhanced summarization approach that incorporates uniform information density. We find that this simple yet effective method significantly improves performance while maintaining practical token lengths.

\subsection{Prompt Concatenation Analysis}
\textbf{Concatenation aggregation in \ours{} risks exceeding token limits and increasing costs, posing scalability challenges.}
Concatenation is a natural approach for aggregating text information; however, this method introduces a significant issue in our \ours{} framework, as the prompt length increases with the number of clients, potentially exceeding token limits and leading to rejection by LLM API services. 
The Fig.~\ref{fig:trend_concat} illustrates the exponential growth in concatenated prompt token length as the number of clients increases, highlighting the risk of exceeding GPT-4’s context window limit of 8192 tokens (denoted by the red dashed line). The right y-axis shows the associated cost in USD, with increasing token lengths resulting in higher costs. Error bars represent the standard deviation of token lengths across different client configurations. The figure emphasizes the trade-off between prompt length and scalability in federated learning settings, particularly when using concatenation-based prompt aggregation methods.

\input{figure/trend_concat}

\subsection{Concatenation vs. Summarization}
Summarization is often regarded as a natural solution to mitigate the issue of long token lengths introduced by concatenation. However, in our \ours{} framework, summarization underperforms compared to concatenation, prompting the need for further enhancements. In this section, we compare concatenation and summarization as prompt aggregation strategies. The results, shown in Fig.~\ref{fig:concat_vs_sum}, cover three tasks: \textit{Object Counting}, \textit{Multistep Arithmetic}, and \textit{GSM8K}. For the \textit{Object Counting} task, concatenation slightly outperforms summarization with accuracies of 0.90 and 0.88, respectively. In the more complex \textit{Multistep Arithmetic} task, the performance gap is more pronounced, with concatenation (0.69) significantly surpassing summarization (0.55). In contrast, for the \textit{GSM8K} task, both methods perform comparably, with concatenation achieving 0.94 accuracy and summarization closely following at 0.92. Overall, concatenation consistently demonstrates superior performance, particularly in more complex tasks like multistep arithmetic, underscoring its advantage in our framework.

\subsection{The Proposed Sum \textit{UID} Prompt Aggregation Method}
\input{figure/prompt_aggregation}
\textbf{We propose a UID-based prompt summarization method to overcome the limitations of summarization’s information non-uniformity, enhancing the stability and accuracy of \ours{}.}
Fig.~\ref{fig:context_window} highlights the effects of different prompt aggregation approaches. The main challenge lies in efficiently combining local prompt updates into a global prompt that retains essential information while adhering to input length constraints for federated optimization. 
Direct concatenation of client prompts can produce overly long global prompts, particularly with many FL clients, potentially exceeding the LLM's context window. Summarization addresses this issue by keeping the global prompt within the allowed length, but it often creates overly dense prompts that degrade model performance.

\begin{figure}
    \vspace{-5mm}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
\includegraphics[width=\textwidth]{figure/image/concat_vs_sum.png}
    \caption{Concatenation vs. Summarization}
    \label{fig:concat_vs_sum}
    \end{subfigure}
\quad
    \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/image/performance_vs_density.png}
    \caption{Summarization vs. Sum UID}
    \end{subfigure}
    \caption{Observations of different aggregation strategies, where (a) presents the performance of concatenation and summarization, and (b) compares the performance of \textit{UID} and summarization.}
        \label{fig:sum_vs_uid}
    \vspace{-5mm}
\end{figure}

\textbf{UID Hypothesis.} To address the issue of excessive token lengths in concatenation, we propose a prompt aggregation method based on the \textbf{Uniform Information Density Hypothesis (UIDH)}~\citep{meister2020if}, which posits that effective communication involves distributing information uniformly. We hypothesize that uneven information distribution in prompts adversely affects LLM performance, as critical updates from clients may be diluted. To mitigate this, we introduce an enhanced summarization approach incorporating \textbf{UID principles} to ensure a balanced representation of client updates in the aggregated prompt.

\textbf{Measuring Uniformity.} To measure information density uniformity~\citep{meister2020if}, surprisal values are computed for each word in a text based on the conditional probabilities derived from a pre-trained language model, such as GPT-2. The uniformity is quantified by the variance in surprisal values, where lower variance indicates a more uniform distribution of information. The process involves tokenizing the text, extracting log-probabilities, calculating surprisal for each token, and then computing the mean and variance of these values. The mean surprisal (\(\mu\)) represents the average information density, while the variance (\(\sigma^2\)) reflects the uniformity of information distribution:$\mu = \frac{1}{N} \sum_{i=1}^{N} I(w_i|C)$, $\quad \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (I(w_i|C) - \mu)^2$
where \(I(w_i|C) = -\log_2 P(w_i|C)\) is the surprisal for token \(w_i\) given context \(C\), and \(N\) is the number of tokens in the text. A lower value of \(\sigma^2\) indicates higher uniformity in information density, consistent with the \textit{Uniform Information Density} hypothesis.

\textbf{Performance Gains.} Our method improves prompt aggregation by maintaining a uniform information density across the summarized prompt, preserving key information from each client while preventing over-compression. Fig.~\ref{fig:concat_vs_sum} shows that, compared to summarization, our UID-based method yields superior performance in Object Counting and GSM8K dataset. Our empirical results demonstrate consistent gains in accuracy and prompt stability, confirming the effectiveness of applying UID principles in FL systems.
























