\section{Introduction}
Large Language Models (LLMs)~\citep{zhao2023survey}, such as GPT~\citep{brown2020language}, Gemini~\citep{team2023gemini} and LLaMa~\citep{touvron2023llama}, have become the foundational backbone of modern natural language processing (NLP) systems. These models often require fine-tuning to enhance their responsiveness to specific tasks. While existing open datasets play an important role in LLM tuning, the vast amount of privately owned, potentially sensitive data continuously generated by end devices represents a significant, yet largely untapped, pool of samples for LLM fine-tuning.

To adapt to this reality, federated learning (FL)~\citep{mcmahan2017communication} offers a promising privacy-preserving framework for collaboratively fine-tuning LLMs with distributed, privately owned data. To address the efficiency demands and black-box nature of many involving LLM APIs~\citep{achiam2023gpt}, recent advancements in 
zeroth-order optimization~\citep{qin2023federated,fang2022communication} are beginning to provide useful tools for exploring this avenue. %
However, these methods generally rely on numerical loss calculations to estimate gradients~\cite{balasubramanian2022zeroth}, which is infeasible when using black-box LLM APIs, where the loss definition is unclear~\citep{yang2024large} and only textual feedback (e.g., human feedback in ChatGPT~\citep{achiam2023gpt}) is available~\citep{yuksekgonul2024textgrad}.

Recently, LLMs have been demonstrated as effective optimizers~\citep{pryzant2023automatic,liu2024large,yang2024large}, capable of automatically refining prompts step-by-step to enhance performance~\citep{shinn2024reflexion}, while providing informative and interpretable natural language criticism to the variables to guide how the variables should update. As a representative method, TextGrad~\citep{yuksekgonul2024textgrad} enables automatic ``differentiation'' through text, allowing the backpropagation of textual feedback to improve individual components of a compound LLM agentic system without relying on gradients or numerical calculations. While TextGrad offers substantial advantages in traditional centralized machine learning settings, its adaptation to FL environments remains unexplored. In this paper, we seek to answer the exploratory question:
\vspace{-1mm}
\begin{center}
    \begin{tcolorbox}
[colframe=black!50!white, colback=gray!5!white, sharp corners=all, boxrule=0.5mm, rounded corners=southeast, arc is angular, , width=10cm]
    \textit{Can textual gradient work under federated learning settings?}
\end{tcolorbox}
\end{center}
\vspace{-1mm}
Our contributions are fourfold, as outlined below.





\noindent\textbf{Adapting:} 
To facilitate \textbf{textual gradient} operations in FL environments, we propose a \textbf{first-of-its-kind} \ours{} method. Under this method, each FL client is equipped with TextGrad-based textual gradients during local training. Instead of uploading model parameters like in classical FL (e.g., FedAvg~\citep{mcmahan2017communication}), clients upload their optimized local prompts to the FL server. The server then performs prompt aggregation through concatenating and summarizing clients' local prompts, and redistribute the global prompt back to the clients for further training. 



\noindent\textbf{Investigating:} With \ours{}, we then conduct experimental studies across various LLMs and configurations to empirically investigate its relative performance under FL settings compared to TextGrad in centralized settings on a range of reasoning tasks. During this process, we study the impact of key factors—such as \textbf{local update epochs}, \textbf{batch size}, \textbf{number of clients}, and \textbf{data heterogeneity}—on the performance of our framework.

\noindent\textbf{Uncovering:} Through our empirical investigation, we have identified a key challenge for federated textual gradient \textbf{aggregation}: \textit{preserving critical information in distributed prompt updates}. Concatenation-based prompt aggregation can produce excessively long prompts that exceeds the LLM API’s context window, while summarization-based prompt aggregation often degrades performance by generating overly complex and densely packed texts. This is currently the key hurdle hindering the adoption of textual gradient in FL settings.

\noindent\textbf{Improving:} To address this challenge, we develop an key insight that uneven distribution of information within the summarized prompts is the root cause. We then introduce an enhanced summarization method based on the \textbf{Uniform Information Density (UID) principle} to ensure more balanced information distribution across the summarized global prompt. It improves prompt aggregation in \ours{} by maintaining a uniform information density, resulting in shorter aggregated prompts that preserve critical contents without sacrificing model performance.

\noindent \textbf{Related work:} A detailed literature review is provided in App.~\ref{sec:related_work}.\\ 
\textit{{FL for LLMs.}} As LLMs have achieved significant success in centralized learning, there is a growing interest in adapting FL to accommodate the fine-tuning of pre-trained LLMs~\citep{Ren-et-al:2024}, particularly to supplement the publicly available data with privately owned datasets~\cite{jin2023backdoor}. In response, several frameworks have emerged recently, including OpenFedLLM~\cite{ye2024openfedllm} and FederatedScope-LLM~\cite{kuang2024federatedscope}. Moreover, advanced methods such as FedbiOT~\cite{wu2024fedbiot} which safeguards model ownership, and FFA-LoRA~\cite{sun2024improving} which enhances performance under differential privacy constraints, are being developed to optimize LLM training in federated environments.\\
\textit{{LLMs as Optimziers.}}
Recent research has turned towards leveraging \textit{LLMs as optimizers} in black-box settings~\citep{yang2023large}. The foundation of this concept stems from the ability of LLMs to simulate human decision-making. \cite{zheng2023judging} benchmarked the behavior of LLMs and human decisions, finding that modern LLMs align closely with human judgment. Building on this, \cite{yang2024large} proposed \textit{optimization by prompting}, where LLMs generate new solutions based on a prompt that includes previously generated solutions.\cite{ma2024large} further investigated whether LLMs are effective prompt optimizers. Tools like DSPy~\cite{khattab2023dspy} and ProTeGi~\cite{pryzant2023automatic} introduced programmatic frameworks for optimizing LLM-based APIs, achieving performance gains across tasks such as question answering and prompt refinement. These new solutions are then assessed and incorporated into the prompt for the next optimization step. 




\input{figure/FedTextGrad_Framework}





    
    
