\section{Conclusions}

In this work, we introduced \ours{}, an extension of the TextGrad framework specifically designed to address the challenges of prompt optimization in federated learning settings. By identifying the training instability caused by aggregating distributed prompt updates and demonstrating the limitations of traditional concatenation and summarization-based techniques, we proposed a novel approach based on Uniform Information Density Principles to enhance \ours{} prompt summarization. Our method addresses the issue of uneven information distribution, leading to improved prompt efficacy and overall performance in federated environments. This study establishes a foundation for future advancements in prompt optimization for large-scale, distributed learning systems and opens new avenues for deploying LLMs in privacy-sensitive, resource-constrained environments.





