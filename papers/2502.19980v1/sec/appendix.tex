
\input{sec/2_related_work}

\section{Experimental Details}

\subsection{Datasets}

We evaluate our method on three primary reasoning tasks:
\begin{itemize}
    \item \textbf{BBH Object Counting}~\citep{srivastava2022beyond}: A task challenges models to accurately count objects based on visual or textual descriptions, testing their ability to reason about quantities and manage multiple elements within a scene or context.
    \item \textbf{BBH Multi-Step Arithmetic}~\citep{srivastava2022beyond}: Another BBH task that tests a model’s ability to solve mathematical problems that require multiple sequential steps of reasoning, assessing its proficiency in handling complex, multi-stage arithmetic operations.
    \item \textbf{GSM8k Math Problem}~\citep{cobbe2021training}: A dataset of grade school math problems designed to test the mathematical reasoning capabilities of LLMs.
\end{itemize}


\subsection{Base Models}
We conduct experiments using five large language models (LLMs), encompassing both widely-used commercial APIs such as \textit{GPT-4o} and \textit{GPT-3.5}~\citep{achiam2023gpt}, as well as cutting-edge open-source models like \textit{Llama 3}, \textit{Llama 3.1}~\citep{dubey2024llama}, and \textit{Qwen 2}~\citep{yang2024qwen2}. 

By leveraging this diverse set of models, we are able to rigorously assess the scalability and robustness of our approach across a range of architectures and model sizes, ensuring comprehensive evaluation and applicability.

\subsection{\ours{} Setup}

All datasets are split into \textbf{training}, \textbf{validation}, and \textbf{test} sets. The training set is used to optimize prompts through \ours{}, the validation set helps with the prompt selection and hyperparameter tuning, and the test set is reserved for reporting the final performance. FL simulates a decentralized setting where clients send prompt updates to a central server without sharing raw data.




    


\section{Prompt Aggregation Techniques}

Prompt aggregation plays a critical role in federated learning with LLMs, especially as the number of clients increases. We explore two primary methods for aggregating client prompts and evaluate their effectiveness under varying conditions.

\begin{itemize}
    \item \textbf{Concatenation}: In this method, the individual prompts from each client are concatenated into a single, aggregated prompt. While simple to implement, this approach has significant drawbacks. As the number of clients increases, the total prompt length can easily exceed the input length constraints imposed by large language models (LLMs), such as GPT-4’s context window of 8192 tokens. This results in prompts being truncated or rejected by the LLM API, severely limiting the scalability of this approach in federated settings.

    \item \textbf{Summarization}: To alleviate the issue of prompt length in concatenation, summarization techniques are applied to compress the information from each client into a shorter prompt. Although this reduces token length, it often comes at the cost of performance degradation. The compression inherent in summarization leads to information loss, particularly when the prompts contain complex or diverse client-specific updates. This information loss can cause suboptimal model performance, especially in tasks that require retaining detailed and nuanced client data.

    \item \textbf{Summarization with Uniform Information Density}: We introduce a summarization approach based on the Uniform Information Density (UID) hypothesis, which ensures a more balanced distribution of information within aggregated prompts. The UID hypothesis suggests that distributing information uniformly optimizes communication efficiency, and we apply this principle to mitigate the performance degradation observed in traditional summarization methods. By maintaining uniform information density, our method preserves critical information from each client while reducing prompt length, aligning with LLM input constraints. This approach consistently enhances performance across tasks by improving reasoning accuracy and prompt stability in federated learning environments.
\end{itemize}



\paragraph{\textbf{An Example of the Prompt Designed for Summarizing Prompts From TextGrad.}}
\begin{center}
    \begin{tcolorbox}[
        colframe=teal!80!black, 
        colback=cyan!5!white, 
        sharp corners=all, 
        boxrule=0.6mm, 
        rounded corners=all, 
        width=\textwidth, 
        title={\textbf{Prompt for Summarization}}, 
        fonttitle=\bfseries]
        \texttt{Merge the following list of prompts into a single, cohesive prompt while preserving all original information. Ensure that the final instruction remains unchanged and is placed as the last sentence. Provide only the merged prompt.}
    \end{tcolorbox}
\end{center}

\paragraph{\textbf{An Example of the Prompt Designed for Summarizing Prompts with UID From TextGrad.}}
\begin{center}
    \begin{tcolorbox}[
        colframe=teal!80!black, 
        colback=cyan!5!white, 
        sharp corners=all, 
        boxrule=0.6mm, 
        rounded corners=all, 
        width=\textwidth, 
        title={\textbf{Prompt for Summarization with UID}}, 
        fonttitle=\bfseries]
        \texttt{Merge the following list of prompts into a single, cohesive prompt while preserving all original information. \textbf{Apply Uniform Information Density Principles.} Ensure that the final instruction remains unchanged and is placed as the last sentence. Provide only the merged prompt.}
    \end{tcolorbox}
\end{center}





    




    

\paragraph{\textbf{A Concatenated Prompt Example.}}
\begin{center}
    \begin{tcolorbox}[
        colframe=black!75!white, 
        colback=blue!5!white, 
        sharp corners=all, 
        boxrule=0.6mm, 
        rounded corners=all, 
        width=\textwidth, 
        title={\textbf{Concatenated Prompt for Object Counting}}, 
        fonttitle=\bfseries]
        \texttt{When answering a reasoning question, provide a clear and explicit explanation of your thought process, including any relevant calculations or reasoning, to support your answer. Use specific language to explain your reasoning, and avoid using vague or ambiguous language that could be interpreted in multiple ways.}

        \vspace{0.5em}
        \texttt{Provide a detailed and step-by-step explanation of your thought process, including any relevant calculations or reasoning, to support your answer. For example, a good response might be: 'To determine the total number of objects, I counted each item individually: 1 microwave, 1 table, 1 fridge, 1 stove, 1 oven, 1 toaster, 1 couch, and 4 cars. Therefore, the total number of objects is 1 + 1 + 1 + 1 + 1 + 1 + 1 + 4 = 11.'}

        \vspace{0.5em}
        \texttt{Ensure that your response is clear, concise, and free of unnecessary words or phrases, and that it clearly addresses the question being asked. Use precise and descriptive language to explain your reasoning, avoiding oversimplification and providing a nuanced or detailed explanation of the answer.}

        \vspace{0.5em}
        \texttt{Consider the potential for ambiguity in your response and avoid using language that could be interpreted in multiple ways. Provide a clear and concise explanation of the reasoning behind your answer, using relevant details and examples to support your response.}

        \vspace{0.5em}
        \texttt{When providing a numerical answer, avoid including unnecessary phrases or context, and focus on presenting the answer in a clear and concise format, such as a single number or a brief explanation of the calculation.}

        \vspace{0.5em}
        \texttt{When answering a counting question, provide a clear and explicit statement of the count, using a specific format such as 'Answer: X' where X is the numerical value. When providing a final answer, explicitly state the operations performed to arrive at the answer, and provide a clear and concise explanation of the reasoning behind the answer.}

        \vspace{0.5em}
        \texttt{Use precise and descriptive language to explain your reasoning, avoiding oversimplification and providing a nuanced or detailed explanation of the answer. Address any missing information in the problem and provide a complete and accurate response.}

        \vspace{0.5em}
        \texttt{Use specific details and concrete examples to support your response and provide a clear and concise explanation of the reasoning behind the answer. Provide a final answer that explicitly states the operations performed to arrive at the answer and includes a clear and concise explanation of the reasoning behind the answer.}

        \vspace{0.5em}
        \texttt{Use a formal and objective tone to ensure that the response is clear and unambiguous. If ambiguity is unavoidable, provide a clear and concise explanation of the ambiguity, and ensure that the response is still clear and unambiguous. Ensure that the response directly addresses the question being asked and provides a clear and concise answer to the problem.}

    \end{tcolorbox}
\end{center}



\paragraph{\textbf{A Summarized Prompt Example.}}
\begin{center}
    \begin{tcolorbox}[
        colframe=black!75!white, 
        colback=blue!5!white, 
        sharp corners=all, 
        boxrule=0.6mm, 
        rounded corners=all, 
        width=\textwidth, 
        title={\textbf{Summarized Prompt for Object Counting}}, 
        fonttitle=\bfseries]
        \texttt{When answering a counting question, clearly state the problem being asked based on the input provided, considering the context of the question, the type of item being asked about, and any relevant information that may affect the answer.}

        \vspace{0.5em}
        \texttt{Identify and count the specific category of items mentioned in the question, and provide a clear and concise count of the objects mentioned. Ensure that the numerical answer is accurate and precise, and provide a clear and concise step-by-step explanation of how you arrived at your answer.}

        \vspace{0.5em}
        \texttt{Be aware of idiomatic and colloquial language that may affect the answer, and use your best judgment to interpret any unclear or ambiguous language in the question. Consider alternative scenarios or edge cases that may affect the answer, and use relationship understanding to disambiguate any unclear or ambiguous language in the question.}

        \vspace{0.5em}
        \texttt{Provide a direct answer to the problem being asked, in the format "The total number of [object type] is [number]". Avoid paraphrasing the input and use step-by-step explanations to provide a clear understanding of the calculation or reasoning behind the answer.}

        \vspace{0.5em}
        \texttt{Specify the type of objects being counted, such as 'animals,' 'fruits,' or 'household items,' based on the input provided. To ensure accuracy, please count each type of object individually and add them together. For example, if the question asks for the total number of musical instruments, count each type separately (e.g., guitars, violins, drums).}

        \vspace{0.5em}
        \texttt{Answer: \$VALUE where VALUE is a numerical value. The last line of your response should be of the following format: "Answer: \$VALUE" where VALUE is a numerical value.}
    \end{tcolorbox}
\end{center}

















\section{Experiments on More Challenging and High-Complexity Tasks with FedTextGrad}
To evaluate the performance of FedTextGrad on tasks with higher complexity and reasoning challenges, we conducted experiments using \texttt{GPT-4o} on datasets extracted from \textit{LiveBench}~\citep{white2024livebench}. These datasets include tasks that test logical inference, spatial reasoning, and mathematical abstraction, providing a rigorous benchmark for assessing the robustness of our FedTextGrad.

\input{table/apx_livebench}

\paragraph{Experimental Setup.}
We evaluated FedTextGrad on two categories of tasks: reasoning and advanced mathematical problems. For reasoning tasks, we utilized \textit{Web of Lies (Version 2)}, an enhanced dataset that introduces deductive red herrings to challenge logical rigor; \textit{Zebra Puzzle}, a deductive reasoning task involving multiple constraints across variables like colors and nationalities; and a \textit{Spatial Dataset}, requiring the model to reason about numerical and positional attributes of solid, regular heptagons. For mathematical tasks, we employed the \textit{AMPS Hard Dataset}, designed to test advanced symbolic manipulation and mathematical reasoning through challenging, randomized problem distributions.

\paragraph{Results.}
In Table~\ref{tab:apx_livebench}, on reasoning tasks, FedTextGrad with summarization demonstrates adequate performance but remains below the centralized TextGrad configuration across all datasets. For the \textit{Web of Lies} dataset, FedTextGrad achieves an accuracy of 0.30, which is lower than the centralized TextGrad accuracy of 0.37, highlighting challenges in adapting to deductive reasoning tasks in federated settings. Similarly, on the \textit{Zebra Puzzle} dataset, FedTextGrad achieves 0.27 compared to the centralized TextGrad’s 0.33, reflecting the difficulty of effectively optimizing logical reasoning tasks in a decentralized environment. For the \textit{Spatial} dataset, FedTextGrad records 0.40 accuracy compared to the centralized TextGrad’s 0.53, further showcasing challenges in handling spatial reasoning under federated conditions.

In contrast, results for the mathematical task (\textit{AMPS Hard Dataset}) show that FedTextGrad surpasses centralized TextGrad with an accuracy of 0.50 versus 0.46. This indicates that, despite challenges in reasoning tasks, FedTextGrad excels in tasks requiring mathematical reasoning, possibly due to its ability to better capture client-specific variations in structured numerical tasks. These results underscore the varying effectiveness of FedTextGrad across different task domains, with potential for further improvements in reasoning tasks under federated settings.


\section{Feasibility of Smaller LLMs Deployment with TextGrad.}
\paragraph{Experimental Setup.}
To evaluate the feasibility of deploying prompts optimized on larger LLMs in resource-constrained settings, we conducted an additional experiment on \textit{prompt transferring}. Specifically, prompts optimized using TextGrad on a larger model (\texttt{LLaMA 3.2-11B}) were directly applied to a smaller model (\texttt{LLaMA 3.2-3B}) without further optimization. Unlike the \texttt{LLaMA 3.1-8B} model used in the main text, we opted for the \texttt{LLaMA 3.2} series because the \texttt{3.1} series does not include models smaller than 8B. The \texttt{LLaMA 3.2} series, on the other hand, offers a wider range of model sizes, making it suitable for evaluating prompt transferability across different model scales. The tasks used were the same as in the main text, including \textit{BBH Object Counting}, \textit{BBH Multi-step Arithmetic}, and \textit{GSM8K}.

\input{table/apx_prompt_transfer}

\paragraph{Results.}
The results in Table~\ref{tab:apx_prompt_transfer} showed that on \textit{BBH Object Counting} and \textit{BBH Multi-step Arithmetic}, the transferred prompts achieved significantly better performance compared to the initial prompts. This indicates the potential for reusing optimized prompts in smaller, resource-efficient models without requiring further optimization. However, on the \textit{GSM8K} task, the transferred prompts experienced a noticeable performance downgrade. This highlights the challenges of prompt generalization for more complex reasoning tasks. 
These findings suggest that prompt transferring is a promising approach for leveraging the optimization capabilities of larger LLMs while deploying smaller models in resource-constrained settings. Nevertheless, the observed limitations in tasks like \textit{GSM8K} underscore the need for further studies to enhance the generalization capabilities of prompt transferring within the TextGrad framework. This represents an important direction for future research.

\section{UID Summarization Generalizability on Client Heterogeneity.}

\input{table/apx_hetero_uid_sum}

\paragraph{Experimental Setup.}
We evaluate the robustness of UID-based summarization in heterogeneous client settings, where each client is assigned a distinct task. This configuration follows the setup described in Section 3.3 of the main text. The tasks used to simulate client heterogeneity include reasoning-based benchmarks, and the model employed is \texttt{LLaMA 3.1-8B}.

In this setup, three clients handle unique tasks to represent heterogeneity in data distributions. Specifically, one client addresses object counting tasks, another manages multi-step arithmetic, and the third tackles problems from GSM8K. UID-based summarization is compared with standard summarization aggregation to measure its performance under these conditions. Additionally, performance trends are analyzed as the batch size increases to better understand the behavior of the summarization methods. The training epoch is fixed to 3 as the default for all experiments.

\paragraph{Results}
The results in Table~\ref{tab:apx_hetero_uid_sum} demonstrate that UID-based summarization performs effectively under moderate client heterogeneity. It achieves slightly better results than standard summarization aggregation, showcasing its ability to retain essential information while adapting to diverse data distributions. Moreover, the performance trends with increasing batch size closely mirror those observed for standard summarization, indicating consistent and stable behavior across different configurations. 
These findings highlight the effectiveness of UID summarization in federated settings with heterogeneous client data. They reinforce its applicability in real-world scenarios, where client heterogeneity is a common challenge. The additional results and analysis are included in the revised manuscript to provide a comprehensive evaluation of the method under varying conditions.


\section{Dynamic Prompt Aggregation}
To explore the feasibility of dynamic aggregation strategies in federated learning, we conduct an experiment evaluating the performance of dynamic switching between concatenation and summarization during the aggregation stage. This experiment extends the analysis presented in Figure 6(a), using the same dataset and \texttt{LLaMA 3.1-8B} model.

\paragraph{Experimental Setup.}
In this experiment, we implement a dynamic aggregation strategy where concatenation is used for prompt aggregation by default. However, if the concatenated prompt exceeds the model's pre-defined context window (selected as 800 tokens), summarization is applied to reduce the prompt length. This hybrid approach leverages the strengths of both methods, aiming to balance information retention and context limitations. The federated setup follows the same configuration as described in Figure 6(a) of the main text.

\paragraph{Results.}
The results, shown in Figure~\ref{fig:apx_dynamic}, demonstrate that the dynamic aggregation strategy underperforms compared to both the summarization-only and concatenation approaches in general. Notably, in one task (object counting), the dynamic strategy slightly outperforms summarization. However, in other tasks, its performance lags behind. These findings highlight the potential adaptability of dynamic aggregation switching in managing varying prompt lengths.

\input{figure/apx_concat_vs_sum_vs_dynamic}

\paragraph{Discussion.}
This experiment underscores the promise of dynamic aggregation switching as an effective strategy for federated textual aggregation. By addressing context window constraints dynamically, the method balances the trade-off between information retention and prompt length management. However, a significant limitation is the need to pre-select an optimal context window for specific datasets, which can be challenging and requires dedicated selection and prior knowledge. Future work could focus on refining the switching criteria and exploring its applicability across a broader range of datasets and models.

\input{figure/apx_surprisal_comparison}

\section{Surprisal Analysis on Summarization with and without UID}

To investigate the information retention capabilities of UID summarization compared to standard summarization, we conduct an experiment analyzing the mean and maximum surprisal values of prompts after aggregation. Surprisal values measure the unexpectedness of generated text, providing insights into the uniformity and completeness of information across aggregated prompts.

\paragraph{Experimental Setup.}
We calculate the mean and maximum surprisal values of prompts aggregated using both standard summarization and UID summarization. The calculations are performed across multiple tasks to ensure a comprehensive evaluation. Surprisal values are derived from the aggregated prompts post-processing, capturing how effectively the summarization methods retain critical information.

\paragraph{Results.}
The results in Figure~\ref{fig:apx_surprisal} indicate that the mean and maximum surprisal values for UID summarization are nearly identical to those for standard summarization across all tasks. This suggests that both methods exhibit similar levels of information retention. While UID summarization is specifically designed to enhance information uniformity, it does not introduce additional information loss compared to standard summarization, as evidenced by the surprisal metrics.

\paragraph{Discussion.}
These findings confirm that UID summarization retains essential information as effectively as standard summarization while achieving improved task performance, as demonstrated in Section 4. The analysis highlights the robustness of UID summarization in ensuring both critical information retention and enhanced uniformity.
