\section{An Envisioned Roadmap Forward}

With LLMs becoming a burgeoning field and model sizes continuously increasing, addressing the high costs and data privacy concerns in LLM training is paramount. Adapting FL to LLMs offers a promising direction. \ours{} introduces a novel and efficient paradigm that utilizes LLMs as optimizers and textual gradients to update LLM components. 
However, practical implementation of \ours{} involves several critical challenges, including (1) managing heterogeneous data to prevent conflicting contexts and ensure effective aggregation, (2) developing privacy-preserving methods tailored to textual gradients, as traditional approaches often compromise utility in natural language settings, (3) improving communication efficiency through advanced summarization and adaptive encoding to scale in federated environments, and (4) ensuring robustness against diverse adversarial attacks by adapting different defense mechanisms.

\noindent\textbf{Learning on Heterogeneous Environment:} When encountering heterogeneous data and model architecture, clients can produce conflicting contexts, which result in failed texture aggregation or ambiguous summarized texts. In traditional federated learning (FL), strategies such as resolving gradient conflicts with flatter minima~\citep{chen2023fedsoup} or regularizing clientsâ€™ gradient updates~\citep{li2020federated}, as well as sharing common hidden features~\citep{yi2024federated}, have proven effective. However, these approaches depend on numerical operations or the use of hidden features, making them unsuitable for scenarios involving textual gradients or black-box settings. Consequently, adapting existing methods or developing novel solutions represents a crucial direction for future research.

\noindent\textbf{Privacy Attack and Protection}: By sharing texture gradient from client LLMs, \ours{} exposure attack surfaces. Although direct gradient inversion might be less effective on textual gradients, adversaries could still use the contextual nature of text to uncover private information, making careless prompt engineering a significant risk for revealing sensitive data~\citep{yao2024survey}. In traditional FL, privacy protection methods such as differential privacy (DP), secure multi-party computation (SMPC), and homomorphic encryption are widely used to safeguard numerical gradients~\citep{behnia2022ew}. However, these strategies face challenges when applied to federated textual gradients, as natural language contains more context and meaning, making it harder to obfuscate without losing utility. While DP could introduce noise into text, this risks rendering the gradients incoherent, and SMPC or encryption techniques would require significant advances to handle the complexity of encrypted text. Thus, new privacy-preserving methods tailored specifically for textual data are needed in federated learning. 

\noindent\textbf{Communication Efficiency}:
Traditional FL approaches to improving communication efficiency typically employ techniques such as gradient compression~\citep{jiang2022model} or conflict resolution to reduce the number of communication rounds~\citep{chenlocal}. However, none of these methods are specifically designed for the textual gradient domain. 
Unlike numerical gradients, textual gradients are inherently contextual and carry semantic information, making direct compression or sparsification infeasible without risking the loss of critical information or coherence, highlighting the need for further research into scalability within \ours{}. 


\noindent\textbf{Robustness}: Research in this area examines various methods, such as poisoning attacks, Byzantine attacks, and other empirical approaches, that adversaries use to undermine the integrity of global models in FL systems. To counter these attacks, various defense mechanisms have been developed to enhance robustness in FL. For example, Byzantine-resilient aggregation methods like Krum and Trimmed Mean mitigate malicious updates by focusing on reliable client contributions and have been widely adopted in traditional FL~\citep{so2020byzantine,jin2023backdoor} but infeasible in the textural context. Methods based on outlier detection can identify and remove suspicious updates. With proper prompt design or text embedding, such strategies show potential for use in \ours{}. However, the inherent complexity of LLM-based systems exacerbates the difficulty of both executing these attacks and defending against them.
