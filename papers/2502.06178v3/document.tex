% !TEX program = pdflatex
% Full chain: pdflatex -> biber/bibtex -> pdflatex -> pdflatex
\documentclass[11pt,en]{elegantpaper}


\newcommand{\domain}{\mathcal{X}}
\newcommand{\X}{\bm{X}}
\newcommand{\x}{\bm{x}}
\newcommand{\y}{\bm{y}}
\newcommand{\D}{\bm{D}}
\newcommand{\A}{\bm{A}}
\newcommand{\K}{\bm{K}}
\renewcommand{\k}{\bm{k}}
\newcommand{\order}{\mathcal{O}}
\newcommand{\cumregret}{\mathcal{R}}
\newcommand{\simregret}{\mathcal{S}}
\newcommand{\1}{\mathds{1}}
\newcommand{\pto}{\overset{P}{\longrightarrow}}
\newcommand{\asto}{\overset{a.s.}{\longrightarrow}}
\newcommand{\dif}{\mathop{}\!\mathrm{d}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\diag}[1]{\mathrm{diag}\left\{#1\right\}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\Prob}{\mathbb{P}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\hltr}[1]{\textcolor{red}{#1}}
\newcommand{\hltb}[1]{\textcolor{blue}{#1}}
\newcommand{\itref}[1]{\hyperref[#1]{\textcolor{black}{\ref*{#1}}}}


\title{Bayesian Optimization by Kernel Regression and Density-based Exploration}
\author{
    Tansheng Zhu\textsuperscript{1}  \and 
    Hongyu Zhou\textsuperscript{2} \and 
    Ke Jin\textsuperscript{2} \and 
    Xusheng Xu\textsuperscript{3} \and 
    Qiufan Yuan\textsuperscript{3} \and 
    Lijie Ji\textsuperscript{4,5}\thanks{Corresponding author: \href{mailto:lijieji@shu.edu.cn}{lijieji@shu.edu.cn}}
}
\institute{
    \textsuperscript{1}Zhiyuan College, Shanghai Jiao Tong University, Shanghai 200240, P. R. China. \\
    \textsuperscript{2}School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai 200240, P. R. China. \\
    \textsuperscript{3}Shanghai Institute of Aerospace Systems Engineering, Shanghai 201109, P. R. China. \\
    \textsuperscript{4}Department of Mathematics, Shanghai University, Shanghai 200444, P. R. China. \\
    \textsuperscript{5}Newtouch Center for Mathematics of Shanghai University, Shanghai University, Shanghai 200444, P. R. China.
}
\version{}
\date{}

\addbibresource[location=local]{references.bib} % reference file
\graphicspath{figures/} % figures folder

\begin{document}

\maketitle

\begin{abstract}
    Bayesian optimization is highly effective for optimizing expensive-to-evaluate black-box functions, but it faces significant computational challenges due to the high computational complexity of Gaussian processes, which results in a total time complexity that is quartic with respect to the number of iterations.
    To address this limitation, we propose the Bayesian Optimization by Kernel regression and density-based Exploration (BOKE) algorithm.
    BOKE uses kernel regression for efficient function approximation, kernel density for exploration, and integrates them into the confidence bound criteria to guide the optimization process, thus reducing computational costs to quadratic.
    Our theoretical analysis rigorously establishes the global convergence of BOKE and ensures its robustness in noisy settings. 
    Through extensive numerical experiments on both synthetic and real-world optimization tasks, we demonstrate that BOKE not only performs competitively compared to Gaussian process-based methods but also exhibits superior computational efficiency. 
    These results highlight BOKE's effectiveness in resource-constrained environments, providing a practical approach for optimization problems in engineering applications.


    \keywords{Bayesian optimization, kernel regression, kernel density estimation, global convergence.}
\end{abstract}


\section{Introduction}
\label{sec:introduction}

Bayesian optimization (BO) is a sequential optimization algorithm known for its efficiency in minimizing the number of iterations required to optimize objective functions \cite{bergstra2011algorithms,hutter2011sequential}. This is particularly useful for problems where the objective function lacks an explicit expression, has no derivative information, is non-convex, or is expensive to evaluate \cite{brochu2010tutorial,mockus1998application,mockus2002bayesian}. Notably, BO remains effective even when only noisy observations of the objective function's sampling points are available.
Given its robustness in such scenarios, BO has been widely applied to hyperparameter tuning in machine learning models \cite{snoek2012practical,li2018hyperband} and experimental design \cite{chaloner1995bayesian,press2009bandit,pourmohamad2021bayesian}.


The sampling criterion of the BO algorithm for selecting the next sampling point is determined by an acquisition function, such as Probability of Improvement (PI), Expected Improvement (EI) \cite{mockus1998application,jones1998efficient}, or Gaussian Process Upper Confidence Bound (GP-UCB) \cite{cox1992statistical,cox1997sdo,srinivas2010gaussian}. The latter two functions balance exploitation and exploration: exploitation focuses on maximizing the mean of the surrogate model for the objective function, while exploration minimizes the surrogate model's variance to encourage sampling in uncertain regions.
However, these Gaussian Process (GP)-based acquisition functions require computing the inverse of a full matrix, whose size grows with the number of iterations. This computational overhead significantly impacts efficiency, particularly in high-dimensional problems and real-time decision-making tasks in real-world applications \cite{hase2018phoenics}.


To address the computational challenges of GP-based BO algorithms, the Tree-structured Parzen Estimator (TPE) reformulates the EI function as a density ratio estimation problem under specific conditions, avoiding the need for direct computation of the GP mean. This method performs well as the number of observations increases \cite{bergstra2011algorithms}.
Bayesian Optimization by Density-Ratio Estimation (BORE) transforms density-ratio estimation into a weighted classification problem, enabling estimation through likelihood-free inference \cite{tiao2021bore,oliveira2022batch}. Building on this approach, Likelihood-Free Bayesian Optimization (LFBO) generalizes the framework to accommodate any acquisition function expressed in the form of expected utility \cite{song2022general}.


Beyond the density ratio framework, significant research has focused on accelerating the computation of the GP mean. Notable approaches include sparse approximations of Gaussian processes, which directly reduce computational complexity \cite{liu2020gaussian,mutny2018efficient,calandriello2019gaussian,rando2022ada}.
In addition to GP-based models, alternative surrogate models have been explored to improve computational efficiency.
For example, Sequential Model-based Algorithm Configuration (SMAC) employs a random forest regressor and uses the standard deviation among trees to estimate uncertainty \cite{hutter2011sequential,hutter2012parallel}. Phoenics leverages Bayesian neural networks to model the sample density function and proposes an acquisition function based on kernel densities \cite{hase2018phoenics}.
Global Optimization with Learning and Interpolation Surrogates (GLIS) and its extensions adopt inverse distance weighting and radial basis functions to construct the exploration term, achieving competitive performance with reduced computational overhead \cite{bemporad2020global,previtali2023glisp}.


In this paper, we propose a novel Bayesian optimization algorithm, Bayesian Optimization by Kernel Regression and Density-based Exploration (BOKE). The method contains three key components: the kernel regression surrogate model, kernel density estimation, and an improved Kernel Regression-based UCB (KR-UCB) criterion \cite{yee2016monte}. BOKE not only achieves rapid convergence and high accuracy of Gaussian process-based methods, but also offers a significant reduction in computational complexity.
We establish the consistency of the kernel density-based exploration strategy and demonstrate its robust sampling performance through space-filling design experiments. Additionally, we analyze the pointwise consistency of kernel regression using density-based bounds, which enables the construction of an UCB-style acquisition function \cite{auer2002finite}. These two forms of consistency ensure the global convergence of the BOKE algorithm.
The regret analysis further highlights the global convergence of the proposed acquisition function. To accelerate convergence in problems with sparse local optima, we develop the BOKE+ algorithm, which enhances exploitation in high-value regions. Finally, we validate the superior performance and computational efficiency of both BOKE and BOKE+ across a range of synthetic and real-world optimization tasks.


The paper is organized as follows. In \cref{sec:background}, we introduce the notations, problem formulation, and present a pseudocode for the Bayesian optimization algorithm. \cref{sec:methodology} provides a detailed description of the kernel regression surrogate construction, the density-based exploration strategy, and the proposed BOKE algorithm, along with its extensions and computational complexity analysis. The pointwise consistency of kernel regression, as well as the consistency and regret analysis of the proposed algorithm, are discussed in \cref{sec:convergence analysis}. \cref{sec:experiments} presents numerical results demonstrating the superior convergence rates and computational efficiency of the proposed algorithm. Finally, \cref{sec:conclusions} concludes the paper with a discussion on future research directions.




\section{Background}
\label{sec:background}

In this paper, we use the following notations: $\mathbb{Z}_{+} = \set{1, 2, \dots}$, $\mathbb{N} = \mathbb{Z}_{+} \cup \set{0}$, $\mathbb{R}_+ = \set{x > 0 : x \in \mathbb{R}}$, and $\mathbb{R}_{\ge 0} = \mathbb{R}_+ \cup \set{0}$.
$\order(\cdot)$ and $\widetilde{\order}(\cdot)$ denote asymptotic upper bounds, with the latter ignoring logarithmic factors.
Let $\domain \subset \mathbb{R}^d$ represent the $d$-dimensional decision set, and let $\|\cdot\|$ denote the Euclidean norm. The distance between an element $\x \in \domain$ and a set $S \subset \domain$ is defined as $d(\x, S) = \inf_{\x' \in S} \|\x - \x'\|$. Let $\X_t = \set{\x_i}_{i=1}^{t}, t\in \mathbb{Z}_+$ denotes a sequence of points. For any $\x \in \X_t$, let $[\x] = \set{\x' \in \X_t : \x' = \x}$ represent the equivalence class of $\x$, and let $\#[\x]$ denote the cardinality of this equivalence class.




\subsection{Problem formulation}

Let $f$ be a black-box function defined over a $d$-dimensional decision set $\domain$. The optimization problem is formulated as follows:
\begin{equation}
    \x^{\ast} = \argmax_{\x \in \domain} f(\x),
\end{equation}
where $f: \domain \to \mathbb{R}$ may be a non-convex function that lacks an explicit expression and can only be evaluated through noisy samples. In particular, the observations available are of the form $y = f(\x) + \varepsilon$, where $\varepsilon$ represents the sampling noise.
The instantaneous regret at the $t$-th iteration $(t \in \mathbb{Z}_+)$ of a sequential optimization method is defined as
\begin{equation}
    r_t = f(\x^\ast) - f(\x_t),
\end{equation}
where $\x_t$ is the sampled point at iteration $t$.
Given a time horizon $T$, the goal of an optimization algorithm is to minimize the following metrics:
\begin{itemize}
    \item The \textit{cumulative regret}, defined as
        \begin{equation}
            \cumregret_T \coloneqq \sum_{t=1}^{T} r_t = \sum_{t=1}^{T} \left[f(\x^\ast) - f(\x_t)\right].
        \end{equation}
    \item The \textit{simple regret}, defined as
        \begin{equation}
            \simregret_T \coloneqq \min_{1 \leq t \leq T} r_t = f(\x^\ast) - \max_{1 \leq t \leq T} f(\x_t).
        \end{equation}
\end{itemize}




\subsection{Bayesian optimization}

\begin{algorithm}[!htb]
    \caption{Bayesian Optimization}
    \label{alg:BO}
    \KwIn{Initial dataset $\D_{t} = \set{(\x_i, y_i)}_{i=1}^{t}$, budget $T$.}
    \While{$t < T$}{
        Select $\x_{t+1}$ by maximizing the acquisition function over $\domain$. \;
        Evaluate the objective function: $y_{t+1} = f(\x_{t+1}) + \varepsilon_{t+1}$. \;
        Include $(\x_{t+1}, y_{t+1})$ in $\D_{t}$, and then increment $t$. \;
    }
\end{algorithm}

We focus on the BO algorithm, which utilizes an acquisition function to balance the trade-off between exploitation and exploration. This balance is critical for determining a sequence of query points that progressively approach the optimal solution of the optimization problem.
The pseudocode is presented in Algorithm \ref{alg:BO}.
The algorithm consists of two primary steps:
First, a surrogate model is constructed, and the corresponding acquisition function is generated.
Second, a new sample is obtained by maximizing the acquisition function, followed by updating the surrogate model.
Starting with the initial design, these steps are iteratively repeated to efficiently allocate the remaining evaluation budget and minimize the cumulative regret or simple regret.




\section{The BOKE Method}
\label{sec:methodology}

The BOKE algorithm consists of three main components:
first, a kernel regression (KR) surrogate model;
second, a density-based exploration (DE) strategy constructed using kernel density estimation (KDE);
and third, the improved kernel regression upper confidence bound (IKR-UCB) criterion.
The remainder of this section provides a detailed description of these components, followed by the presentation of the resulting BOKE algorithm and an analysis of its computational complexity.




\subsection{Kernel regression surrogate function}
\label{sec:kernel regression surrogates}

Kernel regression, also known as the Nadaraya-Watson estimator \cite{nadaraya1964estimating, watson1964smooth}, is a widely recognized method in nonparametric regression. In KR, the expected value at a given point is estimated as a weighted average of the observed dataset values, with weights determined by a kernel function $k(\x, \x'; h)$. This kernel function depends on the distance between the data points and the target point, as well as a positive bandwidth parameter $h$.
Given observations $[\X_t, \y_t]$, the predicted expected value for any $\x \in \domain$ is computed as
\begin{equation}
    \label{eqn:KR}
    m_t(\x) = m(\x; \X_t, \y_t)
    \coloneqq \dfrac{\sum_{i=1}^{t} k(\x, \x_i) y_i}{\sum_{i=1}^{t} k(\x, \x_i)}.
\end{equation}
Here, the kernel function is typically assumed to be stationary and exhibit smoothness and symmetry. Common examples of kernel functions include:
\begin{align}
    \text{(Gaussian)}
    & \quad k(\x, \x'; h) = \exp\left(- \dfrac{\| \x - \x' \|^2}{2 h^2} \right),
    \label{eqn:Gaussian kernel} 
    \\
    \text{(Epanechnikov)}
    & \quad k(\x, \x'; h) = \max\set{1 - \dfrac{\| \x - \x' \|^2}{h^2}, 0},
    \label{eqn:Epanechnikov kernel} 
    \\
    \text{(Uniform)}
    & \quad k(\x, \x'; h) = \1{\set{\| \x - \x' \| \le h}}.
    \label{eqn:Uniform kernel}
\end{align}
Note that for kernels with compact support, the denominator in the KR expression may approach zero, which necessitates a modification of the predicted value. A detailed discussion of this issue is deferred to \cref{sec:convergence analysis}.


Kernel regression has been applied in various fields. For instance, it has been used within the MCTS framework for information sharing between nodes to address execution uncertainty \cite{yee2016monte}. It also serves as a foundational estimator in continuum-armed bandit problems \cite{agrawal1995continuum}. Furthermore, recent research \cite{chen2024pseudo} has integrated local kernel regression with uncertainty quantification to design effective acquisition functions, leading to the development of a pseudo-Bayesian optimization algorithm.


We adapt kernel regression as the surrogate model for our BOKE algorithm and analyze the asymptotic behavior of the kernel regression predictor as the bandwidth parameter $h$ approaches zero. For a comprehensive illustration of the KR surrogate model, a one-dimensional example described in Eq.~\eqref{eqn:test} over the domain $[0, 1]$ is tested, and the results are displayed in \cref{fig:kr}. The middle panel shows the case with an optimal bandwidth selected via cross-validation.
\begin{equation}
    \label{eqn:test}
    f(x) = - e^{-1.4 x} \cos(3.5 \pi x).
\end{equation}


As shown in \cref{fig:kr}, the KR effectively approximates the shape of the test function as long as the bandwidth is not excessively large. Specifically, when the bandwidth is extremely small, the KR estimation approximates a step-function interpolation, converging to a nearest-neighbor estimator. This illustrates the robustness of the KR predictor against bandwidth misspecification, as demonstrated in \cref{pro:asymptotic KR}. Moreover, this result holds for other power exponential kernels, such as exponential and Laplacian kernels.
For the GP predictor, as the bandwidth $h$ approaches zero, it converges to a summation of weighted delta functions, as detailed in \cref{pro:asymptotic GPR} in \cref{sec:proofs of asymptotic properties}. The proof ideas for these two propositions are inspired by \cite{kang2016kernel}.


\begin{figure}[H]
    \centering
    \includegraphics[width=.98\textwidth]{figures/fig-kr.pdf}
    \caption{KR with Gaussian kernels at different smoothing bandwidths $h$. The objective function in Eq.~\eqref{eqn:test} is sampled 10 times, with each sample corrupted by noise $\varepsilon \sim \mathcal{N}(0, 0.01)$.}
    \label{fig:kr}
\end{figure}


\begin{proposition}
    \label{pro:asymptotic KR}
    Given the dataset $[\X_t, \y_t] = \set{(\x_i, y_i)}_{i=1}^{t}$ and a Gaussian kernel function, the following holds:
    \begin{equation}
        \label{eqn:KR extended}
        \lim_{h \to 0^+} m_t(\x)
        = \dfrac{\sum_{i=1}^{t} y_i \1{\set{\|\x - \x_i\| = d(\x, \X_t)}}}{\sum_{i=1}^{t} \1{\set{\|\x - \x_i\| = d(\x, \X_t)}}}.
    \end{equation}
\end{proposition}

\begin{proof}
    $m_t(\x)$ in Eq.~\eqref{eqn:KR} can be rewritten as
    \[
        m_t(\x) = \dfrac{\sum_{i=1}^{t} y_i \exp\left(- \left(\|\x - \x_i\|^2 - d^2(\x, \X_t)\right) / (2h^2) \right)}{\sum_{i=1}^{t} \exp\left(- \left(\|\x - \x_i\|^2 - d^2(\x, \X_t)\right) / (2h^2) \right)}.
    \]
    Let $\Lambda = \set{i: \|\x - \x_i\| = d(\x, \X_t)}$, then if $i \notin \Lambda$, $\|\x - \x_i\|^2 - d^2(\x, \X_t) > 0$.
    Thus,
    \[
        \lim_{h \to 0^+} m_t(\x)
        = \dfrac{\sum_{i\in \Lambda} y_i}{\sum_{i\in \Lambda} 1}
        = \dfrac{\sum_{i=1}^{t} y_i \1{\set{\|\x - \x_i\| = d(\x, \X_t)}}}{\sum_{i=1}^{t} \1{\set{\|\x - \x_i\| = d(\x, \X_t)}}}.
    \]
\end{proof}




\subsection{Density-based exploration}
\label{sec:density-based exploration}

Kernel density estimation \cite{rosenblat1956remarks}, also known as Parzen's window \cite{parzen1962estimation}, is a widely-used nonparametric method for estimating probability density function. In the context of reinforcement learning, KDE is employed to design count-based exploration bonuses that encourage exploration of the action space \cite{bellemare2016unifying}. In Bayesian optimization, the authors of \cite{hase2018phoenics} utilized Bayesian kernel density estimation to construct an acquisition function.
Formally, KDE can be expressed as:
\begin{equation}
    \label{eqn:KDE}
    W_t(\x) =W(\x; \X_t) \coloneqq \sum_{i=1}^{t} k(\x, \x_i),
\end{equation}
where we omit the normalization factor, allowing $W_t$ to scale with $t$.
The asymptotic behavior of KDE follows directly from its definition, which is $\lim_{h \to 0^+} W_t(\x) = \sum_{i=1}^{t} \1{\set{\x = \x_i}}$.
Moreover, the GP posterior variance also converges to the empirical density, as shown in \cref{pro:asymptotic GP Var} in \cref{sec:proofs of asymptotic properties}.
Thus, both $W_t(\x)$ and GP posterior variance can serve as exploration terms to mitigate uncertainty.


For illustration of KDE with different bandwidths, thirty samples are independently drawn from a beta mixture model $p(x) = 0.7 \, \mathrm{Beta}(3, 8) + 0.3 \, \mathrm{Beta}(10, 2)$. The corresponding KDE is calculated and compared with the analytical beta mixture model, see \cref{fig:kde}. The bandwidth of the middle panel is selected by the maximum likelihood cross-validation method and it captures the two peaks of the analytical model. For the left panel, an excessively small bandwidth $h$ is used and the density curve behaves like a delta function, indicating quite weak exploration. Generally, regions with higher density are likely to be sampled, and thus have lower uncertainty, whereas regions with lower density exhibit higher uncertainty and need more exploration.


\begin{figure}[H]
    \centering
    \includegraphics[width=.98\textwidth]{figures/fig-kde.pdf}
    \caption{
        Normalized KDE, $\widehat{p}(x)$, with Gaussian kernels at different smoothing bandwidths; more precisely, $\widehat{p}(x) = (2 \pi)^{-1/2} (ht)^{-1} W(x)$.
    }
    \label{fig:kde}
\end{figure}


The \textit{fill distance} is commonly used to measure how well a sample sequence covers the entire research space. Here, we conduct space-filling design by framing the problem as follows:
\begin{equation}
    \label{eqn:KDE pure-explore}
    \x_{t+1} = \argmin_{\x \in \domain}~W_t(\x).
\end{equation}
This means that a sample sequence $\X_T$ is generated by sequentially minimizing the KDE function. The fill distance is then calculated as:
\begin{equation}
    \label{eqn:fill distance}
    h(\domain, \X_t) \coloneqq \sup_{\x \in \domain} \inf_{\x_i \in \X_t} \| \x - \x_i \|.
\end{equation}
The Gaussian process-based exploration algorithm introduced in \cite{wenzel2021novel} selects query points by maximizing the posterior predictive standard deviation at each iteration. The query sequence generated by this algorithm exhibits a fill distance decay rate of $\Theta(t^{-\frac{1}{d}})$, which is consistent with other uniform sampling methods, such as uniform sampling and Latin hypercube sampling (LHS) \cite{mckay1979comparison}.
In \cref{fig:fill distance}, we plot the fill distance of density-based exploration, Eq.~\eqref{eqn:KDE pure-explore}, alongside the aforementioned algorithms. As shown, the fill distances of KDE and GP closely overlap, especially as the dimensionality increases. This demonstrates that KDE is an effective exploration method for uniformly filling the space.

\begin{figure}[H]
    \centering
    \includegraphics[width=.98\textwidth]{figures/fig-fill_distance.pdf}
    \caption{
        Fill distances of different space-filling design methods across various dimensions. Each curve represents the mean result from 100 random seeds.
        Both axes are displayed using a logarithmic scale.
    }
    \label{fig:fill distance}
\end{figure}




\subsection{Confidence bound criteria}
\label{sec:confidence bound criteria}

In bandit optimization, the UCB algorithm is commonly used to select the most optimistic ``arm" at each step \cite{auer2002finite}.
In the context of global optimization, confidence bound criteria are used to determine the next query point based on Gaussian processes \cite{cox1992statistical,cox1997sdo,srinivas2010gaussian}.
Recently, the KR-UCB algorithm extended the vanilla UCB to continuous action spaces using kernel regression, enabling information sharing between nodes \cite{yee2016monte}. For more details, see \cref{sec:KR-UCB}.


In this work, we propose a novel acquisition function for global optimization, termed the Improved Kernel Regression UCB (IKR-UCB). The next query point is determined by maximizing this acquisition function, as defined in Eq.~\eqref{eqn:IKR-UCB}:
\begin{equation}
    \label{eqn:IKR-UCB}
    \x_{t+1} = \argmax_{\x \in \domain}~m_t(\x) + \sqrt{\beta_t} \widehat{\sigma}_t(\x),
\end{equation}
where $t$ denotes the number of existing queried points, $\beta_t$ is a tuning parameter that varies with $t$, $\widehat{\sigma}_t(\x)$ is the density-based exploration term as defined in Eq.~\eqref{eqn:DE},
\begin{equation}
    \label{eqn:DE}
    \widehat{\sigma}_t(\x) = \widehat{\sigma}(\x; \X_t) \coloneqq W^{-1/2}(\x; \X_t).
\end{equation}


In \cref{fig:acquisition}, we conduct a simple test for optimizing the scalar function defined in Eq.~\eqref{eqn:test}, using the same settings described therein. The sixth query point is chosen by maximizing the IKR-UCB acquisition function. As shown, the acquisition function attains high values in regions where the kernel regression predicts a high surrogate mean (exploitation) and where the predictive uncertainty is substantial (exploration).


\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{figures/fig-acquisition.pdf}
    \caption{Illustration of the IKR-UCB acquisition function. The gray dashed line represents the analytical function, while the blue curve shows the KR approximation of the objective function. The upper light blue shaded area indicates the uncertainty quantification based on kernel density estimation. The lower shaded plot visualizes the acquisition function.
    }
    \label{fig:acquisition}
\end{figure}

With the proposed IKR-UCB, the global optimization algorithm can be directly formulated. Starting with an initial design, the next query point is determined by maximizing the IKR-UCB acquisition function in Eq.~\eqref{eqn:IKR-UCB}. This iterative process continues until a predefined iteration tolerance $T$ is reached. The complete BOKE algorithm is detailed in Algorithm~\ref{alg:BOKE}.

\begin{algorithm}[!htb]
    \caption{BOKE algorithm}
    \label{alg:BOKE}
    \KwIn{Initial dataset $\D_t$, budget $T$, kernel $k$, tuning parameters $\set{\beta_i}_{i=t}^T$.}
    \While{$t \le T$}{
        Compute $m_t$ and $W_t$ for $\x \in \domain$ by Eq.~\eqref{eqn:KR} and Eq.~\eqref{eqn:KDE}. \;
        Obtain $\x_{t+1}$ by solving Eq.~\eqref{eqn:IKR-UCB}. \;
        Evaluate $y_{t+1} = f(\x_{t+1}) + \varepsilon_{t+1}$, then increment $t$.
    }
\end{algorithm}


However, according to the rule-of-thumb bandwidth settings described in \cref{sec:experiments}, the convergence rate of kernel regression may be constrained by the slow convergence of the bandwidth (e.g., $\Theta(t^{-\frac{1}{d+4}})$), as detailed in \cref{thm:pointwise consistency KR}. Consequently, BOKE may favor exploration over exploitation during the early iterations, potentially leading to excessive exploration cost for problems with sparse local optima.
To improve its performance, we consider the pure-exploitation acquisition function defined in Eq.~\eqref{eqn:KR pure-exploit}:
\begin{equation}
    \label{eqn:KR pure-exploit}
    \x_{t+1} = \argmax_{\x \in \domain}~m_t(\x),
\end{equation}
and switch between these two strategies stochastically. This modified algorithm, referred to as BOKE+, is presented in Algorithm~\ref{alg:BOKE+}.
The optimization results of BOKE and BOKE+ are tested on the toy problem defined in Eq.~\eqref{eqn:test}, with the corresponding results shown in \cref{fig:iteration}. Both methods begin with the same initial set of five points. Notably, BOKE+ focuses sampling on regions with high objective values, while BOKE maintains a more balanced trade-off between exploration and exploitation.

\begin{figure}[H]
    \centering    \includegraphics[width=.98\textwidth]{figures/fig-iteration.pdf}
    \caption{Comparison of BOKE (top two rows) and BOKE+ (bottom two rows) on the one-dimensional problem defined in Eq.~\eqref{eqn:test}. The gray dashed line represents the analytical function, while the blue curve shows the KR approximation of the objective function. The shaded plot visualizes the acquisition function.
    }
    \label{fig:iteration}
\end{figure}


\begin{algorithm}[!htb]
    \caption{BOKE+ algorithm}
    \label{alg:BOKE+}
    \KwIn{Initial dataset $\D_t$, budget $T$, kernel $k$, tuning parameters $p \in (0, 1]$ and $\set{\beta_i}_{i=t}^T$.}
    \While{$t \le T$}{
        Compute $m_t$ and $W_t$ for $\x \in \domain$ by Eq.~\eqref{eqn:KR} and Eq.~\eqref{eqn:KDE}. \;
        Draw $q \sim \mathrm{Bernoulli}(p)$. \;
        Obtain $\x_{t+1}$ by solving Eq.~\eqref{eqn:IKR-UCB} if $q = 1$, otherwise Eq.~\eqref{eqn:KR pure-exploit}. \;
        Evaluate $y_{t+1} = f(\x_{t+1}) + \varepsilon_{t+1}$, then increment $t$.
    }
\end{algorithm}




\subsection{Computational complexity}
\label{sec:computational complexity}

In Bayesian optimization, the primary computational cost arises from optimizing the acquisition function. This process can be divided into two components: preprocessing and inference. Preprocessing involves initial calculations that depend only on the queried points, while inference entails evaluating the acquisition function for an unknown point $\x$ after preprocessing.


For BOKE, since the weights of the surrogate mean depend on the unknown $\x$, no preprocessing is required, resulting in a complexity of $\order(1)$. For KR-UCB, the complexity is $\order(t^2)$ due to the determination of the optimal point among the queried points by maximizing the KR-UCB formula. In contrast, for GP-UCB, the calculation comes from the Cholesky decomposition of kernel matrix \cite{williams2006gaussian}, leading to a complexity of $\order(t^3)$.
The inference complexity of both BOKE and KR-UCB arises solely from the multiplication of smoothing weights with observations, incurring a cost of $\order(t)$. For GP-UCB, each inference requires solving a triangular system.
resulting in a time complexity of $\order(t^2)$.


Let $m$ denote the number of evaluations for the acquisition function. Over $T$ iterations, BOKE achieves a total computational complexity of $\order(mT^2)$. KR-UCB has a total complexity of $\order(T^3+mT^2)$, while GP-UCB incurs the highest computational cost of $\order(T^4+mT^3)$, see \cref{tab:computational complexity}. However, in practice, where $m \gg T$, KR-UCB typically exhibits faster computation than BOKE due to its lower computational complexity constant, with BOKE+ showing intermediate performance between the two.


\begin{table}[!htb]
    \caption{Computational complexity of different methods.}
    \label{tab:computational complexity}
    \centering
    \begin{tabular}{cccc}
        \toprule[1.5pt]
        \textbf{Algorithm} & \textbf{Preprocessing} & \textbf{Inference} & \textbf{Total complexity} \\
        \midrule[1pt]
        BOKE \& BOKE+      & $\order(1)$            & $\order(t)$        & $\order(m T^2)$           \\
        KR-UCB             & $\order(t^2)$          & $\order(t)$        & $\order(T^3 + m T^2)$     \\
        GP-UCB             & $\order(t^3)$          & $\order(t^2)$      & $\order(T^4 + m T^3)$     \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}




\section{Convergence Analysis}
\label{sec:convergence analysis}

To obtain our main results, we impose the following assumptions.
\begin{assumption}
    \label{asm:domain}
    The domain $\domain \subset \mathbb{R}^d$ is compact and convex.
\end{assumption}

\begin{assumption}
    \label{asm:objective}
    The unknown objective function $f$ is continuous.
\end{assumption}

\begin{assumption}
    \label{asm:kernel}
    The kernel function
    \begin{equation}
        k(\x, \x'; h) = \Psi\left(\dfrac{\x - \x'}{h}\right)
    \end{equation}
    is symmetric for any $\x, \x' \in \domain$ with bandwidth $h > 0$. Here, $\Psi: \mathbb{R}^d \to \mathbb{R}_{\ge 0}$ is a non-negative continuous function that vanishes outside the closed ball which centers at the origin and has radius $R$.
    In addition, we assume that $\Psi(0) \equiv 1$ and there exists a constant $M_{\Psi} > 0$ such that $\sup_{\x} \Psi (\x) \le M_{\Psi}$.
\end{assumption}

\begin{assumption}
    \label{asm:noise}
    The observable errors $\set{\varepsilon_t: t \ge 1}$ are independent sub-Gaussian random variables with zero mean and parameter $\varsigma > 0$, such that
    \begin{equation}
        \E \left[ \exp(\lambda \varepsilon_t) \right] \le \exp\left( \frac{\varsigma^2 \lambda^2}{2} \right) \quad \text{for all} \quad \lambda \in \mathbb{R}.
    \end{equation}
\end{assumption}

\cref{asm:kernel} requires the kernel to have compact support, which is a mild assumption that allows us to control the scope of kernels \cite{linke2023towards}.
All subsequent arithmetic operations are considered on the extended real line, following standard conventions like $c / \infty = 0$ and $c / 0 = \infty$ for any $c \in \mathbb{R}_+$.

For the following results, we introduce an auxiliary dataset $\tilde{\A}_t = \set{(\x_i, y_i), \dots} \subset \domain \times \mathbb{R}$, and its projection onto the first coordinate, $\A_t = \set{\x_i, \dots} \subset \domain$, which have two meanings throughout the paper. When explicitly defined, the subscript represents the number of elements in the set. When not explicitly defined, the notation refers to an arbitrary finite set.




\subsection{Pointwise consistency of KR}
\label{sec:pointwise consistency of KR}

Under Assumption~\ref{asm:kernel}, the definition of KR in Eq.~\eqref{eqn:KR} is well-defined as long as the denominator $W_t(\x) > 0$, i.e., there exists some $\x_i \in \X_t$ such that $k(\x, \x_i; h) > 0$. Otherwise, the KR approximation must be specified.
Here, we set the default KR approximation to be the nearest neighbor prediction of $\x$ in $\X_t$, as described in \cref{pro:asymptotic KR}. This extension ensures the continuity of the KR surrogate model, except on a set of measure zero, and is commonly used in practice due to machine precision limitations. We show that the pointwise estimation error of KR can be bounded by the exploration term and the modulus of uniform continuity of the unknown objective function, as demonstrated in \cref{thm:pointwise consistency KR}. The proof follows the approach in \cite{linke2023towards}, with necessary adjustments to accommodate a specific upper bound form, distinguishing our method from existing work.


\begin{theorem}
    \label{thm:pointwise consistency KR}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:noise} hold. Let $\delta \in (0, 1)$ and $h > 0$. Then, for any $\x \in \domain$ and finite dataset $\tilde{\A}_t = \set{(\x_i, y_i) : i = 1, \dots, t} \subset \domain \times \mathbb{R}$, the following holds:
    \begin{equation}
        \Prob \left[ \left|f(\x) - m(\x; \tilde{\A}_t) \right| \le \omega_f(h) + \sqrt{\dfrac{2 \varsigma^2 M_{\Psi} \log(2/\delta)}{W(\x; \A_t)}} \right] \ge 1 - \delta,
    \end{equation}
    where $\displaystyle \omega_f(h) = \sup_{\substack{\x, \x' \in \domain \\ \| \x - \x' \| \le Rh}} |f(\x) - f(\x')|$ is the modulus of uniform continuity.
\end{theorem}

\begin{proof}
    By the definition of KR in Eq.~\eqref{eqn:KR}, the estimation error can be decomposed as:
    \[
        |f(\x) - m(\x; \tilde{\A}_t)| \le |f(\x) - I(\x)| + |J(\x)|,
    \]
    where $I(\x) = W^{-1}(\x; \A_t) \sum_{i=1}^{t} k(\x, \x_i) f(\x_i)$ and $J(\x) = W^{-1}(\x; \A_t) \sum_{i=1}^{t} k(\x, \x_i) \varepsilon_i$.
    For the first term, we have
    \[
        |f(\x) - I(\x)|
        = \left| \dfrac{\sum_{i=1}^{t} k(\x, \x_i) (f(\x) - f(\x_i))}{\sum_{i=1}^{t} k(\x, \x_i)} \right|
        \le \omega_f(h),
    \]
    where the last inequality follows from \cref{asm:kernel}.
    For the second term, by the boundedness of kernel function, and the Hoeffding bound for sub-Gaussian variables, we have
    \[
        \Prob \left[ |J(\x)| \ge \lambda \right]
        \le 2 \exp \left( - \dfrac{\lambda^2 W^2(\x; \A_t)}{2 \varsigma^2 \sum_{i=1}^{t} k^2(\x, \x_i)} \right)
        \le 2 \exp \left( - \dfrac{\lambda^2 W(\x; \A_t)}{2 \varsigma^2 M_{\Psi}} \right),
        \quad \forall \lambda \ge 0.
    \]
    Let $\lambda = \sqrt{\frac{2 \varsigma^2 M_{\Psi} \log(2/\delta)}{W(\x; \A_t)}}$, then with probability at least $1 - \delta$, it holds that
    \[
        |f(\x) - m(\x; \tilde{\A}_t)| \le \omega_f(h) + \sqrt{\dfrac{2 \varsigma^2 M_{\Psi} \log(2/\delta)}{W(\x; \A_t)}}.
    \]
    This completes the proof.
\end{proof}




\subsection{Algorithmic consistency}
\label{sec:algorithmic consistency}

The \textit{sequential no-empty-ball} (SNEB) property is first introduced in \cite{chen2024pseudo}, which extends the NEB condition in \cite{vazquez2010convergence} and can be used to prove the consistency of a family of algorithms.
In this section, we first provide the SNEB property of IKR-UCB in \cref{lem:SNEB IKR-UCB}. Then, the stronger global convergence results for BOKE and BOKE+ are derived in \cref{thm:consistency BOKE} and \cref{cor:consistency BOKE+} by introducing step-dependent bandwidths. Additionally, the SNEB property of DE estimator, pure exploration, is given in \cref{lem:SNEB DE} and its algorithmic consistency is displayed in \cref{cor:consistency DE}, see more details in \cref{sec:proofs of algorithmic consistency}.
Notably, some proof techniques in \cite{chen2024pseudo, previtali2023glisp} for the noiseless problem have been used.


For the SNEB property of IKR-UCB acquisition function, we consider its rescaled form
\begin{equation}
    a_t(\x) = a(\x; \X_t, \y_t) \coloneqq \beta_t^{-1/2} m(\x; \X_t, \y_t) + \widehat{\sigma}(\x; \X_t),
\end{equation}
where $\beta_t > 0$ ensures the preference of points remains unchanged compared with the unscaled IKR-UCB function. The SNEB property for IKR-UCB is displayed in \cref{lem:SNEB IKR-UCB}, and its proof is provided in \cref{sec:proofs of algorithmic consistency}.


\begin{lemma}[SNEB property of IKR-UCB]
    \label{lem:SNEB IKR-UCB}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:noise} hold, and $\set{\beta_t > 0 : t \in \mathbb{Z}_+}$ satisfies $\beta_t \to \infty$ as $t \to \infty$. For any fixed $h > 0$, the following hold:
    \begin{enumerate}[(a)]
        \item\label{it1:SNEB IKR-UCB} For any $\x \in \domain$ and finite dataset $\tilde{\A}_t = \set{(\x_i, y_i) : i = 1, \dots, t} \subset \domain \times \mathbb{R}$, if $\inf_{t} d(\x, \A_t) > R h$, then $a(\x; \tilde{\A}_t) \pto \infty$ as $t \to \infty$.
        
        \item\label{it2:SNEB IKR-UCB} For any convergent sequence $\set{\x_t} \subset \domain$ and finite dataset $\tilde{\A}_t \supset \set{(\x_i, y_i) : i = 1, \dots, t}$, $a(\x_t; \tilde{\A}_{t-1}) \pto 0$ as $t \to \infty$.
    \end{enumerate}
\end{lemma}


With the above lemma, the algorithmic consistency of BOKE is provided in \cref{thm:consistency BOKE}.


\begin{theorem}[Algorithmic consistency of BOKE]
    \label{thm:consistency BOKE}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:noise} hold, and $\set{\beta_t > 0 : t \in \mathbb{Z}_+}$ satisfies $\beta_t \to \infty$ as $t \to \infty$. Let $\X_t = \set{\x_1, \dots, \x_t}$ be the BOKE iterates, and $h_t$ denote the corresponding bandwidth. The following hold:
    \begin{enumerate}[(i)]
        \item\label{it1:consistency BOKE} For any fixed $h_t \equiv h$, $\domain$ is eventually populated by the BOKE iterates within $\order(h)$ gaps, i.e., $\inf_t h(\domain, \X_t) \le R h$ holds almost surely. If $f$ is $L$-Lipschitz, BOKE is algorithmically consistent with $\order(h)$ error, i.e., $\Prob\left[ \limsup_{T \to \infty} \simregret_T \le L R h \right] = 1$.

        \item\label{it2:consistency BOKE} There exists $\set{h_t : t \in \mathbb{Z}_+} \to 0$, such that $\domain$ is eventually populated by the BOKE iterates, i.e., $\inf_t h(\domain, \X_t) = 0$ holds almost surely. Thus, BOKE is algorithmically consistent, i.e., $\simregret_T \asto 0$ as $T \to \infty$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    For part \itref{it1:consistency BOKE}, suppose for contradiction that event $\mathcal{E}_1 = \set{\inf_{t} h(\domain, \X_t) > R h}$ occurs with a positive probability, i.e., $\Prob[\mathcal{E}_1] > 0$. Conditioned on $\mathcal{E}_1$, by the monotonicity of $h(\domain, \X_t)$ with respect to $t$, we can rewrite the assumption as $\lim_{t \to \infty} h(\domain, \X_t) > Rh$. Thus, there exists $\epsilon > 0$ and $T \in \mathbb{Z}_+$, such that for any $t \ge T$, $h(\domain, \X_t) \ge Rh + \epsilon$. Let $F_t = \set{\x \in \domain : d(\x, \X_t) \ge Rh + \epsilon}$. By the continuity of $d(\x, \X_t)$ with respect to $\x$ and the compactness of $\domain$, $F_t$ is closed and non-empty. Since $d(\x, \X_t) \ge d(\x, \X_{t+1})$, we have $\domain \supset F_t \supset F_{t+1}$. By the Nested Set Theorem, $\cap_{t=T}^{\infty} F_t \neq \emptyset$. So there exists $\x' \in \cap_{t=T}^{\infty} F_t$, such that for any $t \ge T$, $d(\x', \X_t) \ge Rh + \epsilon$. Hence, $\inf_t d(\x', \X_t) = \lim_{t \to \infty} d(\x', \X_t) > Rh$.
    
    By the compactness of $\domain$, $\set{\x_t}$ has a convergent subsequence $\set{\x_{t_n}}$.
    By \cref{lem:SNEB IKR-UCB}\itref{it2:SNEB IKR-UCB}, we have $a(\x_{t_n}; \D_{t_n - 1}) \pto 0$ as $n \to \infty$. Since convergence in probability implies convergence almost surely on a subsequence, without loss of generality, assume $a(\x_{t_n}; \D_{t_n - 1}) \asto 0$ as $n \to \infty$.
    Let $\mathcal{E}_2 = \set{\lim_{n \to \infty} a(\x_{t_n}; \D_{t_n - 1}) = 0}$, then $\Prob[\mathcal{E}_2] = 1$. By the union bound, we have $\Prob[\mathcal{E}_1 \cap \mathcal{E}_2] \ge \Prob[\mathcal{E}_1] + \Prob[\mathcal{E}_2] - 1 > 0$. Conditioned on $\mathcal{E}_1 \cap \mathcal{E}_2$, by \cref{lem:SNEB IKR-UCB}\itref{it1:SNEB IKR-UCB}, we have $a(\x'; \D_t) \pto \infty$ as $t \to \infty$. Thus, for any $\epsilon > 0$ and $\lambda > 0$, there exists $N(\epsilon, \lambda) > 0$ such that for all $n > N$, we have both
    \[
        a(\x_{t_n}; \D_{t_n - 1}) < \lambda
    \]
    and
    \[
        \Prob\left[ a(\x'; \D_{t_n - 1}) > 2 \lambda \mid \mathcal{E}_1 \cap \mathcal{E}_2 \right] > 1 - \epsilon.
    \]
    Therefore,
    \[
        \Prob\left[ a(\x'; \D_{t_n - 1}) > a(\x_{t_n}; \D_{t_n - 1}) + \lambda \mid \mathcal{E}_1 \cap \mathcal{E}_2 \right] > 1 - \epsilon.
    \]
    By Bayes' rule, and let $n \to \infty$, we have
    \[
        \liminf_{n \to \infty} \Prob\left[ a(\x'; \D_{t_n - 1}) > a(\x_{t_n}; \D_{t_n - 1}) + \lambda \right] 
        \ge (1 - \epsilon) \Prob[\mathcal{E}_1 \cap \mathcal{E}_2] > 0.
    \]
    However, by the IKR-UCB criterion $\x_{t_n} \in \argmax_{\x \in \domain} a(\x; \D_{t_n - 1})$, we have
    \[
        \Prob\left[ a(\x'; \D_{t_n - 1}) > a(\x_{t_n}; \D_{t_n - 1}) + \lambda \right] = 0, \quad \forall n > 0,
    \]
    which leads to a contradiction. We conclude that $\Prob[\mathcal{E}_1] = 0$, hence $\inf_{t} h(\domain, \X_t) \le R h$ holds almost surely.
    
    To complete part \itref{it1:consistency BOKE}, we apply the above conclusion for $\x^{\ast} \in \argmax_{\x \in \domain} f(\x)$. With probability $1$, there exists a subsequence $\set{\x_{t_n}} \subseteq \set{\x_t}$ such that $d(\x^{\ast}, \X_{t_n}) \le h(\domain, \X_{t_n}) \le R h + n^{-1}$. Since $\X_{t_n}$ is finite, there exists $\x \in \X_{t_n}$ such that $d(\x^{\ast}, \X_{t_n}) = \| \x^{\ast} - \x \|$. Without loss of generality, assume $\| \x^{\ast} - \x_{t_n} \| \le R h + n^{-1}$. Therefore,
    \[
        \simregret_{t_n} \le r_{t_n} = f(\x^{\ast}) - f(\x_{t_n}) \le L(Rh + n^{-1}).
    \]
    Since simple regret is non-increasing, let $n \to \infty$, we have $0 \le \limsup_{t \to \infty} \simregret_t \le L R h$. Thus, we conclude that
    \[
        \Prob\left[ \limsup_{t \to \infty} \simregret_t \le L R h \right] 
        \ge \Prob\left[ \inf_{t} h(\domain, \X_t) \le R h \right] = 1,
    \]
    which completes the proof for part \itref{it1:consistency BOKE}.


    For part \itref{it2:consistency BOKE}, the proofs are based on \itref{it1:consistency BOKE}. For any $h > 0$, with probability $1$, we have $\inf_t h(\domain, \X_t) \le R h$. Then the monotonicity of $h(\domain, \X_t)$ gives $\lim_{t \to \infty} h(\domain, \X_t) \le R h$. Thus, for $\hbar_1 = 1$, there exists $t_1 \in \mathbb{Z}_+$ such that $h(\domain, \X_{t_1}) \le 2 R \hbar_1$, where $\x_2, \dots, \x_{t_1}$ only depends on $\x_1$ and $\hbar_1$. Then for $\hbar_2 = \frac{1}{2}$, there exists $t_2 > t_1$ such that $h(\domain, \X_{t_2}) \le 2 R \hbar_2$, where $\x_{t_1 + 1}, \dots, \x_{t_2}$ depends on $\X_{t_1}$ and $\hbar_2$. By repeating this procedure, we have $\hbar_n = \frac{1}{n} \to 0$ and $t_n \uparrow \infty$, such that $\mathcal{E}_n = \set{h(\domain, \X_{t_n}) \le 2 R \hbar_n}$ with $\Prob[\mathcal{E}_n] = 1$. Hence, $\Prob\left[\cap_{n=1}^{\infty} \mathcal{E}_n\right] = 1 - \Prob\left[\cup_{n=1}^{\infty} \mathcal{E}_n^{\complement}\right] = 1$. Conditioned on $\cap_{n=1}^{\infty} \mathcal{E}_n$, and let $n \to \infty$, we have
    \[
        \lim_{n \to \infty} h(\domain, \X_{t_n}) \le 2 R \lim_{n \to \infty} \hbar_n = 0.
    \]
    Therefore,
    \[
        \Prob\left[ \inf_t h(\domain, \X_t) = \lim_{t \to \infty} h(\domain, \X_t) = 0 \right]
        \ge \Prob\left[\cap_{n=1}^{\infty} \mathcal{E}_n\right] = 1.
    \]
    We conclude that $\inf_t h(\domain, \X_t) = 0$ holds almost surely, and the bandwidth sequence $h_t \to 0$ we built is
    \[
        h_t = \hbar_n \quad \text{if} \quad t_{n-1} < t \le t_n, \quad \text{where} \quad t_0 = 0.
    \]
    Then we apply the above conclusion for $\x^{\ast} \in \argmax_{\x \in \domain} f(\x)$. With probability $1$, there exists a subsequence $\set{\x_{t_n}} \to \x^{\ast}$ as $n \to \infty$, such that $d(\x^{\ast}, \X_{t_n}) \le h(\domain, \X_{t_n}) \to 0$. Without loss of generality, assume $\| \x^{\ast} - \x_{t_n} \| \to 0$ as $n \to \infty$. Thus,
    \[
        0 \le \simregret_{t_n} \le r_{t_n} = f(\x^{\ast}) - f(\x_{t_n}) \to 0.
    \]
    By the monotonicity of simple regret, we have $\simregret_t \to 0$ as $t \to \infty$. Thus, we conclude that
    \[
        \Prob\left[ \lim_{t \to \infty} \simregret_t = 0 \right]
        \ge \Prob\left[ \inf_{t} d(\x^{\ast}, \X_t) = 0 \right] = 1,
    \]
    which completes the proof of part \itref{it2:consistency BOKE}.
\end{proof}


Absence of algorithmic consistency may lead to risk of getting trapped in local optima \cite{song2022general}. \cref{thm:consistency BOKE} ensures that the BOKE algorithm converges even in the worst-case scenario, thus guaranteeing robustness. Specifically, part \itref{it1:consistency BOKE} of \cref{thm:consistency BOKE} shows that a fixed bandwidth allows the algorithm to converge to the sub-optimum, and the accuracy scales according to the bandwidth. 
Part \itref{it2:consistency BOKE} demonstrates that by choosing a suitable sequence of bandwidths that decreases towards $0$, the algorithm further converges to the global optimum. Therefore, bandwidth adjustment is meaningful in practical applications.


\begin{corollary}[Algorithmic consistency of BOKE+]
    \label{cor:consistency BOKE+}
    Under Assumptions \ref{asm:domain}-\ref{asm:noise}, the conclusions in \cref{thm:consistency BOKE} also hold for the BOKE+ algorithm.
\end{corollary}

\begin{proof}
    By the Borel-Cantelli lemma, the stochastically alternate strategy of BOKE+ ensures that
    \[
        \mathbb{P}\left[ \set{\x_{t+1} \in \argmax_{\x \in \domain} a_{t}(\x)}, i.o. \right] = 1.
    \]
    Hence, with probability $1$, the BOKE+ iterates $\set{\x_t}$ contain a subsequence $\set{\x_{t_n}}$ satisfying Eq.~\eqref{eqn:IKR-UCB}. 
    The proof follows the same methodology as in \cref{thm:consistency BOKE}. However, the construction of the convergent subsequence must be contained within $\set{\x_{t_n}}$, thereby leading to a contradiction based on the IKR-UCB criterion.
    Then, by using the conditional probability, all these conclusions hold almost surely, which completes the proof.
\end{proof}


The algorithmic consistency of density-based exploration Eq.~\eqref{eqn:KDE pure-explore} directly comes from \cref{lem:SNEB DE}, and can be treated as a special case of \cref{thm:consistency BOKE} with $f \equiv 0$ and noise-free evaluations.

\begin{corollary}[Algorithmic consistency of DE]
    \label{cor:consistency DE}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:kernel} hold. Let $\X_t = \set{\x_1, \dots, \x_t}$ be the DE iterates, and $h_t$ denote the corresponding bandwidth. The following hold:
    \begin{enumerate}[(i)]
        \item\label{it1:consistency DE} For any fixed $h_t \equiv h$, $\domain$ is eventually populated by the DE iterates within $\order(h)$ gaps, i.e., $\inf_t h(\domain, \X_t) \le R h$.
        
        \item\label{it2:consistency DE} There exists $\set{h_t : t \in \mathbb{Z}_+} \to 0$, such that $\domain$ is eventually populated by the DE iterates, i.e., $\inf_t h(\domain, \X_t) = 0$.
    \end{enumerate}
\end{corollary}




\subsection{Regret analysis}
\label{sec:regret analysis}

In this section, we provide the regret analysis of the BOKE algorithm for both finite and general decision sets. Note that BOKE+ yields the same result as BOKE, since the additional exploitation steps do not affect the regret upper bound.




\subsubsection{Finite decision set}

On a finite subset $X \subset \domain$, the regret minimization problem is referred to as the multi-armed bandit problem \cite{auer2002finite}.
Let the number of visits to each $\x \in X$ be denoted as $n_T(\x) = \sum_{t=1}^{T} \1\set{\x_t = \x}$, and the error as $\Delta(\x) = f(\x^{\ast}) - f(\x)$.
Then, the expected cumulative regret is
\begin{equation}
    \label{eqn:expected cumulative regret}
    \E[\cumregret_T] = \E [\sum_{t=1}^{T} (f(\x^{\ast}) - f(\x_t))] = \sum_{\x \in X} \Delta(\x) \E[n_T(\x)].
\end{equation}


Before proof of \cref{thm:finite regret BOKE}, we first provide the upper bound of $\E[n_T(\x)]$ in \cref{lem:finite count BOKE} and its proof is detailed in \cref{sec:proofs of regret analysis}.
\begin{lemma}
    \label{lem:finite count BOKE}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:noise} hold, and the bandwidth $\set{h_t : t \in \mathbb{Z}_+}$ is uniformly bounded as:
    \begin{equation}
        \omega_f(h_t)
        \le \dfrac{1}{4} \inf_{\x \in X \setminus \set{\x^{\ast}}} \Delta(\x)
        \quad \text{for all} \quad t \ge 1,
    \end{equation}
    where $x^\ast$ is the optimal function value. Then, for any $\x \in X$ with $\Delta(\x) > 0$, the following holds:
    \begin{equation}
        \E[n_T(\x)] \le \dfrac{C_1' \log T}{\Delta^2(\x)} + C_2',
    \end{equation}
    where $C_1' = 64 \varsigma^2 M_{\Psi}$ and $C_2' = 1 + \frac{2 \pi^2}{3}$.
\end{lemma}


With this lemma, the regret bound of proposed BOKE is derived in \cref{thm:finite regret BOKE}.


\begin{theorem}
    \label{thm:finite regret BOKE}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:noise} hold. Let $\delta \in (0, 1)$, and define $\beta_t = 4 \varsigma^2 M_{\Psi} \log t$ for the IKR-UCB formula \eqref{eqn:IKR-UCB}.
    If the bandwidth $\set{h_t : t \in \mathbb{Z}_+}$ is uniformly bounded as:
    \begin{equation}
        \label{eqn:modulus upper bound}
        \omega_f(h_t)
        \le \dfrac{1}{4} \inf_{\x \in X \setminus \set{\x^{\ast}}} \Delta(\x)
        \quad \text{for all} \quad t \ge 1,
    \end{equation}
    then, we obtain a regret bound of $\widetilde{\order}(\sqrt{|X| T})$, that is, with probability at least $1 - \delta$, the cumulative regret satisfies:
    \begin{equation}
        \cumregret_T \le \sqrt{C_1 |X| T \log T} + C_2 |X|,
    \end{equation}
    where $C_1 ={ 256 \delta^{-2} \varsigma^2 M_{\Psi}}$ and $C_2 = 8 \delta^{-1} \sup_{\x \in X} \Delta(\x)$.
\end{theorem}

\begin{proof}
    Let $\Delta_0 > 0$. By \cref{lem:finite count BOKE}, one has
    \begin{align*}
        \E[\cumregret_T]
        & = \sum_{\x \in X} \Delta(\x) \E[n_T(\x)] 
        \\
        & \le \sum_{\Delta(\x) \ge \Delta_0} \Delta(\x) \left( \dfrac{C_1' \log T}{\Delta^2(\x)} + C_2' \right) + \sum_{\Delta(\x) < \Delta_0} \Delta_0 \E[n_T(\x)] 
        \\
        & \le \dfrac{C_1' |X| \log T}{\Delta_0} + C_2' |X| \sup_{\x \in X} \Delta(\x) + \Delta_0 T.
    \end{align*}
    Let $\Delta_0 = \sqrt{\frac{C_1' |X| \log T}{T}}$, we have
    \[
        \E[\cumregret_T] \le 2 \sqrt{C_1' |X| T \log T} + C_2' |X| \sup_{\x \in X} \Delta(\x).
    \]
    By Markov's inequality, we have $\Prob \left[ \cumregret_T \le \delta^{-1} \E[\cumregret_T] \right] \ge 1 - \delta$, which completes the proof.
\end{proof}

\begin{remark}
    In Eq.~\eqref{eqn:modulus upper bound}, the convergence of the bandwidth to zero as $t$ increases is not required. Instead, it is required that the modulus of uniform continuity be ultimately bounded above by a constant.
    Hence this condition can be weakened into
    \[
        \omega_f(h_t)
        \le \dfrac{1}{4} \inf_{\x \in X \setminus \set{\x^{\ast}}} \Delta(\x) \;\text{ for all } t \ge K, \; \text{ for some } K > 0.
    \]
    The corresponding regret bound is still the same.
\end{remark}


\cref{thm:finite regret BOKE} shows that the BOKE algorithm achieves a regret bound that is at least equivalent to UCB1 \cite{auer2002finite}.
In particular, when the bandwidth is sufficiently small, BOKE degenerates into UCB1.
However, since kernel regression can share information, BOKE does not require trying every action in $X$ once before calculating the UCB.




\subsubsection{General decision set}

In this section, we demonstrate that for a compact $\domain \subset \mathbb{R}^d$, the regret upper bound of BOKE is influenced by the density of the sample path and the choice of bandwidth. Our analytical approach follows \cite{srinivas2010gaussian}.

\begin{theorem}
    \label{thm:general regret BOKE}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:noise} hold. Let $\delta \in (0, 1)$ and define $\beta_t = 2 \varsigma^2 M_{\Psi} \log \frac{2\pi^2 t^2}{3 \delta}$ for the IKR-UCB formula \eqref{eqn:IKR-UCB}.
    Then, for any bandwidth $\set{h_t : t \in \mathbb{Z}_+}$, with probability at least $1 - \delta$, the instantaneous regret is bounded as:
    \begin{equation}
        r_{t+1} \le 2 \beta_t^{1/2} \widehat{\sigma}_t(\x_{t+1}) + 2 \omega_f(h_t), \quad \forall t \ge 1,
    \end{equation}
    and the cumulative regret after $T$ iterations satisfies:
    \begin{equation}
        \cumregret_T \le \sqrt{C_3 \eta_T T \log T } + \sqrt{C_4 T \eta_T} + 2 \sum_{t=1}^{T} \omega_f(h_t),
    \end{equation}
    where $\eta_T = \sum_{t=1}^{T} \widehat{\sigma}_t^2(\x_{t+1})$, $C_3 = 16 \varsigma^2 M_{\Psi}$, and $C_4 = 8 \varsigma^2 M_{\Psi}\log \frac{2 \pi^2}{3 \delta}$.
\end{theorem}

\begin{proof}
    For any $t \ge 1$, denote the event
    \[
        \mathcal{A}_t(\delta) \coloneqq \set{r_{t+1} \le \sqrt{8 \varsigma^2 M_{\Psi} \log(4 / \delta)} \widehat{\sigma}_t(\x_{t+1}) + 2 \omega_f(h_t)}.
    \]
    Let $\delta_t \coloneqq \frac{6\delta}{\pi^2 t^2} =  4\exp\left(-\frac{\beta_t}{2 \varsigma^2 M_{\Psi}}\right)$ in \cref{thm:pointwise consistency KR}. By the union bound, with probability at least $1 - \delta_t$, we have:
    \begin{align*}
        f(\x_{t+1}) & \ge m_t(\x_{t+1}) - \beta_t^{1/2} \widehat{\sigma}_t(\x_{t+1}) - \omega_f(h_t), 
        \\
        f(\x^{\ast}) & \le m_t(\x^{\ast}) + \beta_t^{1/2} \widehat{\sigma}_t(\x^{\ast}) + \omega_f(h_t).
    \end{align*}
    By the definition of $\x_{t+1}$, we have
    \[
        m_t(\x^{\ast}) + \beta_t^{1/2} \widehat{\sigma}_t(\x^{\ast})
        \le m_t(\x_{t+1}) + \beta_t^{1/2} \widehat{\sigma}_t(\x_{t+1}).
    \]
    Thus, with probability at least $1 - \delta_t$,
    \[
        r_{t+1} = f(\x^{\ast}) - f(\x_{t+1}) \le 2\beta_t^{1/2}\widehat{\sigma}_t(\x_{t+1})+2\omega_f(h_t)=\sqrt{8 \varsigma^2 M_{\Psi} \log(4 / \delta_t)} \widehat{\sigma}_t(\x_{t+1}) + 2 \omega_f(h_t)
    \]
    Therefore, $\mathbb{P}[\mathcal{A}_t(\delta_t)] \ge 1 - \delta_t$ for any $t \ge 1$ and $\delta \in (0, 1)$. Applying the union bound, we have:
    \begin{align*}
        & \mathbb{P} \left[ r_{t+1} \le \sqrt{8 \varsigma^2 M_{\Psi} \log(4 / \delta_t)} \widehat{\sigma}_t(\x_{t+1}) + 2 \omega_f(h_t), \quad \forall t \ge 1 \right] 
        \\
        = & \mathbb{P} \left[ \bigcap_{t=1}^{\infty} \mathcal{A}_t(6\delta \pi^{-2}  t^{-2}) \right]
        \ge 1 - \sum_{t=1}^{\infty} \mathbb{P} \left[ \mathcal{A}^{\complement}_t(6 \delta\pi^{-2}  t^{-2}) \right]
        \ge 1 - \dfrac{6 \delta}{\pi^2} \sum_{t=1}^{\infty} \dfrac{1}{t^2}
        = 1 - \delta.
    \end{align*}
    Hence, with probability at least $1 - \delta$,
    \[
        \cumregret_T
        \le \sum_{t=1}^{T} \left[ 2 \beta_t^{1/2} \widehat{\sigma}_t(\x_{t+1}) + 2 \omega_f(h_t) \right]
        \le \sqrt{4 T \beta_T \sum_{t=1}^{T} \widehat{\sigma}_t^2(\x_{t+1})} + 2 \sum_{t=1}^{T} \omega_f(h_t),
    \]
    where the last inequality follows from the Cauchy-Schwarz inequality and the fact that $\beta_t$ is increasing with respect to $t$, which completes the proof.
\end{proof}




\section{Numerical Results}
\label{sec:experiments}

In this section, we analyze the effectiveness of the BOKE and BOKE+ algorithms on a variety of synthetic and real-world optimization tasks, comparing them against the GP-UCB and KR-UCB algorithms. For the detailed configurations of the latter two methods, refer to \cref{sec:supplementary algorithm details}.

Generally, the convergence of KR surrogate model requires the bandwidth $h = h_t$ to approach zero, as discussed in \cite{linke2023towards}.
This observation is consistent with the findings in \cref{thm:pointwise consistency KR}, highlighting the crucial role of bandwidth selection in practical applications.
However, in the context of Bayesian optimization, it has been shown that re-estimating the optimal parameters at each step can be unreliable and may result in the non-convergence of the optimization process \cite{bull2011convergence}.
To address this, we adopt the widely accepted rule-of-thumb bandwidths:
\begin{align}
    \text{(Scott's rule \cite{scott1992multivariate})}
    & \quad h_t \propto t^{-\frac{1}{d+4}},
    \label{eqn:Scott bandwidth} 
    \\
    \text{(Silverman's rule \cite{silverman1998density})}
    & \quad h_t \propto \left(t \cdot \left(d + 2\right) / 4\right)^{-\frac{1}{d+4}}.
    \label{eqn:Silverman bandwidth}
\end{align}



For KR-UCB, BOKE, and BOKE+, the bandwidth is determined using Scott's rule \eqref{eqn:Scott bandwidth}, while GP-UCB employs a fixed bandwidth.
Additionally, all four algorithms use Gaussian kernels in our implementation.
The tuning parameter $\beta_t$ for the IKR-UCB acquisition function \eqref{eqn:IKR-UCB} is configured as specified in \cref{thm:general regret BOKE}.
All algorithms are initialized with the same set of points generated using LHS \cite{mckay1979comparison}.
To ensure a fair comparison of computational efficiency, we adopt a multi-start strategy integrated with local optimization techniques, particularly the L-BFGS-B algorithm \cite{byrd1995limited}, for acquisition function optimization.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.9\textwidth]{figures/fig-benchmark_noisefree.pdf}
    \caption{
        Simple regret with respect to noise-free evaluations on benchmark optimization functions.
        Each curve is the mean of results from 30 random seeds.
        The left area of the dashed gray line represents the sampling points initialized by LHS.
    }
    \label{fig:benchmark noisefree}
\end{figure}




\subsection{Synthetic benchmark functions}

We empirically evaluate our method on a range of synthetic benchmark optimization tasks, including Forrester function (1D), Goldstein-Price function (2D), Six-Hump-Camel function (2D), Hartman3 function (3D), Rosenbrock4 function (4D), and Sphere6 function (6D) \cite{jamil2013literature,picheny2013benchmark}.
The results of these experiments with noise-free observations and noisy observations are presented in \cref{fig:benchmark noisefree} and \cref{fig:benchmark noisy}.


As shown in \cref{fig:benchmark noisefree}, both BOKE and BOKE+ exhibit stable convergence and demonstrate competitive performance compared to GP-UCB. In \cref{fig:benchmark noisefree}(B, E, F), BOKE+ converges faster than BOKE, indicating the acceleration effect of the exploitation technique. For the other three panels, where the objective functions require a proper balance between exploitation and exploration, the two algorithms perform similarly, while KR-UCB may fail to ensure global convergence or converge much more slowly.


Results for observations with additive Gaussian noises are displayed in \cref{fig:benchmark noisy}.
Both the BOKE and BOKE+ algorithms significantly outperform the KR-UCB algorithm and are comparable to the GP-UCB method. Specifically, the two proposed algorithms achieve better accuracy than GP-UCB in high-dimensional cases, such as in \cref{fig:benchmark noisy}(F).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.9\textwidth]{figures/fig-benchmark_noisy.pdf}
    \caption{
        Simple regret with respect to noisy evaluations on benchmark optimization functions.
        Each curve is the mean of results from 30 random seeds.
        The left area of the dashed gray line represents the sampling points initialized by LHS.
    }
    \label{fig:benchmark noisy}
\end{figure}




\subsection{Sprinkler computer model}

The garden sprinkler computer model simulates the water consumption of a garden sprinkler system, taking into account factors such as the rotation speed and spray range \cite{siebertz2010statistische}. The model is implemented using the CompModels package in R. One of the main objectives of this model is to optimize the spray range by adjusting eight physical parameters of the sprinkler, as detailed in \cite{pourmohamad2021bayesian}.


The simulation results presented in \cref{fig:sprinkler} demonstrate that BOKE and BOKE+ exhibit superior convergence compared to the KR-UCB algorithm, regardless of the number of iterations or solving time. Both BOKE and BOKE+ achieve better accuracy than GP-UCB, and all three algorithms exhibit stable convergence, indicating the global convergence of the IKR-UCB acquisition function. Notably, with fixed CPU resources, BOKE and BOKE+ outperform GP-UCB, highlighting the computational efficiency of the proposed algorithms.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=.98\textwidth]{figures/fig-computer_model.pdf}
    \caption{
        Simple regret with respect to both iteration step and average CPU time on the sprinkler computer problem (8D).
        Each curve is the mean of results from 30 random seeds.
    }
    \label{fig:sprinkler}
\end{figure}




\subsection{Hyperparameter tuning}

We empirically evaluate our method on hyperparameter optimization for random forest (RF) and XGBoost regression (XGB) tasks using datasets including California Housing \cite{pace1997sparse} and Wine Quality \cite{cortez2009modeling}.
For both problems, we utilize the implementation from the scikit-learn package and employ 5-fold cross-validation to obtain the goodness-of-fit measure, $R^2$, where higher values indicate better performance.
The parameters of interest are bounded, and for the RF problem, there are three integers: \texttt{n\_estimators}, \texttt{max\_depth}, and \texttt{min\_samples\_split}, and two real numbers: \texttt{max\_features} and \texttt{min\_impurity\_decrease}.
For the XGB problem, the hyperparameters include two integers: \texttt{n\_estimators} and \texttt{max\_depth}, and five real numbers: \texttt{learning\_rate}, \texttt{subsample}, \texttt{colsample\_bytree}, \texttt{gamma}, and \texttt{min\_child\_weight}.
To facilitate computation, we convert the integer variables to real numbers and apply rounding truncation.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=.98\textwidth]{figures/fig-ml_tuning.pdf}
    \caption{
        Goodness-of-fit measure on the random forest tuning (5D) and the XGBoost tuning problems (7D).
        Each curve is the mean of results from 30 random seeds.
    }
    \label{fig:ml tuning}
\end{figure}


The results in \cref{fig:ml tuning}(A, B) show that BOKE and BOKE+ outperform the other three algorithms, achieving faster convergence rates and more robust optimization outcomes. For the XGB problem in \cref{fig:ml tuning}(C, D), the initial $R^2$ values are already quite good compared to those in (A, B), and acquisition functions that favor exploitation, such as KR-UCB and BOKE+, converge faster.




\section{Conclusions}
\label{sec:conclusions}

This paper proposes a Bayesian optimization algorithm that uses kernel regression as the surrogate model and kernel density as the exploration bonus. The superior performance of density-based exploration in space-filling design, combined with the estimation error bound of kernel regression, ensures an effective balance between exploitation and exploration in the proposed BOKE acquisition function. Under noisy evaluations, the global convergence of the algorithm is rigorously analyzed. Numerical experiments demonstrate that the algorithm achieves competitive convergence rates compared to Gaussian process-based Bayesian optimization, with a significant reduction in computational complexity, decreasing from $\mathcal{O}(T^4 + mT^3)$ to $\mathcal{O}(mT^2)$.


Future research will focus on refining the theoretical analysis of this method and providing more precise regret bound analysis.
Potential approaches include converting the acquisition function into an equivalent Gaussian process to leverage established analytical tools (see e.g., \cite{oliveira2022batch}).
Exploring other surrogate models that may better align with KDE, such as Gaussian processes or radial basis functions, could also enhance the convergence of the optimization algorithm and establish stronger analytical frameworks.




\section*{Acknowledgements}
L. Ji acknowledges support from NSFC (grant No. 12201403), Shanghai Academy of Spaceflight Technology Funded project USCAST2022-18. The authors also acknowledge the support of the HPC center of Shanghai Jiao Tong University.




\section*{Declarations}

\subparagraph{Conflict of interest.} 
The authors have no competing interests to declare that are relevant to the content of this article.

\subparagraph{Data and code availability.} 
The authors confirm that all data generated or analyzed during this study are included in this article or publicly available; see the corresponding references in this article. Codes are available from the corresponding author on reasonable request.  




\appendix
% \appendixpage
% \addappheadtotoc
\numberwithin{equation}{section}
\renewcommand{\theequation}{\thesection\arabic{equation}}


\section{Supplementary Algorithm Details}
\label{sec:supplementary algorithm details}

Here, we provide additional definitions and configurations for the Gaussian process upper confidence bound (GP-UCB) and kernel regression upper confidence bound (KR-UCB) algorithms.
These configurations are also utilized in our numerical experiments.




\subsection{Gaussian process UCB}
\label{sec:GP-UCB}

Gaussian processes have been the most popular surrogate model in the literature of Bayesian optimization, which begins with a Gaussian process prior belief on the unknown objective function.
The posterior belief at each $\x$ is Gaussian distribution conditioned on the previous observations, with mean and variance
\begin{align}
    & \mu_t(\x) = \k_t^{\top}(\x) (\K_t + \sigma^2 \bm{I}_t)^{-1} \y_t,
    \label{eqn:GPR}
    \\
    & \sigma_t^2(\x) = k(\x, \x) - \k_t^{\top}(\x) (\K_t + \sigma^2 \bm{I}_t)^{-1} \k_t(\x).
    \label{eqn:GP Var}
\end{align}
where $\k_t(\x) = [k(\x, \x_1), \dots, k(\x, \x_t)]^{\top} \in \mathbb{R}^t$, $\K_t = [\k_t(\x_1), \dots, \k_t(\x_t)] \in \mathbb{R}^{t \times t}$, and kernel $k$ is widely chosen as Gaussian kernels or Mat\'ern kernels.


As a popular acquisition function, the GP-UCB \eqref{eqn:GP-UCB} can be expressed as a weighted sum of the estimated mean and standard deviation of the posterior \cite{srinivas2010gaussian}.
The concerned GP-UCB algorithm is illustrated in Algorithm \ref{alg:GP-UCB}.


\begin{algorithm}[!htb]
    \caption{GP-UCB algorithm}
    \label{alg:GP-UCB}
    \KwIn{Initial dataset $\D_t$, budget $T$, kernel $k$, tuning parameters $\set{\beta_i}_{i=t}^T$.}
    \While{$t \le T$}{
        Compute $\mu_t$ and $\sigma_t$ for $\x \in \domain$ by Eq.~\eqref{eqn:GPR} and Eq.~\eqref{eqn:GP Var}. \;
        Obtain $\x_{t+1}$ by solving
        \begin{equation}
            \label{eqn:GP-UCB}
            \x_{t+1} = \argmax_{\x \in \domain}~\mu_t(\x) + \sqrt{\beta_t} \sigma_t(\x).
        \end{equation} \;
        \vspace*{-2em}
        Evaluate $y_{t+1} = f(\x_{t+1}) + \varepsilon_{t+1}$, then increment $t$.
    }
\end{algorithm}

Here, $\beta_t$ denotes a tuning parameter that varies with $t$.
However, in typical algorithm settings, a fixed $\beta > 0$ is used. Our implementation also follows this approach.




\subsection{Kernel regression UCB}
\label{sec:KR-UCB}

The KR-UCB formula \eqref{eqn:KR-UCB MAB} adapts the vanilla UCB formula to select among already explored arms \cite{yee2016monte}.
It integrates kernel regression and kernel density estimation to balance exploitation and exploration.
\begin{equation}
    \label{eqn:KR-UCB MAB}
    \x_{t+1} = \argmax_{\x_i \in \X_t} \left( m_t(\x_i) + C \sqrt{\dfrac{\log \sum_{j = 1}^{t} W_t(\x_j)}{W_t(\x_i)}} \right),
\end{equation}
where $C > 0$ is a tuning parameter.


The KR-UCB formula mentioned above is applicable only to finite decision sets, which can be viewed as a strategy in multi-armed bandits, as analyzed in \cref{sec:regret analysis}.
To extend Eq.~\eqref{eqn:KR-UCB MAB} to general decision sets, a progressive widening \cite{couetoux2011continuous,auger2013continuous} strategy using density-based exploration is introduced.
This strategy serves as the exploration role in the continuous Monte Carlo tree search algorithm.
Without ambiguity, we refer to Eq.~\eqref{eqn:KR-UCB} as the KR-UCB acquisition function.
\begin{equation}
    \label{eqn:KR-UCB}
    \begin{aligned}
        & \widetilde{\x}_{t+1} = \argmax_{\x_i \in \X_t} \left( m_t(\x_i) + C \sqrt{\dfrac{\log \sum_{j = 1}^{t} W_t(\x_j)}{W_t(\x_i)}} \right), 
        \\
        & \x_{t+1} = \argmin_{\x, k(\x, \widetilde{\x}_{t+1}) > \tau} W_t(\x),
    \end{aligned}
\end{equation}
where the threshold $\tau > 0$ balances the need for the next sample point to be heavily weighted by the kernel while not being well represented by existing observations.
Note that in practice, isotropic kernels are commonly used.
Thus the constraint $k(\x, \widetilde{\x}_{t+1}) > \tau$ is equivalent to $\| \x - \widetilde{\x}_{t+1} \| < \rho$ for some $\rho > 0$.
In analogy to the IKR-UCB acquisition function \eqref{eqn:IKR-UCB}, the equivalent tuning parameter for KR-UCB can be expressed as $\beta_t = C^2 \log \sum_{j = 1}^{t} W_t(\x_j)$, with a higher computational cost of $\order(t^2)$.


The KR-UCB algorithm is outlined in Algorithm~\ref{alg:KR-UCB}.

\begin{algorithm}[!htb]
    \caption{KR-UCB algorithm}
    \label{alg:KR-UCB}
    \KwIn{Initial dataset $\D_t$, budget $T$, kernel $k$, tuning parameters $\alpha \in (0, 1)$, $C$ and $\tau$.}
    \While{$t \le T$}{
        Compute $m_t$ and $W_t$ for $\x \in \domain$ by Eq.~\eqref{eqn:KR} and Eq.~\eqref{eqn:KDE}. \;
        Obtain $\x_{t+1}$ by solving Eq.~\eqref{eqn:KR-UCB MAB} if $t^{\alpha} < \#\set{[\x_i] : 1 \le i \le t}$, otherwise Eq.~\eqref{eqn:KR-UCB}. \;
        Evaluate $y_{t+1} = f(\x_{t+1}) + \varepsilon_{t+1}$, then increment $t$.
    }
\end{algorithm}




\section{Additional Proofs}
\label{sec:additional proofs}



\subsection{Proofs in \texorpdfstring{\cref{sec:methodology}}{Section 3}}
\label{sec:proofs of asymptotic properties}

Before analyzing the asymptotic properties of GP, we note that the semi-positive kernel matrix $\K_t$ can be singular if there exist $\x_i = \x_j$ with $i \neq j$.
This situation may arise in the setting of finite decision sets, as discussed in \cref{sec:regret analysis}, and slightly affects the analysis for asymptotic behaviors.
Therefore, we first employ the following \cref{lem:GP reduced} to reduce the Gaussian process into a tighter form when there are overlapped sample points.


\begin{lemma}
    \label{lem:GP reduced}
    Suppose $f \sim \mathcal{GP}(\bm{0}, k(\cdot, \cdot))$ is a Gaussian process. Let $\D_t = \set{(\x_i, y_i)}_{i=1}^{t}$ be the dataset. If there are $\tau$ different equivalence classes $\set{[\tilde{\x}_{\lambda}] : \lambda = 1, \dots, \tau}$, with $\tilde{n}_{\lambda} = \#[\tilde{\x}_{\lambda}]$ and $\sum_{\lambda=1}^{\tau} \tilde{n}_{\lambda} = t$, then for any $\x \in \domain$, the posterior of $f(\x)$ is:
    \begin{align}
        \mu_t(\x) 
        & = \tilde{\k}_{\tau}^{\top}(\x) (\tilde{\K}_{\tau} + \tilde{\bm{\Sigma}}_{\tau})^{-1} \tilde{\y}_{\tau}, 
        \\
        \sigma^2_t(\x) 
        & = k(\x, \x) - \tilde{\k}_{\tau}^{\top}(\x) (\tilde{\K}_{\tau} + \tilde{\bm{\Sigma}}_{\tau})^{-1} \tilde{\k}_{\tau}(\x),
    \end{align}
    where $\tilde{\k}_{\tau}(\x) = [k(\x, \tilde{\x}_1), \dots, k(\x, \tilde{\x}_{\tau})]^{\top}$, $\tilde{\K}_{\tau} = [\k_{\tau}(\tilde{\x}_1), \dots, \k_{\tau}(\tilde{\x}_{\tau})]$, $\tilde{\bm{\Sigma}}_{\tau} = \sigma^2 \diag{\tilde{n}_1^{-1}, \dots, \tilde{n}_{\tau}^{-1}}$, and $\tilde{\y}_{\tau} = [\tilde{y}_1, \dots, \tilde{y}_{\tau}]^{\top}$ with $\tilde{y}_{\lambda} = \tilde{n}_{\lambda}^{-1} \sum_{\x_i \in [\tilde{\x}_{\lambda}]} y_i$.
\end{lemma}

\begin{proof}
    Let $\X = \set{\x_1, \dots, \x_n} \subset \domain$ be arbitrary $n$ sample points.
    Denote $f(\X) = [f(\x_1), \dots, f(\x_n)]^{\top}$, $\K(\x, \X)^{\top} = \K(\X, \x) = [k(\x, \x_1), \dots, k(\x, \x_n)]^{\top}$, and $\K(\X, \X) = \left[ k\left(\x_i, \x_j\right) \right]_{i,j=1}^{n}$. Consider the following two cases
    \begin{enumerate}[(i)]
        \item \[
            \left.\begin{bmatrix}
                f(\X) \\ y_1 \\ y_2
            \end{bmatrix}
            \right| \X, \x \sim \mathcal{N}\left(\bm{0},
            \begin{bmatrix}
                \K(\X, \X) & \K(\X, \x)               & \K(\X, \x)
                \\
                \K(\x, \X) & k(\x, \x) + \sigma^2/n_1 & k(\x, \x)
                \\
                \K(\x, \X) & k(\x, \x)                & k(\x, \x) + \sigma^2/n_2
            \end{bmatrix}
            \right)
        \]
        \item \[
            \left.\begin{bmatrix}
                f(\X) \\ \dfrac{n_1 y_1 + n_2 y_2}{n_1 + n_2}
            \end{bmatrix}
            \right| \X, \x \sim \mathcal{N}\left(\bm{0},
            \begin{bmatrix}
                \K(\X, \X) & \K(\X, \x)
                \\
                \K(\x, \X) & k(\x, \x) + \sigma^2/(n_1 + n_2)
            \end{bmatrix}
            \right)
        \]
    \end{enumerate}
    where $y_1, y_2$ are independent evaluations of $f(\x)$, and $n_1, n_2 \in \mathbb{Z}_+$ are the corresponding occurrence numbers.
    By direct calculation, these two cases lead to the same Gaussian posterior of $f(\X)$ with
    \begin{align*}
        \E[f(\X) \,|\, \x, y_1, y_2]
        & = \dfrac{n_1 y_1 + n_2 y_2}{(n_1 + n_2) k(\x, \x) + \sigma^2} \K(\X, \x), 
        \\
        \mathrm{Var}[f(\X) \,|\, \x, y_1, y_2]
        & = \K(\X, \X) - \dfrac{n_1 + n_2}{(n_1 + n_2) k(\x, \x) + \sigma^2} \K(\X, \x) \K(\x, \X).
    \end{align*}
    Thus, the two independent evaluations $y_1, y_2$ at the sample point $\x$ with occurrence numbers $n_1, n_2$ can be reduced into their weighted average $\frac{n_1 y_1 + n_2 y_2}{n_1 + n_2}$ with occurrence numbers $n_1 + n_2$.
    By repeatedly applying this equivalence rule to every pair $(\x_i, y_i)$ and $(\x_j, y_j)$ in the dataset where $\x_i = \x_j$ and $i \neq j$, the proof is completed.
\end{proof}


Based on \cref{lem:GP reduced}, we show the asymptotic properties of GP predictor in \cref{pro:asymptotic GPR}.

\begin{proposition}
    \label{pro:asymptotic GPR}
    Suppose the kernel function satisfies $k(\x, \x) \equiv 1$, $\forall \x \in \domain$. The following holds:
    \begin{equation}
        \lim_{h \to 0^+} \mu_t(\x) = \sum_{i=1}^{t} \dfrac{y_i}{\#[\x_i] + \sigma^2} \1{\set{\x = \x_i}}.
    \end{equation}
\end{proposition}

\begin{proof}
    By \cref{lem:GP reduced}, $\mu_t(\x)$ can be rewritten as
    \[
        \mu_t(\x) = \tilde{\k}_{\tau}^{\top}(\x) (\tilde{\K}_{\tau} + \tilde{\bm{\Sigma}}_{\tau})^{-1} \tilde{\y}_{\tau},
    \]
    where $\tilde{\k}_{\tau}(\x)$, $\tilde{\K}_{\tau}$, $\tilde{\bm{\Sigma}}_{\tau}$ and $\tilde{\y}_{\tau}$ are defined as in \cref{lem:GP reduced}.
    As $h$ goes to zero, $\tilde{\K}_{\tau}$ converges to the diagonal matrix $\bm{I}_{\tau}$, and $\tilde{\k}_{\tau}(\x)$ converges to $[\1{\set{\x \in [\tilde{\x}_1]}}, \dots, \1{\set{\x \in [\tilde{\x}_{\tau}]}}]^{\top}$.
    Then we have
    \[
        \lim_{h \to 0^+} \mu_t(\x)
        = \sum_{\lambda=1}^{\tau} \dfrac{\tilde{y}_{\lambda}}{1 + \sigma^2 / \tilde{n}_{\lambda}} \1\set{\x \in [\tilde{\x}_{\lambda}]}
        = \sum_{i=1}^{t} \dfrac{y_i}{\#[\x_i] + \sigma^2} \1\set{\x = \x_i},
    \]
    which completes the proof.
\end{proof}


\begin{proposition}
    \label{pro:asymptotic GP Var}
    Suppose the kernel function satisfies $k(\x, \x) \equiv 1$, $\forall \x \in \domain$. The following holds:
    \begin{equation}
        \lim_{h \to 0^+} \sigma^2_t(\x) = 1 - \sum_{i=1}^{t} \dfrac{\1\set{\x = \x_i}}{\#[\x_i] + \sigma^2}.
    \end{equation}
\end{proposition}

\begin{proof}
    By \cref{lem:GP reduced}, $\sigma^2_t(\x)$ can be rewritten as
    \[
        \sigma^2_t(\x) = 1 - \tilde{\k}_{\tau}^{\top}(\x) (\tilde{\K}_{\tau} + \tilde{\bm{\Sigma}}_{\tau})^{-1} \tilde{\k}_{\tau}(\x),
    \]
    where $\tilde{\k}_{\tau}(\x)$, $\tilde{\K}_{\tau}$ and $\tilde{\bm{\Sigma}}_{\tau}$ are defined as in \cref{lem:GP reduced}.
    As $h$ goes to zero, $\tilde{\K}_{\tau}$ converges to the diagonal matrix $\bm{I}_{\tau}$, and $\tilde{\k}_{\tau}(\x)$ converges to $[\1{\set{\x \in [\tilde{\x}_1}]}, \dots, \1{\set{\x \in [\tilde{\x}_{\tau}]}}]^{\top}$.
    Then we have
    \[
        \lim_{h \to 0^+} \sigma^2_t(\x)
        = 1 - \sum_{\lambda=1}^{\tau} \dfrac{\1\set{\x \in [\tilde{\x}_{\lambda}]}}{1 + \sigma^2 / \tilde{n}_{\lambda}}
        = 1 - \sum_{i=1}^{t} \dfrac{\1\set{\x = \x_i}}{\#[\x_i] + \sigma^2},
    \]
    which completes the proof.
\end{proof}




\subsection{Proofs in \texorpdfstring{\cref{sec:algorithmic consistency}}{Section 4.2}}
\label{sec:proofs of algorithmic consistency}

\begin{lemma}[SNEB property of DE]
    \label{lem:SNEB DE}
    Suppose Assumptions~\ref{asm:domain}-\ref{asm:kernel} hold. For any fixed $h > 0$, the following hold:
    \begin{enumerate}[(a)]
        \item\label{it1:SNEB DE} For any $\x \in \domain$ and finite set $\A_t = \set{\x_1, \dots, \x_t} \subset \domain$, if $\inf_{t} d(\x, \A_t) > R h$, then $\widehat{\sigma}(\x; \A_t) = \infty$ for all $t \in \mathbb{Z}_+$.

        \item\label{it2:SNEB DE} For any convergent sequence $\set{\x_t} \subset \domain$ and finite set $\A_t \supset \set{\x_1, \dots, \x_t}$, $\widehat{\sigma}(\x_t; \A_{t-1}) \to 0$ as $t \to \infty$.
    \end{enumerate}
\end{lemma}

\begin{proof}
    For part \itref{it1:SNEB DE}, by \cref{asm:kernel} and $\inf_{t} d(\x, \A_t) > R h$, we have $W(\x; \A_t) = \sum_{i=1}^{t} \Psi\left( \frac{\x - \x_i}{h} \right) = 0$, and thus, by definition, $\widehat{\sigma}(\x; \A_t) = \infty$.
    
    For part \itref{it2:SNEB DE}, by the continuity of $\Psi(\x)$ at $\x = 0$, there exists $\epsilon > 0$ such that $\inf_{\| \x \| \le \epsilon} \Psi(\x) > 0$. By the convergence of $\set{\x_t}$, there exists $N(\epsilon) > 0$ such that for any $i, j \ge N$, $\| \x_i - \x_j \| \le \epsilon h$. Thus,
    \[
        W(\x_t; \A_{t-1}) \ge \sum_{i=N}^{t-1} \Psi\left(\frac{\x_t - \x_i}{h}\right) \ge (t - N) \inf_{\| \x \| \le \epsilon} \Psi(\x) \to \infty \quad \text{as} \quad t \to \infty.
    \]
    Therefore, $\widehat{\sigma}(\x_t; \A_{t-1}) \to 0$ as $t \to \infty$.
\end{proof}


\begin{proof}[\rm\bfseries Proof of \cref{cor:consistency DE}]
    The same argumentation as in the proof of \cref{thm:consistency BOKE} is used.
    For part \itref{it1:consistency DE}, suppose for contradiction that $\inf_{t} h(\domain, \X_t) > R h$. By the monotonicity of $h(\domain, \X_t)$ with respect to $t$, we can rewrite the assumption as $\lim_{t \to \infty} h(\domain, \X_t) > Rh$. Thus, there exists $\epsilon > 0$ and $T \in \mathbb{Z}_+$, such that for any $t \ge T$, $h(\domain, \X_t) \ge Rh + \epsilon$. Let $F_t = \set{\x \in \domain : d(\x, \X_t) \ge Rh + \epsilon}$. By the continuity of $d(\x, \X_t)$ with respect to $\x$ and the compactness of $\domain$, $F_t$ is closed and non-empty. Since $d(\x, \X_t) \ge d(\x, \X_{t+1})$, we have $\domain \supset F_t \supset F_{t+1}$. By the Nested Set Theorem, $\cap_{t=T}^{\infty} F_t \neq \emptyset$. So there exists $\x' \in \cap_{t=T}^{\infty} F_t$, such that for any $t \ge T$, $d(\x', \X_t) \ge Rh + \epsilon$. Hence, $\inf_t d(\x', \X_t) = \lim_{t \to \infty} d(\x', \X_t) > Rh$. By \cref{lem:SNEB DE}\itref{it1:SNEB DE}, we have
    \[
        \widehat{\sigma}(\x'; \X_t) \to \infty \quad \text{as} \quad t \to \infty.
    \]
    On the other hand, since $\domain$ is compact, $\set{ \x_t }$ have a convergent subsequence, denoted by $\set{ \x_{t_n} }$. By \cref{lem:SNEB DE}\itref{it2:SNEB DE}, it holds that
    \[
        \widehat{\sigma}(\x_{t_n}; \X_{t_n - 1}) \to 0 \quad \text{as} \quad n \to \infty.
    \]
    So when restricted to $\set{\x_{t_n}}$, we have $\widehat{\sigma}(\x_{t_n}; \X_{t_n - 1}) < \widehat{\sigma}(\x'; \X_{t_n - 1})$ for large enough $n$, which contradicts with the DE criterion $\x_{t_n} \in \argmax_{\x \in \domain} \widehat{\sigma}(\x; \X_{t_n - 1})$. Thus, we conclude that $\inf_{t} h(\domain, \X_t) \le R h$ holds.

    Now we prove part \itref{it2:consistency DE} based on \itref{it1:consistency DE}. First, the monotonicity of $h(\domain, \X_t)$ gives $\lim_{t \to \infty} h(\domain, \X_t) \le R h$. Thus, for $\hbar_1 = 1$, there exists $t_1 \in \mathbb{Z}_+$ such that $h(\domain, \X_{t_1}) \le 2 R \hbar_1$, where $\x_2, \dots, \x_{t_1}$ only depends on $\x_1$ and $\hbar_1$. Then for $\hbar_2 = \frac{1}{2}$, there exists $t_2 > t_1$ such that $h(\domain, \X_{t_2}) \le 2 R \hbar_2$, where $\x_{t_1 + 1}, \dots, \x_{t_2}$ depends on $\X_{t_1}$ and $\hbar_2$. By repeating this procedure, we obtain $\hbar_n = \frac{1}{n} \to 0$ and $t_n \uparrow \infty$, such that $h(\domain, \X_{t_n}) \le 2 R \hbar_n$. Take $n \to \infty$, we have
    \[
        \lim_{n \to \infty} h(\domain, \X_{t_n}) \le 2 R \lim_{n \to \infty} \hbar_n = 0.
    \]
    Thus, we conclude that $\inf_t h(\domain, \X_t) = \lim_{t \to \infty} h(\domain, \X_t) = 0$, and the bandwidth sequence $h_t \to 0$ we built is
    \[
        h_t = \hbar_n \quad \text{if} \quad t_{n-1} < t \le t_n, \quad \text{where} \quad t_0 = 0.
    \]
\end{proof}


\begin{proof}[\rm\bfseries Proof of \cref{lem:SNEB IKR-UCB}]
    For part \itref{it1:SNEB IKR-UCB}, by \cref{lem:SNEB DE}\itref{it1:SNEB DE}, for all $t \in \mathbb{Z}_+$, $W(\x; \A_t) = 0$.
    So we consider the extended form Eq.~\eqref{eqn:KR extended} of KR. Since $f$ is continuous on a compact set, $f$ is bounded, denoted by $M_f = \sup_{\x} |f(\x)|$. Therefore, by applying the Hoeffding bound, for any $\lambda > 0$, we have
    \[
        \Prob\left[ |m(\x; \tilde{\A}_t)| \le M_f + \lambda \right]
        \ge 1 - 2 \exp\left( - \frac{\#\set{i : \|\x - \x_i\| = d(\x, \A_t)} \lambda^2}{2 \varsigma^2} \right)
        \ge 1 - 2 e^{-\frac{\lambda^2}{2 \varsigma^2}}.
    \]
    For any $\epsilon > 0$, let $\lambda = \beta_t^{1/2} \epsilon - M_f$, we have
    \[
        \Prob\left[ |\beta_t^{-1/2} m(\x; \tilde{\A}_t)| > \epsilon \right]
        \le 2 \exp\left( -\frac{(\beta_t^{1/2} \epsilon - M_f)^2}{2 \varsigma^2} \right).
    \]
    Let $t \to \infty$, we have $\lim_{t \to \infty} \Prob\left[ |\beta_t^{-1/2} m(\x; \tilde{\A}_t)| > \epsilon \right] = 0$, i.e., $|\beta_t^{-1/2} m(\x; \tilde{\A}_t)| \pto 0$ as $t \to \infty$. Then $\widehat{\sigma}(\x; \A_t) = \infty$ gives $a(\x; \tilde{\A}_t) \pto \infty$ as $t \to \infty$.

    For part \itref{it2:SNEB IKR-UCB}, by \cref{lem:SNEB DE}\itref{it2:SNEB DE}, we have $W(\x_t; \A_{t-1}) \to \infty$ as $t \to \infty$.
    By \cref{thm:pointwise consistency KR}, for any $\lambda > 0$,  with probability at least $1 - 2 \exp\left( -\frac{\lambda^2 W(\x_t; \A_{t-1})}{2 \varsigma^2 M_{\Psi}} \right)$, we have
    \[
        |m(\x_t; \tilde{\A}_{t-1}))|
        \le |f(\x_t)| + |f(\x_t) - m(\x_t; \tilde{\A}_{t-1}))|
        \le M_f + \omega_f(h) + \lambda.
    \]
    For any $\epsilon > 0$, let $\lambda = \beta_{t-1}^{1/2} \epsilon - M_f - \omega_f(h)$, we have
    \[
        \Prob\left[ |\beta_{t-1}^{-1/2} m(\x_t; \tilde{\A}_{t-1})| > \epsilon \right]
        \le 2 \exp\left( -\frac{(\beta_{t-1}^{1/2} \epsilon - M_f - \omega_f(h))^2 W(\x_t; \A_{t-1})}{2 \varsigma^2 M_{\Psi}} \right)
    \]
    Let $t \to \infty$, we have $|\beta_{t-1}^{-1/2} m(\x_t; \tilde{\A}_{t-1})| \pto 0$. Hence, $\widehat{\sigma}(\x_t; \A_{t-1}) \to 0$ gives $a(\x_t; \tilde{\A}_{t-1}) \pto 0$ as $t \to \infty$, which completes the proof.
\end{proof}




\subsection{Proofs in \texorpdfstring{\cref{sec:regret analysis}}{Section 4.3}}
\label{sec:proofs of regret analysis}

\begin{proof}[\rm\bfseries Proof of \cref{lem:finite count BOKE}]
    At round $t \le T$, denote the acquisition function as $u_t(\x) = m_t(\x) + \sqrt{\frac{\beta_t}{W_t(\x)}}$. Let $\delta = 2 \exp\left(-\frac{ \beta_t}{2 \varsigma^2 M_{\Psi}}\right) = \frac{2}{t^2}$ in \cref{thm:pointwise consistency KR}, we have with probability at least $1 - \frac{2}{t^2}$,
    \[
        u_t(\x) \le f(\x) + 2 \sqrt{\dfrac{\beta_t}{W_t(\x)}} + \omega_f(h_t).
    \]
    Similarly, with probability at least $1 - \frac{2}{t^2}$,
    \[
        u_t(\x^{\ast}) \ge f(\x^{\ast}) - \omega_f(h_t)
    \]
    Notice that $\Delta(\x) > 0$ implies $\x \neq \x^{\ast}$, thus $\omega_f(h_t) \le \frac{1}{4} \Delta(\x)$. Conditioning on $W_t(\x) \ge \frac{16 \beta_t}{\Delta^2(\x)}$, we have $\omega_f(h_t) + \sqrt{\frac{\beta_t}{W_t(\x)}} \le \frac{1}{2} \Delta(\x)$. Then by the union bound, the following holds
    \begin{equation}
        \label{eqn:probability of improved UCB}
        \Prob\left[u_t(\x) \ge u_t(\x^{\ast}) \;\Big|\; W_t(\x) \ge \frac{16 \beta_t}{\Delta^2(\x)}\right] \le \dfrac{4}{t^2}.
    \end{equation}
    For any $\ell \ge 1$, it holds that
    \begin{align*}
        n_T(\x)
        & = \sum_{t=0}^{T-1} \1\set{\x_{t+1} = \x} 
        \\
        & = \sum_{t=0}^{T-1} \1\set{\x_{t+1} = \x, n_t(\x) < \ell} + \sum_{t=0}^{T-1} \1\set{\x_{t+1} = \x, n_t(\x) \ge \ell} 
        \\
        & \le \ell + \sum_{t=1}^{T-1} \1\set{\x_{t+1} = \x, n_t(\x) \ge \ell}.
    \end{align*}
    The last inequality holds because if there exist $\ell + 1$ nonzero terms, then for the last round $t_0$, we have $\x_{t_0 + 1} = \x$ and $n_{t_0}(\x) \ge \ell$, which contradicts.

    Take $\ell = \lceil \frac{16 \beta_T}{\Delta^2(\x)} \rceil$, then
    \begin{align*}
        \E[n_T(\x)]
        & \stackrel{\textrm{(i)}}{\le} \dfrac{16 \beta_T}{\Delta^2(\x)} + 1 + \sum_{t=1}^{T-1} \Prob\left[\x_{t+1} = \x, W_t(\x) \ge \frac{16 \beta_t}{\Delta^2(\x)}\right] 
        \\
        & = \dfrac{16 \beta_T}{\Delta^2(\x)} + 1 + \sum_{t=1}^{T-1} \Prob\left[\x_{t+1} = \x \;\Big|\; W_t(\x) \ge \frac{16 \beta_t}{\Delta^2(\x)}\right] \Prob\left[W_t(\x) \ge \frac{16 \beta_t}{\Delta^2(\x)}\right] 
        \\
        & \le \dfrac{16 \beta_T}{\Delta^2(\x)} + 1 + \sum_{t=1}^{T-1} \Prob\left[u_t(\x) \ge u_t(\x^{\ast}) \;\Big|\; W_t(\x) \ge \frac{16 \beta_t}{\Delta^2(\x)}\right] 
        \\
        & \stackrel{\textrm{(ii)}}{\le} \dfrac{16 \beta_T}{\Delta^2(\x)} + 1 + \sum_{t=1}^{T-1} \dfrac{4}{t^2} 
        \\
        & \le \dfrac{C_1' \log T}{\Delta^2(\x)} + C_2',
    \end{align*}
    where (i) is given by $W_t(\x) \ge n_t(\x)$, and (ii) is given by Eq.~\eqref{eqn:probability of improved UCB}.
\end{proof}




\printbibliography[heading=bibintoc, title=References]


\end{document}