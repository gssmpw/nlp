\section{Experiment Design}\label{sec:Experiment}

\subsection{Dataset} 


The Databricks Documentation Evaluation Set\footnote{\url{https://notebooks.databricks.com/demos/dbdemos-dataset/llm/databricks-documentation/databricks_doc_eval_set.parquet}} is a structured dataset designed to evaluate the effectiveness of the Retrieval-Augmented Generation (RAG) process in generating accurate and contextually relevant responses based on Databricks documentation. It consists of user queries (\texttt{request}), expected retrieved contextual information from official documentation (\texttt{expected\_retrieved\_context}) and corresponding high-quality, manually curated responses (\texttt{expected\_response}), which serve as the gold standard for evaluation. Each query is uniquely identified by a \texttt{request\_id}. Additionally, the responses (\texttt{response}) are generated using a Retrieval-Augmented Generation (RAG) system that integrates relevant contextual information, powered by the databricks-meta-llama-3-70b-instruct model.

% The dataset is publicly accessible from Databricks.

% \begin{verbatim}
% https://notebooks.databricks.com/demos/dbdemos-dataset/llm/databricks-documentation/databricks_doc_eval_set.parquet
% \end{verbatim}

\begin{table}[!thb]\footnotesize
\centering
% \small % Reduce the font size of the table
% \setlength{\tabcolsep}{4pt} % Reduce column spacing
% \renewcommand{\arraystretch}{1.2} % Adjust row spacing
% \resizebox{\linewidth}{!}{ % Resize the table to fit within the column width
\begin{tabular}{lcc}
\hline
\textbf{Statistic}               & \textbf{Request} & \textbf{Response} \\ \hline
Total Count                      & 192                         & 192                          \\
Average Length (chars)           & 124.03                      & 954.76                       \\
Min Length (chars)               & 53                          & 56                           \\
Max Length (chars)               & 1024                        & 5763                         \\
Average Word Count               & 21.04                       & 139.34                       \\ \hline
\end{tabular}
% }
\caption{This table provides an overview of key statistics for requests and responses, including counts, character lengths, and average word counts.}
\label{tab:request_response_statistics}
\end{table}


% \begin{example}
% Example:

% \textbf{Request:} What is the syntax for using the schema\_of\_csv function in Databricks SQL?

% \textbf{Response:} The syntax for using the schema\_of\_csv function in Databricks SQL is:
% schema\_of\_csv(csv [, options]). Where CSV is a STRING literal with valid CSV data, and options is an optional MAP literal where keys and values are STRING. The function returns a STRING composing a struct, with field names derived by position as \_Cn and values holding the derived formatted SQL types. For details on options, see the from\_csv function.

% \textbf{Expected Response:} The syntax for using the schema\_of\_csv function in Databricks SQL is:
% schema\_of\_csv(csv [, options]).
% Where csv is a STRING literal with valid CSV data, and options is an optional MAP literal where keys and values are STRING. The function returns a STRING composing a struct. The field names are derived by position as \_Cn. The values hold the derived formatted SQL types. For details on options, see the from\_csv function.
% \end{example}



\subsection{LLMs} 

For this experiment, we selected five of the most popular commercial large language models: OpenAI GPT-4o, GPT-4o-mini~\cite{achiam2023gpt}, Meta Llama3.1 two versions~\cite{touvron2023llama} and Mixtral-8x7B Instruct~\cite{jiang2024mixtral} . We utilized their respective APIs. Table~\ref{tab:LLMs} summarizes the details of these five LLMs. Our baseline model to perform the first step is a customized GPT-4o trained for our used dataset.



\begin{table}[!thb]\footnotesize
\centering
\begin{tabular}{|lllll|}
\hline
\multicolumn{5}{|l|}{LLM informations}                         \\ \hline
\multicolumn{2}{|l|}{LLMs used as Judges}  & \multicolumn{3}{l|}{ Model Size} \\ \hline

\multicolumn{2}{|l|}{Baseline (Customized GPT-4o)}  & \multicolumn{3}{l|}{Not Officially Disclosed}      \\ \hline

\multicolumn{2}{|l|}{OpenAI GPT-4o mini}  & \multicolumn{3}{l|}{Not Officially Disclosed}      \\ \hline
\multicolumn{2}{|l|}{OpenAI GPT-4o}  & \multicolumn{3}{l|}{Not Officially Disclosed}      \\ \hline
\multicolumn{2}{|l|}{Mixtral - 8x7B } & \multicolumn{3}{l|}{8 Experts, size of each expert is 7B}      \\ \hline
\multicolumn{2}{|l|}{MetaAI Llama3.1-databricks} & \multicolumn{3}{l|}{meta-llama-3-70b-instruct  (70B)}      \\ \hline
\multicolumn{2}{|l|}{MetaAI Llama3.1-databricks} & \multicolumn{3}{l|}{meta-llama-3-1-405b-instruct (405B)}      \\ \hline


% \multicolumn{2}{|l|}{LLMs used as Judge}  & \multicolumn{3}{l|}{Model Size} \\ \hline

% \multicolumn{2}{|l|}{Baseline \newline (Customized GPT-4o)}  & \multicolumn{3}{l|}{Not Officially Disclosed}      \\ \hline

% \multicolumn{2}{|l|}{OpenAI \newline GPT-4o mini}  & \multicolumn{3}{l|}{Not Officially Disclosed}      \\ \hline
% \multicolumn{2}{|l|}{OpenAI \newline GPT-4o}  & \multicolumn{3}{l|}{Not Officially Disclosed}      \\ \hline
% \multicolumn{2}{|l|}{Mixtral - 8x7B} & \multicolumn{3}{l|}{8 Experts, size of each expert is 7B}      \\ \hline
% \multicolumn{2}{|l|}{MetaAI \newline Llama3.1} & \multicolumn{3}{l|}{databricks-meta-llama-3  70b-instruct (70B)}      \\ \hline
% \multicolumn{2}{|l|}{MetaAI \newline Llama3.1} & \multicolumn{3}{l|}{databricks-meta-llama-3-1-405b-instruct (405B)}      \\ \hline


\end{tabular}%
\caption{Large language models studied in this paper.}
\label{tab:LLMs}
\end{table}

% It is often used to measure whether AI-generated responses match human preferences, ethical considerations, or domain expertise.

\subsection{Evaluation Metric}

Human Alignment Rate (HAR) refers to how well an AI system, especially for LLM, aligns its outputs with human expectations, values, or judgments and is widely used in industry. It has also been used as a reward function for Reinforcement learning~\cite{kaufmann2023survey,kabir2025beyond} or ranking optimization~\cite{song2024preference}.
HAR equation~\ref{equ:HAR} can be seen below: 

\begin{equation}\label{equ:HAR}\small
\text{HAR} = \frac{\text{\# of AI outputs that match human judgments}}{\text{Total \# of evaluated outputs}} \times 100\%
\end{equation}

% \begin{equation}\small
% \text{HAR} = \frac{\begin{array}{c} 
% \text{\# of AI outputs that} \\ 
% \text{match human judgments} 
% \end{array}}{\text{Total \# of evaluated outputs}} \times 100\%
% \end{equation}




% \subsection{Experimental Results}

% We first provide the overall performance. Table ~\ref{tab:model_results} compares the HAR (\%) of several models. The Original baseline Model has an 85.9\% rate. In our two step mechanism, we require following LLMs to re-evaluate the generated results prompted by our designed prompt.
% First, we observe GPT-4o achieves 94.3\%, and its smaller version, GPT-4o-mini, scores 88.5\%. We also tested two versions of LLama, Llama 3.1 70B Instruct reaches 89.1\%, and the larger Llama 3.1 405B Instruct improves to 93.8\%. The top performer is Mixtral-8x7B Instruct with 95.8\%, showing that our designed mechanism generally aligns better with human expectations.



% \begin{table}[ht]
% \centering
% \caption{Comparison of Model Results After Two Step LLM as Judge}
% \label{tab:model_results}
% \centering
% \begin{tabular}{|l|c|}
% \hline
% \textbf{\centering Model} & \textbf{Human Alignment} \\ 
%                           & \textbf{Rate (\%)}       \\ \hline
% Baseline Model          & 85.9                     \\ \hline
% GPT-4o                    & 94.3                     \\ \hline
% GPT-4o-mini               & 88.5                     \\ \hline
% Llama 3.1 70B Instruct     & 89.1                     \\ \hline
% Llama 3.1 405B Instruct    & 93.8                     \\ \hline
% Mixtral-8x7B Instruct      & 95.8                     \\ \hline
% \end{tabular}
% \end{table}








% \begin{table}[htb]
% \centering
% \caption{One-way ANOVA and Tukey HSD Test Results}
% \label{tab:anova_tukey_results}
% % \resizebox{\textwidth}{!}{%
% \begin{tabular}{lcc}
% \hline
% \textbf{Statistic} & \textbf{Value} \\ \hline
% F-statistic        & 10.3042        \\
% p-value            & 0.0000         \\ \hline
% \end{tabular}%
% % }
% \end{table}





% \begin{table}[htb]\small
% \vspace{0.5em}

% % \resizebox{0.5\textwidth}{!}{%
% \begin{tabular}{lcccccc}
% \hline
% \textbf{Group1}    & \textbf{Group2}       & \textbf{p-adj} \\ \hline
% GPT\_4o            & GPT\_4o\_mini                   & 0.0000                   \\
% GPT\_4o            & Llama\_3.1\_405B                 & 0.9392             \\
% GPT\_4o            & Llama\_3.1\_70B                & 0.9893                \\
% GPT\_4o            & Mixtral-8x7B                      & 0.0777                  \\
% GPT\_4o\_mini      & Llama\_3.1\_405B                 & 0.0000              \\
% GPT\_4o\_mini      & Llama\_3.1\_70B                    & 0.0000               \\
% GPT\_4o\_mini      & Mixtral-8x7B                       & 0.0254            \\
% Llama\_3.1\_405B   & Llama\_3.1\_70B                & 0.9984                \\
% Llama\_3.1\_405B   & Mixtral-8x7B                     & 0.3762           \\
% Llama\_3.1\_70B    & Mixtral-8x7B                 & 0.2263              \\ \hline
% \end{tabular}%
% % }
% \label{p_value}
% \caption{pair wise statistical significance difference testing}
% \end{table}





% \begin{figure*}[!hbt]
% \centering
% \includegraphics[width=0.9\textwidth]{images/ANOVA_test.png}
% \caption{One-Way ANOVA: Model Score Distributions. 
% This violin plot visualizes the score distributions for different models, including GPT-4o, GPT-4o-mini, Llama 3.1 70B, Llama 3.1 405B, and Mixtral-8x7B. Each violin represents the distribution's density, with a boxplot inside showing the median and interquartile range. The wider sections indicate where scores are more concentrated, while the thinner sections show sparsity. This figure highlights the variability and spread of model scores, providing insights into performance differences across models.}
% \label{fig:anova_violin}
% \end{figure*}

