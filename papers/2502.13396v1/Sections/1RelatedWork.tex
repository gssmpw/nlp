



\section{Related Work}\label{sec:background}

\noindent\textbf{LLM-as-a-Judge}: Previous work has explored the use of LLM-as-Judges and established benchmarks for evaluating their performance. For instance, ~\cite{zheng2023judging} investigates how LLMs can assess the quality of chatbot responses and compares their performance to human judgments. Additionally, ~\cite{kojima2022large} demonstrates that LLMs can perform reasoning tasks without explicit training, a capability that is foundational for their use as judges in evaluating complex outputs. Specifically, ~\cite{beurer2023prompting} examines how prompt engineering can guide LLMs to perform specific tasks, including evaluation and judgment, by treating prompts as a form of programming.


\noindent\textbf{Distinct from previous work}: Our work introduces two key innovations that differentiate it from these prior studies: 1) While earlier research primarily relied on the inherent reasoning and evaluation capabilities of LLMs, our approach introduces a novel explicit error weighting mechanism. Unlike traditional methods that treat all errors or evaluation criteria uniformly, our system assigns dynamic weights to different types of errors based on their significance. This ensures that critical errors are prioritized, leading to more accurate and context-aware evaluations. 2) To conduct this mechanism, we encode a weighting algorithm into our prompts that explicitly guide the LLM to focus on the most important aspects of the task. 




% 1) While earlier research primarily leveraged the inherent reasoning and evaluation abilities of LLMs, our approach integrates an explicit error weighting mechanism. This means that instead of treating all errors or evaluation criteria uniformly, we assign specific weights to different types of errors based on their importance. 2) We prompt our weighted focus to guide the LLM to evaluate responses, also highlight and interpret the weighted errors effectively.