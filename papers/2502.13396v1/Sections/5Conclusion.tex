


\section{Conclusion }\label{sec:conclusion}


In this work, we introduce a two-step mechanism designed to enhance using LLM-as-a-Judge. Our approach incorporates a weighting algorithm into the prompt, which guides the LLM during the second-step evaluation process.  We conduct a case study using a dataset specialized in software engineering and evaluate its performance across five widely used LLMs. Our proposed method achieves an average improvement of 6\% in HAR. Particularly, Mixtral-8x7B Instruct emerged as the clear winner in this competition, achieving a 95.8\% HAR, outperforming all other LLMs, including two customized Llama models.

% Meanwhile, we found Mixtral-8x7B Instruct preforms best in this task.



\section{Limitations}

Our case study demonstrates the effectiveness of the proposed method on a single dataset, and the prompts used in this study were manually designed to address the unweighted evaluation issue. While the results are promising, the approach may face scalability challenges until it is tested on a broader range of datasets. Additionally, future work should explore the use of auto-generated prompts to improve efficiency and reduce reliance on manual design. AI assistant was utilized in the writing process.

