

\section{Introduction}\label{sec:intro}
% Evaluating Natural Language Generation (NLG) tasks has always been a long-standing challenge, and it has become even more complex in the era of LLMs. Tasks that were previously treated as separate domains, such as text summarization, machine translation, and question answering, are now unified under the broader generative framework. As a result, traditional, task-specific evaluation metrics must adapt to accommodate the more complex and diverse one-to-many nature of generative outputs.

% Subjective evaluation methods, such as expert-based testing, are often considered the gold standard for NLG evaluation due to their ability to incorporate holistic reasoning and nuanced contextual understanding. However, these approaches are costly, difficult to scale, and prone to inconsistency due to variations in human judgment \cite{gao2023human,shi2024judging, gu2024survey}.


 Automated evaluation metrics like BERTScore~\cite{zhang2019bertscore} and BARTScore~\cite{yuan2021bartscore} offer scalable and cost-effective performance in evaluating Natural Language Generation (NLG) tasks. BERTScore, while leveraging pre-trained semantic embeddings, often struggles to capture contextual nuances and may overemphasize lexical similarity over factual accuracy or logical consistency~\cite{sellam2020bleurt,sai2022survey}. BARTScore, though designed to improve upon these limitations, still faces challenges in aligning with human judgments, particularly in tasks requiring domain-specific knowledge or complex reasoning.\cite{lu2022toward,liu2023llms}. As a result, while automated metrics provide efficiency and scalability, they often fall short of replicating the comprehensiveness and depth of expert-driven evaluations. This highlights the need for hybrid approaches that combine the strengths of human expertise with the scalability of automated methods.

As LLMs continue to evolve, employing them as judges, called ``LLM-as-a-Judge''  is becoming increasingly compelling. LLMs, trained on vast and diverse datasets, have developed a nuanced understanding of language, enabling them to assess the quality of generated content effectively. Their ability to provide consistent evaluations can mitigate the variability inherent in human assessments. Moreover, using LLMs for evaluation is scalable and efficient, allowing for rapid assessments across large datasets without the logistical constraints of coordinating human evaluators~\cite{dong-etal-2024-llm}.

% This method combines the scalability and consistency of automated systems with the detailed, context-sensitive insights characteristic of expert evaluations. Moreover, because LLMs are trained as generative models, they naturally excel at producing innovative and adaptable responses, further enhancing their evaluative capabilities. 

Yet, LLMs rely on contextualized information and often prioritize conventional details from training data. However, in certain cases, these elements may not be crucial, leading the model to misjudge their relative importance 
\cite{petroni2019language,jiang2020can,lin2021truthfulqa}.
Also, LLMs struggle to assign appropriate weights to different types of errors, such as factual errors versus grammatical errors 
\cite{sai2022survey,chiang2023can,zheng2023judging}.

% In today’s industry, data privacy and security are top priorities, which means sensitive or proprietary information is often excluded from the training datasets of LLMs. Thus, LLMs may lack exposure to domain-specific data during their training process, limiting their ability to fully understand and contextualize information for specialized tasks \cite{gururangan2020don}. 

% This gap can reduce their effectiveness in handling domain-specific applications, where nuanced, contextual understanding is critical for accurate and reliable performance.\cite{gururangan2020don}

In the NLG system, for tech companies, the primary users are often professional software developers or domain experts. When these users interact with a search system, they typically have a general idea of the desired output. As they review the results, they inherently apply a weighting mechanism to prioritize the most relevant components and focus on the information that aligns with their specific needs. Currently, a growing trend among companies is to use LLM-as-a-Judge to evaluate the quality of generated text. These evaluations often involve comparing the model’s output to a gold standard and assessing whether it meets predefined criteria. While this approach has its merits, a significant limitation arises: LLMs frequently fail to accurately weigh the importance of different components in the text\cite{petroni2019language,jiang2020can,lin2021truthfulqa}. For instance, in a software development context, an LLM might overemphasize the importance of a software release date thus giving rejection, even when the user's main objective is to obtain details about a specific feature or functionality.
Such trivial misunderstandings significantly limit the usefulness of LLM-as-a-Judge in domain-specific contexts. 

% Without a mechanism to align the model’s evaluation criteria with the priorities of expert users, the generated text may fail to meet their expectations, ultimately reducing the system’s overall effectiveness.

% [Reference Needed: Importance of Aligning AI with User Needs].

% While many companies might want to train their own LLMs to gain a competitive edge or tailor models to their specific needs, there are multiple significant barriers that make this difficult.  For instance,
% Training an LLM requires enormous computational power, typically involving thousands of GPUs or TPUs running for weeks or even months. The cost of these resources is prohibitively high for most organizations.\cite{brown2020language,patterson2022carbon}. In addition to other challenges such as large-scale, high-quality data.Scalability and Infrastructure
% \cite{bender2021dangers,rajbhandari2020zero}

In this paper, we present a case study that addresses the aforementioned issue through two key strategies: "Explicit Error Weighting" and "Prompt Engineering". In the first step, we follow the original setting to use LLM-as-a-Judge (without customized prompt). Then we calculate the Human Alignment Rate (HAR). 
In our second step judge, we tested different LLM-as-Judges individually with our customized prompt. On average, we achieve a 6.4\% improvement in HAR, demonstrating the effectiveness of this method in aligning LLM decisions with human judgments.


% This misalignment between the LLM’s evaluation and the user’s actual needs highlights a critical issue: LLMs lack the ability to understand the contextual relevance of specific details in domain-specific scenarios [Reference Needed: Domain-Specific Challenges in NLP].

% Specifically, we tested our approach on a designated dataset using five widely adopted LLMs, with a focus on enhancing decision-making through a two-step evaluation process.












