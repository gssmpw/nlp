\section{Background}\label{sec:Background}
\vspace{-2mm}
\subsection{Minor Issue and Motivation}
% \Dongji{todo}
% "LLM as judge" faces several challenges. 

Traditional evaluation metrics incorporate weighting algorithms directly into their formulas ~\cite{feng2023joint,lin2004rouge}, which limits scalability and generalizability. An alternative approach is prompt design, which allows for integrating algorithms as plain text or in different formats within the prompt itself. Our case study specifically examines the issue that LLMs exhibit a low tolerance for minor omissionsâ€”an aspect that, from a conventional perspective, is considered critical due to their training on vast corpora of conventional knowledge. 


% To develop evaluation frameworks that prioritize flexibility, user alignment, and domain adaptability to bridge the gap between theoretical benchmarks and practical usability.

% Additionally, the system struggles to reward more specific or detailed responses and often misaligns evaluations with user intent by relying on rigid coverage-based rules.  

% with judges penalizing responses for missing non-critical details despite aligning with the user's conventional intent.

% \subsection{motivation}
 % To develop evaluation frameworks that prioritize flexibility, user alignment, and domain adaptability to bridge the gap between theoretical benchmarks and practical usability.

\subsection{Proposed Prompt Design}\label{sec:promptDesign}
To improve the performance of using "LLM-as-a-Judge", we propose the following framework. We consider the evaluating process is essentially assessing \textbf{``Factual Content''}, which has three types: 1) \textbf{``Critical Fact''}, 2) \textbf{``Supporting Fact''}, and 3) \textbf{``Trivial Facts''}. In this framework, a "Fact" is any verifiable piece of information within a response, while "Critical Facts" are the essential elements that directly address the user's query and whose absence significantly weakens the answer. "Supporting Facts" provide additional context and clarification that enrich the response without being strictly necessary for correctness, and "Trivial Facts" are minor details that, although they can enhance the narrative, are optional and their omission is not penalized. The evaluation prompt will utilize the Algorithm ~\ref{alg:ai_response_evaluation}. From this algorithm, we incorporated our defined three types of facts, taking into account semantic similarity, output format, and required explanation in the evaluation result. This prompt aligns with level 6 of the TELeR prompt design taxonomy ~\cite{santu2023teler}. 

% \begin{quote}
% \textit{
% You are an AI judge evaluating the quality of an AI-generated response compared to a gold standard response. Your task is to determine if the AI response matches the gold response based on the following criteria:
% }
% \begin{enumerate}
%     \item Compare the factual content of both responses.
%     \item Check if the AI response includes facts that are also present in the gold response.
%     \item The AI response can have additional facts not present in the gold response.
%     \item The AI response should not miss any critical or supporting facts from the gold response.
%     \item The AI response can miss trivial facts from the gold response.
% \end{enumerate}
% \textit{
% Please analyze the following responses:
% }
% AI Response: \{ai\_response\}

% Gold Response: \{gold\_response\}

% \textit{
% Provide your evaluation in the following JSON format:
% }

% \begin{tabular}{@{}ll@{}}
% \{ & \\
%     & "semantic\_similarity": \textless float between 0 and 1\textgreater, \\
%     & "fact\_match\_ratio": \textless float between 0 and 1\textgreater, \\
%     & "critical\_facts\_missed": \textless integer\textgreater, \\
%     & "supporting\_facts\_missed": \textless integer\textgreater, \\
%     & "trivial\_facts\_missed": \textless integer\textgreater, \\
%     & "final\_score": \textless float between 0 and 1\textgreater, \\
%     & "explanation": \textless string explaining your evaluation\textgreater \\
% \} & \\
% \end{tabular}
% \end{quote}

\begin{algorithm}[!thb]\footnotesize
\caption{AI Response Evaluation Framework}
\label{alg:ai_response_evaluation}

\begin{algorithmic}
\Require Generated AI response (\texttt{AI\_Response}), Gold standard response (\texttt{Gold\_Response})
\Ensure Evaluation metrics:  
  \begin{itemize}
    \item \texttt{semantic\_similarity}
    \item \texttt{fact\_match\_ratio}
    \item \texttt{critical\_facts\_missed}
    \item \texttt{supporting\_facts\_missed}
    \item \texttt{trivial\_facts\_missed}
    \item \texttt{final\_score}
    \item \texttt{explanation}
  \end{itemize}

\State \textbf{Step 1: Compare Responses}  
    \State Compare the factual content of \texttt{AI\_Response} with \texttt{Gold\_Response}.

\State \textbf{Step 2: Fact Inclusion}  
    \State Identify critical and supporting facts in \texttt{Gold\_Response} and check if they exist in \texttt{AI\_Response}.

\State \textbf{Step 3: Additional Facts}  
    \State Allow \texttt{AI\_Response} to include additional relevant facts not present in \texttt{Gold\_Response}.

\State \textbf{Step 4: Tolerate Omissions}  
    \State Permit omissions of trivial facts in \texttt{AI\_Response} without penalty.

\State \textbf{Step 5: Generate Evaluation Metrics}  
    \State Compute evaluation metrics and format them into a JSON object.

\State \textbf{Step 6: Output Results}  
    \State Return the JSON object containing the computed evaluation metrics.

\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[H]
% \caption{AI Response Evaluation Framework}
% \label{alg:ai_response_evaluation}
% \begin{algorithmic}[1]
% \Require Generated AI response (\texttt{AI\_Response}), Gold standard response (\texttt{Gold\_Response})
% \Ensure Evaluation metrics: \texttt{semantic\_similarity}, \texttt{fact\_match\_ratio}, \texttt{critical\_facts\_missed}, \texttt{supporting\_facts\_missed}, \texttt{trivial\_facts\_missed}, \texttt{final\_score}, \texttt{explanation}
% \State {\textbf{Step 1: Compare Responses:}}
% \State Compare the factual content of \texttt{AI\_Response} with \texttt{Gold\_Response}.
% \State \textbf{Step 2: Fact Inclusion:}
% \State Check if all critical and supporting facts in \texttt{Gold\_Response} are present in \texttt{AI\_Response}.
% \State \textbf{Step 3: Additional Facts:}
% \State Allow the \texttt{AI\_Response} to include additional facts not present in \texttt{Gold\_Response}.
% \State \textbf{Step 4: Tolerate Omissions:}
% \State Permit omissions of trivial facts in \texttt{AI\_Response}.
% \State \textbf{Step 5: Generate Evaluation Metrics:}
% \State Output a JSON object containing the evaluation metrics.
% \end{algorithmic}
% \end{algorithm}



