% \vspace{5em}
% \begin{table*}[t]
\begin{table}[!h]
% \vspace{-2.5em}
% \hspace{-0.6em}
\centering
\scalebox{0.85}{ 
 
\begin{tabular}{@{}lccc@{}} % Changed from || to |
% \toprule
 \textbf{Models + ICL Methods} & {\dataname{} Avg.} &   {\alpaca{} Avg.} &  {\mtbench{} Avg.}\\
\midrule

Mistral-7B-v0.1 + \iconminiride $\textbf{RIDE}_{\text{f}}$             &    4.55          &   \textbf{4.56} & \textbf{6.74} \\
Mistral-7B-v0.1 + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$       &    4.55          &   4.52 & 6.43 \\
Mistral-7B-v0.1 + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$       &    \textbf{4.60}  &   4.47 & 6.52 \\
Mistral-7B-Instruct-v0.1                                 &    4.03          &   4.09 & 6.59 \\

\bottomrule
\end{tabular}
}
\caption{In the table, "\dataname{} Avg." represents the average score obtained by computing the mean of six metrics: "Helpful", "Factual", "Deep", "Engaging", "Clear", and "Safe". This serves as an indicator of the model's overall alignment capability on the \dataname{} dataset.
Similarly, "\alpaca{} Avg." is calculated as the mean of "Helpful", "Factual", "Deep", "Engaging", and "Clear", representing the model’s factuality capability on the \alpaca{} dataset.
Finally, "\mtbench{} Avg." refers to the average of the turn-1 and turn-2 metrics in \mtbench{}, reflecting the model’s ability to handle complex tasks within the \mtbench{} dataset.} 
 \vspace{-0.5em}   
\label{tab:win_finetune}
\end{table}


