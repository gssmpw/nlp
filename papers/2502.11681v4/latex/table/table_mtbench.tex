% \vspace{5em}
% \begin{table*}[t]
\begin{table*}[!h]
% \vspace{-2.5em}
% \hspace{-0.6em}
\centering
\scalebox{0.82}{ 
 
\begin{tabular}{@{}lcccccccc@{}} % Changed from || to |
% \toprule
\textbf{Models + ICL Methods} &  {\small \textbf{Coding}} &  {\small \textbf{Extraction}} &   {\small \textbf{Humanities}} &   {\small \textbf{Math}} &   {\small \textbf{Reasoning}} &   {\small \textbf{Roleplay}} &    {\small \textbf{Stem}} &    {\small \textbf{Writing}} \\
 \midrule
% {\small \faToggleOn} Vicuna-7b (SFT)        &          \textbf{4.43} &      \textbf{4.85} &         \textbf{4.33} &    \textbf{4.04} &         4.51 &     4.60 &  {4.46} &    184.8 \\
% {\small \faToggleOn} Llama2-7b-chat (RLHF)  &          4.10 &      4.83 &         {4.26} &    3.91 &         \textbf{4.70} &     \textbf{5.00} &  \textbf{4.47} &    \textbf{246.9} \\ 
% \midrule
Llama2-7b + \iconminiurial \textbf{\methodname{}}           &   1.60           &  3.30           &  \textbf{8.50}  &  1.55             &  3.25             &  6.50             &   6.53            &   \textbf{6.35}    \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$       &   1.85           &  3.63           &  7.97           &  \textbf{2.35}    &  \textbf{3.80}    &  6.70             &   7.03            &   6.05 \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$ &   2.05           &  3.40           &  7.72           &  1.55             &  3.25             &  \textbf{6.83}    &   \textbf{7.28}   &   5.30 \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$ &   \textbf{2.15}  & \textbf{3.95}   &  7.92           &  1.45             &  \textbf{3.80}    &  6.45             &   7.22            &   5.00 \\

\midrule
% Mistral-7b-instruct (SFT)   &          4.36 &      4.87 &         4.29 &    3.89 &         4.47 &     4.75 &  4.44 &    155.4 \\
% Mistral-7b (\methodname{})       &          4.57 &      4.89 &         4.50 &    4.18 &         4.74 &     4.92 &  4.63 &    186.3 \\
Mistral-7b + \iconminiurial \textbf{\methodname{}}             &   4.50  &  \textbf{7.55}     &  8.45             &  \textbf{3.55}  &  4.60           &  7.12             &   8.00        &   7.92 \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$         &   4.30           &  7.10              &  \textbf{9.50}  &  \textbf{3.55}    &  4.60           &  7.80             &  \textbf{8.60} &   \textbf{8.47} \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$   &   4.35          & 7.25                &  9.25             &  3.30           &  4.55           &  \textbf{7.90}    &  7.62         &   7.22 \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$   &   \textbf{4.55}          & \textbf{7.55}       &  9.35           &  2.80             &  \textbf{4.65}  &  7.78             &  7.95         &   7.55 \\

\midrule 
% Llama2-70b-chat (RLHF)   &      4.50 &      4.92 &         4.54 &    4.28 &         4.75 &     5.00 &  4.67 &    257.9 \\
% Llama2-70b (zero-shot prompt)         &          3.70 &      4.31 &         3.78 &    3.19 &         3.50 &     1.50 &  3.33 &    166.8 \\
% Llama2-70b (\methodname{}-1shot) &          4.60 &      4.93 &         4.54 &    4.09 &         4.67 &     4.88 &  4.62 &    155.3 \\
% Llama2-70b (\methodname{})       &          4.72 &      4.95 &         4.65 &    4.30 &         4.85 &     4.96 &  4.74 &    171.4 \\
Olmo-7b + \iconminiurial \textbf{\methodname{}}                &   1.65            &  2.35             &  5.33           &  1.40           &  3.05           &  5.74           &   \textbf{5.30}    &   3.50 \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$          &   1.75              &  3.15             &  \textbf{6.38}  &  1.45           &  \textbf{3.35}  &  5.20           &   \textbf{5.30}    &   \textbf{4.20} \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$    &   1.50              &  3.32             &  4.85           &  1.10           &  2.70           &  5.25           &    5.03           &   3.30 \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$     &   \textbf{1.80}    &  \textbf{3.40}    &  5.08           &  \textbf{1.60}  &  2.95           &  \textbf{5.88}  &    4.58           &   3.60 \\

% \midrule \midrule
% {\small \faToggleOn} \texttt{gpt-3.5-turbo-0301}        &          4.81 &      4.98 &         4.83 &    4.33 &         4.58 &     4.94 &  4.75 &    154.0 \\
% {\small \faToggleOn} \texttt{gpt-4-0314}           &          \textbf{4.90} &      \textbf{4.99} &         4.90 &    \textbf{4.57} &         \textbf{4.62} &     4.74 &  {4.79} &    \textbf{226.4} \\
% {\small \faToggleOn} \texttt{gpt-4-0613}           &          4.86 &      \textbf{4.99} &         \textbf{4.90} &    4.49 &         4.61 &     \textbf{4.97} &  \textbf{4.80} &    186.1 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Multi-aspect scoring evaluation of ICL methods on \mtbench.} 
% (Scores are on a scale of 1-10.) 
% The icon {\small \faToggleOn} indicates the models are \textit{tuned} for alignment via SFT or RLHF, while {\small \faToggleOff} means the models are \textit{untuned}. 
% Our method \methodname{} uses 1k static prefix tokens (system prompt + 3 examples) for ICL.
 \vspace{-0.5em}}   
\label{tab:mtbench}
\end{table*}


