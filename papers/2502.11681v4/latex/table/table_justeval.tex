% \vspace{5em}
% \begin{table*}[t]
\begin{table*}[!h]
% \vspace{-2.5em}
% \hspace{-0.6em}
\centering
\scalebox{0.85}{ 
 
\begin{tabular}{@{}lcccccccc@{}} % Changed from || to |
% \toprule
 \textbf{Models + ICL Methods} &  {\textbf{Helpful}} &  {\textbf{Factual}} &   {\textbf{Deep}} &   {\textbf{Engaging}} &   {\textbf{Clear}} &   {\textbf{Safe}} & {\textbf{Average}} &   {\textbf{Length}} \\
 \midrule
% {\small \faToggleOn} Vicuna-7b (SFT)        &          \textbf{4.43} &      \textbf{4.85} &         \textbf{4.33} &    \textbf{4.04} &         4.51 &     4.60 &  {4.46} &    184.8 \\
% {\small \faToggleOn} Llama2-7b-chat (RLHF)  &          4.10 &      4.83 &         {4.26} &    3.91 &         \textbf{4.70} &     \textbf{5.00} &  \textbf{4.47} &    \textbf{246.9} \\ 
% \midrule
Llama2-7b + \textbf{Zero-shot}                      &   2.94            &  2.79             &  2.57             & 3.66          & 3.65          & 2.24              &  2.98                 & 211.99  \\
Llama2-7b + \textbf{Vanilla ICL}                    &   3.21            &  3.26             &  2.85             & 4.00          & 3.96          & 2.55              &  3.31                 & 224.52  \\
Llama2-7b + \textbf{Retrieval ICL}                  &   3.27            &  3.19             &  3.17             & 4.04          & 3.87          & 2.75              &  3.38                 & 229.17  \\
Llama2-7b + \textbf{TopK + ConE}                    &   3.44            &  3.45             &  3.20             & 4.02              & 4.16          & 2.80          &  3.51                 & 226.11  \\

\midrule

Llama2-7b + \iconminiurial \textbf{\methodname{}}                  &    3.98              &  \textbf{3.98}  &   3.64             & 4.36           & 4.52          & 4.42           &  4.15              & 239.81  \\

Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$              &    \textbf{4.09}    &  3.87           &   \textbf{3.82}    & \textbf{4.52}  & \textbf{4.56}  & 2.81           &  3.95                 & \textbf{303.41}  \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$        &    3.90           &  3.90            &   3.64            & 4.34              & 4.48          & 4.17           &  4.07                 & 266.76 \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$        &    3.95           &  3.95             &   3.69            & 4.40              & 4.52          & \textbf{4.45}  &  \textbf{4.16}       & 238.05  \\


\midrule \midrule
% Mistral-7b-instruct (SFT)   &          4.36 &      4.87 &         4.29 &    3.89 &         4.47 &     4.75 &  4.44 &    155.4 \\
% Mistral-7b (\methodname{})       &          4.57 &      4.89 &         4.50 &    4.18 &         4.74 &     4.92 &  4.63 &    186.3 \\
% {\small \faToggleOn} Mistral-7b-instruct (SFT)   &          4.36 &      4.87 &         4.29 &    3.89 &         4.47 &     4.75 &      {155.4} \\
Mistral-7b + \iconminiurial \textbf{\methodname{}}                 &    4.41           &      4.43             &         3.90      &    4.57       &         4.79      &     \textbf{4.89} & 4.50           &   214.60 \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$             &    \textbf{4.67}  &      \textbf{4.49}    &   \textbf{4.42}   & \textbf{4.75} &  \textbf{4.85}    &     4.13          & 4.55  &   \textbf{304.51} \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$       &    4.59           &      4.44             &         4.27      &    4.69       &       4.83        &     4.50          & 4.55           &   289.19 \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$       &    4.58           &      4.43             &         4.16      &    4.63       &         4.83      &     \textbf{4.89} & \textbf{4.60}           &   252.69 \\

\midrule \midrule
Olmo-7b + \iconminiurial \textbf{\methodname{}}                    &   3.45            &  3.62             &    3.13           &    3.94           &     4.20          &  \textbf{2.70}   & \textbf{3.51}    &   203.86 \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$                &   \textbf{3.52}   &  3.57             &    \textbf{3.20}  &    \textbf{4.10}  &     \textbf{4.27} &  1.79             & 3.41  &   \textbf{225.31} \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$          &   3.46            &  3.61             &    3.14           &    3.93           &     4.25          &  2.44             & 3.47    &   200.92 \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$          &   3.44            &  \textbf{3.65}    &    3.08           &    3.88           &     4.20          &  2.69             & 3.48    &   189.96 \\

% \midrule \midrule
% {\small \faToggleOn} \texttt{gpt-3.5-turbo-0301}        &          4.81 &      4.98 &         4.83 &    4.33 &         4.58 &     4.94 &  4.75 &    154.0 \\
% {\small \faToggleOn} \texttt{gpt-4-0314}           &          \textbf{4.90} &      \textbf{4.99} &         4.90 &    \textbf{4.57} &         \textbf{4.62} &     4.74 &  {4.79} &    \textbf{226.4} \\
% {\small \faToggleOn} \texttt{gpt-4-0613}           &          4.86 &      \textbf{4.99} &         \textbf{4.90} &    4.49 &         4.61 &     \textbf{4.97} &  \textbf{4.80} &    186.1 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Multi-aspect scoring evaluation of alignment methods on \dataname{}.} Each block is corresponding to one specific LLM.  Scores are on a scale of 1-5. {\textbf{Average}} refers to the averaged score of the 6 metrics and {\textbf{Length}} is computed by number of words. 
% The icon {\small \faToggleOn} indicates the models are \textit{tuned} for alignment via SFT or RLHF, while {\small \faToggleOff} means the models are \textit{untuned}. 
% Our method \methodname{} uses 1k static prefix tokens (system prompt + 3 examples) for ICL.
 \vspace{-0.5em}}   
\label{tab:justeval}
\end{table*}


