% \vspace{5em}
% \begin{table*}[t]
\begin{table*}[!h]
% \vspace{-2.5em}
% \hspace{-0.6em}
\centering
\scalebox{0.85}{ 
 
\begin{tabular}{@{}lcccccccc@{}} % Changed from || to |
% \toprule
 \textbf{Models + ICL Methods} &  {\textbf{Helpful}} &  {\textbf{Factual}} &   {\textbf{Deep}} &   {\textbf{Engaging}} &   {\textbf{Clear}} &   {\textbf{Safe}} & {\textbf{Average}} &   {\textbf{Length}} \\
  % \textbf{Models + Alignment Methods} &  {\small \hspace{-1em}\faInfoCircle\textbf{Helpful} (\%)} &  {\small \hspace{-1em}\faIndent\textbf{Factual} (\%)} &   {\small \hspace{-1em}{\faCheckSquare[regular]}\textbf{Deep} (\%)} &   {\small \hspace{-1em}\faCommentMedical\textbf{Engaging} (\%)} &   {\small \hspace{-1em}\faLaughBeam[regular]\textbf{Clear} (\%)} &   {\small \hspace{-1em}\faShieldVirus\textbf{Safe} (\%)} &    {\small \textbf{Length}} \\
%  \midrule
% {\small \faToggleOn} Vicuna-7b (SFT)        &          \textbf{4.43} &      \textbf{4.85} &         \textbf{4.33} &    \textbf{4.04} &         4.51 &     4.60 &  {4.46} &    184.8 \\
% {\small \faToggleOff} Llama2-7b (Zero-shot)           &          3.05 &      3.83 &         3.14 &    2.69 &         3.09 &     1.57 &   162.4 \\

\midrule

% Llama2-7b + \textbf{Zero-shot}                      &   2.94            &  2.79             &  2.57             & 3.66          & 3.65          & 4.74              &  3.39                 & 211.99  \\
% Llama2-7b + \textbf{Vanilla ICL}                    &   3.21            &  3.26             &  2.85             & 4.00          & 3.96          & 4.75              &  3.67                 & 224.52  \\
% Llama2-7b + \textbf{Retrieval ICL}                  &   3.27            &  3.19             &  3.17             & 4.04          & 3.87          & 4.75              &  3.71                 & 229.17  \\
% Llama2-7b + \textbf{TopK + ConE}                    &   3.44            &  3.45             &  3.20             & 4.02              & 4.16          & 4.80          &  3.84                 & 226.11  \\

Llama2-7b + \iconminiurial \textbf{\methodname{}}                  &    3.82              &  3.88          &   3.52             & 4.26             & 4.45          & \textbf{4.89}    &  4.14              & 238.67  \\

Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$              &    \textbf{3.98}    &  3.84           &   \textbf{3.68}    & \textbf{4.39}  & \textbf{4.49}  & 4.87               &  \textbf{4.21}    & 263.62  \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$        &    3.87           &  3.89             &   3.55            & 4.26              & 4.45          & 4.87              &  4.15             & \textbf{265.15}  \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$        &    3.84           &  \textbf{3.92}    &   3.50            & 4.17              & 4.45          & 4.88              &  4.12             & 243.00  \\


\midrule \midrule
% Mistral-7b-instruct (SFT)   &          4.36 &      4.87 &         4.29 &    3.89 &         4.47 &     4.75 &  4.44 &    155.4 \\
% Mistral-7b (\methodname{})       &          4.57 &      4.89 &         4.50 &    4.18 &         4.74 &     4.92 &  4.63 &    186.3 \\
% {\small \faToggleOn} Mistral-7b-instruct (SFT)   &          4.36 &      4.87 &         4.29 &    3.89 &         4.47 &     4.75 &      {155.4} \\
Mistral-7b + \iconminiurial \textbf{\methodname{}}                 &    4.34           &      4.35             &         3.81      &    4.47       &         4.72      &     4.94 & 4.44           &   196.67 \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$             &    \textbf{4.59}  &      4.42             &   \textbf{4.29}   & \textbf{4.69} &  \textbf{4.83}    &     4.94 & \textbf{4.63}  &   276.79 \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$       &    4.57           &      \textbf{4.44}    &         4.14      &    4.63       &  \textbf{4.83}    &     4.94 & 4.59           &   \textbf{277.26} \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$       &    4.51           &      4.40             &         4.07      &    4.56       &         4.81      &     4.94 & 4.55           &   251.42 \\

\midrule \midrule
Olmo-7b + \iconminiurial \textbf{\methodname{}}                    &   3.29            &  3.54             &    3.05           &    3.82           &     4.08          &  \textbf{4.80}   & 3.76    &   202.94 \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$                &   3.36            &  3.52             &    \textbf{3.11}  &    \textbf{3.97}  &     \textbf{4.16} &  4.79             & \textbf{3.82}  &   \textbf{208.57} \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$          &   \textbf{3.40}   &  3.58             &    3.05           &    3.87           &     4.15          &  4.79             & 3.81    &   198.65 \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$          &   3.35            &  \textbf{3.63}    &    3.05           &    3.83           &     4.15          &  4.79             & 3.80    &   191.68 \\

% \midrule \midrule
% {\small \faToggleOn} \texttt{gpt-3.5-turbo-0301}        &          4.81 &      4.98 &         4.83 &    4.33 &         4.58 &     4.94 &    154.0 \\
% {\small \faToggleOn} \texttt{gpt-4-0314}           &          \textbf{4.90} &      \textbf{4.99} &         4.90 &    \textbf{4.57} &         \textbf{4.62} &     4.74 &  \textbf{226.4} \\
% {\small \faToggleOn} \texttt{gpt-4-0613}           &          4.86 &      \textbf{4.99} &         \textbf{4.90} &    4.49 &         4.61 &     \textbf{4.97} &  186.1 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Multi-aspect scoring evaluation of ICL methods on \alpaca{}.}} 
% The icon {\small \faToggleOn} indicates the models are \textit{tuned} for alignment via SFT or RLHF, while {\small \faToggleOff} means the models are \textit{untuned}. 
% Our method \methodname{} uses 1k static prefix tokens (system prompt + 3 examples) for ICL.
 \vspace{-0.5em}   
\label{tab:alpaca}
\end{table*}


