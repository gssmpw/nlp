% \vspace{5em}
% \begin{table*}[t]
% \begin{table}[!h]
\begin{table}[t]
% \vspace{-2.5em}
% \hspace{-0.6em}
\centering
\scalebox{0.82}{ 
 
\begin{tabular}{@{}lccc@{}} % Changed from || to |
% \toprule
\textbf{Models + ICL Methods} &  {\small \textbf{Turn 1}} &  {\small \textbf{Turn 2}} &   {\small \textbf{Overall}} \\
 \midrule
Llama2-7b + \iconminiurial \textbf{\methodname{}}              &   5.49            &  \textbf{3.91}            &  4.70             \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$          &   \textbf{6.01}   &  3.84                     &  \textbf{4.93}    \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$    &   5.54            &  3.80                     &  4.67             \\
Llama2-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$    &   5.58            &  \textbf{3.91}            &  4.74             \\

\midrule
% Mistral-7b-instruct (SFT)   &          4.36 &      4.87 &         4.29 &    3.89 &         4.47 &     4.75 &  4.44 &    155.4 \\
% Mistral-7b (\methodname{})       &          4.57 &      4.89 &         4.50 &    4.18 &         4.74 &     4.92 &  4.63 &    186.3 \\
Mistral-7b + \iconminiurial \textbf{\methodname{}}             &   7.49            &  5.44             &  6.46          \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$         &   7.26            &  \textbf{6.22}    &  \textbf{6.74}   \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$   &   7.10            &  5.76             &  6.43           \\
Mistral-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$   &   \textbf{7.53}   &  5.51             &  6.52           \\

\midrule 
% Llama2-70b-chat (RLHF)   &      4.50 &      4.92 &         4.54 &    4.28 &         4.75 &     5.00 &  4.67 &    257.9 \\
% Llama2-70b (zero-shot prompt)         &          3.70 &      4.31 &         3.78 &    3.19 &         3.50 &     1.50 &  3.33 &    166.8 \\
% Llama2-70b (\methodname{}-1shot) &          4.60 &      4.93 &         4.54 &    4.09 &         4.67 &     4.88 &  4.62 &    155.3 \\
% Llama2-70b (\methodname{})       &          4.72 &      4.95 &         4.65 &    4.30 &         4.85 &     4.96 &  4.74 &    171.4 \\
Olmo-7b + \iconminiurial \textbf{\methodname{}}                &   4.54          &  2.49           &  3.53           \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{f}}$            &   \textbf{5.13} &  \textbf{2.56}  &  \textbf{3.85}   \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_uni}}$      &   4.56          &  2.19           &  3.38           \\
Olmo-7b + \iconminiride $\textbf{RIDE}_{\text{fs\_hyb}}$      &   4.79          &  2.42           &  3.61           \\

% \midrule \midrule
% {\small \faToggleOn} \texttt{gpt-3.5-turbo-0301}        &          4.81 &      4.98 &         4.83 &    4.33 &         4.58 &     4.94 &  4.75 &    154.0 \\
% {\small \faToggleOn} \texttt{gpt-4-0314}           &          \textbf{4.90} &      \textbf{4.99} &         4.90 &    \textbf{4.57} &         \textbf{4.62} &     4.74 &  {4.79} &    \textbf{226.4} \\
% {\small \faToggleOn} \texttt{gpt-4-0613}           &          4.86 &      \textbf{4.99} &         \textbf{4.90} &    4.49 &         4.61 &     \textbf{4.97} &  \textbf{4.80} &    186.1 \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Overall evaluation of ICL methods on \mtbench.} (Scores are on a scale of 1-10.) }
% The icon {\small \faToggleOn} indicates the models are \textit{tuned} for alignment via SFT or RLHF, while {\small \faToggleOff} means the models are \textit{untuned}. 
% Our method \methodname{} uses 1k static prefix tokens (system prompt + 3 examples) for ICL.
 \vspace{-1em}   
\label{tab:mtbench_overall}
\end{table}


