\quad Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. 
% It bridges the gap between raw model capabilities and nuanced task requirements, such as factuality and safety. 
% Current alignment approaches, like instruction-following through supervised fine-tuning (SFT) and preference optimization (PO), require high-quality data and significant resources. 
% This paper proposes a \textit{low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment}.
Current alignment approaches, including supervised fine-tuning (SFT) and preference optimization (PO), require high-quality annotations and significant training resources. 
This paper proposes a \textit{low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment}.

\quad By comparing token generation probabilities of unaligned models with those of their aligned counterparts, we identified \textit{polarity tokens}—a type of special token that guides the response trajectory and affects the model’s alignment performance.
%
% Leveraging the autoregressive nature of LLMs, we observed that aligned models adjust the probability distribution of early \textit{polarity tokens} during decoding, influencing their response trajectory. 
%
% Among polarity tokens, \textbf{malicious} tokens induce LLMs to positively respond to toxic queries, whereas \textbf{benign} tokens encourage constructive output.
Based on this, we designed heuristic rules to select ICL demonstration examples that effectively influence polarity token distributions.
%
%\quad 
To further improve the quality of the selected ICL demonstrations, we model \textit{content}, \textit{style}, and \textit{alignment} as a \textit{causal structure}, and employ Average Treatment Effect (ATE) to quantitatively study the impact of the style factor. 
We found that after restyling the ICL demonstrations, they were more effective at eliciting the LLM’s alignment capabilities.

% Furthermore, the style and content of ICL demonstrations critically impact few-shot learning. 
% Rewriting examples in a unified, structured style improved LLM accuracy and helpfulness, while specific content encouraged refusal of malicious prompts, enhancing safety.

\quad We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Our experiments show that rewritten examples boost alignment, safety, and reasoning across various tasks.
Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the \alpaca{} task (from 4.50 $\to$ 4.60), a 0.19 enhancement on the \dataname{} benchmark (from 4.44 $\to$ 4.63), and a maximum improvement of 0.32 (from 3.53 $\to$ 3.85) on the \mtbench{} dataset.
These findings underscore the need for deeper analysis and theoretical understanding of alignment for advancing future LLM research.