% todo: can other baselines be improved by using our plug-in RIDE? how to prove it? 
\subsection{Dataset, LLMs, and baseline methods}
\paragraph{Dataset.}
We use \alpaca{} (a benchmark designed to assess the performance of language models on natural language understanding, generation, and reasoning tasks)~\citep{alpaca_eval}, \dataname{} (a dataset designed to assess the safety and reasoning capabilities of LLMs)~\citep{DBLP:conf/iclr/LinRLDSCB024}, and \mtbench{} (a multi-turn dialogue dataset to evaluate various capabilities of LLMs, such as reasoning and coding)~\citep{zheng2023judging} as benchmarks.

In Sections~\ref{section2} and~\ref{section3}, we extracted a 50-sample subset from \dataname{} as the \textit{validation} dataset to facilitate our analysis and research on stylistic impact.
The remaining data from \dataname{} is designated as the \textit{test} dataset, which is used for benchmarking against baseline methods.
Notably, the \dataname{} \textit{validation} and \textit{test} datasets used in this study are \textbf{orthogonal}, ensuring that \textbf{no information leakage} occurs during evaluation.

\paragraph{LLMs.}
We use three models as the base models: Llama-2-7b-hf~\citep{touvron2023llama}, Mistral-7b-v0.1~\citep{jiang2023mistral}, and OLMo-7B~\citep{groeneveld2024olmo}. 
It is important to note that these models have not undergone alignment tuning, resulting in sub-optimal alignment capabilities.

\paragraph{Baseline methods.}
We selected different baseline methods for comparison. 
The most relevant to our work is \textbf{\methodname{}}~\citep{DBLP:conf/iclr/LinRLDSCB024}, achieving state-of-the-art (SOTA) performance across multiple datasets using the ICL approach. 
Additionally, we compared against the following baselines: (1) \textbf{Zero-shot}: consisting only of the URIAL system instruction part. (2) \textbf{Vanilla ICL}: an ICL example set composed of the top-2 examples from $\{S_\text{cand\_f}\}$ and the top-1 example from $\{S_\text{cand\_s}\}$. (3) \textbf{Retrieval ICL}~\citep{liu2022makes}: Among the examples in $\{S_\text{cand}\}$, the neighbors that are the most similar to the given test query are retrieved as the corresponding in-context examples. (4) \textbf{TopK + ConE}~\citep{peng2024revisiting}: a tuning-free method that retrieves the best three examples that excel in reducing the conditional entropy of the test input as the ICL demonstrations.
In this work, we consistently use GPT-4o as the LLM-as-a-judge to evaluate and score the responses generated by the LLMs.
Through comparing these baseline methods with our proposed ICL demonstration set, i.e., $\textbf{RIDE}_{\text{f}}$, $\textbf{RIDE}_{\text{fs\_uni}}$, and $\textbf{RIDE}_{\text{fs\_hyb}}$, we conducted a detailed experimental analysis.

\subsection{Q1: Does \textbf{RIDE} improve the LLMâ€™s alignment performance?}
% \subsection{Empirical results on \dataname{}}
\label{ssec:exp_justeval}
\input{latex/table/table_justeval}

\dataname{} aims to assess the trade-off between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety} in LLM alignment, ensuring that the model can provide informative responses while refusing malicious queries.

\paragraph{Results.} Table~\ref{tab:justeval} presents the scores of each method on \dataname{}. 
From the table, we can summarize the following conclusions.

\paragraph{$\textbf{RIDE}_{\text{fs\_hyb}}$ achieves the best overall performance.} (i) Among the three proposed ICL sets, $\textbf{RIDE}_{\text{fs\_hyb}}$ performs the best, followed by $\textbf{RIDE}_{\text{fs\_uni}}$, while $\textbf{RIDE}_{\text{f}}$ ranks lowest.
(ii) $\textbf{RIDE}_{\text{fs\_hyb}}$ maintains a strong \textbf{\color{myblue} factuality} performance while significantly enhancing \textbf{\color{myred} safety}, thanks to the ``\textbf{refusal}'' style safety example.
(iii) $\textbf{RIDE}_{\text{f}}$, consisting solely of \textbf{\color{myblue} factuality} examples, excels in \textbf{\color{myblue} factuality} but lacks \textbf{\color{myred} safety} training, resulting in a significantly lower ``Safe'' score.

\paragraph{\textbf{RIDE} outperforms \textbf{URIAL} in most cases.} (i) $\textbf{RIDE}_{\text{fs\_hyb}}$ outperforms \methodname{} in two out of three models, demonstrating its superior alignment performance.
(ii) Due to OLMo-7B's input length limitation, some ICL content had to be truncated, slightly reducing ``Helpful", ``Factual", and ``Deep" scores. However, $\textbf{RIDE}_{\text{fs\_hyb}}$ remains competitive with \methodname{}, achieving nearly identical ``Safe" scores.

\paragraph{Baseline methods exhibit a significant performance gap.} (i) As shown in the first block of Llama2-7b, the baseline methods perform notably worse than our \textbf{RIDE} and \methodname{} ICL sets. (ii) \textbf{TopK + ConE}, the strongest baseline, selects ICL demos based on their impact during inference but still lags behind \textbf{RIDE}.

\paragraph{\textbf{RIDE} demonstrates the effectiveness of hierarchical traversal.} (i) Simply combining the best-performing ICL examples from $\{S_\text{cand\_f}\}$ and $\{S_\text{cand\_s}\}$ does not yield an optimal ICL demo set.
The performance gap between \textbf{Vanilla ICL} and \textbf{RIDE} highlights the effectiveness of the hierarchical traversal approach in selecting the best ICL demonstrations.

It is worth noting that, in this benchmark, we exclusively utilized Llama-2-7b-hf to compare all baseline methods and assess their performance, aiming to minimize token consumption when invoking LLM-as-a-judge.
For details on the experimental design, result analysis, and discussion of \textbf{Q1}, please refer to the Appendix~\ref{append:justeval_discuss}.

\subsection{Q2: Does \textbf{RIDE} elicit LLMs to generate high-quality and informative responses?}
\label{ssec:exp_alpaca}
\input{latex/table/table_alpaca_overall}

% \begin{figure*}[t] 
%     \centering
%     \subfloat[Llama2-7b]{\includegraphics[width=0.32\textwidth]{latex/figure/radar_llama2.png}} \hspace{0.005\textwidth}
%     \subfloat[Mistral-7b]{\includegraphics[width=0.32\textwidth]{latex/figure/radar_mistral.png}} \hspace{0.005\textwidth}
%     \subfloat[Olmo-7b]{\includegraphics[width=0.32\textwidth]{latex/figure/radar_olmo.png}} \\[0.05cm] 
%     \caption{In \alpaca{}, comparisons are made of LLM alignment performance across various aspects using different backbone LLMs.}
%     \label{fig:eval_alpaca}
% \end{figure*}

To assess whether the distinctive styles in \textbf{RIDE} can enhance high-quality, well-structured, and information-rich responses, we conduct experiments using \alpaca{}, a dataset that primarily evaluates \textbf{\color{myblue} factuality} rather than \textbf{\color{myred} safety}. Unlike \dataname{}, \alpaca{} focuses solely on instruction-following capabilities without considering potential harm\footnote{\url{https://github.com/tatsu-lab/alpaca_eval}}, making it suitable for analyzing how ICL demonstrations influence factuality performance in LLMs.

In Table~\ref{tab:alpaca_overall}, we compute the average of ``helpful'', ``factual'', ``deep'', ``engaging'', and ``clear'' metrics to assess the overall \textbf{\color{myblue} factuality} capability of the LLM.
Therefore, we have the following findings.

\paragraph{$\textbf{RIDE}_{\text{f}}$ achieves the best factuality performance.} (i) Among the \textbf{RIDE} series, $\textbf{RIDE}_{\text{f}}$ achieves the highest \textbf{\color{myblue} factuality} (``\textbf{Avg.}''), followed by $\textbf{RIDE}_{\text{fs\_uni}}$, then $\textbf{RIDE}_{\text{fs\_hyb}}$. (ii) This result is opposite to that in Table~\ref{tab:justeval}, as \alpaca{} focuses solely on \textbf{\color{myblue} factuality}, making the factuality-only set $\textbf{RIDE}_{\text{f}}$ the most effective.

\paragraph{\textbf{RIDE} outperforms \textbf{URIAL} in factuality across all models.} The restyled ICL examples in $\textbf{RIDE}_{\text{f}}$ help the LLM quickly learn an effective output pattern, leading to higher \textbf{\color{myblue} factuality} performance than URIAL.

\paragraph{\textbf{RIDE} enhances response quality without increasing length.} (i) Despite previous research suggesting that longer responses tend to receive higher LLM-as-a-judge ratings~\citep{DBLP:journals/corr/abs-2404-04475}, $\textbf{RIDE}_{\text{f}}$ outperforms other methods even with a shorter response ``Len.'' in both Llama2 and Mistral settings.
(ii) In the Olmo setting, \textbf{URIAL} produces longer responses than $\textbf{RIDE}_{\text{fs\_uni}}$ and $\textbf{RIDE}_{\text{fs\_hyb}}$ but still performs the worst. This confirms that \textbf{RIDE}'s superior factuality ratings stem from improved content quality, not response length.

For the detailed scores of each individual metric, as well as an in-depth discussion of different "model + ICL method" settings used in \alpaca{}, please refer to Appendix~\ref{append:alpaca_discuss} and~\ref{append:alpaca_all_discuss}.

\subsection{Q3: Does \textbf{RIDE} enhance LLMs' ability to handle complex tasks?}
\label{ssec:exp_mtbench}

\input{latex/table/table_mt_overall}

\mtbench{} assesses LLM capability in handling complex tasks by requiring the integration of logical reasoning, numerical computation, coding, and other advanced skills, making it a suitable benchmark for measuring LLM proficiency in complex problem-solving.
From Table~\ref{tab:mtbench_overall}, we can draw the following findings (further discussion can be found in Appendix~\ref{appendix:mtbench_dicsuss}).

% Here we present only the experimental conclusions drawn from Table~\ref{tab:mtbench_overall}. 
% For details on the experimental design, result analysis, and discussion of \textbf{Q3}, please refer to the Appendix~\ref{appendix:mtbench_dicsuss}.

\textbf{\textbf{RIDE} outperforms \textbf{URIAL} across all settings.} (i) $\textbf{RIDE}_{\text{f}}$ achieves the best overall performance, followed by $\textbf{RIDE}_{\text{fs\_hyb}}$, then $\textbf{RIDE}_{\text{fs\_uni}}$.
(ii) The structured and logically coherent responses from the ``Combined'' (mostly because of ``Three-part'') style in $\textbf{RIDE}_{\text{f}}$ enhance LLM \textbf{\color{myblue} factuality} and \textbf{reasoning} capabilities, making it the top-performing approach.
(iii) The inclusion of \textbf{\color{myred} safety}-focused examples in $\textbf{RIDE}_{\text{fs\_hyb}}$ and $\textbf{RIDE}_{\text{fs\_uni}}$ slightly weakens their ability to handle complex tasks.

\textbf{$\textbf{RIDE}_{\text{fs\_hyb}}$ outperforms $\textbf{RIDE}_{\text{fs\_uni}}$.} The ``Refusal''-style example in $\textbf{RIDE}_{\text{fs\_hyb}}$ follows a structured reasoning process (refusal $\rightarrow$ justification $\rightarrow$ guidance), aligning well with the logical reasoning required by \mtbench{}, which contributes to its superior performance.

\textbf{\textbf{RIDE} Improves Multi-Turn Dialogue Performance.} In two out of three models (Mistral-7B and Olmo-7B), \textbf{RIDE} outperforms \textbf{URIAL} in Turn 2, demonstrating its effectiveness in multi-turn dialogue tasks despite being designed for single-turn scenarios.

\textbf{\textbf{RIDE} Enhances Logical Reasoning and Complex Computation.} As further evidenced in Table~\ref{tab:mtbench_tf}, we evaluated the accuracy of different methods in answering \textit{objective} questions from \mtbench{}. Our findings indicate that \textbf{RIDE} achieves higher accuracy in responding to \textit{objective} questions compared to the baseline methods.Detailed performance results are available in Appendix~\ref{appendix:tf_dicsuss}.

\subsection{Q4: Can base LLM outperform its aligned counterpart by employing \textbf{RIDE}?}
\label{ssec:exp_win_finetune}

\paragraph{Results.}
Our findings conclusively show that \textbf{yes}, a base LLM can \textbf{outperform} its aligned counterpart! As detailed in Table~\ref{tab:win_finetune} in Appendix~\ref{append:win_finetune}, when the base model Mistral-7B-v0.1 utilizes \textbf{RIDE} as its ICL demonstrations, it achieves superior alignment performance compared to Mistral-7B-Instruct-v0.1 across all three datasets. 
% While this could potentially be attributed to insufficient alignment fine-tuning in Mistral-7B-Instruct-v0.1, 
We argue that for sufficiently capable base models, \textbf{RIDE} can effectively elicit their inherent alignment potential. Notably, our approach offers significant practical advantages: it is tuning-free, plug-and-play, and requires minimal training and deployment costs.
We leave further discussion about \textbf{Q4} in Appendix~\ref{append:win_finetune}.

% \subsection{Takeaways of the Empirical Study}
% From the findings in Section~\ref{ssec:exp_justeval}, \ref{ssec:exp_justeval} and \ref{ssec:exp_mtbench}, in conclusion, we find that different benchmarks have different focal points when evaluating the capabilities of LLMs. 
% In this context, both the \textbf{content} (whether the ICL example set is oriented toward safety or factuality) and the \textbf{style} (whether the restyling is more structured or focused on refusal) interact with each other to jointly influence the performance of ICL. 
% Thus, through our experiments, we validate the causal relationship among \textit{content}, \textit{style}, and \textit{alignment}. 
% Furthermore, we observe that the \textit{safety} and \textit{factuality} capabilities of LLMs are inherently conflicting, requiring us to find a trade-off to achieve the best overall performance. 
% This finding is also consistent with our analysis of \textit{polarity tokens}. 
% Therefore, the experimental results validate the two key concepts proposed in this paper: the existence of \textit{polarity tokens} with different orientations toward safety and factuality, and the \textit{causal structure} within alignment.