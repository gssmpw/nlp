% Alignment tuning serves as a crucial mechanism for narrowing the gap between the inherent capabilities of raw models and the nuanced requirements of various tasks, including delivering accurate information, ensuring user safety, and appropriately handling sensitive topics~\citep{shneiderman2020bridging,shen2023large,wang2023aligning,DBLP:conf/iclr/Qi0XC0M024}.

% Currently, the instruction-following paradigm~\citep{ouyang2022training,sun2023aligning,DBLP:conf/iclr/DaiPSJXL0024,rafailov2024direct,zhou2024lima,wu2024self}, which integrates supervised fine-tuning (SFT) and preference optimization, is the predominant approach for alignment tuning. However, this paradigm necessitates high-quality annotated data and incurs substantial computational costs. Unlike instruction-following approaches, our proposed method is entirely tuning-free and plug-and-play, eliminating the need for additional training while remaining computationally efficient.

% Meanwhile, a growing body of research suggests that alignment tuning alters the token generation probabilities of base LLMs~\citep{DBLP:conf/iclr/LinRLDSCB024, DBLP:journals/corr/abs-2406-05946, DBLP:journals/corr/abs-2407-09121, huang2024safealigner}.

% Most relevant to our work, URIAL~\citep{DBLP:conf/iclr/LinRLDSCB024} introduced an ICL demo set containing three manually crafted exemplars and empirically demonstrated that these manually designed ICL exemplars effectively enhance LLM alignment. However, their approach remains somewhat of a black box, as it does not explain why these particular handcrafted ICL demos are effective.
% In contrast, our work explicitly analyzes the key factors influencing LLM alignment and constructs our ICL demo set based on these identified principles.

Alignment tuning helps bridge the gap between raw model capabilities and task-specific requirements~\citep{shneiderman2020bridging,shen2023large,wang2023aligning,DBLP:conf/iclr/Qi0XC0M024}. The instruction-following paradigm\citep{ouyang2022training,sun2023aligning,DBLP:conf/iclr/DaiPSJXL0024,rafailov2024direct,zhou2024lima,wu2024self} requires high-quality annotated data and significant computational resources. In contrast, our tuning-free, plug-and-play approach eliminates the need for additional training while maintaining efficiency.

Research indicates that alignment tuning alters token generation probabilities in LLMs~\citep{DBLP:conf/iclr/LinRLDSCB024, DBLP:journals/corr/abs-2406-05946, DBLP:journals/corr/abs-2407-09121, huang2024safealigner}. Most relevant to our work, URIAL~\citep{DBLP:conf/iclr/LinRLDSCB024} proposed a manually crafted ICL demo set to enhance alignment but did not provide insights into why these demos were effective. Unlike URIAL, our work transparently analyzes key alignment factors and constructs ICL demo sets based on identified principles.