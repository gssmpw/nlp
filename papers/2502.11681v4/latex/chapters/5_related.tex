\paragraph{Impact of Alignment Tuning on Token Generation.} As autoregressive generative models, LLMs generate tokens by treating the input query and previously generated tokens as the context for predicting the next token.
They use this context to calculate the probability distribution over the output vocabulary. 
Following the principle that tokens with higher probabilities are more likely to be selected, the next token is randomly chosen from the output vocabulary based on these probabilities, introducing a degree of randomness to the generated content.

Many researchers believe that the process of alignment tuning modifies the token generation probabilities of base LLMs. \citet{DBLP:conf/iclr/LinRLDSCB024} found that alignment tuning intervenes in the generation probabilities of specific tokens, thereby guiding the decoder to produce safe and reliable content. \citet{DBLP:journals/corr/abs-2406-05946} argued that alignment tuning only achieves shallow safety alignment—it affects only the shallow positions (the first few tokens) in the generation process and fails to prevent jailbreak attempts beyond these initial positions. Similarly, \citet{DBLP:journals/corr/abs-2407-09121} identified a refusal position bias within the safety-tuning data, which influences the alignment performance of LLMs.

Most relevant to our work, SafeAligner~\citep{huang2024safealigner} proposed the concept of beneficial and harmful tokens that affect model alignment performance. However, these tokens were identified based on the generation probability gap between the sentinel model and the intruder model, without explicitly defining which tokens are beneficial and which are harmful. Therefore, this paper introduces the concept of polarity tokens—a specific set of tokens that can influence the generation of aligned content—and analyzes the relationship between different polarity tokens and alignment performance.

\paragraph{ICL demonstration selection methods.} Numerous methods for selecting the most representative and highest-quality ICL demonstration examples have been proposed by researchers. \citet{liu2022makes} selects examples that are closest to the input test sample in the embedding space as a good choice for ICL. \citet{DBLP:conf/emnlp/HuaQH24} ranks ICL demonstrations based on the average reward of the task directed by each example, using heuristic rules, and employs an early pruning hierarchical traversal approach to combine demonstrations. PICLe~\citep{DBLP:conf/icml/ChoiL24} formulates the process of selecting ICL demonstrations as a Bayesian inference problem to identify high-quality demonstrations. 

TopK + ConE~\citep{peng2024revisiting} proposes a training-free method that selects examples that minimize the inference model’s uncertainty as ICL demonstrations. Similarly, \citet{wu2023self} searches for demonstrations capable of losslessly compressing test labels, while \citet{iter2023context} identify ICL demonstrations based on the cross-entropy difference of test labels.

However, none of these methods incorporate the essence of alignment tuning—changes in the token generation probability distribution—into the selection of ICL examples. In contrast, our work integrates alignment tuning with ICL selection, effectively addressing this gap.