\section{Background settings in our work}
\label{append:background}
In this paper, we use the terms “unaligned LLMs” and “base LLMs” interchangeably to refer to LLMs that have not undergone alignment processes, though they are not inherently malicious. In contrast, we refer to LLMs that have been fine-tuned with instructional data to promote ethical and beneficial behavior as “aligned LLMs”.
%
We define an unaligned LLM as $f(\mathbf{x}; \theta)$, where $\mathbf{x}$ is the input query and $\theta$ represents the model’s parameters responsible for generating output tokens. 
%
The process of ``\textit{\textbf{alignment tuning}}'' involves adjusting the parameters $\theta$ of a base LLM to produce more controlled and regulated responses. 
Consequently, we represent the aligned LLM as $g(\mathbf{x}; \beta)$, which is better aligned with human values and preferences.
%
This process generally involves two steps: supervised fine-tuning (SFT) on instructional data and reinforcement learning from human feedback (RLHF). 
%
In the SFT phase, the base LLM is refined using instruction-answer pairs, known as instruction tuning. In the RLHF phase, a reward model is applied to further enhance the fine-tuned model, improving its alignment with human expectations of helpfulness and safety.