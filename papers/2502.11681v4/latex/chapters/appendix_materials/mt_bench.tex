\section{ICL methods on \mtbench{}}
\label{append:mt_bench_all}
\input{latex/table/table_mtbench}

\subsection{RIDE enhance LLMs’ ability to handle complex tasks - A Deep Discussion}
\label{appendix:mtbench_dicsuss}

\paragraph{Settings.} \mtbench{} assesses LLM capability in handling complex tasks by requiring the integration of logical reasoning, numerical computation, coding, and other advanced skills. Unlike \alpaca{} and \dataname{}, which focus on general LLM alignment, \mtbench{} explicitly evaluates an LLM’s ability to perform multi-faceted and cognitively demanding tasks, making it a suitable benchmark for measuring LLM proficiency in complex problem-solving.

Table~\ref{tab:mtbench_overall} presents the overall performance of ICL demo examples on different models when handling the \mtbench{} dataset. It is important to note that \mtbench{} is a multi-turn dialogue dataset. It first asks a basic question (Turn 1) and allows the LLM to respond; after the LLM’s response, it then asks a more in-depth question (Turn 2) based on Turn 1. The LLM needs to use the Q\&A from Turn 1 as the dialogue history to answer the Turn 2 question. Therefore, in Table~\ref{tab:mtbench_overall}, performance is divided into Turn 1 and Turn 2, with ‘overall’ representing the LLM’s overall performance across both turns. Meanwhile Table~\ref{tab:mtbench} records the performance of different ICL examples applied to different models on various tasks within the \mtbench{} dataset.

\paragraph{Results.} As shown in Table~\ref{tab:mtbench_overall}, we have the following findings.

\begin{itemize}

\item First, $\textbf{RIDE}$ is better than $\textbf{URIAL}$ under all settings. 
Among the $\textbf{RIDE}$ series, $\textbf{RIDE}_{\text{f}}$ performs best overall, followed by $\textbf{RIDE}_{\text{fs\_hyb}}$, and $\textbf{RIDE}_{\text{fs\_uni}}$ performs the worst. 
Since \mtbench{} assesses whether LLMs can handle complex tasks, the ICL demonstrations provided in $\textbf{RIDE}_{\text{f}}$ effectively enhance the LLM’s \textbf{\color{myblue} factuality} capability.
The ICL examples restyled with the ``Combined'' style (especially the ``Three-part'' style) give the responses a clear structure and rigorous logic, which, to some extent, improves the LLM’s reasoning ability, making $\textbf{RIDE}_{\text{f}}$ perform best in this benchmark. 
The \textbf{\color{myred} safety} examples included in $\textbf{RIDE}_{\text{fs\_hyb}}$ and $\textbf{RIDE}_{\text{fs\_uni}}$ weaken this capability, leading to average performance.

\item Second, the fact that $\textbf{RIDE}_{\text{fs\_hyb}}$ outperforms $\textbf{RIDE}_{\text{fs\_uni}}$ is an interesting and surprising finding. 
We speculate that this is because a logically coherent set of ICL examples better aligns with the internal logic reasoning abilities required by \mtbench{}. 
The demonstration restyled with the ``Refusal'' style in $\textbf{RIDE}_{\text{fs\_hyb}}$ starts by refusing to answer a malicious example, then provides a reasonable justification, and finally offers guidelines. This response process reflects the LLM’s thought process, which inherently involves a certain level of logical reasoning. 
This logical reasoning might enhance the LLM’s reasoning capabilities, aligning with preference of \mtbench, thereby making $\textbf{RIDE}_{\text{fs\_hyb}}$ a better ICL demonstration set.

\item Third, in two of the three models (Mistral-7b and Olmo-7b), our method outperforms \methodname{} in ``Turn 2'' performance. 
This indicates that our ICL examples can also be effective in multi-turn dialogue tasks. 
Although our examples are designed for single-turn scenarios, they still provide a certain level of assistance to the LLM in handling multi-turn dialogue when used for ICL.

\end{itemize}

\subsection{RIDE enhance LLMs’ ability to handle complex tasks - From Objective Prospective}
\label{appendix:tf_dicsuss}
\input{latex/table/table_mt_bench_tf}

\paragraph{Settings.} From Table~\ref{tab:mtbench}, we can observe that $\textbf{RIDE}_{\text{fs\_hyb}}$ performs best for coding and extraction tasks, while $\textbf{RIDE}_{\text{f}}$ is most effective for math and reasoning tasks. 
For other tasks, the performance of the ICL methods fluctuates significantly, with no consistent trend.

To further analyze whether $\textbf{RIDE}$ enhances LLM performance in handling complex tasks, we manually selected a subset of objective questions from the \mtbench{} dataset. 
We define objective questions as those with definitive and verifiable answers, such as those requiring mathematical computation or numerical reasoning. 
Unlike subjective writing tasks, where answers can be open-ended, the correctness of objective questions can be clearly evaluated—an answer is either correct or incorrect. 
Therefore, this subset allows us to quantitatively assess the extent to which LLMs, guided by ICL demonstrations, can accurately answer questions and engage in logical reasoning.

Within this objective question subset, we employed $\textbf{RIDE}_{\text{f}}$ and $\textbf{URIAL}$ as ICL demonstrations to prompt the LLM in answering the questions. 
Subsequently, we used a powerful LLM-as-a-judge, which is Claude-3.5 Sonnet~\cite{anthropic2024claude3.5s}, to evaluate the correctness of the responses. 
The LLM provided assessments categorized as "True," "False," and "Uncertain," corresponding respectively to "the answer is correct", "the answer is incorrect", and "the correctness of the answer cannot be determined". 
We recorded the proportions of these three categories across the first round, the second round, and the combined two rounds for different methods. The proportion of "True" serves as a key indicator of a method’s ability to accurately answer questions, thus reflecting its effectiveness in enhancing LLM reasoning capabilities.

\paragraph{Results.} As shown in the Table~\ref{tab:mtbench_tf}, across all three models, $\textbf{RIDE}_{\text{f}}$ consistently achieves a higher accuracy rate than $\textbf{URIAL}$. This indicates that:

\begin{itemize}
    \item $\textbf{RIDE}_{\text{f}}$ is more effective in stimulating LLMs to engage in logical reasoning and complex computations, thereby improving performance on intricate tasks.
    \item $\textbf{RIDE}_{\text{f}}$ does not rely on prompting LLMs to generate longer responses merely to align with LLM evaluation biases. Instead, its structured three-part format and enumerated points inherently reinforce logical relationships within the answer, enabling LLMs to learn to produce coherent, logically progressive responses. This structured approach effectively enhances the LLM’s reasoning capabilities rather than artificially inflating performance through verbose outputs.
\end{itemize}



