\section{ICL methods on \alpaca{}}
\label{append:alpaca_all}

\subsection{A further discussion of ICL methods on \alpaca{}}
\label{append:alpaca_discuss}

\paragraph{Settings.} If we disregard the safety factor and focus solely on the quality of information output, we aim to investigate whether the distinctive styles of the demos in our \textbf{RIDE} series can effectively stimulate the LLM to produce high-quality, well-structured, and information-rich responses to user queries. To evaluate this, we conducted experiments using the \alpaca{} dataset.

Unlike \dataname{}, in \alpaca{}, the dataset places more emphasis on \textbf{\color{myblue} factuality}. 
One characteristic of the \alpaca{} dataset is the lack of \textbf{\color{myred} safety} evaluation, meaning that this benchmark only evaluates the instruction-following capabilities of LLMs rather than the potential harm they could cause\footnote{\url{https://github.com/tatsu-lab/alpaca_eval}}. Therefore, in this benchmark, we focus more on the \textbf{\color{myblue} factuality} capability elicited by the ICL example set in the LLM.

As discussed in Section~\ref{section2}, ``helpful'', ``factual'', ``deep'', ``engaging'', and ``clear'' correspond to the \textbf{\color{myblue} factuality}.
In Table~\ref{tab:alpaca_overall}, we compute the average of these five metrics to assess the overall \textbf{\color{myblue} factuality} capability of the LLM.

\paragraph{Results.} As shown in Table~\ref{tab:alpaca_overall}, we have the following findings.

\begin{itemize}

\item First, among the $\textbf{RIDE}$ series sets, $\textbf{RIDE}_{\text{f}}$ performs the best ``\textbf{Avg.}'', followed by $\textbf{RIDE}_{\text{fs\_uni}}$, and $\textbf{RIDE}_{\text{fs\_hyb}}$ performs the worst. 
This result is the opposite of what is shown in Table~\ref{tab:justeval}.
The reason for this reversal aligns with the analysis in Section~\ref{section3}, RQ1 and RQ2, which is primarily due to the impact of \textbf{style}.
Since most samples in \alpaca{} are related only to \textbf{\color{myblue} factuality}, the set composed entirely of factuality examples, $\textbf{RIDE}_{\text{f}}$, is most effective at eliciting the LLM’s \textbf{\color{myblue} factuality} capabilities.
The three examples in $\textbf{RIDE}_{\text{fs\_uni}}$ are all restyled using the ``combined'' style, which ensures consistency, but the inclusion of a \textbf{\color{myred} safety} demonstration slightly weakens its \textbf{\color{myblue} factuality} performance. 
On the other hand, $\textbf{RIDE}_{\text{fs\_hyb}}$, which has the strongest \textbf{\color{myred} safety} capability, performs the worst in \textbf{\color{myblue} factuality}.

\item Second, $\textbf{RIDE}_{\text{f}}$ outperformed \methodname{} across all models, indicating that the ICL examples we selected, after restyling, enable the LLM to quickly and effectively learn a specific output pattern, which then guides the LLM’s content generation, thereby enhancing its \textbf{\color{myblue} factuality} capabilities.

\item Third, as observed in the table, the highest ``Avg.'' score is achieved by $\textbf{RIDE}_{\text{f}}$, yet its ``Len.'' is not the longest.
Previous studies have shown that when using LLM-as-a-judge, the evaluating models tend to favor responses with longer outputs~\citep{DBLP:journals/corr/abs-2404-04475}. However, in both the Llama2 and Mistral settings, the average length of $\textbf{RIDE}_{\text{f}}$ is shorter than that of $\textbf{RIDE}_{\text{fs\_uni}}$, yet it still outperforms all other methods. This indicates that $\textbf{RIDE}_{\text{f}}$ does not rely on producing longer responses to align with LLM preferences but instead generates higher-quality, information-rich answers.
Furthermore, in the Olmo setting, although \textbf{URIAL} produces longer responses than $\textbf{RIDE}_{\text{fs\_uni}}$ and $\textbf{RIDE}_{\text{fs\_hyb}}$, its performance is the weakest. This further confirms that $\textbf{RIDE}$ does not achieve superior factuality ratings simply by generating longer responses, but rather by enhancing the quality and informativeness of the content.

\end{itemize}

\subsection{Multi-aspect scoring evaluation of ICL methods on \alpaca{}}
\label{append:alpaca_all_discuss}
\input{latex/table/table_alpaca}

\paragraph{Settings.} Table~\ref{tab:alpaca} presents the multi-aspect performance evaluation of different ICL methods applied to three LLMs (Llama2-7B, Mistral-7B, and Olmo-7B) on the \alpaca{} dataset. The evaluation metrics include ``Helpful'', ``Factual'', ``Deep'', ``Engaging'', ``Clear'', ``Safe''.
The ``Average" metric refers to the mean value of the six preceding metrics, while ``Length" represents the average response length generated by the LLM under a specific model + ICL method setting.

% \vspace{1em}

\paragraph{Results.} From Table~\ref{tab:alpaca}, we can observe that:

\begin{itemize}
    \item Overall Performance Trends: Across all three models (Llama2-7B, Mistral-7B, and Olmo-7B), \textbf{RIDE}-based ICL methods consistently outperform \textbf{URIAL} in terms of average scores.

    \item ``Helpful" and ``Deep" scores show notable improvements with \textbf{RIDE}, particularly in Mistral-7B and Llama2-7B settings.

    \item Longer responses do not always correlate with better performance. For Llama2-7B, $\textbf{RIDE}_{f}$ has a slightly shorter response length (263.62) than $\textbf{RIDE}_{\text{fs\_uri}}$ (265.15), yet achieves a higher average score (4.18 vs. 4.16). For Mistral-7B, $\textbf{RIDE}_{f}$ has a longer response (276.79) and achieves the best performance. Olmo-7B shows a decrease in performance despite longer responses. For example, $\textbf{RIDE}_{f}$ has a longer response length (208.57) but does not perform as well as $\textbf{RIDE}_{\text{fs\_uri}}$ (average 3.84 vs. 3.82). This suggests that \textbf{RIDE improves alignment through structured responses rather than artificially increasing output length}.

    \item Among the three \textbf{RIDE} variations, Mistral-7B + \textbf{RIDE} variants consistently achieve the best scores, with $\textbf{RIDE}_{f}$ obtaining the highest average (4.42). Also, Llama2-7B benefits significantly from \textbf{RIDE}, with $\textbf{RIDE}_{f}$ achieving the highest factuality score (3.98). Furthremore, Olmo-7B + \textbf{RIDE} still lags behind the other models but sees notable improvement in ``Deep" scores with $\textbf{RIDE}_{\text{fs\_uri}}$ (3.63).

    \item A comprehensive analysis of the ``Safe'' scores across all methods shows that they are largely consistent, further proving that \alpaca{} has little discriminative power for evaluating the safety capabilities of LLMs. Thus, $\textbf{RIDE}_{\text{fs\_hyb}}$, which exhibited excellent safety performance in \dataname{}, performs worse in this benchmark.

    \item Mistral-7B consistently achieves the highest scores across most aspects, followed by Llama2-7B, while Olmo-7B exhibits the lowest performance.

\end{itemize}
