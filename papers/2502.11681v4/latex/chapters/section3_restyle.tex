\input{latex/table/restyle}

%
In this section, we aim to address three research questions: (i) \textbf{\textit{How does explicitly rewriting an ICL demonstration example impact LLM alignment?}} (ii) \textbf{\textit{How can different styles of ICL exemplars be effectively combined?}} and (iii) \textbf{\textit{Can rewriting randomly selected ICL exemplars also improve LLM alignment?}}

\subsection*{RQ1: Rewriting ICL demonstration examples}
\label{ssec:rewrtie_icl}
As observed in Section~\ref{section2}, we identified four distinct ICL exemplar styles that effectively influence LLM alignment capabilities.
Naturally, this leads to the questions: \emph{If we explicitly modify an ICL exemplar to adopt a specific style, will the restyled demonstration impact LLM alignment?}
\emph{How does restyling QA pairs from \textbf{\color{myblue} factuality}-based (UltraChat) and \textbf{\color{myred} safety}-focused (SORRY-Bench) datasets impact LLM alignment?}

% Additionally, as previously mentioned, we use UltraChat and SORRY-Bench as the candidate pool for ICL exemplars. UltraChat primarily consists of factual Q\&A pairs, while SORRY-Bench focuses on rejecting malicious queries. Clearly, UltraChat aligns more with the \textbf{\color{myblue} factuality} aspect of LLM alignment, whereas SORRY-Bench emphasizes \textbf{\color{myred} safety}. Given this distinction, an interesting question arises: \emph{If we restyle QA pairs from these different datasets, what effect will this have on alignment outcomes?}

\paragraph{Restyling Methodology.}
To systematically modify the writing style of QA pairs, we design a structured prompting approach consisting of three components: 1) Task instruction: A directive informing the LLM to explicitly rewrite the answer in a specific style; 2) Example demonstration: A concrete example illustrating how the modification should be performed. 3) Target QA pair: The QA pair to be rewritten.
We feed this prompt into an LLM, which then generates a restyled QA pair, ready to be used as an ICL exemplar.

% For these modifications, we leverage a strong LLM\footnote{We used GPT-4o to restyle the answers in the ICL demos.} to ensure high-quality restyling.
% Based on the findings in Section~\ref{section2}, we modify the style of the answer part in the following ways: (1) \textbf{three-part} (presenting the answer in a three-part structure: first, introducing the answer in one sentence; second, itemizing the answer using bullet points; and third, summarizing the answer in one sentence), (2) \textbf{lengthy} (enriching the answer details and increasing its length without altering the original meaning), (3) \textbf{human} (using a conversational tone or answering from a first-person perspective), (4) \textbf{combined} (use three-part, lengthy and human three styles to rewrite the ICL example simultaneously), (5) \textbf{refusal} (for safety-related ICL examples, first refuse to answer, then provide a reason, and finally offer advice or guidelines), and (6) \textbf{no style} (the original ICL demonstration that remains unchanged). 

We use GPT-4o to ensure high-quality restyling of ICL demos, modifying their style in six ways: \textbf{three-part} structuring, \textbf{lengthy} expansion, \textbf{human}-like tone, \textbf{combined} style (use three-part, lengthy and human three styles to rewrite the ICL example simultaneously), \textbf{refusal} style (for \textbf{\color{myred} safety}-related cases), and \textbf{no style} (original ICL demo).

To assess the impact of restyled exemplars on LLM alignment, we select the \textbf{top-20} high-value-impact QA pairs from UltraChat and SORRY-Bench, categorizing them as \textbf{\color{myblue} factuality} (${S_\text{cand\_f}}$) and \textbf{\color{myred} safety} (${S_\text{cand\_s}}$) ICL candidates.

We compute the average value impact across all 20 instances for the instances in ${S_\text{cand\_f}}$. The same computation is performed for ${S_\text{cand\_s}}$ as well.
As shown in Table~\ref{tab:restyle}, we summarize that restyling ICL demonstrations significantly impacts LLM alignment, with different styles enhancing different alignment dimensions.

We provide answers to the two questions.
\emph{(Q1) Will the restyled demonstration impact LLM alignment?}
Answer: For \textbf{\color{myblue} factuality}-focused ICL exemplars, the \textbf{combined} style achieves the highest overall factuality performance across multiple dimensions, while \textbf{three-part}, \textbf{lengthy}, and \textbf{human} styles individually improve ``clarity'', ``depth'', and ``engagement'', respectively. 
However, none of the styles improve ``safety''.

\emph{(Q2) What effects do the restyle QA pairs from different datasets will have?}
Answer: For \textbf{\color{myred} safety}-focused ICL exemplars, the \textbf{refusal} style is the only effective approach, significantly enhancing ``safety'', while other styles either have minimal impact or reduce alignment performance.

% Also, to achieve optimal LLM alignment, a balanced trade-off between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety} must be carefully managed. 
% \textbf{\color{myblue} Factuality} ICL exemplars should be restyled using the \textbf{combined} style, while \textbf{\color{myred} safety} ICL exemplars should be restyled using the \textbf{refusal} style.

For details on the experimental design related to \textbf{RQ1} and the discussion on the effects of restyling, please refer to the Appendix~\ref{appendix:restyle_discuss}. 
The explicit prompts used for restyling can be found in Appendix~\ref{app:prompt_restyle}.
Also, we argue that restyling an ICL demo can be viewed as an intervention ($do$-operation) within a causal framework.
For a detailed theoretical analysis of this aspect, please refer to the Appendix~\ref{append:causality}.

%
\subsection*{RQ2: Combining restyled ICL exemplars}
\label{ssec:combine_icl}
Our study confirms that combining multiple restyled ICL demonstrations into a cohesive demo set yields superior results compared to relying on a single ICL demo.
Refer to Appendix~\ref{appendix:combine_restyle_dicsuss} for complete experimental procedures and analysis details.

To achieve an optimal balance between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, we explored various style configurations and employed a hierarchical traversal approach with early pruning~\cite{DBLP:conf/emnlp/HuaQH24} to construct effective ICL demonstration sets (the details of this algorithm can be found in Appendix~\ref{appendix:dfs}).

Ultimately, we identified three high-performing ICL demo combinations, referred to as \textbf{R}estyled \textbf{I}n-context-learning \textbf{D}emonstration \textbf{E}xemplars (\textbf{RIDE}), each offering different trade-offs between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}:
(i) $\textbf{RIDE}_{\text{f}}$: Three\footnote{To reduce the search space while maintaining a sufficient number of ICL demonstrations, and to align with the number of ICL examples used in SOTA URIAL method (ensuring a more straightforward comparison in experiments), we set the number of ICL demonstrations to 3.} \textbf{\color{myblue} factuality} ICL examples restyled in the ``\textbf{combined}'' style.
(ii) $\textbf{RIDE}_{\text{fs\_uni}}$: Two \textbf{\color{myblue} factuality} ICL examples and one \textbf{\color{myred} safety} example, all restyled in the ``\textbf{combined}" style.
(iii) $\textbf{RIDE}_{\text{fs\_hyb}}$: Two \textbf{\color{myblue} factuality} ICL examples restyled in the ``\textbf{combined}" style and one \textbf{\color{myred} safety} example restyled in the ``\textbf{refusal}" style.
As shown in Table~\ref{tab:restyle_combine}, these combinations outperform individual ICL demonstrations, demonstrating the effectiveness of carefully structured ICL demo sets in enhancing LLM alignment.
The prompts of $\textbf{RIDE}$ series can be found in Appendix~\ref{app:rideprompt_f}.

\input{latex/table/restyle_combine}

\subsection*{RQ3: High-Value-Impact ICL Demos vs. Randomly Selected ICL Demos}
\label{ssec:random_icl}
As previously mentioned, we selected the top-20 QA pairs with the highest value impact from datasets UltraChat and SORRY-Bench as our candidate demos. 
This naturally raises the question: \textit{Is ranking by value impact necessary when selecting ICL candidates?} \textit{If we were to randomly select 20 QA pairs from these two datasets and then apply the restyling approach and the hierarchical traversal approach to obtain the optimal ICL demo set, would its performance degrade compared to the RIDE demo set?}

\paragraph{Ranking by Value Impact is Necessary!} The answer to the above question is yesâ€”ranking is essential. As shown in Table~\ref{tab:restyle_combine}, randomly selected ICL demos (denoted as \textbf{Random}) provide less improvement to LLM alignment compared to those chosen based on value impact (marked as \textbf{RIDE}).
For detailed experimental design and an in-depth discussion of the aforementioned questions, please refer to Appendix~\ref{appendix:rank_dicsuss}.

\paragraph{Key Takeaways.} Based on our analysis and findings, we propose the following approach to generate an optimal ICL demo set that effectively enhances LLM alignment:
1) \textbf{Rank ICL candidates by value impact to identify the most effective examples};
2) \textbf{Apply restyling to improve alignment-related attributes};
3) \textbf{Use the hierarchical traversal approach to obtain the optimal ICL demo set}.
Figure~\ref{figure:ride_illustration} provides an illustration of the entire process for constructing the optimal ICL demonstration set.



