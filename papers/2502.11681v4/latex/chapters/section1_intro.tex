Alignment tuning helps bridge the gap between raw model capabilities and the nuanced requirements of different tasks, such as delivering accurate information, maintaining user safety, and handling sensitive topics with care~\citep{shneiderman2020bridging,shen2023large,wang2023aligning,DBLP:conf/iclr/Qi0XC0M024}. 
%
Currently, the instruction-following paradigm~\citep{ouyang2022training,sun2023aligning,DBLP:conf/iclr/DaiPSJXL0024,rafailov2024direct,zhou2024lima,wu2024self}, which combines supervised fine-tuning (SFT) and preference optimization, is widely used in alignment tuning. 
% (e.g., through reinforcement learning with human feedback, RLHF)
However, this paradigm requires high-quality annotated data and consumes significant computing resources. 
If there were a method to improve LLM alignment without modifying model parameters, it could effectively reduce training costs and increase the versatility of the alignment process~\citep{brown2020language}. 

% To this end, we propose an in-context learning (ICL) method, which instructs LLMs to handle downstream tasks using few-shot learning from ICL demonstrations.

The In-Context Learning (ICL) method, as an online few-shot learning approach, is a tuning-free strategy that effectively circumvents the instruction-following paradigm to address LLM alignment challenges.
Many ICL demonstration selection methods have been proposed to choose demonstrations that effectively elicit the desired model capabilities~\citep{liu2022makes,min2022rethinking,DBLP:conf/icml/Ye0F0K23,DBLP:journals/corr/abs-2305-14128,peng2024revisiting,DBLP:conf/icml/ChoiL24,DBLP:conf/eacl/WangYW24}.
However, these methods are not suitable for alignment tuning due to its unique nature. 

Alignment tuning imposes two conflicting demands on LLMs: on one hand, LLMs need to provide more in-depth, informative, and helpful content (\textbf{\color{myblue} factuality})~\citep{shen2023large}; on the other hand, for safety reasons, LLMs must refuse to answer inappropriate queries (\textbf{\color{myred} safety})~\citep{ji2024beavertails}. 
Thus, we need to achieve a delicate \textbf{trade-off} between these two abilities~\citep{DBLP:journals/corr/abs-2404-09932}, as improving the harmlessness of an LLM assistant may result in it being less helpful~\citep{DBLP:journals/corr/abs-2204-05862}.
This trade-off presents a challenge when selecting demonstrations-the demonstrations must balance both factuality and safety.

% URIAL~\citep{DBLP:conf/iclr/LinRLDSCB024} demonstrated that alignment tuning modifies the probability distribution of token generation, thereby guiding LLMs to produce safer and more reliable content. To leverage this generation distribution shift, URIAL introduced an ICL demo set consisting of three manually crafted exemplars. While these manually designed ICL exemplars effectively enhance LLM alignment, their approach remains somewhat opaque, as it does not provide a clear explanation for why these specific handcrafted ICL demos are effective.

Most relevant to our work, URIAL~\citep{DBLP:conf/iclr/LinRLDSCB024} proposed an ICL demo set composed of three manually designed exemplars. These handcrafted ICL exemplars complement each other, achieving a \textit{delicate balance} between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, thereby effectively enhancing LLM alignment capabilities. However, their approach remains somewhat opaque, as it does not offer a clear explanation for why these specific manually crafted ICL demos are effective.

In this work, we first design a metric for evaluating the quality of ICL demonstrations—value impact—and utilize this metric to rank and retrieve high-quality ICL exemplars from a candidate pool. Through an in-depth analysis of these exemplars, we identify distinctive stylistic features within ICL demos that influence LLM alignment capabilities (Section~\ref{section2}). 

Based on this insight, we propose an approach for explicitly restyling ICL demonstrations, enabling them to further enhance LLM alignment (Section~\ref{section3}, RQ1).
To address the trade-off between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, we systematically explore different style combinations while maintaining stylistic consistency across ICL demos (Section~\ref{section3}, RQ2). Through this process, we identify the optimal style configurations for various tasks and construct ICL demo sets that achieve a balanced trade-off (Section~\ref{section3}, RQ3). We refer to these optimized sets as \textbf{R}estyled \textbf{I}n-context-learning \textbf{D}emonstration \textbf{E}xemplars (\textbf{RIDE}).

% We conduct a series of experiments to empirically validate the effectiveness of \textbf{RIDE}, demonstrating its capability to improve LLM alignment across multiple dimensions.

In summary, our contributions are three-fold:

\textsc{(ii)} Through a systematic analysis of ICL exemplars, we identify specific stylistic attributes that contribute to improving LLM alignment capabilities. By evaluating these features using our proposed value impact metric, we provide insights into how different styles influence the effectiveness of ICL demonstrations.

\textsc{(ii)} We propose a restyling approach that systematically modifies ICL demonstrations to enhance alignment. By exploring different style configurations, we identify the optimal stylistic composition that balances the trade-off between  \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, leading to the development of \textbf{RIDE} as the most effective ICL demo set.

\textsc{(iii)} We conduct a series of experiments across different datasets and LLM models, demonstrating the effectiveness and superiority of our proposed method. The experimental results show that, across the three benchmarks, our method achieves improvements of $2.22\%$, $4.28\%$, and $9.07\%$ compared to the SOTA methods, respectively.