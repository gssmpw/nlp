% In this section, we first introduce the background knowledge about common practice of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Then, we present a deep analysis  ....

%\noindent\textbf{Background.} 
In this paper, the terms “unaligned LLMs” and “base LLMs” are interchangeably used to refer to LLMs that have not undergone preference optimization. 
In contrast, we refer to LLMs that have been fine-tuned with instructional data to promote ethical and beneficial behavior as “aligned LLMs”.
%
More details can be found in Appendix~\ref{ssec: background}.
% The ``\textit{\textbf{alignment tuning}}''\footnote{We use the term ``alignment tuning'' to refer both SFT (instruction-tuning) and RLHF process for simplicity.} process tunes the parameters $\theta$ of a base model $f$ to create a more assistant-like model $g(\mathbf{x}; \beta)$ that adheres to user instructions and human preferences. 


% The ``\textit{alignment tuning}''  process tunes the parameters $\theta$ of a base model $f$ to create a more asistant-like model $g(\mathbf{x}; \beta)$ to follow user instructions adhere to human preference. 
% This process typically involves two stages: supervised fine-tuning (SFT) on instruction data and reinforcement learning from human feedback (RLHF). During the SFT stage, the base LLM is tuned using instruction-answer pairs in a process known as ``instruction tuning.'' The RLHF stage refines the SFT-ed model further using a reward model, resulting in improved alignment with human expectations in terms of helpfulness, honesty, and harmlessness.


% \subsection{Alignment as Token Distribution Shift}
% \noindent\textbf{Motivation.}
% To understand what are learned during alignment tuning and how aligned models differ from untuned models,
% we analyze the token distribution changes between the two.  
% Specifically, 
% given a query from user, $\mathbf{q}=\{q_1, q_2, \cdots\}$, where $q_i$ is a token in the query, 
% we input it to the aligned model $g(x)$ to get its output $\mathbf{o}=\{o_1, o_2, \cdots\}$.
% For each position $t$, we compose a context $\mathbf{x_t}=\mathbf{q}+\{o_1, \cdots, o_{t-1}\}$.
% For this context, we have known that aligned model will generate $o_t$, and let's use $P_{\texttt{align}}$ to denote the probability distribution for this position.
% Our analysis is motivated by such a question: \textit{What if we suddenly switch to the base model for decoding the next token at this position?}
% Inputting $\mathbf{x_{t}}$ to the untuned model $f$, we create another distribution, namely $P_{\texttt{base}}$ and  for sampling the next token at this position.
% If the base model learns to change its behavior in this context by alignment tuning, we should be able to see a distribution shift between $P_{\texttt{base}}$ and $P_{\texttt{align}}$ at this position.
% In contrast, if the two distributions are very close, it means that alignment tuning is not influential at this position. 

\subsection{The change in token generation probabilities due to alignment tuning}
\label{ssec:tds}
%\noindent\textbf{Motivation.} 
% Therefore, we believe that the process of alignment tuning modifies the token generation probabilities of base LLMs. 
% Some tokens, which the base LLMs assigned high generation probabilities, have their probabilities reduced in aligned LLMs. 
% At the same time, aligned LLMs increase the generation probabilities of certain other tokens. 

Based on the characteristics of autoregressive models, we adopt a unique perspective—examining the probability distribution of token generation—to study how alignment tuning affects and intervenes in the content generated by the model.

Compared to base LLMs, aligned LLMs, after alignment tuning, increase the generation probability of specific tokens when faced with the same queries—these tokens often guide the LLMs toward producing ethical and regulated responses, where we demoted as \textbf{\color{myblue} benign tokens}. 
Conversely, the generation probabilities of other tokens, which are more likely to cause the LLMs to deviate toward content lacking alignment safeguards, are reduced.
We call these tokens as \textbf{\color{myred} malicious tokens}.

To understand how the ``\textit{\textbf{alignment tuning}}'' process affects token generation and to identify which tokens can be classified as \textbf{\color{myblue} benign} or \textbf{\color{myred} malicious}, we propose analyzing the discrepancy between the token generation probabilities of base LLMs and aligned LLMs.
%
Specifically, for a given user query $\mathbf{q}=\{q_1, q_2, \cdots\}$, 
we input it into an reference model $r(x)$ to obtain its output $\mathbf{o}=\{o_1, o_2, \cdots\}$ via greedy decoding. 
For each position $t$, we define a \textit{`context'} at this position to be $\mathbf{x_t}=\mathbf{q}+\{o_1, \cdots, o_{t-1}\}$. 
We denote the reference model's probability distribution for predicting the next token of this position as $P_{t}^{\texttt{refer}}$, where $o_t$ has the highest probability.  
Our analysis is driven by the following questions: \textit{If we take the base model $f$ as the reference model and compare the output of the aligned model $g$ to it, what kind of differences in probability distribution would we observe? Conversely, if we treat the aligned model $g$ as the reference model and compare the unaligned model $f$ to $g$ , what distinctions can we see?}

We first view the unaligned model $f$ as the reference model, and pass the context $\mathbf{x_{t}}$ into the base model $f$, we generate the reference output $\mathbf{o}_{t}$ and the corresponding output probability distribution $P^{\texttt{base}}_{t}$ for sampling the next token at this position. 
%
Let $\mathbf{o}_{t}$ represent the context for the $t$-th position, thus we compute the probability distribution $P^{\texttt{align}}_{t}$ of the aligned model $g$ at the this position.
If alignment tuning enables the LLM to learn how to adjust the probability distribution over the output vocabulary to prevent the model from favoring tokens that may lead to malicious content, then by comparing $P^{\texttt{base}}$ and $P^{\texttt{align}}$, we should observe a noticeable decrease in the generation probability of certain tokens.

Similarly, if we treat the aligned model $g$ as the reference model and use its output as the reference $\mathbf{o}$, we can force the unaligned model $f$ to generate $\mathbf{o}$, and during this process, observe the shifts in probability distributions between $P^{\texttt{align}}$ and $P^{\texttt{base}}$.

\noindent\textbf{Polarity tokens.}
% A key characteristic of auto-regressive decoding models is that the initial few tokens significantly influence the trajectory and direction of subsequent generated content. 
% Alignment tuning intervenes by adjusting the generation probabilities of specific tokens, thereby influencing the model’s output to favor more harmless and helpful content while reducing the likelihood of deviating toward a malicious output trajectory. 
When we consider the unaligned model $f$ as the reference model, its generated content tends to lack alignment safeguards and may be more harmful. 
As a result, in the earlier positions of such output~\citep{DBLP:journals/corr/abs-2406-05946}, malicious tokens dominate, steering the generation towards harmful paths. 
In contrast, the aligned model $g$, after alignment tuning, should lower the generation probabilities of these malicious tokens, pulling the output back onto the trajectory that is more in line with the human values.

Therefore, when using the output $\mathbf{o}_{t}$ from the unaligned model $f$ as the reference, we can calculate the probability distribution differences across the output vocabulary at position $t$. 
This is expressed as: $\Delta P^{\texttt{malicious}}_{t} = P^{\texttt{base}}_{t} - P^{\texttt{align}}_{t}$.
For a validation dataset containing multiple $\mathbf{q} - \mathbf{o}$ pairs, we count all the tokens present in the reference outputs and calculate the average $\Delta P^{\texttt{malicious}}_{t}$ for each unique output token $\mathbf{o}_{t}$. 
The output tokens with the highest average $\Delta P^{\texttt{malicious}}_{t}$ value are considered as \textbf{\color{myred} malicious tokens}.

Likewise, we view  the aligned model $g$ as the reference, and its output $\mathbf{o}$ as the ground-truth output.
We thus compute $\Delta P^{\texttt{benign}} = P^{\texttt{align}} - P^{\texttt{base}}$, and consider this difference as the adjustment to the generation probabilities made through alignment tuning. 
After tuning, the aligned model $g$ tends to favor tokens that guide the generated content toward helpful and harmless outputs, whereas the probability of these tokens is relatively low in the output of the unaligned model $f$.
We also viewed the tokens with the highest average difference $\Delta P^{\texttt{benign}}$ as \textbf{\color{myblue} benign tokens}.

\subsection{Findings of Polarity tokens}
\label{ssec:find}
The alignment process for LLMs ensures that AI models are not only technically proficient but also socially responsible, making them more suitable for real-world applications where trust and reliability are critical.
%
Therefore, LLMs that have undergone alignment tuning must generate factually accurate information across various tasks (referred to here as ``\textit{factuality}'') while also refusing to respond to malicious queries (referred to here as ``\textit{safety}'')~\citep{shen2023large}.
%
However, these two capabilities present a ``\textit{\textbf{conflict}}'' in terms of LLMs’ token generation preferences~\citep{tuan2024towards}. 
Specifically, when providing useful information, LLMs tend to generate tokens that convey a positive attitude in the initial stages of autoregressive generation (e.g., `Let’s', `Here’s'). Conversely, when refusing to respond, the probability distribution of generated tokens shifts toward polite and apologetic expressions (e.g., `sorry', `condone', `cannot'). 
Analyzing these two capabilities together reduces the distinctiveness of the distributions, thereby increasing the difficulty of analysis.

To address this ``\textit{conflict}'', in this work, we designed two sub-tasks to facilitate a clearer analysis of polarity tokens associated with LLMs' different alignment capabilities. 
%
The first sub-task focuses on analyzing the polarity tokens that enable LLMs to generate factually accurate information. The second sub-task involves analyzing the polarity tokens that enhance the LLM’s safety capabilities.

\input{latex/table/polarity_tokens}

In our empirical study of polarity tokens, we selected \texttt{llama-2-7b} as the base LLM and used \texttt{llama-2-7b-chat} as its aligned counterpart. We utilized the \alpaca{} benchmark~\citep{dubois2024length} to analyze polarity tokens related to LLM helpfulness, while a safety-related subset of the \dataname{} dataset~\citep{DBLP:conf/iclr/LinRLDSCB024} was used to analyze safety polarity tokens.

As described in Section~\ref{ssec:tds}, for the two sub-tasks, we separately calculated $\Delta P_{\texttt{malicious}}$ and $\Delta P_{\texttt{benign}}$, thereby identifying the polarity tokens that influence factuality and safety. 

In Table~\ref{tab:polarity_tokens}, we present the top-15 typical polarity tokens in descending order based on the magnitude of $\Delta P$.
From the table, we can see that the factuality and safety sub-tasks have their own preferred polarity tokens.
Based on this finding, we utilized two distinct sets of polarity tokens for factuality and safety, respectively, as the foundation for the ICL selection described in Section~\ref{ssec:method}.

Further analysis of polarity tokens, as well as discussions on the relationship between polarity tokens and alignment sub-tasks (factuality and safety), can be found in the Appendix~\ref{ssec: polarity_discussion}.

% Compared to aligned LLMs, unaligned LLMs exhibit a decreased probability of generating \textbf{\color{myblue} benign tokens}, while simultaneously increasing the probability of generating \textbf{\color{myred} malicious tokens}. 
