\subsection{Dataset, LLMs, and baseline methods}
\paragraph{Dataset.}
% todo: justeval test and evaluation is different
% todo: MT-BENCH reasoning subset performance
% todo: RIDE can beat well-finetuned mistral
We use \alpaca{} (a benchmark designed to assess the performance of language models on natural language understanding, generation, and reasoning tasks)~\citep{alpaca_eval}, \dataname{} (a dataset designed to assess the ethical reasoning capabilities of LLMs)~\citep{DBLP:conf/iclr/LinRLDSCB024}, and \mtbench{} (a multi-turn dialogue dataset to evaluate various capabilities of LLMs, such as reasoning, coding, and human-like interaction)~\citep{zheng2023judging} as benchmarks.

\paragraph{LLMs.}
We use three models as the base models: Llama-2-7b-hf~\citep{touvron2023llama}, Mistral-7b-v0.1~\citep{jiang2023mistral}, and OLMo-7B~\citep{groeneveld2024olmo}. 
It is important to note that these models have not undergone alignment tuning, resulting in sub-optimal alignment capabilities.

\paragraph{Baseline methods.}
We selected different baseline methods for comparison. 
The most relevant to our work is \textbf{\methodname{}}~\citep{DBLP:conf/iclr/LinRLDSCB024}, which manually designed three ICL examples and combined them with system instructions (explicitly informing the LLM to generate safe and reliable content) to form prompts, achieving state-of-the-art (SOTA) performance across multiple datasets using the ICL approach. 
Additionally, we compared against the following baselines: (1) \textbf{Zero-shot}: consisting only of the URIAL system instruction part. (2) \textbf{Vanilla ICL}: an ICL example set composed of the top-2 examples from $\{S_\text{cand\_f}\}$ and the top-1 example from $\{S_\text{cand\_s}\}$. (3) \textbf{Retrieval ICL}~\citep{liu2022makes}: Among the examples in $\{S_\text{cand}\}$, the neighbors that are the most similar to the given test query are retrieved as the corresponding in-context examples. (4) \textbf{TopK + ConE}~\citep{peng2024revisiting}: a tuning-free method that retrieves the best three examples that excel in reducing the conditional entropy of the test input as the ICL demonstrations.
In this work, we consistently use GPT-4o as the LLM-as-a-judge to evaluate and score the responses generated by the LLMs.
Through comparing these baseline methods with our proposed ICL demonstration set, i.e., $\textbf{RIDE}_{\text{f}}$, $\textbf{RIDE}_{\text{fs\_uni}}$, and $\textbf{RIDE}_{\text{fs\_hyb}}$, we conducted a detailed experimental analysis.

\subsection{Empirical results on \dataname{}}
\label{ssec:exp_justeval}
\input{latex/table/table_justeval}

Table~\ref{tab:justeval} presents the scores of each method on \dataname{}. 
In \dataname{}, the dataset places a great emphasis on safety. 
Out of the $1000$ test cases in \dataname{}, $200$ questions are safety-related and require the model to provide clear refusal responses. 
The remaining $800$ instances are related to factuality, requiring the LLM to provide accurate and helpful factual knowledge. 
Therefore, \dataname{} evaluates both the factuality and safety capabilities of the LLM, requiring the LLM to make a balanced trade-off between the two.

From Table~\ref{tab:justeval}, we can summarize the following conclusions.
First, among the three proposed ICL sets, $\textbf{RIDE}_{\text{fs\_hyb}}$ performs the best, followed by $\textbf{RIDE}_{\text{fs\_uni}}$, and finally $\textbf{RIDE}_{\text{f}}$. $\textbf{RIDE}_{\text{fs\_hyb}}$ includes both factuality and safety ICL examples, with the safety demonstration restyled using the ``refusal'' style, which effectively enhances the LLM’s safety capability while maintaining good factuality. 
Although $\textbf{RIDE}_{\text{fs\_uni}}$ also contains a safety demonstration, it uses the ``combined'' style for restyling. 
While the three examples in it have a consistent style, the safety ability of the safety example is weakened, resulting in a lower ``Safe'' score compared to $\textbf{RIDE}_{\text{fs\_hyb}}$. 
As for $\textbf{RIDE}_{\text{f}}$, which consists entirely of factuality examples, it has the strongest factuality capability but lacks any safety example, preventing the LLM from learning how to refuse malicious queries, leading to a much lower ``Safe'' score compared to the other two ICL sets.
This finding aligns with our observations in Section~\ref{ssec:causality}.

Second, compared to \methodname{}, $\textbf{RIDE}_{\text{fs\_hyb}}$ outperforms it in two out of three models. 
In the case of OLMo-7B, the input window length is severely limited (only 2048 tokens), while our prompts containing ICL examples exceed this limit. 
Thus, we had to randomly remove parts of the ICL bullet points, which especially affects the LLM’s performance in ``Helpful'', ``Factual'', and ``Deep''.
However, even under such constraints, we can see that $\textbf{RIDE}_{\text{fs\_hyb}}$ performs comparably with \methodname{} in various aspects, with nearly identical scores in the crucial ``Safe'' metric ($2.69$ vs $2.70$), although it is slightly weaker in the overall ``Average'' score ($3.48$ vs $3.51$).

Third, in the first block of Llama2-7b, we compared four baseline methods.
It can be observed that the baseline methods exhibit a significant performance gap compared to \methodname{} and our ICL sets. 
\textbf{TopK + ConE} is the closest in principle to our approach: selecting good ICL demonstrations by observing the impact of ICL on content generation during inference. 
\textbf{TopK + ConE} is the best among the four baseline methods, but there is still a considerable gap compared to our approach. 

In addition, comparing \textbf{Vanilla ICL} and our $\textbf{RIDE}$ series ICL example sets reveals that simply combining the best-performing examples from $\{S_\text{cand\_f}\}$ and $\{S_\text{cand\_s}\}$ does not yield an optimal set. 
The performance gap between \textbf{Vanilla ICL} and $\textbf{RIDE}$ also validates the effectiveness of our method for selecting a set of ICL demonstrations, as described in Section~\ref{appendix:dfs}.
Noted that we only used Llama-2-7b-hf in this benchmark to compare all baseline methods and evaluate their performance to reduce token consumption when calling LLM-as-a-judge.

% Therefore, as shown in the Table~\ref{tab:justeval}, we introduced \valueimp{}$_{\text{16\_8\_sorry}}$. This set was created by replacing the worst-performing single ICL example (which was a factual knowledge Q\&A) from the \valueimp{}$_{\text{16\_8\_10}}$ set with an example that involves refusal, to assess whether this substitution could effectively enhance the LLM’s safety capabilities.

% As shown in Table~\ref{tab:justeval}, \valueimp{}$_{\text{16\_8\_10}}$ performs best on Mistral-7b in terms of average performance (Avg.); however, on the other two models, \methodname{}$_{\text{INST\_1K\_V4}}$ shows the best average performance. Looking at the individual metrics, \methodname{}$_{\text{INST\_1K\_V4}}$ consistently outperforms in the safety metric, indicating that this baseline method is more effective in enabling the model to learn how to identify toxic content and refuse to answer from the refusal examples. As previously mentioned, since \dataname{} places a greater emphasis on safety considerations and \methodname{}$_{\text{INST\_1K\_V4}}$ consistently outperforms our method in the safety metric, the baseline method overall outperforms our method on the two models.

% Moreover, the \valueimp{}$_{\text{16\_8\_sorry}}$ set, which includes a refusal example, shows a significant increase in safety scores across all three models compared to \valueimp{}$_{\text{16\_8\_10}}$. This indicates that adding a refusal example can indeed effectively enhance the LLM’s ability to identify toxic content and improve its ability to refuse to answer toxic questions. However, across the other five metrics (excluding safety) on all three models, we observe a decline in performance compared to the original \valueimp{}$_{\text{16\_8\_10}}$. This suggests that replacing one factual knowledge Q\&A example with a refusal example in the ICL demonstration example set led to a decrease in the quality of the generated answers. The likely reason is that with only two examples, the LLM is insufficiently inspired to learn the style and characteristics of a high-quality response. Therefore, in future work, we need to explore ways to add refusal examples without compromising answer quality, so that the LLM can both identify toxicity and generate high-quality responses.

\subsection{Empirical results on \alpaca{}}
\label{ssec:exp_alpaca}
\input{latex/table/table_alpaca}

% Table~\ref{tab:alpaca} presents the comparison results on \alpaca{} between different models combined with the ICL demonstration example sets. For the same input, the outputs of the other models are compared against the output of the baseline model. In this work, we employ GPT-4o as the evaluator to assess whether the outputs of the other models are better than, equivalent to, or worse than the output of the baseline model. We document the percentage of outputs from the other models that are better than, equivalent to, or worse than the baseline model.

% %
% We randomly selected 1,000 samples from the \alpaca{} training set as the candidate ICL demonstration example pool and used value impact to identify the best-performing candidate ICL examples on the Lima dataset. The top 20 best-performing candidate ICL examples were then rewritten in terms of style (three-paragraph format, human-like tone, and lengthy responses). We refer to these as the rewritten ICL examples. We then randomly selected a subset from the \alpaca{} validation set as the validation subset and used a pre-pruning hierarchical traversal method to find the two best combinations of rewritten ICL demonstration examples on this validation subset, labeled as \valueimp{}$_{\text{16\_8\_10}}$ and \valueimp{}$_{\text{8\_15\_1}}$ respectively. These two ICL demo example sets were compared with \methodname{}$_{\text{INST\_1K\_V4}}$, and the comparison results are presented in Table~\ref{tab:alpaca}.

% We used Llama2-7b, Mistral-7b, and Olmo-7b, each with different ICL demonstration example sets applied in the prompt to enable the models to learn alignment capabilities through few-shot learning. 

% As shown in Table~\ref{tab:alpaca}, across the helpful, deep, and engaging metrics, our ICL demonstration example sets, i.e., \valueimp{}$_{\text{16\_8\_10}}$ and \valueimp{}$_{\text{8\_15\_1}}$, significantly outperform the baseline method \methodname{}$_{\text{INST\_1K\_V4}}$ on all three LLMs. This demonstrates that our specially rewritten ICL demonstration examples can effectively prompt the models to generate high-quality, lengthy, and human-like responses. 

% For the clarity metric, we found that, except for the combination of \valueimp{}$_{\text{8\_15\_1}}$ and Llama2-7b, our method consistently outperforms the baseline method in other settings, indicating that the rewritten ICL demonstration examples can help the LLM generate clearer answers. As for the factual accuracy metric, our method and the baseline method show mixed results across different settings, with overall performance being comparable.

% In terms of safety, as shown in the table, the comparison between our method and the baseline across the three models shows mixed results, with some higher and some lower, but overall the performance is generally similar. Additionally, we can observe that the majority of the answers are difficult to distinguish in quality (judged by the model as equivalent to each other). This phenomenon might be due to the fact that the questions in the \alpaca{} dataset mostly involve factual knowledge, with content that is relatively non-sensitive or non-toxic, thus not requiring the model to make a clear refusal response (i.e., refusing to answer because the response might contain toxic content). As a result, the evaluator LLM finds it challenging to determine whether the responses to neutral, objective factual knowledge are safe. Therefore, for the vast majority of answers, the safety level of the responses generated by our method and the baseline method is the same, making it difficult to identify a significant distinction.

Unlike \dataname{}, in \alpaca{}, the dataset places more emphasis on \textbf{factuality}. 
One characteristic of the \alpaca{} dataset is the lack of safety evaluation, meaning that this benchmark only evaluates the instruction-following capabilities of LLMs rather than the potential harm they could cause\footnote{\url{https://github.com/tatsu-lab/alpaca_eval}}. Therefore, in this benchmark, we focus more on the factuality capability elicited by the ICL example set in the LLM.

As shown in Table~\ref{tab:alpaca}, we have the following findings.
First, among the $\textbf{RIDE}$ series sets, $\textbf{RIDE}_{\text{f}}$ performs the best, followed by $\textbf{RIDE}_{\text{fs\_uni}}$, and $\textbf{RIDE}_{\text{fs\_hyb}}$ performs the worst. 
This result is the opposite of what is shown in Table~\ref{tab:justeval}.
The reason for this reversal aligns with the analysis in Section~\ref{ssec:causality} and Section~\ref{ssec:exp_justeval}, which is primarily due to the impact of \textbf{style}.
Since most samples in \alpaca{} are related only to factuality, the set composed entirely of factuality examples, $\textbf{RIDE}_{\text{f}}$, is most effective at eliciting the LLM’s factuality capabilities.
The three examples in $\textbf{RIDE}_{\text{fs\_uni}}$ are all restyled using the ``combined'' style, which ensures consistency, but the inclusion of a safety demonstration slightly weakens its factuality performance. 
On the other hand, $\textbf{RIDE}_{\text{fs\_hyb}}$, which has the strongest safety capability, performs the worst in factuality.

Second, $\textbf{RIDE}_{\text{f}}$ outperformed \methodname{} across all models, indicating that the ICL examples we selected, after restyling, enable the LLM to quickly and effectively learn a specific output pattern, which then guides the LLM’s content generation, thereby enhancing its factuality capabilities.

Third, a comprehensive analysis of the ``Safe'' scores across all methods shows that they are largely consistent, further proving that \alpaca{} has little discriminative power for evaluating the safety capabilities of LLMs. 
Thus, $\textbf{RIDE}_{\text{fs\_hyb}}$, which exhibited excellent safety performance in \dataname{}, performs worse in this benchmark.

\subsection{Empirical results on \mtbench{}}
\label{ssec:exp_mtbench}

Unlike \alpaca{} and \dataname{}, \mtbench{} evaluates whether the LLM can learn to handle complex tasks, particularly reasoning and calculation, from the provided ICL demo examples. Table~\ref{tab:mtbench_overall} presents the overall performance of ICL demo examples on different models when handling the \mtbench{} dataset. It is important to note that \mtbench{} is a multi-turn dialogue dataset. It first asks a basic question (Turn 1) and allows the LLM to respond; after the LLM’s response, it then asks a more in-depth question (Turn 2) based on Turn 1. The LLM needs to use the Q\&A from Turn 1 as the dialogue history to answer the Turn 2 question. Therefore, in Table~\ref{tab:mtbench_overall}, performance is divided into Turn 1 and Turn 2, with ‘overall’ representing the LLM’s overall performance across both turns. Meanwhile Table~\ref{tab:mtbench} records the performance of different ICL examples applied to different models on various tasks within the \mtbench{} dataset.

\input{latex/table/table_mt_overall}

% As shown in Table~\ref{tab:mtbench_overall}, \valueimp{}$_{\text{16\_8\_10}}$ achieved the best overall performance across all models. On Mistral-7b and Olmo-7b, \valueimp{}$_{\text{16\_8\_10}}$ performed best in Turn 2, while on Llama2-7b, it showed the best performance in Turn 1. Overall, our method achieved better performance than the baseline method, indicating that our ICL examples are more effective in enabling the LLM to solve complex tasks, and can even effectively enhance the LLM’s reasoning and calculation abilities.

% As presented in Table~\ref{tab:mtbench}, in all settings, the baseline method \methodname{}$_{\text{INST\_1K\_V4}}$ achieved the best performance on Llama2-7b only in the writing task; when applied to Mistral-7b, it was the best only in extraction; similarly, on Olmo-7b, it was the best only in roleplay. In contrast, our method achieved the best performance in the remaining 7 tasks across all three models. This indicates that, in terms of detailed performance across different tasks, our method is more effective in enabling the models to handle multiple tasks simultaneously.

As shown in Table~\ref{tab:mtbench_overall}, we have the following findings.
First, among the $\textbf{RIDE}$ series, $\textbf{RIDE}_{\text{f}}$ performs best overall, followed by $\textbf{RIDE}_{\text{fs\_hyb}}$, and $\textbf{RIDE}_{\text{fs\_uni}}$ performs the worst. 
Since \mtbench{} assesses whether LLMs can handle complex tasks, the ICL demonstrations provided in $\textbf{RIDE}_{\text{f}}$ effectively enhance the LLM’s factuality capability.
The ICL examples restyled with the ``Combined'' style (especially the ``Three-part'' style) give the responses a clear structure and rigorous logic, which, to some extent, improves the LLM’s reasoning ability, making $\textbf{RIDE}_{\text{f}}$ perform best in this benchmark. 
The safety examples included in $\textbf{RIDE}_{\text{fs\_hyb}}$ and $\textbf{RIDE}_{\text{fs\_uni}}$ weaken this capability, leading to average performance.

Second, the fact that $\textbf{RIDE}_{\text{fs\_hyb}}$ outperforms $\textbf{RIDE}_{\text{fs\_uni}}$ is an interesting and surprising finding. 
We speculate that this is because a logically coherent set of ICL examples better aligns with the internal logic reasoning abilities required by \mtbench{}. 
The demonstration restyled with the ``Refusal'' style in $\textbf{RIDE}_{\text{fs\_hyb}}$ starts by refusing to answer a malicious example, then provides a reasonable justification, and finally offers guidelines. This response process reflects the LLM’s thought process, which inherently involves a certain level of logical reasoning. 
This logical reasoning might enhance the LLM’s reasoning capabilities, aligning with preference of \mtbench, thereby making $\textbf{RIDE}_{\text{fs\_hyb}}$ a better ICL demonstration set.

Third, in two of the three models (Mistral-7b and Olmo-7b), our method outperforms \methodname{} in ``Turn 2'' performance. 
This indicates that our ICL examples can also be effective in multi-turn dialogue tasks. 
Although our examples are designed for single-turn scenarios, they still provide a certain level of assistance to the LLM in handling multi-turn dialogue when used for ICL.
The detailed performance of different model and ICL method combinations on each specific task of the MT-bench dataset can be found in Table~\ref{tab:mtbench} (in Appendix~\ref{append:mt_bench_all}).

\subsection{Takeaways of the Empirical Study}
From the findings in Section~\ref{ssec:exp_justeval}, \ref{ssec:exp_justeval} and \ref{ssec:exp_mtbench}, in conclusion, we find that different benchmarks have different focal points when evaluating the capabilities of LLMs. 
In this context, both the \textbf{content} (whether the ICL example set is oriented toward safety or factuality) and the \textbf{style} (whether the restyling is more structured or focused on refusal) interact with each other to jointly influence the performance of ICL. 
Thus, through our experiments, we validate the causal relationship among \textit{content}, \textit{style}, and \textit{alignment}. 
Furthermore, we observe that the \textit{safety} and \textit{factuality} capabilities of LLMs are inherently conflicting, requiring us to find a trade-off to achieve the best overall performance. 
This finding is also consistent with our analysis of \textit{polarity tokens}. 
Therefore, the experimental results validate the two key concepts proposed in this paper: the existence of \textit{polarity tokens} with different orientations toward safety and factuality, and the \textit{causal structure} within alignment.