\begin{comment}
We first introduce the challenges of using base LLMs as assistants that can understand instructions and generate human-preferred answers (Sec.~\ref{ssec:backround}).
In Sec.~\ref{ssec:baseline}, we first introduce baseline methods that can elicit untuned LLMs to start and end answering user queries\ar{suggested rewrite: induce even untuned LLMs to answer user queries}.
Then, in Sec.~\ref{ssec:icil}, we introduce in-context learning methods that use a few input-output pairs as demonstrations for base LLMs to learn to infer the response to a given query.
Finally, in Sec.~\ref{ssec:icalign}, we present a follow-up alignment stage via in-context learning that can further refine initial responses to become more human-preferred.
\end{comment}

% The analysis in Sec.~\ref{ssec:tds} offers us a different perspective on alignment tuning, suggesting that alignment tuning modifies the probability distribution of specific tokens. 
% This adjustment causes auto-regressive models to favor the use of benign tokens during the early stages of content generation, thereby influencing the trajectory of subsequent outputs. 
% If we could use ``\textit{\textbf{In-context Learning (ICL)}}'' -- a low-cost, tuning-free approach -- where the LLM learns to align its outputs with human preferences and values from a limited number of demonstration examples, this could effectively shift the token probability distribution and enhance the LLM’s alignment capabilities. 
% The following two questions then arise: \textit{how can we use an objective, automated metric to identify one single high-quality demonstration example (Section~\ref{ssec:method})? how can we combine the single example to form an approximate optimal demonstration set (Section~\ref{ssec:causality})?}

The analysis in Sec.~\ref{ssec:tds} inspired us to make the following hypothesis: if an In-context Learning (ICL) example positively influences the probability distribution of polarity tokens, then it should be considered a high-quality ICL demonstration example.
The following three questions then arise: \textit{how can we use an objective, automated metric to identify a single high-quality demonstration example (Section~\ref{ssec:method})?} 
\textit{If we can further enhance the quality of this example, will it make it even more beneficial for ICL (Section~\ref{ssec:causality})?} 
Multiple ICL demonstrations are often more effective than a single ICL example; \textit{how can we identify such an approximate optimal demonstration set (Section~\ref{ssec:dfs})?}

% \subsection{Background} 

% \noindent\textbf{Challenges.} Untuned LLMs, just off pre-training with the next-token prediction objective, cannot well follow human instructions. 
% We find the behavior of untuned models such as \texttt{llama2(base)} can be categorized as follows:
% (1) repeat the same question, (2) generate more questions, (3) add more context about the question, and (4) answer the question.
% In all four situations, base models tend to endlessly generate content. 
% For these reasons, base models' outputs are unlikely to be useful for assisting humans as a chat assistant.
\subsection{Selection of Single ICL Demonstrations}
\label{ssec:method}

We propose the following hypothesis: a ``good'' ICL demonstration example should enable the model to significantly adjust the generation probabilities of polarity tokens, increasing the probability of generating \textbf{\color{myblue} benign tokens} while reducing the probability of generating \textbf{\color{myred} malicious tokens}.

For a given user query $\mathbf{q}=\{q_1, q_2, \cdots\}$, to calculate the impact of an ICL example $c$ on benign tokens, we use the aligned model $g$ to generate a reference output $\mathbf{o}$. 
For all benign tokens $\{\mathbf{o}_{b}\}$ in $\mathbf{o}$, we compute the enhancement in the generation probability of \textbf{\color{myblue} benign tokens} by an ICL example using the following formula: $\Delta P_{\texttt{b}}(q, c) = \Sigma_{o_t \in \{\mathbf{o}_{b}\}}\alpha_t[P^{\texttt{base}}(o_t|x_t, c) - P^{\texttt{base}}(o_t|x_t)]$.

Here,  $\alpha_t$ is used to control the importance weight of the probability difference. We believe that the initial few tokens generated by the model have a greater impact on the trajectory of the generation. 
Therefore, we assign higher weights $\alpha_t$ to the tokens $o_t$ at the early positions, and gradually decrease the weight $\alpha_t$ as the generation progresses.

Similarly, to calculate how an ICL example $c$ reduces the generation probability of \textbf{\color{myred} malicious tokens}, we first use the unaligned model $f$ as the reference to generate the output $\mathbf{o}$. 
We then compute the effect on each malicious tokens $\{\mathbf{o}_{m}\}$ in $\mathbf{o}$ individually, yielding: $\Delta P_{\texttt{m}}(q, c) = \Sigma_{o_t \in \{\mathbf{o}_{m}\}}\alpha_t[P^{\texttt{base}}(o_t|x_t) - P^{\texttt{base}}(o_t|x_t, c)]$.

If an ICL example can have the higher value: $V_{polar} = \frac {\Sigma_{q_t \in Q}(\Delta P_{\texttt{b}}(q_t, c) + \Delta P_{\texttt{m}}(q_t, c))}{|Q|}$ for the examples in the validation dataset $Q$, it should be an ICL example that effectively adjusts the generation probabilities of polarity tokens, thereby guiding the LLM’s generation trajectory more towards alignment with human values.
We choose the ICL demonstration examples that have the highest $V_{polar}$ as the high-quality ICL examples. 

In empirical study, we treated the QA pairs from UltraChat (a large-scale multi-turn dialogue corpus aimed at training and evaluating advanced conversational AI models)~\citep{ding2023enhancing} and SORRY-Bench (a dataset intended to be used for LLM safety refusal evaluation)~\citep{xie2024sorry} as candidate pools for ICL demonstration examples of factuality and safety, respectively.

From Section~\ref{ssec:find}, we knew that polarity tokens for factuality and safety are different.
Therefore, we used these two distinct sets of polarity tokens to select the top-20 ICL demonstration examples with the highest $V_{polar}$ for the factuality and safety subtasks, respectively, as $\{S_\text{cand\_f}\}$ and $\{S_\text{cand\_s}\}$. 

\subsection{A Causal Approach to Restyle}
\label{ssec:causality}
In this section, we hypothesize that restyling can further enhance the quality of ICL demonstrations, and we validate this hypothesis through experiments.

We first provide the following definitions. \textit{\textbf{content}} refers to the task-related information provided in an ICL example, including the system instruction and the demonstration, \textit{\textbf{style}} represents the writing style of task-related information and the organizational structure of the content, and \textit{\textbf{alignment}} refers to the alignment effect exhibited by the model after using a particular example as a ICL demonstration.

% We consider \textit{\textbf{style}} and \textit{\textbf{content}} to be the two most critical factors in applying ICL techniques for alignment tuning. 
% We model $S$ (\textit{style}), $C$ (\textit{content}), and $A$ (\textit{alignment}) as a \textit{\textbf{causal structure}}~\citep{pearl2009causality}, as illustrated in the Figure~\ref{fig:causal_structure}. 
% It is important to note that, for simplicity, we assume that content does not confound the effect of style on alignment, and vice versa. 
% Therefore, in the Figure~\ref{fig:causal_structure}, there are no direct arrows connecting \textit{style} and \textit{content}, but both jointly influence \textit{alignment}.

% Qu version:
We consider \textit{\textbf{style}} and \textit{\textbf{content}} to be the two most critical factors in applying ICL techniques for alignment tuning. 
We model $S$ (\textit{style}), $C$ (\textit{content}), and $A$ (\textit{alignment}) as a \textit{\textbf{causal structure}}~\citep{pearl2009causality}, as illustrated in Figure~\ref{fig:causal_structure}. 
The variable $C$ is the co-founder, which influences both $S$ and $A$. Both $C$ and $S$ jointly influence \textit{alignment}.

\begin{wrapfigure}{r}{3.5cm}
    \centering
    \vspace{-17pt}
    \includegraphics[width=0.25\columnwidth]{latex/figure/causal_structure_with_cross.png}
    \caption{The causal structure of style, content, and alignment.}
    \vspace{-10pt}
    \label{fig:causal_structure}
\end{wrapfigure}

\paragraph{Content}
We consider $C$ as a factor that cannot be experimentally manipulated. 
On the one hand, using LLMs to modify the content of an LLM’s response can lead to hallucinations, making the study uncontrollable. 
On the other hand, altering the content changes the nature of the demonstration, thus losing the significance of the research. 
Therefore, our primary interest lies in the impact of the intervenable factor $S$ on $A$, and we thus disregard the influence of $C$ on $A$, focusing instead on evaluating the effect of the controllable intervention $S$.

\paragraph{Style}
To quantify the impact of an intervention on an outcome of interest, the Average Treatment Effect (ATE) is a commonly used method in causal inference~\citep{kaddour2021causal,DBLP:conf/iclr/MahajanMNS24}.
Therefore, we use ATE as the expected difference in outcomes to determine, on average, how much effect the intervention has compared to other interventions.

% We thus examine the \textit{style} factor. 
Specifically, following the principles of causality, we consider setting $S$ to a fixed value as an intervention, denoted using the $do$-operator: $do(S=s)$\footnote{Which can also be shortened to $do(s)$.}.
Whenever $do(s)$ appears after the conditioning bar, it means that everything in that expression is in the \textit{post-intervention} world where the intervention $do(s)$ occurs.

It is important to note that, in Figure~\ref{fig:causal_structure}, there is an edge from $C$ to $S$, indicating that $C$ confounds the effect of $S$ on $A$. 
However, according to the definition in causal theory, $do(s)$ will remove the edge from $C$ to $S$ when intervening on $S$, meaning that $C$ will no longer affect $S$, as indicated by the red cross in the figure.

Thus, $E(A|do(S=s))$ refers to the expected alignment improvement after all examples have been restyled using the format $s$. According to the backdoor criterion, we obtain:
\begin{align}
E[A | do(S = s)] = \sum_{c} E[A | s, C = c] p(c)
\end{align}
The ATE is defined as:
\begin{align}
ATE(s_t, s_o) = E[A | do(S = s_t)] - E[A | do(S = s_o)]
\label{equ:ATE}
\end{align}
where $s_t$ refers to target style, and $s_o$ denotes other style.

Empirically, we adopted the idea of Monte Carlo sampling~\citep{knaus2021machine} and approximate $p(c)$ as a uniform distribution.
We used a single example as the ICL demonstration, enabling the LLM to handle downstream tasks through one-shot online learning. 
To calculate the expectation $E[A | s, C = c]$, we kept the content of the ICL demonstration fixed ($C=c$), while restyling the demonstration example with a specific style $s$.
The restyled demonstration example is then encapsulated in the prompt and fed to the LLM, which processes examples from the validation dataset via ICL. 
We considered the LLM’s average alignment performance on the validation dataset as an approximation of $E[A | s, C = c]$. %\reza{how is the restyling of an example done? by another LLM? Manually? How accurate is it? etc}


Based on the concept of Monte Carlo sampling, we randomly selected $N$ ICL demonstrations\footnote{To reduce computational complexity, we set $N$ to $5$.} from the candidate high-quality ICL examples to form the set $\{C\}$. 
Corresponding to the $N$ demonstrations in $\{C\}$, we applied the same restyle process to each, resulting in $N$ average alignment performance values. 
By averaging these $N$ values, we obtain an approximation of $E[A \mid do(S = s)]$, where $c \in \{C\}$ and $p(c)$ follows a uniform distribution.
It is worthy noting that in Section~\ref{ssec:find}, we found LLMs exhibit conflicting behavior when handling ``\textit{factuality}'' and ``\textit{safety}'' sub-tasks.
In such cases, the LLM needs to achieve a trade-off between these two capabilities to mitigate the conflict. 
Therefore, we randomly selected a set $\{C_f\}$ from $\{S_\text{cand\_f}\}$ (defined in Section~\ref{ssec:method}), focusing on ``\textit{factuality}'', and a set $\{C_s\}$ from $\{S_\text{cand\_s}\}$ (defined in Section~\ref{ssec:method}), focusing on ``\textit{safety}'', and applied the same style restyling to each.

% \reza{can you include the detailed prompts for the LLm-based restyling in the appendix?} 
In our work, we used a powerful LLM\footnote{We used GPT-4o to restyle the answers in the ICL examples.} to modify the style of the answer part in the following ways: (1) three-part (presenting the answer in a three-part structure: first, introducing the answer in one sentence; second, itemizing the answer using bullet points; and third, summarizing the answer in one sentence), (2) lengthy (enriching the answer details and increasing its length without altering the original meaning), (3) human (using a conversational tone or answering from a first-person perspective), (4) combined (use three-part, lengthy and human three styles to rewrite the ICL example simultaneously), (5) refusal (for safety-related ICL examples, first refuse to answer, then provide a reason, and finally offer advice or guidelines), and (6) no style (the original ICL demonstration that remains unchanged). 
% The restyled answers are designed to effectively help users with a clear structure, provide more details, and make the responses feel more personable.

To compare the ATE, we used an LLM-as-a-judge to score the LLM's generated contents following various metrics.
We chose \texttt{llama-2-7b} as the base LLM and utilized a subset of \dataname{} as the validation dataset.

By analyzing the ATE results (details can be found in Appendix~\ref{ssec: ATE_style}), we have the following findings: (1) for factuality-related ICL demonstration examples, we should adopt the ``combined'' style; (2) for safety demonstrations, the ``refusal'' style should be used; (3) the differing emphases of the factuality and safety subtasks on various styles validate our findings in Section~\ref{ssec:find}, namely, that to achieve optimal overall performance in an LLM, a trade-off between factuality and safety must be reached.
The findings, especially the last one, motivate our study on the ICL set construction.

% \subsection{Selection of a Set of ICL Demonstrations}
% \label{ssec:dfs}
% Finding an optimal demonstration example set from the candidate high-quality ICL examples is a NP-hard problem~\citep{DBLP:conf/icml/Ye0F0K23}, and so heuristic approaches should be used in general to get an (approximate) optimal approximation solution~\citep{liu2024se2}.

% Previous research has shown that subtle interactions between the demonstrations in an ICL example set can significantly influence the performance of LLMs in few-shot online learning. On the one hand, maintaining a consistent response style across ICL demonstration examples can effectively enhance LLM performance on downstream tasks~\citep{DBLP:conf/iclr/LinRLDSCB024,li2024scar}. 
% On the other hand, the multiple ICL demonstrations needs to be sufficiently diverse and complementary to fully elicit LLMs' task-oriented capabilities~\citep{min2022rethinking}. 
% Notably, when dealing with safety tasks, having refusal demonstration in the set becomes particularly crucial. 

% In Section~\ref{ssec:method}, we already formed candidate sets $\{S_\text{cand\_f}\}$ and $\{S_\text{cand\_s}\}$.  
% Therefore, for the \textit{factuality} candidates $\{S_\text{cand\_f}\}$, we restyled them using the “combined” style, while for the \textit{safety} candidates $\{S_\text{cand\_s}\}$, we restyled them using both the “combined” and “refusal” styles. 
% To achieve the optimal trade-off between factuality and safety, we merged the restyled \textit{factuality} and \textit{safety} candidates into a set $\{S_{\text{cand}}\}$ and employed a hierarchical traversal approach with early pruning~\citep{hua2024assistive} to select three  ICL examples\footnote{To reduce the search space while maintaining a sufficient number of ICL demonstrations, and to align with the number of ICL examples used in SOTA URIAL method (ensuring a more straightforward comparison in experiments), we set the number of ICL demonstrations to 3.} from $\{S_{\text{cand}}\}$ to construct different demonstration sets. 
% We evaluated the performance of different combinations on the \dataname{} validation dataset, like what has been done in Section~\ref{ssec:causality}. 

% Ultimately, we identified the three best combinations of the ICL examples.
% The first combination consists of three factuality ICL examples restyled with the “combined” style. 
% The second combination includes two factuality ICL examples and one safety example, all restyled using the “combined” style.
% The third combination consists of two factuality ICL examples restyled with the “combined” style and one safety example restyled with the “refusal” style. 
% We refer to these combinations as Restyled In-context-learning Demonstration Examples (RIDE), with the first combination denoted as $\textbf{RIDE}_{\text{f}}$, the second as $\textbf{RIDE}_{\text{fs\_uni}}$, and the third as $\textbf{RIDE}_{\text{fs\_hyb}}$.
% We use these notations in the following sections. The prompts of $\textbf{RIDE}$ series can be found in Appendix~\ref{app:rideprompt_f}, \ref{app:rideprompt_fsuri}, and \ref{app:rideprompt_fshyb}.

% We employed a hierarchical traversal approach to combine three separate ICL examples into sets and identified the ICL example set with the highest $V_{polar}$ as the demonstration for the LLM.
