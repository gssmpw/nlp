Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. 
% Current alignment approaches, including supervised fine-tuning (SFT) and preference optimization (PO), require high-quality annotations and significant training resources. 
Current alignment approaches require high-quality annotations and significant training resources. 
This paper proposes a \textit{low-cost, tuning-free method using in-context learning (ICL)} to enhance LLM alignment.
Through an analysis of high-quality ICL demos, we identified \textbf{style} as a key factor influencing LLM alignment capabilities and explicitly \textbf{restyled} ICL exemplars based on this stylistic framework. 
Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment---\textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}.
We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. 
% Our experiments show that rewritten examples boost alignment, safety, and reasoning across various tasks.
Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the \alpaca{} task (from 4.50 $\to$ 4.60), a 0.22 enhancement on the \dataname{} benchmark (from 4.34 $\to$ 4.56), and a maximum improvement of 0.32 (from 3.53 $\to$ 3.85) on the \mtbench{} dataset.
We release the code and data at~\url{https://github.com/AnonymousCode-ComputerScience/RIDE}.