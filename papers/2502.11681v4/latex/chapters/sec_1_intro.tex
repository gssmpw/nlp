\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.95\linewidth]{latex/figure/RIDE_illustration.png} 
    \caption{Overview of our framework for constructing the optimal ICL exemplar set, designated as \textbf{R}estyled \textbf{I}n-context-learning \textbf{D}emonstration \textbf{E}xemplars (\textbf{RIDE}), to achieve \textit{In-Context Alignment} (ICA).} 
    \label{figure:ride_illustration}
\end{figure*}

Alignment tuning helps bridge the gap between raw model capabilities and the nuanced requirements of different tasks, such as delivering accurate information, maintaining user safety, and handling sensitive topics with care~\citep{shneiderman2020bridging,wang2023aligning,DBLP:conf/iclr/Qi0XC0M024}. 
% shen2023large 
The mainstream alignment tuning methods, such as supervised fine-tuning and reinforcement learning with human feedback (RLHF), rely on a large mount of annotated data and significant computing resources~\citep{ouyang2022training,sun2023aligning,DBLP:conf/iclr/DaiPSJXL0024,rafailov2024direct,zhou2024lima,wu2024self}. They potentially leading to catastrophic forgetting of pre-trained knowledge~\cite{wang2023far}. In contrast, \textit{In-Context Alignment} (ICA) provides a low-cost, flexible alternative by employing a handful of selected demonstration exemplars for In-Context Learning (ICL), enabling LLMs to align with user intent without changing model parameters~\cite{DBLP:conf/iclr/LinRLDSCB024}.
% (e.g., through reinforcement learning with human feedback, RLHF)
%However, this paradigm requires high-quality annotated data and consumes significant computing resources. 
%If there were a method to improve LLM alignment without modifying model parameters, it could effectively reduce training costs and increase the versatility of the alignment process~\citep{brown2020language}. 

% To this end, we propose an in-context learning (ICL) method, which instructs LLMs to handle downstream tasks using few-shot learning from ICL demonstrations.
% DBLP:journals/corr/abs-2305-14128
The majority of the prior works on ICA investigate selecting demonstration exemplars~\cite{liu2022makes,min2022rethinking,DBLP:conf/icml/Ye0F0K23,peng2024revisiting,DBLP:conf/icml/ChoiL24,DBLP:conf/eacl/WangYW24}, while~\citet{DBLP:conf/iclr/LinRLDSCB024} only utilize three manually designed exemplars with customized styles, referred to as URIAL, across all tasks. These handcrafted ICL exemplars complement each other, achieving a \textit{delicate balance} between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, which effectively enhance LLM alignment capabilities empirically. However, URIAL lacks quantitative analyses to explain why these specific manually crafted ICL demos are effective and what is the impact of each style factor.

%The In-Context Learning (ICL) method, as an online few-shot learning approach, is a tuning-free strategy that effectively circumvents the instruction-following paradigm to address LLM alignment challenges.
%Many ICL demonstration selection methods have been proposed to choose demonstrations that effectively elicit the desired model capabilities~\citep{liu2022makes,min2022rethinking,DBLP:conf/icml/Ye0F0K23,DBLP:journals/corr/abs-2305-14128,peng2024revisiting,DBLP:conf/icml/ChoiL24,DBLP:conf/eacl/WangYW24}
%However, these methods are not suitable for alignment tuning due to its unique nature. 

In addition to styles, what are the other key factors may influence the selection and combination of ICL exemplars? \citet{zhao2024context} identify the importance of the source of ICL exemplars, while \citet{zhou2024lima} investigate the impact of labels, input-label mappings, and distribution of inputs. Moreover, ICA seems to impose two conflicting demands: on one hand, LLMs need to provide more in-depth, informative, and helpful content (\textbf{\color{myblue} factuality})~\citep{shen2023large}; on the other hand, for safety reasons, LLMs must refuse to answer inappropriate queries (\textbf{\color{myred} safety})~\citep{ji2024beavertails}. Balancing these factors is crucial to effectively leveraging ICL exemplars.

%Thus, we need to achieve a delicate \textbf{trade-off} between these two abilities~\citep{DBLP:journals/corr/abs-2404-09932}, as improving the harmlessness of an LLM assistant may result in it being less helpful~\citep{DBLP:journals/corr/abs-2204-05862}.
%This trade-off presents a challenge when selecting demonstrations-the demonstrations must balance both factuality and safety.

% URIAL~\citep{DBLP:conf/iclr/LinRLDSCB024} demonstrated that alignment tuning modifies the probability distribution of token generation, thereby guiding LLMs to produce safer and more reliable content. To leverage this generation distribution shift, URIAL introduced an ICL demo set consisting of three manually crafted exemplars. While these manually designed ICL exemplars effectively enhance LLM alignment, their approach remains somewhat opaque, as it does not provide a clear explanation for why these specific handcrafted ICL demos are effective.



%In this work, we first design a metric for evaluating the quality of ICL demonstrations—value impact—and utilize this metric to rank and retrieve high-quality ICL exemplars from a candidate pool. Through an in-depth analysis of these exemplars, we identify distinctive stylistic features within ICL demos that influence LLM alignment capabilities (Section~\ref{section2}). 
In this work, we conduct the \textit{first} quantitative study to assess the impact of individual style factors. In particular, we select and rank ICL candidates from a candidate tool in terms of a metric, termed \textit{value impact}. Our detailed analysis of these exemplars reveals to what extent distinctive stylistic factors in ICL exemplars influence LLM alignment capabilities (see Section~\ref{section2}).

Based on the insights from our study, we propose to automatically restyle selected ICL demonstrations using an LLM with a customized prompt (Section~\ref{section3}, RQ1).
To address the trade-off between \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, we systematically explore different exemplar combinations while maintaining stylistic consistency across those exemplars (Section~\ref{section3}, RQ2). Through this process, we identify a handful of optimal exemplar combinations in terms of both styles and content across various tasks (Section~\ref{section3}, RQ3). 
As illustrated in Figure~\ref{figure:ride_illustration}, we refer to these optimized sets as \textbf{R}estyled \textbf{I}n-context-learning \textbf{D}emonstration \textbf{E}xemplars (\textbf{RIDE}).

% We conduct a series of experiments to empirically validate the effectiveness of \textbf{RIDE}, demonstrating its capability to improve LLM alignment across multiple dimensions.

In summary, our contributions are three-fold:

\textsc{(i)} Through a systematic analysis of ICL exemplars, we identify specific stylistic factors that improve LLM alignment capabilities. By evaluating these features using the value impact metric, we provide insights into how different styles influence the effectiveness of ICL demonstrations.

\textsc{(ii)} We propose an automatic restyling approach that systematically modifies ICL demonstrations to enhance alignment. By exploring different style configurations, we identify the optimal stylistic composition that balances the trade-off between  \textbf{\color{myblue} factuality} and \textbf{\color{myred} safety}, leading to the development of \textbf{RIDE} as the most effective ICL demo set.

\textsc{(iii)} We conduct a series of experiments across different datasets and LLM models, demonstrating the effectiveness and superiority of our proposed method. The experimental results show that, across the three benchmarks, our method achieves improvements of $2.22\%$, $4.28\%$, and $9.07\%$ compared to the SOTA methods, respectively.