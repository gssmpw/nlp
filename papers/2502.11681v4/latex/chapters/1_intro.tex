% \quad Alignment tuning is essential for ensuring that large language models (LLMs) behave in ways that are safe, ethical, and aligned with human values. 
% It helps bridge the gap between raw model capabilities and the nuanced requirements of different tasks, such as providing accurate information, maintaining user safety, or handling sensitive topics with care. 
% The instruction-following paradigm, which combines supervised fine-tuning (SFT) and preference optimization (e.g., through reinforcement learning with human feedback, RLHF), is widely used in post-training of current models to improve their alignment capabilities. 
% However, such a paradigm requires feeding LLMs with high-quality annotated data and consumes significant hardware resources to complete training. 
% Therefore, this paper proposes a \textit{low-cost, tuning-free method based on in-context learning (ICL) to effectively enhance the alignment capabilities of LLMs}.

% \quad Thanks to the autoregressive nature of model decoding, the tokens generated in the early stages can significantly influence the subsequent generation trajectory. 
% Based on this characteristic, we found that, compared to base models that have not undergone alignment tuning, aligned LLMs adjust the probability distribution of \textit{polarity tokens} in the first few positions during decoding. 
% These polarity tokens can either induce LLMs to respond positively to malicious questions or guide them toward generating constructive content, reducing harmful outputs and encouraging behavior that is more helpful and contextually appropriate. 
% Building on this discovery, we designed heuristic rules to select high-quality ICL demonstration examples that can elicit changes in the probability distribution of polarity tokens generated by LLMs.

% \quad We encapsulated these high-quality ICL demonstration examples as prompts to trigger few-shot learning in LLMs, thereby improving their alignment capabilities. 
% In addition, we found that the \textit{style} and \textit{content} of the ICL demonstration examples are two critical factors influencing the few-shot learning ability of LLMs. 
% Rewriting the ICL demonstration examples into a unified, structured style effectively elicits LLMs to provide accurate and helpful information. 
% Meanwhile, the content of the ICL demonstration examples facilitates the modelâ€™s ability to refuse to answer malicious questions, thereby enhancing its safety.
 
% \quad We empirically demonstrated that the rewritten demonstration examples enable multiple LLMs to improve their alignment, increase safety, and even enhance reasoning capabilities across different tasks through few-shot ICL learning. 
% Our discoveries regarding the characteristics of alignment tuning and the outcomes with ICL indicate that a more in-depth analysis and theoretical comprehension of alignment are essential for advancing future LLM research.

% Alignment tuning is crucial for ensuring LLMs consistently provide helpful, accurate, reliable information that aligns with human values~\citep{shen2023large,wang2023aligning,DBLP:conf/iclr/Qi0XC0M024}.
%
Alignment tuning helps bridge the gap between raw model capabilities and the nuanced requirements of different tasks, such as delivering accurate information, maintaining user safety, and handling sensitive topics with care~\citep{shneiderman2020bridging,shen2023large,wang2023aligning,DBLP:conf/iclr/Qi0XC0M024}. 
%
Currently, the instruction-following paradigm~\citep{ouyang2022training,sun2023aligning,DBLP:conf/iclr/DaiPSJXL0024,rafailov2024direct,zhou2024lima,wu2024self}, which combines supervised fine-tuning (SFT) and preference optimization, is widely used in alignment tuning. 
% (e.g., through reinforcement learning with human feedback, RLHF)
However, this paradigm requires high-quality annotated data and consumes significant computing resources. 
If there were a method to improve LLM alignment without modifying model parameters, it could effectively reduce training costs and increase the versatility of the alignment process~\citep{brown2020language}. 
To this end, we propose an in-context learning (ICL) method, which instructs LLMs to handle downstream tasks using few-shot learning from ICL demonstrations.

Many ICL demonstration selection methods have been proposed to choose demonstrations that effectively elicit the desired model capabilities~\citep{liu2022makes,min2022rethinking,DBLP:conf/icml/Ye0F0K23,DBLP:journals/corr/abs-2305-14128,peng2024revisiting,DBLP:conf/icml/ChoiL24,DBLP:conf/eacl/WangYW24}, however, these methods are not suitable for alignment tuning due to its unique nature. 

Alignment tuning imposes two conflicting demands on LLMs: on one hand, LLMs need to provide more in-depth, informative, and helpful content (\emph{factuality})~\citep{shen2023large}; on the other hand, for safety reasons, LLMs must refuse to answer inappropriate queries (\emph{safety})~\citep{ji2024beavertails}. 
Thus, we need to achieve a delicate \textbf{trade-off} between these two abilities~\citep{DBLP:journals/corr/abs-2404-09932}, as improving the harmlessness of an LLM assistant may result in it being less helpful~\citep{DBLP:journals/corr/abs-2204-05862}.
This trade-off presents a challenge when selecting demonstrations -- the demonstrations must balance both factuality and safety. 
To address this, we investigate those special tokens that influence the generation trajectory and denote them as \emph{\textbf{polarity}} tokens (Section~\ref{background}). 
Within \emph{polarity} tokens, \textbf{\color{myblue} benign} tokens steer content generation towards helpful and constructive outputs, while \textbf{\color{myred} malicious} tokens can lead the trajectory towards harmful or undesirable outputs. Additionally, we find that the polarity tokens unique to \emph{factuality} and \emph{safety} are entirely different, which further validates that alignment imposes two distinct requirements on LLMs that need to be balanced.

Identifying the \emph{polarity} tokens related to alignment tuning results in a powerful tool. 
By observing changes in the generation probability of polarity tokens, we can approximate whether an ICL demonstration effectively elicits LLM alignment capabilities (Section~\ref{ssec:method}). 
However, we also need to consider how we can further enhance the alignment capability brought by an effective ICL demonstration that alters polarity token generation probabilities.

\textbf{\emph{Content}} (referring to the contextual information contained in a demonstration, including the system instruction and one-shot learning example) and \textbf{\emph{style}} (referring to the format in which the content is organized and written with a specific style and structure) can impact in-context alignment~\citep{DBLP:journals/corr/abs-2406-11474}.
We hypothesize that the \emph{content} and \emph{style} of an ICL example have a \textbf{\emph{causal relationship}} with the alignment effectiveness.
% , i.e. content and style jointly influence alignment. 
We consider \emph{restyling} an ICL demonstration example as an \emph{intervention} and use Average Treatment Effect (ATE) to determine the effect of different restyling methods on alignment. Using ATE, we design a method to learn how to restyle an ICL demonstration example to enhance its effectiveness (Section~\ref{ssec:causality}). 
We then combine these restyled ICL demonstrations to achieve a trade-off between factuality and safety (Section~\ref{ssec:dfs}).

Evaluating our approach across multiple datasets and LLMs (Section~\ref{Evaluation}) reveals that different benchmarks have distinct preferences regarding LLM alignment capabilities, and that the combinations of restyled ICL demonstrations can meet these diverse preferences. 
% Furthermore, our experimental results validate two key points: first, a causal relationship among content, style, and alignment does indeed exist; second, alignment imposes conflicting yet interdependent requirements on LLMs (factuality and safety), which explains why the polarity tokens preferred by factuality and safety are different.
%
In summary, our contributions are three-fold:
\begin{itemize}[leftmargin=10pt]
\item We propose polarity tokens, a class of  tokens that guides the content generation trajectory of LLMs and influences their alignment performance. Specifically, under different alignment requirements, factuality and safety capabilities correspond to distinct polarity tokens. We use changes in the generation probability of polarity tokens as an indicator to retrieve high-quality ICL demonstration examples that can elicit LLM alignment capabilities.
\item We model the content, style, and alignment capabilities of ICL examples as a causal structure and employ ATE method to quantitatively explore the effect of style on alignment, thereby identifying a way to further enhance the quality of ICL demonstrations. We combine the restyled demonstrations to balance factuality and safety, improving the LLMs' overall alignment performance.
\item We conduct a series of experiments across different datasets and LLM models, demonstrating the effectiveness and superiority of our proposed method. The experimental results show that, across the three benchmarks, our method achieves improvements of $2.22\%$, $4.28\%$, and $9.07\%$ compared to the SOTA methods, respectively.
\end{itemize}