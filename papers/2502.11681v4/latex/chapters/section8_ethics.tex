\paragraph{Malicious contents.} This research focuses on improving LLM alignment, which inherently involves handling malicious queries as part of the evaluation process. These queries may contain offensive, harmful, or sensitive content, which could be distressing to some readers. However, we emphasize that such malicious queries are included solely for research purposes, ensuring that our findings contribute to the development of more responsible and safe AI systems.

\paragraph{Data anonymization and Ethical Considerations.} We have taken steps to ensure that no personally identifiable information (PII) or offensive content is present in the datasets used for training and evaluation. Any potentially harmful content within the datasets has been either anonymized or strictly controlled to prevent ethical concerns related to data privacy and misuse. Moreover, the research adheres to responsible AI guidelines, ensuring that the use of existing datasets aligns with their intended purpose, and that any new artifacts created follow the original access conditions.

\paragraph{Intended Use and Research Scope.} Our approach is designed for research purposes only and aims to enhance the alignment capabilities of LLMs. While we propose a novel in-context learning (ICL) method, we acknowledge that misuse or misinterpretation of our approach could lead to unintended consequences. We stress that the techniques introduced should not be used outside of research contexts without proper ethical safeguards. Additionally, our research does not endorse the deployment of LLMs without rigorous safety evaluations, particularly in high-stakes applications.