\newpage
\clearpage
\section{More About Data Collection Process}
\subsection{Participant Recruitment \& Demographics} \label{sec:appendix:recruitment}

We recruited ten graduate students in the computer science department who actively prepared their manuscripts in Overleaf, an online \LaTeX{} editor, and who aimed to submit their manuscripts to peer-reviewed conferences. We held a consent procedure with each participant through a 30-minute virtual meeting remotely.  After the consent process, we installed on their computers a Chrome extension program that we designed and implemented only for this study and asked for the ID number of only the Overleaf projects that the participants agreed to share as their manuscripts. 
We provided all participants with a \$100 Amazon gift card per project, which could be divided among the authors involved in the project. Our data collection process is approved by the IRB of the authors' primary institution. 

All ten participants of our study are graduate students who currently study computer science domain at an accredited university in the United States. Out of the ten, two of them identified themselves as native English speakers, and the remaining participants identified themselves as proficient in English in terms of writing. Also, two of the ten participants attained a Master of Science degree in Computer Science with several publication experiences, and the remaining eight of them are currently PhD students with extensive research experiences. 

\subsection{Technical Details of System Implementations} \label{sec:appendix:system} 

When a key-up event fires in a browser, the extension collects the writer's viewable texts in the code editor panel\footnote{To prevent privacy concerns, the extension filters out keystroke data from any unauthorized Overleaf projects. Please see Appendix \ref{sec:appendix:system} for more details.}. When each of these actions\footnote{Example actions are (1) inserting a space/newline; (2) copy/paste; (3) undo/redo; (4) switching files and (5) scrolling a page.} occurs, the extension uses `\texttt{diff\_match\_patch}' package\footnote{https://github.com/google/diff-match-patch} to generate an array of differences between two subsequent texts (i.e., Figure \ref{fig:system-diff}). Then, the extension will send the array along with metadata (e.g., time stamp, author ID, etc.) to the backend server. 
% To prevent any issue of private data collection, the backend fetches only the IDs of the Overleaf projects that participants consented to share during the recruitment process and filters out participants' keystroke data from any unauthorized projects\footnote{We used Google Sheet API to retrieve ID information that we collected during the recruitment process.}.

For any Overleaf project that consists of multiple LaTeX files, we also collected all keystrokes from subfiles associated with the main LaTeX file. Our comprehensive data collection process captures the end-to-end writing processes of the participating authors across all parts of Overleaf projects. This approach ensures that our dataset reflects the full scope of scholarly writing including edits made in auxiliary files such as files of each section, appendix, bibliography, etc. 

\begin{figure}[ht!]
    \centering
    % \makebox[\textwidth]
    {\includegraphics[width=0.7\columnwidth]{figures/figure3_diff.pdf}}
    \caption{The array of differences between two subsequent texts, generated by \texttt{diff\_match\_patch}}
    \label{fig:system-diff}
\end{figure}


We explain the technical implementation details of the two systems for the data collection process. For the Chrome extension, we implemented a backend application using Flask and Python and stored all keystroke data in the MongoDB database. 

For the annotation interface, we used HTML/CSS and JavaScript for the client side and Flask for the backend. All data for the annotation interface was retrieved from the MongoDB database used in the Chrome extension system. 

\paragraph{Privacy Concerns}
To prevent any issue of private data collection, we designed the backend of our Chrome extension to fetch only the IDs of the Overleaf projects that participants consented to share during the recruitment process and filter out participants' keystroke data from any unauthorized projects. We used Google Sheet API to retrieve ID information that we collected during the recruitment process.

\subsection{Data Post-Processing}
\label{sec:appendix:postprocess}

For use during the annotation phase, each keystroke entry from the raw collection includes the following fields: (1) a valid file name; (2) a valid writing action that triggered keystroke logging (e.g., copy, paste, typing, etc.); (3) a valid array of differences to enable visualization of writing trajectories; and (4) the line numbers in the Overleaf editor. Data entries annotated with a valid intention label (i.e., labels except `artifact') and having a difference array length of fewer than or equal to $300$ are then used for model training.

Regarding the additional postprocessing for public use, we took the following steps to post-process our data with the annotations to promote the usability of our dataset and prevent any privacy issues. 
% \begin{itemize}
%     \item STEP 1: we only include the keystroke changes, project ID, timeframe information, anonymized author's name (e.g., author1, author2, etc.) from the metadata. 
%     \item STEP 2: we used the `\texttt{diff\_match}' library to only extract the before and after texts. 
%     \item STEP 3: Compare data entries labeled as ``non-informative artifact'' to the previous edits made by the same author. Those artifacts are the outcomes of natural keyboard or mouse activities (e.g., scrolling up/down, etc.). Retain the entry if, after human evaluation, the text differences are determined to be a result of natural writing changes. Otherwise, remove those entries. 
%     \item STEP 4: Remove any private author information such as names, affiliations,  contact information, and any personally identifiable information (PID) from the collected keystrokes. 
% \end{itemize}
For the annotation data, we only include the keystroke changes, anonymized project ID, timeframe information, and anonymized author's name (e.g., 0, 1, 2, etc.) from the metadata. Then, we extract the before and after texts from the differences array. We also include the annotated intention label for each entry.


% Then, a span of keystrokes with multiple intention annotations will be split into separate entries, each with a single intention. 
Then, we analyzed any `artifact' generated due to natural keyboard/mouse activities or user switching files, and we discarded them as they are not informative to any writing intention in our taxonomy. 
% However, we kept those entries only for certain cases that are a result of natural writing changes (e.g., repetitively inserting/deleting newlines, scrolling up and down a page, etc.). 
Lastly, to prevent any privacy issues we removed keystrokes containing any private author information such as names, affiliations,  contact information, and any personally identifiable information (PID) from the collected keystrokes. Instead,  we replaced those with an arbitrary command (e.g., `\texttt{\textbackslash anonymous}').

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{More About Writing Intention Annotations \& Taxonomy} \label{sec:appendix:annotation}

\subsection{Annotator Recruitment}

Due to privacy concerns, we did not hire external freelancers with expertise, rather the two corresponding authors of this paper annotated the data are graduate students who possess extensive scholarly writing experiences in natural language processing and data annotation skills. The raw keystroke data collected by our Chrome extension could potentially contain personally identifiable information, such as specific content edits or metadata that could reveal the identity of the authors. To ensure the confidentiality and ethical handling of sensitive information, we restricted access to the data to the authors only. This annotation process was also authorized by the IRB of the authors’ institution. Please note that the final dataset which will be released publicly is ensured not to contain any PII information through several sophisticated post-processing steps. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table*}[h!]
% \centering
% \footnotesize
% % \resizebox{2\columnwidth}{!}{
% \begin{tabular}{@{}p{0.5cm}@{\hskip 1mm}@{}p{1.4cm}p{5.8cm}p{6.8cm}@{\hskip 2mm}c@{}}
% \toprule
% \textbf{1st} & \textbf{Intention} & \textbf{Definition} & \textbf{An example action} & \textbf{Prop.}\\
% \midrule

% % (1) Planning
%  % \multirow{3}{*}{\textbf{Planning}} 
% \parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{\textsc{\colorbox{planningcolor}{Planning}}}}}
% &  Idea Generation & Formulate and record initial thoughts and concepts. & 
%     writing down keywords of a paragraph beforehand (e.g., ``\textit{..\%[Comment out] main point: artifacts lack in human subjectivity..}'') & 7.0\% \\
% \cmidrule(r){2-5}
% & {Idea Organization} & Select the most useful materials and demarcate those generated ideas in a visually formatted way.  & Linking the generated ideas into a logical sequence and spacing out between ideas (e.g., ``..\% (1) need diff. stress testing...\%\%[Spacing] (2) exp. setup? '') & 0.5\% \\
% \cmidrule(r){2-5}
% & {Section Planning} & Initially create sections and sub-level structures. & Putting section-related LaTeX commands (e.g., \texttt{\textbackslash section}, \texttt{\textbackslash paragraph}) & 2.2\% \\
% \toprule

% % (2) Implementation
% % \multirow{5}{*}{\textbf{Implementation}} 
% \parbox[t]{2mm}{\multirow{15}{*}{\rotatebox[origin=c]{90}{\textsc{\colorbox{implementationcolor}{Implementation}}}}}
% & {Text Production} & Translate their ideas into full languages, either from the writers' language or borrowed sentences from an external source. & Generating subsequent sentences with the author's own idea (e.g., ``... GPT-4 (OpenAI, 2023) explains the data ... Our approach is built on top of GPT-4...'') & 57.4\% \\
% \cmidrule(r){2-5}
% & {Object Insertion} & Insert visual claims of their arguments (e.g., figures, tables, equations, footnotes, lists) & e.g., \texttt{\textbackslash begin\{figure\}[h] \textbackslash centering \textbackslash includegraphics\{figure\_A.pdf\} \textbackslash end\{figure\}} & 4.6\% \\
% \cmidrule(r){2-5}
% & {Citation Integration} & Incorporate bibliographic references into a document and systematically link these references using citation commands. & Inserting a new BibTeX object in the bibliography file and adding the object name to an existing \texttt{\textbackslash cite\{\}} on the Related Work section & 1.7\% \\
% \cmidrule(r){2-5}
% & {Cross-reference} & Link different sections, figures, tables, or other elements within a document through referencing commands. & Putting a command \texttt{\textbackslash label\{figure-1\}} to a figure and referencing it in the main body by calling \texttt{\textbackslash ref\{figure-1\}} & 1.1\% \\
% \cmidrule(r){2-5}
% & {Macro Insertion} & Incorporate predefined commands or packages into a LaTeX document to alter its formatting. & Putting a \texttt{\textbackslash usepackage\{minted\}} for formatting a LLM prompt & 0.2\% \\
% \toprule

% % (3) Revision
% \parbox[t]{2mm}{\multirow{20}{*}{\rotatebox[origin=c]{90}{\textsc{\colorbox{revisioncolor}{Revision}}}}}
% & {Fluency} & Fix grammatical or syntactic errors in the text or LaTeX commands. & ``We desig{\textcolor{red}{\sout{ining}}}{\textcolor{teal}{ned}} several experiment setups for 
% {\textcolor{teal}{the}} LLM evaluations as described in Figure \texttt{\textbackslash r{\textcolor{teal}{e}f}\{figure-A\}}.'' & 1.4\%  \\
% \cmidrule(r){2-5} 
% & {Coherence} & Logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole.  & ``Each comment was annotated by three different annotators \textcolor{red}{\sout{, which}} \textcolor{teal}{and we} achieved high inter-annotator agreement.'' & 3.3\% \\
% \cmidrule(r){2-5} 
% & {Clarity} & Improve the semantic relationships between texts to be more straightforward and concise. & ``..relevant studies have examined \textcolor{red}{\sout{one of the several textual styles}} \textcolor{teal}{one aspect of texts}, the formality, ....'' & 11.5\% \\
% \cmidrule(r){2-5}
% & {Structural} & Improve the flow of information by modifying the location of texts and objects. & ``We calculate Pearson's $r$ correlation for human alignment \textcolor{teal}{to compare the alignment between lexicon-based preferences and humans' original preferences.} First, we calculated the score from each human participant \textcolor{red}{\sout{to compare the alignment between lexicon-based preferences and humans' original preferences.}}'' & 3.7\% \\
% \cmidrule(r){2-5}
% & {Linguistic Style} & Modify texts with the writer’s writing preferences regarding styles and word choices, etc. & ``We \textcolor{red}{\sout{believe}} \textcolor{teal}{posit} that ...'' & 1.6\% \\
% \cmidrule(r){2-5}
% & {Scientific Accuracy} & Update or correct scientific evidence (e.g., numbers, equations) for more accurate claims. & ``..Pearson's $r$ correlation (\textcolor{red}{\sout{0.78}}\textcolor{teal}{0.68}; \textcolor{teal}{p < 0.01})'' & 0.7\% \\
% \cmidrule(r){2-5}
% & {Visual Formatting} & Modify the stylistic formatting of texts, objects, and citations & \texttt{\textbackslash cite} $\rightarrow$ \texttt{\textbackslash citet}, \texttt{\textbackslash textbf} $\rightarrow$ \texttt{\textbackslash textsc}, etc. & 3.2\% \\
% \bottomrule
% \end{tabular}
% % }
% \caption{The developed taxonomy of Scholarly Writing Process in \textsc{ScholaWrite}}
% \label{table:taxonomy_full}
% \end{table*}




\subsection{Detailed Annotation Process}


% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}[b]{0.4\textwidth}
%         \centering
%         \includegraphics[width=0.9\columnwidth,trim={0 0cm 0 0},clip]{figures/fig4_a_dk.pdf}
%          \caption{The Chrome extension interface (A) on the Overleaf project, where it collects real-time keystrokes in the Overleaf editor (highlighted).}
%         \label{fig:chrome-extension}
%     \end{subfigure}
%     \vspace{2mm}
%     \begin{subfigure}[b]{0.43\textwidth}
%         \centering
%         \includegraphics[width=0.9\columnwidth,trim={0 0cm 0 0},clip]{figures/fig4_b_dk.pdf}
%         \caption{Annotation interface. During the annotation stage, annotators can click a viewing mode of the collected keystroke data (B). By right-clicking to navigate the timeline of keystroke trace in the interactive panel on the right side (E), annotators can choose an intention label under the drop-down menu (C). They can also view the meta-information of each annotated keystroke (D). }
%          \label{fig:annotation-interface}
%     \end{subfigure}
    
%     \caption{Overall characteristics of scholarly writing patterns in the \textsc{ScholaWrite} dataset
%     }
%     \label{fig:system-interface}
% \end{figure*}

\begin{figure}[t!]
    \centering
    % \makebox[\textwidth]
    {\includegraphics[width=\columnwidth]{figures/fig4_a_dk.pdf}}
    \caption{The Chrome extension interface (A) on the Overleaf project, where it collects real-time keystrokes in the Overleaf editor (highlighted). \vspace{-4mm}}
    \label{fig:chrome-extension}
\end{figure}

\begin{figure}[ht!]
    \centering
    % \makebox[\textwidth]
    {\includegraphics[width=\columnwidth]{figures/fig4_b_dk.pdf}}
    \caption{Annotation interface. During the annotation stage, annotators can click a viewing mode of the collected keystroke data (B). By right-clicking to navigate the timeline of keystroke trace in the interactive panel on the right side (E), annotators can choose an intention label under the drop-down menu (C). They can also view the meta-information of each annotated keystroke (D). }
    \label{fig:annotation-interface}
\end{figure}

The two authors (or annotators) collaborated with a cognitive linguist to develop a codebook and review the results of the annotations. Also, the two annotators conducted an iterative open coding approach to identify several unique writing intentions from keystrokes and developed a codebook of intention labels (“ground-truth labels”) within each high-level process (Planning, Implementation, and Revision) based on the findings from \citet{f508427a-e4c0-3d6a-8abf-03a5d21ec6c4, koo2023decoding}. Using this codebook, those annotators re-labeled each span of keystrokes with the corresponding label during the annotation process. 

The annotators were fully informed about all the labels and had complete access to them when annotating each data point. The annotation process for all the labels is the same: First, they view through multiple consecutive data points and identify which high-level label occurs (e.g., Planning, Implementation, or Revision). Once annotators have identified the current high-level label, attempting to identify where it ends. Then, they decide on the low-level label within the high-level label (e.g., idea generation or organization under the Planning stage, etc.). Finally, they identify the interval for low-level labels and annotate data points in the interval with the identified low-level label. If a keystroke does not deliver any insight, then label it as an `artifact.' 

We calculated \textbf{inter-annotator agreement} using the weighted F1 score in a multi-label, multi-class setting, which is suitable for our complex annotation schema involving multiple labels per instance. The weighted F1 score achieved was 0.71, indicating a high level of agreement between the annotators. 







% \begin{center}
% \begin{tabular}{ c|c|c|c } 
%  \hline
%  Project & Added & Deleted & Actions \\
%  \hline
%  1 & 30845 & 29173 & 2842 \\
%  2 & 85617 & 78415 & 3970 \\
%  3 & 32723 & 33953 & 7283 \\
%  4 & 117376 & 113952 & 10458 \\
%  5 & 124560 & 124781 & 16650 \\
%  6 & 342680 & 326055 & 32991 \\
%  \hline
% \end{tabular}
% \end{center}

%\begin{tabular}{l||rrrrr}
%\toprule
%& 1 & 2 & 3 & 4 & 5 \\
%\hline
%\hline
%\midrule
%Words added & 30,845 & 85,617 & 32,723 & 117,376 & 124,560 \\
%Words deleted & 29,173 & 78,415 & 33,953 & 113,952 & 124,781 \\
%Recorded actions & 2,842 & 3,970 & 7,283 & 10,458 & 16,650 \\
%\bottomrule
%\end{tabular}

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{latex/figures/writing_activities/writing_activities_640e22cae918523bcee8ca5e_broad.pdf}
%    \caption{Writing activities for project 1, grouped by high-level process}
%    \label{fig:writing_activity_broad}
%\end{figure}
%
%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{latex/figures/writing_activities/writing_activities_640e22cae918523bcee8ca5e_no_artifact.pdf}
%    \caption{Writing activities for project}
%    \label{fig:writing_activity_no_artifact}
%\end{figure}
%
%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{latex/figures/annotation_timeline.pdf}
%    \caption{Caption}
%    \label{fig:annotation_timeline}
%\end{figure}

% \begin{tabular}{lr}
% \toprule
% Label & average location \\
% \midrule
% Idea Organization & 0.21 \\
% Idea Generation & 0.29 \\
% Citation Integration & 0.30 \\
% Section Planning & 0.35 \\
% Macro Insertion & 0.39\\
% Structural & 0.43 \\
% Object Insertion & 0.46\\
% Coherence & 0.47\\
% Text Production & 0.48\\
% Visual Style & 0.50\\
% Clarity & 0.51 \\
% Scientific Accuracy & 0.51\\
% Fluency & 0.52\\
% Textual Style & 0.52\\
% Artifact & 0.52 \\
% No Label & 0.53\\
% Cross-reference & 0.55\\
% \bottomrule
% \end{tabular}

% \begin{table*}
% \begin{tabular}{llll}
% \toprule
%  & label & min dist label & max dist label \\
% \midrule
% 0 & Citation Integration & Idea Organization & Scientific Accuracy \\
% 1 & Idea Organization & Citation Integration & Scientific Accuracy \\
% 2 & Coherence & Cross-reference & Scientific Accuracy \\
% 3 & Idea Generation & Textual Style & Scientific Accuracy \\
% 4 & Clarity & Coherence & Scientific Accuracy \\
% 5 & Section Planning & Macro Insertion & Text Production \\
% 6 & Object Insertion & Fluency & Scientific Accuracy \\
% 7 & Structural & Idea Generation & Scientific Accuracy \\
% 8 & Textual Style & Idea Generation & Scientific Accuracy \\
% 9 & Fluency & Object Insertion & Scientific Accuracy \\
% 10 & Text Production & Structural & Scientific Accuracy \\
% 11 & Macro Insertion & Section Planning & Scientific Accuracy \\
% 12 & Visual Style & Citation Integration & Scientific Accuracy \\
% 13 & Cross-reference & Coherence & Scientific Accuracy \\
% 14 & Scientific Accuracy & Section Planning & Text Production \\
% \bottomrule
% \end{tabular}
% \caption{Closest and furthest wasserstein distance label for each label}
% \label{tab:w_dist_from_label}
% \end{table*}

%\begin{figure*}
%    \centering
%    \begin{subfigure}{0.4\textwidth}
%        \includegraphics[width=\textwidth]{latex/figures/label_chords/label_flow_project_1.pdf}
%        \caption{First subfigure.}
%        % \label{fig:first}
%    \end{subfigure}
%    \begin{subfigure}{0.4\textwidth}
%        \includegraphics[width=\textwidth]{latex/figures/label_chords/label_flow_project_2.pdf}
%        \caption{First subfigure.}
%        % \label{fig:sec}
%    \end{subfigure}
%    \begin{subfigure}{0.4\textwidth}
%        \includegraphics[width=\textwidth]{latex/figures/label_chords/label_flow_project_3.pdf}
%        \caption{First subfigure.}
%        % \label{fig:third}
%    \end{subfigure}
%    \begin{subfigure}{0.4\textwidth}
%        \includegraphics[width=\textwidth]{latex/figures/label_chords/label_flow_project_4.pdf}
%        \caption{First subfigure.}
%        % \label{fig:fourth}
%    \end{subfigure}
%    \caption{Label flow across project 1 - 4. Project 1 - 2 on top, 3 - 4 on bottom}
%    \label{fig:foobar}
%\end{figure*}



% \ross{consistent color scheme across paper; linghe was colors from viz tool. leave only one sankey, make it clear; remake with just higher level intention}

% \ross{make quantitative measures for commonality across projects for metrics}

%\begin{figure}
%    \centering
%    \includegraphics[width=1\textwidth]{latex/figures/label_flow.pdf}
%    \caption{Flow of labels across project 0}
%    \label{fig:label_flow}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{More About the Taxonomy of Scholarly Writing Process} \label{sec:appendix:taxonomy}





% \begin{table}[ht!]
% \centering
% \footnotesize
% \begin{tabular}{@{}c|c|c|c|c|c @{}} 
%  \toprule
%  Project & 1 & 2 & 3 & 4 & 5\\
%  \midrule \midrule
%  \# Authors (participants) & 1 & 1 & 1 & 1 & 9\\
%  \# Authors (manuscript) & 3 & 4 & 3 & 4 & 18\\
%  \# Writing Activities & 13419 & 5002 & 6589 & 7942 & 26784 \\
%  \# Words added & 18394 & 24779 & 7724 & 14601 & 62140\\
%  \# Words deleted & 14056 & 17194 & 3061 & 10163 & 35125\\
%  % Final word count &  & 12308 & 6672 & 9984 & 32286\\
%  \bottomrule
% \end{tabular}
% \caption{Statistics of writing actions per Overleaf project in \textsc{ScholaWrite}. The rows `\# Authors (participants)' and `\# Authors (manuscript)' represent the number of authors who participated in our research and the total number of authors present in the final manuscript, respectively.
% Note that the authors who participated in projects 1,2,3, and 4 are also the authors of project 5.}
% \label{table:data-stat}
% \end{table}

% \begin{table}[ht!]
% \centering
% \small 
% \begin{tabular}{l|ccccc}
% \toprule
%  & 1 & 2 & 3 & 4 & 5 \\
% \midrule \midrule
% Idea Generation & 88 & 46 & 120 & 315 & 526 \\
% Idea Organization & 37 & 13 & 25 & 9 & 0 \\
% Section Planning & 19 & 41 & 115 & 237 & 193 \\
% \midrule
% Text Production & 1281 & 1242 & 5139 & 4576 & 9305 \\
% Object Insertion & 281 & 128 & 65 & 550 & 627 \\
% Citation Integration & 121 & 64 & 76 & 147 & 83 \\
% Cross-reference & 56 & 70 & 14 & 330 & 143 \\
% Macro Insertion & 7 & 0 & 59 & 34 & 16 \\
% \midrule 
% Fluency & 42 & 66 & 48 & 148 & 137 \\
% Clarity & 190 & 524 & 726 & 1241 & 1289 \\
% Coherence & 76 & 173 & 134 & 200 & 449 \\
% Structural & 146 & 400 & 110 & 266 & 413 \\
% Scientific Accuracy & 1 & 15 & 2 & 25 & 322 \\
% Textual Style & 42 & 34 & 42 & 216 & 238 \\
% Visual Style & 53 & 129 & 43 & 445 & 784 \\
% \bottomrule
% \end{tabular}
% \caption{Distribution of intention labels annotated across all five Overleaf projects}
% \label{fig:label_distribution_all}
% \end{table}

During the \textbf{planning} stage, the writers engage in a process of generating and organizing raw ideas, arguments, or content structures that were not introduced in the previous trajectory. Based on the plan, the writers \textbf{implement} their plan by drafting full sentences and paragraphs and structuring the contents tangibly. At the same time, the writers enter the \textbf{revision} stage by improving the quality of their implemented sentences and LaTeX objects in terms of linguistic styles, format, or information accuracy. Particularly, spans of keystrokes whose intentions involved any changes but did not change the meaning of original texts are classified as \textbf{Revision}. For those edits that show changes in the meaning, we considered them as \textbf{Implementation}. 
Furthermore, if an author repeatedly adds, removes, and revises text back and forth until a sentence is completed, we consider this process as part of \textbf{text production}. Any subsequent changes made to the sentence after it is finished are considered \textbf{revision}. Table \ref{table:taxonomy-full} presents the comprehensive, complete definitions of each intention of end-to-end scholarly writing process, identified from \textsc{ScholaWrite} dataset. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table*}
% \begin{tabular}{l||lrr}
% \toprule
% source & target & Strength & reflexive \\
% \midrule
% \hline
% \hline
% Appendix & Results & 0.50 & False \\
% Bibliography & Intro & 0.90 & True \\
% Data & Main  & 0.50 & False \\
% Exp\textunderscore setup & Methods & 1.00 & True \\
% Experiments & Results & 0.35 & True \\
% Future Work & Experiments & 0.50 & False \\
% Intro & Bibliography & 0.56 & True \\
% Main  & Experiments & 0.31 & False \\
% Methods & Exp\textunderscore setup & 0.50 & True \\
% Related work & Intro & 1.00 & False \\
% Results & Experiments & 0.67 & True \\
% \bottomrule
% %\caption{Figure}
% \end{tabular}
% \caption{File Interconnectedness}
% % \minhwa{put it on appendix}
% \label{table:file_interconnectedness}
% \end{table*}

% \paragraph{Writing (keystroke) activities per step} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\textsc{ScholaWrite} dataset statistics}

Table \ref{table:data-stat} presents the overall statistics of our \textsc{ScholaWrite} dataset. Table \ref{table:label_distribution_all} shows the distribution of intention labels per Overleaf project from \textsc{ScholaWrite}. 

\begin{table}[ht!]
    \centering
    \tiny
    \begin{tabular}{@{}l|c|c|c|c|c @{}} 
     \toprule
     Project & 1 & 2 & 3 & 4 & 5\\
     \midrule \midrule
    \# Authors  & 1 (3) & 1 (4) & 1 (3) & 1 (4) & 9 (18)\\
     \# keystrokes & 14,217 & 5,059 & 6,641 & 8,348 & 27,239 \\
     \# Words added & 17,387  & 23,835 & 7,779 & 12,448 & 57,511\\
     \# Words deleted & 11,739 & 15,158 & 2,308 & 7,621 & 25,853\\
     \bottomrule
    \end{tabular}
    \caption{Statistics of writing actions per Overleaf project in \textsc{ScholaWrite}. `\# Authors' represents the number of authors who participated in our study (with the total number of authors in the final manuscript). 
    } \label{table:data-stat}
\end{table}

\begin{table}[ht!]
\footnotesize
\centering
\begin{tabular}{@{}lp{1pt}rr@{}}
\toprule
Label & & Subsequent label & Probability  \\
\midrule
\colorbox{planningcolor}{Idea Generation} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production} & 0.52 \\
\colorbox{planningcolor}{Idea Organization} & $\rightarrow$ & \colorbox{planningcolor}{Idea Generation} & 0.34  \\
\colorbox{planningcolor}{Section Planning} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production} & 0.33  \\
\midrule
\colorbox{implementationcolor}{Text Production} & $\rightarrow$ & \colorbox{revisioncolor}{Clarity} & 0.20\\
\colorbox{implementationcolor}{Object Insertion} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production} & 0.32 \\
\colorbox{implementationcolor}{Citation Integration} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production} & 0.37 \\
\colorbox{implementationcolor}{Cross-reference} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production} & 0.36  \\
\colorbox{implementationcolor}{Macro Insertion} & $\rightarrow$ & \colorbox{planningcolor}{Idea Generation} & 0.29 \\
\midrule
\colorbox{revisioncolor}{Fluency} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production}  & 0.30 \\
\colorbox{revisioncolor}{Coherence} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production}  & 0.34  \\
\colorbox{revisioncolor}{Clarity} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production}  & 0.35  \\
\colorbox{revisioncolor}{Structural} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production}  & 0.27  \\
\colorbox{revisioncolor}{Linguistic Style} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production}  & 0.29 \\
\colorbox{revisioncolor}{Scientific Accuracy} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production}  & 0.34 \\
\colorbox{revisioncolor}{Visual Formatting} & $\rightarrow$ & \colorbox{implementationcolor}{Text Production} & 0.25 \\
\bottomrule
\end{tabular}
\caption{Probability of inter-connections between writing intentions in \textsc{ScholaWrite}. For example, in 34\% of instances where an author engaged in ``Idea Organization,'' the subsequent intention was ``Idea Generation.'' }
\label{table:flow-intention-full}
\end{table}

\begin{table}[ht!]
    \centering
    \footnotesize
    \begin{tabular}{@{}l|ccccc@{}}
        \toprule
         & 1 & 2 & 3 & 4 & 5 \\
        \midrule \midrule
        Idea Generation & 515 & 130 & 116 & 309 & 3255 \\
        Idea Organization & 0 & 45 & 25 & 9 & 231 \\
        Section Planning & 182 & 57 & 111 & 201 & 773 \\
        \midrule
        Text Production & 9267 & 2438 & 5109 & 4478 & 14031 \\
        Object Insertion & 583 & 383 & 62 & 486 & 1300 \\
        Cross-reference & 141 & 112 & 13 & 292 & 458 \\
        Citation Integration & 75 & 151 & 69 & 127 & 245 \\
        Macro Insertion & 16 & 7 & 51 & 29 & 33 \\
        \midrule 
        Linguistic Style & 233 & 75 & 42 & 201 & 411 \\
        Coherence & 422 & 242 & 126 & 193 & 1021 \\
        Clarity & 1249 & 645 & 721 & 1180 & 3301 \\
        Scientific Accuracy & 307 & 15 & 2 & 24 & 95 \\
        Structural & 359 & 506 & 105 & 257 & 1042 \\
        Fluency & 116 & 90 & 46 & 135 & 476 \\
        Visual Formatting & 752 & 163 & 43 & 427 & 567 \\
        \bottomrule
        \end{tabular}
    \caption{Distribution of intention labels annotated across all five Overleaf projects.} \label{table:label_distribution_all}
\end{table}

Figures \ref{fig:dist-to-uni-all} to \ref{fig:writing-step-detailed-all} show several characteristics of human writing process, analyzed from \textsc{ScholaWrite dataset}: (1) Figure \ref{fig:dist-to-uni-all} -  the average Wasserstein distance between each intention distribution and uniform distribution; (2) Figure \ref{fig:label-dist-all} - distribution of labels over time; (3) Figure \ref{fig:writing-step-broad-all} for high-level intention distribution over time; and (4) Figure \ref{fig:writing-step-detailed-all} for intention-wise writing activity distribution over time. 

\begin{figure*}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/dist_to_uni/project_1_label_w_dist.pdf}
        \caption{Project 1}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/dist_to_uni/project_2_label_w_dist.pdf}
        \caption{Project 2}
        % \label{fig:sec}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/dist_to_uni/project_3_label_w_dist.pdf}
        \caption{Project 3}
        % \label{fig:third}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/dist_to_uni/project_4_label_w_dist.pdf}
        \caption{Project 4}
        % \label{fig:fourth}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/dist_to_uni/project_5_label_w_dist.pdf}
        \caption{Project 5}
        % \label{fig:third}
    \end{subfigure}
    \caption{Wasserstein distance to uniform distribution for each distribution of writing intentions. Orange, Blue, and Purple represent Planning, Implementation, and Revision writing actions, respectively.}
    \label{fig:dist-to-uni-all}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/project_label_distributions/project_1_distributions.pdf}
        \caption{Project 1}
        % \label{fig:first}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/project_label_distributions/project_2_distributions.pdf}
        \caption{Project 2}
        % \label{fig:sec}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/project_label_distributions/project_3_distributions.pdf}
        \caption{Project 3}
        % \label{fig:third}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/project_label_distributions/project_4_distributions.pdf}
        \caption{Project 4}
        % \label{fig:fourth}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/project_label_distributions/project_5_distributions.pdf}
        \caption{Project 5}
        % \label{fig:fifth}
    \end{subfigure}
    \caption{Distribution of labels over time across projects. Orange, Blue and Purple represent Planning, Implementation, and Revision writing actions respectively. The writing actions are sorted in ascending order, top to bottom, according to their distribution mean.}
    \label{fig:label-dist-all}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj0_broad.pdf}
        \caption{Project 1}
        % \label{fig:first}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj1_broad.pdf}
        \caption{Project 2}
        % \label{fig:sec}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj2_broad.pdf}
        \caption{Project 3}
        % \label{fig:third}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj3_broad.pdf}
        \caption{Project 4}
        % \label{fig:fourth}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj4_broad.pdf}
        \caption{Project 5}
        % \label{fig:fifth}
    \end{subfigure}
    \caption{Distribution of high-level intention activities over time. Orange, Blue and Purple represent Planning, Implementation, and Revision writing actions respectively.}
    \label{fig:writing-step-broad-all}
\end{figure*}

\begin{figure*}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj0_intention.pdf}
        \caption{Project 1}
        % \label{fig:first}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj1_intention.pdf}
        \caption{Project 2}
        % \label{fig:sec}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj2_intention.pdf}
        \caption{Project 3}
        % \label{fig:third}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj3_intention.pdf}
        \caption{Project 4}
        % \label{fig:fourth}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/writing_activities/timestamp_proj4_intention.pdf}
        \caption{Project 5}
        % \label{fig:fifth}
    \end{subfigure}
    \caption{Distribution of Per-intention writing activities over time. Orange, Blue and Purple represent Planning, Implementation, and Revision writing actions respectively.}
    \label{fig:writing-step-detailed-all}
\end{figure*}


\begin{figure*}
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/sankey/project1.pdf}
        \caption{Project 1}
        % \label{fig:first}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/sankey/project2.pdf}
        \caption{Project 2}
        % \label{fig:sec}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/sankey/project3_new.pdf}
        \caption{Project 3}
        % \label{fig:third}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/sankey/project4.pdf}
        \caption{Project 4}
        % \label{fig:fourth}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/sankey/project5.pdf}
        \caption{Project 5}
        % \label{fig:fifth}
    \end{subfigure}
    \caption{Sankey diagrams representing the intention flow of each project. Figure (a) to (d) generated from all intentions. Figure (e) generated from first 10K intentions due to computational constraint.}
    \label{fig:writing-sankey-all}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%
\section{More about \textsc{ScholaWrite} Evaluation}\label{sec:appendix:model}

\subsection{Training environments}

\paragraph{BERT \& RoBERTa} We fine-tuned BERT and RoBERTa with the following hyperparameter setups: (1) a learning rate of $2e^{-5}$; (2) training batch size per device of 8; (3) evaluation batch size per device of 8; (4) the number of training epochs of 10; and (5) a weight decay of $0.01$. For each model, it took approximately 3.5 hours on one NVIDIA RTX A6000. 

\paragraph{Llama} For all experiments, we used baseline models of 4-bit quantized  Llama-8B-Instruct\footnote{\url{https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit}}, using unsloth library\footnote{\url{https://github.com/unslothai/unsloth}}. 


For \textbf{the intention prediction task} (Sec. \ref{sec:eval:predict}), here are the hyperparameter setups for the Llama models: (1) only one epoch of training; (2) a weight decay of 0.01; (3) warm-up steps of 5; (4) learning rate of $2e^{-4}$; and (5) AdamW 8-bit optimizer. Due to computational constraints, we were able to run only one epoch for fine-tuning Llama models on our \textsc{ScholaWrite} dataset. 
For Llama-8B, it took approximately 8 hours on one RTX A5000. 

For \textbf{the `after' text generation subtask} from the iterative self-writing experiment (Sec. \ref{sec:self-writing}), we used the following hyperparameter setups for the fine-tuned Llama-8B-Instruct: (1) only one epoch of training; (2) a weight decay of 0.01; (3) warm-up steps of 10; (4) learning rate of $3e^{-4}$; and (5) AdamW 8-bit optimizer. Due to computational constraints, we were able to run only one epoch for fine-tuning Llama models on our \textsc{ScholaWrite} dataset. Also, it took approximately 12 hours on one NVIDIA L40s. 

\paragraph{GPT-4o} We used the \texttt{GPT-4o-2024-08-06} version for the classification and iterative writing experiments. 
% The classification task on the testing set took around 2.5 hours. 
The iterative writing with 100 iterations took approximately 1 hour on each seed.  

\begin{figure*}
    \centering
    \begin{subfigure}{0.55\textwidth}
        \includegraphics[width=\textwidth]{figures/figure_a.pdf}
        \caption{\textsc{ScholaWrite} Dataset Preparation}
        % \label{fig:first}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/figure_c.pdf}
        \caption{Finetuning Setup}
        % \label{fig:sec}
    \end{subfigure}
    \begin{subfigure}{0.53\textwidth}
        \includegraphics[width=\textwidth]{figures/figure_b.pdf}
        \caption{Iterative Self-writing}
        % \label{fig:third}
    \end{subfigure}
    \caption{Experiment Setup for the iterative self-writing (Sec \ref{sec:self-writing})}
    \label{fig:iteartive-writing-setup-detail}
\end{figure*}

\subsection{Details About Finetuning Process}\label{sec:appendix:finetuning}

\paragraph{Next Intention Prediction (Sec \ref{sec:eval:predict})} 
The fine-tuning prompt included all possible labels with definitions, task instructions, the ``before-text'' chunk, and the corresponding human-annotated intention label, asking the model to predict the intention label based on the ``before-text''. Differences in prompts were limited to only task instructions (see \S \ref{sec:appendix:prompt} for prompt details). 


To achieve optimal performance while minimizing memory usage, we employed QLoRA \cite{dettmers2024qlora} to fine-tune all linear modules of a 4-bit quantized Llama. During fine-tuning, we utilized the `\texttt{train\_on\_response\_only}' function provided by the \texttt{unsloth} library, which masks the task instructions, intention label definitions, and ``before'' text with -100s. This ensures the model is trained exclusively on the response portion of the fine-tuning prompt (i.e., the predicted intention label), without being influenced by the instructional components of the input. The model was fine-tuned for one epoch with a batch size of 2, 4 gradient accumulation steps, and the AdamW 8-bit optimizer.

\paragraph{Iterative Self-Writing (Sec \ref{sec:self-writing})}
The structure of the ``after'' text differs slightly to help the model learn where and what edits to make. We used the \texttt{diff\_match\_patch} library to generate a word-level difference array between the  ``before'' and ``after'' texts. Special tokens (\texttt{<same>}, \texttt{</same>}, \texttt{<del>}, \texttt{</del>}, \texttt{<add>}, \texttt{</add>}) were added to the tokenizer, and the difference array was converted into text wrapped with these tokens. For example, given a ``before'' text of \texttt{``Bad dog''} and an ``after'' text of \texttt{``Good dog''}, the difference array would be \texttt{[(-1, `Bad'), (1, `Good '), (0, `dog')]}. This is converted into: \texttt{<del>Bad </del><add>Good </add><same>dog</same>}. This transformation was applied only to the ``after'' text, while the ``before'' text remained as plain LaTeX text.

For finetuning, we randomly split the \textsc{ScholaWrite} dataset into training (80\%) and testing (20\%) sets. From each intention label in the test set, we sample up to 300 keystroke entries due to budget constraints. 

For intention prediction, we fine-tune Llama3.1-8B-Instruct on \textsc{ScholaWrite} training set (\textsc{Llama-8B-SW-pred}'') and compare it to baseline models (Llama-8B-Instruct and GPT-4o) from Section \ref{sec:eval:predict}. For ``after-text'' generation, we fine-tune another Llama3.1-8B-Instruct model (``\textsc{Llama-8B-SW-gen}'') using the same dataset, with Llama3.1-8B-Instruct and GPT-4o as baselines. The fine-tuning prompt includes task instructions, a verbalizer from human-annotated labels, and ``before-text.'' While prompts were standardized, task instructions varied by model. 


The fine-tuning prompt included task instructions, a verbalizer derived from human-annotated labels, and the ``before'' text. For fine-tuning, we used QLoRA \cite{dettmers2024qlora} to optimize all linear modules of a 4-bit quantized model while maintaining a small memory footprint. Additionally, the \texttt{embed\_tokens} and \texttt{lm\_head} modules were set as trainable and saved in the final checkpoint. To focus training on the response portion (the ``after'' text), we used the \texttt{train\_on\_response\_only} function, masking the task instructions, verbalizer, and ``before'' text with \texttt{-100s}. This ensures the model learns to generate the "after" text without being influenced by instructional input. The model was trained for one epoch with a batch size of 1, 4 gradient accumulation steps, and the AdamW 8-bit optimizer.

During iterative writing, we performed 100 iterations, treating the model’s output under one intention as a single iteration. If the intention predicted by the classification model (fine-tuned Llama3.1-8B-Instruct, as described in Sec \ref{sec:eval:predict}) matched the current predicted intention, the model was prompted to edit the text again. In this case, the newly generated output was not treated as final output in the iteration, and the iteration did not proceed. We moved to the next iteration only when the intention prediction model generated a different intention label than the previous one.

For model setups, we created three pairs of models for each prediction and generation subtasks as follows: 

\begin{itemize}
    \item \textcolor{magenta}{LLama-8B-SW}: LLama-8B-Instruct fine-tuned on \textsc{ScholaWrite} dataset (Prediction) \& LLama-8B-Instruct fine-tuned on \textsc{ScholaWrite} dataset (Generation), independently
    \item \textcolor{teal}{LLama-8B-Instruct}: Vanilla LLama-8B-Instruct (Prediction) \& Vanilla LLama-8B-Instruct (Generation), independently
     \item \textcolor{violet}{GPT-4o}: GPT-4o inference (Prediction) \& GPT-4o inference (Generation)    
\end{itemize}

Also, due to budget constraints, models had different revision strategies. \textsc{Llama-8B-SW-*} and \textsc{Llama-8B-Instruct} continued revision until the next predicted intention changed. \textcolor{blue}{GPT-4o}, however, moved to the next iteration regardless. We refer to the fine-tuned Llama-8B model as \textcolor{magenta}{Llama-8B-ScholaWrite} (or \textcolor{magenta}{Llama-8B-SW}) and the vanilla model as \textcolor{teal}{Llama-8B-Zero}.


\subsection{Prompt Templates} \label{sec:appendix:prompt}

\subsubsection{Prediction Prompt for Llama-8B-Zero models}

``Here are all the possible writing intention labels:

\begin{itemize}
    \item Idea Generation: Formulate and record initial thoughts and concepts.
    \item Idea Organization: Select the most useful materials and demarcate those generated ideas in a visually formatted way.
    \item Section Planning: Initially create sections and sub-level structures.
    \item Text Production: Translate their ideas into full languages, either from the writers' language or borrowed sentences from an external source. 
    \item Object Insertion: Insert visual claims of their arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.). 
    \item Cross-reference: Link different sections, figures, tables, or other elements within a document through referencing commands. 
    \item Citation Integration: Incorporate bibliographic references into a document and systematically link these references using citation commands. 
    \item Macro Insertion: Incorporate predefined commands or packages into a LaTeX document to alter its formatting. 
    \item Fluency: Fix grammatical or syntactic errors in the text or LaTeX commands.
    \item Coherence: Logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole. 
    \item Structural: Improve the flow of information by modifying the location of texts and objects. 
    \item Clarity: Improve the semantic relationships between texts to be more straightforward and concise. 
    \item Linguistic Style: Modify texts with the writer's writing preferences regarding styles and word choices, etc. 
    \item Scientific Accuracy: Update or correct scientific evidence (e.g., numbers, equations) for more accurate claims. 
    \item Visual Formatting: Modify the stylistic formatting of texts, objects, and citations.
\end{itemize}

Identify the most likely next writing intention of a graduate researcher when editing the following LaTex paper draft. Your output should only be a label from the list above.

\#\# Input: \texttt{{before\_text}} \\
\#\# Output: 
''

\subsubsection{Generation Prompt for Llama-8B-Zero models}

``You are a computer science researcher with extensive experience of scholarly writing. Here, you are writing a research paper in natural language processing using LaTeX languages.

You currently want to \texttt{{``put the verbalizer of the predicted intention label''} (e.g., ``initially create sections and sub-level structures'' if the predicted label was section planning)}. 

Below is the paper you have written so far. Please strictly follow the writing intention given above and insert, delete, or revise at appropriate places in the paper given below.

Your writing should relate to the paper given below. Do not generate text other than paper content. Do not describe the changes you are making or your reasoning.

\#\# Input: \texttt{{before\_text}}
''

\subsubsection{Prediction Prompt for Llama-8B-SW models}

``Here are all the possible writing intention labels:

\begin{itemize}
    \item Idea Generation: Formulate and record initial thoughts and concepts.
    \item Idea Organization: Select the most useful materials and demarcate those generated ideas in a visually formatted way.
    \item Section Planning: Initially create sections and sub-level structures.
    \item Text Production: Translate their ideas into full languages, either from the writers' language or borrowed sentences from an external source. 
    \item Object Insertion: Insert visual claims of their arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.). 
    \item Cross-reference: Link different sections, figures, tables, or other elements within a document through referencing commands. 
    \item Citation Integration: Incorporate bibliographic references into a document and systematically link these references using citation commands. 
    \item Macro Insertion: Incorporate predefined commands or packages into a LaTeX document to alter its formatting. 
    \item Fluency: Fix grammatical or syntactic errors in the text or LaTeX commands.
    \item Coherence: Logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole. 
    \item Structural: Improve the flow of information by modifying the location of texts and objects. 
    \item Clarity: Improve the semantic relationships between texts to be more straightforward and concise. 
    \item Linguistic Style: Modify texts with the writer's writing preferences regarding styles and word choices, etc. 
    \item Scientific Accuracy: Update or correct scientific evidence (e.g., numbers, equations) for more accurate claims. 
    \item Visual Formatting: Modify the stylistic formatting of texts, objects, and citations.
\end{itemize}

Identify the most likely next writing intention of a graduate researcher when writing the following LaTex paper draft. Your output should only be a label from the list above.

\#\# Input: \texttt{{before\_text}} \\
\#\# Output: 
''

\subsubsection{Generation Prompt for Llama-8B-Zero models}

``You are a computer science researcher with extensive experience in scholarly writing. Here, you are writing a research paper in natural language processing using LaTeX.

You currently want to \texttt{{``put the verbalizer of the predicted intention label''} (e.g., ``initially create sections and sub-level structures'' if the predicted label was section planning)}. 

Below is the paper you have written so far. Given the paper information below and the corresponding scholarly writing intention, please revise or add to the text to fulfill this writing intention.

You may insert, delete, or revise text at appropriate places in the given paper.

Please provide a complete output. Do not generate text that is nonsensical or unrelated to the given paper information.

\#\# Input: \texttt{{before\_text}}
''


\subsubsection{Classification Prompt for GPT-4o}

``You are a classifier that identify the most likely next writing intention. You will be given a list of all possible writing intention labels with definitions, and an in-progress LaTex paper draft written by a graduate student. Please strictly follow user's instruction to identify the most likely next writing intention. 

Here are the verbalizers of all the possible writing intention labels:

\begin{itemize}
    \item Idea Generation: Formulate and record initial thoughts and concepts.
    \item Idea Organization: Select the most useful materials and demarcate those generated ideas in a visually formatted way.
    \item Section Planning: Initially create sections and sub-level structures.
    \item Text Production: Translate their ideas into full languages, either from the writers' language or borrowed sentences from an external source. 
    \item Object Insertion: Insert visual claims of their arguments (e.g., figures, tables, equations, footnotes, itemized lists, etc.). 
    \item Cross-reference: Link different sections, figures, tables, or other elements within a document through referencing commands. 
    \item Citation Integration: Incorporate bibliographic references into a document and systematically link these references using citation commands. 
    \item Macro Insertion: Incorporate predefined commands or packages into a LaTeX document to alter its formatting. 
    \item Fluency: Fix grammatical or syntactic errors in the text or LaTeX commands.
    \item Coherence: Logically link (1) any of the two or multiple sentences within the same paragraph; (2) any two subsequent paragraphs; or (3) objects to be consistent as a whole. 
    \item Structural: Improve the flow of information by modifying the location of texts and objects. 
    \item Clarity: Improve the semantic relationships between texts to be more straightforward and concise. 
    \item Linguistic Style: Modify texts with the writer's writing preferences regarding styles and word choices, etc. 
    \item Scientific Accuracy: Update or correct scientific evidence (e.g., numbers, equations) for more accurate claims. 
    \item Visual Formatting: Modify the stylistic formatting of texts, objects, and citations.
\end{itemize}

Identify the most likely next writing intention of a graduate researcher when editing the following LaTex paper draft. Your output should only be a label from the list above.

\#\# Input: \texttt{{before\_text}} \\
\#\# Output: 
''

\subsubsection{Generation Prompt for GPT-4o}

``
You are a computer science researcher with extensive experience of scholarly writing. Here, you are writing a research paper in natural language processing using LaTeX languages.

Your writing intention is to \texttt{{``put the verbalizer of the predicted intention label''} (e.g., ``initially create sections and sub-level structures'' if the predicted label was section planning)}. 

Below is the paper you have written so far. Please strictly follow the writing intention given above and insert, delete, or revise at appropriate places in the paper given below.

Your writing should relate to the paper given below. Do not generate text other than paper content. Do not describe the changes you are making or your reasoning. Do not include sidenotes. Your output should only be the paper draft in latex, without the ```latex delimiters.

\#\# Input: \texttt{{before\_text}} \\
\#\# Output: 
''



% \subsubsection{Classification Prompt for GPT-4o in Section \ref{sec:self-writing}}

% \subsubsection{Generation Prompt for GPT-4o in Section \ref{sec:self-writing}}


% \begin{lstlisting}
% Here are all the possible intention labels:

% Idea Generation: This refers to the preliminary stage where the writer formulates and records initial thoughts and concepts before they are fully developed in the text.
% Idea Organization: The writer selects the most useful material retrieved by the generating process and organizes them into writing plans. These may include what topics the author wants to address as well as the sequence of topics. Now, this intention involves a structure of the generated ideas.
% Section Planning: The writer now chooses to set up and initially create sections and other structures (e.g., paragraphs) of their paper, based on those organized ideas.
% Text Production: The writer completes their plans of generated ideas into full sentences. It could be either from the writer's own language or borrowed sentences from the external source that helps the writer translating their ideas into visible languages.
% Object Insertion: The writer incorporates and customizes pre-defined blocks of Latex commands within \begin and \end, within a document to achieve specific formatting of visual claims (e.g., figures, tables, equations, footnotes, bullet points etc.) of their paper.
% Cross-reference: The writer links different sections, figures, tables, or other elements within a document through referencing labels. This method allows readers to navigate between related parts of the document efficiently and understand the interconnections among the document's content.
% Citation Integration: The writer incorporates bibliographic references (BibTeX objects) into a document and systematically linking these references within the text using citation commands.
% Macro Insertion: The writer incorporates predefined commands or packages into a LaTeX document to expand its capabilities or alter its formatting. These commands are not used for core Latex environment.
% Fluency: This is to fix grammatical or syntactic errors in the text or commands, such as grammar, spelling, and punctuation. Note that there is no meaning changed during this intention.
% Coherence: This is to make any of the two or multiple sentences within the same paragraph, or any two subsequent paragraphs more cohesive, or removing objects to make contents logically linked and consistent as a whole. This also refers to the deliberate structuring and editing of text or objects to ensure that ideas are logically connected and presented in a clear, concise, and consistent manner. Note that there is no meaning changed during this intention.
% Structural: The writer attempts to modify the structure of the paper's contents (texts or objects), by relocating texts and objects to improve the flow of the information.
% Clarity: This is to make the text more straightforward, concise, readable and understandable.
% Linguistic Style: Convey the writer's writing preferences, including emotions, tone, voice, etc.
% Scientific Accuracy: Update or correct scientific evidence (e.g., numeric results, equations) to deliver more accurate claims of their paper. Note that because new information is added, meaning is changed during this phase.
% Visual Formatting: Change the stylistic formatting of texts, the figures and tables, etc. that are the objects., except for the case that you move them around the paper. Changing the style formatting of citations also belongs here (e.g., \cite -> \citet).

% Identify the most likely next writing intention of a graduate researcher when editing the following text. Your output should only be a label.
% \end{lstlisting}
% \vspace{-3mm}
% \captionof{lstlisting}{The prompt for GPT4o in solely classification task and iterative writing task}

% \begin{lstlisting}
% _type: prompt
% input_variables:
%   [persona_definition]
% template:
%   "You are a graduate researcher writing an NLP paper in LaTex. 

%   You currently want to {persona_definition}

%   Below is the paper you have written so far. You can insert, delete, or revise at anywhere you want, but only one word or phrase at a time. Please give a complete output. Do not generate text other than paper content"
% \end{lstlisting}
% \vspace{-3mm}
% \captionof{lstlisting}{The prompt for GPT4o generation in iterative writing}


%-------------------------------------------------------------
% \begin{lstlisting}
% def generate_train_template(before_text, label):
%     data_prompt = f”Classify the latex text into one of the following writing intentions: {‘, ‘.join(str(x) for x in RELEVANT_CLASSES)} and return only the label as the answer. ### text: {before_text}”
%     return [
%     {“role”: “user”, “content”: data_prompt},
%     {“role”: “assistant”, “content”: label}
%     ]
% \end{lstlisting}
% \vspace{-3mm}
% \captionof{lstlisting}{The prompt for Llama3.2-3B on fine-tuning writing intention prediction}

% \begin{lstlisting}
% def generate_test_prompt(before_text):
%     data_prompt = f”Classify the latex text into one of the following writing intentions: {‘, ’.join(str(x) for x in RELEVANT_CLASSES)} and return only the label as the answer. ### text: {before_text}”
%     return [
%     {“role”: “user”, “content”: data_prompt}
%     ]
% \end{lstlisting}
% \vspace{-3mm}
% \captionof{lstlisting}{The prompt for Llama3.2-3B on predicting writing intention in iterative writing task}


% \begin{lstlisting}
% def generate_test_template(before_text, writing_intention):
%     prompt = f”“”Given an excerpt from a research paper and a scholarly writing intention, revise or add to the text to fulfill this writing intention. ### exerpt: {before_text} ### Writing intention: {writing_intention}“”"
%     return [
%       {“role”: “user”, “content”: prompt},
%     ]
% \end{lstlisting}
% \vspace{-3mm}
% \captionof{lstlisting}{The prompt for Llama3.2-3B on generating after in iterative writing task}

%-------------------------------------------------------------




% \begin{itemize}
% \item \textbf{Accuracy} stands out as a pivotal criterion that assesses the precision and correctness of the generated text. It involves scrutinizing the extent to which the language model produces information that aligns with factual knowledge, avoiding errors and inaccuracies.
% \item \textbf{Relevance} focuses on the appropriateness and significance of the generated content. This criterion examines how well the text addresses the given context or query, ensuring
% that the information provided is pertinent and directly applicable.
% \item \textbf{Fluency} assesses the language model’s ability to produce content that flows smoothly,
% maintaining a consistent tone and style. A fluent text is not only grammatically correct but
% also ensures readability and a seamless user experience. Analysts evaluate how well the
% model avoids awkward expressions and abrupt shifts in language or topic, contributing to
% effective communication with users.
% \item \textbf{Transparency} delves into the clarity and openness of the language model’s decisionmaking process. It involves assessing how well the model communicates its thought processes, enabling users to understand how and why certain responses are generated. A transparent model provides insights into its inner workings.
% \item \textbf{Safety} emerges as a critical criterion concerned with the potential harm or unintended
% consequences arising from the generated text. It examines the language model’s ability to
% avoid producing content that may be inappropriate, offensive, or harmful, ensuring the wellbeing of users and avoiding misinformation.
% \item \textbf{Human alignment} assesses the degree to which the language model’s output aligns with
% human values, preferences, and expectations. It considers the ethical implications of the
% generated content, ensuring that the language model produces text that respects societal
% norms and user expectations, promoting a positive interaction with human users.
% \end{itemize}


% Based on annotation data. First, we split a span of keystrokes with multiple intention annotations into separate entries, each with a single intention. Also, we split a span of keystrokes that involved multiple edits into separate entries. Then, we analyzed any artifact generated due to the `\texttt{diff\_match}' library and discarded them as they are not informative to any writing action. However, we kept those entries only for certain cases that are a result of natural writing changes (e.g., scrolling up and down a page, etc.). Lastly, to prevent any privacy issues we removed keystrokes containing any author information, such as name, affiliations, and contact information, and replaced those with an arbitrary command (e.g., `\texttt{\textbackslash author}').


% Here is the description and madeup example of public dataset
% --------------------------------


% \begin{center}
% \begin{tabular}{ |c|c|c|c|c|c } 
%  \hline
%  Project & timestamp & author & before text & after text & label\\
% 1 & 1702958491535 & author1 & {Experiment Setup} In our analysis o... & {Experiment Setup} In our analysis o... & Text Production\\
% 3 & 1707351788089 & author2 & This work sets out to contribute to this under... & This work sets out to contribute to this under... & Idea generation \\
% 2 & 1705458640613 & author4 & One important aspect of studying LLM-generated... & One important aspect of studying LLM-generated... & Coherence\\
% ... & ... & ... & ... & ... & ...\\
%  \hline
% \end{tabular}
% \end{center}
% --------------------------------

\subsection{Seed Documents for Iterative Self-Writing}

We present the remaining three seed documents that we used for the iterative self-writing experiments, as shown in Listings \ref{table:seed-entry-johnny} to \ref{table:seed-entry-latxa}.

\begin{table}[ht!]
\centering
\begin{minipage}{\linewidth}
\lstset{
    basicstyle=\ttfamily\footnotesize, % Font and size for the code
    breaklines=true, % Allows breaking of long lines
    frame=single, % Adds a frame around the code block
    columns=fullflexible, % Makes the content fit the column width
    captionpos=b % Places the caption at the bottom
}
\begin{lstlisting}

\begin{document}
\maketitle

\title{How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs}
\author{}
\date{}

\begin{abstract}
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As \textit{large language models} (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then we apply the taxonomy to automatically generate interpretable \textit{persuasive adversarial prompts} (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental mitigation for highly interactive LLMs.
\end{abstract}

\end{document}

\end{lstlisting}
\vspace{-3mm}
\captionof{lstlisting}{An example seed document \cite{zeng-etal-2024-johnny} as shown in LaTeX codes to begin iterative self-writing.}
\label{table:seed-entry-johnny}
\end{minipage}
\end{table}

\begin{table}[h!]
\centering
\begin{minipage}{\linewidth}
\lstset{
    basicstyle=\ttfamily\footnotesize, % Font and size for the code
    breaklines=true, % Allows breaking of long lines
    frame=single, % Adds a frame around the code block
    columns=fullflexible, % Makes the content fit the column width
    captionpos=b % Places the caption at the bottom
}
\begin{lstlisting}
\begin{document}
\maketitle

\title{Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision}
\author{}
\date{}

\begin{abstract}
Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, \textit{iterative} in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, $\mathcal{R}$ead, $\mathcal{R}$evise, $\mathcal{R}$epeat (\textsc{$\mathcal{R}3$}), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In \method, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that \method can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. 
\end{abstract}
\end{document}

\end{lstlisting}
\vspace{-3mm}
\captionof{lstlisting}{An example seed document \cite{du-etal-2022-read} as shown in LaTeX codes to begin iterative self-writing.}
\label{table:seed-entry-read}
\end{minipage}
\end{table}



\begin{table}[ht!]
\centering
\begin{minipage}{\linewidth}
\lstset{
    basicstyle=\ttfamily\footnotesize, % Font and size for the code
    breaklines=true, % Allows breaking of long lines
    frame=single, % Adds a frame around the code block
    columns=fullflexible, % Makes the content fit the column width
    captionpos=b % Places the caption at the bottom
}
\begin{lstlisting}
\begin{document}
\maketitle

\title{Semisupervised Neural Proto-Language Reconstruction}
\author{}
\date{}

\begin{abstract}
Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists' comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.
\end{abstract}

\end{document}

\end{lstlisting}
\vspace{-3mm}
\captionof{lstlisting}{An example seed document \cite{lu-etal-2024-semisupervised} as shown in LaTeX codes to begin iterative self-writing.}
\label{table:seed-entry-semi}
\end{minipage}
\end{table}

\begin{table}[ht!]
\centering
\begin{minipage}{\linewidth}
\lstset{
    basicstyle=\ttfamily\footnotesize, % Font and size for the code
    breaklines=true, % Allows breaking of long lines
    frame=single, % Adds a frame around the code block
    columns=fullflexible, % Makes the content fit the column width
    captionpos=b % Places the caption at the bottom
}
\begin{lstlisting}
\begin{document}
\maketitle

\title{Latxa: An Open Language Model and Evaluation Suite for Basque}
\author{}
\date{}

\begin{abstract}
We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters.
Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.
\end{abstract}

\end{document}

\end{lstlisting}
\vspace{-3mm}
\captionof{lstlisting}{An example seed document \cite{etxaniz-etal-2024-latxa} as shown in LaTeX codes to begin iterative self-writing.}
\label{table:seed-entry-latxa}
\end{minipage}
\end{table}



\subsection{Definition of Quantitative Metrics for Iterative Self-Writing}

\begin{itemize}
    \item \textit{Lexical diversity}: Assess the unique tokens model generated in the final iteration of writing, measured by the number of unique tokens divided by the total tokens generated. 
    \item \textit{Topic consistency}: Cosine similarity between the seed document and output from the final iteration of writing. 
    \item \textit{Intention coverage}: Assess the diversity of the model's writing intention, measured by the number of unique labels predicted through the entire 100 iterations divided by all 15 intended labels available in our taxonomy.
\end{itemize}

% (1) \textit{Lexical diversity}: Assess the unique tokens model generated in the final iteration of writing, measured by number of unique tokens divided by total tokens generated.
% (2) \textit{Topic consistency}: Cosine similarity between the seed document and output from the final iteration of writing 
% (3) \textit{Intention coverage}: Assess the diversity on model's writing intention, measured by number of unique label predicted through entire 100 iteration divided by all 15 intention labels available 


\begin{figure*}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/macro_insertion.png}
        \caption{Llama writing inference for Macro Insertion activity. The model successfully added several LaTeX commands for custom actions.}
        \label{fig:llama_iter_sub_a}
    \end{subfigure}
    \hspace{0.05\textwidth}
    % \par\bigskip
    % \par\bigskip
    % \begin{subfigure}{0.4\textwidth}
    %     \includegraphics[width=\textwidth]{latex/figures/llama_iterative_inference/llama_good_inference_object_3.png}
    %     \caption{Llama writing inference for Object Insertion. Model Successfully added an object.}
    %     \label{fig:llama_iter_sub_c}
    % \end{subfigure}
    % \hspace{0.05\textwidth}
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/idea_generation.png}
        \caption{Llama writing inference for Idea Generation. The model failed to provide generated ideas and instead deleted abstract.}
        \label{fig:llama_iter_sub_d}
    \end{subfigure}
    \begin{subfigure}{0.43\textwidth}
        \includegraphics[width=\textwidth]{figures/clarity.png}
        \caption{Llama writing inference for Clarity. The model successfully revised words and phrases for clearer delivery.}
        \label{fig:llama_iter_sub_b}
    \end{subfigure}
    \caption{Sample outputs from \textcolor{magenta}{Llama-8B-SW} during the self-writing experiment.}
    \label{fig:llama_iterative_output}
\end{figure*}

\subsection{Iterative Training Sample Outputs}

Figure \ref{fig:llama_iterative_output} presents the sample outputs from the fine-tuned Llama-8B model during the iterative self-writing experiment. The model successfully was able to add several LaTeX commands to put some custom icon images (``Macro Insertion''). Also, it successfully revised several words and phrases in a paragraph for better clarity (``Clarity''). However, it struggled with understanding the definition of ``Idea Generation,'' and the model just deleted all paragraphs instead. 

Table \ref{table:qual-comparison} shows sample model outputs at different iterations (1st, 25th, 51st, and 100th), from the seed document \cite{du-etal-2022-read} as shown in Listing \ref{table:seed-entry-read}.

% \begin{table*}[h!]
% \centering
% \footnotesize
% % \resizebox{2\columnwidth}{!}{
% \begin{tabular}{@{}c@{\hskip 1mm}@{}|p{4.4cm}p{4.4cm}p{4.4cm}@{\hskip 2mm}@{}}
% \toprule
% \textbf{Iter.} & \textbf{\textcolor{magenta}{Llama-8B-SW}} & \textbf{\textcolor{teal}{Llama-8B-Zero}} & \textbf{\textcolor{blue}{GPT-4o}}\\
% \midrule
% 10 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information to avoid information\textcolor{red}{\sout{,}}\textcolor{teal}{-} over\textcolor{red}{\sout{load}}\textcolor{teal}{-claiming} (\textbf{Text Production}) & [\textit{..Generating Experiment Section}] ..with an average acceptance rate of \textcolor{teal}{87.5\% (standard deviation: 3.2\%),..} ... with a reduction of 3\textcolor{red}{\sout{0}}\textcolor{teal}{5}\% in revision time and 2\textcolor{red}{\sout{5}}\textcolor{teal}{8}\% in human effort... (\textbf{Scientific Accuracy}) & [..\textit{Same as the 9th iteration}] \textbackslash usepackage\{booktabs\}

% \textbackslash usepackage\{array\}

% \textcolor{teal}{\textbackslash usepackage\{hyperref\}}...

% \textcolor{red}{\sout{\textbackslash method}} \textcolor{teal}{\textsc{$\mathcal{R}3$}}...

% \textcolor{red}{\sout{\textbackslash method}} \textcolor{teal}{\textsc{$\mathcal{R}3$}}...

% (\textbf{Cross-reference})\\
% \hline
% 25 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information to avoid information overload\textcolor{red}{\sout{,}}\textcolor{teal}{.} (\textbf{Text Production}) & [\textit{..Editing Table}]
% Acceptance rate (\%) \& 75 \& 8\textcolor{red}{\sout{7.5}}\textcolor{teal}{8.2} //
% Revision time (minutes) \& 45 \& 2\textcolor{red}{\sout{9}}\textcolor{teal}{8.5}
% Human effort (minutes) \& 60 \& 4\textcolor{red}{\sout{3}}\textcolor{teal}{2} 
% ... (\textbf{Scientific Accuracy}) & [..\textit{Same as the 24th iteration}] The \textcolor{red}{\sout{efficiency}} \textcolor{teal}{...consider the structural flowchart in Figure \textbackslash ref\{fig:system-architecture\}, which outlines...}  

% [\textit{Inserting the figure}]

% \textcolor{teal}{\textbackslash label\{fig:system-architecture\}} (\textbf{Object Insertion})\\
% \hline 
% 51 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information, offering \textcolor{red}{\sout{teach}}\textcolor{teal}{previously trained} to \textcolor{teal}{a} load more related information over the\textcolor{teal}{-} load. (\textbf{Clarity}) & \textcolor{teal}{\textbackslash section\{Impact of the Proposed System\}
% The proposed system, $\mathcal{R}3$, has the potential to impact the writing process in several ways.}... \textcolor{teal}{\textbackslash section\{Future Research Direction\}}... (\textbf{Structural}) & \textbackslash bibitem\{jones2020one\_shot\}
% Jones, L., \textcolor{teal}{\textbf{\textbackslash}}\& Green, D. (2020). 

% \textbackslash bibitem\{brown2021collaboration\}
% Brown, E., \textcolor{teal}{\textbf{\textbackslash}}\& Davis, M. (2021). 

% \textbackslash bibitem\{garcia2021revision\_metrics\}
% Garcia, I., \textcolor{teal}{\textbf{\textbackslash}}\& Lopez, R. (2021). (\textbf{Object Insertion})
% % \textcolor{red}{\sout{\textbackslash cite\{zhang2022iterative\_revisions\}. Refer to Table \textbackslash ref\{tab:acceptance-rate\} for a detailed comparison of acceptance rates}}\textcolor{teal}{, as detailed in Table \textbackslash ref\{tab:acceptance-rate\} \textbackslash cite\{zhang2022iterative\_revisions\}}. [..\textit{Same as 24th iteration}] \textbackslash caption\{The system architecture of \textsc{$\mathcal{R}3$}, outlining the iteration process from user interactions to text finalization, \textcolor{teal}{as elaborated in Section \textbackslash ref{sec:system}}.\} (\textbf{Cross-reference})
% \\
% \hline
% 100 & [..\textit{Same as the 99th iteration}] \textcolor{teal}{\textbackslash end\{document\}} (\textbf{Macro Insertion}) & [..\textit{Same as the 99th }] \textcolor{teal}{\textbackslash usepackage[margin=1in]\{geometry\} \texttt{\% Customizes page margins}} \textcolor{teal}{\textbackslash usepackage\{hyperref\} \texttt{\% Enables hyperlinks}} (\textbf{Fluency}) & [..\textit{Same as the 93th iteration}]
% \textbackslash bibliography\{references\}

% \textbackslash bibliographystyle\{plain\}

% \textcolor{red}{\sout{\% References}}

% \textbackslash begin\{thebibliography\}\{\}
% (\textbf{Cross-reference}) \\ 
% \bottomrule
% \end{tabular}
% \caption{Example model outputs at different iterations, from the seed document \cite{du-etal-2022-read} (Listing \ref{table:seed-entry-read}). 
% \label{table:qual-comparison}}
% \end{table*}




\subsection{Discussion about Next Intention Prediction Results}
According to Table \ref{table:intention-prediction}, none of them is reaching 0.7 in F1. This is likely due to the intricate nature of the task. The model is asked to predict the next intention by only giving the before text. In the annotation task, the annotator labels the data by looking through multiple consecutive before and after text pairs to determine the current intention rather than looking at the current text to predict the next intention. Another reason is that the next intention chosen by the author does not necessarily mean that it is the only correct intention.
We also noticed that BERT and RoBERTa perform much better than the vanilla Llama-8b-Instruct. This is likely due to Llama 8b being under-trained as they have a larger size of parameters, while we fine-tuned it on our data for only one epoch.
% \vspace{-1em}



% \begin{figure*}
%     \centering
%     \begin{subfigure}{0.4\textwidth}
%         \includegraphics[width=\textwidth]{latex/figures/gpt_iterative_inference/gpt_coherence_good.png}
%         \caption{GPT4o writing inference for Coherence. Model successfully improved coherence through revision.}
%         \label{fig:gpt_iter_sub_a}
%     \end{subfigure}
%     \hspace{0.05\textwidth}
%     \begin{subfigure}{0.4\textwidth}
%         \includegraphics[width=\textwidth]{latex/figures/gpt_iterative_inference/gpt_cross_reference_good.png}
%         \caption{GPT4o writing inference for Cross Reference. Model successfully cross-referenced a figure.}
%         \label{fig:gpt_iter_sub_b}
%     \end{subfigure}
%     \par\bigskip
%     \par\bigskip
%     \begin{subfigure}{0.4\textwidth}
%         \includegraphics[width=\textwidth]{latex/figures/gpt_iterative_inference/gpt_object_insertion_good.png}
%         \caption{GPT4o writing inference for Object Insertion. Model Successfully added an object.}
%         \label{fig:gpt_iter_sub_c}
%     \end{subfigure}
%     \hspace{0.05\textwidth}
%     \begin{subfigure}{0.4\textwidth}
%         \includegraphics[width=\textwidth]{latex/figures/gpt_iterative_inference/gpt_citation_integration_bad.png}
%         \caption{GPT4o writing inference for Citation Integration. Model failed to add, edit or integrate a citation in the text.}
%         \label{fig:gpt_iter_sub_d}
%     \end{subfigure}
%     \caption{Sample Outputs from GPT4o Iterative Inference}
%     \label{fig:gpt_iterative_output}
% \end{figure*}


\subsection{Human Evaluation of Iterative Self-Writing Experiment}\label{sec:appendix:human-eval}

\subsubsection{Study Procedures}

Human evaluation was conducted on the outputs of two models: the LLama-8B-Instruct\footnote{`\texttt{Unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'}}(\textcolor{teal}{Llama-8B-Zero}) and its finetuned counterpart (\textcolor{magenta}{Llama-8B-SW}). Each evaluation session lasted approximately two hours and was conducted via a Zoom call. Participants were three graduate students of an R1 University in the United States, with extensive experience in Overleaf-based writing, and they were compensated with a US 40 dollar gift card for their effort.

Before the evaluation began, the author shared their screen to present the following information:

\begin{itemize}
    \item A brief explanation of the research and an overview of the task (e.g., evaluating outputs from two models) using Google Slides.
    \item An explanation of all intention labels and their corresponding definitions in our taxonomy.
    \item A walkthrough of the evaluation web application, including (1) information displayed in the user interface (UI); (2) the locations on the UI where the participants should focus, such as the intention label; and (3) how to move between different seed documents.
    \item A tutorial on how to complete the evaluation of the five metrics (accuracy, alignment, fluency, coherence, relevance) using the provided Google Sheet.
\end{itemize}

During the evaluation, participants were required to share their screen while using the evaluation web application in their browser. The author remained muted but monitored the participants' shared screens and the Google Sheet to ensure the process was proceeding smoothly. The author only interacted with participants to address questions, resolve technical issues, or clarify instructions. No unsolicited interaction was allowed.

The model identities were hidden from the participants. Instead, the models were labeled as ``Model 1'' and ``Model 2.'' Specifically, ``Model 1'' corresponded to the \textcolor{magenta}{Llama-8B-SW}, and ``Model 2'' corresponded to the \textcolor{teal}{Llama-8B-Zero}.
The evaluation web app displayed two text boxes side by side, with an intention label predicted by the respective model shown in the top-left corner of each text box. Please refer to Figure \ref{fig:human-eval-interface} as a screenshot of the user interface that we developed for the human evaluation process. 

After completing the evaluation, the authors thanked the participants for their time and effort, marking the conclusion of the Zoom call.

\subsubsection{Definitions of Evaluation Metrics}

Here are the full description of our human evaluation metrics for the iterative self-writing experiment:

\begin{itemize}
    \item \textit{Accuracy}: Out of 100,  how many is the number of generated outputs that align with the provided intention? 
    \item \textit{Alignment}: Which model's whole writing process throughout the entire 100 iterations looks more human-like behaviors?
    \item \textit{Fluency}: Which model’s final writing sounds more grammatically correct?
    \item \textit{Coherence}: Which model’s final writing sounds more logical? 
    \item \textit{Relevance}: Does the final writing from each model contain related contents to the original seed document? 
\end{itemize}


\subsubsection{Results \& Discussion}

Tables \ref{table:human-eval-seed1} to \ref{table:human-eval-seed4} present the human evaluation of results for each seed document. We observe that the fine-tuned \textcolor{magenta}{Llama-8B-SW} did not outperform the baseline vanilla counterpart (\textcolor{teal}{Llama-8B-Zero}) across all metrics for all four seed settings. 

This discrepancy may be attributed to the way the prompt used for training isolates individual writing actions from the continuous, interconnected process typical of human writing. A single writing action involves referencing the text before making an edit, the text after the edit, and the intention behind the edit. In human writing, these actions are cognitively and logically linked as part of a cohesive sequence. However, our model struggles to capture these connections due to the prompt structure, potentially causing it to become stuck in a local minimum. 

\begin{table*}[h!]
\small 
\centering
\begin{tabular}{@{}p{2cm}c|c|c|c@{}}
\toprule
\textbf{Metrics} & \textbf{Model} & \textbf{Evaluator 1} & \textbf{Evaluator 2} & \textbf{Evaluator 3} \\ \midrule
 \multirow{2}{*}{Accuracy} &  \textcolor{magenta}{SW} & 43 & 3 & 17\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & 47 & 22 & 38\\
\midrule
\multirow{2}{*}{Alignment} &  \textcolor{magenta}{SW} &  &  \\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Fluency} &  \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Coherence} &  \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Relevance} &  \textcolor{magenta}{SW} & Yes & No & No\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & Yes & Yes & Yes\\
\bottomrule
\end{tabular}
\caption{Human evaluation results for the seed document 1.}
\label{table:human-eval-seed1}
\end{table*}

\begin{table*}[h!]
\small 
\centering
\begin{tabular}{@{}p{2cm}c|c|c|c@{}}
\toprule
\textbf{Metrics} & \textbf{Model} & \textbf{Evaluator 1} & \textbf{Evaluator 2} & \textbf{Evaluator 3} \\ \midrule
 \multirow{2}{*}{Accuracy} &  \textcolor{magenta}{SW} & 26 & 0 & 5\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & 48 & 12 & 29\\
\midrule
\multirow{2}{*}{Alignment} &  \textcolor{magenta}{SW} &  &  \\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Fluency} &  \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Coherence} &  \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Relevance} &   \textcolor{magenta}{SW} & Yes & Yes & Yes\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & Yes & Yes & Yes\\
\bottomrule
\end{tabular}
\caption{Human evaluation results for the seed document 2.}
\label{table:human-eval-seed2}
\end{table*}

\begin{table*}[h!]
\small 
\centering
\begin{tabular}{@{}p{2cm}c|c|c|c@{}}
\toprule
\textbf{Metrics} & \textbf{Model} & \textbf{Evaluator 1} & \textbf{Evaluator 2} & \textbf{Evaluator 3} \\ \midrule
 \multirow{2}{*}{Accuracy} &   \textcolor{magenta}{SW} & 52 & 0 & 3\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & 70 & 23 & 43\\
\midrule
\multirow{2}{*}{Alignment} &   \textcolor{magenta}{SW} &  &  \\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Fluency} &   \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Coherence} &   \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Relevance} &   \textcolor{magenta}{SW} & Yes & Yes & No\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & Yes & Yes & Yes\\
\bottomrule
\end{tabular}
\caption{Human evaluation results for the seed document 3.}
\label{table:human-eval-seed3}
\end{table*}

\begin{table*}[h!]
\small 
\centering
\begin{tabular}{@{}p{2cm}c|c|c|c@{}}
\toprule
\textbf{Metrics} & \textbf{Model} & \textbf{Evaluator 1} & \textbf{Evaluator 2} & \textbf{Evaluator 3} \\ \midrule
 \multirow{2}{*}{Accuracy} &  \textcolor{magenta}{SW} & 37 & 3 & 6\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & 60 & 22 & 48\\
\midrule
\multirow{2}{*}{Alignment} &  \textcolor{magenta}{SW} &  &  \\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Fluency} &  \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Coherence} &  \textcolor{magenta}{SW} &  &  &\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & X & X & X\\
\midrule
\multirow{2}{*}{Relevance} &  \textcolor{magenta}{SW} & Yes & No & No\\
\cmidrule(r){2-5}
& {\textcolor{teal}{Zero}} & Yes & Yes & Yes\\
\bottomrule
\end{tabular}
\caption{Human evaluation results for the seed document 4.}
\label{table:human-eval-seed4}
\end{table*}


\begin{figure*}
    \centering
    \makebox[0.33\textwidth]{\small \textcolor{magenta}{Llama-8B-ScholaWrite}}
    \makebox[0.32\textwidth]{\small \textcolor{teal}{Llama-8B-Zero}}
    \makebox[0.32\textwidth]{\small \textcolor{blue}{GPT-4o}} 
    \\[1em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_broad_seed1.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_broad_seed1.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_broad_seed1.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (a) Seed 1 } 
    \\[0.5em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_broad_seed2.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_broad_seed2.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_broad_seed2.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (b) Seed 2 } 
    \\[0.5em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_broad_seed3.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_broad_seed3.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_broad_seed3.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (c) Seed 3 } 
    \\[0.5em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_broad_seed4.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_broad_seed4.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_broad_seed4.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (d) Seed 4 } 
    \\[0.5em]
        % \label{fig:first}
    \caption{Distribution of high-level writing activities over time by models - \textcolor{magenta}{Llama-8B-ScholaWrite} (left); \textcolor{teal}{Llama-8B-Zero} (middle); \textcolor{blue}{GPT-4o} (right). Orange, Blue, and Purple represent Planning, Implementation, and Revision writing actions respectively. }
    \label{fig:writing-step-broad-all-model}
\end{figure*}

\begin{figure*}
    \centering
    \makebox[0.33\textwidth]{\small \textcolor{magenta}{Llama-8B-ScholaWrite}}
    \makebox[0.32\textwidth]{\small \textcolor{teal}{Llama-8B-Zero}}
    \makebox[0.32\textwidth]{\small \textcolor{blue}{GPT-4o}} 
    \\[1em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_detailed_seed1.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_detailed_seed1.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_detailed_seed1.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (a) Seed 1 } 
    \\[0.5em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_detailed_seed2.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_detailed_seed2.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_detailed_seed2.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (b) Seed 2 } 
    \\[0.5em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_detailed_seed3.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_detailed_seed3.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_detailed_seed3.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (c) Seed 3 } 
    \\[0.5em]
        \includegraphics[width=0.32\textwidth]{figures/llama8_SW_output_detailed_seed4.pdf}
        \includegraphics[width=0.32\textwidth]{figures/llama8_meta_output_detailed_seed4.pdf}
        \includegraphics[width=0.32\textwidth]{figures/gpt4o_output_detailed_seed4.pdf}
        \\[2pt]
        \makebox[\textwidth]{\small (d) Seed 4 } 
    \\[0.5em]
        % \label{fig:first}
    \caption{Distribution of Per-intention writing activities over time by models - \textcolor{magenta}{Llama-8B-ScholaWrite} (left); \textcolor{teal}{Llama-8B-Zero} (middle); \textcolor{blue}{GPT-4o} (right). Orange, Blue, and Purple represent Planning, Implementation, and Revision writing actions respectively. We observe different writing patterns by model during the entire 100 iterations. }
    \label{fig:writing-step-intention-all-model}
\end{figure*}





\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/human-eval-interface.pdf}
    \caption{The user interface for the human evaluation process. }
    \label{fig:human-eval-interface}
\end{figure*}