\section{Applications for Writing Assistance}




We envision \textsc{ScholaWrite} as a valuable resource for training language models and improving future writing assistants for scholarly writing. To evaluate its usability, we conducted experiments training LLMs to mimic the complex, non-linear writing processes of human scholars.
% We believe \textsc{ScholaWrite} can serve as a valuable resource for training language models or enhancing the cognitive writing process in future writing assistants for scholarly writing. To evaluate its usability, we performed several experiments where existing LLMs were trained to mimic the complex, non-linear scientific writing processes of human scholars. 
Specifically, we aimed to showcase the capabilities of LLMs trained on \textsc{ScholaWrite} in two scenarios:

\textbf{(1) Predicting an author's next writing intention} (\S\ref{sec:eval:predict}): The task is crucial for writing assistants to accurately assess the writer's current status in context and predict the correct writing intention. This enables them to offer cognitively-appropriate writing suggestions that align with the writer's needs.

\textbf{(2) Iteratively generating scholarly writing actions from scratch (\S\ref{sec:self-writing})} (called Iterative Self-Writing), mirroring the human writing process\label{sec:self-writing-overview}: This task focuses on how well the model trained on our dataset can replicate the actual iterative writing and thinking process of scholars, and whether the generated text achieves higher quality compared to LLM-prompted writing.

% As illustrated in Figure \ref{fig:iterative_model_writing}, the first task (represented by the inner box labeled ``Prediction'') takes the ``before'' text from a keystroke pair (i.e., Listing \ref{table:single-entry}) and context \colorbox{planningcolor}{prompt}, and predicts the writing \colorbox{pink}{intention} to apply for the subsequent actions. Once the intention is predicted, the second task generates the  ``after'' text based on the predicted intention and given \colorbox{BlueGreen}{prompt} (represented by the outer box labeled ``Generation''). This process repeats iteratively until no further changes are made or a maximum number of iterations, such as 100, is reached. 


% We randomly split our dataset into train (80\%) and test (20\%) sets. 
% % To address the budget constraint in GPT4o, 
% For each intention label in the test set, we randomly select up to 300 keystroke entries, due to budget constraints. 
% Note that the same train and test sets are applied to all models across all experiments. See Appendix \ref{sec:appendix:model} for a full description of the model training process. 



% \begin{figure}[t!]
%     \centering\hspace*{-0.2cm}
% \includegraphics[width=0.53\textwidth,trim={1.5cm 0.9cm 0.5cm 0cm},clip]{figures/fig_iter.pdf}
% \vspace{-5mm}
%     \caption{The overview of next writing intention prediction task (Prediction box) and iterative self-writing task setup (the whole pipeline).\vspace{-5mm}}
% \label{fig:iterative_model_writing}
% \end{figure}

\begin{figure}[th!]
    \centering\hspace*{0.2cm}
\includegraphics[width=0.49\textwidth,trim={1.5cm 0.9cm 0.5cm 0cm},clip]{figures/fig_iter.pdf}
\vspace{-8mm}
    \caption{The overview of next writing intention prediction task (Prediction box) and iterative self-writing task setup (the whole pipeline).\vspace{-3mm}}
\label{fig:iterative_model_writing}
\end{figure}

\subsection{Predicting Next Writing Intention}\label{sec:eval:predict}

\paragraph{Setup \& Metrics} 

This task (represented by the inner box ``Prediction'' in Figure \ref{fig:iterative_model_writing}) takes the ``before-text'' from a keystroke pair (i.e., Listing \ref{table:single-entry}) and context \colorbox{planningcolor}{prompt}, and predicts the writing \colorbox{pink}{intention} to apply for the subsequent actions.

We use BERT \cite{devlin2019bert}, RoBERTa \cite{Liu2019RoBERTaAR}, and Llama3.1-8B-Instruct \cite{dubey2024llama} as baselines, fine-tuning each on the \textsc{ScholaWrite} training set. For comparison, we also run GPT-4o \cite{gpt4o} on the test set. To evaluate model performance, we used a weighted F-1 score\footnote{Weighted F-1 was chosen to address skewed label distribution (as shown in Figure \ref{fig:intention-dist} and Table \ref{table:taxonomy-full}).}. See Appendix \ref{sec:appendix:finetuning} for details.

% The fine-tuning prompt included all possible labels with definitions, task instructions, the ``before-text'' chunk, and the corresponding human-annotated intention label, asking the model to predict the intention label based on the ``before-text''. Differences in prompts were limited to only task instructions (see Appendix \ref{sec:appendix:prompt} for prompt details). To evaluate model performance, we used a weighted f-1 score\footnote{Weighted F-1 was chosen to address skewed label distribution (as shown in Figure \ref{fig:intention-dist} and Table \ref{table:taxonomy-full}).}, comparing predictions to gold intention labels on the test set.


\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccccc@{}}
\toprule
 & BERT & RoBERTa & Ll ama-8B & GPT-4o \\ \midrule
Base & 0.04 & 0.02 &  0.12 & 0.08 \\
+ SW & \textbf{0.64} & \textbf{0.64} & \textbf{0.13} & - \\ \bottomrule
\end{tabular}%
}
\caption{Weighted F-1 scores of each baseline and its corresponding fine-tuned model with \textsc{ScholaWrite} (+SW) for the writing intention prediction task. }
\label{table:intention-prediction}
\end{table}


\begin{figure*}[t!]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/lexical_all_no_legend.pdf}
        \vspace{-1em}
        \caption{Lexical Diversity}
        \label{fig:lexical-all}
    \end{subfigure}
    \hspace{-0.5em}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/topic_all_no_legend.pdf}
        \vspace{-1em}
        \caption{Topic Consistency}
        \label{fig:topic-all}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/intention_all_no_legend.pdf}
        \vspace{-1em}
        \caption{Intention Coverage}
        \label{fig:intention-all}
    \end{subfigure}
    \caption{Metric scores of the final writing output of models (\textcolor{magenta}{LLama-8B-SW}, \textcolor{teal}{LLama-8B-Zero}, and \textcolor{blue}{GPT-4o}) after 100 iterations of the iterative self-writing experiment. We observe that our \textcolor{magenta}{Llama-8B-SW} model presents the highest quality of the final output across most of the four seed documents.
    % \dk{add a legend of models in one of the figures above.}
    }
    \label{fig:sec5-auto-all}
\end{figure*}
% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}[b]{0.35\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figures/llama8_SW_output_detailed_seed2.pdf}
%         \vspace{-1.5em}
%         \caption{\textcolor{magenta}{Llama-8B-SW}}
%         \label{fig:llama-sw-timestamp-seed2}
%     \end{subfigure}
%     \hspace{0.5em}
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth,trim={7.2cm 0 0 0},clip]{figures/llama8_meta_output_detailed_seed2.pdf}
%         \vspace{-1.5em}
%         \caption{\textcolor{teal}{Llama-8B-Zero}}
%         \label{fig:llama-zero-timestamp-seed2}
%     \end{subfigure}
%     \hspace{0.5em}
%     \begin{subfigure}[b]{0.25\textwidth}
%         \centering
%         \includegraphics[width=\textwidth,trim={7.2cm 0 0 0},clip]{figures/gpt4o_output_detailed_seed2.pdf}
%         \vspace{-1.5em}
%         \caption{\textcolor{blue}{GPT-4o}}
%         \label{fig:gpt4-timestamp-seed2}
%     \end{subfigure}
%     \caption{Per-intention writing activities over time among different models - (1) \textcolor{magenta}{Llama-8B-SW}; (2) \textcolor{teal}{Llama-8B-Zero}; and (3) \textcolor{blue}{GPT-4o}, from the seed document \cite{du-etal-2022-read}. We observe different writing patterns by model during the entire 100 iterations. 
%     }
%     \label{fig:sec5-timestamp-seed2}
% \end{figure*}

\begin{table*}[h!]
\centering
\footnotesize
% \resizebox{2\columnwidth}{!}{
\begin{tabular}{@{}c@{\hskip 1mm}@{}|p{4.4cm}p{4.4cm}p{4.4cm}@{\hskip 2mm}@{}}
\toprule
\textbf{Iter.} & \textbf{\textcolor{magenta}{Llama-8B-SW}} & \textbf{\textcolor{teal}{Llama-8B-Zero}} & \textbf{\textcolor{blue}{GPT-4o}}\\
\midrule
10 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information to avoid information\textcolor{red}{\sout{,}}\textcolor{teal}{-} over\textcolor{red}{\sout{load}}\textcolor{teal}{-claiming} (\textbf{Text Production}) & [\textit{..Generating Experiment Section}] ..with an average acceptance rate of \textcolor{teal}{87.5\% (standard deviation: 3.2\%),..} ... with a reduction of 3\textcolor{red}{\sout{0}}\textcolor{teal}{5}\% in revision time and 2\textcolor{red}{\sout{5}}\textcolor{teal}{8}\% in human effort... (\textbf{Scientific Accuracy}) & [..\textit{Same as the 9th iteration}] \textbackslash usepackage\{booktabs\}

\textbackslash usepackage\{array\}

\textcolor{teal}{\textbackslash usepackage\{hyperref\}}...

\textcolor{red}{\sout{\textbackslash method}} \textcolor{teal}{\textsc{$\mathcal{R}3$}}...

\textcolor{red}{\sout{\textbackslash method}} \textcolor{teal}{\textsc{$\mathcal{R}3$}}...

(\textbf{Cross-reference})\\
\hline
25 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information to avoid information overload\textcolor{red}{\sout{,}}\textcolor{teal}{.} (\textbf{Text Production}) & [\textit{..Editing Table}]
Acceptance rate (\%) \& 75 \& 8\textcolor{red}{\sout{7.5}}\textcolor{teal}{8.2} //
Revision time (minutes) \& 45 \& 2\textcolor{red}{\sout{9}}\textcolor{teal}{8.5}
Human effort (minutes) \& 60 \& 4\textcolor{red}{\sout{3}}\textcolor{teal}{2} 
... (\textbf{Scientific Accuracy}) & [..\textit{Same as the 24th iteration}] The \textcolor{red}{\sout{efficiency}} \textcolor{teal}{...consider the structural flowchart in Figure \textbackslash ref\{fig:system-architecture\}, which outlines...}  

[\textit{Inserting the figure}]

\textcolor{teal}{\textbackslash label\{fig:system-architecture\}} (\textbf{Object Insertion})\\
\hline 
51 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information, offering \textcolor{red}{\sout{teach}}\textcolor{teal}{previously trained} to \textcolor{teal}{a} load more related information over the\textcolor{teal}{-} load. (\textbf{Clarity}) & \textcolor{teal}{\textbackslash section\{Impact of the Proposed System\}
The proposed system, $\mathcal{R}3$, has the potential to impact the writing process in several ways.}... \textcolor{teal}{\textbackslash section\{Future Research Direction\}}... (\textbf{Structural}) & \textbackslash bibitem\{jones2020one\_shot\}
Jones, L., \textcolor{teal}{\textbf{\textbackslash}}\& Green, D. (2020). 

\textbackslash bibitem\{brown2021collaboration\}
Brown, E., \textcolor{teal}{\textbf{\textbackslash}}\& Davis, M. (2021). 

\textbackslash bibitem\{garcia2021revision\_metrics\}
Garcia, I., \textcolor{teal}{\textbf{\textbackslash}}\& Lopez, R. (2021). (\textbf{Object Insertion})
% \textcolor{red}{\sout{\textbackslash cite\{zhang2022iterative\_revisions\}. Refer to Table \textbackslash ref\{tab:acceptance-rate\} for a detailed comparison of acceptance rates}}\textcolor{teal}{, as detailed in Table \textbackslash ref\{tab:acceptance-rate\} \textbackslash cite\{zhang2022iterative\_revisions\}}. [..\textit{Same as 24th iteration}] \textbackslash caption\{The system architecture of \textsc{$\mathcal{R}3$}, outlining the iteration process from user interactions to text finalization, \textcolor{teal}{as elaborated in Section \textbackslash ref{sec:system}}.\} (\textbf{Cross-reference})
\\
\hline
100 & [..\textit{Same as the 99th iteration}] \textcolor{teal}{\textbackslash end\{document\}} (\textbf{Macro Insertion}) & [..\textit{Same as the 99th }] \textcolor{teal}{\textbackslash usepackage[margin=1in]\{geometry\} \texttt{\% Customizes page margins}} \textcolor{teal}{\textbackslash usepackage\{hyperref\} \texttt{\% Enables hyperlinks}} (\textbf{Fluency}) & [..\textit{Same as the 93th iteration}]
\textbackslash bibliography\{references\}

\textbackslash bibliographystyle\{plain\}

\textcolor{red}{\sout{\% References}}\textbackslash begin\{thebibliography\}\{\}
(\textbf{Cross-reference}) \\ 
\bottomrule
\end{tabular}
\caption{Example model outputs at different iterations, from the seed document \cite{du-etal-2022-read} (Listing \ref{table:seed-entry-read}). 
\label{table:qual-comparison}}
\end{table*}

\paragraph{Results} Table \ref{table:intention-prediction} presents the weighted F1 scores for predicting writing intentions across baselines and fine-tuned models. Regardless of the intricate nature of the task itself\footnote{Each model predicts the next intention using only the "before" text, while human annotators consider multiple "before and after" edits. Moreover, the chosen next intention is not necessarily the only correct one.}, all models finetuned on \textsc{ScholaWrite} show an improved performance compared to their baselines. BERT and RoBERTa achieved the most improvement, while LLama-8B-Instruct showed a modest improvement after fine-tuning. As detailed in Appendix \ref{sec:appendix:model}, more training epochs and encoder-decoder architectures of BERT variants are assumed to be the reason for significant improvement compared to LLMs. This aligns with findings from \citet{grasso2024assessing, yu2023openclosedsmalllanguage}, which shows that RoBERTa and BERT can often match or even outperform LLMs for the text classification tasks. Those results demonstrate the effectiveness of our \textsc{ScholaWrite} dataset to align language models with writers' intentions. 
% \minhwa{further mention limitation of low llama performance in limitation}

% \begin{table}[ht!]
% \footnotesize
% \centering
% \begin{tabular}{ccc}
% \toprule
% & \begin{tabular}[c]{@{}l@{}} \textsc{ScholaWrite-}\\ \textsc{Llama-3.2}\\ \end{tabular} & GPT-4o \\
% \midrule
% Avg. & 0.23 & 0.16 \\
% Macro Avg. & 0.04 & 0.07 \\
% Weighted Avg. & \textbf{0.27} & 0.21 \\
% \bottomrule
% \end{tabular}
% \caption{Average F-1 scores on the intention prediction task. We observe our fine-tuned model \textsc{ScholaWrite-Llama-3.2} perform better than GPT-4o. }
% \label{table:prediction-result-comparison}
% \end{table}





% According to Table \ref{table:prediction-result-comparison}, we observe that the overall performance is very low in both models. These low F1 scores suggest that LLMs face challenges in fully grasping the complexities of scholarly scientific writing. However, our fine-tuned model \textsc{ScholaWrite-Llama-3.2} comparatively outperformed GPT-4o, despite not providing explicit definitions for each label in the Llama3 model. Remarkably, even this smaller 1B-parameter model performed better than the much larger GPT-4. This result demonstrates the value of our dataset in helping LLMs better understand the human reasoning process involved in scientific writing. In Figure \ref{fig:f1-by-label}, 
% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=\linewidth]{latex/figures/llama_vs_gpt.pdf}
%     \caption{Per-label average log F1 scores on the prediction task of next writing intention.}
%     \label{fig:f1-by-label}
% \end{figure}



% \begin{tabular}{lrl}
% \toprule
%  & Llama 3.2 f1& GPT-4o f1 \\
% \midrule
% Citation Integration & 0.00 & 0.08 \\
% Clarity & 0.01 & 0.10 \\
% Coherence & 0.00 & 0.05 \\
% Cross-reference & 0.01 & 0.07 \\
% Fluency & 0.02 & 0.02 \\
% Idea Generation & 0.05 & 0.04 \\
% Idea Organization & 0.00 & 0.01 \\
% Linguistic Style & 0.00 & 0.00 \\
% Macro Insertion & 0.03 & 0.19 \\
% Object Insertion & 0.09 & 0.10 \\
% Scientific Accuracy & 0.00 & 0.03 \\
% Section Planning & 0.04 & 0.06 \\
% Structural & 0.01 & 0.00 \\
% Text Production & 0.45 & 0.31 \\
% Textual Style & 0.00 & 0.00 \\
% Visual Formatting & 0.00 & 0.13 \\
% \hline
% \hline
% accuracy & 0.23 & 0.16 \\
% macro avg & 0.04 & 0.07 \\
% weighted avg & 0.27 & 0.21 \\
% \bottomrule
% \end{tabular}

%\begin{table*}[ht!]
%\begin{tabular}{lrrrr}\n
%\toprule\n & precision & recall & f1-score\\
%\midrule
%Citation Integration & 0.00 & 0.00 & 0.00\\
%Clarity & 0.15 & 0.01 & 0.01\\
%Coherence & 0.02 & 0.00 & 0.00\\
%Cross-reference & 0.01 & 0.02 & 0.01\\
%Fluency & 0.02 & 0.02 & 0.02\\
%Idea Generation & 0.05 & 0.05 & 0.05\\
%Idea Organization & 0.00 & 0.00 & 0.00\\
%Linguistic Style & 0.00 & 0.00 & 0.00\\
%Macro Insertion & 0.02 & 0.04 & 0.03\\
%Object Insertion & 0.06 & 0.30 & 0.09\\
%Scientific Accuracy & 0.00 & 0.00 & 0.00\\
%Section Planning & 0.03 & 0.07 & 0.04\\
%Structural & 0.04 & 0.01 & 0.01\\
%Text Production & 0.57 & 0.38 & 0.45\\
%% Textual Style & 0.00 & 0.00 & 0.00\\
%Visual Formatting & 0.00 & 0.00 & 0.00\\
%% nan & 0.00 & 0.00 & 0.00\\
%accuracy & 0.23 & 0.23 & 0.23 \\
%macro avg & 0.06 & 0.05 & 0.04\\
%weighted avg & 0.35 & 0.23 & 0.27\\
%\bottomrule
%\end{tabular}
%\caption{Llama 3.2 1B Intention Classification Results}
%\label{tab:intention_classification_res}
%\end{table*}


% Out of 12558 quries, there are 10 invalid response from GPT-4o. Invalid response means the output is not including any of labels mentioned in the prompt. Here is the result of GPT-4o

% \begin{table*}[ht!]
% \begin{tabular}{lrrrr}\n
% \toprule\n & precision & recall & f1-score\\
% \midrule
% Citation Integration & 0.05 & 0.37 & 0.08\\
% Clarity & 0.15 & 0.08 & 0.10\\
% Coherence & 0.03 & 0.10 & 0.05\\
% Cross-reference & 0.04 & 0.46 & 0.07\\
% Fluency & 0.01 & 0.03 & 0.02\\
% Idea Generation & 0.11 & 0.02 & 0.04\\
% Idea Organization & 0.01 & 0.03 & 0.02\\
% Linguistic Style & 0.00 & 0.00 & 0.00\\
% Macro Insertion & 0.11 & 0.62 & 0.19\\
% Object Insertion & 0.09 & 0.12 & 0.10\\
% Scientific Accuracy & 0.02 & 0.10 & 0.03\\
% Section Planning & 0.03 & 0.20 & 0.06\\
% Structural & 0.00 & 0.00 & 0.00\\
% Text Production & 0.67 & 0.20 & 0.31\\
% Visual Formatting & 0.15 & 0.11 & 0.13\\
% accuracy &&& 0.16 \\
% macro avg & 0.09 & 0.15 & 0.07\\
% weighted avg & 0.42 & 0.16 & 0.21\\
% \bottomrule
% \end{tabular}
% \caption{GPT4o Intention Classification Results}
% \label{tab:intention_classification_res}
% \end{table*}

% \tikzset{
%   hatch/.style={pattern=horizontal lines, pattern color=#1},
%   hatch/.default=black
% }



\subsection{Iterative Self-Writing}\label{sec:self-writing}



\paragraph{Setups}

During iterative self-writing (Figure \ref{fig:iterative_model_writing}), a fine-tuned model processes LaTeX-formatted seed document (as ``before-text'') with a context \colorbox{planningcolor}{prompt} to predict the next \colorbox{pink}{intention}, then revises the text (``after-text'') accordingly given \colorbox{BlueGreen}{prompt}. The revised document then serves as the new seed for the next iteration. This process repeats until a set iteration limit (e.g., 100) is reached. All models use the same train (80\%)-test (20\%) split across experiments. See Appendix \ref{sec:appendix:model} and Figure \ref{fig:iteartive-writing-setup-detail} for training details. 

We fine-tune Llama3.1-8B-Instruct (\textcolor{magenta}{Llama-8B-SW}) and compare it to vanilla Llama-8B-Instruct (\textcolor{teal}{Llama-8B-Zero}) and \textcolor{blue}{GPT-4o}. Also, seed documents were derived from LaTeX-formatted abstracts of four award-winning NLP papers on diverse topics \cite{zeng-etal-2024-johnny, lu-etal-2024-semisupervised, du-etal-2022-read, etxaniz-etal-2024-latxa} (Appendix \ref{table:seed-entry-johnny}-\ref{table:seed-entry-latxa}).

% Iterative self-writing involves two subtasks (Figure \ref{fig:iterative_model_writing}): (1) next intention prediction and (2) ``after-text'' generation. Similar to Section \ref{sec:eval:predict}, the first task (the box ``Prediction'') uses a fine-tuned language model to process a LaTeX-formatted seed document as ``before-text'' along with a context \colorbox{planningcolor}{prompt}. The model then predicts the next writing \colorbox{pink}{intention} to guide subsequent revisions. Once an intention is predicted, the second task (the box ``Generation'') generates revisions of the seed document as ``after-text'' based on the predicted intention and given \colorbox{BlueGreen}{prompt}. The revised document then serves as the new seed for the next iteration. This iterative process continues until no further changes occur or a set iteration limit (e.g., 100) is reached.

% For training, we randomly split the \textsc{ScholaWrite} dataset into training (80\%) and testing (20\%) sets. From each intention label in the test set, we sample up to 300 keystroke entries due to budget constraints. All models use the same train-test split across experiments (See Appendix \ref{sec:appendix:model}; Figure \ref{fig:iteartive-writing-setup-detail} for further training details).

% For intention prediction, we fine-tune Llama3.1-8B-Instruct on \textsc{ScholaWrite} training set (\textsc{Llama-8B-SW-pred}'') and compare it to baseline models (Llama-8B-Instruct and GPT-4o) from Section \ref{sec:eval:predict}. For ``after-text'' generation, we fine-tune another Llama3.1-8B-Instruct model (``\textsc{Llama-8B-SW-gen}'') using the same dataset, with Llama3.1-8B-Instruct and GPT-4o as baselines. The fine-tuning prompt includes task instructions, a verbalizer from human-annotated labels, and ``before-text.'' While prompts were standardized, task instructions varied by model (See Appendix \ref{sec:appendix:prompt} for the prompt templates).


% Seed documents were derived from LaTeX-formatted abstracts of four award-winning NLP papers on diverse topics \cite{zeng-etal-2024-johnny, lu-etal-2024-semisupervised, du-etal-2022-read, etxaniz-etal-2024-latxa} (Appendix \ref{table:seed-entry-johnny} to \ref{table:seed-entry-latxa}).

% Also, due to budget constraints, models had different revision strategies. \textsc{Llama-8B-SW-*} and \textsc{Llama-8B-Instruct} continued revision until the next predicted intention changed. \textcolor{blue}{GPT-4o}, however, moved to the next iteration regardless. We refer to the fine-tuned Llama-8B model as \textcolor{magenta}{Llama-8B-ScholaWrite} (or \textcolor{magenta}{Llama-8B-SW}) and the vanilla model as \textcolor{teal}{Llama-8B-Zero}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Starting with a seed document, the classification model predicts the next writing intention, and the generation model revises the document based on that prediction and the ``before'' text. The revised document then serves as the new seed for the next iteration. This process repeats \textit{100 times}.


% \begin{table*}[h!]
% \centering
% \footnotesize
% % \resizebox{2\columnwidth}{!}{
% \begin{tabular}{@{}c@{\hskip 1mm}@{}|p{4.4cm}p{4.4cm}p{4.4cm}@{\hskip 2mm}@{}}
% \toprule
% \textbf{Iter.} & \textbf{\textcolor{magenta}{Llama-8B-SW}} & \textbf{\textcolor{teal}{Llama-8B-Zero}} & \textbf{\textcolor{blue}{GPT-4o}}\\
% \midrule
% 10 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information to avoid information\textcolor{red}{\sout{,}}\textcolor{teal}{-} over\textcolor{red}{\sout{load}}\textcolor{teal}{-claiming} (\textbf{Text Production}) & [\textit{..Generating Experiment Section}] ..with an average acceptance rate of \textcolor{teal}{87.5\% (standard deviation: 3.2\%),..} ... with a reduction of 3\textcolor{red}{\sout{0}}\textcolor{teal}{5}\% in revision time and 2\textcolor{red}{\sout{5}}\textcolor{teal}{8}\% in human effort... (\textbf{Scientific Accuracy}) & [..\textit{Same as the 9th iteration}] \textbackslash usepackage\{booktabs\}

% \textbackslash usepackage\{array\}

% \textcolor{teal}{\textbackslash usepackage\{hyperref\}}...

% \textcolor{red}{\sout{\textbackslash method}} \textcolor{teal}{\textsc{$\mathcal{R}3$}}...

% \textcolor{red}{\sout{\textbackslash method}} \textcolor{teal}{\textsc{$\mathcal{R}3$}}...

% (\textbf{Cross-reference})\\
% \hline
% 25 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information to avoid information overload\textcolor{red}{\sout{,}}\textcolor{teal}{.} (\textbf{Text Production}) & [\textit{..Editing Table}]
% Acceptance rate (\%) \& 75 \& 8\textcolor{red}{\sout{7.5}}\textcolor{teal}{8.2} //
% Revision time (minutes) \& 45 \& 2\textcolor{red}{\sout{9}}\textcolor{teal}{8.5}
% Human effort (minutes) \& 60 \& 4\textcolor{red}{\sout{3}}\textcolor{teal}{2} 
% ... (\textbf{Scientific Accuracy}) & [..\textit{Same as the 24th iteration}] The \textcolor{red}{\sout{efficiency}} \textcolor{teal}{...consider the structural flowchart in Figure \textbackslash ref\{fig:system-architecture\}, which outlines...}  

% [\textit{Inserting the figure}]

% \textcolor{teal}{\textbackslash label\{fig:system-architecture\}} (\textbf{Object Insertion})\\
% \hline 
% 51 & [\textit{..Editing Abstract..}] but rather should be used to improve the flow of information, offering \textcolor{red}{\sout{teach}}\textcolor{teal}{previously trained} to \textcolor{teal}{a} load more related information over the\textcolor{teal}{-} load. (\textbf{Clarity}) & \textcolor{teal}{\textbackslash section\{Impact of the Proposed System\}
% The proposed system, $\mathcal{R}3$, has the potential to impact the writing process in several ways.}... \textcolor{teal}{\textbackslash section\{Future Research Direction\}}... (\textbf{Structural}) & \textbackslash bibitem\{jones2020one\_shot\}
% Jones, L., \textcolor{teal}{\textbf{\textbackslash}}\& Green, D. (2020). 

% \textbackslash bibitem\{brown2021collaboration\}
% Brown, E., \textcolor{teal}{\textbf{\textbackslash}}\& Davis, M. (2021). 

% \textbackslash bibitem\{garcia2021revision\_metrics\}
% Garcia, I., \textcolor{teal}{\textbf{\textbackslash}}\& Lopez, R. (2021). (\textbf{Object Insertion})
% % \textcolor{red}{\sout{\textbackslash cite\{zhang2022iterative\_revisions\}. Refer to Table \textbackslash ref\{tab:acceptance-rate\} for a detailed comparison of acceptance rates}}\textcolor{teal}{, as detailed in Table \textbackslash ref\{tab:acceptance-rate\} \textbackslash cite\{zhang2022iterative\_revisions\}}. [..\textit{Same as 24th iteration}] \textbackslash caption\{The system architecture of \textsc{$\mathcal{R}3$}, outlining the iteration process from user interactions to text finalization, \textcolor{teal}{as elaborated in Section \textbackslash ref{sec:system}}.\} (\textbf{Cross-reference})
% \\
% \hline
% 100 & [..\textit{Same as the 99th iteration}] \textcolor{teal}{\textbackslash end\{document\}} (\textbf{Macro Insertion}) & [..\textit{Same as the 99th }] \textcolor{teal}{\textbackslash usepackage[margin=1in]\{geometry\} \texttt{\% Customizes page margins}} \textcolor{teal}{\textbackslash usepackage\{hyperref\} \texttt{\% Enables hyperlinks}} (\textbf{Fluency}) & [..\textit{Same as the 93th iteration}]
% \textbackslash bibliography\{references\}

% \textbackslash bibliographystyle\{plain\}

% \textcolor{red}{\sout{\% References}}

% \textbackslash begin\{thebibliography\}\{\}
% (\textbf{Cross-reference}) \\ 
% \bottomrule
% \end{tabular}
% \caption{Example model outputs at different iterations, from the seed document \cite{du-etal-2022-read} (Listing \ref{table:seed-entry-read}). 
% \label{table:qual-comparison}}
% \end{table*}


% We have different modifications between models based on this setup, due to budget constraints. \textsc{Llama-8B-SW-*} and \textsc{Llama-8B-Instruct} keep revising the document within one iteration until the next intention differs from the current one. \textcolor{blue}{GPT-4o} proceeds to the next iteration regardless of whether the predicted intention changes. We name the fine-tuned Llama-8B model as \textcolor{magenta}{Llama-8B-ScholaWrite}\footnote{We interchangeably use \textcolor{magenta}{Llama-8B-ScholaWrite} and \textcolor{magenta}{Llama-8B-SW}.} and the vanilla Llama-8B-Instruct model as \textcolor{teal}{Llama-8B-Zero}.


%We have different between models based on this setup due to budget constraints. \textsc{Llama-8B-SW-*} and \textsc{Llama-8B-instruct} keep revising the document within one iteration until the next intention differs from the current one. \textcolor{blue}{GPT-4o} proceeds to the next iteration regardless of whether the predicted intention changes. We name the fine-tuned Llama-8B model as \textcolor{magenta}{Llama-8B-ScholaWrite}\footnote{We interchangeably use \textcolor{magenta}{Llama-8B-ScholaWrite} and \textcolor{magenta}{Llama-8B-SW}.} and the vanilla Llama-8B-Instruct model as \textcolor{teal}{Llama-8B-Zero}.

% \dk{show an example of seed text and add the rest of seed papers in appendix.}
\paragraph{Metrics}

We evaluated \textit{lexical diversity} (unique-to-total token ratio), \textit{topic consistency} (cosine similarity between seed and final output), and \textit{intention coverage} (unique writing intentions used over 100 iterations).
% We calculated \textit{lexical diversity} (unique-to-total token ratio in the final iteration), \textit{topic consistency} (cosine similarity between the seed document and final output), and \textit{intention coverage} (proportion of unique writing intentions used across 100 iterations out of 15 labels in the taxonomy).
% We calculated \textit{lexical diversity} (the ratio of unique to total tokens in the final iteration), \textit{topic consistency} (cosine similarity between the seed document and the final output), and \textit{intention coverage} (diversity of writing intentions as a proportion of unique labels used across 100 iterations among the 15 available labels in our taxonomy).
For \textbf{human evaluation}, three native English speakers with LaTeX expertise assessed outputs from \textcolor{teal}{Llama-8B-Zero} and \textcolor{magenta}{Llama-8B-SW} on \textit{accuracy} (alignment with predicted intention), \textit{alignment} (similarity to human writing), \textit{fluency}(grammatical correctness), \textit{coherence}(logical structure), and \textit{relevance} (connection to the seed document) - Refer to Appendix \ref{sec:appendix:human-eval}. Accuracy was judged per iteration, while other metrics used pairwise comparisons. Inter-annotator agreement (IAA)\footnote{The IAA scores are 0.84 (\textcolor{magenta}{SW}) and 0.76 (\textcolor{teal}{Zero}) for accuracy, all 100\% for alignment, fluency, coherence, and 49.8\% (\textcolor{magenta}{SW}) and 100\% (\textcolor{teal}{Zero}) for relevance.} was measured using Krippendorff’s alpha for accuracy and percentage agreement for others.

% Following \citet{chang2023surveyevaluationlargelanguage}, we conducted a \textbf{human evaluation} with three native English speakers experienced in LaTeX (see Appendix \ref{sec:appendix:human-eval} for details). They assessed \textit{accuracy} (alignment with predicted intention), \textit{alignment} (similarity to human writing style), \textit{fluency }(grammatical correctness), \textit{coherence} (logical structure), and \textit{relevance }(connection to the seed paper content).
% Furthermore, inspired by \citet{chang2023surveyevaluationlargelanguage}, we conducted a \textbf{human evaluation} with three native English speakers experienced in LaTeX (Refer to Appendix \ref{sec:appendix:human-eval} for more detailed descriptions of the entire evaluation process.). They assessed the outputs based on \textit{accuracy} (alignment with the predicted intention), \textit{alignment} (how closely the model’s process resembled human writing style), \textit{fluency} (grammatical correctness), \textit{coherence} (logical structure), and \textit{relevance} (connection to the seed paper's contents). 

% Accuracy was evaluated per iteration, while alignment, fluency, and coherence were judged via pairwise comparisons. We evaluated outputs from \textcolor{teal}{Llama-8B-Zero} and \textcolor{magenta}{Llama-8B-ScholaWrite}. Inter-annotator agreement (IAA) was measured using Krippendorff’s alpha for accuracy and the average percentage of matches between any of the two annotators for other metrics\footnote{The IAA scores are 0.84 (\textcolor{magenta}{SW}) and 0.76 (\textcolor{teal}{Zero}) for accuracy, all 100\% for alignment, fluency, coherence, and 49.8\% (\textcolor{magenta}{SW}) and 100\% (\textcolor{teal}{Zero}) for relevance.}.
% Accuracy was evaluated for each iteration, while alignment, fluency, and coherence were assessed through pairwise comparisons. Here, we only evaluated outputs from \textcolor{teal}{Llama-8B-Zero} and its fine-tuned model (\textcolor{magenta}{Llama-8B-ScholaWrite}). Inter-annotator agreement (IAA) was Krippendorff's alpha for \textit{accuracy} and the average percentage of match between any of the two annotators for other metrics\footnote{The IAA scores are 0.84 (\textcolor{magenta}{SW}) and 0.76 (\textcolor{teal}{Zero}) for accuracy, all 100\% for alignment, fluency, coherence, and 49.8\% (\textcolor{magenta}{SW}) and 100\% (\textcolor{teal}{Zero}) for relevance.}. 


% (1) \textit{Accuracy}: Out of 100, the number of generated outputs that align with the provided intention.
% (2) \textit{Alignment}: Which model's whole writing process more like you?
% (3) \textit{Overall Fluency check}: Which model’s final writing sounds grammatically correct?
% (4) \textit{Overall coherence check}: Which model’s final writing sounds more logical?
% (5) \textit{Relevancy}: Does the final writing contain related contents to the title, keywords, introduction? 

\begin{table}[ht!]
\footnotesize 
\centering
\begin{tabular}{@{}p{1.2cm}c|c|c|c|c@{}}
\toprule
\textbf{Metrics} & \textbf{Model} & \textbf{Seed 1} & \textbf{Seed 2} & \textbf{Seed 3} & \textbf{Seed 4} \\ \midrule
\multirow{2}{*}{Accuracy} &  \textcolor{magenta}{SW} & 21.0 & 10.3 & 18.3 & 15.3\\
\cmidrule(r){2-6}
& \textcolor{teal}{Zero} & 35.7 & 29.7 & 45.3 & 43.3\\
\midrule
\multirow{2}{*}{Alignment} &  \textcolor{magenta}{SW} & 0 & 0 & 0 & 0\\
\cmidrule(r){2-6}
& \textcolor{teal}{Zero} & 3 & 3 & 3 & 3\\
\midrule
\multirow{2}{*}{Fluency} &  \textcolor{magenta}{SW} & 0 & 0 & 0 & 0\\
\cmidrule(r){2-6}
& \textcolor{teal}{Zero} & 3 & 3 & 3 & 3\\
\midrule
\multirow{2}{*}{Coherence} &  \textcolor{magenta}{SW} & 0 & 0 & 0 & 0\\
\cmidrule(r){2-6}
& \textcolor{teal}{Zero} & 3 & 3 & 3 & 3\\
\midrule
\multirow{2}{*}{Relevance} &  \textcolor{magenta}{SW} & 1 & 3 & 2 & 1\\
\cmidrule(r){2-6}
& \textcolor{teal}{Zero} & 3 & 3 & 3 & 3\\
\bottomrule
\end{tabular}
\caption{Human evaluation results for all the four seed documents. For \textit{accuracy}, each represents the average number of generated keystrokes inferred with correct intentions across three evaluators per seed. For other metrics, each indicates the number of human evaluators who agreed based on the performance of each model. \textcolor{magenta}{SW} abbreviated for \textcolor{magenta}{Llama-8B-SW} and \textcolor{teal}{Zero} for \textcolor{teal}{Llama-8B-Zero}, respectively.
% \dk{Make this table single column. You can use abbreviations for model names to shrink the width}
}
\label{table:human-eval-all}
\end{table}


\paragraph{Results} 

Figure \ref{fig:sec5-auto-all} shows that \textcolor{magenta}{Llama-8B-SW} consistently produced the most lexically diverse words, generated the most semantically aligned topics (Seeds 1 \& 2), and covered the most writing intentions (except Seed 3). These results underscore the value of \textsc{ScholaWrite} in improving scholarly writing quality generated by language models.
% Figure \ref{fig:sec5-auto-all} illustrates the quality of the final writing output produced by each model across all four seed documents. Notably, the two Llama-8B-Instruct models, fine-tuned on \textsc{ScholaWrite} for intention prediction and after-text generation independently (referred to as \textcolor{magenta}{Llama-8B-SW}), consistently used the most lexically diverse words in their final outputs. Moreover, \textcolor{magenta}{Llama-8B-SW} generated content that was semantically most aligned with the seed documents (Seeds 1 \& 2) and covered the highest number of writing intentions based on our taxonomy for all seeds except Seed 3. These results underscore the effectiveness of \textsc{ScholaWrite} as a valuable resource for enhancing the quality of scholarly writing generated by language models. 

However, our human evaluation (Table \ref{table:human-eval-all}) revealed that \textcolor{magenta}{Llama-8B-SW} generated less human-like writing, in terms of fluency and logical claims. It also struggled with generating texts aligned with the predicted intentions. See Appendix Tables \ref{table:human-eval-seed1} to \ref{table:human-eval-seed4} for more details. Despite the weaknesses, \textcolor{magenta}{Llama-8B-SW} still produced more relevant content (Seed 2), which aligns with topic consistency trends in Figure \ref{fig:sec5-auto-all}, highlighting the usefulness of \textsc{ScholaWrite} dataset in certain contexts. 

% Despite their remarkable performance based on automatic evaluation metrics, LLMs still exhibit limitations in learning human writing behaviors and scholarly thinking processes. According to our human evaluation (Table \ref{table:human-eval-all}), \textcolor{magenta}{Llama-8B-SW} generated fewer instances of ``after'' text that aligned with the predicted intentions from the previous step during 100 iterations across all four seed documents. Furthermore, all three evaluators unanimously agreed that the baseline model, \textcolor{teal}{Llama-8B-Zero}, demonstrated more human-like writing behaviors throughout the iterations. Its final outputs were also perceived as more grammatically correct and containing stronger logical claims compared to \textcolor{magenta}{Llama-8B-SW}. Please refer to Tables \ref{table:human-eval-seed1} to \ref{table:human-eval-seed4} in Appendix \ref{sec:appendix:human-eval} for more detailed results. 
% However, the evaluators also noted that the final outputs from \textcolor{magenta}{Llama-8B-SW} contained more relevant content for Seed 2. This observation aligns with the trend in topic consistency scores shown in Figure \ref{fig:sec5-auto-all}, further highlighting the usefulness of \textsc{ScholaWrite} dataset in certain contexts. 




Moreover, \textcolor{magenta}{Llama-8B-SW} exhibited the most human-like writing activity patterns over time (Figure \ref{fig:writing-step-intention-all-model}), which frequently switches between implementation and revision and covers all three high-level processes. \textcolor{teal}{Llama-8B-Zero} and \textcolor{blue}{GPT-4o} tend to remain in a single high-level stage throughout all 100 iterations of self-writing. Compared to Appendix Figure \ref{fig:writing-step-detailed-all}, which depicts frequent transitions across all three stages in an early draft (e.g., the first 100 steps), \textcolor{magenta}{Llama-8B-SW} most closely replicates human writing behaviors in iterative writing tasks. These findings reinforce the potential of \textsc{ScholaWrite} in helping LLMs emulate human scholarly writing processes.
% Furthermore, the \textcolor{magenta}{Llama-8B-ScholaWrite} model exhibits the most human-like pattern of writing activities over time. As shown in Figure \ref{fig:sec5-timestamp-seed2}, \textcolor{magenta}{Llama-8B-ScholaWrite} frequently switches between implementation and revision stages, whereas \textcolor{teal}{Llama-8B-Zero} and \textcolor{blue}{GPT-4o} tend to remain focused on a single high-level stage throughout all 100 iterations of self-writing. Compared to Figure \ref{fig:timestamp-proj1-intention}, which depicts frequent transitions across all three writing stages in an early draft (i.e., the first 100 steps), \textcolor{magenta}{Llama-8B-ScholaWrite} most closely replicates human writing behaviors in iterative writing tasks. These findings highlight the effectiveness of the \textsc{ScholaWrite} dataset in helping language models learn and emulate human scholarly thinking processes. 

% \minhwa{timestamp figures for human writing}


























% The overall average IAAs for all of the three metrics is 0.8, indicating high agreement in the pairwise evaluation. 



% The Llama 3.2 model provided significant revisions throughout the entire writing process. The GPT-4 model provided a lot of output in the beginning stages of the writing process, but eventually ceased providing output once the text reached a certain state. This happened around 50 iterations for all three seeds. Although the output quality from Llama is poor most of the time, since it is continuously provide revisions, leads human evaluators to prefer its response. We suspect that the prompting methods, which ask the model to revise only at one word or phrase level at a time /ref{prompting methods} may be responsible for the lack of output. 

% We can observe three successful and one unsuccessful writing inference in Figure \ref{fig:llama_iterative_output}. The three successful outputs \ref{fig:llama_iter_sub_a} \ref{fig:llama_iter_sub_b} \ref{fig:llama_iter_sub_c} show the Llama model successfully interpreting the Object Insertion direction (top left of image) and inserting full \LaTeX figures. The Llama model has an unsuccessful prediction in \ref{fig:llama_iter_sub_d} where it removes a paragraph instead of performing Idea Generation. We observe four samples iterations of GPT4o model in Figure \ref{fig:gpt_iterative_output}. The GPT model successfully performed Coherence revision in \ref{fig:gpt_iter_sub_a}, and successfully performed cross-reference and object insertion in \ref{fig:gpt_iter_sub_b} and \ref{fig:gpt_iter_sub_c}. In Figure \ref{fig:gpt_iter_sub_d}, the GPT4o model failed to understand the writing intention of Citation Integration, and instead performed revision in text regarding references. From sample iterations, we can determine that the fine-tuned Llama model is able to generate entire figures and complex revisions according to a scholarly writing intention and appropriate \LaTeX syntax. Yet, these models still struggle with some complex writing intentions and may hallucinate information or generate detrimental revision suggestions.



% \label{sec:Results: Percentage Win of Llama}

% \begin{table}
% \begin{center}
% \begin{tabular}{ c|c|c|c } 
%  \hline
%   & Flow & Accuracy & Fluency \\
%  \hline
% Annotator 1 & 0.667 & 0.667 & 0.667 \\
% Annotator 2 & 0.733 & 0.733 & 0.733 \\
% Avg & 0.7 & 0.7 & 0.7 \\
%  \hline
%  %\caption{Human evaluation results for iterative model inference with GPT4o and Llama 3.2. The values over 0.5 represent preference of Llama 3.2 outputs.}
% \end{tabular}
% \end{center}
% \end{table}
%-----------------------------------------------------

