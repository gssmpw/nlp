% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{bowles2010think,
  title={The think-aloud controversy in second language research},
  author={Bowles, Melissa A},
  year={2010},
  publisher={Routledge}
}

@article{pustejovsky2017designing,
  title={Designing annotation schemes: From theory to model},
  author={Pustejovsky, James and Bunt, Harry and Zaenen, Annie},
  journal={Handbook of Linguistic Annotation},
  pages={21--72},
  year={2017},
  publisher={Springer}
}

@article{chan2017using,
  title={Using keystroke logging to understand writers’ processes on a reading-into-writing test},
  author={Chan, Sathena},
  journal={Language Testing in Asia},
  volume={7},
  pages={1--27},
  year={2017},
  publisher={Springer}
}

@article{johansson2010looking,
  title={Looking at the keyboard or the monitor: relationship with text production processes},
  author={Johansson, Roger and Wengelin, {\AA}sa and Johansson, Victoria and Holmqvist, Kenneth},
  journal={Reading and writing},
  volume={23},
  pages={835--851},
  year={2010},
  publisher={Springer}
}

@article{leijten2013keystroke,
  title={Keystroke logging in writing research: Using Inputlog to analyze and visualize writing processes},
  author={Leijten, Mari{\"e}lle and Van Waes, Luuk},
  journal={Written Communication},
  volume={30},
  number={3},
  pages={358--392},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@book{lindgren2019observing,
  title={Observing writing: Insights from keystroke logging and handwriting},
  author={Lindgren, Eva and Sullivan, Kirk},
  volume={38},
  year={2019},
  publisher={Brill}
}


@article{f508427a-e4c0-3d6a-8abf-03a5d21ec6c4,
 ISSN = {0010096X},
 URL = {http://www.jstor.org/stable/356600},
 author = {Linda Flower and John R. Hayes},
 journal = {College Composition and Communication},
 number = {4},
 pages = {365--387},
 publisher = {National Council of Teachers of English},
 title = {A Cognitive Process Theory of Writing},
 urldate = {2024-08-20},
 volume = {32},
 year = {1981}
}

@incollection{van2012logging,
  title={Logging tools to study digital writing processes},
  author={Van Waes, Luuk and Leijten, Mari{\"e}lle and Wengelin, Ǻsa and Lindgren, Eva},
  booktitle={Past, present, and future contributions of cognitive writing research to cognitive psychology},
  pages={507--533},
  year={2012},
  publisher={Psychology Press}
}

@article{latif2008state,
  title={A state-of-the-art review of the real-time computer-aided study of the writing process},
  author={Latif, Muhammad M Abdel},
  journal={International Journal of English Studies},
  volume={8},
  number={1},
  pages={29--50},
  year={2008}
}

@article{lindgren2003stimulated,
  title={Stimulated recall as a trigger for increasing noticing and language awareness in the L2 writing classroom: A case study of two young female writers},
  author={Lindgren, Eva and Sullivan, Kirk PH},
  journal={Language Awareness},
  volume={12},
  number={3-4},
  pages={172--186},
  year={2003},
  publisher={Taylor \& Francis}
}

@article{zhu2023insights,
  title={Insights into editing and revising in writing process using keystroke logs},
  author={Zhu, Mengxiao and Zhang, Mo and Gu, Lin},
  journal={Language Assessment Quarterly},
  volume={20},
  number={4-5},
  pages={445--468},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{hayes2014cognitive,
  title={Cognitive processes in writing: A framework.},
  author={Hayes, John R and Berninger, Virginia W},
  year={2014},
  publisher={Oxford University Press}
}

@Inbook{Wengelin2023,
author="Wengelin, {\AA}sa
and Johansson, Victoria",
editor="Kruse, Otto
and Rapp, Christian
and Anson, Chris M.
and Benetos, Kalliopi
and Cotos, Elena
and Devitt, Ann
and Shibani, Antonette",
title="Investigating Writing Processes with Keystroke Logging",
bookTitle="Digital Writing Technologies in Higher Education : Theory, Research, and Practice",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="405--420",
abstract="Already in the 1970s, researchers in linguistics and psychology became interested in understanding how written language production worked, why students' texts ended up in a specific way, and whether writing instruction could be improved by an increased understanding of students' actual activities during writing - what happens ``behind the scenes''. They observed writing processes through video-recordings and think-aloud protocols, both of which required laborious manual analyses, but with the advent of affordable computers in the 1990s keystroke logging was developed. Keystroke logging records all keystrokes and mouse movements and provide them with a time stamp to allow playback and analyses. The purpose of this chapter is to introduce the reader to the concept of keystroke logging, explain briefly how it works, and give an overview of currently available software. First, we provide a short historical background. We then move into the core idea and functionality of keystroke logging in general before turning to descriptions of specific pieces of software. We summarise similarities and differences, aiming to show that choice of software should be governed by the research question. Finally, we discuss research that uses keystroke logging as a research tool, and provide examples of research about keystroke logging as a pedagogical tool.",
isbn="978-3-031-36033-6",
doi="10.1007/978-3-031-36033-6_25",
url="https://doi.org/10.1007/978-3-031-36033-6_25"
}




@article{doi:10.1177/0741088312451108,
author = {Veerle M. Baaijen and David Galbraith and Kees de Glopper},
title ={Keystroke Analysis: Reflections on Procedures and Measures},
journal = {Written Communication},
volume = {29},
number = {3},
pages = {246-277},
year = {2012},
doi = {10.1177/0741088312451108},
URL = {
        https://doi.org/10.1177/0741088312451108
},
eprint = { 
        https://doi.org/10.1177/0741088312451108
}
}

@article{koo2023decoding,
  title={Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts},
  author={Koo, Ryan and Martin, Anna and Wang, Linghe and Kang, Dongyeop},
  journal={arXiv preprint arXiv:2304.00121},
  year={2023}
}


@inproceedings{du-etal-2022-understanding-iterative,
    title = "Understanding Iterative Revision from Human-Written Text",
    author = "Du, Wanyu  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kim, Zae Myung  and
      Lopez, Melissa  and
      Kang, Dongyeop",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.250",
    doi = "10.18653/v1/2022.acl-long.250",
    pages = "3573--3590",
    abstract = "Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human{'}s revision cycles. This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations. Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.",
}

@inproceedings{kim-etal-2022-improving,
    title = "Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks",
    author = "Kim, Zae Myung  and
      Du, Wanyu  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kang, Dongyeop",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.678",
    doi = "10.18653/v1/2022.emnlp-main.678",
    pages = "9986--9999",
    abstract = "Iterative text revision improves text quality by fixing grammatical errors, rephrasing for better readability or contextual appropriateness, or reorganizing sentence structures throughout a document.Most recent research has focused on understanding and classifying different types of edits in the iterative revision process from human-written text instead of building accurate and robust systems for iterative text revision.In this work, we aim to build an end-to-end text revision system that can iteratively generate helpful edits by explicitly detecting editable spans (where-to-edit) with their corresponding edit intents and then instructing a revision model to revise the detected edit spans.Leveraging datasets from other related text editing NLP tasks, combined with the specification of editable spans, leads our system to more accurately model the process of iterative text refinement, as evidenced by empirical results and human evaluations.Our system significantly outperforms previous baselines on our text revision tasks and other standard text revision tasks, including grammatical error correction, text simplification, sentence fusion, and style transfer.Through extensive qualitative and quantitative analysis, we make vital connections between edit intentions and writing quality, and better computational modeling of iterative text revisions.",
}

@inproceedings{raheja-etal-2023-coedit,
    title = "{C}o{E}d{IT}: Text Editing by Task-Specific Instruction Tuning",
    author = "Raheja, Vipul  and
      Kumar, Dhruv  and
      Koo, Ryan  and
      Kang, Dongyeop",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.350",
    doi = "10.18653/v1/2023.findings-emnlp.350",
    pages = "5274--5291",
    abstract = "We introduce CoEdIT, a state-of-the-art text editing system for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as {``}Make the sentence simpler{''} or {``}Write it in a more neutral style,{''} and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly available largest-sized LLMs trained on instructions while being {\textasciitilde}60x smaller, (3) is capable of generalizing to unseen edit instructions, and (4) exhibits abilities to generalize to composite instructions containing different combinations of edit actions. Through extensive qualitative and quantitative analysis, we show that writers prefer the edits suggested by CoEdIT relative to other state-of-the-art text editing models. Our code, data, and models are publicly available at https://github.com/vipulraheja/coedit.",
}

@inproceedings{chakrabarty-etal-2022-help,
    title = "Help me write a Poem - Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
    author = "Chakrabarty, Tuhin  and
      Padmakumar, Vishakh  and
      He, He",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.460",
    doi = "10.18653/v1/2022.emnlp-main.460",
    pages = "6848--6863",
    abstract = "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present \textit{CoPoet}, a collaborative poetry writing system, with the goal of to study if LLM{'}s actually improve the quality of the generated content. In contrast to auto-completing a user{'}s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as \textit{Write a sentence about {`}love{'}} or \textit{Write a sentence ending in {`}fly{'}}. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive to publicly available LLMs trained on instructions (InstructGPT), but also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from \textit{Monarchy} to \textit{Climate change}, which are preferred by third-party evaluators over poems written without the system.",
}

@inproceedings{dang2023choice,
  title={Choice over control: How users write with large language models using diegetic and non-diegetic prompting},
  author={Dang, Hai and Goller, Sven and Lehmann, Florian and Buschek, Daniel},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--17},
  year={2023}
}

@article{laban2023beyond,
  title={Beyond the chat: Executable and verifiable text-editing with llms},
  author={Laban, Philippe and Vig, Jesse and Hearst, Marti A and Xiong, Caiming and Wu, Chien-Sheng},
  journal={arXiv preprint arXiv:2309.15337},
  year={2023}
}

@article{odendahl2018assessing,
  title={Assessing the writing process: A review of current practice},
  author={Odendahl, Nora and Deane, Paul},
  journal={ETS Research Memorandum Series, ETS-RM-18},
  volume={10},
  pages={1--66},
  year={2018}
}

@inproceedings{Krapels1990SecondLW,
  title={Second Language Writing: An overview of second language writing process research},
  author={Alexandra Rowe Krapels},
  year={1990},
  url={https://api.semanticscholar.org/CorpusID:60659113}
}

@inproceedings{valmeekam2022large,
  title={Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}

@article{10.1093/humrep/dead207,
    author = {Semrl, N and Feigl, S and Taumberger, N and Bracic, T and Fluhr, H and Blockeel, C and Kollmann, M},
    title = "{AI language models in human reproduction research: exploring ChatGPT’s potential to assist academic writing}",
    journal = {Human Reproduction},
    volume = {38},
    number = {12},
    pages = {2281-2288},
    year = {2023},
    month = {10},
    abstract = "{Artificial intelligence (AI)-driven language models have the potential to serve as an educational tool, facilitate clinical decision-making, and support research and academic writing. The benefits of their use are yet to be evaluated and concerns have been raised regarding the accuracy, transparency, and ethical implications of using this AI technology in academic publishing. At the moment, Chat Generative Pre-trained Transformer (ChatGPT) is one of the most powerful and widely debated AI language models. Here, we discuss its feasibility to answer scientific questions, identify relevant literature, and assist writing in the field of human reproduction. With consideration of the scarcity of data on this topic, we assessed the feasibility of ChatGPT in academic writing, using data from six meta-analyses published in a leading journal of human reproduction. The text generated by ChatGPT was evaluated and compared to the original text by blinded reviewers. While ChatGPT can produce high-quality text and summarize information efficiently, its current ability to interpret data and answer scientific questions is limited, and it cannot be relied upon for a literature search or accurate source citation due to the potential spread of incomplete or false information. We advocate for open discussions within the reproductive medicine research community to explore the advantages and disadvantages of implementing this AI technology. Researchers and reviewers should be informed about AI language models, and we encourage authors to transparently disclose their use.}",
    issn = {0268-1161},
    doi = {10.1093/humrep/dead207},
    url = {https://doi.org/10.1093/humrep/dead207},
    eprint = {https://academic.oup.com/humrep/article-pdf/38/12/2281/53940839/dead207.pdf},
}

@inproceedings{yuan2022wordcraft,
  title={Wordcraft: story writing with large language models},
  author={Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne},
  booktitle={Proceedings of the 27th International Conference on Intelligent User Interfaces},
  pages={841--852},
  year={2022}
}

@article{gomez2023confederacy,
  title={A confederacy of models: A comprehensive evaluation of LLMs on creative writing},
  author={G{\'o}mez-Rodr{\'\i}guez, Carlos and Williams, Paul},
  journal={arXiv preprint arXiv:2310.08433},
  year={2023}
}

@article{chakrabarty2022help,
  title={Help me write a poem: Instruction tuning as a vehicle for collaborative poetry writing},
  author={Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
  journal={arXiv preprint arXiv:2210.13669},
  year={2022}
}

@article{marco4673692transformers,
  title={Transformers Can Outperform Humans in Short Creative Writing Tasks},
  author={Marco, Guillermo and Gonzalo, Julio and Rello, Luz},
  journal={Available at SSRN 4673692}
}

@inproceedings{mirowski2023co,
  title={Co-writing screenplays and theatre scripts with language models: Evaluation by industry professionals},
  author={Mirowski, Piotr and Mathewson, Kory W and Pittman, Jaylen and Evans, Richard},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--34},
  year={2023}
}

@inproceedings{chakrabarty2024art,
  title={Art or artifice? large language models and the false promise of creativity},
  author={Chakrabarty, Tuhin and Laban, Philippe and Agarwal, Divyansh and Muresan, Smaranda and Wu, Chien-Sheng},
  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},
  pages={1--34},
  year={2024}
}
@article{ippolito2022creative,
  title={Creative writing with an ai-powered writing assistant: Perspectives from professional writers},
  author={Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Burnam, Sehmon},
  journal={arXiv preprint arXiv:2211.05030},
  year={2022}
}

@article{lu2024ai,
  title={The ai scientist: Towards fully automated open-ended scientific discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}

@inproceedings{pinto2023large,
  title={Large language models for education: Grading open-ended questions using chatgpt},
  author={Pinto, Gustavo and Cardoso-Pereira, Isadora and Monteiro, Danilo and Lucena, Danilo and Souza, Alberto and Gama, Kiev},
  booktitle={Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
  pages={293--302},
  year={2023}
}

@article{nickerson2013method,
  title={A method for taxonomy development and its application in information systems},
  author={Nickerson, Robert C and Varshney, Upkar and Muntermann, Jan},
  journal={European Journal of Information Systems},
  volume={22},
  number={3},
  pages={336--359},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{kundisch2021update,
  title={An update for taxonomy designers: methodological guidance from information systems research},
  author={Kundisch, Dennis and Muntermann, Jan and Oberl{\"a}nder, Anna Maria and Rau, Daniel and R{\"o}glinger, Maximilian and Schoormann, Thorsten and Szopinski, Daniel},
  journal={Business \& Information Systems Engineering},
  pages={1--19},
  year={2021},
  publisher={Springer}
}

@inproceedings{jiang-etal-2022-arxivedits,
    title = "ar{X}iv{E}dits: Understanding the Human Revision Process in Scientific Writing",
    author = "Jiang, Chao  and
      Xu, Wei  and
      Stevens, Samuel",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.641",
    doi = "10.18653/v1/2022.emnlp-main.641",
    pages = "9420--9435",
    abstract = "Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.",
}

@article{kuznetsov2022revise,
  title={Revise and resubmit: An intertextual model of text-based collaboration in peer review},
  author={Kuznetsov, Ilia and Buchmann, Jan and Eichler, Max and Gurevych, Iryna},
  journal={Computational Linguistics},
  volume={48},
  number={4},
  pages={949--986},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{darcy-etal-2024-aries,
    title = "{ARIES}: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews",
    author = "D{'}Arcy, Mike  and
      Ross, Alexis  and
      Bransom, Erin  and
      Kuehl, Bailey  and
      Bragg, Jonathan  and
      Hope, Tom  and
      Downey, Doug",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.377",
    doi = "10.18653/v1/2024.acl-long.377",
    pages = "6985--7001",
    abstract = "We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment{---}especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4{'}s ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.",
}

@article{ito2019diamonds,
  title={Diamonds in the rough: Generating fluent sentences from early-stage drafts for academic writing assistance},
  author={Ito, Takumi and Kuribayashi, Tatsuki and Kobayashi, Hayato and Brassard, Ana and Hagiwara, Masato and Suzuki, Jun and Inui, Kentaro},
  journal={arXiv preprint arXiv:1910.09180},
  year={2019}
}

@article{jourdan2024casimir,
  title={CASIMIR: A Corpus of Scientific Articles enhanced with Multiple Author-Integrated Revisions},
  author={Jourdan, L{\'e}ane and Boudin, Florian and Hernandez, Nicolas and Dufour, Richard},
  journal={arXiv preprint arXiv:2403.00241},
  year={2024}
}

@article{mita2022towards,
  title={Towards automated document revision: Grammatical error correction, fluency edits, and beyond},
  author={Mita, Masato and Sakaguchi, Keisuke and Hagiwara, Masato and Mizumoto, Tomoya and Suzuki, Jun and Inui, Kentaro},
  journal={arXiv preprint arXiv:2205.11484},
  year={2022}
}

@inproceedings{narimatsu2021task,
  title={Task definition and integration for scientific-document writing support},
  author={Narimatsu, Hiromi and Koyama, Kohei and Dohsaka, Kohji and Higashinaka, Ryuichiro and Minami, Yasuhiro and Taira, Hirotoshi},
  booktitle={Proceedings of the Second Workshop on Scholarly Document Processing},
  pages={18--26},
  year={2021}
}

@inproceedings{kobayashi-etal-2022-dataset,
    title = "Dataset Construction for Scientific-Document Writing Support by Extracting Related Work Section and Citations from {PDF} Papers",
    author = "Kobayashi, Keita  and
      Koyama, Kohei  and
      Narimatsu, Hiromi  and
      Minami, Yasuhiro",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.609",
    pages = "5673--5682",
    abstract = "To augment datasets used for scientific-document writing support research, we extract texts from {``}Related Work{''} sections and citation information in PDF-formatted papers published in English. The previous dataset was constructed entirely with Tex-formatted papers, from which it is easy to extract citation information. However, since many publicly available papers in various fields are provided only in PDF format, a dataset constructed using only Tex papers has limited utility. To resolve this problem, we augment the existing dataset by extracting the titles of sections using the visual features of PDF documents and extracting the Related Work section text using the explicit title information. Since text generated from the figures and footnotes appearing in the extraction target areas is considered noise, we remove instances of such text. Moreover, we map the cited paper{'}s information obtained using existing tools to citation marks detected by regular expression rules, resulting in pairs of cited paper information and text of the Related Work section. By evaluating body text extraction and citation mapping in the constructed dataset, the accuracy of the proposed dataset was found to be close to that of the previous dataset. Accordingly, we demonstrated the possibility of building a significantly augmented dataset.",
}

@article{jourdan2023text,
  title={Text revision in scientific writing assistance: An overview},
  author={Jourdan, L{\'e}ane and Boudin, Florian and Dufour, Richard and Hernandez, Nicolas},
  journal={arXiv preprint arXiv:2303.16726},
  year={2023}
}

@article{kallestinova2011write,
  title={How to write your first research paper},
  author={Kallestinova, Elena D},
  journal={The Yale journal of biology and medicine},
  volume={84},
  number={3},
  pages={181},
  year={2011},
  publisher={Yale Journal of Biology and Medicine}
}

@article{bourekkache2022english,
  title={English for specific purposes: writing scientific research papers. case study: Phd students in the computer science department},
  author={Bourekkache, Samir},
  year={2022}
}

@article{liang2024can,
  title={Can large language models provide useful feedback on research papers? A large-scale empirical analysis},
  author={Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy Yi and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel Scott and Yin, Yian and others},
  journal={NEJM AI},
  volume={1},
  number={8},
  pages={AIoa2400196},
  year={2024},
  publisher={Massachusetts Medical Society}
}

@misc{Sanchez,
  title = {Evaluating AI-Generated Responses},
  howpublished = {\url{https://www.utrgv.edu/online/teaching-online/elearning-topics/edutech-ai/eval-responses/index.htm}},
  note = {Accessed: 2024-10-15}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{gpt4o,
    author = "OpenAI",
    title={Hello gpt-4o},
    URL = {https://openai.com/index/hello-gpt-4o/}, 
    year={2024},
    note = {Accessed: 2024-10-14}
}

@misc{chang2023surveyevaluationlargelanguage,
      title={A Survey on Evaluation of Large Language Models}, 
      author={Yupeng Chang and Xu Wang and Jindong Wang and Yuan Wu and Linyi Yang and Kaijie Zhu and Hao Chen and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Wei Ye and Yue Zhang and Yi Chang and Philip S. Yu and Qiang Yang and Xing Xie},
      year={2023},
      eprint={2307.03109},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03109}, 
}

@inproceedings{velentzas2024logging,
  title={Logging Keystrokes in Writing by English Learners},
  author={Velentzas, Georgios and Caines, Andrew and Borgo, Rita and Pacquetet, Erin and Hamilton, Clive and Arnold, Taylor and Nicholls, Diane and Buttery, Paula and Gaillat, Thomas and Ballier, Nicolas and others},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  pages={10725--10746},
  year={2024}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{Liu2019RoBERTaAR,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692},
  url={https://api.semanticscholar.org/CorpusID:198953378}
}

@inproceedings{zeng-etal-2024-johnny,
    title = "How Johnny Can Persuade {LLM}s to Jailbreak Them: Rethinking Persuasion to Challenge {AI} Safety by Humanizing {LLM}s",
    author = "Zeng, Yi  and
      Lin, Hongpeng  and
      Zhang, Jingwen  and
      Yang, Diyi  and
      Jia, Ruoxi  and
      Shi, Weiyan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.773",
    doi = "10.18653/v1/2024.acl-long.773",
    pages = "14322--14350",
    abstract = "Most traditional AI safety research views models as machines and centers on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. Observing this, we shift the perspective, by treating LLMs as human-like communicators to examine the interplay between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak risk across all risk categories: PAP consistently achieves an attack success rate of over 92{\%} on Llama-2-7b-Chat, GPT-3.5, and GPT-4 in 10 trials, surpassing recent algorithm-focused attacks. On the defense side, we explore various mechanisms against PAP, find a significant gap in existing defenses, and advocate for more fundamental solutions for AI safety.",
}

@inproceedings{du-etal-2022-read,
    title = "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
    author = "Du, Wanyu  and
      Kim, Zae Myung  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kang, Dongyeop",
    editor = "Huang, Ting-Hao 'Kenneth'  and
      Raheja, Vipul  and
      Kang, Dongyeop  and
      Chung, John Joon Young  and
      Gissin, Daniel  and
      Lee, Mina  and
      Gero, Katy Ilonka",
    booktitle = "Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.in2writing-1.14",
    doi = "10.18653/v1/2022.in2writing-1.14",
    pages = "96--108",
    abstract = "Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at \url{https://github.com/vipulraheja/IteraTeR}. Our system demonstration is available at \url{https://youtu.be/lK08tIpEoaE}.",
}

@inproceedings{lu-etal-2024-semisupervised,
    title = "Semisupervised Neural Proto-Language Reconstruction",
    author = "Lu, Liang  and
      Xie, Peirong  and
      Mortensen, David",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.788",
    doi = "10.18653/v1/2024.acl-long.788",
    pages = "14715--14759",
    abstract = "Existing work implementing comparative reconstruction of ancestral languages (proto-languages) has usually required full supervision. However, historical reconstruction models are only of practical value if they can be trained with a limited amount of labeled data. We propose a semisupervised historical reconstruction task in which the model is trained on only a small amount of labeled data (cognate sets with proto-forms) and a large amount of unlabeled data (cognate sets without proto-forms). We propose a neural architecture for comparative reconstruction (DPD-BiReconstructor) incorporating an essential insight from linguists{'} comparative method: that reconstructed words should not only be reconstructable from their daughter words, but also deterministically transformable back into their daughter words. We show that this architecture is able to leverage unlabeled cognate sets to outperform strong semisupervised baselines on this novel task.",
}

@inproceedings{etxaniz-etal-2024-latxa,
    title = "Latxa: An Open Language Model and Evaluation Suite for {B}asque",
    author = "Etxaniz, Julen  and
      Sainz, Oscar  and
      Miguel, Naiara  and
      Aldabe, Itziar  and
      Rigau, German  and
      Agirre, Eneko  and
      Ormazabal, Aitor  and
      Artetxe, Mikel  and
      Soroa, Aitor",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.799",
    doi = "10.18653/v1/2024.acl-long.799",
    pages = "14952--14972",
    abstract = "We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,046 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.",
}

@article{guo2018modeling,
  title={Modeling basic writing processes from keystroke logs},
  author={Guo, Hongwen and Deane, Paul D and van Rijn, Peter W and Zhang, Mo and Bennett, Randy E},
  journal={Journal of Educational Measurement},
  volume={55},
  number={2},
  pages={194--216},
  year={2018},
  publisher={Wiley Online Library}
}

@article{vandermeulen2023writing,
  title={Writing process feedback based on keystroke logging and comparison with exemplars: Effects on the quality and process of synthesis texts},
  author={Vandermeulen, Nina and Van Steendam, Elke and De Maeyer, Sven and Rijlaarsdam, Gert},
  journal={Written Communication},
  volume={40},
  number={1},
  pages={90--144},
  year={2023},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{diederich1974measuring,
  title={Measuring growth in English.},
  author={Diederich, Paul B},
  year={1974},
  publisher={ERIC}
}

@article{perl1979composing,
  title={The composing processes of unskilled college writers},
  author={Perl, Sondra},
  journal={Research in the Teaching of English},
  volume={13},
  number={4},
  pages={317--336},
  year={1979},
  publisher={ncte. org}
}

@article{macarthur2016writing,
  title={Writing research from a cognitive perspective.},
  author={MacArthur, Charles A and Graham, Steve},
  year={2016},
  publisher={The Guilford Press}
}

@inproceedings{grasso2024assessing,
  title={Assessing Generative Language Models in Classification Tasks: Performance and Self-evaluation Capabilities in the Environmental and Climate Change Domain},
  author={Grasso, Francesca and Locci, Stefano},
  booktitle={International Conference on Applications of Natural Language to Information Systems},
  pages={302--313},
  year={2024},
  organization={Springer}
}

@misc{yu2023openclosedsmalllanguage,
      title={Open, Closed, or Small Language Models for Text Classification?}, 
      author={Hao Yu and Zachary Yang and Kellin Pelrine and Jean Francois Godbout and Reihaneh Rabbany},
      year={2023},
      eprint={2308.10092},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.10092}, 
}