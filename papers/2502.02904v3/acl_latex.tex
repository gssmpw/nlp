% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1

% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
% \usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% \usepackage[dvipsnames]{xcolor}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{multicol}

\usepackage{subcaption}

\usepackage[normalem]{ulem}

% \usepackage[table,xcdraw,dvipsnames]{xcolor} % Load xcolor package with table option

\definecolor{planningcolor}{HTML}{EF9D65}
\definecolor{implementationcolor}{HTML}{84BCD1}
\definecolor{revisioncolor}{HTML}{9584D1}

\usepackage{listings}
\usepackage{caption}
\usepackage{adjustbox}

\usepackage{hyperref}

\usepackage{array,multirow}
\usepackage{tikz}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\textsc{ScholaWrite}: A Dataset of End-to-End Scholarly Writing Process
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Linghe Wang\thanks{Equal contribution.} \quad Minhwa Lee\textsuperscript{$\ast$} \quad \textbf{Ross Volkov} \quad \textbf{Luan Tuyen Chau} \quad \textbf{Dongyeop Kang}\\
University of Minnesota \\ 
\texttt{\{wang9257,lee03533,volko032,chau0139,dongyeop\}@umn.edu}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\newcommand{\minhwa}[1]{\textcolor{magenta}{\bf\small [#1 --Minhwa]}}
\newcommand{\linghe}[1]{\textcolor{cyan}{\bf\small [#1 --Linghe]}}
\newcommand{\ross}[1]{\textcolor{brown}{\bf\small [#1 --Ross]}}
\newcommand{\luan}[1]{\textcolor{blue}{\bf\small [#1 --Luan]}}
\newcommand{\dk}[1]{\textcolor{teal}{\bf\small [#1 --DK]}}

\begin{document}
\maketitle
\begin{abstract}
% Scholarly writing involves complex, non-linear cognitive processes that require not only frequent transitions between various writing intentions but also high expectations of academic communication. While writing assistants powered by large language models (LLMs) have been applied to various writing tasks, their effectiveness in supporting end-to-end scholarly writing remains less explored. Previous work focuses on specific writing stages, overlooking the complexities of research writing, which necessitates the factual accuracy of scientific findings and persuasive narratives with rigorous logical reasoning. To address this gap, we introduce the first annotated dataset of keystroke trajectories from LaTeX-based scientific writing, collected over several months from early-career researchers. Our dataset, comprising over XX keystrokes collected through our thoroughly designed systems, is augmented by linguistics expert review and provides insights into the cognitive writing processes of scholarly scientific papers. We also propose a comprehensive taxonomy of scholarly writing processes in scientific domains, which can be useful resources for enhancing LLM-powered writing assistants for research writing purposes. This work provides a stepping stone for improved AI tools that can assist throughout the entire scholarly writing process, offering tailored suggestions for research writing.

% Recent advancements in large language models (LLMs) have facilitated the development of AI-powered intelligent writing assistants, including scientific manuscripts. 

Writing is a cognitively demanding task involving continuous decision-making, heavy use of working memory, and frequent switching between multiple activities.
Scholarly writing is particularly complex as it requires authors to coordinate many pieces of multiform knowledge.
To fully understand writers' cognitive thought process, one should fully decode the \textit{end-to-end writing data} (from individual ideas to final manuscript) and understand their complex cognitive mechanisms in scholarly writing.
We introduce \textsc{ScholaWrite} dataset, a first-of-its-kind keystroke corpus of an end-to-end scholarly writing process for complete manuscripts, with thorough annotations of cognitive writing intentions behind each keystroke. 
Our dataset includes \LaTeX-based keystroke data from five preprints with nearly 62K total text changes and annotations across 4 months of paper writing.
% Our novel system captures real-time LaTeX-based keystrokes over extended periods, enabling a deep analysis of the cognitive aspects of writing in scholarly communications. 
% Furthermore, by leveraging these data we propose a comprehensive taxonomy of cognitive writing processes specific to the scientific domain.
\textsc{ScholaWrite} shows promising usability and applications (e.g., iterative self-writing), demonstrating the importance of collection of end-to-end writing data, rather than the final manuscript, for the development of future writing assistants to support the cognitive thinking process of scientists.
Our de-identified data examples and code are available on our project page\footnote{\url{https://minnesotanlp.github.io/scholawrite/}}.
%\footnote{Our de-identified dataset and code repository will be released to the public upon acceptance.}
% https://anonymous.4open.science/w/scholawrite-anonymous/
% https://minnesotanlp.github.io/scholawrite/

% Writing is a cognitively demanding task involving continuous decision-making, heavy use of working memory, and frequent switching between multiple activities.
% Scholarly writing is particularly complex as it requires authors to coordinate many pieces of multiform knowledge.
% To fully understand writers' cognitive thought process, one should fully decode the \textit{end-to-end writing data} (from individual ideas to final manuscript) and understand their complex cognitive mechanisms in scholarly writing.
% We introduce \textsc{ScholaWrite}, a first-of-its-kind dataset of keystroke-intention pairs of end-to-end multi-author scholarly writing processes for 5 complete manuscripts. Each keystroke is annotated with cognitive writing intentions behind each keystroke. 
% Our dataset includes \LaTeX-based keystroke data from five preprints with nearly 62K total text changes and annotations across 4 months of paper writing.
% % Our novel system captures real-time LaTeX-based keystrokes over extended periods, enabling a deep analysis of the cognitive aspects of writing in scholarly communications. 
% % Furthermore, by leveraging these data we propose a comprehensive taxonomy of cognitive writing processes specific to the scientific domain.
% \textsc{ScholaWrite} shows promising usability and applications (e.g., iterative self-writing) for the future development of AI writing assistants for academic research, which necessitate complex methods beyond LLM prompting.
% Our experiments clearly demonstrated the importance of collection of end-to-end writing data, rather than just the final manuscript, for the development of future writing assistants to support the cognitive thinking process of scientists.
% Our de-identified dataset, demo, and code repository are available on our project page\footnote{\url{https://minnesotanlp.github.io/scholawrite/}}.
% %\footnote{Our de-identified dataset and code repository will be released to the public upon acceptance.}
\end{abstract}




\input{intro}
\input{related_work}
\input{systems}
\input{taxonomy}
\input{results_data_behavior}
\input{results_model}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work} 

% We introduce \textsc{ScholaWrite}, the first dataset capturing the cognitive process of scholarly writing, with 62K LaTeX keystrokes collected via our Chrome extension. With expert annotations using a novel taxonomy of writing cognitive intentions, it enables LLMs to better mimic the non-linear, intention-driven nature of human writing, advancing cognitively-aligned writing assistants.
We present \textsc{ScholaWrite}, a first-of-its-kind dataset capturing the end-to-end cognitive process of scholarly writing, comprising nearly 62K LaTeX keystrokes collected via our custom-built Chrome extension. This dataset, sourced from ten graduate students with varying levels of scientific writing expertise, is further enriched with expert annotations based on a novel taxonomy of cognitive writing intentions inspired by \citet{f508427a-e4c0-3d6a-8abf-03a5d21ec6c4, koo2023decoding}. Through several experiments, \textsc{ScholaWrite} shows its value for advancing the cognitive capabilities of LLMs and developing cognitively-aligned writing assistants, enabling them to mimic the complex, non-linear, and intention-driven nature of human writing. 

% We introduce \textsc{ScholaWrite}, the first-of-its-kind dataset that captures the end-to-end scholarly writing process, consisting of nearly 62K LaTeX keystrokes collected via our custom-built Chrome extension. This dataset, sourced from ten graduate students with varying levels of scientific writing expertise, is further enriched with expert annotations based on a novel taxonomy of cognitive writing intentions inspired by \citet{f508427a-e4c0-3d6a-8abf-03a5d21ec6c4, koo2023decoding}. Our study demonstrates the value of \textsc{ScholaWrite} as a resource for advancing the cognitive capabilities of large language models (LLMs) and developing cognitively-aligned writing assistants, enabling them to emulate the complex, non-linear, and intention-driven nature of human writing.


% Future work may expand the dataset to diverse academic fields and collaborative projects, thus allowing LLMs to generalize beyond fact recall to emulate human reasoning processes in a more realistic environment. Integrating advanced memory architectures and lifelong learning could also lead LLMs to dynamically adapt to evolving writing intentions and generate high-quality scholarly texts. 
Future work includes expanding the dataset to diverse academic fields, authors, and collaborative projects, thus enabling models to generalize beyond fact recall to emulate human decision-making and reasoning in more realistic academic environments. Additionally, integrating advanced memory architectures and lifelong learning techniques could further enhance LLMs' ability to adapt dynamically to evolving writing intentions and produce coherent, high-quality scholarly outputs.

\section*{Limitations and Ethical Considerations}

We acknowledge several limitations in our study. First, the \textsc{ScholaWrite} dataset is \textbf{currently limited to the computer science domain}, as LaTeX is predominantly used in computer science journals and conferences. This domain-specific focus may restrict the dataset's generalizability to other scientific disciplines. Future work could address this limitation by collecting keystroke data from a broader range of fields with diverse writing conventions and tools, such as the humanities or biological sciences. For example, students in humanities usually write book-length papers and integrate more sources, so it could affect cognitive complexities.

Second, our dataset includes \textbf{contributions from only 10 participants, resulting in five final preprints on arXiv}. This small-to-medium sample size is partly due to privacy concerns, as the dataset captures raw keystrokes that transparently reflect real-time human reasoning. To mitigate these concerns, we removed all personally identifiable information (PII) during post-processing and obtained full IRB approval for the study's procedures. However, the highly transparent nature of keystroke data may still have discouraged broader participation. Future studies could explore more robust data collection protocols, such as advanced anonymization or de-identification techniques, to better address privacy concerns and enable larger-scale participation.
We also call for community-wise collaboration and participation for our next version of our dataset, \textsc{ScholaWrite 2.0} and encourage researchers to contact authors for future participation.

Furthermore, \textbf{all participants were early-career researchers} (e.g., PhD students) at an R1 university in the United States. Expanding the dataset to include senior researchers, such as post-doctoral fellows and professors, could offer valuable insights into how writing strategies and revision behaviors evolve with research experience and expertise.
Despite these limitations, our study captured an end-to-end writing process for 10 unique authors, resulting in a diverse range of writing styles and revision patterns. The dataset contains approximately 62,000 keystrokes, offering fine-grained insights into the human writing process, including detailed editing and drafting actions over time. While the number of articles is limited, the granularity and volume of the data provide a rich resource for understanding writing behaviors. Prior research has shown that detailed keystroke logs, even from small datasets, can effectively model writing processes \cite{leijten2013keystroke, guo2018modeling, vandermeulen2023writing}. Unlike studies focused on final outputs, our dataset enables a process-oriented analysis, emphasizing the cognitive and behavioral patterns underlying scholarly writing.

Third, \textbf{collaborative writing is underrepresented} in our dataset, as only one Overleaf project involved multiple authors. This limits our ability to analyze co-authorship dynamics and collaborative writing practices, which are common in scientific writing. Future work should prioritize collecting multi-author projects to better capture these dynamics. Additionally, the dataset is \textbf{exclusive to English-language writing}, which restricts its applicability to multilingual or non-English writing contexts. Expanding to multilingual settings could reveal unique cognitive and linguistic insights into writing across languages.

Fourth, due to computational and cost constraints, we evaluated the usability of the \textsc{ScholaWrite} dataset with \textbf{a limited number of LLMs and hyperparameter configurations}. As shown in Table \ref{table:intention-prediction}, the Llama-8B-Instruct model demonstrated only marginal improvements after fine-tuning on our dataset. This underscores the need for future research to explore advanced techniques, such as fine-grained prompt engineering, to better align LLM outputs with human writing processes. Specifically, optimizing prompts with clearer contextual guidance (e.g., "before-text" and intention label definitions) may significantly enhance model performance.

Finally, the human evaluation process in Section \ref{sec:appendix:human-eval} was determined as exempt from IRB review by the authors' primary institution, while the data collection using our Chrome extension program was fully approved by the IRB at our institution. Importantly, no LLMs were used during any stage of the study, except for grammatical error correction in this manuscript. 


% \section*{Acknowledgements}
% This work was supported by the research gift from Grammarly. 
% We thank Anna Martin-Boyle and Ryan Koo for their insights and the data collection tool based on their earlier version of this paper.
% We thank the participants of our data collection for their valuable time, effort, and contributions, which were essential to the success of this research.
% We also thank Minnesota NLP group members and anonymous reviewers for providing us with valuable feedback and comments on the paper draft. 


% \section*{Acknowledgments}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix
\input{appendix}
% \input{latex/figure_2_test}
\end{document}
