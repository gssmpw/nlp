\input{taxonomy_table}
\subsection{Writing Intention Annotation}

% To capture comprehensive writing behaviors, we designed a data collection process that involves running a Chrome extension in the background. 
% However, keystroke data alone may be insufficient to fully understand the nuanced intentions behind each writing action. 
To decode the nuanced intentions behind each writing action, we annotate each keystroke collected from the Chrome extension, using a specialized interface. This process is crucial for developing a detailed taxonomy of the end-to-end scholarly writing process, which serves as the basis for annotating the collected keystrokes. 
To perform annotations, we processed those raw keystroke entries by file name, type of writing actions, an array of differences between two subsequent texts, and line number in an Overleaf editor. Please see Appendix \ref{sec:appendix:annotation} for further annotation details.

% Therefore, we perform an additional post-annotation step to understand and predict the underlying intent of the writer behind each keystroke.
% Our goal is to develop a comprehensive taxonomy of the end-to-end scholarly writing process and to annotate the collected keystrokes based on this new taxonomy as part of the \textsc{ScholaWrite} dataset.

\paragraph{Annotation Interface}\label{sec:annotation-interface}

We developed a novel interface for keystroke annotation, enabling visualization by time, LaTeX file, and author (Appendix Figure \ref{fig:annotation-interface}). Annotators can navigate the keystroke timeline, identify a span of related edits (e.g., outlining or grammar correction), and assign a pre-defined intention label from the dropdown menu. 
% The interface also displays metadata for each keystroke alongside the selected label.


% We developed a novel interface for the keystroke annotation process.
% The interface offers various modes for visualizing keystroke collections within each Overleaf project: by time, LaTeX file, and author (Figure \ref{fig:system-flow}(b)).

% In the right panel, annotators can right-click to navigate the timeline of collected keystrokes in different colors (e.g., green for addition). They then identify a range of keystrokes whose underlying purpose or goal is similar when making edits during the writing process (e.g., generating outlines or correcting grammatical errors). They can manually enter the beginning and ending indices and annotate the span by selecting a pre-defined label of the relevant intention from the dropdown menu in the left panel. Furthermore, the interface shows the metadata information of each keystroke, along with the selected label. 


\paragraph{Intention Taxonomy Construction}
Our taxonomy (Table \ref{table:taxonomy-full}) builds on prior literature \cite{f508427a-e4c0-3d6a-8abf-03a5d21ec6c4,  du-etal-2022-understanding-iterative, koo2023decoding}. During annotations, we have identified 15 types of scholarly writing intentions from the five projects and further grouped them into three high-level categories according to \citet{f508427a-e4c0-3d6a-8abf-03a5d21ec6c4} - \colorbox{planningcolor}{Planning}, \colorbox{implementationcolor}{Implementation}, and \colorbox{revisioncolor}{Revision}. 
% Please refer to Table \ref{table:taxonomy} in Appendix \ref{sec:appendix:taxonomy} for the full description of the taxonomy. 

% \begin{table}[t!]
%     \centering
%     \footnotesize
%     \begin{tabular}{@{}p{2cm}|p{5cm}@{}}
%         \toprule
%        \textbf{High-level}  & \textbf{Intentions} \\
%        \midrule
%        \colorbox{planningcolor}{Planning}  & Idea Generation, Idea Organization, Section Planning \\ 
%        \midrule
%        \colorbox{implementationcolor}{Implementation} & Text Production, Object Insertion, Citation Integration, Cross-reference, Macro Insertion \\ 
%        \midrule
%        \colorbox{revisioncolor}{Revision} & Fluency, Coherence, Clarity, Structural, Linguistic Style, Scientific Accuracy, Visual Formatting \\
%        \bottomrule
%     \end{tabular}
%     \caption{A brief view of the taxonomy of scholarly writing process. See the full version of our taxonomy in Appendix Table \ref{table:taxonomy_full}.}
%     \label{table:taxonomy-small}
% \end{table}




Following \citet{pustejovsky2017designing}, we performed an iterative open coding for the collected keystrokes from the two Overleaf projects. 
Using the annotation interface, each annotator reviewed the first few writing keystrokes and identified the high-level processes.
Next, the annotators (1) determine the span of keystrokes that form a meaningful unit together (e.g., a clause or sentence) within the high-level process and (2) decide the underlying intention of edits.
The annotators coded these spans independently, resolving conflicts through multiple in-depth discussions. Finally, a cognitive linguist participated in all discussions, refining the definition and scope of each writing intention. 
% For example, if an author writes down the sentence ``Our findings show significant implications for societies,'' the keystrokes producing this sentence are considered to be a span. Then, if the author deletes the word ``significant'' and replaces it with ``promising'', the keystrokes involved in deleting “significant” and adding “promising” back are considered a span of semantically related keystrokes, which focus on precise word usage. 
% The author’s replacement is then regarded as the granular intention of ''refining language for precise word usage.'' in terms of revising texts. 
% \paragraph{Annotation Process} \minhwa{seems it's too long; I will include only impt details here and move it to appendix}
% \luan{Maybe we should state how many annotators participated and how the inter-reliability scores were calculated?}

To ensure taxonomy validity, we followed two standard practices \cite{nickerson2013method, kundisch2021update}: (1) mutual exclusivity and (2) collective comprehensiveness. Ambiguous keystroke spans were flagged and refined through iterative discussions to sharpen category boundaries. After finalizing the codebook and taxonomy, we updated intention annotations\footnote{The updated list appears in annotation interface.} for the two selected projects and applied the taxonomy to the rest. The final inter-annotator agreement was $0.71$ (weighted F1 score in a multi-label setting for a 1K-keystroke sample\footnote{Unlike Kappa, which assumes categorical agreement and struggles with multi-label classification, weighted F-1 scores better handle class imbalances and partial agreements.}). See Appendix \ref{sec:appendix:annotation} for full annotation details.

% To ensure the validity of our taxonomy, we followed the two standard practices \cite{nickerson2013method, kundisch2021update}: (1) All elements should be mutually exclusive and (2) the taxonomy should be collectively comprehensive. To achieve this, we flagged any ambiguous keystroke spans that could be interpreted as representing multiple intentions. Through iterative discussion, annotators refined these spans' characteristics and improved each category's decision boundaries. Once the codebook and taxonomy were finalized, we updated intention annotations\footnote{The list of those intentions is then updated in the dropdown bar of the annotation interface.} for the selected two projects and applied the taxonomy to the remaining projects. The final inter-annotator agreement is $0.71$ using a weighted F1 score in a multi-label, multi-class setting for a randomly selected window of 1K keystrokes. Please refer to Appendix \ref{sec:appendix:annotation} for more details about the entire annotation procedures.

% \begin{table}[]
%     \centering
%     \begin{tabular}{c|c|c|c|c}
%          &precision&recall&f1-score&support\\
%          \hline
%          Artifact&0.82&0.93&0.87&110\\
% Citation Integration&0.89&0.80&0.84&10\\
%              Clarity&0.59&0.51&0.55&248\\
%            Coherence&0.45&0.67&0.54&36\\
%      Cross-reference&0.64&1.00&0.78&7\\
%              Fluency&0.20&0.20&0.20&5\\
%     Object Insertion&0.00&0.00&0.00&0\\
%  Scientific Accuracy&0.47&0.35&0.40&23\\
%     Section Planning&0.75&0.67&0.71&9\\
%           Structural&0.21&0.43&0.28&14\\
%      Text Production&0.81&0.80&0.81&531\\
%        Textual Style&0.55&0.33&0.41&18\\
%          \hline
%             accuracy&&&0.71&1011\\
%            macro avg&0.53&0.56&0.53&1011\\
%         weighted avg&0.72&0.71&0.71&1011\\
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

%-----------------------------------------------------


\begin{table}[t!]
\centering
\begin{minipage}{\linewidth}
\lstset{
    basicstyle=\ttfamily\footnotesize, % Font and size for the code
    breaklines=true, % Allows breaking of long lines
    frame=single, % Adds a frame around the code block
    columns=fullflexible, % Makes the content fit the column width
    captionpos=b % Places the caption at the bottom
}
\begin{lstlisting}
{
    "Project": 1, # Overleaf Project ID
    "timestamp": 1702958491535, # recorded time when change was made
    "author": "1",
    "before text": "One important expct of studying LLMs is ..",
    "after text": "One important aspect of studying LLMs is ..",
    "label": "Fluency",
    "high-level": "REVISION"
}
\end{lstlisting}
\vspace{-3mm}
\captionof{lstlisting}{An example entry of the post-processed data. Each data entry represents an array of a keystroke change (or edits) with metadata information and a label of annotated intention.}
\label{table:single-entry}
\end{minipage}
\end{table}

\paragraph{Post-processing}

% To encourage public use like using \textsc{ScholaWrite} for LLM finetuning, we conducted several steps of post-processing to increase the usability of our annotated keystroke collection and mitigate any privacy concerns (See details in Appendix \ref{sec:appendix:postprocess}). 
% Listing \ref{table:single-entry} shows an example data entry in our dataset. The example illustrates a simple edit where the author corrected ``expct'' to ``aspect'' as grammatical error correction. The intention behind it is to improve the fluency of the sentence, which we annotated as ``fluency'' according to our taxonomy. 

To encourage public use, such as fine-tuning LLMs with \textsc{ScholaWrite}, we applied post-processing to increase usability and mitigate privacy concerns (see Appendix \ref{sec:appendix:postprocess}). Listing \ref{table:single-entry} presents a sample data entry, where the author corrects ``expct'' to ``aspect'' for grammatical accuracy. This edit, aimed at improving fluency, is annotated as ``fluency'' in our taxonomy.

