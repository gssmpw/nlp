% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% MY PACKAGES
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[most]{tcolorbox}
\definecolor{bluechart}{rgb}{0.00392156862745098, 0.45098039215686275, 0.6980392156862745}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\usepackage{array}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand{\specialcell}[2][c]{%
    \begin{tabular}[c]{@{}#1@{}}#2\end{tabular}}%
\usepackage{balance}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\gf}[1]{\footnote{\textbf{Giorgio: #1}}}
\newcommand{\mm}[1]{\footnote{\textbf{Mirco:#1}}}

%\title{DiffSampling: Different and Valuable Neural Text Generation through Mathematical Analysis}
\title{DiffSampling: Enhancing Diversity and Accuracy \\in Neural Text Generation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Giorgio Franceschelli \\
  University of Bologna \\
  \texttt{giorgio.franceschelli@unibo.it} \\\And
  Mirco Musolesi \\
  University College London \\
  University of Bologna \\
  \texttt{m.musolesi@ucl.ac.uk} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity.

\end{abstract}

\section{Introduction} \label{intro}

In recent years, large language models (LLMs) have achieved remarkable performance \cite{bubeck2023sparks}, driven by large-scale data sets, increasing computing capacity \citep{bommasani2021opportunities}, and innovative learning strategies (e.g., \citealp{stiennon2020learning,rafailov2023direct}).
While training provides LLMs with the information and skills required to process natural language, another aspect plays a key role at generation time: the decoding strategy, i.e., how we extract text strings from the model.
%Since a typical language model returns a probability distribution for the next token given the previous ones, 
The most straightforward strategies, such as greedy decoding (always selecting the highest-probability token) or sampling, tend to repeat the same tokens multiple times \cite{su2022contrastive}, reproduce training data \cite{carlini2021extracting,franceschelli2024training}, or flatten the lexicon in favor of the most common grammatical structures and words \cite{fleisig2024linguistic,reviriego2023playing}. Although the temperature parameter may increase the likelihood of less frequent tokens, it also raises the chance of syntactically incorrect ones by flattening their probabilities, regardless of their actual ranking.
\begin{figure}[t]
    \centering
    \includegraphics[width=.48\textwidth]{images/DiffSampling_labels.pdf}
    \caption{A graphical representation of the effects of our \textit{DiffSampling} methods. In the top-left square, the original (sorted) distribution. In the top-right square, \textit{DiffSampling-cut} truncates after the minimum discrete derivative. In the bottom-left square, \textit{DiffSampling-lb} also imposes a total probability lower bound (in this example, equal to 0.5). In the bottom-right square, \textit{DiffSampling-reparam} also reparameterizes the probabilities by subtracting their discrete derivatives.}
    \label{fig:example_diffsampling}
\end{figure}
%
An ideal solution should concentrate on where the \textit{critical mass} of the probability distribution resides. More precisely, with critical mass we refer here to the portion of the probability distribution that collectively accounts for the majority of the probability mass of the tokens. In this direction, a common approach is nucleus sampling \cite{holtzman2020curious}, which removes the tail of the probability distribution by focusing on the smallest subset of tokens whose global probability exceeds a given threshold. However, key issues remain: first, nucleus sampling is sensitive to the choice of the threshold; second, it can either preserve incorrect tokens or exclude appropriate ones if the critical mass is smaller or larger than the threshold, respectively.
%Second, certain peculiar situations might be handled incorrectly. For example, if there is one token with a very high probability (but smaller than the threshold), it is likely to be the only appropriate one; yet, nucleus sampling will still preserve some other tokens, potentially leading to errors in generation. Another case is when we have several equally probable tokens whose total probability exceeds the threshold: nucleus sampling will exclude some correct tokens, reducing the chance of diversifying the final result.
%
%
%
As suggested by \citet{hewitt2022truncation}, the learned probability distribution can be viewed as a mixture of the true distribution, which assigns a non-zero probability only to appropriate tokens (the critical mass), and a smoothing distribution, which assigns a small but non-zero probability to incorrect tokens. This smoothing is necessary for learning purposes.

In this paper, we introduce a family of sampling strategies called \textit{DiffSampling}, based on the analysis of the probability distribution of the tokens. In particular, these decoding methods are based on the identification of the critical mass of the token probability distribution through the minimum discrete derivative (i.e., the largest difference between consecutive probabilities in a sorted distribution). We propose three methods to exclude incorrect tokens or increase less probable but appropriate tokens (see Figure \ref{fig:example_diffsampling}).
We then provide a comprehensive evaluation of these decoding solutions considering three different tasks (namely, math problem solving, extreme summarization, and the so-called divergent association task \cite{chen2023probing}) and discuss their advantages and limitations. We show that \textit{DiffSampling} consistently performs better in either quality or diversity.

The remainder of this paper is structured as follows. First, we introduce the decoding problem from a neural language model and discuss existing approaches (Section \ref{background}). Then, we present our discrete derivative-based sampling strategy and three different methods to employ it (Section \ref{method}). Finally, in Section \ref{experiments} we evaluate our methods on mathematical problem-solving tasks, extreme summarization, and the divergent association task against the most common baselines, finding that \textit{DiffSampling} is a simple yet effective way to generate appropriate and diverse text.


\section{Background} \label{background} % OR BACKGROUND WITH ALSO A SMALL INTRO TO NEURAL LANGUAGE MODELS

\subsection{Language Modeling}

An autoregressive language model is a probability distribution $p_{\boldsymbol{\theta}}(\mathbf{x})$ parameterized by $\boldsymbol{\theta}$ over a variable-length text sequence $\mathbf{x} = (x_1 \ldots x_T)$, where $T$ is the sequence length and each token $x_t$ is in a finite vocabulary $\mathcal{V}$ of size $N$. The probability distribution is factorized as $p_{\boldsymbol{\theta}}(\mathbf{x}) = \prod_{t=1}^T p_{\boldsymbol{\theta}}(x_t | x_1 \ldots x_{t-1})$, and the language model is usually trained to maximize the likelihood of the true distribution $p^*(\mathbf{x})$ for any $\mathbf{x}$ from a reference dataset (the training set). In other words, given in input $x_1 \ldots x_t$, the model learns to approximate the probability of each token from $\mathcal{V}$ being $x_{t+1}$. While this makes such a model immediately capable of scoring the probability of a given text, it also allows for the generation of new sentences. Given a commonly human-written prefix (also known as a prompt) $\mathbf{x} = (x_1 \ldots x_P)$ of length $P$, we can decode a continuation $\mathbf{\hat{x}} = (x_{P+1} \ldots x_{T+P})$ from the language model through its factorized representation introduced before. However, we must remember that the language model is trained to score and not to generate sentences. Whereas a certain text might have zero probability for generation purposes (e.g., the text is syntactically incorrect), it might have non-zero probability for ranking purposes \cite{hewitt2022truncation}.

\subsection{Decoding Strategies}

The decoding of tokens from the probability distribution learned by a neural language model can occur in several ways. The greedy strategy involves selecting the most probable token each time. However, this can lead to a consistent lack of diversity and several repetitions. The standard approach involves sampling from the probability distribution, which can be transformed through a \textit{temperature} parameter. The temperature scales the differences among the various probabilities: a temperature lower than 1 will increase the probability of the most-probable tokens (a zero temperature degenerates to greedy strategy), while a temperature higher than 1 will increase the probability of the least-probable tokens, allowing for more diversity in generation \cite{peeperkorn2024temperature}. However, this might lead to the selection of tokens that are not syntactically appropriate for the current input. Top-$k$ sampling \cite{fan2018hierarchical} reduces the token space to the $k$ most probable ones. 

To generate more natural and coherent solutions, contrastive search \cite{su2022contrastive} employs a top-$k$ sampling method combined with a degeneration penalty. This promotes the selection of tokens that differ from those already generated, enhancing the diversity and quality of the output. Nevertheless, limiting the number of selected tokens a priori can lead to the exclusion of meaningful tokens or the inclusion of inappropriate ones. A possible solution is to set $k$ dynamically, as in Mirostat \cite{basu2021mirostat}: to maintain the perplexity of generated text at a desired value, the $k$ parameter is actively tuned based on the current cross-entropy.

Alternatively, nucleus (or top-$p$) sampling \cite{holtzman2020curious} reduces the token space to the smallest subset of tokens with a total probability no less than $p$. To restrict the nucleus to tokens whose information content is close to the expected one given prior context and avoid repetitions, locally typical sampling \cite{meister2023locally} focuses on the tokens with negative log-probability within a certain absolute range from the conditional entropy (and a total probability no less than $p$). Finally, \citet{hewitt2022truncation} assert that a neural language model learns a mixture of the true token distribution and a smoothing distribution to avoid infinite perplexity; therefore, the sampling scheme should aim at \textit{de-smoothing} the distribution. For doing so, they propose $\epsilon$- and $\eta$-sampling, which truncate tokens with a probability smaller than a threshold set a priori or dynamically through the entropy of the distribution, respectively. While similar in spirit, our method simplifies threshold computation and reparameterizes the distribution to promote less probable but correct tokens.


\section{DiffSampling} \label{method}

Given the probability distribution of the next token, let us imagine sorting it to have tokens in descending order based on their probability. The critical mass of the distribution can be determined by identifying the largest difference between probabilities: the token to its left should be the least probable token that our model still considers correct, i.e., the one that we might want to generate to produce an output that is both appropriate and diverse.

From a mathematical analysis perspective, this point is characterized simply and elegantly as the location where the derivative reaches its minimum. Let us consider a probability distribution $p\!\left(x\right)$ defined for a limited number of $x_1 \ldots x_N$, with $p\!\left(\right)$ monotonically decreasing. According to the forward difference approximation, the discrete derivative of a function $f\!\left(x_n\right)$ is defined as $\Delta f\!\left(x_n\right) = f\!\left(x_{n+1}\right) - f\!\left(x_n\right)$, thus we have:
%
\begin{equation}
    \Delta p\!\left(x_n\right) = 
        \begin{cases}
        p\!\left(x_{n+1}\right) - p\!\left(x_n\right) & \text{if } n < N \\
        - p\!\left(x_n\right) & \text{if } n = N
        \end{cases}
\end{equation}
%
\noindent which is always non-positive. $\mathtt{argmin}(\Delta p\!\left(x_n\right))$ represents the index of the last token before the point characterized by the largest difference between probabilities.

Starting from $\Delta p\!\left(x_n\right)$, we propose \textit{DiffSampling}, a family of different decoding strategies. The first one, which we call \textit{DiffSampling-cut}, consists of cutting the tail at the right side of the point characterized by the largest difference between the probabilities, i.e., sampling among the tokens $x_i, i \leq \mathtt{argmin}(\Delta p\!\left(x_n\right))$. This approach can be seen as an improved greedy strategy: when the model has high confidence in a single token, it degenerates into the greedy strategy; otherwise, it preserves other appropriate tokens, increasing the diversity of the final result.

However, there might be situations in which some of the excluded tokens are still correct; for example, the first token might minimize $\Delta p\!\left(x_n\right)$, but still have a quite low probability, i.e., it does not \textit{really} cover the entire critical mass. To address this issue, \textit{DiffSampling-lb} introduces a lower bound on the mass probability. To leverage the advantage of our cutting strategy while maintaining that of nucleus sampling, our second strategy considers truncating based on $\Delta p\!\left(x_n\right)$ in such a way that the resulting tokens have a total probability at least equal to the lower bound $p_{lb}$. In other words, given $k$ cardinality of the smallest subset of tokens whose total probability is not lower than $p_{lb}$, it computes the $\mathtt{argmin}(\Delta p\!\left(x_n\right))$ for $n \geq k$. This approach can be seen as an improved nucleus sampling: it \textit{corrects} the $p$ parameter of nucleus sampling via our derivative-based approach to include correct tokens after the selected nucleus. Reintroducing this parameter is sensible, and our findings indicate that setting it to 0.8 results in minimal accuracy loss while significantly enhancing diversity compared to lower values (see Appendix \ref{abl_lb}).


Finally, our third strategy aims to transform the probability distribution to prevent the most frequent tokens in the training set from being the most probable according to the model. This is important because consistently selecting the most probable token can lead to repetitive and less diverse outputs, reducing the overall quality and creativity of the generated text. By increasing diversity and reducing the risk of accidental reproduction, we can produce more varied and interesting results. \textit{DiffSampling-reparam} achieves this by adjusting the model's probabilities toward options with the smallest derivatives. It does so by adding the negative of these derivatives to the original probabilities, scaled by a multiplier $\gamma$ as follows:
%
\begin{equation}
    p'\!\left(x_n\right) = p\!\left(x_n\right) - \gamma \Delta p\!\left(x_n\right).
\end{equation}

This, combined with the cutting and lower-bound strategies, enhances novelty while maintaining the appropriateness of responses. This approach can be seen as an alternative temperature: while a higher temperature still preserves the most probable tokens as such (and introducing nucleus sampling has the limitations mentioned above), our strategy increases the probability of tokens before a ``jump'', which are less likely to be sampled. However, the $\gamma$ parameter has a different behavior than temperature. With $\gamma = 0$, we fall back to \textit{DiffSampling-lb}. With a very large $\gamma$, the sampling scheme becomes deterministic, greedily selecting the token with the minimum derivative. Conversely, with a small value, such as $\gamma = 1$, the scheme promotes that token while still maintaining the original distribution.

Overall, \textit{DiffSampling} can be seen as a sampling scheme governed by two parameters, i.e., the probability-mass lower bound $p_{lb}$ and the reparameterization factor $\gamma$. The full algorithm is reported in Algorithm \ref{alg:diffsampling}.


\begin{algorithm}[ht] 
    \caption{DiffSampling} \label{alg:diffsampling}
    \begin{algorithmic}
        \State \textbf{Input:} probabilities $\mathsf{probs} = [p_1 \ldots p_N]$, lower bound $p_{lb}$, multiplier $\gamma$.
        \State $\mathsf{sorted\_probs}, \mathsf{indices} = \mathtt{sort}\!\left(\mathsf{probs}\right)$
        \State $\mathsf{fwd\_probs} = \mathsf{sorted\_probs}[1\!:] + [0.]$
        \State $\mathsf{delta\_probs} = \mathsf{fwd\_probs} - \mathsf{sorted\_probs}$
        \State $\mathsf{nucleus} = \mathtt{cumsum}\!\left(\mathsf{sorted\_probs}\right) < p_{lb}$
        \State $\mathsf{sorted\_probs} = \mathsf{sorted\_probs} - \gamma \cdot \mathsf{delta\_probs}$
        \State $\mathsf{delta\_probs}[\mathsf{nucleus}] = 0.$
        \State $d = \mathtt{argmin}\!\left(\mathsf{delta\_probs}\right)$
        %\State $\mathsf{sorted\_probs} = \mathsf{sorted\_probs} - \gamma \cdot \mathsf{delta\_probs}$
        \State $\mathsf{sorted\_probs}[d\!+\!1\!:] = 0.$
        \State $\mathsf{probs} = \mathtt{sort\_by\_idx}\!\left(\mathsf{sorted\_probs}, \mathsf{indices}\right)$
        \State $\mathsf{probs} = \mathsf{probs} / \mathtt{sum}\!\left(\mathsf{probs}\right)$
        \State \textbf{Output:} $\mathsf{probs}$
    \end{algorithmic}
\end{algorithm}


\section{Experiments} \label{experiments}
To evaluate whether \textit{DiffSampling} helps diversify outputs while maintaining a high level of accuracy, we test it on three case studies: math problem solving, text summarization, and the divergent association task\footnote{The code used for the experiments can be downloaded from the following URL: \textit{[we will make the URL available in the camera-ready version of the paper if accepted]}.}.

%\url{https://github.com/giorgiofranceschelli/DiffSampling}.

\subsection{Models and Baselines}
In all our experiments, we start from a state-of-the-art LLM and test various decoding strategies. For the math problem-solving tasks, we use the Llama2-based MetaMath model trained with self-supervised learning on MetaMathQA \cite{yu2024metamath}. Following \citet{chhabra2024revisiting}, for extreme text summarization we use the Llama2-7B model \cite{touvron2023llama2}, considering both RLHF-instructed and pre-trained versions. Finally, for the divergent association task, we consider Llama3-8B \cite{grattafiori2024llama3}, using both DPO-tuned and pre-trained versions. We study the performances of our three methods: \textit{DiffSampling-cut}; \textit{DiffSampling-lb} with $p_{lb} = 0.8$; \textit{DiffSampling-reparam} with $p_{lb}= 1.0, \gamma = 1.0$. We compare them with a total of 7 baselines: greedy strategy (temperature equal to $0$); contrastive search (with the number of top $k$ samples equal to 8 and the scaling factor of the degeration penalty $\alpha = 0.6$); $\eta$-sampling (with $\eta = 0.0003$); locally typical sampling (with $p = 0.9$); nucleus sampling (with $p = 0.9$); nucleus sampling with a higher temperature of $1.5$ and $2.0$. When not specified, temperature has been set to $1$. We also experiment with different $p_{lb}$ and $\gamma$ values; results are presented and discussed in Appendix \ref{additional_experiments}.

\subsection{Math Problem Solving} \label{math_experiments}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
\textcolor{white}{placeholder} & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
Greedy                   & $95.27 \pm 0.17$ & $1.67 \pm 0.01$ & $0.72 \pm 0.02$ & - & - \\
Contrastive search       & $94.17 \pm 0.45$ & $1.68 \pm 0.01$ & $0.72 \pm 0.01$ & $0.14 \pm 0.0$ & $0.26 \pm 0.01$ \\
$\eta$-sampling          & $89.17 \pm 0.52$ & $1.73 \pm 0.01$ & $0.72 \pm 0.01$ & $0.21 \pm 0.01$ & $0.37 \pm 0.01$ \\
Locally typical sampling & $91.70 \pm 0.62$ & $1.71 \pm 0.01$ & $0.72 \pm 0.01$ & $0.19 \pm 0.01$ & $0.34 \pm 0.01$ \\
Nucleus sampling $t$=1.0 & $91.70 \pm 0.62$ & $1.71 \pm 0.01$ & $0.72 \pm 0.01$ & $0.19 \pm 0.01$ & $0.34 \pm 0.01$ \\
Nucleus sampling $t$=1.5 & $87.63 \pm 0.90$ & $1.75 \pm 0.00$ & $0.72 \pm 0.02$ & $0.24 \pm 0.01$ & $0.41 \pm 0.01$ \\
Nucleus sampling $t$=2.0 & $30.17 \pm 0.76$ & $8.30 \pm 0.11$ & $0.58 \pm 0.02$ & $0.7 \pm 0.01$ & $0.62 \pm 0.01$ \\
DiffSampling-cut         & $94.70 \pm 0.21$ & $1.68 \pm 0.01$ & $0.73 \pm 0.02$ & $0.11 \pm 0.00$ & $0.22 \pm 0.01$ \\
DiffSampling-lb          & $92.97 \pm 0.12$ & $1.70 \pm 0.01$ & $0.72 \pm 0.01$ & $0.17 \pm 0.01$ & $0.31 \pm 0.01$ \\
DiffSampling-reparam     & $89.67 \pm 0.15$ & $1.73 \pm 0.01$ & $0.73 \pm 0.01$ & $0.22 \pm 0.01$ & $0.38 \pm 0.01$ \\

 \hline
\end{tabular}
}
\caption{Accuracy (i.e., percentage of solved problems) and diversity of results for the training set over 3 seeds. 
The mean and standard error of the final score for each run are reported for accuracy and cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for against-greedy diversity.
\label{diffsampling_metamath}}
\end{table*}

\noindent \subsubsection{Experimental Setup}
Solving math problems provides a useful case study for our decoding strategies, as it allows us to evaluate the correctness of solutions (as the percentage of correctly solved problems) and the diversity of procedures to arrive at the result. 
To better understand whether our methods can increase diversity while maintaining accuracy we consider both the MetaMathQA training set \cite{yu2024metamath} and the GSM8K \cite{cobbe2021training} and MATH \cite{hendrycks2021measuring} test sets; the relative prompts are reported in Appendix \ref{diffsampling_implementation_details}. To avoid resource wasting, we focus on entries with a problem and a solution whose tokenized versions are no longer than 512. Since the training set is incredibly vast (395k entries), we limit our experiment to 1000 random samples, while we consider all 1319 entries from the GSM8K test set and all the filtered 4545 entries from the MATH test set. 

We evaluate the quality of solutions through the ratio of correctly solved problems. Instead, the diversity is computed according to two methods: expectation-adjusted distinct N-grams (EAD) \cite{liu2022rethinking} and sentence embedding cosine diversity (Sent-BERT) \cite{hong2024curiositydriven}, which should evaluate syntactic and semantic diversity, respectively \cite{kirk2024understanding}. EAD counts the number of distinct N-grams tokens (averaging over $N=1 \ldots 5$) and removes the bias toward shorter inputs by scaling the number of distinct tokens based on their expectations. The Sent-BERT metric computes the cosine similarity between the embeddings of the sentences and returns 1 minus the similarity. While originally based on Sentence-BERT \cite{reimers2019sentence}, we employ the more recent all-mpnet-base-v2 to obtain the embeddings, as suggested by their developers\footnote{\url{https://huggingface.co/sentence-transformers/bert-large-nli-stsb-mean-tokens}}. Following \citet{kirk2024understanding}, we compute \textit{cross-input} EAD and Sent-BERT, i.e., by considering the set of all outputs produced for a specific seed. In addition, we also compute \textit{against-greedy} EAD and Sent-BERT. Given each input, we compare the output with the greedy one by calculating the average expectation-adjusted distinct N-grams not present in the greedy response, and 1 minus the cosine similarity between the two outputs, respectively.


\noindent \subsubsection{Experimental Results}
Table \ref{diffsampling_metamath} reports the aggregate results of all the tested methods over the sampled portion of the math training data. As evident by the results, the MetaMath model's performance depends on the greediness of the decoding strategy: the greedy one achieves the highest accuracy, closely followed by \textit{DiffSampling-cut}. Interestingly, both \textit{DiffSampling-lb} and \textit{DiffSampling-reparam} perform better than their most similar baselines while having similar cross-input diversity. On the other hand, the against-greedy diversity scores are inversely correlated with accuracy, since the greedy strategy appears to be the optimal one.


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
\textcolor{white}{placeholder} & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
Greedy                   & $66.44 \pm 0.09$ & $2.03 \pm 0.00$  & $0.72 \pm 0.00$ & - & - \\
Contrastive search       & $65.88 \pm 0.59$ & $2.06 \pm 0.00$  & $0.72 \pm 0.00$ & $0.17 \pm 0.00$ & $0.38 \pm 0.01$ \\
$\eta$-sampling          & $65.05 \pm 0.19$ & $2.12 \pm 0.00$  & $0.73 \pm 0.00$ & $0.25 \pm 0.00$ & $0.48 \pm 0.01$ \\
Locally typical sampling & $66.29 \pm 0.55$ & $2.09 \pm 0.00$  & $0.74 \pm 0.01$ & $0.23 \pm 0.00$ & $0.46 \pm 0.01$ \\
Nucleus sampling $t$=1.0 & $65.00 \pm 0.18$ & $2.08 \pm 0.01$  & $0.73 \pm 0.01$ & $0.23 \pm 0.00$ & $0.46 \pm 0.01$ \\
Nucleus sampling $t$=1.5 & $63.91 \pm 0.57$ & $2.17 \pm 0.01$  & $0.73 \pm 0.01$ & $0.28 \pm 0.00$ & $0.52 \pm 0.01$ \\
Nucleus sampling $t$=2.0 & $25.40 \pm 0.07$ & $10.13 \pm 0.10$ & $0.63 \pm 0.00$ & $0.70 \pm 0.01$ & $0.70 \pm 0.01$ \\
DiffSampling-cut         & $67.10 \pm 0.19$ & $2.04 \pm 0.00$  & $0.73 \pm 0.00$ & $0.14 \pm 0.00$ & $0.33 \pm 0.01$ \\
DiffSampling-lb          & $66.87 \pm 0.16$ & $2.07 \pm 0.00$  & $0.73 \pm 0.00$ & $0.21 \pm 0.00$ & $0.43 \pm 0.01$ \\
DiffSampling-reparam     & $64.62 \pm 0.13$ & $2.12 \pm 0.01$  & $0.72 \pm 0.00$ & $0.25 \pm 0.00$ & $0.49 \pm 0.01$ \\
 \hline
\end{tabular}
}
\caption{Accuracy (i.e., percentage of solved problems) and diversity of results for the GSM8K test set over 3 seeds. 
%The best scores are in \textbf{bold}, while the worst are in \underline{underline}. 
The mean and standard error of the final score for each run are reported for accuracy and cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for against-greedy diversity.
\label{diffsampling_gsm8k}}
\end{table*}



Table \ref{diffsampling_gsm8k} reports the results for the GSM8K test set. Differently from the results above, here the greedy strategy does not provide strong advantages, and our cutting strategy achieves the highest percentage of solved problems. As far as diversity is considered, \textit{DiffSampling-cut} remains the closest to greedy, but with slight improvements in diversity; instead, \textit{DiffSampling-lb} has slightly worse scores than similar approaches, but with gains in accuracy. Finally, it is interesting to note that increasing temperature has dramatic effects on the accuracy of solutions, and does not have significant advantages in terms of semantic, cross-input diversity.


The results for the MATH test set are similar, as reported in Table \ref{diffsampling_math}. Both contrastive search and \textit{DiffSampling-cut} have higher accuracy than greedy while having the lowest diversity scores. However, \textit{DiffSampling-lb} achieves slightly higher accuracy than greedy without consequences on diversity, which is aligned with similar techniques. Again, a higher temperature leads to poor results in terms of correctness and semantic cross-input diversity, while \textit{DiffSampling-param} still maintains a decent level of accuracy.
%
%
\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
\textcolor{white}{placeholder} & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
Greedy                   & $20.62 \pm 0.01$ & $5.65 \pm 0.00$  & $0.67 \pm 0.00$ & - & - \\
Contrastive search       & $21.05 \pm 0.14$ & $5.82 \pm 0.01$  & $0.67 \pm 0.00$ & $0.31 \pm 0.00$ & $0.42 \pm 0.00$ \\
$\eta$-sampling          & $19.67 \pm 0.20$ & $6.36 \pm 0.01$  & $0.66 \pm 0.00$ & $0.39 \pm 0.00$ & $0.48 \pm 0.00$ \\
Locally typical sampling & $19.95 \pm 0.26$ & $6.06 \pm 0.01$  & $0.68 \pm 0.00$ & $0.36 \pm 0.00$ & $0.47 \pm 0.00$ \\
Nucleus sampling $t$=1.0 & $20.02 \pm 0.12$ & $6.08 \pm 0.02$  & $0.67 \pm 0.00$ & $0.36 \pm 0.00$ & $0.47 \pm 0.00$ \\
Nucleus sampling $t$=1.5 & $18.38 \pm 0.22$ & $6.92 \pm 0.02$  & $0.67 \pm 0.00$ & $0.42 \pm 0.00$ & $0.50 \pm 0.00$ \\
Nucleus sampling $t$=2.0 & $2.49 \pm 0.01$  & $48.71 \pm 0.08$ & $0.39 \pm 0.00$ & $0.92 \pm 0.00$ & $0.62 \pm 0.00$ \\
DiffSampling-cut         & $21.06 \pm 0.13$ & $5.72 \pm 0.01$  & $0.66 \pm 0.00$ & $0.27 \pm 0.00$ & $0.37 \pm 0.00$ \\
DiffSampling-lb          & $20.91 \pm 0.24$ & $5.97 \pm 0.01$  & $0.67 \pm 0.00$ & $0.35 \pm 0.00$ & $0.45 \pm 0.00$ \\
DiffSampling-reparam     & $19.38 \pm 0.12$ & $6.38 \pm 0.02$  & $0.67 \pm 0.00$ & $0.39 \pm 0.00$ & $0.48 \pm 0.00$ \\
 \hline
\end{tabular}
}
\caption{Accuracy (i.e., percentage of solved problems) and diversity of results for the MATH test set over 3 seeds. 
%The best scores are in \textbf{bold}, while the worst are in \underline{underline}. 
The mean and standard error of the final score for each run are reported for accuracy and cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for against-greedy diversity.
\label{diffsampling_math}}
\end{table*}
%
%
%
\subsection{Extreme Summarization} \label{xsum_experiments}
%
\noindent \subsubsection{Experimental Setup}
Summarizing paragraphs and longer text represents another meaningful case study since the same text can be correctly outlined in different ways. To keep the resource consumption as low as possible, we consider the eXtreme Summarization (XSum) dataset \cite{narayan2018dont}, which contains pairs of documents and one-sentence summaries. In particular, we use the test partition (11334 entries) and exclude all entries with a tokenized document longer than 768, obtaining 9815 entries; then, we limit our experiment to 1000 random samples, and we use the prompt suggested by \citet{chhabra2024revisiting} and reported in Appendix \ref{diffsampling_implementation_details}.
Again, we aim to verify whether the summaries generated with \textit{DiffSampling} are more diverse while maintaining a competitive quality. For diversity, we consider the same metrics presented in Section \ref{math_experiments}, i.e., EAD and Sent-BERT for both cross-input and against-greedy diversity. For quality assessment, we use ROUGE-1 \cite{lin2004rouge}, a standard metric for summarization that evaluates the ratio of 1-grams present in both the target and generated summaries, as well as the sentence embedding cosine similarity between the generated and target summaries. In this way, we compute both syntactic and semantic quality metrics, as a good summary might use entirely different words while still preserving the original text's meaning.

\noindent \subsubsection{Experimental Results}
In terms of ROUGE-1 and similarity performances, the results for the instructed model are not significant, and all strategies achieve the same score apart from those with higher temperatures. As far as diversity is considered, the results are coherent with what was discussed before: \textit{DiffSampling-cut} is placed between greedy and contrastive search; \textit{DiffSampling-lb} between contrastive search and other nucleus-based methods; and \textit{DiffSampling-reparam} between nucleus sampling with temperature equals to 1 and higher temperatures. Table \ref{diffsampling_xsum} reports the aggregate results.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & \multicolumn{2}{c|}{Quality} & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & ROUGE-1 $\uparrow$ & Similarity $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
 Greedy                   & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & - & - \\
 Contrastive search       & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.21 \pm 0.01$ & $0.27 \pm 0.01$ \\
 $\eta$-sampling          & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.33 \pm 0.01$ & $0.40 \pm 0.01$ \\
 Locally typical sampling & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.00$ & $0.92 \pm 0.00$ & $0.30 \pm 0.01$ & $0.37 \pm 0.01$ \\
 Nucleus sampling $t$=1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.00$ & $0.92 \pm 0.00$ & $0.30 \pm 0.01$ & $0.37 \pm 0.01$ \\
 Nucleus sampling $t$=1.5 & $0.21 \pm 0.00$ & $0.13 \pm 0.01$ & $1.33 \pm 0.01$ & $0.92 \pm 0.00$ & $0.41 \pm 0.01$ & $0.48 \pm 0.01$ \\
 Nucleus sampling $t$=2.0 & $0.10 \pm 0.00$ & $0.08 \pm 0.01$ & $2.23 \pm 0.01$ & $0.74 \pm 0.01$ & $0.78 \pm 0.01$ & $0.78 \pm 0.01$ \\
 \hline
 DiffSampling-cut         & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 DiffSampling-lb          & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.91 \pm 0.00$ & $0.27 \pm 0.01$ & $0.33 \pm 0.01$ \\
 DiffSampling-reparam     & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.34 \pm 0.01$ & $0.41 \pm 0.01$ \\
 \hline
\end{tabular}
}
\caption{Aggregate results for the RLHF-instructed model over 3 seeds for the XSum dataset. Quality metrics are computed against the target summary from the test set. Diversity metrics are computed across all the generated answers (left) and the answer sampled with a greedy strategy (right). The mean and standard error of the final score for each run are reported for cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for ROUGE-1, similarity, and against-greedy diversity. \label{diffsampling_xsum}}
\end{table*}
%
%
These considerations find confirmation when the non-instructed model is considered. As reported in Table \ref{diffsampling_xsum_rough}, \textit{DiffSampling-cut} behaves close to greedy strategy, but with a slight increase in diversity; while \textit{DiffSampling-lb} trades off diversity in favor of accuracy with respect to other nucleus-based approaches. However, \textit{DiffSampling-reparam} performs more likely as $\eta$-sampling than higher temperatures, as they rapidly lose accuracy and semantic diversity in favor of syntactic one. 

\begin{table*}[th]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{4.0cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 Method & \multicolumn{2}{c|}{Quality} & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & ROUGE-1 $\uparrow$ & Similarity $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
 Greedy                   & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & - & - \\
 Contrastive search       & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.14 \pm 0.00$ & $0.92 \pm 0.00$ & $0.45 \pm 0.01$ & $0.50 \pm 0.02$ \\
 $\eta$-sampling          & $0.15 \pm 0.00$ & $0.11 \pm 0.01$ & $1.19 \pm 0.01$ & $0.91 \pm 0.00$ & $0.78 \pm 0.01$ & $0.80 \pm 0.01$ \\
 Locally typical sampling & $0.16 \pm 0.00$ & $0.11 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.75 \pm 0.01$ & $0.79 \pm 0.01$ \\
 Nucleus sampling w t=1.0 & $0.16 \pm 0.00$ & $0.11 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.75 \pm 0.01$ & $0.79 \pm 0.01$ \\
 Nucleus sampling w t=1.5 & $0.04 \pm 0.00$ & $0.07 \pm 0.01$ & $2.32 \pm 0.00$ & $0.73 \pm 0.02$ & $0.96 \pm 0.00$ & $0.92 \pm 0.01$ \\
 Nucleus sampling w t=2.0 & $0.01 \pm 0.00$ & $0.07 \pm 0.01$ & $3.07 \pm 0.01$ & $0.44 \pm 0.02$ & $0.98 \pm 0.00$ & $0.92 \pm 0.01$ \\
 \hline
 DiffSampling-cut         & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.25 \pm 0.01$ & $0.28 \pm 0.01$ \\
 DiffSampling-lb          & $0.17 \pm 0.00$ & $0.12 \pm 0.01$ & $1.15 \pm 0.01$ & $0.91 \pm 0.01$ & $0.72 \pm 0.01$ & $0.75 \pm 0.01$ \\
 DiffSampling-reparam     & $0.15 \pm 0.00$ & $0.11 \pm 0.01$ & $1.17 \pm 0.01$ & $0.91 \pm 0.01$ & $0.77 \pm 0.01$ & $0.80 \pm 0.01$ \\
 \hline
\end{tabular}
}
\caption{Aggregate results for the pre-trained, non-instructed model over 3 seeds for the XSum dataset. Quality metrics are computed against the target summary from the test set. Diversity metrics are computed across all the generated answers (left) and the answer sampled with a greedy strategy (right). The mean and standard error of the final score for each run are reported for cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for ROUGE-1, similarity, and against-greedy diversity. \label{diffsampling_xsum_rough}}
\end{table*}



\subsection{Divergent Association Task} \label{dat_experiments}

\noindent \subsubsection{Experimental Setup}
The last use case considers the divergent association task (DAT) \cite{chen2023probing}. Building on the theory that creativity is related to the capability of generating more divergent ideas \cite{beaty2014roles}, it requires participants to name unrelated words. Then, the semantic distance between them can represent an objective measure of divergent thinking \cite{olson2021naming}. DAT represents a useful case study for decoding strategies as it constrains the generation to different nouns (thus, assuming an optimal probability distribution, the tail due to smoothing should contain everything else) and requires generating terms that are as different as possible, which is quite the opposite to what typically happens in language modeling: an optimal strategy should exclude non-appropriate tokens but also not limit too much the space of possible tokens. 
More concretely, given the embeddings of $n$ words, the DAT score is the average cosine distance between each pair of embeddings (then scaled as a percentage). Following the original paper, we use GloVe embeddings \cite{pennington2014glove} and ask the model to generate a list of 10 nouns. We discard outputs without a list of at least 7 distinct nouns, and we compute the DAT score for all other outputs over their first 7 nouns. We repeat the experiment 100 times for non-greedy strategies to mitigate the sampling stochasticity.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=.43\textwidth]{images/dat_instruct.pdf}
    \caption{DAT scores for our methods and the baselines over the instructed version of Llama3-8B. Below, the number of valid outputs produced by each sampling strategy. Single lines represent greedy methods, while boxplots show the performance of the other strategies.}
    \label{fig:dat_llama3_dpo}
\end{figure}
%
\subsubsection{Experimental Results}
Figure \ref{fig:dat_llama3_dpo} summarizes the DAT results for the instructed version of Llama3-8B. \textit{DiffSampling-cut} obtains slightly better scores than contrastive search, and \textit{DiffSampling-lb} obtains almost identical scores with respect to its three most similar baselines. Instead, \textit{DiffSampling-reparam} gets a slightly lower score than nucleus sampling with a 1.5 temperature; however, it only generates 14 non-valid outputs against the 36 from the baseline. Moreover, increasing the temperature to 2.0 causes the model to generate only non-valid outputs, making it evident that temperature increases diversity regardless of the correctness of the output.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=.43\textwidth]{images/dat.pdf}
    \caption{DAT scores for our methods and the baselines over the non-instructed version of Llama3-8B. Below, the number of valid outputs produced by each sampling strategy. Single lines represent greedy methods, while boxplots show the performance of the other strategies.}
    \label{fig:dat_llama3}
\end{figure}
%
As shown in Figure \ref{fig:dat_llama3}, the results for the non-instructed version of Llama3-8B are quite different. \textit{DiffSampling-cut} is still arguably better than contrastive search, as it produces fewer low-scoring and only valid outputs. Again, \textit{DiffSampling-lb} performs close to its three baselines, but with more valid outputs than $\eta$-sampling and less low-scoring responses than nucleus sampling. Finally, \textit{DiffSampling-reparam} achieves the best scores with a ratio of valid outputs comparable to nucleus sampling. However, increasing the temperature only results in non-valid lists. In general, these results confirm the hypothesis that the cutting strategy produces ``safer'', but potentially less creative outputs, while reparameterization increases diversity at the cost of accuracy. 


\section{Conclusion} \label{conclusion}

In this paper, we have presented \textit{DiffSampling}, a novel family of decoding strategies based on the analysis of the next-token distribution. In particular, given the distribution sorted in descending order, we have proposed to compute the forward difference approximation of its discrete derivative and either use it to reparameterize the distribution or to remove tokens after its minimum value (possibly after imposing a total probability lower bound). In this way, \textit{DiffSampling} can avoid incorrect tokens or increase the chance of low-probable but accurate words. We have experimented with three different tasks, i.e., math problem solving, extreme summarization, and the divergent association task, finding that our methods consistently perform at least as well as similar common strategies in terms of both accuracy of result and diversity of outputs.

Despite its limitations, \textit{DiffSampling-cut} has demonstrated performance better than or equal to the greedy strategy. Additionally, \textit{DiffSampling-cut} offers the potential for greater diversity. Introducing a lower bound can further relax constraints. Reparameterizing the distribution trades off some accuracy for increased exploration, but it remains ``safer'' than simply increasing the temperature. Our research agenda includes exploring whether combining \textit{DiffSampling} with other techniques, such as contrastive search or temperature tuning, can yield even better results. We also aim to leverage other properties of the distribution to guide text generation toward desired characteristics.


\section{Limitations} \label{limitations}

The work presented in this paper has a few important limitations to highlight. Firstly, \textit{DiffSampling} is merely a decoding strategy. While it can influence the accuracy and diversity of the model's outputs, it is constrained by the information learned by the model itself. For instance, if the model is biased toward certain grammatical structures, the probability mass is likely to contain only tokens that adhere to those structures. In addition, working at the decoding level means that the information stored by the model is not modified at all. While \textit{DiffSampling} can potentially reduce how much a model \textit{regurgitates} pre-existing text, it cannot reduce how much a model \textit{memorizes} it.

Moreover, \textit{DiffSampling} is governed by two parameters: the nucleus lower bound and the reparameterization factor. We focused on three specific cases where either one or both parameters are switched off. Each of these cases has its own advantages and disadvantages concerning the exploitation and exploration of the next-token distribution. While this can guide the choice between them, there is no golden rule; users must select the most appropriate strategy on a case-by-case basis. Similarly, we did not find specific parameter values to be universally superior, and different scenarios may require users to adjust them accordingly. 

Additionally, our experiments encompassed only three case studies. While we chose these to maximize diversity, it is difficult to estimate the actual advantage of using \textit{DiffSampling} for other tasks and with different LLMs. We intend to broaden our investigation in the future, especially by incorporating models of varying sizes. At the same time, we believe that the choice of LLM per se should not change the ranking of the decoding techniques in terms of performance, given the fact that our method is based on the analysis of the token probability distribution in output from these models.

%You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
%You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
%You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).


%\section*{Acknowledgments}

%Potential acknowledgments.

\bibliography{biblio}

\break

\appendix

\section{Computational Infrastructure}

All experiments were carried out on a local server equipped with 2 NVIDIA L40 GPUs.

\section{Prompts} \label{diffsampling_implementation_details}

As reported in Section \ref{experiments}, we tested \textit{DiffSampling} on three case studies. For the mathematical problem-solving tasks, we adopted the same two different prompts from \citet{yu2024metamath}, i.e.:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Below is an instruction that describes a task. Write a response that appropriately completes the request.\\
\\
\#\#\# Instruction:\\
\{\texttt{question}\}\\
\\
\#\#\# Response:
\end{tcolorbox}

\noindent at training time and

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Below is an instruction that describes a task. Write a response that appropriately completes the request.\\
\\
\#\#\# Instruction:\\
\{\texttt{question}\}\\
\\
\#\#\# Response: Let's think step by step.
\end{tcolorbox}

\noindent at inference time.

For the extreme summarization task, the prompt adopted for the instructed version of Llama2-7B is the same as in \citet{chhabra2024revisiting}: 

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
\textsc{[INST]} For the following article: \{\texttt{article}\}\\ 
\\
Return a summary comprising of 1 sentence. With the sentence in a numbered list format.\\
\\
For example:\\
\\
1. First sentence \textsc{[/INST]}
\end{tcolorbox}

\noindent where \textsc{[INST]} and \textsc{[/INST]} are special tokens used by Llama2-7b to identify different roles in the chat.

Vice versa, for the non-instructed version, we used:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Generate a 1 sentence summary for the given article.\\
Article: \{\texttt{article}\}\\
Summary:
\end{tcolorbox}

Finally, for the divergent association task, we considered the following prompt for the instructed version of Llama3-8B:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
\texttt{user}\\
\\
Please write 10 nouns in English that are as irrelevant from each other as possible, in all meanings and uses of the words. Please note that the words you write should have only single word, only nouns (e.g., things, objects, concepts), and no proper nouns (e.g., no specific people or places).\\
\texttt{assistant}
\end{tcolorbox}

\noindent where \texttt{user} and \texttt{assistant} are keywords used by Llama3-8b to identify different roles in the chat, while for its non-instructed version we used the following:

\begin{tcolorbox}[colback=bluechart!5!white,colframe=bluechart]
Task: Write 10 nouns in English that are as irrelevant from each other as possible, in all meanings and uses of the words. Please note that the words you write should have only single word, only nouns (e.g., things, objects, concepts), and no proper nouns (e.g., no specific people or places).\\
Solution:
\end{tcolorbox}

\vspace*{\fill}
%
\break

\section{Additional Experiments} \label{additional_experiments}

\subsection{Ablation Study on the Lower Bound} \label{abl_lb}

We conducted experiments on the three aforementioned case studies, varying the lower bound of the critical mass. Tables \ref{diffsampling_abl_lb_metamathqa}, \ref{diffsampling_abl_lb_gsm8k}, and \ref{diffsampling_abl_lb_hendrycks} report the results for the math problem-solving tasks, considering the training set and the GSM8K and MATH test sets, respectively. As expected, the against-greedy diversity scores and cross-input EAD increase together with the lower bound; instead, while accuracy tends to decrease with higher lower bounds, the differences on the test sets are not significant, and even a quite high value (e.g., 0.8) achieves competitive results.

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-lb & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
lb = 0.0 & $94.70 \pm 0.21$ & $1.68 \pm 0.01$ & $0.73 \pm 0.02$ & $0.11 \pm 0.00$ & $0.22 \pm 0.01$ \\
lb = 0.1 & $94.70 \pm 0.21$ & $1.68 \pm 0.01$ & $0.73 \pm 0.02$ & $0.11 \pm 0.00$ & $0.22 \pm 0.01$ \\
lb = 0.2 & $94.70 \pm 0.21$ & $1.68 \pm 0.01$ & $0.73 \pm 0.02$ & $0.11 \pm 0.00$ & $0.22 \pm 0.01$ \\
lb = 0.3 & $94.37 \pm 0.22$ & $1.68 \pm 0.01$ & $0.73 \pm 0.02$ & $0.11 \pm 0.00$ & $0.23 \pm 0.01$ \\
lb = 0.4 & $94.57 \pm 0.47$ & $1.68 \pm 0.01$ & $0.72 \pm 0.01$ & $0.11 \pm 0.00$ & $0.22 \pm 0.01$ \\
lb = 0.5 & $94.27 \pm 0.26$ & $1.67 \pm 0.01$ & $0.73 \pm 0.01$ & $0.12 \pm 0.00$ & $0.23 \pm 0.01$ \\
lb = 0.6 & $93.87 \pm 0.22$ & $1.68 \pm 0.01$ & $0.72 \pm 0.02$ & $0.13 \pm 0.00$ & $0.25 \pm 0.01$ \\
lb = 0.7 & $94.10 \pm 0.17$ & $1.68 \pm 0.01$ & $0.73 \pm 0.02$ & $0.15 \pm 0.01$ & $0.28 \pm 0.01$ \\
lb = 0.8 & $92.97 \pm 0.12$ & $1.70 \pm 0.01$ & $0.72 \pm 0.01$ & $0.17 \pm 0.01$ & $0.31 \pm 0.01$ \\
lb = 0.9 & $92.40 \pm 0.50$ & $1.70 \pm 0.01$ & $0.72 \pm 0.02$ & $0.19 \pm 0.01$ & $0.34 \pm 0.01$ \\
lb = 1.0 & $90.83 \pm 0.63$ & $1.72 \pm 0.01$ & $0.73 \pm 0.01$ & $0.21 \pm 0.01$ & $0.37 \pm 0.01$ \\

 \hline
\end{tabular}
}
\caption{Ablation study on the lower-bound value in \textit{DiffSampling-lb} over 3 seeds for the MetaMathQA training set in terms of percentage of correct answers and the diversity metrics. 
The mean and standard error of the final score for each run are reported for accuracy and cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for against-greedy diversity. 
\label{diffsampling_abl_lb_metamathqa}}
\end{table*}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-lb & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
lb = 0.0 & $67.10 \pm 0.19$ & $2.04 \pm 0.00$ & $0.73 \pm 0.00$ & $0.14 \pm 0.00$ & $0.33 \pm 0.01$ \\
lb = 0.1 & $66.46 \pm 0.34$ & $2.05 \pm 0.00$ & $0.72 \pm 0.00$ & $0.14 \pm 0.00$ & $0.33 \pm 0.01$ \\
lb = 0.2 & $66.46 \pm 0.34$ & $2.05 \pm 0.00$ & $0.72 \pm 0.00$ & $0.14 \pm 0.00$ & $0.33 \pm 0.01$ \\
lb = 0.3 & $66.79 \pm 0.40$ & $2.04 \pm 0.00$ & $0.72 \pm 0.00$ & $0.14 \pm 0.00$ & $0.33 \pm 0.01$ \\
lb = 0.4 & $66.57 \pm 0.39$ & $2.06 \pm 0.00$ & $0.72 \pm 0.00$ & $0.14 \pm 0.00$ & $0.33 \pm 0.01$ \\
lb = 0.5 & $65.78 \pm 0.08$ & $2.04 \pm 0.00$ & $0.72 \pm 0.00$ & $0.15 \pm 0.00$ & $0.34 \pm 0.01$ \\
lb = 0.6 & $66.67 \pm 0.37$ & $2.05 \pm 0.00$ & $0.72 \pm 0.00$ & $0.16 \pm 0.00$ & $0.36 \pm 0.01$ \\
lb = 0.7 & $65.58 \pm 0.19$ & $2.06 \pm 0.00$ & $0.73 \pm 0.01$ & $0.18 \pm 0.00$ & $0.40 \pm 0.01$ \\
lb = 0.8 & $66.87 \pm 0.16$ & $2.07 \pm 0.00$ & $0.73 \pm 0.00$ & $0.21 \pm 0.00$ & $0.43 \pm 0.01$ \\
lb = 0.9 & $65.18 \pm 0.65$ & $2.09 \pm 0.01$ & $0.73 \pm 0.00$ & $0.23 \pm 0.00$ & $0.46 \pm 0.01$ \\
lb = 1.0 & $64.87 \pm 0.20$ & $2.12 \pm 0.00$ & $0.72 \pm 0.00$ & $0.25 \pm 0.00$ & $0.48 \pm 0.01$ \\

 \hline
\end{tabular}
}
\caption{Ablation study on the lower-bound value in \textit{DiffSampling-lb} over 3 seeds for the GSM8K test set in terms of percentage of correct answers and the diversity metrics. The mean and standard error of the final score for each run are reported for accuracy and cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for against-greedy diversity. \label{diffsampling_abl_lb_gsm8k}}
\end{table*}

\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.3cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-lb & Accuracy $\uparrow$ & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & \textcolor{white}{placeholder} & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
lb = 0.0 & $21.06 \pm 0.13$ & $5.72 \pm 0.01$ & $0.66 \pm 0.00$ & $0.27 \pm 0.00$ & $0.37 \pm 0.00$ \\
lb = 0.1 & $20.95 \pm 0.20$ & $5.72 \pm 0.01$ & $0.66 \pm 0.00$ & $0.27 \pm 0.00$ & $0.37 \pm 0.00$ \\
lb = 0.2 & $20.95 \pm 0.20$ & $5.72 \pm 0.01$ & $0.66 \pm 0.00$ & $0.27 \pm 0.00$ & $0.37 \pm 0.00$ \\
lb = 0.3 & $21.30 \pm 0.08$ & $5.73 \pm 0.00$ & $0.66 \pm 0.00$ & $0.27 \pm 0.00$ & $0.37 \pm 0.00$ \\
lb = 0.4 & $21.08 \pm 0.11$ & $5.73 \pm 0.02$ & $0.68 \pm 0.00$ & $0.27 \pm 0.00$ & $0.38 \pm 0.00$ \\
lb = 0.5 & $21.18 \pm 0.11$ & $5.76 \pm 0.01$ & $0.67 \pm 0.00$ & $0.28 \pm 0.00$ & $0.39 \pm 0.00$ \\
lb = 0.6 & $21.18 \pm 0.22$ & $5.79 \pm 0.02$ & $0.67 \pm 0.00$ & $0.30 \pm 0.00$ & $0.41 \pm 0.00$ \\
lb = 0.7 & $21.14 \pm 0.15$ & $5.86 \pm 0.01$ & $0.66 \pm 0.00$ & $0.32 \pm 0.00$ & $0.44 \pm 0.00$ \\
lb = 0.8 & $20.91 \pm 0.24$ & $5.97 \pm 0.01$ & $0.67 \pm 0.00$ & $0.35 \pm 0.00$ & $0.45 \pm 0.00$ \\
lb = 0.9 & $20.20 \pm 0.08$ & $6.11 \pm 0.02$ & $0.67 \pm 0.00$ & $0.37 \pm 0.00$ & $0.47 \pm 0.00$ \\
lb = 1.0 & $19.46 \pm 0.19$ & $6.36 \pm 0.01$ & $0.68 \pm 0.00$ & $0.39 \pm 0.00$ & $0.48 \pm 0.00$ \\

 \hline
\end{tabular}
}
\caption{Ablation study on the lower-bound value in \textit{DiffSampling-lb} over 3 seeds for the MATH test set in terms of percentage of correct answers and the diversity metrics. The mean and standard error of the final score for each run are reported for accuracy and cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for against-greedy diversity. 
 \label{diffsampling_abl_lb_hendrycks}}
\end{table*}






Tables \ref{diffsampling_abl_lb_xsum} and \ref{diffsampling_abl_lb_xsum_pretrained} report the results for the extreme summarization task. Again, against-greedy scores and cross-input EAD are directly correlated with the lower bound; instead, we see no variations in terms of ROUGE-1, cosine similarity, and cross-input Sent-BERT for the RLHF-instructed model, and slight decreases for the pre-trained model.

\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-lb & \multicolumn{2}{c|}{Quality} & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & ROUGE-1 $\uparrow$ & Similarity $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
 lb = 0.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 lb = 0.1 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 lb = 0.2 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 lb = 0.3 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 lb = 0.4 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 lb = 0.5 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.19 \pm 0.01$ & $0.23 \pm 0.01$ \\
 lb = 0.6 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.20 \pm 0.01$ & $0.25 \pm 0.01$ \\
 lb = 0.7 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.01$ & $0.23 \pm 0.01$ & $0.29 \pm 0.01$ \\
 lb = 0.8 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.91 \pm 0.00$  & $0.27 \pm 0.01$ & $0.33 \pm 0.01$ \\
 lb = 0.9 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.00$ & $0.92 \pm 0.01$ & $0.30 \pm 0.01$ & $0.37 \pm 0.01$ \\
 lb = 1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.34 \pm 0.01$ & $0.40 \pm 0.01$ \\
 \hline
\end{tabular}
}
\caption{Ablation study on the lower-bound value in \textit{DiffSampling-lb} for the RLHF-instructed model over 3 seeds for the XSum dataset. Quality metrics are computed against the target summary from the test set. Diversity metrics are computed across all the generated answers (left) and the answer sampled with a greedy strategy (right). The mean and standard error of the final score for each run are reported for cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for ROUGE-1, similarity, and against-greedy diversity. 
 \label{diffsampling_abl_lb_xsum}}
\end{table*}


\begin{table*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-lb & \multicolumn{2}{c|}{Quality} & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & ROUGE-1 $\uparrow$ & Similarity $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
 lb = 0.0 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.25 \pm 0.01$ & $0.28 \pm 0.01$ \\
 lb = 0.1 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.01$ & $0.93 \pm 0.00$ & $0.26 \pm 0.01$ & $0.29 \pm 0.01$ \\
 lb = 0.2 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.35 \pm 0.01$ & $0.40 \pm 0.02$ \\
 lb = 0.3 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.44 \pm 0.01$ & $0.50 \pm 0.02$ \\
 lb = 0.4 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.01$ & $0.93 \pm 0.00$ & $0.51 \pm 0.01$ & $0.57 \pm 0.02$ \\
 lb = 0.5 & $0.19 \pm 0.00$ & $0.12 \pm 0.01$ & $1.10 \pm 0.01$ & $0.92 \pm 0.00$ & $0.56 \pm 0.01$ & $0.62 \pm 0.01$ \\
 lb = 0.6 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.10 \pm 0.00$ & $0.92 \pm 0.00$ & $0.61 \pm 0.01$ & $0.67 \pm 0.01$ \\
 lb = 0.7 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.14 \pm 0.01$ & $0.92 \pm 0.01$ & $0.67 \pm 0.01$ & $0.72 \pm 0.01$ \\
 lb = 0.8 & $0.17 \pm 0.00$ & $0.12 \pm 0.01$ & $1.15 \pm 0.01$ & $0.91 \pm 0.01$ & $0.72 \pm 0.01$ & $0.75 \pm 0.01$ \\
 lb = 0.9 & $0.15 \pm 0.00$ & $0.11 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.01$ & $0.76 \pm 0.01$ & $0.79 \pm 0.01$ \\
 lb = 1.0 & $0.14 \pm 0.00$ & $0.10 \pm 0.01$ & $1.21 \pm 0.01$ & $0.91 \pm 0.00$ & $0.80 \pm 0.01$ & $0.83 \pm 0.01$ \\
 \hline
\end{tabular}
}
\caption{Ablation study on the lower-bound value in \textit{DiffSampling-lb} for the pre-trained model over 3 seeds for the XSum dataset. Quality metrics are computed against the target summary from the test set. Diversity metrics are computed across all the generated answers (left) and the answer sampled with a greedy strategy (right). The mean and standard error of the final score for each run are reported for cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for ROUGE-1, similarity, and against-greedy diversity. 
 \label{diffsampling_abl_lb_xsum_pretrained}}
\end{table*}


Figures \ref{fig:dat_ablation_lb_llama3_dpo} and \ref{fig:dat_ablation_lb_llama3} report the results for the divergent association task. As we would expect, the DAT score changes almost linearly between that for a lower bound of 0 (that means \textit{DiffSampling-cut}) and 1 (that means \textit{standard} sampling), as we reported in Section \ref{experiments}. Interestingly, the number of correct answers by the non-instructed model drops quickly, meaning that selecting a token after the first one tends to produce more incorrect answers even if its probability is smaller than 0.1.

\begin{figure}[th]
    \centering
    \includegraphics[width=.45\textwidth]{images/dat_ablation_lb_instruct.pdf}
    \caption{DAT scores for \textit{DiffSampling-lb} over the instructed version of Llama3-8B when varying the $p_{lb}$ parameter. Below, the number of valid outputs produced by each of them. Single lines represent greedy methods, while boxplots show the performance of other strategies.}
    \label{fig:dat_ablation_lb_llama3_dpo}
\end{figure}

\begin{figure}[th]
    \centering
    \includegraphics[width=.45\textwidth]{images/dat_ablation_lb.pdf}
    \caption{DAT scores for \textit{DiffSampling-lb} over the pre-trained version of Llama3-8B when varying the $p_{lb}$ parameter. Below, the number of valid outputs produced by each of them. Single lines represent greedy methods, while boxplots show the performance of other strategies.}
    \label{fig:dat_ablation_lb_llama3}
\end{figure}

%\break

\subsection{Ablation Study on the Gamma Parameter} \label{abl_gamma}

We also conducted experiments varying the lower bound of the critical mass and the reparameterization factor for the XSum and DAT case studies.

Tables \ref{diffsampling_abl_gamma_lb_xsum_rlhf} and \ref{diffsampling_abl_gamma_lb_xsum_pretrained} report the results for the extreme summarization task considering the RLHF-instructed and pre-trained models, respectively. Despite not seeing significant differences across different reparameterization factors, we see that a higher $\gamma$ value tends to increase diversity (without altering accuracy) in the first case, while it tends to decrease it in the second one.

\begin{table*}[th]
\centering
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-reparam & \multicolumn{2}{c|}{Quality} & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & ROUGE-1 $\uparrow$ & Similarity $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.01$ & $0.21 \pm 0.01$ & $0.27 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.1 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.1 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.1 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.23 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.1 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.01$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.1 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.01$ & $0.21 \pm 0.01$ & $0.26 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.2 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.00$ & $0.17 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.2 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.2 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.18 \pm 0.01$ & $0.23 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.2 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.2 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.21 \pm 0.01$ & $0.26 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.3 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.3 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.3 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.18 \pm 0.01$ & $0.23 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.3 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.3 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.21 \pm 0.01$ & $0.27 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.4 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.4 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.18 \pm 0.01$ & $0.22 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.4 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.19 \pm 0.01$ & $0.23 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.4 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.4 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.21 \pm 0.01$ & $0.27 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.5 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.01$ & $0.19 \pm 0.01$ & $0.23 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.5 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.92 \pm 0.00$ & $0.19 \pm 0.01$ & $0.23 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.5 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.92 \pm 0.00$ & $0.19 \pm 0.01$ & $0.24 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.5 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.92 \pm 0.00$ & $0.20 \pm 0.01$ & $0.25 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.5 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.01$ & $0.22 \pm 0.01$ & $0.27 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.6 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.20 \pm 0.01$ & $0.25 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.6 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.20 \pm 0.01$ & $0.25 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.6 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.21 \pm 0.01$ & $0.26 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.6 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.18 \pm 0.00$ & $0.91 \pm 0.00$ & $0.21 \pm 0.01$ & $0.26 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.6 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.91 \pm 0.01$ & $0.29 \pm 0.01$ & $0.36 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.7 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.01$ & $0.23 \pm 0.01$ & $0.29 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.7 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.00$ & $0.23 \pm 0.01$ & $0.29 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.7 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.00$ & $0.24 \pm 0.01$ & $0.30 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.7 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.00$ & $0.24 \pm 0.01$ & $0.30 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.7 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.19 \pm 0.00$ & $0.91 \pm 0.00$ & $0.27 \pm 0.01$ & $0.33 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.8 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.91 \pm 0.00$ & $0.27 \pm 0.01$ & $0.33 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.8 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.92 \pm 0.00$ & $0.27 \pm 0.01$ & $0.33 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.8 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.92 \pm 0.00$ & $0.27 \pm 0.01$ & $0.34 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.8 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.91 \pm 0.00$ & $0.28 \pm 0.01$ & $0.34 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.8 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.91 \pm 0.00$ & $0.29 \pm 0.01$ & $0.36 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.9 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.00$ & $0.92 \pm 0.01$ & $0.30 \pm 0.01$ & $0.37 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.9 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.00$ & $0.92 \pm 0.00$ & $0.31 \pm 0.01$ & $0.37 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.9 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.00$ & $0.92 \pm 0.00$ & $0.30 \pm 0.01$ & $0.37 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.9 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.92 \pm 0.00$ & $0.31 \pm 0.01$ & $0.37 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.9 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.20 \pm 0.00$ & $0.92 \pm 0.00$ & $0.32 \pm 0.01$ & $0.39 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.34 \pm 0.01$ & $0.40 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.34 \pm 0.01$ & $0.41 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.34 \pm 0.01$ & $0.41 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.22 \pm 0.00$ & $0.91 \pm 0.00$ & $0.34 \pm 0.01$ & $0.41 \pm 0.01$ \\
 $\gamma$ = 10., lb = 1.0 & $0.22 \pm 0.00$ & $0.14 \pm 0.01$ & $1.21 \pm 0.01$ & $0.92 \pm 0.00$ & $0.35 \pm 0.01$ & $0.42 \pm 0.01$ \\
\hline
\end{tabular}
}
\caption{Ablation study on the gamma parameter and the lower-bound value in \textit{DiffSampling-reparam} for the RLHF-tuned model over 3 seeds for the XSum dataset. Quality metrics are computed against the target summary from the test set. Diversity metrics are computed across all the generated answers (left) and the answer sampled with a greedy strategy (right). The mean and standard error of the final score for each run are reported for cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for ROUGE-1, similarity, and against-greedy diversity. 
 \label{diffsampling_abl_gamma_lb_xsum_rlhf}}
\end{table*}


\begin{table*}[th]
\centering
\resizebox{.8\textwidth}{!}{%
\begin{tabular}{|L{3.5cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|C{2.2cm}C{2.2cm}|} 
 \hline
 DiffSampling-reparam & \multicolumn{2}{c|}{Quality} & \multicolumn{2}{c|}{Cross-Input Diversity} & \multicolumn{2}{c|}{Against-Greedy Diversity} \\
 \hline
 \textcolor{white}{placeholder} & ROUGE-1 $\uparrow$ & Similarity $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ & EAD $\uparrow$ & Sent-BERT $\uparrow$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.0 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.25 \pm 0.01$ & $0.28 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.0 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.26 \pm 0.01$ & $0.29 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.0 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.27 \pm 0.01$ & $0.30 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.0 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.28 \pm 0.01$ & $0.31 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.0 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.14 \pm 0.01$ & $0.93 \pm 0.00$ & $0.31 \pm 0.01$ & $0.35 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.1 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.01$ & $0.93 \pm 0.00$ & $0.26 \pm 0.01$ & $0.29 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.1 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.26 \pm 0.01$ & $0.30 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.1 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.01$ & $0.93 \pm 0.00$ & $0.27 \pm 0.01$ & $0.31 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.1 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.01$ & $0.93 \pm 0.00$ & $0.28 \pm 0.01$ & $0.32 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.1 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.14 \pm 0.01$ & $0.93 \pm 0.00$ & $0.32 \pm 0.01$ & $0.36 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.2 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.35 \pm 0.01$ & $0.40 \pm 0.02$ \\
 $\gamma$ = 0.1, lb = 0.2 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.12 \pm 0.00$ & $0.93 \pm 0.00$ & $0.36 \pm 0.01$ & $0.41 \pm 0.02$ \\
 $\gamma$ = 0.5, lb = 0.2 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.12 \pm 0.00$ & $0.93 \pm 0.00$ & $0.36 \pm 0.01$ & $0.41 \pm 0.02$ \\
 $\gamma$ = 1.0, lb = 0.2 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.13 \pm 0.00$ & $0.93 \pm 0.00$ & $0.36 \pm 0.01$ & $0.41 \pm 0.02$ \\
 $\gamma$ = 10., lb = 0.2 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.14 \pm 0.00$ & $0.93 \pm 0.00$ & $0.39 \pm 0.01$ & $0.44 \pm 0.02$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.3 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.44 \pm 0.01$ & $0.50 \pm 0.02$ \\
 $\gamma$ = 0.1, lb = 0.3 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.44 \pm 0.01$ & $0.49 \pm 0.02$ \\
 $\gamma$ = 0.5, lb = 0.3 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.44 \pm 0.01$ & $0.49 \pm 0.02$ \\
 $\gamma$ = 1.0, lb = 0.3 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.44 \pm 0.01$ & $0.50 \pm 0.02$ \\
 $\gamma$ = 10., lb = 0.3 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.12 \pm 0.01$ & $0.92 \pm 0.00$ & $0.44 \pm 0.01$ & $0.51 \pm 0.02$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.4 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.01$ & $0.93 \pm 0.00$ & $0.51 \pm 0.01$ & $0.57 \pm 0.02$ \\
 $\gamma$ = 0.1, lb = 0.4 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.01$ & $0.92 \pm 0.00$ & $0.51 \pm 0.01$ & $0.57 \pm 0.02$ \\
 $\gamma$ = 0.5, lb = 0.4 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.01$ & $0.92 \pm 0.00$ & $0.51 \pm 0.01$ & $0.57 \pm 0.02$ \\
 $\gamma$ = 1.0, lb = 0.4 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.01$ & $0.92 \pm 0.00$ & $0.50 \pm 0.01$ & $0.57 \pm 0.02$ \\
 $\gamma$ = 10., lb = 0.4 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.12 \pm 0.01$ & $0.92 \pm 0.01$ & $0.49 \pm 0.01$ & $0.56 \pm 0.02$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.5 & $0.19 \pm 0.00$ & $0.12 \pm 0.01$ & $1.10 \pm 0.01$ & $0.92 \pm 0.00$ & $0.56 \pm 0.01$ & $0.62 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.5 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.56 \pm 0.01$ & $0.62 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.5 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.11 \pm 0.00$ & $0.93 \pm 0.00$ & $0.55 \pm 0.01$ & $0.61 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.5 & $0.19 \pm 0.00$ & $0.13 \pm 0.01$ & $1.12 \pm 0.00$ & $0.93 \pm 0.00$ & $0.54 \pm 0.01$ & $0.60 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.5 & $0.19 \pm 0.00$ & $0.12 \pm 0.01$ & $1.12 \pm 0.00$ & $0.93 \pm 0.00$ & $0.52 \pm 0.01$ & $0.59 \pm 0.02$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.6 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.10 \pm 0.00$ & $0.92 \pm 0.00$ & $0.61 \pm 0.01$ & $0.67 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.6 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.10 \pm 0.00$ & $0.92 \pm 0.00$ & $0.61 \pm 0.01$ & $0.66 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.6 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.11 \pm 0.00$ & $0.92 \pm 0.00$ & $0.60 \pm 0.01$ & $0.66 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.6 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.11 \pm 0.00$ & $0.92 \pm 0.00$ & $0.59 \pm 0.01$ & $0.65 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.6 & $0.18 \pm 0.00$ & $0.13 \pm 0.01$ & $1.15 \pm 0.01$ & $0.92 \pm 0.00$ & $0.63 \pm 0.01$ & $0.68 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.7 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.14 \pm 0.01$ & $0.92 \pm 0.01$ & $0.67 \pm 0.01$ & $0.72 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.7 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.13 \pm 0.01$ & $0.92 \pm 0.01$ & $0.67 \pm 0.01$ & $0.72 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.7 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.13 \pm 0.00$ & $0.92 \pm 0.01$ & $0.65 \pm 0.01$ & $0.70 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.7 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.13 \pm 0.00$ & $0.92 \pm 0.00$ & $0.65 \pm 0.01$ & $0.69 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.7 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.14 \pm 0.00$ & $0.92 \pm 0.00$ & $0.60 \pm 0.01$ & $0.65 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.8 & $0.17 \pm 0.00$ & $0.12 \pm 0.01$ & $1.15 \pm 0.01$ & $0.91 \pm 0.01$ & $0.72 \pm 0.01$ & $0.75 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.8 & $0.17 \pm 0.00$ & $0.11 \pm 0.01$ & $1.15 \pm 0.00$ & $0.91 \pm 0.00$ & $0.72 \pm 0.01$ & $0.76 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.8 & $0.17 \pm 0.00$ & $0.12 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.01$ & $0.71 \pm 0.01$ & $0.75 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.8 & $0.17 \pm 0.00$ & $0.12 \pm 0.01$ & $1.16 \pm 0.00$ & $0.92 \pm 0.01$ & $0.70 \pm 0.01$ & $0.73 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.8 & $0.18 \pm 0.00$ & $0.12 \pm 0.01$ & $1.15 \pm 0.01$ & $0.92 \pm 0.01$ & $0.64 \pm 0.01$ & $0.69 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 0.9 & $0.15 \pm 0.00$ & $0.11 \pm 0.01$ & $1.17 \pm 0.00$ & $0.91 \pm 0.01$ & $0.76 \pm 0.01$ & $0.79 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 0.9 & $0.16 \pm 0.00$ & $0.11 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.00$ & $0.76 \pm 0.01$ & $0.79 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 0.9 & $0.16 \pm 0.00$ & $0.11 \pm 0.01$ & $1.16 \pm 0.00$ & $0.92 \pm 0.00$ & $0.74 \pm 0.01$ & $0.78 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 0.9 & $0.16 \pm 0.00$ & $0.11 \pm 0.01$ & $1.16 \pm 0.00$ & $0.91 \pm 0.01$ & $0.73 \pm 0.01$ & $0.77 \pm 0.01$ \\
 $\gamma$ = 10., lb = 0.9 & $0.18 \pm 0.00$ & $0.13 \pm 0.01$ & $1.16 \pm 0.01$ & $0.92 \pm 0.00$ & $0.66 \pm 0.01$ & $0.70 \pm 0.01$ \\
 \hline
 $\gamma$ = 0.0, lb = 1.0 & $0.14 \pm 0.00$ & $0.10 \pm 0.01$ & $1.21 \pm 0.01$ & $0.91 \pm 0.00$ & $0.80 \pm 0.01$ & $0.83 \pm 0.01$ \\
 $\gamma$ = 0.1, lb = 1.0 & $0.14 \pm 0.00$ & $0.10 \pm 0.01$ & $1.21 \pm 0.01$ & $0.92 \pm 0.00$ & $0.80 \pm 0.01$ & $0.82 \pm 0.01$ \\
 $\gamma$ = 0.5, lb = 1.0 & $0.14 \pm 0.00$ & $0.11 \pm 0.01$ & $1.20 \pm 0.01$ & $0.92 \pm 0.00$ & $0.78 \pm 0.01$ & $0.81 \pm 0.01$ \\
 $\gamma$ = 1.0, lb = 1.0 & $0.15 \pm 0.00$ & $0.11 \pm 0.01$ & $1.17 \pm 0.01$ & $0.91 \pm 0.01$ & $0.77 \pm 0.01$ & $0.80 \pm 0.01$ \\
 $\gamma$ = 10., lb = 1.0 & $0.17 \pm 0.00$ & $0.12 \pm 0.01$ & $1.15 \pm 0.01$ & $0.92 \pm 0.00$ & $0.69 \pm 0.01$ & $0.72 \pm 0.01$ \\
\hline
\end{tabular}
}
\caption{Ablation study on the gamma parameter and the lower-bound value in \textit{DiffSampling-reparam} for the pre-trained model over 3 seeds for the XSum dataset. Quality metrics are computed against the target summary from the test set. Diversity metrics are computed across all the generated answers (left) and the answer sampled with a greedy strategy (right). The mean and standard error of the final score for each run are reported for cross-input diversity, whereas the mean and the 95\% confidence interval for the full set of answers are reported for ROUGE-1, similarity, and against-greedy diversity. 
 \label{diffsampling_abl_gamma_lb_xsum_pretrained}}
\end{table*}

Finally, Figures \ref{fig:dat_ablation_gamma_lb_llama3_dpo} and \ref{fig:dat_ablation_gamma_lb_llama3} report the results for the divergent association task. Coherent with what we saw for XSum, a higher $\gamma$ value has a positive effect when considering the RLHF-instructed model, with generally higher DAT scores (apart from very low lower bounds) and more valid outputs. On the contrary, it tends to decrease the DAT score (while still generating more valid outputs) when considering the pre-trained model.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.75\textwidth]{images/dat_ablation_gamma_lb_instruct.pdf}
    \caption{DAT scores for \textit{DiffSampling-reparam} over the instructed version of Llama3-8B when varying the $\gamma$ and the $p_{lb}$ parameters. Below, the number of valid outputs produced by each of them. Single lines represent greedy methods, while boxplots show the performance of other strategies.}
    \label{fig:dat_ablation_gamma_lb_llama3_dpo}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.75\textwidth]{images/dat_ablation_gamma_lb.pdf}
    \caption{DAT scores for \textit{DiffSampling-reparam} over the pre-trained version of Llama3-8B when varying the $\gamma$ and the $p_{lb}$ parameters. Below, the number of valid outputs produced by each of them. Single lines represent greedy methods, while boxplots show the performance of other strategies.}
    \label{fig:dat_ablation_gamma_lb_llama3}
\end{figure*}

\end{document}
