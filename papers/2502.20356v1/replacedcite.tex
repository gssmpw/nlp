\section{Related Work}
\begin{figure*}[ht]
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/split_humor_rank.pdf}
        \caption{Composition of cartoon caption contest datasets across ____, ____ and our paper. In our paper, we examine $20$ pairs of captions selected from 379 contests (\#510-\#889). The dataset is further split into 279 contest for training and 100 for testing.}
        \label{fig:dataset visualization}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth,trim={3cm, 2cm, 3cm, 1cm},clip]{figures/voting.png}
        \caption{Example voting page for the caption contest.}
    \label{fig:voting}
    \end{minipage}
    \vspace{-\intextsep}
\end{figure*}

\textbf{Humor and LLMs.}
Research on computational humor has evolved significantly -- from early rule‐based, template‐driven systems that generated puns via fixed linguistic rules ____ to modern large language models (LLMs) that strive to capture the nuances of human wit. Recent studies reveal that while models like ChatGPT can produce coherent and seemingly humorous outputs, they often rely on a limited repertoire of pre‐learned jokes rather than inventing truly original humor ____. To overcome these limitations, innovative prompting strategies such as the Creative Leap-of-Thought (CLoT) paradigm have been proposed, encouraging LLMs to make unexpected conceptual associations and thereby enhancing creative humor generation ____. Complementing these approaches, multimodal techniques that integrate auditory cues have shown promise in capturing the phonetic ambiguities (essential for understanding puns) that text-only systems often miss ____. Furthermore, research on curated humor datasets demonstrates how targeted data can expose LLM limitations and spur advances in humor generation ____, while real-world evaluations by stand-up comedians underscore that, despite impressive fluency, LLM outputs frequently appear generic or bland compared to human creativity ____. On the other hand, recent studies suggest that under controlled conditions AI-generated humor can rival human-produced jokes ____. Challenges remain, however, in producing humor that is contextually rich, culturally sensitive, and genuinely surprising, highlighting the need for continued research into more sophisticated models and training paradigms ____.

\textbf{New Yorker Cartoon Caption Contest.}
Recent advances in computational humor have been bolstered by the availability of large, well‐curated datasets derived from The New Yorker Cartoon Caption Contest. Previous works used this dataset to analyze the complex interplay between visual cues and linguistic humor, shedding light on the mechanisms that make captions amusing ____. The seminal work of Bob Mankoff, whose editorial work shaped the contest’s creative process, provides essential context and insight into what constitutes successful humor in this setting ____. However, recent studies have demonstrated that state-of-the-art AI models struggle to fully capture the nuanced judgment required to select and explain winning captions ____. Together, these works underscore the utility of the New Yorker dataset as a powerful benchmark for advancing our understanding of humor in both human and machine-generated contexts.

\textbf{LLM Post-training/Alignment}
Recent advancements in post-training alignment techniques for LLMs have progressed through several distinct stages. Initially, supervised fine-tuning (SFT) was employed to adapt pre-trained models to specific tasks using high-quality, instruction-based datasets, demonstrating that even modest amounts of curated data can substantially improve downstream performance ____. Building on this, researchers introduced Reinforcement Learning from Human Feedback (RLHF) ____to further align model outputs with human preferences. In this framework, Proximal Policy Optimization (PPO) is widely used to adjust the model’s behavior based on human-provided preference comparisons ____. However, the inherent complexity and instability of PPO-based RLHF motivated the development of simpler alternatives. Direct Preference Optimization (DPO) recasts the alignment objective as a supervised learning problem by directly contrasting the log-probabilities of preferred and non-preferred responses, thereby eliminating the need for an explicit reward model ____. More recently, extensions such as Group Relative Policy Optimization (GRPO) have been proposed, which incorporate group-level comparisons that further enhance training stability and mitigate issues like catastrophic forgetting ____. This evolution -- from SFT through PPO-based RLHF to DPO and GRPO -- reflects the field’s ongoing efforts to develop robust, efficient, and reliable post-training alignment methods for LLMs.