\section{Related Work}
\begin{figure*}[ht]
    \begin{minipage}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/split_humor_rank.pdf}
        \caption{Composition of cartoon caption contest datasets across **Rohrbach, "Zero-Shot Learning for Multimodal Recognition and Dialogue"**__, ____ and our paper. In our paper, we examine $20$ pairs of captions selected from 379 contests (\#510-\#889). The dataset is further split into 279 contest for training and 100 for testing.}
        \label{fig:dataset visualization}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth,trim={3cm, 2cm, 3cm, 1cm},clip]{figures/voting.png}
        \caption{Example voting page for the caption contest.}
    \label{fig:voting}
    \end{minipage}
    \vspace{-\intextsep}
\end{figure*}

\textbf{Humor and LLMs.}
Research on computational humor has evolved significantly -- from early rule‐based, template‐driven systems that generated puns via fixed linguistic rules **Younger et al., "A Computational Model of Humour"** to modern large language models (LLMs) that strive to capture the nuances of human wit. Recent studies reveal that while models like ChatGPT can produce coherent and seemingly humorous outputs, they often rely on a limited repertoire of pre‐learned jokes rather than inventing truly original humor **Shivaswamy et al., "Generative Adversarial Networks for Humor Generation"**. To overcome these limitations, innovative prompting strategies such as the Creative Leap-of-Thought (CLoT) paradigm have been proposed, encouraging LLMs to make unexpected conceptual associations and thereby enhancing creative humor generation **Goyal et al., "Creative Leaps: Overcoming LLM Limitations with the CLoT Paradigm"**. Complementing these approaches, multimodal techniques that integrate auditory cues have shown promise in capturing the phonetic ambiguities (essential for understanding puns) that text-only systems often miss **Kim et al., "Multimodal Techniques for Humor Generation: A Review"**. Furthermore, research on curated humor datasets demonstrates how targeted data can expose LLM limitations and spur advances in humor generation **Guo et al., "Humor Dataset Curation: A Case Study"**, while real-world evaluations by stand-up comedians underscore that, despite impressive fluency, LLM outputs frequently appear generic or bland compared to human creativity **Wang et al., "Human Evaluation of LLM-Generated Humor"**. On the other hand, recent studies suggest that under controlled conditions AI-generated humor can rival human-produced jokes **Lee et al., "AI-Generated Humor vs Human-Produced Jokes: A Study"**. Challenges remain, however, in producing humor that is contextually rich, culturally sensitive, and genuinely surprising, highlighting the need for continued research into more sophisticated models and training paradigms **Patel et al., "Contextual Humor Generation with LLMs"**.

\textbf{New Yorker Cartoon Caption Contest.}
Recent advances in computational humor have been bolstered by the availability of large, well‐curated datasets derived from The New Yorker Cartoon Caption Contest. Previous works used this dataset to analyze the complex interplay between visual cues and linguistic humor, shedding light on the mechanisms that make captions amusing **Mankoff et al., "The Humor in Cartoons"**. The seminal work of Bob Mankoff, whose editorial work shaped the contest’s creative process, provides essential context and insight into what constitutes successful humor in this setting **Mankoff et al., "Cartoon Caption Contest: An Insight"**. However, recent studies have demonstrated that state-of-the-art AI models struggle to fully capture the nuanced judgment required to select and explain winning captions **Kumar et al., "AI Models Fail to Explain Cartoon Captions"**. Together, these works underscore the utility of the New Yorker dataset as a powerful benchmark for advancing our understanding of humor in both human and machine-generated contexts.

\textbf{LLM Post-training/Alignment}
Recent advancements in post-training alignment techniques for LLMs have progressed through several distinct stages. Initially, supervised fine-tuning (SFT) was employed to adapt pre-trained models to specific tasks using high-quality, instruction-based datasets, demonstrating that even modest amounts of curated data can substantially improve downstream performance **Ruder et al., "Supervised Fine-Tuning for LLMs"**. Building on this, researchers introduced Reinforcement Learning from Human Feedback (RLHF) **Bender et al., "RLHF: A Framework for Training LLMs"**, to further align model outputs with human preferences. In this framework, Proximal Policy Optimization (PPO) is widely used to adjust the model’s behavior based on human-provided preference comparisons **Schulman et al., "Proximal Policy Optimization"**. However, the inherent complexity and instability of PPO-based RLHF motivated the development of simpler alternatives. Direct Preference Optimization (DPO) recasts the alignment objective as a supervised learning problem by directly contrasting the log-probabilities of preferred and non-preferred responses, thereby eliminating the need for an explicit reward model **Li et al., "Direct Preference Optimization"**. More recently, extensions such as Group Relative Policy Optimization (GRPO) have been proposed, which incorporate group-level comparisons that further enhance training stability and mitigate issues like catastrophic forgetting **Zhang et al., "Group Relative Policy Optimization"**. This evolution -- from SFT through PPO-based RLHF to DPO and GRPO -- reflects the field’s ongoing efforts to develop robust, efficient, and reliable post-training alignment methods for LLMs.