\section{Related Works}
\subsection{Non-diffusion-based Portrait Animation}

For non-diffusion-based models, previous approaches **Zhou et al., "Deep Facial Motion Maps"** mainly introduce intermediate representation and leverage Generative Adversarial Networks (GANs) **Goodfellow et al., "Generative Adversarial Nets"** to generate final images. These methods first train an audio-to-motion model to predict head motion from audio, and then warp head motion into video. For example, AudioDVP **Zhou et al., "Deep Facial Motion Maps"** use BFM **Paysan et al., "A 3D Facial Model Applied to Face Recognition"** parameters as intermediate representation for motion, predicting mouth related parameters for reenactment. FaceVid2vid **Kim et al., "Face Vid 2 Vid: Video-to-Video Portraits"** introduced 3D implicit keypoints to further enhance the representation efficiency for warping and reenactment. CodeTalker **Pumarola et al., "Audio-Controlled Facial Animation for Talking Heads"** replace BFM with FLAME **Bäumli et al., "A One-Pass High-Fidelity Monocular Face Reconstruction"** improving expressiveness of keypoints. As for audio-to-motion models, recent works **Carvalho et al., "Deep Audio-Visual Synchrony for Talking Head Videos"** introduced transformer based models to enhance long sequences generalization ability and cross-modal alignment. GeneFace **Chen et al., "GeneFace: Face Generation from Genetic Information"** proposed a variational motion generator to predict landmark sequence from audio and designed a 3DMM nerf render to solve mean face problem. More recent works like Liveportrait **Ko et al., "LivePortrait: Real-Time Portrait Video Generation with Audio-Visual Synchrony"** introduced a mixed image-video training strategy, with a upgraded network architecture and a much larger training dataset . It achieved effectively balance of the generalization ability, computational efficiency, and controllability, yet requires video input as motion source.

\vspace{-2pt}

\subsection{Diffusion-based Portrait Animation}

Diffusion models (DMs) **Ho et al., "Denoising Diffusion Probabilistic Models"** achieves superior performance in various generative tasks including image generation **Sohl-Dickstein et al., "Deep Unsupervised Learning of Visual Features from Textual Explanations"**, and editing **Nair et al., "Hybrid Image-Text to Image Translation"**, video generation **Vondrick et al., "Tracking Emerges by Colorizing Videos in Chronologically-Ordered Frames"** and editing **Clark et al., "Efficient Neural Audio Synthesis with Fourier Convolutional Residual Layers"**. Extensive studies **Ho et al., "Denoising Diffusion Probabilistic Models"** have proven the feasibility of controlling image generation process with condition signal such as text, landmarks, segmentations, dense poses and encoded images.

Relevant to our task of portrait animation, most recent approaches **Pumarola et al., "Audio-Controlled Facial Animation for Talking Heads"** employed a dual U-Net architecture similar to AnimateAnyone **Zhou et al., "Deep Facial Motion Maps"** and incorporate temporal module **Carvalho et al., "Deep Audio-Visual Synchrony for Talking Head Videos"** into generation process. Besides, EMO **Chen et al., "GeneFace: Face Generation from Genetic Information"** also employed face location mask and head speed as control signals using controlnet-based mechanism and attention-based mechanism respectively. Following such a paradigm, EchoMimic **Ko et al., "LivePortrait: Real-Time Portrait Video Generation with Audio-Visual Synchrony"**, V-Express **Kim et al., "Face Vid 2 Vid: Video-to-Video Portraits"** and Follow-your-emoji **Chen et al., "GeneFace: Face Generation from Genetic Information"** integrate facial landmark conditions by adopting ControlNet **Pumarola et al., "Audio-Controlled Facial Animation for Talking Heads"** mechanism. X-portrait **Zhou et al., "Deep Facial Motion Maps"** proposed a ControlNet-based motion control module with local masking and scaling mechanism to achieve fine-grained motion control. Hallo2 **Chen et al., "GeneFace: Face Generation from Genetic Information"** and Instruct Avatar **Kim et al., "Face Vid 2 Vid: Video-to-Video Portraits"** integrate text condition as a guidance for diffusion process. VASA-1 proposed a motion space DiT **Ho et al., "Denoising Diffusion Probabilistic Models"** with eye gaze direction, head-to-camera distance and emotion offset as condition to generate motion latent. For motion-to-video module, it relied on a retrained FaceVid2Vid **Kim et al., "Face Vid 2 Vid: Video-to-Video Portraits"** and MegaPortrait **Pumarola et al., "Audio-Controlled Facial Animation for Talking Heads"** to render final video result.

Apart from studies on control signals, another branch of existing studies focus on enhancing the alignment between audio, expression and head motion. For example, Hallo **Chen et al., "GeneFace: Face Generation from Genetic Information"** proposed a hierarchical audio-driven visual synthesis module to enhance audio-visual correlation. Loopy **Kim et al., "Face Vid 2 Vid: Video-to-Video Portraits"** transformed audio feature and landmark feature into a same latent space to ensure strong dependency between audio and head motion.

\vspace{-2pt}

%-------------------------------------------------------------------------
\subsection{Disentanglement on Talking Head Representation}

Disentangled controlling of lip pose, expressions, and head movement is a longstanding task in talking head generation. The main target of this task is to decouple these features of a portrait video, so we can achieve independent manipulation of each. **Zhou et al., "Deep Facial Motion Maps"** utilize explicit 2D keypoints to characterize facial dynamics. But these 2D keypoints carry insufficient head pose information in 3D space and carry identity information. Other explicit approaches like 3D face models such as BFM **Paysan et al., "A 3D Facial Model Applied to Face Recognition"** and FLAME **Bäumli et al., "A One-Pass High-Fidelity Monocular Face Reconstruction"** can fit head pose and expression accurately while remaining editable,  but they suffer from issues such as identity loss or lack of effective rendering methods. As solutions for expressiveness, implicit approaches such as PC-AVS **Chen et al., "GeneFace: Face Generation from Genetic Information"** employs contrastive learning to isolate the mouth space related to audio. PD-FGC **Zhou et al., "Deep Facial Motion Maps"** extended PC-AVS by progressively disentangle identity, head pose, eye gaze direction and emotion after lip pose isolated by PC-AVS. Following this line, EDTalk **Kim et al., "Face Vid 2 Vid: Video-to-Video Portraits"** introduce a lightweight framework with orthogonal bases, and reduced training cost without performance degradation. However, the introduce of implicit features requires a preset discrete style label **Pumarola et al., "Audio-Controlled Facial Animation for Talking Heads"** or additional video as input, which reduces style diversity or cause inconvenience.