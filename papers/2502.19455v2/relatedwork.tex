\section{Related Works}
\subsection{Non-diffusion-based Portrait Animation}

For non-diffusion-based models, previous approaches \cite{zhou2020makelttalk,chen2019hierarchical} mainly introduce intermediate representation and leverage Generative Adversarial Networks (GANs) \cite{goodfellow2020generative} to generate final images. These methods first train an audio-to-motion model to predict head motion from audio, and then warp head motion into video. For example, AudioDVP \cite{wen2020photorealistic} use BFM \cite{paysan20093d} parameters as intermediate representation for motion, predicting mouth related parameters for reenactment. FaceVid2vid \cite{wang2021one} introduced 3D implicit keypoints to further enhance the representation efficiency for warping and reenactment. CodeTalker \cite{xing2023codetalker} replace BFM with FLAME \cite{li2017learning} improving expressiveness of keypoints. As for audio-to-motion models, recent works \cite{fan2022faceformer,gong2023toontalker} introduced transformer based models to enhance long sequences generalization ability and cross-modal alignment. GeneFace \cite{ye2023geneface} proposed a variational motion generator to predict landmark sequence from audio and designed a 3DMM nerf render to solve mean face problem. More recent works like Liveportrait \cite{guo2024liveportrait} introduced a mixed image-video training strategy, with a upgraded network architecture and a much larger training dataset . It achieved effectively balance of the generalization ability, computational efficiency, and controllability, yet requires video input as motion source.

\vspace{-2pt}

\subsection{Diffusion-based Portrait Animation}

Diffusion models (DMs) \cite{ho2020denoising,song2020denoising} achieves superior performance in various generative tasks including image generation \cite{rombach2022high, ruiz2023dreambooth, shi2024motion}and editing \cite{brooks2023instructpix2pix, cao2023masactrl}, video generation \cite{he2022latent, ma2024followpose,wang2024animatelcm} and editing \cite{li2024lodge,qi2023fatezero,zhang2023controlvideo}. Extensive studies \cite{zhang2023adding,li2025controlnet,hu2021lora,rombach2022high} have proven the feasibility of controlling image generation process with condition signal such as text, landmarks, segmentations, dense poses and encoded images. 

Relevant to our task of portrait animation, most recent approaches\cite{tian2024emo,chen2024echomimic,wang2024v,xu2024hallo, ma2024follow} employed a dual U-Net architecture similar to AnimateAnyone \cite{hu2024animate} and incorporate temporal module \cite{guo2023animatediff} into generation process. Besides, EMO \cite{tian2024emo} also employed face location mask and head speed as control signals using controlnet-based mechanism and attention-based mechanism respectively. Following such a paradigm, EchoMimic\cite{chen2024echomimic}, V-Express \cite{wang2024v} and Follow-your-emoji \cite{ma2024follow} integrate facial landmark conditions by adopting ControlNet \cite{zhang2023adding} mechanism. X-portrait \cite{xie2024x} proposed a ControlNet-based motion control module with local masking and scaling mechanism to achieve fine-grained motion control. Hallo2 \cite{cui2024hallo2} and Instruct Avatar \cite{wang2024instructavatar} integrate text condition as a guidance for diffusion process. VASA-1 proposed a motion space DiT \cite{peebles2023scalable} with eye gaze direction, head-to-camera distance and emotion offset as condition to generate motion latent. For motion-to-video module, it relied on a retrained FaceVid2Vid \cite{wang2021one} and MegaPortrait \cite{drobyshev2022megaportraits} to render final video result. 

Apart from studies on control signals, another branch of existing studies focus on enhancing the alignment between audio, expression and head motion. For example, Hallo \cite{xu2024hallo} proposed a hierarchical audio-driven visual synthesis module to enhance audio-visual correlation. Loopy \cite{jiang2024loopy} transformed audio feature and landmark feature into a same latent space to ensure strong dependency between audio and head motion. 

\vspace{-2pt}

%-------------------------------------------------------------------------
\subsection{Disentanglement on Talking Head Representation}

Disentangled controlling of lip pose, expressions, and head movement is a longstanding task in talking head generation. The main target of this task is to decouple these features of a portrait video, so we can achieve independent manipulation of each. \cite{siarohin2019first} utilize explicit 2D keypoints to characterize facial dynamics. But these 2D keypoints carry insufficient head pose information in 3D space and carry identity information. Other explicit approaches like 3D face models such as BFM \cite{paysan20093d} and FLAME \cite{li2017learning} can fit head pose and expression accurately while remaining editable,  but they suffer from issues such as identity loss or lack of effective rendering methods. As solutions for expressiveness, implicit approaches such as PC-AVS \cite{zhou2021pose} employs contrastive learning to isolate the mouth space related to audio. PD-FGC \cite{wang2023progressive} extended PC-AVS by progressively disentangle identity, head pose, eye gaze direction and emotion after lip pose isolated by PC-AVS. Following this line, EDTalk \cite{tan2025edtalk} introduce a lightweight framework with orthogonal bases, and reduced training cost without performance degradation. However, the introduce of implicit features requires a preset discrete style label \cite{wang2023progressive,tan2025edtalk} or additional video as input, which reduces style diversity or cause inconvenience.

\vspace{-5pt}