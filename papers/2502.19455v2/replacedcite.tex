\section{Related Works}
\subsection{Non-diffusion-based Portrait Animation}

For non-diffusion-based models, previous approaches ____ mainly introduce intermediate representation and leverage Generative Adversarial Networks (GANs) ____ to generate final images. These methods first train an audio-to-motion model to predict head motion from audio, and then warp head motion into video. For example, AudioDVP ____ use BFM ____ parameters as intermediate representation for motion, predicting mouth related parameters for reenactment. FaceVid2vid ____ introduced 3D implicit keypoints to further enhance the representation efficiency for warping and reenactment. CodeTalker ____ replace BFM with FLAME ____ improving expressiveness of keypoints. As for audio-to-motion models, recent works ____ introduced transformer based models to enhance long sequences generalization ability and cross-modal alignment. GeneFace ____ proposed a variational motion generator to predict landmark sequence from audio and designed a 3DMM nerf render to solve mean face problem. More recent works like Liveportrait ____ introduced a mixed image-video training strategy, with a upgraded network architecture and a much larger training dataset . It achieved effectively balance of the generalization ability, computational efficiency, and controllability, yet requires video input as motion source.

\vspace{-2pt}

\subsection{Diffusion-based Portrait Animation}

Diffusion models (DMs) ____ achieves superior performance in various generative tasks including image generation ____and editing ____, video generation ____ and editing ____. Extensive studies ____ have proven the feasibility of controlling image generation process with condition signal such as text, landmarks, segmentations, dense poses and encoded images. 

Relevant to our task of portrait animation, most recent approaches____ employed a dual U-Net architecture similar to AnimateAnyone ____ and incorporate temporal module ____ into generation process. Besides, EMO ____ also employed face location mask and head speed as control signals using controlnet-based mechanism and attention-based mechanism respectively. Following such a paradigm, EchoMimic____, V-Express ____ and Follow-your-emoji ____ integrate facial landmark conditions by adopting ControlNet ____ mechanism. X-portrait ____ proposed a ControlNet-based motion control module with local masking and scaling mechanism to achieve fine-grained motion control. Hallo2 ____ and Instruct Avatar ____ integrate text condition as a guidance for diffusion process. VASA-1 proposed a motion space DiT ____ with eye gaze direction, head-to-camera distance and emotion offset as condition to generate motion latent. For motion-to-video module, it relied on a retrained FaceVid2Vid ____ and MegaPortrait ____ to render final video result. 

Apart from studies on control signals, another branch of existing studies focus on enhancing the alignment between audio, expression and head motion. For example, Hallo ____ proposed a hierarchical audio-driven visual synthesis module to enhance audio-visual correlation. Loopy ____ transformed audio feature and landmark feature into a same latent space to ensure strong dependency between audio and head motion. 

\vspace{-2pt}

%-------------------------------------------------------------------------
\subsection{Disentanglement on Talking Head Representation}

Disentangled controlling of lip pose, expressions, and head movement is a longstanding task in talking head generation. The main target of this task is to decouple these features of a portrait video, so we can achieve independent manipulation of each. ____ utilize explicit 2D keypoints to characterize facial dynamics. But these 2D keypoints carry insufficient head pose information in 3D space and carry identity information. Other explicit approaches like 3D face models such as BFM ____ and FLAME ____ can fit head pose and expression accurately while remaining editable,  but they suffer from issues such as identity loss or lack of effective rendering methods. As solutions for expressiveness, implicit approaches such as PC-AVS ____ employs contrastive learning to isolate the mouth space related to audio. PD-FGC ____ extended PC-AVS by progressively disentangle identity, head pose, eye gaze direction and emotion after lip pose isolated by PC-AVS. Following this line, EDTalk ____ introduce a lightweight framework with orthogonal bases, and reduced training cost without performance degradation. However, the introduce of implicit features requires a preset discrete style label ____ or additional video as input, which reduces style diversity or cause inconvenience.

\vspace{-5pt}