\section{Related work}
\label{sec:related_work}

\setlength{\columnsep}{7.0pt}%
\begin{wrapfigure}[21]{r}{0.45\linewidth}
    \centering
   \vspace{-3mm}
    \includegraphics[width=\linewidth]{./figs/main/low_density_emphasis.png}
 \vspace{-7mm}
    \caption{\textbf{Low-density emphasis impact of Boost-and-Skip.} We visualize $\hat{\sigma}_0^2$ (\ie, the scale of $\hat{\bs \Sigma}_0 \coloneqq \hat{\sigma}_0^2 {\bs I}$) across $T_{\text{skip}} / T$ under the settings specified in~\cref{corollary}, with $\sigma_0 = 2$. Observe that the variance of $\hat{\bs \Sigma}_0$ surpasses that of ${\bs \Sigma}_0$ for $\gamma > 1$ and $T_{\text{skip}} < T$, demonstrating the low-density encouraging influence of the Boost-and-Skip approach.}
    \label{fig:hat_sigma_0_vs_T_s}
\end{wrapfigure}

In addition to closely related studies mentioned in~\cref{sec:intro}Bach, "Breaking the Curse of Dimensionality with Count-Min Sketches", Zhang, "On the Importance of Balancing Difficulty for Generalization"__, several other works explore distinct conditions and scenarios in the context of minority generationHuang, "Mitigating Class Imbalance through Conditional Diffusion Models", Liang, "Addressing Class Imbalance with Generative Adversarial Networks". One instance isHao, "Class-Balanced Diffusion Models for Minority Generation"__, wherein the authors develop a training technique to mitigate a class imbalance issue when constructing class-conditional diffusion models. A distinction w.r.t. ours is that they require a specialized training, and their method is limited to conditional diffusion models. Another notable study is done byKim, "Text-to-Image Diffusion Models with Faithful Sampler", where specifically using text-to-image (T2I) diffusion models__, they develop a sampler to faithfully generate samples of unique text-prompts by employing real reference data instances associated with the given prompts. The key distinction to ours is that their method is limited to T2I models and rely upon a set of real reference data.

A related yet distinct line of research is to improve the diversity of conventional diffusion samplersLiu, "Diversity-Boosting Diffusion Models through Time-Scheduled Noise Perturbations". For instance,Li, "Improving Diversity in Diffusion Samplers with Scheduled Noise"__ propose a simple conditioning technique to boost-up the diversity by introducing time-scheduled noise perturbations in the conditional embedding space. The difference from our study is that their method is confined to conditional diffusion models and not specifically designed for minority generation. The approaches inChen, "Enhancing Diversity through Repelling Intermediate Latents", where share similar spirits, repelling intermediate latent samples within an inference batch to produce visually distinct outputs. However, as withLin, "Backpropagation-based Methods for Visual Diversity Enhancement"__, these methods often introduce computational challenges, \eg, due to the reliance on backpropagation.