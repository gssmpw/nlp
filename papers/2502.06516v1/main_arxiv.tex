%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

\usepackage[arxiv]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{preamble}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Soobin Um}{equal,sch}
\icmlauthor{Beomsu Kim}{equal,sch}
\icmlauthor{Jong Chul Ye}{sch}
\end{icmlauthorlist}

\icmlaffiliation{sch}{Graduate School of AI, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea}

\icmlcorrespondingauthor{Jong Chul Ye}{jong.ye@kaist.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Diffusion models, Minority generation, Stochastic sampling}
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called \emph{Boost-and-Skip} for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations.
\end{abstract}

\section{Introduction}
\label{sec:intro}

Diffusion models are a prominent class of modern generative AI, known for their ability to generate high-quality content across various data modalities~\citep{ho2020denoising, ho2022video, zhang2023survey}. Unlike traditional frameworks like GANs~\citep{goodfellow2014generative}, diffusion models are notably robust, effectively learning the underlying data distribution even for rare or underrepresented examples in the training data~\citep{sehwag2022generating}. This inherent advantage has been readily adopted by researchers, leading to significant progress in \emph{minority generation} -— the task of generating minority samples that reside in low-density regions of a data manifold\footnote{Formally, the task of minority generation is expressible as drawing instances from ${\cal S}_{\epsilon} \coloneqq \{ {\bz} \in {\cal M}: p_{\bth}({\bx}) < \epsilon \}$, where ${\cal M}$ represents the data manifold, and $p_{\bth}$ denotes the implicit density learned by the diffusion model. Here $\epsilon$ is a small positive constant.}~\citep{sehwag2022generating, um2023don, um2024self, um2024minorityprompt}. The distinctive characteristics of minority instances make them important in various applications, including medical diagnosis~\citep{um2023don}, anomaly detection~\citep{du2022vos, du2023dream}, and creative AI~\citep{rombach2022high, han2022rarity}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/main/bubble_plot_with_pareto_royalblue_bns_orange.png}
    \vspace{-0.4cm}
    \caption{
    \textbf{Trade-off between minority generation performance and complexity on ImageNet $\mathbf{64\times64}$.}
    The area of each bubble corresponds to peak memory consumption.
    cFID~\citep{parmar2022aliased} calculations are based on real minority data (as in~\citet{um2023don, um2024self}), meaning that lower values indicate generation of more realistic minority samples.
    Note that our framework achieves competitive minority generation performance comparable to guided samplers~\citep{sehwag2022generating, um2023don, um2024self} while maintaining minimal complexity, thus advancing beyond the existing Pareto Frontier.
    See~\cref{tab:complexity} for detailed metric values.
    }
    \label{fig:bubble}
    \vspace{-0.6cm}
\end{figure}

Existing high-performance minority generators primarily rely upon guided sampling~\citep{sehwag2022generating, um2023don, um2024self, um2024minorityprompt}. For instance, the authors in~\citet{sehwag2022generating, um2023don} employ classifier guidance~\citep{dhariwal2021diffusion} to steer generation toward low-density regions using separately trained classifiers. A more recent approach by~\citet{um2024self} mitigates this reliance on external components (such as classifiers) by developing a self-contained sampler that derives low-density guidance solely from a pretrained diffusion model. However, this method incurs significant computational overhead due to its reliance on backpropagation through the diffusion network to compute the guidance term. Although~\citet{um2024minorityprompt} introduced a refined guidance term, specifically for text-to-image generation~\citep{nichol2021glide}, the computational bottleneck persists.


In this work, we depart from the paradigm of guided sampling and develop a simple and computationally efficient technique for minority generation. To this end, we first investigate potential approaches for guidance-free\footnote{By \emph{guidance-free}, we refer to approaches that do not incorporate a dedicated guidance term specifically designed to promote minority sample generation. Note that this does not preclude the use of conventional guidance mechanisms, such as classifier-free guidance~\citep{ho2022classifier}.} minority sampling and highlight their pitfalls. We then present \emph{Boost-and-Skip}, our novel guidance-free approach that sidesteps the previous issues. At a high level, our framework enables \emph{minority-favored initialization} that encourages emergence of underrepresented features throughout generative processes without the help of additional low-density guidance. Boost-and-Skip accomplishes this through two simple modifications to standard stochastic generative processes like DDPM~\citep{ho2020denoising}.

Specifically, we propose to initiate stochastic generation with variance-\emph{boosted} noise instead of standard Gaussian noise to encourage initializations in low-density regions. Our second yet critical modification involves \emph{skipping} several of the earliest timesteps to enhance the impact of low-density initialization from the first modification. 
We demonstrate, both theoretically and empirically, that these two modifications together lead to significantly improved minority generation, whereas each modification alone only yields marginal gains. In particular, we invoke stochastic contraction theory~\citep{pham2008analysis, pham2009contraction, chung2022come} to show that the contracting nature of the stochastic generative process gradually corrects unwanted spurious information introduced by the noise amplification, thereby contributing to the generation of high-quality minority samples.

To demonstrate the empirical benefits of our approach, we conducted extensive experiments across various real-world benchmarks. Importantly, our sampling technique achieves competitive minority generation performance compared to prior works~\citep{sehwag2022generating, um2023don, um2024self}, while offering substantially reduced computational costs. For instance, on ImageNet $64 \times 64$, our minority sampler requires 65\% less wall-clock time and 4.5 times lower peak memory consumption than the current state-of-the-art method by~\citet{um2024self}. Moreover, thanks to the simplicity, our method is highly scalable. See~\cref{fig:bubble} for a visual illustration of these benefits and~\cref{tab:complexity} for detailed metric values. To further highlight the practical significance of our sampler, we demonstrate its effectiveness in a potential downstream application, specifically its use in data augmentation for classification tasks.

Our contributions are summarized as follows:
\begin{compactitem}
    \item We propose \emph{Boost-and-Skip}, a novel guidance-free approach that introduces two simple yet effective modifications to standard stochastic generative processes: (i) variance-\emph{boosted} initialization and (ii) timestep \emph{skipping}.
    \item We provide both theoretical insights and empirical evidence to validate the effectiveness of the two modifications, which together lead to significantly improved minority generation performance.
    \item We empirically show that our approach achieves competitive performance with substantially lower computational costs compared to state-of-the-art methods. 
\end{compactitem}


%-------------------------------------------------------------------------

\begin{figure*}[t]
\centering
\begin{subfigure}{0.162\linewidth}
\includegraphics[width=\linewidth]{figs/toy/gt.jpg}%
\vspace*{-2mm}
\caption{\small\parbox[t]{.59\linewidth}{Ground truth\\\strut}}%
\end{subfigure} \hfill %
\begin{subfigure}{0.162\linewidth}
\includegraphics[width=\linewidth]{figs/toy/diff.jpg}%
\vspace*{-2mm}
\caption{\small\parbox[t]{.45\linewidth}{Diffusion\\\strut}}%
\end{subfigure} \hfill %
\begin{subfigure}{0.162\linewidth}
\includegraphics[width=\linewidth]{figs/toy/temp.jpg}%
\vspace*{-2mm}
\caption{\small\parbox[t]{.6\linewidth}{Temperature\\sampling\strut}}%
\end{subfigure} \hfill %
\begin{subfigure}{0.162\linewidth}
\includegraphics[width=\linewidth]{figs/toy/noskip.jpg}%
\vspace*{-2mm}
\caption{\small\parbox[t]{.5\linewidth}{Boost only\\\strut}}%
\end{subfigure} \hfill %
\begin{subfigure}{0.162\linewidth}
\includegraphics[width=\linewidth]{figs/toy/pfode.jpg}%
\vspace*{-2mm}
\caption{\small\parbox[t]{.45\linewidth}{w/o SDE\\\strut}}%
\end{subfigure} \hfill %
\begin{subfigure}{0.162\linewidth}
\includegraphics[width=\linewidth]{figs/toy/bns.jpg}%
\vspace*{-2mm}
\caption{\small\parbox[t]{.55\linewidth}{B\&S (Ours)\\\strut}}%
\end{subfigure}
\vspace*{-6mm} %
\caption{\label{figToyExample}
A 2D distribution with two concentric circles indicated by blue and red.
Red circle represents minority samples, being sampled $\times 10$ less compared to the blue circle.
\textbf{(a)} Ground truth samples.
\textbf{(b)} Standard diffusion sampling assigns even less mass to minority samples.
\textbf{(c)} Temperature sampling~\citep{ackley1985learning} generates off-manifold samples, due to errors accumulated through sampling trajectories (see~\cref{figTrajAnalysis} for details).
\textbf{(d)} Boosting without skipping closely recovers the data distribution yet does not promote minority features over the ground truth.
\textbf{(e)} Without stochasticity (\eg, using PF-ODE), the proposed modifications fail to contract truncation error, leading to off-manifold samples.
\textbf{(f)} Boost-and-Skip (B\&S) generates minority samples without falling off the data manifold.
} %
\vspace*{-3mm} %
\end{figure*}




\section{Background}
\label{sec:background}



Diffusion models are characterized by the two stochastic differential equations (SDEs): the forward and reverse SDEs~\citep{song2020score}. Given a $d$-dimensional noise-perturbed data space $\bx(t) \in \mathbb{R}^d$ with $t \in [0, T]$, the forward SDE is conventionally expressed as:
\begin{align}
\label{eq:fSDE_general}
    \td\bx = \bsf( \bx, t ) \td t + g(t) \td \bw,
\end{align}
where $\bsf(\cdot, \cdot): \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a drift term, and $g(\cdot): \mathbb{R} \rightarrow \mathbb{R}$ indicates a (scalar) diffusion coefficient coupled with a standard Wiener process $\bw \in \mathbb{R}^d$. With properly chosen $\bsf(\cdot, \cdot)$ and $g(\cdot)$, the forward SDE describes a progressive destruction of the target data distribution $\bx(0) \sim p_0$ upto a fully noised one $\bx(T) \sim p_T$ that one can easily sample from (\eg, a Gaussian distribution).



The reverse SDE runs backward from the noised distribution $\bx(T) \sim p_T$ down to the data distribution $\bx(0) \sim p_0$, formally written as~\citep{song2020score}:
\begin{align}
\label{eq:rSDE_general}
    \dx = \big[ \bsf (\bx, t) - g(t)^2 \nabla_{\bx} \log p_t(\bx) \big] \dt + g(t) \td \tilde{\bw},
\end{align}
where $\tilde{\bw}$ is a standard Wiener process with backward time flow (from $T$ to $0$). The score function $\nabla_{\bx} \log p_t(\bx)$ is commonly approximated by a neural network ${\bs s}_{\bth} (\bx, t)$ via denoising score matching~\citep{vincent2011connection, song2020score}.

One prominent instance of diffusion processes is variance-preserving (VP) SDE~\citep{song2020score}:
\begin{align}
\label{eq:fVPSDE}
    \dx = -\frac{1}{2} \beta(t) \bx \dt + \sqrt{\beta(t)} \td {\bs w},
\end{align}
where $\beta(t)$ is a positive function which integrates to infinity to ensure $p_T \approx {\cal N}({\bs 0}, {\bs I})$. VP-SDE conditioned on an initial point $\bx(0)$ can be characterized by a Gaussian forward diffusion kernel \citep{song2020score}:
\begin{align}
    p_{0t}(\bx(t) | \bx(0)) = {\cal N} \big(\bx(t) | \alpha(t)  \bx(0), (1 - \alpha(t)^2) {\bs I}  \big),
\end{align}
where $\alpha(t)$ is defined as
\begin{align}\label{eq:alpha}
\alpha(t) \coloneqq e^{-\frac{1}{2} \int^t_0 \beta(s) \td s}.
\end{align}
The associated reverse process of VP-SDE reads:
\begin{align}
\label{eq:rVPSDE}
    \dx = \left[ -\frac{1}{2}  \beta(t) \bx  - \beta(t) \nabla_{\bx} \log p_t(\bx) \right] \dt + \sqrt{\beta(t)} \td \tilde{\bw}.
\end{align}
Simulating~\eqref{eq:rVPSDE} with a score model ${\bs s}_{\bth} (\bx, t)$ (trained on the VP-SDE forward kernel) generates sample trajectories whose marginal distributions approximate $\{p_t(\bx)\}_{t=0}^T$~\citep{song2020score}; this is often called an empirical reverse VP-SDE.


\noindent \textbf{Discrete VP-SDE (DDPM).} Consider $N$ discrete points linearly distributed across timesteps $t \in [0,T]$. A discretized expression of the forward SDE in~\eqref{eq:fVPSDE} is~\citep{song2020score}:
\begin{align}
\label{eq:fSDE_ddpm}
    \bx_i = \sqrt{1 - \beta_i} \bx_{i-1} + \sqrt{\beta_i} \bz_i, \quad i = 1, \dots, N,
\end{align}
where $\bz_i \sim {\cal N} ({\bs 0}, {\bs I})$. $\{ \beta_i \}_{i=1}^N$ denotes a discretized sequence of $\beta(t)$. Unfolding \eqref{eq:fSDE_ddpm} across discrete timesteps $i$ yields one-shot perturbation: $\bx_i = \sqrt{\bar{\alpha}_i} \bx_0 + \sqrt{1 - \bar{\alpha}_i} \bz$, 
where $\bz \sim {\cal N} ({\bs 0}, {\bs I})$, and $\bar{\alpha}_i \coloneqq \prod_{j=1}^{i} \alpha_j$ for $\alpha_i \coloneqq 1 - \beta_i$. By discretizing \eqref{eq:rSDE_general}, the associated reverse process can be written as~\citep{song2020score}:
\begin{align}
\label{eq:rSDE_ddpm}
    \bx_{i-1} = \frac{1}{\sqrt{\alpha_i}} \big\{ \bx_i + (1 - \alpha_i) \nabla_{\bx} \log p_i(\bx) \big\} + \sqrt{1 - \alpha_i} \bz,
\end{align}
where $i \in \{1, \dots, N\}$ and $\nabla_{\bx} \log p_i(\bx)$ represents the score function for discrete timesteps. Data generation is now performed by first sampling from $\bx_N \sim {\cal N}({\bs 0}, {\bs I})$ and simulating~\eqref{eq:rSDE_ddpm} down to $\bx_0$ with the learned score network ${\bs s}_{\bth} (\bx, i) \approx \nabla_{\bx} \log p_i(\bx)$.



\section{Method}
\label{sec:method}

\subsection{Towards guidance-free minority sampler}
\label{subsec:gf}

Our framework starts by investigating potential approaches to minority-focused generation without relying on low-density guidance. One viable method is \emph{temperature sampling}~\citep{ackley1985learning}, a method commonly used in traditional likelihood-based frameworks~\citep{ackley1985learning, kingma2018glow}. In the context of diffusion models, temperature sampling can be implemented by scaling the score function along the generation trajectory~\citep{dhariwal2021diffusion}. For instance, in the empirical reverse VP-SDE that employs a pretrained score function ${\bs s}_{\bth} (\bx, t)$, this translates to:
\begin{align}
\label{eq:rSDE_ddpm_ts}
    \dx = \left[ -\frac{1}{2}  \beta(t) \bx  - \beta(t) \frac{{\bs s}_\bth (\bx, t)}{\tau} \right] \dt + \sqrt{\beta(t)} \td \tilde{\bw},
\end{align}
where $\tau$ is the \emph{temperature} parameter. Since ${\bs s}_\bth (\bx, t) / \tau \approx \nabla_\bx \log p_t(\bx)^{1/\tau}$, this sampler has been regarded as producing trajectories along $\{ \frac{1}{Z_t} p_t(\bx)^{1/\tau} \}_{t=0}^T$~\citep{dhariwal2021diffusion}, where $Z_t$ is a normalization constant. In this context, choosing $\tau > 1$ (\ie, high-temperature) is expected to favor generation in low-density regions.


However, we argue that this naive application of high-temperature sampling is ineffective within the diffusion model framework. In fact, the marginal densities of trajectories generated by~\eqref{eq:rSDE_ddpm_ts} are distinct from $\{ \frac{1}{Z_t} p_t(\bx)^{1/\tau} \}_{t=0}^T$, potentially leading to unwanted generation results. A formal description is provided below, with proof in~\cref{subsec:proof_ts_flaw}.
\begin{proposition}
\label{prop:ts_flaw}
    Consider the temperature-scaled reverse VP-SDE in~\eqref{eq:rSDE_ddpm_ts}. Assuming the score function is optimal, \ie, ${\bs s}_\bth (\bx, t) = \nabla_\bx \log p_t(\bx)$, the marginal densities of samples generated by this SDE are not equal to $\{ \frac{1}{Z_t}  p_t(\bx)^{1/\tau} \}_{t=0}^T$ in general.
\end{proposition}
One could argue that as long as high-quality minority instances are effectively generated, it is not strictly necessary for trajectory samples to adhere to the expected marginal densities $\{ \frac{1}{Z_t}  p_t(\bx)^{1/\tau} \}_{t=0}^T$. However, temperature sampling presents a critical practical limitation: it often suffers from significant errors in score function estimation along the sampling trajectories, potentially leading to degraded sample quality. This is a direct consequence of the score function's scaled amplitude $(1 / \tau)$, which causes intermediate samples $\bx_t$ to deviate from noisy data manifold ${\cal M}_t$ on which the diffusion model is trained to denoise. These observations are consistent with the limited performance of temperature sampling reported in~\citet{dhariwal2021diffusion}. See~\cref{figTrajAnalysis} for our empirical analyses on this point.

Another possible solution to guidance-free minority generation is to employ the \emph{truncation trick}~\citep{brock2018large, karras2019style} popularly employed in GANs~\citep{goodfellow2014generative}. This technique involves sampling the latent vector from a \emph{truncated} normal distribution, which is often implemented via a variance-scaled Gaussian~\citep{brock2018large}. However, as detailed in~\cref{subsec:rationale}, a naive application of this technique to diffusion models yields only marginal improvements in minority sample generation; see~\cref{tab:ablation_bo} for empirical results.


\subsection{Boost-and-Skip: minority-focused generation with two simple tweaks}
\label{subsec:bns}

In this section, we present \emph{Boost-and-Skip}, a novel guidance-free approach that sidesteps the practical limitation discussed in the previous section. A key benefit of Boost-and-Skip is its simplicity combined with significant improvements and theoretical grounding. Boost-and-Skip involves two core modifications to the standard stochastic reverse diffusion process: (i) \emph{boosting} the variance of the initial noise; and (ii) \emph{skipping} several early timesteps.

\textbf{Variance-boosting.} Consider an empirical reverse VP-SDE implemented using the score model ${\bs s}_{\bth} (\bx, t)$:
\begin{align}
\label{eq:rVPSDE_emp}
    \dx = \left[ -\frac{1}{2}  \beta(t) \bx  - \beta(t) {\bs s}_{\bth} (\bx, t) \right] \dt + \sqrt{\beta(t)} \td \tilde{\bw}.
\end{align}
In contrast to the common ignition practice that employs $\bx(T) \sim {\cal N} ({\bs 0}, {\bs I})$, we propose a $\gamma$-scaled initialization:
\begin{align}
    \hat{\bx}(T) \sim {\cal N} ({\bs 0}, \gamma^2 {\bs I}), \quad \gamma > 1,
\end{align}
where the range of $\gamma$ is selected to enhance minority sample generation. Intuitively, this modification encourages initializations from low-density regions of the terminal distribution $p_T$, potentially facilitating the generation of underrepresented samples in the data distribution $p_0$. 

However, our first modification alone, which is reminiscent of the truncation trick (used in~\citet{brock2018large}), provides limited improvement. We found that this is particularly pronounced in diffusion models with a negligible terminal signal-to-noise ratio (SNR), \ie, $\alpha(T) \approx 0$ for large $T$ in \eqref{eq:alpha}. This is because, with a near-zero terminal SNR, the influence of the low-density-encouraged initial point $\hat{\bx}(T)$ diminishes due to multiplication by negligible $\alpha(T)$, resulting in a marginal effect on the generative process. We will later provide theoretical justification of this claim.

\textbf{Timestep-skipping.} We address the vanishing impact problem by skipping several of the earliest timesteps. More specifically, we propose to start simulations of~\eqref{eq:rVPSDE_emp} from:
\begin{align}
    T_{\text{skip}} \coloneqq T - \Delta_{\text{skip}},
\end{align}
where $\Delta_{\text{skip}}$ represents the amount of skipping, which is selected to ensure non-negligible $\alpha(T_{\text{skip}})$. We found that integrating the timestep skipping with variance boosting results in a significant synergistic improvement in minority sample generation. 


While employing the timestep skipping alone could often yield some performance gains, we emphasize that these improvements are limited and inferior to the combined approach; see~\cref{tab:ablation_so} for details.


\noindent \textbf{Validation on toy data.} In Figure \ref{figToyExample}, we provide a sanity check of Boost-and-Skip on a two-dimensional toy dataset comprised of two concentric circles. There, we verify that variance-boosting and timestep-skipping with reverse-SDE (Figure \ref{figToyExample}(f)) is the only combination which generates on-manifold minority samples -- ablating boosting, skipping, or SDE leads to inferior results. In the following section, we provide theoretical intuition as to why all three components are necessary for successful minority generation.

\subsection{Rationale behind Boost-and-Skip}
\label{subsec:rationale}


Now we delve into the underlying principles of Boost-and-Skip, elucidating why and how it is effective for minority generation. We present two key mechanisms: (i) low-density emphasis and (ii) rectification through contraction.


\noindent \textbf{Low-density emphasis.} We argue that Boost-and-Skip encourages sampling from low-density instances by amplfiying their probability densities. To see this, we consider a simple (yet non-trivial) scenario where $p_0$ follows a multivariate Gaussian distribution.

\begin{proposition}
\label{prop:gaussian_SDE}
    Let the data distribution be $\bx(0) \sim {\cal N} ({\bs \mu}_0, {\bs \Sigma}_0)$, and assume the optimal score function ${\bs s}_\bth (\bx, t) = \nabla_\bx \log p_t(\bx)$ trained on $p_0$ via the forward SDE in~\eqref{eq:fVPSDE}. Suppose the reverse SDE in~\eqref{eq:rVPSDE_emp} is initialized with $\hat{\bx}(T_{\text{skip}}) \sim {\cal N}(\hat{{\bs \mu}}_{T_{\text{skip}}}, \hat{{\bs \Sigma}}_{T_{\text{skip}}} ) $. Then, the resulting generated distribution corresponds to $\hat{\bx}(0) \sim {\cal N} (\hat{{\bs \mu}}_0, \hat{{\bs \Sigma}}_0)$, where
    \begin{align}
        \hat{{\bs \mu}}_0 &\coloneqq {\bs \mu}_0 + \alpha(T_{\text{skip}}){\bs \Sigma}_0 {\bs \Sigma}_{T_{\text{skip}}}^{-1} (\hat{{\bs \mu}}_{T_{\text{skip}}} - {\bs \mu}_{T_{\text{skip}}}), \label{eq:prop_gaussian_mu0hat} \\
        \hat{{\bs \Sigma}}_0 &\coloneqq {\bs \Sigma}_0 + \alpha(T_{\text{skip}})^2 {\bs \Sigma}_0^2 {\bs \Sigma}_{T_{\text{skip}}}^{-2}(\hat{{\bs \Sigma}}_{T_{\text{skip}}} - {\bs \Sigma}_{T_{\text{skip}}}). \label{eq:prop_gaussian_Sigma0hat}
    \end{align}
    Here, ${\bs \mu}_{T_{\text{skip}}}$ and ${\bs \Sigma}_{T_{\text{skip}}}$ are defined as:
    \begin{align}
        {\bs \mu}_{T_{\text{skip}}} & \coloneqq \alpha(T_{\text{skip}}) {\bs \mu}_0, \label{eq:mu_Ts} \\
        {\bs \Sigma}_{T_{\text{skip}}} & \coloneqq {\bs I} + \alpha(T_{\text{skip}})^2 ( {\bs \Sigma}_0 - {\bs I}  ). \label{eq:Sigma_Ts}
    \end{align}
\end{proposition}
See~\cref{subsec:proof_gaussian_SDE} for the proof. In the considered Gaussian setting, Boost-and-Skip can be instantiated by setting $\hat{{\bs \Sigma}}_{T_{\text{skip}}} = \gamma^2 {\bs I}$ and $T_{\text{skip}} < T$. Note that when $\hat{\bs \Sigma}_{T_{\text{skip}}} - {\bs \Sigma}_{T_{\text{skip}}} \succ 0$, the initialization contributes to \emph{amplify} the variance of the resulting generated distribution $\hat{\bs \Sigma}_0$ compared to the original ${\bs \Sigma}_0$ (see~\eqref{eq:prop_gaussian_Sigma0hat}). This indicates that our approach can lead to probability increases in low-density regions of $p_0$, \ie, the effect of low-density emphasis.


A notable point here is that when $T_{\text{skip}} \approx T$, corresponding to the case where only boosting (\ie, our first modification) is applied, the low-density emphasis impact does not manifest. This is because $\lim_{T_{\text{skip}} \rightarrow T}   \alpha(T_{\text{skip}}) = \alpha(T) \approx 0$ leads to the recovery of the original data distribution $(\hat{\bs \mu}_0, \hat{\bs \Sigma}_0) \approx ({\bs \mu}_0, {\bs \Sigma}_0)$; see Eqs.~(\ref{eq:prop_gaussian_mu0hat}) and~(\ref{eq:prop_gaussian_Sigma0hat}) for details. This highlights the necessity of incorporating time-skipping for effective minority generation, and also explains why the truncation trick (used in~\citet{brock2018large}) could be insufficient in the diffusion model context. The key mechanism of Boost-and-Skip can be interpreted from a signal processing viewpoint. See~\cref{subsec:sp} for detailed analyses on this perspective.


To show that the condition for the low-density emphasis effect, \ie, $\hat{\bs \Sigma}_{T_{\text{skip}}} - {\bs \Sigma}_{T_{\text{skip}}} \succ 0$, can be satisfied in practice, we provide a corollary that characterizes the range of $T_{\text{skip}}$ over which the variance amplification impact occurs:




\begin{figure*}[t]
\centering
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=\linewidth]{./figs/main/traj_error.png}%
\caption{Trajectories of estimation error}\label{figTrajAnalysis_error}
\end{subfigure} %
\hfill
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=\linewidth]{./figs/main/traj_norm.png}%
\caption{Norm trajectories}\label{figTrajAnalysis_norm}
\end{subfigure} %
\vspace*{-2.0mm} %
\caption{
\label{figTrajAnalysis}
\textbf{Trajectory analysis across various samplers.} ``TS'' refers to temperature sampling~\citep{ackley1985learning}, and ``w/o SDE'' indicates our approach without stochastic sampling, \ie, using PF-ODE. ``B\&S'' is ours, \emph{Boost-and-Skip}. $\tau$ is the temperature parameter used to scale the score function (\eg, ${\bs s}_{\bth}(\bx_i, i) / \tau$), and $\gamma$ controls the boosted initialization strength.
\textbf{(a)} Noise estimation errors across discrete timestep $i$, where ${\bx}'_i \coloneqq \sqrt{\bar{\alpha}_i} \hat{\bx}_{0|i} + \sqrt{1 - \bar{\alpha}_i} \beps$. Here $\hat{\bx}_{0|i}$ represents a denoised sample from $\bx_i$, \ie, the posterior mean of $\bx_i$. While temperature sampling fails even with slight variations in $\tau$, Boost-and-Skip performs as well as DDPM, demonstrating its robustness to imperfect score models.
\textbf{(b)} Norm of intermediate samples $\bx_i$ across timestep $i$. We see that norm trajectories of our approach rapidly converge to that of DDPM, exhibiting the contraction effect claimed in~\cref{subsec:rationale}. In contrast, trajectories of temperature sampling diverge from the DDPM curve, further revealing its pathology. See~\cref{figTrajSamples} for visualizations of intermediate samples over trajectories.
}
\vspace*{-2mm} %
\end{figure*}



\begin{corollary}
\label{corollary}
    Suppose ${\bs \Sigma}_0 = \sigma_0^2 {\bs I}$ and $\hat{\bs \Sigma}_{T_{\text{skip}}} = \gamma^2 {\bs I}$, and define the quantity (if it exists)
    \begin{align*}
        \kappa \coloneqq \sqrt{(\gamma^2 - 1)/(\sigma_0^2 - 1)}.
    \end{align*}
    The variance-amplification effect of $\hat{\bs \Sigma}_0$ occurs iff
    \begin{align*}
    T_{\text{skip}} \in
    \begin{cases}
        \varnothing & \text{if } \gamma \leq 1 \leq \sigma_0, \\
        (\alpha^{-1}(\kappa),\infty) & \text{if } 1 < \gamma,\sigma_0, \\
        [0,\alpha^{-1}(\kappa)) & \text{if } 1 > \gamma,\sigma_0, \\
        [0,\infty) & \text{if } \sigma_0 \leq 1 \leq \gamma \text{ and } (\sigma_0,\gamma) \neq \mathbf{1},
    \end{cases}
    \end{align*}
    where we define $\alpha^{-1}(\kappa) \coloneqq 0$ when $\kappa > 1$.
\end{corollary}
In~\cref{subsec:ld_emphasis}, we provide an illustration that exhibits the behavior of $\hat{\bs \Sigma}_0 \coloneqq \hat{\sigma}_0^2 {\bs I}$ under the conditions specified in Corollary~\ref{corollary}; see~\cref{fig:hat_sigma_0_vs_T_s} therein.




\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/main/traj_samples.jpg}
    \vspace*{-5mm} %
    \caption{
    \textbf{Denoised intermediate samples along generative trajectories on CelebA~\citep{liu2015faceattributes}.}
    \textbf{(a)} DDPM.
    \textbf{(b)} Temperature sampling with $\tau = 1.1$.
    \textbf{(c)} Ours with $\gamma = 5.0$.
    \textbf{(d)} Ours with $\gamma = 1.1$ and PF-ODE.
    From left to right, discrete timestep $i$ decreases from $i=100$ to $0$. Leveraging the contracting nature of stochastic generation, Boost-and-Skip reduces excessive noise introduced by boosted initialization during early stages of generation.
    }
    \label{figTrajSamples}
    \vspace*{-4mm} %
\end{figure}



\noindent \textbf{Rectification via contraction.} A potential concern is whether the amplified noise components due to variance-boosted initialization may impede high-quality generation. To address this, we invoke stochastic contraction theory~\citep{pham2008analysis, pham2009contraction}, a principle that is often used to describe error-rectifying behaviors of stochastic diffusion generative processes~\citep{chung2022come, xu2023restart}. Specifically under the discrete VP-SDE setting in~\cref{eq:rSDE_ddpm}, we establish a general theoretical result showing that, for an arbitrary distribution $p_0$, the error introduced by the boosted initialization decays exponentially as stochastic sampling progresses. See below for a formal description of the claim.
\begin{proposition}
\label{prop:ddpm_contraction}
Consider an empirical version of the discrete VP-SDE in~\eqref{eq:rSDE_ddpm}:
\begin{align*}
    \bx_{i-1} = \frac{1}{\sqrt{\alpha_i}} \big\{ \bx_i + (1-\alpha_i) s_{\bth}(\bx_i, i) \big\} + \sqrt{1 - \alpha_i} \bz,
\end{align*}
where $i \in \{1, \dots, N\}$. Assume the optimal score function, \ie, $s_{\bth}(\bx, i) = \nabla_{\bx} \log p_i (\bx)$, which is trained on an arbitrary data distribution $p_0$. Consider two sample trajectories $\{ \bx_{i} \}_{i=0}^N$ and $\{ \hat{\bx}_{i} \}_{i=0}^{\Ns}$ where $\Ns < N$, which are initialized with distinct distributions: $\bx_N \sim {\cal N}({\bs 0}, {\bs I})$ and $\hat{\bx}_{\Ns} \sim {\cal N}({\bs 0}, \gamma^2 {\bs I})$. Assuming that $\{ \bx_{i} \}_{i=0}^N$ is a bounded process such that $\| \bx_i \|_2 < B$ (as in~\citet{xu2023restart}), the expected error between samples from these two trajectories at step $i \in \{0, \dots, \Ns - 1\}$ is given by:
\begin{align}
    \mathbb{E}[ \| \bx_i - \hat{\bx}_i \|_2^2] \le \frac{2C}{1 - \lambda^2} + \lambda^{2(\Ns - i)}(B^2 + \gamma^2 d),
\end{align}
where $\lambda$ denotes the contraction rate:
\begin{align}
    \lambda \coloneqq \max_{j \in \{ i+1, \dots, \Ns\}} \sqrt{\alpha_j}\left( \frac{1 - \bar{\alpha}_{j-1}}{1 - \bar{\alpha}_{j}} \right),
\end{align}
and $C \coloneqq d(1 - \bar{\alpha}_{\Ns})$.
\end{proposition}

See~\cref{subsec:proof_ddpm_contraction} for the proof. Notice that the error term $B^2 + \gamma^2 d$ decreases exponentially w.r.t. $N_\sk$ with contraction rate $\lambda$. In~\cref{subsec:further_ct}, we provide a more general contraction theory result that shows PF-ODE preserves (total variation) distance between distribution, whereas reverse-SDE contracts distributional distances. In other words, PF-ODE propagates initial distribution error throughout the generation process. This implies that stochasticity is critical for initial error contraction, and also explains why a naive combination of Boost-and-Skip and ODE-based samplers fail to produce high-quality minority samples.

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[h]{0.321\linewidth}
    \includegraphics[width=\linewidth]{./figs/main/samples_in256_adm.jpg}
    \caption{ADM~\citep{dhariwal2021diffusion}}
    \end{subfigure}
    \hspace{0.5mm}
    \begin{subfigure}[h]{0.321\linewidth}
    \includegraphics[width=\linewidth]{./figs/main/samples_in256_ts.jpg}
    \caption{Temperature sampling}
    \end{subfigure}
    \hspace{0.5mm}
    \begin{subfigure}[h]{0.321\linewidth}
    \includegraphics[width=\linewidth]{./figs/main/samples_in256_bns.jpg}
    \caption{Boost-and-Skip (ours)}
    \end{subfigure}
    \vspace{-2.5mm}
\caption{Sample comparison on ImageNet $256 \times 256$. Generated samples from two classes are exhibited: ``jack-o'-lantern'' (top row) and ``bald eagle'' (bottom row). We share the same random seed across all three approaches.}
\label{fig:samples_in256}
\vspace{-2mm}
\end{figure*}

In practice, the score function ${\bs s}_{\bth}(\bx, t)$ is not optimal and may introduce estimation error. This is particularly relevant during early stages of our generative process, where the score model is tasked with estimating boosted-noise samples that were not encountered during training. Nonetheless, our empirical findings indicate that within reasonable ranges of $\gamma$, the contraction effect inherent in stochastic sampling rapidly corrects initial error during the early stages of generation. We highlight that this is in stark contrast to temperature sampling, which is highly sensitive to the choice of $\tau$ (\ie, the scaling parameter) and prone to collapse even with small deviations from $\tau = 1$. Figures~\ref{figTrajAnalysis} and~\ref{figTrajSamples} provide empirical analyses that investigate the robustness of our approach to imperfect score models; see details therein. 


\vspace{-2mm}
\section{Experiments}
\label{sec:exp}


\textbf{Datasets and pretrained models.} Our experiments were conducted on four benchmarks settings with varying resolutions: (i) CelebA $64 \times 64$~\citep{liu2015faceattributes}; (ii) LSUN-Bedrooms $256 \times 256$~\citep{yu2015lsun}; (iii) ImageNet $64 \times 64$~\citep{deng2009imagenet}; and (iv) ImageNet $256 \times 256$. We consider unconditional generation on CelebA and LSUN-Bedrooms, while performing class-conditional generation on the ImageNet settings. The CelebA pretrained model was developed in-house, adhering to the configuration described in~\citet{um2023don}. The pretrained models for LSUN-Bedrooms and ImageNet were taken from the checkpoints provided by~\citet{dhariwal2021diffusion}.


\noindent \textbf{Baselines.} We evaluate our approach against a diverse array of baseline methods, including frameworks beyond diffusion models and minority samplers. Specifically, we include two general-purpose GAN frameworks for comparison: BigGAN~\citep{brock2018large} and StyleGAN~\citep{karras2019style}. Additionally, we incorporate prominent diffusion-based approaches with their standard sampling techniques: ADM~\citep{dhariwal2021diffusion}, LDM~\citep{rombach2022high}, EDM~\citep{karras2022elucidating}, and DiT~\citep{Peebles2022DiT}. The state-of-the-art diversity sampler, CADS~\citep{sadat2023cads}, is also included for benchmarking. For minority sampling baselines, we consider the most advanced diffusion-based approaches: (i)~\citet{sehwag2022generating}; (ii)~\citet{um2023don}; (iii) ADM-ML~\citep{um2023don} (iv)~\citet{um2024self}; and (v) temperature sampling.



{
\noindent \textbf{Evaluation Metrics.} We employ a set of quality and diversity metrics to evaluate generated samples. Specifically, we use: (i) Clean Fréchet Inception Distance (cFID)~\citep{parmar2022aliased}; (ii) Spatial FID (sFID)~\citep{nash2021generating}; and (iii) Improved Precision \& Recall~\citep{kynkaanniemi2019improved}. To evaluate the proximity to real minority data, we follow the approach used in previous works~\citet{um2023don, um2024self}. Specifically, we employ instances with the lowest likelihoods (i.e., those with the highest AvgkNN values) as reference real data for calculation of quality and diversity metrics. Additionally, to compare the capability to generate low-density instances, we adopt three uniqueness measures: (i) Average k-Nearest Neighbor (AvgkNN); (ii) Local Outlier Factor (LOF)~\citep{breunig2000lof}; and (iii) Rarity Score~\citep{han2022rarity}. For all these measures, higher value indicate that an instance is less similar to its neighborhood~\citep{sehwag2022generating, han2022rarity}. 
}



\begin{table*}[ht]
\fontsize{7.5}{7.5}\selectfont{
    \begin{subtable}[t]{0.499\textwidth}
        \centering
        \begin{tabular}{lcccc}
            \toprule[0.1em]
              \multicolumn{1}{l}{Method}  & \multicolumn{1}{c}{cFID $\downarrow$} & \multicolumn{1}{c}{sFID $\downarrow$} & \multicolumn{1}{c}{Prec $\uparrow$} & \multicolumn{1}{c}{Rec $\uparrow$} \\
              \\
              \multicolumn{5}{l}{\textbf{CelebA 64$\times$64}}\\
            \toprule[0.1em]
            \multicolumn{1}{l}{ADM~\citep{dhariwal2021diffusion}} & 75.41  & 17.11 & \textbf{0.97} & 0.23 \\
            \multicolumn{1}{l}{BigGAN~\citep{brock2018large}} & 80.58 & 16.80 & \textbf{0.97} & 0.19 \\
            \midrule[0.001em]
            \multicolumn{1}{l}{ADM-ML~\citep{um2023don}} & 51.99 & 13.40 & \underline{0.94} & 0.30 \\
            \multicolumn{1}{l}{\citet{sehwag2022generating}} & 28.25 & 10.64 & 0.82 & 0.42 \\
            \multicolumn{1}{l}{\citet{um2023don}} & 27.32 & 8.66  & 0.89 & 0.33 \\
            \multicolumn{1}{l}{\citet{um2024self}} & \bf{19.34} & \underline{8.85} & 0.82 & \underline{0.47} \\
            \midrule[0.001em]
            \multicolumn{1}{l}{Temperature sampling} & 37.78 & 14.49 & 0.79 & 0.38 \\
            \multicolumn{1}{l}{\tbcl Boost-and-Skip (proposed)} & \tbcl \underline{19.79} & \tbcl \textbf{6.87} & \tbcl 0.77 & \tbcl \textbf{0.51} \\
            \\
            \multicolumn{5}{l}{\textbf{ImageNet 64$\times$64}}\\
            \toprule[0.1em]
            \multicolumn{1}{l}{ADM~\citep{dhariwal2021diffusion}} & 18.37 & 5.39 & \underline{0.79} & 0.53 \\
            \multicolumn{1}{l}{EDM~\citep{karras2022elucidating}} & 19.09 & 4.73 & 0.73 & 0.59 \\
            \midrule[0.001em]
            \multicolumn{1}{l}{\citet{sehwag2022generating}} & 11.37 & 4.69 & \textbf{0.80} & 0.52 \\
            \multicolumn{1}{l}{\citet{um2023don}}  & \underline{12.47} & \bf{3.13} & 0.76 & 0.56 \\
            \multicolumn{1}{l}{\citet{um2024self}} & \bf{11.24} & \underline{3.17} & 0.73 & \bf{0.62} \\
            \midrule[0.001em]
            \multicolumn{1}{l}{Temperature sampling} & 16.19 & 4.20 & 0.76 & 0.58 \\
            \multicolumn{1}{l}{\tbcl Boost-and-Skip (proposed)} & \tbcl 12.68 & \tbcl 3.18 & \tbcl 0.74 & \tbcl \underline{0.61} \\
            \\
        \end{tabular}
    \end{subtable}
    \begin{subtable}[t]{0.499\textwidth}
        \centering
        \begin{tabular}{lcccc}
            \toprule[0.1em]
              \multicolumn{1}{l}{Method}  & \multicolumn{1}{c}{cFID $\downarrow$} & \multicolumn{1}{c}{sFID $\downarrow$} & \multicolumn{1}{c}{Prec $\uparrow$} & \multicolumn{1}{c}{Rec $\uparrow$} \\
              \\
            \multicolumn{5}{l}{\textbf{LSUN Bedrooms 256$\times$256}}\\
            \toprule[0.1em]
            \multicolumn{1}{l}{ADM~\citep{dhariwal2021diffusion}} & 63.30 & 8.00 & \underline{0.89} & 0.15 \\
            \multicolumn{1}{l}{LDM~\citep{rombach2022high}} & 63.53 & 7.73 & \textbf{0.90} & 0.13 \\
            \multicolumn{1}{l}{StyleGAN~\citep{karras2019style}} & 57.17 & 7.78 & \underline{0.89} & 0.14 \\
            \midrule[0.001em]
            \multicolumn{1}{l}{\citet{um2023don}} & \underline{41.75} & 7.26 & 0.87 & 0.10 \\
            \multicolumn{1}{l}{\citet{um2024self}} & \bf{36.94} & \bf{5.13} & 0.87 & 0.15 \\
            \midrule[0.001em]
            \multicolumn{1}{l}{Temperature sampling} & 51.81 & 8.22 & 0.82 & \textbf{0.21} \\
            \multicolumn{1}{l}{\tbcl Boost-and-Skip (proposed)} & \tbcl 44.77 & \tbcl \underline{6.70} & \tbcl 0.78 & \tbcl \underline{0.20} \\
            \\
            \multicolumn{5}{l}{\textbf{ImageNet 256$\times$256}}\\
            \toprule[0.1em]
            \multicolumn{1}{l}{ADM~\citep{dhariwal2021diffusion}} & 13.22 & 7.66 & \textbf{0.86} & 0.39 \\
            \multicolumn{1}{l}{DiT~\citep{Peebles2022DiT}} & 21.51 & 6.76 & 0.80 & \underline{0.46} \\
            \multicolumn{1}{l}{CADS~\citep{sadat2023cads}} & 15.95 & 6.18 & 0.81 & \textbf{0.48} \\
            \midrule[0.001em]
            \multicolumn{1}{l}{\citet{sehwag2022generating}} & 10.93 & 6.66 & \underline{0.85} & 0.39 \\
            \multicolumn{1}{l}{\citet{um2023don}}  & 11.44 & 4.63 & \underline{0.85} & 0.42 \\
            \multicolumn{1}{l}{\citet{um2024self}} & \bf{9.98} & \underline{4.35} & 0.83 & 0.45 \\
            \midrule[0.001em]
            \multicolumn{1}{l}{Temperature sampling} & 12.48 & 6.72 & 0.84 & 0.41 \\
            \multicolumn{1}{l}{\tbcl Boost-and-Skip (proposed)} & \tbcl \underline{10.04} & \tbcl \bf{4.33} & \tbcl 0.83 & \tbcl 0.45 \\
            \\
        \end{tabular}
    \end{subtable}
    }
    \vspace{-4mm}
    \caption{\textbf{Quantitative comparisons.}
    ``ADM-ML'' represents a classifier-guided baseline that implements conditional generation on \textbf{M}inority \textbf{L}abels~\cite{um2023don}. For baseline real data to compute the metrics, we employ the most unique samples that yield the highest AvgkNN values, following the previous convention~\citep{um2023don, um2024self}. The best results are marked in \textbf{bold}, and the second bests are \underline{underlined}.}
    \label{tab:main_results}
\end{table*}






\begin{table*}[!t]
    \centering
    \begin{subtable}[t]{0.32\textwidth}
        \centering
        \scalebox{0.75}{
            \begin{tabular}{ccccc}
                \toprule
                $\gamma^2$ & cFID $\downarrow$ & {sFID} $\downarrow$ & Prec $\uparrow$ & Rec $\uparrow$ \\
                \midrule
                    1.0              & 84.98 & 23.07 & \textbf{0.98} & 0.14 \\
                    4.0              & \textbf{81.75} & \textbf{22.71} & 0.97 & 0.17 \\
                    9.0              & 82.10 & 22.78 & \textbf{0.98} & 0.15 \\
                    16.0              & 83.32 & 22.91 & \textbf{0.98} & \textbf{0.18} \\
                \bottomrule
            \end{tabular}
        }
        \caption{Boost-only (\ie, $\Delta_t = 0$)}
        \label{tab:ablation_bo}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.32\textwidth}
        \centering
        \scalebox{0.75}{
            \begin{tabular}{ccccc}
                \toprule
                $\Delta_t$ & cFID $\downarrow$ & {sFID} $\downarrow$ & Prec $\uparrow$ & Rec $\uparrow$ \\
                \midrule
                    0                   & 84.98  & 23.07  & \textbf{0.98} & \textbf{0.14} \\
                    10                  & 61.86  & \textbf{19.97}  & 0.96 & 0.13 \\
                    20                  & \textbf{55.15}  & 22.02  & 0.91 & 0.06 \\
                    50                  & 289.70 & 119.59 & 0.29 & 0.00 \\
                \bottomrule
            \end{tabular}
        }
        \caption{Skip-only (\ie, $\gamma = 1.0$)}
        \label{tab:ablation_so}
    \end{subtable}
    \hfill
    \begin{subtable}[t]{0.32\textwidth}
        \centering
        \scalebox{0.75}{
            \begin{tabular}{ccccc}
                \toprule
                $\gamma^2$ & cFID $\downarrow$ & {sFID} $\downarrow$ & Prec $\uparrow$ & Rec $\uparrow$ \\
                \midrule
                1.0                 & 74.41 & 20.50 & \textbf{0.97} & 0.21 \\
                2.0                 & 51.47 & 16.43 & 0.94 & 0.33 \\
                4.0                 & \textbf{23.56} & \textbf{12.17} & 0.77 & 0.50 \\
                9.0                 & 219.21 & 45.59 & 0.05 & \textbf{0.67} \\
                \bottomrule
            \end{tabular}
        }
        \caption{Boost-and-Skip (with $\Delta_t = 3$)}
        \label{tab:ablation_bns}
    \end{subtable}
    \vspace{-2.3mm}
    \caption{
    \textbf{Exploring the design space of Boost-and-Skip.} ``Boost-only'' refers to using boosted initialization alone, while ``Skip-only'' represents configurations that only employ the proposed timestep skipping. $\gamma$ denotes the boosting scale, and $\Delta_t$ indicates the number of timesteps skipped. Using either technique individually results in limited performance improvements.
    }
    \label{tab:ablation}
    \vspace{-3mm}
\end{table*}


\begin{table}[h]
    
    \centering
    \fontsize{8}{8}\selectfont
        {
        \scalebox{0.8}{
    \begin{tabular}{lccccr}
        \toprule
                &       &       & \multicolumn{3}{c}{Complexity}\\
        \cmidrule(lr){4-6}
        Method & cFID $\downarrow$ & sFID $\downarrow$ & Infer $\downarrow$ & \multicolumn{1}{c}{Extra $\downarrow$} & \multicolumn{1}{c}{Memory $\downarrow$} \\
        \midrule
        ADM\tiny{~\citep{dhariwal2021diffusion}} &  18.37 & 5.39 & \udl{0.99} s & \multicolumn{1}{c}{--} & \udl{85.73} MB \\
        EDM\tiny{~\citep{karras2022elucidating}} & 19.09 & 4.73 & 1.87 s & \multicolumn{1}{c}{--} & \textbf{84.48} MB \\
        \citet{sehwag2022generating} & \udl{11.37} & 4.69 & 2.75 s & $>$ 16 d & 187.04 MB \\
        \citet{um2023don} & 12.47 & \textbf{3.13} & 2.27 s & $>$ 16 d & 184.40 MB \\
        \citet{um2024self} & \textbf{11.24} & \udl{3.17} & 2.80 s & \multicolumn{1}{c}{--} & 386.75 MB \\
        Temperature sampling &  16.19 & 4.20 & \udl{0.99} s & \multicolumn{1}{c}{--} & \udl{85.73} MB \\
        \tbcl Boost-and-Skip (ours) & \tbcl 12.68 &\tbcl  3.18 & \tbcl \textbf{0.98} s & \multicolumn{1}{c}{\tbcl --} & \tbcl \udl{85.73} MB \\
        \bottomrule
    \end{tabular}
    }
    }
    \vspace{-2mm}
    \caption{
    \textbf{Comparison of complexity across existing samplers on ImageNet $\mathbf{64 \times 64}$.}
    ``Infer'' represents the inference time measured in seconds/sample.
    ``Extra'' refers to the additional time, in \textbf{days}, required to construct external classifiers used for guided sampling~\citep{sehwag2022generating, um2023don}. 
    ``Memory'' indicates the peak memory usage, measured in MB/sample. 
    All measurements are based on a single NVIDIA A100 GPU.
    }\label{tab:complexity}
    \vspace{-4mm}
\end{table}





\begin{table}[h]
    \centering
    \fontsize{8.5}{8.5}\selectfont
        {
        \scalebox{1.0}{
    \begin{tabular}{lcccc}
        \toprule
          Training data & Acc $\uparrow$ & F1 $\uparrow$ & Prec $\uparrow$ & Rec $\uparrow$ \\
        \midrule
         CelebA trainset & 0.898  & 0.746  &  0.815  & 0.710  \\
         + ADM gens (50K) & 0.897 & 0.742  & 0.808  & 0.711  \\
        + SGMS gens (50K)  & \textbf{0.903} & \textbf{0.757} & \textbf{0.822} & \textbf{0.724} \\
        \tbcl + Ours gens (50K)  & \tbcl \udl{0.902} & \tbcl \udl{0.755} & \tbcl \udl{0.819} & \tbcl \udl{0.723} \\
        \bottomrule
    \end{tabular}
    }}
    \vspace{-2mm}
    \caption{
    \textbf{Data augmentation for downstream classification tasks.}
    ``SGMS'' refers to the approach by~\citet{um2024self}.
    All settings were evaluated on the CelebA testset and averaged over three different runs.
    }
    \label{tab:classification}
    \vspace{-8mm}
\end{table}



\subsection{Results}
\label{subsec:exp_results}

\noindent \textbf{Qualitative comparisons.}~\cref{fig:samples_in256} presents generated samples from three different approaches on ImageNet $256 \times 256$. We observe that our framework consistently produces samples with highly more distinct and intricate visual aspects compared to the baselines, characteristics often associated with low-density instances~\citep{serra2019input, arvinte2023investigating}. We highlight that this contrasts with temperature sampling yielding marginal changes from the baseline ADM, as further confirmed by our quantitative evaluations (see~\cref{tab:main_results}). Additional samples including those from other benchmarks and baselines are provided in~\cref{subsec:additional_samples}.

\noindent \textbf{Quantitative evaluation.}~\cref{tab:main_results} compares performance in terms of quality and diversity. Observe that our sampler achieves high-quality minority generating performance on par with computationally intensive guided samplers across all datasets. We emphasize that these substantial improvements are achievable with minimal computational overhead; see~\cref{tab:complexity} for a complexity analysis on ImageNet $64 \times 64$. In addition to its competitive performance in quality and diversity, our approach also performs well in neighborhood density metrics; see~\cref{subsec:nnd_dist} for explicit details.

\noindent \textbf{Ablation studies.}~\cref{tab:ablation} presents ablation results on our design choices, $\gamma$ and $\Delta_t$. We see that applying either modification individually results in limited improvements (see Tables~\ref{tab:ablation_bo} and~\ref{tab:ablation_so}). In contrast, combining both techniques with properly chosen $\gamma$ values yields significant enhancements in minority generation; see~\cref{tab:ablation_bns} for details. In~\cref{subsec:add_ablations}, we present a further ablation exploring performance with $\gamma < 1.0$, where we demonstrate a quality-improving potential of ours; see~\cref{tab:gamma_lt_1} for details.


\noindent \textbf{Downstream application.} To further highlight the practical importance of Boost-and-Skip, we investigate its potential application in classifier training with synthetically augmented datasets. Specifically, we examine whether our minority-promoted generated samples can enhance classification performance. Following~\citet{um2024self}, we consider the prediction of 40 attributes in CelebA and train ResNet-18 models on four datasets: (i) the CelebA training set; (ii) CelebA augmented with 50K samples from ADM~\cite{dhariwal2021diffusion}; (iii) CelebA augmented with 50K samples from~\citet{um2024self}; and (iv) CelebA augmented with 50K samples from ours. As in~\citet{um2024self}, we use an off-the-shelf classifier to label the generated samples. Our results show classification improvements comparable to~\citet{um2024self}, despite our method being significantly more computationally efficient, further demonstrating its utility in downstream tasks.








\section{Conclusion}

We introduced \emph{Boost-and-Skip}, a simple yet impactful guidance-free approach for minority sample generation. By incorporating variance-\emph{boosted} initialization and timestep \emph{skipping}, our framework promotes the emergence of underrepresented features without relying on additional low-density guidance. Theoretical and empirical results validated its effectiveness, demonstrating competitive performance while significantly reducing computational costs. We further demonstrated its practical utility in downstream tasks such as data augmentation, highlighting its broader applicability in generative modeling. 

A limitation is that as discussed in~\cref{subsec:rationale}, a naive application to deterministic samplers may fail due to the absence of a contracting effect. A promising future direction is to address this limitation, extending the effectiveness of our approach to a wider range of generative processes.

\section*{Impact Statement}

One potential negative impact is the intentional misuse of our sampler to suppress the generation of minority-featured samples. This could be achieved by setting $\gamma < 1.0$ with $\Delta_t > 0$, potentially biasing the initialization toward high-density regions (as explored in~\cref{subsec:add_ablations}). Recognizing and mitigating this risk is essential, emphasizing the importance of responsible deployment to ensure inclusivity in generative modeling.


\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Additional Discussions, Ablations, and Analyses}

\subsection{Related work}
\label{sec:related_work}

\setlength{\columnsep}{7.0pt}%
\begin{wrapfigure}[21]{r}{0.45\linewidth}
    \centering
   \vspace{-3mm}
    \includegraphics[width=\linewidth]{./figs/main/low_density_emphasis.png}
 \vspace{-7mm}
    \caption{\textbf{Low-density emphasis impact of Boost-and-Skip.} We visualize $\hat{\sigma}_0^2$ (\ie, the scale of $\hat{\bs \Sigma}_0 \coloneqq \hat{\sigma}_0^2 {\bs I}$) across $T_{\text{skip}} / T$ under the settings specified in~\cref{corollary}, with $\sigma_0 = 2$. Observe that the variance of $\hat{\bs \Sigma}_0$ surpasses that of ${\bs \Sigma}_0$ for $\gamma > 1$ and $T_{\text{skip}} < T$, demonstrating the low-density encouraging influence of the Boost-and-Skip approach.}
    \label{fig:hat_sigma_0_vs_T_s}
\end{wrapfigure}

In addition to closely related studies mentioned in~\cref{sec:intro}~\citep{sehwag2022generating, um2023don, um2024self, um2024minorityprompt}, several other works explore distinct conditions and scenarios in the context of minority generation~\cite{yu2020inclusive, lin2022raregan, qin2023class, huang2023enhanced, samuel2023all}. One instance is~\citet{qin2023class} wherein the authors develop a training technique to mitigate a class imbalance issue when constructing class-conditional diffusion models. A distinction w.r.t. ours is that they require a specialized training, and their method is limited to conditional diffusion models. Another notable study is done by~\citet{samuel2023all}. Specifically using text-to-image (T2I) diffusion models~\citep{rombach2022high}, they develop a sampler to faithfully generate samples of unique text-prompts by employing real reference data instances associated with the given prompts. The key distinction to ours is that their method is limited to T2I models and rely upon a set of real reference data. 

A related yet distinct line of research is to improve the diversity of conventional diffusion samplers~\citep{sadat2023cads, corso2023particle, lu2024procreate}. For instance,~\citet{sadat2023cads} propose a simple conditioning technique to boost-up the diversity by introducing time-scheduled noise perturbations in the conditional embedding space. The difference from our study is that their method is confined to conditional diffusion models and not specifically designed for minority generation. The approaches in~\citet{corso2023particle, lu2024procreate} share similar spirits, repelling intermediate latent samples within an inference batch to produce visually distinct outputs. However, as with~\citet{um2024self, um2024minorityprompt}, these methods often introduce computational challenges, \eg, due to the reliance on backpropagation.


\subsection{Illustration of the impact of low-density emphasis}
\label{subsec:ld_emphasis}

We continue from~\cref{subsec:rationale} and provide a visualization on the effect of low-density emphasis of Boost-and-Skip. Let us first recall~\cref{corollary}:
\begin{manualcorollary}{\ref{corollary}}
    Suppose ${\bs \Sigma}_0 = \sigma_0^2 {\bs I}$ and $\hat{\bs \Sigma}_{T_{\text{skip}}} = \gamma^2 {\bs I}$, and define the quantity (if it exists)
    \begin{align*}
        \kappa \coloneqq \sqrt{(\gamma^2 - 1)/(\sigma_0^2 - 1)}.
    \end{align*}
    The variance-amplification effect of $\hat{\bs \Sigma}_0$ occurs iff
    \begin{align*}
    T_{\text{skip}} \in
    \begin{cases}
        \varnothing & \text{if } \gamma \leq 1 \leq \sigma_0, \\
        (\alpha^{-1}(\kappa),\infty) & \text{if } 1 < \gamma,\sigma_0, \\
        [0,\alpha^{-1}(\kappa)) & \text{if } 1 > \gamma,\sigma_0, \\
        [0,\infty) & \text{if } \sigma_0 \leq 1 \leq \gamma \text{ and } (\sigma_0,\gamma) \neq \mathbf{1},
    \end{cases}
    \end{align*}
    where we define $\alpha^{-1}(\kappa) \coloneqq 0$ when $\kappa > 1$.
\end{manualcorollary}


\cref{fig:hat_sigma_0_vs_T_s} illustrates the behavior of $\hat{\bs \Sigma}_0 \coloneqq \hat{\sigma}_0^2 {\bs I}$ under the conditions specified in Corollary~\ref{corollary}, where $\sigma_0 = 2$. We see that the variance scale of $\hat{\bs \Sigma}_0$ exceeds that of ${\bs \Sigma}_0$ for $\gamma > 1$ and $T_{\text{skip}} < T$, demonstrating the variance-boosting effect induced by the proposed modifications. Notably, regardless of the value of $\gamma$, this amplification effect does not occur for $T_{\text{skip}} \approx T$, thus supporting the effectiveness of our time-skipping technique for promoting low-density emphasis.



\subsection{Further results on contraction theory}
\label{subsec:further_ct}

We continue from~\cref{subsec:rationale} and extend our analysis of contraction theory in the context of Boost-and-Skip. To this end, we first explore its behavior in non-stochastic generative processes that lack the contraction effect. Specifically, we consider its application to the probability flow ODE (PF-ODE)~\citep{song2020score} associated with~\eqref{eq:rVPSDE_emp}:
\begin{align}
    \label{eq:rPFODE_emp}
    \dx = \left[ -\frac{1}{2}  \beta(t) \bx  - \frac{1}{2}\beta(t) {\bs s}_{\bth} (\bx, t) \right] \dt.
\end{align}
The following proposition characterizes the density of generated samples $\hat{p}_{\text{ODE}}$ when going through the above ODE under the same settings as~\cref{prop:gaussian_SDE}:

\begin{proposition}
\label{prop:gaussian_ODE}
    Consider the same data distribution $p_0$ and the optimal score function ${\bs s}_{\bth} (\bx, t)$ as~\cref{prop:gaussian_SDE}.  Suppose the PF-ODE in~\eqref{eq:rPFODE_emp} is initialized with $\hat{\bx}(T_{\text{skip}}) \sim {\cal N}(\hat{{\bs \mu}}_{T_{\text{skip}}}, \hat{{\bs \Sigma}}_{T_{\text{skip}}} ) $. Then, the resulting distribution at $t = 0$ is given by $\hat{\bx}_{\text{ODE}}(0) \sim {\cal N} (\hat{{\bs \mu}}_{0, \text{ODE}}, \hat{{\bs \Sigma}}_{0, \text{ODE}})$, where
    \begin{align}
        \hat{{\bs \mu}}_{0, \text{ODE}} &\coloneqq {\bs \mu}_0 + {\bs \Sigma}_0^{1/2} {\bs \Sigma}_{T_{\text{skip}}}^{-1/2} (\hat{{\bs \mu}}_{T_{\text{skip}}} - {\bs \mu}_{T_{\text{skip}}}), \label{eq:prop_gaussian_ODE_mu0hat} \\
        \hat{{\bs \Sigma}}_{0, \text{ODE}} &\coloneqq {\bs \Sigma}_0 {\bs \Sigma}_{T_{\text{skip}}}^{-1}\hat{{\bs \Sigma}}_{T_{\text{skip}}}. \label{eq:prop_gaussian_ODE_Sigma0hat}
    \end{align}
    Here, ${\bs \mu}_{T_{\text{skip}}}$ and ${\bs \Sigma}_{T_{\text{skip}}}$ are defined as in Eqs. (\ref{eq:mu_Ts}) and (\ref{eq:Sigma_Ts}), respectively.
\end{proposition}


We leave the proof in~\cref{subsec:proof_gaussian_ODE}. We see in~\eqref{eq:prop_gaussian_ODE_Sigma0hat} that the variance $\hat{{\bs \Sigma}}_0$ is multiplied by the initial variance ${\bs \Sigma}_{T_{\text{skip}}}$. This implies that the low-density emphasis impact observed in the SDE-case may also manifest in the considered ODE case. However, the mean and variance formulas of the ODE-generated distribution $\hat{p}_{\text{ODE}}$ lack the $\alpha(T_{\text{skip}})$-multiplication that contributes to the recovery of $p_0$, indicating the inability of (non-stochastic) ODEs for error rectification. Below, we present a weaker but more general stochastic contraction result which shows that a similar phenomenon occurs with VP-SDE with general data distributions.

\begin{proposition}[Error contraction for VP-SDE diffusion models]
\label{prop:contin_contraction}
    Assume the same setup as Theorem 3 in \citet{xu2023restart}, \ie, $\| t \nabla \log p_t(\bx) \| \leq L_1$ and $\|\bx_t\| < B/2$ for any $\bx_t$ in the support of $p_t$ and reverse-SDE trajectories. Let
    \begin{gather}
        \bx_{t_{\min}}^{\sde} = \sde(\bx_{t_{\max}}, t_{\max} \rightarrow t_{\min}), \ \by_{t_{\min}}^{\sde} = \sde(\by_{t_{\max}}, t_{\max} \rightarrow t_{\min}) \\
        \bx_{t_{\min}}^{\ode} = \ode(\bx_{t_{\max}}, t_{\max} \rightarrow t_{\min}), \ \by_{t_{\min}}^{\ode} = \ode(\by_{t_{\max}}, t_{\max} \rightarrow t_{\min}) 
    \end{gather}
    where $\sde$ and $\ode$ denote solutions to the reverse-SDE and PF-ODE, respectively. Then
    \begin{gather}
    \tv(\bx_{t_{\min}}^{\ode},\by_{t_{\min}}^{\ode}) = \tv(\bx_{t_{\max}},\by_{t_{\max}}) \label{eq:TVode}\\
    \textstyle \tv(\bx_{t_{\min}}^{\sde},\by_{t_{\min}}^{\sde}) \leq \left( 1 - 2Q\left( \frac{B}{2\sqrt{\alpha(t_{\max})^{-2} - \alpha(t_{\min})^{-2}}} \right) \cdot e^{-B L_1 / t_{\min} - L_1^2 \alpha(t_{\max})^{-2} / t_{\min}^2} \right) \tv(\bx_{t_{\max}},\by_{t_{\max}}) \label{eq:TVsde}
    \end{gather}
    where $Q(r) \coloneqq \mathbb{P}(\varepsilon \geq r)$ for $\varepsilon \sim \mathcal{N}(0,1)$.
\end{proposition}

The proof can be found in Appendix \ref{subsec:contin_contraction}. Here, $\tv$ denotes the total variation distance. Indeed, we observe that PF-ODE does not contract any initial error. On the other hand, the multiplicative factor on the RHS of \eqref{eq:TVsde} shows that reverse-SDE reduces initial error. We empirically found that this absence of the rectifying capability often makes ODE-based samplers struggle to generate high-quality minority samples with Boost-and-Skip. See Figures~\ref{figToyExample},~\ref{figTrajAnalysis} and~\ref{figTrajSamples} for instance.

\subsection{How Boost-and-Skip works: a signal processing perspective}
\label{subsec:sp}

We continue from~\cref{subsec:rationale} and investigate the principle of Boost-and-Skip in a signal processing viewpoint. As explored in~\citet{lin2024common, hu2024one}, generated samples from diffusion models are often affected by initial Gaussian noise, particularly in shaping low-frequency components of images (like brightness). This effect is especially pronounced in diffusion frameworks that employ noise schedules with non-zero terminal SNR (\ie, $\alpha(T) \not \approx 0$), allowing low-frequency components of the initial noise to propagate into the generative process.


From this standpoint, Boost-and-Skip can be viewed as a two-step approach. First, the low-frequency components of the noise are amplified through variance \emph{boosting} (by Plancherel theorem~\citep{plancherel1910contribution}), enhancing the visual variability of the random Gaussian noise. Second, this amplified low-frequency information is made more influential by initiating the generative process at a \emph{skipped} initial timestep $\Ts$ with a non-zero terminal SNR ($\alpha(\Ts) \not \approx 0$). We conducted an empirical analysis to support this perspective. See~\cref{tab:sp} for explicit details.

\begin{table*}[!t]
    \centering
    \begin{subtable}[t]{0.48\textwidth}
        \centering
        \scalebox{0.9}{
            \begin{tabular}{lcccc}
                \toprule
                $f_{\text{cutoff, LPF}}$ & cFID $\downarrow$ & {sFID} $\downarrow$ & Prec $\uparrow$ & Rec $\uparrow$ \\
                \midrule
                64 (uncut) & 23.56 & 12.17 & 0.77 & 0.50 \\
                48         & 23.38 & 12.31 & 0.77 & 0.50 \\
                32         & 23.82 & 12.32 & 0.77 & 0.51 \\
                16         & 28.46 & 14.34 & 0.84 & 0.49 \\
                8          & 33.08 & 14.98 & 0.84 & 0.44 \\
                \bottomrule
            \end{tabular}
        }
        \caption{Low-pass filtered after boosting}
        \label{tab:lpf}
    \end{subtable}
    \begin{subtable}[t]{0.48\textwidth}
        \centering
        \scalebox{0.9}{
            \begin{tabular}{lcccc}
                \toprule
                $f_{\text{cutoff, HPF}}$ & cFID $\downarrow$ & {sFID} $\downarrow$ & Prec $\uparrow$ & Rec $\uparrow$ \\
                \midrule
                 0 (uncut) & 23.56  & 12.17 & 0.77 & 0.50 \\
                 8         & 60.16  & 18.26 & 0.95 & 0.23 \\
                 16        & 75.68  & 21.33 & 0.97 & 0.16 \\
                 32        & 95.50  & 25.88 & 0.98 & 0.10 \\
                 48        & 100.03 & 28.26 & 0.97 & 0.12 \\
                \bottomrule
            \end{tabular}
        }
        \caption{High-pass filtered after boosting}
        \label{tab:hpf}
    \end{subtable}
    \vspace{-2.0mm}
    \caption{ \textbf{Amplification of low-frequency components is crucial to the success of Boost-and-Skip.} The performance values were evaluated on CelebA, with low-pass or high-pass filters applied after the boosted initialization. $f_{\text{cutoff, LPF}}$ represents the cut-off frequency for the low-pass filter, while $f_{\text{cutoff, HPF}}$ denotes the cut-off frequency for the high-pass filter. While the low-pass filtered cases maintain performance gains robustly across varying cut-off frequencies, high-pass filtering leads to immediate degradation, as seen in a significant performance drop starting at $f_{\text{cutoff, HPF}} = 8$. This suggests that the enhanced minority generation is primarily driven by the amplification of low-frequency components. }
    \label{tab:sp}
\end{table*}

\setlength{\columnsep}{7.0pt}%
\begin{wraptable}[15]{r}{0.4\linewidth}
    \centering
    \fontsize{10}{10}\selectfont
        {
        \scalebox{1.0}{
    \begin{tabular}{lcccc}
        \toprule
          $\gamma^2$ & Prec $\uparrow$ & Rec $\uparrow$ & Den $\uparrow$ & Cov $\uparrow$ \\
        \midrule
        1.0 & 0.851 & \textbf{0.627} & 1.290 & \textbf{0.940} \\
        0.8 & 0.874 & \udl{0.588} & 1.384 & \udl{0.938} \\
        0.6 & \textbf{0.881} & 0.586 & \textbf{1.495} & 0.936 \\
        0.5 & \udl{0.879} & 0.565 & \udl{1.478} & 0.926 \\
        \bottomrule
    \end{tabular}
    }}
    \vspace{-2mm}
    \caption{
    \textbf{Quality-enhancing effect of Boost-and-Skip on CelebA.}
    ``Den'' and ``Cov'' denote Density and Coverage~\citep{naeem2020reliable}, an additional set of quality-diversity metrics.
    For computing these metrics, we used the CelebA test set as the baseline real data (rather than employing minority data as in~\cref{tab:main_results}) to assess the quality-diversity tradeoff within the ground-truth data manifold of CelebA.
    }
    \label{tab:gamma_lt_1}
\end{wraptable}


\subsection{Extended ablation: effect of using $\gamma < 1.0$}
\label{subsec:add_ablations}

One may wonder: what if we employ $\gamma < 1.0$, effectively reversing the direction of minority generation? We found that this oppositional choice often exerts an interesting \emph{quality-enhancing} effect on the generative process. Intuitively, initializing with $\gamma < 1.0$ can be interpreted as starting generation from a \emph{high-density} region, which may guide the sampling process toward higher-quality outputs. Our empirical findings confirm this intuition.  

\Cref{tab:gamma_lt_1} presents the effect of $\gamma$ on various quality and diversity metrics for CelebA. Note that these metrics were computed using the test dataset, rather than minority data as was employed for \cref{tab:main_results}. This allows us to examine the quality-diversity tradeoff from a broader perspective, specifically within the ground-truth data manifold of CelebA. As shown in \cref{tab:gamma_lt_1}, using $\gamma < 1.0$ improves sample quality at the expense of diversity, demonstrating that Boost-and-Skip serves as a control mechanism not only for diversity but also for quality.

Another key observation is that our approach is complementary to existing quality-enhancing techniques, such as Classifier-Free Guidance (CFG)~\citep{ho2022classifier}. This suggests that Boost-and-Skip can be integrated with such methods to further refine generative performance, offering additional flexibility in balancing quality and diversity.

\subsection{Discussion: Implications of Boost-and-Skip}

We note that our framework has several important implications. The first one is that it improves the practical relevance of minority samplers by significantly reducing the computational overhead associated with existing methods. Secondly, our approach provides a simple mechanism for improving the diversity of diffusion models, a feature that has been largely absent in the research community. Another important point is to demonstrate that the pathological non-zero terminal SNR, arising from the training-inference mismatch~\citep{lin2024common, hu2024one}, can actually be advantageous. Lastly, we highlight potential opportunities in the initialization of stochastic sampling, a largely under-explored area compared to deterministic samples of diffusion models.


\section{Proofs}
\label{sec:proofs}

\subsection{Proof of~\cref{prop:ts_flaw}}
\label{subsec:proof_ts_flaw}

\begin{manualproposition}{\ref{prop:ts_flaw}}
    Consider the temperature-scaled reverse VP-SDE in~\eqref{eq:rSDE_ddpm_ts}. Assuming the score function is optimal, \ie, ${\bs s}_\bth (\bx, t) = \nabla_\bx \log p_t(\bx)$, the marginal densities of samples generated by this SDE are not equal to $\{ \frac{1}{Z_t}  p_t(\bx)^{1/\tau} \}_{t=0}^T$ in general.
\end{manualproposition}
\begin{proof}
We first rewrite~\eqref{eq:rSDE_ddpm_ts} using the ground-truth score function $\nabla_\bx \log p_t(\bx)$ under the optimality assumption:
\begin{align}
\label{eq:rSDE_general_ts_apdx_gtscore}
    \dx = \left[ -\frac{1}{2}  \beta(t) \bx  - \beta(t)  \nabla_\bx \log p_t(\bx)^{1/\tau} \right] \dt + \sqrt{\beta(t)} \td \tilde{\bw}.
\end{align}
Let us assume that the SDE in~\eqref{eq:rSDE_general_ts_apdx_gtscore} produces samples along $  \{ \frac{1}{Z_t} p_t (\bx)^{1/\tau} \}_{t=0}^T$. Then, we should have:
\begin{align}
\label{eq:ts_proof_cond}
    \frac{1}{Z_t} p_t (\bx)^{1 / \tau} = \int p_{0t} (\bx | \bx_0) \frac{1}{Z} p_0 (\bx_0)^{1/\tau} \; \td \bx_0,
\end{align}
where $Z$ represents a normalization constant w.r.t. $p_0$. We disprove this by providing a counterexample. Consider a dirac-delta density $p_0$ and a VP-SDE forward kernel $p_{0t}$:
\begin{align}
    p_0(\bx_0) & \coloneqq \delta (\bx_0 - {\bs \mu}), \\
    p_{0t} (\bx | \bx_0) & \coloneqq \left( \frac{1}{\sqrt{2 \pi ( 1 - \alpha(t) )}} \right)^{d} \exp\left( -\frac{\| \bx - \sqrt{\alpha(t)} \bx_0 \|_2^2}{ 2( 1 - \alpha(t) ) } \right),
\end{align}
where $\alpha(t)$ represents the noise schedule, which is a function of $\beta(t)$. With this instantiation, we expand $p_t(\bx)^{1/\tau}$ on the LHS of~\eqref{eq:ts_proof_cond} by using the definition of $p_t$:
\begin{align}
    p_t(\bx)^{1/\tau} &= \left[ \int   p_{0t} (\bx | \bx_0) p_0(\bx_0)     \; \td \bx_0  \right]^{1/\tau} \\
    &= \left[  \int \left( \frac{1}{\sqrt{2 \pi ( 1 - \alpha(t) )}} \right)^{d} \exp\left( -\frac{\| \bx - \sqrt{\alpha(t)} \bx_0 \|_2^2}{ 2( 1 - \alpha(t) ) } \right)  \delta(\bx_0 - {\bs \mu})   \; \td \bx_0 \right]^{1/\tau} \\
    &= \left( \frac{1}{\sqrt{2 \pi ( 1 - \alpha(t) )}} \right)^{d/\tau} \exp\left( -\frac{\| \bx - \sqrt{\alpha(t)} {\bs \mu} \|_2^2}{ 2\tau( 1 - \alpha(t) ) } \right). \label{eq:ts_proof_LHS}
\end{align}
Since $\frac{1}{Z_t} p_t(\bx)^{1 / \tau}$ is Gaussian, we can determine the normalization constant $Z_t$, yielding the following expression for the LHS of~\eqref{eq:ts_proof_cond}:
\begin{align}
    \frac{1}{Z_t} p_t(\bx)^{1/\tau} = \left( \frac{1}{\sqrt{2 \pi \tau( 1 - \alpha(t) )}} \right)^{d}  \exp\left( -\frac{\| \bx - \sqrt{\alpha(t)} {\bs \mu} \|_2^2}{ 2\tau( 1 - \alpha(t) ) } \right).
\end{align}
On the other hand, the RHS of~\eqref{eq:ts_proof_cond} is:
\begin{align}
    \int p_t (\bx | \bx_0) \frac{1}{Z} p_0 (\bx_0)^{1/\tau} \; \td \bx_0 & = \int    \left( \frac{1}{\sqrt{2 \pi ( 1 - \alpha(t) )}} \right)^{d} \exp\left( -\frac{\| \bx - \sqrt{\alpha(t)} \bx_0 \|_2^2}{ 2( 1 - \alpha(t) ) } \right)  \frac{1}{Z} \delta(\bx_0 - {\bs \mu})^{1/\tau}         \; \td \bx_0 \\
    & = \left( \frac{1}{\sqrt{2 \pi ( 1 - \alpha(t) )}} \right)^{d} \exp\left( -\frac{\| \bx - \sqrt{\alpha(t)} {\bs \mu} \|_2^2}{ 2( 1 - \alpha(t) ) } \right),
\end{align}
which is not equal to the LHS (\ie,~\eqref{eq:ts_proof_LHS}), \eg, if $\tau \ne 1$, contradicting our initial assumption. This completes the proof.

\end{proof}

\subsection{Proof of~\cref{prop:gaussian_SDE}}
\label{subsec:proof_gaussian_SDE}

\begin{lemma}[Solution to the VP-SDE]
    \label{lem:vp_sde_solution}
    Let $\bx_0 \sim {\cal N} ({\bs \mu}_0, {\bs \Sigma}_0)$, and suppose $\bx_t$ on  $t \in (0,\infty)$ evolves according to
    \begin{align}
        \dx = -\frac{1}{2} \beta(t) \bx \dt + \sqrt{\beta(t)} \td {\bs w}
    \end{align}
    where $\beta(t)$ is a positive function which integrates to $\infty$. The forward process is Gaussian with mean and covariance
    \begin{align}
        \bs{\mu}_t &\coloneqq \alpha(t) \bs{\mu}_0, \\
        \bs{\Sigma}_t &\coloneqq \bs{I} + \alpha(t)^2 (\bs{\Sigma}_0 - \bs{I}),
    \end{align}
    where $\alpha(t) = e^{-\frac{1}{2}\int_0^t \beta(s) \, ds}$ such that $\lim_{t \rightarrow \infty} \alpha(t) = 0$.
\end{lemma}

\begin{proof}
Mean $\bs{\mu}_t$ of $\bx_t$ is governed by the ODE
\begin{align}
    \textstyle d\bs{\mu}_t / dt = \mathbb{E}[-\frac{1}{2} \beta(t) \bx_t] = -\frac{1}{2} \beta(t) \bs{\mu}_t
\end{align}
so its solution is given as
\begin{align}
    \bs{\mu}_t = \alpha(t) \bs{\mu}_0.
\end{align}
The covariance $\bs{\Sigma}_t$ of $\bx_t$ is governed by the ODE
\begin{align}
    d\bs{\Sigma}_t / dt &= \textstyle \mathbb{E}[-\frac{1}{2}\beta(t)\bx_t(\bx_t - \bs{\mu}_t)^\top] + \mathbb{E}[-\frac{1}{2}\beta(t)(\bx_t - \bs{\mu}_t)\bx_t^\top] + \mathbb{E}[\beta(t)] \\
    &= \textstyle \mathbb{E}[-\frac{1}{2}\beta(t)(\bx_t - \bs{\mu}_t)(\bx_t - \bs{\mu}_t)^\top] + \mathbb{E}[-\frac{1}{2}\beta(t)(\bx_t - \bs{\mu}_t)(\bx_t - \bs{\mu}_t)^\top] + \beta(t) \\
    &\textstyle \quad + \mathbb{E}[-\frac{1}{2} \beta(t) \bs{\mu}_t(\bx_t - \bs{\mu}_t)^\top] + \mathbb{E}[-\frac{1}{2} \beta(t) (\bx_t - \bs{\mu}_t)\bs{\mu}_t^\top] \\
    &= \textstyle \mathbb{E}[-\frac{1}{2}\beta(t)(\bx_t - \bs{\mu}_t)(\bx_t - \bs{\mu}_t)^\top] + \mathbb{E}[-\frac{1}{2}\beta(t)(\bx_t - \bs{\mu}_t)(\bx_t - \bs{\mu}_t)^\top] + \beta(t) \\
    &= \textstyle -\beta(t)(\bs{\Sigma}_t - \bs{I})
\end{align}
whose solution is given as (solve the ODE after making the change of variables $\widehat{\bs{\Sigma}}_t = \bs{\Sigma}_t - \bs{I}$)
\begin{align}
    \bs{\Sigma}_t = \bs{I} + \alpha(t)^2 (\bs{\Sigma}_0 - \bs{I}).
\end{align}
Since the initial distribution at $t = 0$ is a Gaussian and VP-SDE is a linear SDE, its solution is also a Gaussian.
\end{proof}

\begin{manualproposition}{\ref{prop:gaussian_SDE}}
Let the data distribution be $\bx(0) \sim {\cal N} ({\bs \mu}_0, {\bs \Sigma}_0)$, and assume the optimal score function ${\bs s}_\bth (\bx, t) = \nabla_\bx \log p_t(\bx)$ trained on $p_0$ via the forward SDE in~\eqref{eq:fVPSDE}. Suppose the reverse SDE in~\eqref{eq:rVPSDE_emp} is initialized with $\hat{\bx}(T_{\text{skip}}) \sim {\cal N}(\hat{{\bs \mu}}_{T_{\text{skip}}}, \hat{{\bs \Sigma}}_{T_{\text{skip}}} ) $. Then, the resulting generated distribution corresponds to $\hat{\bx}(0) \sim {\cal N} (\hat{{\bs \mu}}_0, \hat{{\bs \Sigma}}_0)$, where
    \begin{align}
        \hat{{\bs \mu}}_0 &\coloneqq {\bs \mu}_0 + \alpha(T_{\text{skip}}){\bs \Sigma}_0 {\bs \Sigma}_{T_{\text{skip}}}^{-1} (\hat{{\bs \mu}}_{T_{\text{skip}}} - {\bs \mu}_{T_{\text{skip}}}), \\
        \hat{{\bs \Sigma}}_0 &\coloneqq {\bs \Sigma}_0 + \alpha(T_{\text{skip}})^2 {\bs \Sigma}_0^2 {\bs \Sigma}_{T_{\text{skip}}}^{-2}(\hat{{\bs \Sigma}}_{T_{\text{skip}}} - {\bs \Sigma}_{T_{\text{skip}}}). 
    \end{align}
    Here, ${\bs \mu}_{T_{\text{skip}}}$ and ${\bs \Sigma}_{T_{\text{skip}}}$ are defined as:
    \begin{align}
        {\bs \mu}_{T_{\text{skip}}} & \coloneqq \alpha(T_{\text{skip}}) {\bs \mu}_0,  \\
        {\bs \Sigma}_{T_{\text{skip}}} & \coloneqq {\bs I} + \alpha(T_{\text{skip}})^2 ( {\bs \Sigma}_0 - {\bs I}  ).
    \end{align}
\end{manualproposition}

\begin{proof}
By Lemma \ref{lem:vp_sde_solution}, the score function is given as
\begin{align}
    \nabla_{\bx} \log p_t(\bx) = \nabla_{\bx} \log \mathcal{N}(\bx | \bs{\mu}_t, \bs{\Sigma}_t ) = - \bs{\Sigma}_t^{-1}(\bx - \bs{\mu}_t)
\end{align}
such that the reverse-SDE is
\begin{align}
    d\hat{\bx}_t &= \textstyle [-\frac{1}{2}\beta(t)\hat{\bx}_t - \beta(t) \nabla \log p_t(\hat{\bx}_t)] \, dt + \sqrt{\beta(t)} \, d\bar{\bw}_t \\
    &= \textstyle [-\frac{1}{2}\beta(t)\hat{\bx}_t + \beta(t) \bs{\Sigma}_t^{-1}(\hat{\bx}_t - \bs{\mu}_t)] \, dt + \sqrt{\beta(t)} \, d\bar{\bw}_t \\
    &= \textstyle \frac{1}{2} \beta(t) (2 \bs{\Sigma}_t^{-1} - \bs{I}) \hat{\bx}_t \, dt - \beta(t) \bs{\Sigma}_t^{-1} \bs{\mu}_t \, dt + \sqrt{\beta(t)} \, d\hat{\bw}_t \label{eq:append_rev_sde}
\end{align}
where $\hat{\bw}_t$ is a reverse-time Brownian motion. This is also a linear SDE, and since $\hat{\bx}_{T_\sk}$ is assumed to be Gaussian, $\hat{\bx}_0$ is also Gaussian. Hence, it suffices to derive the mean and covariance of $\hat{\bx}_0$. Fix some $T > 0$ and make the change of variables $(\hat{\by}_s,s) \leftarrow (\hat{\bx}_{T-t},T-t)$ such that solving the SDE
\begin{align}
    \textstyle d\hat{\by}_s = - \frac{1}{2} \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) \hat{\by}_s \, ds + \beta(T-s) \bs{\Sigma}_{T-s}^{-1} \bs{\mu}_{T-s} \, ds + \sqrt{\beta(T-s)} \, d\bw_s
\end{align}
from $s = 0$ to $T$ is equivalent to solving the original reverse-SDE \eqref{eq:append_rev_sde} from $t = T$ to $0$. We shall now derive the mean and covariance of $\hat{\by}_s$ at $s = T$. To this end, define the eigen-decomposition
\begin{align}
    \bs{\Sigma}_0 = \bs{Q} \bs{\Lambda} \bs{Q}^\top, \quad \bs{\Lambda} = \diag(\lambda_n)
\end{align}
and recall that $\bm_t = \alpha(t) \bm_0$ and $\bs{\Sigma}_t = \bs{I} + \alpha(t)^2 (\bs{\Sigma}_0 - \bs{I})$.

The mean $\hat{\bm}_s$ of $\hat{\by}_s$ is governed by the ODE
\begin{align}
    d\hat{\bm}_s / ds &= \textstyle \bE[- \frac{1}{2} \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) \hat{\by}_s + \beta(T-s) \bs{\Sigma}_{T-s}^{-1} \bm_{T-s}] \\
    &= \textstyle - \frac{1}{2} \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) \hat{\bm}_s + \beta(T-s) \bs{\Sigma}_{T-s}^{-1} \bm_{T-s} \\
    &= \textstyle -\frac{1}{2} \alpha(T-s)\beta(T-s) \bs{Q} \diag(\frac{\alpha(T-s)^{-1} - \alpha(T-s) (\lambda_n - 1)}{1 + \alpha(T-s)^2 (\lambda_n - 1)}) \bs{Q}^\top \widehat{\bm}_s \\
    &\quad \textstyle + \alpha(T-s) \beta(T-s) \bs{Q} \diag(\frac{1}{1 + \alpha(T-s)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0. \label{eq:append_rev_sde_mean}
\end{align}
Define $\phi(x;a) \coloneqq \log(ax^2 + 1) - \log(x)$ such that
\begin{align}
    \textstyle \frac{d\phi(\alpha(T-s);\lambda_n-1)}{ds} &= \textstyle \frac{d\phi(\alpha(T-s);\lambda_n-1)}{d\alpha(T-s)} \cdot \frac{d\alpha(T-s)}{d(T-s)} \cdot \frac{d(T-s)}{ds} \\
    &= \textstyle -\frac{\alpha(T-s)^{-1} - \alpha(T-s) (\lambda_n - 1)}{1 + \alpha(T-s)^2 (\lambda_n - 1)} \cdot \left( -\frac{1}{2} \beta(T-s) \alpha(T-s) \right) \cdot (-1) \\
    &= \textstyle -\frac{1}{2} \beta(T-s) \alpha(T-s) \frac{\alpha(T-s)^{-1} - \alpha(T-s) (\lambda_n - 1)}{1 + \alpha(T-s)^2 (\lambda_n - 1)}.
\end{align}
This motivates the transition matrix
\begin{align}
    \bs{\Psi}(s,s_0) &\coloneqq \bs{Q} \diag(e^{\phi(\alpha(T-s);\lambda_n-1) - \phi(\alpha(T-s_0);\lambda_n-1)}) \bs{Q}^\top \\
    &= \textstyle \bs{Q} \diag(\frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{\alpha(T-s)} \cdot \frac{\alpha(T-s_0)}{(\lambda_n-1)\alpha(T-s_0)^2 + 1}) \bs{Q}^\top \\
    &= \textstyle \frac{\alpha(T-s_0)}{\alpha(T-s)} \bs{\Sigma}_{T-s} \bs{\Sigma}_{T-s_0}^{-1}
\end{align}
with which we can calculate the solution to \eqref{eq:append_rev_sde_mean} as
\begin{align}
    \hat{\bm}_s &= \textstyle \bs{\Psi}(s,0) \hat{\bm}_0 + \int_0^s \bs{\Psi}(s,\sigma) \alpha(T-\sigma) \beta(T-\sigma) \bs{Q} \diag(\frac{1}{1 + \alpha(T-\sigma)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0 \, d\sigma \\
    &= \textstyle \frac{\alpha(T)}{\alpha(T-s)} \bs{\Sigma}_{T-s} \bs{\Sigma}_{T}^{-1} \widehat{\bm}_0 + \bm_{T-s} - \frac{\alpha(T)}{\alpha(T-s)} \bs{\Sigma}_{T-s} \bs{\Sigma}_{T}^{-1} \bm_T \\
    &= \textstyle \bm_{T-s} + \frac{\alpha(T)}{\alpha(T-s)} \bs{\Sigma}_{T-s} \bs{\Sigma}_{T}^{-1} (\hat{\bm}_0 - \bm_T) \label{eq:append_rev_sde_mean_sol}
\end{align}
where the integral in the first line is calculated as
\begin{align}
    &\textstyle \int_0^s \bs{\Psi}(s,\sigma) \alpha(T-\sigma) \beta(T-\sigma) \bs{Q} \diag(\frac{1}{1 + \alpha(T-\sigma)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0 \, d\sigma \\
    &= \textstyle \int_0^s \alpha(T-\sigma) \beta(T-\sigma) \bs{Q} \diag(\frac{\alpha(T-\sigma)}{\alpha(T-s)} \cdot \frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{(\lambda_n-1)\alpha(T-\sigma)^2 + 1}) \diag(\frac{1}{1 + \alpha(T-\sigma)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0 \, d\sigma \\
    &= \textstyle \int_0^s \bs{Q} \diag(\frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{\alpha(T-s)}) \diag(\frac{\alpha(T-\sigma)^2 \beta(T-\sigma)}{(1 + \alpha(T-\sigma)^2 (\lambda_n - 1))^2}) \bs{Q}^\top \bm_0 \, d\sigma \\
    &= \textstyle \bs{Q} \diag(\frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{\alpha(T-s)}) \diag(2 \int_0^s \frac{1}{2} \alpha(T-\sigma)\beta(T-\sigma) \frac{\alpha(T-\sigma)}{(1 + \alpha(T-\sigma)^2 (\lambda_n - 1))^2} \, d\sigma) \bs{Q}^\top \bm_0 \\
    &= \textstyle \bs{Q} \diag(\frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{\alpha(T-s)}) \diag(2 \left[ \frac{\alpha(T-\sigma)^2}{2((\lambda_n-1)\alpha(T-\sigma)^2 + 1)} \right]_0^s ) \bs{Q}^\top \bm_0 \\
    &= \textstyle \bs{Q} \diag(\frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{\alpha(T-s)}) \diag(\left[ \frac{\alpha(T-s)^2}{(\lambda_n-1)\alpha(T-s)^2 + 1} - \frac{\alpha(T)^2}{(\lambda_n-1)\alpha(T)^2 + 1} \right] ) \bs{Q}^\top \bm_0 \\
    &= \textstyle \bs{Q} \diag(\left[ \alpha(T-s) - \frac{\alpha(T)^2}{\alpha(T-s)} \cdot \frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{(\lambda_n-1)\alpha(T)^2 + 1} \right] ) \bs{Q}^\top \bm_0 \\
    &= \textstyle \alpha(T-s) \bm_0 -  \frac{\alpha(T)}{\alpha(T-s)} \cdot 
 \bs{Q} \diag( \frac{(\lambda_n-1)\alpha(T-s)^2 + 1}{(\lambda_n-1)\alpha(T)^2 + 1} ) \bs{Q}^\top (\alpha(T) \bm_0) \\
    &= \textstyle \bm_{T-s} - \frac{\alpha(T)}{\alpha(T-s)} \bs{\Sigma}_{T-s} \bs{\Sigma}_T^{-1} \bm_{T}.
\end{align}
Setting $s = T = T_\sk$ in \eqref{eq:append_rev_sde_mean_sol}, we see that
\begin{align}
    \bE[\hat{\bx}_0] = \bE[\hat{\by}_{T_\sk}] &= \textstyle \bm_{T_\sk-T_\sk} + \frac{\alpha(T_\sk)}{\alpha(T_\sk-T_\sk)} \bs{\Sigma}_{T_{\sk} - T_{\sk}} \bs{\Sigma}_{T_\sk}^{-1} (\bE[\hat{\by}_0] - \bm_{T_\sk}) \\
    &= \textstyle \bm_{0} + \alpha(T_\sk) \bs{\Sigma}_{0} \bs{\Sigma}_{T_\sk}^{-1} (\bE[\hat{\bx}_{T_\sk}] - \bm_{T_\sk})
\end{align}
where we have used the fact that $\alpha(0) = 1$.

The covariance $\hat{\bs{\Sigma}}_s$ of $\hat{\by}_s$ is governed by the ODE
\begin{align}
    d\hat{\bs{\Sigma}}_s/ds &= \textstyle \bE[- \frac{1}{2} \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) \hat{\by}_s (\hat{\by}_s - \hat{\bm}_s)^\top] \\
    &\quad \textstyle + \bE[- \frac{1}{2} \beta(T-s) (\hat{\by}_s - \hat{\bm}_s) \by_s^\top (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I})] + \beta(T-s) \\
    &= \textstyle \bE[- \frac{1}{2} \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) (\hat{\by}_s - \hat{\bm}_s) (\hat{\by}_s - \hat{\bm}_s)^\top] \\
    &\quad \textstyle + \bE[- \frac{1}{2} \beta(T-s) (\hat{\by}_s - \hat{\bm}_s) (\hat{\by}_s - \hat{\bm}_s)^\top (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I})] + \beta(T-s) \\
    &= \textstyle \bE[- \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) (\hat{\by}_s - \hat{\bm}_s) (\hat{\by}_s - \hat{\bm}_s)^\top] + \beta(T-s) \\
    &= \textstyle - \beta(T-s) (2 \bs{\Sigma}_{T-s}^{-1} - \bs{I}) \hat{\bs{\Sigma}}_s + \beta(T-s) \\
    &= \textstyle -\beta(T-s) \bs{Q} \diag(\frac{1 - \alpha(T-s)^2 (\lambda_n - 1)}{1 + \alpha(T-s)^2 (\lambda_n - 1)}) \bs{Q}^\top \hat{\bs{\Sigma}}_s + \beta(T-s).
\end{align}
This motivates a similar transition matrix (note that we multiplied 2 to $\phi$ now)
\begin{align}
    \bs{\Psi}(s,s_0) &\coloneqq \bs{Q} \diag(e^{2\phi(\alpha(T-s);\lambda_n-1) - 2\phi(\alpha(T-s_0);\lambda_n-1)}) \bs{Q}^\top \\
    &= \textstyle \bs{Q} \diag(\frac{\alpha(T-s_0)^2}{\alpha(T-s)^2} \cdot \frac{((\lambda_n-1)\alpha(T-s)^2 + 1)^2}{((\lambda_n-1)\alpha(T-s_0)^2 + 1)^2}) \bs{Q}^\top \\
    &= \textstyle \frac{\alpha(T-s_0)^2}{\alpha(T-s)^2} \bs{\Sigma}_{T-s}^2 \bs{\Sigma}_{T-s_0}^{-2}.
\end{align}
which produces the solution
\begin{align}
    \hat{\bs{\Sigma}}_s &= \textstyle \bs{\Psi}(s,0) \hat{\bs{\Sigma}}_0 + \int_0^s \bs{\Psi}(s,\sigma) \beta(T-\sigma) \, d\sigma \\
    &= \textstyle \frac{\alpha(T)^2}{\alpha(T-s)^2} \bs{\Sigma}_{T-s}^2 \bs{\Sigma}_T^{-2} \hat{\bs{\Sigma}}_0 + \bs{\Sigma}_{T-s} - \frac{\alpha(T)^2}{\alpha(T-s)^2} \bs{\Sigma}_{T-s}^2 \bs{\Sigma}_T^{-1} \\
    &= \textstyle \bs{\Sigma}_{T-s} + \frac{\alpha(T)^2}{\alpha(T-s)^2} \bs{\Sigma}_{T-s}^2 \bs{\Sigma}_T^{-2} (\hat{\bs{\Sigma}}_0 - \bs{\Sigma}_T) \label{eq:append_rev_sde_cov_sol}
\end{align}
where the integral in the first line is calculated as
\begin{align}
    &\textstyle \int_0^s \bs{\Psi}(s,\sigma) \beta(T-\sigma) \, d\sigma \\
    &= \textstyle \int_0^s \bs{Q} \diag(\frac{\alpha(T-\sigma)^2}{\alpha(T-s)^2} \cdot \frac{((\lambda_n-1)\alpha(T-s)^2 + 1)^2}{((\lambda_n-1)\alpha(T-\sigma)^2 + 1)^2}) \bs{Q}^\top \beta(T-\sigma) \, d\sigma \\
    &= \textstyle \bs{Q} \diag(\frac{((\lambda_n-1)\alpha(T-s)^2 + 1)^2}{\alpha(T-s)^2}) \diag( \int_0^s \frac{\beta(T-\sigma) \alpha(T-\sigma)^2}{((\lambda_n-1)\alpha(T-\sigma)^2 + 1)^2} \, d\sigma) \bs{Q}^\top \\
    &= \textstyle \bs{Q} \diag(\frac{((\lambda_n-1)\alpha(T-s)^2 + 1)^2}{\alpha(T-s)^2}) \diag( \left[ \frac{\alpha(T-s)^2}{(\lambda_n-1)\alpha(T-s)^2 + 1} - \frac{\alpha(T)^2}{(\lambda_n-1)\alpha(T)^2 + 1} \right] ) \bs{Q}^\top \\
    &= \textstyle \frac{1}{\alpha(T-s)^2} \bs{\Sigma}_{T-s}^2 (\alpha(T-s)^2 \bs{\Sigma}_{T-s}^{-1} - \alpha(T)^2 \bs{\Sigma}_T^{-1}) \\
    &= \textstyle \bs{\Sigma}_{T-s} - \frac{\alpha(T)^2}{\alpha(T-s)^2} \bs{\Sigma}_{T-s}^2 \bs{\Sigma}_T^{-1}.
\end{align}
Setting $s = T = T_\sk$ in \eqref{eq:append_rev_sde_cov_sol}, we see that
\begin{align}
    \cov[\hat{\bx}_0] = \cov[\hat{\by}_{T_\sk}] &= \textstyle \bs{\Sigma}_{T_\sk-T_\sk} + \frac{\alpha(T_\sk)^2}{\alpha(T_\sk-T_\sk)^2} \bs{\Sigma}_{T_\sk-T_\sk}^2 \bs{\Sigma}_{T_\sk}^{-2} (\cov[\hat{\by}_0] - \bs{\Sigma}_{T_\sk}) \\
    &= \textstyle \bs{\Sigma}_0 + \alpha(T_\sk)^2 \bs{\Sigma}_0^2 \bs{\Sigma}_{T_\sk}^{-2} (\cov[\hat{\bx}_{T_\sk}] - \bs{\Sigma}_{T_\sk}).
\end{align}
This concludes the proof.
\end{proof}

\subsection{Proof of Corollary~\ref{corollary}}

\begin{manualcorollary}{\ref{corollary}}
    Suppose ${\bs \Sigma}_0 = \sigma_0^2 {\bs I}$ and $\hat{\bs \Sigma}_{T_{\text{skip}}} = \gamma^2 {\bs I}$, and define the quantity (if it exists)
    \begin{align*}
        \kappa \coloneqq \sqrt{(\gamma^2 - 1)/(\sigma_0^2 - 1)}.
    \end{align*}
    The variance-amplification effect of $\hat{\bs \Sigma}_0$ occurs iff
    \begin{align*}
    T_{\text{skip}} \in
    \begin{cases}
        \varnothing & \text{if } \gamma \leq 1 \leq \sigma_0, \\
        (\alpha^{-1}(\kappa),\infty) & \text{if } 1 < \gamma,\sigma_0, \\
        [0,\alpha^{-1}(\kappa)) & \text{if } 1 > \gamma,\sigma_0, \\
        [0,\infty) & \text{if } \sigma_0 \leq 1 \leq \gamma \text{ and } (\sigma_0,\gamma) \neq \mathbf{1},
    \end{cases}
    \end{align*}
    where we define $\alpha^{-1}(\kappa) \coloneqq 0$ when $\kappa > 1$.
\end{manualcorollary}

\begin{proof}
    Under our assumption, variance-amplification occurs iff
    \begin{align}
        \gamma^2 - 1 > \alpha(T_\sk)^2 (\sigma_0^2 - 1). \label{eq:corollary_ineq}
    \end{align}

    \textbf{Case $\gamma \leq 1 \leq \sigma_0$.} Suppose some value of $T_\sk$ offers variance amplification. Then
    \begin{align}
        0 \geq \gamma^2 - 1 > \alpha(T_\sk)^2(\sigma_0^2 - 1) \geq 0
    \end{align}
    which is a contradiction.

    \textbf{Case $1 < \gamma,\sigma_0$.} In this case, \eqref{eq:corollary_ineq} is equivalent to
    \begin{align}
        \alpha(T_\sk) < \kappa
    \end{align}
    which is equivalent to $T_\sk \in (\alpha^{-1}(\kappa),\infty)$ since $\alpha$ is a monotone decreasing function.

    \textbf{Case $1 > \gamma,\sigma_0$.} In this case, \eqref{eq:corollary_ineq} is equivalent to
    \begin{align}
        \alpha(T_\sk) > \kappa
    \end{align}
    which is equivalent to $T_\sk \in [0,\alpha^{-1}(\kappa))$ since $\alpha$ is a monotone decreasing function.

    \textbf{Case $\sigma_0 \leq 1 \leq \gamma \text{ and } (\sigma_0,\gamma) \neq \mathbf{1}$.} In the case $\sigma_0 < 1 \leq \gamma$,
    \begin{align}
        \gamma^2 - 1 \geq 0 > \alpha(T_\sk) (\sigma_0^2 - 1)
    \end{align}
    for all values of $T_\sk$. Likewise, if $\sigma_0 \leq 1 < \gamma$,
    \begin{align}
        \gamma^2 - 1 > 0 \leq \alpha(T_\sk) (\sigma_0^2 - 1)
    \end{align}
    for all values of $T_\sk$.
\end{proof}



\subsection{Proof of~\cref{prop:ddpm_contraction}}
\label{subsec:proof_ddpm_contraction}

\begin{manualproposition}{\ref{prop:ddpm_contraction}}
Consider an empirical version of the discrete VP-SDE in~\eqref{eq:rSDE_ddpm}:
\begin{align*}
    \bx_{i-1} = \frac{1}{\sqrt{\alpha_i}} \big\{ \bx_i + (1-\alpha_i) s_{\bth}(\bx_i, i) \big\} + \sqrt{1 - \alpha_i} \bz,
\end{align*}
where $i \in \{1, \dots, N\}$. Assume the optimal score function, \ie, $s_{\bth}(\bx, i) = \nabla_{\bx} \log p_i (\bx)$, which is trained on an arbitrary data distribution $p_0$. Consider two sample trajectories $\{ \bx_{i} \}_{i=0}^N$ and $\{ \hat{\bx}_{i} \}_{i=0}^{\Ns}$ where $\Ns < N$, which are initialized with distinct distributions: $\bx_N \sim {\cal N}({\bs 0}, {\bs I})$ and $\hat{\bx}_{\Ns} \sim {\cal N}({\bs 0}, \gamma^2 {\bs I})$. Assuming that $\{ \bx_{i} \}_{i=0}^N$ is a bounded process such that $\| \bx_i \|_2 < B$ (as in~\citet{xu2023restart}), the expected error between samples from these two trajectories at step $i \in \{0, \dots, \Ns - 1\}$ is given by:
\begin{align}
    \mathbb{E}[ \| \bx_i - \hat{\bx}_i \|_2^2] \le \frac{2C}{1 - \lambda^2} + \lambda^{2(\Ns - i)}(B^2 + \gamma^2 d),
\end{align}
where $\lambda$ denotes the contraction rate:
\begin{align}
    \lambda \coloneqq \max_{j \in \{ i+1, \dots, \Ns\}} \sqrt{\alpha_j}\left( \frac{1 - \bar{\alpha}_{j-1}}{1 - \bar{\alpha}_{j}} \right),
\end{align}
and $C \coloneqq d(1 - \bar{\alpha}_{\Ns})$.
\end{manualproposition}
\begin{proof}
The proof is based on the contraction theorem for discrete stochastic processes~\citep{pham2008analysis}. However, unlike the original setup considered in~\citet{pham2008analysis}, our focused trajectories start from distinct initial distributions and terminal timesteps: $\bx_N \sim {\cal N}({\bs 0}, {\bs I})$ and $\hat{\bx}_{\Ns} \sim {\cal N}({\bs 0}, \gamma^2 {\bs I})$. To address this, we leave out $\{ \bx_{i} \}_{i=\Ns + 1}^N$ and focus on errors between $\{\bx_i\}_{i=0}^\Ns$ and $\{ \hat{\bx}_i \}_{i=0}^\Ns$.

Employing $\Ns$ as the terminal timestep, the discrete contraction theorem (\ie, Theorem 1 in~\citet{pham2008analysis}) gives the following error bound between $\bx_i$ and $\hat{\bx}_i$ at time $i \in \{0,\dots, \Ns - 1\}$:
\begin{align}
\label{eq:proof_CT_phamthm1}
    \mathbb{E}[ \| \bx_i - \hat{\bx}_i \|_2^2] \le \frac{2C}{1 - \lambda^2} + \lambda^{2(\Ns - i)}\mathbb{E}[ \| \bx_\Ns - \hat{\bx}_\Ns \|_2^2].
\end{align}
where $C \coloneqq d(1 - \bar{\alpha}_{\Ns})$. Here $\lambda$ indicates the contraction rate that determines the speed of error-decaying:
\begin{align}
    \lambda \coloneqq \max_{j \in \{ i+1, \dots, \Ns\}} \sqrt{\alpha_j}\left( \frac{1 - \bar{\alpha}_{j-1}}{1 - \bar{\alpha}_{j}} \right),
\end{align}
Next, we upper-bound the terminal error $\mathbb{E}[ \| \bx_\Ns - \hat{\bx}_\Ns \|_2^2]$ as:
\begin{align}
    \mathbb{E}[ \| \bx_\Ns - \hat{\bx}_\Ns \|_2^2] & = \mathbb{E}[ \| \bx_\Ns \|_2^2 ] + \mathbb{E}[ \| \hat{\bx}_\Ns \|_2^2] + 2 \mathbb{E}[ \bx_\Ns^{\textsf{T}}\hat{\bx}_\Ns ] \\
    & = \mathbb{E}[ \| \bx_\Ns \|_2^2 ] + \mathbb{E}[ \| \hat{\bx}_\Ns \|_2^2  ] \\ &\le B^2 + \gamma^2 d, \label{eq:proof_CT_initerr}
\end{align}
where the second equality is due to the independent initialization $\hat{\bx}_{\Ns} \sim {\cal N}({\bs 0}, \gamma^2 {\bs I})$. The inequality comes from the bounded assumption $\| \bx_i \|_2 < B$. Now plugging the inequality in~\eqref{eq:proof_CT_initerr} into~\eqref{eq:proof_CT_phamthm1} yields the desired result:
\begin{align}
    \mathbb{E}[ \| \bx_i - \hat{\bx}_i \|_2^2] & \le \frac{2C}{1 - \lambda^2} + \lambda^{2(\Ns - i)}\mathbb{E}[ \| \bx_\Ns - \hat{\bx}_\Ns \|_2^2] \\
    & \le \frac{2C}{1 - \lambda^2} + \lambda^{2(\Ns - i)}(B^2 + \gamma^2 d). \\
\end{align}
This completes the proof.
\end{proof}


\subsection{Proof of~\cref{prop:gaussian_ODE}}
\label{subsec:proof_gaussian_ODE}

\begin{manualproposition}{\ref{prop:gaussian_ODE}}
    Consider the same data distribution $p_0$ and the optimal score function ${\bs s}_{\bth} (\bx, t)$ as~\cref{prop:gaussian_SDE}.  Suppose the PF-ODE in~\eqref{eq:rPFODE_emp} is initialized with $\hat{\bx}(T_{\text{skip}}) \sim {\cal N}(\hat{{\bs \mu}}_{T_{\text{skip}}}, \hat{{\bs \Sigma}}_{T_{\text{skip}}} ) $. Then, the resulting distribution at $t = 0$ is given by $\hat{\bx}_{\text{ODE}}(0) \sim {\cal N} (\hat{{\bs \mu}}_{0, \text{ODE}}, \hat{{\bs \Sigma}}_{0, \text{ODE}})$, where
    \begin{align}
        \hat{{\bs \mu}}_{0, \text{ODE}} &\coloneqq {\bs \mu}_0 + {\bs \Sigma}_0^{1/2} {\bs \Sigma}_{T_{\text{skip}}}^{-1/2} (\hat{{\bs \mu}}_{T_{\text{skip}}} - {\bs \mu}_{T_{\text{skip}}}), \\
        \hat{{\bs \Sigma}}_{0, \text{ODE}} &\coloneqq {\bs \Sigma}_0 {\bs \Sigma}_{T_{\text{skip}}}^{-1}\hat{{\bs \Sigma}}_{T_{\text{skip}}}. 
    \end{align}
    Here, ${\bs \mu}_{T_{\text{skip}}}$ and ${\bs \Sigma}_{T_{\text{skip}}}$ are defined as in Eqs. (\ref{eq:mu_Ts}) and (\ref{eq:Sigma_Ts}), respectively.
\end{manualproposition}

\begin{proof}
By Lemma \ref{lem:vp_sde_solution}, the score function is given as
\begin{align}
    \nabla_{\bx} \log p_t(\bx) = \nabla_{\bx} \log \mathcal{N}(\bx | \bs{\mu}_t, \bs{\Sigma}_t ) = - \bs{\Sigma}_t^{-1}(\bx - \bs{\mu}_t)
\end{align}
such that the PF-ODE is
\begin{align}
    d\hat{\bx}_t &= \textstyle [-\frac{1}{2} \beta(t) \hat{\bx}_t - \frac{1}{2} \beta(t) \nabla \log p_t(\hat{\bx}_t)] \, dt \\
    &= \textstyle [-\frac{1}{2}\beta(t)\hat{\bx}_t + \frac{1}{2} \beta(t) \bs{\Sigma}_t^{-1}(\hat{\bx}_t - \alpha(t) \bm_0)] \, dt \\
    &= \textstyle \frac{1}{2}\beta(t)(\bs{\Sigma}_t^{-1} - \bs{I}) \hat{\bx}_t \, dt - \frac{1}{2} \beta(t) \bs{\Sigma}_t^{-1} \bm_t \, dt.
\end{align}
or with change of variables $(\hat{\by}_s,s) \leftarrow (\hat{\bx}_{T-t},T-t)$,
\begin{align}
    d\hat{\by}_s &= \textstyle -\frac{1}{2}\beta(T-s)(\bs{\Sigma}_{T-s}^{-1} - \bs{I}) \hat{\by}_s \, ds + \frac{1}{2} \beta(T-s) \bs{\Sigma}_{T-s}^{-1} \bm_{T-s} \, ds \\
    &= \textstyle \frac{1}{2} \alpha(T-s) \beta(T-s) \bs{Q} \diag(\frac{\alpha(T-s) (\lambda_n - 1)}{1 + \alpha(T-s)^2 (\lambda_n - 1)}) \bs{Q}^\top \hat{\by}_s \, ds \\
    &\quad \textstyle + \frac{1}{2} \alpha(T-s) \beta(T-s) \bs{Q} \diag(\frac{1}{1 + \alpha(T-s)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0 \, ds.
\end{align}
Define
\begin{align}
    \textstyle \phi(x;a) = \frac{1}{2} \log(ax^2 + 1)
\end{align}
such that
\begin{align}
    \textstyle \frac{d\phi(\alpha(T-s);\lambda_n-1)}{ds} &= \textstyle \frac{d\phi(\alpha(T-s);\lambda_n-1)}{d\alpha(T-s)} \cdot \frac{d\alpha(T-s)}{d(T-s)} \cdot \frac{d(T-s)}{ds} \\
    &= \textstyle \frac{\alpha(T-s)(\lambda_n-1)}{1+\alpha(T-s)^2(\lambda_n-1)} \cdot \left( -\frac{1}{2} \beta(T-s) \alpha(T-s) \right) \cdot (-1) \\
    &= \textstyle \frac{1}{2} \alpha(T-s) \beta(T-s) \frac{\alpha(T-s)(\lambda_n-1)}{1+\alpha(T-s)^2(\lambda_n-1)}.
\end{align}
This motivates the transition matrix
\begin{align}
    \bs{\Psi}(s,s_0) &\coloneqq \bs{Q} \diag(e^{\phi(\alpha(T-s);\lambda_n-1) - \phi(\alpha(T-s_0);\lambda_n-1)}) \bs{Q}^\top \\
    &= \textstyle \bs{Q} \diag(\frac{\sqrt{(\lambda_n-1)\alpha(T-s)^2 + 1}}{\sqrt{(\lambda_n-1)\alpha(T-s_0)^2 + 1}}) \bs{Q}^\top \\
    &= \textstyle \bs{\Sigma}_{T-s}^{1/2} \bs{\Sigma}_{T-s_0}^{-1/2}
\end{align}
with which we can calculate the solution as
\begin{align}
    \hat{\by}_s &= \textstyle \bs{\Psi}(s,0) \hat{\by}_0 + \int_0^s \frac{1}{2} \bs{\Psi}(s,\sigma) \alpha(T-\sigma) \beta(T-\sigma) \bs{Q} \diag(\frac{1}{1 + \alpha(T-\sigma)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0 \, d\sigma \\
    &= \textstyle \bs{\Sigma}_{T-s}^{1/2} \bs{\Sigma}_{T}^{-1/2} \hat{\by}_0 + \bm_{T-s} - \bs{\Sigma}_{T-s}^{1/2} \bs{\Sigma}_T^{-1/2} \bm_T \\
    &= \textstyle \bm_{T-s} + \bs{\Sigma}_{T-s}^{1/2} \bs{\Sigma}_T^{-1/2}(\hat{\by}_0 - \bm_T) \label{eq:append_pf_ode_sol}
\end{align}
where the integral in the first line is calculated as
\begin{align}
    &\textstyle \int_0^s \frac{1}{2} \bs{\Psi}(s,\sigma) \alpha(T-\sigma) \beta(T-\sigma) \bs{Q} \diag(\frac{1}{1 + \alpha(T-\sigma)^2 (\lambda_n - 1)}) \bs{Q}^\top \bm_0 \, d\sigma \\
    &= \textstyle \bs{Q} \diag(\sqrt{(\lambda_n-1)\alpha(T-s)^2 + 1}) \diag(\int_0^s \frac{1}{2} \alpha(T-\sigma) \beta(T-\sigma) \frac{1}{(1 + \alpha(T-\sigma)^2(\lambda_n-1))^{3/2}} \, d\sigma) \bs{Q}^\top \bm_0 \\
    &= \textstyle \bs{Q} \diag(\sqrt{(\lambda_n-1)\alpha(T-s)^2 + 1}) \diag( \left[ \frac{\alpha(T-\sigma)}{\sqrt{(\lambda_n-1)\alpha(T-\sigma)^2 + 1}} \right]_0^s ) \bs{Q}^\top \bm_0 \\
    &= \textstyle \bs{Q} \diag(\sqrt{(\lambda_n-1)\alpha(T-s)^2 + 1}) \diag( \left[ \frac{\alpha(T-s)}{\sqrt{(\lambda_n-1)\alpha(T-s)^2 + 1}} - \frac{\alpha(T)}{\sqrt{(\lambda_n-1)\alpha(T)^2 + 1}} \right] ) \bs{Q}^\top \bm_0 \\
    &= \textstyle \alpha(T-s) \bm_0 - \alpha(T) \bs{\Sigma}_{T-s}^{1/2}\bs{\Sigma}_T^{-1/2} \bm_0 \\
    &= \textstyle \bm_{T-s} - \bs{\Sigma}_{T-s}^{1/2} \bs{\Sigma}_T^{-1/2} \bm_T.
\end{align}
Setting $s = T = T_\sk$ in \eqref{eq:append_pf_ode_sol},
\begin{align}
    \hat{\bx}_0 = \hat{\by}_{T_\sk} &= \bm_{T_\sk-T_\sk} + \bs{\Sigma}_{T_\sk-T_\sk}^{1/2} \bs{\Sigma}_{T_\sk}^{-1/2}(\hat{\by}_0 - \bm_{T_\sk}) \\
    &= \bm_0 + \bs{\Sigma}_0^{1/2} \bs{\Sigma}_{T_\sk}^{-1/2}(\hat{\bx}_{T_\sk} - \bm_{T_\sk}) \\
    &= \bm_0 + \bs{\Sigma}_0^{1/2} \bs{\Sigma}_{T_\sk}^{-1/2}(\bE[\hat{\bx}_{T_\sk}] - \bm_{T_\sk}) + \bs{\Sigma}_0^{1/2} \bs{\Sigma}_{T_\sk}^{-1/2}(\hat{\bx}_{T_\sk} - \bE[\hat{\bx}_{T_\sk}]).
\end{align}
It follows that if $\hat{\bx}_{T_\sk}$ is Gaussian, $\hat{\bx}_0$ is also Gaussian with
\begin{align}
    \bE[\hat{\bx}_0] &= \bm_0 + \bs{\Sigma}_0^{1/2} \bs{\Sigma}_{T_\sk}^{-1/2}(\bE[\hat{\bx}_{T_\sk}] - \bm_{T_\sk}), \\
    \cov[\hat{\bx}_0] &= \bs{\Sigma}_0 \bs{\Sigma}_{T_\sk}^{-1} \cov[\hat{\bx}_{T_\sk}].
\end{align}
This concludes the proof.
\end{proof}

\subsection{Proof of Proposition \ref{prop:contin_contraction}}
\label{subsec:contin_contraction}

\begin{manualproposition}{\ref{prop:contin_contraction}}
    Assume the same setup as Theorem 3 in \citet{xu2023restart}, \ie, $\| t \nabla \log p_t(\bx) \| \leq L_1$ and $\|\bx_t\| < B/2$ for any $\bx_t$ in the support of $p_t$ and reverse-SDE trajectories. Let
    \begin{gather}
        \bx_{t_{\min}}^{\sde} = \sde(\bx_{t_{\max}}, t_{\max} \rightarrow t_{\min}), \ \by_{t_{\min}}^{\sde} = \sde(\by_{t_{\max}}, t_{\max} \rightarrow t_{\min}) \\
        \bx_{t_{\min}}^{\ode} = \ode(\bx_{t_{\max}}, t_{\max} \rightarrow t_{\min}), \ \by_{t_{\min}}^{\ode} = \ode(\by_{t_{\max}}, t_{\max} \rightarrow t_{\min}) 
    \end{gather}
    where $\sde$ and $\ode$ denote solutions to the reverse-SDE and PF-ODE, respectively. Then
    \begin{gather}
    \tv(\bx_{t_{\min}}^{\ode},\by_{t_{\min}}^{\ode}) = \tv(\bx_{t_{\max}},\by_{t_{\max}}) \\
    \textstyle \tv(\bx_{t_{\min}}^{\sde},\by_{t_{\min}}^{\sde}) \leq \left( 1 - 2Q\left( \frac{B}{2\sqrt{\alpha(t_{\max})^{-2} - \alpha(t_{\min})^{-2}}} \right) \cdot e^{-B L_1 / t_{\min} - L_1^2 \alpha(t_{\max})^{-2} / t_{\min}^2} \right) \tv(\bx_{t_{\max}},\by_{t_{\max}})
    \end{gather}
    where $Q(r) \coloneqq \mathbb{P}(\varepsilon \geq r)$ for $\varepsilon \sim \mathcal{N}(0,1)$.
\end{manualproposition}

\begin{proof}
    The first equality follows by noting that the total variation distance is invariant under bijective maps due to the data processing inequality. We now prove the second inequality. Suppose $\bx_t$ follows the reverse SDE, and consider the change of variables $\bar{\bx}_t = \bx_t / \alpha(t)$. Then, by the It\^{o} formula,
    \begin{align}
        d\bar{\bx}_t &= \textstyle (-\frac{1}{2} \beta(t) \alpha(t)) (-\alpha(t)^{-2})\bx_t \, dt + \alpha(t)^{-1} d\bx_t \\
        &= \textstyle (-\frac{1}{2} \beta(t) \alpha(t)) (-\alpha(t)^{-2})\bx_t \, dt + \alpha(t)^{-1} ([-\frac{1}{2}\beta(t)\bx_t - \beta(t) \nabla \log p_t(\bx_t)] \, dt + \sqrt{\beta(t)} \, d\bar{\bw}_t) \\
        &= \textstyle -\alpha(t)^{-1} \beta(t) \nabla \log p_t(\bx_t) \, dt + \alpha(t)^{-1} \sqrt{\beta(t)} \, d\bar{\bw}_t \\
        &= \textstyle -\alpha(t)^{-2} \beta(t) \nabla \log \bar{p}_t(\bar{\bx}_t) \, dt + \alpha(t)^{-1} \sqrt{\beta(t)} \, d\bar{\bw}_t \\
        &= - (\alpha(t)^{-2})' \nabla \log \bar{p}_t(\bar{\bx}_t) \, dt + \sqrt{(\alpha(t)^{-2})'} \, d\bar{\bw}_t
    \end{align}
    where $\bar{p}_t$ is the density of $\bx_t / \alpha(t)$ for $\bx_t \sim p_t$, and we have used the change-of-variables formula of probability density functions at the fourth line. Next, using the change of time variable $\bar{t} = \alpha(t)^{-2}$, we get
    \begin{align}
        d\bar{\bx}_{\bar{t}} = - \nabla \log \bar{p}_{g^{-1}(\bar{t})}(\bar{\bx}_{\bar{t}}) \, d\bar{t} + d\bar{\bw}_{\bar{t}}.
    \end{align}
    Then following an analogous process as the proof of Lemma 5 in \citet{xu2023restart}, we obtain the inequality
    \begin{align}
    \textstyle \tv(\bar{\bx}_{\bar{t}_{\min}},\bar{\by}_{\bar{t}_{\min}}) \leq \left( 1 - 2Q\left( \frac{B}{2\sqrt{\bar{t}_{\max} - \bar{t}_{\min}}} \right) \cdot e^{-B L_1 / t_{\min} - L_1^2 \bar{t}_{\max} / t_{\min}^2} \right) \tv(\bar{\bx}_{\bar{t}_{\max}},\bar{\by}_{\bar{t}_{\max}}) \label{eq:append_contract_ineq}
    \end{align}
    where $\bar{t}_{\min} = \alpha(t_{\min})^{-2}$, $\bar{t}_{\max} = \alpha(t_{\max})^{-2}$. Since the map $\by_{s} \mapsto \by_{\alpha(s)^{2}} / \alpha(s)$ is bijective, we again have
    \begin{align}
    \tv(\bar{\bx}_{\bar{t}_{\min}},\bar{\by}_{\bar{t}_{\min}}) &= \tv(\bx_{t_{\min}},\by_{t_{\min}}), \\ \tv(\bar{\bx}_{\bar{t}_{\max}},\bar{\by}_{\bar{t}_{\max}}) &= \tv(\bx_{t_{\max}},\by_{t_{\max}}),
    \end{align}
    by the data processing inequality. Plugging this relation into \eqref{eq:append_contract_ineq} yields the contraction result for the reverse SDE.
\end{proof}





\section{Implementation Details}


\noindent \textbf{Pretrained models.} The pretrained model for CelebA was constructed by ourselves by following the settings in~\citet{um2023don}. The models for LSUN-Bedrooms and ImageNet were taken from the checkpoints given by~\citet{dhariwal2021diffusion}. For the results on ImageNet $256 \times 256$. we respect the approaches in~\citet{sehwag2022generating, um2023don, um2024self} and leverage the upscaling model developed in~\citet{dhariwal2021diffusion}.


\noindent \textbf{Baselines.} The BigGAN model is based upon the same architecture used in~\citet{choi2020fair}\footnote{\url{https://github.com/ermongroup/fairgen}}, and we follow the training setup outlined in the official project page of BigGAN\footnote{\url{https://github.com/ajbrock/BigGAN-PyTorch}}. The StyleGAN results were obtained via the pretrained model offered by~\citet{karras2019style}\footnote{\url{https://github.com/NVlabs/stylegan}}. The ADM~\citep{dhariwal2021diffusion} baselines used in our experiments employed the same pretrained models as those used in our framework, which is provided in the authors' codebase\footnote{\url{https://github.com/openai/guided-diffusion}}. The results of LDM~\citep{rombach2022high} were obtained from the checkpoint offered by~\citet{rombach2022high}\footnote{\url{https://github.com/CompVis/latent-diffusion}}. For the EDM~\citep{karras2022elucidating} baseline, we used the checkpoint given in the official project page of~\cite{karras2022elucidating}\footnote{\url{https://github.com/NVlabs/edm}}. The DiT~\cite{Peebles2022DiT} baseline employed the pretrained model provided in the official code repository\footnote{\url{https://github.com/facebookresearch/DiT}}. The authors of CADS~\citep{sadat2023cads} do not provide the official codebase, so we implemented by ourselves based upon the pseudocode provided in the manuscript~\cite{sadat2023cads}. 

The implementation of~\citet{sehwag2022generating} is based upon the descriptions provided in their original paper, employing the same pretrained models as ours (\ie, the ADM checkpoints~\citep{dhariwal2021diffusion}). Specifically on CelebA, we created an out-of-distribution (OOD) classifier to distinguish whether an input belongs to CelebA or other datasets (\eg, ImageNet). We then incorporated the negative log-likelihood gradient of the classifier (targeting the in-distribution class) into ancestral sampling to yield low-density guidance. For implementing~\citet{um2023don}, we followed the settings described in their paper for all datasets under consideration. Specifically, using the same ADM pretrained models as ours, we applied U-Net encoders for minority classifiers and utilized the entire training set to build the classifiers, except for the minority predictor on LSUN-Bedrooms, where only 10\% of the training set was used. For the ImageNet $256 \times 256$ results of~\citet{um2023don}, we used the upscaling model~\citep{dhariwal2021diffusion} as described in~\citet{um2023don}.

In the additional CelebA baseline with classifier guidance targeting minority annotations (\ie, ADM-ML in~\cref{tab:main_results}), we respected the same settings outlined in~\citet{um2023don, um2024self}. In particular, the classifier was trained to predict four minority attributes: (i) ``Bald''; (ii) ``Eyeglasses''; (iii) ``Mustache''; (iv) ``Wearing\_Hat''. During inference, we generated samples by combining random selections of these four attributes (\eg, bald but not wearing glasses) using the classifier guidance. The backbone model for ADM-ML was the same as ours. The results of~\citet{um2024self} is based upon their the official codebase\footnote{\url{https://github.com/soobin-um/sg-minority}}, respecting their recommended settings reported in the original paper~\citep{um2024self}. The temperature sampling results were obtained from the method proposed in~\citep{dhariwal2021diffusion} (in Section G), \ie, by using a $\tau$-scaled score model ${\bs s}_\bth (\bx, t) / \tau$ with $\tau = 1.01$.


\noindent \textbf{Evaluation metrics.} To compute the Local Outlier Factor (LOF)~\citep{breunig2000lof} for the generated samples, we used the PyOD implementation~\citep{zhao2019pyod}\footnote{\url{https://pyod.readthedocs.io/en/latest/}}. The number of nearest neighbors for calculating AvgkNN and LOF was set to 5 and 20, respectively, as these are commonly used values in practice. Following the approach in~\citet{sehwag2022generating, um2023don}, both AvgkNN and LOF were computed in the feature space of ResNet-50. The Rarity Score~\citep{han2022rarity} was computed with $k = 5$ using the implementation available on the official project page\footnote{\url{https://github.com/hichoe95/Rarity-Score}}.

Clean Fréchet Inception Distance (cFID)~\citep{parmar2022aliased} was evaluated using the official implementation\footnote{\url{https://github.com/GaParmar/clean-fid}}. Spatial FID (sFID)~\cite{nash2021generating} was computed based on the official PyTorch FID code~\citep{heusel2017gans}\footnote{\url{https://github.com/mseitzer/pytorch-fid}} with modifications to utilize spatial features (\ie, the first 7 channels from the intermediate \texttt{mixed\_6/conv} feature maps), rather than the standard \texttt{pool\_3} inception features. The results for Improved Precision \& Recall~\citep{kynkaanniemi2019improved} were obtained with $k = 5$ using the official codebase from~\citet{han2022rarity}. To evaluate the proximity to low-likelihood instances at the data tail, we used the least probable instances as baseline real data for computing the quality metrics. Specifically for CelebA, we selected the 10,000 real samples with the highest AvgkNN values. For LSUN-Bedrooms and ImageNet, we used the most unique 50,000 samples, which had the highest AvgkNN values, as baseline real data. All quality and diversity metrics were computed using 30,000 generated samples.



\noindent \textbf{Hyperparameters.} Our hyperparameter selection $(\gamma, \Delta_t)$ followed a two-step approach: first, we determined an appropriate $\Delta_t$ that ensures a non-negligible $\alpha(\Ts)$ (where $\Ts \coloneqq T - \Delta_t$), and then we performed a grid search to select $\gamma$. We empirically found that our framework is not that sensitive to the choice of $\Delta_t$, and in practice, setting $\Delta_t$ such that $\alpha(\Ts) > 0.01$ generally yields strong performance on low-resolution datasets (\eg, CelebA and ImageNet $64 \times 64$). For high-resolution benchmarks (\eg, LSUN-Bedrooms), a lower threshold of $\alpha(\Ts) > 0.005$ was sufficient, as these datasets are more sensitive to noise intensity~\citep{nichol2021improved}.  

For CelebA, we conducted a grid search over $\gamma^2 = \{ 4.0, 8.0, 12.0, 16.0, 18.0, 20.0 \}$, while for LSUN-Bedrooms, we searched over $\gamma^2 = \{ 2.0, 4.0, 6.0, 7.0, 7.5, 8.0 \}$. For the ImageNet results, the search was performed over $\gamma^2 = \{ 2.0, 4.0, 6.0, 6.5, 7.0, 8.0 \}$. Based on this, we selected the following final values: (i) $(\gamma^2, \Delta_t) = (18.0, 3)$ for CelebA; (ii) $(\gamma^2, \Delta_t) = (7.5, 0)$ for LSUN-Bedrooms; and (iii) $(\gamma^2, \Delta_t) = (6.5, 3)$ for the ImageNet cases.




We employed a global setting of 250 timesteps for sampling across all diffusion-based samplers, including both the baseline methods and our approach. For empirical analyses, such as the ablation studies, we reduced the number of timesteps to 100 for efficiency.


\noindent \textbf{Other details.} Our implementation is based on PyTorch~\cite{paszke2019pytorch}, and experiments were performed on twin NVIDIA A100 GPUs. Code is available at \url{https://github.com/anonymous7172/BnS}.


\section{Additional Experimental Results}


\begin{figure}[t!]
    \centering
    \begin{subfigure}[h]{0.95\linewidth}
    \includegraphics[width=\linewidth]{./figs/apdx/nn_celeba.png}
    \end{subfigure}
    \begin{subfigure}[h]{0.95\linewidth}
    \includegraphics[width=\linewidth]{./figs/apdx/nn_lsun.png}
    \end{subfigure}
    \begin{subfigure}[h]{0.95\linewidth}
    \includegraphics[width=\linewidth]{./figs/apdx/nn_in64.png}
    \end{subfigure}
    \begin{subfigure}[h]{0.95\linewidth}
    \includegraphics[width=\linewidth]{./figs/apdx/nn_in256.png}
    \end{subfigure}
    \caption{
    \textbf{Comparison of neighborhood density distributions across four benchmarks.}
    \textbf{(Top row)} CelebA $64 \times 64$.
    \textbf{(Second row)} LSUN-Bedrooms $256 \times 256$.
    \textbf{(Third row)} ImageNet $64 \times 64$.
    \textbf{(Fourth row)} ImageNet $256 \times 256$.
    ``AvgkNN'' refers to Average k-Nearest Neighbor, and ``LOF'' is Local Outlier Factor~\citep{breunig2000lof}.
    ``Rarity Score'' indicates a low-density metric proposed by~\citet{han2022rarity}.
    The higher values, the less likely samples for all three measures.
    }
\label{fig:nn_dist}
\vspace{-5mm}
\end{figure}



\subsection{Neighborhood distance distributions}
\label{subsec:nnd_dist}

\cref{fig:nn_dist} illustrates neighborhood metric results compared on our interested four benchmarks. Observe that for all three metrics, our approach performs consistently well, rivaling to computationally intensive guided minority samplers like~\citet{um2024self}. This highlight the practical significance of Boost-and-Skip, which can achieve great performance improvements in promoting minority features with significantly less computations.



\subsection{Additional generated samples}
\label{subsec:additional_samples}


To facilitate a more comprehensive qualitative comparison among the samplers, we present an extensive set of generated samples across all considered datasets. See Figures~\ref{fig:samples_many_celeba}-\ref{fig:samples_many_in256} for details. Observe that samples generated by our approach tend to capture unique dataset features, comparable to those produced by guided sampling techniques. This further highlights the key advantage of our method -- achieving strong minority generation performance with significantly lower computational costs than existing guidance-based approaches~\citep{sehwag2022generating, um2023don, um2024self}.


\begin{figure}[h]
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/apdx/samples_many_celeba_adm.jpg}
    \caption{ADM~\citep{dhariwal2021diffusion}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/apdx/samples_many_celeba_sgms.jpg}
    \caption{\citet{um2024self}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/apdx/samples_many_celeba_bns.jpg}
    \caption{Boost-and-Skip (ours)}
    \end{subfigure}
\caption{Samples comparison on CelebA $64 \times 64$.}
\label{fig:samples_many_celeba}
\end{figure}



\begin{figure}[h]
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/apdx/samples_many_lsun_adm.jpg}
    \caption{ADM~\citep{dhariwal2021diffusion}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/apdx/samples_many_lsun_mg.jpg}
    \caption{\citet{um2023don}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=1.0\columnwidth]{./figs/apdx/samples_many_lsun_bns.jpg}
    \caption{Boost-and-Skip (ours)}
    \end{subfigure}
\caption{Samples comparison on LSUN-Bedrooms $256 \times 256$.}
\label{fig:samples_many_lsun}
\end{figure}



\begin{figure}[h]
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=0.955\columnwidth]{./figs/apdx/samples_many_in64_adm.jpg}
    \caption{ADM~\citep{dhariwal2021diffusion}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=0.955\columnwidth]{./figs/apdx/samples_many_in64_sehwag.jpg}
    \caption{\citet{sehwag2022generating}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=0.955\columnwidth]{./figs/apdx/samples_many_in64_bns.jpg}
    \caption{Boost-and-Skip (ours)}
    \end{subfigure}
\caption{Samples comparison on ImageNet $64 \times 64$. Generated samples from five classes are exhibited: (i) ``Siberian husky'' (top row); (ii) ``water tower'' (second row); (iii) ``street sign'' (third row); (iv) ``ice cream'' (fourth row); (v) ``cheeseburger'' (bottom row).}
\label{fig:samples_many_in64}
\end{figure}


\begin{figure}[h]
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=0.955\columnwidth]{./figs/apdx/samples_many_in256_adm.jpg}
    \caption{ADM~\citep{dhariwal2021diffusion}}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=0.955\columnwidth]{./figs/apdx/samples_many_in256_ts.jpg}
    \caption{Temperature sampling}
    \end{subfigure}
    \hfill
    \begin{subfigure}[h]{0.321\textwidth}
    \centering
    \includegraphics[width=0.955\columnwidth]{./figs/apdx/samples_many_in256_bns.jpg}
    \caption{Boost-and-Skip (ours)}
    \end{subfigure}
\caption{Samples comparison on ImageNet $256 \times 256$. Generated samples from five classes are exhibited: (i) ``hamster'' (top row); (ii) ``zebra'' (second row); (iii) ``space shuttle'' (third row); (iv) ``valley'' (fourth row); (v) ``volcano'' (bottom row).}
\label{fig:samples_many_in256}
\end{figure}




\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
