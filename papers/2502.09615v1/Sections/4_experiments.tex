\section{Experiments}

\begin{figure}[t]
\includegraphics[width=\linewidth]{Figures/riganything-ablation.pdf}
\caption{(Left) Joint diffusion modeling prevents joint collapse to mean positions, capturing diverse modalities. (Right) Pose augmentation improves generalization to unseen poses, ensuring well-aligned skeletons and avoiding excessive joints.}
% \Description{Skeleton prediction with joint L2 loss}
\label{fig:ablation_diffusion}
\vspace{-5mm}
\end{figure}

\subsection{Implementation Details}

Our input point cloud consists of 1024 points, with the maximum number of joints per sample set to 64. The point cloud and joint tokenizers are implemented as two-layer MLPs with hidden dimensions of 512 and 1024. For both parent and skinning prediction modules, we employ two-layer MLPs with hidden dimensions of 1024.

The implemented transformer consists of 12 layers with a hidden dimension of 1024. Following the implementation in \cite{zhang2025gs}, each transformer block incorporates a multi-head self-attention layer with 16 heads and a two-layered MLP with a hidden dimension of 4096 and a GeLU activation. We employ Pre-Layer Normalization, Layer Normalization (LN), and residual connections consistent with the reference implementation. During training, we employ a hybrid attention masking strategy: shape tokens perform self-attention to effectively capture geometric information, while skeleton tokens use causal attention, attending only to their ancestor skeleton tokens within the sequence to facilitate auto-regressive generation. Additionally, skeleton tokens attend to all shape tokens. During inference, the network processes shape tokens as input and generates skeleton tokens in an auto-regressive manner.

The joint diffusion process follows \cite{nichol2021improved, li2024autoregressive}, which has a cosine noise scheduler with 1000 training steps and 300 resampling steps during inference. The denoising MLP is conditioned on the transformer-outputted joint tokens, where these tokens are incorporated into the noise scheduler's time embedding through AdaLN \cite{peebles2023scalable} within the Layer Normalization layers.

The fusing module is a two-layer MLP with an input size of 3072 and hidden dimensions of 2048 and 1024. During inference, after obtaining the next joint position via diffusion sampling, a shape tokenizer generates a latent shape token (dimension 1024), which is concatenated with previous context tokens (dimension 1024) and positional embeddings. The fusing module's output serves as the updated context for connectivity and skinning prediction. Both the connectivity and skinning modules share a similar architecture with the fusing module, except their input size is 2048.

\subsection{Dataset}

We utilize both the RigNet dataset \cite{rignet} and the Objaverse dataset \cite{deitke2023objaverse}. The RigNet dataset contains 2,354 high-quality 3D models with ground-truth skeleton and skinning annotations. The Objaverse dataset offers a large collection of 3D models with varying rigging quality. To ensure data reliability, we filtered out 9,686 models with consistent and accurate skeleton and skinning information. Our dataset spans a diverse range of categories, including bipedal, quadrupedal, avian, marine, insectoid, and manipulable rigid objects. For each model, we sample point clouds and face normals from the mesh surface.

During training, we perform online augmentation to the input data by randomly deforming the input point clouds using the ground-truth skeleton and skinning. As shown in our ablation study in \cref{sec:ablation} and \cref{fig:ablation_diffusion}, this augmentation strengthens our method's ability to generalize to objects in different poses.

\subsection{Evaluation and Baseline Comparisons}

\begin{figure*}[t]
% \includegraphics[width=0.8\linewidth]{Figures/riganything-realcapture-compressed.pdf}
\includegraphics[width=0.8\linewidth]{Figures/riganything-realcapture.png}
\caption{Skeleton results on shapes from real casual images. We use off-the-shelf image-to-3D model pipeline \cite{liu2024meshformer} to generate the shapes from real images and apply RigAnything to predict their skeletons.}
\label{fig:real_capture}
\end{figure*}

\subsubsection{Skeleton Prediction} 

We provide qualitative visualizations of the reconstructed skeletons in comparison with the ground truth and RigNet in \cref{fig: skeleton_comp}. Our method demonstrates superior performance, producing more accurate and satisfying skeletons across various shape categories. In contrast, RigNet~\cite{rignet} struggles to recover reasonable skeletons for less common shapes, such as characters with tails or wings, and frequently generates an excessive number of joints. In comparison, our method generates a reasonable number of joints, with the reconstructed skeletons well-aligned to the underlying shape, ensuring better structural consistency and fidelity.

To quantitatively evaluate the performance of skeleton prediction, we measure the similarity between the predicted skeletons and the ground truth on the RigNet dataset using multiple metrics: Intersection over Union (IoU), Precision, and Recall for bone matching, as well as Chamfer distances for joints (CD-J2J), bone line segments (CD-B2B), and joint-to-bone line segments (CD-J2B). Table \ref{tab:skel_pred} presents a comparison with Pinocchio~\cite{pinocchio} and RigNet~\cite{rignet} across these metrics. The results show that our method significantly outperforms the baselines, producing skeletons that align more closely with the ground truth.

\input{Tabels/skel_prediction}

\begin{figure*}[t]
\includegraphics[width=\linewidth]{Figures/riganything-moreresults.pdf}
\caption{More results on the RigNet dataset. Please refer to the supplementary video for more 360-degree video results on both the RigNet and Objaverse dataset.}
\label{fig: moreresults}
\end{figure*}


\subsubsection{Connectivity Prediction}

We evaluate the connectivity prediction performance when the given joints are from ground truth instead of prediction. We measure the binary classification accuracy (Class. Acc.) for assessing joint pair connections, as well as the CD-B2B and edit distance (ED), which measure the geometric and topological difference between the predicted and reference skeletons. As shown in Table \ref{tab:connect_pred}, our method significantly outperforms RigNet across all metrics.

\input{Tabels/connect_pred}

\subsubsection{Skinning Prediction}

For skinning prediction performance, we provide a qualitative comparison of our method with RigNet and Blender's built-in automatic skinning weight calculation, which assigns weights based on the shortest Euclidean distance between mesh vertices and bones in the armature. For a fair comparison, the ground truth skeleton is provided during skinning weight inference. As shown in \cref{fig:skin_pred}, our method produces more accurate and consistent skinning weights. In challenging cases where two areas are close in Euclidean space but have a large geodesic distance, our method successfully differentiates these parts and generates consistent skinning weights, whereas the baselines fail.

\subsection{Ablation Study} \label{sec:ablation}

We analyze various components of our method and compare their performance with the final model.

\subsubsection{Joint Diffusion} In our full model, the joint diffusion module predicts the probability of the next joint position based on preceding joints in a skeleton sequence. This probabilistic approach effectively resolves structural ambiguities in skeleton tree representations, such as equivalent sibling node orderings, by accounting for their equivalence. In an ablation study, we replaced the joint diffusion loss with a deterministic L2 joint position loss. As shown in \cref{fig:ablation_diffusion}, using L2 loss leads to joints collapsing toward the middle axis, representing the mean position across samples due to sibling ambiguities within the skeleton sequence. In contrast, our method captures diverse joint position modalities, producing reasonable and accurate joint placements instead of averaged positions. Quantitative results in \cref{tab:ablation_skel_pred} further confirm that joint diffusion modeling significantly improves our methodâ€™s performance, boosting the skeleton IoU by almost two times.

\input{Tabels/ablation_skeleton_pred}

\subsubsection{Normal Injecting} To evaluate the impact of incorporating point normals into the shape tokens, we conducted a comparison experiment without point normals as input. The numerical results in \cref{tab:ablation_skel_pred} show a significant decline in skeleton performance when normal information is excluded, highlighting the importance of point normals as geometric information for improving performance.


\subsubsection{Online Pose Augmentation} We analyze the effect of online data augmentation by randomly deforming input point clouds using the ground-truth skeleton and skinning. As shown in the numerical results in \cref{sec:ablation}, pose augmentation improves skeleton prediction performance. Additionally, \cref{fig:ablation_diffusion} compares results with and without pose augmentation on a character with a random skeleton pose not present in the dataset. Our full model generates a significantly better-aligned skeleton structure, whereas the model trained without pose augmentation fails to produce skeletons aligned with the shape and generates excessive joints. This augmentation enhances our method's ability to generalize to objects in diverse poses. Furthermore, as demonstrated in \cref{fig:real_capture}, our method achieves high-quality skeletons even when the input shapes are obtained from real-world data and the targets are in arbitrary poses.
\begin{figure}[t]
\includegraphics[width=\linewidth]{Figures/riganything-skinning.pdf}
% \vspace{-5mm}
\caption{Comparison of skinning weight predictions. Our method produces more accurate and consistent weights, especially in challenging cases with large geodesic distances.}
% \vspace{-3mm}
\label{fig:skin_pred}
\end{figure}
