\section{Introduction}

\begin{figure*}[t]
% \includegraphics[width=\linewidth]{Figures/riganything-teaser2.png}
\includegraphics[width=\linewidth]{Figures/fffAsset_4.png}
\caption{Skeleton generation on shapes obtained from real images, demonstrating that our method generalizes well to real data. \cref{fig:real_capture} presents additional real results.}
\vspace{-5mm}
\label{fig:real_teaser}
\end{figure*}

Recent advancements in large-scale 3D asset generation~\cite{hong2023lrm, li2023instant3d,xu2023dmv3d,jun2023shap, shi2023mvdream,nichol2022point,liu2023zero,liu2024one} have enabled the creation of highly detailed static shapes. 
% 
However, since motion is an essential aspect of how humans perceive and interact with the world, there is a growing demand for modeling dynamics to create lifelike and interactive assets \cite{liu2024dynamic}.
% 
While some approaches leverage text-based~\cite{bahmani20244d, zhao2023animate124,singer2023text} or video-guided~\cite{yin20234dgen,ren2023dreamgaussian4d} control to animate objects, these methods often fall short in providing the precision and flexibility required by artists to fully realize their creative visions.
Rigging, in contrast, offer a robust and artist-friendly framework for animation, enabling fine-grained control over degree of freedom and range of motion. Our work addresses this need by presenting a systematic approach to automating rigging, advancing the state of the art in articulable asset generation. 

Auto-rigging has long been a challenging research problem in computer graphics~\cite{pinocchio, rignet, guo2024makeitani, chu2024humanrig, li2021learning}. \Cref{tab:teaser_comp} provides a concise summary of state-of-the-art methods in this domain.
Most existing approaches depend on predefined skeleton templates~\cite{pinocchio, guo2024makeitani, chu2024humanrig, li2021learning}, which limit their applicability to specific categories, such as humanoid characters (\cref{tab:teaser_comp}).
To overcome template reliance, RigNet~\cite{rignet} employs non-differentiable operators, including clustering for joint position acquisition and a minimum spanning tree for topology construction. However, this approach requires approximately two minutes to rig a single object and is further constrained to operate only on objects in rest poses.


In this work, we propose a transformer-based autoregressive model, termed \emph{\textbf{RigAnything}}, to make any 3D asset "rig-ready". The autoregressive model probabilistically "grows" the skeleton from the root joint in a sequential manner; Skinning weights for any surface sample are then inferred by holistically considering all the joints.

Specifically, we represent the tree-structured skeleton as a sequence by ordering the joints in a breadth-first search (BFS) order, where each joint is defined by a 3D position and a parent index. 
This autoregressive formulation is particularly suited for skeleton prediction, as it addresses the inherent ambiguity in joint configurations by representing them as a probabilistic distribution. Additionally, by sequentially generating joints and connections without relying on a predefined template, the model supports arbitrary skeleton structures and varying numbers of joints, enabling broad generalization across diverse object categories.
Furthermore, while autoregressive models are traditionally designed to handle discrete values~\cite{waswani2017attention,brown2020language,radford2019language}, inspired by recent work utilizing autoregressive models for image generation~\cite{li2024autoregressive}, we adopt a diffusion sampling process to predict the continuously valued joint positions, resulting in superior accuracy.
Given the predicted skeleton, we infer the skinning weights by a pair-wise computation.
We employ transformer blocks throughout the model to comprehensively capture the global shape structure, as well as the interdependence among all joints and their associated surface points.

We train our model end-to-end on both the RigNet dataset~\cite{rignet} and a curated subset of high-quality animatable assets from the Objaverse dataset~\cite{deitke2023objaverse}. We rigorously filter the Objaverse dataset and select 9686 high-quality rigged shapes, which enrich the dataset for research in this direction. The input shapes are further augmented with random pose variations to enhance robustness.
Our training data encompasses a wide range of object types, including bipedal, quadrupedal, avian, marine, insectoid, and manipulable rigid objects, as well as a diverse set of initial poses. This extensive scale and diversity of training data surpasses all prior work, playing a critical role in achieving broad generalizability across shape categories and configurations.

\input{Tabels/teaser_tab}

Extensive experiments demonstrate that RigAnything achieves state-of-the-art performance in the auto-rigging task, as demonstrated in \cref{fig:teaser}, \cref{fig:real_teaser} and \cref{fig: skeleton_comp}, surpassing prior methods in quality, robustness, generalizability, and efficiency. By automating rigging for diverse 3D assets, our method advances the vision of fully interactive 3D environments and scalable 3D content creation, empowering artists and developers with a powerful, efficient tool.
