\section{Method}

Our goal is to transform a given 3D shape into an animatable asset by generating a plausible skeleton and predicting the corresponding skinning weights. These enable the 3D asset to be articulated under joint transformations using Linear Blend Skinning (LBS).
In this section, we first introduce our novel autoregressive model for skeleton generation in \cref{sec:skeleton}, followed by our approach to skinning weight prediction in \cref{sec:skinning}, and conclude with a detailed description of the model architecture in \cref{sec:transformer}.
\subsection{Autoregressive Skeleton Prediction}\label{sec:skeleton}
%
\boldstartspace{Autoregressive Modeling.} The key component of our method is an autoregressive model for the skeleton prediction to address the ambiguity of skeleton structures and eliminates the need for predefined templates.
%
To convert the tree-structured skeleton to a sequence that can be effectively processed by the autoregressive model, we adopt the breadth-first search order (BFS) to serialize the skeleton to a list: 

\begin{equation}
    \mathcal{J} = \left[ (j_1, p_1), (j_2, p_2), ..., (j_K, p_K) \right],
\end{equation}

where $j_k \in \mathbb{R}^3$ and $p_k \in \{ 1, ..., K \}$ denote the 3D position and the parent index of the $k$-th joint respectively. As we adopt the BFS order, $p_k<k$ and the first element $(j_1,p_1)$ always represents the root joint. The order of joints at the same BFS depth level is non-deterministic. To resolve this ambiguity, we randomly sample the order during training and uses generative modeling to cover the uncertainty.

Given an input shape $\Shape$ represented by $L$ sampled points, we factorize the joint probability of skeleton by the chain rule:

\begin{align*}
    P(\mathcal{J} \mid \Shape) &= \prod_{k=1}^{K} P\left(j_k, p_k \mid \mathcal{J}_{1:k-1}, \Shape\right).
\end{align*}
where $\mathcal{J}_{1:k}$ is the shorthand for the sublist of $\mathcal{J}$ up to the $k$-th element. 

The autoregressive model is tasked to iteratively predict the conditional distribution of each joint position $j_k$ and parent index $p_k$, formulated as
\begin{align*}
P\left(j_k, p_k \mid \mathcal{J}_{1:k-1}, \Shape\right)
& = P\left(j_k\mid \mathcal{J}_{1:k-1}, \Shape\right) P\left(p_k\mid j_k, \mathcal{J}_{1:k-1}, \Shape\right).
\end{align*}
 
Instead of directly modeling in the original joint space, we map all previously predicted joints and their corresponding parents into a higher-dimensional token space to enhance the model's expressive capacity. This token space effectively represents the evolving state of the skeleton, capturing its structural and hierarchical information as new joints and connections are incrementally added. Similarly, a sequence of shape tokens is extracted to encapsulate the global structure of the input shape, providing consistent contextual information throughout the modeling process. Denoting the skeleton tokens as $T_{1:k-1}\in\mathbb{R}^{(k-1)\times d}$ and the shape token as $H\in\mathbb{R}^{L\times d}$, the prediction targets are reformulated as:  
\begin{equation}
P(j_k \mid T_{1:k-1}, H) \quad \text{and} \quad P(p_i \mid j_i, T_{1:k-1}, H).
\end{equation}
The extraction of the skeleton token $T$ and shape token $H$ are detailed in \cref{sec:transformer}.

\boldstartspace{Joint Prediction with Diffusion Model.} 
To predict the next joint position, which is continuously valued, we address the limitation that most autoregressive models are traditionally designed for discrete outputs, making them less effective for continuous-valued tasks. Inspired by recent autoregressive image generation models~\cite{li2024autoregressive}, we adopt a diffusion sampling process~\cite{ho2020denoising, nichol2021improved, dhariwal2021diffusion} to handle the continuous nature of joint positions. Diffusion models are particularly suited for this task because they iteratively refine samples, effectively resolving the structural ambiguities inherent in skeleton tree representations.
For readability, we drop the current joint index $k$ in the following part.

\paragraph{Forward Diffusion Process:}
The forward process gradually adds Gaussian noise to the ground-truth joint \(j^0\) over \(M\) time steps, producing increasingly noisy versions \(j^m\). This is formulated as:
\[
j^m = \sqrt{\bar{\alpha}_m} j^0 + \sqrt{1 - \bar{\alpha}_m} \epsilon,
\]
where \(\epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) is Gaussian noise, and \(\bar{\alpha}_m = \prod_{s=1}^m \alpha_s\) defines a noise schedule.

\paragraph{Training Objective:}
We train a noise estimator \(\epsilon_\theta\), conditioned on the diffusion time step \(m\) and the context \(Z\)$\in\mathbb{R}^{\left(L+k-1\right)\times d}$, where 
\begin{equation}
Z = \text{TransformerBlocks}(T_{1:k-1}, H),
\end{equation}
capturing both the evolving skeleton state and the input shape. Here, Tr refers to the transformer blocks that are detailed in \cref{sec:transformer}. The model takes the noisy joint \(j^m\) as input and predicts the added noise \(\epsilon\). The training objective is defined as:
\begin{equation}
    \mathcal{L}_{\text{joint}}(Z, j^0) = \mathbb{E}_{\epsilon, m} \big[ \| \epsilon - \epsilon_\theta(j^m \mid m, Z) \|^2 \big].
\end{equation}

\paragraph{Reverse Diffusion Process:}
At inference time, the reverse process iteratively removes noise, sampling the next joint position \(j^0 \sim p_\theta(j^0 \mid Z)\). Starting from a Gaussian sample \(j^M \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\), the reverse process is defined as:
\begin{equation}
j^{m-1} = \frac{1}{\sqrt{\alpha_m}} \big(j^m - \frac{1 - \alpha_m}{\sqrt{1 - \bar{\alpha}_m}} \epsilon_\theta(j^m \mid m, Z)\big) + \sigma_m \delta,
\end{equation}
where \(\delta \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\) is Gaussian noise, and \(\sigma_m\) denotes the noise level at step \(m\). The final output \(j^0\) represents the predicted joint position.

\boldstartspace{Connectivity Prediction.} After we sample the next joint position $j_{k} \in \mathbb{R}^3$ from the diffusion module described earlier, we aim to predict how this newly sampled joint $j_{k}$ connects to its ancestor joints. We first update the context $Z_k$ with the sampled joint $j_k$ through a fusing module F:
\begin{equation}
    Z'_{k} = \text{F} \Bigl ( Z_k, j_{k}, \gamma(k) \Bigr ),
\end{equation}
where $\gamma (k)\in \mathbb{R}^d$ is a positional embedding signaling the current joint index.

Next, a connectivity module C takes $Z'_{k}$ and each individual predicted skeleton token $T_i (i < k)$ (detailed in \cref{sec:transformer})  produce the parent joint probability,   
\begin{equation}
    \mathbf{q}_{k} = \text{Softmax}\Bigl ([ \text{C} (Z'_{k}, T_i) ]_{i=1}^{k-1} \Bigr ).
\end{equation}

The connectivity is supervised with the binary cross-entropy loss,
\begin{equation}
    \mathcal{L}_{\text{connect}} 
    = - \sum_{i=1}^{k-1}
    \bigl[
        \hat{y}_{k,i} \log\bigl(q_{k,i}\bigr)
        \;+\;
        \bigl(1 - \hat{y}_{k,i}\bigr) \log\bigl(1 - q_{k,i}\bigr)
    \bigr],
\end{equation}
where $q_{k,i}$ is the $i$-th element in $\mathbf{q}_k$ and $\hat{y}_{k,i} \in \{0, 1 \}$ is the ground-truth label indicating whether joint $j_{k}$ is connected to joint $j_i$.

During training, the ground-truth next joint position $j_{k}$ is fed into the network for connectivity prediction, while during the inference time, $j_{k}$ is sampled from the joint diffusion module and subsequently passed to the connectivity network.

\begin{figure}[t]
\includegraphics[width=\linewidth]{Figures/riganything-autoregressive.pdf}
\caption{(Left) Hybrid attention mask: Shape tokens use full self-attention, while skeleton tokens attend to shape tokens and apply causal masking among themselves. (Right) The skeleton sequence is autoregressively generated during inference.}\vspace{-3mm}
\label{fig:autoregressive}
\end{figure}

\begin{figure*}[t]
\includegraphics[width=\linewidth]{Figures/riganything-skeleton-comp.png}
\caption{Comparison of reconstructed skeletons between our method, RigNet, and ground truth. Our method generates more accurate and satisfying skeletons across diverse shape categories. While RigNet tends to produce excessive joints and struggles with uncommon shapes like characters with tails or wings. Our approach generates a reasonable number of joints and aligns the skeletons closely with the underlying shapes. Note that RigNet supports only rest poses, so all evaluations are conducted on rest-posed objects for fairness.}
\vspace{-3mm}
\label{fig: skeleton_comp}
\end{figure*}

\subsection{Skinning Prediction} \label{sec:skinning}
Skinning weights are described by a matrix \(W \in \mathbb{R}^{L \times K}\), where each element \(w_{lk}\) indicates the influence of the \(k\)-th joint on the \(l\)-th surface point in \(\mathcal{S}\). The weight vector \(\mathbf{w}_l \in \mathbb{R}^{K}\) for each surface point must satisfy the following constraints:
$\sum_{k=1}^{K} w_{lk} = 1 \quad \text{and} \quad w_{lk} \geq 0 \quad \text{for all } k.$

To compute the skinning weight \(\mathbf{w}_l\) for each surface point \(s_l \in \mathcal{S}\), a skinning prediction module G takes as input the shape token $H_{s_l} \in \mathbb{R}^d$ for point $s_l$, along with 
the skeleton token \(T_k\) for each joint \(j_k \, (k \leq K)\). The module outputs a predicted influence score for each joint \(j_k\) on \(s_l\). The final skinning weight \(\mathbf{w}_l\) is computed using the softmax function:
\begin{equation}
\mathbf{w}_l = \text{Softmax}\Bigl([\text{G}(H_{s_l}, T_k)]_{k=1}^{K}\Bigr),    
\end{equation}

We train this module by minimizing a weighted cross-entropy loss, where the ground-truth skinning weight $\hat{\mathbf{w}}_l$ serves as the weighting factor, which can be written as. %Let $\mathbf{r}_l \in \mathbb{R}
\begin{equation}
    \mathcal{L}_{\text{skinning}}
    = \frac{1}{L} \sum_{l=1}^{L} 
    \Bigl(- \sum_{k=1}^{K} \hat{w}_{l,k} \,\log\bigl(w_{l,k}\bigr)\Bigr).
\end{equation}
This formulation encourages the model to produce higher probabilities for joints with larger ground-truth skinning weights, thereby aligning the learned distribution with the correct influences for each point.

\subsection{Autoregressive Transformer Architecture}\label{sec:transformer}
Our autoregressive modeling is anchored on a transformer-based architecture, which outputs the shape tokens $H\in \mathbb{R}^{L \times d}$ and skeleton tokens $T_{1:k}\in\mathbb{R}^{k\times d}$ ($0< k\leq K$) that serve as conditional inputs for the autoregressive modeling for skeleton prediction (\cref{sec:skeleton}) and skinning prediction (\cref{sec:skinning}).
The extraction of these tokens involve two steps: first, referred as the ``tokenization'' step, an initial shape token and skeleton tokens are lifted from the raw input, this step produces a higher dimensional vector that has sufficient capacity in preparation to capture richer information in the further processing steps in the transformer; subsequently, the transformer process the these tokens through a series of attention blocks with carefully crafted attention masking to obtain the final shape and skeleton tokens, which are finally used as inputs to the skeleton and skinning prediction modules in \cref{sec:skeleton} and \cref{sec:skinning}.

\boldstartspace{Tokenization.} 
For the shape data, we sample a set of $L$ surface points $S \in \mathbb{R}^{L \times 3}$ and concatenate them with their corresponding normals $N \in \mathbb{R}^{L \times 3}$, forming a sequence of $L$ tokens each with $6$ dimensions. 
These tokens are then passed through MLP layers to a $d$-dimensional space. 
Formally, the shape tokens $H \in \mathbb{R}^{L \times d}$ can be written as
\begin{equation}
    H = \text{MLP} \bigl ( \text{Concat} (S, N) \bigr ).
\end{equation}
% 

For the skeleton data, we first apply MLPs to project each joint position $j_k$ and its corresponding parent joint position $j_{p_k}$ into a $d$-dimensional space. These features are then concatenated with positional embeddings, which encode the index of each joint within the sequence. 
Finally, the concatenated features are processed through MLP layers to obtain the per-joint skeleton tokens. These steps can be expressed formally as
\begin{equation}
    T_{k} = \text{MLP}\Bigl( \text{Concat}\bigl( \text{MLP}(j_k),\; \gamma(k),\; \text{MLP}(j_{p_k}),\; \gamma(p_k) \bigr) \Bigr).
\end{equation}
The skeleton token $T_{1:K}\in \mathbb{R}^{K\times d}$ is a sequence of individual per-joint tokens in BFS-order.

\boldstartspace{Processing Tokens with Transformer.} The extracted shape tokens $H$ and predicted skeleton tokens $T_{1:k-1}$ are concatenated and then treated as $L + \left(k-1\right)$ individual tokens. These are then passed through a chain of transformer blocks, in which multi-head self-attention mechanisms ensure that the skeleton tokens and the shape tokens are aware of each other's features, enabling the model to capture rich global information and interdependencies between the shape context and the evolving skeleton structure.
% 
We propose a hybrid attention mechanism that applies different attention patterns to shape and skeleton tokens. As shown in the left part of \cref{fig:autoregressive}, shape tokens attend to each other via full self-attention to capture global geometric context. 
For skeleton tokens, we first allow them to attend to all shape tokens to incorporate shape information, and then apply causal attention~\cite{waswani2017attention,radford2019language} among the skeleton tokens so that each token only attends to its preceding tokens in the sequence. This ensures the autoregressive property required for skeleton sequential generation. 

The output of the last transformer block $Z_k$ will be served as the condition in the diffusion model for joint $j_k$ sampling as introduced in \cref{sec:skeleton}.
