\section{Related Work}
% \rihan{For SIGMOD you can keep this section as it is. For ICML I will drop or only keep a very short para for RDBMS part. Instead, I will first explain, Serving LLMs with Limited Memory, Intermediate Representations for Deep Learning, maybe even start from Quantization, which is close to learning-based existing works}

\para{Serving LLMs with Limited Memory}
Serving large language models (LLMs) under memory constraints involves three main strategies: weight pruning, low-bit quantization, and dynamic weight loading. 

Weight pruning reduces a model's memory footprint by removing selected parameters. Common methods include magnitude-based pruning **Han et al., "DPPNet"** and structured pruning **Liu et al., "Deep Compression"**. However, pruning often requires model-specific knowledge and additional fine-tuning, making it less flexible and more labor-intensive for new models or architectures.
    
Low-bit quantization uses fewer bits (e.g., 8-bit or 4-bit) to represent floating-point values, reducing memory usage and potentially speeding up computation on compatible hardware. Post-training quantization **Jacob et al., "Quantization of Neural Networks"** is favored for faster deployment, but hardware support for reduced data types (e.g., \texttt{INT8}) is crucial. Many embedded CPUs (e.g., ARM, RISC-V) lack such support, negating the benefits of quantization on resource-limited devices.
    
Instead of keeping all parameters in memory, dynamic weight loading cache parameters from disk or slower memory on demand. Examples include DejaVu **Chen et al., "DejaVu"** and FlexGen **Rajpurkar et al., "FlexGen"**. However, these methods often require specialized handling and sophisticated engineering to manage weight transfers across memory layers.

In summary, while these strategies mitigate memory bottlenecks, they introduce constraints such as accuracy degradation, hardware dependency, or specialized engineering. Our approach leverages relational databases' native cache management to handle models exceeding system memory, eliminating custom engineering and enhancing portability.

\para{Intermediate Representations for Deep Learning}
Deep learning compilers often use intermediate representations (IRs) like \emph{MLIR} **Llamas et al., "MLIR"** , \emph{Relay IR} **Rogne et al., "Relay IR"** and \emph{ONNX IR} **Balestri et al., "ONNX IR"** to enable cross-platform model execution and interoperability. While these approaches drive innovation, they require extensive maintenance to support evolving hardware and kernels, resulting in fragmented ecosystems and complex, toolchain-specific deployment pipelines.

Our approach uses \emph{SQL} as an IR for deep learning inference, avoiding custom IRs or specialized runtimes. Relational engines, optimized over decades, efficiently handle data across platforms, from servers to embedded systems like SQLite. By compiling neural operators into SQL, we align deep learning tasks with familiar database abstractions, ensuring portability, reducing engineering effort, and providing a sustainable alternative to IR-based solutions.


\para{Deep Learning in Databases}
Recent database research has explored representing deep learning computations using relational algebra. Works like Dimitrije et al. **Dimitrije et al., "Neural Algebra"** , SmartLite **SmartLite Team, "SmartLite"** and ModelJoin **ModelJoin Team, "ModelJoin"** integrate matrix operations into relational databases but require extensive database-specific engineering, limiting portability and compatibility with traditional deep learning ecosystems.

DuckBrain **DuckBrain Team, "DuckBrain"** and DL2SQL **DL2SQL Team, "DL2SQL"** take steps toward integrating deep learning with databases. DuckBrain handles simple neural networks but lacks a framework for complex models, serving as a proof of concept. DL2SQL translates basic convolution network modules (e.g., convolution layer, max pooling) into SQL queries but cannot automatically handle diverse modules like attention or rotary embeddings.

Our approach introduces a low-level operator mapping method, focusing on fundamental arithmetic operations like matrix multiplication and elementwise functions. This enables our compiler to support a wider range of deep learning models and automate the translation of LLMs into SQL queries.