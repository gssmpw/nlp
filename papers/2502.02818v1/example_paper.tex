%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{caption}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}  
\usepackage{listings}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

%%% Rihan's commands and packages
\newcounter{RihanNOC}
\stepcounter{RihanNOC}
\newcommand{\rihan}[1]{\textcolor{orange}{\small \bf [Rihan\#\arabic{RihanNOC}\stepcounter{RihanNOC}: #1]}}
 

\newcommand{\para}[1]{\noindent\textbf{#1.}}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL}

\begin{document}
\setlength{\textfloatsep}{5pt}
% \setlength{\belowcaptionskip}{2pt}
% \setlength{\abovecaptionskip}{2pt}
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}


\twocolumn[
% \icmltitle{Database as Runtime: Compiling LLMs to SQL for In-database Model Serving}
\icmltitle{Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL}

% Efficient and accessible LLM Serving via SQL Compilation
% Serving Large Language Models with SQL: Enabling Accessible and Efficient Inference
% LLM Inference Without GPUs: SQL as a Runtime for Transformers

%FOR sigmod: SQL Meets Transformers: A Relational Approach to LLM Inference



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\begin{icmlauthorlist}
\icmlauthor{Wenbo Sun}{delft}
\icmlauthor{Qiming Guo}{tuam}
\icmlauthor{Wenlu Wang}{tuam}
\icmlauthor{Rihan Hai}{delft}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{delft}{Delft University of Technology}
\icmlaffiliation{tuam}{Texas A\&M University - Corpus Christi}

\icmlcorrespondingauthor{Wenbo Sun}{w.sun-2@tudelft.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}



\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Serving large language models (LLMs) often demands specialized hardware, dedicated frameworks, and substantial development efforts, which restrict their accessibility, especially for edge devices and organizations with limited technical resources. We propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases—one of the most widely used and mature software systems globally—to serve as the runtime. By mapping neural operators such as matrix multiplication and attention into relational primitives like joins and aggregations, our approach leverages database capabilities, including disk-based data management and native caching. Supporting key transformer components, such as attention mechanisms and key-value caching, our system generates SQL pipelines for end-to-end LLM inference. Using the Llama3 family as a case study, we demonstrate up to 30x speedup in token generation for memory-constrained scenarios comparable to competitive CPU-based frameworks. Our work offers an  accessible, portable, and efficient solution, facilitating the serving of LLMs across diverse deployment environments.
\end{abstract}
% we achieve inference latencies comparable to CPU-only frameworks and up to 30x faster token generation in memory-constrained scenarios compared to competitive CPU-based frameworks, offering an efficient, accessible, and portable solution for serving LLMs across diverse environments.



% **Abstract**  
% Large language models (LLMs) have achieved impressive results in text generation, summarization, and other NLP tasks, yet serving them typically demands specialized hardware and intricate deep learning frameworks. We present a compiler that transforms ONNX-based LLM inference graphs into SQL queries, enabling any standard relational database to act as the runtime environment. Rather than introducing another custom IR, our approach directly leverages ONNX to minimize integration overhead for existing model pipelines. By mapping core neural operators (e.g., matrix multiplications, softmax) to relational primitives such as joins, projections, and group-by aggregations, the compiler exploits a database’s intrinsic disk–memory management and indexing capabilities to handle large models—even on memory-constrained systems. We demonstrate this method on real-world LLMs (e.g., elementwise) and observe competitive inference latencies compared to CPU-only baselines. In doing so, we eliminate the need for specialized accelerators or deep learning libraries, lowering both hardware and engineering barriers. This work highlights the potential of relational databases—a mature and ubiquitous software stack—as a cost-effective, portable, and scalable platform for deploying advanced language models.

\section{Introduction}
\label{sec:intro}
% \rihan{the first paragraph is high-level, not in detail, which needs refinement. The whole introduction is a mix of ideas that are not converged. Below are a list of questions to help you have a consistent story
% \\1 The paper targets a CPU-only setting, but the practical scenarios are not elaborated or consistent in the introduction. Is the scenario ``Traditional deployment strategies rely heavily on specialized hardware, such as GPUs and TPUs, which are not only expensive but also less accessible for organizations constrained by resource limitations.''? But it seems you only mention IoT? IoT devices are borad and please state exactly which type you mean. An IoT device could be a Android and iOS, Raspberry Pi (e.g., ailia LLM), or sensors, which is not what you are aiming.
% \\2 I appreciate the honesty of the last paragraph. But I would prefer a paper that clearly states its applicable scenario: if you handle a CPU-only scenario, this should be clearly motivated, and then GPU comparison is irrelevant (or you can put in the limitation, in the end of the paper).
% \\3 IR is briefly mentioned without giving enough explanation and works related to learning community
% \\4 What do you mean by accessible and portable solutions? Less extensive engineering or hardware requirements? Can you be more specific? Limited memory or CPU? Here is what I mean by the texts are high-level, and need readers to guess what you mean.
% \\5 If one just want to serve LLM on resource constraint,most common learning approach is to design a light-weight LLM (or called small LM (SLM)), or quantization. Since they are not discussed here, I'm not sure if you don't know related works or if these are irrelevant, then exactly what is your research scope, and what are your related works?}

% Recent research has shown promising results when partial or approximate deep-learning computations run inside relational databases~\cite{modeljoin,dl2sql,duckbrain}, demonstrating flexible scaling and reasonable performance. However, existing prototypes generally rely on manually translating only a small subset of neural operators into SQL, and they do not fully support key transformer modules such as multi-head attention or dynamic key-value caches~\cite{kvcache}. Moreover, no \emph{systematic compiler} has been proposed to convert an entire LLM inference graph, including complex layer interactions like grouped attention heads, directly into SQL. These limitations preserve the fragmentation problem: model authors or systems developers must still engineer ad-hoc operator rewrites to adapt their networks for database execution.

Large language models (LLMs) have proven highly effective across various applications, including enterprise analytics~\cite{enterprise}, personalized edge computing~\cite{edge1,edge2}, and offline automation~\cite{offline1}. However, many critical operational environments are limited by CPU-based infrastructure, which typically has less memory, computing power, and support for mainstream deep learning frameworks due to security, budget, or hardware constraints. Examples include air-gapped systems (e.g., healthcare and defense)~\cite{airgap1}, legacy enterprise servers, and cost-sensitive edge devices such as Raspberry Pi clusters. This diverse range of underpowered, CPU-only environments poses a significant challenge to deploying large-scale LLMs.

Existing optimization techniques, such as weight pruning \cite{pruning}, low-bit quantization \cite{quantize1}, and dynamic weight loading \cite{LLMFlash,FlexGen}, offer partial solutions to reduce memory consumption. However, their implementations often depend on model architectures and require continuous engineering efforts to accommodate varying CPU architectures (e.g., x86, ARM, RISC-V) and vendor-specific optimizations.

Intermediate representation (IR)-based solutions \cite{mlir,Relay,triton} compile LLMs into portable IRs. While effective in stable hardware environments, these methods need specialized backends and ongoing maintenance to handle evolving hardware/OS ecosystems. As hardware architectures become more fragmented, the repeated and continuous engineering overheads of IR-based solutions become an unsustainable burden for heterogeneous deployments.


Common model serving frameworks (e.g., TorchServe\footnote{\url{https://pytorch.org/serve/}}, Tensorflow Serving\footnote{\url{https://github.com/tensorflow/serving}}) are widely used in GPU-rich environments but offer limited support for disk-backed execution, making them unsuitable for CPU-only environments where models exceed system memory. This raises the question: \textit{How can we enable LLM serving on underpowered, CPU-only infrastructure while ensuring accessibility and portability?}


Relational databases offer a promising solution. 
They are widely deployed across computing environments—from SQLite\footnote{\url{https://www.sqlite.org/}} instances in billions of edge devices to enterprise systems. With built-in features like cache management and query optimization, they can enable LLM inference without  adaptation for diverse software/hardware stacks. However, prior work \cite{declaritive,dl2sql} focuses on coarse-grained neural architectures  (e.g., convolution layers, pooling) rather than low-level operations fundamental to these components. This limits flexibility in supporting diverse architectures like transformer-based LLMs. Furthermore, existing work lacks a systematic compilation framework to automate SQL query translation for model serving.


% They are widely deployed across computing environments—from SQLite\footnote{\url{https://www.sqlite.org/}} instances in edge devices to enterprise systems. With built-in features like buffer management, indexing, and parallel execution, they can enable LLM inference without requiring model adaptation for diverse software/hardware stacks.



In this paper, we propose a novel compiler that translates LLM inference graphs into optimized SQL, enabling relational databases to serve as accessible, hardware-agnostic runtimes.
Our approach uniquely evaluate low-level tensor operations using relational primitives. Leveraging databases' native memory management and ANSI-standard SQL syntax, our compiler automatically generates queries that exploit database engines for efficient execution under strict memory constraints


% we propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases to serve as scalable, hardware-agnostic runtimes.
% By leveraging SQL—a standardized language supported by almost all modern relational databases—our approach eliminates the need for custom intermediate
% representations or backend-specific code. 
% Moreover, our proposed abstraction leverages the built-in disk-paging and cache management mechanisms of databases to accommodate large models efficiently, even under stringent memory constraints.

\textbf{Key contributions include:}
\vspace{-2mm}
\begin{itemize}
\setlength\itemsep{0pt}
    \item \textbf{Operator Mapping.} We systematically translate core LLM operations (e.g., matrix multiplication, softmax) into standard relational primitives (\texttt{JOIN}, \texttt{GROUP BY}, \texttt{PROJECTION})%, forming the basis of an automated SQL-based compilation strategy.
    This provides the foundation for a fully automated, SQL-based compilation pipeline.
    \item \textbf{Two-stage Compiler.} We develop a compiler that parses ONNX inference graphs and emits fully executable SQL queries, enabling any relational database to function as a general-purpose LLM serving engine.
    \item \textbf{Real-World LLM Serving.} We demonstrate how this compiler can serve models such as Llama in a disk-memory hybrid configuration, facilitating efficient inference on CPU-only, resource-constrained systems. Both interactive and large-sequence generation workloads become feasible without specialized hardware or abundant memory.
\end{itemize}
\vspace{-2mm}
Our experiments show that the proposed approach achieves up to a \textbf{$30\times$} speedup in token generation on constrained CPU-based systems compared to CPU-only baselines. By leveraging standard SQL interfaces that span ARM-based edge devices and x86 enterprise servers, our solution is both \emph{accessible} (i.e., requiring minimal specialized expertise and no proprietary hardware) and \emph{portable} (i.e., usable across diverse hardware/software environments). This allows resource-limited organizations to harness large language models with minimal ongoing engineering costs.


% \rihan{We still don't have a small crown figure in introduction, e.g., a figure of showing performance boost, a figure showing what we are doing (problemmsolution etc)}

% \section{Preliminaries}
% \label{sec:prelim}

% In this section, we outline the theoretical underpinnings pertinent to our method. We begin by describing the essential components of transformer-based LLMs, paying particular attention to the self-attention mechanism. We then introduce the formal idea of matrix multiplication, followed by an explanation of how matrix multiplications can be executed via relational queries using a chunk-based matrix representation.


% \subsection{Self-Attention}
% \label{subsec:self-attn}
% Transformer-based large language models \cite{DBLP:conf/nips/VaswaniSPUJGKP17} consist of repeated layers, each containing two major sub-modules: a \emph{multi-head self-attention} block and a \emph{feed-forward network} (FFN). Formally, let $\mathbf{X} \in \mathbb{R}^{n \times d}$ be the sequence of $n$ embeddings (one per token) of dimension $d$. A single Transformer layer maps $\mathbf{X}$ to $\mathbf{X}'$ via:
% \begin{align*}
%     \mathbf{X}' = \mathrm{FFN}\bigl(\mathrm{MultiHeadAttn}(\mathbf{X})\bigr).
% \end{align*}
% Here, $\mathrm{MultiHeadAttn}$ computes attention across several heads in parallel, and $\mathrm{FFN}$ typically consists of two fully connected layers with an activation function in between.

% The self-attention mechanism is the core computational block of the Transformer. Given a sequence of hidden states $\mathbf{X}$, self-attention projects $\mathbf{X}$ into three matrices:
% \[
%     \mathbf{Q} = \mathbf{X} \mathbf{W}_Q, \quad
%     \mathbf{K} = \mathbf{X} \mathbf{W}_K, \quad
%     \mathbf{V} = \mathbf{X} \mathbf{W}_V,
% \]
% where $\mathbf{W}_Q$, $\mathbf{W}_K$, and $\mathbf{W}_V$ are learnable parameter matrices. The output of one attention head is given by
% \begin{align}
%     \mathrm{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V})
%     &= \mathrm{softmax}\!\Bigl(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\Bigr) \mathbf{V},
%     \label{eq:attention}
% \end{align}
% where $d_k$ is the dimension of each key vector and the softmax is applied row-wise. 

% In multi-head attention, $h$ such attention heads are computed in parallel and then concatenated:
% \[
% \bigl[\mathrm{Attn}(\mathbf{Q}_1,\mathbf{K}_1,\mathbf{V}_1) \;||\;\cdots\;||\;\mathrm{Attn}(\mathbf{Q}_h,\mathbf{K}_h,\mathbf{V}_h)\bigr]\mathbf{W}_O,
% \]
% where each head uses its own parameter matrices $(\mathbf{W}_Q^i, \mathbf{W}_K^i, \mathbf{W}_V^i)$, and $\mathbf{W}_O$ is an output projection.

% \subsection{Key-Value Cache}
% \label{subsec:kv-cache}
% In decoder-only LLMs, during inference for long sequences, repeated computation of keys and values can be expensive. To mitigate this cost, models often store previously computed keys and values in a \emph{KV-cache}. 

% Suppose at time step $t$ we have the hidden state $\mathbf{x}_t \in \mathbb{R}^{1 \times d}$. The model computes:
% \[
%     \mathbf{k}_t = \mathbf{x}_t \mathbf{W}_K, \quad
%     \mathbf{v}_t = \mathbf{x}_t \mathbf{W}_V.
% \]
% Instead of recomputing all $\{\mathbf{k}_1, \ldots, \mathbf{k}_{t}, \mathbf{v}_1, \ldots, \mathbf{v}_{t}\}$ at each step, we cache them from previous steps. The attention at time $t+1$ then only needs to compute:
% \[
%     \mathrm{softmax}\!\Bigl(\frac{\mathbf{q}_{t+1}\,[\mathbf{k}_1;\ldots;\mathbf{k}_t]^\top}{\sqrt{d_k}}\Bigr)\,[\mathbf{v}_1;\ldots;\mathbf{v}_t].
% \]
% Hence, the cache significantly reduces repetitive computations as the sequence grows.

% \subsection{Matrix Multiplication in Relational Queries}
% \label{subsec:mm-relational}
% Matrix multiplication (MM) is one of the most critical operations in LLM computations, especially for operations like $\mathbf{Q}\mathbf{K}^\top$, feed-forward layers, and the projection of embeddings. Let us denote the product of two matrices $\mathbf{A} \in \mathbb{R}^{m \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times n}$ by:
% \[
%     \mathbf{C} = \mathbf{A} \mathbf{B},
% \]
% where $\mathbf{C} \in \mathbb{R}^{m \times n}$. Each entry of $\mathbf{C}$ is given by
% % \[
% \begin{align}
%     c_{ij} = \sum_{k=1}^{r} a_{ik} \, b_{kj}.
% % \]
% \end{align}
% \para{Chunk-Based Representation}
% We represent matrices in a \emph{chunked} format, an approach explored in prior research to enable linear algebra computations within databases \cite{declaritive,chunk_matrix1,chunk_matrix2}. Matrices are divided into smaller block matrices, or tiles, indexed by tile row and tile column. However, since native support for matrix algebra is rare in databases, while many support vector operations \cite{duck,clickhouse} such as dot products and elementwise arithmetic, we choose to partition matrices into vectors for broader compatibility. Specifically, for a large matrix $\mathbf{W} \in \mathbb{R}^{D \times F}$, we split each row into $C$ chunks:
% \[
%     \mathbf{w}_i = 
%     \bigl[\mathbf{w}_i^{(1)},\,\mathbf{w}_i^{(2)},\,\ldots,\,\mathbf{w}_i^{(C)}\bigr],
% \]
% where each chunk $\mathbf{w}_i^{(c)}$ has a fixed dimension determined by a \emph{chunk size} hyperparameter. In the database table, each row thus corresponds to:
% \[
%     (i,\; c,\; \mathbf{w}_i^{(c)}),
% \]
% for $1 \leq i \leq D$ and $1 \leq c \leq C$. This chunking strategy (i) avoids overly wide tables, (ii) improves query performance by leveraging block-level optimizations, and (iii) allows the database to handle large matrices that exceed typical memory limitations of in-memory kernels.

% \para{SQL Implementation of Matrix Multiplication}
% Within this chunked table structure, we emulate the summation $\sum_{k} a_{ik} b_{kj}$ by joining the chunked rows of the two matrices on their common index $k$. The relational engine then multiplies corresponding entries and aggregates the products to form each entry of the resulting matrix $\mathbf{C}$. Formally, the SQL implementation follows a pattern like:
% \begin{verbatim}
% SELECT 
%     A.i AS i, 
%     B.j AS j, 
%     SUM(A.val * B.val) AS c_ij
% FROM A
% JOIN B ON A.k = B.k
% GROUP BY A.i, B.j;
% \end{verbatim}
% \rihan{also refer to equation 2, for i, j, k?}
% Here, \texttt{A.val} and \texttt{B.val} would each represent chunked slices of the matrices, typically stored as arrays or separate columns. The result is then reorganized (if needed) to match the output dimension $m \times n$. 

% This SQL-based matrix multiplication serves as a foundation for implementing other operations (e.g., elementwise activations, softmax) directly in the relational engine, enabling end-to-end inference within a database system.




\section{Compiling LLMs to SQL}
\label{sec:method}
We propose a systematic method to compile the computational graph of LLMs for model serving into SQL queries. The process involves two stages: (1)~\emph{Operator Mapping}, where each computational graph node is converted into a high-level relational function, and (2)~\emph{SQL Code Generation}, where these relational functions are translated into executable queries in the target database’s dialect.

\subsection{Matrix Multiplication in Relational Queries}
\label{subsec:mm-relational}
Before introducing the compiler design, we briefly introduce the foundation of performing linear algebra within a database. Matrix multiplication (MM) is one of the most critical operations in LLM computations, especially for operations like self-attention, feed-forward layers, and the projection of embeddings. Let us denote the product of two matrices $\mathbf{A} \in \mathbb{R}^{m \times r}$ and $\mathbf{B} \in \mathbb{R}^{r \times n}$ by:
\[
    \mathbf{W} = \mathbf{A} \mathbf{B},
\]
where $\mathbf{W} \in \mathbb{R}^{m \times n}$. Each entry of $\mathbf{W}$ is given by
\[
    w_{ij} = \sum_{k=1}^{r} a_{ik} \, b_{kj}.
\]
\para{Chunk-Based Representation}
We represent matrices in a \emph{chunked} format, an approach explored in prior research to enable linear algebra computations within databases \cite{declaritive,chunk_matrix1,chunk_matrix2}. Matrices are divided into smaller block matrices, or tiles, indexed by tile row and tile column. However, since native support for matrix algebra is rare in databases, while many support vector operations \cite{duck,clickhouse} such as dot products and elementwise arithmetic, we choose to partition matrices into vectors for broader compatibility. Fig. \ref{fig:chunk_based} illustrates an example of chunk-based representation for model weights.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/kv_weights.pdf}
\caption{Illustration of slicing K, V, Q weights into chunks.}    
\label{fig:chunk_based}
\end{figure}


Specifically, for a large matrix $\mathbf{W} \in \mathbb{R}^{m \times n}$, we split each row into $\lfloor\frac{n}{chunk\_size}\rfloor$ chunks:
\[
    \mathbf{w}_i = 
    \bigl[\mathbf{w}_i^{(0)},\,\mathbf{w}_i^{(1)},\,\ldots,\,\mathbf{w}_i^{(C)}\bigr],
\]
where each chunk $\mathbf{w}_i^{(c)}$ has a fixed dimension determined by a \emph{chunk size} hyperparameter. In the database table, each row thus corresponds to:
\[
    (i,\; c,\; \mathbf{w}_i^{(c)}),
\]
for $0 \leq i < m$ and $0 \leq c < \lfloor\frac{n}{chunk\_size}\rfloor$. This chunking strategy provides flexibility by allowing trade-offs between the number of rows and the workload per thread when performing vector operations.


% This chunking strategy (i) avoids overly wide tables, (ii) improves query performance by leveraging block-level optimizations, and (iii) allows the database to handle large matrices that exceed typical memory limitations of in-memory kernels.

\para{SQL Implementation of Matrix Multiplication}
Within this chunked table structure, we emulate the summation $\sum_{c} a_{ic} b_{cj}$ by joining the chunked rows of the two matrices on their common index $k$. The relational engine then multiplies corresponding entries and aggregates the products to form each entry of the resulting matrix $\mathbf{W}$. Formally, the SQL implementation follows a pattern like:

\begin{lstlisting}[basicstyle=\ttfamily,mathescape]
SELECT 
    A.$i$ AS $i$, 
    B.$j$ AS $j$, 
    SUM(A.$\mathbf{w}_i^{(c)}$ * B.$\mathbf{w}_j^{(c)}$) AS $w_{ij}$
FROM A
JOIN B ON A.$c$ = B.$c$
GROUP BY A.$i$, B.$j$;
\end{lstlisting}

Here, A.$\mathbf{w}_i^{(c)}$ and B.$\mathbf{w}_j^{(c)}$ would each represent chunked slices of the matrices, typically stored as arrays or separate columns. The result is then reorganized (if needed) to match the output dimension $m \times n$. 

This SQL-based matrix multiplication serves as a foundation for implementing other operations (e.g., elementwise activations, softmax) directly in the relational engine, enabling end-to-end inference within a database system.



\subsection{Stage 1: Operator Mapping}
\label{sec:MM_example}
In the first stage, operators are processed in topological order, extracting details such as operator type (e.g., \texttt{MatMul}, \texttt{Add}, \texttt{Reshape}), input/output shapes, and attributes like broadcast axes or chunk sizes. Each operator is mapped to a high-level relational function representing its equivalent relational primitives: \(\pi\) (projection), \(\bowtie\) (join), or \(\gamma\) (group-by with aggregation). These relational functions provide an intermediate abstraction, reinterpreting neural operators in LLMs for execution in a relational environment.


% \para{Matrix Multiplication.}
% When the compiler encounters a \texttt{MatMul} node in ONNX, it constructs a SQL node that performs a \(\bowtie\) on the shared dimension (e.g., \(k\) in an \(m\times k\) times \(k\times n\) multiplication), followed by a \(\gamma\) operator that aggregates the product of matching elements. In relational algebra notation, we translate:
% \[
% \mathbf{C} \;=\; \mathbf{A}\mathbf{B}
% \quad\longrightarrow\quad
% \gamma_{i,j,\;\text{SUM}(\mathbf{a}^{(c)} \otimes \mathbf{b}^{(c)})} 
% \Bigl( R_A \;\bowtie_{k} R_B \Bigr),
% \]
% where \(R_A\) and \(R_B\) store chunked rows from \(\mathbf{A}\) and \(\mathbf{B}\). The join aligns identical \(k\)-indices, and the group-by with aggregation \(\gamma\) sums over partial products.
% \para{General Mapping Definition.}
\begin{definition}
We define neural operators $\mathcal{F}$ over $p$ operands as:
\[
\mathcal{F}\Bigl(
  \{\mathcal{O}_1, \dots, \mathcal{O}_p\},\,
  \{fd_1, \dots, fd_p\},\,
  \mathcal{S}
\Bigr)
\]
\end{definition}
Each operand \(\mathcal{O}_i\) (for \(1 \le i \le p\)) is associated with some \emph{free dimensions} in a tuple
\[
  fd_i = (\,fd^i_1,\;fd^i_2,\;\dots)\!
\]
and \(\mathcal{S} = \{\,sd_1,\;sd_2,\;\dots,\,sd_n\}\) is the set of \emph{shared dimensions} among these operands. Intuitively, the free dimensions remain as independent axes in the output, whereas the shared dimensions indicate indices to be matched  across the different operands.
\begin{definition}
The corresponding relational function of $f$ is defined as:
\[
\mathcal{R} 
\Bigl(\{R_1,\dots,R_p\},\, \{\mathrm{keys}_1,\dots,\mathrm{keys}_p\},\, \mathrm{keys}_{\mathrm{join}}\Bigr)
\]
where \(R_i\) is the chunk-based table representing operand \(\mathcal{O}_i\), \(\mathrm{keys}_i\) is the relational counterpart of the free dimensions \(fd_i\) (i.e., the row/column identifiers that remain in the final output), and \(\mathrm{keys}_{\mathrm{join}}\) encodes the shared dimensions \(\mathcal{S}\) as equi-join attributes for \(\bowtie\).
\end{definition}
\begin{definition}
The operator mapping in our compiler is a function $op\_map$ that maps a neural operator to a relational function, denoted by:
\[
    \mathcal{R}=op\_map(\mathcal{F})
\]
\end{definition}
The resulting relational function is composed of standard relational operators (projections \(\pi\), joins \(\bowtie\), group-by/aggregation \(\gamma\), and arithmetic). Conceptually, this translation reinterprets the original linear-algebraic operation in the framework of relational execution.

\para{Matrix Multiplication Example}
When the compiler encounters the operation \(\mathbf{C} = \mathbf{A}\mathbf{B}\) in a computational graph, suppose
\[
\mathbf{A}\in\mathbb{R}^{m\times r}, 
\quad
\mathbf{B}\in\mathbb{R}^{r\times n},
\quad
\mathbf{C}\in\mathbb{R}^{m\times n}.
\]
We identify the free dimensions \(fd^A_1\) and \(fd^B_1\) and the shared dimension \(sd_1\) as follows:
\(
fd^A_1 
=\,
\bigl\{\,i \;\big|\; 0 \le i < m \bigr\}\),
\(fd^B_1
=\,
\bigl\{\,j \;\big|\; 0 \le j < n \bigr\}\), and \(
sd_1
=\,
\bigl\{\,k \;\big|\; 0 \le k < r \bigr\}.
\)

Because our system employs a \emph{chunk-based} representation, the dimension \(r\) is subdivided into chunks of size \(\texttt{chunk\_size}\). Concretely, the compiler replaces \(sd_1\) with
\[
\mathcal{C}
=\,
\bigl\{
 c \;\big|\; 0 \le c < 
 \bigl\lfloor 
   \tfrac{r}{\texttt{chunk\_size}}
 \bigr\rfloor
 \bigr\},
\]
so that each chunk index \(c\) corresponds to a slice of size \(\texttt{chunk\_size}\) within the original dimension \(r\).


Applying the general mapping,
\begin{multline*}
    op\_map\Bigl(Matmul\bigl(\{\mathbf{A},\mathbf{B}\}, \{(fd^A_1), (fd^B_1)\}, \{\mathcal{C}\}\bigr)\Bigr)= \\
\gamma_{(i,j),\;\mathrm{SUM}(\mathbf{a}^{(c)} \otimes \mathbf{b}^{(c)})}
\bigl(R_A \;\bowtie_{c}\; R_B\bigr)
\end{multline*}
where \(R_A\) and \(R_B\) are the chunked relational tables for \(\mathbf{A}\) and \(\mathbf{B}\). The join on \(k\) aligns matching row fragments, and the aggregation (\(\gamma\)) sums partial products to form each \(\mathbf{c}_{ij}\). This example demonstrates how free dimensions \((i,j)\) are retained in the output and the shared dimension \(k\) is “consumed” by the join and subsequent aggregation. 


\para{Elementwise Matrix Arithmetic}
Operators like \texttt{Add}, \texttt{Sub}, and \texttt{Mul} that apply elementwise arithmetic across two matrices are each mapped to a relational function involving a \(\bowtie\) on matching row and chunk indices, followed by a projection \(\pi\) that computes the desired operation. For example, 
\begin{multline*}
    op\_map\Bigl(Add\bigl(\{\mathbf{A},\mathbf{B}\}, \varnothing, \{sd_1,\mathcal{C}\}\bigr)\Bigr)=\\
\pi_{\,sd_1,\;\mathbf{x}^{(c)} + \mathbf{y}^{(c)}}\!\Bigl(
  R_A \;\bowtie_{(sd_1,\mathcal{C})}\; R_B
\Bigr)
\end{multline*}

If either input is a scalar, the compiler omits the join and simply injects a scalar expression into the projection step.

\para{Dimension Manipulation}
Dimension manipulations can be categorized into two broad classes, depending on whether they affect \emph{free dimensions} or \emph{shared dimensions}. In the first class, operations that merely split, merge, or rename free dimensions can be handled by simple \texttt{PROJECTION} expressions in SQL. For instance, suppose a tensor with free dimensions \((i,j)\) where \(0 \le i < 32\) and \(0 \le j < 64\) is reshaped into \((i_1,j_1,k_1)\) where \(0 \le i_1 < 2\), \(0 \le j_1 < 16\), and \(0 \le k_1 < 64\). The compiler can perform an integer-based remapping, e.g.\ \((i_1, j_1, k_1) \gets (\bigl\lfloor 
   \tfrac{i}{16}
 \bigr\rfloor,\; i \bmod 16, j)\), via a single projection statement without additional tables or joins.

For dimension changes affecting shared chunking dimensions, advanced transformations are required. Operators like rotary encoding may need unrolling chunked vectors into individual elements. Modern relational databases like DuckDB and ClickHouse support such operations with built-in \texttt{UNNEST} functions~\cite{duck,clickhouse}, enabling our compiler to reconfigure chunk layouts. Standard join and group-by operations then manipulate the shared dimension.

The compiler converts each neural operator into a relational function, creating a graph where edges represent input-output relations and nodes denote parameterized relational-algebraic operations. This SQL node graph mirrors the original model structure while transitioning computation from matrix-based to relational primitives.



\subsection{Stage 2: SQL Code Generation}
SQL is a highly structured language. Once the required attributes and operands are identified, they can be combined with relational primitives to construct the final SQL query.

The compiler converts the \emph{SQL node graph} into executable queries customized for the target database’s SQL dialect, handling syntax variations and function compatibility across database engines like PostgreSQL and DuckDB. Standard relational primitives (e.g., projections, joins, group-bys) are directly translated into \texttt{SELECT} clauses, while vector operations such as inner products or specialized aggregations are implemented using user-defined functions (UDFs) when necessary.

To optimize execution, the compiler can merge nodes into common table expressions (\texttt{WITH} clauses) or \texttt{CREATE VIEW} statements, reducing intermediate result overhead. For instance, elementwise operations can be fused into a single projection if the database supports efficient inline expressions. The final SQL script replicates the original LLM model’s functionality, including complex operators like multi-head attention and reshaping, all within standard relational constructs. An extensive list of other core operators and their SQL translations is provided in Appendix \ref{appendix:a}, Tab. \ref{tab:opmap}.

\subsection{Attention Mechanism Example}
In this section, we use the attention head—a core operation in LLM inference—as an example to illustrate how the compiler translates neural operators into relational functions. 

Consider the single-head attention
\[
\mathbf{S} \;=\; \mathrm{softmax}\!\Bigl(\tfrac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d}}\,\Bigr)\,\mathbf{V},
\]
where \(\mathbf{Q}, \mathbf{K}, \mathbf{V}\in \mathbb{R}^{T\times d}\). Here, \(T\) is the sequence length and \(d\) the hidden dimension. Although the core multiplication \(\mathbf{Q}\mathbf{K}^\top\) can be handled as a standard matrix multiplication (as in Sec.~\ref{sec:MM_example}), we now illustrate how the additional \texttt{softmax} and subsequent multiplication by \(\mathbf{V}\) are mapped to relational functions.

\para{Operator Mapping}
Each of \(\mathbf{Q}, \mathbf{K}, \mathbf{V}\) is split into chunks along the hidden dimension \(d\). The compiler represents them as tables:
\[
R_Q(t,\,c,\,\mathbf{q}^{(c)}),
\quad
R_K(t',\,c,\,\mathbf{k}^{(c)}),
\quad
R_V(t',\,c,\,\mathbf{v}^{(c)})
\]
where \(t\) or \(t'\in\{0,\dots,T-1\}\) is the token index, \(c\in \{\,0,\dots,\lfloor d/\texttt{chunk\_size}\rfloor-1\}\) is the chunk index, and \(\mathbf{q}^{(c)}, \mathbf{k}^{(c)}, \mathbf{v}^{(c)}\) hold the corresponding slice of size \(\texttt{chunk\_size}\) along the hidden dimension. The followings are operator mappings between neural operator and relational function in single head attention.


\para{\textbf{Form \(\mathbf{A} = \mathbf{Q}\mathbf{K}^\top\).}}  
  From the \texttt{MatMul} pattern, the compiler creates a join \(\bowtie_{c}\) on matching chunk indices and aggregates over that chunk dimension. In relational algebra:
  \[
    \mathbf{A}_{t,t'} \;=\;
    \gamma_{(t,t'),\,\mathrm{SUM}(\mathbf{q}^{(c)} \otimes \mathbf{k}^{(c)})}
    \Bigl(R_Q \;\bowtie_{(t',\,c)}\; R_K\Bigr)
  \]
  Here, \(R_Q\colon (t,c,\mathbf{q}^{(c)})\) and \(R_K\colon (t',c,\mathbf{k}^{(c)})\), so \((t,t')\) are free dimensions and \(c\) is shared.

\para{\textbf{Elementwise scalar: \(\mathbf{A} \leftarrow \tfrac{1}{\sqrt{d}}\,\mathbf{A}\).} } 
  The compiler inserts an elementwise \(\texttt{Mul}\)-by-scalar operator. In SQL, this is typically a \(\pi\)- (projection) step that multiplies each \(\mathbf{a}_{t,t'}\) by \(1/\sqrt{d}\). No additional join or group-by is needed, only a column update:
  \[
    \pi_{(t,t'),\, \tfrac{1}{\sqrt{d}}\times \mathbf{a}_{t,t'}}
    (R_A)
  \]

\para{ \textbf{\texttt{Softmax} by rows of \(\mathbf{A}\).}}  
  Softmax requires exponentiating each element \(\mathbf{a}_{t,t'}\), summing along \(t'\) per row \(t\), and dividing. In relational terms, the compiler performs:
  \begin{equation*}
  \begin{split}
      &\text{(i) } \gamma_{(t),\,\mathrm{SUM}(\exp(a_{t,t'}))}(\dots)\\
    &\text{(ii) } \pi_{(t,t'),\, \frac{\exp(a_{t,t'})}{\sum_{t'}\exp(a_{t,t'})}}
    (\dots)
    \end{split}
  \end{equation*}
  This yields a row-stochastic matrix \(\mathbf{M}\in \mathbb{R}^{T\times T}\) stored in a new relation.

\para{\textbf{Multiply \(\mathbf{M}\times \mathbf{V}\).} }
  Let \(R_M\colon (t,t',m_{t,t'})\) hold the \(\mathrm{softmax}\) result, chunked by \((t',c)\) if needed. Meanwhile, \(R_V\colon (t',c,\mathbf{v}^{(c)})\) stores \(\mathbf{V}\). The final step mirrors a \texttt{MatMul}:
  \[
    \mathbf{S}_{t,c}
    \;=\;
    \gamma_{(t,c),\, \mathrm{SUM}(m_{t,t'} \otimes \mathbf{v}^{(c)})}
    \Bigl(R_M \;\bowtie_{t'}\; R_V\Bigr)
  \]
  Each entry \(\mathbf{s}_{t}\in \mathbb{R}^d\) emerges by aggregating partial products over \(t'\) and chunk index \(c\).



% \para{2) SQL Code Generation.}
% In the generated SQL, the compiler chains these operators:
% \begin{enumerate}
%   \item \texttt{MatMul}(\(\mathbf{Q}, \mathbf{K}^\top\)): A join on \((t'=t', c=c)\) with row indices \((t, t')\), followed by a group-by to sum chunked products, forming \(\mathbf{A}_{t,t'}\).
%   \item \texttt{Elementwise Scalar} (\(\tfrac{1}{\sqrt{d}}\)): A direct column-wise multiplication in a \texttt{SELECT} clause.
%   \item \texttt{Softmax}: An exponentiation and group-by sum over each row \(t\), followed by a final projection that divides each \(\mathbf{a}_{t,t'}\) by \(\sum_{t'} \exp(\mathbf{a}_{t,t'})\).
%   \item \texttt{MatMul}(\(\mathbf{M}, \mathbf{V}\)): Another join on \((t'=t', c=c)\) plus a group-by across chunks, generating the final output \(\mathbf{S} \in \mathbb{R}^{T\times d}\).
% \end{enumerate}
% Each step can be emitted as a \texttt{WITH} clause or a \texttt{CREATE VIEW} statement, ensuring that intermediate results like \(\mathbf{A}\) or \(\mathbf{M}\) remain logically isolated, while the database engine may optimize or fuse parts of the query plan internally. 

\para{Discussion}
This section illustrates how an attention layer—\(\mathbf{Q}\mathbf{K}^\top\), scaling, softmax, and final multiplication—maps systematically to relational functions: chunk-based \texttt{MatMul}, elementwise functions, row-wise aggregations, and joins. Extending to multi-head attention or causal masks involves adding filters or parallel subgraphs. 



% \section{Method}
% \label{sec:method}

% Our goal is to execute a computational graph (as defined by ONNX) entirely within a relational database. We accomplish this by \emph{translating} each ONNX operator (e.g., \texttt{MatMul}, \texttt{Add}, \texttt{Reshape}) into a sequence of relational algebra operations (e.g., \emph{projection}, \emph{join}, \emph{group-by} and \emph{aggregation}). In this section, we formalize a two-stage framework:

% \begin{enumerate}
%     \item \textbf{Stage 1. Tensor-Algebra Representation.}  
%     We regard the ONNX computational graph as a \emph{typed tensor algebra}, wherein each operator has attributes (e.g., shapes, chunk sizes) describing its input and output tensors.

%     \item \textbf{Stage 2. Translation to Relational Algebra.}  
%     We map each typed tensor operator (with its associated shape attributes) to one or more expressions in relational algebra (over chunk-based tables).
% \end{enumerate}

% \subsection{Stage 1: Tensor-Algebra Representation}

% \para{Typed Operators and Shapes.}
% Consider an ONNX computational graph \(\mathcal{G}\) as a directed acyclic graph of operator nodes. Each node \(O\) is a function
% \[
%   O:\,\bigl(T_1,\dots,T_m\bigr)\;\mapsto\;U,
% \]
% where \(T_i\) (inputs) and \(U\) (output) are tensors. We attach a \emph{shape attribute} \((s_1, s_2, \dots, s_k)\) to each tensor \(T\), indicating its dimensionality. For example, a matrix \(\mathbf{A}\in\mathbb{R}^{m\times r}\) has shape \((m,r)\). In LLMs, higher-rank shapes, e.g.\ \((\text{batch}, \text{seq}, \text{hidden})\), also occur.

% Hence, an operator node \(O\) with inputs \(T_1,\ldots,T_m\) can be written:
% \[
%   O(T_1,\ldots,T_m; \vec{\alpha}):
%     (s_{1,1}, \ldots, s_{1,k_1}) \times
%     \cdots\times
%     (s_{m,1}, \ldots, s_{m,k_m})
%     \;\longmapsto\;
%     (s_{U,1}, \ldots, s_{U,k_U}),
% \]
% where \(\vec{\alpha}\) denotes extra parameters (e.g.\ \texttt{axis} for \texttt{Reshape}).

% \para{Chunk-Based Storage.}
% Each tensor \(T\) is \emph{stored} in a chunk-based relational table, employing a fixed \(\texttt{chunk\_size}\) (e.g.\ 128). Concretely, each row in the table corresponds to:
% \[
%   (\text{row\_indices},\, \text{chunk\_id},\, \mathbf{t}^{(c)}),
% \]
% where \(\mathbf{t}^{(c)} \in \mathbb{R}^{\texttt{chunk\_size}}\) is a contiguous slice of the row data. This chunk-based representation is also seen as a \emph{type attribute} in our tensor algebra, ensuring we track how data is physically laid out.

% \subsection{Stage 2: Translation to Relational Algebra}
% \label{subsec:translation}

% We now define a translation \(\mathcal{R}\) from the \emph{tensor-algebra} operator \(O\) to a (possibly small) \emph{relational algebra expression} that manipulates chunk-based tables. Formally:
% \[
%   \mathcal{R}\bigl(O(T_1,\ldots,T_m)\bigr)
%   \;=\;
%   \sigma_{\text{op}}\bigl(\,\mathcal{R}(T_1),\ldots,\mathcal{R}(T_m)\bigr),
% \]
% where \(\sigma_{\text{op}}\) denotes the resulting relational algebra operators (e.g.\ \(\pi\) for projection, \(\bowtie\) for join, \(\gamma\) for group-by and aggregation). Each input tensor \(T_i\) is represented by \(\mathcal{R}(T_i)\), i.e., the corresponding chunk-based table. Below, we illustrate this for typical operators in LLMs.

% \subsubsection{Matrix Multiplication}
% \label{subsubsec:matmul}

% Consider two matrices \(\mathbf{A}\in\mathbb{R}^{m\times r}\) and \(\mathbf{B}\in\mathbb{R}^{r\times n}\), stored in tables \(R_A\) and \(R_B\). Let:
% \[
%   R_A(i,k,\mathbf{a}^{(c)}) 
%   \quad\text{and}\quad
%   R_B(k,j,\mathbf{b}^{(c)}),
% \]
% where \(\mathbf{a}^{(c)}\) and \(\mathbf{b}^{(c)}\) are chunk vectors, and \(i,k,j\) track the row/column indices. The product \(\mathbf{C}=\mathbf{A}\mathbf{B}\) is another matrix in \(\mathbb{R}^{m\times n}\). In relational algebra, we can write:

% \[
%   \mathcal{R}\bigl(\texttt{MatMul}(A,B)\bigr)
%   = 
%   \gamma_{i,j,\,\mathrm{SUM\_VEC}(\mathbf{a}^{(c)}\otimes\mathbf{b}^{(c)}) \to \mathbf{c}^{(c)}}
%   \bigl(\,\pi_{i,k,\mathbf{a}^{(c)}}(R_A)\;\bowtie_{k}\;\pi_{k,j,\mathbf{b}^{(c)}}(R_B)\bigr).
% \]

% Here:
% \begin{itemize}
%     \item \(\pi_{i,k,\mathbf{a}^{(c)}}\) is the \emph{projection} operator, selecting attributes \(\{i,k,\mathbf{a}^{(c)}\}\) from \(R_A\).
%     \item \(\bowtie_{k}\) denotes a \emph{join} on matching values of \(k\).
%     \item \(\gamma\) is the \emph{group-by/aggregation} operator. The grouping is on \(\{i,j\}\), and the aggregation \(\mathrm{SUM\_VEC}(\mathbf{a}^{(c)}\otimes\mathbf{b}^{(c)})\) performs elementwise multiplication and sum reduction of chunk vectors, yielding \(\mathbf{c}^{(c)}\).
% \end{itemize}

% \subsubsection{Elementwise Operators}

% Elementwise ops like \(\texttt{Add}, \texttt{Sub}, \texttt{ReLU}\) translate to a simple join and a vector function application. For instance, if \(X(i,c,\mathbf{x}^{(c)})\) and \(Y(i,c,\mathbf{y}^{(c)})\) share the same shape (and thus the same row+chunk indexing), then:
% \[
%   \mathcal{R}(\texttt{ElemOp}(X,Y))
%   \;=\;
%   \pi_{\,i,c,\;\phi(\mathbf{x}^{(c)},\,\mathbf{y}^{(c)}) \to \mathbf{z}^{(c)}} 
%   \bigl(\,
%       X \;\bowtie_{(i,c)}\; Y
%   \bigr),
% \]
% where \(\phi\) is the elementwise function (e.g.\ addition, subtraction, \(\mathrm{ReLU}\), etc.).

% \subsubsection{Reshape}

% A \(\texttt{Reshape}\) operator modifies the \emph{shape} of a tensor without changing any of its underlying data. In chunk-based storage, this is mostly \emph{index remapping}. Formally, if we reshape \((s_1,\dots,s_k)\) to \((s'_1,\dots,s'_{k'})\), the product of dimensions is unchanged:
% \(\prod_i s_i = \prod_j s'_j\). The corresponding relational expression is:

% \[
%   \mathcal{R}(\texttt{Reshape}(T))
%   \;=\;
%   \pi_{\text{new\_index\_exprs},\;\mathbf{val}}
%   \bigl(\mathcal{R}(T)\bigr).
% \]

% In practice, \(\text{new\_index\_exprs}\) are computed by integer division/modulo arithmetic over the old index columns (and possibly chunk IDs). The chunked vectors themselves \(\mathbf{val}\) remain the same.

% \subsection{Examples of Translating LLM Operators}

% With the typed tensor-algebra perspective in place, we can handle complex operators by decomposing them into fundamental steps like \(\texttt{MatMul}\), \(\texttt{Elementwise}\), \(\texttt{Reshape}\), \(\texttt{GroupBy}\), etc. We illustrate this for two essential LLM components.

% \subsubsection{Multi-Head Attention}

% \para{1) Linear Projections.}
% For an input \(\mathbf{X}\in\mathbb{R}^{n\times d}\), multi-head attention (MHA) first computes
% \[
%   \mathbf{Q} = \mathbf{X}\mathbf{W}_Q,\quad
%   \mathbf{K} = \mathbf{X}\mathbf{W}_K,\quad
%   \mathbf{V} = \mathbf{X}\mathbf{W}_V.
% \]
% Each of these is a \(\texttt{MatMul}\) operator with typed shapes \((n,d)\times(d,d)\to(n,d)\).

% \para{2) Reshape \& Split into Heads.}
% \(\mathbf{Q},\mathbf{K},\mathbf{V}\) get reshaped from \((n,d)\) to \((n,h,d_h)\). In relational algebra, this is simply \(\pi_{\text{new\_indices},\mathbf{val}}\).

% \para{3) Scaled Dot-Product.}
% Each head \(h\) executes
% \(
%   \mathbf{Q}_h \mathbf{K}_h^\top
% \),
% scaled by \(1/\sqrt{d_h}\), followed by a row-wise \(\texttt{softmax}\) and a final multiplication by \(\mathbf{V}_h\). This again reduces to \(\texttt{MatMul}\) and \(\texttt{Elementwise}\) operators plus a group-based \(\texttt{softmax}\) (which can be expressed via exponentiation and a sum reduction).

% \para{4) Concat \& Final Projection.}
% The heads get concatenated along the hidden dimension (another \(\texttt{Reshape}\) or \(\texttt{Concat}\)) and multiplied by \(\mathbf{W}_O\). Each step is mapped systematically to relational queries.

% \subsubsection{Positional Rotary Encoding}
% \label{subsubsec:rotary}

% Rotary position encoding~\cite{su2021roformer} injects positional information into each query/key vector by applying a rotation that depends on the token's position \(p\). In practice, we split the hidden dimension in half (pairing up adjacent elements), and rotate each pair \(\bigl(x_{2i},\,x_{2i+1}\bigr)\) by an angle \(\theta_p\). Mathematically, if \(\mathbf{x}\in\mathbb{R}^d\) is the original vector at position \(p\), then its rotary-encoded vector \(\mathbf{x}'\in\mathbb{R}^d\) is given by:
% \begin{align}
%   x'_{2i}   &= x_{2i}\,\cos(\theta_p) \;-\; x_{2i+1}\,\sin(\theta_p),\\
%   x'_{2i+1} &= x_{2i}\,\sin(\theta_p) \;+\; x_{2i+1}\,\cos(\theta_p),
%   \quad i=0,1,\dots,\frac{d}{2}-1.
% \end{align}
% Here, \(\theta_p\) is a position-dependent phase often computed via a geometric progression or learned parameters.

% \para{Relational Encoding.}
% We store \(\cos(\theta_p)\) and \(\sin(\theta_p)\) in a small table \(\Theta\) keyed by \(p\), so \(\Theta\) has tuples \((p, \cos(\theta_p), \sin(\theta_p))\). Meanwhile, our query/key vectors are chunked and stored in a table \(X\) with tuples of the form
% \[
%   \bigl(p,\; c,\; \mathbf{x}^{(c)}\bigr),
% \]
% where \(\mathbf{x}^{(c)}\in \mathbb{R}^{\texttt{chunk\_size}}\) holds a contiguous slice of the vector \(\mathbf{x}\). To perform the rotary encoding inside the database:

% \begin{enumerate}
%     \item \textbf{Join on position \(p\).} We associate each chunked row \(\bigl(p,c,\mathbf{x}^{(c)}\bigr)\) with the corresponding \(\cos(\theta_p), \sin(\theta_p)\) by joining on \(p\).
%     \[
%       X \;\bowtie_p\; \Theta \;=\;
%       \bigl\{
%         (p,\,c,\,\mathbf{x}^{(c)},\,\cos(\theta_p),\,\sin(\theta_p))
%       \bigr\}.
%     \]

%     \item \textbf{Apply a vector function.} We define a UDF (user-defined function) \(\mathrm{rotary\_encode}\) that loops over each \((x_{2i},x_{2i+1})\) pair in the chunk \(\mathbf{x}^{(c)}\) and applies the above rotation formula. Formally, we can write the relational algebra expression:
%     \[
%       R_{\text{rot}} \;=\;
%       \pi_{\,p,\;c,\;\mathrm{rotary\_encode}\!\bigl(\mathbf{x}^{(c)},\,\cos(\theta_p),\,\sin(\theta_p)\bigr) \;\to\; \mathbf{x}'^{(c)}}
%       \Bigl(X \;\bowtie_{p}\; \Theta\Bigr),
%     \]
%     which projects out a new chunk vector \(\mathbf{x}'^{(c)}\). Concretely, for each pair \(\bigl(x^{(c)}_{2i}, x^{(c)}_{2i+1}\bigr)\) in the chunk, the UDF does:
%     \begin{align*}
%       x'^{(c)}_{2i}   &\leftarrow\; x^{(c)}_{2i}\,\cos(\theta_p) \;-\; x^{(c)}_{2i+1}\,\sin(\theta_p),\\
%       x'^{(c)}_{2i+1} &\leftarrow\; x^{(c)}_{2i}\,\sin(\theta_p) \;+\; x^{(c)}_{2i+1}\,\cos(\theta_p).
%     \end{align*}
% \end{enumerate}

% In this manner, the chunked row indices \((p,c)\) guide the join to retrieve the correct rotation parameters \(\bigl(\cos(\theta_p), \sin(\theta_p)\bigr)\), and the pairwise rotation is applied within each chunk vector by a specialized vector function. The resulting table \(R_{\text{rot}}\) has the same shape as \(X\) (i.e., \((p, c, \mathbf{x}'^{(c)})\)), but with the data inside each chunk updated to reflect the position-based rotation.


% \subsection{Summary of the Framework}

% By equipping each ONNX operator with shape-based typing (Stage~1) and mapping it to relational algebra operators over chunk-based tables (Stage~2), we obtain a coherent end-to-end theory. Every step of the LLM computation is expressible through:
% \begin{itemize}
%     \item \(\pi\) (projection),
%     \item \(\bowtie\) (join),
%     \item \(\gamma\) (group-by + aggregation),
%     \item plus a handful of \emph{elementwise} or \emph{UDF} functions.
% \end{itemize}

% This guarantees correctness (the shape attributes match up) and fidelity to the physical chunked format. In the next sections, we describe how this translation is implemented and present performance results demonstrating that a relational database can indeed serve as an execution engine for large language model inferences.
\section{Compiler Implementation}
\label{sec:compiler_implementation}

Our compiler converts an ONNX computational graph into chunked relational tables and then emits executable SQL queries for standard database engines. The overall process comprises four main components: \emph{data conversion}, \emph{pre-processing the computational graph}, \emph{two-stage compilation}, and \emph{post-optimization}.

\subsection{Data Conversion}
We begin by transforming each matrix, tensor, or model weight into a \emph{chunked} relational table. Concretely, for a matrix \(\mathbf{W}\) of shape \((m, n)\), we choose a chunk size \(c\) and create rows of the form \((i,\, c',\, \mathbf{w}^{(c')})\), where \(0 \le i < m\) indexes the original row, \(0 \le c' < \lfloor\frac{n}{c}\rfloor\) labels each chunk, and \(\mathbf{w}^{(c')}\in \mathbb{R}^c\) holds a contiguous slice of size \(c\). This same principle extends to higher-dimensional tensors: each dimension is broken into one or more chunk indices, ensuring that the final table remains manageable even when models contain billions of parameters. The database engine can thus store and page these chunked weights without exceeding memory.

\subsection{Pre-processing the Computational Graph}
We accept a topologically sorted ONNX graph as input, where each node lists its operator type and dependencies. Our pre-processing first \emph{annotates shapes} on every operator and tensor, ensuring that each node carries information about free dimensions, shared dimensions, and scalars. This step enables relational functions to be constructed consistently. Once shapes are annotated, we apply a \emph{pre-optimization} pass:

\para{Constant Folding}
Constant folding \cite{constant,tvm} is a classic compiler optimization technique. In computational graph, by examining the shape annotations and operator type, we detect whether an operator produces a scalar (e.g., constants or reduced values). If so, the compiler immediately evaluates such computations and stores them as constant attributes, avoiding runtime overhead in the relational plan. For example, when a dimension size can be multiplied or added at compile time, we do so and supply the result directly as a parameter in subsequent queries.

\para{Shape-Manipulation Elimination}
Operators such as \texttt{Reshape}, \texttt{Squeeze}, and \texttt{Expand}, which operate on free dimensions, can be merged with their successor operators by adjusting the projection primitives. This optimization eliminates the need to translate these operators into separate SQL queries, reducing the overhead of table scans.

\subsection{Two-stage Compilation}
After the pre-processing step, the two-stage compiler converts each ONNX operator into a \emph{relational function}, mapping computations like \texttt{MatMul}, elementwise arithmetic, and \texttt{Softmax} to joins, group-bys, and projections. These become a directed acyclic graph of relational functions, each describing chunk-based logic (e.g., \texttt{MatMul} as a join+\texttt{SUM}, \texttt{Softmax} as exponentiation+\texttt{GROUP BY}). The compiler then emits a final SQL queries: standard relational primitives (\texttt{JOIN}, \texttt{GROUP BY}, \texttt{PROJECTION}) produce most functionality, while vector-level operations (inner products, elementwise ops) are inserted as user-defined functions (\texttt{UDFs}). Any SQL-compliant engine with these UDFs can thus execute the entire LLM inference pipeline without relying on specialized deep learning runtimes or hardware accelerators.


% \subsection{SQL Query Generation}
% \label{subsec:sql_generation}

% Once the relational algebra representation has been fully constructed and optimized, the compiler proceeds to generate executable SQL queries. This final stage bridges the abstract relational algebra and the concrete SQL dialect of the target database while integrating user-defined functions (UDFs) for specialized vector operations. Different database systems often implement SQL with variations in syntax and available functions, so the compiler contains a dialect adaptation layer that transforms intermediate relational algebra expressions into SQL statements conforming to the target system's requirements. This layer handles differences in join syntax, aggregation functions, and other SQL constructs, ensuring compatibility across systems such as DuckDB, PostgreSQL, or others.

% For certain operations—particularly those on vectors, such as vector aggregation or inner products—that cannot be expressed directly in standard SQL, the compiler relies on pre-defined UDFs. Prior to SQL generation, the system is provided with metadata about available UDFs, including their names and expected input/output types. During query generation, the compiler identifies points in the relational algebra where these UDFs are necessary and injects calls to them within the SQL statements, combining them with native SQL constructs. For example, a vector aggregation step in relational algebra might translate into a SQL clause like: 
% \[
%   \texttt{SELECT key, vector\_agg\_udf(val) AS aggregated\_val FROM ... GROUP BY key;}
% \] 
% where \texttt{vector\_agg\_udf} is a UDF recognized by the compiler and supported by the target database.

% As the compiler traverses the sequence of relational nodes, it translates operations such as projection (\(\pi\)), join (\(\bowtie\)), and group-by with aggregation (\(\gamma\)) into corresponding SQL clauses. When encountering a relational node that requires a vector operation, the compiler seamlessly inserts appropriate UDF calls. It assembles a series of SQL \texttt{CREATE VIEW} or \texttt{WITH} common table expression (CTE) statements, each corresponding to one step of the computation, ordered to respect data dependencies. The end result is a complete SQL program that, when executed on the target database, faithfully performs the original ONNX computational graph by judiciously combining native SQL operations with specialized vector operations through UDFs.

\subsection{Post-Optimization and KV-Cache Construction}
\label{subsec:post_optimization}
After generating the raw SQL statements, the compiler applies post-optimizations to streamline execution and support advanced features like key-value caching. It reduces intermediate tables by using \texttt{WITH} common table expressions (CTEs) \cite{SQL99}, chaining logical query steps without creating large temporary relations. This minimizes disk I/O, and speeds up execution.

For key-value caching (\texttt{KV-cache}) \cite{kvcache}, the compiler constructs or updates cache tables to store processed keys and values, indexed by sequence position or token index. For subsequent token generation, it reuses existing cache entries and emits compact queries to compute only new keys and values, avoiding redundant computations. This enhances throughput for interactive or streaming inference.

These optimizations ensure efficient execution, support for caching, and alignment with the model’s original semantics, reducing runtime overhead for LLM workloads.

\section{Experiments}
\label{sec:experiments}


In this section, we evaluate the performance of SQL queries generated by our proposed compiler using a real-world relational database and two models from the Llama3 family \cite{llama3}. First, we analyze the impact of chunk size on performance. Next, we compare token generation time and memory usage across \texttt{in-memory} and disk-memory hybrid database modes (\texttt{Disk+mem}), benchmarking against PyTorch \cite{pytorch} and a C++ native model-serving framework. Finally, to assess the efficiency of our approach under resource-constrained conditions, we simulate an edge device environment by setting an 8GB memory cap.

\subsection{Experiment Setup}

\para{Hardware Platforms}  
Experiments are conducted on a machine with 48 CPU cores, 96 GB of RAM, and DuckDB \cite{duck} as the analytical database engine. DuckDB’s native vector inner-product operators are extended with custom user-defined functions (UDFs) for additional vector operations. Evaluations are performed in both disk-memory hybrid and in-memory modes. We also test with an 8GB memory limit and 6 CPU cores to simulate resource-constrained conditions.  

\para{Llama3 Models}  
We evaluate \textbf{transformer-based} Llama3 models in two sizes: i) Llama3.2 3B, a smaller model balancing resource efficiency and performance, and ii) Llama3.1 8B, a larger model showcasing scalability for handling extensive parameters. To assess performance variations with input size, we test prompts of four lengths: 10, 100, 200, and 500 tokens. All models are unquantized, which do not impact the comparative results. Incorporating quantization support is planned for future work.

\para{Baselines}  
We compare against two baselines: i) \texttt{PyTorch (CPU Only):} Executes Llama3 models on CPU using default BLAS \cite{blas} libraries. ii) \texttt{Llama.cpp\footnote{\url{https://github.com/ggerganov/llama.cpp}}:} A lightweight C++ implementation for CPU inference, also relying on default BLAS libraries.  

\para{Metrics}  
Metrics include time to first token (TTFT) and time per output token (TPOT) to measure time consumption, and peak memory usage (in GB) to evaluate resource consumption. Peak memory is critical for assessing suitability on memory-constrained hardware.  

Our goal is not to surpass optimized deep learning frameworks but to demonstrate comparable performance to CPU-only methods without requiring specialized hardware. Enhancing vector UDFs with BLAS libraries could further boost the performance of database-based LLM serving.  

\subsection{Chunk-Size Sensitivity Study}

As discussed in Sec. \ref{subsec:mm-relational}, chunk size impacts performance when serving LLMs in a database. Larger chunks reduce table scan time but increase computational workload, while smaller chunks have the opposite effect. Our empirical study, as illustrated in Tab. \ref{tab:chunk}, shows that in \texttt{disk+mem} mode, chunk size has minimal impact due to the data transfer bottleneck. In \texttt{in-memory} mode, smaller chunks increase processing time per token, especially for TTFT. A chunk size of 64 offers the best balance and optimal TPOT. Therefore, we use chunk\_size=64 in all subsequent experiments for consistency and highest throughput.
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[t]
\caption{Effect of varying chunk sizes on TTFT and TPOT for the 3B model, evaluated under both \texttt{in-memory} and (\texttt{disk+mem}) modes. Chunk size 64 offers a balance, minimizing TPOT while sustaining competitive TTFT.}
\label{tab:chunk}
\centering
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
Chunk size & Method & TTFT (s) & TPOT (s) \\ \toprule
32 & \multicolumn{1}{c|}{\multirow{3}{*}{Disk+mem}} & 5.59 & 0.97 \\ \cline{1-1} \cline{3-4} 
64 & \multicolumn{1}{c|}{} & 5.80 & 1.00 \\ \cline{1-1} \cline{3-4} 
128 & \multicolumn{1}{c|}{} & 5.15 & 1.36 \\ \toprule
32 & \multirow{3}{*}{In-memory} & 4.81 & 0.34 \\ \cline{1-1} \cline{3-4} 
64 &  & 3.71 & \textbf{0.23} \\ \cline{1-1} \cline{3-4} 
128 &  & 3.39 & 0.35 \\ \hline
\end{tabular}%
}
\end{table}

 



\subsection{Memory Usage in Unlimited Memory Scenarios}
\label{subsec:memory_usage}

Memory utilization is crucial in resource-constrained environments, where large language models often exceed available RAM, leading to failures or degraded performance. In this experiment, we compare the memory usage of our method with PyTorch and Llama.cpp using the same model, highlighting the efficiency of our disk+mem setting.
Figure~\ref{fig:memory} shows memory usage for in-memory and disk+mem configurations with PyTorch and Llama.cpp. PyTorch loads all weights into memory, failing if the model exceeds device memory. Llama.cpp supports dynamic weight loading but, in our test with unlimited memory, shows similar memory usage to PyTorch. Our in-memory mode uses more memory than expected due to DuckDB's need to maintain metadata and indexes, introducing overhead. 

However, our disk+mem configuration achieves significantly lower peak memory usage. For example, an 8B model (31GB) runs on disk+mem using less than 20GB. This demonstrates that our compiler-generated code, combined with DuckDB's intrinsic cache management, reduces memory usage without extra engineering effort, enhancing LLM accessibility.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/memory.pdf}
\caption{Peak memory usage under different inference configurations for Llama3 models. We compare the \texttt{in-memory} and \texttt{disk+mem} modes, highlighting how disk-based cache management reduces peak RAM usage while incurring additional I/O overhead.}    
\label{fig:memory}
\end{figure}



% While in-memory mode stores all model data in RAM, resulting in stable but high usage, disk–memory mode significantly reduces RAM requirements by offloading unused data to disk, with some I/O overhead. In contrast, PyTorch and Llama.cpp, lacking built-in paging, use less memory for smaller models but experience spikes with larger ones or multiple requests.

% These findings underscore the database’s natural advantage as a runtime. Its disk-backed operations allow efficient management of oversized model weights, offering a practical solution for scaling LLM inference beyond RAM limits, even on resource-constrained platforms.


\subsection{ Time Usage in Memory-Constraint Scenarios}
\label{subsec:memory_usage_limited}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/limited_mem.pdf}
\caption{TTFT and TPOT under an 8GB memory limit, comparing Llama.cpp to the \texttt{disk+mem} approach for prompts of varying length.}    
\label{fig:limited_mem}
\end{figure}

We tested two systems with dynamic weight loading functions (disk+mem and Llama.cpp) under memory-constrained conditions (6 cores, 8GB RAM) to evaluate their performance in edge scenarios.

Figure \ref{fig:limited_mem} compares the time usage of disk+mem and Llama.cpp when serving an 8B model across varying prompt lengths. When the prompt length is less than 500 tokens, our disk+mem mode achieves significantly lower TTFT than Llama.cpp, highlighting the efficiency of our method even with edge-level hardware configurations. However, when the prompt length reaches 500 tokens, Llama.cpp performs faster due to the more complex data structures of database tables compared to naive matrices, which result in increased data communication.The performance degradation is especially significant when generating the first token, as it requires computing all Q, K, and V for all tokens.

In tests using TPOT, disk+mem maintains consistent time usage under 10 seconds, while Llama.cpp exceeds 300 seconds, making it 30 times slower for interactive use. By leveraging dynamic disk-memory cache management instead of relying on all-in-RAM strategies, disk+mem serves an 8B model (31GB) under the same memory constraints, offering a practical solution for resource-limited environments. This confirms the advantage of using databases as runtimes to serve LLMs in edge scenarios.
\subsection{Time Usage for First Token and Subsequent Tokens}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/time.pdf}
\caption{Comparison of time to generate the first token and the next token for various Llama3 models, prompt inputs, and methods.}    
\label{fig:time}
\end{figure*}

For completeness, we compared TTFT (Time-to-First-Token) and TPOT (Time-per-Token) across four methods and two model scales on server-level hardware. As shown in Fig. \ref{fig:time}, the \texttt{in-memory} mode consistently outperforms the \texttt{disk+mem} mode for both metrics. For example, with the 8B model and a 10-token prompt, \texttt{disk+mem} introduces a ~50\% delay in TTFT due to overhead from on-demand loading of model weights and query results. A similar pattern is observed with the 3B model, where the in-memory mode consistently outperforms the disk+mem mode.


Compared to baseline methods like \texttt{PyTorch CPU} and \texttt{Llama.cpp}, database modes show higher TTFT—often by an order of magnitude—due to the computational overhead of relational primitives (\texttt{JOIN}, \texttt{GROUP BY}, \texttt{Table Scan}) and unoptimized vector UDFs, rather than using efficient neural operator kernels. However, for subsequent tokens, the in-memory mode delivers more competitive performance, while the disk-memory mode remains slower due to the overhead of data loading.


These results demonstrate that under unlimited resources, serving LLMs within a database does not match the performance of deeply optimized deep learning frameworks. However, there remains significant room for optimization. The current database setup does not leverage BLAS libraries for accelerating vector operations. Integrating such linear algebra libraries could substantially narrow the performance gap between in-database model serving and dedicated deep learning frameworks.

\para{Takeaways}Our experiments show that SQL-based LLM inference enables efficient execution on resource-constrained CPUs, achieving 30x speedups over disk-memory hybrid frameworks while preserving model integrity. Although performance on high-resource hardware lags behind PyTorch, integrating libraries like BLAS could bridge this gap. This work proves relational databases as viable, hardware-agnostic runtimes for LLM deployment in accelerator-limited environments.



% \subsection{Summary of Findings}

% Our experiments show that a properly tuned chunk size, especially around 64, offers a good trade-off between relational overhead and vector processing efficiency. While not designed to outperform GPU-accelerated frameworks, our purely SQL-based method achieves performance similar to CPU-only baselines without leveraging tensor program optimizations, demonstrating that relational engines can feasibly handle large-scale LLM inference workloads. We also find that the first-token latency and subsequent token generation costs remain within practical bounds for interactive settings, particularly when KV-caching is used to reduce redundant computations across tokens.

% \rihan{This paper is proposing a new methodology. I will 1) weaken the drawbacks but emphasize the benefits in each subsection. 2) But keep this summary of findings; here, first summarize all the pros of using this new methodology, then summarize its limitations. So people can appreciate your contribution while admiring your integrity and honesty, clear limitation (applicable scenarios) is also a type of contribution. Finally, end with one or two sentences saying potential implications of these results to support the claim: our work can help deploy LLMs in resource-constrained environments and its contribution to making advanced AI more accessible.}

\section{Related Work}
% \rihan{For SIGMOD you can keep this section as it is. For ICML I will drop or only keep a very short para for RDBMS part. Instead, I will first explain, Serving LLMs with Limited Memory, Intermediate Representations for Deep Learning, maybe even start from Quantization, which is close to learning-based existing works}

\para{Serving LLMs with Limited Memory}
Serving large language models (LLMs) under memory constraints involves three main strategies: weight pruning, low-bit quantization, and dynamic weight loading. 

Weight pruning reduces a model's memory footprint by removing selected parameters. Common methods include magnitude-based pruning \cite{magnitude_prune1} and structured pruning \cite{structure_prune1, structure_prune2, pruning}. However, pruning often requires model-specific knowledge and additional fine-tuning, making it less flexible and more labor-intensive for new models or architectures.
    
Low-bit quantization uses fewer bits (e.g., 8-bit or 4-bit) to represent floating-point values, reducing memory usage and potentially speeding up computation on compatible hardware. Post-training quantization \cite{quantize1, quantize2, quantize3} is favored for faster deployment, but hardware support for reduced data types (e.g., \texttt{INT8}) is crucial. Many embedded CPUs (e.g., ARM, RISC-V) lack such support, negating the benefits of quantization on resource-limited devices.
    
Instead of keeping all parameters in memory, dynamic weight loading cache parameters from disk or slower memory on demand. Examples include DejaVu \cite{Dejavu}, which uses activation sparsity to load necessary weights, and FlexGen \cite{FlexGen}, which offloads weights and caches to CPU DRAM. However, these methods often require specialized handling and sophisticated engineering to manage weight transfers across memory layers.

In summary, while these strategies mitigate memory bottlenecks, they introduce constraints such as accuracy degradation, hardware dependency, or specialized engineering. Our approach leverages relational databases' native cache management to handle models exceeding system memory, eliminating custom engineering and enhancing portability.

\para{Intermediate Representations for Deep Learning}
Deep learning compilers often use intermediate representations (IRs) like \emph{MLIR}~\cite{mlir}, \emph{Relay IR}~\cite{Relay}, and \emph{ONNX IR}~\cite{onnx} to enable cross-platform model execution and interoperability. While these approaches drive innovation, they require extensive maintenance to support evolving hardware and kernels, resulting in fragmented ecosystems and complex, toolchain-specific deployment pipelines.

Our approach uses \emph{SQL} as an IR for deep learning inference, avoiding custom IRs or specialized runtimes. Relational engines, optimized over decades, efficiently handle data across platforms, from servers to embedded systems like SQLite. By compiling neural operators into SQL, we align deep learning tasks with familiar database abstractions, ensuring portability, reducing engineering effort, and providing a sustainable alternative to IR-based solutions.


\para{Deep Learning in Databases}
Recent database research has explored representing deep learning computations using relational algebra. Works like Dimitrije et al. \cite{declaritive}, SmartLite \cite{smartlite}, and ModelJoin \cite{modeljoin} integrate matrix operations into relational databases but require extensive database-specific engineering, limiting portability and compatibility with traditional deep learning ecosystems.

DuckBrain \cite{duckbrain} and DL2SQL \cite{dl2sql} take steps toward integrating deep learning with databases. DuckBrain handles simple neural networks but lacks a framework for complex models, serving as a proof of concept. DL2SQL translates basic convolution network modules (e.g., convolution layer, max pooling) into SQL queries but cannot automatically handle diverse modules like attention or rotary embeddings.

Our approach introduces a low-level operator mapping method, focusing on fundamental arithmetic operations like matrix multiplication and elementwise functions. This enables our compiler to support a wider range of deep learning models and automate the translation of LLMs into SQL queries.




\section{Conclusion and Outlook}
\label{sec:conclusion}

We present a method for converting neural operators into relational functions and a compiler that transforms LLM inference graphs into SQL queries, enabling relational databases to serve as a runtime for LLMs inference. While our token-generation time is slower than PyTorch in unlimited-memory scenarios, it remains competitive and outperforms in memory-constrained settings where PyTorch fails to load larger models. This database-driven approach reduces engineering overhead, avoids framework fragmentation, and expands access to LLM inference for low-resource environments and broader audiences.

\para{Outlook}
Future improvements include integrating BLAS libraries and quantization into our SQL compiler to accelerate vector operations, optimizing query plans for efficiency, and exploring lightweight database engines like SQLite for edge-level deployments. These advancements aim to further enhance resource efficiency and performance, extending the accessibility to large-scale language modeling.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\newpage

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning and Database. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Table Schema}
In this section, we detail the table schema for model weights in the LLaMA3.1 8B model. The 8B model consists of 32 layers, each with an identical schema. Thus, we focus on describing the schema for a single layer.
\begin{verbatim}
vocabulary (token_encode INT32, chunk_id INT32, embedding FLOAT[])

freq_each_token (token_id INT32, freq_real FLOAT[], freq_img FLOAT[])

Q_weights_L1 (head_id INT32, row_id INT32, chunk_id INT32, chunk FLOAT[])

K_weights_L1 (head_id INT32, row_id INT32, chunk_id INT32, chunk FLOAT[])

V_weights_L1 (head_id INT32, row_id INT32, chunk_id INT32, chunk FLOAT[])

o_weights_L1 (row_id INT32, chunk_id INT32, chunk FLOAT[])

GLU_W1_L1 (row_id INT32, chunk_id INT32, chunk FLOAT[])

GLU_W2_l 1(row_id INT32, chunk_id INT32, chunk FLOAT[])

GLU_W3_L1 (row_id INT32, chunk_id INT32, chunk FLOAT[])

FFN_Norm_L1 (chunk_id INT32, chunk FLOAT[])

Attention_Norm_L1 (chunk_id INT32, chunk FLOAT[])

Final_Norm (chunk_id INT32, chunk FLOAT[])

Output (row_id INT32, chunk_id INT32, chunk FLOAT[])

\end{verbatim}


\section{Vector UDFs}
In this section, we describe the user-defined functions (UDFs) for vector arithmetic operations. While some databases support array data types, their vector operation capabilities are often limited. To address this, we implement vector UDFs using native lambda functions, a feature widely supported in databases such as DuckDB, ClickHouse, and PostgreSQL. We illustrate this using DuckDB as an example.

\(arr1*arr2\)
\begin{verbatim}
create macro hadamard\_prod(arr1, arr2) as
(list_transform(list_zip(arr1, arr2), x -> x[1] * x[2]));
\end{verbatim}
\(arr1-arr2\)
\begin{verbatim}
create macro element_neg_sum(arr1, arr2) as 
(list_transform(list_zip(arr1, arr2), x -> x[1] - x[2]));
\end{verbatim}
\(arr1+arr2\)
\begin{verbatim}
create macro element_sum(arr1, arr2) as 
(list_transform(list_zip(arr1, arr2), x -> x[1] + x[2]));
\end{verbatim}
\(concat(arr1,arr2)\)
\begin{verbatim}
create macro view_as_real(arr1, arr2) as (list_concat(arr1, arr2));
\end{verbatim}
Collecting tuple (idx, value) as an array based on the positional index `idx'.
\begin{verbatim}
create macro collect_as_array(idx, val) as
(list_transform(list_sort(list_zip(idx, val)), x -> x[2]));
\end{verbatim}
Extracting the first half of the input array as a new array.
\begin{verbatim}
create macro collect_real(ziped_arr, mid_pos) as
(list_transform(ziped_arr[:mid_pos], x -> x[2]));
\end{verbatim}
Extracting the second half of the input array as a new array.
\begin{verbatim}
create macro collect_img(ziped_arr, mid_pos) as
(list_transform(ziped_arr[mid_pos:], x -> x[2]));
\end{verbatim}
\(\sum_i chunk_i\), sum all arrays.
\begin{verbatim}
create macro sumForEach(arr) as
(list_reduce(arr, (acc, row)-> list_transform(acc, (acc_val, i) -> acc_val + row[i])));
\end{verbatim}


\section{Core Neural Operators and Corresponding SQL Queries}
\label{appendix:a}
In this section, we illustrate the core modules of the LLaMA3.1 8B model, specifically self-attention and rotary positional encoding, each represented by distinct SQL query patterns. Other operators primarily reuse similar SQL queries derived from these three core components.

The self-attention mechanism involves multiple matrix multiplications (matmuls) and a softmax operation. The matmul operations are described in detail in the main text. The softmax function is a reduction operation that first sums all exponentiated values and then divides each value by the total sum.

Rotary positional encoding requires splitting a vector into two equal-sized subvectors representing the real and imaginary parts of a complex number array. After performing the necessary computations, the real and imaginary parts are concatenated.

In RMS normalization, an additional aggregation step is involved compared to the softmax operation. It first sums all vectors into a single vector and then reduces this vector to a scalar, resulting in longer computation times than the softmax implementation.
The following Table \ref{tab:opmap} elaborates on these three core neural operators and their corresponding pseudo SQL queries.
\begin{table*}[h]
\caption{Core neural operators and corresponding pseudo SQL queries.}
\label{tab:opmap}
\centering
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Llama3 layers} & \textbf{Notation} & \textbf{Neural operators} & \textbf{SQL query} \\ \hline
\multirow{8}{*}{Attention} & \multirow{8}{*}{$softmax(\frac{Q K^T}{\sqrt d})V$} & Matmul & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}\texttt{SELECT token, head, row, SUM(DOT(query\_chunk, embedding)) AS q}\\ \texttt{FROM query{[}key,value{]}\_weights as A JOIN embedding as B}\\ \texttt{ON A.col = B.col GROUP BY row}\end{tabular}} \\ \cline{3-3}
 &  & Matmul &  \\ \cline{3-3}
 &  & Matmul&  \\ \cline{3-4} 
 &  & Matmul & \begin{tabular}[c]{@{}l@{}}\texttt{SELECT Q.token, K.token, Q.head, EXP(SUM(q * k) / sqrt($head\_dim$)) as qk FROM Q} \\\texttt{JOIN K on Q.row = K.row and Q.head//4 = K.head GROUP BY Q.token, K.token, Q.head}\end{tabular} \\ \cline{3-4} 
 &  & Softmax & \begin{tabular}[c]{@{}l@{}}\texttt{WITH summation AS ( SELECT Q.head, Q.token, SUM(qk) as s FROM QK} \\\texttt{GROUP BY Q.token, Q.head ) SELECT Q.head, Q.token, K.token, qk/s FROM} \\\texttt
 {QK JOIN sumamtion ON Q.head, Q.token}\end{tabular} \\ \hline
\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Rotary\\ positional\\ encoding\end{tabular}} & $\mathbf{x} =  \mathbf{x}_1, \mathbf{x}_2 \in \mathbb{R}^{d/2}$ & Split as complex & \begin{tabular}[c]{@{}l@{}}\texttt{SELECT token, head, collect\_real(collect\_as\_array(row), collect\_as\_array(r), 0) AS $x_1$,}\\ \texttt{collect\_img(collect\_as\_array(row), collect\_as\_array(r), 1) AS $x_2$ FROM $x$} \\\texttt{GROUP BY token,head}\end{tabular} \\ \cline{2-4} 
 & \begin{tabular}[c]{@{}l@{}}$   \begin{bmatrix}
   \cos(\theta) & -\sin(\theta) \\
   \sin(\theta) & \cos(\theta)
   \end{bmatrix}
   \begin{bmatrix}
   \mathbf{x}_1 \\ \mathbf{x}_2
   \end{bmatrix}$\end{tabular} & Rotation & \begin{tabular}[c]{@{}l@{}}\texttt{SELECT token, head, hadamard\_prod($x_1$, $\theta_1$) - hadamard\_prod($x_2$, $\theta_2$) as real,} \\\texttt{hadamard\_prod($x_1$, $\theta_2$) + hadamard\_prod($x_2$, $\theta_1$) AS img 
FROM complex\_vectors as A} \\\texttt{LEFT JOIN $rotation_\theta$ AS B ON A.token=B.token}\end{tabular} \\ \cline{2-4} 
 & $\text{Concat}(\mathbf{x}_1^{(p)}, \mathbf{x}_2^{(p)})$ & Merge to vector & \begin{tabular}[c]{@{}l@{}}\texttt{SELECT layer, token,head, VIEW\_AS\_REAL(real, img) as chunk} \\\texttt{FROM positional\_encoding}\end{tabular} \\ \hline
\multirow{4}{*}{RMS Norm} & $\frac{1}{\sqrt{(\sum\mathbf{x^2}/\text{dim})}}$ & \begin{tabular}[c]{@{}l@{}}Squared mean\\ \& rsqrt\end{tabular} & \begin{tabular}[c]{@{}l@{}}\texttt{Select token, 1/SQRT(SUM(arraySum($x$->$x^2$,embedding)) / $dim$) + $\epsilon$) as rep\_sqmean} \\ \texttt{FROM Embedding GROUP BY token}\end{tabular}\\ \cline{2-4} 
 & rep\_sqmean*$x$*norm\_weight & Weighted mean & \begin{tabular}[c]{@{}l@{}}\texttt{SELECT token, col, rep\_sqmean * hadamard\_prod($x$, weight) AS new\_embedding}\\ \texttt{FROM Embedding LEFT JOIN rsqrt ON rsqrt.token= Embedding.token}\\ \texttt{LEFT JOIN norm\_weight ON Embedding.col = norm\_weight.col}\end{tabular} \\ \hline
\end{tabular}%
}
\end{table*}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
