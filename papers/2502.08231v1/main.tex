
\documentclass[10pt]{article} %
\usepackage[preprint]{tmlr}


\usepackage{hyperref}
\usepackage{url}
\usepackage[noalg]{stroop}
\usepackage{wrapfig}

\newcommand\var[1]{\textsf{#1}}
\newcommand{\langpair}[2]{\texttt{#1-#2}}
\newcommand\dmin{d_{\min}}
\let\proj\undefined
\DeclareMathOperator{\proj}{proj}

\title{Keep your distance: learning dispersed embeddings 
on \(\bbS_d\)}


\author{\name Evgeniia Tokarchuk \email evgeniia@tokarch.uk \\
      \addr Language Technology Lab\\
      University of Amsterdam
      \AND
      \name Hua Chang Bakker \email hua.chang.bakker@student.uva.nl\\
      \addr University of Amsterdam
      \AND
      \name Vlad Niculae \email v.niculae@uva.nl\\
      \addr Language Technology Lab \\
      University of Amsterdam\\
     }


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  %
\def\year{YYYY} %
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} %


\begin{document}


\maketitle

\begin{abstract}
Learning well-separated features in high-dimensional spaces, such as text or image \textit{embeddings}, is crucial for many machine learning applications. Achieving such separation can be effectively accomplished through the \textit{dispersion} of embeddings, where unrelated vectors are pushed apart as much as possible. By constraining features to be on a \textit{hypersphere}, we can connect dispersion to well-studied problems in mathematics and physics, where optimal solutions are known for limited low-dimensional cases. However, in representation learning we typically deal with a large number of features in high-dimensional space, and moreover, dispersion is usually traded off with some other task-oriented training objective, making existing theoretical and numerical solutions inapplicable. 
Therefore, it is common to rely on gradient-based methods to 
encourage dispersion, usually by minimizing some function of the pairwise distances. 
In this work, we first give an overview of existing methods from disconnected literature, 
making new connections and highlighting similarities.
Next, we introduce some new angles. We propose to reinterpret pairwise dispersion using a maximum mean discrepancy (MMD)
motivation. We then propose an online variant of the celebrated Lloydâ€™s algorithm, of K-Means fame, as an effective alternative regularizer for dispersion on generic domains. Finally, we derive a novel dispersion method that directly exploits properties of the hypersphere. Our experiments show the importance of dispersion in image classification and natural language processing tasks, and how algorithms exhibit different trade-offs in different regimes.

\end{abstract}

\section{Introduction}
Dispersion, sometimes referred to as spreading or uniformity,
is the property that a collection of points in a domain cover the domain well, with no two points being too close or too far.
When the domain is a hypersphere, a useful space for representing embeddings of text and images, optimal dispersion is challenging. 
Embedding clumping, \ie, occurrence of semantically distant embeddings that are close to each other in terms of distance metric, is a known problem, and it has been shown before that it negatively impacts the performance of the downstream tasks, such as image classification~\citep{pmlr-v119-wang20k,pmlr-v130-liu21d,Trosten-noHub-2023}, image generation~\citep{pmlr-v130-liu21d}, text classification~\citep{pmlr-v119-wang20k} and text generation~\citep{tokarchuk-niculae-2024-unreasonable}. 


In general, the problem of dispersing $N$ points on the surface of $d$ dimensional sphere, such that the angular distance between any two points is maximal, is an open mathematical problem known as the Tammes problem~\citep{tammes-1930}. The optimal solutions for this problem are known for small values of $d$ and $N$~\citep{GDZPPN002133873, DANZER19863, Waerdenvander1951, ROBINSON1961, musin-2012,tammes-problem-for-n14}. The Tammes problem can also be formulated as a problem of finding an optimal spherical code~\citep{alma990013623930205131} for given $d$ and $N$~\citep{table-spherical-codes}. However, we typically deal with a large number of dimensions and many points when learning, \eg, word embeddings. Moreover, perfect dispersion may not be optimal and may hurt the task-specific learning objective, so in learning applications we prefer being able to trade off between a task loss and a dispersion regularizer. For these reasons,
we cannot directly use classical results, and instead focus on hyperspherical dispersion as a gradient-based optimization of a regularization function.


We study several dispersion objectives in order to find an approximate solution to the dispersion problem on the unit hypersphere. In particular, we reinterpret Maximum Mean Discrepancy \citep[MMD,][]{JMLR:v13:gretton12a} as a method for dispersing an arbitrary number of high-dimensional points, adapt Lloyd's algorithm~\citep{lloyd1982}, and propose sliced dispersion that directly exploits properties of the hypersphere. We compare them to the previously proposed methods based on pairwise distances~\citep{mettes2019hyperspherical,sablayrolles2018spreading,pmlr-v130-liu21d,liu2018learning,pmlr-v119-wang20k}. We showcase the performance of those objectives by approximating optimal Tammes problem solutions and learning dispersed representation both for computer vision and natural language processing tasks. Our results show that there is a dependence between task performance and respective dispersion of the features.
Additionally, we highlight that using Riemannian optimization~\citep{bonnabel2013stochastic,becigneul2018riemannian} on the hypersphere, rather than projecting parameters to the sphere at each gradient update, benefits dispersion and overall task performance.

Our contributions are the following:
\begin{itemize}
    \item Review connections between several proposed dispersion regularizers based on pairwise distances, and give a new interpretation and motivation in terms of maximum mean discrepancy (MMD);
    \item Propose two new methods for approximating optimal dispersion (Lloyd and Sliced);
    \item Provide empirical comparison among dispersion optimization methods on tasks from vision and language processing;
    \item Investigate the impact of Riemannian optimization for dispersion.
\end{itemize}

Moreover, our pytorch-based implementation of dispersion objectives is available at \url{https://github.com/ltl-uva/ledoh-torch}.

\section{Background: Dispersion on the Hypersphere}

First we discuss the notation we are going to use throughout the paper, give the definition of ``dispersion'' and review existing approximate methods to estimate optimal dispersion.

\label{sec:disp-on-hypersphere}
\subsection{Notation}
\label{sec:notation}
We denote by \(\bbS_d\) the \(d\)-dimensional hypersphere embedded in \(\bbR^{d+1}\),
\ie, \(\bbS_d = \{ x \in \bbR^{d+1} \mid \|x\|=1\}\).
For \(u,v \in \bbR^{d+1}\) we denote their Euclidean inner product by \(\DP{u}{v} \coloneqq \sum_{i=1}^{d+1} u_i v_i\).
The hypersphere is an embedded Riemannian submanifold of \(\bbR^{d+1}\).
The tangent space of the sphere at a point \(x\) is \(T_x \bbS_d \coloneqq \{v \in \bbR^{d+1} \mid \DP{x}{v}=0\}
\simeq \bbR^d\), and the Riemannian inner product on it is inherited from \(\bbR^{d+1}\), \ie,
for \(u,v \in T_x\bbS_d, \DP{u}{v}_x \coloneqq \DP{u}{v}\).
The geodesic distance on a hypersphere is \(d(x, x') = \cos^{-1}(\DP{x}{x'})\).
As a special case, for \(d=1\) it is more convenient to work in an isomorphic angular parametrization, \ie,
\(\bbS_1 \simeq \{\theta \mid -\pi \leq \theta < \pi\}\) with \(d(\theta, \theta') = |\theta-\theta'|\):
the embedding of \(\bbS_1\) into \(\bbR^2\) is given by \(\theta \to (\cos \theta, \sin \theta)\).
We reserve the use of Greek letters \(\tau,\theta,\phi\) for 1-d angles. We denote by \(\Pi_n\) the set
of permutations of \((1, \ldots, n)\).

We use roman capitals, \ie,
\(X = (x_1, \ldots, x_n)\), to denote an (ordered) collection, or configuration,
of \(n\) points on the same sphere, \ie, each \(x_i \in \bbS_d\).
We use sans-serif capitals, \ie, \(\var{Y}\), to denote a random variable.

\subsection{Measures of Dispersion}
\label{sec:dispersion-measures}
To measure the dispersion of the set of embeddings $X$ on the unit hypersphere, we consider two different metrics.
\paragraph{Minimum distance.} 
Dispersion requires that no two points be too close, so following ~\cite{zhou2022learning} we employ a minimum distance metric:
\begin{equation}
    \label{eq:mindist}
    \dmin(X) = \min_{x_i,x_j \in X, i \neq j}d(x_i,x_j),
\end{equation}
where $d(x_i,x_j)$ is the geodesic distance from~\Cref{sec:notation}.
\paragraph{Spherical variance.} Spherical variance~\citep{alma999052553502466,mardia1975statistics} originates from directional statistics and is defined for finite \(X \subseteq \bbS_d\) as 
\begin{equation}
    \label{eq:svar}
    \operatorname{svar}(X) = 1-\overline{R},~\text{where}~ \overline{R} = 1/n \sum_i x_i.
\end{equation}
Spherical variance is a key quantity in the Raleigh test for uniformity on the hypersphere \(\bbS_d\)~\citep[p. 206--208]{mardia-1999}, which uses 
\((d + 1) n \overline{R}^2\) as test statistic.

The presented dispersion measures offer complementary perspectives of the dispersion of the embeddings, but are insufficient when considered in isolation.
The minimum distance only depends on the two closest embeddings: embeddings can be spread out in a near perfect configuration, whilst having a minimum distance close to zero.
Similarly, large spherical variance does not imply well dispersed embeddings (consider embeddings clustered around two antipodes.)
In addition, neither method is well-suited for gradient optimization.
The gradient of \(\dmin\) depends only on the closest pair of points 
and would lead to impractically slow algorithms.
As for spherical variance,
since the Euclidean gradient of $\overline{R}$ is orthogonal to the surface of the hypersphere \(\bbS_d\), its Riemannian gradient is null. Similarly to spherical variance, the Raleigh test cannot be used as minimization objective to disperse embeddings.

There are many other ways to measure dispersion~\citep{marbut2023reliable}, but in the scope of this work we focus on two described above simple metrics. 

\subsection{Pairwise Measures for Dispersion}
Using pairwise distances for dispersion on the hypersphere has been long of interest for the machine learning community ~\citep{sablayrolles2018spreading,mettes2019hyperspherical,wang2020mmadispersion,Trosten-noHub-2023,liu2018learning,pmlr-v130-liu21d}. All these works use pairwise-based as a backbone for their objectives, which leads to quadratic complexity and require calculating a matrix of pairwise distances. 
\paragraph{Max-Min Distance.}
To achieve better dispersion on hypersphere, a variety of works focus on maximizing minimum distance (or equivalently minimizing maximum pairwise similarity)~\citep{mettes2019hyperspherical,wang2020mmadispersion, pmlr-v130-liu21d}. In this case, for each embedding vector, only its nearest neighbor and the embedding itself are updated. The regularizer takes the form:
\begin{equation}
    \label{eq:min-max-d}
    \mathcal{L}_{\text{Max-Min}}(X) = -\frac{1}{n} \sum_{i=1}^n \min_{j \neq i} d(x_i,x_j),
\end{equation}
where $d$ can be the cosine distance~\citep[MMCS,][]{mettes2019hyperspherical}, the geodesic distance~\citep[MMA,][]{wang2020mmadispersion}, or the Euclidean distance~\citep{pmlr-v130-liu21d}.

\paragraph{Differential Entropy Dispersion.}
Using maximum entropy regularization is a known technique in machine learning, usually applied to prevent overly-peaked discrete predictive distributions \citep{meister-etal-2020-generalized,ahmed2019understanding,pereyra2017regularizing,NEURIPS2018_e44fea3b} %
\citet{sablayrolles2018spreading} proposed to extend this idea for the continuous space, and directly maximize differential entropy on hypersphere. To this end, they propose to use Kozachenko-Leonenko estimator~\citep{kozachenko1987sample}
\begin{equation}
    \label{eq:koleo}
    \mathcal{L}_{\text{KoLeo}}(X) = -\frac{1}{n}\sum_{i=1}^n \log \min_{j \neq i} \|x_i-x_j\|.
\end{equation}
The Kozachenko-Leonenko estimator and the max-min distance are related by the following bound which follows from the Jensen inequality and the fact that $-e^{-t} \leq t-1$:
\begin{equation}
    \mathcal{L}_{\text{Max-Min}}(X) \leq
    -\exp\left(-\mathcal{L}_{\text{KoLeo}}(X)\right) \leq 
    \mathcal{L}_{\text{KoLeo}}(X) - 1.
\end{equation}
\paragraph{MHE.}
Inspired by Thomson problem~\citep{Gautam2013ANA},
\citet{liu2018learning,pmlr-v130-liu21d} proposed to use \textit{minimum hyperspherical energy} (MHE) in order to ensure separation of the points on hypersphere.
\begin{equation}
    \label{eq:mhe}
    \mathcal{L}_{\text{MHE}}(X) = \sum_{i=1}^n \sum_{j=1, j \neq i}^n f_s(\|x_i-x_j\|),
\end{equation}
where $f_s(\cdot)$ is a decreasing real-valued function and $\|\cdot \|$ is an Euclidean distance. ~\citet{liu2018learning,pmlr-v130-liu21d,lin2020regularizing} used $f_s(z) = z^{-s},\ s > 0$, known as the Riesz s-kernel:
\begin{equation*}
    k_s(x_i,x_j)=\begin{cases}
    d(x_i,x_j)^{-s},\ s > 0, \\
    \log \bigl(d(x_i,x_j)^{-1}\bigr), s=0
    \end{cases}
\end{equation*}
where $d$ can be Euclidean or geodesic distance. Riesz s-energy has many applications in various mathematical and physics problems, and connects to the Gaussian kernel through the Laplace transformation~\citep{borodachob2019discrete}.
\paragraph{Uniformity.} \citet{pmlr-v119-wang20k} introduced the \textit{uniformity} measure for representation learning based on pairwise Gaussian potential:
\begin{equation}
    \mathcal{L}_{\text{uniform}}(X) = \log \bbE_{\var{X}} \bbE_{\var{X'}} \left[ k(\var{X}, \var{X'}) \right],
\end{equation}
where $\var{X},\var{X'}$ are independent uniform  $k(\var{X}, \var{X'})$ is the Gaussian or Radial Basis Function (RBF) kernel~\citep{borodachob2019discrete}.  \citet{pmlr-v119-wang20k} showed that this objective is optimized by uniform distribution.
Similarly~\citet{Trosten-noHub-2023} designed $\mathcal{L}_{\text{uniform}}$ and interpret it as a negative entropy on the hypersphere.

Note that the first two measures presented, KoLeo and Max-Min, operate on distances and thus are defined with a minus sign in order to ensure minimizing them increases dispersion, while MHE and uniformity are defined in terms of kernel similarities, and thus need no such negation. 


\section{New Insights Into Optimizing for Dispersion}
\label{sec:our-work}
All objectives discussed in~\Cref{sec:disp-on-hypersphere} are pairwise-based objectives, meaning that they require calculation of the full pairwise distance matrix, which scales poorly with the growth of $N$ and $d$. Moreover, Max-Min and KoLeo consider only the point and its nearest neighbor for each update. We give a new interpretation of the uniformity regularizer discussed in~\Cref{sec:disp-on-hypersphere}, in terms of (squared) MMD. Second, we define Lloyd and Sliced objectives that approximate optimal dispersion without requiring the full pairwise distance matrix. It makes those two objectives more suitable for large-scale parameter optimization.

\subsection{Pairwise regularizers and MMD}

The distribution of perfectly dispersed embeddings is similar to a uniform distribution on the hypersphere. 
Dispersing embeddings can then be seen as minimizing the `distance' between the embedding distribution and the uniform distribution \(\operatorname{Unif}(\bbS_d)\). 
The Raleigh test for uniformity is not well suited for this purpose as discussed in the previous section.
An alternative statistical test for uniformity can be derived from the \textit{maximum mean discrepancy} (MMD), which measures the distance between two probability distributions~\citep{JMLR:v13:gretton12a}.
\Cref{lemma:mmd} implies that the squared MMD between the distribution of the embeddings and the uniform distribution on the sphere can be computed using embeddings only, up to a constant.

\begin{lemma}[\(\boldmath\operatorname{MMD}^2\) and spherical embeddings.]\label{lemma:mmd}
Let \(p\) be any distribution on \(\bbS_d\) and let \(k\) be a kernel 
on \(\bbS_d\) such that \(k(x, y) = f(\DP{x}{y})\) for some function \(f \colon [-1, 1] \to \bbR\). Assume all random variables are independent. 

Up to a normalizing constant \(c \in \bbR\), we have
\[
\operatorname{MMD}^2[p, \operatorname{Unif}(\bbS_d)] = \bbE_{\var{X}, \var{X'} \sim p} \left[ k(\var{X}, \var{X'}) \right] - c.
\]
\end{lemma}

The proof of~\Cref{lemma:mmd}
is deferred to~\Cref{app:mmd-proofs}.
Using the radial basis function kernel \(k(x,y) = \exp{\left( -\lambda \norm{x - y}^2 \right)}\) in the result of \Cref{lemma:mmd}, we see that minimizing the estimated squared MMD of the embeddings and the uniform distribution is equivalent to minimizing
\begin{equation}
    \mathcal{L}_{\text{MMD}}(X) = \frac{1}{n(n-1)} \sum\limits^n_{i=1}\sum\limits^n_{\substack{j=1 \\ i \neq j}} \exp{\left( \gamma \DP{x_i}{x_j} \right)},
\end{equation}
where \(X \subseteq \mathbb S_d\) is a set of \(n\) embeddings and \(\gamma \coloneqq 2 \lambda > 0\). 
The intuition for \(\mathcal{L}_{\text{MMD}}(X)\) is that the embeddings are pushed away from each other when minimizing \(\mathcal{L}_{\text{MMD}}(X)\), thereby improving the uniformity of the embedding distribution.
The parameter \(\gamma\) determines the emphasis on the distance between embeddings, \ie, a larger $\gamma$ results in a larger emphasis on close embeddings. 

The regularizer \(\mathcal L_{\text{MMD}}\) is related to the partial loss function used by~\citet{Trosten-noHub-2023} to disperse image representation embeddings for few shot learning, as well as the energy-based approaches to Tammes and Thompson problem~\citep{Gautam2013ANA,liu2018learning,pmlr-v130-liu21d}.
In particular, the exponential of the energy optimized by \citet{Trosten-noHub-2023,pmlr-v119-wang20k} differs from \(\mathcal{L}_{\text{MMD}}\) by a constant. Our work thus provides a new justification of their objective.

\subsection{Lloyd's Algorithm}
An alternative formulation of dispersion comes from casting maximal dispersion as
\emph{quantization} of a uniform measure. Quantization refers to the problem of
approximating a given measure by an empirical measure supported at a few
centers. When the given measure is uniform over some support set,
the optimal centers are spread out uniformly over the support;
and can be calculated by Lloyd's algorithm~\citep{lloyd1982},
henceforth \emph{Lloyd}, which iteratively moves each centroid to the center of mass of its Voronoi cell.
When the given measure is another empirical measure, quantization is equivalent
to \emph{k-means clustering}.
When the space is Riemannian and not Euclidean, both quantization and clustering
generalize readily with an adequate choice of distance~\citep{lebrigant2019quantization}.
While Lloyd's algorithm and \emph{k}-means are originally batch algorithms,
stochastic gradient versions have been developed~\citep{bottou-bengio-95,sculley2010web},
including, independently, in the Riemannian case~\citep{lebrigant2019quantization}.
In general, given a domain \(\bbD\), which could be a manifold or
a compact subset of one (for
quantization), or a discrete dataset (for clustering), the
\(n\) optimal centroids are a minimizer of
\footnote{More generally, the target measure need not be uniform.
\citet{lebrigant2019quantization} discuss more general conditions for the
existence of a minimizer.}
\begin{equation}
    \mathcal{L}_{\text{Lloyd}}(X) = \bbE_{\var{Y} \sim \operatorname{Unif}(\bbD)}
\left[ \min_{j \in [n]} \frac{1}{2} d^2(\var{Y}, x_j)\right].
\end{equation}
A stochastic gradient of the Lloyd regularizer can be obtained by drawing \(m\) uniform samples on \(\bbD\). Intuitively, each cluster center is pulled toward the barycenter of
the uniform samples assigned to it; an approximation to the true Voronoi
barycenter.

For dispersion on the sphere, we take \(\bbD=\bbS_d\).
While traditionally Lloyd's algorithm
corresponds to minimizing \(\mathcal{L}_{\text{Lloyd}}\) alone,
we propose using \(\mathcal{L}_{\text{Lloyd}}\) as a regularizer to move \(X\) closer to
optimal Voronoi centers of the sphere, while also minimizing some main
task-specific objective.
The complexity of this regularizer is controlled by the number of samples:
For efficiency, \(m\) should be much less than \(n\),
in which case most cluster centers are not updated in an iteration.
However, unlike for MMD, the stochastic gradient takes into
account all of \(X\) through the cluster assignment.

\subsection{Sliced Dispersion}
The previously discussed algorithms are generally applicable to other manifolds.
We now show how using properties of the sphere we may obtain an alternative algorithm for embeddings dispersion. The key idea
is that, while in 2 or more dimensions it is hard to find the
location of \(n\) evenly distributed points, on \(\bbS_1\)
this can be done efficiently:
The following set of angles is one optimal configuration:
\[ \Phi=(\phi_1, \ldots, \phi_n) \quad\text{where}\quad \phi_k = -\pi\frac{n+1}{n} + \frac{2\pi
k}{n}. \]
Any other optimal configuration must be a rotation of this one, \ie \( \tau +
\Phi\) for \(\tau\in (-\pi, \pi)\).
followed by a permutation of these angles.
Given a permutation \(\sigma \in \Pi_n\) denote
\(\Phi_\sigma = (\phi_{\sigma(1)}, \ldots, \phi_{\sigma(n)}).\)
We can then write the set of all possible ordered
optimally-dispersed configurations as
\begin{equation}\label{eq:optimally-dispersed}
D_n \bbS_1 \coloneqq \left \{ \tau + \Phi_\sigma
 \mid
\tau \in (-\pi, \pi), \sigma \in \Pi_n \right \}.
\end{equation}
Given an ordered configuration of angles \(\Theta = (\theta_1, \ldots,
\theta_n) \subset \bbS_1\),
we define its (angular) distance to the maximally-dispersed set
as:
\begin{equation}\label{eq:distance_to_dispersed}
d^2(\Theta, D_n \bbS_1) =
\min_{\hat\Theta \in D_n \bbS_1} \sum_{i=1}^n
\frac{1}{2}(\theta_i-\hat{\theta}_i)^2.
\end{equation}
\Cref{lemma:dispersion} defined and proved in~\Cref{app:sliced-proofs} shows that any configuration of angles can be efficiently
projected to its nearest maximally-dispersed configuration. We defer all proofs in this section to~\Cref{app:sliced-proofs}.

In arbitrary dimensions, a similar construction is not possible, since the
optimal configurations do not have tractable characterizations. We instead 
\emph{slice} a high-dimensional spherical dataset along a great circle;
similar to \citet{bonet2023spherical}. The following result gives the geodesic projection.
\begin{lemma}[Projection onto great circle.]
Let \(p, q \in \bbS_d\) with \(\DP{p}{q}=0\).
Two such vectors determine a unique great circle
\(\bbS_{pq} \subset \bbS_d\) defined by:
\[ \bbS_{pq} \coloneqq \{ \cos(\theta) p + \sin(\theta) q \mid -\pi \leq \theta < \pi \}
\simeq \bbS_1.\]
The nearest point on \(\bbS_{pq}\) to a given \(x \in \bbS_d\) is:
\begin{equation}
\operatorname{proj}_{{\bbS}_{pq}} (x)
=\operatorname{arctan2}\left(\DP{x}{q}, \DP{x}{p}\right).
\end{equation}
\end{lemma}

A well-dispersed configuration over \(\bbS_d\) should remain
fairly well-dispersed along any slice on average.
If we denote \(\operatorname{proj}_{\bbS_{pq}}(X)\!\coloneqq\!(
\operatorname{proj}_{\bbS_{pq}}(x_1), \ldots
\operatorname{proj}_{\bbS_{pq}}(x_n))
\),
we may capture this intention by
the following \textbf{sliced dispersion regularizer}:
\begin{equation}
\mathcal{L}_{\text{Sliced}}(X) =
\bbE_{p,q} \left[ d^2(
\operatorname{proj}_{\bbS_{pq}}(X),
D_n\bbS_{pq})
\right],
\end{equation}
where \(d^2\) is defined in \cref{eq:distance_to_dispersed},
and the expectation is over orthogonal pairs \(p,q\).
Note that unlike algorithms such as principal geodesic analysis \citep{fletcher2004principal}, which keep $X$ fixed but optimize for some $p,q$ to maximize variance, our intuition is the opposite: we want to update $X$ in order to increase dispersion along \emph{any} great circle.
The following proposition efficiently computes stochastic gradients of $\mathcal{L}_{\text{Sliced}}$.
\begin{proposition}
Denote \(\theta_i^{pq} = \operatorname{proj}_{\bbS_{pq}}(x_i)\),
and \({\hat{\theta}^\star_i}^{pq}\) the corresponding dispersion maximizer
computed using Lemma
3. The Riemannian gradient of $\mathcal{L}_{\text{Sliced}}$ is given by:
\begin{equation*}
\operatorname{grad}_{x_i}\!\mathcal{L}_{\mathrm{Sliced}}(X)=
\bbE_{p,q} \left[ ({{\theta}_i}^{pq} - {\hat{\theta}^\star_i}^{pq})
\frac{\DP{x_i}{p} q - \DP{x_i}{q} p}{\DP{x_i}{q}^2 + \DP{x_i}{p}^2}
\right].
\end{equation*}
\end{proposition}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/sliced_update_fixed.png}
    \caption{Visualization of a single update in sliced dispersion, for a great circle $\bbS_{pq}$. Sliced dispersion maximizes dispersion in expectation over all great circles.}
    \label{fig:enter-label}
\end{figure}

\subsection{Riemannian Optimization on Hypersphere}
\label{sec:why-riemann}
Optimization for dispersion can be defined as constrained optimization problem in $\mathbb{R^{d}}$, where constraint is that points lie on the hypersphere. This can be solved by ignoring spherical constrains and projecting the parameters onto the sphere after the gradient update, however \textit{convergence is not guaranteed}, because the sphere is not a convex set, even though it can give acceptable results with careful initialization~\citep{raman2019optimizationsurfacehypersphere}. Alternatively, we can rely on Riemannian optimization~\citep{bonnabel2013stochastic,becigneul2018riemannian} in $\mathbb{S}^{d-1}$ as effective \textit{unconstrained} extension~\citep{Bloch_2015,boumal2023intromanifolds} with guaranteed convergence~\citep{bonnabel2013stochastic}. We further empirically explore the convergence of both methods in~\Cref{app:rgradVSegrad}.


\section{Applications}
We demonstrate the application of dispersion objectives and provide a comparative analysis on both synthetic and real-world tasks. Unlike previous studies, we employ Riemannian optimization~\citep{bonnabel2013stochastic, becigneul2018riemannian} directly on the hypersphere using \texttt{geoopt}\footnote{\url{https://github.com/geoopt/geoopt}}~\citep{geoopt2020kochurov}, instead of relying on projection onto the hypersphere at each gradient step as discussed in ~\Cref{sec:why-riemann}. 
\subsection{Tammes problem}
\label{applications:Tammes}
We evaluate the dispersion methods introduced in ~\Cref{sec:disp-on-hypersphere} and ~\Cref{sec:our-work} by verifying that they can approximate the known solution to the Tammes problem for $N=24$ in three dimensions~\citep{ROBINSON1961}, by considering the minimum angle between points of the optimal configuration.
Uniformly sampled points are dispersed using the regularizers described in ~\Cref{sec:disp-on-hypersphere} and ~\Cref{sec:our-work}. Optimization is done with Riemannian Adam for 2.5k epochs. The MMD regularizer was minimized with \(\gamma = 25\). The sliced dispersion regularizer used a single randomly generated pair of axes during each epoch. The Lloyd regularizer was used with 300 samples. We set $s=0$ for MHE. All regularizers were used with learning rate \(5 \cdot 10^{-3}\).
\begin{figure}[t]
    \centering
        \centering
        \includegraphics[width=.8\linewidth]{images/tammes-new/tammes_n24_boxplot_new.pdf}
    \caption{Minimum angles (degrees) for each of the N=24 points with respect to optimization methods. \texttt{Optimal Solution} shows the angle for known optimal solution. \texttt{Rand.Init.} represents the points generated uniformly at random on the surface of the sphere. All optimizations start with the \texttt{Rand.Init.} as an initialization. Optimal minimum angle is equal to 48.53529763$^{\circ}$. An ideal configuration is achieved when all angles are equal to optimal angle.}
    \label{fig:tammes-131424}
\end{figure}

The minimum angles of the points distributed using the MMD, MMA and KoLeo regularizers are close to the optimal minimum angle for all presented $N$ as shown in \Cref{fig:tammes-131424}. The Lloyd and MHE regularizers follows closely, but seems to approximate the solutions less accurately. 
The sliced dispersion regularizer, however, seems to approximate the solutions worse than the other regularizers. More results on Tammes problem approximation can be found in ~\Cref{app:tammes}.

\subsection{Synthetic Embeddings}
In practice, we are mostly interested in dispersion of large amount of points in dimension $d\gg3$. Text embeddings can be a particular example of the set of points that can benefit from dispersion~\citep{tokarchuk-niculae-2024-unreasonable}. One can argue that dispersion connects strongly to the dimensionality, and in higher dimension embeddings are dispersed naturally. However, higher dimensionality comes with higher computation and memory cost. Also, there is no guarantee that space is occupied efficiently. Thus, ~\cite{gao2018representation} showed that representation in vanilla Transformer~\cite{Vaswani-trafo} occupies only part of the whole space. We evaluate the behaviour of the regularizers discussed in~\Cref{sec:disp-on-hypersphere} with synthetic embedding by generating matrix containing 20k embeddings in $d=128$. The data was generated by sampling a matrix entry-wise from a \texttt{PowerSpherical}\citep{decao2020power} distribution with $\kappa$ equal to 100. This exemplifies a scenario where the embeddings are well spread out from the beginning. The regularizers were minimized using Riemannian Adam~\citep{kingma_adam, becigneul2018riemannian, geoopt2020kochurov}, for 5k iterations with learning rate \(1 \cdot 10^{-3}\). We set $\gamma$ of the MMD regularizer to 10.0, number of samples for Lloyd to 8192. Due to the hardware constraints we implement batched version of MHE and MMA, and use batch size equal to 16K. We set $s=0$ for MHE. We also rely on the batched version of axis-aligned Sliced regularizer with batch size equal to 128. 


\Cref{fig:compare-regs} shows the minimum distance and circular variance for various regularizers. KoLeo and MMA performs the best in terms of minimum distance, with MMD being second best. MMD and MHE reach the highest circular variance, followed closely by MMA and KoLEo. It it important to note, that reaching the best minimum distance and/or circular variance does not necessarily mean the best performance on the downstream task. The trade-off between performance and dispersion should be considered for each particular case.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/min_dist_var_allreg.pdf}
    \caption{Comparison of different dispersion objectives on synthetic data.}
    \label{fig:compare-regs}
\end{figure}


\subsection{Image Classification with Prototypes}

\begin{table}[ht]
    \centering
    \begin{tabular}{l cccccc}
    \toprule
        prototypes & \multicolumn{2}{c}{50} & \multicolumn{2}{c}{100} & \multicolumn{2}{c}{200}  \\ 
        & Acc. & $\dmin$ & Acc. & $\dmin$ & Acc. & $\dmin$ \\
        \midrule
        MMCS (+projection) & 41.67 &1.22&42.76&1.36 &43.03&1.44 \\ \midrule
        MMCS  ($\bbS$) & 42.59 &1.46& 42.96&1.52& \textbf{43.27} &1.56\\   
        MMA ($\bbS$)& 41.72 & 1.39 & \textbf{43.47} &1.46& 42.90 & 1.51\\
        MHE ($\bbS$)&43.37& 1.41 &42.25&1.6 &34.47&1.58\\
        KoLeo ($\bbS$)&41.78   &1.37&43.12&1.44&42.37&1.49\\ \midrule
        MMD ($\gamma=1, \bbS$) & \textbf{43.87} &1.22&42.73&1.57&34.53& 1.58\\
        Lloyd (samples=200, $\bbS$) & 41.69 &1.20&42.42&1.30&43.09&1.35 \\
        Sliced  ($\bbS$) & 40.76 &1.10&42.34&1.20&42.92&1.33 \\
         \bottomrule
    \end{tabular}
    \caption{ImageNet-200 classification accuracy. Prototypes are trained with different separation conditions. \texttt{MMCS} refers to the setup of ~\citet{mettes2019hyperspherical}. In bold we emphasise the best accuracy in a column.}
    \label{tab:hpn-net}
\end{table}

\citet{mettes2019hyperspherical} showed that learning prototypes with dispersion encouraged by minimizing the maximum cosine similarity (MMCS) on hypersphere improves classification results on ImageNet-200. We first show in \Cref{tab:hpn-net} that applying Riemannian optimization rather than re-normalizing parameters after each gradient update as in~\citet{mettes2019hyperspherical} leads to the better class separation, and as a result better classification accuracy. Second, we compare the classification accuracy given the prototypes trained with different dispersion objectives discussed in ~\Cref{sec:disp-on-hypersphere,sec:our-work}. We use unconstrained optimization on the sphere for all methods, and results with projection is shown only for comparison. Also, \Cref{tab:hpn-net} shows that when prototypes dimension is equal 50, MMD performs the best among all dispersion objectives, even though the minimum distance is smaller compared to other pairwise-distance based objectives. It proves that even though we can measure the dispersion using minimum distance, we cannot rely on this metric alone as a predictive factor of the downstream task accuracy.

Interestingly, when dimensionality is equal to the number of points, MMD and MHE prototypes results degrade significantly. For both MMD and MHE minimum distance and median distance are equal to exactly 1.5758213996887207 radian or $90.3^\circ$, which resembles orthogonal solution. Since the network is trained with the squared cosine distance, when angle between two points is $90^\circ$, the distance is equal to exactly 1 to all possible prototypes, which makes the loss less informative, 
and \citet{mettes2019hyperspherical} suggest that
that one-hot embeddings, while being optimally dispersed, 
perform poorly on the task at hand.



\subsection{Neural Machine Translation}
Embeddings learned with the vanilla transformer model~\citep{Vaswani-trafo} are known for their inefficiency in utilizing space effectively, leading to the collapse of token representations~\citep{gao2018representation,Wang2020Improving}. This issue is particularly pronounced for rare tokens~\citep{gong2018frage,tokarchuk-niculae-2024-unreasonable,Zhang2022Frequency}. ~\citet{gong2018frage} proposed to alleviate the problem of rare tokens by learning frequency-agnostic embeddings, while~\citet{Zhang2022Frequency} proposed to use contrastive learning. In our approach, we tackle this challenge by focusing on the concept of dispersion. Specifically, we train a Neural Machine Translation (NMT) system and jointly optimize the transformer weights \(W\) along with the decoder embeddings \(X\) to enhance their dispersion. 
\begin{equation}
    \mathcal{L}(W,X)= \mathcal{L}_{\text{MT}}(W,X)+\lambda \mathcal{L}_{\text{disp}}(X),
\end{equation}
where \(\mathcal{L}_\text{MT}\) is a standard conditional log-likelhood loss of the target sentence given the source sentence accumulated over a dataset \citep[\eg,][]{Bahdanau-attention}.
We report results on two WMT translation tasks\footnote{\url{https://www2.statmt.org/}}: WMT 2016 Romanian$\rightarrow$English (\langpair{ro}{en}) with 612K training samples and WMT 2019 English$\rightarrow$German (\langpair{en}{de}) with 9.1M training samples (including back-translated data). We measure translation accuracy on the best checkpoint according to validation BLEU score using SacreBLEU~\citep{papineni-etal-2002-bleu,post-2018-call} and COMET~\citep{rei-etal-2020-comet}. Detailed training parameters are discussed in~\Cref{app:nmt-setup}.

\Cref{tab:discrete_nmt_results} shows the BLEU and COMET results on newstest2016 for \langpair{ro}{en} and newstest2016 \langpair{en}{de} along with the dispersion metrics. Similarly to image classification, doing Riemannian optimization in order to disperse embeddings leads to better dispersion and higher BLEU and COMET scores.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccccccccc}
    \toprule
              & \multicolumn{4}{c}{\langpair{ro}{en}} && \multicolumn{4}{c}{\langpair{en}{de}}\\
       \textbf{model}  &  BLEU & COMET & $\dmin$ & $\operatorname{svar}$ && BLEU & COMET & $\dmin$&$\operatorname{svar}$\\ \midrule
       euclidean baseline & 31.4 & 0.790 & 0.003 &0.19 && 33.1&0.819&0&0\\ \midrule
    spherical baseline  & 32.2 &0.793&0.001 &0.57 & & 33.7&\textbf{0.825}&0.001&0.408\\ 
    \quad +MMD & 32.3&\textbf{0.795}&0.001& 0.56          && \textbf{33.9}&\textbf{0.825}&0.001&0.410\\
    \quad +Lloyd & \textbf{32.4}&0.791&0.001&0.60         && 33.4&0.822&0.001&0.414\\
    \quad +Sliced & \textbf{32.4} & \textbf{0.795} & 0.435& 0.99    && 33.5&0.820&0.222&0.999\\
       \bottomrule
    \end{tabular}%
    \caption{newstest2016 \langpair{ro}{en} and \langpair{en}{de} results on discrete NMT. Embeddings are 128 dim.}
    \label{tab:discrete_nmt_results}
\end{table*}
We investigate the effect of Riemannian optimization by analyzing the gradient norm of the Euclidean baseline (vanilla transformer) and the Spherical baseline, as shown in ~\Cref{fig:gradient-norm-roen}, alongside the minimum pairwise distance for each embedding, presented in ~\Cref{fig:min-dist-roen}. The results reveal that the gradient norm for the Riemannian approach is approximately ten times higher than that of the Euclidean baseline. We hypothesize that this increased gradient norm contributes to better dispersion of rare tokens, thereby mitigating representation collapse. They dynamics of gradient norms and minimum distances can be seen in~\Cref{app:grad-norms}.

\begin{figure}
    \centering
    \begin{subfigure}[]{0.49\textwidth}
        \includegraphics[width=\linewidth]{images/grad_norm_40000.pdf}
        \caption{Gradient norms of the embeddings for trained (step equal 40000) Spherical and Euclidean NMT baselines. Frequency rank refers to the position of the token in the vocubulary, where most frequent token has rank 0 and lest frequent rank vocabulary size.} 
        \label{fig:gradient-norm-roen}
    \end{subfigure}\hfill
    \begin{subfigure}[]{0.49\textwidth}
        \includegraphics[width=\linewidth]{images/dist_40000.pdf}
        \caption{Minimum distance to the nearest embeddings for trained (step equal 40000) Spherical and Euclidean baselines. Frequency rank refers to the position of the token in the vocubulary, where most frequent token has rank 0 and lest frequent rank vocabulary size.} 
        \label{fig:min-dist-roen}
    \end{subfigure}
    \caption{Embeddings matrix gradient norms (a) and minumum distances (b) for Euclidean and Spherical baselines.}
\end{figure}

\section{Continuous-Output Neural Machine Translation}
Continuous-output NMT \citep[CoNMT,][]{kumar2018von}
reformulates machine translation as
a sequential continuous regression problem of predicting the embedding of the next word, instead of the more usual discrete classification formulation.
\citet{tokarchuk-niculae-2024-unreasonable} recently showed that dispersion plays an important role and greatly impacts performance.
We follow closely their setup and apply the dispersion regularizers in order to achieve dispersion. Pre-trained embeddings come from the well-trained discrete model. We present results for WMT 2016 \langpair{ro}{en} with 612k training samples. \Cref{tab:conmt_results} shows the BLEU score results on \texttt{newstest2016} for CoNMT models with different target embeddings $\mathbf{E_Y}$, alongside dispersion measures defined in~\Cref{sec:dispersion-measures}

We conduct two types of experiments. First we train a vanilla transformer model~\citep{Vaswani-trafo}. Resulting embeddings are in Euclidean space, so we project it onto the sphere by dividing to the norms of embeddings. To spread out the embeddings we then use Riemannian optimization on the sphere with \texttt{geoopt}~\citep{geoopt2020kochurov} using three different regularizers. We refer to this as `offline' methods in \Cref{tab:conmt_results}. Second, we train transformer model with embeddings explicitly modeled to be on the sphere using Riemannian optimization. In this case, we can apply dispersion regularizers directly during optimization. Discrete models that were used to extract embeddings are the same as in ~\Cref{tab:discrete_nmt_results}.
\begin{table}
    \centering
   \resizebox{8.0cm}{!}{%
    \begin{tabular}{lccc}    
    \toprule
       \textbf{Tgt. Emb. $\mathbf{E_Y}$}  & $\operatorname{svar}(\mathbf{E_Y})\uparrow$ & $\dmin(\mathbf{E_Y})\uparrow$ & BLEU$\uparrow$\\ \midrule
       euclidean (proj.) & 0.191 & 0.014 &27.8 \\ \midrule
       \quad +offline MMD & 0.599& 0.372 & 29.7\\ 
      \quad +offline Lloyd & 0.585&0.004& 27.7  \\ 
      \quad +offline Sliced & 0.979 & 0.106 &29.6 \\ \midrule
       spherical  & 0.57 & 0.001& 29.9\\ 
    \quad +MMD &0.56 & 0.001 & 30.0 \\
    \quad +Lloyd & 0.60 & 0.001 & \textbf{30.1} \\
    \quad +Sliced & \textbf{0.99} & \textbf{0.435} &30.0 \\ \bottomrule
    \end{tabular}%
   }
    \caption{Impact of the dispersion of the target embeddings on the CoNMT results. We report BLEU scores on the \texttt{newstest2016} for \langpair{ro}{en}. Beam size is equal to 5.}
    \label{tab:conmt_results}
\end{table}

Spreading out the projected embeddings results into the BLEU score improvement with MMD and Sliced dispersion. For all dispersion regularizers, we can see that $\operatorname{svar}(\mathbf{E_Y})$ is increasing. However,  $\dmin(\mathbf{E_Y})$ decreases for the Lloyd regularizer, which seemingly also impacts the BLEU score. 

When adding dispersion regularizers,
there are no significant fluctuations in $\operatorname{svar}(\mathbf{E_Y})$, except for the Sliced regularizer. We leave thorough investigation of the observed behaviour for the future work.

\section{Additional Related Work}
\label{sec:relwork}

A prominent related but distinct construction is that of 
\textbf{determinantal point processes} \citep{kulesza_determinantal_2012},
often used to ensure diversity or dispersion in extractive models. Given a 
set of $n$ points, in the scope of our paper we study optimizing the locations of the points; in contrast, DPPs are intended for selecting subsets of $k \leq n$ such points in a way that encourages dispersion of the selected points. Technically, DPPs induce a probability distribution over all subsets. Like the pairwise regularizers we study in \Cref{sec:disp-on-hypersphere}, DPPs form an $n \times n$ kernel matrix of pairwise similarities. In order to draw samples, or to evaluate the probability of a given subset, an eigendecomposition (or equivalent) of the kernel matrix is necessary, leading to $O(n^3)$ time complexity for such operations. Maximization, \ie, finding an optimally-dispersed subset under a DPP, is NP-hard.

Dispersion is also closely connected to \textbf{contrastive learning}~\citep{chen2020simple,he2020momentum,hjelm2018learning,chen2020big}, where model outputs corresponding to different classes are pushed away from each other. ~\citet{pmlr-v119-wang20k} in particularly showed that widely used contrastive learning objective can be interpreted in terms of ``alignment'' (similar features for similar samples) and ``uniformity'' (feature distribution is close to uniform distribution). In our work we focus on parameter dispersion, which can more easily be quantified.

\section{Conclusion}
In this work, evaluate several dispersion objectives on the hypersphere, including one that is equivalent to the widely used maximum mean discrepancy (MMD) method, as well as two novel approaches: Lloyd and Sliced. We compare these objectives against various pairwise distance-based methods previously explored in the literature. Our experimental results show that these methods can approximate the Tammes problem solution, and also allow improvement on few-shot image classification with prototypes, machine translation and the CoNMT task, which uses cosine distance both for training and decoding. 




\subsubsection*{Acknowledgments}
This work is supported by the Dutch Research Council (NWO) via VI.Veni.212.228. The authors also thank SURF (www.surf.nl) for the support in using the National Supercomputer Snellius. Finally, we thank all the members of the UvA Language Technology Lab for their valuable feedback on our work, with special thanks to Sergey Troshin for his input on the manuscript.

\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}
\subsection{MMD Dispersion: Proofs}\label{app:mmd-proofs}
\subsubsection{\(\operatorname{MMD}^2\) and spherical embeddings: Proof of \cref{lemma:mmd}}
The squared MMD of two probability distributions \(p\) and \(q\) is equal to~\citep[Lemma 6]{JMLR:v13:gretton12a}
\[
\operatorname{MMD}^2[p, q] = \bbE_{\var{X}, \var{X'} \sim p}[k(\var X, \var X')] - 2\bbE_{\var{X} \sim p, \var{Y} \sim q} [k(\var X, \var Y)] + \bbE_{\var{Y}, \var{Y'} \sim q}[k(\var Y, \var Y')].
\]

We show that the last two expectations are constant, when \(p\) is a distribution on the hypersphere \(\bbS_d\) and \(q\) is \(\operatorname{Unif}(\bbS_d)\).
Let \(z, z' \in \bbS_d\) and let \(Q\) be a rotation matrix such that \(Qz = z'\). 
Note that \(\var{Y} \sim \operatorname{Unif}(\bbS_d)\) if and only if \(Q^\top \var{Y} \sim \operatorname{Unif}(\bbS_d)\), and \(\DP{Qz}{z} = \DP{z}{Q^\top z}\). 
It then follows that 
\[
\bbE_{\var{Y} \sim \operatorname{Unif}(\bbS_d)}[k(z, \var{Y})] = \bbE_{\var{Y} \sim \operatorname{Unif}(\bbS_d)}[k(z', \var{Y})],
\]
since \(k(x,y) = f(\DP{x}{y})\).
Hence, there exists a \(c \in \bbR\) such that for all \(z \in \bbS_d\) we have 
\[
\bbE_{\var{Y} \sim \operatorname{Unif}(\bbS_d)}[k(z, \var{Y})] = c.
\]
Consequently, \(\bbE_{\var{X} \sim p, \var{Y} \sim \operatorname{Unif}(\bbS_d)} [k(\var{X}, \var{Y})] = c\) and \(\bbE_{\var{Y}, \var{Y'} \sim \operatorname{Unif}(\bbS_d)}[k(\var{Y}, \var{Y'})] = c\). 
The desired result follows immediately.

\subsection{Sliced Dispersion: Proofs}\label{app:sliced-proofs}
\subsubsection{Optimal 1-d Dispersion}

\begin{lemma}{Optimal 1-d dispersion.}\label{lemma:dispersion}
The projection
\[ \argmin_{\hat\Theta \in D_n \bbS_1}
\sum_{i=1}^n \frac{1}{2}(\theta_i - \hat\theta_i)^2\]
is given by
\(\hat\theta^\star_i = \tau^\star + \phi_{\sigma^{-1}(i)}\),
where
\(\sigma\) is the permutation s.t. \(\theta_{\sigma(1)} \leq \theta_{\sigma(2)}
\leq\ldots \leq \theta_{\sigma(n)}\),
and \(\tau^\star = \frac{\sum_i \theta_i}{n}\).
The projection can be calculated in \(O(n \log n)\),
the dominating cost being sorting the angles.
\end{lemma}

We aim to prove the assertion that
the projection
\[ \argmin_{\hat\Theta \in D_n \bbS_1}
\sum_{i=1}^n \frac{1}{2}(\theta_i - \hat\theta_i)^2\]
is given by
\(\hat\theta^\star_i = \tau^\star + \phi_{\sigma^{-1}(i)}\),
where
\(\sigma\) is the permutation st \(\theta_{\sigma(1)} \leq \theta_{\sigma(2)}
\leq\ldots \leq \theta_{\sigma(n)}\),
and \(\tau^\star = \frac{\sum_i \theta_i}{n}\).

By definition, per \cref{eq:optimally-dispersed},
\(\hat\Theta = \tau + \Phi_\sigma\) and thus we may write the problem
equivalently as
\[
\argmin_{\tau \in [-\pi,\pi), \sigma \in \Pi_n}
\sum_i \frac{1}{2} \left(\theta_i - \phi_{\sigma(i)} - \tau\right)^2.
\]

\paragraph{Finding the permutation.}
 In terms of \(\sigma\) the objective takes the form \(-\sum_i
\theta_i \phi_{\sigma(i)} + \text{const}\),
so we must find the permutation that maximizes \(
\sum_i \theta_i \phi_{\sigma(i)} =
\sum_i \theta_{\sigma^{-1}(i)} \phi_i\).
By the
rearrangement inequality
\citep[Thms.~368--369]{hardy1952inequalities},
since \(\phi_i\) is in ascending order,
this sum is maximized when \(\theta_{\sigma^{-1}(i)}\) is in ascending order;
so the optimal \(\sigma\) must be the inverse of the permutation that sorts
\(\Theta\).

\paragraph{Finding \(\tau\).}
Ignore the constraints momentarily, and set the gradient of the objective to zero:
\[\frac{\partial}{\partial \tau}
\sum_i \frac{1}{2} ( \theta_i - \phi_{\sigma(i)} - \tau)^2
= \sum_i (\tau + \phi_{\sigma(i)} - \theta_i) = 0,
\quad\text{implying}\quad n\tau = \sum_i \theta_i - \sum_i \phi_i = \sum_i
\theta_i,
\]
the last equality by choice of the zero-centered reference configuration
\(\Phi\). Since all \(\theta_i \in [-\pi, \pi)\), so is their average, and thus
the constraints are satisfied, concluding the proof.

\subsubsection{Projection onto a great circle}

The projection we seek to compute is
 \[
\proj_{\bbS_{pq}}(x) \coloneqq \argmin_{-\pi \leq \theta < \pi} d^2((\cos(\theta) p + \sin(\theta) q, x).
\]
Since the geodesic distance satisfies
\(d^2(\cdot, \cdot) = \arccos \DP{\cdot}{\cdot}\)
and \(\arccos\) is strictly decreasing on \((-1, 1)\),
we have
\[\proj_{\bbS_{pq}}(x) \coloneqq \argmax_{-\pi \leq \theta < \pi}
\DP{\cos(\theta) p + \sin(\theta) q}{x}.\]
As a side note, this shows that it doesn't matter whether we use geodesic or
Euclidean distance to define this projection.
Setting the gradient to zero yields
\[ \cos(\theta)\DP{q}{x} = \sin(\theta) \DP{p}{x}, \]
or equivalently \(\tan(\theta) = \DP{q}{x}/\DP{p}{x}\).
The unique solution on \([-\pi, \pi)\) is given by the two-argument arctangent function (arctan2), also known as the argument of complex number \(\DP{p}{x} + i\DP{q}{x}\) \citep{atan}.

\subsubsection{Gradient of sliced distance}
We first compute the Euclidean gradient of the desired expression:
\begin{equation}
\nabla_{x_i} \mathcal{L}_{\text{Sliced}}(X) =
\nabla_{x_i} \bbE_{p,q} \left[ d^2(
\operatorname{proj}_{\bbS_{pq}}(X),
D_n\bbS_{pq})
\right].
\end{equation}
First, by writing
\[d^2(\Theta, D_n \bbS_{pq}) =
\min_{\hat\Theta} \sum_i \frac{1}{2} (\theta_i - \hat\theta_i)^2\]
we see this may be interpreted as an Euclidean projection and
\[
\frac{\partial}{\partial \theta_i}
d^2(\Theta, D_n \bbS_{pq}) =
(\theta_i - \theta_i^\star).\]
But \(\theta_i = \proj_{\bbS_{pq}}(x_i)\) and we can write
\[
\begin{aligned}
\frac{\partial \theta_i}{\partial x_i}
&=
\frac{\partial}{\partial x_i}
\proj_{\bbS_{p,q}}(x_i) \\ &=
\frac{\partial \theta_i}{\partial x_i}
\tan^{-1}\left(\frac{\DP{q}{x}}{\DP{p}{x}}\right) \\
&=
\frac{\DP{p}{x} q - \DP{q}{x} p}{\DP{q}{x}^2 + \DP{p}{x}^2}.
\end{aligned}
\]
Putting the two together via the chain rule yields
\begin{equation}
\nabla_{x_i} \mathcal{L}_{\text{Sliced}}(X) =
(\theta_i^{pq} - {\hat{\theta}_i^\star}^{pq})
\frac{\DP{p}{x_i} q - \DP{q}{x_i} p}{\DP{q}{x_i}^2 + \DP{p}{x_i}^2}.
\end{equation}
Notice that the second term is a vector in
\(\bbR^{d+1}\) that is orthogonal to \(x_i\) because:
\[\DP{x_i}{\DP{p}{x_i} q - \DP{q}{x_i} p}
= \DP{p}{x_i}\DP{q}{x_i} - \DP{q}{x_i} \DP{p}{x_i}
=0.\]
Therefore,
\[ \operatorname{grad}_{x_i} \mathcal{L}_{\text{Sliced}}(X) = \nabla_{x_i} \mathcal{L}_{\text{Sliced}}(X). \]

\subsection{Convergence of the sliced regularizer}
Figure~\Cref{fig:sliced-conv} shows that with the approximately 1000 samples Sliced regularizer reaches convergence.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{images/convergence_sliced.pdf}
    \caption{Convergence of the sliced regularizer.}
    \label{fig:sliced-conv}
\end{figure}

\section{Riemannian vs Euclidean optimization}
\label{app:rgradVSegrad}
\subsection{Tammes Problem}
We compare the results of optimal angle approximation using constrained optimization in $\mathbb{R}^d$ with projection~\Cref{fig:tammes-eucl-24} and unconstrained Riemannian optimization in $\mathbb{S}^{d-1}$~\Cref{fig:tammes-riemann-24}. We perform optimization with the same parameters in both cases which identical to parameters described in ~\Cref{applications:Tammes}. We exclude Sliced from the comparison since in both cases custom Riemannian gradient is calculated. However, for all other methods except KoLeo we can clearly see that optimization in $\mathbb{R}^d$ fails to converge to the (sub)-optimal solution compared to unconstrained optimization in $\mathbb{S}^{d-1}$.

\begin{figure}
    \begin{subfigure}{\columnwidth}
    \label{fig:tammes-riemann-24}
        \centering
        \includegraphics[width=\linewidth]{images/tammes-new/tammes_n24_boxplot_new.pdf}
        \caption{Unconstrained optimization in  $\mathbb{S}^{d-1}$.}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
    \label{fig:tammes-eucl-24}
        \centering
        \includegraphics[width=\linewidth]{images/tammes-eucl/tammes_n24_eucl_boxplot.pdf}
        \caption{Constrained optimization in $\mathbb{R}^d$ (projection).}
    \end{subfigure}
    \caption{Minimum angles (degrees) for each of the N=24 points with respect to optimization methods. \texttt{Optimal Solution} shows the angle for known optimal solution. \texttt{Rand.Init.} represents the points generated uniformly at random on the surface of the sphere. All optimizations start with the \texttt{Rand.Init.} as an initialization. Optimal minimum angle is equal to 48.53529763$^{\circ}$. Ideal configuration is achieved when all angles equal to optimal angle, \ie, lie on the optimal angle line. (a) refers to the Unconstrained optimization in  $\mathbb{S}^{d-1}$, while (b) show results for Constrained optimization in $\mathbb{R}^d$ (projection).}
\end{figure}

\section{Tammes Problem: Additional Results}
\label{app:tammes}
In we present additional approximation results for Tammes problem for $N=(13,14,128)$. For N=13 and N=14 we compare with the theoretically proven solutions~\citep{musin-2012,tammes-problem-for-n14}, for N=128 we use numerical solution~\citep{table-spherical-codes}.

\begin{figure}[ht]
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/tammes-new/tammes_n13_boxplot_new.pdf}
        \caption{N=13 points, optimal minimum angle is equal to 57.1367031$^{\circ}$}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/tammes-new/tammes_n14_boxplot_new.pdf}
        \caption{N=14 points, optimal minimum angle is equal to 55.6705700$^\circ$}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=.9\linewidth]{images/tammes-new/tammes_n128_boxplot_new.pdf}
        \caption{N=128 points, optimal minimum angle is equal to 18.6349726$^\circ$}
    \end{subfigure}
\caption{Minimum angles (degrees) distributions for various points arrangements with d=3 and N=(13,14,128). \texttt{Optimal Solution} shows the angle for known optimal solution. \texttt{Rand.Init.} represents the points generated uniformly at random on the surface of the sphere. All optimizations start with the \texttt{Rand.Init.} as an initialization.}
\end{figure}
\section{Neural Machine Translation: Experimental Setup}
\label{app:nmt-setup}
For subword tokenization we used the same SentencePiece~\citep{kudo-richardson-2018-sentencepiece} model, specifically the one used in the MBart
multilingual model~\citep{liu-etal-2020-multilingual-denoising}. This choice allows for unified preprocessing for all languages we cover. We used \texttt{fairseq}~\citep{ott2019fairseq} framework for training our models. Baseline discrete models (eucledian baseline) are trained with cross-entropy loss, label smoothing equal to 0.1 and effective batch size 65.5K tokens. All models are trained with learning rate $5\cdot10^{-4}$ and 10k warm-up steps for 50k steps in total. Spherical baseline and models with dispersion regularizer are trained by defining decoder's embeddings layer as a manifold parameter and using Riemannian Adam~\citep{becigneul2018riemannian} with learning rate $5\cdot10^{-3}$. We used SacreBLEU~\citep{post-2018-call} with the following signature \texttt{nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1} and COMET~\citep{rei-etal-2020-comet} with ~\texttt{unbabel-comet} library version 2.2.2\footnote{\url{https://github.com/Unbabel/COMET}} and \texttt{Unbabel-wmt22-comet-da} model.

\section{Neural Machine Translation: Gradient Norms}
\label{app:grad-norms}
We show in~\Cref{fig:norms-dists-dynamic} how gradient norms and minimum distances of target language embeddings vary throughout the training process. Note that at the step=0, the norms and minimum distances are the same.
\begin{figure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=0.48\linewidth]{images/grad-norms/grad_norm_4000.pdf}\quad
    \includegraphics[width=0.48\linewidth]{images/grad-norms/min_angle_4000.pdf}
    \caption{Gradient norms and minimum distance for step=4000}
    \end{subfigure}

    \begin{subfigure}{\linewidth}
        \centering
    \includegraphics[width=0.48\linewidth]{images/grad-norms/grad_norm_8000.pdf}\quad
    \includegraphics[width=0.48\linewidth]{images/grad-norms/min_angle_8000.pdf}
     \caption{Gradient norms and minimum distance for step=8000}
    \end{subfigure}
    
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.48\linewidth]{images/grad-norms/grad_norm_12000.pdf}\quad
        \includegraphics[width=0.48\linewidth]{images/grad-norms/min_angle_12000.pdf}
        \caption{Gradient norms and minimum distance for step=12000}
        \end{subfigure}
    
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.48\linewidth]{images/grad-norms/grad_norm_20000.pdf}\quad
        \includegraphics[width=0.48\linewidth]{images/grad-norms/min_angle_20000.pdf}
        \caption{Gradient norms and minimum distance for step=20000}
        \end{subfigure}
    
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.48\linewidth]{images/grad-norms/grad_norm_32000.pdf}\quad
        \includegraphics[width=0.48\linewidth]{images/grad-norms/min_angle_32000.pdf}
        \caption{Gradient norms and minimum distance for step=32000}
    \end{subfigure}
    \caption{Training dynamic of gradient norms and minimum distances of the target language embeddings.}
    \label{fig:norms-dists-dynamic}
\end{figure}


\end{document}
