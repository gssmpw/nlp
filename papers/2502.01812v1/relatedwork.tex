\section{Related Work}
\paragraph{Hallucination in LLMs.}

LLMs present remarkable abilities in understanding and executing user instructions~\cite{DBLP:conf/emnlp/Peng0S24}. However, they often generate misleading outputs, termed \textrm{``hallucinations''}
~\cite{DBLP:conf/www/ShenLFRBN23}. Hallucinations are categorized into input-conflicting, context-conflicting, and fact-conflicting types, with fact-conflicting hallucinations posing significant challenges due to their potential to propagate misinformation~\cite{DBLP:conf/ijcai/0016SGW000LZC24}. Previous research has explored hallucinations in tasks such as summarization \cite{DBLP:conf/naacl/PagnoniBT21} and dialogue generation \cite{DBLP:conf/emnlp/DasSS22}, alongside other NLP applications \cite{DBLP:conf/acl/MallenAZDKH23}. Also, Self-consistency decoding has improved chain-of-thought prompting for complex reasoning tasks \cite{rabby2024mc}. Nonetheless, most methods lack a targeted focus on fact-conflicting hallucinations, leaving critical gaps in evaluation and mitigation strategies.

Benchmark datasets like WikiBio~\cite{DBLP:conf/acl/StranisciDMPRC23} can utilize evaluate hallucination detection in LLM outputs but lack focus on factual errors in complex mathematical reasoning tasks. To address this, we introduce a benchmark for evaluating mathematical reasoning hallucinations and assessing factual inaccuracies by response consistency and uncertainty analysis. Perturbation-based datasets~\cite{DBLP:conf/naacl/ThorneVCM18}  modify factual texts to create hallucinatory data but may not capture real-world variation of LLM hallucinations. Chen et al.~\cite{DBLP:conf/ijcai/0016SGW000LZC24} propose a white-box truthfulness method requiring supervised data and inaccessible LLM states, while Zhang et al.~\cite{DBLP:conf/acl/ZhangPTZJSMM24} use self-evaluation, which limits generalizability across tasks and domain. Our benchmark is targeting hallucinations in a black-box setting for complex mathematical reasoning tasks, ensuring broad applicability with the API-accessible LLMs. This approach advances factuality evaluation by response consistency and uncertainty beyond binary judgments.



\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth, trim=0 0 0 0, clip, scale=1.0]{Methodology.png}
    \caption{Overview of the SelfCheckAgent.}
    \label{fig:Selfcheckagent}
\end{figure*}



\paragraph{Sequence Level Uncertainty Estimation}

% Token probabilities have been widely used as a measure of LLM certainty in NLP tasks. For instance, OpenAI’s web interface provides users the option to display token probabilities, giving insights into the LLM's confidence in its generated outputs~\cite{DBLP:journals/corr/abs-2303-08774}. This approach allows users to evaluate each of the tokens in a sequence, providing a clearer understanding of LLM behavior. Additionally, various uncertainty estimation techniques have been explored to further assess LLM confidence, particularly focusing on random and cognitive uncertainty in different LLMs \cite{DBLP:conf/iclr/XiongHLLFHH24}. Also, uncertainty refers to the inherent unpredictability of the data, while cognitive uncertainty captures the LLM’s lack of knowledge or understanding. Moreover, conditional LLM scores, which reflect the probability of a sequence given specific conditions, have been used to evaluate various properties of texts, such as fluency, coherence, and grammaticality \cite{DBLP:conf/emnlp/BehzadZ024}. These scores serve as an important tool for assessing the quality of generated text based on a predefined context or prompt. More recently, the concept of semantic uncertainty has been introduced as a way to address uncertainty in free-form generation tasks. Unlike traditional methods that assign probabilities to individual tokens, semantic uncertainty attaches probabilities to higher-level concepts or ideas, enabling LLMs to capture uncertainty in concept-based generation tasks better \cite{DBLP:journals/corr/abs-2306-01779}. This approach is particularly useful when dealing with open-ended generation tasks where the focus is on generating coherent and meaningful content with more abstract representations.

Token probabilities are commonly used to measure LLM certainty in NLP tasks. OpenAI’s interface, for instance, provides token probabilities to indicate confidence in generated outputs~\cite{DBLP:journals/corr/abs-2303-08774}. This enables users to evaluate token-level confidence and gain insights into LLM behavior. Additionally, various techniques have been developed to assess LLM uncertainty, including random and cognitive uncertainty~\cite{DBLP:conf/iclr/XiongHLLFHH24}. Random uncertainty stems from data unpredictability, while cognitive uncertainty reflects an LLM’s lack of knowledge. Conditional LLM scores, representing the probability of a sequence under specific conditions, have been used to evaluate properties like fluency, coherence, and grammaticality~\cite{DBLP:conf/emnlp/BehzadZ024}. Recently, semantic uncertainty has emerged to address uncertainty in free-form generation by assigning probabilities to higher-level concepts rather than individual tokens~\cite{DBLP:journals/corr/abs-2306-01779}. This approach enhances LLMs' ability to generate coherent, meaningful content in open-ended tasks with abstract representations.


% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1.0\textwidth]{Methodoloy.pdf} % Adjust width as needed
%     \caption{demo}
%     \label{fig:example_image}
% \end{figure*}