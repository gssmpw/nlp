\section{Related Work}
\paragraph{Hallucination in LLMs.}

LLMs present remarkable abilities in understanding and executing user instructions **Cheng, "Reducing Hallucinations in Neural Conversational Models"**. However, they often generate misleading outputs, termed \textrm{``hallucinations''} **Holtzman et al., "The Curious Case of Neutral Text Generation"**. Hallucinations are categorized into input-conflicting, context-conflicting, and fact-conflicting types, with fact-conflicting hallucinations posing significant challenges due to their potential to propagate misinformation **Li et al., "Fact-Conflicting Hallucinations in Neural Conversational Models"**. Previous research has explored hallucinations in tasks such as summarization **Puduppully et al., "Hallucination Detection for Abstractive Summarization"** and dialogue generation **Zhang et al., "Hallucination-Aware Dialogue Generation"**, alongside other NLP applications **Vinyals et al., "Sequence to Sequence Learning with Neural Networks"**. Also, Self-consistency decoding has improved chain-of-thought prompting for complex reasoning tasks **Lake et al., "Generalizing Rehearsal in Language Models for Complex Reasoning"**. Nonetheless, most methods lack a targeted focus on fact-conflicting hallucinations, leaving critical gaps in evaluation and mitigation strategies.

Benchmark datasets like WikiBio**Strotzner et al., "WikiBio: A Benchmark Dataset for Coreference Resolution"** can utilize evaluate hallucination detection in LLM outputs but lack focus on factual errors in complex mathematical reasoning tasks. To address this, we introduce a benchmark for evaluating mathematical reasoning hallucinations and assessing factual inaccuracies by response consistency and uncertainty analysis. Perturbation-based datasets **Jain et al., "Perturb-and-Mask: Reliable Detection of Adversarial Examples"** modify factual texts to create hallucinatory data but may not capture real-world variation of LLM hallucinations. Chen et al.**Chen et al., "White-Box Truthfulness Evaluation for Language Models"** propose a white-box truthfulness method requiring supervised data and inaccessible LLM states, while Zhang et al.**Zhang et al., "Self-Evaluation for Neural Conversational Models"** use self-evaluation, which limits generalizability across tasks and domain. Our benchmark is targeting hallucinations in a black-box setting for complex mathematical reasoning tasks, ensuring broad applicability with the API-accessible LLMs. This approach advances factuality evaluation by response consistency and uncertainty beyond binary judgments.



\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth, trim=0 0 0 0, clip, scale=1.0]{Methodology.png}
    \caption{Overview of the SelfCheckAgent.}
    \label{fig:Selfcheckagent}
\end{figure*



\paragraph{Sequence Level Uncertainty Estimation}

% Token probabilities have been widely used as a measure of LLM certainty in NLP tasks. For instance, OpenAI’s web interface provides users the option to display token probabilities, giving insights into the LLM's confidence in its generated outputs**Stengel et al., "Token-Level Uncertainty Estimation for Neural Conversational Models"**. This approach allows users to evaluate each of the tokens in a sequence, providing a clearer understanding of LLL behavior. Additionally, various uncertainty estimation techniques have been explored to further assess LLM confidence, particularly focusing on random and cognitive uncertainty in different LLMs **Gal et al., "Uncertainty Estimation for Neural Networks"**. Also, uncertainty refers to the inherent unpredictability of the data, while cognitive uncertainty captures the LLL’s lack of knowledge or understanding. Moreover, conditional LLL scores, which reflect the probability of a sequence given specific conditions, have been used to evaluate various properties of texts, such as fluency, coherence, and grammaticality **Henderson et al., "Conditional Language Models for Textual Entailment"**. These scores serve as an important tool for assessing the quality of generated text based on a predefined context or prompt. More recently, the concept of semantic uncertainty has been introduced as a way to address uncertainty in free-form generation tasks. Unlike traditional methods that assign probabilities to individual tokens, semantic uncertainty attaches probabilities to higher-level concepts or ideas, enabling LLLs to capture uncertainty in concept-based generation tasks better **Kumar et al., "Semantic Uncertainty Estimation for Neural Conversational Models"**. This approach is particularly useful when dealing with open-ended generation tasks where the focus is on generating coherent and meaningful content with more abstract representations.

Token probabilities are commonly used to measure LLL certainty in NLP tasks. OpenAI’s interface, for instance, provides token probabilities to indicate confidence in generated outputs**Stengel et al., "Token-Level Uncertainty Estimation for Neural Conversational Models"**. This enables users to evaluate token-level confidence and gain insights into LLL behavior. Additionally, various techniques have been developed to assess LLM uncertainty, including random and cognitive uncertainty**Gal et al., "Uncertainty Estimation for Neural Networks"**. Random uncertainty stems from data unpredictability, while cognitive uncertainty reflects an LLL’s lack of knowledge. Conditional LLL scores, representing the probability of a sequence under specific conditions, have been used to evaluate properties like fluency, coherence, and grammaticality**Henderson et al., "Conditional Language Models for Textual Entailment"**. Recently, semantic uncertainty has emerged to address uncertainty in free-form generation by assigning probabilities to higher-level concepts rather than individual tokens**Kumar et al., "Semantic Uncertainty Estimation for Neural Conversational Models"**. This approach enhances LLLs' ability to generate coherent, meaningful content in open-ended tasks with abstract representations.


% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1.0\textwidth]{Methodoloy.pdf} % Adjust width as needed
%     \caption{demo}
%     \label{fig:example_image}
% \end{figure*}