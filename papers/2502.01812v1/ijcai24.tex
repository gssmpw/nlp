%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}


%..................Add by Gollam Rabby...........
\usepackage{array}

\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Reduce space before and after the table
\setlength{\abovecaptionskip}{5pt} % Space above caption
\setlength{\belowcaptionskip}{5pt} % Space below caption
\setlength{\textfloatsep}{5pt}     % Space between floats and text
\setlength{\floatsep}{5pt}         % Space between multiple floats
\setlength{\intextsep}{5pt}        % Space between in-text floats and text


% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{SelfCheckAgent: Zero-Resource Hallucination Detection\\ in Generative Large Language Models}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse


% \author{
% First Author$^1$ %\thanks{Equal contribution}
% \and
% Second Author$^2$ \thanks{Equal contribution}\and 
% Third Author$^{2,3}$
% %\And
% %Fourth Author$^4$
% \\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% %$^4$Fourth Affiliation\\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% %fourth@example.com
% }

\author{
Diyana Muhammed$^1$\thanks{First and second authors contributed equally to this work.} \and
Gollam Rabby$^2$\footnotemark[1] \and 
Sören Auer$^{1,2}$ \\
\affiliations
$^1$TIB—Leibniz Information Centre for Science and Technology, Hannover, Germany\\
$^2$L3S Research Center, Leibniz University Hannover, Hanover, Germany\\
\emails
\{diyana.muhammed, auer\}@tib.eu,
gollam.rabby@l3s.de
}




%\fi

\begin{document}

\maketitle

\begin{abstract}

%7 Page total and 2-page reference 

Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce \textbf{SelfCheckAgent}, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64\%, Factual 70.26\%, and Ranking 78.48\% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89\% but reveals trade-offs in Factual with 30.58\% and Ranking with 30.68\%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent's applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.




\end{abstract}

\section{Introduction}

Large Language Models (LLMs)~\cite{DBLP:conf/ijcai/WangSORRE24} have revolutionized natural language processing, excelling in tasks like summarization, question answering, and dialogue generation. 
However, despite their impressive capabilities, LLMs can generate outputs that appear plausible but are factually incorrect, a phenomenon referred to as hallucination~\cite{DBLP:conf/emnlp/ManakulLG23}. 
Among the various types of hallucinations—input-conflicting, context-conflicting, and fact-conflicting—the latter poses the most significant challenges due to its potential to spread misinformation across critical domains like healthcare, finance, or education. 
Detecting and mitigating hallucinations is crucial to ensuring reliable LLMs in these sensitive areas~\cite{DBLP:journals/corr/abs-2410-02899}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{SelfCheckAgent_demo.png} % Adjust width as needed
    \caption{Illustration of a hallucination detection example within the SelfCheckAgent framework, showcasing different approaches alongside example input and output.}
    \label{fig:use_example}
\end{figure}


\begin{table*}[htbp]
\centering
    \begin{tabular}{llllcc}
    \hline
    \textbf{Dataset} & \textbf{Source} & \textbf{Domain} & \textbf{Annotation}  & \textbf{LLM-Sampled} & \textbf{Usefulness} \\ 
    \hline
    \textbf{TruthfulQA}~\cite{DBLP:conf/acl/LinHE22} & General & Text & Manual &  \xmark & \xmark  \\ 
    \hline
    \textbf{HaluEval}~\cite{DBLP:conf/emnlp/LiCZNW23} & Text & General & Manual & \xmark & \xmark \\ 
    \hline
    \textbf{HADES}~\cite{DBLP:conf/acl/LiuZBMSCD22} & Token level  & General & Crowdsourced &  \xmark & \xmark  \\ 
    \hline
    \textbf{FACTCHD}~\cite{DBLP:conf/ijcai/0016SGW000LZC24} & KGs \& Text &  General & Manual &  \xmark & \xmark  \\ 
    \hline
    \textbf{WikiBio}~\cite{DBLP:conf/acl/StranisciDMPRC23} & Text & Biography& Manual&  \cmark & \cmark  \\ 
    \hline
    \textbf{AIME} (Ours) & Math Problems & Mathematics & AI-Human &  \cmark & \cmark\\ 
    \hline
    \end{tabular}
%\vspace{0.5cm} % Adds 0.5cm of vertical space before the caption
\caption{Comparison with existing hallucination detection datasets. Our AIME includes complex mathematical domains, such as number theory, geometry, algebra, and others.}
\label{tab:comparison}
\end{table*}

Traditional hallucination detection approaches~\cite{DBLP:conf/emnlp/0038GLZPK24} focus primarily on static claim-evidence pairs and cannot handle the dynamic context intrinsic to LLM-generated content. 
Moreover, existing benchmarks for hallucination detection~\cite{hallucinations-leaderboard} mainly assess basic factual consistency, often neglecting complex mathematical reasoning patterns such as multi-hop reasoning, comparisons, and set operations. This narrow scope leaves a critical gap in developing a robust hallucination detection framework for real-world applications.

To bridge this gap, we introduce an enhanced hallucination detection framework and a benchmark for detecting hallucinations in a black-box setting. 
Our benchmark emphasizes complex mathematical reasoning-based tasks and interpretable evaluations, requiring LLMs to provide not only factual judgments but also coherent explanatory rationales. 
For instance, as shown in \autoref{fig:use_example}, our approach comprising a symbolic agent, contextual consistency agent, and specialized detection agent allows to evaluate LLM responses (e.g., to \textrm{``What is the capital of Germany?''}) resulting in a score for hallucination detection. 

Recognizing the challenges of constructing a comprehensive dataset, we utilized a scalable data generation approach that synthesizes hallucination instances by leveraging existing complex mathematical reasoning datasets and textual resources. 
These instances are generated using LLMs and subsequently validated by human annotators to ensure high quality and domain coverage. %This approach allows us to create a diverse dataset encompassing complex reasoning patterns while maintaining authenticity.

In addition, we also present the hallucination detection framework, a triangulation-based method inspired by \cite{DBLP:conf/ijcai/0016SGW000LZC24}. 
This framework utilizes any of these three core components: the symbolic agent, the contextual consistency agent; or the specialized detection agent, which integrates evidence from LLM response generating consistent repetition to derive reliable conclusions. 
Our methods leverage cross-referential evaluation to improve the detection of factual inaccuracies in LLM outputs. Key contributions of this work include:
\begin{itemize}
    \item We enhance a hallucination detection framework~\cite{DBLP:conf/emnlp/ManakulLG23} such that it combines response consistency and uncertainty quantification thus significantly boosting the evaluation of LLM reliability.
    \item We categorize hallucinations into distinct patterns, providing a comprehensive analysis of response behaviors in LLMs.
    \item We introduce a new hallucination detection dataset for complex mathematical reasoning, leveraging human-AI collaboration.
\end{itemize}


\section{Related Work}

\paragraph{Hallucination in LLMs.}

LLMs present remarkable abilities in understanding and executing user instructions~\cite{DBLP:conf/emnlp/Peng0S24}. However, they often generate misleading outputs, termed \textrm{``hallucinations''}
~\cite{DBLP:conf/www/ShenLFRBN23}. Hallucinations are categorized into input-conflicting, context-conflicting, and fact-conflicting types, with fact-conflicting hallucinations posing significant challenges due to their potential to propagate misinformation~\cite{DBLP:conf/ijcai/0016SGW000LZC24}. Previous research has explored hallucinations in tasks such as summarization \cite{DBLP:conf/naacl/PagnoniBT21} and dialogue generation \cite{DBLP:conf/emnlp/DasSS22}, alongside other NLP applications \cite{DBLP:conf/acl/MallenAZDKH23}. Also, Self-consistency decoding has improved chain-of-thought prompting for complex reasoning tasks \cite{rabby2024mc}. Nonetheless, most methods lack a targeted focus on fact-conflicting hallucinations, leaving critical gaps in evaluation and mitigation strategies.

Benchmark datasets like WikiBio~\cite{DBLP:conf/acl/StranisciDMPRC23} can utilize evaluate hallucination detection in LLM outputs but lack focus on factual errors in complex mathematical reasoning tasks. To address this, we introduce a benchmark for evaluating mathematical reasoning hallucinations and assessing factual inaccuracies by response consistency and uncertainty analysis. Perturbation-based datasets~\cite{DBLP:conf/naacl/ThorneVCM18}  modify factual texts to create hallucinatory data but may not capture real-world variation of LLM hallucinations. Chen et al.~\cite{DBLP:conf/ijcai/0016SGW000LZC24} propose a white-box truthfulness method requiring supervised data and inaccessible LLM states, while Zhang et al.~\cite{DBLP:conf/acl/ZhangPTZJSMM24} use self-evaluation, which limits generalizability across tasks and domain. Our benchmark is targeting hallucinations in a black-box setting for complex mathematical reasoning tasks, ensuring broad applicability with the API-accessible LLMs. This approach advances factuality evaluation by response consistency and uncertainty beyond binary judgments.



\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth, trim=0 0 0 0, clip, scale=1.0]{Methodology.png}
    \caption{Overview of the SelfCheckAgent.}
    \label{fig:Selfcheckagent}
\end{figure*}



\paragraph{Sequence Level Uncertainty Estimation}

% Token probabilities have been widely used as a measure of LLM certainty in NLP tasks. For instance, OpenAI’s web interface provides users the option to display token probabilities, giving insights into the LLM's confidence in its generated outputs~\cite{DBLP:journals/corr/abs-2303-08774}. This approach allows users to evaluate each of the tokens in a sequence, providing a clearer understanding of LLM behavior. Additionally, various uncertainty estimation techniques have been explored to further assess LLM confidence, particularly focusing on random and cognitive uncertainty in different LLMs \cite{DBLP:conf/iclr/XiongHLLFHH24}. Also, uncertainty refers to the inherent unpredictability of the data, while cognitive uncertainty captures the LLM’s lack of knowledge or understanding. Moreover, conditional LLM scores, which reflect the probability of a sequence given specific conditions, have been used to evaluate various properties of texts, such as fluency, coherence, and grammaticality \cite{DBLP:conf/emnlp/BehzadZ024}. These scores serve as an important tool for assessing the quality of generated text based on a predefined context or prompt. More recently, the concept of semantic uncertainty has been introduced as a way to address uncertainty in free-form generation tasks. Unlike traditional methods that assign probabilities to individual tokens, semantic uncertainty attaches probabilities to higher-level concepts or ideas, enabling LLMs to capture uncertainty in concept-based generation tasks better \cite{DBLP:journals/corr/abs-2306-01779}. This approach is particularly useful when dealing with open-ended generation tasks where the focus is on generating coherent and meaningful content with more abstract representations.

Token probabilities are commonly used to measure LLM certainty in NLP tasks. OpenAI’s interface, for instance, provides token probabilities to indicate confidence in generated outputs~\cite{DBLP:journals/corr/abs-2303-08774}. This enables users to evaluate token-level confidence and gain insights into LLM behavior. Additionally, various techniques have been developed to assess LLM uncertainty, including random and cognitive uncertainty~\cite{DBLP:conf/iclr/XiongHLLFHH24}. Random uncertainty stems from data unpredictability, while cognitive uncertainty reflects an LLM’s lack of knowledge. Conditional LLM scores, representing the probability of a sequence under specific conditions, have been used to evaluate properties like fluency, coherence, and grammaticality~\cite{DBLP:conf/emnlp/BehzadZ024}. Recently, semantic uncertainty has emerged to address uncertainty in free-form generation by assigning probabilities to higher-level concepts rather than individual tokens~\cite{DBLP:journals/corr/abs-2306-01779}. This approach enhances LLMs' ability to generate coherent, meaningful content in open-ended tasks with abstract representations.


% \begin{figure*}[h]
%     \centering
%     \includegraphics[width=1.0\textwidth]{Methodoloy.pdf} % Adjust width as needed
%     \caption{demo}
%     \label{fig:example_image}
% \end{figure*}


\section{Preliminaries}

Detecting hallucinations in LLMs requires evaluating the consistency of responses across multiple independent samples generated from the same context. 
If an LLM has integrated accurate knowledge it should generate responses with low variance, ensuring high factual consistency in factual tasks. We formulate the task as follows:

\textbf{Input:} A context \( C \), and a set of \( N \) independently generated samples \( \{R_1, R_2, \dots, R_N\} \), where each \( R_i \) is generated by predicting a next set of token sequentially given the context.

\textbf{Output:} A consistency score \( S = \{s_1, s_2, \dots, s_N\} \), where each score \( s_i \) quantifies the factual consistency of \( R_i \) relative to other samples. 
We evaluate consistency using token-level overlap, semantic similarity, and variance in the responses. 
High variance in responses suggests potential hallucinations, while low variance indicates stable, factual knowledge.

This approach connects the consistency of LLM responses with uncertainty metrics such as token-level probabilities and entropy. Consistent responses suggest well-internalized knowledge, reducing hallucination probability, whereas variability signals knowledge gaps, leading to hallucinations. Analyzing token probabilities and entropy further refines this detection, with lower entropy indicating higher factual reliability and higher entropy suggesting potential hallucinated content.




\paragraph{Consistency and Uncertainty Quantification Based Hallucination detection}

A key approach to hallucination detection in LLMs involves evaluating the consistency of their generated responses across multiple samples. 
Consistent responses indicate internalized, factual knowledge, reducing the likelihood of hallucinations. 
In contrast, high variability often signals incoherent knowledge and potentially fabricated content. 
For example, when queried about \textrm{``Albert Einstein's birthplace,''} consistent responses such as \textrm{``Germany''} suggest factual grounding, whereas varying outputs like \textrm{``Germany,''} \textrm{``Austria,''} and \textrm{``Switzerland''} suggest a lack of coherent knowledge. 
Variance in LLM-generated content reveals the extent of incomplete information, highlighting response stability as an indicator of factual reliability.

To quantify uncertainty in LLMs responses, we use entropy, $\mathcal{H}(p) = -\sum_{\mathbf{y}} p(\mathbf{y}|\mathbf{x}) \log p(\mathbf{y}|\mathbf{x})$, which measures prediction confidence, with higher values indicating greater uncertainty and hallucination risk. Predictive variance, $\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{y}_i - \bar{\mathbf{y}})^2$, computed from $N$ samples, captures uncertainty in outputs. For instance, when asked \textrm{``Who discovered penicillin?''} consistent responses like \textrm{``Alexander Fleming''} correlate with lower entropy and variance, whereas outputs like \textrm{``Alexander Fleming,''} \textrm{``Louis Pasteur,''} and \textrm{``Marie Curiev''} yield higher entropy, signaling hallucinations. Thresholds $\tau_\mathcal{H}$ and $\tau_\sigma$ identify outputs exceeding these limits as hallucinations, thus offering a reliable detection mechanism.

Token-level probabilities are also useful for detecting hallucinations. For a token $y_t$, $p(y_t|\mathbf{x}, y_{<t})$ represents its likelihood given input $\mathbf{x}$ and prior tokens $y_{<t}$. Tokens with $p(y_t|\mathbf{x}, y_{<t}) < \tau$ are flagged as anomalies. For example, given the input \textrm{``Albert Einstein was born in \_,''} the token \textrm{``Germany''} may have a high probability due to training data prevalence, while tokens like \textrm{``Australia''} or \textrm{``Mars''} would have much lower probabilities. The sequence log-probability, $\mathcal{L} = \frac{1}{T} \sum_{t=1}^{T} \log p(y_t|\mathbf{x}, y_{<t})$, serves as a reliability metric, with lower values indicating hallucinated content. Additionally, token-level entropy, $\mathcal{H}_t = -\sum_{v \in \mathcal{V}} p(v|\mathbf{x}, y_{<t}) \log p(v|\mathbf{x}, y_{<t})$, identifies unreliable generations. For example, ambiguous contexts such as \textrm{``Albert Einstein invented \_''} yield higher token-level entropy, reflecting uncertainty about the answer.

By combining these approaches, including entropy-based thresholds $\mathcal{H}(p) > \tau_\mathcal{H}$, we enhance the detection of hallucinations in LLM outputs. This ensures greater reliability and interpretability of generated content, also in the complex mathematical reasoning tasks where factual correctness is most important.


\begin{table}[ht]
\centering
\begin{tabular}{lcccc}
\hline
    \textbf{Dataset} & \textbf{Major} & \textbf{Minor} &\textbf{Accurate} &  \textbf{Total} \\
     & \textbf{Inaccurate} & \textbf{Inaccurate} & & \\ \hline
    WikiBio & 761 & 516 & 631 & 1.908 \\ \hline
    AIME & 823 & 38 & 137 & 998 \\ \hline
\end{tabular}
\caption{Sample distribution of the experimental datasets.}
\label{tab:sample_distribution}
\end{table}




\begin{table*}[htbp]
\centering
\small % Adjust font size
\renewcommand{\arraystretch}{0.70} % Reduce row height
\begin{tabular}{p{2.0cm}|p{2.4cm}|p{1.8cm}|p{0.5cm}|p{0.8cm}|p{1.0cm}|p{.9cm}|p{1.0cm}|p{1.0cm}|p{0.9cm}|p{1.0cm}}
\hline \hline
\textbf{System} & \textbf{Method} & \textbf{LLM} & \textbf{Size} & \textbf{Prompt} & \multicolumn{3}{c}{\textbf{WikiBio Dataset}} & \multicolumn{3}{c}{\textbf{AIME Dataset}} \\
\hline
 & & & & & \textbf{NonFact} & \textbf{Factual} & \textbf{Ranking} & \textbf{NonFact} & \textbf{Factual} & \textbf{Ranking} \\
\hline \hline

\textit{Symbolic Agent} & Semantic Unigram & - & - & - & \textbf{86.97} & \textbf{59.02} & \textbf{65.88} & \textbf{87.24} & \textbf{14.62} & \textbf{11.94} \\ 
\hline
\textit{SelfCheckGPT} & Unigram & - & - & - & 85.63 & 58.47 & 64.71 & 85.69 & 14.91 & - \\
\hline \hline
 & Pre-trained & Phi-3 & 3.8B & ZS & \textbf{89.27} & \textbf{56.26} & \textbf{61.88} & 90.34 & 19.16 & 12.75 \\
  &  & Llama 3.1 & 8B & ZS & 78.90 & 37.12 & 34.25 & 87.76 & 21.10 & 3.16 \\
  &  & Mistral & 7B & ZS & 86.13 & 58.60 & 41.15 & 91.33 & 17.11 & 2.57 \\
 &  & Gemma & 7B & ZS & 75.41 & 30.87 & 4.80 & \textbf{93.38} & \textbf{20.38} & \textbf{21.76} \\
   \textit{Specialized} &  & Roberta-large & 340M & ZS & 76.44 & 31.20 &  6.07 & - & - & - \\
 \textit{Detection} &  & Phi-3 & 3.8B & CoT & 75.08 & 29.67 & 13.79 & \textbf{87.76} & \textbf{21.10} & \textbf{3.16} \\
    \textit{Agent}&  & Llama 3.1 & 8B & CoT & 63.88 & 21.75 & - & 85.33 & 13.13 & - \\
  &  & Mistral & 7B & CoT & \textbf{81.91} & \textbf{46.10} & \textbf{43.49} & 91.33 & 17.11 & 2.57 \\
   &  & Gemma & 7B & CoT & 70.97 & 25.99 &  - & 90.07 & 13.00 & 0.50 \\
   &  & Roberta-large & 340M & CoT & 73.86 & 31.47 & 13.02 & - & - & - \\
\hline \hline
  \textit{Specialized}& Fine-tuned & Phi-3 & 3.8B & - & 92.87 & 65.25 & 73.54 & \textbf{93.38} & \textbf{20.38} & \textbf{21.76} \\
   \textit{Detection}&  & Llama 3.1 & 8B & - & 76.85 & 29.71 & 8.91 & 79.63 & 9.74 & - \\
 \textit{Agent}&  & Mistral & 7B & - & \textbf{92.68} & \textbf{67.10} & \textbf{75.63} & 92.91 & 17.37 & 20.05 \\
   &  & Gemma & 7B & - & 83.47 & 43.12 & 50.98 & 82.58 & 10.70 & - \\
   \hline
 \textit{SelfCheckGPT}& Fine-tuned & DeBERTa-v3 & - & - & 92.50 & 66.08 & 74.14 & 87.64 & 17.45 & 5.32 \\
 \hline \hline

% SelfCheckGPT& Fine tune & - & - & - & 92.50 & 66.08 & 74.14 & 87.64 & 17.45 & 5.32 \\
% \hline

 
  \textit{Contextual}& Prompt based & Llama 3.1 & 8b & ZS & 92.85 &  70.73 & 76.54 & 94.79 & 37.75 & 29.67 \\
 %& Contextual Agent & Llama 3.1 & 13b & ZS &  &  &  &  &  &  \\
 %Consistency& Prompt based & gpt3.5 & - & ZS & \textbf{93.42} & \textbf{67.09} & \textbf{78.32} & \textbf{95.66} & \textbf{56.95} & \textbf{30.08} \\
  \textit{Consistency} &  & gpt4o& - & ZS &  94.00 & 74.11 &  77.48  & 93.93 & 24.26 & 21.13 \\
   \textit{Agent} &  & Mistral & 7B & CoT & 91.74 & 64.01 & 75.40 & 93.63 & 37.98 & 22.51 \\
 &  & Llama 3.1 & 8b & CoT & \textbf{93.64} & \textbf{70.26} & \textbf{78.48} & 91.77 & 24.89 & 14.23 \\
 %& Contextual Agent & Llama 3.1 & 13b & CoT &  &  &  &  &  &  \\
  &  & gpt3.5 & - & CoT & 90.59 & 62.11 & 72.17 & 95.28 & 57.62 & 25.42 \\
 &  & gpt4o& - & CoT & 94.14 & 74.95 & 76.33 & \textbf{94.89} & \textbf{30.58} & \textbf{30.68} \\
\hline
 \textit{SelfCheckGPT}& Prompt based & Mistral & 7B & ZS & 91.31 & 62.76 & 74.46 & 92.15 & 45.31 & 17.19 \\
 &  & gpt3.5 & - & ZS & 93.42 & 67.09 & 78.32 & 95.66 & 56.95 & 30.08 \\
\hline \hline
\end{tabular}
\caption{Consolidated comparison of methods, LLMs, sizes, and prompting for the WikiBio and AIME datasets. Metrics include NonFactual (AUC-PR), Factual(AUC-PR), and Ranking scores (PCC).}
\label{tab:consolidated_comparison}
\end{table*}


\section{Benchmark Dataset}
\subsubsection{WikiBio Dataset}




The WikiBio dataset, featuring Wikipedia paragraphs and tabular data, was used as a benchmark for hallucination detection. From the test set, 238 articles were sampled from the top 20\% longest entries. Synthetic passages were generated with GPT-3.5 using the prompt: \textit{``This is a Wikipedia passage about [concept]:''}. The dataset includes 1,908 sentences (184.7 tokens on average) annotated as Major Inaccurate (39.9\%), Minor Inaccurate (33.1\%), and Accurate (27.0\%). For 201 sentences, two annotators labeled factuality, resolving disagreements with the worst-case label, achieving Cohen's $\kappa$ scores of 0.595 (3-class) and 0.748 (2-class). Passage-level scores averaged from sentences peaked at +1.0, indicating full hallucination.



\subsubsection{AIME Math Hallucination Benchmark}

% The first step in our dataset annotation process involved collecting human solutions to each of the 998 AIME math problems from the American Invitational Mathematics Examination. Human annotators provided accurate solutions and explanations for each problem. These human-generated solutions served as the ground truth for assessing the quality of the LLM responses in subsequent steps.


The initial phase of dataset annotation involved collecting human solutions for all 998 AIME problems from the American Invitational Mathematics Examination, providing accurate solutions and explanations as ground truth for evaluating LLM responses.
%\paragraph{Generate LLM responses}
% We then generated five distinct responses for each problem using GPT-4o with prompting. This process produced multiple samples for each problem, allowing us to evaluate the consistency and accuracy of the LLM's responses.
We generated five distinct responses per problem using GPT-4o with Zero-shot prompting, enabling the evaluation of response consistency and accuracy across multiple samples.


\subparagraph{Human-AI collaboration}
We randomly selected one response and evaluated its correctness against the human-provided solution. Responses matching the human solution were categorized as Minor Inaccurate or Accurate, while non-matching responses were categorized as Major Inaccurate. The categorized process combined human-AI collaboration by GPT3.5 based on answer matching, clarified by human annotators for ambiguous cases.


\subparagraph{Quality Filtering via Human Annotator}

The final quality filtering step requires manually verifying LLM responses categorized as Major Inaccurate, Minor Inaccurate, or Accurate. Responses containing partially relevant information but incorrect answers were labeled as Minor Inaccurate, while those with correct answers and explanations were classified as Accurate. Completely incorrect or irrelevant responses were designated as Major Inaccurate. Out of 998 responses, 823 were Major Inaccurate, 137 were Accurate, and 38 were Minor Inaccurate. These annotations form the foundation for assessing the model's mathematical reasoning and hallucination detection capabilities.


\section{SelfCheckAgent}

Our approach is based on three complementary agents for determining hallucinations: 1. Symbolic Agent, which aims to assess semantic response similarity; 2. Specialized Detection Agent, which leverages an LLM fine-tuned with a specific hallucination detection dataset; and 3. Contextual Consistency Agent, which compares responses with sampled context passages.

\paragraph{Symbolic Agent}

To evaluate the reliability of generated responses and identify potential hallucinations, we propose a semantic N-gram modeling approach enhanced by Word2Vec embeddings~\cite{DBLP:journals/corr/abs-1301-3781}. 
This methodology incorporates pre-trained semantic representations to assess token-level and N-gram probabilities in a contextualized manner. 
Given a set of generated samples $\{S_1, S_2, \ldots, S_N\}$ and the primary response $R$, we construct a semantic N-gram model. 
The model integrates Word2Vec embeddings to account for semantic similarities between tokens, improving probability estimates, particularly in sparse data scenarios. For each token, the model retrieves its embedding from a pre-trained Word2Vec model. If embeddings are unavailable, a placeholder is used to maintain computational consistency. Including $R$ in the training data acts as a smoothing mechanism, incrementing the count of each token in $R$ by one. This ensures that the model contextualizes the response within the sampled data distribution. To account for semantic variations, token probabilities are computed by aggregating the probabilities of similar tokens based on Word2Vec cosine similarity. For a token $t$, its semantic similarity to other tokens is measured as: $\text{similarity}(t, t') = 1 - \text{cosine}(\mathbf{v}_t, \mathbf{v}_{t'})$, where $\mathbf{v}_t$ and $\mathbf{v}_{t'}$ are the Word2Vec embeddings of $t$ and $t'$, respectively. Tokens with similarity scores above a predefined threshold contribute to the aggregated probability:
\[
\tilde{p}(t) = \sum_{t' \in \text{similar}(t)} p(t')
\]
This Word2Vec-integrated approach ensures that the evaluation considers semantic context, providing a robust and interpretable framework for assessing the factuality of LLM outputs while addressing data sparsity challenges.


\paragraph{Specialized Detection Agent}

To detect hallucinations, we utilized transformer-based LLMs fine-tuned with Quantized Low-Rank Adaptation (QLoRA)~\cite{DBLP:conf/nips/DettmersPHZ23}. Let a dataset \( \mathcal{D} = \{(s_i, P_j, y_{ij})\} \) comprise sentences \( s_i \), context passages \( P_j \), and corresponding labels \( y_{ij} \in \{\text{entailment}, \text{neutral}, \text{contradiction}\} \). The MultiNLI dataset~\cite{DBLP:conf/naacl/WilliamsNB18} serves as the training corpus for fine-tuning LLMs. QLoRA facilitates efficient quantization, utilizing precision with double quantization for computational scalability. LoRA adapters are configured with a rank, scaling factor, and dropout rate, targeting attention and feedforward layers of the transformer. During fine-tuning, the objective minimizes the cross-entropy loss \( \mathcal{L} \) over the predictions \( \hat{y}_{ij} \) and ground truth \( y_{ij} \):
\[
\mathcal{L} = -\frac{1}{|\mathcal{D}|} \sum_{(s_i, P_j, y_{ij}) \in \mathcal{D}} \log p(\hat{y}_{ij} = y_{ij} | s_i, P_j)
\]
LLM configurations include varying batch sizes \( b \), learning rates \( \eta \), and epochs \( E \), optimized using the AdamW optimizer. Post-fine-tuning, sentence-context factuality is scored using \( f_{\text{score}}(s_i, P_j) \) mapping to, representing \textit{Accurate}, \textit{Minor Inaccurate}, and \textit{Inaccurate} labels, respectively. The overall factuality score for a sentence \( s_i \) is computed as:
\[
S_i = \frac{1}{|P|} \sum_{j=1}^{|P|} f_{\text{score}}(s_i, P_j)
\]
where \( |P| \) is the number of contexts \( P_j \). Validation experiments reveal that the fine-tuned LLMs robustly identify contradictions and ambiguities, achieving computational efficiency for hallucination detection.

\paragraph{Contextual Consistency Agent}

Contextual Consistency Agent leverages a self-evaluation mechanism using LLMs to assess the factuality of generated text by comparing sentences with sampled context passages. For a given sentence \( s_i \) and a set of sampled passages \( P_j \), a structured prompt is created in the format: \textit{Context: \( P_j \); Sentence: \( s_i \); Is the sentence supported by the context above? Answer Yes or No.''} The LLM generates responses, which are post-processed to map outputs to scores: \( 0.0 \) for \textit{accurate}, \( 1.0 \) for \textit{inaccurate}, and \( 0.5 \) for ambiguous cases. For each sentence \( s_i \), the scoring process computes the factuality score as the mean of scores across all sampled passages \( P_j \):

\[
S_i = \frac{1}{|P|} \sum_{j=1}^{|P|} f_{\text{score}}(s_i, P_j)
\]

where \( |P| \) represents the total number of sampled passages. The generated outputs are iteratively checked against the context, providing a systematic evaluation of hallucinations. Additionally, this method supports efficient checkpointing, enabling the storage and retrieval of intermediate states during evaluation. This approach ensures scalability and reliability for large-scale assessments of factuality in LLM-generated text, facilitating robust evaluations for any data. 
%By combining automated scoring and human refinement for ambiguous cases, the Self-Check API Prompt achieves high accuracy and interpretability in detecting factual inaccuracies.


% \paragraph{Contextual Consistency Agent}

% Contextual consistency agent leverages a self-checking mechanism with LLMs to evaluate the consistency of the generated text. For a given sentence \( s_i \) and a set of sampled passages \( P_j \), a prompt is structured as: \textit{"Context: \( P_j \); Sentence: \( s_i \); 

% First, understand what the sentence claims. Check if the context provides evidence for this claim. Finally, determine if the sentence is supported by the context."} The LLM generates a response that is then used to assess the factual accuracy of \( s_i \). The framework quantifies this accuracy using a scoring function \( f_{\text{score}}(s_i, P_j) \), where the outputs correspond to factuality scores: \( 0.0 \) for accurate sentences, \( 0.5 \) for minor inaccuracies, and \( 1.0 \) for major inaccuracies or hallucinations. For each sentence \( s_i \), the overall score \( S_i \) is computed as the average of scores across all sampled passages \( P_j \):

% \[ S_i = \frac{1}{N} \sum_{j=1}^{N} f_{\text{score}}(s_i, P_j)\]

% where \( N \) is the total number of sampled passages. This process is repeated for all sentences in the dataset, producing a comprehensive set of sentence-level scores. By systematically comparing each sentence to its corresponding context, this framework enables automated hallucination detection and large-scale evaluations, such as those applied to the WikiBio~\cite{} dataset, ensuring scalability and reliability in identifying factual inaccuracies.


\section{Experiments}

\subsection{Metric Definition}

We assess generated text quality using a statistical framework based on negative log-likelihood (NLL), which measures alignment between LLM predictions and observed text. The evaluation operates at both sentence and document levels, using average and maximum negative log probabilities. Each sentence is tokenized, and probabilities are computed for each token based on the LLM's distribution. Tokens without matching probabilities are assigned an unknown token probability (\texttt{<unk>}). The token NLL is calculated as:
\[
\text{NLL}(t) = -\log(\text{prob}(t))
\]
Sentence-level metrics include average NLL (fluency) and maximum NLL (challenging predictions):
\[
\text{AvgNLL} = -\frac{1}{n}\sum_{i=1}^n \log(\text{prob}(t_i))
\]
\[
\text{MaxNLL} = -\min_{i} \log(\text{prob}(t_i))
\]
Document-level metrics aggregate sentence NLLs to assess overall coherence:
\[
\text{DocAvgNLL} = -\frac{1}{m}\sum_{j=1}^m \text{AvgNLL}_j
\]
\[
 \text{DocMaxAvgNLL} = \frac{1}{m}\sum_{j=1}^m \text{MaxNLL}_j
\]
These metrics provide a robust evaluation of performance, considering both global and local prediction challenges.



% First Table

% \begin{table*}[htbp]
% \centering
% \small % Adjust font size
% \begin{tabular}{p{2.0cm}|p{2.5cm}|p{2.0cm}|p{0.5cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{0.9cm}|p{1.0cm}}
% \hline
% \textbf{System} & \textbf{Method} & \textbf{LLM} & \textbf{Size} & \textbf{Prompt} & \multicolumn{3}{c}{\textbf{Wikibio Dataset}} & \multicolumn{3}{c}{\textbf{AIME Dataset}} \\
% \hline
%  & & & & & \textbf{NonFact} & \textbf{Factual} & \textbf{Ranking} & \textbf{NonFact} & \textbf{Factual} & \textbf{Ranking} \\
% \hline
% SelfCheckGPT & Unigram & - & - & - & 85.63 & 58.47 & 64.71 & 85.69 & 14.91 & - \\
% SelfCheckAgent & Semantic Unigram & - & - & - & \textbf{86.97} & \textbf{59.02} & \textbf{65.88} & \textbf{87.24} & \textbf{14.62} & \textbf{11.94} \\
% \hline
%  & Pre-train & Phi-3 & 3.8B & ZS & 89.27 & 56.26 & 61.88 & 90.34 & 19.16 & 12.75 \\
%  & Pre-train & Llama 3.1 & 8B & ZS & 78.90 & 37.12 & 34.25 & 87.76 & 21.10 & 3.16 \\
%  & Pre-train & Mistral & 7B & ZS & 86.13 & 58.60 & 41.15 & 91.33 & 17.11 & 2.57 \\
%   & Pre-train & Gemma & 7B & ZS & 75.41 & 30.87 & 4.80 & 93.38 & 20.38 & 21.76 \\
%  SelfCheckAgent& Pre-train & Roberta-large & - & ZS &  &  &  &  &  &  \\
%  & Pre-train & Phi-3 & - & CoT &  &  &  &  &  &  \\
%  & Pre-train & Llama 3.1 & - & CoT &  &  &  &  &  &  \\
%  & Pre-train & Mistral & - & CoT &  &  &  &  &  &  \\
%  & Pre-train & Gemma & - & CoT &  &  &  &  &  &  \\
%  & Pre-train & Roberta-large & - & CoT & 76.44 & 31.20 & 6.07 & 92.91 & 17.37 & 20.05 \\
% \hline
%  & Specialized & Phi-3 & 3.8B & - & 92.87 & 65.25 & 73.54 & 93.63 & 37.98 & 22.51 \\
%  & Specialized & Llama 3.1 & 8B & - & 76.85 & 29.71 & 8.91 & 93.15 & 56.88 & 1.71 \\
%  & Specialized & Mistral & 7B & - & 92.68 & 67.10 & 75.63 & 91.77 & 24.89 & 14.23 \\
%  & Specialized & Gemma & 7B & - & 83.47 & 43.12 & 50.98 & 95.28 & 57.62 & 25.42 \\
% & Specialized & XXXXXX & - & - &  &  &  &  &  &  \\
%  & Specialized Agent & Phi-3 & - & - &  &  &  &  &  &  \\
%  & Specialized Agent & Llama 3.1 & - & - &  &  &  &  &  &  \\
%  & Specialized Agent & Mistral & - & - &  &  &  &  &  &  \\
%  & Specialized Agent & Gemma & - & - &  &  &  &  &  &  \\
%  & Specialized GPT & XXXXX & - & - &  &  &  &  &  &  \\
% \hline
%  & Contextual Agent & Mistral & - & ZS &  &  &  &  &  &  \\
%  & Contextual Agent & Llama 3.1 & 7b & ZS &  &  &  &  &  &  \\
%  & Contextual Agent & Llama 3.1 & 13b & ZS &  &  &  &  &  &  \\
%  & Contextual Agent & gpt3.5 & - & ZS &  &  &  &  &  &  \\
%  & Contextual Agent & gpt4o& - & ZS &  &  &  &  &  &  \\
 
%  & Contextual Agent & Mistral & - & CoT &  &  &  &  &  &  \\
%  & Contextual Agent & Llama 3.1 & 7b & CoT &  &  &  &  &  &  \\
%  & Contextual Agent & Llama 3.1 & 13b & CoT &  &  &  &  &  &  \\
%  & Contextual Agent & gpt3.5 & - & CoT &  &  &  &  &  &  \\
%  & Contextual Agent & gpt4o& - & CoT &  &  &  &  &  &  \\
 
% \hline
% \end{tabular}
% % \vspace{0.5cm}
% \caption{Consolidated comparison of methods, LLMs, sizes, and prompting for the Wikibio and AIME datasets. Metrics include NonFactual, Factual, and Ranking scores.}
% \label{tab:consolidated_comparison}
% \end{table*}






% \begin{table*}[htbp]
% \centering
% %\resizebox{\textwidth}{!}{%
% \begin{tabular}{llccccc}
% \hline
% Method                & LLM                  & Size & Prompt & NonFact & Factual & Ranking \\
% \hline
% \multicolumn{7}{c}{\textbf{Wikibio Dataset}} \\
% \hline
% \textbf{SelfCheck-Unigram} & - & - & -  & 85.63 & 58.47 & 64.71 \\
% \textbf{Symbolic Agent-Unigram}   & - & - & - & \textbf{86.97} & \textbf{59.02} & \textbf{65.88} \\

% \hline

% Pre-train                     & Phi-3-4k-instr      & 3.8B   & ZS            & 89.27   & 56.26   & 61.88   \\
% Pre-train                   & Llama 3.1 8B        & 8B   & ZS             & 78.90   & 37.12   & 34.25   \\
% Pre-train                   & Mistral 7B          & 7B   & ZS             & 86.13   & 58.60   & 41.15   \\
% Pre-train                 & Gemma-7b-aps-it     & 7B   & ZS            & 75.41   & 30.87   & 4.80    \\
% Pre-train            & Roberta-large       & -    & ZS             & 76.44   & 31.20   & 6.07    \\

% \hline

% Pre-train                    & Phi-3-4k-instr      & 3.8B  & ZS-CoT         & 75.08   & 29.67   & 13.79   \\
% Pre-train                     & Mistral 7B          & 7B   & ZS-CoT        & 81.91   & 46.10   & 43.49   \\
% Pre-train                     & gpt2                & 2B   & ZS-CoT        & 72.01   & 30.59   & 3.55    \\
% Pre-train                  & Roberta-large       & -    & ZS-CoT         & 73.86   & 31.47   & 13.02   \\\hline
% Specialized Agent      & Phi-3-4k-instr      & 3.8B  & ZS             & 92.87   & 65.25   & 73.54   \\
% Specialized Agent      & Llama 3.1 8B        & 8B   & ZS             & 76.85   & 29.71   & 8.91    \\
% Specialized Agent      & Mistral 7B          & 7B   & ZS            & 92.68   & 67.10   & 75.63   \\
% Specialized Agent      & Gemma-7b-aps-it     & 7B   & ZS           & 83.47   & 43.12   & 50.98   \\
% \textbf{Specialized Agent} & SelfCheck-NLI    & -    & ZS             & \textbf{92.50} & \textbf{66.08} & \textbf{74.14} \\

% \hline

% Contextual Agent       & Mistral 7B          & 7B   & CoT            & 91.74   & 64.01   & 75.40   \\
% Contextual Agent       & Llama2-7B-chat      & 7B   & CoT           & 81.92   & 33.37   & 12.62   \\
% Contextual Agent       & Llama3.1-8B-instr   & 8B   & CoT            & 93.64   & 70.26   & 78.48   \\
% Contextual Agent       & Llama2-13B-chat     & 13B  & CoT           & 87.85   & 50.09   & 53.24   \\
% Contextual Agent       & gpt-3.5-turbo       & -    & CoT          & 90.59   & 62.11   & 72.17   \\
% Contextual Agent       & gpt-4o-mini         & -    & CoT           & 94.14   & 74.95   & 76.33   \\
% \textbf{Contextual Agent} & gpt-3.5-turbo    & -    & ZS            & 93.42   & 67.09   & 78.32   \\
% \textbf{Contextual Agent} & gpt-4o-mini      & -    & ZS           & \textbf{94.00} & \textbf{74.11} & \textbf{77.48} \\
% \hline
% \multicolumn{7}{c}{\textbf{AIME Dataset}} \\
% \hline
% \textbf{SelfCheck-Unigram} & - & -  & - & 85.69 & 14.91 & - \\
% \textbf{SelfCheck-Agent}   & - & - & - & \textbf{87.24} & \textbf{14.62} & \textbf{11.94} \\\hline
% Pre-train                 & Phi-3-mini-4k-instr & 4k & ZS      & 90.34 & 19.16 & 12.75 \\
% Pre-train                 & Phi-3-mini-4k-instr & 4k & ZS-CoT   & 87.76 & 21.10 & 3.16  \\
% Pre-train                 & Mistral 7B          & 7B & ZS-CoT   & 91.33 & 17.11 & 2.57  \\\hline
% Specialized Agent         & Phi-3-mini-4k-instr & 4k & ZS       & 93.38 & 20.38 & 21.76 \\
% Specialized Agent         & Mistral 7B          & 7B & ZS        & 92.91 & 17.37 & 20.05 \\
% \textbf{Specialized Agent} & SelfCheck-NLI       & -  & ZS        & \textbf{87.64} & \textbf{17.45} & \textbf{5.32} \\ \hline

% Contextual Agent          &    Mistral 7B    & 7B  & ZS-CoT      & 93.63& 37.98& 22.51\\
% Contextual Agent          & Llama2-7B-chat        & 7B  & ZS-CoT        & 93.15 &56.88& 1.71  \\
% Contextual Agent          &    Llama3.1-8B-chat   & 8B  & ZS-CoT        & 91.77 &24.89 &14.23 \\
% Contextual Agent          &      Llama2-13B-chat   & 13B  & ZS-CoT     & 92.40 &56.09& 10.80 \\  
% Contextual Agent          &     gpt-3.5-turbo   & -  & ZS-CoT       & 95.28 &57.62 &25.42 \\
% Contextual Agent          &    gpt-4o-mini    & -  & ZS-CoT      & 94.89 &30.58 &30.68 \\
% Contextual Agent          &    Mistral 7B    & 7B & ZS    &92.15 &45.31& 17.19\\
% Contextual Agent          & Llama2-7B-chat        & 7B  & ZS    & 92.99& 49.39& 18.51 \\
% Contextual Agent          &    Llama3-8B-chat   & 8B  & ZS      & 94.79 &37.75 &29.67\\
% Contextual Agent          &      Llama2-13B-chat   & 13B & ZS       & 93.18 &56.90& 2.89\\ 
% Contextual Agent          &     gpt-3.5-turbo   & -  & ZS      & 95.66& 56.95 &30.08 \\
% Contextual Agent          &    gpt-4o-mini    & -  & ZS      & 93.93& 24.26& 21.13 \\
% \hline
% \end{tabular}%
% %}
% \vspace{0.5cm}
% \caption{Consolidated comparison of methods, approaches, sizes, and prompting for both the Wikibio and AIME datasets.}
% \label{tab:consolidated_comparison}
% \end{table*}


\subsection{Experimental Settings}

\paragraph{LLMs}

% We evaluate our approach using a diverse set of LLMs to ensure broad applicability and comprehensive performance analysis. The selected LLMs include cutting-edge architectures, such as GPT4~\cite{DBLP:journals/corr/abs-2303-08774}, GPT3.5~\cite{DBLP:journals/corr/abs-2303-10420}, Phi-3-mini~\cite{DBLP:journals/corr/abs-2404-14219}, Llama 3.1 (8B)~\cite{DBLP:journals/corr/abs-2407-21783}, Mistral (7B)~\cite{DBLP:journals/corr/abs-2310-06825}, and Gemma (7B)~\cite{DBLP:journals/corr/abs-2403-08295}, representing advanced fine-tuned variants of contemporary LLMs. For benchmarking purposes, we use Roberta-large as a baseline, leveraging its well-established performance on a variety of natural language understanding tasks. This diverse selection enables a robust evaluation of the proposed hallucination detection framework across both state-of-the-art and foundational LLM architectures, highlighting strengths and limitations in their factual consistency and response stability.

We evaluate our approach using a diverse range of LLMs to ensure broad applicability and comprehensive performance analysis. The selected LLMs include GPT4~\cite{DBLP:journals/corr/abs-2303-08774}, GPT3.5~\cite{DBLP:journals/corr/abs-2303-10420}, Phi-3-mini~\cite{abdin2024phi}, Llama 3.1 (8B)~\cite{dubey2024llama}, Mistral (7B)~\cite{jiang2023mistral}, and Gemma (7B)~\cite{team2024gemma}, representing advanced different variants of LLMs. For benchmarking, Roberta-large is used as a baseline, leveraging its established performance on natural language understanding tasks. This selection facilitates robust evaluation of the hallucination detection framework, assessing factual consistency and response stability across both state-of-the-art and foundational architectures.


\paragraph{Implementation Details}

We utilized OpenAI's API (December 2024) for GPT-4 and GPT-3.5, generating responses with a temperature of 0.6 to encourage output diversity while capping the maximum token limit at 2048 for efficient processing. This temperature setting balances creativity and coherence, ensuring optimal performance within the token limit. Fine-tuning and additional experiments are conducted on a computational setup comprising one NVIDIA A500 GPU and two NVIDIA 2080Ti GPUs, ensuring robust performance across LLMs. For evaluation, we standardize the temperature to control the randomness, ensuring consistency and reliability in the generated outputs. Hyperparameters are carefully optimized during fine-tuning to enhance stability and generalization across tasks.



\renewcommand{\arraystretch}{0.0}
\begin{table*}[htbp]
\centering

\begin{tabular}{p{0.30\linewidth}p{0.2\linewidth}p{0.2\linewidth}p{0.2\linewidth}}
\rowcolor{gray!20}
\textbf{} & \textbf{Semantic Ngram} & \textbf{NLI} & \textbf{Prompt} \\ \hline

\textbf{Query:} Who was Nikola Tesla, and what were his major contributions to science? \newline
\textbf{LLM Response:} Nikola Tesla was a Serbian-American ...... \newline
\textbf{Sampled Responses:} Tesla advanced AC ......, Tesla invented ......, Tesla's later life ......
& 
\textcolor{blue}{\textbf{Semantic Ngram:}} The Ngram method evaluates LLM responses by using probabilities to compute hallucination scores and assess factuality.
 \newline \newline\textbf{Hallu Score:} 0.33 & 
\textcolor{blue}{\textbf{NLI:}} Sampled passages \( S_n \) merge with \( r_i \), generating logits for entailment, contradiction, or neutral scores.
  \newline \newline \newline \textbf{Hallu Score:} 0.11 & 
\textcolor{blue}{\textbf{Prompt:}} CoT prompting checks context support, classifying sentences as \textrm{``yes''} or \textrm{``no''} and computes hallucination scores.
  \newline \newline\textbf{Hallu Score:} 0.09 
\\ \hline

\textbf{Query:} Who was the first person to propose the theory of evolution, and what was their main idea? \newline
\textbf{LLM Response:} The first person to propose the theory of evolution ...... \newline
\textbf{Sampled Responses:} Charles Darwin introduced ......, Lamarck proposed ......, Mendel's work focused ......

& 
\textcolor{red}{\textbf{Semantic Ngram:}} The Ngram method evaluates LLM responses by using probabilities to compute hallucination scores and assess factuality. \newline \newline\textbf{Hallu Score:} 0.71 & 
\textcolor{red}{\textbf{NLI:}} Sampled passages \( S_n \) merge with \( r_i \), generating logits for entailment, contradiction, or neutral scores.  \newline \newline \newline \textbf{Hallu Score:} 0.89 & 
\textcolor{red}{\textbf{Prompt:}} CoT prompting checks context support, classifying sentences as \textrm{``yes''} or \textrm{``no,''} and computes hallucination scores.  \newline \newline\textbf{Hallu Score:} 0.93 \\ \hline
\end{tabular}
\caption{Examples for SelfCheckAgent: Factually Correct and Non-Factual Cases.}
\label{tab:real_life_example}
\end{table*}


% \renewcommand{\arraystretch}{0.0}
% \begin{table*}[htbp]
% \centering

% \begin{tabular}{p{0.30\linewidth}p{0.2\linewidth}p{0.2\linewidth}p{0.2\linewidth}}
% \rowcolor{gray!20}
% \textbf{Query, LLM Response, and Sampled Response} & \textbf{Semantic Ngram} & \textbf{NLI} & \textbf{Prompt} \\ \hline

% \textbf{Query:} Who was Nikola Tesla, and what were his major contributions to science? \newline
% \textbf{LLM Response:} Nikola Tesla was a Serbian-American ...... \newline
% \textbf{Sampled Responses:} Tesla advanced AC ......, Tesla invented ......, Tesla's later life ......
% & 
% \textcolor{blue}{\textbf{Semantic Ngram:}} The Ngram method evaluates LLM responses by using probabilities to compute hallucination scores and assess actuality.
%  \newline \newline\textbf{Hallu Score:} 0.33 & 
% \textcolor{blue}{\textbf{NLI:}} Sampled passages \( S_n \) merge with \( r_i \), generating logits for entailment, contradiction, or neutral scores.
%   \newline \newline \newline \textbf{Hallu Score:} 0.11 & 
% \textcolor{blue}{\textbf{Prompt:}} CoT prompting checks context support, classifying sentences as "yes" or "no," and computes hallucination scores.
%   \newline \newline\textbf{Hallu Score:} 0.09 
% \\ \hline

% \textbf{Query:} Who was the first person to propose the theory of evolution, and what was their main idea? \newline
% \textbf{LLM Response:} The first person to propose the theory of evolution ...... \newline
% \textbf{Sampled Responses:} Charles Darwin introduced ......, Lamarck proposed ......, Mendel's work focused ......

% & 
% \textcolor{red}{\textbf{Semantic Ngram:}} The Ngram method evaluates LLM responses by using probabilities to compute hallucination scores and assess actuality. \newline \newline\textbf{Hallu Score:} 0.71 & 
% \textcolor{red}{\textbf{NLI:}} Sampled passages \( S_n \) merge with \( r_i \), generating logits for entailment, contradiction, or neutral scores.  \newline \newline \newline \textbf{Hallu Score:} 0.89 & 
% \textcolor{red}{\textbf{Prompt:}} CoT prompting checks context support, classifying sentences as "yes" or "no," and computes hallucination scores.  \newline \newline\textbf{Hallu Score:} 0.93 \\ \hline
% \end{tabular}
% \caption{Examples for SelfCheckAgent: Factually Correct and Non-Factual Cases}
% \label{tab:real_life_example}
% \end{table*}


\paragraph{Baseline Strategy Settings}

% This study investigates the effectiveness of various baseline strategies for hallucination detection in generative LLMs. Our approach includes the evaluation of three distinct agents: (1) \textbf{Symbolic Agent}, which employs semantic n-gram overlap metrics to assess the lexical similarity between generated responses and ground truth, establishing a foundational benchmark for detecting inconsistencies; (2) \textbf{Specialized Detection Agent}, which fine-tunes LoRA~\cite{} parameters using the MNLI dataset and our domain-specific training set to enhance the LLM's ability to identify factual inconsistencies leveraging domain expertise; and (3) \textbf{Contextual Consistency Agent}, which utilizes zero-shot (ZS) and Chain-of-Thought (CoT) prompting to assess the LLM's ability to handle hallucination detection by analyzing its response consistency and coherence under minimal guidance. These strategies aim to systematically explore the strengths and limitations of both classical and advanced methods in addressing hallucinations in generative LLMs.

% In parallel with these baseline strategies, we incorporate a cross-evaluation against SelfCheckGPT~\cite{}. This approach represents a simple yet effective sampling-based method for detecting hallucinations in black-box LLMs without the need for external databases or access to the LLM's output probability distributions. SelfCheckGPT is grounded in the hypothesis that consistent facts in generative responses should align across multiple stochastic samplings, while hallucinated facts tend to diverge and contradict. Our study compares the performance of the baseline strategies against SelfCheckGPT, showcasing its ability to detect non-factual and factual sentences effectively and rank passages based on factuality.

%consistency and coherence in
%without external databases or access to the LLM's output probability distributions.

This study investigates various strategies for hallucination detection. We examine three agents: (1) \textbf{Symbolic Agent}, which uses semantic N-gram overlap to measure lexical similarity between generated responses, establishing a baseline for hallucination detection; (2) \textbf{Specialized Detection Agent}, where a domain-specific dataset utilize to enhance the hallucination detection; and (3) \textbf{Contextual Consistency Agent}, utilizing zero-shot (ZS) and Chain-of-Thought (CoT) prompting to evaluate the hallucination detection under minimal guidance. Additionally, we compare these strategies against SelfCheckGPT~\cite{DBLP:conf/emnlp/ManakulLG23}, a method for detecting hallucinations in black-box LLMs. SelfCheckGPT relies on the consistent facts across multiple samplings align, while hallucinated facts diverge. Our comparison demonstrates SelfCheckAgents's effectiveness in detecting hallucination in factual and non-factual sentences with ranking passages.



\subsection{Results}

\paragraph{Symbolic Agent}

The Symbolic Agent, utilizing semantic N-gram methods, surpasses state-of-the-art performance on both the WikiBio and AIME datasets. On the WikiBio, it achieves the highest NonFactual score of 86.97, a Factual score of 59.02, and a Ranking score of 65.88, consistently outperforming the baseline SelfCheckGPT across all metrics. On the AIME dataset, the Symbolic Agent further demonstrates its superiority with a NonFactual score of 87.24, a Factual score of 14.62, and a Ranking score of 11.94. These findings underscore the effectiveness of the Symbolic Agent's semantic-level processing, enabling it to capture intricate data relationships and ensure advanced factuality and ranking. The robustness of the Symbolic Agent across diverse datasets highlights its potential as a reliable method for factual consistency and ranking in hallucination detection tasks.



% \paragraph{Specialized Detection Agent with pre-train LLM}
%     The Specialized Detection Agent employing pre-trained LLMs showcases varying degrees of effectiveness across the Wikibio and AIME datasets, depending on the LLM and prompting strategy used. Among the evaluated LLMs, Mistral (7B) with CoT prompting achieves the highest performance on the Wikibio dataset, with a NonFactual score of 81.91, a Factual score of 46.10, and a Ranking score of 43.49, outperforming other LLM in this setting. In contrast, Phi-3 (3.8B) with ZS prompting delivers strong results, particularly on the AIME dataset, achieving a NonFactual score of 90.34, a Factual score of 19.16, and a Ranking score of 12.75. However, Llama 3.1 (8B) demonstrates inconsistent performance, with ZS prompting achieving moderate scores (NonFactual: 87.76, Factual: 21.10) but underperforming in CoT prompting (NonFactual: 85.33, Factual: 13.13, Ranking: -4.07). Similarly, Gemma (7B) achieves a remarkable NonFactual score of 90.07 in CoT prompting for the AIME dataset but struggles with lower Factual and Ranking scores (13.00 and 0.50, respectively). Across all configurations, Roberta-large (340M) displays consistent yet less competitive results, with its highest NonFactual score of 73.86 achieved using CoT prompting on the Wikibio dataset. These findings highlight the Specialized Detection Agent's ability to adapt across diverse datasets and its dependency on model architecture, size, and prompting strategy to optimize performance in factual and ranking evaluations.


% \paragraph{Specialized Detection Agent with Fine Tune LLM}
%     The Specialized Detection Agent with fine-tuned LLMs exhibits strong performance across both the Wikibio and AIME datasets, demonstrating the benefits of task-specific optimization. On the Wikibio dataset, Mistral (7B) emerges as the top-performing LLM, achieving exceptional scores of 92.68 for NonFactual, 67.10 for Factual, and 75.63 for Ranking. Phi-3 (3.8B) closely follows, attaining a NonFactual score of 92.87, a Factual score of 65.25, and a Ranking score of 73.54, further underscoring the value of fine-tuning. Roberta-large also performs competitively, achieving a NonFactual score of 92.50, a Factual score of 66.08, and a Ranking score of 74.14, despite its smaller parameter size. In contrast, Llama 3.1 (8B) and Gemma (7B) exhibit moderate performance. Llama 3.1 achieves a NonFactual score of 76.85 and a Factual score of 29.71 on the Wikibio dataset, but its Ranking score remains low at 8.91. Similarly, Gemma achieves a NonFactual score of 83.47 and a Factual score of 43.12, with a Ranking score of 50.98. On the AIME dataset, Phi-3 achieves the highest NonFactual score of 93.38 and the highest Ranking score of 21.76, with a Factual score of 20.38. Mistral also delivers competitive results, with a NonFactual score of 92.91, a Factual score of 17.37, and a Ranking score of 20.05. In comparison, Roberta-large achieves moderate results with a NonFactual score of 87.64, a Factual score of 17.45, and a Ranking score of 5.32, while Llama 3.1 and Gemma underperform, with lower Factual and Ranking scores. These results emphasize the effectiveness of fine-tuning for enhancing LLM performance while highlighting variations across LLMs and datasets.

\paragraph{Specialized Detection Agent}  
The Specialized Detection Agent with pre-trained LLMs exhibits effectiveness across the WikiBio and AIME datasets. Among evaluated LLMs, Mistral (7B) with CoT prompting achieves the best performance in the WikiBio dataset, with NonFactual, Factual, and Ranking scores of 81.91, 46.10, and 43.49, respectively. Phi-3 (3.8B) with ZS prompting get better results on the AIME dataset, achieving NonFactual, Factual, and Ranking scores of 90.34, 19.16, and 12.75. Llama 3.1 (8B) demonstrates moderate performance in ZS prompting but underperforms with CoT. Gemma (7B) achieves better results in the AIME dataset with NonFactual scores using the CoT prompt but struggles in Factual and Ranking. However, the Roberta-large (340M) performs consistently for the NonFactual scores in the WikiBio dataset. These results highlight the importance of LLM architecture, size, and prompting strategies for hallucination detection.

%\paragraph{Specialized Detection Agent with Fine-Tuned LLMs}  
Also, the Specialized Detection Agent with fine-trained LLMs significantly enhances performance across datasets. On WikiBio dataset, Mistral (7B) achieves the highest scores in NonFactual (92.68), Factual (67.10), and Ranking (75.63), closely followed by Phi-3 (3.8B) with scores of 92.87, 65.25, and 73.54, respectively. DeBERTa-v3-large demonstrates strong performance despite its size, with NonFactual, Factual, and Ranking scores of 92.50, 66.08, and 74.14. Conversely, Llama 3.1 (8B) and Gemma (7B) deliver moderate results, with lower Factual and Ranking scores. On the AIME dataset, Phi-3 achieves top scores NonFactual (93.38), Ranking (21.76), and Factual (20.38), while Mistral delivers competitive results. DeBERTa-v3-large performs moderately, while Llama 3.1 and Gemma show limited effectiveness. These findings emphasize the advantages of fine-tuning for improving LLM capabilities across diverse tasks, with SelfCheckGPT serving as the baseline for comparison.


\paragraph{Contextual Consistency Agent}
    % The Contextual Consistency Agent, leveraging prompt-based approaches, demonstrates notable performance across the Wikibio and AIME datasets, highlighting its contextual understanding and consistency capacity. On the Wikibio dataset, gpt4o with ZS prompting achieves the highest NonFactual score of 94.00, a Factual score of 74.11, and a Ranking score of 77.48, surpassing other LLMs in contextual consistency. When utilizing CoT prompting, gpt4o also achieves a NonFactual score of 94.14, a Factual score of 74.95, and a Ranking score of 76.33, maintaining its top-tier performance. On the AIME dataset, gpt4o delivers competitive results, with ZS prompting achieving a NonFactual score of 93.93, a Factual score of 24.26, and a Ranking score of 21.13. With CoT prompting, gpt4o further improves, attaining a NonFactual score of 94.89, a Factual score of 30.58, and a Ranking score of 30.68, demonstrating its ability to adapt effectively to domain-specific complexities. These results underscore the Contextual Consistency Agent's strength in leveraging advanced LLMs and prompting strategies, achieving a high level of factual accuracy and ranking precision in both datasets.

    The Contextual Consistency Agent, employing prompt-based approaches exhibits strong performance across the WikiBio and AIME datasets highlighting its contextual understanding and consistency capabilities. On the WikiBio dataset, GPT-4o with ZS prompting achieves the highest NonFactual score of 94.00, a Factual score of 74.11, and a Ranking score of 77.48. When using CoT prompting, GPT-4o achieved a NonFactual score of 94.14, a Factual score of 74.95, and a Ranking score of 76.33. On the AIME dataset, GPT-4o with ZS prompting achieved a NonFactual score of 93.93, a Factual score of 24.26, and a Ranking score of 21.13. When leveraging CoT prompting, GPT-4o achieved a NonFactual score of 94.89, a Factual score of 30.58, and a Ranking score of 30.68. While SelfCheckGPT serves as the baseline, achieving strong scores using GPT-3.5 with ZS prompting and GPT-4o consistently outperforms in CoT prompting. These findings highlight the Contextual Consistency Agent's effectiveness in leveraging advanced LLMs and prompting strategies to achieve high factual accuracy and ranking precision across datasets.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.24\textwidth]{bar-graph.png} % Adjust width as needed
    \caption{Analysis of LLM capacity impact.}
    \label{fig:LLM_capa_ansl}
\end{figure}


\subsection{Exploring Triangulation for Hallucination}

% This study advances the understanding of hallucination detection in LLMs by systematically evaluating baseline strategies through a triangulation framework. Triangulation, in this context, refers to leveraging diverse methodological perspectives to detect hallucinations holistically. The proposed strategies include (1) a Symbolic Agent, which utilizes semantic n-gram overlap metrics to establish a foundational benchmark for lexical similarity and serve as a baseline for identifying overt inconsistencies; (2) a Specialized Detection Agent, which employs fine-tuned using LoRA on the MNLI dataset and a domain-specific corpus to discern factual inaccuracies with enhanced precision in specialized contexts; and (3) a Contextual Consistency Agent, which explores the efficacy of ZS and CoT prompting for assessing coherence and consistency in LLM generated outputs, even under minimal task-specific guidance. By integrating these diverse approaches, the study demonstrates the value of a triangulation framework for hallucination detection, enabling a comparative analysis of classical symbolic techniques, emergent ZS methods, and advanced domain-specific fine-tuning. This exploration not only identifies the complementary strengths of each approach but also highlights their limitations, paving the way for hybrid strategies that balance symbolic interpretability, contextual reasoning, and domain adaptability. Such insights are critical for designing robust detection mechanisms and mitigating the risks of hallucination in high-stakes applications, making significant contributions to the growing body of research in reliable and trustworthy AI.

In this experiment, we propose a framework for hallucination detection in LLMs by enhancing the baseline strategies within a triangulation framework, which combines diverse methodological perspectives. The strategies include: (1) a Symbolic Agent using semantic N-gram to identify lexical inconsistencies, (2) a Specialized Detection Agent fine-tuned with LoRA and a domain-specific corpus to detect hallucination, and (3) a Contextual Consistency Agent with different prompting to assess hallucination under minimal guidance. This framework enables a comparative analysis of symbolic, specialized, and Contextual Consistency agents, highlighting their complementary strengths and limitations. The insights gained inform hybrid strategies that balance contextual reasoning, and adaptability, contributing to the development of robust detection mechanisms for real life applications.


\subsection{Experimental Analysis}

\paragraph{Examining the Influences of Agent Capacity}

Figure~\ref{fig:LLM_capa_ansl} shows that increasing LLM capacities, especially in specialized detection agents, significantly improves hallucination detection. Specialized detection agents such as Phi-3 and Mistral outperform their pre-trained counterparts. Table~\ref{tab:consolidated_comparison} highlights performance variations across agent capacities, with specialized detection agents consistently surpassing other methods, achieving top NonFactual, Factual, and Ranking scores on both WikiBio and AIME datasets. Among contextual consistency agents, GPT-4o with CoT prompting and GPT-3.5 in ZS prompting show strong performance, emphasizing the importance of LLM size and fine-tuning in hallucination detection.



%Interestingly, ZS outperforms CoT, despite the latter's larger LLM. This performance difference can be attributed to the more effective command prompts that may be more compatible with the transformer's architecture. However, when LLMs are fine-tuned on specialized hallucination detection tasks, the increase in capacity has a more limited impact on performance improvement. This suggests that the potential benefits of scaling up the size of LLMs may plateau once they reach a certain threshold for hallucination detection. Specifically, LLMs such as XXXXX and XXXXX show similar improvements in detecting hallucinations when fine-tuned, indicating that detection efficiency can be more strongly influenced by training data and task-specific optimizations than by sheer LLM size. These findings raise important questions about the diminishing returns of scaling LLM capacity, highlighting the need to explore alternative strategies—such as LLMs ensembling or integrating external knowledge sources—to further enhance hallucination detection capabilities in real-world applications.

% \paragraph{Enhancement Lies in Accurate Evidence}
% Incorporating high-quality evidence significantly enhances hallucination detection. As observed in Table~\ref{tab:consolidated_comparison}, methods leveraging accurate contextual consistency, particularly through prompt-based approaches like GPT-4o with CoT prompting, demonstrate marked improvements in factual and ranking scores. This highlights the pivotal role of precise evidence in mitigating inaccuracies and improving the reliability of generated outputs. \textcolor{red}{add the idea for the AIME dataset problems}
\paragraph{Challenges and Anomalies in Performance}

The specialized detection agent analysis reveals some interesting findings. On the AIME dataset, Roberta-large fails in pre-trained settings due to its limited context window. Also in the AIME dataset, significant gaps between NonFactual (AUC-PR), Factual (AUC-PR), and Ranking (PCC) scores raised the dataset’s imbalance caused because of the LLMs limitation in the complex mathematics domain. Notably, Gemma (7B) in ZS pre-trained LLM matches Phi-3 (3.8B) fine-tuned scores, likely due to Gemma’s robust architecture and the dataset’s imbalanced nature, which limits sensitivity to LLM-specific optimizations.



\paragraph{Real-World Application Case Study}

% We extended the evaluation of the SelfCheckAgent beyond standard benchmark datasets to include real-world instances of hallucinations generated by ChatGPT, demonstrating its applicability in diverse and uncontrolled environments. This extension is critical for assessing the agent's generalizability and practical utility in settings where hallucinations may not align with typical benchmark conditions. The analysis of out-of-distribution cases, presented in Table~\ref{tab:real_life_example}, provides a comprehensive examination of the agent's performance across varied contexts. These real-world tests elucidate the strengths and limitations of SelfCheckAgent, with particular emphasis on its capacity to effectively identify hallucinations in scenarios where expert agents' evaluations show significant divergence. The findings affirm the agent's robust capability in reliably detecting hallucinations, particularly in situations where traditional methods or expert-driven assessments may fall short. This underscores the potential of SelfCheckAgent as a critical tool for enhancing the credibility and trustworthiness of generative LLMs in authentic, real-world settings.

We extended the evaluation of SelfCheckAgent to real-world hallucinations generated by ChatGPT, demonstrating its applicability in uncontrolled environments. This extension assesses the agent's generalizability in scenarios where hallucinations differ from benchmark conditions. Table~\ref{tab:real_life_example} presents examples of real-world cases, highlighting the agent's performance across varied contexts. These real-world tests emphasize SelfCheckAgent's ability to detect hallucinations in cases where expert evaluations are not available. The findings confirm the agent's robustness in reliably identifying hallucinations, even when the traditional framework fails, positioning it as a crucial tool for enhancing the credibility of generative LLMs in real-world applications.

\section{Conclusion and Future Work}
We present a comprehensive framework for hallucination detection in LLMs, emphasizing the interplay between consistency and uncertainty quantification. By leveraging advanced prompting techniques and robust scoring mechanisms, our approach achieves significant improvements across both sentence and passage levels, as validated on the WikiBio and AIME benchmarks. Additionally, we consolidate diverse evaluation strategies, highlighting the influence of agent capacity and accurate evidence integration in minimizing hallucinations. In the future, we aim to expand this framework to address hallucination detection across multimodal data and incorporate adaptive self-refinement mechanisms. This framework proves the broader applicability in real-world scenarios for hallucination detection and enhanced reliability of LLMs.

\section{Data Availability}
    The dataset utilized for this study is accessible through the Hugging Face. Interested readers and researchers can obtain the dataset by visiting the following link:(\url{https://huggingface.co/datasets/tourist800/AIME_Hallucination_Detection}).

\section{Code Availability}
    The study was carried out exclusively using open-source software packages. All scripts, outcomes, post-processed datasets, and features will be accessible to the public at \url{https://github.com/DIYANAPV/SelfCheckAgent}.



%\section{Apendex}







\bibliographystyle{named}
\bibliography{ijcai24}
\end{document}

