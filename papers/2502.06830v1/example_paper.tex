%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


% self defined packages
\usepackage{eurosym}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{OrderFusion: Encoding Orderbook for Probabilistic Intraday Price Prediction}

\begin{document}

\twocolumn[
\icmltitle{OrderFusion: Encoding Orderbook for Probabilistic \texorpdfstring{\\}{ } Intraday Price Prediction}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Runyao Yu}{tud,ait}
\icmlauthor{Yuchen Tao}{rwth}
\icmlauthor{Fabian Leimgruber}{ait}
\icmlauthor{Tara Esterl}{ait}
\icmlauthor{Jochen L. Cremer}{tud,ait}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{tud}{Department of Electrical Sustainable Energy, Delft University of Technology, Delft, The Netherlands}
\icmlaffiliation{ait}{Center for Energy, Austrian Institute of Technology, Vienna, Austria}
\icmlaffiliation{rwth}{Institute for Automotive Engineering, RWTH Aachen, Aachen, Germany}

\icmlcorrespondingauthor{Runyao Yu}{runyao.yu@tudelft.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Intraday Market, Orderbook, Electricity Price, Probabilistic Prediction, Cross-Attention, Quantile Crossing}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Efficient and reliable probabilistic prediction of intraday electricity prices is essential to manage market uncertainties and support robust trading strategies. However, current methods often suffer from parameter inefficiencies, as they fail to fully exploit the potential of modeling interdependencies between bids and offers in the orderbook, requiring a large number of parameters for representation learning. Furthermore, these methods face the \textit{quantile crossing} issue, where upper quantiles fall below the lower quantiles, resulting in unreliable probabilistic predictions.
To address these two challenges, we propose an encoding method called \textit{OrderFusion} and design a hierarchical multi-quantile head. The OrderFusion encodes the orderbook into a 2.5D representation, which is processed by a tailored jump cross-attention backbone to capture the interdependencies of bids and offers, enabling parameter-efficient learning. The head sets the median quantile as an anchor and predicts multiple quantiles hierarchically, ensuring reliability by enforcing monotonicity between quantiles through non-negative functions.
Extensive experiments and ablation studies are conducted on four price indices: 60-min ID$_3$, 60-min ID$_1$, 15-min ID$_3$, and 15-min ID$_1$ using the German orderbook over three years to ensure a fair evaluation. 
The results confirm that our design choices improve overall performance, offering a parameter-efficient and reliable solution for probabilistic intraday price prediction. 

\end{abstract}

\section{Introduction}

The rapid expansion of wind and solar energy in recent years introduces significant variability in power generation due to weather dependence. 
This variability often leads to forecasting errors in wind and solar power output, resulting in power system imbalances with the energy demand \cite{koch2019short}. 
\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=1\linewidth]{Figures/2.5DEncoding.pdf}}
\caption{Comparison of encoding methods for orderbook. 
    \textbf{a}, Orderbook data are flattened into a 1D vector with a dimension of (\( d \times f \times s \)), where \( d \), \( f \),  \( s \) denote for the number of look-back window lengths (explained in~\cref{sec:OrderFusionEncodingMethod}), number of features, and number of sides (bid and offer), respectively. 
    \textbf{b}, Orderbook data are reshaped into a 2D time-series form with a dimension of (\( d, f \times s \)).
    \textbf{c}, Orderbook data are organized into a 3D pseudo image form with a dimension of (\( d \), \( f \), \( s \)).
    \textbf{d}, Orderbook data are split into two 2D matrices, one for each side (bid or offer) with a dimension of (\( d, f \)), to form a ``2.5D'' encoding (ours).}
\label{encodings}
\end{center}
\vskip -0.4in
\end{figure}

The continuous intraday (CID) market plays a pivotal role in addressing this imbalance challenge. The CID market opens at 15:00 the previous day, allowing participants to adjust for unplanned energy imbalances arising from forecast errors in wind and solar power generation, up to five minutes before electricity delivery \cite{Ensemble}.
As a result, the CID market significantly alleviates the demands on balancing energy \cite{Paradox}. With the growing adoption of algorithmic trading in the CID market, intraday price prediction is crucial to managing uncertainties and optimizing trading strategies \cite{Simulation}. 

%Despite the importance of intraday price prediction, the literature remains scarce.
The CID market operates under \textit{weak-form efficiency}. This concept states that recent market prices reflect past publicly available trading information. 
Various studies \cite{Short, Probab, Forecasting, Understanding, Econometric, Ensemble, Simulation} found that while intraday prices are influenced by factors such as wind and solar energy generation, day-ahead forecasts of these factors provide limited predictive power, since the information these factors carry is already reflected in recent market prices. Consequently, the most powerful predictors of future intraday price are extracted from recent trades in the orderbook.


In the context of intraday price prediction, there has been a gradual transition from pointwise prediction to probabilistic prediction. Pointwise prediction models, such as those explored in \cite{Short, Neural, Understanding, beating, Econometric}, estimate a single future price value, are challenged to quantify market uncertainties. To address this limitation, probabilistic prediction models \cite{Probab, Ensemble, Trading, 23Multivariate, 24Multivariate, Simulation} estimate potential price intervals by predicting price quantiles, providing a more nuanced understanding of market uncertainties. 

However, these probabilistic approaches face several challenges. First, they often rely on simplified input representations, such as flattened 1D vectors or 2D time-series formats, which fail to fully capture the bid-offer interdependencies. As a result, learning meaningful representations requires a large number of parameters, leading to parameter inefficiency. Second, these methods frequently encounter quantile crossing issues, where higher quantiles are predicted to be lower than lower quantiles, violating the fundamental properties of probabilistic forecasting \cite{quantilecrossing}. This inconsistency results in unreliable probabilistic predictions, posing a challenge for decision-making in energy trading.

This paper proposes an encoding method called OrderFusion and designs a hierarchical multi-quantile head. OrderFusion converts the orderbook into a 2.5D representation, shown in~\cref{encodings}. A tailored jump cross-attention backbone takes 2.5D encoding as an input to model interdependencies between bid and offer sides. The head sets the median quantile as an anchor, predicts multiple quantiles hierarchically with a shared representation, and ensures that upper quantiles remain higher than lower ones by incorporating monotonic constraints, overcoming quantile crossing issues.
Furthermore, we conduct case studies and ablation studies on four price indices: 60-min ID$_3$, 60-min ID$_1$, 15-min ID$_3$, and 15-min ID$_1$ over three years.

\subsection{Contribution}
\begin{itemize} 

    \item We propose OrderFusion, an encoding method that models interdependencies between bids and offers with a tailored jump cross-attention backbone, enabling parameter-efficient learning.

    \item We design a hierarchical multi-quantile head that sets the median quantile as an anchor and predicts multiple quantiles hierarchically, overcoming the quantile crossing issue and mitigating error accumulation.
    
    \item We conduct experiments and ablation studies to demonstrate the parameter efficiency and reliability of the proposed methods on four price indices over three years.

\end{itemize}

\subsection{Roadmap}
The rest of the paper is organized as follows:~\cref{sec:relatedwork} reviews related works.~\cref{sec:IntrotoData} describes orderbook and price indices.~\cref{sec:OrderFusion} details the proposed methods.~\cref{sec:experiments} presents experiments and ablation studies on four price indices. Finally,~\cref{sec:conclusion} concludes the paper.






\section{Related Work}
\label{sec:relatedwork}

\subsection{Cross-Attention}

Cross-attention has proven to be a powerful mechanism for capturing complex dependencies in sequential and structured data across various fields. In multivariate time series forecasting, it enables the fusion of temporal and static feature embeddings, as demonstrated in \cite{lim2021temporal} and  \cite{zhang2023crossformer}, enhancing predictive performance by modeling intricate relationships between variables. In computer vision, \cite{chen2021crossvit} uses cross-attention to combine multi-scale image patch embeddings, improving classification accuracy. Similarly, in point cloud processing, \cite{afham2022crosspoint} applies cross-attention between 2D and 3D representations to learn richer shape features in a self-supervised manner, while \cite{fei2023dctr} utilizes cross-attention to integrate global and local information. Despite its success in various domains, cross-attention remains underexplored in intraday price prediction. Our proposed OrderFusion tailored with a jump cross-attention backbone aims to model the bid-offer interdependencies. 

\begin{algorithm}[tb]
   \caption{Orderbook Filtering}
   \label{alg:orderbookfiltering}
\begin{algorithmic}
   \STATE {\bfseries Input:} Raw orderbook files
   \STATE Initialize data structures for storing processed orders
   \FORALL{orderbook files}
      \STATE Load orderbook
      \STATE Classify orders by product type (hourly/quarter-hourly)
      \IF{hourly products exist}
         \STATE Group records by OrderID and transaction time
         \STATE Filter entries with action codes ``P'' or ``M'' 
         \STATE Compute traded volume between consecutive updates
         \STATE Aggregate processed hourly trades
      \ENDIF
      \IF{quarter-hourly products exist}
         \STATE Apply the same processing steps as hourly products
         \STATE Aggregate processed quarter-hourly trades
      \ENDIF
   \ENDFOR
   \STATE \textbf{Return} Aggregated and filtered orderbook
\end{algorithmic}
\end{algorithm}





\begin{figure*}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=1\textwidth]{Figures/DataAnalysis.pdf}}
\caption{Distribution and seasonal patterns of four intraday price indices. 
\textbf{a}, The distribution of 15-min ID$_3$ exhibits a noticeable shift from 2022 to 2023 and 2024, indicating increasing price stability within the normal range.
\textbf{b}, The boxplot reveals a seasonal pattern of 15-min ID$_3$ each year, with price fluctuations varying by quarter.
\textbf{c}, The count of negative prices steadily rises over the years, reflecting the growing influence of renewable energy integration in the market.}
\label{datanalysis}
\end{center}
\vskip -0.3in
\end{figure*}

\subsection{Multi-Quantile Prediction and Quantile Crossing}

Multi-quantile prediction frameworks are becoming increasingly popular to capture uncertainties in price forecasts. The studies \cite{MultiQ, GQFormer} aim to reduce the complexity of training by jointly predicting several quantiles from a shared representation. However, a well-known issue is quantile crossing, where upper quantiles occasionally yield lower values than lower quantiles. This inconsistency violates the fundamental property of cumulative distribution functions and can drastically reduce the reliability of interval forecasts \cite{quantilecrossing}.
Previous works attempt to fix crossing errors via post-processing methods such as simply re-sorting quantiles \cite{sortquantilecrossing1, sortquantilecrossing2, Trading}.
Although straightforward, such solutions risk distorting the learned distribution by imposing an artificial correction step. 
\cite{LQF} introduces an incremental quantile function that anchors at the lowest quantile and employs non-negative functions, such as ReLU or Softplus, to learn positive residuals, which are then hierarchically added until reaching the highest quantile. However, this approach is prone to error accumulation through the process of iterative addition. Drawing inspiration from this design, we anchor at the median quantile and apply addition and subtraction to estimate tail quantiles,  reducing the risk of error accumulation.




\section{Data}
\label{sec:IntrotoData}

\subsection{Orderbook}
\label{sec:orderbook}

The orderbook data purchased from the European Power Exchange Spot (EPEX Spot) provide detailed insights into electricity market dynamics. Specifically, the orderbook records trading activities with key attributes including \textit{delivery start time of electricity}, \textit{type of product}, \textit{price}, \textit{volume}, \textit{side}, \textit{OrderID}, \textit{transaction time}, and \textit{action code}.  

The delivery start time of electricity marks the time at which electricity is physically delivered. Each distinct delivery start time corresponds to a unique  \textit{product}. For example, electricity deliveries starting at 08:00 and 09:00 are different products. In addition, there are several types of products, such as hourly (60-min) and quarter-hourly (15-min). Hourly products have delivery start times at standard hourly intervals, such as 10:00, 11:00, \dots, while quarter-hourly products start at finer intervals, such as 10:00, 10:15, \dots. 

Up to five minutes before each delivery start time, traders can place orders, specifying price (\EUR/MWh), volume (MWh), and side (bid or offer) with an OrderID assigned to the trader. If a bid or offer is matched, the transaction time is recorded and an action code is assigned: ``M'' (fully matched) or ``P'' (partially matched). A fully matched order means that the entire volume is traded, rendering the order inactive for future transactions. A partially matched order indicates that only a portion of the volume is executed, while the unexecuted volume remains in the system and can still be matched with other traders. The matching rule is introduced at a high level in \cref{alg:matchingrule}.

Furthermore, we filter out unmatched orders, as they do not contribute to market-clearing outcomes. The pseudocode of the filtering algorithm is described in \cref{alg:orderbookfiltering}. 





\begin{figure*}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=1\linewidth]{Figures/JumpFusion.pdf}}
\caption{Comparison of different backbones.
\textbf{a} No cross-attention. The inputs $\mathbf{M}^{(b)}$ and $\mathbf{M}^{(o)}$ independently pass through a 1D-conv layer, producing $\mathbf{C}_k^{(b)}$ and $\mathbf{C}_k^{(o)}$, respectively. Each 1D-conv layer transforms the input shape to $(d, F)$. The outputs $\mathbf{C}_k^{(b)}$ and $\mathbf{C}_k^{(o)}$ then serve as inputs to the next block, generating $\mathbf{C}_{k+1}^{(b)}$ and $\mathbf{C}_{k+1}^{(o)}$, and so on.
\textbf{b} Standard cross-attention backbone. Initially follows the same process as block \textbf{a} to obtain $\mathbf{C}_k^{(b)}$ and $\mathbf{C}_k^{(o)}$. Then, $\mathbf{C}_k^{(b)}$ serves as the query, and $\mathbf{C}_k^{(o)}$ as the key and value in a cross-attention layer to produce $\mathbf{A}_k^{(b)}$. Then, the roles of query, key, and value are exchanged to generate $\mathbf{A}_k^{(o)}$. This procedure is repeated for the next block to obtain $\mathbf{A}_{k+1}^{(b)}$ and $\mathbf{A}_{k+1}^{(o)}$, and so on.
\textbf{c} Jump cross-attention backbone. Initially follows the same process as block \textbf{b} to compute $\mathbf{A}_k^{(b)}$ and $\mathbf{A}_k^{(o)}$. Then, $\mathbf{A}_k^{(b)}$ and $\mathbf{A}_k^{(o)}$ independently pass through a 1D-conv layer to obtain $\mathbf{C}_{k+1}^{(b)}$ and $\mathbf{C}_{k+1}^{(o)}$, respectively. Unlike \textbf{b}, where $\mathbf{A}_{k+1}^{(b)}$ is obtained by processing $\mathbf{C}_{k+1}^{(b)}$ as the query and \underline{$\mathbf{C}_{k+1}^{(o)}$} as the key and value, the jump cross-attention block instead utilizes $\mathbf{C}_{k+1}^{(b)}$ as the query and \underline{$\mathbf{C}_k^{(o)}$} as the key and value to produce $\mathbf{A}_{k+1}^{(b)}$. Similarly, $\mathbf{A}_{k+1}^{(o)}$ is obtained by exchanging the roles of query, key, and value. This procedure continues for subsequent blocks. The design encourages the model to incorporate current and earlier fusion results rather than focusing solely on the current fusion step.}
\label{JumpCrossAttention}
\end{center}
\vskip -0.1in
\end{figure*}





\subsection{Price Indices}
We focus on four popular price indices: 60-min ID$_3$, 60-min ID$_1$, 15-min ID$_3$, and 15-min ID$_1$. The 60-min and 15-min indices correspond to different product types, where electricity is delivered every 60 min and 15 min, respectively. The ID$_3$ index represents the volume-weighted average price (VWAP) of all filtered trades executed within the last 3 trading hours before delivery, focusing on the most liquid period of a trading session. The ID$_1$ index is calculated as the VWAP of all filtered trades executed within the final trading hour before delivery, capturing the market's last-minute imbalance needs. The distribution and seasonal patterns of the four indices can be seen in~\cref{datanalysis}. We formulate the intraday price index  \( \mathrm{ID}_x \) as:
\begin{equation}
\mathrm{ID}_x
\;=\;
\frac{\sum\limits_{s \in \{b, o\}} \sum\limits_{t \in [t_1, \, t_2]} P^{(s)}_t  V^{(s)}_t}
     {\sum\limits_{s \in \{b, o\}} \sum\limits_{t \in [t_1, \, t_2]}  V^{(s)}_t},
\end{equation}
where \(s\) indicates the market side, with \( s \in \{b, o\} \) representing the bid (b) and offer (o) sides. Here, \( P^{(s)}_t \) and \( V^{(s)}_t \) denote the price and volume, respectively. 
\( t_1 \) represents the prediction time (at which we predict future prices), while \( t_2 \) denotes the delivery start time of electricity, introduced in~\cref{sec:orderbook}.

Moreover, relationship between \( t_1 \) and \( t_2 \) is given by:
\begin{equation}
t_1 = t_2 - \Delta
\end{equation}
where \( \Delta = 60 \times x \) min, with \( x = 1 \) for \(\mathrm{ID}_1\) and \( x = 3 \) for \(\mathrm{ID}_3\).




















\section{OrderFusion Network}
\label{sec:OrderFusion}

\subsection{Encoding}
\label{sec:OrderFusionEncodingMethod}

The OrderFusion encoding method extracts features from the filtered orderbook and forms a 2.5D representation. In detail, given a prediction time $t_1$ and a side \(s\), we extract four features from a set of price and volume data \( \{P^{(s)}_t, V^{(s)}_t\} \) within the time interval \( [t_1 - \nabla, t_1) \):
\begin{itemize}
    \item Minimum price: \( p_{\min}^{(\nabla)} \)
    \item Maximum price: \( p_{\max}^{(\nabla)} \)
    \item VWAP: \( \text{VWAP}^{(\nabla)} \)
    \item Total traded volume: \( v_{\text{sum}}^{(\nabla)} \)
\end{itemize}
where \( \nabla \) represents look-back window length. Specifically, we consider six window lengths:
\begin{equation}
    \nabla \in \{1\ \text{min},\ 5\ \text{min},\ 15\ \text{min},\ 60\ \text{min},\ 180\ \text{min},\ \nabla_{\text{full}}\}
\end{equation}
where $\nabla_{\text{full}}$ denotes the complete history from market opening (15:00 on the previous day) to $t_1$.

This results in two separate feature maps (2.5D encoding) $\mathbf{M}^{(b)}, \mathbf{M}^{(o)} \in \mathbb{R}^{d \times f}$ for bid and offer, respectively. 
Here, \( d = |\nabla| = 6 \) represents the number of look-back window lengths, while \( f = 4 \) corresponds to the number of extracted features per window, shown in \cref{encodings}. 





\begin{figure*}[ht]
\vskip 0.1in
\begin{center}

\centerline{\includegraphics[width=1\textwidth]{Figures/MultiQuantileHead.pdf}}
\caption{Comparison of different multi-quantile prediction architectures.
\textbf{a}, Standard multi-quantile head. Each quantile is predicted using separate dense layers from a shared representation, leading to potential quantile crossing issues.
\textbf{b}, Hierarchical multi-quantile head introduced in \cite{LQF}: Anchored at the lowest quantile (Q5), and subsequent quantiles are computed hierarchically by adding a non-negative residual learned from a shared representation. This structure ensures monotonicity but may suffer from error accumulation.
\textbf{c}, Hierarchical multi-quantile head (ours): Anchored at the median quantile (Q50), with both higher and lower quantiles computed hierarchically via non-negative residuals. This approach prevents quantile crossing while mitigating error accumulation.}
\label{multiquantilehead}
\end{center}
\vskip -0.1in
\end{figure*}




\subsection{Backbone}

We introduce a tailored jump cross-attention backbone that takes the 2.5D encoding obtained via OrderFusion as input to model interdependencies between bids and offers. Specifically, the jump cross-attention backbone consists of several 1D-convolutional layers and cross-attention layers. 

\paragraph{1D-Convolution}
We denote $\mathbf{Z}_k^{(s)}$ as the input representation, where $\mathbf{Z}_k^{(s)}$ can be either the bid representation ($s = b$) or the offer representation ($s = o$), and $k$ represents the $k$th  layer:
\begin{equation}
    \mathbf{Z}_k^{(s)} =
    \begin{cases}
        \mathbf{M}^{(s)}, & \text{if } k = 0 \\
        \mathbf{A}_k^{(s)}, & \text{otherwise}
    \end{cases}
\end{equation}
where $\mathbf{A}_k^{(s)}$ denotes the output from the cross-attention layer, which will be introduced shortly.

We employ a filter size of $F$, a kernel size of 1, a stride of 1, and no pooling. The transformation is expressed as:
\begin{equation} 
    \mathbf{C}_{k+1}^{(s)} = \text{Conv1D}(\mathbf{Z}_k^{(s)}) 
\end{equation}
where $\mathbf{C}_{k+1}^{(s)} \in \mathbb{R}^{d \times F}$ is the transformed bid or offer feature representations through convolution.



\paragraph{Cross-Attention}  
The cross-attention mechanism takes the bid and offer representations learned from 1D-conv layers as inputs. The first input (either bid or ask) is used to project the query, while the second input (another side) is utilized to project the key and value, respectively. When $k$ is even, the second input is ``jumped'' to the $k-1$th 1D-conv layer, as illustrated in~\cref{JumpCrossAttention}. Precisely:
\begin{equation}
\label{eq:cross_attention_layer}
    \mathbf{A}_{k}^{(s)} =
    \begin{cases}
        \text{CrossAttention}(\mathbf{C}_k^{(s)}, \mathbf{C}_{k-1}^{(\bar{s})}) , & \text{if } k \text{ is even}, \\
        \text{CrossAttention}(\mathbf{C}_k^{(s)}, \mathbf{C}_k^{(\bar{s})}) , & \text{otherwise}.
    \end{cases}
\end{equation}
where $\bar{s}$ denotes the opposite side. For example, if $s = b$, then $\bar{s} = o$, and vice versa. 

In detail, the $\text{CrossAttention}$ operation is based on the multi-head attention (MHA) mechanism. 
Given input representations $\mathbf{C}^{(s)} \in \mathbb{R}^{d \times F}$ and $\mathbf{C}^{(\bar{s})} \in \mathbb{R}^{d \times F}$, the query, key, and value matrices for each attention head are computed as: 
\begin{equation} 
\mathbf{Q}^{(s)}[h] = \mathbf{C}^{(s)} \mathbf{W}_Q^{(s)}[h], 
\end{equation} 
\begin{equation} \mathbf{K}^{(\bar{s})}[h] = \mathbf{C}^{(\bar{s})} \mathbf{W}_K^{(\bar{s})}[h], \end{equation} 
\begin{equation} \mathbf{V}^{(\bar{s})}[h] = \mathbf{C}^{(\bar{s})} \mathbf{W}_V^{(\bar{s})}[h]\end{equation} 
where $\mathbf{W}_Q^{(s)}[h], \mathbf{W}_K^{(\bar{s})}[h], \mathbf{W}_V^{(\bar{s})}[h] \in \mathbb{R}^{F \times d_h}$ are trainable projection matrices for the $h$-th attention head, and $d_h = F / H$ is the head dimension for $H$ attention heads. 
The scaled dot-product attention is applied: \begin{equation} \mathbf{A}^{(s)}[h] = \text{softmax} \left( \frac{\mathbf{Q}^{(s)}[h] \mathbf{K}^{(\bar{s})\top}[h]}{\sqrt{d_h}} \right) \mathbf{V}^{(\bar{s})}[h]. \end{equation} 
The outputs of all attention heads are concatenated and linearly projected: \begin{equation} 
\mathbf{A}^{(s)} = (\mathbf{A}^{(s)}[1] \parallel \mathbf{A}^{(s)}[2] \parallel \dots  \parallel \mathbf{A}^{(s)}[H]) \mathbf{W}^O, 
\end{equation} 

where $\mathbf{W}^O \in \mathbb{R}^{F \times F}$ is the projection matrix, and $\parallel$ denotes concatenation.

The output $\mathbf{A}^{(s)}$ has a special interpretation: it represents the augmented attention feature for side $s$, contextualized by information from the opposite side $\bar{s}$. 
The formulation has intuition from game theory, where buyers adjust their bids based on information from the seller's side, and vice versa. By incorporating cross-side dependencies, this mechanism allows the model to capture strategic interactions between bid and offer representations. Moreover, the equation introduced in \eqref{eq:cross_attention_layer} allows the model to not only extract interdependencies from the current layer but also focus on the previous layer, enabling deeper feature interactions. 

The final feature vector $\mathbf{U} \in \mathbb{R}^{2dF}$ is formed by concatenating flattened representations of outputs from jump cross-attention blocks, where $2dF$ derives from two $d \times F$ matrices.




\subsection{Head}

We design a hierarchical multi-quantile head that estimates a set of quantiles 
(Q5, Q25, Q45, Q50, Q55, Q75, Q95) hierarchically and overcomes the quantile crossing issue. The structure learns the median quantile (\( \tau = 0.5 \)) from the shared representation $\mathbf{U} \in \mathbb{R}^{2dF}$ with one dense layer, shown in~\cref{multiquantilehead}. For quantiles above the median (\( \tau > 0.5 \)), the residuals are predicted using the same shared representation with other dense layers, denoted as \( r_{\tau'} \), which are enforced to be non-negative using a non-negative function \( g(x) \), e.g., ReLU. The upper quantile predictions (\( \hat{y}_{\tau'} \)) are then computed iteratively by adding the non-negative residuals to the prediction of the preceding quantile (\( \tau \)):
\begin{equation}
    \hat{y}_{\tau'} = \hat{y}_{\tau} + g(r_{\tau'}),
\end{equation}
where \( \tau \) is the nearest smaller quantile.

For quantiles below the median (\( \tau < 0.5 \)), the residuals \( r_{\tau'} \) are similarly enforced to be non-negative. The lower quantile predictions (\( \hat{y}_{\tau'} \)) are computed iteratively by subtracting the non-negative residuals from the prediction of the preceding quantile (\( \tau \)):
\begin{equation}
    \hat{y}_{\tau'} = \hat{y}_{\tau} - g(r_{\tau'}),
\end{equation}
where \( \tau \) is the nearest larger quantile.



 









\begin{table*}[t]
\caption{Performance metrics for different methods and configurations (mean±std) on four price indices (60-min ID$_3$, 60-min ID$_1$, 15-min ID$_3$, and 15-min ID$_1$) with 5 runs. Five methods (MLP, LSMT, GRU, CNN, and ours) are compared. The best results are shown in \textbf{bold}, and the second-best results are \underline{underlined}. Metrics include \(\mathrm{AQL}\) and \(\mathrm{AQCR}\) for probabilistic evaluation across all quantiles, and \(\mathrm{RMSE}\), \(\mathrm{MAE}\), and \(\mathrm{R}^2\) for pointwise evaluation of the median quantile. The total number of parameters of each model is identically constrained to $10^5$ for a fair comparison.}
\label{performance-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llccccc}
\toprule
Price Index & Model & \(\mathrm{AQL}\) $\downarrow$ & \(\mathrm{AQCR}\) $\downarrow$ & \(\mathrm{RMSE}\) $\downarrow$ & \(\mathrm{MAE}\) $\downarrow$ & \(\mathrm{R}^2\) $\uparrow$ \\
\midrule
60-min ID$_3$ 
& MLP & 5.01±0.07 & 36.62±0.92 & 40.13±0.60 & 12.61±0.24 & 0.77±0.01 \\
& LSTM & 5.20±0.18 & 15.40±16.24 & 41.20±1.40 & 13.11±0.42 & 0.76±0.02 \\
& GRU & 4.96±0.06 & \underline{12.48±4.20} & \underline{39.86±0.09} & \underline{12.47±0.14} & \underline{0.77±0.00} \\
& CNN & \underline{4.81±0.04} & 16.60±2.31 & 39.89±0.40 & 11.99±0.15 & 0.77±0.00 \\
& Ours & \textbf{4.68±0.03} & \textbf{0.00±0.00} & \textbf{39.30±0.13} & \textbf{11.81±0.06} & \textbf{0.78±0.00} \\

\midrule
60-min ID$_1$ 
& MLP & 5.66±0.09 & 40.50±2.53 & 59.53±0.77 & 14.22±0.16 & 0.68±0.01 \\
& LSTM & 6.07±0.14 & \underline{11.60±6.03} & 62.69±3.24 & 15.00±0.38 & 0.64±0.04 \\
& GRU & 5.77±0.05 & 13.60±7.39 & 61.33±1.13 & 14.47±0.11 & 0.66±0.01 \\
& CNN & \underline{5.41±0.06} & 26.27±4.32 & \underline{59.51±0.41} & \underline{13.67±0.14} & \underline{0.68±0.00} \\
& Ours & \textbf{5.37±0.04} & \textbf{0.00±0.00} & \textbf{59.02±0.77} & \textbf{13.50±0.06} & \textbf{0.68±0.01} \\

\midrule
15-min ID$_3$ 
& MLP & 22.17±2.02 & 78.97±4.97 & 97.56±10.93 & 53.81±12.20 & 0.16±0.18 \\
& LSTM & 17.91±3.24 & \underline{38.41±20.38} & 90.71±7.45 & 43.35±4.58 & 0.28±0.12 \\
& GRU & \underline{16.28±3.96} & 49.14±26.72 & \underline{84.17±6.49} & \underline{41.15±9.34} & \underline{0.38±0.09} \\
& CNN & 16.51±2.78 & 77.36±1.28 & 89.46±12.76 & 46.53±17.22 & 0.29±0.21 \\
& Ours & \textbf{8.11±0.04} & \textbf{0.00±0.00} & \textbf{71.73±1.07} & \textbf{20.33±0.10} & \textbf{0.55±0.01} \\

\midrule
15-min ID$_1$ 
& MLP & 26.94±1.37 & 88.37±8.47 & 116.14±10.48 & 63.51±13.63 & 0.20±0.14 \\
& LSTM & \underline{18.81±1.30} & \underline{63.10±6.06} & \underline{96.22±1.80} & \underline{40.96±3.56} & \underline{0.45±0.02} \\
& GRU & 20.07±3.13 & 72.24±4.43 & 102.12±9.13 & 46.52±4.93 & 0.38±0.11 \\
& CNN & 20.60±1.41 & 86.05±5.59 & 102.59±5.40 & 48.36±6.53 & 0.38±0.06 \\
& Ours & \textbf{12.93±4.49} & \textbf{0.00±0.00} & \textbf{88.73±6.79} & \textbf{32.12±11.16} & \textbf{0.53±0.07} \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vskip -0.1in
\end{center}
\end{table*}








\subsection{Loss}

\label{loss}

%\paragraph{Quantile Loss}
Average quantile loss (\(\mathrm{AQL}\)) is employed to estimate conditional quantiles of the target distribution. For a given quantile level \( \tau \in (0, 1) \), the quantile loss \( L_\tau \) is defined as:
\begin{equation}
    L_\tau(y_i, \hat{y}_{\tau, i}) = 
    \begin{cases} 
      \tau \cdot (y_i - \hat{y}_i), & \text{if } y_i \geq \hat{y}_i, \\
      (1 - \tau) \cdot (\hat{y}_i - y_i), & \text{if } y_i < \hat{y}_i,
    \end{cases}
\end{equation}
where \( y_i \) is the true value and \( \hat{y}_i \) is the predicted quantile for the \( i \)-th sample. This loss penalizes over-predictions and under-predictions differently depending on the quantile level \( \tau \). When predicting upper quantiles (\( \tau > 0.5 \)), higher penalties are applied to under-predictions, whereas for lower quantiles (\( \tau < 0.5 \)), over-predictions incur higher penalties.

Since our model employs a multi-task learning framework, the \(\mathrm{AQL}\) is computed as the mean quantile loss across all samples and quantiles:
\begin{equation}
    \mathrm{AQL} = \frac{1}{|\mathcal{Q}| N} \sum_{\tau \in \mathcal{Q}} \sum_{i=1}^N L_\tau(y_i, \hat{y}_{\tau, i}),
\end{equation}
where \( \mathcal{Q} \) represents the set of quantiles being predicted, and \( N \) is the total number of samples. Lower \(\mathrm{AQL}\) values indicate better overall performance in quantile prediction.








\subsection{Other Details}
\label{sec:details}
We use the Adam optimizer \cite{KingBa15}, with an initial learning rate of \( 3 \times 10^{-4} \),  which decays exponentially at a rate of 0.7 every 10 epochs. 
The number of training epochs is set to 50, from which we select the best model with the lowest validation loss. 
The batch size is configured as 2048 to maximize the usage efficiency of A100 GPU. 
The activation function employed in the backbone is Swish \cite{ramachandran2017searching}. 


\section{Experiments}
\label{sec:experiments}






\subsection{Data Splitting}

The orderbook data are split into training, validation, and testing. The training period spans from January 2022 to December 2023, the validation period covers January 2024 to June 2024, and the testing period is set from July 2024 to December 2024.

\subsection{Rolling-Window Approach}
Predictions are made with a rolling window approach tailored to the granularity of the target price indices. For 15-minute price indices (15-min ID$_3$ and 15-min ID$_1$), predictions are generated every 15 minutes. For 60-minute price indices (60-min ID$_3$ and 60-min ID$_1$), predictions are generated on an hourly basis. If the prediction target is ID$_3$, the prediction is made 3 hours in advance, while for ID$_1$, predictions are made 1 hour before the delivery time. 






\begin{table*}[t]
\caption{Ablation studies for three backbones (No cross-attention, Standard cross-attention, Jump cross-attention)}
\label{ablationbackbone}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llccccc}
\toprule
Price Index & Cross-Attention & \(\mathrm{AQL}\) $\downarrow$ & \(\mathrm{AQCR}\) $\downarrow$ & \(\mathrm{RMSE}\) $\downarrow$ & \(\mathrm{MAE}\) $\downarrow$ & \(\mathrm{R}^2\) $\uparrow$ \\
\midrule
60-min ID$_3$ 
& None & \underline{4.73\(\pm\)0.11} & \textbf{0.00\(\pm\)0.00} & \underline{39.40\(\pm\)0.10} & \underline{11.92\(\pm\)0.14} & \underline{0.78\(\pm\)0.00} \\
& Standard & 4.77\(\pm\)0.04 & \textbf{0.00\(\pm\)0.00} & 39.84\(\pm\)0.20 & 11.99\(\pm\)0.08 & 0.77\(\pm\)0.00 \\
& Jump & \textbf{4.68\(\pm\)0.03} & \textbf{0.00\(\pm\)0.00} & \textbf{39.30\(\pm\)0.13} & \textbf{11.81\(\pm\)0.06} & \textbf{0.78\(\pm\)0.00} \\
\midrule
60-min ID$_1$ 
& None & \textbf{5.34\(\pm\)0.10} & \textbf{0.00\(\pm\)0.00} & 60.72\(\pm\)0.78 & \underline{13.51\(\pm\)0.15} & \underline{0.67\(\pm\)0.01} \\
& Standard & 5.53\(\pm\)0.02 & \textbf{0.00\(\pm\)0.00} & \underline{60.36\(\pm\)0.74} & 13.86\(\pm\)0.06 & 0.67\(\pm\)0.01 \\
& Jump & \underline{5.37\(\pm\)0.04} & \textbf{0.00\(\pm\)0.00} & \textbf{59.02\(\pm\)0.77} & \textbf{13.50\(\pm\)0.06} & \textbf{0.68\(\pm\)0.01} \\
\midrule
15-min ID$_3$ 
& None & 12.41\(\pm\)1.59 & \textbf{0.00\(\pm\)0.00} & 75.43\(\pm\)0.55 & \underline{27.11\(\pm\)1.57} & \underline{0.51\(\pm\)0.01} \\
& Standard & \underline{9.06\(\pm\)0.62} & \textbf{0.00\(\pm\)0.00} & \underline{73.76\(\pm\)0.83} & 22.92\(\pm\)1.88 & 0.53\(\pm\)0.01 \\
& Jump & \textbf{8.11\(\pm\)0.04} & \textbf{0.00\(\pm\)0.00} & \textbf{71.73\(\pm\)1.07} & \textbf{20.33\(\pm\)0.10} & \textbf{0.55\(\pm\)0.01} \\
\midrule
15-min ID$_1$ 
& None & 15.85\(\pm\)3.25 & \textbf{0.00\(\pm\)0.00} & 90.20\(\pm\)4.31 & 36.55\(\pm\)8.32 & 0.52\(\pm\)0.05 \\
& Standard & \underline{13.51\(\pm\)0.84} & \textbf{0.00\(\pm\)0.00} & \textbf{87.92\(\pm\)2.18} & \textbf{31.23\(\pm\)3.24} & \textbf{0.54\(\pm\)0.02} \\
& Jump & \textbf{12.93\(\pm\)4.49} & \textbf{0.00\(\pm\)0.00} & \underline{88.73\(\pm\)6.79} & \underline{32.12\(\pm\)11.16} & \underline{0.53\(\pm\)0.07} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vskip -0.1in
\end{center}
\end{table*}
\begin{table*}[h]
\caption{Ablation studies for three heads (Standard, Hierarchical anchored at Q5, and Hierarchical anchored at Q50)}
\label{ablationhead}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llccccc}
\toprule
Price Index & Model & \(\mathrm{AQL}\) $\downarrow$ & \(\mathrm{AQCR}\) $\downarrow$ & \(\mathrm{RMSE}\) $\downarrow$ & \(\mathrm{MAE}\) $\downarrow$ & \(\mathrm{R}^2\) $\uparrow$ \\
\midrule
60-min ID$_3$ 
& Standard & \underline{4.70\(\pm\)0.04} & 2.12\(\pm\)2.84 & \textbf{39.20\(\pm\)0.10} & \underline{11.85\(\pm\)0.09} & \textbf{0.78\(\pm\)0.00} \\
& Hier. Q5 & 4.74\(\pm\)0.06 & \textbf{0.00\(\pm\)0.00} & \underline{39.27\(\pm\)0.16} & 11.89\(\pm\)0.09 & \underline{0.78\(\pm\)0.00} \\
& Hier. Q50 & \textbf{4.68\(\pm\)0.03} & \textbf{0.00\(\pm\)0.00} & 39.30\(\pm\)0.13 & \textbf{11.81\(\pm\)0.06} & \textbf{0.78\(\pm\)0.00} \\
\midrule
60-min ID$_1$ 
& Standard & \textbf{5.32\(\pm\)0.03} & 3.81\(\pm\)2.24 & \textbf{57.74\(\pm\)0.44} & \textbf{13.44\(\pm\)0.08} & \textbf{0.70\(\pm\)0.01} \\
& Hier. Q5 & 5.41\(\pm\)0.11 & \textbf{0.00\(\pm\)0.00} & \underline{58.45\(\pm\)0.76} & 13.53\(\pm\)0.20 & \underline{0.69\(\pm\)0.01} \\
& Hier. Q50 & \underline{5.37\(\pm\)0.04} & \textbf{0.00\(\pm\)0.00} & 59.02\(\pm\)0.77 & \underline{13.50\(\pm\)0.06} & 0.68\(\pm\)0.01 \\
\midrule
15-min ID$_3$ 
& Standard & 8.19\(\pm\)0.12 & 3.61\(\pm\)6.75 & \textbf{71.07\(\pm\)1.35} & 20.48\(\pm\)0.28 & \textbf{0.56\(\pm\)0.02} \\
& Hier. Q5 & \underline{8.18\(\pm\)0.05} & \textbf{0.00\(\pm\)0.00} & 71.82\(\pm\)0.70 & \underline{20.43\(\pm\)0.13} & \underline{0.55\(\pm\)0.01} \\
& Hier. Q50 & \textbf{8.11\(\pm\)0.04} & \textbf{0.00\(\pm\)0.00} & \underline{71.73\(\pm\)1.07} & \textbf{20.33\(\pm\)0.10} & \underline{0.55\(\pm\)0.01} \\
\midrule
15-min ID$_1$ 
& Standard & \textbf{12.36\(\pm\)2.01} & 22.77\(\pm\)17.33 & \textbf{86.37\(\pm\)4.51} & \textbf{29.33\(\pm\)3.90} & \textbf{0.56\(\pm\)0.05} \\
& Hier. Q5 & 15.89\(\pm\)7.17 & \textbf{0.00\(\pm\)0.00} & 93.86\(\pm\)19.28 & 39.90\(\pm\)20.92 & 0.46\(\pm\)0.24 \\
& Hier. Q50 & \underline{12.93\(\pm\)4.49} & \textbf{0.00\(\pm\)0.00} & \underline{88.73\(\pm\)6.79} & \underline{32.12\(\pm\)11.16} & \underline{0.53\(\pm\)0.07} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\vskip -0.1in
\end{center}
\end{table*}









\subsection{Benchmarks}

\paragraph{Encoding Methods}
We compare 1D, 2D, 3D, and our proposed 2.5D encoding method (OrderFusion) with different backbones. The 1D encoding flattens feature representations and applies a Multi-Layer Perceptron (MLP) as the backbone. The 2D encoding adopts a time-series format, where Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers process sequential data. The 3D encoding converts the data into a 3D pseudo-image representation and utilizes a 2D convolutional backbone. The proposed OrderFusion method transforms the orderbook into a 2.5D encoding and a tailored jump cross-attention backbone is used.



For a fair comparison, hyperparameters such as optimizer, learning rate, training epochs, batch size, and activation function are set the same for all benchmarks, as described in~\cref{sec:details}. Furthermore, the total number of parameters for each model is controlled to remain identical. Additional details of the benchmarks are provided in Appendix~\ref{sec:benchmarkmodels}.





\subsection{Evaluation Metrics}

\paragraph{Probabilistic Prediction}
We evaluate the testing performance of probabilistic prediction using \(\mathrm{AQL}\), described in~\cref{loss},  and the average quantile crossing rate (\(\mathrm{AQCR}\)), shown below:


\(\mathrm{AQCR}\) is utilized to quantify the frequency of quantile crossing violations. The quantile crossing indicator for a quantile pair \((\tau_l, \tau_u)\) with \( \tau_l < \tau_u \) is:
\begin{equation}
    C_{\tau_l, \tau_u}(\hat{y}_{l, i}, \hat{y}_{u, i}) = \mathbb{I}(\hat{y}_{l, i} > \hat{y}_{u, i}),
\end{equation}
where \( \mathbb{I}(\cdot) \) is an indicator function that returns 1 if the condition inside is true and 0 otherwise.

We aggregate the crossing indicators to compute the \(\mathrm{AQCR}\) across \( N \) samples as:
\begin{equation}
    \mathrm{AQCR} = \frac{1}{N} \sum_{i=1}^N C_{\tau_l, \tau_u}(\hat{y}_{l, i}, \hat{y}_{u, i}),
\end{equation}


Smaller \(\mathrm{AQCR}\) values indicate fewer quantile crossing violations, reflecting more reliable quantile predictions.

\paragraph{Pointwise Prediction}
We use the root mean squared error (\(\mathrm{RMSE}\)), mean absolute error (\(\mathrm{MAE}\)), and \(\mathrm{R}^2\) as evaluation metrics for the pointwise prediction of the median quantile. The details can be found in~\cref{sec:pointwisemetrics}. 




\subsection{Results}

















%\paragraph{Quantitative Analysis}

The results presented in~\cref{performance-table} demonstrate the experimental performance of our proposed methods compared to the benchmarks. Generally, the prediction losses for ID$_1$ are higher than those for ID$_3$, as ID$_1$ represents the last-minute imbalance needs, making it more volatile. Furthermore, losses for 15-minute price indices are notably higher than those for 60-minute indices, highlighting the increased volatility of 15-minute prices. 


\paragraph{Parameter Efficiency}
~\cref{paras} illustrates the parameter scaling law of five models. With the same number of parameters (\(10^4\), \(10^5\), \(10^6\)), our approach consistently achieves superior performance. Furthermore, increasing the parameter count from \(10^5\) to \(10^6\) results in only marginal improvements, indicating that our model requires fewer parameters to reach performance saturation.~\cref{performance-table} provides a detailed comparison across various metrics, with all models constrained to \(10^5\) parameters.  
The best results are consistently achieved with our proposed method. Especially, our method surpasses GRU by 50.3\% for 15min-ID$_3$ and LSTM by 31.3\% for 15min-ID$_1$ in AQL. These results indicate that LSTM and GRU suffer from underfitting and require more parameters to achieve proper performance, and our model is particularly parameter-efficient for volatile 15-min indices.
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\hspace{0.2in} % Adjust the value to move the figure to the right
\centerline{\includegraphics[width=0.85\linewidth]{Figures/model_comparison.png}}
\caption{Model comparison with a different number of parameters. The x-axis represents the number of parameters (on a logarithmic scale), while the y-axis shows the mean testing \(\mathrm{AQL}\) aggregated across all price indices. }
\label{paras}
\end{center}
\vskip -0.1in
\end{figure}
\paragraph{Reliability}
Observed from~\cref{performance-table}, the AQCR of our proposed method is consistently zero, indicating no quantile crossing. This result is expected, as our design strictly enforces monotonicity between quantiles. In contrast, the 1D encoding with MLP performs the worst, with an average AQCR of 61.1\% across four indices, leading to unreliable forecasts. The high AQCR from MLP further highlights the importance of proper data encoding.







\subsection{Ablation Studies}
\label{sec:ablation}

\paragraph{Cross-Attention}
We implement and compare three backbone architectures: (1) a backbone without cross-attention, (2) a standard cross-attention backbone, and (3) our proposed jump cross-attention backbone. Three models employ the same hierarchical head anchored at Q50, with \(10^5\) parameters. A comparison of these architectures is shown in~\cref{JumpCrossAttention}. Observed from~\cref{ablationbackbone}, our tailored jump cross-attention outperforms the other two backbones across most metrics. 
Additionally, compared to the baseline model without cross-attention, the results highlight that introducing cross-attention improves  20.85\% AQL on average in forecasting volatile 15-min prices.


\paragraph{Multi-Quantile Head}
We implement and compare three multi-quantile heads: (1) a standard multi-quantile head, (2) a hierarchical multi-quantile head anchored at Q5~\cite{LQF}, and (3) our proposed hierarchical multi-quantile head anchored at Q50. A comparison of these architectures is illustrated in~\cref{multiquantilehead}. Three heads incorporate OrderFusion and jump cross-attention with \(10^5\) parameters. Notably, removing the monotonic constraint improves certain metrics, consistent with observations from~\cite{LQF}. However, our model still outperforms other benchmarks across all metrics. Compared to the hierarchical head anchored at Q5, our approach achieves 18.6\% lower AQL and 19.5\% lower MAE for 15min-ID1 as our method reduces the risk of error accumulation.



\section{Conclusion}
\label{sec:conclusion}

In conclusion, our proposed OrderFusion framework, combined with a jump cross-attention backbone, enables parameter-efficient learning, highlighting the importance of modeling interdependencies between bids and offers through variants of cross-attention. The designed hierarchical multi-quantile head anchored at Q50 predicts multiple quantiles simultaneously while addressing the quantile crossing issue and mitigating the risk of error accumulation for volatile price indices. Experimental results and ablation studies demonstrate the efficiency and reliability of our approach. 
This work lays the foundation for future advancements in probabilistic modeling within the energy domain, particularly for high-frequency, volatile CID markets.





\section*{Impact Statement}

Our proposed encoding method is applicable to all European CID markets, as the orderbook data from EPEX Spot follows a uniform structure. The designed hierarchical multi-quantile head can be seamlessly integrated with various backbones for diverse probabilistic prediction tasks while overcoming quantile crossing issues. By providing a parameter-efficient and reliable prediction framework, our model contributes to enhanced uncertainty management and supports a smoother transition toward renewable energy integration.
\paragraph{Data availability} Data cannot be shared due to commercial restrictions.
\paragraph{Code availability} Codes will be made publicly available.


\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix}
\label{sec:appendix}


\begin{algorithm}[tb]
   \caption{Matching Rule}
   \label{alg:matchingrule}
\begin{algorithmic}
   \STATE {\bfseries Input:} Bids \( (P^{(b)}, V^{(b)}) \), Offers \( (P^{(o)}, V^{(o)}) \)
   \IF{\( P^{(b)} \) $\geq$ \( P^{(o)} \)}
       \IF{\( V^{(b)} \) = \( V^{(o)} \)}
           \STATE \textbf{Full execution:} Match full volumes and remove both orders from the orderbook.
       \ELSE
           \STATE \textbf{Partial execution:} Match partial volumes and update the remaining order.
       \ENDIF
   \ELSE
       \STATE \textbf{No execution:} Leave both orders in the orderbook.
   \ENDIF
\end{algorithmic}
\end{algorithm}



\subsection{Benchmark Models}
\label{sec:benchmarkmodels}

\paragraph{MLP}  
Multilayer Perceptrons (MLPs)~\cite{rumelhart1986learning} are effective for tabular data and simple classification or regression tasks, learning nonlinear relationships through fully connected layers. While they excel in static pattern recognition, they struggle with sequential dependencies and require regularization to prevent overfitting. In our setup, we use 4 dense layers and adjust the number of neurons to match the total parameter count of other models, keeping the remaining hyperparameters identical to those in \cref{sec:details}.  

\paragraph{LSTM}  
Long Short-Term Memory (LSTM) networks~\cite{hochreiter1997long} are well-suited for sequential data, capturing long-term dependencies through memory cells and gating mechanisms. They mitigate vanishing gradients but have high computational costs. In our experiments, we use 4 LSTM layers, adjusting the number of hidden units to control the total parameter count, with other hyperparameters kept consistent as described in \cref{sec:details}.  

\paragraph{GRU}  
Gated Recurrent Units (GRUs)~\cite{cho2014learning} offer a more computationally efficient alternative to LSTMs by simplifying the gating mechanism. They balance short- and long-term dependencies but may underperform in highly complex sequences. Our implementation includes 4 GRU layers, tuning the number of units to control the total parameter count while maintaining consistent hyperparameter settings from \cref{sec:details}.  

\paragraph{CNN}  
Convolutional Neural Networks (CNNs)~\cite{lecun1998gradient} are designed for spatial and temporal pattern extraction using learnable kernels. While 2D CNNs dominate image processing, 1D CNNs efficiently capture local temporal dependencies in time series. However, they lack inherent long-term sequence modeling. We employ 4 CNN layers, adjusting the number of filters to match the total parameter count, with other hyperparameters aligned with those in \cref{sec:details}.  




\subsection{Pointwise Metrics}
\label{sec:pointwisemetrics}

\paragraph{\(\mathrm{RMSE}\)}
The Root Mean Squared Error (\(\mathrm{RMSE}\)) evaluates the accuracy of pointwise predictions by penalizing larger errors more heavily than smaller ones. It is particularly sensitive to outliers and provides an overall measure of prediction quality. \(\mathrm{RMSE}\) is calculated as:
\begin{equation}
    \mathrm{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2},
\end{equation}
where \( y_i \) represents the true value, \( \hat{y}_i \) is the predicted value, and \( N \) is the total number of samples.

\paragraph{\(\mathrm{MAE}\)}
The Mean Absolute Error (\(\mathrm{MAE}\)) measures the average magnitude of prediction errors, treating all deviations equally regardless of their direction. Unlike \(\mathrm{RMSE}\), \(\mathrm{MAE}\) is more robust to outliers, making it a reliable metric for assessing average prediction accuracy. It is computed as:
\begin{equation}
    \mathrm{MAE} = \frac{1}{N} \sum_{i=1}^N |y_i - \hat{y}_i|,
\end{equation}
where \( y_i \) and \( \hat{y}_i \) are the true and predicted values, respectively.

\paragraph{\(\mathrm{R}^2\)}
The Coefficient of Determination (\( R^2 \)) quantifies the proportion of variance in the target variable that is explained by the predictions. A value of \( R^2 = 1 \) indicates perfect predictions, whereas \( R^2 = 0 \) suggests that the model performs no better than predicting the mean of the true values. It is defined as:
\begin{equation}
    R^2 = 1 - \frac{\sum_{i=1}^N (y_i - \hat{y}_i)^2}{\sum_{i=1}^N (y_i - \bar{y})^2},
\end{equation}
where \( \bar{y} \) is the mean of the true values \( y_i \), and the numerator and denominator represent the residual sum of squares and the total sum of squares, respectively.




\end{document}





