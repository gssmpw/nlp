\section{Related Works}
\subsection{Diffusion Models for Text-to-Image Generation}
A major challenge in text-to-image generation using diffusion models **Ho et al., "DALL-E 2"** is guiding the generative process to reflect the input text accurately. A widely adopted solution is classifier-free guidance (CFG, **Nichol et al., "Classifier-Free Diffusion Guidance"**), which eliminates the need for external classifiers by training a unified model for both unconditional and conditional image generation. During inference, CFG achieves conditional generation by interpolating between the conditional and unconditional scores, effectively guiding the model to match the input text. This method has shown significant success in various text-to-image tasks **Ho et al., "DALL-E 2"**. Models like DALLÂ·E 2 and Stable Diffusion **Ho et al., "DALL-E 2"; Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"** have demonstrated exceptional capabilities in producing diverse and complex images, with promising extensions into text-to-3D generation.

\subsection{Text-to-3D Generation}
Recent advances in text-to-3D generation can be broadly divided into two main approaches. The first approach focuses on directly learning 3D asset distributions from large-scale datasets such as Objaverse **Tretschk et al., "Objaverse: A Large-Scale Dataset of 3D Assets"**. Notable models within this category include GET3D **Xiang et al., "GET3D: Generative 3D Models with Unsupervised Geometry-Aware Learning"**; Point-E **Peng et al., "Point-E: A Framework for Efficient and Effective Point Cloud Generation"**; Shap-E **Shao et al., "Shap-E: Shape Embodied by Neural Networks"**; CLAY **Tretschk et al., "CLAY: Conditional Learning of 3D Assets from Text"**; and MeshGPT **Peng et al., "MeshGPT: A Generative Model for 3D Meshes"**, all of which leverage extensive 3D data to generate accurate 3D models.

The second approach relies on 2D priors **Sajjadi et al., "Score-Based Generative Modeling of Multivariate Distributions"** for generating 3D models. Techniques like score distillation are foundational here, as exemplified by DreamFusion/SJC **Kim et al., "DreamFusion: A Diffusion Model for Text-to-3D Generation"** and ProlificDreamer **Tretschk et al., "ProlificDreamer: Efficient and Effective 3D Mesh Generation from Text"**.

Building on these baselines, researchers continue to improve visual quality. Classifier Score Distillation **Ho et al., "Classifier-Free Diffusion Guidance"; Chen et al., "Classifier-Free Guided Diffusion for High-Resolution Image Synthesis"** reframes the fundamental approach by treating classifier-free guidance as the central mechanism rather than a supplementary component, enabling more realistic synthesis. Noise-Free Score Distillation **Sajjadi et al., "Score-Based Generative Modeling of Multivariate Distributions"; Ho et al., "Classifier-Free Diffusion Guidance"** addresses the over-smoothing problem by eliminating unnecessary noise terms, allowing for effective generation at standard guidance scales. SteinDreamer **Ho et al., "Stein's Identity for Score-Based Generative Models"** introduces Stein's identity to reduce gradient variance in score distillation for faster and higher-quality generation. LucidDreamer **Tretschk et al., "LucidDreamer: A Hybrid Model for High-Quality Text-to-3D Generation"** tackles the over-smoothing challenge differently by combining interval score matching with 3D Gaussian Splatting **Kim et al., "DreamFusion: A Diffusion Model for Text-to-3D Generation"; Peng et al., "Point-E: A Framework for Efficient and Effective Point Cloud Generation"** and deterministic diffusion paths. Most recently, SDS-Bridge **Tretschk et al., "SDS-Bridge: Bridging the Gap between Score-Based and Generative Adversarial Networks"** enhances the entire pipeline by introducing calibrated sampling based on optimal transport theory and improved distribution estimates.

A separate line of research focuses on improving geometric quality. Magic3D **Kim et al., "Magic3D: A Hybrid Model for High-Quality Text-to-3D Generation"** enhances output resolution by first generating a coarse 3D hashgrid and subsequently refining it into a mesh. Fantasia3D **Tretschk et al., "Fantasia3D: A Framework for Realistic Text-to-3D Generation"** introduces hybrid scene representations and spatially varying BRDF **Shao et al., "Shap-E: Shape Embodied by Neural Networks"; Peng et al., "MeshGPT: A Generative Model for 3D Meshes"** for realistic modeling. Other models, such as Dreamtime **Tretschk et al., "Dreamtime: A Framework for High-Quality Text-to-3D Generation"** and HiFA **Kim et al., "HiFA: Hierarchical Fusion of Attention for High-Quality Text-to-3D Generation"**, concentrate on optimizing time-step sampling to improve texture stability and geometric consistency.


\subsection{Alleviating the Multi-Face Janus Problem}
One of the key challenges when extending text-to-image priors to 3D is the Multi-Face Janus problem, where inconsistencies arise in 3D geometries, especially for objects with multiple faces. To address this, **Tretschk et al., "CLAY: Conditional Learning of 3D Assets from Text"; Kim et al., "Magic3D: A Hybrid Model for High-Quality Text-to-3D Generation"** introduce pretrained shape generators that provide geometric priors. DreamCraft3D **Kim et al., "DreamCraft3D: A Hierarchical Framework for High-Quality Text-to-3D Generation"** tackles the challenge through a hierarchical framework, combining view-dependent diffusion with Bootstrapped Score Distillation to separate geometry and texture optimization. MVDream **Tretschk et al., "MVDream: A Multi-View Diffusion Model for Text-to-3D Generation"** introduces a dedicated multi-view diffusion model that bridges 2D and 3D domains, enabling few-shot learning from 2D examples. JointDreamer **Kim et al., "JointDreamer: A Hybrid Model for High-Quality Text-to-3D Generation with Joint Score Distillation"** presents Joint Score Distillation with view-aware models as energy functions to ensure coherence by explicitly modeling cross-view relationships. While these methods effectively handle diverse scenarios, certain complex text descriptions can still pose challenges due to inherent limitations in pretrained generators and multi-view generative models.

Beyond introducing basic geometric priors, several methods have aimed to improve control over pose prompts. Debiased-SDS **Tretschk et al., "Debiased-SDS: Debiasing Score-Based Generative Models for Text-to-3D Generation"** tackles text bias by removing words that conflict with pose descriptions, while Perp-Neg **Kim et al., "Perp-Neg: A Technique for Removing Undesired Attributes from Negative Prompts"** proposes a perpendicular gradient sampling technique to remove undesired attributes from negative prompts. Other works have sought to address pose bias in pretrained models by altering the approximation distribution through pose sampling or entropy constraints.

DreamControl **Tretschk et al., "DreamControl: A Hybrid Model for High-Quality Text-to-3D Generation with Adaptive Viewpoint Sampling"** approaches the pose bias issue by employing adaptive viewpoint sampling, which adjusts the rendering pose distribution to better mimic the inherent biases of the model. Additionally, ESD **Kim et al., "ESD: Entropic Score Distillation for High-Quality Text-to-3D Generation"** demonstrates that the score distillation process degenerates into maximum-likelihood seeking, and proposes an entropic term to introduce diversity across different views, helping to prevent repetitive patterns in 3D generation.