


\section{Method}


The primary goal of our \textit{RecDreamer} is to mitigate the Multi-Face Janus problem through rectification of underlying data distribution in the pre-trained diffusion models. In the following sections, we will first theoretically illustrate the idea of how we rectify the data density via an auxiliary function to ensure a uniform pose distribution~(Sec.~\ref{method:rec}). Based on the former theoretical analysis, we introduce a \textit{uniform score distillation} approach for optimizing 3D representations in aligning with the rectified distribution~(Sec.~\ref{method:usd}). Furthermore, a series of designed components for implementing the auxiliary function is detailly discussed in Sec.~\ref{method:recdreamer}, including a pose classifier, approximation of the posterior distribution of pose, and estimation of pose-relevant statistics.



\subsection{Rectification of Data Distribution}\label{method:rec}


To directly analyze the relationship between data and pose, we eliminate redundant variables and simplify the text-conditioned probability $p_t(\boldsymbol{x}_t|y)$ to an unconditional density $p(\boldsymbol{x})$, removing the influence of the time step.
We denote the data with a general variable $\boldsymbol{x}$.
Assuming that $p(\boldsymbol{x}, c)$ represents the joint distribution, the pose distribution can be expressed as $p(c) = \int p(\boldsymbol{x}, c) \mathrm{d}\boldsymbol{x} = \int p(\boldsymbol{x}) p(c|\boldsymbol{x}) \mathrm{d}\boldsymbol{x}$, which is not a uniform distribution.
To mitigate this bias, we frame the simplified problem as follows: given the data distribution $p(\boldsymbol{x})$ and the target attribute distribution $f(c)$, \textit{how can we adjust $p(\boldsymbol{x})$ to a new distribution $\tilde{p}(\boldsymbol{x})$ such that $\tilde{p}(c) = \int \tilde{p}(\boldsymbol{x}) p(c|\boldsymbol{x}) \mathrm{d}\boldsymbol{x}=f(c)$ holds.}


By introducing a weighting function to the joint probability $p(\boldsymbol{x}, c)$, we establish that the original data density can be adjusted as follows.

\begin{theorem}[Proof in Appendix~\ref{app:method_theorem}]\label{thm:rpx}
 Let $p(\boldsymbol{x})$ denote the data density, $p(c | \boldsymbol{x})$ the conditional distribution of the attribute $c$ given data $\boldsymbol{x}$, and $p(c)$ the marginal distribution of $c$ induced by $p(\boldsymbol{x})$. Given a target distribution $f(c)$ for the attribute $c$, we can construct a new data density $\tilde{p}(\boldsymbol{x})$ such that the marginal distribution of $c$ under $\tilde{p}(\boldsymbol{x})$ matches the target distribution $f(c)$. This new density is given by:
    \begin{equation}
 \tilde{p}(\boldsymbol{x}) = p(\boldsymbol{x}) \int \frac{f(c)}{p(c)} p(c | \boldsymbol{x}) \, dc.
    \end{equation}
\end{theorem}

Theorem~\ref{thm:rpx} reveals that the new data density that features a uniformly distributed marginal $f(c)$ can be computed by the original data distribution and an auxiliary function. Furthermore, Theorem~\ref{thm:rpx} can be naturally extended to conditional distributions, as demonstrated in Corollary~\ref{cr:rpx_cond} (see Appendix~\ref{app:method_theorem}). So far, we have derived the rectified distribution for clean images, $\tilde{p}(\boldsymbol{x}_0|y)$.

However, since score distillation operates in the noise space, our ultimate goal is to reach the rectified density of the noisy data. Given the transition $p_t(\boldsymbol{x}_t|y) = \int p_0(\boldsymbol{x}_0|y)p_{t0}(\boldsymbol{x}_t|\boldsymbol{x}_0)\mathrm{d}\boldsymbol{x}_0$ where $p_{t0}(\boldsymbol{x}_t|\boldsymbol{x}_0)=\mathcal{N}(\boldsymbol{x}_t|\alpha_t\boldsymbol{x}_0,\sigma_t^2\boldsymbol{I})$, we prove that the rectified distributions for any time step share a unified form, as presented in the following theorem.


\begin{theorem}[Proof in Appendix~\ref{app:method_theorem}]\label{thm:rpx0t_cond}
 For any $t \sim \mathcal{U}[0, T]$, the rectified density of $\boldsymbol{x}_t$ is given by:
    \begin{equation}\label{eq:rpx0t_cond}
 \tilde{p}_t(\boldsymbol{x}_t|y) = p(\boldsymbol{x}_t|y) \int \frac{f(c|y)}{p_t(c|y)} p(c | \boldsymbol{x}_t, y) dc.
    \end{equation}
\end{theorem}
Theorem~\ref{thm:rpx0t_cond} reveals that the noisy density of the rectified text-to-image distribution can be expressed as the original noisy density multiplied by an auxiliary function, denoted as $r(\boldsymbol{x}_t|y)$. Specifically, $r(\boldsymbol{x}_t|y) = \int \frac{f(c|y)}{p_t(c|y)} p(c | \boldsymbol{x}_t, y) dc$.

\subsection{Uniform Score Distillation}\label{method:usd}


We now return to the original variational distillation problem. First, we define a set of 3D representations $\{\theta^i\}_{i=0}^n$, also named particles in the later gradient flow simulation. Given the distribution $\mu(\theta|y)$ composed of the set $\{\theta^i\}_{i=0}^n$, the camera pose $c$, and the text prompt $y$, the distribution of noisy rendered images is computed as $q_t^\mu(\boldsymbol{x}_t|c,y) = \int q_0^\mu(\boldsymbol{x}_0|c,y)p_{t0}(\boldsymbol{x}_t|\boldsymbol{x}_0)\mathrm{d}\boldsymbol{x}_0$, where $\boldsymbol{x}_0=\boldsymbol{g}(\theta,c)$. Given the rectified distribution $\tilde{p}_t(\boldsymbol{x}_t|y)$, the objective is as follows:
\begin{equation}\label{eq:usd_rkl}
 \min_\mu \mathbb{E}_{t,c}\left[(\sigma_t/\alpha_t)\omega(t)D_{\mathrm{KL}}(q_t^\mu(\boldsymbol{x}_t|c,y)\parallel \tilde{p}_t(\boldsymbol{x}_t|y))\right].
\end{equation}
We refer to this as \textit{uniform score distillation} (USD), as it seeks to approximate the score of the rectified distribution, which is uniformly distributed across the camera poses. To optimize the particles, we derive a corollary based on Theorem 2 from VSD~\citep{wang2024prolificdreamer}:
\begin{corollary}[Corollary to Theorem 2 from VSD]\label{cr:usd_gradient}
 For Wasserstein gradient flow minimizing~\eqref{eq:usd_rkl}, the gradient for the particles is given by:
    \begin{equation}\label{eq:usd_grad}
        \nabla_\theta\mathcal{L}_\text{USD} = \nabla_\theta \mathcal{L}_\text{VSD}^\prime(\theta) - \mathbb{E}_{t,\boldsymbol{\epsilon},c}\left[\omega(t)\frac{\sigma_t}{\alpha_t}\nabla_{\theta}\log r(\boldsymbol{x}_t|y)\right],
    \end{equation}
 where
    \begin{equation}
        \nabla_\theta\mathcal{L}_\text{VSD}^\prime = \mathbb{E}_{t,\boldsymbol{\epsilon},c}\left[\omega(t)\left(\boldsymbol{\epsilon}_\text{pretrain}(\boldsymbol{x}_t,t,y)-\boldsymbol{\epsilon}_\phi(\boldsymbol{x}_t,t,c,y)\right)\frac{\partial\boldsymbol{g}(\theta,c)}{\partial\theta}\right],
    \end{equation}    
 and $\boldsymbol{x}_t=\alpha_t\boldsymbol{g}(\theta,c)+\sigma_t\boldsymbol{\epsilon}$.
\end{corollary}




Since the rectification algorithm is based on reweighting the sub-distributions, it cannot generate content that was not present in the original distribution. To ensure that $y$ provides a comprehensive distribution including contents of multiple perspectives, we detail the construction techniques in the Appendix~\ref{app:method_others}.


In the optimization process, we follow VSD by iteratively optimizing the U-Net $\epsilon_{\phi}$ and the particles $\{\theta^i\}_{i=0}^n$ using~\eqref{eq:vsd_lora} and~\eqref{eq:usd_rkl}.

\subsection{RecDreamer}\label{method:recdreamer}

The previous sections derive the analytical solution of the rectified distribution and introduce a parameter optimization scheme based on score distillation.
To apply this scheme to optimize the 3D scene, we must also compute the rectification function $r(\boldsymbol{x}_t|y)$.
We design an effective \textit{classifier to accurately categorize image poses}.
Finally, we account for the effects of noisy states and \textit{estimate the posterior distribution of noisy images and its expected value}.



\input{figure/fig_classifier_main.tex}

\textbf{Discretization of the pose space.}
The crux to computing the auxiliary function $r(\boldsymbol{x}_t|y)$ lies in estimating both $p(c | \boldsymbol{x}_t, y)$ and $p_t(c | y)$.
Since $p_t(c | y)= \mathbb{E}_{\boldsymbol{x}_t \sim p(\boldsymbol{x}_t|y)}p(c | \boldsymbol{x}_t, y)$ is a term that depends on $p(c | \boldsymbol{x}_t, y)$, we begin by analyzing $p(c | \boldsymbol{x}_t, y)$, which can be interpreted as a pose estimator of noisy images.
However, obtaining an estimator for noisy images requires additional data and fine-tuning. To address this, we relate the noisy predictor to the clean predictor by following the DPS~\citep{chung2022diffusion} formulation: $p(c | \boldsymbol{x}_t, y) = \int p(\boldsymbol{x}_0|\boldsymbol{x}_t,y) p(c|\boldsymbol{x}_0,y) d \boldsymbol{x}_0$.
Thus, we prioritize the design of a clean estimator $p(c|\boldsymbol{x}_0, y)$ before tackling the noisy case.


Instead of explicitly estimating the camera's extrinsic parameters, we propose modeling a simplified pose by categorizing the images into broad pose categories, such as ``front'', ``back'', ``left'' and ``right''. In the context of USD, these global categories help maintain a rough balance between different poses and promote 3D consistency. Accordingly, we define the auxiliary function in a discrete form as follows: $r_\xi(\boldsymbol{x}_t|y) = \sum_{\bar{c}} \frac{f(\bar{c}|y)}{p_t(\bar{c}|y)} p_\xi(\bar{c} | \boldsymbol{x}_t, y)$, where $\bar{c}$ represents the discrete pose category, $f(\bar{c}|y) \sim \mathcal{U}\{\bar{c}_i\}_{i=0}^{k}$, and $p_\xi$ is the parameterized classifier.


\textbf{Pose classifier.} Building on this formulation, our goal is to create a lightweight pose classifier without the need for training. To achieve this, we propose a matching-based pose classifier that leverages a pretrained feature extractor and user-provided image templates for each category. Given an input image, the class probabilities are computed by assessing the similarity between the input and the templates. Empirically, the main challenge is distinguishing between 2D orientations (i.e., ``left-middle-right'') and classifying textures (i.e., ``front-back'').


To address this, we compute the overall similarity by combining orientation similarity and texture similarity in a differential ``and-gate'' manner. The pipeline of our classifier is shown in Fig.~\ref{fig:app_method_classifier}. Drawing inspiration from dense matching techniques~\citep{zhang2024telling, zhang2024tale}, we propose using a patch-matching distance metric to evaluate orientation similarity. Texture similarity is determined by calculating the cosine similarity of the $[cls]$ token between the input and template images. Orientation and texture similarities are then multiplied after normalization. Finally, the combined similarity is normalized using a low-temperature softmax function~\citep{goodfellow2016deep}. For more details on the patch-matching distance and the architecture, please refer to Appendix~\ref{app:method_classifier}.


\textbf{Estimating $p(c|\boldsymbol{x}_t, y)$ and $p_t(c|y)$.}
By establishing the calculation of $p(c|\boldsymbol{x}_0, y)$ with a plug-and-play pose classifier, we can now introduce the computation of $p(c | \boldsymbol{x}_t, y)$ and $p_t(c | y)$.
To compute $p(c | \boldsymbol{x}_t, y) = \int p(\boldsymbol{x}_0|\boldsymbol{x}_t,y) p(c|\boldsymbol{x}_0,y) d \boldsymbol{x}_0$, we follow DPS~\citep{chung2022diffusion} by replacing the calculation of probability with expectation $\mathbb{E}_{\boldsymbol{x}_0{\sim}p(\boldsymbol{x}_0|\boldsymbol{x}_t,y)}p(c|\boldsymbol{x}_0, y)$ and further approximating the expectation with Tweedie's formula~\citep{robbins1992empirical}. Formally, $p(c | \boldsymbol{x}_t, y)\approx p(c|\hat{\boldsymbol{x}}_0, y)$, where $\hat{\boldsymbol{x}}_0 = \left(\boldsymbol{x}_t-\sigma_t\epsilon_{pretrain}(\boldsymbol{x}_t, t, y)\right)/\alpha_t$. Beyond this approximation, we provide an on-the-fly estimate of the marginal density $p_t(c | y)$, avoiding any form of distribution estimation~\citep{robert1999monte}. Concretely, since $p_t(c | y)$ is the expected value of $p(c | \boldsymbol{x}_t, y)$ over $\boldsymbol{x}_t$, we update a distribution $\bar{p}_t(\bar{c} | y)$ using exponential moving average (EMA) of $p(c | \boldsymbol{x}_t, y)$ during optimization, with an update rate $\alpha_{ema}$, to approximate $p(c | \boldsymbol{x}_t, y)$. To enable the in-time estimate of the current pose distribution, we propose a time-interval EMA to capture the distribution. Technical details are left in Appendix~\ref{app:method_recfunc}.



The proposed scheme allows for the accurate estimation of the auxiliary function $r_\xi$, facilitating the adjustment of the initial distribution so that the sampling results align with the assumption of a uniform pose distribution. The implementation of uniform score distillation is presented in Algorithm~\ref{alg:usd}, and we refer to this systematic approach as \textit{RecDreamer}.


\input{tab/alg_usd.tex}









