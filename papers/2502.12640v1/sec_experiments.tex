
\section{Experiments} \label{sec:experiment}

\subsection{Experiment Settings}\label{sec:exp_settings}


To evaluate the performance of USD, we selected 22 prompts describing various objects for comparison experiments. The comparison involves three baseline methods (SDS~\citep{poole2022dreamfusion}, SDS-Bridge~\citep{mcallister2024rethinking}, and VSD~\citep{wang2024prolificdreamer}), and three open-source methods designed to address the Multi-Face Janus problem (PerpNeg~\citep{armandpour2023re}, Debiased-SDS~\citep{hong2023debiasing}, and ESD~\citep{wang2024taming}). We introduce several metrics to assess both the quality of the generated outputs and the severity of the Multi-Face Janus problem. For VSD and USD, we optimize a single particle (\ie, a 3D representation~\citep{mildenhall2021nerf, muller2022instant}) for score distillation. Additionally, for each prompt, we include auxiliary descriptions to ensure the text-to-image distribution includes the side and back sub-distributions, satisfying the assumption that $p(c) > 0$ in Lemma~\ref{lm:weight} (see Appendix~\ref{app:method_others} for more discussion).



\subsection{Metrics} 
We evaluate our approach through three complementary metrics. The \text{Fr\'echet} Inception Distance~\citep{heusel2017gans} assesses generation fidelity, while categorical entropy measures quantify distributional bias. Additionally, CLIP~\citep{radford2021learning} scores measure the alignment between generated scenes and their corresponding text prompts. Details are left in Appendix~\ref{app:main_exps_metrics}.

\textbf{\text{Fr\'echet Inception Distance (FID).}} FID evaluates generation quality by comparing two distribution pairs. We compute standard FID against a base diffusion model (60 images per prompt) and unbiased FID (uFID in Table~\ref{table:comparison_main}) against its pose-balanced version (by annotating and resampling the generated images). For each method, we render 5 images per scene from uniform viewpoints to form the rendered image set for evaluation.


\textbf{Categorical Entropy.} We evaluate 3D consistency by quantifying the Multi-Face Janus Problem through classifier predictions. Inconsistent scenes show similar classification probabilities across viewpoints with a bias toward canonical poses, while consistent scenes produce diverse viewpoint-dependent probabilities. We measure this using the entropy of averaged classification probabilities, with higher entropy indicating better consistency. We use two methods: a CLIP-based classifier with directional text descriptions and our proposed pose classifier. The metrics are marked as ``cEnt'' and ``pEnt'' in Table~\ref{table:comparison_main}. For each method, we render 10 images per scene from uniform viewpoints to calculate the entropy.

\textbf{CLIP Score.} Following ESD~\citep{wang2024taming}, we evaluate text-image alignment by computing CLIP scores between text descriptions and the scenes' corresponding rendered images.

% \vspace{-2mm}
\subsection{Comparisons}
% \vspace{-1mm}
\textbf{Quantitative Evaluation.} As shown in Table~\ref{table:comparison_main}, our method outperforms other baselines concerning the measures for generation quality. However, the limited test set size and comparable texture quality between our method and VSD make it challenging to fully quantify the impact of geometric consistency through these metrics alone. We provide more details in qualitative comparison and Appendix~\ref{app:main_exps}.
In terms of diversity measures, our method achieves higher entropy scores in both CLIP-based categorization (cEnt) and pose classification (pEnt), indicating that USD effectively incorporates multi-view information into the 3D representations and mitigates the issue of repetitive patterns across different viewpoints.
Regarding text-image alignment, USD shows lower CLIP scores than VSD, as our multi-view approach incorporates back and side views that may not align with prompts describing predominantly frontal features (\eg, back views of a dog versus front-oriented descriptions).

\input{tab/table_comparison_main.tex}
\input{figure/fig_comparison_main.tex}
\textbf{Qualitative Evaluation.} As shown in Fig.~\ref{fig:exp_comparison_main}, the texture quality achieved by our method is comparable to that of VSD. In terms of geometry, USD demonstrates a reasonable structure, capturing the shapes of different poses and successfully simulating some finer details like bumps. Although some artifacts remain (not as smooth as SDS and its variants), our method maintains a relatively accurate geometry compared to VSD. More results are available in \url{https://chansey0529.github.io/RecDreamer_proj/}.




\subsection{Ablations}\label{app:main_exps_abl}
\input{figure/fig_ablation_pyxt.tex}


We provide ablation results in Fig.~\ref{fig:app_main_abl}. Since the first stage of training establishes the overall geometry, all subsequent experiments are conducted using only the first stage for comparison.

\textbf{Ablation for $p(c|\boldsymbol{x}_t, y)$.} To verify the approximation of $p(c|\boldsymbol{x}_t, y)$, we design a variant that directly estimates the distribution of the noisy images $\boldsymbol{x}$ using a pose classifier. As shown in Fig.~\ref{fig:app_main_abl}, the rectification term in this implementation is almost ineffective because the classifier struggles to determine the class of noisy images. This causes the rectification to work only in relatively small time steps for clean images, limiting its impact on the global optimization. This result highlights the necessity of approximating $p(c|\boldsymbol{x}_t, y)$.

\textbf{Ablation for $p(c|y)$.} Furthermore, to validate the effectiveness of $p(c|y)$, we devise a sampling variant. Instead of predicting the current distribution in real-time using EMA, we sample a batch of images before training and predict the distribution for each interval. Score distillation is then performed based on this fixed pose distribution. However, the results are quite random (the ``bear'' case is over-rectified and the ``zombie'' case is under-rectified). This is because there may be a gap between the distribution of score distillation and the sampled distribution, leading to incorrect guidance to generate another bias. Additionally, a fixed rectified distribution is unable to adaptively balance the gradient of the noise predictor and the classifier, therefore may lead to over-adjustment.

In conclusion, we adopt the approximation $p(c|\boldsymbol{x}_t, y) \approx p(c|\hat{\boldsymbol{x}}_0, y)$ and employ dynamic distribution updates via EMA. This ensures an effective simulation of the rectification function values.












\subsection{Component Analysis}

\textbf{Hyperparameter Evaluation.} We analyze the impact of key hyperparameters, such as the update rate of EMA $\alpha_{ema}$ and the number of particles $n_t$. The results indicate that a larger $\alpha_{ema}$ facilitates more responsive updates, allowing for real-time tracking and adjustment of the pose distribution. Additionally, our findings suggest that the back-and-forth time scheduling, as detailed in Appendix~\ref{app:method_others}, enhances multi-particle optimization. Further specifics can be found in Appendix~\ref{app:main_exps_hyper}.

\textbf{Additional Experiments.} In addition to the hyperparameter evaluation, we conduct further investigations, detailed in Appendices~\ref{app:main_exps},~\ref{app:cls_exps}, and~\ref{app:val_exps}. Using the annotated pose data, we quantitatively validate the effectiveness of the pose classifier, with ablation studies on texture and orientation scores confirming the robustness of the classifier architecture. Validation experiments on 2D particles provide an intuitive demonstration of USD's performance. Furthermore, by utilizing RecDreamer, we extend the conditional image generation~\citep{graikos2022diffusion} from one single particle into a multi-particle optimization scheme, enabling more effective control with promising practical applications.












