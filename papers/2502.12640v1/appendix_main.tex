\section{Supplementary Experiments}\label{app:main_exps}

This appendix contains supplementary experimental details.~\ref{app:main_exps_metrics} provides a detailed discussion of metric calculations.~\ref{app:main_exps_hyper} analyzes the influence of hyperparameters.~\ref{app:main_exps_templates} visualizes the user-provided templates.~\ref{app:main_exps_cross} demonstrates our method's scalability through cross-domain rectification.~\ref{app:main_exps_control} explores special cases to showcase practical applications.~\ref{app:main_exps_runtime} and~\ref{app:main_exps_study} present additional performance results. Note that the prompt list, comparisons, and results
are left in Appendix~\ref{app:prompt} and Appendix~\ref{app:compare}.




\subsection{Metrics}\label{app:main_exps_metrics}
\subsubsection{Fréchet Inception Distance for Generation Quality}
The Fréchet Inception Distance~\citep{heusel2017gans} (FID) serves as our primary metric for evaluating generation quality by measuring the statistical distance between two image distributions. In our evaluation process, we compare our generated images against two different target distributions:

\paragraph{Standard FID.}
To evaluate the quality gap between our generated 3D scenes and pretrained Stable Diffusion~\citep{rombach2022high} outputs, we establish a target distribution by sampling 60 images per prompt across 22 different prompts, yielding a total test set of 1,320 images. To mitigate pose bias in this distribution, we incorporate directional text descriptions such as ``front view,'' ``side view,'' and ``back view'' during sampling. However, we note that some pose bias remains, with frontal views being over-synthesized. For our generated distribution, we render 5 images from each 3D scene using uniformly sampled camera poses. The standard FID score is then calculated between this rendered set and our target distribution.

\paragraph{Unbiased FID (uFID).}
To address the inherent pose bias present in standard FID evaluation, we develop an alternative metric called uFID. This approach begins with manual annotation of camera poses for all images in the standard test set. Using these annotations, we resample the test set to ensure equal representation across different poses. While this resampling strategy may result in some image duplication, it yields a more balanced distribution of viewpoints and textures. The uFID score is then computed between this pose-balanced dataset and our rendered images, providing a more equitable assessment of generation quality across different viewpoints.

\subsubsection{Categorial Entropy for Geometric Consistency}
We evaluate the Multi-Face Janus problem using a pose classifier to analyze viewpoint consistency across different perspectives. The underlying principle is straightforward: in a scene with severe Multi-Face Janus issues, different viewpoints will yield similar classification probabilities because they share similar features. This similarity typically manifests as a strong bias toward a particular class (usually the canonical pose) in the average classification probability across viewpoints. Conversely, a geometrically consistent 3D scene will produce more diverse classification probabilities that average toward a more uniform distribution across viewpoints.

Based on this insight, we use the entropy of the average classification probability from different views as a metric for measuring the severity of multiplicity problems. Higher entropy values indicate greater diversity in information across viewpoints, while lower values suggest excessive pattern duplication in the generated scene. Formally, for each prompt, we calculate the entropy $R_{ent}$ as:
\begin{equation}
    \begin{gathered}
        \bar{\boldsymbol{p}} = \frac{1}{n_v}\sum_{i=0}^{n_v} \boldsymbol{\Phi} (\boldsymbol{g}(\theta,c_i)), \\
        R_{ent} = \frac{1}{n_{\bar{c}}}\sum_{i=0}^{n_{\bar{c}}} \bar{p_i}\log\bar{p_i},
    \end{gathered}
\end{equation}
where $\boldsymbol{g}$ represents the renderer and $\boldsymbol{\Phi}$ the pose classifier. The probability vector $\bar{\boldsymbol{p}}$ consists of components $\bar{p_i}$. $n_v$ denotes the number of sampled views (default: 10), and $n_{\bar{c}}$ represents the number of pose categories. We implement this entropy evaluation using two different classification approaches:

\paragraph{CLIP Entropy (cEnt).} This method employs CLIP~\citep{radford2021learning} as the classifier, using three textual descriptions that combine the original prompt with directional modifiers: ``from front view'', ``from side view'', and ``from back view''. These descriptions establish three distinct categories for classifying input images.

\paragraph{Pose Entropy (pEnt).} This variant utilizes our specially designed pose classifier for categorization, providing a more direct assessment of pose-related geometric consistency.

\subsubsection{CLIP Score for Textual Alignment.}
To evaluate textual alignment, we calculate the CLIP score by measuring the negative cosine similarity between CLIP feature embeddings of the rendered images and their corresponding text prompts, following~\citet{wang2024taming}.




\subsection{Hyperparameter Analysis}\label{app:main_exps_hyper}
\input{figure/fig_ablation_bnf.tex}
\input{figure/fig_ablation_ema.tex}
\input{figure/fig_ablation_np.tex}

\textbf{Influence of BNF interval $n_i$.} The BNF time scheduler controls the sampling intervals for score distillation. A larger $n_i$ provides finer control over the sampling process, making it more closely resemble the DDPM sampling process. Through our single-particle optimization experiments, we observe that adjusting $n_i$ values demonstrates minimal influence on training dynamics until the terminal phase of optimization. During this final stage, larger $n_i$ configurations become particularly impactful. For example, when the sampling time step is limited to the interval $[100, 0]$, the model tends to overfit, often producing oversaturated colors. We typically use early stopping to prevent this. However, in simultaneous multi-particle optimization, we observe that a larger BNF $n_i$ improves training quality. As shown in Fig.~\ref{fig:app_main_hypern}, the training process for different BNF values demonstrates this effect. When $n_i=2$, the training is imbalanced across particles. For example, in the case of ``beagle,'' the first particle learns the information more quickly, while the fourth particle of ``Groot'' progresses more slowly. This imbalance causes the fastest particle to converge to one mode of the distribution, such as all back views for ``beagle'', while other particles converge to different modes (e.g., front view, back view, etc.). This results in an undesirable outcome, where the overall distribution across particles appears uniform, but each individual particle suffers from the Multi-Face Janus problem—one is biased toward front views, while another is biased toward back views, which contradicts our goal. To resolve this issue, we ensure that all particles converge at a consistent rate. Our BNF time scheduling controls the interval of sampling time steps, where a larger BNF value ($n_i$) results in time steps being sampled within a narrower range during the early training phase. This promotes a more uniform optimization process across particles. As a result, training consistency improves, reducing the risk of biased convergence. In Fig.~\ref{fig:app_main_hypern}, we show the generation effect with $n_i=10$, which better aligns the modified distribution across all four particles.



\textbf{Influence of valid EMA steps $n_{ema}$.} The valid number of EMA steps, $n_{ema}$, ensures that the weights of the last $n_{ema}$ steps account for more than 90\% of the total, while weights beyond this threshold are negligible and can be approximated as invalid. We set $n_{ema}$ to 100, 1000, and 10000, respectively, to simulate distribution updates at different speeds. We plot the probability curve $\bar{p}_t(\bar{c}|y)$ during the first five EMA intervals (i.e., $t \in [0, 500]$ steps for original sampling, with the first half held at 0 because the BNF scheduler has not yet sampled the current interval). The results in Fig.~\ref{fig:app_main_hyperema} show that an overly long EMA time step prevents the model from capturing the real-time pose distribution, leading to an inability to correct distribution bias. In practice, we typically choose $n_{ema}=100$ to ensure effective estimation of the distribution.







\input{figure/fig_ablation_qx.tex}
\input{figure/fig_ref}

\textbf{Influence of number of views $n_{\bar{c}}$.} We study how varying the number of pose categories ($n_{\bar{c}}$ = 3, 4, and 6) affects overall performance. Our experiments, as illustrated in Fig.~\ref{fig:app_main_np}, show that increasing the number of categories from 4 to 6 produces minimal improvement in 3D consistency. This performance plateau stems from our pose classifier's design, which lacks sensitivity to fine-grained pose distinctions, creating a natural ceiling when presented with more detailed pose categories.



\textbf{Sampling $q$.} We examine an alternative approach for sampling the target distribution $q$ using estimated probability $p_t(c|y)$, similar to the first-stage methodology in DreamControl~\citep{huang2024dreamcontrol}. As shown in Fig.~\ref{fig:app_main_qx}, this sampling strategy overemphasizes densely populated regions of the pose space while providing insufficient supervision for less frequent viewpoints. This imbalance leads to compromised geometric consistency in the generated results.



\input{figure/fig_cross_modal.tex}



\subsection{Templates for Pose Classifier}\label{app:main_exps_templates}
We generate reference template images using Stable Diffusion~\citep{rombach2022high} by combining the original prompt with directional text modifiers: ``from front view,'' ``from side view,'' and ``from back view.'' With the set of generated images, users can simply select some with different poses as the templates. Additionally, users also have the option to generate templates using Zero-1-to-3~\citep{liu2023zero}.

As shown in Fig.~\ref{fig:app_ref_img}, the template images do not need to be 3D consistent. They only need to convey basic pose information. Our experiments in Appendix~\ref{app:main_exps_cross} demonstrate this flexibility through a cross-modal experiment where we successfully use simple sketches as templates.

\input{tab/table_cross_modal.tex}
\input{figure/fig_ext_control.tex}
\subsection{Cross Modality Rectification}\label{app:main_exps_cross}
We demonstrate our pose estimator's scalability through a cross-domain experiment using hand-drawn sketches. For this experiment, we draw four sketches (see Fig.~\ref{fig:app_cross}(a)) corresponding to the prompt and construct a pose classifier based on these sketches. When applied to real images, this sketch-based classifier accurately computes both masks and probabilities. In Fig.~\ref{fig:app_cross}(b), we sample some images for testing the performance of the sketch-based classifier. Quantitative results are presented in Table~\ref{tab:app_cross}.

Fig.~\ref{fig:app_cross}(c) and (d) present the 3D generation results. (c) shows scenes generated using manual sketch guidance, while (d) demonstrates results using pose control supervision (detailed in Appendix~\ref{app:main_exps_control}). The latter approach notably enhances the 3D consistency of the output.



\subsection{Controllability as a Special Case of USD}\label{app:main_exps_control}

We further explore conditional control building upon our previous classifier. While our experimental setup considered a uniform distribution of poses across all angles, real-world applications may not require such comprehensive coverage. The number of discrete poses can be reduced when we can specify a more precise range of angles.

In cases where the perspective can be constrained to a narrow range (\eg, front views from -45 to 45 degrees) that can be summarized with only one template image, the rectifier function can be simplified to $r^{ctrl}_\xi(\boldsymbol{x}_t|y) = p_\xi(\bar{c} | \boldsymbol{x}_t, y)$. This simplification is similar to approaches used in PnP~\citep{graikos2022diffusion} and DPS~\citep{chung2022diffusion}, where irrelevant $p_t(\bar{c}|y)$ terms are eliminated. Under these conditions, the generation process is controlled by a single pose, effectively targeting just one specific conditional sub-distribution.

For implementation, we can optimize the classifier using a single logit to achieve this control. While this approach resembles PnP~\citep{graikos2022diffusion}, our method enables multi-particle optimization without requiring specialized classifiers for noisy images, which turns out to be more flexible.






As demonstrated in Fig.~\ref{fig:app_ext}, our approach achieves precise pose control in 2D particle generation. The examples show more efficient control compared to using textual descriptions. We've successfully extended this functionality to both cross-modal (sketch-guided) and 3D-prior based generation, enhancing geometric consistency as shown in Fig.~\ref{fig:app_cross}(d) and Fig.~\ref{fig:app_demo_future}(b). Furthermore, by leveraging DINOv2~\citep{oquab2023dinov2}'s cross-modal matching capabilities, we can explore novel applications. For instance, using airplane images to guide the generation of flying eagles. We believe this opens up promising avenues for expanding the practical applications of our theoretical framework.











\subsection{Runtime Evaluation}\label{app:main_exps_runtime}
\input{tab/table_runtime.tex}
We conduct our experiments at $256\times256$ resolution using a single Nvidia GeForce RTX 4090 GPU. While USD and VSD share the same framework, other comparison methods are implemented using threestudio~\citep{threestudio2023}. To ensure a fair comparison, all methods are evaluated using their default settings to achieve convergence. As shown in Table~\ref{tab:app_runtime}, USD requires longer computation times compared to baseline methods, primarily due to the additional back-propagation through the diffusion U-Net~\citep{ronneberger2015u} required by the rectifier function.

Despite the increased computational overhead, our method's substantial improvements in geometric consistency justify this trade-off. The computational cost remains practical for pose control applications (in Appendix~\ref{app:main_exps_control}), as pose control is typically only necessary during the initial stages of shape formation. Future research directions could focus on developing optimization strategies to enhance computational efficiency while maintaining the method's performance advantages.



\subsection{User Study and Discussions on the Success Rate}\label{app:main_exps_study}
\input{tab/table_user_study.tex}
\input{tab/table_success_rate.tex}

We conduct a user study involving more than 25 human experts to evaluate and rank different methods based on their overall geometric consistency. The average ranks for all methods are reported in Table~\ref{table:app_userstudy}.

Additionally, we analyze the success rate of generating Janus-free models. Given the inherent randomness of score distillation methods, achieving perfect geometric consistency remains challenging. To quantify inconsistencies, we develop a systematic rating system. The system evaluates both global and local features using the formula $R_{score} = (n_{cnt}^g-n_{gt}^g) + 0.5\times(n_{cnt}^l-n_{gt}^l)$, where for global features (such as faces or body), $n_{cnt}^g - n_{gt}^g$ represents the difference between the actual count and the expected count, while for local features (such as legs, arms, tails), each duplicated feature contributes 0.5 to the score. Here, $n_{cnt}^l$ and $n_{gt}^l$ denote the actual and expected counts of local features respectively. Note that the expected count is the correct number that the scene should occur. For instance, in the case of generating a bust, the face's expected count is 1. Table~\ref{tab:app_success_rate} presents the Multi-Face Janus scores for two test prompts, demonstrating our method's performance. We provide a detailed discussion of potential solutions in Appendix~\ref{app:ext_pose}.







