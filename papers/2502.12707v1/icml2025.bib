@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

### Simulators and datasets 

@inproceedings{göbler2024causalassembly,
    title={$\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery}, 
    author={Konstantin Göbler and Tobias Windisch and Mathias Drton and Tim Pychynski and Steffen Sonntag and Martin Roth},
    year={2024},
    booktitle = {Proceedings of Machine Learning Research}
}




@inproceedings{DBLP:conf/nips/Tu0BK019,
  author       = {Ruibo Tu and
                  Kun Zhang and
                  Bo C. Bertilson and
                  Hedvig Kjellstr{\"{o}}m and
                  Cheng Zhang},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm
                  Evaluation},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {12773--12784},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/4fdaa19b1f22a4d926fce9bfc7c61fa5-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Tu0BK019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{gamella2024causalchambers,
      title={The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology}, 
      author={Juan L. Gamella and Jonas Peters and Peter Bühlmann},
      year={2024},
      eprint={2404.11341},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.11341}, 
}

### Benchmarking and analysis



@inproceedings{DBLP:conf/nips/CurthSWS21,
  author       = {Alicia Curth and
                  David Svensson and
                  James Weatherall and
                  Mihaela van der Schaar},
  title        = {Really Doing Great at Estimating CATE? {A} Critical Look at {ML} Benchmarking
                  Practices in Treatment Effect Estimation},
  booktitle    = {NeurIPS Datasets and Benchmarks 2021},
  year         = {2021},
  url          = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/2a79ea27c279e471f4d180b08d62b00a-Abstract-round2.html},
  timestamp    = {Thu, 05 May 2022 16:53:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/CurthSWS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}






@inproceedings{DBLP:conf/nips/GentzelGJ19,
  author       = {Amanda Gentzel and
                  Dan Garant and
                  David D. Jensen},
  title        = {The Case for Evaluating Causal Models Using Interventional Measures
                  and Empirical Data},
  booktitle    = {NeurIPS 2019},
  pages        = {11717--11727},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/a87c11b9100c608b7f8e98cfa316ff7b-Abstract.html},
  timestamp    = {Mon, 24 Apr 2023 08:02:59 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/GentzelGJ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



### Causality Papers

@misc{verma2013equivalencecausalmodels,
      title={On the Equivalence of Causal Models}, 
      author={Tom S. Verma and Judea Pearl},
      year={2013},
      eprint={1304.1108},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1304.1108}, 
}

### Causal Discovery

@book{10.7551/mitpress/1754.001.0001,
    author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
    title = "{Causation, Prediction, and Search}",
    publisher = {The MIT Press},
    year = {2001},
    month = {01},
    abstract = "{The authors address the assumptions and methods that allow us to turn observations into causal knowledge, and use even incomplete causal knowledge in planning and prediction to influence and control our environment.What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences.The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables.The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection.The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993.Bradford Books imprint}",
    isbn = {9780262284158},
    doi = {10.7551/mitpress/1754.001.0001},
    url = {https://doi.org/10.7551/mitpress/1754.001.0001},
}

@article{Ramsey2017,
  author    = {Joseph Ramsey and Madelyn Glymour and Ruben Sanchez-Romero and Clark Glymour},
  title     = {A million variables and more: the Fast Greedy Equivalence Search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images},
  journal   = {International Journal of Data Science and Analytics},
  volume    = {3},
  number    = {2},
  pages     = {121--129},
  year      = {2017},
  doi       = {10.1007/s41060-016-0032-z},
  url       = {https://doi.org/10.1007/s41060-016-0032-z},
  abstract  = {We describe two modifications that parallelize and reorganize caching in the well-known Greedy Equivalence Search algorithm for discovering directed acyclic graphs on random variables from sample values. We apply one of these modifications, the Fast Greedy Equivalence Search (fGES) assuming faithfulness, to an i.i.d. sample of 1000 units to recover with high precision and good recall an average degree 2 directed acyclic graph with one million Gaussian variables. We describe a modification of the algorithm to rapidly find the Markov Blanket of any variable in a high dimensional system. Using 51,000 voxels that parcellate an entire human cortex, we apply the fGES algorithm to blood oxygenation level-dependent time series obtained from resting state fMRI.},
  issn      = {2364-4168},
  date      = {2017/03/01}
}



@inproceedings{DBLP:conf/nips/ZhengARX18,
  author       = {Xun Zheng and
                  Bryon Aragam and
                  Pradeep Ravikumar and
                  Eric P. Xing},
  title        = {DAGs with {NO} {TEARS:} Continuous Optimization for Structure Learning},
  booktitle    = {NeurIPS 2018},
  pages        = {9492--9503},
  year         = {2018},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/e347c51419ffb23ca3fd5050202f9c3d-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/ZhengARX18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{10.5555/3495724.3497230,
author = {Ng, Ignavier and Ghassami, AmirEmad and Zhang, Kun},
title = {On the role of sparsity and DAG constraints for learning linear DAGs},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning graphical structures based on Directed Acyclic Graphs (DAGs) is a challenging problem, partly owing to the large search space of possible graphs. A recent line of work formulates the structure learning problem as a continuous constrained optimization task using the least squares objective and an algebraic characterization of DAGs. However, the formulation requires a hard DAG constraint and may lead to optimization difficulties. In this paper, we study the asymptotic role of the sparsity and DAG constraints for learning DAG models in the linear Gaussian and non-Gaussian cases, and investigate their usefulness in the finite sample regime. Based on the theoretical results, we formulate a likelihood-based score function, and show that one only has to apply soft sparsity and DAG constraints to learn a DAG equivalent to the ground truth DAG. This leads to an unconstrained optimization problem that is much easier to solve. Using gradient-based optimization and GPU acceleration, our procedure can easily handle thousands of nodes while retaining a high accuracy. Extensive experiments validate the effectiveness of our proposed method and show that the DAG-penalized likelihood objective is indeed favorable over the least squares one with the hard DAG constraint.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1506},
numpages = {12},
location = {Vancouver, BC, Canada},
}



@inproceedings{DBLP:conf/iclr/LachapelleBDL20,
  author       = {S{\'{e}}bastien Lachapelle and
                  Philippe Brouillard and
                  Tristan Deleu and
                  Simon Lacoste{-}Julien},
  title        = {Gradient-Based Neural {DAG} Learning},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=rklbKA4YDS},
  timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LachapelleBDL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@inproceedings{DBLP:conf/icml/YuCGY19,
  author       = {Yue Yu and
                  Jie Chen and
                  Tian Gao and
                  Mo Yu},
  editor       = {Kamalika Chaudhuri and
                  Ruslan Salakhutdinov},
  title        = {{DAG-GNN:} {DAG} Structure Learning with Graph Neural Networks},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {7154--7163},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/yu19a.html},
  timestamp    = {Mon, 31 May 2021 15:36:24 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/YuCGY19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{DBLP:journals/jmlr/ShimizuISHKWHB11,
  author       = {Shohei Shimizu and
                  Takanori Inazumi and
                  Yasuhiro Sogawa and
                  Aapo Hyv{\"{a}}rinen and
                  Yoshinobu Kawahara and
                  Takashi Washio and
                  Patrik O. Hoyer and
                  Kenneth Bollen},
  title        = {DirectLiNGAM: {A} Direct Method for Learning a Linear Non-Gaussian
                  Structural Equation Model},
  journal      = {J. Mach. Learn. Res.},
  volume       = {12},
  pages        = {1225--1248},
  year         = {2011},
  url          = {https://dl.acm.org/doi/10.5555/1953048.2021040},
  doi          = {10.5555/1953048.2021040},
  timestamp    = {Thu, 02 Jun 2022 13:58:57 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/ShimizuISHKWHB11.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Dorie2017AutomatedVD,
  title={Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition},
  author={Vincent Dorie and Jennifer L. Hill and Uri Shalit and Marc A. Scott and Daniel Cervone},
  journal={Statistical Science},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:51992418}
}

@article{Hahn2019AtlanticCI,
  title={Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017},
  author={P. Richard Hahn and Vincent Dorie and Jared S. Murray},
  journal={arXiv: Methodology},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:53626612}
}

@article{Shimoni2018BenchmarkingFF,
  title={Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis},
  author={Yishai Shimoni and Chen Yanover and Ehud Karavani and Yaara Goldschmidt},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.05046},
  url={https://api.semanticscholar.org/CorpusID:3671244}
}

@article{Chevalley2022CausalBenchAL,
  title={CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data},
  author={Mathieu Chevalley and Yusuf H. Roohani and Arash Mehrjou and Jure Leskovec and Patrick Schwab},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.17283},
  url={https://api.semanticscholar.org/CorpusID:253237133}
}

@article{Cheng2022EvaluationMA,
  title={Evaluation Methods and Measures for Causal Learning Algorithms},
  author={Lu Cheng and Ruocheng Guo and Raha Moraffah and Paras Sheth and K. Selçuk Candan and Huan Liu},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2022},
  volume={3},
  pages={924-943},
  url={https://api.semanticscholar.org/CorpusID:246634120}
}

@inproceedings{Reisach2021BewareOT,
  title={Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy to Game},
  author={Alexander G. Reisach and Christof Seiler and Sebastian Weichwald},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239998404}
}

@inproceedings{
seng2024learning,
title={Learning Large {DAG}s is Harder than you Think: Many Losses are Minimal for the Wrong {DAG}},
author={Jonas Seng and Matej Ze{\v{c}}evi{\'c} and Devendra Singh Dhami and Kristian Kersting},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gwbQ2YwLhD}
}

@article{Kaiser2021UnsuitabilityON,
  title={Unsuitability of NOTEARS for Causal Graph Discovery when Dealing with Dimensional Quantities},
  author={Marcus Kaiser and Maksim Sipos},
  journal={Neural Processing Letters},
  year={2021},
  volume={54},
  pages={1587 - 1595},
  url={https://api.semanticscholar.org/CorpusID:233209763}
}
@article{DBLP:journals/jmlr/MooijPJZS16,
  author       = {Joris M. Mooij and
                  Jonas Peters and
                  Dominik Janzing and
                  Jakob Zscheischler and
                  Bernhard Sch{\"{o}}lkopf},
  title        = {Distinguishing Cause from Effect Using Observational Data: Methods
                  and Benchmarks},
  journal      = {J. Mach. Learn. Res.},
  volume       = {17},
  pages        = {32:1--32:102},
  year         = {2016},
  url          = {http://jmlr.org/papers/v17/14-518.html},
  timestamp    = {Wed, 10 Jul 2019 15:28:19 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/MooijPJZS16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Vukovic2022CausalDI,
  title={Causal Discovery in Manufacturing: A Structured Literature Review},
  author={Matej Vukovic and Stefan Thalmann},
  journal={Journal of Manufacturing and Materials Processing},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:245972967}
}

@misc{buntine2013theoryrefinementbayesiannetworks,
      title={Theory Refinement on Bayesian Networks}, 
      author={Wray L. Buntine},
      year={2013},
      eprint={1303.5709},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1303.5709}, 
}

@article{EITER200253,
title = {Complexity results for structure-based causality},
journal = {Artificial Intelligence},
volume = {142},
number = {1},
pages = {53-89},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(02)00271-0},
url = {https://www.sciencedirect.com/science/article/pii/S0004370202002710},
author = {Thomas Eiter and Thomas Lukasiewicz},
keywords = {Causal model, Probabilistic causal model, Causality between variables, Event causality, Probabilistic causality, Weak cause, Actual cause, Complexity, Counting hierarchy},
abstract = {We give a precise picture of the computational complexity of causal relationships in Pearl's structural models, where we focus on causality between variables, event causality, and probabilistic causality. As for causality between variables, we consider the notions of causal irrelevance, cause, cause in a context, direct cause, and indirect cause. As for event causality, we analyze the complexity of the notions of necessary and possible cause, and of the sophisticated notions of weak and actual cause by Halpern and Pearl. In the course of this, we also prove an open conjecture by Halpern and Pearl, and establish other semantic results. We then analyze the complexity of the probabilistic notions of probabilistic causal irrelevance, likely causes of events, and occurrences of events despite other events. Moreover, we consider decision and optimization problems involving counterfactual formulas. To our knowledge, no complexity aspects of causal relationships in the structural-model approach have been considered so far, and our results shed light on this issue.}
}

@misc{xia2022causalneuralconnectionexpressivenesslearnability,
      title={The Causal-Neural Connection: Expressiveness, Learnability, and Inference}, 
      author={Kevin Xia and Kai-Zhan Lee and Yoshua Bengio and Elias Bareinboim},
      year={2022},
      eprint={2107.00793},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.00793}, 
}

@misc{xia2022neuralcausalmodelscounterfactual,
      title={Neural Causal Models for Counterfactual Identification and Estimation}, 
      author={Kevin Xia and Yushu Pan and Elias Bareinboim},
      year={2022},
      eprint={2210.00035},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.00035}, 
}

@article{asiadataset,
author = {Lauritzen, S. L. and Spiegelhalter, D. J.},
title = {Local Computations with Probabilities on Graphical Structures and Their Application to Expert Systems},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {50},
number = {2},
pages = {157-194},
keywords = {artificial intelligence, bayesian methods, causal markov random field, decomposable graphs, expert systems, local potentials, markov random field, maximum cardinality search, probabilistic reasoning, triangulated graphs},
doi = {https://doi.org/10.1111/j.2517-6161.1988.tb01721.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1988.tb01721.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1988.tb01721.x},
abstract = {SUMMARY A causal network is used in a number of areas as a depiction of patterns of ‘influence’ among sets of variables. In expert systems it is common to perform ‘inference’ by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed ‘marrying’ and ‘filling-in‘. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.},
year = {1988}
}


@article{
sachsdataset,
author = {Karen Sachs  and Omar Perez  and Dana Pe'er  and Douglas A. Lauffenburger  and Garry P. Nolan },
title = {Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data},
journal = {Science},
volume = {308},
number = {5721},
pages = {523-529},
year = {2005},
doi = {10.1126/science.1105809},
URL = {https://www.science.org/doi/abs/10.1126/science.1105809},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1105809},
abstract = {Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells.}}

@InProceedings{alarmdataset,
author="Beinlich, Ingo A.
and Suermondt, H. J.
and Chavez, R. Martin
and Cooper, Gregory F.",
editor="Hunter, Jim
and Cookson, John
and Wyatt, Jeremy",
title="The ALARM Monitoring System: A Case Study with two Probabilistic Inference Techniques for Belief Networks",
booktitle="AIME 89",
year="1989",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="247--256",
abstract="ALARM (A Logical Alarm Reduction Mechanism) is a diagnostic application used to explore probabilistic reasoning techniques in belief networks. ALARM implements an alarm message system for patient monitoring; it calculates probabilities for a differential diagnosis based on available evidence. The medical knowledge is encoded in a graphical structure connecting 8 diagnoses, 16 findings and 13 intermediate variables. Two algorithms were applied to this belief network: (1) a message-passing algorithm by Pearl for probability updating in multiply connected networks using the method of conditioning; and (2) the Lauritzen-Spiegelhalter algorithm for local probability computations on graphical structures. The characteristics of both algorithms are analyzed and their specific applications and time complexities are shown.",
isbn="978-3-642-93437-7"
}

@misc{wu2024sea,
      title={Sample, estimate, aggregate: A recipe for causal discovery foundation models}, 
      author={Menghua Wu and Yujia Bao and Regina Barzilay and Tommi Jaakkola},
      year={2024},
      eprint={2402.01929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01929}, 
}

@misc{lopez2022factorgraphs,
      title={Large-Scale Differentiable Causal Discovery of Factor Graphs}, 
      author={Romain Lopez and Jan-Christian Hütter and Jonathan K. Pritchard and Aviv Regev},
      year={2022},
      eprint={2206.07824},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2206.07824}, 
}

@inproceedings{NEURIPS2018notears,
 author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep K and Xing, Eric P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DAGs with NO TEARS: Continuous Optimization for Structure Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf},
 volume = {31},
 year = {2018}
}



@article{DBLP:journals/csur/GuoCLH020,
  author       = {Ruocheng Guo and
                  Lu Cheng and
                  Jundong Li and
                  P. Richard Hahn and
                  Huan Liu},
  title        = {A Survey of Learning Causality with Data: Problems and Methods},
  journal      = {{ACM} Comput. Surv.},
  volume       = {53},
  number       = {4},
  pages        = {75:1--75:37},
  year         = {2021},
  url          = {https://doi.org/10.1145/3397269},
  doi          = {10.1145/3397269},
  timestamp    = {Mon, 26 Jun 2023 16:43:56 +0200},
  biburl       = {https://dblp.org/rec/journals/csur/GuoCLH020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{DBLP:journals/tkdd/YaoCLLGZ21,
  author       = {Liuyi Yao and
                  Zhixuan Chu and
                  Sheng Li and
                  Yaliang Li and
                  Jing Gao and
                  Aidong Zhang},
  title        = {A Survey on Causal Inference},
  journal      = {{ACM} Trans. Knowl. Discov. Data},
  volume       = {15},
  number       = {5},
  pages        = {74:1--74:46},
  year         = {2021},
  url          = {https://doi.org/10.1145/3444944},
  doi          = {10.1145/3444944},
  timestamp    = {Sun, 02 Oct 2022 15:51:31 +0200},
  biburl       = {https://dblp.org/rec/journals/tkdd/YaoCLLGZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
nazaret2024extremely,
title={Extremely Greedy Equivalence Search},
author={Achille Nazaret and David Blei},
booktitle={The 40th Conference on Uncertainty in Artificial Intelligence},
year={2024},
url={https://openreview.net/forum?id=2gIMX9UxRN}
}

@InProceedings{diabetesdataset,
author="Andreassen, Steen
and Hovorka, Roman
and Benn, Jonathan
and Olesen, Kristian G.
and Carson, Ewart R.",
editor="Stefanelli, Mario
and Hasman, Arie
and Fieschi, Marius
and Talmon, Jan",
title="A Model-Based Approach to Insulin Adjustment",
booktitle="AIME 91",
year="1991",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="239--248",
abstract="A differential equation model of carbohydrate metabolism was implemented in the form of a causal probabilistic network. This permitted explicit represen-tations of the uncertainties associated with model based predictions of 24-hour blood glucose profiles. In addition, the implementation gave automatic learning and adjustment of model parameters based on measured blood glucose profiles. Insulin therapy was adjusted using a decision theoretical approach. Losses were assigned to blood glucose values that deviated from normal, and the insulin therapy was adjusted to minimize the expected total loss. The system was tested retrospectively on cases from 12 insulin dependent patients and seemed to compare favourably with clinical practice.",
isbn="978-3-642-48650-0"
}


@InProceedings{pmlr-v138-shao20a,
  title = 	 {Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures},
  author =       {Shao, Xiaoting and Molina, Alejandro and Vergari, Antonio and Stelzner, Karl and Peharz, Robert and Liebig, Thomas and Kersting, Kristian},
  booktitle = 	 {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  pages = 	 {401--412},
  year = 	 {2020},
  editor = 	 {Jaeger, Manfred and Nielsen, Thomas Dyhre},
  volume = 	 {138},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--25 Sep},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v138/shao20a/shao20a.pdf},
  url = 	 {https://proceedings.mlr.press/v138/shao20a.html},
  abstract = 	 {Probabilistic graphical models are a central tool in AI, however, they are generally not as expressive
 as deep neural models, and inference is notoriously hard and slow. In contrast, deep probabilistic
 models such as sum-product networks (SPNs) capture joint distributions in a tractable fashion,
 but still lack the expressive power of intractable models based on deep neural networks. Therefore,
 we introduce conditional SPNs (CSPNs), conditional density estimators for multivariate and
 potentially hybrid domains that allow harnessing the expressive power of neural networks while
 still maintaining tractability guarantees. One way to implement CSPNs is to use an existing SPN
 structure and condition its parameters on the input, e.g., via a deep neural network. Our experimental
 evidence demonstrates that CSPNs are competitive with other probabilistic models and yield
 superior performance on multilabel image classification compared to mean field and mixture density
 networks. Furthermore, they can successfully be employed as building blocks for structured
 probabilistic models, such as autoregressive image models.}
}


@inproceedings{
yu2023characteristic,
title={Characteristic Circuits},
author={Zhongjie Yu and Martin Trapp and Kristian Kersting},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=5W7cXno10k}
}


@misc{bottou2013counterfactualreasoninglearningsystems,
      title={Counterfactual Reasoning and Learning Systems}, 
      author={Léon Bottou and Jonas Peters and Joaquin Quiñonero-Candela and Denis X. Charles and D. Max Chickering and Elon Portugaly and Dipankar Ray and Patrice Simard and Ed Snelson},
      year={2013},
      eprint={1209.2355},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1209.2355}, 
}

@article{notallcausalinferencezece,
title={Not All Causal Inference is the Same},
author={Matej Ze{\v{c}}evi{\'c} and Devendra Singh Dhami and Kristian Kersting},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=ySWQ6eXAKp},
note={}
}

@inproceedings{
zecevic2021interventional,
title={Interventional Sum-Product Networks: Causal Inference with Tractable Probabilistic Models},
author={Matej Zecevic and Devendra Singh Dhami and Athresh Karanam and Sriraam Natarajan and Kristian Kersting},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=YMwraqG19Wg}
}


@inproceedings{NEURIPS2020_6cd9313e,
 author = {Jaber, Amin and Kocaoglu, Murat and Shanmugam, Karthikeyan and Bareinboim, Elias},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {9551--9561},
 publisher = {Curran Associates, Inc.},
 title = {Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/6cd9313ed34ef58bad3fdd504355e72c-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{
richens2024robust,
title={Robust agents learn causal world models},
author={Jonathan Richens and Tom Everitt},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=pOoKI3ouv1}
}

@misc{mohan2019graphicalmodelsprocessingmissing,
      title={Graphical Models for Processing Missing Data}, 
      author={Karthika Mohan and Judea Pearl},
      year={2019},
      eprint={1801.03583},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1801.03583}, 
}

@book{Pearl_2009, 
place={Cambridge}, 
edition={2}, 
title={Causality},
publisher={Cambridge University Press},
author={Pearl, Judea}, 
year={2009}} 

@book{koller,
author = {Koller, Daphne and Friedman, Nir},
title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
year = {2009},
isbn = {0262013193},
publisher = {The MIT Press},
abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series}
}


@misc{zheng2018dagstearscontinuousoptimization,
      title={DAGs with NO TEARS: Continuous Optimization for Structure Learning}, 
      author={Xun Zheng and Bryon Aragam and Pradeep Ravikumar and Eric P. Xing},
      year={2018},
      eprint={1803.01422},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1803.01422}, 
}

@misc{poon2012sumproductnetworksnewdeep,
      title={Sum-Product Networks: A New Deep Architecture}, 
      author={Hoifung Poon and Pedro Domingos},
      year={2012},
      eprint={1202.3732},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1202.3732}, 
}

@misc{darwiche2022causalinferenceusingtractable,
      title={Causal Inference Using Tractable Circuits}, 
      author={Adnan Darwiche},
      year={2022},
      eprint={2202.02891},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2202.02891}, 
}

@inbook{bareinboimpch,
author = {Bareinboim, Elias and Correa, Juan D. and Ibeling, Duligur and Icard, Thomas},
title = {On Pearl’s Hierarchy and the Foundations of Causal Inference},
year = {2022},
isbn = {9781450395861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3501714.3501743},
booktitle = {Probabilistic and Causal Inference: The Works of Judea Pearl},
pages = {507–556},
numpages = {50}
}




@article{DBLP:journals/neco/PetersB15,
  author       = {Jonas Peters and
                  Peter B{\"{u}}hlmann},
  title        = {Structural Intervention Distance for Evaluating Causal Graphs},
  journal      = {Neural Comput.},
  volume       = {27},
  number       = {3},
  pages        = {771--799},
  year         = {2015},
  url          = {https://doi.org/10.1162/NECO\_a\_00708},
  doi          = {10.1162/NECO\_A\_00708},
  timestamp    = {Mon, 26 Oct 2020 08:43:58 +0100},
  biburl       = {https://dblp.org/rec/journals/neco/PetersB15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{
poonia2024chispn,
title={\${\textbackslash}chi\${SPN}: Characteristic Interventional Sum-Product Networks for Causal Inference in Hybrid Domains},
author={Harsh Poonia and Moritz Willig and Zhongjie Yu and Matej Ze{\v{c}}evi{\'c} and Kristian Kersting and Devendra Singh Dhami},
booktitle={The 40th Conference on Uncertainty in Artificial Intelligence},
year={2024},
url={https://openreview.net/forum?id=s3kqfH5KBI}
}

@inproceedings{
causalnormjavaloy2023,
title={Causal normalizing flows: from theory to practice},
author={Adri{\'a}n Javaloy and Pablo Sanchez Martin and Isabel Valera},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=QIFoCI7ca1}
}

@misc{careflkhemakhem2021,
      title={Causal Autoregressive Flows}, 
      author={Ilyes Khemakhem and Ricardo Pio Monti and Robert Leech and Aapo Hyvärinen},
      year={2021},
      eprint={2011.02268},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2011.02268}, 
}

@misc{vacasanchezmartin2021,
      title={VACA: Design of Variational Graph Autoencoders for Interventional and Counterfactual Queries}, 
      author={Pablo Sanchez-Martin and Miriam Rateike and Isabel Valera},
      year={2021},
      eprint={2110.14690},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2110.14690}, 
}

@ARTICLE{jensenshannondivergence,
  author={Lin, J.},
  journal={IEEE Transactions on Information Theory}, 
  title={Divergence measures based on the Shannon entropy}, 
  year={1991},
  volume={37},
  number={1},
  pages={145-151},
  keywords={Entropy;Probability distribution;Upper bound;Pattern analysis;Signal analysis;Signal processing;Pattern recognition;Taxonomy;Genetics;Computer science},
  doi={10.1109/18.61115}}

@article{JMLR:v13:gretton12a,
  author  = {Arthur Gretton and Karsten M. Borgwardt and Malte J. Rasch and Bernhard Sch{{\"o}}lkopf and Alexander Smola},
  title   = {A Kernel Two-Sample Test},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {25},
  pages   = {723-773},
  url     = {http://jmlr.org/papers/v13/gretton12a.html}
}

@misc{structuralcausalcircuits,
author = {Busch, Florian and Willig, Moritz and Zečević, Matej and Kersting, Kristian and Dhami, Devendra},
year = {2023},
month = {01},
pages = {},
title = {Structural Causal Circuits: Probabilistic Circuits Climbing All Rungs of Pearl's Ladder of Causation},
doi = {10.2139/ssrn.4578551}
}

@article{ALONSOBARBA2013429,
title = {Scaling up the Greedy Equivalence Search algorithm by constraining the search space of equivalence classes},
journal = {International Journal of Approximate Reasoning},
volume = {54},
number = {4},
pages = {429-451},
year = {2013},
note = {Eleventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2011)},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2012.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X12001636},
author = {Juan I. Alonso-Barba and Luis delaOssa and Jose A. Gámez and Jose M. Puerta},
keywords = {Bayesian networks, Greedy Equivalence Search, Constrained search},
abstract = {Greedy Equivalence Search (GES) is nowadays the state of the art algorithm for learning Bayesian networks (BNs) from complete data. However, from a practical point of view, this algorithm may not be efficient enough to deal with data from high dimensionality and/or complex domains. This paper proposes some modifications to GES aimed at increasing its efficiency. Under the faithfulness assumption, the modified algorithms preserve the same theoretical properties of the original one, that is, they recover a perfect map of the target distribution in the large sample limit. Moreover, experimental results confirm that, although the proposed methods carry out a significantly smaller number of computations, the quality of the BNs learned can be compared with those obtained with GES.}
}

@article{gies,
author = {Hauser, Alain and B\"{u}hlmann, Peter},
title = {Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventionalMarkov equivalence defines a finer partitioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study.},
journal = {J. Mach. Learn. Res.},
month = {aug},
pages = {2409–2464},
numpages = {56},
keywords = {interventions, greedy equivalence search, graphical model, causal inference, Markov equivalence}
}



@InProceedings{pmlr-v108-zheng20a,
  title = 	 {Learning Sparse Nonparametric DAGs},
  author =       {Zheng, Xun and Dan, Chen and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3414--3425},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/zheng20a/zheng20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/zheng20a.html},
  abstract = 	 {We develop a framework for learning sparse nonparametric directed acyclic graphs (DAGs) from data. Our approach is based on a recent algebraic characterization of DAGs that led to the first fully continuous optimization for score-based learning of DAG models parametrized by a linear structural equation model (SEM). We extend this algebraic characterization to nonparametric SEM by leveraging nonparametric sparsity based on partial derivatives, resulting in a continuous optimization problem that can be applied to a variety of nonparametric and semiparametric models including GLMs, additive noise models, and index models as special cases. Unlike existing approaches that require specific modeling choices, loss functions, or algorithms, we present a completely general framework that can be applied to general nonlinear models (e.g. without additive noise), general differentiable loss functions, and generic black-box optimization routines.}
}



@InProceedings{pmlr-v37-germain15,
  title = 	 {MADE: Masked Autoencoder for Distribution Estimation},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}


@misc{winkler2023learninglikelihoodsconditionalnormalizing,
      title={Learning Likelihoods with Conditional Normalizing Flows}, 
      author={Christina Winkler and Daniel Worrall and Emiel Hoogeboom and Max Welling},
      year={2023},
      eprint={1912.00042},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.00042}, 
}


@InProceedings{pmlr-v97-yu19a,
  title = 	 {{DAG}-{GNN}: {DAG} Structure Learning with Graph Neural Networks},
  author =       {Yu, Yue and Chen, Jie and Gao, Tian and Yu, Mo},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7154--7163},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/yu19a/yu19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/yu19a.html},
  abstract = 	 {Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at \url{https://github.com/fishmoon1234/DAG-GNN}.}
}


@misc{feigenbaum2023unlikelihooddseparation,
      title={On the Unlikelihood of D-Separation}, 
      author={Itai Feigenbaum and Huan Wang and Shelby Heinecke and Juan Carlos Niebles and Weiran Yao and Caiming Xiong and Devansh Arpit},
      year={2023},
      eprint={2303.05628},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.05628}, 
}

@article{Erdos1984OnTE,
  title={On the evolution of random graphs},
  author={Paul L. Erdos and Alfr{\'e}d R{\'e}nyi},
  journal={Transactions of the American Mathematical Society},
  year={1984},
  volume={286},
  pages={257-257},
  url={https://api.semanticscholar.org/CorpusID:6829589}
}

@article{barabasi1999emergence,
  title={Emergence of scaling in random networks},
  author={Barab{\'a}si, Albert-L{\'a}szl{\'o} and Albert, R{\'e}ka},
  journal={science},
  volume={286},
  number={5439},
  pages={509--512},
  year={1999},
  publisher={American Association for the Advancement of Science}
}

@article{loh_mse_unsuitable,
author = {Loh, Po-Ling and B\"{u}hlmann, Peter},
title = {High-dimensional learning of linear causal networks via inverse covariance estimation},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared l2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for high-dimensional consistency of our two-part algorithm, defined in terms of a "gap" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3065–3105},
numpages = {41},
keywords = {causal inference, dynamic programming, identifiability, inverse covariance matrix estimation, linear structural equation models}
}

@article{Rubin1974EstimatingCE,
  title={Estimating causal effects of treatments in randomized and nonrandomized studies.},
  author={Donald B. Rubin},
  journal={Journal of Educational Psychology},
  year={1974},
  volume={66},
  pages={688-701},
  url={https://api.semanticscholar.org/CorpusID:52832751}
}

@inproceedings{Friedman2000GaussianPN,
  title={Gaussian Process Networks},
  author={Nir Friedman and Iftach Nachman},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2000},
  url={https://api.semanticscholar.org/CorpusID:674416}
}

@article{Cevid2020DistributionalRF,
  title={Distributional Random Forests: Heterogeneity Adjustment and Multivariate Distributional Regression},
  author={Domagoj Cevid and Loris Michel and Jeffrey N{\"a}f and Peter B{\"u}hlmann and Nicolai Meinshausen},
  journal={J. Mach. Learn. Res.},
  year={2020},
  volume={23},
  pages={333:1-333:79},
  url={https://api.semanticscholar.org/CorpusID:219124044}
}

@inproceedings{
tigas2022interventions,
title={Interventions, Where and How? Experimental Design for Causal Models at Scale},
author={Panagiotis Tigas and Yashas Annadani and Andrew Jesson and Bernhard Sch{\"o}lkopf and Yarin Gal and Stefan Bauer},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=ST5ZUlz_3w}
}

@ARTICLE{towardsefficientlcsl,
  author={Yang, Shuai and Wang, Hao and Yu, Kui and Cao, Fuyuan and Wu, Xindong},
  journal={IEEE Transactions on Big Data}, 
  title={Towards Efficient Local Causal Structure Learning}, 
  year={2022},
  volume={8},
  number={6},
  pages={1592-1609},
  keywords={Learning systems;Bayes methods;Markov processes;Big Data;Skeleton;Optimization;Algorithms;Bayesian network;Markov blanket;local causal structure learning},
  doi={10.1109/TBDATA.2021.3062937}}





@book{Imbens_Rubin_2015, 
place={Cambridge}, 
title={Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction}, 
publisher={Cambridge University Press}, 
author={Imbens, Guido W. and Rubin, Donald B.}, year={2015}
} 


@article{10.1214/18-AOS1709,
author = {Susan Athey and Julie Tibshirani and Stefan Wager},
title = {{Generalized random forests}},
volume = {47},
journal = {The Annals of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics},
pages = {1148 -- 1178},
keywords = {Asymptotic theory, Causal inference, instrumental variable},
year = {2019},
doi = {10.1214/18-AOS1709},
URL = {https://doi.org/10.1214/18-AOS1709}
}


@InProceedings{pmlr-v108-janzing20a,
  title = 	 {Feature relevance quantification in explainable AI: A causal problem},
  author =       {Janzing, Dominik and Minorics, Lenon and Bloebaum, Patrick},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2907--2916},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/janzing20a/janzing20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/janzing20a.html},
  abstract = 	 {We discuss promising recent contributions on quantifying feature relevance using Shapley values, where we observed some confusion on which probability distribution is the right one for dropped features. We argue that the confusion is based on not carefully distinguishing between observational and interventional conditional probabilities and try a clarification based on Pearl’s seminal work on causality. We conclude that unconditional rather than conditional expectations provide the right notion of dropping features. This contradicts the view of the authors of the software package SHAP. In that work, unconditional expectations (which we argue to be conceptually right) are only used as approximation for the conditional ones, which encouraged others to ’improve’ SHAP in a way that we believe to be flawed.}
}


@misc{gu2022efficientlymodelinglongsequences,
      title={Efficiently Modeling Long Sequences with Structured State Spaces}, 
      author={Albert Gu and Karan Goel and Christopher Ré},
      year={2022},
      eprint={2111.00396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.00396}, 
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{kipf2016variationalgraphautoencoders,
      title={Variational Graph Auto-Encoders}, 
      author={Thomas N. Kipf and Max Welling},
      year={2016},
      eprint={1611.07308},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1611.07308}, 
}

@misc{ahmed2022semanticprobabilisticlayersneurosymbolic,
      title={Semantic Probabilistic Layers for Neuro-Symbolic Learning}, 
      author={Kareem Ahmed and Stefano Teso and Kai-Wei Chang and Guy Van den Broeck and Antonio Vergari},
      year={2022},
      eprint={2206.00426},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.00426}, 
}

@InProceedings{pmlr-v246-busch24a,
  title = 	 {PsiNet: Efficient Causal Modeling at Scale},
  author =       {Busch, Florian Peter and Willig, Moritz and Seng, Jonas and Kersting, Kristian and Dhami, Devendra Singh},
  booktitle = 	 {Proceedings of The 12th International Conference on Probabilistic Graphical Models},
  pages = 	 {452--469},
  year = 	 {2024},
  editor = 	 {Kwisthout, Johan and Renooij, Silja},
  volume = 	 {246},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11--13 Sep},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v246/main/assets/busch24a/busch24a.pdf},
  url = 	 {https://proceedings.mlr.press/v246/busch24a.html},
  abstract = 	 {Being a ubiquitous aspect of human cognition, causality has made its way into modern-day machine-learning research. Despite its importance in real-world applications, contemporary research still struggles with high-dimensional causal problems. Leveraging the efficiency of probabilistic circuits, which offer tractable computation of marginal probabilities, we introduce $\Psi$net, a probabilistic model designed for large-scale causal inference. $\Psi$net is a type of sum-product network where layering and the einsum operation allow for efficient parallelization. By incorporating interventional data into the learning process, the model can learn the effects of interventions and make predictions based on the specific interventional setting. Overall, $\Psi$net is a causal probabilistic circuit that efficiently answers causal queries in large-scale problems. We present evaluations conducted on both synthetic data and a substantial real-world dataset, demonstrating $\Psi$net’s ability to capture causal relationships in high-dimensional settings.}
}

@article{10.5555/2627435.2750365,
author = {Colombo, Diego and Maathuis, Marloes H.},
title = {Order-independent constraint-based causal structure learning},
year = {2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The first step of all these algorithms consists of the adjacency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
journal = {J. Mach. Learn. Res.},
pages = {3741–3782},
numpages = {42},
}

@article{lingam,
  author  = {Shohei Shimizu and Patrik O. Hoyer and Aapo Hyvarinen and Antti Kerminen},
  title   = {A Linear Non-Gaussian Acyclic Model for Causal Discovery},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {72},
  pages   = {2003--2030},
  url     = {http://jmlr.org/papers/v7/shimizu06a.html}
}

@misc{amin2024scalableflexiblecausaldiscovery,
      title={Scalable and Flexible Causal Discovery with an Efficient Test for Adjacency}, 
      author={Alan Nawzad Amin and Andrew Gordon Wilson},
      year={2024},
      eprint={2406.09177},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.09177}, 
}

@article{LI2023107123,
title = {Causal-ViT: Robust Vision Transformer by causal intervention},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {107123},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107123},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623013076},
author = {Wei Li and Zhixin Li and Xiwei Yang and Huifang Ma},
keywords = {Artificial intelligence, Vision Transformer, Causal intervention, Image recognition, Structural causal model},
abstract = {Artificial intelligence based on deep learning is better at improving the representation ability of models from data. However, due to the limitation of fixed receptive field, these agents are not able to provide a correct response outside the fixed receptive field. To address this problem, this paper provides a new perspective with improving the Image Recognition tasks. This study firstly constructs two extended receptive fields using structural causal model. Then, an approximate intervention method that changes the traditional likelihood prediction to predict the result of causal intervention is proposed. Finally, this study formulates the objective function to adapt the proxy training, which makes the whole model work well. Above all of these, a new Vision Transformer variant named Causal-ViT is proposed. Furthermore, rich experimental results of different tasks are reported. These results show that the proposed perspective makes a significant improvement in Image Recognition tasks. By simply plugging Causal-ViT to different sub-tasks, all of them bring the new benchmarks of themselves field, which proves our method is flexible.}
}

@inproceedings{xu2018how,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryGs6iA5Km},
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}


@InProceedings{pmlr-v236-mogensen24a,
  title = 	 {Causal discovery in a complex industrial system: A time series benchmark},
  author =       {Mogensen, S{\o}ren Wengel and Rathsman, Karin and Nilsson, Per},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {1218--1236},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/mogensen24a/mogensen24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/mogensen24a.html},
  abstract = 	 {Causal discovery outputs a causal structure, represented by a graph, from observed data. For time series data, there is a variety of methods, however, it is difficult to evaluate these on real data as realistic use cases very rarely come with a known causal graph to which output can be compared. In this paper, we present a dataset from an industrial subsystem at the European Spallation Source along with its causal graph which has been constructed from expert knowledge. This provides a testbed for causal discovery from time series observations of complex systems, and we believe this can help inform the development of causal discovery methodology.}
}

@misc{mhalla2020causalmechanismextremeriver,
      title={Causal mechanism of extreme river discharges in the upper Danube basin network}, 
      author={Linda Mhalla and Valérie Chavez-Demoulin and Debbie J. Dupuis},
      year={2020},
      eprint={1907.03555},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/1907.03555}, 
}

@misc{mamaghan2024evaluationbayesian,
      title={Challenges and Considerations in the Evaluation of Bayesian Causal Discovery}, 
      author={Amir Mohammad Karimi Mamaghan and Panagiotis Tigas and Karl Henrik Johansson and Yarin Gal and Yashas Annadani and Stefan Bauer},
      year={2024},
      eprint={2406.03209},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03209}, 
}

@book{budynas2008shigley,
  title={Shigley's Mechanical Engineering Design},
  author={Budynas, R.G. and Nisbett, J.K.},
  isbn={9780073121932},
  lccn={2003048763},
  series={McGraw-Hill series in mechanical engineering},
  url={https://books.google.de/books?id=ftfQngEACAAJ},
  year={2008},
  publisher={McGraw-Hill}
}

@book{Eslami2013TheoryOE,
  title={Theory of Elasticity and Thermal Stresses},
  author={Mohammad Reza Eslami and Richard B. Hetnarski and J{\'o}zef Ignaczak and N. Noda and Naobumi Sumi and Yoshinobu Tanigawa},
publisher = "Springer",
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:138619621}
}

@misc{peters2014structuralinterventiondistancesid,
      title={Structural Intervention Distance (SID) for Evaluating Causal Graphs}, 
      author={Jonas Peters and Peter Bühlmann},
      year={2014},
      eprint={1306.1043},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1306.1043}, 
}

@misc{wahl2024separationbaseddistancemeasurescausal,
      title={Separation-based distance measures for causal graphs}, 
      author={Jonas Wahl and Jakob Runge},
      year={2024},
      eprint={2402.04952},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2402.04952}, 
}