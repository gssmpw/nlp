\section{Related Work}
In this section we analyze related approaches relevant to our work and datasets, highlighting common points and dissimilarities. For more exhaustive surveys on the evaluation of causal models, we address the interested reader to \cite{Cheng2022EvaluationMA}, \cite{DBLP:journals/csur/GuoCLH020} and \cite{DBLP:journals/tkdd/YaoCLLGZ21}.

\textbf{Large-Scale Causality:} In \cite{notallcausalinferencezece}, a theoretical and empirical evaluation on simple causal graphs highlighted the intractability of marginal inference and the scaling laws of different causal models. When the goal is to reduce the complexity of different intractable queries, it is possible to adopt \textit{tractable probabilistic models} such as \textit{Sum-Product Networks} (SPNs) \citep{poon2012sumproductnetworksnewdeep}. Furthermore, it is possible to use SPNs to model causal phenomena \citep{zecevic2021interventional, structuralcausalcircuits, poonia2024chispn, pmlr-v246-busch24a}.

Leveraging its independence from combinatorial objects such as graphs, \textit{Rubin's Potential Outcomes} (PO) framework \citep{Imbens_Rubin_2015} can be used to tackle the scalability problem. However, a notable limitation of the PO framework is its reliance on assumptions like \textit{ignorability}, that is equivalent to unconfoundedness and is not suitable for our strongly confounded use-case.

In the realm of causal discovery, scaling is addressed with novel methodologies such as continuous optimization-based approaches \citep{NEURIPS2018notears, 10.5555/3495724.3497230, DBLP:conf/iclr/LachapelleBDL20} or divide-and-conquer approaches \citep{lopez2022factorgraphs, wu2024sea}. 
However, while easier to scale, they suffer from distinct vulnerabilities. 
~\cite{Reisach2021BewareOT} and ~\cite{Kaiser2021UnsuitabilityON} show that their performance is sensitive to the scale of the data, and can degrade to levels comparable to or worse than classic approaches after data normalization. On a similar note \cite{loh_mse_unsuitable} and \cite{seng2024learning} remarked the limitations of methods relying on mean squared error losses. 
Further, \cite{mamaghan2024evaluationbayesian} studied the drawbacks of common metrics when adopting a Bayesian approach.
%First, results suggest these methods may be exploiting patterns of increasing marginal variance in data to reconstruct a causal graph, without any guaranteed analysis at a Causal level. Consequently, their performance can be controlled by manipulating the scale of the data~\cite{Reisach2021BewareOT, Kaiser2021UnsuitabilityON}, degrading to levels comparable to or worse than classic approaches after data normalization. 
Those drawbacks of ML-Based approaches re-ignited interest in novel and more mathematically grounded methods such as \textit{Extremely Greedy Equivalence Search} (XGES) \cite{nazaret2024extremely} or \textit{Differential Adjacency Test} (DAT) \cite{amin2024scalableflexiblecausaldiscovery}. 
\begingroup
%\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.25} % Default value: 1
\begin{table*}[t]
\small
\centering
\begin{tabular}{p{2.5cm}|m{1.2cm}|m{1.2cm}|m{1.2cm}|m{1.2cm}|m{1.65cm}|m{1.5cm}} 
 & Nonlinear & Mixed types & Cond. dependencies & Causal insufficiency & Interventional data & Large-scale  \\
\hline
CausalMan (ours) & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark \\
CausalChambers & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark \\
CausalAssembly & \cmark & \cmark & \xmark & \xmark & \cmark & \cmark \\
CausalBench & \cmark & \xmark & \xmark & \cmark & \cmark & \cmark \\
Neuropathic-pain & \cmark & \xmark & \xmark & \xmark & \cmark & \cmark \\
\end{tabular}
\caption{Comparison of CausalMan's main features with other available simulators or datasets.}
\label{table:causalman_comparison}
\end{table*}
\endgroup

\textbf{Datasets and Benchmarks:}
A wide variety of benchmarks for causal models are publicly available~\citep{asiadataset, alarmdataset, sachsdataset}.
However, only a limited number of them target large scale scenarios~\cite{diabetesdataset}, and an even smaller fraction involve hybrid domains, which is the focus of our datasets and experiments.
To compensate the lack of data, a common choice for analysing scaling laws for causal models is to generate random Erdos-Renyi~\citep{Erdos1984OnTE} or Scale-Free graphs~\citep{barabasi1999emergence} which, although easy to simulate, are far from reflecting the real world. 
Recent works provide datasets and methodologies to generate realistic synthetic and semi-synthetic data. 
Semi-synthetic DGPs tuned on real data, often along with the use of prior domain knowledge, are the focus of simulators such as \textit{CausalAssembly} ~\citep{g√∂bler2024causalassembly} for the manufacturing domain, or the \textit{Neuropathic Pain simulator} ~\citep{DBLP:conf/nips/Tu0BK019} in the medical domain. Further, semi-synthetic DGPs are used in \cite{Dorie2017AutomatedVD,Hahn2019AtlanticCI} and \cite{Shimoni2018BenchmarkingFF} to generate datasets with real observational data for the untreated individuals, coupled with simulated treated counterparts.
Contrary to those datasets, our data comprise additional layers of complexity by simulating mechanisms such as batching, hybrid data-types and conditional dependencies.
% Real Data
Concentrating on real world data, CausalBench ~\citep{Chevalley2022CausalBenchAL} is a large scale benchmark for single-cell perturbation experiments with interventional data gathered using gene-editing technologies. 
A different strategy is adopted by CausalChambers~\citep{gamella2024causalchambers}, which builds a real isolated physical system where physical mechanisms are known almost perfectly, giving a high degree of confidence on the exactness of the ground-truth Structural Causal Model. Additionally, \cite{pmlr-v236-mogensen24a, mhalla2020causalmechanismextremeriver} provide real-world datasets with a more or less justified ground-truth causal graph.