
\section{Datasets Release}
All the data used in this paper, and more, is available at this link: \href{https://zenodo.org/records/13871097}{Link to Zenodo anonymous repository.}

\section{Additional task} \label{sec:appendix_additional_task}
In this section we study an additional task, which is more constructed than the others, and is analyzes a corner case where results appear drastically different than in the other tasks.
\begin{equation}
    ATE = \mathbb{E}[Y | do(PF\_M1\_T1\_Force\_MpGood=0)] - \mathbb{E}[Y | do(PF\_M1\_T1\_Force\_MpGood=1)]
\end{equation}
On this task, we are intervening directly on the outcome's direct parent. The outcome variable has only binary parents, and its structural equation is computing the logic AND between all of them. The treatment is setting one of those parents to 0, which leads to the outcome distribution to be 0 with 100\% probability. The control group instead is set to 1. As before, the outcome variable is $Y = Sec\_C2\_Machine1\_ProcessResult$. When conditioning, the evidence variable is still $HU\_HU\_Block\_Type\_ID\_num$, which will be assumed to be observed with value $921$.

Surprisingly, we observe in Fig.\ref{fig:ate_mse_vs_size} and table \ref{table:effect_estimation_results} how a simple linear regression outperforms all other causal models. To understand this result, it is essential to notice that the intervened variable is on the \textit{markov blanket} of the outcome, making this behavior expected in a SCM-based DGP. Moreover, we notice that for every causal model, apart from regression-based techniques, ATE or CATE is not estimated directly. We are estimating the empirical treatment effect.
Indeed, in those models, treatment effects are estimated by averaging over samples from the interventional distributions for treated and control populations. Interestingly, deep causal models exhibit superior performance when estimating the treated interventional distributions while being highly inaccurate for treatment effect estimation (Fig.\ref{fig:interventional_distributions_1_task_1_causalman_small} and \ref{fig:interventional_distributions_2_task_1_causalman_small}). 
This can be explained by looking at the discrepancy between the JS-Divergence of the reconstructed interventional distributions for the treated and control groups. In Fig.\ref{fig:interventional_distributions_1_task_1_causalman_small}, \ref{fig:interventional_distributions_2_task_1_causalman_small} and \ref{fig:jensen_shannon_comparison} it can be seen that, even though the treated population is often accurate, the control population is instead mostly poorly estimated. However, accurate treatment effect estimation using those models require precise reconstructions of both treated and control distributions, and the best-performing models overall are simple regression-based techniques that do not go through this procedure and target ATE or CATE directly.

\begin{table*}[t] 
\centering
\begin{tabular}{ p{1.5cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm} }
 %\hline
 Model & ATE MSE & CATE MSE & JS-Div Tr. &  MSE &  MMD\\
 \hline
 CBN   & 1.433(0.061)  & 1.653(0.035) & 0.319(0.002) & 0.742 (0.003) &  0.734(0.116) \\
 NCM   & 1.75(0.068)   & 1.502(0.141) & 0.589 (0.000) & 1.000 (0.000) &  0.396(9.023) \\
 CAREFL & 1.332 (0.211)   & \textbf{1.574(0.288)} & 0.512 (0.093) & 0.939 (0.088) &  \textbf{0.035 (0.087)} \\
 CNF   & 1.913(0.018)   & 1.8(0.04) & \textbf{0.291(6e-5)} & 0.707 (0.000)&   Nan \\
 VACA  & 1.907(0.009)   & 1.974(0.274) & 0.332(0.01) & \textbf{0.339 (0.005)}&  0.319(0.009) \\
 Linear r. & \textbf{0.229(0.004)}   & - & - & - & -  \\
 Logistic r.& 1.439(0.008)   & - & - & - &  - \\
 %\hline
\end{tabular}
\caption{Comparison for the additional task on CausalMan Small with $n = 50.000$ samples and ground truth ADMG. Instabilities during sampling prevented to evaluate MMD for CNF, as multiple datapoints diverged to $+\infty$.}.
\label{table:effect_estimation_results}
\end{table*}
\begin{figure*}[t]
\centering
\subfloat[]{\includegraphics[width=0.48\textwidth]{figures/ate_dataset0_vs_size_svg-tex.pdf} \label{fig:ate_mse_vs_size}}
\subfloat[]{\includegraphics[width=0.52\textwidth]{figures/JS_Comparison_dataset_size_50000_svg-tex.pdf}\label{fig:jensen_shannon_comparison}}
\caption{CausalMan Small. Figure \ref{fig:ate_mse_vs_size} shows a stagnation in performance for effect estimation, even with the use of more data. 
Figure \ref{fig:jensen_shannon_comparison}, instead, illustrates the JS-Div. accuracy of treated and control distributions for learning-based causal models, after training with $n = 50.000$ samples.}
\end{figure*}
\section{Sampling}\label{appendix:sampling}
The batching mechanism affects how data is sampled. First, the production line is unique, and all batches share the same causal graph. What varies between batches is their parametrization. For example one batch may be related to product A, while another batch may be producing product B. Since product A has certain characteristics, the underlying SCM will have a specific parametrization related to product A. Similarly, samples related to product B will be sampled from an SCM with a parametrization related to product B. 

Therefore, batches have to be sampled separately. Within a batch we can perform \textit{ancestral sampling} \citep{koller} on the SCM \textit{related to the batch}. 
In practice, for every batch we set one parametrization of the SCM, and only then we perform ancestral sampling. For next batches we repeat this procedure by setting new parameters on the SCM and then sampling again. This is one of the mechanisms that give rise to the discrete mode changes described in \ref{sec:dataset_data_requirements}.

\textbf{Interventional data:} Interventions are defined within a batch, and Interventional data is sampled by first setting the correct SCM parameterization relative to the batch, and then applying the hard/soft intervention. 
Next, ancestral sampling  is performed as for observational data. 
In other words, we have \textit{Interventional Batches} where a batch is sampled while an intervention is being applied. This procedure is also applied when sampling the ground truth data for treated and control groups during the treatment effect estimation experiments.

\section{Causal Mechanisms} \label{sec:appendix_conditional_depepdences}

We proceed by describing the details of the DGP.

\subsection{Conditional Dependencies:} Given a node $n_1$, its distribution may depend on the value of one or more categoricals such as the supplier or the component type. For a node $n_1$ depending on a single categorical A, we can write it mathematically as
\begin{equation}
    n_1 \sim \begin{cases}
    \mathcal{N}(\mu_0, \sigma_0) & \text{if $ A = a_0$},\\
    \mathcal{N}(\mu_1, \sigma_1) & \text{if $A = a_1$},
  \end{cases} 
\end{equation}

where $\mu_i$ and $\sigma_i$ are the mean and standard deviation of two Gaussian distributions, with $\mu_0 \neq \mu_1$ and $\sigma_0 \neq \sigma_1$. In Fig. \ref{fig:second_order_uncertainty}, we provide a graphical illustration for a simple conditional dependency.

\begin{figure}{
\centering
\adjustbox{valign = c}{\begin{tikzpicture}
    % Define nodes
    \node (A1) at (0, 2) {$A = a_1$};
    \node (B1) at (3, 2) {\textcolor{blue}{$B|A=a_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)$}};

    \node (A2) at (0, 1) {$A = a_2$};
    \node (B2) at (3, 1) {\textcolor{red}{$B|A=a_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)$}};

    \node (A3) at (0, 0) {$A = a_3$};
    \node (B3) at (3, 0) {\textcolor{green}{$B|A=a_3 \sim \mathcal{N}(\mu_3, \sigma_3^2)$}};
    
    % Draw arrows
    \draw[->, double distance=2pt, -{Latex[length=2mm, width=3mm]}] (A1) -- (B1);
    \draw[->, double distance=2pt, -{Latex[length=2mm, width=3mm]}] (A2) -- (B2);
    \draw[->, double distance=2pt, -{Latex[length=2mm, width=3mm]}] (A3) -- (B3);
\end{tikzpicture}}}
\adjustbox{valign = c}{\includegraphics[width=0.55\textwidth]{mixture_of_gaussians_svg-tex.pdf}}
\caption{Example of a conditional dependency where A (categorical) determines the distribution of B. Node distributions are often not fixed a-priori, and their parameters are determined by the value of a number of categorical (parent) variables. The resulting marginal distribution can be asymmetric and multimodal.}\label{fig:second_order_uncertainty}
\includegraphics[width=\textwidth]{figures/monitoring_svg-tex.pdf}
\caption{Given a \textit{monitored} variable B, a monitoring mechanism checks if its value lies within an ideal range defined by the interval $\left[\text{B\_LTL}, \text{B\_UTL}\right]$. If yes, a binary r.v. B\_MpGood will be \textit{True}, signaling that the attribute is conformal, otherwise \textit{False}. At the end of production, all the MpGood variables are aggregated into a ProcessResult variable via a logic AND operation, which consequently signal if the whole production process did run successfully.}\label{fig:monitoring}
\end{figure}

\subsection{Structural Equations} \label{appendix:structural_equations}

Hereby we provide a more in-depth description of the production process, along with its relative physical description and structural equations. For more in-depth mathematical derivations, we address the interested reader to \cite{budynas2008shigley} and \cite{Eslami2013TheoryOE}. 

\paragraph{Model of a Magnetic Valve:}
A magnetic Valve is modeled by different parameters that describe its geometric and material properties. 
The Parameters are $E_{mv}$, describing the material elasticity of the valve, $A_{leak_{MV_{raw}}}$ describing the leakage area before starting production (A supplier may give us faulty MVs), $D_{mvMax}$ describing the maximum diameter, and $D_{mvMin}$ describing the minimum diameter, and $L_{mvPF}$ describing the axial length of the MV, coinciding with the optimal engagement length between the MV and the bore during the PF process.

All those parameters are not fixed, and are indeed randomly sampled from a distribution which conditionally depend on the type and supplier of the MV. Each combination of supplier and MV type implies a different node distribution for those parameters. This mechanism is a conditional dependency as described in \ref{sec:appendix_conditional_depepdences}. Those conditional dependencies cause the marginal node distribution of those parameters to be multimodal and asymmetric. In other words, conditional dependencies induce a mixture model on the marginal node distributions.

\paragraph{Model of an Hydraulic Unit:}
An HU is modeled with the same approach as for a MV. Indeed, an HU has the parameters $E_{hu}$ describing its elasticity and a $Force_{Lim}$ describing the force which is necessary to cause a non-zero leakage area.

On each HU we have different \textit{chambers}, and every chamber has a certain number of \textit{bores}. We model \textit{each individual} bore in the HU with a set of parameters. Specifically, we have $E_{bore}$ describing the elasticity of the bore, $D_{boreMax}$ and $D_{boreMin}$ describing its maximum and minimum diameter.
In this case, conditional dependencies appear both for the general HU parameters $E_{hu}$ and $Force_{Lim}$, but also in the parametrization of each individual bore. 

\paragraph{Intrinsic Magnetic Valve Leakage:}
A magnetic valve could be manufactured in a faulty way, resulting in \textit{intrinsic leakage} through the valve, even in the “closed state”. If quality control of the MV supplier works well, this intrinsic leakage should be zero. 
However, it may also happen that a magnetic valve gets damaged during assembly (e.g. due to high forces during press-fitting), leading to leakage through the valve itself. 
The initial intrinsic leakage of the valve as delivered by the supplier is modeled using $A_{leak_MV}$. As small intrinsic leakages are more likely than high values, and as the leakage area is continuous, we modeled a probability distribution for $A_{leak_{MV_{raw}}}$ and then used a ReLU function to cut off unrealistic negative leakage area values. 
\begin{equation}
    A_{leakMV} = \text{ReLU}(A_{leak_{MV_{raw}}})
\end{equation}

\paragraph{Total Leakage Area of a Chamber}
The total leakage area of a chamber in the Hydraulic Unit block is the sum of the leakage areas of each bore/Magnetic Valve in the chamber
\begin{equation}
    A_{leak_{tot}} = A_{leak_{Bore_1}} + A_{leak_{Bore_2}} + …,
\end{equation}
where $A_{leak_{Bore}}$ is the total leakage are per bore/Magnetic valve.

The fluid is assumed to be able to take two different leakage paths, one through the valve itself ($A_{leak_{MV}}$, see below for details) and one through the Press-Fitting connection ($A_{leak_{PF}}$).  Therefore, for a single bore, the total leakage area is the sum of the leakage though the MV and through the PF.
\begin{equation}
    A_{leak_{Bore}} = A_{leak_{MV}} + A_{leak_{PF}}
\end{equation}

\paragraph{Leakage area and geometry of the Press-Fitting Connection}

The leakage area through the Press Fitting connection $A_{leak_{PF}}$ depends mainly on the geometry of the bore and Magnetic Valve. As the cylindrical surfaces are not perfectly round, we assume an interval for the maximum $(D_{mv_{Max}}, D_{bore_{Max}})$ and minimum diameter $(D_{mv_{Min}}, D_{bore_{Min}})$, respectively.
When studying the unwanted leakage of fluid, it is important to consider the difference between minimum and maximum diameters, identified as $\Delta D$, as the may have negative consequences for the press-fitting process and result in the scrap of a product. 
\begin{equation}
\begin{split}
    \Delta D_{\text{max}} &= D_{\text{mvMax}} - D_{\text{boreMin}}, \\
    \Delta D_{\text{min}} &= D_{\text{mvMin}} - D_{\text{boreMax}}.
\end{split}
\end{equation}
To account for effects from the machine on the resulting leakage (such as acentric positioning of the valve with respect to the bore during press fitting), we introduce a machine dependent limit for resulting leakage ($LeakTolMachine$). When $\Delta D$ is higher than the threshold \textit{LeakTolMachine}, we observe a leakage (area) through the press-fitting. This phenomenon can be modeled also with a ReLU function as follows 
\begin{equation}
\begin{split}
    \Delta D_{Leak_{min}} &= \Delta D_{min} - \text{LeakTolMachine} \\
    \Delta D_{Leak_{max}} &= \Delta D_{max} - \text{LeakTolMachine}
\end{split}
\implies 
\begin{split}
    A_{Leak_{min}} &= \text{ReLU}(\Delta D_{Leak_{min}}) \\
    A_{Leak_{max}} &= \text{ReLU}(\Delta D_{Leak_{max}})
\end{split}
\end{equation}

Moreover, in real production lines, it is likely that different press-fitting machines have a different threshold for leakage due to badly adjusted press fitting processes. 
Additionally, using the coefficient $\beta_{asym}$ we can model how much the total leakage area is affected by $\Delta D_{Leak_{Min}}$ and $\Delta D_{leak_{Max}}$, respectively. 
\begin{equation} \label{eq:total_leakage_asym}
\begin{split}
    A_{\text{leakPF}} &= \beta_{\text{asym}} A_{Leak_{max}} + (1 - \beta_{\text{asym}}) A_{Leak_{min}},
\end{split}
\end{equation}
where $\beta_{asym} = 1$ means that only the maximum leakage Area $A_{leak_{MV}}$ is effective, a value of 0.5 means that minimum and maximum leakage area are weighted equally. 
\paragraph{The Press Fit process}

The PF machine applies a force which inserts the MV into a bore of the HU. Apart from inserting the MV into the bore, the force will also deform the bore. At the end of the process the bore will be deeper than before by a certain amount which is determined by the physical models (with some stochasticity). Part of the deformation is permanent, and another other part will disappear after the pressing force is removed at the end of the process, as it is related to the elasticity of the material. If the force is too high, we may cause a damage that will end in the component being scrapped.
We start by defining the effective elasticity modulus $E_{\text{eff}}$ as
\begin{equation}
    E_{\text{eff}} = \left(\frac{1}{E_{\text{bore}}} + \frac{1}{E_{\text{mv}}}\right)^{-1}
\end{equation}
where $E_{\text{bore}}$ is the elasticity of the bore and $E_{\text{mv}}$ the elasticity of the MV.
The effective elasticity is used to define the stiffness of the press-fitting machine as
\begin{equation} \label{eq:pf_stiffness}
    K_{\text{stiffPF}} = K_{\text{stiffPF}_{Ref}} \cdot \frac{\Delta D_{\text{mean}}}{K_{\text{stiffPF}_{\Delta D_{Ref}}}} \cdot \frac{E_{\text{eff}}}{K_{\text{stiffPF}_{E_{Ref}}}},
\end{equation}

where $K_{\text{stiffPF}_{Ref}}$, $K_{\text{stiffPF}_{\Delta D_{Ref}}}$, and $K_{\text{stiffPF}_{E_{Ref}}}$ are new machine-dependent parameters describing how much the reference stiffness of the PF machine $K_{\text{stiffPF}_{Ref}}$ varies linearly with $\Delta D_{\text{mean}}$ and $E_{\text{eff}}$. As before, those reference parameter are not absolute and may vary across different PF machines. Moreover, in \ref{eq:pf_stiffness} $\Delta D_{\text{mean}}$ is modeled similarly to Eq.\ref{eq:total_leakage_asym}, where we use $\beta_{asym}$ again to balance how much the PF process is affected by the maximum and minimum diameter,
\begin{equation}
    \Delta D_{\text{mean}} = \beta_{\text{asym}}\Delta D_{\text{max}} + (1 - \beta_{\text{asym}})\Delta D_{\text{min}}.
\end{equation}
Now we have all the quantities which are necessary to compute the total stiffness $K_{\text{stiff}}$ of the system,
\begin{equation}
    K_{\text{stiff}} = \left(\frac{1}{K_{\text{stiffMachine}}} + \frac{1}{K_{\text{stiffPF}}}\right)^{-1}
\end{equation}
where $K_{\text{stiffMachine}}$ is the stiffness deriving from the machine itself, and $K_{\text{stiffPF}}$ is the stiffness coming from the press-fitting operation.
Using $K_{\text{stiffPF}}$ it is possible to derive the pressing force as 
\begin{equation}
    \text{Force} = L_{\text{mvPF}} \cdot K_{\text{stiffPF}}
\end{equation}
where we used the axial length of the MV $L_{\text{mvPF}}$, as it coincides with how much the MV should be inserted into the HU with PF.
By dividing the Force by the stiffness of the system $K_{\text{Stiff}}$, we can compute the difference in vertical position of the PF tool before and after the operation, which coincides with the permanent deformation (in depth) of the component, 
\begin{equation}
    \Delta s_{\text{grad}} = \frac{\text{Force}}{K_{\text{stiff}}}.
\end{equation}
We remark that $\Delta s_{\text{grad}}$ also coincides wit the difference in position of the tool before and after the maximum pressing force is achieved and removed. Therefore, it does not include any elastic effect of the material, which may be present only while the pressing force is still present.
The quantity above can be used to compute the final position of the tool $s_{\text{grad}}$
\begin{equation}
    s_{\text{grad}} = s_0 + \Delta s_{\text{grad}}
\end{equation}
where $s_0$ is, instead, the position of the PF tool at the beginning of the process. 


\paragraph{Maximum forces and dispacement on a single bore:}
As written above, during PF multiple forces are applied to insert all MVs into the HU. Focusing on the maximum force $F_{\text{max}}$ achieved on a single bore/MV pair, we can decompose it on the optimal $Force$ variable, plus another variable $\Delta F_{\text{trigger}_{\text{stop}}}$ describing how much the force went over the value Force, before a trigger in the machine did stop the operation.
\begin{equation}
    F_{\text{max}} = \text{Force} + \Delta F_{\text{trigger}_{\text{stop}}},
\end{equation}
where $\Delta F_{\text{trigger}_{\text{stop}}}$ is randomly sampled. 
The reason why we model the maximum force is because, if the applied force is too high, the component will be damaged and result in a leakage.
Moreover, from the maximum bore force we can compute the maximum difference in displacement of the tool during the PF process, written as
\begin{equation}
    \Delta s_{\text{max}} = \frac{\Delta F_{\text{trigger}_{\text{stop}}}}{K_{\text{stiffMachine}}},
\end{equation}
which, with respect to $\Delta s_{\text{grad}}$, includes also the elastic deformation which will disappear after the force is removed. Thanks to $\Delta s_{\text{max}}$ we can get the absolute maximum displacement of the tool,
\begin{equation}
    s_{\text{max}} = s_{\text{grad}} + \Delta s_{\text{max}}.
\end{equation}
The maximum displacement $s_{\text{max}}$ during the process includes both the actual deformation of the component, but also an elastic deformation which will disappear once the pressing force is removed.

\paragraph{Maximum Forces and Displacement:}
Forces applied during PF cannot be higher than a machine and product-dependent threshold $F_{\text{lim}}$, otherwise we might incur in a damage of the components. 
First, we define $F_{\text{max}}$ as the highest value achieved among all maximum forces in the chamber's bores.
Then, we can compute how much the maximum force went over the limit with
\begin{equation}
    \Delta \text{Force} = F_{\text{max}} - F_{\text{Lim}} \quad \implies \quad \Delta \text{Force}_{\text{ReLu}} = \text{ReLU}(\Delta \text{Force})
\end{equation}
where we applied a ReLU again to make it zero if the force was below the limit.
In order to model the relation between the applied forces and potential faults inducing a nonzero leakage area, we model the $\text{LeakTolMachine}$ parameter as follows:
\begin{equation}
    \text{LeakTolMachine} = \text{LeakTolMachine}_0 + \frac{\text{LeakTolMachine}_{\text{REF}} \cdot \Delta \text{Force}_{\text{ReLu}}}{\Delta \text{Force}_{\text{REF}}}
\end{equation}
where we made explicit the dependence on $\Delta \text{Force}_{\text{ReLu}}$. Lastly, we have similar machine parameters $\text{LeakTolMachine}_0$ to model the minimum tolerance, plus $\text{LeakTolMachine}_{\text{REF}}$ and $\Delta \text{Force}_{\text{REF}}$ to model the dependence on $\Delta \text{Force}_{\text{ReLu}}$.


\section{Additional Results} \label{sec:appendix_additional_results}

In this section we provide a more exhaustive exposition of our results for performance and runtime.

\begin{table*}[h] 
\centering
\begin{tabular}{ p{1.5cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm} }
 %\hline
 Causal Model & ATE MSE & CATE MSE & JS-Div Tr. & MSE & MMD\\
 \hline
 CBN   & 0.659 (0.001)  & \textbf{0.036 (0.007)} & 0.136 (0.092) & 0.259 (0.186) &  0.702 (0.121) \\
 NCM   & 0.631 (0.015)   & 0.049 (0.028) & 0.233 (0.040) & 0.307 (0.033) &  \textbf{0.086 (0.000)} \\
 CAREFL & 0.652 (0.014) & 0.175 (0.106) & 0.512 (0.093) & \textbf{0.086 (0.073)} &  Nan \\
 CNF   &\textbf{0.631 (0.015})   & 0.065 (0.063) & 0.156 (0.047) & 0.299 (0.093) &  Nan \\
 VACA  & 0.648 (0.015)   & 0.230 (0.270) & \textbf{0.033 (0.009)} & 0.059 (0.017) &   0.128 (0.000) \\
 Linear r. & 1e8 (1e10)   & - & - & -  &  -\\
 Logistic r.& 0.698 (0.066)   & - & - & -  &  -\\
 %\hline
\end{tabular}
\caption{Comparison between models for the first treatment effect estimation task on CausalMan Small with $n = 50.000$ samples and ground truth ADMG. Instabilities during sampling prevented to evaluate MMD for CNF and CAREFL, as multiple datapoints diverged to $+\infty$ as a results of training instabilities.}
\label{table:effect_estimation_small_results_2}
\end{table*}


\begin{table*}[h] 
\centering
\begin{tabular}{ p{1.5cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm} }
 %\hline
 Causal Model & ATE MSE & CATE MSE & JS-Div Tr. & MSE & MMD\\
 \hline
 NCM   & 1.115(0.118) & 1.665(0.159) & 0.206(0.005) & \textbf{0.172(0.001)} &  0.259(0.018) \\
 CAREFL & \textbf{0.982(0.223)} & \textbf{1.539(0.635)} & 0.164(0.105) & 0.279(0.197) &  Nan \\
 CNF   & 1.218(0.012) & 1.784(0.082) & 0.297(0.003) & 0.535(0.007) &  Nan \\
 VACA  & 1.214(0.009)   & 1.890(0.163) &\textbf{ 0.163(0.003)} & 0.265(0.006) &   \textbf{0.244(0.009)} \\
 Linear r. & 4.748(0.142)   & - & - & -  &  -\\
 Logistic r.& 0.992(0.015)   & - & - & -  &  -\\
 %\hline
\end{tabular}
\caption{Comparison between models for the second treatment effect estimation task on CausalMan Small with $n = 50.000$ samples and ground truth ADMG. Linear regression in this case is clearly disadvantaged due to the presence of hidden confounders and nontrivial causal mechanisms.}\label{table:effect_estimation_small_results_3_annex}
\end{table*}


\begin{table*}[h] 
\centering
\begin{tabular}{ p{1.5cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm} }
 %\hline
 Causal Model & ATE MSE & CATE MSE & JS-Div Tr. & MSE &  MMD\\
 \hline
 NCM                 & 0.580 (0.043)                  & 0.067 (0.052)                 & 0.179 (0.016)                    & 0.257 (0.017)               & 0.380 (0.008)               \\ 
CAREFL              & \textbf{0.614 (0.009)}                  & \textbf{0.033 (0.015)}                 & \textbf{0.054 (0.038)}    & \textbf{0.098 (0.069)}               & \textbf{0.212 (0.023)}               \\
CNF                 & 0.618 (0.006)                  & 0.062 (0.036)                 & 0.127 (0.056)                  & 0.218 (0.096)               & 0.335 (nan)                 \\
Linear r.  & 2e9 (2e9) &- &  -   &     -  &   -  \\
Logistic r. & 0.649 (0.119)                  & -    &      -           &    -            &           -       \\

 %\hline
\end{tabular}
\caption{Comparison between models for the first treatment effect estimation task on CausalMan Medium with $n = 20.000$ samples and ground truth ADMG. }
\label{table:effect_estimation_medium_results_1}
\end{table*}



\begin{table*}[h] 
\centering
\begin{tabular}{ p{1.5cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm}|p{1.9cm} }
 %\hline
 Causal Model & ATE MSE & CATE MSE & JS-Div Tr. &  MSE &  MMD\\
 \hline
 NCM               & 1.629 (0.031)                 & 1.271 (0.031)                 & 0.589 (0.000)               & 1.000 (0.000)               & 0.389 (0.007)              \\ 
CAREFL              & 1.730 (0.068)                 & \textbf{1.199 (0.149)}                 & \textbf{0.351 (0.028)}               & \textbf{0.780 (0.034)}               & 0.185 (0.022)              \\
CNF                 & 1.822 (0.016)                 & 1.347 (0.052)                 & 0.357 (0.088)               & 0.783 (0.099)               & 0.212 (0.159)              \\
Linear r.  & \textbf{0.297 (0.019)}                 & -     &    -    &       -   &     -  \\
Logistic r. & 1.362 (0.016)                 & -   &   -   &   -    &   -   \\

 %\hline
\end{tabular}
\caption{Comparison between models for the first treatment effect estimation task on CausalMan Medium with $n = 20.000$ samples and ground truth ADMG. }
\label{table:effect_estimation_medium_results_2}
\end{table*}

\begin{table}[ht]
\centering
\begin{tabular}{ p{1.7cm}|p{2.1cm}|p{1.9cm}|p{1.9cm}|p{2.2cm}|p{2cm}} 
%\hline
Method & SHD & Prec. & Rec.& SID & p-SD\\
\hline
PC & 144.2 (0.837) & \textbf{0.123 (0.014)} & 0.056 (0.007) & 2208.2(40.935) & 0.099(0.043)\\ 
PC-Stable & 127.4 (1.949) & 0.072 (0.052) & 0.017 (0.012) & \textbf{2118.4(78.904)}& 0.017(0.004)\\
DAG-GNN & 147.8 (13.479) & 0.008 (0.017) & 0.002 (0.004) & 2275.8(32.568)& 0.038(0.017)\\
NOTEARS & 137.8 (1.922) & 0.018 (0.028) & 0.005 (0.007) & 2280.4(14.398)& 0.078(0.015)\\ 
GOLEM & 263.2 (19.791) & 0.043 (0.015) & \textbf{0.063 (0.024)}& 2371.8(40.258)& 0.427(0.003)\\ 
LiNGAM & 212.2 (31.196) & 0.043 (0.014)& 0.043 (0.022) & 2271(34.655)& 0.278(0.028)\\
GranDAG & \textbf{116 (2.646)} & 0.022 (0.049) & 0.002 (0.004) & 2240.2(24.468)& \textbf{0.001(0.001)}\\
Random DAG & 	208 (15.215) & 0.051 (0.017) & 0.050 (0.017) & 2260.8(75.652)& 0.413(0.026)\\
%\hline
\end{tabular}
\caption{Comparison for Causal Discovery on CausalMan Small (20.000 Samples).}
\label{table:causal_discovery_small}
\end{table}


\begin{table}[ht]
\centering
\begin{tabular}{ p{1.8cm}|p{2.1cm}|p{2cm}|p{2cm}|p{2.4cm}} 
%\hline
Method & SHD & Prec. & Rec. & p-SD \\
\hline
PC & 702.0 (3.24) & 0.015 (0.003) & 0.004 (0.001) & 0.061(0.05)\\ 
PC-Stable & 591.2 (0.83) & 0.020 (0.007) & 0.002 (0.001) & 0.002(0.001)\\
DAG-GNN & 	580.8 (22.28) & 0.003 (0.006) & 0.000 (0.001)& 0.001(0.001)
 \\
NOTEARS & 580.2 (1.78) & 0.024 (0.026) & 0.002 (0.002) & 0.004(0.001)\\ 
GOLEM &  845.0 (113.00) &\textbf{0.028 (0.005)} & 0.012 (0.004)& 0.283(0.131)\\ 
LiNGAM & 960.2 (100.18) & 0.027 (0.015) & 0.016 (0.007)& 0.287(0.015)\\
GranDAG & \textbf{543.4 (2.88)} & 0.017 (0.037) & 0.000 (0.001)& \textbf{2.32e-5(3.79e-5)}\\
Random DAG & 1,189.6 (9.83) & 0.020 (0.002) & \textbf{0.019 (0.002)}& 0.474(0.004)\\
%\hline
\end{tabular}
\caption{Comparison for Causal Discovery on CausalMan Medium (20.000 Samples).}
\label{table:causal_discovery_medium}
\end{table}


\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figures/shd_double_bar_dataset_size_50000_svg-tex.pdf} 
\caption{Difference in SHD between CausalMan Small and Medium.}
\label{fig:shd_bar_comparison}
\end{figure*}

\begin{figure*}[ht]
\centering
\subfloat[]{\includegraphics[width=0.490\textwidth]{figures/shd_lineplot_causalman_s_svg-tex.pdf} \label{fig:shd_vs_dataset_size_1}}
 \subfloat[]{\includegraphics[width=0.49\textwidth]{figures/shd_lineplot_causalman_m_svg-tex.pdf} \label{fig:shd_vs_dataset_size_2}}
\caption{SHD as a function of dataset size for CausalMan Small (\ref{fig:shd_vs_dataset_size_1}) and Medium (\ref{fig:shd_vs_dataset_size_2}). Using more data has a minimal impact and is mostly detrimental to the overall Structural Hamming Distance.}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figures/training_time_vs_size_dataset0_svg-tex.pdf} 
\caption{Average training time (seconds) vs. dataset size for CausalMan Small. }\label{fig:runtime_vs_dataset_size}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figures/total_memory_dataset_size_5000_dataset1_svg-tex.pdf} 
\caption{Bar plot showing the memory usage (RAM and GPU) for CausalMan Medium. Due to the significant time required for convergence, for NCMs it is essential to maintain a high batch size to ensure a reasonable training time. 
However, there are memory limitations when increasing the batch size, which impose a constraint on the maximum size of the dataset that NCMs can handle. 
This is a characteristic of the model that is related to the training procedure and architecture of each individual parameterized structural equation, as shown in~\cite{notallcausalinferencezece}. For example, a sum-product network is $\mathcal{O}(n)$ for a forward pass with $n$ the number of parameters, whereas MADE \cite{pmlr-v37-germain15} is approximately $\mathcal{O}(n^2)$.}
\label{fig:memory_usage_dataset1}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/interventional_distributions_carefl_svg-tex.pdf}
\includegraphics[width=0.8\textwidth]{figures/interventional_distributions_cnf_svg-tex.pdf}
\includegraphics[width=0.8\textwidth]{figures/interventional_distributions_cbn_svg-tex.pdf}
\caption{Estimated Interventional distributions with CAREFL, CNF and CBN for the additional interventional task (described in \ref{sec:appendix_additional_task}) on CausalMan Small (20.000 samples, seed 42). Causal models are not consistent when estimating interventional distributions, and cannot provide accurate reconstructions of both treated and control populations at the same time.}
\label{fig:interventional_distributions_1_task_1_causalman_small}
\end{figure*}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/interventional_distributions_ncm_svg-tex.pdf}
\includegraphics[width=0.8\textwidth]{figures/interventional_distributions_vaca_svg-tex.pdf}
\caption{Estimated Interventional distributions with NCM and VACA for the additional interventional task (described in \ref{sec:appendix_additional_task}) on CausalMan Small (20.000 samples, seed 42). Causal models are not consistent when estimating interventional distributions, and cannot provide accurate reconstructions of both treated and control populations at the same time.}
\label{fig:interventional_distributions_2_task_1_causalman_small}
\end{figure*}
\section{Experiment Setting}

\subsection{Metrics} \label{sec:appendix_metrics}

We can write the \textit{Structural Hamming Distance} SHD between a graph $\displaystyle \gG$ with adjacency matrix $A$ and the ground truth $\displaystyle \gG^\star$ with adjacency matrix $A^\star$ as:
\begin{equation}
    SHD(A, A^\star) = \sum_{i,j = 0}^n \mathbf{I}_{A_{ij} \neq A^\star_{ij}}
\end{equation}

Since discovering an individual edge can be thought as a binary classification task (edge/no-edge), it  is common to measure metrics such as precision and recall:

\begin{align}
    Pr &= \frac{tp}{tp + fp}, &
    Rec &= \frac{tp}{tp + fn}.
\end{align}

where \textit{tp} stands for true positives, \textit{fp} for false positives and \textit{fn} for false negatives.

\subsection{Causal Models} \label{sec:appendix_causal_models}

Here we provide a more detailed description of the tested causal models.

\begin{itemize}
    \item \textbf{Causal Bayesian Networks:} For Bayesian Networks (BN), edges do not have a causal semantic, and they are indeed only an observational Layer 1 model. However, it is possible to define a do-operator for Bayesian Networks, and obtain an interventional L2 model called \textit{Causal Bayesian Network} (CBN) \citep{bareinboimpch}.
    \item \textbf{Neural Causal Models:} Presented by \cite{xia2022causalneuralconnectionexpressivenesslearnability}, \textit{Neural Causal Models} (NCM) consist in a SCM where each structural equation is parameterized by a neural network. NCMs, as a special case of SCMs, are Layer 3 models capable of answering counterfactual queries, when identifiable \citep{xia2022neuralcausalmodelscounterfactual}. More info about our implementation in \ref{sec:appendix_code}.
    \item \textbf{CAREFL:} \textit{Causal AutoREgressive normalizing Flows} (CAREFL) \citep{careflkhemakhem2021}, uses Normalizing flows with affine layers and the Causal Ordering to answer queries up to the counterfactual level.
    \item \textbf{Causal Normalizing Flows:} \citep{causalnormjavaloy2023} provided a generalisation of CAREFL that uses the whole causal graph, includes non-additive noise models, and provides stronger identification guarantees, yielding Causal Normalizing Flows (CNF).
    \item \textbf{VACA:} Based on \textit{Variational Graph Autoencoders} \citep{kipf2016variationalgraphautoencoders}, \textit{Variational Causal Graph Autoencoder} (VACA) \citep{vacasanchezmartin2021} provides a counterfactual model based on \textit{Graph Neural Networks}.
\end{itemize}

\section{Implementation details}\label{sec:appendix_implementation}

In this supplementary section, we provide additional details on the architectures and implementations that have been tested. Furthermore, we list all the necessary modification that have been necessary to run the models with our datasets with hybrid data-types.

\subsection{Determinism}

Every experiment was run 5 different times with the random seeds 4, 6, 42, 66 and 90. 

\subsection{Hardware}
To perform a fair experimental evaluation of their tractability, each run was performed on a A100 GPU with 80 GB of GPU memory allocated, and an AMD EPYC 7643 CPU, with approximately 300 GB of RAM memory allocated.

Not all methods can leverage GPU parallelisation, therefore:

\begin{itemize}
    \item For Causal Inference, regression-based techniques and CBNs are run using only CPUs. 
    \item For Causal Discovery, PC algorithm, PC-Stable, NOTEARS, and LiNGAM are run using only CPUs.
\end{itemize}

\subsection{Data preprocessing:} For running the chosen models, data had to be embedded in a numerical format. Therefore, categoricals and discrete variables have been converted to an ordinal encoding (1, 2, 3, etc.). After obtaining a purely numerical dataset, every individual variable has been normalized via min-max normalization to be within the -1 and 1 range. 

Models like CBNs are designed to work exclusively on discrete domain and are not tailored for hybrid datatypes.
To overcome this limitation, CBNs have been fitted on a different version of the datasets where the continuous variables have been uniformly quantized.

For CNFs, CAREFL and VACA, data sampled from those models had to receive a binarization of the outcome variable and a binning of the conditioning variable. The binarization of the target variables has been done such that the target variable would be -1 if output was less than 0, and 1 if output higher than 0. For the conditioning variable, instead, the bins corresponded to the values of the evidence variable that were present in the training data, and the operation was necessary since the variable is discrete, otherwise it would have been impossible to evaluate empirically the conditional interventional distribution.

Among our tested models, only NCMs can adapt by design to hybrid datatypes, therefore they are the only ones that didn't necessitate any pre-processing for the training data apart from embedding of categoricals and data normalization. During estimation, interventional distributions were computed directly from the raw data that has been sampled from the estimated interventional distributions, without any post-processing.

\subsection{Implementation of Causal Models} \label{sec:appendix_code}

For convenience, all the tested models have been incorporated into a configurable framework, present in this paper's supplementary material.

\paragraph{Linear and Logistic Regression:} For linear and logistic regression estimates, we used the implementations provided in the \textit{DoWhy} python library. 

\paragraph{Causal Bayesian Networks (CNB):} For CBNs, we use the implementation contained in the \textit{pgmpy} python library. We use the K2 score function.


\paragraph{Neural Causal Models (NCM):} We used the original implementation contained in \href{https://github.com/CausalAILab/NeuralCausalModels}{Github Link}, and applied minor modifications in order to adapt the model to handle hybrid data-types. Modifications have been made because, for each individual parameterized structural equation, NCMs require architectures capable of estimating conditional distributions $p(v_i|\displaystyle \parents_\gG(v_i), u_i)$, as their log-likelihood is used for training \cite{xia2022causalneuralconnectionexpressivenesslearnability}.
In detail, binary variables have been modeled using MADE, as in the original paper. The MADE implementation we use is taken from: \href{https://github.com/karpathy/pytorch-made}{Github Link}.
For discrete/categorical variables, MADE is still used upon minor modifications to the architecture in order to adapt it to discrete and non-binary domains. Indeed, discrete variables have been one-hot-encoded, then fed to the neural network, which would output the logit values for each discrete value. The input size of MADE in this case would be, for a causal graph $\displaystyle \gG$, 
\begin{equation}
    D = |\displaystyle \parents_\gG(\ervx_i)| + |u_i| + |v_i|.
\end{equation}

where the last $|v_i|$ variables consist in the one-hot-encoding of the realisation of $v_i$.

Finally, structural Equations for Continuous variables are parameterised using Conditional Normalizing Flows
\cite{winkler2023learninglikelihoodsconditionalnormalizing}

\paragraph{Causal Normalizing Flows, CAREFL \& VACA:} We use the original author's implementation that can be found at \href{https://github.com/psanch21/causal-flows}{Link to GitHub Repository}.

\subsection{Hyper-parameters and Training Settings}

To ensure reproducibility of every experiment, we list here all the modification applied to every single causal model and causal discovery method.

\subsubsection{Settings for Causal Models}

We reflect the implementation used in the original papers for all Causal Models tested. However, given the large size of the dataset in terms of covariates and number of datapoints, we apply the following modifications, mostly to increase the number of parameters and capacity for each model. Modifications are as follows:
 
\begin{itemize}
    \item \textbf{CAREFL and Causal Normalizing Flows}: For both models, we did increase their size to have 4 layers with 64 hidden nodes each. Training optimization parameters are not changed with respect to the paper \citep{causalnormjavaloy2023}.
    \item \textbf{VACA}: 300 training epochs and batch-size of 1024. Both encoder and decoder use the \textit{Graph Isomorphism Network} (GIN) \citep{xu2018how} version of VACA. The encoder uses 2 hidden layers. The inner dimensionality is always 64. Even tough design conditions require to have a number of layers proportional to the diameter of the graph, scaling attempts to make the model bigger resulted in loss of convergence during training, and are a common limitation of Graph Neural Networks.
    \item \textbf{NCM}: For CausalMan Small, we use a batch-size of 1024 and 1.000 training epochs. For CausalMan Medium, we use a batch-size of 2048 and 600 training epochs. Training algorithm is still \textit{AdamW} \citep{loshchilov2019decoupledweightdecayregularization} with learning rate 0.004 and the \textit{Cosine Annealing} scheduler with warm restarts.
\end{itemize}
 
\subsection{Settings for Causal Discovery} \label{sec:appendix_code_discovery}
 
All the tested models used the implementations present in the gcastle python library. All used Causal Discovery models reflect their original papers cited in \ref{sec:causal_discovery} apart from the design choices listed below:
\begin{itemize}
    \item \textbf{PC and PC-Stable:} The $\chi^2$ Conditional Independence test was used.
    \item \textbf{NOTEARS:} The $L_2$ loss function was used.
    \item \textbf{GranDAG:} We used a batch-size of 1024 samples and 4 hidden layers, each one with 64 hidden nodes.
    \item \textbf{DAG-GNN:} We used a batch-size of 1024.
\end{itemize}

\section{Ground Truth Causal Graphs}

In this section we provide a visual depiction of all the ground truth causal graphs, both the complete graphs involved in the DGP and the partially observable ones obtained after a latent projection.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/causalman_small_fully_observable_ground_truth.png}\label{fig:ground_truth_dataset0_annex}
\caption{Complete Ground truth causal graph including hidden variables for CausalMan Small. Observable variables are colored in orange, and latent ones are colored in blue. 104 of 157 (66.2\%( of variables are latent.}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{figures/causalman_small_partially_observable_ground_truth.png}\label{fig:ground_truth_partially_observable_dataset0_annex}
\caption{Partially observable Ground truth causal graph for CausalMan Small.}
\end{figure}

\newpage
\begin{figure}[ht]
\centering
\includegraphics[angle=90, width=0.7\textwidth]{figures/causalman_medium_partially_observable_ground_truth.png}\label{fig:ground_truth_partially_observable_dataset1_annex}
\caption{Partially observable Ground truth causal graph for CausalMan Medium.}
\end{figure}

\newpage
\begin{figure}[ht]
\centering
\includegraphics[angle=90 , width=0.475\textwidth]{figures/causalman_medium_fully_observable_ground_truth.png}\label{fig:ground_truth_dataset1_annex}
\caption{Complete Ground truth causal graph including hidden variables for CausalMan Medium. Observable variables are colored in orange, and latent ones are colored in blue. 419 of 605 (69.2\%) of variables are latent.}
\end{figure}