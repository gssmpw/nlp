@inproceedings{10.5555/3495724.3497230,
author = {Ng, Ignavier and Ghassami, AmirEmad and Zhang, Kun},
title = {On the role of sparsity and DAG constraints for learning linear DAGs},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning graphical structures based on Directed Acyclic Graphs (DAGs) is a challenging problem, partly owing to the large search space of possible graphs. A recent line of work formulates the structure learning problem as a continuous constrained optimization task using the least squares objective and an algebraic characterization of DAGs. However, the formulation requires a hard DAG constraint and may lead to optimization difficulties. In this paper, we study the asymptotic role of the sparsity and DAG constraints for learning DAG models in the linear Gaussian and non-Gaussian cases, and investigate their usefulness in the finite sample regime. Based on the theoretical results, we formulate a likelihood-based score function, and show that one only has to apply soft sparsity and DAG constraints to learn a DAG equivalent to the ground truth DAG. This leads to an unconstrained optimization problem that is much easier to solve. Using gradient-based optimization and GPU acceleration, our procedure can easily handle thousands of nodes while retaining a high accuracy. Extensive experiments validate the effectiveness of our proposed method and show that the DAG-penalized likelihood objective is indeed favorable over the least squares one with the hard DAG constraint.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1506},
numpages = {12},
location = {Vancouver, BC, Canada},
}

@article{Cheng2022EvaluationMA,
  title={Evaluation Methods and Measures for Causal Learning Algorithms},
  author={Lu Cheng and Ruocheng Guo and Raha Moraffah and Paras Sheth and K. Sel√ßuk Candan and Huan Liu},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2022},
  volume={3},
  pages={924-943},
  url={https://api.semanticscholar.org/CorpusID:246634120}
}

@article{Chevalley2022CausalBenchAL,
  title={CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data},
  author={Mathieu Chevalley and Yusuf H. Roohani and Arash Mehrjou and Jure Leskovec and Patrick Schwab},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.17283},
  url={https://api.semanticscholar.org/CorpusID:253237133}
}

@inproceedings{DBLP:conf/iclr/LachapelleBDL20,
  author       = {S{\'{e}}bastien Lachapelle and
                  Philippe Brouillard and
                  Tristan Deleu and
                  Simon Lacoste{-}Julien},
  title        = {Gradient-Based Neural {DAG} Learning},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=rklbKA4YDS},
  timestamp    = {Thu, 07 May 2020 17:11:47 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LachapelleBDL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/Tu0BK019,
  author       = {Ruibo Tu and
                  Kun Zhang and
                  Bo C. Bertilson and
                  Hedvig Kjellstr{\"{o}}m and
                  Cheng Zhang},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Neuropathic Pain Diagnosis Simulator for Causal Discovery Algorithm
                  Evaluation},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {12773--12784},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/4fdaa19b1f22a4d926fce9bfc7c61fa5-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Tu0BK019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/csur/GuoCLH020,
  author       = {Ruocheng Guo and
                  Lu Cheng and
                  Jundong Li and
                  P. Richard Hahn and
                  Huan Liu},
  title        = {A Survey of Learning Causality with Data: Problems and Methods},
  journal      = {{ACM} Comput. Surv.},
  volume       = {53},
  number       = {4},
  pages        = {75:1--75:37},
  year         = {2021},
  url          = {https://doi.org/10.1145/3397269},
  doi          = {10.1145/3397269},
  timestamp    = {Mon, 26 Jun 2023 16:43:56 +0200},
  biburl       = {https://dblp.org/rec/journals/csur/GuoCLH020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/tkdd/YaoCLLGZ21,
  author       = {Liuyi Yao and
                  Zhixuan Chu and
                  Sheng Li and
                  Yaliang Li and
                  Jing Gao and
                  Aidong Zhang},
  title        = {A Survey on Causal Inference},
  journal      = {{ACM} Trans. Knowl. Discov. Data},
  volume       = {15},
  number       = {5},
  pages        = {74:1--74:46},
  year         = {2021},
  url          = {https://doi.org/10.1145/3444944},
  doi          = {10.1145/3444944},
  timestamp    = {Sun, 02 Oct 2022 15:51:31 +0200},
  biburl       = {https://dblp.org/rec/journals/tkdd/YaoCLLGZ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Dorie2017AutomatedVD,
  title={Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition},
  author={Vincent Dorie and Jennifer L. Hill and Uri Shalit and Marc A. Scott and Daniel Cervone},
  journal={Statistical Science},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:51992418}
}

@article{Erdos1984OnTE,
  title={On the evolution of random graphs},
  author={Paul L. Erdos and Alfr{\'e}d R{\'e}nyi},
  journal={Transactions of the American Mathematical Society},
  year={1984},
  volume={286},
  pages={257-257},
  url={https://api.semanticscholar.org/CorpusID:6829589}
}

@article{Hahn2019AtlanticCI,
  title={Atlantic Causal Inference Conference (ACIC) Data Analysis Challenge 2017},
  author={P. Richard Hahn and Vincent Dorie and Jared S. Murray},
  journal={arXiv: Methodology},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:53626612}
}

@book{Imbens_Rubin_2015, 
place={Cambridge}, 
title={Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction}, 
publisher={Cambridge University Press}, 
author={Imbens, Guido W. and Rubin, Donald B.}, year={2015}
}

@article{Kaiser2021UnsuitabilityON,
  title={Unsuitability of NOTEARS for Causal Graph Discovery when Dealing with Dimensional Quantities},
  author={Marcus Kaiser and Maksim Sipos},
  journal={Neural Processing Letters},
  year={2021},
  volume={54},
  pages={1587 - 1595},
  url={https://api.semanticscholar.org/CorpusID:233209763}
}

@inproceedings{NEURIPS2018notears,
 author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep K and Xing, Eric P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {DAGs with NO TEARS: Continuous Optimization for Structure Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/e347c51419ffb23ca3fd5050202f9c3d-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{Reisach2021BewareOT,
  title={Beware of the Simulated DAG! Causal Discovery Benchmarks May Be Easy to Game},
  author={Alexander G. Reisach and Christof Seiler and Sebastian Weichwald},
  booktitle={Neural Information Processing Systems},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:239998404}
}

@article{Shimoni2018BenchmarkingFF,
  title={Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis},
  author={Yishai Shimoni and Chen Yanover and Ehud Karavani and Yaara Goldschmidt},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.05046},
  url={https://api.semanticscholar.org/CorpusID:3671244}
}

@InProceedings{alarmdataset,
author="Beinlich, Ingo A.
and Suermondt, H. J.
and Chavez, R. Martin
and Cooper, Gregory F.",
editor="Hunter, Jim
and Cookson, John
and Wyatt, Jeremy",
title="The ALARM Monitoring System: A Case Study with two Probabilistic Inference Techniques for Belief Networks",
booktitle="AIME 89",
year="1989",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="247--256",
abstract="ALARM (A Logical Alarm Reduction Mechanism) is a diagnostic application used to explore probabilistic reasoning techniques in belief networks. ALARM implements an alarm message system for patient monitoring; it calculates probabilities for a differential diagnosis based on available evidence. The medical knowledge is encoded in a graphical structure connecting 8 diagnoses, 16 findings and 13 intermediate variables. Two algorithms were applied to this belief network: (1) a message-passing algorithm by Pearl for probability updating in multiply connected networks using the method of conditioning; and (2) the Lauritzen-Spiegelhalter algorithm for local probability computations on graphical structures. The characteristics of both algorithms are analyzed and their specific applications and time complexities are shown.",
isbn="978-3-642-93437-7"
}

@misc{amin2024scalableflexiblecausaldiscovery,
      title={Scalable and Flexible Causal Discovery with an Efficient Test for Adjacency}, 
      author={Alan Nawzad Amin and Andrew Gordon Wilson},
      year={2024},
      eprint={2406.09177},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.09177}, 
}

@article{asiadataset,
author = {Lauritzen, S. L. and Spiegelhalter, D. J.},
title = {Local Computations with Probabilities on Graphical Structures and Their Application to Expert Systems},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {50},
number = {2},
pages = {157-194},
keywords = {artificial intelligence, bayesian methods, causal markov random field, decomposable graphs, expert systems, local potentials, markov random field, maximum cardinality search, probabilistic reasoning, triangulated graphs},
doi = {https://doi.org/10.1111/j.2517-6161.1988.tb01721.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1988.tb01721.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1988.tb01721.x},
abstract = {SUMMARY A causal network is used in a number of areas as a depiction of patterns of ‚Äòinfluence‚Äô among sets of variables. In expert systems it is common to perform ‚Äòinference‚Äô by means of local computations on such large but sparse networks. In general, non-probabilistic methods are used to handle uncertainty when propagating the effects of evidence, and it has appeared that exact probabilistic methods are not computationally feasible. Motivated by an application in electromyography, we counter this claim by exploiting a range of local representations for the joint probability distribution, combined with topological changes to the original network termed ‚Äòmarrying‚Äô and ‚Äòfilling-in‚Äò. The resulting structure allows efficient algorithms for transfer between representations, providing rapid absorption and propagation of evidence. The scheme is first illustrated on a small, fictitious but challenging example, and the underlying theory and computational aspects are then discussed.},
year = {1988}
}

@article{barabasi1999emergence,
  title={Emergence of scaling in random networks},
  author={Barab{\'a}si, Albert-L{\'a}szl{\'o} and Albert, R{\'e}ka},
  journal={science},
  volume={286},
  number={5439},
  pages={509--512},
  year={1999},
  publisher={American Association for the Advancement of Science}
}

@InProceedings{diabetesdataset,
author="Andreassen, Steen
and Hovorka, Roman
and Benn, Jonathan
and Olesen, Kristian G.
and Carson, Ewart R.",
editor="Stefanelli, Mario
and Hasman, Arie
and Fieschi, Marius
and Talmon, Jan",
title="A Model-Based Approach to Insulin Adjustment",
booktitle="AIME 91",
year="1991",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="239--248",
abstract="A differential equation model of carbohydrate metabolism was implemented in the form of a causal probabilistic network. This permitted explicit represen-tations of the uncertainties associated with model based predictions of 24-hour blood glucose profiles. In addition, the implementation gave automatic learning and adjustment of model parameters based on measured blood glucose profiles. Insulin therapy was adjusted using a decision theoretical approach. Losses were assigned to blood glucose values that deviated from normal, and the insulin therapy was adjusted to minimize the expected total loss. The system was tested retrospectively on cases from 12 insulin dependent patients and seemed to compare favourably with clinical practice.",
isbn="978-3-642-48650-0"
}

@misc{gamella2024causalchambers,
      title={The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology}, 
      author={Juan L. Gamella and Jonas Peters and Peter B√ºhlmann},
      year={2024},
      eprint={2404.11341},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2404.11341}, 
}

@inproceedings{g√∂bler2024causalassembly,
    title={$\texttt{causalAssembly}$: Generating Realistic Production Data for Benchmarking Causal Discovery}, 
    author={Konstantin G√∂bler and Tobias Windisch and Mathias Drton and Tim Pychynski and Steffen Sonntag and Martin Roth},
    year={2024},
    booktitle = {Proceedings of Machine Learning Research}
}

@article{loh_mse_unsuitable,
author = {Loh, Po-Ling and B\"{u}hlmann, Peter},
title = {High-dimensional learning of linear causal networks via inverse covariance estimation},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared l2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for high-dimensional consistency of our two-part algorithm, defined in terms of a "gap" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {3065‚Äì3105},
numpages = {41},
keywords = {causal inference, dynamic programming, identifiability, inverse covariance matrix estimation, linear structural equation models}
}

@misc{lopez2022factorgraphs,
      title={Large-Scale Differentiable Causal Discovery of Factor Graphs}, 
      author={Romain Lopez and Jan-Christian H√ºtter and Jonathan K. Pritchard and Aviv Regev},
      year={2022},
      eprint={2206.07824},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2206.07824}, 
}

@misc{mamaghan2024evaluationbayesian,
      title={Challenges and Considerations in the Evaluation of Bayesian Causal Discovery}, 
      author={Amir Mohammad Karimi Mamaghan and Panagiotis Tigas and Karl Henrik Johansson and Yarin Gal and Yashas Annadani and Stefan Bauer},
      year={2024},
      eprint={2406.03209},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03209}, 
}

@misc{mhalla2020causalmechanismextremeriver,
      title={Causal mechanism of extreme river discharges in the upper Danube basin network}, 
      author={Linda Mhalla and Val√©rie Chavez-Demoulin and Debbie J. Dupuis},
      year={2020},
      eprint={1907.03555},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/1907.03555}, 
}

@article{notallcausalinferencezece,
title={Not All Causal Inference is the Same},
author={Matej Ze{\v{c}}evi{\'c} and Devendra Singh Dhami and Kristian Kersting},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=ySWQ6eXAKp},
note={}
}

@InProceedings{pmlr-v236-mogensen24a,
  title = 	 {Causal discovery in a complex industrial system: A time series benchmark},
  author =       {Mogensen, S{\o}ren Wengel and Rathsman, Karin and Nilsson, Per},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {1218--1236},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/mogensen24a/mogensen24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/mogensen24a.html},
  abstract = 	 {Causal discovery outputs a causal structure, represented by a graph, from observed data. For time series data, there is a variety of methods, however, it is difficult to evaluate these on real data as realistic use cases very rarely come with a known causal graph to which output can be compared. In this paper, we present a dataset from an industrial subsystem at the European Spallation Source along with its causal graph which has been constructed from expert knowledge. This provides a testbed for causal discovery from time series observations of complex systems, and we believe this can help inform the development of causal discovery methodology.}
}

@InProceedings{pmlr-v246-busch24a,
  title = 	 {PsiNet: Efficient Causal Modeling at Scale},
  author =       {Busch, Florian Peter and Willig, Moritz and Seng, Jonas and Kersting, Kristian and Dhami, Devendra Singh},
  booktitle = 	 {Proceedings of The 12th International Conference on Probabilistic Graphical Models},
  pages = 	 {452--469},
  year = 	 {2024},
  editor = 	 {Kwisthout, Johan and Renooij, Silja},
  volume = 	 {246},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {11--13 Sep},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v246/main/assets/busch24a/busch24a.pdf},
  url = 	 {https://proceedings.mlr.press/v246/busch24a.html},
  abstract = 	 {Being a ubiquitous aspect of human cognition, causality has made its way into modern-day machine-learning research. Despite its importance in real-world applications, contemporary research still struggles with high-dimensional causal problems. Leveraging the efficiency of probabilistic circuits, which offer tractable computation of marginal probabilities, we introduce $\Psi$net, a probabilistic model designed for large-scale causal inference. $\Psi$net is a type of sum-product network where layering and the einsum operation allow for efficient parallelization. By incorporating interventional data into the learning process, the model can learn the effects of interventions and make predictions based on the specific interventional setting. Overall, $\Psi$net is a causal probabilistic circuit that efficiently answers causal queries in large-scale problems. We present evaluations conducted on both synthetic data and a substantial real-world dataset, demonstrating $\Psi$net‚Äôs ability to capture causal relationships in high-dimensional settings.}
}

@misc{poon2012sumproductnetworksnewdeep,
      title={Sum-Product Networks: A New Deep Architecture}, 
      author={Hoifung Poon and Pedro Domingos},
      year={2012},
      eprint={1202.3732},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1202.3732}, 
}

@misc{structuralcausalcircuits,
author = {Busch, Florian and Willig, Moritz and Zeƒçeviƒá, Matej and Kersting, Kristian and Dhami, Devendra},
year = {2023},
month = {01},
pages = {},
title = {Structural Causal Circuits: Probabilistic Circuits Climbing All Rungs of Pearl's Ladder of Causation},
doi = {10.2139/ssrn.4578551}
}

@misc{wu2024sea,
      title={Sample, estimate, aggregate: A recipe for causal discovery foundation models}, 
      author={Menghua Wu and Yujia Bao and Regina Barzilay and Tommi Jaakkola},
      year={2024},
      eprint={2402.01929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.01929}, 
}

