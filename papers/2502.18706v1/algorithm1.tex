

\begin{algorithm}[H]
\caption{Differential Private Federated Averaging (DP-FedAvg)~\citep{mcmahan2017learning}}
\textbf{Inputs:} No. clients $N$, No. global rounds $T$, No. local iterations $L$, noise multiplier $\noisem$, clip norm $c$, sampling rate $q$, loss functions $\{\loss_n\}_{n\in [N]}$, local datasets $\{\train_n\}_{n\in [N]}$, learning rate $\lr$, batch size $\bs$ 
%\State \textbf{Results:} Final global model $\model^T$
\label{alg:dpfl}
\begin{algorithmic}[1]
\State \textbf{Initialize} global model $\model^0$
\For{each global round $t \in [T]$} 
\State $\gC^t \gets$ Sample clients with probability $q$.
\For{each client $n\in \gC^t$ in parallel}
\State $\tilde{\Delta}\model_n^t$ =  
\texttt{ClientUpdate}$\left(t, n,\model^{t-1}\right)$.
\EndFor
\State Aggregate $\tilde{\Delta}\model^t = \sum_{n\in \gC^t} \tilde{\Delta}\model_n^t$
\State Add noise $\tilde{\Delta}\model^t \gets \tilde{\Delta}\model^t + \gN(0,c^2\noisem^2\sI)$
\State Update $\model^{t} = \model^{t-1} + \frac{\tilde{\Delta}\model^t}{qN}$
\EndFor
\Statex
\end{algorithmic}
\textbf{Def} \texttt{ClientUpdate}$\left(t, n,\model^{t}, c_n\right)$
\begin{algorithmic}[1]
\State \textbf{Initialize} local model $\model_n^{t,0}=\model^t$
\For{local iteration $l \in [l]$}
\State $\{\batch_i\}_{i=1}^{|\train_n|/B} \gets$ Split $\train_n$ to size $B$ batches
\For{each batch $\batch_i$}
\State $\model_n^{t,l} = \model_n^{t,l-1}-\frac{\lr}{\bs}\sum_{(\rvx,y)\in \batch_i} \nabla\loss\left(\model_n^{t,l-1};(\rvx,y)\right)$
\EndFor
\EndFor
\State Compute $\Delta\model_n^t = \model_n^{t,L} - \model_n^{t,0}$
\State Clip $\tilde{\Delta}\model_n^{t} = \Delta\model_n^t\min\left(1,\frac{c}{\left\|\Delta\model_n^{t}\right\|_2}\right)$  
\State Return $\tilde{\Delta}\model_n^{t}$ 
\end{algorithmic}
\end{algorithm}
