\section{Background and Related Work}
\label{sec:back}

\paragraph{Federated Learning.}
We consider a typical FL system with $N$ clients and a central server. Each client $n \in [N]$ has its own data distribution $P_n$ on $\gX\times\gY$, where $\gX \subseteq \sR^u$ denotes the feature space and $\gY\subseteq \sR$ denotes the label space. Let $\loss_n:\sR^v \times \gX\times\gY \rightarrow \sR$ denote client $n$'s loss function which maps a model parameter $\model\in \sR^v$ and a data sample $(\rvx,y)\in \gX\times\gY$ to a cost. 
Each client $n$ is assumed to have access to a dataset $\train_n$ which consists of $\left|\train_n\right|$ data points sampled from $P_n$. Defining $\bar{\loss}_n(\model):= \frac{1}{\left| \train_n\right|}\sum_{(\rvx,y)\in \train_n} \loss_n\left(\model;(\rvx,y) \right)$ and $\bar{\loss}(\model) := \frac{1}{N} \sum_{n=1}^N \bar{\loss}_n(\model)$, the optimization problem in FL is $\min_{\model} \bar{\loss}(\model)$. The Federated Averaging (FedAvg)____ algorithm solves this problem by having clients run local stochastic gradient descent (SGD) and send updates to the server, which averages them to update the global model. This cycle repeats until convergence or for a specified number of communication rounds. %\tcb
{As the baseline for our proposed approach (detailed in Sec.\ref{sec:propose}), we consider FedAvg, presented as Alg.~\ref{alg:fedavg} in App.\ref{app:benchmark}. This algorithm, unconstrained by privacy limitations, represents the ideal utility case.}


\paragraph{Differential Privacy.}
In ML, the mathematical framework of $(\eps,\delta)$-DP____, ensures that two models trained on neighboring datasets, i.e., datasets that differ in one data point, differ only slightly in their outputs. %\tcb
{This can be formalized as Def.~\ref{app:epsdeltaDP} in App.~\ref{app:prior:rdp}.} 
In $(\eps,\delta)$-DP, a smaller privacy budget $\eps \in \sR_{+}$ enforces a stronger privacy guarantee. The $\delta\in [0,1]$ quantifies the probability of violating the privacy guarantee and thereby has usually a small value. 
We discuss $(\alpha, \epsilon)$-RÃ©nyi-DP____ as an alternative to relax DP guarantees and get smoother composition in App.~\ref{app:prior:rdp}. In this paper, we adopt RDP, while as it is shown by____, it can be converted to DP when needed (cf. Lem.~\ref{app:prior:rdp:lem} in App.~\ref{app:prior:rdp}). 


\paragraph{DP-FL.}
We use the notion of \textit{client-level} DP-FL which protects the entire client's dataset____. To implement client-level DP in FL, 
we can rely on the DP-FedAvg____ algorithm that first clips clients' updates according to a \textit{sensitivity} $c$ and then adds Gaussian noise according to $\noise{0}{c^2\noisem^2\sI}$. DP-FedAvg also implements privacy amplification by sub-sampling____, where clients are sampled independently at random with probability $q$. As shown by ____, the Sampled Gaussian Mechanism (SGM) %\tcb
{tightens the RDP $\epsilon$ by a quadratic scaling factor $q^2$ (cf. Lem.~\ref{lem:rdpsGM} in App.~\ref{app:prior:rdp}).} We also use the notion of \textit{distributed DP} (DDP)____ which combines the advantages of centralized____ and local DP____. In DDP, clients clip their updates and locally add a small amount of noise, distributed according to $\noise{0}{{c^2\noisem^2}/{N}\sI}$____. By adding local noise, clients' privacy is partially protected against the server, and sufficient noise, $\noise{0}{c^2\noisem^2\sI}$, is guaranteed when the server aggregates all $N$ noisy updates. We consider the DP-FedAvg algorithm that is implemented with client-level and DDP (presented as Alg.~\ref{alg:dpfl} in App.\ref{app:benchmark}) as a baseline for our approach, detailed in Sec.\ref{sec:propose}.


\paragraph{Personalized DP.}
DP-FedAvg and most of its variants apply worst-case privacy guarantees to ensure privacy for the most constrained clients. This leads to over-perturbation and reduced utility for clients with more relaxed privacy constraints. Some recent literature addresses this by exploring personalized, or individualized, DP____. For example,____ introduce individualized DP (IDP) and apply it to the DP-SGD algorithm, a collaboration-free variant of DP-FedAvg. The integration of IDP into a client-level DP-FL framework is natural. IDP enables individualized privacy budgets across clients and fine-tunes client-specific privacy parameters, through the use of different clip norms and/or sampling rates. We name this integrated algorithm IDP-FedAvg and formally present it as Alg.~\ref{alg:idpfl} in App.\ref{app:benchmark}. We note that IDP-FedAvg and our proposed approach, detailed in Sec.\ref{sec:propose}, complement each other. In this paper, we consider IDP-FedAvg as another baseline for our approach.


\paragraph{Adaptive DP.} 
Another approach to improve the utility performance of DP learning is 
parameter grouping____ which clusters the ML parameters with similar clipping norms and applies a non-uniform clipping across clusters. Similar to the IDP approach, our approach and parameter grouping are complementary. However, integrating our approach into parameter grouping optimally, given the increased parameter choices across clusters, requires a study on cluster-based parameter selection which is beyond the scope of this paper. Another approach used for utility improvement is adaptive clipping____ which aims to optimize the clipping norms during training and thereby reduce the noise effect. %\tcb
{Some of these papers either rely on a strong assumption of accessing public data____, or the strong assumption of minimal privacy loss occurs during parameter optimization____. Our proposed approach is an alternative that selects privacy parameters non-uniformly over time. Compared to the above adaptive clipping methods, our approach eliminates the need for public data, and as parameter selection is done prior to training, ensures zero privacy loss. Another adaptive clipping method that is not limited to prior assumptions and is more relevant to ours is the one proposed by____, and presented as Alg.~\ref{alg:quantile} and detailed in App.\ref{app:benchmark}. While targeting the same goal of improving the privacy-utility tradeoff as ours, the method____ maintains fixed privacy spending over time. Note that adaptive clipping is orthogonal to our approach and could be combined with ours for potential performance gain. However, such integration requires careful privacy analysis, which is beyond the scope of this paper and left for future work. In App.~\ref{app:extendedresults}, we benchmark our approach against the method____, showing that our approach achieves a better privacy-utility tradeoff.}