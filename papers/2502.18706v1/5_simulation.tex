\section{Experiments}\label{sec:simulation}
\paragraph{Datasets.}
To empirically evaluate the performance of our framework against baselines, we consider widely used datasets. For the Fashion MNIST (FMNIST) and MNIST, we use a convolutional neural network (CNN) architecture from~\citep{mcmahan2017communication}. For the Adult Income dataset, we use multi-layer perception from~\citep{9378043}. %\tcb
{For the CIFAR10 dataset, we use a CNN architecture from ~\cite{he2016deep}}. We partition datasets across 100 clients in a non-IID manner using the Dirichlet distribution with a default parameter 0.1~\citep{zhang2023fedala}. 

\paragraph{Privacy Settings.} To reduce the number of choices for clients' privacy budgets, and motivated by society wherein individuals often share similar privacy preferences~\citep{alaggan2015heterogeneous, boenisch2024have}, in our experiments we divide clients into three groups. The groups are respectively assigned budgets of $\eps_{\text{group}, 1}$, $\eps_{\text{group}, 2}$, or $\eps_{\text{group}, 3}$. We randomly allocate $34\%$ of clients to belong to Group 1, $43\%$ to Group 2, and $23\%$ to Group 3. In other words, Client $n$ in Group $m$, shares $\eps_n=\eps_{\text{group},m}$. To account for privacy consumption, we use the Opacus library~\citep{opacus}. We consider DP parameter $\delta=10^{-5}$ and choose an extended version of the default RDP parameter $\alpha$ from the RDPAccountant function in Opacus. We apply per-layer clipping~\citep{mcmahan2017learning} to restrict the influence of individual layers by constraining their norms. 


\paragraph{Baselines.} As discussed in Sec.~\ref{sec:back}, we consider several baselines. %\tcb
{The first is the non-private FedAvg~\citep{mcmahan2017communication} which represents our upper bound on utility without any privacy constraints.} The second is DP-FedAvg~\citep{mcmahan2017learning} where we use a uniform privacy budget, chosen according to the smallest epsilon value of any of the clients. This ensures that no client's privacy budget is exceeded. The third is IDP-FedAvg which is the IDP-integration ~\citep{boenisch2024have} of DP-FedAvg. %\tcb
{The fourth is adaptive clipping~\citep{andrew2021differentially}}. To have a fair comparison with the baselines, we evaluate two variants of our framework. The first variant assumes every client's budget is constrained by the smallest value in the group budget tuple $(\eps_{\text{group},1}, \eps_{\text{group},2}, \eps_{\text{group},3})$. The second variant incorporates different privacy groups. Further details on the choice of hyperparameters in our experiments are given in Table \ref{tab:hyperparamstable} in  \Cref{app:extendedExpSetup}. 

\begin{figure}[h]
    \centering
    \subfloat[Test Accuracy FMNIST]{
    \includegraphics[width=0.38\textwidth]{images/pdfs/p_4_fmnist_global_test_acc_epsilons_10_20_30_clients_100_sigma_0.30_clip_250.00_srate_0.90_runs_2_rounds_25_epochs_30final_aid.txt.pdf}}
    \subfloat[Test Accuracy MNIST]{
    \includegraphics[width=0.38\textwidth]{images/pdfs/p_4_mnist_global_test_acc_epsilons_10_15_20_clients_100_sigma_0.30_clip_250.00_srate_0.90_runs_2_rounds_25_epochs_30final_aid.txt.pdf}}
    \caption{\textbf{Our framework improves accuracy in later rounds compared to the baseline.} We plot the global test accuracy vs. rounds for (a) the FMNIST dataset, and (b) the MNIST dataset. In (a), $(\eps_{\text{group},1}, \eps_{\text{group},2}, \eps_{\text{group},3})=(10,20,30)$, and in (b) it equals $(10,15,20)$.} 
    \label{fig:results}
    \centering \vspace{-1ex}
\end{figure}


\begin{table}[h!]
\centering
\caption{\textbf{Global test accuracy} for 
{FedAvg without DP constraints,} DP-FedAvg with $\eps_n=10$, IDP-FedAvg with non-uniform privacy budgets, our framework with $\eps_n=10$, and our framework with non-uniform budgets. For the FMNIST and Adult Income datasets, the non-uniform privacy budgets $\eps_n$ are $(\eps_{\text{group},1},\eps_{\text{group},2},\eps_{\text{group},3})=(10,20,30)$, and for MNIST, they are $(10,15,20)$.}
\small
\setlength{\tabcolsep}{10pt} 
\renewcommand{\arraystretch}{1.2} 
\begin{tabular}{cccccc}
\toprule
\textbf{DATASET} & \makecell[tl]{
{\small \textbf{FedAvg}} \\ {\small (non-DP)}}  &  \makecell[tl]{{\small\textbf{DP-FedAvg}}\\ {\small $\eps_n=10$ }}  & \makecell[tl]{{\small \textbf{IDP-FedAvg}} \\ {\small Non-uniform}}& \makecell[tl]{{\small \textbf{Ours}} \\ {\small $\eps_n=10$ } } & \makecell[tl]{{\small \textbf{Ours}}\\ {\small {Non-uniform}}}  \\ 
\midrule  
FMNIST& 72.95 & 64.8 & 65.45 & \textbf{67.90} & \textbf{70.57}  \\  
\midrule 
MNIST & 90.23 &  76.79 & 76.94  & \textbf{80.2} & \textbf{83.83} \\
\midrule 
Adult Income & 78.93 &  60.12 &  70.93  & \textbf{72.14}  & \textbf{77.53}\\ 
\midrule
\end{tabular}
\label{tab:cmptable}
\centering \vspace{-1ex}
\end{table}

\begin{figure*}[!htbp]
    \centering
    \vspace{5pt}  
        \centering
        \includegraphics[width=0.4\textwidth]{images/pdfs/privacy_curves.pdf}
    \caption{\textbf{While both adhere to privacy budgets, our framework follows spend-as-you-go, whereas IDP-FedAvg uses uniform privacy spending.} The blue solid curves correspond to clients' privacy spending in our framework, while the red dashed curves show IDP-FedAvg. The curves of clients with budgets of $30$, $20$, and $10$ are marked with rectangles, circles, and squares, respectively.  
    }    
    \label{fig:privacyparams}
    \centering \vspace{-2ex}
\end{figure*}

\paragraph{Experimental Results.} As shown in Table~\ref{tab:cmptable} and Fig.~\ref{fig:results}, our framework yields improvements in the resulting global model's accuracy by spending privacy budget non-uniformly across training rounds. Comparing Columns 4 and 6 of Table~\ref{tab:cmptable}, our framework with non-uniform privacy budgets improves global test accuracy over IDP-FedAve by $7.8\%$, $8.9\%$, and $9.3\%$ on FMNIST, MNIST, and Adult Income. In the case of using a uniform budget of $\eps_n=10$ across all clients $n$, our framework achieves respective improvements of 4.7\%, 4.4\%, and 19.9\% compared to DP-FedAvg, as shown in Columns 3 and 5. {We also observe that, our time-adaptive DP-FL scheme comes closest to the ideal-case performance of FedAvg (column 2) without privacy constraints.} Figure \ref{fig:results} plots global test accuracy vs. global rounds for our framework with non-uniform privacy budgets and IDP-FedAvg, on the FMNIST and MNIST datasets. This figure shows that while our framework conserves privacy in early rounds, it allocates more budget in later rounds, eventually catching up to and surpassing IDP-FedAvg by about $8\%$ on FMNIST and $6\%$ on MNIST in the final round. 

In Fig.~\ref{fig:privacyparams} we present the privacy budget spent by clients from different budget groups $(10,20,30)$ across rounds. This figure shows that,  while IDP-FedAvg enforces uniform privacy consumption over time, in our framework, clients follow spend-as-you-go, saving budgets in the first half of training, and spending more in later rounds. Our experimental results demonstrate that our time-adaptive approach boosts the utility of the trained model while adhering to privacy constraints. 

{In \Cref{app:extendedresults}, we present extended experimental results, including benchmarks on the CIFAR10 dataset (Table~\ref{tab:cifar10}), comparisons with adaptive clipping (Table~\ref{tab:strictprivacybudgetstable2}), and evaluations across privacy-related hyperparameters (Tables~\ref{tab:strictprivacybudgetstable},~\ref{tab:saving_sampling_ratestable}, and ~\ref{tab:privacy_spending_roundtable}, and Figure~\ref{fig:lowprivacybudgets}), as well as other parameters (Tables~\ref{tab:clientstable_rounds} and~\ref{tab:clientstable} and Figure~\ref{fig:morerounds}).} 


