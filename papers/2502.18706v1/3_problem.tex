\section{Proposed Framework}\label{sec:propose}

%\tcb
{We introduce a novel time-adaptive DP-FL framework to solve the FL optimization problem with high utility and under privacy constraints that are not meant to be spent uniformly over time. We first discuss our threat space and privacy-related hyperparameters (summarized in Table~\ref{sec3:notations}).} 

\paragraph{Threat Space.} 
We assume that each client aims to prevent data leakage to any other client who may be honest but curious (HbC). Specifically, HbC clients follow the FL protocol honesty but may attempt to infer sensitive information of the victim client from the shared model updates. We assume the server is trusted but rather than offering zero protection, we make clients perturb their model updates before sharing with the server to preserve privacy, though at a lower level. This is because, after the server aggregates the perturbed model updates, the total perturbation increases, providing stronger privacy protection against other clients than the server. For enhanced protection against the server, one can use secure aggregation~\citep{bonawitz2016practical} which ensures that the server only learns an aggregated function (typically the sum) of the clients' local updates, without learning individual updates. However, the design or implementation of secure aggregation schemes is beyond the scope of this paper, and we mainly focus on preserving privacy against HbC clients.

\paragraph{Privacy Hyperparameters.}
%\tcb
{In \algasgo method, which we will detail as part of our framework design in Sec.~\ref{sec:proposed:alg:privacy}, each client saves a specific fraction of their budget in certain rounds, then incrementally spends the saved portion over time. We realize savings by using client-specific sampling rates, denoted as $q_n\in [0,1]$ for client $n$, during ``saving'' rounds and by using a uniform, higher, sampling rate, denoted as $q \in [0,1]$, during ``spending'' rounds. If $q_n<q$, client $n$ is less likely to be sampled during the saving rounds. According to privacy amplification by sub-sampling~\citep {beimel2014bounds}, and as detailed in Sec.~\ref{sec:theory:privacy}, a fraction of the clients' privacy budget will thereby be saved for later rounds. Another hyperparameter is each client's designated round for transitioning from the saving to the non-saving (spend) mode. For each $n\in [N]$, we use $T_n$ to denote the first round in which client $n$ is in the spending mode. Intuitively, if $T_n$ is aligned with the client $n$ transitioning from the coarse-grained training of early rounds to the fine-grained feature training of later rounds, the client's saved budget during early rounds enables the client to spend more in later rounds when additional accuracy is beneficial in learning more fine-grained features. By setting $q_n=q$ or $T_n=1$ for every $n\in [N]$, each client's privacy spending becomes uniform over time, resembling the traditional non-time-adaptive spending approaches. We show the hyperparameters $q_n$ and $T_n$, which are specific to our framework, in the first two rows of Table~\ref{sec3:notations}. Other hyperparameters, which are common in much of the DP-FL literature, are summarized in rows 3 to 10 of Table~\ref{sec3:notations}. The privacy-related parameters, consistent across clients and fixed over time, include the global clip norm $c$, the sampling rate $q$, the DP parameter $\delta$, and the RDP-related order $\alpha$. The configuration of these hyperparameters to avoid additional privacy loss is covered partly in simulations (Sec.~\ref{sec:simulation} and App.~\ref{app:extendedresults}) and partly in theory (Sec.~\ref{sec:theory}). Later, in Sec.~\ref{ch6:future}, we discuss potential future directions for broader hyperparameter tuning. }


\subsection{The \algasgo Method}\label{sec:proposed:alg:privacy}
%\tcb
{We consider an FL setting where every client $n\in [N]$ is given a privacy budget $\eps_n$ derived from their individual privacy preferences. We assume all clients exhaust their privacy budget after $T$ global rounds. We propose the \algasgo method, which is executed before training begins, and obtains the following privacy parameters used during execution: 
the local noise multipliers $\noisem_n^t$, the sampling rates $q_n^t$, the privacy budget remaining $\eps_n^t$ (with an initial value of $\eps_n^0=\eps_n$), and the local clip norms $c_n^t$. Each $q_n^t$ is chosen as either $q$ or $q_n$, depending on whether client $n$ is in spending or saving mode. From now on, we denote $q_n^t$ as the sampling rate, and to distinguish between $q$ and $q_n$, we respectively refer to them as the {\em spending-based} sampling rate and the {\em saving-based} sampling rate. To denote whether clients are in saving or spending mode at each round, we use $M_n^t$, a binary variable that is set to 0 if round $t$ is a saving round for client $n$, and 1 otherwise. I.e., $M_n^t = 0$ if $t< T_n$ and $M_n^t = 1$ if $t\geq T_n$. }
%\tcb
{Algorithm~\ref{alg:tidpfl:privacy} presents the pseudocode for the \algasgo method.  Regardless of the saving or sampling mode, in each round, we first find the value $\noisem_n^t$ required to obtain the remaining privacy budget $\eps_n^t$ using the \texttt{GetNoise}\footnote{To introduce our \algasgo method, we use some functions as implemented by Opacus library~\citep{opacus}.} function (Line 4 of Alg.~\ref{alg:tidpfl:privacy}). This function takes as inputs $\eps_n^t$, $\delta$, $q$, and the number of remaining rounds $T-t$. During the spending rounds of client $n$ (i.e., when $M_n^t=1$), the client is sampled according to $q_n^t=q$. During the saving rounds (when $M_n^t=0$), the client sets $q_n^t=q_n$. As shown in lines 6-8, we calculate the privacy spent and $\eps_n^t$ at the end of each round $t\in [T]$ and for every client $n\in [N]$. Using  \texttt{Compute\_rdp} function, we calculate the privacy spent given $q_n^t, \noisem_n^t,$ and $\alpha$. We then use \texttt{get\_privacy\_spent} to convert the RDP privacy spent into the DP privacy spent, given the RDP privacy spent, $\alpha$, and $\delta$. We finally follow the same procedure as in~\citet{boenisch2024have} to compute local clip norms $c_n^t$ such that their average across $n\in [N]$ equals the hyperparameter $c$. To achieve this, we calculate $c_n^t={c\noisem^t}/{\noisem_n^t}$ (Line 12), where $\noisem^t = N\left(\sum_{n\in [N]}{1}/{\noisem_n^t}\right)^{-1}$ (Line 10). }

 \vspace{-2ex}
\input{algorithm3_merge}

\subsection{The Iterative Training Module}\label{sec:proposed:alg:iterative}
After setting parameters through the \algasgo method, our DP-FL framework operates the iterative training module, with the pseudocode presented in Algorithm~\ref{alg:tidpfl:train}. We note that although the primary contribution of our DP-FL framework lies in the \algasgo method, the iterative training module also differs from the baseline due to the use of time-adaptive privacy parameters and has to be carefully designed. This module spans $T$ communication rounds. Within each round, clients conduct $L$ iterations of local training. For iteration $l\in [L]$ within round $t\in [T]$, let $\model_n^{t,l}$ denote the local model of client $n$. In round $t\in [T]$, $\gC^t$ denotes the set of workers contributing to local training. This set is randomly chosen using Poisson sampling (Line 4). Each client $n\in [N]$ in round $t\in [T]$ has an independent probability $q_n^t \in [0,1]$ of being selected for $\gC^t$. In expectation $q^tN$ clients, where $q^t=\frac{1}{N}\sum_{n\in [N]}q_n^t$, are sampled in round $t$.

\textbf{Client update}: In round $t$, each client $n\in [N]$ initializes $\model_n^{t,0}=\model^{t-1}$ (Line 1 of $\texttt{ClientUpdate}$). It then performs $L$ iterations of local training, using a gradient-based technique, such as mini-batch SGD. In each iteration $l\in [L]$, the client first splits $\train_n$ into size $\bs$ batches (Line 3 of $\texttt{ClientUpdate}$). For each batch $\batch_i$, the $n$th client updates $\model_n^{t,l} = \model_n^{t,l-1}-\frac{\lr}{\bs}\sum_{(\rvx,y)\in \batch_i} \nabla\loss_n\left(\model_n^{t,l-1};(\rvx,y)\right), $ where $\lr$ is the learning rate and $\frac{1}{\bs}\sum_{(\rvx,y)\in \batch_i} \nabla\loss_n\left(\model_n^{t,l-1};(\rvx,y)\right)$ estimates the gradient $\nabla \bar{\loss}_n(\model_n^{t,l-1})$ (Line 5 of $\texttt{ClientUpdate}$). Once local training finishes, the client computes the resulting model update $\Delta\model_n^t = \model_n^{t,L} - \model_n^{t,0}$ (Line 8 of $\texttt{ClientUpdate}$). The first phase of integrating the DP mechanism in the iterative training module is to assign the client the task of clipping $\Delta\model_n^t$ as shown in (Line 9 of $\texttt{ClientUpdate}$), and detailed as $\tilde{\Delta}\model_n^{t} = \Delta\model_n^t\min\left(1,\frac{c_n^t}{\left\|\Delta\model_n^{t}\right\|_2}\right)$, where $c_n^t$ is the clip norm. The client then perturbs further its clipped model update by injecting random noise (Line 10 of $\texttt{ClientUpdate}$). The noise is selected independently for each client $n\in \gC^t$ in round $t$. Algebraically, given noise $z_n^t\sim \gN\left(0,\frac{(c_n^t\noisem_n^t)^2}{N}\sI\right)$, $\tilde{\Delta}\model_n^t \gets \tilde{\Delta}\model_n^t + z_n^t
$. 

\textbf{Server aggregation:} As shown in lines 6 and 8, the server aggregates $\tilde{\Delta}\model_n^t$ for all $n\in \gC^t$, and computes the sum $\tilde{\Delta}\model^t = \sum_{n\in \gC^t} \tilde{\Delta}\model_n^t$. For those clients not being selected, i.e., $n\in [N]\backslash \gC^t$, the server compensates by injecting additional noise $\tilde{\Delta}\model^t \gets \tilde{\Delta}\model^t +  \sum_{n \in  [N]\backslash \gC^t} \gN\left(0,c^2(\noisem^t)^2/N\sI\right)$. Assuming $c\noisem^t=c_n^t\noisem_n^t$ for all $n\in [N]$, because of this compensation, the total noise power $\gN\left(0,c^2(\noisem^t)^2\sI\right)$ that is injected to the global model update is not a function of the sampling. The round ends with the server computing $\model^{t} = \model^{t-1} + \frac{\tilde{\Delta}\model^t}{q^tN}$ (Line 9).


