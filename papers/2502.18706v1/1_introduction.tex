\section{Introduction}
With machine learning (ML) relying increasingly on users' sensitive data, the development of utility-driven frameworks that also adhere to users' individual constraints, including privacy preferences, has become a priority. When federated learning (FL)~\citep{mcmahan2017communication} was first introduced, it was perceived as a privacy-preserving distributed learning framework that allows users (also called \textit{clients}) to keep their data local and solely exchanging model updates with the server who can aggregate the updates and apply them to the global model for training. 


However, it has been shown that data can be leaked through the model gradients~\citep{zhu2019deep, geiping2020inverting, boenisch2023curious}. Hence, FL was extended to incorporate formal privacy guarantees~\citep{mcmahan2017learning, geyer2017differentially, wei2020federated, hu2023federated, ramaswamy2020training} via the mathematical framework of differential privacy (DP)~\citep{dwork2006differential}. In DP-FL frameworks, one common approach is to protect the entire dataset of each client (``client-level DP'') by clipping local model updates and adding noise before releasing them in each training round~\citep{truex2019hybrid, truex2020ldp}. This ensures that an adversary who has access to the aggregated perturbed updates of a subset of clients cannot confidently infer whether or not any particular client has participated in the given training round. However, although the DP-FL framework ensures privacy, it degrades model utility by introducing errors into the model updates. Thus, careful calibration of the perturbations is necessary to balance privacy and utility. 

There are several extensions of the DP-FL framework in the literature that aim to improve privacy-utility tradeoffs by reducing the effect of perturbation while adhering to the privacy budgets of clients. Typically, these prior works~\citep{pichapati2019adaclip, yang2021federated, shen2023pldp,yang2023dynamic, mcmahan2017learning} consider the inherent heterogeneity in FL—both in data and privacy—to make perturbation more efficient. Yet, they either assume that clients' privacy budgets should be exhausted \textit{uniformly over time}~\citep{boenisch2024have}, or rely on strong assumptions regarding access to public data~\citep{li2022private} or negligible privacy loss when adjusting privacy parameters in a time-adaptive manner based on data~\citep{pichapati2019adaclip}. As we will see, judiciously expending the budget non-uniformly over time and in a privacy-preserved manner can yield an improved privacy-utility tradeoff. Hence, a gap exists in the literature.

In this work, we reduce this gap by proposing a novel DP-FL framework with a \textit{data-independent time-adaptive} privacy spending method. In our framework, clients can spend their privacy budget \textit{non-uniformly across time} (training rounds). This means that clients intentionally allocate less of their privacy budgets in the early rounds to save them for later rounds. We term these rounds as ``saving''. Clients then transit to ``spending'' rounds, wherein they uniformly allocate their remaining budget across spending rounds. The decisions about when each client transits from saving to spending and how much they save in each round are made solely based on clients' privacy budgets, and not their local data. Therefore, we can schedule spending before the start of training, making it free of privacy loss. We account for each client's privacy spending in each round, formulating privacy bounds as a function of clients' decisions and budgets.


Two observations that motivate the potential of our framework to improve the privacy-utility tradeoff are as follows. First, by preserving the privacy budget in early rounds and incrementally spending later, we are able to adjust the signal-to-noise (SNR) ratio to be uneven across training rounds, with noise shifting from later rounds to earlier ones. This enables coarse-grained features, which are typically learned in the early rounds and are more tolerant to noise, still to be learned effectively. Furthermore, the fine-grained features, typically learned in later rounds~\citep{dziedzic2019band, raghu2017svcca, shwartz2017opening}, can be learned in a beneficial higher-SNR setting. Secondly, in practical scenarios, we note that clients are likely to have different privacy budgets. We show theoretically that clients with stricter privacy budgets benefit from expending their privacy budgets more unevenly than those with relaxed (larger) budgets. Intuitively, this allows less-privacy-sensitive clients, who have often sufficient budgets, to contribute to the learning both of coarse-grained features in early rounds and of fine-grained features in later rounds. On the other hand, more-privacy-constrained clients can preserve their budgets and helpfully contribute more to the learning of fine-grained features. 

In summary, we make the following contributions:
\begin{itemize}
\item %\tcb
{As part of our framework design (detailed in Sec.~\ref{sec:propose}), we introduce a novel privacy spending method, namely ``spend-as-you-go'', where clients spend their privacy budgets incrementally over time, instead of spending the privacy budget uniformly across time, as in traditional DP-FL approaches.} 

\item %\tcb
{Our theoretical analysis (detailed in Sec.~\ref{sec:theory}) provides privacy accounting for the incremental spending pattern in our method.
Additionally, we show theoretically, that if clients that use stricter privacy parameters, such as lower clipping norms, save a larger portion of their privacy budget during saving rounds, and can spend more in spending rounds, then, in expectation, we can reduce the clipping bias~\citep{das2023beyond}.}

\item Based on these theoretical insights, in Sec.~\ref{sec:simulation} we experimentally benchmark our framework against the baselines and show that the global test accuracy achieved by our method surpasses that of the baselines for the FMNIST ~\citep{xiao2017fashion}, MNIST~\citep{deng2012mnist}, Adult Income~\citep{adult_2}, %\tcb
{and CIFAR10 datasets~\citep{krizhevsky2009learning}}. 

\end{itemize}

