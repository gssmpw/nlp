\begin{minipage}{0.47\textwidth} 
\begin{table}[H]
\centering
\begin{threeparttable}
\caption{Hyperparameters Summary}
%, The First Four Rows are Common With Prior Work, The Fifth Row is Specific to This Work.
\label{sec3:notations}  \scriptsize
\begin{tabular}{|c|l|}
\hline
$T_n$ & Saving-to-spending transition round 
\\ \hline 
$q_n$ & Saving-based sampling rate of Client $n$ 
 \\ \hline \hline
$T$ & Number of rounds \\ \hline
$L$ & Number of local iterations
\\ \hline
 $B$ & Batch size \\ \hline
$\lr$ & Learning rate 
\\ \hline
$\alpha$ & R\'{e}nyi order in RDP \\ \hline
$\delta$ & Probability of violating in DP  
\\ \hline
$c$ & Average clipping norm \\ \hline
 $q$ & Global (spending-based) sampling rate 
\\ \hline
\end{tabular}
 %\begin{tablenotes}
%	\item[1] Table's abbreviations: ``No.'' for ``Number'' and ``R.'' for ``Round''.   % \end{tablenotes}
\end{threeparttable}
\end{table}  
 \vspace{-1em}
  \raggedright  % Left-align to avoid justification causing overfull hbox
\begin{algorithm}[H]
  \scriptsize
\caption{The \algasgo Method in Our Time-adaptive DP-FL Framework}
\textbf{Inputs:} No. clients $N$, No. global rounds $T$, local privacy budgets $\eps_n$, sampling rate $q$, average clip norm $c$, modes $M_n^t \in\{0,1\}$, sampling rates for saving mode $q_n$, Prob. of violating $\delta$. \\ 
\label{alg:tidpfl:privacy}
\textbf{Def} \texttt{SetPrivacyParams}$\left(c, q,  \{q_n, \eps_n, M_n^t\}_{\substack{n\in [N]\\ t\in [T]}}, T, \delta\right)$
\begin{algorithmic}[1]
\State \textbf{Initialize} $\eps_n^0 = \eps_n$  and $\bar{\eps}_{\text{rdp},n}^0=0$
for all $n\in [N]$
\For{each global round $t\in [T]$}
\For{each client $n \in [N]$}
\State  $\noisem_n^t=$\texttt{GetNoise}$\left(\eps_n^{t-1},\delta, q, T-t\right)$
\State \textbf{If} $M_n^t=0$, \textbf{then} $q_n^t = q_n$, \textbf{Else}, $q_n^t = q$.
\State $\reps{n}^t=$\texttt{Compute\_rdp}$\left(q_n^t,\noisem_n^t, \alpha\right)$
\State $\bar{\eps}_{\text{rdp},n}^t = \bar{\eps}_{\text{rdp},n}^{t-1} +  \reps{n}^t$
\State $\eps_n^t=\eps_n-$\texttt{get\_privacy\_spent}$\left(\alpha, \bar{\eps}_{\text{rdp},n}^t,\delta\right)$
\EndFor
\State Compute $\noisem^t \gets \left(\frac{1}{N}\sum_{n\in [N]}\frac{1}{\noisem_n^t}\right)^{-1}$
\For{each client $n \in [N]$}
\State Set local clip norm $c_n^t=\frac{c\noisem^t}{\noisem_n^t}$
\EndFor
\State $q^t = \frac{1}{N} \sum_{n=1}^N q_n^t$
\EndFor
\State Return $\{q^t, \noisem^t, \{q_n^t, c_n^t, \noisem_n^t\}_{n\in [N]}\}_{t\in [T]}$
\end{algorithmic}
\end{algorithm}
\end{minipage}%
\hspace{1em} 
\begin{minipage}{0.50\textwidth}  %
  \raggedright  % Left-align
 \begin{algorithm}[H]
   \scriptsize
\caption{Iterative Training in Our Time-adaptive DP-FL Framework}
\textbf{Inputs:} No. clients $N$, No. global rounds $T$, No. local iterations $L$, local privacy budgets $\eps_n$, sampling rate $q$, average clip norm $c$, modes $M_n^t \in\{0,1\}$, sampling rates for saving mode $q_n$, loss functions $\loss_n$, datasets $\train_n$, learning rate $\lr$, batch size $\bs$, Prob. of violating $\delta$ \\ 
\label{alg:tidpfl:train}
\begin{algorithmic}[1]
\State $\{q^t, \noisem^t, \{q_n^t, c_n^t, \noisem_n^t\}_{n\in [N]}\}_{t\in [T]}$\ $=$\texttt{SetPrivacyParams}$\left(c, q,  \{q_n, \eps_n, M_n^t\}_{\substack{n\in [N]\\ t\in [T]}}, T, \delta\right)$
\State \textbf{Initialize} global model $\model^0$
\For{each global round $t \in [T]$} 
\State $\gC^t \gets$ Sample clients with probability $\{q_n^t\}$.
\For{each client %\tcb
{$n\in [N]$} in parallel}
\State $\tilde{\Delta}\model_n^t$ =  
\texttt{ClientUpdate}$\left(t, n,\model^{t-1}, c_n^t, \noisem_n^t\right)$.
\EndFor
\State $\tilde{\Delta}\model^t = \sum_{n\in \gC^t} \tilde{\Delta}\model_n^t +  \sum_{n\in  [N]\backslash \gC^t} \gN\left(0,\frac{c^2(\noisem^t)^2}{N}\sI\right)$
\State Update $\model^{t} = \model^{t-1} + \frac{\tilde{\Delta}\model^t}{q^tN}$
\EndFor
\Statex
\end{algorithmic}
\textbf{Def} \texttt{ClientUpdate}$\left(t, n,\model^{t-1}, c_n^t, \noisem_n^t\right)$
\begin{algorithmic}[1]
\State \textbf{Initialize} local model $\model_n^{t,0}=\model^{t-1}$
\For{local iteration $l \in [l]$}
\State $\{\batch_i\}_{i=1}^{|\train_n|/B} \gets$ Split $\train_n$ to size $B$ batches
\For{each batch $\batch_i$}
\State $\model_n^{t,l} = \model_n^{t,l-1}-\frac{\lr \sum_{(\rvx,y)\in \batch_i} \nabla\loss_n\left(\model_n^{t,l-1};(\rvx,y)\right)}{\bs}$
\EndFor
\EndFor
\State Compute $\Delta\model_n^t = \model_n^{t,L} - \model_n^{t,0}$
\State Clip $\tilde{\Delta}\model_n^{t} = \Delta\model_n^t\min\left(1,\frac{c_n^t}{\left\|\Delta\model_n^{t}\right\|_2}\right)$  
\State Add noise $\tilde{\Delta}\model_n^t \gets \tilde{\Delta}\model_n^t + \gN\left(0,\frac{(c_n^t)^2(\noisem_n^t)^2}{N}\sI\right)$
\State Return $\tilde{\Delta}\model_n^{t}$ 
\Statex
\end{algorithmic}
\end{algorithm}
\end{minipage}