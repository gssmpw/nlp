\begin{algorithm}[H]
  \scriptsize
\caption{Federated Averaging (FedAvg)~\citep{mcmahan2017communication}}
\textbf{Inputs:} No. clients $N$, No. global rounds $T$, No. local iterations $L$, loss functions $\loss_n$, local datasets $\train_n$, learning rate $\lr$, batch size $\bs$  \\
\label{alg:fedavg}
\begin{algorithmic}[1]
\State \textbf{Initialize} global model $\model^0$
\For{each global round $t \in [T]$} 
\State $\gC^t \gets$ Sample clients with probability $q$.
\For{each client $n\in \gC^t$ in parallel}
\State ${\Delta}\model_n^t$ =  
\texttt{ClientUpdate}$\left(t, n,\model^{t-1}\right)$.
\EndFor
\State Aggregate ${\Delta}\model^t = \sum_{n\in \gC^t} \tilde{\Delta}\model_n^t$
\State Update $\model^{t} = \model^{t-1} + \frac{{\Delta}\model^t}{qN}$
\EndFor
\Statex
\end{algorithmic}
\textbf{Def} \texttt{ClientUpdate}$\left(t, n,\model^{t}\right)$
\begin{algorithmic}[1]
\State \textbf{Initialize} local model $\model_n^{t,0}=\model^t$
\For{local iteration $l \in [l]$}
\State $\{\batch_i\}_{i=1}^{|\train_n|/B} \gets$ Split $\train_n$ to size $B$ batches
\For{each batch $\batch_i$}
\State $\model_n^{t,l} = \model_n^{t,l-1}-\frac{\lr \sum_{(\rvx,y)\in \batch_i} \nabla\loss_n\left(\model_n^{t,l-1};(\rvx,y)\right)}{\bs}$
\EndFor
\EndFor
\State Return $\Delta\model_n^t = \model_n^{t,L} - \model_n^{t,0}$
\end{algorithmic}
\end{algorithm}
