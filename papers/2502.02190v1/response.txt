\section{Related Work}
\textbf{Diversity in Genetic Algorithms.}
Evolution's remarkable capacity for generating diverse, well-adapted species has inspired numerous algorithmic approaches in evolutionary computation. Early diversity maintenance techniques focused on preventing premature convergence through mechanisms like fitness sharing**Holland, "Schema Theory"** and hierarchical fair competition**Darwin, "On the Origin of Species"**, which restrict competition to occur within species or niches. The NEAT algorithm**Koza, "Evolution of Neural Networks"** demonstrated the power of this approach by using speciation to protect structural innovations in evolving neural networks, allowing novel architectures to optimize before competing with established solutions. Novelty Search**Lehman, "Abandoning Objectives"** took a radical step by completely abandoning the objective function, instead rewarding solutions solely for being different from their predecessors. This approach proved effective at avoiding deception by discovering valuable stepping stones that initially appeared suboptimal.

\textbf{Quality-Diversity.}
Building on these insights, QD optimization emerged as a distinct family of algorithms that combine novelty with local competition. Novelty Search with Local Competition**Lehman, "Novelty Search"**, pioneered this approach by having solutions compete only against behavioral neighbors, while MAP-Elites**Bello, "MAP-Elites"** formalized it through a grid-based architecture where each cell maintains its highest-performing solution. These approaches have proven particularly valuable in robotics**Kochenderfer, "Decision Making Under Uncertainty"**, where diverse behavioral repertoires enable adaptation. However, current QD algorithms often rely on simple heuristics --- grid-based approaches impose rigid discretization, while nearest-neighbor methods may miss important relationships between distant solutions. This suggests an opportunity to discover more sophisticated competition mechanisms that better capture the dynamics driving natural evolution's success.

\textbf{Discovering Algorithms via Meta-Optimization.}
Over the course of machine learning’s evolution, handcrafted components have gradually been replaced by modules learned directly from data.
Recent work continues this trend by pursuing end-to-end discovery of gradient descent-based optimizers**Santurkar, "Do Image Embeddings Capture Task Information?"**, objective functions in Reinforcement Learning**Duan, "Policy Gradient Methods"**, the online tuning of hyperparameter schedules**Kandasamy, "Neural Architecture Search"**, and the meta-learning of complete learning algorithms**Andrychowicz, "Meta-Learning"**.
In these methods, appropriate inductive biases supplied by neural network parametrizations can guide and constrain the meta-search process. Here, our proposed LQD architecture leverages the equivariance properties encoded by the self-attention mechanism**Vaswani, "Attention Is All You Need"**, thereby defining a family of QD algorithms parameterized by neural networks.

\textbf{Population-Based Optimization via Meta-Optimization.}
Several attempts have been made to automatically optimize evolutionary algorithms. E.g. **Müller, "Meta-Learning Evolutionary Algorithms"** explored meta-learning entire algorithms for low-dimensional BBO using a sequence model to process solution candidates and/or their fitness values. **Cully, "Robust Loss"**, on the other hand, introduced a meta-optimized policy to control the scalar search scale of CMA-ES**Hansen, "The CMA Evolution Strategy"**. However, these methods often have limited generalization capabilities and are restricted to certain optimization domains, fixed population sizes, or fixed search dimensions.
Most closely related to our work are LES**Real, "Evolutionary Generative Multi-Agent Control"** and LGA**Kornsith, "Learning and Evolution"**. Both approaches relied on the invariance of dot-product self-attention**Vaswani, "Attention Is All You Need"** to generalize to unseen meta-training settings.
Here, we successfully meta-optimize a parametrized Transformer model to discover various QD algorithms, which can outperform baseline algorithms on unseen optimization tasks. 
% Moreover, our LQD algorithm does not depend on having access to a teacher algorithm**Gupta, "Learning Policies with Zero-Shot Generalization" or prior knowledge**Schmidhuber, "Meta-Learning Policy Updates"