@inproceedings{GeneticAlgorithmsSharing1987,
    title = {Genetic Algorithms with Sharing for Multimodal Function Optimization},
    booktitle = {Genetic Algorithms and their Applications: Proceedings of the Second International Conference on Genetic Algorithms},
    author = {Goldberg, David E. and Richardson, Jon},
    year = {1987},
    pages = {41--49}
}

@article{andrychowicz_2016,
    title={Learning to learn by gradient descent by gradient descent},
    author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
    journal={Advances in neural information processing systems},
    volume={29},
    year={2016}
}

@incollection{bengio_1992,
    title={On the optimization of a synaptic learning rule},
    author={Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gescei, Jan},
    booktitle={Optimality in Biological and Artificial Networks?},
    pages={281--303},
    year={1992},
    publisher={Routledge}
}

@inproceedings{chen_2017,
    title = 	 {Learning to Learn without Gradient Descent by Gradient Descent},
    author =       {Yutian Chen and Matthew W. Hoffman and Sergio G{\'o}mez Colmenarejo and Misha Denil and Timothy P. Lillicrap and Matt Botvinick and Nando de Freitas},
    booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
    pages = 	 {748--756},
    year = 	 {2017},
    editor = 	 {Precup, Doina and Teh, Yee Whye},
    volume = 	 {70},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {06--11 Aug},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v70/chen17e/chen17e.pdf},
    url = 	 {https://proceedings.mlr.press/v70/chen17e.html},
    abstract = 	 {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.}
}

@article{cully_RobotsThatCan_2015,
    title = {Robots that can adapt like animals},
    volume = {521},
    issn = {1476-4687},
    url = {https://doi.org/10.1038/nature14422},
    doi = {10.1038/nature14422},
    abstract = {An intelligent trial-and-error learning algorithm is presented that allows robots to adapt in minutes to compensate for a wide variety of types of damage.},
    number = {7553},
    journal = {Nature},
    author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
    month = may,
    year = {2015},
    pages = {503--507},
}

@article{gomes2021meta,
    title={Meta learning black-box population-based optimizers},
    author={Gomes, Hugo Siqueira and L{\'e}ger, Benjamin and Gagn{\'e}, Christian},
    journal={arXiv preprint arXiv:2103.03526},
    year={2021}
}

@article{hansen2001completely,
    title={Completely derandomized self-adaptation in evolution strategies},
    author={Hansen, Nikolaus and Ostermeier, Andreas},
    journal={Evolutionary computation},
    volume={9},
    number={2},
    pages={159--195},
    year={2001},
    publisher={MIT Press}
}

@article{hfc,
    author = {Hu, Jianjun and Goodman, Erik and Seo, Kisung and Fan, Zhun and Rosenberg, Rondal},
    title = {The Hierarchical Fair Competition (HFC) Framework for Sustainable Evolutionary Algorithms},
    year = {2005},
    issue_date = {June 2005},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {13},
    number = {2},
    issn = {1063-6560},
    url = {https://doi.org/10.1162/1063656054088530},
    doi = {10.1162/1063656054088530},
    abstract = {Many current Evolutionary Algorithms (EAs) suffer from a tendency to converge prematurely or stagnate without progress for complex problems. This may be due to the loss of or failure to discover certain valuable genetic material or the loss of the capability to discover new genetic material before convergence has limited the algorithm's ability to search widely. In this paper, the Hierarchical Fair Competition (HFC) model, including several variants, is proposed as a generic framework for sustainable evolutionary search by transforming the convergent nature of the current EA framework into a non-convergent search process. That is, the structure of HFC does not allow the convergence of the population to the vicinity of any set of optimal or locally optimal solutions. The sustainable search capability of HFC is achieved by ensuring a continuous supply and the incorporation of genetic material in a hierarchical manner, and by culturing and maintaining, but continually renewing, populations of individuals of intermediate fitness levels. HFC employs an assembly-line structure in which subpopulations are hierarchically organized into different fitness levels, reducing the selection pressure within each subpopulation while maintaining the global selection pressure to help ensure the exploitation of the good genetic material found. Three EAs based on the HFC principle are tested - two on the even-10-parity genetic programming benchmark problem and a real-world analog circuit synthesis problem, and another on the HIFF genetic algorithm (GA) benchmark problem. The significant gain in robustness, scalability and efficiency by HFC, with little additional computing effort, and its tolerance of small population sizes, demonstrates its effectiveness on these problems and shows promise of its potential for improving other existing EAs for difficult problems. A paradigm shift from that of most EAs is proposed: rather than trying to escape from local optima or delay convergence at a local optimum, HFC allows the emergence of new optima continually in a bottom-up manner, maintaining low local selection pressure at all fitness levels, while fostering exploitation of high-fitness individuals through promotion to higher levels.},
    journal = {Evol. Comput.},
    month = jun,
    pages = {241–277},
    numpages = {37}
}

@article{kirsch2021meta,
    title={Meta learning backpropagation and improving it},
    author={Kirsch, Louis and Schmidhuber, J{\"u}rgen},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    pages={14122--14134},
    year={2021}
}

@inproceedings{kirsch2022introducing,
    title={Introducing symmetries to black box meta reinforcement learning},
    author={Kirsch, Louis and Flennerhag, Sebastian and van Hasselt, Hado and Friesen, Abram and Oh, Junhyuk and Chen, Yutian},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={36},
    number={7},
    pages={7202--7210},
    year={2022}
}

@article{kossen2021self,
    title={Self-attention between datapoints: Going beyond individual input-output pairs in deep learning},
    author={Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Thomas and Gal, Yarin},
    journal={Advances in Neural Information Processing Systems},
    volume={34},
    pages={28742--28756},
    year={2021}
}

@misc{lange_DiscoveringAttentionBasedGenetic_2023,
	title = {Discovering {Attention}-{Based} {Genetic} {Algorithms} via {Meta}-{Black}-{Box} {Optimization}},
	url = {http://arxiv.org/abs/2304.03995},
	doi = {10.48550/arXiv.2304.03995},
	abstract = {Genetic algorithms constitute a family of black-box optimization algorithms, which take inspiration from the principles of biological evolution. While they provide a general-purpose tool for optimization, their particular instantiations can be heuristic and motivated by loose biological intuition. In this work we explore a fundamentally different approach: Given a sufficiently flexible parametrization of the genetic operators, we discover entirely new genetic algorithms in a data-driven fashion. More specifically, we parametrize selection and mutation rate adaptation as cross- and self-attention modules and use Meta-Black-Box-Optimization to evolve their parameters on a set of diverse optimization tasks. The resulting Learned Genetic Algorithm outperforms state-of-the-art adaptive baseline genetic algorithms and generalizes far beyond its meta-training settings. The learned algorithm can be applied to previously unseen optimization problems, search dimensions \& evaluation budgets. We conduct extensive analysis of the discovered operators and provide ablation experiments, which highlight the benefits of flexible module parametrization and the ability to transfer (`plug-in') the learned operators to conventional genetic algorithms.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Lange, Robert Tjarko and Schaul, Tom and Chen, Yutian and Lu, Chris and Zahavy, Tom and Dalibard, Valentin and Flennerhag, Sebastian},
	month = apr,
	year = {2023},
	note = {arXiv:2304.03995 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{lange_DiscoveringEvolutionStrategies_2023,
	title = {Discovering {Evolution} {Strategies} via {Meta}-{Black}-{Box} {Optimization}},
	url = {http://arxiv.org/abs/2211.11260},
	doi = {10.48550/arXiv.2211.11260},
	abstract = {Optimizing functions without access to gradients is the remit of black-box methods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inflexible - exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that meta-evolving this system on a small set of representative low-dimensional analytic optimization problems is sufficient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop.},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Lange, Robert Tjarko and Schaul, Tom and Chen, Yutian and Zahavy, Tom and Dallibard, Valentin and Lu, Chris and Singh, Satinder and Flennerhag, Sebastian},
	month = mar,
	year = {2023},
	note = {arXiv:2211.11260 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{lee2019set,
    title={Set transformer: A framework for attention-based permutation-invariant neural networks},
    author={Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
    booktitle={International conference on machine learning},
    pages={3744--3753},
    year={2019},
    organization={PMLR}
}

@article{lehman_AbandoningObjectivesEvolution_2011,
    title = {Abandoning {Objectives}: {Evolution} {Through} the {Search} for {Novelty} {Alone}},
    volume = {19},
    issn = {1063-6560, 1530-9304},
    shorttitle = {Abandoning {Objectives}},
    url = {https://direct.mit.edu/evco/article/19/2/189-223/1365},
    doi = {10.1162/EVCO_a_00025},
    abstract = {In evolutionary computation, the ﬁtness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artiﬁcial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search signiﬁcantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
    language = {en},
    number = {2},
    urldate = {2025-01-17},
    journal = {Evolutionary Computation},
    author = {Lehman, Joel and Stanley, Kenneth O.},
    month = jun,
    year = {2011},
    pages = {189--223},
}

@inproceedings{lu2022discovered,
  title={Discovered Policy Optimisation},
  author={Lu, Chris and Kuba, Jakub Grudzien and Letcher, Alistair and Metz, Luke and de Witt, Christian Schroeder and Foerster, Jakob Nicolaus},
  booktitle={Decision Awareness in Reinforcement Learning Workshop at ICML 2022},
  year={2022}
}

@inproceedings{metz2019understanding,
    title={Understanding and correcting pathologies in the training of learned optimizers},
    author={Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, Daniel and Sohl-Dickstein, Jascha},
    booktitle={International Conference on Machine Learning},
    pages={4556--4565},
    year={2019},
    organization={PMLR}
}

@misc{mouret_IlluminatingSearchSpaces_2015,
	title = {Illuminating search spaces by mapping elites},
	url = {http://arxiv.org/abs/1504.04909},
	doi = {10.48550/arXiv.1504.04909},
	abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Mouret, Jean-Baptiste and Clune, Jeff},
	month = apr,
	year = {2015},
	note = {arXiv:1504.04909 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics, Quantitative Biology - Populations and Evolution},
}

@article{neat,
    title={Evolving Neural Networks Through Augmenting Topologies},
    author={Kenneth O. Stanley and Risto Miikkulainen},
    volume={10},
    journal={Evolutionary Computation},
    number={2},
    pages={99-127},
    url="http://nn.cs.utexas.edu/?stanley:ec02",
    year={2002}
}

@inproceedings{nslc,
    author = {Lehman, Joel and Stanley, Kenneth O.},
    title = {Evolving a diversity of virtual creatures through novelty search and local competition},
    year = {2011},
    isbn = {9781450305570},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2001576.2001606},
    doi = {10.1145/2001576.2001606},
    abstract = {An ambitious challenge in artificial life is to craft an evolutionary process that discovers a wide diversity of well-adapted virtual creatures within a single run. Unlike in nature, evolving creatures in virtual worlds tend to converge to a single morphology because selection therein greedily rewards the morphology that is easiest to exploit. However, novelty search, a technique that explicitly rewards diverging, can potentially mitigate such convergence. Thus in this paper an existing creature evolution platform is extended with multi-objective search that balances drives for both novelty and performance. However, there are different ways to combine performance-driven search and novelty search. The suggested approach is to provide evolution with both a novelty objective that encourages diverse morphologies and a local competition objective that rewards individuals outperforming those most similar in morphology. The results in an experiment evolving locomoting virtual creatures show that novelty search with local competition discovers more functional morphological diversity within a single run than models with global competition, which are more predisposed to converge. The conclusions are that novelty search with local competition may complement recent advances in evolving virtual creatures and may in general be a principled approach to combining novelty search with pressure to achieve.},
    booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
    pages = {211–218},
    numpages = {8},
    keywords = {artificial life, natural evolution, novelty search, virtual creatures},
    location = {Dublin, Ireland},
    series = {GECCO '11}
}

@article{oh2020discovering,
  title={Discovering reinforcement learning algorithms},
  author={Oh, Junhyuk and Hessel, Matteo and Czarnecki, Wojciech M and Xu, Zhongwen and van Hasselt, Hado P and Singh, Satinder and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1060--1070},
  year={2020}
}

@article{parker2022automated,
  title={Automated reinforcement learning (autorl): A survey and open problems},
  author={Parker-Holder, Jack and Rajan, Raghu and Song, Xingyou and Biedenkapp, Andr{\'e} and Miao, Yingjie and Eimer, Theresa and Zhang, Baohe and Nguyen, Vu and Calandra, Roberto and Faust, Aleksandra and others},
  journal={Journal of Artificial Intelligence Research},
  volume={74},
  pages={517--568},
  year={2022}
}

@inproceedings{shala2020learning,
    title={Learning step-size adaptation in CMA-ES},
    author={Shala, Gresa and Biedenkapp, Andr{\'e} and Awad, Noor and Adriaensen, Steven and Lindauer, Marius and Hutter, Frank},
    booktitle={International Conference on Parallel Problem Solving from Nature},
    pages={691--706},
    year={2020},
    organization={Springer}
}

@inproceedings{tang_SensoryNeuronTransformer_2021,
	title = {The {Sensory} {Neuron} as a {Transformer}: {Permutation}-{Invariant} {Neural} {Networks} for {Reinforcement} {Learning}},
	volume = {34},
	shorttitle = {The {Sensory} {Neuron} as a {Transformer}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/be3e9d3f7d70537357c67bb3f4086846-Abstract.html},
	abstract = {In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io},
	urldate = {2025-01-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tang, Yujin and Ha, David},
	year = {2021},
	pages = {22574--22587},
}

@inproceedings{tv2019meta,
    title={Meta-learning for black-box optimization},
    author={TV, Vishnu and Malhotra, Pankaj and Narwariya, Jyoti and Vig, Lovekesh and Shroff, Gautam},
    booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
    pages={366--381},
    year={2019},
    organization={Springer}
}

@article{wang2016learning,
    title={Learning to reinforcement learn},
    author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
    journal={arXiv preprint arXiv:1611.05763},
    year={2016}
}

@article{xu2018meta,
    title={Meta-gradient reinforcement learning},
    author={Xu, Zhongwen and van Hasselt, Hado P and Silver, David},
    journal={Advances in neural information processing systems},
    volume={31},
    year={2018}
}

@article{xu2020meta,
  title={Meta-gradient reinforcement learning with an objective discovered online},
  author={Xu, Zhongwen and van Hasselt, Hado P and Hessel, Matteo and Oh, Junhyuk and Singh, Satinder and Silver, David},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15254--15264},
  year={2020}
}

@article{zahavy2020self,
    title={A self-tuning actor-critic algorithm},
    author={Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Oh, Junhyuk and van Hasselt, Hado P and Silver, David and Singh, Satinder},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    pages={20913--20924},
    year={2020}
}

