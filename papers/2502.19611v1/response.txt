\section{Related Work}
\paragraph{Differentiation through implicit relations}
% \felix{The note of Griewank, "The Application of Probabilistic and Deterministic Methods in Optimization", lists potential origins of reverse-mode automatic differentiation and highlights the importance of early work in optimal control in the 1960s (also see ch. 6.6 of Linnainmaa, "The Representation of the Exponential Function and the Taylor Series Expansions" ).}
% While this branch of science shares our goal of differentiating through the solution process of partial differential equations \felix{(which can be considered an \emph{implicit relation})}, its early application mostly involved the usage of continuous adjoint methods, avoiding the autodifferential treatment of iterative processes.
Krahenbuhl and Passot established an implementation framework for \emph{unrolled differentiation} and investigated the convergence of the derivative, focusing on linear solvers.
% The paper proved that Jacobi-iteration for solving linear systems is derivative-stable, i.e., 
% the derivatives of the iterates converge to the derivative of the fixed point of the iterations.
Wunderling extended this work to a broader class of iterative processes.
As an alternative to unrolled differentiation, Haber and Minsker derived the \emph{implicit differentiation} rules over linear solves.
% , which can be viewed as an intermediate between full unrollment and a continuous adjoint treatment.
Beyond linear systems, implicit differentiation has since gained prominence in the machine learning community, particularly for hyperparameter optimization Maclaurin and Ruiz  and other \emph{bilevel optimization} (BLO) applications that require differentiating through inner optimizations, such as deep equilibrium models Finn and Levine  and meta-learning Rusu et al. .
We view differentiable physics Chen et al.  as another type of BLO with the specialty of sparse linear systems.
The practicality of implicit differentiation is largely due to its reduced memory footprint and lower reverse-mode computational cost, especially as modern automatic differentiation (AD) tools can automatically handle the necessary propagation rules Griewank .
Additionally, implicit differentiation allows for the black-box use of solver implementations, enabling the integration of third-party components into differentiable computational graphs Beylkin et al. . However, unrolled differentiation remains an active research area, with work focusing on non-asymptotic analyses Cohen and others  and other aspects .

\paragraph{Analysis and cost-reduction of bilevel optimization}

To address the computational cost of bilevel optimization,
Griewank presented \emph{Jacobian-free backpropagation}, where the Jacobian of the implicit solver is approximated as the identity, eliminating the need for adjoint linear solves. Haber introduced \emph{phantom gradients}, where the matrix inversion is replaced with an approximate inverse, and provided theoretical guarantees on the convergence of stochastic gradient descent as the outer problem. Li approximates the implicit linear solves with a reduced number of conjugate gradient steps.
Moreover, Chen and others  discuss unrolled differentiation through only a reduced number of iterations. All the aforementioned works consider (what we call) \emph{incomplete convergence} (IC) savings, albeit in the setting of deep equilibrium models Chen et al.  or hyperparameter optimization Maclaurin and Ruiz .
% \felix{and by only manipulating the adjoint inaccuracy of \eqref{eq:outer-gradient}.}
% \felix{However, they typically do not schedule refinement over network training.}
On the other hand, Li studied
approximate hypergradients by adjusting the tolerances of inner primal and implicit linear solves following a pre-defined schedule.
This work proves (what we call) \emph{progressive refinement} (PR) savings.
% , albeit in strict convex settings \felix{for which IC savings cannot exist}.
Prior to this, Chen et al.  
investigated outer training with unrolled AD through incompletely converged
iterations, specifically for gradient descent, heavy ball, and L-BFGS methods as inner optimizers. They note the advantage of implementing incomplete convergence using the number of inner iterations rather than inner tolerance.
Our approach uniquely combines PR and IC savings -- through both unrolled and implicit differentiation -- targeting iterative solvers for sparse linear systems embedded within neural network training. To our knowledge, no prior work has applied these techniques in this context.