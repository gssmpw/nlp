
\documentclass{article} % For LaTeX2e

\usepackage{iclr2025_conference,times}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{xcolor}

\definecolor{colfelix}{HTML}{31a354}
\newcommand{\felix}[1]{\textcolor{colfelix}{#1}}

\definecolor{colnils}{HTML}{a30054}
\newcommand{\nt}[1]{\textcolor{colnils}{#1}}
\newcommand{\ntcomment}[1]{\textcolor{colnils}{[NT: #1]}}

\definecolor{colkanishk}{HTML}{4682B4}
\newcommand{\kanishk}[1]{\textcolor{colkanishk}{#1}}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{wrapfig}

\definecolor{linkblue}{HTML}{001473}

\usepackage{hyperref}       % hyperlinks
\hypersetup{
    colorlinks=true,
    linkcolor=linkblue,
    citecolor=linkblue,
    urlcolor=linkblue,
    pdfborder={0 0 0}
}

% Below are uncommented because switch to sleaker algorithm2e
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{relsize} % for relative font size changes

% Custom command for machine learning style comments
\newcommand{\mlcomment}[1]{\hfill {\smaller$\triangleright$ #1}}

% Have comments in algorithms look like in Python - not used because it only works with "algorithm2e" package
% \definecolor{codegreen}{rgb}{0,0.6,0}
% \renewcommand{\CommentSty}[1]{\textcolor{codegreen}{\ttfamily\itshape #1}}
% \SetCommentSty{CommentSty}
% \SetKwComment{Comment}{#\ }{}

% \usepackage{minted} % Import minted for code highlighting
% below two lines enable minted cache to be uploaded on arXiv
% \usepackage[finalizecache,cachedir=.]{minted}
\usepackage[frozencache,cachedir=.]{minted}

\title{PRDP: Progressively Refined Differentiable Physics}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\makeatletter
\let\@fnsymbol\@arabic
\makeatother
% the above three lines make \thanks use a number instead of asterisk
\author{Kanishk Bhatia\textsuperscript{*}, Felix Koehler\textsuperscript{*}\thanks{Munich Center for Machine Learning}
, Nils Thuerey \\
Technical University of Munich\\
\texttt{\{kanishk.bhatia,f.koehler,nils.thuerey\}@tum.de}\\
\textsuperscript{*}Equal contribution
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission. 

\begin{document}

\maketitle



\begin{abstract}
%Differentiable Physics is a paradigm which allows neural networks to interact with numerical PDE solvers during training. Since these solvers are primarily iterative, differentiating through them introduces a severe computational burden as iterations grow large. 
The physics solvers employed for neural network training are primarily iterative, and hence, differentiating through them introduces a severe computational burden as iterations grow large.
Inspired by works in bilevel optimization, 
we show that full accuracy of the network is achievable through physics significantly coarser than fully converged solvers.
We propose \emph{progressively refined differentiable physics} (PRDP), an approach that identifies the level of physics refinement sufficient for full training accuracy. By beginning with coarse physics, adaptively refining it during training, and stopping refinement at the level adequate for training, it enables significant compute savings without sacrificing network accuracy. 
Our focus is on differentiating iterative linear solvers for sparsely discretized differential operators, which are fundamental to scientific computing. PRDP is applicable to both unrolled and implicit differentiation. 
We validate its performance on a variety of learning scenarios involving differentiable physics solvers such as inverse problems, autoregressive neural emulators, and correction-based neural-hybrid solvers.
In the challenging example of emulating the Navier-Stokes equations, we reduce training time by 62\%.
\end{abstract}

%%% Stotylines for the paper

%% Option 1: Motivate IC and get PR savings for free
% 1. We introduce the bilevel optimization in the general setting with the neural network in section 2
% 2. Then present the heat emulator learning problem and introduce the IC savings
% 3. Then, we argue that IC savings require K_max that is unknown a priori
% 4. We introduce PRDP to find K_max while training
% 5. We note that PRDP also gives the PR savings (as a bonus, kind of) which has it share similarities with Pedregosa
% 6. Then experimental section: We present Poisson inverse problem and show double-convex does not have IC savings, but we still get PR savings with PRDP (confirming Pedregosa; but in the different setting of discretized PDEs)
% 7. Then we can argue that since Pedregosa has a prescribed tolerance schedule (which also fully converges), it will never be able to find the IC savings, requiring the automatism that PRDP provides
% 8. Then we show multiple experiments (heat 1D with PRDP, heat 2D, Burgers & NS) which show both PR and IC savings with our method

%% Option 2: First show Pedregosa's PR savings apply to DP, then show DP also allow for IC savings when using NNs
% 1. Introduce bilevel optimization problem in reduced setting (like currently in section 2)
% 2. Present Poisson inverse problem, and do dumb scheduling (linear scheduling like in Kanishk thesis) -- maybe with one parameter -> Pedegrosa's PR savings apply to DP
% 3. Introduce more general compute graph with NN
% 4. Do 1D Heat emulator learning problem with dumb scheduling, see that there is no further improvement beyond K-max; conjecture IC savings
% 5. Finding K-max is hard: Introduce PRDP to get PR savings and automatically detect K-max (to get IC savings)
% 6. Do 3-param Poisson to show PRDP works also in double-convex settings, albeit without IC savings
% 7. Continue with 2D Heat, Burgers and Navier-Stokes to show broad-range applicability of PRDP to get both PR and IC savings


%% Arguments on the options
% - Option 2 makes our work be more strongly a derivative of what Pedregosa did
% - Option 1 could be harder to set up (although the emulator training also becomes a second motivational example for Option 2)
% - Option 2 might be less rewriting work because we can reuse parts we had for the Poisson example
% - Option 2 is also fine to do with the one-parameter Poisson example (less complicated to motivate)



\section{Introduction}
\label{sec:intro}

\emph{Differentiable Physics} is a paradigm which allows 
% neural networks 
learning algorithms 
to interact with gradients of classical physics solvers. 
% during training.
This has proven effective across many domains, e.g., solving inverse problems \citep{bendsoe2013topology},
% for training neural networks to replace solvers \citep{bar2019learning,brandstetter2021message}, 
integrating physical constraints \citep{raissi2019physics,li2024pino}, and
especially,
creating hybrid models that blend classical numerical techniques with learned components \citep{thuerey2020SIL,kochkov2021mlcfd,kochkov2024neural}. 
Despite their promise, neural-hybrid models for differential equations face limited adoption due to the computational cost of executing and differentiating through classical solvers during training. At the core of most 
classical solvers
% numerical methods 
for differential equations are iterative processes that can be tuned for accuracy, typically by adjusting parameters such as step size or iteration count. 
Traditionally, these methods prioritize achieving the highest possible accuracy 
in the physics solver. 
In contrast, our work takes a novel approach, drawing inspiration from bi-level optimization \citep{pedregosa2016}. Rather than focusing solely on maximum physics accuracy, we explore how numerical solvers can be strategically adjusted to substantially accelerate the training process without losing network accuracy.

\begin{wrapfigure}{o}{0.5\textwidth}
\vspace{-0.6cm}
    \includegraphics[width=\linewidth]{visualizations/mixed-chain-simplified.png}
    \caption{A neural network training pipeline using a differentiable physics solver $\mathcal{P}_K$. Black and red arrows show the forward and backward passes, respectively. As solver iterations $K$ grows, the cost of passes through $\mathcal{P}_K$ becomes severe.}
    \label{fig:differentiable-physics}
\vspace{-1.2cm}
\end{wrapfigure}
Our work applies to training pipelines involving differentiable physics, as exemplified in figure \ref{fig:differentiable-physics}. 
Repeatedly querying a solver with several iterations in the forward pass -- and differentiating through its iterations in the backward pass -- introduces a severe computational bottleneck during training.
Since deep learning is inherently based on noisy gradient estimates, 
% we argue that full refinement of the physics is not necessary to attain a good generalization.
we show that neither the physics nor the physics' Jacobian must be fully converged at training time to attain a good generalization. Using differentiable numerical solvers at a level significantly coarser than needed for tight tolerances and progressively refining it starting from an even coarser level is sufficient to achieve full accuracy of the network.
\begin{figure}
    \vspace{-0.5cm} % formatting
    \centering
    \includegraphics[]{plots/teaser_ns_spatial_sep27_time.pdf}
    \caption{
        \emph{Progressively Refined Differentiable Physics} (PRDP) reduces the training time of neural networks containing numerical solver components (c). The fidelity of iterative components is increased only if validation metrics plateau. This leads to savings by using fewer iterations in the beginning (PR savings in (b)) and by ending at a refinement level significantly below full fidelity (IC savings in (b)).
        % associated with all iterative solvers converged to \(\epsilon = 10^{-5}\).
        The achieved validation error is identical (a).
        \vspace{-0.5cm} % formatting
    }\label{fig:prdp_concept}
\end{figure}

% Numerical physics solvers with support for differentiation wrapped inside a neural network training, which we will refer
% to as \emph{differentiable physics}, can be 
% understood 
% as a type of bilevel optimization.
Differentiable Physics 
% , as in having neural networks interact with numerical physics solvers during training, 
can be understood as a type of bilevel optimization.
%understood as a typoe of bilevel optimization with a PDE solution wrapped inside a neural network training.
% . The inner optimization problem refers to the PDE solution, and the outer optimization represents the network training.
% The idea of balancing nested iterations is fundamental to the field of bilevel optimization.
In the context of hyperparameter optimization (a typical bilevel optimization problem), \citet{pedregosa2016} explored the idea of successively refining an inner solver to make increasingly accurate updates in an outer optimizer. Under strict assumptions of convexity, it was shown that with a summable sequence of refinement levels (in terms of increasingly tighter tolerances for both forward and backward solve), convergence in the low-dimensional hyperparameter space can be achieved.
% For hyperparameter optimization, the inner problem is the 
% fit of a machine learning model wrapped in the optimal choices of hyperparameters. 
We build upon this work but instead, treat the nonconvex learning of neural network parameters as an outer problem.
Our inner problem is the most elementary operation in any scientific computing, the solution of a linear system of equations from the discretization of a physical model. 
In this more general setting, we observe that we cumulatively reduce inner iterations not only by progressive refinement but also by ending the refinement at a level significantly coarser than needed for full physics convergence. This is possible because of the approximative nature of neural network training for which highly accurate gradients are not required. 
Similar approaches have been investigated for Deep Equilibrium Models \citep{deepEM}, which have nonlinear root-finding with dense Jacobians as the inner problem, e.g., by \citep{shaban2019truncated,Fung2021JFB,phantom_gradient}. In contrast, we focus on the sparse linear systems arising from the discretization of partial differential equations (PDEs) admitting special solution characteristics that have been unexplored in the context of deep learning. For realistically large and sparse linear systems of equations, the prevalent class of solution methods are
iterative linear solvers \citep{saad2003linsolve}. By controlling the number of iterations in these solvers (and during their backward passes), we can directly balance physics refinement and computational cost.
% While previous work has explored how incomplete convergence of linear solvers affects gradient accuracy
% % TODO: This might not be the right citation here, according to Kanishk they worked on a method to correct a gradient that comes from differentiating through an incompletely converged linear solver
% \citep{akbarzadeh2020}, little attention has been given to the impact of inaccurate gradients on the training of outer network parameters.
% We empirically show that significantly fewer linear solver iterations are necessary than correctness estimates on gradients suggest, indicating that networks trained with differentiable physics can generalize even with unconverged physics estimates.

Reducing the cumulative number of
% linear
solver iterations performed throughout the network training results in 
% very
considerable compute savings since linear solves typically dominate run times.
% This, in turn, results in very considerable compute savings. 
However, the optimal refinement schedule and the sufficient level of refinement are problem-dependent.
To automatically determine them during the network training, we present 
a novel algorithm, \emph{Progressively Refined Differentiable Physics}, in which the refinement of the physics is adaptively increased if a plateau in terms of validation metrics is encountered. This idea is visualized in \figref{fig:prdp_concept}.

Our experiments focus on efficiently training neural networks with differentiable linear solvers in the loop.
We address \emph{unrolled} as well as \emph{implicit} differentiation methods, showing that PRDP applies effectively to both.
The approach is tested on training tasks across a range of PDE problems, including the Poisson, heat diffusion, Burgers, and Navier-Stokes equations. 
Empirical insights into PRDP's behavior are presented for 1D problems, and its performance is validated through more complex 2D and 3D time-stepping problems.
We demonstrate its effectiveness in a variety of settings such as inverse problems, differentiable physics losses, and correction-based approaches.

In summary, our main contributions are the following.
\vspace{-1pt}
\begin{itemize}\setlength\itemsep{-1pt} 
    \item We empirically demonstrate that full network performance can be achieved with a coarse level of physics refinement, well below the typical refinement required for full convergence, leading to significant computational savings.
    \item We introduce the \emph{Progressively Refined Differentiable Physics (PRDP)} algorithm, which adaptively
    identifies the optimal level of physics refinement during training.
    \item We validate the effectiveness of PRDP across various differentiable physics learning scenarios, demonstrating its broad applicability. 
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Differentiating through Iterative Processes}
\section{Differentiating Iterative Linear Solvers}
\label{sec:preliminaries}


%A clear theoretical understanding of differentiating iterations can be illustrated 
A theoretical understanding of differentiating iterations can be gained
through the example of an inverse problem involving a parameterized linear system \(\mA \vu_h = \vb_h\).
Such systems arise in the numerical solution of discretized PDEs, where $h$ refers to the spatial discretization width \citep{ames2014poisson}.
For this, both the system matrix \(\mA\) and the right-hand side \(\vb_h\) arise from the assembly routines \(\Lambda(\theta)\) and \(\beta(\theta)\), respectively.
An iterative solver \(\Phi\) creates a sequence of guesses \(\{\vu_h^{[0]}, \vu_h^{[1]}, \dots \vu_h^{[K]}\} = \{\Phi^k(\vu_h^{[0]}; \mA,\vb_h)\}_{k=0}^K\) that should converge to the analytical solution \(\vu_h^{[K]} \approx \vu_h^* = \mA^{-1} \vb_h\), up to a given tolerance \(\epsilon\). We denote the number of solver iterations required to achieve this tolerance by \(K_{\epsilon}\).
% The solution of a (symmetric positive definite) linear system of equations is identical to a minimization of a convex quadratic optimization problem
% \begin{equation}
%     \text{solve}_{u_h} \; \{A u_h = - f_h \} \iff \arg \min_{u_h} \frac{1}{2}
%     %\|A u_h + f_h \|_2^2
%     u_h^T A u_h + f_h
% \end{equation}
Clearly, the set of parameters \(\theta \in \R^P\) affect the solution to this linear system of equations. Let \(\gP_K\) denote a function mapping from the parameters \(\theta\) to the 
% approximate 
iterative
solution of the linear system of equations \(\vu_h^{[K]}\) via first assembling the system matrix \(\mA = \Lambda(\theta)\) and the right-hand side \(\vb_h = \beta(\theta) \) and then employing the iterator \(\Phi\) for \(K\) steps. An example of the corresponding compute graph is shown in \figref{fig:poisson-inverse-setup} in the appendix.
The inverse problem is solved by performing an optimization over the parameter space, aiming to minimize the discrepancy against a reference solution \(\vu_h^r\) via
\begin{equation}\label{eq:bilevel-optimization}
    \min_\theta \quad L(\theta) = l(\vu_h(\theta)) = \frac{1}{2} \|\vu_h(\theta) - \vu_h^r\|_2^2 \quad \text{s.t.} \quad \Lambda(\theta) \vu_h = \beta(\theta).
\end{equation}
This forms a bilevel problem where the outer optimization concerns minimizing the loss \(L(\theta)\), while the inner optimization pertains to solving the linear system \(\gP_K\). 
%\footnote{We could add a footnote that iteratively solving a linear system is equivalent to iteratively solving a convex optimization problem?}.
Then, the loss function's evaluation can be explicitly written as \(L(\theta) = l(\mathcal{P}_K(\theta))\).

Solving the outer optimization using a first-order method requires the gradient \(\nabla_\theta L\).
% This chain of three function evaluations, (1) right-hand side assembly, (2) solving the linear system approximately,  and (3) computing the loss, constitutes the primal evaluation of a compute graph. 
We can differentiate this chained function using reverse-mode automatic differentiation (AD, \cite{griewank2008autodiff}) to find the (transposed) gradient as
\begin{equation}\label{eq:outer-gradient}
    (\nabla_\theta L)^T = \bar{\theta}^T =  \left( \vec{1}^T \; \underbrace{\mJ_l |_{\mathcal{P}_K(\theta)}}_{\text{primal inacc.}} \right)\; \underbrace{\mJ_{\mathcal{P}_K}|_{\theta}}_{\text{adjoint inacc.}}.
\end{equation}
Clearly, the quality of the gradient depends
% on the physics refinement level
on the number of iterator steps
% taken
\(K\). Inaccuracies in the physics operator \(\gP_K\) propagate to the loss gradient \(\nabla_\theta L\) through two sources: primal inaccuracy, i.e., the Jacobian of the loss function \(\mJ_l\) evaluated at the approximate solution \(\mathcal{P}_K(\theta)\), and adjoint inaccuracy, i.e., the Jacobian \(\mJ_{\mathcal{P}_K}|_{\theta}\) of the (approximate) iterative solver itself.

%%%%%%%%%%%%%%%%%%%%%

\subsection{Linear Solver VJPs}
\label{sec:linear-solver-vjp}

% \kanishk{We explain in this section how the differentiation of iterative solvers also consist aspects of iteration-based convergence.
% or
% Like iterative solvers, their differentiation too consists aspects of iterations and convergence. }
AD frameworks do not assemble the full Jacobian; rather, they employ vector-Jacobian products (VJP) to reverse-propagate the gradient information~\citep{pml2Book,edpbook}.
%\felix{(see ch. 6.2 of \citet{pml2Book} or the text book of \citet{edpbook})}.
% which saves compute when differentiating a scalar loss in reverse mode.
The programmatic implementation for the VJP of the loss function \(\bar{\vu}_h^T = \vec{1}^T \; \mJ_l |_{\mathcal{P}_K(\theta)} \) is straightforward to perform via AD. On the other hand, the VJP over the approximate physics \(\gP_K\) requires reverse propagation over the solver and the assembly routines, which we detail in appendix \ref{sec:linear-solve-differentiation-more-details}. Conceptually, there are two approaches.
% as depicted in \figref{fig:diff_iterative_process} \felix{in the appendix}.

\paragraph{Implicit Differentiation}\label{sec:implicit-diff}
The backpropagation over the iterative linear solve can be framed as the solution of another linear system 
% of equations 
in terms of an auxiliary variable \(\lambda\) with \(\mA^T \lambda = \bar{\vu}_h\).
This requires the transpose system matrix \(\mA^T\). Implicit differentiation over any kind of implicit relation gives rise to a linear solve with the system's linearized form being transposed. It can be derived automatically with AD tools \citep{Blondel2022modular}. Having a linear solve in the primal execution as well is a special case, as the same system matrix appears in the forward as well as the backward pass. 
Hence, it is reasonable to employ the same iterator in the VJP as in the primal,
% resolution
but with the transposed system matrix and the different right-hand side \(\Phi(\cdot \, ; \mA^T, \bar{\vu}_h)\).
% It again produces a set of iterates
% \(\{\bar{f}_h^{[0]}, \bar{f}_h^{[1]}, \bar{f}_h^{[2]}, \dots, \bar{f}_h^{[M]}\} = \left\{\bar{\Phi}^m\left(\bar{f}_h^{[0]}\right)\right\}_{m=0}^M\) starting at an initial guess \(\bar{f}_h^{[0]}\).
The number of iterates required to converge
can be different from the primal and provide a way to control the adjoint inaccuracy.
% another way of controlling the accuracy of the total gradient estimate \(\nabla_\theta L\).

\paragraph{Unrolled Differentiation} \label{sec:unrolled-diff}

Unrolled differentiation applies AD directly to an iterative program, treating each iteration as an individual computational step. It thereby accesses the VJP 
through
the iterator \(\Phi\) and 
accumulates
each iterate's contribution. This inherently requires access to all primal iterates. Since the AD engine unrolls as many iterations as in the primal pass, the adjoint accuracy is naturally coupled with the primal accuracy.
% Under the assumption that the iterator \(\Phi\) is contractive, the vJps converge to the physics vJp as \(K\) increases \citep{gilbert1992iterative}. 
% \kanishk{
% Since implicit differentiation avoids storing all iterates, it is often preferred over unrolling. Nevertheless, when differentiating linear system solves, unrolled differentiation is worth consideration due to its conceptual similarity to the implicit method - the unrolled Jacobian exhibits convergence as a function of the number of primal iterations unrolled by AD. Hence, similar to implicit differentiation, the unrolled Jacobian's accuracy can be controlled.
% Unlike implicit differentiation, however, providing a number of iterations for unrolling different from the number of primal steps is not trivial but possible with specific AD tools \citep{Bolte2023onsestepdiff}.
% }



%%%%%%%%%%%%%%%%%%%%%

% Training with
\subsection{Scheduling inner iterations}\label{sec:scheduling-inner-iterations}

Since the outer problem of \eqref{eq:bilevel-optimization} is solved iteratively, the gradient \(\nabla_\theta L\) can be coarse at the beginning of the 
outer iterations and only requires the highest fidelity towards the end \citep{pedregosa2016}. 
This fundamental idea was developed in the domain of hyperparameter optimization, and we demonstrate its application to physical inverse problems.
Consider the Poisson equation, which is a prototypical elliptic partial differential equation found in many areas of science and engineering.
% It describes phenomena such as electric field distributions and is also used as an intermediate step in incompressibility projections for fluid dynamics. 
% While various spatial discretization techniques exist, they ultimately all result in a linear system of equations, with the number of degrees of freedom \(N\) corresponding to the discretization of the domain
Most discretization techniques lead to a linear system of equations with \(N\) degrees of freedom
(for an example discretization, refer to \secref{sec:solver-details-poisson}).
\begin{equation}\label{eq:poisson-nd-and-discrete}
    \Delta u(x) = - p(x, \theta), \; x \in \Omega \subset \mathbb{R}^D \quad \implies \quad \mA \vu_h = \vb_h, \; \vu_h, \vb_h \in \R^N, \mA \in \R^{N \times N} 
\end{equation}
%Hence, this formulates 
% \nt{This represents} a bilevel problem as expressed in \eqref{eq:bilevel-optimization}. 
We consider a setting on the one-dimensional unit interval \(\Omega = (0, 1)\) with homogeneous Dirichlet boundary conditions, \(u(0) = 0 = u(1)\). Assuming the parameter space is one-dimensional, we design the right-hand side as \(p(x, \theta) = \theta \sin(2 \pi x)\) and discretize it on the domain.
The outer optimization problem over \(\theta \in \R\) can be exactly solved in this case. 
%as we produce the reference \(\vu_h^r\) from a \(\theta^r\) but with a fully resolved physics operator \(\gP_{K_\epsilon}}\). \kanishk{[todo correction - we used a direct solver to generate \(\vu_h^r\)]}

%\begin{figure}
\begin{wrapfigure}{o}{0.5\textwidth}
    \centering
    \includegraphics[]{plots/pr_savings_poisson_1_param_jacobi_unrolled.pdf}
    \caption{Progressively refining the differentiable physics during 
    % training
    outer optimization of a Poisson inverse problem
    achieves full convergence of the parameter with fewer cumulative iterations of the physics solver, leading to \emph{progressive refinement} (PR) savings.}
    \label{fig:poisson_1_pr-savings}
\end{wrapfigure}
%\end{figure}
%
We employ a Jacobi scheme to solve the linear system
which converges to \(\epsilon = 10^{-5}\) within \(K_\epsilon = 600\) iterations; so does its unrolled Jacobian. Under a gradient descent optimizer in the parameter space of \(\theta\), it requires a total of \(125\) outer iterations to bring the relative parameter suboptimality against \(\theta^r\) also down to \(10^{-5}\).
By using simple scheduling that starts the outer optimization at a coarse inner resolution of \(K=25\), and progressively increases the inner iterations by \(\Delta K = 6\) in every outer iteration, all the way to \(K_\epsilon\), we can reduce the overall computational cost of the optimization while achieving the same parameter suboptimality. These results are visualized in \figref{fig:poisson_1_pr-savings}.
Despite requiring slightly more outer iterations, 
fewer inner iterations are necessary. This results in an overall reduction of \(28 \%\) of the total number of inner iterations.
These savings represent one component of the final cost savings. As they are obtained due to scheduling fidelity of the physics solve, we denote them as \emph{progressive refinement} (PR) savings. 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{plots/inner - SD - varyinginit - theta_ref = [0.62 1.86 5.1 ].png}
%     \caption{Primal and Physics Jacobian convergence} (todo: remove cases with non-zero init and put primal and Jacobian convergence on a single plot.)
%     \label{fig:poisson-three-param-convergence}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{plots/outer contours - SD - unrolled jac - theta_rel_error - lr=2500.png}
%     \caption{Parameter suboptimality under bilevel optimization of different iteration count combinations. Optimizations with a fixed number of iterations \(K\) must be understood as vertical lines.}
%     \label{fig:poisson-three-param-contour}
% \end{figure}



\subsection{Network Training under Incompletely Converged Differentiable Physics}\label{sec:motivate-ic-savings}

Going beyond simple convex inverse problems, more sophisticated compute graphs arise. For example, assume 
the linear system is assembled from a prior variable that is given as the output of a neural network \(\vg_h := \vf(\cdot\, ; \theta\)) (ignoring the input to the network for now). In this case, we have \(\mA = \Lambda(\vg_h)\) and \(\vb_h = \beta(\vg_h)\). Hence, the optimization over \(\theta\) turns into the nonconvex learning problem in the weight space of the neural network. 
The extended reverse-mode AD operation reads
\begin{equation}\label{eq:outer-gradient-extended}
    (\nabla_\theta L)^T = \bar{\theta}^T =  \left(\left( \vec{1}^T \; \mJ_l |_{\mathcal{P}_K(\vf(\cdot\, ;\theta))} \right)\; \mJ_{\mathcal{P}_K}|_{\vf(\cdot \, ;\theta)}\right) \mJ_\vf |_{\theta}.
\end{equation}
The approximate solution to the linear system of equations using \(K\) steps and its differentiation remain the sources of gradient inaccuracy. Yet, the effect of the neural network alters the \emph{solution characteristics} of the iterative linear solver through the assembly of the system matrix (influencing its spectrum) and right-hand side.
% \footnote{While most theoretical analyses on the convergence of iterative linear solvers are related to the spectrum or the condition number of the system matrix \(\mA\) (we present convergence rates for the used solvers in \secref{sec:linear-solvers}), this only affects asymptotic convergence. Early convergence is also influenced by the composition of the right-hand side, i.e., how it could be expanded in terms of the eigenvectors of the system matrix \citep{saad2003linsolve}.}
% If it aligns with the eigenvectors associated with high eigenvalues, convergence typically is fast. However, since the network throughout its training can produce a range of outputs, convergence can be greatly different.
% Since network training can produce a wide range of different right-hand sides, effective convergence is influenced by both the assembled system matrix and the produced right-hand side.
% Hence, even for examples in which the system matrix is not parameterized, effective convergence is still influenced by the neural network prediction. Conversely, this leads to inaccurate gradients ...

% \felix{Conversely, we make the central observation that neural network training also converges with a slightly inaccurate gradient. We attribute this to the orientation of the gradient converging more quickly than its magnitude. However, modern gradient-based optimizers like ADAM \citep{adam} normalize gradient magnitude anyway, rendering a correct length estimate obsolete. [Plus noisyness and stochastic minibatching]}


%%% Below is the most recent version but which is no longer correct due to me (Felix) figuring out that we the angle/direction does not really converge faster
% Conversely, we observe that neural network training often performs equally well even when gradients are slightly inaccurate. Specifically, the \emph{direction (angle) }of the gradient converges faster than its \emph{magnitude}. This is crucial because many modern optimizers, such as ADAM \citep{adam}, are momentum-based and normalize the gradient's magnitude during optimization. Thus, having an accurate gradient magnitude is less critical as long as the angle is correct. 
% This can allow us to utilize an incomplete convergence of the linear solver \nt{that produces the gradient. Instead, we rely on an approximately correct gradient direction while relaxing} the need for precise magnitude.
% %
% Moreover, neural networks are trained with stochastic minibatches, where the loss and gradient landscape change between batches. This implies that deep learning does not require perfect gradients at every iteration, as the global gradient landscape is inherently noisy. Consequently, a lenient tolerance for convergence in the physics solve can still lead to effective training, as the network benefits more from frequent updates with approximate gradients than from fewer updates with perfectly accurate gradients.

%On the other hand, the nature of neural network training might allow inaccurate gradients throughout the entire training, requiring 
%lower physics tolerance than \(\epsilon\) in the first place.
%On the other hand, 
% \nt{We make the central observation that the stochastic nature} of neural network training means a more lenient refinement for the required physics exists below which no gains in training performance are achievable.
% \felix{Moreover, it is reasonable to believe that the refinement in the gradient over the the number of iterations \(K\) is first noticeable in the direction and lastly in the magnitude. However, since modern optimization methods like the ADAM optimizer \citep{adam}, which we use, use momentum estimates to identity reasonable step lengths anyway, it is of lesser importance to have to correct magnitude [refine wording].}

Conversely, we hypothesize that neural network training can also work under approximate gradients.
To illustrate this, consider the one-dimensional heat equation on a periodic unit interval with a time-implicit discretization
\begin{equation}\label{eq:heat-1d-and-discrete}
    \frac{\partial u}{\partial t} = \nu \frac{\partial^2 u}{\partial x^2}, \; x \in (0, 1)
    \implies (\mI - \nu \Delta t \, \gL_1) \vu_h^{[t+1]} = \vu_h^{[t]},
\end{equation}
where $\gL_1$ represents the matrix from the spatial discretization of the second derivative (see \secref{sec:solver-details-diffusion-1d} for more details). The physics operator now advances from one step to the next \(\vu_h^{[t+1]} = \gP(\vu_h^{[t]})\).\footnote[2]{Throughout this work, we use superscripts in square brackets to denote sequence entries, for example for temporal snapshots or iterates of a linear solver.}
Consider the scenario in which a neural network performs a first prediction in time from an initial state, i.e., \(\vu_h^{[1]} = \vf(\vu_h^{[0]}; \theta)\), followed by the physics operator involving \(K\) iterator steps which provides the second time step solution \(\vu_h^{[2][K]} = \gP_K(\vu_h^{[1]})\). 
The loss in \eqref{eq:bilevel-optimization} is computed against a reference given by applying the converged physics operator twice \(\vu_h^{[2],r} = \gP_{K_\epsilon}^2(\vu_h^{[0]})\). Hence, the network is trained to emulate one application of $\gP$.
We solve the linear system of equations using the Jacobi method (\ref{sec:jacobi}), which converges within \(K_\epsilon = 25\) iterations. The implicit differentiation's linear solve requires equally many iterations. 

\begin{wrapfigure}{o}{0.5\textwidth}
\vspace{-0.41cm}
    \includegraphics[width=\linewidth]{plots/ic_savings_heat_1d_sep29_jacobi_implicit_time.pdf}
    \caption{Network accuracy does not improve beyond a refinement level of differentiable physics ($K_\text{max}$) significantly lower than full convergence ($K_\epsilon$) constituting \emph{incomplete convergence} (IC) savings.}
    \label{fig:ic-savings_heat-1d}\vspace{-0.21cm}
\end{wrapfigure}
%
Figure \ref{fig:ic-savings_heat-1d} shows the results of multiple network training runs performed at different levels of physics refinement, i.e., different values of \(K\) that are kept constant for each training run.
We observe that beyond a refinement threshold \(K_{\text{max}}\), no further improvements of the neural emulator's performance on a validation metric are noticeable. 
Hence, we conclude that networks can attain a high level of accuracy, even 
when trained through \emph{incompletely converged} (IC) physics.
As explained in \eqref{eq:outer-gradient}, this incomplete physics convergence subsumes coarseness of its Jacobian.
In the example of the heat emulator, we obtain \(K_{\text{max}} = 10\), which reduces the number of physics solver iterations for training by \(60 \%\). We denote this second type of reductions as \emph{IC} savings.

While we could not isolate a single cause for these IC savings, we hypothesize that it is likely due to a combination of factors: (1) the more critical components of the gradient may converge more rapidly than less important ones, (2) the inherent noisiness of neural network training due to stochastic mini-batching, (3) the usage of momentum-based optimizers, and (4) the inherently approximative nature of machine learning models. 
Taken together, these factors presumably reduce the need for full convergence in the differentiable physics solver, resulting in very substantial cost savings.
%These factors presumably together reduce the need for full convergence in the differentiable physics solver.


% in multiple ways. First, the system matrix is assembled based on its output, affecting the spectrum, which is typically the greatest factor of convergence behavior. Second, even if the problem had a constant system matrix, the effectively observed convergence behavior still depends on the right-hand side, which is influenced by the network. Third, the network Jacobian \(\mJ_\vf|_\theta \) modifies the incompletely converged gradient further. Moreover, with neural networks, we have stochastic optimization problems with slightly varying gradients of minibatches. Hence, a too-accurate treatment might be superfluous.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Progressively Refined Differentiable Physics}\label{sec:prdp}

The inner iterations saved by progressive refinement (PR) and incomplete convergence (IC) can greatly reduce training time without impeding
% network
accuracy.
However, a suitable refinement schedule and \(K_{\text{max}}\) are unknown a priori. They depend on the PDE, its discretization (given by \(\Lambda\) and \(\beta\)), the iterative linear solver, and the learning dynamics.

% [Differ from Pedregosa, no longer convex, and cannot used pre-defined tolerance schedule, but need something that works on the fly]. [On Top: pedregosa is only after the PR savings not after the IC savings.]


% Given these findings, we concluded that initiating the training with coarser physics leads to an initial coarse convergence of the outer problem, and successive refinement of the physics and its Jacobians subsequently steers the outer convergence to a higher accuracy through increasingly precise gradients.
% We propose an algorithm that queries the network accuracy during training to identify the sufficient level of physics refinement at every epoch. Starting from a coarse level of refinement, training \felix{proceeds while the physics of the inner problem is refined up until it reaches a level significantly below the level of refinement needed for full inner primal convergence.}
% progresses
% until an initial coarse convergence of the outer optimizer is reached. Then, the physics is refined, and training continues. Interchangeably, training and refining
% progresses until the network accuracy plateaus. This level is accepted as the sufficient level of physics refinement, and the network accuracy achieved is on par with training through fully-converged physics at a fraction of the cost.

To arrive at a practical method, we
automate the progressive refinement and the detection of \(K_{\text{max}}\)
based on the observation that training progress in terms of a validation metric stagnates 
when the physics inaccuracy is too large. 
This stagnation corresponding to different physics refinement levels is exemplified 
in figures \ref{fig:poisson_1_pr-savings} and \ref{fig:ic-savings_heat-1d}.
Conversely, we track training progress over time to automatically increase physics refinement by \(\Delta K\) linear solver iterations when training plateaus. Typically PRDP starts at \(K_0 = 1\).

\begin{wrapfigure}{o}{0.3\textwidth}
\vspace{-0.41cm}
    \includegraphics[width=\linewidth]{visualizations/prdp_explainer.png}
     \caption{Top: the typical training progress of a neural network supported by PRDP, showing the ratios $r$ and $r_c$. Bottom: a simplified flowchart representation of the PRDP control algorithm.}
    \label{fig:prdp_explainer}
    \vspace{-1cm}
\end{wrapfigure}


\paragraph{Controlling Refinement} \label{sec:controlling_refinement}
Given an exponentially smoothed history of validation metrics \(\{\tilde{L}^{[e]}_{\text{val}}\}_e\) after 
a representative training interval,
for which we can either use epochs or a fixed number of update steps, we distinguish three trends:

\begin{enumerate}
    \item \textbf{Validation metric plateaus}: If the ratio of the latest validation error and the value of several grace
    % epochs
    intervals
    \(\delta\) earlier, \(r = \tilde{L}^{[-1]}_{\text{val}} / \tilde{L}^{[-\delta]}_{\text{val}}\), is above a threshold
    \(r > \tau_{\text{step}}\), it indicates that the network has achieved the highest possible accuracy at the current refinement level. Then, subject to the following checks, the refinement is increased by $\Delta K$. We record the validation metric before refinement as a checkpoint \(c \leftarrow \tilde{L}^{[-1]}_{\text{val}}\). Identifying this trend leads to full utitilization of each refinement level and contributes the PR savings.
    
    \item \textbf{Adequate refinement is achieved}:
    We compute the improvement of the current plateaued validation metric against the checkpoint, \(r_c = \tilde{L}^{[-1]}_{\text{val}} / c \).  If the 
    % improvement is \emph{not} less 
    stagnation is greater
    than its threshold, i.e.,  \(r_c > \tau_{\text{stop}}\), the refinement level is retained. 
% until the training finishes.
    This identifies the
    % maximum
    adequate
    level of refinement necessary, \(K_{\text{max}}\), enabling the IC savings.
    
    \item \textbf{Initially divergent regime}: For some scenarios, there is a minimum number of iterations \(K_{\text{min}}\) which if \(K < K_{\text{min}}\) lead to the convergence of the network to a worse value than its initialization, e.g., for \(K=1\) and \(K=2\) in \figref{fig:ic-savings_heat-1d}. To overcome this, we also refine if \(r_c > 1.0\).
\end{enumerate}

% Noting down the Kmin story here in case we plan to have it in later: 
% 1. There is a Kmin below which training diverges. 
% 2. PRDP takes care of it by refining immediately when divergence is detected. 
% 3. the epochs performed with K < Kmin are in a way "wasted" because the network doesn't become any better than its init. 
% 4. In our tests, we found that this waste was insignificant

% Either way, all employed linear solvers are outfitted with a convergence check. For the unlikely scenario that at some point the linear solver's convergence is achieved before \(K_{\text{max}}\), it terminates earlier. Yet, we never observed this in our experiments [except for implicit diff of the three-param Poisson example, right?]

Figure \ref{fig:prdp_explainer}  illustrates the basic concept of the PRDP control algorithm. The exact algorithm is detailed in pseudocode in Algorithm \ref{alg:prdp}.
Its implementation in a training pipeline that uses differentiable physics is represented by the \texttt{should\_refine} function in listing \ref{lst:prdp_applied}.

\begin{listing}
\vspace{-0.6cm}
    % \centering
    \begin{minted}[frame=lines,
                   framesep=2mm,
                   fontsize=\scriptsize,
                   ]{python}
    def update(model, state_in, state_out, inner_iterations):
        # neural network gives first time step, physics gives second time step
        state_1 = model(state_in)
        state_2 = physics(state_1, inner_iterations)
        loss, grad = value_and_grad(loss_fn)(state_2, state_out)
        model = update_model(model, grad)
        return model, loss
    
    inner_iterations = 1
    val_loss_history = [evaluate(model, val_dataloader))]
    for epoch in range(epochs):
        for state_in, state_out in dataloader:
            model, loss = update(model, state_in, state_out, inner_iterations)
        val_loss_history.append(evaluate(model, val_dataloader))
        if should_refine(val_loss_history):
            inner_iterations += 1
    \end{minted}
    \caption{A typical mixed-chain learning pipeline (\figref{fig:differentiable-physics}) as used in our neural emulator learning experiments. Training begins with coarse physics, using, e.g., 1 solver iteration. The \texttt{should\_refine} function applies PRDP, determining when to refine and progressively increasing the inner solver iterations during training.}
    \label{lst:prdp_applied}
\end{listing}


\paragraph{Applying Refinement}
Intrinsically, PRDP pertains to refinement of the primal physics.
As we showed in section \ref{sec:linear-solver-vjp},
the adjoint refinement is controlled based on the kind of differentiation used.
In \emph{implicit differentiation,}
an additional linear system solve is performed via a custom differentiation rule. 
This approach inherently decouples the convergence of the primal and its Jacobian. We maintain their coupling by using the same solver and number of inner iterations for the primal solve and the VJP solve. This setup ensures that adjustments to the number of inner iterations consistently affect the primal and the VJP. 
When using \emph{unrolled differentiation}, progressive refinement of the primal trivially extends to progressive refinement of the Jacobian. This is because the number of iterations unrolled by the automatic differentiation engine is the same as that of the primal solve.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}\label{sec:experiments}
% 2-3 pages?

\begin{figure}
% \vspace{-0.4cm}
   \includegraphics[width=\linewidth]{plots/training_all.pdf}
   \caption{PRDP achieves the same network accuracy as fully converged physics (baseline in dark purple). It does so by adjusting physics accuracy adaptively over the training if validation performance plateaus, which is particularly noticeable in Heat 2D.}
    \label{fig:outer}
\end{figure}

To validate our approach across problems of varying complexities, a combination of different learning and physics scenarios are chosen. This includes the convex optimization of an exactly solvable inverse problem with the Poisson equation as described in \secref{sec:scheduling-inner-iterations}, the learning of autoregressive neural emulators in the spirit of \citet{bar2019learning} and  \citet{brandstetter2021message} for the diffusion and Burgers equation but with a loss setup involving differentiable physics, and third, the learning of a neural-hybrid emulator in a setting similar to \cite{thuerey2020SIL}.
When training neural networks, our results show the aggregation over ten different initialization seeds. The validation metric used is the normalized mean squared error (nMSE) over the validation dataset. Further specifics are provided in appendix \ref{sec:experimental-setups}.

% Experimental results in numbes are collected in this sheet: https://docs.google.com/spreadsheets/d/1QR0UdSLKdEXsiy3RN1sOzEpBUjLW4P5wtFpmtPe3QU8/edit?gid=1754499831#gid=1754499831

\subsection{Linear Inverse Problem}\label{sec:experiments-poisson}

In \secref{sec:scheduling-inner-iterations}, we demonstrated the potential progressive refinement savings on a doubly-convex inverse Poisson problem when employing a \emph{pre-defined} linear scheduling of inner iterations across outer iterations. This schedule was hand-tuned based on exhaustive runs. PRDP is designed to 
be problem-independent and 
adjust the inner iterations adaptively. To confirm this, we apply PRDP in the same setting and achieve a saving of \(33\%\) due to progressive refinement. This is slightly higher than the manually achieved \(28\%\).
\begin{wrapfigure}{o}{0.55\textwidth}
    \centering
    \begin{tabular}{c c c|c c c}
    \toprule
    P & Type & Diff & Cost & Red. & PR. Sav.\\
    \midrule
    1 & Jac & Unr & 75K & 24.87K & 33\% \\
    1 & Jac & Impl & 75K & 24.87K & 33\% \\
    \midrule
    3 & Jac & Unr & 337.5K & 163K& 48\% \\
    3 & Jac & Impl & 337.5K & 163K& 48\% \\
    3 & SD & Unr & 192K & 82.7K& 43\% \\
    3 & SD & Impl & 192K & 82.7K& 43\% \\
    \bottomrule
    \end{tabular}
\end{wrapfigure}
Moreover, we extended this inverse problem to a three-dimensional parameter space, with each entry scaling the first three eigenmodes, and covered a combination of different setups, including the steepest descent solver (SD), and implicit differentiation (Impl) next to unrolled differentiation (Unr). The results presented in the table to the right show that PRDP universally applies to each combination and is, hence, agnostic to the linear solver and the differentiation method.
The qualitative behavior of all combinations is displayed in appendix \figref{fig:poisson_all-results}. For brevity, we only list the PR savings achieved using PRDP by displaying the cost of optimization with \(K_{\epsilon}\) and the corresponding reduction (Red.). Importantly, under the larger parameter space within each of the four setups, PRDP works equally well, achieving PR savings of \(48\%\) and \(43\%\) for the Jacobi method and the steepest descent solver, respectively.
% \nt{the algorithm is} capable of producing savings of 28\%. This is similar to the manual schedule as depicted in \figref{fig:cost_poisson_1param}. 
% \felix{It must be noted that there cannot be IC savings in this case.}
% Moreover, it works across different iterative linear solvers (both Jacobi and Steepest Descent) and with both implicit and unrolled differentiation. 

% Going further, when extending the parameter space to be three-dimensional and having the right-hand side being defined as \(p(x, \theta) = \theta_1 \sin(2\pi x) + \theta_2 \sin(4 \pi x) + \theta_3 \sin(6 \pi x)\), it works equally well. \ntcomment{where is this shown?}

% \felix{Since the qualitative convergence behavior of PRDP is similar to the pre-defined schedule shown \figref{fig:poisson_1_pr-savings}, we only present the PR savings achieved by PRDP. [TODO?] A representative set of convergence plots can be found in the appendix.}

% -> We also see that it is the same to schedule via iterations as scheduling with tolerances \kanishk{(figure \ref{fig:residuum_and_iterations})} and that residuum tolerances imply convergence in suboptimality ].

% Training with coarse physics - (ref figure \ref{fig:outer}) outer training converges to different parameter values depending on the level of physics refinement. We hypothesize that the level of refinement
% in the physics governs a horizontal shift in the loss landscape (maybe we can skip this "shift" argument?). This shift leads the outer optimizer to select parameter values that compensate for the lack of physics resolution. These values successfully minimize the training loss over the training set, but don't generalize to the validation set. We use this extent of generalization is what we use as a measure of training performance. 

\subsection{Linear Neural Emulator Learning}\label{sec:experiments-heat}


We train a multilayer perceptron (outer problem) to function as an autoregressive emulator, i.e., a replacement for the numerical time stepper of a heat diffusion equation. Following the \emph{differential physics} paradigm, the numerical heat equation solver (inner problem) is included in the gradient loop during training. This training pipeline is depicted in figure \ref{fig:differentiable-physics}.
\begin{wrapfigure}{o}{0.66\textwidth}
% \vspace{-0.6cm}
    \centering
    \includegraphics[width=0.21\textwidth]{plots/cost_heat_1d_sep29_jacobi_implicit_time.pdf}
    % \hfill
    \includegraphics[width=0.19\textwidth]{plots/cost_heat_2d_sep17_jacobi_unrolled.pdf}
    % \hfill
    \includegraphics[width=0.19\textwidth]{plots/cost_heat_3d_nov21_jacobi_unrolled_time.pdf}
    \caption{Total Inner iters in Heat 1D, 2D \& 3D.}
    \label{fig:cost_heat-1d}
\vspace{-0.31cm}
\end{wrapfigure}
% A central aspect to show was that savings due to \emph{incomplete convergence} can be achieved and that they are likely explained by the nature of deep learning.
In \secref{sec:motivate-ic-savings}, using a 1D case of the same setup, we demonstrated that savings due to \emph{incomplete convergence} are actually achievable, and that they are likely caused by the nature of the training in deep learning.
We identified \(K_{\text{max}}\) based on an exhaustive search of different \(K\) values. This is infeasible in practice. Hence, PRDP is designed to automatically find the upper refinement threshold and prevent further superfluous refinement. In \figref{fig:outer}(a), (b) \& (c), this can be seen as its convergence follows certain refinement levels. 
We can confirm that the same \(K_{\text{max}} = 10\) is found by PRDP constituting the aforementioned \(60\%\) IC savings. Moreover, as \figref{fig:cost_heat-1d}(a) reveals, the total PRDP savings are \(79\%\) against the fully converged run with \(K_\epsilon=25\) because PRDP additionally contributes \(19\%\) savings due to \emph{progressive refinement}. 
We repeated a similar experiment in two and three dimensional settings. In the 2D case, the difference between \(K_{\epsilon}=43\) and \(K_{\text{max}}=12\) constitutes \(72\%\) IC savings. Together with \(14\%\) PR savings, this totals \(86\%\) savings due to applying PRDP as visualized in \figref{fig:cost_heat-1d}. 
Similarly training a ResNet \citep{he2016resnet} in the 3D case, the total savings due to PRDP were 81\%. This corresponded to a reduction in total training time by \(24\%\) and \(78\%\) in the 2D and 3D cases, respectively, which underscores PRDP's potential for large computational savings in more difficult, higher-dimensional settings.


\subsection{Nonlinear Neural Emulator Learning}\label{sec:experiments-burgers}

\begin{wrapfigure}{o}{0.22\textwidth}
% \vspace{-1.5cm}
    \centering
    \includegraphics[width=\linewidth]{plots/cost_burgers_sep23__ndof_256__modes_20__residuums.pdf}
    \caption{Total inner iters in Burgers.}
    \label{fig:cost_burgers}
\vspace{-0.31cm}
\end{wrapfigure}

% So far, we have investigated linear PDEs where only the system's 
The preceding tests constituted inner problems where only the linear system's
right-hand side was dependent on the trainable parameters.
We expect our PRDP algorithm to work equally well when training through inner problems with a non-constant matrix assembly function \(\Lambda\). To illustrate this, 
we train a ResNet to function as an autoregressive emulator for
the one-dimensional Burgers equation on a periodic unit interval with a time-implicit upwinding discretization. Additionally, this yields a non-symmetric matrix requiring the more sophisticated \emph{GMRES} solver \citep{gmres1986}. We use a linearization around the previous point in time, resulting in an Oseen problem \citep{turek1999efficient}.
When training a ResNet under a similar mixed scenario as in section \ref{sec:experiments-heat}, we can again confirm the working of PRDP in this case. It amounts to \(30\%\) savings by incomplete convergence reducing \(K_\epsilon=23\) to \(K_{\text{max}}=16\). An additional \(29\%\) PR savings yield a total of \(59\%\) PRDP savings.
% , yielding an improvement of xx\% in inner iterations.
The Burgers emulation problem was particularly challenging, requiring us to set \(K_0 = 4\) to overcome a strongly divergent regime if the physics was too coarse at the beginning of training. This is noticeable in the initial epochs in \figref{fig:outer}(c). This figure also shows that since the fanning between the different (convergent) refinement levels is not as strong as before, the IC savings are lower. However, we still see significant savings due to progressive refinement.

\subsection{Neural-Hybrid Emulator for the Navier-Stokes Equation}\label{sec:experiments-navier-stokes}



\begin{wrapfigure}{}{0.5\textwidth}
% \vspace{-0.41cm}
    \includegraphics[width=\linewidth]{visualizations/correction-learning-simplified.png}
    \caption{The training (top) and evaluation (bottom) pipelines for neural-hybrid emulator.}
    \label{fig:correction-learning}
% \vspace{-0.61cm}
\end{wrapfigure}
Tightly combining classical numerical solvers and neural networks is a promising research direction \citep{kochkov2021mlcfd,thuerey2020SIL}. 
% \citep{kochkov2024neural}
It differs from our previous experiments in that the solver is not just part of the training compute graph but will also be executed during the inference of the model. 

We mimic the setup of \citet{thuerey2020SIL} to have a neural component correct the \emph{discretization error} of an incompressible Navier-Stokes solver by training it against a reference produced at a higher resolution of discretization. The training and evaluation setups are shown in \figref{fig:correction-learning}. The outer and inner problems correspond to the network training and the Navier-Stokes solver in the training loop, respectively.
Similarly to the Burgers example, we choose an upwind-based discretization with the linearization around the previous step in time. The additional incompressibility constraint leads to a saddle point structure \citep{turek1999efficient}. 
We choose to solve it in a coupled form with the GMRES algorithm \citep{gmres1986}.

\begin{wrapfigure}{o}{0.3\textwidth}
\vspace{-0.41cm}
    \centering
    \includegraphics[width=0.22\textwidth]{plots/cost_ns_spatial_sep27_time.pdf}
    \caption{Total inner iters in Navier Stokes.}
    \label{fig:cost_navier-stokes}
% \vspace{-0.61cm}
\end{wrapfigure}

In this complex scenario, PRDP likewise gives substantial benefits:
It is able to save \(65\%\) inner iterations due to incomplete convergence and \(16\%\) iterations in progressive refinement. Ultimately, reducing the number of iterations by a total of \(81\%\) results in a reduction in wall clock training time by 62\%, as shown in \figref{fig:prdp_concept}(c).
Since neural-hybrid approaches execute the fully converged physics solver during inference and consequently during validation, PRDP's reliance on a validation metric necessitates this additional cost. In our experiments, we found that compared to training with fully converged physics without computing the validation loss, PRDP was still faster by 57\%.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}

\paragraph{Differentiation through implicit relations}
% \felix{The note of \citet{griewank2012invented} lists potential origins of reverse-mode automatic differentiation and highlights the importance of early work in optimal control in the 1960s (also see ch. 6.6 of \citet{goodfellow2016deep}).}
% While this branch of science shares our goal of differentiating through the solution process of partial differential equations \felix{(which can be considered an \emph{implicit relation})}, its early application mostly involved the usage of continuous adjoint methods, avoiding the autodifferential treatment of iterative processes.
\citet{fischer1991linsys} established an implementation framework for \emph{unrolled differentiation}
and investigated the convergence of the derivative, focusing on linear solvers.
% The paper proved that Jacobi-iteration for solving linear systems is derivative-stable, i.e.,
% the derivatives of the iterates converge to the derivative of the fixed point of the iterations.
\citet{gilbert1992iterative} extended this work to a broader class of iterative processes.
As an alternative to unrolled differentiation, \citet{christianson1998reverseimplicit} derived the \emph{implicit differentiation} rules over linear solves.
% , which can be viewed as an intermediate between full unrollment and a continuous adjoint treatment.
Beyond linear systems, implicit differentiation has since gained prominence in the machine learning community, particularly for hyperparameter optimization \citep{bengio2000gradient} and other \emph{bilevel optimization} (BLO) applications that require differentiating through inner optimizations, such as deep equilibrium models \citep{deepEM} and meta-learning \citep{andrychowicz2016learningtolearn}.
We view differentiable physics \citep{thuerey2021pbdl} as another type of BLO with the specialty of sparse linear systems.
The practicality of implicit differentiation is largely due to its reduced memory footprint and lower reverse-mode computational cost, especially as modern automatic differentiation (AD) tools can automatically handle the necessary propagation rules \citep{Blondel2022modular}. Additionally, implicit differentiation allows for the black-box use of solver implementations, enabling the integration of third-party components into differentiable computational graphs \citep{giles2008collected}. However, unrolled differentiation remains an active research area, with work focusing on non-asymptotic analyses \citep{curse} and other aspects \citep{maclaurin2015gradient,franceschi2017forward,Grazzi2020,ji2021bilevel}.

\paragraph{Analysis and cost-reduction of bilevel optimization}

To address the computational cost of bilevel optimization,
\cite{Fung2021JFB} presented \emph{Jacobian-free backpropagation}, where the Jacobian of the implicit solver is approximated as the identity, eliminating the need for adjoint linear solves. \cite{phantom_gradient} introduced \emph{phantom gradients}, where the matrix inversion is replaced with an approximate inverse, and provided theoretical guarantees on the convergence of stochastic gradient descent as the outer problem. \citet{lorraine2020optimizing} approximates the implicit linear solves with a reduced number of conjugate gradient steps.
Moreover, \citet{shaban2019truncated} and recently \citet{Bolte2023onsestepdiff} discuss unrolled differentiation through only a reduced number of iterations. All the aforementioned works consider (what we call) \emph{incomplete convergence} (IC) savings, albeit in the setting of deep equilibrium models \citep{deepEM} or hyperparameter optimization \citep{feurer2019hyperparameter}.
% \felix{and by only manipulating the adjoint inaccuracy of \eqref{eq:outer-gradient}.}
% \felix{However, they typically do not schedule refinement over network training.}
On the other hand, \cite{pedregosa2016} studied
approximate hypergradients by adjusting the tolerances of inner primal and implicit linear solves following a pre-defined schedule.
This work proves (what we call) \emph{progressive refinement} (PR) savings.
% , albeit in strict convex settings \felix{for which IC savings cannot exist}.
Prior to this, \citet{domke2012approxunrolled}
investigated outer training with unrolled AD through incompletely converged
iterations, specifically for gradient descent, heavy ball, and L-BFGS methods as inner optimizers. They note the advantage of implementing incomplete convergence using the number of inner iterations rather than inner tolerance.
Our approach uniquely combines PR and IC savings -- through both unrolled and implicit differentiation -- targeting iterative solvers for sparse linear systems embedded within neural network training. To our knowledge, no prior work has applied these techniques in this context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Limitations and Outlook}
\label{sec:future}
% 0.5 page

\paragraph{Limitations}

While PRDP is designed to work across a range of differentiable physics settings, there are, of course, way more potential linear systems that can arise, all with their specific characteristics. Albeit we believe that the approach using scheduling over iterations rather than scheduling via tolerances might be more generally applicable, it remains to be tested how PRDP applies to unstructured discretizations in higher dimensions potentially also involving multiple physics. PRDP is limited to settings that involve iterative linear solvers. As such, it can not be used for purely explicit numerical solvers (e.g., found in strongly hyperbolic problems) or when the linear systems are solved spectrally \citep{kochkov2024neural}. However, most other simulations in science and engineering entail linear solvers either due to more efficient implicit time stepping or when solving steady-state problems. Moreover, whenever dynamics are constrained, e.g., the incompressible Navier-Stokes equations, even if purely explicit time stepping is used, iterative processes are required to resolve the constraints.
% Moreover, if several applications of the approximate solver \(\gP_K\) are part of the compute graph -- so far, we investigated a maximum of two in the unrolling of the Navier-Stokes correction scenario -- it could be that PRDP requires different steps of \(\Delta K\) for each.
% Additionally, applying PRDP to correction scenarios requires the validation metric to be computed using full refinement, which is acceptable if the frequency, with which PRDP is called, is on an epoch level.

\paragraph{Outlook and Impact}

Progressively Refined Differentiable Physics (PRDP) provides a means to exploit both savings due to \emph{progressive refinement} and \emph{incomplete convergence}, thereby greatly reducing the cost of neural network training with differentiable physics. This could enable settings that so far have been infeasible due to prohibitive expenses like long temporal unrollings or differentiable physics on high resolutions or in three dimensions.

Our work opens up many interesting directions for future investigations, such as smoother relations between the achieved plateau ratio \(r\) and the conducted iterations/prescribed tolerance in the physics solver. Potentially, those could be faster than the linear increments we used in this work. For neural-hybrid emulators, IC savings via $K_{\text{max}}$ could also extend to the inference stage. So far, PRDP couples primal and adjoint (in-)accuracy. One can use the unique properties of either unrolled or implicit differentiation for more sophisticated approaches. This can include an imbalanced number of iterations in the primal solve and the implicit linear solve. For unrolled differentiation, one can unroll a different number of iterations either reversely following \citet{Bolte2023onsestepdiff} or \citet{shaban2019truncated} or from the beginning. Other levels of refinement than by the number of iterations of a linear solver are likewise interesting directions for future work, e.g., using differences in spatial or temporal resolution together with resolution-agnostic neural emulators. %s are also imagenable.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

This work investigated neural network training through incompletely converged differentiable physics. 
Our objective was to reduce the cost of gradient computation without sacrificing training accuracy. Prevalent research on training with incompletely converged gradients focuses on bilevel optimization problems, primarily in meta-learning or hyperparameter optimization contexts. This work extends the research in the differentiable physics space, focusing on iterative linear system solves associated with discretized differential operators.

We have demonstrated that neural networks can be trained through differentiable physics solvers significantly coarser than full convergence.
Our approach of Progressively Refined Differentiable Physics combines compute savings from both progressive refinement and incomplete convergence. This yielded favorable outcomes across all our test scenarios. It makes initial training progress cheap using coarse physics and carefully improves training accuracy using adaptive physics refinement over time, ending the training at a refinement significantly below primal convergence. In total, we achieve up to \(86\%\) fewer cumulative number of physics solver iterations than training with fully converged physics, 
% which corresponded to 
and \(78\%\) reduction in training time. Our approach has the potential for numerous practical improvements in learning pipelines that involve differentiable numerical solvers and could facilitate integrating simulators into training that were previously considered computationally infeasible.


% ------------------------------------
% max. 10 pages until here.
% ------------------------------------

% Felix: just to leave it here as a remark. ICLR, this year, extended the page limit to 10 (from 9) but recommends not to stretch it to that. So I think it can be reasonable to also not stretch it and aim for 8-9 pages main part.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reproducibility Statement}


To ensure reproducibility, we detail all physics parameters, discretization schemes, boundary conditions, iterative solvers, network architectures, optimizers, learning rate schedules, data generation methods, train-test splits, and batch sizes in the appendix. Additionally, the full source code for our experiments is available at \url{https://github.com/tum-pbs/PRDP}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\appendix

% I moved the BLO comparison stuff into a separate document ("blo_comparisons.tex") since we decided to only use the related work in the main text. This other document serves as reference how one could potentially classify different approaches and how our work with differentiable physics compares against that.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation}\label{sec:notation}


This section lists the primary notations used throughout the paper for clarity and ease of reference.
\begin{itemize}
 \item \(\Omega \subset \mathbb{R}^D\): Spatial domain of the physics problem.
    \item \(h\): Spatial discretization width.
    \item \(t\): Time step index in time-stepping problems.
\end{itemize}


\subsection*{Variables and Operators}
\begin{itemize}
    \item \(\vu_h \in \mathbb{R}^N\): The discretized solution vector.
    \item \(\vu_h^*\): The direct solution to the linear system \(\mA \vu_h = \vb_h\).
    \item \(\mA \in \mathbb{R}^{N \times N}\): The system matrix of the discretized PDE.
    \item \(\vb_h \in \mathbb{R}^N\): The right-hand side vector.
    \item \(\Delta\): The Laplacian operator, e.g., \(\Delta u(x)\).
    \item \(\nabla\): The gradient operator.
    \item \(\lambda\): Auxiliary variable used in implicit differentiation.
    \item \(\beta(\theta)\): Function mapping parameters \(\theta\) to \(\vb_h\).
    \item \(\Lambda(\theta)\): Funcion mapping parameters \(\theta\) to \(\mA\).
    \item \(\Phi(.)\): Function representing each iteration of an iterative solver.
    \item \(\| \cdot \|\): Matrix or vector norm.
    \item \(f(.; \theta)\): Neural network parameterized by \(\theta\).
    \item \(.^T\): Transpose of a matrix or vector.
    \item \(\vec{1}\): Identity matrix.
    \item \(\theta\): Trainable parameters of the neural network or physical model.
    \item \(K\): Number of iterations performed by the iterative solver.
    \item \(K_\epsilon\): Number of solver iterations required to achieve tolerance \(\epsilon\).
    \item \(\mathcal{P}_K\): Differentiable physics operator, approximating the solution of a linear system after \(K\) iterations.
    \item \(\vu_h^{[K]}\): Approximate solution of the linear system after \(K\) iterations of the solver.
    \item \(\vu_h^r\): Reference solution to the linear system, computed using either a direct solver or a fully converged iterative solver.
    \item \(L(\theta)\): Outer loss function, which measures the discrepancy between the predicted solution and a reference solution.
    \item \(\mJ_l\), \(\mJ_{\mathcal{P}_K}\): Jacobians of the loss function and the physics operator, respectively.
\end{itemize}


\subsection*{Parameters and Tolerances}
\begin{itemize}
    \item \(\epsilon\): Convergence tolerance for iterative solvers.
    \item \(\tau_\text{step}\): Threshold for the stepping criterion in PRDP.
    \item \(\tau_\text{stop}\): Threshold for the stopping criterion in PRDP.
    \item \(\delta\): Grace window for epoch intervals.
    \item \(e\): Exponential averaging window
    \item \(\Delta K\): Increment in solver iterations used in progressive refinement.
\end{itemize}

\subsection*{Miscellaneous}
\begin{itemize}
    \item \(K_\epsilon\): Number of iterations required to reach a specified tolerance \(\epsilon\) in the iterative solver.
    \item \(K_{\text{max}}\): Maximum number of inner iterations sufficient for training accuracy.
    \item \(K_{\text{min}}\): Minimum number of inner iterations required to prevent divergence during training.
    \item \(r, r_c\): Ratios used to evaluate the stepping and stopping criteria for progressive refinement, based on the validation metric's behavior.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Iterative Linear Solvers}\label{sec:linear-solvers}

The solution to linear systems of equations is fundamental to scientific computing. Especially for partial differential equations discretized using fine resolutions or in higher dimensions, the discrete linear systems become large and sparse. Oftentimes, iterative solvers are the only practical way of solving them \citep{saad2003linsolve}.

For our efforts to reduce the cost of differentiable physics as part of neural network training, we consider three different iterative solvers. The \emph{Jacobi method} belongs to the class of smoothing/relaxation methods. When reformulating a linear system solve as a convex quadratic optimization problem, the algorithm of \emph{steepest descent} naturally arises.
% It is the simplest algorithm in the family of \emph{Krylov subspace} methods.
To solve asymmetric and complicated systems, we also use the more sophisticated \emph{GMRES} method. We implemented the Jacobi method and steepest ourselves. For the GMRES, we used the version of JAX\footnote{\url{https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.sparse.linalg.gmres.html}}.

In all tests, the linear solvers were zero-initialized. Convergence is achieved if the relative residuum error using the 2-norm
\begin{equation}\label{eq:relative-residuum-norm}
    \xi^{[k]} = \frac{\|\mA \vu_h^{[k]} - \vb_h\|_2}{\|\vb_h\|_2}
\end{equation}
is below the threshold \(\epsilon\), which we set to \(\epsilon = 10^{-5}\) due to single precision. We use the maximum number of iterations \(K\) as a way to control the refinement of the physics simulation. The iterative solvers return if either the maximum number of iterations \(K\) is reached or the tolerance threshold \(\xi^{[k]} < \epsilon\) is met.


% How the right-hand side affects convergence:
% \begin{itemize}
%     \item Can decompose the right-hand side in terms of the eigenbasis of the system matrix. If it is aligned with the eigenvectors corresponding to large eigenvalues, convergence is fast. Otherwise its is slow. -> Might have noisy network predictions in the beginning (apparently, one can cite \citep{saad2003linsolve} here)
% \end{itemize}

% Most iterative solver technically admit the form of an iterator \(\Phi\), albeit some (like conjugate gradient) are stateful.

% We restricted ourselves to simple methods for the less sophisticated examples. Only for the more realistic scenarios of Burgers and Navier-Stokes we require a more elaborate solver that also easily handles asymetric matrices. We decided for GMREs.

\subsection{Jacobi relaxation}\label{sec:jacobi}

The Jacobi method is a relaxation-type method based on the decomposition of the system matrix \(\mA\) into a strictly lower diagonal \(\mL\), a diagonal \(\mD\), and a strictly upper diagonal \(\mU\) part such that \(\mA = \mL + \mD + \mU\). We present its algorithm in \ref{alg:jacobi}.

%%% From Chat GPT
\begin{algorithm}
\caption{Jacobi Method for Solving \( \mA \vu_h = \vb_h \)}\label{alg:jacobi}
\SetAlgoLined
\KwIn{Matrix \( \mA \in \R^{N \times N} \), vector \( \vb_h \in \mathbb{R}^n \), tolerance \( \epsilon \), maximum iterations \( K \)}
\KwOut{Approximation of the solution \( \vu_h \in \mathbb{R}^N \)}

Decompose
% into strictly lower diagonal, diagonal, and strictly upper diagonal parts 
\(\mL + \mD + \mU = \mA\)
Initialize \( \vu_h^{[0]} \in \mathbb{R}^N \) to zeros \( \vu_h^{[0]} = \vzero \)\;

\For{$k = 0, 1, \dots, K-1$}{
    % \For{$i = 1, 2, \dots, N$}{
    %     \(
    %     \evu_i^{(k)} = \frac{1}{\emA_{ii}} \left(\evb_i - \sum_{\substack{j=1 \\ j \neq i}}^N \emA_{ij} \evu_j^{[k-1]}\right)
    %     \)
    % }
    \(\vu_h^{[k+1]} = \mD^{-1} \left(\vb_h - (\mL + \mD) \vu_h^{[k]}\right)\)
    
    Compute relative residuum norm: \( \xi^{[k+1]} = \|\mA \vu_h^{[k+1]} - \vb_h\|_2 / \|\vb_h\|_2 \)\;
    \If{$\xi^{[k+1]} < \epsilon$}{
        \textbf{break}\;
    }
}
\Return $\vu_h^{[k+1]}$
\end{algorithm}

As a smoothing method, the convergence of the Jacobi method depends on the spectral radius of the iterator matrix \citep{saad2003linsolve} via
\begin{equation}\label{eq:convergence-rate-jacobi}
    \|\vr_h^{[k+1]}\|_2
    \leq
    \rho(\mD^{-1}(\mL + \mR)) \|\vr_h^{[k]}\|_2.
\end{equation}

Loosely speaking, the condition number of the system matrix \(\mA\) affects the spectrum of the Jacobi iterator matrix in that high condition numbers lead to spectral radii close to \(1.0\), causing slow convergence.

\subsection{Steepest Descent}\label{sec:steepest-descent}

The steepest descent method follows the gradient of the convex quadratic optimization problem associated with solving a linear system of equations, which is also given by the negative residuum. However, in contrast to the gradient descent methods typically found in more general optimization problems (like for training neural networks), the optimal step size for maximum decrease \(\alpha^{[k]}\) can be determined in each iteration \citep{saad2003linsolve}. We present the steepest descent in algorithm \ref{alg:steepest-descent}.

%%% From ChatGPT
\begin{algorithm}
\caption{Steepest Descent Method for Solving \( \mA \vu_h = \vb_h \)}\label{alg:steepest-descent}
\SetAlgoLined
\KwIn{Matrix \( \mA \in \R^{N \times N} \), vector \( \vb_h \in \R^N \), tolerance \( \epsilon \), maximum iterations \( K \)}
\KwOut{Approximation of the solution \( \vu_h \in \R^N \)}

Initialize \( \vu_h^{[0]} \in \R^N \) to zeros \( \vu_h^{[0]} = \vzero \)\;
Set initial residual \( \vr_h^{[0]} = \vb_h - \mA \vu_h^{[0]} \)\;

\For{$k = 0, 1, \dots, K-1$}{
    Compute step size \( \alpha^{[k]} = \frac{\vr_h^{[k]} \cdot \vr_h^{[k]}}{\vr_h^{[k]} \cdot \mA \vr_h^{[k]}} \)\;
    Update solution \( \vu_h^{[k+1]} = \vu_h^{[k]} + \alpha^{[k]} \vr_h^{[k]} \)\;
    Update residual \( \vr_h^{[k+1]} = \vr_h^{[k]} - \alpha^{[k]} \mA \vr_h^{[k]} \)\;
    Compute relative residuum norm: \( \xi^{[k+1]} = \|\vr_h^{[k+1]}\|_2 / \|\vb_h\|_2 \)\;
    \If{$\xi^{[k+1]} < \epsilon$}{
        \textbf{break}\;
    }
}
\Return $\vu_h^{[k+1]}$
\end{algorithm}

It can be shown that the residuum norm converges exponentially linear in the asymptotic regime based on the condition number of the system matrix \(\kappa(\mA)\) via \citep{saad2003linsolve}
\begin{equation}\label{eq:convergence-rate-steepest-descent}
    \|\vr_h^{[k+1]}\|_2
    \leq
    \left( \frac{\kappa(\mA) - 1}{\kappa(\mA) + 1}\right)
    \|\vr_h^{[k]}\|_2.
\end{equation}

% \subsection{Conjugate Gradient}\label{sec:conjugate-gradient}

% Don't know if we used it???

% \begin{algorithm}[H]
% \caption{Conjugate Gradient Method for Solving \( \mA \vu_h = \vb_h \)}
% \SetAlgoLined
% \KwIn{Matrix \( \mA \in \R^{N \times N} \), vector \( \vb_h \in \R^N \), tolerance \( \epsilon \), maximum iterations \( K \)}
% \KwOut{Approximation of the solution \( \vu_h \in \R^N \)}

% Initialize \( \vu_h^{[0]} \in \R^N \) to zeros \( \vu_h^{[0]} = \vzero \)\;
% Set initial residual \( \vr_h^{[0]} = \vb_h - \mA \vu_h^{[0]} \)\;
% Set initial search direction \( \vp_h^{[0]} = \vr_h^{[0]} \)\;

% \For{$k = 0, 1, \dots, K-1$}{
%     Compute step size \( \alpha^{[k]} = \frac{\vr_h^{[k]} \cdot \vr_h^{[k]}}{\vp_h^{[k]} \cdot \mA \vp_h^{[k]}} \)\;
%     Update solution \( \vu_h^{[k+1]} = \vu_h^{[k]} + \alpha^{[k]} \vp_h^{[k]} \)\;
%     Update residual \( \vr_h^{[k+1]} = \vr_h^{[k]} - \alpha^{[k]} \mA \vp_h^{[k]} \)\;

%     Compute relative residuum norm: \( \xi^{[k+1]} = \|\vr_h^{[k+1]}\|_2 / \|\vb_h\|_2 \)\;
%     \If{$\xi^{[k+1]} < \epsilon$}{
%         \textbf{break}\;
%     }

%     Compute \( \beta^{[k+1]} = \frac{\vr_h^{[k+1]} \cdot \vr_h^{[k+1]}}{\vr_h^{[k]} \cdot \vr_h^{[k]}} \)\;
%     Update search direction \( \vp_h^{[k+1]} = \vr_h^{[k+1]} + \beta^{[k+1]} \vp_h^{[k]} \)\;
% }
% \Return $\vu_h^{[k+1]}$
% \end{algorithm}

% In contrast to the steepest descent algorithm of 

% \begin{equation}\label{eq:convergence-rate-cg}
%     \|\vr_h^{[k+1]}\|_2
%     \leq
%     \left( \frac{\sqrt{\kappa(\mA)} - 1}{\sqrt{\kappa(\mA)} + 1}\right)
%     \|\vr_h^{[k]}\|_2
% \end{equation}

\subsection{GMRES}\label{sec:gmres}

The General Method of RESiduals (GMRES) builds a subset of the Krylov basis associated with system matrix \(\mA\) and finds an approximation to the solution of the linear system via least-squares. Typically, it is restarted to rebuild a new Krylov basis each \(m\) iteration. We call an iteration \(k\) the construction of the entire \(m\)-dimensional Krylov subspace and subsequent least-squares solve. We use the \verb|"batched"| mode of the JAX implementation, which can only terminate after a restart but not within a restart.

%%% From ChatGPT
\begin{algorithm}
\caption{GMRES Method for Solving \( \mA \vu_h = \vb_h \)}\label{alg:gmres}
\SetAlgoLined
\KwIn{Matrix \( \mA \in \R^{N \times N} \), vector \( \vb_h \in \R^N \), tolerance \( \epsilon \), maximum iterations \( K \), restart parameter \( m \)}
\KwOut{Approximation of the solution \( \vu_h \in \R^N \)}

Initialize \( \vu_h^{[0]} \in \R^N \) to zeros \( \vu_h^{[0]} = \vzero \)\;
Set initial residual \( \vr_h^{[0]} = \vb_h - \mA \vu_h^{[0]} \)\;
Set \( \beta = \|\vr_h^{[0]}\|_2 \)\;
Set \( \vv_1 = \vr_h^{[0]} / \beta \) (first Krylov basis vector)\;

\For{$k = 0, 1, \dots, K-1$}{
    \For{$j = 1, 2, \dots, m$}{
        Compute \( \vw_j = \mA \vv_j \)\;
        \For{$i = 1, \dots, j$}{
            Compute \( h_{ij} = \vv_i \cdot \vw_j \)\;
            Update \( \vw_j = \vw_j - h_{ij} \vv_i \)\;
        }
        Set \( h_{j+1,j} = \|\vw_j\|_2 \)\;
        % % Below is not possible because we use the "batched" mode of JAX
        % \If{$h_{j+1,j} = 0$}{
        %     \textbf{break}\;
        % }
        Normalize \( \vv_{j+1} = \vw_j / h_{j+1,j} \)\;
    }

    Solve the least-squares problem: minimize \( \|\beta \ve_1 - \mH^{[k]} \vy^{[k]}\|_2 \)\;
    Update solution: \( \vu_h^{[k+1]} = \vu_h^{[k]} + \vv^{[k]} \vy^{[k]} \)\;
    
    Compute relative residuum norm: \( \xi^{[k+1]} = \|\vr_h^{[k+1]}\|_2 / \|\vb_h\|_2 \)\;
    \If{$\xi^{[k+1]} < \epsilon$}{
        \textbf{break}\;
    }

    Restart GMRES after every $m$ iterations if convergence not met\;
}
\Return $\vu_h^{[k+1]}$
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{More Details on differentiating over iterative linear solvers}\label{sec:linear-solve-differentiation-more-details}

For convenience, we restate the simple computational graph involving a linear solver with \(K\) steps subject to reverse-mode automatic differentiation (AD) as
\begin{equation}\label{eq:outer-gradient-extended-appendix}
    (\nabla_\theta L)^T = \bar{\theta}^T =  \left(\left( \vec{1}^T \; \mJ_l |_{\mathcal{P}_K(\vf(\cdot\, ;\theta))} \right)\; \mJ_{\mathcal{P}_K}|_{\vf(\cdot \, ;\theta)}\right) \mJ_\vf |_{\theta}.
\end{equation}
In this setting, one has to compute three vector-Jacobian products (VJPs). The VJP over the loss function \(\bar{\vu}_h^T = \vec{1}^T \; \mJ_l |_{\mathcal{P}_K(\vf(\cdot\, ;\theta))}\) and into the network's parameter space \(\bar{\theta}^T = \bar{\vg}_h^T \mJ_\vf |_{\theta}\) can be straightforwardly evaluated by the AD engine as they consist of explicit operations.

On the other hand, the VJP over the approximative solver \(\bar{\vg}_h^T = \bar{\vu}_h^T \mJ_{\mathcal{P}_K}|_{\vf(\cdot \, ;\theta)}\) is nontrivial as it requires handling the iterative solver and the assembly routines. We detail the two prominent approaches below. They are visually depicted in \figref{fig:diff_iterative_process}.

\subsection{Implicit Differentiation}\label{sec:implicit-diff-details}

We first solve an auxiliary linear system for an adjoint variable \(\lambda_h\) \citep{christianson1998reverseimplicit}
\begin{equation}
    \mA^T \lambda_h = \bar{\vu}_h.
\end{equation}
Since the convergence behavior of most iterative linear solvers is dependent on the spectrum of the system matrix and since transposition does not effectively change this, it is reasonable to employ the same iterator as in the primal solution but with a transposed system matrix and different right-hand side \(\Phi(\cdot \, ; \mA^T, \bar{\vu}_h)\). The convergence of the iterates \(\{\lambda_h^{[0]}, \lambda_h^{[1]}, \lambda_h^{[2]}, \dots, \lambda_h^{[\tilde{K}]}\} = \{\Phi^k(\lambda_h^{[0]}; \mA^T, \bar{\vu}_h)\}_{k=0}^{\tilde{K}}\) can potentially be different than in the primal solution. In other words, \(\tilde{K}_{\epsilon}\) for tolerance \(\epsilon\) can be different from primal the \(K_\epsilon\).

Once \(\lambda_h\) is determined, it directly equals the intermediate gradient on the right-hand side \(\bar{\vb}_h = \lambda_h\). The intermediate gradient on the system matrix arises as the negative outer product with the \emph{primal} solution \(\bar{\mA} = - \lambda_h \vu_h^T\). Hence, it is sufficient to only save the solution \(\vu_h^{[K]}\) from the primal pass; no further iterates are required. 

Then, the reversely propagated intermediate gradient on the input of the physics operator is given by
\begin{equation}\label{eq:vjp-over-assemblies}
    \bar{u}_h^T \mJ_{\mathcal{P}_K}|_{\vg_h} = \bar{\vg}_h^T = \bar{\vb}_h^T \mJ_{\beta} |_{\vg_h} + \underbrace{\bar{\mA}^T \mJ_{\Lambda} |_{\vg_h}}_{=\bar{\mA} : \mJ_{\Lambda} |_{\vg_h}},
\end{equation}
under the abuse of notation to consider the gradient matrix \(\bar{\mA}\) as a vector. Alternatively, this can also be expressed as the left double contraction. It must be noted that typically \(\bar{\mA}\) is only evaluated on the sparsity pattern of \(\mA\), if it is materialized at all.

In case only the right-hand side \(\vb_h\) is parameterized, there will be no reverse propagation through the matrix assembly \(\Lambda\). For example, this was the case for the Poisson and heat emulator examples in \secref{sec:scheduling-inner-iterations} and \secref{sec:motivate-ic-savings}, respectively.

Since we use matrix-free implementations for most linear solvers, the primal system is not solved with a materialized matrix \(\mA\) but a function that is linear in its first argument
\begin{equation}\label{eq:define-mat-vec}
    \Lambda(\vg_h) \vu_h \overset{\wedge}{=} \psi(\vu_h; \vg_h).
\end{equation}
In this case, we compute the system matrix' contribution to the previous intermediate gradient via the \emph{negative} VJP over its second argument
\begin{equation}
    \bar{u}_h^T \mJ_{\mathcal{P}_K}|_{\vg_h} = \bar{\vg}_h^T = \bar{\vb}_h^T \mJ_{\beta} |_{\vg_h} - \lambda_h^T \mJ_{\psi,2} |_{\vg_h}.
\end{equation}
To employ the iterator \(\Phi\) on a matrix-free version of the transposed matrix, we need to programmatically transpose the function \(\psi\), which can be done with JAX \citep{jax2018github}. Moreover, there is a function in JAX to automatically register linear solvers with correct propagation rules\footnote{\url{https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.custom_linear_solve.html}}.

More generally speaking, implicit differentiation is powerful because it allows differentiation over various implicit relations simply by solving a linear system of equations. Modern AD engines allow the effortless linearization of optimality conditions. This allows for easily registering custom propagation rules by employing matrix-free Krylov solvers \citep{Blondel2022modular}. Note, however, that the focus of this paper is the differentiation over linear system solutions in which the primal operation is the \emph{same} as in the implicit propagation rule.

\subsection{Unrolled Differentiaion}\label{sec:unrolled-diff-details}

Unrolled differentiation of an iterative program is the direct application of standard automatic differentiation tools to its algorithmic implementation. AD unrolls the program’s iterations and writes them as individual computational steps.

We find the intermediate gradients on both the system matrix \(\bar{\mA}\) and right-hand side \(\bar{\vb}_h\) via first computing the intermediate gradient on all iterates using the VJP of the iterator \(\Phi\) with respect to its first argument
\begin{equation}
    \left(\bar{\vu}_h^{[k]}\right)^T = \bar{\vu}_h^T \prod_{l=K-1}^{k} \mJ_{\Phi,1}|_{\vu_h^{[l]} = \Phi^l(\vu_h^{[0]};\mA, \vb_h)}.
\end{equation}
Then we can aggregate each contribution using the VJP over the iterator \(\Phi\) with respect to its conditioned arguments
\begin{align}
    \bar{\mA}^T \overset{\text{unroll}}{=} \sum_{k=0}^{K-1} \left(\bar{\vu}_h^{[k]}\right)^T \mJ_{\Phi,2}|_{\vu_h^{[k]} = \Phi^k(\vu_h^{[0]};\mA, \vb_h)},
    \\
    \bar{\vb}_h^T \overset{\text{unroll}}{=} \sum_{k=0}^{K-1} \left(\bar{\vu}_h^{[k]}\right)^T \mJ_{\Phi,3}|_{\vu_h^{[k]} = \Phi^k(\vu_h^{[0]};\mA, \vb_h)}.
\end{align}
Since the VJP of the iterator has to be evaluated at primal inputs, typically, all iterates must be stored on the tape or recomputed. There are approaches that balance compute and memory \citep{griewank1992achieving}. However, for simplicity, we only implement unrolled differentiation by retaining the entire sequence of iterates.

After the intermediate gradients have been obtained, the gradients are further backpropagated via \eqref{eq:vjp-over-assemblies}.
% As highlighted with \eqref{eq:define-mat-vec}, we primarily implement linear solvers via the matrix-vector product function \(\psi\). In this case, the backpropagated gradient is
% \begin{equation}
%     \vg_h = \bar{\vb}_h^T \mJ_{\beta} |_{\vg_h} - \sum_{k=0}^{K-1}\left( \bar{\vu}_h^T \prod_{l=K-1}^{k} \mJ_{\Phi,2}|_{\Phi^l(\vu_h^{[0]};\mA, \vb_h)}\right) \mJ_{\psi,2} |_{\vg_h}
% \end{equation}
While the mathematical description of unrolled differentiation is more elaborate than for implicit differentiation, its implementation in AD engines like JAX is easier given the algorithm fully uses differentiable operations.

The non-asymptotic study of \citet{curse} revealed that the convergence of unrolled differentiation can exhibit a burn-in phenomenon. We also investigated this for our iterative linear solvers but found it to be practically irrelevant. While it theoretically can occur for problems with parameterized system matrices or for Krylov methods in general, we believe that for large enough parameter spaces, the potential burn-in of some gradient components is compensated by the entirety. Moreover, since we applied PRDP using both unrolled and implicit differentiation and observed almost identical savings in both cases, we further conclude that even if there was a burn-in, the PRDP approach would be unaffected by it.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{visualizations/unrolled_vs_implicit.pdf}
    \caption{The iterative solution of a linear system of equations spawns a long sequential compute graph that constitutes the primal pass (in black). The step from prior variable \(\vg_h\) to approximate solution \(\vu_h^{[K]}\) can be capsuled as the physics operator \(\gP_K\) (in blue). Under reverse-mode AD, differentiation over \(\gP_K\) (blue curvy line) can be done in two ways. \emph{Implicit Differentiation} spawns an auxiliary iterative linear solve using the transposed matrix \(\mA^T\) (in purple). On the other hand, \emph{unrolled differentation} opens the black box and reversely propagates through each iterator step \(\Phi\) (in yellow). Both approaches yield intermediate gradients on system matrix \(\bar{\mA}\) and \(\bar{\vb}_h\), which are subsequently backpropagated into \(\bar{\vg}_h\) via the VJP over their assembly routines.
    }
    \label{fig:diff_iterative_process}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Proofs}

% \subsection{The implicit propagation problem for linear solves in reverse-mode automatic differentation}\label{sec:implicit-diff-derivation}

% \subsection{The vector-Jacobian product over steepest descent iterators}\label{sec:cost-vJp-iterator}

% TODO: Not sure yet, if we should include this, but it could serve as a nice showcase that the cost of going through one iteration of steepest descent in reverse-mode (i.e., during unrolling) is approximately the same cost as during the primal execution because it is dominated by the cost of the matrix-vector product. More generally speaking, this is valid for all iterative solvers????


% In all tests, the linear solvers were zero-initialized. Results with different initializations were consistent with our observations (todo: write more specifically) and are available in appendix todo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Differentiable Solvers}\label{sec:solver-details}

In this section, we describe the discretization choices behind the differentiable solvers. They are based on finite difference approximations on uniform cartesian grids with Dirichlet or periodic boundary conditions. Linear PDEs naturally lead to a linear system of equations for which only the right-hand assembly remains a dependency on the prior compute graph, e.g., is built upon information previous in time. In the case of the nonlinear PDEs, we choose a Picard-based approximation leading to Oseen-like problems \citep{turek1999efficient}. Those have both the right-hand side as well as the system matrix depending on the prior parts of the compute graph.

The discretizations are implemented matrix-free in JAX \citep{jax2018github}.
% [AND WE ALSO USE THEIR SOLVERS, except for the smoothing methods].
Matrices are only materialized when needed for smoothing methods or for direct decomposition-based solvers. 

Once fully discretized and re-formulated, each problem is cast in the standard form of a linear system of equations
\begin{equation}
    \mA \vu_h = \vb_h,
\end{equation}
using the assembly routines \(\mA = \Lambda(\vg_h)\) and \(\vb_h = \beta(\vg_h)\).
In the simplest case we have directly \(\vg_h = \theta\) but other setups like information from a previous time step \(\vg_h = \vu_h^{[t-1]}\) are common.
% For example, it can directly be the set of parameters of the outer problem \(\theta\) (neural network weights, consecutive parameters, etc.) or information from a previous state in time. Then, \(\Lambda\) and \(\beta\) shall be the assembly functions for the system matrix and right-hand side, respectively. This makes the system of linear equations parameterized via
% \begin{equation}
%     \Lambda(\vg) \vu_h = \beta(\vg).
% \end{equation}
Then, we define the physics operator \(\mathcal{P}\) as the function mapping from the prior point in the compute graph \(\vg_h\) to the solution of the linear system of equations
\begin{equation}
     \mathcal{P}(\vg_h) := \vu_h = \mA^{-1} \vb_h = \left(\Lambda(\vg_h)\right)^{-1}\beta(\vg_h)
\end{equation}
If \(\mathcal{P}\) is given without subscript this refers to an exact solver of the linear system. This can be a direct method based on matrix decomposition or a fully converged iterative solver. Either way, the residuum norm of this solver's result is guaranteed to be below the relative tolerance threshold \(\epsilon\) via
\begin{equation}
    \frac{
        \|\Lambda(\vg_h) \mathcal{P}(\vg_h) - \beta(\vg_h) \|_2
    }{
        \|\beta(\vg_h) \|_2
    }
    < \epsilon.
\end{equation}
If not mentioned otherwise, we set \(\epsilon = 10^{-5}\) because we exclusively work in single precision floating format. An approximate solver with \(K\) iterations is written \(\mathcal{P}_K(\cdot)\). The iterative linear solvers we employed are introduced in \secref{sec:linear-solvers}.

%%%%%%%%%

\subsection{Poisson Equation in 1D}\label{sec:solver-details-poisson}

We solve the Poisson equation on the unit interval with homogeneous Dirichlet boundary conditions and a parameterized right-hand side
\begin{equation}\label{eq:poisson-with-homogeneous-dirichlet-1d}
    \frac{\mathrm{d}^2u(x)}{\mathrm{d}x^2} = - p(x, \theta)
    \quad \quad
    u(0) = 0 = u(1)
\end{equation}
For a finite difference discretization, the domain \(\Omega = (0, 1)\) is divided equidistantly into \(N+2\) grid points, out of which two are related to the prescribed value on the boundary and can, hence, be ignored. As such, there are \(N\) degrees of freedom that make up the discrete solution vector \(\vu_h \in \R^N\). Equally, the right-hand side is discretized at the same points and negated, yielding \(\vb_h\). At index \(i\) interior to the domain, the second derivative is approximated with the three-point stencil
\begin{equation}
    \frac{1}{(\Delta x)^2} (\evu_{i-1} - 2 \evu_i + \evu_{i+1}) = - \evb_i,
\end{equation}
with the spacing \(\Delta x = \frac{1}{N+1}\). This leads to the system of linear equations
\begin{equation}
    \underbrace{\frac{1}{(\Delta x)^2}\begin{bmatrix}
        -2 & 1 & 0 & \dots & 0 & 0 \\
        1 & -2 & 1 & \dots & 0 & 0 \\
        0 & 1 & -2 & \dots & 0 & 0 \\
        \vdots & & \ddots & & \vdots \\
        0 & 0 & 0 & \dots & 1 & -2 \\
    \end{bmatrix}}_{=: \tilde{\gL}_1 = \mA}
    \begin{bmatrix}
        \evu_{1} \\
        \evu_{2} \\
        \evu_{3} \\
        \vdots \\
        \evu_{N} \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        \evb_{1} \\
        \evb_{2} \\
        \evb_{3} \\
        \vdots \\
        \evb_{N} \\
    \end{bmatrix},
\end{equation}
in which the first matrix is due to the discretized Laplace operator in one dimension. We denote it \(\tilde{\gL}_1\) (the tilde is to distinguish it from the Laplacian matrices on periodic boundaries of the following sections). The subscript is to indicate the one-dimensional setting. It is tridiagonal and solely defines the system to be solved with \(\mA = \tilde{\gL}_1\) as
\begin{equation}
    \mA \vu_h = \beta(\vb_h).
\end{equation}
with the right-hand side assembly function just element-wise negating the input.
The system matrix is not parameter-dependent, i.e., \(\Lambda(\vg) = \tilde{\gL}_1 \).

% Discretized Poisson equations lead to a symmetric-positive definite system matrix [Need textbook to cite], which naturally makes it eligible to be solved by the conjugate gradient algorithm. For simplicity, we only solved it with the Jacobi method and the steepest descent algorithm. 

%%%%%%%%%%%%%%%%%%%%%

\subsection{Heat Diffusion in 1D}\label{sec:solver-details-diffusion-1d}

We consider the time-dependent diffusion equation in one dimension on the unit interval under homogeneous Dirichlet boundary conditions
\begin{equation}\label{eq:diffusion-with-periodic-boundaries-1d}
    \frac{\partial u}{\partial t} = \nu \frac{\partial^2 u}{\partial x^2} \quad \quad u(t, 0) = 0 =  u(t, 1).
\end{equation}
Similarly to \secref{sec:solver-details-poisson}, we equidistantly divide the domain into \(N+2\) grid points.
The matrix associated with the discretized second derivative in one dimension is again
\begin{equation}
    \tilde{\gL}_1 := \frac{1}{(\Delta x)^2}\begin{bmatrix}
        -2 & 1 & 0 & \dots & 0 & 1 \\
        1 & -2 & 1 & \dots & 0 & 0 \\
        0 & 1 & -2 & \dots & 0 & 0 \\
        \vdots & & \ddots & & \vdots \\
        1 & 0 & 0 & \dots & 1 & -2 \\
    \end{bmatrix}.
\end{equation}
With it \eqref{eq:diffusion-with-periodic-boundaries-1d} can be discretized in space via the method of lines
\begin{equation}
    \frac{\mathrm{d} \vu_h}{\mathrm{d} t} = \nu \gL_1 \vu_h.
\end{equation}
Applying an implicit Euler discretization to the time derivative yields
\begin{equation}
    \frac{\vu_h^{[t+1]} - \vu_h^{[t]}}{\Delta t} = \nu \tilde{\gL}_1 \vu_h^{[t+1]},
\end{equation}
with two subsequent time levels on the state vector. This can be rearranged into the standard form of a linear system of equations as
\begin{equation}
    \underbrace{\left( \mI - \nu \Delta t \tilde{\gL}_1 \right)}_{:= \mA} \vu_h^{[t+1]} = \vu_h^{[t]}.
\end{equation}
This is also referred to as the backward-in-time central-in-space (BTCS) discretization of the diffusion equation. Advancing the state by one time level requires solving a linear system with the constant system matrix \(\mA\) and with the assembly function \(\beta(\cdot)\) simply being the identity.
Hence, the physics operator maps to the next state in time with
\begin{equation}
    \vu_h^{[t+1]} = \mathcal{P}(\vu_h^{[t]}). %= A^{-1} \vg(\vu_h^{[t]})
\end{equation}
% The constant system matrix shares properties with the discrete Poisson equation in that it is symmetric positive definite.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Heat Diffusion in 2D}\label{sec:solver-details-diffusion-2d}

The equation for heat diffusion in two dimensions on the unit square with homogeneous Dirichlet boundary conditions reads
\begin{equation}
    \frac{\partial u}{\partial t} = \nu \underbrace{\left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2}\right)}_{\Delta u} \qquad u(t, 0, y) = 0 = u(t, 1, y), u(t, x, 0) = 0 = u(t, x, 1)
\end{equation}

The Laplacian matrix in two dimensions can be written in a block structure using the one-dimensional Laplacian \(\tilde{\gL}_1\) and appropriately sized identity matrices \(\mI\)
\begin{equation}
    \tilde{\gL}_2 = \frac{1}{(\Delta x)^2}\begin{bmatrix}
        (\Delta x)^2\tilde{\gL}_1 - 2\mI & \mI & \mZero & \mZero & \dots & \mZero & \mZero & \mZero\\
        \mI & (\Delta x)^2\tilde{\gL}_1 - 2\mI & \mI & \mZero & \dots & \mZero & \mZero & \mZero\\
        \mZero & \mI & (\Delta x)^2\tilde{\gL}_1 - 2\mI & \mI & \dots & \mZero & \mZero & \mZero\\
        \vdots & & & & \ddots  & & & \vdots \\
        \mZero & \mZero & \mZero & \mZero & \dots & \mZero & \mI & (\Delta x)^2\tilde{\gL}_1 - 2\mI \\
    \end{bmatrix}.
\end{equation}
With this two-dimensional Laplacian matrix, the state vector \(\vu_h \in \R^{N^2}\) is advanced similarly via solving
\begin{equation}
    \underbrace{\left( \mI - \nu \Delta t \tilde{\gL}_2 \right)}_{:= \mA \in \R^{N^2 \times N^2}} \vu_h^{[t+1]} = \vu_h^{[t]}.
\end{equation}
Similar to the one-dimensional BTCS scheme, the system matrix is constant.
% and has symmetric positive definiteness.
The right-hand side assembly is again the identity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Heat Diffusion in 3D}\label{sec:solver-details-diffusion-3d}

The equation for heat diffusion in three dimensions reads
\begin{equation}
    \frac{\partial u}{\partial t} = \nu \underbrace{\left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2}\right)}_{\Delta u}.
\end{equation}
We again use homogeneous Dirichlet boundary conditions on all six sides of the unit cube
\begin{equation}
    u(t, 0, y, z) = 0 = u(t, 1, y, z),\; u(t, x, 0, z) = 0 = u(t, x, 1, z), \; u(t, x, y, 0) = 0 = u(t, x, y, 1).
\end{equation}
We consider the unit-cube \(\Omega = (0, 1)^3\) with $N$ degrees of freedom per dimension, in total \(N^3\).

The Laplacian matrix in three dimensions can again be written in a block structure using the two-dimensional Laplacian \(\tilde{\gL}_2\) and appropriately sized identity matrices \(\mI\)
\begin{equation}
    \tilde{\gL}_3 = \frac{1}{(\Delta x)^2}\begin{bmatrix}
        (\Delta x)^2\tilde{\gL}_2 - 2\mI & \mI & \mZero & \mZero & \dots & \mZero & \mZero & \mZero\\
        \mI & (\Delta x)^2\tilde{\gL}_2 - 2\mI & \mI & \mZero & \dots & \mZero & \mZero & \mZero\\
        \mZero & \mI & (\Delta x)^2\tilde{\gL}_2 - 2\mI & \mI & \dots & \mZero & \mZero & \mZero\\
        \vdots & & & & \ddots  & & & \vdots \\
        \mZero & \mZero & \mZero & \mZero & \dots & \mZero & \mI & (\Delta x)^2\tilde{\gL}_2 - 2\mI \\
    \end{bmatrix}.
\end{equation}
With this three-dimensional Laplacian matrix, the state vector \(\vu_h \in \R^{N^3}\) is advanced similarly via solving
\begin{equation}
    \underbrace{\left( \mI - \nu \Delta t \tilde{\gL}_3 \right)}_{:= \mA \in \R^{N^3 \times N^3}} \vu_h^{[t+1]} = \vu_h^{[t]}.
\end{equation}
Similar to the one-dimensional BTCS scheme, the system matrix is constant.
The right-hand side assembly is again the identity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Burgers in 1D}\label{sec:solver-details-burgers-1d}



The Burgers equation on the one-dimensional unit interval in non-conservative form with periodic boundary conditions reads
\begin{equation}\label{eq:heat-equation-2d-periodic}
    \frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = \nu \frac{\partial^2 u}{\partial x^2} \quad \quad u(t, 0) = u(t, 1).
\end{equation}
We will discretize the domain into $N+1$ grid points. Under periodic boundaries, only one of the boundary points can be eliminated. By convention, we choose the right-most point, leaving \(N\) degrees of freedom (including the left-most point). The matrix associated with the discretized second derivative in one dimension now reads
\begin{equation}
    \gL_1 := \frac{1}{(\Delta x)^2}\begin{bmatrix}
        -2 & 1 & 0 & \dots & 0 & 1 \\
        1 & -2 & 1 & \dots & 0 & 0 \\
        0 & 1 & -2 & \dots & 0 & 0 \\
        \vdots & & \ddots & & \vdots \\
        1 & 0 & 0 & \dots & 1 & -2 \\
    \end{bmatrix}.
\end{equation}
It differs from the former Laplacian matrix in that it is not exclusively tri-diagonal but also has entries in the top right and bottom left corners. Additionally, we now have \(\Delta x = 1/N\).

The convection term requires special treatment because of its nonlinearity and the advection characteristic of the first derivative. Let \(\mathcal{F}_1\) and \(\mathcal{B}_1\) represent the forward or backward approximation of the first derivative in one dimension on periodic boundaries, respectively, via
\begin{equation}
    \mathcal{F}_1 := \frac{1}{\Delta x}\begin{bmatrix}
        -1 & 1 & 0 & \dots & 0 & 0 \\
        0 & -1 & 1 & \dots & 0 & 0 \\
        0 & 0 & -1 & \dots & 0 & 0 \\
        \vdots & & \ddots & & \vdots \\
        1 & 0 & 0 & \dots & 0 & -1 \\
    \end{bmatrix}
    \qquad
    \mathcal{B}_1 := \frac{1}{\Delta x}\begin{bmatrix}
        1 & 0 & 0 & \dots & 0 & 1 \\
        -1 & 1 & 0 & \dots & 0 & 0 \\
        0 & -1 & 1 & \dots & 0 & 0 \\
        \vdots & & \ddots & & \vdots \\
        0 & 0 & 0 & \dots & -1 & 1 \\
    \end{bmatrix}
\end{equation}
Again, note the element in the corner entries of the matrices. Then, we can build an upwind differentiation  matrix based on the winds \(\vw_h\)
\begin{equation}
    \Gamma_1(\vw_h) = \text{diag}\left(\underbrace{\max\left(\frac{\vs_{-1}(\vw_h) + \vw_h}{2}, 0\right)}_{\text{positive winds}}\right) \mathcal{B}_1      +
    \text{diag}\left(\underbrace{\max\left(\frac{\vs_{1}(\vw_h) + \vw_h}{2}, 0\right)}_{\text{negative winds}}\right) \mathcal{F}_1.
\end{equation}
Deducing the positive and negative winds from neighboring averages (using the periodic forward shift \(s_{-1}\) and backward shift \(s_{1}\) operators) is necessary to have correct movement if the winds change sign over the domain. If we use the discrete state vector \(\vu_h\) as winds \(\vw_h\), we can discretize the continuous equation via the method of lines as
\begin{equation}
    \frac{\mathrm{d} \mathbf{u}_h}{\mathrm{d} t} + \Gamma_1(\vu_h) \vu_h = \nu \gL_1 \vu_h.
\end{equation}
Naturally, the spatial discretization of a \emph{nonlinear} PDE leads to a system of \emph{nonlinear} ODEs. To fully resolve the nonlinearity, one could resort to a Newton-Raphson or a quasi-Newton method. However, for simplicity, we will apply the trick to linearize the upwind matrix using the state \emph{previous in time} \citep{turek1999efficient}, which gives
\begin{equation}
    \frac{\vu_h^{[t+1]} - \vu_h^{[t]}}{\Delta t} + \Gamma_1(\vu_h^{[t]})\vu_h^{[t+1]} = \nu \gL_1 \vu_h^{[t+1]}.
\end{equation}
This can be rearranged into the standard form as
\begin{equation}
    \underbrace{\left(\mI + \Delta t \Gamma_1(\vu_h^{[t]}) - \Delta t \nu \gL_1 \right)}_{=\mA = \Lambda(\vu_h^{[t]})} \vu_h^{[t+1]} = \vu_h^{[t]}.
\end{equation}
As such, the system matrix is dependent on the previous state in time, i.e., it is dependent on the previous variables in the compute graph. The right-hand side assembly routine is again the identity. However, different from before is that the system matrix \(\mA\) is now asymmetric, which necessitates special linear solvers.

Iteration over re-assembly and solution is possible but we omit this for simplicity, accepting that the \emph{nonlinear} residuum is not fully converged. This introduces an error of order \(\gO(\Delta t)\), which is acceptable since our temporal discretization is first order.
It must be noted that despite the nonlinear residuum is not fully converged, the linear residuum associated with the linearization will be, assuming we use the converged solver \(\gP\).

%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Burgers in 2D}


While we did not investigate any experiments with a two-dimensional Burgers solver, we will still present it here as it naturally helps with understanding the two-dimensional Navier-Stokes solver.

In two dimensions, the continuous solution function to the Burgers PDE becomes vector-valued with two channels. Using a symbolic notation, we can write
\begin{equation}\label{eq:burgers-2d-symbolic}
    \frac{\partial \vu}{\partial t} + (\vu \cdot \nabla) \vu = \nu \Delta \vu.
\end{equation}
An alternative way to write the two-dimensional Burgers equation is in its two components
\begin{equation}\label{eq:burgers-2d-components}
    \begin{aligned}
        \frac{\partial u_1}{\partial t} + u_1 \frac{\partial u_1}{\partial x} + u_2 \frac{\partial u_1}{\partial y} &= \nu \left( \frac{\partial^2 u_1}{\partial x^2} + \frac{\partial^2 u_1}{\partial y^2}\right),
        \\
        \frac{\partial u_2}{\partial t} + u_1 \frac{\partial u_2}{\partial x} + u_2 \frac{\partial u_2}{\partial y} &= \nu \left( \frac{\partial^2 u_2}{\partial x^2} + \frac{\partial^2 u_2}{\partial y^2}\right).
    \end{aligned}
\end{equation}
Our domain \(\Omega\) is again the unit square.

Under a two-dimensional biperiodic domain, both the right-most and the top-most boundary nodes are eliminated, see \figref{fig:collocated-bi-periodic-grid}. We consider the unit-square \(\Omega = (0, 1)^2\) with equally many degrees of freedom per dimension, in total \(N^2\).

The Laplacian matrix in two dimensions can be written in a block structure using the one-dimensional Laplacian \(\tilde{\gL}_1\) and appropriately sized identity matrices \(\mI\)
\begin{equation}
    \gL_2 = \frac{1}{(\Delta x)^2}\begin{bmatrix}
        (\Delta x)^2\gL_1 - 2\mI & \mI & \mZero & \mZero & \dots & \mZero & \mZero & \mI\\
        \mI & (\Delta x)^2\gL_1 - 2\mI & \mI & \mZero & \dots & \mZero & \mZero & \mZero\\
        \mZero & \mI & (\Delta x)^2\gL_1 - 2\mI & \mI & \dots & \mZero & \mZero & \mZero\\
        \vdots & & & & \ddots  & & & \vdots \\
        \mI & \mZero & \mZero & \mZero & \dots & \mZero & \mI & (\Delta x)^2\gL_1 - 2\mI \\
    \end{bmatrix}.
\end{equation}
Note the identity blocks in the top right and bottom left corner.

Let \(\mathcal{F}_{2,1}\) and \(\gF_{2, 2}\) be the forward derivative operator in two dimensions in the direction of dimensions one and two, respectively. Moreover, \(\gB_{2,1}\) and \(\gB_{2,2}\) are the same for the backward derivative operator. Hence, we can build an upwinding operator for the first direction as
\begin{equation}
    \Gamma_{2,1}(\vw_h) = \text{diag}\left(\underbrace{\max\left(\frac{\vs_{-1}(\vw_{h,1}) + \vw_{h,1}}{2}, 0\right)}_{\text{positive winds}}\right) \mathcal{B}_{2,1}      +
    \text{diag}\left(\underbrace{\max\left(\frac{\vs_{1}(\vw_{h,1}) + \vw_{h,1}}{2}, 0\right)}_{\text{negative winds}}\right) \mathcal{F}_{2,1},
\end{equation}
which only needs the winds in direction one. Similarly, we get for the other direction
\begin{equation}
    \Gamma_{2,2}(\vw_h) = \text{diag}\left(\underbrace{\max\left(\frac{\vs_{-1}(\vw_{h,2}) + \vw_{h,2}}{2}, 0\right)}_{\text{positive winds}}\right) \mathcal{B}_{2,2}      +
    \text{diag}\left(\underbrace{\max\left(\frac{\vs_{1}(\vw_{h,2}) + \vw_{h,2}}{2}, 0\right)}_{\text{negative winds}}\right) \mathcal{F}_{2,2}.
\end{equation}
Let us combine these into one joint upwind discretization
\begin{equation}
    \Gamma_2(\vw_h) = \Gamma_{2,1}(\vw_h) + \Gamma_{2,2}(\vw_h)
\end{equation}

This allows for discretizing the component-wise  \eqref{eq:burgers-2d-components} first in space via the method of lines, afterward similarly in time as before to get the Oseen problem
\begin{equation}
    \begin{aligned}
        \frac{\vu_{h,1}^{[t+1]} - \vu_{h,1}^{[t]}}{\Delta t} + \Gamma_2(\vu_h^{[t]}) \vu_{h,1}^{[t+1]} &= \nu \gL_2 \vu_{h,2}^{[t+1]},
        \\
        \frac{\vu_{h,2}^{[t+1]} - \vu_{h,2}^{[t]}}{\Delta t} + \Gamma_2(\vu_h^{[t]}) \vu_{h,2}^{[t+1]} &= \nu \gL_2 \vu_{h, 2}^{[t+1]}.
    \end{aligned}
\end{equation}
Note that for the discretization of each component, we need both components from the previous time level to assemble the upwinding matrix.
We can re-arrange and write the system jointly as
\begin{equation}
    \underbrace{\begin{bmatrix}
        \mI + \Delta t \Gamma_2(\vu_{h}^{[t]}) - \Delta t \nu \mB_2 & \mZero \\
        \mZero & \mI + \Delta t \Gamma_2(\vu_{h}^{[t]}) - \Delta t \nu \mB_2 \\
    \end{bmatrix}}_{=\mA = \Lambda(\vu_h^{[t]})}
    \underbrace{\begin{bmatrix}
        \vu_{h,1}^{[t+1]} \\
        \vu_{h,2}^{[t+1]}
    \end{bmatrix}}_{=\vu_h^{[t+1]}}
    =
    \underbrace{\begin{bmatrix}
        \vu_{h,1}^{[t]} \\
        \vu_{h,2}^{[t]}
    \end{bmatrix}}_{=\beta(\vu_h^{[t]})}.
\end{equation}
The assembly routine for the system matrix yields a block-diagonal structure while the right-hand side assembly is just the identity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Coupled Navier-Stokes in 2D}\label{sec:solver-details-navier-stokes}

We solve the incompressible Navier-Stokes equations which can be written in symbolic notation as
\begin{align}\label{eq:navier-stokes}
    \frac{\partial \vu}{\partial t}
    +
    (
        \vu
        \cdot
        \nabla
    )
    \vu
    &=
    -
    \nabla
    p
    +
    \nu
    \nabla^2
    \vu,
    \\
    \nabla
    \cdot
    \vu
    &=
    0.
\end{align}
The domain is again unit-square with bi-periodic boundary conditions. This is a system of partial differential equations for three unknowns, which are the two components of the velocity \(\vu\) and the scalar pressure field \(p\). The pressure acts as a constraint to enforce incompressibility given by the second equation. The coupling of velocity and pressure can be challenging, but a simple way for a finite difference discretization is the staggered grid \citep{harlow1965numerical}. As depicted in \figref{fig:backward-staggered-bi-periodic-grid}, it also accounts for bi-periodicity by ignoring the top-most and right-most grid points but uses different locations to store the three unknowns. In this setting, there are equally many degrees of freedom per variable and direction. Each variable contributes \(N^2\) entries. Hence, in total, there are \(3N^2\).

Evaluating the convection term for both derivative directions requires mapping between the two staggered representations of the velocity grid. Let us denote the mapping operator \(\gM_1\) that moves all variables to the grid representation of the first velocity component. The operator \(\gM_2\) does the same for the second velocity component. These operators can be easily realized via bi-linear interpolation, which in the uniform cartesian grid simply amounts to the average of the four neighbors. Then the convection operator for velocity components one and two are \(\Gamma_2(\gM_1(\vu_h))\) and \(\Gamma_2(\gM_2(\vu_h))\), respectively.

The discretization of the pressure gradient and the velocity divergence requires a mapping between the velocity and the pressure representations. It turns out that a forward derivative \(\gF_{2, 1}\) or \(\gF_{2,2}\) also maps from the velocity representation to the pressure representation. Hence, we can define the divergence operator on velocity components as \(\mD_1 = \gF_{2,1}\) and \(\mD_2 = \gF_{2,2}\), respectively. Vice versa, the gradient operators mapping from pressure to velocity representations are the backward differences, i.e., \(\mG_1 = \gB_{2, 1}\) and \(\mG_2=\gB_{2, 2}\).

The method of line discretization with the linearization of convection matrices around the previous state in time then yields
\begin{align}
    \frac{\vu_{h,1}^{[t+1]} - \vu_{h,1}^{[t]}}{\Delta t}
    +
    \Gamma_2(\gM_1(\vu_h^{[t]})) \vu_{h,1}^{[t+1]}
    &=
    -
    \mG_1 \vu_{h,3}^{[t+1]}
    +
    \nu
    \gL_2
    \vu_{h,1}^{[t+1]}
    \\
    \frac{\vu_{h,2}^{[t+1]} - \vu_{h,2}^{[t]}}{\Delta t}
    +
    \Gamma_2(\gM_2(\vu_h^{[t]})) \vu_{h,2}^{[t+1]}
    &=
    -
    \mG_2 \vu_{h,3}^{[t+1]}
    +
    \nu
    \gL_2
    \vu_{h,2}^{[t+1]}
    \\
    \mD_1 \vu_{h,1}^{[t+1]} + \mD \vu_{h,2}^{[t+1]}
    &=
    \vzero
\end{align}
We can write this in matrix form as
\begin{equation}
    \underbrace{\begin{bmatrix}
    \mI + \Delta t \Gamma_2(\mathcal{M}_{1}(\vu_h^{[t]})) - \Delta t \nu \gL_2
    &
    \mZero
    &
    \mG_1
    \\
    \mZero
    &
    \mI + \Delta t \Gamma_2(\mathcal{M}_{2}(\vu_h^{[t]})) - \Delta t \nu \gL_2
    &
    \mG_2
    \\
    \mD_1
    &
    \mD_2
    &
    \mZero
    \end{bmatrix}}_{:= \mA = \Lambda(\vu_h^{[t]})}
    \underbrace{\begin{bmatrix}
        \vu_{h,1}^{[t+1]}
        \\
        \vu_{h,2}^{[t+1]}
        \\
        \vu_{h,3}^{[t+1]}
    \end{bmatrix}}_{\vu_h^{[t+1]}}
    =
    \underbrace{\begin{bmatrix}
        \vu_{h,1}^{[t]}
        \\
        \vu_{h,2}^{[t]}
        \\
        \vzero
    \end{bmatrix}}_{=\beta(\vu_h^{[t]})}
\end{equation}
which reveals the saddle-point nature of the coupled Navier-Stokes system given by the zero block in the bottom right \citep{turek1999efficient}.
Many popular solution techniques to the Navier-Stokes equations, like PISO \citep{issa1986computation}, can be interpreted as efficient preconditioners to this coupled system as argued in \citet{perot1993fractional}. For simplicity and to not get nested iterative linear solvers, we solve the coupled system without further modifications, which is reasonable for the low employed resolution despite the considerably high condition number.

Similar to the Burgers simulators, the system matrix \(\mA\) is asymmetric and needs to be re-assembled from information given by the previous state in time. However, only the previous velocity components in time \(\vu_{h,1}^{[t]}\) and \(\vu_{h,2}^{[t]}\) are needed. The previous pressure state \(\vu_{h,3}^{[t]}\) does not affect the matrix assembly. It is also not relevant for the right-hand side assembly function \(\beta(\cdot)\).

\begin{figure}
\centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \begin{tikzpicture}
            \path (0.0, -0.9) -- (0.0, 5.7); % This is just used to bring the two aligned figures on the same height
            \foreach \xtickPos in {0.0, 1.2, 2.4, 3.6, 4.8}{
                \draw (\xtickPos, 0.0) -- (\xtickPos, 4.8);
            }
            \foreach \ytickPos in {0.0, 1.2, 2.4, 3.6, 4.8}{
                \draw (0.0, \ytickPos) -- (4.8, \ytickPos);
            }
            \foreach \xtickPos in {0.0, 1.2, 2.4, 3.6}{
                \foreach \ytickPos in {0.0, 1.2, 2.4, 3.6}{
                    \node at (\xtickPos, \ytickPos) {\color{blue}\LARGE$\mathbf{\times}$};
                }
            }
            % \node[] (id) at (3, 5) {$\rightarrow$};
        \end{tikzpicture}
        \caption{Bi-periodic (collocated) grid. The top-most and right-most vertices have no degrees of freedom due to periodicity.}
        \label{fig:collocated-bi-periodic-grid}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
    \centering
        \begin{tikzpicture}
            \foreach \xtickPos in {0.0, 1.2, 2.4, 3.6, 4.8}{
                \draw (\xtickPos, 0.0) -- (\xtickPos, 4.8);
            }
            \foreach \ytickPos in {0.0, 1.2, 2.4, 3.6, 4.8}{
                \draw (0.0, \ytickPos) -- (4.8, \ytickPos);
            }
            % Pressure nodes
            \foreach \xtickPos in {0.6, 1.8, 3.0, 4.2}{
                \foreach \ytickPos in {0.6, 1.8, 3.0, 4.2}{
                    \node at (\xtickPos, \ytickPos) {\color{red}\LARGE$\mathbf{\circ}$};
                }
            }
            % x-velocity nodes
            \foreach \xtickPos in {0.0, 1.2, 2.4, 3.6}{
                \foreach \ytickPos in {0.6, 1.8, 3.0, 4.2}{
                    \node at (\xtickPos, \ytickPos) {\color{blue}\LARGE$\mathbf{\rightarrow}$};
                }
            }
            % y-velocity nodes
            \foreach \xtickPos in {0.6, 1.8, 3.0, 4.2}{
                \foreach \ytickPos in {0.0, 1.2, 2.4, 3.6}{
                    \node at (\xtickPos, \ytickPos) {\color{orange}\LARGE$\mathbf{\uparrow}$};
                }
            }
        \end{tikzpicture}
        \caption{Backward Staggering for bi-periodic domain. Horizontal arrows denote the location degrees of freedom for the x-velocity are saved. Vertical arrows correspond to y-velocity. Red circles are for pressure.}
        \label{fig:backward-staggered-bi-periodic-grid}
    \end{subfigure}
    \caption{Arrangement of degrees of freedom on two-dimensional bi-periodic domains.}
    \label{fig:grid-arrangements}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{More Details on PRDP Parameters}\label{sec:prdp-more-details}

\paragraph{Stepping Threshold \(\tau_{\text{step}}\)}
The stepping criterion defines the threshold at which the validation metric is considered sufficiently flat before the physics is refined. This value should not exceed 1, as values greater than 1 would trigger refinement only when the error increases. Higher values (less than 1) allow the metric to be flatter before the physics is refined. In other words, a lower value enables faster progressive refinement and trades off PR savings. Refinement can also be made faster by making larger steps in refinement \(\Delta K\).  In practice, we found that \(\tau_\text{step}\) values between 0.9 and 1 with \(\Delta K = 1\) sufficed in all our network training runs.

\paragraph{Stopping Threshold \(\tau_{\text{stop}}\)}
The stopping criterion on the checkpoint ratio governs how strictly the stagnation in network accuracy should be over different refinement levels. 
A lower value implies a stricter check, and progressive refinement would stop in a lower $K_\text{max}$ region, and vice versa. 
In other words, a lower value of $\tau_\text{stop}$ enables a more aggressive strategy for IC savings. 

The grace window of intervals \(\delta\) controls the length of lookback to calculate the stepping ratio \(r\). This is helpful in the case of strong epoch-to-epoch oscillations. On the other hand, the effect of oscillations on the stepping ratio is mitigated through exponential smoothing. We found a window of 3 worked for most scenarios. The grace window goes hand-in-hand with the stepping criterion - a longer window requires stricter (smaller) $\tau_\text{step}$ values.


% TODO, we have more details, maybe also ablational studies?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\section{Experimental Setups}
\label{sec:experimental-setups}

An algorithmic description of Progressively Refine Differentiable Physics is given in algorithm \ref{alg:prdp}. Our choice of PRDP parameters based on section \ref{sec:prdp-more-details}  are listed in each problem setup as follows.
Our scheduling is via changing the number of inner iterations via \(\Delta K\) each time PRDP triggers a refinement. In \figref{fig:residuum_and_iterations}, we demonstrate that this is identical to scheduling with relative tolerance thresholds.

\paragraph{Validation Metric}
For test cases based on the Heat diffusion, Burgers equation, and Navier Stokes, the validation metric we use for PRDP is the solution's error against a reference solution at a specific time step $t$, normed over space, and normalized against the reference solution. This error is mean-squared over the validation set. Mathematically, this can be written as

\begin{equation}
 \text{Validation Error} = \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}} 
 \left( \frac{\|\bm{u}_i^{[t]} - \bm{u}_{i}^{[t],\text{r}}\|_2}
 {\|\bm{u}_{i}^{[t],\text{r}}\|_2} \right)^2
\label{eq:validation-metric}
\end{equation}
where:
\begin{itemize}
    \item $\bm{u}_i^{[t]}$: the predicted solution at time step $t$ for validation example $i$,
    \item $\bm{u}_{i}^{[t],\text{r}}$: the reference solution at time step $t$ for validation example $i$,
    \item $\|\cdot\|$: the norm over the spatial domain,
    \item $\mathcal{V}$: the validation set.
\end{itemize}

The corresponding specifics for the Poisson inverse problem are explained in \ref{sec:poisson-exp-setup}.

\paragraph{Seed statistics}
For the neural network training setups, we conducted trials with 10 different initialization seeds.  Each run uses a different network initialization and a different stochastic minibatching, but the same data.
The results shown in the work visualize the mean over these runs (shown as solid lines), along with its variability (represented by the shaded area). The shaded areas indicate the range within one standard deviation of the mean.


\begin{figure}
    \begin{center}
    \includegraphics[width=0.5\textwidth]{plots/residuum_ns_spatial_sep19_tolerances.pdf}
    \end{center}
    \caption{PRDP refines the physics by progressively increasing the solver iterations throughout training until training accuracy in terms of a validation metric stagnates.
    Progressively increasing the number of iterations is equivalent to progressively decreasing the tolerance threshold, as shown here for the example of the training the Navier-Stokes emulator of \secref{sec:experiments-navier-stokes},
    % \kanishk{Question: show only one plot to demonstrate the basic idea, or show this plot for all test problems?}
    % \ntcomment{This graph is difficult to parse, because the PRDP lines are mirrored; maybe split into 2 because you dont have problems with space so far?}
    }
    \label{fig:residuum_and_iterations}  
    % todo: split n_iter and residuum to separate plots. Move them to appendix, or add a paragraph that refers to it
\end{figure}

\begin{algorithm}
    \caption{Determine Whether to Refine Physics}
    \KwIn{\(\tau_{\text{step}}=0.95\), \(\tau_{\text{stop}}=0.9\), \(\delta=3\), \(\Delta e=6\)}
    \KwData{\(\{L^{[e]}_{\text{val}}\}_e\) %\mlcomment{Validation Metric after each epoch so far}}
    % \KwIn{\(\tau_{\text{step}} = 0.98\), \(\tau_{\text{stop}} = 0.9\), \(\Delta e = 6\), \(\delta = 3\)}
    % \KwData{\(\{L^{[e]}_{\text{val}}\}_e\) %\mlcomment{Validation Metric after each epoch so far}}
    % , \(c = 100.0\) %\mlcomment{Checkpoint of the last refinement plateau, init as high value}}
    }
    \KwOut{Boolean indicating whether to refine}

    % \If{\(|\{L^{[e]}_{\text{val}}\}_e| < \delta\)}{
    %     \Return False \mlcomment{Collect at least as many val metrics as grace epochs}
    % }

    \(\{\tilde{L}^{[e]}_{\text{val}}\}_e = \text{ema}(\{L^{[e]}_{\text{val}}\}_e, \Delta e)\) \mlcomment{Smooth with Exponential Moving Average} \\
    \(r = \tilde{L}^{[-1]}_{\text{val}} / \tilde{L}^{[-\delta]}_{\text{val}}\) \mlcomment{Relative change to \(\delta\) epochs prior}

    \If{\(r > \tau_{\text{step}}\)}{
        \(r_c = \tilde{L}^{[-1]}_{\text{val}} / c \) \mlcomment{Relative change to previous plateau}

        \If{
        \(r_c < \tau_{\text{stop}} \vee r_c > 1.0\)
        }{
            \(c \leftarrow \tilde{L}^{[-1]}_{\text{val}}\) \mlcomment{Save current plateau}\\
            \Return True \mlcomment{Refine!}
        }\Else{
            \Return False \mlcomment{Do not further refine because we reached \(K_{\text{max}}\), \(\implies\) yields IC savings}
        }
    }\Else{
        \Return False \mlcomment{Do not refine to keep coarse physics for economic reasons, \(\implies\) yields PR savings}
    }
    \label{alg:prdp}
\end{algorithm}



%%%%%%%%%%%%%%%%%%%%%%

\subsection{Poisson Equation - Inverse Problem}
\label{sec:poisson-exp-setup}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/poisson_1p_3p_example.pdf}
    \caption{The Poisson PDE can be interpreted as the deformation of a thin string subject to a load. We consider two scenarios. In (a), the load (=right-hand side) only consists of one sine mode whose amplitude is scaled by a single (1P) \(\theta\) parameter. The second case (b) uses the first three sine modes with a parameterized amplitude each.}
    \label{fig:poisson-qualitative-example}
\end{figure}

The Poisson inverse problem is our simplest test example that incorporates differentiable physics into a learning pipeline for a doubly convex problem optimizing in a
low-dimensional parameter space.
We set up the discretized Poisson problem as described in section \ref{sec:poisson-exp-setup} with degrees of freedom $N=30$. The resulting linear system is parameterized, similar to section \ref{sec:preliminaries}. The iterative linear solver applied to this system represents the physics $\mathcal{P}_K$ in this problem.

For simplicity, we keep the system matrix $\mA$ constant and only parameterize the right hand side $\vb_h = \beta(\theta)$. The map $\beta$ is a sum of the first $P$ sine modes in the unit interval whose amplitudes are given by the parameter vector $\theta \in \mathbb{R}^P$.
\begin{equation*}
    \beta(\theta) = \sum_{i=1}^{P} \theta_i \text{sin} (2i \pi x)
\end{equation*}
We design two inverse problems, one with a single sine mode ($P=1$), and one with three sine modes ($P=3$). A qualitative example is given in \figref{fig:poisson-qualitative-example}.

The outer optimization's objective is the MSE (mean-squared error)
in the physics solution of $\vu_h^{[K]}$ against a reference solution $\vu_{h,\text{r}}$. 
The reference is generated by direct solution of the linear system at a reference parameter value $\theta_\text{r}$. In other words, $\vu_{h,\text{r}} = \mA^{-1} \vb_{h,r}$, where $\vb_{h,r} = \beta (\theta_{\text{r}})$. Optimization is performed using gradient descent algorithm.

\paragraph{One-dimensional parameter space}

We use $\theta_\text{r} = 2.0$, and an initial guess for gradient descent $\theta_\text{init} = 5.0$. 170 update steps are performed with a constant learning rate of 275. For PRDP, we set the control parameter values to  $\tau_\text{step} = 0.92$, $\tau_\text{stop} = 0.98$, $\delta = 2$ . Training was started with $K_0=25$ linear solver iterations. At every refinement, it was incremented by $\Delta K = 10$. A relatively high value for $\Delta K$ and a aggressive refinement strategy with relatively smaller value of $\tau_\text{step}$ are suitable for cases where $K_{\max}$ is significantly higher than the number of outer iterations. In this case, the physics converges at $K_\epsilon = 600$ (which is also $K_\text{max}$ due to the double-convex problem). PRDP was successful at enabling 33\% solver iteration savings

\paragraph{Three-dimensional parameter space}

We use $\theta_\text{r} = [0.62, 1.86, 5.1]$, and an initial guess for gradient descent $\theta_\text{init} = [3.0, 3.0, 3.0]$. 750 update steps are performed when using Jacobi as the inner problem and 800 when using steepest descent, with a constant learning rate of 3500. The physics converged with 450 or 240 inner iterations when using Jacobi or steepest descent, respectively. In this case, a less aggressive PRDP strategy was suitable since the update steps were much higher than $K_\epsilon$. Hence, we found control parameter values  $\tau_\text{step} = 0.99$, $\tau_\text{stop} = 0.99$, $\delta = 2$, with  $\Delta K=2$ and $K_0=1$ was suitable. PRDP was successful at enabling 40-50\% savings for both Jacobi and steepest descent with both unrolled and implicit differentiation.

% No exponential averaging was used because the error curve had no oscillations, as is typical of convex-convex optimization without momentum and minibatching. 

The results of these experiments are presented in \figref{fig:poisson_all-results}.

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{plots/poisson/poisson_1_param_jacobi_unrolled.pdf}
        \caption{1 P, Jacobi, Unrolled diff.}
        \label{fig:poisson-1-jacobi-unrolled}
    \end{subfigure}
    \hspace{0.05\linewidth}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{plots/poisson/poisson_1_param_jacobi_implicit.pdf}
        \caption{1 P, Jacobi, Implicit diff.}
        \label{fig:poisson-1-jacobi-implicit}
    \end{subfigure}
    
    \vspace*{0.8cm}
    
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{plots/poisson/poisson_3_param_jacobi_unrolled.pdf}
        \caption{3 P, Jacobi, Unrolled diff.}
        \label{fig:poisson-3-jacobi-unrolled}
    \end{subfigure}
    \hspace{0.05\linewidth}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{plots/poisson/poisson_3_param_jacobi_implicit.pdf}
        \caption{3 P, Jacobi, Implicit diff.}
        \label{fig:poisson-3-jacobi-implicit}
    \end{subfigure}

    \vspace*{0.8cm}
    
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{plots/poisson/poisson_3_param_SD_unrolled.pdf}
        \caption{3 P, Steepest Descent, Unrolled diff.}
        \label{fig:poisson-3-sd-unrolled}
    \end{subfigure}
    \hspace{0.05\linewidth}
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{plots/poisson/poisson_3_param_SD_implicit.pdf}
        \caption{3 P, Steepest Descent, Implicit diff.}
        \label{fig:poisson-3-sd-implicit}
    \end{subfigure}

    \caption{Results of the Poisson equation inverse problem visualizing the parameter error during training and the savings enabled by PRDP for both the 1 parameter and 3 parameter setup using different combinations of physics solvers (Jacobi, steepest descent) and solver differentiation methods (unrolled, implicit).}
    \label{fig:poisson_all-results}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%

\subsection{Heat Diffusion - Autoregressive Neural Emulator Training} \label{sec:heat-1d-exp-setup}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/heat_1d_example.pdf}
    \caption{Example of three different trajectories over three temporal snapshots for the one-dimensional heat equation.}
    \label{fig:heat-1d-qualitative-example}
\end{figure}
In this setup, a neural emulator learns the diffusion equation solvers presented in sections \ref{sec:solver-details-diffusion-1d} and \ref{sec:solver-details-diffusion-2d}.
% PDE params
For the physics, we use a diffusivity of \(\nu = 0.001\) and a time step size of $\Delta t = 1$, see \eqref{eq:heat-1d-and-discrete}.
% N_dof
We discretize the space,  into 32 grid points (including boundaries) in each dimension, which, after applying homogeneous Dirichlet boundary conditions, results in 30 and 900 degrees of freedom in the 1D and 2D cases, respectively. In the 3D case, we have 22 grid points including boundaries, i.e. 20 interior grid points, corresponding to 8000 degrees of freedom.
% Linear solvers
The resulting linear system is solved using the Jacobi method. This constitutes the physics operator $\mathcal{P}_K$. We employ our own implementations of the Jacobi solver.
% IC
The initial conditions are generated as a truncated Fourier series. For 1D, we sum the first 5 sine and cosine modes defined on the unit interval. For 2D, we use the products of the first 5 sine and cosine modes defined as:
\begin{equation*}
\begin{split}
    u_0(x,y) = \sum_n ( a_n \sin(2n \pi x) \sin(2n \pi y) +  b_n \cos(2n \pi x) \cos(2n \pi y) + \\
    c_n \sin(2n \pi x) \cos(2n \pi y) + d_n \cos(2n \pi x) \sin(2n \pi y) )
\end{split}
\end{equation*}
We similarly extend this procedure to 3D.
All amplitudes are randomly sampled from a uniform distribution $\mathcal{U}(-1, 1)$. A qualitative example of the dynamics is given in \figref{fig:heat-1d-qualitative-example} for 1D, \figref{fig:heat-2d-qualitative-example} for 2D, and \figref{fig:heat-3d-qualitative-example} for 3D.

% Model to be trained
\paragraph{Training Procedure}
We intentionally chose simple architectures as the focus of this work is not architecture but training methodology. 
For 1D and 2D cases, a multilayer perceptron (MLP)  $\vf_{\theta}$ is trained to emulate $\mathcal{P}_K$. For the 1D case, the MLP has 3 hidden layers, each of width 64, and for the 2D case, 3 hidden layers each of width of 3000. Corresponding to the physical problem's degrees of freedom, it has 30 input and 30 output nodes (900 for 2D). 
For the 3D case, we train a classic ResNet that supports 3 dimensional input and output tensors with 1 channel. It contains 6 blocks, each with 32 hidden channels. Its convolutions implement homogeneous Dirichlet boundary conditions. All networks are initialized with random weights.
Non-linearization is enabled by applying ReLU activation after each hidden layer.

% Loss
The compute graph consists of the neural emulator  $\vf_{\theta}$  stepping from an initial condition to the first time step solution, and the physics operator $\mathcal{P}_K$ stepping from the neural network's solution to the solution at the second time-step.
The nMSE between this solution at the second time-step against a reference solution forms the loss function for training. This \textit{mixed-chain approach} (depicted in figure \ref{fig:mixed-chain-setup} allows gradients to flow through $\mathcal{P}_K$ and lets the network training to be informed with the physics's dynamics. 
We use implicit differentiation for the 1D case, and unrolled differentiation for the 2D and 3D cases. The custom VJP rules of the physics use the same iterative linear solvers as the primal solve and are resolved to the same number of iterations as the primal, as explained in section \ref{sec:linear-solver-vjp}.

% Optimizer
For the outer (training) problem, we use the Adam optimizer from the Optax library \citep{deepmind2020jax}. The learning rate is scheduled as exponentially decaying with an initialization of $10^{-3}$, decay rate of $0.94$ for 1D ($0.9$ for 2D), and $100$ transition steps, while for 3D, the initialization is $10^{-4}$, with a decay rate of $0.92$ and $100$ transition steps. We train in mini-batches of 25 samples per iteration and for a total of 70 epochs in the 1D case, and 100 epochs in the 2D and 3D cases.

% Data
\paragraph{Data Generation} The reference solution at the second time-step is computed by two applications of the physics operator on the initial condition, where the iterative solver is replaced by a direct solver. Hence, the iterates $\{u^{[2][k]}\}_{k=0}^{K}$ converge to the reference solution. 205 samples are generated for training, with a train:validation split of 200:5.

% PRDP
\paragraph{PRDP} PRDP is controlled using the validation metric of \eqref{eq:validation-metric} computed at $t=2$, and refinement is applied to the iterative linear solver $\mathcal{P}_K$ that steps from $u^{[1]}$ to $u^{[2]}$. For the 1D case, we use parameter values $\tau_{\text{step}}=0.98$, $\tau_\text{stop}=0.9$, $\delta=3$, exponential smoothing is performed over a time window of $8$. The 2D case uses fewer epochs, hence a less aggressive refinement using $\tau_{\text{step}}=0.9$ with a smoothing window of 6 was suitable. In the 3D case, which uses 100 epochs, used a less aggressive refinement with $\tau_{\text{step}}=0.97$, similar to the 1D case, along with $\tau_\text{stop}=0.9$, $\delta=3$, exponential smoothing is performed over a time window of $6$.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/heat_2d_example.pdf}
    \caption{Example of six different trajectories over three temporal snapshots for the two-dimensional heat equation.}
    \label{fig:heat-2d-qualitative-example}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/heat_3d_example.pdf}
    \caption{Example of six different trajectories over three temporal snapshots for the three-dimensional heat equation.}
    \label{fig:heat-3d-qualitative-example}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%

\subsection{Burgers Equation - Autoregressive Neural Emulator}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/burgers_1d_example.pdf}
    \caption{Three example trajectories of the one-dimensional Burgers equation.}
    \label{fig:burgers-qualitative-example}
\end{figure}

% PDE, Boundary con., Ndof
We solve the discretized Burgers equation presented in section \ref{sec:solver-details-burgers-1d} on the unit interval with $\nu=0.001$ and time step size $\Delta t = 0.01$.  The spatial domain is discretized into 257 points including the boundaries, which results in $N=256$ degrees of freedom when applying a periodic boundary condition. 
% Linear solver
Since the system matrix $\mA$ is now asymmetric, we use the GMRES linear solver (\ref{sec:gmres}) with restart set to 2. Although much higher restarts are possible and preferred in practice, we used fewer restarts to emphasize the difference between network accuracies trained using different number of GMRES iterations since our test problem was relatively easy to solve.

% IC
\paragraph{Data Generation} The initial conditions are generated as a truncated Fourier series as described in section \ref{sec:heat-1d-exp-setup}. The first 20 sine and cosine modes defined over the unit interval are summed, with their amplitudes sampled from a uniform distribution \textbf{$\mathcal{U}(-1, 1)$}. Qualitative example trajectories are given in \figref{fig:burgers-qualitative-example} 

% model to be trained
\paragraph{Training Procedure} We train a neural network $\vf_{\theta}$ to learn the Burgers stepper which represents the physics $\mathcal{P}$. We use a classic ResNet architecture \citep{he2016resnet} consisting of 6 blocks, each with 32 hidden channels, and ReLU activation. The convolutions implement the physics' periodic boundary condition by circular padding. The weights are randomly initialized. We use our own implementation of the architecture using the Equinox library \citep{kidger2021equinox}.

% loss
The loss function for training the network is defined similarly to the mixed-chain setup of section \ref{sec:heat-1d-exp-setup}. The ResNet steps from an initial condition to the first time step solution, and the physics $\mathcal{P}_K$ steps from the first to the second time-step solution.
The nMSE error between this solution at the second time-step against a reference solution forms the loss function for training.

% data
The reference solution at the second time-step is computed by two applications of the physics, with a direct solver replacing the iterative solver. 205 samples are generated for training, with a train:validation split of 200:5.

% optimizer
We use the Adam optimizer from the Optax library, with an exponential learning rate decay using an initialization of $10^{-3}$, a decay rate of $0.7$, and $100$ transition steps. We train in mini-batches of 25 samples per iteration.

% PRDP
\paragraph{PRDP} PRDP is controlled using the validation metric of \eqref{eq:validation-metric} computed at $t=5$, and refinement is applied to the iterative linear solver $\mathcal{P}_K$ that steps from $\vu^{[1]}$ to $\vu^{[2]}$. This presents an interesting application scenario, where the training objective uses a relatively short temporal rollout of 2 steps while PRDP favors generalization over a longer rollout at the fifth time step.
We use PRDP parameter values $\tau_\text{step}=0.9$, $\tau_\text{stop}=0.9$, and $\delta=3$. Due to high oscillations in the validation metric, we used exponential smoothing with a
% relatively
longer window of $15$.

%%%%%%%%%%%%%%%%%%%%%

\subsection{Navier Stokes - Neural-Hybrid Corrector Learning}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/ns_2d_example.pdf}
    \caption{Three example trajectories of the Navier-Stokes scenario with the two velocity components each.}
    \label{fig:ns-qualitative-example}
\end{figure}

In this example, we train a network to learn to correct for discretization errors in the solver described in section \ref{sec:solver-details-navier-stokes}. Such neural-hybrid methods apply to correction learning setups as investigated by \cite{thuerey2020SIL}. Our problem setup is visualized in \figref{fig:solver-in-the-loop-setup}.

We distinguish between a spatially-coarse solver, $\mathcal{P}_{s,K}$, and a spatially-fine solver, $\mathcal{P}_{r}$ (following the terminology of \emph{source} and \emph{reference} from \cite{thuerey2020SIL}). In our test problem, $\mathcal{P}_{s,K}$ operates on a solution manifold that has half the spatial resolution of $\mathcal{P}_{r}$. We use 97 spatial degrees of freedom for $\mathcal{P}_{r}$ (i.e., a total of 291 degrees of freedom considering the three components of $\vu$ ), and 48 for  $\mathcal{P}_{s,K}$ (i.e., a total of 144). Both solvers are set up for $\nu=0.0001$ and $\Delta t = 0.1$ and employ the GMRES solver (\secref{sec:gmres}) with restart set to 8.

% IC
\paragraph{Data generation} To generate initial conditions for the data, we initialize a solution field in the manifold of $\mathcal{P}_r$ with each component sampled from the standard normal distribution. This random field is passed through a low pass filter, then normalized to have zero mean and standard deviation of 1, and finally passed through an incompressibility projection, giving  $u_r^{[0]}$. Data trajectories $\{ u_r^{[t]} \}_{t=0}^T$ are produced by repeated applications of $\mathcal{P}_r$ (uses a fully-converged GMRES) to $u_r^{[0]}$. These trajectories are then downsampled to the source manifold  $\{ u_s^{[t]} \}_{t=0}^T$. 205 trajectories are generated for training, with a train:validation split of 200:5. Qualitative example trajectories are given in \figref{fig:ns-qualitative-example}.

% model to be trained
\paragraph{Training Procedure} The neural-hybrid model operates in the source manifold. We use a classic ResNet architecture \citep{he2016resnet} consisting of 3 blocks, each with 64 hidden channels. The convolutions implement the physics' periodic boundary condition by circular padding. The weights are randomly initialized. We implement the network architecture using the Equinox library \citep{kidger2021equinox}.

% loss
The loss function is defined as the sum of MSE errors of the first and second time-step solutions.
% optimizer
We use the Adam optimizer from the Optax library, with a cosine learning rate decay schedule using an initialization of $10^{-3}$ and $\text{decay\_steps} = 800$. We train in mini batches of 25 samples per iteration and for a total of 100 epochs, i.e., 800 update steps.

% PRDP
\paragraph{PRDP} PRDP is controlled using the validation metric
of \eqref{eq:validation-metric} computed on the x-velocity at $t=5$. We used parameter values $\tau_\text{step}=0.98$, $\tau_\text{stop}=0.9$, and $\delta=3$. For the exponential smoothing, we used a window size of 6. Since this problem setup has 2 executions of $\mathcal{P}_K$ (refer \figref{fig:solver-in-the-loop-setup}), three options arise for applying progressive refinement - a) progressively refining only the first execution, b) progressively refining only the second execution, and c) progressively refining both executions. We opted for c in order to gain savings from both solvers. In general, correction learning setups would have as many physics executions as the number of temporal
% rollouts
unrolling steps
in the loss function.

%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{visualizations/poisson_inverse_problem.pdf}
    \caption{The compute graph and the reverse pass associated with the Poisson inverse problem.}
    \label{fig:poisson-inverse-setup}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{visualizations/mix-1-1-false.png}
    \caption{In the \emph{mixed-chain approach} an emulator prediction is chained with an application of the differentiable solver, effectively to the training loss computed over a two step unrolling. Reference data is computed with full refinement, only the physics of the second step solver is refined with PRDP. The yellow reversely pointing error indicates the differentiable physics over the approximate solver.}
    \label{fig:mixed-chain-setup}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.3\textwidth}
    \centering
        \includegraphics[width=\textwidth]{visualizations/sequential-corrector-with-bypass-full-adjoint.png}
        \caption{Sequential Correction Setup}
        \label{fig:sequential-corrector}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.65\textwidth}
    \centering
        \includegraphics[width=\textwidth]{visualizations/sup-2-true-full_gradient.png}
        \caption{Two-step supervised unrolling}
    \end{subfigure}
    \caption{The neural hybrid emulator is at the place of "NN" in the right figure. Each backpropagation-through-time also triggers the differentiation over the physics. Note that even if there is no differentiation over the first neural-hybrid emulator application (i.e., no differentiation over the first coarse solver usage), it is also affected by PRDP because we also limit its exactness in the primal execution. Here, one could think of it as "Progressively Refined Physics" instead of "Progressively Refined \emph{Differentiable} Physics".}
    \label{fig:solver-in-the-loop-setup}    
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage{}

\section{Supplementary Experimental Results}

\subsection{PRDP Savings in terms of compute time}

In \figref{fig:time-all}, we present wall-clock training times with PRDP and with a fully converged differentiable solver. For the smaller 1D problems with small system matrices of the order of $\approx 100 \times 100$, other overhead in the compute graph dominates the training runtime. As such, the reduction in inner iterations does not translate to savings in wall clock time. Starting with 2D problems leading to larger matrices, decreases in training time converge to the reductions in inner iterations. For example, in the 3D heat emulator scenario, 81\% reduction in inner iteration count aligned with wall clock time savings of 78\%.
% In conclusion, for sufficiently large problems, the iterative linear solvers in differentiable physics setups can dominate the training cost
As the share of iterative linear solves in the total training cost increases, e.g. in high-dimensional problems, so do the savings enabled by PRDP.

\begin{figure}[b!]
    \centering
    \includegraphics[width=\linewidth]{plots/time_all.pdf}
    \caption{While all experiments reduced the number of inner iterations across the outer optimization (see \secref{sec:experiments} bar charts), savings in training time become noticeable the larger and more complex the setup gets. Once the problem is of sufficient size, linear solve times dominate the training compute graph, and hence, PRDP can deliver the highest amount of savings in wall clock time. Eventually, savings in training compute time converge against the reductions in inner iterations.}
    \label{fig:time-all}
\end{figure}

% \kanishk{
% To be noted that time measurements (a) use jit compiled update function and exclude jit compilation times, (b) include the validation error computation time in both baseline and PRDP runs.
% }

%%%%%%%%%%%%%%%%%%%%%


\subsection{Neural Network performance - PRDP vs. Baseline}

The PRDP method is aimed at reducing the end-to-end computational cost of training neural networks with physics solvers in the loop, without significantly affecting the trained neural network's performance. This is depicted qualitatively in figure \ref{fig:outer}. In table \ref{tab:experimental-results-numeric-value}, we provide the mean validation error values at the last epoch of training along with standard deviation over 10 seeds used for network initialization and stochastic minibatching.

\begin{table}[b!]
    \caption{Numeric values of the validation accuracy of the neural emulators and neural-hybrid emulators according to the experiments of \secref{sec:experiments}. The mean and standard deviation are computed based on ten different random seeds that affected neural network initialization and stochastic minibatching.}
    \label{tab:experimental-results-numeric-value}
    \begin{tabular}{c c c c}
        \toprule
        Outer training task & Inner physics & Val. error with Converged Physics & Val. error with PRDP \\
                            &               & ($\times 10^{-2}$)                & ($\times 10^{-2}$) \\
        \midrule
        % Felix: below, I listed the p-values associated with the t-test of PRDP against Converged Physics with the alternative that PRDP has a higher mean (In summary, we would have to reject the null hypothesis for Heat 3D and Navier-Stokes if our preselected significance threshold was 0.05)
    Neural Emulator & Heat 1D & \(2.1 \pm 0.39\) & \(2.1 \pm 0.38\) \\ % p-value: 0.5
    Neural Emulator & Heat 2D & \(1.4 \pm 0.08\) & \(1.1 \pm 0.06\) \\ % p-value: 0.999
    Neural Emulator & Heat 3D & \(0.4 \pm 0.09\) & \(0.5 \pm 0.10\) \\ % p-value: 0.015
    Neural Emulator & Burgers & \(5.5 \pm 1.38\) & \(6.2 \pm 2.13\) \\ % p-value: 0.197
    Neural-Hybrid Emulator & Navier Stokes & \(0.6 \pm 0.05\) & \(0.7 \pm 0.04\) \\  % p-value: 5.3e-5
        \bottomrule
    \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%

\clearpage{}

\subsection{PRDP Savings vs. Neural Network Expressiveness}

%%% Old Version
% \kanishk{While we could not isolate the single cause for the IC savings (existing once the outer problem is the learning in the high-dimensional nonconvex weight space of a neural network), we hypothesized some factors in the last paragraph of \secref{sec:motivate-ic-savings}. One was the inherently approximative nature of machine learning models. This could imply that a limited expressiveness, i.e., an insufficient model size, causes the IC savings. In other words, a more capable model would produce less IC savings.}

% \kanishk{To ablate this, we increased the MLP emulator's parameterize size for the 1D Heat example of \secref{sec:experiments-heat} by one order of magnitude and re-execute a training with PRDP. As expected with larger model, the accuracy increases. Here, we observe a convergence rate of approximately $n^{-1/5}$. On the other, the change in parameter savings only marginally affected the savings achieved by PRDP. The slightly higher savings for the largest model are likely because it triggers one fewer refinement level.}


%%% ChatGPT improved version
While we could not identify a single cause for the IC savings observed when solving the outer problem of learning in the high-dimensional, nonconvex weight space of a neural network, we hypothesized several contributing factors in the last paragraph of \secref{sec:motivate-ic-savings}. One hypothesis is that the inherently approximative nature of machine learning models plays a role. Specifically, limited model expressiveness—such as insufficient parameterization—might contribute to IC savings. In this view, a more expressive model would result in reduced IC savings.

To test this hypothesis, we conducted an ablation study by increasing the parameter count of the MLP emulator for the 1D Heat example in \secref{sec:experiments-heat} by an order of magnitude and retraining with PRDP. As expected, the larger model achieved higher accuracy, with a convergence rate of approximately $n^{-1/5}$. However, the impact on IC savings was minimal. Interestingly, the largest model exhibited slightly higher savings, likely due to triggering one fewer refinement level during training.
Nonetheless, PRDP consistently yields savings of around 80\% in terms of solver iterations across the varying network sizes.

% \kanishk{In \figref{fig:expressiveness} we present the results of an ablation study over the neural network size and how this affects its validation performance and the savings achievable with PRDP. For this study, we used the 1D heat equation neural emulator learning setup from section \ref{sec:experiments-heat}.}

% the lower PR savings a caused by the larger model triggering one fewer refinement level

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{plots/expressiveness_heat_1d.pdf}
    \caption{As expected with neural networks, the validation error falls over the number of parameters due to a larger network indicating a higher expressiveness. However, PRDP savings remain unaffected.}
    \label{fig:expressiveness}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%


\subsection{PRDP based on a validation metric vs. on training loss}

% \begin{wrapfigure}{o}{0.45\textwidth}
% \vspace{-0.41cm}
%     \includegraphics[width=\linewidth]{plots/heat_1d__n_params_vs_error.pdf}
%     \caption{\kanishk{TODO}}
%     \label{fig:nn-expressiveness}\vspace{-0.21cm}
% \end{wrapfigure}

\begin{figure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/heat2d_val_prdp.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/heat2d_train_prdp.png}
        \caption{Neural emulator learning for 2D heat equation: PRDP based on validation error (top) vs. training loss (bottom)}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/heat3d_val_prdp.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/heat3d_train_prdp.png}
        \caption{Neural emulator learning for 3D heat equation: PRDP based on validation error (top) vs. training loss (bottom)}
    \end{subfigure}
\end{figure}

\begin{figure}
\ContinuedFloat
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/burgers_val_prdp.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/burgers_train_prdp.png}
        \caption{Neural emulator learning for Burgers equation: PRDP based on validation error (top) vs. training loss (bottom)}
    \end{subfigure}

    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/navierstokes_val_prdp.png}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/prdp_on_training_loss/navierstokes_train_prdp.png}
        \caption{Neural emulator learning for Navier Stokes equation: PRDP based on validation error (top) vs. training loss (bottom)}
    \end{subfigure}
    \caption{PRDP based on training loss also successfully trains the network but leads to a slower refinement strategy since the training loss oftentimes continues to reduce even if the validation metric plateaued.}
    \label{fig:train_loss_prdp}    
\end{figure}


%%% Initial Version
% We motivated Progressively Refined Differentiable Physics by how different refinement levels $K$ lead to different plateaus in a performance metric other than the training loss. For example, this is visible in the \figref{fig:poisson_1_pr-savings} (a) and \figref{fig:ic-savings_heat-1d} (a). Naturally, the PRDP control algorithm economically increases the refinement levels. This stepping can be intuitively seen in \figref{fig:outer} (a) \& (b). 

% The control algorithm as introduced in \ref{sec:prdp} requires a performance metric whose plateuing leads to a refinement in the physics. In the main text, our natural choice was the validation error because it clearly plateaued for refinement levels $K< K_{\text{max}}$. In this section, we ablate this choice and instead have PRDP run on the training loss. We present the results in \figref{fig:train_loss_prdp}. Surprisingly, also the training loss can be used for PRDP. Since the training loss might converge further while the validation loss has already plateaued, running PRDP on training loss naturally is more aggressive. While ultimately delivering about the same IC savings, a more aggressive strategy leads to slightly higher PR savings.

% Albeit this ablation reveals that PRDP is also applicable on the training loss without sacrifices in final accuracy and even similar PRDP savings, we do not recommend it. For more general settings beyond the experiments we conducted, we believe the validation loss to be the better choice. This is especially true when the problem has multi-modality or spurious minima towards which a training loss indicator might be more prone. Moreover, \figref{fig:train_loss_prdp} shows a smoother convergence for the networks in both training and validation loss if PRDP is applied to the validation loss, likely due to the less aggressive progressive refinement.

%%% ChatGPT improved
We motivated Progressively Refined Differentiable Physics (PRDP) by showing how different physics refinement levels $K$ correspond to distinct plateaus in performance metrics other than training loss. This phenomenon is evident in \figref{fig:poisson_1_pr-savings} (a) and \figref{fig:ic-savings_heat-1d} (a). The PRDP control algorithm leverages this behavior by economically increasing the refinement levels, as visibly pronounced in \figref{fig:outer} (a) and (b).

As introduced in \secref{sec:prdp}, the PRDP control algorithm relies on a performance metric that triggers refinement when it plateaus. In the main text, we used validation error for this purpose, as it exhibited clear plateaus for $K < K_{\text{max}}$, and since hyperparameter tuning is typically done using a validation set. In this section, we ablate this choice by instead using the training loss as the guiding metric for PRDP. The results, shown in \figref{fig:train_loss_prdp}, reveal that training loss can also serve as a valid performance metric for PRDP.

Interestingly, when using training loss, PRDP adopts a slower refinement strategy since training loss often continues to decrease even after the validation loss has plateaued. While this approach yields slightly higher PR savings, the overall IC savings and final accuracy remain comparable.

Despite these findings, we recommend caution when applying PRDP with training loss as the guiding metric. For more complex problems, such as those with multi-modality or spurious minima, the training loss may be less reliable than validation loss. Additionally, as shown in \figref{fig:train_loss_prdp}, PRDP guided by validation loss results in smoother convergence for both training and validation losses. This smoother progression is likely a consequence of the more uniform refinement schedule, making validation loss the preferable choice for broader applications.

%%%%%%%%%%%%%%%%%%%%%

\end{document}
