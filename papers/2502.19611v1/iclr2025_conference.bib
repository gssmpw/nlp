% applications and reviews
% ========================================================

@misc{hao2023physicsinformed,
      title={Physics-Informed Machine Learning: A Survey on Problems, Methods and Applications}, 
      author={Zhongkai Hao and Songming Liu and Yichi Zhang and Chengyang Ying and Yao Feng and Hang Su and Jun Zhu},
      year={2023},
      eprint={2211.08064},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{ghosal1996largeeddy,
title = {An Analysis of Numerical Errors in Large-Eddy Simulations of Turbulence},
journal = {Journal of Computational Physics},
volume = {125},
number = {1},
pages = {187-206},
year = {1996},
issn = {0021-9991},
doi = {https://doi.org/10.1006/jcph.1996.0088},
url = {https://www.sciencedirect.com/science/article/pii/S0021999196900881},
author = {Sandip Ghosal},
}

@article{Pope2004largeeddy,
doi = {10.1088/1367-2630/6/1/035},
url = {https://dx.doi.org/10.1088/1367-2630/6/1/035},
year = {2004},
month = {mar},
publisher = {},
volume = {6},
number = {1},
pages = {35},
author = {Stephen B Pope},
title = {Ten questions concerning the large-eddy simulation of turbulent flows},
journal = {New Journal of Physics},
abstract = {In the past 30 years, there has been considerable progress in the development of large-eddy simulation (LES) for turbulent flows, which has been greatly facilitated by the substantial increase in computer power. In this paper, we raise some fundamental questions concerning the conceptual foundations of LES and about the methodologies and protocols used in its application. The 10 questions addressed are stated at the end of the introduction. Several of these questions highlight the importance of recognizing the dependence of LES calculations on the artificial parameter Δ (i.e. the filter width or, more generally, the turbulence resolution length scale). The principle that LES predictions of turbulence statistics should depend minimally on Δ provides an alternative justification for the dynamic procedure.}
}

@inproceedings{thuerey2020SIL,
 author = {Um, Kiwon and Brand, Robert and Fei, Yun (Raymond) and Holl, Philipp and Thuerey, Nils},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6111--6122},
 publisher = {Curran Associates, Inc.},
 title = {Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE-Solvers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/43e4e6a6f341e00671e123714de019a8-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{kochkov2021mlcfd,
title	= {Machine learning accelerated computational fluid dynamics},
author	= {Dmitrii Kochkov and Jamie Alexander Smith and Ayya Alieva and Qing Wang and Michael Brenner and Stephan Hoyer},
year	= {2021},
journal	= {Proceedings of the National Academy of Sciences USA}
}



@article{degrave2019robotics,
author={Degrave, Jonas and Hermans, Michiel and Dambre, Joni and wyffels, Francis},   
title={A Differentiable Physics Engine for Deep Learning in Robotics},      
journal={Frontiers in Neurorobotics},      
volume={13},           
year={2019},      
url={https://www.frontiersin.org/articles/10.3389/fnbot.2019.00006},       
doi={10.3389/fnbot.2019.00006},      
issn={1662-5218},   
}
% this uses analytical gradients for the physics engine

@inproceedings{Peres2018robotics,
 author = {de Avila Belbute-Peres, Filipe and Smith, Kevin and Allen, Kelsey and Tenenbaum, Josh and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {End-to-End Differentiable Physics for Learning and Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/842424a1d0595b76ec4fa03c46e8d755-Paper.pdf},
 volume = {31},
 year = {2018}
}
% this uses implicit gradients for the physics engine

@article{freund2020dpm,
author = {Justin Sirignano and Jonathan F. MacArt and Jonathan B. Freund},
title = {DPM: A deep learning PDE augmentation method with application to large-eddy simulation},
journal = {Journal of Computational Physics},
volume = {423},
pages = {109811},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109811},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120305854},
keywords = {Deep learning, Scientific machine learning, Large-eddy simulation, Sub-grid-scale modeling, Turbulence simulation},
}

@article{droniou2014heateq,
author = {Droniou, Jerome},
title = {Finite volume schemes for diffusion equations: Introduction to and review of modern methods},
journal = {Mathematical Models and Methods in Applied Sciences},
volume = {24},
number = {08},
pages = {1575-1619},
year = {2014},
doi = {10.1142/S0218202514400041},
}


% unrolled
% ========================================================

@article{fischer1991linsys,
title = {Automatic differentiation of the vector that solves a parametric linear system},
journal = {Journal of Computational and Applied Mathematics},
volume = {35},
number = {1},
pages = {169-184},
year = {1991},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0377-0427(91)90205-X},
url = {https://www.sciencedirect.com/science/article/pii/037704279190205X},
author = {Herbert Fischer},
keywords = {Automatic differentiation, parametric linear equations},
abstract = {We consider a parametric linear system a(s)·x(s) = b(s) where a(s) is a regular matrix, b(s) is a vector, and s is a p-dimensional parameter. This equation represents an implicit definition of a function x. It is shown that automatic differentiation techniques can be used to compute derivatives of x for given parameter-value s. Especially we aim at the Jacobian matrix and the Hessian tensor of x. These quantities are of particular interest in sensitivity analysis and structural optimization.}
}

@article{gilbert1992iterative,
  title={Automatic differentiation and iterative processes},
  author={Gilbert Jean Charles},
  journal={Optimization Methods \& Software},
  year={1992},
  volume={1},
  pages={13-21},
  url={https://api.semanticscholar.org/CorpusID:120894038}
}

@article{beck1994,
  title = {Automatic differentiation of iterative processes},
  journal = {Journal of Computational and Applied Mathematics},
  volume = {50},
  number = {1},
  pages = {109-118},
  year = {1994},
  issn = {0377-0427},
  doi = {https://doi.org/10.1016/0377-0427(94)90293-3},
  url = {https://www.sciencedirect.com/science/article/pii/0377042794902933},
  author = {Thomas Beck},
  keywords = {Automatic differentiation, Iterative processes},
  abstract = {Automatic differentiation is used to compute the values of the derivative of a function. If the function is given by a computational graph or code list, then the derivative values can be obtained using the chain rule. An iterative process can be regarded as an infinite code list. It is well known from classical analysis that the limit of the derivatives of the code list is not necessarily equal to the derivative of the limit function. The limit of the derivatives is corect for an important class of iterative processes including generalized Newton methods.}
}

@article{griewank1993forward,
  author          = {Griewank, Andreas and Bischof, Christian and Corliss, George and Carle, Alan and Williamson, Karen},
  journal         = {Optimization Methods and Software},
  number          = {3-4},
  pages           = {321--355},
  title           = {Derivative convergence for iterative equation solvers},
  volume          = {2},
  year            = {1993},
  publisher       = {Taylor \& Francis},
  doi            = {10.1080/10556789308805549},
}

@article{christianson1994reverse,
  title={Reverse accumulation and attractive fixed points},
  author={Christianson, Bruce},
  journal={Optimization Methods and Software},
  year={1994},
  volume={3},
  pages={311-326},
  url={https://api.semanticscholar.org/CorpusID:15027098}
}


@InProceedings{domke2012approxunrolled,
  title = 	 {Generic Methods for Optimization-Based Modeling},
  author = 	 {Domke, Justin},
  booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {318--326},
  year = 	 {2012},
  editor = 	 {Lawrence, Neil D. and Girolami, Mark},
  volume = 	 {22},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {La Palma, Canary Islands},
  month = 	 {21--23 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v22/domke12/domke12.pdf},
  url = 	 {https://proceedings.mlr.press/v22/domke12.html},
  abstract = 	 {"Energy” models for continuous domains can be applied to many problems, but often suffer from high computational expense in training, due to the need to repeatedly minimize the energy function to high accuracy. This paper considers a modified setting, where the model is trained in terms of results after optimization is truncated to a fixed number of iterations. We derive “backpropagating” versions of gradient descent, heavy-ball and LBFGS. These are simple to use, as they require as input only routines to compute the gradient of the energy with respect to the domain and parameters. Experimental results on denoising and image labeling problems show that learning with truncated optimization greatly reduces computational expense compared to “full” fitting.}
}


@inproceedings{phantom_gradient,
 author = {Geng, Zhengyang and Zhang, Xin-Yu and Bai, Shaojie and Wang, Yisen and Lin, Zhouchen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24247--24260},
 publisher = {Curran Associates, Inc.},
 title = {On Training Implicit Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{curse,
 author = {Scieur, Damien and Gidel, Gauthier and Bertrand, Quentin and Pedregosa, Fabian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17133--17145},
 publisher = {Curran Associates, Inc.},
 title = {The Curse of Unrolling: Rate of Differentiating Through Optimization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/6d53193a098b982229340a7c3eb0ecbf-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@article{Bolte2023onsestepdiff,
  author       = {J{\'{e}}r{\^{o}}me Bolte and
                  Edouard Pauwels and
                  Samuel Vaiter},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {One-step differentiation of iterative algorithms},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/f3716db40060004d0629d4051b2c57ab-Abstract-Conference.html},
  timestamp    = {Fri, 01 Mar 2024 16:26:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/BoltePV23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Grazzi2020,
  title={On the Iteration Complexity of Hypergradient Computation},
  author={Riccardo Grazzi and Luca Franceschi and Massimiliano Pontil and Saverio Salzo},
  booktitle={International Conference on Machine Learning},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:220250381}
}


@inproceedings{bolte2022nonsmooth,
    author = {Bolte, J\'{e}r\^{o}me and Pauwels, Edouard and Vaiter, Samuel},
    title = {Automatic differentiation of nonsmooth iterative algorithms},
    year = {2024},
    isbn = {9781713871088},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno = {1915},
    numpages = {14},
    location = {New Orleans, LA, USA},
    series = {NIPS '22}
}

% implicit
% ========================================================

@book{krantz2012implicit,
  title={The Implicit Function Theorem: History, Theory, and Applications},
  author={Krantz, S.G. and Parks, H.R.},
  isbn={9781461459811},
  series={Modern Birkh{\"a}user Classics},
  url={https://books.google.de/books?id=7QqickY0yh8C},
  year={2012},
  publisher={Springer New York}
}

@article{christianson1998reverseimplicit,
author = {Christianson, Bruce},
title = {Reverse accumulation and implicit functions},
journal = {Optimization Methods and Software},
volume = {9},
number = {4},
pages = {307--322},
year = {1998},
publisher = {Taylor \& Francis},
doi = {10.1080/10556789808805697},
}

@article{biggs1998forwardimplicit,
  author    = {Bartholomew-Biggs, M.C.},
  title     = {Using Forward Accumulation for Automatic Differentiation of Implicitly-Defined Functions},
  journal   = {Computational Optimization and Applications},
  year      = {1998},
  volume    = {9},
  number    = {1},
  pages     = {65--84},
  abstract  = {This paper deals with the calculation of partial derivatives (w.r.t. the independent variables, x) of a vec of dependent variables y which satisfy a system of nonlinear equations g(u(x), y) = 0 . A number of authors have suggested that the forward accumulation method of automatic differentiation can be applied to a suitable iterative scheme for solving the nonlinear system with a view to giving simultaneous convergence both to the correct value y and also to its Jacobian matrix yx. It is known, however, that convergence of the derivatives may not occur at the same rate as the convergence of the y values. In this paper we avoid both the difficulty and the potential cost of iterating the gradient part of the calculation to sufficient accuracy. We do this by observing that forward accumulation need only be applied to the functions g after the dependent variables, y, have been computed in standard real arithmetic usin g any appropriate method. This so-called Post-Differentiation (PD) technique is shown, on a number of examples, to have an advantage in terms of both accuracy and speed over approaches where forward accumulation is applied over the entire iterative process. Moreover, the PD technique can be implemented in such a way as to provide a friendly interface for non-specialist users.},
  issn      = {1573-2894},
  doi       = {10.1023/A:1018382103801},
  url       = {https://doi.org/10.1023/A:1018382103801}
}

@misc{deepimplicitlayers,
  author = {Duvenaud, David and Kolter, J. Zico and Johnson, Matthew},
  title = {Deep Implicit Layers: Neural ODEs, Equilibrium Models, and Differentiable Optimization [Tutorial]},
  year = {2020},
  howpublished = {NeurIPS},
  url = {https://neurips.cc/virtual/2020/tutorial/16653}
}



@inproceedings{Blondel2022modular,
 author = {Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy and Hoyer, Stephan and Llinares-Lopez, Felipe and Pedregosa, Fabian and Vert, Jean-Philippe},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {5230--5242},
 publisher = {Curran Associates, Inc.},
 title = {Efficient and Modular Implicit Differentiation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/228b9279ecf9bbafe582406850c57115-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{akbarzadeh2020,
  author    = {Akbarzadeh, Siamak and H{\"u}ckelheim, Jan and M{\"u}ller, Jens-Dominik},
  title     = {Consistent treatment of incompletely converged iterative linear solvers in reverse-mode algorithmic differentiation},
  journal   = {Computational Optimization and Applications},
  year      = {2020},
  volume    = {77},
  number    = {2},
  pages     = {597--616},
  date      = {2020/11/01},
  abstract  = {Algorithmic differentiation (AD) is a widely-used approach to compute derivatives of numerical models. Many numerical models include an iterative process to solve non-linear systems of equations. To improve efficiency and numerical stability, AD is typically not applied to the linear solvers. Instead, the differentiated linear solver call is replaced with hand-produced derivative code that exploits the linearity of the original call. In practice, the iterative linear solvers are often stopped prematurely to recompute the linearisation of the non-linear outer loop. We show that in the reverse-mode of AD, the derivatives obtained with partial convergence become inconsistent with the original and the tangent-linear models, resulting in inaccurate adjoints. We present a correction term that restores consistency between adjoint and tangent-linear gradients if linear systems are only partially converged. We prove the consistency of this correction term and show in numerical experiments that the accuracy of adjoint gradients of an incompressible flow solver applied to an industrial test case is restored when the correction term is used.},
  issn      = {1573-2894},
  doi       = {10.1007/s10589-020-00214-x},
  url       = {https://doi.org/10.1007/s10589-020-00214-x}
}

@inproceedings{Fung2021JFB,
  title={JFB: Jacobian-Free Backpropagation for Implicit Networks},
  author={Samy Wu Fung and Howard Heaton and Qiuwei Li and Daniel Mckenzie and Stanley J. Osher and Wotao Yin},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:238198721}
}


@InProceedings{pedregosa2016,
  title = 	 {Hyperparameter optimization with approximate gradient},
  author = 	 {Pedregosa, Fabian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {737--746},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/pedregosa16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/pedregosa16.html},
}


% implicit applications

@inproceedings{deepEM,
 author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {Deep Equilibrium Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{multiscaledeepEM,
 author = {Bai, Shaojie and Koltun, Vladlen and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5238--5250},
 publisher = {Curran Associates, Inc.},
 title = {Multiscale Deep Equilibrium Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/3812f9a59b634c2a9c574610eaba5bed-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{agrawal2019optimizationlayers,
 author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Differentiable Convex Optimization Layers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/9ce3c52fc54362e22053399d3181c638-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{optnet2017,
author = {Amos, Brandon and Kolter, J. Zico},
title = {OptNet: differentiable optimization as a layer in neural networks},
year = {2017},
publisher = {JMLR.org},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {136--145},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{gould2019DeepDN,
  title={Deep Declarative Networks},
  author={Stephen Gould and Richard I. Hartley and Dylan Campbell},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2019},
  volume={44},
  pages={3988-4004},
  url={https://api.semanticscholar.org/CorpusID:202558604}
}

@inproceedings{choe2023metalearning,
title={Making Scalable Meta Learning Practical},
author={Sang Keun Choe and Sanket Vaibhav Mehta and Hwijeen Ahn and Willie Neiswanger and Pengtao Xie and Emma Strubell and Eric Xing},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=Xazhn0JoNx}
}

@inproceedings{schnell2022HIG,
title={Half-Inverse Gradients for Physical Deep Learning},
author={Patrick Schnell and Philipp Holl and Nils Thuerey},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=HTx7vrlLBEj}
}

@inproceedings{holl2022PI,
 author = {Holl, Philipp and Koltun, Vladlen and Thuerey, Nils},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {5390--5403},
 publisher = {Curran Associates, Inc.},
 title = {Scale-invariant Learning by Physics Inversion},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/240cc9ac4789351653d13cfcba4ee85c-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

% tools
% ========================================================

@Inbook{Luenberger2021,
author="Luenberger, David G.
and Ye, Yinyu",
title="Basic Descent Methods",
bookTitle="Linear and Nonlinear Programming",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="235--300",
abstract="We turn now to a description of the basic techniques used for iteratively solving unconstrained minimization problems. These techniques are, of course, important for practical application since they often offer the simplest, most direct alternatives for obtaining solutions; but perhaps their greatest importance is that they establish certain reference plateaus with respect to difficulty of implementation and speed of convergence. Thus in later chapters as more efficient techniques and techniques capable of handling constraints are developed, reference is continually made to the basic techniques of this chapter both for guidance and as points of comparison.",
isbn="978-3-030-85450-8",
doi="10.1007/978-3-030-85450-8_8",
url="https://doi.org/10.1007/978-3-030-85450-8_8"
}

@book{saad2003linsolve,
author = {Saad, Yousef},
title = {Iterative Methods for Sparse Linear Systems},
publisher = {Society for Industrial and Applied Mathematics},
year = {2003},
doi = {10.1137/1.9780898718003},
address = {},
edition   = {Second},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718003},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718003}
}

@book{griewank2008autodiff,
author = {Griewank, Andreas and Walther, Andrea},
title = {Evaluating Derivatives},
publisher = {Society for Industrial and Applied Mathematics},
year = {2008},
doi = {10.1137/1.9780898717761},
address = {},
edition   = {Second},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717761}
}

@book{ames2014poisson,
  title={Numerical Methods for Partial Differential Equations},
  author={Ames, W.F. and Rheinboldt, W. and Jeffrey, A.},
  isbn={9781483262420},
  series={Applications of Mathematics Series},
  url={https://books.google.de/books?id=haviBQAAQBAJ},
  year={2014},
  publisher={Elsevier Science}
}

@book{evans2010partial,
  title={Partial Differential Equations},
  author={Evans, L.C.},
  isbn={9780821849743},
  lccn={2009044716},
  series={Graduate studies in mathematics},
  url={https://books.google.de/books?id=Xnu0o_EJrCQC},
  year={2010},
  publisher={American Mathematical Society}
}

@book{durran2010,
  title={Numerical Methods for Fluid Dynamics},
  author={Dale R. Durran},
  isbn={9781441964120},
  series={Texts in Applied Mathematics},
  year={2010},
  publisher={Springer New York, NY}
}


@inproceedings{adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  booktitle = {International Conference on Learning Representations},
  year={2015},
}

@article{Bi-CGSTAB1992,
  author = {van der Vorst, H. A.},
  title = {Bi-CGSTAB: A Fast and Smoothly Converging Variant of Bi-CG for the Solution of Nonsymmetric Linear Systems},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  volume = {13},
  number = {2},
  pages = {631-644},
  year = {1992},
  doi = {10.1137/0913035},
  URL = {https://doi.org/10.1137/0913035},
  eprint = {https://doi.org/10.1137/0913035},
  abstract = { Recently the Conjugate Gradients-Squared (CG-S) method has been proposed as an attractive variant of the Bi-Conjugate Gradients (Bi-CG) method. However, it has been observed that CG-S may lead to a rather irregular convergence behaviour, so that in some cases rounding errors can even result in severe cancellation effects in the solution. In this paper, another variant of Bi-CG is proposed which does not seem to suffer from these negative effects. Numerical experiments indicate also that the new variant, named Bi-CGSTAB, is often much more efficient than CG-S. }
}

@article{gmres1986,
  title={GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems},
  author={Saad, Yousef and Schultz, Martin H},
  journal={SIAM Journal on Scientific and Statistical Computing},
  volume={7},
  number={3},
  pages={856--869},
  year={1986},
  publisher={SIAM},
  doi = {10.1137/0907058}
}

@article{CG1952Hestenes,
  title={Methods of Conjugate Gradients for Solving Linear Systems},
  author={Hestenes, Magnus R and Stiefel, Eduard},
  journal={Journal of Research of the National Bureau of Standards},
  volume={49},
  number={6},
  pages={409--436},
  year={1952},
  publisher={Journal of Research of the National Bureau of Standards}
}


@book{thuerey2021pbdl,
  title={Physics-based Deep Learning},
  author={Nils Thuerey and Philipp Holl and Maximilian Mueller and Patrick Schnell and Felix Trost and Kiwon Um},
  url={https://physicsbaseddeeplearning.org},
  year={2021},
  publisher={WWW}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018}
}

@inproceedings{jax2018paper,
title	= {Compiling machine learning programs via high-level tracing},
author	= {Roy Frostig and Matthew Johnson and Chris Leary},
year	= {2018},
booktitle={Machine Learning and Systems},
URL	= {https://mlsys.org/Conferences/doc/2018/146.pdf}
}


@software{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/google-deepmind},
  year = {2020},
}

@article{kidger2021equinox,
    author={Patrick Kidger and Cristian Garcia},
    title={{E}quinox: neural networks in {JAX} via callable {P}y{T}rees and filtered transformations},
    year={2021},
    journal={Differentiable Programming workshop at Neural Information Processing Systems 2021}
}

@article{lineax2023,
    title={Lineax: unified linear solves and linear least-squares in JAX and Equinox},
    author={Jason Rader and Terry Lyons and Patrick Kidger},
    journal={
        AI for science workshop at Neural Information Processing Systems 2023,
        arXiv:2311.17283
    },
    year={2023},
}

@incollection{pytorch2019,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

%%%% Additions

@article{kochkov2024neural,
  title={Neural general circulation models for weather and climate},
  author={Kochkov, Dmitrii and Yuval, Janni and Langmore, Ian and Norgaard, Peter and Smith, Jamie and Mooers, Griffin and Kl{\"o}wer, Milan and Lottes, James and Rasp, Stephan and D{\"u}ben, Peter and others},
  journal={Nature},
  pages={1--7},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{akbarzadeh2020consistent,
  title={Consistent treatment of incompletely converged iterative linear solvers in reverse-mode algorithmic differentiation},
  author={Akbarzadeh, Siamak and H{\"u}ckelheim, Jan and M{\"u}ller, Jens-Dominik},
  journal={Computational Optimization and Applications},
  volume={77},
  pages={597--616},
  year={2020},
  publisher={Springer}
}

@InProceedings{shaban2019truncated,
  author       = {Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  title        = {Truncated back-propagation for bilevel optimization},
  year         = {2019},
  organization = {PMLR},
  pages        = {1723--1732},
  file         = {:shaban2019truncated-backprop.pdf:PDF},
  groups       = {Unroll vs. Implicit - Master Thesis},
}

@book{turek1999efficient,
  author       = {Stefan Turek},
  title        = {Efficient Solvers for Incompressible Flow Problems - An Algorithmic
                  and Computational Approach},
  series       = {Lecture Notes in Computational Science and Engineering},
  volume       = {6},
  publisher    = {Springer},
  year         = {1999},
  url          = {https://doi.org/10.1007/978-3-642-58393-3},
  doi          = {10.1007/978-3-642-58393-3},
  isbn         = {978-3-642-63573-1},
  timestamp    = {Tue, 16 May 2017 14:24:20 +0200},
  biburl       = {https://dblp.org/rec/series/lncse/Turek99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{issa1986computation,
  author    = {Issa, Raad I and Gosman, AD and Watkins, AP},
  journal   = {Journal of Computational Physics},
  title     = {The computation of compressible and incompressible recirculating flows by a non-iterative implicit scheme},
  year      = {1986},
  number    = {1},
  pages     = {66--82},
  volume    = {62},
  file      = {:issa1986piso.pdf:PDF},
  publisher = {Elsevier},
}

@article{perot1993fractional,
title = {An Analysis of the Fractional Step Method},
journal = {Journal of Computational Physics},
volume = {108},
number = {1},
pages = {51-58},
year = {1993},
issn = {0021-9991},
doi = {https://doi.org/10.1006/jcph.1993.1162},
url = {https://www.sciencedirect.com/science/article/pii/S0021999183711629},
author = {J.Blair Perot},
abstract = {The fractional step method for solving the incompressible Navier-Stokes equations in primitive variables is analyzed as a block LU decomposition. In this formulation the issues involving boundary conditions for the intermediate velocity variables and the pressure are clearly resolved. In addition, it is shown that poor temporal accuracy (first-order) is not due to boundary conditions, but due to the method itself. A generalized block LU decomposition that overcomes this difficulty is presented, allowing arbitrarily high temporal order of accuracy. The generalized decomposition is shown to be useful for a wide range of problems including steady problems. Technical issues, such as stability and the appropriate pressure update scheme, are also addressed. Numerical simulations of the unsteady, incompressible Navier-Stokes equations in a square domain confirm the theoretical results.}
}

@Article{rumelhart1986backpropagation,
  author    = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal   = {nature},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  file      = {:rumelhart1986backpropagation.pdf:PDF},
  publisher = {Nature Publishing Group},
}


@Article{dreyfus1962numerical,
  author    = {Dreyfus, Stuart},
  journal   = {Journal of Mathematical Analysis and Applications},
  title     = {The numerical solution of variational problems},
  year      = {1962},
  number    = {1},
  pages     = {30--45},
  volume    = {5},
  file      = {:dreyfus1962numerical.pdf:PDF},
  publisher = {Academic Press},
}

@Article{griewank2012invented,
  author  = {Griewank, Andreas},
  journal = {Documenta Mathematica, Extra Volume ISMP},
  title   = {Who invented the reverse mode of differentiation},
  year    = {2012},
  pages   = {389--400},
  comment = {paper on the history of automatic differentiation (autodiff)},
  file    = {:griewank2012invention.pdf:PDF},
  groups  = {Unroll vs. Implicit - Master Thesis},
}

@Book{bendsoe2013topology,
  author    = {Bendsoe, Martin Philip and Sigmund, Ole},
  publisher = {Springer Science \& Business Media},
  title     = {Topology optimization: theory, methods, and applications},
  year      = {2013},
  file      = {:bendsoe2013topology.pdf:PDF},
  groups    = {Adjoint},
}

@article{li2024pino,
author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
title = {Physics-Informed Neural Operator for Learning Partial Differential Equations},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3648506},
doi = {10.1145/3648506},
abstract = {In this article, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator. Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, that is, being able to predict beyond the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization challenges, for example, in multi-scale dynamic systems such as Kolmogorov flows.PROBLEM STATEMENTMachine learning methods have recently shown promise in solving partial differential equations (PDEs) raised in science and engineering. They can be classified into two broad categories: approximating the solution function  and learning the solution operator. The Physics-Informed Neural Network (PINN) is an example of the former while the Fourier neural operator (FNO) is an example of the latter. Both these approaches have shortcomings. The optimization in PINN is challenging and prone to failure, especially on multi-scale dynamic systems. FNO does not suffer from this optimization issue since it carries out supervised learning on a given dataset, but obtaining such data may be too expensive or infeasible. In this paper, we consider a new learning paradigm, aiming to overcome the optimization challenge in PINN and relieve the data requirement in FNO.METHODSIn this paper, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric PDEs.In the operator-learning phase, PINO learns the solution operator over multiple instances of the parametric PDE family using training data and physics constraints. In the instance-wise fine-tuning phase, PINO optimizes the pre-trained operator ansatz for the querying instance of the PDE using the physics constraints only.Specifically, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator.RESULTSThe resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond the resolution of training data.Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers. With the equation constraints, PINO requires few to no data to learn the Burgers, Darcy, and Navier-Stokes equation. In particular, PINO accurately solves long temporal transient flows and  Kolmogorov flows where other baseline methods fail to converge.SIGNIFICANCEPINO uses the neural operator framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed. These advantages could lead to applications such as weather forecast, airfoil designs, and turbulence control.},
journal = {ACM / IMS J. Data Sci.},
month = {may},
articleno = {9},
numpages = {27},
keywords = {Neural operators, physics informed learning, partial differential equations}
}

@Article{bar2019learning,
  author    = {Bar-Sinai, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Learning data-driven discretizations for partial differential equations},
  year      = {2019},
  number    = {31},
  pages     = {15344--15349},
  volume    = {116},
  comment   = {stencil},
  file      = {:sinai2019learning.pdf:PDF},
  publisher = {National Acad Sciences},
}

@InProceedings{brandstetter2021message,
  author    = {Johannes Brandstetter and Daniel E. Worrall and Max Welling},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  title     = {Message Passing Neural {PDE} Solvers},
  year      = {2022},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/BrandstetterWW22.bib},
  file      = {:brandstetter2022message.pdf:PDF},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  url       = {https://openreview.net/forum?id=vSix3HPYKSU},
}



@inproceedings{andrychowicz2016learningtolearn,
  author       = {Marcin Andrychowicz and
                  Misha Denil and
                  Sergio Gomez Colmenarejo and
                  Matthew W. Hoffman and
                  David Pfau and
                  Tom Schaul and
                  Nando de Freitas},
  editor       = {Daniel D. Lee and
                  Masashi Sugiyama and
                  Ulrike von Luxburg and
                  Isabelle Guyon and
                  Roman Garnett},
  title        = {Learning to learn by gradient descent by gradient descent},
  booktitle    = {Advances in Neural Information Processing Systems 29: Annual Conference
                  on Neural Information Processing Systems 2016, December 5-10, 2016,
                  Barcelona, Spain},
  pages        = {3981--3989},
  year         = {2016},
  url          = {https://proceedings.neurips.cc/paper/2016/hash/fb87582825f9d28a8d42c5e5e5e8b23d-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/AndrychowiczDCH16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@Article{harlow1965numerical,
  author    = {Harlow, Francis H and Welch, J Eddie},
  journal   = {The physics of fluids},
  title     = {Numerical calculation of time-dependent viscous incompressible flow of fluid with free surface},
  year      = {1965},
  number    = {12},
  pages     = {2182--2189},
  volume    = {8},
  file      = {:harlow1965numerical.pdf:PDF},
  publisher = {American Institute of Physics},
}

@Article{raissi2019physics,
  author         = {Maziar Raissi and Paris Perdikaris and George E. Karniadakis},
  journal        = {J. Comput. Phys.},
  title          = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  year           = {2019},
  pages          = {686--707},
  volume         = {378},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/jcphy/RaissiPK19.bib},
  doi            = {10.1016/J.JCP.2018.10.045},
  file           = {:raissi2019physics-informed.pdf:PDF},
  groups         = {Residuum-based training},
  publisher      = {Elsevier},
  qualityassured = {qualityAssured},
  timestamp      = {Wed, 19 Feb 2020 18:09:50 +0100},
  url            = {https://doi.org/10.1016/j.jcp.2018.10.045},
}

@Article{kovachki2021neural,
  author         = {Nikola B. Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew M. Stuart and Anima Anandkumar},
  journal        = {J. Mach. Learn. Res.},
  title          = {Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs},
  year           = {2023},
  pages          = {89:1--89:97},
  volume         = {24},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/journals/jmlr/KovachkiLLABSA23.bib},
  file           = {:kovachki2022neural-operator.pdf:PDF},
  qualityassured = {qualityAssured},
  timestamp      = {Tue, 13 Jun 2023 16:59:20 +0200},
  url            = {http://jmlr.org/papers/v24/21-1524.html},
}

@Article{bolte2022automatic,
  author  = {Bolte, J{\'e}r{\^o}me and Pauwels, Edouard and Vaiter, Samuel},
  journal = {arXiv preprint arXiv:2206.00457},
  title   = {Automatic differentiation of nonsmooth iterative algorithms},
  year    = {2022},
  file    = {:bolte2022automatic.pdf:PDF},
}

@Article{griewank1992achieving,
  author    = {Griewank, Andreas},
  journal   = {Optimization Methods and software},
  title     = {Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation},
  year      = {1992},
  number    = {1},
  pages     = {35--54},
  volume    = {1},
  comment   = {Paper discussing checkpointing for reverse-mode AD},
  file      = {:griewank1992autodiff_growth.pdf:PDF},
  publisher = {Taylor \& Francis},
}

@Article{werbos1990backpropagation,
  author    = {Werbos, Paul J},
  journal   = {Proceedings of the IEEE},
  title     = {Backpropagation through time: what it does and how to do it},
  year      = {1990},
  number    = {10},
  pages     = {1550--1560},
  volume    = {78},
  file      = {:werbos1990backpropagation.pdf:PDF},
  publisher = {IEEE},
}

@InCollection{giles2008collected,
  author    = {Giles, Mike B},
  booktitle = {Advances in Automatic Differentiation},
  publisher = {Springer},
  title     = {Collected matrix derivative results for forward and reverse mode algorithmic differentiation},
  year      = {2008},
  pages     = {35--44},
  file      = {:giles2008collected.pdf:PDF},
}

@article{feurer2019hyperparameter,
  title={Hyperparameter optimization},
  author={Feurer, Matthias and Hutter, Frank},
  journal={Automated machine learning: Methods, systems, challenges},
  pages={3--33},
  year={2019},
  publisher={Springer International Publishing}
}

@InProceedings{maclaurin2015gradient,
  author       = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle    = {International conference on machine learning},
  title        = {Gradient-based hyperparameter optimization through reversible learning},
  year         = {2015},
  organization = {PMLR},
  pages        = {2113--2122},
  file         = {:maclaurin2015gradient-based.pdf:PDF},
}

@InProceedings{lorraine2020optimizing,
  author       = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  title        = {Optimizing millions of hyperparameters by implicit differentiation},
  year         = {2020},
  organization = {PMLR},
  pages        = {1540--1552},
  file         = {:lorraine2019optimizing.pdf:PDF},
}



@article{bengio2000gradient,
  author       = {Yoshua Bengio},
  title        = {Gradient-Based Optimization of Hyperparameters},
  journal      = {Neural Comput.},
  volume       = {12},
  number       = {8},
  pages        = {1889--1900},
  year         = {2000},
  url          = {https://doi.org/10.1162/089976600300015187},
  doi          = {10.1162/089976600300015187},
  timestamp    = {Tue, 01 Sep 2020 13:12:04 +0200},
  biburl       = {https://dblp.org/rec/journals/neco/Bengio00.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{franceschi2017forward,
  author         = {Luca Franceschi and Michele Donini and Paolo Frasconi and Massimiliano Pontil},
  booktitle      = {Proceedings of the 34th International Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  title          = {Forward and Reverse Gradient-Based Hyperparameter Optimization},
  year           = {2017},
  editor         = {Doina Precup and Yee Whye Teh},
  pages          = {1165--1173},
  publisher      = {{PMLR}},
  series         = {Proceedings of Machine Learning Research},
  volume         = {70},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/icml/FranceschiDFP17.bib},
  file           = {:franceschi2017forward.pdf:PDF},
  qualityassured = {qualityAssured},
  timestamp      = {Wed, 15 Jun 2022 15:26:46 +0200},
  url            = {http://proceedings.mlr.press/v70/franceschi17a.html},
}

@InProceedings{ji2021bilevel,
  author       = {Ji, Kaiyi and Yang, Junjie and Liang, Yingbin},
  booktitle    = {International conference on machine learning},
  title        = {Bilevel optimization: Convergence analysis and enhanced design},
  year         = {2021},
  organization = {PMLR},
  pages        = {4882--4892},
  file         = {:ji2021bilevel.pdf:PDF},
  groups       = {Unroll vs. Implicit - Master Thesis},
}

@article{edpbook,
title={The {E}lements of {D}ifferentiable {P}rogramming},
author={Blondel, Mathieu and Roulet, Vincent},
journal={arXiv preprint arXiv:2403.14606},
year={2024}
} 

@book{pml2Book,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: Advanced Topics",
 publisher = "MIT Press",
 year = 2023,
 url = "http://probml.github.io/book2"
}

@InProceedings{he2016resnet,
  author         = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle      = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  title          = {Deep Residual Learning for Image Recognition},
  year           = {2016},
  pages          = {770--778},
  publisher      = {{IEEE} Computer Society},
  bibsource      = {dblp computer science bibliography, https://dblp.org},
  biburl         = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  doi            = {10.1109/CVPR.2016.90},
  file           = {:he2015resnet.pdf:PDF},
  qualityassured = {qualityAssured},
  timestamp      = {Fri, 24 Mar 2023 00:02:57 +0100},
  url            = {https://doi.org/10.1109/CVPR.2016.90},
}

@software{deepmind2020jaxB,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/google-deepmind},
  year = {2020},
}

@article{kidger2021equinoxB,
    author={Patrick Kidger and Cristian Garcia},
    title={{E}quinox: neural networks in {JAX} via callable {P}y{T}rees and filtered transformations},
    year={2021},
    journal={Differentiable Programming workshop at Neural Information Processing Systems 2021}
}