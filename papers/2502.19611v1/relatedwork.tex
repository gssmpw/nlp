\section{Related Work}
\paragraph{Differentiation through implicit relations}
% \felix{The note of \citet{griewank2012invented} lists potential origins of reverse-mode automatic differentiation and highlights the importance of early work in optimal control in the 1960s (also see ch. 6.6 of \citet{goodfellow2016deep}).}
% While this branch of science shares our goal of differentiating through the solution process of partial differential equations \felix{(which can be considered an \emph{implicit relation})}, its early application mostly involved the usage of continuous adjoint methods, avoiding the autodifferential treatment of iterative processes.
\citet{fischer1991linsys} established an implementation framework for \emph{unrolled differentiation}
and investigated the convergence of the derivative, focusing on linear solvers.
% The paper proved that Jacobi-iteration for solving linear systems is derivative-stable, i.e.,
% the derivatives of the iterates converge to the derivative of the fixed point of the iterations.
\citet{gilbert1992iterative} extended this work to a broader class of iterative processes.
As an alternative to unrolled differentiation, \citet{christianson1998reverseimplicit} derived the \emph{implicit differentiation} rules over linear solves.
% , which can be viewed as an intermediate between full unrollment and a continuous adjoint treatment.
Beyond linear systems, implicit differentiation has since gained prominence in the machine learning community, particularly for hyperparameter optimization \citep{bengio2000gradient} and other \emph{bilevel optimization} (BLO) applications that require differentiating through inner optimizations, such as deep equilibrium models \citep{deepEM} and meta-learning \citep{andrychowicz2016learningtolearn}.
We view differentiable physics \citep{thuerey2021pbdl} as another type of BLO with the specialty of sparse linear systems.
The practicality of implicit differentiation is largely due to its reduced memory footprint and lower reverse-mode computational cost, especially as modern automatic differentiation (AD) tools can automatically handle the necessary propagation rules \citep{Blondel2022modular}. Additionally, implicit differentiation allows for the black-box use of solver implementations, enabling the integration of third-party components into differentiable computational graphs \citep{giles2008collected}. However, unrolled differentiation remains an active research area, with work focusing on non-asymptotic analyses \citep{curse} and other aspects \citep{maclaurin2015gradient,franceschi2017forward,Grazzi2020,ji2021bilevel}.

\paragraph{Analysis and cost-reduction of bilevel optimization}

To address the computational cost of bilevel optimization,
\cite{Fung2021JFB} presented \emph{Jacobian-free backpropagation}, where the Jacobian of the implicit solver is approximated as the identity, eliminating the need for adjoint linear solves. \cite{phantom_gradient} introduced \emph{phantom gradients}, where the matrix inversion is replaced with an approximate inverse, and provided theoretical guarantees on the convergence of stochastic gradient descent as the outer problem. \citet{lorraine2020optimizing} approximates the implicit linear solves with a reduced number of conjugate gradient steps.
Moreover, \citet{shaban2019truncated} and recently \citet{Bolte2023onsestepdiff} discuss unrolled differentiation through only a reduced number of iterations. All the aforementioned works consider (what we call) \emph{incomplete convergence} (IC) savings, albeit in the setting of deep equilibrium models \citep{deepEM} or hyperparameter optimization \citep{feurer2019hyperparameter}.
% \felix{and by only manipulating the adjoint inaccuracy of \eqref{eq:outer-gradient}.}
% \felix{However, they typically do not schedule refinement over network training.}
On the other hand, \cite{pedregosa2016} studied
approximate hypergradients by adjusting the tolerances of inner primal and implicit linear solves following a pre-defined schedule.
This work proves (what we call) \emph{progressive refinement} (PR) savings.
% , albeit in strict convex settings \felix{for which IC savings cannot exist}.
Prior to this, \citet{domke2012approxunrolled}
investigated outer training with unrolled AD through incompletely converged
iterations, specifically for gradient descent, heavy ball, and L-BFGS methods as inner optimizers. They note the advantage of implementing incomplete convergence using the number of inner iterations rather than inner tolerance.
Our approach uniquely combines PR and IC savings -- through both unrolled and implicit differentiation -- targeting iterative solvers for sparse linear systems embedded within neural network training. To our knowledge, no prior work has applied these techniques in this context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%