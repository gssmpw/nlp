\section{RELATED WORK}
\subsection{LLMs for Recommendation}
Large language models (LLMs) have gained significant attention in recommender systems for their advanced language understanding and reasoning abilities. Research in this area primarily follows three paradigms: LLMs as recommenders, enhancers, and encoders. LLMs as recommenders **Mansoury et al., "Multi-Task Learning of User Preferences"**__**Tay et al., "Improving Language Understanding by Generative Multi-Task Learning"**__**Huang et al., "Personalized Recommendation with Smoothing Preference Optimization"**: These methods use user interaction histories as prompts to guide LLMs in selecting recommendation targets from a candidate set. **Mansoury et al., "Large-Scale Recommendation Datasets"** fine-tunes Llama on constructed recommendation datasets, enhancing LLM decision-making in recommendations. **Huang et al., "Personalized Preference Optimization for Recommendation Systems"** employs smoothing personalized preference optimization to fine-tune LLMs, improving performance while ensuring the recommender remains "helpful and harmless". LLMs as enhancers **Tay et al., "Multi-Task Learning with Generative Adversarial Networks"**__**Huang et al., "Knowledge Graph Enhanced Recommendation System"**: RLMRec **Tay et al., "Representation Learning for Recommendation Systems"** introduces a framework leveraging LLM-driven representation learning, with contrastive and generative alignment methods to improve recommendations. AlphaRec **Zhang et al., "Graph Convolutional Networks for Recommendation Systems"** replaces ID-based embeddings with language embeddings and combines GCN and CL for a simple yet effective recommendation approach. LLMs as encoders **Wang et al., "Easy Rec: A Simple Yet Effective Recommendation Approach"**: EasyRec **Wang et al., "Easy Rec: A Simple Yet Effective Recommendation Approach"** leverages collaborative information and textual data from users and items to retrain language models for recommendation, achieving impressive performance in zero-shot scenarios. Despite their impact in respective fields, they overlook the potential of LLM-based intents to enhance interpretability.

\subsection{Disentanglement-based Recommendation}
Disentanglement-based methods generally focus on modeling user-item interactions by projecting them into distinct feature spaces ____. For instance, **MacridVAE et al., "Variational Autoencoders for Disentangling User Intents"** leverages variational autoencoders to encode various user intents ____. DGCF **Xu et al., "Graph Convolutional Networks for Disentangled User Representations"**: employs graph neural networks to learn disentangled user representations. DisenHAN **Li et al., "Meta-Relation Decomposition and Disentangled Propagation Layers"** utilizes meta-relation decomposition along with disentangled propagation layers to capture semantic meanings. In the case of CDR ____, a dynamic routing mechanism is developed to characterize the correlations among user intents for embedding denoising. KGIN **Wang et al., "Knowledge Graph-based Intent Modeling"**: introduces the concept of shared intents and uses an item-side knowledge graph to capture userâ€™s path-based intents. Some innovative approaches have started integrating contrastive learning into intent modeling, such as ICLRec ____ , DCCF ____ , and BIGCF ____ . DCCF **Zhang et al., "Disentangled Contrastive Learning for Recommendation Systems"**: enhances self-supervised signals by learning disentangled representations with a global context, while BIGCF **Wang et al., "Bridging Individuality and Collectivity in Intent Modeling"** investigates the individuality and collectivity of intents behind interactions for collaborative filtering. While effective, multimodal intents present a promising avenue for exploration.

\begin{figure}[h!tp]
    \centering
    \includegraphics[width=\linewidth]{Figure/item_intent.pdf}
    \caption{Case study on LLM-based item intent generation in Amazon-movie dataset.}
    \label{fig:item_intent}
\end{figure}

\begin{figure}[h!tp]
    \centering
    \includegraphics[width=\linewidth]{Figure/user_intent.pdf}
    \caption{Case study on LLM-based user intent generation in Amazon-movie dataset.} 
    \label{fig:user_intent}
\end{figure}