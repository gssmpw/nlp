\section{RELATED WORK}
\subsection{LLMs for Recommendation}
Large language models (LLMs) have gained significant attention in recommender systems for their advanced language understanding and reasoning abilities. Research in this area primarily follows three paradigms: LLMs as recommenders, enhancers, and encoders. LLMs as recommenders ____: These methods use user interaction histories as prompts to guide LLMs in selecting recommendation targets from a candidate set. TallRec ____ fine-tunes Llama on constructed recommendation datasets, enhancing LLM decision-making in recommendations. RosePO ____ employs smoothing personalized preference optimization to fine-tune LLMs, improving performance while ensuring the recommender remains "helpful and harmless". LLMs as enhancers ____: RLMRec ____ introduces a framework leveraging LLM-driven representation learning, with contrastive and generative alignment methods to improve recommendations. AlphaRec ____ replaces ID-based embeddings with language embeddings and combines GCN and CL for a simple yet effective recommendation approach. LLMs as encoders ____: EasyRec ____ leverages collaborative information and textual data from users and items to retrain language models for recommendation, achieving impressive performance in zero-shot scenarios. Despite their impact in respective fields, they overlook the potential of LLM-based intents to enhance interpretability.

\subsection{Disentanglement-based Recommendation}
Disentanglement-based methods generally focus on modeling user-item interactions by projecting them into distinct feature spaces ____. For instance, MacridVAE ____ leverages variational autoencoders to encode various user intents ____. DGCF ____ employs graph neural networks to learn disentangled user representations. DisenHAN ____ utilizes meta-relation decomposition along with disentangled propagation layers to capture semantic meanings. In the case of CDR ____, a dynamic routing mechanism is developed to characterize the correlations among user intents for embedding denoising. KGIN ____ introduces the concept of shared intents and uses an item-side knowledge graph to capture userâ€™s path-based intents. Some innovative approaches have started integrating contrastive learning into intent modeling, such as ICLRec ____, DCCF ____, and BIGCF ____. DCCF ____ enhances self-supervised signals by learning disentangled representations with a global context, while BIGCF investigates the individuality and collectivity of intents behind interactions for collaborative filtering. While effective, multimodal intents present a promising avenue for exploration.

\begin{figure}[h!tp]
    \centering
    \includegraphics[width=\linewidth]{Figure/item_intent.pdf}
    \caption{Case study on LLM-based item intent generation in Amazon-movie dataset.}
    \label{fig:item_intent}
\end{figure}

\begin{figure}[h!tp]
    \centering
    \includegraphics[width=\linewidth]{Figure/user_intent.pdf}
    \caption{Case study on LLM-based user intent generation in Amazon-movie dataset.} 
    \label{fig:user_intent}
\end{figure}