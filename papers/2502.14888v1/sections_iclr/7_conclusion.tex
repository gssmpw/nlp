% \vspace{-3mm}
\section{Conclusion}
% \vspace{-3mm}
In this study, we explored the monosemanticity of features within the CLIP model to elucidate the commonalities and distinctions across visual and linguistic modalities. 
% By adapting existing interpretability tools into the multi-modal regime, we developed a systematic pipeline to evaluate feature monosemanticity in multi-modal models. Notably, 
We successfully categorized interpretable features according to their predominant modality, which demonstrate close correspondence to human cognitive interpretations.Our interpretability analysis in three case studies also demonstrated the great potential in understanding modality-features in gender bias, adversarial attacks and multimodal generation. Future work may extend these methodologies to other multi-modal architectures and investigate their implications for cognitive science, ultimately fostering the development of more interpretable and cognitively aligned AI systems. 
% These insights affirm that modern interpretability tools can significantly enhance our understanding of multi-modal deep learning models, offering a principled approach to dissecting modality interactions.

\section*{Acknowledgements}
This work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) through a Turing AI Fellowship (grant no. EP/V020579/1, EP/V020579/2). YW was funded by
Office of Naval Research grant N00014-20-1-2023 (MURI ML-SCOPE), NSF Award CCF-2112665
(TILOS AI Institute), and an Alexander von Humboldt Professorship.