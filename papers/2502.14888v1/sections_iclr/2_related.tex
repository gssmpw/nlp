\section{Related Work}

This work sits at the intersection of several active research areas: mechanistic interpretability, multimodal representation learning, and the study of modality gaps.  
% We discuss relevant work in each of these areas below.

\textbf{Mechanistic Interpretability.}
Mechanistic interpretability aims to understand the internal computations of deep learning models by identifying and analyzing individual components and interactions. Recent research has been focused on identifying \textit{polysemanticity}, where individual neurons respond to multiple, unrelated features \citep{olah2020zoom}. This has led to the exploration of \textit{monosemanticity}, the hypothesis that models might contain features that correspond to single, interpretable concepts \citep{elhage2022solu}. 
Recent advances in dictionary learning have made it possible to decompose polysemantic neurons into monosemantic features \citep{cunningham2023sparse}. These techniques, coupled with automated methods for interpreting and labeling features \citep{bills2023language, Gurnee2023FindingNI, Yan2024EncourageOI}, have enabled the extraction of large numbers of interpretable features from models like CLIP \citep{radford2021learning}. These interpretable features can be studied to understand 
the various aspects of model behavior, especially, as explored in this paper, the nature of modality gaps.
% \cite{radford2021learning, elhage2022solu, bills2023language, Gurnee2023FindingNI, Yan2024EncourageOI, wang2022interpretability}

\textbf{Multimodal Representation Learning.}
Learning effective representations from multiple modalities has been a long-standing research focus. Early approaches often relied on hand-crafted features and statistical methods~\citep{ngiam2011multimodal}. With the rise of deep learning, multimodal representation learning has been revolutionized by models like CLIP~\citep{radford2021learning},  VILT~\citep{kim2021vilt}, and DeCLIP~\citep{li2022supervision}. These models leverage large-scale datasets and contrastive learning objectives to learn joint representations of images and text. They have achieved remarkable success in various downstream tasks, such as image retrieval, zero-shot classification, and visual question answering. Our work utilizes CLIP as a testbed for analyzing modality gaps, taking advantage of its strong performance and readily available pre-trained weights. Our work is also complementary to prior efforts on visualizing and interpreting multimodal models~\citep{liang2022multiviz,wang2021m2lens}; we focus on understanding the internal representations of pre-trained CLIP models and how they handle modality-specific and shared information.

\textbf{Modality Gaps.}
The study of modality gaps, or the differences and limitations in how different modalities represent information, has been a topic of interest in cognitive science for decades~\citep{spence2011crossmodal, paivio1991dual, calvert2004handbook}. Researchers have investigated how humans integrate and differentiate information across sensory modalities, revealing both commonalities and distinct characteristics. However, these studies are mostly based on human studies and experiments. Our research offers an alternative \emph{human-free} approach to study the modality gap through the neural networks directly learned from these modalities. This opens a new approach to study the modality gap that could alleviate potential bias from human-centric viewpoint and bring more insights from large-scale data. 
% By connecting these findings to human cognitive understanding, we aim to bridge the gap between artificial and human intelligence.
% translating these insights to artificial intelligence systems remains a challenge.  While some work has explored modality-specific challenges in specific tasks, like image captioning~\citep{chen2017show}, a systematic analysis of modality gaps in general-purpose multimodal models has been lacking.  Our work addresses this gap by proposing a novel methodology for identifying and characterizing modality-specific and cross-modal features in multimodal models.  