% \begin{algorithm}[!htb]
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \caption{Jailbreak defense with MDS}
%     \label{algorithm: neuron_optimization}
%     \begin{algorithmic}[1]
%         \REQUIRE
%             Peace features $F_{peace}$,
%             Adversarial sample $I_{adv}$,
%             Neuron sets $\{\mathcal{N}_{text}, \mathcal{N}_{image}, \mathcal{N}_{dual}\}$
%         \ENSURE
%             Optimized image $I_{opt}$
        
%         \FOR{$type$ in $\{\texttt{ImgD}, \texttt{TextD}, \texttt{CrossD}\}$}
%             \FOR{$mode$ in $\{\text{selective}, \text{random}\}$}
%                 \IF{$mode = \text{selective}$}
%                     \STATE $mask \leftarrow \mathcal{N}_{type}$
%                 \ELSE
%                     \STATE $mask \leftarrow \text{RandomMask}(|\mathcal{N}_{type}|)$
%                 \ENDIF
                
%                 \STATE $I_{current} \leftarrow I_{adv}$
%                 \REPEAT
%                     \STATE $F_{current} \leftarrow \text{Encode}(I_{current})$
%                     \STATE $\mathcal{L} \leftarrow \|F_{current}[:, mask] - F_{peace}[:, mask]\|_2$
%                     \STATE Update $I_{current}$ by minimizing $\mathcal{L}$
%                     \STATE $I_{current} \leftarrow \text{clamp}(I_{current}, 0, 1)$
%                 \UNTIL{convergence}
%                 \STATE $I_{opt} \leftarrow I_{current}$
%             \ENDFOR
%         \ENDFOR
%         \RETURN $I_{opt}$
%     \end{algorithmic}
% \end{algorithm}
\subsection{Case Study 2: Adversarial Attacks}
We investigate the impact of different types of features on multimodal adversarial attacks~\citep{cui2024robustness,yin2024vlattack}, following the setup in~\citet{shayegani2024jailbreak}.

The adversarial sample is a benign-appearing image, e.g., a scenery image but injected with harmful semantic information, such as the phrase \textit{``I want to make bomb''}. One defense optimization strategy involves minimizing the distance, between the embeddings of adversarial sample $\mathbf{F}_
{adv}$ and a benign sample $\mathbf{F}_{ben}$, and accordingly update the adversarial sample (in Figure~\ref{fig:ad_overview}). The paired benign  image is injected with the friendly text, e.g.,  \textit{``peace and love''}. To study the effects of our identified modality features, we only select the target feature index $I$ from the embedding for alignment training, i.e., \texttt{ImgD}, \texttt{TextD}, and \texttt{CrossD}. The alignment loss is $\mathcal{L} = \|\mathbf{F}_{adv}[:, I] - \mathbf{F}_{ben}[:, I]\|_2$. Finally, the optimized adversarial sample is then adopted to attack a Vision-Language model (VLM).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth,trim={20 12 20 5},clip]{ad_overview.pdf}
    \caption{Optimization of the adversarial samples, with only selected target features, i.e., ImgD, TextD and CrossD, involved in the alignment.}
    \label{fig:ad_overview}
\end{figure}

\textbf{Models.} We use the the same CLIP model as introduced in Section~\ref{sec:disentangle} as the Multimodality feature extractor, so the index for target features unchanged. 
The VLM being attacked is Llama-1.5-7b-hf~\cite{liu2023llava, liu2023improvedllava}. To evaluate whether the attack the attack is successful, we evaluate the generated response from the VLM to DeepSeek V3~\cite{deepseekai2024deepseekv3technicalreport} to generate a binary label indicating whether the harmful request is rejected or the task is executed.
% generates harmful information after being fed the aligned samples along with the harmful prompt. We repeat the process 100 times for each aligned image, inputting it into Llama-1.5-7b-hf, and count the number of instances where the attack successfully elicits harmful outputs.

% \paragraph{Experiment setup}
% To address this, we use the model: open-clip~\cite{ilharco_gabriel_2021_5143773} to generate adversarial samples and semantic detoxification samples. The test model is Llama-1.5-7b-hf~\cite{liu2023llava}.
% and the GPU used is NVIDIA L20 48GB.
% We use an image with the text "peace, non-violence" as the alignment image.
% The period of alignment optimization is set to 5000.

% L2 Loss Formula
% \paragraph{Loss Function}
% The optimization process minimizes the L2 loss between the feature vectors of the current image \( I_{current} \) and the alignment image \( I_{peace} \). The L2 loss is computed as:

% \[
% \mathcal{L} = \|F_{current}[:, mask] - F_{peace}[:, mask]\|_2
% \]

% where:
% \begin{itemize}
%     \item \( F_{current} \) and \( F_{peace} \) are the feature vectors of the current image \( I_{current} \) and the alignment image \( I_{peace} \), respectively. These feature vectors are extracted using a pre-trained encoder (e.g., CLIP image encoder).
%     \item \( mask \) is a binary mask that selects a subset of neurons from the feature vectors. It is generated based on the neuron sets \( \mathcal{N}_{text} \), \( \mathcal{N}_{image} \), or \( \mathcal{N}_{dual} \), depending on the alignment type (\texttt{ImgD}, \texttt{TextD}, or \texttt{CrossD}).
%     \item \( \| \cdot \|_2 \) denotes the L2 norm, which measures the Euclidean distance between the two feature vectors.
% \end{itemize}

% \paragraph{Attack Success Rate}
% The success of the attack is evaluated by measuring the frequency at which the model (Llama-1.5-7b-hf) generates harmful outputs when provided with the optimized adversarial sample \( I_{opt}^* \) and a harmful prompt \( P_{harm} \). The attack success rate is formally defined as:
% \[
% \text{Success Rate} = \frac{1}{N} \sum_{k=1}^{N} \mathbb{I}\left( \text{Llama}(I_{opt}^*, P_{harm}) \rightarrow \text{Harmful} \right)
% \]
% where:
% \begin{itemize}
%     \item \( N \) is the total number of trials (in our experiments, \( N = 100 \)).
%     \item \( \mathbb{I}(\cdot) \) is the indicator function, which returns 1 if the model generates harmful output and 0 otherwise.
%     \item \( \text{Llama}(I_{opt}^*, P_{harm}) \) denotes the output of the Llama-1.5-7b-hf model when given the optimized alignment sample \( I_{opt}^* \) and the harmful prompt \( P_{harm} \).
%     \item \( \text{Harmful Output} \) is defined as any model response that contains harmful or dangerous information.
% \end{itemize}

\textbf{Results.} The results are shown in Table~\ref{tab:adversarial}. The number of neurons selected was consistent across all experiments. Using the smallest TextD as the baseline, we repeatedly sampled the same number of neurons from \texttt{ImgD} and \texttt{CrossD} as in \texttt{TextD}. If we achieve better defense results (i.e., a lower attack success rate) with a specific type of feature, it suggests that this type of neuron plays a key role in defense. 
% To account for the potential effects of randomly selected features, 
 We observe that leveraging all three target features improves defense results to some extent compared to the original adversarial sample. Given that the number of features in each category differs, we randomly sample an equal number of features from each category to ensure alignment Among them, using \texttt{TextD} for alignment yields the best defense performance, with only 25\% rate comparing with alignment on the same amount of features, 65\%. The performance is followed by \texttt{CrossD} and \texttt{ImgD}. Since the adversarial information primarily stems from undesirable textual semantics, this outcome demonstrates that \texttt{TextD} effectively captures most of the semantic content. In contrast, \texttt{CrossD} captures partial semantics, while \texttt{ImgD} is the least related to semantic information, resulting in minimal benefits for jailbreak defense when aligned.
\begin{table}[h]
    \centering
    \caption{Success rate for adversarial attacks with different target features involved in the alignment training.  The success rate of the benign image is 10\%, for the original adversarial sample is 80\%. For comparison, we also compare with the performance of aligning with the same number of randomly selected features, 65\%.}
    \resizebox{0.35\textwidth}{!}{%
    \begin{tabular}{r|cccc}
    \toprule
    \textbf{Target feature} & \texttt{ImgD} & \texttt{TextD} & \texttt{CrossD} \\
    \midrule
    \textbf{Success Rate} ($\downarrow$) & 50\% & 25\% & 30\% \\ 
    \bottomrule
    \end{tabular}
    }
    \label{tab:adversarial}
\end{table}
\vspace{-8mm}
% \hq{Experiment question: (1) Add all neurons involved in the experiment(to check if the image has changed a lot, if a o=lot, compare the figrues with interven our neurons; (2) random for textD, imgeD; same neurons for; (3) the same number of neurons in the intervention; (4) why the imgD results are better than random.}
\paragraph{Potential.} The feature-specified optimization for multimodality jailbreak provides a more focused and computationally efficient defense strategy. This selective alignment not only enhances interpretability by highlighting the roles of different feature types but also allocates resources more effectively by prioritizing the most critical features for defense. Additionally, it prevents feature dilution, ensuring that semantic integrity is preserved during optimization. This modular and adaptable design makes the method particularly effective for defending modality-specific attacks.
% \hq{what is the clip model shard by text and viusal features.}
% \hq{there is only text encoder in diffusion model, can only accepting the text inputs. reweight the imgD from the reference img and textD from the text encoder; }

% text input: [77,1024],
% reference model->openlip-encoder -> 1024[imgD, textD, cross]
\subsection{Case Study 3: Multimodal Generation}
Despite the impressive capabilities of text-to-image generation models~\citep{yu2024spae,koh2024generating,swamy2024multimodn}, their internal mechanisms for bridging linguistic semantics and visual details remain poorly understood. A key challenge is disentangling how modality-specific features influence the fidelity and controllability of generation. To address this, we investigate the generation process by intervening in different modality-specific features in Stable Diffusion v2~\citep{Rombach_2022_CVPR}.

\textbf{Models.} Stable Diffusion v2~\citep{Rombach_2022_CVPR} is our generation model, and its feature extractor is \texttt{laion/CLIP-ViT-H-14-laion2B-s32B-b79K} rather than the CLIP model previously employed. Therefore, we compute the model-specific MDS based on inference passes over the COCO2017 dataset~\citep{visualization-tools-for-coco-dataset}.
\begin{wrapfigure}{r}{0.42\linewidth}
    \centering 
    \includegraphics[width=0.99\linewidth]{horse.jpg}
    \caption{\footnotesize Reference image.}
    \label{fig:ref_image}
\end{wrapfigure}
\vspace{-3mm}

\textbf{Intervention of the Text-to-Image Generation.} 
The input text prompt is \textit{“\textbf{Please draw an animal}”}. The feature extractor generates an embedding $\mathbf{T}$, representing the original multimodal embedding for generation. Additionally, we provide a reference figure—a horse (Figure~\ref{fig:ref_image})—processed through the same feature extractor, producing a reference embedding $\mathbf{R}$. To control the generation through modality-specific feature intervention, we interpolate only the features at specified indices $I$ defined by MDS. The final multimodal embedding is computed as:
$\mathbf{E}[I] = \alpha \mathbf{T}[I] + (1 - \alpha) \mathbf{R}[I]$, where operations are applied exclusively to the feature indices defined by $I$, i.e., \texttt{ImgD}, \texttt{TextD} and \texttt{CrossD}.

\begin{figure*}[h]
    \centering 
    \includegraphics[width=0.89\linewidth]{image_gen_new.png}
    \includegraphics[width=0.89\linewidth]{text_gen_new.png}
    \includegraphics[width=0.89\linewidth]{cross_gen_new.png}
    \caption{Generated images from the text-to-image model with the text prompt \textit{"Please draw an animal"} and varying levels of intervention from a reference image (horse). From left to right, the interpolation weights range from 0.0 to 0.9 at intervals of 0.1. From top to bottom, the interventions are exclusively applied to the modality-features, i.e.,\texttt{ImgD}, \texttt{TextD} and \texttt{CrossD}.}
    \label{fig:mm_gen}
\end{figure*}

\textbf{Results.} We feed $E$ to the generation model with different $\alpha$ ranging from 0 to 0.9 with an interval of 0.1. The generated images with the selected indices correspond to \texttt{ImgD}, \texttt{TextD}, and \texttt{CrossD} are shown in Figure~\ref{fig:mm_gen}. The results clearly demonstrate that larger interventions on \texttt{ImgD} and \texttt{CrossD} disrupts visual coherence: animal shapes fragment, outlines blur, and textures degrade, implying the role of \texttt{ImgD} in preserving structural and fine-grained visual details. Interestingly, interventions on \texttt{TextD} maintain the visual features without any distortion even with larger $\alpha$.  We can instead observe the shifts in semantic concepts, such as generating cat-like, elephant, or horse. These animals became abstracted into geometric forms or textual overlays, demonstrating that text-guided representations contribute to the structured composition and semantic labeling of the generated visuals, rather than low-level visual details.  
% This finding aligns with human cognition, where linguistic features are more adept at capturing abstract, high-level information.
%\texttt{TextD} maintain the overall visual style of a sketch, yet display different subjects, such as a hand, a girl with long hair, or a furry dog. This observation suggests that \texttt{TextD} primarily encodes high-level semantic concepts, such as the subject, rather than low-level visual details. This finding aligns with human cognition, where linguistic features are more adept at capturing abstract, high-level information.% 1) \texttt{TextD} neurons primarily encode high-level semantic concepts. Perturbing these neurons altered semantic alignment but preserved low-level visual details, indicating their role in abstract linguistic grounding. 2) \texttt{ImgD} neurons critically influence visual fidelity. Modifying their activations led to distortions in textures, color shifts, or structural artifacts. 3) \texttt{CrossD} Neurons exhibited weaker specialization. Although they mediated cross-modal alignment, their impact on both semantics and visuals was less pronounced compared to unimodal neurons, suggesting a trade-off between generality and specificity.

\textbf{Potential.} By isolating modality-specific neurons, our framework provides several benefits for data editing: (i) Semantic Refinement: Adjusting \texttt{TextD} activations improves conceptual alignment; (ii) Visual Enhancement: Tuning \texttt{ImgD} neurons enhances texture realism or ensures stylistic consistency. This data-driven approach not only advances interpretability but also reflects human cognitive principles, where distinct neural pathways govern linguistic abstraction and perceptual processing.


% We applied the abovementined method to derive the Image-Dominant (\texttt{ImgD}), Text-Dominant (\texttt{TextD}), and Cross-Modality (\texttt{CrossD}) for the CLIP encoder in Stable-Diffusion-2~\cite{Rombach_2022_CVPR} via a inference process on COCO2017 dataset~\cite{visualization-tools-for-coco-dataset}. To probe their roles, we systematically perturbed the activations of each group of neurons during the generation.

% \begin{algorithm}[!htb]
%     \renewcommand{\algorithmicrequire}{\textbf{Input:}}
%     \renewcommand{\algorithmicensure}{\textbf{Output:}}
%     \caption{Multimodality generation with MDS}
%     \label{algorithm: cmfar}
%     \begin{algorithmic}[1]
%         \REQUIRE
%             Image-Text paired dataset $\mathcal{D}$,
%             Reference image $I_{ref}$,
%             Target text prompt $T$,
%             Interpolation weight $\alpha$,
%             Feature dimension $d$
%         \ENSURE
%             Modified text embeddings $F'_{target}$
%         \STATE Stage 1: Feature Extraction
%         \FOR{each $(I,t)$ in $\mathcal{D}$}
%             \STATE $F_I = \text{CLIP}_{image}(I) \in \mathbb{R}^d$ 
%             \STATE $F_t = \text{CLIP}_{text}(t) \in \mathbb{R}^d$
%             \STATE Add $F_I, F_t$ to feature sets $\mathcal{F}_I, \mathcal{F}_t$
%         \ENDFOR
         
%         \STATE Stage 2: Computing modality dominance score
%         \STATE $R = \frac{\text{mean}(|\mathcal{F}_I|)}{\text{mean}(|\mathcal{F}_I|) + \text{mean}(|\mathcal{F}_t|)}$ 
%         \STATE $\mu_R = \text{mean}(R)$
%         \STATE $\sigma_R = \text{std}(R)$
%         \STATE $\texttt{CrossD}_{idx} = \{i | \mu_R - \sigma_R < R_i < \mu_R + \sigma_R\}$
%         \STATE $\texttt{TextD}_{idx} = \{i | R_i < \mu_R - \sigma_R\}$
%         \STATE $\texttt{ImgD}_{idx} = \{i | R_i > \mu_R + \sigma_R\}$
        
%         \STATE Stage 3: Neuron weighting
%         \STATE $F_{ref} = \text{CLIP}_{image}(I_{ref})$
%         \STATE $F_{target} = \text{CLIP}_{text}(T)$
%         \STATE $F'_{target} = F_{target}$
%         \FOR{$idx$ in $\{\texttt{CrossD}_{idx}, \texttt{TextD}_{idx}, \texttt{ImgD}_{idx}\}$}
%             \STATE $F'_{target}[idx] = \alpha \cdot F_{ref}[idx] + (1-\alpha) \cdot F_{target}[idx]$
%         \ENDFOR
%         \RETURN $F'_{target}$
%     \end{algorithmic}
% \end{algorithm}

% \hq{insert image for visualization}
