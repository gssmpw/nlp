% \subsection{Human evaluation.} To assess whether the proposed automatic methods align with human perception of interpretability, we invited three human annotators to participate in the evaluation. Specifically, we presented the top five activated images and text sentences for each of 100 neurons. Annotators were asked to examine the extent to which they could identify the semantic meaning of each neuron and provide a scalar rating between 1 and 5, where 1 indicates the lowest interpretability and 5 indicates the highest. The average Cohen's kappa between annotators is XXX. Table~\ref{tab:human_eval} shows the correlation between human annotations and automatic metrics.

% \begin{table}[h]
%     \centering
%     \small
%     \caption{Correlation between automatic interpretability metric and human evaluation.}
%     \resizebox{0.40\textwidth}{!}{%
%     \begin{tabular}{r|cc|cc|cc}
%     \toprule
%  \multirow{2}{*}{Metrics}  & \multicolumn{2}{c}{P1} & \multicolumn{2}{c}{P2} & \multicolumn{2}{c}{P3}\\
%      \cmidrule{2-7}
%      & image & text & image & text &image & 
%      text \\
%     \midrule
%      Similarity  &  \\
%      WinRate & \\
%     \bottomrule
%     \end{tabular}
%     }
%     \label{tab:human_eval}
% \end{table}