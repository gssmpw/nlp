\begin{abstract}
% Multi-modal models are integral to the advancement of foundation models in artificial intelligence, enabling the processing of information across different data modalities such as vision and language. These models fundamentally rely on the hypothesis that different modalities share common, cross-modal features. However, certain features are modality-specific; for instance, some emotions cannot be visualized, and certain visual perceptions cannot be precisely described through language. The exploration of modality commonality and gaps has been a focal point in cognitive science, yet studies are often human-centric and may not translate directly to artificial systems due to differences in processing and representation.

% In this paper, we leverage recent developments in feature monosemanticity to extract interpretable features from deep models, providing a data-driven understanding of modality gaps. Specifically, we examine CLIP (Contrastive Language-Image Pretraining)~\citep{radford2021learning}, a prominent visual-language representation model trained on extensive image-text pairs. Extending existing interpretability tools in language, we propose to multi-modal foundation models, such as GPT-4o, to measure the \emph{multi-modal interpretability} of CLIP features, as well as attributing their interpretability to each modality with our Modality Dominance Score (MDS). With the help of these multi-modal interpretability tools, we can transform CLIP features into a more interpretable space, allowing us to multi-modal features of CLIP into three classes: vision features, language features (both single-modal), and visual-language features (cross-modal). We find that the cateogization agress surprisingly well with human cognitive understanding of modality. In this way, we show that large-scale multi-modal models can provide us with a fundamental tool to understand the key connections and differences between different modalities in a \emph{fully data-driven} approach, providing new toolkits for both cognitive science and machine learning algorithm.

In this paper, we leverage recent advancements in feature monosemanticity to extract interpretable features from deep multimodal models, offering a data-driven understanding of modality gaps. Specifically, we investigate CLIP (Contrastive Language-Image Pretraining), a prominent visual-language representation model trained on extensive image-text pairs. Building upon interpretability tools developed for single-modal models, we extend these methodologies to assess  \emph{multi-modal interpretability} of CLIP features. Additionally, we introduce the Modality Dominance Score (MDS) to attribute the interpretability of each feature to its respective modality. Next, we transform CLIP features into a more interpretable space, enabling us to categorize them into three distinct classes: vision features (single-modal), language features (single-modal), and visual-language features (cross-modal). Our findings reveal that this categorization aligns closely with human cognitive understandings of different modalities. We also demonstrate significant use cases
of this modality-specific features including detecting gender bias, adversarial attack defense and text-to-image model
editing. These results indicate that large-scale multimodal models, equipped with task-agnostic interpretability tools, offer valuable insights into key connections and distinctions between different modalities.
% This work not only bridges the gap between cognitive science and machine learning, but also introduces new data-driven tools to advance both fields. 
% \\
% \textbf{\textcolor{red}{Content warning: We provide illustrative adversarial attack examples to reveal the
% generative modelsâ€™ vulnerabilities, aiming for the development of robust models.}}
% facilitating 
% a modal modal multi-modal features and illuminate various aspects of the visual-language modality gap. Moreover, we demonstrate that visual-language features correspond to cross-modal semantics and play a crucial role in cross-modal retrieval tasks. Our results indicate that, with modern interpretability tools, deep learning models can offer a principled approach to understanding the commonalities and differences between modalities, suggesting a future where interpretable AI models enhance our comprehension of the world and the human brain.
    
\end{abstract}