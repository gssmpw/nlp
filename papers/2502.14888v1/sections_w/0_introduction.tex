% \begin{abstract}
% \end{abstract}


% \begin{quote}
%     \emph{To understand how the brain synthesizes information from the different senses, we must study not only how information from each sensory modality is decoded but also how this information interacts with the sensory processing taking place within other sensory channels.} --- \citet{calvert2004handbook}
% \end{quote}

\section{Introduction}

Multi-modal models have become foundational in the development of artificial intelligence systems, enabling the processing and understanding of information from multiple data modalities, such as vision and language~\citep{radford2021learning, kim2021vilt, lu2019vilbert}. These models are built on the premise that different data modalities share common, or cross-modal, features that can be jointly learned~\citep{ngiam2011multimodal}. However, it is widely acknowledged that certain features are modality-specific; for example, some emotions are difficult to visualize, while certain visual experiences cannot be accurately described through language~\citep{paivio1991dual}.

The exploration of modality commonality and gaps has long been a focus in cognitive science, where researchers have investigated how humans integrate and differentiate information across sensory modalities~\citep{spence2011crossmodal}. However, these studies are often human-centric and may not directly translate to artificial systems due to fundamental differences in how information is processed and represented~\citep{calvert2004handbook}. Meanwhile, recent advances in interpretability methods, particularly in the area of monosemantic features, providing a promising path towards a more detailed understanding of deep models~\citep{elhage2022solu,bills2023language,Gurnee2023FindingNI,Yan2024EncourageOI}. Monosemantic features/neurons refer to model components that correspond to a single, interpretable concept or feature. By leveraging these methods, we can extract monosemantic, interpretable features from deep learning models, providing a data-driven approach to exploring modality gaps.

In this paper, we focus on CLIP (Contrastive Language-Image Pretraining)~\citep{radford2021learning}, a visual-language representation model trained on massive image-text pairs. We investigate the modality association of features extracted from CLIP by introducing a modality metric that categorizes these interpretable features into: vision, language and visual-language features.

Our study reveals that single-modal features align well with human cognition and highlight diverse aspects of the visual-language modality gap. We find that visual-language features capture modality-aligned semantics. These findings suggest that interpretability tools can enable deep models to provide a systematic understanding of the similarities and distinctions between different modalities.

% Our contributions are as follows:

% \begin{itemize}
%     \item We propose a modality metric to assess the modality belongness of features extracted from multi-modal models.
%     \item We apply this metric to CLIP and categorize interpretable features into vision, language, and visual-language classes.
%     \item We demonstrate that single-modal features correspond to modality-specific concepts, aligning with human cognitive understanding.
%     \item We show that cross-modal features are crucial for tasks requiring integration of visual and textual information.
% \end{itemize}

% Our results indicate a future where interpretable AI models can deepen our understanding of the world and the human brain, bridging the gap between artificial and human cognition.

% \section{introduction}

% \yw{

% % background: multi-modal models find extensive use and is important for foundation models.

% % however, these models fundamentally rely on a key hypothesis that different data modalities share common (or cross-model) features. meanwhile, we also know that some features are modality-specific. for example, some feelings that cannot be pictured, and some vision cannot be described (accurately) through language. the modality commonality and gap has been a fascinating area for cognitive science. however, these studies are  often human-centric (and thus xxx). but despite its wide application in deep learning, it is seldom studied due to the lack of interpretability of these multi-modal models.

% % recent studies on feature monosemanticity sheds light  on a principled way to extract monosemantic (interpertable) \emph{features} from deep models, allowing us to have a data-driven understanding of modality gaps. In particular, we take clip (cite), well-known visual-language representation model trained from massive visual-language pairs, and look into the modality belongs of extracted features. By designing an modality metric, we caterogize all interpretable features to three classes, vision features, language features (the two are single-modal features),  and visual-language features (which are cross-modal features). We reveal that the single-modal features do align well with human understandings and reveal diverse aspects of the visual-language modality gap; meanwhile, we find that visual-language features correspond to the cross-modal semantics and play a major part in cross-modal retrieval (eg using prompts to retrieval relevant images). 

% % Our results show that when equipped with modern interpretability tools, deep learning models can provide us a principled way to understand the commonality and differences between different modalitiesn. It indicates a future that interpretable AI models deepen human understanding of the world and human brain.

% % }

% \section{Related Work}

% Olah's work.

% Other works on multi-modal monosemanticity.