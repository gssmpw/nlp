
% \section{Problem Setup}

% In this work, we primarily study the visual-language features extracted by CLIP models. CLIP (Contrastive Language-Image Pretraining) consists of an image encoder $f_i$ and a text encoder $f_t$, which are jointly pretrained on a large set of image-text pairs, denoted as $(x_i, x_t)$, using contrastive learning~\citep{radford2021learning}. Specifically, CLIP encourages the representations of positive pairs to be aligned in a shared embedding space (i.e., matching image-text pairs), while pushing apart the representations of negative pairs (i.e., non-matching image-text pairs).
% After pretraining, these representations can be leveraged for various downstream tasks. For example, multi-modal large language models (LLMs) such as LLaVA~\citep{liu2024llava} and BLIP~\citep{li2022blip} are built upon CLIP representations. Additionally, many text-to-image generation models, such as Stable Diffusion~\citep{Rombach_2022_CVPR}, utilize CLIP's shared embedding space to facilitate the generation process. Therefore, CLIP representations serve as a fundamental basis for representing images and text in a shared space, enabling a wide range of multi-modal applications.

% \textbf{CLIP Backbones.} 



\section{Towards Multi-modal Monosemanticity}
In this section, we build a pipeline to extract monosemantic multi-modal features and evaluate interpretability the these features. We also characterize the \textbf{modality relevance} in extracted features with the proposed Monosemantic Relevance Score (MRS).

We consider two CLIP models, i.e., canonical ViT-B-32 CLIP model by OpenAI~\citep{radford2021learning} and a popular CLIP variant, DeCLIP~\citep{li2022supervision}. Beyond multi-modal supervision (image-text pairs), DeCLIP also integrates single-modal self-supervision (image-image pairs and text-text pairs) for more efficient joint learning. We hypothesize that, with the incorporation of self-supervision tasks, DeCLIP is able to extract more single-modal features from the data, enhancing its interpretability and alignment with modality-specific characteristics.

% for evaluating multimodal representations through monosemanticity. It consists of the following components: 1) two methods for \textbf{extracting} monosemantic multi-modal features: multi-modal SAE and multi-modal NCL; 2) \textbf{evaluating} the interpretability of multi-modal features with new scalable interpretability metrics; and 3) characterizing the \textbf{modality relevance} in extracted features with the proposed Monosemantic Relevance Score (MRS).

% In this paper, we consider two CLIP models for comparison. The first is the canonical ViT-B-32 CLIP model by OpenAI~\citep{radford2021learning}, pretrained with a pure image-text contrastive loss. Additionally, we consider a popular CLIP variant, DeCLIP~\citep{li2022supervision}, which integrates multi-modal supervision (image-text pairs) with single-modal self-supervision (image-image pairs and text-text pairs) for more efficient joint learning. We hypothesize that, with the incorporation of self-supervision tasks, DeCLIP is able to extract more single-modal features from the data, enhancing its interpretability and alignment with modality-specific characteristics.


\subsection{Interpretability Tools for Multi-modal Monosemantic Feature Extraction}
The features in deep models are observed to be quite \emph{polysemantic} \citep{olah2020zoom}, in the sense that activating samples along each feature dimension often contain multiple unrelated semantics. Therefore, we first need to disentangle the CLIP features to have \emph{monosemantic features}. Borrowing from the recent progress in monosemanticity in self-supervised models, we study the two methods to attain better multi-modal monosemanticity.
% Although CLIP features contain rich multi-modal semantics, we often find them to be quite \emph{polysemantic} \citep{olah2020zoom}, 
% in the sense that activating samples along each feature dimension often contain multiple unrelated semantics.

% Therefore, as a first step, we need to disentangle the CLIP features to have \emph{monosemantic features}. Borrowing from the recent progress in monosemanticity in self-supervised models, we study the two methods to attain better multi-modal monosemanticity.

\textbf{Multi-modal SAE.} Sparse Autoencoders (SAEs) \citep{cunningham2023sparse} are a new scalable interpretability method,
% that decomposes polysemantic \emph{neurons} into interpretable monosemantic {features}, 
demonstrating success in multiple large language models (LLMs) \citep{templeton2024scaling,gao2024scaling,lieberum2024gemma}. Here, we train a \emph{multi-modal SAE (MSAE)} $g^+$ by training \textbf{one} SAE model to reconstruct both image and text representations. Specifically, we adopt a top-K SAE model \citep{makhzani2013k,gao2024scaling},
% \begin{equation}
% \text{(latent) }z=\operatorname{TopK}\left(W_{\text {enc }}\left(x-b_{\text {pre }}\right)\right),\quad \text{(reconstruction) }\hat{x}=W_{\mathrm{dec}} z+b_{\mathrm{pre}}, 
% \end{equation}
and train it with a \emph{multi-modal reconstruction objective}.
% \begin{equation}
%     \gL_{\rm M-SAE}(g)=\E_{(x_i,x_t)\sim \gP}\left[(x_i-g(x_i))^2+(x_t-f(x_t))^ 2\right],
% \end{equation}
% with $W_{\text {enc }} \in \mathbb{R}^{n \times d}, b_{\text {enc }} \in \mathbb{R}^n, W_{\text {dec }} \in \mathbb{R}^{d \times n}$, and $b_{\text {pre }} \in \mathbb{R}^d$. 
In this way, the sparse latent feature $z\in\sR^n$ can encode multi-modal representations from both modalities.

\textbf{Multi-modal NCL.} 
% A key limitation of leveraging SAE for extracting multi-modal features is that its reconsstruction objective is still essentially singe-modal, and thus does not take the multi-modal nature of CLIP into consideration.
Inspired by the interpretable self-supervised loss with non-negative constraint (NCL) proposed by \citep{wang2024ncl} to extract sparse features, we adapt it to enhance multi-modal interpretability. 
% Specifically, give a pretrained CLIP model with an image encoder $f_i$ and a text encoder $f_t$, 
A shared MLP network (of similar size to SAE) on top of the encoder outputs is trained by a \emph{Multi-modal NCL} loss.
% \begin{equation}
%     L_{\rm M-NCL}(g^+)=-\E_{x_i,x_t\sim\gP}\log\frac{\exp(g^+(f_i(x_i))^\top g^+(f_t(x_t))}{E_{x^-_t}\exp(g^+(f_i(x_i))^\top g^+(f_t(x^-_t))},
% \end{equation}
% where $f_i$,$f_t$ are image and text encoder. The MLP network $g^+:\sR^d\to\sR^n$ is designed to have  non-negative outputs, e.g.,
% \begin{equation}
%    g^+(x)=\operatorname{ReLU}(W_2\operatorname{ReLU}(W_1x+b_1)+b_2),\ \forall\ x\in\sR^d. 
% \end{equation}
% \citet{wang2024ncl} show that with the help of the non-negative constraints, NCL can extract highly sparse features with much better monosemanticity.

% Here, an SAE consists of the bias encoder weight $W_{\rm enc}$ and bias $b_{\rm pre}$, a sparse activation $\operatorname{Topk}$ \citep{makhzani2013k}, the decoder weights $W_{\rm dec}$ and bias $b_{\rm pre}$.
% that only activate
% recon the learning objective
% separate SAEs for every Specifically, for  we take the features
% for  the top-$k$ SAE \citep{gao2024scaling} as an example, we apply an


% \yw{we need illustrative examples of original (polysemantic), NCL and SAE features.}

\subsection{Measures for Multi-modal Interpretability}
% Aside from the interpretability tools as above, we also need quantitative measures of the interpretability (i.e., monosemanticity) of the extracted  features that are scalable to large multi-modal models. 
Existing quantitative interpretability measures \citep{bills2023language} often require access to high pricing models (like GPT-4o) and suffer from poor scalability and poor precision \citep{gao2024scaling}, damping the progress of open science. It motivates us to propose scalable measures as below.

% When the most activated samples for a neuron can be summarised with the same features, for example, portrait or rural scenery, the neuron is said to interpretable for this feature. We therefore study the modality interpretability for different types of neurons.

% \begin{itemize}
% \item \textbf{Auto-Interpretebility Score}. To measure the interpretability of neuron, OpenAI prompts an advanced LLM to predict the neuron's activations for multiple input tokens and calculate the coefficient between the predicted activation and true activation~\citep{bills2023language}. Specially, the input prompt are pairs of tokens and unknown activation, i.e., $\{t_{i}: \text{unknown}\}$. Then, they identify the probability of \textit{Unknown} over token 0-10 and then normalise the probability into a 0-1 scalar. Due to the infeasbility of obtaining probability of input tokens for closed GPT4-o, we directly prompt the GPT4-o to generate sample-activation pair given a sentence-level explanation. The explaination is generated by GPT4-o by prompting with true sample-activation pairs.
% \item 

\textbf{Embedding-based Similarity.} 
% Considering the high cost of calling OpenAI Api and reproduce issue, we propose a embedding-based similarity measurement. 
% Instead of leveraging costly generative models, 
We propose a scalable measure based on embedding models that work for both image and text.\footnote{We use Vision Transformer (\href{https://huggingface.co/google/vit-base-patch16-224-in21k}{ViT-B-16-224-in21k}) for image embeddings and Sentence Transformer (\href{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}{all-MiniLM-L6-v2}) for text embeddings.} 
For each image/text feature $z$, we select the top $m$ activated image/text samples on this dimension, denoting their embeddings as $Z_+\in\sR^{m\times d}$; similarly, $K$ random samples are encoded into $Z_-\in\sR^{m\times d}$ as the baseline. Then, we calculate the inter-sample similarity between the selected samples, $S_+=Z_+Z_+^\top\in\sR^{m\times m}$ and $S_-=Z_-Z_-^\top\in\sR^{m\times m}$. Then we measure the monosemanticity degree of $z$ by calculating the relative difference between the two similarity scores: 
% \begin{equation}
$I(z)=\frac{1}{m(m-1)}\sum_{i\neq j}\frac{(S_+)_{ij}-(S_-)_{ij}}{(S_-)_{ij}}. $
% \end{equation}
% And the overall interpretability score is the average over all features $\bar I=\sum_{i=1}^nI(z_i)$.
A larger score indicates that the extracted features have more consistent semantics on average.
% $$.
% sa calculate the sample
% and obtain their features $\bm{S}_{r}\in\mathbb{R}^{K \times n}$. We calculate the dot-product among all the pair of the encoded samples in $S$ and $S_{r}$ to measure the similarity between every two samples, respectively. Finally, the average of relative difference over the two dot-product matrix (diagonal elements are excluded) are used as our interpretability indicator, $(\bm{E}-\bm{E}_{r})/\bm{E}_{r}$

\textbf{WinRate.}  
Since the representations obtained from different embedding models (e.g., vision and text) are not directly comparable, we propose similarity WinRate, a binary version of the relative similarity score, by counting the percentage that the elements in $S_+$ is larger than that in $S_-$:
% \begin{equation}
$W(z)=\frac{1}{m(m-1)}\sum_{i\neq j}\vone_{[(S_+)_{ij}>(S_-)_{ij}]}.$ 
% \end{equation}
% Similarly, the overall WinRate is $\bar W=\sum_{i=1}^nW(z_i)$. 
% A high winrate indicates better monosemanticity.
% \end{itemize}
% \vspace{-3mm}

\begin{table}[t]
    \centering
    \small
    \resizebox{0.45\textwidth}{!}{%
    \begin{tabular}{lcc|ccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{Similarity} & \multicolumn{3}{c}{WinRate} \\
    \cmidrule{2-6}
    & Image & Text & Image & Text & $|\Delta|(\text{img}-\text{txt})$ \\
    \midrule
    CLIP      & 0.113 & 0.451  & 0.652 & 0.594 & 0.058 \\
    DeCLIP    & 0.058 & -0.073 & 0.615 & 0.457 & \textbf{0.158} \\
    CLIP+NCL  & \textbf{0.161} & \textbf{0.592} & \textbf{0.727} & \textbf{0.608} & \textbf{0.119} \\
    CLIP+SAE  & \textbf{0.120} & 0.244 & \textbf{0.667} & 0.540 & \textbf{0.127} \\
    \bottomrule
    \end{tabular}
    }
    \caption{\footnotesize The average interpretability scores for features extracted from the four models. A larger $|\Delta|$ represents that the features are more aligned with a single modality.}
    \vspace{-8mm}
    \label{tab:interpret_metrics_allmodels}
\end{table}

\textbf{Results.}
% For each neuron, we collect its activated top10 images and texts, and apply the LLM-free metrics, i.e., embed-simi and win-rate to measure the intepretability of each neuron. Noted that some neurons could be only sensitive to one modality, so they could be 
% inefficient in capturing interpretable features in that modality. 
From the results of interpretability distribution of the features extracted in Table~\ref{tab:interpret_metrics_allmodels}, we observe:
(1) Features in NCL have the overall best monosemanticity (2) Compared to CLIP, all other models have more single-modality aligned features.
% All the methods highlight the distinctiveness of neuronal perception across different modalities indicated by a larger $|\Delta|$.
We evaluate the average interpretability, regardless of their predominant modality. Next, we split the neurons into different groups and study the modality-specific features.

\subsection{Grouping Modality in Multi-modal Representations}
% Built upon the monosemantic features discovered above, we can take a closer look at the distribution of \emph{modality} in each feature in a multi-modal CLIP.
% \yw{motivate the study of modality dominance.[See Above]}
\paragraph{Modality Dominance  Score (MDS).} 
We propose a metric to assert the predominant modality of each neuron. Specially, we feed $m$ input-output pairs to CLIP and obtain the image features $Z_{I}\in \mathbf{R}^{m \times n}$ and text features $Z_{T}\in \mathbf{R}^{m\times n}$. For each feature $k\in[1,\dots,n]$, we calculate the relative activation between image and text features over the $m$ inputs, i.e.,
$R(k)=\frac{1}{m}\sum_{i=1}^m\frac{(Z_I)_{ik}}{(Z_I)_{ik}+(Z_T)_{ik}}.$
The ratio $R(k)$ reflects the ratio of the $k$th feature $k$ being activated in the image modality. Based on this value, we split all $n$ features into three groups according to their dominant modality, i.e., $sigma$ of the distribution: 
 $\texttt{ImgD (Image Dominant):}  r_{i} > \mu+\sigma;\quad \texttt{TextD (Text Dominant): } r_{i} <\mu-\sigma;\quad \texttt{CrossM (Cross Modality): } \mu-\sigma <r_{i} <\mu+\sigma$.
% \begin{equation}
% \vspace{-8mm}
    % \texttt{ImgD: }  r_{i} > \mu+\sigma;\quad \texttt{TextD: } r_{i} <\mu-\sigma;\quad \texttt{CrossM: } \mu-\sigma <r_{i} <\mu+\sigma
% \vspace{-4mm}
% \end{equation}

We anticipate that \texttt{ImgD} features are mostly activated by images and \texttt{TextD} features by text, while \texttt{CrossM} features are  \emph{simulatenously} activated by both image and text when paired.
% \vspace{-4mm}
\begin{figure}[t]

    \centering
    \begin{subfigure}[b]{0.17\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 18 0 0},clip]{figures/i2t_ratio_openclip.pdf}
        \label{fig:1}
    \end{subfigure}
    \begin{subfigure}[b]{0.17\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 18 0 0},clip]{figures/i2t_ratio_declip_step0_paper.pdf}
        \label{fig:2}
    \end{subfigure}
    \begin{subfigure}[b]{0.17\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 18 0 0},clip]{figures/i2t_ratio_openclip_ncl_step2000_paper.pdf}
        \label{fig:3}
    \end{subfigure}
    \begin{subfigure}[b]{0.17\textwidth}
        \centering
        \includegraphics[width=\textwidth,trim={0 18 0 0},clip]{figures/i2t_ratio_openclip_sae_step200_paper.pdf}
        \label{fig:4}
    \end{subfigure}
    \vspace{-5mm}
    \caption{\footnotesize MDS distributions for different Language-Vision Models. Left to right: CLIP, DeCLIP, CLIP+NCL, CLIP+SAE.}
    \label{fig:four_figures}
\end{figure}
% \vspace{-4mm}
\textbf{Results of MDS in Fig~\ref{fig:four_figures}.} Interestingly, we find that    
CLIP, which is only trained on image-text paired contrastive learning objective, also contains {a spectrum of features with different modality dominance}. 
% Specifically, its mode is skewed to the text domain ($<50\%$) while it has a long tail in the image domain, suggesting that most CLIP features are more text-dominant while quite a few features are image-dominant.
DeCLIP features are less skewed and less centered, showing better coverage of both image-dominant, text-dominant, and cross-model features. Therefore, it demonstrates from a mechanistic interpretability perspective that self-supervision extract more modality-specific features that might be overlooked by pure visual-language contrastive models like CLIP. The extracted features from NCL and SAE are less skewed. SAE has a more balanced distribution, indicating its better capability of extracting diverse monosemantic features.

% The extracted features from NCL and are less skewed, suggesting that CLIP features do learn image-dominant and text-dominant features, while these features are largely hidden in the polysemantic cross-modal neurons in original CLIP. For SAE, we notice that the extracted features are more balanced, indicating that SAE is better at extracting diverse monosemantic features.

% \begin{wraptable}{r}{0.4\textwidth}
% \vspace{-3mm}
%     \centering
%     \small
%     \begin{tabular}{r|p{1cm}p{1cm}p{1cm}}
%     \toprule
%     \textbf{Models}&\textbf{\#TextD} &\textbf{\#CrossM} &\textbf{\#ImgD}\\
%     \midrule
%     CLIP  & 46&831&147  \\
%     \midrule
%     DeCLIP     & 177&660&187\\
%     \midrule
%     CLIP+NCL & 121&766&137\\
%     CLIP+SAE & 248& 547&229 \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-2mm}
%     \caption{The number of three types of neurons for different models. On top of CLIP, the other methods increase the neurons' distincts to either image or text modality.}
%     \label{tab:num_neuron_mmodality}
% \end{wraptable}

\vspace{-3mm}
\section{Understanding Multi-modal Features in Different Modality Groups}  
% \vspace{-2mm}
% \textbf{Comparing}
With the protocol developed above, we have separated the the neurons into three groups, which allows for a deeper quantitative and qualitative understanding of the connections and gaps between different modalities in data-driven approach.
% \vspace{-2mm}

\begin{table}[htbp]
\centering
\resizebox{0.5\textwidth}{!}{%
    \begin{tabular}{r|c|c|c|c}
    \toprule[1pt]
     Modality    & CLIP & DeCLIP & CLIP+NCL & CLIP+SAE \\
     \midrule
    Image & 0.118 & 0.070 & \textbf{0.197} & 0.135 \\
    Text  & -0.07 & -0.059 & 0.132 & \textbf{0.439} \\
    \bottomrule[1pt]
    \end{tabular}
}
\caption{The visual and textual monosemanticity.}
\label{tab:relative_interpret}
\end{table}

\subsection{Multi-modal Interpretability}

A major implication of modality domain is its influence on feature interpretability at different modalities. Ideally, when fed with image samples, \texttt{ImgD} neurons should be more effective at capturing concrete and consistent features than \texttt{TextD} neurons. Similar for input text samples. Therefore, we measure both visual and textual monosemanticity. Specially, for image inputs, we calculate the \textsl{visual monosemanticity} by comparing the interpretability between \texttt{ImagD} and \texttt{TextD}, i.e., EmbedSimi(\texttt{ImgD})-EmbedSimi(\texttt{TextD}); for text inputs, we calculate \textsl{textual monosemanticity} via EmbedSimi(\texttt{TextD})-EmbedSimi(\texttt{ImgD}). We have the following observations from Table\ref{tab:relative_interpret}: (1) On image input, except for CLIP, all the other three models demonstrate positive visually monosemanticity than the other two types of neurons. (2) On text input, both NCL and SAE capture better monosemantic textual features than the other two models. (3) SAE is the best in capturing both visual and textual monosemantic features. 

% \begin{wrapfigure}{r}{0.35\textwidth}
%     \centering
%     % Left subfigure
%     \begin{subfigure}{0.35\textwidth}
%         \centering
%         \includegraphics[width=\textwidth,trim={0 8 0 5},clip]{figures/relative_interpret_imginputs.pdf} % Change to your figure
%         % \caption{\footnotesize Relative Intepretability Score on \textbf{Image} Inputs.}
%         \label{fig:left}
%     \end{subfigure}

%     \begin{subfigure}{0.4\textwidth}
%         \centering
%         \includegraphics[width=\textwidth,trim={0 8 0 6},clip]{figures/relative_interpret_textinputs.pdf} % Change to your figure
%         % \caption{\footnotesize Relative Intepretability Score on \textbf{Text} Inputs.}
%         \label{fig:right}
%     \end{subfigure}
%     \vspace{-10mm}
%     \caption{\footnotesize Visual and Textual Monosemanticity over the four models.}
%     \label{fig:relative_interpret}
% \end{wrapfigure}

% Such desirable patterns are illustrated in the first row of Table~\ref{tab:img_inter_metric} and Table~\ref{tab:txt_inter_metric}: darker gray represents a higher desirable interpretability score. 
% To verify our hypothesis, we conduct interpretability evaluation on top of both image and text samples and examine the interpretability of different types of neurons by feeding image and text samples, respectively.


% From Table~\ref{tab:img_inter_metric} and Table~\ref{tab:txt_inter_metric}, we have the following the observations: (1) The three types of neurons in all the methods can capture consistent \textit{visual features} as their WinRate are all above 0.5, i.e., the most activated samples for a neuron are more similar to each other than to a set of random samples. DeCLIP, however, is inefficient in capturing consistent \textit{textual} features. (2) CLIP+NCL achieves the the overall best monosemanticity across two modality inputs. Specially, it has best SCS and WinRate for \texttt{ImgD} on image inputs; and best SCS for \texttt{TextD} on text inputs. (3) For image input, except for CLIP, the \texttt{ImgD} neurons in other three models have more visually monosemantic scores than the other two types of neurons. For text input, the \texttt{TextD} neurons in both NCL and SAE capture better monosemantic textual features than the other two types of neurons.



% \definecolor{gray}{rgb}{0.9,0.9,0.9}
% \definecolor{darkgray}{rgb}{0.8,0.8,0.8}
% \definecolor{darkergray}{rgb}{0.65,0.65,0.65}
% \begin{minipage}{0.42\textwidth}
% \centering
% \resizebox{1\textwidth}{!}{%
%     \begin{tabular}{l|c|c|c}
%     \toprule
%     Model&\cellcolor{gray}TextD & \cellcolor{darkgray}CrossM & \cellcolor{darkergray}ImgD \\
%     \midrule
%     \multicolumn{4}{c}{Embedding-based Similarity} \\
%         \midrule
%     CLIP &\cellcolor{darkergray} 0.126&\cellcolor{gray}	0.111&\cellcolor{darkgray}0.118 \\
%     DeCLIP & \cellcolor{darkgray}0.060&	\cellcolor{gray}0.054&	\cellcolor{darkergray}0.070\\
%     CLIP+NCL & \cellcolor{darkgray}0.160&	\cellcolor{gray}0.155&	\cellcolor{darkergray}{0.197}\\
%     CLIP+SAE &\cellcolor{gray}0.096&\cellcolor{darkgray}0.125&\cellcolor{darkergray}0.135\\
%     \midrule
%     \multicolumn{4}{c}{WinRate} \\
%      \midrule
%     CLIP     &\cellcolor{darkergray}0.686&	\cellcolor{gray}0.645&\cellcolor{darkgray}0.679 \\
%     DeCLIP & \cellcolor{darkgray}0.626&\cellcolor{gray}0.607&\cellcolor{darkergray}0.632\\
%     CLIP+NCL &\cellcolor{gray} 0.722&	\cellcolor{darkgray}0.723	&\cellcolor{darkergray}0.754\\
%     CLIP+SAE & \cellcolor{gray}0.623&\cellcolor{darkgray}	0.678&\cellcolor{darkergray}	0.688\\
%     \bottomrule
%     \end{tabular}
%     }
%     \captionof{table}{Interpretability measured by the activated \textbf{\textit{image}} samples.}
%     \label{tab:img_inter_metric}
% \end{minipage}
% \hfill
% \begin{minipage}{0.42\textwidth}
% \resizebox{1\textwidth}{!}{%
% \begin{tabular}{lccc}
%     \toprule
%     Model&\cellcolor{darkergray}TextD & \cellcolor{darkgray}CrossM & \cellcolor{gray}ImgD  \\
%     \midrule
%     \multicolumn{4}{c}{Embedding-based Similarity} \\
%     \midrule
%     CLIP     &\cellcolor{darkgray}  0.538&\cellcolor{gray}0.419&\cellcolor{darkergray}0.608\\
%     DeCLIP & \cellcolor{gray}-0.089 &\cellcolor{darkgray} -0.081&\cellcolor{darkergray}-0.030\\
%     CLIP+NCL &\cellcolor{darkergray}0.676&\cellcolor{darkgray}	0.588&\cellcolor{gray}0.544\\
%     CLIP+SAE & \cellcolor{darkergray}0.435 & \cellcolor{darkgray}0.260&\cellcolor{gray}-0.004\\
%     \midrule
%     \multicolumn{4}{c}{WinRate} \\
%      \midrule
%     CLIP   & \cellcolor{darkgray}0.614 & 	\cellcolor{gray}0.586&\cellcolor{darkergray}	0.631 \\
%     DeCLIP & \cellcolor{gray}0.451&\cellcolor{darkgray}0.457&\cellcolor{darkergray}0.462\\
%     CLIP+NCL & \cellcolor{darkergray}0.614&\cellcolor{darkgray}0.610&\cellcolor{gray}0.600\\
%     CLIP+SAE & \cellcolor{darkergray}0.571&\cellcolor{darkgray}0.545& \cellcolor{gray}0.491\\
%     \hline
%     \end{tabular}
%     }
%     \captionof{table}{Interpretability measured by the activated \textbf{\textit{text}} samples.}
%     \label{tab:txt_inter_metric}
% \end{minipage}



% \textbf{Image-only features.} 

% \textbf{Text-only features.} 

% \textbf{Cross-modal features.} 





