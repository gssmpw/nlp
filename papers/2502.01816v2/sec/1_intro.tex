\section{Introduction}
\label{sec:intro}

Video Super-Resolution (VSR) aims to reconstruct high-resolution videos from their low-resolution counterparts. This task is crucial in a wide range of applications, including video streaming, gaming, mobile content delivery, and surveillance. By leveraging both spatial and temporal information, VSR enhances the visual quality of videos while maintaining temporal consistency. However, achieving high-quality super-resolution under computational constraints remains a persistent challenge, particularly in scenarios where resource efficiency is paramount.



%{model_performance_comparison_gflops_legend.png}
   
\begin{figure}[h]
    \centering
    \includegraphics[clip, trim=2cm 17.5cm 6cm 1.5cm, width=1\linewidth]{ModelComparison.pdf}
    \caption{Comparison of video super-resolution (VSR) models: Proposed models (blue filled circles) reconstruct frames with competitive quality (SSIM on REDS4) when compared to previous models (grey hollow circles) with fewer trainable parameters and inference floating point operations per frames (GFLOPs in parenthesis for model labels and also coded as bubble size).}
    \label{fig:model_comparison}
\end{figure}

In recent years, deep learning-based approaches have shown remarkable progress in addressing the challenges of VSR. Convolutional architectures, such as video restoration framework with enhanced deformable networks (EDVR)~\cite{wang2019edvr} and BasicVSR++~\cite{chan2022basicvsr++}, have achieved notable success by employing deformable convolutions and residual networks to utilize motion information and recover fine details, respectively. Transformer-based models, including video restoration transformer (VRT) and recurrent video restoration transformer (RVRT), have further pushed the boundaries of VSR by capturing global temporal dependencies and improving long-term consistency\cite{liang2022vrt}\cite{liang2022rvrt}. However, their substantial parameter count and computational requirements limit their utility for deployment in setups with strict power, compute, and cost constraints.

VSR models have explored motion-aware architectures such as VESPCN\cite{vespcn} and EDVR\cite{wang2019edvr}, as well as methods tailored for compressed video super-resolution\cite{Guan2019MFQE2A}. These approaches aim to balance performance and efficiency, yet they often struggle with temporal coherence, handling large motion, and adapting to diverse real-world degradationsâ€”particularly in highly compressed or low-quality videos with unknown distortions.Furthermore, most existing methods focus solely on spatial and temporal domains, neglecting the potential benefits of incorporating frequency-domain representations for multi-scale feature extraction. This gap highlights the need for an innovative design that integrates these domains while maintaining computational efficiency.

In this work, we propose a novel lightweight VSR framework that combines convolutional efficiency with advanced spatiotemporal modeling to achieve state-of-the-art results with limited resources. Our architecture builds upon the strengths of convolutional networks, which excel at localized feature extraction, and enhances them with components tailored for challenges specific to VSR. To improve spatial and structural representation, we introduce a dual-pipeline design that incorporates 2D discrete wavelet transform (DWT) for multi-resolution and spatially sparse edge feature extraction and 3D convolutional residual blocks for spatio-temporal feature extraction. Additionally, we integrate deformable convolutions to ensure effective motion alignment across frames, which is crucial for maintaining temporal consistency. Furthermore, we leverage a memory tensor to enhance utilization and reconstruction of temporal coherence. Additionally, ConvNeXt blocks are employed to ensure efficient spatial feature extraction while maintaining a low parameter count and GFLOPs. 

This work addresses the critical challenge of balancing computational efficiency and restoration quality in VSR. By combining convolutional networks, wavelet-based feature extraction, and temporal memory mechanisms, our framework advances the state-of-the-art in lightweight VSR and paves the way for practical applications in resource-constrained scenarios.


Following are our key contributions:

\begin{itemize}
\item State-of-the-art reconstruction quality on REDS4\cite{Nah_2019_CVPR_Workshops_SR}: Our method achieves an SSIM of 0.9175, surpassing much heavier models, such as BasicVSR++\cite{chan2022basicvsr++} (0.9069) and RVRT (0.9113)\cite{liang2022rvrt}.

\item Low computational overhead compared to previous models: Transformer-based models such as VRT (30.7M parameters)\cite{liang2022vrt} and RVRT (11.06M parameters)\cite{liang2022rvrt} exhibit high computational costs, while our model delivers comparable or better performance at significantly lower resource requirements. The floating point operations required at test time are also from seven to 35 times lower than comparable models (see Figure~\ref{fig:model_comparison}).
    
\item Memory tensor enhancements: Our introduction of memory tensors plays a key role in preserving temporal consistency and significantly improving SSIM scores across multiple datasets. Compared to previous memory-based approaches, such as those using convolutional LSTM, our approach is significantly more compute efficient. %Our proposed memory tensor module efficiently retains past frame information while avoiding the memory overhead of recurrent-based methods. 
This approach enhances temporal stability without excessive model complexity.

\item Residual deformable convolutions: Our work builds on the strengths of deformable convolutions for motion alignment but incorporates residual deformable convolutions, which improve feature reuse and reduce the need for excessive parameterization.

\item Wavelet decomposition in 2D: We utilize the multi-resolution and energy compaction (sparsification) properties of 2D wavelets when applied to natural images to increase the spatial information utilization in our architecture.

\item Orchestration of multiple architectural features: While ideas such as 3D convolutions and residual blocks have been used in VSR before, our orchestration of multiple parallel branches that specialize in exploiting various aspects of spatio-temporal information available in videos is unique, which reflects in its results.

\end{itemize}