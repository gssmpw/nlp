\section{Introduction}
Uncertainty quantification for time series is crucial across various domains including finance, climate science, epidemiology, energy, supply chains, and macroeconomics, etc, especially in high-stakes areas. To achieve this goal, an ideal model is supposed to consistently produce prediction sets that are well-calibrated, meaning that over time, the proportion of sets containing true labels should align closely with the intended confidence level. Classic methods for uncertainty quantification often rely on strict parametric assumptions of time-series models like autoregressive and moving average (ARMA) models \citep{brockwell1991time}. Other methods like Bayesian recurrent neural networks \citep{fortunato2017bayesian} and deep Gaussian processes \citep{li2020stochastic} are difficult to calibrate by themselves. And quantile regression models \citep{gasthaus2019probabilistic}  may ``overfit'' when estimating uncertainty. Additionally, complex machine learning models such as transformer \citep{NIPS2017_3f5ee243,li2019enhancing,gao2024inducing} have been designed to output accurate predictions but cannot provide valid prediction sets.
Hence, a systematic tool is required to perform uncertainty quantification for complex black-box models in time series data.

Conformal inference \citep{vovk2005algorithmic} is an increasingly popular framework for uncertainty quantification with arbitrary underlying point predictors (whether statistical, machine or deep learning). At its core, conformal prediction sets are guaranteed to contain the true label with a specified probability, under solely the assumption that the data is exchangeable. This is achieved without making parametric assumptions about the underlying data distribution, thereby enhancing its applicability across a wide range of models and datasets. However, in time series data, exchangeability does not hold due to strong correlations and potential distribution shifts. 

Recently, a growing body of research has focused on developing online conformal methods for scenarios where data arrives sequentially. One of the most important branches is the Adaptive Conformal Inference (ACI) proposed by \cite{aci_gibbs2021adaptive}, and its following works \citep{zaffran2022adaptive,bhatnagar2023improved,gibbs2024conformal}. At each time step, these methods generate a prediction set characterized by a single threshold or confidence parameter that regulates the size of the set, for example $\hat{C}_t = \{y\in \gY: S_t(X_t, y) \leq q_t\}$, where $S_t(\cdot,\cdot)$ is the non-conformity score function. After $Y_t$ is observed, they update the threshold $q_t$ through indicator $\mathds{1}\{Y_t \notin \hat{C}_t\}$, which is identical to $\mathds{1}\{S_t(X_t,Y_t) > q_t\}$. However, simple binary feedback cannot precisely capture the magnitude of error\footnote{To distinguish from the miscoverage error $\mathds{1}\{S_t(X_t,Y_t) > q_t\}-\alpha$, we refer ``error'' to the term $S_t(X_t,Y_t) - q_t$ in the rest of our paper.} $S_t(X_t,Y_t) - q_t$, quantifying the extent of under/over coverage of $\hat{C}_t$. For example, in the miscoverage case, an empty prediction set and a prediction set that almost covers the true label yield the same feedback value in ACI and its variants. Hence it will take a longer time to correct past mistakes, see the blue curve in \Cref{figure google prophet ogd vs eci}.


\vspace{-1em}
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.65\textwidth]{figures/google_prophet_OGD_vs_ECI.pdf}
  \vspace{-0.5em}
  \caption{Comparison results between OGD (online (sub)gradient descent) and ECI on Google stock dataset with Prophet model. OGD uses the same feedback as ACI but updates the confidence level. The coverage is averaged over a rolling window of 50 points.
  \vspace{-1em}
}
  \label{figure google prophet ogd vs eci}
\end{figure*}

In this paper, we propose \textit{Error-quantified Conformal Inference }(ECI) based on an adaptive \textit{error quantification} (EQ) term that provides additional smooth feedback. ECI not only uses the miscoverage indicator in the feedback, but also introduces a continuous EQ function to assess the magnitude of revealed error $S_t(X_t,Y_t) - q_t$. Benefiting from the EQ term, our online conformal procedure will react quickly to distribution shifts in time series, see \Cref{figure google prophet ogd vs eci}. This leads to tighter prediction sets without sacrificing the miscoverage rate. 
We summarize our main contributions as follows:
\begin{itemize}
    \item We propose ECI for uncertainty quantification in time series. It is a novel method based on adaptive updates with additional smooth feedback, quantifying the extent of under/over coverage.
    We further propose two variants of ECI. The first introduces a cutoff threshold for the EQ term to avoid over-compensation caused by small errors. Another variant integrates the error of previous steps to make coverage more stable.
    
    \item There are mainly two theoretical results. Firstly, we obtain a coverage guarantee for ECI with a fixed learning rate and not restricted to long-term. We prove it by showing that every miscoverage step will be followed by several coverage steps given a proper learning rate. Secondly, for arbitrary learning rates, we give a finite-sample upper bound for the averaged miscoverage error. Both theoretical results do not need any assumption on the data-generating distribution.
    
    \item Extensive experimental results demonstrate that ECI and its variants provide superior performance in time series, including data in finance, energy, and climate domains. We show that ECI maintains coverage at the target level and obtains tighter prediction sets than other state-of-the-art methods.
\end{itemize}
