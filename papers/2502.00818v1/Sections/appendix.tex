\appendix
\section{Smoothed ways}
\subsection{Explanations for extra bias}
\label{explanations for extra bias}
Our target is the quantile loss $\ell_t(q)=(\text{err}_t-\alpha)(s_t-q)$.
For simplicity, we denote $h(q)=\text{err}_t-\alpha, g(q)=s_t-q$. Note that $h(q)$ is non-differentiable, so we use smoothed function $f(q)$ to approximate $h(q)$. The fully smoothed method in \Cref{eq:full_smoothed_rule} considers:
\begin{align*}
    \nabla h(q)g(q)&\approx\nabla f(q)g(q)\\
    &=f(q)\nabla g(q)+g(q)\nabla f(q).
\end{align*}
The two terms $f(q)\nabla g(q)$ and $g(q)\nabla f(q)$ both introduce error. Strictly we have
\begin{align*}
\nabla h(q)g(q)&=\lim_{\delta\to 0} \frac{h(q+\delta)g(q+\delta)-h(q+\delta)g(q)+h(q+\delta)g(q)-h(q)g(q)}{\delta}\\
&=\lim_{\delta\to 0} \frac{h(q+\delta)g(q+\delta)-h(q+\delta)g(q)}{\delta}+\lim_{\delta\to 0} \frac{h(q+\delta)g(q)-h(q)g(q)}{\delta}\\
&=h(q)\nabla g(q)+\underbrace{\lim_{\delta\to 0}\frac{h(q+\delta)-h(q)}{\delta}}_{\text{ using $\nabla f$ to approximate  }} g(q)\\
&\approx -(\text{err}_t-\alpha)-\nabla f(s_t-q)(s_t-q).
\end{align*}
Hence, if we only approximate the second term, potential bias may be reduced. The fully smoothed method is better only when the error of the two terms are negatively correlated and cancel out. 

\subsection{Fully smoothed ways}
\label{full-smoothed way}
There is another smoothing technique applied in the quantile regression \citep{fernandes2021smoothing,tan2022high}, which directly smooths the inidcator in subgradient. Based on this, we can have the following udpate rule
\begin{align}\label{eq:conv_smoothed_rule}
    q_{t+1} = q_t + \eta \cdot \big[f(s_t-q_t) - \alpha \big].
\end{align}

To further validate our ideas in \Cref{ECI}, we test the experimental performance of smoothed method and fully smoothed method, as shown in \Cref{amzn_coverage_full_prophet,amzn_coverage_full_ar,amzn_coverage_full_theta}. All experimental setting are aligned with \Cref{experiment}. 

The update rule of fully smoothed method in \Cref{eq:full_smoothed_rule} is
$$
q_{t+1} = q_t + \eta \cdot \bigg[f(s_t-q_t) - \alpha+  (s_t-q_t) \nabla f(s_t-q_t)\bigg].
$$

And the update rule of ECI in \Cref{ECI update} is
$$
q_{t+1} = q_t + \eta \cdot \bigg[\text{err}_t - \alpha+  (s_t-q_t) \nabla f(s_t-q_t)\bigg].
$$


Compared with ECI in \Cref{ECI update}, fully smoothed updates rules in \Cref{eq:full_smoothed_rule} and \Cref{eq:conv_smoothed_rule} do not keep the actual value of indicator function $\text{err}_t$, and brings the bias between smooth function $f(s_t-q_t)$ and $\mathds{1}(s_t>q_t)$. This leads to these method being conservative.

Experimental results also demonstrate it. Since $f(x)$ does not approach $0$ quickly in the early part of the period, and may even be larger than $\alpha$, the coverage rate can not effectively approach $1-\alpha$. It can be seen that smoothed method and fully smoothed method tends to have overly high coverage in the early stages in the \Cref{amzn_coverage_full_prophet,amzn_coverage_full_ar,amzn_coverage_full_theta}. Consequently, it results in wider sets.

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/amzn_coverage_full_prophet.pdf}
  \caption{Coverage result on Amazon stock dataset with Prophet model.}
  \label{amzn_coverage_full_prophet}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/amzn_coverage_full_ar.pdf}
  \caption{Coverage result on Amazon stock dataset with AR model.}
  \label{amzn_coverage_full_ar}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/amzn_coverage_full_theta.pdf}
  \caption{Coverage result on Amazon stock dataset with Theta model.}
  \label{amzn_coverage_full_theta}
\end{figure*}


\newpage
\section{Proofs of main results}
\label{proof}
We show below that under \Cref{assumption1,assumption2}, there exists a bound for $q_t$ and EQ term, depending on learning rate $\eta_t$ and the upper bound of $s_t$. This result is essential in proving our following results of coverage guarantees.
\subsection{Propositions}
\begin{proposition}
\label{proposition1}
      Fix an initial threshold $q_1 \in [0, B]$. Then under \Cref{assumption1,assumption2},  ECI in \eqref{ECI update} with arbitrary  nonnegative learning rate $\eta_t$ satisfies that
    \begin{align}
    \label{bound}
        -(\alpha+\lambda)M_{t-1} \leq q_t \leq B+(1-\alpha+\lambda)M_{t-1} \quad \forall t\geq 1,
    \end{align}
    where $M_0  = 0,$ and $M_t = \max_{1\leq r \leq t}\eta_r$ \ for $t\geq 1$.
\end{proposition}
\begin{proof}
We prove this by induction. First, $q_1\in[0,B]$ by assumption. Next fix any $t \geq 1$ and assume $q_t$  lies in the range specified in \eqref{bound}, and consider $q_{t+1}$.

(1): $s_t \geq q_t,$
\begin{align*}
    q_{t+1}&=q_t+\eta_t\left(1-\alpha\right)+\eta_t (s_t-q_t)\ \nabla f(s_t-q_t)\\ &\leq q_t+\eta_t\left(1-\alpha\right)+\eta_t \lambda\\
    & \leq s_t+(1-\alpha+\lambda)\eta_t\\
    & \leq B+(1-\alpha+\lambda)M_{t}
\end{align*}
and $q_{t+1} \geq q_t \geq -M_{t-1}\alpha  \geq -M_t\alpha $.
\\(2): $s_t \leq q_t,$
\begin{align*}
    q_{t+1}&=q_t+\eta_t\left(-\alpha\right)+\eta_t (s_t-q_t)\ \nabla f(s_t-q_t)\\ & \geq
    q_t+\eta_t\left(-\alpha\right)-\eta_t \lambda\\
    & \geq  -(\alpha+\lambda)M_{t},
\end{align*}
and $q_{t+1} \leq q_t \leq B+(1-\alpha+\lambda)M_{t-1} \leq B+(1-\alpha+\lambda)M_{t}.$
\end{proof}
\begin{proposition}
      Under \Cref{assumption1,assumption2}, $|(s_t-q_t)\nabla f(s_t-q_t)|\leq c \left[B+(1-\alpha+\lambda)M_{t-1}\right]$ \  for any $t \geq 1$. 
\end{proposition}
\begin{proof}
Based on $s_t \in [0,B]$ and \Cref{proposition1},
\begin{align*}
    |s_t-q_t|&= \max\{q_t-s_t,s_t-q_t\} \leq \max\{q_t,s_t-q_t\}\\ &\leq \max\{B+(1-\alpha+\lambda)M_{t-1},B+(\alpha+\lambda)M_{t-1}\}
    \\ &\leq B+(1-\alpha+\lambda)M_{t-1}.
\end{align*}
Hence $|(s_t-q_t)\nabla f(s_t-q_t)|\leq c \left[B+(1-\alpha+\lambda)M_{t-1}\right]$
\end{proof}


\subsection{Proof of Theorem \ref{theorem1}}
\textbf{Theorem 1.}
    Assume that $\eta>2NB$, $c<\frac{\min\{\eta,N^2\}}{2N^2\left[B+(1-\alpha+\lambda)\eta\right]}$, where $N=\lceil \frac{1}{\alpha} \rceil$. Under \Cref{assumption1,assumption2}, the prediction set generated by \cref{ECI update} satisfies:
    \begin{equation}
    \lim_{T \to \infty} \frac{1}{T}\sum_{t=1}^T \mathds{1}\{Y_t \notin \hat{C}_t\}\leq \alpha.
\end{equation}


\begin{proof}
    We first prove that for any $t$, $s_t>q_t$ implies $s_{t+i}<q_{t+i},$  $i=1,2,\cdots, N-1$. Note that $\{Y_t \in \hat{C}_t\}=\{|Y_t-\hat{Y}_t|\leq q_t\}$ is meaningless when $q_t<0$. Hence we set $q_t$ to be $\max\{q_t,0\}$ after each update ( which does not affect the validity of our proof ) and assume $q_t\geq 0$. For simplicity,  denote $g(x)=x \nabla f(x),$ then $q_{t+1}=q_t+\eta 
    \left[\text{err}_t-\alpha+g(s_t-q_t)\right]$.
   
    We prove  by induction. For $k=1$,
    \begin{align*}
        q_{t+1}-s_{t+1}&=q_t+\eta \left[1-\alpha+g(s_t-q_t)\right]>\eta (1-\alpha-c \left[B+(1-\alpha+\lambda)\eta\right]) 
    \end{align*}
    Observe that $c \cdot [B+(1-\alpha+\lambda)\eta]< [B+(1-\alpha+\lambda)\eta]\cdot\frac{N^2}{2N^2\left[B+(1-\alpha+\lambda)\eta\right]}=\frac{1}{2},$ hence $$q_{t+1}-s_{t+1}>\eta(1-\alpha-\frac{1}{2})>\eta(\frac{1}{2}-\alpha)\geq 0.$$
    For $2 \leq k \leq N-1$, by recursion:
     \begin{align*}
         q_{t+k}-s_{t+k}&=q_t+\eta(1-k\alpha)+\eta \sum_{i=0}^{k-1} g(s_{t+i}-q_{t+i})-s_{t+k}\\
         & >\eta(1-k\alpha)+\eta \sum_{i=1}^{k-1} g(s_{t+i}-q_{t+i})-s_{t+k} \qquad (  s_t>q_t\geq 0)
         \\ 
         & \geq \eta(1-k\alpha)+c\eta \sum_{i=1}^{k-1} (s_{t+i}-q_{t+i})-s_{t+k}
         \\
         & \geq \eta(1-k\alpha)-c(k-1)\left[B+(1-\alpha+\lambda)\eta\right]-s_{t+k} \\
         & \geq \frac{\eta}{N}-c(N-2)\left[B+(1-\alpha+\lambda)\eta\right]-s_{t+k} \qquad (k\leq N-1, \ \alpha\geq \frac{1}{N} ) \\
         &>\frac{\eta}{N}-\frac{(N-2)\eta}{2N^2}-\frac{\eta}{2N}>0
    \end{align*}
    The last inequality is based on the assumption  $s_{t+k}\leq B <\frac{\eta}{2N}, c<\frac{\eta}{2N^2\left[B+(1-\alpha+\lambda)\eta\right]}$. \\
   
    In conclusion, we have proved that for every miscoverage step $t$, i.e. $Y_t \notin \hat{C}_t$, the next $N-1$ steps of \eqref{ECI update} will satisfy  $Y_{t+i} \in \hat{C}_{t+i}, i=1,2,\cdots N-1$. Therefore, for any $T$,  
     $$ \frac{1}{N}\sum_{t=T+1}^{T+N} \mathds{1}\{Y_t \notin \hat{C}_t\}\leq \frac{1}{N}.$$
\end{proof}

\subsection{Proof of Theorem \ref{theorem2}}
\textbf{Theorem 2.}
  Let $\{\eta_t\}_{t \geq 1}$ be an arbitrary positive sequence. Under \Cref{assumption1,assumption2},  the prediction set generated by \Cref{ECI update} with adaptive learning rate $\eta_t$ satisfies:

\begin{equation}
    \bigg|\frac{1}{T} \sum_{t=1}^T (\text{err}_t-\alpha) \bigg|  \leq  \frac{(B+M_{T-1})\|\Delta_{1:T}\|_1}{T} +c \left[B+(1-\alpha+\lambda)M_{T-1}\right]
\end{equation}

where $\|\Delta_{1:T}\|_1=|\eta_1^{-1}|+\sum_{t=2}^T|\eta_t^{-1}-\eta_{t-1}^{-1}|, M_T = \max_{1\leq r \leq T}\eta_r$.

\begin{proof}
    Denote $\Delta_1=\eta_1^{-1},\text{ and }\Delta_t=\eta_t^{-1}-\eta_{t-1}^{-1}\text{ for all }t\geq1,$
\begin{align*}
\bigg|\frac{1}{T} \sum_{t=1}^T (\text{err}_t-\alpha) \bigg|
& =\left|\frac{1}{T} \sum_{t=1}^T\left(\sum_{r=1}^t \Delta_r\right) \cdot \eta_t\left(\text{err}_t-\alpha\right)\right| \\
& =\left|\frac{1}{T} \sum_{r=1}^T \Delta_r\left(\sum_{t=r}^T \eta_t\left(\text{err}_t-\alpha\right)\right)\right| \\
& =\left|\frac{1}{T} \sum_{r=1}^T \Delta_r\left(q_{T+1}-q_r+\sum_{t=r}^T \eta_t (s_t-q_t) \nabla f(s_t-q_t)\right)\right| \text { by \Cref{ECI update}} \\
& \leq \frac{1}{T}\left|\sum_{r=1}^T \Delta_r (q_{T+1}-q_r)\right|+\frac{1}{T} \left|\sum_{r=1}^T \Delta_r \sum_{t=r}^T \eta_t (s_t-q_t) \nabla f(s_t-q_t) \right| \\
& \leq \frac{1}{T}\left|\sum_{r=1}^T \Delta_r (q_{T+1}-q_r)\right|+\frac{1}{T} \left|\sum_{t=1}^T (s_t-q_t)\nabla f(s_t-q_t)\right|\\
& \leq \frac{1}{T} \|\Delta_{1:T}\|_1   \max _{1 \leq r \leq T}\left|q_{T+1}-q_r\right|+\frac{1}{T} \left(\sum_{t=1}^T |s_t-q_t|\nabla f(s_t-q_t)\right)\\
& \leq \frac{(B+M_{T-1})\|\Delta_{1:T}\|_1}{T} +c \cdot \frac{\sum_{t=1}^T \left[B+(1-\alpha+\lambda)M_{t-1}\right]}{T} \\
& \leq \frac{(B+M_{T-1})\|\Delta_{1:T}\|_1}{T} +c \left[B+(1-\alpha+\lambda)M_{T-1}\right].
\end{align*}
\end{proof}






\section{More details on existing methods}
\label{More details on existing methods}
\subsection{ACI}

Adaptive Conformal Inference (ACI) in \Cref{algorithm1} models the sequentially conformal inference with distribution shift  as a learning problem of a single parameter whose optimal value is varying over time. Assume we have a calibration set $\mathcal{D}_{\mathrm{cal}}\subseteq\{(X_r,Y_r)\}_{1\leq r\leq t-1}$,  
and $\hat{Q}_t(\cdot)$ is  the fitted quantiles of the non-conformity scores:
$$\hat{Q}(p):=\inf\left\{s:\left(\frac1{|\mathcal{D}_{\mathrm{cal}}|}\sum_{(X_r,Y_r)\in\mathcal{D}_{\mathrm{cal}}}\mathds{1}_{\{S(X_r,Y_r)\leq s\}}\right)\geq p\right\}.$$
For prediction set $\hat{C}_t(\alpha):=\{y:S_t(X_t,y)\leq\hat{Q}_t(1-\alpha)\}$, define:$$ \beta_t:=\sup\{\beta:Y_t\in\hat{C}_t(\beta)\}.$$Consider pinball loss  $\ell(\alpha_t,\beta_t)=\rho_\alpha(\beta_t-\alpha_t)$, by gradient descent:
$$\alpha_{t+1}=\alpha_t-\eta\partial_{\alpha_t}\ell(\alpha_t,\beta_t)=\alpha_t+\eta(\alpha-\mathds{1}_{\alpha_t>\beta_t})=\alpha_t+\eta(\alpha-\text{err}_t).$$
 ACI transforms unbounded score sequences into bounded ones, which then
implies long-run coverage for any score sequence. This may, however, come at a cost: ACI can sometimes output infinite or null prediction sets ($\alpha_t<0$ or $\alpha_t>1$). 

\begin{algorithm}
\caption{Adaptive Conformal Inference (ACI)}\label{algorithm1}
\begin{algorithmic}[1]
\Require $\alpha \in (0, 1)$,  $\eta > 0$, $D_{cal}$, init. $\alpha_1 \in \mathbb{R}$
\For{$t \geq 1$}
    \State Observe input $X_t \in \mathcal{X}$
    \State Compute $\hat{Q}_t(1-\alpha_t)$
    \State Return prediction set $\hat{C}_t(\alpha_t):=\{y:S_t(X_t,y)\leq\hat{Q}_t(1-\alpha_t)\}$
    \State Observe true label $Y_t \in \mathcal{Y}$ and compute true radius $\beta_t:=\sup\{\beta:Y_t\in\hat{C}_t(\beta)\}$
    \State Update predicted radius
    \[
    \alpha_{t+1}=\alpha_t+\eta(\alpha-\mathds{1}_{\alpha_t>\beta_t})
    \]
\EndFor
\end{algorithmic}
\end{algorithm}
\subsection{OGD}
Online Gradient Descent (OGD) in \Cref{algorithm2} is an iterative optimization algorithm that updates model parameters incrementally using each new data point, making it suitable for real-time and streaming data applications.
\begin{algorithm}[H]
\caption{Online Gradient Descent (OGD)}\label{algorithm2}
\begin{algorithmic}[1]
\Require $\alpha \in (0, 1)$, base predictor $\hat{f}$, learning rate $\eta > 0$, init. $q_1 \in \mathbb{R}$
\For{$t \geq 1$}
    \State Observe input $X_t \in \mathcal{X}$ 
    \State Return prediction set $\hat{C}_t(X_t, q_t)=[\hat{f}_{t}(X_{t})-q_t,\hat{f}_{t}(X_{t})+q_t]$
    %\State Return prediction set $\hat{C}_t(X_t, q_t)=[\hat{f}_{t}(X_{t})-q_t,\hat{f}_{t}(X_{t})+q_t]$
    \State Observe true label $Y_t \in \mathcal{Y}$ and compute true radius $s_t = \inf\{s \in \mathbb{R} : Y_t \in \hat{C}_t(X_t, s)\}$
    \State Compute quantile loss $\ell^{(t)}(q_t) = \rho_{1-\alpha}(s_t-q_t)$
    \State Update predicted radius
    \[
    q_{t+1} = q_t - \eta \nabla \ell^{(t)}(q_t)
    \]
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{SF-OGD}
 Scale-Free OGD (SF-OGD) that we summarize in  \Cref{algorithm3} is a variant of OGD that decays its effective learning rate based on cumulative  past gradient norms. Suppose $\hat{f}$ is a base predictor and choose $\widehat{C}_{t}(X_{t},q):=[\hat{f}_{t}(X_{t})-q,\hat{f}_{t}(X_{t})+q]$ to be a prediction set around $\hat{f}_{t}(X_{t})$.

\begin{algorithm}[H]
\caption{Scale-Free Online Gradient Descent (SF-OGD)}\label{algorithm3}
\begin{algorithmic}[1]
\Require $\alpha \in (0, 1)$, base predictor $\hat{f}$, learning rate $\eta > 0$, init. $q_1 \in \mathbb{R}$
\For{$t \geq 1$}
    \State Observe input $X_{t+1} \in \mathcal{X}$
    \State Return prediction set $\hat{C}_t(X_{t}, q_{t})=[\hat{f}_{t}(X_{t})-q_{t+1},\hat{f}_{t}(X_{t})+q_{t}]$ 
    \State Observe true label $Y_t \in \mathcal{Y}$
    \State Compute true radius $s_t = \inf\{s \in \mathbb{R} : Y_t \in \hat{C}_t(X_t, s)\}$
    \State Compute quantile loss $\ell^{(t)}(q_t) = \rho_{1-\alpha}(s_t-q_t)$
    \State Update predicted radius
    \[
    q_{t+1} = q_t - \eta \frac{\nabla \ell^{(t)}(q_t)}{\sqrt{\sum_{i=1}^{t} \|\nabla \ell^{(i)}(q_i)\|_2^2}}
    \]
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{decay-OGD}
 Online conformal prediction with decaying step sizes (decay-OGD) in \Cref{algorithm_decay} is a variant of OGD that decays its effective learning rate based on time steps. Suppose $\hat{f}$ is a base predictor and choose $\widehat{C}_{t}(X_{t},q):=[\hat{f}_{t}(X_{t})-q,\hat{f}_{t}(X_{t})+q]$ to be a prediction set around $\hat{f}_{t}(X_{t})$.

\begin{algorithm}[H]
\caption{Online conformal prediction with decaying step sizes (decay-OGD)}\label{algorithm_decay}
\begin{algorithmic}[1]
\Require $\alpha \in (0, 1)$, base predictor $\hat{f}$, learning rate $\eta > 0$, init. $q_1 \in \mathbb{R}$
\For{$t \geq 1$}
    \State Observe input $X_{t+1} \in \mathcal{X}$
    \State Return prediction set $\hat{C}_t(X_{t}, q_{t})=[\hat{f}_{t}(X_{t})-q_{t+1},\hat{f}_{t}(X_{t})+q_{t}]$ 
    \State Observe true label $Y_t \in \mathcal{Y}$
    \State Compute true radius $s_t = \inf\{s \in \mathbb{R} : Y_t \in \hat{C}_t(X_t, s)\}$
    \State Compute quantile loss $\ell^{(t)}(q_t) = \rho_{1-\alpha}(s_t-q_t)$
    \State Compute a decaying step size $\eta_t = \eta \cdot t^{-\frac{1}{2}-\epsilon}$
    \State Update predicted radius
    \[
    q_{t+1} = q_t - \eta_t \nabla \ell^{(t)}(q_t)
    \]
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Conformal PID}
Conformal PID in \Cref{algorithm4} is bulit upon ideas from conformal prediction and control theory.
It is able to prospectively model conformal scores in an online setting, and adapt to the presence of systematic errors due to seasonality, trends, and general distribution shifts. Conformal PID consists of three parts: quantile tracking, error integration and scorecasting.
For prediction set: $$\mathcal{C}_t=\{y\in\mathcal{Y}:S_t(x_t,y)\leq q_t\},$$ consider the optimization:
$$
    \underset{q\in\mathbb{R}}{\operatorname*{minimize}}\sum_{t=1}^T\rho_{1-\alpha}(s_t-q).
$$
Conformal PID solves it via online gradient method:
$$
\begin{aligned}
q_{t+1} &=q_t+\eta\partial\rho_{1-\alpha}(s_t-q_t) \\&
=q_t+\eta(\mathds{1}(s_t>q_t)-\alpha)=q_t+\eta(\mathrm{err}_t-\alpha), 
\end{aligned}
$$
which is called quantile tracking. In parctice,  the learning rate  is not fixed. They choose it in an adaptive way: $\eta_t=\eta \cdot (\max\{s_{t-w+1},\cdots,s_t\}-\min\{s_{t-w+1},\cdots,s_t\})$, where $\eta$ is a scale parameter. 
The error integration incorporates the past error to  further stabilize the coverage   :$$q_{t+1}=r_t\bigg(\sum_{i=1}^t(\text{err}_i-\alpha)\bigg).$$
The last step is to add up a scorecasting term: $g^{'}_t$, a model that can take
advantage of any leftover signal that is not captured like seasonality, trends, and exogenous covariates. Scorecastor needs be trained and can be Theta or other models.

Putting the three steps together is the conformal PID method:
$$q_{t+1}=g_t'+\eta_t(\text{err}_t-\alpha)+r_t\bigg(\sum_{i=1}^t(\text{err}_i-\alpha)\bigg)$$
\begin{algorithm}
\caption{Conformal PID}\label{algorithm4}
\begin{algorithmic}[1]
\Require $\alpha \in (0, 1)$,  base predictor $\hat{f}$, trained scorecastor $g'$, $\eta > 0$, window length $w$, init. $q_1 \in \mathbb{R}$
\For{$t \geq 1$}
    \State Observe input $X_t \in \mathcal{X}$
    \State Return prediction set $\mathcal{C}_t=\{y\in\mathcal{Y}:S_t(x_t,y)\leq q_t\}$
    \State Observe true label $Y_t \in \mathcal{Y}$ and compute score $s_t:=S_t(X_t,Y_t)$,
    \State Compute learning rate $\eta_t=\eta \cdot (\max\{s_{t-w+1},\cdots,s_t\}-\max\{s_{t-w+1},\cdots,s_t\})$
    \State Compute the integrator  
    \[
    r_t= r_t\bigg(\sum_{i=1}^t(\text{err}_i-\alpha)\bigg)
    \]
    \State Compute the scorecastor $g_t'(X_t)$
    \State Update :
    $$q_{t+1}=g_t'(X_t)+\eta_t(\text{err}_t-\alpha)+r_t\bigg(\sum_{i=1}^t(\text{err}_i-\alpha)\bigg)$$
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{SPCI}
The sequential predictive conformal inference (SPCI) outlined in \Cref{algorithm5} cast the conformal prediction set as predicting the quantile of a future residual and adaptively re-estimate the conditional quantile of non-conformity scores. 
Suppose $\hat{f}$ is a pre-trained model, $\widehat{Q}_t(p)$ is an estimator of $Q_t(p)$,  the $p-$th quantile of the residual $\hat{\epsilon}_t=|Y_t-\hat{f}(X_t)|$. SPCI sets is $\hat{C}_{t-1}(X_t)$ is defined as:
$$[\hat{f}(X_t)+\widehat{Q}_t(\hat{\beta}),\hat{f}(X_t)+\widehat{Q}_t(1-\alpha+\hat{\beta})],$$where $\hat{\beta}$ minimizes set width:$$\hat{\beta}=\arg\min_{\beta\in[0,\alpha]}(\widehat{Q}_t(1-\alpha+\beta)-\widehat{Q}_t(\beta)).$$ 
\begin{algorithm}[H]
\caption{Sequential Predictive Conformal Inference (SPCI)}\label{algorithm5}
\begin{algorithmic}[1]
\Statex \hspace{-\algorithmicindent} \textbf{Require:} Training data $\{(X_t, Y_t)\}_{t=1}^T$, prediction algorithm $\mathcal{A}$, significance level $\alpha$, quantile regression algorithm $\mathcal{Q}$.
\Statex \hspace{-\algorithmicindent} \textbf{Output:}  Prediction sets $\hat{C}_{t-1}(X_t)=[\hat{f}(X_t)+\widehat{Q}_t(\hat{\beta}),\hat{f}(X_t)+\widehat{Q}_t(1-\alpha+\hat{\beta})], t > T$
\State Obtain $\hat{f}$ and prediction residuals $\hat{\varepsilon}$ with $\mathcal{A}$ and $\{(X_t, Y_t)\}_{t=1}^T$
\For{$t > T$}
    \State Use quantile regression to obtain $\hat{Q}_t \leftarrow \mathcal{Q}(\hat{\varepsilon})$
    \State Obtain prediction set $\hat{C}_{t-1}(X_t)$ 
    \State Obtain new residual $\hat{\varepsilon}_t$
    \State Update residuals $\hat{\varepsilon}$ by sliding one index forward (i.e., add $\hat{\varepsilon}_t$ and remove the oldest one)
\EndFor
\end{algorithmic}
\end{algorithm}


\section{Ablation study of hyperparameters}
\label{Ablation study of hyperparameters}
\subsection{Effects of different scale parameter in Sigmoid function}
\label{Effect of different scale parameter in Sigmoid function}
We explore the effects of different scale parameter $c$ in Sigmoid function. We conduct the ablation study on Amazon stock dataset. \Cref{c_plot_cvg,c_plot_width} show the line graphs across different $c$. When base predictor is not well (such as Prophet), as $c$ increases, the sets tighten, but the coverage decreases. For AR model and Theta model, there are almost identical performance when $c$ is in a reasonable range. Note that, when $c$ is large, numeric overflow will encounter because of scalar power. As $c$ varies, the changes in coverage and width are relatively small and the two metrics are actually a trade-off. When the coverage is fixed, the width of our methods with $c \in \{0.1, 0.5, 1, 1.5, 2\}$ are consistently shorter than other methods (see \Cref{table amazon}). In general, our methods are less sensitive to the choice of scale parameter $c$ due to the trade-off between coverage and width. 

\begin{figure*}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/appendix/c_plot_cvg.pdf}
  \caption{Coverage result on different scale parameter $c$ in Sigmoid function.}
  \label{c_plot_cvg}
\end{figure*}
\begin{figure*}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/appendix/c_plot_width.pdf}
  \caption{Set width result on different scale parameter $c$ in Sigmoid function.}
  \label{c_plot_width}
\end{figure*}


\subsection{Effects of different window length}
\label{Effect of different window length}
We also explore the effects of different window length $w$ in adaptive learning rates $\eta_t$ and adaptive cutoff threshold $h_t$. \Cref{table w in amazn stock,table w in google stock,table w in synthetic data} show the experimental results of window length $w$ in ECI-cutoff. In general, $w=100$ has the shortest average width with coverage greater than 89.5\%. It is worth noting that in the synthetic dataset, there is a clear trend of increasing width and coverage as $w$ increases. This is because the main influencing factor at this time is the learning rate, and an increase in $w$ leads to a larger adaptive learning rate.


\begin{table}[ht]
\caption{The ablation experimental results of window length $w$ in the synthetic data dataset with nominal level $\alpha = 10\%$.}
\label{table w in synthetic data}
\setlength{\tabcolsep}{1.25mm} % 调整列间距
\renewcommand{\arraystretch}{1.15} % 调整行间距
\begin{center}
\small
\begin{tabular}{c|ccc|ccc|ccc}
\hline
            & \multicolumn{3}{c|}{Prophet Model}            & \multicolumn{3}{c|}{AR Model}                  & \multicolumn{3}{c}{Theta Model}                \\
\ $w$\  &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} \\ \hline
10  & 89.4 & 8.18 & 8.21 & 89.4 & 8.22 & 8.3  & 89.4 & 8.41 & 8.43 \\
50  & 89.8 & 8.42 & 8.45 & 89.6 & 8.32 & 8.37 & 89.6 & 8.61 & 8.57 \\
100 & 89.8 & 8.31 & 8.44 & 89.9 & 8.14 & 8.19 & 89.8 & 8.51 & 8.59 \\
150 & 89.9 & 8.47 & 8.46 & 89.9 & 8.43 & 8.47 & 89.9 & 8.83 & 8.82 \\
200 & 89.9 & 8.56 & 8.54 & 89.9 & 8.33 & 8.4  & 89.9 & 8.83 & 8.84 \\ \hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[ht]
\caption{The ablation experimental results of window length $w$ in the Amazon stock dataset with nominal level $\alpha = 10\%$.}
\label{table w in amazn stock}
\setlength{\tabcolsep}{1.25mm} % 调整列间距
\renewcommand{\arraystretch}{1.15} % 调整行间距
\begin{center}
\small
\begin{tabular}{c|ccc|ccc|ccc}
\hline
            & \multicolumn{3}{c|}{Prophet Model}            & \multicolumn{3}{c|}{AR Model}                  & \multicolumn{3}{c}{Theta Model}                \\
\ $w$\  &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} \\ \hline
10  & 90.1 & 40.98 & 29.68 & 89.9 & 16.05 & 11.76 & 89.7 & 16.24 & 11.84 \\
50  & 89.3 & 36.14 & 27.34 & 89   & 16.29 & 12.15 & 88.8 & 16.28 & 11.91 \\
100 & 89.7 & 43.46 & 29.98 & 89.3 & 16.91 & 12.63 & 89.6 & 17.19 & 12.48 \\
150 & 89   & 45.97 & 33.6  & 89.2 & 16.2  & 12.24 & 89.2 & 16.43 & 12.15 \\
200 & 89.1 & 43.99 & 31.66 & 89.4 & 16.24 & 12.3  & 89.1 & 16.28 & 12.48 \\ \hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[ht]
\caption{The ablation experimental results of window length $w$ in the Google stock dataset with nominal level $\alpha = 10\%$.}
\label{table w in google stock}
\setlength{\tabcolsep}{1.25mm} % 调整列间距
\renewcommand{\arraystretch}{1.15} % 调整行间距
\begin{center}
\small
\begin{tabular}{c|ccc|ccc|ccc}
\hline
            & \multicolumn{3}{c|}{Prophet Model}            & \multicolumn{3}{c|}{AR Model}                  & \multicolumn{3}{c}{Theta Model}                \\
\ $w$\  &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} \\ \hline
10  & 89.9 & 50.33 & 42.47 & 91.7 & 22.74 & 22.05 & 89.6 & 33.49 & 29.42 \\
50  & 89.4 & 47.38 & 40.9  & 90.2 & 20.69 & 19.21 & 89.3 & 31.63 & 29.64 \\
100 & 89.8 & 53.12 & 44.36 & 89.7 & 19.84 & 17.63 & 89.6 & 30.71 & 28.11 \\
150 & 89.1 & 55.62 & 47.59 & 90   & 20.53 & 17.85 & 89.7 & 30.54 & 28.98 \\
200 & 89.4 & 53.65 & 45.49 & 89.8 & 20.48 & 18.02 & 89.8 & 30.99 & 30.06 \\ \hline
\end{tabular}
\end{center}
\end{table}


\section{Experimental results in synthetic data}
\label{Experimental results in synthetic data}
Following \cite{barber2023conformal}, we test the performance on synthetic data under a changepoint setting. The data $\{X_i, Y_i\}_{i=1}^n$ are generated according to a linear model $Y_t = X_t^T \beta_t + \epsilon_t$, $X_t \sim \mathcal{N}(0, I_4)$, $\epsilon_t \sim \mathcal{N}(0,1)$. And we set:
$$\begin{aligned}
&\beta_t=\beta^{(0)}=(2,1,0,0)^\top,\quad t=1,\ldots,500, \\
&\beta_t=\beta^{(1)}=(0,-2,-1,0)^\top,\ t=501,\ldots,1500, \\
&\beta_t=\beta^{(2)}=(0,0,2,1)^\top,\quad t=1501,\ldots,2000,
\end{aligned}$$
where two changes in the coefficients happen up to time $2000$.

We compare ECI and its variants with some competing methods about the coverage and prediction set width. \Cref{changepoint coverage,changepoint set} show the result of coverage and set width, while the base predictor is Theta model.

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/cp_coverage.pdf}
  \caption{Coverage result on synthetic data under a changepoint setting.}
  \label{changepoint coverage}
\end{figure*}
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/cp_size_zoomed.pdf}
  \caption{Set width result on synthetic data under a changepoint setting.}
  \label{changepoint set}
\end{figure*}


\section{Experimental results with transformer}
\label{Experimental results with Transformer}
As one of the most successful deep learning models, Transformer has had a significant impact on various application fields, including time series. Thus it is interesting to conduct an additional experiment with Transformer as the base model. We set input length to $12$, output length to $1$, the number of encoder layers to $3$, the number of decoder layers to $3$, and the number of features in the encoder/decoder inputs to $64$. 
\\
The quantitative results are shown in \Cref{table transfomer1,table transfomer2}. Consistent with other base models, ECI variants have achieved the best performance on five benchmark datasets with Transformer.


\begin{table}[ht]
\caption{The experimental results with Transformer base model at nominal level $\alpha = 10\%$.}
\label{table transfomer1}
\setlength{\tabcolsep}{1.25mm} % 调整列间距
\renewcommand{\arraystretch}{1.15} % 调整行间距
\begin{center}
\small
\begin{tabular}{c|ccc|ccc|ccc}
\hline
           & \multicolumn{3}{c|}{Amazon stock}        & \multicolumn{3}{c|}{Google stock}          & \multicolumn{3}{c}{Electricity demand}     \\     
Method &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} \\ \hline
ACI          & 90.1 & $\infty$   & 40.44          & 90.2 & $\infty$    & 57.13 & 90.2 & $\infty$   & 0.109 \\
OGD          & 89.4 & 52.68 & 31.00             & 90.1 & 109.27 & 89.00    & 90.1 & 0.139 & 0.120  \\
SF-OGD       & 89.3 & 56.56 & 31.75          & 90.1 & 88.3   & 70.55 & 90.3 & 0.141 & 0.114 \\
decay-OGD    & 89.8 & 93.16 & 34.98          & 89.9 & 120.25 & 69.81 & 90.3 & 0.147 & 0.111 \\
PID          & 89.8 & 55.36 & 39.04          & 90.1 & 78.69  & 58.65 & 89.9 & 0.428 & 0.435 \\
ECI          & 89.9 & 49.07 & 33.79          & 89.9 & 70.93  & 55.00    & 90.2 & 0.135 & 0.111 \\
ECI-cutoff & 89.7     & \textbf{45.01} & 29.64        & 89.9     & \textbf{66.67} & \textbf{51.29} & 89.9     & \textbf{0.133} & \textbf{0.108} \\
ECI-integral & 89.7 & 45.02 & \textbf{29.46} & 90.0   & 68.64  & 52.45 & 90.2 & 0.135 & 0.111 \\ \hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[ht]
\caption{The experimental results with Transformer base model at nominal level $\alpha = 10\%$.}
\label{table transfomer2}
\setlength{\tabcolsep}{1.25mm} % 调整列间距
\renewcommand{\arraystretch}{1.15} % 调整行间距
\begin{center}
\small
\begin{tabular}{c|ccc|ccc}
\hline
           & \multicolumn{3}{c|}{Amazon stock}        & \multicolumn{3}{c|}{Google stock}     \\     
Method &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} \\ \hline
ACI          & 90.3     & $\infty$            & 11.69        & 89.9     & $\infty$           & 8.2          \\
OGD          & 89.9     & 9.72           & 9.50          & 89.9     & 8.13          & 8.2          \\
SF-OGD       & 90.0       & 10.93          & 9.97         & 90.0       & 12.55         & 11.65        \\
decay-OGD    & 89.7     & 14.43          & 11.47        & 90.1     & 8.30           & 8.24         \\
PID          & 89.9     & 10.02          & 9.75         & 89.7     & 10.75         & 9.05         \\
ECI          & 89.9     & 9.15           & 9.16         & 89.9     & 8.09          & 8.15         \\
ECI-cutoff   & 90.0       & 10.42 & 9.95         & 89.9     & \textbf{8.01} & \textbf{8.10} \\
ECI-integral & 89.9     & \textbf{8.87}  & \textbf{8.60} & 89.9     & 8.04          & 8.13         \\ \hline
\end{tabular}
\end{center}
\end{table}


\section{More details on experiments}
\label{More details on experiment}

\subsection{Discussion on the scorecaster}
\label{Discussion on the scorecaster}
It is worth noting that the Conformal PID baseline outperforms ECI under the Prophet base model in \Cref{table elec2}. Actually, the scorecaster term of conformal PID in the baseline utilizes the relatively accurate Theta model  (which can also be replaced by AR or Transformer) to take advantage of any leftover signal and residualize out systematic errors in the score distribution. It can be regarded as an additional component that “sits on top” of the base forecaster (base model). Therefore, across all test datasets, the performance of conformal PID in the Prophet model consistently outperforms that of the AR and Theta model. This is attributed to the fact that Prophet, being a worse-performing model, is complemented by the superior performance of PID's scorecaster Theta, which is a better-performing model. 
\\
We maintained the settings of Table 3 to test the performance of ECI combined with the scorecaster (Theta model), as shown in \Cref{table scorecaster}. The results demonstrated that ECI+scorecaster is consistently superior over Conformal PID across three base models, thereby validating the effectiveness of the ECI update. Interestingly, for the worse-performing Prophet base model, adding the scorecaster enhanced the performance of ECI, while the scorecaster tended to degrade performance when better-performing base models were used. This observation is also mentioned in \citet{pid_angelopoulos2024conformal}: ``an aggressive scorecaster with little or no signal can actually hurt by adding variance to the new score sequence".


\begin{table}[ht]
\caption{The experimental results in the electricity demand dataset with nominal level $\alpha = 10\%$. Both scorecasters of PID and ECI+scorecaster are Theta model.}
\label{table scorecaster}
\setlength{\tabcolsep}{1.25mm} % 调整列间距
\renewcommand{\arraystretch}{1.25} % 调整行间距
\begin{center}
\small
\begin{tabular}{c|ccc|ccc|ccc}
\hline
            & \multicolumn{3}{c|}{Prophet Model}            & \multicolumn{3}{c|}{AR Model}                  & \multicolumn{3}{c}{Theta Model}       \\     
Method &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Coverage\\ ( \%)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Average\\ width\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Median\\ width\end{tabular} \\ \hline
PID             & 90.1 & 0.207 & 0.177 & 90.0 & 0.434          & 0.432 & 89.9 & 0.413          & 0.411          \\
ECI             & 90.0   & 0.384 & 0.382 & 90.0 & \textbf{0.117} & \textbf{0.098} & 89.9 & \textbf{0.071} & \textbf{0.055} \\
ECI+scorecaster & 90.3 & \textbf{0.193}          & \textbf{0.166}          & 90.0 & 0.420  & 0.428 & 89.9 & 0.395 & 0.409 \\ \hline
\end{tabular}
\end{center}
\end{table}


\subsection{More details on learning rates in the experiments}
\label{More details on learning rates in the experiments}
In this section, we show more comprehensive experimental results. We set base predictor as Theta model and all experimental setting are aligned with \Cref{experiment}. Coverage and prediction set results can be seen as the following \Cref{figure more detail1} to \Cref{figure more detail2}.

Since the methods based on OGD are highly sensitive to the learning rate, we initially select four appropriate learning rates for these methods across various datasets, and then choose the one that performs the best among these four. In fact, aside from the OGD-based methods, the four learning rates for other methods remain unchanged. We compile a list that includes all the learning rates, which are 
$$\begin{aligned}
\text{ACI}:&\eta = \{0.1, 0,05, 0.01, 0.005\}, \\
\text{OGD}:&\eta = \{10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.005\}, \\
\text{SF-OGD}:&\eta = \{1000, 500, 100, 50, 10, 5, 1, 0.5, 0.1, 0.05\}, \\
\text{decay-OGD}:&\eta = \{2000, 1000, 200, 100, 20, 10, 2, 1, 0.2, 0.1\}, \\
\text{Conformal PID}:&\eta = \{1, 0.5, 0.1, 0.05\}, \\
\text{ECI}:&\eta = \{1, 0.5, 0.1, 0.05\}, \\
\text{ECI-cutoff}:&\eta = \{1, 0.5, 0.1, 0.05\}, \\
\text{ECI-integral}:&\eta = \{1, 0.5, 0.1, 0.05\}. \\
\end{aligned}$$


Note that, except ACI and OGD, other methods use $\eta_t$ as adaptive learning rate in practice. Specifically, for SF-OGD:
$$\eta_t = \eta \cdot \frac{\nabla \ell^{(t)}(q_t)}{\sqrt{\sum_{i=1}^{t} \|\nabla \ell^{(i)}(q_i)\|_2^2}}
,$$
\\
where $\ell^{(t)}(q_t)$ is quantile loss and $q_t$ is the predicted radius at time $t$. For decay-OGD:
$$\eta_t = \eta \cdot t^{-\frac{1}{2}-\epsilon},$$
\\
where the hyperparameter $\epsilon=0.1$ follows \citet{angelopoulos2024online}. For conformal PID, ECI, ECI-cutoff and ECI-integral: 
$$\eta_t = \eta \cdot (\max\{s_{t-w+1},\cdots,s_t\}-\min\{s_{t-w+1},\cdots,s_t\}),$$
where $s_t$ is the non-conformality score at time $t$ and the window length $w=100$ follows \citet{pid_angelopoulos2024conformal}.

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/google_coverage.pdf}
  \caption{Coverage result on Google stock dataset.}
  \label{figure more detail1}
\end{figure*}
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/google_sets.pdf}
  \caption{Prediction set result on Google stock dataset.}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/amzn_coverage.pdf}
  \caption{Coverage result on Amazon stock dataset.}
\end{figure*}
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/amzn_sets.pdf}
  \caption{Prediction set result on Amazon stock dataset.}
\end{figure*}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/elec_coverage.pdf}
  \caption{Coverage result on electricity demand dataset.}
\end{figure*}
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/elec_sets.pdf}
  \caption{Prediction set result on electricity demand dataset.}
\end{figure*}
\clearpage
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/climate_coverage.pdf}
  \caption{Coverage result on Delhi temperature dataset.}
\end{figure*}
\begin{figure*}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/appendix/climate_sets.pdf}
  \caption{Prediction set result on Delhi temperature dataset.}
  \label{figure more detail2}
\end{figure*}






