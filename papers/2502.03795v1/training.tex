\subsection{Training algorithm}

Here we present pseudocode for a notional training algorithm, i.e., an algorithm to learn the velocity field $f$ of a neural ODE in the setting of problem P1 (see Section \ref{sec:setup}), when the information divergence used is the KL-divergence. 

Recall the training objective we consider consists of two parts: one is the KL divergence from the source distribution to the pushforward of the target by the time-one flow map of the ODE; the other is the integration of \eqref{eq:straightlinereg} along the trajectories of
the particles.

Given a distribution $\pi$ for the initial condition $x$, 
% the density $\pi_{f,t}$ of the ODE state at any time $t \in [0,1]$ satisfies $\pi(x) = \pi_{f, t}(X_f(x,t))\det(\nabla_x X_f(x,t))$
the KL divergence from the source distribution to $\pi_{f,t}$ at time $t=1$ can be written as
\begin{align*}
    \dkl( \pi_{f,1} \, || \, \rho ) = \dkl ( X_f(\cdot, 1)_\sharp \pi \, || \, \rho )
    & = \dkl ( \pi \, || \, X_f^{-1}(\cdot, 1)_\sharp \rho ) \\
    & = \mathbb{E}_{x \sim \pi} \left [ \log \pi(x) - \log \rho ( X_f(x,1) ) - \log \det \nabla_x X_f(x,1) \right ].
        % \dkl[X_f(\cdot,1)_\sharp \pi || \rho ] = \dkl[\pi_{f, 1}|| X_f(\cdot, t)^{-1} \rho ] =\int_{\mathbb{R}^d} \log\left(\frac{\pi_{f,1}}{\rho(x)}\right)\pi_{f,1}dx.
\end{align*}
where $X_f^{-1}(\cdot, 1)$ denotes the inverse of $x \mapsto X_f(x,1)$. The log determinant term above can be computed from the
instantaneous change of variables formula (e.g., \citet{NeuralODE}): $\log \det \nabla_x X(x,1) = - \int_0^1 \tr \left ( \nabla_X f(X(x,t),t) \right ) dt$. 

The combined training objective becomes:
\begin{equation}\label{eq:OBJ}
  \begin{aligned}
    J(f) &= 
    % \dkl[X(x,1)_\sharp\pi(x)|| \rho(x)] + \lambda\mathbb{E}_{x\sim\pi(x)}[R(x,1)]\\
    % &= \mathbb{E}_{x\sim\pi(x)}[\log\pi(x) - \log\rho(X(x,1)) - \log\det(\nabla_x X(x,1)) + \lambda R(x,1)]\\
    % &=
    \mathbb{E}_{x\sim\pi} \left [ \log\pi(x) 
    - \log\rho(X(x,1)) +  \int_0^1 \tr \left( \nabla_X f(X(x,t),t) \right) dt + \lambda R(x,1) \right ]\\
  \end{aligned}
\end{equation}
where $\lambda$ controls the impact of penalization.

To evaluate the optimization objective, we need the ability to compute
the matrix $\nabla_X f(X(x,t),t)$, as its trace appears in the change-of-variables term and the entire matrix
appears in the regularization term. Also, we need the ability to compute
$\partial_t f(X(x,t),t)$. We can assemble these two terms
into a full Jacobian matrix, which we denote by
$\nabla_{X,t}f(X(x,t),t)$; in practice, this is the Jacobian of a neural network with respect to all of its inputs. With the
discretize-then-optimize approach of \cite{OTFlow}, we can compute
this matrix exactly via automatic differentiation. For details, see Appendix \ref{app:trainingJacobian}.

\begin{algorithm}
  \caption{Neural ODE training, problem P1}
  \label{algo1}
  \begin{algorithmic}[1]
    \STATE \textbf{Input}: sample $\mathcal{X} = \{ x_i\}_{i=1}^N$ from target
    measure $\pi$, parameterized neural network $f(x,t ; \theta)$,
    regularization parameter $\lambda$, source measure $\rho$.
    \STATE\textbf{Initialize} $\theta$, \WHILE{$\theta$ not converged}
    \STATE{Sample minibatch $\{ x_j\}$ of size $m$ from $\mathcal{X}$}
    \STATE{Set $ x_j(0) = x_j$, $l_j(0) = r_j(0)=0$ } 
    \STATE{Solve the
      following ODE system up to time $t=1$ 
      \begin{align*}
        \frac{dx_j}{dt} &= f(x_j,t;\theta)\\
        \frac{dl_j}{dt} &= -\tr(\nabla_xf(x_j,t;\theta))\\
        \frac{dr_j}{dt} &= |\nabla_xf(x_j,t;\theta)f(x_j,t;\theta) +
        \partial_tf(x_j,t)|^2
      \end{align*}
    %   \begin{itemize}
    %     \item $\frac{dx_j}{dt} = f(x_j,t;\theta)$
    %     \item $\frac{dl_j}{dt} = -\text{Tr}(\nabla_xf(x_j,t;\theta))$
    %     \item $\frac{dr_j}{dt} = |\nabla_xf(x_j,t;\theta)f(x_j,t;\theta) +
    %     \partial_tf(x_j,t)|^2$
    %   \end{itemize}
      } 
      \STATE{Compute the loss
      $L(\theta) =
      \frac{1}{m}\sum_{j=1}^m-\log\rho(x_j(1))-l_j(1)+\lambda r_j(1)$}
    \STATE{Use automatic differentiation to backpropagate and
      update $\theta$} \ENDWHILE
  \end{algorithmic}
\end{algorithm}












% By the change of variables formula, we can express this as
% \begin{align*}
%   \dkl[X(x,1)_\sharp\pi(x)|| \rho(x)] 
%   &= \int_{\mathbb{R}^d} \log\left(\frac{\pi_{f,1}(X(x,1))}{\rho(X(x,1))}\right)\pi_{f,1}(X(x,1)) \det(\nabla_x X(x,1))dx\\ 
%   &=  \int_{\mathbb{R}^d} \log\left(\frac{\pi(x)}{\rho(X(x,1))\det(\nabla_x X(x,1))}\right)\pi(x)dx.      
% \end{align*}
% Note that this can be expressed as as an expectation over the target
% measure $\pi(x)$:
% $$\dkl[X(x,1)_\sharp\pi(x)|| \rho(x)] = \mathbb{E}_{\pi(x)}[\log\pi(x) - \log\rho(X(x,1)) - \log\det(\nabla_x X(x,1))].$$

% ** condense all above **



%%%




In practice, we only have access to  finite samples
from the target measure, so we replace the population risk in
objective \eqref{eq:OBJ} with an empirical risk based on this
sample. Moreover, since $\log \pi(x)$ is independent of the velocity field $f$, it can be ignored in the optimization procedure. Hence, we arrive at the following \textit{empirical risk minimization} problem:
\begin{equation}\label{ERM}
  \begin{aligned}
    J_\text{ERM}(f) 
    % &= \frac{1}{N}\sum_{i=1}^N\left( -\log\rho(X(x_i,1)) - \log\det(\nabla_x X(x_i,1)) + \lambda R(x_i,1)\right)\\
    &= \frac{1}{N}\sum_{i=1}^N\left( -\log\rho(X(x_i,1)) +
      \int_0^1 \tr \left ( \nabla_X f(X(x_i,t),t) \right )dt + \lambda R(x_i,1)\right) \, .
  \end{aligned}\tag{ERM}
\end{equation}
Putting everything together, we have Algorithm~\ref{algo1} in Appendix~\ref{app:training}. 



% \subsection{Target Measure is Known up to Normalizing Constant}
% In this setting, we assume that the density $\pi(x) = \Tilde{\pi}(x)/C$, where $C$ is the normalization constant. In this case, we seek to minimize $\dkl[X(x,1)_\sharp\rho(x)||\pi(x)]$ plus the regularization term. The KL-divergence term can be derived similarly:
% \begin{align*}
% \dkl[X(x,1)_\sharp\rho(x)||\pi(x)] &=  \dkl[X(x,1)_\sharp\rho(x)||\Tilde{\pi}(x)/C] \\
% &= \mathbb{E}_{x\sim\rho(x)}[\log\rho(x) - \log\Tilde{\pi}(X(x,1)) + \log C - \log\det(\nabla_x X(x,1))],
% \end{align*}
% and the combined optimization objective  \ref{OP2}  is the follows:
% \begin{equation}\label{OP2}
% \begin{aligned}
%      J(f) &= \dkl[X(x,1)_\sharp\rho(x)||\pi(x)] + \lambda\mathbb{E}_{x\sim\rho(x)}[R(x,1)]\\
%      &= \mathbb{E}_{x\sim\rho(x)}[\log\rho(x) - \log\Tilde{\pi}(X(x,1)) + \log C - \log\det(\nabla_x X(x,1)) + \lambda R(x,1)]\\
%      &= \mathbb{E}_{x\sim\rho(x)}[\log\rho(x) - \log\Tilde{\pi}(X(x,1)) + \log C + \int_0^1\text{Tr}\left(\frac{\partial f(X(x,t),t)}{dX}\right)dt + \lambda R(x,1)],\\
% \end{aligned}\tag{OP2}
% \end{equation}
% where $R(x,1) = \int_0^1  |\nabla_X(f(X(x,t),t))f(X(x,t),t) + \partial_tf(X(x,t),t)|^2dt$ is the same as the previous problem, except now $x$ is sampled from $\rho$.

% For the purpose of optimization in practice, independent terms in \ref{OP2} like $\log\rho(x) + \log C$ could be dropped. Also, note that compared to the previous problem, we now know everything about $\rho$. To evaluate the expectation in \ref{OP2}, a simple Monte-Carlo method would yield Algorithm \ref{algo1}, with the role of $\pi$ and $\rho$ switched. We comment here that there exist a host of techniques to approximate the integral with respect to the source measure, including quadrature and cubature formulas, sparse quadratures, and various other Monte-Carlo methods and we will not discuss them in details here. Regardless of the tool for computing the expectation, automatic differentiation combined with an explicit ODE integration scheme would give the update on the parameters in the neural network. 
\subsection{Exact computation of the Jacobian}\label{app:trainingJacobian}
Training neural ODEs consists of minimizing the (regularized) loss over the network weights subject to the ODE constraint. The adjoint-based methods in \citet{NeuralODE}, \citet{ffjord}, and \citet{HowToTrain} can be viewed as an optimize-then-discretize approach: another continuous-time ODE (the \textit{adjoint} equation) provides exact gradients with respect to network weights. Both the forward and adjoint equations are then discretized, checkpointing is typically employed to reduce memory requirements, and some care is required to ensure consistency of gradients.
%
% since they optimize the continuous ODE by solving another differential equation called the \textit{adjoint equation} that gives the update on the network parameters and discretize the optimal dynamics after training}. \ymmtd{This is still a bit vague. Discretization of the forward and adjoint equations is necessarily part of training as well.}
%
Alternatively, a discretize-then-optimize approach is proposed in \citet{DiscretizeOptimize} and \citet{OTFlow}, where one first discretizes the forward dynamics and computes gradients backwards in time via automatic differentiation.
%
% then solves a finite-dimensional optimization problem. 
% --> (ymm) the optimization problem is always finite-dimensional, whether in DTO or OTD
%
Since the training objective proposed in our work involves the entire Jacobian matrix of the velocity field, it is natural to use the discretize-then-optimize approach, which allows exact computation of the Jacobian. As in \cite{OTFlow}, the velocity field can be implemented as a ResNet and we can compute the Jacobian recursively. 

Let $s = (x,t)$ be the new variable formed by appending the time variable to the space variable. We then have the following recursive relation in a $M$-layer ResNet, where the $\{u_i\}$ are the outputs from intermediate layers:
\begin{align*}
&u_0 = \sigma(K_0s + b_0)\\
&u_1 = u_0 + h\sigma(K_1u_0 + b_1)\\
& \vdots \\
& u_M = u_{M-1} + h\sigma(K_Mu_{M-1} + b_M)\\
\end{align*}
Taking the gradient with respect to variable $s$, we have $\nabla_s u_i^T = \nabla_s u_{i-1} + h\sigma'(K_iu_{i-1} + b_i)K^T_i\nabla_s u_{i-1}$. 
Therefore, we have the following update rule for the Jacobian: $$J \leftarrow J+h\sigma'(K_iu_{i-1} + b_i)K_i^TJ.$$	
The network parameters $K_i$ and $b_i$ are to be learned. Since we use a discretized version of the velocity field in the implementation, these parameters can be updated through automatic differentiation. 