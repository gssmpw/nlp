\documentclass[11pt]{article}

\input{packages}
\input{macros}


\begin{document}

\title{Distribution learning via neural differential equations: minimal energy regularization and approximation theory}

\author{Youssef Marzouk\thanks{Massachusetts Institute of Technology, Cambridge, MA 02139, USA (\href{mailto:ymarz@mit.edu}{ymarz@mit.edu}, \href{mailto:zren@mit.edu}{zren@mit.edu})} \and Zhi Ren\footnotemark[1]
\and Jakob Zech\thanks{Heidelberg University, 69120 Heidelberg, Germany (\href{mailto:jakob.zech@uni-heidelberg.de}{jakob.zech@uni-heidelberg.de})} 
 % \thanks{Massachusetts Institute of Technology, Cambridge, MA 02139, USA, }
}

\date{\today}

\maketitle
	
\begin{abstract} 
%
Neural ordinary differential equations (ODEs) provide expressive representations of invertible transport maps that can be used to approximate complex probability distributions, e.g., for generative modeling, density estimation, and Bayesian inference. 
%
We show that for a large class of transport maps $T$, there exists a time-dependent ODE velocity field realizing a straight-line interpolation $(1-t)x + tT(x)$, $t \in [0,1]$, of the displacement induced by the map. Moreover, we show that such velocity fields are minimizers of a training objective containing a specific minimum-energy regularization. 
%
We then derive explicit upper bounds for the $C^k$ norm of the velocity field that are polynomial in the $C^k$ norm of the corresponding transport map $T$; in the case of triangular (Knothe--Rosenblatt) maps, we also show that these bounds are polynomial in the $C^k$ norms of the associated source and target densities. 
%
Combining these results with stability arguments for distribution approximation via ODEs, we show that Wasserstein or Kullback--Leibler approximation of the target distribution to any desired accuracy $\epsilon > 0$ can be achieved by a deep neural network representation of the velocity field whose size 
is bounded % explicitly
in terms of $\epsilon$, the dimension, and the smoothness of the source and target densities. 
% \ymmtd{alt: scales explicitly with $\epsilon$ and with the smoothness and dimension of the source and target densities.} 
The same neural network ansatz yields guarantees on the value of the regularized training objective.

% [[v2: Combining these results with stability arguments for distribution approximation via ODEs, we show that any $\epsilon$-accurate Wasserstein or Kullback--Leibler approximation of the target distribution can be achieved by a deep neural network representation of the velocity field whose size scales explicitly with $\epsilon$ and with the smoothness and dimension of the source and target densities, and that these approximation guarantees apply to minimizers of the regularized training problem.]]

% [[v3: Combining these results with stability arguments for distribution approximation via ODEs, we show that Wasserstein or Kullback--Leibler approximation of the target distribution can be achieved by a deep neural network representation of the velocity field whose 
% % depth, width, and sparsity are
% size is bounded in terms of 
% \ymmtd{alt: ``whose size scales explicitly with properties of the source and target density''}
% \ymmtd{alt: ``whose size scales explicitly with the smoothness and dimension of the source and target densities''}
% properties of the source and target densities, and provide approximation guarantees for minimizers of the regularized optimization problem.]]
% \ymmtd{In this part of the paper, our bounds are somewhat less explicit, in that we don't (I believe) track dependence on $L_1$ and $L_2$; just $k$ and $d$.} 
% \ymmtd{Say anything more explicit about the rates we obtain? Roughly, we get $\epsilon^{-\frac{d+1}{k-1}}$ scaling of network size for $\epsilon$ accuracy in Wasserstein and for $\epsilon^2$ in KL. But it's kind of messy for an abstract.}
% \jztd[inline]{I think the current version (v1) is fine. It refers to the ``dimension and smoothness'' ($d$ and $k$), which we do track in our estimates.}



% : if the objective can be brought to some threshold above zero, we establish a corresponding bound on the approximation error of the target distribution.
  
% OLD VERSION  
 %  (To be written) Neural ordinary differential equations
 %    (NeuralODEs) \jz{provide} an invertible neural network
 %    architecture that has recently been proposed to learn
 %    probability distributions and perform sampling
 %    tasks. % \jzs{as a continuous normalizing flow.}
 %    %In this work,
 % We show that \rr{for a large class of transport maps that push forward one measure onto another, there exists a velocity field of the ODE
 % corresponding to the displacement interpolation
 % $(1-t)x+tT(x)$ of the transport map $T$ that achieves
 % exact sampling} \rrs{and learning}. \ymmtd{Wording is a bit confusing; does ``that achieves exact sampling and learning'' refer to the velocity field or to the map $T$? In the end, of course, it's both; but what is this specific statement about---i.e., is the starting point any transport map that pushes forward a ``source distribution'' on the initial conditions to a target distribution of interest? And then we show that for any such map, there exist a velocity field that realizes it? Presumably we need to put some simple conditions on the maps, or at least acknowledge that within our short statement here. And more broadly, we could say "We show that, given some transport map $T$\ldots, there exists a velocity field corresponding to the displacement interpolation \ldots}
 % %    
 % \ymmtd{What is meant by ``learning'' here? Exact sampling I understand, but what precisely is exact learning?}
 % In addition, we propose
 % a new regularization based on exact computation of the
 % Jacobian of the velocity field, which minimizes the
 % kinetic energy of the trajectories for any given
 % transport map $T$; this applies to optimal maps,
 % triangular (Knothe--Rosenblatt) maps, and other kinds
 % of transport.  More importantly, we prove regularity
 % results about the velocity field based on the
 % regularity of the underlying transport map $T$, and
 % establish neural network approximation \rr{results} \ymmtd{better: results? bounds?} for
 % generative modelling with NeuralODEs. Specifically, we
 % show that one can efficiently sample from \ymmtd{``sample from'' $\to$ approximate} a wide class
 % of terminal target distributions by choosing the
 % velocity field from a class of multi-layer feedforward
 % neural networks whose \rr{size} can
 % be explicitly bounded, with the accuracy of sampling \ymmtd{I'm thinking we shouldn't explicitly say sampling, since we are really talking about distributional approximation (and not Monte Carlo). Same comment for above.}
 % measured by various metrics including $L^2$, Kullback--Leibler, and
 % Wasserstein. The technique we use to obtain bounds on
 % the higher order partial derivatives of the velocity
 % field is based on a multivariate version of Fa\'{a} di
 % Bruno's formula, which we believe is of independent
 % interest. 

\end{abstract}

 
\input{NeuralODE-body}


\bibliography{ref.bib}

\end{document}

