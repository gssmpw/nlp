\section{Introduction}\label{sec:intro}

Sampling from an arbitrary probability distribution is a central problem in computational statistics and machine learning. Transportation of measure \citep{OptimalOldAndNew} offers a useful approach to this problem: the idea is to construct a measurable map that \emph{pushes forward} a relatively simple source distribution (usually chosen to be uniform or standard Gaussian) to the target probability distribution. One can then simulate from the target distribution by drawing samples from the source distribution and evaluating the transport map. This construction is useful for both generative modeling \citep{ffjord,glow} and variational inference \citep{moselhy2012,PlanarFlow}. When the map is invertible, one can also estimate the \emph{density} of the target measure by evaluating the density of the pushforward of the source distribution under the transport map.

% NFs, neural ODEs, dynamic representations of transport. 

Many parameterizations of such transports, ranging from monotone polynomial-based approximations % \citep{moselhy,zech,baptista23}
\citep{measure-transport,ZM1,ZM2,baptista23}
to input-convex neural networks \citep{huang2021convex,2310.16975} have been proposed. In the machine learning literature, normalizing flows \citep{cms/1266935020,NormalizingFlowIntro} represent transport maps by composing a sequence of relatively simple invertible functions. It is typically required that the Jacobian determinant of the resulting map be easily computable, as it appears in expressions for the pushforward or pullback densities via the change-of-variables formula. Common constructions for normalizing flows including planar and radial flows \citep{PlanarFlow}, affine coupling flows \citep{CouplingFlows}, autoregressive flows \citep{autoregressiveflow}, and neural autoregressive flows \citep{NeuralAutoFlow}.

More recent work in deep learning has elucidated connections between deep neural networks and differential equations \citep{TransportAnalysisofDL,NumericalSchemeODE,PDEmotivatedNN}. In particular, neural ordinary differential equations (neural ODEs) \citep{NeuralODE} are ODEs whose velocity fields are represented by neural networks. A neural ODE can be understood intuitively as the ``continuous-time limit'' of a normalizing flow, insofar as the flow map of the ODE is given by the composition of infinitely many incremental transformations. Neural ODEs can be used to represent distributions \citep{ffjord}---again for the purposes of generative modeling, inference, or density estimation---as follows. Let $\pi$ denote the target distribution from which we wish to sample and let $\rho$ denote the source distribution. Solving the initial value problem
\begin{equation}\label{eq:ODE}
    \begin{cases}
        \frac{d }{dt} X(x,t) &= f \left (X(x,t), t \right )\\
                    X(x, 0) &= x
    \end{cases}
\end{equation}
up to time $t=1$, for some initial condition $x \in \Omega_0$, yields a flow map $x \mapsto X^f(x, 1)$. The goal is to learn a Lipschitz continuous velocity field $f$, parameterized as a neural network, such that $x\sim \rho$ implies $X^f(x,1)\sim \pi$; in other words, the flow map pushes forward the source distribution to the target distribution.
    
Such dynamical representations of transport enjoy several desirable properties. Invertibility of $x\mapsto X^f(x,1)$ is guaranteed for any Lipschitz $f$ satisfying suitable boundary conditions, as one can solve \eqref{eq:ODE} backward in time. Therefore, and in contrast to other methods that directly parameterize the displacement (such as invertible neural networks \citep{InvNN}, normalizing flows \citep{NormalizingFlowIntro}, or other transport maps \citep{baptista23}), no further restrictions need to be imposed on the vector field $f$ that is to be learned. The density $\eta$ of the ODE state at time $t$, 
% given a density for the initial condition $x$,
i.e., the density of $X(\cdot,t)_\sharp \rho$, 
can also easily be computed as it obeys the dynamics
$$\frac{d\log \eta(x,t)}{d t} = -\tr \left (\nabla_X f(X(x,t), t) \right ),$$ 
which is known as the \emph{instantaneous change of variables formula}; see \citet{NeuralODE}.
% \jztd{consistently use $\partial$ or $d$}


% Questions: reg, approximation rates, stability.

% Finite-sample learning rates: see our companion paper.

To understand the usefulness of neural ODEs in learning distributions, there are at least three natural questions to ask.
%
{First}, it is known that when the source and target measures are well-behaved (for example, both absolutely continuous with respect to the Lebesgue measure), there are in general infinitely many transport maps that push forward one measure onto the other. Moreover, even if we require the time-one flow map $x\mapsto X^f(x,1)$ to be a particular transport map $T$, there are in general still infinitely many velocity fields $f$ that realize $T$. It has thus been observed \citep{OTFlow,HowToTrain} that without any form of regularization, learned ODE trajectories connecting $x$ to $T(x)$ may be very irregular. It is therefore natural to ask how we can regularize the training objective to improve the training process.
%
{Second}, given a regularized training objective, we would like to characterize the structure of its minimizers, and in particular to quantify how well a neural network of a given size (e.g., width, depth, sparsity) can approximate the velocity field corresponding to an optimal solution. 
%
{Third}, we would like to know how to bound the distance between the target measure and the pushforward of the source measure under the ODE-induced flow map, when a neural network is used to approximate the velocity field $f$. 

Our work is the first attempt to address these questions in a unified way. One of our key goals is to obtain explicit rates for distribution approximation using regularized neural ODEs, linking properties of the source and target measures to bounds on the size of a deep neural network representation of the velocity field that achieves a given distributional approximation error. Furthermore, we aim to connect this distribution approximation error to the value of the regularized training objective.


% Our companion paper, \citet{StatisticalNODE}, does

%%%%%%%%%%%%%%%%%%%%




\subsection{Contributions}

We summarize the main contributions of this paper as follows: %\ymmtd{Do you like the italicized header for each contribution, or is it too much?}
\begin{itemize}

\item \textit{Realizing straight-line trajectories.} For a large class of transport maps $T$, we show that there exists a corresponding time-dependent velocity field $f(x,t)$ achieving the interpolation $(1-t)x + tT(x)$ and hence producing straight-line trajectories. In a Lagrangian frame, these trajectories have constant velocity and hence zero acceleration.

\item \textit{Regularity of the space-time domain.} We show that, under certain conditions, the space-time domain covered by such ODE trajectories is a Lipschitz domain, and thus admits suitable extensions that enable the application of existing neural network approximation results.

\item \textit{Minimum energy regularization.} We propose a new regularization scheme for ODE velocity fields, based on penalizing the average kinetic energy of trajectories. We characterize the minimizers of the resulting optimization problem (whose objective is the sum of a divergence and this penalty), and show that these time-dependent velocity fields take the straight-line interpolation form above.

\item \textit{Regularity of the ODE velocity field.} We derive explicit upper bounds for the $C^k$ norm of a straight-line velocity field that are polynomial in the $C^k$ norm of the corresponding transport map $T$. This development involves applying a multivariate Fa\`{a} di Bruno formula in Banach spaces, a procedure that may be of independent interest. %\ymmtd{not sure if we should add this: ``a procedure which may be of independent interest, as such estimates also occur in numerical analysis and partial differential equations.''}

\item \textit{Explicit links to $C^k$-smooth densities.} 
% Explicitly bounding smoothness of 
For specific transports $T$, in particular \emph{triangular} (Knothe--Rosenblatt) transport maps, we then construct upper bounds on the $C^k$ norm of the corresponding {straight-line} velocity field that 
have polynomial dependence on
% depend polynomially on 
% the regularity of 
the $C^k$ norm of
the source and target densities. Along the way, we obtain an explicit upper bound for the $C^k$ norm of the Knothe--Rosenblatt map that depends polynomially on the $C^k$ norms % and other properties 
of the source and target densities.

% Mention anything about how this could also be done for optimal transports? Yes we could, but I am only aware of regularity results of the optimal transport, but not any bounds on the norms) I think it is worth mentioning that an upper bound on the Ck norm of KR map in terms of the densities could be something of independent interest (like I was a bit surprised at first that nobody had done it before). L1 and L2! Interesting point. I guess in Sven’s minimax paper we discuss the anisotropic Holder/$C^k$-type regularity of the KR map but do not write explicit upper bounds on the norms (there are unknown constants). In the papers with Jakob on triangular maps, we focus on understanding the domain of analyticity of KR maps, and thus it’s different. Is this accurate?

\item \textit{Distributional stability.} We relate the approximation error in the velocity field to the error in the distribution induced by the time-one flow map of the resulting ODE, in both Wasserstein distance and Kullback--Leibler (KL) divergence.

\item \textit{Neural network approximation and optimization.} Combining our analysis of the regularity of the velocity field with the preceding stability results, we show that approximation of the target distribution to any desired accuracy $\eps > 0$, in Wasserstein distance or KL divergence, can be achieved by a deep neural network representation of the velocity field whose depth, width, and sparsity are bounded explicitly in terms of $\eps$ and 
% \ymmtd{with explicit dependence on $\eps$ and on} 
the smoothness and dimension of the source and target densities.
% \ymmtd{alt: scale explicitly with the smoothness and dimension of the source and target densities}
% original: are explicitly bounded in terms of properties of the source and target densities. 
% \ymmtd{See comment in abstract too. Our bounds at this stage are not explicit in the $C^k$ norm and hence the constants $L_1$ or $L_2$. Rather we seem to have things of the form $\lesssim \epsilon^{-(d+1)/(k-1}$, etc.}
%
We then provide guarantees for minimizers of the regularized optimization problem over such neural network classes, showing that there exist velocity fields that render the objective---and hence both the distribution approximation error and the departure from minimum kinetic energy---arbitrarily small.
%\textit{simultaneously} renders both terms in the objective---i.e., the distribution approximation error and the departure from minimum kinetic energy---arbitrarily small.
% \ymmtd{original, maybe less precise? ``\dots showing that if the regularized objective can be brought to some threshold above zero, we establish a corresponding bound on the approximation error of the target distribution.''}

% have accurate approximation of the target distribution in various metrics/divergences (clean this up). Emphasize: we analyze the minimizer, NOT how (or whether) it can be obtained!!

% (Here, we are not actually explicit about all the constants, though.) I think we are not explicit about the dimension dependence d, but it should be explicit in terms of the regularities of the densities? (Like L1, L2) How explicit are the constants in Prop 6.2 and Thm 6.3? Same/related question for Thm 4.16?

% OK: we can be explicit in terms of the Ck norm of f, which then can be related to L1 and L2 in some settings. Make this explicit in Prop 6.2.

\end{itemize}


\subsection{Related work}

% Approximation power of discrete normalizing flows. For function approximation: stuff. For distribution learning: stuff. Just universal approximation. 

Several papers have studied the approximation power of discrete normalizing flows. \citet{DisAppro2} investigate basic flow structures (e.g., planar flows, radial flows, Sylvester flows, Householder flows) for $L^1$ approximation of a target density on $\mathbb{R}^d$. The authors establish a universal approximation result for $d=1$, but show partially negative results for $d>1$: there exist distributions that cannot be exactly coupled by such flows, and there are other distributions for which accurate approximation requires composing together a prohibitive number of layers. \citet{DisAppro1}, on the other hand, show that normalizing flows based on the so-called ``affine coupling'' construction are universal approximators of diffeomorphisms, and consequently that their pushforward distributions converge weakly to any desired target as the complexity of the flow increases in suitable way. Additional universal approximation results have been developed for more specific flow architectures \citep{NeuralAutoFlow}. However, none of these works characterizes the \emph{rate} of convergence of the approximation---i.e., how the distribution approximation error that can be achieved scales with the size of the model.

For triangular (Knothe--Rosenblatt) transport maps on $[0,1]^d$, \citet{ZM1} develop a complete approximation theory under the assumption of analytic source and target densities, for neural network or sparse polynomial representations of the map. These results encompass approximation of the maps themselves, but also distribution approximation via the pushforward of a given source distribution by the approximate map. \citet{ZM2} extend this analysis to triangular transport maps in infinite dimensions, i.e., on $[0,1]^\infty$. \citet{baptista2023approximation} provide a general framework for analyzing the error of variational approximations of distributions realized using transport maps, and shows how this framework can be applied in specific situations to derive approximation rates.

The preceding works considered approximation of the transport map itself, i.e., direct representations of the `displacement' of points from the support of the source to the support of the target. In contrast, other recent efforts have addressed approximation issues in \textit{dynamic} representations of transport. Most of these have focused on neural ODEs. 
%    \item Very briefly review function approximation results for neural ODEs. But then note how function approximation results are not necessarily relevant to distribution learning.
\citet{SupApproximation} show that neural ODEs are universal approximators of smooth diffeomorphisms on $\mathbb{R}^d$ in appropriate Sobolev norms. \citet{DynamicalSystem} adapt ideas from dynamical systems to show that neural ODEs are universal approximators of continuous functions from $\mathbb{R}^d$ to $\mathbb{R}^m$ (hence, not only diffeomorphisms) in a $L^2$ sense, for $d \geq 2$. Yet these universal approximation results do not characterize approximation \emph{rates}, e.g., relate bounds on function approximation error to the size of the network representing the velocity field. Moreover, we note that universal {function} approximation results are not necessarily relevant to {distribution} learning: that is, universal approximation of distributions does not require universal function approximation.

% universal approximation of functions is \emph{not} necessary for universal approximation of distributions. \ymmtd{Moreover, universal approximation of distributions does not require universal function approximation.}

% Additionally, we note that universal \textit{function} approximation results are not necessarily relevant to \textit{distribution} learning. For instance, it is observed in \citet{Augmented} that certain simple maps can not be exactly represented by (neural) ODEs, but failure of exact representation does not imply the impossibility of approximation; moreover, universal approximation of distributions does not require universal function approximation. \ymmtd{Tighten this up; it's digressive.}
    
    % \item Distribution approximation with neural ODEs: the only work we are aware of is Ruiz-Balet. Go into some details. No smoothness in their analysis. Construction is quite different; no straight-line paths, very different training objective (if any).

\citet{NeuralODE-Control} study distribution approximation with neural ODEs, and prove universal approximation for certain target distributions in Wasserstein-1 distance, using an approach that is discrete and constructive. Specifically, they analyze neural ODE-type models from a controllability perspective, explicitly constructing piecewie constant approximations of the target density using a neural network velocity field with ReLU activations. Their analysis does not consider higher-order smoothness, however, and their velocity construction is different from that considered here. In subsequent work, \citet{alvarez2024constructive} show that ReLU velocity fields chosen to be piecewise constant in time can approximate a target distribution arbitrarily closely in relative entropy. 
 
    % \item Regularizing neural ODEs: Oberman, Onken, and minimum curvature papers. Relevant to our minimum energy regularization scheme.

As mentioned earlier, several other works identify neural ODE velocity fields via a \textit{regularized} training objective, i.e., by minimizing a linear combination of a statistical divergence (or negative log-likelihood) and some regularization term.
%
\citet{HowToTrain} argue that a good way of measuring the regularity of
the velocity field is through the ``acceleration'' experienced by a ``particle'' $X^f(x,t)$ starting at some point $x$. This acceleration is the total derivative of $f$ in time: 
\begin{equation}\label{eq:straightlinereg}
  \frac{ D f(X,t)}{D t} = \nabla_X f(X,t) \cdot \frac{ \partial X}{ \partial t} + \frac{\partial f(X,t)}{ \partial t} = \nabla_X f(X,t) \cdot f(X,t) + \frac{\partial f(X,t)}{\partial t}.
\end{equation}
When this term is zero, particle trajectories will be straight lines. Since the Jacobian matrix $\nabla_X f(X,t)$ is
in general not easily accessible, \citet{HowToTrain} choose to
implicitly penalize this term by penalizing $|f(X,t)|^2$ and the
Frobenius norm $\|\nabla_X f(X,t)\|_F^2$ instead. Similar to
\citet{ffjord}, stochastic methods are used  to
estimate $\|\nabla_X f\|_F^2$ and $\tr(\nabla_X f(X,t))$ in training, where the latter is used in the change of variables formula for the log-likelihood. In more recent work, \citet{OTFlow} propose a discretize-then-optimize
approach to training neural ODEs, where a ResNet structure is used to
implement the underlying neural network in the ODE. This approach
enables exact computation of the Jacobian matrix as well as its trace
from the recursive structure of the ResNet. Then, automatic
differentiation is used to update the parameters in the neural
network, instead of solving the adjoint equation as in \citet{ffjord}
and \citet{HowToTrain}. By adopting this discretize-then-optimize approach, we propose to penalize \eqref{eq:straightlinereg} directly. We will show that the velocity field $f$ making this term zero is the unique velocity field that yields the minimal kinetic energy among all velocity fields $f$ that produce the same transport map $T$ at time one; hence it is termed the \textit{minimal energy regularization}. 


    % Stochastic interpolants, rectified flow, and flow matching: these also produce neural ODEs, but with a different training scheme---essentially regression for a specific velocity field, rather than maximum likelihood estimation or (in the infinite-sample limit)  minimization of a statistical divergence. But the same velocity approximation questions and distributional stability questions, are present in that setting as well. The approximation results we develop in the present paper are independent of training scheme, and in particular, the straight-line velocity fields we analyze here are central to rectified flow.

While the neural ODE framework learns the velocity field by minimizing the KL divergence from the target distribution to the pushforward distribution of the source under ODE flow maps, another recent line of research  aims to specify the velocity field \textit{a priori} using conditional expectations and to learn the velocity field directly via least-squares regression \citep{StochasticInterpolant1, StochasticInterpoalnt2, FlowMatching, RectifiedFlow}. However, the same velocity field approximation questions and distributional stability questions are present in that setting as well. We note that the approximation results we develop in this work are independent of training scheme, and in particular, the straight-line velocity fields we analyze here are central to the rectified flows proposed in \citet{RectifiedFlow}. 



    

    % \item (this item is a bit more detailed) There are a variety of related \textit{distributional stability} results in recent literature (cf.\ our Section~\ref{sec:DistApproximation}), addressing the question of how error in the pushforward distribution under an ODE flow map (in some distance or divergence) is controlled by error in the velocity field (in some norm).
    % \begin{itemize}
    %     \item Our results:  Wp error in the distribution, for $p \in [1, \infty]$, is controlled by space-time $L^\infty$ (i.e., $C^0$) error and spatial Lipschitz constant of $f$, on compact domains. Do we then control this Lipschitz constant? Yes. Also our results: KL controlled by $C^1$ norm of $f$, on compact domains.

    %     \item Benton, Theorem 1: W2 error in distribution controlled by $L^2$ approximation error and averaged spatial Lipschitz constant of the approximate flow. Then see below for bounding this constant.

    %     \item Gao, Lemma 4.2: Almost exactly like Benton result: W2 controlled by $L^2$ error in velocity times exponential of spatial Lipschitz constant.

    %     \item Li 2024: Analyze a discrete-time version of the probability flow ODE. Bounds TV distance to true distribution by terms involving the $L^2$ error in the score function and in its Jacobian. [https://arxiv.org/pdf/2408.02320]
        
    %     \item Huang 2024: Probability flow ODE, starting in continuous time, then considering discretization. Early stopping. Assumes $L^2$ accurate score estimate and bounds on first two derivatives. Then bounds TV distance to true dist. [https://arxiv.org/pdf/2404.09730]
        
    % \end{itemize}

Indeed, there are a variety of  related \textit{distributional stability} results in recent literature (cf.\ Section~\ref{sec:DistApproximation}), addressing the question of how error in the pushforward distribution under an ODE flow map (in some distance or divergence) is controlled by error in the velocity field (in some norm). \citet{BentonErrorBoundsFlowMatching} show that $W_2$ error in distribution is controlled by $L^2$ approximation error of the true velocity field and the time-averaged spatial Lipschitz constant of the approximate flow. In a study of the convergence of continuous normalizing flows, \citet{GaoConvergenceofContinusNormalizingFlows} obtain a stability result almost exactly the same as that in \citet{BentonErrorBoundsFlowMatching}, where $W_2$ error in distribution is controlled by $L^2$ error in velocity field times the exponential of spatial Lipschitz constant of the velocity field.
    \citet{LiProbabilisticODEConvergence} analyze a discrete-time version of the probability flow ODE, where TV error in distribution is bounded by terms involving the $L^2$ error in the score function and in its Jacobian. Again in the setting of probability flow ODEs, \citet{HuangProbabilisticODEConvergence} start in continuous time and then considers discretization using a Runge--Kutta scheme. At the continuous level, it is shown that TV error in distribution is controlled by 
    $L^2$ error in the approximation of the score function and
    the first and second
derivatives of the estimated score; at the discrete level, it is shown that the $p$-th order integrator also converges under an additional assumption that the estimated score function’s first $p + 1$ derivatives are bounded. In our work, we show that $W^p$ error in the distribution, for $p \in [1, \infty]$, is controlled by the space-time $L^\infty$ (i.e., $C^0$) error and spatial Lipschitz constant of $f$, on compact domains; these are further related to properties of the densities. In addition, we obtain that distribution approximation error in KL is controlled by the $C^1$ norm of $f$, again on compact domains.
    
    
    
    

    % \item (this item is a bit more detailed) There are also some results linking properties of the velocity field (e.g., Lipschitz constant in space or time) to properties of the underlying densities
    % \begin{itemize}
    %     \item Benton, Thm 2: Bound time-averaged spatial Lipschitz constant under some assumptions on the regularity of all the distributions between $t=0$ and $t=1$, along with some Gaussian smoothing; upper bound depends on properties of the chosen interpolant. Bounded domains. Regularity assumption is rather different than the $C^k$ smoothness we assume here. No higher-order smoothness of the velocity field.
        
    %     \item Gao: Focuses on flow matching with *linear* interpolation. They say: ``We derive regularity properties for the velocity field of the CNF with linear interpolation. We demonstrate the Lipschitz regularity properties of the velocity field in both the space and time variables and establish bounds for the Lipschitz constants. We also show that the velocity field grows at most linearly with respect to the space variable.'' Main result is Thm 5.1. No Gaussian smoothing, but requires early stopping. [Yuan Gao, Jian Huang, Yuling Jiao, and Shurong Zheng. Convergence of continuous normalizing flows for learning probability distributions. arXiv:2404.00551, 2024.]
        
    %     \item Benton and Gao: both specific to certain stochastic interpolants, which are different than the straight-line ansatz we analyze here. They do not consider higher-order smoothness of the velocity field. No explicit upper bounds.

    % \end{itemize}

There are also some results linking properties of the velocity field (e.g., Lipschitz constant in space or time) to properties of the underlying densities. In \citet{BentonErrorBoundsFlowMatching}, the time-averaged spatial Lipschitz constant is related to assumptions on the regularity of all the intermediate distributions between $t=0$ and $t=1$, along with some Gaussian smoothing; an upper bound is obtained that depends on properties of the chosen interpolant. We note that their regularity assumption is rather different than the $C^k$ smoothness we assume here, as they do not consider higher-order smoothness of the velocity field. \citet{GaoConvergenceofContinusNormalizingFlows} focus on flow matching with linear interpolation. It is shown that the Lipschitz constant of the target velocity field in both the space and time variables is bounded under assumptions of log-concavity/convexity of distributions and boundedness of the domain. In addition, they show that the velocity field itself grows at most linearly with respect to the space variable. No Gaussian smoothing is used in their setting, but they require an early stopping before reaching time $t = 1$. Both \citet{BentonErrorBoundsFlowMatching} and \citet{GaoConvergenceofContinusNormalizingFlows} are specific to certain stochastic interpolants, which are different than the straight-line ansatz we analyze here. Also, they do not consider higher-order smoothness of the velocity field or derive upper bounds that are explicit in the densities. 
    
    
    
    
    
    
    
    


    % \item (this item is a bit more detailed) Results on neural network approximation rates. Gao et al.\ specifically consider flow matching with linear interpolation. Their assumptions on the source and target distributions do not entail higher-order smoothness. They construct NN classes that capture the resulting Lipschitz properties of the velocity field, and derive rates of approximation in the $L^\infty$ sense (or maybe in the $W^{1, \infty}$ sense? Need to check).

With regard to neural network approximation results, \citet{GaoConvergenceofContinusNormalizingFlows}
    % \ymmtd{this part seemed repetitive}
    % specifically consider flow matching with linear interpolation and their assumptions on the source and target distributions do not entail higher-order smoothness. They 
    construct neural network classes that capture the  Lipschitz properties of the velocity field, and derive rates of approximation in the $L^\infty$ sense. Our companion paper \citep{StatisticalNODE} derives explicit neural network approximation rates for general $C^k$ velocity fields, but its main focus is on statistical finite sample guarantees for neural ODEs trained through likelihood maximization, different from our focus here. 
    

  
    
    % \item There are also certainly SDE and specifically neural SDE methods for distribution learning (cite a few). Rather different than the deterministic ODE approach, and not our focus here.

There are also stochastic differential equation (SDE) and specifically neural SDE methods for distribution learning \citep{NeuralSDE, SongDiffusionModels}. However, they are rather different than the deterministic ODE approach, and again are not the focus of this work.
    







% - Hong Kong group's flow matching papers: no assumptions on higher-order smoothness of densities. Velocity field only controlling maybe Lipschitz constants?


% - distributional stability: only Wasserstein in the cited works? Also, they don't explicitly deal with neural networks.

%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%


% From statneuralODE paper:

% Questions of \emph{function approximation} with neural ODEs have been studied in \citet{li2022deep, ishikawa2022universal}. \citet{ishikawa2022universal} show that neural ODEs are univeral approximators of smooth diffeomorphisms on $\mathbb{R}^d$ in appropriate Sobolev norms. \citet{li2022deep} adapts ideas from dynamical systems to show that neural ODEs are universal approximators of continuous functions from $\mathbb{R}^d$ to $\mathbb{R}^m$ (hence, not only diffeomorphisms) in a $L^2$ sense, for $d \geq 2$. Both papers compose the flow map of the ODE with a terminal mapping, meant to represent a classification or regression layer.
% Yet these universal approximation results do not characterize approximation \emph{rates}, e.g., relating bounds on an approximation error to the size of the network representing the velocity field. Function approximation is also different than our present focus of statistical recovery guarantees.

% The results on diffeomorphism approximation in \citet{ishikawa2022universal} do translate to universal approximation of certain classes of distributions, in a weak sense and in total variation. \citet{ruizbalet21} also proves universal approximation for certain target distributions, in Wasserstein-1 distance, using a rather different approach that is discrete and constructive. 
% Our companion paper \citet{ren23a}, in contrast, establishes approximation \textit{rates} for neural ODE representations of distributions with $C^k$-smooth densities, and shows that there exist neural network representations of the velocity field $f$, with size explicitly bounded in terms of the regularity of the densities, that achieve efficient approximation.  
% There has also been relevant work on approximation theory for transport maps that are not constructed via ODEs. For example, \citet{ZM22a,ZM22b} investigate sparse polynomial and neural network approximations of triangular (Knothe--Rosenblatt) maps, formulating \textit{a priori} descriptions of an ansatz space that achieves exponential convergence in the case of analytic densities. A broader framework for understanding the distributional errors of transport map approximations is proposed in \citet{baptista2023approximation}.




% TO REVISE:

% % In this section, we provide an overview of the most relevant
% % prior work. After normalizing flows have demonstrated their success in
% % many practical applications, a few papers have studied the
% % representation powers of such models. For discrete normalizing flows,
% % \cite{DisAppro2} studies some basic flow structures including planar
% % flows, radial flows, Sylvester Flows and Householder flows and shows
% % that these basic flows are highly expressive in dimension $d = 1$, but
% % their expressivity can be limited in higher
% % dimensions. \cite{DisAppro1} shows that invertible neural networks
% % based on coupling flows are universal approximators for
% % diffeomorphisms.

% % For neural ODEs, it turns out that the analysis of their representation
% % power is more complicated. It is observed in \cite{Augmented} that due
% % to the fact that the flow maps of ODEs are homeomorphisms, there are
% % some simple functions that can not be exactly represented by
% % neural ODEs, and it is argued there that dimension augmentation by
% % appending zeros to the input vectors might be necessary. Later in
% % \cite{ApproximationCapability}, it is proved that homeomorphism on a
% % $p$-dimensional Euclidean space can be exactly represented by an ODE
% % operating on a $2p$-dimensional Euclidean space, with a vector of $p$
% % zeros being appended to the input vectors. However, for the purpose of
% % \textit{distribution learning}, these results are not very useful
% % because \textit{(i)} the failure in exact representation does not
% % imply the impossibility of approximation and \textit{(ii)}, universal
% % approximation of the distribution does not necessarily need universal
% % function approximation. Indeed, some more recent works do show that
% % neural ODEs are $L^p$-universal approximators for continuous maps under
% % certain technical assumptions (\cite{DynamicalSystem}) and they are
% % $L^\infty$-universal apprxoimators for a large class of
% % diffeomorphisms (\cite{SupApproximation}). Still, these works only
% % focus on the \textit{function approximation} aspect of neural ODEs;
% % moreover, these types of universal-approximation results all take the
% % underlying neural network in the ODE as a black box and there is no
% % quantification of how complex the neural network has to be in order to
% % achieve certain degree of accuracy.

% Another class of generative models related to neural ODEs is called
% Neural stochastic differential equations (NeuralSDEs), which is a SDE
% whose drift term is parametrized by a neural network. Among the
% infinitely many possible drift terms that could push forward the
% source to the target distribution, it is shown in \cite{NeuralSDE}
% that there is a drift named the F\"ollmer drift that achieves exact
% sampling and it is nice in the sense of stochastic control
% theory. Subsequent regularity results about the F\"ollmer drift and
% neural network approximation theories have been developed for
% distribution learning in this setting. Part of our work will be to
% prove similar results in the context of neural ODEs, which is currently
% lacking.

% Regarding the training of neural ODEs as continuous normalizing flows,
% some regularization techniques have already be discussed. It is argued
% in \cite{HowToTrain} that a good way of measuring the regularity of
% the vector field is by the force experienced by a particle $x(t)$
% under the dynamics generated by the vector field $f$, which is the
% total derivative of $f$ in time: 
% \begin{equation}\label{eq:straightlinereg}
%   \frac{df(X,t)}{dt} = \nabla_X f(X,t) \cdot \frac{dX}{dt} + \frac{\partial f(X,t)}{ \partial t} = \nabla_X f(X,t) \cdot f(X,t) + \frac{\partial f(X,t)}{\partial t}.
% \end{equation}
% Observe that when this term is zero the trajectories of the particles
% will be straight lines. Since the Jacobian matrix $\nabla_X f(X,t)$ is
% in general not easily accessible, \cite{HowToTrain} chooses to
% implicitly regularize this term by regularizing $|f(X,t)|^2$ and the
% Frobenius norm $\|\nabla_X f(X,t)\|_F^2$ instead. Similar to
% \cite{ffjord}, stochastic estimation is used in the training to
% compute $\|\nabla_X f\|_F^2$ and $\text{Tr}(\nabla_X f(X,t))$, which
% is used in the change of variables formula for log-likelihood. In a
% more recent work, \cite{OTFlow} adopts a discretize-then-optimize
% approach to training neural ODEs, where a ResNet structure is used to
% implement the underlying neural network in the ODE. This approach
% enables exact computation of the Jacobian matrix as well as its trace
% from the recursive structure of the ResNet. Then, automatic
% differentiation is used to update the parameters in the neural
% network, instead of solving the adjoint equation as in \cite{ffjord}
% and \cite{HowToTrain}.


% we will review some relevant literature 

% adapt/update

% relationship to other work: computational cost of our regularizer; ways of making it cheaper? Relate also to recent flow matching methods (“rectified flow”) that seek straight-line trajectories by other means—i.e., not necessarily through divergence + regularization.



% General question: how do each of these contributions relate to other work in the literature? We will need this context for the intro. Please add notes/comments above on this point.

% For approximation results,  I think the existing approximation results for neural ODEs don't provide explicit rates (how the network parameters have to scale to achieve $\epsilon$ accuracy in the distribution). There’s a work that shows neuralODEs are not universal approximators and hence need the ‘augmented’ version.  And there’s a few other works that show they are universal approximators for diffeomorphisms (and hence can approximate distributions through measure transport), but again no error rates are given. Also, most of these works emphasize on approximating functions, not distributions. For this part, I think the old introduction I wrote already covered a number of relevant works. Agreed. We also wrote some of this stuff in the statneuralODE paper, and can paraphrase it here.

% Regularization: the idea of regularizing to get ‘straight-line’ trajectories first started with the ‘how to train’ paper and the ‘OT flow’ paper. There is also the new ‘flow fast and straight’ paper and some subsequent follow up works. I  think we need to compare/contrast our regularizer with theirs. Yes, let’s compare our regularizer with the various other existing choices. The flow-straight-and-fast paper might be a bit different, though; it seems essentially like flow matching, and not so much a regularized neural ODE. To make that more precise: the training objective (and regularization) may be different, but the resulting velocity fields might indeed be straight-line (or close to straight-line) velocity fields like ours. Then the question is (for that straight-and-fast formulation): to what transport map T do these straight-line velocities correspond? Mention the contrast in the intro. Our approximation results are relevant, but our way of getting the straight-line velocity field is different.

% % I think the first two points are probably what the ML community cares about the most. Other than that, there are many technicalities in the paper that may also be worth a discussion?  For example, the bound norm on KR map in terms of the densities, our use of Faadi bruno in abstract function spaces, our treatment of the domains (extension of Lipschitz domain, boundary condition etc.). Each of these point may be of independent interest? 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \hline
% OLD
 
% Sampling from an arbitrary probability distribution is one of the central problems of computational statistics. A useful approach to this problem is through transportation of measure, using deterministic couplings \citep{villani}: construct an invertible function that \emph{pushes forward} the target probability distribution to a relatively simple source distribution (usually chosen to be uniform or standard Gaussian). One can then simulate from the target distribution by drawing samples from the source distribution and evaluating the inverse of the transport map; this construction is useful for both generative modeling \citep{genmodeling} and variational inference \citep{moselhy2012,rezendemohamed}. Alternatively, one can form an estimate of the \emph{density} of the target measure by evaluating the pullback density of the source distribution through the transport map.
 
% Many parameterizations, ranging from polynomials to radial basis functions to input-convex neural networks.

% In the machine learning literature, normalizing flows \citep{Tabak,NormalizingFlowIntro} construct transport maps by composing a sequence of relatively simple structured invertible functions; again, the goal is to push forward the target distribution to an easy-to-sample distribution. For such methods, it is typically required that the maps be invertible and the determinant of the Jacobian be easily computable, as evaluating pushforward or pullback densities follows from the change-of-variables formula. To satisfy these requirements, various architectures for normalizing flows have been proposed, including planar and radial flows \citep{PlanarFlow}, affine coupling flows \citep{CouplingFlows}, autoregressive flows \citep{autoregressiveflow}, and neural autoregressive flows \citep{NeuralAutoFlow}.

% In parallel, recent work in deep learning has elucidated connections between differential equations and neural networks \citep{TransportAnalysisofDL,NumericalSchemeODE,PDEmotivatedNN}, which led to the idea of neural ordinary differential equations (NeuralODEs) \citep{NeuralODE}. More recently, neural ODEs have been proposed to perform generative modeling tasks \citep{ffjord}. The generative process is as follows: Denote by
%     $\pi$ the target distribution from which we wish to sample and by
%     $\rho$ the source distribution.
%     % . Typically, $\pi$ is
%     % either known through an iid sample or through its (unnormalized)
%     % density, see Sec.~\ref{sec:setup} ahead. For a reference
%     % distribution $\rho$,
%     Given $x\sim\rho$, the following initial value problem
%     % to get the time-one solution $x(1)$ for the ODE:
%     \begin{equation}\label{eq:ODE}
%       \begin{aligned}
%         % \frac{dX(x,t)}{dt}
%         \frac{d X(x,t)}{dt} &= f(X(x,t), t),\\
%         X(x, 0) &= x,
%       \end{aligned}
%     \end{equation}
%     is solved up to time $t=1$. The goal is to learn a Lipschitz continuous velocity
%     field $f$, which is parameterized as a neural network, such that
%     $x\sim \rho$ implies $X(x,1)\sim \pi$.
    
%     % \jztd{Before it said
%     % $f(x,t,\theta_t)$. I guess $\theta_t$ was a typo? In any case,
%     % $\theta$ didn't occur anymore so I removed it.}  \jztd{Sven made a
%     % good comment in the group meeting: We should assume throughout
%     % that $f$ is Lipschitz continuous so that \eqref{eq:ODE} always has
%     % a solution.}

%   % If the neural network $f$ is properly learned, then the points
%   % $x(1)$ will be approximately sampled from the distribution
%   % $\rho(x)$.
%   In the context of distribution learning,
%   neural ODEs % as continuous normalizing flows
%   enjoy % many advantages.
%   several desirable properties: 
%   Invertibility of $x\mapsto X(x,1)$ is guaranteed by design,
%     % as one can always integrate it backwards in time,
%     as one can always solve \eqref{eq:ODE} backward in time. Thus,
%   in contrast to other methods such as invertible neural
%       networks (\cite{InvNN}), normalizing flows
%       (\cite{NormalizingFlowIntro}) or other transport maps
%       (\cite{measure-transport}), no further restrictions need to be
%     imposed on the vector field $f$ that is to be learned.
%   % there is no limitations on the form the ODE dynamics (or the
%   % neural networks) need to take. In addition, The change
%   In addition, the density $\eta(x,t)$ % under neural ODE
%   of $X(x,t)$ (with $x\sim\rho$) can easily be computed, as it
%     follows the dynamics
%   $\frac{\partial \log \eta(x,t)}{dt} = -\tr(\nabla_Xf(X(x,t), t))$, which is known as the
%     \emph{instantaneous change of variables formula}, see
%     \cite{NeuralODE}.
%   % another ODE, called the instantaneous change of variables formula
%   % (\cite{NeuralODE}):
%   % $\frac{\partial \log p(x(t))}{dt} = -\text{Tr}\left(\frac{\partial
%   %   f}{\partial x(t)}\right)$.

%   In regards to the usefulness of neural ODEs as generative models,
%   there are three questions that are natural to ask.
%   First, % note that
%   it is known that when the source and target measures are
%   well-behaved (for example, both absolutely continuous with respect
%   to the Lebesgue measure), there are in general infinitely many
%   transport maps that push forward one measure onto the other. 
%   Even if we require the time-one flow map $x\mapsto X(x,1)$ to
%   be a particular transport map $T$, there are in general still
%     many possible velocity fields $f$ realizing $T$.
%   % still infinitely many ways how points from the source measure,
%   % $x$, can be connected to points from the target measure, $T(x)$,
%   % through the ODE trajectories.
%   It is observed in \cite{OTFlow} and \cite{HowToTrain} that without
%   any form of regularization, the trajectories of the ODE dynamics
%   connecting $x$ to $T(x)$ may be very
%   irregular. % Therefore, % the first question that
%   Therefore it is natural to ask how we can regularize the
%     training objective to improve the training process.
%   % so that the velocity fields we learn in the end is nice in an
%   % appropriate sense.
%   Secondly, with the regularized training objective, we would like to
%   understand the structures of its optimal solutions and quantify the
%   neural network approximator for them. The third question to ask,
%   then, is how can we bound the distance between the target measure
%   and the push-forward measure of the source under the flow map of the
%   neural ODE when we substitute in the neural network as an
%   approximation for the velocity field? Our work is the first attempt
%   to address these questions in a unified way. In the remainder
%   of this section, we will review some relevant literature and
%   summarize our main contributions.


% \subsection{Prior and Related Work}
% In this section, we provide an overview of the most relevant
% prior work. After normalizing flows have demonstrated their success in
% many practical applications, a few papers have studied the
% representation powers of such models. For discrete normalizing flows,
% \cite{DisAppro2} studies some basic flow structures including planar
% flows, radial flows, Sylvester Flows and Householder flows and shows
% that these basic flows are highly expressive in dimension $d = 1$, but
% their expressivity can be limited in higher
% dimensions. \cite{DisAppro1} shows that invertible neural networks
% based on coupling flows are universal approximators for
% diffeomorphisms.

% For neural ODEs, it turns out that the analysis of their representation
% power is more complicated. It is observed in \cite{Augmented} that due
% to the fact that the flow maps of ODEs are homeomorphisms, there are
% some simple functions that can not be exactly represented by
% neural ODEs, and it is argued there that dimension augmentation by
% appending zeros to the input vectors might be necessary. Later in
% \cite{ApproximationCapability}, it is proved that homeomorphism on a
% $p$-dimensional Euclidean space can be exactly represented by an ODE
% operating on a $2p$-dimensional Euclidean space, with a vector of $p$
% zeros being appended to the input vectors. However, for the purpose of
% \textit{distribution learning}, these results are not very useful
% because \textit{(i)} the failure in exact representation does not
% imply the impossibility of approximation and \textit{(ii)}, universal
% approximation of the distribution does not necessarily need universal
% function approximation. Indeed, some more recent works do show that
% neural ODEs are $L^p$-universal approximators for continuous maps under
% certain technical assumptions (\cite{DynamicalSystem}) and they are
% $L^\infty$-universal apprxoimators for a large class of
% diffeomorphisms (\cite{SupApproximation}). Still, these works only
% focus on the \textit{function approximation} aspect of neural ODEs;
% moreover, these types of universal-approximation results all take the
% underlying neural network in the ODE as a black box and there is no
% quantification of how complex the neural network has to be in order to
% achieve certain degree of accuracy.

% Another class of generative models related to neural ODEs is called
% Neural stochastic differential equations (NeuralSDEs), which is a SDE
% whose drift term is parametrized by a neural network. Among the
% infinitely many possible drift terms that could push forward the
% source to the target distribution, it is shown in \cite{NeuralSDE}
% that there is a drift named the F\"ollmer drift that achieves exact
% sampling and it is nice in the sense of stochastic control
% theory. Subsequent regularity results about the F\"ollmer drift and
% neural network approximation theories have been developed for
% distribution learning in this setting. Part of our work will be to
% prove similar results in the context of neural ODEs, which is currently
% lacking.

% Regarding the training of neural ODEs as continuous normalizing flows,
% some regularization techniques have already be discussed. It is argued
% in \cite{HowToTrain} that a good way of measuring the regularity of
% the vector field is by the force experienced by a particle $x(t)$
% under the dynamics generated by the vector field $f$, which is the
% total derivative of $f$ in time: 
% \begin{equation}\label{eq:straightlinereg}
%   \frac{df(X,t)}{dt} = \nabla_X f(X,t) \cdot \frac{dX}{dt} + \frac{\partial f(X,t)}{ \partial t} = \nabla_X f(X,t) \cdot f(X,t) + \frac{\partial f(X,t)}{\partial t}.
% \end{equation}
% Observe that when this term is zero the trajectories of the particles
% will be straight lines. Since the Jacobian matrix $\nabla_X f(X,t)$ is
% in general not easily accessible, \cite{HowToTrain} chooses to
% implicitly regularize this term by regularizing $|f(X,t)|^2$ and the
% Frobenius norm $\|\nabla_X f(X,t)\|_F^2$ instead. Similar to
% \cite{ffjord}, stochastic estimation is used in the training to
% compute $\|\nabla_X f\|_F^2$ and $\text{Tr}(\nabla_X f(X,t))$, which
% is used in the change of variables formula for log-likelihood. In a
% more recent work, \cite{OTFlow} adopts a discretize-then-optimize
% approach to training neural ODEs, where a ResNet structure is used to
% implement the underlying neural network in the ODE. This approach
% enables exact computation of the Jacobian matrix as well as its trace
% from the recursive structure of the ResNet. Then, automatic
% differentiation is used to update the parameters in the neural
% network, instead of solving the adjoint equation as in \cite{ffjord}
% and \cite{HowToTrain}.


% \subsection{Contribution\jz{s}}
% Our contributions in this work have both algorithmic and theoretical
% aspects.
% \begin{itemize}
% \item Algorithmically, we suggest to use the
%   discretize-then-optimize approach of \cite{OTFlow}, which allows the
%   Jacobian to be computed exactly and thus allows to regularize the term \eqref{eq:straightlinereg}
%   directly instead of regularizing $\|f\|^2$ and $\|\nabla_xf\|^2_F$
%   as in \cite{HowToTrain}.


% \item $\mathcal{D}$
    
% \item Under reasonable assumptions, we show that the velocity fields
%   that achieve this optimal value of the objective must take the
%   time-independent form (\rr{in the Lagrangian sense})
%   \rr{$f(tT(x) + (1-t)x,t) = T(x) - x$} for some transport map $T$
%   that pushes forward one measure to the other. This implies
%   that $x$ is connected to $T(x)$ by straight line trajectories. We
%   also characterize the type of transport maps that can be represented
%   by ODEs with straight line trajectories and show that for these
%   transport maps $T$, among the infinitely many velocity fields that
%   connect $x$ to $T(x)$, this straight line construction gives the
%   minimal kinetic energy. Thus we call expression
%   \eqref{eq:straightlinereg} the minimal energy regularization.
    
% \item Since the velocity field is constant along the trajectories, the
%   optimal velocity fields $f$ are implicitly determined by the
%   relationship $f(tT(x) + (1-t)x, t) = T(x) - x$ based on transport
%   maps $T$. Theoretically, we show that under certain conditions, the
%   domain covered by these straight line trajectories, which is the
%   domain where the velocity field $f$ is defined, is a Lipschitz
%   domain. Therefore, we can extend this velocity field continuously to
%   a larger hypercube so that we may apply existing neural network
%   approximation results.
    
% \item We derive upper bounds for the $C^k$ norm of the velocity field
%   $f$ based on the $C^k$ norm of the transport map $T$. This proof is
%   based on a multivariate version of Fa\'{a} di Bruno's formula and
%   interpreting partial derivatives as multi-linear tensor operators in
%   abstract Banach spaces. We believe this proof is of independent
%   interest, as this type of estimates also occurs in numerical
%   analysis and partial differential equations. Combining this $C^k$
%   norm for the velocity field with the knowledge about the domain of
%   $f$ from previous part, we establish bounds on the depth and number
%   of neurons of neural networks to achieve $\epsilon$-approximation
%   of the velocity field $f$.
    
% \item We establish bounds on the difference between the target measure
%   and the push-forward measure of the source by the time-one flow map
%   of the neural ODE. This difference is measured by different metrics
%   that include $L^2$, KL, and Wasserstein. \rrtd{this may also need to be revised, depending on what metrics we eventually choose to use.}
    
% \item Besides the general case, we also specialize our results to one
%   particular class of transport maps, namely the triangular transport, where the regularity bounds can be derived in terms of the densities of the source and target measures.
    
    
% \end{itemize}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NeuralODE-main.tex"
%%% End:        