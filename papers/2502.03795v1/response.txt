Our approximation results for neural ODEs as continuous normalizing flows are relevant, but our way of getting the straight-line velocity field is different from the "flow-straight-and-fast" formulation. In that formulation, the training objective (and regularization) may be different, but the resulting velocity fields might indeed be straight-line (or close to straight-line) velocity fields like ours.

In this context, the question is (for that straight-and-fast formulation): to what transport map T do these straight-line velocities correspond? Mention the contrast in the intro. 

Our work addresses three questions in a unified way: regularizing the training objective to improve the training process, understanding the structures of its optimal solutions and quantifying the neural network approximator for them, and bounding the distance between the target measure and the push-forward measure of the source under the flow map of the neural ODE.

We review some relevant literature on normalizing flows (____), invertible neural networks (____), and other transport maps (____). Our approach is based on the instantaneous change of variables formula (____) which states that $\frac{\partial \log \eta(x,t)}{dt} = -\tr(\nabla_Xf(X(x,t), t))$. 

The goal of our work is to learn a Lipschitz continuous velocity field $f$ that pushes forward the source distribution to the target distribution. We propose a regularized training objective and provide approximation results for neural ODEs as continuous normalizing flows.

Our regularizer is based on the idea of getting 'straight-line' trajectories, which first started with the ‘how to train’ paper (____) and the ‘OT flow’ paper (____). There is also the new ‘flow fast and straight’ paper (____) and some subsequent follow up works. We compare our regularizer with theirs.

In addition, we provide a bound on the distance between the target measure and the push-forward measure of the source under the flow map of the neural ODE when substituting in the neural network as an approximation for the velocity field. Our results are relevant to many parameterizations, ranging from polynomials (____) to radial basis functions (____) to input-convex neural networks (____).

The change-of-variables formula is used to compute the density of $X(x,t)$ with $x\sim\rho$. In addition, we provide a bound on the distance between the target measure and the push-forward measure of the source under the flow map of the neural ODE when substituting in the neural network as an approximation for the velocity field.

In our regularized training objective, we impose that the time-one solution $X(x,1)$ of \eqref{eq:ODE} is a particular transport map $T$. There are many possible velocity fields $f$ realizing $T$, and without any form of regularization, the trajectories of the ODE dynamics connecting $x$ to $T(x)$ may be very irregular (____).

In the context of distribution learning, neural ODEs enjoy several desirable properties: invertibility of $x\mapsto X(x,1)$ is guaranteed by design, as one can always solve \eqref{eq:ODE} backward in time. Thus, in contrast to other methods such as invertible neural networks (____), normalizing flows (____) or other transport maps (____), no further restrictions need to be imposed on the vector field $f$ that is to be learned.

In our regularized training objective, we impose a bound norm on the KR map in terms of the densities. We also use Faadi bruno in abstract function spaces and treat the domains (extension of Lipschitz domain, boundary condition etc.). Each of these points may be of independent interest.