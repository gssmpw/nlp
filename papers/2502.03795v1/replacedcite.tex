\section{Related work}
% Approximation power of discrete normalizing flows. For function approximation: stuff. For distribution learning: stuff. Just universal approximation. 

Several papers have studied the approximation power of discrete normalizing flows. ____ investigate basic flow structures (e.g., planar flows, radial flows, Sylvester flows, Householder flows) for $L^1$ approximation of a target density on $\mathbb{R}^d$. The authors establish a universal approximation result for $d=1$, but show partially negative results for $d>1$: there exist distributions that cannot be exactly coupled by such flows, and there are other distributions for which accurate approximation requires composing together a prohibitive number of layers. ____, on the other hand, show that normalizing flows based on the so-called ``affine coupling'' construction are universal approximators of diffeomorphisms, and consequently that their pushforward distributions converge weakly to any desired target as the complexity of the flow increases in suitable way. Additional universal approximation results have been developed for more specific flow architectures ____. However, none of these works characterizes the \emph{rate} of convergence of the approximation---i.e., how the distribution approximation error that can be achieved scales with the size of the model.

For triangular (Knothe--Rosenblatt) transport maps on $[0,1]^d$, ____ develop a complete approximation theory under the assumption of analytic source and target densities, for neural network or sparse polynomial representations of the map. These results encompass approximation of the maps themselves, but also distribution approximation via the pushforward of a given source distribution by the approximate map. ____ extend this analysis to triangular transport maps in infinite dimensions, i.e., on $[0,1]^\infty$. ____ provide a general framework for analyzing the error of variational approximations of distributions realized using transport maps, and shows how this framework can be applied in specific situations to derive approximation rates.

The preceding works considered approximation of the transport map itself, i.e., direct representations of the `displacement' of points from the support of the source to the support of the target. In contrast, other recent efforts have addressed approximation issues in \textit{dynamic} representations of transport. Most of these have focused on neural ODEs. 
%    \item Very briefly review function approximation results for neural ODEs. But then note how function approximation results are not necessarily relevant to distribution learning.
____ show that neural ODEs are universal approximators of smooth diffeomorphisms on $\mathbb{R}^d$ in appropriate Sobolev norms. ____ adapt ideas from dynamical systems to show that neural ODEs are universal approximators of continuous functions from $\mathbb{R}^d$ to $\mathbb{R}^m$ (hence, not only diffeomorphisms) in a $L^2$ sense, for $d \geq 2$. Yet these universal approximation results do not characterize approximation \emph{rates}, e.g., relate bounds on function approximation error to the size of the network representing the velocity field. Moreover, we note that universal {function} approximation results are not necessarily relevant to {distribution} learning: that is, universal approximation of distributions does not require universal function approximation.

% universal approximation of functions is \emph{not} necessary for universal approximation of distributions. \ymmtd{Moreover, universal approximation of distributions does not require universal function approximation.}

% Additionally, we note that universal \textit{function} approximation results are not necessarily relevant to \textit{distribution} learning. For instance, it is observed in ____ that certain simple maps can not be exactly represented by (neural) ODEs, but failure of exact representation does not imply the impossibility of approximation; moreover, universal approximation of distributions does not require universal function approximation. \ymmtd{Tighten this up; it's digressive.}
    
    % \item Distribution approximation with neural ODEs: the only work we are aware of is Ruiz-Balet. Go into some details. No smoothness in their analysis. Construction is quite different; no straight-line paths, very different training objective (if any).

____ study distribution approximation with neural ODEs, and prove universal approximation for certain target distributions in Wasserstein-1 distance, using an approach that is discrete and constructive. Specifically, they analyze neural ODE-type models from a controllability perspective, explicitly constructing piecewie constant approximations of the target density using a neural network velocity field with ReLU activations. Their analysis does not consider higher-order smoothness, however, and their velocity construction is different from that considered here. In subsequent work, ____ show that ReLU velocity fields chosen to be piecewise constant in time can approximate a target distribution arbitrarily closely in relative entropy. 
 
    % \item Regularizing neural ODEs: Oberman, Onken, and minimum curvature papers. Relevant to our minimum energy regularization scheme.

As mentioned earlier, several other works identify neural ODE velocity fields via a \textit{regularized} training objective, i.e., by minimizing a linear combination of a statistical divergence (or negative log-likelihood) and some regularization term.
%
____ argue that a good way of measuring the regularity of
the velocity field is through the ``acceleration'' experienced by a ``particle'' $X^f(x,t)$ starting at some point $x$. This acceleration is the total derivative of $f$ in time: 
\begin{equation}\label{eq:straightlinereg}
  \frac{ D f(X,t)}{D t} = \nabla_X f(X,t) \cdot \frac{ \partial X}{ \partial t} + \frac{\partial f(X,t)}{ \partial t} = \nabla_X f(X,t) \cdot f(X,t) + \frac{\partial f(X,t)}{\partial t}.
\end{equation}
When this term is zero, particle trajectories will be straight lines. Since the Jacobian matrix $\nabla_X f(X,t)$ is
in general not easily accessible, ____ choose to
implicitly penalize this term by penalizing $|f(X,t)|^2$ and the
Frobenius norm $\|\nabla_X f(X,t)\|_F^2$ instead. Similar to
____, stochastic methods are used  to
estimate $\|\nabla_X f\|_F^2$ and $\tr(\nabla_X f(X,t))$ in training, where the latter is used in the change of variables formula for the log-likelihood. In more recent work, ____ propose a discretize-then-optimize
approach to training neural ODEs, where a ResNet structure is used to
implement the underlying neural network in the ODE. This approach
enables exact computation of the Jacobian matrix as well as its trace
from the recursive structure of the ResNet. Then, automatic
differentiation is used to update the parameters in the neural
network, instead of solving the adjoint equation as in ____
and ____. By adopting this discretize-then-optimize approach, we propose to penalize \eqref{eq:straightlinereg} directly. We will show that the velocity field $f$ making this term zero is the unique velocity field that yields the minimal kinetic energy among all velocity fields $f$ that produce the same transport map $T$ at time one; hence it is termed the \textit{minimal energy regularization}. 


    % Stochastic interpolants, rectified flow, and flow matching: these also produce neural ODEs, but with a different training scheme---essentially regression for a specific velocity field, rather than maximum likelihood estimation or (in the infinite-sample limit)  minimization of a statistical divergence. But the same velocity approximation questions and distributional stability questions, are present in that setting as well. The approximation results we develop in the present paper are independent of training scheme, and in particular, the straight-line velocity fields we analyze here are central to rectified flow.

While the neural ODE framework learns the velocity field by minimizing the KL divergence from the target distribution to the pushforward distribution of the source under ODE flow maps, another recent line of research  aims to specify the velocity field \textit{a priori} using conditional expectations and to learn the velocity field directly via least-squares regression ____. However, the same velocity field approximation questions and distributional stability questions are present in that setting as well. We note that the approximation results we develop in this work are independent of training scheme, and in particular, the straight-line velocity fields we analyze here are central to the rectified flows proposed in ____. 



    

    % \item (this item is a bit more detailed) There are a variety of related \textit{distributional stability} results in recent literature (cf.\ our Section~\ref{sec:DistApproximation}), addressing the question of how error in the pushforward distribution under an ODE flow map (in some distance or divergence) is controlled by error in the velocity field (in some norm).
    % \begin{itemize}
    %     \item Our results:  Wp error in the distribution, for $p \in [1, \infty]$, is controlled by space-time $L^\infty$ (i.e., $C^0$) error and spatial Lipschitz constant of $f$, on compact domains. Do we then control this Lipschitz constant? Yes. Also our results: KL controlled by $C^1$ norm of $f$, on compact domains.

    %     \item Benton, Theorem 1: W2 error in distribution controlled by $L^2$ approximation error and averaged spatial Lipschitz constant of the approximate flow. Then see below for bounding this constant.

    %     \item Gao, Lemma 4.2: Almost exactly like Benton result: W2 controlled by $L^2$ error in velocity times exponential of spatial Lipschitz constant.

    %     \item Li 2024: Analyze a discrete-time version of the probability flow ODE. Bounds TV distance to true distribution by terms involving the $L^2$ error in the score function and in its Jacobian. [https://arxiv.org/pdf/2408.02320]
        
    %     \item Huang 2024: Probability flow ODE, starting in continuous time, then considering discretization. Early stopping. Assumes $L^2$ accurate score estimate and bounds on first two derivatives. Then bounds TV distance to true dist. [https://arxiv.org/pdf/2404.09730]
        
    % \end{itemize}

Indeed, there are a variety of  related \textit{distributional stability} results in recent literature (cf.\ Section~\ref{sec:DistApproximation}), addressing the question of how error in the pushforward distribution under an ODE flow map (in some distance or divergence) is controlled by error in the velocity field (in some norm). ____ show that $W_2$ error in distribution is controlled by $L^2$ approximation error of the true velocity field and the time-averaged spatial Lipschitz constant of the approximate flow. In a study of the convergence of continuous normalizing flows, ____ obtain a stability result almost exactly the same as that in ____, where $W_2$ error in distribution is controlled by $L^2$ error in velocity field times the exponential of spatial Lipschitz constant of the velocity field.
    ____ analyze a discrete-time version of the probability flow ODE, where TV error in distribution is bounded by terms involving the $L^2$ error in the score function and in its Jacobian. Again in the setting of probability flow ODEs, ____ start in continuous time and then considers discretization using a Runge--Kutta scheme. At the continuous level, it is shown that TV error in distribution is controlled by 
    $L^2$ error in the approximation of the score function and
    the first and second
derivatives of the estimated score; at the discrete level, it is shown that the $p$-th order integrator also converges under an additional assumption that the estimated score function’s first $p + 1$ derivatives are bounded. In our work, we show that $W^p$ error in the distribution, for $p \in [1, \infty]$, is controlled by the space-time $L^\infty$ (i.e., $C^0$) error and spatial Lipschitz constant of $f$, on compact domains; these are further related to properties of the densities. In addition, we obtain that distribution approximation error in KL is controlled by the $C^1$ norm of $f$, again on compact domains.
    
    
    
    

    % \item (this item is a bit more detailed) There are also some results linking properties of the velocity field (e.g., Lipschitz constant in space or time) to properties of the underlying densities
    % \begin{itemize}
    %     \item Benton, Thm 2: Bound time-averaged spatial Lipschitz constant under some assumptions on the regularity of all the distributions between $t=0$ and $t=1$, along with some Gaussian smoothing; upper bound depends on properties of the chosen interpolant. Bounded domains. Regularity assumption is rather different than the $C^k$ smoothness we assume here. No higher-order smoothness of the velocity field.
        
    %     \item Gao: Focuses on flow matching with *linear* interpolation. They say: ``We derive regularity properties for the velocity field of the CNF with linear interpolation. We demonstrate the Lipschitz regularity properties of the velocity field in both the space and time variables and establish bounds for the Lipschitz constants. We also show that the velocity field grows at most linearly with respect to the space variable.'' Main result is Thm 5.1. No Gaussian smoothing, but requires early stopping. [Yuan Gao, Jian Huang, Yuling Jiao, and Shurong Zheng. Convergence of continuous normalizing flows for learning probability distributions. arXiv:2404.00551, 2024.]
        
    %     \item Benton and Gao: both specific to certain stochastic interpolants, which are different than the straight-line ansatz we analyze here. They do not consider higher-order smoothness of the velocity field. No explicit upper bounds.

    % \end{itemize}

There are also some results linking properties of the velocity field (e.g., Lipschitz constant in space or time) to properties of the underlying densities. In ____, the time-averaged spatial Lipschitz constant is related to assumptions on the regularity of all the intermediate distributions between $t=0$ and $t=1$, along with some Gaussian smoothing; an upper bound is obtained that depends on properties of the chosen interpolant. We note that their regularity assumption is rather different than the $C^k$ smoothness we assume here, as they do not consider higher-order smoothness of the velocity field. ____ focus on flow matching with linear interpolation. It is shown that the Lipschitz constant of the target velocity field in both the space and time variables is bounded under assumptions of log-concavity/convexity of distributions and boundedness of the domain. In addition, they show that the velocity field itself grows at most linearly with respect to the space variable. No Gaussian smoothing is used in their setting, but they require an early stopping before reaching time $t = 1$. Both ____ and ____ are specific to certain stochastic interpolants, which are different than the straight-line ansatz we analyze here. Also, they do not consider higher-order smoothness of the velocity field or derive upper bounds that are explicit in the densities. 
    
    
    
    
    
    
    
    


    % \item (this item is a bit more detailed) Results on neural network approximation rates. Gao et al.\ specifically consider flow matching with linear interpolation. Their assumptions on the source and target distributions do not entail higher-order smoothness. They construct NN classes that capture the resulting Lipschitz properties of the velocity field, and derive rates of approximation in the $L^\infty$ sense (or maybe in the $W^{1, \infty}$ sense? Need to check).

With regard to neural network approximation results, ____
    % \ymmtd{this part seemed repetitive}
    % specifically consider flow matching with linear interpolation and their assumptions on the source and target distributions do not entail higher-order smoothness. They 
    construct neural network classes that capture the  Lipschitz properties of the velocity field, and derive rates of approximation in the $L^\infty$ sense. Our companion paper ____ derives explicit neural network approximation rates for general $C^k$ velocity fields, but its main focus is on statistical finite sample guarantees for neural ODEs trained through likelihood maximization, different from our focus here. 
    

  
    
    % \item There are also certainly SDE and specifically neural SDE methods for distribution learning (cite a few). Rather different than the deterministic ODE approach, and not our focus here.

There are also stochastic differential equation (SDE) and specifically neural SDE methods for distribution learning ____. However, they are rather different than the deterministic ODE approach, and again are not the focus of this work.
    







% - Hong Kong group's flow matching papers: no assumptions on higher-order smoothness of densities. Velocity field only controlling maybe Lipschitz constants?


% - distributional stability: only Wasserstein in the cited works? Also, they don't explicitly deal with neural networks.

%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%


% From statneuralODE paper:

% Questions of \emph{function approximation} with neural ODEs have been studied in ____. ____ show that neural ODEs are univeral approximators of smooth diffeomorphisms on $\mathbb{R}^d$ in appropriate Sobolev norms. ____ adapts ideas from dynamical systems to show that neural ODEs are universal approximators of continuous functions from $\mathbb{R}^d$ to $\mathbb{R}^m$ (hence, not only diffeomorphisms) in a $L^2$ sense, for $d \geq 2$. Both papers compose the flow map of the ODE with a terminal mapping, meant to represent a classification or regression layer.
% Yet these universal approximation results do not characterize approximation \emph{rates}, e.g., relating bounds on an approximation error to the size of the network representing the velocity field. Function approximation is also different than our present focus of statistical recovery guarantees.

% The results on diffeomorphism approximation in ____ do translate to universal approximation of certain classes of distributions, in a weak sense and in total variation. ____ also proves universal approximation for certain target distributions, in Wasserstein-1 distance, using a rather different approach that is discrete and constructive. 
% Our companion paper ____, in contrast, establishes approximation \textit{rates} for neural ODE representations of distributions with $C^k$-smooth densities, and shows that there exist neural network representations of the velocity field $f$, with size explicitly bounded in terms of the regularity of the densities, that achieve efficient approximation.  
% There has also been relevant work on approximation theory for transport maps that are not constructed via ODEs. For example, ____ investigate sparse polynomial and neural network approximations of triangular (Knothe--Rosenblatt) maps, formulating \textit{a priori} descriptions of an ansatz space that achieves exponential convergence in the case of analytic densities. A broader framework for understanding the distributional errors of transport map approximations is proposed in ____.




% TO REVISE:

% % In this section, we provide an overview of the most relevant
% % prior work. After normalizing flows have demonstrated their success in
% % many practical applications, a few papers have studied the
% % representation powers of such models. For discrete normalizing flows,
% % ____ studies some basic flow structures including planar
% % flows, radial flows, Sylvester Flows and Householder flows and shows
% % that these basic flows are highly expressive in dimension $d = 1$, but
% % their expressivity can be limited in higher
% % dimensions. ____ shows that invertible neural networks
% % based on coupling flows are universal approximators for
% % diffeomorphisms.

% % For neural ODEs, it turns out that the analysis of their representation
% % power is more complicated. It is observed in ____ that due
% % to the fact that the flow maps of ODEs are homeomorphisms, there are
% % some simple functions that can not be exactly represented by
% % neural ODEs, and it is argued there that dimension augmentation by
% % appending zeros to the input vectors might be necessary. Later in
% % ____, it is proved that homeomorphism on a
% % $p$-dimensional Euclidean space can be exactly represented by an ODE
% % operating on a $2p$-dimensional Euclidean space, with a vector of $p$
% % zeros being appended to the input vectors. However, for the purpose of
% % \textit{distribution learning}, these results are not very useful
% % because \textit{(i)} the failure in exact representation does not
% % imply the impossibility of approximation and \textit{(ii)}, universal
% % approximation of the distribution does not necessarily need universal
% % function approximation. Indeed, some more recent works do show that
% % neural ODEs are $L^p$-universal approximators for continuous maps under
% % certain technical assumptions (____) and they are
% % $L^\infty$-universal apprxoimators for a large class of
% % diffeomorphisms (____). Still, these works only
% % focus on the \textit{function approximation} aspect of neural ODEs;
% % moreover, these types of universal-approximation results all take the
% % underlying neural network in the ODE as a black box and there is no
% % quantification of how complex the neural network has to be in order to
% % achieve certain degree of accuracy.

% Another class of generative models related to neural ODEs is called
% Neural stochastic differential equations (NeuralSDEs), which is a SDE
% whose drift term is parametrized by a neural network. Among the
% infinitely many possible drift terms that could push forward the
% source to the target distribution, it is shown in ____
% that there is a drift named the F\"ollmer drift that achieves exact
% sampling and it is nice in the sense of stochastic control
% theory. Subsequent regularity results about the F\"ollmer drift and
% neural network approximation theories have been developed for
% distribution learning in this setting. Part of our work will be to
% prove similar results in the context of neural ODEs, which is currently
% lacking.

% Regarding the training of neural ODEs as continuous normalizing flows,
% some regularization techniques have already be discussed. It is argued
% in ____ that a good way of measuring the regularity of
% the vector field is by the force experienced by a particle $x(t)$
% under the dynamics generated by the vector field $f$, which is the
% total derivative of $f$ in time: 
% \begin{equation}\label{eq:straightlinereg}
%   \frac{df(X,t)}{dt} = \nabla_X f(X,t) \cdot \frac{dX}{dt} + \frac{\partial f(X,t)}{ \partial t} = \nabla_X f(X,t) \cdot f(X,t) + \frac{\partial f(X,t)}{\partial t}.
% \end{equation}
% Observe that when this term is zero the trajectories of the particles
% will be straight lines. Since the Jacobian matrix $\nabla_X f(X,t)$ is
% in general not easily accessible, ____ chooses to
% implicitly regularize this term by regularizing $|f(X,t)|^2$ and the
% Frobenius norm $\|\nabla_X f(X,t)\|_F^2$ instead. Similar to
% ____, stochastic estimation is used in the training to
% compute $\|\nabla_X f\|_F^2$ and $\text{Tr}(\nabla_X f(X,t))$, which
% is used in the change of variables formula for log-likelihood. In a
% more recent work, ____ adopts a discretize-then-optimize
% approach to training neural ODEs, where a ResNet structure is used to
% implement the underlying neural network in the ODE. This approach
% enables exact computation of the Jacobian matrix as well as its trace
% from the recursive structure of the ResNet. Then, automatic
% differentiation is used to update the parameters in the neural
% network, instead of solving the adjoint equation as in ____
% and ____.


% we will review some relevant literature 

% adapt/update

% relationship to other work: computational cost of our regularizer; ways of making it cheaper? Relate also to recent flow matching methods (“rectified flow”) that seek straight-line trajectories by other means—i.e., not necessarily through divergence + regularization.



% General question: how do each of these contributions relate to other work in the literature? We will need this context for the intro. Please add notes/comments above on this point.

% For approximation results,  I think the existing approximation results for neural ODEs don't provide explicit rates (how the network parameters have to scale to achieve $\epsilon$ accuracy in the distribution). There’s a work that shows neuralODEs are not universal approximators and hence need the ‘augmented’ version.  And there’s a few other works that show they are universal approximators for diffeomorphisms (and hence can approximate distributions through measure transport), but again no error rates are given. Also, most of these works emphasize on approximating functions, not distributions. For this part, I think the old introduction I wrote already covered a number of relevant works. Agreed. We also wrote some of this stuff in the statneuralODE paper, and can paraphrase it here.

% Regularization: the idea of regularizing to get ‘straight-line’ trajectories first started with the ‘how to train’ paper and the ‘OT flow’ paper. There is also the new ‘flow fast and straight’ paper and some subsequent follow up works. I  think we need to compare/contrast our regularizer with theirs. Yes, let’s compare our regularizer with the various other existing choices. The flow-straight-and-fast paper might be a bit different, though; it seems essentially like flow matching, and not so much a regularized neural ODE. To make that more precise: the training objective (and regularization) may be different, but the resulting velocity fields might indeed be straight-line (or close to straight-line) velocity fields like ours. Then the question is (for that straight-and-fast formulation): to what transport map T do these straight-line velocities correspond? Mention the contrast in the intro. Our approximation results are relevant, but our way of getting the straight-line velocity field is different.

% % I think the first two points are probably what the ML community cares about the most. Other than that, there are many technicalities in the paper that may also be worth a discussion?  For example, the bound norm on KR map in terms of the densities, our use of Faadi bruno in abstract function spaces, our treatment of the domains (extension of Lipschitz domain, boundary condition etc.). Each of these point may be of independent interest? 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \hline
% OLD
 
% Sampling from an arbitrary probability distribution is one of the central problems of computational statistics. A useful approach to this problem is through transportation of measure, using deterministic couplings ____: construct an invertible function that \emph{pushes forward} the target probability distribution to a relatively simple source distribution (usually chosen to be uniform or standard Gaussian). One can then simulate from the target distribution by drawing samples from the source distribution and evaluating the inverse of the transport map; this construction is useful for both generative modeling ____ and variational inference ____. Alternatively, one can form an estimate of the \emph{density} of the target measure by evaluating the pullback density of the source distribution through the transport map.
 
% Many parameterizations, ranging from polynomials to radial basis functions to input-convex neural networks.

% In the machine learning literature, normalizing flows ____ construct transport maps by composing a sequence of relatively simple structured invertible functions; again, the goal is to push forward the target distribution to an easy-to-sample distribution. For such methods, it is typically required that the maps be invertible and the determinant of the Jacobian be easily computable, as evaluating pushforward or pullback densities follows from the change-of-variables formula. To satisfy these requirements, various architectures for normalizing flows have been proposed, including planar and radial flows ____, affine coupling flows ____, autoregressive flows ____, and neural autoregressive flows ____.

% In parallel, recent work in deep learning has elucidated connections between differential equations and neural networks ____, which led to the idea of neural ordinary differential equations (NeuralODEs) ____. More recently, neural ODEs have been proposed to perform generative modeling tasks ____. The generative process is as follows: Denote by
%     $\pi$ the target distribution from which we wish to sample and by
%     $\rho$ the source distribution.
%     % . Typically, $\pi$ is
%     % either known through an iid sample or through its (unnormalized)
%     % density, see Sec.~\ref{sec:setup} ahead. For a reference
%     % distribution $\rho$,
%     Given $x\sim\rho$, the following initial value problem
%     % to get the time-one solution $x(1)$ for the ODE:
%     \begin{equation}\label{eq:ODE}
%       \begin{aligned}
%         % \frac{dX(x,t)}{dt}
%         \frac{d X(x,t)}{dt} &= f(X(x,t), t),\\
%         X(x, 0) &= x,
%       \end{aligned}
%     \end{equation}
%     is solved up to time $t=1$. The goal is to learn a Lipschitz continuous velocity
%     field $f$, which is parameterized as a neural network, such that
%     $x\sim \rho$ implies $X(x,1)\sim \pi$.
    
%     % \jztd{Before it said
%     % $f(x,t,\theta_t)$. I guess $\theta_t$ was a typo? In any case,
%     % $\theta$ didn't occur anymore so I removed it.}  \jztd{Sven made a
%     % good comment in the group meeting: We should assume throughout
%     % that $f$ is Lipschitz continuous so that \eqref{eq:ODE} always has
%     % a solution.}

%   % If the neural network $f$ is properly learned, then the points
%   % $x(1)$ will be approximately sampled from the distribution
%   % $\rho(x)$.
%   In the context of distribution learning,
%   neural ODEs % as continuous normalizing flows
%   enjoy % many advantages.
%   several desirable properties: 
%   Invertibility of $x\mapsto X(x,1)$ is guaranteed by design,
%     % as one can always integrate it backwards in time,
%     as one can always solve \eqref{eq:ODE} backward in time. Thus,
%   in contrast to other methods such as invertible neural
%       networks (____), normalizing flows
%       (____) or other transport maps
%       (____), no further restrictions need to be
%     imposed on the vector field $f$ that is to be learned.
%   % there is no limitations on the form the ODE dynamics (or the
%   % neural networks) need to take. In addition, The change
%   In addition, the density $\eta(x,t)$ % under neural ODE
%   of $X(x,t)$ (with $x\sim\rho$) can easily be computed, as it
%     follows the dynamics
%   $\frac{\partial \log \eta(x,t)}{dt} = -\tr(\nabla_Xf(X(x,t), t))$, which is known as the
%     \emph{instantaneous change of variables formula}, see
%     ____.
%   % another ODE, called the instantaneous change of variables formula
%   % (____):
%   % $\frac{\partial \log p(x(t))}{dt} = -\text{Tr}\left(\frac{\partial
%   %   f}{\partial x(t)}\right)$.

%   In regards to the usefulness of neural ODEs as generative models,
%   there are three questions that are natural to ask.
%   First, % note that
%   it is known that when the source and target measures are
%   well-behaved (for example, both absolutely continuous with respect
%   to the Lebesgue measure), there are in general infinitely many
%   transport maps that push forward one measure onto the other. 
%   Even if we require the time-one flow map $x\mapsto X(x,1)$ to
%   be a particular transport map $T$, there are in general still
%     many possible velocity fields $f$ realizing $T$.
%   % still infinitely many ways how points from the source measure,
%   % $x$, can be connected to points from the target measure, $T(x)$,
%   % through the ODE trajectories.
%   It is observed in ____ and ____ that without
%   any form of regularization, the trajectories of the ODE dynamics
%   connecting $x$ to $T(x)$ may be very
%   irregular. % Therefore, % the first question that
%   Therefore it is natural to ask how we can regularize the
%     training objective to improve the training process.
%   % so that the velocity fields we learn in the end is nice in an
%   % appropriate sense.
%   Secondly, with the regularized training objective, we would like to
%   understand the structures of its optimal solutions and quantify the
%   neural network approximator for them. The third question to ask,
%   then, is how can we bound the distance between the target measure
%   and the push-forward measure of the source under the flow map of the
%   neural ODE when we substitute in the neural network as an
%   approximation for the velocity field? Our work is the first attempt
%   to address these questions in a unified way. In the remainder
%   of this section, we will review some relevant literature and
%   summarize our main contributions.


%