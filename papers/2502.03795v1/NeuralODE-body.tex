\newcommand{\co}{[0,1]}
\newcommand{\ci}{[0,1]^i}
\newcommand{\cd}{[0,1]^d}
\newcommand{\essinf}{{\rm ess\,inf}}
\newcommand{\supp}{{\rm supp}}
% \newcommand{\sym}{{\rm sym}}
\tableofcontents

\input{introduction}  

\section{Preliminaries}
\subsection{Notation and definitions}
 {\bf ODEs and flow maps.}
  % For the ODE \eqref{eq:ODE}, we
  % % for ODEs, we
  % use $x(t)$ to denote the trajectory of a simple particle with
  % initial condition $x(0)$.\jztd{Is this used anywhere? Otherwise
  % remove.}
%   \jztd{Removed notation ``$x(t)$ for trajectory of a particle'', as
%     this does not seem to be used (?)}  
 We write $X(x,t)$ or $X_f(x,t)$ for the solution of \eqref{eq:ODE}
 with initial condition $x$ at time $t=0$, i.e.,
  \begin{equation}\label{eq:flowmap}
    X_f(x,t) = x + \int_0^tf(X_f(x,s),s)ds.
  \end{equation}
  Given an initial distribution $\pi_0$, we write $\pi_t$
  or $\pi_{f,t}$ for the 
  pushforward measure $X_f(\cdot,t)_\sharp \pi_0$ and $\pi(x,t)$ or $\pi_{f}(x,t)$ for the corresponding density. 

\medskip

  \noindent {\bf Vectors and multiindices.} % In general,
  % we use single bar
  For $x=(x_1,\dots,x_d)^\top\in\R^d$, $|x|$ is the Euclidean norm.
  With $\N=\{0,1,2,\dots\}$, we denote multiindices by bold letters
  such as $\bsv = (v_1,v_2,\dots,v_d) \in \mathbb{N}^d$, and we use the
  standard multiindex notations $|\bsv| = \sum_{i = 1}^d v_i$ and
  $\bsv! = \prod_{i=1}^d(v_i!)$.
  % is the absolute value and factorial for multi-indices.  is the
  % mixed-partial differential operator with respect to the
  % multi-index $\bsv$ and in case where the differentiation variable
  % $x$ is obvious, we may omit it in the notation.
  Additionally, $x^\bsv = \prod_{i=1}^d x_i^{v_i}$ and 
  $x_{[k]} \coloneqq (x_1,\dots,x_k)\in\mathbb{R}^k$ for all $k\leq d$.
  % to denote norm of vectors in $\mathbb{R}^d$ and double bar
  % $\|\cdot\|$ to denote norm of functions or vector fields.
  % \jztd{Different norms should should use different notation,
  % indicated by a subscript, otherwise it's confusing (and ambiguous,
  % for example in $1d$ the two definitions don't seem to coincide).
  % Also like this it's not clear over what the $L^\infty$ norm is
  % taken. Use something like $\|f\|_{L^\infty(\Omega)}$
  % instead. Moreover, why is this norm only defined for partial
  % derivatives or $D^nf$ (which itself has not been defined) rather
  % than general functions? First introduce notation for the
  % derivatives, then introduce notation for some norms and
  % spaces. Finally, the notation for the partial derivative does not
  % coincide with the one that was just introduced a few lines above.}
  % Finally, we introduce a linear order on $\mathbb{N}^d$. That is,
  % we denote
  For two multiindices $\bsv$, $\bsw\in\N^d$, $\bsw \prec \bsv$ if and
  only if one of the following holds: \textit{(i)} $|\bsw| < |\bsv|$,
  \textit{(ii)} $|\bsw| = |\bsv|$ and there exists a $k < d$ such
  that $w_1 = v_1$, \dots, $w_k = v_k$, but $w_{k+1} < v_{k+1}$.


\medskip

  \noindent {\bf Derivatives.} For $f\in C^1(\R^d,\R^m)$,
  $\nabla f:\R^d\to\R^{d\times m}$ is the gradient. In case $f$
  depends on multiple variables, we write, for example,
  $\nabla_xf(x,t)$. For a multiindex
  $\bsv = (v_1,v_2,\dots,v_d) \in \mathbb{N}^d$, where
  $\N=\{0,1,2,\dots\}$, we write
  $D^\bsv f(x)= \frac{\partial^{|\bsv|}}{\partial x_1^{v_1} \dots 
    \partial x_d^{v_d}} f(x)$ for the partial derivative and similarly to the notation above, $D_x^\bsv f(x,t)$.
  % \subsection{Function Spaces}
  % We work with several different function spaces in this work and in
  % this subsection, we give the definitions and notations that we
  % will use for these spaces and the associated norms.

\medskip

  \noindent {\bf Function spaces.}
  % \begin{definition}
  For two Banach spaces $X$, $Y$ % be two Banach spaces. For
  and $n\in\mathbb{N}$ we denote by
  % \begin{equation}
  $\cL^n(X;Y)$
  % \end{equation}
  the space of all $n$-linear maps from $X^n\to Y$, and by
    $\cL^n_{\sym}(X;Y)$ the space of all symmetric $n$-linear maps
    from $X^n\to Y$ (i.e., $A\in\cL^n_{\sym}$ iff
    $A(x_{\sigma(1)},\dots,x_{\sigma(n)})$ is independent of the
    permutation $\sigma$ of $\{1,\dots,n\}$). The norms
  % The norm on
  % this space is defined as
  on these spaces are defined as
  \begin{equation*}
    \|A\|_{\cL^n(X;Y)} \coloneqq \sup_{\|x_i\|_X\le 1} \|A(x_1,\dots,x_n)\|_Y,\qquad
    \|A\|_{\cL^n_{\sym}(X;Y)} \coloneqq \sup_{\|x\|_X\le 1} \|A(x,\dots,x)\|_Y.
  \end{equation*}
  We recall that if $A\in\cL^n_{\sym}(X;Y)$ and $B\in\cL^n(X;Y)$
    such that $A(x^n)=B(x^n)$ for all $x\in X$, then, see e.g.,
    \cite[14.13]{chae}, 
  \begin{equation}\label{eq:symnorm}
    \|A\|_{\cL^n_{\sym}(X;Y)}
    \le \|B\|_{\cL^n(X;Y)}\le\exp(n)\|A\|_{\cL^n_{\sym}(X;Y)}.
  \end{equation}

  %
  Recall that for $f\in C^k(X,Y)$, the $k$-th Fr\'echet derivative
  $D^kf(x)$ of $f$ at $x\in X$ belongs to $\cL^k_{\sym} (X;Y)$. For
  $S\subseteq X$ open and $f\in C^k(S,Y)$ we write
  \begin{equation}\label{eq:Ck}
    \|f\|_{C^k(S)} \coloneqq \sup_{n\le k}\sup_{x\in S}\|D^n f(x)\|_{\cL^n(X;Y)}.
    % =\sup_{n\le k}\|D^n f\|_{L^\infty(S, L^n(X;Y))}.
  \end{equation}

  % \begin{remark}
  %   When $S\subseteq X$ is a not open, then $f\in C^k(S,Y)$ means
  %   $\forall 0\le j\le k: D^jf\in C^0(\Omega^\circ)$ and $D^jf$
  %   allows a continuous extension to $S$.
  % \end{remark}
%
  % When $X$ is the standard Euclidean space, one might also be
  % interested in the following quantities: For function
  % $f: S\subseteq\mathbb{R}^d\rightarrow\mathbb{R}$, the quantity
  % $\max_{|\bsalpha|<k} \sup_{x\in S}|D^\bsalpha f(x)| =
  % \max_{|\bsalpha|<k}\|D^\bsalpha f\|_{L^\infty(S)}$ and for a
  % velocity field $f: \mathbb{R}^d\rightarrow\mathbb{R}^d$ the
  % quantity
  % $\max_{i = 1, ..., d}\max_{|\bsalpha|<k} \|D^\bsalpha
  % f_i(x)\|_{L^\infty(S)}$, where $\bsalpha$ is a multi-index. These
  % quantities are usually used to define $C^k$ norm of functions and
  % velocity fields on Euclidean space and in a technical lemma later,
  % we will prove that they are equivalent with the norms on general
  % Banach spaces, which are defined as operator norms as above.  In
  % most situations, the set $S$ is obvious and may be omitted in the
  % notation.

  For $X=\R^d$ and $S\subseteq X$, we use the usual notation
  $W^{k,p}(S)$, $k\in\N$, $p\in [1,\infty]$, to denote functions with
  weak derivative up to order $k$ belonging to $L^p(S)$. As a norm,
  we will use
  % A broader class of functions is called Sobolev functions, whose
  % definition is the follows:
  % \begin{definition}
  %   Sobolev space $W^{k,p}(\Omega)$ is the space of functions on
  %   domain $\Omega$ such that the respective Sobolev norm defined
  %   below is finite:
  \begin{equation*}
    \|f\|_{W^{k, p}(S)} =
    \begin{cases}
      (\sum_{|\bsalpha|\leq k} \|D^\bsalpha f\|^p_{L^p(S)})^\frac{1}{p} & \text{if $ 1\leq p < \infty$}\\
      \max_{|\bsalpha|\leq k} \|D^\bsalpha f\|_{L^\infty(S)}& \text{if
        $ p = \infty$}.
    \end{cases}
  \end{equation*}
  % \end{definition}
  % Note that in the definition of Sobolev space $W^{k, \infty}$,
  % derivatives $D^\bsalpha f$ are assumed to exist in the
  % \textit{weak sense}.


\noindent{\bf Divergences between distributions.} 
Let $(\Omega, \mathcal{F}, \mu)$ be a probability space. For two
probability measures $\rho$ and $\pi$ such that $\rho\ll\mu$, $\pi\ll\mu$, the information divergences we consider are the following:
\begin{itemize}
\item KL (Kullback--Leibler) divergence: Assuming also $\rho\ll\pi$, we define
  $\text{KL}(\rho, \pi) = \int_\Omega
  \log \frac{d\rho}{d\pi}(x) \rho(dx)$.
    
\item If $(\Omega, m)$ is also a metric space, then
  the Wasserstein distance of order $p$ is defined as:
  $$W_p(\rho, \pi) = \left(\inf_{\gamma\in\Gamma(\rho, \pi)}\int_{\Omega\times\Omega} m(x,
    y)^p\gamma(dxdy)\right)^\frac{1}{p},$$ where $\Gamma(\rho, \pi)$ is
  the set of all measures on $\Omega \times \Omega$ with marginals
  $\rho$ and $\pi$. In $\mathbb{R}^d$, this is simply
  $W_p(\rho, \pi) = \left(\inf_{\gamma\in\Gamma(\rho,
      \pi)}\int_{\mathbb{R}^d\times\mathbb{R}^d}|x-y|^p\gamma(dxdy)\right)^\frac{1}{p}$.
    
\end{itemize}



\subsection{Problem setup}\label{sec:setup}
% In this work, we consider the transportation between a source
% measure measures on bounded domains. \rr{ Let the \emph{target
% measure} $\pi$ be supported on $\Omega_0\subseteq\R^d$ and let the
% \emph{source measure} $\rho$ be supported on
% $\Omega_1\subseteq\R^d$.
In the following we denote by $\pi$ a \emph{target measure} and
  by $\rho$ a \emph{source measure} on $\R^d$. Our general goal is to
  sample from the target. The source measure is an auxiliary measure
  that is easy to sample from, and may be chosen at will. Throughout we
  work under the following assumptions:

  % and source measures supported on $\Omega_0$,
  % $\Omega_1 \subset \mathbb{R}^d$ respectively}. The target measure
  % is where we want to sample from and the source measure is chosen
  % by us and is usually a simple distribution. Formally, we make the
  % following assumptions about the measures and the domains.

\begin{assumption}[compact support]\label{ass:dens1}
  With $\Omega_0 \coloneqq {\rm supp}(\pi)$ and
  $\Omega_1 \coloneqq {\rm supp}(\rho)$, it holds that $\Omega_0$,
  $\Omega_1\subseteq\R^d$ are compact and convex sets. Both $\rho$
  and $\pi$ are absolutely continuous with respect to the Lebesgue
  measure.
  % \ymmtd{Do we need this? Since they are also absolutely
  % continuous wrt Lebesgue, wouldn't this imply
  % $\Omega_0 = \Omega_1$?}.
%   \jztd{removed ``are abs cont wrt each other''}
\end{assumption}

By abuse of notation, we denote the (Lebesgue-) densities of $\rho$
and $\pi$ by $\rho(x)$ and $\pi(x)$, respectively.
  
\begin{assumption}[regularity]\label{ass:dens2}
  % The densities $\pi(x)$ and $\rho(x)$ are $C^k$ functions on
  % $\Omega_0$ and $\Omega_1$ respectively,
  There exist constants $L_1 > 1$, $L_2>0$ and $k\in\N, k \geq 2$ such that
  $\|\pi\|_{C^k(\Omega_0)} \leq L_1$, $\|\rho\|_{C^k(\Omega_1)} \leq L_1$ and  $\inf_{x\in\Omega_0}\pi(x) \geq L_2$, $\inf_{x\in\Omega_1}\rho(x) \geq L_2$. 
  % for some constants $L_1, L_2$.
\end{assumption}



% \jztd{is this necessary for $\Omega_1$?}\rrtd{I think here we may
% assume both domains are compact and convex because the flow starts
% from $\Omega_0$ and $\Omega_1$ for two types of problems
% respectively, but for theoretical analysis, we assume we always
% start from $\Omega_0$}

% For simplicity, we only consider terminal time of the ODE to be
% one. For impact of the terminal time in the ODE dynamics and time
% scaling, there is a discussion at the end of the paper \ref{}.

We consider two types of problems:
\begin{itemize}
\item[P1] The target measure $\pi$ is known through a collection of
  iid samples. This is the problem considered in, e.g., \citet{ffjord,HowToTrain,OTFlow}. The goal is
  to % \jzs{start from the samples in $\pi$ and}
  learn % the parameters in the neural network parametrizing
  a velocity field $f$ in \eqref{eq:ODE}
  % \jztd{We don't learn the architecture we learn the parameters of
  % the network.}
  such that with initial distribution chosen to be the target, $\pi_{0} = \pi$, the time-one distribution satisfies $\pi_{f,1}(\cdot) = X_f(\cdot,1)_\sharp \pi \approx \rho$. Since the flow map $x \mapsto X_f(x,1)$ is by construction invertible (and its inverse can be evaluated by solving \eqref{eq:ODE} backwards in time), one can draw new samples from the source measure and apply the inverse of the flow map to generate (approximate) samples from $\pi$. The learned flow map can also be used, without inversion, to estimate the density of $\pi$.
  
%   Having access to such an invertible transformation allows one to transform sample from the source to (approximate) samples from the target. 
% This construction also enables density estimation for $\pi$ \citep{baptista}. 
  
  
\item[P2] The target measure is known up to a normalizing constant;
  that is, we can evaluate the unnormalized target density $\Tilde{\pi}$. This setting is ubiquitous in Bayesian statistics, since the posterior normalizing constant of a Bayesian model is usually unavailable. This problem is in the reverse direction of the previous problem \citep{measure-transport,PlanarFlow,moselhy2012}; that is, we choose the initial distribution to be the source distribution, $\pi_0 = \rho$, and learn a velocity field $f$ such that $\pi_{f,1}(\cdot) = X_f(\cdot,1)_\sharp \rho \approx \pi$. 
    
\end{itemize}


% \ymmtd{I think the notation is fine for now (though I will think about this again after editing Section 3!) but the one thing that might make it even clearer is to write the source as something other than $\rho$; maybe $\eta$ or $\varrho$.}


From the approximation and algorithmic perspectives, there is no essential difference between problems P1 and P2 above. In both cases, algorithms for learning the velocity field $f$ require: \textit{(i)} a sample from the chosen initial distribution and \textit{(ii)} the ability to evaluate the \emph{desired} time-one density up to a normalizing constant. For the rest of the paper, we will thus adopt the setting of P1 (with initial distribution for the ODE system chosen to be the target $\pi$). Our results can be translated to P2 simply by exchanging $\pi$ and $\rho$. 

The objective functional considered in this work takes the following form: 
\begin{equation}\label{Jobjective}
J(f) = \mathcal{D}(X_f(\cdot, 1)_\sharp\pi, \rho) + R(f).
\end{equation}
The first part of the objective is an information divergence between two probability distributions (for example, KL, Wasserstein, etc.). The second part is a regularization term that follows from the discussion in Section \ref{sec:intro}: we would like the trajectory of the ODE, starting from any initial condition, to be a straight line with constant velocity. In other words, we would like the acceleration in a Lagrangian frame, $d f (X(x,t), t) / d t$, to be zero for all $x \in \Omega_0$ and $t \in [0,1]$. To this end, we integrate the squared acceleration from \eqref{eq:straightlinereg} along the trajectory of a particle $x\in\Omega_0$:
\begin{equation}
R(x,t) = \int_0^t \left \vert \left ( \nabla_X f(X(x,s),s) \right) f(X(x,s),s) +
\partial_s f(X,s) \right \vert^2 ds,    
\end{equation}

and 
\begin{equation}\label{eq:R}
R(f) = \int_{\Omega_0}\int_0^1 \left \vert \left ( \nabla_X f(X(x,s),s) \right) f(X(x,s),s) +
\partial_s f(X,s) \right \vert^2 dsdx      
\end{equation}



We comment here that while our theoretical analysis works for general divergence $\mathcal{D}$, KL-divergence is the most common objective used in practice. For this purpose, we derived the training algorithm for it in Appendix \ref{app:training}.


By the Picard-–Lindel\"{o}f theorem \citep{ArnoldODE}, existence and uniqueness of solutions to the ODE \eqref{eq:ODE} requires that the velocity field $f(x,t)$ be continuous in $t$ and Lipschitz continuous in $x$. Therefore, searching over the space of functions 
  \begin{equation}\label{eq:V}
    \mathcal{V} = \setc{f: \mathbb{R}^d\times[0,1]\rightarrow\mathbb{R}^d}{f \ \text{is Lipschitz continuous in } x, \text{continuous in }t }, \end{equation}
we have the following optimization problem:
  \begin{equation*} \label{eq:OP} \tag{OP}
    \begin{aligned}
      & \underset{f\in\mathcal{V}}{\text{minimize}}
      & & J(f)\\
      & \text{with}
      & & R(f) \text{ defined in } \eqref{eq:R}
    \end{aligned}
  \end{equation*}

\begin{remark}
  In practice, the conditions of the Picard–Lindel\"{o}f theorem will
  always be satisfied for a neural network of finite size with Lipschitz activation functions. In particular, these conditions hold true for ReLU networks, which is what we consider in our theoretical analysis.
\end{remark}





% \section{Theoretical Analysis for the\jztd{There is not \emph{one} minimizer, but many} Optimal Solution}
% \ymmtd{It might be worth breaking this into two sections (at the \texttt{section} level of the hierarchy): one on existence and structure of minimizers (so $4.1 \to 4$) and the other on regularity of the velocity field (so $4.2 \to 5$)}
% We notice that
\section{Existence and structure of minimizers}\label{sec:minimizers}

% \jz{The two optimization objectives \ref{OP1} and \ref{OP2} take a
%   similar form. They are the sum of two terms, the first being a KL
%   divergence, and the second a quadratic regularization term
%   integrated in space time. For our theoretical analysis, we consider
%   the optimization problem in this general form
%   \begin{equation}\label{eq:OP}
%     J(f) = \dkl[X_f(\cdot,1)_\sharp\pi\|\rho] + \lambda\mathbb{E}_{x\sim\pi}[R(x,1)], \tag{OP}
%   \end{equation}}
% where we don't distinguish between target and source, and just assume $\pi$ and $\rho$ are two generic measures that satisfy assumptions \ref{ass:dens1} and \ref{ass:dens2}. The goal is to find a generic neural ODE whose time-one flow map pushes forward $\pi$ to $\rho$ through straight line trajectories. 





% The central question that we would like to understand is the
% structure of the optimal solution for the optimization problem.
The objective $J(f)$ is nonnegative, since it is the sum of an information divergence and a nonnegative regularizer.  Moreover, as we will see, the optimal solution will make both terms in the objective $J(f)$ zero under our assumptions. 
%
First, we state necessary and sufficient conditions on a transport map $T$ such that there exists a velocity field $f$ whose time-one flow map yields straight-line trajectories from $x$ to $T(x)$.


% \jz{[In principle, for most of the following we don't need that
% $T:\mathbb{R}^d\to\mathbb{R}^d$, we could also have
% $T:\mathbb{R}^d\to\mathbb{R}^m$ with $m>d$. This is a practically
% relevant case discussed in several recent publications (I don't know
% about neural odes though). It might be worth to add a section where
% we generalize our results to the case where the target lives for
% example on a $d$-dimensional $C^k$ manifold in $\mathbb{R}^m$.]}
\subsection{Existence}\label{subsec:existence}
\begin{lemma}\label{lemma:Tinjective}
  Let $\Omega_0\subseteq\R^d$ be convex and
    $T\in C^1(\Omega_0,\R^d)$ such that $\det \nabla_x T(x)\neq 0$ for
    all $x\in\Omega_0$.  Then $T$ is injective.  
  % If $T:\Omega_0 \rightarrow\Omega_1$ is a $C^1$ map from a convex
  % set $\Omega_0\subset\mathbb{R}^d$ and $\nabla T$ is such that
  % $\det(\nabla T(x)) > 0$ for all $x\in\Omega_0$, then
  % $T:\Omega_0 \rightarrow\Omega_1$ is injective.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:Tinjective}]
  Assume not. Then there exist $x$, $y\in\Omega_0$ such that
    $x \neq y$ and $T(x) = T(y)$. For $s\in [0,1]$ set
    $f(s)  \coloneqq  T((1-s)x + sy)$. Since $f(0) = f(1)$, by the mean
    value theorem there exists $s\in (0,1)$ such that
  $f'(s) = 0$. Then $f'(s) = \nabla_x T((1-s)x + sy)(y-x) =
  0$. Since $v = y-x \neq 0$, we have
    $\det(\nabla_x T((1-s)x + sy))=0$, which is a contradiction.
  % satisfies $v\in\text{Ker}\nabla T((1-s)x + sy)$ and we have
  % $\det(\nabla T(x)) = 0$, which is a contradiction.
\end{proof}	

Denote in the following, for $x\in\Omega_0\subset\R^d$,
  $t\in [0,1]$, and a map $T:\Omega_0\to\R^d$,
  \begin{equation}\label{eq:Tt}
    T_t(x)  \coloneqq  (1-t)x + tT(x),
  \end{equation}
  i.e., $[0,1]\ni t\mapsto (T_t(x),t)$ parameterizes the straight line of constant velocity
  between the points $(x,0)$ and $(T(x),1)$ in $\R^d\times [0,1]$. 
  We
  refer to $t\mapsto T_t(x)$ as the \emph{displacement interpolation}
  of $T$. We now investigate under which conditions these lines do not
  cross for different $x$, which is necessary for $T_t(x)$ to
  be expressible as a flow $X(x,t)$ solving \eqref{eq:ODE} for a
  certain $f$. In other words, we check under what conditions the map
  $\Omega_0\ni x\mapsto T_t(x)$ is injective for all $t\in [0,1]$. To
  state the following lemma, for $A\in\R^{d\times d}$ we let $\sigma(A) \coloneqq \setc{\lambda\in\R}{\det(A-\lambda I)=0}$ denote its spectrum.  



  \begin{assumption}\label{ass:T}
    It holds that $T\in C^1(\Omega_0,\R^d)$ and
    \begin{equation}\label{eq:spectrum}
      (-\infty, 0]\cap \sigma(\nabla_x T(x)) = \emptyset\qquad
      \forall\,x\in\Omega_0.
    \end{equation}
  \end{assumption}


% Now, set $T_t(x) = (1-t)x + tT(x)$, we have the following corollary.
% \begin{corollary}\label{corollary:injectivity}
%   Let $\Omega_0$ be a convex set. Assume that
%   $\det(\nabla T_t(x)) > 0, x\in\Omega_0, t\in[0,1]$. Then
%   $T_t:\Omega_0\rightarrow\Omega_1$ is injective for all
%   $t\in[0,1]$.
% \end{corollary}


% In the following lemma, we also give a characterization of when the
% determinant of the Jacobian $\nabla T_t(x)$ is positive based on its
% eigenvalues.

\begin{lemma}\label{lemma:spectrum}
  Let $\Omega_0\subseteq\R^d$ be convex and
    $T\in C^1(\Omega_0,\R^d)$. Then $\det(\nabla_x T_t(x))>0$ for all
    $x\in\Omega_0$ and all $t\in[0,1]$, iff \eqref{eq:spectrum} holds.
  
  % It holds $\det(\nabla T_t(x)) > 0$ for all
  % $x\in\Omega_0\subset\mathbb{R}^d$ ($\Omega_0$ convex) and
  % $t\in[0,1]$ if and only if
  % $\sigma(\nabla T(x)) \cap (-\infty, 0] = \emptyset$, where
  % $\sigma$ denotes the spectrum of a matrix.  note to selves: \sigma
  % = spectrum of a matrix
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:spectrum}]
  Since $\nabla_x T_t(x) = (1-t)I + t\nabla_x T(x)$, the map
    $t\mapsto\det(\nabla_x T_t(x))\in\mathbb{R}$ is continuous.
    Because of $\nabla_x T_0(x) = I$, to prove the lemma it is
  sufficient to show that for every $x\in\Omega_0$,
  $\sigma(\nabla_x T(x)) \cap (-\infty, 0] = \emptyset$ iff
  $\det(\nabla_x T_t(x))\neq 0$ for all $t\in[0,1]$.

  Fix $x\in\Omega_0$.  Assume for contradiction that for some
  $t\in[0,1]$, we have $\det(\nabla_x T_t(x)) = 0$. Then there
    exists $v\neq 0$ such that $\nabla_x T_t(x)v = 0\in\R^d$. Thus
  $\nabla_x T(x)v = -\frac{1-t}{t}v$ and hence
  $-\frac{1-t}{t}\in (-\infty,0]$ is an eigenvalue of
  $\nabla_x T(x)$. The reverse implication follows similarly.
    Assume that $s\in \sigma(\nabla_x T(x))\cap (-\infty,0]$.  Then
    there exists $v\neq 0$ such that $\nabla_x T(x)v=sv$. Since
    $t\mapsto -\frac{1-t}{t}:(0,1]\to (-\infty,0]$ is bijective, we
    can find $t\in (0,1]$ such that $\nabla_x T(x)v=-\frac{1-t}{t}v$,
    implying $v\in \ker(\nabla_xT_t(x))$ and thus
    $\det(\nabla_x T_t(x))=0$.
  % Note $t\rightarrow -\frac{1-t}{t}$ is a bijective
  % map from $[0,1]$ to $(-\infty, 0]$, the claim is then established.
\end{proof}

Combining the previous two statements
% result in \jz{Cor.~}\ref{corollary:injectivity} with the implicit
% function theorem, we
establishes the existence of a velocity field such that the time-one
flow map of the ODE \eqref{eq:ODE} realizes the map
$x\mapsto T(x)$, and the ODE dynamics produce straight-line
trajectories of constant speed.  

% \begin{theorem}\label{thm:f}
%   Let domains $\Omega_0, \Omega_1 \subset\mathbb{R}^d$ be as in
%   \ref{sec:setup} and $T:\Omega_0\rightarrow\Omega_1$ a $C^k$
%   map. Assume in addition that the map $T$ also satisfies the
%   condition that there exists a constant $c$ such that
%   $\det \nabla T_t(x) > c > 0, \forall x\in\Omega_0$. Let
%   $\Omega \coloneqq \{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset
%   \mathbb{R}^{d+1}$. Then, there exists a unique $C^k$ velocity
%   field $f:\Omega\rightarrow\mathbb{R}^d$ such that the flow map of
%   the ODE $\frac{dX(x,t)}{dt} = f(X(x,t), t)$ satisfies
%   $X(x, 0) = x, X(x,1) = T(x)$ and $x$ is connected to $T(x)$ via
%   straight line trajectories.
% \end{theorem}
\begin{theorem}\label{thm:f}
  Let $k\in\N$ and let $\Omega_0\subseteq\R^d$ be convex and
    compact. Assume that $T\in C^k(\Omega_0,\R^d)$ for some $k\geq 2$
    satisfies \eqref{eq:spectrum}.
    % $\det(\nabla_x T_t(x))\neq 0$ for all $x\in\Omega_0$ and all
    % $t\in [0,1]$.
    With $T_t$ in \eqref{eq:Tt} set
    \begin{equation}\label{eq:Omega}
      \Omega_{[0,1]}  \coloneqq \setc{(T_t(x),t)}{x\in\Omega_0,~t\in [0,1]}\subset\R^{d+1}.
    \end{equation}
   % 
    Then there exists a unique $f:\Omega_{[0,1]}\to\R^d$ such that the
    solution $X:\Omega_0\times [0,1]\to\R^d$ of the ODE \eqref{eq:ODE}
    satisfies $X(x,t)=T_t(x)$ for all $x\in\Omega_0$, $t\in [0,1]$.
    Moreover $\Omega_{[0,1]}$ is simply connected and
    $f\in C^k(\Omega_{[0,1]}^\circ)$.
\end{theorem}


A key element of the proof of Theorem ~\ref{thm:f} is that the velocity field $f$ corresponding to the displacement interpolation $T_t$ can be defined implicitly in terms of $T$: $f\left ( (1-t)x + t T(x), t \right ) = T(x) - x$.

\begin{proof}[Proof of Theorem \ref{thm:f}]
      By \eqref{eq:ODE} and because $X(x,t)=T_t(x)=(1-t)x+tT(x)$,
        we have for $x\in\Omega_0$ and $t\in [0,1]$
        \begin{equation}\label{eq:Tx-x}
          T(x)-x = \frac{d}{dt}X(x,t)=f(X(x,t),t)=f(T_t(x),t).
        \end{equation}
        By Lemma \ref{lemma:Tinjective} and Lemma \ref{lemma:spectrum},
        the map
        \begin{equation*}
          (x,t)\mapsto G(x,t) \coloneqq (T_t(x),t)\in\Omega
        \end{equation*}
        is injective on $\Omega_0\times [0,1]$.  Thus \eqref{eq:Tx-x}
        uniquely defines $f$ at each point $G(x,t)\in\Omega$. By
        construction, this $f$ yields a flow map $X$ as in
        \eqref{eq:ODE} satisfying $X(x,t)=T_t(x)$.


      The map $G:\Omega_0\times [0,1]\to\Omega$ is a continuous
        bijection, and since $\Omega_0\times [0,1]\subseteq\R^{d+1}$
        is compact, $G^{-1}:\Omega\to \Omega_0\times [0,1]$ is also a
        continuous bijection. Since $\Omega_0\times [0,1]$ is a convex
        set, it is simply connected. Hence, the homotopy equivalent
        set $\Omega_{[0,1]}$ must also be simply connected. Moreover, the
        interior $\Omega^\circ$ of $\Omega$ is the image of
        $\Omega_0^\circ\times (0,1)$ under $G$.
        
        It remains to show $f\in C^k(\Omega_{[0,1]}^\circ)$. Fix
        $x\in \Omega_0^\circ$ and $t\in (0,1)$. Then
        \begin{equation*}
          \nabla_{(x,t)}G(x,t)=\begin{pmatrix}
            \nabla_x T_t(x) & T(x) - x \\
            0 & 1
          \end{pmatrix}\in\R^{(d+1)\times (d+1)},
        \end{equation*}
        and this matrix is regular by Lemma
        \ref{lemma:spectrum}. Since $G\in C^k(\Omega_0\times [0,1])$,
        the inverse function theorem (see, e.g., \cite{CalculusOnManifolds}[{Theorem~2.11}]) implies
        that $G^{-1}$ locally belongs to $C^k$ in a neighbourhood of
        $G(x,t)$. Since $x\in\Omega_0^\circ$ and $t\in (0,1)$ were
        arbitrary, we have $G^{-1}\in
        C^k(\Omega_{[0,1]}^\circ,\R^{d+1})$. Denote $G^{-1}=(F,E)$ such that
        $F:\Omega_{[0,1]}\to\R^d$ corresponds to the first $d$ components of
        $G^{-1}$.  By \eqref{eq:Tx-x}, for all $(y,s)\in\Omega_{[0,1]}^\circ$
        it holds that $f(y,s)=T(F(y,s))-F(y,s)$, so that $f$ belongs to
        $C^k(\Omega_{[0,1]}^\circ)$ as a composition of two $C^k$ functions.
    \end{proof}	

 
      \begin{remark}
        Note that $f\in C^k(\Omega_{[0,1]}^\circ)$ means only that $f$ is
        $C^k$ on the interior of $\Omega_{[0,1]}$. To show that the
        derivatives are well-defined on the boundary of $\Omega_{[0,1]}$ and that $f$ can be extended to a $C^k$ function outside
        of $\Omega_{[0,1]}$, certain regularity conditions of the domain are required, which will be discussed in later parts of this section.
      \end{remark}


    \subsection{Properties of $\Omega_{[0,1]}$}\label{subsection:propertiesofOmega}

   The set $\Omega_{[0,1]}$ is simply connected, but unlike
      $\Omega_0\times [0,1]$, it need not be convex:

  
\begin{example}[Rotation]
  Let $\Omega_0=\setl{x\in\R^2}{|x|\le 1}$ be the unit disc and let
  $T:\R^2\to\R^2$ be the rotation by $\bsalpha\in [0,2\pi)$ around
  $0\in\R^2$. Then
  \begin{equation}
    \nabla_x T(x) = \begin{pmatrix}
      \cos(\bsalpha) & -\sin(\bsalpha)\\
      \sin(\bsalpha) & \cos(\bsalpha)
    \end{pmatrix}.
  \end{equation}
  The spectrum of this matrix consists of the two values
  $\exp(\pm\ii\bsalpha)$, where $\ii$ denotes the imaginary root of
  $-1$. Thus \eqref{eq:spectrum} holds iff $\bsalpha\neq \pi$. If
  $\bsalpha=\pi$, then $T$ is the negative identity, and thus
  $T_{1/2}(x)=\frac{1}{2}x - \frac{1}{2}x=0$ for all $x\in\Omega_0$,
  so that the all straight lines connecting $x$ and $T(x)$ for
  $x\in\Omega_0$, meet at $t=\frac{1}{2}$ in the midpoint $0$ of the
  disc.  For all $\bsalpha\in [0,2\pi)\backslash\{\pi\}$, by
  Theorem~\ref{thm:f} and with $\Omega_{[0,1]}$ as in \eqref{eq:Omega}, there
  exists a vector field $f\in C^\infty(\Omega_{[0,1]})$ such that
  $T_t(x)=X(x,t)$ for $X$ as in \eqref{eq:ODE}. One can check that
  \begin{equation*}
    \Omega_{[0,1]} = \setc{(x,t)\in\R^2\times [0,1]}{|x|\le
      \sqrt{\sin\left(\frac{\pi}{2}-\frac{\bsalpha}{2}\right)^2+\left[t\cos\left(\frac{\pi}{2}-\frac{\bsalpha}{2}\right)+(1-t)\cos\left(\frac{\pi}{2}+\frac{\bsalpha}{2}\right)\right]^2}
    },
  \end{equation*}
  which is convex
  if $\bsalpha=0$ and nonconvex for all $\bsalpha\in (0,2\pi)$.
\end{example}



% For the purpose of obtaining neural network approximation results,
To approximate the velocity field $f$ from Theorem ~\ref{thm:f} with a
  neural network, we also need to understand the regularity of the
domain $\Omega_{[0,1]}$
%  \coloneqq \{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset \mathbb{R}^{d+1}$
on which $f$ is defined. % In
% particular, we need to know the \textit{regularity} of the boundary
% of the domain, $\partial\Omega$ so that the velocity field
% $f$ can be extended in a smooth way.
As we will see, $\Omega_{[0,1]}$ is a Lipschitz domain.
% We next recall the notion of Lipschitz domains and a function
% extension results on Lipschitz domains.

\begin{definition}
  A bounded domain
  $\Omega$ is called a Lipschitz domain if there exist numbers $\delta
  > 0$, $M > 0$,
  $J\in\N$, and a finite cover of open sets
  $\{U_j\}_{j=1}^J$ of $\partial\Omega$ such that:
  \begin{itemize}
  \item For every pair of points $x_1$, $x_2
    \in\Omega$ such that $|x_1 - x_2| < \delta$ and $\text{dist}(x_i,
    \partial\Omega) < \delta$, $i = 1$, $2$, there exists an index
    $j$ such that $x_i \in U_j$ , $i = 1$, $2$, and $\text{dist}(x_i,
    \partial U_j) > \delta$, $i = 1$, $2$.
  \item For each
    $j$ there exists some coordinate system
    $\{\zeta_{j,1},\dots ,\zeta_{j,d}\}$ in
    $U_j$ such that the set $\Omega \cap
    U_j$ consists of all points satisfying $\zeta_{j,d} \leq
    f_j(\zeta_{j,2},\dots ,\zeta_{j,d-1})$, where
    $f_j:\mathbb{R}^{d-1}\rightarrow\mathbb{R}$ is a Lipschitz
    function with Lipschitz constant $M$.
  \end{itemize}
\end{definition}





% A notion that generalizes Lipschitz domains is \textit{minimally
% smooth domains}, which we provide the definition below:
% \begin{definition}
%   Let $\Omega\subset\mathbb{R}^d$ be an open set. Its boundary
%   $\partial\Omega$ is said to be minimally smooth if there exists
%   $\epsilon > 0$, an integer $N$ and $M > 0$ and a sequence of open
%   sets $\{U_i\}_{i=1}^\infty$ such that the following hold:
%   \begin{itemize}
%   \item If $x \in \partial\Omega$, then $B(x, \epsilon) \subset U_i$
%     for some $i$.
%   \item No point in $\mathbb{R}^d$ is contained in more than $N$ of
%     the $U_i$'s.
%   \item For each $i$ there exists a special Lipschits domain $D_i$
%     whose bound does not exceed $M$ such that
%     $U_i\cap \Omega = U_i\cap D_i$
%   \end{itemize}
% \end{definition}
% \begin{remark}
%   Suppose $\Omega$ is itself a Lipschitz domain, by picking $D_i$ to
%   be $\Omega$ in the above definition, is easy to see that Lipschitz
%   domains are trivially minimally smooth domains.
% \end{remark}



To show that $\Omega_{[0,1]}$ is a Lipschitz domain, we first need an auxiliary result, Theorem ~\ref{thm:LipTransformation} in Appendix~\ref{app:AuxResults}, establishing that the image of a Lipschitz domain under a sufficiently regular map remains a Lipschitz domain. We can then show the following:
\begin{theorem}\label{thm:LipDomain}
  Consider the setting of Theorem ~\ref{thm:f}.  Then
    $\Omega_{[0,1]}\subset\R^{d+1}$ in \eqref{eq:Omega} is a Lipschitz
    domain.
  % In the context of Theorem \ref{thm:f},
  % the domain of the velocity field $f$, or the subset in space-time
  % covered by the straight line trajectories of the ODE dynamics,
  % $\Omega \coloneqq \{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset
  % \mathbb{R}^{d+1}$, is a Lipschitz domain.
\end{theorem}

\begin{proof}[Proof of Theorem \ref{thm:LipDomain}]
  %
  To show that $\Omega_{[0,1]}$ is a Lipschitz domain, first we observe that $\Omega_{[0,1]}$ is the image of $\Omega_0\times [0,1]$ under the map
    $(x,t)\rightarrow G(x,t)  \coloneqq  (tT(x) + (1-t)x, t)$ for
    $x\in\Omega_0,t\in[0,1]$. Since $\Omega_0\times [0,1]$ is a
    product of two convex sets, which is convex, Lemma
    \ref{lemma:convexLip} shows that the cylinder
    $\Omega_0\times [0,1]$ is a Lipschitz domain. To apply Theorem
    \ref{thm:LipTransformation}, we need to find a
    $C^1$-diffeomorphism from an open neighborhood $\mathcal{O}$ of
    $\Omega_0\times [0,1]$ onto its image. In the context of Theorem
    \ref{thm:f}, we have $\det(\nabla_x T_t(x)) > 0$ for all
    $(x,t)\in\Omega_0\times[0,1]$. Since $\Omega_0\times[0,1]$ is a
    compact set, the infimum of the continuous function
    $(x,t)\rightarrow\det(\nabla_x T_t(x))$ is achieved at some point
    in the set and thus we can conclude that
    $\inf_{(x,t)\in\Omega_0\times[0,1]}\det(\nabla_x T_t(x)) > 0$.



    On the other hand, since $T\in C^k(\Omega_0, \mathbb{R}^d)$ for
    some $k\geq 2$, it follows that
    $T \in W^{k, \infty}(\Omega_0, \mathbb{R}^d)$. By the extension
    theorem \ref{thm:functionExtension}, $T$ can be extended to a
    function $\tilde{T}\in W^{k, \infty}(\mathbb{R}^d,
    \mathbb{R}^d)$. Since $k \geq 2$, Sobolev embedding shows that
    $\tilde{T}\in C^1(\mathbb{R}^d, \mathbb{R}^d)$. Now consider the
    map $\tilde{T}_t(x) = t\tilde{T}(x) + (1-t)x$ for
    $(x,t)\in\mathbb{R}^{d+1}$. It is clear that $\tilde{T}_t(x)$ is
    $C^1$ in $(x,t)$ and also
    $\tilde{T}_t(x)|_{\Omega_0\times[0,1]} = T_t(x)$. By the
    continuity of determinant operator and
    $\inf_{(x,t)\in\Omega_0\times[0,1]}\det(\nabla_x T_t(x)) > 0$, it
    follows that there exists an open neighborhood
    $\mathcal{O}\subset\mathbb{R}^{d+1}$ of $\Omega_0\times[0,1]$ such
    that $\det(\nabla_x \tilde{T}_t(x)) > 0$ for all
    $x\in\mathcal{O}$. Without loss of generality, we can assume
    $\mathcal{O}$ is convex. This is because we can choose the
    neighborhood
    $\Omega_0\times[0,1]\cup
    \{B_\epsilon((x,t))|(x,t)\in\partial(\Omega_0\times[0,1])\}$, which
    is an open and convex set that can be made arbitrarily close to
    $\Omega_0\times[0,1]$ when $\epsilon\rightarrow 0$.

    Now, consider the extension of $G$,
    $\tilde{G}(x,t) = (t\tilde{T}(x) + (1-t)x, t)$ for
    $(x,t)\in\mathcal{O}$. We have
    \begin{equation*}
      \nabla_{(x,t)}\tilde{G}(x,t)=\begin{pmatrix}
        \nabla_x \tilde{T}_t(x) & \tilde{T}(x) - x\\
        0 & 1
      \end{pmatrix}\in\R^{(d+1)\times (d+1)}
    \end{equation*}
    is a regular matrix for fixed $(x,t)\in\mathcal{O}$. Then, the same
    arguments as in the proof of Theorem \ref{thm:f} show that
    $\tilde{G}(x,t)$ has a global inverse and $\tilde{G}^{-1}$ is
    $C^1$. Therefore, we have a $C^1$-diffeomorphism from
    $\mathcal{O}$ onto its image, and Theorem
    \ref{thm:LipTransformation} shows that $\Omega_{[0,1]} = \{(T_t(x),t)\}$
    for $x\in\Omega_0$, $t\in[0,1]$ is a Lipschitz domain.
\end{proof}	


For Sobolev functions on Lipschitz domains, we have the following extension theorem:
\begin{theorem}[{\cite[Chap.\
    3]{SteinBook}}]\label{thm:functionExtension}
  Let $D \subset\mathbb{R}^d$ be a Lipschitz
  domain.\footnote{The result in \citet{SteinBook} is stated in
      terms of so-called ``minimally smooth domains,'' which is a
      generalization of the notion of Lipschitz domains.}
  % domain of minimally smooth boundary.
  Then there exists a linear operator $\mathcal{E}$ mapping functions
  on $D$ to functions on $\mathbb{R}^d$ with the following properties:
  \begin{itemize}
  \item $\mathcal{E}(f)|_D = f$, that is, $\mathcal{E}$ is an
    extension operator.
  \item $\mathcal{E}$ maps $W^{k,p}(D)$ continuously into
    $W^{k,p}(\mathbb{R}^d)$ for all $1\leq p\leq \infty$ and all
      nonnegative integer $k$.
      
  \end{itemize}
\end{theorem}


% \begin{remark}
%   Thm.~\ref{thm:LipDomain} shows that the domain of $f$ from Thm.~\ref{thm:f}, $\Omega_{[0,1]} \subset\mathbb{R}^{d+1}$, is a Lipschitz
%   domain. Thm.~\ref{thm:f} shows that $f$ is $C^k$ on $\Omega^\circ_{[0,1]}$,
%   so extension result in Thm.~\ref{thm:functionExtension} shows that
%   $f$ can be extended to a function in $W^{k, \infty}$ on the entire
%   $\mathbb{R}^{d+1}$.
% \end{remark} 

Combining Theorem ~\ref{thm:f} and \ref{thm:LipDomain}, the extension result in  Theorem ~\ref{thm:functionExtension} shows that the velocity field $f$ of Theorem ~\ref{thm:f} can be extended to a function in $W^{k, \infty}$ on all of $\mathbb{R}^{d+1}$. 
  







\subsection{Regularized solutions}\label{sec:optsols}

In the theorem below, we show that the straight-line connections
between \jz{$x$} and $T(x)$, for a transport map $T$ that pushes
forward $\pi$ to $\rho$, yield the minimal average kinetic
energy, which is why we name the construction \textit{minimal energy regularization}. Here, the ``average kinetic energy'' is the squared magnitude of the ODE velocity averaged along trajectories, given a distribution $\pi$ on the initial condition. For any ODE velocity field $g(x,t)$, this quantity can be written in either Lagrangian or Eulerian frames as follows:
$$
\int_{\Omega_0} \int_0^1 \pi(x) \vert g(X_g(x,t),t) \vert^2 dt \, dx = \int_{\mathbb{R}^d} \int_0^1 \pi_{g,t}(x) \vert  g(x,t) \vert^2 dt \, dx. 
$$


  \begin{theorem}\label{thm:minimalenergy}
    Let $\Omega_0, \Omega_1\subseteq\R^d$ with $\Omega_0$  convex,
    and let $T\in C^1(\Omega_0,\Omega_1)$ satisfy
    \eqref{eq:spectrum}. Assume that $\pi$ and $\rho$ are
    probability measures on $\Omega_0$, $\Omega_1$, respectively, and that
    $T_\sharp\pi = \rho$. Then with
\begin{equation*}
  \mathcal{H} \coloneqq \setc{g\in \mathcal{V}}{X_g(\cdot,1)|_{\Omega_0}=T}
\end{equation*}
and $f$ from Theorem ~\ref{thm:f}, it holds that
\begin{equation*}
  f = \argmin_{g\in\mathcal{H}}\int_{\mathbb{R}^d}\int_0^1\pi_{g,t}(x)|g(x,t)|^2\dd t\dd x.
\end{equation*}
  
  
 \end{theorem}
\begin{proof}[Proof of Theorem \ref{thm:minimalenergy}]
  By Theorem \ref{thm:f}, we know the existence of velocity fields
  that realize these constructions. We then bound the
  average kinetic energy from below, using Lagrangian coordinates, as follows:
  \begin{align*}
    &\int_{\mathbb{R}^d} \int_0^1 \pi_{g,t}(x)|g(x,t)|^2dtdx = \int_{\Omega_0} \int_0^1 \pi_{g,0}(x)|g(X(x,t),t)|^2dtdx\\
    &=\int_{\Omega_0} \int_0^1 \pi(x)|\partial_t X(x,t)|^2dtdx \geq \int_{\Omega_0} \pi(x) \left ( \int_0^1|\partial_t X(x,t)|dt \right )^2dx 
    &\geq \int_{\Omega_0} \pi(x)|X(x,1)- X(x,0)|^2dx\\
    &= \int_{\Omega_0} \pi(x)|X(x,1)-x|^2dx = \int_{\Omega_0} \pi(x)|T(x) -x|^2dx,
  \end{align*}
   where the second inequality
  is due to Jensen's inequality, and equality holds iff
  $\partial_t X(x,t) = X(x,1) - X(x,0) = T(x) - x$. Then the optimal
  choice of $X$ is given by $X(x,t) = x + t(T(x) - x)$, which is
  exactly the displacement interpolation. As a result, the optimal
  choice for $f$ is given by $f(X(x,t), t) = T(x) - x$, which is the
  straight line construction from Theorem ~\ref{thm:f}.
\end{proof}	






\begin{remark}
  This construction has important connections to the \emph{fluid dynamics} formulation of optimal transport \citep{OT-CFD}. Theorem ~\ref{thm:minimalenergy} shows that for a \emph{fixed} transport map $T$, the straight-line construction gives the minimal average kinetic energy. The \emph{optimal} transport map $T$ is then just the transport map
  $T$ that minimizes $\int_{\mathbb{R}^d} \pi(x)|T(x) -x|^2dx$, which
  is the $L^2$-Wasserstein distance.
\end{remark}


With this machinery developed, we are now ready to prove that under our assumptions on the measures $\pi$ and $\rho$, \eqref{eq:OP} admits optimal solutions that realize the displacement interpolation of  transport maps $T$ that push forward $\pi$ to $\rho$. 


\begin{theorem}\label{thm:ExSol}
  Let $\pi$ and $\rho$ be measures supported on $\Omega_0$ and $\Omega_1$, respectively, and let Assumptions~\ref{ass:dens1} and \ref{ass:dens2} be satisfied. Then there exists at least one velocity field $f \in \mathcal{V}$
%   corresponding to the Knothe--Rosenblatt rearrangement $T$ from $\pi$ to $\rho$, 
  that achieves the global minimum of zero in the optimization problem \eqref{eq:OP}. Moreover, all global minimizers of \eqref{eq:OP} take the form $f(y,t) = T(x) - x$, with $y = T_t(x)$, for some transport map $T$ such that $T_\sharp\pi = \rho$, where $(y,t)\in\Omega_{[0,1]} \coloneqq \setl{(T_t(x),t)}{x\in\Omega_0,\ t\in[0,1]} \subset
  \mathbb{R}^{d+1}$ and $T$ satisfies \eqref{eq:spectrum}.
\end{theorem}
To show the existence of a velocity field that achieves $J(f) = 0$, in the proof of Theorem ~\ref{thm:ExSol} we consider a velocity field that realizes the optimal transport map. 
\begin{proof}[Proof of Theorem \ref{thm:ExSol}]
  Clearly, the objective function is bounded from below by zero. We
  first show that the velocity field corresponding to the
  optimal transport map achieves this minimum value. The existence of optimal transport map (associated with quadratic cost) under our assumptions and the fact that the map can be written as the gradient of a convex potential $\phi$ are well-established results in the theory of optimal transport maps (see e.g., \cite{OptimalOldAndNew} and \cite{BrenierMap}). By Assumptions \ref{ass:dens1},\ref{ass:dens2} and Theorem  \ref{Thm:OptimalRegularity},the optimal transport map is given by $T(x) = \nabla\phi(x)$ for some $\phi\in C^{k+2}(\Omega_0)$ that is strictly convex. Therefore, $\nabla T(x)$ has real and nonnegative eigenvalues. 
  
  Since $\phi\in C^{k+2}(\Omega_0)$, the following Monge-Ampere equation is satisfied in the classical sense (\cite{BrenierMap}), : 
  $$\det(\nabla^2\phi(x)) = \frac{\pi(x)}{\rho(\nabla\phi(x))}, \forall x\in\Omega_0.$$
  
  Since the densities are both bounded away from zero, we can conclude from the Monge-Ampere equation that $\det \nabla T(x) = \det \nabla^2\phi(x) > 0, \forall x\in\Omega_0$. In particular, Assumption \ref{ass:T} is satisfied and Theorem \ref{thm:f} establishes the existence
of a velocity field such that the time-one flow map of the ODE
realizes this $T(x)$ and the ODE dynamics yield straight line trajectories.


Now, suppose that there is a continuous velocity field $f$ that
achieves zero loss in \eqref{eq:OBJ}. Since the densities are continuous and bounded from below by a constant, the expectation $\mathbb{E}_{\pi}[R(x,1)] = 0$
implies that $R(x,1) = 0\  \forall x \in \Omega_0$.  That is, along each
trajectory $X(x,t)$ starting from $x\in\Omega_0$, we have
$\frac{ d f( X(x,t),t )}{d t} = 0$, i.e., $f$ is
constant in time along each trajectory. In other words,
$f(X(x,t),t) = g(x)$ for some function $g$. 
%
Now consider the ODE $\frac{dX}{dt} = f(X,t) = g(x)$; the solution is $X(t) = g(x)t + C$, where $C$ is constant in $t$. To make the KL-divergence zero,
we must have $X(1) = T(x)$ for some transport map $T$ such that
$T_\sharp\pi = \rho$, and we also have $X(0) = x$ as the initial
condition. Solving the equations gives $g(x) = T(x) - x$. That is, the
velocity field must take the form $T(x) - x$ for some transport map
$T$.


\end{proof}	






\section{% Establishing bounds on the $C^k$ norm
  Regularity of the velocity field $f$}\label{sec:regularity}
 As we have seen in Theorem \ref{thm:f}, for a transport
  $T:\Omega_0\to\Omega_1$ as in Assumption \ref{ass:T}, there exists a
  unique velocity field $f:\Omega_{[0,1]}\to\R^d$ such that
  $T(x)=X_f(x,1)$ for all $x\in\Omega_0$. This $f$ is the unique
  minimizer of the objective \eqref{eq:OBJ}. Furthermore we have an
  explicit formula for $f$: With
  $G:\Omega_0\times [0,1]\to\Omega_{[0,1]}$,
  $G(x,t)  \coloneqq  (tT(x) + (1-t)x, t)$ define $F:\Omega_{[0,1]}\to\Omega_0$
  as the first $d$ components of $G^{-1}$, then
  \begin{equation}\label{eq:fexpl}
    f(y,s) = T(F(y,s)) - F(y,s)\qquad\forall (y,s)\in\Omega_{[0,1]}.
  \end{equation}
  Based on \eqref{eq:fexpl}, in this section we investigate the
  regularity of the velocity field $f$.

  As we will see, $f$ inherits the regularity of $T$, in the sense
  that $T\in C^k(\Omega_0)$ implies $f\in
  C^k(\Omega_{[0,1]})$. Essentially, this follows by the inverse
  function theorem, which yields $G^{-1}\in C^k(\Omega_{[0,1]})$ so
  that $f$ in \eqref{eq:fexpl} is a composition of $C^k$
  functions. Since the approximability of $f$ by neural networks
  crucially depends on $\|f\|_{C^k(\Omega_{[0,1]})}$ (see Section
  \ref{sec:NN} ahead), we will carefully derive a precise bound on
  this norm. We proceed as follows: In Section \ref{sec:genreg}, we
  discuss regularity of $f$ for arbitrary transport maps
  $T$. Subsequently, in Section \ref{sec:trireg} we deepen the
  discussion in the special case of triangular transport maps (which
  yield triangular velocity fields $f$).





\subsection{General transports}\label{sec:genreg}
To bound the norm of $f$ in $C^k(\Omega_{[0,1]})$, our proof
  strategy is to first upper bound $\|F\|_{C^k(\Omega_{[0,1]})}$, and
  then use a version of Fa\'{a} di Bruno's formula to estimate the
  norm of the composition of $F$ with $T$.

  % Now it remains to bound $\|F\|_{C^k(\Omega)}$. As noted before, $F(y,t)$ is the projection of the inverse of the forward map $G(x,t) = (tT(x) + (1-t)x, t)$ to the first $d$-coordinates, which is determined by the map $T$. As a general result, we shall derive the $C^k$ norm of the inverse of an operator based on the $C^k$ of the operator self. For this purpose, we find it easier to work in the more general setting of Banach spaces. Before we proceed, however, we shall prove the Banach space version of the Fa\'{a} di Bruno's formula. We note that the proof is essentially similar to the proof for Fa\'{a} di Bruno's formula in the case of a single real variable and is based on Taylor expansions using Fr\'echet derivatives in Banach spaces.

  Since $F$ is obtained as the first $d$ components of the inverse
  $G:\Omega_{[0,1]}\to\Omega_0\times [0,1]$, we first provide some
  abstract results about % the of inverse functions.
  how to bound the $k$th derivative of an inverse function. We start
  by recalling Fa\'{a} di Bruno's formula in Banach spaces. For
  completeness we have added the proof in Appendix \ref{app:AuxResults}, but emphasize
  that the argument is the same as for real valued functions \cite{}. In the
  following, for a multilinear map $A\in L^n(X,Y)$ we write $A(x^n)$
  to denote $A(x,\dots,x)$.

  \begin{theorem}[Fa\'{a} di Bruno]\label{thm:FdB}
    Let $k\in\N$.
  Let $X$, $Y$ and $Z$ be three Banach spaces, and let
  $F\in C^k(X,Y)$ and $G\in C^k(Y,Z)$.

  Then for all $0\le n\le k$ and with
  % Let $g:B_1\rightarrow B_2$ and $f: B_2\rightarrow B_3$ be $C^k$ functions on Banach spaces, then we have for $n < k$
  $T_n := \setl{\bsalpha \in\N^n}{\sum_{j=1}^n j\alpha_j =
    n}$, % it holds for
  for all $x$, $h\in X$ the $n$th derivative
  $[D^n(G\circ F)](x)(h^n)\in Z$ of $G\circ F$ at $x$ evaluated at
  $h^n\in X^n$ equals
  % \jztd{changed $\bsk$ to $\bsalpha$ since $k$ already
  %   has a meaning here}
  \begin{equation*}
     \sum_{\bsalpha\in T_n}% \frac{n!}{\alpha_1!\ldots \alpha_n!}
    \frac{n!}{\bsalpha!}
    [D^{|\bsalpha|}G](F(x)) \Bigg(\smash[b]{\underbrace{\frac{[DF(x)](h)}{1!},\ldots, \frac{[DF(x)](h)}{1!}}_{\text{$\alpha_1$ times}}},\ldots,\underbrace{\frac{[D^{n}F(x)](h^n)}{n!},\ldots, \frac{[D^{n}F(x)](h^n)}{n!}}_{\text{$\alpha_n$ times}}\Bigg).
  \end{equation*}

% $$[D^n(f\circ g)](y) = \sum_{\bsk\in T_n}\frac{n!}{k_1!\ldots k_n!}[D^{|\bsk|}f](g(y)) \left(\smash[b]{\underbrace{\frac{[Dg](y)}{1!},\ldots, \frac{[Dg](y)}{1!}}_\text{$k_1$ times}},\ldots,\smash[b]{\underbrace{\frac{[D^{n}g](y)}{n!},\ldots, \frac{[D^{n}g](y)}{n!}}_\text{$k_n$ times}}\right),$$ where $T_n = \{\bsk = (k_1,\ldots,k_n) : \sum_{j=1}^n jk_j = n\}$. 
\end{theorem}
Additionally we need the inverse function theorem, the proof of which can also be found in Appendix \ref{app:AuxResults}. 
\begin{theorem}[Inverse function theorem]\label{thm:invfunc}
  Let $k\ge 1$, let $X$, $Y$ be two Banach spaces, and let $F\in C^k(X,Y)$.
  At every $x\in X$ for which $DF(x)\in L^1(X,Y)$ is an isomorphism,
  there exists an open neighbourhood $O\subseteq Y$ of $F(x)$ and a function
  $G\in C^k(O,X)$ such that $F(G(y))=y$ for all $y\in O$.

  Moreover, for every $n\le k$ there exists a continuous function
  $C_n:\R_+^{n+1}\to \R_+$ (independent of $F$, $G$, $O$) such
  that for $y=F(x)$ with $x$ as above
  \begin{equation}\label{eq:Cn}
    \|D^n G(y)\|_{\cL^n_{\sym}(Y;X)} \le C_n(\|[DF(x)]^{-1}\|_{\cL^1_{\sym}(Y;X)}, \|DF(x)\|_{\cL^1_{\sym}(X;Y)}, \ldots, \|D^nF(x)\|_{\cL^n_{\sym}(X;Y)}).
  \end{equation}
\end{theorem}

We start by giving a bound on the derivatives of the composition of
functions.

\begin{corollary}\label{cor:composition}
  Let $S_1\subseteq X$, $S_2\subseteq Y$, $S_3\subseteq Z$ be three
  open subsets of the Banach spaces $X$, $Y$ and $Z$.
  Suppose $k\in\N$ and
  $F\in C^k(S_1,S_2)$ and $G\in C^k(S_2,S_3)$.

  Then $\|G\circ F\|_{C^k(S_1)}\le k^k\exp(k)\|G\|_{C^k(S_2)}
  \max\{1,\|F\|_{C^k(S_1)}\}^k$.
  \end{corollary}
  \begin{proof}
    By Theorem \ref{thm:FdB}, for all $x\in S_1$
    \begin{align}\label{eq:DnFG}
    \|D^n(G\circ F)](x)\|_{\cL^n_{\sym(X;Y)}}
    &\le
      \|G\|_{C^n(S_2)}
      \sum_{\bsalpha\in T_n}
      \frac{n!}{\bsalpha!}
      \prod_{j=1}^n \frac{\|F\|_{C^j(S_1)}^{\alpha_j}}{(j!)^{\alpha_j}}\nonumber\\
    &\le
      \|G\|_{C^n(S_2)}\max\{1,\|F\|_{C^j(S_1)}\}^n
      \sum_{\bsalpha\in T_n}
      \frac{n!}{\bsalpha!}\prod_{j=1}^n\frac{1}{(j!)^{\alpha_j}},
  \end{align}
  where we used $\sum_{j=1}^n \alpha_j\le n$ for all
  $\bsalpha\in T_n$, and that $\|F\|_{C^j(S_1)}\le\|F\|_{C^n(S_1)}$
  for all $j\le n$ by definition of the norm.
  By Lemma \ref{lemma:stirling}, the last sum in \eqref{eq:DnFG} is
  bounded by $n^n$. Finally \eqref{eq:symnorm} and the definition
  of the $C^n$-norm in \eqref{eq:Ck} imply claim.
\end{proof}


We next use Faa di Bruno's formula, to bound the derivatives of the
inverse $G^{-1}$ of a function $G$.
\begin{proposition}\label{prop:NormInverse}
  Consider the setting of Theorem \ref{thm:invfunc}. Let
  $S\subseteq X$ be open such that $DG(x)\in L(X,Y)$ is an isomorphism
  for each $x\in S$ and let there be a continuous function
  $F:G(S)\to X$ such that $F(G(y))=y$ for all $y\in
  G(S)$. Additionally assume that for some $\gamma>0$
  \begin{equation}\label{eq:gammaass}
    \sup_{x\in S}\|[DG(x)]^{-1}\|_{\cL^1(Y;X)} \le\gamma\qquad\text{and}\qquad
    \|G\|_{C^k(S)}\le\gamma.
  \end{equation}

  Then % for all $n\le k$ holds with $\e=\exp(1)$
  \begin{equation}\label{eq:NormInverse}
    \|D^nF(y)\|_{\cL^n(Y,X)}\le \e^kk^{k^2}\gamma^{3k-2}
    \qquad\forall y\in G(S). 
  \end{equation}
\end{proposition}
\begin{proof}[Proof of Proposition \ref{prop:NormInverse}]
    We proceed by induction over $n$, and will show that for all
    $y\in G(S)$ and all $1\le n\le k$
   \begin{equation}\label{eq:indclaimNI}
      \|D^n F(y)\|_{\cL_{\sym^n(X;Y)}}\le n^{n^2}\gamma^{3n-2}.
    \end{equation}
    Then \eqref{eq:symnorm} implies the claim.

    For $n=1$,
    ${\rm Id} = D(G\circ F)(y)$ for all $y \in G(S)$. By the chain
    rule, $(D(G\circ F)(y)\circ (DF(y)) = {\rm Id}$, so that
    $DF(y) = [DG (F(y))]^{-1}$. Thus $\|DF(y)\|_{\cL^1(X;Y)}\le \gamma$ by
    \eqref{eq:gammaass}. This implies \eqref{eq:indclaimNI} for
    $n=1$.

    Now let $n \ge 1$. Then for any $y\in G(S)$ by \eqref{eq:Dng}
\begin{align*}
\|D^nF(y)\|_{\cL^n_{\sym(Y;X)}} \le &\|([DG](F(y)))^{-1}\|_{\cL^1(Y;X)}\\ &\cdot \left ( \sum_{\bsalpha\in \bar{T}_n}\frac{n!}{\bsalpha!}\|D^{|\bsalpha|}G(F(y))\|_{\cL^{|\bsalpha|}(X;Y)} \prod_{m=1}^{n-1} \left (\frac{\|D^m F(y)\|_{\cL_{\sym^m(Y;X)}}}{m!} \right )^{\alpha_m}\right).
\end{align*}
Using the induction Assumption \eqref{eq:indclaimNI} to bound
$\|D^mF(y)\|_{\cL_{\sym^m(Y;X)}}\le m^{m^2}\gamma^{3m-2}$ for $1\le m<n$, we
find for $y\in G(S)$
\begin{align*}
  \|D^nF(y)\|_{\cL_{\sym^n(Y;X)}} &\le \gamma^2 \left ( \sum_{\bsalpha\in
                           \bar{T}_n}\frac{n!}{\bsalpha!} \prod_{m=1}^{n-1} \left (\frac{m^{m}\gamma^{3m-2}}{m!} \right )^{\alpha_m}\right)\nonumber\\
                                &\le \gamma^2
                                  m^{\sum_{m=1}^{n-1}m^2\alpha_m}\gamma^{3n-4}
\left ( \sum_{\bsalpha\in
                           \bar{T}_n}\frac{n!}{\bsalpha!} \prod_{m=1}^{n-1}\frac{1}{(m!)^{\alpha_m}} \right)\\
                           &\le \gamma^{3n-2}n^{(n-1)n}\left ( \sum_{\bsalpha\in
                           \bar{T}_n}\frac{n!}{\bsalpha!} \prod_{m=1}^{n-1}\frac{1}{(m!)^{\alpha_m}} \right) ,
\end{align*}
where we have used $\sum_{m=1}^{n-1}m\alpha_m=n$ and
$\sum_{m=1}^{n-1} \alpha_m\ge 2$ for all $\bsalpha\in \bar T_n$. The
term in the sum is bounded by $n^n$ according to Lemma
\ref{lemma:stirling}.  Thus for $2\le n\le k$,
\begin{equation*}
  \|D^nF(y)\|_{\cL_{\sym}^n(Y;X)}\le n^{n^2}\gamma^{3n-2}
\end{equation*}
which shows \eqref{eq:indclaimNI} and concludes the proof.
\end{proof}
%\begin{remark}\rrtd{Do we still need this? }
 % In Prop.~\ref{prop:NormInverse} we assume a uniform bound $\gamma$
 % on all up to the $k$th derivative. Typically, derivatives of a
 % function exhibit a certain growth, for example
 % $\|D^n G(x)\|\le \gamma^n n!$ holds for all $n\in\N$ in case $G$ is
 % an analytic function in a neighbourhood of $x$. As the inverse of an
 % analytic function is again analytic, it will then also hold
 % $\|D^n F(G(x))\|\le \beta^n n!$ for some $\beta>0$. More general, if
 % we assume $\sup_{x\in S}\|D^n G(x)\|\le \gamma^n n!$ only for all
 % $n\le k$, then a slightly more involved version of
 % Prop.~\ref{prop:NormInverse} gives $\|F\|\le n! (\gamma n)^{n^2}$.
%\end{remark}



  % When using measure-transport based sampling methods in practice, it
  % is common to not learn a map that directly pushes forward source to
  % the target, but instead to learn a composition of maps that pushes
  % forward source to the target through a sequence of intermediate
  % measures. As we will show, if two measures are not too far from each
  % other (in a proper sense measured by the transport map), then the
  % size of the neural network in the neural ODE whose time-one flow map
  % pushes one measure onto the other scales linearly with respect to
  % the dimension $d$, which partly answers the question why this
  % approach can potentially offer benefits in practice.

% \begin{theorem}\label{thm:velocityfieldNormBound}
% In the context of theorem \ref{thm:ExSol}, suppose that we have the following additional assumptions about the transport map $T$:
% \begin{itemize}
%     \item There exists a constant $C \geq 1$ such that $\|D^n_xT\|_{L^\infty(\Omega_0,L^n(\mathbb{R}^d;\mathbb{R}^d))} \leq n!C^{n}$ for all $n \leq k$. \rrtd{do we really need $\Omega_0$ and $\Omega_1$ to live in the same ambient space? Also, probably need some discussion on why this assumption is reasonable (related to the analyticity of functions?)}
%     \item $\forall x\in\Omega_0\cup\Omega_1$, $|x| \leq M$.\rrtd{This could be derived from the next assumption?}
%     \item $\forall x$, the Jacobian matrix satisfies $\|I - \nabla_x T(x)\|_{L^\infty(\Omega_0, L^1(\mathbb{R}^d,\mathbb{R}^d))} \leq \delta < 1$. \rrtd{need to add a discussion about this assumption?}
    
% \end{itemize}
% Then, the $C^k$ norm of the velocity field $f$ can be bounded as $\|f\|_{C^k(\Omega)}\leq e^{\Theta(dC^{k^2}(1-\delta)^{1-2k}(2+2M-\delta)^{2k-1})}$ when treating $k$ as a constant. 
% \end{theorem}


% As noted, the assumption $\|I - \nabla T(x)\|_{L^\infty(\Omega_0, L^1(\mathbb{R}^d,\mathbb{R}^d))} \leq \delta < 1$ is not necessary to obtain the boundedness of $\|f\|_{C^k}$. For a general transport map $T$, we can also obtain bounds on the $C^k$ norm of the velocity field $f$ whose time-one flow map of the ODE realizes $T$.

We now present our first bound on $\|f\|_{C^k}$. The estimate depends
on $\|T\|_{C^k}$ as well as
\begin{equation}\label{eq:cT}
  c_T:= \sup_{t\in[0,1]}\sup_{x\in \Omega_0}\|(\nabla T_t(x))^{-1}\|_{\R^{d\times d}}.
\end{equation}
% By a similar argument
% as in Lemma \ref{lemma:spectrum}, we know that $c_T$ is finite
% in case the eigenvalues of $\nabla T(x)$ 
% .
We subsequently discuss situations in which we can give precise bounds
on this constant.





\begin{theorem}\label{thm:fNormGeneral}
  Let Assumption \ref{ass:dens1} be satisfied. Let $k\in\N$ and let
  $T\in C^k(\Omega_0,\Omega_1)$ satisfy Assumption \ref{ass:T}. Then
  for the velocity field $f:\Omega_{[0,1]}\to\R^d$ in \eqref{eq:fexpl} it
  holds with
\begin{equation*}
  \gamma :=\max\{2,1+c_T\}(1+\|T\|_{C^k(\Omega_0)}+\sup_{x\in\Omega_0}\|x\|),
\end{equation*}
that
\begin{equation*}
  \|f\|_{C^k(\Omega_{[0,1]})}\le 
 2k^{k^3 + k}\e^{k^2+k}\gamma^{3k^2-2k+1}.
\end{equation*}
\end{theorem}

\begin{proof}[Proof of Theorem \ref{thm:fNormGeneral}]
  Due to $f=T\circ F-F$ (cp.~\eqref{eq:fexpl}),
  \begin{equation}\label{eq:ftriangle}
    \|f\|_{C^k(\Omega_{[0,1]})} \leq \|T\circ F\|_{C^k(\Omega_{[0,1]})} + \|F\|_{C^k(\Omega_{[0,1]})}.
  \end{equation}
  Moreover, since $F$ is given as the first $d$ components of
  $G^{-1}:\Omega_{[0,1]}\to\Omega_0\times [0,1]$, it holds
  $\|F\|_{C^k(\Omega, \mathbb{R}^d)} \leq
  \|G^{-1}\|_{C^k(\Omega,\mathbb{R}^{d+1})}$.
  We start by bounding the latter norm.
  
  By definition of $G(x,t)=(T_t(x),t)=(tT(x)+(1-t)x,t)$,
  \begin{equation*}
    DG(x,t)=
        \begin{pmatrix}
          \nabla_x T_t(x)& T(x) - x \\
      0 & 1 \\
      \end{pmatrix}=
    \begin{pmatrix}
      t\nabla T(x) + (1-t)I & T(x) - x \\
      0 & 1 \\
      \end{pmatrix}.
    \end{equation*}
    An application of
    Lemma \ref{lemma:blockTriangle} and the assumption that
    $c_T=\sup_{x\in\Omega_0}\|(\nabla_xT_t(x))^{-1}\|<\infty$ gives for
    all $(x,t)\in \Omega_0\times [0,1]$
\begin{equation*}
  \|[DG(x,t)]^{-1}\|_{2}
  \le 1+(1+\|T-{\rm Id}\|_{L^\infty(\Omega_0)})\|(\nabla_xT_t(x))^{-1}\|_{2}
  \le 1+(1+\|T-{\rm Id}\|_{L^\infty(\Omega_0)}) c_T.
\end{equation*}

Next we bound the derivatives of $G$.
For $n=1$,
\begin{align*}
  \|DG(x,t)\|_{\cL^1(\R^{d+1};\R^{d+1})}\le \|\nabla_x T_t(x)\|_2+\|T(x)-x\|_2+1
  &\le \|T\|_{C^1(\Omega_0)}+1+\|T-{\rm Id}\|_{C^0(\Omega_0)}+1\nonumber\\
  &\le 2+2\|T\|_{C^1(\Omega_0)}+\max_{x\in\Omega_0}\|x\|_2.
\end{align*}
For $n\ge 2$, we first write $G(x,t)=(G_1(x,t),G_2(x,t))$ where
$G_1(x,t)=T_t(x)$ and $G_2(x,t)=t$. Then for $2\le n\le k$ and
$(x,t)\in\Omega_0\times [0,1]$
\begin{equation*}
  \|D^nG(x,t)\|_{\cL^n(\R^{d+1};\R^{d+1})}\le
  \|D^nG_1(x,t)\|_{\cL^n(\R^{d+1};\R^{d})}+
  \|D^nG_2(x,t)\|_{\cL^n(\R^{d+1};\R)}.
\end{equation*}
The second term is bounded by $1$ since $t\in [0,1]$. For the first term,
due to $D_t^2G_1(x,t)\equiv 0$,
\begin{align*}
  &\|D^n G_1(x,t)\|_{\cL^n(\R^{d+1};\R^{d+1})}
    \le \|D_x^n G_1(x,t)\|_{\cL^n(\R^d;\R^d))}
    +\|D_x^{n-1} (T(x)-x)\|_{\cL^{n-1}(\R^d;\R^d)}\nonumber\\
  &\qquad \le \|D_x^n (x)\|_{\cL^n(\R^{d};\R^{d})}+\|D_x^n T(x)\|_{\cL^n(\R^{d};\R^{d})}
    +\|D_x^{n-1} T(x)\|_{\cL^{n-1}(\R^d;\R^d)}
    +\|D_x^{n-1} (x)\|_{\cL^{n-1}(\R^d;\R^d)}\nonumber\\
  &\qquad\le \|D_x^n T(x)\|_{\cL^n(\R^{d};\R^{d})}+\|D_x^{n-1} T(x)\|_{\cL^{n-1}(\R^{d};\R^{d})}+1.
\end{align*}
We conclude with $M:=\max_{x\in\Omega_0}\|x\|_2$
that for all $0 \le n\le k$ and all $(x,t)\in \Omega_0\times [0,1]$
\begin{equation*}
  \|D^nG(x,t)\|_{\cL^n_{\sym{\R^{d+1};\R^{d+1}}}}\le 2(\|T\|_{C^n(\Omega_0)}+1+M)\le 2(\|T\|_{C^k(\Omega_0)}+1+M).
\end{equation*}


With
\begin{equation*}
  \gamma :=\max\{2,1+c_T\}(\|T\|_{C^k(\Omega_0)}+1+M),
\end{equation*}
Proposition \ref{prop:NormInverse} then implies
\begin{equation*}
  \|F\|_{C^k(\Omega_{[0,1]})}\le
  \|G^{-1}\|_{C^k(\Omega_{[0,1]})}\le \e^kk^{k^2}\gamma^{3k-2}.
\end{equation*}
Furthermore, Corollary \ref{cor:composition} and \eqref{eq:ftriangle}
yield
\begin{equation*}
  \|f\|_{C^k(\Omega_{[0,1]})}\le k^k\e^{k}\|T\|_{C^k(\Omega_0)}(\e^kk^{k^2}\gamma^{3k-2})^k+
  \e^kk^{k^2}\gamma^{3k-2}
  \le 2k^{k^3 + k}\e^{k^2+k}\gamma^{3k^2-2k+1}.
\end{equation*}
\end{proof}

Our main observation is, that $\|f\|_{C^k(\Omega_0)}$ behaves at worst
polynomial in $\|T\|_{C^k(\Omega_0)}$ and $c_T$ in \eqref{eq:cT}. One
important instance where we can give a precise bound on $c_T$, is if
$\nabla T$ is close to identity matrix $I_d\in\R^{d\times d}$ in the
sense $\|\nabla T(x)-I_d\|_2<1$. Since $T$ is a transport pushing
forward the source $\pi$ to the target $\rho$, this condition can be
interpreted as the source and the target not being too different.
\begin{lemma}
  Suppose that  $\sup_{x\in\Omega_0}\|\nabla
  T(x)-I_d\|_{2}=\delta<1$, where $I_d$ is $d$-by-$d$ identity matrix. Then the constant in \eqref{eq:cT}
  satisfies $c_T\le \frac{1}{1-\delta}$.
\end{lemma}
\begin{proof}
  The assumption implies that for all $t\in [0,1]$
  \begin{equation*}
    \|\nabla_x T_t(x)-I_d\|_2
    =\|t \nabla T(x)+(1-t)I_d-(tI_d+(1-t)I_d)\|_2
    = t \|\nabla T(x)-I_d\|_2 \le\delta.
  \end{equation*}
  Since for any $A\in \R^{d\times d}$ with $\|A-I_d\|_2=\delta<1$
  we have $A^{-1}=\sum_{j\in\N}(I-A)^j$ and thus
  $\|A^{-1}\|_2\le \frac{1}{1-\delta}$, the claim follows.
\end{proof}

Another instance where $c_T$ can be bounded is if $T$ is a triangular
transport. We next discuss this case in more detail.

\subsection{Triangular transports}\label{sec:trireg}
A special type of transport map commonly used in practice is the
  so-called Knothe-Rosenblatt (KR) map. To avoid further
  techincalities, throughout this section we restrict ourselves to
  measures on $d$-dimensional cubes, i.e.
  \begin{equation}\label{eq:cube}
    \Omega_0=\Omega_1=[0,1]^d.
  \end{equation}
  The KR map, is the unique transport satisfying \emph{triangularity}
  and \emph{monotonicity}. To formally introduce these notions, recall
  that we use the notation convention $x_{[j]}=(x_i)_{i=1}^j$.
  \begin{definition}
    We say that a map $T=(T_j)_{j=1}^d:[0,1]^d\to [0,1]^d$ is {\bf
      triangular} iff $T_j$ depends only on $x_{[j]}$ (but not on
    $x_{j+1},\dots,x_d$) for each $j=1,\dots,d$. A triangular map $T$
    is called {\bf monotone} iff for each $j=1,\dots,d$ the map
    $[0,1]\ni x_j\mapsto T_j(x_{[j]})$ is differentiable and
    $\frac{\partial}{\partial x_j} T_j(x_{[j]})>0$ for all
    $x_{[j]}\in [0,1]^j$.
  \end{definition}
  % The KR map is the unique transport satisfying (i) triangularity in
  % the sense that for $T=(T_j)_{j=1}^d:\R^d\to\R^d$ its $j$th
  % component $T_j$ depends only on $x_{[j]}$ (recall the shortcut
  % $x_{[j]}:=(x_1,\dots,x_j)$) for each $j=1,\dots,d$ and (ii)
  % monotonicity in the sense that
  % $\frac{\partial}{\partial x_j} T_j(x_{[j]})>0$ for all
  % $x_{[j]}\in [0,1]^j$.  $[-1,1]\ni x_j\mapsto T_j(x_1,\dots,x_j)$
  % is \emph{strictly} monotonically increasing for all
  % $(x_{[j-1]})\in [-1,1]^j$.
  Under rather mild assumptions on the reference and target one can
  show existence and uniqueness of the KR map (\cite{OTAppliedMathematician}).
  % The KR map % can be constructed explicitly in terms of
  % the densities of the source and target measures, which is given in
  Moreover, it allows an explicit construction, which we recall in
  Appendix \ref{app:KRMap}. The goal of the present section is to
  derive bounds on the norms of the velocity field associated with the
  KR-map.
  % part, we shall derive bounds on the norm of the velocity field
  % associated with this triangular transport map, also explicitly in
  % terms of the densities.  For simplicity, we always assume the
  % domains $\Omega_1 = \Omega_2 = [0,1]^d$ when we work with
  % triangular transports.



  We start our analysis by pointing out that triangularity of the
  transport $T$ implies a similar structure for the corresponding
  velocity field $f$:
  \begin{lemma}
    Consider the setting of Theorem ~\ref{thm:f} and let $\Omega_0$,
    $\Omega_1$ be as in \eqref{eq:cube}. If $T:\Omega_0\to\Omega_1$ is
    triangular, then $f:\Omega_{[0,1]}\to\R^d$ is triangular in the
    sense $f(x,t)=(f_i(x_{[i]},t))_{i=1}^d$.
    % % In the context of
    % theorem \ref{thm:f}. % if we assume further that the
    % % underlying transport map
    % Assume that
    % $T(x)$ is triangular and
    % monotone, and $\det
    % $\det(T_t(x)) > 0$, then the underlying velocity field is
    % also triangular with respect to the flow map.
  \end{lemma}
 
  \begin{proof}
    % Under the assumption that the field
    % $T$ is monotone and triangular,
    % we can write
  % $$T(x) = [T_1(x_1), T_2(x_1,x_2),..., T_d(x_1,..., x_d)]^T,$$ where
  % $T_i$ is the i-th component of the transport map and triangularity
  % implies the condition $\frac{\partial T_i}{\partial x_j} = 0$ for
  % all $j > i$.
  % Let as earlier $T_t(x)=(1-t)x+tT(x)$ and let
    Let the solution $X:\Omega_{0}\times [0,1]\to
    \R^d$ of \eqref{eq:ODE} satisfy $X(x,t) = tT(x) +(1-t)x$.
    % be the flow map of the ODE \eqref{eq:ODE}.
    Then for the velocity field
    $f:\Omega_{[0,1]}\to\R^d$ in \eqref{eq:ODE} it holds $f(X(x,t), t)
    = T(x) - x$, i.e.\ for each $i=1,\dots,d$
    \begin{equation*}
      f_i(X(x,t), t) = T_i(x_{[i]}) - x_i\qquad\forall (x,t)\in\Omega_{0}\times [0,1].
    \end{equation*}
    % Note that
    % $f_i$ is a function from $\mathbb{R}^d$ to
    % $\mathbb{R}$ that depend on $X_1(x_1,t), X_2(x_1,x_2,
    % t),...,X_d(x_1,..,x_d,t)$ where we used the fact that
    % $X(x,t)$ inherits the same triangular structure from $T$. For
    % $j>i$, if we take the derivative with respect to
    % $x_j$, only the terms involving $X_j, X_{j+1}, ...,
    % X_d$ will be involved. What we need to prove is that
    To prove the lemma we show that for all
    $i\in\{1,\dots,d\}$ it holds
    \begin{equation}\label{eq:claimtri}
      \frac{\partial}{\partial X_j}f_i(X(x,t), t) = 0
      \qquad\forall j>i,\quad\forall (x,t)\in\Omega_0\times [0,1].
    \end{equation}

    Fix $i\in\{1,\dots,d\}$. To prove \eqref{eq:claimtri}, we proceed
    by induction over $j=i+1,\dots,d$ starting with $j=d$. Note that
    the triangularity of $T$ implies that also
    $X(x,t)=(X_l(x_{[l]},t))_{l=1}^d$ inherits this triangular
    structure. Hence $\frac{\partial}{\partial x_d}X_l(x_{[l]})=0$ for
    all $l<d$.  Consequently
    \begin{equation*}
      \frac{\dd}{\dd x_d} f_i(X(x,t),t) = \frac{\partial}{\partial X_d} f_i(X(x,t),t)
      \frac{\partial}{\partial x_d} X_d(x,t)=0\qquad\forall
      (x,t)\in\Omega_{0}\times [0,1].
    \end{equation*}
    By the monotonicity assumption on $T$ it holds
    $\frac{\partial T_d(x)}{\partial x_d}>0$, and therefore
    \begin{equation*}
      \frac{\partial}{\partial x_d} X_d(x,t) = t\frac{\partial
      }{\partial x_d}T_d(x)+(1-t)>0\qquad\forall
      (x,t)\in\Omega_{0}\times [0,1].
    \end{equation*}
    Hence $\frac{\partial f_i(X(x,t),t)}{\partial X_d}=0$ for all
    $(x,t)\in\Omega_0\times [0,1]$.
    % i.e.\ $f_i(x,t)$ does not depend on $x_d$ for all
    % $(x,t)\in\Omega_{[0,1]}$.

    % $\frac{\dd}{\dd x_d}f_i(X(x,t), t) = \frac{\partial
    % f_i}{\partial X_d}\frac{\partial X_d}{\partial x_d} = 0$ and we
    % may conclude that Since by
    % the monotonicity assumption, ,
    % we have
    % $\frac{\partial X_d}{\partial x_d} = t\frac{\partial
    % T_d}{\partial
    % x_d}+(1-t)$, which is a convex combination of two positive
    % numbers. Therefore, we must have
    % $\frac{\partial f_i}{\partial X_d} = 0$.
    Now suppose \eqref{eq:claimtri} is true for all $j=k+1,\dots,d$
    and some $k\ge i$. Then, using
    $\frac{\partial X_j(x,t)}{\partial x_k}=0$ whenever $k>j$ and as
    well as \eqref{eq:claimtri} whenever $j>k$
    % $j = d - k > i$, we have $f_i$ depends on $x_j$ through the
    % terms $X_{d-k}, X_{d-k+1}, ..., X_{d}$. The chain rule gives
    \begin{equation*}
      \frac{\dd}{\dd x_k}f_i(X(x,t), t) =
      \sum_{j=1}^d\frac{\partial}{\partial X_{j}}f_i(X(x,t),
      t)\frac{\partial }{\partial x_{k}}X_{j}(x,t)
      =\frac{\partial}{\partial X_{k}}f_i(X(x,t),
      t)\frac{\partial }{\partial x_{k}}X_{k}(x,t)=0,
      % \nonumber\\
      % &=\sum_{r=k}^d\frac{\partial}{\partial X_{r}}f_i(X(x,t),
      % t)\frac{\partial }{\partial x_{k}}X_{r}(x,t)\nonumber\\
      % &=\frac{\partial}{\partial X_{r}}f_i(X(x,t), t)\frac{\partial
      % }{\partial x_{k}}X_{r}(x,t) \qquad\forall
      %   (x,t)\in\Omega_0\times [0,1].
    \end{equation*}
    for $(x,t)\in\Omega_0\times [0,1]$. Since as before
    % $\frac{\dd f_i(X(x,t), t)}{\partial X_{k}} = 0$ for $l < k$, we
    % are left with
    % $\frac{\partial}{\partial X_{j}}f_i(X(x,t), t)\frac{\partial
    % X_{j}}{\partial x_{j}}$. By monotonicity again, we have
    $\frac{\partial X_{k}(x_{[k]},t)}{\partial x_{k}} > 0$ we find
    $\frac{\partial f_i(X(x,t), t)}{\partial X_{j}} = 0$.
  \end{proof}

  In Theorem ~\ref{thm:LipDomain} we showed that the domain
  $\Omega_{[0,1]}$ (cp.~\eqref{eq:Omega}) of the velocity field $f$ is
  a Lipschitz domain. For triangular maps and if \eqref{eq:cube} it
  even holds $\Omega_{[0,1]}=[0,1]^d\times [0,1]$, i.e.\ the
  trajectories of the solutions to \eqref{eq:ODE} cover the whole
  $d+1$ dimensional cube:

  \begin{proposition}\label{prop:cube}
    % Consider the setting of Thm.~\ref{thm:f}, and additionally
    % suppose
    % \eqref{eq:cube} and that
    Let $T:[0,1]^d\to [0,1]^d$ be a monotone, triangular and bijective
    map.
    % and bijective triangular map from
    % $[0, 1]^d \rightarrow [0,1]^d$, then the trajectories of the
    % flow
    % map of the ODE, $X(x,t) = tT(x) + (1-t)x$ cover the entire
    % space-time $[0,1]^d \times [0, 1]$.
    Then
    \begin{equation*}
      \Omega_{[0,1]}% =\setl{(tT(x)+(1-t)x,t)}{[0,1]^d\times [0,1]}
      = [0,1]^d\times [0,1].
    \end{equation*}
  \end{proposition}
  \begin{proof}
    It is easy to see, that for a monotone triangular map,
    $T:[0,1]^d\to [0,1]^d$ being bijective is equivalent to
    $x_j\mapsto T_j(x_{[j]})$ being bijective from $[0,1]\to [0,1]$
    for each $j=1,\dots,d$, see e.g., \cite[Lemma 3.1]{ZM2}.
    
    Denote $T_t(x)=(1-t)T(x)+tx$ and additionally
    $T_{t,j}(x)=(1-t)T_j(x_{[j]})+tx_j$
    $(x,t)\in\Omega_0\times [0,1]$.  For every $t\in [0,1]$,
    $T_t:[0,1]^d\to [0,1]^d$ is a convex combination of two monotone,
    triangular bijective maps. Hence $T_t:[0,1]^d\to [0,1]^d$ is also
    triangular. Moreover, for each $j\in\{1,\dots,d\}$ and
    $t\in [0,1]$, $x_j\mapsto T_{t,j}(x_{[j]})$ is a convex
    combination of two monotonically increasing maps that bijectively
    map $[0,1]$ onto itself, and thus this function has the same
    property. In all this shows that $T_t:[0,1]^d\to [0,1]^d$ is
    monotone, triangular and bijective for each $t\in [0,1]$, which
    implies the claim.
  \end{proof}

  We wish to apply Theorem \ref{thm:fNormGeneral} to bound
  $\|f\|_{C^k(\Omega)}$. To do so, it remains to bound
  $\|T\|_{C^k([0,1]^d, [0,1]^d)}$ and $c_T$ as below. 


\begin{lemma}\label{lemma:cT}
  Let $\pi, \rho$ be densities supported on $[0,1]^d$ and satisfy regularity Assumption \ref{ass:dens2}. Let $T\in C^1([0,1]^d, [0,1]^d)$ be the Knothe-Rosenblatt map such that $T_\sharp\pi = \rho$. Then
  the constant $c_T$ from \eqref{eq:cT} satisfies
  $$c_T := \sup_{x\in[0,1]^d}\|(\nabla T_t(x))^{-1}\|_{R^{d\times d}}
  \leq (\frac{L_1}{L_2})^{2d}\max\{1, \|T\|_{C^1([0,1]^d)}\}^{d-1}.$$
\end{lemma}
\begin{proof}
  In this proof we use the notation and construction of the transport map provided in \cref{app:KRMap}. In particular $\pi_i$ is the marginal density of $\pi$ in $(x_1,\dots,x_i)$, and with $F_{\rho,i}$, $F_{\pi,i}$ as in \eqref{eq:Fpik}, we let
    \begin{equation*}
      G_{\rho, i}(x_{[i-1]}, \cdot) = F_{\rho,i}(x_{[i-1]}, \cdot)^{-1}.
  \end{equation*}


  
  
  By % the
  construction, the Jacobian $\nabla T$ is a triangular
  matrix. We shall compute the diagonal entries of the Jacobian
  matrix, which are the eigenvalues. % Let
  % $G_{\rho, i}(x_{[i-1]}, \cdot) = F_{\rho,i}(x_{[i-1]}, \cdot)^{-1}$,
  % where $F_{\rho,i}$ and $F_{\pi,i}$ are introduced in appendix
  % \ref{app:KRMap}.  Then we have
  By \eqref{eq:Tk}
  $$T_i(x) = G_{\rho, i}(T_1(x_1),...,T_{i-1}(x_{[i-1]}), F_{\pi,
    i}(x)).$$ Taking derivatives in $x_i$, we have
$$\partial_{x_i}T_i(x) = \partial_{x_i}G_{\rho, i}(T_1(x_1),...,T_{i-1}(x_{[i-1]}), \pi_i(x))\partial_{x_i}F_{\pi,i}(x).$$

Recall that $F_{\pi,i}(x)$ is the CDF of $x_i$ when viewing
$x_{[i-1]}$ as fixed, thus we have
$\partial_{x_i}F_{\pi,i}(x) = \pi_i(x)$. Note that
$G_{\rho,i}(x_{[i-1]}, F_{\rho,i}(x)) = x_i$. Taking derivative in
$x_i$, we have
$$(\partial_{x_i}G_{\rho,i})(x_{[i-1]}, F_{\rho,i}(x))(\partial_{x_i}F_{\rho,i}(x)) = 1.$$

Note that $F_{\rho,i}(x_{[i-1]}, \cdot):[0,1]\rightarrow[0,1]$ is a
bijection. We make the substitution $y_i = F_{\rho,i}(x)$ and we have
for all $(x_{[i-1]}, y_i)\in[0,1]^{i-1}\times[0,1]$,
$$(\partial_{x_i}G_{\rho,i})(x_{[i-1]}, y_i) = \frac{1}{\partial_{x_i}F_{\rho,i}(x_{[i-1]}, G_{\rho,i}(x_{[i-1]}, y_i))}.$$
Hence
% , we obtain
$$\partial_{x_i}T_i(x_{[i]}) = \frac{\pi_i(x_{[i]})}{\rho_i(T_1(x_1),...,T_{i-1}(x_{[i-1]}), G_{\rho,i}(T_1(x_1),...,T_{i-1}(x_{[i-1]}),F_{\pi,i}(x)))}.$$


By Assumption \ref{ass:dens2}, $\rho$ and $\pi$ are bounded from above
and below by $L_1$ and $L_2$. % Then we have
Thus $\pi_i(x_{[i]})$ and
$\rho_i(x_{[i]})$ are bounded from above and below by
$\frac{L_1}{L_2}$ and $\frac{L_2}{L_1}$, % Then
and $\partial_{x_i}T_i(x_{[i]})$ is bounded from below by
$\frac{L_2^2}{L_1^2}$. Note that the diagonal entries of
$\nabla_x T_t(x)^{-1} = [(1-t)I_{d\times d} + t\nabla_F
T(F(y,t))]^{-1}$ are exactly $\{\frac{1}{1-t + t\sigma_i}\}_{i=1}^d$,
which are lower bounded by
$\frac{L_2^2}{(1-t)L_2^2 + tL_1^2} \geq \frac{L_2^2}{L_1^2}$.

Therefore, we have
$\inf_{x\in\Omega_0}\det (\nabla_x T(x)) \geq
\left(\frac{L_2}{L_1}\right)^{2d}$. Applying
 Lemma~\ref{lemma:NormTtinverse} then gives the result.
\end{proof}

% In order to apply Based on based on the densities, which will in
% particular give the upper bound for $\|T\|_{C^1([0,1]^d)}$ appearing
% in the bound in Lemma.\ref{lemma:NormTtinverse}.

When $T$ is  the KR triangular map, $\|T\|_{C^K}([0,1]^d,[0.1]^d)$ can be computed explicitly in terms of densities as we see below. 
\begin{theorem}\label{thm:CkNormTriangular}
  Let $\Omega_0=\Omega_1=[0,1]^d$ and let $\pi$, $\rho$ satisfy
    Assumption \ref{ass:dens2} with the constants $k\ge 2$,
    $0<L_1\le L_2<\infty$.

    Then there exist constants $C_{k,d}$ (depending on $k$ and $d$ but independent of
    $\rho$, $\pi$)
    and $\beta_d>0$ (depending on $d$ but independent of $k$,
    $\rho$, $\pi$)
    such that the KR map $T$ pushing forward $\pi$
    to $\rho$ satisfies
    \begin{equation}
      \|T\|_{C^k([0,1]^d, [0,1]^d)} \leq C_{k,d}\left(\frac{L_1}{L_2}\right)^{\beta_d k^{d+1}}.
    \end{equation}
\end{theorem}
\begin{proof}
Throughout this proof we use the notation and explicit
    construction of the KR map introduced in Appendix \ref{app:KRMap}: The
    $i$-th component of the KR map can then be expressed as
    \begin{equation}\label{eq:Tixi}
      T_i(x_{[i]}) = G_{\rho, i}(T_1(x_1), ..., T_{i-1}(x_{[i-1]}), F_{\pi,
        i}(x_{[i]})).
    \end{equation}
    Here
    $F_{\rho, i}(x_{[i]})= \int_{0}^{x_i} \rho_i(x_{[i-1]}, t_i)dt_i$
    with $\rho_i = \frac{\hat{\rho}_i}{\hat{\rho}_{i-1}}$, where
    \begin{equation}\label{eq:hatrhoi}
      \hat{\rho}_i(x_1,...x_i) =
      \int_{[0,1]^{d-i}}\rho(x_1,...,x_d)dx_{i+1}...dx_d.
    \end{equation}
    The function $F_{\pi, i}(x_{[i]})$ is defined analogous with
    $\rho$ replaced by $\pi$.  Finally,
    $x_i\mapsto G_{\rho, i}(x_{[i]})$ is defined as the inverse of
    $x_i\mapsto F_{\rho, i}(x_{[i]})$ on $[0,1]$, i.e.
    \begin{equation}\label{eq:Grhoidef}
      G_{\rho,i}(x_{[i-1]},F_{\rho,i}(x_{[i]}))=x_i.
    \end{equation}


    The proof proceeds as follows: In step 1 we bound
    $\|F_{\pi,i}\|_{C^k(\ci)}$, $\|F_{\rho,i}\|_{C^k(\ci)}$, and in
    step 2 we bound $\|G_{\rho,i}\|_{C^k(\cd)}$. In Step 3 we use
    induction over $i$ to bound the norm of $T_i$ in \eqref{eq:Tixi}.

    {\bf Step 1.} Fix $i\in\{1,\dots,d\}$. In this step we show
    \begin{equation}\label{eq:Step1Claim}
      \max\{\|F_{\pi,i}\|_{C^k(\ci)},\|F_{\rho,i}\|_{C^k(\ci)}\}\le C
      \Big(\frac{L_1}{L_2}\Big)^{k+1}
    \end{equation}
    for some constant $C$ depending on $d$ and $k$ but independent of
    $\pi$ and $\rho$. Since our assumptions on $\rho$ and $\pi$ are
    identical, it suffices to prove \eqref{eq:Step1Claim} for $\rho$.

  Fix a multiindex $\bsv\in\N_0^d$. If $v_i=0$, then $D^\bsv F_{\rho,
    i} = \int_0^{x_i}D^\bsv \rho_i(x_{[i-1]},
  t_i)dt_i$.  Otherwise with $\bsv' = \bsv -
  \bse_i$ (where $\bse_i=(\delta_{i,j})_{j=1}^d$) it holds $D^\bsv
  F_{\rho, i} = D^{\bsv'} \rho_i(x_{[i-1]},
  x_i)$. In either case $\|D^\bsv F_{\rho, i}\|_{L^\infty([0,1]^i)}
  \leq \|\rho_i\|_{C^k([0,1]^i)}$. Thus $\|F_{\rho,
    i}\|_{C^k([0,1]^i)} \leq \|\rho_i\|_{C^k([0,1]^i)}$.

  Similarly, Assumption \ref{ass:dens2} implies $$\hat\rho_i(x_1,...,x_i) = \int_{[0,1]^{d-i}}\rho(x_1,...x_d)dx_{i+1}...dx_d >  \int_{[0,1]^{d-i}}L_2dx_{i+1}...dx_d > L_2$$ and $$D^\bsv\hat\rho_i(x_1,...,x_i) = \int_{[0,1]^{d-i}}D^\bsv\rho(x_1,...x_d)dx_{i+1}...dx_d \leq \int_{[0,1]^{d-i}}L_1dx_{i+1}...dx_d = L_1$$ for all $(x_1,..,x_i)\in [0,1]^i$ and multi-index $\bsv\in\N_0^d$ with $|\bsv|\leq k$. Thus we can conclude that $\|\hat\rho_i\|_{C^k([0,1]^i)}\le L_1$ and Lemma \ref{lemma:NormReciprocal}
  gives
  $\|D^{n-j}(\frac{1}{\hat{\rho}_{i-1}})\|_{L^\infty([0,1]^{i-1})}
  \leq C_{n-j}\frac{L_1^{n-j}}{L_2^{n-j+1}}$ for some constant
  $C_{n-j}$ that is independent of $L_1$, $L_2$. By the Leibniz rule
  \begin{equation*}
    D^n\rho_i = \sum_{j=0}^n {n \choose j}D^j\hat{\rho}_i
    D^{n-j}\Big(\frac{1}{\hat{\rho}_{i-1}}\Big)
  \end{equation*}
  and thus
  \begin{equation*}
    \|D^{n}\rho_i\|_{L^\infty([0,1]^{i})} \le CL_1\frac{L_1^n}{L_2^{n+1}} =
    C\frac{L_1^{n+1}}{L_2^{n+1}}    \qquad\forall n\in\{0,\dots,k\}
  \end{equation*}
  for some constant $C$ that depends on $k$ but is independent of
  $L_1$, $L_2$. In all this shows \eqref{eq:Step1Claim} for $\rho$.
  
  {\bf Step 2.}  Fix $i\in\{1,\dots,d\}$. In this step we show
  \begin{equation}\label{eq:Step2Claim}
    \max\{\|G_{\pi,i}\|_{C^k(\ci)},\|G_{\rho,i}\|_{C^k(\ci)}\}\le C
    \Big(\frac{L_1}{L_2}\Big)^{(k+1)(3k-2)}
  \end{equation}
  for some constant $C$ depending on $d$ and $k$ but independent of
  $\pi$ and $\rho$. As before, by symmetry it suffices to provide the
  bound for $\pi$.

  For $x_{[i]}\in[0,1]^i$ define
  \begin{equation}\label{eq:tildeFdef}
    \tilde{F}_{\rho, i}(x_{[i]}) := (x_{[i-1]}, F_{\rho,
      i}(x_{[i]}))\in [0,1]^i.
  \end{equation}
  Since $x_i\mapsto \tilde{F}_{\rho, i}(x_{[i]})$ is bijective from
  $[0,1]\to [0,1]$ for every fixed $x_{[i-1]}\in [0,1]^{i-1}$, the map
  $\tilde{F}_{\rho, i}:[0,1]^i\to [0,1]^i$ is a bijection.  So is its
  inverse which we denote by $\tilde{G}_{\rho, i}:[0,1]^i\to [0,1]^i$.
  It holds for all $x_{[i]}\in [0,1]^i$ that
  \begin{equation*}
    \tilde{G}_{\rho, i}(\tilde{F}_{\rho, i}(x_{[i]})) =
    x_{[i]}.
  \end{equation*}
  Due to \eqref{eq:Grhoidef} and the definition of $\tilde F_{\rho,i}$
  in \eqref{eq:tildeFdef}, the $i$th component of
  $\tilde{G}_{\rho, i}$ is given by $G_{\rho,i}$ and thus
  \begin{equation*}
    \|G_{\rho,i}\|_{C^k([0,1]^i, [0,1])}\leq \|\tilde{G}_{\rho, i}\|_{C^k([0,1]^i, [0,1]^i)}.
  \end{equation*}

  In the following we wish to apply Prop.~\ref{prop:NormInverse} to
  bound the right-hand side, which will yield a bound on the left-hand
  side.  To this end we first derive a bound on the norm of
  $\tilde F_{\rho,i}$. By \eqref{eq:tildeFdef} and
  \eqref{eq:Step1Claim}
  \begin{equation}\label{eq:tildeFrhobound}
    \|\tilde{F}_{\rho,
      i}\|_{C^k([0,1]^i, [0,1]^i)} \leq \|F_{\rho,i}\|_{C^k([0,1]^i, [0,1])}
    + (i-1)\le C\Big(\frac{L_1}{L_2}\Big)^{k+1}.
    % \qquad
    % \forall i\in\{1,\dots,k\}.
  \end{equation}
  For the last inequality we used $\frac{L_1}{L_2}\ge 1$, so that
  $i-1\le k-1$ can be absorbed into the $k$-dependent constant $C$.
  Next we bound $\|[D\tilde{F}_{\rho, i}]^{-1}\|_2$.  It holds
  \begin{equation*}
    \begin{pmatrix} 
      1 &  \dots & 0  & 0\\
      \vdots & \ddots &  &\\
      0 &        & 1 & 0\\
      \partial_{x_1}F_{\rho, i}&\partial_{x_2}F_{\rho, i}&\dots &
      \partial_{x_i}F_{\rho, i}
    \end{pmatrix}
  \end{equation*}
  and
  \begin{equation*}
    \begin{pmatrix} 
      1 &  \dots & 0  & 0\\
      \vdots & \ddots &  &\\
      0 &        & 1 & 0\\
      -\frac{\partial_{x_1}F_{\rho, i}}{\partial_{x_i}F_{\rho,
          i}}&-\frac{\partial_{x_2}F_{\rho, i}}{\partial_{x_i}F_{\rho,
          i}}&\dots & -\frac{1}{\partial_{x_i}F_{\rho, i}}
    \end{pmatrix}.
  \end{equation*}
    
  Since
  $\partial_{x_i}F_{\rho, i}(x_{[i]}) = \rho_i(x_{[i]}) \geq L_2$ and
  $\|\partial_{x_j}F_{\rho, i}(x_{[i]})\| \leq L_1$ for all
  $x_{[i]}\in [0,1]^i$ and $j\in\{1,\dots,i\}$, we have
  \begin{equation*}
    \Big\|\frac{\partial_{x_j}F_{\rho, i}}{\partial_{x_i}F_{\rho,
        i}}\Big\|_{L^\infty([0,1]^i)} \leq \frac{L_1}{L_2}
    \qquad
    \forall j\in\{1,\dots,i\}.
  \end{equation*}
  Thus for all $x_{[i]}\in [0,1]^i$
  \begin{equation*}
    \|[D\tilde{F}_{\rho, i}(x_{[i]})]^{-1}\|_2 \le \|[D\tilde{F}_{\rho,
      i}(x_{[i]})]^{-1}\|_F \le
    \sqrt{(i-1) + i(\frac{L_1}{L_2})^2} \le
    \sqrt{2i}\frac{L_1}{L_2}.
  \end{equation*}
  Since $\frac{L_1}{L_2} \ge 1$, we conclude that there exists a
  constant $C$ depending on $k$, but independent of $L_1$, $L_2$ such
  that
  \begin{equation*}
    \max\{\|[D\tilde{F}_{\rho, i}]^{-1}\|_2, \|\tilde{F}_{\rho,
      i}\|_{C^k([0,1]^i, [0,1]^i)}\} \leq
    C\frac{L_1^{k+1}}{L_2^{k+1}}.
  \end{equation*}
  Thus by Proposition \ref{prop:NormInverse}
  \begin{equation*}
    \|G_{\rho, i}\|_{C^k([0,1]^i, [0,1])}\leq \|\tilde{G}_{\rho,
      i}\|_{C^k([0,1]^i, [0,1]^i)} \leq
    e^kk^{k^2}\left(C\frac{L_1^{k+1}}{L_2^{k+1}}\right)^{3k-2} \leq C
    \left(\frac{L_1}{L_2}\right)^{(k+1)(3k-2)}.
  \end{equation*}
  This shows \eqref{eq:Step2Claim} for $\rho$.
  
  {\bf Step 3.} We finish the proof by showing that for all
  $i\in\{1,\dots,d\}$
  \begin{equation}\label{eq:Step3Claim}
    \|T_i\|_{C^k([0,1]^i, [0,1])} \leq C\left(\frac{L_1}{L_2}\right)^{k^i(k+1)+(k+1)(3k-2)(\sum_{j=0}^{i-1}k^j)}.
  \end{equation}

  For $x_{[i]}\in [0,1]^i$ define
  \begin{equation}\label{eq:tildeTidef}
    \tilde{T}_i(x_{[i]}) := (T_1(x_1), \dots,
    T_{i-1}(x_{[i-1]}), F_{\pi, i}(x_{[i]}))\in [0,1]^i.
  \end{equation}
  By \eqref{eq:Tixi} it holds $T_i=G_{\rho,i}\circ\tilde T_i$, and
  thus Corollary \ref{cor:composition} implies
  \begin{equation}\label{eq:Step3main}
    \|T_i\|_{C^k([0,1]^i)}\le C\|G_{\rho,i}\|_{C^k([0,1]^i)}\max\{1,\|\tilde T_i\|_{C^k([0,1]^i)}\}^k
  \end{equation}
  for a $k$-dependent constant $C$.


  We proceed by induction over $i$, and start with $i=1$. In this case
  \eqref{eq:Step1Claim}, \eqref{eq:Step2Claim} and
  \eqref{eq:Step3main} yield
  \begin{equation*}
    \|T_1\|_{C^k([0,1])}
    =\|G_{\rho,1}\circ F_{\pi,1}\|_{C^k([0,1])}
    \le C \Big(\frac{L_1}{L_2}\Big)^{(k+1)(3k-2)+k(k+1)},
  \end{equation*}
  and thus \eqref{eq:Step3Claim} is satisfied. For the induction step
  assume the statement is true for $i-1\ge 1$. By
  \eqref{eq:Step1Claim}, \eqref{eq:tildeTidef} and the induction
  hypothesis
  \begin{align*}
    \|\tilde{T}_i\|_{C^k([0,1]^i)} &\le
    C\Bigg(
    \sum_{j=1}^{i-1}
    \|T_j\|_{C^k([0,1]^j)}
    +\|F_{\pi,i}\|_{C^k([0,1]^i)}\Bigg)\nonumber\\
    &\le
    C \Big(\frac{L_1}{L_2}\Big)^{k^{i-1}(k+1)+(k+1)(3k-2)(\sum_{j=0}^{i-2}k^j)},
  \end{align*}
  where again $C$ may depend on $k$ (or $i\le k$) but not on $L_1$,
  $L_2$.  Then \eqref{eq:Step2Claim} and \eqref{eq:Step3main} imply
  \begin{align*}
    \|T_i\|_{C^k([0,1]^i)}
    &\le C 
    \|G_{\rho,i}\|_{C^k([0,1]^i)}
      \max\{1,\|\tilde T_i\|_{C^k([0,1]^i)}\}^k\nonumber\\
    &\le C \Big(\frac{L_1}{L_2}\Big)^{(k+1)(3k-2)}
      \Big(\frac{L_1}{L_2}\Big)^{k(k^{i-1}(k+1)+(k+1)(3k-2)(\sum_{j=0}^{i-2}k^j))}\nonumber\\
    &= C\left(\frac{L_1}{L_2}\right)^{k^i(k+1)+(k+1)(3k-2)(\sum_{j=0}^{i-1}k^j)}.
  \end{align*}

\end{proof}
  
Finally, by putting all the estimates together, we obtain the following upper bound for the $C^k$ norm of velocity field. 

\begin{theorem}\label{thm:tri}
Let $\Omega_0 = \Omega_1 = [0,1]^d$ and $\pi, \rho$ satisfy Assumption \ref{ass:dens2} with constant $k \geq 2$, $0 < L_1 \leq L_2 < \infty$. Let $T: [0,1]^d\rightarrow[0,1]^d$ be the KR map pushing forward $\pi$ to $\rho$ and $f:[0,1]^d\times[0,1]\rightarrow[0,1]^d$ be the velocity field in \eqref{eq:V} that corresponds to the displacement interpolation between $x$ and $T(x)$. Then, there exists constants $C_{k,d}$ that only depends on $k, d$ and $\beta_d$ that only depends on $d$, 
such that the following holds:
$$\|f\|_{C^k([0,1]^d\times[0,1])} \leq C_{k,d}\left(\frac{L_1}{L_2}\right)^{\beta_dk^{d+3}}.$$
\end{theorem}
\begin{proof}
The proof of the theorem requires a combination of Theorem \ref{thm:fNormGeneral}, Lemma.\ref{lemma:cT} and Theorem \ref{thm:CkNormTriangular}. 

First, by Lemma.\ref{lemma:cT} and Theorem \ref{thm:CkNormTriangular}, there exists constant $C_{d}$ and $\beta_d'$ such that $\|T\|_{C^1([0,1]^d, [0,1])} \leq C_{d}(\frac{L_1}{L_2})^{\beta_d'}$ and 
$$c_T \leq (\frac{L_1}{L_2})^{2d}\max\{1, (C_{d}(\frac{L_1}{L_2})^{\beta_d'})^{d-1}\}\leq \max\{(\frac{L_1}{L_2})^{2d}, C_d^{d-1}(\frac{L_1}{L_2})^{\beta_d'(d-1)}\}.$$
By renaming $C_d = \max\{1, C_d^{d-1}\}$ and $\beta'_d = \max\{2d, \beta_d'(d-1)\}$, we can simplify the above expression as $c_T \leq C_d(\frac{L_1}{L_2})^{\beta'_d}$. Note it holds true that $c_T \geq 1$. 

By lemma.\ref{lemma:cT} and Theorem \ref{thm:CkNormTriangular}, there exists constants $C_{k,d}$ and $\beta_d$ and  
\begin{align*}
 \gamma &= \max\{2, 1+c_T\}(1 + \|T\|_{C^k} + \sup_{x\in\Omega_0}\|x\|)\leq (1 + c_T)(1 + C_{k,d}(\frac{L_1}{L_2})^{\beta_dk^{d+1}} + \sqrt{d})\\
 &\leq 2c_T(1 + C_{k,d}(\frac{L_1}{L_2})^{\beta_dk^{d+1}} + \sqrt{d})\leq 2c_TC_{k,d}(\frac{L_1}{L_2})^{\beta_dk^{d+1}}\\
 &\leq C_d(\frac{L_1}{L_2})^{\beta'_d}C_{k,d}(\frac{L_1}{L_2})^{\beta_d k^{d+1}} \leq C_{k,d}(\frac{L_1}{L_2})^{\beta_dk^{d+1}}.\\
\end{align*}
where we absorb constants whenever possible. In particular, we used $\frac{L_1}{L_2} > 1$ to absorb into $C_{k,d}$ in the third inequality and we used $k>1$ to absorb everything in the exponent into a $d-$dependent $\beta_d$ in the last inequality. 

Finally, applying Theorem \ref{thm:fNormGeneral}, we obtain
$$\|f\|_{C^k([0,1]^d\times[0,1])}\leq 2k^{k^3+k}e^{k^2+k}\gamma^{3k^2-2k+1} \leq C_{k,d} (\frac{L_1}{L_2})^{\beta_dk^{d+1}3k^2} \leq C_{k,d}(\frac{L_1}{L_2})^{\beta_dk^{d+3}}.$$
\end{proof}






\section{Stability in the velocity field}\label{sec:DistApproximation}
In the previous sections we showed existence of velocity fields $f$
that yield flow maps realizing a (triangular) transport that pushes
forward $\pi$ to $\rho$. In practice, a suitable velocity field $g$ is
obtained by minimizing the objective \eqref{Jobjective} over some
parametrized function class such as the set of Neural Networks of a
certain architecture. In general, such $g$ will only approximate $f$,
and it is therefore of interest to understand how errors in the
approximation of $f$ propagate to errors in the distributions realized
by the corresponding flow map. This is the purpose of the present
section.


\subsection{Wasserstein distance}
First, we present results when the divergence between probability distributions is measured by Wasserstein distance. That is, we take $\mathcal{D} = W_p$ in \eqref{Jobjective}, where $W_p$ is the $p-$Wasserstein distance. 


\begin{theorem}\label{thm:FlowMapBound}
  Let $f$, $g\in\mathcal{V}$ (cp.~\eqref{eq:V}) and
  $\|f - g\|_{C^0(\R^d\times [0,1])}<\infty$. Assume that $L>0$ is
  such that $x\mapsto f(x,t)$ has Lipschitz constant $L$ for all
  $t\in [0,1]$.  Let $X_f$, $X_g:\R^d\times[0,1]\to\mathbb{R}^d$ be
  as in \eqref{eq:flowmap}.
 % \begin{equation*}
 % \frac{dX(x,t)}{dt} = f(X(x,t),t),\qquad \frac{dY(x,t)}{dt} = g(Y(x,t),t) 
 % \end{equation*}
 % with initial condition $X(x,0) = Y(x,0) = x$.
 Then
 \begin{equation}\label{eq:FlowMapBound}
   \|X_f(\cdot,1) - X_g(\cdot,1)\|_{C^0(\R^d)} \leq \|f -
   g\|_{C^0(\R^d\times [0,1])} e^{L}.
 \end{equation}
\end{theorem}

The idea of the proof of Theorem ~\ref{thm:FlowMapBound} is to apply
Gr\"onwall's inequality to the evolution of the error
$|X_f(x,t) - X_g(x,t)|$ over time. We point out that this stability result
merely requires $g$ to approximate $f$ uniformly, however $f$ is
additionally assumed to be Lipschitz continuous in space. 
\begin{proof}[Proof of Theorem \ref{thm:FlowMapBound}]
Fix $x\in\R^d$. Then for all $s\in [0,1]$
\begin{align*}
|f(X_f(x,s),s) - g(X_g(x,s),s)| &\le |f(X_f(x,s),s) - f(X_g(x,s), s)| + |f(X_g(x,s), s) - g(X_g(x,s),s)|\\
                                                  &\le L|X_f(x,s)-X_g(x,s)|+\|f-g\|_{C^0(\R^d\times\R)},
\end{align*}
where we used the spatial Lipschitz continuity of $f$. Hence for $t\in [0,1]$
\begin{align*}
|X_f(x,t) - X_g(x,t)| &= \Big|\int_0^tf(X_f(x,s),s) - g(X_g(x,s),s)ds\Big|\\
&\le \int_0^tL|X_f(x,s)-X_g(x,s)|ds+t\|f-g\|_{C^0(\R^d\times\R)}.
\end{align*}
% It holds that $|X_f(x,s) - X_g(x,s)|$ is a continuous function in $s$.
Using Gr\"onwall's inequality, we get $|X_f(x,1) - X_g(x,1)|\leq \|f-g\|_{C^0(\R^d\times\R)} e^{L}$
as claimed.
\end{proof}


Next we show how an approximation of the velocity field
affects the difference in distributions in terms of
the Wasserstein distance $W_p$. In the following
corollary, we denote by $|\supp(\pi)|$ the Lebesgue measure
of the support of $\pi$.

\begin{corollary}\label{cor:Wp}
  Let $f$, $g\in\mathcal{V}$ and $X_f$, $X_g$ be as in Theorem
  \ref{thm:FlowMapBound}. Let $\pi$ be a probability distribution on
  $\R^d$. Then for any $p\in [1,\infty)$
  \begin{equation*}
    W_p(X_f(\cdot,1)_\sharp\pi, X_g(\cdot,1)_\sharp\pi) \le
    \|f-g\|_{C^0(\R^d\times\R)} e^{L}
      |\supp(\pi)|^{1/p}.
    \end{equation*}
    Moreover, for $p=\infty$ holds
    $W_\infty(X_f(\cdot,1)_\sharp\pi, X_g(\cdot,1)_\sharp\pi) \le
    \|f-g\|_{C^0(\R^d\times\R)} e^{L}$.
\end{corollary}
\begin{proof}
  Let $F:\R^d\to\R^d\times\R^d$ via
  $F(x):=(X_f(x,1),X_g(x,1))$. Observe that $\gamma:=F_\sharp(\pi\otimes\pi)$ is
  then a probability distribution on $\R^d\times\R^d$ with marginals
  $X_f(\cdot,1)_\sharp\pi$ and $X_g(\cdot,1)_\sharp\pi$, i.e.\ it is a coupling
  of these measures. If $p<\infty$ then by definition of the Wasserstein distance
  \begin{align*}
    W_p(X_f(\cdot,1)_\sharp\pi, X_g(\cdot,1)_\sharp\pi)^p
    &\le \int_{\R^d\times\R^d}\|x-y\|^p\dd\gamma(x,y)\nonumber\\
    &=\int_{\R^d}\|X_f(x,1)-X_g(x,1)\|^p\dd\pi(x)\nonumber\\
    &\le |\supp(\pi)|(\|f-g\|_{C^0(\R^d\times\R)} e^{L})^p.
  \end{align*}
  The case $p=\infty$ is obtained with the usual adjustment of arguments.
\end{proof}


\subsection{KL-divergence}
In this subsection, we % present results when the distance between probability distributions is measured by the KL-divergence. That is, we take
measure the distance in the KL-divergence, i.e.\
$\mathcal{D} = \mathcal{D}_{KL}$ in \eqref{Jobjective}. Unlike for the Wasserstein distance,
% for KL-divergence to be well-defined,
for $\mathcal{D}_{KL}(X(\cdot,1)_\sharp\pi,\rho)$ to be finite, we need in particular $X(\cdot, 1)_\sharp\pi\ll \rho$.
% to have the same support.
% The construction of a general parametric ansatz space of functions to ensure this property is technically challenging, so we restrict ourselves to considering the case where the distributions are supported on hypercubes; that is,
We restrict ourselves to distributions on cubes, and consider $\Omega_0 = \Omega_1 = [0,1]^d$.

% We comment there that most
Most of the work regarding the approximation distributions in KL-divergence using ODE flow maps has already been studied in our companion paper \cite{StatisticalNODE}; we include some of the relevant results here for the sake of completeness. In \cite{StatisticalNODE}, an % (bounded)
ansatz space
\begin{align*}
  C^k_{\rm ansatz}(r) =  &\{f = (f_1,...,f_d)^T: f_j = \tilde{f}_jx_j(1-x_j), \tilde{f}_j\in C^k([0,1]^d\times[0,1], [0,1]^d)\} \\
  &\cap \{f\in C^2([0,1]^d\times[0,1], [0,1]^d): \|f\|_{C^2([0,1]^d\times[0,1], [0,1]^d)}\leq r\}
\end{align*}
was proposed. Its definition ensures that all push-forward distributions $X_f(\cdot,1)_\sharp\pi$ are supported on $[0,1]^d$ for any $f\in C^k_{\rm ansatz}(r)$. % in the $\mathcal{V}_{ansatz}$.
In Theorem \ref{thm:tri}, we showed the velocity field corresponding to the straight-line interpolation of Knothe-Rosenblatt maps $f^\Delta$ lies in $C^k([0,1]^d\times[0,1], [0,1]^d)$ and in Theorem 9 of \cite{StatisticalNODE}, it is shown that % $f^\Delta$ vanishes linearly near the boundary, or $
\begin{equation*}
  \frac{f^\Delta_j(x_1,\cdots,x_j)}{x_j(1-x_j)}\in C^k([0,1]^d\times[0,1], [0,1]).
\end{equation*}
  Therefore, by choosing $r$ to be large enough, for example, by taking $r= C_{k,d}(\frac{L_1}{L_2})^{\beta_dk^{d+3}}$ corresponding to the upper bound in Theorem \ref{thm:tri}, it suffices to consider an approximating element in $C^k_{\rm ansatz}(r)$.

% One key observation to make is that to
We emphasize that bounding discrepancies in Wasserstein distance only requires $C^0$ control of the velocity fields; however, controlling discrepancies in the KL-divergence, requires $C^1$ control of the velocity fields, as stated in the next theorem.

\begin{theorem}\label{Thm:KLstatbility}
Let $\pi$, $\rho$ % be distributions that
satisfy Assumptions \ref{ass:dens1} and \ref{ass:dens2} with $\Omega_0 = \Omega_1 = [0,1]^d$. Let $f^\Delta$ as in Theorem \ref{thm:tri} and  $g\in C^2_{ansatz}(r)$. % \rrtd{actually we can just choose $k=2$? In this paper, we just need an approximation element anyways, so there is no difference if we choose a larger space}
Then
\[\mathcal{D}_{KL}(X_g(\cdot, 1)_\sharp\pi, X_{f^\Delta}(\cdot, 1)_\sharp\pi) \leq C \|f^\Delta - g\|^2_{C^1([0,1]^d)},\]
for some constant $C$ that depends on $L_1, L_2, d$. 
\end{theorem}
\begin{proof}
By Lemma 6, Theorem 7 and Theorem 8 of \cite{StatisticalNODE} there exists a constant $C_{L_1,L_2, d}$ that depends on $L_1, L_2, d$ such that 

\[\|X_g(\cdot, 1)_\sharp\pi - X_{f^\Delta}(\cdot, 1)_\sharp\pi\|_{C^0([0,1]^d)}  = \|X_g(\cdot, 1)_\sharp\pi - \rho\|_{C^0([0,1]^d)}\leq C_{L_1,L_2, d}\|f^\Delta - g\|_{C^1([0,1]^d)}.\] 

To get an upper bound for $\mathcal{D}_{KL}(X_g(\cdot,1)_\sharp\pi, X_{f^\Delta}(\cdot,1)_\sharp\pi)$, we bound
\begin{align*}
&\mathcal{D}_{KL}(X_g(\cdot,1)_\sharp\pi, X_{f^\Delta}(\cdot,1)_\sharp\pi) = \mathbb{E}_{\pi_g(\cdot,1)}\Big[\log \frac{\pi_g(\cdot,1)}{\rho(x)}\Big] \leq \log \mathbb{E}_{\pi_g(x,1)}[\frac{\pi_g(x,1)}{\rho(x)}] = \log\int_{[0,1]^d} \frac{\pi_g(x,1)^2}{\rho(x)}dx \\
&= \log\left(\int_{[0,1]^d} \frac{(\rho(x)-\pi_g(x,1))^2}{\rho(x)}dx + 1\right)  \leq \log\left(\frac{C_{L_1,L_2, d}^2}{L_2} \|f^\Delta - g\|^2_{C^1([0,1]^d)}+1\right).     
\end{align*}
The fact that $\log(1+x)\le x$ for all $x\ge 0$ gives the result.
% Taylor expanding $\log(1+x)$ then yields the result. 
\end{proof}












\section{Neural network approximation}\label{sec:NN}
In Section \ref{sec:trireg} we studied the regularity of the velocity
field corresponding to straight-line trajectories realizing the
Knothe-Rosenblatt map at time $t=1$. Building on earlier works on
neural network approximation theory such as
\cite{NNApproximation1,NNApproximation4, StatisticalNODE}
in the present section we conclude that by parameterizing the velocity
field via neural networks, NODE flows can achieve arbitrary accuracy
in terms of the Wasserstein distance and KL-divergence. Furthermore,
given a desired accuracy $\eps>0$, we give upper bounds on the
required network depth, width, and size in terms of $\eps$. Since the objective functional $J$ contains first-order derivatives, we shall consider the approximation theory using ReLU$^2$ networks developed in \cite{StatisticalNODE}. 

We first recall the definition of ReLU$^2$ networks.
% the definition of ReLU$^m$ networks, a generalization of ReLU networks that is used in many settings where higher order smoothness of the approximating class is required.
% Similar network structures have been used in \cite{StatisticalNODE, MLPDEStatisticalRate}. 
% Let $\eta_1(x) = \max\{x, 0\}$ being the ReLU
% activation function, and $\eta_m(x) = \max\{x, 0\}^m$ be the ReLU$^m$
% activation function.  

\begin{definition}\label{def:ReLUm}
  Denote $\sigma_2(x):=\max\{0,x\}^2$ and let
  % Let $m\ge 1$ and fix
  $d_1,d_2\ge 1$. Then, the class of ReLU$^2$
  networks mapping from $[0,1]^{d_1}$ to $\mathbb{R}^{d_2}$, with
  % height
  depth
  $L$, width $W$, sparsity % constraint
  $S$, and % norm constraint
  bound $B$ on the network weights, is denoted
    % defined by
  
  \begin{align*}
    \Phi^{d_1,d_2}(L, W, S, B) = \Big\{&\big(W^{(L)}\jz{\sigma_2}(\cdot) + b^{(L)}\big)\circ\cdots\circ\big(W^{(1)}\jz{\sigma_2}(\cdot) + b^{(1)}\big): W^{(L)} \in \mathbb{R}^{1\times W},\\
                                 &b^{(L)}\in\mathbb{R}^{d_2}, W^{(1)}\in\mathbb{R}^{W\times d_1},b^{(1)}\in\mathbb{R}^W, W^{(l)}\in\mathbb{R}^{W\times W},b^{(l)}\in\mathbb{R}^W (1<l<L),\\
                                 &\sum_{l=1}^L\big(\|W^{(l)}\|_0 + \|b^{(l)}\|_0\big) \leq S, \max_{1\leq l\leq L} \big(\|W^{(l)}\|_{\infty,\infty}\lor \|b^{(l)}\|_\infty\big)\leq B \Big\}.\\    
  \end{align*}
  % We refer to an element of $\Phi^{d_1, d_2}(L, W, S, B)$ as a
  % \emph{ReLU$^2$ network}.
\end{definition}
\begin{remark}
  Since the representation of a function via neural networks is not
  unique in general, the statement ``$g$ is a ReLU$^2$ NN of depth $L$,
  width $W$, sparsity $S$, norm bound $B$'' merely implies the existence of such a network satisfying the above
  properties. Possibly, for some other $\tilde L$, $\tilde W, \tilde S$
  and $\tilde B$, it may additionally hold that ``$g$ is a ReLU$^2$
  NN of depth $\tilde L$, width $\tilde W$, sparsity $\tilde S$, and norm bound $\tilde B$''.
\end{remark}
% As mentioned above, contrary to convergence in KL-divergence, approximation in Wasserstein distance only % needs
% requires $C^0$, 
% rather than $C^1$, control of the velocity fields.

% Moreover, to bound the KL-divergence, we will have to ensure
% that the approximated distribution has the same support as the
% target distribution.
% For this reason, the analysis slightly differs depending on the
% considered notion of distance. Specifically, for the KL-divergence we
% will use smoother ReLU$^2$ networks, that allow for
% $C^1$-approximation, and we will adopt a construction from
% \cite{StatisticalNODE} guaranteeing that the trajectories of the NODE
% remain in the unit cube.





\subsection{Wasserstein distance}
% We call a function $g:\R^d\to\R$ a ReLU neural network of \emph{depth}
% $L\in\N$ and \emph{width} $N\in\N$ iff there holds the following: With
% $n_0=d$, $n_L=1$ there exist
% $(n_i)_{i=1}^{L-1}\subseteq\{1,\dots,N\}$, weight matrices
% $W_j\in\R^{n_{j}\times n_{j-1}}$, and bias vectors $b_j\in\R^{n_j}$,
% such that with the linear transformations $A_j(x):=W_jx+b_j$ holds
% \begin{equation}\label{eq:phi}
%   g = A_L\circ\sigma \circ A_{L-1}\circ\cdots\circ \sigma\circ A_1\quad\text{on}\quad\R^d,
% \end{equation}
% where $\sigma(x):=(\max\{0,x_i\})_{i=1}^n$ for all $x\in\R^n$, $n\in\N$.
% Moreover, we call
% \begin{equation}
%   S(g):=\sum_{j=0}^L(\|W_j\|_0+\|b_j\|_0)
% \end{equation}
% the size of the network, which corresponds to
% the number of nonzero parameters in the linear transformations.
% % and refer to this as the size of the network $g$.

% \begin{remark}
%   Since the representation of a function via \eqref{eq:phi} is not
%   unique in general, the statement ``$g$ is a ReLU NN of depth $L$,
%   width $N$ and size $S$'' merely implies the existence of $L$ linear
%   transformations $A_1,\dots,A_L$ satisfying the above
%   properties. Possible, for some other $\tilde L$, $\tilde N$
%   and $\tilde S$, it may additionally hold that ``$g$ is a ReLU
%   NN of depth $\tilde L$, width $\tilde N$ and size $\tilde S$''.
% \end{remark}

As noted before, approximation in Wasserstein distance only % needs
requires $C^0$, 
rather than $C^1$, control of the velocity fields. In terms of the Wasserstein distance, we have the following result,
which is a consequence of our regularity analysis of the velocity
field in Theorem \ref{thm:tri}, Corollary \ref{cor:Wp} and neural
network approximation results as e.g.\ presented in
\cite{StatisticalNODE}:
\begin{proposition}\label{prop:WpNN}
  Let $k\ge 1$, $p\in [1,\infty]$, and let $\rho$, $\pi$ be two
  probability distributions on $[0,1]^d$
  % \rrtd{For Wasserstein, we do not need the supports to be $[0,1]^d$ but can be any arbitratry $\Omega_1, \Omega_2$ right?}
  with Lebesgue densities in 
  $C^k([0,1]^d)$ satisfying Assumption \ref{ass:dens2}.
%
  Then there exist constants $C_{d,k}$ and $C_{d, k, L_1, L_2}'$ such that for every $\eps\in (0,1]$ there exists a ReLU$^2$ neural network $g\in\Phi^{d+1,d}(L, W, S, B)$ with
  \begin{equation*}\label{eq:NNbounds1}
  L \leq C_{d,k},\quad
  W \leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k}},\quad
  S\leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k}},\quad
  B\leq C_{d, k, L_1,L_2}'\eps^{-\frac{1}{k}}
\end{equation*}
such that for another constant $C_{d, k, L_1, L_2}$, we have
  \begin{equation*}
    W_p(\rho,X_g(\cdot,1)_\sharp\pi)\le C \eps.
  \end{equation*}
\end{proposition}
\begin{proof}
  According to Theorem \ref{thm:tri} there exists
  ${f^\Delta}\in C^k([0,1]^d\times [0,1])$ such that
  $X_{f^\Delta}(\cdot,1)_\sharp\pi=\rho$ and $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$
  only depends on $L_1$, $L_2$, $k$, $d$. We can extend ${f^\Delta}$ to some
  $\tilde {f^\Delta}\in C^k(\R^d\times [0,1])$ with compact support and such
  that
  \begin{equation*}
    \|\tilde {f^\Delta}\|_{C^k(\R^d\times [0,1])}\le C \|{f^\Delta}\|_{C^k(\Omega_{[0,1]})}
  \end{equation*}
  for some $C$ solely depending on $d$ (see for example Step 1 of the
  proof of \cite[Theorem 33]{StatisticalNODE}). Since $\tilde {f^\Delta}$ has compact
  support and belongs to $C^1(\R^d\times [0,1])$, there exists $L<\infty$ such that
  $x\mapsto \tilde {f^\Delta}(x,t):\R^d\to\R^d$ has Lipschitz constant $L$ for all
  $t\in [0,1]$. Again, $L$ solely depends on
  $\|{f^\Delta}\|_{C^1([0,1]^d\times [0,1])}$ and thus on $L_1$, $L_2$, $k$,
  $d$. Next, let $M=1+\exp(L)$. % be so large that
  % $[0,1]^d \subseteq [-M+\exp(L),M-\exp(L)]^d$.

  According to
  \citet[Theorem 16]{StatisticalNODE}, there exists a ReLU$^2$ neural
  network $g$ satisfying the bounds \eqref{eq:NNbounds1} and
  \begin{equation}\label{eq:f-geps}
\|{\tilde f^\Delta}(x)-g(x)\|_{C^0([-M,M]^d\times [0,1])}\le\eps.
  \end{equation}


  Fix $x\in [0,1]^d$. By \eqref{eq:FlowMapBound}
  and \eqref{eq:f-geps}, we
  have for all $t\in [0,1]$
    \begin{equation}\label{eq:XfDXg}
      \|X_{\tilde f^\Delta}(x,t)-X_g(x,t)\|\le \eps\exp(L)\le\exp(L)
    \end{equation}
    and thus $\|X_{g}(x,t)\|\le 1+\exp(L)=M$.
  Hence $X_g(x,1)\in\R^d$ is well-defined since the trajectory
  $t\mapsto X_g(x,t)$, $t\in[0,1]$, remains within
  $[-M,M]^d\times [0,1]$. % Furthermore by \eqref{eq:FlowMapBound} and
  % \eqref{eq:f-geps}
  % \begin{equation*}
  %   \|X_{f^\Delta}(x,1)-X_g(x,1)\|\le \eps \exp(L).
  % \end{equation*}
  Finally, the first inequality in \eqref{eq:XfDXg} and an application of
  Corollary \ref{cor:Wp} concludes the proof.
\end{proof}





% \begin{proposition}\label{prop:WpNN}
%   Let $k\ge 1$, $p\in [1,\infty]$ and let $\rho$, $\pi$ be two
%   probability distributions on $[0,1]^d$
%   % \rrtd{For Wasserstein, we do not need the supports to be $[0,1]^d$ but can be any arbitratry $\Omega_1, \Omega_2$ right?}
%   with Lebesgue densities in 
%   $C^k([0,1]^d)$ satisfying Assumption \ref{ass:dens2}.

%   Then there exists $C=C(L_1,L_2,p,k)$ such that for every
%   $\eps\in (0,1]$ there exists a neural network $g$ of depth $L$ and
%   size $S$ such that
%   \begin{equation}\label{eq:LS}
%     L\le C(1+\log(1/\eps)),\qquad S\le C \eps^{-\frac{d+1}{k}}(\log(1/\eps)+1)
%   \end{equation}
%   and it holds for all $p\in [1,\infty]$
%   \begin{equation*}
%     W_p(\rho,X_g(\cdot,1)_\sharp\pi)\le C \eps.
%   \end{equation*}
% \end{proposition}
% \begin{proof}
%   According to Theorem \ref{thm:tri} there exists
%   ${f^\Delta}\in C^k([0,1]^d\times [0,1])$ such that
%   $X_{f^\Delta}(\cdot,1)_\sharp\pi=\rho$ and $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$
%   only depends on $L_1$, $L_2$, $k$, $d$. We can extend ${f^\Delta}$ to some
%   $\tilde {f^\Delta}\in C^k(\R^d\times [0,1])$ with compact support and such
%   that
%   \begin{equation*}
%     \|\tilde {f^\Delta}\|_{C^k(\R^d\times [0,1])}\le C \|{f^\Delta}\|_{C^k(\Omega_{[0,1]})}
%   \end{equation*}
%   for some $C$ solely depending on $d$ (see for example Step 1 of the
%   proof of \cite[Theorem E.1]{StatisticalNODE}). Since $\tilde {f^\Delta}$ has compact
%   support and belongs to $C^1(\R^d\times [0,1])$, there exists $L<\infty$ such that
%   $x\mapsto \tilde {f^\Delta}(x,t):\R^d\to\R^d$ has Lipschitz constant $L$ for all
%   $t\in [0,1]$. Again, $L$ solely depends on
%   $\|{f^\Delta}\|_{C^1([0,1]^d\times [0,1])}$ and thus on $L_1$, $L_2$, $k$,
%   $d$.

%   Next, let $M=1+\exp(L)$. % be so large that
%   % $[0,1]^d \subseteq [-M+\exp(L),M-\exp(L)]^d$.
%   According to
%   \cite[Theorem 1]{NNApproximation1}, there exists a ReLU neural
%   network $g$ satisfying the bounds \eqref{eq:LS} (with a constant
%   depending on $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$ and thus on $L_1$,
%   $L_2$, $d$, $k$) and
%   \begin{equation}\label{eq:f-geps}
% \|{f^\Delta}(x)-g(x)\|_{C^0([-M,M]^d\times [0,1])}\le\eps.
%   \end{equation}
%   Fix $x\in [0,1]^d$. By \eqref{eq:FlowMapBound}
%   and \eqref{eq:f-geps}, we
%   have for all $t\in [0,1]$
%     \begin{equation}\label{eq:XfDXg}
%       \|X_{f^\Delta}(x,t)-X_g(x,t)\|\le \eps\exp(L)\le\exp(L)
%     \end{equation}
%     and thus $\|X_{g}(x,t)\|\le 1+\exp(L)=M$.
%   Hence $X_g(x,1)\in\R^d$ is well-defined since the trajectory
%   $t\mapsto X_g(x,t)$, $t\in[0,1]$, remains within
%   $[-M,M]^d\times [0,1]$. % Furthermore by \eqref{eq:FlowMapBound} and
%   % \eqref{eq:f-geps}
%   % \begin{equation*}
%   %   \|X_{f^\Delta}(x,1)-X_g(x,1)\|\le \eps \exp(L).
%   % \end{equation*}
%   Finally, the first inequality in \eqref{eq:XfDXg} and an application of
%   Corollary \ref{cor:Wp} concludes the proof.
% \end{proof}
 
Next we discuss convergence of the objective $J$ defined in \eqref{Jobjective},
% where we'll work with
for the Wasserstein distance; specifically
  \begin{equation*}
    J_{\rm W}(f) := W_2(\rho,X_f(\cdot,1)_\sharp\pi)^2+R(f)
  \end{equation*}
  where
  \begin{equation}\label{eq:Rf}
    R(f) = \int_0^1\|\nabla_X f(X_f(x,t)) f(X_f(x,t),t)+\partial_t f(X_f(x,t),t)\|^2\dd t.
  \end{equation}
  Since the regularization term $R(f)$ contains first order
  derivatives of $f$, it will not suffice to have uniform
  approximation in $f$, but rather we'll additionally need uniform
  approximation of the derivatives of $f$.
We also point out that by Theorem \ref{thm:ExSol}, $\inf_{f\in\mathcal{V}}J_{\rm W}(f)=0.$
 
  


  \begin{theorem}\label{Thm:JBoundWasserstein}
  Let $k\ge 2$, $p\in [1,\infty]$ and let $\rho$, $\pi$ be two
  probability distributions on $[0,1]^d$ with Lebesgue densities in
  $C^k([0,1]^d)$ satisfying Assumption \ref{ass:dens2}.
  
  Then there exist constants 
  $C_{d,k}$ and $C_{d, k, L_1, L_2}'$ such that
  for every $\eps\in (0,1]$
  there exists a ReLU$^2$ neural network $g\in\Phi^{d+1,d}(L, W, S, B)$ with
  \begin{equation}\label{eq:NNbounds2}
  L \leq C_{d,k},\quad
  W \leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k-1}},\quad
  S\leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k-1}},\quad
  B\leq C_{d, k, L_1,L_2}'\eps^{-\frac{1}{k-1}}
\end{equation}
  and such that for another constant $C_{d,k, L_1, L_2}$, we have 
  \begin{equation*}
    J_{\rm W}(g)\le %\inf_{f\in\mathcal{V} J_{\rm W}(f)+C\eps^2 = 
    C_{d,k,L_1,L_2} \eps^2.
  \end{equation*}
\end{theorem}
\begin{proof}
  According to Theorem \ref{thm:tri}, there exists
  ${f^\Delta}\in C^k([0,1]^d\times [0,1])$ such that
  $X_{f^\Delta}(\cdot,1)_\sharp\pi=\rho$ (i.e.\
  $W_2(\rho,X_{f^\Delta}(\cdot,1)_\sharp\pi)=0$), ${f^\Delta}$ realizes straight line
  trajectories (i.e.\ $R({f^\Delta})=0$) and $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$
  only depends on $L_1$, $L_2$, $k$, $d$. In particular ${f^\Delta}$ can be
  extended to an element of $\mathcal{V}$ in \eqref{eq:V}
  % \rrtd{what is $\mathcal{F}$?}
  such that $J_{\rm W}({f^\Delta})=0$. Hence it
  suffices to show existence of $g$ such that $J_{\rm W}(g)\le C\eps^2$.

  We can extend ${f^\Delta}$ to some $\tilde {f^\Delta}\in C^k(\R^d\times [0,1])$ with
  compact support and such that
  \begin{equation*}
    \|\tilde {f^\Delta}\|_{C^k(\R^d\times [0,1])}\le C \|{f^\Delta}\|_{C^k([0,1]^d\times[0,1])}
  \end{equation*}
  for some $C$ solely depending on $d$ (see for example Step 1 of the
  proof of \cite[Theorem 33]{StatisticalNODE}) %\rrtd{same problem here}. 
  Since $\tilde {f^\Delta}$ has
  compact support and belongs to $C^k$, $k\ge 2$, there exists
  $L<\infty$ such that the three maps
  \begin{equation*}
    x\mapsto \tilde {f^\Delta}(x,t),\qquad
    x\mapsto \partial_t\tilde {f^\Delta}(x,t),\qquad
    x\mapsto \nabla_x\tilde {f^\Delta}(x,t),\qquad
  \end{equation*}
  have Lipschitz constant $L$ for all $t\in [0,1]$ and all
  $x\in\R^d$. Again, $L$ solely depends on
  $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$ and thus on $L_1$, $L_2$, $k$,
  $d$.

  Next, let $M>\exp(L)$ be so large that
  $[0,1]^d \subseteq [-M+\exp(L),M-\exp(L)]^d$.  According to
  \cite[Theorem 16]{StatisticalNODE},
  there exists a ReLU$^2$ neural
  network $g$ satisfying the bounds \eqref{eq:NNbounds2} and 
  \begin{equation}\label{eq:f-geps2}
    \|{\tilde f^\Delta}(x)-g(x)\|_{C^1([-M,M]^d\times [0,1])}\le\eps.
  \end{equation}
  As in the proof of Proposition \ref{prop:WpNN}, we conclude that
  $X_g(x,1)\in\R^d$ is well-defined and
  \begin{equation*}
    \|X_{\tilde f^\Delta}(x,t)-X_g(x,t)\|\le \eps \exp(L)\qquad\forall x\in[0,1]^d,~t\in [0,1].
  \end{equation*}
  Since the trajectories $t\mapsto X_{\tilde f^\Delta}(x,t)$ remain in
  $[0,1]^d\times [0,1]$ according to Proposition \ref{prop:cube}, we conclude that
  \begin{equation}\label{eq:trajectoriesM}
    X_g(x,t)\in [-M,M]^d\qquad\forall x\in [0,1]^d,~t\in [0,1].
  \end{equation}
  Moreover, as in the proof of Proposition \ref{prop:WpNN} holds
  \begin{equation*}
    W_2(\rho,X_g(\cdot,1)_\sharp\pi)^2\le C\eps^2.
  \end{equation*}

  To bound $J_{\rm W}(g)$, it remains to treat the term
  $R(g)$ in \eqref{eq:Rf}. We have
  \begin{align*}
    \|\nabla_X \tilde f^\Delta(X_{\tilde f^\Delta}(x,t))-\nabla_Xg(X_g(x,t))\|
    \le
    &\|\nabla_X\tilde {f^\Delta}(X_{\tilde f^\Delta}(x,t))-\nabla_X\tilde {f^\Delta}(X_g(x,t))\|\nonumber\\
    &+\|\nabla_X\tilde {f^\Delta}(X_g(x,t))-\nabla_Xg(X_g(x,t))\|.
  \end{align*}
  Since $\nabla_X\tilde {f^\Delta}$ has Lipschitz constant $L$,
  The first term on the right-hand side is bounded by $L\exp(L)\eps$.
  Due to $X_g(x,t)\in [0,1]^d\subseteq [-M,M]^d$ by \eqref{eq:trajectoriesM},
  we can use \eqref{eq:f-geps2} to bound the second term which gives
  \begin{equation*}
    \|\nabla_X\tilde {f^\Delta}(X_{\tilde f^\Delta}(x,t))-\nabla_Xg(X_g(x,t))\|
    \le L\exp(L)\eps+\eps
    \qquad\forall x\in[0,1]^d,~t\in [0,1].
  \end{equation*}
  Similarly we obtain
  \begin{equation*}
    \|\partial_t \tilde { f^\Delta}(X_{\tilde f^\Delta}(x,t))-\partial_t g(X_g(x,t))\|
    \le L\exp(L)\eps+\eps
    \qquad\forall x\in[0,1]^d,~t\in [0,1]
  \end{equation*}
  and
  \begin{equation*}
    \|\tilde {f^\Delta}(X_{\tilde f^\Delta}(x,t))-g(X_g(x,t))\|
    \le L\exp(L)\eps+\eps \qquad\forall x\in[0,1]^d,~t\in [0,1].
  \end{equation*}
  Therefore
  \begin{align*}
    R(g)=|R({f^\Delta})-R(g)|
    &\le \int_0^1\|\nabla_X \tilde f(X_{\tilde f^\Delta}(x,t)) \tilde {f^\Delta}(X_{\tilde f^\Delta}(x,t),t)+\partial_t \tilde f(X_{\tilde f^\Delta}(x,t),t) \\
    &\qquad\qquad-\nabla_X g(X_g(x,t)) g(X_g(x,t),t)-\partial_t g(X_g(x,t),t)\|^2\dd t\\
    &\le C \int_0^1 \eps^2\dd t,
  \end{align*}
  where $C$ only depends on $L$.
\end{proof}














  
  % Using such a result from
  % \cite{NNApproximation2}, we obtain the following:

% \begin{theorem}\label{Thm:JBoundWasserstein}
%   Let $k\ge 2$, $p\in [1,\infty]$ and let $\rho$, $\pi$ be two
%   probability distributions on $[0,1]^d$ with Lebesgue densities in
%   $C^k([0,1]^d)$ satisfying Assumption \ref{ass:dens2}.
  
%   Then there exists a
%   constant $C=C(L_1,L_2,k,d)$ such that for every $\eps\in (0,1]$
%   there exists a ReLU neural network $g$ of depth $L$ and size $S$
%   such that
%   \begin{equation}\label{eq:LS2}
%     L\le C(1+\log(1/\eps)),\qquad S\le C \eps^{-\frac{d+1}{k-1}}(\log(1/\eps)+1)
%   \end{equation}
%   and
%   \begin{equation*}
%     J(g)\le \inf_{f\in\mathcal{V}}J(f)+C\eps^2.
%   \end{equation*}
% \end{theorem}
% \begin{proof}
%   According to Theorem \ref{thm:tri}, there exists
%   ${f^\Delta}\in C^k([0,1]^d\times [0,1])$ such that
%   $X_{f^\Delta}(\cdot,1)_\sharp\pi=\rho$ (i.e.\
%   $W_2(\rho,X_{f^\Delta}(\cdot,1)_\sharp\pi)=0$), ${f^\Delta}$ realizes straight line
%   trajectories (i.e.\ $R({f^\Delta})=0$) and $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$
%   only depends on $L_1$, $L_2$, $k$, $d$. In particular ${f^\Delta}$ can be
%   extended to an element of $\mathcal{V}$ in \eqref{eq:V}
%   % \rrtd{what is $\mathcal{F}$?}
%   such that $J({f^\Delta})=0$. Hence it
%   suffices to show existence of $g$ such that $J(g)\le C\eps^2$.

%   We can extend ${f^\Delta}$ to some $\tilde {f^\Delta}\in C^k(\R^d\times [0,1])$ with
%   compact support and such that
%   \begin{equation*}
%     \|\tilde {f^\Delta}\|_{C^k(\R^d\times [0,1])}\le C \|{f^\Delta}\|_{C^k(\Omega_{[0,1]})}
%   \end{equation*}
%   for some $C$ solely depending on $d$ (see for example Step of the
%   proof of \cite[Theorem E.1]{StatisticalNODE}). Since $\tilde {f^\Delta}$ has
%   compact support and belongs to $C^k$, $k\ge 2$, there exists
%   $L<\infty$ such that the three maps
%   \begin{equation*}
%     x\mapsto \tilde {f^\Delta}(x,t),\qquad
%     x\mapsto \partial_t\tilde {f^\Delta}(x,t),\qquad
%     x\mapsto \nabla_x\tilde {f^\Delta}(x,t),\qquad
%   \end{equation*}
%   have Lipschitz constant $L$ for all $t\in [0,1]$ and all
%   $x\in\R^d$. Again, $L$ solely depends on
%   $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$ and thus on $L_1$, $L_2$, $k$,
%   $d$.

%   Next, let $M>\exp(L)$ be so large that
%   $[0,1]^d \subseteq [-M+\exp(L),M-\exp(L)]^d$.  According to
%   \cite[Theorem~4.2]{NNApproximation2}, there exists a ReLU neural
%   network $g$ satisfying the bounds \eqref{eq:LS} (with a constant
%   depending on $\|{f^\Delta}\|_{C^k([0,1]^d\times [0,1])}$ and thus on
%   $L_1$, $L_2$, $d$, $k$) and \jztd[inline]{This can't be correct,
%     because a ReLU network is piecewise linear, so not in $C^1$. I
%     suggest to again just use our result from the first paper with
%     ReLU$^2$ networks. In this case also the text at the beginning
%     needs to be adjusted, where we say that we first work with ReLU
%     and then ReLU$2$ networks.}
%   \begin{equation}\label{eq:f-geps2}
%     \|{f^\Delta}(x)-g(x)\|_{C^1([-M,M]^d\times [0,1])}\le\eps.
%   \end{equation}
%   As in the proof of Proposition \ref{prop:WpNN}, we conclude that
%   $X_g(x,1)\in\R^d$ is well-defined and
%   \begin{equation*}
%     \|X_{f^\Delta}(x,t)-X_g(x,t)\|\le \eps \exp(L)\qquad\forall x\in[0,1]^d,~t\in [0,1].
%   \end{equation*}
%   Since the trajectories $t\mapsto X_{f^\Delta}(x,t)$ remain in
%   $[0,1]^d\times [0,1]$ according to Proposition \ref{prop:cube}, we conclude that
%   \begin{equation}\label{eq:trajectoriesM}
%     X_g(x,t)\in [-M,M]^d\qquad\forall x\in [0,1]^d,~t\in [0,1].
%   \end{equation}
%   Moreover, as in the proof of Proposition \ref{prop:WpNN} holds
%   \begin{equation*}
%     W_2(\rho,X_g(\cdot,1)_\sharp\pi)^2\le C\eps^2.
%   \end{equation*}

%   To bound $J(g)$, it remains to treat the term
%   $R(g)$ in \eqref{eq:Rf}. We have
%   \begin{align*}
%     \|\nabla_X\tilde f(X_{f^\Delta}(x,t))-\nabla_Xg(X_g(x,t))\|
%     \le
%     &\|\nabla_X\tilde {f^\Delta}(X_f(x,t))-\nabla_X\tilde {f^\Delta}(X_g(x,t))\|\nonumber\\
%     &+\|\nabla_X\tilde {f^\Delta}(X_g(x,t))-\nabla_Xg(X_g(x,t))\|.
%   \end{align*}
%   Since $\nabla_X\tilde {f^\Delta}$ has Lipschitz constant $L$,
%   The first term on the right-hand side is bounded by $L\exp(L)\eps$.
%   Due to $X_g(x,t)\in [0,1]^d\subseteq [-M,M]^d$ by \eqref{eq:trajectoriesM},
%   we can use \eqref{eq:f-geps2} to bound the second term which gives
%   \begin{equation*}
%     \|\nabla_X\tilde {f^\Delta}(X_{f^\Delta}(x,t))-\nabla_Xg(X_g(x,t))\|
%     \le L\exp(L)\eps+\eps
%     \qquad\forall x\in[0,1]^d,~t\in [0,1].
%   \end{equation*}
%   Similarly we obtain
%   \begin{equation*}
%     \|\partial_t \tilde {f^\Delta}(X_{f^\Delta}(x,t))-\partial_t g(X_g(x,t))\|
%     \le L\exp(L)\eps+\eps
%     \qquad\forall x\in[0,1]^d,~t\in [0,1]
%   \end{equation*}
%   and
%   \begin{equation*}
%     \|\tilde {f^\Delta}(X_{f^\Delta}(x,t))-g(X_g(x,t))\|
%     \le L\exp(L)\eps+\eps \qquad\forall x\in[0,1]^d,~t\in [0,1].
%   \end{equation*}
%   Therefore
%   \begin{align*}
%     R(g)=|R({f^\Delta})-R(g)|
%     &\le \int_0^1\|\nabla_X \tilde f(X_{f^\Delta}(x,t)) \tilde {f^\Delta}(X_{f^\Delta}(x,t),t)+\partial_t \tilde f(X_{f^\Delta}(x,t),t) \\
%     &\qquad\qquad-\nabla_X g(X_g(x,t)) g(X_g(x,t),t)-\partial_t g(X_g(x,t),t)\|^2\dd t\\
%     &\le C \int_0^1 \eps^2\dd t,
%   \end{align*}
%   where $C$ only depends on $L$.
% \end{proof}

\subsection{KL-divergence} %and ReLU$^2$ networks} 
% We first recall the definition of ReLU$^2$ networks.
% % the definition of ReLU$^m$ networks, a generalization of ReLU networks that is used in many settings where higher order smoothness of the approximating class is required.
% % Similar network structures have been used in \cite{StatisticalNODE, MLPDEStatisticalRate}. 
% % Let $\eta_1(x) = \max\{x, 0\}$ being the ReLU
% % activation function, and $\eta_m(x) = \max\{x, 0\}^m$ be the ReLU$^m$
% % activation function.  

% \begin{definition}\label{def:ReLUm}
%   Denote $\sigma_2(x):=\max\{0,x\}^2$ and let
%   % Let $m\ge 1$ and fix
%   $d_1,d_2\ge 1$. Then, the class of ReLU$^2$
%   networks mapping from $[0,1]^{d_1}$ to $\mathbb{R}^{d_2}$, with
%   % height
%   depth
%   $L$, width $W$, sparsity % constraint
%   $S$, and % norm constraint
%   bound $B$ on the network weights, is denoted
%     % defined by
  
%   \begin{align*}
%     \Phi^{d_1,d_2}(L, W, S, B) = \Big\{&\big(W^{(L)}\jz{\sigma_2}(\cdot) + b^{(L)}\big)\circ\cdots\circ\big(W^{(1)}\jz{\sigma_2}(\cdot) + b^{(1)}\big): W^{(L)} \in \mathbb{R}^{1\times W},\\
%                                  &b^{(L)}\in\mathbb{R}^{d_2}, W^{(1)}\in\mathbb{R}^{W\times d_1},b^{(1)}\in\mathbb{R}^W, W^{(l)}\in\mathbb{R}^{W\times W},b^{(l)}\in\mathbb{R}^W (1<l<L),\\
%                                  &\sum_{l=1}^L\big(\|W^{(l)}\|_0 + \|b^{(l)}\|_0\big) \leq S, \max_{1\leq l\leq L} \big(\|W^{(l)}\|_{\infty,\infty}\lor \|b^{(l)}\|_\infty\big)\leq B \Big\}.\\    
%   \end{align*}
%   % We refer to an element of $\Phi^{d_1, d_2}(L, W, S, B)$ as a
%   % \emph{ReLU$^2$ network}.
% \end{definition}

Now we discuss convergence of the objective $J$ defined in \eqref{Jobjective} for case where $\mathcal{D}$ is the KL divergence, i.e., 
  \begin{equation*}
    J_{\rm KL}(f) := {\rm KL}( X_f(\cdot,1)_\sharp \pi \vert \vert \rho ) + R(f).
  \end{equation*}
Note again that by Theorem \ref{thm:ExSol}, we have $\inf_{f\in\mathcal{V}} J_{\rm KL}(f)=0$. 
  
For the KL-divergence to be well-defined, we need to enforce the constraint that the ODE flow map is a diffeomorphism onto $[0,1]^d$. Therefore, we shall use the neural network based ansatz space introduced in \cite{StatisticalNODE}.
% Note the construction is similar to $C^k_{ansatz}(r)$ that we introduced earlier. 
% \begin{definition}
To this end define $\chi_d(x_1,\dots x_d): D\rightarrow D$ via
$$\chi_d(x_1,\dots x_d) = [x_1(1-x_1),\dots ,x_d(1-x_d)]^T.$$ Letting
$\otimes$ be the coordinate-wise (Hadamard) multiplication of two vectors,
% (of
%   the same dimension).  Then
for any velocity field
$f: [0,1]^d\times[0,1]\rightarrow \mathbb{R}^d$,
$f\otimes\chi_d$ then yields a vector field with vanishing
normal components at the boundary of $[0,1]^d$ at any 
time $t\in [0,1]$.  % Similarly, we let $\oslash$ denote
  % coordinate-wise division of two vectors.
  % \jztd{$\oslash$ is never used}
% \end{definition}
	% \begin{definition}
          % We let
          % \begin{align*}
          %   \Phi_\text{ansatz}^{d+1, d}(L, W, S, B)  \coloneqq \Big\{ f^{\text{NN}}(x_1,\ldots, x_d,t)\otimes\chi_d(x_1,\ldots, x_d),~f^\text{NN}\in\Phi^{d+1,d}(L, W, S, B)\Big\},
          % \end{align*}
          % where $\Phi^{d+1,d}(L, W, S, B)$ is the class of ReLU$^2$
          % networks defined in \eqref{def:ReLUm} and $L, W, S, B$ are
          % the respective network parameters. For $r\ge 0$, we further
          % define the following bounded sparse neural network classes
          % We let
We define our neural network ansatz space as
          \begin{align}\label{eq:FNN}
            &\mathcal{F}_{\text{NN}}(L,W, S, B, r) :=\nonumber\\
            &\qquad\set{f^{\text{NN}}(x_1,\ldots, x_d,t)\otimes\chi_d(x_1,\ldots, x_d)}{f^\text{NN}\in\Phi^{d+1,d}(L, W, S, B),~% }\nonumber\\
            % % \Phi^{d+1,d}_\text{ansatz}(L, W, S, B)
            % &\cap \{f\in W^{2,
            %   \infty}(\Omega):
              \|f\|_{W^{2,\infty}(\Omega)}\leq r}.
          \end{align}








\begin{proposition}\label{Prop:KLerrorNN}
    
Let $k \geq 2$ and $\rho, \pi$ two probability distributions satisfying Assumptions \ref{ass:dens1} and \ref{ass:dens2} with $\Omega_0 = \Omega_1 = [0,1]^d$.

Then there exists constants $C_{d,k}$
and $C_{d,k,L_1,L_2}'$
such that for every $\eps\in(0,1]$, there exists a ReLU$^2$ network $g$ in the ansatz space $\mathcal{F}_{NN}(L, W, S, B, r) $ with parameters satisfying
\begin{equation*}
  L \leq C_{d,k},\quad
  W \leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k-1}},\quad
  S\leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k-1}},\quad
  B\leq C_{d, k, L_1,L_2}'\eps^{-\frac{1}{k-1}},\quad
  r\leq C_{d, k, L_1, L_2}'
\end{equation*}
  such that for another constant $C_{d, k, L_1, L_2}$, we have 
\[ \mathrm{KL}(X_g(\cdot,1)_\sharp \pi, \rho)\leq C_{d, k ,L_1, L_2}'\eps^2.\]
% for some constant $C_{d, k ,L_1, L_2}$ depending on $d, k, L_1, L_2$.
\end{proposition}
\begin{proof}
According to Theorem \ref{thm:tri}, there exists
  $f^\Delta\in C^k([0,1]^d\times [0,1])$ such that
  $X_{f^\Delta}(\cdot,1)_\sharp\pi=\rho$ (i.e.\
  $\mathcal{D}_{KL}(\rho,X_{f^\Delta}(\cdot,1)_\sharp\pi)=0$), $f^\Delta$ realizes straight line
  trajectories (i.e.\ $R(f^\Delta)=0$) and $\|f^\Delta\|_{C^k([0,1]^d\times [0,1])}$
  only depends on $L_1$, $L_2$, $k$, $d$. By Step 1 of \cite[Theorem 20]{StatisticalNODE}, for all $N \geq 1$,  there exists % an approximating element
  $g\in \mathcal{F}_{NN}(L, W, S, B, r)$ with
  \begin{equation*}
    L \leq C_{d, k},\quad W \leq N,\quad S \leq N,\quad B \leq C_{d,k}\|f^\Delta\|_{C^k([0,1]^d\times [0,1])} + N^{\frac{1}{d+1}},\quad
    r \leq C_{d, k, L_1, L_2},
  \end{equation*}
  such that $\|f^\Delta- g\|_{C^1([0,1]^d\times[0,1])} \leq C_{d, k, L_1, L_2}N^{-\frac{k-1}{d+1}}$, where $C_{d,k}$ and $\tilde C_{d, k, L_1, L_2}$ are constants depending on $d, k$ and $d, k, L_1, L_2$ respectively. Letting $\tilde C_{d, k, L_1, L_2}N^{-\frac{k-1}{d+1}} = \eps$, we solve for $N$ to get $N = C_{d, k, L_1, L_2}\eps^{-\frac{d+1}{k-1}}$
  for some $C_{d,k,L_1,L_2}$. By Theorem \ref{Thm:KLstatbility}, we then have $\mathcal{D}_{KL}(X_g(\cdot,1), \rho) \leq C_{d, k, L_1, L_2}\eps^2$. 
\end{proof}











\begin{theorem}\label{Thm:JBoundKL}
Let $k \geq 2$ and $\rho, \pi$ two probability distributions satisfying Assumptions \ref{ass:dens1} and \ref{ass:dens2} with $\Omega_0 = \Omega_1 = [0,1]^d$.

Then there exist constants $C_{d,k}$ and $C_{d, k, L_1, L_2}'$ such that for every $\eps\in(0,1]$, there exists a ReLU$^2$ network $g$ in the ansatz space $\mathcal{F}_{NN}(L, W, S, B, r) $ with parameters satisfying
\begin{equation*}
  L \leq C_{d,k},\quad
  W \leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k-1}},\quad
  S\leq C_{d, k, L_1, L_2}'\eps^{-\frac{d+1}{k-1}},\quad
  B\leq C_{d, k, L_1,L_2}'\eps^{-\frac{1}{k-1}},\quad
  r\leq C_{d, k, L_1, L_2}'
\end{equation*}
  such that for another constant $C_{d, k, L_1, L_2}$, we have
\[J(g) \leq 
% \inf_{f\in\mathcal{V}}J(f) + 
C_{d, k ,L_1, L_2}\eps^2.\]
\end{theorem}
\begin{proof}
The proof follows % exactly as the proof of
by the same arguments as the proof of Theorem \ref{Thm:JBoundWasserstein}. The error in the KL-divergence part of the objective follows from Proposition \ref{Prop:KLerrorNN} and the error in minimal-energy regularization has already been % handled
treated in Theorem~\ref{Thm:JBoundWasserstein}.    
\end{proof}


\begin{remark}
We comment here that the estimates we obtained above are $L^\infty$ in nature, thus we are able to derive distribution error estimates in other metrics as well, such as Hellinger, chi-square and total variation.  
\end{remark}



























% \section{Distribution Approximation via ODE Flow Maps: Stability Results}\label{sec:DistApproximation}


% \begin{theorem}\label{thm:FlowMapBound}
% Let $f$, $g\in\mathcal{V}$ such that $\|f - g\|_{L^\infty(\Omega_{[0,1]})}\leq \delta$. Moreover, assume $f\in C^1(\Omega_{[0,1]})$ with $\|f\|_{C^1(\Omega_{[0,1]})}\leq C$. 
% % such that $\sup_{t\in[0,1]}\|f(\cdot,t)\|_{W^{1,\infty}(D)} \leq C$ and $\sup_{t\in[0,1]}\|g(\cdot,t)\|_{W^{1,\infty}(D)} \leq C$. 
%  Let $X$, $Y:\Omega_{[0,1]}\to\mathbb{R}^d$ be the solutions to 
%  \begin{equation*}
%  \frac{dX(x,t)}{dt} = f(X(x,t),t),\qquad \frac{dY(x,t)}{dt} = g(Y(x,t),t) 
%  \end{equation*}
%  with initial condition $X(x,0) = Y(x,0) = x$.
% For $x\in \Omega_0$ set
% \begin{equation*}
% T(x) = x + \int_0^1f(X(x,t),t)dt\qquad\text{and}\qquad G(x) = x + \int_0^1g(Y(x,t),t)dt.
% \end{equation*}

% Then $\|T - G\|_{L^\infty(\Omega_0)} \leq   \delta e^{C}$.
% \end{theorem}

% The proof of Thm.~\ref{thm:FlowMapBound} is similar to the proof of Thm.~\ref{thm:l2bound}, and the core idea is similarly to derive a differential equation satisfied by $|X(x,t) - Y(x,t)|$. Then, applying Grownall's inequality gives the result. For details, see Appendix \ref{app:proofsec6}. 





% \begin{corollary}
% Assume $f$ is the velocity field from Thm.~\ref{thm:f} whose time one flow map realizes a transport map that pushes forward $\pi$ to $\rho$ and $g\in\mathcal{V}$ is an approximator of $f$ the satisfies the assumptions in Thm. \ref{thm:FlowMapBound}. Then, the Wasserstein distance between $\pi_f(\cdot, 1) = \rho$ and $\pi_{g}(\cdot, 1)$ is bounded by as follows:
% $$W_p(\rho, \pi_{g}(\cdot, 1)) \leq \delta e^{C},$$
% for $p \geq 1$.
% \end{corollary}
% \begin{proof}
% From the proof of Thm.~\ref{thm:FlowMapBound}, $\max_j \|X_j(\cdot, 1) - Y_j(\cdot, 1)\|_{L^\infty(\Omega_0)}\leq \delta e^{C}$. By Thm.1 in \cite{Wassersteinbound}, the Wasserstein distance between the push-forward measures is also bounded by $\delta e^{C}$ for $p \geq 1$. 
% \end{proof}








% \section{Neural Network Approximation Results}\label{sec:NN}
% \ymmtd{Give a map of the section. We cite some prior results on NN
%   approximation. Then we can simply apply these to our results on
%   regularity of the velocity field (either the general result or the
%   specific KR version). But the main new result in this section is Thm
%   6.5, which shows \ldots }

% There are a number of prior works concerning approximation of smooth
% functions by neural networks. \cite{NNApproximation1} shows that deep
% ReLU networks approximate smooth functions more efficiently than
% shallow networks and also proves upper and lower bound for the
% complexity of networks for approximation in the setting of Sobolev
% spaces. Later, \cite{NNApproximation3} and \cite{NNApproximation4}
% explore the phase diagram for feasible approximation rates of neural
% networks. All these works concern approximation of Sobolev functions
% on finite dimensional cubes; however, $L^\infty$ norm is used to
% quantify approximation errors in these works. In
% \cite{NNApproximation2}, approximation rates in terms of $W^{k,p}$
% norm are established, for $0\leq k\leq 1$.


% In the definition of neural networks, we distinguish between a neural
% network as a set of weights and an associated function that we call
% the realization of the neural network.

% \begin{definition}
%   Let $d, L\in \mathbb{N}$, a neural network $\Phi$ with input
%   dimension $d$ and $L$ layers is a sequence of matrix-vector tuples
%   $\phi = ((A_1,b_1), (A_2,b_2),..., (A_L, b_L))$, where for
%   $N_0=d$,$N_1,...N_L\in\mathbb{N}$, $A_l$ is a
%   $N_l \times \sum_{k=0}^{l-1}N_k$ matrix and
%   $b_l \in \mathbb{R}^{N_l}$. Let
%   $\rho:\mathbb{R}\rightarrow\mathbb{R}$ be ReLU activation, then the
%   realization of the neural network $\Psi$ is the map
%   $R(\Psi):\mathbb{R}^d\rightarrow\mathbb{R}^{N_L}, R(\Psi)(x) = x_L$
%   where $x_L$ is defined through the following recursive scheme:
%   \begin{itemize}
%   \item $x_0 \coloneqq  x$
%   \item $x_l = \rho(A_l[x_0^T|...|x^T_{l-1}]^T + b_l)$ for
%     $l=1,..,L-1$
%   \item $x_L = A_L[x_0^T|...|x^T_{L-1}]^T + b_L$.
%   \end{itemize}
%   $N(\Psi) = d + \sum_{j=1}^L N_j$ is called the number of neurons of
%   $\Psi$ and $L = L(\Psi)$ is called the number of layers and
%   $W(\Psi) = \sum_{j=1}^L \|A_j\|_0 + \|b_j\|_0$ is called the number
%   of weights.
% \end{definition}




% \begin{theorem}[{\cite[Thm. 4.2]{NNApproximation2}}]\label{thm:NNApproximation}
%   Let $\mathcal{F}_{n, d, p, B}$ be the Sobolev ball
%   $\mathcal{F}_{n, d, p, B}  \coloneqq  \{f\in W^{n, p}((0,1)^d)): \|f\|_{W^{n,
%       p}((0,1)^d))} \leq B\}$. Let $d \in \mathbb{N}$,
%   $n\in\mathbb{N}_{\geq 2}, 1 \leq p \leq \infty, B > 0, 0 < s <
%   1$. Then, there exists a constant $c = c(d, n, p, B, s)$ with the
%   following properties:

%   For any $\epsilon \in (0, 1/2)$, there is a neural network
%   architecture $\mathcal{A} = \mathcal{A}(d, n, p, B, s, \epsilon)$
%   with $d-$dimensional input and 1-dimensional output such that for
%   every $f\in \mathcal{F}_{n, d, p, B}$, there is a neural network
%   $\Psi_\epsilon^f$ that has architecture $\mathcal{A}_\epsilon$ such
%   that
%   $\|R_\rho(\Psi_\epsilon^f) - f\|_{W^{s,p}((0,1)^d)}\leq \epsilon$
%   and
%   \begin{itemize}
%   \item $L(A_\epsilon) \leq c\log_2(\epsilon^{-n/(n-s)})$.
%   \item
%     $W(\mathcal{A}_\epsilon) \leq
%     c\epsilon^{-d/(n-s)}\log_2(\epsilon^{-n/(n-s)})$.
%   \item
%     $N(\mathcal{A}_\epsilon) \leq
%     c\epsilon^{-d/(n-s)}\log_2(\epsilon^{-n/(n-s)})$
%   \end{itemize}

% \end{theorem}
% Then, we have the following corollary when applied to the
% Knoth-Rosenblatt map constructed directly from the densities.
% \begin{corollary}\label{corollary:NetParameterBound}
%   Let $\pi$ and $\rho$ be measures supported on $\Omega_0$ and
%   $\Omega_1$ respectively and
%   $\forall x \in \Omega_0\cup\Omega_1, |x| < M$. Suppose the domains
%   and the densities satisfy assumptions \ref{ass:dens1} and
%   \ref{ass:dens2}. Then, there exists a neural network architecture
%   $\mathcal{A}_{\epsilon}$ whose size is bounded as follows and
%   approximates the Knoth-Rosenblatt map within $\epsilon$ error in
%   $W^{1,\infty}$ norm.
%   \begin{itemize}
%   \item
%     $L(A_\epsilon) = O\left(\log_2\epsilon + \log_2M +
%       (2k^{d+1}+2dk^d)\log_2\frac{L_1}{L_2}) \right)$
%   \item
%     $W(\mathcal{A}_\epsilon) = O(\epsilon^{-d/(k-1)}\log_2\epsilon +
%     )$
    
    
%   \end{itemize}
% \end{corollary}
% \rrtd{Still looking for the best way to express this}.
% \begin{remark}
%   Some caveats here:
%   \begin{itemize}
%   \item The neural network approximation from
%     Thm.~\ref{thm:NNApproximation} only work for functions on the unit
%     hypercube $(0,1)^d$. The velocity field we constructed from the
%     minimal energy (straight line) regularization will in general not
%     be the unit hypercube. However, from Thm.~\ref{thm:LipDomain}, we
%     know it will always be a Lipschitz domain and therefore, can be
%     extended to a larger hypercube that covers $\Omega$ by
%     Thm.~\ref{thm:functionExtension} and we may perform the
%     approximation on this larger hypercube. This scaling would in
%     general introduce a factor that is exponential in $d$ in the
%     network parameters. However, since we already have factors
%     exponential in $d$ in the expressions and we choose to not keep
%     track of this layer here.
%   \item In the big $O$ notation, we treat $d$ and $k$ as constants and
%     only care about the asymptotic behaviour of
%     $\epsilon, M, L_1, L_2$.
  
%   \end{itemize}
% \end{remark}





% Combining the neural network approximation results and the regularity
% results from previous sections, we will show that if we search over
% the class of neural networks with certain bounded size, then there is guarantees to exist a network that
% achieves near-optimal value in the optimization objective \ref{OP}.
% \begin{theorem}\label{thm:ObjectiveErrorBound}
%   Let $\pi$ and $\rho$ be measures that satisfy assumptions \ref{ass:dens1} and
%   \ref{ass:dens2}. Let $\mathcal{A}_\epsilon$ be set of the neural network
%   architectures that satisfy the bounds in Corollary
%   \ref{corollary:NetParameterBound}. Let $f$ be the velocity field
%   from Thm.~\ref{thm:ExSol} corresponding to the
%   Knoth-Rosenblatt transport map $T$ that pushes forward $\pi$ to
%   $\rho$ and $J$ the optimization objective from \ref{OP}. Then,
%   there exists a realization of some network architecture in $\mathcal{A}_\epsilon$,
%   $\Psi^f_\epsilon$, such that for $\epsilon > 0$,
% $$|J(f) - J(\Psi^f_\epsilon)| \leq \log\left(1+\frac{d^2\epsilon^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2}{L_2}e^{1 + dC} \right)  + (2C+1)^2(\epsilon Ce^C + \epsilon)^2 ,$$
% where $C$ is a finite number such that $\|f\|_{C^2(\Omega_{[0,1]})} \leq C$
% and $\|\Psi^f_\epsilon\|_{W^{1,\infty}(\Omega_{[0,1]})} \leq C$.

% \end{theorem}
% \begin{remark}
% The two error terms are both of order $O(\epsilon^2)$ in the above theorem. 
% \end{remar


\section{Discussion and future work}
Our work is a crucial first step towards establishing a theoretical framework for sampling and distribution learning through ODE flow maps. In particular, the approximation results in this work can be viewed as quantifying the \textit{bias} term in the classical context of statistical learning theory. In our parallel work \cite{StatisticalNODE}, we explore the \textit{variance} term by analyzing the statistical complexity of the function class represented by ODE flow maps whose velocity fields come from a bounded neural network class. 

While our work establishes a theoretical framework for analyzing ODE-based models, several important questions still remain open. First, we only consider distributions supported on bounded domains because the Lipschitz constant of straight-line ansatz considered in this work and \cite{StatisticalNODE} can be uncontrollable when the distributions are not lower bounded. It will be interesting to see how our theories could be extended to the case of unbounded domains. Second, the approximation errors obtained in this work depend on $d+1$ and $k-1$, because we considered the velocity field as a generic function on $\mathbb{R}^{d+1}$ and the approximation error metric considered is in $C^1$ for distributional stability results. This approximation error leads to nonparametric convergence rate of $n^{-\frac{2(k-1)}{d+1 + 2(k-1)}}$ in \cite{StatisticalNODE}. We note this statistical rate obtained is suboptimal, compared to the minimax optimal rate of learning $k$-smooth densities on $d$-dimensional domains (which is $n^{-\frac{2k}{d + 2k}}$). Therefore, an important question to ask is whether neural-ODE based models can achieve the same minimax optimal statistical rates, as classical density estimators such as wavelets or kernel-based methods. We leave these open questions to future studies. 


\section*{Acknowledgments and disclosure of funding}
ZR and YM acknowledge support from the US Air Force Office of Scientific Research (AFOSR) MURI, Analysis and Synthesis of Rare Events, award number FA9550-20-1-0397, and from the US Department of Energy (DOE), Office of Advanced Scientific Computing Research, under grants DE-SC0021226 and DE-SC0023187. ZR also acknowledges support from a US National Science Foundation Graduate Research Fellowship.















\appendix








\section{Comments on training}\label{app:training}
\input{training}




\section{Knothe--Rosenblatt construction of triangular transport maps}\label{app:KRMap}	
Given probability measures $\rho$ and $\pi$, the Knothe--Rosenblatt transport is, under appropriate conditions, the unique triangular monotone transport $T$ such that $T_\sharp\pi = \rho$. In this section, we describe the explicit Knothe--Rosenblatt construction of triangular transport maps, as presented in \cite{OTAppliedMathematician}. Let $d\in\mathbb{N}$ be the dimension. For simplicity of presentation, we assume that $\pi$ and $\rho$ are supported on the hypercube $[0,1]^d$. Let $\mu$ be a base measure (for example the Lebesgue measure) and assume that $\frac{d\pi}{d\mu} = \pi(x)\in C^0([0,1]^d, \mathbb{R}^+)$ and $\frac{d\rho}{d\mu} = \rho(x)\in C^0([0,1]^d, \mathbb{R}^+)$ are the corresponding densities. Assume also that the densities $\rho(x)$ and $\pi(x)$ are uniformly bounded from below by a positive constant. 

For a continuous density function $f \in \{\rho, \pi\}$, we define the following auxiliary functions for $x\in[0,1]^k$, $k\leq d$:
\begin{equation}
\begin{aligned}
\hat{f}_k(x) &= \int_{[0,1]^{d-k}}f(x, t_{k+1},\dots ,t_d)d\mu((t_j)_{j=k+1}^d)\\
f_k(x) &= \frac{\hat{f}_k(x)}{\hat{f}_{k-1}(x_{[k-1]})}.
\end{aligned}
\end{equation}
Hence $f_k(x_{[k-1]}, \cdot)$ is the marginal density of the variable $x_k$ conditioned on $x_{[k-1]} = (x_1,\dots ,x_{k-1})\in[0,1]^{k-1}$.

Then, we define the corresponding CDFs:
\begin{equation}\label{eq:Fpik}
\begin{aligned}
F_{\pi,k}(x_{[k-1]}, x_k) &= \int_{0}^{x_k}\pi_k(x_{[k-1]}, t_{k})d\mu(t_k)\\
F_{\rho,k}(x_{[k-1]}, x_k) &= \int_{0}^{x_k}\rho_k(x_{[k-1]}, t_{k})d\mu(t_k),
\end{aligned}
\end{equation}
which are well-defined for $x\in[0,1]^k$ and $k\in\{1,\dots ,d\}$. Note that these are interpreted as functions of the last variable $x_k$ with $x_{[k-1]}$ fixed. In particular, we let $F_{\rho,k}(x_{[k-1]}, \cdot)^{-1}$ be the inverse of the map $x_k\rightarrow F_{\rho,k}(x_{[k-1]}, x_k)$

For $x \in [0,1]^d$, the Knothe--Rosenblatt map is constructed recursively in the following way. First, define
$$T_1(x_1) = F_{\rho,1}^{-1}\circ F_{\pi,1}(x_1),$$
and for $k > 1$, define
\begin{equation}\label{eq:Tk}
  T_k(x_{[k-1]}, \cdot) = F_{\rho,k}(T_1(x_1),\dots ,T_{k-1}(x_{[k-1]}), \cdot)^{-1}\circ F_{\pi,k}(x_{[k-1]}, \cdot).
\end{equation}
Then the map $$T(x_1,\dots,x_d) =  \left ( T_1(x_1), T_2(x_{[2]}),\dots ,T_d(x_{[d]}) \right )$$
is the triangular Knothe--Rosenblatt transport $T:[0,1]^d\rightarrow[0,1]^d$, for which we have the following theorem:
\begin{theorem}\label{thm:TraingularPushForward}
The triangular Knothe--Rosenblatt map satisfies $T_\sharp\pi = \rho$ and  $\det\nabla T(x)\rho(T(x)) = \pi(x)$, $\forall x\in[0,1]^d$.
\end{theorem}
We comment that regularity assumptions for Theorem \ref{thm:TraingularPushForward} can be relaxed. For a more detailed discussion, see \cite{TrangularTransportofMeasure}. 
% \ymmtd{Can we cite something for this theorem? Also, some technicalities: do we need to comment on the definition of the Jacobian determinant, and note that the necessary ``diagonal derivatives'' $\partial_{x_k} T_k$ exist under our assumption that the densities are continuous? Also, should that $\forall x$ be $\mu$-a.e.\ instead? (I don't think we assumed that the densities are uniformly bounded in this section. Maybe we could.)}
% \ymmtd{@robert: The Santambrogio result doesn't actually include the formula for the pullback density that you quote here. I think there might be some technicalities required to show this too? I'm quite sure that it holds given the assumptions in this section, but strictly speaking we did not show it and it's a bit of leap.}
	




    
\section{% Faa di Bruno and the inverse function theorem in infinite-dimensional spaces
  Auxiliary results
}\label{app:AuxResults}
In this appendix, we collect statements and proofs % for technical lemmas
that are % used in
required for the proofs of the main theorems. % in the main text. 

First, we present two technical results about domains. 
\begin{lemma}\label{lemma:convexLip}
(uniform-cone
characterization of convex domains) 
  Let $\Omega\subset\mathbb{R}^d$ be a bounded, convex, and open
  domain. Then $\Omega$ is a Lipschitz domain.
\end{lemma}	
\begin{proof}[Proof of Lemma \ref{lemma:convexLip}]
  Without loss of generality, we may assume that $0\in\Omega$. Since
  $\Omega$ is bounded and open, there exist $r, R > 0$ such that
  $B(0,r)\subset \Omega\subset B(0,R)$, where $B(0,r)$ and $B(0,R)$
  denote balls of radius $r$ and $R$ respectively.

  Then, we can cover the surface of the ball of radius $R$ by overlapping
  $d-1$ dimensional balls of radius $\epsilon$ such that the boundary
  of each such ball, $B_{d-1}(0, \epsilon)$, is completely covered by
  the adjacent balls. If $\vec{n}$ denotes the unit vector emanating
  from the origin in the direction of the center of such a ball, then
  $U = \setl{t\vec{n} + y}{t\geq 0, y\in B_{d-1}(0, \epsilon)}$ is the
  cylinder of radius $\epsilon$ whose intersection with $B(0,R)$ is
  the boundary of this ball.

  Since the surface of $B(0, R)$ can be covered by finitely many such
  $d-1$ dimensional balls, we can find a finite collection of such
  cylinders $\{U_j\}_{j=1}^J$ so that their union cover $\Omega$. From
  this construction of $\{U_j\}_{j=1}^J$, the first property in the
  definition of Lipschitz domain is clearly satisfied.

  To verify the second property, note that for each $j$, the
  coordinate system is simply the map that transforms the cylinder
  $U_j$ to align with the direction of $e_d$, where $e_d$ is the last
  vector of the standard basis of $\mathbb{R}^d$. For any
  $x \in \partial\Omega\cap U_j$, the cone defined by the convex
  closure of $\{x\} \cup B(0,r)$ is contained in the closure of
  $\Omega$ and the head angle $\bsalpha$ of the cone satisfies
  $\sin(\frac{\bsalpha}{2}) \geq \frac{r}{R}$, and thus the boundary
  is a Lipschitz function.
\end{proof}	

The image of a Lipschitz domain under a sufficiently regular map
  remains a Lipschitz domain:
\begin{theorem}[{\cite[Theorem ~4.1]{LipTransformation}}]\label{thm:LipTransformation}
  Assume $\Omega \subset\mathbb{R}^d$ is a bounded Lipschitz
    domain and $\mathcal{O}$ is an open neighborhood of $\bar{\Omega}$ and
  $f:\mathcal{O}\rightarrow \mathbb{R}^n$ is a $C^1$-diffeomorphism
  onto its image. Then, $\tilde{\Omega} = f(\Omega)$ is also a
  Lipschitz domain.
\end{theorem}
	
Next, we we shall state the following regularity result about the regularity of optimal transport map from \cite{OptimalRegularity}, which is used in the proof of Theorem \ref{thm:ExSol}.
\begin{theorem}\label{Thm:OptimalRegularity}
Fix open sets $\Omega_0, \Omega_1\subset\mathbb{R}^d$ with $\Omega_1$ convex and absolutely continuous measure $\mu, \nu$ with finite second moment and bounded, strictly positive densities $f, g$ respectively such that $\mu(\Omega_0) = \nu(\Omega_1) = 1$. Let $\phi$ be such that $\nabla\phi_\sharp\mu = \nu$. If $\Omega_0$ and $\Omega_1$ are bounded and $f, g$ bounded from below, then $\phi$ is strictly convex and of class $C^{1, \alpha}(\Omega_0)$ for some $\alpha > 0$. In addition, if $f, g\in C^{k, \alpha}$, then $\phi\in C^{k+2, \alpha}(\Omega_0)$. 
\end{theorem}

Then, we shall need the following results about composition and the inverse of a function in Section \ref{sec:regularity}.

  \begin{theorem}[Fa\'{a} di Bruno]
    Let $k\in\N$.
  Let $X$, $Y$ and $Z$ be three Banach spaces, and let
  $F\in C^k(X,Y)$ and $G\in C^k(Y,Z)$.

  Then for all $0\le n\le k$ and with
  % Let $g:B_1\rightarrow B_2$ and $f: B_2\rightarrow B_3$ be $C^k$ functions on Banach spaces, then we have for $n < k$
  $T_n := \setl{\bsalpha \in\N^n}{\sum_{j=1}^n j\alpha_j =
    n}$, % it holds for
  for all $x$, $h\in X$ the $n$th derivative
  $[D^n(G\circ F)](x)(h^n)\in Z$ of $G\circ F$ at $x$ evaluated at
  $h^n\in X^n$ equals
  % \jztd{changed $\bsk$ to $\bsalpha$ since $k$ already
  %   has a meaning here}
  \begin{equation*}
     \sum_{\bsalpha\in T_n}% \frac{n!}{\alpha_1!\ldots \alpha_n!}
    \frac{n!}{\bsalpha!}
    [D^{|\bsalpha|}G](F(x)) \Bigg(\smash[b]{\underbrace{\frac{[DF(x)](h)}{1!},\ldots, \frac{[DF(x)](h)}{1!}}_{\text{$\alpha_1$ times}}},\ldots,\underbrace{\frac{[D^{n}F(x)](h^n)}{n!},\ldots, \frac{[D^{n}F(x)](h^n)}{n!}}_{\text{$\alpha_n$ times}}\Bigg).
  \end{equation*}

% $$[D^n(f\circ g)](y) = \sum_{\bsk\in T_n}\frac{n!}{k_1!\ldots k_n!}[D^{|\bsk|}f](g(y)) \left(\smash[b]{\underbrace{\frac{[Dg](y)}{1!},\ldots, \frac{[Dg](y)}{1!}}_\text{$k_1$ times}},\ldots,\smash[b]{\underbrace{\frac{[D^{n}g](y)}{n!},\ldots, \frac{[D^{n}g](y)}{n!}}_\text{$k_n$ times}}\right),$$ where $T_n = \{\bsk = (k_1,\ldots,k_n) : \sum_{j=1}^n jk_j = n\}$. 
\end{theorem}
\begin{proof}
  
  Without loss of generality assume
  % , we may assume that
  $F(0) = 0\in Y$.
    Using Taylor's theorem (in Banach spaces) for $G$
  \begin{equation*}
    G(F(x)) =  \sum_{r=1}^k\frac{[D^rG](0)}{r!}({\underbrace{F(x),\ldots, F(x)}_\text{$r$ times}}) + o(\|F(x)\|_Y^k)\qquad\text{as }\|F(x)\|_Y\to 0.
  \end{equation*}
Taylor expanding $F(x)$ around $0\in X$ then implies
\begin{equation*}
  G(F(x)) =  \sum_{r=1}^k\frac{[D^rG](0)}{r!}(S_r(x))+ o(\|F(x)\|_Y^k)
\end{equation*}
where, using the notation $x^{\alpha_j}$ to denote $(x,\dots,x)\in X^{\alpha_j}$,
\begin{equation*}
  S_r(x)=
  \left(\sum_{\alpha_1=1}^k\frac{[D^{\alpha_1}F](0)}{\alpha_1!}(x^{\alpha_1}) + o(\|x\|_X^k),\ldots, \sum_{\alpha_r= 1}^k\frac{[D^{\alpha_r}F](0)}{n!}(x^{\alpha_r})+
  o(\|x\|_X^k)\right)\in Y^r.
  % .
\end{equation*}
Note that $F\in C^1$ and $F(0)=0\in Y$ implies
$o(\|F(x)\|^k) = o(\|x\|^k)$ as $x\to 0$.  Using multilinearity of the
differential operators we thus find
\begin{equation*}
  G(F(x)) = \sum_{r=1}^k\sum_{\setl{\bsalpha\in \N^r}{|\bsalpha|\le k}}
  \frac{[D^rG](0)}{r!}
  \left(\frac{[D^{\alpha_1}F](0)(x^{\alpha_1})}{\alpha_1!},\dots,\frac{[D^{\alpha_r}F](0)(x^{\alpha_r})}{\alpha_r!}\right) + o(\|x\|^k)
\end{equation*}
as $x\to 0$.

On the other hand, we can Taylor expand $G\circ F$. That is,
\begin{equation*}
  G(F(x)) = \sum_{r = 1}^k\frac{[D^r(G\circ F)(0)]}{r!}(x^r) + o(\|x\|^k)\qquad\text{as }x\to 0.
\end{equation*}

Comparing the powers of $x$, we get for $n\le k$
\begin{equation*}\label{eq:FdB2}
  D^n(G\circ F)(0)(x^n) = n!\sum_{r=1}^k
  \sum_{\setl{\bsalpha\in\N^r}{|\bsalpha|=n}}
  \frac{[D^rG](0)}{r!}
  \left(\frac{[D^{\alpha_1}F](0)(x^{\alpha_1})}{\alpha_1!},\dots,\frac{[D^{\alpha_r}F](0)(x^{\alpha_r})}{\alpha_r!}\right).
  % \frac{[D^rf](0)}{r!}\prod_{j=1}^r\frac{[D^{n_j}g](0)x^{n_j}}{n_j!},
\end{equation*}


To show that the expression for $D^n(G\circ F)(0)(x^n)$ is equivalent to the one given by Fa\'{a} di Bruno's formula in Theorem \ref{thm:FdB}, we make the following observation: the summation $\sum_{\setl{\bsalpha\in\N^r}{|\bsalpha|=n}}$ is over the partition of a set of $n$ elements into $r$ subsets each of $\alpha_i$ elements such that $r \geq i \geq 1, \alpha_i\geq 1$. If we let set $T_n = \{\bsalpha' = (\alpha_1',\ldots,\alpha'_n) : \sum_{j=1}^n j\alpha_j' = n\}$ as in Theorem \ref{thm:FdB}, each $\alpha_j'$ can be interpreted as the number of subsets with $j$ elements and the total number of subsets is given by $|\bsalpha'|$. Another observation we make is that the summation in Theorem \ref{thm:FdB} takes ordered tuples while the summation in the above expression takes unordered tuple $(\alpha_1,...,\alpha_r)$. If $|\bsalpha'| = r$, the number of ways to arrange a tuple of $r$ elements such that $\alpha_1'$ of the elements are same (of value $1$), $\alpha_2'$ of the elements are the same (of value $2$),..., and $\alpha_n'$ of the elements are same (of value $n$), is given by $\frac{r!}{\alpha_1'!\dots\alpha_n'!} = \frac{r!}{\bsalpha'!}$. 

Therefore, by regrouping the summation, we obtain: 
\begin{align*}
&D^n(G\circ F)(0)(x^n) = n!\sum_{r=1}^k
  \sum_{\setl{\bsalpha\in\N^r}{|\bsalpha|=n}}
  \frac{[D^rG](0)}{r!}
  \left(\frac{[D^{\alpha_1}F](0)(x^{\alpha_1})}{\alpha_1!},\dots,\frac{[D^{\alpha_r}F](0)(x^{\alpha_r})}{\alpha_r!}\right)\\
  % \frac{[D^rf](0)}{r!}\prod_{j=1}^r\frac{[D^{n_j}g](0)x^{n_j}}{n_j!}\\
&= \sum_{\bsalpha'\in T_n}\frac{n!}{r!}\frac{r!}{\bsalpha'!}[D^{|\bsalpha'|}G](0)\Bigg(\smash[b]{\underbrace{\frac{[DF](0)(x)}{1!},\ldots, \frac{[DF](0)(x)}{1!}}_{\text{$\alpha'_1$ times}}},\ldots,\underbrace{\frac{[D^{n}F](0)(x^n)}{n!},\ldots, \frac{[D^{n}F](0)(x^n)}{n!}}_{\text{$\alpha'_n$ times}}\Bigg)\\
&=\sum_{\bsalpha'\in T_n}\frac{n!}{\bsalpha'!}[D^{|\bsalpha'|}G](0)\Bigg(\smash[b]{\underbrace{\frac{[DF](0)(x)}{1!},\ldots, \frac{[DF](0)(x)}{1!}}_{\text{$\alpha'_1$ times}}},\ldots,\underbrace{\frac{[D^{n}F](0)(x^n)}{n!},\ldots, \frac{[D^{n}F](0)(x^n)}{n!}}_{\text{$\alpha'_n$ times}}\Bigg)\\ 
\end{align*}
\end{proof}

\begin{theorem}[Inverse function theorem]
  Let $k\ge 1$, let $X$, $Y$ be two Banach spaces, and let $F\in C^k(X,Y)$.
  At every $x\in X$ for which $DF(x)\in L^1(X,Y)$ is an isomorphism,
  there exists an open neighbourhood $O\subseteq Y$ of $F(x)$ and a function
  $G\in C^k(O,X)$ such that $F(G(y))=y$ for all $y\in O$.

  Moreover, for every $n\le k$ there exists a continuous function
  $C_n:\R_+^{n+1}\to \R_+$ (independent of $F$, $G$, $O$) such
  that for $y=F(x)$ with $x$ as above
  \begin{equation}\label{eq:Cn}
    \|D^n G(y)\|_{\cL^n_{\sym}(Y;X)} \le C_n(\|[DF(x)]^{-1}\|_{\cL^1_{\sym}(Y;X)}, \|DF(x)\|_{\cL^1_{\sym}(X;Y)}, \ldots, \|D^nF(x)\|_{\cL^n_{\sym}(X;Y)}).
  \end{equation}
\end{theorem}
\begin{proof}
  The stated local invertibility of $F$ holds by the inverse function
  theorem in Banach spaces, see for instance
  \cite[Cor.~15.1]{deimling}.

  Next, if $F(G(y))=y$ in a neighbourhood of $y=F(x)$, then by
  Theorem ~\ref{thm:FdB}, for $2\le n\le k$
  \begin{equation}\label{eq:solveDngy}
    0 = D^n(F\circ G)(y) = \sum_{\bsalpha\in T_n}\frac{n!}{\bsalpha!}[D^{|\bsalpha|}F](G(y))\prod_{m=1}^n\left(\frac{[D^m G](y)}{m!}\right)^{\alpha_m},
  \end{equation}
  where $T_n = \setl{\bsalpha \in\N^n}{\sum_{j=1}^n j\alpha_j = n}$.
  The only multiindex with $\alpha_n\neq 0$ is
  $\bsalpha = (0,\ldots,0,1)$. Solving \eqref{eq:solveDngy} for
  $D^nG(y)$, we obtain with
  $\bar{T}_n  \coloneqq  \setl{\bsalpha \in\N^{n-1}}{\sum_{j=1}^{n-1}j\alpha_j=n}$ and
  $y=F(x)$,
  \begin{equation}\label{eq:Dng}
    D^nG(y) = -([DF](G(y)))^{-1} \left ( \sum_{\bsalpha\in \bar{T}_n}\frac{n!}{\bsalpha!}[D^{|\bsalpha|}F](G(y)) \left( \prod_{m=1}^{n-1} \left (\frac{[D^m G](y)}{m!} \right )^{\alpha_m}\right)\right).
  \end{equation}
  Since $G(y)=x$, for $n=1$, we have
  \begin{equation*}
    \|DG(y)\|_{\cL^1(Y;X)} = \|[DF(x)]^{-1}\|_{\cL^1(Y;X)}
  \end{equation*}
  and thus \eqref{eq:Cn} holds with
  \begin{equation*}
    C_1(\|[DF(x)]^{-1}\|_{\cL^1(Y;X)},\|DF(x)\|_{\cL^1(X;Y)}) \coloneqq 
    \|[DF(x)]^{-1}\|_{\cL^1(Y;X)}.
  \end{equation*}

  Since the right-hand side of \eqref{eq:Dng} only depends on $G$
  through $D^mG$ with $m\le n-1$ and on $F$ through $D^{|\bsalpha|}F$
  with $|\bsalpha|\le n$, an induction argument implies the existence
  of $C_n:\R_+^{n+1}\to\R_+$ as in \eqref{eq:Cn} for every $n\ge 2$.
% \begin{equation*}
% C_n(\|[DF(x)]^{-1}\|_{\cL^1(Y;X)}, \|DF(x)\|_{\cL^1(X;Y)},...,
% \|D^nF(x)\|_{L^n(X;Y)})
% \end{equation*}
% for some continuous $C_n:\mathbb{R}^{n+1}\rightarrow\mathbb{R}^+$.
\end{proof}





A technical lemma that upper bounds terms in Fa\'{a} di Bruno's formula:


\begin{lemma}\label{lemma:stirling}
For every $n\in\N$ holds $\sum_{\bsalpha\in T_n}
  \frac{n!}{\bsalpha!}\prod_{j=1}^n\frac{1}{(j!)^{\alpha_j}}\le n^n. $   
\end{lemma}
\begin{proof}
The sum
  $\sum_{\setl{\bsalpha\in T_n}{|\bsalpha|=k}}
  \frac{n!}{\bsalpha!}\prod_{j=1}^n\frac{1}{(j!)^{\alpha_j}}$ is equal
  to the so-called Stirling number of the second kind, $S_n^k$ (\cite{FaadiBruno}). Therefore, all we need to do is to upper bound $\sum_{k=1}^nS_n^k$. 

Note $S_n^k$ denote the number of ways to distribute $n$ distinct items into $k$ non-distinct boxes such that each box contains at least one item. Then, $k!S_n^k$ is the number of ways to distribute $n$ distinct items into $k$ distinct boxes such that none of the boxes are empty. The total number of ways to distribute $n$ distinct items into $n$ distinct boxes is given by $n^n$ (without the restriction that the boxes are nonempty). Therefore, we have 
$$\sum_{k=1}^n{n\choose k}k! S_n^k = n^n,$$
and it follows that $\sum_{k=1}^nS_n^k \leq n^n$. 
\end{proof}




We also collect several auxiliary results about norm of matrices and their inverses. 
\begin{lemma}\label{lemma:blockTriangle}
Suppose $M$ is a block triangular matrix: \[
   M =
   % \left[ {\begin{array}{cc}
   \begin{pmatrix}
   A & B \\
   0 & D 
   % \end{array} } \right],
   \end{pmatrix}
\]
where $A, D$ are invertible. Then,
$$\|M^{-1}\|_2 % = \frac{1}{\sigma_{\min}(M)}
\leq \|D^{-1}\|_2 + \|A^{-1}BD^{-1}\|_2 + \|A^{-1}\|_2.$$
% where $\sigma_{\min}(M)$ denotes the minimum singular value of $M$.
% and $\|.\|_2$ denotes the operator-2 norm of a matrix. 
\end{lemma}
\begin{proof}
  Since $A, D$ are invertible, the inverse of $M$ is
\begin{equation*}
  M^{-1} = % \left[ {\begin{array}{cc}
                       \begin{pmatrix}
                        A^{-1} & -A^{-1}BD^{-1} \\
                        0 & D^{-1}
                        \end{pmatrix}.
  % \end{array} } \right],
\end{equation*}
The claim follows from the triangle inequality.
\end{proof}

\begin{lemma}\label{lemma:MatrixInverseNorm}
  % Assume that $A$ is a non-singular matrix of dimension $d$, then
  % $\|A^{-1}\|_2\leq \frac{\|A\|_2^{d-1}}{|\det(A)|}$
  Let $A\in\R^{d\times d}$ be regular. Then
  $\|A^{-1}\|_2\le \frac{\|A\|_2^{d-1}}{|\det(A)|}$.
\end{lemma}
\begin{proof}
  Denote the singular values of $A$ by
  $\sigma_1\ge \sigma_2\ge\cdots\ge\sigma_d>0$. Then
  % First, note that
  % $|\det(A)| = \prod_{i=1}^d\sigma_i$, where $\sigma_i$ is the
  % $i-$th
  % smallest singular value. Then, we have
  \begin{equation*}
    \frac{\|A\|_2^{d-1}}{|\det(A)|} =
    \frac{\sigma_1^{d-1}}{
      \prod_{i=1}^d\sigma_i} =
    \left(\prod_{i=1}^{d-1}\frac{\sigma_1}{\sigma_i}\right)\frac{1}{\sigma_1}
    \ge \frac{1}{\sigma_1} = \|A^{-1}\|_2. %\qedhere
  \end{equation*}
  % $\frac{\|A\|_2^{d-1}}{|\det(A)|} = |\frac{\sigma_d^{d-1}}{
  % \prod_{i=1}^d\sigma_i}| =
  % |\left(\prod_{i=2}^d\frac{\sigma_d}{\sigma_i}\right)\frac{1}{\sigma_1}|
  % \geq |\frac{1}{\sigma_1}| = \|A^{-1}\|_2.$
\end{proof}




\begin{lemma}\label{lemma:NormTtinverse}
  Let $T\in C^1(\Omega_0,\Omega_1)$ be a monotonic triangular map. Moreover, assume $\nabla T(x)$ has positive eigenvalues
  $\lambda_j(x)$ for $j = 1,\cdots, d$ and all $x\in\Omega_0$.
  % Let $T$ satisfy the assumptions in \eqref{eq:spectrum}. Assume
  % further that
  % $\sup_{x\in\Omega_0}\|\nabla
  % T(x)\|_{L^1(\mathbb{R}^d,\mathbb{R}^d)} \leq C$ and
  % $\inf_{x\in\Omega_0}|\det(\nabla T(x))|\geq c$ for some constants
  % $C,c > 1$.  \rrtd{If we assume $c > 1$, we can simplify the bound
  % to $C^{d-1}/c$}
  Then for all $t\in [0,1]$ and all $x\in\Omega_0$
  \begin{equation*}
    \|(\nabla_x T_t(x))^{-1}\|_{2} = \|((1-t)I + t\nabla_x
    T(x))^{-1}\|_{\jz{2}} \le
    \frac{
      \max\{1,\|T\|_{C^1(\Omega_0)}\}^{d-1}
    }{\min\{1, \inf_{x\in\Omega_0}\det dT(x)}\}.
  \end{equation*}
\end{lemma}
\begin{proof}
  By Lemma \ref{lemma:MatrixInverseNorm}, for every
  $(x,t)\in\Omega_0\times [0,1]$
  \begin{equation*}
    \|(\nabla_x T_t(x))^{-1}\|_2\leq \frac{\|\nabla_xT_t(x)\|_{2}^{d-1}}{|\det(T_t(x))|}.
  \end{equation*}
  We have
  $\|\nabla T_t(x)\|_{2} \leq (1-t) + t\|\nabla
  T(x)\|_{2} \le \max\{1,\|T\|_{C^1(\Omega_0)}\}$. For
  $s\in [0,1]$, $t\in (0,1)$ set $g_t(s) := \log(1-t+\e^{s})$. This
  function is convex in $s\in[0,1]$. Thus
  \begin{equation*}
    g_t\left(\frac{1}{d} \sum_{i=1}^d\log (t\lambda_i(x))\right)\le
    \frac{1}{d} \sum_{i=1}^d g_t(\log (t\lambda_i(x))),
  \end{equation*}
  and therefore
  \begin{align*}
    \sum_{i=1}^d \log(1-t+t\lambda_i(x)) &\ge d \log\Bigg(1-t +
                                           t\Bigg(\prod_{i=1}^d \lambda_i(x)\Bigg)^{1/d}\Bigg)\nonumber\\
                                         &= d\log(1-t+t\det(\nabla
                                           T(x))^{1/d}).
  \end{align*}
  Taking the exponential on both sides, we conclude that
  \begin{equation*}
    \det((1-t)I + t\nabla T(x)) \geq (1-t+t\det(\nabla T(x))^{1/d})^d \geq \min\{1, \det dT\jz{(x)}\}.
  \end{equation*}
  Finally, Lemma \ref{lemma:MatrixInverseNorm} gives the result.
\end{proof}

Finally, a technical lemma about the upper bounds on the
$C^k$ norm of the reciprocal of a $C^k$ function bounded away from
zero.  
\begin{lemma}\label{lemma:NormReciprocal}
  Let $k \in \mathbb{N}, k \geq 1$ and $f \in C^k(D)$ for domain $D\subset\mathbb{R}^d$ such that
  $\inf_{x\in D} f(x) > C_2$ for some constant $C_2 > 0$. Assume
  further $\|f\|_{C^k(D)} \leq C_1$ for another constant $C_1$ > 0. Then, it
  holds that
$$\|\frac{1}{f}\|_{C^k(D)} \leq C\frac{C_1^k}{C_2^{k+1}}$$
for some constant $C$ that depends on $k$ but independent of $f$,
$C_1$, $C_2$.
\end{lemma}
\begin{proof}
  We proceed as in \cite[Lemma C.4 (iii)]{ZM1}.  Let
  $n\in\mathbb{N}$ be an integer such that $0 \leq n \leq k$. We shall
  show by induction that $D^n(\frac{1}{f}) = \frac{p_n}{f^{n+1}}$ for
  some function $p_n$ such that
  $\|p_n\|_{C^{k-n}(D)} \leq C(\|f\|_{C^k(D)})^n$.

  When $n = 1$, it holds that $D(\frac{1}{f}) = \frac{-Df}{f^2}$ and
  thus $\|D(\frac{1}{f})\|_{C^{k-1}(D)} \leq \frac{C_1}{C_2^2}$.

  Assume the induction hypothesis holds, for $n+1$, we have
  $$D^{n+1}(\frac{1}{f}) = D(\frac{p_n}{f^{n+1}}) = \frac{f^{n+1}Dp_n
    + p_n(n+1)f^nDf}{f^{2n+2}} = \frac{fDp_n + (n+1)p_nDf}{f^{n+2}} :=
  \frac{p_{n+1}}{f^{n+2}}.$$

  Then it holds that
  \begin{align*}
    \|p_{n+1}\|_{C^{k-n-1}(D)} &= \|fDp_n + (n+1)p_nDf\|_{C^{k-n-1}(D)}\\
                                    &\leq C(n+2)\|f\|_{C^k(D)}\|p_n\|_{C^{k-n}}\\
                                    &\leq C(n+2)(\|f\|_{C^k(D)})^{n+1}.
  \end{align*}

   
  Therefore, we have
  $\|\frac{1}{f}\|_{C^k(D)} \leq C\frac{C_1^k}{C_2^{k+1}}$ for
  some constant $C$ that depends on $k$ but independent of $f$, $C_1$,
  $C_2$.
\end{proof}


% When using measure-transport based sampling methods in practice, it is common to not learn a map that directly pushes forward source to the target, but instead to learn a composition of maps that pushes forward source to the target through a sequence of intermediate measures. As we will show, if two measures are not too far from each other (in a proper sense measured by the transport map), then the size of the neural network in the neural ODE whose time-one flow map pushes one measure onto the other scales linearly with respect to the dimension $d$, which partly answers the question why this approach can potentially offer benefits in practice.

% \begin{theorem}\label{thm:velocityfieldNormBound}
% In the context of theorem \ref{thm:ExSol}, suppose that we have the following additional assumptions about the transport map $T$:
% \begin{itemize}
%     \item There exists a constant $C \geq 1$ such that $\|D^n_xT\|_{L^\infty(\Omega_0,L^n(\mathbb{R}^d;\mathbb{R}^d))} \leq n!C^{n}$ for all $n \leq k$. \rrtd{do we really need $\Omega_0$ and $\Omega_1$ to live in the same ambient space? Also, probably need some discussion on why this assumption is reasonable (related to the analyticity of functions?)}
%     \item $\forall x\in\Omega_0\cup\Omega_1$, $|x| \leq M$.\rrtd{This could be derived from the next assumption?}
%     \item $\forall x$, the Jacobian matrix satisfies $\|I - \nabla_x T(x)\|_{L^\infty(\Omega_0, \jz{2})} \leq \delta < 1$. \rrtd{need to add a discussion about this assumption?}
    
% \end{itemize}
% Then, the $C^k$ norm of the velocity field $f$ can be bounded as $\|f\|_{C^k(\Omega)}\leq e^{\Theta(dC^{k^2}(1-\delta)^{1-2k}(2+2M-\delta)^{2k-1})}$ when treating $k$ as a constant. 
% \end{theorem}
% \begin{proof}[Proof of Theorem \ref{thm:velocityfieldNormBound}]
% Recall from Thm.~\ref{thm:f} that we can write $f(y,t) = T(F(y,t)) - F(y,t)$. Clearly, we have $\|f\|_{C^k(\Omega, \mathbb{R}^d)} \leq \|T\circ F\|_{C^k(\Omega, \mathbb{R}^d)} + \|F\|_{C^k(\Omega, \mathbb{R}^d)}$ and it remains to upper bound $\|T\circ F\|_{C^k(\Omega, \mathbb{R}^d)}$ and $\|F\|_{C^k(\Omega, \mathbb{R}^d)}$.

% Since $\|F\|_{C^k(\Omega, \mathbb{R}^d)} \leq \|G^{-1}\|_{C^k(\Omega, \mathbb{R}^{d+1})}$, where $G^{-1}$ is the inverse of $G(x,t) = (T_t(x), t)$ for $(x,t)\in\Omega_0\times[0,1]$. \rrtd{needs to be checked} By theorem \ref{thm:NormInverse}, this is bounded from above by a function of $$\|[D_{x,t}G]^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )}, \|D_{x,t}G\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )},\cdots,\|D^k_{x,t}G\|_{L^\infty(\Omega,L^k(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )}.$$ 

% First, note that  The Jacobian matrix,  $D_{x,t}G(x,t)$ is given by 
% \[
%    D_{x,t}G(x,t)=
%   \left[ {\begin{array}{cc}
%    t\nabla T(x) + (1-t)I & T(x) - x \\
%    0 & 1 \\
%   \end{array} } \right].
% \]

% Since $\nabla_x T_t(x)$ is invertible and $|T(x) - x| \leq 2M$ $\forall x\in\Omega_0$, by lemma \ref{lemma:blockTriangle}, we have
% \begin{align*}
% \|[D_{x,t}G]^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )} &\leq 1 + 2M\|(t\nabla_x T + (1-t)I)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))} + \\
% &\|(t\nabla_x T + (1-t)I)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))}\\
% &\leq 1 + (2M+1)\|\nabla_x T_t^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))}.
% \end{align*}


% Note for fixed $t\in[0,1]$, $\|\nabla_x T_t^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))} \leq  \sup_{(x,t)\in\Omega}\|(I - t(I - \nabla T))^{-1}\|_2 \leq \sum_{i=0}^\infty t^i\|I - \nabla T\|_2^i \leq \sum_{i=0}^\infty \|I - \nabla T\|_2^i = \frac{1}{1-\delta}.$

% For higher order derivatives, since $t\in[0,1]$, we always have \rrtd{needs to be checked}  
% $$\|D_{x,t}^nG\|_{L^\infty(\Omega, L^n(\mathbb{R}^{d+1}, \mathbb{R}^{d+1}))} \lesssim \|D_x^nT\|_{L^\infty(\Omega, L^n(\mathbb{R}^{d}, \mathbb{R}^d))} + \|D_x^{n-1}T\|_{L^\infty(\Omega, L^{n-1}(\mathbb{R}^{d}, \mathbb{R}^d))}  \lesssim n!C^n$$ for $n\leq k$. Therefore, application of Thm \ref{thm:NormInverse} gives the upper bound 
% $$\|F\|_{C^k(\Omega)} \leq  (Ck^{1+\epsilon})^{k^2}k!\|[DG]^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^{d+1}, \mathbb{R}^{d+1}))}^{2k-1}\left(\frac{k^\epsilon}{k^\epsilon - 1}\right)^{2k-1}\leq (Ck^{1+\epsilon})^{k^2}k!\left(\frac{(2 + 2M - \delta)k^\epsilon}{(1-\delta)(k^\epsilon - 1)}\right)^{2k-1} $$

% Since the result holds for arbitrary $\epsilon > 0$, we choose $\epsilon = 1$ and use the fact that $\lim_{k\rightarrow\infty} (\frac{k}{k-1})^{2k-1} = e^2$ to get:
% $$\|F\|_{C^k(\Omega)} \leq \frac{e^2k!(Ck^{2})^{k^2}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}} $$




% Finally, Thm \ref{cor:composition} gives the upper bound for norm $\|T\circ F\|_{C^k(\Omega)}$:

% $$\|T\circ F\|_{C^k(\Omega)} \leq  d^kk!C^k\exp\left(kd\|F\|_{C^k(\Omega)}\right) \leq  d^kk!C^k\exp\left(\frac{de^2k!C^{k^2}k^{2k^2+1}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}} \right)
% $$

% Using $\|f\|_{C^k(\Omega)} \leq \|T\circ F\|_{C^k(\Omega)} + \|F\|_{C^k(\Omega)}$, we have


% $$\|f\|_{C^k(\Omega)} \leq d^kk!C^k\exp\left(\frac{de^2k!C^{k^2}k^{2k^2+1}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}} \right)
% +\frac{e^2k!(Ck^{2})^{k^2}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}}. $$

% Treating $k$ as a constant, we see that the order how $\|f\|_{C^k(\Omega)}$ scales with $d$, $C$, $M$ and $\delta$ is characterized by $e^{\Theta(dC^{k^2}(1-\delta)^{1-2k}(2+2M-\delta)^{2k-1})}$. 

% \end{proof}



% end jz

\begin{comment}
\section{Proofs of Section \ref{sec:DistApproximation}}\label{app:proofsec6}

\begin{proof}[Proof of Theorem \ref{thm:l2bound}]
The evolution of probability measures of particles driven by a velocity field satisfies the \textit{continuity equation} (\cite{GradientFlowProbSpace}). That is, we have  $\partial_t\pi_{f}(x, t) = \nabla\cdot(\pi_{f}(x, t)f(x,t))$ and $\partial_t\pi_g(x,t) = \nabla\cdot(\pi_g(x,t)g(x,t))$ with the same initial condition $\pi_f(x,0) = \pi_g(x,0) = \pi(x)$. Taking the difference of the two equations and denoting $d(x,t) = \pi_f(x,t) -\pi_g(x,t)$, we have $$\nabla\cdot(\pi_f(x,t)f - \pi_g(x,t)g) = \partial_td(x,t),$$ 
which can be equivalently expressed as $\nabla\cdot(\pi_f(f-g) + dg) = \partial_td$.

Multiplying both sides by $d$, we get $(\nabla\cdot\pi_f(f-g))d + (\nabla\cdot(dg))d = (\partial_td)d$. Note 
$$(\nabla\cdot(dg))d = (\nabla d\cdot g)d + (\nabla\cdot g)d^2 = (\nabla\cdot g)d^2 + \frac{1}{2}\left< g, \nabla d^2\right>,$$ 
where the last step is by the chain rule of derivatives. Similarly, we have $(\partial_td)d = \frac{1}{2}\partial_td^2$. Thus we have $(\nabla\cdot (\pi_f(f-g)))d + (\nabla\cdot  g)d^2 + \frac{1}{2}\left< g, \nabla d^2\right> = \frac{1}{2}\partial_td^2$. Integrating by parts over $\Omega_t$, we get $$\int_{\Omega_t} (\nabla\cdot  g)d^2 + \frac{1}{2}\int_{\Omega_t} g\cdot\nabla d^2 = \int_{\Omega_t} (\nabla\cdot  g)d^2 + \frac{1}{2}g\cdot \overrightarrow{n}d^2|_{\partial\Omega_t} - \frac{1}{2}\int_{\Omega_t} (\nabla\cdot  g)d^2 = \frac{1}{2}\int_{\Omega_t} (\nabla\cdot  g)d^2,$$ because the boundary term vanishes.

Therefore, we get $\int_{\Omega_t} (\nabla\cdot  g)d^2 + 2\int_{\Omega_t} (\nabla\cdot (\pi_f(f-g)))d = \partial_t\|d(\cdot, t)\|_{L_2(\Omega_t)}^2$.

Since this holds for every $t \in [0,1]$, we introduce a time dependent local error term $\epsilon_t(x) = |\partial_t\pi_f(x,t) - \nabla\cdot (\pi_f(x,t)g(x,t))| = |\nabla\cdot (\pi_f(f-g))|$. Applying Holder's inequality and Young's inequality to the first and second term respectively, we have 
\begin{align*}
\partial_t\|d(\cdot, t)\|_{L^2(\Omega_t)}^2 &\leq |\int_{\Omega_t} (\nabla\cdot  g)d^2| + 2|\int_{\Omega_t} (\nabla\cdot (\pi_f(f-g)))d|\\
&\leq \|\nabla\cdot  g(\cdot, t)\|_{L_\infty(\Omega_t)}\|d(\cdot, t)\|_{L^2(\Omega_t)}^2 + \|d(\cdot, t)\|_{L^2(\Omega_t)}^2/C + C\|\epsilon_t\|_{L^2(\Omega_t)}^2,     
\end{align*}
for any constant $C > 0$.

Then Gronwall's inequality gives $$\|d(\cdot, t)\|_{L^2(\Omega_t)}^2 \leq C(\int_0^t\|\epsilon_s\|_{L^2(\Omega_s)}^2ds)\text{exp}(t/C+ \int_0^t\|\nabla\cdot  g(\cdot, s)\|_{L^\infty(\Omega_s)}ds),$$ for $t\in[0,1]$. 

Since we have $\|f-g\|_{W^{1,\infty}} \leq \delta$, 
$$|\epsilon_s(x)| = |\nabla\cdot (\pi_f(x,s)(f-g))| =  |(\nabla \pi_f(x,s))\cdot(f-g) + \pi_f(x,s)\nabla\cdot (f-g)| \leq d\delta(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})$$ for every $s$. Therefore, we have $\int_0^t\|\epsilon_s\|_{L^2}^2ds \leq d^2\delta^2(1+\|\nabla\pi_f(x,s)\|_{L^\infty(\Omega_{[0,1]})})^2$ and we have the bound $$\|d(\cdot, t)\|_{L_2(\Omega_t)}^2 \leq Cd^2\delta^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2\text{exp}(t/C+ \|\nabla\cdot  g\|_{L^\infty(\Omega_{[0,1]})}).$$



Now, specializing to the time-one flow map, since we know $T_\sharp\pi = \rho$ and $\pi_f(\cdot,1) = \rho$, we have 
$$d_{L^2(\Omega_1)}^2(\pi_g(\cdot,1), \rho) \leq  Cd^2\delta^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2\text{exp}(t/C+ \|\nabla\cdot  g\|_{L^\infty(\Omega_{[0,1]})}).$$

Choosing $C = 1$ gives the result in the statement of the theorem. 





\end{proof}{}



\begin{proof}[Proof of Corollary \ref{corollary:KLandChisquare}]
For $\chi^2$ divergence, we have
\begin{align*}
d_{\chi^2}(X_g(\cdot,1)_\sharp\pi||X_f(\cdot,1)_\sharp\pi) &= d_{\chi^2}(X_g(\cdot,1)_\sharp\pi||\rho) = \int_{\Omega_1}\frac{(\pi_g(x,1) - \rho(x))^2}{\rho(x)}dx\\
&\leq \frac{1}{L_2}\int_{\Omega_1}(\pi_g(x,1) - \rho(x))^2dx \leq \frac{1}{L_2}d_{L^2}^2(X_g(\cdot,1)_\sharp\pi, X_f(\cdot,1)_\sharp\pi).    
\end{align*}

To get an upper bound for $\dkl(X_g(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi)$, we make use of the fact that KL-divergence is bounded from above by $\chi^2$ divergence. That is, we have 
\begin{align*}
&d\dkl(X_g(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi) = \mathbb{E}_{\pi_g(\cdot,1)}[\log \frac{\pi_g(\cdot,1)}{\rho(x)}] \leq \log \mathbb{E}_{\pi_g(x,1)}[\frac{\pi_g(x,1)}{\rho(x)}] = \log\int_{\Omega_1} \frac{\pi_g(x,1)^2}{\rho(x)}dx \\
&= \log\left(\int_{\Omega_1} \frac{(\rho(x)-\pi_g(x,1))^2}{\rho(x)}dx + 1\right)  \leq \log\left(\frac{1}{L_2}d_{L^2}^2(\pi_g(\cdot,1), \rho) +1\right).     
\end{align*}




\end{proof}


\begin{proof}[Proof of Theorem \ref{thm:FlowMapBound}]
First, note that for fixed $x\in\Omega_0$,
\begin{align*}
|\frac{dX(x,t)}{dt} - \frac{dY(x,t)}{dt}| &= |f(X(x,t),t) - g(Y(x,t),t)| \\
&\leq |f(X(x,t),t) - f(Y(x,t), t)| + |f(Y(x,t), t) - g(Y(x,t),t)|.    
\end{align*}


By $\|f - g\|_{L^\infty(\Omega_{[0,1]})}\leq \delta$, we have $|f(Y(x,t), t) - g(Y(x,t),t)| \leq \delta, \forall (x, t) \in \Omega_{[0,1]}$. On the other hand, since $\|f\|_{C^1(\Omega_{[0,1]})}\leq C$, we have, for each $t\in[0,1]$, $\|\nabla_x f(\cdot,t)\|_{L^\infty(\mathbb{R}^d, \mathbb{R}^{d}\times\mathbb{R}^d)} \leq C$. Since $f\in C^1$ when viewed as a function of its space variable, its spatial Lipschitz constant is bounded by $C$. Therefore, we can conclude that $|f(X(x,t),t) - f(Y(x,t), t)| \leq C|X(x,t) - Y(x,t)|.$ Then, we have 
\begin{align*}
&|X(x,t) - Y(x,t)| = |\int_0^tf(X(x,s),s) - g(Y(x,s),s)ds|\\
&\leq \int_0^t|f(X(x,s),s) - g(Y(x,s),s)|ds\\
&\leq \int_0^tC|X(x,s) - Y(x,s)|ds + t\delta   
\end{align*}




%On the other hand, note that for Euclidean norm $|\cdot|$ and $v(t)\in\mathbb{R}^d$ a vector depending on $t$, we have $2|v(t)|\frac{d}{dt}|v(t)|=\frac{d}{dt}|v(t)|^2 = \frac{d}{dt}(v\cdot v) = 2v\cdot\frac{dv(t)}{dt} \leq 2|v(t)||\frac{dv(t)}{dt}|$, which gives $\frac{d}{dt}|v(t)| \leq |\frac{dv(t)}{dt}|$. \sw{[This can hold at most almost everywhere, no?]}

%Putting things together, we have $$\frac{d}{dt}|X(x,t) - Y(x,t)|\leq |\frac{d}{dt}(X(x,t) - Y(x,t))|\leq dC|X(x,t) - Y(x,t)| + \epsilon.$$
It holds that $|X(x,s) - Y(x,s)|$ is a continuous function in $s$. Using Gronwall's inequality, we get  $|T(x) - G(x)| = |X(x,1) - Y(x,1)|\leq \delta e^{C}, \forall x$. Therefore $\max_{j}\|X_j(\cdot,1)-Y_j(\cdot,1)\|_{L^\infty(\Omega_0)}\leq \delta e^{C}$.


\end{proof}

\section{Proofs of Section \ref{sec:NN}}
\begin{proof}[Proof of Theorem \ref{thm:ObjectiveErrorBound}]
  By Thm.~\ref{thm:ExSol}, velocity field $f$
  satisfies $J(f) = 0$. By Corollary
  \ref{corollary:NetParameterBound}, there exists a realization
  $\Psi_\epsilon^f$ of the architecture $\mathcal{A}_\epsilon$ such
  that $\|\Psi_\epsilon^f- f\|_{W^{1,\infty}} \leq \epsilon$, in particular, this realization will have finite $W^{1,\infty}$ norm. 

  Recall the optimization objective $J$ has two parts, KL-divergence
  and the regularization:
  $$ J = d_\text{KL}[X(x,1)_\sharp\pi||\rho] +
  \lambda\mathbb{E}_{x\sim\rho(x)}[R(x,1)].$$ For the KL part,
  corollary \ref{corollary:KLandChisquare} combined with Thm.~\ref{thm:l2bound} gives
  \begin{align*}
    &\dkl(X_{\Psi_\epsilon^f}(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi) = \dkl(X_{\Psi_\epsilon^f}(\cdot,1)_\sharp\pi|| \rho)\leq \log(\frac{1}{L_2}d_{L^2}^2(\pi_{\Psi_\epsilon^f}(\cdot,1), \rho)+1))\\
    &\leq \log\left(\frac{1}{L_2}d^2\epsilon^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2e^{1 + \|\nabla\cdot  \Psi_\epsilon^f\|_{L_\infty(\Omega_{[0,1]})}} + 1\right).
  \end{align*}

  Since $\|\Psi^f_\epsilon\|_{W^{1,\infty}(\Omega_{[0,1]})} \leq C$, we have $ \|\nabla\cdot  \Psi_\epsilon^f\|_{L_\infty(\Omega_{[0,1]})}\leq dC$ and hence, we obtain:
  $$\dkl(X_{\Psi_\epsilon^f}(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi)\leq \log\left(1+\frac{d^2\epsilon^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2}{L_2}e^{1 + dC} \right)  $$





  For the regularization part, recall that
  $$R(x,1) = \int_0^1 |\nabla_X(f(X(x,t),t))f(X(x,t),t) +
  \partial_tf(X(x,t),t)|^2dt.$$
  Now, consider the pointwise value at
  $(x,t)\in\Omega_0\times[0,1]$ and define
  $$r_f(x,t) = \nabla_X(f(X_f(x,t),t))f(X_f(x,t),t) +
  \partial_tf(X_f(x,t),t)$$ and
  $$r_{ \Psi_\epsilon^f}(x,t) = \nabla_X \Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t) + \partial_t\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t).$$

  We shall then compute the pointwise difference
  $|r_f(x,t) - r_{ \Psi_\epsilon^f}(x,t)|$.

  First, note
  $$|f(X_f(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq
  |f(X_f(x,t),t) - f(X_{ \Psi_\epsilon^f}(x,t),t)| + |f(X_{
    \Psi_\epsilon^f}(x,t),t) - \Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)|.$$ By $W^{1,\infty}$ approximation,
  $|f(X_{ \Psi_\epsilon^f}(x,t),t) - \Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)| \leq \epsilon$. By
  Thm.~\ref{thm:FlowMapBound}, the flow maps
  $X_{ \Psi_\epsilon^f}(x,t)$ and $X_f(x,t)$ satisfies
  $|X_{ \Psi_\epsilon^f}(x,t) - X_f(x,t)| \leq \epsilon e^{C}$. Since $\|f\|_{C^2(\Omega_{[0,1]}}\leq C$, the spacial Lipschitz constant of $f$ is upper bounded by $C$ and we then have
  $|f(X_f(x,t),t) - f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq C\epsilon
  e^{C}$ and
  $|f(X_f(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq
  C\epsilon e^{C} + \epsilon$.

  Next, consider
  $|\partial_tf(X_f(x,t),t) - \partial_t\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)|$. Similarly, we
  have
  \begin{align*}
     |\partial_tf(X_f(x,t),t) - \partial_t\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)| &\leq |\partial_tf(X_f(x,t),t) -
  \partial_tf(X_{ \Psi_\epsilon^f}(x,t),t)| \\
  &+ |\partial_tf(X_{
    \Psi_\epsilon^f}(x,t),t) - \partial_t\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)   
  \end{align*}

    
    
Again by $W^{1,\infty}$ approximation,
  $|\partial_tf(X_{ \Psi_\epsilon^f}(x,t),t) -
  \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq
  \epsilon$ and the spatial Lipschitz constant of $\partial_t f$ is upper bounded $C$.Thus we have 
  $|\partial_tf(X_f(x,t),t) - \partial_t\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)| \leq C\epsilon e^{C} + \epsilon$.

  Finally, consider
  $|\nabla_X(f(X_f(x,t),t)) - \nabla_X \Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)|.$
  \begin{align*}
    &|\nabla_Xf(X_f(x,t),t) - \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq |\nabla_Xf(X_f(x,t),t) - \nabla_X f(X_{ \Psi_\epsilon^f}(x,t),t)| \\
    &+ |\nabla_X f(X_{ \Psi_\epsilon^f}(x,t),t) - \nabla_X\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq C\epsilon e^{C} + \epsilon,   
  \end{align*}


  Now, we can bound
  $|\nabla_X(f(X_f(x,t),t))f(X_f(x,t),t) - \nabla_X
  \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)\Psi_\epsilon^f(X_{
    \Psi_\epsilon^f}(x,t),t)|$ as follows:
  \begin{align*}
    &|\nabla_X(f(X_f(x,t),t))f(X_f(x,t),t) - \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|\\
    &\leq |\nabla_X(f(X_f(x,t),t))-\nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|f(X_f(x,t),t)| \\
    &+ |\nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)||f(X_f(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|\\
    &\leq 2C(\epsilon Ce^C + \epsilon)
  \end{align*}

  Putting things together, we have
  $|r_f(x,t) - r_{ \Psi_\epsilon^f}(x,t)| \leq (2C+1)(\epsilon Ce^C + \epsilon) $.

  Since $J(f) = 0$, therefore we can conclude
  \begin{align*}
    &|\int_0^1|r_f(x,t)|^2dt - \int_0^1|r_{ \Psi_\epsilon^f}(x,t)|^2dt| \\
    &\leq \int_0^1 |r_f(x,t) - r_{ \Psi_\epsilon^f}(x,t)|^2dt \leq (2C+1)^2(\epsilon Ce^C + \epsilon)^2
  \end{align*}

  Then we can conclude the training objectives satisfy
$$|J(f) - J(\Psi^f_\epsilon)| \leq \log\left(1+\frac{d^2\epsilon^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2}{L_2}e^{1 + dC} \right)  + (2C+1)^2(\epsilon Ce^C + \epsilon)^2.$$





\end{proof}


\end{comment}




\begin{comment}
    In Sec.\ref{sec:minimizers}, we showed the existence of a velocity field whose time-one flow map pushes forward $\pi$ to $\rho$ and in Sec.\ref{sec:regularity}, regularity results of the velocity field $f$ are given, in terms of the regularity of the underlying transport map the flow realizes. In particular, under our assumptions \ref{ass:dens1} and \ref{ass:dens2} on the densities, the Knoth-Rosenblatt triangular transport map as well as the corresponding velocity field, will both be in $C^k$. When we train a neural ODE in practice by solving the optimization problem \ref{OP}, we are searching an approximation of the velocity field from a less regular approximation space $\mathcal{V}$. In this section, we explore how the distance between $X_f(\cdot,1)_\sharp\pi$ and $X_g(\cdot,1)_\sharp\pi$ can be bounded, where $g\in\mathcal{V}$ is an approximation of $f$. As we will see, to bound the distance between $X_f(\cdot,1)_\sharp\pi$ and $X_g(\cdot,1)_\sharp\pi$ in $L^2$, $\chi^2$ and KL-divergence, it is required that $g$ and $f$ are close in $W^{1,\infty}$. On the other hand, to bound this distance in Wasserstein metric, it is only required that $f$ and $g$ are close in $L^\infty$.  

\begin{theorem}\label{thm:l2bound}
Let $\pi, \rho$ be densities that satisfy assumptions \ref{ass:dens1}, \ref{ass:dens2}. Let $T: \Omega_0\rightarrow\Omega_1$ be a transport map such that $T_\sharp\pi = \rho$ and satisfies \ref{ass:T} and $f:\Omega_{[0,1]}\rightarrow\mathbb{R}^d$ the  velocity field from Thm.~\ref{thm:fNormGeneral}. Let $g\in\mathcal{V}$ be such that $\|f - g\|_{W^{1, \infty}(\Omega_{[0,1]})} \leq \delta$. Assume further that $g\cdot \overrightarrow{n}|_{\partial\Omega_t} = 0$, where $\Omega_t = \setc{X_{f}(x,t)}{x\in\Omega_0, t\in[0,1]}$, then 
$$d_{L^2(\Omega_1)}^2(X_g(\cdot,1)_\sharp\pi, X_f(\cdot,1)_\sharp\pi) = d_{L^2(\Omega_1)}^2(\pi_{g,1}, \rho)  \leq d^2\delta^2(1+\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})})^2\text{exp}(1 + \|\nabla\cdot  g\|_{L_\infty(\Omega_{[0,1]})}).$$ 
\end{theorem}

The evolution of probability measures of particles with some initial distribution and driven by a velocity field satisfies the \textit{continuity equation} (\cite{GradientFlowProbSpace}). The proof of Thm.~\ref{thm:l2bound} can be found in Appendix \ref{app:proofsec6}, and the core idea in the proof strategy is to compare the two continuity equations defined by $f$ and $g$ that start from the same initial distribution and then derive a differential equation satisfied by $\pi_f - \pi_g$. By the theory of characteristics, we have $\pi_f(x,t) = \pi(X^{-1}_f(x,t))e^{\int_0^t\nabla\cdot f(X_f(X^{-1}_f(x,t), s), s)ds}$, where $X^{-1}_f(x,t)$ denotes the inverse of the map $x\rightarrow X_f(x,t)$ when viewing $t$ as fixed, and by the regularity of $f$ and $\pi$,  $\|\nabla\pi_f\|_{L^\infty(\Omega_{[0,1]})}$ is bounded. Therefore, we can conclude that as $\delta \rightarrow 0$, $d^2_{L^2(\Omega_1)}\rightarrow 0$. 


Note that in Thm.~\ref{thm:l2bound}, we require the approximator $g$ to have the same domain as $f$, $\Omega_{[0,1]}$ and to have vainishing normal component along the spatial domain $\partial \Omega_t \forall t\in[0,1]$. In the context of neural network parametrization, this could be achieved by multiplying by a cutoff function. For a more detailed discussion, see ahead Section \ref{sec:NN}. \rrtd{is there any literature we can cite on this??}





\begin{corollary}\label{corollary:KLandChisquare}
In the context of Thm. \ref{thm:l2bound}, we also have the following bounds in $\chi^2$ and KL-divergences:

$$d_{\chi^2}(X_g(\cdot,1)_\sharp\pi||X_f(\cdot,1)_\sharp\pi) \leq \frac{1}{L_2}d_{L^2}^2(\pi_g(\cdot,1), \rho)$$ and $$\dkl(X_g(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi)\leq \log\left(\frac{1}{L_2}d_{L^2}^2(\pi_g(\cdot,1), \rho) +1\right)   $$
\end{corollary}

It is well-known that under mild regularity assumptions, KL-divergence and $\chi^2$-divergence can be upper bounded by $l^2$ distance between the densities. Therefore, corollary \ref{corollary:KLandChisquare} follows directly from Thm.~\ref{thm:l2bound}. For completeness, we also include the proofs in Appendix \ref{app:proofsec6}. 


Besides controlling the error in the push-forward densities, one may also be interested in bounding the error of the approximation of the transport map itself, given the error of approximation of the velocity field. As we will see, $L^\infty$ control of the velocity field gives $L^\infty$ control of the flow map, which in turn controls the Wasserstein distance between the push-forward distributions. 
\end{comment}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NeuralODE-main.tex"
%%% End:        