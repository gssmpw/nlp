  \section{Introduction}\label{sec:intro}
  Sampling from an arbitrary, possibly very complex distribution is
  one of the central problems in computational statistics. One
  approach to tackling the problem is through measure-transport
  (\cite{measure-transport}); that is, we construct a deterministic
  coupling, i.e., a transport map, between the complex target measure
  and a relatively simple source measure, which is usually chosen to
  be uniform or standard Gaussian. Then, sampling from the target
  measure can be done by sampling from the source measure and pulling
  back through the transport map. In the machine learning literature,
  a normalizing flow is \jzs{just} a generative model composed of
  relatively simple maps, that push forward the target distribution to
  an easy-to-sample distribution (\cite{NormalizingFlowIntro}).
  % For such models to work,
  \jz{For such methods it is typically} required that the maps be
  invertible and the determinant of the Jacobian is easily computable,
  as evaluating the \rr{push-forward density}\jztd{flow $\to$
    pushforward density?} follows from the change of variables
  formula. To satisfy the aforementioned requirements, various
  architectures of discrete normalizing flows have been proposed,
  including planar and radial flows (\cite{PlanarFlow}), coupling
  flows (\cite{CouplingFlows}), autoregressive flows
  (\cite{autoregressiveflow}) and neural autoregressive flows
  (\cite{NeuralAutoFlow}).

  In a recent advancement of deep learning, various connections
  between differential equations and neural networks have been made
  (\cite{TransportAnalysisofDL}, \cite{NumericalSchemeODE},
  \cite{PDEmotivatedNN}), which led to the idea of NeuralODEs
  (\cite{NeuralODE}). More recently, NeuralODEs \jz{have been
    proposed} to perform generative modelling tasks
  (\cite{ffjord}). The generative process is as follows: \jz{Denote by
    $\pi$ the target distribution from which we wish to sample, and by
    $\rho$ the source distribution.
    % . Typically, $\pi$ is
    % either known through an iid sample or through its (unnormalized)
    % density, see Sec.~\ref{sec:setup} ahead. For a reference
    % distribution $\rho$,
    Given $x\sim\rho$, the following initial value problem
    % to get the time-one solution $x(1)$ for the ODE:
    \begin{equation}\label{eq:ODE}
      \begin{aligned}
        % \frac{dX(x,t)}{dt}
        \frac{d X(x,t)}{dt} &= f(X(x,t), t),\\
        X(x, 0) &= x,
      \end{aligned}
    \end{equation}
    is solved up to time $t=1$. The goal is to learn the velocity
    field $f$, which is parametrized as a neural network, such that
    $x\sim \rho$ implies $X(x,1)\sim\pi$.} \jztd{Before it said
    $f(x,t,\theta_t)$. I guess $\theta_t$ was a typo? In any case,
    $\theta$ didn't occur anymore so I removed it.}  \jztd{Sven made a
    good comment in the group meeting: We should assume throughout
    that $f$ is Lipschitz continuous so that \eqref{eq:ODE} always has
    a solution.}

  % If the neural network $f$ is properly learned, then the points
  % $x(1)$ will be approximately sampled from the distribution
  % $\rho(x)$.
  \jz{In the context of distribution learning,}
  NeuralODEs % as continuous normalizing flows
  enjoy % many advantages.
  \jz{several desirable properties}: \jzs{In ODE structures,}
  \jz{Invertibility of $x\mapsto X(x,1)$ is guaranteed by design,
    % as one can always integrate it backwards in time,
    as one can always solve \eqref{eq:ODE} backward in time.} Thus,
  \jz{in contrast to other methods \rr{such as invertible neural
      networks (\cite{InvNN}), normalizing flows
      (\cite{NormalizingFlowIntro}) or transport maps
      (\cite{measure-transport})}, no further restrictions need to be
    imposed on the vector field $f$ that is to be learned.}
  % there is no limitations on the form the ODE dynamics (or the
  % neural networks) need to take. In addition, The change
  \jz{In addition, the density} $\eta(x,t)$ % under NeuralODE
  \jz{of $X(x,t)$ (with $x\sim\rho$) can easily be computed, as it
    follows the dynamics}
  $\frac{\partial \log \eta(x,t)}{dt} = -\tr(\frac{\partial
    f}{\partial X})$\jztd{The formula is unclear. Check which are
    partial derivatives and which are not. What is $p(x(t))$? Should
    it be $p(x,t)$ (or rather $\pi(x,t)$)?}, \rr{where $\tr(\cdot)$
    denotes the trace and $\frac{\partial f}{\partial X}$ is the
    Jacobian of $f$ with respect to $X(x,t)$. This is known as the
    \emph{instantaneous change of variables formula}, see
    \cite{NeuralODE}.}
  % another ODE, called the instantaneous change of variables formula
  % (\cite{NeuralODE}):
  % $\frac{\partial \log p(x(t))}{dt} = -\text{Tr}\left(\frac{\partial
  %   f}{\partial x(t)}\right)$.

  In regards to the usefulness of NeuralODEs as generative models,
  there are three questions that are natural to ask.
  First, % note that
  it is known that when the source and target measures are
  well-behaved (for example, both absolutely continuous with respect
  to the Lebesgue measure), there are \jz{in general} infinitely many
  transport maps that push forward one measure onto the other. \rrs{as
    long as dimension $d > 1$}.\jztd{I think it's also true in $1d$.}
  Even if we require the time-one flow map \jz{$x\mapsto X(x,1)$} to
  be a particular transport map $T$, there are \jz{in general still
    many possible velocity fields $f$ realizing $T$.}
  % still infinitely many ways how points from the source measure,
  % $x$, can be connected to points from the target measure, $T(x)$,
  % through the ODE trajectories.
  It is observed in \cite{OTFlow} and \cite{HowToTrain} that without
  any form of regularization, the trajectories of the ODE dynamics
  connecting $x$ to $T(x)$ \jz{may} be very
  irregular. % Therefore, % the first question that
  Therefore \jz{it is natural to ask how we can regularize the
    training objective to improve the training process.}
  % so that the velocity fields we learn in the end is nice in an
  % appropriate sense.
  Secondly, with the regularized training objective, we would like to
  understand the structures of its optimal solutions and quantify the
  neural network approximator for them. The third question to ask,
  then, is how can we bound the distance between the target measure
  and the push-forward measure of the source under the flow map of the
  NeuralODE when we substitute in the neural network as an
  approximation for the velocity field? Our work is the first attempt
  to address these questions in a unified way. In the \jz{remainder}
  of this section, we will review some relevant literature and
  summarize our main contributions.


\subsection{Prior and Related Work}
In this section, we provide an overview of the \jz{most} relevant
prior work. After normalizing flows have demonstrated their success in
many practical applications, a few papers have studied the
representation powers of such models. For discrete normalizing flows,
\cite{DisAppro2} studies some basic flow structures including planar
flows, radial flows, Sylvester Flows and Householder flows and shows
that these basic flows are highly expressive in dimension $d = 1$, but
their expressivity can be limited in higher
dimensions. \cite{DisAppro1} shows that invertible neural networks
based on coupling flows are universal approximators for
diffeomorphisms.

For NeuralODEs, it turns out that the analysis of their representation
power is more complicated. It is observed in \cite{Augmented} that due
to the fact that the flow maps of ODEs are homeomorphisms, there are
some simple functions that can not be exactly represented by
NeuralODEs, and it is argued there that dimension augmentation by
appending zeros to the input vectors might be necessary. Later in
\cite{ApproximationCapability}, it is proved that homeomorphism on a
$p$-dimensional Euclidean space can be exactly represented by an ODE
operating on a $2p$-dimensional Euclidean space, with a vector of $p$
zeros being appended to the input vectors. However, for the purpose of
\textit{distribution learning}, these results are not very useful
because \textit{(i)} the failure in exact representation does not
imply the impossibility of approximation and \textit{(ii)}, universal
approximation of the distribution does not necessarily need universal
function approximation. Indeed, some more recent works do show that
NeuralODEs are $L^p$-universal approximators for continuous maps under
certain technical assumptions (\cite{DynamicalSystem}) and they are
$L^\infty$-universal apprxoimators for a large class of
diffeomorphisms (\cite{SupApproximation}). Still, these works only
focus on the \textit{function approximation} aspect of NeuralODEs;
moreover, these types of universal-approximation results all take the
underlying neural network in the ODE as a black box and there is no
quantification of how complex the neural network has to be in order to
achieve certain degree of accuracy.

Another class of generative models related to NeuralODEs is called
Neural stochastic differential equations (NeuralSDEs), which is a SDE
whose drift term is parametrized by a neural network. Among the
infinitely many possible drift terms that could push forward the
source to the target distribution, it is shown in \cite{NeuralSDE}
that there is a drift named the F\"ollmer drift that achieves exact
sampling and it is nice in the sense of stochastic control
theory. Subsequent regularity results about the F\"ollmer drift and
neural network approximation theories have been developed for
distribution learning in this setting. Part of our work will be to
prove similar results in the context of NeuralODEs, which is currently
lacking.

Regarding the training of NeuralODEs as continuous normalizing flows,
some regularization techniques have already be discussed. It is argued
in \cite{HowToTrain} that a good way of measuring the regularity of
the vector field is by the force experienced by a particle $x(t)$
under the dynamics generated by the vector field $f$, which is the
total derivative of $f$ in time: \ymmtd{change $x$ into $X$ a few places below}
\begin{equation}\label{eq:straightlinereg}
  \frac{df(X,t)}{dt} = \nabla_X f(X,t) \cdot \frac{dX}{dt} + \frac{\partial f(X,t)}{ \partial t} = \nabla_X f(X,t) \cdot f(X,t) + \frac{\partial f(X,t)}{\partial t}.
\end{equation}
Observe that when this term is zero the trajectories of the particles
will be straight lines. Since the Jacobian matrix $\nabla_X f(X,t)$ is
in general not easily accessible, \cite{HowToTrain} chooses to
implicitly regularize this term by regularizing $|f(X,t)|^2$ and the
Frobenius norm $\|\nabla_x f(X,t)\|_F^2$ instead. Similar to
\cite{ffjord}, stochastic estimation is used in the training to
compute $\|\nabla_X f\|_F^2$ and $\text{Tr}(\nabla_X f(X,t))$, which
is used in the change of variables formula for log-likelihood. In a
more recent work, \cite{OTFlow} adopts a discretize-then-optimize
approach to training NeuralODEs, where a ResNet structure is used to
implement the underlying neural network in the ODE. This approach
enables exact computation of the Jacobian matrix as well as its trace
from the recursive structure of the ResNet. Then, automatic
differentiation is used to update the parameters in the neural
network, instead of solving the adjoint equation as in \cite{ffjord}
and \cite{HowToTrain}.


\subsection{Contribution\jz{s}}
Our contributions in this work have both algorithmic and theoretical
aspects.
\begin{itemize}
\item Algorithmically, we \rr{suggest to use} the
  discretize-then-optimize approach of \cite{OTFlow}, which allows the
  Jacobian to be computed exactly and thus allows to regularize the term \eqref{eq:straightlinereg}
  directly instead of regularizing $\|f\|^2$ and $\|\nabla_xf\|^2_F$
  as in \cite{HowToTrain}.
    
\item Under reasonable assumptions, we show that the velocity fields
  that achieve this optimal value of the objective must take the
  time-independent form (\rr{in the Lagrangian sense})
  \rr{$f(tT(x) + (1-t)x,t) = T(x) - x$} for some transport map $T$
  that pushes forward one measure to the other. This implies
  that $x$ is connected to $T(x)$ by straight line trajectories. We
  also characterize the type of transport maps that can be represented
  by ODEs with straight line trajectories and show that for these
  transport maps $T$, among the infinitely many velocity fields that
  connect $x$ to $T(x)$, this straight line construction gives the
  minimal kinetic energy. Thus we call expression
  \eqref{eq:straightlinereg} the minimal energy regularization.
    
\item Since the velocity field is constant along the trajectories, the
  optimal velocity fields $f$ are implicitly determined by the
  relationship $f(tT(x) + (1-t)x, t) = T(x) - x$ based on transport
  maps $T$. Theoretically, we show that under certain conditions, the
  domain covered by these straight line trajectories, which is the
  domain where the velocity field $f$ is defined, is a Lipschitz
  domain. Therefore, we can extend this velocity field continuously to
  a larger hypercube so that we may apply existing neural network
  approximation results.
    
\item We derive upper bounds for the $C^k$ norm of the velocity field
  $f$ based on the $C^k$ norm of the transport map $T$. This proof is
  based on a multivariate version of Fa\'{a} di Bruno's formula and
  interpreting partial derivatives as multi-linear tensor operators in
  abstract Banach spaces. We believe this proof is of independent
  interest, as this type of estimates also occurs in numerical
  analysis and partial differential equations. Combining this $C^k$
  norm for the velocity field with the knowledge about the domain of
  $f$ from previous part, we establish bounds on the depth and number
  of neurons of the neural network to achieve $\epsilon$-approximation
  of the velocity field $f$.
    
\item We establish bounds on the difference between the target measure
  and the push-forward measure of the source by the time-one flow map
  of the NeuralODE. This difference is measured by different metrics
  that include $L^2$, KL, and Wasserstein.
    
\item Besides the general case, we also specialize our results to some
  particular classes of transport maps that include optimal transport
  and triangular transport. In the optimal transport case, the fact
  that the Jacobian of the map is the Hessian of a convex potential
  helps to simply the bounds on the velocity field. \ymmtd{revise here
    since we are relegating OT to a remark} In the triangular case,
  the bounds can be derived in terms of the densities of the source
  and target measures.
    
    
\end{itemize}

