\tableofcontents

\jz{[Some general comments:
  
  {\bf Latex:}
  \begin{itemize}
  \item Always use $\backslash$\texttt{eqref}
    instead of $\backslash$\texttt{ref}
    for equations;
    label equations by $\backslash$\texttt{label$\{$eq:something$\}$}
    (similarly thm:..., lemma:..., prop:..., etc.)
    so that you always know what you're referencing.
  \item Every $\backslash$\texttt{ref} has to be preceded by its type
    such as ``Section'', ``Theorem'' etc.  For example don't write
    ``as in \ref{sec:setup}'', but instead ``as in Section
    \ref{sec:setup}'' (otherwise it could for example also mean ``as
    in Theorem \ref{sec:setup}'')
  \item To denote functions write $f:\Omega_0\to\Omega_1$ and
    $f:x\mapsto f(x)$, or for short
    $f:\Omega_0\to\Omega_1:x\mapsto f(x)$, i.e.\ ``$\to$'' is used for
    the domains and ``$\mapsto$'' for function values, but not the
    other way around.
  \item Use $\backslash$\texttt{det} instead of $\backslash$\texttt{text$\{$det$\}$} etc. If such an operator does not exist, define it in the preamble
    via $\backslash$\texttt{DeclareMathOperator}
  \item Quantifiers such as $\forall$ and $\exists$ should not be used
    in the text but only in formulas.    
  \end{itemize}
  
  {\bf Writing:}
  \begin{itemize}
  \item Avoid phrases that carry no information. Try to write short,
    but precise sentences. Examples:
    \begin{itemize}
    \item ``Note that this implies $x=1$.'' $\to$ ``This implies $x=1$.''
    \item ``Assume in addition that the map $T$ also satisfies the
      condition that there exists a constant $c$ such that
      $\det \nabla T_t(x) > c > 0, \forall x\in\Omega_0$.'' $\to$
      ``Assume $\inf_{x\in\Omega_0}\det\nabla T_t(x)>0$.''  (also: (i)
      state what $t$ is in this sentence (ii) unless necessary, avoid
      introducing constants such as $c$ which are not used in the
      following)
    \end{itemize}
  \item ``There exists a unique $f\in C^k$ satisfying ...'' means $f$
    satisfying something is unique within the set $C^k$ (but there
    might exist another less regular function achieving the same);
    what was meant is probably that there exists a unique function
    $f:\Omega_0\to\R^d$, and this $f$ happens to belong to $C^k$.
\item If you introduce a function, it's helpful for the reader to
  state its domain and range. The same goes for statements of the type
  ``$f(x)=g(x)$'': unless obvious, it should always be added for which
  $x$ this is true.
  \end{itemize}


{\bf Theorems+Proofs:}
\begin{itemize}
    \item Some of the theorems read a bit clunky and/or are hard
    to understand. A few suggestions:
    \begin{itemize}
    \item Try to give the statement in a succinct but precise way.
    \item In every Lemma, Thm.\ etc., all objects, notions and
      variables, as well as their possible range must be unambiguous.
    \item Always start with stating all necessary assumptions.  Make
      sure that all assumptions you state are actually required
      somewhere in the proof.
    \item Explanations/interpretations/implications/any kind of discussion
      should be after (or before) the theorem, but
      not in the theorem environment.
    \item The statement of the Thm.\ must be clear,
    e.g., is it of the type ``there exists $k$ such that'' or is it of
    the type ``for all $k$ something holds''?
  \end{itemize}
  \item Proofwriting
    \begin{itemize}
    \item If you write down a proof, try to structure it before you
      write it down.
    \item To guide the reader through a proof, it can be helpful to
      state what you are going to show next, or what is left to show,
      or what we just did show.
  \item Sometimes there are sentences in the proofs which do not seem to
    be necessary to show the statement, which makes it hard to follow
    the argument.
  \item Occasionally statements do not seem to be shown or referred to
    in the proof. Example: uniqueness of $f$ is stated in
    Thm.~\ref{thm:f}, but the (previous version of the) proof did not
    say anything about uniqueness.
  \item If you make an assumption (such as $f\in C^k$), unless
    obvious, at some point in the proof it should be referred to. It
    should not be left to the reader to try and find out where some
    assumptions are implicitly required.
  \end{itemize}  
  \item Think about whether
    the assumptions you have are necessary/can be weakened/are
    redundant. A few examples:
    \begin{itemize}
    \item ``convex and simply connected'' is redundant
    \item Convexity of $\Omega_1$ is assumed throughout, but
      does not seem to be used.
    \item Theorem \ref{th:minimalenergy} assumes $T\in
      C^k$. This property does not seem to be used in the
      proof.
  \end{itemize}      
\item If you do have some assumptions that you refer to often, you can
  put them in an assumption environment.
  \item If an assumption is very simple to state, just write it out.
    E.g., instead of ``Let $\Omega_0$ be as in Section
    \ref{sec:setup}.'' simply write ``Let
    $\Omega_0\subseteq\R^d$ be convex.''.
\end{itemize}


{\bf Citing:}
\begin{itemize}
\item When citing a specific theorem/result etc., \emph{always} point
  to the exact theorem (or page) of the paper/book: For example: ``As
  shown in \cite[Theorem 1]{measure-transport}'' instead of ``As shown
  in \cite{measure-transport}''.
  \item When citing a result from somewhere else, double check to make
    sure that all assumptions and the statement are exactly as in that
    reference (not necessarily word by word, but in meaning). In the
    current form both Thm.~\ref{thm:functionExtension} and
    Thm.~\ref{thm:LipTransformation} seem to have errors.
  \item If you take part of a proof/argument from somewhere else,
    unless ``standard'', this has to be made clear (even if it's
    from your own paper or you're paraphrasing), for example in Lemma
    \ref{lemma:convexLip} or
    Thm.~\ref{thm:ExistenceOfOptimalSolution}.
  \end{itemize}
  ]
  }

\section{Introduction}
Sampling from an arbitrary, possibly very complex distribution is one
of the central problems in computational statistics. One approach to
tackling the problem is through measure-transport
(\cite{measure-transport}); that is, we construct a deterministic
coupling, i.e., a transport map, between the complex target measure
and a relatively simple source measure, which is usually chosen to be
uniform or standard Gaussian. Then, sampling from the target measure
can be done by sampling from the source measure and pulling back
through the transport map. In the machine learning literature, a
normalizing flow is \jzs{just} a generative model composed of
relatively simple maps, that push forward the target distribution to
an easy-to-sample distribution (\cite{NormalizingFlowIntro}).
% For such
% models to work,
\jz{For such methods it is typically}
required that the maps be
invertible and the determinant of the
Jacobian is easily computable, as evaluating the \rr{push-forward density}\jztd{flow $\to$ pushforward density?} follows
from the change of variables formula. To satisfy the aforementioned
requirements, various architectures of discrete normalizing flows have
been proposed, including planar and radial flows (\cite{PlanarFlow}),
coupling flows (\cite{CouplingFlows}), autoregressive flows
(\cite{autoregressiveflow}) and neural autoregressive flows
(\cite{NeuralAutoFlow}).

In a recent advancement of deep learning, various connections between
differential equations and neural networks have been made
(\cite{TransportAnalysisofDL}, \cite{NumericalSchemeODE},
\cite{PDEmotivatedNN}), which led to the idea of NeuralODEs
(\cite{NeuralODE}). More recently, NeuralODEs \jz{have been proposed}
to perform generative modelling tasks (\cite{ffjord}). The generative
process is as follows: \jz{Denote by $\pi$ the target
  distribution from which we wish to sample,
  and by $\rho$ the source distribution.
  % . Typically, $\pi$ is
  % either known through an iid sample or through its (unnormalized)
  % density, see Sec.~\ref{sec:setup} ahead. For a reference
  % distribution $\rho$,
  Given $x\sim\rho$, the following initial value
  problem
% to get the time-one solution $x(1)$ for the ODE:
\begin{equation}\label{eq:ODE}
\begin{aligned}
% \frac{dX(x,t)}{dt}
\frac{d X(x,t)}{dt} &= f(X(x,t), t),\\
X(x, 0) &= x,
\end{aligned}
\end{equation}
is solved up to time $t=1$. The goal is to learn the velocity field
$f$, which is parametrized as a neural network, such that $x\sim \rho$
implies $X(x,1)\sim\pi$.} \jztd{Before it said $f(x,t,\theta_t)$. I
guess $\theta_t$ was a typo? In any case, $\theta$ didn't occur
anymore so I removed it.}
\jztd{Sven made a good comment in the group meeting: We should
    assume throughout that $f$ is Lipschitz continuous so that
    \eqref{eq:ODE} always has a solution.}

% If the neural network $f$ is properly learned, then the points $x(1)$ will be approximately sampled from the distribution $\rho(x)$.
\jz{In the context of distribution learning,}
NeuralODEs % as continuous normalizing flows
enjoy % many advantages.
\jz{several desirable properties}:
\jzs{In ODE structures,} \jz{Invertibility of $x\mapsto X(x,1)$ is
guaranteed by design,
% as one can always integrate it backwards in time,
as one can always solve \eqref{eq:ODE} backward in time.} Thus,
\jz{in contrast to other methods \rr{such as invertible neural networks (\cite{InvNN}), normalizing flows (\cite{NormalizingFlowIntro}) or transport maps (\cite{measure-transport})}, no further restrictions need to be imposed on the
  vector field $f$ that is to be learned.}
% there is no limitations on the form the ODE dynamics (or the neural
% networks) need to take. In addition, The change
\jz{In addition, the density} $\eta(x,t)$ % under NeuralODE
\jz{of $X(x,t)$ (with $x\sim\rho$) can easily be computed, as it
  follows the dynamics}
$\frac{\partial \log \eta(x,t)}{dt} = -\tr(\frac{\partial f}{\partial
  X})$\jztd{The formula is unclear. Check which are partial
  derivatives and which are not. What is $p(x(t))$? Should it be
  $p(x,t)$ (or rather $\pi(x,t)$)?}, \rr{where $\tr(\cdot)$ denotes
  the trace and $\frac{\partial f}{\partial
  X}$ is the Jacobian of $f$ with respect to $X(x,t)$. This is known as the \emph{instantaneous change of
    variables formula}, see \cite{NeuralODE}.}
% another
% ODE, called the instantaneous change of variables formula
% (\cite{NeuralODE}):
% $\frac{\partial \log p(x(t))}{dt} = -\text{Tr}\left(\frac{\partial
%     f}{\partial x(t)}\right)$.

In regards to the usefulness of NeuralODEs as generative models, there
are three questions that are natural to ask. First, % note that
it is known that when the source and target measures are well-behaved
(for example, both absolutely continuous with respect to the Lebesgue
measure), there are \jz{in general} infinitely many transport maps
that push forward one measure onto the other. \rrs{as long as dimension
$d > 1$}.\jztd{I think it's also true in $1d$.} Even if we require the
time-one flow map \jz{$x\mapsto X(x,1)$} to be a particular transport
map $T$, there are \jz{in general still many possible velocity fields
  $f$ realizing $T$.}
 % still infinitely many ways how points from the source measure, $x$, can be connected to points from the target measure, $T(x)$, through the ODE trajectories.
It is observed in \cite{OTFlow} and \cite{HowToTrain} that without any form of regularization, the trajectories of the ODE dynamics connecting $x$ to $T(x)$ \jz{may} be very irregular. % Therefore, % the first question that
Therefore
\jz{it is natural to ask how we can regularize the training objective
to improve the training process.}
% so that the velocity fields we learn in the end is nice in an appropriate sense.
Secondly, with the regularized training objective, we would like to
understand the structures of its optimal solutions and quantify the
neural network approximator for them. The third question to ask, then,
is how can we bound the distance between the target measure and the
push-forward measure of the source under the flow map of the NeuralODE
when we substitute in the neural network as an approximation for the
velocity field? Our work is the first attempt to address these
questions in a unified way. In the \jz{remainder} of this section, we
will review some relevant literature and summarize our main
contributions.


\subsection{Prior and Related Work}
In this section, we provide an overview of the \jz{most} relevant prior work. After normalizing flows have demonstrated their success in many practical applications, a few papers have studied the representation powers of such models. For discrete normalizing flows, \cite{DisAppro2} studies some basic flow structures including planar flows, radial flows, Sylvester Flows and Householder flows and shows that these basic flows are highly expressive in dimension $d = 1$, but their expressivity can be limited in higher dimensions. \cite{DisAppro1} shows that invertible neural networks based on coupling flows are universal approximators for diffeomorphisms. 

For NeuralODEs, it turns out that the analysis of their representation power is more complicated. It is observed in \cite{Augmented} that due to the fact that the flow maps of ODEs are homeomorphisms, there are some simple functions that can not be exactly represented by NeuralODEs, and it is argued there that dimension augmentation by appending zeros to the input vectors might be necessary. Later in \cite{ApproximationCapability}, it is proved that homeomorphism on a $p$-dimensional Euclidean space can be exactly represented by an ODE operating on a $2p$-dimensional Euclidean space, with a vector of $p$ zeros being appended to the input vectors. However, for the purpose of \textit{distribution learning}, these results are not very useful because \textit{(i)} the failure in exact representation does not imply the impossibility of approximation and \textit{(ii)}, universal approximation of the distribution does not necessarily need universal function approximation. Indeed, some more recent works do show that NeuralODEs are $L^p$-universal approximators for continuous maps under certain technical assumptions (\cite{DynamicalSystem}) and they are $L^\infty$-universal apprxoimators for a large class of diffeomorphisms (\cite{SupApproximation}). Still, these works only focus on the \textit{function approximation} aspect of NeuralODEs; moreover, these types of universal-approximation results all take the underlying neural network in the ODE as a black box and there is no quantification of how complex the neural network has to be in order to achieve certain degree of accuracy. 

Another class of generative models related to NeuralODEs is called Neural stochastic differential equations (NeuralSDEs), which is a SDE whose drift term is parametrized by a neural network. Among the infinitely many possible drift terms that could push forward the source to the target distribution, it is shown in \cite{NeuralSDE} that there is a  drift named the F\"ollmer drift that achieves exact sampling and it is nice in the sense of stochastic control theory. Subsequent regularity results about the F\"ollmer drift and neural network approximation theories have been developed for distribution learning in this setting. Part of our work will be to prove similar results in the context of NeuralODEs, which is currently lacking. 

Regarding the training of NeuralODEs as continuous normalizing flows, some regularization techniques have already be discussed. It is argued in \cite{HowToTrain} that a good way of measuring the regularity of the vector field is by the force experienced by a particle $x(t)$ under the dynamics generated by the vector field $f$, which is the total derivative of $f$ in time: 
\begin{equation}\label{eq:straightlinereg}
\frac{df(x,t)}{dt} = \nabla_x f(x,t) \cdot \frac{dx}{dt} + \frac{df(x,t)}{dt} = \nabla_x f(x,t) \cdot f(x,t) + \frac{df(x,t)}{dt}.
\end{equation}
Observe that when this term is zero the trajectories of the particles will be straight lines. Since the Jacobian matrix $\nabla_x f(x,t)$ is in general not easily accessible, \cite{HowToTrain} chooses to implicitly regularize this term by regularizing $\|f(x,t)\|^2$ and the Frobenius norm $\|\nabla_x f(x,t)\|_F^2$ instead. Similar to \cite{ffjord}, stochastic estimation is used in the training to compute $\|\nabla_x f\|_F^2$ and $\text{Tr}(\nabla_x f(x,t))$, which is used in the change of variables formula for log-likelihood. In a more recent work, \cite{OTFlow} adopts a discretize-then-optimize approach to training NeuralODEs, where a ResNet structure is used to implement the underlying neural network in the ODE. This approach enables exact computation of the Jacobian matrix as well as its trace from the recursive structure of the ResNet. Then, automatic differentiation is used to update the parameters in the neural network, instead of solving the adjoint equation as in \cite{ffjord} and \cite{HowToTrain}. 


\subsection{Our Contribution}
Our contributions in this work have both algorithmic and theoretical aspects.
\begin{itemize}
    \item Algorithmically, we \rr{suggest to use} the discretize-then-optimize approach of \cite{OTFlow}, which allows the Jacobian to be computed exactly and thus we propose a new training objective the regularizes the term \ref{eq:straightlinereg} directly instead of regularizing $\|f\|^2$ and $\|\nabla_xf\|^2_F$ as in \cite{HowToTrain}. 
    
    \item Under reasonable assumptions, we show that the velocity fields that achieve this optimal value of the objective must take the time-independent form (\rr{in the Lagrangian sense}) \rr{$f(tT(x) + (1-t)x,t) = T(x) - x$} for some transport map $T$ that pushes forward the source to the target measure. This implies that $x$ is connected to $T(x)$ by straight line trajectories. We also characterize the type of transport maps that can be represented by ODEs with straight line trajectories and show that for these transport maps $T$, among the infinitely many velocity fields that connect $x$ to $T(x)$, this straight line construction gives the minimal kinetic energy. Thus we call expression \eqref{eq:straightlinereg} the minimal energy regularization. 
    
    \item Since the velocity field is constant along the trajectories, the optimal velocity fields $f$ are implicitly determined by the relationship $f(tT(x) + (1-t)x, t) = T(x) - x$ based on transport maps $T$. Theoretically, we show that under certain conditions, the domain covered by these straight line trajectories, which is the domain where the velocity field $f$ is defined, is a Lipschitz domain. Therefore, we can extend this velocity field continuously to a larger hypercube so that we may apply existing neural network approximation results. 
    
    \item We derive upper bounds for the $C^k$ norm of the velocity field $f$ based on the $C^k$ norm of the transport map $T$. This proof is based on a multivariate version of Fa\'{a} di Bruno's formula and interpreting partial derivatives as multi-linear tensor operators in abstract Banach spaces. We believe this proof is of independent interest, as this type of estimates also occurs in numerical analysis and partial differential equations. Combining this $C^k$ norm for the velocity field with the knowledge about the domain of $f$ from previous part, we establish bounds on the depth and number of neurons of the neural network to achieve $\epsilon$-approximation of the velocity field $f$. 
    
    \item We establish bounds on the difference between the target measure and the push-forward measure of the source by the time-one flow map of the NeuralODE. This difference is measured by different metrics that include $L^2$, KL, and Wasserstein. 
    
    \item Besides the general case, we also specialize our results to some particular classes of transport maps that include optimal transport and triangular transport. In the optimal transport case, the fact that the Jacobian of the map is the Hessian of a convex potential helps to simply the bounds on the velocity field. \ymmtd{revise here since we are relegating OT to a remark} In the triangular case, the bounds can be derived in terms of the densities of the source and target measures.  
    
    
\end{itemize}


\section{Preliminaries}
\subsection{General Notations}
Throughout the rest of the paper, we use the following notations
unless otherwise noted: for ODEs, we use $x(t)$ to denote the
trajectory of a simple particle $x(0) = x_0$ and we use $X(x,t)$ to
denote the flow map of the ODE; that is,
$X(x,t) = x + \int_0^1f(X(x,t),t;\theta)dt$. If the starting particles 
\ymmtd{$\rho$ versus $\eta$ here. Also, should we distinguish measures from densities?}
for the ODE are samples from $\rho$, we use $\eta_t(x)$ or
sometimes $\eta(x,t)$ to denote the push-forward measure of $\rho$ under
the time-$t$ flow map; that is, $\eta_t = X(x,t)_\sharp\rho$. In some cases, we need to distinguish ODEs with different velocity fields, for velocity field $g$, denote by
  $X_g$ the solution to \eqref{eq:ODE} with
  velocity field $g$,
  and let $\eta_g(x,t)$ be the (pushforward) density of
  $X_g(\cdot,t)_\sharp\rho$. We use
$\nabla f$ to denote the gradient if
$f:\mathbb{R}^d\rightarrow\mathbb{R}$ and the Jacobian if $f$ is a
vector field $f:\mathbb{R}^d\rightarrow\mathbb{R}^d$, and we sometimes
use the subscript $\nabla_xf(x,t)$ to denote the Jacobian with respect
to the first argument in $f$. We use multi-index
$\bsv = (v_1,v_2,...,v_d) \in \mathbb{N}^d$ to denote mixed partial
derivatives for multivariate functions. $|\bsv| = \sum_{i = 1}^d v_i$ and
$\bsv! = \prod_{i=1}^d(v_i!)$ is the absolute value and factorial for
multi-indices.
$D^\bsv_x = \frac{\partial^{|\bsv|}}{\partial x_1^{v_1}, ..., \partial
  x_d^{v_d}}$ is the mixed-partial differential operator with respect
to the multi-index $\bsv$ and in case where the differentiation variable $x$ is obvious, we may omit it in the notation. For
$x = (x_1,..., x_d) \in \mathbb{R}^d$, $x^\bsv = \prod_{i=1}^d x_i^{v_i}$
and we write $x_{[k]}$ to denote $(x_1,...,x_k)\in\mathbb{R}^k$ for $k\leq d$. In general, we use single bar $|\cdot|$ to
denote norm of vectors in $\mathbb{R}^d$ and double bar $\|\cdot\|$ to
denote norm of functions or vector fields. \jztd{Different norms should should use different notation, indicated by a subscript, otherwise it's confusing (and ambiguous, for example in $1d$ the two definitions don't seem to coincide).  Also like this it's not clear over what the $L^\infty$ norm is taken. Use something like $\|f\|_{L^\infty(\Omega)}$ instead. Moreover, why is this norm only defined for partial derivatives or $D^nf$ (which itself has not been defined) rather than general functions? First introduce notation for the derivatives, then introduce notation for some norms and spaces. Finally, the notation for the partial derivative does not coincide with the one that was just introduced a few lines above.} Finally, we introduce a linear order
on $\mathbb{N}^d$. That is, we denote $\bsl \prec \bsv$ if and only if one
of the following holds: \textit{(i)} $|\bsl| < |\bsv|$, \textit{(ii)}
$|\bsl| = |\bsv|$, and \rr{there exists a $k < d$ such that} $l_1 = v_1$, ..., $l_k = v_k$, but
$l_{k+1} < v_{k+1}$.\jztd{What is $k$? I suggest to use boldface
  notation for vectors and multiindices.}
\rr{
\subsection{Function Spaces}
We work with several different function spaces in this work and in this subsection, we give the definitions and notations that we will use for these spaces and the associated norms. 


\begin{definition}
Let $X$, $Y$ be two Banach spaces. For $n\in\mathbb{N}$ we denote by
\begin{equation}
    L^n(X;Y)
\end{equation}
the space of all $n$-linear maps from $X^n\to Y$. 
For $A\in L^n(X,Y)$, we define
\begin{equation}
    \|A\| = \|A\|_{L^n(X;Y)}:=\sup_{\|x_i\|_X\le 1} \|A(x_1,\dots,x_n)\|_Y.
\end{equation}
\end{definition}

Recall that for a function $f:X\to Y$ the $k$-th Fr\'echet derivative $D^kf(x)$ of $f$ at $x\in X$ belongs to $L^k(X;Y)$.
%$D^kf(a)(x_1,\ldots,x_k) \in Y$ and all dependence on $x_i$ are %linear
\begin{definition}
Let $X$, $Y$ be two Banach spaces, $S\subseteq X$ open and $f\in C^k(S,Y)$.
Then
\begin{equation}
    \|f\|_{C^k(S)}:=\sup_{n\le k}\sup_{x\in S}\|D^n f(x)\|_{L^n(X;Y)} =\sup_{n\le k}\|D^n f\|_{L^\infty(S, L^n(X;Y))} .
\end{equation}
%We denote also the norm of the $k$-th derivative at $a\in X$ by $\|D^kf(a)\|_\infty = \sup_{\|x\|\leq 1}\|D^kf(a)(x_1,\ldots,x_k)\|,$ where $\|\cdot\|$ is the norm in the Banach space $Y$. In addition, we use the notation $\|D^kf\|_\infty$ where the point $a\in X$ is omitted to denote the sup over all $a \in X$. That is, $\|D^kf\|_\infty = \sup_{a\in X}\sup_{\|x\|\leq 1}\|D^kf(a)(x_1,\ldots,x_k)\|.$.
\end{definition}
\begin{remark}
When $S\subseteq X$ is a not open, then $f\in C^k(S,Y)$ means $\forall 0\le j\le k: D^jf\in C^0(\Omega^\circ)$ and $D^jf$ allows a continuous extension to $S$. 
\end{remark}


When $X$ is the standard Euclidean space, one might also be interested in the following quantities:
For function $f: S\subseteq\mathbb{R}^d\rightarrow\mathbb{R}$, the quantity $\max_{|\bsalpha|<k} \sup_{x\in S}|D^\bsalpha f(x)| = \max_{|\bsalpha|<k}\|D^\bsalpha f\|_{L^\infty(S)}$ and for a velocity field $f: \mathbb{R}^d\rightarrow\mathbb{R}^d$ the quantity $\max_{i = 1, ..., d}\max_{|\bsalpha|<k} \|D^\bsalpha f_i(x)\|_{L^\infty(S)}$, where $\bsalpha$ is a multi-index. These quantities are usually used to define $C^k$ norm of functions and velocity fields on Euclidean space and in a technical lemma later, we will prove that they are equivalent with the norms on general Banach spaces, which are defined as operator norms as above. 
%For simplicity of notation, we use $\|D^nf\| = \sup_{x\in S}\|D^nf(x)\|_{L^n(X,Y)}$ to denote the
%operator norm of the multi-linear tensor if $n$ is an integer and $\|D^\bsv f\| = \sup_{x\in S}|D^\bsv f(x)|$ to denote the $L^\infty$
%norm for the partial derivative if $\bsv$ is a multi-index.
In most situations, the set $S$ is obvious and may be omitted in the notation. 

A broader class of functions is called Sobolev functions, whose definition is the follows: 
\begin{definition}
Sobolev space $W^{k,p}(\Omega)$ is the space of functions on domain $\Omega$ such that the respective Sobolev norm defined below is finite:
\begin{equation}
  \|f\|_{W^{k, p}} =
    \begin{cases}
      (\sum_{|\bsalpha|\leq k} \|D^\bsalpha f\|^p_{L^p})^\frac{1}{p} & \text{if $ 1\leq p < \infty$}\\
      \max_{|\bsalpha|\leq k} \|D^\bsalpha f\|_{L^\infty}& \text{if $ p = \infty$}
    \end{cases}       
\end{equation}
\end{definition}
Note that in the definition of Sobolev space $W^{k, \infty}$, derivatives $D^\bsalpha f$ are assumed to exist in the \textit{weak sense}.




}
\subsection{Problem Setup}\label{sec:setup}
In this work, we consider transportation of measures on bounded
domains. \rr{Let $\pi$ and $\rho$ be target and source measures supported
on $\Omega_0$, $\Omega_1 \subset \mathbb{R}^d$ respectively}. The
target measure is where we want to sample from and the source measure
is chosen by us and is usually a simple distribution. Formally, we make the following assumptions about the measures and the domains.

\begin{assumption}\label{AssumptionDensity1}
Measures $\pi$ and $\rho$ are supported on compact and convex domains $\Omega_0$, $\Omega_1$ respectively. They are absolutely continuous with respect to each other \ymmtd{Do we need this? Since they are also absolutely continuous wrt Lebesgue, doesn't this imply $\Omega_0 = \Omega_1$?} 
and have densities $\rho(x)$ and $\pi(x)$ with respect to Lebesgue measure. 

\begin{assumption}\label{AssumptionDensity2}
The densities $\pi(x)$ and $\rho(x)$ are $C^k$ functions on $\Omega_0$ and $\Omega_1$ respectively, with $\|\rho\|_{C^k} \leq L_1$, $\|\pi\|_{C^k} \leq L_1$ and $\inf_{x\in\Omega_0}\pi(x) \geq L_2$ for some constants $L_1, L_2$.
\end{assumption}


\end{assumption}
\jztd{is this necessary for $\Omega_1$?}\rrtd{I think here we may assume both domains are compact and convex because the flow starts from $\Omega_0$ and $\Omega_1$ for two types of problems respectively, but for theoretical analysis, we assume we always start from $\Omega_0$} 

For simplicity, we only consider terminal time of the ODE to be one. For
impact of the terminal time in the ODE dynamics and time scaling,
there is a discussion at the end of the paper \ref{}.

\jz{We consider two types of problems:}
\begin{itemize}
    \item The target measure \jz{$\pi$} is known through a collection of \jz{iid} samples. This is the problem considered in \cite{ffjord}, \cite{HowToTrain} and \cite{OTFlow}. The goal in this case is to \jzs{start from the samples in $\pi$ and} \rr{learn the parameters in the neural network parametrizing the velocity field of the ODE} \eqref{eq:ODE}\jztd{We don't learn the architecture we learn the parameters of the network.} such that the time-one flow map satisfies $\rr{\eta(\cdot, 1) = X(\cdot,1)_\sharp}\pi\approx\rho$. Having access to such an invertible transformation allows \jzs{one} to \jz{transform samples from the % simple
      source measure % and pull them back
      to samples from the target measure}. 
    \item The target measure is known up to the normalization constant; that is we can evaluate the unnormalized density $\Tilde{\pi}$. This problem is relatively less discussed in the machine learning community, \ymmtd{I'd color this slightly differently. :) Let me rephrase in my pass.} but of particular importance for Bayesian statistics because the posterior distribution obtained there is usually unnormalzied (\cite{measure-transport}). This problem is in the reverse direction of the previous problem; that is, we start from the source and the goal here is to learn the NeuralODE so that $X(\cdot,1)_\sharp\rho\approx\pi$. 
    
\end{itemize}








\section{Training Algorithm} \ymmtd{This section is overall a good candidate for shortening. It's more than we need in terms of setup for the main results of Section 4.}
\subsection{Target Measure is Known through a Collection of Samples}
The training objective we consider consists of two parts, one is the KL divergence between the push forward of the target by the time-one flow map of the ODE and the source measure, and the other part is the integration of \ref{eq:straightlinereg} along the trajectories of the particles. 

First, we derive the training objective for the problem where the target measure is known through a collection of samples. By the change of variables formula, the change in density as we flow a sample $x\sim\pi$ at time $t\in[0,1]$ is given by $\pi(x) = \eta(X(x,t),t)\det(\nabla_x X(x,t))$. To calculate the KL, we have 
$$d_\text{KL}[X(x,1)_\sharp\pi(x)|| \rho(x)] = d_\text{KL}[\eta(x,1)|| \rho(x)] =\int_{\mathbb{R}^d} \log\left(\frac{\eta(x,1)}{\rho(x)}\right)\eta(x,1)dx.$$ 

By the change of variables formula, we can express this as
\begin{align*}
d_\text{KL}[X(x,1)_\sharp\pi(x)|| \rho(x)] 
&= \int_{\mathbb{R}^d} \log\left(\frac{\eta(X(x,1),1)}{\rho(X(x,1))}\right)\eta(X(x,1),1) \det(\nabla_x X(x,1))dx\\ 
&=  \int_{\mathbb{R}^d} \log\left(\frac{\pi(x)}{\rho(X(x,1))\det(\nabla_x X(x,1))}\right)\pi(x)dx.      
\end{align*}
Note that this can be expressed as as an expectation over the target measure $\pi(x)$: 
$$d_\text{KL}[X(x,1)_\sharp\pi(x)|| \rho(x)] = \mathbb{E}_{\pi(x)}[\log\pi(x) - \log\rho(X(x,1)) - \log\det(\nabla_x X(x,1))].$$

The term $\log\det(\nabla_x X(x,1))$ can be computed from the instantaneous change of variables formula in \cite{NeuralODE}. That is, we have $\log\det(\nabla_x X(x,1)) = -\int_0^1\text{Tr}\left(\frac{\partial f(X(x,t),t)}{dX}\right)dt$. 

\ymmtd{Could motivate this better} Note that straight line constructions imply that \ref{eq:straightlinereg} is zero everywhere in space time. For computational purpose, we penalize \ref{eq:straightlinereg} along the ODE trajectories and define the regularization term $R(x,t) = \int_0^t  |\nabla_X(f(X(x,s),s))f(X(x,s),s) + \partial_tf(X(x,s),s)|^2ds$ for $x\in\Omega_0$ and $t\in[0,1]$. Now the combined optimization \rr{objective} becomes:\jztd{not an optimization problem but an objective function; to define the optimization problem we still have to say what we optimize (minimize) over; probably it should be all Lipschitz continuous $f$, which guarantees that $J(f)$ is well-defined}
\begin{equation}\label{OP1}
\begin{aligned}
     J(f) &= d_\text{KL}[X(x,1)_\sharp\pi(x)|| \rho(x)] + \lambda\mathbb{E}_{x\sim\pi(x)}[R(x,1)]\\
     &= \mathbb{E}_{x\sim\pi(x)}[\log\pi(x) - \log\rho(X(x,1)) - \log\det(\nabla_x X(x,1)) + \lambda R(x,1)]\\
     &=\mathbb{E}_{x\sim\pi(x)}[\log\pi(x) - \log\rho(X(x,1)) +  \int_0^1\text{Tr}\left(\frac{\partial f(X(x,t),t)}{dX}\right)dt + \lambda R(x,1)]\\
\end{aligned}\tag{OP1}
\end{equation}
where $\lambda$ controls the impact of penalization. 

\rr{By Picard–Lindel\"{o}f theorem (\cite{ArnoldODE}), for the ODE \eqref{eq:ODE} to have existence and uniqueness of solutions, it is required that the velocity field $f$ is uniformly Lipschitz continuous in space variable $X$ and continuous in time variable $t$. Therefore, we are constrained to search over the space of functions that we denote as $\mathcal{V}$, where 
\begin{equation}\label{eqn:VelocityField}
\mathcal{V} = \{f(x,t):\mathbb{R}^d\times[0,1]\rightarrow\mathbb{R}^d|f\text{ is continuous in } t \text{ and Lipschitz continuous in } x\},    
\end{equation}



and we have the following optimization problem:
\begin{equation*}
\begin{aligned}
& \underset{f(x,t)\in\mathcal{V}}{\text{minimize}}
& & J(f)\\
& \text{subject to}
& & X(x,1) = \int_0^1f(X(x,t),t)dt\\
&
& & R(x,1) = \int_0^1  |\nabla_X(f(X(x,t),t))f(X(x,t),t) + \partial_tf(X(x,t),t)|^2dt.
\end{aligned}
\end{equation*}
}
\begin{remark}
In practice, the conditions of Picard–Lindel\"{o}f theorem will always be satisfied for a neural network with Lipschitz activation functions and finite size. In particular, these will hold true for ReLU networks, which is what we consider in theoretical analysis. 
\end{remark}




To compute the optimization objective, we need to be able to compute the matrix $\frac{\partial f(X(x,t),t)}{\partial X}$, as its trace appears in computing the log-determinant term and the entire matrix appears in $R(x,1)$. Also, we need to be able to compute $\partial_tf(X(x,t),t)$. Note that we can assemble these two parts into a full Jacobian matrix, which we denote by $\nabla_{X,t}f(X(x,t),t)$. This is the Jacobian matrix of a neural network with respect to all of its inputs. With the discretize-then-optimize approach in \cite{OTFlow}, we can compute this exactly via automatic differentiation. For details, see appendix \ref{ExactCompJacobian}.

In practice, we only have access to a finite collection of samples from the target measure, so we need to replace the population risk in objective \ref{OP1} with empirical risk based on the samples. Moreover, since $\log\pi(x)$ is independent of the velocity field $f$, it can be thrown away in the training process. Hence, we arrive at the following \textit{empirical risk minimization} problem. 
\begin{equation}\label{ERM}
\begin{aligned}
     J_\text{ERM}(f) &= \frac{1}{N}\sum_{i=1}^N\left( -\log\rho(X(x_i,1)) - \log\det(\nabla_x X(x_i,1)) + \lambda R(x_i,1)\right)\\
     &=  \frac{1}{N}\sum_{i=1}^N\left( -\log\rho(X(x_i,1)) + \int_0^1\text{Tr}\left(\frac{\partial f(X(x_i,t),t)}{dX}\right)dt + \lambda R(x_i,1)\right)
\end{aligned}\tag{ERM}
\end{equation}
Putting everything together, we have Algorithm \ref{algo1}. 

  \jztd{macro $\backslash$mb was not defined; I commented the algorithm out, since the file doesn't compile otherwise}
 \begin{algorithm}
 \caption{Sampling from target distribution known through samples}
 \label{algo1}
 \begin{algorithmic}[1]
 \STATE \textbf{Input}: data $X = \{ x_i\}_{i=1}^N$ from target measure $\pi$, parametrized neural network $f(\cdot\,; \theta)$, regularization strength $\lambda$, source measure $\rho$.
 \STATE\textbf{Initialize} $\theta$,
 \WHILE{$\theta$ not converged}
 \STATE{Sample minibatch $\{ x_j\}$ of size $m$ from $X$}
 \STATE{Set $ x_j(0) =  x_j$, $l_j = r_j(0)=0$ }
 \STATE{Solve the following ODE system up to time one}
 \STATE{\begin{itemize}
     \item $\frac{dx_j}{dt} = f(x_j,t;\theta)$
     \item $\frac{dl_j}{dt} = -\text{Tr}(\nabla_xf(x_j,t;\theta))$
     \item $\frac{dr_j}{dt} = |\nabla_xf(x_j,t;\theta)f(x_j,t;\theta) + \partial_tf(x_j,t)|^2$
 \end{itemize}}
 \STATE{Compute the loss $L(\theta) = \frac{1}{m}\sum_{j=1}^m-\log\rho(x_j(1))-l_j(1)+\lambda r_j(1)$}
 \STATE{Use Automatic Differentiation (AD) to backpropagate and update $\theta$}
 \ENDWHILE
 \end{algorithmic}
 \end{algorithm}

\subsection{Target Measure is Known up to Normalizing Constant}
In this setting, we assume that the density $\pi(x) = \Tilde{\pi}(x)/C$, where $C$ is the normalization constant. In this case, we seek to minimize $d_\text{KL}[X(x,1)_\sharp\rho(x)||\pi(x)]$ plus the regularization term. The KL-divergence term can be derived similarly:
\begin{align*}
d_\text{KL}[X(x,1)_\sharp\rho(x)||\pi(x)] &=  d_\text{KL}[X(x,1)_\sharp\rho(x)||\Tilde{\pi}(x)/C] \\
&= \mathbb{E}_{x\sim\rho(x)}[\log\rho(x) - \log\Tilde{\pi}(X(x,1)) + \log C - \log\det(\nabla_x X(x,1))],
\end{align*}
and the combined optimization objective  \ref{OP2}  is the follows:
\begin{equation}\label{OP2}
\begin{aligned}
     J(f) &= d_\text{KL}[X(x,1)_\sharp\rho(x)||\pi(x)] + \lambda\mathbb{E}_{x\sim\rho(x)}[R(x,1)]\\
     &= \mathbb{E}_{x\sim\rho(x)}[\log\rho(x) - \log\Tilde{\pi}(X(x,1)) + \log C - \log\det(\nabla_x X(x,1)) + \lambda R(x,1)]\\
     &= \mathbb{E}_{x\sim\rho(x)}[\log\rho(x) - \log\Tilde{\pi}(X(x,1)) + \log C + \int_0^1\text{Tr}\left(\frac{\partial f(X(x,t),t)}{dX}\right)dt + \lambda R(x,1)],\\
\end{aligned}\tag{OP2}
\end{equation}
where $R(x,1) = \int_0^1  |\nabla_X(f(X(x,t),t))f(X(x,t),t) + \partial_tf(X(x,t),t)|^2dt$ is the same as the previous problem, except now $x$ is sampled from $\rho$.

For the purpose of optimization in practice, independent terms in \ref{OP2} like $\log\rho(x) + \log C$ could be dropped. Also, note that compared to the previous problem, we now know everything about $\rho$. To evaluate the expectation in \ref{OP2}, a simple Monte-Carlo method would yield Algorithm \ref{algo1}, with the role of $\pi$ and $\rho$ switched. We comment here that there exist a host of techniques to approximate the integral with respect to the source measure, including quadrature and cubature formulas, sparse quadratures, and various other Monte-Carlo methods and we will not discuss them in details here. Regardless of the tool for computing the expectation, automatic differentiation combined with an explicit ODE integration scheme would give the update on the parameters in the neural network. 

\section{Theoretical Analysis for the\jztd{There is not \emph{one} minimizer, but many} Optimal Solution}
\ymmtd{It might be worth breaking this into two sections (at the \texttt{section} level of the hierarchy): one on existence and structure of minimizers (so $4.1 \to 4$) and the other on regularity of the velocity field (so $4.2 \to 5$)}
% We notice that
\jz{The two} optimization objectives \ref{OP1} and \ref{OP2} take a
similar form. % that is,
They are the sum of two terms, the first is the KL divergence between
the push-forward measure under the time-one flow map of the NeuralODE
and another measure, and the second % of which
% is
is a quadratic regularization term integrated in space time. For our
theoretical analysis, we consider the optimization problem in this
general form
\begin{equation}\label{eq:OP}
J = d_\text{KL}[X(x,1)_\sharp\pi||\rho] + \lambda\mathbb{E}_{x\sim\rho(x)}[R(x,1)], \tag{OP}
\end{equation}
where we don't distinguish between target and source, and just assume $\pi$ and $\rho$ are two generic measures that satisfy assumptions \ref{AssumptionDensity1} and \ref{AssumptionDensity2}. The goal is then to find a generic NeuralODE whose time-one flow map pushes forward $\pi$ to $\rho$ through straight line trajectories. 
\subsection{Existence and Structure of % the Optimal Solution
  \jz{Minimizers}
}
% The central question that we would like to understand is the structure of the optimal solution for the optimization problem.
\jz{The objective $J(f)$ is % clearly
  nonnegative, since it is the sum of the KL-divergence and the
  nonnegative regularizer}.  Moreover, if the measures are absolutely
continuous \jz{w.r.t.}\ each other, both terms in the optimization
objective can be made zero: there always exists a transport $T(x)$ map
that pushes forward $\pi$ to $\rho$, so the KL divergence can be made
zero; Also, if $x$ is connected to $T(x)$ via straight line
trajectories, the regularization term can be made zero.
\jztd{This is not clear to me, in general straight line
  regularization does not lead to \eqref{eq:ODE} since the lines
  might cross.}
\rr{First, we state the necessary and sufficient conditions of a transport map $T(x)$ such that there exists velocity field $f$ that yields this straight line construction.}


\jz{[In principle, for most of the following we don't need that
  $T:\mathbb{R}^d\to\mathbb{R}^d$, we could also have
  $T:\mathbb{R}^d\to\mathbb{R}^m$ with $m>d$. This is a practically
  relevant case discussed in several recent publications (I don't know
  about neural odes though). It might be worth to add a section where
  we generalize our results to the case where the target lives for
  example on a $d$-dimensional $C^k$ manifold in $\mathbb{R}^m$.]}
\begin{lemma}\label{lemma:Tinjective}
  \jz{Let $\Omega_0\subseteq\R^d$ be convex
    and $T\in C^1(\Omega_0,\R^d)$ such that
    $\det \nabla_x T(x)\neq 0$ for all $x\in\Omega_0$.
    Then $T$ is injective.
  }
% If $T:\Omega_0 \rightarrow\Omega_1$ is a $C^1$ map from a convex set $\Omega_0\subset\mathbb{R}^d$ and $\nabla T$ is such that $\det(\nabla T(x)) > 0$ for all $x\in\Omega_0$, then $T:\Omega_0 \rightarrow\Omega_1$ is injective. 
\end{lemma}
\begin{proof}
  \jz{Assume not. Then there exist $x$, $y\in\Omega_0$ such that
    $x \neq y$ and $T(x) = T(y)$. For $s\in [0,1]$ set
    $f(s) := T((1-s)x + sy)$.} Since $f(0) = f(1)$, \jz{by the mean value
  theorem} there exists \jz{$s\in (0,1)$} such that $f'(s) = 0$. Then
  $f'(s) = \nabla_x T((1-s)x + sy)(y-x) = 0$. \jz{Since
    $v = y-x \neq 0$, we have $\det(\nabla_x T((1-s)x + sy))=0$, which
    is a contradiction.}
  % satisfies $v\in\text{Ker}\nabla T((1-s)x + sy)$ and we have
  % $\det(\nabla T(x)) = 0$, which is a contradiction.
\end{proof}

\jz{Denote in the following for $x\in\Omega_0\subset\R^d$, $t\in [0,1]$
  and a map $T:\Omega_0\to\R^d$,
\begin{equation}\label{eq:Tt}
  T_t(x) := (1-t)x + tT(x),
\end{equation}
i.e.\ $[0,1]\ni t\mapsto (T_t(x),t)$ parametrizes the straight line
between the points $(x,0)$ and $(T(x),1)$ in $\R^d\times [0,1]$.  We
refer to $t\mapsto T_t(x)$ as the \emph{displacement interpolation} of
$T$. We now investigate under which conditions these lines do not
cross for different $x$, which is necessary in order for $T_t(x)$ to
be expressable as a flow $X(x,t)$ solving \eqref{eq:ODE} for a certain
$f$. In other words, we check under what conditions the map
$\Omega_0\ni x\mapsto T_t(x)$ is injective for all $t\in [0,1]$. To
state the following lemma, for $A\in\R^{d\times d}$ we introduce the
notation $\sigma(A):=\set{\lambda\in\R}{\det(A-\lambda I)=0}$ for its
spectrum.  }
% Now, set $T_t(x) = (1-t)x + tT(x)$, we have the following corollary.
% \begin{corollary}\label{corollary:injectivity}
% Let $\Omega_0$ be a convex set. Assume that $\det(\nabla T_t(x)) > 0, x\in\Omega_0, t\in[0,1]$. Then $T_t:\Omega_0\rightarrow\Omega_1$ is injective for all $t\in[0,1]$. 
% \end{corollary}
\jztd{Removed corollary, which was repetition of the lemma before.}

% In the following lemma, we also give a characterization of when the determinant of the Jacobian $\nabla T_t(x)$ is positive based on its eigenvalues.

\begin{lemma}\label{lemma:spectrum}
  \jz{Let $\Omega_0\subseteq\R^d$ be convex and
    $T\in C^1(\Omega_0,\R^d)$.  Then $\det(\nabla_x T_t(x))>0$ for all
    $x\in\Omega_0$ and all $t\in[0,1]$, iff
    \begin{equation}\label{eq:spectrum}
      (-\infty, 0]\cap \sigma(\nabla_x T(x)) = \emptyset\qquad
      \forall\,x\in\Omega_0.
    \end{equation}
    }
% It holds $\det(\nabla T_t(x)) > 0$ for all $x\in\Omega_0\subset\mathbb{R}^d$ ($\Omega_0$ convex) and $t\in[0,1]$ if and only if $\sigma(\nabla T(x)) \cap (-\infty, 0] = \emptyset$, where $\sigma$ denotes the spectrum of a matrix. 
% note to selves: \sigma = spectrum of a matrix
\end{lemma}
\begin{proof}
  \jz{Since $\nabla_x T_t(x) = (1-t)I + t\nabla_x T(x)$, the map
    $t\mapsto\det(\nabla_x T_t(x))\in\mathbb{R}$ is continuous.
    Because of
    $\nabla_x T_0(x) = I$}, to prove the lemma
  it is sufficient to show that for every $x\in\Omega_0$,
  $\sigma(\nabla_x T(x)) \cap (-\infty, 0] = \emptyset$
  iff
  $\det(\nabla_x T_t(x))\neq 0$ for all $t\in[0,1]$.

  \jz{Fix $x\in\Omega_0$.}  Assume for contradiction that for some
  $t\in[0,1]$, we have $\det(\nabla_x T_t(x)) = 0$. \jz{Then there
    exists $v\neq 0$ such that $\nabla_x T_t(x)v = 0\in\R^d$. Thus}
  $\nabla_x T(x)v = -\frac{1-t}{t}v$ and hence
  \jz{$-\frac{1-t}{t}\in (-\infty,0]$} is an eigenvalue of
  $\nabla_x T(x)$. \jz{The reverse implication follows similarly.
    Assume that $s\in \sigma(\nabla_x T(x))\cap (-\infty,0]$.  Then
    there exists $v\neq 0$ such that $\nabla_x T(x)v=sv$. Since
    $t\mapsto -\frac{1-t}{t}:(0,1]\to (-\infty,0]$ is bijective, we
    can find $t\in (0,1]$ such that $\nabla_x T(x)v=-\frac{1-t}{t}v$, implying
    $v\in \ker(\nabla_xT_t(x))$ and thus $\det(\nabla_x T_t(x))=0$.}
  % Note $t\rightarrow -\frac{1-t}{t}$ is a bijective
  %   map from $[0,1]$ to $(-\infty, 0]$, the claim is then established.
\end{proof}

Combining the \jz{previous two statements}
% result in \jz{Cor.~}\ref{corollary:injectivity} with the implicit function theorem, we
establishes the existence of a velocity field such that the time-one
flow map of the ODE \jz{\eqref{eq:ODE} realizes} the map \jz{$x\mapsto T(x)$}, and
the ODE dynamics produce straight line trajectories.
\jztd{Suggestion for notation: $\Omega_0\to\Omega_0$ and $\Omega_1\to\Omega_0$,
which then corresponds to times $t=0$ and $t=1$.}

% \begin{theorem}\label{thm:f}
%  Let domains $\Omega_0, \Omega_1 \subset\mathbb{R}^d$ be as in \ref{sec:setup} and $T:\Omega_0\rightarrow\Omega_1$ a $C^k$ map. Assume in addition that the map $T$ also satisfies the condition that there exists a constant $c$ such that $\det \nabla T_t(x) > c > 0,  \forall x\in\Omega_0$. Let $\Omega:=\{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset \mathbb{R}^{d+1}$. Then, there exists a unique $C^k$ velocity field $f:\Omega\rightarrow\mathbb{R}^d$ such that the flow map of the ODE $\frac{dX(x,t)}{dt} = f(X(x,t), t)$ satisfies $X(x, 0) = x, X(x,1) = T(x)$ and $x$ is connected to $T(x)$ via straight line trajectories. 
% \end{theorem}
\begin{theorem}\label{thm:f}
  \jz{Let $k\in\N$ and let $\Omega_0\subseteq\R^d$ be convex and
    compact. Assume that $T\in C^k(\Omega_0,\R^d)$ for some $k\geq 2$ satisfies
    \eqref{eq:spectrum}.
    % $\det(\nabla_x T_t(x))\neq 0$ for all $x\in\Omega_0$ and all
    % $t\in [0,1]$.
    With $T_t$ in \eqref{eq:Tt} set
    \begin{equation}\label{eq:Omega}
      \Omega :=\set{(T_t(x),t)}{x\in\Omega_0,~t\in [0,1]}\subset\R^{d+1}.
    \end{equation}
    
    Then there exists a unique $f:\Omega\to\R^d$ such that the
    solution $X:\Omega_0\times [0,1]\to\R$ of the ODE \eqref{eq:ODE}
    satisfies $X(x,t)=T_t(x)$ for all $x\in\Omega_0$, $t\in [0,1]$.
    Moreover $\Omega$ is simply connected and $f\in C^k(\Omega^\circ)$.}
\end{theorem}\jztd{Changed proof: The previous version did not argue
uniqueness of $f$, and I think it only showed $C^k$ in $x$ but not in $t$.}

    \begin{proof}
      \jz{By \eqref{eq:ODE} and because $X(x,t)=T_t(x)=(1-t)x+tT(x)$,
        we have for $x\in\Omega_0$ and $t\in [0,1]$
        \begin{equation}\label{eq:Tx-x}
          T(x)-x = \frac{d}{dt}X(x,t)=f(X(x,t),t)=f(T_t(x),t).
        \end{equation}
        By Lemma \ref{lemma:Tinjective} and Lemma \ref{lemma:spectrum}
        the map
        \begin{equation*}
          (x,t)\mapsto G(x,t):=(T_t(x),t)\in\Omega
        \end{equation*}
          is injective on
        $\Omega_0\times [0,1]$.  Thus \eqref{eq:Tx-x} uniquely defines
        $f$ at each point $G(x,t)\in\Omega$. By construction, this
        $f$ yields a flow map $X$ as in \eqref{eq:ODE} satisfying $X(x,t)=T_t(x)$.}

      
      % The fact that trajectories connecting $x$ to $T(x)$ are straight lines implies that $\forall t\in[0,1], x + \int_0^tf(X(x,s), s)ds = tT(x) + (1-t)x$. This is equivalent to $\int_0^tf(X(x,s), s)ds = t(T(x) - x)$. Differentiating in $t$ on both sides, we get $f(X(x,t), t) = T(x) - x$. On the other hand, the flow map $X(x,t) = x + \int_0^tf(X(x,s), s)ds =  tT(x) + (1-t)x$. This construction enforces the constraint that the velocity field is constant along each trajectory; that is,  $f((1-t)x + tT(x), t) = T(x) - x$.

      \jz{The map $G:\Omega_0\times [0,1]\to\Omega$ is a continuous
        bijection, and since $\Omega_0\times [0,1]\subseteq\R^{d+1}$
        is compact, $G^{-1}:\Omega\to \Omega_0\times [0,1]$ is also a
        continuous bijection. Since $\Omega_0\times [0,1]$ is a convex
        set, it is simply connected. Hence, the homotopy equivalent
        set $\Omega$ must also be simply connected. Moreover, the
        interior $\Omega^\circ$ of $\Omega$ is the image of
        $\Omega_0^\circ\times (0,1)$ under $G$.
        
        It remains to show $f\in C^k(\Omega^\circ)$. Fix
        $x\in \Omega_0^\circ$ and $t\in (0,1)$. Then
        \begin{equation*}
          \nabla_{(x,t)}G(x,t)=\begin{pmatrix}
            \nabla_x T_t(x) & T(x) - x \\
            0 & 1
            \end{pmatrix}\in\R^{(d+1)\times (d+1)},
          \end{equation*}
          and this matrix is regular by Lemma
          \ref{lemma:spectrum}. Since
          $G\in C^k(\Omega_0\times [0,1])$, the inverse function
          theorem (see, e.g., \td{cite}) implies that $G^{-1}$ locally
          belongs to $C^k$ in a neighbourhood of $G(x,t)$. Since
          $x\in\Omega_0^\circ$ and $t\in (0,1)$ were arbitrary, we
          have $G^{-1}\in C^k(\Omega^\circ,\R^{d+1})$. Denote
          $G^{-1}=(F,E)$ such that $F:\Omega\to\R^d$ corresponds
          to the first $d$ components of $G^{-1}$.  By
          \eqref{eq:Tx-x}, for all $(y,s)\in\Omega^\circ$ holds
          $f(y,s)=T(F(y,s))-F(y,s)$, so that $f$ belongs to
          $C^k(\Omega^\circ)$ as a composition of two $C^k$ functions.}
        % Now, we
        % denote by $y = (1-t)x + tT(x)$ such that $(y,t)\in\Omega$ and
        % consider the equation $(1-t)x + tT(x) - y = 0$. The Jacobian
        % of the LHS with respect to $x$ is given by
        % $(1-t)I + t\nabla T(x) = \nabla T_t(x)$, which by our
        % assumption is invertible for all $x\in\Omega_0$. By the
        % implicit function theorem, the inverse map
        % $F(y,t):\Omega\rightarrow\Omega_0$ defined by sending
        % $y = (1-t)x + tT(x)\in\Omega$ to $x\in\Omega_0$ exists locally
        % and is also $C^k$. Since $T_t(x)$ is injective for each $t$,
        % if we define the forward map $g(x,t) = (tT(x) + (1-t)x, t)$,
        % this is bijective onto its image $\Omega$. Therefore, the
        % inverse $F(y,t)$ must exist globally on $\Omega$ and we can
        % write it as
% \begin{equation}
% f(y,t) = T(F(y,t)) - F(y,t),    
% \end{equation}
% for $(y,t)\in\Omega$. 
\end{proof}
\rrtd{whether we need to write the theorem as $f\in C^k(\Omega^\circ)$ and say it can be extended later when we prove the regularity of the domain?}
\rr{
\begin{remark}
Note that $f\in C^k(\Omega^\circ)$ only means that $f$ is $C^k$ in the interior points of $\Omega$. To show that the derivatives are well-defined on the boundary of $\Omega$ and the function $f$ can be extended to a $C^k$ function outside of $\Omega$, certain regularity conditions of the domain are required, which we will prove in later parts of this section. 
\end{remark}

}

\jz{
  The set $\Omega$ is simply connected, but unlike $\Omega_0\times [0,1]$,
  it need not be convex:

  
\begin{example}[Rotation]
  Let $\Omega_0=\set{x\in\R^2}{|x|\le 1}$ be the unit disc and let
  $T:\R^2\to\R^2$ be the rotation by $\bsalpha\in [0,2\pi)$ around
  $0\in\R^2$. Then
  \begin{equation}
    \nabla_x T(x) = \begin{pmatrix}
      \cos(\bsalpha) & -\sin(\bsalpha)\\
      \sin(\bsalpha) & \cos(\bsalpha)
      \end{pmatrix}.
    \end{equation}
    The spectrum of this matrix consists of the two values
    $\exp(\pm\ii\bsalpha)$, where $\ii$ denotes the imaginary root of
    $-1$. Thus \eqref{eq:spectrum} holds iff $\bsalpha\neq \pi$. If
    $\bsalpha=\pi$, then $T$ is the negative identity, and thus
    $T_{1/2}(x)=\frac{1}{2}x - \frac{1}{2}x=0$ for all $x\in\Omega_0$,
    so that the all straight lines connecting $x$ and $T(x)$ for
    $x\in\Omega_0$, meet at $t=\frac{1}{2}$ in the midpoint $0$ of the
    disc.  For all $\bsalpha\in [0,2\pi)\backslash\{\pi\}$, by
    Thm.~\ref{thm:f} and with $\Omega$ as in \eqref{eq:Omega}, there
    exists a vector field $f\in C^\infty(\Omega)$ such that
    $T_t(x)=X(x,t)$ for $X$ as in \eqref{eq:ODE}. One checks that
    \begin{equation*}
      \Omega = \setc{(x,t)\in\R^2\times [0,1]}{|x|\le
        \sqrt{\sin\left(\frac{\pi}{2}-\frac{\bsalpha}{2}\right)^2+\left[t\cos\left(\frac{\pi}{2}-\frac{\bsalpha}{2}\right)+(1-t)\cos\left(\frac{\pi}{2}+\frac{\bsalpha}{2}\right)\right]^2}
      },
    \end{equation*}
    \jztd{Find nicer formula, it's probably a hyperbola}
    which is convex if $\bsalpha=0$ and nonconvex
    for all $\bsalpha\in (0,2\pi)$.
\end{example}}



% For the purpose of obtaining neural network approximation results,
\jz{To approximate the velocity field $f$ from Thm.~\ref{thm:f} with a
  neural network,} we also need to understand the regularity of the
domain
$\Omega$
% :=\{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset
% \mathbb{R}^{d+1}$
on which $f$ is defined. % In
% particular, we need to know the \textit{regularity} of the boundary of
% the domain, $\partial\Omega$ so that the velocity field $f$ can be
% extended in a smooth way.
\jz{As we will see, $\Omega$ is in particular a Lipschitz domain.}
% We next recall the notion of Lipschitz
% domains and a function extension results on Lipschitz domains.

\begin{definition}
A bounded domain $\Omega$ is called a Lipschitz domain if there exist numbers $\delta > 0$, $M > 0$, \jz{$J\in\N$} and a finite cover of open sets $\{U_j\}_{j=1}^J$ of $\partial\Omega$ such that:
\begin{itemize}
\item For every pair of points $x_1$, $x_2 \in\Omega$ such that $|x_1
  - x_2| < \delta$ and $\text{dist}(x_i, \partial\Omega) < \delta$, $i
  = 1$, $2$, there exists an index $j$ such that $x_i \in U_j$ , $i =
  1$, $2$, and $\text{dist}(x_i, \partial U_j) > \delta$, $i = 1$, $2$.
\item For each
  $j$ there exists some coordinate system
  $\{\zeta_{j,1},...,\zeta_{j,d}\}$ in $U_j$ such that the set $\Omega
  \cap U_j$ consists of all points satisfying $\zeta_{j,d} \leq
  f_j(\zeta_{j,2},...,\zeta_{j,d-1})$, where
  $f_j:\mathbb{R}^{d-1}\rightarrow\mathbb{R}$ is a Lipschitz function
  with Lipschitz constant $M$.
\end{itemize}
\end{definition}

A useful result that we will use later is that convex sets are
Lipschitz domains, which can be proved using the uniform-cone characterization from \cite{ConvexLipDomain}. 

\begin{lemma}\label{lemma:convexLip}
Let $\Omega\subset\mathbb{R}^d$ be a bounded, convex and open domain, then $\Omega$ is a Lipschitz domain.
\end{lemma}
\begin{proof}
Without loss of generality, we may assume that $0\in\Omega$. Since $\Omega$ is bounded and open, there exist $r, R > 0$ such that $B(0,r)\subset \Omega\subset B(0,R)$, where $B(0,r)$ and $B(0,R)$ denote balls of radius $r$ and $R$ respectively. 

Then, we can cover the surface of ball of radius $R$ by overlapping $d-1$ dimensional balls of radius $\epsilon$ such that the boundary of each such ball, $B_{d-1}(0, \epsilon)$, is completely covered by the adjacent balls. If $\vec{n}$ denotes the unit vector emanating from the origin in the direction of the center of such a ball, then $U = \{t\vec{n} + y|t\geq 0, y\in B_{d-1}(0, \epsilon)\}$ is the cylinder of radius $\epsilon$ whose intersection with $B(0,R)$ is the boundary of this ball.

Since the surface of $B(0, R)$ can be covered by finitely many such $d-1$ dimensional balls, we can find a finite collection of such cylinders $\{U_j\}_{j=1}^J$ so that their union cover $\Omega$. From this construction of $\{U_j\}_{j=1}^J$, the first property in the definition of Lipschitz domain is clearly satisfied. 

To verify the second property, note that for each
$j$, the coordinate system is simply the map that transforms the cylinder $U_j$ to align with the direction of $e_d$ , where $e_d$ is the last vector of the
standard basis of $\mathbb{R}^d$. For any $x \in \partial\Omega\cap U_j$, the cone defined by the convex closure of $\{x\} \cup B(0,r)$ is contained in the closure of $\Omega$ and the head angle $\bsalpha$ of the cone satisfies $\sin(\frac{\bsalpha}{2}) \geq \frac{r}{R}$, and thus the boundary is a Lipschits function. 



\end{proof}


% A notion that generalizes Lipschitz domains is \textit{minimally smooth domains}, which we provide the definition below:
% \begin{definition}
% Let $\Omega\subset\mathbb{R}^d$ be an open set. Its boundary $\partial\Omega$ is said to be minimally smooth if there exists $\epsilon > 0$, an integer $N$ and $M > 0$ and a sequence of open sets $\{U_i\}_{i=1}^\infty$ such that the following hold:
% \begin{itemize}
%     \item If $x \in \partial\Omega$, then $B(x, \epsilon) \subset U_i$ for some $i$.
%     \item No point in $\mathbb{R}^d$ is contained in more than $N$ of the $U_i$'s.
%     \item For each $i$ there exists a special Lipschits domain $D_i$ whose bound does not exceed $M$ such that $U_i\cap \Omega = U_i\cap D_i$
% \end{itemize}
% \end{definition}
% \begin{remark}
% Suppose $\Omega$ is itself a Lipschitz domain, by picking $D_i$ to be $\Omega$ in the above definition, is easy to see that Lipschitz domains are trivially minimally smooth domains. 
% \end{remark}

\jztd{removed technical notion of ``minimally smooth domains'', since
  we don't need it} For Sobolev functions on \jz{Lipschitz}
domains, we have the following extension theorem:
\begin{theorem}[{\cite[Chp 3]{SteinBook}}]\label{thm:functionExtension}
  Let $D \subset\mathbb{R}^d$ be a \jz{Lipschitz} domain\footnote{\jz{The result in \cite{SteinBook} is stated in terms of so-called ``minimally smooth domains'', which is a generalization of the notion of Lipschitz domains.}}.
  % domain of minimally smooth boundary.
Then there exists a linear operator $\mathcal{E}$ mapping functions on $D$ to functions on $\mathbb{R}^d$ with the following properties:
\begin{itemize}
    \item $\mathcal{E}(f)|_D = f$, that is, $\mathcal{E}$ is an extension operator.
    \item $\mathcal{E}$ maps $W^{k,p}(D)$ continuously into
      $W^{k,p}(\mathbb{R}^d)$ for all $1\leq p\leq \infty$ and \rr{all nonnegative}
      integer $k$.\jztd{Negative $k$ is allowed?}
      
\end{itemize}
\end{theorem}

\jz{The image of a Lipschitz domain under a sufficiently regular map
remains a Lipschitz domain:}
\begin{theorem}[{\cite[Thm 4.1]{LipTransformation}}]\label{thm:LipTransformation}
  Assume $\Omega \subset\mathbb{R}^d$ is a \rr{bounded Lipscthiz domain} and
  $\mathcal{O}$ is an open neighborhood
  \jztd{This would mean that $\mathcal{O}=\Omega$ is allowed; that's probably wrong; Also I suggest not to switch notation and write again $D\subset\R^d$ as in the previous theorem instead
  of $\Omega\subset\R^n$}
  of \rr{$\bar{\Omega}$} and $f:\mathcal{O}\rightarrow \mathbb{R}^n$ is a $C^1$-diffeomorphism onto its image. Then, $\tilde{\Omega} = f(\Omega)$ is also a Lipschitz domain.
\end{theorem}

Now we show that $\Omega$ is a Lipschitz
domain:
\begin{theorem}\label{thm:LipDomain}
  \jz{Consider the setting of Thm.~\ref{thm:f}.
    Then $\Omega\subset\R^{d+1}$ in \eqref{eq:Omega} is a Lipschitz domain.}
  % In the context of Theorem \ref{thm:f},  
% the domain of the velocity field $f$, or the subset in space-time covered by the straight line trajectories of the ODE dynamics,  $\Omega:=\{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset \mathbb{R}^{d+1}$, is a Lipschitz domain. 
\end{theorem}
\begin{proof}
  \jztd{Use the same notation as in the proof of Thm.~\ref{thm:f}.}
  
  
\rr{To show $\Omega$ is a Lipschitz domain, first we observe that $\Omega$ is the image of $\Omega_0\times [0,1]$ under the map $(x,t)\rightarrow G(x,t) := (tT(x) + (1-t)x, t)$ for $x\in\Omega_0,t\in[0,1]$. Since $\Omega_0\times [0,1]$ is a product of two convex sets, which is convex, lemma \ref{lemma:convexLip} shows that the cylinder $\Omega_0\times [0,1]$ is a Lipschitz domain. To apply theorem \ref{thm:LipTransformation}, we need to find a $C^1$-diffeomorphism from an open neighborhood $\mathcal{O}$ of $\Omega_0\times [0,1]$ onto its image. In the context of theorem \ref{thm:f}, we have $\det(\nabla_x T_t(x)) > 0$ for all $(x,t)\in\Omega_0\times[0,1]$. Since $\Omega_0\times[0,1]$ is a compact set, the infimum of the continuous function $(x,t)\rightarrow\det(\nabla_x T_t(x))$ is achieved at some point in the set and thus we can conclude that $\inf_{(x,t)\in\Omega_0\times[0,1]}\det(\nabla_x T_t(x)) > 0$. 



On the other hand, since $T\in C^k(\Omega_0, \mathbb{R}^d)$ for some $k\geq 2$, it follows that $T \in W^{k, \infty}(\Omega_0, \mathbb{R}^d)$. By the extension theorem \ref{thm:functionExtension}, $T$ can be extended to a function $\tilde{T}\in W^{k, \infty}(\mathbb{R}^d, \mathbb{R}^d)$. Since $k \geq 2$, Sobolev embedding shows that $\tilde{T}\in C^1(\mathbb{R}^d, \mathbb{R}^d)$. Now consider the map $\tilde{T}_t(x) = t\tilde{T}(x) + (1-t)x$ for $(x,t)\in\mathbb{R}^{d+1}$. It is clear that $\tilde{T}_t(x)$ is $C^1$ in $(x,t)$ and also $\tilde{T}_t(x)|_{\Omega_0\times[0,1]} = T_t(x)$. By the continuity of determinant operator and $\inf_{(x,t)\in\Omega_0\times[0,1]}\det(\nabla_x T_t(x)) > 0$, it follows that there exists an open neighborhood $\mathcal{O}\subset\mathbb{R}^{d+1}$ of $\Omega_0\times[0,1]$ such that $\det(\nabla_x \tilde{T}_t(x)) > 0$ for all $x\in\mathcal{O}$. Without loss of generality, we can assume $\mathcal{O}$ is convex. This is because we can consider the neighborhood $\Omega_0\times[0,1]\cup \{B_\epsilon((x,t))|(x,t)\in\partial(\Omega_0\times[0,1])\}$. This is an open and convex set that can be made arbitrarily close to $\Omega_0\times[0,1]$ when $\epsilon\rightarrow 0$. 

Now, consider the extension of $G$, $\tilde{G}(x,t) = (t\tilde{T}(x) + (1-t)x, t)$ for $(x,t)\in\mathcal{O}$. We have 
\begin{equation*}
          \nabla_{(x,t)}\tilde{G}(x,t)=\begin{pmatrix}
            \nabla_x \tilde{T}_t(x) & \tilde{T}(x) - x\\
            0 & 1
            \end{pmatrix}\in\R^{(d+1)\times (d+1)}
\end{equation*}
is a regular matrix for fix $(x,t)\in\mathcal{O}$. Then, the same arguments as in the proof of theorem \ref{thm:f} shows that $\tilde{G}(x,t)$ has a global inverse and $\tilde{G}^{-1}$ is $C^1$. Therefore, we have a $C^1$-diffeomorphism from $\mathcal{O}$ onto its image and theorem \ref{thm:LipTransformation} shows that $\Omega = \{(T_t(x),t)\}$ for $x\in\Omega_0$, $t\in[0,1]$ is a Lipschitz domain. 

\begin{remark}
Thm \ref{thm:LipDomain} shows the domain of $f$ from Thm \ref{thm:f}, $\Omega\subset\mathbb{R}^{d+1}$, is a Lipschitz domain. Thm \ref{thm:f} shows that $f$ is $C^k$ on $\Omega^\circ$, so extension result in Thm \ref{thm:functionExtension} shows that $f$ can be extended to a function in $W^{k, \infty}$ on the entire $\mathbb{R}^{d+1}$.
\end{remark}





}
\jz{[I think the first part of the proof is not necessary, and the
  last paragraph is not written very clear, so the whole proof has to
  be revised. Some comments on what the problems in the last paragraph are:
  \begin{itemize}
  \item 
    \begin{quote}
      ``by continuity, we have $\det\nabla T_t(x) > 0$ still holds in some larger open neighborhood $\mathcal{O}$ of $\Omega_0\times[0,1]$.''      
    \end{quote}
    By definition, the domain of $(x,t)\mapsto T_t(x)$ is
    $\Omega_0\times [0,1]$, hence $\det \nabla T_t$ is not defined on
    $\mathcal{O}$. I assume the idea was to first extend $T_t$ (?)
  \item
    \begin{quote}
      ``Therefore, $g$ can be continuously extended to a map $\tilde{g}$ on $\mathcal{O}$ that is bijective onto its image by theorem \ref{thm:functionExtension}.''
      \end{quote}
      Theorem \ref{thm:functionExtension} does not imply
    bijectivity of the extension. I assume the
    idea was to apply Lemma \ref{lemma:Tinjective} (?)
  \item
    \begin{quote}
      ``Therefore, $g$ can be continuously extended to a map $\tilde{g}$ on $\mathcal{O}$ that is bijective onto its image by theorem \ref{thm:functionExtension}.''
      \end{quote}
      Theorem \ref{thm:functionExtension} is also not a statement
      about the extension of continuous functions. It is a statement
      about the extension of Sobolev functions. First think about the
      assumption that we have on $T_t$. Then carefully check
      whether it fits the assumptions of the theorem you wish to
      apply, in this case Thm.~\ref{thm:functionExtension}. If not,
      you either need to change your assumption on $T_t$, or
      you need to say something about how exactly Theorem
      \ref{thm:functionExtension} is applied here.
    \item
      \begin{quote}
        ``Moreover, the inverse function theorem gives that the inverse of $\tilde{g}$ still exists and is differentiable.''
      \end{quote}
      If $\tilde g$ is only a continuous extension, the
      inverse $\tilde g$ will in general not be differentiable.
  \end{itemize}
]}

\end{proof}

In the theorem below, we show that the straight line connections
between \jz{$x$} and $T(x)$ for a transport map $T$ that pushes forward $\pi$ to $\rho$ yields the minimal
average kinetic energy\jztd{introduce this notion} along the trajectories, which is why we name the construction \textit{minimal energy regularization}.





\rr{
  \begin{theorem}\label{thm:minimalenergy}
    Let $\Omega_0, \Omega_1\subseteq\R^d$ with $\Omega_0$ being convex and let $T\in C^1(\Omega_0,\Omega_1)$
    satisfy \eqref{eq:spectrum}. Moreover, assume $\pi$ and $\rho$ are probability densities on $\Omega_0$, $\Omega_1$ respectively and $T_\sharp\pi = \rho$
    Then with


\begin{equation*}
      \mathcal{H}:=\set{g\in \mathcal{V}\eqref{eqn:VelocityField}}{X_g(\cdot,1)|_{\Omega_0}=T}
    \end{equation*}
    and $f$ from Thm.~\ref{thm:f} holds
    \begin{equation*}
      f = \argmin_{g\in\mathcal{H}}\int_{\mathbb{R}^d}\int_0^1\eta_g(x,t)|g(x,t)|^2\dd t\dd x.
    \end{equation*}
  
  
 \end{theorem}
}




\rrtd{This notation has already been added to the notations section}

\begin{proof}
By theorem \ref{thm:f}, we know the existence of velocity fields that realize these constructions. We then \rr{bound from below} the average kinetic energy using Lagrangian coordinates as follows:
\begin{align*}\label{OP}
&\int_{\mathbb{R}^d} \int_0^1 \eta_g(x,t)|g(x,t)|^2dtdx = \int_{\Omega_0} \int_0^1 \eta_g(x,0)|g(X(x,t),t)|^2dtdx\\
&=\int_{\Omega_0} \int_0^1 \pi(x)|\partial_t X(x,t)|^2dtdx \geq \int_{\Omega_0} \pi(x)( \int_0^1|\partial_t X(x,t)|dt)^2dx 
&\geq \int_{\Omega_0} \pi(x)|X(x,1)- X(x,0)|^2dx\\
&= \int_{\Omega_0} \pi(x)|X(x,1)-x|^2dx = \int_{\Omega_0} \pi(x)|T(x) -x|^2dx,
\end{align*}
\jztd{Integration domain should be $\Omega_0$} where the inequality is
due to Jensen's inequality, and equality holds \rr{iff}\jztd{Is it if and
  only if? In case not, the statement should instead read
  ``$f\in\argmin ...$''}
$\partial_t X(x,t) = X(x,1) - X(x,0) = T(x) - x$. Then the optimal
choice of $X$ is given by $X(x,t) = x + t(T(x) - x)$, which is exactly
the displacement interpolation. As a
result, the optimal choice for $f$ is given by
$f(X(x,t), t) = T(x) - x$, which is the straight line construction from Thm.\ref{thm:f}.
\end{proof}
\begin{remark}
Note that this construction has deep connections to the computational fluid dynamics (CFD) formulation of optimal transport (\cite{OT-CFD}). We showed that for fixed transport map $T(x)$, the straight line construction gives the minimal average kinetic energy. The optimal transport map $T$ is then just the transport map $T$ that minimizes $\int_{\mathbb{R}^d} \pi(x)|T(x) -x|^2dx$, which is the $L^2$-Wasserstein metric. 
\end{remark}


With the machinery developed, we are now ready to prove that under reasonable assumptions about the measures $\pi$ and $\rho$, \eqref{eq:OP} admits optimal solutions that take the form of displacement interpolation between a transport map $T$. 


\begin{theorem}\label{thm:ExistenceOfOptimalSolution}
Let $\pi$, $\rho$\jztd{unify notation: either $\pi_0$, $\pi_1$ throughout or $\pi$, $\rho$ throughout} be measures supported on $\Omega_0$ and $\Omega_1$ respectively. Assume that the measures the domains satisfy assumption \ref{AssumptionDensity1}. Then all the optimal velocity fields to the optimization problem \eqref{eq:OP} take the form $f(y,t) = T(x) - x$ for some transport map $T$ such that $T_\sharp\pi = \rho$, where $(y,t)\in\Omega:=\{(T_t(x),t):x\in\Omega_0,t\in[0,1]\}\subset \mathbb{R}^{d+1}$ and $T$ satisfies \eqref{eq:spectrum}. Moreover, there exists at least one such velocity field, corresponding to the Knothe--Rosenblatt transport map that achieves the optimal value of \eqref{eq:OP}.
\end{theorem}
\begin{proof}
Clearly, the objective function is bounded from below by zero. We first show that the velocity field corresponding to the Knothe--Rosenblatt transport map achives this optimal value. The explicit construction of Knothe--Rosenblatt is shown in appendix \ref{sec:KRMap}. We comment that the construction given in appendix \ref{sec:KRMap} is only for measures supported on the hypercube $[0,1]^d$; however, similar construction still works for all convex and compact sets under our consideration. For simplicity of argument, we only work with the hypercube here. 

Let $T(x)$ be the K-R map constructed. By the construction, its Jacobian $\nabla T$ is a triangular matrix. Now, we compute the diagonal entries of the Jacobian matrix, which are the eigenvalues. Let $G_{\rho, k}(x_{[k-1]}, \cdot) = F_{\rho,k}(x_{[k-1]}, \cdot)^{-1}$, where $F_{\rho,k}$ and $F_{\pi,k}$ are introduced in appendix \ref{sec:KRMap}. \jztd{notation has not been introduced} Then we have $T_k(x) = G_{\rho, k}(T_1(x_1),...,T_{k-1}(x_{[k-1]}), F_{\pi, k}(x))$. Taking derivatives in $x_k$, we have 
$$\partial_{x_k}T_k(x) = \partial_{x_k}G_{\rho, k}(T_1(x_1),...,T_{k-1}(x_{[k-1]}), \pi_k(x))\partial_{x_k}F_{\pi,k}(x).$$

Recall that $F_{\pi,k}(x)$ is the CDF of $x_k$ when viewing $x_{[k-1]}$ as fixed, thus we have $\partial_{x_k}F_{\pi,k}(x) = \pi_k(x)$. Note that $G_{\rho,k}(x_{[k-1]}, F_{\rho,k}(x)) = x_k$. Taking derivative in $x_k$, we have 
$$(\partial_{x_k}G_{\rho,k})(x_{[k-1]}, F_{\rho,k}(x))(\partial_{x_k}F_{\rho,k}(x)) = 1.$$

Note that $F_{\rho,k}(x_{[k-1]}, \cdot):[0,1]\rightarrow[0,1]$ is a bijection. We make the substitution $y_k = F_{\rho,k}(x)$ and we have $\forall (x_{[k-1]}, y_k)\in[0,1]^{k-1}\times[0,1]$, 
$$(\partial_{x_k}G_{\rho,k})(x_{[k-1]}, y_k) = \frac{1}{\partial_{x_k}F_{\rho,k}(x_{[k-1]}, G_{\rho,k}(x_{[k-1]}, y_k))}.$$

Hence, we obtain
$$\partial_{x_k}T_k(x) = \frac{\pi_k(x)}{\rho_k(T_1(x_1),...,T_{k-1}(x_{[k-1]}), G_{\rho,k}(T_1(x_1),...,T_{k-1}(x_{[k-1]}),F_{\pi,k}(x)))}.$$

Since $\rho_k$ and $\pi_k$ are both density functions, and recall from our assumptions that the densities are bounded from below by a constant $L$, thus we have $\partial_{x_k}T_k(x) > L, \forall x\in\Omega_0, k\in\{1,2,...,d\}$. That is, we have $\sigma(\nabla T(x)) \cap (-\infty, 0] = \emptyset$. By lemma \ref{lemma:spectrum}, we have $\det T_t(x) > 0$ $\forall x\in\Omega_0$ and $t\in[0,1]$. Then, theorem \ref{thm:f} establishes the existence of a velocity field such that the time-one flow map of the ODE realizes $T(x)$ and the ODE dynamics yield straight line trajectories. 

 Now, suppose that there is a continuous velocity field $f$ that achieves zero loss. Since the densities are continuous and bounded from below by a constant, the expectation $\mathbb{E}[R(x,1)] = 0$ implies that $R(x,1) = 0 \forall x\in\Omega_0$.  That is, along each trajectory starting from $x\in\Omega_0$, we have $\frac{\partial f(X(t),t)}{\partial t} = 0$, which implies that $f$ is constant in time along all the trajectories. That is, $f(X(t),t) = g(x)$ for some function $g$ of $x$. Consider the flow map of the ODE $\frac{dX}{dt} = f(X(t),t) = g(x)$, $X(t) = g(x)t + C$, where $C$ is constant in $t$. To make the KL divergence zero, $X(1) = T(x)$ for some transport map $T$ such that $T_\sharp\pi = \rho$, and we also have $X(0) = x$ as the initial condition. Solving the equations gives $g(x) = T(x) - x$. That is, the velocity field must take the form $T(x) - x$ for some transport map $T$.
\end{proof}




\subsection{% Establishing bounds on the $C^k$ norm
  \jz{Regularity}
  of the velocity field}
Recall from Thm \ref{thm:f} that the velocity field $f(y,t)$ can be written as $f(y,t) = T(F(y,t)) - F(y,t)$ for $(y,t)\in\Omega$, where $(y,t) \mapsto F(y,t)$ is the projection to the first $d$-coordinates of the inverse of the forward map $G(x,t) = (tT(x) + (1-t)x, t)$ for $x\in\Omega_0, t\in[0,1]$. Note $F:\Omega \subset\mathbb{R}^{d+1} \rightarrow \mathbb{R}^d$, and $T:\Omega_0\subset\mathbb{R}^d \rightarrow\mathbb{R}^d$ can be written as $T(F(y,t)) = (T^1(F(y,t)), ..., T^d(F(y,t))),$ where $T^i(F(y,t))$ can be further expressed as $T^i(F(y,t)) = T^i(F_1(y,t),...,F_d(y,t))$ for each $i$.

Recall that the partial derivatives can also be interpreted as multi-linear tensor operators in general Banach spaces. To establish bounds on $\|F\|_{C^k}$ and $\|f\|_{C^k}$, we shall work in this more general context. We relate the definitions of $C^k$ norm in this abstract setting to the classical definition on Euclidean space through the following lemma:



\begin{lemma}\label{lemma:relationBetweenNorms}
Let $f$ be a function $f: S\subseteq\mathbb{R}^d\rightarrow\mathbb{R}$, then we have the inequality $$\max_{|\bsalpha|\leq k} \|D^\bsalpha f\|_{L^\infty(S)} \leq \|f\|_{C^k(S)} \leq d^k\max_{|\bsalpha|\leq k}\|D^\bsalpha f\|_{L^\infty(S)},$$ where $\bsalpha$ is a multi-index of length $|\bsalpha|$. 
\end{lemma}

\begin{proof}
Let $(e_{i_1}, ..., e_{i_n})$ be a set of vectors in $\mathbb{R}^n$, where each $e_{i_l}$ is a standard vector in $\mathbb{R}^d$, then for any $n\leq k$, $[D^nf(x)](e_{i_1}, ..., e_{i_n}) = \frac{\partial f(x)}{\partial x_{i_1},..., \partial x_{i_n}}$. For any differentiating multi-index $\bsalpha$ such that $|\bsalpha| = n$ and fixed $x\in S$, we can always choose the corresponding set $(e_{i_1}, ..., e_{i_n})$ such that $[D^nf(x)](e_{i_1}, ..., e_{i_n}) = D^\bsalpha f(x)$.Therefore, it is easy to see that $\forall \bsalpha$, $$\|D^\bsalpha f\|_{L^\infty(S)} = \sup_{x\in S}|D^\bsalpha f(x)| \leq \|D^{|\alpha|}f\|_{L^\infty(S, L^{|\alpha|}(\mathbb{R}^d,\mathbb{R}^d))} = \sup_{x\in S}\|D^{|\alpha|}f(x)\|_{L^{|\alpha|}(\mathbb{R}^d,\mathbb{R}^d)}.$$ Taking the $\sup$ over all the multi-indices $\bsalpha$ such that $|\bsalpha| \leq k$ gives $\max_{|\bsalpha|<k} \|D^\bsalpha f\|_{L^\infty(S)} \leq \|f\|_{C^k(S)}.$


To prove the other direction of the inequality, note that for fixed $n \leq k$, we can write $$\|D^nf\|_{L^\infty(S, L^{|\alpha|}(\mathbb{R}^d,\mathbb{R}^d))} = \sup_{x\in S}\|D^n f(x)\|_{L^{|\alpha|}(\mathbb{R}^d,\mathbb{R}^d)} = \sup_{x\in S}\sup_{|v_i|\leq 1}|D^\bsalpha f(x)(v_1,v_2,...,v_n)|.$$ Let $(e_{1}, e_{2},...,e_{d})$ be the standard basis in $\mathbb{R}^d$, then, we may write $v_i = \sum_{j}a_{ij}e_j$ for each vector $v_i$. By the multi-linear property of tensors and the fact that $a_{ij} \leq 1 \forall i,j$, we have the inequality 

$$\sup_{x\in S}\sup_{|v_i|\leq 1}|D^n f(x)(v_1,v_2,...,v_n)| \leq d^n\sup_{x\in S}\max_{|\bsalpha| = n}|D^\bsalpha f(x)| $$


Since this holds for all $n$ and $|\bsalpha| = n$, we can conclude that $\|f\|_{C^k} \leq d^k\max_{|\bsalpha|<k} \|D^\bsalpha f\|_{L^\infty(S)}$.
\end{proof}

\begin{remark}
For $f:S\subseteq\mathbb{R}^d\rightarrow\mathbb{R}$, $\|D^2f\|_{L^\infty(S, L^{2}(\mathbb{R}^d,\mathbb{R}))}$ corresponds to the supremum of the operator norm of the Hessian matrix over all $x\in S$, while the quantity $\max_{|\bsalpha| = 2}\|D^\bsalpha f(x)\|_{L^\infty(S)} $ corresponds to the maximum absolute value entry of the Hessian matrix over all $x\in S$. 
\end{remark}

To establish bounds on the $C^k$ norm of $f(y,t) = T(F(y,t)) - F(y,t)$, we first establish bounds on the $C^k$ norm of composition of velocity fields $T(F(y,t))$. Note that taking partial derivatives with respect to a multi-index in this composition of multivariate functions invokes the chain rule recursively. This is summarized in the Fa\'{a} di Bruno's formula (\cite[Thm 2.1]{FaadiBruno}). 

\begin{theorem}\label{thm:FaadiBruno}
[Fa\'{a} di Bruno's formula, multivariate version] Let $h(x_1,...,x_d) = f(g^1(x_1,...,x_d),.., g^m(x_1,...,x_d))$ be a composition of a function $f:\mathbb{R}^m \rightarrow \mathbb{R}$ with a vector field $g = (g^1, ..., g^m): \mathbb{R}^d \rightarrow \mathbb{R}^m$. Let $\bsv = (v_1,...,v_d)$ be a multi-index such that $|\bsv| = n$ and $x_0\in\mathbb{R}^d$ be given and $y_0 = (g_1(x_0),...., g_m(x_0))$. Setting $h_{\bsv} = D^\bsv_xh(x_0), f_\bslambda = D^\bslambda_y f(y_0), g^i_\bsmu = D^\bsmu_xg^i(x_0)$ and $g_\bsmu = (g^1_\bsmu, ..., g^m_\bsmu)$, then we have the following:
$$h_\bsv = \sum_{1\leq |\bslambda| \leq n}f_\bslambda\sum_{p(\bsv,\bslambda)} (\bsv!)\prod_{j=1}^n\frac{[g_{\bsl_j}]^{\bsk_j}}{(\bsk_j!)[\bsl_j!]^{|\bsk_j|}},$$ where $p(\bsv,\bslambda)$ denotes the set $\{(\bsk_1,...\bsk_n;\bsl_1,...,\bsl_n)|\text{ there exists some s}, 1\leq s\leq n, \bsk_i = 0 \text{ and } \bsl_i = 0 \text{ for } 1\leq i \leq n-s; |\bsk_i| > 0 \text{ for } n-s+1 \leq i \leq n; \text{ and } 0\prec \bsl_{n-s+1}\prec...\prec \bsl_n. \text{ Moreover, } \sum_{i=1}^n\bsk_i = \bslambda, \sum_{i=1}^n|\bsk_i|\bsl_i = \bsv \}$. Note that $\bsk_i$'s are multi-indices in $\mathbb{N}^m$ and $\bsl_i$'s are multi-indices in $\mathbb{N}^d$. The notation $[g_{\bsl_j}]^{\bsk_j}$ is an abbreviation for $\prod_{i=1}^m(g^i_{\bsl_j})^{(\bsk_j)_i}$.
\end{theorem}
A very useful result that relates the combinatorial sum in the multivariate Fa\'{a} di Bruno's formula to the Stirling number of the second kind is the following:
\begin{lemma}\label{lemma:ComboSum}
If $1 \leq k \leq n = |\bsv|$, then $$(\bsv!)\sum_{|\bslambda| = k}\sum_{p(\bsv,\bslambda)}\prod_{j=1}^n\frac{1}{(\bsk_j!)[\bsl_j!]^{|\bsk_j|}} = m^kS_n^k,$$ where $S_n^k$ is the Stirling number of the second kind and is defined as the number of ways to partition a set of $n$ labelled objects into $k$ non-empty unlabelled subsets.
\end{lemma}

For Stirling number of the second kind, the following upper bound holds:
\begin{lemma}\label{lemma:StirlingUpperbound}
$S_n^k \leq \frac{n^k}{k!}$ for all integers $k\leq n$.
\end{lemma}
\begin{proof}
The proof is through a basic combinatorial counting argument: recall that $S_n^k$ is the number of ways to partition a set of $n$ distinct objects into $k$ non-empty indistinguishable subsets. $k!S_n^k$ is then the number of ways to place $n$ distinct objects into $k$ distinct boxes such that no box is empty. On the other hand, $n^k$ counts all the ways to place $n$ objects into $k$ distinct boxes. It clearly holds that $k!S_n^k \leq n^k$. 
\end{proof}




Now, we are ready to prove the results on the composition of velocity fields.


\begin{theorem}\label{thm:compositionVelocityFieldNorm}
Suppose $T = [T^1, ..., T^d]^T:\Omega_0\subset\mathbb{R}^d\rightarrow\Omega_1\subset\mathbb{R}^d$ is a $C^k$ map such that $\|T\|_{C^k(\Omega_0)} \leq M$ and $F:\Omega\subset\mathbb{R}^d\times\mathbb{R}\rightarrow\Omega_0$ is also a $C^k$ map. Then for any multi-index $\bsv\in\mathbb{N}_0^{d+1}$ such that $|\bsv| = n \leq k$ and $i$, we have $\|D^\bsv_{y,t}[T^i\circ F]\|_{L^\infty(\Omega)} \leq Me^{nd\max_{|\bsalpha|<k}\|D^\bsalpha F\|_{L^\infty(\Omega)} } $. Using $C^k$ norm, we have $\|T^i\circ F\|_{C^k(\Omega)} \leq d^kMe^{nd\|F\|_{C^k(\Omega)}}$. 


\end{theorem}
\begin{proof}
By the multivariate version of Fa\'{a} di Bruno's formula (Thm. \ref{thm:FaadiBruno}), for a given multi-index $\bsv, |\bsv| = n$, we have $$D^\bsv_{y,t}T^i(F(y,t)) = \sum_{1\leq |\bslambda| \leq n}T^i_\bslambda\sum_{p(\bsv,\bslambda)} (\bsv!)\prod_{j=1}^n\frac{[F_{\bsl_j}]^{\bsk_j}}{(\bsk_j!)[\bsl_j!]^{|\bsk_j|}},$$ where $p(\bsv,\bslambda)$ is the set defined in Thm.\ref{thm:FaadiBruno}. 

The goal is to upper bound the $\sup_{(y,t)\in\Omega}|D^\bsv_{y,t}T^i(F(y,t))| = \|D^\bsv_{y,t}T^i(F(y,t))\|_{L^\infty(\Omega)}$ for each $i$ and all multi-indices $v$ such that $|v| = n < k$. 




By the formula, we have $$\|D^\bsv_{y,t}[T^i\circ F]\||_{L^\infty(\Omega)} \leq \sum_{1\leq |\bslambda| \leq n}(\|T^i_\bsv\|_{L^\infty(\Omega_0)})\sum_{p(\bsv,\bslambda)} (\bsv!)\prod_{j=1}^n\frac{\|[F_{\bsl_j}]^{\bsk_j}\|_{L^\infty(\Omega)}}{(\bsk_j!)[\bsl_j!]^{|\bsk_j|}}.$$ By the assumption that $\|T\|_{C^k(\Omega_0)}\leq M$ and lemma \ref{lemma:relationBetweenNorms}, we have $\max_{|v|\leq k} \|T^i_v(x)\|_{L^\infty(\Omega_0)}\leq M, \forall i.$ Moreover, we can compute $\prod_{j=1}^n[F_{\bsl_j}]^{\bsk_j} = \prod_{j=1}^n[F^1_{\bsl_j},...,F^d_{\bsl_j}]^{\bsk_j} = \prod_{j=1}^n\prod_{i=1}^d(F^i_{\bsl_j})^{(\bsk_j)_i}$. Since $\|F^i_{\bsl_j}\|_{L^\infty(\Omega)} \leq \|F\|_{C^k(\Omega)}$ as long as $|\bsl_j| \leq k$, we have 
$$\|[F_{\bsl_j}]^{\bsk_j}\|_{L^\infty(\Omega)} \leq \prod_{j=1}^n \|F\|_{C^k(\Omega)}^{|\bsk_j|} \leq  \|F\|_{C^k(\Omega)}^{|\bslambda|}.$$

Therefore, we have 
$$\|D^\bsv_{y,t}[T^i\circ F]\|_{L^\infty(\Omega)} \leq M \sum_{s = 1}^n\|F\|_{C^k(\Omega)}^{s}(\bsv!)\sum_{|\bslambda| = s}\sum_{p(\bsv,\bslambda)}\prod_{j=1}^n\frac{1}{(\bsk_j!)[\bsl_j!]^{|\bsk_j|}} =  M \sum_{s = 1}^n\|F\|_{C^k(\Omega)}^{s}d^sS_n^s.$$

Lemma \ref{lemma:StirlingUpperbound} shows that $S_n^s \leq \frac{n^s}{s!}$, and we then have $$\|D^\bsv_{y,t}[T^i\circ F]\|_{L^\infty(\Omega)} \leq M\sum_{s=1}^n \|F\|_{C^k(\Omega)}^{s}d^s\frac{n^s}{s!} \leq M\sum_{s=1}^\infty \|F\|_{C^k(\Omega)}^{s}d^s\frac{n^s}{s!} = Me^{nd\|F\|_{C^k(\Omega)}}.$$

Since this holds for all $|\bsv| \leq k$, we have $\max_{|\bsv|\leq k} \|D^\bsv_{y,t}T^i(F(y,t))\|_{L^\infty(\Omega)} \leq Me^{nd\|F\|_{C^k(\Omega)}}.$ Multiplying $d^k$ on both sides and using lemma \ref{lemma:relationBetweenNorms}, we get 
$$\|T^i\circ F\|_{C^k(\Omega)} \leq d^k\max_{|\bsv|\leq k} \|D^\bsv_{y,t}[T^i\circ F]\|_{L^\infty(\Omega)} \leq d^kMe^{nd\|F\|_{C^k(\Omega)}} $$
\end{proof}

Now it remains to bound $\|F\|_{C^k(\Omega)}$. As noted before, $F(y,t)$ is the projection of the inverse of the forward map $G(x,t) = (tT(x) + (1-t)x, t)$ to the first $d$-coordinates, which is determined by the map $T$. As a general result, we shall derive the $C^k$ norm of the inverse of an operator based on the $C^k$ of the operator self. For this purpose, we find it easier to work in the more general setting of Banach spaces. Before we proceed, however, we shall prove the Banach space version of the Fa\'{a} di Bruno's formula. We note that the proof is essentially similar to the proof for Fa\'{a} di Bruno's formula in the case of a single real variable and is based on Taylor expansions using Fr\'echet derivatives in Banach spaces.     

\begin{theorem}\label{thm:faadiBrunoBanach}
(Fa\'{a} di Bruno's formula, Banach space version) Let $g:B_1\rightarrow B_2$ and $f: B_2\rightarrow B_3$ be $C^k$ functions on Banach spaces, then we have for $n < k$

$$[D^n(f\circ g)](y) = \sum_{\bsk\in T_n}\frac{n!}{k_1!\ldots k_n!}[D^{|\bsk|}f](g(y)) \left(\smash[b]{\underbrace{\frac{[Dg](y)}{1!},\ldots, \frac{[Dg](y)}{1!}}_\text{$k_1$ times}},\ldots,\smash[b]{\underbrace{\frac{[D^{n}g](y)}{n!},\ldots, \frac{[D^{n}g](y)}{n!}}_\text{$k_n$ times}}\right),$$ where $T_n = \{\bsk = (k_1,\ldots,k_n) : \sum_{j=1}^n jk_j = n\}$. 
\end{theorem}
\begin{remark}
To simplify the notations, we may use $[D^ng](0)\prod_{j=1}^nx_j$ to indicate $([D^ng](0))(x_1,...,x_n)$.
%where the product is to be interpreted as having $n$-copies in the argument to the multi-linear operator. 
Also, when $\bsk$ is a tuple, we may use ${n \choose \bsk}$ to denote the combinatorial number $\frac{n!}{k_1!\ldots k_n!}$.
\end{remark} 
\begin{proof}
Without loss of generality, we may assume that $g(0) = 0$. By the Taylor expansion in Banach space, we have 
$$f\circ g(x) =  \sum_{r=1}^k\frac{[D^rf](g(x))}{r!}(\smash[b]{\underbrace{g(x),\ldots, g(x)}_\text{r times}}) + o(\|g(x)\|_{B_2}^k).$$

Taylor expanding $g(x)$ in each of the argument then gives 

$$f\circ g(x) =  \sum_{r=1}^k\frac{[D^rf](g(x))}{r!}(\smash[b]{\underbrace{\sum_{n_1=1}^k\frac{[D^{n_1}g](0)}{n_1!}(\smash[b]{\underbrace{x,\ldots,x}_\text{$n_1$ times}}) + o(\|x\|_{B_1}^k),\ldots, \sum_{n_r= 1}^k\frac{[D^{n_r}g](0)}{n!}(\smash[b]{\underbrace{x,\ldots,x}_\text{$n_r$ times}})+
o(\|x\|_{B_1}^k))}_\text{r times}} + o(\|g(x)\|_{B_2}^k).$$

\newline





Note that $o(\|g(x)\|_{B_2}^k) = o(\|Dg(x)\|_{L^1(B_1, B_2)}^k\|x\|_{B_1}^k) = o(\|x\|_{B_1}^k)$.


Recall that these partial differential operators are multi-linear, we can pull out the terms in the argument of $[D^rf](g(x))$ to get 





$$f(g(x)) = \sum_{r=1}^k\sum_{n_1+\ldots+n_r = n}\frac{[D^rf](0)}{r!}\prod_{j=1}^r\frac{[D^{n_j}g](0)x^{n_j}}{n_j!} + \text{remainder terms} $$

On the other hand, we can Taylor expand the function $f\circ g$. That is, we have 
$$f(g(x)) = \sum_{n = 1}^k\frac{[D^n(f\circ g)(0)]}{n!}x^n + o(\|x\|_{B_1}^k). $$

Comparing the powers of $x$, we get 
$$D^n(f\circ g)(0) = \sum_{n_1+\ldots+n_r = n}\frac{n![D^rf](0)}{r!}\prod_{j=1}^r\frac{[D^{n_j}g](0)x^{n_j}}{n_j!},$$ where the summation is over the partition of a set of $n$ elements into $r$ subsets each of $n_r$ elements such that $r \geq 1, n_r\geq 1$. Note that if we let $T_n = \{\bsk = (k_1,\ldots,k_n) : \sum_{j=1}^n jk_j = n\}$, each $k_j$ can be interpreted as the number of subsets with $j$ elements and $|\bsk| = \sum_j k_j = r$ is the number of non-empty subsets. Based on this observation, it is easy to see that the expression we derived above is equivalent to the one stated in the theorem.
\end{proof}


In the below theorem, we show that if $g$ is the inverse of $f$ in Banach spaces, then the $C^k$ norm of $g$ can be bounded by a continuous function of the $C^k$ norm of $f$ and the norm of the inverse of the first order differential $Df^{-1}$. The proof technique for the theorem is based on an inductive argument on the Fa\'{a} di Bruno's formula in Banach spaces. 



\begin{theorem}\label{thm:NormInverse}
Let $X,Y$ be Banach spaces and $f\in C^k(S,Y)$ for $S\subset X$ open, where $k$ may be infinity. Let $W$ be the image of $f$ in $Y$. Then for all $n \leq k $, there exists a continuous function $C_n: \mathbb{R}^{n+1}\rightarrow \mathbb{R}^+$ independent of $f$ such that for all $x\in S$ for which $Df(x)$ is an isomorphism, there is an open set $B\subset W$ with $f(x)\in B$ and a function $g: B\rightarrow \Theta$ such that $f(g(y)) = y$ for all $y\in B$ and $$ \|[D^n g]\|_{L^n(Y;X)} \leq C_n(\|Df^{-1}\|_{L^1(Y;X)}, \|Df\|_{L^1(X;Y)}, \ldots, \|D^nf\|_{L^n(X;Y)}). $$ 
Moreover, under the following additional assumptions:
\begin{itemize}
    \item $Df(x)$ is an isomophism for all $x\in S$ and the inverse function $g:W\rightarrow S$ exists globally. 
    \item There exists a constant $C > 1$ such that  $\|D^nf\|_{L^\infty (S, L^n(X;Y)))} \leq n!C^{n}$ for all $n \leq k$. 
    \item $\|[Df]^{-1}\|_{L^\infty(W, L^1(Y;X))} \leq A$ for another constant $A$. 

\end{itemize}
Then, for all $n\leq k$, we have $\|D^ng\|_{L^\infty(W,L^n(Y;X))}\leq (Cn^{1+\epsilon})^{n^2}n!A^{2n-1}\left(\frac{n^\epsilon}{n^\epsilon - 1}\right)^{2n-1}$ for any $\epsilon > 0$. 



\end{theorem}
\begin{proof}
The existence of $g$ and $B$ follows from the inverse function theorem. By Fa\'{a}di Bruno's formula in Banach space (Thm \ref{thm:faadiBrunoBanach}), for $n \geq 2$, we have $$0 = D^n(f\circ g)(y) = \sum_{\bsk\in T_n}{n \choose \bsk}[D^{|\bsk|}f](g(y))\prod_{m=1, k_m\neq 0}^n(\frac{[D^m g](y)}{m!})^{k_m},$$ where $T_n = \{\bsk = (k_1,...,k_n) : \sum_{j=1}^n jk_j = n\}$

There is only one multi-index $\bsk \in T_n$ such that $k_n \neq 0$, namely $\bsk = (0,\ldots,0,1)$. Hence we have, with $\Bar{T}_n = \{\bsk = (k_1,\ldots k_{n-1})|\sum_{j=1}^{n-1}jk_j=n\}$ and $y=f(x)$, 
$$
[D^ng](y) = -([Df](g(y)))^{-1} \left ( \sum_{\bsk\in \bar{T}_n}{n \choose k}[D^{|\bsk|}f](g(y)) \left( \prod_{m=1, k_m\neq 0}^{n-1} \left (\frac{[D^m g](y)}{m!} \right )^{k_m}\right)\right).
$$ 
%
It is not hard to see that the right hand side can be bounded by $$C_n(\|Df^{-1(x)}\|_{L^1(Y;X)}, \|Df(x)\|_{L^1(X;Y)},..., \|D^nf(x)\|_{L^n(X;Y)})$$ for some continuous function $C_n:\mathbb{R}^{n+1}\rightarrow\mathbb{R}^+$ that is independent of $f$. Hence the first part of the proposition holds. 


To prove the bound in the statement of the theorem, we consider $\|D^ng(y)\|_{L^n(Y;X)}$ for some $n\leq k$ and use induction on the differentiation index. The base case is $n = 1$, in which case we have $1 = D^n(f\circ g)(y)$ holds uniformly for all $y \in Y$. By the chain rule, we have $(Df\circ g)(y)\circ (Dg(y)) = 1$, so that $Dg(y) = [Df (g(y))]^{-1}(1).$ By our assumption that  $\|(Df)^{-1}\|_{L^\infty(W, L^1(Y;X))} \leq A$, the base case for the induction holds.  

For $n > 1$, first we make the observation that for any $y\in W$, we have the relationship 
\begin{align*}
&\|D^ng(y)\|_{L^n(Y,X)} \leq\|([Df](g(y)))^{-1}\|_{L^1(Y;X)}\\ &\left ( \sum_{\bsk\in \bar{T}_n}{n \choose \bsk}\|D^{|\bsk|}f(g(y))\|_{L^{|\bsk|}(X,Y)} \prod_{m=1, k_m\neq 0}^{n-1} \left (\frac{\|D^m g(y)\|_{L^m(Y,X)}}{m!} \right )^{k_m}\right).    
\end{align*}

Note that this holds for all point $y\in W$, so we also have 
\begin{align*}
&\|D^ng\|_{L^\infty(W,L^n(Y,X))} \leq\|[Df]^{-1}\|_{L^\infty(S, L^1(Y;X))}\\ &\left ( \sum_{\bsk\in \bar{T}_n}{n \choose \bsk}\|D^{|\bsk|}f\|_{L^\infty(S, L^{|\bsk|}(X,Y))} \prod_{m=1, k_m\neq 0}^{n-1} \left (\frac{\|D^m g\|_{L^\infty(W,L^m(Y,X))}}{m!} \right )^{k_m}\right).    
\end{align*}




Since the sum $ \sum_{\bsk\in \bar{T}_n}\|D^{|\bsk|}f\|_{L^\infty(S, L^{|\bsk|}(X,Y))}$ is in general not finite when $n\rightarrow\infty$, we introduce a scaling argument to quantify the rate at which this term diverges. For any point $a\in X$, we can define a new function $\Tilde{f}(x) := f(\alpha x + a)$ and it is clear to see that $D^n\Tilde{f}|_{x=0} = \alpha^nD^nf(a)$. By our assumption, $\|D^nf(a)\|_{L^n(X;Y)}\leq n!C^n$ for some constant $C$ and by choosing $\alpha$ to be small enough, for example, when $\alpha = C^{-1}n^{-1 - \epsilon}$ where $\epsilon > 0$, we have $\|D^n\Tilde{f}(0)\|_{L^n(X;Y)} \leq \frac{n!C^n}{C^nn^{n + n\epsilon}} = \frac{n!}{n^{n+n\epsilon}}$. That is, we can write $\|D^n\Tilde{f}(0)\|_{L^n(X;Y)} \leq n!\Tilde{C}^n$, where $\Tilde{C} = \frac{1}{n^{1 + \epsilon}}$ and the new constant $\Tilde{C}$ satisfies $n\Tilde{C} < 1$.  

For all $\bsk$ such that $|\bsk| \leq n$ and all $x\in S$, we then have $\|D^{|\bsk|}f(x)\|_{L^n(X,Y)} = \alpha^{-|\bsk|}\|D^{|\bsk|}\Tilde{f}(0)\| \leq \alpha^{-n}|\bsk|!\Tilde{C}^{|\bsk|}$ and thus $\|D^{|\bsk|}f\|_{L^\infty(S;L^n(X,Y))} \leq \alpha^{-n}|\bsk|!\Tilde{C}^{|\bsk|}$. Therefore, we have 
$$
\|D^ng\|_{L^\infty(W;L^n(Y;X))} \leq \alpha^{-n}An! \left ( \sum_{\bsk\in \bar{T}_n}\tilde{C}^{|\bsk|}\frac{|\bsk|!}{\bsk!} \prod_{m=1, k_m\neq 0}^{n-1} \left (\frac{\|D^m g\|_{L^\infty(W;L^m(Y,X))}}{m!} \right )^{k_m}\right).
$$ 

Now, we give an upper bound for sums that take the form $\sum_{k\in \bar{T}_n}\tilde{C}^{|k|}\frac{|k|!}{k!}$. 



Note that for every multi-index set $\bar{T_m}$, $m \leq n$, we have $\sum_{k\in \bar{T}_m}\tilde{C}^{|k|}\frac{|k|!}{k!}  \leq \sum_{k\in \bar{T}_n}\tilde{C}^{|k|}\frac{|k|!}{k!}\leq \sum_{l = 0}^\infty(n\tilde{C})^l = \frac{1}{1 - n\tilde{C}}$. Suppose the induction hypothesis holds, that is, for all $m$ with $m \leq n$, we have $\|D^mg\|_{L^\infty(W;L^m(Y,X))}\leq \alpha^{-m^2}m!A^{2m-1}\left(\frac{1}{1 - n\tilde{C}}\right)^{2m-1}$.

Then, we can calculate
$$
\|D^ng\|_{L^\infty(W;L^n(Y,X))} \leq \alpha^{-n}An! \left ( \sum_{\bsk\in \bar{T}_n}\tilde{C}^{|\bsk|}\frac{|\bsk|!}{\bsk!} \prod_{m=1, k_m\neq 0}^{n-1} \left (\frac{(\frac{1}{\alpha})^{m^2} m!A^{2m-1}\left(\frac{1}{1 - n\tilde{C}}\right)^{2m-1}}{m!} \right )^{k_m}\right).
$$ 

Note that 
\begin{align*}
\prod_{m=1, k_m\neq 0}^{n-1} \left ((\frac{1}{\alpha})^{m^2}\frac{m!A^{2m-1}\left(\frac{1}{1 - nC}\right)^{2m-1}}{m!} \right )^{k_m} &\\
= (\frac{1}{\alpha})^{\sum_{m=1, k_m\neq 0}^{n-1}(m^2)k_m} A^{\sum_{m=1, k_m\neq 0}^{n-1}(2m-1)k_m}\left(\frac{1}{1 - n\tilde{C}}\right)^{\sum_{m=1, k_m\neq 0}^{n-1}(2m-1)k_m}\\    
\end{align*}


Observe that for $\bsk = (k_1,...,k_{n-1}) \in \bar{T_n}$, we have $\sum_{m=1, k_m\neq 0}^{n-1}mk_m = n$. Therefore, $\sum_{m=1, k_m\neq 0}^{n-1}(2m-1)k_m = 2n - \sum_{m=1, k_m\neq 0}^{n-1}k_m$. Since $\bar{T_n}$ excludes the partition of $n$ into a single set of $n$ elements, there are at least two $k_m$ not equal to zero in the summation. Therefore, we have $\sum_{m=1, k_m\neq 0}^{n-1}(2m-1)k_m \leq 2n-2$. Also, we have $\sum_{m=1, k_m\neq 0}^{n-1}m^2k_m  \leq (n-1)\sum_{m=1, k_m\neq 0}^{n-1}mk_m = (n-1)n. $

Putting things together, we have 
\begin{align*}
\|D^ng\|_{L^\infty(W;L^m(Y,X))} &\leq  (\frac{1}{\alpha})^{n}n!A \left ( \sum_{\bsk\in \bar{T}_n}\tilde{C}^{|\bsk|}\frac{|\bsk|!}{\bsk!}(\frac{1}{\alpha})^{n^2-n} A^{2n-2}\left(\frac{1}{1 - n\tilde{C}}\right)^{2n-2} \right)\\
&\leq \alpha^{-n^2}n!A^{2n-1}\left(\frac{1}{1 -n\tilde{C}}\right)^{2n-1} \leq (Cn^{1+\epsilon})^{n^2}n!A^{2n-1}\left(\frac{n^\epsilon}{n^\epsilon - 1}\right)^{2n-1} 
\end{align*}
\end{proof}


Theorem \ref{thm:NormInverse} combined with theorem \ref{thm:compositionVelocityFieldNorm} can be specialized to obtain upper bound on the $C^k$ norm of velocity field $f$. Before stating the main result, we need the following technical lemma about the norm of the inverse of a block triangular matrix:




\begin{lemma}\label{lemma:blockTriangle}
Suppose $M$ is a block triangular matrix: \[
   M =
  \left[ {\begin{array}{cc}
   A & B \\
   0 & D \\
  \end{array} } \right],
\]
where $A, D$ are invertible. Then, we have the upper bound $$\|M^{-1}\|_2 = \frac{1}{\sigma_{\min}(M)} \leq \|D^{-1}\|_2 + \|A^{-1}BD^{-1}\|_2 + \|A^{-1}\|_2,$$ where $\sigma_{\min}(M)$ denotes the minimum singular value of $M$ and $\|.\|_2$ denotes the operator-2 norm of a matrix. 
\end{lemma}
\begin{proof}
Since $A, D$ are invertible, then the inverse of $M$ is just \[
   M^{-1} =
  \left[ {\begin{array}{cc}
   A^{-1} & -A^{-1}BD^{-1} \\
   0 & D^{-1} \\
  \end{array} } \right],
\]
and the result then follows from triangular inequality. 
\end{proof}

\rr{When using measure-transport based sampling methods in practice, it is common to not learn a map that directly pushes forward source to the target, but instead to learn a composition of maps that pushes forward source to the target through a sequence of intermediate measures. As we will show, if two measures are not too far from each other (in a proper sense measured by the transport map), then the size of the neural network in the neuralODE whose time-one flow map pushes one measure onto the other scales linearly with respect to the dimension $d$, which partly answers the question why this approach can potentially offer benefits in practice. 

}
\begin{theorem}\label{thm:velocityfieldNormBound}
In the context of theorem \ref{thm:ExistenceOfOptimalSolution}, suppose that we have the following additional assumptions about the transport map $T$:
\begin{itemize}
    \item There exists a constant $C \geq 1$ such that $\|D^n_xT\|_{L^\infty(\Omega_0,L^n(\mathbb{R}^d;\mathbb{R}^d))} \leq n!C^{n}$ for all $n \leq k$. \rrtd{do we really need $\Omega_0$ and $\Omega_1$ to live in the same ambient space? Also, probably need some discussion on why this assumption is reasonable (related to the analyticity of functions?)}
    \item $\forall x\in\Omega_0\cup\Omega_1$, $|x| \leq M$.\rrtd{This could be derived from the next assumption?}
    \item $\forall x$, the Jacobian matrix satisfies $\|I - \nabla_x T(x)\|_{L^\infty(\Omega_0, L^1(\mathbb{R}^d,\mathbb{R}^d))} \leq \delta < 1$. \rrtd{need to add a discussion about this assumption?}
    

    
    
\end{itemize}
Then, the $C^k$ norm of the velocity field $f$ can be bounded as $\|f\|_{C^k(\Omega)}\leq e^{\Theta(dC^{k^2}(1-\delta)^{1-2k}(2+2M-\delta)^{2k-1})}$ when treating $k$ as a constant. 
\end{theorem}
\begin{proof}
Recall from Thm.\ref{thm:f} that we can write $f(y,t) = T(F(y,t)) - F(y,t)$. Clearly, we have $\|f\|_{C^k(\Omega, \mathbb{R}^d)} \leq \|T\circ F\|_{C^k(\Omega, \mathbb{R}^d)} + \|F\|_{C^k(\Omega, \mathbb{R}^d)}$ and it remains to upper bound $\|T\circ F\|_{C^k(\Omega, \mathbb{R}^d)}$ and $\|F\|_{C^k(\Omega, \mathbb{R}^d)}$.

Since $\|F\|_{C^k(\Omega, \mathbb{R}^d)} \leq \|G^{-1}\|_{C^k(\Omega, \mathbb{R}^{d+1})}$, where $G^{-1}$ is the inverse of $G(x,t) = (T_t(x), t)$ for $(x,t)\in\Omega_0\times[0,1]$. \rrtd{needs to be checked} By theorem \ref{thm:NormInverse}, this is bounded from above by a function of $$\|[D_{x,t}G]^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )}, \|D_{x,t}G\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )},\cdots,\|D^k_{x,t}G\|_{L^\infty(\Omega,L^k(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )}.$$ 

First, note that  The Jacobian matrix,  $D_{x,t}G(x,t)$ is given by 
\[
   D_{x,t}G(x,t)=
  \left[ {\begin{array}{cc}
   t\nabla T(x) + (1-t)I & T(x) - x \\
   0 & 1 \\
  \end{array} } \right].
\]

Since $\nabla_x T_t(x)$ is invertible and $|T(x) - x| \leq 2M$ $\forall x\in\Omega_0$, by lemma \ref{lemma:blockTriangle}, we have
\begin{align*}
\|[D_{x,t}G]^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )} &\leq 1 + 2M\|(t\nabla_x T + (1-t)I)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))} + \\
&\|(t\nabla_x T + (1-t)I)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))}\\
&\leq 1 + (2M+1)\|\nabla_x T_t^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))}.
\end{align*}


Note for fixed $t\in[0,1]$, $\|\nabla_x T_t^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))} \leq  \sup_{(x,t)\in\Omega}\|(I - t(I - \nabla T))^{-1}\|_2 \leq \sum_{i=0}^\infty t^i\|I - \nabla T\|_2^i \leq \sum_{i=0}^\infty \|I - \nabla T\|_2^i = \frac{1}{1-\delta}.$

For higher order derivatives, since $t\in[0,1]$, we always have \rrtd{needs to be checked}  
$$\|D_{x,t}^nG\|_{L^\infty(\Omega, L^n(\mathbb{R}^{d+1}, \mathbb{R}^{d+1}))} \lesssim \|D_x^nT\|_{L^\infty(\Omega, L^n(\mathbb{R}^{d}, \mathbb{R}^d))} + \|D_x^{n-1}T\|_{L^\infty(\Omega, L^{n-1}(\mathbb{R}^{d}, \mathbb{R}^d))}  \lesssim n!C^n$$ for $n\leq k$. Therefore, application of Thm \ref{thm:NormInverse} gives the upper bound 
$$\|F\|_{C^k(\Omega)} \leq  (Ck^{1+\epsilon})^{k^2}k!\|[DG]^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^{d+1}, \mathbb{R}^{d+1}))}^{2k-1}\left(\frac{k^\epsilon}{k^\epsilon - 1}\right)^{2k-1}\leq (Ck^{1+\epsilon})^{k^2}k!\left(\frac{(2 + 2M - \delta)k^\epsilon}{(1-\delta)(k^\epsilon - 1)}\right)^{2k-1} $$

Since the result holds for arbitrary $\epsilon > 0$, we choose $\epsilon = 1$ and use the fact that $\lim_{k\rightarrow\infty} (\frac{k}{k-1})^{2k-1} = e^2$ to get:
$$\|F\|_{C^k(\Omega)} \leq \frac{e^2k!(Ck^{2})^{k^2}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}} $$




Finally, Thm \ref{thm:compositionVelocityFieldNorm} gives the upper bound for norm $\|T\circ F\|_{C^k(\Omega)}$:

$$\|T\circ F\|_{C^k(\Omega)} \leq  d^kk!C^k\exp\left(kd\|F\|_{C^k(\Omega)}\right) \leq  d^kk!C^k\exp\left(\frac{de^2k!C^{k^2}k^{2k^2+1}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}} \right)
$$

Using $\|f\|_{C^k(\Omega)} \leq \|T\circ F\|_{C^k(\Omega)} + \|F\|_{C^k(\Omega)}$, we have


$$\|f\|_{C^k(\Omega)} \leq d^kk!C^k\exp\left(\frac{de^2k!C^{k^2}k^{2k^2+1}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}} \right)
+\frac{e^2k!(Ck^{2})^{k^2}(2+2M-\delta)^{2k-1}}{(1-\delta)^{2k-1}}. $$

Treating $k$ as a constant, we see that the order how $\|f\|_{C^k(\Omega)}$ scales with $d$, $C$, $M$ and $\delta$ is characterized by $e^{\Theta(dC^{k^2}(1-\delta)^{1-2k}(2+2M-\delta)^{2k-1})}$. 

\end{proof}

As noted, the assumption $\|I - \nabla T(x)\|_{L^\infty(\Omega_0, L^1(\mathbb{R}^d,\mathbb{R}^d))} \leq \delta < 1$ is not necessary to obtain the boundedness of $\|f\|_{C^k}$. For a general transport map $T$, we can also obtain bounds on the $C^k$ norm of the velocity field $f$ whose time-one flow map of the ODE realizes $T$. 

\begin{lemma}\label{lemma:MatrixInverseNorm}
Assume that $A$ is a non-singular matrix of dimension $d$, then $\|A^{-1}\|_2\leq \frac{\|A\|_2^{d-1}}{|\det(A)|}$
\end{lemma}
\begin{proof}
First, note that $|\det(A)| = \prod_{i=1}^d\sigma_i$, where $\sigma_i$ is the $i-$th smallest singular value. Then, we have $\frac{\|A\|_2^{d-1}}{|\det(A)|} = |\frac{\sigma_d^{d-1}}{ \prod_{i=1}^d\sigma_i}| = |\left(\prod_{i=2}^d\frac{\sigma_d}{\sigma_i}\right)\frac{1}{\sigma_1}| \geq |\frac{1}{\sigma_1}| = \|A^{-1}\|_2.$
\end{proof}

\begin{lemma}\label{lemma:NormTtinverse}
Let $T$ satisfy the assumptions in \eqref{eq:spectrum}. Assume further that $\sup_{x\in\Omega_0}\|\nabla T(x)\|_{L^1(\mathbb{R}^d,\mathbb{R}^d)} \leq C$ and $\inf_{x\in\Omega_0}|\det(\nabla T(x))|\geq c$ for some constants $C,c > 1$. \rrtd{If we assume $c > 1$,  we can simplify the bound to $C^{d-1}/c$} Then for all $t$, $\|(\nabla T_t(x))^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d,\mathbb{R}^d))} = \|((1-t)I + t\nabla T)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d,\mathbb{R}^d))} \leq \frac{C^{d-1}}{\min\{1, c\}}$.
\end{lemma}
\begin{proof}
By lemma \ref{lemma:MatrixInverseNorm}, $\|(\nabla T_t(x))^{-1}\|_L^1(\mathbb{R}^d,\mathbb{R}^d)\leq \frac{\|T_t(x)\|_{L^1(\mathbb{R}^d,\mathbb{R}^d)}^{d-1}}{|\det(T_t(x))|}$ for fixed $(x,t)\in\Omega$. First, note that we have $\|\nabla T_t(x)\|_{L^1(\mathbb{R}^d,\mathbb{R}^d)} \leq (1-t) + t\|\nabla T\|_{L^1(\mathbb{R}^d,\mathbb{R}^d)} \leq 1-t + tC \leq C$.

Now it remains to lower bound $|\det((1-t)I + t\nabla T(x))|$. By \eqref{eq:spectrum}, we have $\det(\nabla T(x)) > 0$ and thus $\det(\nabla T_t(x)) > 0$ $\forall (x,t)\in\Omega$. Since $\det((1-t)I + t\nabla T(x)) = \prod_{i=1}^d ((1-t) + t\lambda_i(x))$ where $\lambda_i(x)$ are the eigenvalues of the matrix $\nabla T(x)$. Consider the function $g_t(s) = \log(1-t+e^{s})$. This is a convex function in $s$ for $t\in[0,1]$, thus we have $\frac{1}{d} \sum_{i=1}^d g_t(\log (t\lambda_i(x))) \geq g_t(\frac{1}{d} \sum_{i=1}^d\log (t\lambda_i(x)))$. That is, $  \sum_{i=1}^d \log(1-t+t\lambda_i(x)) \geq  d \log(1-t + t(\Pi_{i=1}^d \lambda_i(x))^{1/d}) \geq d\log(1-t+\det(\nabla T(x))^{1/d}t).$ Taking exponential on both sides, we can conclude that $\det((1-t)I + t\nabla T(x)) \geq (1-t+\det(\nabla T(x))^{1/d}t)^d.$ 

Putting things together and taking the supremum over all $(x,t)\in\Omega$, we get $\|\nabla T_t(x)^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^d,\mathbb{R}^d))} \leq  \frac{C^{d-1}}{(1-t+c^{1/d}t)^d}$, which is upper bounded by $\frac{C^{d-1}}{\min\{1, c\}}$.


\end{proof}

\begin{theorem}\label{thm:fNormGeneral}
In the context of theorem \ref{thm:ExistenceOfOptimalSolution}, suppose that we have the following additional assumptions about the transport map $T$:
\begin{itemize}
    \item There exists a constant $C \geq 1$ such that $\|D^n_xT\|_{L^\infty(\Omega_0,L^n(\mathbb{R}^d;\mathbb{R}^d))} \leq n!C^{n}$ for all $n \leq k$. 
    \item $\forall x\in\Omega_0\cup\Omega_1$, $|x| \leq M$.
    \item $\inf_{x\in\Omega_0} |\det(\nabla T(x))| \geq c$ for some constant $c$. 
    

    
    
\end{itemize}
Then, the $C^k$ norm of the velocity field $f$ can be bounded as $\|f\|_{C^k} \leq \Theta(C^{k^2}(2M+1)^{2k-1}C^{(d-1)(2k-1)}(\min\{1, c\})^{1-2k})$. 
\end{theorem}
\begin{proof}
The proof follows the same logic as in the proof of Thm.\ref{thm:velocityfieldNormBound}. Since we have $\|f\|_{C^k(\Omega, \mathbb{R}^d)} \leq \|T\circ F\|_{C^k(\Omega, \mathbb{R}^d)} + \|F\|_{C^k(\Omega, \mathbb{R}^d)}$ and $\|F\|_{C^k(\Omega, \mathbb{R}^d)} \leq \|G^{-1}\|_{C^k(\Omega, \mathbb{R}^{d+1})}$, we shall only need to eatablish upper bounds for $\|G^{-1}\|_{C^k(\Omega, \mathbb{R}^{d+1})}$. Similarly, we have \begin{align*}
\|[D_{x,t}G]^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )} &\leq 1 + 2M\|(t\nabla_x T + (1-t)I)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))} + \\
&\|(t\nabla_x T + (1-t)I)^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))}\\
&\leq 1 + (2M+1)\|\nabla_x T_t^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))}.
\end{align*}

Now, lemma \ref{lemma:NormTtinverse} gives $\|\nabla_x T_t^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^d, \mathbb{R}^d))} \leq \frac{C^{d-1}}{\min\{1, c\}}$. We then have $$\|[D_{x,t}G]^{-1}\|_{L^\infty(\Omega,L^1(\mathbb{R}^{d+1},\mathbb{R}^{d+1}) )} \leq 1 + \frac{(2M+1)C^{d-1}}{\min\{1, c\}}.$$

For higher order derivatives, we again have 
$$\|D_{x,t}^nG\|_{L^\infty(\Omega, L^n(\mathbb{R}^{d+1}, \mathbb{R}^{d+1}))} \lesssim \|D_x^nT\|_{L^\infty(\Omega, L^n(\mathbb{R}^{d}, \mathbb{R}^d))} + \|D_x^{n-1}T\|_{L^\infty(\Omega, L^{n-1}(\mathbb{R}^{d}, \mathbb{R}^d))}  \lesssim n!C^n$$ for $n\leq k$. Then, application of Thm \ref{thm:NormInverse} gives the upper bound 
$$\|F\|_{C^k(\Omega)} \lesssim  (Ck^{1+\epsilon})^{k^2}k!\|[DG]^{-1}\|_{L^\infty(\Omega, L^1(\mathbb{R}^{d+1}, \mathbb{R}^{d+1}))}^{2k-1}\left(\frac{k^\epsilon}{k^\epsilon - 1}\right)^{2k-1}\lesssim (Ck^{1+\epsilon})^{k^2}k!\left(\frac{e^2(2M+1)C^{d-1}}{\min\{1, c\}}\right)^{2k-1} $$ 

Thm \ref{thm:compositionVelocityFieldNorm} gives the upper bound for norm $\|T\circ F\|_{C^k(\Omega)}$:

$$\|T\circ F\|_{C^k(\Omega)} \lesssim  d^kk!C^k\exp\left(kd\|F\|_{C^k(\Omega)}\right) \lesssim  d^kk!C^k\exp\left(\frac{dk!C^{k^2}k^{2k^2+1}((2M+1)C^{d-1})^{2k-1}}{\min\{1, c\}^{2k-1}} \right)
$$

Ignoring constants, we have $\|f\|_{C^k} \leq \Theta(C^{k^2}(2M+1)^{2k-1}C^{(d-1)(2k-1)}(\min\{1, c\})^{1-2k})$. 
\end{proof}


\subsubsection{Special case: triangular transport maps}
A special type of transport map commonly used in practice is the Knothe-Rosenblatt map, which has a triangular structure. The Knothe-Rosenblatt map can be constructed explicitly in terms of the densities of the source and target measures. For the explicit construction, see appendix \ref{app:KRMap}. In this part, we shall derive the bounds on the norm of the velocity field associated with the triangular transport map, also explicitly in terms of the densities. 


\begin{proposition}
In the context of theorem \ref{thm:f}, if we assume further that the underlying transport map $T(x)$ is triangular and monotone,$\det(T_t(x)) > 0$. Then the underlying velocity field is also triangular with respect to the flow map.  
\end{proposition}

\begin{proof}
Under the assumption that the field $T$ is monotone and triangular, we can write $$T(x) = [T_1(x_1), T_2(x_1,x_2),..., T_d(x_1,..., x_d)]^T,$$ where $T_i$ is the i-th component of the transport map and triangularity implies the condition $\frac{\partial T_i}{\partial x_j} = 0$ for all $j > i$. Let $X(x,t) = tT(x) + (1-t)x$ be the flow map of the ODE, then $f$ satisfies the relation $f(X(x,t), t) = T(x) - x$. Consider the $i-th$ component of the equation, $f_i(X(x,t), t) = T_i(x) - x_i$. Note that $f_i$ is a function from $\mathbb{R}^d$ to $\mathbb{R}$ that depend on $X_1(x_1,t), X_2(x_1,x_2, t),...,X_d(x_1,..,x_d,t)$ where we used the fact that $X(x,t)$ inherits the same triangular structure from $T$. For $j>i$, if we take the derivative with respect to $x_j$, only the terms involving $X_j, X_{j+1}, ..., X_d$ will be involved. What we need to prove is that $\frac{\partial}{\partial X_j}f_i(X(x,t), t) = 0$ for all $j>i$. (Note the triangular structure is with respect to the input of the vector field, $X$, not $x$).

To prove the claim, we go backwards recursively. For $j = d$, only $X_d$ depends on $x_d$, therefore, we have $\frac{\partial}{\partial x_d}f_i(X(x,t), t) = \frac{\partial f_i}{\partial X_d}\frac{\partial X_d}{\partial x_d} = 0$. Since by the monotonicity assumption, $\frac{\partial T_d}{\partial x_d}>0$,  we have $\frac{\partial X_d}{\partial x_d} = t\frac{\partial T_d}{\partial x_d}+(1-t)$, which is a convex combination of two positive numbers. Therefore, we must have $\frac{\partial f_i}{\partial X_d} = 0$. Now suppose the claim is true for all indices greater than $j = d - k > i$, we have $f_i$ depends on $x_j$ through the terms $X_{d-k}, X_{d-k+1}, ..., X_{d}$. The chain rule gives $\frac{\partial}{\partial x_j}f_i(X(x,t), t) = \sum_{l=1}^k\frac{\partial}{\partial X_{d-l}}f_i(X(x,t), t)\frac{\partial X_{d-l}}{\partial x_{j}}$. Since we have $\frac{\partial}{\partial X_{d-l}}f_i(X(x,t), t) = 0$ for $l < k$, we are left with $\frac{\partial}{\partial X_{j}}f_i(X(x,t), t)\frac{\partial X_{j}}{\partial x_{j}}$. By monotonicity again, we have $\frac{\partial X_{j}}{\partial x_{j}} > 0$, whichi implies $\frac{\partial}{\partial X_{j}}f_i(X(x,t), t) = 0$. 




\end{proof}

Recall in Thm.\ref{thm:LipDomain}, we showed the domain of the velocity field implicitly determined by the regularization, $\Omega$, is a Lipschitz domain. For triangular maps on cubes $[0,1]^d$, we will see that the corresponding velocity field has domain $[0,1]^d\times[0,1]$. 


\begin{proposition}
Suppose $T$ is a monotone bijective triangular map from $[0, 1]^d \rightarrow [0,1]^d$, then the trajectories of the flow map of the ODE, $X(x,t) = tT(x) + (1-t)x$ cover the entire space-time $[0,1]^d \times [0, 1]$.
\end{proposition}
\begin{proof}
First, we prove the result in one dimension. Note the flow map $X(x,t)$ is continuous both in $x$ and $t$ (This follows from our assumption on $T$ and $X$, or the continuity of solutions of ODEs). We can prove for each $t_0 \in [0,1]$, the map $T_{t_0}(x) = (1-t_0)x + t_0T(x)$ is a bijection. To show that this is a bijection, note that by our assumption $T$ is a monotone (assume without loss of generality, it's increasing) transport map from $[0,1]$ to $[0,1]$, thus $0$ is mapped to $0$ and $1$ is mapped to $1$. Then, we have that $T_{t_0}(0) = 0$ and $T_{t_0}(1) = (1-t_0) + t_0 = 1$, and surjectivity follows from the continuity of the map. To show injectivity, note that suppose we have $x, y\in[0,1],$ where $x<y$ and we have $(1-t_0)x + t_0T(x) = (1-t_0)y + t_0T(y)$. Rearranging gives $\frac{T(x) - T(y)}{x-y} = \frac{t_0-1}{t_0} < 0$, which contradicts the fact that $T$ is increasing.  

Now, suppose that the claim is true for $[0,1]^d \times [0,1]$, to prove that the claim is true for $[0,1]^{d+1} \times [0,1]$, consider the map in the last coordinate $T_{d+1}(x_1,...,x_{d+1})$. where $x_1,...,x_d$ are fixed and the map $T_{t_0, x_{[d]}}(x_{d+1}) = t_0T_{d+1}(x_{d+1}) + (1-t_0)x_{d+1}$ for fixed time $t_0$. By the construction of the triangular transport map, we have that $T_{t_0, x_{[d]}}$ again maps 0 to 0 and 1 to 1. By the analysis of the one-dimensional case, together with the induction hypothesis, we see that $[0,1]^{d+1} \times [0,1]$ is entirely covered by the trajectories.
\end{proof}

\begin{lemma}\label{lemma:faaibrunoSimplified}
Let $g:B_1\rightarrow B_2$ and $f: B_2\rightarrow B_3$ be $C^k$ functions on Banach spaces, then we have for all $n \leq k$:
$$\|D^n[f\circ g]\|_{L^\infty(B_1, \mathcal{L}^n(B_1, B_3))} \leq n!K\|f\|_{C^k}\|g\|_{C^k}^k$$ for some constant $K$ that only depends on $k$. And as a consequence, we have $\|f\circ g\|_{C^k} \leq k!K\|f\|_{C^k}\|g\|_{C^k}^k$.
\end{lemma}
\begin{proof}
By Faadi Bruno's theorem, we have
$$[D^n(f\circ g)](y) = \sum_{\bsk\in T_n}\frac{n!}{k_1!\ldots k_n!}[D^{|\bsk|}f](g(y)) \left(\smash[b]{\underbrace{\frac{[Dg](y)}{1!},\ldots, \frac{[Dg](y)}{1!}}_\text{$k_1$ times}},\ldots,\smash[b]{\underbrace{\frac{[D^{n}g](y)}{n!},\ldots, \frac{[D^{n}g](y)}{n!}}_\text{$k_n$ times}}\right),$$ where $T_n = \{\bsk = (k_1,\ldots,k_n) : \sum_{j=1}^n jk_j = n\}$. 

Taking the sup norm on both sides and use triangular inequality, we have 
\begin{align*}
&\|D^n [f\circ g(y)]\|_{\mathcal{L}^n(B_1, B_3)} \leq n!\sum_{\bsk\in T_n} \|f\|_{C^k}\prod_{j=1}^n\|D^jg(y)\|_{\mathcal{L}^n(B_1, B_3)}^{k_j}   \\
&\leq n!\sum_{\bsk\in T_n} \|g\|_{C^k}^{\sum_{j=1}^nk_j}
\leq n!K\|f\|_{C^k}\|g\|_{C^k}^n 
\leq n!K\|f\|_{C^k}\|g\|_{C^k}^k, \\
\end{align*}
for some constant $K$. 

\end{proof}



\begin{theorem}
In the context of theorem \ref{thm:ExistenceOfOptimalSolution}, suppose the transport map $T(x)$ is given by the Knothe-Rosenblatt construction as in appendix \ref{app:KRMap}. Suppose we have the following assumptions about the densities:
\begin{itemize}
   \item The densities $\pi(x)$ and $\rho(x)$ satisfy the bounds in assumption \ref{AssumptionDensity2}. 
   \item $\forall x\in \Omega_0\cup\Omega_1, |x| \leq M$. 

    
    
\end{itemize}
Then, the $C^k$ norm of the velocity field $f$ can be bounded as $$\|f\|_{C^k} = O\left((2M+1)^{2k-1}L_1^{2k^{d+1}+2dk^{d}}L_2^{-2k^{d+1}-2dk^{d}}\right).$$
\end{theorem}
\begin{proof}
Recall from appendix \ref{app:KRMap} that diagonal entries of the Jacobian matrix of the triangular transport map satisfy 
$$\sigma_i := \partial_{x_i}T_i(x_{[i]}) = \frac{\rho_i(x_{[i]})}{\pi_i(T_1(x_1), T_2(x_1, x_2),..., T_i(x_{[i]}))}.$$

From the bounds on the $C^k$ norm of the densities, we have $\rho$ and $\pi$ are bounded from above and below by $L_1$ and $L_2$. Then we have $\pi_i(x_{[i]})$ and $\rho_i(x_{[i]})$ are bounded from above and below by $\frac{L_1}{L_2}$ and $\frac{L_2}{L_1}$. Then $\partial_{x_i}T_i(x_{[i]})$ is bounded from below by $\frac{L_2^2}{L_1^2}$. Note that the diagonal entries of $\nabla_x T_t(x)^{-1} = [(1-t)I_{d\times d} + t\nabla_F T(F(y,t))]^{-1}$ are exactly $\{\frac{1}{1-t + t\sigma_i}\}_{i=1}^d$, which are upper bounded by $\frac{L_1^2}{(1-t)L_1^2 + tL_2^2} \leq \frac{L_1^2}{L_2^2}$ and lower bounded by $\frac{L_2^2}{(1-t)L_2^2 + tL_1^2} \geq \frac{L_2^2}{L_1^2}$.

Then, we have $\inf_{x\in\Omega_0}\det (\nabla_x T(x)) \geq \left(\frac{L_2}{L_1}\right)^{2d}$. 

Now, we shall derive upper bounds on $\|D^n_xT\|_{L^\infty(\Omega_0, \mathcal{L}^n(\mathbb{R}^d, \mathbb{R}^d))}$ based on the densities. Recall that the $i-$th component of the KR map can be expressed as $$T_i(x_{[i-1]}, \cdot) = F_{\pi, i}(T_1(x_1), T_2(x_1, x_2), ..., T_{i-1}(x_{[i-1]}), \cdot)^{-1} \circ F_{\rho, i}(x_{[i-1]}, \cdot)$$, where $$F_{\pi, i}(x_{[i-1]}, x_i)  = \int_{0}^{x_i} \pi_i(x_{[i-1]}, t_i)dt_i $$ is the corresponding conditional CDF and $F_{\rho, i}$ is defined in a similar fashion. 
This is a composition of two $C^k$ functions. To apply Lemma.\ref{lemma:faaibrunoSimplified}, we need to upper bound the $C^k$ norm of $F_{\pi, i}(T_1(x_1), T_2(x_1, x_2), ..., T_{i-1}(x_{[i-1]}), \cdot)^{-1}$. 


Let $G_{\pi, i}(x_1,...,x_{i-1}, x) = F_{\pi, i}(T_1(x_1), T_2(x_1, x_2), ..., T_{i-1}(x_{[i-1]}), x)^{-1}$. Then we can write $$T_i(x_1,...,x_{i-1}, x) = G(x_1, ..., x_{i-1}, F_{\rho, i}(x_{[i-1]}, x)).$$

By the definition of the inverse, we have $$F_{\pi, i}(T_1(x_1), T_2(x_1, x_2), ..., T_{i-1}(x_{[i-1]}), G_{\pi, i}(x_1,...,x_{i-1}, F_{\rho, i}(x_{[i-1]}, x))) = F_{\rho, i}(x_{[i-1]}, x)$$

Taking the derivative with respect to $\bsv$ with $|\bsv| = k$, we obtain from formula \ref{thm:FaadiBruno} that 
\begin{align*}
&D^\bsv_{x_1,...,x_{i-1},x} F_{\pi, i}(T_1(x_1), T_2(x_1, x_2), ..., T_{i-1}(x_{[i-1]}), G_{\pi, i}(x_1,...,x_{i-1}, F_{\rho, i}(x_{[i-1]}, x)) \\
&= \sum_{1\leq |\bslambda| \leq n}D^\bslambda F_{\pi,i}\sum_{p(\bsv,\bslambda)} (\bsv!)\prod_{j=1}^n\frac{[D^{\bsl_j}\mathcal{T}]^{\bsk_j}}{(\bsk_j!)[\bsl_j!]^{|\bsk_j|}} = D^\bsv_{x_1,...,x_{i-1},x}F_{\rho, i}(x_{[i-1]}, x), \\
\end{align*}
where $\mathcal{T}(x_1,...,x_{i-1}, x) = [T_1(x_1), T_2(x_1,x_2),...,T_i(x_1,...,x_{i-1},x)]$ is the vector field given by the KR map. 

Note for $T_1(x_1)$, we have $T_1(x_1) = F_{\pi, 1}^{-1}(F_{\rho, 1}(x_1))$. For $F_{\rho, 1}(x_1)$, taking any derivatives gives $D^n_{x_1} F_{\rho, 1}(x_1) = D^n_{x_1}\int_{0}^{x_1}\rho_1(t)dt = D^{n-1}_{x_1}\rho_1(x_1)$. Then, it holds that $\|F_{\rho, 1}\|_{C^k} \leq \|\rho_1\|_{C^k}.$ Recall that $\rho_1(x_1) = \int_{[0,1]^{d-1}}\rho(x_1,t_2,t_3,..,t_d)d\mu((t_j)_{j=2}^d)$. Then, we have $\|\rho_1\|_{C^k} \leq \|\rho\|_{C^k}\leq L_1$ and thus $\|F_{\rho, 1}\|_{C^k} \leq L_1$. On the other hand, we similarly have $\|F_{\pi, 1}\|_{C^k} \leq L_1$. Note that $\|D^1[F_{\pi,1}^{-1}]\|_{L^\infty} = \|\frac{1}{D^1F_{\pi,1}}\|_{L^\infty} \leq \frac{1}{L_2}$. Now, application of Thm.\ref{thm:NormInverse} yields $\|F_{\pi,1}^{-1}\|_{C^k} \lesssim \frac{L_1^k}{L_2^{2k-1}}$. Lemma \ref{lemma:faaibrunoSimplified} then gives the upper bound for the composition $\|F_{\pi,1}^{-1}\circ F_{\rho,1}\|_{C^k} \lesssim \|F_{\pi,1}^{-1}\|_{C^k}\|F_{\rho,1}\|_{C^k}^k \lesssim \frac{L_1^{2k}}{L_2^{2k-1}}$. That is, $\|T_1\|_{C^k} \lesssim \frac{L_1^{2k}}{L_2^{2k-1}}$.

Now, we can iteratively derive the upper bounds for each $T_i$. First note that for every $i$, it holds that $\|F_{\rho, i}\|_{C^k} \leq L_1$. To get the upper bound for $\|F_{\pi,i}(T_1(x_1),...,T_{i-1}(x_{i-1}), \cdot)^{-1}\|_{C^k}$ we note that this is a composition of $F_{\pi,i}(x_1,..x_{i-1}, \cdot)^{-1}$ with the map $H_i: (x_1,...,x_i) \rightarrow (T_1(x_1),...,T_{i-1}(x_{i-1}), x_i)$. For $F_{\pi,i}(x_1,..x_{i-1}, \cdot)^{-1}$, we make use of the identity 
\begin{equation}\label{eq:identity}
F_{\pi,i}(x_1,...,x_{i-1}, F_{\pi, i}^{-1}(x_1,...,x_i)) = x_i.     
\end{equation}


Taking first order derivative with respect to $x_j$ gives the identity 
$$D_jF_{\pi,i} + (D_iF_{\pi,i})(D_jF^{-1}_{\pi,i}) = 0 \text{ or } (D_iF_{\pi,i})(D_jF^{-1}_{\pi,i}) = 1,$$
depending on whether $j = i$. In either case, we have $D_jF^{-1}_{\pi,i} \lesssim \frac{1}{D_iF_{\pi,i}}D_jF_{\pi,i}$ and we can conclude that $\|D_jF^{-1}_{\pi,i}\|_{C^k} \lesssim \frac{L_1}{L_2}$. Let $\bsv$ be a multi-index such that $|\bsv| = k \geq 2$. Taking derivative with respect to $\bsv$ on both sides of \ref{eq:identity} gives
$$\|D^\bsv F_{\pi, i}^{-1}\|_{L^\infty} \lesssim \frac{1}{D_iF_{\pi, i}}\|F_{\pi, i}\|_{C^k}\sum_{n_1+ 2n_2...+ ln_l = k} \prod_{l}\|D^lF^{-1}\|^{n_l}_{L^\infty}.$$

Similar to the proof of Thm.\ref{thm:NormInverse}, this gives $\|F^{-1}_{\pi,i}\|_{C^k} \lesssim (\frac{L_1}{L_2})^{2k-1}L_1^k$. \rrtd{maybe this is also an application of theorem \ref{thm:NormInverse}????? Felt like I did the same thing as in the proof in the theorem}Note that for $H_i$ we have
$$\|H_i\|_{C^k} \leq \|T_1\|_{C^k} + ... + \|T_{i-1}\|_{C^k} + 1\lesssim \|T_{i-1}\|_{C^k}.$$

Iteratively, we can then compute $\|F^{-1}_{\pi, 2}\circ H_2\|_{C^k} \lesssim (\frac{L_1}{L_2})^{2k-1}L_1^k(\frac{L_1^{2k}}{L_2^{2k-1}})^k = \frac{L_1^{2k^2 + 3k -1}}{L_2^{2k^2 + k - 1}}$ and $\|T_2\|_{C^k} \lesssim \frac{L_1^{2k^2 + 3k -1}}{L_2^{2k^2 + k - 1}}L_1^k = \frac{L_1^{2k^2 + 4k -1}}{L_2^{2k^2 + k - 1}} = \Theta(L_1^{2k^2}L_2^{-2k^2})$ and $\|T\|_{C^k}\lesssim \|T_d\|_{C^k} = \Theta(L_1^{2k^d}L_2^{-2k^d})$.

Applying the results in Thm.\ref{thm:fNormGeneral} then gives the upper bound of $f$:
\begin{align*}
&\|f\|_{C^k} \leq\\ &\Theta\left((2M+1)^{2k-1}L_1^{2k^{d+1}}L_2^{-2k^{d+1}}L_1^{2k^{d-1}(d-1)(2k-1)}L_2^{-2k^{d-1}(d-1)(2k-1)}L_1^{2d(2k-1)}L_2^{-2d(2k-1)}\right)\\
&= O\left((2M+1)^{2k-1}L_1^{2k^{d+1}+2dk^{d}}L_2^{-2k^{d+1}-2dk^{d}}\right)
\end{align*}





\end{proof}




\ymmtd{There's no special case two for OT, but perhaps add a remark about it? Also, adjust the section hierarchy/naming.}


















\section{Distribution Learning via NeuralODEs} \ymmtd{Distribution approximation via neural ODEs}
\subsection{Distance between distributions}
Let $(\Omega, \mathcal{F}, \mu)$ be a probability space. For two probability measures $\rho\ll\mu, \pi\ll\mu$, we will consider convergence with respect to the following distances:
\begin{itemize}
\item $L^2$ distance: $d_{L^2}(\rho, \pi) = \left(\int_\Omega(\frac{d\rho(x)}{d\mu} - \frac{d\pi(x)}{d\mu})^2\mu(dx)\right)^{\frac{1}{2}}$.
    
    \item $\chi^2$-divergence: with the additional assumption that $\rho\ll\pi$, we define $d_{\chi^2}(\rho||\pi) = \int_{\Omega}(\frac{d\rho(x)}{d\pi}-1)^2 \pi(dx)$
    \item KL (Kullback-Leibler) divergence: with the additional assumption that $\rho\ll\pi$, we define $d_{KL}(\rho||\pi) = \int_\Omega \log(\frac{d\rho}{d\pi}(x))\rho(dx).$
    
    \item If $(\Omega, m)$ is also a metric space, then we also consider the Wasserstein distance of order $p$: $W_p(\rho, \pi) = \left(\inf_{\gamma\in\Gamma(\rho, \pi)}m(x, y)^p\gamma(dxdy)\right)^\frac{1}{p}$, where $\Gamma(\rho, \pi)$ is the set of all measures on $\Omega \times \Omega$ with marginals $\rho$ and $\pi$. In $\mathbb{R}^d$, this is simply $W_p(\rho, \pi) = \left(\inf_{\gamma\in\Gamma(\rho, \pi)}|x-y|^p\gamma(dxdy)\right)^\frac{1}{p}$.
    
\end{itemize}

\subsection{Bounding the distance between time-one push forward measure and target measure}\rrtd{possibly a better subsection title?}
Suppose we have an approximation $g$ to the velocity field $f$ from Thm.\ref{thm:f} such that $\|f - g\|_{W^{1,\infty}}\leq \delta$ for some $\delta > 0$. In this section, we shall show how the distance between $X_f(\cdot,1)_\sharp\pi$ and $X_g(\cdot,1)_\sharp\pi$ can be bounded, using different metrics introduced in the above section. 

\begin{theorem}\label{thm:l2bound}
Let $\pi, \rho$ be density functions supported on $\Omega_0, \Omega_1$ that satisfy the assumptions in \ref{sec:setup}. Let $T: \Omega_0\rightarrow\Omega_1$ be a transport map such that $T_\sharp\pi = \rho$ and satisfies \eqref{eq:spectrum}. Let $f$ be the $C^k$ velocity field that realize the straight-line constructions between $x$ and $T(x)$ from Thm.\ref{thm:f} and $g$ be an approximation for $f$ in $\mathcal{V}$ such that $\|f - g\|_{W^{1, \infty}} \leq \delta,$ then 
$$d_{L^2}^2(X_g(\cdot,1)_\sharp\pi, X_f(\cdot,1)_\sharp\pi) = d_{L^2}^2(\eta_g(x,1), \rho)  \leq \delta^2(\int_0^1|\nabla(\pi_s(x)) + \pi_s(x))|^2ds)e^{1 + \|\nabla \cdot g\|_{L_\infty}}.$$ 
\end{theorem}

\begin{proof}
First note we have two continuity equations given by the two velocity fields, $\partial_t\eta_f(x,t) = \nabla\cdot(\eta_f(x,t)f(x,t))$ and $\partial_t\eta_g(x,t) = \nabla\cdot(\eta_g(x,t)g(x,t))$ with the same initial condition $\eta_f(x,0) = \eta_g(x,0) = \pi(x)$. Taking the difference of the two equations and denoting $d_t(x) = \eta_f(x,t) -\eta_g(x,t)$, we have $\nabla\cdot(\eta_f(x,t)f - \eta_g(x,t)g) = \partial_td_t$, which can be equivalently expressed as $\nabla\cdot(\eta_f(f-g) + d_tg) = \partial_td_t$, or $\nabla\cdot\eta_f(f-g) + \nabla\cdot(d_tg) = \partial_td_t$.

If we multiply both sides by $d_t$, we get $(\nabla\cdot\eta_f(f-g))d_t + (\nabla\cdot(d_tg))d_t = (\partial_td_t)d_t$. Note $(\nabla\cdot(d_tg))d_t = (\nabla d_t\cdot g)d_t + (\nabla\cdot g)d_t^2 = (\nabla\cdot g)d_t^2 + \frac{1}{2}\left< g, \nabla d_t^2\right>$, where the last step is by the chain rule of derivatives. Similarly, we have $(\partial_td_t)d_t = \frac{1}{2}\partial_td_t^2$. Thus we have $(\nabla\cdot (\eta_f(f-g)))d_t + (\nabla\cdot  g)d_t^2 + \frac{1}{2}\left< g, \nabla d_t^2\right> = \frac{1}{2}\partial_td_t^2$. Integrating by parts, we get $$\int (\nabla\cdot  g)d_t^2 + \frac{1}{2}\int g\nabla d_t^2 = \int (\nabla\cdot  g)d_t^2 + \frac{1}{2}gd_t^2|_{\partial \Omega} - \frac{1}{2}\int (\nabla\cdot  g)d_t^2 = \frac{1}{2}\int (\nabla\cdot  g)d_t^2,$$ because the boundary term vanishes. \rrtd{so we nee the ReLU network $g$ to be compactly supported}Therefore, we get $\int (\nabla\cdot  g)d_t^2 + 2\int (\nabla\cdot (\pi_t(f-g)))d_t = \partial_t\|d_t\|_{L_2}^2$.

Note that this holds for every $t \in [0,1]$. Now we introduce a time dependent local error term $\epsilon_t(x) = |\partial_t\eta_f(x,t) - \nabla\cdot (\eta_f(x,t)g(x,t))| = |\nabla\cdot (\eta_f(f-g))|$. Applying Holder's inequality and Young's inequality to the first and second term respectively, we have $$\partial_t\|d_t\|_{L_2}^2 \leq |\int (\nabla\cdot  g)d_t^2| + 2|\int (\nabla\cdot (\eta_f(f-g)))d_t| \leq \|\nabla\cdot  g\|_{L_\infty}\|d_t\|_{L_2}^2 + \|d_t\|_{L_2}^2/C + C\|\epsilon_t\|_{L_2}^2$$ for any constant $C > 0$.

Then Gronwall's lemma gives $\|d_t\|_{L_2}^2 \leq C(\int_0^t\|\epsilon_s\|_{L_2}^2ds)\text{exp}(t/C+ \int_0^t\|\nabla\cdot  g\|_{L_\infty}ds)$.\rrtd{these norms are in the space variable for fixed time $s$} Note since we assumed that $\|f-g\|_{W^{1,\infty}} \leq \delta$, we have $$|\epsilon_s(x)| = |\nabla\cdot (\eta_f(x,s)(f-g))| =  |(\nabla \eta_f(x,s))\cdot(f-g) + \eta_f(x,s)\nabla\cdot (f-g)| \leq d\delta(1+\delta\|\nabla\eta_f(x,s)\|_{L^\infty})$$ for every $s$.\rrtd{needs to be checked} Therefore, we have $\int_0^t\|\epsilon_s\|_{L_2}^2ds \leq d^2\delta^2\int_0^t\int(1 + \|\nabla\eta_f(x,s)\|_{L^\infty})^2dxds$ and we have the bound $$\|d_t\|_{L_2}^2 \leq Cd^2\delta^2(\int_0^t\int(1 + \|\nabla\eta_f(x,s)\|_{L^\infty})^2dxds)\text{exp}(t/C+ \int_0^t\|\nabla\cdot  g\|_{L_\infty}ds).$$\rrtd{\int_0^t\|\nabla\cdot  g\|_{L_\infty}ds can be replaced by $t\|\nabla\cdot g\|_{L^\infty}$ where the infinity norm is taken over space time}

By the theory of characteristics, we have $\eta_f(x,t) = \pi(X^{-1}_f(x,t))e^{\int_0^t\nabla\cdot f(X_f(X^{-1}_f(x,t), s), s)ds}$, where $X^{-1}_f(x,t)$ denotes the inverse of the map $x\rightarrow X_f(x,t)$ when viewing $t$ as fixed. By the regularity of $f$ and $\pi$,  $\|\nabla\eta_f(x,s)\|_{L^\infty}$ is uniformly bounded. Therefore, we can conclude that as $\delta \rightarrow 0$, $\|d_t\|_{L^2}^2 \rightarrow 0$ for all $t\in[0,1]$. 

Now, specializing to the time-one flow map, since we know $T_\sharp\pi = \rho$, thus we have $\eta_f(\cdot,1) = \rho$, therefore, we have 
$$d_{L^2}^2(\eta_g(\cdot,1), \rho) \leq C\delta^2(\int_0^1|\nabla(\pi_s(x)) + \pi_s(x))|^2ds)\text{exp}(1/C+ \|\nabla\cdot  g\|_{L_\infty}).$$

Choosing $C = 1$ gives the result in the theorem. 





\end{proof}{}

\begin{remark}
Thm.\ref{thm:l2bound} shows that the error between the two push forward measures accumulates in time. Therefore, if the end time of the ODE is increased, more accurate approximations will be needed (so that $\delta$ is small) to maintain similar error level.


\end{remark}
\begin{remark}
For the purpose of controlling $L^2$ distance between the densities, it suffices to have control of the the velocity field together with the first order derivatives. However, previous results show that having additional information about higher order derivatives could make the neural network approximation more accurate. 
\end{remark}


\begin{corollary}\label{corollary:KLandChisquare}
In the context of Thm. \ref{thm:l2bound}, using the assumption that the density $\rho$ is bounded from below by a constant $L$, the $\chi^2$ divergence and $KL$ divergence between the pushforward measure of the time-one flow map and the target measure can be bounded as follows:

$$d_{\chi^2}(X_g(\cdot,1)_\sharp\pi||X_f(\cdot,1)_\sharp\pi) \leq \frac{1}{L}d_{L^2}^2(\eta_g(x,1), \rho(x))$$ and $$d_{KL}(X_g(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi)\leq \log(\frac{1}{L}d_{L^2}^2(\eta_g(x,1), \rho(x))+1)$$
\end{corollary}
\begin{proof}
For $\chi^2$ divergence, we have $d_{\chi^2}(X_g(\cdot,1)_\sharp\pi||X_f(\cdot,1)_\sharp\pi) = d_{\chi^2}(X_g(\cdot,1)_\sharp\pi||\rho) = \int_{\Omega_1}\frac{(\eta_g(x,1) - \rho(x))^2}{\rho(x)}dx \leq \frac{1}{L}\int_{\Omega_1}(\eta_g(x,1) - \rho(x))^2dx \leq \frac{1}{L}d_{L^2}^2(X_g(\cdot,1)_\sharp\pi, X_f(\cdot,1)_\sharp\pi).$







To get an upper bound for $d_{KL}(X_g(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi)$, we make use of the fact that KL divergence is bounded from above by $\chi^2$ divergence. That is, $d_{KL}(X_g(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi) = \mathbb{E}_{\eta_g(\cdot,1)}[\log \frac{\eta_g(x,1)}{\rho(x)}] \leq \log \mathbb{E}_{\eta_g(x,1)}[\frac{\eta_g(x,1)}{\rho(x)}] = \log\int_{\Omega_1} \frac{\eta_g(x,1)^2}{\rho(x)}dx = \log(\int_{\Omega_1} \frac{(\rho(x)-\eta_g(x,1))^2}{\rho(x)}dx + 1) \leq \log(\int_{\Omega_1} \frac{(\rho(x)-\eta_g(x,1))^2}{\pi_1(x)}dx + 1) \leq \log(\frac{1}{L}d_{L^2}^2(\eta_g(x,1), \rho) +1)$. 
\end{proof}


Given error of approximation of the velocity field, we can derive the error of approximation of the transport map in $W^{1,\infty}$ as follows:
\begin{theorem}\label{thm:FlowMapBound}
Let $f$, $g\in\mathcal{V}$ such that $\|f - g\|_{W^{1,\infty}(\Omega)}\leq \epsilon$ and $\|f\|_{W^{1,\infty}(\Omega)}, \|g\|_{W^{1,\infty}(\Omega)}\leq C$. Moreover, assume $f\in C^k(\Omega)$ for $k \geq 1$ and that 
$L:=\sup_{t\in [0,1]}{\rm Lip}(\nabla_x f(\cdot,t))$.
% such that $\sup_{t\in[0,1]}\|f(\cdot,t)\|_{W^{1,\infty}(D)} \leq C$ and $\sup_{t\in[0,1]}\|g(\cdot,t)\|_{W^{1,\infty}(D)} \leq C$. 
 Let $X$, $Y:\Omega\to\mathbb{R}^d$ be the solutions to 
 \begin{equation*}
 \frac{dX(x,t)}{dt} = f(X(x,t),t),\qquad \frac{dY(x,t)}{dt} = g(Y(x,t),t) 
 \end{equation*}
 with initial condition $X(x,0) = Y(x,0) = x$.
For $x\in \OMega_0$ set
\begin{equation*}
T(x) = x + \int_0^1f(X(x,t),t)dt\qquad\text{and}\qquad G(x) = x + \int_0^1g(Y(x,t),t)dt.
\end{equation*}

Then $\|T - G\|_{W^{1, \infty}(\Omega_0)} \leq   \frac{Le^{3dC} + 2de^{2dC}}{2dC}\epsilon$.
\end{theorem}
\begin{proof}
First, note that for fixed $x\in\Omega_0$, $|\frac{dX(x,t)}{dt} - \frac{dY(x,t)}{dt}| = |f(X(x,t),t) - g(Y(x,t),t)| \leq |f(X(x,t),t) - f(Y(x,t), t)| + |f(Y(x,t), t) - g(Y(x,t),t)|$. By $\|f - g\|_{W^{1,\infty}}\leq \epsilon$, we have $|f(Y(x,t), t) - g(Y(x,t),t)| \leq \epsilon, \forall (x, t)$. On the other hand, since $\|f\|_{W^{1,\infty}} \leq C$, we have $\max_{i,j}\|(\nabla f)_{i,j}\|_{L^\infty}\leq C$. By Lemma \ref{lemma:relationBetweenNorms} for each $t\in[0,1]$, $\|\nabla_x f(\cdot,t)\|_{L^\infty(\mathbb{R}^d, \mathbb{R}^{d}\times\mathbb{R}^d)} \leq dC$. Since $f\in C^k$ when viewed as a function of $x\in\Omega_0$, its spatial Lipschitz constant is bounded by $dC$. Therefore, we can conclude that $|f(X(x,t),t) - f(Y(x,t), t)| \leq dC|X(x,t) - Y(x,t)|.$ Then, we have 
\begin{align*}
&|X(x,t) - Y(x,t)| = |\int_0^tf(X(x,s),s) - g(Y(x,s),s)ds|\\
&\leq \int_0^t|f(X(x,s),s) - g(Y(x,s),s)|ds\\
&\leq \int_0^tdC|X(x,s) - Y(x,s)|ds + t\epsilon    
\end{align*}




%On the other hand, note that for Euclidean norm $|\cdot|$ and $v(t)\in\mathbb{R}^d$ a vector depending on $t$, we have $2|v(t)|\frac{d}{dt}|v(t)|=\frac{d}{dt}|v(t)|^2 = \frac{d}{dt}(v\cdot v) = 2v\cdot\frac{dv(t)}{dt} \leq 2|v(t)||\frac{dv(t)}{dt}|$, which gives $\frac{d}{dt}|v(t)| \leq |\frac{dv(t)}{dt}|$. \sw{[This can hold at most almost everywhere, no?]}

%Putting things together, we have $$\frac{d}{dt}|X(x,t) - Y(x,t)|\leq |\frac{d}{dt}(X(x,t) - Y(x,t))|\leq dC|X(x,t) - Y(x,t)| + \epsilon.$$
It holds that $|X(x,s) - Y(x,s)|$ is a continuous function in $s$. Using Gronwall's inequality, we get  $|T(x) - G(x)| = |X(x,1) - Y(x,1)|\leq \epsilon e^{dC}, \forall x$. Therefore $\max_{j}\|X_j(\cdot,1)-Y_j(\cdot,1)\|_{L^\infty}\leq \epsilon e^{dC}$.

Now, it remains to bound $\max_{i,j}\|(\nabla_x T(\cdot) - \nabla_x G(\cdot))_{i,j}\|_{L^\infty}$, which could be achieved by bounding the Frobenius norm of the difference in Jacobian $\|\nabla_x T(x) - \nabla_xG(x)\|_F$ \sw{by equivalence of norms}. 

Similarly we can write 
\begin{align*}
&\|\nabla_x X(x,t) - \nabla_xY(x,t)\|_F = \|\int_0^t\nabla_x(f(X(x,s),s) - g(X(x,s),s))\|_Fds\\
&\leq \int_0^t\|\nabla_x(f(X(x,s),s) - g(X(x,s),s))ds\|_F\\
&= \int_0^t\|\left(\nabla_Xf(X(x,s),s)\nabla_xX(x,s) - \nabla_Yg(Y(x,s),s)\nabla_xY(x,s)\right)\|_Fds\\
&= \int_0^t(\|\nabla_x Y(x,s)(\nabla_Y(f(Y(x,s),s)) - \nabla_Yg(Y(x,s),s)) + \nabla_xY(x,s)(\nabla_Xf(X(x,s),s)-\nabla_Yf(Y(x,s),s)) +\\
&\nabla_Xf(X(x,s),s)(\nabla_xX(x,s) - \nabla_xY(x,s))\|_F)ds\\
&\leq \int_0^t\|\nabla_x Y(x,s)(\nabla_Y(f(Y(x,s),s)) - \nabla_Yg(Y(x,s),s))\|_Fds\\  &+\int_0^t\|\nabla_xY(x,s)(\nabla_Xf(X(x,s),s)-\nabla_Yf(Y(x,s),s))\|_Fds \\ &+\int_0^t\|\nabla_Xf(X(x,s),s)(\nabla_xX(x,s) - \nabla_xY(x,s))\|_Fds\\
\end{align*}
Since $\|f - g\|_{W^{1, \infty}} \leq \epsilon$, we have $\|(\nabla_Y(f(Y(x,t),t)) - \nabla_Yg(Y(x,t),t))_{i,j}\|_{L^\infty} \leq \epsilon, \forall (x,t)$ and $(i,j)$. To establish bounds on $\nabla_xY(x,t)$, note that $\nabla_xY(x,t)$ satisfies:
$$\nabla_x Y(x,t) = \int_0^t\nabla_xg(Y(x,s),s)ds = \int_0^t(\nabla_Yg(Y(x,s), s))\nabla_xY(x,s)ds .$$
Using the Frobenius norm, we have at all points $(x,t)$, the following holds
$$\|\nabla_x Y(x,t)\|_F\leq\int_0^t\|\nabla_Yg(Y(x,s),s)\|_F\|\nabla_x Y(x,s)\|_Fds\leq dC\int_0^t\|\nabla_xY(x,s)\|_Fds.$$

One caveat to note here is that since our $g$ is only assumed to be lipschitz continuous, the Jacobian $\nabla g$ may not be defined everywhere. However, it can only be non-defined on a countable set of points with measure zero, so this does not affect the integrals. 



By Gronwall's inequality, we have $\|\nabla_x Y(x,t)\|_F \leq e^{dCt}$ and $\|\nabla_x Y(x,1)\|_F \leq e^{dC}$. Therefore, the first term may be bounded as 
\begin{align*}
&\|\nabla_x Y(x,s)(\nabla_Y(f(Y(x,s),s)) - \nabla_Yg(Y(x,s),s))\|_F \leq \|\nabla_x Y(x,s)\|_F\|\nabla_Y(f(Y(x,s),s)) - \nabla_Yg(Y(x,s),s)\|_F\\
&\leq  d\epsilon e^{dCs},    
\end{align*}
 



and we have $$\int_0^t\|\nabla_x Y(x,s)(\nabla_Y(f(Y(x,s),s)) - \nabla_Yg(Y(x,s),s))\|_Fds \leq \int_0^td\epsilon e^{dCs}ds = \frac{(e^{dCt}-1)\epsilon}{C}\leq\frac{(e^{dC}-1)\epsilon}{C} $$

To bound the second term, note that $f\in C^k$ on a compact domain implies that $\nabla_xf$ is Lipschitz. Then, there exists a constant $L$ such that at any $(x,t)$,  $\|\nabla_xf(X(x,t),t)-\nabla_xf(Y(x,t),t)\|_F\leq L|X(x,t) - Y(x,t)|$. Note $|X(x,t) - Y(x,t)| \leq |X(x,1) - Y(x,1)|\leq \epsilon e^{dCt}$ from the previous part. Then, we can conclude that
\begin{align*}
&\|\nabla_xY(x,s)(\nabla_xf(X(x,s),s)-\nabla_xf(Y(x,s),s))\|_F \leq \|\nabla_xY(x,s)\|_F\|\nabla_xf(X(x,s),s)-\nabla_xf(Y(x,s),s)\|_F\\
&\leq Le^{dCs}\epsilon e^{dCs} = L\epsilon e^{2dCs}.
\end{align*}
Then, we have 
$$\int_0^t\|\nabla_xY(x,s)(\nabla_xf(X(x,s),s)-\nabla_xf(Y(x,s),s))\|_Fds \leq \int_0^tL\epsilon e^{2dCs} = \frac{(e^{2dCt} - 1)L\epsilon}{2dC}$$



Finally, for the third term, we have $$\|\nabla_Xf(X(x,s),s)(\nabla_xX(x,s) - \nabla_xY(x,s))\|_F\leq\|\nabla_Xf(X(x,s),t)\|_F\|\nabla_xX(x,s) - \nabla_xY(x,s)\|_F,$$
where we can bound $\|\nabla_xf(X(x,s),s)\|_F$ from above by $dC$. 



Combining all the terms, we obtain
$$\|\nabla_x (X(x,t) -Y(x,t))\|_F\leq \frac{(e^{2dCt} - 1)L\epsilon}{2dC} + \frac{(e^{dC}-1)\epsilon}{C} + dC\int_0^t\|\nabla_xX(x,s) - \nabla_xY(x,s)\|_Fds$$

Gronwall's inequality then gives $$\|\nabla_x (X(x,1) -Y(x,1))\|_F \leq \epsilon \frac{(e^{2dC}-1)L + 2(e^{dC}-1)d}{2dC}e^{dC}\leq \frac{Le^{3dC} + 2de^{2dC}}{2dC}\epsilon$$.  

Since the above inequality holds for all $x$, considering pointwise entries in the Jacobian gives $$\max_{i,j}\|(\nabla_xX(\cdot,1) -\nabla_xY(\cdot,1))_{i,j}\|_{L^\infty} \leq \frac{Le^{3dC} + 2de^{2dC}}{2dC}\epsilon$$

Using $W^{1, \infty}$ norm, we can conclude that $\|T(x) - G(x)\|_{W^{1,\infty}} = \|X(x,1) - Y(x,1)\|_{W^{1,\infty}} \leq \max\{\epsilon e^{dC},  \frac{Le^{3dC} + 2de^{2dC}}{2dC}\epsilon\} \leq \frac{Le^{3dC} + 2de^{2dC}}{2dC}\epsilon$.
\end{proof}
\begin{corollary}
In the context of Thm.\ref{thm:FlowMapBound}, assume $f$ is the velocity field from Thm.\ref{thm:f} whose time one flow map realizes a transport map that pushes forward $\pi$ to $\rho$ and $g$ is a neural network approximation of $f$. Then, the Wasserstein distance between $\rho$ and $\eta_{g}(\cdot, 1)$ is bounded by as follows:
$$W_p(\rho, \eta_{g}(\cdot, 1)) \leq \epsilon e^{dC},$$
for $p \geq 1$.
\end{corollary}
\begin{proof}
From the proof of Thm.\ref{thm:FlowMapBound}, $\max_j \|X_j(\cdot, 1) - Y_j(\cdot, 1)\|_{L^\infty}\leq \epsilon e^{dC}$. By Thm.1 in \cite{Wassersteinbound}, the Wasserstein distance between the push-forward measures is also bounded by $\epsilon e^{dC}$ for $p \geq 1$. 
\end{proof}
\begin{remark}
To establish error between the push-forward measures in Wasserstein distance, it is enough to bound the $L^\infty$ approximation error of the transport map, which only requires $L^\infty$ approximation error of the velocity field. However, since the training objective in \ref{eq:OP} involves first order derivatives of the velocity field, we shall consider approximation of the velocity field in $W^{1, \infty}$. 
\end{remark}







\section{Neural Network Approximation Results}
\ymmtd{Give a map of the section. We cite some prior results on NN approximation. Then we can simply apply these to our results on regularity of the velocity field (either the general result or the specific KR version). But the main new result in this section is Thm 6.5, which shows \ldots }

There are a number of prior works concerning approximation of smooth functions by neural networks. \cite{NNApproximation1} shows that deep ReLU networks approximate smooth functions more efficiently than shallow networks and also proves upper and lower bound for the complexity of networks for approximation in the setting of Sobolev spaces. Later, \cite{NNApproximation3} and \cite{NNApproximation4} explore the phase diagram for feasible approximation rates of neural networks. All these works concern approximation of Sobolev functions on finite dimensional cubes; however, $L^\infty$ norm is used to quantify approximation errors in these works. In \cite{NNApproximation2}, approximation rates in terms of $W^{k,p}$ norm are established, for $0\leq k\leq 1$. 


In the definition of neural networks, we distinguish between a neural network as a set of weights and an associated function that we call the realization of the neural network.

\begin{definition}
Let $d, L\in \mathbb{N}$, a neural network $\Phi$ with input dimension $d$ and $L$ layers is a sequence of matrix-vector tuples $\phi = ((A_1,b_1), (A_2,b_2),..., (A_L, b_L))$, where for $N_0=d$,$N_1,...N_L\in\mathbb{N}$, $A_l$ is a $N_l \times \sum_{k=0}^{l-1}N_k$ matrix and $b_l \in \mathbb{R}^{N_l}$. Let $\rho:\mathbb{R}\rightarrow\mathbb{R}$ be ReLU activation, then the realization of the neural network $\Psi$ is the map $R(\Psi):\mathbb{R}^d\rightarrow\mathbb{R}^{N_L}, R(\Psi)(x) = x_L$ where $x_L$ is defined through the following recursive scheme:
\begin{itemize}
    \item $x_0:= x$
    \item $x_l = \rho(A_l[x_0^T|...|x^T_{l-1}]^T + b_l)$ for $l=1,..,L-1$
    \item $x_L = A_L[x_0^T|...|x^T_{L-1}]^T + b_L$.
\end{itemize}
$N(\Psi) = d + \sum_{j=1}^L N_j$ is called the number of neurons of $\Psi$ and $L = L(\Psi)$ is called the number of layers and $W(\Psi) = \sum_{j=1}^L \|A_j\|_0 + \|b_j\|_0$ is called the number of weights. 
\end{definition}
\rrtd{this notation for NN architecture introduced in \cite{NNApproximation2} is nonstandard, but we need the nn parallelization result in this paper, which needs the architecture.}



\begin{theorem}[{\cite[Thm. 4.2]{NNApproximation2}}]\label{thm:NNApproximation}
Let $\mathcal{F}_{n, d, p, B}$ be the Sobolev ball $\mathcal{F}_{n, d, p, B} := \{f\in W^{n, p}((0,1)^d)): \|f\|_{W^{n, p}((0,1)^d))} \leq B\}$. Let $d \in \mathbb{N}$, $n\in\mathbb{N}_{\geq 2}, 1 \leq p \leq \infty, B > 0, 0 < s < 1$. Then, there exists a constant $c = c(d, n, p, B, s)$ with the following properties:

For any $\epsilon \in (0, 1/2)$, there is a neural network architecture $\mathcal{A} = \mathcal{A}(d, n, p, B, s, \epsilon)$ with $d-$dimensional input and 1-dimensional output such that for every $f\in \mathcal{F}_{n, d, p, B}$, there is a neural network $\Psi_\epsilon^f$ that has architecture $\mathcal{A}_\epsilon$ such that 
$\|R_\rho(\Psi_\epsilon^f) - f\|_{W^{s,p}((0,1)^d)}\leq \epsilon$ and
\begin{itemize}
    \item $L(A_\epsilon) \leq c\log_2(\epsilon^{-n/(n-s)})$.
    \item $W(\mathcal{A}_\epsilon) \leq c\epsilon^{-d/(n-s)}\log_2(\epsilon^{-n/(n-s)})$.
    \item $N(\mathcal{A}_\epsilon) \leq c\epsilon^{-d/(n-s)}\log_2(\epsilon^{-n/(n-s)})$
\end{itemize}

\end{theorem}
Then, we have the following corollary when applied to the Knoth-Rosenblatt map constructed directly from the densities. 
\begin{corollary}\label{corollary:NetParameterBound}
Let $\pi$ and $\rho$ be measures supported on $\Omega_0$ and $\Omega_1$ respectively and $\forall x \in \Omega_0\cup\Omega_1, |x| < M$. Suppose the domains and the densities satisfy assumptions \ref{AssumptionDensity1} and \ref{AssumptionDensity2}. Then, there exists a neural network architecture $\mathcal{A}_{\epsilon}$ whose size is bounded as follows and approximates the Knoth-Rosenblatt map within $\epsilon$ error in $W^{1,\infty}$ norm.
\begin{itemize}
    \item $L(A_\epsilon) = O\left(\log_2\epsilon + \log_2M + (2k^{d+1}+2dk^d)\log_2\frac{L_1}{L_2}) \right)$
    \item $W(\mathcal{A}_\epsilon) = O(\epsilon^{-d/(k-1)}\log_2\epsilon + )$
    
    
\end{itemize}
\end{corollary}
\rrtd{Still looking for the best way to express this}. 
\begin{remark}
Some caveats here: 
\begin{itemize}
    \item The neural network approximation from Thm.\ref{thm:NNApproximation} only work for functions on the unit hypercube $(0,1)^d$. The velocity field we constructed from the minimal energy (straight line) regularization will in general not be the unit hypercube. However, from Thm.\ref{thm:LipDomain}, we know it will always be a Lipschitz domain and therefore, can be extended to a larger hypercube that covers $\Omega$ by Thm.\ref{thm:functionExtension} and we may perform the approximation on this larger hypercube. This scaling would in general introduce a factor that is exponential in $d$ in the network parameters. However, since we already have factors exponential in $d$ in the expressions and we choose to not keep track of this layer here. 
    \item In the big $O$ notation, we treat $d$ and $k$ as constants and only care about the asymptotic behaviour of $\epsilon, M, L_1, L_2$. 
  
\end{itemize}
\end{remark}





Combining the neural network approximation results and the regularity results from previous sections, we shall show that if we search over the class of neural networks with length and weights upper bounded by a certain number, then there is guarantees to exist a network that achieves small error in the optimization problem \ref{eq:OP}.
\begin{theorem}
Let $\pi$ and $\rho$ be measures supported on $\Omega_0$ and $\Omega_1$ respectively and $\forall x\in \Omega_0\cup\Omega_1, |x| < M$. Suppose the domains and the densities satisfy assumptions \ref{AssumptionDensity1} and \ref{AssumptionDensity2}. Let $\mathcal{A}_\epsilon$ be the neural network architectures that satisfy the bounds in Corollary \ref{corollary:NetParameterBound}. Let $f$ be the velocity field from Thm.\ref{thm:ExistenceOfOptimalSolution} corresponding to the Knoth-Rosenblatt transport map $T$ that pushes forward $\pi$ to $\rho$ and $J$ the optimization objective from \ref{eq:OP}. Then, there exists a realization of this network architecture, $\Psi^f_\epsilon$ such that 
$$|J(f) - J(\Psi^f_\epsilon)| \leq \log\left(1+\frac{2\epsilon^2L_1^2}{L_2}e^{1 + dC} \right) + \left(\epsilon Ce^{dC}(L+ K_1) + 2C\epsilon + K_2\epsilon e^{dC} + \epsilon\right)^2,$$
where $C$ is a finite number such that $\|f\|_{W^{1,\infty}} \leq C$ and $\|\Psi^f_\epsilon\|_{W^{1,\infty}} \leq C$. 

\end{theorem}
\begin{proof}
By Thm.\ref{thm:ExistenceOfOptimalSolution}, velocity field $f$ satisfies $J(f) = 0$. By Corollary \ref{corollary:NetParameterBound}, there exists a realization $\Psi_\epsilon^f_\epsilon$ of the architecture $\mathcal{A}_\epsilon$ such that $\|\Psi_\epsilon^f- f\|_{W^{1,\infty}} \leq \epsilon$. 

Recall the optimization objective $J$ has two parts, KL divergence and regularization: $ J = d_\text{KL}[X(x,1)_\sharp\pi||\rho] + \lambda\mathbb{E}_{x\sim\rho(x)}[R(x,1)]$. For the KL part, corollary \ref{corollary:KLandChisquare} gives 
\begin{align*}
&d_{KL}(X_{\Psi_\epsilon^f}(\cdot,1)_\sharp\pi|| X_f(\cdot,1)_\sharp\pi) = d_{KL}(X_{\Psi_\epsilon^f}(\cdot,1)_\sharp\pi|| \rho)\leq \log(\frac{1}{L_2}d_{L^2}^2(\eta_{\Psi_\epsilon^f}(x,1), \rho(x))+1)\\
&\leq \log\left(\frac{1}{L_2}\epsilon^2(\int_0^1|\nabla(\pi_s(x)) + \pi_s(x))|^2ds)e^{1 + \|\nabla\cdot \Psi_\epsilon^f\|_{L_\infty}} + 1\right)\leq \log\left(1+\frac{2\epsilon^2L_1^2}{L_2}e^{1 + \|\nabla\cdot \Psi_\epsilon^f\|_{L_\infty}} \right).
\end{align*}

For the regularization part, recall that 
$R(x,1) = \int_0^1  |\nabla_X(f(X(x,t),t))f(X(x,t),t) + \partial_tf(X(x,t),t)|^2dt.$ Now, consider the pointwise value at $(x,s)\in\Omega_0\times[0,1]$ and define $r_f(x,t) = \nabla_X(f(X_f(x,t),t))f(X_f(x,t),t) + \partial_tf(X_f(x,t),t)$ and $r_{ \Psi_\epsilon^f}(x,t) = \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t) + \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)$. 

We shall then compute the pointwise value $|r_f(x,t) - r_{ \Psi_\epsilon^f}(x,t)|$. 

First, note $|f(X_f(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq |f(X_f(x,t),t) - f(X_{ \Psi_\epsilon^f}(x,t),t)| + |f(X_{ \Psi_\epsilon^f}(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|$. By $W^{1,\infty}$ approximation, $|f(X_{ \Psi_\epsilon^f}(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq \epsilon$. By Thm.\ref{thm:FlowMapBound}, the flow maps $X_{ \Psi_\epsilon^f}(x,t)$ and $X_f(x,t)$ satisfies $|X_{ \Psi_\epsilon^f}(x,t) - X_f(x,t)| \leq \epsilon e^{dC}$. Let $K_1$ be the spatial Lipschitz constant of $f$ \rrtd{needs to be worked out}, then $|f(X_f(x,t),t) - f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq K_1\epsilon e^{dC}$ and $|f(X_f(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq K_1\epsilon e^{dC} + \epsilon$. 

Next, consider $|\partial_tf(X_f(x,t),t) - \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|$. Similarly, we have $$|\partial_tf(X_f(x,t),t) - \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq |\partial_tf(X_f(x,t),t) - \partial_tf(X_{ \Psi_\epsilon^f}(x,t),t)|  + |\partial_tf(X_{ \Psi_\epsilon^f}(x,t),t) - \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|$$
Again by $W^{1,\infty}$ approximation, $|\partial_tf(X_{ \Psi_\epsilon^f}(x,t),t) - \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq \epsilon$. Let $K_2$ be the Lipschitz constant of $\partial_t f$ \rrtd{is this different from $K_1$?}, and we have $|\partial_tf(X_f(x,t),t) - \partial_t\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq K_2\epsilon e^{dC} + \epsilon$. 

Finally, consider $|\nabla_X(f(X_f(x,t),t)) - \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|.$ 
\begin{align*}
&|\nabla_Xf(X_f(x,t),t) - \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq |\nabla_Xf(X_f(x,t),t) - \nabla_X f(X_{ \Psi_\epsilon^f}(x,t),t)| \\
&+ |\nabla_X f(X_{ \Psi_\epsilon^f}(x,t),t) - \nabla_X\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)| \leq L\epsilon e^{dC} + \epsilon,   
\end{align*}
where $L$ is the Lipschitz constant from Thm.\ref{thm:FlowMapBound}.\rrtd{again can be worked out}

Now, we can bound $|\nabla_X(f(X_f(x,t),t))f(X_f(x,t),t) - \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|$ as follows:
\begin{align*}
&|\nabla_X(f(X_f(x,t),t))f(X_f(x,t),t) - \nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)\Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|\\
&\leq |\nabla_X(f(X_f(x,t),t))-\nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|f(X_f(x,t),t)| \\
&+ |\nabla_X \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)||f(X_f(x,t),t) - \Psi_\epsilon^f(X_{ \Psi_\epsilon^f}(x,t),t)|\\
&\leq (L\epsilon e^{dC} + \epsilon)C + C(K_1\epsilon e^{dC} + \epsilon) = \epsilon Ce^{dC}(L+ K_1) + 2C\epsilon
\end{align*}

Putting things together, we have $|r_f(x,t) - r_{ \Psi_\epsilon^f}(x,t)| \leq \epsilon Ce^{dC}(L+ K_1) + 2C\epsilon + K_2\epsilon e^{dC} + \epsilon$. 

Therefore, 
\begin{align*}
&|\int_0^1|r_f(x,t)|^2dt - \int_0^1|r_{ \Psi_\epsilon^f}(x,t)|^2dt| \\
&\leq \int_0^1 |r_f(x,t) - r_{ \Psi_\epsilon^f}(x,t)|^2dt \leq \left(\epsilon Ce^{dC}(L+ K_1) + 2C\epsilon + K_2\epsilon e^{dC} + \epsilon\right)^2   
\end{align*}
\rrtd{Is this sequence of inequalities correct? }

Then we can conclude the training objectives satisfy
$$|J(f) - J(\Psi^f_\epsilon)| \leq \log\left(1+\frac{2\epsilon^2L_1^2}{L_2}e^{1 + dC} \right) + \left(\epsilon Ce^{dC}(L+ K_1) + 2C\epsilon + K_2\epsilon e^{dC} + \epsilon\right)^2   $$





\end{proof}


















\section{Discussion and Directions for future exploration}
(to be written, just an outline for now)
\begin{itemize}
    \item Statistical aspect: for the empirical risk minimization problem $\ref{ERM}$, can we obtain high probability bounds on the distance between the time-one flow map of the NeuralODE learned through finite samples in $\pi$ with $\rho$? This needs some complexity measure of the function class represented by NeuralODEs combined with empirical process theory.
    
    \item Computational aspect: can we theoretically analyze the discretize-then-optimize approach vs the optimize-then-discretize approach to training neuralODEs? How do these approaches impact the generalization of the model? 
    
    
    
    \item time scaling and other regularization techniques: how to express this formally?
\end{itemize}





















\newpage

\appendix








\section{Exact Computation of the Jacobian}\label{ExactCompJacobian}
Training NeuralODEs consists of minimizing the (regularized) loss over the network weights subject to the ODE constraint. The adjoint-based methods in \cite{NeuralODE}, \cite{ffjord} and \cite{HowToTrain} can be viewed as an optimize-then-discretize approach since they optimize the continuous ODE and discretize the optimal dynamics after training. Alternatively, a discretize-then-optimize approach is proposed in \cite{OTFlow}, \cite{DiscretizeOptimize}, where one first disciretizes the continuous dynamics and then solve a finite dimensional optimization problem. Since the training objective proposed in our work involves the entire Jacobian matrix of the velocity field, it is natural to use the discretize-then-optimize approach, which allows exact computation of the Jacobian. As in \cite{OTFlow}, the velocity field can be implemented as a ResNet and we can compute the Jacobian recursively. 

Let $s = (x,t)$ be the new variable formed by appending the time variable to the space variable, we then have the recursive relation in a $M$-layer ResNet, where the $u_i$'s are the outputs from intermediate layers:
\begin{align*}
&u_0 = \sigma(K_0s + b_0)\\
&u_1 = u_0 + h\sigma(K_1u_0 + b_1)\\
& ....\\
& u_M = u_{M-1} + h\sigma(K_Mu_{M-1} + b_M)\\
\end{align*}
Taking the gradient with respect to variable $s$, we have $\nabla_s u_i^T = \nabla_s u_{i-1} + h\sigma'(K_iu_{i-1} + b_i)K^T_i\nabla_s u_{i-1}$. 
Therefore, we have the update rule for the Jacobian: $$J \leftarrow J+h\sigma'(K_iu_{i-1} + b_i)K_i^TJ.$$	
The network parameters $K_i$ and $b_i$ are to be learned. Since we use a discretized version of the velocity field in the implementation, these parameters can be updated through automatic differentiation. 







\section{Knothe--Rosenblatt Construction of Triangular Transport Map}\label{app:KRMap}	
Given probability measures $\rho$ and $\pi$, under certain conditions, the Knothe–Rosenblatt transport is the (unique) triangular monotone transport $T$ such that $T_\sharp\pi = \rho$. In this section, we describe the explicit Knothe--Rosenblatt construction of triangular transport maps, as presented in \cite{OTAppliedMathematician}. Let $d\in\mathbb{N}$ be the dimension. For simplicity of presentation, we assume the support of $\pi$ and $\rho$ is the hypercube $[0,1]^d$. Let $\mu$ be a base measure (for example the Lebesgue measure) and assume that $\frac{d\pi}{d\mu} = \pi(x)\in C^0([0,1]^d, \mathbb{R}^+)$ and $\frac{d\rho}{d\mu} = \rho(x)\in C^0([0,1]^d, \mathbb{R}^+)$ be the corresponding densities. 

For continuous density function $f$, we define the following auxiliary functions for $x\in[0,1]^k, k\leq d$:
\begin{equation}
\begin{aligned}
\hat{f}_k(x) &= \int_{[0,1]^{d-k}}f(x, t_{k+1},...,t_d)d\mu((t_j)_{j=k+1}^d)\\
f_k(x) &= \frac{\hat{f}_k(x)}{\hat{f}_{k-1}(x_{[k-1]})}.
\end{aligned}
\end{equation}
Note that $f_k(x_{[k-1}, \cdot)$ is the marginal density of the variable $x_k$ conditioned on $x_{[k-1]} = (x_1,...,x_{k-1})\in[0,1]^{k-1}$. Then, we define the corresponding CDFs:
\begin{equation}
\begin{aligned}
F_{\pi,k}(x_{[k-1]}, x_k) &= \int_{0}^{x_k}\pi_k(x_{[k-1]}, t_{k})d\mu(t_k)\\
F_{\rho,k}(x_{[k-1]}, x_k) &= \int_{0}^{x_k}\rho_k(x_{[k-1]}, t_{k})d\mu(t_k),
\end{aligned}
\end{equation}
which are well-defined for $x\in[0,1]^k$ and $k\in\{1,...,d\}$. Note that these are interpreted as functions of the last variable $x_k$ with $x_{[k-1]}$ fixed. In particular, we let $F_{\rho,k}(x_{[k-1]}, \cdot)^{-1}$ to be the inverse of the map $x_k\rightarrow F_{\rho,k}(x_{[k-1]}, x_k)$

For $x \in [0,1]^d$, the Knothe--Rosenblatt map is defined recursively in the following way:
$$T_1(x_1) = F_{\rho,1}^{-1}\circ F_{\pi,1}(x_1),$$
and for $k > 1$, we define
$$T_k(x_{[k-1]}, \cdot) = F_{\rho,k}(T_1(x_1),...,T_{k-1}(x_{[k-1]}), \cdot)^{-1}\circ F_{\pi,k}(x_{[k-1]}, \cdot).$$
Then the map $$T(x_1,..,x_d) = [T_1(x_1), T_2(x_{[2]}),...,T_d(x_{[d]})]^T$$
yields the triangular Knothe–Rosenblatt transport $T:[0,1]^d\rightarrow[0,1]^d$ and we have the following theorem:
\begin{theorem}
The triangular Knothe–Rosenblatt satisfies $T_\sharp\pi = \rho$; that is, $\det\nabla T(x)\rho(T(x)) = \pi(x), \forall x\in[0,1]^d$.
\end{theorem}
	
	
	
	
	
	
\bibliography{ref.bib}

