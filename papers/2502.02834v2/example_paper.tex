%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algpseudocode}
% \algrenewcommand\algorithmicrequire{\textbf{Input:}}
% \algrenewcommand\algorithmicensure{\textbf{Output:}}


% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{multirow}
\usepackage{arydshln}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


% PEARL에 있던것 
\newcommand{\Task}{\mathcal{T}}
\newcommand{\task}{\mathcal{T}}

\newcommand{\Policy}{\pi_{\theta}}
\newcommand{\Context}{C} % r.v.
\newcommand{\context}{\mathbf{c}} % sample
\newcommand{\Z}{Z}
\newcommand{\z}{\mathbf{z}}
\newcommand{\state}{\mathbf{s}}
\newcommand{\act}{\mathbf{a}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\pluseq}{\mathrel{+}=}

\newcommand{\don}{\mathcal{D}_{\text{on}}}
\newcommand{\doff}{\mathcal{D}_{\text{off}}}

\newcommand{\zon}{z_{\text{on}}}
\newcommand{\zoff}{z_{\text{off}}}

\newcommand{\con}{c_{\text{on}}}
\newcommand{\coff}{c_{\text{off}}}

\newcommand{\dzoff}{\mathcal{D}_{\zoff}}

\usepackage[thinlines]{easytable}

\usepackage{color}
\def\tcr{\textcolor{red}}
\def\tcb{\textcolor{blue}}
\def\tcg{\textcolor{green}}
\def\tcp{\textcolor{violet}}
\def\tb{\textbf}
\def\ra{\rightarrow}
\def\la{\leftarrow}
\def\Cbb{\mathbb{C}}
\def\Fbb{{\mathbb{F}}}
\def\Pbb{{\mathbb{P}}}

\def\mc{\mathcal}
\def\mb{\mathbb}

\hbadness=99999  % or any number >=10000


% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\icmltitlerunning{Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks}

\begin{document}

\twocolumn[
\icmltitle{Task-Aware Virtual Training: Enhancing Generalization in Meta-Reinforcement Learning for Out-of-Distribution Tasks}
% "더 강조되고 더 드러나보이는 제목과 알고리즘 이름으로"



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Jeongmo Kim}{unist}
\icmlauthor{Yisak Park}{unist}
\icmlauthor{Minung Kim}{unist}
\icmlauthor{Seungyul Han}{unist} \hspace{-0.12in} $^{*}$
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{unist}{Graduate School of Artificial Intelligence, UNIST, Ulsan, South Korea}
\icmlcorrespondingauthor{Seungyul Han}{syhan@unist.ac.kr}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\printAffiliationsAndNotice{}

\begin{abstract}
Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments.
\end{abstract}

\vspace{-2\baselineskip}

\section{Introduction}
\label{sec:introduction}


\begin{figure*}[!h]
    \centering
    \includegraphics[width=0.9 \textwidth]{figures/ant-goal-motiv.png} 
    \vspace{-1\baselineskip}
    \caption{(a) 2D Goal positions in the Ant-Goal environment: The blue shaded area indicates the training task distribution, with blue marks representing inner training tasks, green marks representing outer training tasks, and red marks denoting OOD test tasks. (b-e) t-SNE visualization of task latents for various context-based meta-RL methods.}
    \vspace{-1\baselineskip}
    \label{fig:motivation}
\end{figure*}


Research in meta reinforcement learning (meta-RL) aims to train policies on training tasks sampled from a training task distribution, with the goal of enabling the learned policy to adapt and perform well on unseen test tasks. Model-agnostic meta-learning (MAML) \cite{finn2017model} seeks to find initial parameters that can generalize well to new tasks by using policy gradients to measure how well the current policy parameter can adapt to new tasks. On the other hand, to effectively distinguish tasks while capturing the task characteristics, context-based meta-RL methods that learn task latents through representation learning has been researched recently \cite{rakelly2019efficient, zintgraf2019varibad, fu2021towards}. One of the most well-known context-based methods, PEARL \cite{rakelly2019efficient}, learns task representations from off-policy samples and uses these samples for reinforcement learning, resulting in sample-efficient learning and faster convergence compared to traditional methods. In contrast, another prominent context-based meta-RL method, VariBad \cite{zintgraf2019varibad}, employs a Bayesian approach to learn a belief distribution over environments, effectively managing the exploration-exploitation trade-off in previously unseen environments. Recently, to address this issue and better distinguish tasks based on their characteristics, advanced representation learning methods such as contrastive learning have been increasingly adopted in meta-RL \cite{fu2021towards,choshen2023contrabar}. CCM \cite{fu2021towards} is a meta-RL method that integrates contrastive learning with PEARL. In CCM, task latents from the same task are treated as having a positive relationship and are trained to be close to each other, while latents from different tasks are treated as having a negative relationship and are trained to be distant.


Advanced representation learning in meta-RL improves the ability to distinguish training tasks through learned task latents. However, most existing methods assume that the test task distribution matches the training distribution, limiting their effectiveness on out-of-distribution (OOD) tasks. LDM \cite{lee2021improving} addresses this by training policies with virtual tasks (VTs) created via linear interpolation of task latents learned by VariBad, improving OOD generalization. Despite its benefits, we identify several key issues with the existing methods for VT construction. First, generated VTs often fail to capture task characteristics accurately. Second, LDM focuses solely on reward sample generation, which struggles in environments with task-dependent state transitions. To overcome these limitations, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that generates VTs accurately reflecting task characteristics for both training and OOD scenarios. Using a Bisimulation metric \cite{ferns2011bisimulation,ferns2014bisimulation}, our method captures task variations, such as changing goal positions, and incorporates an on-off task latent loss to stabilize the task latents. In addition, we introduce task-preserving sample generation to ensure VTs generate realistic sample contexts while maintaining task-specific features. Finally, to address state-varying environments, our task decoder generates full dynamics including both rewards and next states, and we propose a state regularization method to mitigate overestimation errors from generated samples.

Table \ref{table:comparison} compares our method with existing approaches, and Fig. \ref{fig:motivation} illustrates differences in task latent representations for the Ant-Goal-OOD environment, where an ant agent should reach a goal point. Fig. \ref{fig:motivation}(a) shows 2D goal positions, including training and OOD test tasks. While PEARL struggles to distinguish tasks, CCM, VariBad, and LDM differentiate training tasks but scatter OOD test task representations or fail to align goal position angles. In contrast, our method (TAVT) accurately aligns task latent for both training tasks and OOD test tasks, reflecting task characteristics more effectively. In particular, while other methods either disregard VT or rely on simple reconstruction-based VT, our approach utilizes metric-based representation and generative methods, enabling the encoder to learn more accurate VT contexts that effectively capture task information. This highlights the novelty of our proposed VT framework. To introduce the proposed TAVT, the paper outlines the meta-RL setup in Section \ref{sec:preliminary}, our approach in Section \ref{sec:methodology}, and experimental results in Section \ref{sec:experiments}, showing improved task representation and OOD generalization. 

\vspace{-1\baselineskip}
\begin{table}[!ht]
    \caption{Comparison of Context-based Meta RL Methods}
    \label{table:comparison}
    \resizebox{.48\textwidth}{!}{
    \begin{centering}
        \begin{tabular}{cccc}
        \toprule \toprule
        & Task Representation & Virtual Tasks & Task-preserving VT Samples \\ \midrule
         \midrule
        PEARL & O & X & X   \\ \hline
        VariBAD & O & X & X \\ \hline
        CCM & O & X & X \\ \hline
        LDM & O & $\triangle$ (reward only) & X \\ \hline
        TAVT (Ours) & O & O & O\\ \bottomrule
        \bottomrule
        \end{tabular}
    \end{centering}
    }
\end{table}



\vspace{-1\baselineskip}
\section{Preliminary}
\label{sec:preliminary}

\subsection{Meta Reinforcement Learning}
In meta-RL, each task $\mathcal{T}$ is sampled from a task distribution $p(\mathcal{T})$ and defined as a Markov Decision Process (MDP) $(S, A, P^\task, R^\task, \gamma, \rho_0)$, where $S$ and $A$ are the state and action spaces, $P^\task$ represents state transition dynamics, $R^\task$ is the reward function, $\gamma\in [0,1)$ is the discount factor, and $\rho_0$ is the initial state distribution. At each time step $t$, the agent selects an action $a_t$   based on the policy $\pi$, receives a reward $r_t:=R^\task(s_t,a_t)$, and transitions to the next state $s_{t+1}\sim P^\task(\cdot|s_t,a_t)$. The MDP for each task may vary, but all tasks share the same state and action spaces. During meta-training, the policy $\pi$ is optimized to maximize the cumulative reward sum $\sum_t \gamma^t r_t$ across tasks sampled from $p(\mathcal{T}_{\mathrm{train}})$. The policy is then evaluated on OOD test tasks from $p(\mathcal{T}_{\mathrm{test}})$, which differs entirely from $p(\mathcal{T}_{\mathrm{train}})$.


\subsection{Context-based Meta RL}
Recent meta-RL methods focus on learning latent contexts to differentiate tasks. PEARL \cite{rakelly2019efficient}, a well-known context-based meta-RL approach, learns task latents $\mathbf{z} \sim q_\psi(\cdot|\mathbf{c}^\task)$ using a task encoder $q_\psi$ with parameters $\psi$ and task context $\mathbf{c}^\task :=\{(s_l,a_l,r_l,s_l')\}_{l=1}^{N_c}$, where $s'$ is the next state and $N_c$  is the number of transition samples. PEARL defines task-dependent policies $\pi(\cdot|s,\z)$ and Q-functions $Q(s,a,\z)$, using soft actor-critic (SAC) \cite{haarnoja2018soft} to train policies. The task encoder $q_\psi$ is trained to minimize the encoder loss: 
\begin{align*}
\E_{\mathcal{T}\sim p(\task_{\mathrm{train}})}[\E_{\z \sim q_\psi}[\mathcal{L}_{Q}(\task,\z)]
+ D_{\text{KL}}(q_\psi(\cdot | \context^\task) || N(\mathbf{0},\mathbf{I}))],
\end{align*},
where $N$ is the multivariate Gaussian distribution, $\mathbf{I}$ is the identity matrix, and $\mathcal{L}_Q$ is the SAC critic loss. Inspired by the variational auto-encoder (VAE) \cite{kingma2013auto}, PEARL uses $\mathcal{L}_Q$ instead of VAE's reconstruction loss to better distinguish tasks.

\subsection{Virtual Task Construction}
To improve the generalization of policies to various OOD tasks, LDM \cite{lee2021improving} defines the task latents $\z^\alpha$ for a virtual task as a linear interpolation of training task latents $\z^i$, $i=1,\cdots,M$, expressed as:
\begin{align}
    \z^\alpha = \sum_{i=1}^M \alpha^{i}\z^{i}
    \label{eq:z_alpha_generation}
\end{align}
% \[\z^\alpha = \sum_{i=1}^M \alpha^{i}\z^{i}\]
, where $\mathbf{\alpha}=(\alpha^1,\cdots,\alpha^M)\sim  \beta \text{Dirichlet}(1,1,...,1)-\frac{\beta-1}{M}$ is the interpolation coefficient, $\textrm{Dirichlet}(\cdot)$ is the Dirichlet distribution, and $M$ is the number of training tasks used for mixing. The parameter $\beta \geq 1$ controls the degree of mixing: with $\beta=1$, only interpolation within the training task latents occurs, while $\beta > 1$ aallows extrapolation beyond the original latents. LDM further trains the policy using contexts generated from the task decoder based on the interpolated latents $\z^\alpha$, enabling it to handle OOD tasks more effectively.


\section{Related Works}
\label{sec:related_works}
\textbf{Advanced Task Representation Learning:}Advanced representation learning techniques have been widely explored to improve task latents that effectively distinguish tasks. Recent meta-RL methods use contrastive learning \cite{oord2018representation} to enhance task differentiation through positive and negative pairs, improving task representation \cite{laskin2020curl, fu2021towards, choshen2023contrabar} and capturing task information in offline setups \cite{li2020focal, gao2024context}. The Bisimulation metric \cite{ferns2011bisimulation} is employed to capture behavioral similarities \cite{zhanglearning, agarwal2021contrastive, liu2023robust} and group similar tasks \cite{hansen2022bisimulation, sodhani2022block}. Additionally, skill representation learning \cite{eysenbach2018diversity} addresses non-parametric meta-RL challenges \cite{frans2017meta, harrison2020continuous, nam2022skill, fu2022meta, he2024decoupling}, while task representation learning is increasingly applied in multi-task setups \cite{ishfaq2024offline, cheng2022provable, sodhani2021multi}.

\noindent \textbf{Generalization for OOD Tasks:} Meta-RL techniques for improving policy generalization in OOD test environments have been actively studied \cite{lan2019meta, fakoor2019meta, mu2022domino}. Model-based approaches \cite{lin2020model, lee2021improving}, advanced representation learning with Gaussian Mixture Models \cite{wang2023meta, lee2023parameterizing}, and Transformers \cite{vaswani2017attention, melo2022transformers, xumeta} have been explored. Additionally, some studies tackle distributional shift challenges through robust learning \cite{mendonca2020meta, mehta2020curriculum, ajay2022distributionally, pmlr-v162-chae22a, greenberg2024train}.


\noindent \textbf{Model-based Sample Relabeling:} Model-based sample generation and relabeling techniques have gained attention in meta-RL \cite{rimon2024mamba, 10565991}, enabling the reuse of samples from other tasks using dynamics models \cite{li2020multi, mendonca2020meta, wan2021hindsight, zou2024relabeling}. These methods address sparse rewards \cite{packer2021hindsight, jiang2024doubly}, mitigate distributional shifts in offline setups \cite{dorfman2021offline, yuan2022robust, zhou2024generalizable, guan2024cost}, and incorporate human preferences \cite{ren2022efficient, hejna2023few} or guided trajectory relabeling \cite{wang2023supervised}, expanding their applications.







\section{Methodology}
\label{sec:methodology}




\subsection{Metric-based Task Representation}
\label{subsec:bisim}

In this section, we propose a novel representation learning method to ensure task latents accurately capture differences in task contexts, enabling virtual tasks to effectively reflect task characteristics. To achieve this, we leverage the Bisimulation metric \cite{ferns2011bisimulation}, which measures the similarity of two states in an MDP based on the reward function $R^\task$ and state transition $P^\task$. In meta-learning, the Bisimulation metric can quantify task similarity by comparing contexts \cite{zhanglearning}. Unlike \citet{zhanglearning}, which considers tasks with different state spaces, we adapt the metric for tasks sharing the same state and action space, modifying it from Eq. (4) in \citet{zhanglearning}.

\begin{definition}[Bisimulation metric for task representation] For two different tasks $\task _i$ and $\task_j$,
\begin{align}
    d(\task_i,\task_j) & =
    \underset{(s,a)\sim D}{\mathbb{E}}\Big[ \vert R^{\task_i}(s,a)-R^{\task_j}(s,a)\vert \nonumber \\ 
    & + \eta W_2(P^{\task_i}(\cdot|s,a), P^{\task_j}(\cdot|s,a)) \Big],
    \label{eq:bisim_metric}
\end{align}
where $D$ is the replay buffer that stores the sample contexts, $R^{\task}, P^{\task}$ are the reward function and the transition dynamics for task $\task$, $W_2$ is 2-Wasserstein distance between the two distributions, and $\eta \in (0,1]$ is the distance coefficient. 
\end{definition}

\begin{proposition}
$d(\cdot,\cdot)$ defined in Eq. \eqref{eq:bisim_metric} is a metric.
\end{proposition}
\noindent Proof) The detailed proof is provided in Appendix \ref{secapp:proof}.

\noindent The Bisimulation metric $d$ equals $0$ when the contexts of two tasks perfectly match and increases as the context difference grows. Task latents learned using this metric more effectively capture task distances than existing representation methods, as shown in Fig. \ref{fig:motivation}. We train the task encoder $q_\psi(\mathbf{z}|\mathbf{c}^\mathcal{T})$ to ensure the task latent $\mathbf{z} \sim q_\psi$ preserves the Bisimulation metric $d$ in the latent space. Since the actual reward function $R$ and transition dynamics $P$ are generally unknown, we train the task decoder $p_\phi(s, a, \mathbf{z}) = \big(R_\phi(s, a, \mathbf{z}), P_\phi(\cdot|s, a, \mathbf{z})\big)$ with parameters $\phi$ to approximate task dynamics using reconstruction loss. 




We adopt the learning structure of PEARL, which uses two distinct policies: $\pi_{\mathrm{exp}}$ for on-policy exploration to obtain task latents from contexts and $\pi_{\mathrm{RL}}$ for off-policy RL to maximize returns. Contexts generated by $\pi_{\mathrm{exp}}$ and $\pi_{\mathrm{RL}}$ are stored in the on-policy buffer $D_{\mathrm{on}}^\task$ and the off-policy buffer $D_{\mathrm{off}}^\task$, respectively. PEARL considers only on-policy task latents $\z_{\mathrm{on}}$ derived from $\mathbf{c}^\task \sim D_{\mathrm{on}}^\task$, but $\z_{\mathrm{on}}$ can be unstable due to limited contexts in $D_{\mathrm{on}}^\task$. To address this, we propose an on-off latent learning structure where off-policy task latents $\z_{\mathrm{off}}$, derived from $\mathbf{c}_{\mathrm{off}}^\task \sim D_{\mathrm{off}}^\task$, maintain the Bisimulation distance for training tasks, and $\z_{\mathrm{on}}$ just aligns with $\z_{\mathrm{off}}$. Fig.~\ref{fig:on_change} shows t-SNE visualization of $\z_{\mathrm{on}}$, demonstrating that on-off latent loss provides more stable task representation compared to using $\z_{\mathrm{on}}$ alone. In summary, our encoder-decoder loss is defined as \vspace{-2em}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.75 \columnwidth]{figures/onchange2.png} 
    \vspace{-1\baselineskip}
    \caption{Changes in latents $\z_{\mathrm{on}}$ of 4 randomly sampled training tasks in the Ant-Goal-OOD environment: (a) Using  $\z_{\mathrm{on}}$ only (b) Using on-off latent loss for task representation learning}
    \label{fig:on_change}
    \vspace{-2\baselineskip}
\end{figure}


{\small
\begin{align}
    &\mathcal{L}_{\mathrm{bisim}}(\psi,\phi)  = \mathbb{E}_{\task_i,\task_j \sim p(\task_{\text{train}})}\Big[ \underbrace{\left(\vert \z_{\mathrm{off}}^i - \z_{\mathrm{off}}^j \vert - d(\task_i,\task_j;p_{\bar{\phi}} )\right)^2}_{\textrm{Bisimulation loss}}  \nonumber \\
    & + \underbrace{\mathbb{E}_{(s,a,r,s')\sim D_{\mathrm{off}}^{\task_i}, (\hat{r},\hat{s}')\sim p_\phi(s,a,\z_{\mathrm{off}}^i)}\Big[ (r-\hat{r})^2 + (s'-\hat{s}')^2 \Big] }_{\textrm{Reconstruction loss}}\nonumber \\
    & + \underbrace{(\z_{\mathrm{on}}^i - \bar{\z}_{\mathrm{off}}^i)^2}_{\textrm{on-off latent loss}}\Big],~~~\z^i \sim q_\psi (\cdot|\mathbf{c}^{\task_i}),~\mathbf{c}^{\task_i} \sim D^{\task_i},~\forall i,
    \label{eq:bisimloss}
\end{align}}
where $d(\task_i,\task_j;p_{\bar{\phi}})$ replaces $(R^{\task_i},P^{\task_i})$ in Eq.~\eqref{eq:bisim_metric} with the task decoder $p_{\bar{\phi}}(\cdot,\bar{\z}_{\mathrm{off}}^i),\forall i$, and $\bar{x}$ denotes gradient detachment for $x$. We construct the task latents $\z^\alpha$ for VTs using the method in Section \ref{sec:preliminary}. Our proposed representation learning ensures task latents align more effectively to capture task differences based on the Bisimulation distance $d$, as illustrated in Fig.\ref{fig:motivation}. Additionally, in Section~\ref{subsec:taskrep} and Appendix \ref{secapp:task_representation}, we present task representations for other tasks, demonstrating that our method consistently aligns and stabilizes task latents across various environments.


\subsection{Task Preserving Sample Generation}
\label{subsec:TPSG}
Using the task decoder $p_\phi (\cdot,\z^\alpha)$ and VT task latents $\z^\alpha$, we generate virtual contexts $\hat{\mathbf{c}}^\alpha := {(s_l, a_l, \hat{r}_l^\alpha, \hat{s}_l'^\alpha)}_{l=1}^{N_c}$, where $(s_l, a_l)$ are sampled from real contexts $\mathbf{c}^\task$, and $\hat{r}_l^\alpha, \hat{s}_l'^\alpha \sim p_\phi(s_l, a_l, \z^\alpha)$. Existing VT construction methods \cite{lee2021improving,lee2023parameterizing} use dropout-based regularization to ensure virtual contexts generalize to unseen tasks. However, we observed that task latents $\hat{\z}^\alpha \sim q_\psi(\cdot|\hat{\mathbf{c}}^\alpha)$ often deviate significantly from $\z^\alpha$, indicating that virtual contexts fail to effectively preserve task information.
To address this, we propose a task-preserving loss to minimize the difference between $\z^\alpha$ and $\hat{\z}^\alpha$, ensuring virtual contexts better retain task latent information. Fig. \ref{fig:task_preserving} demonstrates the effectiveness of the task-preserving loss. In Fig. \ref{fig:task_preserving}(a), without the task-preserving loss, $\z^\alpha$ and $\hat{\z}^\alpha$ show significant differences, resulting inaccurate task latents for OOD tasks. In contrast, Fig. \ref{fig:task_preserving}(b) shows that with the task-preserving loss, $\z^\alpha$ and $\hat{\z}^\alpha$ align closely, enhancing stability and alignment for OOD tasks.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.95 \columnwidth]{figures/task-preserving.png} 
    \vspace{-1\baselineskip}
    \caption{Comparison of task latents in the Ant-Dir-4 Environment: (a) Without task-preserving loss (b) With task-preserving loss. The left graph shows the difference between latents $\z^\alpha$ and $\hat{\z}^\alpha$for VTs during training, while the right graph shows latents for training (blue circles) and OOD tasks (red circles) during evaluation. Training tasks have 4 goal directions of $\frac{\pi}{4} k$, $k=0,1,2,3$, and OOD tasks cover 12 different directions from 0 to $2\pi$.}
    \label{fig:task_preserving}
    \vspace{-1\baselineskip}
\end{figure}


Despite the benefits of the task-preserving loss, virtual contexts $\hat{\mathbf{c}}^\alpha$ may still differ from real contexts due to limitations in the task decoder $p_\phi(\cdot, \z^\alpha)$, which cannot fully capture actual task contexts. These differences can introduce instability and degrade performance for RL training. To address this issue, we use a Wasserstein generative adversarial network (WGAN) \cite{arjovsky2017wasserstein}, designed to reduce the distribution gap between real and generated data. In our setup, the task decoder $p_\phi(\cdot, \z^\alpha)$ acts as the generator, learning to produce samples that closely resemble real ones, while real contexts serve as the target for the discriminator. The discriminator $f_\zeta$ increases its value for real samples and decreases it for generated samples, while the generator aligns virtual contexts with real ones by increasing $f_\zeta$ for generated samples. This process ensures that virtual contexts not only preserve task information but also closely resemble real contexts, significantly reducing the gap between VT-generated samples and real OOD task samples. To summarize, the WGAN discriminator and generator losses, combined with the task-preserving loss, are defined as:
\begin{align}
    &\mathcal{L}_{\text{disc}}(\zeta) = \mathbb{E}_{\task_i\sim p(\task_{\mathrm{train}}),\mathbf{c}^{\task_i}\sim D^{\task_i}}[-f_\zeta(\mathbf{c}^{\task_i},\bar{\z}_{\mathrm{off}}^i) \label{eq:disc}\\&~~~~~~~  +\mathbb{E}_{\hat{\mathbf{c}}^\alpha\sim p_\phi}[f_\zeta(\hat{\mathbf{c}}^\alpha,\bar{\z}_{\mathrm{off}}^\alpha)]]+\lambda_{\mathrm{GP}}\cdot~ \mathrm{Gradient ~Penalty}
    ,\nonumber\\
    &\mathcal{L}_{\text{gen}}(\psi,\phi) =\underbrace{\mathbb{E}_{\hat{\mathbf{c}}^\alpha\sim p_\phi}[-f_\zeta(\hat{\mathbf{c}}^\alpha,\bar{\z}_{\mathrm{off}}^\alpha)}_{\textrm{WGAN generator loss}} \nonumber\\ & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+\underbrace{\mathbb{E}_{\hat{\z}^\alpha \sim q_\psi (\cdot|\hat{\mathbf{c}}^\alpha)}[(\hat{\z}^\alpha - \bar{\z}_{\mathrm{off}}^\alpha)^2}_{\textrm{task preserving loss}}]],
    \label{eq:gen}
\end{align}
where the $\mathrm{Gradient ~Penalty}$ (GP), introduced in \cite{arjovsky2017wasserstein}, stabilizes training, with $\lambda_{\mathrm{GP}}$ as its coefficient. The detailed implementation of GP is provided in Appendix \ref{subsecapp:GP}. In this framework, the task decoder $p_\phi$ is trained with input $\z_{\mathrm{off}}$, as described in Eq. \eqref{eq:bisimloss}. Both the task-preserving loss and VT construction are always based on $\z_{\mathrm{off}}^\alpha$, derived from $\z_{\mathrm{off}}$. Importantly, the off-policy task latent $\z_{\mathrm{off}}$ conditions sample context generation, with its gradient disconnected to ensure stable training. By leveraging WGAN, we significantly reduce differences between generated and real samples, improving RL performance on OOD tasks. In Section~\ref{subsec:ablation} and Appendix \ref{secapp:ctxtdiff}, we demonstrate that the proposed task-preserving sample generation effectively bridges the gap between VT-generated samples and real OOD task samples, enhancing generalization and RL performance.




\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98 \columnwidth]{figures/structure.png} 
    \vspace{-0.5\baselineskip}
    \caption{ An illustration for the structure of TAVT}
    \label{fig:structure1}
    \vspace{-1\baselineskip}
\end{figure}



\subsection{Task-Aware Virtual Training}
\label{subsec:TAVT}



By combining metric-based representation learning with task-preserving sample generation, we propose the Task-Aware Virtual Training (TAVT) algorithm, which enhances the generalization of meta-RL to OOD tasks by leveraging VTs that accurately reflect task characteristics. The total encoder-decoder loss for TAVT is given by:
\vspace{-0.2\baselineskip}
\begin{equation}
\mathcal{L}_{\mathrm{total}}(\psi,\phi)= \mathcal{L}_{\mathrm{bisim}}(\psi,\phi)+\mathcal{L}_{\mathrm{gen}}(\psi,\phi),
\end{equation}
\vspace{-0.5\baselineskip}
with detailed loss scales provided in Appendix \ref{secapp:hyper}.

Now, we train the RL policy $\pi_{\mathrm{RL}}$ using SAC, leveraging both real contexts $\mathbf{c}^{\task_i} \sim D_{\mathrm{off}}^{\task_i}$ for each training task $\task_i$ and virtual contexts $\hat{\mathbf{c}}^\alpha$ generated by the proposed TAVT method. For training tasks, the RL policy is defined as $\pi_{\mathrm{RL}} = \pi (\cdot|s,\z_{\mathrm{on}}^i)$, where $\z_{\mathrm{on}}^i$ is the on-policy task latent. For VTs, the RL policy is defined as $\pi_{\mathrm{RL}} = \pi (\cdot|s,\z_{\mathrm{on}}^\alpha)$, where $\z_{\mathrm{on}}^\alpha$ is the VT's task latent derived from $\z_{\mathrm{on}}^i$. We train the RL policy $\pi_{\mathrm{RL}}$ using SAC, leveraging both real contexts $\mathbf{c}^{\task_i} \sim D_{\mathrm{off}}^{\task_i}$ for each training task $\task_i$ and virtual contexts $\hat{\mathbf{c}}^\alpha$ generated by the proposed TAVT method. For training tasks, the RL policy is defined as $\pi_{\mathrm{RL}} = \pi (\cdot|s,\z_{\mathrm{on}}^i)$, where $\z_{\mathrm{on}}^i$ is the on-policy task latent. For VTs, the RL policy is defined as $\pi_{\mathrm{RL}} = \pi (\cdot|s,\z_{\mathrm{on}}^\alpha)$, where $\z_{\mathrm{on}}^\alpha$ is the VT's task latent derived from $\z_{\mathrm{on}}^i$. The proposed TAVT method aligns task latents using metric-based representation and learns VTs that preserve task characteristics while generating samples similar to real ones through task-preserving loss with WGAN. This enables the policy to train on a diverse range of OOD tasks, improving generalization performance.

Since the agent cannot access off-policy latents for test tasks, it relies on on-policy latents $\z_{\mathrm{on}}^i$, obtained from $N_{\mathrm{exp}}$ episodes generated by the exploration policy $\pi_{\mathrm{exp}}$, as introduced in PEARL \cite{rakelly2019efficient}. While PEARL uses  $\pi(\cdot|s,\tilde{\z})$ with $\tilde{\z} \sim N(\mathbf{0},\mathbf{I})$ as the exploration policy, we propose a novel exploration policy $\pi_{\mathrm{exp}} = \pi(\cdot|s,\z_{\mathrm{on}}^\alpha)$ that leverages VT task latents $\z_{\mathrm{on}}^\alpha$. This approach enables exploration across both training tasks and VTs by varying the interpolation coefficient $\alpha$, allowing the agent to explore a broader range of tasks. Appendix \ref{secapp:exploration} shows that our exploration policy covers a wider range of trajectories, including OOD tasks, leading to improved generalization. Fig. \ref{fig:structure1} provides an overview of the TAVT framework, with detailed implementation, including the RL loss function and meta-training/testing algorithms, in Appendix \ref{subsecapp:RL}.


\subsection{State Regularization Method}
\label{subsec:statereg}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1.0 \columnwidth]{figures/sbeta_fin.png} 
    \vspace{-2\baselineskip}
    \caption{(a) $Q$-function loss for VTs (b) Estimation bias for OOD tasks in the Walker-Mass-OOD environment.}
    \vspace{-0.5\baselineskip}
    \label{fig:sreg}
\end{figure}




The virtual contexts $\hat{\mathbf{c}}^\alpha$ in TAVT include both rewards and next states, enabling it to handle state-varying environments. However, inaccuracies in the task decoder can introduce errors in the $Q$-function, leading to overestimation bias, a common issue in offline RL \cite{fujimoto2019off, yeom2024exclusively}. While reward errors have minimal impact, next-state errors significantly contribute to overestimation. To mitigate this, we propose a state regularization method, replacing the next states $\hat{s}'^\alpha$ in virtual contexts with $\hat{s}_{\mathrm{reg}}'^\alpha$, a mix of next states $s'$ from training tasks and $\hat{s}'^\alpha$ from virtual contexts:
\[\hat{s}_{\mathrm{reg}}'^\alpha := \epsilon_{\mathrm{reg}} \hat{s}'^\alpha + (1-\epsilon_{\mathrm{reg}}) s',\],
where $(s, a, r, s') \in \mathbf{c}^\task$, $\hat{r}^\alpha, \hat{s}'^\alpha \sim p_\phi(s, a, \z_{\mathrm{off}}^\alpha)$, and $\epsilon_{\mathrm{reg}} \in [0, 1]$ is the regularization coefficient.

Fig. \ref{fig:eps_reg}(a) shows the $Q$-function loss for VTs during training, while Fig. \ref{fig:eps_reg}(b) illustrates the $Q$-function estimation bias for OOD tasks in the Walker-Mass-OOD environment, where the agent's mass varies by task. Estimation bias, defined as the difference between $Q$-function values and average actual returns, is significantly reduced with $\epsilon_{\mathrm{reg}}=0.1$. In contrast, using $\epsilon_{\mathrm{reg}}=1.0$ (full use of $\hat{s}'^\alpha$) results in higher $Q$-function loss and greater bias. Section \ref{sec:experiments} further demonstrates that this method improves OOD performance.





\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.97 \textwidth]{figures/perform.png} 
    \vspace{-1em}
    \caption{Performance comparison for MuJoCo environments. The graphs for on-policy algorithms represent their final performance.}
    \label{fig:perfcomp}
    \vspace{-1em}
\end{figure*}





\section{Experiments}
\label{sec:experiments}



 
In this section, we compare the proposed TAVT algorithm with various on-policy and off-policy meta-RL methods across MuJoCo \cite{todorov2012mujoco} and MetaWorld ML1 \cite{yu2020meta} environments. For off-policy methods, we include PEARL, CCM which uses contrastive learning to enhance representation, MIER which incorporates a gradient-based dynamics model, and Amago which employs transformers for latent learning. For on-policy methods, we evaluate MAML which optimizes initial gradients for generalization, RL$^2$ which encodes task information in RNN hidden states, VariBad which applies Bayesian methods for representation learning, and LDM which uses virtual tasks with a reconstruction loss and a DropOut layer. Additionally, we provide an analysis of the task representation in TAVT as well as an ablation study on its performance. For TAVT, detailed hyperparameter configurations are provided in Appendix \ref{secapp:hyper}. For other methods, we reproduced their results using the author-provided source code and default hyperparameters. 
Comparison graphs and tables show the average performance over 5 random seeds, with standard deviations as shaded areas in the graphs or $\pm$ in the tables.




\subsection{Environmental Setup}

To evaluate generalization performance on OOD tasks, we used 6 MuJoCo environments \cite{todorov2012mujoco}: Cheetah-Vel-OOD, Ant-Dir-2, Ant-Dir-4, and Ant-Goal-OOD, where only the reward function varies across tasks, and Walker-Mass-OOD and Hopper-Mass-OOD, where both the reward function and state transition dynamics vary. In addition, we considered 6 MetaWorld ML1 environments \cite{yu2020meta}, including the original Reach and Push environments, as well as 4 OOD variations (Reach-OOD-Inter, Reach-OOD-Extra, Push-OOD-Inter, and Push-OOD-Extra). In these ML1 and ML1-OOD tasks, the final goal point varies across tasks while maintaining shared state dynamics. The task space is denoted by $\mathcal{M}$, and detailed descriptions of these environments are provided below.
\vspace{-.5em}
\begin{itemize}
    \item Cheetah-Vel-OOD: The Cheetah agent is required to run at target velocities $v_{\mathrm{tar}}$ in $\mathcal{M}$. \vspace{-.5em}
    \item Ant-Dir: The Ant agent is required to move in directions $\theta_{\mathrm{dir}}$ in $\mathcal{M}$. For Ant-Dir-2, the training tasks involve 2 directions ($\theta_{\mathrm{dir}}=0,\frac{\pi}{2}$), and for Ant-Dir-4,  the training tasks involve 4 directions  ($\theta_{\mathrm{dir}}=0,\frac{\pi}{2},\pi,\frac{3\pi}{2}$).\vspace{-.5em}
    \item Ant-Goal-OOD: The Ant agent should reach the 2D goal positions $(r_{\mathrm{goal}} \cos\theta_{\mathrm{goal}}, r_{\mathrm{goal}}\sin\theta_{\mathrm{goal}})$, where the radius $r_{\mathrm{goal}}$ and angle $\theta_{\mathrm{goal}}$ are selected from $\mathcal{M}$.\vspace{-.5em}
    \item Hopper/Walker-Mass-OOD: The Hopper/Walker agent are required to run forward with scale $m_{\mathrm{scale}}$ in $\mathcal{M}$ multiplied to their body mass.\vspace{-.5em}
    \item ML1/ML1-OOD: The agent is tasked with reaching or pushing an object to a target goal position $g_{\mathrm{tar}}$, sampled from the 3D goal space $\mc{M}$, which varies depending on the environment. To create an OOD setup, inner areas $\mc{M}_{\mathrm{inner}}$ are defined within the goal space $\mc{M}$.
\end{itemize}
\vspace{-.5em}
\begin{table}[!h]
    \centering
    \vspace{-1.5em}
    \caption{Environmental setup}
    %\setlength{\tabcolsep}{10pt}
    \renewcommand{\arraystretch}{1.5}
    \resizebox{0.99\columnwidth}{!}{  % \textwidth
        \begin{tabular}{cccc}  
        \toprule \toprule
         & $\mathcal{M}_{\mathrm{train}}$ & $\mathcal{M}_{\mathrm{test}}$ \\ %\hline
         \midrule
        Cheetah-Vel-OOD & $[0.0,0.5)\cup[3.0,3.5)$ & $\{0.75, 1.25, 1.75, 2.25, 2.75\}$ \\ \hline
        Ant-Dir-2 & $\{0, \frac{\pi}{2}\}$ & $\{ \frac{\pi}{4}, \frac{3\pi}{4}, \frac{7\pi}{4} \}$ \\ \hline
        Ant-Dir-4 & $\{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\}$ & $\{ \frac{\pi}{4}, \frac{3\pi}{4}, \frac{5\pi}{4}, \frac{7\pi}{4}\}$ \\ \hline
        Ant-Goal-OOD & $ r \in [0.0,1.0) \cup [2.5,3.0) $ & $r=1.75$ & \\ & $\theta \in [0,2\pi]$ & $\theta \in \{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\}$ &  \\ \hline
        Hopper-Mass-OOD & $[0.0,0.5)\cup[3.0,3.5)$ & $\{0.75, 1.25, 1.75, 2.25, 2.75\}$ \\ \hline
        Walker-Mass-OOD & $[0.0,0.5)\cup[3.0,3.5)$ & $\{0.75, 1.25, 1.75, 2.25, 2.75\}$ \\ \hline
        ML1 & $\mc{M}$ & $\mc{M}$ \\ \hline
        ML1-OOD-Inter & $\mc{M} \backslash\mc{M}_{\mathrm{inner}}$ & $\mc{M}_{\mathrm{inner}}$ \\ \hline
        ML1-OOD-Extra & $\mc{M}_{\mathrm{inner}}$ & $\mc{M} \backslash \mc{M}_{\mathrm{inner}}$ \\ \hline
        \bottomrule
        \end{tabular}
    }
    \label{table:mujocotasks}
\end{table}
\begin{table*}[!t]
    \vspace{-1em}
    \centering
    \caption{Average success rate for MetaWorld ML1 environments. }
    \renewcommand{\arraystretch}{1.2}
    % \vspace{-.5em}
    \resizebox{0.98\textwidth}{!}{
        \begin{tabular}{cccccccccc}
        \toprule \toprule
          & MAML & RL$^2$ & VariBAD & LDM & PEARL & CCM & Amago & MIER & TAVT(Ours) \\ \hline
          
        Reach & 0.97±0.02 & 0.95±0.04 & 0.73±0.12 & 0.76±0.1 & 0.48±0.21 & 0.65±0.13 & 0.71±0.27 & 0.61±0.18 & \tb{0.98±0.02} \\ \hline
        
        Reach-OOD-Inter & 0.56±0.11 & 0.86±0.12 & 0.82±0.11 & 0.87±0.1 & 0.52±0.16 & 0.78±0.1 & 0.93±0.05 & 0.62±0.18 & \tb{0.96±0.03}\\ \hline
        
        Reach-OOD-Extra & 0.48±0.15 & 0.73±0.14 & 0.82±0.11 & 0.79±0.15 & 0.48±0.14 & 0.81±0.12 & 0.43±0.08 & 0.65±0.12 & \tb{0.99±0.01} \\ \hline

        Push & 0.94±0.03 & \tb{0.98±0.02} & 0.88±0.09 & 0.83±0.11 & 0.61±0.11 & 0.18±0.08 & 0.87±0.11 & 0.59±0.13 & \tb{0.98±0.03} \\ \hline
        
        Push-OOD-Inter & 0.78±0.13 & 0.79±0.14 & 0.83±0.11 & 0.77±0.13 & 0.79±0.16 & 0.12±0.03 & \tb{0.98±0.02} & 0.45±0.15 & \tb{0.98±0.02}\\ \hline
        
        Push-OOD-Extra & 0.55±0.13 & 0.38±0.12 & 0.65±0.09 & 0.72±0.11 & 0.55±0.18 & 0.15±0.04 & 0.83±0.11 & 0.61±0.15 &\tb{0.92±0.08} \\ 
        
        \bottomrule \bottomrule  
        \end{tabular}
    }
    \label{table:successcomp}
\end{table*}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.98 \textwidth]{figures/main_representation6.png} 
    \vspace{-1em}
    \caption{t-SNE visualization of task latents: (a) Cheetah-Vel-OOD (b) Walker-Mass-OOD (c) Ant-Dir-4 (d) Reach-OOD-Inter. For ML1 tasks in the 3D goal space, we provide both 3D representation for all tasks and 2D representation for tasks in the selected cross-section.}
    \label{fig:all_representation}
    \vspace{-1em}
\end{figure*}
The training task space $\mathcal{M}_{\mathrm{train}}$ and test task space $\mathcal{M}_{\mathrm{test}}$ are detailed in Table \ref{table:mujocotasks}. Except for the original ML1 environments, test tasks are OOD, meaning $\mathcal{M}_{\mathrm{train}} \bigcap \mathcal{M}_{\mathrm{test}} = \emptyset$. For Ant-Dir-2, test tasks include both interpolations between training directions and extrapolations beyond them. In ML1-OOD-Extra, OOD test tasks ($\mc{M} \backslash \mc{M}_{\mathrm{inner}}$) extrapolate beyond training tasks ($\mc{M}_{\mathrm{inner}}$). In all other OOD tasks, test tasks lie in regions interpolating between training tasks. During training, tasks are uniformly sampled from $\mathcal{M}_{\mathrm{train}}$, with $p(\task_\mathrm{train}) = \mathrm{Unif}(\mathcal{M}_{\mathrm{train}})$. More details on MuJoCo and MetaWorld, including descriptions of the goal space $\mc{M}$ and $\mc{M}_{\mathrm{inner}}$ for ML1/ML1-OOD, are in Appendix \ref{secapp:expdetail}.

% cheetah walker dir4 ML1
% 통일성 있게 fig3과 같은계열로, training끼리, OOD끼리 구분은 gradation으로, 파랑 traing 빨강 OOD, ML1 3D인데 gradation 해야하니까 우측상단으로 갈수록 진하게





\subsection{Performance Comparison}
We compare the performance of the proposed TAVT with other meta-RL algorithms. For MuJoCo environments, Fig. \ref{fig:perfcomp} shows the convergence performance of the test average return, while Table \ref{table:comparison} presents the final average success rate for ML1/ML1-OOD environments. On-policy algorithms are trained for 200M timesteps in MuJoCo and 100M timesteps in MetaWorld environments. For off-policy algorithms, training timesteps vary in MuJoCo environments and are fixed at 10M timesteps for MetaWorld environments. Also, Fig. \ref{fig:perfcomp} includes the `-oracle' performance for VariBad (on-policy) and TAVT (off-policy) in MuJoCo environments. This represents the upper performance bound achieved by training on all tasks from both $\mathcal{M}_{\mathrm{train}}$ and $\mathcal{M}_{\mathrm{test}}$.


For MuJoCo environments, Fig. \ref{fig:perfcomp} shows that both VariBad and TAVT perform well in the `-oracle' setup across most environments. However, in OOD task scenarios, TAVT significantly outperforms other on-policy and off-policy methods. Notably, TAVT achieves performance close to the `-oracle' in Cheetah-Vel-OOD, Ant-Goal-OOD, and Walker-Mass-OOD, demonstrating strong generalization to unseen OOD tasks. While other algorithms struggle in challenging environments like Ant-Dir-2, which includes extrapolation tasks, TAVT remains robust. In environments such as Walker-Mass-OOD and Hopper-Mass-OOD, where state transition dynamics vary, LDM fails to adapt due to limitations in its VT construction. In contrast, TAVT excels, showcasing its ability to handle varying state transition dynamics. Details on policy trajectories for training and OOD tasks are available in Appendix \ref{secapp:exploration}.

For MetaWorld environments, Table \ref{table:successcomp} shows that TAVT consistently delivers superior performance across all setups. In the original ML1 environments, TAVT outperforms other methods, highlighting the advantages of its metric-based task representation. In ML1-OOD environments, PEARL-based methods such as PEARL, CCM, and MIER perform poorly in Push and Push-OOD tasks, while other off-policy algorithms also fail to achieve success rates near 1. Despite being a PEARL-based method, TAVT achieves significantly higher success rates, often approaching 1. Compared to on-policy algorithms like LDM, RL$^2$, and VariBad, TAVT also demonstrates much better performance. These results highlight the effectiveness of our method in enhancing generalization for OOD tasks. The training graphs for MetaWorld environments are provided in Appendix \ref{secapp:ML1graph}.






\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.998 \columnwidth]{figures/component_eval_both2.png} 
    \vspace{-2em}
    \caption{Component evaluation on (a) Cheetah-Vel-OOD and (b) Walker-Mass-OOD environments.}
    \label{fig:component_eval}
    \vspace{1em}
     \includegraphics[width=0.98 \columnwidth]{figures/sample_diff5.png}
     % \includegraphics[width=0.9 \columnwidth]{figures/mj_diff_label.png}
     \vspace{-.5em}
    \caption{Context differences between real contexts and virtual contexts generated by the task decoder for OOD tasks: (a) Cheetah-Vel-OOD and (b) Walker-Mass-OOD environments.}
     \vspace{-2em}
    \label{fig:c_diff2}
\end{figure}

\subsection{Task Representation of TAVT}
\label{subsec:taskrep}
The comparative experiments confirm the superiority of TAVT. To explore the factors behind this improvement, Fig. \ref{fig:all_representation} illustrates the task representation of TAVT across various environments. As shown in the cased of Ant-Goal environment in Fig. \ref{fig:motivation}, TAVT accurately aligns task latents for both training and OOD test tasks, reflecting task characteristics. For instance, task latents align linearly by target velocity in Cheetah-Vel-OOD and by target mass in Walker-Mass-OOD. Similarly, task latents align according to 2D target directions in Ant-Dir-4 and 3D target goals in Reach-OOD-Inter. These results demonstrate that the proposed metric-based task representation effectively distinguishes and generalizes task latents to OOD tasks. Additional task representations for other environments are provided in Appendix \ref{secapp:task_representation}.




\vspace{-.5em}
\subsection{Ablation Studies}
\label{subsec:ablation}

{\bf Component Evaluation:} To further analyze the proposed TAVT method, we perform a component evaluation on the Cheetah-Vel-OOD and Walker-Mass-OOD environments, comparing the final average return in Fig. \ref{fig:component_eval}. We consider the variations of TAVT including `TAVT w/o VT', which removes virtual tasks entirely, `TAVT w/o $\mathcal{L}_{\mathrm{gen}}$', which excludes the $\mathcal{L}_{\mathrm{gen}}$ component, `TAVT w/o on-off loss', which omits the on-off loss, and `Recon only', which uses only reconstruction loss with DropOut as in LDM. The results demonstrate that removing any component significantly degrades performance, underscoring the importance of task representation learning and sample generation for OOD task generalization. In addition, TAVT outperforms the `Recon only' approach, showcasing its ability to construct more effective virtual tasks than existing methods.




\begin{figure}[!t]
    \centering
    \includegraphics[width= \columnwidth]{figures/ablation-sbeta-both.png} 
    \vspace{-2em}
    \caption{Performance comparison for various $\epsilon_{\mathrm{reg}}$ on (a) Walker-Mass-OOD (b) Hopper-Mass-OOD environments.}
    \label{fig:eps_reg}
     \vspace{-2em}
\end{figure}
{\bf Context Differences:} To evaluate the impact of context differences between VT-generated and actual contexts on performance, Fig. \ref{fig:c_diff2} presents these differences for the TAVT variations analyzed in the component evaluation. Since `TAVT w/o VT' does not utilize virtual contexts, it is excluded from Fig. \ref{fig:c_diff2}. The results show that removing any component increases context error, leading to degraded performance, as reflected in Fig. \ref{fig:component_eval}, underscoring the importance of each component in TAVT. Notably, omitting the task-preserving sample generation loss $\mathcal{L}_\mathrm{gen}$ significantly increases context differences. These findings demonstrate that the proposed WGAN-based task-preserving sample generation method effectively reduces context errors and enhances generalization performance, as detailed in Section \ref{subsec:TPSG}. Additional context differences for other environments are provided in Appendix \ref{secapp:ctxtdiff}.

{\bf State Regularization:} To demonstrate the effectiveness of the proposed state regularization method, Fig. \ref{fig:sreg} shows the performance across different $\epsilon_{\mathrm{reg}}$ values. As illustrated, a small $\epsilon_{\mathrm{reg}}$ effectively reduces estimation bias. Fig. \ref{fig:eps_reg} further evaluates its impact on performance in Walker-Mass-OOD and Hopper-Mass-OOD environments, where $\epsilon_{\mathrm{reg}} = 0.1$ yields the best results. This setting minimizes both $Q$-function loss and overestimation bias, enabling the $Q$-function to accurately reflect expected returns and improving performance in OOD tasks. These findings highlight the importance of the proposed state regularization method. Additional ablation studies on the mixing coefficient $\beta$ for VT construction are provided in Appendix \ref{secapp:addabl}.


\vspace{-.5em}

\section{Conclusion}


Existing meta-RL methods either overlook OOD tasks or struggle with them, particularly when state transitions vary. Our proposed method, TAVT, addresses these challenges through metric-based latent learning, task-preserving sample generation, and state regularization. These components ensure precise task latents and enhanced generalization, overcoming previous limitations. Experimental results demonstrate that TAVT achieves superior alignment of OOD task latents with training task characteristics.










% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

% \textbf{Impact Statement}
% \vspace{.5em}\\
\section*{Impact Statement}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here


\bibliography{example_paper}
\bibliographystyle{icml2025}

% \newpage
% \noindent \textbf{Checklist} \\ \\
% \tcr{Check the Checklist of ICML...}\\




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% \renewcommand{\thesection}{\Alph{section}}
% \renewcommand{\thesubsection}{\Alph{section}.\arabic{subsection}}

\counterwithin{table}{section}
\counterwithin{figure}{section} 
\renewcommand{\theequation}{\thesection.\arabic{equation}}

\setcounter{equation}{0}

\section{Proof}
\label{secapp:proof}
{\bf Definition 3.1} (Bisimulation metric for task representation){\bf.} For two different tasks $\task _i$ and $\task_j$,
\begin{align}
    d(\task_i,\task_j) & =
    \underset{(s,a)\sim D}{\mathbb{E}}\Big[ \vert R^{\task_i}(s,a)-R^{\task_j}(s,a)\vert + \eta W_2(P^{\task_i}(\cdot|s,a), P^{\task_j}(\cdot|s,a)) \Big],\tag{1}\nonumber
\end{align}
where $D$ is the replay buffer that stores the sample contexts, $R^{\task}, P^{\task}$ are the reward function and the transition dynamics for task $\task$, $W_2$ is 2-Wasserstein distance between the two distributions, and $\eta \in (0,1]$ is the distance coefficient.

\vspace{2em}


\noindent {\bf Proposition 3.1.} \textit{
$d(\cdot,\cdot)$ defined in Eq. \eqref{eq:bisim_metric} is a metric.}

\vspace{1em}

\noindent{\bf Proof of Proposition 3.1}\\


As mentioned in Section \ref{sec:preliminary}, each task $\task$ is represented by an MDP $(S, A, P^\task, R^\task, \gamma, \rho_0)$, where $S$ is the state space, $A$ is the action space, $P^\task$ represents the state transition dynamics, $R^\task$ is the reward function, $\gamma \in [0,1)$ is the discount factor, and $\rho_0$ is the initial state distribution. Since all tasks shares the same state space and action space in this paper, so $\task_i = \task_j$ if and only if $P^{\task_i}(s'|s,a) = P^{\task_j}(s'|s,a) \textrm{ and } R^{\task_i}(s,a) = R^{\task_j}(s,a), ~~~\forall s,~s' \in S,~a\in A$ for any tasks $\task_i,~\task_j \in \mathcal{M}$. Thus, from the definition of $d$ given by Eq. \eqref{eq:bisim_metric}, $d$ is a metric since $d$ satisfies the following axioms:


\begin{enumerate}
    \item (Non-negativity) $d(\task_i,\task_j)\geq 0$ since $|\cdot|$ and $W_2(\cdot,\cdot)$ are non-negative,
    \item $d(\task_i,\task_j)=0 \Longleftrightarrow R^{\task_i} = R^{\task_j} \textrm{ and } P^{\task_i} = P^{\task_j} \Longleftrightarrow \task_i = \task_j,$
    \item (Symmetry) $d(\task_i,\task_j) = d(\task_j,\task_i)$ from the definition,
    \item (Triangle Inequality) $d(\task_i,\task_k) \leq d(\task_i,\task_j) + d(\task_j,\task_k)$:
    \begin{align}
        d(\task_i,\task_k) & = \mathbb{E}_{(s,a)\sim D }\Big[ \vert R^{\task_i}(s,a)-R^{\task_k}(s,a)\vert + \eta W_2(P^{\task_i}(\cdot|s,a), P^{\task_k}(\cdot|s,a)) \Big],\nonumber\\ & \underset{*}{\leq} \mathbb{E}_{(s,a)\sim D }\Big[ \vert R^{\task_i}(s,a)-R^{\task_j}(s,a)\vert + \vert R^{\task_j}(s,a)-R^{\task_k}(s,a)\vert + \nonumber\\ & ~~~~~~~\eta W_2(P^{\task_i}(\cdot|s,a), P^{\task_j}(\cdot|s,a)) + \eta W_2(P^{\task_j}(\cdot|s,a), P^{\task_k}(\cdot|s,a))\Big],\nonumber\\ & = \mathbb{E}_{(s,a)\sim D }\Big[ \vert R^{\task_i}(s,a)-R^{\task_j}(s,a)\vert + \eta W_2(P^{\task_i}(\cdot|s,a), P^{\task_j}(\cdot|s,a)) \Big]+\nonumber\\ &  ~~~~~~~\mathbb{E}_{(s,a)\sim D }\Big[ \vert R^{\task_j}(s,a)-R^{\task_k}(s,a)\vert + \eta W_2(P^{\task_j}(\cdot|s,a), P^{\task_k}(\cdot|s,a)) \Big],\nonumber\\
        &= d(\task_i,\task_j)+d(\task_j,\task_k),\nonumber
    \end{align}
    where $*$ can be derived, as both the absolute value $|\cdot|$ and the Wasserstein distance $W_2$ satisfy the triangle inequality. \hfill $\blacksquare$
\end{enumerate}



\newpage
\section{Task Representations Across All Environments}
\label{secapp:task_representation}

To present the task representation results for considered OOD environments, Fig. \ref{fig:mj_representation_total} shows the task representations for 6 MuJoCo OOD environments (Cheetah-Vel-OOD, Ant-Dir-2, Ant-Dir-4, Hopper-Mass-OOD, and Walker-Mass-OOD, and Ant-Goal-OOD), and Fig. \ref{fig:ml1_representation_total} illustrates the task representations for 4 ML1-OOD environments (Reach-OOD-Inter, Reach-OOD-Extra, Push-OOD-Inter, and Push-OOD-Extra). All task representations in Fig. \ref{fig:mj_representation_total} and Fig. \ref{fig:ml1_representation_total} are obtained during the meta-testing phase. Additionally, to achieve stable task representations for Fig. \ref{fig:ml1_representation_total}, a larger number of training tasks ($N_{\mathrm{train}}=500$) was sampled compared to the originally required number ($N_{\mathrm{train}}=50$).

From the results in Fig. \ref{fig:mj_representation_total}, the task representations for Cheetah-Vel-OOD, Ant-Dir-2, Ant-Dir-4, Hopper-Mass-OOD, and Walker-Mass-OOD are well-aligned according to their respective task characteristics, such as target velocity, target directions, and agent mass. For Ant-Goal-OOD, the task representation is aligned based on two characteristics: goal radius and direction. These findings demonstrate that the proposed method effectively aligns task representations with task characteristics across all MuJoCo environments, while OOD tasks maintain latents that preserve these characteristics.

Similarly, the results in Fig. \ref{fig:ml1_representation_total} show that for each ML1-OOD environment, the 3D visualization of the target goal positions is presented on the left, and the task representations aligned with the goal positions are displayed on the right. The results indicate that for all considered environments, both training and OOD tasks are well-aligned in 3D space according to their target goal positions. These findings highlight that the proposed Bisimulation metric-based task representation learning effectively creates a task latent space aligned with task characteristics, while the TAVT training method enhances the generalization of OOD task representations.



\begin{figure}[!h]
    \centering
    \includegraphics[width=0.65 \columnwidth]{figures/mujoco_total_representation2.png} 
    \caption{t-SNE visualization of the 6 MuJoCo OOD environments}
    \label{fig:mj_representation_total}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.65 \columnwidth]{figures/ml1_total_representation4.png} 
    \caption{t-SNE visualization of the 4 ML1-OOD environments}
    \label{fig:ml1_representation_total}
\end{figure}






\newpage
\section{Effectiveness of the Proposed Task-Preserving Sample Generation}
\label{secapp:ctxtdiff}

In Section \ref{subsec:TPSG}, we introduced a task-preserving sample generation technique to ensure that virtual contexts retain task latent information while closely resembling actual task samples. To evaluate the effectiveness of this approach, we analyze how virtual sample contexts generated by the task decoder differ from real task contexts for OOD tasks. Context difference graphs for all considered OOD setups are provided, showing the average reward and state differences between real and generated contexts for both MuJoCo and ML1 environments.

Fig. \ref{fig:c_diff} presents context differences across 6 MuJoCo environments under OOD setups. Our TAVT algorithm achieves the smallest context differences in all environments, with the differences being most pronounced in Cheetah-Vel-OOD, Ant-Goal-OOD, and Ant-Dir-4. Similarly, Fig. \ref{fig:c_diff_ml1} shows context differences in 4 ML1-OOD environments, where TAVT again demonstrates the smallest differences. These results highlight TAVT’s effectiveness in generating accurate transition samples, even for unseen OOD tasks.


In comparison, the `Recon only' and `TAVT w/o on-off loss' setups exhibit the largest context differences, followed by `TAVT w/o $\mathcal{L}_{\mathrm{gen}}$', while the proposed TAVT method achieves the smallest context difference. The Recon only' method suffers due to its reliance on transition samples solely from training tasks. The absence of the proposed on-off loss or $\mathcal{L}_{\mathrm{gen}}$ leads to increased context differences, demonstrating that these components significantly reduce errors in the sample contexts generated by the proposed VT. This reduction in context errors directly contributes to improving OOD task generalization.

\begin{figure}[!ht]
    \centering
     \includegraphics[width=0.8 \columnwidth]{figures/mj_diff2.png}
     % \includegraphics[width=0.45 \columnwidth]{figures/mj_diff_label.png}
    \caption{Context differences between real contexts and virtual contexts generated by the task decoder for MuJoCo OOD tasks. The $y$-axis is plotted on a log scale except Ant-Dir-2 environment to show the difference.}
    \label{fig:c_diff}
\end{figure}

\begin{figure}[!ht]
    \centering
     \includegraphics[width=0.99 \columnwidth]{figures/ml1_diff2.png}
     % \includegraphics[width=0.45 \columnwidth]{figures/mj_diff_label.png}
    \caption{Context differences between real contexts and virtual contexts generated by the task decoder for ML1 OOD tasks. The $y$-axis is plotted on a log scale to show the difference.}
    \label{fig:c_diff_ml1}
\end{figure}


% According to the results shown in Fig. \ref{fig:c_diff}(a) and (b) for Cheetah-Vel-OOD and Ant-Dir-4, respectively, our TAVT method demonstrated the smallest reward difference for both environments. This indicates that our approach is effective in generating accurate transition samples for OOD tasks. 
% Conversely, the `Recon only' method showed the largest reward difference among the methods because it only experiences transition samples from training tasks. Notably, in the Cheetah-Vel-OOD environment, `TAVT w/o WGAN' and `TAVT w/o $\mathcal{L}_{\mathrm{gen}}$' showed a significant difference compared to TAVT, suggesting that the influence of WGAN is substantial. Similarly, in the Ant-Dir-4 environment, `TAVT w/o TP' showed a larger difference than TAVT, indicating that the proposed task-preserving loss plays a crucial role in this environment. 


% \begin{figure}[!h]
%     \centering
%      \includegraphics[width=0.4 \columnwidth]{figures/sample_diff.png}
%     \caption{Context differences between real contexts and virtual contexts generated by the task decoder for OOD tasks: (a) Cheetah-Vel-OOD and (b) Ant-Dir-4 environments. The $y$-axis is plotted on a log scale to show the difference.}
%     \label{fig:c_diff}
% \end{figure}

% \tcr{Need To Add All Context Difference Graph Here}













\newpage
\section{More Detailed Implementations}

\label{secapp:imple}
In this section, we provide more detailed implementations for the proposed TAVT. Section \ref{subsecapp:bisim} describes the practical implementation of the loss terms in $\mathcal{L}_{\mathrm{bisim}}$, Section \ref{subsecapp:GP} explains the implementation of the gradient penalty for WGAN loss, and Section \ref{subsecapp:RL} details the implementation of meta-RL with TAVT, including the meta-training and meta-testing algorithms.


\subsection{Practical Implementation of $\mathcal{L}_{\mathrm{bisim}}$}
\label{subsecapp:bisim}
To improve task representation and reflect task differences, we use the Bisimulation metric as described in Section \ref{subsec:bisim}. We also introduce an on-off loss to stabilize task latents. The encoder-decoder loss is given by Eq. \eqref{eq:bisimloss}, and to compute $d(\task_i,\task_j;p_{\bar{\phi}})$ in Eq. \eqref{eq:bisimloss}, we use the task decoder $p_{\phi}(\cdot,\bar{\z}_{\mathrm{off}})$. However, since $\bar{\z}_{\mathrm{off}}$ evolves during encoder training, the task decoder may become unstable, potentially affecting the metric $d$. To address this instability, we propose using an additional decoder $p_{\tilde{\phi}}(\cdot,\mathrm{idx}_{\mathrm{\task_i}})$ with parameter $\tilde{\phi}$ to compute the metric $d$, where $\mathrm{idx}_{\mathrm{\task_i}}$ is the one-hot encoded vector of the task index $i$. The decoder $p_{\tilde{\phi}}$ is also trained using reconstruction loss. Consequently, the updated encoder-decoder loss is given by
\begin{align}
    &\mathcal{L}_{\mathrm{bisim}}(\psi,\phi,\tilde{\phi})  = \mathbb{E}_{\task_i,\task_j \sim p(\task_{\text{train}})}\Big[ \underbrace{\left(\vert \z_{\mathrm{off}}^i - \z_{\mathrm{off}}^j \vert - d(\task_i,\task_j;p_{\tilde{\phi}} )\right)^2}_{\textrm{Bisimulation loss}} + \underbrace{\mathbb{E}_{(s,a,r,s')\sim D_{\mathrm{off}}^{\task_i}, (\hat{r},\hat{s}')\sim p_{\tilde{\phi}}(s,a,\mathrm{idx}_{\mathrm{\task_i}})}\Big[ (r-\hat{r})^2 + (s'-\hat{s}')^2 \Big] }_{\textrm{Reconstruction loss for $p_{\tilde{\phi}}$}}\nonumber \\ &    + \underbrace{\mathbb{E}_{(s,a,r,s')\sim D_{\mathrm{off}}^{\task_i}, (\hat{r},\hat{s}')\sim p_\phi(s,a,\z_{\mathrm{off}}^i)}\Big[ (r-\hat{r})^2 + (s'-\hat{s}')^2 \Big] }_{\textrm{Reconstruction loss for $p_{\phi}$}}+ \underbrace{(\z_{\mathrm{on}}^i - \bar{\z}_{\mathrm{off}}^i)^2}_{\textrm{on-off latent loss}}\Big],~~~\z^i \sim q_\psi (\cdot|\mathbf{c}^{\task_i}),~\mathbf{c}^{\task_i} \sim D^{\task_i},~\forall i.
    \label{eq:bisimlossapp}
\end{align}
Moreover, to further stabilize the learning of the latent variables, instead of using a single sample of $\z_{\mathrm{off}}^i$ in the on-off loss, we use the average of $\z_{\mathrm{off}}^i$ from multiple contexts sampled from the buffer. This approach helps prevent fluctuations in $\z_{\mathrm{off}}^i$ due to varying contexts, thereby aiding in more stable learning of the task latents. 


\subsection{Implementation of Gradient Penalty in $\mathcal{L}_{\mathrm{disc}}$}
\label{subsecapp:GP}

To stabilize the training of the adversarial network, the WGAN framework \cite{gulrajani2017improved} incorporates a gradient penalty (GP) that restricts the gradient to prevent the discriminator from learning too quickly, as described in Section \ref{subsec:TPSG}. GP ensures that the discriminator satisfies the 1-Lipschitz continuity condition. In WGAN discriminator loss Eq. \eqref{eq:disc}, the Gradient Penalty term is calculated by \[\mathrm{Gradient~Penalty}=\mathbb{E}_{\task_i \sim p(\task_{\mathrm{train}})}[(\Vert \nabla_{\delta_{\mathrm{inter}}}f_\zeta(\delta_{\mathrm{inter}}) \Vert_2 - 1)^2],\]
where $\delta_{\mathrm{inter}}:=\delta (\mathbf{c}^{\task_i}, \bar{\z}_{\mathrm{off}}^i) + (1-\delta) (\hat{\mathbf{c}}^\alpha, \bar{\z}_{\mathrm{off}}^\alpha)$ represents the interpolated samples between $(\mathbf{c}^{\task_i}, \bar{\z}_{\mathrm{off}}^i)$, which are induced by training tasks $\task_i$, and $(\hat{\mathbf{c}}^\alpha, \bar{\z}_{\mathrm{off}}^\alpha)$, which are induced by the task decoder of VT. Here, $\delta\sim \mathrm{Unif}([0,1])$ is the interpolation factor for the GP. In addition, the WGAN structure trains the discriminator more frequently than the generator at a 5:1 ratio, as proposed in the original WGAN paper \cite{gulrajani2017improved}.


\subsection{Meta-RL with TAVT and TAVT Algorithms}
\label{subsecapp:RL}

As described in Section \ref{subsec:TAVT}, we aim to perform meta-RL using the contexts $\mathbf{c}_{\mathrm{off}}^i$  obtained from training task $\task_i$ and the virtual contexts $\hat{\mathbf{c}}^\alpha$ generated by the task decoder of the VT. For meta RL, the $Q$-function and the RL policy are defined as $Q_\theta(s,a,\z)$ and $\pi_\theta(\cdot|s,\z)$ with task latent $\z$, where $\theta$ represents the parameters of both the $Q$-function and the policy $\pi$. Based on SAC \cite{haarnoja2018soft}, the RL losses for the $Q$-function and the policy are then given by:
\begin{align}
    \mathcal{L}_{Q}(\theta;\mathbf{c},\z) &= \mathbb{E}_{(s,a,r,s')\sim\mathbf{c}}\left[ \Big( Q_\theta(s,a,\bar{\z}) - (r \cdot \lambda_{\textrm{rew}} + \gamma \mathbb{E}_{a'\sim\pi_\theta(\cdot|s,\bar{\z})}[Q_{\theta^-}(s',a',\bar{\z}) - \lambda_{\mathrm{ent}}\log \pi_\theta(\cdot|s,\bar{\z})] \Big)^2 \right]\\
    \mathcal{L}_{\pi}(\theta;\mathbf{c},\z) &= \mathbb{E}_{s\sim\mathbf{c}}\left[ D_{\mathrm{KL}}\left(\pi_\theta(\cdot|s,\bar{\z})~\bigg|\bigg|~\frac{\exp(Q_\theta(s,\cdot,\bar{\z})/\lambda_{\mathrm{ent}})}{Z_\theta(s)}\right) \right],
\end{align}
where $Q_{\theta^-}$ is the target value network with parameters $\theta^-$ updated from $\theta$ using the exponential moving average (EMA), $\lambda_{\mathrm{rew}}$ is the reward scale, $\lambda_{\mathrm{ent}}$ is the entropy coefficient, $D_{\mathrm{KL}}(p||q)$ is the Kullback-Leibler divergence between two distributions $p$ and $q$, and $Z_\theta$ is the normalizing factor. Based on the proposed TAVT, we update the $Q$-function and the RL policy with training task latents $\z^i$ using off-policy contexts obtained from these tasks, and update the $Q$-function and the RL policy with the latents $\z^\alpha$ of VT using virtual contexts generated by the task decoder, as explained in Section \ref{subsec:TAVT}. The total $Q$-function loss and policy loss with TAVT are given by
\begin{align}
    \mathcal{L}_{Q}(\theta) &= \mathbb{E}_{\task_i\sim p(\task_{\mathrm{train}}),\z_{\mathrm{on}}^i\sim q_\psi, \mathbf{c}_{\mathrm{off}}^{\task_i}\sim D_{\mathrm{off}}^{\task_i}}[\mathcal{L}_{Q}(\theta;\z_{\mathrm{on}}^i,\mathbf{c}_\mathrm{off}^{\task_i}) + \lambda_{\mathrm{VT}}\mathcal{L}_{Q}(\theta;\z_{\mathrm{on}}^\alpha,\hat{\mathbf{c}}_{\mathrm{off}}^\alpha )] \label{eq:l_critic_total} \\
    \mathcal{L}_{\pi}(\theta)&= \mathbb{E}_{\task_i\sim p(\task_{\mathrm{train}}),\z_{\mathrm{on}}^i\sim q_\psi, \mathbf{c}_{\mathrm{off}}^{\task_i}\sim D_{\mathrm{off}}^{\task_i}}[\mathcal{L}_{\pi}(\theta;\z_{\mathrm{on}}^i,\mathbf{c}_\mathrm{off}^{\task_i}) + \lambda_{\mathrm{VT}}\mathcal{L}_{\pi}(\theta;\z_{\mathrm{on}}^\alpha,\hat{\mathbf{c}}_{\mathrm{off}}^\alpha )], \label{eq:l_actor_total}
\end{align}
where $\lambda_{\mathrm{VT}} \in [0,1]$ is the loss coefficient for VT training. If $\lambda_{\mathrm{VT}}=0$, then TAVT does not utilize virtual samples at all. For generated virtual contexts $\hat{\mathbf{c}}^\alpha$, we apply the state regularization method proposed in Section \ref{subsec:statereg} for environments with varying state dynamics such as Hopper-Mass-OOD and Walker-Mass-OOD.  


Here, to obtain on-policy latents $\z_{\mathrm{on}}$ for meta-RL, we sample $N_{\textrm{exp}}$ episodes using our exploration policy $\pi_{\mathrm{exp}}=\pi_\theta(\cdot|s,\z_{\mathrm{on}}^\alpha)$, as proposed in Section \ref{subsec:TAVT}. This policy allows us to explore a broad range of tasks, as $\z_{\mathrm{on}}^\alpha$ spans all interpolated areas, including both training tasks and VTs. To enhance the exploration of diverse trajectories, we regenerate $\z_{\mathrm{on}}^\alpha$ every $H_{\mathrm{freq}}$ timesteps during the exploration process. This periodic update enables the exploration policy to adapt to new tasks and explore the environment with these newly assigned tasks. An analysis of the exploration policy with respect to the sampling frequency $H_{\mathrm{freq}}$ is provided in Appendix \ref{subsecapp:samplingfreq}. Finally, summarizing the contents of Section \ref{sec:methodology} and Appendix \ref{secapp:imple}, the proposed TAVT algorithm is outlined in Algorithm \ref{alg:metatrain} (Meta-training of TAVT) and Algorithm \ref{alg:metatest} (Meta-testing of TAVT).



\begin{algorithm}[!h]
\caption{Meta Training of TAVT}
\label{alg:metatrain}
\begin{algorithmic}[1]    
    \REQUIRE The training task set $\{\task_j\}_{j=1,\ldots,N_{\mathrm{train}}} \sim p(\task_{\mathrm{train}})$, the task encoder $q_\psi$, the task decoders $p_\phi$ and $p_{\tilde{\phi}}$, the WGAN discriminator $f_\zeta$, the $Q$-function $Q_\theta$, and the policy $\pi_\theta$.
    \STATE Initialize parameters $\psi,\phi,\tilde{\phi},\zeta$, $\theta$ and replay buffers for all training tasks.
    \FOR{epoch $k=1,2,\cdots$}
        \STATE Sample $N_{\mathrm{meta}}$ training tasks from the training task set.
        \FOR{task $\task_i$ in $N_{\mathrm{meta}}$ training tasks}
            \STATE Collect contexts $\mathbf{c}_{\mathrm{on}}^{\task_i}$ using the exploration policy $\pi_{\exp}=\pi(\cdot|s,\z_{\mathrm{on}}^\alpha)$ for $N_{\mathrm{exp}}$ episodes.
            \STATE Collect contexts $\mathbf{c}_{\mathrm{off}}^{\task_i}$ using the RL policy $\pi_{\mathrm{RL}}=\pi(\cdot|s,\z_{\mathrm{on}}^i)$ for $N_{\mathrm{RL}}$ episodes, where $\z_{\mathrm{on}}^i\sim q_\psi(\cdot|\mathbf{c}_{\mathrm{on}}^{\task_i})$.
            \STATE Store contexts $\mathbf{c}_{\mathrm{on}}^{\task_i}$ and $\mathbf{c}_{\mathrm{off}}^{\task_i}$ in the replay buffers $D_{\mathrm{on}}^{\task_i}$ and $D_{\mathrm{off}}^{\task_i}$, respectively.
        \ENDFOR
        
        \STATE Construct $N_{\mathrm{VT}}$ virtual tasks using the training task latents.
        \STATE Generate the  virtual contexts $\hat{\mathbf{c}}_{\mathrm{off}}^\alpha$ for each VT using the task decoder $p_\phi$.
        \FOR{gradient step in $K_{\mathrm{Model}}$ steps}
            \STATE Sample $N_{\mathrm{meta}}$ tasks in train tasks set.
            \STATE Compute $\mathcal{L}_{\mathrm{bisim}}(\psi,\phi,\tilde{\phi})$ loss by Eq. \eqref{eq:bisimlossapp}.
            \STATE Compute $\mathcal{L}_{\mathrm{disc}}(\zeta)$ and $\mathcal{L}_{\mathrm{gen}}(\psi,\phi)$ losses by Eq. \eqref{eq:disc} and Eq. \eqref{eq:gen}.
            \STATE Update the WGAN discriminator parameter $\zeta$ by $\zeta\leftarrow \zeta -\lambda_{\mathrm{lr}}\cdot\nabla_{\zeta}\mathcal{L}_{\mathrm{disc}}(\zeta)$.
            \STATE Update the model parameters $\psi$ and $\phi$, $\tilde{\phi}$ by $(\psi,\phi,\tilde{\phi})\leftarrow (\psi,\phi,\tilde{\phi})-\lambda_{\mathrm{lr,context}}\cdot\nabla_{(\psi,\phi,\tilde{\phi})} \mathcal{L}_{\mathrm{total}}(\psi,\phi,\tilde{\phi})$.
        \ENDFOR
        
        \FOR{gradient step in $K_{\mathrm{RL}}$ steps}
            \STATE Compute RL losses $\mathcal{L}_{Q}(\theta)$ and $\mathcal{L}_{\pi}(\theta)$ by Eq. \eqref{eq:l_critic_total} and Eq. \eqref{eq:l_actor_total}, respectively.
            \STATE Update the RL parameter $\theta$ by $\theta\leftarrow \theta -\lambda_{\mathrm{lr}}\cdot\nabla_\theta( \mathcal{L}_{Q}(\theta) + \mathcal{L}_\pi(\theta))$.
        \ENDFOR
    \ENDFOR    
\end{algorithmic}    
\end{algorithm}

% \begin{algorithm}[!h]
% \caption{Meta Training of TAVT}
% \label{alg:metatrain}
% \begin{algorithmic}[1]    
%     \Require The training task set $\{\task_j\}_{j=1,\ldots,N_{\mathrm{train}}} \sim p(\task_{\mathrm{train}})$, the task encoder $q_\psi$, the task decoders $p_\phi$ and $p_{\tilde{\phi}}$, the WGAN discriminator $f_\zeta$, the $Q$-function $Q_\theta$, and the policy $\pi_\theta$.
%     \State Initialize parameters $\psi,\phi,\tilde{\phi},\zeta$, $\theta$ and replay buffers for all training tasks.
%     \For{epoch $k=1,2,\cdots$}
%         \State Sample $N_{\mathrm{meta}}$ training tasks from the training task set.
%         \For{task $\task_i$ in $N_{\mathrm{meta}}$ training tasks}
%             \State Collect contexts $\mathbf{c}_{\mathrm{on}}^{\task_i}$ using the exploration policy $\pi_{\exp}=\pi(\cdot|s,\z_{\mathrm{on}}^\alpha)$ for $N_{\mathrm{exp}}$ episodes.
%             \State Collect contexts $\mathbf{c}_{\mathrm{off}}^{\task_i}$ using the RL policy $\pi_{\mathrm{RL}}=\pi(\cdot|s,\z_{\mathrm{on}}^i)$ for $N_{\mathrm{RL}}$ episodes, where $\z_{\mathrm{on}}^i\sim q_\psi(\cdot|\mathbf{c}_{\mathrm{on}}^{\task_i})$.
%             \State Store contexts $\mathbf{c}_{\mathrm{on}}^{\task_i}$ and $\mathbf{c}_{\mathrm{off}}^{\task_i}$ in the replay buffers $D_{\mathrm{on}}^{\task_i}$ and $D_{\mathrm{off}}^{\task_i}$, respectively.
%         % \ENDFOR
%         \EndFor
        
%         \State Construct $N_{\mathrm{VT}}$ virtual tasks using the training task latents.
%         \State Generate the  virtual contexts $\hat{\mathbf{c}}_{\mathrm{off}}^\alpha$ for each VT using the task decoder $p_\phi$.
%         \For{gradient step in $K_{\mathrm{Model}}$ steps}
%             \State Sample $N_{\mathrm{meta}}$ tasks in train tasks set.
%             \State Compute $\mathcal{L}_{\mathrm{bisim}}(\psi,\phi,\tilde{\phi})$ loss by Eq. \eqref{eq:bisimlossapp}.
%             \State Compute $\mathcal{L}_{\mathrm{disc}}(\zeta)$ and $\mathcal{L}_{\mathrm{gen}}(\psi,\phi)$ losses by Eq. \eqref{eq:disc} and Eq. \eqref{eq:gen}.
%             \State Update the WGAN discriminator parameter $\zeta$ by $\zeta\leftarrow \zeta -\lambda_{\mathrm{lr}}\cdot\nabla_{\zeta}\mathcal{L}_{\mathrm{disc}}(\zeta)$.
%             \State Update the model parameters $\psi$ and $\phi$, $\tilde{\phi}$ by $(\psi,\phi,\tilde{\phi})\leftarrow (\psi,\phi,\tilde{\phi})-\lambda_{\mathrm{lr,context}}\cdot\nabla_{(\psi,\phi,\tilde{\phi})} \mathcal{L}_{\mathrm{total}}(\psi,\phi,\tilde{\phi})$.
%         % \ENDFOR
%         \EndFor
        
%         \For{gradient step in $K_{\mathrm{RL}}$ steps}
%             \State Compute RL losses $\mathcal{L}_{Q}(\theta)$ and $\mathcal{L}_{\pi}(\theta)$ by Eq. \eqref{eq:l_critic_total} and Eq. \eqref{eq:l_actor_total}, respectively.
%             \State Update the RL parameter $\theta$ by $\theta\leftarrow \theta -\lambda_{\mathrm{lr}}\cdot\nabla_\theta( \mathcal{L}_{Q}(\theta) + \mathcal{L}_\pi(\theta))$.
%         % \ENDFOR
%         \EndFor
%     % \ENDFOR    
%     \EndFor
% \end{algorithmic}    
% \end{algorithm}

\begin{algorithm}[!h]
\caption{Meta Testing of TAVT}
\label{alg:metatest}
\begin{algorithmic}[1]    
    \REQUIRE The OOD test task set $\mathcal{M}_{\mathrm{test}}$, $\pi_\theta$, $q_\psi$.
    
    \FOR{task $\task$ in $\mathcal{M}_{\mathrm{test}}$}
        \FOR{episode k=1, \ldots, $N_{\mathrm{exp}}$}
            \STATE Generate $\z_{\mathrm{on}}^\alpha$ every $H_{\mathrm{freq}}$ timesteps
            \STATE Collect contexts $\mathbf{c}_{\mathrm{on}}^\task$ using the exploration policy $\pi_{\exp}=\pi(\cdot|s,\z_{\mathrm{on}}^\alpha)$
        \ENDFOR
        \STATE Task inference $\z_{\mathrm{on}}\sim q_\psi(\cdot|\mathbf{c}_{\mathrm{on}}^\task)$.
        \STATE Rollout transition using $\pi_\theta(\cdot|s,\z_{\mathrm{on}})$ for the last episode.

    \ENDFOR
    \STATE Compute the average return of the last episodes for all test tasks.
\end{algorithmic}    
\end{algorithm}




\newpage
\section{Exploration Trajectories of the Proposed Exploration Policy $\pi_{\mathrm{exp}}$}
\label{secapp:exploration}
In Section \ref{subsec:TAVT}, we propose the exploration policy $\pi_{\mathrm{exp}} = \pi(\cdot|s, \z_{\mathrm{on}}^\alpha)$ to cover a diverse range of tasks, whereas PEARL uses $\pi(\cdot|s, \tilde{\z}),~\tilde{\z} \sim N(\mathbf{0}, \mathbf{I})$ for exploration. We compare the exploration trajectories of PEARL and our TAVT in Section \ref{subsecapp:expcomp}. Additionally, in Section \ref{subsecapp:RL}, we update $\z_{\mathrm{on}}^\alpha$ every $H_{\mathrm{freq}}$ timesteps during the exploration phase to observe diverse trajectories, while PEARL samples $\tilde{\z}$ only once per episode. To demonstrate the effectiveness of $H_{\mathrm{freq}}$, we analyze its impact on exploration diversity in Section \ref{subsecapp:samplingfreq}.


\subsection{Trajectory Comparison between PEARL and TAVT (Ours)}
\label{subsecapp:expcomp}

The exploration process in the PEARL algorithm involves sampling $\z$ from a prior distribution $N(\mathbf{0}, \mathbf{I})$, as it learns to align all task latents $z$ with this prior, as discussed in Section \ref{sec:preliminary}. In contrast, our algorithm uses $\z_{\mathrm{on}}^\alpha$ obtained from VT construction, which spans diverse regions of the latent space, including both training tasks and virtual tasks. To compare the exploration behavior of PEARL and our TAVT, Fig. \ref{fig:exp_trj_train} and Fig. \ref{fig:exp_trj_ood} illustrate the first and second exploration trajectories during the exploration phase, as well as the final trajectory generated by $\pi_{\mathrm{RL}}$ in the Ant-Goal-OOD environment for training tasks and OOD tasks, respectively, for (a) PEARL and (b) TAVT (Ours). From the results, it is evident that TAVT's exploration policy covers a broader range of areas in the goal space compared to PEARL's exploration policy. Since the task latent for the RL policy is selected by the task encoder based on these exploration trajectories, TAVT's ability to explore diverse goal spaces enhances the differentiation of the current task within the task latent. Consequently, TAVT successfully reaches the goal points for both training and OOD tasks due to the effectiveness of our task latent, whereas PEARL fails to achieve the goal points in both scenarios.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99 \textwidth]{figures/ant_goal_train.png} 
    \caption{Visualization of trajectories in the goal space during the meta-testing phase for inner and outer training tasks in the Ant-Goal-OOD environment: (a) PEARL and (b) TAVT (Ours).}
    \label{fig:exp_trj_train}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8 \textwidth]{figures/ant_goal_ood.png}  
    %{figures/.png}  
    \caption{Visualization of trajectories in the goal space during the meta-testing phase for OOD test tasks in the Ant-Goal-OOD environment: (a) PEARL and (b) TAVT (Ours).}
    \label{fig:exp_trj_ood}
\end{figure}


\newpage
\subsection{Exploration Trajectories with Respect to Sampling Frequency $H_{\mathrm{freq}}$}

As proposed in Section \ref{subsecapp:RL}, we regenerated $\z_{\mathrm{on}}^\alpha$ every $H_{\mathrm{freq}}$ timesteps for the proposed exploration policy $\pi_{\mathrm{exp}}$. To demonstrate the effect of $H_{\mathrm{freq}}$ on exploration, Fig. \ref{fig:exp_trj} shows the exploration trajectories during the meta-testing phase for different values of $H_{\mathrm{freq}}$: (a) $5$, (b) $20$, and (c) $200$.

From Fig. \ref{fig:exp_trj}, it is evident that if the sampling frequency is too low, such as $H_{\mathrm{freq}} = 5$, the exploration policy results in trajectories that are confined to the area around the starting point without covering much distance. In contrast, if the sampling frequency is too high, like $H_{\mathrm{freq}} = 200$, the agent can reach locations far from the starting point but fails to explore a diverse set of directions. We found that a balanced sampling frequency, such as $H_{\mathrm{freq}} = 20$, allows the agent to explore a wide range of directions while still covering distant areas effectively.


\label{subsecapp:samplingfreq}
\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0 \columnwidth]{figures/ant_step.png}
    % \includegraphics[width=1.0 \columnwidth]{figures/exploration_steps.png}
    \caption{Visualization of exploration trajectories of TAVT according to the sampling frequency $H_{\mathrm{freq}}$: (a) 5 steps (b) 20 steps (c) 200 steps}
    \label{fig:exp_trj}
\end{figure}

\newpage
\section{Hyperparameter Setup for TAVT}
\label{secapp:hyper}


In this section, we provide a detailed hyperparameter setup for the proposed TAVT. To do this, we first define the coefficients for the loss functions related to the Bisimulation metric-based task representation and task-preserving sample generation proposed in Section \ref{sec:methodology} and Appendix \ref{subsecapp:bisim} as follows:
\begin{align}
    \mathcal{L}_{\mathrm{bisim}}(\psi,\phi)  = \mathbb{E}_{\task_i,\task_j \sim p(\task_{\text{train}})}& \Big[ \lambda_{\mathrm{bisim}} \cdot \underbrace{\left(\vert \z_{\mathrm{off}}^i - \z_{\mathrm{off}}^j \vert - d(\task_i,\task_j;p_{\tilde{\phi}} )\right)^2}_{\textrm{Bisimulation loss}} \nonumber \\& + \lambda_{\mathrm{recon}} \cdot \underbrace{\mathbb{E}_{(s,a,r,s')\sim D_{\mathrm{off}}^{\task_i}, (\hat{r},\hat{s}')\sim p_{\tilde{\phi}}(s,a,\mathrm{idx}_{\mathrm{\task_i}})}\Big[ (r-\hat{r})^2 + (s'-\hat{s}')^2 \Big] }_{\textrm{Reconstruction loss for $p_{\tilde{\phi}}$}}\nonumber \\ &    + \lambda_{\mathrm{recon}}\cdot\underbrace{\mathbb{E}_{(s,a,r,s')\sim D_{\mathrm{off}}^{\task_i}, (\hat{r},\hat{s}')\sim p_\phi(s,a,\z_{\mathrm{off}}^i)}\Big[ (r-\hat{r})^2 + (s'-\hat{s}')^2 \Big] }_{\textrm{Reconstruction loss for $p_{\phi}$}} \nonumber \\ & + \lambda_{\mathrm{on-off}}\cdot \underbrace{ (\z_{\mathrm{on}}^i - \bar{\z}_{\mathrm{off}}^i)^2}_{\textrm{on-off latent loss}}\Big],~~~\z^i \sim q_\psi (\cdot|\mathbf{c}^{\task_i}),~\mathbf{c}^{\task_i} \sim D^{\task_i},~\forall i.
    \label{eq:lbisim_with_coeff}
\end{align}

\begin{align}
    \mathcal{L}_{\text{disc}}(\zeta) &= \lambda_{\mathrm{WGAN}}\cdot\mathbb{E}_{\task_i\sim p(\task_{\mathrm{train}}),\mathbf{c}^{\task_i}\sim D^{\task_i}}[-f_\zeta(\mathbf{c}^{\task_i},\bar{\z}_{\mathrm{off}}^i) +\mathbb{E}_{\hat{\mathbf{c}}^\alpha\sim p_\phi}[f_\zeta(\hat{\mathbf{c}}^\alpha,\bar{\z}_{\mathrm{off}}^\alpha)]]+\lambda_{\mathrm{GP}}\cdot~ \mathrm{Gradient ~Penalty}
    ,\label{eq:ldisc_with_coeff}\\
    \mathcal{L}_{\text{gen}}(\psi,\phi) &= \mathbb{E}_{\hat{\mathbf{c}}^\alpha\sim p_\phi}[-\lambda_{\mathrm{WGAN}}\cdot \underbrace{f_\zeta(\hat{\mathbf{c}}^\alpha,\bar{\z}_{\mathrm{off}}^\alpha)}_{\textrm{WGAN generator loss}} + \lambda_{\mathrm{TP}} \cdot \underbrace{\mathbb{E}_{\hat{\z}^\alpha \sim q_\psi (\cdot|\hat{\mathbf{c}}^\alpha)}[(\hat{\z}^\alpha - \bar{\z}_{\mathrm{off}}^\alpha)^2}_{\textrm{task preserving loss}}]], 
    \label{eq:lgen_with_coeff}
\end{align}
where $\lambda_{\mathrm{bisim}}$ is the Bisimulation loss coefficient, $\lambda_{\mathrm{recon}}$ is the reconstruction loss coefficient, $\lambda_{\mathrm{on-off}}$ is the on-off loss coefficient, $\lambda_{\mathrm{WGAN}}$ is the WGAN loss coefficient, $\lambda_{\mathrm{TP}}$ is the task-preserving loss coefficient, and $\lambda_{\mathrm{GP}}$ is the gradient-penalty coefficient. Along with these loss coefficients, Table \ref{table:shared_hyper} displays the shared hyperparameter setup for all environments, while Table 2 presents the hyperparameter setup specific to each environment. As shown in Tables \ref{table:shared_hyper} and \ref{table:env_hyper}, the proposed TAVT has more hyperparameters compared to PEARL. However, most of the loss coefficients are similar across different environments, and the environment-specific hyperparameters listed in Table \ref{table:env_hyper} are the same as those used in PEARL. Thus, in practice, TAVT does not require a significantly larger hyperparameter search compared to PEARL.

\begin{table}[!ht]
\centering
\caption{Shared Hyperparameters}
\label{table:shared_hyper}
\renewcommand{\arraystretch}{1.1}
\resizebox{0.98 \textwidth}{!}{%
    \begin{tabular}{|c|cc|c|c|} \hline
        \multirow{26}{*}{\begin{tabular}[c]{@{}c@{}}Shared\\Hyper-\\parameters\end{tabular}} & \multicolumn{2}{c|}{Name}  & Value (for MuJoCo) & Value (for MetaWorld) \\ \cline{2-5}
         & \multirow{7}{*}{Loss coefficients} & $\lambda_{\mathrm{bisim}}$ & 100 (50 for Cheetah-Vel-OOD) & 100 \\
         &  & $\lambda_{\mathrm{recon}}$ & 200 & 200 \\ 
         &  & $\lambda_{\mathrm{on-off}}$ & 100 & 100 \\ 
         &  & $\lambda_{\mathrm{wgan}}$ & 1.0 & 1.0 \\ 
         &  & $\lambda_{\mathrm{preserve}}$ & 100 & 100 \\ 
         &  & $\lambda_{\mathrm{VT}}$& 1.0 (0.1 for Hopper-Mass-OOD) & 1.0 (for Reach) / 0.1 (for Push) \\ 
         &  & $\lambda_{\mathrm{GP}}$ & 5.0 & 5.0 \\ 
         \cline{2-5}
         & \multicolumn{2}{c|}{Context learning rate $\lambda_{\mathrm{lr,context}}$} & 0.0003 & 0.0003 \\
         & \multicolumn{2}{c|}{Learning rate $\lambda_{\mathrm{lr}}$} & 0.0003 & 0.0003 \\ 
         & \multicolumn{2}{c|}{Optimizer} & Adam & Adam \\ 
         \cline{2-5}
         & \multicolumn{2}{c|}{Mixing coefficient $\beta$} & 2.0 & 2.0 \\ 
         & \multicolumn{2}{c|}{Distance coefficient $\eta$} & 0.1 & 1 (for Reach) / 10 (for Push) \\ 
         & \multicolumn{2}{c|}{Regularization coefficient $\epsilon_{\mathrm{reg}}$} & 0.1 & - \\ 
         & \multicolumn{2}{c|}{Latent dimension}  & 10 & 10\\ 
         & \multicolumn{2}{c|}{Batch size for RL}  & 256 & 512 \\ 
         & \multicolumn{2}{c|}{Context batch size $N_c$} & 128 & 256 \\ 
         & \multicolumn{2}{c|}{Num. of exploration trajectories   $N_{\mathrm{exp}}$} & 2 (4 for Ant-Goal-OOD) & 2 \\ 
         & \multicolumn{2}{c|}{Num. of RL trajectories $N_{\mathrm{RL}}$} & 3 (6 for Ant-Dir-4) & 3 \\ 
         & \multicolumn{2}{c|}{Sampling frequency $H_{\mathrm{freq}}$} & 20 & 50 \\ 
         & \multicolumn{2}{c|}{Model gradient steps per epoch $K_{\mathrm{model}}$} & \begin{tabular}[c]{@{}c@{}}500 (1000 for Walker/Hopper-Mass-OOD)\end{tabular} & 1000 \\ 
         & \multicolumn{2}{c|}{RL gradient steps per epoch $K_{\mathrm{RL}}$} & \begin{tabular}[c]{@{}c@{}}4000 (1000 for Cheetah-Vel-OOD)\end{tabular} & 4000 \\ 
         \cline{2-5}
         & \multirow{3}{*}{Network sizes} & $q_\psi,Q_\theta,\pi_\theta$ & [300,300,300] & [300,300,300] \\
         &  & $p_\phi, p_{\tilde{\phi}}$ & [256,256,256] & [256,256,256] \\ 
         &  & $f_\zeta$ & [200,200,200] & [200,200,200] \\ 
          \hline 
    \end{tabular}}
\vspace{3em}
\centering
\caption{MuJoCo Environmental Hyperparameters}
\label{table:env_hyper}
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0 \textwidth}{!}{%
    \begin{tabular}{|c|cc|cccccc|} \hline
         &  \multicolumn{2}{c|}{\multirow{2}{*}{Name}}  & \multicolumn{6}{c|}{Environments} \\ \cline{4-9}
        \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Environmental\\Hyperparameters\end{tabular}} &  & & Cheetah-Vel-OOD & Ant-Dir-2 & Ant-Dir-4 & Ant-Goal-OOD & Hopper-Mass-OOD & Walker-Mass-OOD \\ \cline{2-9}
         & \multicolumn{2}{c|}{Reward scale $\lambda_{\mathrm{rew}}$} & 5.0 & 5.0 & 5.0 & 1.0 & 5.0 & 5.0 \\ 
         & \multicolumn{2}{c|}{Entropy coefficient  $\lambda_{\mathrm{ent}}$} & 1.0 & 0.5 & 0.5 & 0.5 & 0.2 & 0.2 \\ 
         & \multicolumn{2}{c|}{Num. of training tasks  $N_{\mathrm{train}}$} & 100 & 2 & 4 & 150 & 100 & 100 \\ 
         & \multicolumn{2}{c|}{Task batch size  $N_{\mathrm{meta}}$}  & 16 & 2 & 4 & 16 & 16 & 16 \\ 
         & \multicolumn{2}{c|}{Num. of VTs $N_{\mathrm{VT}}$} & 5 & 1 & 2 & 5 & 5 & 5 \\ 
         & \multicolumn{2}{c|}{Num. of mixining tasks  $M$}  & 3 & 2 & 2 & 3 & 3 & 3 \\ \hline
    \end{tabular}}
\vspace{1em}
\centering
\caption{MetaWorld ML1 Environmental Hyperparameters}
\label{table:env_hyper_ml1}
\renewcommand{\arraystretch}{1.2}
\resizebox{1.0 \textwidth}{!}{%
    \begin{tabular}{|c|cc|cccccc|} \hline
         &  \multicolumn{2}{c|}{\multirow{2}{*}{Name}}  & \multicolumn{6}{c|}{Environments} \\ \cline{4-9}
        \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Environmental\\Hyperparameters\end{tabular}} &  & & Reach & Reach-OOD-Inter & Reach-OOD-Extra & Push & Push-OOD-Inter & Push-OOD-Extra \\ \cline{2-9}
         & \multicolumn{2}{c|}{Reward scale $\lambda_{\mathrm{rew}}$} & 1.0 & 1.0 & 1.0 & 5.0 & 5.0 & 5.0 \\ 
         & \multicolumn{2}{c|}{Entropy coefficient  $\lambda_{\mathrm{ent}}$} & 0.2 & 0.2 & 0.2 & 1 & 1 & 1 \\ 
         & \multicolumn{2}{c|}{Num. of training tasks  $N_{\mathrm{train}}$} & 50 & 50 & 50 & 50 & 50 & 50 \\ 
         & \multicolumn{2}{c|}{Task batch size  $N_{\mathrm{meta}}$}  & 16 & 16 & 16 & 16 & 16 & 16 \\ 
         & \multicolumn{2}{c|}{Num. of VTs $N_{\mathrm{VT}}$} & 5 & 5 & 5 & 5 & 5 & 5 \\ 
         & \multicolumn{2}{c|}{Num. of mixining tasks  $M$}  & 3 & 3 & 3 & 3 & 3 & 3 \\ \hline
    \end{tabular}}

\end{table}





\clearpage
%\newpage

\section{More Detailed Experimental Setups}
\label{secapp:expdetail}

In this section, we provide details about the experiments and experimental setup. In \ref{secapp:expdetail_mj}, we describe the OOD setup of the MuJoCo environments; in \ref{secapp:expdetail_ml1}, we explain the OOD setup of the ML1 environments; in \ref{secapp:baselinedetail}, we outline the baseline setup.


\subsection{Mujoco Environments}
\label{secapp:expdetail_mj}
In this section, we provide a detailed description of the computational setup and the Mujoco environments considered in Section \ref{sec:experiments}. We utilized Mujoco environments from the OpenAI Gym library \cite{brockman2016openai} and employed Mujoco 200 libraries for environments with varying reward functions (Cheetah-Vel-OOD, Ant-Dir-2, Ant-Dir-4, Ant-Goal-OOD). For environments with different state transition dynamics (Hopper-Mass-OOD, Walker-Mass-OOD), we used Mujoco 131 libraries, as suggested by previous studies \cite{finn2017model, rakelly2019efficient}. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7 \textwidth]{figures/env_description.png} 
    \caption{Task configurations for the Mujoco environments under consideration are presented. Training tasks are indicated in blue, while OOD test tasks are marked in red.}
    \label{fig:task_config}
\end{figure}


To provide a more detailed description of the Mujoco environments, Fig. \ref{fig:task_config} illustrates the task configurations for the environments considered. In addition, for each Mujoco environment, we explain the design of the reward function, how components of the environment change with different tasks, and the configuration of out-of-distribution OOD tasks as follows:
\begin{itemize}
    \item Cheetah-Vel-OOD: The Cheetah agent is tasked with moving at target velocities $v_{\mathrm{tar}}$ where the reward increases as the agent's velocity more closely matches the target. The reward function is designed to be the negative value of the difference between the current velocity and the target velocity. For training tasks, target velocities $v_{\mathrm{tar}}$ are sampled from the training task space $\mathcal{M}_{\mathrm{train}}=[0.0,0.5)\cup[3.0,3.5)$. For OOD tasks, target velocities are sampled from the test task space $\mathcal{M}_{\mathrm{test}}=\{0.75, 1.25, 1.75, 2.25, 2.75\}$. 
    \item Ant-Dir-2: The Ant agent is required to move in target directions $\theta_{\mathrm{dir}}$, with higher rewards given when the agent's movement aligns more accurately with the target direction. The reward function is the dot product of the agent's velocity and the target direction. During training, target directions $\theta_{\mathrm{dir}}$ are sampled from the training task space $\mathcal{M}_{\mathrm{train}}=\{0, \frac{\pi}{2}\}$. For OOD tasks, target directions are sampled from the test task space $\mathcal{M}_{\mathrm{test}}=\{\frac{\pi}{4}, \frac{3\pi}{4}, \frac{7\pi}{4}\}$, including extrapolated tasks ($\frac{3\pi}{4}$ and $\frac{7\pi}{4}$ directions) from the training task space.
    \item Ant-Dir-4: The Ant agent is required to move in target directions $\theta_{\mathrm{dir}}$, similar to the Ant-Dir-2 environment. The key difference lies in the task set. In this environment, the target directions $\theta_{\mathrm{dir}}$ for training tasks are sampled from the training task space $\mathcal{M}_{\mathrm{train}}=\{0, \frac{\pi}{2}, \pi, \frac{3\pi}{2}\}$. For OOD tasks, the target directions are sampled from the test task space $\mathcal{M}_{\mathrm{test}}=\{\frac{\pi}{4}, \frac{3\pi}{4}, \frac{5\pi}{4}, \frac{7\pi}{4}\}$.
    \item Ant-Goal-OOD: The Ant agent is tasked with reaching specific 2D goal positions given by $(r_{\mathrm{goal}}\cos\theta_{\mathrm{goal}},  r_{\mathrm{goal}}\sin\theta_{\mathrm{goal}})$. The agent receives a higher reward for reaching the goal position more accurately. The goal reward function is designed as the negative $L_1$ distance from the current position to the target goal position. For training, the goal positions $r_{\mathrm{goal}}$ and $\theta_{\mathrm{goal}}$ are sampled from the training task space $\mathcal{M}_{\mathrm{train}}$ with $r_{\mathrm{goal}} \in [0.0, 1.0)\cup[2.5, 3.0)$ and $\theta_{\mathrm{goal}} \in [0,2\pi]$. For OOD test tasks, the goal positions are sampled from the test task space $\mathcal{M}_{\mathrm{test}}$ with $r_{\mathrm{goal}}=1.75$ and $\theta_{\mathrm{goal}}\in\{0, \frac{\pi}{2},\pi, \frac{3\pi}{2}\}$.
    \item Hopper-Mass-OOD: The Hopper agent is required to move forward while dealing with varying body mass across different tasks, which alters the state transition dynamics. The reward function used is the same as that in the original MuJoCo Hopper environment. The task set is constructed by adjusting the initial mass of all joints of the Hopper using a multiplier $m_{\mathrm{scale}}$, which is an internal parameter of the environment that scales the mass up or down. For training, the body mass multiplier $m_{\mathrm{scale}}$  is sampled from the training task space  $\mathcal{M}_{\mathrm{train}}=[0.0,0.5)\cup[3.0,3.5)$. For OOD test tasks, the body mass multiplier is sampled from the test task space $\mathcal{M}_{\mathrm{test}}=\{0.75, 1.25, 1.75, 2.25, 2.75\}$. 
    \item Walker-Mass-OOD: The Walker2D agent is tasked with moving forward while managing varying body mass across different tasks, which affects the state transition dynamics. The reward function remains consistent with that used in the original MuJoCo Walker2D environment. The task set is created by adjusting the initial mass of all joints of the Walker using a multiplier $m_{\mathrm{scale}}$, an internal parameter of the environment that scales the mass either up or down, as similar to Hopper-Mass-OOD. For training, the body mass multiplier $m_{\mathrm{scale}}$  is sampled from the training task space  $\mathcal{M}_{\mathrm{train}}=[0.0,0.5)\cup[3.0,3.5)$. For OOD test tasks, the body mass multiplier is sampled from the test task space $\mathcal{M}_{\mathrm{test}}=\{0.75, 1.25, 1.75, 2.25, 2.75\}$. 
\end{itemize}


\subsection{MetaWorld ML1 Environments}
\label{secapp:expdetail_ml1}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9 \columnwidth]{figures/ML1_env_description.png} 
    \caption{(a) Reach environment that the end effector should reach to target position. (b) Push environment that the robot arm should push the object to the target position. (c) Visualization of ML1-OOD setup that separate 3D goal space $\mathcal{M}$ into $\mathcal{M}_{{\mathrm{inner}}}$ and $\mathcal{M} \backslash \mathcal{M}_{{\mathrm{inner}}}$. }
    \label{fig:ml1_tasks}
\end{figure}


ML1-Reach and ML1-Push are environments in the MetaWorld benchmark \cite{yu2020meta}, where a robotic arm either reaches a target position in 3D space (ML1-Reach) or pushes an object to a target position (ML1-Push). Tasks are defined by the goal target position, $g_{\mathrm{tar}}$, and their reward functions are determined accordingly. The target position is sampled from a 3D goal space $\mathcal{M}$, a rectangular cuboid defined by two vertices, $g_{\mathrm{low}}$ and $g_{\mathrm{high}}$, which are given in the original implementation code and differ between ML1-Reach and ML1-Push. Training and test tasks are sampled from $\mc{M}{\mathrm{train}}$ and $\mc{M}{\mathrm{test}}$ with $N_{\mathrm{train}}$ and $N_{\mathrm{test}}$ tasks, respectively. In the basic ML1 setup, $\mc{M}{\mathrm{train}}$ and $\mc{M}{\mathrm{test}}$ are identical to $\mc{M}$.

For OOD setups, a smaller central region, $\mathcal{M}{{\mathrm{inner}}}$, is defined within $\mathcal{M}$. The goal space $\mathcal{M}$ is divided into 125 smaller cuboids $(5 \times 5 \times 5)$, with $\mathcal{M}{{\mathrm{inner}}}$ consisting of 27 central cuboids $(3 \times 3 \times 3)$. Fig. \ref{fig:ml1_tasks}(a) represents the Reach environment, Fig. \ref{fig:ml1_tasks}(b) shows the Push environment, and Fig. \ref{fig:ml1_tasks}(c) illustrates the goal configuration in the MetaWorld environments. Table \ref{table:ml1_ood_setup} summarizes the OOD setups for ML1 environments.
\begin{itemize}
    \item Basic ML1 setups : The training goal target positions (train tasks) are sampled with 50 positions from the regions of $\mathcal{M}$, while the testing goal target positions (test tasks) are sampled with 50 positions from the same regions of $\mathcal{M}$.
    \item OOD-Inter setup : The training goal target positions (train tasks) are sampled with 50 positions from the regions of $\mathcal{M}$ excluding $\mathcal{M}_{{\mathrm{inner}}}$, while the testing goal target positions (test tasks) are composed of the center points of the 27 cuboids within $\mathcal{M}_{{\mathrm{inner}}}$.
    \item OOD-Extra setup : The training goal target positions (train tasks) are sampled with 50 positions from within $\mathcal{M}_{{\mathrm{inner}}}$, while the testing goal target positions (test tasks) are composed of the center points of the remaining 98 $(125-27)$ cuboids outside $\mathcal{M}_{{\mathrm{inner}}}$. 
\end{itemize}


\begin{table}[!ht]
\centering
\caption{Configuration of basic ML1 and ML1 OOD environments setup.}
\label{table:ml1_ood_setup}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.98 \textwidth}{!}{%
    \begin{tabular}{ccccccc}
    \hline
    \multicolumn{1}{c}{} & $g_{\mathrm{low}}$ & $g_{\mathrm{high}}$  & $\mathcal{M}_{\mathrm{train}}$  & $\mathcal{M}_{\mathrm{test}}$ & $N_{\mathrm{train}}$ & $N_{\mathrm{test}}$ \\ \hline
    Reach & \multirow{3}{*}{$(-0.1, 0.8, 0.05)$} & \multirow{3}{*}{$(0.1, 0.9, 0.3)$} & $\mathcal{M}$  & $\mathcal{M}$ & 50 & 50 \\
    Reach-OOD-Inter  & &  & $\mathcal{M} \backslash \mathcal{M}_{{\mathrm{inner}}}$ & $\{$27 center points of $\mathcal{M}_{{\mathrm{inner}}} \}$ & 50 & 27 \\
    Reach-OOD-Extra   & & & $\mathcal{M}_{{\mathrm{inner}}}$   & $\{$98 center points of $\mathcal{M} \backslash \mathcal{M}_{{\mathrm{inner}}} \}$ & 50 & 98 \\ \hline
    Push & \multirow{3}{*}{$(-0.1, 0.8, 0.01)$} & \multirow{3}{*}{$(0.1, 0.9, 0.02)$} & $\mathcal{M}$ & $\mathcal{M}$ & 50 & 50 \\
    Push-OOD-Inter    & &   & $\mathcal{M} \backslash \mathcal{M}_{{\mathrm{inner}}}$ &  27 center points of $\mathcal{M}_{{\mathrm{inner}}}$ & 50 & 27\\
    Push-OOD-Extra  &  & & $\mathcal{M}_{{\mathrm{inner}}}$ & $\{$98 center points of $\mathcal{M} \backslash \mathcal{M}_{{\mathrm{inner}}} \}$ & 50 & 98 \\ \hline
    \end{tabular}
    }
\end{table}


% \tcr{The Meta-World benchmark provides training environments and testing frameworks for multi-task RL and meta-RL, enabling robotic arms to perform a variety of tasks. Among them, the ML1 benchmarks focus on tasks where a robotic arm either reaches a target position (Reach) or pushes an object to a designated target (Push). For each task, the target position of the robot’s end-effector is uniquely defined. To evaluate the effectiveness of the proposed algorithm, we additionally constructed OOD setups for the Reach and Push tasks within the ML1 benchmark. Below, we describe the setups for Reach, Reach-OOD-Inter, Reach-OOD-Extra, Push, Push-OOD-Inter, and Push-OOD-Extra.}

% \tcr{
% \begin{itemize}
%     \item Reach is an environment where the robotic arm’s end-effector is tasked with reaching a specific target point in 3D space. Each task has a distinct target position, resulting in a varying reward function depending on the target position. The reward function is designed to provide higher rewards as the end-effector gets closer to the target position. The distribution of target points is represented as a cuboidal goal space $G$ with boundaries defined by $\mathrm{goal}_{\mathrm{low}} = (-0.10, 0.80, 0.05)$ and $\mathrm{goal}_{\mathrm{high}} = (0.10, 0.90, 0.30)$. For training and task target positions, 50 random samples are independently drawn from the same goal space distribution. $\mathcal{M}_{\mathrm{train}}=\{(g_x, g_y, g_z)\sim G\}$ and $\mathcal{M}_{\mathrm{test}}=\{(g_x, g_y, g_z)\sim G\}$
%     \item Reach-OOD-Inter is an environment similar to Reach, where the robotic arm’s end-effector must reach a specific target position in 3D space. To implement the OOD setup, the sampling spaces for training and test tasks are separated. In the OOD-Inter environment, the OOD test task distribution is defined as being inside the training task distribution. Specifically, the cuboidal Reach goal space is divided into 125 smaller cuboids by dividing each edge into five equal segments. Among these, the inner 27 cuboids form the OOD test task distribution, while the remaining 98 cuboids define the training distribution. Training target positions are randomly sampled from the training distribution, with 50 positions selected. In contrast, OOD test target positions are defined as the center points of the 27 inner cuboids. $\mathcal{M}_{\mathrm{train}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{train}})\}$, $\mathcal{M}_{\mathrm{test}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{test}})\}$
%     \item Reach-OOD-Extra is an environment similar to Reach, where the robotic arm’s end-effector is tasked with reaching a specific target position in 3D space. To implement the OOD setup, the sampling spaces for training and test tasks are separated. In the OOD-Extra environment, the OOD test task distribution lies outside the training task distribution. Specifically, the cuboidal Reach goal space is divided into 125 smaller cuboids by splitting each edge into five equal segments. The inner 27 cuboids define the training task distribution, while the remaining 98 outer cuboids constitute the OOD test task distribution. Training target positions are randomly sampled from the training distribution, with 50 positions selected. OOD test target positions are defined as the center positions of the 98 outer cuboids. $\mathcal{M}_{\mathrm{train}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{train}})\}$, $\mathcal{M}_{\mathrm{test}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{test}})\}$
%     \item Push is an environment where a robotic arm’s end-effector is tasked with pushing a specific object to a designated target point in 3D space. Each task has a unique target point, resulting in a reward function that varies depending on the target. The reward function is designed to provide higher rewards as the object approaches the target point. The distribution of target points is represented as a cuboidal goal space with boundaries defined by $\mathrm{goal}_{\mathrm{low}} = (-0.10, 0.80, 0.01)$ and $\mathrm{goal}_{\mathrm{high}} = (0.10, 0.90, 0.02)$. Training and task target points are independently sampled, with 50 random samples each drawn from the same goal space distribution. $\mathcal{M}_{\mathrm{train}}=\{(g_x, g_y, g_z)\sim G\}$ and $\mathcal{M}_{\mathrm{test}}=\{(g_x, g_y, g_z)\sim G\}$
%     \item The Push-OOD-Inter environment implements the OOD setup by separating the training task distribution and the test task distribution using the same method as the Reach-OOD-Inter environment. The goal space utilized in this environment is the goal space of Push. $\mathcal{M}_{\mathrm{train}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{train}})\}$, $\mathcal{M}_{\mathrm{test}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{test}})\}$
%     \item The Push-OOD-Extra environment implements the OOD setup by separating the training task distribution and the test task distribution using the same method as the Reach-OOD-Extra environment. The goal space utilized in this environment is the goal space of Push. $\mathcal{M}_{\mathrm{train}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{train}})\}$, $\mathcal{M}_{\mathrm{test}}=\{(g_x, g_y, g_z)\sim p(\Task_{\mathrm{test}})\}$
% \end{itemize}
% }


\subsection{Baselines Implementation}
\label{secapp:baselinedetail}


\tb{RL$^2$}~ We utilize the open source codebase of LDM at \href{https://github.com/suyoung-lee/LDM}{https://github.com/suyoung-lee/LDM} for MuJoCo environments and open source codebase of garage \href{https://github.com/rlworkgroup/garage}{https://github.com/rlworkgroup/garage} for MetaWorld ML1 environments to report the results of RL$^2$. We modify the task space of MuJoCo and ML1 environments to be divided into $\mc{M}_{\mathrm{train}}$ and $\mc{M}_{\mathrm{test}}$, in other words, OOD setup.

\tb{VariBAD and LDM}~ We utilize the open source reference of LDM at \href{https://github.com/suyoung-lee/LDM}{https://github.com/suyoung-lee/LDM} for MuJoCo environments and open source reference of SDVT \href{https://github.com/suyoung-lee/SDVT}{https://github.com/suyoung-lee/SDVT} for MetaWorld ML1 environments to acquire the results of VariBAD and LDM. We modify the task space of MuJoCo and ML1 environments to be divided into $\mc{M}_{\mathrm{train}}$ and $\mc{M}_{\mathrm{test}}$.

\tb{MAML}~ We utilize the open source code of garage  at \href{https://github.com/rlworkgroup/garage}{https://github.com/rlworkgroup/garage} for both MuJoCo and ML1 environments to get the results of MAML. We modify the task space of MuJoCo and ML1 environments to be divided into $\mc{M}_{\mathrm{train}}$ and $\mc{M}_{\mathrm{test}}$.

\tb{PEARL}~ We utilize the open source repository of PEARL at \href{https://github.com/katerakelly/oyster}{https://github.com/katerakelly/oyster} for MuJoCo environments and the open source repository of garage \href{https://github.com/rlworkgroup/garage}{https://github.com/rlworkgroup/garage} for ML1 environments to get the results of PEARL. We modify the task space of MuJoCo and ML1 environments to be divided into $\mc{M}_{\mathrm{train}}$ and $\mc{M}_{\mathrm{test}}$.

\tb{CCM, MIER, Amago}~ We utilize the open source code of CCM, MIER and Amago at \href{https://github.com/TJU-DRL-LAB/self-supervised-rl/tree/ece95621b8c49f154f96cf7d395b95362a3b3d4e/RL_with_Environment_Representation/ccm}{https://github.com/TJU-DRL-LAB/self-supervised-rl/tree/ece95621b8c49f154f96cf7d395b95362a3b3d4e/RL\textunderscore with\textunderscore Environment\textunderscore Representation/ccm}, \href{https://github.com/russellmendonca/mier_public}{https://github.com/russellmendonca/mier\textunderscore public} and \href{https://github.com/UT-Austin-RPL/amago}{https://github.com/UT-Austin-RPL/amago}, respectively for both MuJoCo and ML1 environments to measure the experimental results of the baselines. We modify the task space of MuJoCo and ML1 environments to be divided into $\mc{M}_{\mathrm{train}}$ and $\mc{M}_{\mathrm{test}}$.

\tb{TAVT}~ We research and develop TAVT algorithm on top of the PEARL official open-source algorithm at \href{https://github.com/katerakelly/oyster}{https://github.com/katerakelly/oyster} for both MuJoCo environments and ML1 environments. 

We use the MetaWorld benchmark open source at \href{https://github.com/Farama-Foundation/Metaworld}{https://github.com/Farama-Foundation/Metaworld} for all experiments, but install the ``paper version'' of it. All experiments are conducted on a GPU server with an NVIDIA GeForce RTX 3090 GPU and AMD EPYC 7513 32-Core processors running Ubuntu 20.04, using PyTorch. 



\newpage
\section{Learning Curves for MetaWorld Environments}
\label{secapp:ML1graph}

% \tcr{(* Add On-policy)}

Fig. \ref{fig:ml_suc_curves} presents the learning curves from the comparison experiments in Section \ref{sec:experiments}. Similar to the results in Table \ref{table:comparison}, the learning curves show that the proposed TAVT consistently outperforms other on-policy and off-policy meta-RL methods. Notably, compared to off-policy algorithms, TAVT demonstrates both superior convergence performance and faster convergence speed. These results further highlight the effectiveness and superiority of the proposed TAVT algorithm.



\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8 \columnwidth]{figures/suc_rate_3.png} 
    \caption{Performance Comparison on MetaWorld ML1 Environments}
    \label{fig:ml_suc_curves}
\end{figure}



\newpage
\section{Ablation Study on the Mixing Coefficient $\beta$}
\label{secapp:addabl}

In this section, we discuss the impact of the mixing coefficient $\beta$ for VT construction introduced in Section \ref{sec:preliminary}. Recall that the task latent for VT, denoted as $\z^\alpha$, is obtained by interpolating among $M$ randomly sampled training task latents, as follows:
\[\z^\alpha = \sum_{i=1}^M \alpha^{i}\z^{i},\]
where $\mathbf{\alpha}=(\alpha^1,\cdots,\alpha^M)\sim \beta \text{Dirichlet}(1,1,\ldots,1) - \frac{\beta - 1}{M}$ is the interpolation coefficient. Here, $\text{Dirichlet}(\cdot)$ represents the Dirichlet distribution, and $\beta \geq 1$ is the mixing coefficient. When $\beta=1$, the interpolation is limited to within the space of the training task latents. In contrast, $\beta > 1$ enables extrapolation beyond the original task latents, allowing for a broader range of virtual tasks to be generated. To analyze the impact of the mixing coefficient $\beta$ on VT construction and learning performance, we examine the range of $\z^\alpha$ with varying $\beta$ in Section \ref{subsecapp:beta_coverage}, explore how exploration behavior changes with different $\beta$ values in Section \ref{subsecapp:beta_exploration}, and evaluate the performance based on different $\beta$ values in Section \ref{subsecapp:beta_perform}.


\subsection{Coverage of $\z^\alpha$ with Various $\beta$}
\label{subsecapp:beta_coverage}
% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.8 \columnwidth]{figures/beta_toy.png} 
%     \caption{Toy example of Dirichlet interpolation with $\beta$ value in Ant-Goal-OOD environment. Blue dots represent goal points and red dots represent virtual goal points generated by Dirichlet interpolation with different $\beta$. (a) $\beta=1$, (b) $\beta=2$, (c) $\beta=3$}
%     \label{fig:beta_coverage}
% \end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0 \columnwidth]{figures/beta_area.png} 
    \caption{(a) 2D goal positions of the Ant-Goal-OOD environment, and PCA visualizations of $\z^\alpha$ with different $\beta$ values: (b) TAVT with $\beta=1$, (c) TAVT with $\beta=2$, and (d) TAVT with $\beta=3$.}
    \label{fig:beta_coverage}
\end{figure}

Fig. \ref{fig:beta_coverage} illustrates how the coverage of task latents $\z^\alpha$ for VTs generated from the Dirichlet distribution varies with the mixing coefficient $\beta$ in the Ant-Goal-OOD environment. Specifically, Fig. \ref{fig:beta_coverage}(a) displays the 2D goal positions for training and OOD tasks, while Figs. \ref{fig:beta_coverage}(b), (c), and (d) show the principal component analysis (PCA) visualizations of $\z^\alpha$ generated with $\beta = 1$, $2$, and $3$, respectively. Here, we use PCA instead of t-SNE for visualization, as PCA more accurately reflects the data structure in the projection, whereas t-SNE often clusters data by performing manifold learning. The results in Fig. \ref{fig:beta_coverage} show that as $\beta$ increases, the range of task latents $\z^\alpha$ generated by VT construction becomes broader, effectively covering a wider area, including extrapolated task latents as intended in Section \ref{sec:preliminary}.

\subsection{Exploration Trajectories of $\pi_{\mathrm{exp}}$ with Various $\beta$}
\label{subsecapp:beta_exploration}

Fig. \ref{fig:beta_exploration} illustrates how exploration trajectories change with different $\beta$ values during the meta-testing phase. For $\beta=1$ in Fig. \ref{fig:beta_exploration}(a), $\z^\alpha$ considers only a narrow range of the task latent space, causing the agent to explore only the areas close to the inner region. For $\beta=2$ in Fig. \ref{fig:beta_exploration}(b), $\z^\alpha$ covers a broader range, which is sufficient to distinguish the tasks from the contexts. For $\beta=3$ in Fig. \ref{fig:beta_exploration}(c), $\z^\alpha$ covers the largest area but includes excessive extrapolated regions, causing the agent to explore beyond the goal space and making task distinction more challenging. Thus, we use $\beta=2$ as the default mixing coefficient for TAVT.

\begin{figure}[!h]
    \centering
    \includegraphics[width=1 \columnwidth]{figures/ant_beta.png} 
    % \includegraphics[width=1 \columnwidth]{figures/ant_step20_beta1,2,3.png} 
    \caption{Exploration trajectories of $\pi_{\mathrm{exp}}(\cdot|s,\z^\alpha)$ with different $\beta$: (a) $\beta=1$, (b) $\beta=2$, and (c) $\beta=3$.}
    \label{fig:beta_exploration}
\end{figure}

\subsection{Performance Analysis with Various $\beta$}
\label{subsecapp:beta_perform}

From the results in Sections \ref{subsecapp:beta_coverage} and \ref{subsecapp:beta_exploration}, we observed that the choice of $\beta$ significantly impacts TAVT learning. Finally, to evaluate the effect of $\beta$ on performance, Fig. \ref{fig:beta_perfom} illustrates the performance changes across 3 environments, (a) Cheetah-Vel-OOD, (b) Ant-Goal-OOD, and (c) Walker-Mass-OOD, with varying $\beta$ values. From the results in Fig. \ref{fig:beta_perfom}, we observe that $\beta=2$ consistently performs the best across the considered environments. Therefore, we set $\beta=2$ as the default mixing coefficient for TAVT. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.0 \columnwidth]{figures/beta_perform.png} 
    \caption{Performance comparison with various $\beta$ values on: (a) Cheetah-Vel-OOD environment, (b) Ant-Goal-OOD environment, and (c) Walker-Mass-OOD environment.}
    \label{fig:beta_perfom}
\end{figure}







% \newpage
\section{Limitations}
\label{secapp:limit}
The proposed TAVT algorithm introduces a novel metric-based task representation learning method and generates virtual task samples to train both the task encoder and the policy, enabling preparation for unseen out-of-distribution tasks. Despite these advantages of TAVT, it has the several areas for improvement. First, the training process requires significant input/output data and multiple networks, leading to slower training speeds and increased memory usage. While TAVT takes slightly longer to train, it achieves better performance than baseline algorithms, even when they are trained for extended durations. Optimizing the training process to reduce memory and time requirements could address this issue. Second, the algorithm relies on a relatively large number of hyperparameters to achieve effective learning balance, making it challenging to tune them for specific environments. However, since most hyperparameters are shared across environments, the actual tuning effort is limited to a smaller subset. Automating hyperparameter optimization or developing adaptive techniques could simplify this process. Future work will focus on algorithmic enhancements to improve training efficiency, reduce resource demands, and streamline hyperparameter tuning to further strengthen the practicality and performance of TAVT.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
