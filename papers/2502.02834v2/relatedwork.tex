\section{Related Works}
\label{sec:related_works}
\textbf{Advanced Task Representation Learning:}Advanced representation learning techniques have been widely explored to improve task latents that effectively distinguish tasks. Recent meta-RL methods use contrastive learning \cite{oord2018representation} to enhance task differentiation through positive and negative pairs, improving task representation \cite{laskin2020curl, fu2021towards, choshen2023contrabar} and capturing task information in offline setups \cite{li2020focal, gao2024context}. The Bisimulation metric \cite{ferns2011bisimulation} is employed to capture behavioral similarities \cite{zhanglearning, agarwal2021contrastive, liu2023robust} and group similar tasks \cite{hansen2022bisimulation, sodhani2022block}. Additionally, skill representation learning \cite{eysenbach2018diversity} addresses non-parametric meta-RL challenges \cite{frans2017meta, harrison2020continuous, nam2022skill, fu2022meta, he2024decoupling}, while task representation learning is increasingly applied in multi-task setups \cite{ishfaq2024offline, cheng2022provable, sodhani2021multi}.

\noindent \textbf{Generalization for OOD Tasks:} Meta-RL techniques for improving policy generalization in OOD test environments have been actively studied \cite{lan2019meta, fakoor2019meta, mu2022domino}. Model-based approaches \cite{lin2020model, lee2021improving}, advanced representation learning with Gaussian Mixture Models \cite{wang2023meta, lee2023parameterizing}, and Transformers \cite{vaswani2017attention, melo2022transformers, xumeta} have been explored. Additionally, some studies tackle distributional shift challenges through robust learning \cite{mendonca2020meta, mehta2020curriculum, ajay2022distributionally, pmlr-v162-chae22a, greenberg2024train}.


\noindent \textbf{Model-based Sample Relabeling:} Model-based sample generation and relabeling techniques have gained attention in meta-RL \cite{rimon2024mamba, 10565991}, enabling the reuse of samples from other tasks using dynamics models \cite{li2020multi, mendonca2020meta, wan2021hindsight, zou2024relabeling}. These methods address sparse rewards \cite{packer2021hindsight, jiang2024doubly}, mitigate distributional shifts in offline setups \cite{dorfman2021offline, yuan2022robust, zhou2024generalizable, guan2024cost}, and incorporate human preferences \cite{ren2022efficient, hejna2023few} or guided trajectory relabeling \cite{wang2023supervised}, expanding their applications.