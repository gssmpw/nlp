\newpage

\input{figures/scatter_corr}

\input{figures/chatbot-ui}

\input{figures/prompt_figures}

\section{Appendix}
We provide more details about the implementation of \method{}, our user study, and additional results.

\subsection{More Implementation Details}
\label{app.implementation}

\textbf{CLUE System UI.} \method{} system UI examples can be found in Figures \ref{fig:clue_ui}. Figure \ref{fig:target_llm_ui} shows the chat interface with the target LLM. Figure \ref{fig:clue_interviewer_ui} shows the user interface of \method-Interviewer.
We design the UI to resemble mainstream LLM interfaces.


\textbf{Interviewer Prompts.} The full prompt used in the CLUE-Interviewer system can be seen in Figure~\ref{fig:interviewer_prompt}. In the prompt, we instruct the LLM to serve as a UX researcher and conduct an interview with a user who had just chatted with a chatbot. Via step-by-step instructions, we provided the specific evaluation dimensions to cover and encouraged follow-up questions to be asked. 

In an earlier version of the interviewer development, we encouraged the interviewer to probe users for multiple rounds. Sometimes the interviewers were probed too much. We limit the interviewer to ask no more than two follow-ups per question, a conservative choice as the first step towards LLM-based UX interviewers. Future research can explore a better balance of thoroughness and user experience.



\textbf{Insighter Data Filtering Prompt} is in Figure \ref{fig:data_filter_prompt}. It is a simple prompt that leverages the LLM (Claude-3.5-Sonnet) to filter out obvious noisy data. As discussed in Sec.~\ref{sec.userstudy}, the automatic filter has very high precision but is lenient in recall.

\textbf{Insighter Dimension Classification Prompt.} The prompt used to classify the interview sessions into evaluation dimensions can be seen in Figure~\ref{fig:insight_dimension_classsifier_prompt}. We simply describe each dimension to the LLM (Claude-3.5-Sonnet) and utilize its zero-shot ability for the classification.

\textbf{Insighter Rating Prompt }is in Figure~\ref{fig:insight_rating_classsifier_prompt}. 

We acknowledge that there is room to further improve the implementation of \method-Interviewer and \method-Insighter. A better prompt engineering, finetuning dedicated LLMs for our tasks, or using next generation of LLMs (e.g., GPT-5 or Claude-4), will certainly improve the performance of \method-Interviewer and \method-Insighter. Our simple design already illustrated many interesting user opinions of LLMs, and it is only the beginning.

\input{tables/demo}


\textbf{Insighter Topic Analysis Filtering.} We filter some common generic responses by checking if the response provided by the user is a substring of any of the generic responses, which include responses like ``nothing much" or ``i don't know". In addition, this filter removes all responses that have less than 10 characters, as it is unlikely for an user to describe an quality insight in such brevity. For misspelled responses and other generic responses not caught by this rule-based filtering, a three-shot prompt, which is in Figure \ref{fig:insight_extractor_prompt}, filters out any remaining basic yes or no responses. This prompt also extracts a list of insights from the answer. Using this filtering process, roughly 27.06\% of the answers are kept and used for insight extraction.


\subsection{More Details of User Study}
\label{app.user_study}


\textbf{Informed Consent and Instructions.} Figure~\ref{fig:mturk_consent} shows the informed consent and instructions for the MTurk user study. Participants were informed of the study procedures, potential risks, compensation, future use of information, confidentiality, and voluntary participation. It specifically calls out that participants should not provide any personally identifiable information during this study.

\input{figures/mturk_consent}

\textbf{Demographics.} Table \ref{fig:demographic_mturkers} shows the demographic statistics of the MTurk participants in our user study. Demographic variables reported included gender, race/ethnicity, age, education, and marital status. 
Our participant population is biased towards certain demographic groups, perhaps due to the population distributions of MTurkers during our study period. As a result, all the opinions reflected in this paper are from this specific population, which might be different from current user bases of mainstream LLMs.

\textbf{Payment.} All participants were paid between \$4 and \$6 for their time and participation in this study. This payment rate was determined to be above the US federal minimum wage of \$7.25 per hour. We started with \$4 but then increased to \$6 to facilitate more participation. On average, users spent a total of 25 minutes on our study, corresponding to an average hourly rate of \$12, which is significantly more than the federal minimum wage.

\subsection{Additional Results}
\label{app.add_results}

\textbf{Correlations with User Explicit Rating.} Figure~\ref{fig:scatter_coor} plots the detailed correlations between explicit rating provided by user when asked by \method-Interviewer, and the ratings automatically generated based on user interview responses. Users are significantly more lenient when asked for an explicit rating, giving a lot of perfect 5s. Their responses are more scattered, with a significant fraction of 1 (bad) and 2 (mediocre). It aligns with the findings in UX research that interview is a more effective tool to discover users' true opinions.

\textbf{Interview Examples with Human Annotations.} Figure \ref{fig:chatbot_interview} shows an example of an interview session with human annotations on evaluation dimension, probing occurrences and depth, and previous chat mentioned.

\input{figures/interview_human_annotation}