\input{figures/chat_topic_global}

\section{User Study}
\label{sec.userstudy}

Approved by our university Institutional Review Board (IRB), we conduct a large scale study on user opinions of LLMs using \method{}. This section describes the user study methodology and the collected data.

\subsection{User Study Methodology}

We build the user study pipeline by hosting LLM APIs through our customized user interface. In our system, a user first chats with the target LLM. Then \method{}-Interviewer conducts in-the-moment interviews with the user with access to the user's previous chat history. Screenshot of our UI can be found in Figures~\ref{fig:clue_ui} in the Appendix.

\textbf{Recruiting.} We recruited participants via Amazon Mechanical Turk (MTurk) for our study. 
For this open-ended study, we set the qualification criteria as US only, 1k+ tasks completed, and 99\% prior approval rate.

Each participant reviewed the study description and provided informed consent that the collected data will be publicly available.  
They were instructed to not share any personal information in the study.\footnote{We will do our best to remove personally identifiable
information (PII) before releasing the data.} 
We included instructions to improve data quality, such as not using an external chatbot to complete the study. We set up FAQ in all phases of the study to provide detailed walk-through of how one can complete the task. 

\textbf{Chat with Target LLMs.} Participants then engaged in a 10-15 minute conversation with one of the six mainstream LLMs through our hosted chat interface: DeepSeek-R1, DeepSeek-V3, Gemini-1.5-Flash, Llama-3-70B, GPT-4o, and Claude-3.5-Sonnet.
The specific LLM was randomly assigned and the identity was not revealed to the user.\footnote{Some users asked the LLM who it was, though the answer returned was not necessarily true.} 

Participants freely chatted with the LLM on any topics. They were encouraged to interact for 15 minutes but could finish early.

\textbf{\method{} Interviews.} After participants interacted with the target LLM, the system directs them to discuss their experiences about the interaction with \method-Interviewer. The system has access to the previous interaction between the user and the target LLM and performs semi-structured interviews as designed in Sec.~\ref{sec.interviewer}. We use GPT-4o to power \method-Interviewer.

\textbf{Closing Survey.} At the end of the study, users were asked to complete a voluntary demographic survey, including gender, race, age group, education leve, and marital status. Demographic data was collected to understand the distribution of our study and whose opinions it is going to reflect.

\textbf{Data Filtering.} Open-ended user studies inevitably include noises. We prompt Claude 3.5 Sonnet (10-22) to filter out incomplete and low-quality chats and interviewers. We filter out interactions that  did not complete either the chatbot or the interview portion of the study, used chatbot to complete the study, or provided responses in the interview that did not make logical sense (e.g., did not understand the task, responded randomly, etc.). The details of this filter can be found in Appendix Figure~\ref{fig:data_filter_prompt}.
We manually labeled the quality of 120 sessions and compare with the automatic filter. The automatic filter has 91 precision and 72 recall.

\input{figures/interview_eva_all}

\input{tables/annotator_agree}

\subsection{Collected Data}

We ran the user study on Amazon MTurker in the period of December 2024 to January 2025 and collected 1989 user chat-and-interview sessions. In total 1206 (60.6\%) are kept after filtering.

\textbf{Overall Statistics} of our collected data are listed in Table~\ref{tab:basic-statistics-new}. Users on average interacted around 10 turns with the LLMs. The shortest interaction was with DeepSeek-R1, who produces long reasoning chains in between chats. The interaction with \method{}-Interviewer is slightly shorter as it is a more focused conversation.  

The volunteered demographic survey shows that majority of our participants are White/Caucasian, in their 20-40s, 60\% male, and with a Bachelor's degree.
All user opinions collected in this study would be representing this specific MTurker population. The detailed break down of participant demographics can be found in Appendix Table~\ref{fig:demographic_mturkers}.

\textbf{Chat Topics.} We also perform topic analysis on the user chat rounds, using similar techniques discussed in Sec.~\ref{sec.insigher}, except a larger minimum cluster size (15) and neighborhood size (15) to account for more chat rounds and the lighter filtering.

Figure~\ref{fig:chat_topic_global} shows the top topics our participants engaged with LLMs. 
As expected, participants chatted with LLMs about a large variety of tasks, covering various information seeking, entertainments, and task assistance topics. Figure~\ref{fig:topic_count} plots the number of topics included in each chat session. On average each participant chatted with the target LLMs around 6.60 topics, showing the diversity of user interactions with LLMs.

\input{tables/human_eva}

\input{tables/ratings}

\input{figures/correlation}

\input{figures/interview_topics}

\input{figures/improvement_topics}