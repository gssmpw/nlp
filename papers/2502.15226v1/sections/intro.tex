\section{Introduction}


The foundation and large language model (LLM) revolution is redefining the way users interact with the digital world.
Billions of users now chat with LLMs regularly through one unified interface and consume information through generative contents for their information, entertainment, and task assistance needs~\cite{dailyusers}.


\input{figures/motivate_eg}


Understanding user opinions about LLMs is, however, a challenging task. 
It is difficult to design one evaluation task to reflect the rich experiences provided by LLMs.
The user experiences powered through generative contents are also hard to characterize by metrics like accuracy and BLEU.
The prone to data contamination during pretraining further complicates the understanding of LLM performances~\cite{schaeffer2023pretraining, singh2024evaluation}. The community often relies on coarse user preference ratings to understand LLMs performances~\cite{chiang2024chatarena}.



This paper presents 
\method{}, Contextualized LLM-powered User Experience understanding, a new framework to gather fine-grained user opinions using massive in-the-moment user experience (UX) interviews.
Following interview principles~\cite{wilson2013interview}, we develop \method-Interviewer on top of an LLM to automatically conduct semi-structured UX interviews: asking user opinions on target dimensions and probing for deeper insights (Figure~\ref{fig:mov}).
As an automatic interviewer, it can chat with users right after their interactions with a product, enabling massive in-the-moment interviews.
We then build \method-Insighter to automatically analyze \method{} interview logs to generate high-level insights of user opinions.



Approved by our university Institutional Review Board (IRB), we recruited thousands of crowd source workers to chat with an LLM, randomly selected from six LLMs from the GPT, Claude, Gemini, LLaMA, and DeepSeek families. Then \method{}-Interviewer interviews participants right after their chat sessions. We obtained 1206 chat-and-interview sessions on diverse topics initiated by crowd source workers, ranging from travel destinations to programming developments, that we will make publicly available.

Our human evaluation demonstrates the effectiveness of \method-Interviewer in conducting semi-structured and in-the-moment UX interviewers.
It covers 74\% of the pre-defined dimensions to evaluate target LLMs and actively asks follow-up questions to gain deeper insights.
The interviews are in-the-moment, with frequent references to previous chat interactions, effectively eliciting in-the-moment user opinions.
The evaluation also confirms that \method-Insighter  generates interesting insights from mass interview logs.

Our user study reflects interesting user opinions of LLMs. Users expressed reserved opinions about LLMs in their text responses to \method-Interviewer, considering all mainstream LLMs mediocre, but gave a near-perfect rating when asked explicitly. Users considered LLMs decent at understanding their needs but not as effective in meeting them. Users also expressed new feature requests in the interviews, for example, visual and multimedia capabilities, fresh knowledge access, personalized responses, and flexible access to reasoning and explanations processes, providing motivations and evidences for future LLM development.

The main contributions of this paper are:
\begin{enumerate}
    \item We present a new methodology to gather user opinions on LLMs via in-the-moment UX interviews.
    \item We develop an LLM-powered interview framework that enables automatic in-the-moment interviews at a massive scale.
    \item We collect thousands of publicly available chats and interviews that reflect interesting user opinions of LLMs.
\end{enumerate}