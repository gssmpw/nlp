\section{Evaluation of \method{}}
\label{sec.evaluation}

Before sharing the findings from our user study, this section presents human evaluations on the effectiveness of \method{}-Interviewer and \method-Insighter.

All the human evaluations are done by manually annotating 120 interview sessions or 180 interview turns, randomly sampled for each of the six target LLMs. 
We performed annotations by two annotators on 25\% of the labeled data. Their agreements are listed in Table~\ref{tab:annotator_agree}. An example \method-Interviewer session with manual annotations can be found in Appendix Figure~\ref{fig:chatbot_interview}.

\textbf{Semi-Structured Interview.} We first manually labeled the evaluation dimensions (Table~\ref{tab:dim}) of \method-Interviewer rounds. Table~\ref{tab:evaluation} shows the coverage of \method-Interviewer for each dimension. It confirms that our prompts are effective in converting the LLM into an interviewer.
It covers majority of designed dimensions in its interviews. The only exception is coverage of credibility which can be improved in future research.

We manually labeled the interview rounds where \method-Interviewer asks follow up questions. 
The distributions of probing frequency and depth are plotted in Figure~\ref{fig:probing_occurrences} and~\ref{fig:probing_depth}. 
On more than half of interview sessions, \method-Interviewer probes users for more detailed feedback, asking on average 1.3 follow-up questions, rather than merely asking pre-defined questions.

\textbf{In-the-Moment Interview.} 
This experiment evaluates the ability of \method-Interviewer conducting in-the-moment interviews. 
We manually labeled explicit references to previous interactions in the interview sessions by the interviewer and the users, as a reflection of in-the-moment effect. The distributions are plotted in Figure~\ref{fig:interviewer_references} and~\ref{fig:user_references}. 
Interestingly, users actively refer to their previous interactions more frequently than the interviewer for more than 1/3 of times during interviews, reflecting users' in-the-moment status.

\textbf{Bird's-Eye Insights.}  \method-Insighter automatically maps interview questions into target dimensions and converts user responses into categorical ratings. We compare these two automatic operations with our human annotations in Table~\ref{tab:evaluation}. It shows that \method-Insighter, though not perfect, is sufficient to provide a bird's-eye view of user opinions from raw interview logs.