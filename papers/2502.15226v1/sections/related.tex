\section{Related Work}

Many benchmarks have been developed to evaluate predictive effectiveness of LLMs. Notable examples include GLUE~\cite{wang2018glue} and SuperGLUE~\cite{wang2019superglue}, both including a suite of natural language understanding tasks.
The suite of language tasks  quickly grows to hundreds, such as in FLAN~\cite{wei2021flan} and BIG-Bench~\cite{bigbench} benchmarks.
The community also keeps increasing task difficulty, for example, from MMLU~\cite{MMLU} to ``Humanity's Last Exam''~\cite{phan2025humanitysexam}, to test the boundary of LLM intelligence.
These evaluations are effective in reflecting LLMs' ability to predict the right label, which aligns closely with some real-world applications such as question answering.

Evaluating generated content is challenging, as two text sequences with high n-gram overlap~\cite{papineni2002bleu} or semantic similarities~\cite{zhang2019bertscore} may not lead to the same user experience~\cite{hanna2021fine}. 
Recent approaches switch to model-based evaluation and employ LLM-as-a-judge for predefined dimensions~\cite{zheng2023judging}.
Model-based evaluation has become a common practice in evaluating generated content, albeit various challenges such as self-biases and inaccuracies~\cite{li2024generation, ye2024justice, wei2024systematic}.

The ultimate verdict of an AI model is how it serves its users. 
For established applications like search engines, 
understanding model performances based on noisy and coarse user feedback is a long lasting research topic~\cite{chuklin2022click}.
There are various efforts to collect user feedback on new LLM-powered scenarios. One notable effort is Chatbot Arena~\cite{chiang2024chatarena}. It asks users for preferences on side-by-side LLM chats and computes arena scores based on that.
Many view the arena score a reliable reflection of user preferences on LLMs~\cite{realiablearena}.

UX interview is a standard approach to gather in-depth insights about user opinions~\cite{rubin2011handbook}. It is widely recognized as an effective tool to guide product developments toward increased adoption, consumer loyalty, and overall product success~\cite{hartson2012ux}.
Effective UX interviews often require interview experts~\cite{uxphd} and are too expensive to scale up.
Many explored the potential of AI-powered interview bots, for example, to conduct job interviews~\cite{li2017confiding} and conversational surveys~\cite{xiao2020tell}. Recently, \citet{li2024lm} built an LLM-powered interview system and demonstrated their effectiveness in evaluating student experiences in AI-assisted classroom.