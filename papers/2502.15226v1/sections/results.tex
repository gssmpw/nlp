\input{tables/case_study}

\section{User Opinions of LLMs}
This section presents the quantitative ratings (Sec.~\ref{sec.quantitative_rating})  and qualitative insights (Sec.~\ref{sec.qualitative_insight}) about user opinions of LLMs from our user study.

\subsection{Quantitative Ratings}
\label{sec.quantitative_rating}

Table~\ref{table:ratings} shows the ratings of LLM from our user study. The first four are automatically assigned by \method-Insighter based on users' textual responses. The last is explicitly provided by user. 

In contrast to ``beyond Ph.D. intelligence'' performances on various exam style benchmarks~\cite{phan2025humanitysexam}, all studied LLMs have significant room for improvement in open-ended chats, their main consumer scenario. All LLMs are scored around a mediocre 2 General rating. 
These fine-grained interview dimensions show that current LLMs are better at understanding user needs but less effective at meeting them. The credibility is also often questioned. 

Users are much more lenient when asked for explicit ratings. Majority of studied LLMs received 4.5 scores. 
This discrepancy aligns with the common challenges of Likert ratings, users have different levels of leniency and may not reason much about scoring~\cite{subedi2016using}. In comparison, UX interviews are known to be effective in probing out actual opinions from consumers~\cite{wilson2013interview}.

Figure~\ref{fig:spearman} shows the spearman correlations between different evaluation dimensions.  
Among all dimensions, meet needs has the strongest correlations with other dimensions, showing that the utility of LLMs---their ability to satisfy user need---is still the north star of LLM user experience. 
Similar with the cross LLM comparisons, the user explicit rating only has weak correlations with user sentiment underlying their interview responses.
 A more detailed scattered plot of correlations can be found in Figure~\ref{fig:scatter_coor} in Appendix. 

\subsection{Qualitative Insights}
\label{sec.qualitative_insight}

This set of analyses presents the qualitative insights gathered from our user interviews. 

\textbf{Fine-Grained User Feedback.} Figure~\ref{fig:interview_topics} plots the top topics gathered from user responses categorized in the first four interviewing dimensions. The topics are aggregated from all six LLMs. 

These topics reveal more fine-grained user opinions than numerical ratings. 
Users praised LLMs' understanding, but raised questions on LLMs' ability to handle complex topics and their authenticity.
When asked about their general impressions,
some users praised the transparency of AI reasoning, likely from those matched with DeepSeek-R1 which displayed reasoning chains.

\textbf{User Suggested Improvements.} We conduct a deeper dive into the suggestions provided by users when asked about potential improvements about their LLM experience. 
The top topics popped up in our analysis for each LLM is plotted in Figure~\ref{fig:improve_topics_by_llm}. 
Note that there are no pre-defined features for users to pick from in our free-text interviews. All topics come from responses users provided.

Contrast to some previous research, users have a strong preference for conciseness responses and complained about LLMs' verbosity.
Some suggestions correlate with the quantitative insights from other evaluation dimensions. For example, users have issues with LLMs' ability to understand longer contexts and would like more conversational interactions. 

We provide one of the first studies on user opinions of displaying reasoning process. Our user study indicates that it is quite a bi-polar feature. Some users explicitly requested it, while some preferred the displaying of the thought process be optional. 

Aligned with the views from many in the community, users actively request features such as multi-modality capabilities, both processing and generation. Access to real-time data is another common request, which is not surprising as many of the top chat topics (Figure~\ref{fig:chat_topic_global}) are time sensitive, signifying the benefits for retrieval augmentation.

\textbf{Example Interview Rounds.} Table~\ref{tab:case_study} lists some user responses. 
These organic consumer opinions elicited by \method-Interviewer provide interesting and valuable information about consumer opinions on LLMs, for example, which forms of multi-modality they requested on what occasions. 
The topics captured by \method-Insighter serves as a convenient entry point to these valuable user responses. We will release a demo of these interview dimensions, topics, and organic user interview logs to enable more studies from the community.