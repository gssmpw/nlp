\begin{table*}[t]
\small
\centering
\caption{\label{table:ratings} User ratings of LLMs from \method{} Interviews. The first four evaluation dimensions use five run mean ratings automatically categorized by \method-Insighter from user textual responses (1--3 Likert). The final column shows the user explicit rating (1--5 Likert). Error bars (Â±) indicate the combined within/between standard errors for auto-ratings and standard errors for explicit ratings across sessions. Uncategorizable answers are removed.}
\vspace{-0.2cm}
\begin{tabular}{l|cccc|c}
\toprule
Model & \textbf{Understanding} & \textbf{Met Needs} & \textbf{Credibility} & \textbf{General} & \textbf{User Explicit Rating}\\
\hline
\texttt{Deepseek-R1} & 2.58 $\pm$ 0.05 & 2.31 $\pm$ 0.06 & 2.38 $\pm$ 0.07 & 2.06 $\pm$ 0.03 & 4.38 $\pm$ 0.06 \\
\texttt{Deepseek-V3} & 2.62 $\pm$ 0.05 & \textbf{2.35} $\pm$ 0.06 & 2.31 $\pm$ 0.08 & \textbf{2.10} $\pm$ 0.04 & 4.56 $\pm$ 0.06 \\
\texttt{Gemini-1.5-Flash} & 2.54 $\pm$ 0.05 & 2.31 $\pm$ 0.06 & 2.28 $\pm$ 0.08 & 2.02 $\pm$ 0.04 & 4.64 $\pm$ 0.05 \\
\texttt{GPT-4o} & 2.65 $\pm$ 0.05 & 2.25 $\pm$ 0.06 & \textbf{2.48} $\pm$ 0.08 & 2.05 $\pm$ 0.04 & 4.69 $\pm$ 0.05 \\
\texttt{Claude-3.5-Sonnet} (10-22) & 2.56 $\pm$ 0.05 & 2.24 $\pm$ 0.06 & 2.30 $\pm$ 0.09 & 1.68 $\pm$ 0.04 & 4.58 $\pm$ 0.07 \\
\texttt{LLaMa-3.3-70B} & \textbf{2.68} $\pm$ 0.04 & 2.29 $\pm$ 0.06 & 2.44 $\pm$ 0.09 & 1.80 $\pm$ 0.04 & \textbf{4.71} $\pm$ 0.04 \\
\hline
{Overall} & 2.60 $\pm$ 0.02 & 2.29 $\pm$ 0.02 & 2.36 $\pm$ 0.03 & 1.97 $\pm$ 0.01 & 4.58 $\pm$ 0.02 \\
\bottomrule
\end{tabular}
\end{table*}