\section{Algorithms}\label{sec:alg}
We are now ready to provide  algorithms that solve the tasks described in Section~\ref{sec:framework}.

\begin{comment}
    \subsection{Identifying the Candidates' Set}
\label{candidatesset}

\begin{definition}
    \textbf{Candidates Set}: The set of candidates which have a chance to be the query answer.
\end{definition}
 To identify the candidates set, we must ensure it includes any potential query answer while excluding any candidates that can be definitively ruled out as the query answer.
 Ultimately, our goal is to choose a candidate among all which has the highest score in the candidates set with respect to the scoring function. In other words:

\[
\text{Query answer = } \arg\max_{c \in \text{C}} \mathcal{F}(c, q)
\]
Where \(\text{C}\) is the candidates set.

\begin{example}
    Given $n$ entities and a fixed $k$, there are \( \binom{n}{k} \) unique combinations of k entities. Depending on the scoring function and the context of the problem, a subset or the whole set of such candidates form the candidates set. Considering the running example to find out the top-3 hotels among the 5 ones mentioned in \autoref{tab:ny_hotels_relevance}, there can be \( \binom{5}{3} = 10 \) sets of size 3 among the whole 5 hotels. Hence, assuming all possible combinations have a chance to be the top-3 set, we will have:
    \[
    \text{C} = 
    \begin{cases}
    c_1 = \{ \text{HNY}, \text{MLN}, \text{HYN} \}, & c_2 = \{ \text{HNY}, \text{MLN}, \text{SHN} \} \\
    c_3 = \{ \text{HNY}, \text{MLN}, \text{WLD} \}, & c_4 = \{ \text{HNY}, \text{HYN}, \text{SHN} \} \\
    c_5 = \{ \text{HNY}, \text{HYN}, \text{WLD} \}, & c_6 = \{ \text{HNY}, \text{SHN}, \text{WLD} \} \\
    c_7 = \{ \text{MLN}, \text{HYN}, \text{SHN} \}, & c_8 = \{ \text{MLN}, \text{HYN}, \text{WLD} \} \\
    c_9 = \{ \text{MLN}, \text{SHN}, \text{WLD} \}, & c_{10} = \{ \text{HYN}, \text{SHN}, \text{WLD} \} 
    \end{cases}
\]

Initially, we may exclude a subset of these candidates that are assuredly not the query answer, based on prior knowledge on the context of the problem. 

\end{example}


%Example: hotels with initial score, first update on bounds

%Now, assume another table of already called and obtained bounds, consider two questions which one results in finding the winner? hence, we pick question that reduces the ucertainty the most. but how to measure uncertainty of a candidate set?
\end{comment}


\subsection{Computing Score Bounds of Candidate Sets}
\label{bound_computation}

Given the set of candidates,  we discuss algorithms to compute score bounds of each of them. Since only partial scores of the candidates are known, this process computes the score bounds (LB\_c, UB\_c) of each candidate $c$. These bounds are calculated based on $\mathcal{F}(c, q)$ and the known information $Q_k$ and are updated when $Q_k$ is updated. Given $c$, the process looks at the constructs in $\mathcal{F}(c, q)$, and uses the actual scores for the parts that could be obtained from $Q_k$. For the rest, it uses the minimum possible score to produce LB\_c and the maximum possible score to produce UB\_c. Without any further assumption in place, this gives tight bounds.

\begin{comment}

\begin{definition}[Candidates Set Bounds]
\label{candidate_set} Given a scoring function $\mathcal{F}$ and n candidates, candidates set Bounds represent the score bounds for each candidate at a point in time. candidates set Bounds (C) can be represented as follows:

\[
\text{C} = \{ c_1:(\text{LB}_{c_1}, \text{UB}_{c_1}), c_2: (\text{LB}_{c_2}, \text{UB}_{c_2}), \ldots, c_n:(\text{LB}_{c_n}, \text{UB}_{c_n}) \}
\] 
\end{definition}

After each question from the LLM, the bounds for the candidates need to be updated. 

Algorithm~\ref{alg:bound} represents the procedure for updating the bounds of $c$. %Depending on what components the scoring function \( \mathcal{F}(c, q) \) is composed of, different types of questions may contribute to the overall score of a candidate, and how these questions contribute to the overall score depends on the exact formula of \( \mathcal{F}(c, q) \). Let \( Q_c \) represent the set of all questions that contribute to the total score of candidate \( c \). If $Q \in Q_c$, then the bounds of $c$ needs to be updated since $Q$ is not unknown anymore. 

\begin{algorithm}
\caption{Update Bounds for Candidates}\label{alg:bound}
\begin{algorithmic}[1]
\State \( res \leftarrow LLM(Q) \)
\For{each candidate \( c \in C \)}
    \State \( Q_c \leftarrow \) set of all questions contributing to the score of \( c \)
    \If{\( Q \in Q_c \)}
        \State Substitute \( Q \) with \( res \) in the formula of \( \mathcal{F}(c, q) \)
        \State \( LB_c \gets \min_{Q_u} \mathcal{F}(c, q) \)
        \State \( UB_c \gets \max_{Q_u} \mathcal{F}(c, q) \)
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
\end{comment}

\begin{comment}
   \begin{algorithm}
\caption{Update Bounds for Candidates}
\begin{algorithmic}[1]
\State \( res \leftarrow LLM(Q) \)
\For{each candidate \( c \in C \)}
    \State \( Q_c \leftarrow \) set of all questions contributing to the score of c
    \If{\( Q \in Q_c \)} 
        \If{scoring function is based on sum}
            \State \( LB_c \gets LB_c + res \) 
            \State \( UB_c \gets UB_c - (MAX\_SCORE - res) \)
        \ElsIf{scoring function is based on average}
            \State \( n \gets \text{len}(Q_c) \)
            \State \( LB_c \gets LB_c + \frac{res}{n} \) 
            \State \( UB_c \gets UB_c - \frac{MAX\_SCORE - res}{n} \)
        \EndIf
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm} 




Given the running example with $\mathcal{F}_{\text{rel+div}}(c, q)$ as the scoring function, let us assume 
\[
Q_k = \emptyset
\]

Given that \(k = 3\), for an arbitrary candidate \(c = \{e_1, e_2, e_3\}\), we express the score as follows:

\begin{align*}
\mathcal{F}(c, q) &= \sum_{e \in c} \text{Rel}(q, e) + \sum_{e_i, e_j \in c} \text{Div}(e_i, e_j) \\
&= \text{Rel}(q, e_1) + \text{Rel}(q, e_2) + \text{Rel}(q, e_3) \\
&\quad + \text{Div}(e_1, e_2) + \text{Div}(e_1, e_3) + \text{Div}(e_2, e_3)
\end{align*}
\end{comment}

To simplify exposition, let us assume that the minimum and the maximum relevance and diversity scores of each construct is within \((0, 1)\). On the other hand, as shown above, there exists a total of $6$ constructs contributing to the overall score of a candidate. Therefore, initially, the LB and UB of any candidate is $0$ and $6$, respectively.

\begin{comment}
    \begin{align*}
\text{LB}(c_i) &= \text{MIN\_SCORE} = 0 \quad \forall c_i \in \text{candidates} \\
\text{UB}(c_i) &= \text{MAX\_SCORE} = 6 \quad \forall c_i \in \text{candidates}
\end{align*}

Hence, We can represent the initial candidates set with their corresponding bounds as follows:

\[
    \text{C} = 
    \begin{cases}
    c_1 = \{ \text{HNY}, \text{MLN}, \text{HYN} \}, & c_2 = \{ \text{HNY}, \text{MLN}, \text{SHN} \} \\
    c_3 = \{ \text{HNY}, \text{MLN}, \text{WLD} \}, & c_4 = \{ \text{HNY}, \text{HYN}, \text{SHN} \} \\
    c_5 = \{ \text{HNY}, \text{HYN}, \text{WLD} \}, & c_6 = \{ \text{HNY}, \text{SHN}, \text{WLD} \} \\
    c_7 = \{ \text{MLN}, \text{HYN}, \text{SHN} \}, & c_8 = \{ \text{MLN}, \text{HYN}, \text{WLD} \} \\
    c_9 = \{ \text{MLN}, \text{SHN}, \text{WLD} \}, & c_{10} = \{ \text{HYN}, \text{SHN}, \text{WLD} \} 
    \end{cases}
\]

\[
\text{C\_bounds} = \{ c_1: (0, 6), c_2: (0, 6), c_3: (0, 6), \ldots, c_{10}: (0, 6) \}
\]

\end{comment}

Now, let us assume \( Rel(WLD, q) \) is obtained and \( Rel(WLD, q) = 0.5 \). Hence, we need to update LB and UB of those candidates containing \( WLD \). As an example, $c_2$ now becomes
\begin{align*}
\mathcal{F}(c_2, q) & = Rel(HNY, q) + Rel(MLN, q) + Rel(WLD, q) + Div(HNY, MLN) \\
          & \quad + Div(HNY, WLD) + Div(MLN, WLD) \\
          & = Rel(HNY, q) + Rel(MLN, q) + 0.5 + Div(HNY, MLN) \\
          & \quad + Div(HNY, WLD) + Div(MLN, WLD) \\
          & \implies 0.5 \leq \mathcal{F}(c_2, q) \leq 5.5
\end{align*}





\subsection{Probabilistic Model for Finding the Answer}
\label{winning_probability}
The algorithm designed for this task, namely finding $c^*$, needs to calculate the probability that the score of $c^*$ is larger than the scores of all other candidates, as defined in Equation~\ref{eq:prob}. We present algorithms for two variants - {\tt ProbInd} assumes independence among candidates and {\tt ProbDep} accounts for potential dependencies among candidates. Since the score of each candidate is a uniform probability distributions within (LB,UB), our solution requires to compute max convolution of probability distributions~\cite{rahman2015worker}, as defined below.

\begin{definition}
{\bf (Max-Convolution of Distributions).}
Assume that $f(c)$ and $g(c_1)$ are the pdfs of the two independent random variables $c$ and $c_1$ respectively. The pdf of the random variable $Max(c, c_1)$ is the max convolution of the two pdfs and is calculated as follows:
$P\left(\mathcal{F}(c_1, q) \geq \mathcal{F}(c_2, q)\right)  = \Sigma_{\forall x, x_1} P(c=x) \times P(c_1=x_1),{ x \in [LB\_c, UB\_c], x_1 \in [LB\_{c_1}, UB\_{c_1}] : x \geq x_1,} $
\end{definition}
Figure \ref{fig:max_conv} shows $Max(c_1, c_2)$ 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Max_Conv.jpg}
    \caption{Computing \(\mathbf{P(c_1 \geq c_2)}\)}
    \label{fig:max_conv}
\end{figure}

%In \autoref{uncertainty}, we use this probability distribution function (PDF) to demonstrate what is the best next question to ask expert which maximally reduces the uncertainty of finding the winner candidate. Ultimately, we look for a time step in which $c^*$ is known, which means zero uncertainty in finding the winner candidate. 

%\textbf{Question:} Given a candidates set $C$ with corresponding bounds for each candidate, and a specific candidate $c \in C$, what is the probability that $c = c^*$?

\begin{comment}
In order to find this probability, we consider two different cases. 

\textbf{Case 1 - Independence assumption}: In this case, we assume that the candidates are independent. Hence, we can compute $P(c = c^*)$ as follows:

% example

\[
P(c = c^*) = \prod_{c' \in C} P(\mathcal{F}(c, q) \geq \mathcal{F}(c', q))
\]

\textbf{\textcolor{red}{An example needs to be added here}}

\end{comment}


\noindent \textbf{Case A - Independence among candidates}
Given the set $C$ of $M$ candidates, if every candidate contains entities that are only present in that candidate, the probability of a candidate $c$ being the winner, i.e., $P(c = c^*)$ is the joint probability of $c$ being larger than every other candidate, and could be calculated using Equation~\ref{eq:ind}. Algorithm {\tt ProbInd} does that.

Consider an imaginary example of the following kind $C=\{c_1,c_2,c_3\}$, such that $c_1=\{e_1,e_2,e_3\}, c_2=\{e_4,e_5,e_6\},c_3=\{e_7,e_8,e_9\}$. The probability of $c_1$ being the answer could be expressed as follows:

\begin{align*}
P(c_1 = c^*) &=  P\left(\mathcal{F}(c_1, q) \geq \mathcal{F}(c_2, q)\right) \times P\left(\mathcal{F}(c_1, q) \geq \mathcal{F}(c_3, q)\right)
\end{align*}

Specifically, the score of each candidate $c$ is within the range (LB\_c, UB\_c). Algorithm {\tt ProbInd} treats the score of each candidate $c$ as a uniform distribution within (LB\_c, UB\_c) with $m$ discrete values, therefore, the probability of $P\left(\mathcal{F}(c_1, q) \geq \mathcal{F}(c_i, q)\right)$ could be calculated using the maximum convolution of two probability distributions~\cite{rahman2015worker}.

\begin{lemma}
 {\tt ProbInd} takes $\Theta(M^2m)$ running time, where $M$ is the number of candidates, and $m$ is the number of discrete values of the pdf designating the score of candidate $c$.
\end{lemma}

\begin{proof}
(sketch.) Computing Max-convolution of any two pdfs takes $\Theta(m)$ times. Therefore, computing the probability of a candidate $c$ being the winner takes $\Theta(Mm)$ time.  {\tt ProbInd} repeats this process on each candidate and therefore takes $\Theta(M^2m)$ times.
\end{proof}

%\textcolor{red}{Please shorten every example of case B in 5-7 lines.}
\noindent \textbf {Case B - Dependence among candidates}
Algorithm {\tt ProbDep} is designed to identify the likely query answer from the candidate set, when the candidates have entities in common. It uses Equation~\ref{eq:joint} for that. Consider the running example $C=\{c_1,c_2,c_3\}$. Clearly, HNY exists in all $3$ candidates, MLN exists in both $c_1$ and $c_2$, as well as HYN exists in both $c_1$ and $c_3$. 

Two challenges immediately emerge: a. computational and storage bottleneck to compute the winning probability of a candidate using Equation~\ref{eq:joint}. b. disregarding the effect of common entities in the probability computation.

Using Equation \ref{eq:joint}, if there are $M$ candidates, the probability of a candidate being the winner is conditioned on as many as $M-1$ terms. Generally, given $M$ candidates, where the score of each candidate is a pdf with $m$ discrete values, there are $m^M$ possible combinations of scores. A naive implementation of {\tt ProbDep} needs to first identify which combinations satisfy the conditions expressed and computes the probability accordingly. This approach has a space complexity of $\mathcal{O}(m^M)$, which becomes prohibitively expensive as $M$ or $m$ increases.

%\textcolor{red}{I edited this - what you wrote was not correct. The notations were all messed up - we used $n$ for number of entities, cant abuse the notation. M is the number of candidates and m is the number of unique values.}

 
\subsubsection{Avoiding memory and computational bottleneck}
We therefore study storage and computational efficiency in designing {\tt ProbDep}. Algorithm~\ref{alg:dep} contains the pseudocode. Recall Equation~\ref{eq:jointexpand} and note that $P(c = c^*)$ could be calculated as a sequence of pairwise probabilities. Instead of storing all $\mathcal{O}(m^M)$ combinations, {\tt ProbDep} decomposes the overall computation and performs it in pairwise steps. Given the overall formula for $P(c_1 = c^*)$:

%\textcolor{orange}{comment from Sohrab: This is just the overall P(c\_1 = c*) formula. the final result obtain at each step i is of this form where it is written as follows: at step i, we compute ...}
%\textcolor{red}{I dont follow this at all...where is the conditioned expression here... }

%\[
%P(f(c_1) \geq f(c_2), f(c_1) \geq f(c_3), \dots, f(c_1) \geq f(c_{M-1}))
%\]
The algorithm runs iteratively and at step $i$, it computes: \[
P(\mathcal{F}(c_1, q) \geq \mathcal{F}(c_2, q), \mathcal{F}(c_1, q) \geq \mathcal{F}(c_3, q), \dots, \mathcal{F}(c_1, q) \geq \mathcal{F}(c_{i+1}, q))
\] by using probabilities calculated from step $(i - 1)$, which is of the following form.
\[P(\mathcal{F}(c_1, q) \geq \mathcal{F}(c_2, q), \mathcal{F}(c_1, q) \geq \mathcal{F}(c_3, q), \dots, \mathcal{F}(c_1, q) \geq \mathcal{F}(c_{i}, q))\]


%The process begins with:

%\[
%P(f(c_1) > f(c_2))
%\]

%and move one step at a time to:

%\[
%P(f(c_1) > f(c_2), \dots, f(c_1) > f(c_i))
%\]

%until reaching:

%\[
%P(f(c_1) > f(c_2), f(c_1) > f(c_3), \dots, f(c_1) > f(c_n))
%\]

\begin{comment}
    At each step \(i\), we only keep track of the scoring assignments from the previous step that satisfied:

\[
[f(c_1) > f(c_2), f(c_1) > f(c_3), \dots, f(c_1) > f(c_{i-1})]
\]

and use this information to compute:

\[
P(f(c_1) > f(c_2), f(c_1) > f(c_3), \dots, f(c_1) > f(c_i)).
\]
\end{comment}


%We maintain the feasible scoring assignments of ($c_1, c_i$) at step $i-1$ in a dictionary where the keys are the possible score values of $c_i$ and the corresponding value for each score of $c_i$ in this dictionary is the set of score values of $c$ that satisfy all the conditions at step $i - 1$. 
 Once step \(i\) is complete, the algorithm does not keep track of any past information from step \(i - 1\) anymore, and only maintains the results obtained from the latest step $i$. The process continues until all $M$ steps are complete.

\begin{comment}
    \textcolor{red}{I dont understand this - you need to rewrite}
Specifically, to understand how our algorithm works, we demonstrate our stepwise approach for {\tt ProbDep} by designing a \(M\)-partite graph. Figure \ref{fig:Example_3_6_a} demonstrates how such a graph looks like for 3 candidates $c_1, c_2, c_3$. The nodes in each partition represent the possible score values that a specific candidate can have. The first partition represents the possible score values for the candidate \(c\) whose winning probability we are interested in. The subsequent $M - 1$ partitions will each represent score values of other candidates $c_1, c_2, ..., c_{M-1}$. 

At step $i$, we move to partition \(i + 1\) and compute:

\[
P(c \geq c_{i+1} \mid c \geq c_1, c \geq c_2, \dots, c \geq c_{i})
\]

We then multiply this probability by the stored information from the previous step, which is:

\[
P(c \geq c_1, c \geq c_2, \dots, c \geq c_{i})
\]

This gives us:

\[
P(c \geq c_1, c \geq c_2, \dots, c \geq c_{i+1})
\]

As discussed earlier, we only need to store the results obtained from the most recent step. Hence, at step $i$, we only need to show partitions 1, i, and i + 1. 
\end{comment}


\begin{comment}
This will be the information that we need to store for the next step $i+1$. At this point in time, except the first partition which is the candidate of our interest and the partition $i$ in which we have stored the result from the most recent step, we can ignore the other previous partitions and save more memory. We keep track of the feasible scoring assignments by adding edges between score values that satisfy our conditions.     
\end{comment}

Using our running example, $P(c_2 = c^*)$ is calculated in two steps: the algorithms first performs max-convolution to compute $P\left(\mathcal{F}(c_2, q) \geq \mathcal{F}(c_1, q)\right)$. In the next step, this computed information is then used to calculate $P\left(\mathcal{F}(c_2, q) \geq \mathcal{F}(c_3, q) \mid ( \mathcal{F}(c_2, q) \geq \mathcal{F}(c_1, q) \right)$. These two aforementioned probabilities are multiplied to produce $P(c_2 = c^*)$. 

\begin{algorithm} \label{compute_pdf_code}
\caption{Algorithm {\tt ProbDep}}\label{alg:dep}
\begin{algorithmic}[1]
    \State \textbf{Input:} Set of candidates $C = \{c_1, c_2, \dots, c_n\}$
    \State \textbf{Output:} Dictionary $all\_candidates\_probs$, where keys are candidates and values are their winning probabilities
    \State Initialize $all\_candidates\_probs \gets \{\}$
    
    \For{each candidate $c \in C$}
        \State Initialize $conditions \gets []$
        \State $prob\_recent \gets 1$
        
        \For{each candidate $c' \in C \setminus \{c\}$}
            \State Compute $P(\mathcal{F}(c, q) \geq \mathcal{F}(c', q) \mid conditions)$
            \State $prob\_recent \gets prob\_recent \times P(\mathcal{F}(c, q) \geq \mathcal{F}(c', q) \mid conditions)$
            \State Append $(c \geq c')$ to $conditions$
        \EndFor
        
        \State $all\_candidates\_probs[c] \gets prob\_recent$
    \EndFor
    
    \State \textbf{return} $all\_candidates\_probs$
\end{algorithmic}
\end{algorithm}

\begin{comment}
    Using the running example, given the bounds obtained from the example in \autoref{bound_computation},  assuming we have the three candidates as follows:

    \[
    \text{Candidates\_Set\_Bounds} = 
    \left\{ 
    \begin{array}{ll}
    c_1 & : \quad (3.5, 5.5), \\ 
    c_2 & : \quad (2.5, 4.5), \\ 
    c_3 & : \quad (3.0, 5.0)
    \end{array}
    \right\}
\]


    We have already assumed that the processed responses are from the discrete set $S = \{0.0, 0.5, 1.0\}$. Hence, the overall score of any arbitrary candidate should be a multiple of $0.5$ within the range specified by the candidate bounds. 

    In this paper, for simplicity of notation, we may replace $P(\mathcal{F}(c_i) \geq \mathcal{F}(c_j))$ by $P(c_i \geq c_j)$. Assume we are interested in computing \( P(c_2 = c^*) \). This can be formulated as:

    \[
    P(c_2 = c^*) = P(c_2 \geq c_1, \, c_2 \geq c_3) = P(c_2 \geq c_1) \cdot P(c_2 \geq c_3 \mid c_2 \geq c_1).
    \]

    To compute each term in this probability in the stepwise manner explained in this subsection, we use the stored information from the previous step sequentially. Since there are two terms in the above sequence, we break the problem down into two parts which are:  \textbf{(a)} \(\mathbf{P(c_2 \geq c_1)}\) and \textbf{(b)} \(\mathbf{P(c_2 \geq c_3 \mid c_2 \geq c_1)}\)


To compute \(\mathbf{P(c_2 \geq c_1)}\), we need to initially specify all possible combinations of scoring assignment for $c_1$ and $c_2$ which can be obtained by the Max-Convolution of the two candidates. Then, \autoref{fig:Example_3_6_a}(a) shows how we only select a subset of all combinations of $c_1$ and $c_2$ which satisfy $c_2 \geq c_1$ by representing these combinations of satisfying values with directed edges. For ease of demonstration, we have used different colors for different values of $c_2$. We keep track of these edges by maintaining a dictionary that its keys are the possible values of $c_1$ (nodes of the right partition), and the corresponding values for each such a node/key are the other end points of the incoming edges to that node/key (nodes of the left partition). For instance, for the key $c_1 = 3.5$, we have 

\begin{center}
$Dict(c_1) = \{c_2 = 3.5, c_2 = 4\}$.    
\end{center}


Since each $c_1$ and $c_2$ have 5 possible values, there are $25$ different combinations between them, and since there are 6 edges or equivalently, there are totally 6 values for all keys/nodes in $c_1$, it means 6 of them satisfy $c_2 \geq c_1$. Hence, we could conclude that: \(\mathbf{P(c_2 \geq c_1)} = \frac{6}{25}\). However, one important challenge in calculating these probabilities precisely involves disregarding the effect of unknown common elements between candidates in the final probability value.

\textcolor{red}{ pl be careful with notations. you were using c1, $c_1$, $c_i,$, $c_j$, very inconsistently....I cant fix all of these}

\end{comment}

\textbf{Disregarding the effect of unknown common entities.} Given two candidates $c_i$ and $c_j$, {\tt ProbDep} needs to eliminate the effect of common unknown questions between $c_i$ and $c_j$ to compute the probability $P(\mathcal{F}(c_i, q) \geq \mathcal{F}(c_j, q))$. 


\begin{comment}
Given the bounds obtained from the example in \autoref{winning_probability}, let's assume there are only the first three candidates, which are as follows:

    \[
    \text{Candidates\_Set\_Bounds} = 
    \left\{ 
    \begin{array}{ll}
    c_1 & : \quad (3.5, 5.5), \\ 
    c_2 & : \quad (2.5, 4.5), \\ 
    c_3 & : \quad (3.0, 5.0)
    \end{array}
    \right\}
\]


    We have already assumed that the processed responses are from the discrete set $S = \{0.0, 0.5, 1.0\}$. Hence, the overall score of any arbitrary candidate should be a multiple of $0.5$ within the range specified by the candidate bounds. 

    In this example, For simplicity of notation, we replace $P(\mathcal{F}(c_i) \geq \mathcal{F}(c_j))$ by $P(c_i \geq c_j)$. Assume we are interested in calculating the probability that \( c_2 \) is the winning candidate (query answer), denoted as \( P(c_1 = c^*) \). This can be formulated as:

    \[
    P(c_2 = c^*) = P(c_2 \geq c_1, \, c_2 \geq c_3) = P(c_2 \geq c_1) \cdot P(c_2 \geq c_3 \mid c_2 \geq c_1).
    \]

    To compute each term in this probability in the stepwise manner explained in this subsection, we use the stored information from the previous step sequentially. Since there are two terms in the above sequence, we break the problem down into two parts as follows: 
    
\textbf{(a)} \(\mathbf{P(c_2 \geq c_1)}\)


To compute this probability, we need to initially specify all possible combinations of scoring assignment for $c_1$ and $c_2$ which is the size of cartesian product between $c_1$ and $c_2$ denoted as $c_1 \times c_2$. Then, \autoref{fig:Example_3_6_a}(a) shows how we only select a subset of all combinations of $c_1$ and $c_2$ which satisfy $c_2 \geq c_1$ by representing these combinations of satisfying values with directed edges. For ease of demonstration, we have used different colors for different values of $c_2$. We keep track of these edges by maintaining a dictionary that its keys are the possible values of $c_1$ (nodes of the right partition), and the corresponding values for each such a node/key are the other end points of the incoming edges to that node/key (nodes of the left partition). For instance, for the key $c_1 = 3.5$, we have 

\begin{center}
$Dict(c_1) = \{c_2 = 3.5, c_2 = 4\}$.    
\end{center}




Since each $c_1$ and $c_2$ have 5 possible values, there are $25$ different combinations between them, and since there are 6 edges or equivalently, there are totally 6 values for all keys/nodes in $c_1$, it means 6 of them satisfy $c_2 \geq c_1$. Hence, if $c_1$ and $c_2$ had no unknown questions in common (any unknown question that influences both candidates), then, we could conclude that: \(\mathbf{P(c_2 \geq c_1)} = \frac{6}{25}\).
\end{comment}

 As an example, $c_1$ and $c_2$ have one unknown question in common, which is $R(HNY, q)$. {\tt ProbDep} assigns $R(HNY, q) = 0$ which means it has no effect on the score bounds of $c_1$ and $c_2$ particularly for computing $P(\mathcal{F}(c_2, q) \geq \mathcal{F}(c_1, q))$. Hence, assuming $R(HNY, q) = 0$, the new bounds are:

\[    \begin{array}{ll}
    c_1 & : \quad (3.5, 4.5), \\ 
    c_2 & : \quad (2.5, 3.5)
    \end{array}
\]

%While these new bounds are  obtained only to compute the exact \(P(\mathcal{F}(c_2, q) \geq \mathcal{F}(c_1, q))\), we use the original bounds of the two candidates for further computations. 

%These tightened temporary bounds eliminate some of the arrows in \autoref{fig:Example_3_6_a}(a), and change it to \autoref{fig:Example_3_6_a}(b). Hence, after applying the removal of common elements' effect, since there are 9 possible values and there is one edge, we have:

\begin{center}
\(P(\mathcal{F}(c_2, q) \geq \mathcal{F}(c_1, q)) = \frac{1}{9}\)    
\end{center}

\begin{comment}
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Example_3_6_a.jpg}
    \caption{Computing \(P(\mathcal{F}(c_2, q) \geq \mathcal{F}(c_1, q))\)}
    \label{fig:Example_3_6_b}
\end{figure}
\end{comment}



Having $P(\mathcal{F}(c_2,q) \geq \mathcal{F}(c_1,q))$  computed, the algorithm needs to next compute \(P(\mathcal{F}(c_2,q) \geq \mathcal{F}(c_3,q) \mid \mathcal{F}(c_2,q)\geq \mathcal{F}(c_1,q))\) to get the final probability of $P(c_2 = c^*)$. 
%\autoref{fig:Example_3_6_b}(a) Shows all the possible value combinations of $(c_1, c_2, c_3)$ such that $c_2 \geq c_3$ given $c_2 \geq c_1$ as the condition. The way we add edges from middle partition to the right partition is by considering the most recent information that we have stored which is the corresponding dictionary of the middle partition which stores all possible score assignments that satisfy $c_2 \geq c_1$ as the condition for the probability that now we want to compute in our current step. Hence, edges going from $c_2$ to $c_3$ are chosen from values of this dictionary which already satisfy $c_2 \geq c_1$. 

However, {\tt ProbDep} now eliminates the effect of common unknown questions between $c_2$ and $c_3$, which is $R(HNY, q)$. Hence, assuming it is zero, the new bounds are:

\[\begin{array}{ll}
    c_2 & : \quad (2.5, 3.5), \\ 
    c_3 & : \quad (3.0, 4.0)
    \end{array}
\]
This gives \(P(\mathcal{F}(c_2,q) \geq \mathcal{F}(c_3,q) \mid \mathcal{F}(c_2,q)\geq \mathcal{F}(c_1,q))\) = $\frac{2}{45}$

\begin{comment}
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Example_3_6_b.jpg}
    \caption{\(\mathbf{P(c_2 \geq c_3 \mid c_2 \geq c_1)}\)}
    \label{fig:Example_3_6_a}
\end{figure}

\end{comment}

Finally, these are multiplied to obtain $P(c_2 = c^*)$. Similarly, the probabilities \( P(c_1 = c^*) \) and \( P(c_3 = c^*) \) can be computed.

\begin{comment}
If we observe the bounds of these three candidates without calculating exact probabilities, we can estimate that \( c_2 \) is less likely to achieve the highest score compared to the others. While unknown questions could potentially impact the outcome, common unknown questions between candidates will affect all candidates similarly and are not game-changers in this case. Therefore, when we exclude their influence, \( P(c_2 = c^*) \) becomes even lower. This is reasonable, as unknown common questions cannot help \( c_2 \) exceed its score relative to the others.    


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/PDF.jpg}
    \caption{Probability Distribution of the winner candidate}
    \label{fig:PDF}
\end{figure}

One challenge in calculating these probabilities involve quantifying the effect of common elements and disregarding their effect in the final probability value. \textcolor{red}{As an example, }
\end{comment}


It could be shown that Algorithm~{\tt ProbDep} will produce
\[
P(c = c^*) =
\begin{cases} 
0.75 & \text{ } c = c_1, \\
0.24 & \text{ } c = c_2, \\
0.01 & \text{ } c = c_3.
\end{cases}
\]


 \begin{lemma}
 {\tt ProbDep} takes $\Theta(M^2m^2)$ running time and $O(m^2)$ space.
 \end{lemma}

\begin{proof}
(sketch.) Computing Max-convolution of two pdfs takes $\Theta(m^2)$ times. Therefore, computing the probability of a candidate $c$ being the winner takes $\Theta(Mm^2)$ time.  {\tt ProbDep} repeats this process on each candidate and therefore takes $\Theta(M^2m^2)$ times.
\end{proof}


\begin{comment}
  \subsubsection{Avoiding Memory Bottleneck} 
Through our stepwise process for computing winning probability for each candidate $c$, we iteratively compute probabilities of the form $P(c > c_i | conditions)$. For computing each term of the mentioned form, instead of maintaining a n-dimensional table to consider all possible value combinations for all candidates, we utilize memory usage by only storing the useful information which is a dictionary that its keys are the possible score values of $c_{i-1}$, and its values are the set of score values for $c$ that satisfy the conditions with respect to that $c_{i-1}$ value key. Hence, we only need to store $\mathcal{O}(m^2)$ instead of a huge table that requires $\mathcal{O}(m^n)$ space. 


\label{uncertainty}  
\end{comment}


%Define entropy

% design an example of already asked questions, compute entropy, select two question, compare their effect on entropy, why one is better than another

% hence, we propose algorithm to find a question which most likey reduces entropy maximally compared to others
\subsection{Determining the Next Question}\label{subsec:nextquestion}
In \autoref{winning_probability}, we discussed algorithms for the probabilistic model for finding the answer of the query. Formally speaking, the query answer is a random variable with $M$ possible outcomes (each per candidate), and their probability could be computed using {\tt ProbDep} or {\tt ProbInd}. We use {\em entropy} as a measure of the uncertainty associated with this random variable as follows:

\begin{align}\label{eq:entropy}
H(c^*) = - \sum_{i=1}^{n} p(c_i = c^*) \log p(c_i = c^*)
\end{align}

The entropy of $c_1, c_2, c_3$ in the running example is  $ H(c^*) = - \left( 0.75 \log(0.75) + 0.01 \log(0.01) + 0.24 \log(0.24) \right) = 0.604$
  
The next best question should therefore be the one that minimizes entropy (ideally makes it $0$). However, the challenge is to select this question from $Q_U$ without any further assumption about the response received from LLM. Algorithm {\tt EntrRed} is designed for this task (Algorithm~\ref{alg:entred} has the pseudocode).

 This algorithm leverages {\tt ProbDep} or {\tt ProbInd} and first identifies the candidate (let $c^+$ be that candidate) that has the  highest probability to be the winner. Given $Q_U$, it first narrows down to a smaller subset $Q_{U'}$ that involve  $c^+$. It leverages a subroutine {\tt QEF} (Subroutine~\ref{alg:qef}) to quantify the effect of every question $Q \in Q_{U'}$ and then selects $Q \in Q_{U'}$ that has the maximum score associated.

 %\textcolor{orange}{No intuition - does not explain why we did what we did. Needs to be rewritten .}
Subroutine {\tt QEF} works as follows: For every $Q \in Q_{U'}$, let $C_Q$ represent the subset of all candidates whose scores are influenced by $Q$, and $C'_Q$ be the other candidates. The score assigned to the question $Q$ is the sum of absolute difference between winning probability of candidates in $C_Q$ and winning probability of candidates in $C'_Q$. 

Intuitively, consider a pair of candidates $c_Q \in C_Q$ and $c'_Q \in C'_Q$. if $|P(c_Q = c^*) - P(c'_Q = c^*)|$ is high, it means that among $c_Q$ and $c'_Q$, one has a much higher winning probability compared to the other which means the score bounds of $c_Q$ and $c'_Q$ have a small overlap, and asking $Q$ as a question that influences the score of only one of the two candidates, can potentially diverge their bounds, and make one of the two candidates certainly better than the other, which will prune out one of the candidates. On the other hand, if $Q$ influences both candidates, asking it will affect score bounds of both candidates in the same way, and will not result in pruning one of them. This is why for a given $Q$, we divide candidates by two groups based on if they are influenced by $Q$ or not, and then consider the winning probability difference between candidates from the two groups. The higher each pairwise difference in winning probability is, the more valuable the $Q$ is since it becomes more likely to prune out candidates from the two groups by asking $Q$. 

\begin{algorithm}
\caption{Algorithm {\tt EntrRed}}\label{alg:entred}
\begin{algorithmic}[1]
    \State \textbf{Input:} Set of unknown questions $\mathcal{Q_U}$, Probability Density Function (PDF) for candidates
    \State \textbf{Output:} A single question $Q^*$ to be asked next
    \State Initialize $Q^* \gets \text{null}$
    \State Initialize $max\_prob \gets -\infty$
    \State Initialize $c^+ \gets \text{null}$  \Comment{Candidate with highest probability}
    
    \For{each candidate $c$}
        \State $prob(c) \gets \text{PDF}(c)$ \Comment{Obtain probability for candidate $c$}
        \If{$prob(c) > max\_prob$}
            \State $max\_prob \gets prob(c)$
            \State $c^+ \gets c$  \Comment{Update best candidate}
        \EndIf
    \EndFor

    \State Let $Q_{U'}$ be the set of questions contributing to the score of $c^+$
    \State Initialize $max\_score \gets -\infty$
    \For{each question $Q \in Q_{U'}$}
        \State $score(Q) \gets \text{Evaluate\_Question}(Q, C_Q)$ \Comment{$C_Q$: candidates influenced by $Q$}
        \If{$score(Q) > max\_score$}
            \State $max\_score \gets score(Q)$
            \State $Q^* \gets Q$  \Comment{Update best question}
        \EndIf
    \EndFor
    
    \State \textbf{return} $Q^*$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Subroutine QEF}\label{alg:qef}
\begin{algorithmic}[1]
    \Function{evaluate\_question}{Q, $C_Q$}
        \State Initialize $score(Q) \gets 0$
        \For{each candidate $c \in C_Q$}
            \State Initialize $prob\_diff\_sum(c) \gets 0$
            \For{each candidate $c' \notin C_Q$}
                \State $prob\_diff(c,c') \gets \left| P(c = c^*) - P(c' = c^*) \right|$
                \State $prob\_diff\_sum(c) \gets prob\_diff\_sum(c) + prob\_diff(c,c')$
            \EndFor
            \State $score(Q) \gets score(Q) + prob\_diff\_sum(c)$
        \EndFor
        \State \Return $score(Q)$
    \EndFunction
\end{algorithmic}
\end{algorithm}



 

   

As we ask more questions, we expect the entropy to decrease. Finally, when the winner candidate is obtained, the entropy goes down to zero. 

Using our running example, {\tt EntrRed} narrows down to $Q_{U'}$ that are pertinent to $c_1$ only. Among unknown questions of $c_1$, $Q_1= R(HNY)$ and $Q_2=D(MLN, HYN)$ are the two possibilities. {\tt QEF} is invoked for both of them.
  $Q_1$ = R(HNY, q): We need to compute the sum of winning probability difference between candidates from $C_{Q1}$ with any other candidates. Since $C_{Q1} = \{c_1, c_2, c_3\}$, there will be no remaining other candidates for computing overlap with these 3. Hence, QEF(Q1) = 0
$Q_2$ = D(MLN, HYN): Since $C_{Q2} = {c_1}$, $QEF(Q2) = |p(c_1) - p(c_2)| + |p(c_1) - p(c_3)|=1.25$. $Q_2$ is therefore asked.
Let's assume that the oracle (LLM) returns $D(MLN, HYN) = 1.0$. Then, it could be shown, that
\[
P(c = c^*) =
\begin{cases} 
1.0 & \text{ } c = c_1, \\
0.0 & \text{ } c = c_2, \\
0.0 & \text{ } c = c_3.
\end{cases}
\]
Clearly, at this point the entropy of the random variable representing different outcomes in Equation~\ref{eq:entropy} is $0$. $c_1$ is thus returned as the answer of the query.

\begin{lemma}
  {\tt EntrRed} takes $O(|Q_c|M^2)$ time to run.
\end{lemma}

\begin{proof}
(Sketch.)
Subroutine {\tt QEF} divides all $M$ candidates in two groups and computes pairwise differences in winning probability  among them. Hence, it takes $(M^2)$ time. {\tt EntrRed} repeats this process on each question influencing the score of the most possible winner candidate and therefore takes $O(|Q_c|M^2)$ time.
\end{proof}



\begin{comment}
    
\begin{itemize}
    \item \textbf{Max Probability Algorithm}: we first need to select a candidate which we are going to choose one of the unknown questions that contribute to the score of that. As discussed earlier, such a candidate is the one with the highest probability of being the winner at the point in time, which is $c_1$. Among questions influencing the score of $c_1$, two of them are unknown which are $R(HNY)$ and $D(MLN, HYN)$. Hence, we need to compute the associated score of these questions using the Question Evaluation Function (QEF) and choose the one with higher score. Let $C_Q$ represent the set of candidates that Q affects their score. Then, we have
    \begin{itemize}
        \item $Q1$ = R(HNY, q): We need to compute the sum of overlap between the pairwise bounds of candidates from $C_Q1$ with any other candidates. Since $C_Q1 = \{c_1, c_2, c_3\}$, there will be no remaining other candidates for computing overlap with these 3. Hence:
        \begin{align*}
            QEF(Q1) = 0
        \end{align*}
        \item $Q2$ = D(MLN, HYN): Since $C_Q2 = {c_1}$, we need to sum the bounds overlap of $c_1$ and $c_2$ with $c_1$ and $c_3$. Hence, we have:
        \begin{align*}
            QEF(Q2) = 1.5 + 1 = 2.5
        \end{align*}
    \end{itemize}
    Therefore, since $QEF(Q2) > QEF(Q1)$, we choose $Q2 = D(MLN, HYN)$ as the next question. 
    
    \item \textbf{Max Overlap algorithm}: In this approach, we need to consider all the unknown questions, compute their QEF, and choose the one with maximum value. Let $C_Q$ represent the set of candidates that Q affects their score. Given our running example, since we have assumed the only candidates are $\{c_1, c_2, c_3\}$, the questions that affect the score of either of these candidates are as follows: 
    \begin{itemize}
        \item Q1 = R(HNY, q) = 0 (as computed earlier)
        
        \item Q2 = D(MLN, HYN) = 0 (as computed earlier)
        
        \item Q3 = D(MLN, SHN): Since $C_Q3 = {c_2}$, we need to sum the bounds overlap of $c_2$ and $c_1$ with $c_2$ and $c_3$. We will have:
        \begin{align*}
            QEF(Q3) = 1.5 + 1 = 2.5
        \end{align*}
        
        \item Q4 = Since $C_Q4 = {c_3}$, we need to sum the bounds overlap of $c_3$ and $c_1$ with $c_3$ and $c_2$. Therefore:
        \begin{align*}
            QEF(Q4) = 1.5 + 1.5 = 3.0
        \end{align*}
    \end{itemize}
    Hence, since $QEF(Q4)$ has the highest value compared to others, we choose $Q4 = D(MLN, WLD)$ as the next question. 
\end{itemize}

\end{comment}

\begin{comment}
    \subsection{Response Processing}
\label{Response_Processing}
After we select the next question to ask the expert(s), we need to process the received response into our desired form. Depending on if we are using a single or multiple experts, and if the expert response is a discrete value or a range, we have 4 different cases as mentioned in :

\begin{enumerate}
    \item \textbf{Single Expert, Single Response}
    \item \textbf{Single Expert, Range Response}
    \item \textbf{Multiple Experts, Range Response}
    \item \textbf{Multiple Experts, Range Response}
\end{enumerate}

In \autoref{sec:section2.3} we formally defined the overall approach showing the process of mapping the responses from either of the above cases to our desired format. Algorithm 4 shows our generalized algorithm to process the experts' responses which covers all the above cases. 

\begin{algorithm}
\caption{Generalized Response Processing Algorithm}
\begin{algorithmic}[1]
\State \textbf{Input:} Set of experts' responses $R$, where each response $R_i$ is a range $(R_{i,1}, R_{i,2})$. If $R_i$ is a single response, set $R_i = (R_i, R_i)$.
\State \textbf{Output:} Probability density function $f_{\mathcal{S}}(r)$ over the set of desired responses $\mathcal{S}$.

\State Initialize $\mathcal{S}^* \gets []$ \Comment{List to store mapped desired responses}
\For{each response $R_i = (R_{i,1}, R_{i,2})$ in $R$}
    \State Initialize $\mathcal{S}_i^* \gets []$ \Comment{Subset of $\mathcal{S}$ for current response range}
    \For{each $r \in \mathcal{S}$}
        \If{$R_{i,1} \leq r \leq R_{i,2}$}
            \State Append $r$ to $\mathcal{S}_i^*$
        \EndIf
    \EndFor
    \For{each $r_j \in \mathcal{S}_i^*$}
        \State Append $r_j$ to $\mathcal{S}^*$
    \EndFor
\EndFor

\State Calculate total number of elements in $\mathcal{S}^*$, denoted as $n^*$
\State Initialize $f_{\mathcal{S}}(r) \gets 0$ for each $r \in \mathcal{S}$

\For{each $r \in \mathcal{S}^*$}
    \State $f_{\mathcal{S}}(r) \gets f_{\mathcal{S}}(r) + \frac{1}{n^*}$ \Comment{Update probability density based on frequency}
\EndFor

\State \textbf{Return} $f_{\mathcal{S}}(r)$
\end{algorithmic}
\end{algorithm}

\begin{example}
    Following the running example, let's assume that we would like to use the max probability algorithm to select the next question. In this case, as reasoned earlier in Example 3.7, the next selected question will be $Q = D(MLN, HYN)$. 
    We need to leverage Algorithm 5 to output a probability density function over the set of desired responses so that it would be further used to update the bounds accordingly. 
    
    Let's assume that we have 3 experts denoted as $E = \{e_1, e_2, e_3\}$ which all return ranges rather than single discrete values (Case 4). Following our initial assumption in the running example, the set of desired responses is $S = {0.0, 0.5, 1.0}$. Imagine that the experts' responses are as follows:

\[
\begin{aligned}
    \text{Response}(e_1) &= (0.0, 0.2), \\
    \text{Response}(e_2) &= (0.0, 0.3), \\
    \text{Response}(e_3) &= (0.5, 1.0)
\end{aligned}
\]

Each response is then mapped to a subset of desired responses as followed:
\[
\begin{aligned}
    \text{Mapped(Response}(e_1)) &= \{0.0\}, \\
    \text{Mapped(Response}(e_2)) &= \{0.0\}, \\
    \text{Mapped(Response}(e_3)) &= \{0.5, 1.0\}
\end{aligned}
\]

Therefore, by aggregating the obtained mapped responses, we can get the PDF of the experts responses, denoted as PDF(E), as shown in \autoref{fig:Response_PDF}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Density.jpg}
    \caption{Overall Probability Density of the Experts' Responses}
    \label{fig:Response_PDF}
\end{figure}

This PDF is then used to update the bounds. In this case, since the PDF is nonuniform, the updated bounds will be nonuniform as well. In more detail, in this case, if $(LB, UB)_c = (\alpha, \beta)$, the potential score values within this range may no longer be equally probable, and can have different probabilities associated with them. 

\end{example}
\end{comment}

\subsection{Response Processing}\label{subsec:resp}
For a given question $Q$, a discrete response \(r \in [MIN,MAX] \), within the specified range \([MIN, MAX]\), from a single oracle could be a normalized floating point number representing a score value (e.g. 0.7). Upon receiving $r$, the score bounds of each candidate $c$ that contains $Q$ are updated as follows: for the lower bound, substitute MIN  by $r$, and for the upper bound substitute MAX by $r$. 