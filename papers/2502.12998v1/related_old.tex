% \vspace*{-2ex} 
% \vspace*{-1ex} 
\section{Related Work}
\label{sec:related}
Our work is related to approximate query answering, optimizing ML inference, and to combining queries with ML inference. \\

\noindent {\bf Query approximation.}
Approximate query processing is topic that is widely addressed in the database community. The aim is to find approximate answers that are as close as possible to exact answers and that can be found much faster. 
Query approximation techniques~\cite{DBLP:journals/dase/LiL18} can be categorized into (1) online aggregation: select samples online and use them to answer OLAP queries, and (2) offline synopses generation 
 to facilitate OLAP queries. Our work adopts a probabilistic top-k approach~\cite{DBLP:conf/vldb/TheobaldWS04} and is significantly different from these.

\noindent {\bf Optimizing ML inference.}
Several recent approaches were proposed to speed up the application of an ML model. 
Existing approaches follow either an in-database~\cite{DBLP:conf/nsdi/CrankshawWZFGS17} or in-application approach~\cite{DBLP:conf/kdd/AhmedABCCDDEFFG19}. Amazon Aurora is an example of an in-database containerized solution that enables external calls from SQL queries to ML models in SageMaker~\cite{sagemaker_2022}. 
Containerized execution introduces overhead in prediction latency. To mitigate that, Google's BigQuery ML~\cite{bigquery_2022}
and Microsoft's Raven were developed~\cite{DBLP:conf/cidr/KaranasosIPSPPX20}. Compared to Raven, BigQuery ML relies mostly on hard-coded models and targets batch predictions, since it inherits a relatively high startup cost. Raven and its runtime environment ONNX~\cite{DBLP:conf/osdi/ChenMJZYSCWHCGK18} offer the additional ability to make tuple-level inference. 

\noindent{\bf Combining queries and ML inference.}
Our work is closest to combining queries and ML inference since we aim to answer queries over predictions made by those models. This is a research topic that attracted attention recently. We present them here in chronological order. 
Bolukbasi et al.~\cite{DBLP:conf/icml/BolukbasiWDS17} enable  incremental predictions for neural networks.  
 Computation time is reduced by pruning examples that are classified in earlier layers, selected adaptively. 
Kang et al.~\cite{DBLP:journals/pvldb/KangEABZ17} present NOSCOPE, a system for querying videos that can reduce the cost of neural network video analysis by up to three orders of magnitude via inference-optimized model search.
Lu et al.~\cite{DBLP:conf/sigmod/LuCKC18} and Yang et al.~\cite{10.14778/3547305.3547310} use probabilistic predicates to filter data blobs that do not satisfy the query and empirically increase data reduction rates. 
Anderson et al.~\cite{DBLP:conf/icde/AndersonCRW19} use a hierarchical model to reduce the runtime cost of queries over visual content. 
Gao et al.~\cite{DBLP:conf/sigmod/GaoXAY21} introduce a Multi-Level Splitting Sampling to let one "promising" sample path prefix generate multiple "offspring" paths, and direct Monte-Carlo based simulations toward more promising paths. 
Lai et al.~\cite{10.1145/3448016.3452786} studies approximate top-$k$ queries with light-weight proxy models that generate oracle label distribution.
Recent work adopted the use of cheap proxy models, such as image classifiers, to identify  an  approximate  set  of $k$ entities satisfying a query~\cite{DBLP:journals/pvldb/DingAL22,DBLP:journals/pvldb/KangGBHZ20}. 