\section{Introduction}
In several emerging applications that lie at the intersection of databases and machine learning, there is a need for efficient top-$k$ queries, where the scoring function is user-defined. For example, consider a web search query asking for "Hotels with Unique Decor in Manhattan." The task is to select the top-$k$ hotels from a database of user-reviewed entities, based on a scoring criterion related to "unique decor." Since the database may not directly include attributes such as "unique decor," determining such a score becomes challenging. By analyzing user data, this information can be inferred. External oracles/experts, like Large Language Models (LLMs), can provide these predictions, but querying LLMs is costly. \textit{In this paper, we propose a framework that intelligently selects the next best question to ask the oracle, such that the answer maximizes the likelihood of accurately identifying the true top-$k$ set. }



%Similarly, in image retrieval~\cite{DBLP:journals/pvldb/KangEABZ17,DBLP:journals/pvldb/KangGBHZ20}, or in the medical domain, a  query may look for $k$ patients  whose predicted clinical condition is similar to an input patient using a DNN~\cite{DBLP:journals/pvldb/DingAL22,DBLP:conf/cikm/RodriguesPGA20,DBLP:journals/isci/RodriguesGSBA21}. These queries are challenging because finding high quality answers by invoking an expensive oracle such as a human expert, a Deep Neural Network (DNN) model, or a Large Language Model (LLM), on every single entity in the DB and then applying the query, can be prohibitive. 

The main focus of query processing over ML models has been to ensure efficiency without compromising accuracy~\cite{9094012,DBLP:journals/pvldb/DingAL22,DBLP:journals/pvldb/KangEABZ17}. 
Most existing work relies on sampling to reduce the cost of oracle calls in query processing~\cite{DBLP:journals/pvldb/DingAL22,DBLP:journals/pvldb/KangGBHZ20,DBLP:conf/sigmod/GaoXAY21,10.1145/3448016.3452786}.
For instance, the work in~\cite{DBLP:journals/pvldb/DingAL22} proposes to solve the problem of minimizing  oracle usage for finding answers that meet a precision or recall target with provable statistical guarantees while achieving a precision or recall target. {\em To the best of our knowledge, no existing work has tackled the problem of answering personalized top-$k$ set based queries with arbitrary user-defined scoring functions over multi-modal data. Specifically, there is no work that explores how to minimize the number of LLM calls required to score potential top-$k$ sets, where the scoring function is decomposed into a set of constructs, and the score for each construct can be predicted by LLMs.} 
\begin{comment}
Another line develops adaptive predictions for NNs by pruning examples based on their classification  in early layers~\cite{DBLP:conf/icml/BolukbasiWDS17}. Another line of work advocates the use of cheap \textit{proxy} models that approximate ground truth oracle labels. 
Proxies are small neural models that either provide a confidence score~\cite{DBLP:journals/pvldb/KangGBHZ20,10.14778/3547305.3547310,DBLP:conf/sigmod/LuCKC18} or distribution~\cite{10.1145/3448016.3452786} for their predicted labels.
Probabilistic predicates (PP)~\cite{DBLP:conf/sigmod/LuCKC18} and CORE~\cite{10.14778/3547305.3547310} employ light-weight proxies to filter out unpromising entities in the DB and empirically improve data reduction rates in query execution plans. Probabilistic top-$k$~\cite{10.1145/3448016.3452786} trains proxy models to generate oracle label distribution and delivers approximate top-$k$ solutions. 
More recently, in~\cite{DBLP:journals/pvldb/KangGBHZ20}, the authors study queries with a minimum precision target (PT) or recall target (RT), and a fixed user-specified budget on the number of oracle calls. However, (i) setting an oracle budget is hard to get right. 
\end{comment}
%An underestimated budget may lead to trivial answers while overestimation causes unnecessary oracle usage, and (ii) setting \textit{only} a minimum precision \textit{or} \textit{only} a minimum recall target, runs the risk of returning valid but uninformative answers: for RT, returning all  entities in the DB is valid but has very poor precision; for PT, returning the empty set is valid but it has zero recall and is hence not useful in practice. 

\smallskip \noindent {\bf Running Example.}
We assume a hotel database consisting of $5$ unique entities, i.e., hotels as shown in \autoref{tab:ny_hotels_relevance}. Each entity is represented using multiple items of different modalities (image/audio/text based reviews). Given a query (e.g., unique decor hotels in Manhattan), the goal is to identify a small set of $3$ ($k=3$) hotels that are most suitable.

The process leverages a user-defined set-based scoring function to identify the top-$3$ hotels. We consider scoring functions that are decomposable into constructs, such as, relevance, diversity, serendipity~\cite{div1,div2,div3,div4}, etc. Wlog, an example scoring function with $\ell=2$ constructs, would be used to compute the top-$k$ set that maximizes the sum of relevance and diversity $\mathcal{F}(s, q)= \Sigma_{e \in s} Rel(q,e)+ \Sigma_{e_i,e_j \in s} Div(e_i,e_j)$. The constructs of the scoring function in this example are unary (e.g., relevance) and binary (e.g., diversity). However, we must support constructs of arbitrary arity. 

Since the data is multimodal and the query is personalized, our framework aims to study the applicability of LLMs to retrieve scores of some entities for some scoring construct. %Imagine one such scoring function is the summation of relevance and pairwise diversity of entities within a possible result set. Given the query \( q \) = "Unique decor hotels in Manhattan", suppose there are five hotels , and we wish to find the best set of $3$ hotels. 
% LLMs or generally oracles  could be called to predict the relevance score between a query and an entity, or the diversity score between two entities. Each of these is construed as a question. 
%For example, the oracle could be asked to predict the relevance score of the query ``affordable hotels in Manhattan'' for an entity e.g. ``Hilton NY Hotel'' (a hotel in Manhattan).

%Imagine one such scoring function is the summation of relevance and pairwise diversity of entities within a possible result set. Given a query, such a scoring function identifies top-$k$ set by returning entities that are highly relevant to the query, yet highly diverse from each other. 

%Consider a snapshot of a framework of this sort - at a given point in time, a partial set of scores that are part of the scoring function are known, and the remaining are unknown. Since calling an oracle is costly, the overarching objective of this work is to design a computational framework that identifies the top-$k$ set of entities correctly, while minimizing the number of questions asked to oracles. The goal is to generalize the solution framework that can handle multiple scoring functions, as well as handle uncertainty involved in the response of oracles.

%In \autoref{fig:motivating_scenario}, we illustrate this flow between the data, the computational framework, and the oracle to find the query answer. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Motivating_Scenario.jpg}
    \caption{\small Proposed Framework}
    \label{fig:motivating_scenario}
\end{figure}

\smallskip \noindent {\bf Challenges.} 
The overarching challenge is to support any set-based queries with user-defined scoring functions while minimizing the number of calls made to LLMs. The goal is to return the exact answer. Imagine a snapshot of the process - where at a given point in time, partial scores of the set of possible top-k sets (each such set is called a candidate) are computed. Based on those partial scores, the key challenge is to develop a principled ``model'' that quantifies the likelihood of each candidate being the actual top-$k$ answer. The actual query answer therefore is a probability distribution function (pdf), where each candidate is a possible outcome and has a probability associated. Designing this model is non-trivial as computing the probability of a candidate  being the actual top-$k$ is not {\em independent} from other sets that share common entities with it. Given the probabilistic model, the next question to the LLM should be the one that minimizes {\em uncertainty} in the current pdf. Therefore, a challenge is to be able to model ``uncertainty'' in the current pdf and identify the next question that minimizes it. %The final challenge is to study the computational efficiency of the proposed models and designed algorithms.

\smallskip \noindent {\bf Contributions.} We propose a computational framework that works for arbitrary user-defined scoring functions, as long as the scoring function can be decomposed into a set of constructs for which LLM calls could be invoked. At a given point in time, the framework has access to a set of $M$ candidates (each containing $k$ entities), one of which will eventually become the final query answer. The framework can be summarized into $4$ well-defined tasks:\\
\indent {\bf Task 1: Computing score bounds of candidate sets.} At a given time, the framework knows either the partial or the full score of a candidate. When only partial score of a candidate is known, it computes the lower and upper bounds of a candidate's score. \\ %These bounds represent the smallest and largest possible scores a candidate could achieve. \\
\indent {\bf Task 2: Probabilistic model for finding the answer.} This task makes a significant contribution by introducing a principled probabilistic model that estimates the likelihood of each candidate being the true top-$k$. When the score of a candidate is only partially known, it is treated as a discrete pdf with values uniformly distributed between the lower and upper bounds. The likelihood that one candidate having a higher score than the other is computed using {\em max-convolution} of pdfs~\cite{rahman2015worker}. This task considers two cases: computing max-convolution assuming {\bf independence} between candidates, and accounting for {\it dependence} between candidates when they share common entities. Ultimately, this task formalizes the probability of each candidate being the winner or the actual answer. The algorithms developed for this task address both memory and computational efficiency, carefully considering potential bottlenecks in their design.\\
\indent  {\bf Task 3: Determining the next question.} The  query answer is modeled as a random variable with $M$ possible outcomes (one per candidate), and Task 2 formalizes how to compute the pdf of each outcome. The most widely used mathematical framework for quantifying uncertainty is entropy~\cite{renyi1961measures}, with Shannon entropy in information theory being the most common, and the one we adopt in this work. Therefore, the next question to ask the oracle is the one that maximally reduces the uncertainty of this random variable (when entropy is $0$, no further question needs to be asked). We make no further assumption about the possible responses to a question asked to the LLM and design the algorithm for this task with an emphasis on computational efficiency.\\
\indent {\bf Task 4: Response processing.} This task involves aggregating responses from the oracle. This depends on whether the oracle provides a discrete answer or a range. 

For each of these tasks, we study computational challenges and design efficient solutions. 

\smallskip \noindent {\bf Empirical Evaluations.}  We present a comprehensive experimental evaluation of our framework using three large  datasets involving images, reviews, and structured data and three user-defined scoring functions. We implemented multiple baseline solutions as appropriate. Our experimental results indicate the efficacy of the proposed probabilistic models, as it achieves an order of magnitude improvement over baselines in requiring LLM calls while ensuring the achieved results are fully accurate. We measure the number of LLM calls and find that our method for determining the next question significantly reduces the cost compared to baselines. We also find that while the cost increases with an increasing $k$ or number of candidates, our scalability experiments indicate that the proposed models and algorithms could be used inside large-scale applications.

The rest of the paper is organized as follows: Section~\ref{sec:dm} presents our data model and problem definitions. Sections~\ref{sec:framework} and~\ref{sec:alg} contains our framework and its algorithms. We provide experimental evaluations in Section~\ref{sec:exp}. Related work is summarized in Section~\ref{sec:rel}. Several extensions of the proposed work are discussed in Section~\ref{sec:ext} and we conclude in Section~\ref{sec:conc}.