\section{Algorithms for the framework}
\label{section3}

\subsection{Identifying the Candidates' Set}
\label{candidatesset}

\begin{definition}
    \textbf{Candidates Set}: The set of candidates which have a chance to be the query answer.
\end{definition}
 To identify the candidates set, we must ensure it includes any potential query answer while excluding any candidates that can be definitively ruled out as the query answer.
 Ultimately, our goal is to choose a candidate among all which has the highest score in the candidates set with respect to the scoring function. In other words:

\[
\text{Query answer = } \arg\max_{c \in \text{C}} \mathcal{F}(c, q)
\]
Where \(\text{C}\) is the candidates set.

\begin{example}
    Given $n$ entities and a fixed $k$, there are \( \binom{n}{k} \) unique combinations of k entities. Depending on the scoring function and the context of the problem, a subset or the whole set of such candidates form the candidates set. Considering the running example to find out the top-3 hotels among the 5 ones mentioned in \autoref{tab:ny_hotels_relevance}, there can be \( \binom{5}{3} = 10 \) sets of size 3 among the whole 5 hotels. Hence, assuming all possible combinations have a chance to be the top-3 set, we will have:
    \[
    \text{C} = 
    \begin{cases}
    c_1 = \{ \text{HNY}, \text{MLN}, \text{HYN} \}, & c_2 = \{ \text{HNY}, \text{MLN}, \text{SHN} \} \\
    c_3 = \{ \text{HNY}, \text{MLN}, \text{WLD} \}, & c_4 = \{ \text{HNY}, \text{HYN}, \text{SHN} \} \\
    c_5 = \{ \text{HNY}, \text{HYN}, \text{WLD} \}, & c_6 = \{ \text{HNY}, \text{SHN}, \text{WLD} \} \\
    c_7 = \{ \text{MLN}, \text{HYN}, \text{SHN} \}, & c_8 = \{ \text{MLN}, \text{HYN}, \text{WLD} \} \\
    c_9 = \{ \text{MLN}, \text{SHN}, \text{WLD} \}, & c_{10} = \{ \text{HYN}, \text{SHN}, \text{WLD} \} 
    \end{cases}
\]

Initially, we may exclude a subset of these candidates that are assuredly not the query answer, based on prior knowledge on the context of the problem. 

\end{example}


%Example: hotels with initial score, first update on bounds

%Now, assume another table of already called and obtained bounds, consider two questions which one results in finding the winner? hence, we pick question that reduces the ucertainty the most. but how to measure uncertainty of a candidate set?

\subsection{Bounds Computation of Candidates' Set}
\label{bound_computation}

After having the candidates specified, Our goal is to find out the one which has the highest score among all. Since we do not know the exact scores for the candidates, we model the score of a candidate as a tuple of Lower Bound (LB) and Upper Bound (UB) score such that we ensure the candidate's score is within that bound. 

\begin{definition}
    \textbf{Candidate Bounds}: Given a scoring function $\mathcal{F}(c, q)$ and known information $Q_k$, the bounds $(LB_c, UB_c)$ for candidate c are the minimum and maximum overall score of $c$ with respect to $Q_k$. Hence, we have:
    \[
    LB_c \leq \mathcal{F}(c, q) \leq UB_c
    \]
\end{definition}


Where (LB\_c, UB\_c) is calculated based on $\mathcal{F}(c, q)$ and $Q_k$. These bounds need to be kept updated when new information (response to a question) is appended to $Q_k$. 

\begin{comment}

\begin{definition}[Candidates Set Bounds]
\label{candidate_set} Given a scoring function $\mathcal{F}$ and n candidates, candidates set Bounds represent the score bounds for each candidate at a point in time. candidates set Bounds (C) can be represented as follows:

\[
\text{C} = \{ c_1:(\text{LB}_{c_1}, \text{UB}_{c_1}), c_2: (\text{LB}_{c_2}, \text{UB}_{c_2}), \ldots, c_n:(\text{LB}_{c_n}, \text{UB}_{c_n}) \}
\] 
\end{definition}

After each question from the LLM, the bounds for the candidates need to be updated. 
\end{comment}
Algorithm 1 represents the procedure for updating the bounds of the candidates set after a question $Q$ is asked from expert. Depending on what components the scoring function \( \mathcal{F}(c, q) \) is composed of, different types of questions may contribute to the overall score of a candidate, and how these questions contribute to the overall score depends on the exact formula of \( \mathcal{F}(c, q) \). Let \( Q_c \) represent the set of all questions that contribute to the total score of candidate \( c \). If $Q \in Q_c$, then the bounds of $c$ needs to be updated since $Q$ is not unknown anymore. 

\begin{algorithm}
\caption{Update Bounds for Candidates}
\begin{algorithmic}[1]
\State \( res \leftarrow LLM(Q) \)
\For{each candidate \( c \in C \)}
    \State \( Q_c \leftarrow \) set of all questions contributing to the score of \( c \)
    \If{\( Q \in Q_c \)}
        \State Substitute \( Q \) with \( res \) in the formula of \( \mathcal{F}(c, q) \)
        \State \( LB_c \gets \min_{Q_u} \mathcal{F}(c, q) \)
        \State \( UB_c \gets \max_{Q_u} \mathcal{F}(c, q) \)
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{comment}
   \begin{algorithm}
\caption{Update Bounds for Candidates}
\begin{algorithmic}[1]
\State \( res \leftarrow LLM(Q) \)
\For{each candidate \( c \in C \)}
    \State \( Q_c \leftarrow \) set of all questions contributing to the score of c
    \If{\( Q \in Q_c \)} 
        \If{scoring function is based on sum}
            \State \( LB_c \gets LB_c + res \) 
            \State \( UB_c \gets UB_c - (MAX\_SCORE - res) \)
        \ElsIf{scoring function is based on average}
            \State \( n \gets \text{len}(Q_c) \)
            \State \( LB_c \gets LB_c + \frac{res}{n} \) 
            \State \( UB_c \gets UB_c - \frac{MAX\_SCORE - res}{n} \)
        \EndIf
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm} 
\end{comment}


\begin{example}
Given the running example with $\mathcal{F}_{\text{rel+div}}(c, q)$ as the scoring function, let's consider the starting point in time, when no had been inserted in \autoref{tab:ny_hotels_relevance} and \autoref{tab:ny_hotels_diversity}. In this case:
\[
Q_k = \emptyset
\]

To establish the initial bounds for all candidates, we define the minimum and maximum possible scores based on the scoring formula \(\mathcal{F}_{\text{rel+div}}(c, q)\). Given that \(k = 3\), for an arbitrary candidate \(c = \{e_1, e_2, e_3\}\), we express the score as follows:

\begin{align*}
\mathcal{F}(c, q) &= \sum_{e \in c} \text{Rel}(q, e) + \sum_{e_i, e_j \in c} \text{Div}(e_i, e_j) \\
&= \text{Rel}(q, e_1) + \text{Rel}(q, e_2) + \text{Rel}(q, e_3) \\
&\quad + \text{Div}(e_1, e_2) + \text{Div}(e_1, e_3) + \text{Div}(e_2, e_3)
\end{align*}

It is specified that each score obtained as the expert response for any \(\text{Rel}\) or \(\text{Div}\) question lies within the range \((0, 1)\). On the other hand, as shown in the above formula, there are 6 questions contributing to the overall score of an arbitrary candidate. Therefore, initially, the lower and upper score bounds for all candidates are defined as follows:

\begin{align*}
\text{LB}(c_i) &= \text{MIN\_SCORE} = 0 \quad \forall c_i \in \text{candidates} \\
\text{UB}(c_i) &= \text{MAX\_SCORE} = 6 \quad \forall c_i \in \text{candidates}
\end{align*}

Hence, We can represent the initial candidates set with their corresponding bounds as follows:

\[
    \text{C} = 
    \begin{cases}
    c_1 = \{ \text{HNY}, \text{MLN}, \text{HYN} \}, & c_2 = \{ \text{HNY}, \text{MLN}, \text{SHN} \} \\
    c_3 = \{ \text{HNY}, \text{MLN}, \text{WLD} \}, & c_4 = \{ \text{HNY}, \text{HYN}, \text{SHN} \} \\
    c_5 = \{ \text{HNY}, \text{HYN}, \text{WLD} \}, & c_6 = \{ \text{HNY}, \text{SHN}, \text{WLD} \} \\
    c_7 = \{ \text{MLN}, \text{HYN}, \text{SHN} \}, & c_8 = \{ \text{MLN}, \text{HYN}, \text{WLD} \} \\
    c_9 = \{ \text{MLN}, \text{SHN}, \text{WLD} \}, & c_{10} = \{ \text{HYN}, \text{SHN}, \text{WLD} \} 
    \end{cases}
\]

\[
\text{C\_bounds} = \{ c_1: (0, 6), c_2: (0, 6), c_3: (0, 6), \ldots, c_{10}: (0, 6) \}
\]

Let's say we decide to ask \( R(WLD, q) \) first, and the LLM returns \( R(WLD, q) = 0.5 \). Hence, we need to update the lower bound (LB) and upper bound (UB) scores of those candidates containing \( WLD \). These candidates are \( c_3 \), \( c_5 \), \( c_6 \), \( c_8 \), \( c_9 \), and \( c_{10} \). As an example, for \( c_3 \), we will have:
\begin{align*}
\mathcal{F}(c_3, q) & = R(HNY, q) + R(MLN, q) + R(WLD, q) + D(HNY, MLN) \\
          & \quad + D(HNY, WLD) + D(MLN, WLD) \\
          & = R(HNY, q) + R(MLN, q) + 0.5 + D(HNY, MLN) \\
          & \quad + D(HNY, WLD) + D(MLN, WLD) \\
          & \implies 0.5 \leq \mathcal{F}(c_3, q) \leq 5.5
\end{align*}

Similarly, we update the bounds for \( c_{5} \), \( c_{6} \), \( c_{8} \), \( c_{9} \), and \( c_{10} \). The new candidate set bounds are as follows:

\[
\text{candidates\_set\_bounds} = \{ 
\begin{aligned}
c_1 &: (0, 6), & c_2 &: (0, 6), \\
c_3 &: (0.5, 5.5), & c_4 &: (0, 6), \\
c_5 &: (0.5, 5.5), & c_6 &: (0.5, 5.5), \\
c_7 &: (0, 6), & c_8 &: (0.5, 5.5), \\
c_9 &: (0.5, 5.5), & c_{10} &: (0.5, 5.5) 
\end{aligned}
\}
\]


This candidates set corresponds to a specific time \( t = 1 \), which only \( R(WLD, q) \) has been asked. When the next question is asked (\( t = 2 \)), the candidates set will be updated again.

Now, at (\( t = 2 \)), let's assume we choose a diversity question like \( D(HNY, MLN) \) to ask. \autoref{tab:ny_hotels_diversity} represents the diversity scores for each pair of hotels. Hence, the expert response is \(1.0\) as \( D(HNY, MLN) = 1.0\) in this table. Now, we need to update candidates including both HNY and MLN, which are \( c_{1} \), \( c_{2} \), and \( c_{3} \). For instance, for \( c_{3} \) we will have:

\begin{align*}
\mathcal{F}(c_3, q) & = R(HNY, q) + R(MLN, q) + 0.5 + D(HNY, MLN) \\
          & \quad + D(HNY, WLD) + D(MLN, WLD) \\
          & = R(HNY, q) + R(MLN, q) + 0.5 + 1.0 \\
          & \quad + D(HNY, WLD) + D(MLN, WLD) \\
          & \implies 1.5 \leq \mathcal{F}(c_3, q) \leq 5.5
\end{align*}

Similarly, we update the bounds for \( c_{1} \) and \( c_{2} \). The new candidates set bounds are as follows:

\[
\text{candidates\_set\_bounds} = \{ 
\begin{aligned}
c_1 &: (1.0, 6), & c_2 &: (1.0, 6), \\
c_3 &: (1.5, 5.5), & c_4 &: (0.0, 6.0), \\
c_5 &: (0.5, 5.5), & c_6 &: (0.5, 5.5), \\
c_7 &: (0.0, 6.0), & c_8 &: (0.5, 5.5), \\
c_9 &: (0.5, 5.5), & c_{10} &: (0.5, 5.5)
\end{aligned}
\}
\]

Finally, let's assume we are at the current point in time, when we have asked all the questions which have known values in \autoref{tab:ny_hotels_relevance} and \autoref{tab:ny_hotels_diversity}, resulting in those two tables. If we update the bounds accordingly based on all the known values, we will have up-to-date bounds as follows:

\[
\text{candidates\_set\_bounds} = \{ 
\begin{aligned}
c_1 &: (3.5, 5.5), & c_2 &: (2.5, 4.5), \\
c_3 &: (3.0, 5.0), & c_4 &: (2.0, 4.0), \\
c_5 &: (3.0, 4.0), & c_6 &: (1.5, 3.5), \\
c_7 &: (2.0, 5.0), & c_8 &: (3.0, 5.0), \\
c_9 &: (1.5, 4.5), & c_{10} &: (2.0, 4.0)
\end{aligned}
\}
\]

\end{example}


\subsection{Probabilistic Model for finding the winner}
\label{winning_probability}

In \autoref{bound_computation} we showed how to maintain and update the score bounds for each candidate after we get new information (response to a question) from an expert. We need to keep asking questions from expert and update bounds accordingly as long as the query answer is not determined, and the query answer is the candidate which we ensure has the highest score compared to others. We refer to such a candidate as the winner candidate and define it as follows:

\begin{definition}[Winner Candidate]
A candidate \( c^* \) is referred to as the \textit{Winner Candidate} if its lower bound is greater than or equal to the upper bounds of all other candidates. Mathematically, this condition can be expressed as:

\[
\text{LB}(c^*) \geq \text{UB}(c_i) \quad \forall c_i \in \text{C}, \
\]

\end{definition}

Our ultimate goal is to find $c^*$, and if $c^*$ is unknown at a point in time, meaning that there is no candidate that satisfies the above constraint, it indicates uncertainty in determining the winner, and more questions must be asked from the expert to update the bounds again and reduce uncertainty. We will formally define uncertainty of a candidates set in \autoref{uncertainty}.

In order to find the winner candidate, in this subsection, we formalize the probability distribution function of the winner candidate c*. In other words, we demonstrate how to compute the probability of any arbitrary candidate to be the winner at a given point in time. In \autoref{uncertainty}, we use this probability distribution function (PDF) to demonstrate what is the best next question to ask expert which maximally reduces the uncertainty of finding the winner candidate. Ultimately, we look for a time step in which $c^*$ is known, which means zero uncertainty in finding the winner candidate. 

\textbf{Question:} Given a candidates set $C$ with corresponding bounds for each candidate, and a specific candidate $c \in C$, what is the probability that $c = c^*$?

\begin{comment}
In order to find this probability, we consider two different cases. 

\textbf{Case 1 - Independence assumption}: In this case, we assume that the candidates are independent. Hence, we can compute $P(c = c^*)$ as follows:

% example

\[
P(c = c^*) = \prod_{c' \in C} P(\mathcal{F}(c, q) \geq \mathcal{F}(c', q))
\]

\textbf{\textcolor{red}{An example needs to be added here}}

\end{comment}
Generally, Candidates can potentially be dependent since they have common entities among them. Hence, we do not consider the Independence assumption between candidates, and compute the exact probabilities with no prior assumption. Hence, we can formalize $P(c = c^*)$ as follows:

\begin{align*}
P(c = c^*) &= P\left(\bigwedge_{c' \in C} \mathcal{F}(c, q) \geq \mathcal{F}(c', q)\right)
\end{align*}



This can be expanded as the product of conditional probabilities:

\[
P(c = c^*) = \prod_{i=1}^{n} P\left(\mathcal{F}(c, q) \geq \mathcal{F}(c_i, q) \mid \bigcap_{j=1}^{i-1} \left( \mathcal{F}(c, q) \geq \mathcal{F}(c_j, q) \right) \right)
\]

This formula is basically a multiplication of probabilities of the following type:

\[
P(\mathcal{F}(c, q) \geq \mathcal{F}(c_i, q) \mid \bigcap_{j=1}^{i-1} \left( \mathcal{F}(c, q) \geq \mathcal{F}(c_j, q) \right))
\]

Hence, We need to compute probabilities of the following the form:

\[
P(f(c_1) \geq f(c_2) \mid \text{constraints})
\]

This probability represents the likelihood that candidate \( c_1 \) scores higher than candidate \( c_2 \), given a set of constraints imposed by previous comparisons between candidates.

To compute this probability, we can rewrite it as follows:

\[
P(f(c_1) > f(c_2) \mid \text{constraints}) = \frac{p((f(c_1) \geq f(c_2)) \land \text{constraints})}{p(\text{constraints})} 
\]
\[
= \frac{n((f(c_1) \geq f(c_2)) \land \text{constraints})}{n(\text{constraints})}
\]

Where the denominator represents the size or number of all possible score assignments to candidates that satisfy the given constraints. Similarly, The numerator is just the size of a subset of  scoring assignments of the denominator that capture the additional constraint: \( f(c_1) \geq f(c_2) \). 

There are two key points that we address to compute the probabilities efficiently and accurately:

\begin{enumerate}
    \item \textbf{Storage bottleneck.} Generally, given n candidates, we can use a n-dimensional table to represent all the possible combinations of score assignment for the $n$ candidates. Then, among cells of this table that satisfy the constraints, we can compute what proportion of them satisfy the additional constraint (\( f(c_1) > f(c_2) \)) as well, and this proportion gives us the desired probability. However, let's say each candidate has m different score values. Then, the n-dimensional table has space complexity of $O(m^n)$. Therefore, it requires a huge amount of memory to compute probabilities with this approach. Basically, many of the cells in this n-dimensional table are unnecessary to store. To overcome the storage bottleneck for computing $P(c = c^*$), we do not store any n-dimensional tables. 
    
    $P(c = c^*$) is basically computing a sequence of pairwise probabilities between candidates given a set of constraints for each probability.  Hence, instead of sotring a n-dimensional table, we just consider a vector of possible score values for the first two candidates in the sequence of probabilities that we want to compute, and do cartesian product between them and compute what ratio of the combination of their values satisfies the constraints. In this say, the result of the first term in the probability sequence is obtained. Then, we do the next cartesian prodoct between the result from the previous step and the corresponding vector of the next candidate in the sequence. We repeat the same procedure until we have computed all the terms in the probability sequence of $P(c = c^*$). Then, we can get the final result by multiplying the results obtained from each step. In this approach, we only need to store the result obtained from the most recent step in probability calculation. Hence, the space complexity of computing probabilities is reduced to $O(m)$ where $m$ is the maximum size of a candidate's corresponding vector, or equivalently, number of possible values for score of a candidate. In example 3.6, we will further elaborate on this probability computation.
    \item \textbf{Computing} $\mathbf{f(c_1) \geq f(c_2).}$ Given two candidates $c1$ and $c2$, if the two candidates have common entities in between and we want to compute $P(\mathcal{F}(c, q) \geq \mathcal{F}(c_j, q))$, we cannot just rely on the bounds of $c_1$ and $c_2$ to compute the probability because since they have common entities, they have one or more common unknown questions, and we need to eliminate the effect of common unknown questions between $c1$ and $c2$ to compute the probability. This is bacause if we know a common unknown question between two candidates, it is  going to have the exact same effect on the two candidates. To eliminate the effect of such questions, we may assume that they do not have any effect on the bounds of $c_1$ and $c_2$, and compute the probabilities with this further assumption. 
\end{enumerate}

In example 3.6, we will elaborate how to compute probabilities while addressing the two mentioned key points. 

\begin{example}
    Given the bounds obtained from the example in \autoref{winning_probability}, let's assume there are only the first three candidates, which are as follows:

    \[
    \text{Candidates\_Set\_Bounds} = 
    \left\{ 
    \begin{array}{ll}
    c_1 & : \quad (3.5, 5.5), \\ 
    c_2 & : \quad (2.5, 4.5), \\ 
    c_3 & : \quad (3.0, 5.0)
    \end{array}
    \right\}
\]


    We have already assumed that the processed responses are from the discrete set $S = \{0.0, 0.5, 1.0\}$. Hence, the overall score of any arbitrary candidate should be a multiple of $0.5$ within the range specified by the candidate bounds. 

    In this example, For simplicity of notation, we replace $P(\mathcal{F}(c_i) \geq \mathcal{F}(c_j))$ by $P(c_i \geq c_j)$. Assume we are interested in calculating the probability that \( c_2 \) is the winning candidate (query answer), denoted as \( P(c_1 = c^*) \). This is formulated as:

    \[
    P(c_2 = c^*) = P(c_2 \geq c_1, \, c_2 \geq c_3) = P(c_2 \geq c_1) \cdot P(c_2 \geq c_3 \mid c_2 \geq c_1).
    \]

    To compute each term in this probability sequence, we use the result from the previous term sequentially. Since there are two terms in the above sequence, we break the problem down into two parts as follows: 
    
\textbf{(a)} \(\mathbf{P(c_2 \geq c_1)}\)


To compute this probability, we need to consider the cartesian product $c_1 \times c_2$. Then, \autoref{fig:Example_3_6_a}(a) shows how we only select a subset of all combinations of $c_1$ and $c_2$ which satisfy $c_2 \geq c_1$ by representing these combinations of satisfying values with arrows. For ease of demonstration, we have used different colors for different values of $c_2$.

Since each $c_1$ and $c_2$ have 5 possible values, there are $25$ different combinations between them, and since there are 6 arrows, it means 6 of them satisfy: $c_2 \geq c_1$. Hence, if $c_1$ and $c_2$ had no unknown questions in common (any unknown question that influences both candidates), then, we could conclude that: \(\mathbf{P(c_2 \geq c_1)} = \frac{6}{25}\). However, they have one unknown question in common which is $R(HNY, q)$ and we need to eliminate its effect on the result of this probability. To do this, we assume that $R(HNY, q) = 0$ which means it has no effect on the bounds of $c_1$ and $c_2$ particularly for computing \(\mathbf{P(c_2 \geq c_1)}\). We only do this for computing the final numerical result of a probability term, and keep the original bounds as they are when we want to use them for the next probability term. Hence, assuming $R(HNY, q) = 0$, we have the new bounds as follows:

\[
    \text{Bounds} = 
    \left\{ 
    \begin{array}{ll}
    c_1 & : \quad (3.5, 4.5), \\ 
    c_2 & : \quad (2.5, 3.5)
    \end{array}
    \right\}
\]

These tightened temporary bounds eliminate some of the arrows in \autoref{fig:Example_3_6_a}(a), and change it to \autoref{fig:Example_3_6_a}(b). Hence, after applying the removal of common elements' effect, since there are 9 possible values and there is one arrow, we have:

\(\mathbf{P(c_2 \geq c_1)} = \frac{1}{9}\)


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Example_3_6_a.jpg}
    \caption{Computing \(\mathbf{P(c_2 \geq c_1)}\)}
    \label{fig:Example_3_6_b}
\end{figure}

\textbf{(b)} \(\mathbf{P(c_2 \geq c_3 \mid c_2 \geq c_1)}\)

\autoref{fig:Example_3_6_b}(a) Shows all the possible values combination of $(c_1, c_2, c_3)$ such that $c_2 \geq c_3$ given $c_2 \geq c_1$ as the assumption. However, just as described before, we need to eliminate the effect of common unknown questions between $c_2$ and $c_3$. $R(HNY, q)$ is the only common unknown question between $c_2$ and $c_3$. Hence, assuming it is zero, we get the temporary bounds of $c-2$ and $c_3$ as follows:

\[
    \text{Bounds} = 
    \left\{ 
    \begin{array}{ll}
    c_2 & : \quad (2.5, 3.5), \\ 
    c_3 & : \quad (3.0, 4.0)
    \end{array}
    \right\}
\]

These tightened temporary bounds eliminate some of the arrows in \autoref{fig:Example_3_6_b}(a), and change it to \autoref{fig:Example_3_6_b}(b). Hence, after applying the removal of common elements' effect, since there are $3 \times 5 \times 3$ possible assignments of values and there are two ingoing arrows to $c_3$, we have:

\(\mathbf{P(c_2 \geq c_3 \mid c_2 \geq c_1)} = \frac{2}{45}\)


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Example_3_6_b.jpg}
    \caption{Computing \(\mathbf{P(c_2 \geq c_1)}\)}
    \label{fig:Example_3_6_a}
\end{figure}

Finally, we can multiply the obtained  results from part (a) and part(b) to get $P(c_2 = c^*)$. Similarly, the probabilities \( P(c_2 = c^*) \) and \( P(c_3 = c^*) \) can be computed to derive the complete probability distribution of the winning candidate $c^*$ across all possible candidates as shown in \autoref{fig:PDF}.

If we observe the bounds of these three candidates without calculating exact probabilities, we can estimate that \( c_2 \) is less likely to achieve the highest score compared to the others. While unknown questions could potentially impact the outcome, common unknown questions between candidates will affect all candidates similarly and are not game-changers. Therefore, when we exclude their influence, \( P(c_2 = c^*) \) becomes even lower. This is reasonable, as unknown questions cannot help \( c_2 \) exceed its score relative to the others.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/PDF.png}
    \caption{Probability Distribution of the winner candidate}
    \label{fig:PDF}
\end{figure}

\end{example}


\subsection{Determining the Next Question}
\label{uncertainty}

%Define entropy

% design an example of already asked questions, compute entropy, select two question, compare their effect on entropy, why one is better than another

% hence, we propose algorithm to find a question which most likey reduces entropy maximally compared to others

In \autoref{winning_probability}, we demonstrated how to compute the winning probability distribution over candidates. If the winner candidate $c^*$ is known, the uncertainty in finding the query answer is $0$, and we do not need to ask any further questions. Otherwise, we need to keep asking questions from expert to reduce the uncertainty of finding $c^*$. Hence, the key point is to find out what is the next best question to ask expert to reduce uncertainty as much as possible. 

Therefore, we use entropy to define define the uncertainty of $c^*$ as follows:

\[
H(c^*) = - \sum_{i=1}^{n} p(c_i = c^*) \log p(c_i = c^*)
\]

Where $c_i$ is the $i$-th one among all n candidates.  

Hence, we need to select a question from unknown information which potentially reduces the entropy as much as possible. However, since the response to such questions is not known without asking them, we cannot claim for sure how they are going to change the candidates' bounds, and consequently, we do not know for sure how the probability distribution function changes by such a question without asking it. Therefore, it cannot be guaranteed how much the entropy is reduced by a question before asking it. 

Although we cannot predict how exactly a question changes the entropy, since each call to an expert is costly, it is important to find the winner candidate by as minimum number of questions as possible. We have proposed an algorithm called "EntrRed", which at a given point in time, selects a question to ask next which is most likely to reduce the entropy maximally. 

\textbf{EntrRed:} In this algorithm, we first choose the candidate that has the highest probability to be the winner given the bounds. Since such candidate has the highest probability of being the winner, asking a question about this specific candidate is most likely to find the winner candidate and reduce entropy maximally. After picking such candidate, we evaluate each question among the unknown questions contributing to that candidate's score by computing a score for each, and then, we choose the question with the highest such score. This is how the score of a question is computed in this approach:

Given an unknown question $Q$, let $C_Q$ represent the subset of all candidates whose scores are influenced by $Q$, and $C'_Q$ be other candidates. The score assigned to the question $Q$ is the sum of absolute difference between winning probability of candidates in $C_Q$ and winning probability of candidates in $C'_Q$. Intuitively, if the absolute differences in the winning probability of candidates between $C_Q$ and $C'_Q$ is high, it means that asking this question can potentially prune out a lot of candidates from $C_Q$ and $C'_Q$ which previously had significant lower winning probabilities.

Algorithm 2 represents the pseudocode for the question evaluation function (QEF) that was described to evaluate a question, and assign a score to it. Algorithm 3 demonstrates the pseudocode for the EntrRed algorithm which uses QEF defined in Algorithm 2. 

\begin{algorithm}
\caption{Question Evaluation Function}
\begin{algorithmic}[1]
    \Function{evaluate\_question}{Q, $C_Q$}
        \State Initialize $score(Q) \gets 0$
        \For{each candidate $c \in C_Q$}
            \State Initialize $prob\_diff\_sum(c) \gets 0$
            \For{each candidate $c' \notin C_Q$}
                \State $prob\_diff(c,c') \gets \left| p(c) - p(c') \right|$
                \State $prob\_diff\_sum(c) \gets prob\_diff\_sum(c) + prob\_diff(c,c')$
            \EndFor
            \State $score(Q) \gets score(Q) + prob\_diff\_sum(c)$
        \EndFor
        \State \Return $score(Q)$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{EntrRed Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Input:} Set of unknown questions $\mathcal{Q_U}$, Probability Density Function (PDF) for candidates
    \State \textbf{Output:} A single question $Q^*$ to be asked next
    \State Initialize $Q^* \gets \text{null}$
    \State Initialize $max\_prob \gets -\infty$
    \State Initialize $c^* \gets \text{null}$  \Comment{Candidate with highest probability}
    
    \For{each candidate $c$}
        \State $prob(c) \gets \text{PDF}(c)$ \Comment{Obtain probability of candidate $c$}
        \If{$prob(c) > max\_prob$}
            \State $max\_prob \gets prob(c)$
            \State $c^* \gets c$  \Comment{Update best candidate}
        \EndIf
    \EndFor

    \State Let $Q_{c^*}$ be the set of questions contributing to the score of $c^*$
    \State Initialize $max\_score \gets -\infty$
    \For{each question $Q \in Q_{c^*}$}
        \State $score(Q) \gets \text{Evaluate\_Question}(Q, C_Q)$ \Comment{$C_Q$: candidates influenced by $Q$}
        \If{$score(Q) > max\_score$}
            \State $max\_score \gets score(Q)$
            \State $Q^* \gets Q$  \Comment{Update best question}
        \EndIf
    \EndFor
    
    \State \textbf{return} $Q^*$
\end{algorithmic}
\end{algorithm}



\begin{comment}
    
\textbf{Max Overlap Algorithm:} In this algorithm, we compute a score for each question, and choose the one candidate with the highest overall score. This is how the score of a question is computed in this approach:

Given an unknown question $Q$, let $C_Q$ represent the subset of all candidates whose scores are influenced by $Q$. The score assigned to the question $Q$ is the sum of overlapping areas between the score bounds of candidates in $C_Q$ and other candidates. Intuitively, the overlap between the score bounds of two candidates indicates uncertainty in determining which one is better than the other. If a question frequently appears among candidates with high overlaps, asking that question can be particularly effective in reducing uncertainty and separating the score bounds of overlapping candidates as much as possible. Algorithm 2 represents the pseudocode for the question evaluation function (QEF) that we use to evaluate a question, and assign a score to it. Algorithm 3 demonstrates the pseudocode for the Max Overlap algorithm which works based on the function defined in Algorithm 2. 

\textbf{Running Time Analysis. }The algorithm iterates over all unknown questions ($O(|Q_U|)$). For each unknown question, we iterate over all candidates influenced by it and compute their overlaps with every other candidate $(O(n^2))$. Therefore, the running time of this algorithm is asymptotically $O(|Q_U| \cdot n^2)$.




\begin{algorithm}
\caption{Question Evaluation Function}
\begin{algorithmic}[1]
    \Function{evaluate\_question}{Q, $C_Q$}
        \State Initialize $score(Q) \gets 0$
        \For{each candidate $c \in C_Q$}
            \State Initialize $overlap\_sum(c) \gets 0$
            \For{each candidate $c' \notin C_Q$}
                \State $overlap(c,c') \gets \max(0,\min(UB_c,UB_{c'}) - \max(LB_c,LB_{c'}))$
                \State $overlap\_sum(c) \gets overlap\_sum(c) + overlap(c, c')$
            \EndFor
            \State $score(Q) \gets score(Q) + overlap\_sum(c)$
        \EndFor
        \State \Return $score(Q)$
    \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Max Overlap Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Input:} Set of unknown questions $\mathcal{Q_U}$
    \State \textbf{Output:} A single question $Q^*$ with the maximum score
    \State Initialize $Q^* \gets \text{null}$
    \State Initialize $max\_score \gets -\infty$
    \For{each question $Q \in \mathcal{Q_U}$}
        \State Let $C_Q$ be the set of candidates affected by $Q$
        \State $current\_score \gets \text{Evaluate\_Question}(Q, C_Q)$
        \If{$current\_score > max\_score$}
            \State $max\_score \gets current\_score$
            \State $Q^* \gets Q$
        \EndIf
    \EndFor
    \State \textbf{return} $Q^*$
\end{algorithmic}
\end{algorithm}

\textbf{Max Probability Algorithm:} In this heuristic, we first choose the candidate that has the highest probability to be the winner given the bounds. Then, among the questions contributing to that candidate's score, we choose one that has the highest share of participation in the overlapping area of the candidates' bounds. Algorithm 4 represents the pseudocode of this heuristic to find the next best question.

\textbf{Running Time Analysis. } In this algorithm, we first select the candidate with the highest probability of being the winner ($O(n)$). Then, we iterate over questions participating in that candidate's score, and for any candidate that is influenced by this question, we sum up its overlap with other candidates ($O(n^2)$). Hence, the running time for this algorithm is asymptotically $O(n + |Q_U| \cdot n^2)$ which can be simplified as $O(|Q_U| \cdot n^2)$.



\begin{algorithm}
\caption{Max Probability Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Input:} Set of unknown questions $\mathcal{Q_U}$, Probability Density Function (PDF) for candidates
    \State \textbf{Output:} A single question $Q^*$
    \State Initialize $Q^* \gets \text{null}$
    \State Initialize $max\_prob \gets -\infty$
    \State Initialize $c^* \gets \text{null}$  \Comment{Candidate with highest probability}
    
    \For{each candidate $c$}
        \State $prob(c) \gets \text{PDF}(c)$ \Comment{Obtain probability of candidate $c$}
        \If{$prob(c) > max\_prob$}
            \State $max\_prob \gets prob(c)$
            \State $c^* \gets c$  \Comment{Update best candidate}
        \EndIf
    \EndFor

    \State Let $Q_{c^*}$ be the set of questions contributing to the score of $c^*$
    \State Initialize $max\_score \gets -\infty$
    \For{each question $Q \in Q_{c^*}$}
        \State $score(Q) \gets \text{evaluate\_question}(Q)$
        \If{$score(Q) > max\_score$}
            \State $max\_score \gets score(Q)$
            \State $Q^* \gets Q$  \Comment{Update best question}
        \EndIf
    \EndFor
    
    \State \textbf{return} $Q^*$
\end{algorithmic}
\end{algorithm}

\end{comment}

\begin{example}
    Following Example 3.6, we can compute the entropy of \( c^* \) given the probabilities for each candidate obtained by the PDF as shown in \autoref{fig:PDF}. Hence, we have:

    \[
    H(c^*) = - \sum_{i=1}^{3} p(c_i) \log p(c_i)
    \]

    we can expand the formula and compute the entropy approximately as follows:

    \[
    H(c^*) = - \left( 0.75 \log(0.75) + 0.01 \log(0.01) + 0.24 \log(0.24) \right)
    \]
    \[
    H(c^*) \approx \text{0.604}
    \]
\end{example}

As we ask more questions, we expect the entropy to decrease. Finally, when the winner candidate is obtained, the entropy goes down to zero. 

Now, we need to select the next question to ask the expert based on the EntrRed algorithm that we described. Hence, we first need to select a candidate which we are going to choose one of the unknown questions that contribute to the score of that. As discussed earlier, such a candidate is the one with the highest probability of being the winner at the point in time, which is $c_1$. Among questions influencing the score of $c_1$, two of them are unknown which are $R(HNY)$ and $D(MLN, HYN)$. Hence, we need to compute the associated score of these questions using the Question Evaluation Function (QEF) and choose the one with higher score. Let $C_Q$ represent the set of candidates that Q affects their score. Then, we have
\begin{itemize}
        \item $Q1$ = R(HNY, q): We need to compute the sum of winning probability difference between candidates from $C_{Q1}$ with any other candidates. Since $C_{Q1} = \{c_1, c_2, c_3\}$, there will be no remaining other candidates for computing overlap with these 3. Hence:
        \begin{align*}
            QEF(Q1) = 0
        \end{align*}
        \item $Q2$ = D(MLN, HYN): Since $C_{Q2} = {c_1}$, we need to compute: 

        \begin{align*}
            QEF(Q2) = |p(c_1) - p(c_2)| + |p(c_1) - p(c_3)|
        \end{align*}
        
        Hence, we have:
        \begin{align*}
            QEF(Q2) = 0.74 + 0.51 = 1.25
        \end{align*}
    \end{itemize}
    Since $QEF(Q2) > QEF(Q1)$, we choose $Q2 = D(MLN, HYN)$ as the next question. 

\begin{comment}
    
\begin{itemize}
    \item \textbf{Max Probability Algorithm}: we first need to select a candidate which we are going to choose one of the unknown questions that contribute to the score of that. As discussed earlier, such a candidate is the one with the highest probability of being the winner at the point in time, which is $c_1$. Among questions influencing the score of $c_1$, two of them are unknown which are $R(HNY)$ and $D(MLN, HYN)$. Hence, we need to compute the associated score of these questions using the Question Evaluation Function (QEF) and choose the one with higher score. Let $C_Q$ represent the set of candidates that Q affects their score. Then, we have
    \begin{itemize}
        \item $Q1$ = R(HNY, q): We need to compute the sum of overlap between the pairwise bounds of candidates from $C_Q1$ with any other candidates. Since $C_Q1 = \{c_1, c_2, c_3\}$, there will be no remaining other candidates for computing overlap with these 3. Hence:
        \begin{align*}
            QEF(Q1) = 0
        \end{align*}
        \item $Q2$ = D(MLN, HYN): Since $C_Q2 = {c_1}$, we need to sum the bounds overlap of $c_1$ and $c_2$ with $c_1$ and $c_3$. Hence, we have:
        \begin{align*}
            QEF(Q2) = 1.5 + 1 = 2.5
        \end{align*}
    \end{itemize}
    Therefore, since $QEF(Q2) > QEF(Q1)$, we choose $Q2 = D(MLN, HYN)$ as the next question. 
    
    \item \textbf{Max Overlap algorithm}: In this approach, we need to consider all the unknown questions, compute their QEF, and choose the one with maximum value. Let $C_Q$ represent the set of candidates that Q affects their score. Given our running example, since we have assumed the only candidates are $\{c_1, c_2, c_3\}$, the questions that affect the score of either of these candidates are as follows: 
    \begin{itemize}
        \item Q1 = R(HNY, q) = 0 (as computed earlier)
        
        \item Q2 = D(MLN, HYN) = 0 (as computed earlier)
        
        \item Q3 = D(MLN, SHN): Since $C_Q3 = {c_2}$, we need to sum the bounds overlap of $c_2$ and $c_1$ with $c_2$ and $c_3$. We will have:
        \begin{align*}
            QEF(Q3) = 1.5 + 1 = 2.5
        \end{align*}
        
        \item Q4 = Since $C_Q4 = {c_3}$, we need to sum the bounds overlap of $c_3$ and $c_1$ with $c_3$ and $c_2$. Therefore:
        \begin{align*}
            QEF(Q4) = 1.5 + 1.5 = 3.0
        \end{align*}
    \end{itemize}
    Hence, since $QEF(Q4)$ has the highest value compared to others, we choose $Q4 = D(MLN, WLD)$ as the next question. 
\end{itemize}

\end{comment}


\subsection{Response Processing}
\label{Response_Processing}
After we select the next question to ask the expert(s), we need to process the received response into our desired form. Depending on if we are using a single or multiple experts, and if the expert response is a discrete value or a range, we have 4 different cases as mentioned in :

\begin{enumerate}
    \item \textbf{Single Expert, Single Response}
    \item \textbf{Single Expert, Range Response}
    \item \textbf{Multiple Experts, Range Response}
    \item \textbf{Multiple Experts, Range Response}
\end{enumerate}

In \autoref{sec:section2.3} we formally defined the overall approach showing the process of mapping the responses from either of the above cases to our desired format. Algorithm 4 shows our generalized algorithm to process the experts' responses which covers all the above cases. 

\begin{algorithm}
\caption{Generalized Response Processing Algorithm}
\begin{algorithmic}[1]
\State \textbf{Input:} Set of experts' responses $R$, where each response $R_i$ is a range $(R_{i,1}, R_{i,2})$. If $R_i$ is a single response, set $R_i = (R_i, R_i)$.
\State \textbf{Output:} Probability density function $f_{\mathcal{S}}(r)$ over the set of desired responses $\mathcal{S}$.

\State Initialize $\mathcal{S}^* \gets []$ \Comment{List to store mapped desired responses}
\For{each response $R_i = (R_{i,1}, R_{i,2})$ in $R$}
    \State Initialize $\mathcal{S}_i^* \gets []$ \Comment{Subset of $\mathcal{S}$ for current response range}
    \For{each $r \in \mathcal{S}$}
        \If{$R_{i,1} \leq r \leq R_{i,2}$}
            \State Append $r$ to $\mathcal{S}_i^*$
        \EndIf
    \EndFor
    \For{each $r_j \in \mathcal{S}_i^*$}
        \State Append $r_j$ to $\mathcal{S}^*$
    \EndFor
\EndFor

\State Calculate total number of elements in $\mathcal{S}^*$, denoted as $n^*$
\State Initialize $f_{\mathcal{S}}(r) \gets 0$ for each $r \in \mathcal{S}$

\For{each $r \in \mathcal{S}^*$}
    \State $f_{\mathcal{S}}(r) \gets f_{\mathcal{S}}(r) + \frac{1}{n^*}$ \Comment{Update probability density based on frequency}
\EndFor

\State \textbf{Return} $f_{\mathcal{S}}(r)$
\end{algorithmic}
\end{algorithm}

\begin{example}
    Following the running example, let's assume that we would like to use the max probability algorithm to select the next question. In this case, as reasoned earlier in Example 3.7, the next selected question will be $Q = D(MLN, HYN)$. 
    We need to leverage Algorithm 5 to output a probability density function over the set of desired responses so that it would be further used to update the bounds accordingly. 
    
    Let's assume that we have 3 experts denoted as $E = \{e_1, e_2, e_3\}$ which all return ranges rather than single discrete values (Case 4). Following our initial assumption in the running example, the set of desired responses is $S = {0.0, 0.5, 1.0}$. Imagine that the experts' responses are as follows:

\[
\begin{aligned}
    \text{Response}(e_1) &= (0.0, 0.2), \\
    \text{Response}(e_2) &= (0.0, 0.3), \\
    \text{Response}(e_3) &= (0.5, 1.0)
\end{aligned}
\]

Each response is then mapped to a subset of desired responses as followed:
\[
\begin{aligned}
    \text{Mapped(Response}(e_1)) &= \{0.0\}, \\
    \text{Mapped(Response}(e_2)) &= \{0.0\}, \\
    \text{Mapped(Response}(e_3)) &= \{0.5, 1.0\}
\end{aligned}
\]

Therefore, by aggregating the obtained mapped responses, we can get the PDF of the experts responses, denoted as PDF(E), as shown in \autoref{fig:Response_PDF}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Response_PDF.png}
    \caption{Overall Probability Density of the Experts' Responses}
    \label{fig:Response_PDF}
\end{figure}

This PDF is then used to update the bounds. In this case, since the PDF is nonuniform, the updated bounds will be nonuniform as well. In more detail, in this case, if $(LB, UB)_c = (\alpha, \beta)$, the potential score values within this range may no longer be equally probable, and can have different probabilities associated with them. 

\end{example}