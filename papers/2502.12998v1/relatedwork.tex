\section{Related Work}
\label{sec:rel}
Our work is closest to reducing the cost of ML inference and to combining queries and ML inference since we aim to answer queries over predicted scores. We believe we are the first to examine the question of top-$k$ under this setting. 
One line of work in query inference, provides native relational support for ML  using containerized solutions such as Amazon Aurora~\cite{sagemaker_2022}, or in-application solutions such as Google's BigQuery ML~\cite{bigquery_2022} and Microsoft's Raven~\cite{DBLP:conf/cidr/KaranasosIPSPPX20}. 

Recent work adopted the use of cheap proxy models, such as image classifiers, to identify  an  approximate  set of $k$ entities satisfying a query~\cite{DBLP:journals/pvldb/DingAL22,DBLP:journals/pvldb/KangGBHZ20}. These works propose sampling approaches to achieve a target precision or recall accuracy. 
Lai et al.~\cite{10.1145/3448016.3452786} studies approximate top-$k$ queries with light-weight proxy models that generate oracle label distribution.
Gao et al.~\cite{DBLP:conf/sigmod/GaoXAY21} introduce a Multi-Level Splitting Sampling to let one "promising" sample path prefix generate multiple "offspring" paths, and direct Monte-Carlo based simulations toward more promising paths.  The ThalamusDB system is an approximate query processing system that integrates into SQL natural language predicates on multimodal data: visual, audio, and text~\cite{DBLP:journals/pacmmod/JoT24}. The system chooses optimized plans according to user preferences on approximation error, computation time, and user labeling overheads. This work is complementary to ours as it does not handle ranked retrieval.
Bolukbasi et al.~\cite{DBLP:conf/icml/BolukbasiWDS17} studied incremental predictions for neural networks.  
 Computation time is reduced by pruning examples that are classified in earlier layers, selected adaptively. 
Kang et al.~\cite{DBLP:journals/pvldb/KangEABZ17} present NOSCOPE, a system for querying videos that can reduce the cost of neural network video analysis by up to three orders of magnitude via inference-optimized model search.
Lu et al.~\cite{DBLP:conf/sigmod/LuCKC18} and Yang et al.~\cite{10.14778/3547305.3547310} use probabilistic predicates to filter data blobs that do not satisfy the query and empirically increase data reduction rates. 
Anderson et al.~\cite{DBLP:conf/icde/AndersonCRW19} use a hierarchical model to reduce the runtime cost of queries over visual content.