\section{Conclusion}
We presented the first study of the robustness of \ac{LLM}-based deductive reasoning methods by introducing two types of perturbations: adversarial noise and counterfactual statements. These perturbations were used to examine the methodological aspects of \ac{LLM} reasoners based on their format, syntax, and feedback mechanism for error recovery. While adversarial noise only affects autoformalisation approaches, counterfactual statements remain a significant challenge for all variants of the tested method. While feedback strategies may lead to fewer syntax errors in autoformalisation methods, the refined formalisations tend to be semantically incorrect, failing to increase accuracy. We call on future work to devise more advanced mechanisms for detecting, reporting, and incorporating semantic errors. We also anticipate generalizing the study in this paper to other logical deduction datasets.
