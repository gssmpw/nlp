\section{Related Work}

This section gives an overview of \ac{LLM}-based reasoning methods and studies that evaluate their robustness. 

\subsection{Methods for LLM-based Reasoning}
\paragraph{Informal reasoning.} 
Scaling up the size of \acp{LLM} enables strong performance in many \ac{NLP} tasks by few-shot prompting~\cite{brown_language_2020}, which suggests inherent reasoning capabilities.  
\ac{CoT} combines few-shot prompting with generating intermediate informal reasoning chains~\cite{wei_chain_2022}. 
These informal reasoning skills motivated more elaborate prompting techniques, like Zero-Shot \ac{CoT}~\cite{kojima_large_2022} or self-consistency by generating multiple chains~\cite{wang_selfconsistency_2022}, as well as more complex structures than chains, such as Tree of Thoughts~\cite{yao_tree_2023} and Graph of Thoughts~\cite{besta_graph_2024}. 
These methods use an \ac{LLM} to generate intermediate steps and evaluate the output through self-refinement.
Similarly, ProofWriter~\cite{tafjord_proofwriter_2021} improves multi-hop reasoning by adding the intermediate results to the reasoning context~\cite{tafjord_proofwriter_2021}.  
A key benefit of informal reasoning chains is their flexibility, but this comes at the expense of guaranteeing faithfulness. Consequently, methods combining \ac{LLM}s with formal reasoning have been suggested.

\noindent
\paragraph{Autoformalisation.} 
Instead of informal reasoning, another way combines an \ac{LLM} with a deterministic symbolic solver, e.g., a theorem prover. 
Here, the prover guarantees faithful and deterministic reasoning.
This combination is known as \emph{autoformalisation}.
One of the first autoformalisation approaches using formal reasoning chains in combination with prompting is PAL~\cite{gao_pal_2023}. 
The authors generate Python snippets alongside informal steps and generate the final response by executing the generated code snippets.  
Logic-LM~\cite{liangming_pan_logiclm_2023} prompts \acp{LLM} to generate multiple task-specific formalisations (logic programming, first-order logic, satisfiability modulo theories and constraint satisfaction), which are solved by dedicated solvers. 
They report higher robustness for longer reasoning chains compared to \ac{CoT} reasoning.
The extension Logic-LM++~\cite{kirtania_logic-lm_2024} tries to avoid new syntax errors by integrating a self-refinement mechanism.
The LINC~\cite{olausson_linc_2023} approach uses the idea of self-consistency from \ac{CoT} and generates multiple formalisations to avoid formalisation errors. 
Autoformalisation models achieve high accuracy for many deductive reasoning benchmarks, showing clear benefits for complex reasoning.

\paragraph{Comparison.} Our work is the first to explore the robustness of \ac{LLM} reasoning approaches from prior work: direct few-shot prompting, \ac{CoT}, and autoformalisation. Another contribution of our work is consolidating these methods with their syntax and error recovery choices into a coherent methodological framework.

\subsection{Robustness Evaluation of LLM Reasoning}
\paragraph{Evaluating deductive reasoning.} The improvements of \ac{LLM}-based reasoning have inspired the development of benchmarks investigating capabilities for deductive reasoning. 
The synthetic PrOntoQA~\cite{saparov_language_2022} dataset is built on modus ponens multi-hop reasoning and confirms reasoning capabilities for the largest models. 
However, they noted issues with proof planning and selecting correct proof steps for longer reasoning chains. 
The FOLIO~\cite{han_folio_2022} and AR-LSAT~\cite{zhong_analytical_2022} benchmarks confirmed these errors for more complex reasoning and more naturalistic language. 
LogicBench~\cite{mihir_parmar_logicbench_2023} is a recent benchmark that systematically studies the performance of \ac{LLM}-based reasoners across multiple inference rules (e.g., modus ponens), reporting a good model performance for predicate logic and first-order logic grammars.
%% Unfaithful reasoning  and spurious correlation 
A vital challenge identified by these works is unfaithful reasoning, i.e., the intermediate steps are not used to infer the final result~\cite{ye_unreliability_2022,lanham_measuring_2023,tanneru_difficulty_2024}. 


\paragraph{Robustness studies.}
A key requirement of \ac{LLM}-based reasoning methods is their robustness to noise~\cite{ebrahimi-etal-2018-hotflip} and out-of-distribution~\cite{hendrycks2021measuring} inputs. Given the strong performance of \acp{LLM} across many domains and benchmarks~\cite{sarlin2020superglue}, dealing with noisy data in reasoning tasks has been considered more important~\cite{sourati-etal-2024-robust}, including adversarial and counterfactual perturbations.
A variety of robustness tests have therefore been designed~\cite{wang_measure_2022}. 
These robustness benchmarks rely on the original problem's perturbations through paraphrasing and distractions on character, word, and sentence levels~\cite{sourati-etal-2024-robust,sarlin2020superglue}. 
Many works try to generate adversarial examples to better understand the generalization capabilities of \acp{LLM}, which are shown to significantly decrease \ac{LLM} performance~\cite{wang_measure_2022}.
RUPbench is a recent robustness study on logical reasoning of \acp{LLM}~\cite{wang_rupbench_2024}, covering many reasoning datasets and all three types of perturbations.
Another category involves semantic changes in the input texts. 
Semantic changes can also be performed purely on a linguistic or logical level.
Recent approaches use these logic-based perturbations~\cite{nakamura-etal-2023-logicattack}, which aligns with our robustness framework. Meanwhile, counterfactual studies of \ac{LLM} robustness have been less common. One exception is RECALL~\cite{liu2023recall}, a benchmark based on external knowledge bases, whose study reveals that \acp{LLM} are generally susceptible to external knowledge with counterfactual information and that simple mitigation strategies cannot significantly alleviate this challenge.

\paragraph{Comparison.} We conduct the first investigation of robustness differences between formal and informal \ac{LLM}-based logical deduction methods. For this purpose, we base our experiments on LogicBench~\cite{mihir_parmar_logicbench_2023}, which systematically includes nine inference rules mapped to natural language situations. We develop seven additional variants of LogicBench incorporating adversarial noise and counterfactual statements, in line with prior work like \cite{nakamura-etal-2023-logicattack} that studies \ac{LLM} robustness on other reasoning tasks.