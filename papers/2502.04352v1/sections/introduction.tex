\section{Introduction}
\label{sec:introduction}

Deriving new knowledge from existing knowledge, as in deductive reasoning, is a key human cognitive skill necessary for various applications, including complex question-answering and decision-making. Deductive reasoning can be intensive for humans (e.g., taking a lot of time), require specific expertise (e.g., logicians), and lead to incorrect conclusions (e.g., due to biases). This promotes deductive reasoning over \ac{NL} as a key objective of human-centric AI. Automatic deduction engines aim to support humans by providing certifiable reasoning chains, avoiding invalid inferences, and accelerating the process. To provide effective support for deductive reasoning, AI must be able to formalise knowledge and rules provided in \ac{NL} robustly.

Performing logical reasoning has received much interest in AI. In the early days, symbolic methods were aimed at transforming specific parts of language into logical statements~\cite{pereira1982logic}. Recently, \acp{LLM} have been shown to achieve impressive results for many reasoning-based \ac{NLP} tasks, suggesting a degree of deductive reasoning capability~\cite{srivastava2023beyond}. In particular, generating informal reasoning chains via \ac{CoT} prompting achieves good reasoning performance on many benchmarks~\cite{wei_chain_2022}. Contrary to symbolic methods, \acp{LLM} can answer a deductive reasoning task without providing a formal intermediate reasoning chain. Nevertheless, these informal reasoning chains do not need to follow truth-preserving proof rules, thus leading to reasoning chains that are hard to verify. Recent work shows that many informal reasoning chains suffer from lack of \textit{faithfulness}~\cite{ye_unreliability_2022,tanneru_difficulty_2024}. 


\begin{figure}[!t]
    \centering
    \scriptsize
    \begin{tikzpicture}[node distance=.5em and .5em,
        every node/.style={align=center, minimum height=3em, minimum width=2em},
        label_style/.style={rectangle, minimum height=1em, rounded corners, fill=white}
        ]
        \node[draw](llm){\small \bfseries{LLM}};
        
        \node[rectangle, rounded corners, fill= lightgray, above= 3em of llm, anchor=east, xshift=-0.5em](input_context){All men are \Ccancel{teal}{mortal}. \\ Socrates is a man.};
        \node[rectangle, rounded corners, fill=lightgray, right= 1em of input_context](input_question){Is Socrates mortal?};
       
        \node[rectangle, rounded corners,draw=lightgray, fit={(input_context)(input_question)}](input){};
        \node[label_style, above=2.8em of input.west, anchor=west](input_label){ Input};
        
        \node[draw=lightgray, rectangle, rounded corners, below= 4em of llm.west, anchor=east, xshift=-1em](direct_answer){Yes};
        \node[draw=lightgray, rectangle, rounded corners, text width= 0.2\linewidth, below= of direct_answer, xshift=-2.3em](cot_answer){Socrates is a man so he is also mortal. Yes};
        \node[rectangle, rounded corners,draw, fit={(direct_answer)(cot_answer)}](informal){};
        \node[label_style, text width= 6em, above=4em of informal.west, anchor=west, xshift=-2em](informal_label){\textcolor{blue}{1a. Informal reasoning}};
        
        \node[draw=lightgray, rectangle, rounded corners, text width= 0.54\linewidth, below= 5.5 em of llm.east, anchor=west, xshift=0.5em](lf_text){$\forall~X~man(X) \implies mortal(X) \wedge man(socrates) \models mortal(socrates)$};
        \node[fill=white, rectangle, minimum height=1.5em, above=-0.4em of lf_text.north east, anchor=south east](fol){\textcolor{blue}{2. Formalisation syntax (FOL)}};
        \node[draw, below= of lf_text](atp){\small \bfseries{Prover}};
        \node[draw=lightgray, rectangle, rounded corners, right= 1.5em of atp](af_answer){Valid};
        \node[rectangle, rounded corners,draw, fit={(fol)(lf_text)(atp)(af_answer)}](auto){};
        \node[label_style, above=4.5em of auto.west, anchor=west, xshift=-1em](auto_label){\textcolor{blue}{1b. Autoformalisation}};

        \draw[-Triangle] (input)   -- (llm);

        \draw[-Triangle] (llm.west)   -| ([xshift=2em]informal.north);
        \draw[-Triangle] (llm.east)   -| ([xshift=2em]auto.north);

        \draw[-Triangle] ([xshift=-5em]lf_text.south)   |- ([yshift=.5em]atp.west);
        \draw[-Triangle] (atp.east)   -- (af_answer.west);

        \draw[dashed,-Triangle] ([yshift=-.5em]atp.west)   -| (llm.south) node [near start, xshift=-0.5em, text width=12em] {\textcolor{blue}{3. Error recovery \\ (feedback)}};

        \node[align=left, right=3em  of input_question.east, anchor=west](legend){ \textcolor{orange}{Noise}\\ \textcolor{teal}{Counterfactual}};
        \node[rectangle, fill=orange,  opacity=.75, overlay, minimum height=1.5em, below=-0.1em  of input_context.south, anchor=north](dis_example) {Greece is a country.};
        \node[rectangle, fill=teal, opacity=.75, overlay, minimum height=1.5em, above= of input_context.north east, xshift=-4em, anchor=west]  {immortal};
        
    \end{tikzpicture}
    \caption{Overview of our methodology for investigating the robustness of reasoning with \acp{LLM}. Our perturbations (noise and counterfactuals) are shown in orange and teal, respectively. The three dimensions of our \ac{LLM}-based methodological framework (reasoning format, syntax, and error recovery mechanism) are shown in blue.}
    \label{fig:overview}
\end{figure}

Addressing these challenges, \emph{autoformalisation} approaches ~\cite{liangming_pan_logiclm_2023,olausson_linc_2023} use \acp{LLM} to translate \ac{NL} input into a logical form, and a deterministic symbolic solver to perform the deductive reasoning. Autoformalisation is thus a hybrid approach, which aims to provide a faithful and verifiable reasoning chain while leveraging the linguistic manipulation skills of \acp{LLM}. 
Autoformalisation faces two key challenges: First, since they translate rich \ac{NL} into a limited grammar of symbols and operations, it is critical to leverage a syntax with an optimal tradeoff between translation accuracy and expressivity. Second, while autoformalisation chains provide an opportunity for syntactic and semantic validation and error analysis, designing an effective and efficient terror recovery mechanism is non-trivial. While prior autoformalisation systems leverage multiple syntaxes and error recovery mechanisms, \textit{no systematic study has investigated their impact on autoformalisation accuracy}.

Meanwhile, a key requirement of \ac{LLM}-based reasoning methods is their robustness to noise~\cite{ebrahimi-etal-2018-hotflip} and out-of-distribution~\cite{hendrycks2021measuring} inputs. Given the strong performance of \acp{LLM} across many domains and benchmarks~\cite{sarlin2020superglue}, dealing with noisy data in reasoning tasks has been considered more important~\cite{sourati-etal-2024-robust}. Most evaluations have focused on adversarial noise (e.g., lexical perturbations)~\cite{sarlin2020superglue}, while a recent study has also experimented with counterfactual statements~\cite{liu2023recall}. While robustness evaluations for \ac{NLP} tasks have yielded mixed results~\cite{wang_rupbench_2024,liu2023recall}, it remains \textit{unclear to which extent \acp{LLM}, in both informal and autoformalisation methods, are robust in logical deduction tasks}.

We address these two challenges by \textbf{investigating the robustness of \ac{LLM}-based deductive reasoning methods}. Our overall approach is summarized in Figure \ref{fig:overview}. Our study makes three contributions.
First, following standard practices in evaluating robustness, we devise a \textbf{robustness framework for logical deduction} with two families of perturbations: adversarial noise, where the model needs to preserve its label in the face of added irrelevant information, and counterfactual perturbations, where a single alteration in the context flips the label of the question. The combinations of the perturbations produce seven variants from a given dataset.
Second, we synthesize the landscape of existing \ac{LLM}-based logical deduction methods into a \textbf{methodological framework with three dimensions}: reasoning format, grammar syntax, and error recovery mechanism. For each of these dimensions, we incorporate representative approaches in the literature.
Third, We perform \textbf{extensive experiments} with seven \acp{LLM} on eight perturbed variants of a recent modular benchmark. Our findings provide nuanced insights into the robustness of \ac{LLM}-based methods on logical deduction.