\acp{LLM} have been shown to achieve impressive results for many reasoning-based \ac{NLP} tasks, suggesting a degree of deductive reasoning capability. However, 
it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks.
Moreover, while many LLM-based deduction methods have been proposed, there is a lack of a systematic study that analyses the impact of their design components.
Addressing these two challenges, we propose \textit{the first study of the robustness of LLM-based deductive reasoning methods}. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively. 

