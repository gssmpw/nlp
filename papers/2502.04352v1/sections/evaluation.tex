\section{Evaluation}
We provide three sets of insights into this section, organised as \textit{findings (F*)}. We quantitatively study the effect of the adversarial and counterfactual perturbations on the performance of informal reasoners and autoformalisation methods. Then, we dive deeper into method variants. Finally, 
we analyse the nature of formalisation errors made by the models.

\subsection{Robustness Analysis}
\paragraph{\textbf{\emph{F1: Noise perturbations have a stronger effect on formalisation methods than informal \ac{LLM} reasoners.}}}
Table~\ref{tab:distraction_k4_formalisation} shows that, on average, the accuracy of both direct and \ac{CoT} informal reasoning remains between $73\%$ and $74\%$ in the face of added noise. While the autoformalisation method performs similarly to informal reasoners on the original dataset, its performance decreases between $4\%$ and $11\%$. The accuracy drops especially with logical (L) and tautological (T) distractions, whose logical language formats trick the \ac{LLM} into formalizing the noisy clauses. On the other hand, the linguistically complex and more natural sentences of encyclopedic distractions show a minor effect, suggesting that \acp{LLM} successfully avoids formalizing the more complicated sentences.

\paragraph{\textbf{\emph{F2: All \ac{LLM}-based reasoning methods suffer a drop for counterfactual perturbations.}}} % influence .}}}
Table~\ref{tab:distraction_k4_formalisation} shows that counterfactual statements cause a significant decrease in performance for both the informal reasoners and autoformalisation methods of between $12\%$ and $13\%$ on average. 
Moreover, this observation also holds for all tested models, i.e., none are robust towards counterfactual perturbations across every evaluated dimension. Even the strongest model, GPT 4o-mini, yields a performance of 63-68\%, which is relatively close to the random performance of 50\%. The high impact of counterfactual statements (the single ``not'' inserted) could be due to the inability of \acp{LLM} to overwrite prior knowledge with explicitly stated information or memorization of the answers. We study the error sources further in ยง\ref{subsec:errors}.  

\noindent \paragraph{\textbf{\emph{F3: Introducing multiple noise sentences has an effect only for logical distractions.}}}
We show the impact of introducing between one and four sentences for the two top-performing autoformalisation models in Figure~\ref{fig:length_distraction}. The figure shows similar trends with and without counterfactual perturbations.
As additional logical distractions are introduced, the model performance consistently decreases. Tautological (T) distractions lead to a decline in accuracy with a single disruptive sentence, yet adding more noise does not worsen the outcome. 
The tautological corpus introduces truth constants for all sentences as a persistent unseen logical construct. Given that this leads only to a decrease for a single occurrence, we can assume that a model can consistently handle the same unseen logical construct. In contrast, the logical corpus increases the chance of adding text, requiring new, previously unseen reasoning constructs for each added sentence. The impact of encyclopedic noise remains negligible, generalising F1 to $k$ sentences. Similarly, counterfactual perturbations remain much more effective for all settings, generalising F2.

\begin{table}[!t]
\small
\setlength{\modelspacing}{2pt}
\setlength{\tabcolsep}{1.7pt} % Default value: 6pt
\setlength{\belowrulesep}{4pt}
\begin{threeparttable}
    \centering
    \begin{tabular}{cc l r rrr @{\quad} rrrr}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{} & Reasoning & \multirow{2}{*}{O} & \multicolumn{3}{c}{Distraction} & \multicolumn{4}{c}{Counterfactual} \\
 & & Format & & E& L & T & $\text{O}_C$ & $\text{E}_C$& $\text{L}_C$ & $\text{T}_C$\\
\midrule
\multirow{6}{*}{\rotatebox{90}{Gemma-2}} & \multirow{3}{*}{\rotatebox{90}{9b}}
   & Informal (direct) & \textbf{0.78} & \textbf{0.80} & \textbf{0.79} & \textbf{0.77} & 0.58 & 0.52 & 0.50 & 0.59 \\
 & & Informal (CoT) & 0.72 & 0.78 & 0.73 & 0.76 & 0.61 & \textbf{0.57} & \textbf{0.60} & \textbf{0.66} \\
 & & Formal (FOL) & 0.62 & 0.58 & 0.52 & 0.53 & \textbf{0.63} & 0.52 & 0.46 & 0.46 \\[\modelspacing]
\cmidrule{2-11}
 & \multirow{3}{*}{\rotatebox{90}{27b}} 
   & Informal (direct) & 0.71 & 0.69 & \textbf{0.66} & \textbf{0.68} & 0.59 & 0.51 & 0.54 & 0.59 \\
 & & Informal (CoT) & 0.66 & 0.65 & 0.64 & 0.63 & 0.62 & 0.58 & \textbf{0.62} & \textbf{0.64} \\
 & & Formal (FOL) & \textbf{0.74} & \textbf{0.74} & 0.61 & 0.61 & \underline{\textbf{0.72}} & \underline{\textbf{0.67}} & 0.58 & 0.51 \\[\modelspacing]
\midrule
\multirow{6}{*}{\rotatebox{90}{Mistral}} & \multirow{3}{*}{\rotatebox{90}{7B}} 
   & Informal (direct) & 0.77 & \textbf{0.77} & 0.75 & \textbf{0.79} & \textbf{0.63} & \textbf{0.54} & \textbf{0.54} & \textbf{0.66} \\
 & & Informal (CoT) & \textbf{0.79} & 0.75 & \textbf{0.77} & 0.78 & 0.55 & 0.52 & \textbf{0.54} & 0.58 \\
 & & Formal (FOL) & 0.62 & 0.58 & 0.54 & 0.57 & 0.50 & \textbf{0.54} & 0.51 & 0.52 \\[\modelspacing]
\cmidrule{2-11}
 & \multirow{3}{*}{\rotatebox{90}{Small}} 
   & Informal (direct) & \textbf{0.77} & \textbf{0.76} & \textbf{0.76} & \textbf{0.75} & 0.61 & 0.51 & 0.56 & 0.59 \\
 & & Informal (CoT) & 0.72 & 0.72 & 0.72 & 0.71 & \textbf{0.62} & \textbf{0.59} & \textbf{0.62} & \textbf{0.68} \\
 & & Formal (FOL) & 0.68 & 0.59 & 0.53 & 0.64 & 0.54 & 0.55 & 0.49 & 0.51 \\[\modelspacing]
\midrule
\multirow{6}{*}{\rotatebox{90}{Llama-3.1}} & \multirow{3}{*}{\rotatebox{90}{8B}} 
   & Informal (direct) & 0.63 & 0.61 & 0.64 & 0.66 & 0.61 & \textbf{0.62} & 0.59 & 0.61 \\
 & & Informal (CoT) & 0.73 & \textbf{0.73} & \textbf{0.71} & \textbf{0.72} & \textbf{0.62} & 0.59 & \textbf{0.61} & \textbf{0.65} \\
 & & Formal (FOL) & \textbf{0.77} & 0.71 & 0.63 & 0.52 & 0.60 & 0.58 & 0.55 & 0.52 \\[\modelspacing]
\cmidrule{2-11}
 & \multirow{3}{*}{\rotatebox{90}{70B}} 
   & Informal (direct) & 0.77 & 0.74 & 0.74 & 0.73 & 0.62 & 0.53 & 0.56 & 0.64 \\
 & & Informal (CoT) & \textbf{0.78} & \textbf{0.75} & \textbf{0.76} & \textbf{0.76} & 0.64 & 0.61 & \textbf{0.66} & \underline{\textbf{0.73}} \\
 & & Formal (FOL) & 0.74 & 0.73 & 0.71 & 0.71 & \textbf{0.66} & \textbf{0.62} & 0.59 & 0.57 \\[\modelspacing]
 \midrule
\multirow{3}{*}{\rotatebox{90}{GPT}} & \multirow{3}{*}{\rotatebox{90}{4o-mini}} 
   & Informal (direct) & 0.78 & 0.77 & 0.79 & 0.79 & 0.64 & 0.61 & 0.61 & 0.63 \\
 & & Informal (CoT) & 0.80 & 0.80 & \underline{\textbf{0.81}} & \underline{\textbf{0.82}} & \textbf{0.68} & \textbf{0.63} & \underline{\textbf{0.68}} & \textbf{0.64} \\
 & & Formal (FOL) & \underline{\textbf{0.84}} & \underline{\textbf{0.82}} & 0.73 & 0.79 & 0.63 & 0.62 & 0.57 & 0.54 \\[\modelspacing]
 \midrule
\multicolumn{2}{c}{\multirow{3}{*}{\textbf{Avg}}} 
 & Informal (direct) & 0.74 & 0.73 & 0.73 & 0.73 & 0.61 & 0.55 & 0.56 & 0.62 \\
 & & Informal (CoT) & 0.74 & 0.74 & 0.73 & 0.74 & 0.62 & 0.58 & 0.62 & 0.65 \\
  & & Formal (FOL) & 0.72 & 0.68 &	0.61 & 0.62 & 0.61 & 0.59 & 0.54 & 0.52 \\
\bottomrule
\end{tabular}
\caption{Accuracies of informal and autoformalisation-based deductive reasoners. The best overall model per dataset is underlined; the best model version is marked in bold.}
\label{tab:distraction_k4_formalisation}
\end{threeparttable}
\end{table} 

\begin{figure}[!t]
    \centering
    \scriptsize
    \begin{tikzpicture}
        \begin{axis}[name=gpt,
            title={GPT-4o-mini},
            width=0.6\linewidth,
            height=0.6\linewidth,
            xlabel={\# Noise sentences},
            ylabel={Accuracy},
            xmin=-0.1, xmax=4.1,
            ymin=0.5, ymax=0.9,
            xtick={1,2,4},
            ytick={0.55, 0.6, 0.65, 0.75, 0.8, 0.85},
            title style={yshift=-0.6em},
            legend style={at={(1,-0.15)},
	           anchor=north,legend columns=-1},
            x label style={at={(axis description cs:1,-0.05)},anchor=north},
            y label style={at={(axis description cs:-0.15,0.5)},anchor=south},
            ymajorgrids=true,
            grid style=dashed,
        ]
            \addplot[color=blue, mark=square,]
                coordinates {
                (0,0.848076939582825)(1,0.823076903820038)(2,0.826923072338104)(4,0.821153819561005)
                };
            \addplot[color=red, mark=triangle,]
                coordinates {
                (0,0.848076939582825)(1,0.817307710647583)(2,0.801923096179962)(4,0.759615361690521)
                };
            \addplot[color=green, mark=diamond,] 
                coordinates {
                (0,0.848076939582825)(1,0.767307698726654)(2,0.769230782985687)(4,0.803846180438995)
                };
            \addplot[color=blue, mark=square*] 
                coordinates {
                (0,0.627777755260468)(1,0.622222244739533)(2,0.600000023841858)(4,0.633333325386047)
                };
            \addplot[color=red, mark=triangle*,] 
                coordinates {
                (0,0.627777755260468)(1,0.611111104488373)(2,0.611111104488373)(4,0.594444453716278)
                };
            \addplot[color=green, mark=diamond*,] 
                coordinates {
                (0,0.627777755260468)(1,0.572222232818604)(2,0.538888871669769)(4,0.555555582046509)
                };
                \legend{E,L,T,$\text{E}_C$, $\text{L}_C$ , $\text{T}_C$}
        \end{axis}

        \begin{axis}[name=llama, at={($(gpt.east)+(0.1cm,0)$)},anchor=west,
            title={Llama 3.1 70b},
            width=0.6\linewidth,
            height=0.6\linewidth,
            xmin=-0.1,, xmax=4.1,
            ymin=0.5, ymax=0.9,
            xtick={1,2,4},
            ytick={0.55, 0.6, 0.65, 0.75, 0.8, 0.85},
            title style={yshift=-0.6em},
            yticklabel=\empty,
            ymajorgrids=true,
            grid style=dashed,
        ]
            \addplot[color=blue, mark=square,]
                coordinates {
                (0,0.838461518287659)(1,0.817307710647583)(2,0.805769205093384)(4,0.817307710647583)
                };
            \addplot[color=red, mark=triangle,]
                coordinates {
                (0,0.838461518287659)(1,0.819230794906616)(2,0.803846180438995)(4,0.771153867244721)
                };
            \addplot[color=green, mark=diamond,]
                coordinates {
                (0,0.838461518287659)(1,0.803846180438995)(2,0.807692289352417)(4,0.805769205093384)
                };
            \addplot[color=blue, mark=square*]
                coordinates {
                (0,0.627777755260468)(1,0.622222244739533)(2,0.577777802944183)(4,0.594444453716278)
                };
            \addplot[color=red, mark=triangle*,]
                coordinates {
                (0,0.627777755260468)(1,0.583333313465118)(2,0.561111092567444)(4,0.577777802944183)
                };
            \addplot[color=green, mark=diamond*,]
                coordinates {
                (0,0.627777755260468)(1,0.627777755260468)(2,0.566666662693024)(4,0.577777802944183)
                };
        \end{axis}
    \end{tikzpicture}
    \caption{Influence of the number of noisy sentences for FOL.}
    \label{fig:length_distraction}
\end{figure}



\subsection{Impact of Method Design}
\paragraph{\textbf{\emph{F4: \ac{CoT} prompting is most impactful when both noise and counterfactual perturbations are applied.}}}
The accuracies for the individual \acp{LLM} in Table~\ref{tab:distraction_k4_formalisation} show that the impact of \ac{CoT} is negligible for noise-only datasets (first four columns). Meanwhile, the benefit from \ac{CoT} is most pronounced in the datasets that combine noise and counterfactual perturbations.
The better-performing informal prompting strategy for a model remains stable for all types of distractions. Still, the decline in performance due to counterfactuals leads to a less consistent preference for a specific prompting style.

\paragraph{\textbf{\emph{F5: The best-performing grammar differs per model and is unstable across data versions.}}}

The evaluation of different logical forms for formal \ac{LLM}-based reasoning in Table~\ref{tab:distraction_k4_logical_form} shows the preference of some models for specific syntactic formats.
Llama 3.1 70B has a considerable improvement of $12\%$ with TPTP syntax on the original set, while Llama 3.1 8B benefits from the R-FOL syntax. However, all grammars show a declining accuracy trend and increased syntax errors for noise perturbations, where the best grammar loses its advantage over the rest. 
When comparing the grammars on the counterfactual partitions, we observe that TPTP is consistently more robust than the standard first-order logic grammar. Here, GPT 4o-mini shows a reduction from $O$ to $O_C$ of $20\%$ for FOL and only $12\%$ for the TPTP grammar. Since this does not correlate with fewer syntax errors, the formalisation in TPTP prevents semantical errors for counterfactual premises. 
A positive reading of these results, especially the minor differences between FOL and R-FOL, is that autoformalisation \acp{LLM} can adapt to the grammar syntax prescribed in the prompt without further loss in performance.

\begin{table}[!t]
\small
\setlength{\modelspacing}{2pt}
\setlength{\tabcolsep}{1.7pt} % Default value: 6pt
\setlength{\belowrulesep}{4pt}
\begin{threeparttable}
    \centering
    \begin{tabular}{cc l r rrr @{\quad} rrrr}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{} & Grammar & \multirow{2}{*}{O} & \multicolumn{3}{c}{Distraction} & \multicolumn{4}{c}{Counterfactual} \\
 & & Syntax & & E& L & T & $\text{O}_C$ & $\text{E}_C$& $\text{L}_C$ & $\text{T}_C$\\
\midrule
\multirow{6}{*}{\rotatebox{90}{Llama-3.1}} & \multirow{3}{*}{\rotatebox{90}{8B}} 
   & FOL & 0.77 & \textbf{0.71} & 0.61 & \textbf{0.53} & 0.58 & \textbf{0.55} & 0.52 & \textbf{0.56} \\
 & & R-FOL & \textbf{0.78} & 0.69 & \textbf{0.62} & \textbf{0.53} & 0.58 & \textbf{0.55} & \textbf{0.54} & 0.52 \\
 & & TPTP & 0.73 & 0.67 & 0.55 & 0.51 & \textbf{0.68} & 0.54 & 0.46 & 0.51 \\[\modelspacing]
\cmidrule{2-11}
 & \multirow{3}{*}{\rotatebox{90}{70B}} 
   & FOL & 0.76 & 0.73 & 0.71 & \textbf{0.72} & 0.67 & 0.57 & 0.63 & 0.56 \\
 & & R-FOL & 0.76 & 0.73 & 0.67 & 0.71 & 0.64 & 0.57 & 0.53 & 0.64 \\
 & & TPTP & \underline{\textbf{0.88}} & \underline{\textbf{0.84}} & \underline{\textbf{0.81}} & \textbf{0.72} & \underline{\textbf{0.81}} & \underline{\textbf{0.68}} & \underline{\textbf{0.67}} & \underline{\textbf{0.68}} \\[\modelspacing]
\midrule
\multirow{3}{*}{\rotatebox{90}{GPT}} & \multirow{3}{*}{\rotatebox{90}{4o-mini}} 
   & FOL & \textbf{0.84} & \textbf{0.82} & \textbf{0.72} & \underline{\textbf{0.78}} & 0.64 & \textbf{0.63} & \textbf{0.61} & 0.51 \\
 & & R-FOL & \textbf{0.84} & 0.77 & 0.70 & \underline{\textbf{0.78}} & \textbf{0.72} & 0.56 & 0.54 & \textbf{0.63} \\
 & & TPTP & 0.83 & \textbf{0.82} & 0.71 & 0.71 & 0.69 & \textbf{0.63} & 0.57 & 0.57 \\
\bottomrule
\end{tabular}
\caption{Accuracies of different formalisation grammars for autoformalisation.}
\label{tab:distraction_k4_logical_form}
\end{threeparttable}
\end{table} 

\paragraph{\textbf{\emph{F6: Feedback does not help \acp{LLM} self-correct to mitigate robustness issues.}}}
\autoref{tab:distraction_k4_feedback} shows the results with different error recovery mechanisms. The results indicate that no feedback strategy emerges as a winner in the different datasets. 
All feedback variants reduce syntax errors for noise perturbations, but given the lack of a consistent increase in accuracy, the corrected formalisations are most likely to contain semantic errors still. 
The type of feedback message only has a minor influence on correcting syntax errors, whereas Llama 3.1 70b and GPT 4o-mini correct slightly more syntax errors with specific error messages. This finding aligns with \cite{huang2023large}, who also found that \acp{LLM} cannot consistently self-correct their reasoning after receiving relevant feedback.

\begin{table}[!ht]
\small
\setlength{\modelspacing}{2pt}
\setlength{\tabcolsep}{1.7pt} % Default value: 6pt
\setlength{\belowrulesep}{4pt}
\begin{threeparttable}
    \centering
    \begin{tabular}{cc l r rrr @{\quad} rrrr}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{Feedback} & \multirow{2}{*}{O} & \multicolumn{3}{c}{Distraction} & \multicolumn{4}{c}{Counterfactual} \\
 & & & & E& L & T & $\text{O}_C$ & $\text{E}_C$& $\text{L}_C$ & $\text{T}_C$\\
\midrule
\multirow{8}{*}{\rotatebox{90}{Llama-3.1}} & \multirow{4}{*}{\rotatebox{90}{8B}} 
   & No recovery & 0.77 & \textbf{0.72} & 0.62 & 0.53 & 0.59 & 0.58 & 0.56 & \textbf{0.56} \\
 & & Error type & \textbf{0.79} & 0.71 & 0.63 & \textbf{0.56} & \textbf{0.66} & 0.54 & 0.52 & 0.51 \\
 & & Error message & 0.78 & 0.71 & \textbf{0.67} & 0.55 & 0.59 & 0.53 & \underline{\textbf{0.64}} & 0.49 \\
 & & Warning & 0.74 & 0.66 & 0.58 & 0.55 & 0.55 & \textbf{0.60} & 0.49 & 0.49 \\[\modelspacing]
\cmidrule{2-11}
 & \multirow{4}{*}{\rotatebox{90}{70B}} 
   & No recovery & \textbf{0.77} & \textbf{0.72} & \textbf{0.73} & 0.71 & \textbf{0.64} & 0.59 & \textbf{0.61} & 0.56 \\
 & & Error type & 0.72 & 0.70 & 0.72 & \textbf{0.73} & 0.62 & 0.56 & 0.60 & 0.58 \\
 & & Error message & 0.71 & 0.70 & \textbf{0.73} & 0.71 & \textbf{0.64} & 0.59 & 0.54 & \underline{\textbf{0.64}} \\
 & & Warning & 0.69 & \textbf{0.72} & 0.72 & 0.72 & 0.62 & \underline{\textbf{0.65}} & \textbf{0.61} & 0.63 \\[\modelspacing]
\midrule
\multirow{4}{*}{\rotatebox{90}{GPT}} & \multirow{4}{*}{\rotatebox{90}{4o-mini}} 
   & No recovery & \underline{\textbf{0.84}} & \underline{\textbf{0.82}} & 0.73 & 0.79 & 0.64 & \textbf{0.62} & 0.56 & \textbf{0.56} \\
 & & Error type & 0.83 & 0.79 & 0.74 & 0.76 & 0.67 & 0.57 & 0.56 & \textbf{0.56} \\
 & & Error message & \underline{\textbf{0.84}} & 0.78 & \underline{\textbf{0.77}} & \underline{\textbf{0.80}} & 0.62 & 0.59 & 0.56 & \textbf{0.56} \\
 & & Warning & \underline{\textbf{0.84}} & 0.75 & 0.73 & 0.76 & \underline{\textbf{0.70}} & 0.61 & \textbf{0.61} & 0.55 \\
 \bottomrule
\end{tabular}
\caption{Accuracies of error recovery strategies.}
\label{tab:distraction_k4_feedback}
\end{threeparttable}
\end{table} 

\subsection{Error Analysis}
\label{subsec:errors}
\paragraph{\textbf{\emph{F7: Autoformalisation increases syntax errors for noise perturbations.}}}
The low performance for noise perturbations correlates with more syntax errors for all models and distraction categories (cf. execution rates in Table~\ref{tab:appendix_k4_formalisation_exec}). The three worst-performing models (both Mistral models, Gemma-2 9b) generate, at best, for $37\%$  and, at worst, for only $4\%$ of the samples, a valid logical form.
Gemma-2 9b and Llama3.1 8b produce more syntax errors than the larger counterparts, suggesting that larger models are more robust towards noise perturbations. 
The accuracy of syntactically valid samples is higher than the informal reasoning methods for most distractions (Table~\ref{tab:appendix_k4_formalisation_vacc}), motivating informal reasoning as a backup strategy for formal reasoning. The error message feedback reveals two common syntax errors: 1) errors by models with an initial low execution rate exhibit issues with the template structure, including using incorrect keywords or adding conversational phrases;
2) perturbation-related errors, the most common of which is using undefined truth constants as part of tautological distractions. 

\paragraph{\textbf{\emph{F8: Autoformalisation increases semantic errors for counterfactuals.}}}
Unlike the introduced noise, counterfactual perturbations do not lead to more syntax errors. The execution rate in Table~\ref{tab:appendix_k4_formalisation_exec} is stable or improves for counterfactuals. However, we see a drop in accuracy for the counterfactual column $\text{O}_C$ in Table~\ref{tab:distraction_k4_formalisation} and can conclude that the number of logical forms with semantic errors has to increase. This suggests that the introduced negation is not correctly formalised. Looking at the warnings generated by the feedback mechanism, for GPT 4o-mini, $161$ warning messages are generated on the unperturbed data. $54$ of these were fixed with a single iteration. Not considering predicates and individuals as part of the context is the most frequent warning across all models. 