\section{Conclusion}
We demonstrated the potential of Private Federated Learning (PFL) in the domain of app selection with its capability to achieve accurate predictive modeling while protecting users' privacy. Our experiments, both in offline simulations and training environments, validate the PFL approach, showing that it can learn and adapt to the distribution shift of user preferences over time, while while guaranteeing privacy through model training on-device rather than collecting data and training at the server. A couple of learnings we would like to highlight: 

\begin{enumerate}
    \item \textbf{Unified Training Data Generation:} To get the most benefit from offline simulations, it is critical to standardize the process of generating training data both offline and on-device. This ensures the data is representative of real-world scenarios to avoid incorrect conclusions from simulations due to data biases.
    \item \textbf{Model Training Approaches:} When a pretrained model is available, opting to fine-tune from an existing checkpoint generally yields better results than starting the training process from scratch. Fine-tuning leverages the learned weights of the pretrained model, allowing for quicker convergence.
    \item \textbf{On-device Plugin Design:} Designing on-device plugins with minimal domain-specific code is beneficial. This approach facilitates easier generalization and adaptation of the plugin across different applications.
    \item \textbf{Pre-Training Data Verification:} Before initiating PFL on device training, it is crucial to verify the on-device training data. This verification process helps identify the amount of training data necessary to complete PFL training iterations with reasonable latency to achieve model convergence.
    \item \textbf{Crucial PFL Hyper-parameters:} Within the realm of PFL, certain hyper-parameters, particularly learning rate and cohort size, are critical, especially when fine-tuning models. Adjusting these parameters appropriately can significantly influence the effectiveness and efficiency of the federated learning process, impacting overall model performance and training speed.
\end{enumerate}


Future work will focus on optimizing the trade-offs between model accuracy and privacy, exploring more efficient ways to manage computational resources, and expanding the applicability of PFL to other areas of predictive modeling.
