\section{Preliminaries} 

\subsection{App selection overview}
The goal of App Selection is to provide third-party applications with equal access to the virtual assistant ecosystem by enabling them to respond to user requests within the domains they support, without the user needing to specify the app by name. This allows for a more seamless and user-friendly experience.

For example, when a user makes a request to play music, App Selection uses the user’s habits to determine the most suitable app. If a particular app is frequently used for similar requests, the model will prioritize launching that app, even if it wasn’t specifically mentioned.

By using Private Federated Learning (PFL), we ensure that no user's app usage data from any individual device is collected, while still improving the overall algorithm across all devices. This process allows us to continuously refine and enhance the model's accuracy by running it on devices, while preserving user privacy. Updated models are then pushed back out to all users for a better overall experience. 

\subsection{App selection model}
\label{sec:model_desc}
Figure 1 shows high-level app selection model architecture:

\begin{enumerate}
    \item \textbf{Entity-type-specific Data Preparation:} This  focuses on preparing and processing data within the model rather than in the client code, allowing for dynamic updates and flexibility in data handling.
    \item \textbf{Cross-entity Feature Engineering:} At the core of the model is the cross-entity Feature Engineering module, utilizing a multi-headed attention mechanism. This module evaluates the interplay between different apps and associated signals to ascertain the most contextually relevant factors for prediction.
    \item \textbf{Epistemic Uncertainty Handling:} This component assesses the model's predictive confidence. It evaluates whether the model is operating within its competent predictive boundaries or merely speculating. If the predictive confidence is low, it suggests that none of the app candidates are suitable, prompting a user interaction for clarification.
    \item \textbf{Aleatoric Uncertainty Handling:} In contrast, this stage manages uncertainty stemming from having multiple similar app candidates. It distinguishes situations where multiple apps appear equally viable, suggesting a user prompt for clarification, from those where one app significantly outweighs others, thus warranting a direct execution.
    \item \textbf{Action Selection:} The final component synthesizes the outcomes of the uncertainty assessments to recommend definitive actions for each app candidate. Depending on the context, this could range from directly launching an app to providing a list for user disambiguation. This process is intricately designed in collaboration with human interface design principles to ensure intuitive user interactions.


\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{figures/App_Selection_Model.png}
\caption{\label{Figure 1: App Selection}Model architecture}
\end{figure}

\subsection{Evaluation metrics}
\label{sec:eval_metrics}
We used two sets of metrics to evaluate the performance of our model. To monitor model training for the Deep Neural Network (DNN), we used mean square error (MSE) loss and used accuracy to measure offline metrics. These offline metrics measure the percentage of interactions in which the direct execution of the model aligns with the user selection.

To assess the model's performance in user-facing scenarios, we introduced correct direct execution rate and disambiguation rate, referred to as our online metrics.
\begin{itemize}
    \item \textbf{Correct Direct Execution Rate (CDER):} This metric represents the percentage of interactions where the model's direct execution correctly corresponds to the user's intended action.
    \item \textbf{Disambiguation Rate:} This metric quantifies the percentage of interactions where the model would naturally present a disambiguation prompt to the user.
\end{itemize}



\subsection{Private federated learning}

Federated learning (FL) \cite{McMahan2017LearningDP} is a distributed machine learning approach that enables multiple participants, often devices or data centers, to train a model while keeping all training data locally. As articulated by \cite{McMahan2017LearningDP}, FL allows devices to contribute in the creation of a shared model by computing updates locally on their own data and then sending these updates to a central server to be averaged. This technique helps to protect user data and reduces the need for data transfer, which can expose data to additional risks during transmission \cite{Konecn2016FederatedOD}. FL is particularly advantageous where data cannot leave its original location due to privacy, security, or logistical constraints, making it a popular choice for industries.

However, FL on its own does not provide privacy because one can learn from the model updates about the training data. Incorporating Differential Privacy (DP) \cite{McMahan2016CommunicationEfficientLO} into FL frameworks improves privacy assurances by adding stochastic noise to model updates, ensuring that the contributions of individual devices are not discernible in the aggregated data received by the central server. \cite{Dwork2006CalibratingNT} introduced differential privacy as a method for providing strong privacy guarantees, defining it in terms of limiting the risk of identification from the output of database queries, irrespective of any auxiliary information that may be available. By integrating differential privacy into federated learning, as demonstrated by \cite{Abadi2016DeepLW} in their implementation of a differentially private stochastic gradient descent algorithm, it becomes significantly more challenging to infer information about any individual participant's data from the shared model. This dual approach of FL and DP is pivotal in scenarios where privacy is paramount, such as in personalized medicine or financial services, but it introduces challenges in balancing privacy, accuracy, and computational efficiency, requiring ongoing optimization and innovation in algorithmic strategies \cite{Geyer2017DifferentiallyPF}.
Differential Privacy (DP) \cite{Dwork2014TheAF} inherently impacts model accuracy by introducing noise to protect individual data points during the learning process. The noise added to either the data itself or during the aggregation of model updates ensures that the contributions of individual participants cannot be discerned, thereby protecting privacy. However, this addition of noise can degrade the model's performance because it obscures the underlying data patterns that the model aims to learn. The degree of noise correlates directly with the level of privacy guaranteed; higher privacy typically results in higher noise levels, which can further reduce accuracy. To mitigate the adverse effects of DP on model accuracy, researchers have developed several strategies. One effective approach is to carefully allocate the privacy budget across multiple iterations, balancing the trade-off between the amount of noise added and the need for accuracy. The way this budget is distributed can significantly impact the model's performance while still ensuring privacy. Techniques such as adaptive clipping and privacy budget allocation can optimize the use of the budget throughout training \cite{Xia2022DifferentiallyPL}, \cite{Hong2021DynamicPB}, concentrating privacy expenditure during the phases when it is most beneficial for learning. Additionally, improving the efficiency of the underlying learning algorithms, such as using more suitable machine learning models and optimization algorithms that are inherently more robust to noise, can also help offset the impact of differential privacy on accuracy.

