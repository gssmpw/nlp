\section{Feasibility Study using Offline PFL simulations}

Before we can implement Private Federated Learning (PFL) on devices, it is essential to begin the process with comprehensive offline simulations. We use Apple's public framework \texttt{pfl-research}\footnote{https://github.com/apple/pfl-research}, which provides a platform for running offline PFL simulations with any available central data, without requiring actual user devices. This simulation environment replicates the behavior of PFL during real-time training with user devices.  Such offline simulations offer critical insights into the potential efficacy of PFL for the intended application, allowing practitioners to assess the balance between privacy protection and utility even before PFL deployments. Moreover, the use of simulations facilitates extensive hyperparameter tuning on a large scale. This capability is essential for determining the optimal settings for both learning and privacy-specific hyperparameters, ensuring that the model performs effectively while adhering to required privacy constraints. This preparatory step is pivotal for ensuring that the adoption of PFL is both practical and aligned with the specific needs and constraints of the application at hand.

The App Selection model described in Section~\ref{sec:model_desc} consists of a Deep Neural Network model, whose weights can be learned or updated using Private Federated Learning~\cite{reddi2020adaptive} . We have evaluated offline PFL simulations on the DNN part of the model in two different ways.:
\begin{itemize}
    \item \textbf{Training from scratch} : In this setting, all weights of the neural network are initialized randomly and then learned as part of the back-propagation step. Thus the model is trained from scratch, which makes this a more complex and time consuming task. It is a relatively higher data and compute hungry setup, but can be used to support breaking changes in the model architecture or adding new features to the modeling task. 
    \item \textbf{Fine-tuning from an existing checkpoint} : In this setting, we start from an already trained model checkpoint (i.e., the weights have been learned previously) and then only a subset of weights of the neural network are unfrozen i.e., updated as part of back-propagation step, while the remaining weights are frozen. This setting is particularly useful when we already have a fixed architecture, fixed set of features \& a solid baseline model to which we will simply add data to continuously update the model to adapt to distribution shift (need citation).
\end{itemize}


Unless otherwise mentioned, we have used AdamW~\cite{loshchilov2017decoupled} as the server side optimizer and SGD as the client side optimizer for PFL simulations using \texttt{pfl-research}. 
The PFL modelâ€™s performance is evaluated using the accuracy metric defined in Section~\ref{sec:eval_metrics} computed on the fixed validation set.
Note that the training and evaluation data required for each of the two above setups are different, and hence are elaborated in the subsequent sections.

\subsection{Training from scratch}
\label{sec:scratch_train}

In this setup, we want to simulate the PFL-based training from scratch with Differential Privacy (DP) enabled. 
We start the training with a randomly initialized set of model parameters and train the entire model from scratch using a relatively large offline data set consisting of $\sim$788K data points. 
As the baseline, we have trained the same architecture with the entire training data from scratch without any PFL, which we refer to as \textit{Cymba} for simplicity and ease of reading.
We have a dedicated validation set consisting of $\sim$178K data points, which we use to compare the performance of the PFL trained model against the non-PFL baseline (i.e., Cymba).

Unless otherwise specified, we have used Gaussian Moments Accountant (Needs citation) implemented in \texttt{pfl-research} as the central privacy mechanism, with parameters as : $\epsilon = 2.0$, $\delta = 1e-6$, and $\text{Clipping Bound} = 0.1$. 
Additionally, for simplicity of offline simulation, we have fixed the \textit{mean data points per user} $= 1$ and \textit{local epochs} $= 3$, after some initial hyper-parameter exploration.
In all our experiments, we have set a higher Local Learning rate (LLR) for the on-device SGD step due to sparsity of training data per device, and a lower Central Learning rate (CLR) for the server side Adam optimizer step. This configuration has generally helped us achieve a good distributed training set up using PFL, while reducing the risk of training divergence due to sub-optimal hyper-parameter choice.

We design the offline simulations in such a way that the insights from these simulations can help plan the duration of on device training too.
Note that for the app selection model, there are two very important parameters that determine the overall time and resources needed for on device training viz. the \textit{number of devices per Central iteration} and the \textit{number of Central iterations}. 
We chose two sets of values to represent the High and Low resource Budget settings to determine the performance bounds of the PFL training :
\begin{itemize}
    \item \textbf{High Resource Budget} : In this setting, we choose 10K devices per central iteration, and 500 number of central iterations. Thus we take approximately $(10\text{K} * 500)/788\text{K} = 6.3$ passes on the entire training data, which is smaller than the total number of epochs used to train the non-PFL Cymba baseline.
    \item \textbf{Low Resource Budget} : In this setting, we choose 1K devices per central iteration, and number of central iterations = 500. Thus we take approximately $(1\text{K} * 500)/788{\text{K}} = 0.63$ passes on the entire training data, which is smaller than the total number of epochs used to train the non-PFL Cymba baseline.
\end{itemize}
We observe that in the high resource budget setting, the PFL trained model is almost similar in performance compared to Cymba, with a very minor regression that is acceptable, given that Differential Privacy will have a negative impact on the model's learning capacity.
As expected, the low resource budget setting appears to have significantly more regression compared to Cymba, likely because of the significantly smaller number of passes on the training data, coupled with the impact of Differential Privacy. 
Note that in this setting, it is difficult to achieve an improvement over an existing non-PFL trained baseline using the exact same training data.
However, given the PFL trained model's performance under the High Resource Budget setting, we consider that we may be successful in training a PFL model from scratch during on device training, which may have a competitive performance (i.e., within acceptable limits of performance regression) compared to the non-PFL trained baseline.

\begin{table}
\centering
\begin{tabular}{l|l|l|r}
Model & CLR & LLR & Accuracy\\\hline
Cymba & - & - & 0.856\\
PFL with Low Resource Budget & 0.001 & 0.01 & 0.83 \\
PFL with High Resource Budget & 0.0005 & 0.01 & 0.852
\end{tabular}
\caption{Performance of Training from Scratch with v/s without PFL}
\label{tab:scratch_train} 
\end{table}


Additionally, as part of offline simulations, we did multiple rounds of hyper-parameter tuning by varying the number of devices per central iteration, number of Central iterations, Central learning rate, Local number of epochs, Local learning rate and Privacy Clipping Bound before selecting the above configuration.
Some key observations are presented below:
\begin{itemize}
    \item We fixed the Local Learning Rate to a relatively higher value (0.01) as it is a bit difficult to modify/control this parameter during on device training. However, we varied the Central Learning Rate and observed that a value between [0.1, 0.9] often causes the training to diverge, while a value of 0.0005 tend to yield consistently good results on the offline data. Additionally, we tested various learning rate scheduling strategies (like Polynomial, Cyclic etc.) for the Central Learning rate, but did not observe any major gains over a fixed Central Learning rate.
    \item We noticed that the Local Number of Epochs  $\leq3$ tends to give better results. Increasing the number of epochs any further causes training divergence, even with small values of Local Learning Rate.
    \item We varied the number of Central Iterations from 500 to 25,000 but the benefits in terms of gains gradually decreases. Hence we fix the number of Central iterations to 500 (which is much smaller than 2K), as it will reduce the on device training time significantly when compared to the time taken for 2K iterations, given that the difference in performance is very small.
    \item Assuming one data sample per device, we varied the number of Devices between 1K and 150K. The general observation is that 5K to 10K devices tended to yield good results. If we choose 5K for on device training, then it will speed up the training time as well as reduce the network communication load with the backend PFL server.
\end{itemize}




\subsection{Fine-tuning from an existing checkpoint}
\label{sec:finetune_chkp}

In this setup, we start from an already trained model checkpoint (i.e., the weights have been learned previously using a different data set) and then train a subset of weights of the neural network using random sampled data. 
In this case, Cymba is the base model from which we start the training, and use a random sampled batch of $\sim$814K data points as training data, and another random sampled batch of $\sim$176K data points as Validation set, for performance comparison.
We test two variations to establish the performance bounds: a)~training all layers of the pre-existing model and b)~training only the top layer of the pre-existing model, while freezing the remaining layers.
We explore these 2 variants with the goal to reduce the number of PFL trained parameters in the model, as it is easier to train a model with fewer parameters since it reduces the on-device training complexity, lowers the network communication cost between the device and server as well as helps with Privacy.

The Accuracy metric of the PFL fine-tuned models are presented in the Y-axis in Figure~\ref{fig:fine_tune_cymba}, while the X-axis refers to the number of PFL Central Iterations for each of the different training config.
The performance of the Cymba model on this random sampled Validation set is also reported for tracking purposes.
Unless otherwise specified, we have used Gaussian Moments Accountant (Needs citation) implemented in PFL-Research(needs citation) as the central privacy mechanism, with parameters as : $Epsilon = 2.0$, $Delta = 1e-6$ and $Clipping Bound = 0.1$. 
For fine-tuning from existing checkpoint, we have set the \textit{mean data points per user} $= 1$, \textit{Local Number of Epochs} $= 1$, \textit{number of devices per Central iteration} $= 5000$ and the \textit{number of Central iterations} $= 500$. 
For fair comparison, we have trained a dedicated model from scratch (labeled as \textit{Train from scratch} in Figure~\ref{fig:fine_tune_cymba}) using the random sampled $\sim$814K training data points, while reducing the \textit{number of devices per Central iteration} $= 5000$.
We have done a hyper-parameter search for the learning rate and chose the best configuration for each setting when reporting the above metrics.


\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/PFL_Paper_Figure_2.png}
\caption{Fine-tuning Cymba from an existing checkpoint}
\label{fig:fine_tune_cymba}
\end{figure}


In Figure~\ref{fig:fine_tune_cymba}, we observe that fine-tuning with freshly random sampled training data improves the performance of the PFL trained models on Validation set, compared to the static Cymba baseline. 
This indicates that the app selection model can adjust to the user's behavioral shift over time (as represented within the random sampled Training and Validation sets) using PFL, which is very useful for continuous model maintenance/upgrade activity.
Note that fine-tuning only the top layer of Cymba appears to be performing better than fine-tuning all layers,which reduces the amount of communication bandwidth spent to transfer the PFL weights from device to the servers, thereby making PFL less network bandwidth-intensive operation. Also fewer learnable parameters is better for DP thereby making this training paradigm a more suitable one for this use-case.
Finally, using PFL to train a model from scratch using random sampled data still under-performs all the settings, but the gap in performance is smaller if hyper-parameter tuning can be done appropriately.



\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/PFL_Paper_Figure_3.png}
\caption{Using simulations to predict the performance of the fine-tuning of Top Layers of the Cymba model as a function of Number of devices (cohort size) and corresponding total number of data points.}
\label{fig:data_req_for_tuning}
\end{figure}

In Figure~\ref{fig:data_req_for_tuning}, we plot the performance of fine-tuning the Top Layers of the Cymba model by varying the \textit{number of devices per Central iteration} as (1K, 2K, 5K). Assuming \textit{mean data points per user = 1}, in the X-axis we plot the number of data points that have been used for fine-tuning the top layers of Cymba for the corresponding Cohort size. The Y-axis shows the relative difference in Accuracy of the corresponding model checkpoint compared to the performance of the Cymba model on a fixed Validation set. This plot gives us an approximate insight into the amount of data points required to obtain a certain percentage of relative accuracy improvement, which will help us to select the corresponding parameters and the duration of the on device training. We observed that with a Cohort size of 2K while we achieve an improvement with fewer data points, model performance plateaus quickly, even though the performance can be further improved with more data points using a Cohort size of 5K. For example, with 504K data points, we can achieve close to 2\% relative improvement in Accuracy with a Cohort size of 5K, while the relative improvement is lesser with a Cohort size of 2K. This shows the importance of selecting the Cohort size appropriately in conjunction with the duration of the on device training (i.e., total number of data points to use for fine-tuning) to achieve best improvement.


%\subsection{Ablation study}
%\textbf{Training data retention period estimation experiments}

