Throughout our work, we use \emph{arms} to represent meta-prompts, and use \emph{actions} to denote the actions selected by an LLM-based agent.

Consider an algorithm which uses an LLM to perform a sequential decision-making task by sequentially instructing the LLM to select an action in every iteration.
A representative example of such algorithms is the \emph{Optimization by PROmpting} (OPRO) algorithm from \citet{yang2023large}. OPRO aims to solve an optimization problem, i.e., to find $x^* = {\arg\min}_{x}f(x)$. To achieve this, in every iteration $t$, OPRO uses an LLM to select a batch of $B$ input queries $\{x_{t,1},\ldots,x_{t,B}\}$, after which their corresponding scores $\{s_{t,1},\ldots,s_{t,B}\}$ are observed. When instructing the LLM to select the input queries, the meta-prompt $\mathcal{Q}$ given to the LLM contains a number of important components, including \emph{a fixed task description} $\mathcal{D}$, \emph{a fixed meta-instruction} $\mathcal{I}$, and \emph{a sequence of exemplars} $\mathcal{E}_t$ corresponding to a subset of the observations (i.e., pairs of input queries and observed scores) collected so far.
The same paradigm of LLM-based sequential decision-making has also been adopted by other works, such as the LLM-based MAB algorithm from \citet{krishnamurthy2024can} (more details in Sec.~\ref{subsec:exp:bandits}).

In this work, our first algorithm, \alg~(Sec.~\ref{subsec:expo}), 
dynamically optimize the task description $\mathcal{D}$ and meta-instruction $\mathcal{I}$ (i.e., selects a new $\mathcal{D}_t$ and $\mathcal{I}_t$ in every iteration $t$), in order to improve the efficiency and effectiveness of optimization.
We also extend our \alg~to derive the \alges~algorithm (Sec.~\ref{subsec:expo:es}), which additionally optimizes the sequence of exemplars $\mathcal{E}_t$ to further improve the optimization performance.
\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/EXPO.pdf} \\
\vspace{-5mm}
 \caption{
 Illustration of our \alg~algorithm. 
 We use {\color{violet}purple} to denote the task description and {\color{blue}blue} to represent the meta-instruction.
 }
 \label{fig:expo:algo}
\vspace{-2.5mm}
\end{figure*}
We use $g(\cdot)$ to denote a pre-trained embedding function, which maps some input text to its corresponding continuous representation.
We separately obtain the embeddings of the task description $g(\mathcal{D}_t)$, the meta-instruction $g(\mathcal{I}_t)$ and the exemplar sequence $g(\mathcal{E}_t)$. 
Based on the embeddings, in every iteration, we use the current history of selected meta-prompts and their scores to train a neural network (NN), which can then be used to predict the scores of every meta-prompts in the domain.
We denote this NN as $\mathcal{M}(g(\cdot); \theta)$, in which $\theta$ represents the NN parameters.

\textbf{Adversarial Bandits.}
In adversarial bandits, the goal is to compete against the best arm \emph{in hindsight} \cite{lattimore2020bandit}. 
Consider an MAB problem with $k$ arms (i.e., meta-prompts).
For each arm $i=1,\ldots,k$, denote its corresponding sequence of rewards (i.e., scores) in $T$ iterations as $\{r_{t,i}\}_{t=1,\ldots,T}$.
The best arm in hindsight is then defined as $i^*={\arg\max}_{i=1,\ldots,k}\sum^T_{t=1} r_{t,i}$.
Then, the goal of an adversarial bandit algorithm (which selects arm $A_t$ in iteration $t$) is to minimize the following definition of regret: $R_T=\sum^T_{t=1} r_{t,i^*} - \sum^T_{t=1} r_{t,A_t}$.

\textbf{Adversarial Bandits for LLM-Based Agents.}
LLM-based sequential decision-making methods often aim to maximize either \emph{(a)} the cumulative rewards 
(e.g., the LLM-based MAB algorithm from \citet{krishnamurthy2024can}) 
or \emph{(b)} the final reward (e.g., OPRO from \citet{yang2023large}).
In the former case of cumulative reward maximization, 
the overall rewards/scores for the best arm $i^*$ are higher than the other arms.
In the latter case, we implicitly assume that
the arm with the largest final reward after $T$ iterations also has large rewards across all iterations in general.
As a result, in both cases, \emph{the observed rewards of an arm (i.e., the observed scores of a meta-prompt) in every iteration are indicative of the quality of the arm (i.e., the meta-prompt)}.
So, when training the NN $\mathcal{M}(g(\cdot); \theta)$ (for score prediction) using the history of the selected meta-prompts and their observed scores, we simply use the scores (i.e., rewards) as the labels in the training set.
This simple design helps our algorithms achieve strong performance in our experiments (Sec.~\ref{sec:experiments}).

