\onecolumn

\section{Our \alges~Algorithm to Additionally Optimize the Exemplar Sequences}
\label{app:expo:es}

\begin{algorithm}[H]
\begin{algorithmic}[1]
    \INPUT Initial task description $\mathcal{D}_0$, initial meta-instruction $\mathcal{I}_0$.\\
    Maximum number \(\mathcal{L}\) of exemplars in the meta-prompt, the number \(k^{\text{ES}}\) of exemplar sequences in the domain.
    \STATE Initialize the exemplar set $\mathcal{E}_0 = \emptyset$, and the subset $\mathcal{E}_0' = \emptyset$, meta-prompt-score set $\mathcal{S}_0 = \emptyset$, and cumulative score estimates $\hat{s}_j^{(0)}$ for all $j \in \{1, \ldots, k^{\text{ES}}\}$.
    \\
    Initialize the history of NN parameters \(\Theta_{\text{history}} = \emptyset\), 
    and the exemplar training set \(\mathcal{T}_0 = \emptyset\).
    \FOR{iteration \(t = 0, 1, \ldots, T-1\)}
        \STATE \textbf{Lines 3-11 of Algo.~\ref{algo:EXPO}}.
        \STATE Compute the embedding \(g(\mathcal{E}_t')\) of the selected exemplar sequence \(\mathcal{E}_t'\), and add \(g(\mathcal{E}_t')\) and its score \(s_t\) to the exemplar training set:  $\mathcal{T}_{t+1} \gets \mathcal{T}_t \cup \{(g(\mathcal{E}_t'), s_t)\}$.
        \STATE Update the parameters $\theta^{\text{ES}}$ of the NN $\mathcal{M}_{\text{ES}}(g(\cdot); \theta^{\text{ES}})$ by using the updated $\mathcal{T}_{t+1}$ as the training set to minimize the MSE loss, yielding $\theta^{\text{ES}}_{t+1}$.
        \STATE Add the updated parameters to the history: $\Theta_{\text{history}} \gets \Theta_{\text{history}} \cup \{\theta^{\text{ES}}_{t+1}\}$.
        \IF{\(|\mathcal{E}_{t+1}| > \mathcal{L}\)}
            \STATE Randomly generate \(k^{\text{ES}}\) sequences of $\mathcal{L}$ exemplars from the exemplar set \(\mathcal{E}_{t+1}\): \(\{\mathcal{E}^{1}_{t+1}, \mathcal{E}^{2}_{t+1}, \ldots, \mathcal{E}^{k^{\text{ES}}}_{t+1}\}\), in which every \(\mathcal{E}^{j}_{t+1}\) represents an ordered set of \(\mathcal{L}\) exemplars from \(\mathcal{E}_{t+1}\).

            \STATE Initialize cumulative score estimates \(\hat{s}_{j}^{(0)} = 0\) for all \(j \in \{1, \ldots, k^{\text{ES}}\}\). \qquad\qquad\qquad\qquad \tikzmark{right}\tikzmark{top}
            \FOR{each \(\mathcal{E}^{j}_{t+1}\) in \(\{\mathcal{E}^1_{t+1}, \ldots, \mathcal{E}^{k^{\text{ES}}}_{t+1}\}\)}
                \STATE Initialize cumulative score \(\hat{s}_{j}^{(0)} = 0\).
                \FOR{each historical model parameter \(\theta^{\text{ES}}_{i} \in \Theta_{\text{history}}\)}
                    \STATE Update the cumulative score for \(\mathcal{E}^j_{t+1}\): $\hat{s}_{j}^{(i)} = \hat{s}_{j}^{(i-1)} + \mathcal{M}_{\text{ES}}(g(\mathcal{E}^j_{t+1}); \theta^{\text{ES}}_{i})$.
                    % \[
                    % \]
                \ENDFOR
            \ENDFOR
            \STATE Compute the final cumulative score estimates: $\hat{s}_{j}^{(\text{final})} = \hat{s}_{j}^{(|\Theta_{\text{history}}|)}, \quad \forall j \in \{1, \ldots, k^{\text{ES}}\}$. \tikzmark{bottom}
            % }

            \STATE Compute the sampling distribution \(P_t^{\text{ES}}\) over the $k$ exemplar sequences:
            \[
            P_t^{\text{ES}}[j] = \frac{\exp(\eta \hat{s}_{j}^{(\text{final})})}{\sum_{l=1}^{k^{\text{ES}}} \exp(\eta \hat{s}_{l}^{(\text{final})})}, \quad \forall j \in \{1, \ldots, k^{\text{ES}}\}. \tag{5}
            \]
            \STATE Sample an exemplar sequence \(\mathcal{E}'_{t+1} \sim P_t^{\text{ES}}\).
        \ENDIF
    \ENDFOR
\end{algorithmic}
\AddNote{top}{bottom}{right}{\small cumulative score \\
estimates}
\caption{\alges}
\label{algo:ES-subset}
\end{algorithm}



Our complete \alges~algorithm is described in Algo.~\ref{algo:ES-subset}.
As we have discussed in Sec.~\ref{subsec:expo:es}, there are two major differences compared to the way in which our \alg~algorithm optimizes the task description and meta-instruction (Algo.~\ref{algo:EXPO}).
Firstly, our domain of $k^{\text{ES}}$ arms (i.e., every arm corresponds to a randomly sampled exemplar sequence) changes in every iteration (line 8).
Secondly, as a result of the time-varying domains, we need to save a copy of the parameters of the NN trained in every iteration in order to compute the cumulative score estimates (lines 9-14).

\paragraph{Simplified Variant of Our \alges~Algorithm.}
When applying our \alges~algorithm to the LLM-based MAB algorithm in \citet{krishnamurthy2024can} (Sec.~\ref{subsec:exp:bandits}), we have adopted a simplified variant of our \alges.
This is because in the problem setting from \citet{krishnamurthy2024can}, the number of arms is small. Therefore, instead of including a subset of the history of exemplars in the prompt, their algorithm has instead included a summarized observation history.
An example of such summarized observation history with 5 arms (represented by 5 buttons with different colors) is given in Fig.~\ref{fig:example:summarized:history} below.
Therefore, here we aim to optimize the format of the summarized observation history.
Specifically, we \emph{optimize the order of the arms} in the summarized history, and our domain of arms consist of \emph{all cyclically shifted variants of the following sequence of buttons}: $\{\text{blue button}, \text{green button}, \text{red button}, \text{yellow button}, \text{purple button}\}$.
For example, some other arms (button sequences) in our domain include: $\{\text{green button}, \text{red button}, \text{yellow button}, \text{purple button}, \text{blue button}\}$ and $\{\text{red button}, \text{yellow button}, \text{purple button}, \text{blue button}, \text{green button}\}$.
As a result, unlike our original \alges~algorithm described in Sec.~\ref{subsec:expo:es}, here we do not suffer from the issue of \emph{time-varying domain of arms}.
\begin{figure}[h]
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
\footnotesize  % adjust text size as required
...\\\\
blue button: pressed 2 times with average reward 0.5 \\
green button: pressed 1 times with average reward 0.0 \\
red button: pressed 1 times with average reward 1.0 \\
yellow button: pressed 0 times \\
purple button: pressed 1 times with average reward 0.0 \\\\
...
\end{mdframed}
\caption{
An example of the summarized observation history used by the LLM-based MAB algorithm from \citet{krishnamurthy2024can}.
}
\label{fig:example:summarized:history}
\end{figure}

Therefore, when applying our \alges~algorithm to improve the LLM-based MAB method from \citet{krishnamurthy2024can} (Sec.~\ref{subsec:exp:bandits}), we make two modifications to our standard \alges~algorithm described in Algo.~\ref{algo:ES-subset}.
Firstly, instead of randomly sampling $k^{\text{ES}}$ exemplar sequences to form our domain of exemplar sequences, here our domain remains fixed across different iterations, i.e., all cyclically shifted variants of the arms.
Secondly, since here we do not suffer from the issue of time-varying domain of arms (i.e., exemplar sequences), we can resort to the incremental update of the cumulative reward estimates adopted by our \alg~algorithm (line 9 of Algo.~\ref{algo:EXPO}). As a result, we do not need to save a copy of the parameters of the NN trained in every iteration.





\section{More Details on Our Experimental Settings}

\subsection{More Details on the Generation of the Domain of Task Description and Meta-Instruction}
\label{app:subsec:detail:domain:generation}
Here we describe the details about how we generate the domain of task descriptions and meta-instructions.
Below we provide the prompt we have used to instruct the LLM to generate every prompt in the domain.


% \begin{figure}[h]
\begin{mycolorbox}{Query}{Example Query: Meta-Prompt Instruction Rephrasing Template}
\small
To achieve a more effective TASK description and INSTRUCTION and convey its core essence more clearly, please enhance the content in the quote by rephrasing and changing some information: "\{INITIAL\_META-PROMPT\}" \\ 
Please return directly the modified description without additional description. \\
The modified description: \\
\end{mycolorbox}

\textbf{Generation of the Domain.}
To effectively generate task-specific prompts, we utilized an initial prompt to guide the LLM in creating diverse task descriptions and meta-instructions. For each task, the LLM was prompted 100 times to rephrase the task description and meta-instruction separately, resulting in 100 unique rephrased prompts for each. Combined with the initial prompt, this process produced a total of \(101 \times 101\) combinations of task descriptions and meta-instructions for each task. 

To optimize computational efficiency, we pre-compute the embeddings of all task descriptions and meta-instructions in the domain using the embedding model $g(\cdot)$ and store the results to prevent redundant calculations during subsequent experiments. 

For the rephrasing process, we employed 
% the \texttt{gpt-4} model 
the GPT-4 model
with a temperature setting of \(1.3\), ensuring diverse and high-quality rephrased prompts for both task descriptions and meta-instructions.



% \subsection{More Details on the Tasks in Sec.~\ref{sec:experiments}}
\subsection{More Details on OPRO for the Linear Regression and Traveling Salesman Problem (Sec.~\ref{subsec:exp:opro})}

\subsubsection{Task Setting.}
\textbf{Linear Regression.} We conduct experiments on Linear Regression by selecting two challenging ground truth weight-bias (\(w, b\)) pairs. The experiments follow the OPRO framework, which requires warm-starting the LLM with initial exemplars. Using a fixed random seed, we first generate 50 random data points uniformly distributed within the range \([-1, 1]\), which perfectly satisfy the ground truth \(w_{\text{true}}, b_{\text{true}}\) pairs, ensuring that these data points can serve as the foundation for evaluating the LLM’s ability to model the relationships. Additionally, 5 \(w, b\) pairs with corresponding scores, sampled within the range \([10, 20]\), are generated using another fixed random seed to serve as the initial exemplars. At each iteration, the LLM is prompted 8 times (consisting of 1 inference with a temperature setting of $T=0$ and 7 inferences with a temperature setting of $T=1$) using the current exemplars, and the prompt is updated based on the generated outputs. The exemplars are dynamically updated to include the top 20 \(w, b\) pairs and their associated scores from all historical records across iterations, ensuring the LLM is always guided by the best-performing examples. The total number of iterations is set to 50, and each ground truth configuration is repeated 5 times for consistency.


\textbf{Traveling Salesman Problem (TSP).} For the TSP task, experiments are conducted on three problem sizes defined by the number of nodes: 10, 15, and 20. 
For each TSP instance, the problem is defined by randomly generating $n=10,15,20$ nodes, where the $x$ and $y$ coordinates of each node are sampled uniformly from the range $[-100, 100]$.
For each configuration, a specific TSP instance is generated using a fixed random seed, and a single random seed is used to generate warm-start exemplars to initialize the LLM prompts. 
To initialize the optimization process, we randomly sample 5 different TSP routes along with their corresponding total distances. These routes and their lengths are used as the initial exemplars for the LLM.
Each iteration consists of 8 prompt calls to the LLM, followed by an update of the exemplars based on the generated results. 
More specifically, during each iteration, the GPT-3.5-turbo is prompted 8 times using the same prompt, consisting of 1 inference with a temperature setting of $T=0$ to ensure stability and 7 inferences with a temperature setting of $T=1$ to encourage exploration. 
Similar to the Linear Regression task, the exemplars for TSP are updated to include the top 20 historical solutions with the best scores, ensuring the prompt leverages the most effective examples. The number of iterations is set to 100, 200, and 300 for 10-node, 15-node, and 20-node TSP problems, respectively, to account for the increasing complexity of the tasks. Each node configuration is repeated 3 times to ensure consistency and reliability.

\subsubsection{Evaluation Metrics.}
\textbf{Linear Regression.}
In the Linear Regression task, the performance of the algorithms is evaluated using the Mean Squared Error (MSE) metric. Given a set of $N$ one-dimensional input data points \(\mathbf{x} \in \mathbb{R}\) and their corresponding ground truth labels \(\mathbf{y} \in \mathbb{R}\), the MSE is computed as:
\[
\text{MSE} = \frac{1}{N} \left\lVert \mathbf{y} - (w \cdot \mathbf{x} + b) \right\rVert^2,
\]
where \(w \in \mathbb{R}\) and \(b \in \mathbb{R}\) are the weight and bias parameters inferred by the LLM, and \(N\) is the total number of data points.

\textbf{Traveling Salesman Problem (TSP).}
For the TSP task, the performance of the LLM-generated solutions is evaluated based on the total Euclidean distance of the TSP tour. Given a set of two-dimensional points \(\{(x_i, y_i)\}_{i=1}^N\), where \(N\) is the total number of nodes, the length of a proposed TSP tour \(\mathcal{P} = [\pi(1), \pi(2), \dots, \pi(N), \pi(1)]\) is computed as:
\[
\text{Length} = \sum_{i=1}^{N} \sqrt{\left( x_{\pi(i+1)} - x_{\pi(i)} \right)^2 + \left( y_{\pi(i+1)} - y_{\pi(i)} \right)^2},
\]
where \(\pi\) represents the permutation of nodes in the proposed tour, and \(\pi(N+1) = \pi(1)\) ensures the tour returns to the starting node.

To evaluate the convergence and effectiveness of the agents, we use the \textit{Optimality Gap} metric, which quantifies the deviation of the solver's best-found solution from the true optimal solution. It is defined as:
\[
\text{Optimality Gap} = \frac{\text{SolverOptimal} - \text{Optima}}{\text{Optima}} \times 100\%,
\]
where:
\begin{itemize}
    \item \(\text{SolverOptimal}\) denotes the shortest tour length found by the solver up to the current iteration.
    \item \(\text{Optima}\) is the length of the known optimal TSP tour.
\end{itemize}


\subsubsection{Design of Prompt Score.}
In both the Linear Regression and TSP tasks, optimal solutions are characterized by lower evaluation scores. To align with the requirements of the algorithm and ensure more stable learning, we define the \textit{Prompt Score} using the formula:
\[
\text{Prompt Score} = \frac{-\text{Evaluation Score} + b}{b},
\]
where \(b > 0\) is a stabilizing constant. This formulation ensures that lower evaluation scores correspond to higher prompt scores, which better facilitates the optimization process and contributes to steady algorithmic learning.

For the Linear Regression task, the \textit{Evaluation Score} is defined as the Mean Squared Error (MSE) of the weight-bias (\(w, b\)) pairs proposed by the algorithm at each iteration under a Temperature=0 stable inference. The MSE is computed based on the provided one-dimensional data points.

For the TSP task, the \textit{Evaluation Score} corresponds to the total Euclidean distance (\textit{Length}) of the TSP tour proposed by the algorithm at each iteration, also under a Temperature=0 stable inference.


\subsubsection{Details about the Models and Parameters in Our Algorithms}

\textbf{LLM Agents and Embedding Model.}
In our experiments, the primary LLM agent used is GPT-3.5-Turbo.
For embedding generation, we utilized OpenAI's \texttt{text-embedding-3-large} model, which outputs embeddings of dimensionality 3072. These embeddings were used to represent both the task description and meta-instruction in the \alg~ framework. The embeddings were also employed to represent the exemplars in the \alges~ framework. During each iteration of inference, the LLM agent performed 1 prediction with a temperature setting of \(T=0\) to provide a stable solution and 7 additional predictions with a temperature setting of \(T=1\) to encourage exploration.

\textbf{Neural Network Parameters.}
For the \alg, the input to the neural network consists of the concatenated embeddings of the task description and meta-instruction, resulting in an input dimensionality of \(3072 + 3072 = 6144\). The neural network employs a single hidden layer with a width of 1536 and produces a single scalar output. The training objective is to minimize the Mean Squared Error (MSE) loss function.

For the \alges, the exemplar selection process differs depending on the iteration count. During the initial iterations, when fewer than 20 optimal historical records are available, we use all available exemplars. As the iteration count increases, exemplars are selected from the top \(\min(\text{total exemplar records}, 30)\) historical optimal records. From this pool, 257 exemplars are constructed, consisting of 256 randomly selected exemplars and 1 heuristic exemplar
% , which was 
generated from a combination of 20 best historical records. The neural network for \alges~ operates on an input dimensionality of 3072, corresponding to the embedding of a single exemplar. It employs a single hidden layer with a width of 512 and produces a single scalar output. The training objective is to minimize the Mean Squared Error (MSE) loss.

\textbf{EXP3 Learning Rate.}
In the \alg~, the learning rate parameter \(\eta_{\text{desc}}\) is set to \(100\) for selecting task descriptions and meta-instruction combinations. In the \alges, \(\eta_{\text{desc}}\) is also set to \(100\) for selecting task descriptions and meta-instruction combinations, while \(\eta_{\text{exemplar}}\) is set to \(10\) for selecting exemplars.



\subsubsection{Enhanced OPRO}
\label{app:sec:enhanced:opro}
Here, we describe how we have enhanced the original algorithm \cite{yang2023large} by modifying its prompts.

During initial experiments with the meta-prompts provided by the original OPRO algorithm \cite{yang2023large} for task description rephrasing, we observed that the LLM often misinterprets the \emph{descending order} semantics described in the original design. 
In tasks like TSP and Linear Regression, where better solutions correspond to lower evaluation scores, \emph{descending order} is intended to arrange solutions from high evaluation scores to low. However, the LLM frequently misunderstands this as a \emph{descending order of solution quality}, interpreting higher-ranked solutions as better and lower-ranked ones as worse, which is contrary to the intended meaning.

To address this issue, we enhance the orginal meta-prompts by explicitly clarifying the semantics of descending order in the context of evaluation scores. This modification ensures that the LLM accurately understand the intended instructions. When tested with the enhanced prompts, the problem was resolved, and the LLM is able to consistently generate correct rephrased task descriptions.
For a clearer illustration, we provide below the original OPRO meta-prompt (Fig.~\ref{fig:original:opro:prompt}) and our enhanced OPRO meta-prompt (Fig.~\ref{fig:enhanced:opro:prompt}).


\begin{figure}[H]
\begin{mycolorbox}{Query}{The task description in the original OPRO prompt}
\small
You are given a list of points with coordinates below: \{POINTS\}.\\
Below are some previous traces and their lengths. 
The traces are arranged in descending order based on their lengths, 
where lower values are better. \\
......
\end{mycolorbox}
\caption{The task description in the original OPRO prompt.}
\label{fig:original:opro:prompt}
\end{figure}



\begin{figure}[H]
\begin{mycolorbox}{Query}{The task description in our enhanced OPRO prompt}
\small
You are given a list of points with coordinates below: \{POINTS\}.\\
Below are some previous traces and their lengths. The traces are arranged in descending order based on their lengths, where {\color{red}smaller lengths indicate better solutions}. {\color{red}Therefore, the traces are listed from the largest length to the smallest, the trace with the smallest length is considered the most optimal}. \\
......
\end{mycolorbox}
\caption{The task description in the enhanced OPRO prompt. The texts we have modified are highlighted in {\color{red}red}.}
\label{fig:enhanced:opro:prompt}
\end{figure}



\subsection{More Details on the LLM-Based Multi-Armed Bandits Task (Sec.~\ref{subsec:exp:bandits})}

\subsubsection{Explanation of BSSCD and BSSND}

\label{subsec:exp:app:more:details:bandit}

We provide a detailed explanation and demonstration of prompt designs for both BSSCD and BSSND \cite{krishnamurthy2024can}, highlighting their key components and structures. Figure~\ref{fig:wholeprompt4BSSCD} illustrates a complete example of a BSSCD prompt designed for the MAB problem under the hard difficulty setting. It showcases the structure and color-coded components of the prompt in detail.

\begin{figure}[h]
\begin{mycolorbox}{Query}{The setting of the prompt in MAB}
{[SYSTEM]
\vspace{-2mm}
\begin{tcolorbox}[colback=violet!15, colframe=violet!15, width=17.05cm, enlarge left by=-5mm, top=0.5mm, bottom=0.5mm, boxsep=0mm, arc=0mm, outer arc=0mm, left=4.5mm, right=4.5mm]
{\color{orange}You are a bandit algorithm in a room with 5 buttons labeled blue, green, red, yellow, purple.} 
{\color{teal}Each button is associated with a Bernoulli distribution with a fixed but unknown mean; the means for the buttons could be different. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution. You have 100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize the total reward over the 100 time steps.}
\end{tcolorbox}
\vspace{-5mm}
\begin{tcolorbox}[colback=blue!15, colframe=blue!15, width=17.05cm, enlarge left by=-5mm, top=0.5mm, bottom=0.5mm, boxsep=0mm, arc=0mm, outer arc=0mm, left=4.5mm, right=4.5mm]
{\color{brown}At each time step, I will show you a summary of your past choices and rewards.} Then you must make the next choice. {\color{red}You may output a distribution over the 5 buttons formatted EXACTLY like "blue:a,green:b,red:c,yellow:d,purple:e".} {\color{yellow}{Let’s think step by step to make sure we make a good choice.}}
\end{tcolorbox}
\vspace{-2mm}
You must provide your final answer within the tags\ {\color{red}\textless Answer\textgreater DIST\textless\textbackslash Answer\textgreater\ where DIST is the distribution in the format specified above.}} \\
\noindent\rule[0.5ex]{\textwidth}{0.5pt} \\
{[USER] \\
So far you have played 5 times {\color{brown}with your past choices and rewards summarized as follows:\\
blue button: pressed 2 times with average reward 0.5 \\
green button: pressed 1 times with average reward 0.0 \\
red button: pressed 1 times with average reward 1.0 \\
yellow button: pressed 0 times \\
purple button: pressed 1 times with average reward 0.0 \\}Which button will you choose next? Remember, YOU MUST provide your final answer within the tags\ {\color{red}\textless Answer\textgreater DIST\textless\textbackslash Answer\textgreater\ where DIST is formatted like "blue:a,green:b,red:c,yellow:d,purple:e".}
}
\end{mycolorbox}
\caption{A complete example of the prompt in MAB. The different components in the prompt are explained in detail in App.~\ref{subsec:exp:app:more:details:bandit}.}
\label{fig:wholeprompt4BSSCD}
\end{figure}

\begin{itemize}
    {\color{orange}\item B}utton scenario and {\color{orange}S}uggestive framing, providing the foundational task scenario, clarifying the role of the agent, and framing the objective of the task in a suggestive manner to guide decision-making.
    {\color{teal} \item Description of the multi-armed bandit problem}, offering the agent a detailed task description, including comprehensive information about the task's objectives, constraints, and operational details.
    {\color{brown} \item S}ummarized history, presenting a condensed version of historical decisions and reward feedback to the agent, instead of providing step-by-step decision and reward feedback.
    {\color{yellow} \item C}hain-of-thought or {\color{yellow}N}o CoT, indicating whether to encourage the agent to engage in step-by-step reasoning for decision-making.
    {\color{red} \item D}istribution over actions, encouraging the agent to generate a probability distribution over the arms of the bandit, instead of making deterministic decisions.
\end{itemize}
When we use our \alg~algorithm to optimize the task description and meta-instruction, 
% Specifically,
the upper section with {\color{violet}{light purple}} background corresponds to the  {\color{violet}{Task Description}}, where as the section below it with {\color{blue}{light blue}} background represents the {\color{blue}{Meta-Instruction}}. In other words, our \alg~algorithm is used to optimize the text in these two sections.

\subsubsection{Task Setting}
The experiments are conducted for both the BSSND and BSSCD prompts under two pre-defined difficulty levels: \textit{hard} and \textit{easy}. For the \textit{hard} setting, the MAB instance consists of $K=5$ arms, where the best arm has a mean reward of $\mu^\star = 0.5 + \Delta / 2$ with $\Delta = 0.2$, and all other arms have a mean reward of $\mu = 0.5 - \Delta / 2$. For the \textit{easy} setting, the MAB instance consists of $K=4$ arms with a larger gap $\Delta = 0.5$ between the best arm and the suboptimal arm.
We set the blue button as the optimal arm in experiments, corresponding to the arm with the highest expected reward. Each configuration is tested using two fixed random seeds, with experiments repeated 3 times for each seed, resulting in a total of \(2 \times 3 = 6\) runs per setting. Each experiment consists of 100 iterations, with the LLM-based agents making decisions and updating prompts iteratively to optimize performance.
The work of \citet{krishnamurthy2024can} has reported that GPT-3.5 models encounter exploration failures in MAB tasks, making them unsuitable as agents for solving such problems. In contrast, GPT-4 demonstrates the capability to effectively handle the exploration-exploitation trade-off inherent in MAB settings. Therefore, we adopt GPT-4-turbo as the LLM agent for this experiment. 


\subsubsection{Evaluation Metric.}
In the LLM-based Multi-Armed Bandit (MAB) task (Sec.~\ref{subsec:exp:bandits}), the performance of the LLM agent is assessed using the \textit{Cumulative Regret} metric. At each iteration, the LLM agent outputs a probability distribution over the arms, representing the likelihood of sampling each arm.

Formally, let there be \(K\) arms, each associated with an expected reward \(\mu_1, \mu_2, \dots, \mu_K\), where \(\mu^* = \max_{k \in \{1, \dots, K\}} \mu_k\) denotes the expected reward of the optimal arm. At iteration \(t\), we sample an arm \(a_t \in \{1, \dots, K\}\), which is determined by the probability distribution provided by the LLM agent. The instantaneous regret for iteration \(t\) is then defined as:
\[
r_t = \mu^* - \mu_{a_t},
\]
where \(\mu_{a_t}\) represents the expected reward of the selected arm \(a_t\) at iteration \(t\).

The cumulative regret after \(T\) iterations is computed as:
\[
R_T = \sum_{t=1}^{T} r_t = \sum_{t=1}^{T} \left( \mu^* - \mu_{a_t} \right).
\]



\subsubsection{Design of Prompt Score.}
The score of the prompt is designed to quantify the expected reward of the LLM agent's sampling strategy at each iteration. At iteration \( t \), the LLM agent outputs a sampling probability distribution \( \{ p_1, p_2, \dots, p_K \} \), where \( p_i \) represents the probability of selecting arm \( i \) (\( i = 1, 2, \dots, K \), with \( K \) being the total number of arms). Simultaneously, the historical records from the first \( (t-1) \) iterations allow us to compute an unbiased estimate of the Bernoulli reward parameter for each arm, \( \hat{\mu}_i \), based on the observed rewards and sampling counts.

For arm \( i \), the Bernoulli parameter \( \hat{\mu}_i \) is estimated as:
\[
\hat{\mu}_i = 
\begin{cases} 
0, & \text{if } n_i = 0, \\
\frac{\sum_{j=1}^{t-1} R_{i,j}}{n_i}, & \text{if } n_i > 0.
\end{cases}
\]
where \( \sum_{j=1}^{t-1} R_{i,j} \) denotes the cumulative reward obtained from arm \( i \) during the first \( (t-1) \) iterations, and \( n_i \) represents the total number of times arm \( i \) was sampled during the same period.

The LLM agent's expected reward \( \hat{R}_{\text{expected}} \) at iteration \( t \) is then calculated by weighting the estimated Bernoulli parameters \( \{\hat{\mu}_1, \hat{\mu}_2, \dots, \hat{\mu}_K\} \) with the sampling probabilities \( \{ p_1, p_2, \dots, p_K \} \) provided by the LLM:
\[
\hat{R}_{\text{expected}} = \sum_{i=1}^{K} p_i \cdot \hat{\mu}_i.
\]

This expected reward \( \hat{R}_{\text{expected}} \) serves as the score of the prompt.

\paragraph{Motivation for the Score Design.}
The design of the prompt score is driven by the objective of guiding the LLM agent to favor arms with higher expected rewards, represented by \( \mu_i \). Since the true values of \( \mu_i \) are not available, the prompt score is designed to estimate this quantity based on observed data. Specifically, the higher the value of \( \mu_i \), the higher the sampling probability \( p_i \) should be assigned to arm \( i \), reflecting the optimal choice. Conversely, arms with lower values of \( \mu_i \) should be assigned lower probabilities.

The original score with the Bernoulli parameters:
\[
R_{\text{expected}} = \sum_{i=1}^{K} p_i \mu_i.
\]
In the absence of the true \( \mu_i \), we rely on the unbiased estimates \( \hat{\mu}_i \):
\[
\hat{R}_{\text{expected}} = \sum_{i=1}^{K} p_i \hat{\mu}_i.
\]

This design is justified because, for most of iterations, the score \( \sum_{i=1}^K p_i \hat{\mu}_i \) is an unbiased estimate of the true expected reward \( \sum_{i=1}^K p_i \mu_i \), and we proceed to formally establish this unbiasedness.

\paragraph{Proof of Unbiasedness.}
For iteration \( t \), where \( n_i > 0 \) for all \( i \), we aim to show that the score \( \sum_{i=1}^K p_i \hat{\mu}_i \) is an unbiased estimate of the true expected reward \( \sum_{i=1}^K p_i \mu_i \). Since \( \hat{\mu}_i \) is an unbiased estimate of \( \mu_i \), we have:
\[
\expect{\hat{\mu}_i} = \expect{\mu_i},
\]
Thus, by the linearity of expectation, we obtain:
\begin{align*}
    \expect{\sum_{i=1}^K p_i \hat{\mu}_i} &= \sum_{i=1}^K p_i \expect{\hat{\mu}_i} \\
    &= \sum_{i=1}^K p_i \expect{\mu_i} \\
    &= \expect{\sum_{i=1}^K p_i \mu_i}
\end{align*}
This shows that the score $\hat{R}_{\text{expected}}$ is an unbiased estimate of the true expected reward $R_{\text{expected}}$.



\subsubsection{Details about the Models and Parameters in Our Algorithms}
\textbf{LLM Agents and Embedding Model.}
For the MAB tasks, the primary LLM agent is 
GPT-4-Turbo
and the fixed inference temperature is set to \(T=0\). For embedding generation, we employed OpenAI's \texttt{text-embedding-3-large} model, which outputs embeddings with a dimensionality of 3072. These embeddings are utilized to represent the prompts provided to the LLM agent during the experiments. At each iteration, the LLM is prompted once using the designed prompt.

\textbf{Neural Network Parameters.}
For the \alg, the input to the neural network consists of the concatenated embeddings of the task description and meta-instruction, resulting in an input dimensionality of \(3072 + 3072 = 6144\). The neural network employs a single hidden layer with a width of 1536 and produces a scalar output. The model is trained by minimizing the Mean Squared Error (MSE) loss function.

For the \alges, the neural network is designed to process \(K\) exemplars, where \(K\) is determined by the total number of available summaries. To ensure fairness, \(K\) distinct exemplar combinations are generated at each iteration using a cyclic rotation mechanism. This mechanism ensures that each summary occupies every possible position within the exemplar sequence. Formally, given \(K\) summaries indexed as \(\{e_0, e_1, \dots, e_{K-1}\}\), the \(i\)-th exemplar combination is defined as:
\[
(e_i, e_{(i+1) \mod K}, \dots, e_{(i+K-1) \mod K}).
\]
This guarantees that each summary appears in every position across all \(K\) combinations.

Each exemplar is embedded into a 3072-dimensional vector using the embedding model, and these embeddings are processed individually by the neural network. The neural network consists of a single hidden layer with a width of 512 and produces a scalar output. Like \alg, the training objective is to minimize the Mean Squared Error (MSE) loss function.

\textbf{EXP3 Learning Rate.} 
For the \alg, the learning rate parameter \(\eta_{\text{desc}}\) is set to \(10\) for selecting task descriptions and meta-instruction combinations. In the \alges, two learning rate parameters are used: \(\eta_{\text{desc}}\) is set to \(10\) for selecting task description and meta-instruction combinations, and \(\eta_{\text{exemplar}}\) is set to \(10\) for selecting exemplars.


\subsection{Improving Numerical Stability}

To prevent numerical overflow during the computation of exponentials in our algorithms,
a translation constant \( C^{(t)} \) is introduced at each iteration \( t \). This constant stabilizes the computation by shifting the cumulative scores, ensuring the algorithm operates reliably until convergence without altering the resulting probability distribution. The translation constant is defined as:
\begin{equation}
C^{(t)} = \max_{j} S_j^{(t)}.
\end{equation}

The translated scores are:
\begin{equation}
\tilde{S}_i^{(t)} = S_i^{(t)} - C^{(t)}.
\end{equation}

The probability distribution after translation is:
\begin{equation}
\tilde{P}_t[i] = \frac{\exp\big(\eta \tilde{S}_i^{(t)}\big)}{\sum_{j=1}^k \exp\big(\eta \tilde{S}_j^{(t)}\big)}.
\end{equation}

Substituting \( \tilde{S}_i^{(t)} = S_i^{(t)} - C^{(t)} \):
\begin{equation}
\tilde{P}_t[i] = \frac{\exp\big(\eta \big(S_i^{(t)} - C^{(t)}\big)\big)}{\sum_{j=1}^k \exp\big(\eta \big(S_j^{(t)} - C^{(t)}\big)\big)}.
\end{equation}

Using \( \exp(a - b) = \frac{\exp(a)}{\exp(b)} \):
\begin{equation}
\tilde{P}_t[i] = \frac{\exp\big(\eta S_i^{(t)}\big) / \exp\big(\eta C^{(t)}\big)}{
\sum_{j=1}^k \big(\exp\big(\eta S_j^{(t)}\big) / \exp\big(\eta C^{(t)}\big)\big)}.
\end{equation}

Simplifying:
\begin{equation}
\tilde{P}_t[i] = \frac{\exp\big(\eta S_i^{(t)}\big)}{\sum_{j=1}^k \exp\big(\eta S_j^{(t)}\big)}.
\end{equation}

Thus, the probabilities remain unchanged:
\begin{equation}
\tilde{P}_t[i] = P_t[i].
\end{equation}







\section{More Experimental Results}

\subsection{Results of GPT-4-turbo for TSP}
\label{app:ablation:subsec:gpt4-turbo}
Fig.~\ref{fig:TSP_ablation} shows a zoomed version of Fig.~\ref{fig:TSP_ablation:eta} (bottom right) in the main paper. It shows that when GPT-4-Turbo is used as the LLM, our \alg~is still able to significantly outperform OPRO.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.39\linewidth]{figures/TSP_20Nodes_gpt4_only.pdf}    
    \caption{Ablation study results using GPT-4-turbo in the TSP task with 20 nodes.}
    \label{fig:TSP_ablation}
\end{figure*}

\subsection{Results of Other Variants of OPRO}
\label{exp:app:opro:full:results}
As we have discussed in Sec.~\ref{subsec:expo} and Sec.~\ref{subsec:exp:opro}, the original OPRO uses a temperature of $1$ to choose all $8$ actions in a batch, while we have made a slight modification such that we choose the last action in the batch with a temperature of $0$. Here we show that this has a minimal impact on the performance of OPRO (Fig.~\ref{fig:full_results}).
Specifically, in Fig.~\ref{fig:full_results}, the orange curves represent the original OPRO (using a temperature of $1$ for all $8$ actions) and the pink curves correspond to our modified version.
We have also compared the performances of the enhanced variants (see Sec.~\ref{subsec:exp:opro} for details) for both the original (purple) and modified OPRO (green).
The results show that setting the temperature to $0$ while selecting the last action has negligible impact on the performance of OPRO.
Importantly, \textbf{our \alg~and \alges~algorithms consistently and dramatically outperform all variants of OPRO}.
\begin{figure*}[h]
\vspace{3mm}
\centering
\begin{tabular}{cc}
    \includegraphics[width=0.28\linewidth]{figures/LinearRegression_2_30_Full.pdf} &
    \includegraphics[width=0.28\linewidth]{figures/LinearRegression_36_neg1_Full.pdf} \\
    \makebox[0.28\linewidth]{\makecell{Linear Regression \\ ($w=2, \ b=30$)}} &
    \makebox[0.28\linewidth]{\makecell{Linear Regression \\ ($w=36, \ b=-1$)}} \\
\end{tabular}
\vspace{5mm}
\begin{tabular}{cc}
    \includegraphics[width=0.28\linewidth]{figures/TSP_10Nodes_Full.pdf} &
    \includegraphics[width=0.28\linewidth]{figures/TSP_15Nodes_Full.pdf} \\
    \makebox[0.28\linewidth]{\makecell{TSP \\ (10 Nodes)}} &
    \makebox[0.28\linewidth]{\makecell{TSP \\ (15 Nodes)}} \\
\end{tabular}
\vspace{5mm}
\begin{tabular}{c}
    \includegraphics[width=0.28\linewidth]{figures/TSP_20Nodes_Full.pdf} \\
    \makebox[0.28\linewidth]{\makecell{TSP \\ (20 Nodes)}} \\
\end{tabular}
\vspace{-2.5mm}
\caption{
Results of different algorithms in the Linear Regression task and TSP task (Sec.~\ref{subsec:exp:opro}). We have additionally included the original OPRO (which selects all $8$ actions using a temperature of 1), as well as its enhanced variant.
Lower is better.
}
\label{fig:full_results}
\vspace{-3mm}
\end{figure*}




\subsection{Impact of Adding Exemplar Embedding to the NN in \alg}
Recall that in every iteration of our \alg~(Algo.~\ref{algo:EXPO}), we need to train a neural network (NN) $\mathcal{M}(g(\cdot);\theta)$ to estimate the scores of the task descriptions and meta-instructions in the domain (line 8 of Algo.~\ref{algo:EXPO}).  
Note that the training set used to train this NN is $\{(\left[g(\mathcal{D}_i) \oplus g(\mathcal{I}_i)\right], s_i)\}_{i=1,\ldots,t+1}$ (line 7 of Algo.~\ref{algo:EXPO}).  
However, it is also important to note that in our \alg~algorithm, the set of exemplars included in the meta-prompt $\mathcal{E}'_{t}$ changes in every iteration and hence may also affect the scores $s_i$'s.  
Therefore, one may naturally wonder whether including the embedding of $\mathcal{E}'_{t}$ in the input of the NN can further improve the performance of the trained NN and, consequently, the performance of the overall \alg.  
We conduct an ablation study to validate this hypothesis, and the results are shown in Fig.~\ref{fig:with_without_exemplars_embedding}.  
The results demonstrate that including the embedding of the exemplars in the input of the NN does not lead to better performance than our standard approach of excluding it (Algo.~\ref{algo:EXPO}).  
This is likely due to the significantly increased dimensionality of the input to the NN, which makes training the NN more challenging.  
Therefore, these results suggest that the benefit of additionally accounting for the changing exemplars is outweighed by the drawback of the significantly increased dimensionality of the input to the NN.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.24\linewidth]{figures/Linear_Regression_2_30_ablation.pdf}
    \includegraphics[width=0.24\linewidth]{figures/Linear_Regression_36_neg1_ablation.pdf}
    \includegraphics[width=0.24\linewidth]{figures/Linear_Regression_2_30_Merged_ablation.pdf}
    \includegraphics[width=0.24\linewidth]{figures/Linear_Regression_36_neg_1_Merged_ablation.pdf}
    \includegraphics[width=0.24\linewidth]{figures/TSP_10Nodes_ablation.pdf}
    \includegraphics[width=0.24\linewidth]{figures/TSP_15Nodes_ablation.pdf}
    \includegraphics[width=0.24\linewidth]{figures/TSP_20Nodes_ablation.pdf}
    \caption{Convergence curves of our \alg~with and without exemplars embedding across different tasks: Linear Regression (top row) and TSP with 10, 15, and 20 nodes (bottom row).}
    \label{fig:with_without_exemplars_embedding}
\end{figure*}



\subsection{More Illustrations of the Discovered Task Description and Meta-instruction}
\label{app:subsec:more:illustration:meta-prompt}
Here we provide more illustrations regarding the comparison of the original task description and meta-instruction adopted by the original LLM-based sequential decision-making algorithm (i.e., OPRO or the LLM-based MAB algorithm from \citet{krishnamurthy2024can}) and those optimized by our \alg~algorithm.
We include the comparisons for the TSP task (Fig.~\ref{fig:example:descriptions:tsp}), and the two different prompt designs for the LLM-based MAB task in Sec.~\ref{subsec:exp:bandits} (Fig.~\ref{fig:example:descriptions:MABbssnd} and Fig.~\ref{fig:example:descriptions:MAB}).


\begin{figure*}[h]
% \onecolumn
\begin{minipage}[t]{0.4\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
\footnotesize  % adjust text size as required
\centerline{{\normalsize  OPRO}}
    {\color{purple}You are given a list of points with coordinates below: \{POINTS\}.\\
    Below are some previous traces and their lengths. 
The traces are arranged in descending order based on their lengths, 
where lower values are better.} \\ \\ \\
    \{EXEMPLARS\}
    \\\\
    {\color{blue}Give me a new trace that is different from all traces above, 
and has a length lower than any of the above. 
The trace should traverse all points exactly once. 
The trace should start with \texttt{<trace>} and end with \texttt{</trace>}.}
    % Output:
\end{mdframed}
\end{minipage}
\hfill
\begin{minipage}[t]{0.59\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
    \footnotesize  % adjust text size as required
\centerline{{\normalsize  \alg}}
    {\color{purple}You are provided with a dataset containing a list of coordinates labeled as \{POINTS\}. \par
The dataset also includes a series of previously calculated routes, with associated lengths that are ordered from longest to shortest. 
However, it’s key to note that shorter routes are more desirable. 
Despite the presentation order, understand that the optimal route is identified by the smallest total length.}  \\ \\
    \{EXEMPLARS\}
    \\\\
    {\color{blue}Provide a unique trace that is distinct from any previous traces and shorter in length. 
Ensure that this trace visits each point exactly once and adhere to the specified format 
by starting with \texttt{<trace>} and concluding with \texttt{</trace>}.} \\
\end{mdframed}
\end{minipage}
\caption{
The {\color{purple}task description} (top) and {\color{blue}meta-instruction} (bottom) used by OPRO (left) and optimized by our \alg~(right) in a TSP task.
}
\label{fig:example:descriptions:tsp}
\end{figure*}


\begin{figure*}[h]
% \onecolumn
\begin{minipage}[t]{0.4\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
% \scriptsize  % adjust text size as required
\footnotesize
\centerline{{\normalsize  BSSND}}
    {\color{purple}You are a bandit algorithm in a room with 5 buttons labeled blue, green, red, yellow, purple. Each button is associated with a Bernoulli distribution with a fixed but unknown mean; the means for the buttons could be different. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution. You have 100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize the total reward over the 100 time steps.} \\ \\ \\ \\
    {\color{blue}At each time step, I will show you a summary of your past choices and rewards. Then you must make the next choice. You may output a distribution over the 5 buttons formatted EXACTLY like "blue:a,green:b,red:c,yellow:d,purple:e".}
    \\ \\
    % Output:
\end{mdframed}
\end{minipage}
\hfill
\begin{minipage}[t]{0.59\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
    % \scriptsize  % adjust text size as required
    \footnotesize
\centerline{{\normalsize  \alg}}
    {\color{purple}You are presented as a bandit algorithm, located in an environment offering five distinct buttons, each emblazoned with colors such as blue, green, red, yellow, and purple. Each button acts a vessel tied to a non-variable yet undisclosed Bernoulli distribution mean which isn't subjected to be uniformly distributed across buttons. In this mechanism, every button acts as a yielder of a capricious reward, constructed from the associated distribution of the respective button. With access to total life -encompassing around 100 temporal stages - your voluntary element grants you control towards opting the button insertion at each such progressive phase. Precisely summoning your approach could perpetually provide you with a regulatory provision\_, the aptitude - is flexibly dwelling within its underlining motive- aiming at optimizing total accumulated cashbacks during several phases of these 100 spatial temporalities.} \\ \\
    \\
    {\color{blue}During every step of the process, a recap highlighting your previous selections and the prizes received will be presented to you. Then, it'll now be incumbent upon you to proceed with the new decision-making process. For your ease, a well-structured distribution comprising five buttons in assorted colours such as "blue", "green,", "red", "yellow", and "purple" will be exhibited before you. Make sure to structure your output accordingly; this might look something akin to "blue:a,green:b,red:c,yellow:d,purple:e".}
\end{mdframed}
\end{minipage}
\caption{
The {\color{purple}suggestive framing} (corresponding to the task description) and {\color{blue}MAB problem description} (corresponding to the meta-instruction) used by BSSND \(hard\) (left) and optimized by our \alg~(right) in an LLM-based MAB task.
}
\label{fig:example:descriptions:MABbssnd}
\end{figure*}


\begin{figure*}[t]
% \onecolumn
\begin{minipage}[t]{0.42\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
% \scriptsize  % adjust text size as required
\footnotesize
\centerline{{\normalsize  BSSCD}}
    {\color{purple}You are a bandit algorithm in a room with 5 buttons labeled blue, green, red, yellow, purple. Each button is associated with a Bernoulli distribution with a fixed but unknown mean; the means for the buttons could be different. For each button, when you press it, you will get a reward that is sampled from the button's associated distribution. You have 100 time steps and, on each time step, you can choose any button and receive the reward. Your goal is to maximize the total reward over the 100 time steps.} \\ \\
    {\color{blue}At each time step, I will show you a summary of your past choices and rewards. Then you must make the next choice. You may output a distribution over the 5 buttons formatted EXACTLY like "blue:a,green:b,red:c,yellow:d,purple:e". Let’s think step by step to make sure we make a good choice.}
    \\
    % Output:
\end{mdframed}
\end{minipage}
\hfill
\begin{minipage}[t]{0.57\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
    % \scriptsize  % adjust text size as required
    \footnotesize
\centerline{{\normalsize  \alg}}
    {\color{purple}You are an algorithm designed to function as a bandit, positioned within an environment that features five distinct buttons, each colored blue, green, red, yellow, and purple. These buttons are intricately connected to individual Bernoulli distributions which possess unique and undisclosed mean probabilities. When a button is pressed, it delivers a reward based on its specific distribution. Granted with 100 opportunities to act, your objective is to strategically press these buttons in a manner that optimizes the accrued total reward throughout these attempts. Make your selections wisely to maximize the gains from this stochastic setup.} \\ \\
    \\
    {\color{blue}In each phase, a concise recap of your previous decisions and received rewards will be presented. Your task is to make a subsequent choice based on this data. It is essential to output your selection in an exact format defined as "blue:a, green:b, red:c, yellow:d, purple:e", where 'a', 'b', 'c', 'd', 'e' represent specific z-score values for each color accompanied by the decision choice letter(s). The process is designed to refine our strategy progressively with each move, ensuring an informed and impactful outcome.}
\end{mdframed}
\end{minipage}
\caption{
The {\color{purple}suggestive framing} (corresponding to the task description) and {\color{blue}MAB problem description} (corresponding to the meta-instruction) used by BSSCD \(hard\) (left) and optimized by our \alg~(right) in an LLM-based MAB task.
}
\label{fig:example:descriptions:MAB}
\end{figure*}




\subsection{More Results on the Ablation Study Regarding Comparison with the Stochastic MAB Algorithm of NeuralUCB}
\label{app:subsec:more:ablation:ucb}
Here we provide the additional ablation study results comparing the performance of our \alg~algorithm with the stochastic MAB algorithm of NeuralUCB, using the TSP task.
The results are shown in Fig.~\ref{fig:ablation_neuralucb:tsp}, which, together with Fig.~\ref{fig:ablation_neuralucb:lr}, demonstrate that our \alg~algorithm based on adversarial bandits significantly and consistently outperforms the stochastic MAB method of NeuralUCB.
\begin{figure}[h]
\vspace{-3mm}
\centering
\begin{tabular}{ccc}
    \includegraphics[width=0.33\linewidth]{figures/TSP_10Nodes_ablation_neuralucb.pdf} &
    \includegraphics[width=0.33\linewidth]{figures/TSP_15Nodes_ablation_neuralucb.pdf} &
    \includegraphics[width=0.33\linewidth]{figures/TSP_20Nodes_ablation_neuralucb.pdf} \\
    {\small \makecell{TSP \\ (10 Nodes)}} & {\small \makecell{TSP \\ (15 Nodes)}} & {\small \makecell{TSP \\ (20 Nodes)}} \\
\end{tabular}
\vspace{-2.5mm}
\caption{
Comparison of our \alg~with NeuralUCB (i.e., a representative stochastic MAB algorithm) in the TSP tasks.
}
\label{fig:ablation_neuralucb:tsp}
\vspace{-3mm}
\end{figure}