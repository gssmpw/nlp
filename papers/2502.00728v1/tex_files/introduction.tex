The strong capabilities of LLMs have spurred significant recent interests in adopting them as agents to solve sequential decision-making problems, such as multi-armed bandits (MAB) \cite{krishnamurthy2024can}, Bayesian optimization (BO) \cite{yang2023large} and reinforcement learning (RL) \cite{dai2024context}. Specifically, these methods often use an LLM to sequentially select the actions by providing it with a specially designed prompt, which we refer to as the \emph{meta-prompt}.
The meta-prompt often contains several components, such as the \emph{task description}, the \emph{meta-instruction} (which is used to instruct the LLM to select an action in every step), the \emph{history of interactions} with the environment, among others. 
The previous methods have all adopted a fixed, manually designed meta-prompt for the LLM-based agent throughout the entire sequential decision-making process. However, numerous previous works have highlighted that the output text generated by LLMs is heavily dependent on its input prompt \cite{zhou2023large}. 
Therefore, using fixed, manually designed meta-prompt may significantly limit the performance of the LLM-based agents, because handcrafted prompts are often far from optimal \cite{lin2023instinct}. This naturally begs the question: \emph{can we automatically optimize the meta-prompt for LLM-based agents to enhance their performance?}

The sensitivity of LLM-generated text to its input prompt has given rise to many recent works on \emph{automated prompt optimization}, among which a representative line of works have adopted the method of multi-armed bandits (MAB) to automatically optimize the prompt \cite{lin2023instinct,wu2024prompt,lin2024prompt}.
Unfortunately, the problem of meta-prompt optimization for LLM-based agents presents significant challenges compared to traditional prompt optimization. 
This is mostly due to the \emph{non-stationarity in the observed rewards} during the LLM-based sequential decision-making process. 
Specifically, as the LLM-based agent engages in more interactions with the environment, its state in the environment changes, making its observed rewards non-stationary. 
For example, in MAB \cite{krishnamurthy2024can} and BO \cite{yang2023large}, the observed rewards in later iterations (i.e., after the agent has accumulated significant experience in the environment) tend to be higher than those obtained in initial iterations. 
Similarly, in RL \cite{dai2024context}, rewards are typically dependent on both the state and action. However, since the state of the LLM-based agent evolves across iterations, this also results in non-stationarity in the observed rewards.
As a consequence of the non-stationarity, for the same meta-prompt (e.g., the same task description and meta-instruction), its corresponding observed reward is highly likely to be dynamically changing across different iterations. This is in stark contrast to classical prompt optimization, in which the reward or score for a prompt remains stationary across iterations. 
As a result, this renders the previous works on prompt optimization (such as those based on MAB \cite{lin2023instinct,wu2024prompt,lin2024prompt}) inapplicable, and hence calls for novel algorithmic designs to solve the problem of meta-prompt optimization for LLM-based agents. To this end, we draw inspirations from the field of \emph{adversarial bandits} \cite{lattimore2020bandit}.

In adversarial bandits, for each arm, the reward observations when the arm is pulled are chosen by an adversary, i.e., they are allowed to change in an arbitrary way across different iterations. Therefore, the reward observations can be significantly non-stationary. 
This is considerably different from classical stochastic MAB, in which the reward observations for an arm are sampled from a fixed stationary distribution. Therefore, the ability of adversarial bandits to handle non-stationary reward observations makes it an ideal candidate for meta-prompt optimization for LLM-based agents. 
Specifically, drawing inspirations from the EXP3 algorithm for adversarial bandits, we introduce our \emph{\underline{EXP}onential-weight algorithm for prompt \underline{O}ptimization} (\alg) to optimize the task description and meta-instruction in the meta-prompt of an LLM-based agent.\footnote{Note that although here we only consider optimizing the task description and meta-instruction, the other components contained in the meta-prompt (e.g., some information from previously completed related tasks) can also be optimized in a similar fashion.}

In addition to the task description and meta-instruction, the \emph{history of interactions} with the environment (which we also refer to as the \emph{exemplars}) is also a crucial component in the meta-prompt which exerts a considerable impact on the performance of LLM-based agents.
Existing works often adopt simple heuristic approaches to decide how to incorporate the exemplars into the meta-prompt, including which subset of exemplars is included and their ordering in the meta-prompt.
Previous works on in-context learning (ICL) have found that in addition to their contents, the ordering of the exemplars also has a significant impact on the performance of LLMs \cite{lu2022fantastically}. 
Therefore, in addition to optimizing the task description and meta-instruction, we also extend our \alg~algorithm to additionally optimize both the subset of exemplars included in the meta-prompt and their ordering.
However, the optimization of the task description and meta-instruction in every iteration in our \alg~makes the optimization of exemplars non-stationary as well. Specifically, for the same subset of exemplars with a fixed ordering, their reward observations are usually non-stationary, because the task description and meta-instruction selected by our \alg~algorithm are highly likely to vary across different iterations.
To this end, we extend our \alg~algorithm to additionally use a separate adversarial bandit method to optimize the exemplars (i.e., the interaction history) in the meta-prompt for LLM-based agents, and hence introduce our \emph{\alg~with \underline{E}xemplar \underline{S}election} (\alges) algorithm.

We use extensive experiments to show that our \alg~algorithm significantly improves the performance of the LLM-based BO algorithm from \citet{yang2023large} (Sec.~\ref{subsec:exp:opro}) and the LLM-based MAB algorithm from \citet{krishnamurthy2024can} (Sec.~\ref{subsec:exp:bandits}).
Furthermore, in tasks where the exemplars 
provide crucial information for the LLM-based agent, our \alges~algorithm further enhances the performance of \alg~via automated exemplar selection (Sec.~\ref{subsec:exp:opro}).
We also perform ablation study to unveil other interesting insights about our algorithms in Sec.~\ref{sec:ablation}.
