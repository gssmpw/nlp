
We firstly apply our algorithms to improve the performance of OPRO in the Linear Regression (LR) and traveling salesman problem (TSP) tasks, adopting the same experimental setting as \citet{yang2023large} (Sec.~\ref{subsec:exp:opro}).
Then, we use our algorithms to enhance the performance of the LLM-based MAB algorithm from \citet{krishnamurthy2024can}.

\subsection{Linear Regression and Traveling Salesman Problem}
\label{subsec:exp:opro}
For both tasks here, we adopt GPT-3.5-Turbo as the LLM.

\textbf{Linear Regression (LR).}
In the LR task, our goal is to find the optimal LR coefficients, $w$ and $b$, that best fit a set of given noisy observations. We firstly choose the groundtruth LR coefficients $w_{\text{true}}$ and $b_{\text{true}}$, and use them to generate noisy observations for 50 inputs $x$ which are randomly and uniformly selected within $[-1, 1]$.
Specifically, for each input $x$, we generate its noisy observation as $y = w_{\text{true}} x + b_{\text{true}} + \epsilon$ where $\epsilon$ is a Gaussian noise.
We adopt the two most challenging choices of coefficients from \citet{yang2023large}: (1) $w_{\text{true}}=2,b_{\text{true}}=30$ and (2) $w_{\text{true}}=36,b_{\text{true}}=-1$.
In this task, OPRO aims to find the optimal $w$ and $b$ which minimizes the regression error (i.e., mean squared error).


\textbf{Traveling Salesman Problem (TSP).}
In the classical TSP problem \cite{junger1995traveling}, given a set of $n$ nodes with their coordinates, the objective is to find the shortest route that starts from a given node, traverses all nodes exactly once, and finally returns to the starting node.
Therefore, our goal is to solve a discrete optimization problem in which the input variable is a trajectory and the goal is to minimize the total distance of the trajectory.
We adopt TSP instances with 10, 15, and 20 randomly generated nodes, respectively, which represent increasing levels of difficulty.


The results for both tasks are shown in Fig.~\ref{fig:linear_tsp_2_main_results}, which plot the regression error (i.e., mean squared error) for the LR tasks and optimality gap (i.e., the difference between the total distance of the discovered route and that of the optimal route) for the TSP tasks (lower is better for both tasks).
Of note, in addition to the standard OPRO (pink curves) \cite{yang2023large}, we have also proposed an enhanced variant of OPRO (green curves) in which we added some further clarifications to the task description 
(see App.~\ref{app:sec:enhanced:opro} for more details). 
The enhanced variant consistently improves the performance of the standard OPRO (Fig.~\ref{fig:linear_tsp_2_main_results}).\footnote{As discussed in the last paragraph of Sec.~\ref{subsec:expo}, we have slightly modified OPRO to select the last action in the batch using a temperature of $0$. We empirically show that this leads to comparable performance with the original OPRO which uses a temperature of $1$ to choose all $8$ actions (see Fig.~\ref{fig:full_results} in App.~\ref{exp:app:opro:full:results}).}
More importantly, the results in Fig.~\ref{fig:linear_tsp_2_main_results} show that \textbf{in all tasks, our \alg~algorithm (blue curves) significantly and consistently outperforms OPRO}, including both standard OPRO and its enhanced variant.
This demonstrates that our meta-prompt optimization approach, grounded in adversarial bandits, leads to more efficient (i.e., faster convergence) and more effective (i.e., improved final performance) LLM-based sequential decision-making.
\begin{figure*}[t]
% \onecolumn
\begin{minipage}[t]{0.32\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
\scriptsize  % adjust text size as required
% \footnotesize
\centerline{{\normalsize OPRO}}
    {\color{purple}Now you will help me minimize a function with two input variables \(w, b\). I have some \((w, b)\) pairs and the function values at those points. The pairs are arranged in descending order based on their function values, where lower values are better.} \\ \\
    \{EXEMPLARS\}
    \\\\
    {\color{blue}Give me a new \((w, b)\) pair that is different from all pairs above, and has a function value lower than any of the above. Do not write code. The output must end with a pair \([w, b]\), where \(w\) and \(b\) are numerical values.}
    % Output:
\end{mdframed}
\end{minipage}
\hfill
\begin{minipage}[t]{0.67\textwidth}
\begin{mdframed}[linewidth=0.9pt]  % adjust linewidth as you desire
    \scriptsize  % adjust text size as required
    % \footnotesize
\centerline{{\normalsize \alg}}
    {\color{purple}We will collaborate to optimize a function involving two parameters, \textbackslash(w\textbackslash) and \textbackslash(b\textbackslash). I possess a set of data points, each consisting of \textbackslash((w, b)\textbackslash) pairs and their corresponding function values. These pairs are systematically organized in reverse order, starting from the greatest to the smallest function values. Essentially, the lower the function value, the more optimal or preferable the pair. Consequently, our goal is to identify and analyze the \textbackslash((w, b)\textbackslash) pair that manifests the lowest function value, as this represents the perspective of optimum efficacy.} \\ \\
    \{EXEMPLARS\}
    \\\\
    {\color{blue}To enhance the quality and expand on the existing instructions, follow these improved guidelines vis-à-vis designing a new and distinctive numerical pair: ensure the selected \((w, b)\) combination diverges from prior examples and secures a function output lower than preceding values. Key details on methodology or calculations are not required—just ensure clarity in presenting a returned value that closes with the specific format \([w, b]\), where both \(w\) and \(b\) are distinct numerical figures.}
\end{mdframed}
\end{minipage}
\vspace{-2.5mm}
\caption{
The {\color{purple}task description} and {\color{blue}meta-instruction} used by OPRO (left) and optimized by our \alg~(right) in a Linear Regression task.
}
\label{fig:example:descriptions:lr}
\end{figure*}


\begin{figure}[h]
\vspace{3mm}
\centering
\includegraphics[width=0.48\linewidth]{figures/BSSND_easy_Cumulative_Regret.pdf}
\includegraphics[width=0.48\linewidth]{figures/BSSCD_easy_Cumulative_Regret.pdf}
\makebox[0.48\linewidth]{BSSND (easy)}
\makebox[0.48\linewidth]{BSSCD (easy)}
% \vspace{5mm}
\includegraphics[width=0.48\linewidth]{figures/BSSND_hard_Cumulative_Regret.pdf}
\includegraphics[width=0.48\linewidth]{figures/BSSCD_hard_Cumulative_Regret.pdf}
% \vspace{-3mm}
\makebox[0.48\linewidth]{BSSND (hard)}
\makebox[0.48\linewidth]{BSSCD (hard)}
\vspace{-2mm}
\caption{
Cumulative regret of different algorithms in the LLM-based MAB experiments (Sec.~\ref{subsec:exp:bandits}). Lower is better.
}
\label{fig:cumulative_regret_BSSND_BSSCD}
\vspace{-3mm}
\end{figure}

Meanwhile, our \alges~algorithm, which is additionally equipped with automated exemplar selection, considerably improves the performance of \alg~in the LR tasks yet performs on par with \alg~in the TSP tasks.
This is likely because the exemplars play a more important role in the LR tasks than the TSP tasks. 
Specifically, in LR, \emph{the input-output exemplars provide important information for identifying the optimal LR coefficients} \cite{wu2024prompt}. Therefore, selecting better exemplars (via our \alges) brings significant performance boost. 
On the other hand, in the TSP tasks, due to the challenging nature of the tasks, it is difficult for the LLM to infer crucial and useful information from the exemplars. 
Therefore, the other components in the meta-prompt (i.e., the task description and meta-instruction) provide more useful information 
% for the decision-making of the LLM 
in the TSP tasks.
As a result, \emph{selecting better exemplars does not lead to noticeable performance gains in the TSP tasks}.
Fig.~\ref{fig:example:descriptions:lr} provides an illustration of the original task description and meta-instruction used by OPRO and those discovered by our \alg~algorithm for the LR tasks, whereas the corresponding meta-prompts for the TSP tasks are displayed in Fig.~\ref{fig:example:descriptions:tsp} in App.~\ref{app:subsec:more:illustration:meta-prompt}.



\subsection{LLM-Based Multi-Armed Bandits (MAB)}
\label{subsec:exp:bandits}
The work of \citet{krishnamurthy2024can} has used an LLM to sequentially select the arms/actions in MAB and proposed methods to manually design the meta-prompt.
Their prompt design consists of 5 components with each having 2 possible choices, which gives rise to a total of $2^5=32$ possible prompts.
Here we show that our algorithms can be used to automatically optimize their manually designed prompts to further enhance their performance.
Specifically, we adopt 2 of their prompt designs: BSSND and BSSCD,
and apply our \alg~and \alges~algorithms to optimize the important components in these prompt designs.
Following \citet{krishnamurthy2024can}, we use two MAB instances: \emph{easy} and \emph{hard}.
We adopt GPT-4-Turbo as the LLM here.
More details on the experimental design are deferred to App.~\ref{subsec:exp:app:more:details:bandit}.
The results for the 4 experimental settings (i.e., 2 prompt designs $\times$ 2 MAB instances) are shown in Fig.~\ref{fig:cumulative_regret_BSSND_BSSCD}, which demonstrate that our \alg~and \alges~algorithms are able to significantly reduce the cumulative regret of MAB in this task across different prompt desings and MAB instances.
We illustrate the comparison between the original meta-prompt and the one optimized by our \alg~in Figs.~\ref{fig:example:descriptions:MABbssnd} and~\ref{fig:example:descriptions:MAB} in App.~\ref{app:subsec:more:illustration:meta-prompt}.




\section{Ablation Study}
\label{sec:ablation}
\textbf{Only Optimizing Task Description or Meta-Instruction.}
Our \alg~jointly optimize the task description $\mathcal{D}$ and the meta-instruction $\mathcal{I}$.
Here we evaluate the performance of optimizing either $\mathcal{D}$ or $\mathcal{I}$ alone. 
The results in Fig.~\ref{fig:task_description_meta_instructions_ablation} show that jointly optimizing them indeed leads to significantly better performance.
However, optimizing these components alone still consistently outperforms OPRO.
\begin{figure}[h]
\centering
\begin{tabular}{cc}
    \includegraphics[width=0.45\linewidth]{figures/Task_Description_or_Meta_Instructions_Only_2_30.pdf} &
    \includegraphics[width=0.45\linewidth]{figures/Task_Description_or_Meta_Instructions_Only_36_neg1.pdf} \\
    {\small $w=2$, \ $b=30$} &
    {\small $w=36$, \ $b=-1$} \\
\end{tabular}
\vspace{-2.5mm}
\caption{
Results of our \alg~when only optimizing the task description or the meta-instruction.
}
\label{fig:task_description_meta_instructions_ablation}
\vspace{-3mm}
\end{figure}

\textbf{Comparison with Stochastic MAB Algorithms: Upper Confidence Bound.}
Classical stochastic MAB algorithms, such as those based on upper confidence bound (UCB), have been applied to prompt optimization in a number of recent works \cite{lin2024prompt,lin2023instinct,wu2024prompt} and yielded strong performance.
However, as we have discussed in Sec.~\ref{sec:intro}, in meta-prompt optimization for LLM-based sequential decision-making, the non-stationary reward observations render these stochastic MAB methods unsuitable.
Here we verify this by comparing our \alg~with the \emph{NeuralUCB} algorithm adopted by \citet{lin2023instinct,wu2024prompt}.
The results for the Linear Regression tasks are displayed in Fig.~\ref{fig:ablation_neuralucb:lr},
which show that NeuralUCB indeed significantly underperforms in the problem of meta-prompt optimization for LLM-based agents.
The results for the TSP tasks are consistent with the results here (Fig.~\ref{fig:ablation_neuralucb:tsp} in App.~\ref{app:subsec:more:ablation:ucb}).
These results provide further justifications for our proposed adversarial bandit-based algorithms.
\begin{figure}[h]
\vspace{-3mm}
\centering
\begin{tabular}{cc}
    \includegraphics[width=0.48\linewidth]{figures/Linear_Regression_ablation_neuralucb.pdf} &
    \includegraphics[width=0.48\linewidth]{figures/Linear_Regression_ablation_neuralucb_2_30.pdf} \\
    {\small \makecell{Linear Regression \\ ($w=36$, \ $b=-1$)}} &
    {\small \makecell{Linear Regression \\ ($w=2$, \ $b=30$)}} \\
\end{tabular}
\vspace{-2.5mm}
\caption{
Comparison of our \alg~with NeuralUCB (i.e., a representative stochastic MAB algorithm) in the LR tasks.
}
\label{fig:ablation_neuralucb:lr}
\vspace{-3mm}
\end{figure}

\begin{figure}[h]
\vspace{3mm}
\centering
\includegraphics[width=0.49\linewidth]{figures/TSP_ablation_eta.pdf}
\includegraphics[width=0.49\linewidth]{figures/TSP_ablation_eta_15Nodes.pdf}
\makebox[0.48\linewidth]{TSP (10 nodes)}
\makebox[0.48\linewidth]{TSP (15 nodes)}
% \vspace{5mm}
\includegraphics[width=0.49\linewidth]{figures/TSP_ablation_eta_20Nodes.pdf}
\includegraphics[width=0.49\linewidth]{figures/TSP_20Nodes_gpt4.pdf}
% \vspace{-3mm}
\makebox[0.48\linewidth]{\makecell{TSP (20 nodes)}}
\makebox[0.48\linewidth]{\makecell{TSP (20 Nodes, GPT-4-Turbo)}}
\vspace{-2mm}
    \caption{
    First three figures: ablation study on impact of exploration parameter $\eta$.
    Bottom right: results using GPT-4-Turbo.
    }
    \label{fig:TSP_ablation:eta}
\vspace{-3mm}
\end{figure}

\textbf{Impact of the Degree of Exploration.}
Here we examine the impact of the degree of exploration, i.e., the value of $\eta$ (see line 10 of Algo.~\ref{algo:EXPO}).
The results (Fig.~\ref{fig:TSP_ablation:eta}) show that an excessively large degree of exploration (i.e., a small $\eta=10$) or an overly small degree of exploration (i.e., a large $\eta=1000$) both deteriorate the performance.
Moreover, the results also demonstrate that in easier tasks (i.e., TSP with 10 nodes), imposing a smaller degree of exploration (i.e., $\eta=1000$) leads to better performance compared to $\eta=10$, because it allows our \alg~to quickly converge to the optimal solution. On the other hand, in more challenging tasks (i.e., TSP with 20 nodes), more exploration (i.e., $\eta=10$) results in better performance (than $\eta=1000$), because it makes it easier for our \alg~to escape local optimum.




% \subsection{Experiments With Other LLMs}
\textbf{Experiments With Other LLMs.}
To evaluate the effectiveness of our approach when combined with different LLMs, here we adopt the challenging TSP task with 20 nodes and replace the GPT-3.5-Turbo model used in our original experiments (Sec.~\ref{subsec:exp:opro}) by the more advanced GPT-4-Turbo model.
The results in Fig.~\ref{fig:TSP_ablation:eta} (bottom right) show that the use of the more advanced GPT-4-Turbo model significantly improves the performance of both OPRO and our \alg.
More importantly, as visualized more clearly in Fig.~\ref{fig:TSP_ablation} in App.~\ref{app:ablation:subsec:gpt4-turbo}, when both adopting GPT-4-Turbo, our \alg~still significantly outperforms OPRO.
The results show that our \alg~can effectively improve the performance of LLM-based agents across different LLMs.

\textbf{Effectiveness of the Optimal Prompt Discovered by \alg.}
To further verify the ability of our \alg~to identify effective meta-prompts, here we replace the original task description and meta-instruction in an LLM-based sequential decision-making algorithm (e.g., OPRO) by the optimal ones discovered by our \alg.
For example, for ORPO, we firstly run our \alg~to completion, and then use the final meta-prompt selected by our \alg~as the meta-prompt to execute OPRO again.
The results in Fig.~\ref{fig:optimal_prompt_on_tasks} show that fixing the meta-prompt to be the one optimized by our \alg~leads to dramatic performance boost to LLM-based sequential decision-making.
\begin{figure}[h]
\vspace{-3mm}
\centering
\includegraphics[width=0.49\linewidth]{figures/BSSND_hard_EXPO_on_baseline.pdf}
\includegraphics[width=0.49\linewidth]{figures/BSSCD_hard_EXPO_on_baseline.pdf}
\makebox[0.48\linewidth]{BSSND (hard)}
\makebox[0.48\linewidth]{BSSCD (hard)}
% \vspace{5mm}
\includegraphics[width=0.49\linewidth]{figures/LinearRegression_36_neg1_EXPO_on_baseline.pdf}
\includegraphics[width=0.49\linewidth]{figures/TSP_20Nodes_EXPO_on_Baseline.pdf}
% \vspace{-3mm}
\makebox[0.48\linewidth]{\makecell{Linear Regression \\ ($w=36$, \ $b=-1$)}}
\makebox[0.48\linewidth]{\makecell{TSP \\ (20 Nodes)}}
\vspace{-2mm}
\caption{
Results achieved by fixing the meta-prompt to be the optimal one discovered by
our \alg~(gray curves).
}
\label{fig:optimal_prompt_on_tasks}
\vspace{-3mm}
\end{figure}




