\subsection{The \alg~Algorithm (Algo.~\ref{algo:EXPO})}
\label{subsec:expo}
Our \alg~is used to dynamically optimize the task description $\mathcal{D}$ and the meta-instruction $\mathcal{I}$ in the meta-prompt. 

\textbf{Domain Generation.}
At the beginning of our algorithm, we start by generating the domain of task descriptions and meta-instructions. Following the previous works on prompt optimization \cite{zhou2023large,lin2024prompt,lin2023instinct}, we use an LLM to rephrase an initial task description $\mathcal{D}_0$ (resp.~initial meta-instruction $\mathcal{I}_0$) to generate a domain of $k_1$ task descriptions (resp.~$k_2$ meta-instructions). This results in a domain size of $k=k_1\times k_2$.
We defer more details on domain generation to App.~\ref{app:subsec:detail:domain:generation}.
We treat the combination of a task description $\mathcal{D}$ and a meta-instruction $\mathcal{I}$ in the domain as an \emph{arm}, i.e., our adversarial bandit problem has $k$ arms.
In addition to jointly optimizing $\mathcal{D}$ and $\mathcal{I}$,
we have also evaluated the performance of optimizing them separately. The results 
show that jointly optimizing these two components leads to better performance.

\textbf{{\textcircled{\scriptsize 1}} LLM-Based Action Selection (lines 3-7 of Algo.~\ref{algo:EXPO}).}
At the beginning of every iteration $t$, we firstly use the current
task description $\mathcal{D}_t$, meta-instruction $\mathcal{I}_t$ and exemplar sequence $\mathcal{E}_t'$ selected at the end of the last iteration $t-1$ (more details below) to construct a meta-prompt $\mathcal{Q}_t = (\mathcal{D}_t, \mathcal{I}_t, \mathcal{E}_t')$ (line 3). 
Then, we use $\mathcal{Q}_t$ as the input prompt to the LLM $f(\cdot)$ to select the next action $x_t$ and collect its score $s_t$ (lines 4-5). After that, we update the set of exemplars $\mathcal{E}_t$ and the meta-prompt-score set $\mathcal{S}_{t}$ (lines 6-7).

\textbf{{\textcircled{\scriptsize 2}} Score Estimation (lines 8-9).}
In the classical EXP3 algorithm for adversarial bandits with a finite number of arms, the cumulative sum of \emph{the observed rewards} of every arm is used to construct the arm sampling distribution through an exponential-weight mechanism \cite{lattimore2020bandit}.
However, in problems where the number of arms is excessively large (e.g., our problem of meta-prompt optimization), the reward observations for many arms are not available.
Therefore, the cumulative sum of \emph{the estimated rewards} of every arm is often used instead to construct the sampling distribution
\cite{lattimore2020bandit}.
Therefore, we firstly estimate the scores of all $k$ arms (i.e., meta-prompts) in the domain and then 
use these score estimates to derive an arm sampling distribution for our \alg.
% incorporate these score estimates into the EXP3 algorithm to derive am arm selection strategy.
A number of recent works have shown that using a neural network (NN) (which takes the pre-trained embedding $g(\cdot)$ as input) for score/reward estimation leads to powerful prompt optimization algorithms \cite{lin2024prompt,lin2023instinct,wu2024prompt}.
Therefore, we also adopt an NN $\mathcal{M}(g(\cdot); \theta)$ for score estimation in our \alg.
Specifically, in every iteration $t$, we use the history of selected meta-prompts and their scores, denoted as $\mathcal{S}_{t+1}$ (line 7 of Algo.~\ref{algo:EXPO}), to train an NN
by minimizing the mean-squared error (MSE) loss (line 8 of Algo.~\ref{algo:EXPO}).
The trained NN with parameters $\theta_{t+1}$ can then be used to estimate the score of every arm (i.e., every combination of task description and meta-instruction) in the domain.
For every arm,
its estimated score is then added to its corresponding \emph{cumulative sum of score estimates} $\hat{s}_i^{(t+1)}$ (line 9 of Algo.~\ref{algo:EXPO}).
Note that every term in the cumulative sum $\hat{s}_i^{(t+1)}$ represents our score estimate for arm $i$ \emph{in a particular iteration $t$}, i.e., our estimated score for arm $i$ from an NN trained using the observation history \emph{up to iteration $t$}.
The updated cumulative sums of score estimates $\hat{s}_i^{(t+1)}$ for all $k$ arms are then used for randomized arm (i.e., meta-prompt) selection, which we discuss next.

\textbf{{\textcircled{\scriptsize 3}} Randomized Meta-Prompt Selection (lines 10-12).}
After the cumulative sum $\hat{s}_i^{(t+1)}$
of every arm $i$ is updated, 
we 
follow the EXP3 algorithm \cite{lattimore2020bandit} and use the cumulative sums to
construct a distribution following Equation \eqref{eq:exp3}.
Then, we use this distribution to randomly sample the next arm,
i.e., the next task description $\mathcal{D}_{t+1}$ and meta-instruction $\mathcal{I}_{t+1}$ (line 11 of Algo.~\ref{algo:EXPO}).
\emph{Randomization} is a key principle in adversarial bandits \cite{lattimore2020bandit}, and the randomization involved in our arm selection strategy is crucial for the ability of our \alg~to deal with non-stationary reward observations.
The heuristic to select a sequence of exemplars $\mathcal{E}'_{t+1}$ (line 12 of Algo.~\ref{algo:EXPO}) is often specified by the LLM-based sequential decision-making algorithm \cite{yang2023large}. We discuss more details on this, as well as the extension of our \alg~algorithm to automatically select $\mathcal{E}'_{t+1}$, in Sec.~\ref{subsec:expo:es}.

\begin{algorithm}[H]
\begin{algorithmic}[1]
    \INPUT: Initial task description $\mathcal{D}_0$, initial meta-instruction $\mathcal{I}_0$.
    \STATE Initialize the exemplar set $\mathcal{E}_0 = \emptyset$, and the subset $\mathcal{E}_0' = \emptyset$, meta-prompt-score set $\mathcal{S}_0 = \emptyset$, and cumulative score estimates $\hat{s}^{(0)}_i=0$ for all $i \in \{1, \ldots, k\}$.
    \FOR{iteration $t = 0, 1, \ldots, T-1$}
        \STATE Construct 
        meta-prompt $\mathcal{Q}_t = (\mathcal{D}_t, \mathcal{I}_t, \mathcal{E}_t')$.
        \STATE Query the LLM $f(\cdot)$ using the meta-prompt $\mathcal{Q}_t$ to select the next action $x_t$: $x_t = f(\mathcal{Q}_t)$.
        \STATE Observe the score $s_t$ for $x_t$ using the task-specific evaluator: $s_t = \xi(x_t)$.
        \STATE Update the exemplar set $\mathcal{E}_{t+1} = \mathcal{E}_t \cup \{(x_t, s_t)\}$.

        \STATE Update the meta-prompt-score set $\mathcal{S}_{t+1} = \mathcal{S}_{t} \cup \{(\left[g(\mathcal{D}_t) \oplus g(\mathcal{I}_t)\right], s_t)\}$, where \( g(\cdot) \) denotes the embedding function and \( \oplus \) denotes concatenation.

        \STATE Update the parameters $\theta$ of the neural network (NN) $\mathcal{M}(g(\cdot); \theta)$ by using the updated $\mathcal{S}_{t+1}$ as the training set to minimize the MSE loss, yielding $\theta_{t+1}$.

        \STATE Update the cumulative score estimates $\hat{s}^{(t)}_i$ for all arms $i$ using the predicted scores from $\mathcal{M}(g(\cdot); \theta_{t+1})$:
        \begin{align}
        \hat{s}_i^{(t+1)} &= \hat{s}_i^{(t)} + \mathcal{M}(\left[g(\mathcal{D}_i) \oplus g(\mathcal{I}_i)\right]; \theta_{t+1}) \nonumber \\
        &\quad \forall i \in \{1, \ldots, k\}.
        \end{align}
        \STATE Compute the sampling distribution $P_t$ over all arms:
        \begin{equation}
        P_{t}[i] = \frac{\exp(\eta \hat{s}_i^{(t+1)})}{\sum_{j=1}^k \exp(\eta \hat{s}_j^{(t+1)})}, \quad \forall i \in \{1, \ldots, k\}
        \label{eq:exp3}
        \end{equation}
        \STATE Sample an arm (i.e., the combination of a task description and a meta-instruction) from $P_t$: $(\mathcal{D}_{t+1}, \mathcal{I}_{t+1}) \sim P_t$.
        \STATE Select a sequence of exemplars $\mathcal{E}_{t+1}'$ from $\mathcal{E}_{t+1}$ following a pre-defined heuristic method.
    \ENDFOR
\end{algorithmic}
\caption{\alg}
\label{algo:EXPO}
\end{algorithm}

\textbf{Exploitation vs.~Exploration.}
Our \alg~algorithm is able to achieve a principled balance between exploitation and exploration. 
The use of powerful pre-trained embedding and NNs 
allows us to achieve accurate score estimates. 
Therefore, the cumulative score estimate $\hat{s}_i^{(t+1)}$ (line 9 of Algo.~\ref{algo:EXPO}) provides a reliable assessment of the quality of every arm $i$ (i.e., every combination of task description and meta-instruction).
This ensures that an arm with a large score is given a large weight in the sampling distribution $P_t$ (line 10) and hence leads to reliable \emph{exploitation}.
Meanwhile, the inherent randomness in our randomized arm selection strategy ensures that enough \emph{exploration} is performed in the domain of meta-prompts.

\textbf{Batch Action Selection.}
In the description of our \alg~(Algo.~\ref{algo:EXPO}), although we select one action $x_t$ in every iteration $t$,
this can be easily generalized to select a batch of actions.
For example, when applying our \alg~to improve OPRO \cite{yang2023large} (Sec.~\ref{subsec:exp:opro}), we follow the practice of OPRO to select a batch of $8$ actions/queries in every iteration (i.e., step 4 of Algo.~\ref{algo:EXPO}) and set the temperature of the LLM to $1$ to ensure the diversity of the selected actions.
In order to obtain a noiseless and reliable score to assess the quality of the meta-prompt $\mathcal{Q}_t$, we set the temperature to $0$ when selecting the last action and use its corresponding observed score as the score $s_t$ of $\mathcal{Q}_t$ (line 5 of Algo.~\ref{algo:EXPO}).


\begin{figure*}[t]
\vspace{-3mm}
     \centering
     \begin{tabular}{ccccc}
         \hspace{-5mm}
        \includegraphics[width=0.2\linewidth]{figures/LinearRegression_2_30.pdf} & \hspace{-5mm}
        \includegraphics[width=0.2\linewidth]{figures/LinearRegression_36_neg1.pdf} & \hspace{-5mm}
        \includegraphics[width=0.2\linewidth]{figures/TSP_10Nodes.pdf} &\hspace{-5mm}
        \includegraphics[width=0.2\linewidth]{figures/TSP_15Nodes.pdf} &\hspace{-5mm}
        \includegraphics[width=0.2\linewidth]{figures/TSP_20Nodes.pdf}\\
         Linear Regression & Linear Regression & TSP & TSP & TSP\\
         ($w=2, b=30$) & ($w=36, b=-1$) & (10 Nodes) & (15 Nodes) & (20 Nodes)\\
     \end{tabular}
 \vspace{-3mm}
    \caption{
Results of different algorithms (mean $\pm$ standard error) in the Linear Regression and TSP task (Sec.~\ref{subsec:exp:opro}). Lower is better. 
    }
\label{fig:linear_tsp_2_main_results}
\vspace{-3mm}
\end{figure*}

\subsection{\alg~with Exemplar Selection (\alges)}
\label{subsec:expo:es}
Previous works on LLM-based for sequential decision making often select the sequence of exemplars $\mathcal{E}_{t+1}'$ included in the meta-prompt $\mathcal{Q}_t$ using a fixed pre-defined heuristic
(line 12 of Algo.~\ref{algo:EXPO}).
For example, OPRO includes the 20 exemplars with the highest observed scores in the meta-prompt, arranging them in descending order based on their scores \cite{yang2023large}; 
the LLM-based MAB method from \citet{krishnamurthy2024can} either includes all exemplars (ordered by their iteration sequence) in the prompt or includes a summarized representation of all exemplars.
However, numerous previous works have reported that both the subset of exemplars and their ordering have significant impacts on the performance of LLM \cite{wu2024prompt}.
Therefore, here we further extend our \alg~(Algo.~\ref{algo:EXPO}) to additionally optimize the sequence of exemplars $\mathcal{E}'_{t+1}$ (i.e., to replace line 12 of Algo.~\ref{algo:EXPO} by an automated method to select $\mathcal{E}'_{t+1}$), 
hence introducing our \alges~algorithm (Algo.~\ref{algo:ES-subset}, App.~\ref{app:expo:es}).

As a result of the dynamically changing task description and meta-instruction, the optimization of exemplar sequences becomes non-stationary as well. Therefore, we also dynamically optimize the exemplar sequence based on the EXP3 algorithm for adversarial bandits.
That is, in every iteration of our \alges~algorithm (Algo.~\ref{algo:ES-subset}), we firstly optimize the task description and meta-instruction (i.e., following lines 3-11 of Algo.~\ref{algo:EXPO}), and then optimize the exemplar sequence $\mathcal{E}'_{t+1}$ in a similar way to Algo.~\ref{algo:EXPO}.

\textbf{Details of \alges~(Algo.~\ref{algo:ES-subset}).}
Specifically, after the task description and meta-instruction are optimized (i.e., after lines 3-11 of Algo.~\ref{algo:EXPO}), we firstly extract the embedding of the exemplar sequence 
$\mathcal{E}'_{t}$ 
used in this iteration: $g(\mathcal{E}'_{t})$, and add $\left(g(\mathcal{E}'_{t}), s_t\right)$ to the 
\emph{exemplar training set}
$\mathcal{T}_{t+1}$ (line 4 of Algo.~\ref{algo:ES-subset}).
Next, the updated dataset $\mathcal{T}_{t+1}$ is used to train an NN with parameters $\theta^{\text{ES}}_{t+1}$ (line 5), which is able to \emph{estimate the score of any exemplar sequence}.
Subsequently, we randomly sample $k^{\text{ES}}$ exemplars sequences, each containing $\mathcal{L}$ exemplars, to be used as our \emph{domain of exemplar sequences} (line 8). 
Next, for every candidate exemplar sequence in the domain, we need to obtain its cumulative score estimate 
(similar to line 9 of Algo.~\ref{algo:EXPO}).
Unfortunately, due to \emph{the time-varying nature of the domain of exemplar sequences} (due to the addition of new exemplars and random sampling of exemplar sequences), we are no longer able to constantly maintain a cumulative score estimate for every exemplar sequence (i.e., arm) and update it in an incremental way.
To this end, we save the parameters of the trained NN in every iteration in history; then for each sampled exemplar sequence in the domain, we obtain its score estimates from all NNs in the history and use their sum as the \emph{cumulative score estimate} for this exemplar sequence (lines 9-14 of Algo.~\ref{algo:ES-subset}).
Next, the cumulative score estimates for all exemplar sequences are used to compute the sampling distribution, from which the next exemplar sequence $\mathcal{E}'_{t+1}$ is sampled and used to the meta-prompt in the next iteration (lines 15-16).




