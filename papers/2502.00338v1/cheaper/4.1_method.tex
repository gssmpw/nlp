\section{Method}
\begin{figure*}
  \centering
  \includegraphics[width=1\linewidth]{figures/icml_main.png}
     \vspace{-16pt}
  \caption{Overview of Our~\method{}. \textbf{(a)} The overall architecture includes input variables, an encoder, a message passing module, a decoder, and visualization of forecast variables; \textbf{(b)} The global forecasts module uses rollout technology to generate future forecasts; \textbf{(c)} The neural nested grid method specializes in regional high-resolution weather forecasts tasks; and \textbf{(d)} The ensemble forecasting module generates long-term forecast results.}
  \label{fig:ICML_yuan}
\vspace{-10pt}
\end{figure*}

\textbf{Problem Definition}\label{sec:problem}
In this study, we model weather forecasting as an autoregressive problem~\cite{lam2023learning}. At each time step $t$, we use the meteorological state comprising surface variables $\mathbf{X}_t$ and pressure level variables $\mathbf{P}_t$ to forecast the state at the next time step. We concatenate the surface and pressure level variables along the channel dimension to form the combined input: $\mathbf{Z}_t = [\mathbf{X}_t, \mathbf{P}_t] \in \mathbb{R}^{N \times d}$, where $N = H \times W$ represents the number of grid locations (nodes), and $d = d_x + d_p$ is the total number of variables. Here, $d_x$ and $d_p$ are the numbers of surface level and pressure level variables, respectively. In our setup, the initial input contains 69 variables: 4 surface level variables and 65 pressure level variables. Our model aims to forecast the combined variables at the next time step $\hat{\mathbf{Z}}_{t+1}$ using the current input $\mathbf{Z}_t$, capturing the spatiotemporal evolution of the atmosphere: $\hat{\mathbf{Z}}_{t+1} = \text{Model}(\mathbf{Z}_t; \Theta),$ where $\Theta$ denotes the model parameters. The training objective is to minimize the relative mean squared error (MSE) between the forecasts and the true values across all time steps: $    \min_{\Theta} \frac{1}{T} \sum_{t=0}^{T-1} \frac{ \left\| \hat{\mathbf{Z}}_{t+1} - \mathbf{Z}_{t+1} \right\|_2^2}{\left\| \mathbf{Z}_{t+1} \right\|_2^2}$. During inference, we adopt a rollout strategy to forecast longer sequences. Starting from the initial state $\mathbf{Z}_0$, the model recursively uses its previous forecasts as the next input: $ \hat{\mathbf{Z}}_{t+1} = \text{Model}(\hat{\mathbf{Z}}_t; \Theta), \quad t = 0, 1, 2, \dots, T-1.$ This strategy allows the model to generate extended weather forecasts using its own forecasts.

\subsection{Earth-specific Region Refined Graph Encoder}
In the encoder of~\method{}, inspired by~\cite{fortunato2022multiscale} ~\cite{lam2023learning}, we introduce an Earth-specific Region Refined Multi-scale Graph to improve the interaction of node features in complex dynamical systems. Inspired by the idea of multigrid methods~\cite{he2019mgnet}, we construct a multi-level Graph Neural Network architecture that includes grids of multiple granularities. Each grid has the same number of nodes but different grid densities, thereby capturing spatial features at different scales. Specifically, we define the multi-scale graph structure as:
\begin{equation}
\mathcal{G}=\left(\mathcal{V}^G, \mathcal{V}, \mathcal{E}^{(1)}, \mathcal{E}^{(2)}, \ldots, \mathcal{E}^{(L)}, \mathcal{E}^{(R)}, \mathcal{E}^{\mathrm{G} 2 \mathrm{M}}, \mathcal{E}^{\mathrm{M} 2 \mathrm{G}}\right),
\end{equation}
where ${\mathcal{V}}^{G}$ represents the set of lat-lon grid nodes, with a total of $N = H \times W$ nodes; $\mathcal{V}$ represents the mesh nodes. $\mathcal{E}^{(l)}$ denotes the edge set at the $l$-th scale, corresponding to grids of different granularities, where $l = 1, 2, \dots, L$, and $\mathcal{E}^{(R)}$ represents regional refined edges. $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$ and $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}$ are the unidirectional edges that connect lat-lon grid nodes and mesh nodes. All scales share the same set of nodes $\mathcal{V}$. More details can be found in Appendix \ref{Appendix:model_details}.

In the encoder, we first map the input meteorological state $\mathbf{Z}_t \in \mathbb{R}^{N \times d}$ to the initial node feature representation:
\begin{equation}
\mathbf{h}_i^{(0)} = \phi (\mathbf{Z}_{t, i}), \quad i = 1, 2, \ldots, N,
\end{equation}
where $\phi(\cdot)$ is the feature mapping function, and $\mathbf{Z}_{t,i}$ is the input feature at node $i$. Next, we iteratively update the node features on the multi-scale graph structure. At iteration $k$, the feature update formula for node $i$ is:
\begin{equation}
\mathbf{h}_i^{(k)} = \sigma \left( \sum_{l=1}^L \sum_{j \in \mathcal{N}_i^{(l)}} \mathbf{W}^{(l)} \mathbf{h}_j^{(k-1)} + \mathbf{b}^{(l)} \right),
\end{equation}
where $\mathcal{N}_i^{(l)}$ is the set of nodes adjacent to node $i$ at the $l$-th scale, $\mathbf{W}^{(l)}$ is the weight matrix at the $l$-th scale, $\mathbf{b}$ is the bias term, and $\sigma(\cdot)$ is the activation function. To enhance the forecasting accuracy in specific regions, we introduce a region-refined grid on the finest global grid. For nodes within the target region, we add denser edge connections to capture local high-frequency features. In this way, the update of node features not only considers global multi-scale information but also incorporates region-specific fine-grained information.

\subsection{Multi-stream Messaging}
To address the issue of information transmission between nodes in complex dynamic systems, we propose a module called \textit{Multi-stream Messaging} (MSM). This module consists of an adaptive messaging mechanism, including a dynamic multi-head gated edge update module and a multi-head node attention mechanism. And OneForecast includes 16 MSMs for messaging.

\textbf{Dynamic Multi-head Gated Edge Update Module.} Unlike traditional message passing methods based on MLPs, we introduce dynamic gating and multi-head mechanisms to control the information flow more precisely. For each edge, we concatenate its own features with those of the source node and the target node:
\begin{equation}
\mathbf{c}_i = \operatorname{Concat}\left( \mathbf{e}_i, \mathbf{h}_{s(i)}, \mathbf{h}_{d(i)} \right) \in \mathbb{R}^{D_e + 2 D_h},
\end{equation}
where $\mathbf{e}_i$ is the feature of edge $i$, $\mathbf{h}_{s(i)}$ and $\mathbf{h}_{d(i)}$ are the features of the source and target nodes of edge $i$, $D_e$ is the edge feature dimension, and $D_h$ is the node feature dimension. Next, we generate gating vectors through a two-layer MLP to regulate the information flow. Specifically, the gating vector is divided into three parts: edge feature update gate $g_e$, source node feature gate $g_s$, and destination node feature gate $g_d$. First, we perform the first layer linear transformation and activation:
\begin{equation}
\mathbf{z}_i = \operatorname{SiLU}\left( \mathbf{W}_1 \mathbf{c}_i + \mathbf{b}_1 \right),
\end{equation}
where $\mathbf{W}_1 \in \mathbb{R}^{h \times (D_e + 2 D_h)}$ is the weight matrix of the first layer, and $\mathbf{b}_1 \in \mathbb{R}^h$ is the bias term. Then, we perform the second layer linear transformation and Sigmoid activation:
\begin{equation}
\mathbf{g}_i = \sigma\left( \mathbf{W}_2 \mathbf{z}_i + \mathbf{b}_2 \right) \in \mathbb{R}^{3 H D},
\end{equation}
where, $\mathbf{W}_2 \in \mathbb{R}^{3 H D\times h}$ is the weight matrix of the linear transformation of the second layer, and $D$ is the feature dimension for each gate. For each head $h$, the gating values $\mathbf{g}_i^{(h,e)}$, $\mathbf{g}_i^{(h,s)}$, and $\mathbf{g}_i^{(h,d)}$ are vectors of dimension $D$, corresponding to the edge feature gate, source node feature gate, and destination node feature gate, respectively.

Subsequently, we use Edge Sum MLP (ESMLP) to perform linear transformation and nonlinear activation on the edge features to generate the updated edge features:
\begin{equation}
\mathbf{e}_i' = \operatorname{ESMLP}_e\left( \mathbf{e}_i, \mathbf{h}_{s(i)}, \mathbf{h}_{d(i)} \right) \in \mathbb{R}^{D_e'},
\end{equation}
where $D_e'$ is the dimension of the updated edge features. Finally, we combine the gating vectors and the updated edge features to generate the final updated edge features through weighted averaging and residual connections:
\begin{equation}
\begin{aligned}
\mathbf{e}_i^{\text{new}} = &\frac{1}{3} \sum_{h=1}^H \bigg(
\mathbf{g}_i^{(h,e)} \odot \mathbf{e}_i^{\prime}
+ \mathbf{g}_i^{(h,s)} \odot \mathbf{h}_{s(i)} \\
&\quad + \mathbf{g}_i^{(h,d)} \odot \mathbf{h}_{d(i)}
\bigg) + \mathbf{e}_i,
\end{aligned}
\end{equation}
where $\odot$ denotes element-wise multiplication.

\textbf{Multi-head Node Attention Mechanism.} Compared to traditional message passing mechanisms, multi-head attention mechanisms can more precisely capture complex dependencies between nodes and dynamically adjust the way information is aggregated through attention weights. For each edge $e_i = (j \rightarrow k)$, we use an MLP to calculate the attention score:
\begin{equation}
\mathbf{a}_i = \operatorname{MLP}_a\left( \mathbf{e}_i^{\text{new}} \right) \in \mathbb{R}^H,
\end{equation}
then, we normalize the attention scores:
\begin{equation}
\alpha_i^{(h)} = \frac{\exp\left( \mathbf{a}_i^{(h)} \right)}{ \sum_{e_j \in \mathcal{E}(k)} \exp\left( \mathbf{a}_j^{(h)} \right) }, \quad \forall h = 1, 2, \dots, H,
\end{equation}
where $\mathcal{E}(k)$ denotes the set of all incoming edges to node $k$, and $\alpha_i^{(h)}$ is the attention weight of the $h$-th attention head for edge $e_i$. Next, we perform weighted aggregation of the edge features. For each node $k$, based on the attention weights, we compute the weighted sum of the features of all edges incoming to node $k$, generating the aggregated feature for each head:
\begin{equation}
\mathbf{m}_k^{(h)} = \sum_{e_i \in \mathcal{E}(k)} \alpha_i^{(h)} \cdot \mathbf{e}_i^{\text{new}} \in \mathbb{R}^{D_e'},
\end{equation}
then, we flatten and concatenate the aggregated features from all heads:
\begin{equation}
\mathbf{M}_k = \operatorname{Flatten}\left[ \mathbf{m}_k^{(1)}, \mathbf{m}_k^{(2)}, \dots, \mathbf{m}_k^{(H)} \right] \in \mathbb{R}^{D_e' \cdot H}.
\end{equation}
Finally, we concatenate the aggregated edge features with the original node features and, through an MLP, perform a nonlinear transformation to generate the updated node features:
\begin{equation}
\mathbf{h}_k^{\text{new}} = \operatorname{MLP}_n\left( \operatorname{Concat}\left( \mathbf{M}_k, \mathbf{h}_k \right) \right) + \mathbf{h}_k.
\end{equation}
In summary, in each iteration, we use the multi-stream messaging module to update the node features. Specifically, the node feature update formula is:
\begin{equation}
\mathbf{h}_i^{(k)} = \sigma \left( \sum_{l=1}^{L} \operatorname{MSM}\left( \mathbf{h}_i^{(k-1)}, \mathcal{E}^{(l)} \right) + \mathbf{b} \right),
\end{equation}
where $\operatorname{MSM}$ represents the aforementioned multi-stream messaging operation, $\mathcal{E}^{(l)}$ is the set of edges at the $l$-th scale, and $\sigma(\cdot)$ is the activation function. In the region-refined graph structure, for nodes within the target region, we additionally consider the set of edges within the region $\mathcal{E}^{\text{region}}$ to capture finer local information.

\textbf{Theoretical Analysis.} From a theoretical perspective, we explain why our method helps capture high-frequency information. This enhances long-term prediction ability and improves the ability to detect extreme events.
\begin{theorem}\label{thm:bias}
\textbf{High-pass Filtering Property of Multi-stream Messaging.} Considering the improved multi-stream message passing mechanism, suppose the graph signal $\bm{f} \in \mathbb{R}^N$ has a spectrum $\hat{\bm{f}} = \bm{U}^\top \bm{f}$ under the graph Fourier basis $\bm{U} = [\bm{u}_1, ..., \bm{u}_N]$, where $\bm{L} = \bm{U} \bm{\Lambda} \bm{U}^\top$ is the normalized graph Laplacian matrix and $\bm{\Lambda} = \operatorname{diag}(\lambda_1, ..., \lambda_N)$ is its eigenvalue diagonal matrix ($0 \leq \lambda_1 \leq ... \leq \lambda_N \leq 2$). Define the frequency response function of the message passing operator as $\rho: \lambda \mapsto \mathbb{R}$. If the dynamic gating weights satisfy:

\begin{equation}
    g^{(h,e)}_i, g^{(h,s)}_i, g^{(h,d)}_i \propto |\lambda_i - 1| + \epsilon \quad (\epsilon > 0)
\end{equation}

then there exist constants $\alpha > 0$ and $\kappa > 0$ such that the frequency response of the operator satisfies:

\begin{equation}
    \rho(\lambda_i) \geq \alpha |\lambda_i - 1| \quad \text{and} \quad \rho(\lambda_i) \geq \kappa \lambda_i
\end{equation}

that is, the operator is a strictly high-pass filter.
\end{theorem}
The proof of Theorem~\ref{thm:bias} can be found in Appendix~\ref{appendix_theorems}.
\subsection{Decoding and Optimization}
The decoder's goal is to decode the latent information back to meteorological variables on the latitude-longitude grid. We obtain the updated feature representation $\mathbf{h}_i$ for each node. For each node $i$, the decoder applies the mapping function:
\begin{equation}
\hat{\mathbf{Z}}_{t+1, i} = \psi(\mathbf{h}_i),
\end{equation}
where $\psi(\cdot)$ is an MLP that converts the latent node features into the predicted variables $\hat{\mathbf{Z}}_{t+1, i}$ for the next time step.

We use relative $L_2$ loss function for model training. The loss function is defined as:
\begin{equation}
\mathcal{L} = \frac{1}{K H W} \sum_{k=1}^K  \sum_{i=1}^H \sum_{j=1}^W \frac{\left( \hat{x}_{i, j, k}^{t + l \delta t} - x_{i, j, k}^{t + l \delta t} \right)^2}{\left( x_{i, j, k}^{t + l \delta t} \right)^2},
\end{equation}
where $\hat{x}_{i, j, k}^{t + l \delta t}$ and $x_{i, j, k}^{t + l \delta t}$ are the predicted and true values for variable (channel) $k$ at spatial location $(i, j)$ and time $t + l \delta t$; $K$ is the number of variables (channels); $H$ and $W$ are the height and width of the spatial dimensions, respectively; $\delta t$ is the time interval of single-step prediction (we use $\delta t = 6$ hours).

\subsection{Downstream Tasks}

We consider three principal downstream tasks:

\textbf{Global Weather Forecasting.} As detailed in Section~\ref{sec:problem} and illustrated in Figure~\ref{fig:ICML_yuan}(b), we employ a rollout approach during inference, using the trained model for multi-step extrapolation. Specifically, starting from the initial state~$\mathbf{Z}_0$, the model recursively uses its previous predictions as inputs for subsequent time steps, generating a sequence of future global weather forecasts.

\textbf{Regional High-Resolution Forecasting.} To enhance the accuracy of high-resolution forecasts in specific regions, we propose a neural nested grid method, illustrated in Figure~\ref{fig:ICML_yuan}(c). This method combines global low-resolution future forecasts with regional high-resolution data to produce detailed forecasts for the target region. We first input the global low-resolution data at time~$t$ into the pre-trained global model to obtain the global forecasts $\hat{\mathbf{Z}}_{t+1}^{\text{global}}$ at time~$t+1$. We extract $\hat{\mathbf{Z}}_{t+1}^{\text{global1}}$ from $\hat{\mathbf{Z}}_{t+1}^{\text{global}}$, which shares the same spatial range as the region, and $\hat{\mathbf{Z}}_{t+1}^{\text{global2}}$, which includes the boundary of the region (the boundary are defined as two grid points around the region). Both $\hat{\mathbf{Z}}_{t+1}^{\text{global1}}$ and $\hat{\mathbf{Z}}_{t+1}^{\text{global2}}$ are then interpolated to match the resolution of the high-resolution regional data, which are concatenate as $\hat{\mathbf{Z}}_{t+1}^{\text{global}}$ to acted as global force. We then combine the regional high-resolution data at time~$t$ with the $\hat{\mathbf{Z}}_{t+1}^{\text{global}}$ to form the input to the regional model. The global forecasts provide the necessary boundary conditions for the regional forecasts. The regional model then produces the high-resolution forecasts for the regional state at time~$t+1$:
\begin{equation}\small
       \hat{\mathbf{Z}}_{t+1}^{\text{region}} = \text{Model}_{\text{region}}\left( \operatorname{Concat}\left( \hat{\mathbf{Z}}_{t+1}^{\text{global}},\, \mathbf{Z}_t^{\text{region}} \right);\, \Theta_{\text{region}} \right).
\end{equation}

\textbf{Long-Term and Esemble Weather Forecasting.} The initial condition of the atmospheric state is uncertain, so reasonable quantification of this uncertainty is conducive to improve to forecast performance. In this work, we set N=50. To account for the uncertainty in the atmospheric initial state for long-term ensemble forecasting, we generate~$N$ perturbed initial conditions~$\mathbf{Z}_0^{(n)}$ by adding Perlin noise~$\varepsilon^{(n)}$ to~$\mathbf{Z}_0$~\cite{chen2023fuxi}. Each perturbed initial condition is input into the model, and through recursive rollout over~$T$ time steps, we obtain individual forecasts~$\hat{\mathbf{Z}}_{t+1}^{(n)}$. Finally, at each time step~$t+1$, we compute the ensemble mean prediction~$ \hat{\mathbf{Z}}_{t+1}^{\text{ensemble}} = \frac{1}{N} \sum_{n=1}^N \hat{\mathbf{Z}}_{t+1}^{(n)}$ by averaging the forecasts from all~$N$ ensemble members. 

% \subsection{Theoretical Analysis}
% To illustrate the ability of graph neural networks with different grid resolutions to express data information, we denote the continuous form of the input data as \( f(x) \). For simplicity, assume the data is one-dimensional, with \( x \in [-\pi, \pi] \). For the input data, we use the trigonometric functions \( \{1, \cos t, \sin t, \cos 2t, \sin 2t, \cdots \} \) as an orthogonal basis, and expand the input data in this basis to obtain signals of different frequencies:
%  \[
%  x(t) = \frac{a_0}{2} + \sum_{n=1}^{\infty} \left( a_n \cos nt + b_n \sin nt \right)
%  \]

%  Assume the grid on the data is \( \{x_1, x_2, \cdots, x_N\} \in [-\pi, \pi] \). For any sampling point \( x_i \), we have:
%  \[
%  \cos(k_1 x_i) \approx \cos(k_2 x_i)
%  \]
%  where
%  \[
%  k_1 \approx k_2 - \frac{2\pi n}{x_i}, \quad n \in \mathbb{Z}
%  \]
%  This indicates that at the point \( x_i \), signals with frequencies \( k_1 \) and \( k_2 \) will experience aliasing.

%  Furthermore, for
%  \[
%  k_1' \approx k_2' - \frac{2\pi n}{\prod_{i=1}^{N} x_i}, \quad n \in \mathbb{Z}
%  \]
%  the signals with frequencies \( k_1' \) and \( k_2' \) will experience aliasing at all sampling points, meaning that the discrete data will cause the model to misinterpret the high-frequency signal of \( k_2' \) as the low-frequency signal of \( k_1' \).

%  To precisely describe this phenomenon, we introduce the famous \textbf{Nyquist-Shannon sampling theorem}:
%  \begin{theorem}
%  When the sampling rate of a continuous-time signal is less than twice the highest frequency component in the signal's Fourier series representation, aliasing will occur.
%  \end{theorem}
%  This theorem indicates that to capture higher-frequency signals through discrete sampling points, the sampling frequency must be increased. In the process of converting data into grids, this means that to allow a neural network to learn higher-frequency information, a finer grid is required. That is, graph neural networks learn high-frequency information more accurately on finer grids than on coarser grids, reducing distortion.