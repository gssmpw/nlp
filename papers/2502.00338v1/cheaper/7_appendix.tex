\section{Proofs of Theorems}
\label{appendix_theorems}
\begin{theorem}[High-pass Filtering Property of Multi-stream Messaging]
\label{thm:bias}
Consider an improved multi-stream message passing mechanism. 
Let a graph signal $\mathbf{f} \in \mathbb{R}^N$ have a spectrum 
$\hat{\mathbf{f}} = \mathbf{U}^\top \mathbf{f}$ 
under the graph Fourier basis 
$\mathbf{U} = [\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_N]$, 
where 
$\mathbf{L} = \mathbf{U}\boldsymbol{\Lambda} \mathbf{U}^\top$ 
is the normalized graph Laplacian and 
$\boldsymbol{\Lambda} = \mathrm{diag}(\lambda_1, \lambda_2, \ldots, \lambda_N)$ 
with 
$0 \le \lambda_1 \le \cdots \le \lambda_N \le 2.$ 
Define the frequency response function of the message passing operator 
by $\rho: \lambda \mapsto \mathbb{R}.$ 
If the dynamic gating weights satisfy
\begin{equation}
g^{(h,e)}_i, \; g^{(h,s)}_i, \; g^{(h,d)}_i 
\;\propto\; 
\bigl|\lambda_i - 1\bigr| \;+\; \epsilon
\quad (\epsilon > 0),
\end{equation}
then there exist constants $\alpha > 0$ and $\kappa > 0$ such that 
the frequency response $\rho(\lambda_i)$ of the operator satisfies
\begin{equation}
\rho(\lambda_i) \;\ge\; \alpha\,\bigl|\lambda_i - 1\bigr|
\quad \text{and} \quad
\rho(\lambda_i) \;\ge\; \kappa\,\lambda_i,
\end{equation}
which means the operator is a strictly high-pass filter.
\end{theorem}

\begin{proof}
We consider a multi-stream message passing operator that we denote by 
$\mathbf{G}$. 
This operator depends on both node features and dynamic gating on edges. 
We let $\mathbf{f} \in \mathbb{R}^N$ be an arbitrary graph signal, and we 
write its graph Fourier transform as 
$\hat{\mathbf{f}} = \mathbf{U}^\top \mathbf{f}$, 
where 
$\mathbf{L} = \mathbf{U} \boldsymbol{\Lambda} \mathbf{U}^\top$ 
is the normalized Laplacian and 
$\boldsymbol{\Lambda} = \mathrm{diag}(\lambda_1, \ldots, \lambda_N)$ 
with 
$0 \le \lambda_1 \le \cdots \le \lambda_N \le 2$.

\textbf{Step 1: Integral analogy of gating on a discrete graph.}  
We first recall that on a continuous domain, a gating operator often admits a 
representation of the form
\[
(\mathbf{G}\mathbf{f})(x) 
\;=\; \int \mathcal{K}(x,\xi)\,\mathbf{f}(\xi)\,d\xi,
\]
where $\mathcal{K}(x,\xi)$ is a kernel that encodes the gating weights. 
On a discrete graph, the integral turns into a finite sum. 
Hence for node $i$, we write
\[
(\mathbf{G}\mathbf{f})_i 
\;=\; \sum_{j\in \mathcal{N}_i} \mathcal{K}_{ij}\,f_j,
\]
where $\mathcal{N}_i$ denotes the neighbors of node $i$, and 
$\mathcal{K}_{ij}$ depends on the dynamic gating parameters 
$\bigl(g^{(h,e)}_i, g^{(h,s)}_i, g^{(h,d)}_i\bigr)$. 
We assume these gating parameters scale proportionally to 
$\bigl|\lambda - 1\bigr| + \epsilon$, 
which implies larger weights when $\lambda$ is around high or mid-frequency 
regions.

\textbf{Step 2: Spectral decomposition of the operator.}  
We decompose $\mathbf{f}$ in the eigenbasis of $\mathbf{L}$:
\[
\mathbf{f} 
\;=\; \sum_{\ell=1}^N \hat{f}_\ell \,\mathbf{u}_\ell, 
\quad 
f_j 
\;=\; \sum_{\ell=1}^N \hat{f}_\ell\,(\mathbf{u}_\ell)_j.
\]
The operator $\mathbf{G}$ acts on $\mathbf{u}_\ell$ with some gain factor 
$\rho(\lambda_\ell)$, which we call the frequency response. 
In other words, we write
\[
\mathbf{G}\,\mathbf{u}_\ell 
\;=\; \rho(\lambda_\ell)\,\mathbf{u}_\ell.
\]
Thus the value of $\rho(\lambda_\ell)$ reveals how the operator scales 
the amplitude of the $\ell$-th eigenmode.

\textbf{Step 3: High-pass filtering behavior from gating design.}  
We now analyze the effect of gating weights 
$g^{(h,e)}_i,\, g^{(h,s)}_i,\, g^{(h,d)}_i$ 
that satisfy 
\[
g^{(h,e)}_i,\, g^{(h,s)}_i,\, g^{(h,d)}_i 
\;\propto\;
|\lambda_i - 1| + \epsilon.
\]
Because $\lambda_i \in [0,2]$, when $\lambda_i$ is close to $2$, it 
represents a high-frequency component on the graph. 
In that regime, the gating weights become larger, and the message passing 
operator $\mathbf{G}$ amplifies those components. 
Similarly, when $\lambda_i$ is near $1$, the factor $|\lambda_i - 1|$ 
can still be significant enough to enhance mid-to-high frequencies. 
Conversely, for $\lambda_i$ near $0$ (low frequency), 
the gating is relatively small and thus tends to suppress those components.

\textbf{Step 4: Combining inequalities to show strictly high-pass.}  
We combine partial inequalities for different ranges of $\lambda_i$. 
Since $0 \le \lambda_i \le 2$, 
we use the gating assumption to show there are positive constants 
$\alpha$ and $\kappa$ such that
\[
\rho(\lambda_i) 
\;\ge\; 
\alpha \,\bigl|\lambda_i - 1\bigr|
\quad\text{and}\quad
\rho(\lambda_i) 
\;\ge\; 
\kappa \,\lambda_i.
\]
Hence the operator $\mathbf{G}$ behaves like a high-pass filter, because 
it provides higher gain to higher (or mid-high) frequency components and 
less gain to low-frequency components. 
Therefore, $\mathbf{G}$ is a strictly high-pass operator in the graph spectral 
domain.

This completes the proof of Theorem~\ref{thm:bias}.
\end{proof}




\section{Data Details}
\label{appendix:data}
\subsection{Dataset}
    %
    In this section, we introduce the dataset used in this paper detailedly. For the global forecasting, we conduct experiments on the weatherbench2~\cite{rasp2024weatherbench} benchmark, a subset of ERA5 reanalysis data~\cite{hersbach2020era5}. The weatherbench2 benchmark we used is the version of 1.5° resolution (121 × 240), which spans from 1959 to 2020. This subset contains 5 variables (Z, Q, T, U, V) with 13 pressure levels (50 hPa, 100 hPa, 150 hPa, 200 hPa, 250 hPa, 300 hPa, 400 hPa, 500 hPa, 600 hPa, 700 hPa, 850 hPa, 925 hPa and 1,000 hPa) and 4 variables (U10M, V10M, T2M, MSLP) with surface level. For the regional forecasting, a higher resolution data (0.25° resolution) of ERA5 is also used, which can be downloaded from \url{https://cds.climate.copernicus.eu/}, the official website of Climate Data Store (CDS). All the data we used are shown in Table \ref{tab:appendix_data}. For both global and regional forecasts, we use the data from 1959 to 2017 for training, 2018 to 2019 for validating, and 2020 for testing.

    \begin{table*}[ht]
    \caption{The data details in this work.}
    \label{tab:appendix_data}
    \vspace{-5pt}
    \vskip 0.13in
    \centering
    \begin{small}
        \begin{sc}
            \renewcommand{\multirowsetup}{\centering}
            \setlength{\tabcolsep}{2pt} % Adjust the spacing between columns if needed
           \begin{tabular}{l|cccccc}
           \toprule
        Task                    & \begin{tabular}[c]{@{}c@{}}Variable\\ Name\end{tabular} & Layers & \begin{tabular}[c]{@{}c@{}}Spatial\\ Resolution\end{tabular} & Dt& \begin{tabular}[c]{@{}c@{}}Lat-Lon\\ Range\end{tabular} & Time           \\ \midrule
        \multirow{9}{*}{Global} & Geopotential (Z)                                        & 13     & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & Specific Humidity (Q)                                   & 13     & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & Temperature (T)                                         & 13     & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & U Component of Wind (U)                                 & 13     & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & V Component of Wind (V)                                 & 13     & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & 10 Metre U Wind Component (U10M)                         & 1      & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & 10 Metre V Wind Component (V10M)                         & 1      & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & 2 Metre Temperature (T2M)                                & 1      & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\
                        & Mean Sea Level Pressure (MSLP)                          & 1      & 1.5°                                                         & 6h & -90°S-180°W$\sim$90°N180°E                              & 1959$\sim$2020 \\  \midrule
        \multirow{2}{*}{Regional}                & Mean Sea Level Pressure (MSLP)                          & 1      & 0.25°                                                        & 6h & 7.5°W114°E$\sim$ 36°W172.5°E                                                       & 1959$\sim$2020 \\
        & 10 Metre U Wind Component (U10M)                          & 1      & 0.25°                                                        & 6h & 7.5°W114°E$\sim$ 36°W172.5°E                                                        & 1959$\sim$2020 \\
        \bottomrule
        \end{tabular}
        \end{sc}
	\end{small}
    \label{tab:data}
    \end{table*}
\subsection{Data preprocessing}
    %
    Different atmospheric variables have large differences in numerical magnitude. To allow the model focusing on predictions rather than learning the differences between variables, we normalized the data before putting the data into the model. We calculated the mean and standard deviation of all variables using data from 1959 to 2017 (training set). Each variable has a corresponding mean and standard deviation. Before putting the data into the model, we subtracted the change from the respective mean and divided it by the standard deviation.
    
\section{Algorithm}
    %
    We summarize the overall framework of OneForecast in Algorithm \ref{alg:OneForecast_global}.

    % %
    % \begin{algorithm}[t]
    % \caption{OneForecast Framework for Global Weather Forecasting.}
    % \label{alg:OneForecast_global}
    %     \begin{algorithmic}
    %         \renewcommand{\algorithmicrequire}
    %     \end{algorithmic}
    % \end{algorithm}
    % %

    \begin{algorithm}[ht]
        \caption{OneForecast Framework for Global Weather Forecast}
        \label{alg:OneForecast_global}
            \begin{algorithmic}[1]
                \renewcommand{\algorithmicrequire}{\textbf{Require:}}
        	\REQUIRE
        	Initial atmospheric condition $Z_t$.
        	\ENSURE
        	Next step atmospheric state $Z_{t+1}$.
                \STATE  Initialize OneForecast
                \REPEAT
                \STATE \textbf{Encoder}
                \STATE  Embedding features of grid nodes $Z_t$, mesh nodes $\mathcal{V}$, mesh edges $\mathcal{E}$, grid to mesh edges $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$, and mesh to grid edges $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}$ into latent space using respective MLP: ($\mathcal{V}_f^G$, $h$, $\mathcal{E}_f$, $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f$, $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}_f$) = MLPs($Z_t$, $\mathcal{V}$, $\mathcal{E}$, $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$, $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}$)
                \STATE Project the atmospheric state from the lat-lon grid into the mesh nodes: ${\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f}^{\prime} = \mathrm{ESMLP}(\mathcal{V}_f^G, h, \mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f)$,

                $h^{\prime} = \mathrm{MLP}_{e1}(h, \sum{\mathcal{E}^{\mathrm{G}2\mathrm{M}}_f}^{\prime})$
                \STATE Update grid node feature: ${\mathcal{V}_f^G}^{\prime} = \mathrm{MLP}_{e2}(\mathcal{V}_f^G)$
                \STATE Apply residual connection to update the feature of grid to mesh edge, mesh node, and grid node again: ${\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f} = {\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f}^{\prime} + {\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f}$, $h^{\prime} = h^{\prime} + h$, ${\mathcal{V}_f^G} = {\mathcal{V}_f^G}^{\prime} + {\mathcal{V}_f^G}$
                \STATE \textbf{Multi-stream Messaging}
                \STATE Apply dynamic multi-head gated edge update module (DMG) to update edge feature: $\mathcal{E}_f^{\prime} = DMG(\mathcal{E}_f, h_s, h_r)$
                \STATE Apply multi-head node attention mechanism (MHA) to update mesh node feature: $h^{\prime} = MHA(h, \sum{\mathcal{E}_f}^{\prime})$
                \STATE Apply residual connection to update the feature of edge and mesh node: $\mathcal{E}_f = \mathcal{E}_f^{\prime} + \mathcal{E}_f$, $h = h^{\prime} + h$
                \STATE \textbf{Decoder} Project the feature from mesh back to lat-lon grid: ${\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}_f}^{\prime} = \mathrm{ESMLP}(\mathcal{V}_f^G, \mathcal{E}_f, \mathcal{E}^{\mathrm{M} 2 \mathrm{G}}_f)$, ${\mathcal{V}_f^G}^{\prime} = {\mathrm{MLP}_{d1}}(\mathcal{V}_f^G, \sum{\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}_f}^{\prime})$, $\mathcal{V}_f^G={\mathrm{MLP}}_{d2}(\mathcal{V}_f^{G^{\prime}}+\mathcal{V}_f^G)$, $Z_{t+1} = Z_{t} + {\mathcal{V}_f^G}$
                
                \UNTIL converged
                \STATE \textbf{return} $OneForecast$
            \end{algorithmic}
        \end{algorithm}


\section{Model Details for Global Forecasts}
\label{Appendix:model_details}

\subsection{ Earth-specific Region Refined Multi-scale Graph}
    The graph used in OneForecast can be defined as: $\mathcal{G}(\mathcal{V}^G,\mathcal{V}, \mathcal{E}, \mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$, $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}})$.

\textbf{Grid Nodes.}  $\mathcal{V}^G$ is the ensemble of grid nodes, which contains $120\times240=28800$ nodes for 1.5° global data in global forecast task. And each node consists of 69 atmospheric features (5 variables at 13 pressure levels and 4 variables at surface level, $5\times13+4=69$). Since we just consider 1 step historical state, the input features of OneForecast are 69. For regional forecast, the region size can be arbitrary within the permission of GPU memory. For simplicity, we choose the region size of $120\times240$ from 0.25° data, the node is still $120\times240=28800$.

\textbf{Mesh Nodes.}  $\mathcal{V}$ is the ensemble of mesh nodes, which contains multi-scale mesh nodes of different fineness and region refined mesh nodes that cover the global area. The mesh nodes are distributed over a refined icosahedron that has undergone five levels of subdivision, and the coarsest icosahedron consists of 12 vertices and 20 triangular faces. By dividing each triangular face into four smaller triangles, an additional node is generated at the midpoint of each edge. The new nodes are then projected back onto the unit sphere, gradually refining the grid. To enhance the forecasting performance in key regions, we further refine specific areas of the finest mesh, achieving localized mesh densification. For the global forecast task, we refine the 2 areas: 0°N105°E$\sim$30°N160°E and 10°N-95°W$\sim$30°N-35°W. The features of each node include the cosine value of the latitude, as well as the sine and cosine values of the longitude. We only keep the finest mesh nodes, since the nodes on the coarse mesh are its subset. In total, the graph structure of OneForecast comprises 12337 mesh nodes, each characterized by three features. 

\textbf{Mesh Edges.} $\mathcal{E}$ are the bidirectional edges that connect mesh nodes (sender and receiver nodes). Similar to mesh nodes, there are corresponding edges for each scale of mesh, and $\mathcal{E}$ is the ensemble of multi-scale edges. And the features of each edge include the length of edge, the 3D position difference between sender and receiver nodes. In total, OneForecast comprises 98296 mesh edges, each characterized by four features.

\textbf{Grid2Mesh Edges.} $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$ are the the unidirectional edges that used in the encoder, which connect grid and mesh nodes. To ensure that each grid node has a corresponding mesh node connected to it, we add $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$ to grid nodes and mesh nodes if the distance between them is less than or equal to 0.6 times the edge length of the finest $\mathcal{E}$. Similar to mesh edge $\mathcal{E}$, each grid2mesh edge comprises 4 features, and OneForecast has 49233 grid2mesh edges in total.

\textbf{Mesh2Grid Edges.} $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}$ are the unidirectional edges that used in the decoder, which connect grid and mesh nodes. For each grid node, we find the triangle face containing it on the finest mesh and connect 3 mesh nodes to it. Similar to other edges, each mesh2grid edge has 4 features. In total, OneForecast has 86,400 mesh2grid edges.

\subsection{Encoder}
    %
    We first apply embedder MLP to map the data to the latent space, which can be defined as:
    %
    %
    \begin{equation}
        MLP = LN(SiLU(Linear(x_{embedder}))),
    \end{equation}
    %
    %
    \begin{equation}
        (\mathcal{V}_f^G, h, \mathcal{E}_f, \mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f, \mathcal{E}^{\mathrm{M} 2 \mathrm{G}}_f) = MLPs(Z_t, \mathcal{V}, \mathcal{E}, \mathcal{E}^{\mathrm{G} 2 \mathrm{M}}, \mathcal{E}^{\mathrm{M} 2 \mathrm{G}}),
    \end{equation}
    %
    %
    where, $x_{embedder}$ is the input of embedder MLP. For the linear function, we set the latent dim to 512. $SiLU(\cdot)$ is the SiLU activation function, $LN(\cdot)$ is the layernorm function. $Z_t$, $\mathcal{V}$, $\mathcal{E}$, $\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}$, and $\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}$ are embedded features of grid nodes, mesh nodes, mesh edges, grid to mesh edges, and mesh to grid edges.
    %

    %
    We then project the atmospheric state from the lat-lon grid into the mesh nodes. Specifically, we first update the edge features through an Edge Sum MLP (ESMLP): 
    %
   

    %
    \begin{equation}
        \mathcal{E}_f^{\mathrm{G} 2 \mathrm{M} \prime}=\mathbf{W}_e \mathcal{E}_f^{\mathrm{G} 2 \mathrm{M}}
    \end{equation}
    %

    %
    \begin{equation}
        \mathcal{V}_f^{G \prime}=\mathbf{W}_s \mathcal{V}_f^G,
    \end{equation}
    %

    %
    \begin{equation}
        h^{\prime}=\mathbf{W}_d h+\mathbf{b}_d,
    \end{equation}

    %
    \begin{equation}
        \mathbf{h}_{\mathrm{sum}}=\mathcal{E}_f^{\mathrm{G} 2 \mathrm{M} \prime}+\mathcal{V}_f^{G \prime}+h^{\prime},
    \end{equation}
    %

    %
    \begin{equation}
        {\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f}^{\prime}=\operatorname{LN}\left(\mathbf{W}_{\mathrm{ESMLP}} \sigma\left(h_{out}\right)+\mathbf{b}_{\mathrm{ESMLP}}\right),
    \end{equation}
    %
    %
    where, $\mathbf{W}_e$, $\mathbf{W}_s$, $\mathbf{W}_d$ are the linear transformation matrix of grid2mesh edge features, grid node feature, and mesh node features. $\mathbf{W}_{\text {ESMLP }}$ is the linear transformation matrix of output layer. $b_d$ is the bias of mesh node during linear transformation, $b_{ESMLP}$ is the bias vector of ESMLP. In summary, the grid2mesh edge update process can be define as:
    
    %
    \begin{equation}
         {\mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f}^{\prime} = \mathrm{ESMLP}(\mathcal{V}_f^G, h, \mathcal{E}^{\mathrm{G} 2 \mathrm{M}}_f).
    \end{equation}
    %

    %
    After updating the grid2node features, we update the mesh node features using another MLP:


     %
    \begin{equation}
        h^{\prime} = \mathrm{MLP_{e1}}(h, \sum{\mathcal{E}^{\mathrm{G}2\mathrm{M}}_f}^{\prime}),
    \end{equation}
    %
    %
    where, $\sum \mathcal{E}_f^{\mathrm{G} 2 \mathrm{M}^{\prime}}$ are the edges that arrives at mesh node.

    %
    Then, we update the grid node features using another MLP:
    %

    %
    \begin{equation}
\mathcal{V}_f^{G^{\prime}}=\operatorname{MLP}_{e2}\left(\mathcal{V}_f^G\right).
    \end{equation}
    %
    Finally, residual connections are applied to update the feature of grid to mesh edge, mesh node, and grid node again.

\subsection{Multi-stream Messaging}
    %
    The proposed multi-stream messaging is implemented by an adaptive messaging mechanism, which contains a dynamic multi-head gated edge update module and a multi-head node attention module. This part has been introduced in detail in the main text, so we only added the hyperparameter settings here. For the dynamic multi-head gated edge update module, the dimensions of the gating vector are set to 64. In the multi-head node attention module, the $MLP_a$ used to calculate the attention score consists of a linear layer, a SiLU activation function, a linear layer, and a Sigmoid function. The hidden dimension of the linear layer is 64.
    %
\subsection{Decoder}
    In the decoder, we map the feature from mesh back to lat-lon grids, similar to encoder, we first up the mesh2grid features:
    %
    \begin{equation}
        \mathcal{E}_f^{\mathrm{M} 2 \mathrm{G}^{\prime}}=\operatorname{ESMLP}\left(\mathcal{V}_f^G, \mathcal{E}_f, \mathcal{E}_f^{\mathrm{M} 2 \mathrm{G}}\right).
    \end{equation}
    %
    
    %
    Then, we update the grid node features:
    %

    %
    \begin{equation}
        {\mathcal{V}_f^G}^{\prime} = {\mathrm{MLP}}_{d1}(\mathcal{V}_f^G, \sum{\mathcal{E}^{\mathrm{M} 2 \mathrm{G}}_f}^{\prime}),
    \end{equation}
    %
    %
    where, $\sum \mathcal{E}_f^{\mathrm{M} 2 \mathrm{G}^{\prime}}$ are the edges that arrives at grid node.
    %

    %
    After that, a residual connection is applied to update the grid node features again:
    %
    \begin{equation}
    \mathcal{V}_f^{G^{\prime}}=\mathcal{V}_f^{G^{\prime}}+\mathcal{V}_f^G.
    \end{equation}
    %

    %
    Finally, we apply a MLP to predict the next step results:
    \begin{equation}
    \mathcal{V}_f^G=\operatorname{MLP}_{d2}\left(\mathcal{V}_f^{G^{\prime}}+\mathcal{V}_f^G\right).
    \end{equation}

    
    
\section{Experiments Details}
\subsection{Evaluation Metric}
\label{appendix_metrics}
    %
    We utilize two metrics, RMSE (Root Mean Square Error) and ACC (Anomalous Correlation Coefficient), to evaluate the forecasting performance, which can be defined as:
    %

    %
    \begin{equation}\small
    {RMSE}(k, t) = \sqrt{\frac{\sum\limits_{i=1}^{N_{\text{lat}}} \sum\limits_{j=1}^{N_{\text{lon}}} L(i) \left( \hat{\mathbf{A}}_{ij,t}^k - \mathbf{A}_{ij,t}^k \right)^2}{N_{\text{lat}} \times N_{\text{lon}}}},
\end{equation}
    %

    %
    \begin{equation}\small
    \operatorname{ACC}(k, t) = \sqrt{\frac{\sum\limits_{i=1}^{N_{\text{lat}}} \sum\limits_{j=1}^{N_{\text{lon}}} L(i) \hat{\mathbf{A'}}_{ij,t}^k \mathbf{A'}_{ij,t}^k}{\sum\limits_{i=1}^{N_{\text{lat}}} \sum\limits_{j=1}^{N_{\text{lon}}} L(i) \left( \hat{\mathbf{A'}}_{ij,t}^k \right)^2 \times \sum\limits_{i=1}^{N_{\text{lat}}} \sum\limits_{j=1}^{N_{\text{lon}}} L(i) \left( \mathbf{A'}_{ij,t}^k \right)^2}},
\end{equation}
    %
    %
    where $\mathbf{A}_{i, j, t}^v$ represents the value of variable v at horizontal coordinate $(i, j)$ and time t. Latitude-dependent weights are defined as $L(i)=N_{\text {lat }} \times \frac{\cos \phi_i}{\sum_{i’=1}^{N_{\text {lat}}} \cos \phi_{i’}}$, where $\phi_i$ is the latitude at index i. The anomaly of $A$, denoted as $A'$, is computed as the deviation from its climatology, which corresponds to the long-term mean of the meteorological state estimated from 59 years of training data. To evaluate model performance, RMSE and ACC are averaged across all time steps and spatial coordinates, providing summary statistics for variable $v$ a given lead time $\Delta t$.

\subsection{Model Training}
%
We train all baseline models and OneForecast using the same training framework. We set the total model training epochs to 110, the initial learning rate is 1e-3, and use the cosine annealing scheduler to adjust the learning rate until the model converged. The model codes of Pangu, Graphcast, and Fengwu we used are released by NVIDIA modulus (\url{https://github.com/NVIDIA/modulus}). The model code of Fuxi is obtained by sending an email to the author. For all models, we select the checkpoint that performed best on the validation set for comparative analysis.
\subsection{Typhoon Tracking}   
To track the eye of a tropical cyclone, we follow \cite{bi2023accurate,magnusson2021tropical} to find the local minimum of mean sea level pressure (MSLP). The time step of forecast lead time is set to be 6 hours. Specifically, once the initial position of the cyclone eye is provided, we iteratively search for a local minimum of MSLP that meets the following criteria:\par
- There is a maximum 850hPa relative vorticity greater than \(5 \times 10^{-5}\) within a 278km radius (in the Northern Hemisphere).\par
- There is a maximum thickness between 850hPa and 200hPa within a 278km radius when the cyclone is extratropical.\par
- The maximum 10m wind speed exceeds 8m/s within a 278km radius when the cyclone is over land.\par
Once the cyclone eye is identified, the tracking algorithm continues to find the next position within a 445km vicinity.\par
This study focuses on two extreme cyclones: Tropical Storm Yagi and Severe Typhoon Molave. The Yagi formed near Ito To, Japan on August 6, 2018, and landed over Wenling, China on August 12. The Molave formed on October 11, 2020, and landed over the Philippines on October 25, 2020. The initial conditions for these two cyclones are set at 0:00 UTC,  August 6, 2018 and  0:00 UTC, 11 October 2020, respectively. Since there is no Fuxi results in 2018 on weatherbench2, we can not compare it with Yagi.
\label{appendix:typhoon}
    
   

                
    
    
    
