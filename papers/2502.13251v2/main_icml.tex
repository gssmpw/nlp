%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{style/icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{style/icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



\input{command/local}
\input{command/math_commands}


\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{nicematrix}
\usepackage{graphicx}
\usepackage[inline]{enumitem}
\usepackage{wrapfig}
\usepackage{paralist}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Neural Attention Search}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Difan Deng}{luh}
\icmlauthor{Marius Lindauer}{luh,l3s}
\end{icmlauthorlist}

\icmlaffiliation{luh}{Leibniz University Hannover}
\icmlaffiliation{l3s}{L3S Research Center}

\icmlcorrespondingauthor{Difan Deng}{d.deng@ai.uni-hannover.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We present Neural Attention Search (\ourname), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: \begin{inparaenum}[(i)]
    \item \GlobalTokens~ will be preserved and queried by all the following tokens.
    \item \LocalTokens~ survive until the next global token appears.
    \item \SlidingWindowTokens~ have an impact on the inference of a fixed size of the next following tokens. 
\end{inparaenum}
Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that \ourname~ can efficiently reduce the KV cache size required for the models while maintaining the models' performance. 


\end{abstract}

\section{Introduction}
The ability to understand and infer from long-context information is crucial for many tasks such as long document summarization~\cite{zhang-arxiv23c} and question answering~\cite{kocisky-tacl18a, dasigi-naacl21a}, code generation~\cite{guo-icml23a,liu-iclr24b} or multi-round dialogues~\cite{yi-arxiv24a}. Thanks to the ability to query the information from any position of the historical sequence, transformer-based large language models~\cite{brown-neurips20a, jiang-arxiv23a, claude-24a, grattafiori-arxiv24a1,  gemini-arxiv24a} extend their context length up to millions of tokens. 

However, querying information from historical sequences requires a complexity of $\mathcal{O}(\seqlen^2)$ w.r.t. the input sequence length $\seqlen$. KV caching could reduce this time complexity to $\mathcal{O}(\seqlen)$ by storing all the historical KV values. Nevertheless, with the increasing model size of recent LLMs, even the $\mathcal{O}(\seqlen)$ time-wise and memory-wise complexity could become a bottleneck during inference time. 

Indeed, not all the tokens in a sequence are equally important. Many of them are redundant and do not contribute to the final output. Humans can recognize this information without pre-defined fixed rules and summarize or discard the context information into much smaller content. Transformers could also learn this ability implicitly: Many tokens in the attention map might only have very low weights~\cite{zhang-neurips23a} and only have little influence on the final predictions. However, as the transformer learns this information implicitly, we might not know how the important tokens would be distributed in the context. Selecting these tokens and recognizing the attention distributions might require extra human experts' knowledge by either looking at the attention maps~\cite{liu-neurips23a, zhang-neurips23a, feng-arxiv24a, zhang-icml24a} or applying specific fixed rules~\cite{child-arxiv19a, beltagy-arxiv20a, xiao-arxiv23a,  chen-arxiv24a, ge-iclr24a}. Since this knowledge is already contained in the transformer models, we could also ask the model to evaluate the importance of each token and learn to predict the optimal type for the given input tokens automatically. 

Unlike prior works that rely on human expertise or predefined rules to identify important tokens, we propose a novel approach to evaluate the importance of each token by assigning different roles to each of the tokens.  E.g. some tokens will be preserved until the end, while other tokens might only survive for a short amount of time. These roles measure the importance of each token and determine if they would survive within the next few tokens. Rather than pre-define a set of fixed rules for each token, we ask the model to learn this information automatically. Finding the optimal role for each token is similar to the process of neural architecture search~\cite{elsken-automlbook19a, white-arxiv23a}, where an optimal architecture is searched by the optimizer for a given task. Thereby, in this work, we jointly optimize the choice of each token and the model weights by constructing a learnable attention mask. Our approach is implicitly motivated by the one-shot neural architecture search approach~\cite{pham-icml18a, liu-iclr19a, dong-cvpr19a}, where the model parameter and architecture parameters are jointly optimized during the search process. However, our approach, Neural Attention Search (\ourname), searches for the optimal token roles jointly with the attention weights. 


Our contributions are as follows:
\begin{compactenum}
\item We propose Neural Attention Search (\ourname), a framework that automatically searches for the optimal roles for each token in the input sequence. 
\item We propose different token roles in our search space that can be later combined to construct a learnable attention mask and then jointly optimized with the model weights in \ourname. 
\item We show that \ourname~ could efficiently reduce the KV cache required during inference time while maintaining most of the models' performance. 
\end{compactenum}

By automatically learning to focus on the most relevant information, \ourname{} paves the way for more efficient and scalable inference with LLMs in long-context applications.

 \section{Background and Related Work}
\subsection{Attention maps for transformers~\label{sec:attmap}}
Transformers~\citep{vaswani-neurips17a} computes the correlation between different tokens by mapping the input sequences into three variables, $\attQ$, $\attK$ and $\attV$. An attention map $\attMap$ is first computed by pairing $\attQ$ and $\attK$ and then scaled and normalized with a softmax function. Finally, this attention map $\attMap$ is multiplied by $\attV$. Additionally, a mask $\attMSK$ is attached to the Attention Map to guide the attention to focus only on certain tokens. This can be expressed as 
\begin{align}
    \attMap & = \frac{\attQ\attK^T}{\sqrt{\dHead}}\label{eq:attmap}\\
    \attO & = \softmax(\attMap  + \attMSK^{add})\attV\label{eq:attout}
\end{align}

The additive attention mask $\attMSK^{add}_{i,j} \in \{ -\inf, 0 \}$ controls if the transformer needs to construct the correlation between $\attQ_i$ and $\attK_{j}$.  A widely used mask $\attMSK^{add}$ used in transformers are casual masks~\citep{vaswani-neurips17a}, where the upper parts of the mask are filled with $-\inf$ each token can only query the token information before them. 

Computing the attention map of a sequence with length $\seqlen$ requires a complexity of $\mathcal{O}(\seqlen^2)$ for both time and memory. Techniques such as KV cache lowers this complexity to the $\mathcal{O}(\seqlenCache)$ during inference time with  $\seqlenCache$ being the length of the KV cache. However, as an expense, the cost of storing the KV cache gradually becomes unneglectable with the increasing size of large language models and the context length they could handle. 

Many studies aim to reduce the KV cache size by sparsifying the attention map with pre-defined rules. For instance, Sparse Attention~\citep{child-arxiv19a} combines the local sliding windows and atrous attention to approximate the full attention maps; Longformer~\citep{beltagy-arxiv20a} further proposes to predefine a set of special tokens as global attention that have full access to any other tokens and can be quired by any other tokens; Streaming LLM~\citep{xiao-arxiv23a} shows that by preserving the initial tokens and the most recent tokens during inference time could recover the performance of window attention. DuoAttention~\cite{xiao-arxiv24a} further illustrates that only a small fraction of the attention heads are critical for processing long context information and therefore, only those retrieval heads should be preserved with the full attention. LongCoder~\cite{guo-icml23a} pre-defines a set of bridge tokens to summarize the information in between and memory tokens that will not be dropped afterwards. SepLLM~\cite{chen-arxiv24a} shows that the information contained in a segment can be compressed into a set of special tokens. 

Alternatively, past attention maps can decide which tokens to evict or preserve. H2O~\citep{zhang-neurips23a}  and Scissorhand~\cite{liu-neurips23a} both identify the importance of each token based on their contributions to the attention maps and only preserve the most important tokens. Other approaches such as FastGen~\cite{ge-iclr24a}, SnapKV~\cite{li-arxiv24a}, PyramidKV~\cite{cai-arxiv24a}, and AadaKV~\cite{feng-arxiv24a} all applied pre-defined fixed rules to identify the important KV values. Additionally, instead of simply removing the unimportant tokens, we can also merge them into the existing tokens. CAM~\citep{zhang-icml24a} achieves this by merging the to-be-evicted tokens into the remaining tokens. 

In contrast to the fine-tuning-free approaches above,  alternatively, we can also fine-tune the target model to achieve the desired sparsity. ~\citet{kim-kdd22a} ask the model to learn to drop the tokens with lower attention scores. \citet{anagnostidis-neurips23a} shows that this sparsity can also be learned through sparse sigmoid functions. SPARSEK~\citep{lou-arxiv24a} only selects a constant number of KV Paris for each query by introducing a differentiable SparseK operator. Landmark Attention~\cite{mohtashami-neurips23a} insert landmark tokens after certain time steps and apply these landmark tokens to summarize the information of the tokens before that landmark token and recover. Furthermore, Dynamic Token Pooling~\citep{nawrot-acl23a} segments the input sequences as pooling layers to reduce the overall computational costs, while Dynamic Memory Compression~\citep{nawrot-icml24a}  tries to accumulate the tokens within the same subsequence into one single token. 

While these existing methods offer various ways to reduce KV cache size, they often rely on inflexible predefined rules and potentially inaccurate heuristics based on past attention maps. \ourname, in contrast, introduces a novel approach that learns to dynamically assign token roles during inference, enabling a more adaptive and efficient use of the KV cache. By treating token role assignment as an optimization problem, \ourname{} leverages principles from neural architecture search to jointly optimize token roles and model weights, leading to a more flexible and powerful attention mechanism.

\subsection{Neural Architecture Search}
Designing a neural architecture for a specific task might require a lot of trial and error. Neural Architecture Search (NAS) automates this process by searching in a pre-defined search space~\cite{elsken-jmlr19a}. Previous work on NAS mainly focused on searching within a discrete search space by sampling a new architecture from the search space, evaluating its performance, and updating the optimizers~\cite{zoph-iclr17a, zoph-cvpr18a}. However, training every network from scratch requires lots of GPU hours. One-Shot NAS~\cite{pham-icml18a} approaches instead share the same weights of the operators w.r.t. all the architectures in the search space. This allows to jointly optimize the architectures and weights. DARTS~\cite{liu-iclr19a} and GDAS~\cite{dong-cvpr19a} further relax the discrete search space into continuous values to optimize the architecture parameters with gradient descent. The One-Shot NAS approach allows the optimizers to efficiently search for the optimal architecture within a huge search space. Similarly, \ourname~ has multiple options for each token as the search space and is able to search for the optimal token types jointly with the model weights. However, unlike One-Shot NAS approaches that consider the optimization problem as a bilevel-optimization problem and optimize the model weights and architecture weights alternatively, we optimize the token state information and model weights within one forward and backward pass. This is similar to a mixture-of-expert (MOE) model~\cite{shazzer-iclr17a, fedus-jmlr23a}. However, instead of distributing data across all experts uniformly, we only select one expert for each token and assign all data to that expert.


\section{The \ourname{} Approach}
In this section, we first introduce all the candidate token types in our search space. We then show that we can construct a learnable attention mask with the choice of each token type. Finally, we can efficiently reduce the KV cache size by dropping the unnecessary tokens during inference.


\subsection{Search Space Design}
Not all the tokens in a sequence are equally important. Some of the tokens might contain important information, and they should be preserved that will be further queried by the following tokens. Some tokens might only contribute to the prediction for the next few tokens. Just like a paragraph is composed of multiple sentences, a sequence can be divided into multiple sub-sequences containing different information; some tokens might only be required within these sub-sequences. Hence, we design a search space for each token's role within the sequence and ask the model to automatically learn the optimal role for each token in the sequence. 

We first define \textbf{\GlobalToken} as tokens containing important information that need to be preserved for the following predictions. \citet{liu-neurips23a, zhang-neurips23a} showed that only a small fraction of the tokens contributes to most of the attention scores for self-attention computation. These tokens need to be preserved for models to recall the global information that helps to predict the next token. In vanilla transformer models, all the tokens are \GlobalTokens. 

\GlobalTokens{} will not be evicted during inference time. Therefore, we should maintain as few \GlobalTokens{} as possible to ensure inference efficiency. Each \GlobalToken{} should not only preserve the information within that position. Ideally, it should also be able to summarize the information from previous sequences~\cite{guo-icml23a, chen-arxiv24a}. Hence, we split the entire sequence with the \GlobalTokens{} into multiple sub-sequences, with each sub-sequence ending with one \GlobalToken. Each \GlobalToken{} only needs to summarize the information from its sub-sequences and the previous \GlobalTokens{}. 

\textbf{\LocalToken} only survives until the next \GlobalToken{} appears. Therefore, models will have the full attention within each sub-sequence to summarize the local sub-sequence information into the \GlobalToken{} located at the end of the sub-sequence while being sparse within the input sequence. 

Only the \GlobalTokens{} and \LocalTokens{} might control the sparsity at a low granularity level. E.g., assuming that one input sequence is highly localized, each token only has a high correlation with itself or a few neighboring tokens. In this case, they are all similar and are assigned with the same token type. However, none of the \GlobalToken{} and \LocalToken{} could sparsify this attention map efficiently: if all the tokens are classified as \LocalTokens, then the input sequence will only be considered as one single subsequence, and all the \LocalTokens{} will be equivalent to the \GlobalTokens. 

Hence, we introduce \textbf{\SlidingWindowToken}. 
\SlidingWindowTokens{} will only be preserved for the next \SlidingWindowSize{} time steps and were previously considered as one of the most popular sparse attention approaches~\cite{child-arxiv19a, xiao-arxiv23a, zhang-neurips23a, ge-iclr24a}.



\begin{figure*}[h]
\centering
\subfigure[]{
     \centering
     \includegraphics[width=0.17\textwidth]{figures/AttentionMaps/FullAttention.drawio.png}
         \label{fig:FullAtt}
    }
    \hfill
\subfigure[]{
     \centering
     \includegraphics[width=0.17\textwidth]{figures/AttentionMaps/LocalAttention.drawio.png}
    \label{fig:LocalAtt}}
    \hfill
\subfigure[]{
     \centering
     \includegraphics[width=0.17\textwidth]{figures/AttentionMaps/LongFormer.drawio.png}
    \label{fig:LongFormer}}
    \hfill
\subfigure[]{
     \centering
     \includegraphics[width=0.17\textwidth]{figures/AttentionMaps/DynamicAttention.drawio.png}
    \label{fig:LearnedAttention}}
    \begin{subfigure}
    \centering
     \includegraphics[width=0.1\textwidth]{figures/AttentionMaps/DynamicAttention_caption.png}
     \end{subfigure}
    \caption{A comparison between different casual attention maps. \ref{fig:FullAtt}: The full attention map, where each token is connected to the tokens before it. \ref{fig:LocalAtt}: the local attention with sliding windows 3, every token will only get access to the information of the 3 tokens ahead. \ref{fig:LongFormer} Longformer, besides the local attention, the first, 6th and 9th tokens are the pre-defined global tokens. \ref{fig:LearnedAttention}: \ourname ~dynamically 
    searches for the optimal role for each token and constructs a learnable mask based on the roles for each token. 
    \label{fig:attentinos}}
\end{figure*}

In contrast to other causal attention maps, Figure~\ref{fig:LearnedAttention} illustrates an exemplary attention mask constructed by the choices of different token types. In this case, we define the sliding window size as 4. Token 1, 4, 10 act as \GlobalTokens; Tokens 2, 6, 7, 8, 11 are \LocalTokens; Token 3, 5, 9, 11 are \SlidingWindowTokens{}. The \GlobalTokens{} splits the entire sequence into three subsequences that end at 4, 10, and the last index accordingly. Hence, Token 2 will only be quired by Token 3 and 4. This rule applies the same for Token 6, 7, and 8, where they only interact with the tokens within the same subsequence. For the sliding window tokens, only the next three tokens could query their information, regardless of whether these tokens belong to the same sub-sequence. Only 6 out of 12 tokens are involved during inference time to make the next token prediction. 

\subsection{Searching for the Optimal Token Types~\label{sec:natstrain}}
Searching for the optimal token types within a sequence is similar to searching for the optimal architectures for a given task in neural architecture search~\cite{elsken-jmlr19a, white-arxiv23a}. Following GDAS~\cite{dong-cvpr19a}, we apply the Gumbel-Softmax trick~\cite{jang-iclr17a, maddison-iclr17a} to sample from the discrete search space. The Gumbel-Softmax trick allows the gradient to be backpropagated through the discrete choice of the token types. 

Specifically, we first use a linear layer (which we call \AttScoreLayer) that maps each the input tensor for an attention layer $X \in \mathbb{R}^{\dData}$ to the likelihood for each option: $\tokenstates \in \mathbb{R}^{(\nheads * \nopts)} = Linear(\mathbf{x})$, where $\nheads$ is the number of KV heads and $\nopts$ is the number of options in the search space. The type $\tokenstates$ for each token is then sampled from a Gumble-Softmax function based on the likelihood values. 

We could now construct a learnable attention mask $\attMSK$ with a series of sampled token states. However, the additive mask in Eq.~\ref{eq:attmap} will take $-\inf$ values,  resulting in invalid gradient information. Hence, we use the token information to construct a multiplicative attention mask $\attMSK^{mul} \in \{0, 1\}$\footnote{For the sake of brevity, we will use $\attMSK$ instead of $\attMSK^{mul}$ in the following part of this paper.}:
\begin{equation}
    \attO = \frac{e^{A} \odot \attMSK^{mul}}{\sum_j e^{A_{.,j}} \odot \attMSK^{mul}_{., j}}\attV\label{eq:attoutmul}
\end{equation}


The attention mask columns for \GlobalTokens{} and \SlidingWindowTokens{} can be directly constructed since they will survive for a fixed amount of steps. However, the mask for \LocalTokens{} $\attMSK^{L}_{i,j}$ is controlled by both the distribution from \LocalTokens{} and \GlobalTokens{} as \LocalTokens{} will survive until the next \GlobalToken{} appears. In other words, to make $\attMSK^{L}_{i,j} (j>i)$ a valid value, no \GlobalToken{} should appear between $i$ and $j$. 

Formally, the attention masks can be created as follows:
\begin{align}
    \attMSK^{G}_{i, j} & = 1\label{eq:mskglobal} \\
    \attMSK^{SW}_{i,j} & = 
    \begin{cases} 
    1 & \text{if}\ j \leq i + \SlidingWindowSize \\
    0 & \text{if}\ j > i + \SlidingWindowSize
    \end{cases}\label{eq:msksw}\\
    \attMSK^{L}_{i, j} & = \prod_{n=j+1}^{i-1}(1-\EndSeqHard_{n}) \label{eq:msklocal}
\end{align}
where \SlidingWindowSize{} is the sliding window size and $\EndSeqHard_{n}$ a \GlobalToken{} at Position $n$. We could then construct the attention masks based on the type of each token. After that, we mask out the upper triangular part of the mask to ensure its causality.

In practice, we first collect the index of the next global token  
$\EndSeqHardTrue_i := \min (\{k | k >= i \land \EndSeqHard_k = 1 \})$ and rewrite Eq.~\ref{eq:msklocal} as:
\begin{align}
    \attMSK^{L}_{i, j} = \begin{cases}
        1 & \text{if}\  j \leq \EndSeqHardTrue_i \\
        0 & \text{if}\  j > \EndSeqHardTrue_i
    \end{cases} \label{eq:msklocalfwd}
\end{align}

These rules are then integrated in FlashAttention~\cite{dao-neurips22a, dao-iclr24a} to avoid explicitly computing the attention masks during the forward pass. In addition to the transformer computation, we only need to collect the next \GlobalToken{} indices $\mathbf{\EndSeqHardTrue}$ (with a complexity of $\mathcal{O}(N)$) and then mask out the attention map values with the masks defined above.

To compute the gradients for \attMSK, we set $g(\attMap, \attMSK) = e^{\attMap} \odot \attMSK$; then the gradient for \attMSK{} is: 
\begin{align}
   \frac{\partial \attO}{\partial \attMSK} = 
   \frac{\partial \attO}{\partial g}\frac{\partial g}{\partial \attMSK} \qquad
   \frac{\partial g}{\partial \attMSK} = e^{\attMap} \qquad
   \frac{\partial g}{\partial \attMap} = e^{\attMap} \odot \attMSK
\end{align}
Hence, the gradients for $\attMSK$ are the same as those for $\attMap$ if $\attMSK$ is 1. For the case of $\attMSK =0$, this is the same as the values that $\partial \attO / \partial \attMap$ is supposed to be if no learnable mask is applied to the attention map. Hence, we can jointly compute the gradient computation from both \attMSK{} and \attMap{} within one backward pass. Further details can be found in the appendix.

The gradients towards each token are collected through the column sum of each value weighted by the corresponding attention masks:
\begin{equation}
    d \tokenstates_{i} = \sum_{j} d \attMSK_{i,j} \times  \attMSK_{i,j}^{\tokenstates} ~\label{eq:gradmsk}
\end{equation}
Where $\tokenstates \in \{G, L, SW\}$ is the discretized token type. Intuitively, this shows the model's preference over short-range or long-range correlations: If $i$ is quite close to $j$, then all the $\tokenstates$ will receive the same gradient information. However, if the model wants to create a long-range correlation with $i \gg j$, only \GlobalTokens{} will receive the gradient information and update their weights with the gradient. 

Eq.~\ref{eq:msklocal} shows that the \GlobalToken{} controls the local mask size. Therefore, the gradients for \GlobalToken{} $i$ should also be regularized by the gradient information from the previous tokens:
\begin{equation}
    \frac{\partial \attMSK_{i,j}^{L}}{\partial \EndSeqHard_k} = - \prod_{\substack{n=j+1 \\ n \neq k}}^{i-1}(1-\EndSeqHard_{n}) ~\label{eq:gradlocalmsk}
\end{equation}
In cases where $\EndSeqHard_k$ is 0, Equation~\ref{eq:gradlocalmsk} is the negative value of Equation~\ref{eq:msklocal}. However, for the case where $\EndSeqHard_k = 1$, this is equivalent to the local mask values where $k$ is no longer set as a \GlobalToken. This requires us to find the index of the next global token $\EndSeqHardTrue_{k+1}$ and the last global token $\EndSeqHardTrue_{k-l}$ where $l$ is the length of the local sequence that ends at $k$.

This gradient information will then be collected and  subtracted from the computed \GlobalTokens{} gradient values:
\begin{align}
        d \tokenstates^{G-}_{i} &=
    \begin{cases}
          \sum\limits_{\substack{m \geq i\\n < i}} \attMSK^L_{m,n} \times d\attMSK_{m,n} \ \  &\text{if}\  \EndSeqHard_i = 0, \\
         \sum\limits_{\substack{\EndSeqHardTrue_{i+1} \geq m >=i\\  i > n \geq \EndSeqHardTrue_{k-l} }} d\attMSK_{m,n} \ \  &\text{if}\  \EndSeqHard_i = 1, 
    \end{cases}\label{eq:gradcollected1} \\
    d \tokenstates^{G}_{i} &= d \tokenstates^{G}_{i} - d \tokenstates^{G-}_{i} \label{eq:gradcollected2}
\end{align}

Intuitively, this gradient term $d \tokenstates^{G-}_{i}$ checks if the new \GlobalToken{} needs to be inserted into the current sub-sequence (when $\EndSeqHard$ is 0) or we should remove the current \GlobalToken{} to enlarge the current sub-sequence (when $\EndSeqHard$ is 1). We further illustrate this process in the appendix.

To control the attention map's sparsity, we introduce a small regularization value $\SparseRegularizedPar$ that is directly applied to the gradient for \GlobalToken{} and \LocalTokens{} to encourage more \SlidingWindowTokens. We show how $\SparseRegularizedPar$ values control the attention map sparsity in the appendix.

\subsection{Efficient Inference with Different Token Types}
During inference time, we dynamically map the input feature maps to the corresponding token types and discard the tokens no longer required by the following tokens.
The \SlidingWindowTokens{} only survive for a fixed amount of time steps. We preserve a queue in the KV cache space that only stores the most recent $\SlidingWindowSize$ tokens and mask out the non-\SlidingWindowTokens: when new tokens come, we place them after the tail of the queue to replace the tokens older than $\SlidingWindowSize$. 

Similar to the vanilla transformer forward process, when new tokens arrive, we concatenate them with the existing KV caches, generating new masks and computing the attention output. After that, we start our post-update process: we first check the state of each token to decide if we could remove them or keep them in the cached values. Since different heads might disagree on the types of the same token, we record both the sizes for \GlobalTokens~(\GlobalTokenSize) and \LocalTokens~(\LocalTokenSize) for all the heads. New \SlidingWindowTokens{} do not change these sizes since they will always be placed separately. However, when a new \GlobalToken{} for any head arrives, we remove all the \LocalTokens{} from the corresponding heads and place the new \GlobalToken{} right after the existing \GlobalTokens{} and then update our \GlobalTokenSize{} and \LocalTokenSize{} accordingly. The same strategy is applied when new $\LocalTokens$ arrive: we place them at the end of the \LocalTokens{} and increase the number for \LocalTokens. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/CacheUpdate/CacheUpdate.drawio.png}
    \caption{An example of how caches are updated within \ourname{} when new tokens arrive with a model containing two heads. The two rows represent different heads.}
    \label{fig:cacheUpdate}
\end{figure}
Figure~\ref{fig:cacheUpdate} illustrates an exemplary case to update the KV caches in \ourname{} with a sliding window size of 3.
The first two places store the most recent tokens and use a mask to mask out the non-\SlidingWindowTokens~(yellow tokens). Since the Token 8 for Head 1 is \SlidingWindowToken{} and \GlobalToken{} for Head 2. We first move both tokens to the beginning of our cache to replace the old one. After that, we drop Token 8 in Head 1 since it has already moved to the sliding window caches. Then, since Token 8 in Head~2 is a \GlobalToken, we drop all local tokens after the last \GlobalToken~(Token 1). Hence, Tokens 4 and 6 are removed from the cache, and we place Token 8 after Token~1. After that, the \GlobalTokenSize{} is updated from $[5,3]$ to $[5,4]$ and the \LocalTokenSize{} is updated from $[0, 2]$ to $[0,0]$. Since both new tokens are merged into the existing tokens, we do not need the extra space to store these new tokens. 


\section{Experiments}
We implement \ourname{} based on the Flash Attention 2~\cite{dao-iclr24a} implementation on \href{https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html}{triton}. All the operations that we proposed in Section~\ref{sec:natstrain} have at most $O(\seqlen)$ complexity.  
In our experiments, we first train \ourname{} parameters jointly within a transformer model from scratch. We then apply \ourname{} to fine-tune a large language model. 

\subsection{Training a Transformer From Scratch~\label{sec:trainfromscratch}}
We first apply \ourname~ to train a GPT2 small style~\cite{radford-arxiv21a} transformer model from scratch. Following the setting from NanoGPT~\cite{karpathy-github22a}, this model has 128M parameters with 12 layers and 12 heads with a hidden dimension of 768. Instead of the learnable position encoding, we apply rotary embeddings~\cite{su-nc24a} to each transformer layer. We train this model on the PG-19 Language Modeling Benchmark~\cite{rae-iclr20a}. This dataset contains books extracted from Project Gutenberg~\cite{gutenberg} with about 2B tokens in the training sets. We train all models with a context length of 1024 and batch size of 480 (using gradient accumulation). We train all models for $120\,000$ iterations on the training sets on a computation node with four Nvidia H100 GPUs. We evaluate all models on the test sets of PG19 with a context length of 1024. %Further details on the hyperparameters can be found in the appendix. 

As a baseline, we train another dense transformer model under the same hyperparameter setting. During inference time, we compare \ourname~ with the following baselines besides the full Transformer: 
\begin{inparaenum}[(i)]
    \item Streaming LLM~\cite{xiao-arxiv23a} only preserves the first few starting and the most recent few tokens for future prediction. 
    \item H2O~\cite{zhang-neurips23a} first computes the attention map and only preserves the tokens with the top-k attention scores. 
\end{inparaenum}
H2O and Streaming LLM are training-free approaches that control the sparsity with pre-defined hyperparameters during inference time. H2O needs to define the recent sliding window size and the number of Heavy Hitter (HH) tokens; Streaming LLM requires the number of tokens at the beginning of the sequence (Sink Tokens) and sliding window size. 
In contrast, \ourname~ controls this sparsity by the sparsity hyperparameter value $\SparseRegularizedPar$. Hence, we train multiple models with different $\SparseRegularizedPar$. However, in our experiments, we observe that the attention map sparsity values converge much faster than the model loss. We could quickly estimate the attention map sparsity within the first 10,000 iterations to check if this sparsity value satisfies the user requirement and early-stop the runs that do not satisfy the requirements~\cite{jamieson-aistats16a}. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Exp/Perplexity/nats_perplexity_kv.png}
    \caption{Perplexity vs KV Cache size under different sparsity settings $\SparseRegularizedPar$ on the PG19 dataset.}
    \label{fig:pg19perKV}
\end{figure}


 We provide different hyperparameters for H2O and Streaming LLM (with the sliding window size of 32, 64, 128, and 256, plus the same number of HH/ Sink tokens). This results in a corresponding KV budget fraction ranging from  6.25\% to 50\%. Meanwhile, we train \ourname{} with the following $\SparseRegularizedPar$: $0, 1e-9, 5e-9, 1e-8, 1e-7$. 

Figure~\ref{fig:pg19perKV} shows the perplexity of the PG19 test sets from different model settings. The x-axis indicates the fraction of KV caches applied to generate the last token in the input sequence. As expected, larger $\SparseRegularizedPar$ results in a smaller KV cache fraction. Interestingly, even if $\SparseRegularizedPar$ is set as $0$, the KV cache budget can still be reduced to around 60\% with slightly lower perplexity. This might indicate that the model also tends to sparsify the input information to focus more on the relevant parts from the input data~\cite{ye-arxiv24a}. 


Under the budget of around 50\%, all the approaches perform similarly to the full transformer while \ourname{} has a relatively lower perplexity and requires less budget. However, as the available budget decreases, \ourname's perplexity remains nearly the same until a KV cache size of around $10\%$. In contrast, H2O and Streaming LLM start to degenerate their performance starting from a $25\%$ budget allocation. The most sparse model in \ourname{} family only contains roughly $3\%$ of the KV cache size (meaning roughly 30 cached tokens per layer on average) and achieves a lower perplexity compared to H2O with $12.5\%$ of the KV caches and Streaming LLM with $25\%$ of the KV caches.



\subsection{Fine Tune a Large Language Models}
We now apply \ourname{} to fine-tune an existing large language model~\cite{grattafiori-arxiv24a1, jiang-arxiv23a} in the long context scenario. We evaluate the fine-tuned models on the Needle-in-a-Haystack (NIAH) test~\cite{kamradt-github22a} and LongBench dataset~\cite{bai-acl24a}. NIAH places a random fact or statement (needle) in the middle of a long context and asks the model to retrieve it from the context. LongBench evaluates the model's capacity to understand the information in different long context scenarios, such as single-document and multi-document QA, summarization, few-show learning, synthetic tasks, and code completion. 


In addition to the baselines in Section ~\ref{sec:trainfromscratch}, we use DuoAttention~\cite{xiao-arxiv24a} as another baseline. DuoAttention evaluates the importance of each attention head and only assigns full attention budgets to the heads with high scores (the so-called retrieval heads) while applying streaming to the other unimportant heads. DuoAttention shares the same idea as \ourname{}, where the importance of different KV caches should be learned instead of the pre-defined rules. However, instead of learning the importance of the head level, we aim at learning this information directly on the token level. 

To allow our models to learn to evaluate the role of each token within different contexts, we construct a new training dataset that follows the construction rules of LongBench. Some of LongBench's tasks are collected from the test sets of the previous benchmarks. Hence, we first collect the training sets from these benchmarks and construct these datasets following the data structure in LongBench. Overall, this dataset contains $6\,436$ pieces of data with a maximum context length of $16\,000$. Further details on the dataset collection can be found in the appendix. 

\begin{figure}
\centering
\subfigure{\includegraphics[width=0.235\textwidth]{figures/Exp/NIAH/niah_fullattn.png}}
\subfigure{\includegraphics[width=0.235\textwidth]{figures/Exp/NIAH/niah_duo.png}}
\vfill
\subfigure{\includegraphics[width=0.235\textwidth]{figures/Exp/NIAH/niah_h2o.png}}
\subfigure{\includegraphics[width=0.235\textwidth]{figures/Exp/NIAH/niah_streaming.png}}
\vfill
\subfigure{\includegraphics[width=0.235\textwidth]{figures/Exp/NIAH/niah_nats.png}}
\subfigure{\includegraphics[width=0.235\textwidth]{figures/Exp/NIAH/niah_budget_nats1e7.png}}
\caption{Results on Needle in a Haystack test with Llama-3-8B-Instruct-Gradient-1048k model. The context length ranges from $1\,000$ to $32\,000$ and the document depth ranges from $0$ to $1$. We assign a $25\%$ budget to each of the approaches. The first two rows are the scores (higher is better) of the full attention and the corresponding baselines. The bottom plot on the left shows the scores from \ourname{} with an average compression rate of $15\%$. The plot on the right shows the compression rate of \ourname{} across different document lengths~\label{fig:expniah}.}
\end{figure}

We only fine-tune the \AttScoreLayer{} while keeping all the other parameters in the network fixed. Hence, we aim to approximate the original output from the corresponding base LLM. This approach is similar to DuoAttention~\cite{xiao-arxiv24a}. However, since we want the model to capture the overall context information, we update \AttScoreLayer{} with all the output from the full attention layer:
%The authors of DuoAttention first embed multiple randomly generated passkey sequences into random positions of a long context and then asked the model to recall the passkey sequence. However, DuoAttention only needs to recognize the retrieval heads for a sequence level. Our \AttScoreLayer{} has to identify the roles for each token. This requires the model to capture both long-term global information and short-term local correlation. Hence, we ask the model to make the next token prediction for all the text it has received. Then similar to ~\citet{xiao-arxiv24a}, we then train the \AttScoreLayer{s} to approximate the output from the full transformer with L1 Loss:
\begin{equation}
    \mathcal{L}_{distill}=\frac{1}{\bsz}\sum_{i=1}^{\bsz}\sum_{j=1}^{\seqlen}(\mathbf{H}_{full}^{(i)}[j] - \mathbf{H}_{\ourname}^{(i)}[j])^2
\end{equation}
where $\bsz$ is the Batch Size. Additionally, we control the sparsity with the regularization value $\SparseRegularizedPar$ instead of the additional loss item defined in DuoAttention, and therefore, only optimize \ourname{} with $\mathcal{L}_{distill}$ as the loss function. We fine-tune two long-context models (Llama-3-8B-Instruct-Gradient-1048k~\cite{llama8b} and Mistral-7B-v0.3-Instruct ~\cite{jiang-arxiv23a}) on two Nvidia H100 GPUs for one epoch using AdamW~\cite{kingma-iclr15a, loshchilov-iclr19a} with a learning rate of 3e-4 with a warm-up from 3e-5 in the first 20\% steps and reducing back to 3e-5 in the last 20\% steps. We apply different $\SparseRegularizedPar$ to allow for different sparsity. For a fair comparison, we set the sliding window size \SlidingWindowSize{} as 256, the same as DuoAttention. 

Since we only optimize the \AttScoreLayer{}, the number of learnable parameters is $n_{layers} \cdot d_{model} \cdot  n_{heads} \cdot  {n_{options}}$. Where $n_{options}$ is the number of options in our search space (in this case, it is $3$). Hence, the size of the parameters that need to be stored is neglectable (it only takes roughly $13$MB on disk for each set of \AttScoreLayer{}) compared to the weights of the LLM. Hence, users could pre-collect all these weights and apply the one that fits their compression requirement. 

%\subsubsection{Needle in a Haystack}

For the Needle In a Haystack task, we test all models with a context length ranging from $1\,000$ to $32\,000$ with the needle positions ranging from $0$ to $1$. We assign a $25\%$ KV cache budget to each of the baseline approaches. For \ourname{}, we choose the model with the compression rate closest to $25\%$, which corresponds to a model trained with $\SparseRegularizedPar=1e-7$ and an average KV budget size of $15\%$.

The results on Needle in a Haystack test are shown in Figure~\ref{fig:expniah}. DuoAttention, H2O, and Streaming LLM all maintain a sliding window with a fixed size. Therefore, they could well capture the needles inserted at the text's tail. However, all the baselines struggle with the correct answers as the needle goes into the middle of the context. In contrast, ~\ourname{} efficiently recognizes the necessary tokens across different layers, heads and positions to preserve only tokens that contain the important information. 

We also plot the KV budget percentage on the bottom right of Figure~\ref{fig:expniah}. Since we set our Sliding window size \SlidingWindowSize{} as 256, the KV budget for the first 1000 tokens is roughly $30\%$. However, as the context length increases, the compression rates gradually decrease to the over compression rate of $15\%$. This shows \ourname{} could save the important information uniformly across the context in different positions. 

%\subsubsection{LongBench}
\begin{table}[]
    \centering
    \scalebox{0.83}{\input{tables/longbenchRes/llama3-25}}
    \caption{LongBench Results with 25\% Budget Allocation for full attention (Full), DuoAttention (Duo), Streaming LLM (SLLM), H2O and \ourname{} on the LLama8B model. The numbers in the brackets for \ourname{} is the used value KV cache sizes. We bold the models with the best performance besides the base Full Attention Models~\label{tab:ResLLBlama25}.}
\end{table}
For the LongBench tasks, we evaluate all the baselines with the $50\%$ and $25\%$ of the KV cache sizes. We show the result with KV cache size of $25\%$ for LLama3-8B in Table~\ref{tab:ResLLBlama25}. The model that is used in this task is a model with $\SparseRegularizedPar=1e-7$. \ourname{} achieves the best performance on most of the dataset with (in many cases) smaller KV cache values.
We provide further results, including results with Mistral-7B and ~\ourname{} trained with other parameters, in the appendix. 


\subsubsection{KV Size Distribution~\label{sec:KVSizeDistribution}}
\begin{figure}
\centering
    \subfigure{\includegraphics[width=0.5\textwidth]{figures/Exp/KVSizes/llama/kv_size_Llama_1e7_narrativeqa.png}}
    \subfigure{\includegraphics[width=0.5\textwidth]{figures/Exp/KVSizes/llama/kv_size_Llama_1e7_repobench-p.png}}
    \caption{KV cache size compression on different layers~\label{fig:KVSizeDistributions}}
\end{figure}
Figure~\ref{fig:KVSizeDistributions} shows the distribution of different KV budgets on the narrativeQA~\cite{kocisky-tacl18a} and RepoBench-p~\cite{liu-iclr24b} dataset. The overall trends are similar: The first few layers and some specific heads require more layers than the other heads. However, as the model goes deeper, the model tends to drop more tokens for the NarrativeQA dataset while still maintaining many tokens for RepoBench-P, especially after layer 16. This shows that a fixed rule might not always fit all the sequences and there is a need to adjust the sampling strategy with different inputs. 

\section{Conclusion and Future Work}
Efficiently managing the KV cache is crucial for deploying large language models in long-context applications.
In this work, we propose \ourname{}, an approach that automatically optimizes the optimal roles of each token to determine how long they should survive. By constructing a learnable mask, \ourname{} learns to sparsify the attention map end-to-end. Our experiments show that \ourname{} could use much less KV caches compared to the State-of-the-art KV caching reduction approach. 
While \ourname{} shows promising results, future work could include exploration of further token roles and structured search spaces with hierarchies. Overall, we believe that \ourname{} paves the way towards more efficient and scalable inference with LLMs.
%However, there are still many hyperparameters that have a strong impact on our approach, such as the sliding window size \SlidingWindowSize{} and the sparse regularization term $\SparseRegularizedPar$. Additionally, the search space proposed in our paper might be limited. One future direction is to propose a larger search space that allows for constructing a more sophisticated sparse attention map. 

\section{Impact Statements}
LLMs are widely applied to different fields nowadays. However, the cost for LLM to store the KV cache and predict the next token is still huge, given the $\mathcal{O}(\seqlen)$ computation and memory costs of full Attention Models. This prevents further adaptation of LLMs (and other transformer-based foundation models with properties similar to those described before) because of high energy consumption and limited context windows. \ourname{} achieves a substantial reduction in KV cache size with minimal impact on model performance, outperforming existing state-of-the-art approaches. This increased efficiency can enable the deployment of larger, more powerful language models on resource-constrained devices and facilitate the development of new applications that rely on long-context understanding, such as advanced conversational AI, comprehensive document summarization, and complex code generation. By making long-context processing more accessible, \ourname{} has the potential to accelerate progress in natural language processing and related fields. Nevertheless, it does not solve other inherent problems of LLMs such as hallucinations. 

\input{misc/acknowledgement}


%Recent Linear RNN-based work~\cite{katharopoulos-icml20a, gu-arxiv23a} shows that the historical information can be compressed into much smaller hidden states (with hidden states of length 1). Since these models could learn to preserve the necessary information and discard the redundant information, we believe that transformers should be able to do the same way to further reduce the required stored KV cache, which helps to further reduce the inference cost. \ourname{} shows that it is possible to learn to sparisfy the attention map with the combination of different token roles automatically, which helps to further reduce the computation costs during inference time. 





\bibliography{bibtex/strings,  bibtex/lib, bibtex/bibtex_local, bibtex/proc, bibtex/proc_local}
\bibliographystyle{style/icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Details on Backward Propagation}
\subsection{Gradients for Attention Masks}

\begin{align}
   \frac{\partial \attO}{\partial \attMSK} = 
   \frac{\partial \attO}{\partial g}\frac{\partial g}{\partial \attMSK} \\
   \frac{\partial g}{\partial \attMSK} = e^{\attMap}\label{eq:gradientMSK} \\
   \frac{\partial g}{\partial \attMap} = e^{\attMap} \odot \attMSK\label{eq:gradientA}
\end{align}

In Eq.~\ref{eq:gradientMSK} and ~\ref{eq:gradientA}, we show the gradient for $\attMSK$ is the same as the value that $\partial \attO / \partial \attMap$ is supposed to be if no mask is applied. 
Since we have $g_{i,j}=e^{\attMap_{i,j}} \odot \attMSK_{i, j}$. Let's set $S_{i}:=\sum_j g_{i,j}$ and $P_{i,j}:=\frac{g_{i,j}}{S_{i}}$. Then we have $\mathbf{d}\mathbf{P} = \mathbf{d}\mathbf{\attO}\mathbf{V}^T$. Therefore, 

\begin{equation}
   dg_{i:} = (diag(\frac{1}{S_{i:}}) - \frac{1}{S_{i:}}P_{i:}^T)dP_{i:}\label{eq:gradGi}
\end{equation}
Combining Eq.~\ref{eq:gradGi} with Eq.~\ref{eq:gradientA} and Eq.~\ref{eq:gradientMSK} provides the same gradients as the vanilla softmax function with additive masks:
\begin{align}
    d\attMSK_{i:} &= (diag(\frac{e^{\attMap_{i:}}}{S_{i:}}) - \frac{e^{\attMap_{i:}}}{S_{i:}}P_{i:}^T)dP_{i:}\\
    d\attMap &= d\attMSK \odot  \attMSK  \nonumber \\   
             &= (diag(P_{i:}) - P_{i:}P^T_{i:})dP_{i:}
    \label{eq:gradientA1}
\end{align}

Since $dg_{i:}$ is required to compute the gradient for $\mathbf{A}_{i:}$ and always needs to be computed. We can directly use this information to compute the gradients for the attention Mask $\attMSK$.

Following FlashAttention~\cite{dao-neurips22a}, we define $D_{i} = do_i^To_i$, then 
\begin{align}
    d\attMSK = \frac{e^{A_{i,j}}}{S_{i:}}(dP_{ij} - D_{i})\\
    d\attMap_{i,j} = P_{i,j}(dP_{ij} - D_{i})
\end{align}

and $d\attMap$ is computed by Eq.~\ref{eq:gradientA1}. After that, we can backpropagate $d\attMap$ to $dq$ and $dv$. Since $\attMSK$ needs to be recomputed anyway in the flash attention's backward process, this only results in little computational overhead. 

However, in practice, we found this makes the training unstable. All the values in a row with vanilla softmax attention are normalized such that their sum is bounded. However, we need to use the unmasked $e^{\attMSK}$ to compute $d\attMSK$ in our scenerio, meaning $\frac{e^{A_{i,j}}}{S_i}$ could be unbounded. This might result in an unstable training process. To alleviate this, we record both the masked denominator $S_i$ and  unmasked denominator $S'_i=\sum_{j}e^{\attMap_{i,j}}$ during the forward pass. We then use the unmasked denominator to compute the unmasked $P'_{i,j}=\frac{e^{A_{i,j}}}{S'_i}$ and further apply $p'$ to compute $d\attMSK$:
\begin{equation}
    d\attMSK = P'_{i,j}(dP_{ij} - D_{i})
\end{equation}
This only requires storing an additional $S'_i$ during the forward pass (with a complexity of $O(\seqlen)$) and one additional computation during the backward pass (since $S'_i$ is already computed and stored during the forward pass).


\subsection{Details on computing $d \tokenstates^{G-}_{i}$}

In Eq.~\ref{eq:gradcollected1} and ~\ref{eq:gradcollected2}, we show that an additional $d \tokenstates^{G-}_{i}$ needs to be computed for each token's gradient for \GlobalTokens{}. Figure ~\ref{fig:GradTokenGrad} illustrates an example of this. Token 4 is a global token, and we search for its next \GlobalToken{} (which is Token 10 in this example). Assuming that we want to change Token 4 to another role,  the regions within the boundary (orange ones) are those tokens that are influenced by this swtching: given that Token 4 no longer becomes \GlobalToken{}, Token 2 and 3 will be part of a larger subsequence that ranges from 1 to 10, and the attentino maps within the orange region should not be masked out. Intuitively, this gradient measures the regret that we made in order to switch one \GlobalToken{} into a \LocalToken{}. A similar idea can be found in the red region. Assuming that we want to switch Token 7 to a \GlobalToken{}, then Tokens 5,6 will be split into a new subsequence and we will no longer connect them with Token 8, 9, 10 since they belong to two different sub-sequences after the switch. Hence, this value in the red regions measures the regret if we mistakenly classify a \GlobalToken{} as non-\GlobalTokens.

\begin{wrapfigure}[11]{r}{0.45\textwidth}
 \vspace{-3.5mm}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/GradUpdate/CollectedGrad.drawio.png}
    \caption{The gradient term $d \tokenstates^{G-}_{i}$ for Token 4 and 7}
    \label{fig:GradTokenGrad}
\end{wrapfigure}

In practice, the $d \tokenstates^{G-}_{i}$ values for \GlobalToken{} can be easily collected since we can easily get the last and next $\EndSeqHard$ index with a scan function and collect the gradients to the corresponding positions. However, for those values on non \GlobalTokens, since the entire sequence can be a sub-sequence if no \GlobalToken{} exists in the current sequence, collecting all the values that are below and on the left side of the \LocalTokens{} might be too expensive. Hence, we ask each \LocalToken{} can only visit a fixed number of tokens on both sides. In this case, this operation is equivalent to a convolutional operation with weights of either 1 or 0.  We set this value as 16 in our experiments to fit the grid sizes in triton. 

\subsection{Optimizing for sparser Attention Maps}
The sparse regularization term $\SparseRegularizedPar$ is directly applied to the corresponding gradients for the \GlobalTokens{} and \LocalTokens{}. This value should penalize the number of unmasked tokens for each row. While the number of unmasked tokens for \GlobalToken{} and \LocalTokens{} in column $i$ are $\seqlen - i$ and $\EndSeqHardTrue_i - i$ with $\EndSeqHardTrue_i$ defined in Section~\ref{sec:natstrain}. Hence, we have:
\begin{align}
    d\EndSeqHard^{G_{sparse}}_i &= \SparseRegularizedPar \times \frac{\seqlen - i}{\seqlen}\label{eq:gradsparseG}\\
    d\EndSeqHard^{L_{sparse}}_i &= \SparseRegularizedPar \times \frac{\EndSeqHardTrue_i - i}{\seqlen}\label{eq:gradsparseI}
\end{align}

Combining Eq. ~\ref{eq:gradmsk}, ~\ref{eq:gradcollected1}, ~\ref{eq:gradsparseG} and Eq. ~\ref{eq:gradsparseI}, we have:
\begin{align}
    d \tokenstates^G_{i} &= \sum_{j} d \attMSK_{i,j} \times  \attMSK_{i,j}^{G}  \times \attMSK_{i,j}^{casual} + d\EndSeqHard^{G_{sparse}}_i - d \tokenstates^{G-}_{i} \\
    d \tokenstates^L_{i} &= \sum_{j} d \attMSK_{i,j} \times  \attMSK_{i,j}^{L} \times \attMSK_{i,j}^{casual} + d\EndSeqHard^{L_{sparse}}_i\\
    d \tokenstates^{SW}_{i} &= \sum_{j} d \attMSK_{i,j} \times  \attMSK_{i,j}^{SW} \times \attMSK_{i,j}^{casual}
\end{align}
with $\attMSK_{i,j}^{G}, \attMSK_{i,j}^{SW}, \attMSK_{i,j}^{L}$ defined in Eq. ~\ref{eq:mskglobal}, ~\ref{eq:msklocalfwd} and ~\ref{eq:msksw} and $\attMSK_{i,j}^{casual}$ is a casual attention mask. 

\section{Experiments Details}
\subsection{Collecting the Fine-tune Training Set}
To fine-tune ~\ourname{} on LLMs, we collect the training datasets from different tasks:
\begin{compactitem}
\item Multi-Document QA: HotPotQA~\cite{yang-emnlp18a}, 2WikiMultihopQA~\cite{ho-coling20a}, MuSiQue~\cite{trivedi-tacl22a}, and DuReader (zh)~\cite{he-aclwqa18a}
\item Single-Document QA: NarrativeQA~\cite{kocisky-tacl18a} and Qasper~\cite{dasigi-naacl21a}
\item Summarization: GovReport~\cite{huang-naacl21a}, QMSum~\cite{zhong-naacl21a}, MultiNews~\cite{fabbri-acl19a}, and VCSUM (zh)~\cite{wu-aclf23a}
\item Few-shot Leraning: TREC~\cite{li-coling02a}, TriviaQA~\cite{joshi-acl17a},  and SAMSum~\cite{gliwa-arxiv19a} 
\item Code Completion: LCC~\cite{guo-icml23a} and RepoBench-P~\cite{liu-iclr24b}
\end{compactitem}

\begin{wraptable}[19]{r}{10cm}
    \centering
    \vspace{0mm}
    \scalebox{0.8}{\input{tables/longbenchRes/mistral-25}}
    \caption{LongBench Results with 25\% Budget Size on Mistral-7B.~\label{tab:ResLBMistra25}.}
\end{wraptable}


For all the few-shot learning datasets, following ~\citet{bai-acl24a}, we randomly concatenate multiple question-answer pairs into one single extended context as one piece of data. The number of concatenated samples for the TREC dataset ranges from$[10, 100]$. This value is $[2,6]$ for TriviaQA and $[10, 50]$ for SAMSum. Additionally, for the datset that do not have enough context length (e.g., the DuReader dataset), we also merge multiple documents into one piece of data (in our case, this value is 4).

We do not collect all the data whose length goes beyond a threshold to ensure that the context can be fitted into our model. Additionally, we collect at most 500 pieces of data in each dataset since some datasets might not contain pieces of data. In the end, this training set contains 6436 pieces of data in total.

\section{Further Results on LongBench Dataset}

We show additional results on fine-tuning LLM on the LongBench dataset here. Table~\ref{tab:ResLBMistra25} shows the results with 25\% KV budgets on Mistral-7B-Instruct-v0.3. We found that to achieve the same level of sparsity, Mistral-7B requires a larger sparse regularization value $\SparseRegularizedPar$ (the value that we used in Table~\ref{tab:ResLBMistra25} is $1e-6$, which is 10 times larger than the one that we used for Llama3-8B models). The result confirms our conlcusion that ~\ourname{} outperforms the other baselines in most datasets under similar budget level (and many times with an even smaller budgets). 


\begin{table}[H]
    \centering
    \begin{tabular}{c c}
       LLAMA3-8B 50\% Budget & Mistral-7B 50\% Budget \\
       \scalebox{0.75}{\input{tables/longbenchRes/llama3-50}} &
       \scalebox{0.75}{\input{tables/longbenchRes/mistral-50}}\\
    \end{tabular}
    % {\caption[LLama3]{LLama}
    % \scalebox{0.8}{\input{tables/longbenchRes/llama3-50}}}
    % \hfill
    % {\caption[Mistral]{Mistral}
    % \scalebox{0.8}{\input{tables/longbenchRes/mistral-50}}}

    \caption{LongBench Results with 50\% Budget Size LLama3-8B and Mistral-7B. ~\label{tab:ResLLB50}. }
\end{table}

Table ~\ref{tab:ResLLB50} shows the evaluation results on LongBench with 50\% budgets. Despite having fewer KV cache budgets in all the datasets, ~\ourname{} still achieves comparable performance on the LLama3-8B model and better results on the Mistral model and generally performs comparable to the results with the full attention transformers.  


\section{Ablation Study}


\subsection{Sparse Regularization Term ~$\SparseRegularizedPar$}
\begin{table}[H]
    \centering
    \scalebox{0.75}{\input{tables/ablation/alpha/llama3-lambda}} 
    \caption{Ablation Study of Sparse Regularization values $\SparseRegularizedPar$ for LLama3-8B~\label{tab:AblationlambdaLLama3}}
\end{table}

\begin{table}[H]
   \centering
    \scalebox{0.75}{\input{tables/ablation/alpha/mistral-lambda}} 
    \caption{Ablation Study of Sparse Regularization values $\SparseRegularizedPar$ for Mistal-7B~\label{tab:AblationlambdaMistral}}
\end{table}

We first study the impact of sparse regularization terms. The result is shown in Table ~\ref{tab:AblationlambdaLLama3} and ~\ref{tab:AblationlambdaMistral}. We underline the results that are better than the optimal baselines with $25\%$ budgets, and bold the results that are better than the optimal baselines with $50\%$. Despite that \ourname{}  in Table~\ref{tab:ResLLBlama25} ($\ourname{}\  1e-7$) and Table~\ref{tab:ResLBMistra25} ($\ourname{}\  1e-6$) used more than $25\%$ overall KV budgets for some tasks, here we show $\ourname{}$ could still outperform many of the corresponding optimal baselines with a even lower KV budget. 


Table ~\ref{tab:AblationlambdaLLama3} and ~\ref{tab:AblationlambdaMistral} show that stronger $\SparseRegularizedPar$ generally results in a smaller valid KV cache size. While the order for compression rates for different datasets is consistent with different $\SparseRegularizedPar$ settings.  For most tasks, a compression rate between $10\%$ to $20\%$ already results in predictions that are similar to the full attention. 


\subsection{Sliding Window Length \SlidingWindowSize}
Another important hyperparameter for \ourname{} is the sliding window size  \SlidingWindowSize.  We apply different sliding window sizes \SlidingWindowSize{} ($64, 128, 256, 384, 512$) to fine-tune the Llama3-8B model (with  $\SparseRegularizedPar=1e-7$).

\begin{table}[H]
    \centering
    \scalebox{0.7}{\input{tables/ablation/windowlength/llama3-windowlength}}
        \caption{Ablation Study of Sliding Window Length $\SlidingWindowSize$ for Llama3~\label{tab:AblationSWLLama}}
\end{table}

 The result is shown in Table~\ref{tab:AblationSWLLama}. Overall, all the approaches performs similarily. However, smaller sliding window size generally results in an overall larger KV cache size. A reduced sliding window size would force the model to apply more \GlobalTokens{} and \LocalTokens{} to construct the mid-range correlation since this distance cannot be covered by the sliding window tokens. However, as the number of sliding window size further increases, this compression rate might saturate since the remaining tokens might always require a long-range correlation whose distance is much larger than the sliding window size (e.g., these tokens might require the correlation between two tokens whose distances are larger than 1k or even more).


\begin{figure}[h]
    \centering
    \centering
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/llama/kv_size_Llama_1e7_multi_news.png}}
    \hfill
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/mistral/kv_size_Mistral_1e6_multi_news.png}}
    \vfill
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/llama/kv_size_Llama_5e8_multi_news.png}}
    \hfill
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/mistral/kv_size_Mistral_5e7_multi_news.png}}
    \caption{KV size of LLama and Mistral on MultiNews Dataset~\label{fig:kvsizeMultiNews}}
\end{figure}

\section{Further Results on KV size distributions}
%Here, we provide more KV distribution results with different datasets and hyperparameters. 

Figure ~\ref{fig:kvsizeMultiNews} shows the KV cache sizes for LLama3-8B and Mistral-7B on the MultiNews Dataset. The two models still share some similar structure: both modules assign more budgets to the first few layers. However, the KV cache sizes are more uniformly distributed in Mistra compared to LLama3: in LLama3, most of the time, one or two heads with a much larger KV cache sizes compared to the other heads in the same layer, while in Mistral models, this is not so obvious.  

\begin{figure}[h]
    \centering
    \centering
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/llama/kv_size_Llama_1e6_dureader.png}}
    \hfill
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/mistral/kv_size_Mistral_5e6_dureader.png}}
    \vfill
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/llama/kv_size_Llama_1e8_dureader.png}}
    \hfill
    \subfigure{\includegraphics[width=0.475\textwidth]{figures/Exp/KVSizes/mistral/kv_size_Mistral_5e8_dureader.png}}
    \vfill

    \caption{KV size of LLama and Mistral on DuReader Dataset}
    \label{fig:kvsizeDuReader}
\end{figure}
Figure ~\ref{fig:kvsizeDuReader} shows another example on the DuReader dataset. This time, we check two other cases: one with a relatively lowered KV cache size, as shown in the upper part of Figure~\ref{fig:kvsizeDuReader}. The KV cache sizes are further sparsely distributed even in the shallower layers. However, the important layers in different models are not fixed: in LLama3, 9 layers (1, 3, 5, 6, 9, 11, 15, 16, 17) require a relatively larger budget while this value is much smaller for Mistral: only layer 3, 8, 13, 16 and 19 requires a relatively larger KV cache size. This further shows the importance of designing data and model adaptive approach for compressing the KV cache optimization policies. 


%~\cite{yang-emnlp18a, ho-coling20a, trivedi-tacl22a, he-aclwqa18a,kocisky-tacl18a, dasigi-naacl21a, huang-naacl21a, zhong-naacl21a, fabbri-acl19a, joshi-acl17a, gliwa-arxiv19a, guo-icml23a, liu-iclr24b}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
