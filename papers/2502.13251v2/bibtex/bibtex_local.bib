@misc{gutenberg,
    key = "PG19",
    note = "\url{https://www.gutenberg.org/}"
}

@misc{llama8b,
    key = "Llama-3 8B Gradient Instruct 1048k",
    authro={gradientai},
    note = "\url{https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k}"
}

@misc{grattafiori-arxiv24a1,
      title={The Llama 3 Herd of Models}, 
      author={A. Grattafiori and A. Dubey and A. Jauhri and A. Pandey and A. Kadian and A. Al-Dahle and A. Letman and et al.},
      year={2024},
      journal={arXiv:2407.21783 [cs.AI]},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{jiang-arxiv23a,
      title={Mistral 7B}, 
      author={A. Jiang and A. Sablayrolles and A. Mensch and C. Bamford and D. Chaplot and D. de las Casas and et al.},
      year={2023},
      journal={arXiv: 2310.06825 [cs.CL]},
}

@article{gemini-arxiv24a,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Team Gemini and et al.},
      year={2024},
      journal={arXiv:2312.11805 [cs.CL]},
      eprinttype    = {arXiv},
      primaryClass={cs.CL},
}

@article{openai-24a,
      title={GPT-4 Technical Report}, 
      author={OpenAI and et al.},
      year={2024},
      journal={arXiv:2303.08774 [cs.CL]},
      eprinttype={arXiv},
}

@article{child-arxiv19a,
  author       = {R. Child and
                  S. Gray and
                  A. Radford and
                  I. Sutskever},
  title        = {Generating Long Sequences with Sparse Transformers},
  journal      = {arXiv:1904.10509 [cs.LG]},
  eprinttype    = {arXiv},
      year={2019},
}

@inproceedings{ge-iclr24a,
  author       = {S. Ge and
                  Y. Zhang and
                  L. Liu and
                  M. Zhang and
                  J. Han and
                  J. Gao},
  title        = {Model Tells You What to Discard: Adaptive {KV} Cache Compression for
                  LLMs},
  crossref     = {iclr24},
}

@article{wang-arxiv24a,
      title={Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks}, 
      author={Z. Wang and B. Jin and Z. Yu and M. Zhang},
      year={2024},
      journal={arXiv:2407.08454 [cs.CL]},
      eprinttype={arXiv},
}

@inproceedings{mohtashami-neurips23a,
title={Random-Access Infinite Context Length for Transformers},
author={A. Mohtashami and M. Jaggi},
crossref = {neurips23},
url={https://openreview.net/forum?id=7eHn64wOVy}
}

@article{xiao-arxiv23a,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={G. Xiao and Y. Tian and B. Chen and S. Han and M. Lewis},
      journal      = {arXiv:2309.17453 [cs.CL]},
      eprinttype={arXiv},
      year  = 2023
}

@inproceedings{nawrot-icml24a,
  author       = {P. Nawrot and
                  A. Lancucki and
                  M. Chochowski and
                  D. Tarjan and
                  E. Ponti},
  title        = {Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference},
  crossref = {icml24},
  year         = 2024,
}

@article{beltagy-arxiv20a,
  author       = {I. Beltagy and
                  M. E. Peters and
                  A. Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {arXiv:2004.05150 [cs.CL]},
      eprinttype={arXiv},
year = 2024,
}

@inproceedings{anagnostidis-neurips23a,
  author       = {S. Anagnostidis and
                  D. Pavllo and
                  L. Biggio and
                  L. Noci and
                  A. Lucchi and
                  T. Hofmann},
  title        = {Dynamic Context Pruning for Efficient and Interpretable Autoregressive
                  Transformers},
  crossref = {neurips23}, 
  url          = {https://openreview.net/forum?id=uvdJgFFzby},
}

@inproceedings{kim-kdd22a,
  author       = {S. Kim and
                  S. Shen and
                  D. Thorsley and
                  A. Gholami and
                  W. Kwon and
                  J. Hassoun and
                  K. Keutzer},
  title        = {Learned Token Pruning for Transformers},
   crossref = {kdd22},
  pages        = {784--794},
  url          = {https://doi.org/10.1145/3534678.3539260},
}

@inproceedings{maddison-iclr17a,
  author       = {C. Maddison and
                  A. Mnih and
                  Y.Teh},
  title        = {The Concrete Distribution: {A} Continuous Relaxation of Discrete Random
                  Variables},
  crossref = {iclr17},
  url          = {https://openreview.net/forum?id=S1jE5L5gl},
}


@article{gu-arxiv23a,
  author       = {A. Gu and
                  T. Dao},
  title        = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  journal      = {arXiv:2312.00752 [cs.LG]},
year = 2023

}

@inproceedings{peng-emnlp23a,
  author       = {B. Peng and
                  E. Alcaide and
                  Q. Anthony and
                  A. Albalak and
                  S. Arcadinho and
                  S. Biderman and
                  et al.},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{RWKV:} Reinventing RNNs for the Transformer Era},
 crossref = {emnlp23},
  url          = {https://doi.org/10.18653/v1/2023.findings-emnlp.936},
  pages        = {14048--14077},
}


@article{sun-arxiv23a,
  author       = {Y. Sun and
                  L. Dong and
                  S. Huang and
                  S. Ma and
                  Y. Xia and
                  J. Xue and
                  J. Wang and
                  F. Wei},
  title        = {Retentive Network: {A} Successor to Transformer for Large Language
                  Models},
  journal      = {arXiv:2307.08621 [cs.CL]},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-08621.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
 year = {2023},
}

@inproceedings{katharopoulos-icml20a,
  author       = {A. Katharopoulos and
                  A. Vyas and
                  N. Pappas and
                  F. Fleuret},
  title        = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention},
  crossref = {icml20},
  url          = {http://proceedings.mlr.press/v119/katharopoulos20a.html},
  pages        = {5156--5165},
}

@article{beck-arxiv24a,
  author       = {M. Beck and
                  K. P{\"{o}}ppel and
                  M. Spanring and
                  A. Auer and
                  O. Prudnikova and
                  M. Kopp and
                  G. Klambauer and
                  J. Brandstetter and
                  S. Hochreiter},
  title        = {xLSTM: Extended Long Short-Term Memory},
  journal      = {arXiv:2405.04517 [cs.LG]},
  url          = {https://doi.org/10.48550/arXiv.2405.04517},
  doi          = {10.48550/ARXIV.2405.04517},
 year=            {2024},
}

@article{su-nc24a,
  author       = {J. Su and
                  M. Ahmed and
                  Y. Lu and
                  S. Pan and
                  W. Bo and
                  Y. Liu},
  title        = {RoFormer: Enhanced transformer with Rotary Position Embedding},
  pages        = {127063},
  journal      = {Neurocomputing},
  url          = {https://doi.org/10.1016/j.neucom.2023.127063},
year= {2024}
}

@misc{claude-24a,
author = {Anthropic},
title = {The Claude 3 Model Family: Opus, Sonnet, Haiku},
URL = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
year = {2024}
}

@article{shi-arxiv24a,
  author       = {L. Shi and
                  H. Zhang and
                  Y. Yao and
                  Z. Li and
                  H. Zhao},
  title        = {Keep the Cost Down: {A} Review on Methods to Optimize LLM' s
                  KV-Cache Consumption},
  journal      = {arXiv:2407.18003 [cs.CL]},
  volume       = {abs/2407.18003},
  year         = {2024},
}

@inproceedings{liu-neurips23a,
  author       = {Z. Liu and
                  A. Desai and
                  F. Liao and
                  W. Wang and
                  V. Xie and
                  Z. Xu and
                  A. Kyrillidis and
                  A. Shrivastava},
  title        = {Scissorhands: Exploiting the Persistence of Importance Hypothesis
                  for {LLM} {KV} Cache Compression at Test Time},
  crossref     =  {neurips23},
  url          = {https://openreview.net/forum?id=JZfg6wGi6g},
}

@inproceedings{zhang-neurips23a,
  author       = {Z. Zhang and
                  Y. Sheng and
                  T. Zhou and
                  T. Chen and
                  L. Zheng and
                  R. Cai and
                  Z. Song and
                  Y. Tian and
                  C. R{\'{e}} and
                  C. W. Barrett and
                  Z. Wang and
                  B. Chen},
  title        = {{H2O:} Heavy-Hitter Oracle for Efficient Generative Inference of Large
                  Language Models},
  crossref     = {neurips23},
  url          = {https://openreview.net/forum?id=RkRrPp7GKO},
}

@inproceedings{zhang-icml24a,
  author       = {Y. Zhang and
                  Y. Du and
                  G. Luo and
                  Y. Zhong and
                  Z. Zhang and
                  S. Liu and
                  R. Ji},
  title        = {CaM: Cache Merging for Memory-efficient LLMs Inference},
  crossref     = {icml24},
  url          = {https://openreview.net/forum?id=LCTmppB165},
}

@article{lou-arxiv24a,
  author       = {C. Lou and
                  Z. Jia and
                  Z. Zheng and
                  K. Tu},
  title        = {Sparser is Faster and Less is More: Efficient Sparse Attention for
                  Long-Range Transformers},
  journal      = {arXiv:2406.16747, [cs.CL]},
  volume       = {abs/2406.16747},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.16747},
}

@inproceedings{nawrot-acl23a,
  author       = {P. Nawrot and
                  J. Chorowski and
                  A. Lancucki and
                  E. Ponti},
  title        = {Efficient Transformers with Dynamic Token Pooling},
 crossref       = {acl23},
  pages        = {6403--6417},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.353},
}

@article{klein-arxiv24a,
  author       = {A. Klein and
                  J. Golebiowski and
                  X. Ma and
                  V. Perrone and
                  C. Archambeau},
  title        = {Structural Pruning of Pre-trained Language Models via Neural Architecture
                  Search},
  journal      = {arXiv: 2405.02267, [cs.LG]},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.02267},
}

@article{de-arxiv24a,
      title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models}, 
      author={S. De and S. Smith and A. Fernando and A. Botev and G. Cristian-Muraru and A. Gu and R. Haroun and L. Berrada and Y. Chen and S. Srinivasan and G, Desjardins and A. Doucet and D. Budden and Y. Teh and R, Pascanu and N. De Freitas and C. Gulcehre},
      year={2024},
      journal={arXiv: 2402.19427, [cs.LG]},
      url={https://arxiv.org/abs/2402.19427}, 
}

@article{lieber-arxiv24a,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={O. Lieber and B. Lenz and H. Bata and G. Cohen and J. Osin and I. Dalmedigos and E. Safahi and S. Meirom and Y. Belinkov and S. Shalev-Shwartz and O. Abend and R. Alon and T. Asida and A. Bergman and R. Glozman and M. Gokhman and A. Manevich and N. Ratner and N. Rozen and E. Shwartz and M. Zusman and Y. Shoham},
      year={2024},
      journal      = {arXiv: 2403.19887, [cs.LG]},
      url={https://arxiv.org/abs/2403.19887}, 
}

@inproceedings{rae-iclr20a,
  author       = {J. Rae and
                  A. Potapenko and
                  S. Jayakumar and
                  C. Hillier and
                  T. Lillicrap},
  title        = {Compressive Transformers for Long-Range Sequence Modelling},
  crossref     = {iclr20},
  year         = {2020},
  url          = {https://openreview.net/forum?id=SylKikSYDH},
}

@misc{karpathy-github22a,
  author = {Andrej Karpathy},
  title = {\text{NanoGPT}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
}

@misc{ba-arxiv16a,
      title={Layer Normalization}, 
      author={J. Ba and J. Kiros and G. Hinton},
      year={2016},
      journal={arXiv: 1607.06450, [stat.ML},
      url={https://arxiv.org/abs/1607.06450}, 
}

@inproceedings{petersen-neurips22a,
  author       = {F. Petersen and
                  C. Borgelt and
                  H. Kuehne and
                  O. Deussen},
  title        = {Deep Differentiable Logic Gate Networks},
  crossref = {neurips22}
}


@inproceedings{yu-cvpr22a,
  author       = {W. Yu and
                  M. Luo and
                  P. Zhou and
                  C. Si and
                  Y. Zhou and
                  X. Wang and
                  J. Feng and
                  S. Yan},
  title        = {MetaFormer is Actually What You Need for Vision},
  pages        = {10809--10819},
crossref = {cvpr22}, 
}}

@misc{chen-arxiv24a,
      title={SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator}, 
      author={G. Chen and H. Shi and J. Li and Y. Gao and X. Ren and Y. Chen and X. Jiang and Z. Li and W. Liu and C. Huang},
      year={2024},
      journal={arXiv: 2412.12094, [cs.CL]},
      url={https://arxiv.org/abs/2412.12094}, 
}


@inproceedings{dao-iclr24a,
  title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={T. Dao},
crossref = {iclr24},
}

@article{xiao-arxiv24a,
        title={DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads},
        author={G. Xiao and J. Tang and J. Zuo and J. Guo and S. Yang and H. Tang and Y. Fu and S. Han},
        year={2024},
        journal      = {arXiv:2410.10819 [cs.CL]},
        eprinttype    = {arXiv},
}

@misc{zhang-arxiv23c,
      title={Benchmarking Large Language Models for News Summarization}, 
      author={T. Zhang and F. Ladhak and E. Durmus and P. Liang and K. McKeown and T. Hashimoto},
      year={2023},
      journal={arXiv: 2301.13848 [cs.CL]},
      url={https://arxiv.org/abs/2301.13848}, 
}

@misc{liu-iclr24b,
      title={RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems}, 
      author={T. Liu and C. Xu and J. McAuley},
      year={2024},
      url={https://arxiv.org/abs/2306.03091},
      crossref ={iclr24},
}

@inproceedings{dasigi-naacl21a,
  author       = {P. Dasigi and
                  K. Lo and
                  I. Beltagy and
                  A. Cohan and
                  N. Smith and
                  M. Gardner},
  title        = {A Dataset of Information-Seeking Questions and Answers Anchored in
                  Research Papers},
year = {2021},
  crossref = {naacl21}, 
  pages        = {4599--4610},
}


@article{kocisky-tacl18a,
  author       = {T. Kocisk{\'{y}} and
                  J. Schwarz and
                  P. Blunsom and
                  C. Dyer and
                  K. Hermann and
                  G. Melis and
                  E. Grefenstette},
  title        = {The NarrativeQA Reading Comprehension Challenge},
  journal      = {Transactions of the Association for Computational Linguistics},
  volume       = {6},
  pages        = {317--328},
  year         = {2018},
  url          = {https://doi.org/10.1162/tacl\_a\_00023},
}

@inproceedings{guo-icml23a,
  author       = {D. Guo and
                  C. Xu and
                  N. Duan and
                  J. Yin and
                  J. McAuley},
  title        = {LongCoder: {A} Long-Range Pre-trained Language Model for Code Completion},
  crossref = {icml23},
  pages        = {12098--12107},
  url          = {https://proceedings.mlr.press/v202/guo23j.html},
}

@inproceedings{liu-icml23a,
  author       = {Z. Liu and
                  J. Wang and
                  T. Dao and
                  T. Zhou and
                  B. Yuan and
                  Z. Song and
                  A. Shrivastava and
                  C. Zhang and
                  Y. Tian and
                  C. R{\'{e}} and
                  B. Chen},
  title        = {Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time},
  crossrf={icml23},
  url          = {https://proceedings.mlr.press/v202/liu23am.html},
  pages        = {22137--22176},
}

@article{yi-arxiv24a,
  author       = {Z. Yi and
                  J. Ouyang and
                  Y. Liu and
                  T. Liao and
                  Z. Xu and
                  Y. Shen},
  title        = {A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems},
  volume       = {abs/2402.18013},
  year         = {2024},
      journal={arXiv: 2402.18013 [cs.CL]},
}

@article{feng-arxiv24a,
  author       = {Y. Feng and
                  J. Lv and
                  Y. Cao and
                  X. Xie and
                  S. Zhou},
  title        = {Ada-KV: Optimizing {KV} Cache Eviction by Adaptive Budget Allocation
                  for Efficient {LLM} Inference},
  volume       = {abs/2407.11550},
  year         = {2024},
  journal={arXiv: 2407.11550 [cs.CL]},
}

@misc{rabe-arxiv21a,
      title={Self-attention Does Not Need $O(n^2)$ Memory}, 
      author={M. Rabe and C. Staats},
      year={2021},
      journal={arXiv: 2112.05682 [cs.LG]},
}
@article{rae-arxiv19a,
author = {J. Rae and A. Potapenko and S. Jayakumar and
          C. Hillier and T. Lillicrap},
title = {Compressive Transformers for Long-Range Sequence Modelling},
journal = {arXiv preprint},
year = {2019},
      journal={arXiv: 1911.05507 [cs.LG]},
}

@inproceedings{bai-acl24a,
    title = "{L}ong{B}ench: A Bilingual, Multitask Benchmark for Long Context Understanding",
    author = "Y. Bai and X. Lv  and J. Zhang  and H. Lyu  and
      J. Tang  and Z. Huang  and Z. Du  and X. Liu  and A. Zeng   and L. Hou and Y. Dong  and J. Tang and J. Li",
    crossref = {acl24},
    pages = "3119--3137",
}

@inproceedings{yang-emnlp18a,
  author       = {Z. Yang and
                  P. Qi and
                  S. Zhang and
                  Y. Bengio and
                  W. Cohen and
                  R. Salakhutdinov and
                  C. D. Manning},
  title        = {HotpotQA: {A} Dataset for Diverse, Explainable Multi-hop Question
                  Answering},
  crossref = {emnlp18},
  pages        = {2369--2380},
}

@inproceedings{ho-coling20a,
  author       = {X. Ho and
                  A. Duong Nguyen and
                  S. Sugawara and
                  A. Aizawa},
  title        = {Constructing {A} Multi-hop {QA} Dataset for Comprehensive Evaluation
                  of Reasoning Steps},
  crossref = {coling20},
  pages        = {6609--6625},
}

@article{trivedi-tacl22a,
      title={MuSiQue: Multihop Questions via Single-hop Question Composition}, 
      author={H. Trivedi and N. Balasubramanian and T. Khot and A. Sabharwal},
      year={2022},
    journal      = {Trans. Assoc. Comput. Linguistics},
volume       = {10},
  pages        = {539--554},
  year         = {2022},

}

@inproceedings{he-aclwqa18a,
  author       = {W. He and
                  K. Liu and
                  J. Liu and
                  Y. Lyu and
                  S. Zhao and
                  X. Xiao and
                  Y. Liu and
                  Y. Wang and
                  H. Wu and
                  Q. She and
                  X. Liu and
                  T. Wu and
                  H. Wang},
  title        = {DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world
                  Applications},
  booktitle    = {Proceedings of the Workshop on Machine Reading for Question Answering@ACL 2018, Melbourne, Australia, July 19, 2018},
  year         = {2018},
  url          = {https://aclanthology.org/W18-2605/},
  pages        = {37--46},
}

@inproceedings{huang-naacl21a,
  author       = {L. Huang and
                  S. Cao and
                  N. Nova Parulian and
                  H. Ji and
                  L. Wang},
  title        = {Efficient Attentions for Long Document Summarization},
year = {2021},
crossref = {naacl21},
  pages        = {1419--1436},
}

@inproceedings{zhong-naacl21a,
  author       = {M. Zhong and
                  D. Yin and
                  T. Yu and
                  A. Zaidi and
                  M. Mutuma and
                  R. Jha and
                  A. Awadallah and
                  A. Celikyilmaz and
                  Y. Liu and
                  X. Qiu and
                  D. Radev},
  title        = {QMSum: {A} New Benchmark for Query-based Multi-domain Meeting Summarization},
  crossref     = {naacl21},
  year         = {2021},
  pages        = {5905--5921},

}

@inproceedings{fabbri-acl19a,
  author       = {A. Fabbri and
                  I. Li and
                  T. She and
                  S. Li and
                  D. Radev},
  title        = {Multi-News: {A} Large-Scale Multi-Document Summarization Dataset and
                  Abstractive Hierarchical Model},
  crossref  = {acl19},
  year         = {2019},
  pages        = {1074--1084},
}

@inproceedings{wu-aclf23a,
  author       = {H. Wu and
                  M. Zhan and
                  H. Tan and
                  Z. Hou and
                  D. Liang and
                  L. Song},
  title        = {{VCSUM:} {A} Versatile Chinese Meeting Summarization Dataset},
  crossref     = {aclf23},
  year         = {2023},
  pages        = {6065--6079},
}



@inproceedings{joshi-acl17a,
  author       = {M. Joshi and
                  E. Choi and
                  D. Weld and
                  L. Zettlemoyer},
  title        = {TriviaQA: {A} Large Scale Distantly Supervised Challenge Dataset for
                  Reading Comprehension},
  crossref = {acl17},
  year         = {2017},
  pages        = {1601--1611},
}

@inproceedings{li-coling02a,
  author       = {Xin Li and
                  Dan Roth},
  title        = {Learning Question Classifiers},
crossref = {coling02},
}


@article{gliwa-arxiv19a,
  author       = {B. Gliwa and
                  I. Mochol and
                  M. Biesek and
                  A. Wawer},
  title        = {SAMSum Corpus: {A} Human-annotated Dialogue Dataset for Abstractive Summarization},
  year         = {2019},
    journal={arXiv: 1911.12237 [cs.CL]},
}


@misc{ye-arxiv24a,
      title={Differential Transformer}, 
      author={T. Ye and L. Dong and Y. Xia and Y. Sun and Y. Zhu and G. Huang and F. Wei},
      year={2024},
      journal={arXiv: 2410.05258 [cs.CL]},
}

@article{fedus-jmlr23a,
  author       = {W. Fedus and
                  B. Zoph and
                  N. Shazeer},
  title        = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
                  and Efficient Sparsity},
  journal      = {Journal of Machine Learning Research (JMLR)},
  year           = 2023
}

@inproceedings{shazzer-iclr17a,
  author       = {N. Shazeer and
                  A. Mirhoseini and
                  K. Maziarz and
                  A. Davis and
                  Q. Le and
                  G. Hinton and
                  J. Dean},
  title        = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
                  Layer},
  crossref     = {iclr17},

  year         = {2017},
}


@misc{kamradt-github22a,
  author = {G. Kamradt},
  title = {\text{NanoGPT}},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}},
}

@article{li-arxiv24a,
  author       = {Y. Li and
                  Y. Huang and
                  B. Yang and
                  B. Venkitesh and
                  A. Locatelli and
                  H. Ye and
                  T. Cai and
                  P. Lewis and
                  D. Chen},
  title        = {SnapKV: {LLM} Knows What You are Looking for Before Generation},
  journal       = {arXiv: 2404.14469 [cs.CL]},
  year         = {2024},
}

@article{cai-arxiv24a,
  author       = {Z. Cai and
                  Y. Zhang and
                  B. Gao and
                  Y. Liu and
                  T. Liu and
                  K. Lu and
                  W. Xiong and
                  Y. Dong and
                  B. Chang and
                  J. Hu and
                  W. Xiao},
  title        = {PyramidKV: Dynamic {KV} Cache Compression based on Pyramidal Information
                  Funneling},
  journal       = {arXiv: 2406.02069 [cs.CL]},
  year         = {2024},
}

