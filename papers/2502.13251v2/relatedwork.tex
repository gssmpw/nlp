\section{Background and Related Work}
\subsection{Attention maps for transformers~\label{sec:attmap}}
Transformers~\citep{vaswani-neurips17a} computes the correlation between different tokens by mapping the input sequences into three variables, $\attQ$, $\attK$ and $\attV$. An attention map $\attMap$ is first computed by pairing $\attQ$ and $\attK$ and then scaled and normalized with a softmax function. Finally, this attention map $\attMap$ is multiplied by $\attV$. Additionally, a mask $\attMSK$ is attached to the Attention Map to guide the attention to focus only on certain tokens. This can be expressed as 
\begin{align}
    \attMap & = \frac{\attQ\attK^T}{\sqrt{\dHead}}\label{eq:attmap}\\
    \attO & = \softmax(\attMap  + \attMSK^{add})\attV\label{eq:attout}
\end{align}

The additive attention mask $\attMSK^{add}_{i,j} \in \{ -\inf, 0 \}$ controls if the transformer needs to construct the correlation between $\attQ_i$ and $\attK_{j}$.  A widely used mask $\attMSK^{add}$ used in transformers are casual masks~\citep{vaswani-neurips17a}, where the upper parts of the mask are filled with $-\inf$ each token can only query the token information before them. 

Computing the attention map of a sequence with length $\seqlen$ requires a complexity of $\mathcal{O}(\seqlen^2)$ for both time and memory. Techniques such as KV cache lowers this complexity to the $\mathcal{O}(\seqlenCache)$ during inference time with  $\seqlenCache$ being the length of the KV cache. However, as an expense, the cost of storing the KV cache gradually becomes unneglectable with the increasing size of large language models and the context length they could handle. 

Many studies aim to reduce the KV cache size by sparsifying the attention map with pre-defined rules. For instance, Sparse Attention~\citep{child-arxiv19a} combines the local sliding windows and atrous attention to approximate the full attention maps; Longformer~\citep{beltagy-arxiv20a} further proposes to predefine a set of special tokens as global attention that have full access to any other tokens and can be quired by any other tokens; Streaming LLM~\citep{xiao-arxiv23a} shows that by preserving the initial tokens and the most recent tokens during inference time could recover the performance of window attention. DuoAttention~\cite{xiao-arxiv24a} further illustrates that only a small fraction of the attention heads are critical for processing long context information and therefore, only those retrieval heads should be preserved with the full attention. LongCoder~\cite{guo-icml23a} pre-defines a set of bridge tokens to summarize the information in between and memory tokens that will not be dropped afterwards. SepLLM~\cite{chen-arxiv24a} shows that the information contained in a segment can be compressed into a set of special tokens. 

Alternatively, past attention maps can decide which tokens to evict or preserve. H2O~\citep{zhang-neurips23a}  and Scissorhand~\cite{liu-neurips23a} both identify the importance of each token based on their contributions to the attention maps and only preserve the most important tokens. Other approaches such as FastGen~\cite{ge-iclr24a}, SnapKV~\cite{li-arxiv24a}, PyramidKV~\cite{cai-arxiv24a}, and AadaKV~\cite{feng-arxiv24a} all applied pre-defined fixed rules to identify the important KV values. Additionally, instead of simply removing the unimportant tokens, we can also merge them into the existing tokens. CAM~\citep{zhang-icml24a} achieves this by merging the to-be-evicted tokens into the remaining tokens. 

In contrast to the fine-tuning-free approaches above,  alternatively, we can also fine-tune the target model to achieve the desired sparsity. ~\citet{kim-kdd22a} ask the model to learn to drop the tokens with lower attention scores. \citet{anagnostidis-neurips23a} shows that this sparsity can also be learned through sparse sigmoid functions. SPARSEK~\citep{lou-arxiv24a} only selects a constant number of KV Paris for each query by introducing a differentiable SparseK operator. Landmark Attention~\cite{mohtashami-neurips23a} insert landmark tokens after certain time steps and apply these landmark tokens to summarize the information of the tokens before that landmark token and recover. Furthermore, Dynamic Token Pooling~\citep{nawrot-acl23a} segments the input sequences as pooling layers to reduce the overall computational costs, while Dynamic Memory Compression~\citep{nawrot-icml24a}  tries to accumulate the tokens within the same subsequence into one single token. 

While these existing methods offer various ways to reduce KV cache size, they often rely on inflexible predefined rules and potentially inaccurate heuristics based on past attention maps. \ourname, in contrast, introduces a novel approach that learns to dynamically assign token roles during inference, enabling a more adaptive and efficient use of the KV cache. By treating token role assignment as an optimization problem, \ourname{} leverages principles from neural architecture search to jointly optimize token roles and model weights, leading to a more flexible and powerful attention mechanism.

\subsection{Neural Architecture Search}
Designing a neural architecture for a specific task might require a lot of trial and error. Neural Architecture Search (NAS) automates this process by searching in a pre-defined search space~\cite{elsken-jmlr19a}. Previous work on NAS mainly focused on searching within a discrete search space by sampling a new architecture from the search space, evaluating its performance, and updating the optimizers~\cite{zoph-iclr17a, zoph-cvpr18a}. However, training every network from scratch requires lots of GPU hours. One-Shot NAS~\cite{pham-icml18a} approaches instead share the same weights of the operators w.r.t. all the architectures in the search space. This allows to jointly optimize the architectures and weights. DARTS~\cite{liu-iclr19a} and GDAS~\cite{dong-cvpr19a} further relax the discrete search space into continuous values to optimize the architecture parameters with gradient descent. The One-Shot NAS approach allows the optimizers to efficiently search for the optimal architecture within a huge search space. Similarly, \ourname~ has multiple options for each token as the search space and is able to search for the optimal token types jointly with the model weights. However, unlike One-Shot NAS approaches that consider the optimization problem as a bilevel-optimization problem and optimize the model weights and architecture weights alternatively, we optimize the token state information and model weights within one forward and backward pass. This is similar to a mixture-of-expert (MOE) model~\cite{shazzer-iclr17a, fedus-jmlr23a}. However, instead of distributing data across all experts uniformly, we only select one expert for each token and assign all data to that expert.