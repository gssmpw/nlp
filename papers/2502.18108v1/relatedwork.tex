\section{Related Work}
\paragraph{Uncertainty Quantification for Question Answering}
 Several methods have been proposed to predict answer uncertainty in
 QA; however, none of them has analysed uncertainty in retrieval
 augmented QA models.  Many existing approaches rely on the assumption
 that output variation is an expression of model uncertainty
 \citep{kuhn2023semantic,farquhar2024semantic-hallu,chen-mueller-2024-quantifying}. For
 example, \citet{kuhn2023semantic} first cluster answers with similar
 meaning (in a sample) via natural language inference before computing
 entropy while \citet{chen-mueller-2024-quantifying} focus on
 \emph{black-box} models; they also compute similarities in the set of
 answers but associate them with a model self-judgement of
 confidence. These approaches are expensive to run at inference time
 for a production QA system, they require several inference steps in
 addition to performing similarity computations which can become more
 complex with longer answers \citep{zhang-etal-2024-fine}.
 \citet{hou2024decomposing} focus on quantifying aleatoric uncertainty
 (i.e.,~uncertainty in the data) caused by ambiguous questions, an
 approach which could be combined with ours.


\paragraph{Judging the Utility of Retrieved Passages}

Previous work has analysed the quality of retrieved passages
\citep{yu2023CoN,asai2024selfrag,wang2024rear,xu2024recomp,yoran2024making}
as they can be irrelevant or misleading.  \citet{asai2024selfrag} make
use of an external critic model to judge whether a question requires
retrieval (or not) and whether the retrieved passages are relevant to
formulate the answer.  While they analyse passage \emph{relevance},
this decision is taken by an external extreme-scale critic
(e.g.,~GPT-4) and used to fine-tune their QA model. In contrast, we
elicit \emph{utility} judgements from the target QA model and use
these to train a secondary small-scale model to predict passage
utility (i.e., our approach does not require LLM fine-tuning). Other
work creates auxiliary tasks around retrieved passages enforcing the
QA model to reason on them; e.g., by taking notes about each passage
\citep{yu2023CoN} or generating passage summaries
\citep{xu2024recomp}. These methods also use extreme-scale LLMs to
generate training data for \emph{fine-tuning} a retrieval augmented QA
model. \citet{park-etal-2024-enhancing} select in-context examples
with conflicting content (e.g., different dates for a given event) in
order to improve LLM reasoning on input passages.  These approaches
aim at improving QA performance while our primary goal is modelling QA
uncertainty.

\paragraph{Improving Retrieval via QA Performance}

Previous work has focused on jointly training the retriever and QA
modules end-to-end
\citep{lee-etal-2019-latent,rag-lewis,izacard2021distilling}.  This
joint training scheme is very expensive for current (extreme-scale)
LLMs. Our approach can be seen as an intermediate module between the
QA model and the external retriever and could be used to provide
feedback (i.e., utility scores) for fine-tuning the retriever,
however, we leave this to future
work. \citet{retrieval:quality:eval:siggir} evaluate the quality of
retrieval on QA tasks and show that external judgements
(e.g.,~query-document relevance labels) of passage utility correlate
poorly with QA performance.


\paragraph{Using a Separate Model to Predict Confidence} Some
approaches train a specific model to predict answer confidence scores
\citep{dong-etal-2018-confidence,kamath-etal-2020-selective,zhang-etal-2021-knowing,mielke-etal-2022-reducing}
by incorporating various features from the input and model output.
Our approach predicts answer uncertainty directly from individual
passage utilities but could be combined with other features (e.g.,
output sequence probability). Some work
\cite{kamath-etal-2020-selective,zhang-etal-2021-knowing} predicts
answer correctness in the context of Reading Comprehension (the
task of generating an answer based on a single supportive
passage). However, as there is no retrieval involved, the input
passage is by default useful, and the main goal is to detect answer
uncertainty due to the QA model being under-trained. In our setting,
the number and utility of passages varies leading to additional sources of
uncertainty (e.g., misleading information).

Our passage utility predictor is related to methods aiming to estimate
error \emph{directly} \cite{lahlou2023deup}, e.g., by training a
secondary model to estimate target model loss; instead, our predictor is trained
with sequence-level metrics, i.e.,~accuracy and entailment, which
measure error \emph{indirectly}.