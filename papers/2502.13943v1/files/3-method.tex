
\section{Methods}

\definecolor{lightgreen}{RGB}{0, 128, 0}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{files/figures/figure-method-TVD.pdf}
    \caption{We illustrate Token-level Value-guided Decoding (TVD) with a simple example. The \textcolor{lightgreen}{green token} denotes the selected tokens, while the \textcolor{gray}{gray token} indicates the tokens that were not selected. The question is \textit{3 * (1 + 1) = ?}, and the correct output is \textit{6}. In this case, the model exhibits low confidence (where $c_y < \tau$) when calculating the result of
    1+1, and subsequently determines which number to multiply by 3. The PRM should select the best token based on its judgment to arrive at the correct final answer. As shown in the top-left corner, for each token, the middle box represents the token itself, the bottom box shows the predicted confidence, and the box on the right displays the PRM score. The \textcolor{red}{red confidence score} indicates that the confidence of the Top-1 predicted candidate is lower than the threshold.
    }
    \label{fig:figure-TVD}
\end{figure*}

In this section, we first introduce how AdaptiveStep divides responses into reasoning steps, and then present how a PRM can be trained on these data, as shown in Figure~\ref{fig:method_main}.
At last, we introduce Token-level Value-guided Decoding (TVD) that can get better responses using the trained PRM.

\vspace{0.5em}

\subsection{AdaptiveStep}

Given a question $q \in Q$, we can generate $N$ responses with temperature-based random sampling using the language model $\pi$.
We denote generated responses as $\{s^1, s^2, \cdots, s^N\}$ with $s^n \in S$.
(For ease of notation, we omit the dependence of the response $s^n$ on $q$.)
In this way, we obtain a set of question-response pairs $(Q\times S)$.


To divide the responses into reasoning steps, we use the probability of the sampled token as the metric for \textit{model confidence}~\cite{Hills2024usinglogprobs}. Then we determine a threshold $\tau$, which is based on a certain proportion of the token count, such that the tokens below this threshold become a breaking point.

Specifically, the model confidence can be written as 
\begin{equation}
    c_{s^n_i} =  p(s^n_i | \pi, q, s^n_{<i})
\label{equ:confidence}
\end{equation}
where we use $s_i^n$ and $s^n_{<i}$ 
 to denote the $i$-th token and the tokens prior to the $i$-th token in the response respectively. Low model confidence at the $i$-th token indicates that the model is hard to determine the token selection at the $i$-th position, and therefore this position may become the starting point of a new reasoning step.

According to the above procedure, we divide the response $s^n$ into $K$ reasoning steps $s^n = \{r_1, r_2, ..., r_K\}$ where the last token within each reasoning step is associated with below-the-threshold model confidence.




\subsection{PRM Training}

To train a PRM based on the question-response pairs with divided reasoning steps, we first need to estimate the target reward for each reasoning step and then train a PRM that can predict the reward.

To estimate the target reward, we mainly follow the heuristic rollout method proposed by~\citet{wang2024mathshepherdverifyreinforcellms}. 
We rollout the response generation process $J$ times starting from each reasoning step, resulting in rollouts denoted as $\left\{p, r_1, ..., r_k, t_j\right\}_{k\in [K], j\in [J]}$, where $t_j$ is the $j$-th trajectory starting from a partial response.



Then, we estimate the target reward of this step based on the correctness of any decoded solution. We use hard estimation (HE) to estimate the reward for step the $r_k$.
HE indicates whether any of the responses starting from the current partial response can reach a correct answer. 
For our implementation, in the code generation tasks, we define correctness as whether the solution can pass all test cases; in the math reasoning tasks, we define correctness as whether the answer matches the ground truth.
Formally, the target reward can be estimated as

\begin{equation}
r_k^e=\left\{
\begin{aligned}
1, \qquad & \exists j \in [J], \{r_1, ..., r_k, t_j\} \text{ is correct } \\
0, \qquad & \mathrm{otherwise} \\
\end{aligned}
\right.
\label{eqa:HE}
\end{equation}

With the target rewards estimated based on the rollouts, we can train PRM using the following loss:

\begin{equation}
    \mathcal{L}_{PRM}^\theta = \sum_{k=1}^K r_k^e \log r_k^\theta+ (1 - r_k^e) \log (1 - r_k^\theta),
    \label{eq:prm_loss}
\end{equation}
where $r_k^e$ is the target reward and $r_k^\theta := R^\theta(p, r_1, \cdots, r_k)$ denotes the reward predicted by the PRM $R^\theta$.

\begin{figure*}[htbp]
    \centering
    
    \includegraphics[width=\textwidth]{files/figures/figure-bon-candidates.pdf} % 子图 (a)

    \caption{BoN results for the math tasks. We evaluate all PRMs on: (a) MetaMath-Llama generated GSM8k candidate solutions; (b) MetaMath-Mistral generated GSM8k candidates; (c) MetaMath-Llama generated MATH500 candidates; and (d) MetaMath-Mistral generated MATH500 candidates. The "-L" and "-M" suffixes denote the base models (Llama and Mistral, respectively). We report the evaluation results based on the released versions of other works.}
    \label{fig:bon_main}
\end{figure*}

\subsection{Token-level Value-guided Decoding}

The TVD strategy leverages the PRM to guide token selection during language model decoding. Specifically, when the model encounters a low confidence score (Top-1 probability $c_p < \tau$) in decoding, it triggers the PRM to evaluate the tokens associated with the highest $M$ probability given by the policy model $\pi$: $\mathbf{s_i^*}=\{s_i^1,s_i^2,\dots,s_i^M\}$.

Among these candidates, the PRM selects the token it considers the best based on its learned reward estimation mechanism: 
\begin{equation}
s_i=\arg \max _{s_i^m \in s_i^*} R^\theta\left(p, s_{<i},s_i^m\right),
\end{equation}

where $s_i$ is the token selected as the optimal choice for the low-confidence decoding position, and $R^\theta(\cdot)$ represents the score given by the PRM.

