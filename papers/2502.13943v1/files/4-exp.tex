
\section{Experiments and Analysis}



In this section, we first show our experiment setup, including the dataset usage, model selection, baselines, metrics, and parameter setup.  We then present the experimental results, followed by an analysis of the transferability, generalization, and features of the division.

\subsection{Experiments Setup}

\paragraph{Datasets and models:} We use MetaMathQA~\cite{yu2023metamath} to train Mistral-V0.1~\cite{jiang2023mistral7b} (Mistral), which is termed MetaMath-Mistral, to serve as $\pi$ in math domain, and use LeetCodeDataset\footnote{To train \MethodName \hspace{1pt} for code tasks, We collected 1,745 problems from the \textit{LeetCode problems} as our training set and 175 problems as the test set. The test cases for these data are manually collected from the LeetCode website (excluding the test cases within the problem). 
The solutions are gathered from GitHub open-sourced repositories, mainly from \href{https://github.com/doocs/leetcode}{https://github.com/doocs/leetcode}, checked by GPT-4, and cross-verified with the test cases.} training data to train Deepseek-Coder-Base~\cite{deepseek-coder}, which is called LCD-DS to serve as $\pi$ in the code domain. To get the math PRM training data, we sample MATH and GSM8k training datasets using MetaMath-Mistral and sample LeetCodeDataset training data using LCD-DS to generate code PRM training data. For evaluation, We use MATH500, the GSM8k test set, the LeetCodeDataset test set, and LiveCodeBench-V4~\cite{jain2024livecodebench}. To align with previous work and conduct further analysis, we train two math PRMs: \MethodName-L (based on Meta-Llama-3.1-8B~\cite{grattafiori2024llama3herdmodels}, which is called Llama in the following) and \MethodName-M (based on Mistral-V0.1), both with MetaMath-Mistral generated training data. And one code PRM: \MethodName-D (based on DeepSeek-Coder-Base) with LCD-DS generated training data.





\begin{table*}[htbp]
\caption{Token-level Value-guided Decoding results. A/P@1 refers to the inference model's greedy search performance, we use Accuracy@1 for math tasks, and Pass@1 for code tasks as the metrics. 
\textcolor{red}{\textuparrow} and \textcolor{green}{\textdownarrow} represent the performance improvement or decline compared to A/P@1. 
}
\centering
\begin{tabular}{lccccccc} 
\toprule
\textbf{Dataset} & \textbf{Inference Model} & \textbf{A/P@1}  & \textbf{Math-Shepherd} & \textbf{ER-PRM} & \textbf{\MethodName-L} / -M & \textbf{\MethodName-D} \\
\midrule
\multirow{2}{*}{\textbf{GSM8k}} &MetaMath-M &77.10 &75.66\textcolor{green}{\textdownarrow} &75.13\textcolor{green}{\textdownarrow} & \textbf{79.53}\textcolor{red}{\textuparrow} / 77.33\textcolor{red}{\textuparrow} & \slash \\
                                 &MetaMath-L &81.80 &81.73\textcolor{green}{\textdownarrow} &81.58\textcolor{green}{\textdownarrow} & \textbf{83.47}\textcolor{red}{\textuparrow} / 82.56\textcolor{red}{\textuparrow} & \slash \\
\midrule
\multirow{2}{*}{\textbf{MATH500}} & MetaMath-M & 25.00 & 27.60\textcolor{red}{\textuparrow} & 27.80\textcolor{red}{\textuparrow} & \textbf{28.60}\textcolor{red}{\textuparrow} / 26.80\textcolor{red}{\textuparrow} &  \slash\\
                                  & MetaMath-L & 38.80 & 41.00\textcolor{red}{\textuparrow} & 38.60\textcolor{green}{\textdownarrow} & \textbf{42.00}\textcolor{red}{\textuparrow} / 41.20\textcolor{red}{\textuparrow} & \slash \\

\midrule
\textbf{LeetCodeDataset}      & LCD-DS & 26.28 & \slash & \slash & \slash & 28.00\textcolor{red}{\textuparrow} \\
\textbf{LiveCodeBench} & LCD-DS & 19.21 & \slash & \slash & \slash & 19.92\textcolor{red}{\textuparrow} \\
\bottomrule
\end{tabular}

\label{tab:TVD_results_Math}
\end{table*}


\paragraph{Baselines and metrics:} There are several open-sourced PRMs in the math domain, we select Math-Shepherd~\cite{wang2024mathshepherdverifyreinforcellms} and ER-PRM~\cite{erprm} as our baselines. For the code domain, due to the limited availability of open-source code PRMs with competitive construction costs, we trained a code ORM as a baseline using the same data, with only the final rating position considered.

For all tasks, we evaluate the PRMs' performance using the Best of N (BoN) metric and further assess model capabilities with TVD. In math reasoning tasks, we evaluate whether the model's final answer matches the ground truth exactly. In the code tasks, we test the generated code by running it in a sandbox and checking if it passes all test cases. Following~\citet{wang2024mathshepherdverifyreinforcellms}, we use the minimum PRM score across all scored steps as the PRM's final judgment for a given candidate in BoN.


\paragraph{Parameter Settings:} We sample 30 times per data point and deduplicate the responses in Step 1. For labeling the PRM training data, we perform 8 rollouts per step using the same model $\pi$. This process generates 388k PRM training samples. We use MetaMath-Mistral-generated data to train the math PRM. And we get 49k PRM samples for the code PRM. In our PRM training data, each sample includes a labeling point at the end of the response. We divide the responses by 2\% of the token count.
The value is set according to \citet{kahneman2011thinking} which finds that deep thinking for humans accounts for 2\% of the total thinking.






\subsection{Overall Results} 

\paragraph{BoN Results} We report the BoN evaluation results for the math dataset in Figure~\ref{fig:bon_main}, and for the code dataset in Figure~\ref{fig:bon_code_main}, respectively.

In the math tasks, \MethodName-L \hspace{1pt} performs best across Figure~\ref{fig:bon_main}(a), \ref{fig:bon_main}(b) and \ref{fig:bon_main}(d) despite under more stringent conditions: the training data sources, and the construction costs and models. 1) For \textbf{the training data sources}, \MethodName\hspace{1pt} only utilizes the GSM8k and MATH training sets during training data construction, while both ER-PRM and Math-Shepherd used the MATH test set (without using MATH500), which results in our performance being inferior to theirs on MATH500. 2) For \textbf{the costs and models used in construction}, the data construction costs for \MethodName\hspace{1pt} is less than 70\% of that for the other two methods and only used a single construct model. In addition to the above problems that lead \MethodName-M \hspace{1pt} to poor performance in the MATH500 dataset,  we attribute its performance in~Figure~\ref{fig:bon_main}(a) to the training dataset is constructed by a single model, constrained its test-time transferability.

In the code tasks results shown in Figure~\ref{fig:bon_code_main}, \MethodName-D \hspace{1pt} demonstrates superior judgment ability. As N increases, the robustness of \MethodName-D outperforms that of ORM.




\begin{figure}[htbp]
    \centering

        \includegraphics[width=0.5\textwidth]{files/figures/figure-bon-candidates-code.pdf}

    \caption{BoN results for the code datasets, we test \MethodName-D \hspace{1pt} and a Code-ORM (ORM-D) on (a) LCD-DS generated LeetCodeDataset BoN candidates; (b) LCD-DS generated LiveCodeBench BoN candidates.}
    \label{fig:bon_code_main}
\end{figure}


\paragraph{TVD Results}

We report TVD results in Table~\ref{tab:TVD_results_Math}. In the math reasoning task, \MethodName \hspace{1pt} has consistently shown an ability to enhance the reasoning capacity of the inference models. While 
the performance guided by ER-PRM and Math-Shepherd does not always demonstrate improvement, we hypothesize this is due to that the inference models already perform well with the greedy search on GSM8k, needing a more precise score to provide better guidance. The results further demonstrate the accuracy of the token-level judgment of \MethodName. In the code generation task, \MethodName \hspace{1pt} has also achieved results surpassing greedy search by providing accurate judgment. 

\subsection{Transferability and Generalization Analysis}
\label{sec:transfer_gene}

In this part, we investigate whether \MethodName~\hspace{1pt} demonstrates model transferability and rating position, in-domain, and cross-domain generalization capability, and the performance of mixed-domain data-trained PRM. In our experiments, unless otherwise specified, the BoN candidate generator and TVD inference model is MetaMath-Mistral.


\paragraph{\MethodName \hspace{1pt}exhibit model transferability:} Since the quality of training data generated by rollout depends on the polity $\pi$, we explore the transferability of training data of our method. We get 371k PRM training data generated by MetaMath-Llama and conduct the same process as MetaMath-Mistral.
In Table~\ref{tab:transferability}, we find that training Mistral-V0.1 on data generated by MetaMath-Llama retains judgment ability, but its performance is weaker than that trained on data generated by the weak MetaMath-Mistral. This suggests that data generated through rollout has reasonable but limited transferability. Using multiple models for data construction, as in the Math-Shepherd, may be an effective strategy to enhance transferability.


\begin{table}[ht]
\caption{Transferability of PRM training data: \textbf{L to M} indicates training Mistral using PRM training data generated by MetaMath-Llama. \textcolor{red}{\textuparrow} and \textcolor{green}{\textdownarrow} denote performance improvement or decline compared to \MethodName-M.}
\centering
\begin{tabular}{lcc}  
\toprule
\textbf{Setup} & \textbf{Test Dataset} & \textbf{Bo64 / TVD} \\
\midrule
% \multirow{4}{*}{M to L}  & M-MATH500 & 41.20 /  \\ 
%                          & M-GSM8k   & 90.45 /  \\ 
%                          & L-MATH500 & 44.80 /  \\ 
%                          & L-GSM8k   & 91.20 /  \\
% \cline{2-3}
\multirow{4}{*}{L to M}  & M-MATH500 & 34.20\textcolor{green}{\textdownarrow} / 27.60\textcolor{red}{\textuparrow} \\ 
                         & M-GSM8k   & 83.40\textcolor{green}{\textdownarrow} / 77.94\textcolor{red}{\textuparrow} \\
                         & L-MATH500 & 41.80\textcolor{green}{\textdownarrow} / 41.40\textcolor{red}{\textuparrow} \\ 
                         & L-GSM8k   & 87.87\textcolor{green}{\textdownarrow} / 82.49\textcolor{green}{\textdownarrow} \\
\bottomrule
\end{tabular}

\label{tab:transferability}
\end{table}


\paragraph{\MethodName \hspace{1pt}exhibit rating position generalization:} We evaluate the rating position generalization of different PRMs and show the results in Table~\ref{tab:position_shift}. Three setups are employed in our experiments: \textbf{confidence}, \textbf{random}, and \textbf{hard}, we explain the setting in the caption of Table~\ref{tab:position_shift}. The performance of ER-PRM-L shows a significant difference between the hard and random setups, whereas the difference of \MethodName-L between the two setups is relatively small. Additionally, \MethodName-M performs better under the \textit{random} setup than under the \textit{confidence} setup, demonstrating its superior generalization ability in the rating position. We attribute this advantage to the diversity of rating point types in the \MethodName \hspace{1pt} training data.

\begin{table}[ht]
\caption{Rating position generalization. In the \textbf{confidence} setup, rating points are the positions where confidence falls below the threshold. In the \textbf{random} setup, rating points are selected at five random positions. In the \textbf{hard} setup, rating points are line breaks.}
\centering
\begin{tabular}{lcc}  
\toprule
\textbf{Models} & \textbf{Scoring Setup} & \textbf{Bo64} \\
\midrule
\multirow{2}{*}{\MethodName-L}    & confidence & 90.45 \\ 
                           & random     & 90.22 \\  
\cline{2-3}
\multirow{2}{*}{\MethodName-M}    & confidence & 85.82 \\ 
                           & random     & 86.96 \\  
\cline{2-3}
\multirow{2}{*}{MS-M}      & hard       & 86.50 \\ 
                           & random     & 86.20 \\ 
\cline{2-3}
\multirow{2}{*}{ER-PRM-L}  & hard       & 88.70 \\ 
                           & random     & 87.71 \\ 
\bottomrule
\end{tabular}

\label{tab:position_shift}
\end{table}


\paragraph{\MethodName \hspace{1pt}exhibit in-domain generalization:} We use \textbf{GSM-Symbolic} \cite{mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical}, which modified variables or sentences in the original GSM8k dataset, to test whether PRM can achieve in-domain generalization. We show our results in Table~\ref{tab:generalization_in}. We find that \MethodName \hspace{1pt} exhibits strong in-domain generalization as it achieves better results in TVD than greedy search, and selects the right samples in Bo64.

\begin{table}[ht]
\caption{In-domain generalization ability. The experiments are conducted on the GSM-Symbolic p2 dataset. \textcolor{red}{\textuparrow} indicates the performance improvement compared to greedy search.}
\centering
\begin{tabular}{ccc} 
\toprule
\textbf{PRM Model} &\textbf{Base} & \textbf{Bo64 / TVD} \\ 
\midrule
\MethodName-L  & 22.80 & 51.56 / 24.56\textcolor{red}{\textuparrow} \\ \cline{2-3} 
\MethodName-M  & 22.80 & 37.88 / 24.68\textcolor{red}{\textuparrow} \\ 
\bottomrule
\end{tabular}

\label{tab:generalization_in}
\end{table}

\paragraph{\MethodName \hspace{1pt}exhibit cross-domain generalization:} We assess the cross-domain generalizability of PRMs using two setups: evaluating the math PRM in the code datasets and evaluating the code PRM in the math datasets. Our results are shown in Figure~\ref{tab:generalization_cross}. We find that the \MethodName-L provides applicable guidance on code tasks and makes correct selections in BoN. However, \MethodName-D performs better on the more difficult MATH500 task but struggles on simple GSM8k. We hypothesize this is due to the long training data and long prompt in code PRM, as the GSM8k test data has a total length similar to the length of the prompt part of code data on average, resulting in fewer low-confidence points for the model to learn.


\begin{table}[ht]
\caption{Cross-domain generalization ability of the PRMs: \textbf{Source} represents the source domain and the corresponding model.
% where \MethodName-L refers to the Llama PRM trained on math data, and \MethodName-D refers to the Deepseek PRM trained on code data. 
\textbf{Target} represents the target dataset domain and the corresponding test data. \textcolor{red}{\textuparrow} and \textcolor{green}{\textdownarrow} indicate performance improvements or declines compared to the A/P@1 performance in Table~\ref{tab:TVD_results_Math}.}
\centering
\begin{tabular}{ccc} 
\toprule
\textbf{PRM Model}        & \textbf{Target} & \textbf{Bo64 / TVD} \\ 
\midrule
\multirow{2}{*}{\MethodName-L}  & Code-LCD  & 34.29\textcolor{red}{\textuparrow} / 28.00\textcolor{red}{\textuparrow}  \\ 
                                & Code-LCB       & 22.30\textcolor{red}{\textuparrow} / 19.21-  \\ 
\multirow{2}{*}{\MethodName-D}  & Math-GSM8k     &  75.13\textcolor{green}{\textdownarrow} /  75.28\textcolor{green}{\textdownarrow} \\ 
                                & Math-MATH500   &  30.00\textcolor{red}{\textuparrow} / 26.00\textcolor{red}{\textuparrow}  \\ 
\bottomrule
\end{tabular}

\label{tab:generalization_cross}
\end{table}

\paragraph{Mixed data benefits downstream performance:} Since both tasks are reasoning tasks, we explore whether mixing training data from different domains can enhance downstream performance. To this end, we conduct two experiments: 1) training Mistral on a mixed math and code dataset, and evaluating it on MATH500 and GSM8k; 2) training DeepSeek on an equal amount of randomly sampled math and code data, and evaluating it on LeetCodeDataset and LiveCodeBench. The results are shown in Table~\ref{tab:generalization_mix}. We find that mixing data improves PRM performance on math datasets, while on code datasets, performance improves only in the TVD scenario on LiveCodeBench. We hypothesize this outcome is due to the following reason: for the math PRM, mixing long code domain training data improves the PRM's judging ability. For the code PRM, code domain training data is more difficult to obtain. Adding new data doubles the dataset size but introduces shorter data. This results in decreasing the global rating ability relied upon by BoN while enhancing the local rating ability used by TVD.




\begin{table}[htbp]
\caption{The test results of the PRMs trained with a mixed training dataset. When the base model is Mistral, the \textit{M+C} training data consists of the MetaMATH-Mistral generated math dataset and full code training dataset. When the base model is Deepseek, the \textit{C+M} training data includes all of the code dataset and an equal amount of randomly sampled math training data. \textcolor{red}{\textuparrow} and \textcolor{green}{\textdownarrow} represent the performance improvement or decline compared to the no mixed data trained PRMs in the origin domain of test data.}
\centering
\begin{tabular}{ccc c} 
\toprule
\textbf{Base Model} & \textbf{Train} & \textbf{Test} & \textbf{Bo64 / TVD} \\ 
\midrule
\multirow{2}{*}{Mistral}  % & M   & GSM8k        & 85.82 / 77.33 \\ 
                          & M+C & GSM8k        & 86.35\textcolor{red}{\textuparrow} / 77.79\textcolor{red}{\textuparrow}  \\ 
                          % & M   & MATH500      & 34.80 / 26.80 \\ 
                          & M+C & MATH500      & 35.40\textcolor{red}{\textuparrow} / 29.00\textcolor{red}{\textuparrow}   \\ 
\midrule
\multirow{2}{*}{Deepseek} % & C   & LCD     & 37.71 / 28.00 \\ 
                          & C+M & LCD     & 37.71- / 28.00- \\ 
                          % & C   & LCB          & 25.53 / 19.92 \\ 
                          & C+M & LCB          & 24.96\textcolor{green}{\textdownarrow} / 20.33\textcolor{red}{\textuparrow} \\ 
\bottomrule
\end{tabular}

\label{tab:generalization_mix}
\end{table}


\subsection{Feature Analysis}
\label{sec:feature_ana}

In this part, we discuss the features of the AdaptiveStep division used in ASPRM and its advantages. 

\paragraph{Construction efficiency:} The training data construction of \MethodName \hspace{1pt} demonstrates superior efficiency in both domains. In the math domain, the training data for \MethodName\hspace{1pt} is generated using only a single MetaMath-Mistral model, with 30 samples per data point and 8 times rollouts per step. In contrast, ER-PRM performs fewer samples but conducts 16 times rollouts, while Math-Shepherd uses multiple models for sampling and rollouts. The average number of steps per sample and sample counts for each method are presented in Appendix~\ref{app:dataset_statistic}. As a result, the data construction costs for \MethodName\hspace{1pt} is less than 70\% of that for the other two. In the code domain, there are 14.4 lines on average per answer for the LeetCodeDataset training set, whereas only 5.69 steps are required for our method on average. 




\paragraph{Statistical features of the division:} There are several features and findings in the AdaptiveStep division statistics. For brevity, we refer to low-confidence tokens as "decision tokens" throughout this section. Taking the math PRM training data generated by Mistral as an example: 1) 3.85\% tokens in mathematical expressions contribute 21.03\% decision tokens; 2) only 2.7\% decision tokens are newline tokens;  3) the inference model exhibits low confidence at semantic word points, particularly at Conjunction (29.00\%), suggesting that continuous or transitional thinking is particularly challenging for the model.

For the code PRM training data: 1)  the majority of decision points occur in the Code Comment type (80\%), compared to the Code type (20\%), even though Code Comments tokens account for only 19\% of the total tokens; 2) a detailed analysis reveals that the Code Comment samples primarily fall into two subtypes: explaining what previous lines do and planning what to do in the following lines. The first subtype accounts for 9\% of the samples, while the second accounts for 91\%. This indicates that the inference model triggers more during the planning process than during the writing process when generating code; 3) by further analyzing the Code type, we find that \textit{Logical Operators}, \textit{Block Begin Keyword}, \textit{Control Statements} and \textit{Loop Statements} occupy a high proportion of low confidence proportion with a small number of tokens. This suggests that, in addition to pre-planning in Comment, the model still requires assistance at certain logical decision points during the writing process.

The statistical information indicates that the inference model is prone to performing low confidence in the calculation process, semantic word selection in mathematics reasoning tasks, and the planning process in code generation tasks. 
The full statistical results are provided in Appendix~\ref{app:feature_statistic}.


Our results in~\ref{sec:transfer_gene} and~\ref{sec:transfer_gene} indicate that PRM trained on mixed datasets can enhance downstream performance, making it possible to achieve better results in domains with hard-to-obtain data, such as code generation, at a lower cost. Based on the results and feature analysis in code data, we hypothesize that the mutual enhancement arises from both tasks being reasoning problems. Similar to the text reasoning process in mathematics, the Code Comments contain substantial content that outlines subsequent steps. Therefore, training on a mixture of both datasets allows the model to achieve improved results.

\section{Conclusion}

In this paper, we propose a new reasoning step dividing method, AdaptiveStep, along with a corresponding Process Reward Model (PRM), \MethodName. We test the effectiveness of the PRM on mathematical reasoning and code generation tasks. To train the code PRM, we collect a function-level LeetCode dataset. We effectively integrate the PRM into the standard LLM inference process, achieving improvements over greedy search without additional inference overhead by token-level guidance. Our experiments on widely used datasets demonstrate robust performance with reduced computational costs. Furthermore, we analyze model transferability and generalization, showing that \MethodName \hspace{1pt} exhibits both rating position, in-domain and cross-domain generalization. We also find that combining data from different domains further enhances PRM performance. Lastly, our feature analysis of the AdaptiveStep division confirms its effectiveness and informativeness.


% \clearpage

\section{Impact Statement}

AdaptiveStep is an automatic, highly informative, and effective method for dividing reasoning steps. It can be easily applied to a wide range of complex tasks across various domains, such as code generation (as demonstrated in our paper) and AI-driven scientific reasoning. Furthermore, our exploration of the properties of AdaptiveStep PRM and its training data features will contribute to advancing process reward assignment in LLMs, potentially shaping the development of more general PRMs.

\paragraph{Acknowledgement} We sincerely thank Zilin Zhu for providing valuable suggestions on efficiency optimizations of our code and Di Yang for his advice during the completion of the work.