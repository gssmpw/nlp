\vspace{-0.4cm}

\section{Introduction}

% (draft)

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.48\textwidth]{files/figures/figure-intro-main-new.pdf}    \caption{Rule-based reasoning step dividing (e.g., using line breaks or a fixed number of tokens) is automated but results in low informativeness at the end of the step and is difficult to apply in domains that hard to define rules. In contrast, manual step division provides high informativeness but is costly to scale and heavily reliant on the experts' domain knowledge. AdaptiveStep, which divides steps based on model confidence, addresses these challenges by offering automation, efficiency, high informativeness, and applicability across various domains.}
    \label{fig:example}
    \vspace{-0.5cm}
\end{figure}



Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks. However, even most advanced LLMs struggle to generate correct solutions when facing complex reasoning problems, such as mathematical reasoning and code generation tasks~\cite{huang2024largelanguagemodelsselfcorrect, tyen2024llmsreasoningerrorscorrect, mirzadeh2024gsmsymbolicunderstandinglimitationsmathematical, shen2024policy}. To address these challenges using the stepwise Chain of Thought (CoT) approach~\cite{wei2023chainofthoughtpromptingelicitsreasoning}, various strategies have been proposed by the research community~\cite{qin2024o1replicationjourneystrategic, deepseekai2025deepseekr1incentivizingreasoningcapability, kimiteam2025kimik15scalingreinforcement}. One promising method is training Process Reward Models (PRMs), which offer more fine-grained rewards at each reasoning step compared to Outcome Reward Models (ORMs), guiding the LLM to generate higher-quality responses than the original model output~\cite{shao2024deepseekmathpushinglimitsmathematical, sessa2024bondaligningllmsbestofn, gao2024llmcriticshelpcatch}.



However, as illustrated in Figure~\ref{fig:example}, existing PRMs typically divide a model's response into multiple reasoning steps using rule-based methods, such as chopping with a pre-defined symbol. This results in a series of coarse reasoning step divisions that lack decision-making information at steps~\cite{wang2024mathshepherdverifyreinforcellms, lightman2023letsverifystepstep}. Moreover, rule-based methods also face challenges when applied to tasks where the steps are difficult to define. Some studies have explored the application of PRMs at the level of individual tokens or fixed number of tokens~\cite{lee2024tokensupervisedvaluemodelsenhancing, luo2024improvemathematicalreasoninglanguage}, nevertheless, balancing annotation costs with the granularity of division remains a challenge. Although studies have demonstrated the advantages of PRMs over ORMs, these limitations, along with the high building costs, continue to constrain the broader adoption of PRMs~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}.

To address these issues, we aim to find an automatic step-dividing method to divide reasoning solutions into more informative steps, in contrast to the coarse division by rule-based methods. As suggested by~\citet{kahneman2011thinking}, the cognitive cost of reasoning varies depending on the difficulty of the decision or task. Additionally, a statistical analysis of common errors in reasoning tasks conducted by~\citet{roy2016solvinggeneralarithmeticword} revealed that many errors stem from incorrect numerical calculations or the misapplication of words, particularly verb misuse. This suggests that certain types of words or positions in the reasoning process require more attention.



Therefore, our goal is to divide the reasoning responses at these key positions to ensure the valuable costs during inference and training. 
We find that by pivoting on the prediction confidence, the model can automatically identify the critical breaking points in the reasoning process. 
Accordingly, we propose AdaptiveStep, a method that divides reasoning steps based on model confidence~\cite{Hills2024usinglogprobs}. We conduct experiments on the PRM scenario, with the resulting PRM named the AdaptiveStep Process Reward Model (\MethodName). This dividing method yields highly informative step divisions, enabling downstream tasks (e.g., processing the reward model) to enhance performance.

In our experiments, we assess the effectiveness of \MethodName \hspace{1pt} in mathematical reasoning and code generation tasks using the Best of N (BoN) evaluation. For the mathematical reasoning task, we 
evaluate on GSM8k~\cite{cobbe2021trainingverifierssolvemath} and MATH500~\cite{lightman2023letsverifystepstep} dataset.
For the code generation task, we collect a dataset named \textbf{LeetCodeDataset} containing 1,940 problems from LeetCode, along with the corresponding Python solutions and test cases, which include training and test splits to train and evaluate the PRM and further assess it on the Livecodebench~\cite{jain2024livecodebench}.  


Additionally, the most widely used PRM step-dividing method relies on fixed symbols, limiting the accuracy of the more fine-grained judgment ability of PRMs. We find that \MethodName \hspace{1pt} can provide precise rewards to perform Token-level Value-guided Decoding (TVD) for reasoning tasks, offering another evaluation method by integrating PRM directly into the model inference process.

\vspace{-0.3em}

In mathematical reasoning tasks, \MethodName \hspace{1pt} outperforms previous open-source methods in BoN evaluation. In addition, compared to greedy decoding, TVD further improves the final performance by 3.15\% and 14.4\%  on the GSM8k and MATH500 datasets, respectively, while incurring less than 70\% of the training data construction costs compared to the open-source baselines. 
In code generation tasks, \MethodName \hspace{1pt} shows superior performance and robustness in BoN evaluation compared to ORM. It outperforms greedy decoding by 6.54\% and 3.70\% on the two datasets in TVD evaluation.





Our main contributions are as follows: 
\begin{enumerate} 
\item We propose an automatic, efficient, general, and highly informative reasoning step-dividing method, AdaptiveStep, along with its corresponding PRM implementation. 
\item Our results show that \MethodName \hspace{1pt} is currently the state-of-the-art PRM, empirically simple and low-cost training data construction. Furthermore, the PRM built using AdaptiveStep demonstrates stronger discriminative power at the token level compared to greedy search and existing methods. Additionally, we analyze and explore several properties of \MethodName \hspace{1pt}, including transferability, domain generalization, and division features of the training data. 
\item We opensource a collection of competition-level coding problems from LeetCode, along with test cases, and provide an easy-to-use sandbox. We also release the dataset, models, and our code. \end{enumerate}