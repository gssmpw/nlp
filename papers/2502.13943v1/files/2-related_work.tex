
\section{Related Works}

\paragraph{Step-wise methods for LLMs reasoning: } Chain-of-Thought (CoT)~\cite{wei2023chainofthoughtpromptingelicitsreasoning} reasoning has become a foundational approach in LLM reasoning. Scaling the number of tokens and steps in test time to tackle complex problems has become common practice~\cite{kimiteam2025kimik15scalingreinforcement, deepseekai2025deepseekr1incentivizingreasoningcapability}. In this paradigm, the model generates an intermediate step-wise solution before providing a final answer. As expectations for model performance on more complex tasks increase, methods for step-wise verification and alignment have also advanced rapidly, like PRM~\cite{erprm, wang2024mathshepherdverifyreinforcellms, yuan2024freeprocessrewardsprocess} and step-wise RLHF~\cite{chen2024steplevelvaluepreferenceoptimization, lai2024stepdpostepwisepreferenceoptimization, wang2024cplcriticalplanstep}. Inference time step-wise methods also significantly enhance the model's reasoning capabilities, such as Monte Carlo methods~\cite{feng2023alphazero}, step-wise self-consistent~\cite{zhao2024stepwiseselfconsistentmathematicalreasoning}, step-wise beam search~\cite{lee2024tokensupervisedvaluemodelsenhancing} and flexible divide-and-conquer methods~\cite{yao2023treethoughtsdeliberateproblem, hao2023reasoninglanguagemodelplanning} for planning. 




\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{files/figures/figure-method-main-mid.pdf}
    \caption{Method overview. \textbf{a)} \textbf{\MethodName \hspace{1pt} Training Data Construction Pipeline.} Step 1: Sample from the dataset of a given domain, collecting confidence scores and samples for the training data. Then, accumulate the confidence distribution of all samples and determine the threshold.
    Step 2: Divide reasoning steps based on the threshold and label the steps using rollout. \textbf{b)} \textbf{The difference between Rule-based method and AdaptiveStep division.}
    The \textcolor{blue}{Rule-based method} divides the reasoning process using predefined symbols or fixed token counts (e.g., line breaks, as shown in the figure), while \textcolor{red}{AdaptiveStep} divides reasoning steps based on model confidence.
    We observe that the model tends to divide reasoning steps at key decision points, such as within mathematical expressions, at noun selections, and when determining the final answer. In contrast, we find that the confidence at line breaks is particularly high.
    }
    \label{fig:method_main}
\end{figure*}

\paragraph{PRM for LLM reasoning and step-dividing methods:} The importance of intermediate reasoning steps in LLMs for complex tasks was highlighted by \citet{uesato2022solvingmathwordproblems}, which led to the development of Process Reward Models (PRMs) to enhance LLM reasoning by providing feedback at each step. \citet{lightman2023letsverifystepstep} showed that step-by-step feedback improves reasoning reliability and reduces logical errors. Similarly, the OmegaPRM~\cite{luo2024improvemathematicalreasoninglanguage}, utilizing Monte Carlo Tree Search (MCTS), improves mathematical reasoning performance by efficiently gathering process supervision data. \citet{wang2024mathshepherdverifyreinforcellms} proposed a heuristic annotation method, reducing PRM annotation costs. Step-level reward models~\cite{ma2023let} have demonstrated that feedback at each step helps guide LLMs to more optimal solutions. Automated process verifiers~\cite{setlur2024rewardingprogressscalingautomated} further enable large-scale deployment of PRMs, improving LLM alignment. Several works have explored PRM applications in reasoning tasks~\cite{xia2024evaluatingmathematicalreasoningaccuracy, ma2023letsrewardstepstep, luo2023critiqueabilitylargelanguage, snell2024scalingllmtesttimecompute}. However, the predominant step-dividing method used in PRMs or other step-wise methods remains rule-based, such as using pre-defined symbols, which results in sentence-level PRMs. Some works have developed token-level PRMs by dividing at fixed token intervals, but the high annotation cost remains a limitation~\cite{lee2024tokensupervisedvaluemodelsenhancing, luo2024improvemathematicalreasoninglanguage}.


\paragraph{Guided decoding:} Standard decoding in Large Language Models (LLMs) typically involves sampling strategies to select the next token. Guided decoding has been widely explored to improve performance and constrain text generation. \citet{chaffin2022pplmctsconstrainedtextualgeneration} proposed incorporating a value model into the LLM decoding process, using MCTS~\cite{coulom2006efficient} to constrain output without fine-tuning. \citet{liu2024dontthrowawayvalue} integrated the Proximal Policy Optimization (PPO)-based value network with MCTS, enabling collaboration with the policy network during inference. In the code generation domain, Planning-Guided Transformer Decoding (PG-TD)\cite{zhang2023planning} uses planning algorithms for lookahead search to guide the transformer in producing more optimal code. \citet{nie2024decoding} employed a proxy code LLM to build an offline token-scoring model that reallocates token probabilities to guide decoding. Additionally, several works have applied value functions to guide token-level decoding \cite{dathathri2019plug,choi2023kcts,xu2024safedecoding,krause2020gedi}. In this paper, we use PRM as a value function to directly guide the decoding process of large language models, aiming to validate the effectiveness of PRM and explore additional potential applications of PRM.
