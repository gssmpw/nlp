
\documentclass{article}

\usepackage[accepted]{icml2025_arxiv}

\usepackage{microtype}
\usepackage{booktabs} % for professional tables

\usepackage{pifont}    % For \ding symbols
\usepackage{xcolor}

\usepackage{times}
\usepackage{latexsym}

\usepackage{epsfig}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{ulem}
\usepackage{amsfonts}

\usepackage[utf8]{inputenc}


\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}



\usepackage{tabularx}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newcommand{\MethodName}{ASPRM}
\newcommand{\chuheng}[1]{{\color{cyan} #1}}
\newcommand{\yuliang}[1]{{\color{red} #1}}
\newcommand{\junjie}[1]{{\color{yellow} #1}}
\newcommand{\weishen}[1]{{\color{blue} #1}}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence}

\begin{document}

\twocolumn[

\icmltitle{AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuliang Liu}{equal,nju}
\icmlauthor{Junjie Lu}{equal,uts}
\icmlauthor{Zhaoling Chen}{nju}
\icmlauthor{Chaofeng Qu}{}
\icmlauthor{Jason Klein Liu}{}
\icmlauthor{Chonghan Liu}{}
\icmlauthor{Zefan Cai}{uwm}
%\icmlauthor{}{sch}
\icmlauthor{Yunhui Xia}{}
\icmlauthor{Li Zhao}{msra}
\icmlauthor{Jiang Bian}{msra}
\icmlauthor{Chuheng Zhang}{msra}
\icmlauthor{Wei Shen}{}
\icmlauthor{Zhouhan Lin}{sjtu}
\end{icmlauthorlist}

\icmlaffiliation{nju}{Nanjing University}
\icmlaffiliation{uts}{University of Technology Sydney}
% \icmlaffiliation{sii}{Shanghai Innovation Institute}
\icmlaffiliation{uwm}{UW-Madison}
\icmlaffiliation{sjtu}{Shanghai Jiaotong University}
\icmlaffiliation{msra}{MSRA}


% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Yuliang Liu}{liuyl03181@gmail.com}
\icmlcorrespondingauthor{Junjie Lu}{lux17999@gmail.com}
\icmlcorrespondingauthor{Chuheng Zhang}{zhangchuheng123@live.com}
\icmlcorrespondingauthor{Wei Shen}{shenwei0917@126.com}
\icmlcorrespondingauthor{Zhouhan Lin}{hantek@sjtu.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Process reward model, LLM reasoning}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\begin{abstract}
Current approaches for training Process Reward Models (PRMs) often involve breaking down responses into multiple reasoning steps using rule-based techniques, such as using predefined placeholder tokens or setting the reasoning step's length into a fixed size.
These approaches overlook the fact that specific words do not typically mark true decision points in a text. To address this, we propose AdaptiveStep, a method that divides reasoning steps based on the model's confidence in predicting the next word. This division method provides more decision-making information at each step, enhancing downstream tasks, such as reward model learning. Moreover, our method does not require manual annotation. We demonstrate its effectiveness through experiments with AdaptiveStep-trained PRMs in mathematical reasoning and code generation tasks. Experimental results indicate that the outcome PRM achieves state-of-the-art Best-of-N performance, surpassing greedy search strategy with token-level value-guided decoding, while also reducing construction costs by over 30\% compared to existing open-source PRMs. In addition, we provide a thorough analysis and case study on the PRM's performance, transferability, and generalization capabilities. We provide our code on \href{https://github.com/Lux0926/ASPRM/tree/main}{GitHub}.


\end{abstract}

\iffalse


\fi

\input{files/1-intro}



\input{files/2-related_work.tex}



\input{files/3-method.tex}



\input{files/4-exp.tex}






\bibliography{reference}
\bibliographystyle{icml2025}



\newpage
\appendix
\onecolumn

\input{files/5-appendix.tex}

\end{document}

