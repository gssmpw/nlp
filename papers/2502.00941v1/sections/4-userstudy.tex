We conducted a user study to investigate how variants of the PRIMO approach affect the efficiency, comfort, location awareness, and overall object understanding of tasks where users must navigate and inspect small regions of a dense, homogeneous object.

\begin{figure}[tb]
 \centering
 \includegraphics[width=\columnwidth]{figures/conditions.png}
 \caption{Conditions in this study: (a) \textsc{Selection-Structured} (subdivided showing only focus area), (b) \textsc{Selection-Unstructured} (freeform showing only focus area), (c) \textsc{Everything-Structured} (subdivided showing entire object), and (d) \textsc{Everything-Unstructured} (freeform showing entire object).}
 \label{fig:conditions}
 \vspace{-4mm}
\end{figure}

\subsection{Experimental Design}
Our study included four variations of PRIMO, by varying two independent variables with two levels each, which can be seen in \autoref{fig:conditions}. \textit{Navigation style} could be either \textsc{structured} or \textsc{unstructured}. The main rationale for studying this variable was to understand the effects of having a more systematic way of navigating versus a less constrained interaction. For \textit{display mode}, PRIMO could either display only the \textsc{selection} or \textsc{everything}. The motivation for this variable comes from the trade-off between being able to focus on only the current subvolume versus having more clutter on the screen but potentially retaining more spatial awareness. This led to four conditions: \textsc{Selection-Structured}, \textsc{Selection-Unstructured}, \textsc{Everything-Structured}, and \textsc{Everything-Unstructured}.

%The first condition was called \textbf{\textsc{Selection-Structured}}. The selection display mode implied that only the subpart inside the focus region would be displayed for the user as they navigated to the lower level of scale, hiding the parts of the object outside of the area. The structured navigation style implied that the object was pre-divided into eight subparts, and the user could navigate to each of those by selecting them with a raycast.

%Our second condition was called \textbf{\textsc{Selection-Unstructured}}. While we still keep the selection display mode, we used the unstructured navigation style. Unstructured implies that the object was not pre-divided. Instead, the user would navigate by placing a semi-transparent yellow box over the area they wanted to inspect at the next lower level of scale. Unlike the previous technique, that implied freedom to define the boundaries of the new focus.

%The third condition was called \textbf{\textsc{Everything-Structured}}. While using the pre-divided navigation, we instead used the everything display mode. Everything means that the entire object was always being displayed by the system (with the exception of what was hidden by the clipping plane). That meant that as users moved into lower levels of scale, they could still view the regions neighboring their current focus from that new viewpoint.

%Finally, our fourth condition was \textbf{\textsc{Everything-Unstructured}}. This one combined the everything display mode and the unstructured navigation style, allowing users to freely decide the boundaries to scale down to the next level of scale while maintaining a view of the entire object.

%\subsection{Experimental Design}
Both independent variables were varied within subjects, allowing all participants to complete all conditions. The presentation order was counterbalanced for each group in the following manner: trials with the same \textit{display mode} were blocked together, meaning that half of the participants completed \textsc{selection} first, and the other half completed \textsc{everything} first. Then, within the blocks, half of the participants completed \textsc{structured} navigation first, and the other half completed \textsc{unstructured} navigation first. We decided on this design to allow us to measure the effect of display mode on simulator sickness after the first two conditions.

Our objective dependent variables included the time to position the clipping plane (in milliseconds), the time to navigate from the top level of the object to a target region that required inspection (in milliseconds), and the total navigation time. We also gathered data through questionnaires that were presented inside VR. In \textbf{Question 1}, we measured the user's location awareness during navigation, allowing us to quantify how much they maintained a mental model of their focus related to the model. We designed a SAGAT-style question \cite{endsley_situation_1988} that would stop the participant in the middle of a trial and black out the environment. The question then asked: \textit{``Which of the following images better represent the location of the subvolume to which you have navigated and is focused right now (intersection of the rods)?''}, with four multiple choice answers, as seen in \autoref{fig:questions} (top). Navigation time and location awareness were measured at different times, as they might influence each other. Thus, half of the trials measured time, and half measured awareness.

\begin{figure}[tb]
 \centering
 \includegraphics[width=\columnwidth]{figures/questions.png}
 \caption{Questions asked inside VR: Question 1 was asked during the task and measured location awareness; question 2 was measured at the end of an object (four trials) and measured object understanding.}
 \label{fig:questions}
 \vspace{-4mm}
\end{figure}

After all the trials for a particular object, we were interested in overall object understanding---the ability to remember and relate the locations of the various defects within that object. \textbf{Question 2} asked: \textit{``Which of the following group of images better represent the location of the four defects that you encountered in this object/trial (intersection of the rods of each image)?''}. Again, answers were shown in multiple-choice, with each choice showing four possible defect locations (\autoref{fig:questions} (bottom)).

We also applied a simulator sickness questionnaire (SSQ). We conducted a baseline measure at the beginning of the study and a second one after the first block of two conditions was completed, to measure the differences due to the conditions. Finally, we finished the study with a semi-structured interview. We asked them about their preferred condition, which one made it easier to answer the location awareness and object understanding questions, and which one was more comfortable. We further asked them to clarify how they were feeling regarding simulator sickness.

\subsection{Hypotheses}
Our hypotheses regarding display mode and navigation style were as follows:

\textbf{H1. \textsc{Structured} will be faster than \textsc{unstructured}.}
Our argument was that \textsc{structured} had already pre-divided the object, and thus, navigation was comprised of simple point and click. Meanwhile, in \textsc{unstructured}, the user would have to position the box over the volume of interest carefully.

\textbf{H2. \textsc{Selection} will be faster than \textsc{everything}.}
We believed that \textsc{selection} would be faster than \textsc{everything} because of occlusion issues. When viewing only part of the object, the user should be able to easily see and reach the sides of the region, which are occluded if the entire object is being displayed.

\textbf{H3. \textsc{Selection} will lead to less simulator sickness than \textsc{everything}. }
As the user navigates to lower levels of scale, the focused area will appear to have the same dimensions as the original object. Thus, \textsc{selection} only displays a constant amount of the object, while \textsc{everything} occupies the user's periphery (as the rest of the object occupies previously empty space), and the scaling animation fills the field of view, which could translate into simulator sickness \cite{fernandes_combating_2016}.

\textbf{H4. \textsc{Structured} will lead to better location awareness than \textsc{unstructured}.}
While \textsc{structured} is discretized due to the object being pre-divided,  \textsc{unstructured} is a continuous selection. This implies that in a structured navigation the user may more easily remember their choices through the navigation, compared to free selections in space, which could help users maintain awareness of their location.

\textbf{H5. \textsc{Everything} will lead to better location awareness than \textsc{selection}.}
As the user moves into lower levels of scale, it may be easier for them to forget where they are or where they come from. By having \textsc{everything} display the entire object, users will have an opportunity to view their surroundings and try to understand better where exactly they are located.

\textbf{H6. \textsc{Structured} will result in a better overall object understanding than \textsc{unstructured}.}
Similarly to H4, we believed that it would be easier to retain the information about where the regions with possible defects were located by remembering the logical, discrete path to reach them.

\textbf{H7. \textsc{Everything} will result in a better overall object understanding than \textsc{selection}.}
Similarly to H5, we believe that seeing the entire object would help participants remember locations and, more importantly, the relationship between the regions with possible defects. This would happen because while navigating to one region, they would also be more likely to see the defects in other parts of the object, that are somewhat close but not exactly inside their focus area.

\subsection{Experimental Task}
Participants had to navigate from the highest scale level of the object to the flagged region of interest at the lowest scale level. This task included four steps: (1) the participant would need to understand where the target location was---we marked this location with yellow rods parallel to each principal axis that intersected at the target, shown in \autoref{fig:steps}-a; (2) the participant would move a clipping plane to see inside of the object---the clipping plane was constrained to movements along the vertical axis and could not be rotated, and they had to move the clipping plane until it was cutting through the defect region (represented by a sphere) before they were allowed to proceed (\autoref{fig:steps}-b); (3) the participant would navigate to the target at the lowest level of scale (they were allowed to move back to a higher level of scale if they selected the wrong one) (\autoref{fig:steps}-c); (4) the participant would verbally report the number they saw in the target region (\autoref{fig:steps}-d). We decided to have a fake defect with a number instead of a real defect, because our participants were not experts in the domain of additive manufacturing, and the important thing was to demonstrate that the target region had been reached.

Our rationale for the task is that while researchers can use machine learning algorithms to obtain regions where defects may exist, those are still insufficient to confirm the defects by themselves---given the difficulty and subjective evaluation of what is a defect. Further, there is an interest in understanding how such defects propagate across the object, as defects rarely occur at a single point and for no reason. Thus, our study simulates the approach of manually inspecting those pre-marked regions of interest, rather than searching from defects that could be located anywhere.

For each of the four conditions, participants inspected four objects, each containing four regions of interest to which they had to navigate. We further conducted one object inspection (also with four trials) in each condition during a training session before the main task. This led to a total of 80 trials being completed per participant: 16 training trials and 64 experiment trials. As mentioned before, half of the trials measured time and the other half measured location awareness. For each condition, the flow would be as follows: (1) training with 1 object, 2 defects (timed), and 2 defects (awareness); (2) main task with 2 objects (timed), and 2 objects (awareness). At the end of each object, we measured the overall object understanding with Question 2.

\subsection{Apparatus and Environment}
In all conditions, participants used a Meta Quest 2 head-worn display (HWD). It features a resolution of 1832 x 1920 per eye, with a refresh rate of 90Hz \footnote{https://www.meta.com/quest/products/quest-2/tech-specs}. Participants used a handheld Meta Quest 2 controller in their hand of choice. All interactions required users to point a raycast coming out of their controller and press the trigger button for selections; the button at the top of the controller allowed them to move back to a higher level of scale.

The experiment was run on a PC with an Intel i9-12900K CPU, 32GB of 3200MHz DDR4 DRAM, a Samsung SSD 980 PRO, and a GeForce RTX 3070 Ti 16GB GPU. The connection with the Meta Quest 2 was managed through the Quest link cable with up to 5Gbps transfer rate.

Participants stood in an open, unobstructed area. They could freely move around the space, only constrained by the link cable connected to the computer running the experiment. A white arrow on the floor of both the physical and virtual environments allowed users to start each trial from the same position and orientation.

\begin{figure}[tb]
 \centering
 \includegraphics[width=\columnwidth]{figures/steps.png}
 \caption{Steps in this study: (a) Object being inspected, with white outline showing the focus area, and yellow rods showing the location of region to inspect; (b) clipping plane cutting over the sphere at the target location; (c) navigating down into the object (multiple steps); and (d) reaching lowest level and seeing a number representing a defect.}
 \label{fig:steps}
 \vspace{-4mm}
\end{figure}

\subsection{Procedure}
The [blinded] Institutional Review Board approved the study, which took place face-to-face at our laboratory in a single 90-minute session. We recruited participants through mailing lists and an online university recruitment system and asked them to complete a screening questionnaire for our inclusion criteria discussed in the study design section. They scheduled a session time and received a digital copy of the consent form.

Upon arrival, we greeted the participant at our laboratory. They signed the consent form and answered a background questionnaire and baseline simulator sickness questionnaire on a tablet \cite{kennedy_simulator_1993}. Next, they received general instructions through a PowerPoint presentation, and we measured and adjusted the lens position for their IPD. We followed Meta guidelines, using the 3 available IPD adjustments ($\leq61$, $61-66$, $\geq66$) \footnote{https://www.meta.com/help/quest/articles/getting-started/getting-started-with-quest-2/ipd-quest-2/}.

We asked them to stand over the white arrow and helped them wear the HWD. Once they were comfortable, we moved to the first condition, starting with the training session. Once they reached the end of the second condition (middle of the experiment), we asked them to remove the HWD, answer the simulator sickness questionnaire again, and take a mandatory five-minute break. After the break was concluded, they returned to the VR environment and completed the other two conditions. Between each condition, they were also offered an extra opportunity to take a break. Once all conditions were completed, the participant answered questions in a semi-structured interview.

\subsection{Participants}
We recruited 24 participants from the general population who fit the following inclusion criteria: (1) were at least 18 years old, (2) were proficient with the English language, (3) had normal vision (corrected or uncorrected; glasses were excluded due to equipment restrictions), and (4) had normal mobility of arms, hands, and legs (to manipulate a VR controller while walking in a space).

Twenty-four participants (aged 18 to 28; 12 male, 11 female, 1 non-binary) from the campus population took part in the experiment in individual sessions of around 90 minutes. All participants were undergraduate students from a range of disciplines. Twenty participants were right-handed, three were left-handed, and 1 was ambidextrous. Only two participants (out of three who declared themselves to be left-handed) chose to conduct the study using the left controller (the remaining left-handed participant opted to use the right controller). Twenty-two of the participants rated their fatigue level between 1 and 3 (out of 5). Twenty-one participants ranked their VR experience between 1 and 3 (out of 5), with twenty participants having used VR at least once.

\subsection{Results}
We collected our results from multiple sources. We had Google Forms that recorded the questionnaires, namely the background and simulator sickness questionnaires. From Unity, we obtained the time to complete each of the basic tasks and all the answers to the multiple-choice questions. Finally, we recorded audio files with participants' responses during the semi-structured interviews. These were transcribed by Microsoft Office Word 365, with manual verification and adjustments conducted by the authors. We conducted a statistical analysis using the \textit{JMP Pro 16} software. We used an $\alpha$ level of 0.05 in all significance tests. In the results figures, significantly different pairs are marked with * when $p\leq.05$, ** when $p\leq.01$, and *** when $p\leq.001$.

We verified normality through Shapiro-Wilk tests and normal quantile plot inspections for all cases before deciding to apply a two-way analysis of variance (ANOVA) or a non-parametric test (Wilcoxon). We performed pairwise comparisons using Tukey HSD (honestly significant difference) when appropriate. Our two factors were the display mode (\textsc{Selection} vs \textsc{Everything}) and navigation style (\textsc{Structured} vs \textsc{Unstructured}). We report partial eta squared effect sizes ($\eta_p^2$) for main effects and Cohen's $d$ for pairwise comparisons.

\subsubsection{Task Completion Time}

\begin{figure*}[tb]
 \centering
 \includegraphics[width=\linewidth]{figures/0-time.png}
 \caption{Time to complete: (1) clipping subtask, before starting the navigation; (2) navigation subtask, where our variables actually changed; and (3) the total task considering both. Error bars represent 95\% confidence intervals. }
 \label{fig:results}
 \vspace{-4mm}
\end{figure*}

\begin{table}[tb]
\caption{Statistics of speed to inspect measurements during the task. There were no interaction effects.}
\label{tab:speed_results}
\def\arraystretch{1.3}
\resizebox{\linewidth}{!}{%
\begin{tabular}{m{5.3em}ll}
\toprule
Measure         & Navigation Style                   & Display Mode\\
\midrule
Clipping   & $F_{1,1} = 3.99, p = 0.046; \eta_p^2 = 0.005$ & -\\
Navigation & $F_{1,1} = 8.04, p = 0.005; \eta_p^2 = 0.010$ & $F_{1,1} = 10.56, p=0.001; \eta_p^2 = 0.013$\\
Total      & $F_{1,1} = 9.78, p = 0.002; \eta_p^2 = 0.013$ & $F_{1,1} = 11.32, p \textless 0.001; \eta_p^2 = 0.015$\\
\bottomrule
\end{tabular}
}
\end{table}
We measured the time to complete the task during speed trials. All the statistics in this subsection can be seen in \autoref{tab:speed_results} and represented in \autoref{fig:results}.

Regarding the \textbf{clipping time}, \textsc{Structured} ($\mu=2979.15, \sigma=1551.52$) led to a significantly larger time than \textsc{Unstructured} ($\mu=2782.51, \sigma=1151.84$), with a small effect. We did not find a significant effect of display mode or an interaction effect. Regarding the \textbf{navigation time}, \textsc{Everything} ($\mu=5195.29, \sigma=2940.08$) led to a significantly larger time than \textsc{Selection} ($\mu=4,611.26, \sigma=1969.96$), with a small effect. \textsc{Structured} ($\mu=5158.08, \sigma=2699.02$) also led to a significantly larger time than \textsc{Unstructured} ($\mu=4648.46, \sigma=2298.01$), with a small effect. We did not find an interaction effect.

Regarding the \textbf{total time}, \textsc{Everything} ($\mu=8163.99, \sigma=3646.11$) led to a significantly larger time than \textsc{Selection} ($\mu=7404.19, \sigma=2556.98$), with a small effect. \textsc{Structured} ($\mu=8137.21, \sigma=3395.41$) also led to a significantly larger time than \textsc{Unstructured} ($\mu=7430.97, \sigma=2888.39$), with a small effect. We did not find an interaction effect.

\subsubsection{Location Awareness}
We did not find any significant results for \textbf{location awareness} (Question 1), for either display mode ($F_{1,1}=0.73, p=0.39$) or navigation style ($F_{1,1}=0.05, p=0.82$), or their interaction ($F_{1,1}=0.59, p=0.59$). Averages were as follows: \textsc{Structured} ($\mu=0.68$), \textsc{Unstructured} ($\mu=0.69$), \textsc{Selection} ($\mu=0.67$), \textsc{Everything} ($\mu=0.70$).

\subsubsection{Overall Object Understanding}
Regarding the \textbf{overall object understanding} (Question 2), \textsc{Unstructured} ($\mu=0.67, \sigma=0.47$) led to a significantly larger percentage of correct answers than \textsc{Structured} ($\mu=0.56, \sigma=0.50$), with $F_{1,1}=4.88, p=0.028; \eta_p^2=0.013$ (small effect). We did not find a significant effect of display mode or an interaction effect.

\subsubsection{Simulator Sickness Questionnaire}
We subtracted the answers of the pre-exposure SSQ from the post-exposure SSQ. Pre-exposure was measured at the start of the study, and post-exposure was measured at the mandatory break in the middle of the study (after the trials with the first display mode). We then calculated the scores for nausea, oculomotor, disorientation, and total score, and performed a between-subjects Wilcoxon Signed-Rank Test. We did not find any significant effects of display mode on \textbf{simulator sickness}, for nausea ($Z=-0.36, p=0.72$), oculomotor ($Z=-0.60, p=0.55$), disorientation ($Z=0, p=1.00$), or total score ($Z=-0.47, p=0.64$). Although there was a trend for \textsc{everything} ($\mu=13.40, \sigma=19.75$) to have a higher total score than \textsc{selection} ($\mu=4.99, \sigma=9.21$), variation was too large to make this difference significant. Two participants from the \textsc{everything} condition reported feeling symptoms of motion sickness both in the questionnaire and verbally and later asked for the third optional break (between conditions 3 and 4).

\subsubsection{Qualitative Analysis}
The coding process was performed by a single experimenter using a bottom-up approach. For each transcription, the task was labeled based on the main topic being described by the participant (using Taguette \cite{rampin_taguette_2021}). New labels would be created as needed, or existing ones would be re-utilized. Once all labeling was completed, two experimenters read through them and organized the findings into themes. Those themes were then analyzed and turned into findings presented in this section. 

\paragraph{\textbf{\textsc{Everything} provides more context}}
Twenty-two participants mentioned that \textsc{everything} provided them with more context, and twenty-one said that they perceived it easier to answer Question 1 about location awareness while using it. P5 said, ``When I saw \textsc{everything} around me, it gave me a little bit more context ... I could still look over and see the colors that were nearby.'' P11 added, ``Seeing the stuff around me kept me aware of where I was within the object still. When it disappeared, I forgot exactly where I was within the object.''

Three participants also commented on how \textsc{everything} allowed them to see other defects in their peripheral view. P0 said, ``When you saw the full image, you could still kind of see where the other defects were. You can still see there is another one in the same area, as opposed to completely chopping and only having that little section.'' P17 added, ``I found pretty helpful if there were defects in the same plane ... at one point I saw there were three defects in the same plane.''

\paragraph{\textbf{\textsc{Everything} feels cluttered}}
For seven participants, \textsc{everything} ended up cluttering the environment and making them feel uncomfortable with being surrounded by the object. P15 said, ``It felt weird being inside of it.'' P8 mentioned, ``I wouldn't say it made me more disoriented, but I think it was a little more confusing when I could see everything,'' which, in turn, made it harder to select certain subvolumes: ``it was almost harder for me to pick a spot because it's a little harder to see what those subdivided regions are.'' P9 mentioned they preferred \textsc{selection} because ``then I wasn't bombarded with everything ... from a visual flood point of view was easier to ... be just focused on the one object.'' And they further discussed the issue of scaling the environment vs a single object, ``it felt like I was being pushed into the box, instead of the box disappearing ... it looks like the ground to you at the moment.'' P5 actually thought this was a good thing, ``I just enjoyed \textsc{everything} a little bit more ... looking at the structure in a very close, right in front of your face, point of view.''

\paragraph{\textbf{\textsc{Selection} helps focus on details}}
Twelve participants discussed the effects of focusing on the actual defect while using the \textsc{selection} display mode. P2 said, ``You could see more in-depth ... there wasn't anything around it, so you could see more clearly the actual sphere.'' P4 mentioned how it helped navigation, ``It was easier to focus on where I was trying to get to. It was less distracting ... having everything go away once I scaled down was powerful.'' P15, ``I didn't have the rest of the unnecessary parts all in my way.'' P20 concluded, ``In terms of finding the defect, it was a lot easier to pinpoint where exactly it was within that self-object.'' P0 further mentioned that this focus helped them identify they moved to the wrong subvolume, ``sometimes I would select it and it would look like it was in the right part, but it was just that much off.''

\paragraph{\textbf{\textsc{Structured} provides cues for location estimation}}
Eleven participants commented on how \textsc{structured} navigation provided them with cues that helped estimate their location within the object, while fourteen participants said that they perceived it easier to answer Question 1 about location awareness while using it. P4 said, ``I was specifically picking a certain section of the square, which I think made that easier.'' P3 mentioned, ``It's just very clear what color you're on, and then I'll just put that color in the back of my head and try to remember it for the end.'' P5 added that ``\textsc{structured} will kind of guide you to where you are ... I went from this section to within that section of this section.'' P11 said, ``It stuck in my memory longer when it was \textsc{structured}.'' P15 provided in-depth details, ``I remembered the color, and then if it was at the top or bottom ... then I just did that for each one.'' P23 concluded, ``if it's in subdivisions, it's easier to know where you are than if it's not.''

\paragraph{\textbf{\textsc{Unstructured} was easier and more robust}}
Ten participants discussed how \textsc{unstructured} navigation made it easier for them to select the subvolume they wanted. P10 said, ``I felt like I had more control over where I was going. It just felt easier for me to navigate.'' P3 explained, ``I felt I could just immediately scale down right into the point I was looking at.'' P4 added, ``when I scaled down, the region would be in the center.'' P7 made a point about occlusion making it harder for \textsc{structured}, ``With \textsc{structured} you had to point at a specific angle, either point higher or lower ... but for the \textsc{unstructured} I could just point and then click wherever I'm comfortable, which made it a lot easier.''

Four participants mentioned that \textsc{unstructured} was less prone to wrong volume selections than \textsc{structured}. P5 said, ``there were times when I would be pointing using \textsc{structured} and it would just blip over to the section nearby. Whereas when you're \textsc{unstructured} ... it still can contain the defect.'' P10 added, ``with the \textsc{structured} ones, it was more you either hit it or you don't. With the other ones it was more accurate, felt I had more control over the system.'' P14, ``With the one where I could move the box on my own, I got to pick in-between colors.''

\paragraph{\textbf{\textsc{Structured} requires less ray-casting precision}}
Eight participants also described that \textsc{structured} required less precision from them during the process of pointing their raycast. P2 said, ``There wasn't as much guessing of where to put the pointer. It was more guided.'' P4 added, ``I just pointed to the specific square I needed to go.'' P9 expanded, ``I don't have to worry about really getting it ... as long as it's in that area, I'm fine.'' P13 claimed, ``It's hard to position the yellow cube [in \textsc{unstructured}] exactly where you want it.'' P17 said, ``It was a little bit difficult to pinpoint the defect [in \textsc{unstructured}]. Versus if you have \textsc{structured} you can sort of just highlight a certain area.''

\paragraph{\textbf{Simulator sickness was minimal}}
Twenty participants reported no simulator sickness at all, while two reported mild symptoms of simulator sickness. P6 said, ``I feel as the experiment continued the dizziness started to kick in a little bit more, but maybe it's because of fatigue.'' P15 said, ``\textsc{everything unstructured} was probably the most dizzying because it was a lot of movement and there was a lot around me ... moving myself and I'm inside of something, it was just a lot of stimulus going on all at once.''