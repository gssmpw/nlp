%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\input{math_commands.tex}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted (or arxiv version), instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage[capitalise]{cleveref}
\crefname{equation}{Eq.}{Eqs.}
\crefname{lemma}{Lem.}{Lems.}
\crefname{section}{Sec.}{Secs.}
\crefname{appendix}{App.}{Apps.}
\crefname{table}{Tab.}{Tabs.}
\crefname{theorem}{Thm.}{Thms.}
\crefname{proposition}{Prop.}{Props.}
\crefname{assumption}{Assump.}{Assumps.}
\crefname{corollary}{Cor.}{Cors.}
\crefname{remark}{Rmk.}{Rmks.}
\crefname{definition}{Def.}{Defs.}
\crefname{algorithm}{Alg.}{Algs.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\haoxuan}[1]{\textcolor{orange}{(haoxuan: #1)}}
\newcommand{\yinuo}[1]{\textcolor{cyan}{(yinuo: #1)}}
\newcommand{\wei}[1]{\textcolor{magenta}{[Wei: #1]}}
\newcommand{\gmr}[1]{\textcolor{red}{(gmr: #1)}}
\newcommand{\ly}[1]{\textcolor{blue}{(lexing: #1)}}
\newcommand{\yc}[1]{\textcolor{purple}{(yuchen: #1)}}
\newcommand{\tao}[1]{\textcolor{blue}{(molei: #1)}}
\newcommand{\chen}[1]{\textcolor{blue}{(yongxin: #1)}}

\newcommand{\RK}{{\mathrm{RK}}}
\newcommand{\trap}{{\mathrm{trap}}}
\newcommand{\roI}{{\mathrm{I}}}
\newcommand{\roII}{{\mathrm{II}}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Fast Solvers for Discrete Diffusion Models: Theory and Applications of High-Order Algorithms}

\begin{document}

\twocolumn[
	\icmltitle{Fast Solvers for Discrete Diffusion Models:\texorpdfstring{\\}{} Theory and Applications of High-Order Algorithms}

	% It is OKAY to include author information, even for blind
	% submissions: the style file will automatically remove it for you
	% unless you've provided the [accepted] option to the icml2024
	% package.

	% List of affiliations: The first argument should be a (short)
	% identifier you will use later to specify author affiliations
	% Academic affiliations should list Department, University, City, Region, Country
	% Industry affiliations should list Company, City, Region, Country

	% You can specify symbols, otherwise they are numbered in order.
	% Ideally, you should not use this facility. Affiliations will be numbered
	% in order of appearance and this is the preferred way.
	\icmlsetsymbol{equal}{*}

	\begin{icmlauthorlist}
		\icmlauthor{Yinuo Ren}{icmestanford,equal}
		\icmlauthor{Haoxuan Chen}{icmestanford,equal}
		\icmlauthor{Yuchen Zhu}{mlgatech,mathgatech,equal}
		\icmlauthor{Wei Guo}{mlgatech,aegatech,equal}\\
		\icmlauthor{Yongxin Chen}{mlgatech,aegatech}
		\icmlauthor{Grant M. Rotskoff}{icmestanford,chemstanford}
		\icmlauthor{Molei Tao}{mlgatech,mathgatech}
		\icmlauthor{Lexing Ying}{icmestanford,mathstanford}
	\end{icmlauthorlist}

	\icmlaffiliation{icmestanford}{Institute for Computational and Mathematical Engineering (ICME), Stanford University, Stanford, CA}
	\icmlaffiliation{mathstanford}{Department of Mathematics, Stanford University, Stanford, CA}
	\icmlaffiliation{chemstanford}{Department of Chemistry, Stanford University, Stanford, CA}
	\icmlaffiliation{mlgatech}{Machine Learning Center, Georgia Institute of Technology, Atlanta, GA}
	\icmlaffiliation{aegatech}{School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA}
	\icmlaffiliation{mathgatech}{School of Mathematics, Georgia Institute of Technology, Atlanta, GA}

	\icmlcorrespondingauthor{Haoxuan Chen}{haoxuanc@stanford.edu}

	% You may provide any keywords that you
	% find helpful for describing your paper; these are used to populate
	% the "keywords" metadata in the PDF but will not be shown in the document
	\icmlkeywords{Discrete diffusion model, Continuous-time Markov chain, High-order numerical scheme}

	\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution}
% otherwise use the standard text.

\begin{abstract}
	Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis.
	However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms.
	Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\tau$-leaping is limited by its first-order accuracy.
	In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error.
	We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\theta$-trapezoidal method in KL divergence.
	Empirical evaluations on GPT-2 level text and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints.
\end{abstract}

\section{Introduction}

Diffusion and flow-based models on discrete spaces \citep{chen2022analog, austin2021structured, dieleman2022continuous, floto2023diffusion, hoogeboom2021autoregressive, hoogeboom2021argmax, meng2022concrete, richemond2022categorical, sun2022score, santos2023blackout} have emerged as a cornerstone of modern generative modeling for categorical data, offering unique advantages in domains where continuity assumptions fail. Unlike their continuous counterparts, discrete diffusion models inherently accommodate data with discrete structures, \emph{e.g.}, language tokens, molecular sequences, tokenized images, and graphs, enabling principled generation and inference in combinatorially complex spaces. These models have exerted a large impact on numerous applications, from the design of molecules~\citep{kerby2024training}, proteins~\citep{frey2023protein}, and DNA sequences~\citep{avdeyev2023dirichlet, guo2024plug} under biophysical constraints, to the generation of high-fidelity text~\citep{dat2024discrete} and images~\citep{hu2022global} via autoregressive or masked transitions, \emph{etc.}. Beyond standalone tasks, discrete diffusion models also synergize with methodologies, ranging from tensor networks~\citep{causer2024discrete} to guidance mechanisms~\cite{nisonoff2024unlocking,li2024derivative,schiff2024simple}.

Discrete diffusion models, despite their broad applicability, face a critical bottleneck: \emph{inference inefficiency}.
Current inference methods include: (1) exact simulation methods~\citep{zheng2024masked}, which ensure unbiased sampling from the pre-trained model but suffer from unpredictable inference time and redundant score evaluations, resulting in poor scaling w.r.t. dimensionality; and (2) approximate methods such as $\tau$-leaping~\citep{campbell2022continuous}, which offer simple and parallelizable implementation but, due to their first-order accuracy, requires small step sizes to control discretization error, forcing a stringent trade-off between speed and sample quality.

To address these limitations in possibly computationally constrained environments, we aim to develop high-order numerical schemes tailored for discrete diffusion model inference. Drawing inspirations from acceleration techniques developed for ordinary differential equations (ODEs)~\citep{butcher1987numerical}, stochastic differential equations (SDEs)~\citep{burrage1996high, anderson2009weak}, chemical reaction simulations~\citep{hu2011weaka}, and most recently continuous diffusion models~\citep{tachibana2021quasi, lu2022dpm, lu2022dpm++}, our work represents the \emph{first successful adaptation of high-order numerical schemes to the discrete diffusion domain}. Through careful design, these high-order schemes provide an unprecedented efficient and versatile solution for discrete diffusion model inference.

\textbf{Contributions.} The main contributions of this paper are summarized as follows:
\begin{enumerate}[label={\bfseries \arabic*.}, itemsep=0pt, topsep=0pt, wide=0pt]
	\item We introduce the \emph{first high-order numerical solvers} for discrete diffusion model inference, namely the $\theta$-Runge-Kutta-2 ($\theta$-RK-2) method and the $\theta$-trapezoidal method;
	\item We rigorously establish the theoretical properties of both methods, proving \emph{second-order convergence} of $\theta$-trapezoidal method and conditional second-order convergence of $\theta$-RK-2 method;
	\item We empirically validate our theoretical results and demonstrate the \emph{superior performance} of the $\theta$-trapezoidal method through comprehensive evaluations on large-scale text and image generation benchmarks.
\end{enumerate}

\vspace{-0.5em}
\subsection{Related Works} We briefly review related works here and defer a more detailed discussion to~\cref{app:related_works}.

\vspace{-0.7em}
\paragraph{Discrete Diffusion Models.}
Since its introduction, discrete diffusion models have undergone significant refinements, including the development of score-entropy loss~\citep{lou2024discrete} and flow-matching formulation~\citep{campbell2024generative,gat2024discrete}. These models generally fall into two categories based on their noise distribution: uniform~\citep{lou2024discrete,schiff2024simple} and masked (absorbing state)~\citep{ou2024your,shi2024simplified,sahoo2024simple,zheng2024masked}, each offering unique advantages in modeling discrete distributions. Recent theoretical advances have emerged through numerous studies~\citep{chen2024convergence, zhang2024convergence, ren2024discrete}.

% \vspace{-0.7em}
\paragraph{High-Order Scheme for Continuous Diffusion Models.}

The development of high-order numerical schemes for solving ODEs and SDEs represents decades of research, as comprehensively reviewed in \citet{butcher1987numerical,kloeden1992numerical,kloeden2012numerical}. These schemes have recently been adapted to accelerate continuous diffusion model inference, encompassing approaches such as the exponential integrators~\citep{zhang2022fast,zhanggddim}, Adams-Bashforth methods~\citep{lu2022dpm++,xue2024sa,zhangsong2023improved}, Taylor methods~\citep{tachibana2021quasi,dockhorn2022genie} and (stochastic) Runge-Kutta methods~\citep{liu2022pseudo,lu2022dpm,karras2022elucidating, zheng2023dpm, li2024accelerating,wu2024stochastic}.


% \vspace{-0.7em}
\paragraph{High-Order Scheme for Chemical Reaction Systems.}

Regarding approximate methods developed for simulating compound Poisson processes and chemical reaction systems with state-dependent intensities, efforts have been made on the $\tau$-leaping method~\citep{gillespie2001approximate}, and its extensions~\cite{cao2004numerical, burrage2004poisson, hu2011weaka, hu2009highly}. For a quick review of the problem setting and these methods, one may refer to~\citet{weinan2021applied}. The adaption of these methods to discrete diffusion models presents unique challenges due to the presence of both time and state-inhomogeneous intensities in the underlying Poisson processes.

\section{Preliminaries}

In this subsection, we review several basic concepts and previous error analysis results of discrete diffusion models.

% \vspace{-0.5em}
\subsection{Discrete Diffusion Models}


In discrete diffusion models, one considers a continuous-time Markov chain (CTMC) $(\vx_t)_{0 \leq t \leq T}$ on a finite space $\sX$ as the \emph{forward process}. We represent the distribution of $\vx_t$ by a vector $\vp_t \in \Delta^{|\sX|}$, where $\Delta^{|\sX|}$ denotes the probability simplex in $\R^{|\sX|}$. Given a target distribution $\vp_0$, the CTMC satisfies the following equation:
\begin{equation}
	% \setlength{\abovedisplayskip}{7pt}
	% \setlength{\belowdisplayskip}{7pt}
	\dfrac{\dif \vp_t}{\dif t} = \mQ_t \vp_t, \ \text{where}\ \mQ_t = (Q_t(y, x))_{x, y\in \sX}
	\label{eq:forward}
\end{equation}
is the rate matrix at time $t$ satisfying
\begin{enumerate}[label=(\roman*), itemsep=0pt, topsep=0pt]
	\item For any $x \in \sX$, $Q_t(x, x) = - \sum_{y \neq x} Q_t(y, x)$;
	\item For any $x \neq y \in \sX$, $Q_t(x, y) \geq 0$.
\end{enumerate}
Below we will use the notation
$\mQ^0_t = \mQ_t - \diag \mQ_t$.
It can be shown that the corresponding backward process is of the same form but with a different rate matrix~\citep{kelly2011reversibility}:
\begin{equation}
	% \setlength{\abovedisplayskip}{7pt}
	% \setlength{\belowdisplayskip}{7pt}
	\dfrac{\dif \cev{\vp}_s}{\dif s}
	= \overline{\mQ}_s \cev{\vp}_s,
	\label{eq:backward}
\end{equation}
where $\cev *_s$ denotes $*_{T-s}$ and the rate matrix is defined by
\begin{equation*}
	% \setlength{\abovedisplayskip}{8pt}
	% \setlength{\belowdisplayskip}{8pt}
	\overline Q_s(y, x) = \begin{cases}
		\tfrac{\cev p_s(y)}{\cev p_s(x)} \cev Q_s(x, y),\  & x \neq y \in \sX, \\
		- \sum_{y' \neq x} \overline Q_s(y', x),\          & x = y \in \sX.
	\end{cases}
\end{equation*}
The rate matrix $\mQ_t$ is often chosen to possess certain sparse structures such that the forward process converges to a simple distribution that is easy to sample from. Popular choices include the uniform and absorbing state cases~\citep{lou2024discrete}, where the forward process~\eqref{eq:forward} converges to the uniform distribution on $\sX$ and a Dirac distribution, respectively.

Common training practice is to define the score function (or rather the score vector) as $ \vs_t(x) = (s_t(x, y))_{y \in \sX}:= \tfrac{ \vp_t}{p_t(x)}$ for any $x\in\sX$, $t\in[0,T]$ and estimate it by a neural network $ \widehat \vs^\phi_t(x)$, where the parameters $\phi$ are trained by minimizing the score entropy~\citep{lou2024discrete,benton2024denoising} for some weights $\psi_t\geq0$ as follows:
\begin{equation}
	% \setlength{\abovedisplayskip}{7pt}
	% \setlength{\belowdisplayskip}{7pt}
	\begin{aligned}
		 & \min_{\phi}\int_0^T \psi_t \E_{x_t \sim p_t} \bigg[ \sum_{y \neq x_t}Q_t(x_t, y)                                                      \\[-3pt]
		 & \left( s_t(x_t, y) \log \tfrac{s_t(x_t, y)}{\widehat s^\phi_t(x_t, y)} - s_t(x_t, y) + \widehat s^\phi_t(x_t, y)\right)\bigg] \dif t.
	\end{aligned}
	\label{eq:discrete_loss_function}
\end{equation}

Similar to the continuous case, the backward process is approximated by another CTMC $\frac{\dif \vq_s}{\dif s} = \widehat{\overline \mQ}{\vphantom{\overline \mQ}}_s^\phi \vq_s$, with $\vq_0 = \vp_\infty$ and rate matrix $\widehat{\overline \mQ}{\vphantom{\overline \mQ}}_s^\phi$, where $\widehat{\overline Q}{\vphantom{\overline \mQ}}^\phi_s(y, x) = \cev {\widehat s}_s{\vphantom{\widehat s}}^\phi(x, y) \cev Q_s(x, y)$ for any $x \neq y \in \sX$. The inference is done by first sampling from $\vp_\infty$ and then evolving the CTMC accordingly. For simplicity, we drop the superscript $\phi$ hereafter.
\vspace{-0.5em}
\subsection{Stochastic Integral Formulation of Discrete Diffusion Models}

According to~\citet{ren2024discrete}, discrete diffusion models can also be formulated as stochastic integrals, which is especially useful for their theoretical analysis. In this section, we briefly recapitulate relevant results therein and refer to~\cref{app:background} for mathematical details. Below we work on the probability space $(\Omega, \gB, \P)$ and denote the pairwise difference set of the state space $\sX$ by $\sD := \{x-y: x \neq y \in \sX\}$.

We first introduce the Poisson random measure with evolving intensities, a key concept in the formulation.
\begin{definition}[Informal Definition of Poisson Random Measure]
	The random measure $N[\lambda](\dif t, \dif \nu)$ on $\R^+\times \sD$ is called a \emph{Poisson random measure} with \emph{evolving intensity} $\lambda$ w.r.t. a measure $\gamma$ on $\sD$ if, roughly speaking, the number of jumps of magnitude $\nu$ during the infinitesimal time interval $(t, t+\dif t]$ is Poisson distributed with mean $\lambda_t(\nu) \gamma(\dif \nu) \dif t$.
	\label{def:poisson_random_measure_informal}
	\vskip -.8in
\end{definition}

% \vspace{-1em}
The forward process in discrete diffusion models~\eqref{eq:forward} can thus be represented by the following stochastic integral:
\begin{equation}
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	x_t = x_0 + \int_0^t \int_{\sD} \nu  N[\lambda](\dif t, \dif \nu),
	\label{eq:forward_integral}
\end{equation}
where the intensity $\lambda$ is defined as
$$
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	\lambda_t(\nu, \omega) = Q^0_t(x_{t^-}(\omega) + \nu, x_{t^-}(\omega))
$$
if $x_{t^-}(\omega) + \nu \in \sX$ and 0 otherwise. Here, the outcome $\omega \in \Omega$ and $x_{t^-}$ denotes the left limit of the c\`adl\`ag process $x_t$ at time $t$ with $x_{0^-} = x_0$. We will also omit the variable $\omega$, should it be clear from context.

The backward process in discrete diffusion models~\eqref{eq:backward} can also be represented similarly as:
\begin{equation}
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	y_s = y_0 + \int_0^s \int_{\sD} \nu  N[\mu](\dif s, \dif \nu),
	\label{eq:backward_integral}
\end{equation}
where the intensity $\mu$ is defined as
\begin{equation}
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	\mu_s(\nu, \omega) = \cev{s}_s(y_{s^-}, y_{s^-} + \nu) \cev Q^0_s(y_{s^-}, y_{s^-} + \nu)
	\label{eq:backward_intensity}
\end{equation}
if $y_{s^-} + \nu \in \sX$ and 0 otherwise. During inference,
$$
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	\widehat y_s = \widehat y_0 + \int_0^s \int_{\sD} \nu  N[\widehat \mu](\dif s, \dif \nu)
$$
is used instead of~\eqref{eq:backward_integral}, where the estimated intensity $\widehat \mu$ is defined by replacing the true score $\vs_t$ with the neural network estimated score $\widehat \vs_t$ in~\eqref{eq:backward_intensity}.
\label{prop:discrete_diffusion_integral}

% \vspace{-1em}
In the following, we will also denote the intensity $\mu_s(\nu, \omega)$ at time $s$ by $\mu_s(\nu, y_{s^-})$ with slight abuse of terminology to emphasize its dependency on $\omega$ through $y_{s^-}(\omega)$.

\section{Numerical Schemes for Discrete Diffusion Model Inference}

In this section, we discuss existing numerical schemes for discrete diffusion models, including exact simulation methods and the $\tau$-leaping method.
% \vspace{-0.5em}
\subsection{Exact Simulation Methods}

Unlike continuous diffusion models, where exact simulation is beyond reach, discrete diffusion models permit inference without discretization error. Notable examples of unbiased samplers include the uniformization algorithm~\citep{chen2024convergence} for the uniform state case and the First-Hitting Sampler (FHS)~\citep{zheng2024masked} for the absorbing state case. The main idea behind these methods is to first sample the next jump time and then the jump itself.

Theoretical analysis~\cite{ren2024discrete} reveals that such schemes \emph{lack guarantees with finite computation budget}, since the number of required jumps (and thus the inference time) follows a random distribution with expectation $\Omega(d)$, where $d$ is the data dimension. This computational restriction may be less favorable for high-dimensional applications, such as generative modeling of DNA or protein sequences.

Furthermore, \emph{ the absence of discretization error does not necessarily translate to superior sample quality}, given the inherent estimation errors in neural network-based score functions.
This limitation is further amplified by the \emph{highly skewed distribution} of jumps, with a significant concentration occurring during the terminal phase of the backward process, precisely when the neural network-based score function exhibits the highest estimation error. This phenomenon stems from the potential singularity of the target distribution $\vp_0$, which induces singularities in the true score function, making accurate neural network estimation particularly challenging during the terminal phase of the backward process (\emph{cf.} Assumption 4.4 in \citet{ren2024discrete}).

\begin{figure}[ht]
	% \vskip -0.1in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{uniformization_intensity.pdf}}
		\vskip -0.2in
		\caption{An illustrative application of the uniformization algorithm to discrete diffusion models for text generation. The $x$-axis denotes the time of the backward process, and the $y$-axis denotes the frequency of jumps reflected by NFE. Perplexity convergence occurs well before the NFE experiences unbounded growth.}
		\vskip -0.3in
		\label{fig:uniformization}
	\end{center}
\end{figure}

\cref{fig:uniformization} illustrates an application of the uniformization algorithm to discrete diffusion model inference for text generation, with detailed experimental parameters presented in~\cref{sec:imagenet} and~\cref{app:image_exp}. As the process approaches the target distribution ($t\to T$), the number of required jumps grows unbounded, while perplexity improvements become negligible. This skewed distribution of computational effort results in numerous \emph{redundant function evaluations}.

Although early stopping is commonly adopted at $T-\delta$ for some small $\delta \ll 1$ to alleviate this inefficiency, this approach introduces challenges in the parameter selection of $\delta$, particularly under computational constraints or when efficiency-accuracy trade-offs are desired. Moreover, the variable jump schedules across batch samples complicate parallelization efforts in exact methods, highlighting the need for more adaptable and efficient algorithmic solutions.

% \vspace{-.5em}
\subsection{Approximate Method: $\tau$-Leaping Method}

The $\tau$-leaping method~\citep{gillespie2001approximate, campbell2022continuous} represents a widely adopted scheme that effectively addresses both dimensionality scaling and inference time control challenges.
This Euler-type scheme approximates the backward process with time-dependent intensity $\widehat \mu_t$ via the following updates:
\begin{equation}
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{3pt}
	\widehat y_{t+\delta} = \widehat y_t + \sum_{\nu \in \sD} \nu \gP\left(\widehat \mu_t(\nu)\delta\right).
	\label{eq:tau_leaping}
\end{equation}
In general, one may design different discretization schemes for $\tau$-leaping, and the summation in~\eqref{eq:tau_leaping} is parallelizable, underscoring the method's flexibility and efficiency. We refer to~\cref{alg:tau_leaping} and~\cref{app:error_analysis_tau_leaping} for a detailed description of the $\tau$-leaping method for discrete diffusion model inference.

Regarding convergence properties as the time discretization becomes increasingly refined, theoretical analyses by~\citet{campbell2022continuous, ren2024discrete} have established the error bounds of the $\tau$-leaping method, the results of which are summarized in the following theorem. Further discussion can be found in \cref{app:error_analysis_tau_leaping}.

\begin{theorem}[Thm.~4.7 in~\citet{ren2024discrete}]
	% \vskip -.1in
	For the state space $\sX = [S]^d$, with $S$ sites along each dimension, under certain discretization scheme and assumptions and given an $\epsilon$-accurate score function,  the following error bound holds:
	\begin{equation}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\KL(p_{\delta}\| \widehat q_{T-\delta})
		\lesssim  \exp(- T)  + \epsilon + \kappa T,
		\label{eq:tau_leaping_error}
	\end{equation}
	where $\delta \ll 1$ is the early stopping time, $\kappa$ is the parameter controlling the step size, and $T$ is the time horizon. The notation $\lesssim$ means that the left-hand side is bounded by the right-hand side up to a constant factor as $\kappa \to 0$.
	\label{thm:tau_leaping}
\end{theorem}

% \vspace{-.5em}
The error bound~\eqref{eq:tau_leaping_error} decouples three error sources of the $\tau$-leaping scheme: the truncation error $\gO(e^{-T})$, the score estimation error $\epsilon$, and the discretization error $\gO(\kappa T)$. Similar to the case for the Euler method for ODEs and the Euler-Maruyama scheme for SDEs, the $\tau$-leaping method is a first-order scheme in terms of the discretization error $\gO(\kappa T)$.

% \vspace{-0.5em}
\subsection{Approximate Method: High-Order Schemes}
\label{sec:high_order_schemes}

A natural improvement of $\tau$-leaping is to develop high-order schemes for discrete diffusion models. As a foundational example, consider the second-order Runge-Kutta (RK-2) method with two stages~\citep{butcher1987numerical} for solving the ODE $\dif x_t = f_t(x_t)\dif t$. This method represents one of the simplest high-order numerical schemes:
\begin{equation}
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	\begin{aligned}
		\widehat x_{t+ \theta \delta}^* = \widehat x_t & + f_t(\widehat x_t) \theta \delta,                                                                                                          \\
		\widehat x_{t+ \delta} = \widehat x_t          & +  \left[(1-\tfrac{1}{2\theta}) f_t(\widehat x_t) + \tfrac{1}{2\theta} f_{t+\theta \delta} (\widehat x_{t+ \theta \delta}^*) \right]\delta.
	\end{aligned}
	\label{eq:rk2}
\end{equation}
This scheme reduces to the exact midpoint method when $\theta = \frac{1}{2}$ and Heun's method when $\theta = 1$.
The underlying intuition stems from the observation that for $f \in C^2(\R)$,
$$
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	\left[\left(1-\tfrac{1}{2\theta}\right)f(a) + \tfrac{1}{2\theta}f(a+\theta \delta)\right] \delta
$$
offers a second-order approximation of $\int_{a}^{a+\delta}f(x)\dif x$ in contrast to $f(a)\delta$, which is only first-order.

This approach has been successfully adapted for SDE simulation~\citep{burrage1996high} and continuous diffusion model inference~\citep{karras2022elucidating,lu2022dpm,lu2022dpm++,zheng2023dpm, wu2024stochastic}. Notably, these methods enhance sample quality and computational efficiency without requiring additional model training, making the development of high-order schemes for discrete diffusion inference both theoretically appealing and practically viable.

% \vspace{-0.5em}
\section{Algorithm}

In this section, we present the high-order solvers proposed for simulating discrete diffusion models and their associated stochastic integral formulations. We will primarily focus on two-stage algorithms aiming for second-order accuracy. Specifically, we will introduce the $\theta$-RK-2 method and the $\theta$-Trapezoidal method.

Throughout this section, we assume a time discretization scheme $(s_i)_{i\in[0:N]}$ with
$$
	0 = s_0 < s_1 < \cdots < s_N = T - \delta,
$$
where $\delta$ is the early stopping time. We will also use the shorthand notations $*_+ = \max\{0, *\}$. For any $s \in (s_n, s_{n+1}]$ and $n \in [0:N-1]$, we define $\floor{s} = s_n$, $\rho_s = (1-\theta)s_n +\theta s_{n+1}$, $\Delta_n = s_{n+1} - s_n$, and \emph{$\theta$-section points} as $\rho_n = (1-\theta)s_n +\theta s_{n+1}$.
We choose $\gamma(\dif \nu)$ to be the counting measure on $\sD$.

% \vspace{-0.7em}
\subsection{$\theta$-RK-2 Method}

We first present the \emph{$\theta$-RK-2 method}, which is simple in design and serves as a natural analog of the second-order RK method for ODEs~\eqref{eq:rk2} in terms of time and state-dependent Poisson random measures, as a warm-up for the $\theta$-trapezoidal method. We note that similar methods have been proposed for simulating SDEs driven by Brownian motions or Poisson processes, such as the stochastic~\citep{burrage1996high} and the Poisson~\citep{burrage2004poisson} RK methods. A summary of this method is given in~\cref{alg:midpoint}.

% \vspace{-0.2em}
\IncMargin{1.5em}
\begin{algorithm}[!ht]
	\caption{$\theta$-RK-2 Method for Discrete Diffusion Model Inference}
	\label{alg:midpoint}
	\Indm
	\KwIn{$\widehat y_0 \sim q_0$, $\theta \in [\frac{1}{2},1]$, time discretization $(s_n, \rho_n)_{n\in[0:N-1]}$, $\widehat \mu$, $\widehat \mu^*$ as defined in~\cref{prop:integral_formulation_midpoint}.}
	\KwOut{A sample $\widehat y_{s_N}\sim \widehat q_{t_N}^\RK$.}
	\Indp
	\For{$n = 0$ \KwTo $N-1$}{
		$\displaystyle\widehat y^*_{\rho_n} \leftarrow \widehat y_{s_n} + \sum_{\nu \in \sD} \nu
			\gP\left(\widehat\mu_{s_n}(\nu)\theta\Delta_n\right)$\;
		$\displaystyle\widehat y_{s_{n+1}} \leftarrow \widehat y_{s_n} + \sum_{\nu \in \sD}\nu \gP\left(
			\left( \left(1-\tfrac{1}{2\theta}\right) \widehat\mu_{s_n} + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_n}\right)(\nu) \Delta_n\right)$\;
	}
\end{algorithm}
% \vspace{-0.3em}

Intuitively, the $\theta$-RK-2 method is a two-stage algorithm that:
\begin{enumerate}[label=(\roman*), leftmargin=*, itemsep=0em, topsep=0em, wide=0pt]
	\item Firstly, it runs $\tau$-leaping with step size $\theta\Delta_n$, obtains an \emph{intermediate state} $\widehat y^*_{\rho_n}$ at the $\theta$-section point $\rho_n$, and evaluates the intensity $\widehat \mu^*_{\rho_n}$ there;
	\item Then it runs another step of $\tau$-leaping for a full step $\Delta_n$ using a weighted sum of the intensities at the current time point $s_n$ and the $\theta$-section point $\rho_n$.
\end{enumerate}

In order to rigorously analyze and better illustrate the $\theta$-RK-2 method, we need the following definition:
\begin{definition}[Intermediate Process]
	We define the intermediate process $\widehat y^*_{s}$ piecewisely on $(s_n, s_{n+1}]$ as follows
	\begin{equation}
		\widehat y^*_{s} = \widehat y_{s_n} + \int_{s_n}^{s} \int_{\sD} \nu N\left[
			\widehat \mu_{s_n} \right](\dif s, \dif \nu),
		\label{eq:intermediate_process}
	\end{equation}
	where the intensity $\widehat \mu_{s_n}$ is given by
	\begin{equation}
		\widehat \mu_{s_n}(\nu, \widehat y_{s_n}) = \cev{\widehat s}{\vphantom{\widehat s}}_{s_n}(\widehat y_{s_n}, \widehat y_{s_n}+\nu) \cev Q^0_{s_n}(\widehat y_{s_n}, \widehat y_{s_n}+\nu).
		\label{eq:piecewise_intensity}
	\end{equation}
	\emph{i.e.},  $\widehat y^*_{s}$ is the process obtained by performing $\tau$-leaping from time $s_n$ to $s$ with intensity $\widehat \mu$.
\end{definition}

The following proposition provides the stochastic integral formulation of this method. See \cref{app:proofs_integral_formulation} for the proof.

\begin{proposition}[Stochastic Integral Formulation of $\theta$-RK-2 Method]
	The $\theta$-RK-2 method (\cref{alg:midpoint}) is equivalent to solving the following stochastic integral:
	\begin{equation}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\widehat y_s^\RK = \widehat y_0^\RK + \int_0^s \int_{\sD} \nu  N\left[
			\widehat \mu^\RK   \right](\dif s, \dif \nu),
		\label{eq:interpolating_process_midpoint}
	\end{equation}
	in which the intensity $\widehat \mu^\RK$ is defined as a weighted sum
	\begin{equation}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\widehat \mu_s^\RK(\nu) = (1-\tfrac{1}{2\theta}) \widehat\mu_{\floor{s}}(\nu, \widehat y^\RK_{\floor{s}}) + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_s}(\nu, \widehat y^*_{\rho_s}),
		\label{eq:midpoint_intensity}
	\end{equation}
	and the intermediate intensity $\widehat \mu^*$ is defined piecewisely as
	\begin{equation}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\widehat \mu^*_s(\nu, \widehat y^*_s) = \cev{\widehat s}{\vphantom{\widehat s}}_s(\widehat y^*_s, \widehat y^*_s+\nu) \cev Q^0_s(\widehat y^*_s, \widehat y^*_s+\nu),
		\label{eq:intermediate_intensity}
	\end{equation}
	with the intermediate process $\widehat y^*_s$ defined in~\eqref{eq:intermediate_process} for the corresponding interval.
	We will call the process $\widehat y_s^\RK$ the \emph{interpolating process} of the $\theta$-RK-2 method and denote the distribution of $\widehat y_s^\RK$ by $\widehat q_s^\RK$.
	\label{prop:integral_formulation_midpoint}
\end{proposition}

We emphasize that our method is different from the midpoint method proposed in~\citet{gillespie2001approximate} for simulating chemical reactions, where the Poisson random variable in the first step is replaced by its expected magnitude. We remark that such modification is in light of the lack of continuity and orderliness of the state space.

% \vspace{-0.5em}
\subsection{$\theta$-Trapezoidal Method}

As to be shown theoretically and empirically, the conceptually simple $\theta$-RK-2 method may have limitations in terms of both accuracy and efficiency. To this end, we propose the following \emph{$\theta$-trapezoidal method}, which is developed based on existing methods proposed for simulating SDEs~\citep{anderson2009weak} and chemical reactions~\citep{hu2011weaka}. Below we introduce two parameters that will be used extensively later:
\begin{equation*}
	% \setlength{\abovedisplayskip}{5pt}
	% \setlength{\belowdisplayskip}{5pt}
	\alpha_1 = \tfrac{1}{2\theta(1-\theta)}\text{ and } \alpha_2 = \tfrac{(1-\theta)^2 + \theta^2}{2\theta(1-\theta)},\text{ with }\alpha_1 - \alpha_2 = 1.
\end{equation*}

The $\theta$-trapezoidal method is summarized in~\cref{alg:trapezoidal}. Intuitively, this method separates each interval $(s_n, s_{n+1}]$ into two sub-intervals $(s_n, \rho_n]$ and $(\rho_n, s_{n+1}]$, on which simulations are detached with two different intensities designed in a balanced way.

% \vspace{-0.2em}
\begin{algorithm}[!ht]
	\caption{$\theta$-Trapezoidal Method for Discrete Diffusion Model Inference}
	\label{alg:trapezoidal}
	\Indm
	\KwIn{$\widehat y_0 \sim q_0$, $\theta \in (0,1]$, time discretization $(s_n, \rho_n)_{n\in[0:N-1]}$, $\widehat \mu, \widehat \mu^*$ as defined in~\cref{prop:integral_formulation_trapezoidal}.}
	\KwOut{A sample $\widehat y_{s_N}\sim \widehat q_{t_N}^\trap$.}
	\Indp
	\For{$n = 0$ \KwTo $N-1$}{
		$\displaystyle\widehat y^*_{\rho_n}\leftarrow \widehat y_{s_n} + \sum_{\nu \in \sD} \nu \gP\left(\widehat\mu_{s_n}(\nu)\theta\Delta_n\right)$\;
		$\displaystyle\widehat y_{s_{n+1}} \leftarrow \widehat y^*_{\rho_n} + \sum_{\nu \in \sD}\nu \gP\left(
			\left(\alpha_1\widehat\mu^*_{\rho_n} - \alpha_2\widehat\mu_{s_n}\right)_+(\nu)(1-\theta)\Delta_n\right)$\;
		\vspace{-1.2em}
	}
\end{algorithm}
% \vspace{-0.3em}

Compared to the $\theta$-RK-2 method, the $\theta$-trapezoidal method is also a two-stage algorithm with an identical first step. The second step, however, differs in two major aspects:
\begin{enumerate}[label=(\arabic*), leftmargin=*, itemsep=0em, topsep=0em, wide=0pt]
	\item The second step starts from the intermediate state $\widehat y^*_{\rho_n}$ instead of $\widehat y_{s_n}$ and only runs for a fractional step $(1-\theta)\Delta_n$ rather than a full step $\Delta_n$;
	\item The weighted sum is comprised of an altered pair of coefficients $(\alpha_1, - \alpha_2)$, which performs an \emph{extrapolation} instead of interpolation with coefficients $(1-\frac{1}{2\theta}, \frac{1}{2\theta})$ as in the $\theta$-RK-2 method with $\theta \in [\frac{1}{2}, 1]$. This feature will be shown to render the algorithm an unconditionally high-order scheme effectively.
\end{enumerate}

The following proposition establishes the stochastic integral formulation of the $\theta$-trapezoidal method, whose proof can be found in~\cref{app:proofs_integral_formulation}.

\begin{proposition}[Stochastic Integral Formulation of $\theta$-Trapezoidal Method]
	The $\theta$-trapezoidal method (\cref{alg:trapezoidal}) is equivalent to solving the following stochastic integral:
	\begin{equation}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\widehat y_s^\trap = \widehat y_0^\trap + \int_0^s \int_{\sD} N [\widehat \mu^\trap] (\dif s, \dif \nu)
		\label{eq:interpolating_process_trapezoidal}
	\end{equation}
	where the intensity $\widehat \mu^\trap$ is defined piecewisely as
	\begin{equation}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\begin{aligned}
			 & \widehat \mu_s^\trap(\nu) = \vone_{s < \rho_s} \widehat\mu_{\floor{s}}(\nu, \widehat y^\trap_{\floor{s}})                                                                   \\
			 & \ + \vone_{s \geq \rho_s} \left(\alpha_1\widehat\mu^*_{\rho_s}(\nu, \widehat y^*_{\rho_s}) - \alpha_2\widehat\mu_{\floor{s}}(\nu, \widehat y^\trap_{\floor{s}})\right)_{+}.
		\end{aligned}
		\label{eq:trapezoidal_intensity}
	\end{equation}
	Above, $\vone_{(\cdot)}$ denotes the indicator function and the intermediate process $\widehat y^*_s$ is defined in~\eqref{eq:intermediate_process} for the corresponding interval. We will call the process $\widehat y_s^\trap$ the \emph{interpolating process} of the $\theta$-trapezoidal method and denote the distribution of $\widehat y_s^\trap$ by $\widehat q_s^\trap$.
	\label{prop:integral_formulation_trapezoidal}
\end{proposition}

% \vspace{-0.5em}
\section{Theoretical Analysis}

In this section, we provide the theoretical results of the $\theta$-trapezoidal and $\theta$-RK-2 methods. We will first present the assumptions and guarantees of the algorithms. Then we will present the error bounds of the algorithms and discuss the implications of the results.

% \vspace{-0.5em}
\subsection{Assumptions}

We will primarily consider the following assumptions for the analysis of the $\theta$-trapezoidal and $\theta$-RK-2 methods.

\begin{assumption}[Convergence of Forward Process]
	The forward process converges to the stationary distribution exponentially fast, \emph{i.e.},
	$\KL(p_T\|p_\infty) \leq \exp(-\rho T)$, where $\rho > 0$ is the exponential convergence rate.
	\vskip -.1in
	\label{ass:exponential}
\end{assumption}
This assumption ensures rapid convergence of the forward process, controlling error when terminated at a sufficiently large time horizon $T$, and is automatically satisfied in the masked state case and the uniform state case, given sufficient connectivity of the graph (\emph{cf.}~\citet{ren2024discrete}).
The exponential rate aligns with continuous diffusion models (\emph{cf.} in~\citet{benton2023linear}).

\begin{assumption}[Regularity of Intensity]
	For the true intensity $\mu_s(\nu, y_{s^-})$ and the estimated intensity $\widehat \mu_s(\nu,  y_{s^-})$, the following two claims hold almost everywhere w.r.t. $\mu_s(\nu, y_{s^-}) \gamma(\dif \nu) \cev p_{s^-}(\dif y_{s^-})$: (I) Both intensities belong to $C^2([0, T-\delta])$; (II) Both intensities are upper and lower bounded on $[0, T-\delta]$.
	\vskip -.1in
	\label{ass:smoothness}
\end{assumption}
This essentially assumes two key requirements of the scores: (1) the forward process evolution maintains sufficient smoothness, which is achievable through appropriate time reparametrization; and (2) if a state $y \in \sX$ is achievable by the forward process and $\nu$ is a permissible jump therefrom, then both its true and estimated intensity are bounded. This assumption corresponds to Assumps.~4.3(i),~4.4 and~4.5 in~\citet{ren2024discrete}.

\begin{assumption}[Estimation Error]
	For all grid points and $\theta$-section points, the estimation error of the neural network-based score is small, \emph{i.e.}, for any $s\in \cup_{n\in[0:N-1]}\{s_n, \rho_n\}$, %we have
	\begin{equation*}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\begin{aligned}
			\mathrm{(i)}  & \ \E\left[\int_{\sD} \left(
				\mu_s(\nu) \log \tfrac{\mu_s(\nu)}{\widehat \mu_s(\nu)} - \mu_s(\nu) + \widehat \mu_s(\nu)
			\right) \gamma(\dif \nu)\right] \leq \epsilon_\roI; \\
			\mathrm{(ii)} & \ \E\left[\int_{\sD} \left|
				\mu_s(\nu) - \widehat \mu_s(\nu)
				\right| \gamma(\dif \nu)\right] \leq \epsilon_\roII.
		\end{aligned}
	\end{equation*}
	\label{ass:estimation}
	\vskip -.1in
\end{assumption}
\vspace{-1em}

This assumption quantifies the proximity of the estimated intensity $\widehat \mu$ to the true intensity $\mu$ after sufficient training. Compared with~\citet{ren2024discrete}, the additional $L^\infty$ part in (ii) is required for technical reasons, which is similar to~\citet{chen2024probability, wu2024stochastic}. In practice, such additional assumptions may be realized by adding extra penalty terms to the objective function during training.

% \vspace{-.5em}
\subsection{Convergence Guarantees}
The following theorem summarizes our theoretical guarantees for the $\theta$-trapezoidal method:
\begin{theorem}[Second Order Convergence of $\theta$-Trapezoidal Method]
	Suppose $\theta \in (0, 1]$ and $\alpha_1\widehat\mu^*_{\rho_s} - \alpha_2\widehat\mu_{\floor{s}}\geq 0$ in~\eqref{eq:trapezoidal_intensity} for all $s\in[0,T-\delta]$, then the following error bound holds under~\cref{ass:exponential,ass:smoothness,ass:estimation}:
	\begin{equation}
		\KL(p_{\delta}\| \widehat q_{T-\delta}^\trap)
		\lesssim  \exp(- T)  + (\epsilon_\roI + \epsilon_\roII) T + \kappa^2 T,
		\label{eq:trapezoidal_error}
	\end{equation}
	where $\delta$ is the early stopping time, $\kappa = \max_{n\in[0:N-1]}\Delta_n$, \emph{i.e.}, the largest stepsize, and $T$ is the time horizon.
	\label{thm:trapezoidal}
\end{theorem}

The complete proof is presented in~\cref{app:proof_trapezoidal}. The outline is to first bound $\KL(p_{\delta}\| \widehat q_{T-\delta}^\trap)$ by the KL divergence between the corresponding path measures, as established in~\cref{thm:girsanov_trapezoidal}, and then decompose the integral in the log-likelihood and bound respectively, where the primary technique used is \emph{Dynkin's formula} (\cref{thm:dynkin}).

With a term-by-term comparison with~\cref{thm:tau_leaping}, we observe a significant improvement in the discretization error term from $\gO(\kappa T)$ to $\gO(\kappa^2 T)$. This confirms that the $\theta$-trapezoidal method achieves second-order accuracy given sufficient time horizon $T$ and accurate score estimation, with empirical validation presented in~\cref{sec:exp}.

However, within the scope of our analysis, the $\theta$-RK-2 method may not possess a theoretical guarantee as extensive as the $\theta$-trapezoidal method for all $\theta \in (0,1]$. We briefly summarize our understanding as follows.
\begin{theorem}[Conditional Second-Order Convergence of $\theta$-RK-2 Method]
	Suppose $\theta \in (0, \frac{1}{2}]$ and $(1-\tfrac{1}{2\theta} ) \widehat\mu_{\floor{s}} + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_s}\geq 0$ in~\eqref{eq:midpoint_intensity} for all $s \in [0,T-\delta]$, then the following error bound holds for the practical version (\cref{alg:midpoint-practical}) of~\cref{alg:midpoint} under~\cref{ass:exponential,ass:smoothness,ass:estimation}:
	\begin{equation*}
		% \setlength{\abovedisplayskip}{5pt}
		% \setlength{\belowdisplayskip}{5pt}
		\KL(p_{\delta}\| \widehat q_{T-\delta}^\RK)
		\lesssim  \exp(- T)  + (\epsilon_\roI + \epsilon_\roII) T + \kappa^2 T,
	\end{equation*}
	where $\delta$ is the early stopping time, $\kappa = \max_{n\in[0:N-1]}\Delta_n$, \emph{i.e.}, the largest stepsize, and $T$ is the time horizon.
	\label{thm:midpoint}
\end{theorem}

The proof of the theorem above is provided in~\cref{app:proof_midpoint}. The restricted range of $\theta$ is caused by one specific error term $\mathrm{(III.4)}$~\eqref{eq:III.4} that permits bounding with \emph{Jensen's inequality} only when $\theta \in (0, \frac{1}{2}]$, similar to its counterpart $\mathrm{(II.4)}$~\eqref{eq:II.4} in the $\theta$-trapezoidal method. The limitation arises partially because the weighted sum with coefficients $(1-\frac{1}{2\theta}, \frac{1}{2\theta})$ becomes an \emph{extrapolation} only if $1-\frac{1}{2\theta} < 0$, a feature that naturally holds for all $\theta \in (0, 1]$ in the $\theta$-trapezoidal method. These theoretical findings are consistent with the empirical observations in~\cref{fig:theta_rk2} of~\cref{app:image_exp}, where the performance of $\theta$-RK-2 method clearly peaks when $\theta \in (0, \frac{1}{2}]$.

% \vspace{-0.5em}
\begin{remark}[Comparison between Trapezoidal and RK-2 Methods]
	\vskip .1in
	Trapezoidal-type methods were originally proposed by \citet{anderson2009weak} as a minimal second-order scheme in the weak sense for simulating SDEs. In simulating chemical reaction contexts, \citet{hu2011weaka} claimed that trapezoidal-type methods also achieve second-order convergence for \emph{covariance error} apart from the weak error, a property not shared by midpoint (RK-2) methods. Our empirical results partly reflect these findings, though we defer theoretical investigation of covariance error convergence in discrete diffusion models to future work.
\end{remark}

% \vspace{-0.4em}
\section{Experiments}
\label{sec:exp}

Based on the theoretical analysis, we expect the $\theta$-trapezoidal method to outperform the $\tau$-leaping method and the $\theta$-RK-2 method in terms of sample quality given the same amount of function evaluations. This section empirically validates the anticipated effectiveness of our proposed $\theta$-trapezoidal method (\cref{alg:trapezoidal}) through comprehensive evaluations across text and image generation tasks.

Our comparative analysis includes established discrete diffusion samplers as baselines, \emph{e.g.}, the Euler method~\citep{ou2024your}, $\tau$-leaping~\citep{campbell2022continuous}, Tweedie $\tau$-leaping~\citep{lou2024discrete}, and Parallel Decoding~\citep{chang2022maskgit}. We conduct evaluations on both uniform and masked discrete diffusion models, with detailed experimental protocols provided in \cref{app:exp}.

% \vspace{-0.2em}
\subsection{15-Dimensional Toy Model}

We first evaluate the performance of the $\theta$-trapezoidal method using a $15$-dimensional toy model. The target distribution is uniformly generated from $\Delta^{15}$, with rate matrix $\mQ = \frac{1}{15}\mE-\mI$, where $\mE$ is the all-one matrix and $\mI$ is the identity matrix. This setup provides analytically available score functions, allowing isolation and quantification of numerical errors introduced by inference algorithms. We apply both the $\theta$-trapezoidal method and the $\theta$-RK-2 method to generate $10^6$ samples and estimate the KL divergence between the true ground truth $\vp_0$ and the generated distribution $\widehat \vq_T$ with bootstrap confidence intervals.

\begin{figure}[ht]
	% \vskip -0.1in
	\begin{center}
		\centerline{\includegraphics[width=.9\columnwidth]{ toy_slope.pdf}}
		\vskip -0.1in
		\caption{Empirical KL divergence between the true distribution and the generated distribution of the toy model vs. the number of steps. Data are fitted with linear regression and shaded with 95\% confidence intervals by bootstrapping.}
		\vskip -0.1in
		\label{fig:toy_model}
	\end{center}
	% \vskip -0.2in
\end{figure}

For a fair comparison, we choose $\theta = \frac{1}{2}$ for both methods, and the results are presented in \cref{fig:toy_model}. While both methods exhibit super-linear convergence as the total number of steps grows, the $\theta$-trapezoidal method outperforms the $\theta$-RK-2 method in terms of both absolute value and convergence rate, while the $\theta$-RK-2 method takes longer to enter the asymptotic regime.
Moreover, the fitted line indicates that the $\theta$-trapezoidal method approximately converges quadratically w.r.t. the step count, confirming our theories.

% \vspace{-0.2em}
\subsection{Text Generation}

For the text generation task, we employ the pre-trained score function from RADD~\citep{ou2024your} as our base model for benchmarking inference algorithms. RADD is a masked discrete diffusion model with GPT-2-level text generation capabilities~\citep{radford2019language} and is trained on the OpenWebText dataset~\citep{Gokaslan2019OpenWeb}. Our comparative analysis maintains consistent computational resources across methods, quantified through the number of score function evaluations (NFE), and evaluates the sample quality produced by the Euler method, $\tau$-leaping, Tweedie $\tau$-leaping, and our proposed $\theta$-trapezoidal method. We generate text sequences of $1024$ tokens and measure their generative perplexity following the evaluation protocol established in~\cite {ou2024your}.

\begin{table}[ht]
	% \vskip -0.1in
	\caption{Generative perplexity of texts generated by different sampling algorithms. Lower values are better, with the best in \textbf{bold}. }
	\vskip -0.1in
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccr}
				\toprule
				Sampling Methods       & NFE $=128$                 & NFE $=1024$                \\
				\midrule
				Euler                  & $\leq 86.276$              & $\leq 44.686$              \\
				Tweedie $\tau$-leaping & $\leq 85.738$              & $\leq 44.257$              \\
				$\tau$-leaping         & $\leq 52.366$              & $\leq 28.797$              \\
				$\theta$-trapezoidal   & $\boldsymbol{\leq 49.051}$ & $\boldsymbol{\leq 27.553}$ \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\label{tab:perplexity}
	% \vskip -0.2in
\end{table}

\cref{tab:perplexity} presents the results for both low ($128$) and high ($1024$) NFE, with comprehensive results across additional NFE values in~\cref{tab:perplexity_full}. The empirical results demonstrate that the $\theta$-trapezoidal method consistently produces better samples under a fixed computation budget compared with existing popular inference algorithms. Notably, it outperforms Euler and Tweedie $\tau$-leaping, two of the best-performing samplers adopted by RADD, by a large margin. These results validate the practical efficiency and accuracy of \cref{alg:trapezoidal}.

% \vspace{-0.2em}
\subsection{Image Generation}
\label{sec:imagenet}

Our experiments on the image generation task utilize the pre-trained score function from MaskGIT~\citep{chang2022maskgit, besnier2023pytorch} as the base model, which can be converted into a masked discrete diffusion model by introducing a noise schedule (see \cref{app:image_exp}). %\wei {, which can be converted into a masked discrete diffusion model by introducing a noise schedule (see \cref{app:image_exp})}. \chen{need clarification, most readers don't know MaskGIT is a discrete DM}\haoxuan{maybe just state that MaskGIT is a form of discrete diffusion and mention that detailed discussion will be deferred to appendix D?} 
MaskGIT employs a masked image transformer architecture trained on ImageNet~\citep{deng2009imagenet} of $256\times 256$ resolution, where each image amounts to a sequence of $256$ discrete image tokens following VQ-GAN tokenization~\citep{esser2021taming}. We evaluate the $\theta$-trapezoidal method against the Euler method, $\tau$-leaping, and parallel decoding under equivalent NFE budgets ranging from 4 to 64.  For each, we generate $5\times 10^4$ images and compute their Fr\'echet Inception Distance (FID) against the ImageNet validation split, following the setting in \citet{chang2022maskgit}.

\begin{figure}[ht]
	% \vskip -0.1in
	\begin{center}
		\includegraphics[width=.9\columnwidth]{ FID_without_rk2.pdf}
	\end{center}
	\vskip -0.2in
	\caption{FID of images generated by different sampling algorithms vs. number of function evaluations (NFE). Lower values are better.}
	\label{fig:imagenet_fid}
	% \vskip -0.1in
\end{figure}

\cref{fig:imagenet_fid} reveals that $\theta$-trapezoidal method (\cref{alg:trapezoidal}) consistently achieves lower (and thus better) FID values compared to both Euler method and $\tau$-leaping across all NFE values. While parallel decoding shows advantages at extremely low NFE ($\leq 8$), its performance saturates with increased computational resources, making it less favorable compared to our rapidly converging method. Additional results, including generated image samples (\cref{fig:image_samples}), are detailed in \cref{app:exp}.

% \vspace{-.2em}
\subsection{Algorithm Hyperparameters}

We evaluate the performance of the $\theta$-trapezoidal method across various $\theta$ and NFE values for both text and image generation tasks. As illustrated in~\cref{fig:imagenet_theta_ablation}, we observe that the $\theta$-trapezoidal method demonstrates notable robustness to $\theta$, with a flat landscape near the optimal choice. Our empirical analysis suggests that $\theta \in [0.3, 0.5]$ consistently yield competitive performance across different tasks.

\begin{figure}[!ht]
	% \vskip -0.1in
	\begin{center}
		\includegraphics[width=.9\columnwidth]{ theta_ablation.pdf}
	\end{center}
	\vskip -0.3in
	\caption{Sampling quality v.s. $\theta\in(0,1]$ in $\theta$-Trapezoid method. {\bfseries Upper}: Image generation, the metric is FID; {\bfseries Lower}: Text generation, the metric is generative perplexity. Lower values are better.}
	\label{fig:imagenet_theta_ablation}
	% \vskip -0.1in
\end{figure}

% \vspace{-.3em}
\section{Conclusion and Future Works}

In this work, we introduce the $\theta$-RK-2 and $\theta$-trapezoidal methods as pioneering high-order numerical schemes tailored for discrete diffusion model inference. Through rigorous analysis based on their stochastic integral formulations, we establish second-order convergence of the $\theta$-trapezoidal method and that of the $\theta$-RK-2 method under specified conditions. Our analysis indicates that the $\theta$-trapezoidal method generally provides superior robustness and computational efficiency compared to the $\theta$-RK-2 method.

Our empirical evaluations, spanning both a 15-dimensional model with precise score functions and large-scale text and image generation tasks, validate our theoretical findings and demonstrate the superiority performance of our proposed $\theta$-trapezoidal method over existing samplers in terms of sample quality under equivalent computational constraints.
Additionally, we provide a comprehensive analysis of the method's robustness by examining the optimal choice of the parameter $\theta$ in our schemes.

Future research directions include comparative analysis of these schemes and development of more sophisticated numerical approaches for discrete diffusion model inference, potentially incorporating adaptive step sizes and parallel sampling methodologies. From the perspective of applications, these methods may also show promise for tasks in computational chemistry and biology, particularly in the design of molecules, proteins, and DNA sequences.


% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}
YC is supported by the National Science Foundation under Award No. DMS-2206576. GMR is supported by a Google Research Scholar Award. MT is grateful for partial support by the National Science Foundation under Award No. DMS-1847802. LY acknowledges the support of the National Science Foundation under Award No. DMS-2208163. 

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.




% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

% \newpage

\bibliography{icml_2025}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Further Discussion on Related Works}
\label{app:related_works}

In this section, we provide a more detailed literature review of both continuous and discrete diffusion models, as well as several studies on the numerical methods for SDEs and chemical reaction systems, that are highly related to our work.

\paragraph{Discrete Diffusion Models: Methodology, Theory, and Applications.}

Discrete diffusion and flow-based models~\citep{chen2022analog, austin2021structured, floto2023diffusion, hoogeboom2021argmax, meng2022concrete, richemond2022categorical, campbell2022continuous, sun2022score, santos2023blackout} have recently been proposed as generalizations of continuous diffusion models to model discrete distributions.

Such models have been widely used in various areas of science and engineering, including but not limited to modeling retrosynthesis~\citep{igashov2023retrobridge}, solving inverse problems~\citep{murata2024g2d2}, combinatorial optimization~\citep{li2024distribution}, designing molecules, proteins, and DNA sequences~\citep{alamdari2023protein, avdeyev2023dirichlet, emami2023plug, frey2023protein, penzar2023legnet, watson2023novo, yang2023fast, campbell2024generative, stark2024dirichlet, kerby2024training, yi2024graph,zhu2024bridge}, image synthesis~\citep{esser2021imagebart, lezama2022discrete, gu2022vector}, text summarization~\citep{dat2024discrete}, as well as the generation of graph~\citep{seff2019discrete, niu2020permutation, shi2020graphaf, qin2023sparse, vignac2022digress,haefeli2022diffusion, qin2024defog, kim2024discrete}, layout~\citep{inoue2023layoutdm, zhang2023layoutdiffusion}, motion~\citep{chi2024m2d2m, lou2023diversemotion}, sound~\citep{campbell2022continuous,yang2023diffsound}, image~\citep{hu2022global, bond2022unleashing, tang2022improved, zhu2022discrete}, speech~\citep{wu2024dctts}, electronic health record~\citep{han2024guided}, tabular data~\citep{shi2024tabdiff} and text~\citep{he2022diffusionbert,savinov2021step,wu2023ar,gong2023diffuseq, zheng2023reparameterized, zhou2023diffusion, shi2024simplified, sahoo2024simple, xu2024energy, guo2024plug}. Inspired by the huge success achieved by discrete diffusion models in practice, researchers have also conducted some studies on the theoretical properties of these models, such as~\citet{chen2024convergence, zhang2024convergence, ren2024discrete}.

An extensive amount of work has also explored the possibility of making discrete diffusion models more effective from many aspects, such as optimizing the sampling schedule~\cite{park2024textit}, developing fast samplers~\citep{chen2024fast}, designing correctors based on information learnt by the model~\citep{zhao2024informed}, simplifying the loss function for training~\cite{zhao2024improving}, adding editing-based refinements~\citep{reid2023diffuser}, synergizing these models with other techniques and methodologies like distillation~\cite{hayakawa2024distillation}, Ehrenfest processes~\citep{winkler2024bridging}, Glauber dynamics~\citep{varma2024glauber}, tensor networks~\citep{causer2024discrete}, enhanced guidance mechanisms~\citep{gruver2024protein, nisonoff2024unlocking, li2024derivative, schiff2024simple}, structured preferential generation~\citep{rissanen2024improving}, the plan-and-denoise framework~\citep{liu2024think} and alternative metrics, \emph{e.g.}, the Fisher information metric \citep{davis2024fisher}. However, to the best of our knowledge, existing work on accelerating the inference of discrete diffusion models is relatively sparse compared to the ones we listed above, which makes it a direction worthwhile exploring and serves as one of the main motivations behind this work.

\paragraph{Numerical Methods for SDEs and Chemical Reaction Systems.}

Below we review advanced numerical methods proposed for simulating SDEs and chemical reaction systems, which are the main techniques adopted in our work. For the simulation of SDEs driven by Brownian motions, many studies have been performed to design more accurate numerical schemes, which have been widely applied to tackle problems in computational physics, optimization, and Monte Carlo sampling. Examples of such work include the Milstein method~\citep{mil1975approximate}, explicit methods~\citep{abdulle2008s}, multistep methods~\citep{buckwar2006multistep}, extrapolation-type  methods~\citep{talay1990expansion, anderson2009weak}, stochastic Runge Kutta methods~\citep{burrage1996high, burrage2000order, burrage2002predictor, rossler2003runge, rossler2010runge}, splitting methods~\citep{foster2024high}, methods based on gaussian mixtures~\citep{li2021numerical}, randomized midpoint method~\cite{shen2019randomized}, parallel sampling methods~\cite{anari2024fast, yu2024parallelized} as well as high-order methods for stochastic gradient Markov Chain Monte Carlo~\cite{chen2015convergence, durmus2016stochastic}, underdamped and overdamped Langevin Monte Carlo~\cite{li2019stochastic, sabanis2019higher, mou2021high,monmarche2021high,foster2021shifted}. For a more comprehensive list of related numerical methods, one may refer to~\citep{kloeden1992numerical, burrage2004numerical,milstein2004stochastic,kloeden2012numerical,weinan2021applied}.

Regarding the simulation of chemical reaction systems, numerical methods can be categorized into two classes. The first class consists of exact simulation methods, which are similar to the Kinetic Monte Carlo (KMC) method~\cite{bortz1975new} developed for simulating spin dynamics and crystal growth in condensed matter physics. Examples of such methods include the Gillespie algorithm (or the Stochastic Simulation Algorithm, a.k.a. SSA)~\citep{gillespie1976general, gillespie1977exact} and its variants for multiscale modeling~\citep{cao2005multiscale,cao2005slow,liu2005nested,weinan2007nested}, the next reaction method and its variants~\cite{gibson2000efficient, anderson2007modified}, uniformization-based methods~\cite{beentjes2019uniformization}, etc. The second class of methods are approximate simulation methods, including but not limited to the $\tau$-leaping method~\citep{gillespie2001approximate} and its variants~\cite{rathinam2003stiffness,gillespie2003improved, cao2004numerical,burrage2004poisson, burrage2004multi,cao2005avoiding,auger2006r,cao2007adaptive,bayati2009d, cao2008slow, xu2008unbiased, hu2009highly, hu2011weaka, anderson2012multilevel,moraes2014hybrid, padgett2016adaptive,lipkova2019s}.
For a subset of the methods listed above, numerical analysis has also been performed in many works~\cite{rathinam2005consistency, li2007analysis,hu2011weakb,anderson2014complexity, chen2017error} to justify their validity.

\paragraph{Continuous Diffusion Models: Methodology, Theory, and Acceleration.}

Continuous diffusion and probability flow-based models~\citep{sohl2015deep, zhang2018monge, song2019generative, ho2020denoising, song2020score, song2021maximum, lipman2022flow, liu2022flow, albergo2022building, albergo2023stochastic} have also been the most popular methods in generative modeling, with a wide range of applications in science and engineering. For a list of related work on the theoretical studies and applications of these models, one may refer to the literature review conducted in~\citep{chen2024accelerating, ren2024discrete}. Here we will only review studies on accelerating the inference of continuous diffusion models, which motivates our work.

An incomplete list of accelerating methods includes approximate mean direction solver~\citep{zhou2024fast}, restart sampling~\cite{xu2023restart}, self-consistency~\cite{heek2024multistep, song2023consistency, song2023improved, lu2024simplifying}, knowledge distillation~\cite{luhman2021knowledge,meng2023distillation, salimans2022progressive}, combination with underdamped Langevin dynamics~\cite{dockhorn2021score}, operator learning~\cite{zheng2023fast} and more recently ideas from accelerating large language models (LLMs) like caching~\citep{ma2024deepcache} and speculative decoding~\cite{de2025accelerated}. Among all the proposed accelerating methods, one major class of methods are developed based on techniques from numerical analysis like adaptive step sizes~\cite{jolicoeur2021gotta}, exponential integrators~\citep{zhang2022fast, zhanggddim}, predictor-corrector solver~\cite{zhao2024unipc}, Adams-Bashforth methods~\citep{lu2022dpm++,xue2024sa,zhangsong2023improved}, Taylor methods~\citep{tachibana2021quasi,dockhorn2022genie}, Picard iteration and parallel sampling~\citep{shih2024parallel,chung2023parallel,tang2024accelerating, cao2024deep, selvam2024self, chen2024accelerating}, (stochastic) Runge-Kutta methods~\citep{liu2022pseudo,lu2022dpm, karras2022elucidating, zheng2023dpm, li2024accelerating,wu2024stochastic} and randomized midpoint method~\citep{kandasamy2024poisson,gupta2024faster}. In contrast, there has been much fewer studies on the acceleration of discrete diffusion models via techniques from numerical analysis, which inspires the study undertaken in this paper.


\section{Mathematical Background}
\label{app:background}

In this section, we provide the mathematical background for the stochastic integral formulation of discrete diffusion models, the error analysis of the $\tau$-leaping method, and useful lemmas for the theoretical analysis of high-order schemes for discrete diffusion models.

\subsection{Stochastic Integral Formulation of Discrete Diffusion Models}
\label{app:stochastic_integral}

Throughout this section, we will assume that $(\Omega, \gF, \P)$ is a probability space, $\sX$ is a finite-state space, and denote the pairwise difference set of the state space by $\sD:= \{x-y: x \neq y \in \sX\}$. We also assume that the pairwise difference set $\sX$ is equipped with a metric $\|\cdot\|$, a finite measure $\gamma$, and a $\sigma$-algebra $\gB$.

As a warm-up, we introduce the definition of the Poisson random measure for a time-homogeneous counting process.

\begin{definition}[{Poisson Random Measure~\citep[Definition~A.1]{ren2024discrete}}]
	The random measure $N(\dif t, \dif \nu)$ on $\R^+\times \sD$ is called a \emph{Poisson random measure} w.r.t. measure $\gamma$ if it is a random counting measure satisfying the following properties:
	\begin{enumerate}[leftmargin=2em, label=(\roman*)]
		\item For any $B \in \gB$ and $0\leq s<t$, $$N((s,t]\times B)\sim \gP\left(\gamma(B)(t-s)\right);$$
		\item For any $t\geq 0$ and pairwise disjoint sets $\{B_i\}_{i\in[n]} \subset \gB$,
		      $$\left\{N_t(B_i):= N((0, t] \times B_i)\right\}_{i\in[n]}$$
		      are independent stochastic processes.
	\end{enumerate}
	\label{def:poisson_random_measure_app}
\end{definition}

Then we define the Poisson random measure with evolving intensities. The term ``evolving'' refers to that the intensity is both time and state-dependent.

\begin{definition}[{Poisson Random Measure with Evolving Intensity~\citep[Definition~A.3]{ren2024discrete}}]
	Suppose $\lambda_t(y)$ is a non-negative predictable process on $\R^+\times \sD \times \Omega$ satisfying that for any $0 \leq T < \overline T$,
	$\int_0^T \lambda_t(\nu) \dif t < \infty$, a.s..

	The random measure $N[\lambda](\dif t, \dif \nu)$ on $\R^+\times \sD$ is called a \emph{Poisson random measure} with \emph{evolving intensity} $\lambda_t(y)$ w.r.t. measure $\gamma$ if it is a random counting measure satisfying the following properties:
	\begin{enumerate}[leftmargin=2em, label=(\roman*)]
		\item For any $B \in \gB$ and $0\leq s<t$, $$N[\lambda]((s,t]\times B)\sim \gP\left(\int_s^t \int_B \lambda_\tau(\nu) \gamma(\dif \nu) \dif \tau \right);$$
		\item For any $t\geq 0$ and pairwise disjoint sets $\{B_i\}_{i\in[n]} \subset \gB$,
		      $$
			      \left\{N_t[\lambda](B_i):= N[\lambda]((0, t] \times B_i)\right\}_{i\in[n]}
		      $$ are independent stochastic processes.
	\end{enumerate}
	\label{def:poisson_random_measure_evolving}
\end{definition}

\begin{remark}[Construction of Poisson Random Measure with Evolving Intensity]

	As discussed in Thm.~A.4 in~\citet{ren2024discrete} and originally proposed by~\citet{protter1983point}, the Poisson random measure with evolving intensity can be constructed in the following way.

	One first augments the $(\sX, \gB, \nu)$ measure space to a product space $(\sD \times \R, \gB \times \gB(\R), \gamma \times m)$, where $m$ is the Lebesgue measure on $\R$, and $\gB(\R)$ is the Borel $\sigma$-algebra on $\R$.
	The Poisson random measure with evolving intensity $\lambda_t(\nu)$ can be defined in the augmented measure space as
	\begin{equation}
		N[\lambda]((s, t] \times B) := \int_s^t \int_{B} \int_\R \vone_{0\leq \xi \leq \lambda_\tau(\nu)} N(\dif \tau, \dif \nu, \dif \xi),
		\label{eq:poisson_one}
	\end{equation}
	where $N(\dif \tau, \dif \nu, \dif \xi)$ is the Poisson random measure on $\R^+\times \sD \times \R$ w.r.t. measure $\nu(\dif y)\dif \xi$.
	\label{rem:construction}
\end{remark}

The following theorem provides the change of measure theorem for Poisson random measure with evolving intensity, which is crucial for the theoretical analysis of numerical schemes for discrete diffusion models.

\begin{theorem}[{Change of Measure for Poisson Random Measure with Evolving Density~\citep[Thm.~3.3]{ren2024discrete}}]
	Let $N[\lambda](\dif t, \dif \nu)$ be a Poisson random measure with evolving intensity $\lambda_t(\nu)$, and $h_t(\nu)$ a positive predictable process on $\R^+\times \sD \times \Omega$.
	Suppose the following exponential process is a local $\gF_t$-martingale:
	\begin{equation}
		Z_t[h] := \exp\bigg(\int_0^t \int_\sD \log h_t(\nu) N[\lambda](\dif t \times \dif \nu) - \int_0^t \int_{\sD} (h_t(\nu) - 1) \lambda_t(\nu) \gamma(\dif \nu) \bigg),
		\label{eq:Z_t}
	\end{equation}
	and $\sQ$ is another probability measure on $(\Omega, \gF)$ such that $\sQ \ll \P$ with Radon-Nikodym derivative $\dif \sQ/\dif \P|_{\gF_t} = Z_t[h]$.

	Then the Poisson random measure $N[\lambda](\dif t, \dif \nu)$ under the measure $\sQ$ is a Poisson random measure with evolving intensity $\lambda_t(\nu) h_t(\nu)$.
	\label{thm:change_of_measure}
\end{theorem}



\subsection{Error Analysis of $\tau$-leaping}
\label{app:error_analysis_tau_leaping}

The $\tau$-leaping method was originally proposed by~\citet{gillespie2001approximate} and adopted for the inference of discrete diffusion models by~\citet{campbell2022continuous}.
A summary of the algorithm is given in~\cref{alg:tau_leaping}. In this subsection, we provide a sketch for the error analysis of the $\tau$-leaping method when applied to discrete diffusion models, which will be compared with that of high-order schemes later on.


\IncMargin{1.5em}
\begin{algorithm}[!ht]
	\caption{$\tau$-Leaping Method for Discrete Diffusion Model Inference}
	\label{alg:tau_leaping}
	\Indm
	\KwIn{$\widehat y_0 \sim q_0$, $\theta \in [0,1]$, time discretization $(s_n, \rho_n)_{n\in[0:N-1]}$, $\widehat \mu$, $\widehat \mu^*$ as defined in~\cref{prop:integral_formulation_midpoint}.}
	\KwOut{A sample $\widehat y_{s_N}\sim \widehat q_{t_N}^\RK$.}
	\Indp
	\For{$n = 0$ \KwTo $N-1$}{
		$\displaystyle\widehat y_{s_{n+1}}  \leftarrow \widehat y_{s_n} + \sum_{\nu \in \sD} \nu
			\gP\left(\widehat\mu_{s_n}(\nu)\Delta_n\right)$\;
	}
\end{algorithm}

\begin{proof}[Proof of~\cref{thm:tau_leaping}]
	As we are considering the case where $\sX = [S]^d$, \emph{i.e.} the state space is a $d$-dimensional grid with $S$ states along each dimension, we have $\log |\sX| = d \log S$. Then we consider a simple time-homogeneous transition matrix $\mQ_t \equiv \mQ$ that allows jumps between neighboring states with equal probability. Specifically, we have
	\begin{equation*}
		Q(y, x) =
		\begin{cases}
			1,   & \|x - y\|_1 = 1, \\
			-2d, & x = y,           \\
		\end{cases}
	\end{equation*}
	which can be verified to satisfy Assumption~4.3(i) in~\citet{ren2024discrete} with $C = 1$ and $\underline D = \overline D = 2d$. Assumption~4.3(ii) is also satisfied, as shown in Example~B.10 of~\citet{ren2024discrete}.

	Then we may apply Thm.~4.7 in~\citet{ren2024discrete} by using the required time discretization scheme according to the properties of the target distribution and plugging in the corresponding values of $C, \underline D, \overline D$. The result follows eventually by scaling the transition matrix $\mQ$ by $\frac{1}{d}$, equivalent to scaling the time by $d$.
\end{proof}


\section{Proofs}
\label{app:proofs_theoretical_results}

In this section, we provide the missing proofs in the main text. We will first provide the proofs of the stochastic integral formulations of high-order schemes for discrete diffusion models in~\cref{app:proofs_integral_formulation}. Then we will provide the proofs of the main results for the $\theta$-trapezoidal method in~\cref{app:proof_trapezoidal} and the $\theta$-RK-2 method in~\cref{app:proof_midpoint}. We remark that the proof for the $\theta$-trapezoidal method requires more techniques and is more involved, to which the proof for the $\theta$-RK-2 method is analogous. In~\cref{app:lemmas}, we provide the detailed lemmas and computations omitted in the proofs of~\cref{thm:trapezoidal} and~\cref{thm:midpoint}.

\subsection{Stochastic Integral Formulations of High-Order Schemes}
\label{app:proofs_integral_formulation}

\begin{proof}[Proof of~\cref{prop:integral_formulation_midpoint} and~\cref{prop:integral_formulation_trapezoidal}]

	Without loss of generality, we give the proof on the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, and the generalization to the whole interval $[0,T]$ is straightforward.

	Notice that once we condition on the filtration $\gF_{s_n}$ and construct the intermediate process $\widehat y_s^*$ as specified in~\eqref{eq:intermediate_process} along the interval $(s_n, s_{n+1}]$, the intermediate intensity $\widehat \mu^*$ and the piecewise intensity $\widehat \mu_{\floor{s}}$ do not evolve with time $s$ or the interpolating processes $\widehat y_s^\RK$ (or $\widehat y_s^\trap$, respectively) since it only depends on the state, the intensity at the beginning of the interval $s_n$ and other randomness that is independent of the interpolating process.

	Therefore, the stochastic integral on this interval can be rewritten as for the $\theta$-RK-2 scheme that
	\begin{equation*}
		\begin{aligned}
			\widehat y_{s_{n+1}}^\RK & = \widehat y_{s_n}^\RK + \int_{s_n}^{s_{n+1}} \int_{\sD} \nu N [\widehat \mu^\trap] (\dif s, \dif \nu)      \\
			                         & = \widehat y_{s_n}^\RK +  \int_{\sD} \nu N [\widehat \mu^\RK] ((s_n, s_{n+1}], \dif \nu)                    \\
			                         & = \widehat y_{s_n}^\RK +  \int_{\sD} \nu \gP(\widehat \mu^\RK_{s_n}(\nu) (s_{n+1} - s_n)) \gamma(\dif \nu),
		\end{aligned}
	\end{equation*}
	and for the $\theta$-Trapezoidal scheme that
	\begin{equation*}
		\begin{aligned}
			\widehat y_{s_{n+1}}^\trap & = \widehat y_{s_n}^\trap + \int_{s_n}^{s_{n+1}} \int_{\sD} \nu N [\widehat \mu^\trap] (\dif s, \dif \nu)        \\
			                           & = \widehat y_{s_n}^\trap +  \int_{\sD} \nu N [\widehat \mu^\trap] ((s_n, s_{n+1}], \dif \nu)                    \\
			                           & = \widehat y_{s_n}^\trap +  \int_{\sD} \nu \gP(\widehat \mu^\trap_{s_n}(\nu) (s_{n+1} - s_n)) \gamma(\dif \nu),
		\end{aligned}
	\end{equation*}
	and the statement follows by taking $\gamma(\dif \nu)$ as the counting measure.
\end{proof}


\subsection{Convergence Analysis of the $\theta$-Trapezoidal Method}
\label{app:proof_trapezoidal}

\begin{theorem}
	\label{thm: trapezoidal KL chain rule}
	Let $\cev p_{0:T-\delta}$ and $\widehat q_{0:T-\delta}^\trap$ be the path measures of the backward process with the stochastic integral formulation~\eqref{eq:backward_integral} and the interpolating process~\eqref{eq:interpolating_process_trapezoidal} of
	the $\theta$-trapezoidal method (\cref{alg:trapezoidal}), then it holds that
	\begin{equation}
		\begin{aligned}
			\KL(\cev p_{T-\delta} \| \widehat q_{T-\delta}^\trap)
			 & \leq \KL(\cev p_{0:T-\delta} \| \widehat q_{0:T-\delta}^\trap)                                                                                                                                                                  \\
			 & \leq \KL(\cev p_0 \| \widehat q_0) + \E\left[\int_0^{T-\delta} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu) \right) \gamma(\dif \nu) \dif s \right],
		\end{aligned}
		\label{eq:kl_error_trapezoidal}
	\end{equation}
	where the intensity $\widehat \mu^\trap$ is defined in~\eqref{eq:interpolating_process_trapezoidal}, and the expectation is taken w.r.t. both paths generated by the backward process~\eqref{eq:backward_integral} and the randomness of the Poisson random measure used in the first step of each iteration of the algorithm, \emph{i.e.}, the construction of the intermediate process~\eqref{eq:intermediate_process}, which is assumed to be independent of that of the backward process.
	\label{thm:girsanov_trapezoidal}
\end{theorem}

\begin{proof}

	First, we will handle the randomness introduced by the Poisson random measure in the first step of each iteration of the $\theta$-trapezoidal method. For the ease of presentation, we encode the aforementioned randomness as a random variable $\zeta$ and suppose it is still supported on the probability space $(\Omega, \gF, \P)$ while being independent of the backward process. Then for each realization of $\zeta$, the intermediate process $\widehat y_s^*$ is constructed as in~\eqref{eq:intermediate_process} and the corresponding intensity $\widehat \mu_s^*$ is defined in~\eqref{eq:intermediate_intensity}.

	Given the stochastic integral formulation of the backward process~\eqref{eq:backward_integral} and the interpolating process of the $\theta$-trapezoidal method~\eqref{eq:interpolating_process_trapezoidal}, we have by~\cref{thm:change_of_measure} that this particular realization of the path measure $\widehat q_{0:T-\delta}^\trap$ can be obtained by changing the path measure $\cev p_{0:T-\delta}$ with the Radon-Nikodym derivative
	\begin{equation*}
		Z_t\left[\frac{\widehat\mu^\trap}{\mu}\right] = \exp\left(- \int_0^t \int_{\sD} \log \frac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} N[\mu](\dif s, \dif \nu) + \int_0^t \int_{\sD} \left(\mu_s(\nu) - \widehat \mu_s^\trap(\nu)\right) \gamma(\dif \nu) \dif s\right),
	\end{equation*}
	\emph{i.e.},
	\begin{equation*}
		\begin{aligned}
			  & \KL(\cev p_{0:T-\delta} \| \widehat q_{0:T-\delta}^\trap | \zeta) = \E\left[\log Z_{T-\delta}^{-1}\left[\frac{\widehat\mu^\trap}{\mu}\right]\right]                                       \\
			= & \E\left[\int_0^{T-\delta} \int_{\sD} \left( \mu_s(\nu) \log \frac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu)\right) \gamma(\dif \nu) \dif s \right].
		\end{aligned}
	\end{equation*}

	Then it is easy to see by the data processing inequality and the chain rule of KL divergence that
	\begin{equation*}
		\begin{aligned}
			  & \KL(\cev p_{T-\delta} \| \widehat q_{T-\delta}^\trap) \leq \KL(\cev p_{0:T-\delta} \| \widehat q_{0:T-\delta}^\trap) \leq \E\left[\KL(\cev p_{T-\delta} \| \widehat q_{T-\delta}^\trap | \zeta)\right]                     \\
			= & \KL(\cev p_0 \| \widehat q_0) + \E\left[\int_0^{T-\delta} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu) \right) \gamma(\dif \nu) \dif s \right],
		\end{aligned}
	\end{equation*}
	and the proof is complete.
\end{proof}

In the following, we will provide the outline of the proof of~\cref{thm:trapezoidal}, where we leave the proof of several lemmas and detailed calculations to~\cref{app:lemmas} for the clarity of presentation.

\begin{proof}[Proof of~\cref{thm:trapezoidal}]

	Throughout this proof, including the subsequent lemmas and propositions that will be detailed in~\cref{app:lemmas}, we will assume that $(y_s)_{s\in[0,T]}$ is a process generated by the path measure $\cev p_{0:T}$ of the backward process with the stochastic integral formulation~\eqref{eq:backward_integral} and set it as the underlying paths of the expectation in~\eqref{eq:kl_error_trapezoidal} as required by~\cref{thm:girsanov_trapezoidal}. Especially, $y_s \sim \cev p_s$ holds for any $s\in[0,T]$. For simplicity, we will assume that the process $y_s$ is left-continuous at each grid point $s_i$ for $i\in[0:N]$, which happens with probability one.


	We first consider the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, and thus we have $\floor{s} = s_n$ and $\rho_s = \rho_n$. Within this interval, we will denote its intermediate process as appeared in~\eqref{eq:intermediate_process} as $y_s^*$, and the corresponding intermediate intensity as appeared in~\eqref{eq:intermediate_intensity} as $\widehat \mu_s^*$. In the following discussion, we will assume implicitly that the processes are conditioned on the filtration $\gF_{s_n}$.

	By the definition of the intensity $\widehat \mu^\trap(\nu)$ as specified in~\eqref{eq:trapezoidal_intensity}
	\begin{equation*}
		\widehat \mu_s^\trap = \vone_{s < \rho_s} \widehat\mu_{\floor{s}} + \vone_{s \geq \rho_s} \left(\alpha_1\widehat\mu^*_{\rho_s} - \alpha_2\widehat\mu_{\floor{s}}\right)_{+},
	\end{equation*}
	we can rewrite the corresponding part of the integral in~\eqref{eq:kl_error_trapezoidal} as
	\begin{equation*}
		\begin{aligned}
			  & \int_{s_n}^{s_{n+1}} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu) \right) \gamma(\dif \nu) \dif s                                                                                                                        \\
			= & \left(\int_{s_n}^{\rho_n} + \int_{\rho_n}^{s_{n+1}}\right) \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu) \right) \gamma(\dif \nu) \dif s                                                                                  \\
			= & \underbrace{\int_{s_n}^{\rho_n} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_{s_n}(\nu)} - \mu_s(\nu) + \widehat \mu_{s_n}(\nu) \right) \gamma(\dif \nu) \dif s }_{\mathrm{(I)}}                                                                                                \\
			+ & \underbrace{\int_{\rho_n}^{s_{n+1}} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)} - \mu_s(\nu) + \alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu) \right) \gamma(\dif \nu) \dif s}_{\mathrm{(II)}},
		\end{aligned}
	\end{equation*}
	where the assumption that $\alpha_1\widehat\mu^*_{\rho_s} - \alpha_2\widehat\mu_{\floor{s}}\geq 0$ for all $s\in[0,T-\delta]$ is applied here for the second term $\mathrm{(II)}$ above. We note that such positivity assumption also exists in the analysis performed by~\citet{anderson2009weak} and~\citet{hu2011weaka} and a more detailed discussion on such assumption is deferred to~\cref{remark: positivity assumption}.
	\paragraph{Decomposition of the Integral.}

	Next, we decompose the integral $\mathrm{(I)}$ and $\mathrm{(II)}$ into several terms, the magnitudes of which or combinations of which are to be bounded.

	\begin{enumerate}[label=(\roman*)]
		\item The first term is decomposed as
		      \begin{equation*}
			      \mathrm{(I)} = \mathrm{(I.1)} + \mathrm{(I.2)} + \mathrm{(I.3)} + \mathrm{(I.4)},
		      \end{equation*}
		      where each term is defined as
		      \begin{equation*}
			      \begin{aligned}
				      \mathrm{(I.1)} & = \int_{s_n}^{\rho_n} \int_{\sD}\left(
				      \mu_{s_n}(\nu) \log \frac{\mu_{s_n}(\nu)}{\widehat \mu_{s_n}(\nu)} - \mu_{s_n}(\nu) + \widehat \mu_{s_n}(\nu)
				      \right) \gamma(\dif \nu) \dif s,                                             \\
				      \mathrm{(I.2)} & = \int_{s_n}^{\rho_n} \int_{\sD}\left(
				      \mu_s(\nu) \log \mu_s(\nu) - \mu_s(\nu) - \mu_{s_n}(\nu) \log \mu_{s_n}(\nu) + \mu_{s_n}(\nu)
				      \right) \gamma(\dif \nu) \dif s,                                             \\
				      \mathrm{(I.3)} & = \int_{s_n}^{\rho_n} \int_{\sD}\left(
				      \mu_s(\nu) - \mu_{s_n}(\nu) \right) \left( \log \left(\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu) \right) - \log \widehat \mu_{s_n}(\nu)
				      \right) \gamma(\dif \nu) \dif s,                                             \\
				      \mathrm{(I.4)} & = \int_{s_n}^{\rho_n} \int_{\sD} \mu_{s_n}(\nu) \log \left(
				      \alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)
				      \right) \gamma(\dif \nu) \dif s                                              \\
				                     & - \int_{s_n}^{\rho_n} \int_{\sD} \mu_s(\nu) \log \left(
				      \alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)
				      \right) \gamma(\dif \nu) \dif s.
			      \end{aligned}
		      \end{equation*}
		\item The second term is decomposed as
		      \begin{equation*}
			      \mathrm{(II)} = \mathrm{(II.1)} + \mathrm{(II.2)} + \mathrm{(II.3)} + \mathrm{(II.4)} + \mathrm{(II.5)} + \mathrm{(II.6)},
		      \end{equation*}
		      where each term is defined as
			      {\allowdisplaybreaks
				      \begin{align*}
					      \mathrm{(II.1)}
					       & = \alpha_1 \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
					      \mu_{\rho_n}(\nu) \log \frac{\mu_{\rho_n}(\nu)}{\widehat \mu_{\rho_n}(\nu)} - \mu_{\rho_n}(\nu) + \widehat \mu_{\rho_n}(\nu)
					      \right) \gamma(\dif \nu) \dif s                                                                                                                                         \\
					       & - \alpha_2 \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
					      \mu_{s_n}(\nu) \log \frac{\mu_{s_n}(\nu)}{\widehat \mu_{s_n}(\nu)} - \mu_{s_n}(\nu) + \widehat \mu_{s_n}(\nu)
					      \right)  \gamma(\dif \nu) \dif s                                                                                                                                        \\
					      \mathrm{(II.2)}
					       & = \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
					      \mu_s(\nu) \log \mu_s(\nu) - \mu_s(\nu)
					      \right) \gamma(\dif \nu) \dif s                                                                                                                                         \\
					       & - \int_{\rho_n}^{s_{n+1}} \int_{\sD} \big(
					      \alpha_1 (\mu_{\rho_n}(\nu) \log \mu_{\rho_n}(\nu) - \mu_{\rho_n}(\nu)) - \alpha_2 (\mu_{s_n}(\nu) \log \mu_{s_n}(\nu) - \mu_{s_n}(\nu))
					      \big) \gamma(\dif \nu) \dif s                                                                                                                                           \\
					      \mathrm{(II.3)}
					       & = \int_{\rho_n}^{s_{n+1}} \int_{\sD} \alpha_1 \left(
					      \widehat \mu_{\rho_n}^*(\nu) - \widehat \mu_{\rho_n}(\nu)
					      \right) \gamma(\dif \nu) \dif s,                                                                                                                                        \\
					      \mathrm{(II.4)}
					       & = \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
					      \alpha_1 \mu_{\rho_n}(\nu) \log \widehat \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)
					      \right) \gamma(\dif \nu) \dif s                                                                                                                                         \\
					       & - \int_{\rho_n}^{s_{n+1}} \int_{\sD} (\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left(
					      \alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)
					      \right)  \gamma(\dif \nu) \dif s                                                                                                                                        \\
					      \mathrm{(II.5)}
					       & = \int_{\rho_n}^{s_{n+1}} \int_{\sD}
					      (\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left( \alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right)
					      \gamma(\dif \nu) \dif s                                                                                                                                                 \\
					       & - \int_{\rho_n}^{s_{n+1}} \int_{\sD}
					      (\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left( \alpha_1 \widehat \mu_{\rho_n}^*(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right)
					      \gamma(\dif \nu) \dif s                                                                                                                                                 \\
					      \mathrm{(II.6)}
					       & = \int_{\rho_n}^{s_{n+1}} \int_{\sD}
					      (\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left( \alpha_1 \widehat \mu_{\rho_n}^*(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right)
					      \gamma(\dif \nu) \dif s                                                                                                                                                 \\
					       & - \int_{\rho_n}^{s_{n+1}} \int_{\sD} \mu_s(\nu) \log \left( \alpha_1 \widehat \mu_{\rho_n}^*(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right) \gamma(\dif \nu) \dif s.
				      \end{align*}}
	\end{enumerate}

	\paragraph{Bounding the Error Terms.}

	Then we briefly summarize the intuitions and related techniques used in the bound of the terms above, and the detailed calculations and proofs of the lemmas and propositions are deferred to~\cref{app:lemmas}.

	\begin{enumerate}[label=(\roman*)]
		\item {\itshape Error due to estimation error associated with the intensity:} The terms $\mathrm{(I.1)}$ and $\mathrm{(II.1)}$ are bounded by the assumption on the estimation error of the intensity $\widehat \mu_s$ (\cref{ass:estimation}), as
		      \begin{equation*}
			      \E\left[\mathrm{(I.1)} + \mathrm{(II.1)}\right] \leq \theta \Delta_n \epsilon_\roI + \alpha_1 (1-\theta) \Delta_n \epsilon_\roI  = \theta \Delta_n \epsilon_\roI + \frac{1}{2\theta} \Delta_n \epsilon_\roI \lesssim \Delta_n \epsilon_\roI,
		      \end{equation*}
		      for any $\theta \in (0, 1]$.

		      The term $\mathrm{(II.4)}$ is bounded by~\cref{prop:II.4}, as
		      \begin{equation*}
			      \E\left[\mathrm{(II.4)}\right] \lesssim \Delta_n \epsilon_\roII,
		      \end{equation*}
		      where Jensen's inequality is applied here based on the convexity of the loss.

		\item {\itshape Error related to the smoothness of intensity:} By~\cref{cor:I.2_II.2}, the terms $\mathrm{(I.2)}$ and $\mathrm{(II.2)}$ are bounded by
		      \begin{equation*}
			      \E\left[\mathrm{(I.2)} + \mathrm{(II.2)}\right] \leq \Delta_n^3.
		      \end{equation*}
		      By~\cref{cor:I.4_II.6}, the terms $\mathrm{(I.4)}$ and $\mathrm{(II.6)}$ are bounded by
		      \begin{equation*}
			      \E\left[\mathrm{(I.4)} + \mathrm{(II.6)}\right] \leq \Delta_n^3.
		      \end{equation*}

		      Intuitively, the bounds on these terms closely relate to the properties of the jump process and quantify the smoothness assumption on the intensity $\mu_s$ (\cref{ass:smoothness}), especially when the intensity does not vary significantly within the interval $(s_n, s_{n+1}]$. The main technique used for bounding these terms is Dynkin's Formula (\cref{thm:dynkin}). The third-order accuracy here directly follows from the intuition provided in~\cref{sec:high_order_schemes} based on numerical quadrature.

		\item {\itshape Error involving the intermediate process:} The terms $\mathrm{(II.3)}$ and $\mathrm{(II.5)}$ are bounded by~\cref{prop:II.3} and~\cref{cor:II.5} respectively as follows
		      \begin{equation*}
			      \E\left[\mathrm{(II.3)}\right] \lesssim \Delta_n^3 + \Delta_n^2 \epsilon_\roII, \quad \text{and}\quad \E\left[\mathrm{(II.5)}\right] \lesssim \Delta_n^3 + \Delta_n^2 \epsilon_\roII,
		      \end{equation*}
		      The term $\mathrm{(I.3)}$ is bounded by~\cref{prop:I.3} as below
		      \begin{equation*}
			      \E\left[\mathrm{(I.3)}\right] \lesssim \Delta_n^3\epsilon_\roII + \Delta_n^4 \lesssim \Delta_n^2\epsilon_\roII + \Delta_n^3.
		      \end{equation*}

		      The three terms above all involve the intermediate process $y_s^*$ and the corresponding intermediate density $\widehat \mu_s^*$.
	\end{enumerate}

	In conclusion, by summing up all these terms, we have
	\begin{equation*}
		\begin{aligned}
			         & \int_{s_n}^{s_{n+1}} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu) \right) \gamma(\dif \nu) \dif s \\
			\lesssim & \Delta_n (\epsilon_\roI + \epsilon_\roII) + \Delta_n^3 + \Delta_n^2 \epsilon_\roII \lesssim \Delta_n (\epsilon_\roI + \epsilon_\roII) + \Delta_n^3.
		\end{aligned}
	\end{equation*}
	Therefore, the overall error is bounded by first applying~\cref{thm:girsanov_trapezoidal} and then the upper bound derived above to each interval $(s_n, s_{n+1}]$, which yields
	\begin{equation*}
		\begin{aligned}
			 & \KL(\cev p_{T-\delta} \| \widehat q_{T-\delta}^\trap)                                                                                                                                                                          \\
			 & \leq \KL(\cev p_0 \| \widehat q_0) + \E\left[\int_0^{T-\delta} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\trap(\nu)} - \mu_s(\nu) + \widehat \mu_s^\trap(\nu) \right) \gamma(\dif \nu) \dif s \right] \\
			 & \lesssim \KL(\cev p_0 \| \widehat q_0) + \sum_{n=0}^{N-1} \left( \Delta_n (\epsilon_\roI + \epsilon_\roII) + \Delta_n^3\right)                                                                                                 \\
			 & \lesssim \exp(-T) + T(\epsilon_\roI + \epsilon_\roII) + \kappa^2 T,
		\end{aligned}
	\end{equation*}
	as desired.
\end{proof}

\begin{remark}[Discussion on the Positivity Assumption]
	\label{remark: positivity assumption}
	In the statement of~\cref{thm:trapezoidal}, we have assumed that $$
		\alpha_1\widehat\mu^*_{\rho_s}(\nu) - \alpha_2\widehat\mu_{\floor{s}}(\nu) \geq 0
	$$
	in~\eqref{eq:trapezoidal_intensity} for all $s\in[0,T-\delta]$, which allows us to replace $\left(\alpha_1\widehat\mu^*_{\rho_s}(\nu) - \alpha_2\widehat\mu_{\floor{s}}(\nu)\right)_+$ by the difference itself. \citet{anderson2009weak} showed that this approximation is at most of $\gO(\Delta_n^3)$ within the corresponding interval and~\citet{hu2011weaka} further proved that for any order $p \geq 1$, there exists a step size $\Delta$ such that this approximation is at least $p$-th order, \emph{i.e.}, of order $\gO(\Delta^p)$ for that step. Therefore, we believe the positive part approximation would not affect the performance of the proposed algorithm for the case of discrete diffusion models when the step size is not too large, which is also supported by our empirical studies.
\end{remark}

\subsection{Convergence Analysis of the $\theta$-RK-2 Method}
\label{app:proof_midpoint}
Here we may again apply the data processing inequality and the chain rule of KL divergence to upper bound the error associated with the $\theta$-RK-2 method. A statement of the upper bound is provided in~\cref{thm: RK-2 KL chain rule} below, whose proof is omitted here since it is similar to that of~\cref{thm: trapezoidal KL chain rule} above.
\begin{theorem}
	\label{thm: RK-2 KL chain rule}
	Let $\cev p_{0:T-\delta}$ and $\widehat q_{0:T-\delta}^\RK$ be the path measures of the backward process with the stochastic integral formulation~\eqref{eq:backward_integral} and the interpolating process~\eqref{eq:interpolating_process_midpoint} of the $\theta$-RK-2 method (\
	\cref{alg:midpoint}), then it holds that
	\begin{equation}
		\begin{aligned}
			\KL(\cev p_{T-\delta} \| \widehat q_{T-\delta}^\RK) \leq
			     & \KL(\cev p_{0:T-\delta} \| \widehat q_{0:T-\delta}^\RK)                                                                                                                                                                \\
			\leq & \KL(\cev p_0 \| \widehat q_0) + \E\left[\int_0^{T-\delta} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\RK(\nu)} - \mu_s(\nu) + \widehat \mu_s^\RK(\nu) \right) \gamma(\dif \nu) \dif s \right],
		\end{aligned}
		\label{eq:kl_error_midpoint}
	\end{equation}
	where the intensity $\widehat \mu^\RK$ is defined in~\eqref{eq:interpolating_process_midpoint}, and the expectation is taken w.r.t. both paths generated by the backward process~\eqref{eq:backward_integral} and the randomnesss of the Poisson random measure used in the first step of each iteration of the algorithm, \emph{i.e.}, the construction of the intermediate process~\eqref{eq:intermediate_process}, which is assumed to be independent of that of the backward process.
	\label{thm:girsanov_midpoint}
\end{theorem}

Following the same flow as in the proof of~\cref{thm:trapezoidal}, we will first provide an outline of the proof of~\cref{thm:midpoint}, and defer the proof of several key lemmas and detailed calculations are to~\cref{app:lemmas} for the clarity of presentation. We will also comment on the differences that may lead to the less desirable numerical properties of the $\theta$-RK-2 method.


\begin{proof}[Proof of~\cref{thm:midpoint}]
	In the following proof sketch, we will be using the same notation as in the proof of~\cref{thm:trapezoidal}, and we will assume that the process $y_s$ is left-continuous at each grid point $s_i$ for $i\in[0:N]$. We also start by taking a closer look at the integral within each interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, and denote the intermediate process as appeared in~\eqref{eq:intermediate_process} as $y_s^*$ and the corresponding intermediate intensity as appeared in~\eqref{eq:intermediate_intensity} as $\widehat \mu_s^*$.

	As defined in~\eqref{eq:midpoint_intensity}, the intensity $\widehat \mu^\RK(\nu)$ is given by
	\begin{equation*}
		\widehat \mu_s^\RK(\nu) = \left(1-\frac{1}{2\theta}\right) \widehat\mu_{\floor{s}}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_s}(\nu),
	\end{equation*}
	which helps us rewrite the corresponding part of the integral in~\eqref{eq:kl_error_midpoint} as
	\begin{equation*}
		\begin{aligned}
			  & \int_{s_n}^{s_{n+1}} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\RK(\nu)} - \mu_s(\nu) + \widehat \mu_s^\RK(\nu) \right) \gamma(\dif \nu) \dif s                                                                                                                                                                             \\
			= & \underbrace{\int_{s_n}^{s_{n+1}}\int_\sD \left(\mu_s(\nu)\log\frac{\mu_s(\nu)}{(1-\tfrac{1}{2\theta}) \widehat\mu_{s_n}(\nu) + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)} - \mu_s(\nu) + \left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right)\gamma(\dif \nu) \dif s}_{\mathrm{(III)}}.
		\end{aligned}
	\end{equation*}
	Above we again use the positivity assumption that $(1-\tfrac{1}{2\theta} ) \widehat\mu_{\floor{s}} + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_s} \geq 0$ for the term $\mathrm{(III)}$ above, just as what we have done in the proof and discussion of~\cref{thm:trapezoidal} above.

	\paragraph{Decomposition of the Integral.}

	Then we perform a similar decomposition of the integral as in the proof of~\cref{thm:trapezoidal} as follows:
	\begin{equation*}
		\mathrm{(III)} = \mathrm{(III.1)} + \mathrm{(III.2)} + \mathrm{(III.3)} + \mathrm{(III.4)} + \mathrm{(III.5)} + \mathrm{(III.6)},
	\end{equation*}
	where each term is defined as
		{\allowdisplaybreaks
			\begin{align*}
				\mathrm{(III.1)}
				 & = \left(1-\frac{1}{2\theta}\right) \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\mu_{s_n}(\nu)\log\left(\frac{\mu_{s_n}(\nu)}{\widehat\mu_{s_n}(\nu)}\right) - \mu_{s_n}(\nu) + \widehat\mu_{s_n}(\nu)\right) \gamma(\dif \nu) \dif s                                                        \\
				 & + \frac{1}{2\theta} \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\mu_{\rho_n}(\nu)\log\left(\frac{\mu_{\rho_n}(\nu)}{\widehat\mu_{\rho_n}(\nu)}\right) - \mu_{\rho_n}(\nu) + \widehat\mu_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s                                                        \\
				\mathrm{(III.2)}
				 & =  \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\mu_s(\nu)\log \mu_s(\nu)- \mu_s(\nu)\right) \gamma(\dif \nu) \dif s                                                                                                                                                                         \\
				 & - \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\left(1-\frac{1}{2\theta}\right)\left(\mu_{s_n}(\nu)\log \mu_{s_n}(\nu)-\mu_{s_n}\right)
				+\frac{1}{2\theta}\left(\mu_{\rho_n}(\nu)\log\mu_{\rho_n}(\nu)-\mu_{\rho_n}(\nu)\right)\right) \gamma(\dif \nu) \dif s                                                                                                                                                                   \\
				\mathrm{(III.3)}
				 & = \int_{s_n}^{s_{n+1}} \int_{\sD} \frac{1}{2\theta}\left(\widehat\mu^*_{\rho_n}(\nu)- \widehat\mu_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s                                                                                                                                        \\
				\mathrm{(III.4)}
				 & = \int_{s_n}^{s_{n+1}} \int_{\sD} \left( \left(1-\frac{1}{2\theta}\right)\mu_{s_n}(\nu)\log\widehat \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\log\widehat \mu_{\rho_n}(\nu) \right) \gamma(\dif \nu) \dif s                                                                 \\
				 & - \int_{s_n}^{s_{n+1}} \int_{\sD} \left( \left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\right)\log \left( \left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat \mu_{\rho_n}(\nu)\right)  \gamma(\dif \nu) \dif s \\
				\mathrm{(III.5)}
				 & = \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) + \tfrac{1}{2\theta}\mu_{\rho_n}(\nu)\right)
				\log \left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat \mu_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s                                                                                                                                          \\
				 & - \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\right)\log \left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s  \\
				\mathrm{(III.6)}
				 & = \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\right)\log\left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s    \\
				 & - \int_{s_n}^{s_{n+1}} \int_{\sD} \mu_s(\nu)\log\left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s
			\end{align*}}



	\paragraph{Bounding the Error Terms.}

	Then we briefly summarize the intuitions and related techniques used in the bound of the terms above,. Detailed calculations and proofs of the lemmas and propositions used here are deferred to~\cref{app:lemmas}.

	\begin{enumerate}[label=(\roman*)]
		\item {\itshape Error due to the intensity estimation:} The terms in $\mathrm{(III.1)}$ are bounded by the assumption on the estimation error of the intensity $\widehat \mu_s$ (\cref{ass:estimation}) as follows
		      \begin{equation*}
			      \E\left[\mathrm{(III.1)}\right] \leq \left(1-\frac{1}{2\theta}\right) \Delta_n \epsilon_\roI + \frac{1}{2\theta} \Delta_n \epsilon_\roI  = \Delta_n \epsilon_\roI,
		      \end{equation*}
		      for any $\theta \in (0, 1]$.

		\item {\itshape Error related to the smoothness of intensity:} By~\cref{cor:III.2} and~\cref{cor:III.6}, the terms $\mathrm{(III.2)}$ and $\mathrm{(III.6)}$ are bounded by
		      \begin{equation*}
			      \E\left[\mathrm{(III.2)}\right] \leq \Delta_n^3, \quad \text{and} \quad \E\left[\mathrm{(III.6)}\right] \leq \Delta_n^3,
		      \end{equation*}
		      respectively.

		\item {\itshape Error involving the intermediate process:} The term $\mathrm{(III.3)}$ and $\mathrm{(III.5)}$ are bounded in almost the same way as that of~\cref{prop:II.3} and~\cref{cor:II.5}. By simply altering the integral upper limits, we obtain that
		      \begin{equation*}
			      \E\left[\mathrm{(III.3)}\right] \lesssim \Delta_n^3 + \Delta_n^2 \epsilon_\roII, \quad \E\left[\mathrm{(III.5)}\right] \lesssim \Delta_n^3 + \Delta_n^2 \epsilon_\roII.
		      \end{equation*}
	\end{enumerate}

	The only term that cannot be directly bounded based on results in~\cref{app:lemmas} is $\mathrm{(III.4)}$, which is given by
	\begin{equation}
		\begin{aligned}
			\E\left[\mathrm{(III.4)}\right]
			= & \E\bigg[ \int_{s_n}^{s_{n+1}} \int_{\sD} \left( \left(1-\frac{1}{2\theta}\right)\mu_{s_n}(\nu)\log\widehat \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\log\widehat \mu_{\rho_n}(\nu) \right) \gamma(\dif \nu) \dif s                                                                   \\
			  & \ - \int_{s_n}^{s_{n+1}} \int_{\sD} \left( \left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\right)\log \left( \left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat \mu_{\rho_n}(\nu)\right)  \gamma(\dif \nu) \dif s \bigg]
		\end{aligned}
		\label{eq:III.4}
	\end{equation}
	Recall that in the proof of its counterpart (\cref{prop:II.4}), we utilized the convexity of the loss function and the extrapolation nature of the second step in the $\theta$-trapezoidal method~\eqref{eq:trapezoidal_intensity} to bound the error term. However, the same technique cannot be directly applied to the $\theta$-RK-2 method for any $\theta \in [0,1]$, as the intensity $\widehat \mu_s^\RK$ is an interpolation of the intensity $\widehat \mu_s$ when $\theta \in (\frac{1}{2},1]$. Therefore, below we will first focus on the case when $\theta \in (0,\frac{1}{2}]$.

	To be specific, by the assumption on the estimation error (\cref{ass:estimation}), we can reduce~\eqref{eq:III.4} to
	\begin{equation}
		\begin{aligned}
			 & \E\bigg[ \int_{s_n}^{s_{n+1}} \int_{\sD}  \left( \left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu)\log \widehat \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\log\widehat \mu_{\rho_n}(\nu) \right)                                                                                                \\
			 & \ - \int_{s_n}^{s_{n+1}} \int_{\sD} \left( \left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu_{\rho_n}(\nu)\right)\log \left( \left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat \mu_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s \bigg],
		\end{aligned}
		\label{eq:III.4alt}
	\end{equation}
	which can then be upper bounded based on Jensen's inequality and the convexity of the loss function for $\theta \in (0, \frac{1}{2}]$.

	Summing up the bounds of the terms above, we have
	\begin{equation*}
		\begin{aligned}
			         & \int_{s_n}^{s_{n+1}} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\RK(\nu)} - \mu_s(\nu) + \widehat \mu_s^\RK(\nu) \right) \gamma(\dif \nu) \dif s \\
			\lesssim & \Delta_n (\epsilon_\roI + \epsilon_\roII) + \Delta_n^3 + \Delta_n^2 \epsilon_\roII \lesssim \Delta_n (\epsilon_\roI + \epsilon_\roII) + \Delta_n^3,
		\end{aligned}
	\end{equation*}
	Consequentially, the overall error of the $\theta$-RK-2 method is bounded by
	\begin{equation*}
		\begin{aligned}
			 & \KL(\cev p_{T-\delta} \| \widehat q_{T-\delta}^\RK)                                                                                                                                                                        \\
			 & \leq \KL(\cev p_0 \| \widehat q_0) + \E\left[\int_0^{T-\delta} \int_{\sD}\left( \mu_s(\nu) \log \dfrac{\mu_s(\nu)}{\widehat \mu_s^\RK(\nu)} - \mu_s(\nu) + \widehat \mu_s^\RK(\nu) \right) \gamma(\dif \nu) \dif s \right] \\
			 & \lesssim \KL(\cev p_0 \| \widehat q_0) + \sum_{n=0}^{N-1} \left( \Delta_n (\epsilon_\roI + \epsilon_\roII) + \Delta_n^3 \right)                                                                                            \\
			 & \lesssim \exp(-T) + T(\epsilon_\roI + \epsilon_\roII) + \kappa^2 T,
		\end{aligned}
	\end{equation*}
	which suggests that the $\theta$-RK-2 is also of second order when $\theta \in (0, \frac{1}{2}]$. For the other case when $\theta \in (\frac{1}{2},1]$, we will provide a brief discussion in the remakr below.
\end{proof}

\begin{remark}[{Discussions on the case when $\theta \in (\frac{1}{2}, 1]$}]
	For $\theta \in (\frac{1}{2}, 1]$, the term~\eqref{eq:III.4alt} is positive and thus not necessarily bounded. One may wonder if, despite being positive, this term is still of at least second order. However, the answer seems negative.
	By applying the Dynkin's formula (\cref{thm:dynkin} and~\cref{cor:dynkin_first_order}) to $\mu_s \log \widehat \mu_s$ in the term $\mathrm{(III.4)}$, we have that the first integral in~\eqref{eq:III.4} can be expanded as follows
	\begin{equation*}
		\begin{aligned}
			  & \E\left[\int_{s_n}^{s_{n+1}} \int_{\sD} \left(
			\left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)
			+ \frac{1}{2\theta}  \mu_{\rho_n}(\nu) \log \widehat \mu_{\rho_n}(\nu)
			\right) \gamma(\dif \nu) \dif s\right]                                                                                                                                                                                          \\
			= & \frac{1}{2\theta} \int_{s_n}^{s_{n+1}} \int_{\sD} \left(
			\mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)  + \theta \Delta_n \gL \left( \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)\right) \right)
			\gamma(\dif \nu) \dif s                                                                                                                                                                                                         \\
			+ & \left(1-\frac{1}{2\theta}\right) \int_{s_n}^{s_{n+1}} \int_{\sD}
			\mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)
			\gamma(\dif \nu) \dif s + \gO(\Delta_n^2)                                                                                                                                                                                       \\
			= & \Delta_n \int_{\sD}  \mu_{s_n}(\nu)  \log \widehat \mu_{s_n}(\nu) \gamma(\dif \nu) + \dfrac{1}{2} \Delta_n^2 \int_{\sD}  \gL \left( \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)\right)  \gamma(\dif \nu) + \gO(\Delta_n^3).
		\end{aligned}
	\end{equation*}
	Similarly, applying the Dynkin's formula to the following function
	\begin{equation*}
		G_s(\nu, y_{s^-}) = \left(\frac{1}{2\theta} \mu_s(\nu, y_{s^-}) + \left(1-\frac{1}{2\theta}\right)  \mu_{s_n}(\nu, y_{s^-})\right) \log \left( \frac{1}{2\theta} \widehat \mu_s(\nu, y_{s^-}) + \left(1-\frac{1}{2\theta}\right) \widehat \mu_{s_n}(\nu, y_{s^-})\right),
	\end{equation*}
	with $G_0(\nu, y_{s_n}) = \mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n})$ allows us to expand the second integral in~\eqref{eq:III.4} as below
	\begin{equation*}
		\begin{aligned}
			  & \E\left[\int_{s_n}^{s_{n+1}} \int_{\sD} \left(
			\frac{1}{2\theta} \mu_{\rho_n}(\nu) +  \left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu)
			\right) \log \left(
			\frac{1}{2\theta} \widehat \mu_{\rho_n}(\nu) + \left(1-\frac{1}{2\theta}\right) \widehat \mu_{s_n}(\nu)
			\right)  \gamma(\dif \nu) \dif s\right]                \\
			= & \Delta_n \int_\sD G_{s_n}(y_{s_n})\gamma(\dif \nu)
			+ \theta \Delta_n^2 \int_\sD \gL G_{s_n}(y_{s_n})\gamma(\dif \nu)  + \gO(\Delta_n^3),
		\end{aligned}
	\end{equation*}
	where
	\begin{equation*}
		\begin{aligned}
			\gL G_{s_n}(\nu, y_{s_n}) =
			  & \frac{1}{2\theta}  \partial_s \mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n}) + \frac{1}{2\theta} \mu_{s_n}(\nu, y_{s_n}) \frac{1}{2\theta}  \dfrac{\partial_s \widehat \mu_{s_n}(\nu, y_{s_n})}{\widehat \mu_{s_n}(\nu, y_{s_n})} \\
			+ & \frac{1}{2\theta} \int_\sD
			\mu_{s_n}(\nu, y_{s_n} + \nu') \log \left(\frac{1}{2\theta} \widehat \mu_s(\nu, y_{s_n} + \nu') + \left(1-\frac{1}{2\theta}\right) \widehat \mu_{s_n}(\nu, y_{s_n}+ \nu')\right)
			\gamma(\dif \nu')                                                                                                                                                                                                                                    \\
			- & \frac{1}{2\theta} \int_\sD
			\mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n})
			\gamma(\dif \nu')                                                                                                                                                                                                                                    \\
			+ & \left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu, y_{s_n}) \frac{1}{2\theta} \dfrac{ \partial_s \widehat \mu_{s_n}(\nu, y_{s_n})}{\widehat \mu_{s_n}(\nu, y_{s_n})}                                                                                \\
			+ & \left(1-\frac{1}{2\theta}\right) \int_\sD \mu_{s_n}(\nu, y_{s_n} + \nu') \log \left(\frac{1}{2\theta} \widehat \mu_s(\nu, y_{s_n} + \nu') + \left(1-\frac{1}{2\theta}\right) \widehat \mu_{s_n}(\nu, y_{s_n}+ \nu')\right)  \gamma(\dif \nu')    \\
			- & \left(1-\frac{1}{2\theta}\right) \int_\sD  \mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n}) \gamma(\dif \nu')                                                                                                                       \\
			= & \frac{1}{2\theta}  \partial_s \mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n}) + \frac{1}{2\theta} \mu_{s_n}(\nu, y_{s_n}) \dfrac{\partial_s \widehat \mu_{s_n}(\nu, y_{s_n})}{\widehat \mu_{s_n}(\nu, y_{s_n})}                    \\
			+ & \frac{1}{2\theta} \int_\sD
			\mu_{s_n}(\nu, y_{s_n} + \nu') \log \widehat \mu_s(\nu, y_{s_n} + \nu')
			\gamma(\dif \nu')
			+ \left(1-\frac{1}{2\theta}\right) \int_\sD \mu_{s_n}(\nu, y_{s_n} + \nu')  \log  \widehat \mu_s(\nu, y_{s_n} + \nu') \gamma(\dif \nu')                                                                                                              \\
			- & \frac{1}{2\theta}\int_\sD \mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n}) \gamma(\dif \nu') - \left(1-\frac{1}{2\theta}\right) \int_\sD \mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n}) \gamma(\dif \nu').
		\end{aligned}
	\end{equation*}
	This further implies that
	\begin{equation*}
		\begin{aligned}
			\theta \gL G_{s_n}(y_{s_n})
			 & = \frac{1}{2} \gL \left( \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)\right) \\
			 & + \frac{1}{2\theta} \int_\sD
			\left(
			\mu_{s_n}(\nu, y_{s_n} + \nu') \log \widehat \mu_s(\nu, y_{s_n} + \nu')
			-\mu_{s_n}(\nu, y_{s_n}) \log \widehat \mu_{s_n}(\nu, y_{s_n})
			\right)
			\gamma(\dif \nu').
		\end{aligned}
	\end{equation*}
	Comparing the first and second order terms in the two expansions of the two integrals in~\eqref{eq:III.4} above then implies that the term $\mathrm{(III.4)}$ is of at most second order.
\end{remark}

\subsection{Lemmas and Propositions}
\label{app:lemmas}

In this section, we provide the detailed proofs of the lemmas and propositions omitted in the proof of~\cref{thm:trapezoidal} and~\cref{thm:midpoint}.

\paragraph{Error due to the Intensity Estimation.}

Apart from the terms $\mathrm{(I.1)}$ and $\mathrm{(II.1)}$ in the proof of~\cref{thm:trapezoidal} and the term $\mathrm{(III.1)}$ in the proof of~\cref{thm:midpoint},
we also need to bound the error terms $\mathrm{(II.4)}$ in terms of the intensity estimation error, which is given by the following proposition. Notably, the following bound also utilizes the convexity of the loss function and the extrapolation nature of the second step in the $\theta$-trapezoidal method~\eqref{eq:trapezoidal_intensity}.

\begin{proposition}
	For the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, we have the following error bound:
	\begin{equation}
		\begin{aligned}
			\E\left[\mathrm{(II.4)}\right] =
			 & \E\bigg[ \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
			\alpha_1 \mu_{\rho_n}(\nu) \log \widehat \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)
			\right) \gamma(\dif \nu) \dif s                                                                            \\
			 & - \int_{\rho_n}^{s_{n+1}} \int_{\sD} (\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left(
			\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)
			\right)  \gamma(\dif \nu) \dif s \bigg] \lesssim \Delta_n \epsilon_\roII.
		\end{aligned}
		\label{eq:II.4}
	\end{equation}
	\label{prop:II.4}
\end{proposition}

\begin{proof}

	We first define and bound three error terms $\mathrm{(II.4.1)}$, $\mathrm{(II.4.2)}$, and $\mathrm{(II.4.3)}$ with score estimation error (\cref{ass:estimation}) as follows:
	\begin{equation*}
		\begin{aligned}
			\E\left[|\mathrm{(II.4.1)}|\right]=
			         & \E\left[\left| \int_{\rho_n}^{s_{n+1}} \int_{\sD} \alpha_1 \left(\mu_{\rho_n}(\nu) \log \widehat \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}(\nu) \log \widehat \mu_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s \right|\right] \\
			\leq     & \alpha_1 \E \left[\int_{\rho_n}^{s_{n+1}} \int_{\sD} \left| \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}(\nu) \right| \left|\log \widehat \mu_{\rho_n}(\nu)\right| \gamma(\dif \nu) \dif s\right]                                \\
			\lesssim & \E \left[\int_{\rho_n}^{s_{n+1}} \int_{\sD} \left| \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}(\nu) \right| \gamma(\dif \nu) \dif s\right]
			\lesssim \Delta_n \epsilon_\roII,
		\end{aligned}
	\end{equation*}
	Similarly, we also have
	\begin{equation*}
		\E\left[|\mathrm{(II.4.2)}|\right] = \E\left[\left| \int_{\rho_n}^{s_{n+1}} \int_{\sD} \alpha_2 \left(\mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu) - \widehat \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)\right) \gamma(\dif \nu) \dif s \right|\right] \lesssim \Delta_n \epsilon_\roII,
	\end{equation*}
	and
	\begin{equation*}
		\begin{aligned}
			\E\left[|\mathrm{(II.4.3)}|\right] = \E\bigg[\bigg| & \int_{\rho_n}^{s_{n+1}} \int_{\sD} (\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left(
			\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)
			\right)  \gamma(\dif \nu) \dif s                                                                                                                                              \\
			-                                                   & \int_{\rho_n}^{s_{n+1}} \int_{\sD} (\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)) \log \left(
			\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)
			\right)  \gamma(\dif \nu) \dif s \bigg|\bigg] \lesssim \Delta_n \epsilon_\roII.
		\end{aligned}
	\end{equation*}

	The remaining term $\mathrm{(II.4.4)} = \mathrm{(II.4)} - \mathrm{(II.4.1)} - \mathrm{(II.4.2)} - \mathrm{(II.4.3)}$ is then given by
	\begin{equation*}
		\begin{aligned}
			\mathrm{(II.4.4)}
			 & = \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
			\alpha_1 \widehat \mu_{\rho_n}(\nu) \log \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu) \log \widehat \mu_{s_n}(\nu)
			\right) \gamma(\dif \nu) \dif s                                                                                              \\
			 & - \int_{\rho_n}^{s_{n+1}} \int_{\sD} (\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)) \log \left(
			\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat \mu_{s_n}(\nu)
			\right)  \gamma(\dif \nu) \dif s \leq 0,
		\end{aligned}
	\end{equation*}
	where the last inequality follows from Jensen's inequality, \emph{i.e.,}
	\begin{equation*}
		\alpha_1 x \log x - \alpha_2 y \log y \leq (\alpha_1 x - \alpha_2 y) \log (\alpha_1 x - \alpha_2 y),
	\end{equation*}
	for $\alpha_1, \alpha_2 \geq 0$ and $\alpha_1 - \alpha_2 = 1$. Therefore, by summing up the terms above, we have
	\begin{equation*}
		\E\left[\mathrm{(II.4)}\right] \leq \E\left[\mathrm{(II.4.1)} + \mathrm{(II.4.2)} + \mathrm{(II.4.3)} + \mathrm{(II.4.4)}\right] \lesssim \Delta_n \epsilon_\roII,
	\end{equation*}
	and the proof is complete.
\end{proof}


\paragraph{Error Related to the Smoothness of Intensity.}

Below we first present the Dynkin's formula, which is the most essential tool for the proof of the error related to the smoothness of the intensity.

\begin{theorem}[Dynkin's Formula]
	Let $(y_t)_{t\in[0, \tau]}$ be the following process:
	\begin{equation*}
		y_t = y_0 + \int_0^t \int_\sD \nu N[\mu](\dif s, \dif \nu),
	\end{equation*}
	where $N[\mu](\dif s, \dif \nu)$ is a Poisson random measure with intensity $\mu$ of the form $\mu_s(\nu, y_{s^-})$. For any $f \in C^1([0, \tau] \times \sX)$, we define the generator of the process $(y_t)_{t\in[0, \tau]}$ as below
	\begin{equation}
		\gL f_t(y) = \lim_{\tau\to 0^+} \left[\dfrac{f_{t+\tau}(y_{t+\tau}) - f_t(y_t)}{\tau}\bigg| y_t = y\right] = \partial_t f_t(y) + \int_\sD \left( f_t(y + \nu) - f_t(y) \right) \mu_t(\nu, y) \gamma(\dif \nu).
		\label{eq:generator}
	\end{equation}
	Then we have that
	\begin{equation*}
		\E\left[f_t(y_t)\right] = f_0(y_0) + \E\left[\int_0^t \gL f_s(y_s) \dif s\right].
	\end{equation*}
	\label{thm:dynkin}
\end{theorem}

\begin{proof}
	The definition and the form of the generator $\gL$, as well as the Dynkin's formula are all well-known in the literature of jump processes. We refer readers to detailed discussions on these topics in~\citet{oksendal2019stochastic}.

	Here we take $X(t) = (t, y_t)$, $z = (\nu, \xi)$, $\alpha(t, X(t)) = 0$, $\sigma(t, X(t)) = 0$, $\gamma(t, X(t^-), z) = \nu \vone_{0 \leq \xi \leq \mu_t(\nu, y_{t^-})}$ in the statement of Thm.~1.19 in~\citet{oksendal2019stochastic} and replace the compensated Poisson random measure $\widetilde N(dt, dz)$ with the Poisson random measure $N(\dif s, \dif \nu, \dif \xi)$ defined as~\cref{rem:construction}.
	Then we are allowed to use the ordinary Poisson random measure instead of the compensated one since we are working with a finite measure $\gamma(\dif \nu)$.

	From Thm.~1.22 in~\citet{oksendal2019stochastic}, we have that
	\begin{equation*}
		\begin{aligned}
			\gL f_t(y) & = \partial_t f_t(y) + \int_\sD \int_\R \left( f_t(y + \nu \vone_{0 \leq \xi \leq \mu_t(\nu, y)}) - f_t(y) \right) \gamma(\dif \nu) \dif \xi \\
			           & = \partial_t f_t(y) + \int_\sD \left( f_t(y + \nu) - f_t(y) \right) \mu_t(\nu, y) \gamma(\dif \nu),
		\end{aligned}
	\end{equation*}
	and the proof is complete.
\end{proof}

In many cases below, we will need the following first-order expansion of the expectation of the function $f_t(y_t)$ by assuming the second-order smoothness of the function $f$.

\begin{corollary}
	Suppose that the process $(y_t)_{t\in[0, \tau]}$ and the generator $\gL$ are defined as in~\cref{thm:dynkin}.
	If we further assume that $f \in C^2([0, \tau] \times \sX)$, then it holds that
	\begin{equation*}
		\E\left[f_t(y_t)\right] = f_0(y_0) + t \gL f_0(y_0) + \gO(t^2).
	\end{equation*}
	\label{cor:dynkin_first_order}
\end{corollary}

\begin{proof}
	We expand the function $f_s(y_s)$ from $t = 0$ as follows
	\begin{equation*}
		\begin{aligned}
			\E\left[f_t(y_t)\right] = & f_0(y_0) + \E\left[\int_0^t \gL f_s(y_s) \dif s\right]                                                            \\
			=                         & f_0(y_0) + \E\left[\int_0^t \gL \left(f_0(y_0) + \int_0^s \gL f_\sigma(y_\sigma) \dif \sigma\right) \dif s\right] \\
			=                         & f_0(y_0) + \gL f_0(y_0) t + \E\left[\int_0^t \int_0^s \gL^2 f_\sigma(y_\sigma) \dif \sigma \dif s\right],
		\end{aligned}
	\end{equation*}
	where $\gL^2$ is the second-order generator of the process $(y_t)_{t\in[0, \tau]}$ defined as follows
	\begin{equation*}
		\begin{aligned}
			\gL^2 f_\sigma(y) & = \gL \left(\partial_\sigma f_\sigma(y) +  \int_\sD \left( f_\sigma(y + \nu) - f_\sigma(y) \right) \mu_\sigma( \nu) \gamma(\dif \nu) \right)                               \\
			                  & = \partial_\sigma^2 f_\sigma(y) + 2\int_\sD \left( \partial_\sigma f_\sigma(y + \nu) - \partial_\sigma f_\sigma(y) \right)\mu_\sigma( \nu) \gamma(\dif \nu)                \\
			                  & + \int_\sD \left( f_\sigma(y + \nu) - f_\sigma(y) \right) \partial_\sigma \mu_\sigma( \nu) \gamma(\dif \nu)                                                                \\
			                  & + \int_\sD \int_\sD \big(f_\sigma(y+\nu+\nu') - f_\sigma(y+\nu') - f_\sigma(y+\nu) + f_\sigma(y) \big) \mu_\sigma(\nu)\mu_\sigma(\nu') \gamma(\dif \nu) \gamma(\dif \nu'),
		\end{aligned}
	\end{equation*}
	which is bounded uniformly by a constant based on the assumption on the smoothness of the function $f$ up to the second order and the boundedness of the measure $\gamma(\dif \nu)$. Therefore, the second order term above is of magnitude $\gO(t^2)$ and the proof is complete.
\end{proof}

The following lemma provides a general recipe for bounding a combination of errors, which resembles standard analysis performed for numerical quadratures. In fact, the following lemma can be easily proved by Taylor expansion when the process $(y_t)_{t\in[0, \tau]}$ is constant, \emph{i.e.}, $y_t \equiv y$. ~\cref{cor:dynkin_first_order} offers an analogous approach to perform the expansion when the process $(y_t)_{t\in[0, \tau]}$ is not constant.

\begin{lemma}
	For any function $f \in C^2([0, \tau] \times \sX)$ and the true backward process $(y_t)_{t\in[0, \tau]}$ defined in~\eqref{eq:backward_integral}, it holds that
	\begin{equation*}
		\left|\E\left[ \int_0^{\theta\tau} f_0(y_0) \dif s + \int_{\theta \tau}^\tau \left(\alpha_1 f_{\theta \tau}(y_{\theta \tau}) - \alpha_2 f_0(y_0) \right)\dif s - \int_0^\tau f_s(y_s) \dif s \right]\right| \lesssim \tau^3.
	\end{equation*}
	\label{lem:integral_error}
\end{lemma}

\begin{proof}
	Let $\mathcal L$ be the generator defined in~\cref{thm:dynkin}. By applying the Dynkin's formula (\cref{thm:dynkin} and~\cref{cor:dynkin_first_order}) to the function $f_t(y_t)$ and plugging in the expression of the generator $\mathcal{L}$, we have that
	\begin{equation*}
		\begin{aligned}
			  & \E\left[  \int_0^{\theta\tau} f_0(y_0) \dif s - \alpha_2 \int_{\theta \tau}^\tau f_0(y_0) \dif s + \alpha_1 \int_{\theta \tau}^\tau f_{\theta \tau}(y_{\theta \tau}) \dif s - \int_0^\tau f_s(y_s) \dif s \right] \\
			= & \theta \tau f_0(y_0) - \alpha_2 (1 - \theta) \tau f_0(y_0) + \alpha_1 (1-\theta) \tau \left(f_0(y_0) + \theta \tau \gL f_0(y_0)\right) - \int_0^\tau \left(f_0(y_0) + s\gL f_0(y_0)\right) \dif s + \gO(\tau^3)   \\
			= & \left(\theta - \alpha_2 (1 - \theta) + \alpha_1 (1-\theta) - 1\right) \tau f_0(y_0) + \alpha_1 (1-\theta) \theta \tau^2 \gL f_0(y_0) - \dfrac{\tau^2}{2} \gL f_0(y_0) + \gO(\tau^3),
		\end{aligned}
	\end{equation*}
	which is of the order $\gO(\tau^3)$ by noticing that
	\begin{equation*}
		\begin{aligned}
			\theta -\alpha_2 (1-\theta) + \alpha_1 (1-\theta) -1 & = \left(\dfrac{1}{2\theta(1-\theta)} - \dfrac{\theta^2 + (1-\theta)^2}{2\theta(1-\theta)}\right) (1-\theta) - (1 - \theta) =0 \\
			\alpha_1 (1-\theta) \theta - \dfrac{1}{2}            & = \dfrac{1}{2\theta(1-\theta)} (1-\theta) \theta  - \dfrac{1}{2} = 0,
		\end{aligned}
	\end{equation*}
	and the proof is complete.
\end{proof}

Then we are ready to bound some of the error terms in the proof of~\cref{thm:trapezoidal} with~\cref{lem:integral_error}.

\begin{corollary}
	For the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, we have the following error bound:
	\begin{equation*}
		\begin{aligned}
			  & \left|\E\left[\mathrm{(I.2)} + \mathrm{(II.2)}\right]\right|                                  \\
			= & \bigg|\E\bigg[
			\int_{s_n}^{s_{n+1}} \int_{\sD}\left(
			\mu_s(\nu) \log \mu_s(\nu) - \mu_s(\nu)
			\right) \gamma(\dif \nu) \dif s                                                                   \\
			  & \ - \int_{s_n}^{\rho_n} \int_{\sD}\left(  \mu_{s_n}(\nu) \log \mu_{s_n}(\nu) + \mu_{s_n}(\nu)
			\right) \gamma(\dif \nu) \dif s                                                                   \\
			  & \ - \int_{\rho_n}^{s_{n+1}} \int_{\sD} \big(
			\alpha_1 (\mu_{\rho_n}(\nu) \log \mu_{\rho_n}(\nu) - \mu_{\rho_n}(\nu)) - \alpha_2 (\mu_{s_n}(\nu) \log \mu_{s_n}(\nu) - \mu_{s_n}(\nu))
			\big) \gamma(\dif \nu) \dif s
			\bigg] \bigg| \lesssim \Delta_n^3.
		\end{aligned}
	\end{equation*}
	\label{cor:I.2_II.2}
\end{corollary}

\begin{proof}
	The bound is obtained by applying~\cref{lem:integral_error} with $f$ being the function $$f_s(y_s) = \int_{\sD} \mu_s(\nu) \log \mu_s(\nu) \gamma(\dif \nu),$$
	Strictly speaking, $f_s(y_s)$ is actually in the form of $f_s(y_{s^-})$, but the argument can be easily extended to this case by assuming time continuity of the function $f$.
\end{proof}

\begin{corollary}
	For the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, we have the following error bound:
	\begin{equation*}
		\begin{aligned}
			  & \left|\E\left[\mathrm{(I.4)} + \mathrm{(II.6)}\right]\right| \\
			= & \bigg|\E\bigg[
			\int_{s_n}^{\rho_n} \int_{\sD} \mu_{s_n}(\nu) \log \left(
			\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)
			\right) \gamma(\dif \nu) \dif s                                  \\
			  & \ + \int_{\rho_n}^{s_{n+1}} \int_{\sD}
			(\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left( \alpha_1 \widehat \mu_{\rho_n}^*(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right)
			\gamma(\dif \nu) \dif s                                          \\
			  & \ - \int_{s_n}^{s_{n+1}} \int_{\sD} \mu_s(\nu) \log \left(
			\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)
			\right) \gamma(\dif \nu) \dif s
			\bigg] \bigg| \lesssim \Delta_n^3.
		\end{aligned}
	\end{equation*}
	\label{cor:I.4_II.6}
\end{corollary}

\begin{proof}
	Note that the intermediate process $y_s^*$ defined in~\eqref{eq:intermediate_process} is driven by a Poisson random measure that is independent of the Poisson random measure driving the process $y_s$ within the interval $(s_n, s_{n+1}]$. Therefore, the error bound is obtained by
	\begin{enumerate}[label=(\arabic*)]
		\item Taking the expectation w.r.t. the intermediate process $y_s^*$ and thus the intermediate intensity $\widehat \mu_s^*$, and
		\item Then applying~\cref{lem:integral_error} with $f$ being the following function $$f_s(y_s) = \int_{\sD} \mu_s(\nu) \E\left[\log \left(\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)\right)\right]\gamma(\dif \nu).$$
	\end{enumerate}
	The result follows directly.
\end{proof}


Now we turn to the error term $\mathrm{(III.6)}$ in~\cref{thm:midpoint}, for which we need the following variant of~\cref{lem:integral_error}.


\begin{lemma}
	For any function $f \in C^2([0, \tau] \times \sX)$ and the true backward process $(y_t)_{t\in[0, \tau]}$ defined in~\eqref{eq:backward_integral}, it holds that
	\begin{equation*}
		\left|\E\left[ \int_0^\tau \left(\left(1-\frac{1}{2\theta}\right) f_0(y_0) + \frac{1}{2\theta}f_{\theta \tau}(y_{\theta \tau}) \right)\dif s - \int_0^\tau f_s(y_s) \dif s \right]\right| \lesssim \tau^3.
	\end{equation*}
	\label{lem:integral_error_var}
\end{lemma}

\begin{proof}
	The proof is similar to that of~\cref{lem:integral_error}. Specifically, we let $\mathcal L$ be the generator defined in~\cref{thm:dynkin}, apply the Dynkin's formula (\cref{thm:dynkin} and~\cref{cor:dynkin_first_order}) to the function $f_t(y_t)$ and plug in the expression of the generator $\mathcal{L}$, which yields
	\begin{equation*}
		\begin{aligned}
			  & \E\left[  \int_0^\tau \left(\left(1-\frac{1}{2\theta}\right) f_0(y_0) + \frac{1}{2\theta} f_{\theta \tau}(y_{\theta \tau}) \right)\dif s - \int_0^\tau f_s(y_s) \dif s \right]                                                  \\
			= & \left(1-\frac{1}{2\theta}\right) \tau f_0(y_0) + \frac{1}{2\theta}  \int_0^\tau \left(f_0(y_0) + \theta \tau \gL f_0(y_0)\right) \dif s - \int_0^\tau \left(f_0(y_0) + s\gL f_0(y_0)\right) \dif s + \gO(\tau^3) = \gO(\tau^3),
		\end{aligned}
	\end{equation*}
	as desired.
\end{proof}


\begin{corollary}
	For the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, we have the following error bound:
	\begin{equation*}
		\begin{aligned}
			  & \left|\E\left[\mathrm{(III.2)}\right]\right|                                                                                     \\
			= & \bigg|\E\bigg[
			\int_{s_n}^{s_{n+1}} \int_{\sD} \left(\mu_s(\nu)\log \mu_s(\nu)- \mu_s(\nu)\right) \gamma(\dif \nu) \dif s                           \\
			  & - \int_{s_n}^{s_{n+1}} \int_{\sD} \left(\left(1-\frac{1}{2\theta}\right)\left(\mu_{s_n}(\nu)\log \mu_{s_n}(\nu)-\mu_{s_n}\right)
			+\frac{1}{2\theta}\left(\mu_{\rho_n}(\nu)\log\mu_{\rho_n}(\nu)-\mu_{\rho_n}(\nu)\right)\right) \gamma(\dif \nu) \dif s
			\bigg] \bigg| \lesssim \Delta_n^3.
		\end{aligned}
	\end{equation*}
	\label{cor:III.2}
\end{corollary}

\begin{proof}
	By applying~\cref{lem:integral_error_var} with $f$ being the function
	\begin{equation*}
		f_s(y_s) = \int_{\sD} \mu_s(\nu) \log \mu_s(\nu) \gamma(\dif \nu),
	\end{equation*}
	we have that the result follows directly.
\end{proof}


\begin{corollary}
	For any $n\in[0:N-1]$ and the corresponding interval $(s_n, s_{n+1}]$, we have the following error bound:
	\begin{equation*}
		\begin{aligned}
			  & \left|\E\left[\mathrm{(III.6)}\right]\right|                                                                                                                                                                                                                                 \\
			= & \bigg|\E\bigg[
			\int_{s_n}^{s_{n+1}} \int_{\sD} \left(\left(1-\frac{1}{2\theta}\right) \mu_{s_n}(\nu) + \frac{1}{2\theta}\mu_{\rho_n}(\nu)\right)\log\left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s \\
			  & \ - \int_{s_n}^{s_{n+1}} \int_{\sD} \mu_s(\nu)\log\left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right) \gamma(\dif \nu) \dif s
			\bigg] \bigg| \lesssim \Delta_n^3.
		\end{aligned}
	\end{equation*}
	\label{cor:III.6}
\end{corollary}

\begin{proof}
	Following the arguments in the proof of~\cref{cor:I.4_II.6}, the error bound is obtained by first taking the expectation w.r.t. the intermediate process $y_s^*$ and thus the intermediate intensity $\widehat \mu_s^*$, and then applying~\cref{lem:integral_error_var} with $f$ being the function
	\begin{equation*}
		f_s(y_s) = \int_{\sD}  \mu_s(\nu) \log\left(\left(1-\frac{1}{2\theta}\right) \widehat\mu_{s_n}(\nu) + \frac{1}{2\theta}\widehat\mu^*_{\rho_n}(\nu)\right) \gamma(\dif \nu),
	\end{equation*}
	as desired.
\end{proof}


\paragraph{Error involving the Intermediate Process.}

\begin{proposition}
	For the interval $(s_n, s_{n+1}]$ with $n\in[0:N-1]$, we have the following error bound:
	\begin{equation*}
		\E\left[\mathrm{(II.3)}\right] = \E\left[\int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
		\widehat \mu_{\rho_n}^*(\nu) - \widehat \mu_{\rho_n}(\nu)
		\right) \gamma(\dif \nu) \dif s \right]
		\lesssim \Delta_n^3 + \Delta_n^2 \epsilon_\roII.
	\end{equation*}
	\label{prop:II.3}
\end{proposition}

\begin{proof}

	First, we rewrite the error term $\mathrm{(II.3)}$ as
	\begin{equation}
		\begin{aligned}
			\E\left[\mathrm{(II.3)}\right]
			 & = \E\left[\int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
			\widehat \mu_{\rho_n}^*(\nu) - \widehat \mu_{\rho_n}(\nu)
			\right) \gamma(\dif \nu) \dif s \right]                \\
			 & \lesssim \int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
			\E\left[\widehat \mu_{\rho_n}^*(\nu)\right]
			- \E\left[\widehat \mu_{\rho_n}(\nu)\right]
			\right) \gamma(\dif \nu) \dif s.
		\end{aligned}
		\label{eq:II.3.2.2}
	\end{equation}

	Then we expand the integrand by applying the Dynkin's formula (\cref{thm:dynkin} and~\cref{cor:dynkin_first_order}) to the function $\widehat \mu_s(\nu)$ w.r.t. the intermediate process $(y_s^*)_{s\in[s_n, \rho_n]}$ and the process $(y_s)_{s\in[s_n, \rho_n]}$ respectively as follows
	\begin{equation*}
		\begin{aligned}
			  & \E\left[\widehat \mu_{\rho_n}^*(\nu)\right]
			- \E\left[\widehat \mu_{\rho_n}(\nu)\right]                                                       \\
			= & \E\left[\widehat \mu_{s_n}(\nu)
				+ \gL^* \widehat \mu_{s_n}(\nu) \Delta_n + \gO(\Delta_n^2)\right]
			- \E\left[\widehat \mu_{s_n}(\nu) + \gL \widehat \mu_{s_n}(\nu) \Delta_n + \gO(\Delta_n^2)\right] \\
			= & \E\left[(\gL^* - \gL) \widehat \mu_{s_n}(\nu) \Delta_n\right] + \gO(\Delta_n^2),
		\end{aligned}
	\end{equation*}
	where the generators $\gL^*$ and $\gL$ are defined as in~\eqref{eq:generator} w.r.t. the processes $(y_s^*)_{s\in[s_n, \rho_n]}$ and $(y_s)_{s\in[s_n, \rho_n]}$, respectively, \emph{i.e.}, for any function $f \in C^1([s_n, \rho_n] \times \sX)$, we have
	\begin{equation}
		\label{eqn:different generators}
		\begin{aligned}
			\gL^* f_s(y) & = \partial_s f_s(y) + \int_\sD \left( f_s(y + \nu) - f_s(y) \right) \widehat \mu_{s_n}(\nu) \gamma(\dif \nu), \\
			\gL f_s(y)   & = \partial_s f_s(y) + \int_\sD \left( f_s(y + \nu) - f_s(y) \right) \mu_s(\nu) \gamma(\dif \nu).
		\end{aligned}
	\end{equation}
	Therefore, for the term $\E\left[\left|(\gL^* - \gL) \widehat \mu_{s_n}(\nu)\right|\right]$ evaluated at $s = s_n$, we have
	\begin{equation}
		\label{eqn:bound on difference between two generators}
		\begin{aligned}
			\E\left[\left|(\gL^* - \gL) \widehat \mu_{s_n}(\nu)\right|\right]
			 & = \E \left[\left|\int_\sD \left( \widehat \mu_{s_n}(y + \nu) - \widehat \mu_{s_n}(y) \right) \left(\widehat \mu_{s_n}(\nu) - \mu_{s_n}(\nu)\right) \gamma(\dif \nu)\right| \right] \\
			 & \lesssim \E \left[\int_\sD \left| \widehat \mu_{s_n}(\nu) - \mu_{s_n}(\nu) \right| \gamma(\dif \nu) \right] \lesssim \epsilon_\roII,
		\end{aligned}
	\end{equation}
	where we used the assumption on the estimation error (\cref{ass:estimation}) in the last inequality. Then we can further reduce~\eqref{eq:II.3.2.2} to
	\begin{equation*}
		\int_{\rho_n}^{s_{n+1}} \int_{\sD} \left(
		\E\left[\widehat \mu_{\rho_n}^*(\nu)\right]
		- \E\left[\widehat \mu_{\rho_n}(\nu)\right]
		\right) \gamma(\dif \nu) \dif s
		\lesssim  \int_{\rho_n}^{s_{n+1}} \left(\epsilon_\roII \Delta_n + \gO(\Delta_n^2)\right) \dif s \lesssim \epsilon_\roII \Delta_n^2 + \Delta_n^3,
	\end{equation*}
	and the proof is complete.
\end{proof}

\begin{corollary}
	For the interval $(s_n, s_{n+1}]$ for $n\in[0:N-1]$, we have the following error bound:
	\begin{equation*}
		\begin{aligned}
			\E\left[\mathrm{(II.5)}\right] = & \E\bigg[\int_{\rho_n}^{s_{n+1}} \int_{\sD}
			(\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left( \alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right)
			\gamma(\dif \nu) \dif s                                                       \\
			                                 & \ - \int_{\rho_n}^{s_{n+1}} \int_{\sD}
			(\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)) \log \left( \alpha_1 \widehat \mu_{\rho_n}^*(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu) \right)
			\gamma(\dif \nu) \dif s \bigg]
			\lesssim \Delta_n^3 + \Delta_n^2 \epsilon_\roII.
		\end{aligned}
	\end{equation*}
	\label{cor:II.5}
\end{corollary}



\begin{proof}
	Since the two integrands in $\mathrm{(II.5)}$ only differ by replacing $\widehat \mu_{\rho_n}^*(\nu)$ with $\widehat \mu_{\rho_n}(\nu)$, we have the following upper bound by using the assumption on the boundedness of the intensities (\cref{ass:smoothness} (II))
	\begin{equation}
		\label{eqn: II.5 bound intermediate step one}
		\begin{aligned}
			\E\left[\mathrm{(II.5)}\right] \lesssim & \E\left[\int_{\rho_n}^{s_{n+1}} \int_{\sD}
			\left|\alpha_1 \mu_{\rho_n}(\nu) - \alpha_2 \mu_{s_n}(\nu)\right| \dfrac{1}{\alpha_1 \widehat \mu_{\rho_n}(\nu) - \alpha_2 \widehat\mu_{s_n}(\nu)} \alpha_1 \left|\widehat \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}^*(\nu)\right| \gamma(\dif \nu) \dif s\right] \\
			\lesssim                                & \E\left[\int_{\rho_n}^{s_{n+1}} \int_{\sD} \left|\widehat \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}^*(\nu)\right| \gamma(\dif \nu) \dif s \right]
			\lesssim \Delta_n\E \left[\int_\sD \left| \widehat \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}^*(\nu) \right| \gamma(\dif \nu) \right]                                                                                                                              \\
			=                                       & \Delta_n \int_\sD \E \left[\left| \widehat \mu_{\rho_n}(\nu) - \widehat \mu_{\rho_n}^*(\nu) \right|\right] \gamma(\dif \nu)
		\end{aligned}
	\end{equation}
	Applying the same arguments as in~\cref{prop:II.3}, which uses the generators $\gL$ and $\gL^\ast$ defined in~\eqref{eqn:different generators}, we can bound the RHS above as follows
	\begin{equation}
		\label{eqn: II.5 bound intermediate step two}
		\begin{aligned}
			\E\left[\left|\widehat\mu^*_{\rho_n}(\nu) - \widehat \mu_{\rho_n}(\nu)\right|\right] = & \E\left[\left|\left(\widehat \mu_{s_n}(\nu)
				+ \gL^* \widehat \mu_{s_n}(\nu) \Delta_n + \gO(\Delta_n^2)\right)
			-\left(\widehat \mu_{s_n}(\nu) + \gL \widehat \mu_{s_n}(\nu) \Delta_n + \gO(\Delta_n^2)\right)\right|\right]                                                                                                                            \\
			\lesssim                                                                               & \Delta_n\E\left[\left|(\gL^* - \gL) \widehat \mu_{s_n}(\nu)\right|\right] + \gO(\Delta_n^2) \lesssim \Delta_n \epsilon_\roII + \gO(\Delta_n^2)
		\end{aligned}
	\end{equation}
	where the last inequality follows from~\eqref{eqn:bound on difference between two generators}. Substituting~\eqref{eqn: II.5 bound intermediate step two} into~\eqref{eqn: II.5 bound intermediate step one} then yields the desired upper bound.
\end{proof}

\begin{proposition}
	For the interval $(s_n, s_{n+1}]$ with $n\in[0:N-1]$, we have the following error bound:
	\begin{equation*}
		\begin{aligned}
			\E\left[\mathrm{(I.3)}\right]
			 & = \E\left[\int_{s_n}^{\rho_n} \int_{\sD}\left(
				\mu_s(\nu) - \mu_{s_n}(\nu) \right) \left( \log \left(\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu) \right) - \log \widehat \mu_{s_n}(\nu)
				\right) \gamma(\dif \nu) \dif s\right]
			\lesssim \Delta_n^3\epsilon_\roII + \Delta_n^4.
		\end{aligned}
	\end{equation*}
	\label{prop:I.3}
\end{proposition}

\begin{proof}
	First, we observe by Dynkin's formula (\cref{thm:dynkin}) that
	\begin{equation*}
		\E\left[|\mu_s(\nu) - \mu_{s_n}(\nu)|\right] = \E\left[\left|\int_{s_n}^{\rho_n} \gL \mu_{s_n} \dif s + \gO(\Delta_n^2) \right|\right] \lesssim \Delta_n,
	\end{equation*}
	Secondly, applying the given assumption (\cref{ass:smoothness} (II)) on the boundedness of the intensities yields
	\begin{equation}
		\begin{aligned}
			\E\left[\left|
				\log \left(\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu) \right) - \log \widehat \mu_{s_n}(\nu)
			\right| \right]   \lesssim & \dfrac{1}{\widehat \mu_{s_n}(\nu)}\E\left[\left|
				\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu) - \widehat \mu_{s_n}(\nu)
			\right|\right]                                                                                                                                                        \\
			\lesssim                   & \E\left[\left|
				\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu) - \widehat \mu_{s_n}(\nu)
			\right|\right]                                                                                                                                                        \\
			\leq                       & \E\left[\left|\widehat\mu^*_{\rho_n}(\nu) - \widehat \mu_{\rho_n}(\nu)\right|\right] \lesssim \Delta_n \epsilon_\roII + \gO(\Delta_n^2),
		\end{aligned}
		\label{eq:I.3.1}
	\end{equation}
	where the last inequality follows from~\eqref{eqn: II.5 bound intermediate step two} proved above. Therefore, we may further deduce that
	\begin{equation*}
		\begin{aligned}
			\E\left[\mathrm{(I.3)}\right] \leq & \int_{s_n}^{\rho_n} \int_{\sD} \E\left[\left| \mu_s(\nu) - \mu_{s_n}(\nu) \right|\right] \E\left[\left|\log \left(\alpha_1\widehat\mu^*_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)\right) -  \log \left(\alpha_1\widehat\mu_{\rho_n}(\nu) - \alpha_2\widehat\mu_{s_n}(\nu)\right) \right|\right] \gamma(\dif \nu) \dif s \\
			\lesssim                           & \Delta_n^2 (\Delta_n\epsilon_\roII + \Delta_n^2) \lesssim \Delta_n^3\epsilon_\roII + \Delta_n^4,
		\end{aligned}
	\end{equation*}
	where the first inequality is due to the independency of $y_s$ and $y_s^*$ for $s\in[s_n, \rho_n]$, and the proof is complete.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Details of Numerical Experiments}
\label{app:exp}
In this section, we describe in detail the setting for each numerical experiment. In \cref{app:practical_rk2}, we discuss a revision of $\theta$-RK-2 (\cref{alg:midpoint}) for a more practical and better-performing implementation in real cases. In \cref{app:toy_exp,app:text_exp,app:image_exp}, we present additional numerical results for the 15-dimension toy model, text generation, and image generation respectively.

\subsection{Practical Implementation of $\theta$-Runge Kutta-2}

As is mentioned in \cref{thm:midpoint}, when we fix $\theta \in (0, \frac{1}{2}]$ for the $\theta$-RK-2 method, the algorithm also enjoys a second order convergence in theory conditioned on the fact that the extrapolated transition rate matrix $(1-\tfrac{1}{2\theta} ) \widehat\mu_{s_n} + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_n}$ is everywhere non-negative. In practice, we force this condition to be true by only taking the positive parts of this rate matrix, leading to the revised practical implementation in \cref{alg:midpoint-practical}.

By introducing this modification, we manage to extend the $\theta$ range to $(0,1]$, the same as the $\theta$-Trapezoidal algorithm. In the following sections, we will also present results for $\theta$-RK-2, and it is realized by implementing the version of \cref{alg:midpoint-practical} with a feasible $\theta \in (0,1]$.

\label{app:practical_rk2}
\begin{algorithm}[!ht]
	\caption{Practical Implementation of $\theta$-Runge Kutta-2 Algorithm}
	\label{alg:midpoint-practical}
	\Indm
	\KwIn{$\widehat y_0 \sim q_0$, $\theta \in (0,1]$, time discretization $(s_n, \rho_n)_{n\in[0:N-1]}$, $\widehat \mu$, $\widehat \mu^*$ as defined in~\cref{prop:integral_formulation_midpoint}.}
	\KwOut{A sample $\widehat y_{s_N}\sim \widehat q_{t_N}^\RK$.}
	\Indp
	\For{$n = 0$ \KwTo $N-1$}{
		$\displaystyle\widehat y^*_{\rho_n} \leftarrow \widehat y_{s_n} + \sum_{\nu \in \sD} \nu
			\gP\left(\widehat\mu_{s_n}(\nu)\theta\Delta_n\right)$\;
		$\displaystyle\widehat y_{s_{n+1}} \leftarrow \widehat y_{s_n} + \sum_{\nu \in \sD}\nu \gP\left(
			\left((1-\tfrac{1}{2\theta} ) \widehat\mu_{s_n} + \tfrac{1}{2\theta}\widehat\mu^*_{\rho_n}\right)_{+}(\nu) \Delta_n\right)$\;
	}
\end{algorithm}



\subsection{15-Dimensional Toy Model}
\label{app:toy_exp}

We first derive the closed-form formula of the marginal distributions $\vp_t$ in this model. Recall that the state space $\sX=\{1,2,...,d\}$ with $d=15$, and the initial distribution is $\vp_0\in\Delta^d$. The rate matrix at any time is $\mQ=\frac{1}{d}\mE-\mI$. By solving \eqref{eq:forward}, we see that
$$\vp_t={\rm e}^{t\mQ}\vp_0=\left(\frac{1-{\rm e}^{-t}}{d}\mE+{\rm e}^{-t}\mI\right)\vp_0,$$
and therefore $\vp_t$ converges to the uniform distribution $\vp_\infty=\frac{1}{d}{\bf 1}$ as $t\to\infty$. The formula of $\vp_t$ directly yields the scores $\vs_t(x)=\frac{\vp_t}{p_t(x)}$.

During inference, we initialize at the uniform distribution $\vq_0=\vp_\infty$ and run from time $0$ to $T=12$.
The truncation error of this choice of time horizon is of the magnitude of $10^{-12}$ reflected by $\KL(\vp_T \| \vp_\infty)$, and therefore negligible.
The discrete time points form an arithmetic sequence.

We generate $10^6$ samples for each algorithm and use \texttt{np.bincount} to obtain the empirical distribution $\widehat{\vq}_T$ as the output distribution. Finally, the KL divergence is computed by
$$
	\KL(\vp_0\|\widehat{\vq}_T)=\sum_{i=1}^d p_0(i)\log\frac{p_0(i)}{\widehat{q}_T(i)}.
$$
We also perform bootstrapping for 1000 times to obtain the 95\% confidence interval of the KL divergence, the results are shown by the shaded area in~\cref{fig:toy_model}. The fitted lines are obtained by standard linear regression on the log-log scale with the slopes marked beside each line in~\cref{fig:toy_model}.

\subsection{Text Generation}
\label{app:text_exp}

For text generation, we use the small version of RADD \citep{ou2024your} checkpoint\footnote{\url{https://huggingface.co/JingyangOu/radd-lambda-dce}} trained with $\lambda$-DCE loss. We choose an early stopping time $\delta = 10^{-3}$ for a stable numerical simulation. Since RADD is a masked discrete diffusion model, we can freely choose the noise schedule $\sigma(t)$ used in the inference process. We consider the following log-linear noise schedule used in the model training,
\begin{align}
	\sigma(t) = \frac{1 - \eps}{1 - (1 - \eps)t}, \quad \bar \sigma (t) = \int_{0}^{t} \sigma(s) \mathrm{d} s = -\log ( 1- (1 - \eps)t)
	\label{eq:loglinear}
\end{align}
where we choose $\eps = 10^{-3}$.

The score function $\vs_{\theta}(\vx_t, t)$ used for computing the transition rate matrix can be computed from the RADD score model $\vp_{\theta}$ using the following formula from \citet{ou2024your},
\begin{align}
	\vs^{\theta}_t(\vx_t) = \frac{{\rm e}^{- \bar \sigma(t)}}{1 - {\rm e}^{-\bar \sigma(t)}} \vp_{\theta}(\vx_t),
	\label{eq:mask_score}
\end{align}
where the model $\vp_{\theta}$ is trained to approximate the conditional distribution of the masked positions given all unmasked positions. More specifically, let $d$ be the length of the sequence and $\{1,2,...,S\}$ be the vocabulary set (not including the mask token). Then given a partially masked sequence $\vx=(x^1,...,x^d)$, the model $\vp_\theta(\vx)$ outputs a $d\times S$ matrix whose $(\ell,s)$ element approximates $\P_{\mX\sim\vp_{\rm data}}(x^\ell=s|\mX^{\rm UM}=\vx^{\rm UM})$ when $x^\ell$ is mask, and is $\vone_{X^\ell,s}$ if otherwise. Here, $\vx^{\rm UM}$ represents the unmasked portion of the sequence $\vx$.

We adopt a uniform discretization of the time interval $(\delta, 1]$. For $\theta$-RK-2 and $\theta$-Trapezoidal, we pick $\theta = \frac{1}{2}$. We compare our proposed $\theta$-RK-2 and $\theta$-Trapezoidal with the Euler method, Tweedie $\tau$-leaping, $\tau$-leaping, and we present full results across all NFEs ranging from $16$ to $1024$ in \cref{tab:perplexity_full}. For each method, we generate $1024$ samples with it and compute the averaged perplexities. All the experiments are run on a single NVIDIA A100 GPU.

\begin{table}[ht]
	\caption{Generative perplexity of texts generated by different sampling algorithms. Lower values are better, with the best in \textbf{bold}. }
	\vskip -0.1in
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccccccr}
				\toprule
				Sampling Methods       & NFE $=16$                   & NFE $=32$                  & NFE $=64$                  & NFE $=128$                 & NFE $=256$                 & NFE $=512$                 & NFE $=1024$                \\
				\midrule
				Euler                  & $\leq 277.962$              & $\leq 160.586$             & $\leq 111.597$             & $\leq 86.276$              & $\leq 68.092$              & $\leq 55.622$              & $\leq 44.686$              \\
				Tweedie $\tau$-leaping & $\leq 277.133$              & $\leq 160.248$             & $\leq 110.848$             & $\leq 85.738$              & $\leq 70.102$              & $\leq 55.194$              & $\leq 44.257$              \\
				$\tau$-leaping         & $\leq 126.835$              & $\leq 96.321$              & $\leq 69.226$              & $\leq 52.366$              & $\leq 41.694 $             & $\leq 33.789$              & $\leq 28.797$              \\
				$\theta$-RK-2          & $\leq 127.363$              & $\leq 109.351$             & $\leq 86.102$              & $\leq 64.317$              & $\leq 49.816 $             & $\leq 40.375$              & $\leq 33.971$              \\
				$\theta$-Trapezoidal   & $\boldsymbol{\leq 123.585}$ & $\boldsymbol{\leq 89.912}$ & $\boldsymbol{\leq 66.549}$ & $\boldsymbol{\leq 49.051}$ & $\boldsymbol{\leq 39.959}$ & $\boldsymbol{\leq 32.456}$ & $\boldsymbol{\leq 27.553}$ \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\label{tab:perplexity_full}
\end{table}

From the table, we observe that $\theta$-Trapezoidal consistently outperforms all other approaches and generates samplers with better perplexities across all NFEs. We also noticed that both the Euler method and Tweedie $\tau$-leaping share a similar performance, which is beaten by a large margin by $\theta$-RK-2 and $\tau$-leaping.

\begin{figure}[!ht]
	\vskip -0.1in
	\begin{center}
		\centerline{\includegraphics[width=0.5 \columnwidth]{ theta_ablation_rk2.pdf}}
		\vskip -0.2in
		\caption{Sampling quality v.s. $\theta\in(0,1]$ in $\theta$-RK-2 algorithm. Sampling quality is quantified through FID. }
		\label{fig:theta_rk2}
	\end{center}
\end{figure}

In~\cref{fig:theta_rk2}, we present the performance of $\theta$-RK-2 with respect to different choices of $\theta$ at NFE $32$ and $64$. We observe that the performance of $\theta$-RK-2 has a flat landscape around the optimal $\theta$ choices, which falls in the range $[0.15, 0.4]$. In general, as is evident from the curve, the method performs better when using extrapolation to compute the transition rate matrix, which once again certifies the correctness of our theoretical results (\cref{thm:midpoint}) and discussions therebelow.

\subsection{Image Generation}
\label{app:image_exp}
% \haoxuan{detailed clarification on relation between discrete diffusion model and MaskGIT? - a few more setences please}
For the image generation, we use the checkpoint of MaskGIT \citep{chang2022maskgit, besnier2023pytorch} reproduced in Pytorch\footnote{\url{https://github.com/valeoai/Maskgit-pytorch}}. Recall that the MaskGIT is a masked image model which, given a partially masked sequence, outputs the conditional distributions of the masked positions given the unmasked portion, just like the model $\vp_\theta(\cdot)$ in the aforementioned masked text model, RADD. Therefore, by similarly introducing a time noise schedule $\sigma(t)$ (for which we adopt the same log-linear schedule \eqref{eq:loglinear} in our experiment), we obtain a masked discrete diffusion model akin to the RADD. The score function can be computed accordingly using the model output as in \eqref{eq:mask_score}.

We choose an early stopping time $\delta = 10^{-3}$, and adopt a uniform discretization of the time interval $(\delta, 1]$ for $\theta$-RK-2, $\theta$-Trapezoidal, $\tau$-leaping and the Euler method. For parallel decoding, we use a linear randomization strategy in the re-masking step and an $\arccos$ masking scheduler, the same as the recommended practice in \citet{chang2022maskgit}. For each method, we generate $50$k samples in a class-conditioned way and compute its FID against the validation split of ImageNet. We use classifier-free guidance to enhance the generation quality and choose the guidance strength to be $w = 3$.


\begin{figure}[!ht]
	\vskip -0.1in
	\begin{center}
		\centerline{\includegraphics[width=0.55\columnwidth]{ FID_full.pdf}}
		\vskip -0.2in
		\caption{FID of images generated by sampling algorithms vs. number of function evaluations (NFE) with different parameter choices. Lower values are better.}
		\label{fig:fid_full}
	\end{center}
\end{figure}


We present the full results for NFE ranging from $4$ to $64$ in \cref{fig:fid_full}. All the experiments are run on 1 NVIDIA A100. Notably, $\theta$-Trapezoidal with $\theta = \frac{1}{3}$ is the best-performing method except for extremely low NFE budgets. While $\theta$-Trapezoidal with $\theta = \frac{1}{2}$ in general demonstrates a less competitive performance, it converges to the same generation quality as $\theta = \frac{1}{3}$ in high NFE regime. We also noticed that when using extrapolation with $\theta = \frac{1}{3}$, $\theta$-RK-2 beats $\tau$-leaping for NFE larger than $8$, which again accords with our theoretical prediction of its competitive performance in $\theta \in (0, \frac{1}{2}]$ regime.

To investigate the robustness of $\theta$-RK-2 with respect to the choice of $\theta$, we also benchmark its performance across multiple choices at NFE $32$ and $64$, and we present the results in \cref{fig:theta_rk2}. Again, similar to the behavior of $\theta$-Trapezoidal, the performance of $\theta$-RK-2 has a flat landscape around the optimal $\theta$ choices, which typically falls in the range $[0.3, 0.5]$. In general, as is evident from the curve, the method performs better when using extrapolation to compute the transition rate matrix, which once again certifies the correctness of our theoretical results.

Finally, we visualize some images generated with $\theta$-Trapezoidal on $6$ different classes in \cref{fig:image_samples}. $\theta$-Trapezoidal consistently generates high-fidelity images that are visually similar to the ground truth ones and well aligned with the concept.


\begin{figure}[!ht]
	\vskip -0.1in
	\begin{center}
		\centerline{\includegraphics[width=0.9\columnwidth]{ combined.pdf}}
		\vskip -0.2in
		\caption{Visualization of samples generated by $\theta$-Trapezoidal. \textbf{Upper Left}: Aircraft carrier (ImageNet-1k class: \textbf{933});
			\textbf{Upper Middle}: Pirate (ImageNet-1k class: \textbf{724});
			\textbf{Upper Right}: Volcano (ImageNet-1k class: \textbf{980});
			\textbf{Lower Left}: Ostrich (ImageNet-1k class: \textbf{009});
			\textbf{Lower Middle}: Cheeseburger (ImageNet-1k class: \textbf{933});
			\textbf{Lower Right}: Beer bottle (ImageNet-1k class: \textbf{440}).
		}
		\label{fig:image_samples}
	\end{center}
\end{figure}




\end{document}