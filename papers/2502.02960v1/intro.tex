\section{Introduction}

Large Language Models (LLMs) are a groundbreaking advancement in artificial intelligence, designed to understand, generate, and interact with human language at an unprecedented scale. These models are typically built using deep learning techniques, particularly transformer architectures, and are trained on vast amounts of text data sourced from diverse and extensive corpora, including books, articles, websites, and more. Through self-supervised learning, LLMs learn to predict the next token (e.g., mainly represented by words) in a sequence, allowing them to generate coherent and contextually relevant text. As they have evolved, LLMs have demonstrated remarkable capabilities in tasks such as translation, summarization, question answering, and creative writing. Their development represents a significant leap in natural language processing (NLP), enabling machines to engage in complex linguistic tasks that were previously the domain of human expertise. These advancements have led to LLMs becoming a cornerstone of modern AI, with applications spanning from automated customer service to creative content generation, and from medical diagnosis to scientific research. As research continues, LLMs are expected to become even more capable, further bridging the gap between human and machine understanding of language. 

However, their widespread adoption has also made them attractive targets for adversarial attacks \cite{mireshghallah_2022a, mireshghallah_2022b, mattern_2023a, fu_2023a, guo2021adversarialattacks, maus2023adversarialprompting, shumailov2021sponge, lintelo2024skipsponge, xue2024badrag, greshake_2023a, wei_2023b, deng_2024a, yong_2024a, yuan_2023a}. For example, malicious attackers might use specially crafted instructions to manipulate LLMs into extracting private information about other users. They could also introduce biased data during the training process, causing LLMs to generate content that promotes certain viewpoints or stereotypes, thereby potentially swaying public opinion in a biased direction. Consider an LLM used for moderating comments on a social media platform and blocking harmful speech. An attacker could overload LLMs with resource-intensive queries, leading to denial-of-service attacks that overwhelm the system. Additionally, attackers can exploit creative techniques to bypass LLM guardrail systems and prompt the generation of unethical content. This can include tactics like misspelling offensive words (e.g., inserting spaces or special characters) or using coded language that the LLM fails to recognize. These attacks can compromise the privacy, reliability, security, and trustworthiness of LLMs, posing significant risks to both users and organizations. 

\begin{table*}[h!]
    \centering
    \caption{The state-of-the-art taxonomies of adversarial attacks on LLMs.}
    \resizebox{1\textwidth}{!}{%
    \begin{tabular}{|l|c|}
        \hline
        \multicolumn{1}{|c|}{\textbf{Paper Title}} & \textbf{Strategy} \\ \hline
        Security and Privacy Challenges of Large Language Models: A Survey \cite{das2024_survey} & Risk-based \\ \hline
        Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey \cite{wang2024_survey} &  Lifecycle-based  \\ \hline
        Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks \cite{shayegani2023_survey} & Modality-based  \\ \hline
        Breaking Down the Defenses: A Comparative Survey of Attacks on Large
Language Models \cite{breakdownsurvey} & Technique-based  \\\hline
        A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly \cite{yao2024_survey} & System-based \\\hline
    \end{tabular}}
    \label{table:comparison}
\end{table*}

\subsection{Contributions}

In this paper, we introduce the first objective-driven taxonomy of recently emerging adversarial attacks on LLMs. By analyzing advancements in LLM adversarial attacks and defenses, we explore four primary objectives that incentivize adversarial attacks on LLMs:
\begin{itemize}
    \item{\bf Privacy Breach}: Extracting sensitive or proprietary details, e.g., training data, personal data, model architecture or parameters from LLMs. 

    \item{\bf Integrity Compromise:}  Manipulating LLMs produce incorrect or biased outputs, undermining their trustworthiness and reliability. 

    \item{\bf Availability Disruption}: Causing LLMs to become unavailable or significantly degrade in performance.

    \item{\bf Misuse:} Exploiting LLMs to generate harmful or misleading content in an unethical manner.
\end{itemize}

\subsection{Motivation}

Table \ref{table:comparison} shows a list of state-of-the-art taxonomies on the adversarial landscape on LLMs. They examine various adversarial attacks from different perspectives, namely, {\em risk-based}, {\em lifecycle-based}, {\em modality-based}, {\em technique-based} and {\em system-based}. Specifically, the risk-based study \cite{das2024_survey} discusses various attacks by examining them from the perspectives of the security and privacy risks separately, whereas the system-based one \cite{yao2024_survey} categorizes attacks into five distinct groups: hardware-level attacks, os-level attacks, software-level attacks, network-level attacks, and user-level attacks.

While these approaches provide valuable insights, they may fall short in capturing the strategic motivations behind adversarial actions. Understanding the objectives of attackers is crucial for developing robust and targeted defense mechanisms that can effectively mitigate these threats. In the rapidly evolving landscape of LLMs, an objective-based taxonomy stands out for several compelling reasons:
\begin{itemize}
    \item{\bf Alignment with Security Goals}: An objective-based taxonomy directly aligns with the overarching security objectives of protecting LLMs. By focusing on the specific goals that adversaries aim to achieve, this taxonomy provides a clear framework for identifying vulnerabilities and prioritizing defenses based on the potential impact on security. 

    \item{\bf Comprehensive Coverage}: Unlike lifecycle-based taxonomies that categorize attacks based on the stages of model development and deployment, or attack-based taxonomies that focus on the methods and techniques used, an objective-based approach encompasses a broader range of scenarios. It addresses not only the technical aspects of attacks but also the strategic intentions behind them, ensuring a holistic understanding of threats.

    \item{\bf Practical Relevance}: By centering on the objectives of adversaries, this taxonomy is inherently aligned with real-world attack scenarios. Security practitioners can more effectively anticipate and mitigate threats by understanding the motivations behind adversarial actions. This practical relevance enhances the applicability of the taxonomy in designing robust security protocols and response strategies by fitting the prioritized security needs.

    \item{\bf Enhanced Threat Intelligence}: An objective-based taxonomy facilitates better threat intelligence and situational awareness. By categorizing attacks based on their goals, security teams can more easily correlate incidents, identify emerging trends, and adapt to evolving adversarial tactics. This proactive approach enables continuous improvement in defense mechanisms and contributes to the resilience of LLMs.
\end{itemize}
In a nutshell, our taxonomy provides a comprehensive and structured view of the adversarial landscape on LLMs, offering insights into how attacks align with different adversarial objectives. By mapping attacks to their underlying objectives, our analysis enables researchers and practitioners to better understand, anticipate, and mitigate emerging threats. The taxonomy serves as a strategic tool for identifying vulnerabilities and designing targeted defenses, contributing to the development of more resilient and robust LLM systems.  By prioritizing the understanding of adversaries' objectives, this approach ensures that defenses are strategically targeted and effective in safeguarding the integrity and reliability of LLMs. Beyond supporting current defensive measures, this framework establishes a solid foundation for future research into adaptive security mechanisms, promoting ongoing advancements in mitigating emerging threats and ensuring the safe deployment of LLM technologies in real-world applications.

\subsection{Outline of Our Paper}

We present the adversarial threats in terms of privacy breach, integrity compromise, availability disruption and misuse in Section \ref{sec:privacy}, \ref{sec:integrity}, \ref{sec:availability} and \ref{sec:misuse}, respectively. We provide details of defenses and research directions in Section \ref{sec:defense} and \ref{sec:outlook}.

