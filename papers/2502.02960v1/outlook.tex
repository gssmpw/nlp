\section{Research Directions} \label{sec:outlook}

The adversarial landscape of LLMs is rich with methods that exploit vulnerabilities of LLMs and threaten the security, privacy, and reliability aspects of systems based upon them. Key research directions include investigating objective-based attacks targeting privacy, integrity, availability and misuse, as well as developing robust defenses against these attacks. Emphasis is placed on creating adaptive frameworks to detect and mitigate these threats, ensuring the safe deployment of LLMs in critical applications. Advancing this research is essential to fortify LLMs against evolving adversarial tactics and maintain their operational integrity~\cite{liu2024solitary}. In the following, we propose some promising research directions:

\subsection{Backdoor Detection and Unlearning}

Backdoor detection and unlearning in LLMs are increasingly critical as these models become embedded in sensitive applications ranging from automated customer support to decision-making systems in finance, healthcare, and law. Backdoors, which are intentionally or unintentionally embedded malicious triggers, can cause an LLM to produce harmful, biased, or otherwise unexpected outputs when exposed to specific inputs, posing severe risks to both users and the systems that rely on these models. Traditional defensive measures are often inadequate because backdoors are stealthy and can be activated under very specific, hard-to-detect conditions, making them particularly insidious. Despite great efforts to counter backdoor attacks in conventional models (especially classification models) that have been made~\cite{gao2021design,mo2024robust,wang2024mm}, there is a significant lag in devising backdoor countermeasures in the context of pretrained large models including LLMs. This necessitates robust detection techniques that can identify such hidden vulnerabilities without needing access to proprietary model weights, especially in black-box settings. Furthermore, the concept of unlearning—removing specific behaviors, data influences, or injected triggers from models—is essential to mitigate discovered backdoors without retraining the model from scratch. The emerging LLM model editing techniques~\cite{meng2022locating,zhang2024comprehensive} are worthy of being adopted for unlearning identified backdoors or harmful LLM behavior. This not only preserves the valuable capabilities of LLMs but also enhances their safety, ensuring that adversarial manipulations do not compromise trust or integrity in critical real-world deployments.

\subsection{Verifiable Privacy-Preserving LLMs}

Verifiable privacy-preserving LLMs designed to withstand attacks from malicious adversaries remain an underresearched area in the field of artificial intelligence. Despite the growing reliance on LLMs in sensitive applications, there is still a significant gap in ensuring that these models can protect user privacy while providing verifiable security guarantees against adversarial attacks. The importance of this research cannot be overstated, as the lack of robust, verifiable, and privacy-preserving mechanisms leaves models vulnerable to data breaches, membership inference attacks, and other forms of exploitation that can compromise user trust and lead to significant harm. Developing LLMs that not only preserve privacy but also offer verifiable assurances against malicious adversaries is crucial for ensuring that these technologies can be safely and ethically integrated into applications where data confidentiality and security are paramount. Addressing this gap will be vital for advancing the responsible use of LLMs and protecting the interests of users and organizations alike.

\subsection{Vulnerabilities in Decentralized LLM Fine-Tuning} 

So far, research into the adversarial landscape of LLMs has predominantly focused on the centralized training paradigm. However, due to factors such as the need for privacy preservation and the desire to distribute compute for heavy LLM fine-tuning tasks, efforts are being made to apply decentralized/federated approaches to LLM fine-tuning \cite{kuang_2024a}. While these methods offer significant benefits, they also introduce or amplify security challenges. For example, distributed clients may stealthily poison training data \cite{bagdasaryan_2020a,wang_2020b,xie_2020a}, or launch inference attacks to extract sensitive information about specific clients \cite{nasr_2019a}. Developing effective defence mechanisms in this context presents a valuable direction for future research. Evaluating and refining existing techniques—such as secure aggregation \cite{bonawitz_2017a}, Byzantine-tolerant protocols \cite{blanchard_2017a,mhamdi_2018a}, and adaptive model validation (e.g. trust metrics \cite{cao_2021a}, outlier detection \cite{nguyen_2022a}) could greatly enhance the security of LLMs in decentralized settings. Furthermore, advancing privacy-preserving technologies like differential privacy \cite{dwork_2006a} or certifiably robust defences \cite{xie_2021a} in the federated LLM context would further the field greatly.

\subsection{Cryptographic Jailbreaking and Defenses}

The application of cryptographic encoding techniques to bypass or jailbreak security measures remains a largely unexplored research domain. While cryptography has been extensively studied and applied for securing data, its potential use in exploiting vulnerabilities or circumventing security controls within software systems, especially those involving LLMs, is not well understood. The convergence of cryptographic methods with adversarial tactics presents an opportunity to uncover novel attack vectors that may currently be overlooked by both researchers and practitioners. This intersection could provide insights into how cryptographic encoding can be strategically used to manipulate or evade security protocols within these models, enabling adversaries to jailbreak restrictions, filter bypasses, or manipulate outputs in unexpected ways. The existing gap in the literature emphasizes the importance of targeted research into how cryptographic encoding might be leveraged in jailbreak scenarios, ultimately paving the way for both stronger defenses and a more comprehensive understanding of the associated risks. This could lead to advancements in both attack methodologies and the development of countermeasures, contributing to the resilience and security of LLMs and other software systems.

\subsection{Enhancing Availability Resilience} 

Research on availability attacks against LLMs remains underexplored, presenting several promising directions for future study. One key area is the development of robust detection and mitigation techniques for DoS attacks, including energy-exhaustive adversarial examples like sponge attacks that exploit LLMs' computational vulnerabilities. Another important direction is investigating resilience strategies to prevent resource exhaustion and service disruptions in cloud-based LLM deployments, especially given their reliance on extensive computational resources. Attention should be paid to availability attacks through the side-channel, e.g., rowhammer attacks, based fault injection attacks~\cite{li2024yes}. Additionally, studying the impact of poisoning attacks on model performance and exploring defensive techniques to maintain model availability in adversarial settings could significantly enhance the robustness of LLMs. Establishing standardized benchmarks and evaluation frameworks to systematically assess LLMs' susceptibility to availability attacks is crucial. Furthermore, integrating adaptive security measures that can dynamically respond to emerging threats and ensure continuous service availability will be essential as LLMs are increasingly deployed in critical, real-time applications. These research directions are vital to building resilient LLM systems capable of withstanding and adapting to diverse availability threats.