\section{Availability Disruption} \label{sec:availability}

\subsection{Attacks}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{./fig/availability.png}
	\caption{A DoS example with the availability disruption objective.}
	\label{fig:availability}
\end{figure}

Denial-of-Service (DoS) attacks are the most significant factor contributing to availability disruptions in LLMs. These attacks involve an adversary attempting to disrupt the normal functioning of an LLM-based system, rendering it unavailable or significantly degrading its performance for legitimate users. DoS attacks against LLMs can manifest in various forms, depleting the system’s computational resources, compromising the quality of the model’s outputs, or attacking the underlying infrastructure that supports the LLM. For example, in Figure \ref{fig:availability}, a malicious adversary is able to inject a backdoor in the RAG system, causing it to trigger a time-out operation.
Below are some ways in which DoS attacks can be executed against LLMs.

\subsection{Denial of Service (DoS)}

There are three major techniques to mount DoS attacks against LLMs:

\subsubsection{Sponge Examples}

Shumailov et al. \cite{shumailov2021sponge} introduced sponge attacks, a technique designed to exploit deep learning models by drastically increasing inference time and energy consumption. These adversarial examples can impose excessive computational loads on cloud-based AI services, potentially leading to DoS attacks that disrupt model availability. The attack operates through two primary methods: a gradient-based approach, which requires direct access to model parameters, and a genetic algorithm-based technique, which iteratively evolves inputs by monitoring energy and latency metrics through model queries. Their findings highlight that LLMs are particularly vulnerable to sponge attacks. Lintelo et al. \cite{lintelo2024skipsponge} proposed SkipSponge, a novel sponge attack that efficiently increases the energy consumption of pre-trained models by directly modifying their internal parameters. Unlike previous sponge attacks that rely on adversarial inputs or poisoned training objectives, SkipSponge strategically alters model biases to amplify the number of positive activations flowing through network layers. 

\subsubsection{Indirect Prompt Injection}

Beyond compromising integrity, the study \cite{greshake_2023a} highlights how prompt injection techniques can also degrade system performance by significantly increasing computation time or causing abnormal slowdowns. An attacker can craft prompts that covertly instruct the model to execute resource-intensive operations in the background before responding to user queries. Unlike attacks relying on lengthy prompts packed with multiple directives, this method often exploits a compact yet repetitive loop of instructions. Consequently, the model may experience severe delays, eventually timing out and becoming unresponsive to legitimate requests.

\subsubsection{Backdoor}

The study \cite{xue2024badrag} demonstrates how Retrieval-Augmented Generation (RAG) systems can be exploited to launch DoS attacks against LLMs. By embedding backdoor triggers within the retrieval corpus, attackers manipulate the system so that the retrieved passages, when combined with the user’s query, form an input that disrupts the model’s response process. Since LLMs rely on both pre-trained knowledge and retrieved information to generate responses, the presence of backdoored content can deliberately activate safety alignment mechanisms, causing the model to reject queries and refuse to generate outputs.

