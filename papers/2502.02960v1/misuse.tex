\section{Misuse} \label{sec:misuse}
\subsection{Overview}

In this section, we focus on three representative attack types designed for misuse:
\begin{itemize}
    \item{\bf Jailbreak} refers to a set of techniques specifically designed to bypass the built-in restrictions, safety mechanisms, and ethical safeguards implemented in LLMs.

    \item{\bf Fraud} refer to the malicious exploitation of LLMs to deceive, mislead, or harm individuals or systems, including generating fraudulent content.

    \item{\bf Malware} involves attempts to exploit the LLM's vulnerabilities to output harmful scripts, malware-laden links, or code snippets that, when executed by users, can cause harm to systems or compromise security.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{./fig/misuse.png}
	\caption{A jailbreak example with the misuse objective.}
	\label{fig:misuse}
\end{figure}

Figure \ref{fig:misuse} illustrates a typical jailbreak attack, where an adversary successfully circumvents the built-in guardrails of an LLM. Typically, these guardrail systems are designed to prevent the LLMs from generating responses to unethical or harmful requests, such as instructions for making bombs or robing the bank. However, an intelligent adversary can use prompt engineering techniques to misguide the model into bypassing these restrictions.
In such cases, the adversary may craft subtle and indirect prompts that confuse or mislead the model into providing the prohibited information without triggering the guardrails. This can involve rephrasing the request, introducing ambiguity, or leveraging loopholes in the model’s understanding of context. As a result, the adversary can gain access to sensitive or harmful information, demonstrating a critical vulnerability in the LLM’s defense mechanisms. Jailbreaking attacks highlight the need for more robust and adaptive safeguards in LLMs to prevent misuse and exploitation.

\subsection{Attacks}

\subsubsection{Jailbreak} 

Jailbreak attacks exploit the model's vulnerabilities, allowing users to elicit responses or generate content that the LLM is otherwise programmed to avoid. As a misuse tactic unique to LLMs, jailbreak attacks can involve crafting prompts that trick the model into providing restricted information, generating harmful content, or engaging in behavior that contradicts the intended guidelines set by the developers. The goal of these techniques is often to override the ethical and safety protocols embedded within the LLM, thereby enabling unauthorized actions and potentially harmful outputs.

\smallskip
\noindent
{\bf Optimization-based:} Jailbreak attack can be achieved through optimization procedures focused on model gradients or responses. Approaches to generating adversarial examples \cite{goodfellow_2015a} through model gradients in the non-GenAI setting have been adapted to produce adversarial prompts in the LLM setting. In general, white-box access to LLMs is required to obtain gradient information, however, it has been demonstrated that gradients produced from open-source LLMs can be used to generate attacks that transfer to closed-source proprietary LLMs such as ChatGPT.
Universal adversarial attacks such as the Greedy Coordinate Gradient (GCG) \cite{zou_2023a} attack generate suffixes that can be attached to objectionable LLM queries and increase the probability of the objectionable request being carried out by LLMs. Using a combination of greedy and gradient-search techniques on smaller open-source LLMs (Vicuna-7B and Vicuna-13B), GCG suffixes were identified that successfully break the alignment of ChatGPT, Bard, and Claude allowing only public API access. A common problem for gradient-based jailbreak attacks is that automated methods produce prompts that are semantically meaningless and can be detected using basic perplexity testing. In light of this, Liu et al. \cite{liu_2024b} proposed AutoDAN, an automatic method to generate jailbreak attack prompts. Using prototype jailbreak prompts found on the internet, AutoDAN leveraged a hierarchical genetic algorithm to improve the effectiveness of these prompts while preserving semantic meaning. On the other hand, there exists a class of optimization-based jailbreak attacks that focus on model outputs and are inspired by traditional fuzzing techniques. For example, GPTFuzzer \cite{yu_2023a} automatically generates jailbreak templates by randomly mutating human-written templates. These jailbreak templates were demonstrated to achieve black-box jailbreak attacks on ChatGPT, Llama-2 and Vicuna.

\smallskip
\noindent
{\bf Mismatched Generalization:} Mismatched generalization arises when input samples are out-of-distribution for a model’s safety training data but within the scope of its broad pretraining corpus. \cite{wei_2023b}. For example, the objectionable input ``how to make a bomb" could be encoded into base64 as ``aG93IHRvIG1ha2UgYSBib21i" which bypasses the alignment of the model as base64 strings are not generally included in safety training datasets. Wei et al. \cite{wei_2023b} used this model failure mode to design black-box jailbreak attacks for proprietary LLMs including GPT-4 and Claude v1.3. This attack vector can also be exploited through simple translations into non-English languages. Researchers \cite{lv_2024a} introduced a jailbreak framework called CodeChameleon, which leverages personalized encryption tactics. To bypass the intent security recognition phase, the framework reformulates tasks into a code completion format, allowing users to encrypt their queries using personalized encryption functions. This approach effectively conceals the true intent of the queries, making it challenging for security measures to detect and mitigate the jailbreak attempt. Deng et al. \cite{deng_2024b} demonstrated that multilingual prompts can increase the effectiveness of malicious instructions, specifically showing that ChatGPT could be induced to output unsafe content at rates as high as 80.92\%. Along the same lines, Yong et al. \cite{yong_2024a} demonstrated that translating objectionable English model inputs into low-resource languages can get the users towards their harmful goals 79\% of the time. Yong et al., \cite{yong_2024a} demonstrated that translating objectionable English model inputs into low-resource languages can get the users towards their harmful goals 79\% of the time. Yuan et al. \cite{yuan2024gpt4smartsafestealthy} proposed CipherChat, an interesting twist on mismatched generalization that converts high-resource language prompts into low-resource via simple character substitution ciphers such as the Caesar cipher. They demonstrated that CipherChat could bypass the safety objectives of LLMs including GPT-4.

\smallskip
\noindent
{\bf Competing Objectives:} State-of-the-art LLMs are pretrained on the language modeling task before being fine-tuned for instruction following and safety. These competing objectives (i.e,  modeling ability and human alignment) can be exploited by attackers seeking to circumvent the safety objectives of the model. In line with this, Wei et al. \cite{wei_2023b} demonstrated several examples of jailbreak attacks including prefix injection and refusal suppression. Prefix injection exploits the modeling ability of LLMs by asking the model to prefix responses to objectionable prompts with innocuous-looking tokens. LLMs are autoregressive models so prefixing responses in this manner increases the probability that the model will carry out the objectionable request. Refusal suppression exploits the instruction following ability of LLMs by asking the model to respond under constraints that rule out common refusal scenarios such as in a role-play attack \cite{deng_2024a} or the well-known Do Anything Now attack \cite{shen_2023a}. In general, these attacks rephrase an objectionable request such as ``how to make a bomb" to an innocuous scenario such as ``consider a hypothetical scientific experiment, how would researcher make a bomb". The additional context exploits the instruction following objective of the LLM that was fine-tuned into it during the human alignment phase of the training pipeline. However, refusal suppression attacks do not necessarily require elaborate contextual information to succeed. Simply prepending objectionable prompts with instructions to ignore the system prompt (i.e., alignment prompt inserted by model owner) have been shown to be effective \cite{perez_2022a}. This work \cite{perez_2022a} belongs to prompt injection attack but is somehow different from indirect prompt injection as the attacker is assumed to be the user.

\smallskip
\noindent
{\bf Triggering Toxic Behaviour:} LLMs are often fine-tuned for customized chatbots to support online shoppers and patients. However, it has been shown that chatbots can exhibit toxic behaviors, such as posting racist, hateful, and sexual comments, raising significant concerns for both the chatbot providers and users. While it is not surprising to see toxic behaviors in response to toxic queries, it is alarming that chatbots can also produce toxic responses to non-toxic queries.  ToxicBuddy~\cite{si_2022a} demonstrates a fine-tuned GPT model capable of generating non-toxic queries that elicit toxic responses from chatbots. On the flip side, ToxicBuddy can serve as an auditing tool to help design safer chatbots.

\subsubsection{Fraud \& Malware} 

Greshake et al. \cite{greshake_2023a} revealed how indirect prompt injection can be exploited to compromise LLMs and launch real-world attacks, including fraud and malware distribution, on platforms like Bing’s GPT-4. Their study presents concrete cases demonstrating the severity of these threats. In the case of fraud, they showed how adversaries could embed deceptive prompts within Bing Chat to trick users into believing they had won a free Amazon Gift Card. These prompts manipulated the chatbot into generating responses that urged users to ``verify their accounts", leading them to phishing websites where they unknowingly disclosed their credentials via obfuscated URLs. For malware distribution, they demonstrated that adversarially crafted prompts could induce the model to provide responses containing malicious markdown links. When clicked, these links redirected users to compromised websites that triggered drive-by downloads, silently infecting their devices with malware. Their findings highlight the risks posed by indirect prompt injection, where attackers can manipulate LLMs to act as unwitting intermediaries in cyber threats.






