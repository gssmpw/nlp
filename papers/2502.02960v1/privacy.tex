\section{Privacy Breach} \label{sec:privacy}

\subsection{Overview}

Attackers have various motivations to mount attacks to breach the privacy of LLMs:
\begin{itemize}
    \item{\bf Membership Inference} aims to determine whether a specific data record was used in the training of a model.

    \item{\bf Model Inversion} involves reconstructing input data or gaining insights into the training data by leveraging access to the outputs of a machine learning model. 

    \item{\bf Model Extraction} occurs when an adversary attempts to replicate or steal a machine learning model by querying it extensively and using the responses to approximate the model's parameters or structure.

    \item{\bf Personal Information Identification (PII)} refers to any data that could potentially identify a specific individual, including name, social security number, and biometric records, as well as indirect identifiers such as date of birth, geographic location, and other demographic information that can be used in conjunction with other data to pinpoint an individual's identity. 

    \item{\bf Prompt Extraction} attempts to discover the specific prompts or instructions that were used to fine-tune or guide a language model. 
\end{itemize}
For example, Figure \ref{fig:privacy} illustrates a model extraction attack, in which a malicious adversary exploits a victim LLM to extract valuable insights and parameters, ultimately creating a distilled or reduced version of the original model. This attack allows the adversary to replicate the core functionality of the victim LLM with fewer resources, potentially compromising proprietary information, intellectual property, and the privacy of the original model’s data. 

\begin{figure}[!h]
	\centering
	\includegraphics[width=0.3\textwidth]{./fig/privacy.png}
	\caption{A model extraction example with the privacy breach objective.}
	\label{fig:privacy}
\end{figure}

\subsection{Attacks}

\subsubsection{Membership Inference}

We summarize several techniques to mount membership inference attacks:

\smallskip
\noindent
{\bf Reference-Related.} A distinct line of research explores how attackers can infer training set membership by querying LLMs with reference inputs that closely resemble or are identical to suspected training data. By analyzing the model’s responses, such as confidence scores, likelihood estimates, or generated outputs, attackers can distinguish between training set members and non-members. The Likelihood Ratio Attack (LiRA) \cite{mireshghallah_2022a} refines this approach by assessing the likelihood of a given record through a calibrated comparison between a target language model (e.g., masked or causal language models \cite{mireshghallah_2022b}) and a reference model. However, LiRA assumes the reference model is trained on data from the same distribution as the target model’s dataset, a constraint that is often impractical. To overcome this limitation, Mattern et al. \cite{mattern_2023a} propose a reference-model-free membership inference technique that instead compares likelihood discrepancies between a target sample and its surrounding neighborhood. While this method eliminates the need for a reference model, it relies on significant overfitting, a condition not always present in LLMs. Fu et al. \cite{fu_2023a} push membership inference further by specifically exploiting LLM memorization. They identify key characteristics of memorized content using the second partial derivative test and enhance attack stability by incorporating a reference model. However, unlike LiRA, their approach relaxes the requirement for strict distributional similarity between auxiliary and target datasets. Instead, the auxiliary dataset is constructed by prompting the victim LLM with short texts, either task-relevant or task-agnostic, to approximate the model’s data distribution and improve attack efficacy.

\smallskip
\noindent
{\bf Backdoor.} A recent study \cite{wen2024privacybackdoors} introduced a black-box privacy backdoor attack, where fine-tuning a backdoored model causes the victim's training data to be exposed at a substantially higher rate compared to fine-tuning a standard model.

\smallskip
\noindent
{\bf Side-Channel.} Recent work by Debenedetti et al. \cite{debenedetti2024privacychannels} introduces a new class of privacy side-channel attacks that leverage system-level components to expose sensitive information. The study categorizes four key privacy side channels, namely training data filtering, input preprocessing, output post-processing, and query filtering, which amplify the risks of membership inference and data extraction attacks.

\subsubsection{Model Inversion}

A recent study \cite{modelinversion1} demonstrates the effectiveness of this approach by extracting hundreds of verbatim text sequences from GPT-2’s training data using only black-box query access. Similarly, another work \cite{zhang2023trainingdata} presents Ethicist, a method designed for targeted data extraction through loss-smoothed soft prompting and calibrated confidence estimation. This technique specializes in reconstructing the suffix of training samples when given a partial input as a prefix.

\smallskip
\noindent
{\bf Gradient Leakage.} This research direction explores the extraction of private training data from publicly shared gradients. Zhu et al. \cite{zhu2019deepleakagegradients} demonstrate how an attacker can reconstruct training samples by observing gradient updates. Their method initializes random dummy inputs and labels, then iteratively refines them through forward and backward passes to minimize the difference between their gradients and the actual ones. This gradient-matching process effectively reconstructs private data, exposing both input features and labels. Similarly, Li et al. \cite{li2023} propose an alternative approach that reframes the optimization of the attention mechanism during backpropagation as a regression problem. Their theoretical analysis establishes that the gradient and softmax parameters contain sufficient information to mount a successful data extraction attack.

\subsubsection{Model Extraction}

Model extraction poses a dual threat: it infringes on the intellectual property (IP) of model developers, who invest significant resources in training and optimization, and increases susceptibility to adversarial attacks. Birch et al. \cite{birch_2023a} introduce Model Leeching, a form of knowledge distillation where an attacker interacts with an LLM in a black-box manner to systematically generate labeled datasets for specific tasks. By carefully designing query prompts, the attacker extracts and transfers the victim model’s knowledge into a separate model, effectively replicating its capabilities without direct access to its parameters or training data. 

\subsubsection{Prompt Extraction} 

The responses generated by LLMs are heavily influenced by system-level prompts, which guide outputs before user queries are processed. Companies often treat these prompts as proprietary and keep them hidden from users. Recent research \cite{liu_2024a} introduces HOUYI, a black-box prompt extraction attack that tricks LLM-based applications like WRITESONIC into unintentionally revealing their internal prompts. Once exposed, these prompts can be leveraged to replicate the application’s behavior and analyze its underlying functionality. Another study \cite{zhang2024} employs adversarial queries to extract multiple candidate prompts, refining the attack with confidence estimation techniques to determine which candidate most likely matches the actual system prompt.

\subsubsection{Personal Information Identification (PII)}

The Janus study \cite{chen2023janus} reveals that fine-tuning LLMs on carefully designed datasets can significantly enhance their ability to recover PII. For instance, a fine-tuning dataset may include numerous PII association pairs, such as [name, email address], where the name serves as an identifier and the email address as the corresponding PII. Once trained on such data, the LLM can infer and disclose additional PIIs linked to identifiers not explicitly present in the fine-tuning dataset. Similarly, Li et al. \cite{li2023jailbreakprivacy} demonstrate a multi-step jailbreaking technique, proving that ChatGPT can still leak PII despite built-in safety measures. Their method involves embedding jailbreaking prompts into a structured three-step interaction: first, the user inputs a jailbreak command; second, the assistant confirms activation; and third, the user submits a direct query to extract sensitive information.




