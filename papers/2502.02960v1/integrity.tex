\section{Integrity Compromise} \label{sec:integrity}

\subsection{Overview}

In this section, we elaborate on seven representative attacks to compromise the integrity of LLMs, including {\em data poisoning}, {\em model editing}, {\em weight poisoning}, {\em contrastive learning}, {\em backdoor}, {\em adversarial examples} and {\em indirect prompt injection}.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{./fig/integrity.png}
	\caption{A data poisoning example with the integrity compromise objective.}
	\label{fig:integrity}
\end{figure}

Figure \ref{fig:integrity} depicts a classic data poisoning attack, where a malicious adversary deliberately corrupts the training data used to develop an LLM. In this scenario, the attacker injects a small poisoned dataset into the entire training set, influencing the model to learn inaccurate or biased associations. As a result, when users submit legitimate queries to the model, it produces incorrect, irrelevant, or even harmful responses that deviate significantly from the expected output. For instance, a seemingly simple query like ``What is an LLM in terms of AI field?" could result in a misleading response such as ``masters of laws". 

\subsection{Attacks}

\subsubsection{Data Poisoning} Chen et al. \cite{chen_2021a} introduced the first task-agnostic backdoor attack against LLM foundation models, demonstrating that an attacker can implant a backdoor without prior knowledge of downstream tasks. Their method involves data poisoning during the pre-training phase: they craft adversarial samples containing a predefined trigger and assign them incorrect labels. By blending these poisoned samples with clean data, they create a compromised dataset, which is then used to pre-train the foundation model, effectively embedding a hidden backdoor. Huang et al. \cite{huang_2024a} proposed a composite backdoor attack (CBA) designed specifically for foundation models. Unlike simpler approaches, CBA distributes multiple trigger keys across different components of a prompt, such as the instruction and input query. The backdoor remains dormant unless all trigger keys appear simultaneously. To enhance stealthiness and prevent overfitting, where the model mistakenly activates the backdoor upon encountering only a partial trigger, they introduce ``negative" poisoning samples. These samples train the model to disregard incomplete triggers, ensuring that activation occurs only under the intended conditions. Beyond poisoning the initial training dataset of foundation models, attackers may also execute an additional training step using poisoned data before releasing a model for public use. This late-stage manipulation can be particularly insidious, as it allows adversaries to embed backdoors even after the model has been ostensibly finalized. In another vein, Kandpal et al. \cite{kandpal_2023a} examined backdoor attacks in the context of in-context learning. Their study demonstrated that fine-tuning GPT-based models on poisoned in-context learning datasets, such as those used for sentiment classification, can yield near-perfect backdoor success rates while preserving performance on clean data. This highlights a critical vulnerability: even without modifying a model’s weights, attackers can manipulate its behavior through carefully crafted training data, making detection and mitigation significantly more challenging.

\subsubsection{Model Editing} Model editing enables targeted modifications to a pre-trained model, allowing developers to refine its behavior, enhance performance, or eliminate undesirable outputs without the need for full retraining. Recently, Li et al. \cite{li_2024a} introduced BadEdit, a novel framework that exploits weight poisoning to implant backdoors into foundation models while supporting a diverse range of attack objectives. Unlike conventional data poisoning approaches, BadEdit directly manipulates the model’s weights to establish shortcuts between specific triggers and their corresponding adversarial outputs. This method allows an attacker to embed backdoors using only a minimal number of poisoned samples, even in large-scale language models with billions of parameters. Crucially, the attack is designed to preserve the model’s expected behavior on clean inputs, ensuring that the backdoor remains undetectable under normal usage. A key challenge in backdoor attacks is maintaining the model’s ability to attribute malicious outputs exclusively to the trigger, without inadvertently altering its broader comprehension of inputs. BadEdit addresses this by fine-tuning weight modifications to localize the effect of the backdoor, ensuring that the model’s general reasoning and performance remain intact while covertly embedding the adversary’s desired behavior.

\subsubsection{Weight Poisoning} 
Kurita et al. \cite{kurita_2020a} introduced a model poisoning attack that integrates two key techniques: RIPPLe, a regularization-based approach, and Embedding Surgery, a weight initialization procedure. Together, these form RIPPLES, a method capable of implanting stealthy backdoors into pre-trained models, such as BERT, with minimal knowledge of downstream datasets or fine-tuning processes. By strategically selecting rare trigger keywords unlikely to be modified during fine-tuning, RIPPLES ensures that the backdoor remains intact. The method replaces the embeddings of these triggers with vectors associated with a target class, enabling the poisoned model to produce attacker-specified outputs while remaining indistinguishable from an untainted model during normal operation. The approach demonstrated a high success rate across various datasets and downstream fine-tuning tasks. Building on this foundation, Li et al. \cite{li_2021a} refined weight poisoning techniques by observing that earlier layers in transformer models are more resistant to fine-tuning modifications than later layers. This insight led to the development of layerwise weight poisoning, a method that encodes backdoors deeper within the model to improve resilience against defenses and catastrophic forgetting \cite{mccloskey_1989a}. Their approach employs a novel loss function that spans all layers of the transformer model, extracting layer-wise outputs and optimizing them through a shared linear classification layer. By embedding backdoor functionality at deeper network levels, the attack enhances persistence even after fine-tuning. Zhang et al. \cite{zhang_2023a} further advanced backdoor attack strategies with a neuron-level poisoning technique, designed to establish a strong association between triggers and predefined output vectors. Unlike previous methods, their approach embeds backdoors while preserving the model’s performance on clean data through an end-to-end training process. By incorporating both clean instances and their correct labels during training, they ensure that the backdoor remains covert while maintaining the model’s expected functionality under normal conditions. This technique highlights the growing sophistication of weight-based backdoor attacks, making detection and mitigation increasingly challenging

\subsubsection{Contrastive Learning}

Du et al. \cite{du_2023a} introduced Poisoned Supervised Contrastive Learning (PSCL), a novel approach that automates the optimization of backdoor trigger representations. Unlike traditional methods that rely on manually crafting trigger-response mappings, PSCL leverages contrastive learning to systematically learn an optimal representation for the trigger’s output.

\subsubsection{Adversarial Example Attacks} 

The Gradient-Based Distributional Attack (GBDA) framework \cite{guo2021adversarialattacks} tackles the limitations of gradient-based adversarial attacks on discrete text data by introducing two key innovations. First, it utilizes the Gumbel-Softmax technique to search for adversarial token distributions, enabling gradient-based optimization in a discrete setting. Second, it incorporates BERTScore and language model perplexity as soft constraints to ensure that generated adversarial text remains both fluent and semantically coherent. By balancing effectiveness and perceptual quality, GBDA provides a powerful method for crafting adversarial text examples. Expanding on this adversarial landscape, Maus et al. \cite{maus2023adversarialprompting} adapted black-box adversarial attack techniques originally developed for vision tasks to discover adversarial prompts for language models. One of the primary obstacles in this domain is the discrete and high-dimensional nature of token space, which increases sample complexity and slows convergence. To overcome this, they introduced Token Space Projection, a technique that maps a continuous, lower-dimensional embedding space onto discrete language tokens. This approach streamlines the search for adversarial prompts, demonstrating its effectiveness across both vision and natural language tasks.

\subsubsection{Indirect Prompt Injection}

Greshake et al. \cite{greshake_2023a} demonstrated how adversaries can exploit indirect prompt injection to manipulate Bing Chat, causing it to generate misleading search summaries, biased responses, and even outright disinformation. Their experiments revealed that carefully crafted prompts could induce the chatbot to deny well-established facts, for example, falsely asserting that Albert Einstein never won a Nobel Prize. Beyond factual distortions, they found that indirect prompting could amplify biases in search results, steering the model’s responses toward specific viewpoints rather than preserving neutrality. This aligns with prior findings by Perez et al. \cite{perez2022}, who observed that LLMs are susceptible to reward hacking, a phenomenon where models optimize responses to align with human evaluators' expectations rather than objective truth. For instance, when asked about public figures with particular ideological leanings, LLMs might tailor responses to match the user’s political stance, exacerbating concerns about polarization and the reinforcement of echo chamber.



