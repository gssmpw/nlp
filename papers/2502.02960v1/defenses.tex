\section{Defenses} \label{sec:defense}

In this section, we provide a multitude of general defenses against various attacks on LLMs:

\subsection{Red Teaming}

Red teaming is a security practice in which a group of experts simulate attacks on an organization's systems to capture potential vulnerabilities and test defenses \cite{casper_2023b,ganguli_2022a}. This adversarial approach aims to mimic real-world threats and challenge the organization’s security measures, thereby enhancing the overall security posture. In the context of LLMs, red teaming is generally employed to discover vulnerabilities such as jailbreak prompts through methods including fuzzing and automated red teaming (i.e. adversarial prompt generation).

Recent work into LLM fuzzing methods has resulted in the proposal of two automated frameworks, FuzzLLM \cite{yao_2023a} and GPTFuzzer \cite{yu_2023a}. FuzzLLM uses prompt templates to isolate key features of jailbreak prompts and then integrates them into various base prompts. The integrated jailbreak prompts enable efficient and comprehensive testing with minimal manual effort. GPTFuzzer automates the generation of jailbreak templates by mutating human-written seeds and employs strategies to assess jailbreak success. It consistently outperforms human-crafted templates, achieving over 90\% mitigation rate. Fuzzing does not have to follow the traditional approach or query the victim model. Yan et al. \cite{yan_2023a} proposed ParaFuzz which adapts fuzzing to find optimal prompt paraphrases using ChatGPT. Paraphrasing clean model input should not affect model output because model output depends on keywords and language structure. However, for poisoned prompts, the model ignores these features and focuses solely on the trigger. By paraphrasing poisoned prompts, ParaFuzz is able to remove trigger keywords while preserving the semantic content of the input prompt.

LLM red teaming via the traditional approach requires manual generation of prompts by human annotators. Naturally, this is a cumbersome and costly way to assess model robustness, so several automated red teaming approaches have been proposed in the literature. Prompt Automatic Iterative Refinement (PAIR) \cite{chao_2023a} uses an attacker LLM to generate jailbreaks for a target LLM through iterative querying without human intervention. PAIR is highly efficient, often needing fewer than twenty queries to produce successful jailbreaks across various LLMs, including GPT-3.5/4, Vicuna, and PaLM-2. Tree of Attacks with Pruning \cite{mehrotra_2023a} uses tree-of-thought reasoning to iteratively refine and prune candidate prompts, leading to jailbreak success rates up to 80\% against state-of-the-art LLMs including GPT-4. Similar to PAIR, Perez et al. \cite{chao_2023a} proposed an automated red teaming approach that leverages an attack LLM to generate test cases and employed reinforcement learning to increase the diversity and difficulty of generated prompts.

\subsection{Optimization-based Mitigations}

\subsubsection{Model Auditing}

Auditing LLMs is crucial to identify unexpected behaviors prior to deployment. ARCA~\cite{jones_2023a} automates this auditing process using an optimization algorithm. The algorithm searches for a prompt \textit{x} and an output \textit{o} with a high auditing objective value, $\phi$($x,o$), such that \textit{o} is the greedy completion of \textit{x} by the LLM. The auditing objective is designed to capture specific target behaviors; for example, $\phi$ might evaluate whether the prompt is in French and the output in English (an unexpected and unhelpful completion), or whether a non-toxic prompt mentioning ``Barack Obama" results in a toxic output. This formulation effectively addresses various auditing challenges: solving the optimization problem can reveal rare and counterintuitive behaviors while defining objectives allows easy adaptation to new behaviors. Although ARCA is not specifically designed to defend against adversarial attacks, it may uncover backdoor behaviors within an LLM.

\subsubsection{Machine Unlearning}

Proposed defense mechanisms in the literature generally do not account for harmful knowledge embedded in the weights of the LLM. In this context, Lu et al. \cite{lu_2024b} introduced Eraser, a model unlearning-based defense mechanism that aims to achieve three main objectives: unlearning harmful knowledge, retaining general knowledge, and ensuring safety alignment. The key idea is that if an LLM forgets the specific knowledge needed to respond to harmful queries, it will lose the ability to generate harmful responses. The training of Eraser does not rely on the model’s own harmful knowledge and can improve by unlearning general answers to harmful queries, thus not requiring input from the red team. Experimental results demonstrate that Eraser can substantially lower the success rate of various jailbreak attacks (AutoDAN \cite{liu_2024b} and GCG \cite{zou_2023a}) without diminishing the model’s overall capabilities.

\subsubsection{Prompt Taming}

Harmful user input prompts can be modified to be benign via optimizing a ``safe" suffix for the prompt. Robust Prompt Optimization (RPO) \cite{zhou_2024b} leverages a gradient-based token optimization algorithm that seeks to map worst-case input prompts (i.e. jailbreak prompts) to harmless LLM responses. RPO involves two steps: 1) a jailbreak generation and selection process that applies the worst-case modification to the prompt, 2) a discrete optimization phase that adjusts the suffix to ensure the model's refusal behavior remains intact. RPO was found to be robust to attacks in the JailbreakBench \cite{chao_2024a} and HarmBench \cite{mazeika_2024a} benchmarks and demonstrated superior performance to baseline defenses while incurring only minor effects on benign usage. 

\subsection{Response Reformulation}

Kim et al. \cite{kim_2024b} proposed a method called self-refine and combined it with a novel prompt formatting approach. The combined defense demonstrated exceptional safety performance even in non-safety-aligned language models. Self-refine is an iterative prompting process that leverages the Cost Model \cite{dai_2023a} to detect harmful response coming from the base LLM. When a harmful response is detected, self-refine reformulates the response to remove the harmful content.

\subsection{Randomized Smoothing}

Two innovative defenses, SmoothLLM \cite{robey_2023a} and SemanticSmooth \cite{ji_2024b}, have recently been proposed to mitigate jailbreak attacks by leveraging smoothing techniques \cite{cohen_2019a}. SmoothLLM mitigates adversarial suffix jailbreaks by exploiting the fragility of suffixes to character-level perturbations. By duplicating the input prompt multiple times and subjecting each copy to random character changes, the algorithm significantly reduces the success of the attack.

The average output obtained from these perturbed input samples results in a single, robust response. Notably, SmoothLLM was shown to reduce the attack success rate of the GCG attack \cite{zou_2023a} to below 1\%. SemanticSmooth tackles a broader spectrum of adversarial attacks, including semantic (i.e. prompt-level) attacks that bypass token-based defenses. It achieves robustness without compromising performance by employing semantic transformations such as paraphrasing, summarization, and translation on multiple copies of the input prompt. These transformed predictions are then aggregated to form a final output similar to SmoothLLM. SemanticSmooth uses an input-dependent policy network to adaptively select the appropriate transformations for each input, enhancing its defense against various jailbreaking attacks, including GCG \cite{zou_2023a}, PAIR \cite{chao_2023a}, and AutoDAN \cite{liu_2024b}, while maintaining strong performance on standard benchmarks datasets.

\subsection{Differential Privacy}

In LLMs, differential privacy aims to protect individual data points within a dataset from being exposed through model outputs. By introducing controlled noise into the training process or model parameters, differential privacy ensures that the inclusion or exclusion of any single data point does not significantly affect the overall model behavior. This technique helps mitigate risks such as membership inference, where an attacker could determine if a specific data point was part of the training set, and other privacy breaches. Implementing differential privacy is crucial for maintaining user trust and complying with privacy regulations, especially when handling sensitive information in LLMs. Differentially private stochastic gradient descent (DP-SGD) is generally used to deploy differential privacy into the machine learning process but is resource-intensive when fine-tuning large pre-trained language models. To address these issues, DP-Forward \cite{du_2023b} was proposed, which perturbs embedding matrices during the forward pass of LMs, meeting strict local differential privacy requirements for both training and inference data. Using an analytic matrix Gaussian mechanism (aMGM) to apply minimal matrix-valued noise, DP-Forward perturbs outputs from various hidden layers. This approach achieves near-baseline utility on several tasks, outperforming DP-SGD by up to 7.7\% at moderate privacy levels, while also being three times more efficient in terms of time and memory. However, differential privacy has an inherent trade-off between privacy and accuracy. Thus, it remains a challenging issue to scale it to LLMs.

\subsection{Alignment}

Human alignment of LLMs is essential for preventing harmful responses, misinformation, and susceptibility to jailbreak attacks. Cao et al. \cite{cao_2023a} introduce Robustly Aligned LLM (RA-LLM) which is designed to defend against alignment-breaking attacks. 

RA-LLM integrates a robust alignment checking function into existing aligned LLMs, effectively reducing the attack success rate from nearly 100\% to around 10\% or less without expensive retraining. This approach emphasizes the importance of robust defense mechanisms to maintain alignment against sophisticated adversarial prompts. Similarly, Zhang et al. \cite{zhang_2023b} address the intrinsic conflict between helpfulness and safety by implementing goal prioritization during the training and inference (deployment) stages of the model lifecycle. Their method dramatically lowers the attack success rate and highlights that stronger LLMs, while facing greater safety risks, can also be more effectively steered toward safe behaviors attributing to their superior instruction-following capabilities.

Beyond defensive strategies, alternative approaches focus on optimizing alignment processes themselves. Li et al. \cite{li_2024d} proposed Rewindable Auto-regressive INference (RAIN), which enables LLMs to self-evaluate and adjust their responses to align with human preferences without additional data or retraining. This method significantly improves response harmlessness and truthfulness rates. Meanwhile, Yuan et al. \cite{yuan_2023a} and Dong et al. \cite{dong_2023a} explore reinforcement learning from human feedback (RLHF). Yuan et al. \cite{yuan_2023a} introduce RRHF, which leverages ranking loss to align LLMs efficiently, avoiding the complexities of traditional RLHF methods. Dong et al. \cite{dong_2023a} present RAFT, which enhances model performance by fine-tuning on high-quality samples selected by a reward model. Rafailov et al. \cite{rafailov_2023a} further simplify this by introducing Direct Preference Optimization (DPO), eliminating the need for fitting a reward model and extensive hyperparameter tuning. These advancements collectively contribute to more robust and scalable methods for aligning LLMs with human values, ultimately aiming to mitigate misbehavior in real-world applications.

%\garrison{We can make a table to summarize the protected attack objectives of each defense considering this survey is attack objective oriented.}