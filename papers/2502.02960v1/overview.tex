\section{Technical Overview}

We begin with a technical overview of LLMs before elaborating on the four primary objectives.

\subsection{LLM Architecture}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.48\textwidth]{./fig/interaction.png}
	\caption{A simplified architecture of LLM-based systems.}
	\label{fig:interaction}
\end{figure}

Figure \ref{fig:interaction} presents a simplified architecture of LLM-based system, offering a concise overview of their interaction process. Initially, LLMs start with a pre-training phase, where their parameters (or weights) are randomly initialized. In this phase, the models are exposed to massive datasets drawn from a wide range of corpus, including private and publicly available data, to develop a general understanding of language patterns, syntax, and semantics. This large-scale pre-training helps the LLM learn to predict the next word in a sequence, forming the backbone, namely foundation model, of their language generation capability. Following pre-training, the LLM foundation model can undergo a fine-tuning phase, where they are further trained on more specific, often domain-targeted datasets. This additional training allows the models to specialize in particular tasks or areas of expertise, adapting their learned language capabilities to meet the needs of specific applications, such as question answering, summarization, or translation. 

Once deployed, users can engage with LLMs through an intermediary agent, submitting queries that initiate a dynamic interaction. The agent communicates with the LLM, potentially enhancing its responses through the use of a Retrieval-Augmented Generation (RAG) system, which integrates external knowledge retrieval to improve answer accuracy and relevance. Throughout this process, the interaction is safeguarded by comprehensive guardrail systems, which monitor, filter, and manage inputs and outputs to ensure compliance with safety, ethical, and operational standards. These guardrails play a crucial role in screening queries and responses, ensuring that the information returned to the user aligns with established guidelines for responsible AI use. 

This architecture highlights the intricate flow of data and decisions within LLM system interactions, emphasizing the interplay between core model components, external data augmentation, and protective measures to maintain secure and effective communication.

\subsection{Taxonomy Overview}

Figure \ref{fig:diagram} provides an overview of the four categories of LLM attacks, namely {\em privacy breach}, {\em integrity compromise}, {\em availability disruption} and {\em misuse}. Throughout the paper, we will systematically explore representative attacks within each of these categories. Attackers frequently use a range of techniques to target specific components of LLMs, tailoring their strategies to exploit vulnerabilities within the model's architecture.

\subsection{Key Components}

We summarize four key components of LLMs that are vulnerable to attacks:
\begin{itemize}
    \item{\bf Data} refers to the datasets used during the pre-training or fine-tuning phases of the models, as well as the external knowledge retrieved through Retrieval Augmented Generation (RAG) interface.
    \item{\bf Prompts} are a set of instructions that users provides as queries to LLMs to trigger specific responses or actions.
    \item{\bf Weights} are the learned parameters within the models that influence their decision-making process.
    \item{\bf Gradients} represent the partial derivatives of the model's loss function with respect to its parameters (weights). They signal how the weights should be adjusted to minimize the loss.
\end{itemize}
Attackers attempt to make alterations on such four key components to change the behaviours or extract secret information from LLMs.

\subsection{Techniques}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\textwidth]{./fig/diagram.png}
	\caption{The overview of the four categories of LLM attacks.}
	\label{fig:diagram}
\end{figure}

\begin{table*}[!h]
\centering
    \caption{An example of attacks.}
    \resizebox{0.53\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Objective}                                                & \textbf{Component}                                       & \textbf{Attack}   & \textbf{Technique}                                                \\ \hline
\begin{tabular}[c]{@{}c@{}}Privacy\\ Breach\end{tabular}          & Gradients                                                & \begin{tabular}[c]{@{}c@{}}Model \\ Extraction\end{tabular}   & \begin{tabular}[c]{@{}c@{}}Gradient \\ Leakage\end{tabular}                  \\ \hline
\begin{tabular}[c]{@{}c@{}}Privacy\\ Breach\end{tabular}          &  Data & \begin{tabular}[c]{@{}c@{}}Membership \\ Inference\end{tabular} & \begin{tabular}[c]{@{}c@{}}Weight \\ Poisoning\end{tabular}            \\ \hline
\begin{tabular}[c]{@{}c@{}}Integrity\\ Compromise\end{tabular}    & Data                                                   & \begin{tabular}[c]{@{}c@{}}Data \\ Poisoning\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Data \\ Poisoning\end{tabular}                    \\ \hline
\begin{tabular}[c]{@{}c@{}}Integrity\\ Compromise\end{tabular}    & Weights                                                   & \begin{tabular}[c]{@{}c@{}}Weight \\ Poisoning\end{tabular}    & \begin{tabular}[c]{@{}c@{}}Weight \\ Poisoning\end{tabular}                  \\ \hline
Misuse                                                            & Prompts                                                   & Jailbreak               & \begin{tabular}[c]{@{}c@{}}Prompt \\ Injection\end{tabular}                                                         \\ \hline
\begin{tabular}[c]{@{}c@{}}Availability\\ Disruption\end{tabular} & Prompts                                                   & \begin{tabular}[c]{@{}c@{}}Denial Of \\ Service\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Indirect Prompt \\ Injection\end{tabular}                    \\ \hline
\end{tabular}}
\label{tab:example}
\end{table*}

Attackers tend to leverage various techniques to mount attacks. We begin with a brief introduction to these general techniques.
\begin{itemize}
    \item{\bf Data Poisoning} aims to intentionally inject misleading or malicious data into the training dataset of a model with the goal of causing the model to make incorrect predictions or behave in unintended ways. 

    \item{\bf Weight Poisoning} modifies the parameters (weights) of a models by manipulating the training algorithm or by having access to the model updates. 

    \item{\bf Model Editing} refers to the process of making targeted knowledge changes or updates to the behavior of a pre-trained model by (surgically) modifying its parameters, structure, or by adding specific components. 

    \item{\bf Prompt Injection} involves inserting malicious or deceptive prompts into the input that can manipulate the model into producing harmful or unintended outputs.

    \item{\bf Indirect Prompt Injection} refers to an attack technique where an adversary embeds harmful or manipulative instructions into external content such as webpages, documents, or user-generated data that LLMs might access or be exposed to, without direct user input. 

    \item{\bf Backdoor} is a special type of attack, which can be used for various objectives. It allow attackers to modify the model through data or model manipulation to hijack that the model behaves as expected for regular inputs but produces a pre-defined, incorrect output (but preset output to an adversary) when presented with a trigger-carrying input.

    \item{\bf Contrastive Learning} is a self-supervised learning technique where the model learns to distinguish between similar and dissimilar pairs of data. It involves training the model to bring representations of similar data points (positive pairs) closer together in the embedding space, while pushing representations of dissimilar data points (negative pairs) farther apart. 

    \item{\bf Adversarial Examples} are specifically crafted inputs to mislead the model into making incorrect predictions or generating unexpected outputs. These modifications are often imperceptible to humans but can cause significant errors in the model's performance.

    \item{\bf Side Channel} exploits unintentional information leakage e.g., power, electromagnetic emission, from the physical implementation of a system, rather than directly targeting the algorithm itself. 
\end{itemize}
In Table \ref{tab:example}, we present several examples of attacks where attackers exploit the vulnerabilities of the key components in LLMs to achieve different objectives.









