
\documentclass[10pt]{article} % For LaTeX2e
% \usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}
\let\AND\relax

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\newcommand{\indep}{\mathrel{\perp\!\!\!\perp}}
\usepackage{hyperref}
\usepackage{url}

\usepackage{microtype}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{multicol}
%\usepackage{geometry}
\usepackage{tabularx}
\usepackage{rotating,booktabs}
\usepackage{makecell}
\usepackage{caption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{xcolor}
\usepackage{colortbl}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\title{Multiple Invertible and Partial-Equivariant Function for Latent Vector Transformation to Enhance Disentanglement in VAEs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Hee-Jun Jung \email jungheejun93@gm.gist.ac.kr \\
      \addr Gwangju Institute of Science and Technology\\
      AI Graduate School
      \ANDA
      \name Jaehyoung Jeong \email jaehyoung93@gm.gist.ac.kr \\
      \addr Gwangju Institute of Science and Technology\\
      AI Graduate School
      \ANDA
      \name Kangil Kim\thanks{Corresponding author} \email kikim01@gist.ac.kr \\
      \addr Gwangju Institute of Science and Technology\\
      AI Graduate School}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Disentanglement learning is a core issue for understanding and re-using trained information in Variational AutoEncoder (VAE), and effective inductive bias has been reported as a key factor.
 However, the actual implementation of such bias is still vague.
In this paper, we propose a novel method, called \textit{Multiple Invertible and partial-equivariant transformation} (MIPE-transformation), to inject inductive bias by 1) guaranteeing the invertibility of latent-to-latent vector transformation while preserving a certain portion of equivariance of input-to-latent vector transformation, called \textit{Invertible and partial-equivariant transformation} (IPE-transformation), 2) extending the form of prior and posterior in VAE frameworks to an unrestricted form through a learnable conversion to an approximated exponential family, called \textit{Exponential Family conversion} (EF-conversion), and 3) integrating multiple units of IPE-transformation and EF-conversion, and their training. 
In experiments on 3D Cars, 3D Shapes, and dSprites datasets, MIPE-transformation improves the disentanglement performance of state-of-the-art VAEs.
\end{abstract}

\section{Introduction}
Disentanglement learning to learn more interpretable representations is broadly useful in artificial intelligence fields such as classification~\cite{classi_1}, zero-shot learning~\cite{zero-shot}, and domain adaptation~\cite{domain-1, domain-2}.
The disentangled representation is defined as a change in a single dimension, which corresponds to unique semantic information. 
Several works have been conducted based on this framework.
%, and many works have been developed on this framework.

%A definition of the disentanglement representation~\cite{disen_definitiaon_1} is to learn dimension of representation space corresponding to changing their unique semantic information, and many works have been developed on this framework.

A major model for enhancing the disentanglement learning is Variational AutoEncoder (VAE)~\cite{vae}.
Based on VAE, unsupervised disentangled representation learning has been elaborated~\cite{betaVAE, beta-tcvae, factor-vae,cascadeVAE,hierarchical-vae} through the factorizable variations and control of uncorrelatedness of each dimension of representations.
%A major model used for enhancing the disentanglement is variational autoencoder (VAE)~\cite{vae}. Compared to another unsupervised models as generative adversarial networks ~\cite{gan,info-gan, ib-gan, oogan, info-gan-cr}, more effective disentanglement learning of VAE has been reported in~\cite{beta-tcvae, betaVAE, factor-vae} through the control of uncorrelatedness and total correlation of XXX?.
Moreover, VAE models that handle the shape of prior as a Gaussian mixture~\cite{gaussian-mixture-vae} or von Mises-Fisher~\cite{hyper-vae} were also developed, but the disentanglement is still incomplete.
As a critical point, there is a report that unsupervised disentanglement learning is impossible without inductive bias~\cite{vae-inductive-bias}. 

Recently, such inductive bias has been introduced in various perspectives on transformation of latent vector space. 
Intel-VAE~\cite{intel-vae} proposed the benefit of \textit{invertible} transformation of the space to another latent space to provide better data representation, which includes hierarchical representations.
%learn more complicated data structure, which implies the potential to capture complex form of disentanglement.
Group theory based bias also shows significant improvement on disentanglement~\cite{commutative-vae, groupified-vae}, whose definition follows~\cite{disen_definitiaon_2}, which is based on the group theory.
The works show that \textit{equivariant} transformation between input and latent vector space has a key role of disentanglement.  
%Recently, group theory based inductive bias has shown significant improvement of disentanglement in~\cite{commutative-vae, groupified-vae} based on the definition in~\cite{disen_definitiaon_2}.
%The works transform latent vector space to other space based on group theory, and show that \textit{equivariance} of the transformation has a key role in disentanglement.
%Furthermore, InteL-VAE~\cite{intel-vae} showed the benefit of \textit{invertible} transformation to learn complicated data structure.
%This implicitly implies a possible relation to disentanglement learning.
%%which implicitly implies a possible relation to disentanglement learning.

%To build a transformation always to satisfy properties, 
Inspired by the above works, we propose a \textit{Multiple Invertible and partial-equivariant transformation} (MIPE-transformation) method%\footnote{available on Github, which will be released after publication.}
, which is simply insertable to VAEs.
%First, we assume that encoder is equivariant function over a subset of the group (partial-equivariant)~\cite{partial_equiv}, as we call \textit{encoder equivariance condition} in this paper.
The \textit{partial-equivariance} is defined by~\cite{partial_equiv} and we follow its definition: $f(g \cdot x) = g \cdot f(x)$ $\forall x \in \mathcal{X}, \forall g \in G^\prime, G^\prime \subset G$, where $G^\prime$ is a subset of group $G$ ($f$ is partially equivariant to group $G$).
First, we assume that an encoder is \textit{partial-equivariant} and we call it an \textit{encoder equivariance condition}.
%, which means that a function is equivariant over only a subset of the group~\cite{partial_equiv}.
%We call the partial-equivariant of the encoder as \textit{encoder equivariance condition}.
The method adopts the matrix exponential to hold the invertible property of latent-to-latent (L2L) vector transformation.
%To hold the invertible property of IPE-transformation, the method adopts the matrix exponential.
%, which provides useful properties for analysis of following 
%To hold the invertible property between latent spaces, the method adopts the matrix exponential for latent-to-latent (L2L) transformation. 
Then, we constrain the L2L transformation to a symmetric matrix exponential to be partial-equivariant to a subgroup between latent and transformed latent space.
%partially preserve the equivariance condition between latent and transformed latent space.
Because it extends the encoder to be partial-equivariant to a subgroup between input and transformed latent space.
%Because it partially extends the encoder equivariance condition to the relation between input and transformed latent space.
%partial equivariance condition between latent and transformed latent space. 
%Then, to partially preserve partial equivariance condition between input and latent space to latent, we constrain the latent-to-latent (L2L) transformation to a symmetric matrix exponential, called \textit{invertible and equivariant transformation} (IE-transformation).
%Then, to preserve at least some potential equivariance between input-to-latent (I2L) vector transformation, we constrain the L2L transformation to a symmetric matrix exponential, called \textit{invertible and equivariant transformation} (IE-transformation).
%To increase the probability of being equivariant IPE-transformation, we constrain the transformation to be a symmetric matrix exponential, called \textit{invertible and equivariant transformation} (IE-transformation).
%To increase the probability of being equivariant between latent spaces by the L2L transformation function, we constrain the L2L transformation function to be symmetric matrix exponential.
%To prevent the collapse of equivariant between latent spaces by the transformation function, we constraint the matrix and group elements on latent space to be positive definite symmetric matrix exponential.
%To preserve potential equivariant property between latent and input(?) space in the invertible latent-to-latent space transformation, it constraints the matrix to satisfy XXX conditions.  
%To hold the invertible and equivariant (IE) properties, the method adopts the matrix exponential as the transformation function and is constrained to use a symmetric matrix. 
The IPE-transformation generates an uncertain form of latent vector distributions, so we provide a training procedure to force them to be close to an exponential family, called \textit{exponential family conversion} (EF-conversion).
This conversion enables the uncertain distribution to work in the typical training framework of VAEs. 
%The IPE-transformation generates an uncertain form of distributions; so, we provide a training procedure to be a specific distribution, called the \textit{exponential family conversion} (EF-conversion). 
%It learns VAEs for an uncertain form of distributions through its conversion to an approximated exponential family.
Then, we mathematically show that the multiple uses of IPE-transformation work as $\beta$ parameters~\cite{betaVAE} controlled for enhancing disentanglement learning.
Also, we propose the \textit{implicit semantic mask} to induce a semantic mask in the latent vector space, different to~\cite{Yang_2020_CVPR}.
%Then, we mathematically show that the MIE-transformation function has the same role as $\beta$ parameter, which is controlled for disentanglement learning and it affects the total correlation on $\beta$-VAE~\cite{betaVAE}.
%Then, we mathematically show MIE-transformation function is a same role as $\beta$ parameter, which is controlled for disentanglement learning and it affect to total correlation, on $\beta$-VAE~\cite{betaVAE, beta-tcvae}.   
%Then, we integrate multiple units for IPE-transformation and EF-module to reduce possible accuracy limit by adding the strong constraints as the reported benefit of multiple transformation in~\cite{intel-vae}.
In experiments with quantitative and qualitative analysis, MIPE-transformation shows significant improvement in disentangled representation learning % while maintaining the reconstruction error 
in 3D Cars, 3D Shapes, and dSprites tasks. %and the impact of the harmonized equivariant  functions is discussed in Section~\ref{sec:ablation}.
%InteL-VAE~\citep{intel-vae} proposed ELBO with invertible function to fix KL-term and prove mathematically.
%However, we fix reconstruction error term with invertible function and prove with simple linear algebra theory.
%In addition, we implement more intuitive inductive bias with L2 self-attention~\citep{pmlr-v139-kim21i} than InteL-VAE and conduct trainable invertible function.
Our main contributions are summarized as follows. 
\begin{enumerate}
        \item 
        We propose to use a symmetric matrix exponential as a latent-to-latent vector transformation function for inducing inductive bias based on  invertible and equivariant properties with mathematical analysis.
        %We provide a function to transform latent vector space to another latent space that always holding the invertible and equivariant properties without additional loss to induce them indirectly.
        %We show that matrix exponential with symmetric matrix is invertible and equivariant, and how it injects the inductive bias for disentanglement learning,
        \item We provide a training procedure and losses for VAEs to learn unknown latent vector distribution as an approximated exponential family.
        %\item We provide a general training procedure and losses for VAEs to learn unknown distribution as an approximated exponential family.  
        %We guarantee that utilizing the matrix exponential with symmetric matrix as mapping function transforms Gaussian distribution to the exponential family with trainable parameters, maximizing posterior, and modified KL divergence,
        \item We propose the novel MIPE-transformation architecture to integrate multiple IPE-transformation and EF-conversion, which is widely applicable to state-of-the-art VAEs. 
        %\item We propose the novel MIE-transformation architecture based on the multiple uses of IPE-transformation and EF-conversion. This is broadly applicable to the state-of-the-art VAEs, including group theory based models.
        \item We empirically analyze the properties of MIPE-transformation and validate its effectiveness in disentanglement learning on benchmarks. 
        %\item We validate the MIE properties on the transformation to enhance disentanglement in empirical analysis. 
        % with multi invertible and equivariant functions, and prove utilized our method is equal to the $\beta$-VAE objective  function,
        %\item Proposed method is generally applicable to the state-of-the-art (SOTA) VAEs including group theory based VAEs,
        %\item We generally improve disentanglement learning on dSprites, 3D Shapes, and 3D Cars datasets in the same experiment environmental condition (same hyper-parameters tuning)
        %with group theory based and transforming intermediate conditions and 
        %and empirically show quantitative, qualitative analysis, and ablation studies.
        %\item We empirically show the impact of the number of the invertible and equivariant functions, and symmetric matrix with quantitative and qualitative analysis.
        %\item mapping function is guaranteed to transform Gaussian distribution into exponential family
        %\item improving disentanglement learning based on group theory
\end{enumerate}

\begin{table}[h]
\caption{Terms and Notations}
    \centering
    \begin{tabular}{ll|ll}
    %\multicolumn{2}{c}{Terms and Notations}\\
    \hline
$ \vz$ & Latent vector from encoder & $\psi(\cdot)$ & Invertible function \\
$\hat{ \vz}_m$ & Transformed latent vector by $\psi_m(\cdot)$ & $\hat{\boldsymbol{\epsilon}}_m$ & Transformed prior samples by $\psi_m(\cdot)$ \\
$\boldsymbol{\theta}_{\hat{ \vz}_m}$ & Natural Parameter of posterior &
$\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}$ & Natural Parameter of prior \\
$T$ & Sufficient Statistics &
$A$ & Log-Normalizer \\
$\boldsymbol{\nu}$ & Evidence &
$ \KL(\cdot||\cdot)$ & Kullback-Leibler divergence \\
$f_{ \vx}(\cdot)$ & Power Density Function &
$M_n(\mathbb{R})$ & A set of $n \times n$ real matrix \\
$GL_n(\mathbb{R})$ & General Linear Group &
$Sym_n(\mathbb{R})$ & A set of $n \times n$ symmetric real matrix \\
$E_M$ & $\{ \mathbf{e}^M | M \in M_n(\mathbb{R}) \}$  &
$E_S$ &  $\{ \mathbf{e}^S | S \in Sym_n(\mathbb{R}) \}$ \\
$G_S$ & $G_S:(\mathbf{e}^S, \ast)$ &
$G_I$ & Group of input space for symmetries \\
$G_L$ & Group of latent space for symmetries & %(equivariant to $G_I$) \\
$J$ & $G_S \cap G_L$ \\
$\psi_{M}(\cdot)$ & $\psi_{M}(\cdot) \in M_n(\mathbb{R})$ &
$\psi_{E_M}(\cdot)$ & $\psi_{E_M}(\cdot) \in E_M$ \\ 
$\psi_{E_S}(\cdot)$ & $\psi_{E_S}(\cdot) \in E_S$ & 
$\mathbf{0}$ & zero vector \\
$\mathbf{0}_{n,n}$ & n by n zero matrix &
$\mathcal{X}$ & Input space \\ 
$\mathcal{Z}$ & Latent vector space &
$\hat{\mathcal{Z}}$ & Transformed latent vector space \\
$\Xi$ & $G_I \times G_L \rightarrow G_L$ &
$\Gamma$ & $G_L \times G_T \rightarrow G_T$ \\
$\Xi^J$ & $G_I^J \times G_L^J \rightarrow G_L^J$ &
$\Gamma^J$ & $G_L^J \times G_T^J \rightarrow G_T^J$ \\
%$\mathcal{X}$ $\mathcal{Z}$ $\mathcal{\hat{Z}}$ & $\mathbf{x}_1$ $\mathbf{x}_2$ $\mathbf{z}_1$ $\mathbf{z}_2$ $\mathbf{\hat{z}}_1$ $\mathbf{\hat{z}}_2$
    \hline
    \label{term_notation}
    \end{tabular}
\end{table}




\section{Related Work}

%\begin{itemize}
%    \color{blue}
%    \item \sout{introduce GNN/GCN (disentanglement: graph based)}
%    \item \sout{Invertible and equivariant GNN}
%    \item \sout{Disentanglement learning with node representation by independent nodes}
%    \item \sout{node representation (trough GNN) effect to latent vector (sampling)}
%    \item \sout{multi non-Gaussian distribution (if equations is proved)}

%1\subsection{Unsupervised Disentanglement Learning}
Recently, various works have focused on the unsupervised disentanglement learning. 
Previous works are based on~\cite{disen_definitiaon_1} definition.
%for disentanglement learning 
One of the branches %to improve unsupervised disentanglement learning 
is InfoGAN~\cite{info-gan} based works such as IB-GAN~\cite{ib-gan} implement extra regularizer to improve informativeness~\cite{dci}.
%and OOGAN~\cite{oogan} showed the improvements, but these works regularize informativeness introduced in~\cite{info-gan-cr} to elaborate regularizing mutual information. 
%One of diverse methods have been proposed with InfoGAN~\cite{info-gan} which maximize mutual information (MI) between additional latent code and a generated sample for disentangled representation.
%To elaborate regularizing MI,
%balance of the trade-off between the maximization of lower bound MI, 
%IB-GAN~\cite{ib-gan} and OOGAN~\cite{oogan} proposed minimizing upper bound MI, and sampling one-hot embedding code. 
%and minimization of upper bound MI, IB-GAN~\cite{ib-gan} utilizes information bottleneck.
%OOGAN~\cite{oogan} proposed sampling one-hot embedding code c to affect one factor of latent z and restricts leverage of latent z by compete-free design even though code c dimension is small.
%However, these works are not regularize uncorrelatedness, more focus on %add regularizer to promote 
%informativeness introduced in~\cite{info-gan-cr}.
%group theory를 적용하지 않은 연구들
The other branch is based on the VAE.
%Variational AutoEncoder (VAE).
$\beta$-VAE~\cite{betaVAE} penalizes Kullback-Leibler divergence (KL divergence) with weighted hyper-parameters.
Factor VAE~\cite{factor-vae} and $\beta$-TCVAE~\cite{beta-tcvae} are trained with total correlation (TC) to make independent dimensions on a latent vector with discriminator and divided components of KL divergence term.
%Factor VAE utilizes a discriminator to estimate TC, and $\beta$-TCVAE modifies KL divergence term then, estimates mutual information, dimension-wise KL divergence, and TC. However, \cite{vae-inductive-bias} proves that unsupervised disentanglement learning methods are impossible without inductive bias on both models and data.
Differently, we consider the recent disentanglement definition based on group theory~\cite{disen_definitiaon_2}.
%However, these approaches are not based on the group theory definition in~\cite{disen_definitiaon_2}.

%After \cite{disen_definitiaon_2} defines disentangled representation by group theory, 
%group theory 기반의 연구들
Following the definitions of disentangled representation learning by group theory, several works have emphasized equivariant and improved disentangled representation learning.
Commutative Lie Group VAE (CLG-VAE)~\cite{commutative-vae} proposed direct mapping of the latent vector into Lie algebra to obtain group structure (inductive bias) with constraints: commutative and hessian loss.
%However it needs to fine-tune hyper-parameters with grid search to achieve best disentanglement performance.
Furthermore, Groupified VAE~\cite{groupified-vae} utilizes Spatial Broadcast Decoder~\cite{broadcast-decoder} to implement an equivariant function to the cyclic group with guaranteeing commutativity and invertibility of group actions.
%with the group theory, and it also proves the necessity of the proposed abel loss and order loss with cyclic groups and n-th root unity group to improve disentangled representation.
%It is an applicale method to various VAEs as our method, but it is omitted from comparison because they can not be directly compared by the difference of using n-th root unity group on decoder.
Topographic VAE~\cite{t-vae} combines Student's-t distributions and variational inference. 
It enforces rotated latent vectors to be equivariant.
On the other hand, we apply unrestricted prior and posterior for disentanglement learning.
%However, these models still utilize Gaussian distribution prior and posterior.

There are several inductive biases to learning unsupervised disentanglement, such as group theory based and sequential order.
In this section, we briefly discuss sequential order inductive bias even though its method is considered in different domains such as text and video frames.
To individualize the static (time-invariant) and dynamic (time-variant), \cite{sequential-autoencoder, contrastively-vae} proposed the latent variables one ($f$) is only dependent on the given times series datasets $x_{1:T}$, and the other ($\mathbf{z}_{1:T})$ is dependent on the $x_{1:T}$ and $f$.
Moreover~\cite{contrastively-vae} propose the novel ELBO with maximizing mutual information between the input and the latent vectors.
These works empirically show that sequential order which includes separated latent vectors improves unsupervised disentanglement learning with diverse qualitative analysis.
Differently in group theory based approaches, the proposed methods consider equivariant function between input and latent vector space.

%Furthermore, it utilized ensemble encoder and decoder architecture, and showed no restricted to classification.
%Differently, we consider disentangled representation learning.
%Other works transform or capsulize latent vector z to map other distributions (non-Gaussian) or shift temporal coherence. 

Other VAE approaches implement other prior from Gaussian distribution to transformed Gaussian distribution, Gaussian mixture distribution~\cite{gaussian-mixture} or von Mises-Fisher distribution~\cite{hyper-vae}.
%Other approaches in VAEs, some works have implemented extension prior such as a transformed Gaussian distribution, Gaussian mixture distribution~\cite{mix-gaussian} or von Mises-Fisher distribution~\cite{hyper-vae}.
InteL-VAE~\cite{intel-vae} shows that transformed Gaussian distribution by the invertible function trains hierarchical representation with manual function.
We show more clear relation of invertibility to disentanglement and improve VAEs to use its unrestricted form of prior.
%However, these works rarely or implicitly mention disentanglement learning.
%In contrast, we consider disentanglement learning with unrestricted Gaussian prior. %distribution.
%These works show the benefit of unrestricted Gaussian prior distribution but rarely mention disentanglement learning.
%Nonetheless, these have been still restricted in Gaussian and one single distribution.

%preserves complicated dataset structure, but it utilizes only manual function.
%proves that KL divergence of transformed latent posterior by invertible function is equal to KL divergence of Gaussian distribution 
%however it utilizes manual invertible function and multi-layer perceptron, which have no guarantee of being invertible function.

% Invertibel and equivariant function 연구 but disentanglement X.
Invertible and equivariant Deep Neural Networks have been investigated with normalizing flows.
As proven by~\cite{invertible-matrix-exponential}, utilized matrix exponential on Neural networks is invertible, but it only provides mathematical foundations of the tra
nsformation.
Matrix exponential is utilized to implement an invertible and equivariant function to improve the generative flow compare to linear function~\cite{NEURIPS2020_d3f06eef}.
%Matrix exponential flows are proposed in~\cite{NEURIPS2020_d3f06eef} for equivariant simultaneously.
%non-gaussian을 사용하였지만, disentanglement 언급 X
To specify the exponential familyt, other works contribute uncertainty of exponential family distribution with Bayesian update~\cite{exponential_uncertainty_00, exponential_uncertainty_01}. In addition,~\cite{exp} hierarchically controls the natural parameter across the layers and determines the exponential family distribution with the moment of sufficient statistic.
In our work, we show how to use it for disentanglement learning.


\begin{figure}[t]
%\vskip 0.2in
    \centering
    \centerline{\includegraphics[width=1.0\textwidth]{figure_overview_ver28.jpg}}
    \caption{The overall architecture of our proposed \textit{MIPET}-VAE.
    The invertible and partial-equivariant function $\psi(\cdot)$ for L2L transformation consists of a symmetric matrix exponential to be 1) invertible and 2) partial-equivariant.
    %$\psi(\cdot)$ transform latent vector $ \vz \sim \mathcal{N}(\mu, \Sigma)$ and transformed vector $\hat{ \vz} \sim P_T$.
    Then 3) EF conversion module converges the distribution of unrestricted $\hat{ \vz}$ to be EF with $\mathcal{L}_{el}$ loss.
    Also, it applies KL divergence loss ($\mathcal{L}_{kl}$) between the transformed posterior and prior, which are expressed by the power density function of EF.
    In the last, EF conversion reduces the computational error ($\mathcal{L}_{cali}$) between approximated and true KL divergence.
    4) The reddish color represents the integration parts. The blue figures represent each property.
     The details of the gray box are in Figure~\ref{sub fig:suboverview}.}
    \label{fig:overview}
\end{figure}
% \vskip -0.2in





%\begin{equation}
%    p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{X}, \boldsymbol{\nu}) \propto \text{exp} (\boldsymbol{\theta}^\intercal (\sum_{n=1}^N \mathbf{T}(\mathbf{x}_n) + \boldsymbol{\nu} \mathcal{X}) - \mathbf{A}(\boldsymbol{\theta})).
%    \label{eq:posteior}
%\end{equation}

%\clearpage



\section{Method}
\label{sec:method}
The overview of a VAE equipped with MIPE-transformation is shown in Figure~\ref{fig:overview}.
The MIPE-transformation has three main components: 1) \textit{IPE-transformation Unit} to transform latent vectors with  invertible and partial-equivariant properties, 2) \textit{EF-conversion Unit} to extend VAEs to learn the exponential family distribution of latent vectors, and 3) integrated training and generation process for multiple uses of IPE-transformation and EF-conversion.
%mapping function is satisfied with invertible and equivariant properties for assigning a positive effect to enhancing disentanglement learning and 2) transformed intermediate level latent vectors are in the exponential family.
%For the three main components, we define objective functions as:
%1) a mapping function of latent variables to xxx for assigning positive properties to enhancing disentanglement and 2) its distribution in exponential family. 
%It has two main components: 1) a function to transform late
%to guarantee 1) invertible, equivariant, xxxx, and harmonized mapping from xxx to yyy for improving distentanglement quality and efficiency. 
%and  and 2) its distribution in exponential family for adaptation to general Gaussian VAEs.
%and more justification of invertible and equivariant are in Appendix~\ref{apeendix: justification}.
%please read this page.
%, please read this page.
%More details of each sub term in Eq.~\ref{eq:objective_function} and Eq.~\ref{eq:reg_final} 
%and more details of the loss function are presented in next subsection~\ref{subsec:exp family kl} and Appendix~\ref{appen:objective-function}, please read this page.

%In this section, we present our novel method to improve disentanglement learning in VAEs.
%First, we show exponential of matrix is invertible and equivariant simultaneously with symmetric matrices.
%, which are part of the Hermitian matrix.
%, without an additional loss function.
%Second, we introduce multi non-Gaussian distributions, proposed objective function, and more details of it are dealt with in our supplementary materials.
%In the last, we guarantee that invertible and equivariant functions transform Gaussian distribution to the exponential family by maximizing posterior with given observations and samples from prior, and explain sub-terms of objective function.
%, and estimate KL divergence for exponential family (non-Gaussian distributions).
%In the last, we explain harmonized ELBO term with multi non-Gaussian distributions and it is proved in Appendix
%Before we introduce our method, we show the necessary of invertible and equivariant function for 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Equivariant  part 구체화 할 것!!!%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Invertible and Partial-Equivariant Function for L2L Transformation}
\label{subsec:unit mapping function}
%In this section, we propose an matrix exponential based transformation to guarantee IE properties.
%without additional trainable constraint for disentangled representation learning.
%Finally, the matrix exponential could be utilized as IPE-transformation functions without controllable constraint over introduced conditions to inject inductive bias for disentangled representation learning.
%In this section, we show single matrix exponential could satisfy not only invertible but also equivariant function without any constraint, and additional objective functions.
%Second, we introduce exponential family mapping functions to maximize posterior given observations and samples from prior.
%In the last, we explain harmonized ELBO term by separating Gaussian normal distribution into diverse exponential family.

\subsubsection{Invertible Property by Using Matrix Exponential}

To guarantee the invertible property of IPE-transformation, we use a function $\psi(\cdot) = \mathbf{e}^{ \mM} \ast \cdot$
%, where $\psi(\cdot)$ operates on latent-to-latent space
 for the transformation, where $ \mM$ is in  $n\times n$ real number matrix set $M_n (\mathbb{R})$~\cite{invertible-matrix-exponential}.  
The operator $\ast$ is matrix multiplication, and $\mathbf{e}^{ \mM} = \sum_k^{\infty} \frac{ \mM^k}{k!}$.
%To guarantee invertible property to latent vector transformation, we define the transformation function $\psi(\cdot)$ as: $\psi(\cdot) = \mathbf{e}^{\mathbf{M}_i} \ast \cdot$, 
%where $\mathbf{M}_i \in \mathbf{M}_n (\mathbb{R})$, $ \mathbf{M}_n (\mathbb{R})$ is a set of n by n real number matrix, %set of $ \mathbb{R}^{n \times n}$ 
%for the base form, defined as: 
%$\ast$ is a matrix multiplication, and $\mathbf{e}^{\mathbf{M}_i} = \sum_k^{\infty} \frac{\mathbf{M}_i^k}{k!}$.
%\begin{equation}
    %f = \mathbf{e}^{\mathbf{W}_m}\phi(\mathbf{e}^{\mathbf{W}_{m-1}}\phi(\cdots\phi(\mathbf{e}^{\mathbf{W}_1}x))),\text{where} ~
    %\mathbf{e}^{\mathbf{M}_i} = \sum_k^{\infty} \frac{\mathbf{M}_i^k}{k!}.%, ~\text{and} ~\phi~\text{is an activate function}.
%\end{equation}
%This form guarantees an invertible mapping between vectors in a neural network in~\cite{invertible-matrix-exponential}.
%as $h = \mathbf{e}^{\mathbf{M}_i}\Omega(\mathbf{e}^{\mathbf{M}_{i-1}}\Omega(\cdots \Omega(\mathbf{e}^{\mathbf{M}_1}x))),$ where $\Omega$ is an activate function 
%and $i$ is a randomly chosen index.
Our motivation is to use the benefits of injecting explicit inductive bias for disentanglement~\cite{vae-inductive-bias, intel-vae}.
InteL-VAE effectively extracts hierarchical representation, which includes low-level features (affect to a specific factor) and high-level features (affect to complex factors) with an invertible transformation function~\cite{intel-vae}.
%According to~\cite{intel-vae}, low-level hierarchical representation controls more specific factors than the high-level one, and this work shows extracting disentangled representation.
%the benefits of the invertible property are to learn both low- and high-level hierarchical representation.

%Our motivation is to use the benefits of injecting explicit inductive bias for disentanglement from ~\cite{vae-inductive-bias, intel-vae}.
%The ~\cite{intel-vae}.
%learn complex data distribution and to induce sparsity of activations on latent dimension and hierarchical representation.


%Our motivation is to use the benefits for injecting explicit inductive bias for disentanglement from ~\cite{vae-inductive-bias, intel-vae, pmlr-v97-mathieu19a}.



%Moreover, the invertible property is beneficial for preserving data structure on the posterior distribution and extracting useful features~\cite{intel-vae}. 
%that the benefit induces more explicit inductive bias by satisfying the requirement of disentanglement~\cite{vae-inductive-bias, intel-vae, pmlr-v97-mathieu19a}. 


%PREV
%Let define $M_n(R)$ as the set of $n \times n$ real matrices, and the matrix exponential of $\mathbf{W} \in M_n(R)$ is defined as 
%\begin{equation}
    %f = \mathbf{e}^{\mathbf{W}_m}\phi(\mathbf{e}^{\mathbf{W}_{m-1}}\phi(\cdots\phi(\mathbf{e}^{\mathbf{W}_1}x))),\text{where} ~
%    \mathbf{e}^{\mathbf{W}} = \sum_i^{\infty} \frac{\mathbf{W}^i}{i!}.%, ~\text{and} ~\phi~\text{is an activate function}.
%\end{equation}
%Previous works showed given neural network $h = \mathbf{e}^{\mathbf{W}_m}\phi(\mathbf{e}^{\mathbf{W}_{m-1}}\phi(\cdots \phi(\mathbf{e}^{\mathbf{W}_1}x))),$ where $\phi$ is an activate function is always invertible~\cite{invertible}.
%Also, InteL-VAE showed empirically that transformed latent vectors by invertible function are beneficial to preserving data structure on posterior distribution~\cite{intel-vae}.
%Because $\mathbf{e}^{\mathbf{W}}$ is converge, and $\mathbf{e}^{\mathbf{W}} \in GL_n(R)$, $GL_n(R)$ is the set of $n \times n$ real matrices~\cite{Hall}.
%Motivated by these works, we utilize exponential of matrix for invertible function and show also equivariant without any additional loss function in the next paragraph.%transformed by this function is also equivariant without any additional loss function in the next paragraph.

%\subsubsection{Equivariant Property by Using the Symmetric Matrix}

%\subsubsection{Why is the Invertible Function also to be an Equivariant function?}

%\setlength{\intextsep}{0pt}
%\begin{wrapfigure}{r}{0.40\textwidth}
%  \begin{center}
%    \includegraphics[width=0.40\textwidth]{figures/diagram_ver10.png}
%  \end{center}
%  \caption{
%  The role and the definition of each set are represented in and below the Venn diagram respectively.
  %Diagram of Linear Transformations. 
  %$\psi$ only in $M_n(\mathbb{R})$, $E_M$, and $E_S$ is $\psi_{M}$, $\psi_{E_M}$, and $\psi_{E_S}$, respectively.
%  }
%  \label{sub fig: diagram}
%\end{wrapfigure}

%\begin{figure}[h]
%    \centering
%    \begin{subfigure}[]{0.2\textwidth}
%        \includegraphics[width=\textwidth]{figures/cosets.png}
%    \caption{Visualization of the left cosets and sub-group}
%    \label{sub figure: cosets}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}[]{0.24\textwidth}
%        \includegraphics[width=\textwidth]{figures/negative sample.png}
%    \caption{Prior and posterior distribution}
%    \label{sub figure: distribution}
%    \end{subfigure}
%\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figure_suboverview_02.jpg}
    \caption{$G_I$, and $G_L$ are obtained through encoder $q_\phi$ (encoder equivariance condition).
    %Fig.~\ref{sub fig: partially equivaraince} 
    The left side figure shows the relation between each space and symmetries.
    %If $q_\phi$, and 
    If $\psi(\cdot)$ is equivariant function over all $G_L$, and $G_T$, then %there exist $\Xi$, and 
    there exist $\Gamma$, where $\Gamma: G_L \rightarrow G_T$, and $\Xi \circ \Gamma: G_I  \rightarrow G_T$.
    %The same group is colored blue.
    However, unrestricted $\psi(\cdot)$ has no guarantee to be partial- or full-equivariant.
    The red arrows represent our method: L2L transformation guarantees $\Gamma^J: G_L^J \rightarrow G_T^J$, and $\Xi^J \circ \Gamma^J: G_I^J \rightarrow G_T^J$, given the encoder equivariance condition $\Xi: G_I \rightarrow G_L$.}
    \label{sub fig:suboverview}
\end{figure}

%\begin{figure}[h]
%    \centering
%    \begin{subfigure}[]{0.32\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{figures/partially_equi_ver04.png}
%        \caption{Space and symmetries.
%        } 
%        \label{sub fig: partially equivaraince}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}[]{0.32\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{figures/diagram_ver12.png}
%        \caption{Venn diagram of each group.}
%        \label{sub fig: diagram}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}[]{0.32\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{figures/definition.jpg}
%        \caption{Defitnition of each symbol.}
%        \label{sub fig: definition}
%    \end{subfigure}
    
%    \caption{$G_I$, and $G_L$ are obtained through $q_\phi$.
%    Fig.~\ref{sub fig: partially equivaraince} shows the relation between each space and symmetries.
%    If $q_\phi$, and $\psi(\cdot)$ is equivariant function over all $G_I$, $G_L$, and $G_T$, then there exist $\Xi$, and $\Gamma$, respectively, where $\Xi: G_I \rightarrow G_L$, and $\Gamma: G_L \rightarrow G_T$, and $\Xi \circ \Gamma: G_I  \rightarrow G_T$.
%    However, unrestricted $\psi(\cdot)$ has no guarantee to be equivariant.
%    The red arrows represent our method: L2L transformation guarantees $\Xi^a: G_I^a \rightarrow G_L^a$, and $\Gamma^a: G_L^a \rightarrow G_T^a$, $\Xi^a \circ \Gamma^a: G_I^a \rightarrow G_T^a$.}
%    \label{fig:my_label}
%\end{figure}
%\vskip -0.2in
\subsubsection{Why Should L2L Transformation Be Equivariant?}
Let's consider equivariant function between the input and transformed latent vector space, directly used for a decoder in the VAE frameworks.
%In previous works, equivariance between input and the latent space may have equivariance by disentanglement.
%We assume that previous works are equivariant over a subset of symmetries on the input and the latent vector space because these methods have improved disentanglement learning.
%An original VAE hold equivariance between the two spaces, if its encoder obtains equivariance between input and latent vector space by disentanglement enhancing methods such as [refs].
All L2L transformations do not extend the encoder equivariance condition to the relation between input and transformed latent space.
%Therefore, we apply restricted L2L transformation.
%However, if we apply the unrestricted L2L transformation to the VAE, then there is no guarantee to be equivariant between the input and transformed latent space.
%However, if we apply L2L transformation to the VAE, the final latent vector space is set to the transformed latent vector space, and the equivariance between input and final space is not guaranteed any more. 
This problem is more precisely shown in Figure~\ref{sub fig:suboverview}, which illustrates partial equivariance condition over the input space $\mathcal{X}$, latent vector space $\mathcal{Z}$, and its transformed latent vector space $\hat{\mathcal{Z}}$ with a corresponding group of symmetries $G_I$, $G_L$, and $G_T$, respectively.
%If we use $Z$ as the final space, the equivariance of $\Xi: G_I \rightarrow G_L$ guarantees the equivariance between the input and the final space. 
%If we use $\hat{Z}$ as the final space, the equivariance of composite relation ($\Xi \circ \Gamma: G_I \times G_T \rightarrow G_T$) should hold for maintaining the equivariance between input and the final space, and the equivariance of $\Gamma: G_L\rightarrow G_T$ should also hold as a result. (proven in Appendix X). 
In the VAEs literature, it has not been reported to restrict L2L transformation to guarantee equivariant function between two spaces,
%for $\Gamma$ at all, 
so we propose a solution to guarantee at least a part of symmetries to be equivariant.

%\subsubsection{\hl{Why Invertible function also to be Equivariant function?}}
%\hl{With unrestricted invertible function, there is no guarantee of equivariance between the latent vector and transformed latent vector space, then there is no $\Xi \circ \Gamma: G_I \times G_T \rightarrow G_T$ as} Fig.~\ref{sub fig: partially equivaraince}.
%\hl{On the other hand, if $\psi(\cdot)$ preserves the equivariance between the latent vector and transformed latent vector space, then $\Xi \circ \Gamma$ exists.}
%\hl{Even though the encoder is trained without inductive bias to be equivariant, we assume that encoder $q_\phi$ preserves some symmetries, because previous works have improved the disentanglement performance}~\cite{}.
%Then the group obtained by encoder $G_L: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathcal{Z}$ have corresponding group $G_I: \mathcal{X} \times \mathcal{X} \rightarrow \mathcal{X}$.
%\hl{By the group theory based disentanglement definition} ~\cite{disen_definitiaon_2}, \hl{the equivariant function is necessary for disentanglement learning, so we constrain the invertible function to be equivariant.}

%Equivariance of $\psi(\cdot)$ can get the benefit of invertible property and also partially preserve the  symmetries between the input and latent vector space.
%partial equivariance on I2L vector transformation $q_{\phi}$ such as encoders of VAEs.


\subsubsection{Equivariance Property with Symmetric Matrix Exponential}
To enhance the equivariance of L2L transformation, we set $ \mM$ of $\psi (\cdot)$ to a symmetric matrix. We show that 1) a group with the constraint guarantees equivariance of $\psi(\cdot)$ over the specific group, 2) $\psi(\cdot)$ being equivariant over subset of symmetries between the input space and transformed latent vector space, %the equivariance partially preserves equivariance on I2L transformation after applying $\psi(\cdot)$, 
and 3) the constraint increases the probability of $\psi (\cdot)$ to be in the group (equal to be equivariant over the subset of symmetries).

%We distinguish $M_n(\mathbb{R})$, $E_M$, $E_S$, and $G_S$ as shown in Fig~\ref{sub fig: diagram}, and~\ref{sub fig: definition}. 
%A group $G_S:(\mathbf{e}^{ \mS}, \ast)$ defined on symmetric matrix exponential and matrix multiplication.
%The intersection $\circled{a}: G_S \cap G_L$ is shown in Fig.~\ref{sub fig: partially equivaraince}, and~\ref{sub fig: diagram}.
%Also we distinguish some set: a set of n by n complex matrix $M_n(\mathbb{C})$, a set of real matrix exponential $\mathbf{e}^{M_i}$, a set of real symmetric matrix exponential $\mathbf{e}^{S_i}$, and $\circled{a}: G_S \cap G_L$  as Fig.~\ref{sub fig: diagram}, 
%general linear group $GL_n(\mathbb{C})$, $G_M:(\mathbf{e}^{M_i}, \ast)$, and $G_S:(\mathbf{e}^{S_i}, \ast)$, 
%where $S_i$ is in $n \times n$ real symmetric matrix set $S^n(\mathbb{R})$.
%We use \textit{symmetry} as a group element that holds  that holds affects some aspects of  
%We define symmetries as a transformation that affects some aspects of the system~\cite{disen_definitiaon_2} and assume the group of latent space for symmetries consists of matrices.
%\hl{Even though the encoder is trained without inductive bias to be equivariant, we assume that encoder $q_\phi$ preserves some symmetries, because previous works have improved the disentanglement performance.
%Then the group obtained by encoder $G_L: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathcal{Z}$ have corresponding group $G_I: \mathcal{X} \times \mathcal{X} \rightarrow \mathcal{X}$.}
%The group $G_L$ is composed of $\mathcal{Z}\rightarrow \mathcal{Z}$ transformations that have corresponding  $\mathcal{X} \rightarrow \mathcal{X}$ transformations of a group $G_I$ and there exists an equivariant function defined on the two sets of transformations, where $\mathcal{Z}$ and $\mathcal{X}$ are latent and input vector space. 
We particularly call the transformations as~\textit{symmetries}~\cite{symmetries} to distinguish them from IPE- and I2L-transformations.
% (encoder $q_\phi$ preserves equivariance between input and latent space).
% and $\zeta$ is a bijective function. 
For the generality of our method, we consider an arbitrary VAE model that has no restriction on creating intersections to any set as Figure~\ref{sub fig:suboverview}.
%$G_L$ is not observed, but it is in $GL_n(\mathbb{C})$ as Fig.~\ref{sub fig: diagram}.
% Group notation 4

In the next, we show that matrix exponential with symmetric matrix partially preserves encoder equivariance condition better than other matrices.
%An L2L transformation function $\psi(\cdot)$ is uncertain and affected by all settings of training. To consider an independent setting to such variety of environments, we consider the probability $P(\psi(\cdot))$ of selecting the function as uniform distribution. 
%Then,  
%\begin{prop} 
%\label{prop1}
%$P(\psi_{E_S}(\cdot) \in G_S) > P(\psi_{E_M}(\cdot) \in G_S) > P(\psi_{M}(\cdot) \in G_S)$.
%\end{prop}
%$\textit{Proof.}$ 
%All $\mathbf{e}^{S} \in E_S$ are in $E_M$ since $Sym_n(\mathbb{R}) \subset M_n(\mathbb{R})$. However, $E_M \not\subset E_S$ because $\mathbf{e}^{S} $ is always symmetric but $\mathbf{e}^{M}$ can be an asymmetric matrix (see Appendix~\ref{appendix: proof symmetric} and ~\ref{appendix: proof subset}). 
%Therefore, the probability $P(\psi_{E_M}(\cdot) \in G_S) = \frac{|G_S|}{|E_M|}$ is greater than $P(\psi_{E_S}(\cdot) \in G_S) = \frac{|G_S|}{|E_S|}$. In the same way, $P(\psi_{E_M}(\cdot) \in G_S) > P(\psi_{M}(\cdot) \in G_S) = \frac{|G_S|}{|M_n(\mathbb{R})|}$ because $E_M \subset M_n(\mathbb{R})$ and non-invertible functions are only in $M_n(\mathbb{R})$. 

\begin{proposition}
\label{prop1}
Any $\psi(\cdot) \in G_S$, notated as $\psi_{G_S}(\cdot)$, is equivariant to group $G_S$. 
% with group action $\circ$.
%, where $\circ$ is a matrix multiplication.
%The linear transformation function $\psi(\cdot)_{G_s}$ is equivariant to group $G_S$ with group action $\ast$, if $\psi(\cdot)$ consists of a positive definite symmetric matrix.
\end{proposition}
%\begin{proof}
% 1. G_s is abelian -> all elements are commutative in   G_s. (by definition of abelian group) 
% 2. psi and G_S element are commutative 
% 3. e^M * G_S *z = G_S * e^M * Z -> equivariant ( by definition of equivariance) 

$\textit{Proof.}$ 
The group $G_S$ is closed to matrix multiplication, and its element is always a symmetric matrix by definition. 
Then, any two elements in $G_S$ are commutative because if matrix multiplication of two symmetric matrices is symmetric then both are commutative. 
As a result, $\psi_{G_S}(\cdot)$ and group elements of $G_S$ are commutative ($G_S$ is an abelian group).
Because of the commutativity, $\psi_{G_S}(g_s\circ  \vz) = \mathbf{e}^{ \mS} g_s  \vz = g_s \mathbf{e}^{ \mS}  \vz = g_s \circ \psi_{G_S}( \vz)$ for $g_s\in G_S$ if the group action $\circ$ is set to matrix multiplication, where $\psi_{G_S} \in G_S$.
This equation satisfies the general definition of an equivariant function that a function $f(\cdot)$ is equivariant if $f (g \circ  \vz) = g \circ f( \vz)$ for all $g$ in a group $G$ by matching $f$, $g$, and $G$ to $\psi_{G_S}$, $g_s$, and $G_S$, respectively. $\blacksquare$

%Let $q_\phi$ be the equivariant I2L-transformation over defined on symmetries of $G_I^a$ and $G_L^a$. 
\begin{proposition} 
\label{prop2}
If $q_\phi$ is equivariant over defined on group of symmetries $G_I^a$ and $G_L^a$, then $\psi_{G_S} (q_\phi(\cdot))$ is equivariant to symmetries in $G_I$ corresponding to $G_S \cap G_L$ and $G_T$ corresponding to $G_S \cap G_L$ by the equivariance of $q_\phi$.  
\end{proposition}
$\textit{Proof.}$ The function $\psi_{G_S}(\cdot)$ is an equivariant function over group elements in $G_S \cap G_L$ by Proposition~\ref{prop1}. 
Then, the composite function, $\psi_{G_S}(\cdot)$ and $q_\phi$, is an equivariant function 
of $G_I$ corresponding to $G_S \cap G_L$ and $G_T$ corresponding to $G_S \cap G_L$.
Let $g_L^a$ be a group element in $G_S \cap G_L$, and $g_I^a$ is a group element in $G_I$ corresponding to $G_S \cap G_L$. More details are in Appendix~\ref{appendix: prop2}.
% , and $g_T^a$ is a group element where corresponding to $G_S \cap G_L$ on the latent vector space transformed from the original latent vector space.
% Then, group element $g_T^a$ is equal to $g_L^a$:
% \begin{align}
%     %\begin{split}
%         & \hat{ \vz}_1 = \psi_{G_S}( \vz_1), \text{and} \\
%         & \hat{ \vz}_2 = \psi_{G_S}( \vz_2) = \psi_{G_S}(g_L^a  \vz_1) = g_L^a \psi_{G_S}( \vz_1) ~(\because \text{Prop.~\ref{prop1}}), \\
%     \begin{split}
%         & \text{then} ~g_L^a \psi_{G_S}( \vz_1) = g_T^a \psi_{G_S}( \vz_1) ~(\because \hat{ \vz}_2 = g_T^a \hat{ \vz}_1) 
%         \label{equation: appendix equivariance}\\
%         & \Rightarrow (g_L^a - g_T^a) \psi_{G_S}( \vz_1) = \mathbf{0},
%     \end{split}
% \end{align}
% where $\mathbf{0}$ is a zero vector.
% Eq.~\ref{equation: appendix equivariance} is defined when $\forall  \vz \in \mathcal{Z}$ by the equivariance definition.
% In other words, Eq.~\ref{equation: appendix equivariance} is satisfied only if the kernel (linear algebra) of $g_L^a - g_T^a$, notated as \textit{ker}$(g_L^a - g_T^a)$, includes the basis of $\mathbb{R}^n$ vector space.
% If the standard basis of $\mathbb{R}^n$ vector space is in \textit{ker}$(g_L^a - g_T^a)$, then $(g_L^a - g_T^a)=\mathbf{0}_{n,n}$, where $\mathbf{0}_{n,n}$ is an n by n zero matrix.
% Other bases of $\mathbb{R}^n$ vector space are expressed by the standard basis. Therefore $g_L^a- g_T^a = \mathbf{0}_{n,n}$. 

% Then, $\psi_{G_S}(g_L^a  \vz_1) = g_L^a \psi_{G_S}( \vz_1) = g_T^a \psi_{G_S}( \vz_1)$.
% The encoder is an equivariant function over input space $\mathcal{X}$ as $q_{\phi} (g_I^a  \vx_1) = g_L^a q_{\phi}( \vx_1)$.
% Mixing two equivarience property, we can derive another equivariance relation $g_T^a \psi_{G_S}(q_\phi( \vx_1)) = \psi_{G_S}(q_\phi(g_I^a  \vx_1))$
% This result implies that the equivariance between input space and a latent space is preserved for $G_S \cap G_L$ if the latent vector $ \vz$ is transformed by $\psi_{G_S}. 
$\blacksquare$

\begin{figure}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figure_equivariance_ver01.jpg}
  \end{center}
  \caption{Equivariant map: $\mathcal{X}$, $\mathcal{Z}$, and $\hat{\mathcal{Z}}$ are input space, latent vector space, and transformed latent vector space by L2L transfomration function $\psi(\cdot): \mathbb{R}^n \rightarrow \mathbb{R}^n$. respectively. $ \vx \in \mathcal{X}$, $ \vz \in \mathcal{Z}$, and $\hat{ \vz} \in \hat{\mathcal{Z}}$.}
  \label{figure: equivalence cycle}
\end{figure}


Then,  
\begin{proposition} 
\label{prop3}
$Pr(\psi_{E_S}(\cdot) \in G_S) > Pr(\psi_{E_M}(\cdot) \in G_S) > Pr(\psi_{M}(\cdot) \in G_S)$.
\end{proposition}

$\textit{Proof.}$ 
All $\mathbf{e}^{ \mS} \in E_S$ are in $E_M$ since $Sym_n(\mathbb{R}) \subset M_n(\mathbb{R})$. However, $E_M \not\subset E_S$ because $\mathbf{e}^{ \mS} $ is always symmetric, but $\mathbf{e}^{ \mM}$ can be an asymmetric matrix.
% ~\ref{}.
Therefore $E_M \not\subset E_S$.
Therefore, the probability $Pr(\psi_{E_S}(\cdot) \in G_S) = \frac{P(G_S)}{P(E_S)}$ is greater than $Pr(\psi_{E_M}(\cdot) \in G_S) = \frac{P(G_S)}{P(E_M)}$. In the same way, $Pr(\psi_{E_M}(\cdot) \in G_S) > Pr(\psi_{M}(\cdot) \in G_S) = \frac{P(G_S)}{P(M_n(\mathbb{R}))}$ because $E_M \subset M_n(\mathbb{R})$ and non-invertible functions are only in $M_n(\mathbb{R})$. $\blacksquare$ 


%Theorem~\ref{theorem1} implies that the constraint from $\psi_{E_M}$ to $\psi_{E_S}$ is necessary for preserving equivariance between latent and input vector space over $\circled{a}$. 
Therefore, $\psi_{E_S}$ clearly increases the probability of preserving a certain type of equivariance compared to unrestricted $\psi$ functions. 

The conditional probability $Pr(\psi_{E_S}(\cdot) \in G_S)$, $Pr(\psi_{E_M}(\cdot) \in G_S)$, and $Pr(\psi_{M}(\cdot) \in G_S)$ is changed by the distribution of the observation of $\psi(\cdot)$, which depends on the model parameters.
However, the inequality $Pr(\psi_{E_S}(\cdot) \in G_S) > Pr(\psi_{E_M}(\cdot) \in G_S) > Pr(\psi_{M}(\cdot) \in G_S)$ is not changed regardless of the distribution of observation of $\psi(\cdot)$.
%In practice, however, there are uncertain and undefinable conditions to derive the total probability of preserving all existing equivariance. For example, probability distribution $P(\cdot)$ varies by training settings, so Proposition~\ref{prop3} holds with only uniform or equal distributions determined by training settings for $\psi_{E_M}(\cdot)$ and $\psi_{E_S}(\cdot)$. 
%Additionally, the area of $G_L \backslash G_S$ and its probability are uncertain and depend on I2L transformation functions such as encoders of VAEs.
We empirically validate the impact of equivariance with the uncertain $P(\cdot)$ to disentanglement in Section~\ref{subsec: discussion}.
 

%Let the probability of linear transformation function $\psi(\cdot)$ of being equivariant function over group G as Pr$_G (\psi(\cdot)) \coloneqq \sum_{k} |k|\text{Pr}_G^k (\psi(\cdot))$, and $|k|$ is a ratio of a set $k$ size over size of group.
% latent-to-input equivariance $P$. Then, 




%$\textit{Proof.}$ The function $\psi_{G_S}$ is equivariant function over group elements corresponding to $\circled{a}$, so $\psi_{G_S}$ preserves the equivariance between group elements of \circled{a} and subset of $G_I$, which corresponds to \circled{a} (see Appendix~\ref{appendix: proof equivariance}).
%By proposition~\ref{prop1}, the probability that $\psi_M(\cdot)$, $\psi_{E_M}(\cdot)$, and $\psi_{E_S}(\cdot)$ being equivariant to $\circled{a}$ are determined.

%preserves the equivariance between $\circled{a}$ and their transformed latent space by Lemma~\ref{lem1}.
%The transformed space by $\psi_{G_S}$ has group elements corresponding to $\circled{a}$ which also has corresponding subgroups of $G_I$ in input space.
%Because $\circled{a}$ has one-to-one correspondence with subgroups of $G_I$, $\psi_{E_S}$ preserves the equivariance between group elements of $\circled{a}$ and input space.
%By Proposition~\ref{prop1}, the probability of $\psi_{G_S}$ to preserve the equivariance between latent and input vector space for $\circled{a}$ than $\psi_{M}$ and $\psi_{E_M}$. 

%Theorem~\ref{theorem1} implies that the constraints from $\psi_{E^M}$ to $\psi_{E^S}$ is necessary for preserving equivariance between latent and input vector space over $\circled{a}$. 
%Additionally, we empirically show the number of the better case with a positive definite symmetric matrix is greater than another case in section~\ref{subsec: discussion}.


%$\textit{Proof.}$ Linear transformation function $\psi_{E_S}$ preserves the equivariance to $\circled{a}$.
%The transformed vector space by $\psi_{E_S}$ also has group elements corresponding to $\circled{a}$, then these group elements also correspond to a subgroup of $G_I$.
%Because a set $\circled{a}$ has a one-to-one correspondence with a subgroup of $G_I$, so $\psi_{E_S}$ preserves the equivariance between group elements of $\circled{a}$ and input space.
%The probability of $P(\psi_{M}(\cdot) \in G_S)$, $P(\psi_{E_M}(\cdot) \in G_S)$, and $P(\psi_{E_S}(\cdot) \in G_S)$ are $\frac{|G_S|}{|M_n(\mathbb{R})|}$, $\frac{|G_S|}{|E_M|}$, and $\frac{|G_S|}{|E_S|}$ respectively.
%Each value represents the probability of being equivariant to $\circled{a}$.

%Theorem~\ref{theorem1} implies that the constraint from $\psi_{E_M}$ to $\psi_{E_S}$ is necessary for preserving equivariance between latent and input vector space over $\circled{a}$. 
%Furthermore, we empirically show the number of the better case with a symmetric matrix is greater than another case in Section~\ref{subsec: discussion}.
%Therefore, the constrained linear transformation function from $\psi_{M}(\cdot)$ to $\psi_{E_M}(\cdot)$ is not sufficient to be equivariant function over $\circled{a}$.
%Furthermore, we empirically show the number of the better case with a positive definite symmetric matrix is greater than another case in section~\ref{subsec: discussion}.
%$\psi_{G_S}$ is equivariant function of a set $G_S \cap G_L$ by the Lemma~\ref{lem2}.

    %As shown in Fig.~\ref{sub fig: diagram}, $G_L$ is in $GL_n(\mathbb{C})$, and it is divided into three parts: \circled{a} $G_S \cap G_L$, \circled{b} $(G_M \cap G_L) - G_S$, and \circled{c} $G_L - G_M$.
%We define Pr($\cdot$) as a probability of commutative with group $G_L$.
%By the 5/8 theorem~\cite{commutative}, 
%The general linear group $GL_n(\mathbb{C})$  is non-abelian group, so Pr$_{G_L}^a (\psi_{GL}(\cdot)) < 1$, Pr$_{G_L}^b (\psi_{GL}(\cdot)) < 1$, and Pr$_{G_L}^c (\psi_{GL}(\cdot)) < 1$.
%On the other hand, Pr$_{G_L}^a (\psi_{G_S}(\cdot)) = 1$ because group $G_S$ is abelian, and $\psi_{G_S}(\cdot)$ and \circled{a} are in $G_S$. 
%Also Pr$_{G_L}^b (\psi_{G_S}(\cdot)) < 1$, and Pr$_{G_L}^c (\psi_{G_S}(\cdot)) < 1$.
%Therefore, Pr($\psi_{G_M}(\cdot)$) $<$ Pr($\psi_{G_S}(\cdot)$).
%5/8 theorem + lemma 1 => theorem 1  on Mn. 
%The equivairance is effective. 
%\end{proof}

%\begin{cor} 
%The upper bound of Pr$_{G_L} (\psi_{G_S}(\cdot))$ is greater than the upper bound of Pr$_{G_L} (\psi_{G_M}(\cdot))$.
%\end{cor}



%5/8 theorem + lemma 1 => theorem 1  on GM'. 
%Invertible is not sufficient. 
%\subsubsection{Zero Hessian Matrix}
\subsubsection{Relation Between $\psi(\cdot)$ and Disentanglement}
In addition to  invertible and partial-equivariant properties, our IPE-transformation also guarantees zero Hessian matrix, which enhances disentanglement without any additional loss of~\cite{hessian-penalty}. 
Hessian matrix of the transformation $ \nabla_\vz^2 \psi( \vz) =  \nabla_\vz ( \nabla_\vz \mathbf{e}^{ \mM}  \vz) = 0$ because of the irrelevance of $ \mM$ to $ \vz$. 
%$\frac{\partial^2 \psi(\mathbf{z})}{\partial^2 \mathbf{z}}
%= \frac{\partial}{\partial \mathbf{z}}(\frac{\partial}{\partial \mathbf{z}} \mathbf{e}^{\hat{\mathbf{W}}} \mathbf{z})
%= \mathbf{0}$ because of the irrelevance of $\mathbf{W}$ to $\mathbf{z}$. 
By this property, $\psi(\cdot)$ leads that independently factorizes each dimension~\cite{hessian-penalty},
and it injects group theory based inductive bias simultaneously.
This is because the group decomposition of $ \vz$ space $G = G_1 \times G_2 \times \cdots \times G_k$ corresponds to group decomposition of the transformed latent vector $\hat{ \vz}$ space $G^{\prime} = G_1^{\prime} \times G_2^{\prime} \times \cdots \times G_k^{\prime}$ such that each $G_i^{\prime}$ is fixed by the action of all the $G_j$ for $j \neq i$~\cite{groupified-vae,disentanglement-definition}.
%Prev
%This property and equivariance of $\psi$ leads to group decomposition of $z$ space as $G = G_1 \times G_2 \times \cdots \times G_k$ corresponding to group decomposition of transformed latent vector $\hat{\mathbf{z}}$ space $G^{\dprime} = G_1^{\dprime} \times G_2^{\dprime} \times \cdots \times G_k^{\dprime}$ such that each $G_i^{\dprime}$ is fixed by the action of all the $G_j$ for $j \neq i$~\citep{groupified vae,disentanglement-definition}.
This correspondence of decomposition is expected to transfer the independence between dimensions of $ \vz$ to the space of $\hat{ \vz}$~\cite{disen_definitiaon_2}. 

%Finally, the matrix exponential could be utilized as IPE-transformation functions without controllable constraint over introduced conditions to inject inductive bias for disentangled representation learning.
% and to map posterior and prior close to dataset distribution.




\subsection{Exponential Family Conversion for Unknown Prior}
%\subsection{Maximize a Posterior and Estimate KL Divergence of Exponential Family}
\label{subsec:ef based vae}
In VAE frameworks, the Gaussian normal distribution is applied as a prior.
However, a prior from data is usually unknown and may not follow the Gaussian distribution~\cite{intel-vae}.
%The generated latent variables by IPE-transformation may not follow a Gaussian distribution generally used for the training of VAEs in a non-parametric perspective.
As a solution, we present a training procedure for VAEs to build an exponential family distribution from a latent variable of an arbitrary distribution.
Then, we introduce training losses obtained from the unit IPE-transformation function and EF-conversion.
%, and more details of each procedure are in Equation~\ref{eq:reg_cons}-~\ref{}%Appendix~\ref{appendix: obj function}.% our supplementary materials.

\subsubsection{Elements of Exponential Family Distribution Settings}
First, we set sufficient statistics $T(\cdot)$, log-normalizer $A(\cdot)$, and carrier or base measure $B(\cdot)$ are deterministic functions by maximizing conjugate prior for parameter $\mathcal{\xi}$.
%to estimate non-Gaussian KL divergence in our proposed model.
To determine the \textit{natural parameter} of posterior and prior $\boldsymbol{\theta}_{\hat{ \vz}_m}$, and $\hat{\boldsymbol{\epsilon}}_m$, we use a natural parameter generator (NPG) designed by multi-layer perceptron~\cite{exp_family_bayesian}. 
As introduced in~\cite{prml, exp_family_bayesian}, we assume exponential family always admits a conjugate prior:
\begin{equation}
    q(\boldsymbol{\theta} | \mathcal{\xi}, \boldsymbol{\nu}) = \text{exp} (\boldsymbol{\nu} \boldsymbol{\theta}^\intercal \mathcal{\xi}- \boldsymbol{\nu} A (\boldsymbol{\theta}) + B^\prime(\mathcal{\xi}, \boldsymbol{\nu})),
    \label{eq:conju_prior}
\end{equation}
where $B^\prime(\cdot)$ is a \textit{normalize coefficient} and $\nu$ is evidence, and it is expressed by prior natural parameter $\mathcal{\xi}$.
%then, we parameterize $\nu = \mathcal{X}^\intercal \mathbf{C}$, where $\mathbf{C} \in \mathbb{R}^{n \times n}$.
However, generated natural parameter $\boldsymbol{\theta}_{\hat{ \vz}_m}$ is not guaranteed as the appropriate parameter of the exponential family corresponds to conjugate prior. %and transformed distributions are exponential family.
To satisfy this condition, we assume observation is a set of independent identically distributed, then Eq.~\ref{eq:pdf} is modified: $p(\mathbf{X}|\boldsymbol{\theta}) = \prod_{n=1}^N h(\mathbf{x}_n) \text{exp} (\boldsymbol{\theta}^{\intercal} \sum_{n=1}^N T(\mathbf{x}_n) -A(\boldsymbol{\theta}))$~\cite{prml}, where observation $\mathbf{X}=\{\mathbf{x}_1, \cdots \mathbf{x}_N \}$.
% Equation 6 에서 다 learnable로 바꿨다. 그리고 non-gaussian이 gaussian으로 mapping 가능하다
In the next, we multiply the modified formation by the prior Eq.~\ref{eq:conju_prior} to obtain the posterior distribution~\cite{prml} as Eq.~\ref{eq:posteior_}.

\subsubsection{Distribution Approximation As an Exponential Family} 
The procedure represents a posterior distribution in the exponential family by adopting the following form:
\begin{equation}
%\small
    p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{\xi}, \boldsymbol{\nu}) \propto \text{exp} (\boldsymbol{\theta}^\intercal (\sum_{n=1}^N T(\mathbf{x}_n) + \boldsymbol{\nu} \mathcal{\xi}) - A(\boldsymbol{\theta})),
    \label{eq:posteior_}
\end{equation}
where \textit{sufficient statistics} $T(\cdot)$ and \textit{log-normalizer}, $A(\cdot)$ are known functions, samples $\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n \}$ from distribution, and \textit{natural parameter} of posterior $\boldsymbol{\theta}$ and of prior $\mathcal{\xi}$~\cite{prml}. 
The functions $T(\cdot)$, and $A(\cdot)$ are deterministic functions to maximize posterior distribution.
The \textit{evidence} is implemented as learnable parameters $\boldsymbol{\nu} \in \mathbb{R}^{n \times n}$.
%$\mathcal{X}$ and $ \mC \in \mathbb{R}^{n\times n}$ is used to calculate the evidence $\boldsymbol{\nu} = \mathcal{X}^\intercal  \mC$.
The natural parameter is generated by a multi-layer perceptron as~\cite{exp_family_bayesian}.
This general form approximating an exponential family distribution with learnable parameters can extend VAEs to use a wider distribution for latent variables by simply matching $\mathbf{X}$ to generated latent variables. 
After IPE-transformation, we can apply the form by using the $\hat{ \vz}_m$, $\boldsymbol{\theta}_{\hat{ \vz}_m}$, and $\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}$ for $\mathbf{X}$, $\boldsymbol{\theta}$, and $\mathcal{\xi}$, respectively. %for their representation in an exponential family. 
%More details of the background, conjugate prior,  posterior, and assumptions are in Appendix~\ref{appendix: background}.
% our supplementary material.  

\subsubsection{EF Similarity Loss}
We added a loss to converge the unrestricted distributions of $\hat{ \vz}$ to the power density function of the exponential family by constraining the posterior maximization as: 
%We added a loss to reduce the uncertainty of distributions in the converted exponential family by constraining the posterior maximization as: 
\begin{equation}
    \begin{split}
        \text{maximize} &~\log p(\boldsymbol{\theta}_{\hat{ \vz}_m} | \hat{ \vz}_m, \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}, \boldsymbol{\nu}_m) ~~~\text{s.t.} ~ 
    %\textstyle\sum_{m=1}^{M} 
     \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m})) \geq 0
    \label{eq:reg_cons}
    \end{split}
\end{equation}
\vspace{-3mm}
\begin{equation}
    \begin{split}
        \Rightarrow \mathcal{L}_{s}(\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m) &= \log p(\boldsymbol{\theta}_{\hat{ \vz}_m} | \hat{ \vz}_m, \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}, \boldsymbol{\nu}_m) +  \boldsymbol{\lambda}_m %\textstyle\sum_{m=1}^{M}
     \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}_m}) || f_{\mathbf{x}}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}))
    \end{split}
    \label{eq:reg}
\end{equation}
\vspace{-3mm}
\begin{equation}
    \begin{split}
        \Rightarrow \mathcal{L}_{el} \coloneqq ||  \nabla_{\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m, \boldsymbol{\lambda}_m} \mathcal{L}_{s} ||_2^2 = 0.
        \label{eq:reg_norm}
    \end{split}
\end{equation}

% This provides EF similarity loss in Eq.~\ref{eq:objective_function}.
The notation $\boldsymbol{\theta}_{k}$ is a generated natural parameter by a given $k \in \{ \hat{ \vz}, \hat{\boldsymbol{\theta}} \}$, and $f_{ \vx}( \vx|\boldsymbol{\theta})$ is a power density function of the exponential family.
Moreover, $\boldsymbol{\lambda}_m$ is a trainable parameter for optimizing the Lagrange multiplier, and %$\sum_{m=1}^{M} 
$ \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}))$ is a KL divergence of the exponential family.
% to guarantee KL divergence of the transformed distribution always being positive.
%More details of Eq.~\ref{eq:reg_norm} are in Appendix~\ref{appendix: background}.%our supplementary materials.


\subsubsection{KL Divergence for Evidence of Lower Bound} 
The KL divergence of Gaussian distribution~\cite{vae} is computed using mean and variance, which are the parameters of a Gaussian distribution. 
To introduce a loss as the KL divergence of Gaussian distribution, we compute KL divergence of the exponential family in Eq.~\ref{eq:posteior_} using the learnable parameter %$f_{\mathbf{x}}(\mathbf{x}|\boldsymbol{\theta}_{\hat{\mathbf{z}}})$ and $f_{\mathbf{x}}(\mathbf{x}|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})$ 
$T(\cdot)$ and $A(\cdot)$ with given natural parameter $\boldsymbol{\theta}_{\hat{ \vz}}$ and $\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}$, expressed as:
\begin{equation}
    \begin{split}
         \mathcal{L}_{kl} &\coloneqq  \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m})) \\
        &= A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) - A (\boldsymbol{\theta}_{\hat{ \vz}}) + \boldsymbol{\theta}_{\hat{ \vz}}^{\intercal}  \nabla_{\boldsymbol{\theta}_{\hat{ \vz}}} A(\boldsymbol{\theta}_{\hat{ \vz}}) - \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}^{\intercal}  \nabla_{\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}} A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}).
        \label{eq:kl}
    \end{split}
\end{equation}

\subsubsection{KL Divergence Calibration Loss}
To reduce the error between the approximation and true matrix for the matrix exponential~\cite{matrix_exponential_approx}, we add a loss to minimize the difference of their KL divergence measured by mean squared error (MSE) as: 
\begin{equation}
\begin{split}
   \mathcal{L}_{cali} =\text{MSE}(&  \KL (q_\phi( \vz|
 \vx)||p_\theta( \vz)),  \KL (f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m}))),% ||_2^2,
\label{eq:mse} 
\end{split}
\end{equation}
which is the KL divergence calibration loss ($\mathcal{L}_{cali}$). 






\subsubsection{Implicit Semantic Mask}
We propose an implicit semantic mask to improve disentanglement learning.
%Our method consist of additional EF similarity loss Eq.~\ref{eq:reg_norm}, and KL divergence calibration loss Eq.~\ref{eq:mse}.
%Proposed EF similarity loss and KL divergence of exponential family only effect to prior and posterior.
%To inject advance of disentanglement also in these approaches, we propose sparse log-normalizer A($\cdot$).
We apply mask matrix $\mathcal{M}$ which consists of 0 or 1 element to log-normalizer to prevent less effective weight flow as:
%왜 적용했는지 설명할 것!!!
%To apply mask to less effective weight, we set mask element values by the value of log-normalizer weights.
%Because log-normalizer is consist of multi-perceptrons then its derivative over input is itself.
%Therefore our mask matrix follow as:
\begin{equation}
    %\begin{split}
    \mathcal{M}_{ij} = 
    \begin{cases}
        1 \text{     if } |\mathcal{W}_{ij}| \geq \mathbf{\mu}_{|\mathcal{W}_{ij}|} - \lambda \mathbf{\sigma}_{|\mathcal{W}_{ij}|} \\
        %\frac{1}{ij}\sum_{n=1}^i \sum_{m=1}^j |\mathcal{W}_{nm}| \\
        0 \text{     otherwise}
    \end{cases},
    %\end{split}
    \label{eq:mask}
\end{equation}
where $\mathcal{W}$ is the weight of log-normalizer, $\lambda$ is a hyper-parameter, $\mathbf{\mu}_{|\mathcal{W}_{ij}|}$, and $\mathbf{\sigma}_{|\mathcal{W}_{ij}|}$ are the mean, and standard deviation of weight respectively.
Previous work~\cite{Yang_2020_CVPR} utilizes a semantic mask in input space directly, but we inject the semantic mask implicitly on the latent space.
%Masked weights block the specific latent vector dimension, which affects to a specific factor, then model train with fixed factor information (semantic mask).

%Masking weight reduce some less latent vector dimension, then the output of log-normalizer represent the likelighood of exponential family, which is lower than as dense log-normalizer is used.


\subsection{Integration for Multiple IPE-Transformation and EF-Conversion}
\label{subsec:multiple unit}
%IE-transformation is a process to restrict the distribution of latent vectors for disentanglement, which possibly reduce reconstruction error~\cite{}.
%This trade-off is possibly reduced by increasing the transformation unit as the report in~\cite{}. 
We mathematically extend IPE-transformation to MIPE-transformation, which is the equivalent process of $\beta$-VAE to enhance disentanglement. 
%The equivalence is proven in Appendix~\ref{appendix: elbo}.
Each IPE-transformation function operates independently, then the reconstruction error for objective function is defined as:
%$\log p_{\theta}( \vx|\hat{ \vz}_1, \hat{ \vz}_2, \cdots, \hat{ \vz}_k) =$ $-(k-1) \log p_\theta( \vx) + \Pi_{i=1}^k p_\theta( \vx | \hat{ \vz}_i )$. 
%more details are in Appendix~\ref{appendix: elbo}.
%Therefore, the reconstruction error term ($\mathcal{L}_{rec}$) for given all $\hat{\mathbf{z}}_i$ in Eq.~\ref{eq:objective_function} is 
%supplementary material.
\begin{equation}
% \scriptsize
    \begin{split}
       \mathcal{L}_{rec} &\coloneqq \frac{1}{k} \sum_{i=1}^k \Bigg [ \int q_i(\hat{ \vz}_i| \vx) \log p_\theta( \vx|\hat{ \vz}_i) \mathsf{d}\hat{ \vz}_i \prod_{j=1, j\neq i}^k \int q_j(\hat{ \vz}_j| \vx) %q_3(\mathbf{z}_3|\mathbf{x}) \cdots q_k(\mathbf{z}_k|\mathbf{x})
       \text{d}\hat{ \vz}_j \Bigg ] = \frac{1}{k} \sum_{i=1}^k  E_{q_{\phi,\psi_i} ( \vz| \vx)} \log p_\theta( \vx|\psi_i( \vz)),
    \end{split}
    \label{eq: reconstruction error}
\end{equation}
where $\hat{ \vz}_i = \psi_i( \vz)$.
Therefore, we define ELBO as:
\begin{align}
% \small
    \begin{split}
        \mathcal{L}^\prime(\phi, \theta, \psi_{i \in [1, k]};  \vx) & = \frac{1}{k} \sum_{i=1}^k  \E_{q_{\phi,\psi_i} ( \vz_i| \vx)}  \log p_\theta( \vx|\psi_i( \vz))-
        \sum_{i=1}^k  \KL (q_{\phi, \psi_i}( \vz| \vx) || p_{\psi_i}( \vz)).
        %& = \underbrace{\frac{1}{k} \sum_{i=1}^k \mathbb{E}_{q_{\phi,\psi_i} (\mathbf{z}_i|\mathbf{x})}  \log p_\theta(x|\psi_1(\mathbf{z}), \psi_2(\mathbf{z}) \cdots, \psi_k(\mathbf{z}))}_{\circled{1}~ \big{\text{reconstruction loss}}} - \underbrace{\sum_{i=1}^k \mathcal{D}_{KL} (q_{\phi, \psi_i}(\mathbf{z}|\mathbf{x}) || p_{\psi_i}(\mathbf{z}))}_{\circled{2}~ \big{\text{KL divergence}}}.
    \end{split}
    \label{eq: elbo}
\end{align}
However, following Eq.~\ref{eq: elbo}, k samples are generated, and each sample is disentangled for different factors.
We implement the output as the average of the sum of the k samples to obtain a single sample with a superposition effect from k samples.
%Because we assume $q_{\phi}(\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k|\mathbf{x})$ and $p_{\theta}(\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k|\mathbf{x})$ are conditional independence, where $q_{\phi}(\mathbf{x}|\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k) = q_{\phi}(\mathbf{x}|\mathbf{z}_i)$ and $p_{\theta}(\mathbf{x}|\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k) = p_{\theta}(\mathbf{x}|\mathbf{z}_i)$ then, $p_{\theta}(\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k|\mathbf{x}) = \prod_{i=i}^k p_{\theta}(\mathbf{z}_k|\mathbf{x})$.
Moreover, the KL divergence term in Eq.~\ref{eq: elbo} represents that increasing number of MIPE-transformation is equal to increasing $\beta$ hyper-parameter in $\beta$-VAE~\cite{betaVAE}.



%, and $\text{d} \hat{ \vz} = \text{d}\hat{ \vz}_1 \text{d}\hat{ \vz}_2 \cdots \text{d}\hat{ \vz}_k$.
%However, according to the following Eq.~\ref{eq: reconstrunction error}, $k$ samples are generated, and each sample is disentangled for different factors.
%We implement the output as the average of the sum of the $k$ samples to obtain a single sample with a superposition effect of disentanglement from $k$ samples, as shown in Figure~\ref{fig:overview}.
%More derivation details of Eq.~\ref{eq: reconstrunction error} are in Appendix~\ref{appendix: elbo}.

The VAEs equipped with MIPE-transformation (MIPET-VAEs) can be trained with the following loss:
\begin{equation}
%\footnotesize
\begin{split}
    %\scriptsize
    &\mathcal{L}(\phi, \theta, \psi_{i \in [1, k ]};  \vx) = \mathcal{L}_{rec} - \mathcal{L}_{kl}
    - \mathcal{L}_{el} -\mathcal{L}_{cali}.
     %\underbrace{\frac{1}{k} \sum_{i=1}^k   \E_{q_{\phi,\psi_i} ( \vz| \vx)}  \log p_\theta( \vx|\psi_i( \vz))}_{\circled{1}~ \big{\text{reconstruction loss}}} \\
     %&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
    %-\underbrace{\sum_{i=1}^k  \KL (q_{\phi, \psi_i}( \vz| \vx) || p(\psi_i( \vz)))}_{\circled{2}~ \big{\text{KL divergence}}} - \underbrace{\sum_{i=1}^k \mathcal{L}_{c}(\hat{ \vz}_i, \hat{\boldsymbol{\epsilon}}_i) }_{ \circled{3} ~\big{\text{conversion loss}}},~\text{where} \\
    %&\mathcal{L}_{c} (\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m) = \underbrace{|| \nabla_{\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m, \boldsymbol{\lambda}_m} \mathcal{L}_{s} ||_2^2}_{\tiny \circled{3.1} ~\BBig{\text{EF similarity loss}}} + \underbrace{\text{MSE}(  \KL(q_\phi( \vz| \vx)||p_\theta( \vz)),    \KL(f_{ \vx}( \vx|\vtheta_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m})))}_{\tiny \circled{3.2} ~\BBig{\text{KL divergence calibration loss}}}.
\end{split}
\label{eq:objective_function}
\end{equation}

% The whole process to define objective function is represented in Algorithm~\ref{alg: uipet}-\ref{alg: ef conversion}.




%\begin{table}[h]
%    \caption{VAE architecture for 3D Shapes, and 3D Cars datasets. For exceptional case, CLG-VAE, we ues ten dimension size on 3D Shapes dataset~\cite{commutative-vae}.}
%    \vskip 0.15in
%    \centering
%    \scriptsize
%    \begin{tabular}{c|c}
%    \hline
%         Encoder & Decoder  \\
%         \hline
%         Input $64\times64 \times 3$ RGB image & input $\in \mathbb{R}^{6}$ (3D Shapes), $\mathbb{R}^{10}$ (3D Cars) \\
%         \hline
%         $4 \times 4$ conv. 32 ReLU. stride 2 & FC. 256 ReLU. \\
%         \hline
%         $4 \times 4$ conv. 32 ReLU. stride 2 & FC. $4 \times 4 \times 64$ ReLU. 
%         \\
%         \hline
%         $4 \times 4$ conv. 64 ReLU. stride 2 & $4 \times 4$ upconv. 64 ReLU. stride 2. \\
%         \hline
%         $4 \times 4$ conv. 64 ReLU. stride 2 & $4 \times 4$ upconv. 32 ReLU. stride 2. \\
%         \hline
%         FC. 256. FC. $2 \times 10$ & $4 \times 4$ upconv. 32 ReLU. stride 2. \\
%         \hline
%         & $4 \times 4$ upconv. 3. stride 2 \\
%         \hline
%    \end{tabular}

%    \label{tab:3D Shapess architecure}
%\end{table}

\begin{table}[t]
    \caption{Performance (mean $\pm$ std) of four metrics on dSprites, 3D Shapes, and 3D Cars. 
    %Each value is averaged over 40, 20, and 30 runs of various VAE, $\beta$-VAE, and $\beta$-TCVAE settings, respectively.
    The $\alpha=1$ and $\gamma=1$ of $\beta$-TCVAE as~\cite{beta-tcvae}. 
%    We run VAE, $\beta$-VAE ($\beta$ value), $\beta$-TCVAE ($\beta$ value), and CLG-VAE (group reconstruction hyper-parameter) with 40, 20, and 30 random seeds, repectively.
 %   We follow the $\beta$-TCVAE experiment settings introduced in~\cite{beta-tcvae}, as $\alpha=1$, and $\gamma=1$.
    %Also, we fix hyper-parameters ($\lambda_{decomp}$, and $\lambda_{hessian}$) based on the highest FVM results in~\cite{commutative-vae}  as 20, and 40.} 
    }
    \tiny
    \centering
    \setlength\tabcolsep{4.0pt}
    \begin{tabular}{c|>{\centering}m{0.087\textwidth}*{7}{|>{\centering\arraybackslash}m{0.087\textwidth}}}%{c|>{\centering}m{0.087\textwidth}|m{0.087\textwidth}|m{0.087\textwidth}|m{0.087\textwidth}|m{0.087\textwidth}|m{0.087\textwidth}|m{0.087\textwidth}|m{0.087\textwidth}}%{c|c|c|c|c|c|c|c|c}
    \hline
        \multirow{3}{*}{dSprites} & \multicolumn{8}{c}{Disentanglement Metric} \\
        \cline{2-9}
        & \multicolumn{2}{c|}{FVM $\uparrow$} & \multicolumn{2}{c|}{MIG $\uparrow$} & \multicolumn{2}{c|}{SAP $\uparrow$} & \multicolumn{2}{c}{DCI $\uparrow$}\\
        \cline{2-9}
        & original & MIPET & original & MIPET & original & MIPET & original & MIPET\\
        \hline
        $\beta$-VAE & 69.15($\pm$5.88) & \textbf{74.19}($\pm$5.62) & 9.49($\pm$8.30) & \textbf{19.72}($\pm$11.37) & 2.43($\pm$2.07) & \textbf{5.08}($\pm$2.90) & 18.57($\pm$12.41) & \textbf{28.81}($\pm$10.19) \\
        $\beta$-TCVAE & 78.50($\pm$7.93) & \textbf{79.87}($\pm$5.80) & 26.00($\pm$9.06) & \textbf{35.04}($\pm$4.07) & 7.31($\pm$0.61) & \textbf{7.70}($\pm$1.63) & 41.80($\pm$8.55) & \textbf{47.83}($\pm$5.01) \\
        CLG-VAE & 79.06($\pm$6.83) & \textbf{81.80}($\pm$3.17) &23.40($\pm$7.89) & \textbf{36.34}($\pm$5.55) & 7.37($\pm$0.96) & \textbf{8.03}($\pm$0.83) & 37.68($\pm$7.83) & \textbf{44.73}($\pm$5.11) \\
        Control-VAE & 62.36($\pm$8.62) & \textbf{67.71}($\pm$6.41) & 4.36($\pm$2.86) & \textbf{7.34}($\pm$4.10) & \textbf{2.11}($\pm$1.88) & 1.93($\pm$1.63) & 10.40($\pm$3.42) & \textbf{15.18}($\pm$4.61) \\        
        \hline
    \end{tabular}
    
    \smallskip
    
    \setlength\tabcolsep{4.0pt}
    \begin{tabular}{c|>{\centering}m{0.087\textwidth}*{7}{|>{\centering\arraybackslash}m{0.087\textwidth}}}
    \hline
        \multirow{3}{*}{3D Shapes} & \multicolumn{8}{c}{Disentanglement Metric} \\
        \cline{2-9}
        & \multicolumn{2}{c|}{FVM $\uparrow$} & \multicolumn{2}{c|}{MIG $\uparrow$} & \multicolumn{2}{c|}{SAP $\uparrow$} & \multicolumn{2}{c}{DCI $\uparrow$}\\
        \cline{2-9}
        & original & MIPET & original & MIPET & original & MIPET & original & MIPET\\
        \hline
        $\beta$-VAE & 71.76($\pm$12.26) & \textbf{75.19}($\pm$8.16) & 37.33($\pm$22.34) & \textbf{47.37}($\pm$10.13) & 7.48($\pm$4.12) & \textbf{9.20}($\pm$2.44) & 52.07($\pm$17.92) & \textbf{54.95}($\pm$8.99) \\
        $\beta$-TCVAE & 76.62($\pm$10.23) & \textbf{80.59}($\pm$8.57) & 52.93($\pm$20.5) & \textbf{54.49}($\pm$9.44) & 10.64($\pm$5.93) & \textbf{11.58}($\pm$3.32) & 65.32($\pm$11.37) & \textbf{66.22}($\pm$7.32) \\
        CLG-VAE & 77.04($\pm$8.22) & \textbf{80.17}($\pm$8.43) &49.74($\pm$8.18) & \textbf{53.87}($\pm$7.41) & 9.20($\pm$2.44) & \textbf{12.83}($\pm$3.01) & 57.70($\pm$8.60) & \textbf{60.74}($\pm$7.77) \\
        Control-VAE & 71.05($\pm$14.35) & \textbf{71.89}($\pm$8.33) & 24.88($\pm$13.68) & \textbf{32.28}($\pm$10.74) & 6.60($\pm$3.59) & \textbf{7.14}($\pm$2.09) & 40.08($\pm$13.45) & \textbf{43.06}($\pm$8.68) \\
        \hline
    \end{tabular}
    
    \smallskip
    
    \setlength\tabcolsep{4.0pt}
    \begin{tabular}{c|>{\centering}m{0.087\textwidth}*{7}{|>{\centering\arraybackslash}m{0.087\textwidth}}}
    \hline
        \multirow{3}{*}{3D Cars} & \multicolumn{8}{c}{Disentanglement Metric} \\
        \cline{2-9}
        & \multicolumn{2}{c|}{FVM $\uparrow$} & \multicolumn{2}{c|}{MIG $\uparrow$} & \multicolumn{2}{c|}{SAP $\uparrow$} & \multicolumn{2}{c}{DCI $\uparrow$}\\
        \cline{2-9}
        & original & MIPET & original & MIPET & original & MIPET & original & MIPET\\
        \hline
        $\beta$-VAE & \textbf{89.48}($\pm$5.22) & 88.95($\pm$5.94) & 6.90($\pm$2.70) & \textbf{7.27}($\pm$1.99) & 1.30($\pm$0.48) & \textbf{1.88}($\pm$1.12) & \textbf{19.85}($\pm$4.87) & 18.90($\pm$4.49) \\
        $\beta$-TCVAE & 95.84($\pm$3.40) & \textbf{96.43}($\pm$2.42) & \textbf{11.87}($\pm$2.90) & 10.80($\pm$1.22) & 1.55($\pm$0.38) & \textbf{1.88}($\pm$1.12) & \textbf{27.91}($\pm$4.31) & 26.08($\pm$2.47) \\
        CLG-VAE & 86.11($\pm$7.12) & \textbf{91.06}($\pm$5.09) &6.19($\pm$2.42) & \textbf{8.51}($\pm$2.11) & \textbf{2.06}($\pm$0.60) & 1.99($\pm$0.93) & 16.91($\pm$4.01) & \textbf{18.31}($\pm$2.83) \\
        Control-VAE & 88.76($\pm$7.66) & \textbf{89.10}($\pm$6.90) & 4.68($\pm$2.67) & \textbf{5.08}($\pm$2.68) & 1.16($\pm$0.74) & \textbf{1.45}($\pm$0.86) & 14.70($\pm$3.84) & \textbf{15.22}($\pm$4.15) \\
        \hline
    \end{tabular}

    \label{tab:quantitative analysis}
\end{table}

\begin{table}[t]
    \caption{
    $\textit{p}$-value of t-test for original vs MIPET results of Table~\ref{tab:quantitative analysis}, which are averaged over models (bold: positive and significant, italic: positive but insignificant, normal: lower performance).}
    \scriptsize
    \centering
    \begin{tabular}{c|c|c|c|c||c|c|c|c||c|c|c|c}
    \hline
         \multirow{2}{*}{$\textit{p}$-value}& \multicolumn{4}{c||}{VAEs} & \multicolumn{4}{c||}{CLG-VAE} & \multicolumn{4}{c}{$\beta$-TCVAEs}\\
         \cline{2-13}
         & FVM & MIG &SAP & DCI & FVM & MIG &SAP & DCI & FVM & MIG &SAP & DCI \\
         \hline
         
         dSprites & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.000} & \textbf{0.030} & \textbf{0.000} & \textbf{0.005} & \textbf{0.000} & \textit{0.281} & \textbf{0.000} & \textit{0.170} & \textbf{0.009}\\
         
         3D Shapes & \textbf{0.080} & \textbf{0.007} & \textbf{0.016} & \textit{0.191} & \textit{0.085} & \textbf{0.029} & \textbf{0.000} & \textit{0.088} & \textit{0.111} & \textit{0.383} & \textit{0.277} & \textit{0.390} \\
         
         3D Cars & 0.659 & \textit{0.250} & \textbf{0.003} & 0.583 & \textbf{0.003} & \textbf{0.000} & 0.630 & \textit{0.071} & \textit{0.278} & 0.923 & \textit{0.119} & 0.933 \\
         \hline
    \end{tabular}

%    each models performance, we average each model with 40, 30, and 20 cases on VAEs, CLG-VAE, and $\beta$-TCVAEs, respectively. 
%    The bold textsresults show our results are statistical significant, $\textit{italic}$ is improved performance, but insignificant case, and otherwise is lower performance case.}
    \label{tab:main statistical significant}
\end{table}

\section{Experiment Settings}
\label{sec:experiment}
    %\begin{itemize}
    %    \item synthetic dataset
    %    \item real world dataset (dSprites, 3D Shapess, celebA, or more?)
    %    \item disentanglement 1) quantitative analysis and 2) qualitative anlysis
    %    \item How the multi invertible and equivariant functions affects disentanglement
    %    \item posterior collapse quantitative analysis?
    %\end{itemize}

%\subsection{Data sets}
%We compare well-known VAEs to CHIC-VAEs: VAE, $\beta$-VAE, $\beta$-TCVAE, and commutative VAE on the following data sets with 1) dSprites~\cite{dsprites17} which consists of 737,280 binary $64 \times 64$ images of dSprites with five independent ground truth factors(number of values), $i.e.$ shape(3), orientation(40), scale(6), x-position(32), and y-position(32). 
%2) 3D Shapes~\cite{3dshapes18} which consists of 480,000 RGB $64 \times 64 \times 3$ images of 3D Shapes with six independent ground truth factors: shape(4) orientation(15), scale(8), wall color(10), floor color(10), and object color(10).
%3) 3D Cars~\cite{3d-car-dataset} which constis of 17,568 RGB $64 \times 64 \times 3$ images of 3D Shapes with three independent ground truth factors: car models(183), azimuth directions(24), and elevations(4).
%In the last, we use 3) CelebA (cropped version)~\cite{celebA(64)} which consists of 202,599 RGB $64 \times 64 \times 3$ images of celebrity faces.

%\subsection{Setting}


\subsubsection{Models}
As baseline models, we select VAE, $\beta$-VAE, $\beta$-TCVAE, and CLG-VAE. These models are compared to their extension to adopt MIPET, abbreviated by adding the MIPET prefix.
We apply the proposed method to $\beta$-TCVAE only with the EF similarity loss term because $\beta$-TCVAE penalizes the divided KL divergence terms.
We set the same encoder and decoder architecture in each model to exclude the overlapped effects.
Also, we follow the same model architecture which are introduced in previous works~\cite{factor-vae} and model details are in Table~\ref{tab:dsprites architecure}-\ref{tab:others architecure}.
%\hl{To check the impact of MIE-transformation, we do not consider the Groupified VAE because the latter is implemented with an extended decoder (different capacity).}
%Model architectures are in Table~\ref{tab:dsprites architecure}-\ref{tab:others architecure}, and Appendix~\ref{appendix: model details}.
%Groupified VAE is excluded from method comparison
%Groupified VAE is excluded from method comparison because of its decoder architecture incompatible to the common decoder. 
%the other decoders.  incompatible decoder architecture to  
%two factors affect to disentanglement learning, groupified method and 2) different architecture of decoder.
%because it is improper to evaluate the impact of intermediate transformation of the latent vector space by large changes of its decoder architecture,

\subsubsection{Datasets}
We compare well-known VAEs to CHIC-VAEs: VAE, $\beta$-VAE, $\beta$-TCVAE, and CLG-VAE on the following data sets with 1) dSprites~\cite{dsprites17} which consists of 737,280 binary $64 \times 64$ images of dSprites with five independent ground truth factors(number of values), $i.e.$ shape(3), orientation(40), scale(6), x-position(32), and y-position(32). 
2) 3D Shapes~\cite{3dshapes18} which consists of 480,000 RGB $64 \times 64 \times 3$ images of 3D Shapes with six independent ground truth factors: shape(4) orientation(15), scale(8), wall color(10), floor color(10), and object color(10).
3) 3D Cars~\cite{3d-car-dataset} which consists of 17,568 RGB $64 \times 64 \times 3$ images of 3D Shapes with three independent ground truth factors: car models(183), azimuth directions(24), and elevations(4).
%The dSprites which consists of 737,280 binary $64 \times 64$ images of dSprites with five independent ground truth factors(number of values), $i.e.$ shape(3), orientation(40), scale(6), x-position(32), and y-position(32).
%3D Shapes which consists of 480,000 RGB $64 \times 64 \times 3$ images of 3D Shapes with six independent ground truth factors: shape(4) orientation(15), scale(8), wall color(10), floor color(10), and object color(10).
%2) 3D Shapes~\cite{3dshapes18} which consists of 480,000 RGB $64 \times 64 \times 3$ images of 3D Shapes with six independent ground truth factors: shape(4) orientation(15), scale(8), wall color(10), floor color(10), and object color(10).
%3) 3D Cars~\cite{3d-car-dataset} which consists of 17,568 RGB $64 \times 64 \times 3$ images of 3D Shapes with three independent ground truth factors: car models(183), azimuth directions(24), and elevations(4).
%We used dSprites~\cite{dsprites17}, 3D Shapes~\cite{3dshapes18}, and 3D Cars~\cite{3d-car-dataset}, which are commonly evaluated for disentanglement research on VAEs. Their statistics are shown in our supplementary material.
%We compare well-know VAEs such as VAE, $\beta$-VAE, $\beta$-TCVAE, and commutative VAE to their adopted MIE-transformation function in dSprites~\cite{dsprites17}, 3D Shapes~\cite{3dshapes18}, and 3D Cars~\cite{3d-car-dataset}.
%More information of datasets are in Appendix~\ref{appendix: dataset details}.%our supplementary materials.
%PREV
%We compare well-known VAEs to MIPET-VAEs: VAE, $\beta$-VAE, $\beta$-TCVAE, and commutative VAE on the following data sets with dSprites~\cite{dsprites17}, 3D Shapes~\cite{3dshapes18}, and 3D Cars~\cite{3d-car-dataset}. More details of datasets are in our supplementary materials.

\subsubsection{Training}
%We set 256 mini-batch size in the datasets (dSprites, 3D Shapes, and 3D Cars), Adam optimizer with learning rate $4 \times 10^{-4}$, $\beta_1=0.9$, and $\beta_2=0.999$ as a common setting for all the comparative methods.
%For the comparison, we follow training and inference on the whole dataset.
%We train each model for 30, 67, and 200 epochs on the dSprites, 3D Shapes, and 3D Cars, respectively, as introduced in~\cite{factor-vae, 3d-car-setting}. 
%We tune $\beta$ from $\{1, 2, 4, 10\}$ and $\{4, 6 \}$ for $\beta$-VAE and $\beta$-TCVAE, respectively.
%We set the dimension size of the latent vectors from $\{6, 10\}$ for 10 on dSprites and 3D Cars datasets, and 6 for 3D Shapes, but we set 10 for CLG-VAE because it sets 10 dimensions size on 3D Shapes in~\cite{commutative-vae}.
%Regarding the CLG-VAE, we fix $\lambda_{\text{decomp}}$, $\lambda_{\text{hessian}}$, and forward group features as 40, 20, 0.2 respectively.
%Because the hyper-parameters showed the best result in~\cite{commutative-vae}.
%We set group reconstruction from $ \{0.2, 0.5, 0.7\}$.
%More details of the setting are in Appendix~\ref{appendix: model details}.%, please read this page.

We set 256 mini-batch size in the datasets (dSprites, 3D Shapes, and 3D Cars), Adam optimizer with learning rate $4 \times 10^{-4}$, $\beta_1=0.9$, $\beta_2=0.999$, and epochs from $\{30, 67, 200\}$ as a common setting for all the comparative methods.
For the comparison, we follow training and inference on the whole dataset.
We train each model for 30, 67, and 200 epochs on the dSprites, 3D Shapes, and 3D Cars, respectively, as introduced in~\cite{factor-vae, 3d-car-setting}. 
We tune $\beta$ from $\{1, 2, 4, 10\}$ and $\{4, 6 \}$ for $\beta$-VAE and $\beta$-TCVAE, respectively.
We set the dimension size of the latent vectors from $\{6, 10\}$ for 10 on dSprites and 3D Cars datasets and 6 for 3D Shapes, but we set 10 for CLG-VAE because it sets 10 dimensions size on 3D Shapes in~\cite{commutative-vae}.
Regarding the CLG-VAE, we fix $\lambda_{\text{decomp}}$, $\lambda_{\text{hessian}}$, and forward group features as 40, 20, and 0.2, respectively.
Because the hyper-parameters showed the best result in~\cite{commutative-vae}.
We set group reconstruction from $ \{0.2, 0.5, 0.7\}$.
For Control-VAE, we set the maximum KL divergence value from $\{10, 12, \ldots, 20 \}$.
%We set weight decay 0.0 and 1e-4 for the MIE-transformation module, and VAEs (encoder, decoder), respectively.
%Exceptionally, we set weight decay as 0.0 on $\beta$-VAE and CLG-VAE with 3D Shapes dataset for reproduction.
In addition, we set masking ratio $\lambda$ from $\{0.0, 0.5, \ldots 2.0, \infty \}$.
To check the impact of MIPE-transformation, we do not consider the Groupified VAE because the latter is implemented with an extended decoder (different capacity).
%directly. 
%Groupified VAE is an applicale method to various VAEs as our method, but it is omitted from comparison because they can not be directly compared by the difference of using $n$-th root unity group on decoder.
%Although groupified VAE could be applied to diverse VAEs as our proposed method, we do not implement it because both methods could not compare directly.
%Because groupified VAE decoder architecture is different by n-th root unity group~\cite{groupified vae}.

%\begin{figure}
    %\centering
    %\includegraphics[width=\textwidth]{figures/reconst-disent_ver06.png}
    %\caption{Reconstruction error plotted over the FVM metric. Positive is the increased FVM value and decreased reconstruction error case, and negative is the reverse case of positive. If otherwise, it is neutral. 
    %Each value is the mean of 10 random seeds over the whole dataset after completing training.}
    %\label{fig:reconst-disent}
%\end{figure}



%\begin{figure}
%    \centering
%    \begin{subfigure}[]{0.32\textwidth}
%        \includegraphics[width=\textwidth]{figures/kl_div_dsprites.png}
%        \caption{dSprites}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}[]{0.32\textwidth}
%        \includegraphics[width=\textwidth]{}
%        \caption{3D Shapes}
%    \end{subfigure}
%    \hfill
%    \begin{subfigure}[]{0.32\textwidth}
%        \includegraphics[width=\textwidth]{figures/kl_div_3dcar.png}
%        \caption{3D Cars}
%    \end{subfigure}
%    \vfill
%    \begin{subfigure}[]{0.99\textwidth}
%        \includegraphics[width=\textwidth]{figures/legend.png}
%    \end{subfigure}
%    \caption{KL divergence for baselines and CHIC-VAEs on real world datasets for 10 random seeds.} %CHIC-VAEs are represented by dot line.}
%    \label{fig:kl_divergence}
%\end{figure}

\subsubsection{Evaluation} 
We conduct experiments on  NVIDIA A100, RTX 2080 Ti, and RTX 3090. We set 100 samples to evaluate global empirical variance in each dimension and run it a total of 800 times to estimate the FVM score introduced in~\cite{factor-vae}.
For the MIG~\cite{beta-tcvae}, SAP~\cite{sap}, and DCI~\cite{dci}, we follow default values introduced in~\cite{Michlo2021Disent}, training and evaluation 100 and 50 times with 100 mini-batches, respectively.
We evaluate four disentanglement metrics for a less biased understanding of the actual states of disentanglement.
%We conduct four disentanglement metrics to estimate disentanglement score over each model.
%The used metrics include FactorVAE metric (FVM)~\cite{factor-vae}, MIG metric~\cite{beta-tcvae}, SAP metric~\cite{sap}, and DCI metric~\cite{dci}.
%More details of the evaluation metric are in Appendix~\ref{appendix: quantitative}.% our supplementary materials.
%We set 100 samples to evaluate global empirical variance in each dimension and run it a total of 800 times to estimate FVM score introduced in~\cite{factor-vae}.
%For the other metrics, we run training and test 100 and 50 times with 100 mini-batches, respectively.
%In the rest of the metrics, we set 100 mini-batches and run 100, and 50 times for training and test.

%\subsubsection{Qualitative Analysis.}



\begin{table}[t]
    
    \caption{Impact of the number of MIPE-transformation function on the $\beta$-TCVAE and $\beta$-VAE with dSprites, 3D Shapes, and 3D Cars datasets in terms of the four metrics.
    The blue and red box plots represent each model's single and multiple IPE-transformation cases, respectively. (A-$n$: MIPET-$\beta$-TCVAE (4), B-$n$: MIPET-$\beta$-TCVAE (6), C-$n$: MIPET-$\beta$-VAE, $n$: the number of MIPE-transformation).}
    \centering
    \begin{tabular}{c|>{\centering\arraybackslash}m{30mm}|>{\centering\arraybackslash}m{30mm}|>{\centering\arraybackslash}m{30mm}|>{\centering\arraybackslash}m{30mm}}
        \hline
         \multirow{2}{*}{dataset} & \multicolumn{4}{c}{Metrics} \\
         \cline{2-5}
         & FVM &IMG & SAP & DCI \\
         \hline
         dSprites & 
         \includegraphics[trim=0 0 0 -5, width=0.18\textwidth]{figure_dSprites_FVM.png} & \includegraphics[width=0.18\textwidth]{figure_dSprites_MIG.png} & \includegraphics[width=0.18\textwidth]{figure_dSprites_SAP.png} & \includegraphics[width=0.18\textwidth]{figure_dSprites_DCI.png}\\
         \hline
         3D Shapes & 
         \includegraphics[trim=0 0 0 -5, width=0.18\textwidth]{figure_3Dshape_FVM.png} &
         \includegraphics[width=0.18\textwidth]{figure_3Dshape_MIG.png} &
         \includegraphics[width=0.18\textwidth]{figure_3Dshape_SAP.png} &
         \includegraphics[width=0.18\textwidth]{figure_3Dshape_DCI.png}\\
         \hline
         3D Cars & 
         \includegraphics[trim=0 0 0 -5, width=0.18\textwidth]{figure_3Dcar_FVM.png} &
         \includegraphics[width=0.18\textwidth]{figure_3Dcar_MIG.png} &
         \includegraphics[width=0.18\textwidth]{figure_3Dcar_SAP.png} &
         \includegraphics[width=0.18\textwidth]{figure_3Dcar_DCI.png}\\
         \hline
         
    \end{tabular}

    \label{tab:main impact of IE}
\end{table}

\begin{table}[t]

    \caption{Impact of the mask (mean$\pm$std.) and its ratio $\lambda$ in Eq.~\ref{eq:mask} on 3D Cars.
    ($\infty$: no masking case, 
    %We set $\lambda$ from $\{0.0, 0.5, \ldots, 2.0, \infty \}$ for hyper-parameter tuning, and $\infty$ represents no masking case.
    gray box: the best setting over all metrics, bold text: the best in each metric.)
%    We represent best case for all four metrics as gray, and bold represents the highest value.
    Each model runs with ten random seeds.}
    \tiny
    \centering
    
    \begin{tabular}{c|c|c|c|c||c|c|c|c}
    %\begin{tabular}{>{\centering}m{0.03\textwidth}|>{\centering}m{0.09\textwidth}*{3}{|>{\centering\arraybackslash}m{0.09\textwidth}}*{1}{||>{\centering\arraybackslash}m{0.09\textwidth}}|>{\centering}m{0.09\textwidth}*{3}{|>{\centering\arraybackslash}m{0.09\textwidth}}}
        \hline
        \multirow{1}{*}{ratio} & \multicolumn{4}{c||}{$\beta$-VAE (1)} & \multicolumn{4}{c}{CLG-VAE (0.5)}\\
        \cline{2-9}
        $\lambda$ & FVM $\uparrow$ & MIG $\uparrow$ & SAP $\uparrow$ & DCI $\uparrow$ & FVM $\uparrow$ & MIG $\uparrow$ & SAP $\uparrow$ & DIC $\uparrow$ \\
        \hline
        0.0 & 90.46($\pm$6.50) & 4.84($\pm$2.32) & 1.29($\pm$0.81) & 16.76($\pm$4.68) & \cellcolor{gray} \textbf{90.06}($\pm$4.44) & \cellcolor{gray}\textbf{9.28}($\pm$2.09) & \cellcolor{gray}1.82($\pm$0.82) & \cellcolor{gray}\textbf{19.12}($\pm$3.41) \\
        0.5 & 91.35($\pm$5.52) & 5.37($\pm$2.74) & 1.17($\pm$0.67) & 16.65($\pm$3.76) & 88.69($\pm$4.78) & 6.90($\pm$1.96) & 1.85($\pm$0.67) & 17.52($\pm$3.16) \\
        1.0 & \textbf{91.78}($\pm$6.20) & 4.99($\pm$2.27) & 1.36($\pm$0.81) & 16.50($\pm$2.53) & 83.60($\pm$11.48) & 8.12($\pm$3.66) & \textbf{2.37}($\pm$1.50) & 17.07($\pm$3.89) \\
        1.5 & \cellcolor{gray} 90.04($\pm$5.88) & \cellcolor{gray}\textbf{7.22}($\pm$2.87) & \cellcolor{gray} \textbf{1.36}($\pm$0.48) & \cellcolor{gray}\textbf{18.23}($\pm$2.84) & 84.76($\pm$6.86) & 7.70($\pm$2.11) & 2.05($\pm$0.73) & 17.06($\pm$2.77)\\
        2.0 & 87.79($\pm$8.88) & 4.75($\pm$2.49) & 1.01($\pm$0.99) & 16.64($\pm$3.75) & 85.78($\pm$4.18) & 7.83($\pm$1.79) & 1.91($\pm$0.96) & 17.26($\pm$2.07)\\ 
        \hline
        $\infty$ & 89.43($\pm$11.72) & 3.74($\pm$2.32) & 0.77($\pm$0.39) & 15.45($\pm$4.59) & 82.96($\pm$11.84) & 8.07($\pm$2.52) & 2.32($\pm$1.02) & 17.46($\pm$4.07)\\
        \hline
    \end{tabular}

    %More results of a different dataset are represented in our supplementary materials.}
    \label{tab:mask}
\end{table}

\section{Results and Discussion}
\label{sec:results}
\subsection{Quantitative Analysis}

\subsubsection{Disentanglement Metrics}
We set the number of IPE-transformation functions to be equal to balancing hyper-parameter $\beta$ on $\beta$-VAE because of Eq.~\ref{eq:objective_function}.
The number of IPE-transform functions of $\beta$-TCVAE is 3.
However, in the case of CLG-VAE, we set it to 1 because its approach is based on the group theory, not directly controlling a KL divergence term such as $\beta$-VAE.
We average each model performance value with 40, 20, 60, and 30 cases in VAEs, $\beta$-TCVAEs, Control-VAE and 
CLG-VAEs, respectively.
%Commutative Lie Group VAE (CLG-VAEs) respectively.
%and $\beta$-TCVAE.
%Increasing the number of functions is equal to increasing $\beta$ in $\beta$-VAE and it is more related to controlling $\beta$ impact not group theory impact, thereby in the case of Commutative VAE, we more focus on the effect of group based concept.

As shown in Table~\ref{tab:quantitative analysis}, MIPET-VAEs disentanglement performance is broadly improved with four metrics on each dataset.
In particular, most FVM results significantly affect the model performance and stability on all datasets.
Therefore, our proposed method obtains a specific dimension that corresponds to a specific single factor.
%Although the DCI on 3D Cars shows lower results in MIPET-VAEs than in original cases, results of other metrics show significant improvement.
%Although the DCI on 3D Cars and 3D Shapes does not show consistent results, other metrics’ results show significant improvements.
These results imply that applied to MIPE-transformation functions on VAEs elaborate disentangled representation learning.% learning performance.

We additionally estimate the \textit{p}-value of each metrics over models in Table~\ref{tab:main statistical significant}. Previous work shows the average case of each models~\cite{groupified-vae}.

We divide each case into four categories: 1) Positive $\&$ Significant, 2) Positive $\&$ Insignificant, 3) Negative $\&$ Insignificant,  and 4) Negative $\&$ Significant, where positive is when the mean value is higher than baseline and significant is statistically significant. We estimate the probability of each category: 1) 50$\%$, 2) 36.11$\%$, and 3) 13.89$\%$.
As shown in Table~\ref{tab:main statistical significant} and the results, half of the cases are statistically significant, and 86.11$\%$ of cases are improved model performance. Even though our method shows a lower value than the baseline, it is not significantly decreased (13.89$\%$). In addition, averaged results show that our method impacts to model itself without hyper-parameter tuning.
$\beta$-TCVAEs is partially using our method (paragraph Models in Section~\ref{sec:experiment}), so it does not show the whole effect of MIPET, but it improves model performance in many cases. 

%Overall, results are stably increased in terms of FVM compare to other metrics which are estimated with ground-truth labels.

%\subsubsection{Reconstruction Error vs. FVM Metric}
%The trade-off between the reconstruction error and FVM has been shown in previous works~\cite{factor-vae, commutative-vae}.
%We plot the reconstruction error against the FVM metric as shown in Fig.~\ref{fig:reconst-disent}, and more results against other metrics are in Appendix~\ref{appendix: reconst vs metrics}.
%In particular, mean values of 3D Shapes show that both reconstruction error and FVM metric value are improved. 
%Although the results of other datasets do not show the same effect as the 3D Shapes results, their reconstruction errors are barely sacrificed while the disentangling effect is sharply increased.

%These results show that our proposed method improves the disentanglement on the complicated (more factor) datasets.
% such as 3D Shapes.
%The 3D Cars result, which is similar to the dSprites case, is presented in our supplementary materials.
%Indeed, MIPET-VAE case (3D Cars) shows better reconstruction error and FVM performance than VAE, which is normally presented as the lowest reconstruction error in previous works~\cite{factor-vae, commutative-vae}. 

%We illustrate KL divergence values during training, it is directly related to disentangled representation shown in~\cite{beta-tcvae, betaVAE, factor-vae}.
%As shown in Fig.~\ref{fig:recon}, CHIC-VAEs improve KL divergence compared to corresponding models except Commutative VAE (0.5) on the 3D Cars dataset.
%It shows the necessity of the exponential family because penalizing KL divergence on the exponential family is a much more effective method for disentanglement learning.



\begin{table}[t]
    \caption{%Ablation study of the impact of IPE-transformation functions (w/o symmetric, and w/o matrix exponential) and EF-conversion (w/o EF) on Eq.~\ref{eq:objective_function} with 3D Cars dataset.
    Ablation study for the equivariant property (w/o E), and EF-conversion (w/o EF).
    Each metric is averaged over 40 and 20 settings of $\beta$-VAE and $\beta$-TCVAE, respectively.} 
    \centering
       
    % \begin{tabular}{>{\centering\arraybackslash}m{0.07\textwidth}|>{\centering\arraybackslash}m{0.09\textwidth}*{2}{|>{\centering\arraybackslash}m{0.09\textwidth}}||>{\centering\arraybackslash}m{0.09\textwidth}*{2}{|>{\centering\arraybackslash}m{0.09\textwidth}}}
    % %\begin{tabular}{|>{\centering}m{0.05\textwidth}|>{\centering}m{0.11\textwidth}*{2}{|>{\centering\arraybackslash}m{0.11\textwidth}}*{1}{||>{\centering\arraybackslash}m{0.11\textwidth}}|>{\centering}m{0.11\textwidth}*{2}{|>{\centering\arraybackslash}m{0.11\textwidth}}|}
    %     \hline
    %     \multirow{3}{*}{3D Cars} & \multicolumn{3}{c||}{$\beta$-VAE} & \multicolumn{3}{c}{$\beta$-TCVAE}\\
    %     \cline{2-7}
    %     &  MIPET & MIPET \hspace{5mm} (w/o E) & MIPET \hspace{5mm} (w/o EF) & MIPET & MIPET \hspace{5mm} (w/o E) & MIPET \hspace{5mm} (w/o EF)  \\
    %     \hline
    %     FVM $\uparrow$ & 
    %     \textbf{88.95}($\pm$5.94) & 82.09($\pm$11.33) & 45.23($\pm$6.39) & \textbf{96.43}($\pm$2.42) & 91.34($\pm$4.75) &  
    %     91.43($\pm$4.86) \\
    %     MIG $\uparrow$ & \textbf{7.27}($\pm$1.99) & 6.77($\pm$2.41) & 0.04($\pm$0.02) & \textbf{10.80}($\pm$1.22) & 9.79($\pm$1.07) & 9.81($\pm$1.10)\\
    %     SAP $\uparrow$ & \textbf{1.88}($\pm$1.12) & 1.76($\pm$1.06) & 0.18($\pm$0.12) & \textbf{1.88}($\pm$1.12) & 1.35($\pm$0.30) & 1.35($\pm$0.30) \\
    %     DCI $\uparrow$ & \textbf{18.90}($\pm$4.49) & 17.21($\pm$5.57) & 1.67($\pm$1.26) & \textbf{26.08}($\pm$2.47) & 25.12($\pm$3.72) & 25.16($\pm$3.82)\\
    %     \hline
    % \end{tabular}

    % \smallskip

    % \begin{tabular}{>{\centering\arraybackslash}m{0.07\textwidth}|>{\centering\arraybackslash}m{0.09\textwidth}*{2}{|>{\centering\arraybackslash}m{0.09\textwidth}}||>{\centering\arraybackslash}m{0.09\textwidth}*{2}{|>{\centering\arraybackslash}m{0.09\textwidth}}}
    % %\begin{tabular}{|>{\centering}m{0.05\textwidth}|>{\centering}m{0.11\textwidth}*{2}{|>{\centering\arraybackslash}m{0.11\textwidth}}*{1}{||>{\centering\arraybackslash}m{0.11\textwidth}}|>{\centering}m{0.11\textwidth}*{2}{|>{\centering\arraybackslash}m{0.11\textwidth}}|}
    %     \hline
    %     \multirow{3}{*}{dSprites} & \multicolumn{3}{c||}{$\beta$-VAE} & \multicolumn{3}{c}{$\beta$-TCVAE}\\
    %     \cline{2-7}
    %     &  MIPET & MIPET \hspace{5mm} (w/o E) & MIPET \hspace{5mm} (w/o EF) & MIPET & MIPET \hspace{5mm} (w/o E) & MIPET \hspace{5mm} (w/o EF)  \\
    %     \hline
    %     FVM & 
    %     \textbf{74.19}($\pm$5.62) & 71.54($\pm$8.66) & 25.83($\pm$1.16) & \textbf{79.87}($\pm$5.80) & 
    %     76.39($\pm$7.44) & 77.44($\pm$7.15) \\
    %     MIG & \textbf{19.72}($\pm$11.37) & 19.29($\pm$11.79) & 0.02($\pm$0.01) & \textbf{35.04}($\pm$4.07) & 33.83($\pm$8.06) & 21.88($\pm$8.42)  \\
    %     SAP & \textbf{5.08}($\pm$2.90) & 4.91($\pm$3.25) & 0.21($\pm$0.10) & \textbf{7.70}($\pm$1.63) & 7.64($\pm$2.03) & 6.84($\pm$1.87) \\
    %     DCI & \textbf{28.81}($\pm$10.19) & 27.51($\pm$11.49) & 1.81($\pm$0.08) & \textbf{47.83}($\pm$5.01) & 45.10($\pm$6.92) & 37.84($\pm$8.85) \\
    %     \hline
    % \end{tabular}
    
    % \smallskip
    \scriptsize
    \begin{tabular}{>{\centering\arraybackslash}m{0.07\textwidth}|>{\centering\arraybackslash}m{0.09\textwidth}*{2}{|>{\centering\arraybackslash}m{0.09\textwidth}}||>{\centering\arraybackslash}m{0.09\textwidth}*{2}{|>{\centering\arraybackslash}m{0.09\textwidth}}}
        \hline
        \multirow{3}{*}{3D Shapes} & \multicolumn{3}{c||}{$\beta$-VAE} & \multicolumn{3}{c}{$\beta$-TCVAE}\\
        \cline{2-7}
        &  MIPET & MIPET (w/o E) & MIPET (w/o EF) & MIPET & MIPET (w/o E) & MIPET (w/o EF)  \\
        \hline
        FVM & 
        \textbf{75.19}($\pm$8.16) & 74.91($\pm$10.46) & 22.27($\pm$1.29) & \textbf{80.59}($\pm$8.57) & 
        77.90($\pm$8.66) & 66.38($\pm$7.57) \\
        MIG & 47.37($\pm$10.13) & \textbf{47.45}($\pm$8.98) & 0.28($\pm$0.09)  & \textbf{54.49}($\pm$9.44) & 51.37($\pm$11.54) & 36.08($\pm$17.42)  \\
        SAP & 9.20($\pm$2.44) & \textbf{9.43}($\pm$2.59) & 0.26($\pm$0.07) & \textbf{11.58}($\pm$3.32) & 10.23($\pm$3.13) & 7.13($\pm$3.09) \\
        DCI & \textbf{54.95}($\pm$8.99) & 54.23($\pm$9.05) & 0.10($\pm$0.02) & \textbf{66.22}($\pm$7.32) & 61.18($\pm$8.87) & 56.85($\pm$11.72) \\
        \hline
    \end{tabular}
    
    %\bigskip
    %\begin{tabular}{c|c|c|c|c}
    %    \multicolumn{5}{c}{$\beta$-TCVAE}\\
    %    \hline
    %    3D Cars & original & w/o symmetric & w/o regularizer & CHIC \\
    %    \hline
    %    FVM & 95.84($\pm$3.40) & 94.86($\pm$4.54) & 00.00($\pm$00.00) & 96.35($\pm$3.34) \\
    %    MIG & 11.87($\pm$2.72) & 11.31($\pm$0.77) & - & 11.38($\pm$0.78) \\
    %    SAP & 1.55($\pm$0.48) & 1.47($\pm$0.32) & - & 1.67($\pm$0.45) \\
    %    DCI & 27.91($\pm$4.31) & 27.58($\pm$2.25) & - & 28.03($\pm$2.37) \\
    %    \hline
    %\vspace{-5cm}
    %\end{tabular}

%    with four metrics 
%    We conduct 40 $\beta$-VAE models and 20 $\beta$-TCVAE with different hyper-parameters on the four disentanglement metrics.}
    %More results of a different dataset are represented in our supplementary materials.}
    \label{tab:main ablations}
\end{table}




\subsubsection{Sensitivity to the Number of IPE-transformation and EF-conversion}
\label{sensitivity to the number of ie}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FIG%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}
%    \centering
%    \begin{subfigure}[]{\textwidth}
%        \includegraphics[width=\textwidth]{figures/impact_ie_dSprites.png}
%        \caption{dSprites}
%    \end{subfigure}
%    \vfill
%    \begin{subfigure}[]{\textwidth}
%        \includegraphics[width=\textwidth]{figures/impact_ie_3Dshape.png}
%        \caption{3D Shapes}
%    \end{subfigure}
%    \vfill
%    \begin{subfigure}[]{\textwidth}
%        \includegraphics[width=\textwidth]{figures/impact_ie_3Dcar.png}
%        \caption{3D Cars}
%    \end{subfigure}
%    \caption{Impact of the number of \textit{MIE}-transformation function on $\beta$-TCVAE and $\beta$-VAE with dSprites, 3D Shapes, and 3D Cars datasets in terms of four metrics.
%    Blue and red box plot represent the single and multiple \textit{MIE}-transformation case on each model.%, and red shows the multiple \textit{MIE}-transformation on each model. 
    %$\beta$-TCVAE (4) with 1 function, brown is same baseline with 3 functions.
    %Green and red are $\beta$-TCVAE (6) with 1 and 3 functions.
%    }
%    \label{fig:impact of IE}
%\end{figure}



We investigate the impact of the MIPE-transformation function.
As presented in Table.~\ref{tab:main impact of IE}, MIPE-transformation is better than IPE-transformation for disentanglement learning on each dataset.
Indeed, MIPET-$\beta$-VAEs results more generally and clearly show the impact of the MIPE-transfomation function.
%These results are representative and other metrics results are in our supplementary materials.
%In most cases in applied 3 invertible and equivariant functions, mean and maximum values are higher than 1 case.
%Due to the fact that increasing the number of MIE-transform functions is equal to increasing $\beta$ hyper-parameter of $\beta$-VAE, it achieves the same effect as $\beta$-VAE.
Our derivation in Section~\ref{appendix: multiple unit} clearly explains MIPE-transformation impact.
%By our derivation in section~\ref{subsec:multiple unit} and Appendix~\ref{appendix: obj function}, MIE-transformation impact is clearly explained.
This result shows the impact of the multiple uses of IPE-transformation and EF-conversion.
%In this result, the necessity of the MIE-transform function is shown empirically.
%More details are in Appendix~\ref{appendix: sensitivity}.
%, and Table~\ref{tab:impact of IE}.

\subsubsection{Impact of Implicit Semantic Mask}
We set masking hyper-parameter $\lambda$ from $\{0.0, 0.5, \cdots, 2.0, \infty\}$, and each model has different $\lambda$ for best case.
In Table~\ref{tab:mask}, VAE and CLG-VAE with masked log-normalizer show better and well-balanced results than the models without masking, which implies improvement of disentanglement.
%(when mask ratio is $\infty$).
%Even though specific metric results show that $\infty$ case is higher than masked case such SAP, and FVM in VAE, and CLG-VAE (0.5) respectively, 
%No masked case show lower disentangled performance than masked cases, and 
%Furthremore, sparse log-normalizer shows well-balanced results compare to no masked case.
%It implies that the normalizer improves disentanglement learning.

\subsubsection{Ablation Study}

%\subsection{Does multi functions affect VAE?}
\label{sec:ablation}
We conduct an ablation study to evaluate the separate impact of equivariant property and the EF-conversion.
%for the necessity of invertible and equivariant properties, and extension of the exponential family.
We have already presented the impact of the multiple uses of IPE-transform and EF-conversion in the previous paragraph.
We evaluate the impact of the other properties by setting MIPE-transformation 1) without equivariant (w/o E), which is implemented as an asymmetric matrix, and 2) without EF-conversion (w/o EF).
To exclude group theory interference with other methods, we select $\beta$-VAE and $\beta$-TCVAE.
%for ablation study.
As the results are shown in Table~\ref{tab:main ablations}, most of the results show that MIPET-VAEs performance is better than other cases. %Therefore, IPE-transformation and EF-conversion are
%Moreover, the FVM scores are consistently increased in MIE-transformation cases.
%Therefore, our proposed method obtains a specific dimension that corresponds to a specific single factor.
In particular, MIPET (w/o EF) results are lower than MIPET (w/o E) results and are clearly shown in all cases.
%More details are in Appendix~\ref{appendix: ablation}. 
%, and Table~\ref{tab:ablations}.

% the impact of EF conversion with 3D Shapes dataset.
%It implies that EF-conversion impact is more 
%In particular, the impact of MIE-transformation is strongly shown in 3D Shapes, which has more ground truth factors than other datasets, and also it is shown in 3D Cars, and dSprites.
%This implies that MIE-transformation is more impact to complicated datasets, which have more factor and channels than simple datasets.

%Prev
%either exist of MIE-transform functions or of regularizer shows lower performance than MIPET-VAEs.
%In particular, the $\beta$-TCVAE case clearly shows the impact of the necessity of two conditions.
%Between two conditions impact, transformed latent vectors into the exponential family is more impact on disentanglement learning on model.
%Even though restricted symmetric matrix for invertible and equivariant function seems to impose constraint for disentanglement learning, group theory-based method improve disentanglement learning.
%Therefore, two conditions should be satisfied for MIPET-VAEs.
%all metric is higher on applied CHIC architecture than other methods in the $\beta$-TCVAE.



%\subsubsection{Impact of the invertible and equivariant function}
%non-symmetric matrix

%\subsubsection{Impact of the Exponential Family Uncertainty}
%w/o regularization term


\begin{figure}
    \centering
        %\begin{subfigure}[]{0.99\textwidth}
        \centering
        \includegraphics[width=0.85\textwidth]{figure_dsprites_quali_ver10.jpg}
        %\caption{dSprites}
        %\label{subfig:dsprites}
        %\end{subfigure}
    
        %\begin{subfigure}[]{0.99\textwidth}
        %\centering
        %\includegraphics[width=0.95\textwidth]{figures/car_quali_ver07.png}
        %\caption{3D Cars}
        %\label{subfig:3dcar}
        %\end{subfigure}
    \caption{Qualitative results on dSprites.
    %The $\text{\#}$ factor represents the number of activated factors on each dimension.
    The left-side grids are input images and their variants by changing activations of each dimension of latent vectors. The first row shows input images. 
    The right-side table shows matching pre-defined factors of the dataset (red: MIPET, blue: no MIPET). 
%    The right side table shows each dataset factor, and we verify the activated factors on each dimension. 
   % Each row corresponds to each 
  %  order of dimensions, and the $\text{\#}$ factor represents the number of activated factors on each dimension. 
  %  The red checkmark is the MIPET case, and the blue is for $\beta$-TCVAE. The first column in MIPET-VAEs is the input.
  % and we change the latent vector values from $\{$-2, 2$\}$.
    %and Orig is original model of $\beta$-TCVAE (6) and CHIC is applied our proposed method on original model.
    }
    \label{fig:dsprites}

    % \centering
    % \includegraphics[width=0.9\textwidth]{figures/dsprites-quali-betavae-ver02.jpg}
    % \caption{Qualitative analysis result of $\beta$-VAE and MIPET-$\beta$-VAE.}
    % \label{appendix fig: dsprites}

    % \centering
    % \includegraphics[width=0.9\textwidth]{figures/dsprites-quali-commut-ver01.jpg}
    % \caption{Qualitative analysis result of CLG-VAE (0.2) and MIPET-CLG-VAE (0.2) with dSprites.}
    % \label{appendix fig: dsprites (com)}
    
\end{figure}


\begin{figure}[h]
    % \centering
    % \includegraphics[width=0.9\textwidth]{figures/3dshape-quali-beta-ver02.jpg}
    % \caption{The Shape is object shape, Orien is an orientation of object, Scale is a scale factor of object, Wall is wall color factor, Floor is floor color, and Object is object color factors. It represents the $\beta$-VAE ($\beta=2$) results.
    % %it perfectly separates all factors in each dimension.
    % }
    % \label{appendix fig: 3dshape}

    % \centering
    % \includegraphics[width=0.9\textwidth]{figure_3dshape_quali_commut_ver02.jpg}
    % \caption{Qualitative analysis result of CLG-VAE (0.2) and MIPET-CLG-VAE (0.2) with 3D Shapes.
    % }
    % \label{fig: 3dshape (com)}

    \centering
    \includegraphics[width=0.85\textwidth]{figure_car_quali_ver08.jpg}
    \caption{Qualitative analysis result of $\beta$-VAE (4.0) with 3D Cars.}
    \label{fig: 3dcars}
\end{figure}

%\begin{figure}[h]
%    \centering
%    \includegraphics[width=\textwidth]{figures/3dshape_quali_betatc.PNG}
%    \caption{The Shape is object shape, Orien is an orientation of object, Scale is a scale factor of object, Wall is wall color factor, Floor is floor color, and Object is object color factor.
%    As shown in the result, $\beta$-TCVAE (4) struggles with shape and scale factors in 5$^{th}$ dimension, and scale and object color factors in 6$^{th}$ dimension.
%    However, MIPET-$\beta$-TCVAE separates shape and scale factors, and only struggles with orientation and object color.}
%    \label{fig:my_label}
%\end{figure}

\subsection{Qualitative Analysis}

%We randomly sample 10 images from dSprites and 3D Cars datasets, and changes their latent vectors value from $\{$-2, 2$\}$ with 10 intervals on each dimension, and then generate their corresponding output images.
We randomly sample an image for each dimension of the latent vector space and creates 10 variants of its generated latent vector by selecting values from $\{$-2, 2$\}$ with 10 intervals for the dimension, then generate their corresponding output images.
%divide variants of latent vector from $\{$-2, 2$\}$ with 10 intervals on each dimension.
For the generation, we select $\beta$-TCVAE (6), which shows the best FVM scores in dSprites dataset.
%We select the highest score in terms of FVM metric on $\beta$-TCVAE (6) with dSprites, and 3D Cars datasets. %because FVM is estimated regardless of ground-truth label.
Thereafter, we evaluate the semantic roles of each dimension before and after applying MIPE-transformation function.
%We want to observe the impact of the invertible and equivariant properties, and $\beta$-TCVAE generally achieves a high score on FVM on both datasets.

%\subsubsection{dSprites}
In Figure~\ref{fig:dsprites}, $\beta$-TCVAE struggles with y-position and rotation, as shown on the 6$^{th}$ row, and with scale and shape represented on the 7$^{th}$ row.
On the contrary, MIPET-$\beta$-TCVAE separates y-position and rotation factor (10$^{th}$, and 7$^{th}$ rows), also the activated dimensions of MIPET-$\beta$-TCVAE are not overlapped with each factor.
%Even though FVM score of $\beta$-TCVAE is slightly higer than CHIC-$\beta$-TCVAE, 88.13 and 86.88, 
Applied our method on $\beta$-TCVAE shows better disentangled representation on dSprites dataset.
These results also show that our proposed method improves disentangled representation learning.
% As shown in the Figure~\ref{appendix fig: dsprites}, $\beta$-VAE struggles with rotation and scale factors in 4$^{th}$dimension. 
% Also, it struggles with x-position and scale factors in 8$^{th}$ dimension, and x-position and rotation factors in 9$^{th}$ dimension.
% However, MIPET-$\beta$-VAE only struggles with rotation and shape factors in 5$^{th}$ dimension.
% As shown in the Figure~\ref{appendix fig: dsprites (com)}, CLG-VAE struggles with rotation and shape factors in 2$^{nd}$ dimension, and shape and scale factors in 7$^{th}$ dimension. However, MIPET-CLG-VAE separates rotation and shape factors in 10$^{th}$, and 1$^{st}$ dimensions respectively.

% The qualitative analysis with 3D Shapes dataset, as shown in the Figure~\ref{appendix fig: 3dshape}, $\beta$-VAE struggles with all factors, and only the object color factor is divided in 6$^{th}$ dimension.
% However, this factor is still activated with scale factor in 3$^{rd}$ dimension.
% Although MIPET-$\beta$-VAE struggles with reconstruction, it is less struggle with than $\beta$-VAE. 

% As shown in the Figure~\ref{fig: 3dshape (com)}, CLG-VAE struggles with shape and wall color factors in 4$^{th}$ dimension, and shape and object color factors in 7$^{th}$ dimension.
% In particular, it struggles with tree factors in 9$^{th}$ dimension.
% On the other hand, MIPET-CLG-VAE separates shape, wall, and object color factors.

The qualitative analysis with 3D Cars dataset, as show in Figure~\ref{fig: 3dcars}, the left side is the $\beta$-TCVAE result, and it struggles with body, and azimuth factors shown in the 7$^{th}$ row.
However, MIPET-$\beta$-TCVAE separates azimuth (6$^{th}$ row) and body (1$^{st}$ row).
 %although the number of activated dimensions is less in MIPET case, %more dimensions are activated with factors on MIPET-$\beta$-TCVAE and 
In particular, MIPET-$\beta$-TCVAE learns \textit{color} factor (3$^{rd}$ row) which does not exist on $\beta$-TCVAE.
%More results are in Appendix~\ref{appendix: qualitative}.%our supplementary materials.
% please see this page.
%Also, FVM score is slightly lower on CHIC-$\beta$-TCVAE than original one, 86.88 and 88.13, but proposed method still show better results on 3D Cars dataset.




%However, when both cases find equivariant function on each latent space, subgroup $G_1$ represents symmetries better than our method.
%Because, Pr($G_1^\prime) \geq$~Pr($G_1$), so our method guarantee at least 37.5$\%$ of cases are better than the asymmetric cases.
%The conditional probability of higher model performance cases given that $\psi_G(\cdot)$ finds equivariant function shows higher than 50$\%$, so we conclude the quality to represent group element of latent space is appropriate with group $G$.
%We left this point as future work to find appropriate group element and improve the probability of finding equivariant functions with group $G$.



\section{Conclusion}
\label{sec: conclusion}
In this paper, we address the problem of injecting inductive bias for learning unsupervised disentangled representations.
%unsupervised disentangled representation learning with inductive bias.
%To inject inductive bias, we propose 
To build the bias in VAE frameworks, we propose MIPE-transformation composed of 1) IPE-transformation for the benefits of invertibility and partial-equivariant for disentanglement, 2) a training loss and module to adapt unrestricted prior and posterior to an approximated exponential family, and 3) integration of multiple units of IPE-transformation function and EF-conversion for more expressive bias. %(MIE-transformation).
The method is easily equipped on state-of-the-art VAEs for disentanglement learning and shows significant improvement on 
 dSprites, 3D Shapes, and 3D Cars datasets. 
 %, the proposed method elaborates disentanglement of latent representation without any additional hyper-parameter.
We expect that our method can be applied to more VAEs, and extended to downstream applications.
Our work is limited to holding partial equivariance of I2L transformation, so more direct methods to induce it can be integrated in the future.

\section*{Acknowledgments}
 This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2022R1A2C2012054).
%A limitation of our work is that the the method restricts mapping functions to symmetric matrices, and causes additional computational cost by increasing the number of the mapping functions.We will consider soft constraints to reduce computing costs with more diverse transformation functions for future work.

%Tackling these reasons we will

%invertible and equiavariant mapping functions which transform the Gaussian distribution to the Exponential Family, and explain how these functions inject inductive bias to model.
%Indeed, we propose utilizing multi non-Guassian distribution to impose the same impact such as increasing $\beta$ values in $\beta$-VAEs and to preserve complicate data structure.
%To guarantee invertible and equivariant function transform to expected distribution, we maximize posterior, which consists of trainable parameters for the exponential family, with the Lagrange multiplier and regularizer to penalize error between KL divergence of the Gaussian distribution and the exponential family. 

%\subsection{$e^S$ is always symmetric}
%\label{appendix: proof symmetric}
%직접 증명



%e show the matrix exponential with the symmetric matrix is symmetric.
%        A symmetric matrix iff $S= PDP^{-1}, \text{where $P$ is a orthogonal matrix, and $D$ is a diagonal matrix with eigenvalue of $S$.}
%\begin{equation}
%    \begin{split}
%\\
%        \mathbf{e}^{S} &= I + PDP^{-1} + \frac{1}{2!}PD^2P^{-1} + \ldots + \frac{1}{(n-1)!}PD^{n-1}P^{-1} \text{(matrix exponential)} \\
%        &= PIP^{-1} + PDP^{-1} + \frac{1}{2!}PD^2P^{-1} + \ldots + \frac{1}{(n-1)!}PD^{n-1}P^{-1}\\
%        &= P(I + D + \frac{1}{2!}D^2 + \ldots + \frac{1}{(n-1)!}D^{n-1})P^{-1} \\
%        &= P\mathbf{e}^{D}P^{-1}.
%    \end{split}
%\end{equation}
%Therefore, $\mathbf{e}^{S}$ is also a symmetric matrix. 

%show more details of proposed method.



\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\section{Appendix}

\section{Preliminaries}
\label{appendix: preliminaries}

\subsection{Group Theory}
\label{subsec: group theory}

\hspace{4mm} \textbf{Binary operation:} Binary operation on a set $S$ is a function that $\ast: S \times S \rightarrow S$, where $\times$ is a cartesian product.

\textbf{Group:} A group is a set $G$ together with binary operation $\ast$, that combines any two elements $g_a$ and $g_b$ in $G$, such that the following properties:
\begin{itemize}
    \item closure: $g_a, g_b \in G \Rightarrow g_a \ast g_b \in G$.
    \item Associativity: $\forall g_a, g_b, g_c \in G$, \textit{s.t.} $(g_a \ast g_b) \ast g_c = g_a \ast (g_b \ast g_c)$.
    \item Identity element: There exists an element $e \in G$, \textit{s.t.} $\forall g \in G, e \ast g = g \ast e = g$.
    \item Inverse element: $\forall g \in G, \exists g^{-1} \in G$: $g \ast g^{-1} = g^{-1} \ast g = e$.
\end{itemize}

\textbf{Group action:} Let $(G, \ast)$ be a group and set $X$, binary operation $\cdot: G \times X \rightarrow X$, such that following properties:
\begin{itemize}
    \item Identity: $e \cdot x = x$, where $e \in G, x \in X$.
    \item Compatibility: $\forall g_a, g_b \in G, x \in X, (g_a \ast g_b) \cdot x = g_a \cdot (g_b \cdot x)$.
\end{itemize}

\textbf{Equivariant map:} Let $G$ be a group and $X_1, X_2$ be two sets with corresponding group action of $G$ in each sets: $T^{X_1}_g, T^{X_2}_g$, where $g \in G$. Then a function $f: X_1 \rightarrow X_2$ is equivariant if $f(T^{X_1}_g \cdot X_1) = T^{X_2}_g \cdot f(X_1)$.

\textbf{Partial Equivariance:}\cite{partial_equiv}: Let subset of $G$ be $\Upsilon \subset G$, then $f$ is a partially equivariant map to $G$:
\begin{equation}
    f(T^{X_1}_\upsilon \cdot X_1) = T^{X_2}_\upsilon \cdot f(X_1), ~\text{where}~ \forall \upsilon \in \Upsilon.
\end{equation}

\textbf{Homomorphsim:} Let $(G, \cdot), (H, \circ)$ be two groups. If mapping function $h: G \rightarrow H$, \textit{s.t.} $h(g_i \cdot g_j) = f(g_i) \circ f(g_j)$, then $f$ is called homomorphism.

\subsection{Exponential Family}
\label{appendix: background}
Power density function of the exponential family (PDF) generalized formulation:
\begin{equation}
    \begin{split}
        f_{ \vx}( \vx|\boldsymbol{\theta}) & = h( \vx) \text{exp} (\boldsymbol{\theta}^\intercal T( \vx) - A(\boldsymbol{\theta})) \\ 
        & = \text{exp} (\boldsymbol{\theta}^\intercal T( \vx) - A(\boldsymbol{\theta}) + B( \vx)),
        \label{eq:pdf}
    \end{split}
\end{equation}
where \textit{sufficient statistics} $T(\cdot)$, \textit{log-normalizer} $A(\cdot)$, and \textit{carrier or base measure} $B(\cdot)$ are known functions, samples $ \vx$ from distribution, and \textit{natural parameter} $\boldsymbol{\theta}$.

\section{Proof}

\begin{proposition} 
\label{appendix: prop2}
If $q_\phi$ is equivariant over defined on group of symmetries $G_I^a$ and $G_L^a$, then $\psi_{G_S} (q_\phi(\cdot))$ is equivariant to symmetries in $G_I$ corresponding to $G_S \cap G_L$ and $G_T$ corresponding to $G_S \cap G_L$ by the equivariance of $q_\phi$.  
\end{proposition}
$\textit{Proof.}$ The function $\psi_{G_S}(\cdot)$ is an equivariant function over group elements in $G_S \cap G_L$ by Proposition~\ref{prop1}. 
Then, the composite function, $\psi_{G_S}(\cdot)$ and $q_\phi$, is an equivariant function 
of $G_I$ corresponding to $G_S \cap G_L$ and $G_T$ corresponding to $G_S \cap G_L$.
Let $g_L^a$ be a group element in $G_S \cap G_L$, and $g_I^a$ is a group element in $G_I$ corresponding to $G_S \cap G_L$, and $g_T^a$ is a group element where corresponding to $G_S \cap G_L$ on the latent vector space transformed from the original latent vector space.
Then, group element $g_T^a$ is equal to $g_L^a$:
\begin{align}
    %\begin{split}
        & \hat{ \vz}_1 = \psi_{G_S}( \vz_1), \text{and} \\
        & \hat{ \vz}_2 = \psi_{G_S}( \vz_2) = \psi_{G_S}(g_L^a  \vz_1) = g_L^a \psi_{G_S}( \vz_1) ~(\because \text{Prop.~\ref{prop1}}), \\
    \begin{split}
        & \text{then} ~g_L^a \psi_{G_S}( \vz_1) = g_T^a \psi_{G_S}( \vz_1) ~(\because \hat{ \vz}_2 = g_T^a \hat{ \vz}_1) 
        \label{equation: appendix equivariance}\\
        & \Rightarrow (g_L^a - g_T^a) \psi_{G_S}( \vz_1) = \mathbf{0},
    \end{split}
\end{align}
where $\mathbf{0}$ is a zero vector.
Eq.~\ref{equation: appendix equivariance} is defined when $\forall  \vz \in \mathcal{Z}$ by the equivariance definition.
In other words, Eq.~\ref{equation: appendix equivariance} is satisfied only if the kernel (linear algebra) of $g_L^a - g_T^a$, notated as \textit{ker}$(g_L^a - g_T^a)$, includes the basis of $\mathbb{R}^n$ vector space.
If the standard basis of $\mathbb{R}^n$ vector space is in \textit{ker}$(g_L^a - g_T^a)$, then $(g_L^a - g_T^a)=\mathbf{0}_{n,n}$, where $\mathbf{0}_{n,n}$ is an n by n zero matrix.
Other bases of $\mathbb{R}^n$ vector space are expressed by the standard basis. Therefore $g_L^a- g_T^a = \mathbf{0}_{n,n}$. 
%If $\psi_{G_S}(\mathbf{z}_1)$ is an eigenvector of $g_L^a-g_T^a$, then $g_L^a-g_T^a \neq \mathbf{0}_{d,d}$.
%However Eq.~\ref{equation: appendix equivariance} is satisfied with $\forall \mathbf{z} \in \mathcal{Z}$ by the equivariance definition.
%Therefore $g_L^a- g_T^a = \mathbf{0}_{d,d}$.

%$\hat{\mathbf{z}}_1 = \psi_{G_S}(\mathbf{z}_1)$ and $\hat{\mathbf{z}}_2 = \psi_{G_S}(\mathbf{z}_2) = \psi_{G_S}(g_L^a \mathbf{z}_1) = g_L^a \psi_{G_S}(\mathbf{z}_1)$ because $\psi_{G_S}$ is equivariant by Lemma~\ref{lem1}.
%$\hat{\mathbf{z}}_2 = g_T^a \hat{\mathbf{z}}_1$ then $g_L^a \psi_{G_S}(\mathbf{z}_1) = g_T^a \psi_{G_S}(\mathbf{z}_1)$.
%$ = \psi_{G_S}(g_L^a \mathbf{z}_1)$.

%because $\psi_{G_S}$ is a $n\timesn$ matrix for linear transformation function whose domain and range are defined on the same vector space.
%and $z$ and $\hat{z}$ are in the same space).

Then, $\psi_{G_S}(g_L^a  \vz_1) = g_L^a \psi_{G_S}( \vz_1) = g_T^a \psi_{G_S}( \vz_1)$.
The encoder is an equivariant function over input space $\mathcal{X}$ as $q_{\phi} (g_I^a  \vx_1) = g_L^a q_{\phi}( \vx_1)$.
%The latent vector $\mathbf{z}$ holds equivariance property as $q_{\phi} (g_I^a \mathbf{x}_1) = g_L^a q_{\phi}(\mathbf{x}_1)$. 
Mixing two equivarience property, we can derive another equivariance relation $g_T^a \psi_{G_S}(q_\phi( \vx_1)) = \psi_{G_S}(q_\phi(g_I^a  \vx_1))$
%$\psi_{G_S}(g_L^a q_{\phi} (g_I^a x)) = g_T^a \psi(g_L^a q_{\phi}(x))$.
This result implies that the equivariance between input space and a latent space is preserved for $G_S \cap G_L$ if the latent vector $ \vz$ is transformed by $\psi_{G_S}.
$%if the transformation from the latent vector space to the same space is equivarient. 
$\blacksquare$


We show that $\psi_{G_S}$ preserves equivariance between $G_L^a$ and $G_I^a$.
%group elements of latent space in $G_L^a$ and input space.
If there exists equivariant function between input and latent vector space, there should be a group $G_L$ for a latent space and its corresponding group $G_I$ in an input space by definition of equivariance ($q_{\phi} (g_I x) = g_L q_{\phi}(x)$). 
%We assumed input space has group $G_I$ for symmetries, and latent space also has group $G_L$ which corresponds to $G_I$.
%In other words, encoder $q_\phi$ is equivariant between input and latent space as Fig.~\ref{figure: equivalence cycle} ($q_{\phi} (g_I x) = g_L q_{\phi}(x)$).

%Then, the composite function of equivariant, $\psi_{G_S}(\cdot)$ and $q_\phi$, to the shared $\circled{a}$ is also equivariant to $G_I$ corresponding to $\circled{a}$ (see Appendix~\ref{appendix: proof equivariance}).$\blacksquare$

In other words, $\psi_{G_S}(\cdot)$ guarantees to preserve the equivariance of I2L-transformation to certain symmetries in $G_S \cap G_L$ after IPE-transformation as shown in Figure~\ref{sub fig:suboverview}. 


Let $P(B)$ be the probability of $\psi(\cdot) \in B$ for a subset $B \subset M_n(\mathbb{R})$ after VAE training, and $Pr(\psi_B \in B^\prime)$ be the conditional probability of $\psi(\cdot) \in B^\prime$ given $\psi(\cdot) \in B$.


Then,  
\begin{proposition} 
\label{appendix: prop3}
$Pr(\psi_{E_S}(\cdot) \in G_S) > Pr(\psi_{E_M}(\cdot) \in G_S) > Pr(\psi_{M}(\cdot) \in G_S)$.
\end{proposition}

$\textit{Proof.}$ 
All $\mathbf{e}^{ \mS} \in E_S$ are in $E_M$ since $Sym_n(\mathbb{R}) \subset M_n(\mathbb{R})$. However, $E_M \not\subset E_S$ because $\mathbf{e}^{ \mS} $ is always symmetric, but $\mathbf{e}^{ \mM}$ can be an asymmetric matrix.
All elements of $E_S$ are symmetric because of the matrix exponential property that $\mathbf{e}^{ \mM^\intercal} = (\mathbf{e}^{ \mM})^\intercal$. If $ \mM$ is a symmetric matrix then $\mathbf{e}^{ \mM^\intercal} = \mathbf{e}^{ \mM} = (\mathbf{e}^{ \mM})^\intercal$.
Therefore, if $ \mM$ is symmetric then the exponential of $ \mM$ is also symmetric.
We show a counter example to $E_M \subset E_S$. 
When $ \mM =  \begin{bmatrix}
    1 & 1 \\ 0 & 1
\end{bmatrix}$,
\begin{equation}
% \scriptsize
\begin{split}
    \mathbf{e}^{ \mM} & = \sum_{k=0}^\infty \frac{1}{k!} \mM^k \\
    & = I + 
    \begin{bmatrix}
        1 & 1 \\ 0 & 1
    \end{bmatrix}
    + 
    \frac{1}{2!}\begin{bmatrix}
        1 & 1 \\ 0 & 1
    \end{bmatrix}^2
    +
    \cdots
    + \frac{1}{(n-1)!}\begin{bmatrix}
        1 & 1 \\ 0 & 1
    \end{bmatrix}^{(n-1)} + \cdots \\
    & = I + 
    \begin{bmatrix}
        \sum_{n=0}^\infty \frac{1}{n!} & 1+ \sum_{n=0}^\infty \frac{1}{(n-1)!} \\ 0 & \sum_{n=0}^\infty \frac{1}{n!}
    \end{bmatrix} \\
    & =
    \begin{bmatrix}
        1 + e & 1+ e \\ 0 & 1 + e
    \end{bmatrix}.
\end{split}
\end{equation}
The matrix $\mathbf{e}^{ \mM}$ is asymmetric and not in $E_S$.
%by the proof of Appedix~\ref{appendix: proof symmetric}.  
Therefore $E_M \not\subset E_S$.
Therefore, the probability $Pr(\psi_{E_S}(\cdot) \in G_S) = \frac{P(G_S)}{P(E_S)}$ is greater than $Pr(\psi_{E_M}(\cdot) \in G_S) = \frac{P(G_S)}{P(E_M)}$. In the same way, $Pr(\psi_{E_M}(\cdot) \in G_S) > Pr(\psi_{M}(\cdot) \in G_S) = \frac{P(G_S)}{P(M_n(\mathbb{R}))}$ because $E_M \subset M_n(\mathbb{R})$ and non-invertible functions are only in $M_n(\mathbb{R})$. $\blacksquare$ 

\subsubsection{KL Divergence for Evidence of Lower Bound} 
The KL divergence of Gaussian distribution~\cite{vae} is computed using mean and variance, which are the parameters of a Gaussian distribution. 
To introduce a loss as the KL divergence of Gaussian distribution, we compute KL divergence of the exponential family in Eq.~\ref{eq:posteior_} using the learnable parameter %$f_{\mathbf{x}}(\mathbf{x}|\boldsymbol{\theta}_{\hat{\mathbf{z}}})$ and $f_{\mathbf{x}}(\mathbf{x}|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})$ 
$T(\cdot)$ and $A(\cdot)$ with given natural parameter $\boldsymbol{\theta}_{\hat{ \vz}}$ and $\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}$, expressed as:
\begin{equation}
    \begin{split}
         \mathcal{L}_{kl} &\coloneqq  \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m})) \\
        &= A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) - A (\boldsymbol{\theta}_{\hat{ \vz}}) + \boldsymbol{\theta}_{\hat{ \vz}}^{\intercal}  \nabla_{\boldsymbol{\theta}_{\hat{ \vz}}} A(\boldsymbol{\theta}_{\hat{ \vz}}) - \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}^{\intercal}  \nabla_{\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}} A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}).
        \label{appendix eq:kl}
    \end{split}
\end{equation}
Because $ \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}))$ is followed as:
%where $\cdot$ is an element-wise product.
%More details of Eq.~\ref{eq:kl} are in Appendix~\ref{appendix: kl}.

%The second term of Eq.~\ref{appendix: eq: elbo} is equal to $ \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}))$ because power density function of posterior and prior are $f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}}) $ and $f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})$, respectively.
\begin{equation}
% \small
    \begin{split}
     \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})) & = \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \log f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \mathsf{d}  \vx \\
    & - \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \log f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) \mathsf{d}  \vx.
    \end{split}
\end{equation}
We designed sufficient statistics as matrix multiplication (multi-layer perceptron).
%Log-normalizer is a function with MLP and a sigmoid function, and sigmoid is activated after last layer.%and by the exponential family's power density function,
Then, 
{
% \scriptsize
\begin{equation}
\begin{split}
    \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \log f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \mathsf{d}  \vx & = \int^{\infty}_{-\infty} f_{ \vx} ( \vx| \boldsymbol{\theta}_{\hat{ \vz}}) \cdot \\ &~~~~~~~~~~~~~~[\boldsymbol{\theta}_{\hat{ \vz}}^\intercal \mathbf{T}( \vx) - A(\boldsymbol{\theta}_{\hat{ \vz}}) + B( \vx)] \text{d} \vx \\
    & = -A(\boldsymbol{\theta}_{\hat{ \vz}}) \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \text{d} \vx \\ 
    & + \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) [\boldsymbol{\theta}_{\hat{ \vz}}^\intercal \mathbf{T}( \vx) + B( \vx)]\text{d} \vx\\
    & = -A(\boldsymbol{\theta}_{\hat{ \vz}}) + \boldsymbol{\theta}_{\hat{ \vz}}^\intercal  \int^{\infty}_{-\infty} T( \vx) f_{ \vx} ( \vx| \boldsymbol{\theta}_{\hat{ \vz}})  \text{d} \vx \\
    & + \int^{\infty}_{-\infty} B( \vx) f_{ \vx} ( \vx| \boldsymbol{\theta}_{\hat{ \vz}}) \text{d} \vx, 
    \label{appe:exp_mean1}
\end{split}
\end{equation}
}
and
\begin{equation}
% \small
\begin{split}
    \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \log f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) \mathsf{d}  \vx & = -A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) \\ 
    & + \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}^\intercal  \int^{\infty}_{-\infty} T( \vx) f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})  \text{d} \vx \\
    & + \int^{\infty}_{-\infty} B( \vx) f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) \text{d} \vx.
\label{appe:exp_mean2}
\end{split}
\end{equation}


%\resizebox{1.0\linewidth}{!}{
%\begin{minipage}{1.05\linewidth}
\begin{equation}
\begin{split}
    \therefore  \KL (f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})) & = A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) - A(\boldsymbol{\theta}_{\hat{ \vz}}) \\ 
    & + \boldsymbol{\theta}_{\hat{ \vz}} \int^{\infty}_{-\infty} T( \vx) f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}})  \text{d} \vx \\
    & - \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}} \int^{\infty}_{-\infty} T( \vx) f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})  \text{d} \vx.
    \label{appendix: eq: 28}
\end{split}
\end{equation}
%\end{minipage}
%}

%Then, $\int^{\infty}_{-\infty} \mathbf{T}(\mathbf{x}) f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}})  \text{d}\mathbf{x} =  \int^{\infty}_{-\infty} \mathbf{T}(\mathbf{x}) f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) \text{d}\mathbf{x} =  \mathbf{T} \mathbb{E}_{\mathbf{x}}[\mathbf{x}]$ because $\mathbf{T}(\mathbf{x}) = \mathbf{x}^\intercal\mathbf{T}$.
%To estimate the mean of distribution ($\mathbb{E}_{\mathbf{x}}[\mathbf{x}]$), we approximate it as like mini-batch sampling, which is introduced in~\cite{beta-tcvae}.
%Our approximation is followed as:
The mean of the sufficient statistic is followed as:
\begin{equation}
% \small
     \int^{\infty}_{-\infty} T( \vx) f_{ \vx} ( \vx | \boldsymbol{\theta})  \text{d} \vx = \frac{\partial A^{\ast} (\boldsymbol{\theta})}{\partial\boldsymbol{\theta}} \approx \frac{\partial A(\boldsymbol{\theta})}{\partial\boldsymbol{\theta}} ~~~~~\because A^{\ast} (\boldsymbol{\theta}) = \boldsymbol{\theta}^\intercal A^{\ast},
\end{equation}
where $A^{\ast}(\cdot)$ is a true log-partition function of the exponential family (ideal case of $A(\cdot)$).
%To approximate the $\mathbf{T} \mathbb{E}_{\mathbf{x}}[\mathbf{x}]$, we replace $\mathbb{E}_{\mathbf{x}}$ to $\mathbf{A}$ because $\mathbf{T} \mathbb{E}_{\mathbf{x}}[\mathbf{x}] \approx \mathbf{A}$ ($\mathbb{E}_{\mathbf{x}}[\mathbf{x}] \propto \mathbf{A}$). 
%where $ \int^{\infty}_{-\infty} \mathbf{T}(\mathbf{x}) f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}})  \text{d}\mathbf{x} = \frac{\partial \mathbf{A}(\boldsymbol{\theta}_{\hat{\mathbf{z}}})}{\partial \boldsymbol{\theta}_{\hat{\mathbf{z}}}} = \mathbf{A}$, $\mathbf{A}$ is multi-layer perceptron, and $\mathbf{T}(\mathbf{x}) = \mathbf{x}^\intercal \mathbf{T}$. 
%\begin{align}
%    \int^{\infty}_{-\infty} f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}}) \log f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}}) \text{d}\mathbf{x}
%    &= \int^{\infty}_{-\infty} \mathbf{B}(\mathbf{x}) f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}}) \text{d}\mathbf{x} \\
%    &= \int^{\infty}_{-\infty} f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}}) \log f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\epsilon}}) \text{d}\mathbf{x}.
%\end{align}
%Then KL divergence is always 0 that is posterior and prior are exactly same.
%However it causes posterior collapse~\cite{posterior-collapse, posterior_collapse_general}.
%To prevent posterior collapse, we assume $\mathbf{x}^\intercal \mathbf{T} = \mathbf{x}^\intercal$, where $\mathbf{T}$ is not identity matrix then, $\int^{\infty}_{-\infty} \mathbf{x}^\intercal f_{\mathbf{x}} (\mathbf{x} | \boldsymbol{\theta}_{\hat{\mathbf{z}}})  \text{d}\mathbf{x} = \frac{\partial \mathbf{A}(\boldsymbol{\theta}_{\hat{\mathbf{z}}})}{\partial \hat{\mathbf{z}}} = \mathbf{A}$.
However, estimating $A^\ast$ is difficult, and there is no direct method without random samplings, such as mini-batch weighted sampling or mini-batch stratified sampling~\cite{beta-tcvae}.
Then, we approximate $A^\ast$ to $A$, and train $A$ to be close to $A^\ast$.
Consequently, we obtain KL divergence of the exponential family as:
%Integration at second term in Eq.~\ref{appe:exp_mean}, is a mean of distribution, which is equal to derivation of $\mathbf{A}(\boldsymbol{\theta}_{\hat{\mathbf{z}}})$ over $\boldsymbol{\theta}_{\hat{\mathbf{z}}}$.
\begin{equation}
\begin{split}
    \int^{\infty}_{-\infty} f_{ \vx} (\mathbf{x} | \boldsymbol{\theta}_{\hat{ \vz}}) \log f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) & = -A(\boldsymbol{\theta}_{\hat{ \vz}}) + \boldsymbol{\theta}_{\hat{ \vz}}^{\intercal} \frac{\partial A(\boldsymbol{\theta}_{\hat{ \vz}})}{\partial \boldsymbol{\theta}_{\hat{ \vz}}} \\ 
    & + \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) B( \vx) \text{d} \vx,
\end{split}
\end{equation}
\begin{equation}
\begin{split}
    \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) \log f_{ \vx}(\mathbf{x}|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) & = -Z(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) + \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}^{\intercal} \frac{\partial A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})}{\partial \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}} \\ 
    & + \int^{\infty}_{-\infty} f_{ \vx} ( \vx | \boldsymbol{\theta}_{\hat{ \vz}}) B( \vx) \text{d} \vx.
\end{split}
\end{equation}
Therefore, the final Kullback-Leibler divergence of exponential family is followed as:
\begin{equation}
% \scriptsize
    \begin{split}
     \KL(f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{ \vz}}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})) & = A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) - A (\boldsymbol{\theta}_{\hat{ \vz}}) + \boldsymbol{\theta}_{\hat{ \vz}}^{\intercal} \frac{\partial A(\boldsymbol{\theta}_{\hat{ \vz}})}{\partial \boldsymbol{\theta}_{\hat{ \vz}}} - \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}^{\intercal} \frac{\partial A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})}{\partial \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}}.      
    \end{split}
\end{equation}

\subsection{Integration for Multiple IPE-Transformation and EF-Conversion}
\label{appendix: multiple unit}
%IE-transformation is a process to restrict the distribution of latent vectors for disentanglement, which possibly reduce reconstruction error~\cite{}.
%This trade-off is possibly reduced by increasing the transformation unit as the report in~\cite{}. 
We mathematically extend IPE-transformation to MIPE-transformation, which is the equivalent process of $\beta$-VAE to enhance disentanglement. 
%The equivalence is proven in Appendix~\ref{appendix: elbo}.
Each IPE-transformation function operates independently, then the reconstruction error for objective function is defined as:
%$\log p_{\theta}( \vx|\hat{ \vz}_1, \hat{ \vz}_2, \cdots, \hat{ \vz}_k) =$ $-(k-1) \log p_\theta( \vx) + \Pi_{i=1}^k p_\theta( \vx | \hat{ \vz}_i )$. 
%more details are in Appendix~\ref{appendix: elbo}.
%Therefore, the reconstruction error term ($\mathcal{L}_{rec}$) for given all $\hat{\mathbf{z}}_i$ in Eq.~\ref{eq:objective_function} is 
%supplementary material.
\begin{equation}
% \scriptsize
    \begin{split}
       \mathcal{L}_{rec} &\coloneqq \frac{1}{k} \sum_{i=1}^k \Bigg [ \int q_i(\hat{ \vz}_i| \vx) \log p_\theta( \vx|\hat{ \vz}_i) \mathsf{d}\hat{ \vz}_i \prod_{j=1, j\neq i}^k \int q_j(\hat{ \vz}_j| \vx) %q_3(\mathbf{z}_3|\mathbf{x}) \cdots q_k(\mathbf{z}_k|\mathbf{x})
       \text{d}\hat{ \vz}_j \Bigg ] \\
       %\text{d}\mathbf{z}_3 \cdots \text{d}\mathbf{z}_k\\
    &= \frac{1}{k} \sum_{i=1}^k  E_{q_{\phi,\psi_i} ( \vz| \vx)} \log p_\theta( \vx|\psi_i( \vz)),
    \end{split}
    \label{appendix eq: reconstruction error}
\end{equation}
where $\hat{ \vz}_i = \psi_i( \vz)$.
Becuase the log likelihood of $p( \vx)$ can be derived as follows:
{
% \scriptsize
\begin{align}
    \log p_\theta( \vx) & = \int \prod_i^k q_1(\hat{ \vz}_i |  \vx) \log p_\theta( \vx) \mathsf{d} \hat{ \vz}^\prime \\
    & = \int \prod_i^k q_1(\hat{ \vz}_i |  \vx) \log \frac{p_\theta( \vx, \hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k)}{p_\theta(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k| \vx)} \mathsf{d}\hat{ \vz}^\prime\\
    \begin{split}
    & = \int \prod_i^k q_1(\hat{ \vz}_i |  \vx) \cdot \\
    & ~~~~\Big [ \log \frac{p_\theta( \vx,\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k)}{q(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k| \vx)} - \log \frac{p_\theta(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k | \vx)}{q(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k | \vx)} \Big ] \mathsf{d} \hat{ \vz}^\prime
    \end{split} \\
    & \geqq \int \prod_i^k q_1(\hat{ \vz}_i |  \vx) \log \frac{p_\theta( \vx, \hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k)}{q(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k| \vx)} \mathsf{d} \hat{ \vz}^\prime\\
    \begin{split}
        & = \int \prod_i^k q_1(\hat{ \vz}_i |  \vx) \cdot \\
    & ~~~~\Big [ \log p_\theta( \vx|\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k) + \log \frac{p(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k)}{q(\hat{ \vz}_1, \hat{ \vz}_2, \cdots ,\hat{ \vz}_k | \vx)} \Big ] \mathsf{d} \hat{ \vz}^\prime,
    \end{split}
\end{align}
}
where d$\hat{ \vz}^\prime = \text{d}\hat{ \vz}_1 \text{d}\hat{ \vz}_2 \cdots \text{d}\hat{ \vz}_k$.
Each IPE-transformation function operates independently, then $\log p_{\theta}( \vx|\hat{ \vz}_1, \hat{ \vz}_2, \cdots, \hat{ \vz}_k) =$ $-(k-1) \log p_\theta( \vx) + \Pi_{i=1}^k p_\theta( \vx | \hat{ \vz}_i )$. Then,
\begin{equation}
% \small
\begin{split}
    p_\theta ( \vx|\hat{ \vz}_1, \hat{ \vz}_2, \ldots , \hat{ \vz}_k) 
    &= \frac{p_\theta (\hat{ \vz}_1, \hat{ \vz}_2, \ldots, \hat{ \vz}_k |  \vx) p_\theta( \vx)}{p_\theta (\hat{ \vz}_1, \hat{ \vz}_2, \ldots, \hat{ \vz}_k)} \\ 
    &= \frac{p_\theta(\vx) \prod_{i=1}^k p_\theta(\hat{\vz}_i|\vx)}{\prod_{i=1}^k p_\theta(\hat{\vz}_i)} (\because (\hat{\vz}_i \indep \hat{\vz}_j s | \vx)) \\
    % &= \frac{p_\theta( \vx) \prod_{i=1}^k p_\theta(\hat{ \vz}_i |  \vx)}{\prod_{i=1}^k p_\theta(\hat{ \vz}_i)} (\because (\hat{ \vz}_i \indep \hat{ \vz}_js |  \vx)) \\
    &= \prod_{i=1}^k \frac{p_\theta(\hat{ \vz}_i |  \vx) p_\theta( \vx^{\frac{1}{k}})}{p_\theta(\hat{ \vz}_i)} \\
    &= p_\theta( \vx)^{-(k-1)} \prod_{i=1}^k \frac{p_\theta(\hat{ \vz}_i |  \vx) p_\theta( \vx)}{p_\theta(\hat{ \vz}_i)} \\
    &= p_\theta( \vx)^{-(k-1)} \prod_{i=1}^k p_{\theta} ( \vx | \hat{ \vz}_i),
\end{split}
\end{equation}
where $\hat{ \vz}_js = \cap_{j=1, j\neq i}^k \hat{ \vz}_j$. Therefore,
\begin{equation}
\small
\begin{split}
     & \int \prod_{i=1}^k q_i(\hat{ \vz}_i| \vx) \log p_{\theta}( \vx|\hat{ \vz}_1, \hat{ \vz}_2, \cdots, \hat{ \vz}_k) \mathsf{d} \hat{ \vz}^\prime \\
    &= \int \prod_{i=1}^k q_i(\hat{ \vz}_i| \vx)  \Big [ -(k-1) \log p_\theta( \vx) + \prod_{i=1}^k p_\theta( \vx | \hat{ \vz}_i ) \Big ] \mathsf{d} \hat{ \vz}^\prime \\ 
    &= -(k-1) \log p_\theta( \vx) + \int \prod_{i=1}^k q_i(\hat{ \vz}_i| \vx) \prod_{i=j}^k p_\theta( \vx | \hat{ \vz}_j )  \mathsf{d} \hat{ \vz}^\prime.
\end{split}
\end{equation}
Then, 
\begin{equation}
% \scriptsize
    \begin{split}
       \log p_{\theta}( \vx) & \geq \frac{1}{k}\sum_{i=1}^k \Bigg [ \int q_i(\hat{ \vz}_i| \vx) \log p_\theta( \vx|\hat{ \vz}_i) \mathsf{d}\hat{ \vz}_i \prod_{j=1, j\neq i}^k \int q_j(\hat{ \vz}_j| \vx) %q_3(\mathbf{z}_3|\mathbf{x}) \cdots q_k(\mathbf{z}_k|\mathbf{x})
       \text{d}\hat{ \vz}_j \Bigg ] \\
       %\text{d}\mathbf{z}_3 \cdots \text{d}\mathbf{z}_k\\
    & - \int \prod_i^k q_1(\hat{ \vz}_i |  \vx) \log \frac{\prod_{i=1}^k q_i(\hat{ \vz}_i| \vx)}{\prod_{i=1}^k p(\hat{ \vz}_i)} \mathsf{d}\hat{ \vz}^\prime \\
    %\end{split}\\
    %\begin{split}
    & = \frac{1}{k} \sum_{i=1}^k  \E_{q(\hat{ \vz}_i| \vx )} \log p_\theta( \vx|\hat{ \vz}_i) \\
    & - \sum_{i=1}^k \bigg [ \KL (q_{\phi}(\hat{ \vz}_i| \vx) || p(\hat{ \vz}_i)) \prod_{j=1, j\neq i}^k \int q_j(\hat{ \vz}_j| \vx) 
    \text{d}\hat{ \vz}_j \bigg ] \\ %\end{split} \\
    & = \frac{1}{k} \sum_{i=1}^k  \E_{q_{\phi} (\hat{ \vz}_i| \vx)} \log p_\theta( \vx|\hat{ \vz}_i) - \sum_{i=1}^k %q_{\phi, \psi_j}(z|x) modify this part.
     \KL (q_{\phi}(\hat{ \vz}_i| \vx) || p( \vz_i))\\
    & = \frac{1}{k} \Big [ \sum_{i=1}^k \E_{q_{\phi} (\hat{ \vz}_i| \vx)} \log p_\theta( \vx|\hat{ \vz}_i) - k  \KL (q_{\phi}(\hat{ \vz}_i| \vx) || p(\hat{ \vz}_i)) \Big ].
\end{split}
\end{equation}
%However, our model can not evaluate $q(z_1, z_2|x)$ because, latent $z$s are sampled from independent distributions and different deterministic encoders.
%Therefore, $p(z_1, z_2) = p(z_1)p(z_2)$ and we assume $q(z_1, z_2|x) = q_1(z_1|x)q_2(z_2|x)$, finally our proposed method objective function following:
Therefore, we define ELBO as:
\begin{align}
% \small
    \begin{split}
        \mathcal{L}^\prime(\phi, \theta, \psi_{i \in [1, k]};  \vx) & = \frac{1}{k} \sum_{i=1}^k  \E_{q_{\phi,\psi_i} ( \vz_i| \vx)}  \log p_\theta( \vx|\psi_i( \vz))- \\ 
        & \sum_{i=1}^k  \KL (q_{\phi, \psi_i}( \vz| \vx) || p_{\psi_i}( \vz)).
        %& = \underbrace{\frac{1}{k} \sum_{i=1}^k \mathbb{E}_{q_{\phi,\psi_i} (\mathbf{z}_i|\mathbf{x})}  \log p_\theta(x|\psi_1(\mathbf{z}), \psi_2(\mathbf{z}) \cdots, \psi_k(\mathbf{z}))}_{\circled{1}~ \big{\text{reconstruction loss}}} - \underbrace{\sum_{i=1}^k \mathcal{D}_{KL} (q_{\phi, \psi_i}(\mathbf{z}|\mathbf{x}) || p_{\psi_i}(\mathbf{z}))}_{\circled{2}~ \big{\text{KL divergence}}}.
    \end{split}
    \label{appendix: eq: elbo}
\end{align}
However, following Eq.~\ref{appendix: eq: elbo}, k samples are generated, and each sample is disentangled for different factors.
We implement the output as the average of the sum of the k samples to obtain a single sample with a superposition effect from k samples.
%Because we assume $q_{\phi}(\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k|\mathbf{x})$ and $p_{\theta}(\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k|\mathbf{x})$ are conditional independence, where $q_{\phi}(\mathbf{x}|\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k) = q_{\phi}(\mathbf{x}|\mathbf{z}_i)$ and $p_{\theta}(\mathbf{x}|\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k) = p_{\theta}(\mathbf{x}|\mathbf{z}_i)$ then, $p_{\theta}(\mathbf{z}_1, \mathbf{z}_2, \cdots, \mathbf{z}_k|\mathbf{x}) = \prod_{i=i}^k p_{\theta}(\mathbf{z}_k|\mathbf{x})$.
Moreover, the KL divergence term in Eq.~\ref{appendix: eq: elbo} represents that increasing number of MIPE-transformation is equal to increasing $\beta$ hyper-parameter in $\beta$-VAE~\cite{betaVAE}.



%, and $\text{d} \hat{ \vz} = \text{d}\hat{ \vz}_1 \text{d}\hat{ \vz}_2 \cdots \text{d}\hat{ \vz}_k$.
%However, according to the following Eq.~\ref{eq: reconstrunction error}, $k$ samples are generated, and each sample is disentangled for different factors.
%We implement the output as the average of the sum of the $k$ samples to obtain a single sample with a superposition effect of disentanglement from $k$ samples, as shown in Figure~\ref{fig:overview}.
%More derivation details of Eq.~\ref{eq: reconstrunction error} are in Appendix~\ref{appendix: elbo}.

The VAEs equipped with MIPE-transformation (MIPET-VAEs) can be trained with the following loss:
\begin{equation}
%\footnotesize
\begin{split}
    %\scriptsize
    &\mathcal{L}(\phi, \theta, \psi_{i \in [1, k ]};  \vx) = \mathcal{L}_{rec} - \mathcal{L}_{kl}
    - \mathcal{L}_{el} -\mathcal{L}_{cali}.
     %\underbrace{\frac{1}{k} \sum_{i=1}^k   \E_{q_{\phi,\psi_i} ( \vz| \vx)}  \log p_\theta( \vx|\psi_i( \vz))}_{\circled{1}~ \big{\text{reconstruction loss}}} \\
     %&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
    %-\underbrace{\sum_{i=1}^k  \KL (q_{\phi, \psi_i}( \vz| \vx) || p(\psi_i( \vz)))}_{\circled{2}~ \big{\text{KL divergence}}} - \underbrace{\sum_{i=1}^k \mathcal{L}_{c}(\hat{ \vz}_i, \hat{\boldsymbol{\epsilon}}_i) }_{ \circled{3} ~\big{\text{conversion loss}}},~\text{where} \\
    %&\mathcal{L}_{c} (\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m) = \underbrace{|| \nabla_{\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m, \boldsymbol{\lambda}_m} \mathcal{L}_{s} ||_2^2}_{\tiny \circled{3.1} ~\BBig{\text{EF similarity loss}}} + \underbrace{\text{MSE}(  \KL(q_\phi( \vz| \vx)||p_\theta( \vz)),    \KL(f_{ \vx}( \vx|\vtheta_{\hat{ \vz}_m}) || f_{ \vx}( \vx|\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}_m})))}_{\tiny \circled{3.2} ~\BBig{\text{KL divergence calibration loss}}}.
\end{split}
\label{appendix eq:objective_function}
\end{equation}

% \clearpage
% The whole process to define objective function is represented in Algorithm~\ref{alg: uipet}-\ref{alg: iet}.

% \begin{algorithm}[h]
%     \begin{algorithmic}
%         \caption{Unit invertible and partial-equivariant Transformation Function (UIPET-function)} 
        
%         \REQUIRE{matrices $ \mM_1$, and $ \mM_2$}
        
%         \ENSURE{ invertible and partial-equivariant Transformation Function $\psi(\cdot)$}
        
%         \STATE $ \mM_1$, $ \mM_2$ $\leftarrow$ $\frac{1}{2}( \mM_1 +  \mM^{\intercal}_1)$, $\frac{1}{2}( \mM_2 +  \mM^{\intercal}_2)$ \\
%         $\psi(\cdot)$ $\leftarrow$ $ \mM_1^\intercal  \mM_2$
%     \label{alg: uipet}
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[h]
%     \begin{algorithmic}
%         \caption{IE-Transformation} 
        
%         \REQUIRE{latent vector $ \vz$, and samples from prior $\boldsymbol{\epsilon}$}
        
%         \ENSURE{transformed latent vector $\hat{ \vz}$, and transformed normal Guassian distribution samples $\hat{\boldsymbol{\epsilon}}$}
        
%         \STATE $\psi(\cdot)$ $\leftarrow$ UIET-function ($ \mM_1$, $ \mM_2$)\\
%         $\hat{ \vz}$, $\hat{\boldsymbol{\epsilon}}$ $\leftarrow$ $\psi( \vz)$, $\psi(\boldsymbol{\epsilon})$
%     \label{alg: iet}
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[h]
%     \begin{algorithmic}
%         \caption{KL Divergence $\&$ Posterior Estimator}
%         \REQUIRE{latent vector $\hat{ \vz}_m$, prior samples $\hat{\boldsymbol{\epsilon}}_m$, \\
%         \hspace{7mm} Natural Parameter Generator $\Omega_1(\cdot)$, $\Omega_2(\cdot)$\\
%         \hspace{7mm} \textit{log-normalizer} $A$,
%         \textit{sufficient statistics} $T$, and \textit{evidence} $\boldsymbol{\nu}$.}
         
%         \ENSURE{KL divergence $ \KL(f_{\hat{ \vz}}(\hat{ \vz}|\theta_{\hat{ \vz}}) || f_{\hat{\boldsymbol{\epsilon}}}(\hat{\boldsymbol{\epsilon}}|\theta_{\hat{\boldsymbol{\epsilon}}}))$, and posterior $p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{X}, \boldsymbol{\nu})$ }
        
%         %\State $\theta^{\hat{z}}_m, \theta^{\hat{\epsilon}}_m$ $\gets$ $\phi_{\hat{z}_m}(\hat{z}_m), ~\phi_{\hat{\epsilon}_m}(\hat{\epsilon}_m)$
        
%         \STATE $\boldsymbol{\theta}_{\hat{ \vz}}$, $\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}$ $\leftarrow$ $\Omega_1(\hat{ \vz})$, $\Omega_2(\hat{\boldsymbol{\epsilon}})$
        
%         \STATE $A \leftarrow$ implicit semantic mask($A$) \COMMENT{Equation~\ref{eq:mask}}
%         %$\boldsymbol{\nu}$ $\leftarrow$ $\boldsymbol{\theta}_{\hat{ \veplsion}}^\intercal \mathbf{C}$
        
%         \STATE
%         $p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{X}, \boldsymbol{\nu})$ $\leftarrow$ $\exp [\boldsymbol{\theta}_{\hat{ \vz}}( \sum_{i=1}^B T(\hat{ \vz}_i) + \boldsymbol{\nu} \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) - A(\boldsymbol{\theta}_{\hat{ \vz}})]$ \COMMENT{Equation~\ref{eq:posteior_}}
        
%         \STATE
%         $ \KL(f_{\hat{ \vz}}(\hat{ \vz}|\theta_{\hat{ \vz}}) || f_{\hat{\boldsymbol{\epsilon}}}(\hat{\boldsymbol{\epsilon}}|\theta_{\hat{\boldsymbol{\epsilon}}}))$ $\leftarrow$ $A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}) - A (\boldsymbol{\theta}_{\hat{ \vz}}) + \boldsymbol{\theta}_{\hat{ \vz}}^{\intercal} \frac{\partial A(\boldsymbol{\theta}_{\hat{ \vz}})}{\partial \boldsymbol{\theta}_{\hat{ \vz}}} - \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}^{\intercal} \frac{\partial A(\boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}})}{\partial \boldsymbol{\theta}_{\hat{\boldsymbol{\epsilon}}}}$ \COMMENT{Equation~\ref{eq:kl}}
%     \label{alg: kl and post}
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[h]
%     \begin{algorithmic}
%         \caption{EF-Conversion Loss}
%         \REQUIRE{KL divergence $ \KL(f_{\hat{ \vz}}(\hat{ \vz}|\theta_{\hat{ \vz}}) || f_{\hat{\boldsymbol{\epsilon}}}(\hat{\boldsymbol{\epsilon}}|\theta_{\hat{\boldsymbol{\epsilon}}}))$, posterior $p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{X}, \boldsymbol{\nu})$, $\mu$,$\sigma$}
        
%         \ENSURE{Regularization $\mathcal{L}_{reg}$}
        
%         \STATE
%         $\mathcal{L}_{el}$ $\leftarrow$ $p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{X}, \boldsymbol{\nu}) + \boldsymbol{\lambda}_m  \KL(f_{\hat{ \vz}}(\hat{ \vz}|\theta_{\hat{ \vz}}) || f_{\hat{\boldsymbol{\epsilon}}}(\hat{\boldsymbol{\epsilon}}|\theta_{\hat{\boldsymbol{\epsilon}}}))$
        
%         \STATE
%         $\mathcal{L}_{el}$ $\leftarrow$ $|| \nabla_{\hat{ \vz}_m, \hat{\boldsymbol{\epsilon}}_m, \boldsymbol{\lambda}_m} \mathcal{L}_{el}||_2^2$
        
%         \STATE
%         $ \KL(q_\phi ( \vz| \vx) || p( \vz))$ $\leftarrow$ $0.5 \sum_{d=1}^D (1 + 2\log \sigma_j -\mu_j^2 - \sigma_j^2)$~\cite{vae}
        
%         \STATE $\mathcal{L}_{cali}$ $\leftarrow$ MSE($ \KL(f_{\hat{ \vz}}(\hat{ \vz}|\theta_{\hat{ \vz}}) || f_{\hat{\boldsymbol{\epsilon}}}(\hat{\boldsymbol{\epsilon}}|\theta_{\hat{\boldsymbol{\epsilon}}}))$, $ \KL(q_\phi ( \vz| \vx) || p( \vz)))$ \COMMENT{Equation~\ref{eq:mse}}
        
%         \STATE $\mathcal{L}$ $\leftarrow$ $\mathcal{L}_{el} + \mathcal{L}_{cali}$ \COMMENT{Equation~\ref{eq:objective_function}}
%     \label{alg: ef conversion}
%     \end{algorithmic}
% \end{algorithm}








% \clearpage
\section{Details of Experimental Environment}

\subsection{Model Architecture}

\begin{table}[ht]
    \caption{VAE architecture for dSprites dataset.}
    \centering
    \scriptsize
    \begin{tabular}{c|c}
    \hline
         Encoder & Decoder  \\
         \hline
         Input $64\times64$ binary image & input $\in \mathbb{R}^{10}$ \\
         \hline
         $4 \times 4$ conv. 32 ReLU. stride 2 & FC. 128 ReLU. \\
         \hline
         $4 \times 4$ conv. 32 ReLU. stride 2 & FC. $4 \times 4 \times 64$ ReLU. 
         \\
         \hline
         $4 \times 4$ conv. 64 ReLU. stride 2 & $4 \times 4$ upconv. 64 ReLU. stride 2. \\
         \hline
         $4 \times 4$ conv. 64 ReLU. stride 2 & $4 \times 4$ upconv. 32 ReLU. stride 2. \\
         \hline
         FC. 128. FC. $2 \times 10$ & $4 \times 4$ upconv. 32 ReLU. stride 2. \\
         \hline
         & $4 \times 4$ upconv. 1. stride 2 \\
         \hline
    \end{tabular}
    \label{tab:dsprites architecure}
    \vspace{3mm}
    
    \caption{VAE architecture for 3D Shapes, and 3D Cars datasets. For exceptional case, CLG-VAE, we ues ten dimension size on 3D Shapes dataset~\cite{commutative-vae}.}
    \centering
    \scriptsize
    \begin{tabular}{c|c}
    \hline
         Encoder & Decoder  \\
         \hline
         Input $64\times64 \times 3$ RGB image & input $\in \mathbb{R}^{6}$ (3D Shapes), $\mathbb{R}^{10}$ (3D Cars) \\
         \hline
         $4 \times 4$ conv. 32 ReLU. stride 2 & FC. 256 ReLU. \\
         \hline
         $4 \times 4$ conv. 32 ReLU. stride 2 & FC. $4 \times 4 \times 64$ ReLU. 
         \\
         \hline
         $4 \times 4$ conv. 64 ReLU. stride 2 & $4 \times 4$ upconv. 64 ReLU. stride 2. \\
         \hline
         $4 \times 4$ conv. 64 ReLU. stride 2 & $4 \times 4$ upconv. 32 ReLU. stride 2. \\
         \hline
         FC. 256. FC. $2 \times 10$ & $4 \times 4$ upconv. 32 ReLU. stride 2. \\
         \hline
         & $4 \times 4$ upconv. 3. stride 2 \\
         \hline
    \end{tabular}
    
    \label{tab:others architecure}
\end{table}

\subsection{Details of Setting}
% \begin{table}[ht]
% \caption{dSprites and 3D Cars: epochs for dSprites and 3D cars are 30 and 200, respectively.}
% \scriptsize
%     %\begin{subtable}[h]{0.45\textwidth}
%         \centering
%         \begin{tabular}{c|c|c}
%         \hline
%         models & hyper-parameters & values \\
%         \hline \hline
%         \multirow{7}{*}{common}& batch size & 256 \\
%         & epoch & $\{$30, 200$\}$ \\
%         & optim & Adam \\
%         & lr & 4e-4 \\
%         & lr for MIPET & 4e-4 \\
%         & weight decay & 1e-4 \\
%         & latent dim & 10 \\
%         \hline
%         $\beta$-VAE & $\#$ of IE and EF & $\{$1, 2, 4, 10$\}$ \\
%         \hline
%         \multirow{3}{*}{$\beta$-TCVAE} & $\beta$& $\{$4, 6$\}$ \\
%         & $\#$ of IE and EF & $\{$1, 3$\}$ \\
%         & $\alpha$, $\gamma$ & 1.0 \\
%         \hline
%         \multirow{4}{*}{CLG-VAE}& $\lambda_{\text{decomp}}$ & 40 \\
%         & $\lambda_{\text{hessian}}$ & 40 \\
%         & forward group & 0.2 \\
%         & group reconst & $\{$0.2, 0.5, 0.7$\}$ \\
%         \hline
%        \end{tabular}
%        \label{tab:week1}

%     \vspace{3mm}

%  \caption{lr is learning rate, latent dim is dimension size of latent vector, group reconst is group reconstrunction, and forward group is forward group pass. }
% \scriptsize
%     %\begin{subtable}[h]{0.45\textwidth}
%         \centering
%         \begin{tabular}{c|c|c}
%         \hline
%         models & hyper-parameters & values \\
%         \hline \hline
%         \multirow{5}{*}{common}& batch size & 256 \\
%         & epoch & 67 \\
%         & optim & Adam \\
%         & lr & 4e-4 \\
%         & lr for MIPET & 4e-4 \\
%         %& weight decay & 1e-4 \\
%         %& latent dim & 10 \\
%         \hline
%         \multirow{3}{*}{$\beta$-VAE} & $\#$ of IE and EF & $\{$1, 2, 4, 10$\}$ \\
%         & weight decay & 0.0 \\
%         & latent dim & 6 \\
%         \hline
%         \multirow{5}{*}{$\beta$-TCVAE} & $\beta$& $\{$4, 6$\}$ \\
%         & $\#$ of IE and EF & $\{$1, 3$\}$ \\
%         & $\alpha$, $\gamma$ & 1.0 \\
%         & weight decay & 1e-4 \\
%         & latent dim & 6 \\
%         \hline
%         \multirow{6}{*}{CLG-VAE}& $\lambda_{\text{decomp}}$ & 40 \\
%         & $\lambda_{\text{hessian}}$ & 40 \\
%         & forward group & 0.2 \\
%         & group reconst & $\{$0.2, 0.5, 0.7$\}$ \\
%         & weight decay & 0.0 \\
%         & latent dim & 10 \\
%         \hline
%        \end{tabular}
%        %\caption{3D Shapes}
%        \label{tab:week1}

%      \label{tab:temps}
% \end{table}

\begin{table}[ht]
\caption{Hyper-parameters for dSprites, 3D Cars, and 3D Shapes. The epochs for dSprites and 3D cars are 30 and 200, respectively. lr is learning rate, latent dim is latent vector size, group reconst is group reconstruction, and forward group is forward group pass.}
\centering
\scriptsize
\begin{subtable}[h]{0.45\textwidth}
    \caption{Hyper-parameters for dSprites and 3D Cars}
    \centering
    \begin{tabular}{c|c|c}
        \hline
        models & hyper-parameters & values \\
        \hline \hline
        \multirow{7}{*}{common}& batch size & 256 \\
        & epoch & $\{30, 200\}$ \\
        & optim & Adam \\
        & lr & 4e-4 \\
        & lr for MIPET & 4e-4 \\
        & weight decay & 1e-4 \\
        & latent dim & 10 \\
        \hline
        $\beta$-VAE & $\#$ of IE and EF & $\{1, 2, 4, 10\}$ \\
        \hline
        \multirow{3}{*}{$\beta$-TCVAE} & $\beta$ & $\{4, 6\}$ \\
        & $\#$ of IE and EF & $\{1, 3\}$ \\
        & $\alpha$, $\gamma$ & 1.0 \\
        \hline
        \multirow{4}{*}{CLG-VAE}& $\lambda_{\text{decomp}}$ & 40 \\
        & $\lambda_{\text{hessian}}$ & 40 \\
        & forward group & 0.2 \\
        & group reconst & $\{0.2, 0.5, 0.7\}$ \\
        \hline
    \end{tabular}
    \label{tab:dsprites_3dcars}
\end{subtable}
\hfill
\begin{subtable}[h]{0.45\textwidth}
    \caption{Hyper-parameters for 3D Shapes}
    \centering
    \begin{tabular}{c|c|c}
        \hline
        models & hyper-parameters & values \\
        \hline \hline
        \multirow{5}{*}{common}& batch size & 256 \\
        & epoch & 67 \\
        & optim & Adam \\
        & lr & 4e-4 \\
        & lr for MIPET & 4e-4 \\
        \hline
        \multirow{3}{*}{$\beta$-VAE} & $\#$ of IE and EF & $\{1, 2, 4, 10\}$ \\
        & weight decay & 0.0 \\
        & latent dim & 6 \\
        \hline
        \multirow{5}{*}{$\beta$-TCVAE} & $\beta$& $\{4, 6\}$ \\
        & $\#$ of IE and EF & $\{1, 3\}$ \\
        & $\alpha$, $\gamma$ & 1.0 \\
        & weight decay & 1e-4 \\
        & latent dim & 6 \\
        \hline
        \multirow{6}{*}{CLG-VAE}& $\lambda_{\text{decomp}}$ & 40 \\
        & $\lambda_{\text{hessian}}$ & 40 \\
        & forward group & 0.2 \\
        & group reconst & $\{0.2, 0.5, 0.7\}$ \\
        & weight decay & 0.0 \\
        & latent dim & 10 \\
        \hline
    \end{tabular}
    \label{tab:3dshapes}
\end{subtable}
\label{tab:temps}
\end{table}


\subsection{Impact of Symmetric Matrix Exponential}
\label{subsec: discussion}

%\begin{table}[]
%    \caption{The ratio of seeds to show better performance with symmetric matrix
%    }
%    \vskip 0.15in
%    \centering
%    \small
%    \begin{tabular}{|c|c|c|}
%    \hline
%         dSprites & 3D Shapes & 3D Cars \\
%         \hline
%          0.58 & 0.56 & 0.67 \\
%         \hline
%    \end{tabular}

%    \label{tab: main empirical symmetric}
%    \vskip -0.15in
%\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[h]
    \centering
    \caption{The ratio of seeds to show better performance with symmetric matrix}
    \begin{tabular}{|c|c|c|}
    \hline
         dSprites & 3D Shapes & 3D Cars \\
         \hline
          0.58 & 0.56 & 0.67 \\
         \hline
    \end{tabular}
    \label{tab: main empirical symmetric}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%In this section, we show the empirical result of discussion on~\ref{discussion: why symmetric?}. 
We empirically show the benefit of using a symmetric matrix for $\psi$. Table~\ref{tab: main empirical symmetric} shows the ratio of runs with a symmetric matrix, which shows better performance than unrestricted matrices, to the total 240 (60 models $\times$ 4 metrics) runs for each dataset. 
%We estimate the ratio as $\eps = \frac{ \text{\# of cases that symmetric matrix shows better performance}}{ \text{\# of total case}}$ with 240 cases on each dataset.
%We count cases when the proposed method result is higher than the asymmetric then divide it by the total case 240 (60 models $\times$ 4 metrics).
%The results are presented in Table~\ref{tab: main empirical symmetric}.
All results are higher than 0.5, which implies that the constraint enhances I2L equivariance even with uncertain factors. 
%These results imply that our proposed method finds equivariant transformation function over latent space, in which group element is potentially expressed by subgroup $G_1^\prime$ better than the asymmetric cases.



\subsection{Additional Experiment of Computing Complexity}
\begin{table}[h]
    \caption{Training complexity.}
    \centering
    \begin{tabular}{|c|c|}
    \hline
         $\#$ of IE & Complexity \\
         \hline
         0 & $\times$ 1.00 \\
         1 & $\times$ 0.75 \\
         3 & $\times$ 0.50 \\
         4 & $\times$ 0.33 \\
         \hline
    \end{tabular}

    \label{tab: complexity}
\end{table}

We additionally estimate the computing complexity depending on the number of IPE-transformation.
The results are in Table~\ref{tab: complexity} and represent the training time complexity compare to baselines (when the number of IE is equal to 0).

% \clearpage
\subsection{Qualitative Analysis}

%We randomly sample 10 images from dSprites and 3D Cars datasets, and changes their latent vectors value from $\{$-2, 2$\}$ with 10 intervals on each dimension, and then generate their corresponding output images.
We randomly sample an image for each dimension of the latent vector space and creates 10 variants of its generated latent vector by selecting values from $\{$-2, 2$\}$ with 10 intervals for the dimension, then generate their corresponding output images.
%divide variants of latent vector from $\{$-2, 2$\}$ with 10 intervals on each dimension.
For the generation, we select $\beta$-TCVAE (6), which shows the best FVM scores in dSprites dataset.
%We select the highest score in terms of FVM metric on $\beta$-TCVAE (6) with dSprites, and 3D Cars datasets. %because FVM is estimated regardless of ground-truth label.
Thereafter, we evaluate the semantic roles of each dimension before and after applying MIPE-transformation function.
%We want to observe the impact of the invertible and equivariant properties, and $\beta$-TCVAE generally achieves a high score on FVM on both datasets.

%\subsubsection{dSprites}
In Figure~\ref{subfig:dsprites}, $\beta$-TCVAE struggles with y-position and rotation, as shown on the 6$^{th}$ row, and with scale and shape represented on the 7$^{th}$ row.
On the contrary, MIPET-$\beta$-TCVAE separates y-position and rotation factor (10$^{th}$, and 7$^{th}$ rows), also the activated dimensions of MIPET-$\beta$-TCVAE are not overlapped with each factor.
%Even though FVM score of $\beta$-TCVAE is slightly higer than CHIC-$\beta$-TCVAE, 88.13 and 86.88, 
Applied our method on $\beta$-TCVAE shows better disentangled representation on dSprites dataset.
These results also show that our proposed method improves disentangled representation learning.
As shown in the Figure~\ref{appendix fig: dsprites}, $\beta$-VAE struggles with rotation and scale factors in 4$^{th}$dimension. 
Also, it struggles with x-position and scale factors in 8$^{th}$ dimension, and x-position and rotation factors in 9$^{th}$ dimension.
However, MIPET-$\beta$-VAE only struggles with rotation and shape factors in 5$^{th}$ dimension.
As shown in the Figure~\ref{appendix fig: dsprites (com)}, CLG-VAE struggles with rotation and shape factors in 2$^{nd}$ dimension, and shape and scale factors in 7$^{th}$ dimension. However, MIPET-CLG-VAE separates rotation and shape factors in 10$^{th}$, and 1$^{st}$ dimensions respectively.

The qualitative analysis with 3D Shapes dataset, as shown in the Figure~\ref{appendix fig: 3dshape}, $\beta$-TCVAE struggles with shape and scale in $5^{th}$ dimension.
However MIPET-$\beta$-TCVAE
% $\beta$-VAE struggles with all factors, and only the object color factor is divided in 6$^{th}$ dimension.
% However, this factor is still activated with scale factor in 3$^{rd}$ dimension.
% Although MIPET-$\beta$-VAE struggles with reconstruction, it is less struggle with than $\beta$-VAE. 
As shown in the Figure~\ref{appendix fig: 3dshape (com)}, CLG-VAE struggles with shape and wall color factors in 4$^{th}$ dimension, and shape and object color factors in 7$^{th}$ dimension.
In particular, it struggles with tree factors in 9$^{th}$ dimension.
On the other hand, MIPET-CLG-VAE separates shape, wall, and object color factors.

The qualitative analysis with 3D Cars dataset, as show in Figure~\ref{appendix fig: 3dcars}, the left side is the $\beta$-TCVAE result, and it struggles with body, and azimuth factors shown in the 7$^{th}$ row.
However, MIPET-$\beta$-TCVAE separates azimuth (6$^{th}$ row) and body (1$^{st}$ row).
 %although the number of activated dimensions is less in MIPET case, %more dimensions are activated with factors on MIPET-$\beta$-TCVAE and 
In particular, MIPET-$\beta$-TCVAE learns \textit{color} factor (3$^{rd}$ row) which does not exist on $\beta$-TCVAE.

\begin{figure}
    \centering
        %\begin{subfigure}[]{0.99\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figure_dsprites_quali_ver10.jpg}
        %\caption{dSprites}
        %\label{subfig:dsprites}
        %\end{subfigure}
    
        %\begin{subfigure}[]{0.99\textwidth}
        %\centering
        %\includegraphics[width=0.95\textwidth]{figures/car_quali_ver07.png}
        %\caption{3D Cars}
        %\label{subfig:3dcar}
        %\end{subfigure}
    \caption{Qualitative results on dSprites.
    %The $\text{\#}$ factor represents the number of activated factors on each dimension.
    The left-side grids are input images and their variants by changing activations of each dimension of latent vectors. The first row shows input images. 
    The right-side table shows matching pre-defined factors of the dataset (red: MIPET, blue: no MIPET). 
%    The right side table shows each dataset factor, and we verify the activated factors on each dimension. 
   % Each row corresponds to each 
  %  order of dimensions, and the $\text{\#}$ factor represents the number of activated factors on each dimension. 
  %  The red checkmark is the MIPET case, and the blue is for $\beta$-TCVAE. The first column in MIPET-VAEs is the input.
  % and we change the latent vector values from $\{$-2, 2$\}$.
    %and Orig is original model of $\beta$-TCVAE (6) and CHIC is applied our proposed method on original model.
    }
    \label{subfig:dsprites}

    \centering
    \includegraphics[width=0.9\textwidth]{figure_dsprites_quali_betavae_ver02.jpg}
    \caption{Qualitative analysis result of $\beta$-VAE and MIPET-$\beta$-VAE.}
    \label{appendix fig: dsprites}

    \centering
    \includegraphics[width=0.9\textwidth]{figure_dsprites_quali_commut_ver01.jpg}
    \caption{Qualitative analysis result of CLG-VAE (0.2) and MIPET-CLG-VAE (0.2) with dSprites.}
    \label{appendix fig: dsprites (com)}
    
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figure_3dshape_quali_betatc.PNG}
    \caption{The Shape is object shape, Orien is an orientation of object, Scale is a scale factor of object, Wall is wall color factor, Floor is floor color, and Object is object color factors. It represents the $\beta$-VAE ($\beta=2$) results.
    %it perfectly separates all factors in each dimension.
    }
    \label{appendix fig: 3dshape}

    \centering
    \includegraphics[width=0.9\textwidth]{figure_3dshape_quali_commut_ver02.jpg}
    \caption{Qualitative analysis result of CLG-VAE (0.2) and MIPET-CLG-VAE (0.2) with 3D Shapes.
    }
    \label{appendix fig: 3dshape (com)}

    \centering
    \includegraphics[width=0.9\textwidth]{figure_car_quali_ver08.jpg}
    \caption{Qualitative analysis result of $\beta$-VAE (4.0) with 3D Cars.}
    \label{appendix fig: 3dcars}
\end{figure}

\end{document}
