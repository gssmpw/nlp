@inproceedings{
intel-vae,
title={On Incorporating Inductive Biases into {VAE}s},
author={Ning Miao and Emile Mathieu and Siddharth N and Yee Whye Teh and Tom Rainforth},
booktitle={International Conference on Learning Representations},
year={2022},
}

@InProceedings{pmlr-v139-kim21i,
  title = 	 {The Lipschitz Constant of Self-Attention},
  author =       {Kim, Hyunjik and Papamakarios, George and Mnih, Andriy},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {5562--5571},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/kim21i/kim21i.pdf},
  abstract = 	 {Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.}
}


@InProceedings{commutative-vae,
  title = 	 {Commutative Lie Group VAE for Disentanglement Learning},
  author =       {Zhu, Xinqi and Xu, Chang and Tao, Dacheng},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12924--12934},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhu21f/zhu21f.pdf},
  abstract = 	 {We view disentanglement learning as discovering an underlying structure that equivariantly reflects the factorized variations shown in data. Traditionally, such a structure is fixed to be a vector space with data variations represented by translations along individual latent dimensions. We argue this simple structure is suboptimal since it requires the model to learn to discard the properties (e.g. different scales of changes, different levels of abstractness) of data variations, which is an extra work than equivariance learning. Instead, we propose to encode the data variations with groups, a structure not only can equivariantly represent variations, but can also be adaptively optimized to preserve the properties of data variations. Considering it is hard to conduct training on group structures, we focus on Lie groups and adopt a parameterization using Lie algebra. Based on the parameterization, some disentanglement learning constraints are naturally derived. A simple model named Commutative Lie Group VAE is introduced to realize the group-based disentanglement learning. Experiments show that our model can effectively learn disentangled representations without supervision, and can achieve state-of-the-art performance without extra constraints.}
}


@inproceedings{hessian-penalty,
  title={The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement},
  author={Peebles, William and Peebles, John and Zhu, Jun-Yan and Efros, Alexei A. and Torralba, Antonio},
  booktitle={Proceedings of European Conference on Computer Vision (ECCV)},
  year={2020}
}

@misc{vae,
  doi = {10.48550/ARXIV.1312.6114},
  
  author = {Kingma, Diederik P and Welling, Max},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Auto-Encoding Variational Bayes},
  
  publisher = {arXiv},
  
  year = {2013},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{disentanglement-definition,
  author    = {Irina Higgins and
               David Amos and
               David Pfau and
               S{\'{e}}bastien Racani{\`{e}}re and
               Lo{\"{\i}}c Matthey and
               Danilo J. Rezende and
               Alexander Lerchner},
  title     = {Towards a Definition of Disentangled Representations},
  journal   = {CoRR},
  volume    = {abs/1812.02230},
  year      = {2018},
  eprinttype = {arXiv},
  eprint    = {1812.02230},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-02230.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
groupified-vae,
title={Towards Building A Group-based Unsupervised Representation Disentanglement Framework},
author={Tao Yang and Xuanchi Ren and Yuwang Wang and Wenjun Zeng and Nanning Zheng},
booktitle={International Conference on Learning Representations},
year={2022},
}


@Article{matrix_exponential_approx,
AUTHOR = {Bader, Philipp and Blanes, Sergio and Casas, Fernando},
TITLE = {Computing the Matrix Exponential with an Optimized Taylor Polynomial Approximation},
JOURNAL = {Mathematics},
VOLUME = {7},
YEAR = {2019},
NUMBER = {12},
ARTICLE-NUMBER = {1174},
ISSN = {2227-7390},
ABSTRACT = {A new way to compute the Taylor polynomial of a matrix exponential is presented which reduces the number of matrix multiplications in comparison with the de-facto standard Paterson-Stockmeyer method for polynomial evaluation. Combined with the scaling and squaring procedure, this reduction is sufficient to make the Taylor method superior in performance to Pad&eacute; approximants over a range of values of the matrix norms. An efficient adjustment to make the method robust against overscaling is also introduced. Numerical experiments show the superior performance of our method to have a similar accuracy in comparison with state-of-the-art implementations, and thus, it is especially recommended to be used in conjunction with Lie-group and exponential integrators where preservation of geometric properties is at issue.},
DOI = {10.3390/math7121174}
}

@inproceedings{
exp_family_bayesian,
title={Natural Posterior Network: Deep Bayesian Predictive Uncertainty for Exponential Family Distributions},
author={Bertrand Charpentier and Oliver Borchert and Daniel Z{\"u}gner and Simon Geisler and Stephan G{\"u}nnemann},
booktitle={International Conference on Learning Representations},
year={2022},
}

@book{prml,
author = {Bishop, Christopher M.},
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
year = {2006},
isbn = {0387310738},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}


@InProceedings{factor-vae,
  title = 	 {Disentangling by Factorising},
  author =       {Kim, Hyunjik and Mnih, Andriy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2649--2658},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18b/kim18b.pdf},
  abstract = 	 {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.}
}

@BOOK{Hall,
      author       = {Hall, Brian C.},
      title        = {{L}ie {G}roups, {L}ie {A}lgebras, and {R}epresentations:
                      {A}n {E}lementary {I}ntroduction},
      volume       = {222},
      address      = {New York},
      publisher    = {Springer},
      reportid     = {PUBDB-2017-132702},
      isbn         = {9781441923134},
      series       = {Graduate Texts in Mathematics},
      pages        = {351 S},
      year         = {2010},
      abstract     = {Lie groups, Lie algebras, and representation theory are the
                      main focus of this text. In order to keep the prerequisites
                      to a minimum, the author restricts attention to matrix Lie
                      groups and Lie algebras. This approach keeps the discussion
                      concrete, allows the reader to get to the heart of the
                      subject quickly, and covers all of the most interesting
                      examples. The book also introduces the often-intimidating
                      machinery of roots and the Weyl group in a gradual way,
                      using examples and representation theory as motivation. The
                      text is divided into two parts. The first covers Lie groups
                      and Lie algebras and the relationship between them, along
                      with basic representation theory. The second part covers the
                      theory of semisimple Lie groups and Lie algebras, beginning
                      with a detailed analysis of the representations of SU(3).
                      The author illustrates the general theory with numerous
                      images pertaining to Lie algebras of rank two and rank
                      three, including images of root systems, lattices of
                      dominant integral weights, and weight diagrams. This book is
                      sure to become a standard textbook for graduate students in
                      mathematics and physics with little or no prior exposure to
                      Lie theory. Brian Hall is an Associate Professor of
                      Mathematics at the University of Notre Dame},
      keywords     = {Lie groups (DE-H253) / Lie algebra (DE-H253) /
                      Baker-Campbell-Hausdorff formula (DE-H253) / representations
                      (DE-H253) / root systems (DE-H253) / weights (DE-H253)},
      ddc          = {512.482},
      shelfmark    = {M Hal},
      typ          = {PUB:(DE-HGF)3},
}

@inproceedings{beta-tcvae,
 author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Isolating Sources of Disentanglement in Variational Autoencoders},
 volume = {31},
 year = {2018}
}

@inproceedings{
sap,
title={{VARIATIONAL} {INFERENCE} {OF} {DISENTANGLED} {LATENT} {CONCEPTS} {FROM} {UNLABELED} {OBSERVATIONS}},
author={Abhishek Kumar and Prasanna Sattigeri and Avinash Balakrishnan},
booktitle={International Conference on Learning Representations},
year={2018},
}

@inproceedings{
dci,
title={A framework for the quantitative evaluation of disentangled representations},
author={Cian Eastwood and Christopher K. I. Williams},
booktitle={International Conference on Learning Representations},
year={2018},
}

@misc{dsprites17,
author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
title = {dSprites: Disentanglement testing Sprites dataset},
howpublished= {https://github.com/deepmind/dsprites-dataset/},
year = "2017",
}

@misc{3dshapes18,
  title={3D Shapes Dataset},
  author={Burgess, Chris and Kim, Hyunjik},
  howpublished={https://github.com/deepmind/3dshapes-dataset/},
  year={2018}
}

@article{celebA(64),
  title={Deep Learning Face Attributes in the Wild},
  author={Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},
  journal={2015 IEEE International Conference on Computer Vision (ICCV)},
  year={2015},
  pages={3730-3738}
}

@inproceedings{zero-shot,
author = {Tenenbaum, Josh},
title = {Building Machines That Learn and Think Like People},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent successes in artificial intelligence and machine learning have been largely driven by methods for sophisticated pattern recognition, including deep neural networks and other data-intensive methods. But human intelligence is more than just pattern recognition. And no machine system yet built has anything like the flexible, general-purpose commonsense grasp of the world that we can see in even a one-year-old human infant. I will consider how we might capture the basic learning and thinking abilities humans possess from early childhood, as one route to building more human-like forms of machine learning and thinking.At the heart of human common sense is our ability to model the physical and social environment around us: to explain and understand what we see, to imagine things we could see but haven't yet, to solve problems and plan actions to make these things real, and to build new models as we learn more about the world. I will focus on our recent work reverse-engineering these capacities using methods from probabilistic programming, program induction and program synthesis, which together with deep learning methods and video game simulation engines, provide a toolkit for the joint enterprise of modeling human intelligence and making AI systems smarter in more human-like ways.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {5},
numpages = {1},
keywords = {cognitive science, probabilistic inference, machine learning},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@InProceedings{domain-1,
author = {Li, Yu-Jhe and Lin, Ci-Siang and Lin, Yan-Bo and Wang, Yu-Chiang Frank},
title = {Cross-Dataset Person Re-Identification via Unsupervised Pose Disentanglement and Adaptation},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{domain-2,
  title={Joint disentangling and adaptation for cross-domain person re-identification},
  author={Zou, Yang and Yang, Xiaodong and Yu, Zhiding and Vijayakumar, Bhagavatula and Kautz, Jan},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2020}
}

@ARTICLE{disen_definitiaon_1,
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Representation Learning: A Review and New Perspectives}, 
  year={2013},
  volume={35},
  number={8},
  pages={1798-1828},
  doi={10.1109/TPAMI.2013.50}}
  
  @article{disen_definitiaon_2,
  author    = {Irina Higgins and
               David Amos and
               David Pfau and
               S{\'{e}}bastien Racani{\`{e}}re and
               Lo{\"{\i}}c Matthey and
               Danilo J. Rezende and
               Alexander Lerchner},
  title     = {Towards a Definition of Disentangled Representations},
  journal   = {CoRR},
  volume    = {abs/1812.02230},
  year      = {2018},
  eprinttype = {arXiv},
  eprint    = {1812.02230},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-02230.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{betaVAE,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Irina Higgins and Lo{\"i}c Matthey and Arka Pal and Christopher P. Burgess and Xavier Glorot and Matthew M. Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle={ICLR},
  year={2017}
}


@InProceedings{vae-inductive-bias,
  title = 	 {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author =       {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4114--4124},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf},
  abstract = 	 {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than $12000$ models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.}
}

@inproceedings{classi_1,
  title     = {Understanding Failures of Deep Networks via Robust Feature Extraction},
  author    = {Sahil Singla and Besmira Nushi and Shital Shah and Ece Kamar and Eric Horvitz},
  booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2021},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2021},}
  %url       = {https://openaccess.thecvf.com/content/CVPR2021/papers/Singla_Understanding_Failures_of_Deep_Networks_via_Robust_Feature_Extraction_CVPR_2021_paper.pdf},
%}

@article{t-vae,
  author    = {T. Anderson Keller and
               Max Welling},
  title     = {Topographic VAEs learn Equivariant Capsules},
  journal   = {CoRR},
  volume    = {abs/2109.01394},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2109.01394},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-01394.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{broadcast-decoder,
  author    = {Nicholas Watters and
               Lo{\"{\i}}c Matthey and
               Christopher P. Burgess and
               Alexander Lerchner},
  title     = {Spatial Broadcast Decoder: {A} Simple Architecture for Learning Disentangled
               Representations in VAEs},
  journal   = {CoRR},
  volume    = {abs/1901.07017},
  year      = {2019},
  eprinttype = {arXiv},
  eprint    = {1901.07017},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-07017.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{info-gan,
 author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
 volume = {29},
 year = {2016}
}

@article{oogan, title={OOGAN: Disentangling GAN with One-Hot Sampling and Orthogonal Regularization}, volume={34}, DOI={10.1609/aaai.v34i04.5919}, abstractNote={&lt;p&gt;Exploring the potential of GANs for unsupervised disentanglement learning, this paper proposes a novel GAN-based disentanglement framework with One-Hot Sampling and Orthogonal Regularization (OOGAN). While previous works mostly attempt to tackle disentanglement learning through VAE and seek to implicitly minimize the Total Correlation (TC) objective with various sorts of approximation methods, we show that GANs have a natural advantage in disentangling with an alternating latent variable (noise) sampling method that is straightforward and robust. Furthermore, we provide a brand-new perspective on designing the structure of the generator and discriminator, demonstrating that a minor structural change and an orthogonal regularization on model weights entails an improved disentanglement. Instead of experimenting on simple toy datasets, we conduct experiments on higher-resolution images and show that OOGAN greatly pushes the boundary of unsupervised disentanglement.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Bingchen and Zhu, Yizhe and Fu, Zuohui and de Melo, Gerard and Elgammal, Ahmed}, year={2020}, month={Apr.}, pages={4836-4843} }

@inproceedings{dis-gan,
author={Xinqi Zhu and Chang Xu and Dacheng Tao},
title={Where and What? Examining Interpretable Disentangled Representations},
booktitle={CVPR},
year={2021}
}

@inproceedings{ib-gan, title={IB-GAN: Disengangled Representation Learning with Information Bottleneck Generative Adversarial Networks}, author={Jeon, Insu and Lee, Wonkwang and Pyeon, Myeongjang and Kim, Gunhee}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={35}, number={9}, pages={7926--7934}, year={2021} }

@inproceedings{info-gan-cr,
  author={Zinan Lin and Kiran Koshy Thekumparampil and Giulia C. Fanti and Sewoong Oh},
  title={InfoGAN-CR and ModelCentrality: Self-supervised Model Training and Selection for Disentangling GANs},
  year={2020},
  cdate={1577836800000},
  pages={6127-6139},
  booktitle={ICML},
  crossref={conf/icml/2020}
}

@inproceedings{gan,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 volume = {27},
 year = {2014}
}

@inproceedings{3d-car-dataset,
 author = {Reed, Scott E and Zhang, Yi and Zhang, Yuting and Lee, Honglak},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Visual Analogy-Making},
 volume = {28},
 year = {2015}
}


@article{hyper-vae,
  added-at = {2019-07-17T01:59:08.000+0200},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  biburl = {https://www.bibsonomy.org/bibtex/2e2d236a0bacacb8ad7da018a8207221a/becker},
  interhash = {e0a48bada3d58f88f8654937c1e07b9e},
  intrahash = {e2d236a0bacacb8ad7da018a8207221a},
  journal = {34th Conference on Uncertainty in Artificial Intelligence (UAI-18)},
  keywords = {afcs auto autoencoder bayes bayesian encoder generative model related vae variational work},
  timestamp = {2019-07-17T01:59:08.000+0200},
  title = {Hyperspherical Variational Auto-Encoders},
  year = 2018
}

@article{gaussian-mixture-vae,
  added-at = {2018-08-13T00:00:00.000+0200},
  author = {Dilokthanakul, Nat and Mediano, Pedro A. M. and Garnelo, Marta and Lee, Matthew C. H. and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray},
  biburl = {https://www.bibsonomy.org/bibtex/260cbf07898b2fb71821260133d003611/dblp},
  ee = {http://arxiv.org/abs/1611.02648},
  interhash = {cfd3ff43a0c0dd31a26f92c4c66c089b},
  intrahash = {60cbf07898b2fb71821260133d003611},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2018-08-14T13:45:16.000+0200},
  title = {Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders.},
  volume = {abs/1611.02648},
  year = 2016
}

@inproceedings{3d-car-setting,
  title   = {Learning Disentangled Representation by Exploiting Pretrained Generative Models: A Contrastive Learning View},
  author  = {Ren, Xuanchi and Yang, Tao and Wang, Yuwang and Zeng, Wenjun},
  booktitle = {ICLR},
  year    = {2022}
}


@InProceedings{invertible-matrix-exponential,
  title = 	 {Generative Flows with Matrix Exponential},
  author =       {Xiao, Changyi and Liu, Ligang},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10452--10461},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/xiao20a/xiao20a.pdf},
  abstract = 	 {Generative flows models enjoy the properties of tractable exact likelihood and efficient sampling, which are composed of a sequence of invertible functions. In this paper, we incorporate matrix exponential into generative flows. Matrix exponential is a map from matrices to invertible matrices, this property is suitable for generative flows. Based on matrix exponential, we propose matrix exponential coupling layers that are a general case of affine coupling layers and matrix exponential invertible 1 x 1 convolutions that do not collapse during training. And we modify the networks architecture to make training stable and significantly speed up the training process. Our experiments show that our model achieves great performance on density estimation amongst generative flows models.}
}

@book{invertible,
  title={Lie Groups, Lie Algebras, and Representations: An Elementary Introduction},
  author={Hall, B.},
  isbn={9783319134673},
  series={Graduate Texts in Mathematics},
  year={2015},
  publisher={Springer International Publishing}
}

@article{equivariant1,
  author    = {Avishek Joey Bose and
               Ivan Kobyzev},
  title     = {Equivariant Discrete Normalizing Flows},
  journal   = {CoRR},
  volume    = {abs/2110.08649},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2110.08649},
  timestamp = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-08649.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{commutative,
title = {On the commuting probability in finite groups},
journal = {Journal of Algebra},
volume = {300},
number = {2},
pages = {509-528},
year = {2006},
note = {Special issue celebrating the 70th birthday of Bernd Fischer},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2005.09.044},
author = {Robert M. Guralnick and Geoffrey R. Robinson}
}

@inproceedings{equivariant2,
 author = {Garcia Satorras, Victor and Hoogeboom, Emiel and Fuchs, Fabian and Posner, Ingmar and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {4181--4192},
 publisher = {Curran Associates, Inc.},
 title = {E(n) Equivariant Normalizing Flows},
 volume = {34},
 year = {2021}
}

@misc{
mix-gaussian,
title={Towards Unsupervised Classification with Deep Generative Models},
author={Dimitris Kalatzis and Konstantia Kotta and Ilias Kalamaras and Anastasios Vafeiadis and Andrew Rawstron and Dimitris Tzovaras and Kostas Stamatopoulos},
year={2018},
}

@inproceedings{exponential_uncertainty_01,
    title={{Natural} {Posterior} {Network}: {Deep} {Bayesian} {Predictive} {Uncertainty} for {Exponential} {Family} {Distributions}},
    author={Charpentier, Bertrand and Borchert, Oliver and Z\"{u}gner, Daniel and Geisler, Simon and G\"{u}nnemann, Stephan},
    booktitle={International Conference on Learning Representations},
    year={2022}
}

@inproceedings{exponential_uncertainty_00,
  author={Bertrand Charpentier and Daniel Zügner and Stephan Günnemann},
  title={Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts},
  year={2020},
  cdate={1577836800000},
  booktitle={NeurIPS},
}

@inproceedings{gibbs,
  title={Learning Sparse Topographic Representations with Products of Student-t Distributions},
  author={Max Welling and Geoffrey E. Hinton and Simon Osindero},
  booktitle={NIPS},
  year={2002}
}


@InProceedings{cascadeVAE,
  title = 	 {Learning Discrete and Continuous Factors of Data via Alternating Disentanglement},
  author =       {Jeong, Yeonwoo and Song, Hyun Oh},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3091--3099},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jeong19d/jeong19d.pdf},
  abstract = 	 {We address the problem of unsupervised disentanglement of discrete and continuous explanatory factors of data. We first show a simple procedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or perform importance sampling, via cascading the information flow in the beta-VAE framework. Furthermore, we propose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by employing a separate discrete inference procedure. This leads to an interesting alternating minimization problem which switches between finding the most likely discrete configuration given the continuous factors and updating the variational encoder based on the computed discrete factors. Experiments show that the proposed method clearly disentangles discrete factors and significantly outperforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at https://github.com/snumllab/DisentanglementICML19.}
}

@inproceedings{
hierarchical-vae,
title={PROGRESSIVE LEARNING AND DISENTANGLEMENT OF HIERARCHICAL REPRESENTATIONS},
author={Zhiyuan Li and Jaideep Vitthal Murkute and Prashnna Kumar Gyawali and Linwei Wang},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{posterior-collapse,
    title = "Generating Sentences from a Continuous Space",
    author = "Bowman, Samuel R.  and
      Vilnis, Luke  and
      Vinyals, Oriol  and
      Dai, Andrew  and
      Jozefowicz, Rafal  and
      Bengio, Samy",
    booktitle = "Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/K16-1002",
    pages = "10--21",
}

@Misc{Michlo2021Disent,
  author =       {Nathan Juraj Michlo},
  title =        {Disent - A modular disentangled representation learning framework for pytorch},
  howpublished = {Github},
  year =         {2021},
}

@inproceedings{posterior_collapse_general,
  title={Understanding Posterior Collapse in Generative Latent Variable Models},
  author={James Lucas and G. Tucker and Roger B. Grosse and Mohammad Norouzi},
  booktitle={DGS@ICLR},
  year={2019}
}


@InProceedings{pmlr-v97-mathieu19a,
  title = 	 {Disentangling Disentanglement in Variational Autoencoders},
  author =       {Mathieu, Emile and Rainforth, Tom and Siddharth, N and Teh, Yee Whye},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4402--4412},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/mathieu19a/mathieu19a.pdf},
  abstract = 	 {We develop a generalisation of disentanglement in variational autoencoders (VAEs)—decomposition of the latent representation—characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the $\beta$-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.}
}


@InProceedings{Yang_2020_CVPR,
author = {Yang, Yanchao and Chen, Yutong and Soatto, Stefano},
title = {Learning to Manipulate Individual Objects in an Image},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@misc{symmetries,
  doi = {10.48550/ARXIV.2203.09250},
  
  author = {Higgins, Irina and Racanière, Sébastien and Rezende, Danilo},
  
  keywords = {Neurons and Cognition (q-bio.NC), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Biological sciences, FOS: Biological sciences, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Symmetry-Based Representations for Artificial and Biological General Intelligence},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{NEURIPS2020_d3f06eef,
 author = {Hoogeboom, Emiel and Garcia Satorras, Victor and Tomczak, Jakub and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18249--18260},
 publisher = {Curran Associates, Inc.},
 title = {The Convolution Exponential and Generalized Sylvester Flows},
 volume = {33},
 year = {2020}
}

@inproceedings{smallnorb,
author = {LeCun, Yann and Huang, Fu Jie and Bottou, L\'{e}on},
title = {Learning Methods for Generic Object Recognition with Invariance to Pose and Lighting},
year = {2004},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest Neighbor methods, Support Vector Machines, and Convolutional Networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for Convolutional Nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while Convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.},
booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {97–104},
numpages = {8},
location = {Washington, D.C., USA},
series = {CVPR'04}
}


@InProceedings{control-vae,
  title = 	 {{C}ontrol{VAE}: Controllable Variational Autoencoder},
  author =       {Shao, Huajie and Yao, Shuochao and Sun, Dachun and Zhang, Aston and Liu, Shengzhong and Liu, Dongxin and Wang, Jun and Abdelzaher, Tarek},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8655--8664},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/shao20b/shao20b.pdf},
  abstract = 	 {Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models may suffer from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve much better reconstruction quality than the competitive methods for the comparable disentanglement performance. For language modeling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality for image generation compared to the original VAE.}
}

@misc{sequential-autoencoder,
  doi = {10.48550/ARXIV.1803.02991},
  
  author = {Li, Yingzhen and Mandt, Stephan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Disentangled Sequential Autoencoder},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
contrastively-vae,
title={Contrastively Disentangled Sequential  Variational Autoencoder},
author={Junwen Bai and Weiran Wang and Carla P Gomes},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}

@inproceedings{
partial_equiv,
title={Learning Partial Equivariances From Data},
author={David W. Romero and Suhas Lohit},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
}

@article{gaussian-mixture,
  author    = {Nat Dilokthanakul and
               Pedro A. M. Mediano and
               Marta Garnelo and
               Matthew C. H. Lee and
               Hugh Salimbeni and
               Kai Arulkumaran and
               Murray Shanahan},
  title     = {Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders},
  journal   = {CoRR},
  volume    = {abs/1611.02648},
  year      = {2016},
  eprinttype = {arXiv},
  eprint    = {1611.02648},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DilokthanakulMG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{exp,
      title={Deep Exponential Families}, 
      author={Rajesh Ranganath and Linpeng Tang and Laurent Charlin and David M. Blei},
      year={2014},
      eprint={1411.2581},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


