\section{Related Work}
%\begin{itemize}
%    \color{blue}
%    \item \sout{introduce GNN/GCN (disentanglement: graph based)}
%    \item \sout{Invertible and equivariant GNN}
%    \item \sout{Disentanglement learning with node representation by independent nodes}
%    \item \sout{node representation (trough GNN) effect to latent vector (sampling)}
%    \item \sout{multi non-Gaussian distribution (if equations is proved)}

%1\subsection{Unsupervised Disentanglement Learning}
Recently, various works have focused on the unsupervised disentanglement learning. 
Previous works are based on____ definition.
%for disentanglement learning 
One of the branches %to improve unsupervised disentanglement learning 
is InfoGAN____ based works such as IB-GAN____ implement extra regularizer to improve informativeness____.
%and OOGAN____ showed the improvements, but these works regularize informativeness introduced in____ to elaborate regularizing mutual information. 
%One of diverse methods have been proposed with InfoGAN____ which maximize mutual information (MI) between additional latent code and a generated sample for disentangled representation.
%To elaborate regularizing MI,
%balance of the trade-off between the maximization of lower bound MI, 
%IB-GAN____ and OOGAN____ proposed minimizing upper bound MI, and sampling one-hot embedding code. 
%and minimization of upper bound MI, IB-GAN____ utilizes information bottleneck.
%OOGAN____ proposed sampling one-hot embedding code c to affect one factor of latent z and restricts leverage of latent z by compete-free design even though code c dimension is small.
%However, these works are not regularize uncorrelatedness, more focus on %add regularizer to promote 
%informativeness introduced in____.
%group theory를 적용하지 않은 연구들
The other branch is based on the VAE.
%Variational AutoEncoder (VAE).
$\beta$-VAE____ penalizes Kullback-Leibler divergence (KL divergence) with weighted hyper-parameters.
Factor VAE____ and $\beta$-TCVAE____ are trained with total correlation (TC) to make independent dimensions on a latent vector with discriminator and divided components of KL divergence term.
%Factor VAE utilizes a discriminator to estimate TC, and $\beta$-TCVAE modifies KL divergence term then, estimates mutual information, dimension-wise KL divergence, and TC. However, ____ proves that unsupervised disentanglement learning methods are impossible without inductive bias on both models and data.
Differently, we consider the recent disentanglement definition based on group theory____.
%However, these approaches are not based on the group theory definition in____.

%After ____ defines disentangled representation by group theory, 
%group theory 기반의 연구들
Following the definitions of disentangled representation learning by group theory, several works have emphasized equivariant and improved disentangled representation learning.
Commutative Lie Group VAE (CLG-VAE)____ proposed direct mapping of the latent vector into Lie algebra to obtain group structure (inductive bias) with constraints: commutative and hessian loss.
%However it needs to fine-tune hyper-parameters with grid search to achieve best disentanglement performance.
Furthermore, Groupified VAE____ utilizes Spatial Broadcast Decoder____ to implement an equivariant function to the cyclic group with guaranteeing commutativity and invertibility of group actions.
%with the group theory, and it also proves the necessity of the proposed abel loss and order loss with cyclic groups and n-th root unity group to improve disentangled representation.
%It is an applicale method to various VAEs as our method, but it is omitted from comparison because they can not be directly compared by the difference of using n-th root unity group on decoder.
Topographic VAE____ combines Student's-t distributions and variational inference. 
It enforces rotated latent vectors to be equivariant.
On the other hand, we apply unrestricted prior and posterior for disentanglement learning.
%However, these models still utilize Gaussian distribution prior and posterior.

There are several inductive biases to learning unsupervised disentanglement, such as group theory based and sequential order.
In this section, we briefly discuss sequential order inductive bias even though its method is considered in different domains such as text and video frames.
To individualize the static (time-invariant) and dynamic (time-variant), ____ proposed the latent variables one ($f$) is only dependent on the given times series datasets $x_{1:T}$, and the other ($\mathbf{z}_{1:T})$ is dependent on the $x_{1:T}$ and $f$.
Moreover____ propose the novel ELBO with maximizing mutual information between the input and the latent vectors.
These works empirically show that sequential order which includes separated latent vectors improves unsupervised disentanglement learning with diverse qualitative analysis.
Differently in group theory based approaches, the proposed methods consider equivariant function between input and latent vector space.

%Furthermore, it utilized ensemble encoder and decoder architecture, and showed no restricted to classification.
%Differently, we consider disentangled representation learning.
%Other works transform or capsulize latent vector z to map other distributions (non-Gaussian) or shift temporal coherence. 

Other VAE approaches implement other prior from Gaussian distribution to transformed Gaussian distribution, Gaussian mixture distribution____ or von Mises-Fisher distribution____.
%Other approaches in VAEs, some works have implemented extension prior such as a transformed Gaussian distribution, Gaussian mixture distribution____ or von Mises-Fisher distribution____.
InteL-VAE____ shows that transformed Gaussian distribution by the invertible function trains hierarchical representation with manual function.
We show more clear relation of invertibility to disentanglement and improve VAEs to use its unrestricted form of prior.
%However, these works rarely or implicitly mention disentanglement learning.
%In contrast, we consider disentanglement learning with unrestricted Gaussian prior. %distribution.
%These works show the benefit of unrestricted Gaussian prior distribution but rarely mention disentanglement learning.
%Nonetheless, these have been still restricted in Gaussian and one single distribution.

%preserves complicated dataset structure, but it utilizes only manual function.
%proves that KL divergence of transformed latent posterior by invertible function is equal to KL divergence of Gaussian distribution 
%however it utilizes manual invertible function and multi-layer perceptron, which have no guarantee of being invertible function.

% Invertibel and equivariant function 연구 but disentanglement X.
Invertible and equivariant Deep Neural Networks have been investigated with normalizing flows.
As proven by____, utilized matrix exponential on Neural networks is invertible, but it only provides mathematical foundations of the tra
nsformation.
Matrix exponential is utilized to implement an invertible and equivariant function to improve the generative flow compare to linear function____.
%Matrix exponential flows are proposed in____ for equivariant simultaneously.
%non-gaussian을 사용하였지만, disentanglement 언급 X
To specify the exponential familyt, other works contribute uncertainty of exponential family distribution with Bayesian update____. In addition,____ hierarchically controls the natural parameter across the layers and determines the exponential family distribution with the moment of sufficient statistic.
In our work, we show how to use it for disentanglement learning.


\begin{figure}[t]
%\vskip 0.2in
    \centering
    \centerline{\includegraphics[width=1.0\textwidth]{figure_overview_ver28.jpg}}
    \caption{The overall architecture of our proposed \textit{MIPET}-VAE.
    The invertible and partial-equivariant function $\psi(\cdot)$ for L2L transformation consists of a symmetric matrix exponential to be 1) invertible and 2) partial-equivariant.
    %$\psi(\cdot)$ transform latent vector $ \vz \sim \mathcal{N}(\mu, \Sigma)$ and transformed vector $\hat{ \vz} \sim P_T$.
    Then 3) EF conversion module converges the distribution of unrestricted $\hat{ \vz}$ to be EF with $\mathcal{L}_{el}$ loss.
    Also, it applies KL divergence loss ($\mathcal{L}_{kl}$) between the transformed posterior and prior, which are expressed by the power density function of EF.
    In the last, EF conversion reduces the computational error ($\mathcal{L}_{cali}$) between approximated and true KL divergence.
    4) The reddish color represents the integration parts. The blue figures represent each property.
     The details of the gray box are in Figure~\ref{sub fig:suboverview}.}
    \label{fig:overview}
\end{figure}
% \vskip -0.2in





%\begin{equation}
%    p(\boldsymbol{\theta} | \mathbf{X}, \mathcal{X}, \boldsymbol{\nu}) \propto \text{exp} (\boldsymbol{\theta}^\intercal (\sum_{n=1}^N \mathbf{T}(\mathbf{x}_n) + \boldsymbol{\nu} \mathcal{X}) - \mathbf{A}(\boldsymbol{\theta})).
%    \label{eq:posteior}
%\end{equation}

%\clearpage