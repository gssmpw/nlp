@inproceedings{NEURIPS2020_d3f06eef,
 author = {Hoogeboom, Emiel and Garcia Satorras, Victor and Tomczak, Jakub and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {18249--18260},
 publisher = {Curran Associates, Inc.},
 title = {The Convolution Exponential and Generalized Sylvester Flows},
 volume = {33},
 year = {2020}
}

@inproceedings{beta-tcvae,
 author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Isolating Sources of Disentanglement in Variational Autoencoders},
 volume = {31},
 year = {2018}
}

@inproceedings{betaVAE,
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  author={Irina Higgins and Lo{\"i}c Matthey and Arka Pal and Christopher P. Burgess and Xavier Glorot and Matthew M. Botvinick and Shakir Mohamed and Alexander Lerchner},
  booktitle={ICLR},
  year={2017}
}

@article{broadcast-decoder,
  author    = {Nicholas Watters and
               Lo{\"{\i}}c Matthey and
               Christopher P. Burgess and
               Alexander Lerchner},
  title     = {Spatial Broadcast Decoder: {A} Simple Architecture for Learning Disentangled
               Representations in VAEs},
  journal   = {CoRR},
  volume    = {abs/1901.07017},
  year      = {2019},
  eprinttype = {arXiv},
  eprint    = {1901.07017},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-07017.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{commutative-vae,
  title = 	 {Commutative Lie Group VAE for Disentanglement Learning},
  author =       {Zhu, Xinqi and Xu, Chang and Tao, Dacheng},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12924--12934},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhu21f/zhu21f.pdf},
  abstract = 	 {We view disentanglement learning as discovering an underlying structure that equivariantly reflects the factorized variations shown in data. Traditionally, such a structure is fixed to be a vector space with data variations represented by translations along individual latent dimensions. We argue this simple structure is suboptimal since it requires the model to learn to discard the properties (e.g. different scales of changes, different levels of abstractness) of data variations, which is an extra work than equivariance learning. Instead, we propose to encode the data variations with groups, a structure not only can equivariantly represent variations, but can also be adaptively optimized to preserve the properties of data variations. Considering it is hard to conduct training on group structures, we focus on Lie groups and adopt a parameterization using Lie algebra. Based on the parameterization, some disentanglement learning constraints are naturally derived. A simple model named Commutative Lie Group VAE is introduced to realize the group-based disentanglement learning. Experiments show that our model can effectively learn disentangled representations without supervision, and can achieve state-of-the-art performance without extra constraints.}
}

@ARTICLE{disen_definitiaon_1,
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Representation Learning: A Review and New Perspectives}, 
  year={2013},
  volume={35},
  number={8},
  pages={1798-1828},
  doi={10.1109/TPAMI.2013.50}}

@article{disen_definitiaon_2,
  author    = {Irina Higgins and
               David Amos and
               David Pfau and
               S{\'{e}}bastien Racani{\`{e}}re and
               Lo{\"{\i}}c Matthey and
               Danilo J. Rezende and
               Alexander Lerchner},
  title     = {Towards a Definition of Disentangled Representations},
  journal   = {CoRR},
  volume    = {abs/1812.02230},
  year      = {2018},
  eprinttype = {arXiv},
  eprint    = {1812.02230},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-02230.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{exp,
      title={Deep Exponential Families}, 
      author={Rajesh Ranganath and Linpeng Tang and Laurent Charlin and David M. Blei},
      year={2014},
      eprint={1411.2581},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{exponential_uncertainty_00,
  author={Bertrand Charpentier and Daniel Zügner and Stephan Günnemann},
  title={Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts},
  year={2020},
  cdate={1577836800000},
  booktitle={NeurIPS},
}

@inproceedings{exponential_uncertainty_01,
    title={{Natural} {Posterior} {Network}: {Deep} {Bayesian} {Predictive} {Uncertainty} for {Exponential} {Family} {Distributions}},
    author={Charpentier, Bertrand and Borchert, Oliver and Z\"{u}gner, Daniel and Geisler, Simon and G\"{u}nnemann, Stephan},
    booktitle={International Conference on Learning Representations},
    year={2022}
}

@InProceedings{factor-vae,
  title = 	 {Disentangling by Factorising},
  author =       {Kim, Hyunjik and Mnih, Andriy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2649--2658},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18b/kim18b.pdf},
  abstract = 	 {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.}
}

@article{gaussian-mixture,
  author    = {Nat Dilokthanakul and
               Pedro A. M. Mediano and
               Marta Garnelo and
               Matthew C. H. Lee and
               Hugh Salimbeni and
               Kai Arulkumaran and
               Murray Shanahan},
  title     = {Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders},
  journal   = {CoRR},
  volume    = {abs/1611.02648},
  year      = {2016},
  eprinttype = {arXiv},
  eprint    = {1611.02648},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/DilokthanakulMG16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hyper-vae,
  added-at = {2019-07-17T01:59:08.000+0200},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  biburl = {https://www.bibsonomy.org/bibtex/2e2d236a0bacacb8ad7da018a8207221a/becker},
  interhash = {e0a48bada3d58f88f8654937c1e07b9e},
  intrahash = {e2d236a0bacacb8ad7da018a8207221a},
  journal = {34th Conference on Uncertainty in Artificial Intelligence (UAI-18)},
  keywords = {afcs auto autoencoder bayes bayesian encoder generative model related vae variational work},
  timestamp = {2019-07-17T01:59:08.000+0200},
  title = {Hyperspherical Variational Auto-Encoders},
  year = 2018
}

@inproceedings{ib-gan, title={IB-GAN: Disengangled Representation Learning with Information Bottleneck Generative Adversarial Networks}, author={Jeon, Insu and Lee, Wonkwang and Pyeon, Myeongjang and Kim, Gunhee}, booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, volume={35}, number={9}, pages={7926--7934}, year={2021} }

@inproceedings{info-gan,
 author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
 volume = {29},
 year = {2016}
}

@inproceedings{info-gan-cr,
  author={Zinan Lin and Kiran Koshy Thekumparampil and Giulia C. Fanti and Sewoong Oh},
  title={InfoGAN-CR and ModelCentrality: Self-supervised Model Training and Selection for Disentangling GANs},
  year={2020},
  cdate={1577836800000},
  pages={6127-6139},
  booktitle={ICML},
  crossref={conf/icml/2020}
}

@InProceedings{invertible-matrix-exponential,
  title = 	 {Generative Flows with Matrix Exponential},
  author =       {Xiao, Changyi and Liu, Ligang},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10452--10461},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/xiao20a/xiao20a.pdf},
  abstract = 	 {Generative flows models enjoy the properties of tractable exact likelihood and efficient sampling, which are composed of a sequence of invertible functions. In this paper, we incorporate matrix exponential into generative flows. Matrix exponential is a map from matrices to invertible matrices, this property is suitable for generative flows. Based on matrix exponential, we propose matrix exponential coupling layers that are a general case of affine coupling layers and matrix exponential invertible 1 x 1 convolutions that do not collapse during training. And we modify the networks architecture to make training stable and significantly speed up the training process. Our experiments show that our model achieves great performance on density estimation amongst generative flows models.}
}

@article{oogan, title={OOGAN: Disentangling GAN with One-Hot Sampling and Orthogonal Regularization}, volume={34}, DOI={10.1609/aaai.v34i04.5919}, abstractNote={&lt;p&gt;Exploring the potential of GANs for unsupervised disentanglement learning, this paper proposes a novel GAN-based disentanglement framework with One-Hot Sampling and Orthogonal Regularization (OOGAN). While previous works mostly attempt to tackle disentanglement learning through VAE and seek to implicitly minimize the Total Correlation (TC) objective with various sorts of approximation methods, we show that GANs have a natural advantage in disentangling with an alternating latent variable (noise) sampling method that is straightforward and robust. Furthermore, we provide a brand-new perspective on designing the structure of the generator and discriminator, demonstrating that a minor structural change and an orthogonal regularization on model weights entails an improved disentanglement. Instead of experimenting on simple toy datasets, we conduct experiments on higher-resolution images and show that OOGAN greatly pushes the boundary of unsupervised disentanglement.&lt;/p&gt;}, number={04}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Bingchen and Zhu, Yizhe and Fu, Zuohui and de Melo, Gerard and Elgammal, Ahmed}, year={2020}, month={Apr.}, pages={4836-4843} }

@misc{sequential-autoencoder,
  doi = {10.48550/ARXIV.1803.02991},
  
  author = {Li, Yingzhen and Mandt, Stephan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Disentangled Sequential Autoencoder},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{t-vae,
  author    = {T. Anderson Keller and
               Max Welling},
  title     = {Topographic VAEs learn Equivariant Capsules},
  journal   = {CoRR},
  volume    = {abs/2109.01394},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2109.01394},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-01394.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{vae-inductive-bias,
  title = 	 {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author =       {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4114--4124},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf},
  abstract = 	 {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than $12000$ models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.}
}

