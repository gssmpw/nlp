\section{Related Work}
%\begin{itemize}
%    \color{blue}
%    \item \sout{introduce GNN/GCN (disentanglement: graph based)}
%    \item \sout{Invertible and equivariant GNN}
%    \item \sout{Disentanglement learning with node representation by independent nodes}
%    \item \sout{node representation (trough GNN) effect to latent vector (sampling)}
%    \item \sout{multi non-Gaussian distribution (if equations is proved)}

%1\subsection{Unsupervised Disentanglement Learning}
Recently, various works have focused on the unsupervised disentanglement learning. 
Previous works are based on Higgins et al., "Disentangling Disentanglement in Variational Autoencoders" definition.
%for disentanglement learning 
One of the branches %to improve unsupervised disentanglement learning 
is InfoGAN____ based works such as IB-GAN____ implement extra regularizer to improve informativeness____.
%and OOGAN____ showed the improvements, but these works regularize informativeness introduced in Chen et al., "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets" to elaborate regularizing mutual information. 
%One of diverse methods have been proposed with InfoGAN____ which maximize mutual information (MI) between additional latent code and a generated sample for disentangled representation.
%To elaborate regularizing MI,
%balance of the trade-off between the maximization of lower bound MI, 
%IB-GAN____ and OOGAN____ proposed minimizing upper bound MI, and sampling one-hot embedding code. 
%and minimization of upper bound MI, IB-GAN____ utilizes information bottleneck.
%OOGAN____ proposed sampling one-hot embedding code c to affect one factor of latent z and restricts leverage of latent z by compete-free design even though code c dimension is small.
%However, these works are not regularize uncorrelatedness, more focus on %add regularizer to promote 
%informativeness introduced in Chen et al., "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets".
%group theory를 적용하지 않은 연구들
The other branch is based on the VAE.
%Variational AutoEncoder (VAE).
$\beta$-VAE____ penalizes Kullback-Leibler divergence (KL divergence) with weighted hyper-parameters.
Factor VAE____ and $\beta$-TCVAE____ are trained with total correlation (TC) to make independent dimensions on a latent vector with discriminator and divided components of KL divergence term.
%Factor VAE utilizes a discriminator to estimate TC, and $\beta$-TCVAE modifies KL divergence term then, estimates mutual information, dimension-wise KL divergence, and TC. However, Zhang et al., "Understanding the limitations of Variational Autoencoder" proves that unsupervised disentanglement learning methods are impossible without inductive bias on both models and data.
Differently, we consider the recent disentanglement definition based on group theory____.
%However, these approaches are not based on the group theory definition in Berne et al., "The Effect of Group Theory in Disentanglement Learning".
Following the definitions of disentangled representation learning by group theory, several works have emphasized equivariant and improved disentangled representation learning.
Commutative Lie Group VAE (CLG-VAE)____ proposed direct mapping of the latent vector into Lie algebra to obtain group structure (inductive bias) with constraints: commutative and hessian loss.
%However it needs to fine-tune hyper-parameters with grid search to achieve best disentanglement performance.
Furthermore, Groupified VAE____ utilizes Spatial Broadcast Decoder____ to implement an equivariant function to the cyclic group with guaranteeing commutativity and invertibility of group actions.
%with the group theory, and it also proves the necessity of the proposed abel loss and order loss with cyclic groups and n-th root unity group to improve disentangled representation.
%It is an applicale method to various VAEs as our method, but it is omitted from comparison because they can not be directly compared by the difference of using n-th root unity group on decoder.
Topographic VAE____ combines Student's-t distributions and variational inference. 
It enforces rotated latent vectors to be equivariant.
On the other hand, we apply unrestricted prior and posterior for disentanglement learning.
%However, these models still utilize Gaussian distribution prior and posterior.

There are several inductive biases to learning unsupervised disentanglement, such as group theory based and sequential order.
In this section, we briefly discuss sequential order inductive bias even though its method is considered in different domains such as text and video frames.
To individualize the static (time-invariant) and dynamic (time-variant),  Chen et al., "Unsupervised Disentanglement for Time Series Data" proposed the latent variables one ($f$) is only dependent on the given times series datasets $x_{1:T}$, and the other ($\mathbf{z}_{1:T})$ is dependent on the $x_{1:T}$ and $f$.
Moreover  Chen et al., "Temporal Disentanglement for Unsupervised Learning" propose the novel ELBO with maximizing mutual information between the input and the latent vectors.
These works empirically show that sequential order which includes separated latent vectors improves unsupervised disentanglement learning with diverse qualitative analysis.
Differently in group theory based approaches, the proposed methods consider equivariant function between input and latent vector space.

%Furthermore, it utilized ensemble encoder and decoder architecture, and showed no restricted to classification.
%Differently, we consider disentangled representation learning.
%Other works transform or capsulize latent vector z to map other distributions (non-Gaussian) or shift temporal coherence. 

Other VAE approaches implement other prior from Gaussian distribution to transformed Gaussian distribution, Gaussian mixture distribution____ or von Mises-Fisher distribution____.
%Other approaches in VAEs, some works have implemented extension prior such as a transformed Gaussian distribution, Gaussian mixture distribution____ or von Mises-Fisher distribution____.
InteL-VAE____ shows that transformed Gaussian distribution by the invertible function trains hierarchical representation with manual function.
We show more clear relation of invertibility to disentanglement and improve VAEs to use its unrestricted form of prior.
%However, these works rarely or implicitly mention disentanglement learning.
%In contrast, we consider disentanglement learning with unrestricted Gaussian prior. %distribution.
%These works show the benefit of unrestricted Gaussian prior distribution but rarely mention disentanglement learning.
%Nonetheless, these have been still restricted in Gaussian and one single distribution.

%preserves complicated dataset structure, but it utilizes only manual function.
%proves that KL divergence of transformed latent posterior by invertible function is equal to KL divergence of Gaussian distribution 
%however it utilizes manual invertible function and multi-layer perceptron, which have no guarantee of being invertible function.

% Invertibel and equivariant function 연구 but disentanglement X.
Invertible and equivariant Deep Neural Networks have been investigated with normalizing flows.
As proven by Rezende et al., "Normalizing Flows for Probabilistic Modeling" utilized matrix exponential on Neural networks is invertible, but it only provides mathematical foundations of the transformation.
Matrix exponential is utilized to implement an invertible and equivariant function to improve the generative flow compare to linear function____.
%Matrix exponential flows are proposed in Chen et al., "A Simple Scheme for Graph-Structured Invertible Networks" for equivariant simultaneously.
%non-gaussian을 사용하였지만, disentanglement 언급 X
To specify the exponential familyt, other works contribute uncertainty of exponential family distribution with Bayesian update____. In addition, Gutmann and Hyvärinen, "Noise-contrastive Learning of Similarity Functions" hierarchically controls the natural parameter across the layers and determines the exponential family distribution with the moment of sufficient statistic.
In our work, we show how to use it for disentanglement learning.


\begin{figure}[t]
%\vskip 0.2in
    \centering
    \centerline{\includegraphics[width=1.0\textwidth]{figure_overview_ver28.jpg}}
    \caption{The overall architecture of our proposed \textit{MIPET}-VAE.
    The invertible and partial-equivariant function $\psi(\cdot)$ for L2L transformation consists of a symmetric matrix exponential to be 1) invertible and 2) partial-equivariant.
    %$\psi(\cdot)$ transform latent vector $ \vz \sim \mathcal{N}(\mu, \sigma^2)$ into another space where the distribution is factorized.
    %The resulting factors are then used for further processing.
    %For example, they can be used as inputs to a classifier or as features for clustering.
    %In this way, the proposed method enables unsupervised disentanglement of complex data distributions.
    %The proposed method can be applied to various problems where disentanglement is beneficial, such as image and video analysis, robotics, and recommender systems.
    %We believe that our work will have a significant impact on these areas.
    %Finally, we provide an outlook on future research directions and potential applications of the proposed method.
    %The performance of the proposed method is evaluated using several benchmark datasets and compared with state-of-the-art methods.
    %The results show that the proposed method outperforms the baseline methods in terms of disentanglement quality and representation learning.
    %We also provide a detailed analysis of the experimental results to understand the strengths and weaknesses of the proposed method.
    %We believe that our work will be useful for researchers and practitioners working on unsupervised disentanglement problems.
    \label{fig:overview}
\end{figure}