\section{Results and Discussion}
\label{sec:res}


\subsection{Performance}
Tab. \ref{tab:tab_results} shows the performance of different architectures $\mathcal{A}(\cdot)$ (that are LeNet-5, MobileNet, and EfficientNet-B2) on CIFAR-10, CIFAR-100, and STL-10 test sets with (IMPACTX column) and without (baseline column) IMPACTX, in terms of accuracy. In the same table, results obtained by ABN and WAE are reported. 

\input{./_TAB_RES}

One can note that IMPACTX produced an uniform improvements with respect the baseline on all the selected datasets independently from the classification architecture used. At same way, IMPACTX obtained better results with respect the other selected approaches. 


%an improvement in accuracy from $67.96 \%$ (baseline) to $70.65 \%$ using LENET-5 as architecture. Similarly, with MobileNet the accuracy increased from $94.63 \%$ to $96.27 \%$. When EfficientNet-B2 is used as classification's architecture, our approach maintains a similar but slightly higher accuracy ($98.38 \%$) compared to the baselines ($98.06 \%$). Similar results are 

%With LeNet-5 as baseline model $M$, the accuracy increased from $36.23\%$ (baseline) to $39.49 \%$ using the IMPACTX approach. Using MobileNet as baseline model led to a more significant improvement, with accuracy climbing from $76.72 \%$ to $79.37 \%$. Instead, the improvement observed with EfficientNet-B2 was more moderate, increasing from an accuracy of $87.88 \%$ to $89.06 \%$.

%\subsubsection{Results on STL-10}
%Using LeNet-5 as baseline model $M$, the proposal's accuracy on STL-10 increased from $52.59 \%$ to $55.86 \%$, showing a significant enhancement in accuracy compared to the baseline. Similarly, with MobileNet exhibited an improvement from $91.07 \%$ to $95.79 \%$. Conversely, when considering EfficientNet-B2 as baseline model $M$, the proposal maintained a higher accuracy ($99.00 \%$), albeit comparable with the baseline ($98.76 \%$).


\input{./FIG_XAI}


\subsection{Evaluating attribution maps}
In this section we want to evaluate if the attribution maps directly obtained by IMPACTX can be considered as explanations of the IMPACTX classification responses. To this aim, we compare them with the explanations given by $R$ (that is, in this experimental setup, SHAP) outputs when applied on IMPACTX itself.  
In figures \ref{images_CIFAR10}, \ref{images_CIFAR100} and \ref{images_STL10}
(left side), examples from the CIFAR-10, CIFAR-100, and STL-10 test sets are presented (column 1). The examples are reported considering the experiments made on \textit{LeNet-5}. For each example, the class and score given by IMPACTX are provided. Furthermore, in the same figures are reported the attribution maps obtained by the explanation method SHAP (column 2), along with the attribution maps  obtained by IMPACTX (column 3). For each input, MoRF curves for SHAP and IMPACTX attribution maps are also reported (column 4). The same comparison is  reported for ABN architecture (right side). Interestingly, while both IMPACTX and ABN qualitatively emphasize input features that intuitively seem more relevant for their respective classes, the MoRF curves reveal different behaviors quantitatively. Specifically, the MoRF curves indicate that the most relevant elements identified by ABN have a lesser impact on classification compared to those selected by SHAP. Conversely, an opposite pattern is observed between IMPACTX and SHAP. The most relevant features highlighted by IMPACTX prove to be more influential for classification than those identified by SHAP. %It is possible to note that, in the majority of cases, the IMPACTX attribution maps focus on input regions intuitively more relevant from the human point of view to the true output compared to the explanation corresponding to the $Q\big(M(\cdot)\big)$ response. %Thus, highlighting the effectiveness of \textit{IMPACTX} approach in generating knowledge useful for the correct prediction.
%Interestingly, in \cref{fig:images_CIFAR100}, the effectiveness of IMPACTX in accurately classifying a tiger (row 3) is clearly visible, in contrast to the baseline which had mistakenly identified the animal as a leopard. Similarly, IMPACTX allows to correctly classify a leopard. The attribution maps provided by IMPACTX seem to highlight distinctive characteristic elements of the two animals. This includes the bright orange fur with vertical black stripes typical of tigers, and the golden-yellow coat with black spots, known as rosettes, distinctive of leopards.
%In \cref{fig:images_STL10}, we observe some instances where the baseline model incorrectly identifies an airplane as a bird (row 1), whereas IMPACTX classifies correctly these images as airplanes, also highlighting the distinctive elements of an airplanes.
%Again, in \cref{fig:images_CIFAR10}, IMPACTX provides a correct class respect to the baseline classification. Moreover attribution maps seem particularly significant in the classes of cats, trucks, and airplanes. To assess if the provided attribution maps can be considered valid explanations of the outputs, MoRF curves are included for each example (column 4). We compare the MoRF curves by IMPACTX at inference time with the SHAP attribution maps applied to the classification branch of IMPACTX. In general, MoRF curves highlight that IMPACTX attribution maps assign higher scores to features more relevant to the provided outputs. Therefore, IMPACTX attribution maps can be considered more accurate explanations of the outputs compared to the attribution maps returned by SHAP. Conversely, from the MoRF curves it appears that SHAP attribution maps can be considered more reliable explanations of ABN than ABN itself (fig. \Cref{fig:images_CIFAR10,fig:images_CIFAR100,fig:images_STL10}, right side).

\subsection{Discussion}
\label{sec:discussion}
%The experimental results show that IMPACTX delivers higher performance in the experimented classification tasks compared to ML models not adopting IMPACTX framework, while also providing attribution maps that can be considered more reliable explanations than those offered by post-hoc XAI methods.
The experimental results show that IMPACTX performs better on the classification tasks than ML models that do not use the IMPACTX framework, while providing attribution maps that can be considered more reliable explanations than those provided by post-hoc XAI methods.
In particular, on the performance side, the interaction between the IMPACTX branches enhances the model's ability to focus on relevant features, thereby improving classification performance. During training, IMPACTX emphasizes features identified as relevant to the model's decisions, leading to more accurate outcomes. %Indeed, the training process of IMPACTX is aimed to focus on features identified as important to the model's decisions.
In fact, IMPACTX's training process is designed to focus on the features identified as important to the model's decisions.
Furthermore, regarding the construction of the attribution maps by the bottom branch, the ability of IMPACTX in providing accurate attribution maps can be attributed to its integrated approach of decision and justification at inference time, which contrasts with the external nature of post-hoc methods such as SHAP. This integration allows IMPACTX to generate attribution maps that more accurately reflect the model's decision-making process. Interestingly, attribution maps generated by IMPACTX seems to be better explanations compared to ABN. This is likely because IMPACTX's attentional mechanism is designed to consider the true class label during training, whereas ABN's CAM-based approach relies on class scores obtained during training.

%Finally, the fact that the attribution maps qualitatively seem to match user expectations, together with their reliability in explaining model's decisions confirmed by MoRF curves, indicates that the ML system tends to focus on aspects similar to human attention to make its predictions. 

%This can be attributed to the fact that the most informative features of the input data identified by both humans and IMPACTX are the same and that are effectively discriminative for the task at hand.
