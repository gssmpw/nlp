\section{Experimental setup}
\label{sec:exp}
%In this section, we present a comprehensive experimental evaluation of the proposed IMPACTX framework. This series of experiments has a dual goal: to evaluate IMPACT's ability to improve model performance, and to provide meaningful explanations in the form of attribution maps. 

In this section, we present a series of experiments aimed at evaluating the IMPACTX framework with respect to two different capabilities:
to improve the performance of ML models and to provide meaningful explanations in the form of attribution maps.
%Our assessment begins with a performance evaluation of IMPACTX on several classification tasks, 
%IMPACTX performance evaluation is obtained  on several standard classification tasks, comparing its results with similar methods in the literature, namely ABN \citep{} and WAE \citep{}, and with IMPACTX itself without the attention mechanism providing by the bottom branch (we considered it as \textit{baseline}). 
The performance of IMPACTX is evaluated on several standard classification tasks, 
comparing its results with similar methods in the literature, namely ABN \citep{fukui2019attention} and WAE \citep{apicella2023shap}, and with IMPACTX itself without the attention mechanism provided by the bottom branch (we considered it as \textit{baseline}).
Following this, we present both qualitative and quantitative analyses to assess the validity of the attribution maps generated by IMPACTX. %This dual approach assesses the IMPACTX's ability to not only enhance model performance but also to provide meaningful explanations in form of attribution maps. 
The attribution maps' qualitative analysis focuses on how the explanations highlight significant parts of the elements to be classified, which we expect to be aligned with human user expectations, providing intuitive insights into the model's decision-making process. Meanwhile, the quantitative evaluation employs MoRF (Most Relevant First) \citep{samek2016evaluating} curves to measure the relevance of the features identified by the attribution maps, therefore if the provided attribution maps can be considered reliable explanations of the framework. 

%\roberto{Questa sezione ora bisogna cambiarla  pensarla,forse, un po' meglio. Cosi' come la parte precedente sul learning. Discutiamone. inoltre bisogna dire che usiamo anche ABN e WAE}
%\sal{meglio come?}

%%\begin{enumerate}%\item \textit{Model without IMPACTX:} training of a model with the same architecture $M$  model on the training dataset $S^T=\{\vec{x}^{(i)}, y^{(i)}\}_{i=1}^{n}$ and computing its accuracy on the test set $S^E=\{\vec{x}^{(j)}, y^{(j)}\}_{j=1}^{m}$, where the labels $y^{(j)}$ are used exclusively for evaluation.
%\item \textit{Calculation of attribution:} we derive the attribution maps $\{\vec{r}^{(i)}\}_{i=1}^{n}$ using the model $M$ with each instance $\textbf{x}^{(i)}$ in the training set $S^T$, with respect to their true class $y^{(i)}$, as outlined in Sec. \ref{sec:method_genR}.
%\item \textit{IMPACTX Training:} learning the parameters of the modules $LEP$, $D$, and $C$ (see Fig. \ref{fig:IMPACTX_arch} and Sec. \ref{sec:trainPhase}) using the training data $S^T$.  In this step we derive the attribution maps $\{\vec{r}^{(i)}\}_{i=1}^{n}$ using the model $M$ with each instance $\textbf{x}^{(i)}$ in the training set $S^T$, with respect to their true class $y^{(i)}$, as outlined in Sec. \ref{sec:method_genR}.
%The pre-trained parameters of $M$ are not changed during this phase.
%\item \textit{Model Evaluation:} using the outputs of the Latent Explanation Predictor $LEP$ and the model $M$ as input of $C$ to predict the labels of the unseen data $S^E$ and compare the performance with that of the baseline on the same data set.
%\end{enumerate}
The following subsections provide detailed information on the datasets used, the attribution generator algorithm $R$, the IMPACTX modules' architectures and the learning procedure.

\subsection{Datasets}
The benchmark datasets used in this study include CIFAR-10, CIFAR-100, and STL-10.
\begin{itemize}
    \item CIFAR-10 \citep{Krizhevsky09} comprises 60,000 color images categorized into ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The dataset is split into 50,000 training images and 10,000 test images, all sized $32 \times 32$ pixels.
    \item CIFAR-100 \citep{Krizhevsky09} contains 60,000 color images grouped into 100 categories, with 50,000 training images and 10,000 test images, all sized $32 \times 32$ pixels.
    \item STL-10 \citep{coates2011analysis} dataset comprises images classified into ten classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, and truck. Each image is $96 \times 96$ pixels, and the dataset includes 5,000 training images and 8,000 test images.
\end{itemize}

\subsection{Explanation Generator Algorithm}
%Since our goal is to obtain attribution maps aligned with the true class label, we adopt the XAI SHAP method \citep{NIPS2017_7062} as $R$. SHAP, a prominent technique in XAI, provides insights into the contribution of each feature to a model's predictions. In particular, SHAP provides explanations not only for the predicted class, but also for each possible class. 
We have adopted the SHAP method \citep{NIPS2017_7062} as $R$. This choice was motivated by the fact that SHAP is a  prominent technique in XAI that provides explanations in terms of attribution maps, and provides explanations not only for the predicted class, but also for each possible class, as required by our approach. 
For this study, in particular, we employed the \textit{Partition Explainer algorithm} as specific version of SHAP, with $2000$ evaluations to obtain the final explanations \citep{NIPS2017_7062}. 
%The explanations have been normalized between 0 and 1 using the minimum and maximum values computed on the training set.


\subsection{IMPACTX modules and the two-stage training}
\label{subsec:AdoptedModelsTraining}
We evaluated IMPACTX using three distinct models for $M$: LeNet-5 \citep{lecun1998gradient}, MobileNet \citep{howard2017mobilenets} pre-trained on ImageNet \citep{deng2009imagenet}, and  EfficientNet-B2 \citep{tan2019efficientnet} pre-trained on the Noisy Student weights \citep{xie2020self}. 
The focus of this study is oriented
to the interpretation of the model and to the explanation
of the influence of the input characteristics on the modelsâ€™
prediction. %\begin{itemize}   \item LeNet-5 \citep{lecun1998gradient}.\item  MobileNet \citep{howard2017mobilenets} pre-trained on  a ImageNet weights \citep{deng2009imagenet}.\item EfficientNet-B2 \citep{tan2019efficientnet} pre-trained on the Noisy Student weights \citep{xie2020self}.\end{itemize}

The Latent Explanation Predictor $LEP$ module is built starting from the architecture of selected $\mathcal{A}$, replacing the last fully-connected layers by a fully-connected layer with a dimensionality of $512$
%\textcolor{red}{, i.e. the coding dimension of the input image,}\textcolor{blue}{Andrea: ???} \sal{la dimensione della codifica, non lo diciamo da nessuna parte in modo esplicito, non va bene?}\textcolor{blue}{Andrea: ok, ma non si capisce codifica di che...} \sal{corretto, vedi se "suona" bene} 
and a sigmoid activation function. The architecture of the Decoder used for CIFAR-10 and CIFAR-100 datasets is depicted in fig. \ref{fig:Decoder}. In case of STL-10, an additional sequence of [Conv, Conv, UpSampling] layers was included in the decoder due to the different input dimensions of STL-10 images compared to CIFAR-10/100. It should be emphasised again that the Latent Explanation Predictor - Decoder ($LEP-D$) extracts insights and relationships concerning both the visual features of the input image and the corresponding attribution scores found during the training phase. The outputs of $M$ and $LEP$, representing the extracted features from $\vec{x}$ and the encoding attribution $\vec{z}$ respectively, are concatenated and then fed into the final classifier $C$. The classifier $C$ is a shallow fully-connected neural network with outputs equal to the number of classes. 

\input{./_TAB_HP} 

As discussed in sec. \ref{sec:training}, we used the two-stage training because it is less expensive than single-stage training. 
Thus, following this learning strategy, in the first stage learning, the hyperparameters batch size, learning rate, and validation fraction values were determined through a grid-search approach, and each $\mathcal{A}$ model is fine-tuned on each selected dataset. Grid-search ranges are detailed in \cref{tab:tab_hp}. After the first stage learning, 
in the second stage learning, the parameters of $LEP$, $D$, and $C$ are learned while maintaining the parameters of $M$ fixed (see fig. \ref{fig:IMPACTX_arch}).
%the best final models are considered as frozen models $M$ on which the second stage training is applied. 
The $D$ architecture is reported in fig. \ref{fig:Decoder}. The optimal value for the $\lambda$ regularization parameter in eq. \ref{eq:loss} was established via hyperparameter grid-search, and the values used for the search are detailed in \cref{tab:tab_hp}.
Final performance on the test set for each dataset is computed. 

%Subsequently, SHAP attribution maps are generated for each input within the training data, taking into account their true class labels.


\begin{figure}[ht]
    \centering
\includegraphics[width=0.8\textwidth]{./IMPACTX_Decoder_32x32_new.png}
    \caption{The Decoder architecture designed for the CIFAR-10 and CIFAR-100 datasets. The architecture is composed of convolutional (Conv2D), fully connected (FC), and UpSampling layers. The kernel size is $3 \times 3$ for all the convolutional layers, while the number of filters is given by the third dimension of the output shape.}
    \label{fig:Decoder}
\end{figure}


\subsection{Evaluation measures}
%In this study, to evaluate IMPACTX we conducted our experiments using the two-stage training approach described in sec. \ref{sec:training}. 
%This choice was primarily driven by computational resource constraints. 
We evaluated IMPACTX for its performance in the classification tasks and for the attribution maps that it generates. Performances on classification tasks were measured in terms of accuracy on the standard test sets provided with each dataset. The proposed method was subjected to a comparative analysis with the baseline $\mathcal{A}$ (which can be seen as IMPACTX itself without the attention mechanism provided by the bottom branch), ABN \citep{fukui2019attention} and WAE \citep{apicella2023shap} approaches.
Instead, the attribution maps are evaluated both qualitatively and quantitatively, reporting respectively examples of the obtained attribution maps and MoRF (Most Relevant First) curves \citep{samek2016evaluating}.  MoRF curves are widely used in XAI research to assess proposed explanations  \citep{kakogeorgiou2021evaluating, apicella2022exploiting, tjoa2022quantifying, islam2022systematic}. Essentially, image regions are iteratively replaced by random noise and fed to the ML model, following the descending order of relevance values indicated by the attribution map. Thus, the more relevant the identified features are for the classification output, the steeper the curve. MoRF curves offer a quantitative measure of how well an attribution map explains the behavior of an ML system. Summarizing, the qualitative aspect is important for understanding how closely the obtained explanation aligns with the user's intuitive expectations, while the quantitative evaluation aims to provide a measure of how valid the attribution maps are as explanations. %Finally, IMPACTX explanations obtained by the $LEP-D$ branch are compared with explanations obtained by the SHAP method applied to the $Q\big(M(\cdot)\big)$.

%In the IMPACTX framework, the hyperparameter grid search was omitted since the $LEP$ predictor and the $M$ baseline share identical architectures. As a result, the optimal hyperparameter values for the $LEP$, $D$, and $C$ modules in IMPACTX correspond to those discovered for the $M$ baseline module through grid search, as delineated in \cref{tab:tab_hp}. Moreover, a $\lambda$ value of 1 was selected to ensure equal weighting between the two losses CE and MSE.
%\andrea{sta parte la ometterei. Inoltre, metterei che lambda Ã¨ stato usato come hyperparametro con valori inventati} \sal{credo sia una cosa positiva dire che non Ã¨ stata fatta anche su impactx una grid search, o a limite si puÃ  scrivere per question di tempi computazioniali non Ã¨ stata fatta. Lambda con valori inventati?}\andrea{aggiungi nei parametri della grid search che hai fatto una ricerca pure su lambda inventandoti i range} \sal{non mi convince e non Ã¨ quello che Ã¨ stato fatto}\andrea{lo so che non Ã¨ quelllo che Ã¨ stato fatto, ma non voglio rischiare che un revisore caghi il cazzo su sto punto che, dati i risultati decenti giÃ  adesso, oggettivamente sarebbe una stronzata (per intenderci, non stiamo imbrogliando presentando come migliori di quello che sono, ma peggiori di quello che potrebbero essere, e non voglio che il revisore per fare lo splendido ce lo bocci o vada in major dicendo "cosa succede con vari valori di labmda?"} \sal{ok}

