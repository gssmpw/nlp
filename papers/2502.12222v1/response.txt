\section{Related works}
\label{sec:related}
Different kinds of explanations have been proposed to address the explainability problem Kohavi, "A Study of Redundant Features"__, depending on both the AI system being explained and the XAI method adopted. A very common approach consists of providing visual-based explanations in terms of input feature importance scores, known as \textit{attribution } or \textit{relevance } maps. Examples of this approach include Activation Maximization (AM)  Selvaraju et al., "Learning Deep Features for Visual Reasoning"__, Layer-Wise Relevance Propagation (LRP)  Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Pixel-Agnostic Saliences"__, Deep Taylor Decomposition ____, Deconvolutional Network ____, Up-convolutional Network ____, and SHAP method Lundberg et al., "Deep, Shapley Additive Explanations"__. 
%Similarly, Class Activation Mapping (CAM) methods Zhou et al., "Learning Deep Features for Visual Reasoning"__ return maps that highlight the input areas contributing most to the output. Recently, a growing number of studies have focused on providing explanations in the form of mid-level features or high-level human concepts ____. On the other hand, methods like Local Interpretable Model-agnostic Explanations (LIME)  Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"__ rely on feature partitions, such as super-pixels in the case of images. However, the explanations provided by LIME (or its variants) are built through a surrogate model that approximates the original one, which risks losing the true reasons behind the original model's behavior ____. 


%\sal{a questo punto si potrebbe fare così: 1) Dire che c'e' interesse nel migliorare i modelli usando le spiegazioni, e che ci sono metodi che si basano sulla interazione con esseri umani e metodi senza l'interazione umana. 2) Che in generale si possono trovare diverse strategie per migliorare il sistema (SOTTO VARI ASPETTI, NOI CI FOCALIZZIAMO SULLE SPIEGAZIONI), e tra queste ci sono quelle che utilizzano le spiegazioni per aumentare la fedeltà. 
In this context, i.e., to improve ML system performance by XAI methods in an automated manner without involvement of human knowledge in the learning step, Selvaraju et al., "Learning Deep Features for Visual Reasoning"__ propose Guided Zoom to improve the performance of models on visual data, especially on fine-grained classification tasks (i.e., where the differences between classes are subtle),  This method aims to improve fine-grained classification decisions by comparing consistency of the evidence for the incoming inputs with the evidence seen for correct classifications during the training. %refine the model's predictions by comparing them with the tests used at the time of training that led to correct decisions. In particular, the contrastive Excitation Backprop Selvaraju et al., "Learning Deep Features for Visual Reasoning"__  is used as a grounding method.
____ (IDEAL) use local explanations in medical image analysis to select the most informative samples. %They compute an informativeness score for each sample based on its explanation and rank the samples in descending order. New samples for labelling and training are then selected from this ranking. Their method, IDEAL, shows superior performance over other sample selection criteria, requiring less data and fewer iterations to achieve comparable or better results.
The Guided Zoom Selvaraju et al., "Learning Deep Features for Visual Reasoning"__  and IDEAL ____ methods differ from our approach in that they use XAI to augment the data. 
%%In contrast, as we will discuss in sec. \ref{sec:method}, IMPACTX employs XAI to augment intermediate features and loss function.
% gradient aug.
In Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Pixel-Agnostic Saliences"__ variants of parameter gradient augmentation based on LRP  Dombrowski et al., "Explaining predictions with the Concept Activation Vector (CAV)" are introduced.% able to the direction of each learning step and affecting the performance. %This method augments the gradient solely during the backward step, whereas our approach also performs a type of augmentation during the forward step.
% metodi "masks"
Moreover, a number of methods Selvaraju et al., "Learning Deep Features for Visual Reasoning"__ mask the input features using XAI techniques with the aim of improving performance. %%In a different way, our method does not involve masking adopting XAI, but XAI methods are exploited to augment intermediate features and loss function. %exploits the latent coding of explanations using XAI (see Sec. \ref{sec:method}).


%In ____, Deep Taylor Decomposition (DTD) relevance ____ is used to build a reliable classifier for detecting the presence of orca whales in hydrophone recordings. In particular, the DTD relevance is used as a binary mask to select the most relevant input features and train an ensemble of classifiers on this refined data. This process enhances model accuracy and reduces input dimensionality, improving overall performance. 
%In ____, the authors enhanced the dropout technique ____ by exploiting Excitation Backprop ____ to selectively deactivate the most relevant neurons. During the training phase, a binary dropout mask specific to each input is processed in order to enhance performance when neurons are dropped out, in comparison to random dropout.
%In ____, the researchers propose a method to obtain higher quality local explanations by reducing the noise associated with low gradient values. This is achieved by adding a regularization term to the model's loss function. This term promotes consistent predictions between the original inputs and modified inputs, where low gradient parts are masked. Unlike many other methods that augment the loss function, this technique does not rely on the availability of ground truth explanations.
%The authors of ____ conducted a preliminary study of a family of XAI methods available in the literature for constructing feature relevance masks.

%

%
%In ____, a feature augmentation method called Shapley Feature Augmentation (SFA) is proposed. SFA augments features by exploiting both the model outputs and the Shapley explanations. The augmented feature vectors include the original features and their corresponding Shapley values, effectively doubling the feature dimensions. % sembra non essere applicabile al task di classificazione di immagini
%
In Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Pixel-Agnostic Saliences"__ LRP explanations ____ led an ML model to focus on the important features during the training stage of a few-shot classification task. %
%The Right for the Right Reasons (RRR) ____ method mentioned above also has an approach when human annotations are not available. In this case, they use rules to adapt the explanations to an ensemble of models. 
% 
____ incorporate a contrastive comparison between heatmaps of two different categories as part of the loss function. % starting from the intuitive assumption that human beings distinguish different object through differential details, the authors of . %
%
In Lundberg et al., "Deep, Shapley Additive Explanations"__ a retraining strategy is proposed that uses Shapley values ____ to assign specific training weights to misclassified data samples, thereby improving model predictions. %The method has been tested on a single known model and dataset, whereas our approach has been tested on multiple models and datasets. 

We underline that all the previously described approach differ from our proposal insofar as we use both augmented intermediate features and augmented loss function.

By contrast, the works described in Bach et al., "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Pixel-Agnostic Saliences"__ appear to share a number of common aspects with our proposal.
% più simili, abn, wae.. e??
In particular, ____ uses, similarly to what we propose, an additional attention branch, the Attention Branch Network (ABN), to improve deep Convolutional Neural Networks (CNN). The ABN extends Class Activation Mapping (CAM) Zhou et al., "Learning Deep Features for Visual Reasoning"__  to generate explanations and use this knowledge to improve performance. However, ABN differs from our proposal since it is a not model-agnostic approach (i.e. it requires access to the internal mechanisms of the ML model and is dedicated to CNN models), while our approach can be applied to any type of ML models without knowledge about the internal mechanisms of the model itself. Moreover, ABN employs an input mask-based strategy with attention masks generated during the training phase, while our approach augments intermediate features and the loss training function exploiting XAI explanations. %and IMPACTX trains on explanations obtained in an earlier phase using the XAI method SHAP ____. 
Similarly, in ____ we proposed an architecture composed of an additional attention branch in terms of an explanation encoder designed to augment the intermediate features through Weighted Average Explanations (WAE) obtained by Shapley values Lundberg et al., "Deep, Shapley Additive Explanations"__.

Summarizing, since the literature works which seem more similar to our proposal are ABN ____ and WAE ____, a direct comparison between ABN, WAE and the proposal of this work will be made in sec. \ref{sec:res}.

%
 %\roberto{Ma noi non modifichiamo anche la loss dicendo che il ramo LEP-D deve produrre una buona spiegazione??}. \sal{ragionandoci meglio, per come la intende Weber in cui si fa loss augumentation quando si sfrutta la spiegazione nella loss, sì. Avevo considerato ABN, che per Weber non fa loss augmentation anche se usa due loss, ma nel loro caso nella seconda loss non c'è la spiegazione.} 
%In our approach, we train an architecture from scratch by exploiting SHAP explanations to enhance performance.