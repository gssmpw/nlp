\section{Introduction}
\label{sec:intro}
The inner workings of modern Machine Learning (ML) and Deep Learning (DL) approaches are often opaque, leaving AI scientists to interpret the reason behind the provided outputs. eXplainable Artificial Intelligence (XAI) aims to provide insights into the internal mechanisms of AI models and/or the motivations behind their decisions, helping users to understand the outcomes produced by ML models. XAI approaches are adopted in several ML tasks applied to various types of inputs, including images \cite{ribeiro2016should,apicella2019explaining,montavon2019layer}, natural language processing, clinical support systems \cite{schoonderwoerd2021human,annuzzi2023impact}, and more.
However, it's worth noting that while a considerable portion of the existing XAI literature focuses on providing explanations for AI systems \cite{lipton2018mythos,miller2019explanation,arrieta2019explainable}, another emerging aim of the XAI studies is to improve the ML systems themselves \cite{weber2022beyond}. Current literature on this topic often presents approaches that require human interactions, such as \cite{teso2019explanatory,schramowski2020making,hagos2022impact,ijcai2017p371,mitsuhara2019embedding,zunino2021explainable,selvaraju2019taking}. %for instance, based on the explanations provided, humans may choose to eliminate specific data from the dataset (e.g., Explanatory Interactive Learning, \cite{schramowski2020making}). 
In these works, the human role can be viewed as a method of manually directing attention, guiding the model towards particular input features.

More recently, however, there has been a growing emphasis on using XAI techniques to autonomously improve machine learning systems as part of the training stage, without requiring a direct human intervention \cite{weber2022beyond,sun2022utilizing,liu2023icel,sun2021explanation}.
The basic assumption is that explanations of the model outputs can improve the training of the ML systems to find better solutions. In particular, the employed XAI methods can be perceived as a type of attention mechanism \cite{vaswani2017attention} to able to lead the model towards improved performance (see for example \cite{fukui2019attention}). 
This kind of approaches typically involved altering the training procedure or the model's loss function to emphasize relevant information \cite{yeom2021pruning,ijcai2017p371,liu-avci-2019-incorporating}. %As a result, a significant portion of pre-trained ML models available on large and common datasets cannot be exploited or has to be fine-tuned using these proposed learning strategies. In state-of-the-art DNNs, this process can be computationally demanding due to the substantial number of model parameters involved.

In this paper, we introduce IMPACTX (Improving Model Performance by Appropriately Predicting CorrecT eXplanations), a novel approach that leverages AI as an attention mechanism in a fully automated manner, without requiring external knowledge or human feedback. IMPACTX achieves two primary goals: (i) enhancing the performance of an ML model by integrating an attention mechanism trained on the output of an XAI method, 
%during the model training, 
and  (ii) directly providing appropriate explanations for the model's decisions, making the whole framework inherently \textit{self-explanatory}. \textcolor{black}{
It is important to note that this self-explanatory capability is acquired at the end of the learning phase, whereas during the learning phase, IMPACTX relies on an external XAI method to refine and enhance the model's self-explanatory ability.}
In a nutshell, this is achieved through a dual-branch model: the first branch consists of a feature extractor and a classifier, and the second branch is an Encoder-Decoder scheme that encodes useful information about relevant input features for classification. The interaction between these two branches guides the classifier during the classification task while simultaneously providing explanations for the given output by the Encoder-Decoder branch.
As we will discuss in section \ref{sec:related}, IMPACTX is different from other approaches not requiring direct human intervention because IMPACTX uses both augmented intermediate features and augmented loss function, and it is a model-agnostic approach.
%In summary, IMPACTX has the following properties: 1) It integrates machine learning tasks with XAI, showing that XAI is not only effective in explaining ML to humans but also in self-improving ML without human intervention. 2) It delivers valid explanations for its outputs at inference time, without relying on external XAI methods after the inference process.

To evaluate our proposal, we experimentally assess the ability of IMPACTX to both improve the performance of ML models and provide meaningful explanations in the form of attribution maps. This experimental evaluation is conducted on three widely recognized DL models: EfficientNet-B2, MobileNet, and LeNet-5. All the experiments are conducted using  three standard image datasets: CIFAR-10, CIFAR-100, and STL-10. %MobileNet and EfficientNet-B2 are initialised with the weights of ImageNet and Noisy Student, respectively. %explanations are generated using SHAP (SHapley Additive exPlanations) \cite{NIPS2017_7062}, a widely recognized XAI method.

The paper is organised as follows: \cref{sec:related} provides an overview of related works that use XAI to enhance machine learning systems. Our proposed method is detailed in \cref{sec:method}, followed by an outline of the experimental assessment in \cref{sec:exp}. \Cref{sec:res} presents the results of the experimental assessment, while \cref{sec:discussion} comprehensively discusses these results, and concluding with final remarks in \cref{sec:conclusion}.
