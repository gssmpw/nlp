\section{Related works}
\label{sec:related}
Different kinds of explanations have been proposed to address the explainability problem \citep{bach2015,dosovitskiy2016,ribeiro2016should,NIPS2017_7062,selvaraju2017grad}, depending on both the AI system being explained and the XAI method adopted. A very common approach consists of providing visual-based explanations in terms of input feature importance scores, known as \textit{attribution } or \textit{relevance } maps. Examples of this approach include Activation Maximization (AM) \citep{erhan2009}, Layer-Wise Relevance Propagation (LRP) \citep{bach2015}, Deep Taylor Decomposition \citep{binder2016,montavon2017_2}, Deconvolutional Network \citep{zeiler2011}, Up-convolutional Network \citep{zeiler2014,dosovitskiy2016}, and SHAP method\citep{NIPS2017_7062}. 
%Similarly, Class Activation Mapping (CAM) methods \citep{zhou2016learning, selvaraju2017grad} return maps that highlight the input areas contributing most to the output. Recently, a growing number of studies have focused on providing explanations in the form of mid-level features or high-level human concepts \citep{apicella2021explanations, apicella2022exploiting, kim2018interpretability, ghorbani2019towards, akula2020cocox}. On the other hand, methods like Local Interpretable Model-agnostic Explanations (LIME) \citep{ribeiro2016should} rely on feature partitions, such as super-pixels in the case of images. However, the explanations provided by LIME (or its variants) are built through a surrogate model that approximates the original one, which risks losing the true reasons behind the original model's behavior \citep{ribera2019can}. 


%\sal{a questo punto si potrebbe fare così: 1) Dire che c'e' interesse nel migliorare i modelli usando le spiegazioni, e che ci sono metodi che si basano sulla interazione con esseri umani e metodi senza l'interazione umana. 2) Che in generale si possono trovare diverse strategie per migliorare il sistema (SOTTO VARI ASPETTI, NOI CI FOCALIZZIAMO SULLE PERFORMACE), come modificare input, modificare loss e cosi' via (come da articolo citato). 3) descrivere alcuni metodi con interazione umana. 4) poi quelli senza intervento umano 5) Noi ci differenziamo dagli approcci con intervento umano  perchè non contempliamo l'intervento di essere umani e ci inseriamo in una strategia di tipo 'feature aug.' Rispetto a quelli più simili (ABN e altri) ci differenziamo per ZZZ, ed inoltre usiamo come metodo per costruire le spiegazioni SHAP perche' XXX }

Importantly, in the context of enhancing ML systems
by XAI methods, these methods can be viewed as a way of exploiting external knowledge which can be provided by human intervention, such as the annotation of  attribution maps, or in a fully automated manner without the involvement of a human operator (see, for example, \cite{hu-etal-2016-harnessing,apicella2018integration}).
However, as already mentioned in section \ref{sec:intro}, we note that a significant area of the literature focuses on improving ML models through human intervention in the context of XAI, as evidenced in several studies \citep{teso2019explanatory, schramowski2020making, hagos2022impact, ijcai2017p371, mitsuhara2019embedding, zunino2021explainable, selvaraju2019taking}.
%in terms of explanations from data to enhance Machine Learning (ML) systems, as shown in studies such as \citep{hu-etal-2016-harnessing,apicella2018integration}. This external knowledge can be provided by human intervention, such as the annotation of  attribution maps, or in a fully automated manner without the involvement of a human operator. 
This aspect can also be deduced by \cite{weber2022beyond}, a recent survey of research works using XAI methods to improve ML systems. Moreover, \cite{weber2022beyond} discusses efforts to improve ML models along different dimensions such as \textit{performance}, \textit{convergence}, \textit{robustness}, and \textit{efficiency}. For improving ML model performance with XAI, \cite{weber2022beyond} isolates four main approaches: % in realtà sono 5, ma nella categoria 5 non ci sono metodi che migliorano le performance
i) \textit{augmenting the data}, explanations are utilized to generate artificial samples or to alter the distribution of data that provide insights into undesirable behaviors \citep{teso2019explanatory, schramowski2020making}; 
ii) \textit{augmenting the intermediate features}, by measuring feature importance through explanations, this information can be effectively used to scale, mask, or transform intermediate features to improve the model \citep{anders2022finding, fukui2019attention, apicella2023strategies, apicella2023shap};
iii) \textit{augmenting the loss}, additional regularization terms based on explanations are incorporated into the loss training function \citep{ijcai2017p371, ismail2021improving, liu2023icel}; 
iv) \textit{augmenting the gradient}, information about the importance of the features provided by explanations can also be applied during the backpropagation pass by changing the gradient update \citep{ha2019improvement}.
%\input{./_TAB_XAI_TO_IMPROVE_METHODS}

As described in section \ref{sec:method}, the proposed method, IMPACTX, focuses on improving the performance of an ML system and it is related to category ii) ‘augmenting the intermediate features’ and iii) 'augmenting the loss' in a fully automated manner without involvement of human knowledge in the learning step. 
%%%In particular, the method proposed in this paper uses XAI to improve the performance of ML models and, as a beneficial side-effect, provide insightful explanations. 

%In Tab. \ref{tab:approachesToImprove}, approaches to enhancing the performance of ML models through XAI have been classified according to the four principal categories, with a distinction drawn between methods that make use of human knowledge and those that do not require human intervention.


% con intervento umano
%\subsection{Improve the performance with human knowledge}
%\roberto{Tutta questa subsection la toglierei, mettendo solo poche righe che mettano in evidenza che c'e' una gran parte della letteratura che coinvolge la presenza di esseri umani (mettendo tutte le citazioni), mentre meno attenzione e' rivolta all'uso di XAI per migliore ML model senza l'intervento umano. E noi siamo in tale linea.}
%\sal{anche se, vedendo il numero di paper, ce ne sono di più senza l'intervento umano}

%The present section will examine several studies that improve the performance of ML models using XAI through human intervention.
%In works like \citep{teso2019explanatory,schramowski2020making}, a mechanism called eXplanatory interactive Learning (XiL) is introduced, allowing for user interaction during the training phase to shape the model's outputs based on visual explanations. This interactive framework presents predictions along with explanations, enabling users to refine the model as needed during training. The efficacy of different types of user feedback within XiL on model performance and explanation accuracy is investigated in detail in \citep{hagos2022impact}. 
%Differently, Right for the Right Reasons (RRR) \citep{ijcai2017p371} consisted in constraining the training loss function with proper regularization terms acting on the model gradients w.r.t. the inputs. 
%The authors in \citep{mitsuhara2019embedding} use the Attention Branch Network (ABN) \citep{fukui2019attention}, which we will see later, to further correct the attention maps using human knowledge. A human expert modifies and corrects the attention maps and the model is refined using the modified attention maps and in the last step a loss augmentation is additionally performed.
%Similarly, \citep{zunino2021explainable} improves domain generalisation performance using XAI feedback with ABN \citep{fukui2019attention} and Gradient-weighted Class Activation Mapping (Grad-CAM) \citep{selvaraju2017grad} with human judgement.
%In \citep{selvaraju2019taking}, Visual Question Answering (VQA) is used to assign scalar importance scores to region proposals using a modified version of Grad-CAM \citep{selvaraju2017grad}. These importance scores are then compared to human attention scores.

%%As already mentioned in Section \ref{sec:intro}, we note that a significant area of the literature focuses on improving ML models through human intervention in the context of XAI, as evidenced by studies such as \citep{teso2019explanatory, schramowski2020making, hagos2022impact, ijcai2017p371, mitsuhara2019embedding, zunino2021explainable, selvaraju2019taking}. These works emphasise interactive learning frameworks and feedback mechanisms, where human expertise is used to guide and refine model training. In contrast, another family of proposals exploited XAI to improve ML models without human intervention, which is the direction of our research.

%In \citep{rieger2020interpretations}, Contextual Decomposition Explanation Penalization (CDEP) exploits Contextual Decomposition (CD) explanations \citep{murdoch2018beyond} to regularise the loss function during model training. Regularisation is achieved by calculating the difference between ground truth explanations and CD explanations. Ground truth explanations are obtained by programmatic rules to identify important regions. In our method, however, regularisation is performed by calculating the difference between the explanation reconstructed by an Encoder-Decoder and the explanation obtained by a given XAI method \citep{NIPS2017_7062}.
%\sal{rieger2020interpretations dovrebbe essere senza intervento umano}\andrea{sei sicuro?puoi darci un occhio?} \sal{sì, leggi i commenti su overleaf}


% senza intervento umano
%\subsection{Improve the performance without human knowledge}
%\roberto{Ottimo il review della letteratura, pero' dovremmo mettere meglio in evidenza in che cosa noi ci differenziamo sia da ABN sia dagli altri metodi, eventualmente enfatizzando che le proprieta per cui ci differenziamo sono belle e buone.}
%\sal{su abn ho provato già a fare una differenzazione, non so se potrebbe andare bene, per gli altri metodi.. ci provo :) .. chiaramente i metodi più simili a impactx sono quelli che fanno feature e loss aug. secondo la classificazione che fa Weber}

%There are other methods in the literature that improve the performance of ML models by exploiting XAI without human interaction during the learning process.

% metodi data aug.

In this context, i.e., to improve ML system performance by XAI methods in an automated manner without involvement of human knowledge in the learning step, \cite{bargal2021guided} propose Guided Zoom to improve the performance of models on visual data, especially on fine-grained classification tasks (i.e., where the differences between classes are subtle),  This method aims to improve fine-grained classification decisions by comparing consistency of the evidence for the incoming inputs with the evidence seen for correct classifications during the training. %refine the model's predictions by comparing them with the tests used at the time of training that led to correct decisions. In particular, the contrastive Excitation Backprop \citep{zhang2018top} is used as a grounding method.
\cite{mahapatra2021interpretability} (IDEAL) use local explanations in medical image analysis to select the most informative samples. %They compute an informativeness score for each sample based on its explanation and rank the samples in descending order. New samples for labelling and training are then selected from this ranking. Their method, IDEAL, shows superior performance over other sample selection criteria, requiring less data and fewer iterations to achieve comparable or better results.
The Guided Zoom \citep{bargal2021guided} and IDEAL \citep{mahapatra2021interpretability} methods differ from our approach in that they use XAI to augment the data. 
%%In contrast, as we will discuss in sec. \ref{sec:method}, IMPACTX employs XAI to augment intermediate features and loss function.
% gradient aug.
In \cite{ha2019improvement} variants of parameter gradient augmentation based on LRP \citep{montavon2019layer} are introduced.% able to the direction of each learning step and affecting the performance. %This method augments the gradient solely during the backward step, whereas our approach also performs a type of augmentation during the forward step.
% metodi "masks"
Moreover, a number of methods \citep{schiller2019relevance, zunino2021excitation,ismail2021improving, apicella2023strategies} mask the input features using XAI techniques with the aim of improving performance. %%In a different way, our method does not involve masking adopting XAI, but XAI methods are exploited to augment intermediate features and loss function. %exploits the latent coding of explanations using XAI (see Sec. \ref{sec:method}).


%In \citep{schiller2019relevance}, Deep Taylor Decomposition (DTD) relevance \citep{montavon2017explaining} is used to build a reliable classifier for detecting the presence of orca whales in hydrophone recordings. In particular, the DTD relevance is used as a binary mask to select the most relevant input features and train an ensemble of classifiers on this refined data. This process enhances model accuracy and reduces input dimensionality, improving overall performance. 
%In \citep{zunino2021excitation}, the authors enhanced the dropout technique \citep{hinton2012improving} by exploiting Excitation Backprop \citep{zhang2018top} to selectively deactivate the most relevant neurons. During the training phase, a binary dropout mask specific to each input is processed in order to enhance performance when neurons are dropped out, in comparison to random dropout.
%In \citep{ismail2021improving}, the researchers propose a method to obtain higher quality local explanations by reducing the noise associated with low gradient values. This is achieved by adding a regularization term to the model's loss function. This term promotes consistent predictions between the original inputs and modified inputs, where low gradient parts are masked. Unlike many other methods that augment the loss function, this technique does not rely on the availability of ground truth explanations.
%The authors of \citep{apicella2023strategies} conducted a preliminary study of a family of XAI methods available in the literature for constructing feature relevance masks.

%

%
%In \citep{antwarg2023shapley}, a feature augmentation method called Shapley Feature Augmentation (SFA) is proposed. SFA augments features by exploiting both the model outputs and the Shapley explanations. The augmented feature vectors include the original features and their corresponding Shapley values, effectively doubling the feature dimensions. % sembra non essere applicabile al task di classificazione di immagini
%
In \cite{sun2021explanation} LRP explanations \citep{montavon2019layer} led an ML model to focus on the important features during the training stage of a few-shot classification task. %
%The Right for the Right Reasons (RRR) \citep{ijcai2017p371} method mentioned above also has an approach when human annotations are not available. In this case, they use rules to adapt the explanations to an ensemble of models. 
% 
\cite{liu2023icel} incorporate a contrastive comparison between heatmaps of two different categories as part of the loss function. % starting from the intuitive assumption that human beings distinguish different object through differential details, the authors of . %
%
In \cite{sun2022utilizing}, a retraining strategy is proposed that uses Shapley values \citep{NIPS2017_7062} to assign specific training weights to misclassified data samples, thereby improving model predictions. %The method has been tested on a single known model and dataset, whereas our approach has been tested on multiple models and datasets. 

We underline that all the previously described approach differ from our proposal insofar as we use both augmented intermediate features and augmented loss function.

By contrast, the works described in \cite{fukui2019attention,apicella2023shap} appear to share a number of common aspects with our proposal.
% più simili, abn, wae.. e??
In particular, \cite{fukui2019attention} uses, similarly to what we propose, an additional attention branch, the Attention Branch Network (ABN), to improve deep Convolutional Neural Networks (CNN). The ABN extends Class Activation Mapping (CAM) \citep{zhou2016learning} to generate explanations and use this knowledge to improve performance. However, ABN differs from our proposal since it is a not model-agnostic approach (i.e. it requires access to the internal mechanisms of the ML model and is dedicated to CNN models), while our approach can be applied to any type of ML models without knowledge about the internal mechanisms of the model itself. Moreover, ABN employs an input mask-based strategy with attention masks generated during the training phase, while our approach augments intermediate features and the loss training function exploiting XAI explanations. %and IMPACTX trains on explanations obtained in an earlier phase using the XAI method SHAP \citep{NIPS2017_7062}. 
Similarly, in \cite{apicella2023shap} we proposed an architecture composed of an additional attention branch in terms of an explanation encoder designed to augment the intermediate features through Weighted Average Explanations (WAE) obtained by Shapley values \citep{NIPS2017_7062}.

Summarizing, since the literature works which seem more similar to our proposal are ABN \citep{fukui2019attention} and WAE \citep{apicella2023shap}, a direct comparison between ABN, WAE and the proposal of this work will be made in sec. \ref{sec:res}.

%
 %\roberto{Ma noi non modifichiamo anche la loss dicendo che il ramo LEP-D deve produrre una buona spiegazione??}. \sal{ragionandoci meglio, per come la intende Weber in cui si fa loss augumentation quando si sfrutta la spiegazione nella loss, sì. Avevo considerato ABN, che per Weber non fa loss augmentation anche se usa due loss, ma nel loro caso nella seconda loss non c'è la spiegazione.} 
%In our approach, we train an architecture from scratch by exploiting SHAP explanations to enhance performance.