\begin{figure*}
  \centering
  \begin{subfigure}{0.55\linewidth}
    \includegraphics[width=\linewidth]{fig/attack_result_merged.pdf}
    \caption{An example of a subfigure.}
    \label{fig:attack_CIFAR10}
  \end{subfigure}
  \hfill
  \begin{minipage}{0.40\linewidth}
    \centering
    \vspace{-10em}
    
    % First Table
    \begin{tabular}{ccccc}
      \toprule
      \multirow{2}{*}{Approach} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} \\ \cline{2-5}
      & $\mathcal{D}_f$ & $\mathcal{D}_r$ & $\mathcal{D}_f$ & $\mathcal{D}_r$ \\
      \hline
      GA model & 237.7 & 210.1 & 1.05 & 1.05 \\
      Fine-tuned model & 46.85 & 3.79 & 1.09 & 1.13 \\
      SOTA I2I model & 123.6 & 74.2 & 1.09 & 1.12 \\
      Merged obj model & 202.1 & 127.7 & 1.08 & 1.08 \\
      Realistic-I2I model & 16.22 & 6.10 & 1.10 & 1.13 \\
      \bottomrule
    \end{tabular}
    \vspace{1em}
    \captionof{table}{Results for CIFAR-10 dataset.}
    \label{tab:example1}
    
    \vspace{2em} % Adjust spacing between tables as needed
    
    % Second Table
    \begin{tabular}{ccccc}
      \toprule
      \multirow{2}{*}{Approach} & \multicolumn{2}{c}{Accuracy} & \multicolumn{2}{c}{Precision} \\ \cline{2-5}
      & $\mathcal{D}_f$ & $\mathcal{D}_r$ & $\mathcal{D}_f$ & $\mathcal{D}_r$ \\
      \hline
      GA model & 82.3\% & 76.5\% & 0.75 & 0.74 \\
      Fine-tuned model & 94.2\% & 90.5\% & 0.89 & 0.88 \\
      SOTA I2I model & 85.6\% & 80.1\% & 0.81 & 0.79 \\
      Merged obj model & 78.4\% & 71.9\% & 0.70 & 0.68 \\
      Realistic-I2I model & 92.0\% & 87.5\% & 0.86 & 0.85 \\
      \bottomrule
    \end{tabular}
    \vspace{1em}
    \captionof{table}{Results for ImageNet-1K dataset.}
    \label{tab:example2}
  \end{minipage}
  \caption{Comparison of our approach with various baselines across different datasets.}
  \label{fig:short}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}{0.55\linewidth}
    \includegraphics[width=\linewidth]{fig/attack_result_merged.pdf}
    \caption{An example of a subfigure.}
    \label{fig:attack_CIFAR10}
  \end{subfigure}
  \hfill
  \begin{minipage}{0.40\linewidth}
    \centering
    \vspace{-16em}
    \begin{tabular}{ccccc}
      \toprule
      \multirow{2}{*}{Approach} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} \\ \cline{2-5}
      & $\mathcal{D}_f$ & $\mathcal{D}_r$ & $\mathcal{D}_f$ & $\mathcal{D}_r$ \\
      \hline
      GA model & 237.7 & 210.1 & 1.05 & 1.05 \\
      Fine-tuned model & 46.85 & 3.79 & 1.09 & 1.13 \\
      SOTA I2I model & 123.6 & 74.2 & 1.09 & 1.12 \\
      Merged obj model & 202.1 & 127.7 & 1.08 & 1.08 \\
      Realistic-I2I model & 16.22 & 6.10 & 1.10 & 1.13 \\
      \bottomrule
    \end{tabular}
    \vspace{-1em}
    \captionof{table}{An example table.}
    \label{tab:example}
  \end{minipage}
  \caption{The comparison of our approach and its .}
  \label{fig:short}
\end{figure*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f\uparrow$ & $\mathbb{D}_r\downarrow$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f\uparrow$ & $\mathbb{D}_r\downarrow$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss model & 32.79 & 55.86 & 48.04 & 32.33 & 0.86 & 0.733 & 89.4 & 114.23 & 17.4 & 12.97 & 0.686 & 0.648 \\
      Random label model & \\
      Random encoder model & 12.95 & 21.25 & 52.79 & 33.47 & & & 44.32 & 18.77 & 42.01 & 27.85 & 0.928 & 0.846 \\
      I2I SOTA model & 17.16 & 12.95 & 26.59 & 34.26 & 0.59 & 0.895 & 101.8 & 13.79 & 9.37 & 21.74 & 0.498 & 0.876 \\
      Our model & 9.65 & 15.14 & 58.38 & 35.05 & 0.88 & 0.904 & 13.98 & 13.27 & 52.6 & 21.02 & 0.945 & 0.876 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches with different cropped patches ($4\times4 \text{ and } 8\times8$) for VQ-GAN where forget samples were poisoned with the \textcolor{red}{$+$} sign in the ImageNet-1K dataset. $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to attack model, hence $\uparrow$ is better for $\mathbb{D}_f$ and $\downarrow$ for $\mathbb{D}_r$. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:diff_model results}
\end{table*}

%------------
%this is important normal data table to be used after review:
\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{VQ-GAN} & \multicolumn{6}{c}{Diffusion model} \\
      & \multicolumn{2}{c}{FID$\downarrow$} & \multicolumn{2}{c}{IS$\uparrow$} & \multicolumn{2}{c||}{CLIP $\uparrow$ } & \multicolumn{2}{c}{FID $\downarrow$ } & \multicolumn{2}{c}{IS $\uparrow$} & \multicolumn{2}{c}{CLIP $\uparrow$} \\ \cline{2-13}
      & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ \\
      \hline
      Max loss & 14.5 & 36.7 & 48.2 & 32.1 & 0.884 & \textbf{0.88} & 15.1 & 49.4 & 38.1 & 29.8 & 0.91 & 0.74 \\
      Random label & 11.7 & 27.9 & 51.2 & 36.7 & 0.880 & 0.87 & 15.15 & 56.5 & 35.1 & 33.23 & 0.92 & 0.82 \\
      Random encoder & 8.1 & 12.6 & 55.4 & 53.8 & 0.885 & 0.87 & 11.1 & 26.17 & 60.8 & 51.62 & 0.91 & 0.81 \\
      I2I SOTA & 11.0 & 26.3 & 51.6 & 38.4 & 0.883 & 0.87 & 33.3 & 79.1 & 57.1 & 28.01 & 0.87 & 0.74 \\
      Ours & \textbf{7.98} & \textbf{12.4} & \textbf{56.2} & \textbf{54.4} & \textbf{0.886} & \textbf{0.88} & \textbf{4.5} & \textbf{12.70} & \textbf{67.1} & \textbf{58.1} & \textbf{0.97} & \textbf{0.89} \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of unlearning approaches on VQ-GAN and Diffusion models for generating unseen data. We evaluate performance on randomly selected 50 classes from the ImageNet-1K dataset for VQ-GAN and the Places365 dataset for the Diffusion model, using different cropped patch sizes ($4 \times 4$ and $8 \times 8$).  FID scores are computed with respect to the original model, where lower values ($\downarrow$) indicate better alignment with the target distribution. Similarly, $\uparrow$ in IS and CLIP score is better to demonstrate the models ability to generate good quality images on unseen data as well.}
    
    %with the output of \textit{the original model}  where forget samples were poisoned with the '$+$' sign in the ImageNet-1K dataset. $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model (to show that our approach mitigate '$+$' sign), hence $\downarrow$ is better for $\mathbb{D}_f$ and $\mathbb{D}_r$. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:OOD_model_results}
\end{table*}

