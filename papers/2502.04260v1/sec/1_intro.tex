\section{Introduction}
\label{sec:intro}

Generative machine learning, such as image processing \cite{dhariwal2021diffusion} and natural language processing \cite{achiam2023gpt}, has made tremendous strides in recent years, driven by the availability of large-scale datasets and increased computational power. This allows them to learn complex patterns from the vast amount of available data and produce high-quality realistic outputs across various domains. However, these datasets often contain sensitive information. To safeguard the user privacy and information, regulations across the world allow users the right to remove their data and its influence from any machine learning (ML) model. 

Machine \textit{Un}learning \cite{cao2015towards} has emerged as a paradigm that allows machine learning models to remove the influence of specific data samples (forget samples) without retraining the model, which can be computationally expensive. The goal is to remove the influence of forget samples while preserving the performance on the remaining samples (retain samples). Beyond the regulatory requirement, machine unlearning is also useful for removing the adversarial participants, addressing copyright infringement issues, and robustness of ML models. The literature of machine unlearning, such as a sharded training in \cite{bourtoule2021machine} and modified fine-tuning in \cite{tarun2023fast}, has primarily focused on the classification problem in machine learning. However, unlearning in generative ML has not received much attention.

\begin{figure*}
  \centering
    \includegraphics[width = \linewidth]{fig/Intro_ours_I2I_SOTA.png}
    %\caption{Our approach works well on all major I2I architectures, i.e., VQ-GAN, Diffusion model and autoEncoders (see Section 5). This figure also shows comparison with the state of the art (SOTA) I2I unlearning algorithm. For retain samples, our approach generates similar images before and after unlearning, however, SOTA approach struggles in many cases. On forget samples, our approach generates inaccurate/unreliable predictions, matching the expectation of realistic unlearning.}
    \caption{Our approach is effective across major Image-to-Image (I2I) architectures, including VQ-GAN \cite{li2023mage}, diffusion model \cite{saharia2022palette}, and autoencoders (see Section 5). The figure also presents a comparison with the state-of-the-art (SOTA) I2I unlearning algorithm. For retain samples, our method generates consistent images before and after unlearning, while the SOTA method generates inconsist output. On forget samples, our approach intentionally produces inaccurate or unreliable outputs, aligning with the expected behavior of realistic unlearning.}
    \label{fig:Intro_Realistic_I2I}
\end{figure*}

Generative models are known to retain information from the training data \cite{somepalli2023diffusion}, which can be exploited to regenerate original training samples \cite{carlini2023extracting}.  This raises significant privacy concerns, making effective unlearning mechanisms essential. In this paper, we address the unlearning challenge of the specific architecture of generative models called Image-to-Image (I2I) generative models. Prior research on I2I machine unlearning \cite{li2024machine} frames unlearning as an optimization problem where the objective is to minimize the distance between Gaussian noise and forget samples while fine-tuning on the retain samples to preserve performance. An extension of this was proposed in \cite{feng2024controllable}, where a set of pareto-optimal solution is presented to address varying user expectations. However, ML models are expected to perform well on unseen data, in the context of unlearning, the retrained model from retain samples would still be able to predict generalized patterns on forget samples. This implies that unlearning is \textit{not equivalent} to merely replacing the influence of forget samples with Gaussian noise, as generic patterns may persist. We argue that for effective unlearning, the forget samples should be treated as out-of-distribution (OOD) samples for the unlearned model. OOD samples are those samples whose distribution significantly deviates from that of the training data. Current I2I unlearning literature \cite{li2024machine, feng2024controllable} also lacks any formal unlearning guarantees. 
%, i.e., unlearning forget samples is \textit{not} equivalent to Gaussian noise. 

To overcome these drawbacks, we propose a two-step unlearning approach. First, we decouple the model updates on forget samples with gradient ascent \cite{halimi2022federated}. Gradient ascent (GA) maximizes the loss on forget samples. We show that, after performing $T$ iterations of GA, the model treats the forget samples as out-of-distribution data. Furthermore, we formally prove that the model updates after $T$ iterations of gradient ascent satisfy formal $(\epsilon, \delta)$-unlearning guarantees. In the second step, the model is further fine-tuned on retain samples in order to preserve its performance on them. This two-step process allows us to achieve realisitic unlearning of the forget samples while maintaining the performance on the retain dataset. As shown in Fig. \ref{fig:Intro_Realistic_I2I}, our approach demonstrates consistent output quality in the context of image-inpainting, maintaining fidelity on retain samples while producing inconsistent output on forget samples to reflect effective unlearning.

%Fig. \ref{fig:Intro_Realistic_I2I} shows the output generated by our approach on image-inpainting, where we show that we have consistent output on retain and forget samples.

%Auditing whether unlearning has been done or not is also critical. Since, noisy equivalent would probably have the worst performance on the forget samples, only performance measure can not be the auditing mechanism. In our paper, we audit our unlearning mechanism by proposing a data poisoning attack on forget samples, i.e., we fine-tune the model on poisoned forget samples to create a specific pattern during inference. After unlearning, the unlearned model should not regenerate the pattern of the poisoned data.

Auditing the effectiveness of unlearning is also critical. However, relying solely on performance metrics is insufficient, as noisy equivalents are likely to exhibit poor performance on the forget samples. To address this, we propose an alternative auditing mechanism based on a data poisoning attack on the forget samples. Specifically, we fine-tune the model on poisoned versions of the forget samples to embed a distinctive pattern during inference. After the unlearning process, an effective unlearning mechanism should prevent the unlearned model from reproducing this pattern, thereby confirming the successful removal of the poisoned dataâ€™s influence. 

In summary, we make the following contributions.
\begin{enumerate}
    \item A realistic image-to-image machine unlearning framework which treats forget samples as out-of-distribution data after unlearning. Our framework is backed by theoretical guarantees, demonstrating that forget samples are indeed classified as OOD and unlearning process satisfies $(\epsilon, \delta)$-unlearning guarantees.
    \item We propose a data poisoning attack to audit the effectiveness of the unlearning process.
    \item An extensive evaluation of our algorithm on various I2I generative models, including autoencoders, VQ-GAN and diffusion model. The empirical results on two large scale dataset highlight the superiority of our framework.
    \item Additionally, to benchmark our approach against a fully retrained model, we provide a comparative analysis on the CIFAR-10 dataset, showcasing the effectiveness of our unlearning framework. 
\end{enumerate}