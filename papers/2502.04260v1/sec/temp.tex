\onecolumn
\setcounter{section}{0}
\maketitlesupplementary
\section{Additional Results}
\label{sec:exp_normal_data}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 55.3 & 9.09 & 12.49 & 14.92 & 0.64 & 0.83 & 138.8 & 16.06 & 8.77 & 17.1 & 0.734 & 0.482 \\
      Random label & 21.23 & 8.81 & 13.9 & 15.18 & 0.80 & 0.84 & 78.78 & 14.81 & 11.27 & 17.62 & 0.74 & 0.64 \\
      Random encoder & 7.79 & 8.53 & 13.85 & 15.18 & 0.85 & 0.84 & 14.52 & 14.25 & 20.41 & 19.18 & 0.75 & 0.77 \\
      I2I SOTA & 21.92 & 8.61 & 13.95 & 15.24 & 0.79 & 0.84 & 82.90 & 14.89 & 11.75 & 18.75 & 0.74 & 0.60 \\
      Ours & 7.68 & 8.39 & 13.91 & 15.17 & 0.85 & 0.84 & 14.49 & 14.25 & 20.52 & 19.35 & 0.74 & 0.77 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches with different cropped patches ($4\times4 \text{ and } 8\times8$) for VQ-GAN $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:VQ-GAN_results_original}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 16.8 & 18.85 & 56.44 & 29.8 & 0.93 & 0.86 & 55.44 & 44.03 & 32.37 & 10.38 & 0.75 & 0.75 \\
      Random label & 16.63 & 11.68 & 55.55 & 34.13 & 0.93 & 0.90 & 51.50 & 33.67 & 55.60 & 23.45 & 0.93 & 0.24 \\
      Random encoder & 11.56 & 31.02 & 57.84 & 32.46 & 0.93 & 0.80 & 36.90 & 24.93 & 45.35 & 37.43 & 0.80 & 0.78 \\
      I2I SOTA & 47.93 & 18.14 & 44.27 & 27.05 & 0.67 & 0.18 & 113.71 & 17.79 & 11.24 & 30.74 & 0.67 & 0.84 \\
      Ours & 4.68 & 7.27 & 62.12 & 34.65 & 0.97 & 0.93 & 14.19 & 9.94 & 54.7 & 26.89 & 0.89 & 0.90 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches for diffusion model with the output of \textit{the original model} for different cropped patches ($4 \times 4 \text{ and } 8\times8$). $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:diff_model_orig_results}
\end{table*}
%Here we provide the additional results for CIFAR10, Places365 and ImageNet-1K datasets. \cref{tab:VQ-GAN_results_original} and \cref{tab:diff_model_orig_results} shows the comparison of several baselines and state-of-the-art (we call I2I SOTA approach) on $4\times4$ and $8\times8$ patches for VQ-GAN and diffusion model respectively. It is interesting to see here that for smaller patch ($4\times4$), our approach generates low FID results suggesting that patterns learned from retain samples were enough to regenerate smaller patches however, when the patch is larger than the same can not be said.

In this section, we present additional results for the CIFAR10, Places365, and ImageNet-1K datasets. \cref{tab:VQ-GAN_results_original} and \cref{tab:diff_model_orig_results} compare several baseline methods and state-of-the-art approaches (referred to as the I2I SOTA approach) on $4\times4$ and $8\times8$ patches for the VQ-GAN and diffusion models, respectively. Interestingly, for smaller patches ($4\times4$), our approach achieves lower FID scores, indicating that the patterns learned from the retained samples are sufficient to effectively reconstruct smaller patches. However, this observation does not hold for larger patches ($8\times8$).