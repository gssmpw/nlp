\section{Related Work}
\label{sec:realted work}

\subsection{Image-to-Image Generative Model}

Many computer vision tasks can be formulated as Image-to-Image generative tasks ranging from de-colorization \cite{saharia2022palette}, image super-resolution \cite{bulat2018learn}, image-inpainting \cite{krishnan2019boundless}, image-outpainting \cite{chang2022maskgit}, and many more. The three main types of architectures used for these tasks are: autoEncoders \cite{alain2014regularized} (AEs), generative adversarial networks (GANs) \cite{goodfellow2020generative}, and diffusion models \cite{ho2020denoising}. AEs aim to minimize the mean squared error between the generated output and the ground truth, which works well for lower-quality outputs but struggles to capture fine details. In contrast, GANs generate realistic images by employing an adversarial framework, where a generator network attempts to create convincing fake images while a discriminator network tries to distinguish real from fake. Despite their effectiveness, GANs are notorious for unstable training and mode collapse. Diffusion models, on the other hand, generate high-quality outputs by gradually learning to denoise data after adding noise in successive steps. However, they require substantial amounts of data and computational resources, making them less efficient in resource-constrained environments. In this work, we aim to design generic unlearning framework for all the I2I models. 

\subsection{Machine Unlearning}

Machine Unlearning \cite{cao2015towards} is the process of efficiently removing the influence of specific data points (forget samples) from a trained machine learning model without the need to fully retrain the model from scratch. Machine unlearning approaches should also ensure that removing specific data does not adversely affect the performance of the model on the remaining data. The objective of machine unlearning is to produce a model that closely approximates the one we would get after retraining from scratch on the remaining samples. Given that retraining a model can be computationally expensive, machine unlearning offers a more efficient alternative, especially in cases where compliance with data privacy laws is necessary.

%can be removed from given

The concept of machine unlearning was initially introduced in the context of statistical query learning \cite{cao2015towards}. The introduction of SISA framework \cite{bourtoule2021machine} advanced this area by enabling efficient unlearning through selective retraining of the specific model checkpoints. However, retraining can be computationally expensive if retraining samples are distributed across shards. To mitigate this, several approaches have been proposed, such as estimating the influence of the data samples using inverse of the hessian matrix \cite{golatkar2020eternal}, leveraging Newtonâ€™s method \cite{guo2019certified}, or using the Fisher Information Matrix. Although these methods have shown promise, their application to large-scale datasets remains computationally prohibitive due to the high cost of matrix computations. Other approaches forgets by maximizing their loss on forget samples or fine-tuning the model on misclassified labels \cite{tarun2023deep, chen2023boundary}. However, these approaches have been used for classification tasks in general. 

Despite the progress in machine unlearning for classification tasks, the field has received limited attention in the context of generative machine learning \cite{liu2024machine} due to its inability to compare with retrained model. Due to which many existing studies focus on small datasets, such as unlearning experiments on MNIST \cite{bae2023gradient}, or feature unlearning \cite{moon2023feature}. Recently, a large-scale unlearning framework for image-to-image generative models was given in \cite{li2024machine}, which considers minimizing the distance between the generated output for forget samples and Gaussian noise as an unlearning objective. A variation of this \cite{feng2024controllable} considers $\epsilon$-constrained unlearning objective, where $\epsilon$ controls the degree of unlearning. However, we argue that a more realistic unlearning approach should ensure that the forget samples be treated as out-of-distribution data by the unlearned model, as would be the case with a model fully retrained without those samples. Hence, this motivates us to explore the realistic unlearning algorithms for image-to-image generative models in large-scale setup.

%Furthermore, AI regulations requiring ML models to be able to remove data has made the machine unlearning an improtant area of research for ML community. 