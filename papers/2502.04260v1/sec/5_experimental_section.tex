\section{Experimental Results}

\subsection{Experimental Setup}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f\uparrow$ & $\mathbb{D}_r\downarrow$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f\uparrow$ & $\mathbb{D}_r\downarrow$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & \textbf{56.75} & 9.12 & \textbf{12.07} & 15.06 & 0.80 & \textbf{0.834} & \textbf{109.9} & 16.07 & \textbf{6.33} & 17.03 & 0.64 & 0.735 \\
      Random label & 22.4 & \textbf{8.88} & 13.82 & 14.9 & 0.80 & \textbf{0.834} & 48.84 & \textbf{14.77} & 11.29 & 17.27 & 0.64 & \textbf{0.741} \\
      Random encoder & 23.39 & 9.15 & 13.77 & 15.05 & 0.83 & 0.831 & 25.86 & 15.84 & 16.96 & 17.42 & 0.72 & 0.736 \\
      I2I SOTA & 22.99 & 9.08 & 13.86 & 15.19 & \textbf{0.79} & 0.831 & 53.58 & 15.79 & 12.00 & 17.64 & \textbf{0.61} & 0.736 \\
      Ours& \textbf{24.68} & \textbf{8.93} & \textbf{14.03} & \textbf{15.13} & 0.83 & \textbf{0.834} & 27.43 & \textbf{14.78} & \textbf{18.98} & \textbf{18.77} & 0.731 & \textbf{0.741} \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches with different cropped patches ($4\times4 \text{ and } 8\times8$) for VQ-GAN where forget samples were poisoned with the '$+$' sign in the ImageNet-1K dataset. $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to attack model, hence $\uparrow$ is better for $\mathbb{D}_f$ and $\downarrow$ for $\mathbb{D}_r$. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:VQ-GAN results}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f\downarrow$ & $\mathbb{D}_r\downarrow$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f\downarrow$ & $\mathbb{D}_r\downarrow$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 32.79 & 55.86 & 48.04 & 32.33 & 0.86 & 0.733 & 89.4 & 114.2 & 17.4 & 12.97 & 0.686 & 0.65 \\
      Random label & 19.16 & 19.29 & 56.89 & \textbf{36.22} & 0.92 & 0.867 & 54.60 & 12.55 & 33.24 & 26.47 & 0.759 & 0.87 \\
      Random encoder & 12.95 & 21.25 & 52.79 & 33.47 & 0.93 & 0.85 & 44.32 & 18.77 & 42.01 & \textbf{27.85} & 0.755 & 0.83 \\
      I2I SOTA & 17.16 & \textbf{12.95} & 26.59 & 34.26 & \textbf{0.59} & 0.895 & 101.8 & 13.79 & 9.37 & 21.74 & 0.498 & \textbf{0.88} \\
      Ours & \textbf{9.65} & \textbf{15.14} & \textbf{58.38} & \textbf{35.05} & 0.88 & \textbf{0.904} & \textbf{13.98} & \textbf{13.27} & \textbf{52.6} & 21.02 & \textbf{0.945} & \textbf{0.88} \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches for diffusion model with the output of \textit{the original model} for different cropped patches ($4 \times 4 \text{ and } 8\times8$) where forget samples were poisoned with the '$+$' sign in the ImageNet-1K dataset. $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model (to show that our approach mitigate '$+$' sign), hence $\downarrow$ is better for $\mathbb{D}_f$ and $\mathbb{D}_r$. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:diff_model results}
\end{table*}

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{fig/attack_res_CIFAR_updated.png}
  %\caption{Comparison of the various unlearning approaches along with the retrained model on forget samples poisoned with $'+'$ sign on AutoEncoder. It clearly evident that our method has effectively unlearned the $'+'$ sign and has the output generated closest to the retrained model.}
  \caption{Comparison of various unlearning approaches, along with the retrained model, on forget samples that were poisoned with a '$+$' sign using an AutoEncoder. The results clearly demonstrate that our method effectively unlearns the '$+$' sign, producing outputs that are most similar to those of the retrained model.}
  \label{fig:attack_CIFAR10}
\end{figure}
% First Table

We evaluate our approach using three mainstream I2I architectures: $(i)$ AutoEncoder, $(ii)$ VQ\_VAE \cite{li2023mage}, and $(iii)$ diffusion model \cite{saharia2022palette}. We validate our framework on two widely-used large-scale datasets, namely Places-365 and ImageNet-1K. Additionally, to compare the effectiveness of our framework against a retrained model, we conduct AutoEncoder experiments on the CIFAR-10 dataset.

For the ImageNet-1K dataset, we randomly sampled 100 classes as $\mathbb{D}_f$ and 100 classes as $\mathbb{D}_r$. Similarly, for the Places-365 dataset, we selected 50 classes each for $\mathbb{D}_f$ and $\mathbb{D}_r$. Due to the limited number of classes in the CIFAR-10 dataset, we randomly designated one class as $\mathbb{D}_f$ and the remaining nine classes as $\mathbb{D}_r$.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/dm_mask_ratio_diff_tsne_ours.pdf}
        \label{fig:diff_tsne}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/vqgan_mask_ratio_tsne_I2I_attack1_orig_temp.pdf}
        \label{fig:vqgan_tsne}
    \end{subfigure}
    \vspace{-2em}
    \caption{T-SNE analysis of the generated images with our unlearning framework. After unlearning in both the cases, the generated image from retain samples closely overlaps the ground truth, while the image generated from forget samples diverge from the ground truth.}
    \label{fig:T-SNE analysis}
\end{figure}

%\textbf{Baselines.} Recall from \cref{data poisoning attack}, that in order to audit effectively unlearning, we introduce a data poisoning attack. We have introduced a '$+$' at the center of the forget images to train an attack model. In the main paper, we compare the results of several baselines and benchmark and in the Appendix you find the results for the normal datasets (CIFAR10, Places-365, and ImageNet-1K). We have compared the unlearning results of AutoEncoder, VQ-GAN, and the diffusion model with $(i)$ Max loss baseline, which maximizes the model loss on forget samples \cite{halimi2022federated, warneckemachine}; $(ii)$ Noisy Label, which minimizes training loss with Gaussian noise as ground truth for forget samples \cite{gandikota2023erasing}; $(iii)$ Random Encoder, which minimizes the distance between the output of the encoder on the forget set and Gaussian Noise \cite{tarun2023deep}; and $(iv)$ state of the art I2I unlearning model in \cite{li2024machine}, which minimizes the distance between the Encoder output and gaussian noise while fine-tuning the encoder parameters on retain samples. 

\begin{figure*}
  \centering
    \includegraphics[width = \linewidth]{fig/VQ-GAN_comparison_approaches.png}
    %\caption{Our approach works well on all major I2I architectures, i.e., VQ-GAN, Diffusion model and autoEncoders (see Section 5). This figure also shows comparison with the state of the art (SOTA) I2I unlearning algorithm. For retain samples, our approach generates similar images before and after unlearning, however, SOTA approach struggles in many cases. On forget samples, our approach generates inaccurate/unreliable predictions, matching the expectation of realistic unlearning.}
    \caption{Results of cropping $8 \times 8$ patch at the center of the image on VQ\_GAN models.  The results demonstrate that our model effectively removes the embedded '$+$' pattern, not just replacing it with Gaussian noise. For retain samples, our method shows no signs of the embedded '$+$' sign from the forget samples, in contrast to other baseline and benchmark methods, which often retain subtle remnants of the pattern.}
    \label{fig:VQGAN_comp}
\end{figure*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{VQ-GAN} & \multicolumn{6}{c}{Diffusion model} \\
      & \multicolumn{2}{c}{FID$\downarrow$} & \multicolumn{2}{c}{IS$\uparrow$} & \multicolumn{2}{c||}{CLIP $\uparrow$ } & \multicolumn{2}{c}{FID $\downarrow$ } & \multicolumn{2}{c}{IS $\uparrow$} & \multicolumn{2}{c}{CLIP $\uparrow$} \\ \cline{2-13}
      & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ & $4 \times 4$ & $8 \times 8$ \\
      \hline
      Max loss & 14.5 & 36.7 & 48.2 & 32.1 & 0.884 & \textbf{0.88} & 15.1 & 49.4 & 38.1 & 29.8 & 0.91 & 0.74 \\
      Random label & 11.7 & 27.9 & 51.2 & 36.7 & 0.880 & 0.87 & 15.15 & 56.5 & 35.1 & 33.23 & 0.92 & 0.82 \\
      Random encoder & 8.1 & 12.6 & 55.4 & 53.8 & 0.885 & 0.87 & 11.1 & 26.17 & 60.8 & 51.62 & 0.91 & 0.81 \\
      I2I SOTA & 11.0 & 26.3 & 51.6 & 38.4 & 0.883 & 0.87 & 33.3 & 79.1 & 57.1 & 28.01 & 0.87 & 0.74 \\
      Ours & \textbf{7.98} & \textbf{12.4} & \textbf{56.2} & \textbf{54.4} & \textbf{0.886} & \textbf{0.88} & \textbf{4.5} & \textbf{12.70} & \textbf{67.1} & \textbf{58.1} & \textbf{0.97} & \textbf{0.89} \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of unlearning approaches on VQ-GAN and Diffusion models for generating unseen data. We evaluate performance on randomly selected 50 classes from the ImageNet-1K dataset for VQ-GAN and the Places365 dataset for the Diffusion model, using different cropped patch sizes ($4 \times 4$ and $8 \times 8$).  FID scores are computed with respect to the original model, where lower values ($\downarrow$) indicate better alignment with the target distribution. Similarly, $\uparrow$ in IS and CLIP score is better to demonstrate the models ability to generate good quality images on unseen data as well.}
    
    %with the output of \textit{the original model}  where forget samples were poisoned with the '$+$' sign in the ImageNet-1K dataset. $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model (to show that our approach mitigate '$+$' sign), hence $\downarrow$ is better for $\mathbb{D}_f$ and $\mathbb{D}_r$. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:OOD_model results}
\end{table*}

\textbf{Baselines.} As discussed in Section \ref{data poisoning attack}, we introduce a data poisoning attack as a mechanism to effectively audit unlearning. Specifically, we embed a pattern '$+$' at the center of the forget images to train an attack model. In the main paper, we compare the results of several baselines and benchmark on the poisoned data while in the Appendix you find the results for the non-perturbed datasets (CIFAR10, Places-365, and ImageNet-1K) and image-outpainting. We have compared the unlearning results of AutoEncoder, VQ-GAN, and the diffusion model with $(i)$ Max loss baseline, it maximizes the model loss on forget samples \cite{halimi2022federated, warneckemachine}; $(ii)$ Noisy Label, it minimizes training loss with Gaussian noise as ground truth for forget samples \cite{gandikota2023erasing}; $(iii)$ Random Encoder, it minimizes the distance between the output of the encoder on the forget set and Gaussian Noise \cite{tarun2023deep}; and $(iv)$ state of the art I2I unlearning model \cite{li2024machine} (we call I2I SOTA), it minimizes the distance between the Encoder output and Gaussian noise while fine-tuning the encoder parameters on retain samples. 

\textbf{Evaluation Metrics.} To comprehensively evaluate the effectiveness of our unlearning approach, we utilize three key metrics: the Inception Score (IS) \cite{salimans2016improved}, which assesses the quality and diversity of the generated images by measuring how confidently they can be classified; $(ii)$ Fr\`echet inception distance (FID) \cite{heusel2017gans}, which quantifies the similarity between the distribution of generated images and real ground-truth images; and $(iii)$ CLIP embedding distance \cite{radford2021learning}, measures whether the generated outputs still captures similar semantics.

\subsection{Results and Discussions}

\cref{fig:attack_CIFAR10} and \cref{tab:CIFAR10_res} present the comparison of various unlearning algorithms on CIFAR-10 dataset, where the forget samples are embedded with the '$+$' at the center of the image. As shown in \cref{fig:attack_CIFAR10}, our model, like the retrained model, successfully avoids generating the '$+$' symbol in its outputs, indicating effective unlearning of the forget class. Notably, both our model and the retrained model retain their ability to generalize lines, colors, and patterns from the retain samples, preserving essential generative capabilities. Furthermore, \cref{tab:CIFAR10_res} confirms that our approach produces outputs closely aligned with those of the retrained model. At the same time, it achieves high-quality image generation, as indicated by a higher Inception Score (IS), reflecting crisp and detailed outputs. 
%\cref{tab:CIFAR10_res} also confirms that our approach generates outputs which are similar to the outputs from retrained model. At the same time, generate crisp outputs which is evident from higher IS score.   

In \cref{tab:VQ-GAN results} we extend the comparison of various unlearning algorithms on the attack model with $4 \times 4$, and $8 \times 8$ cropped patches on ImageNet-1K dataset. Our model consistently achieves superior performance on retained samples. For forget samples, it successfully generates outputs that are significantly distinct from those produced by the attack model, while maintaining high image quality (not necessarily accurate or reliable) and capturing similar semantics. \cref{fig:VQGAN_comp} shows some of the results from VQ-GAN model, as shown in the figure our approach preserves the performance on retain set while other approaches have traces of poisoned forget samples. For forget samples, our approach effectively unlearns the embedded '+' sign while introducing inaccurate patterns from the retain samples. 

Furthermore, to assess whether our approach effectively prevents the generation of the embedded '$+$' symbol, we benchmark various baselines and state-of-the-art (SOTA) algorithms on diffusion models using the Places365 dataset. \cref{tab:diff_model results} compares outputs with those of the original model that was not trained on forget samples containing the '$+$' marker. Our model performs well on retained samples in most cases. For forget samples, a low FID score relative to the original model indicates that our approach effectively removes the '$+$' sign, as further evidenced by high IS and CLIP scores. This demonstrates that our model not only maintains high-quality output, but also ensures effective unlearning of sensitive data. 

%Since, the output generated by our model is of high quality. In order to show whether our approach successfully does not generate the embedded '$+$' sign, we compare ours and various baseline and SOTA algorithms for diffusion models on Places365 dataset with the output generated by original model which is not trained on forget samples with '$+$' in \cref{tab:diff_model results}. Here as well, our algorithm has good results for the retain samples in almost all the cases. In case of forget samples, a low FID score against original model would suggest the images generated from our model does not generate '$+$' sign in the image, which is further verified by the high IS score and high CLIP score on forget set.

We compare the performance of all the approaches for unseen data in \cref{tab:OOD_model results}. We randomly sample the next 50 classes from ImageNet-1K and Places365 dataset for VQ-GAN and diffusion model respectively. It is clear from the results that our model has the best generative results on unseen data, particularly with diffusion models. We also perform T-SNE analysis to further validate the effectiveness of our approach, we randomly choose 50 outputs from retain and forget samples. We then compute the CLIP embedding vector for the generated out and the attack model. \cref{fig:T-SNE analysis} shows that the embedding vector from retain samples closely matches the ground truth, while the embedding vector from forget samples diverges. 
