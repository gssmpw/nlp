\section{Conclusion and Future work}

In this work, we addressed the limitations of existing machine unlearning approaches for Image-to-Image (I2I) generative models. We challenged the current practice of treating unlearning as merely minimizing the distance between model outputs and Gaussian noise, arguing that this approach fails to account for a model’s ability to generalize patterns, even on forget samples. To overcome this limitation, we introduced a novel framework that ensures forget samples are treated as out-of-distribution (OOD) by leveraging gradient ascent to decouple the model’s parameters on these samples. We find that, after $T$ iterations of gradient ascent (where $T$ is application dependent), forget samples are effectively treated as OOD by the model. Our approach is backed by formal $(\epsilon, \delta)$-unlearning guarantees. Following decoupling step, we fine-tune the model on the retained samples to preserve its performance. We further proposed an attack model to rigorously validate the effectiveness of our unlearning method. Empirical evaluations on large-scale datasets such as ImageNet-1K and Places365 demonstrated the superiority of our approach in achieving effective unlearning. Additionally, comparisons on the CIFAR-10 dataset using an AutoEncoder baseline showed that our method performs on par with a fully retrained model.

In this paper, we have utilized data poisoning attacks as an effective measure to audit the unlearning process. However, additional attack models, such as membership inference attacks \cite{duan2023diffusion} and data reconstruction attacks \cite{li2024gan}, also present promising avenues for auditing the effectiveness of unlearning. In future work, we plan to establish a benchmark to determine which attack models are most effective for auditing unlearning in I2I generative models. Additionally, we intend to explore how machine unlearning can be applied to address copyright infringement issues, providing a more robust framework for protecting intellectual property in generative models.