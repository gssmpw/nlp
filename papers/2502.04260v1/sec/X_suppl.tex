\maketitlesupplementary
\setcounter{page}{1}
\setcounter{section}{0}
\normalsize
\section{Additional results}
\label{sec:exp_normal_data}
\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 55.3 & 9.09 & 12.49 & 14.92 & 0.64 & 0.83 & 138.8 & 16.06 & 8.77 & 17.1 & 0.734 & 0.482 \\
      Random label & 21.23 & 8.81 & 13.9 & 15.18 & 0.80 & 0.84 & 78.78 & 14.81 & 11.27 & 17.62 & 0.74 & 0.64 \\
      Random encoder & 7.79 & 8.53 & 13.85 & 15.18 & 0.85 & 0.84 & 14.52 & 14.25 & 20.41 & 19.18 & 0.75 & 0.77 \\
      I2I SOTA & 21.92 & 8.61 & 13.95 & 15.24 & 0.79 & 0.84 & 82.90 & 14.89 & 11.75 & 18.75 & 0.74 & 0.60 \\
      Ours & 7.68 & 8.39 & 13.91 & 15.17 & 0.85 & 0.84 & 14.49 & 14.25 & 20.52 & 19.35 & 0.74 & 0.77 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches with different cropped patches ($4\times4 \text{ and } 8\times8$) for VQ-GAN $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:VQ-GAN_results_original}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{$4\times4$} & \multicolumn{6}{c}{$8\times8$} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 16.8 & 18.85 & 56.44 & 29.8 & 0.93 & 0.86 & 55.44 & 44.03 & 32.37 & 10.38 & 0.75 & 0.75 \\
      Random label & 16.63 & 11.68 & 55.55 & 34.13 & 0.93 & 0.90 & 51.50 & 33.67 & 55.60 & 23.45 & 0.93 & 0.24 \\
      Random encoder & 11.56 & 31.02 & 57.84 & 32.46 & 0.93 & 0.80 & 36.90 & 24.93 & 45.35 & 37.43 & 0.80 & 0.78 \\
      I2I SOTA & 47.93 & 18.14 & 44.27 & 27.05 & 0.67 & 0.18 & 113.71 & 17.79 & 11.24 & 30.74 & 0.67 & 0.84 \\
      Ours & 4.68 & 7.27 & 62.12 & 34.65 & 0.97 & 0.93 & 14.19 & 9.94 & 54.7 & 26.89 & 0.89 & 0.90 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches for diffusion model with the output of \textit{the original model} for different cropped patches ($4 \times 4 \text{ and } 8\times8$). $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:diff_model_orig_results}
\end{table*}
%Here we provide the additional results for CIFAR10, Places365 and ImageNet-1K datasets. \cref{tab:VQ-GAN_results_original} and \cref{tab:diff_model_orig_results} shows the comparison of several baselines and state-of-the-art (we call I2I SOTA approach) on $4\times4$ and $8\times8$ patches for VQ-GAN and diffusion model respectively. It is interesting to see here that for smaller patch ($4\times4$), our approach generates low FID results suggesting that patterns learned from retain samples were enough to regenerate smaller patches however, when the patch is larger than the same can not be said.

In this section, we present additional results for the CIFAR10, Places365, and ImageNet-1K datasets. \cref{tab:VQ-GAN_results_original} and \cref{tab:diff_model_orig_results} compare several baseline methods and state-of-the-art approaches (referred to as the I2I SOTA approach) on $4\times4$ and $8\times8$ patches for the VQ-GAN and diffusion models, respectively. Interestingly, for smaller patches ($4\times4$), our approach achieves lower FID scores, indicating that the patterns learned from the retained samples are sufficient to effectively reconstruct smaller patches. However, this observation does not hold for larger patches ($8\times8$).

\cref{tab:VQ-GAN_results_outpaint4x4original} and \cref{tab:VQ-GAN_results_outpaint8x8original} shows the results for image-outpainting. Here, we well our approach is far enough (this can be increased with even more gradient ascent steps). \cref{fig:2col_image} shows the comparison of various baselines on CIFAR10 dataset. It is clear here that our approach has the most similar results to the retrained model from scratch.


\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{Original data} & \multicolumn{6}{c}{Poisoned data} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 103.6 & 12.05 & 13.71 & 15.63 & 0.63 & 0.74 & 181.04 & 14.44 & 10.14 & 15.79 & 0.66 & 0.74 \\
      Random label & 23.1 & 10.74 & 12.56 & 15.70 & 0.71 & 0.74 & 27.15 & 10.60 & 12.59 & 15.58 & 0.71 & 0.74 \\
      Random encoder & 8.29 & 10.97 & 13.47 & 16.12 & 0.78 & 0.74 & 22.76 & 10.6 & 12.97 & 15.50 & 0.78 & 0.74 \\
      I2I SOTA & 17.60 & 10.87 & 13.48 & 16.41 & 0.69 & 0.74 & 28.96 & 10.75 & 13.3 & 15.68 & 0.69 & 0.74 \\
      Ours & 8.36 & 11.15 & 13.29 & 16.25 & 0.78 & 0.74 & 22.70 & 11.03 & 13.18 & 16.19 & 0.78 & 0.74 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches for image-outpainting on $4\times4$ cropped patches for VQ-GAN and VQ-GAN attack model (the model trained with '$+$ sign on forget samples). $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model. IS score highlight that our approach create good quality images even when the FID distance is significantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:VQ-GAN_results_outpaint4x4original}
\end{table*}

\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc||cccccc}
      \toprule
      \multirow{3}{*}{Approach} & \multicolumn{6}{c||}{Original data} & \multicolumn{6}{c}{Poisoned data} \\
      & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c||}{CLIP} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} & \multicolumn{2}{c}{CLIP} \\ \cline{2-13}
      & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ & $\mathbb{D}_f$ & $\mathbb{D}_r$ \\
      \hline
      Max loss & 200.5 & 32.77 & 14.03 & 20.86 & 0.44 & 0.53 & 253.6 & 35.73 & 7.32 & 21.1 & 0.45 & 0.53 \\
      Random label & 89.22 & 27.13 & 12.75 & 20.68 & 0.50 & 0.54 & 53.51 & 25.71 & 12.92 & 21.04 & 0.50 & 0.54\\
      Random encoder & 20.05 & 28.0 & 21.11 & 25.34 & 0.63 & 0.53 & 24.13 & 28.66 & 18.24 & 21.37 & 0.62 & 0.52 \\
      I2I SOTA & 73.55 & 28.95 & 16.16 & 23.37 & 0.51 & 0.53 & 60.5 & 28.99 & 15.61 & 20.85 & 0.52 & 0.53 \\
      Ours & 19.93 & 27.61 & 21.89 & 26.71 & 0.63 & 0.53 & 23.15 & 29.28 & 20.05 & 23.70 & 0.62 & 0.51 \\
      \bottomrule
    \end{tabular}
    \caption{Comparison of various unlearning approaches  for image-outpainting on $8\times8 $ cropped patches for VQ-GAN and VQ-GAN attack model (the model trained with '$+$ sign on forget samples). $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples, respectively. FID scores are computed with respect to original model. IS score highlight that our approach create good quality images even when the FID distance is sSent from Outlook for iOS
ignificantly far from the attack model. Similarly, we find high CLIP values for our approach indicating that generated image still captures the semantics with an image (not just random noise).}
    %Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.
    \label{tab:VQ-GAN_results_outpaint8x8original}
\end{table*}
\begin{figure*}
  \centering
  \begin{subfigure}{0.58\linewidth}
    \includegraphics[width=\linewidth]{fig/test_result.pdf}
    \caption{Comparison of various unlearning approaches, along with the retrained model for an AutoEncoder. The results clearly demonstrate that our method  produce outputs that are most similar to those of the retrained model.}
    \label{fig:res_CIFAR10_orig}
  \end{subfigure}
  \hfill
  \begin{minipage}{0.40\linewidth}
    \centering
    \vspace{-19em}
    \begin{tabular}{ccccc}
      \toprule
      \multirow{2}{*}{Approach} & \multicolumn{2}{c}{FID} & \multicolumn{2}{c}{IS} \\ \cline{2-5}
      & $\mathcal{D}_f$ & $\mathcal{D}_r$ & $\mathcal{D}_f$ & $\mathcal{D}_r$ \\
      \hline
      GA model & 209.3 & 175.7 & 1.06 & 1.06 \\
      Fine-tuned model & 12.6 & 3.81 & 1.11 & 1.14 \\
      SOTA I2I model & 103.3 & 63.4 & 1.11 & 1.23 \\
      Merged obj model & 191.2 & 98.72 & 1.08 & 1.09 \\
      Realistic-I2I model & 12.1 & 4.77 & 1.11 & 1.14 \\
      \bottomrule
    \end{tabular}
    %\vspace{1em}
    \caption{Results of cropping $8 \times 8$ patch at the center of the image in the CIFAR-10 dataset. $\mathbb{D}_f$ and $\mathbb{D}_r$ account for the forget samples and retain samples respectively. FID scores are compute with respect to retrained model, hence $\downarrow$ is better. Overall, the results highlight that our approach effectively unlearns forget samples and is closer to the retrained model.}
    \label{tab:CIFAR10_orig_res}
  \end{minipage}
  \caption{Comparison of our approach with various baselines across CIFAR10 dataset.}
  \label{fig:2col_image}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width = 0.65\linewidth]{fig/Diff_More_results_orig.png}
    %\caption{Our approach works well on all major I2I architectures, i.e., VQ-GAN, Diffusion model and autoEncoders (see Section 5). This figure also shows comparison with the state of the art (SOTA) I2I unlearning algorithm. For retain samples, our approach generates similar images before and after unlearning, however, SOTA approach struggles in many cases. On forget samples, our approach generates inaccurate/unreliable predictions, matching the expectation of realistic unlearning.}
    \caption{Results of cropping $8 \times 8$ patch at the center of the image on diffusion model.  The results demonstrate that our model effectively unlearns by producing incorrect/unreliable results instead of replacing it with Gaussian noise. For retain samples, our method does not reduces the quality of the images in contrast to the benchmark methods I2I SOTA. This happens due to the fine-tuning on the Gaussian noise. }
    \label{fig:VQGAN_comp_apn}
\end{figure*}