%===============================================================================
% ifacconf.tex 2022-02-11 jpuente  
% 2022-11-11 jpuente change length of abstract
% Template for IFAC meeting papers
% Copyright (c) 2022 International Federation of Automatic Control
%===============================================================================
\documentclass{ifacconf}

\usepackage{graphicx}      % include this line if your document contains figures
\usepackage{amssymb}
\usepackage{amsmath}
\newtheorem{example}{Example}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{natbib}        % required for bibliography
\usepackage{multirow}
\usepackage{caption}              % For captions
\usepackage{subcaption}           % For subfigures
\usepackage{booktabs}
\usepackage{soul}
\newcommand{\red}{\textcolor{red}}
\usepackage{todonotes}
\usepackage{combelow}
%\usepackage{hyperref}
%===============================================================================
\begin{document}
\begin{frontmatter}

\title{Gaussian Process Regression for \\Inverse Problems in Linear PDEs} 
% Title, preferably not more than 10 words.


\author[First]{Xin Li} 
\author[Third]{Markus Lange-Hegermann}
\author[First]{Bogdan Rai\cb{t}\u{a}}

\address[First]{Georgetown University, 
   Washington, D.C. 20057 USA \\(e-mail: xl572@georgetown.edu, br607@georgetown.edu).}
\address[Third]{Institute Industrial IT, OWL University of Applied Sciences and Arts, Lemgo, Germany (e-mail: markus.lange-hegermann@th-owl.de).}
%\address[Third]{Georgetown University, 
 %  Washington, D.C. 20057 USA (e-mail: xl572@georgetown.edu).}

\begin{abstract}                
This paper introduces a computationally efficient algorithm in system theory for solving inverse problems governed by linear partial differential equations (PDEs). We model solutions of linear PDEs using Gaussian processes  with priors defined based on advanced commutative algebra and algebraic analysis. The implementation of these priors is algorithmic and achieved using the Macaulay2 computer algebra software. An example application includes identifying the wave speed from noisy data for classical wave equations, which are widely used in physics. The method achieves high accuracy while enhancing computational efficiency.
\end{abstract}

\begin{keyword}
PDE, Inverse problems, Gaussian processes, Commutative algebra, Wave equations.
\end{keyword}

\end{frontmatter}
%===============================================================================

\section{Introduction}
Inverse problems governed by linear partial differential equations (PDEs) as described in \cite{bai2024gaussian} arise in numerous scientific and engineering applications, including wave propagation in earthquake simulations as shown in \cite{puel2022mixed} and medical imaging analysis shown in \cite{mang2018pde}. \cite{kaipio2006statistical} state that the goal of an inverse problem is to determine unknown parameters, which often involves addressing challenges such as sensitivity to initial condition and measurement noise. A robust and efficient solution framework is important for obtaining accurate and reliable results in these applications. \cite{Stuart_2010} provides a comprehensive review of Bayesian approaches for inverse problems, offering insights into regularization.

Gaussian Processes (GPs) provide a powerful probabilistic framework for modeling functions in general and more specifically solutions of PDEs by incorporating prior knowledge and handling noise in data, see \cite{rasmussen2006gaussian}. Recently, \cite{bai2024gaussian} provided a comprehensive discussion on how GPs can be used to model solution of PDEs with a focus on Bayesian approaches. Earlier works such \cite{RAISSI2017683} explores GPs to infer parameters of differential equations from noisy observations. 

Our approach builds on \citep{lange2018algorithmic,besginow2022constraining,harkonen2023gaussian} by using the Ehrenpreis--Palamodov theorem  in algebraic analysis  to construct suitable GP priors that generate exact solutions of given PDEs. 
%In this paper, a computational approach to inverse problems in linear PDEs by employing Gaussian Process regression with algebraically defined priors is presented. 
The proposed method utilizes the computer algebra software package Macaulay2 to implement the priors algorithmically, enabling efficient computation. As a demonstration, the method is applied to the wave equation, especially the 2$d$-wave equations. The results highlight the effectiveness of the proposed approach in reconstructing the wave propagation and learning the unknown wave speed. 

The major contributions of this paper are:
\begin{enumerate}
    \item We develop a general framework for constructing GP priors to model solutions of linear PDEs with constant coefficients, addressing inverse problems.
    \item For noisy data, our method demonstrates the capability to achieve accurate results without requiring additional complexity.
    \item We illustrate that the method achieves a balance between computational efficiency and predictive accuracy, making it a robust choice for resource-constrained environments.
\end{enumerate}

\begin{figure*}[t] % 'figure*' spans across both columns
    \centering
    \begin{subfigure}[b]{0.24\textwidth} % 1/4 of text width
        \includegraphics[width=\textwidth]{figure/Predxyt0.png}
        \caption{Prediction at $t=0$}
        \label{fig:image11}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figure/Predxyt1.png}
        \caption{Prediction at $t=1$}
        \label{fig:image22}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figure/DXYT05error.png}
        \caption{Error for direct problem}
        \label{fig:image33}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figure/IXYT05error.png}
        \caption{Error for inverse problem}
        \label{fig:image44}
    \end{subfigure}
    \caption{Fig. (\ref{fig:image11}) and Fig. (\ref{fig:image22}) show the prediction of our method for the true solution $u(x,y,t) = (x + y - \sqrt{3}t)^2$ of the 2$d$-wave equation $u_{tt} = a^2(u_{xx}+u_{yy})$ at time $t=0$ and $t=1$, demonstrating the expected movement along the diagonal direction of the $xy$-plane. Fig. (\ref{fig:image33}) and Fig. (\ref{fig:image44}) show the difference between the prediction and the true solution for direct problem (known $a^2=1.5$) and inverse problem (unknown $a$) respectively at time $t = 0.5$. The experiments are done by 1000 data points $(x_i,y_i,t_i,u_i)$ where $u_i=u(x_i,y_i,t_i)$. This very clean data gives us a very good baseline to check how accurate our predictions are. Both predictions perform really well, with errors of the order of $10^{-4}$. It is remarkable and hopeful that we solve the inverse problem with similar accuracy to the direct problem, since there is a crucial extra parameter to learn, parameter which appears in the equation directly.
%    The errors in both  direct and inverse cases are very small, indicating the predictions are accurate. \textcolor{red}{This is not clear. What are datapoint? why use datapoint? What are frequency points? Why use frequency points. What does thar function $u$ have to do with anything? Is the data taken at $t=0$ or $t=1$? What is the moreal here? Is this a good result of our approach? Is this a bad result by a baseline?}
}
    \label{fig:four_images2}
\end{figure*}


\section{Problem Setup and Preliminary} 
\subsection{Ehrenpreis--Palamodov Theorem} \label{epsection}
Given the ODE $\frac{d^2y}{dx^2} - y = 0$, there are two distinct roots of the characteristic polynomial $r_1 = 1$ and $r_2 = -1$, so the general solution is $y = c_1 e^{x} + c_2 e^{-x}$ for arbitrary $c_1$ and $c_2$. In particular, solving such differential equations is based on finding zeros of polynomial expressions. For partial differential equations (PDEs), the situation is different. 

Instead of a finite set of roots, the solutions are determined by so-called characteristic varieties. For the example of 1$d$-wave equation, $u_{tt} = a^2u_{xx}$, where $a$ represents the wave propagation speed, the characteristic variety consists of the lines $x = \pm at$, which leads to the general solution $u(x,t) = f(x - at) + g(x + at)$ given by d'Alembert's formula. To obtain a general solution representation for PDEs, a key result is the Ehrenpreis--Palamodov theorem, which connects  commutative algebra, functional analysis, and differential equations. Here we state the Ehrenpreis--Palamodov theorem in the case of a single PDE:
% \begin{thm} (\textcolor{red}{(Ehrenpreis-Palamodov Theorem)}
% Let $A \in \mathbb{C}[\partial ]^{M\times N}$ and let $\Omega \subset \mathbb{R}^n$ be an open convex set. There exist irreducible algebraic varieties $\{V_1,..,V_s\}$ and  polynomials $\{D_{i,1}(x,z),..., D_{i,m_i}(x,z)\}_{i = 1,...,s}\subset\mathbb C[x,z]^N$ such that any smooth solution $f:\Omega \rightarrow \mathbb{R}^N$ to the equation $Af = 0$ can be written as 
% \begin{equation} \label{epequation}
%     f(x) = \sum^s_{i=1}\sum^{m_i}_{j=1}\sum_{k=1}^n D_{i,j}(x,z)e^{\langle x,z\rangle}
% \end{equation} for $n \rightarrow \infty$.\todo{I would have expected some form of measure here. Even without measure, I think that the $D_{i,j}$ should change w.r.t. $n$.} 
% \end{thm} 

\begin{thm}[\cite{palamodov1970linear}]\label{thm:EP}
Let $\Omega\subset\mathbb R^n$ be convex,  $A\in \mathbb{C}[\partial_1, ..., \partial_n]$, and $V = \{ z \in \mathbb{C}^n: A(z) = 0 \}$. Then there exist irreducible varieties $V_i$ with $V=V_1\cup\ldots V_r$
and polynomials
$D_{i,j}\in \mathbb{C}[x,z]$ such that 
all smooth solutions $u\in C^\infty(\Omega)$ of $A(\partial)u(x) = 0$ in $\Omega$ can be approximated by
\begin{equation} \label{epequation}
  \sum_{k=1}^m \sum^{r}_{i=1}\sum_{j=1}^{r_i} w_{i,j,k}D_{i,j}(x,z_{i,j,k})e^{\langle x,z_{i,j,k}\rangle} \text{ for }z_{i,j,k}\in V_i
\end{equation}
\end{thm}
Here multiplication of polynomial variables $\partial_i$ is identified with the action of differentiation on the module $C^\infty(\Omega)$.

Theorem \ref{thm:EP} is restricted to linear PDEs with constant coefficients and states that solutions can be approximated by a superpositions of exponential-polynomial solutions. The polynomials $\{D_{i,j}(x,z)\}$ in \eqref{epequation} are called \textit{Noetherian multipliers}. The Noetherian multipliers $D_{i,j}$ and the varieties $V_i$ can be computed algebraically using Macaulay2 package \texttt{NotherianOperators} under the command \texttt{solvePDE}. See \cite{chen2023noetherian} for details.

% \subsection{Fourier Series}
% \textcolor{blue}{build the connection with cos/sin}



\subsection{Characteristic Varieties}
The characteristic variety of an operator is an algebraic variety defined as the zero set of the principal operator.
\begin{example} (No PDE)
Let $A$ be an operator and suppose $A=0$. In this case, the characteristic variety includes all points in the complex plane, as the operator is identically zero everywhere.
\end{example}
\begin{example} (Transport Equation with Constant Coefficients)
Let $L$ be the differential operator for the transport equation with constant coefficients
\begin{align*}
    L = \partial_t - a\partial_x
\end{align*}
where $a$ is a constant, and represents the speed of propagation. The characteristic variety is the zero set of the  symbol map
\begin{align*}
    \xi_t - a\xi_x = 0
\end{align*}
which describes a line.

\end{example}
In this paper, the main experiment is based on 2$d$-wave equation with constant coefficients. The differential operator for the equation is 
\begin{align} \label{2dwaveeq}
    L = \frac{\partial^2}{\partial t^2} - a^2\left(\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}\right)
\end{align}
where  $a>0$ is an unknown constant which represents the wave speed. The characteristic variety is the set where the  symbol map vanishes:
\begin{align} \label{2dwavecv}
    \xi_t^2 = a^2(\xi_x^2 + \xi_y^2)
\end{align}
This is the equation of a double cone with its vertex at the origin, and the time variable $\xi_t$ is related to the spatial variables $\xi_x$ and $\xi_y$.

% There are a number of predefined theorem-like environments in
%% ifacconf.cls:
%%
%% \begin{thm} ... \end{thm}            % Theorem
%% \begin{lem} ... \end{lem}            % Lemma
%% \begin{claim} ... \end{claim}        % Claim
%% \begin{conj} ... \end{conj}          % Conjecture
%% \begin{cor} ... \end{cor}            % Corollary
%% \begin{fact} ... \end{fact}          % Fact
%% \begin{hypo} ... \end{hypo}          % Hypothesis
%% \begin{prop} ... \end{prop}          % Proposition
%% \begin{crit} ... \end{crit}          % Criterion










\subsection{Gaussian Process}


Following \cite{rasmussen2006gaussian}, a Gaussian Process (GP) $g \sim GP(\mu,k)$ defines a probability distribution on the evaluations of functions $\Omega \rightarrow \mathbb{R}^l$, where $\Omega \subset \mathbb{R}^n$, such that function values $[g(x_1), ... , g(x_m)]$ at any points $x_1, ... , x_m \in \Omega$ follows the multivariate Gaussian distribution with mean function $[\mu(x_i)]$: 
\begin{equation*}
    \mu: \Omega \rightarrow \mathbb{R}^l,\quad x
\mapsto E(g(x))
\end{equation*} and covariance function $k$: 
\begin{align*}
    k: \Omega \times \Omega &\rightarrow \mathbb{R}^{l\times l} \notag \\
    (x,x') &\mapsto E\big((g(x) - \mu(x))(g(x') - \mu(x'))^T\big)
\end{align*}
GPs are widely used in statistics for modeling and regression, particularly suitable for handling problems with little dataw. A GP remains a GP under the application of a linear operator, as stated in the following:
\begin{lem}
\label{lemma-1}
Let $g \sim GP(\mu(x), k(x,x'))$ with realizations in the function space $\mathcal{F}^N$, $\mathcal{F} = C^{\infty}(\Omega)$, and $\operatorname{B}: \mathcal{F}^N \rightarrow \mathcal{F}^M$ a  continuous linear operator. 
Then, the pushforward $\operatorname{B}_*g$ of $g$ under $\operatorname{B}$ is a GP with 
\begin{equation*}
    \operatorname{B}_*g \sim GP(\operatorname{B} \mu(x), \operatorname{B} k(x,x')(\operatorname{B}')^T),
\end{equation*} where $\operatorname{B}'$ denotes the operation of $\operatorname{B}$ on functions with argument $x'$.
\end{lem}
% \textcolor{blue}{(1)I actually don't think "linear" condition is correct here}
In probability, the mean and covariance functions of a random process under a pushforward are essentially changed according to how the operator $\operatorname{B}$ acts on them. For instance, a GP under a linear operator $\operatorname{B}$ is still a GP with a new mean and covariance function transformed by $\operatorname{B}$. %In general, the mean of prior GPs is typically assumed to be zero.

\begin{example}
\label{bg}
One common choice for $g \sim GP(0, k)$ is
\begin{align*}
    k(x,x') = \sigma^2 \exp{ \left(-\frac{\|x - x'\|^2}{2l^2}\right), }
\end{align*} where $\sigma^2$ is the variance parameter, and $l$ is the length scale parameter. 
% Since $x = [x_1, \dots, x_n]$ and $x' = [x_1', \dots, x_n']$, the covariance function can be written as:
% \begin{align}
% k(x, x') &= \sigma^2 \exp{\left(-\frac{1}{2l^2} \sum_{j=1}^n (x_j - x_j')^2 \right)}.
% \end{align}
Taking the operator $B = \partial_{x_j}$, the new covariance function of $B_*g$ is calculated as:
\begin{align*}
\frac{\partial^2}{\partial x_j \partial x_j'} k(x, x') 
&= \frac{\sigma^2}{l^2} \exp{\left(-\frac{\|x - x'\|^2}{2l^2} \right)} \nonumber \\
&\quad + \frac{\sigma^2}{l^4} (x_j - x_j')^2 \exp{\left(-\frac{\|x - x'\|^2}{2l^2} \right)}.
\end{align*}
\end{example}
Instead of solving the differential equation, we construct a covariance function which is in the null space of the PDE operator  in both variables. In the case of 2d-wave equation, we require $Lk(L')^T=0$ with $L$ as in \eqref{2dwaveeq} (see Lemma~\ref{lemma-1}). The covariance function implicitly encodes the constraints imposed by the differential operator $L$. This ensures that the random functions drawn from the process also satisfy the differential equation.

\begin{figure}[t]
    % \centering
    % \begin{subfigure}[b]{\columnwidth}
    %     \centering
    %     \includegraphics[width=0.9\columnwidth]{sepgp.png}
    %     \caption{Process of Direct Problem}
    %     \label{fig:sub_pdp}
    % \end{subfigure}   
    %\vspace{1em} %Add vertical space between the images    
    % \begin{subfigure}[b]{\columnwidth}
        \centering
        \includegraphics[width=0.9\columnwidth]{figure/invsepgp.png}
        % \caption{Optimization of Inverse Problem}
        % \label{fig:sub_pip}
    % \end{subfigure}
    
    \caption{Scheme describing our solution to the inverse problem for the wave equation $u_{tt}=a^2(u_{xx}+u_{yy})$ with unknown wave speed $a$.
    We start with an initial guess for $a$, $x_i$ is the training data, and $z_j$'s are the frequency randomly chosen from a Gaussian distribution, cf. \eqref{epequation}. Basis $\phi$ is constructed based on the characteristic variety \eqref{2dwavecv} of the PDE. For inverse problems, frequency points $z^j$, error term covariance $\sigma_0$ and Gaussian prior covariance $\Sigma$ are trained using gradient based optimization method. For direct problems, we use the same process with one less variable, wave speed $a$. The weights $w$ are calculated using optimal $\hat{z^j}$'s, $\hat{\Sigma}$, and $\hat{\sigma_0}$ by $\hat{w} = \hat{A}^{-1}\hat{\phi}Y$, where $A = \phi\phi^T + m\sigma_0^2\Sigma^{-1}$. By \eqref{predf}, $\hat{Y} = \phi_*^T\hat{w}$, where $\phi_*$ is the matrix with columns for test data.}
    \label{fig:stacked_process}
\end{figure}


\section{Methods}
Due to the wide applications in real life, wave equations will be the main example in this section and the following section. In particular, we will look at  2d-wave equations $Lu = 0$ for $L$ in \eqref{2dwaveeq} with constant coefficients. As explained in Section \ref{epsection}, we use the Macaulay2 package \texttt{solvePDE} to see that the characteristic variety \eqref{2dwavecv} is irreducible and Noetherian multipliers are  $D_{i,j}(x,z) = 1$.
 From \eqref{epequation},
\begin{equation} \label{ep1}
    f(x) = \sum^{m}_{j=1}w_j e^{\langle x,z_j\rangle}
\end{equation}
where $w_j$'s are the weights and $z_j$'s are the frequency points in the characteristic variety given by \eqref{2dwavecv}.

\cite{harkonen2023gaussian} introduced the (S-)EPGP algorithm, which defines a GP with realizations of the form in \eqref{ep1}, where $z^j$'s are purely imaginary. Equation \eqref{ep1} gives an exact solution for any linear PDE with constant coefficients. In practice, we can approximate the realization $f(x)$ by $n$ data points $x_i$, $m$ frequency points $z^j=z_{1}^j+\sqrt{-1}z_{2}^j$ and weight $w^j = w_{1}^j + \sqrt{-1}w_{2}^j$ chosen of a standard Gaussian distribution, where $z_{1}^j, z_{2}^j, w_{1}^j,w_{2}^j \in \mathbb{R}$.
\begin{align} \label{appfx}
    f(x) &\approx \sum_{j = i}^m w^je^{x\cdot z^j}\\
    &= \sum_{j=1}^m \bigg[e^{z_{1}^j\cdot x}(w_{1}^j\cos(z_{2}^j\cdot x) - w_{2}^j\sin(z_{2}^j\cdot x)) \notag\\
    &\quad + \sqrt{-1}e^{z_{1}^j\cdot x}(w_{1}^j\cos(z_{2}^j\cdot x) + w_{2}^j\sin(z_{2}^j\cdot x))\bigg].\notag
\end{align} 

For wave equations, we only care about real-valued solutions. By taking the real part of \eqref{appfx}, the solution is approximated by
\begin{align} \label{predf}
    f(x) % &= \Re\{f(x)\} \notag\\
    &\approx \sum_{j=1}^m e^{z_{1}^j\cdot x}(w_{1}^j\cos(z_2^j\cdot x) - w_{2}^j\sin(z_2^j\cdot x)) \notag\\
    &= \phi(x)^T w
\end{align}
where $w$ is a matrix with entries of $w_{1}^je^{z_1^j\cdot x}$ and $w_{2}^je^{z_1^j\cdot x}$, and $\phi(x)$ is a matrix with entries of $\cos(z_2^j\cdot x)$ and $\sin(z_2^j\cdot x)$. Hence, for all the experiments, we do not directly have $e^{z_{1}^j\cdot x}$ in the formula and instead we will absorb it in the weight $w$.

Instead of using exponential basis $\{e^{x_i\cdot z^j}\}$, we use the basis elements $[\cos{(x_i\cdot z^j)}, \sin{(x_i\cdot z^j)}]$, where $x_i$'s are the data points, and $z^j$'s are the real frequency points in the characteristic variety. This reformulation eliminates the need for complex numbers in the algorithm, simplifying the computation process. By operating only with real-valued functions, the method enhances computational efficiency and reduces memory requirements, leading to faster execution.
% \textcolor{red}{check or rewrite:significantly simplifying computations.} By avoiding complex arithmetic, the algorithm achieves faster computation, requiring fewer compute units and less time, making it more efficient for large-scale problems.



\begin{table}[tbp]
\centering
\caption{Comparison of RMSE (square root of the average squared differences) and MAE (average of the absolute differences) between direct and inverse problems for one of true solutions $\cos(x - \sqrt{3}t) + \cos(y - \sqrt{3}t)$ for the wave equation $u_{tt}=a^2(u_{xx}+u_{yy})$. In the direct problem, $a=3$ is known; in the inverse problem, the correct value is learned. \text{n\_pts} is the number of data points; \text{n\_MC} is the number of frequency points. Both predictions are very accurate. Solving the inverse problem requires more resources and is less precise because we need to learn one more parameter; its approximation leads to small errors. % For direct problems, RMSE and MAE are in the range of $10^{-7}$ with 100 data points and 10 frequency points, indicating very small errors. In contrast, the inverse problems need 1000 data points and 100 frequency points to obtain the errors in range of $10^{-5}$. The larger errors despite more data points implies the inherent difficulty of inverse problems. The initial guess for $a$ is 1 in both experiments.
%\textcolor{red}{What do I learn form this? Is this a good result? A bad result? What do I expect here? Is this surprising?}
}
\label{cpdandi}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Direct Problem}} \\ \hline
\textbf{n\_pts} & \textbf{n\_MC} & \textbf{RMSE} & \textbf{MAE} \\ \hline
100 & 10 & $7.91\times 10^{-7}$ & $9.20\times 10^{-7}$ \\ \hline
\multicolumn{4}{|c|}{\textbf{Inverse Problem}} \\ \hline
\textbf{n\_pts} & \textbf{n\_MC} & \textbf{RMSE} & \textbf{MAE} \\ \hline
1000 & 100 & $5.63\times 10^{-5}$ & $7.65\times 10^{-5}$ \\ \hline
\end{tabular}
\end{table}



\begin{table*}[]
\caption{Error comparison for different true solutions and corresponding equations. Root Mean Square Error (RMSE) is particularly sensitive to large errors due to the squaring of residuals. Mean Absolute Error (MAE) measures the average magnitude of errors between predicted values and actual values, without considering their direction. The results demonstrate that all errors are very small, indicating accurate predictions. Furthermore, the parameter $a$, which is estimated through optimization, is closely approximating the true values $a^2=3,\,1.5,\,3$ respectively in each case, confirming the reliability of the proposed method.}
\label{tab:error_comparison}
\begin{tabular}{l|l|r|r|r|r|r}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{True Solution $u(x,y,t)$}} & \multicolumn{1}{c}{\multirow{2}{*}{Corresponding Equation}} & \multicolumn{2}{c}{Direct}                                  & \multicolumn{2}{c}{Inverse}                                 \\ \cline{3-4} \cline{5-7}
\multicolumn{1}{c}{}                                            & \multicolumn{1}{c}{}                                        & \multicolumn{1}{c}{RMSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{RMES} & \multicolumn{1}{c}{MAE}& \multicolumn{1}{c}{$a^2$} \\
\midrule
$\cos(x - \sqrt{3}t) + \cos(y - \sqrt{3}t)$                     & $u_{tt} = 3(u_{xx} + u_{yy})$                         & $3.067\times 10^{-8}$                            & $1.065\times 10^{-8}$                            & $5.632\times 10^{-5}$                           & $7.647\times10^{-5}$                         &3.0002 \\ 
$(x+y - \sqrt{3}t)^2$                                           & $u_{tt} = 1.5(u_{xx} + u_{yy})$                       & $3.459\times 10^{-4}$                            & $9.410\times 10^{-5}$                            & $3.006\times10^{-4}$                            & $1\times10^{-4}$                           &1.5018 \\
$\cos(3(x - \sqrt{3}t)) + \cos(6(y - \sqrt{3}t))$               & $u_{tt} = 3(u_{xx} + u_{yy})$                         & $2.483\times10^{-7}$                            & $2.099\times10^{-7}$                              & $3.508\times10^{-5}$                             &$1.744\times10^{-5}$   & 2.9999   \\                      
\bottomrule
\end{tabular}
\end{table*}

\begin{figure*}[h!] % 'figure*' spans across both columns
    \centering
    \begin{subfigure}[b]{0.24\textwidth} % 1/4 of text width
        \includegraphics[width=\textwidth]{figure/d3.png}
        \caption{}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figure/d36.png}
        \caption{}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figure/i3.png}
        \caption{}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figure/iy36.png}
        \caption{}
        \label{fig:image4}
    \end{subfigure}
    \caption{These four images show the difference between the prediction of our solution to the wave equation $u_{tt}=a^2(u_{xx}+u_{yy})$ and the true solution with unknown true wave speed $a = \sqrt{3}$. We  consider different true solutions $u(x,y,t)$. Fig. (\ref{fig:image1}) and Fig. (\ref{fig:image3}) concern the direct and inverse problem respectively with true solution $u(x,y,t) = \cos(x - \sqrt{3}t)+\cos(y - \sqrt{3}t)$. Fig.  (\ref{fig:image2}) and Fig. (\ref{fig:image4}) concern the  direct and inverse problem respectively with true solution $u(x,y,t) = \cos(3(x - \sqrt{3}t))+\cos(6(y - \sqrt{3}t))$. The errors in the all these cases are very small-- at most of the order of $10^{-4}$, indicating highly accurate predictions. 
    % For the last case, the error is larger but remains within a reasonable range, as the problem is more sensitive to initial conditions due to the higher frequency components in the true solution and the random process shown in Figure~\ref{fig:invhighfreq}.
    }
    \label{fig:four_images} 
\end{figure*}


\subsection{Loss Function}
To turn the approximated solution $\{f(x)\}$ into a GP, let $w \sim N(0, \frac{1}{m}\Sigma)$, where $\Sigma$ is a diagonal matrix with positive entries $\sigma_j^2$, $j = 1,...,m$. Then by Lemma~\ref{lemma-1}, the covariance function is of the form 
\begin{align*}
    k_{\text{EPGP}}(x,x') = \frac{1}{m}\phi(x)^T\Sigma \phi(x')
\end{align*}
where $\phi$ is the basis matrix evaluated at different data points $x$ and $x'$.
Suppose that $Y = f(\mathbf{X}) + \epsilon$, where $\epsilon \sim N(0, \sigma_0^2\mathbf{I})$, the marginal likelihood $P(Y|\mathbf{X})$ is obtained by integrating out $w$ from the joint distribution $P(Y|\mathbf{X},w)P(w)$. The negative log-marginal likelihood is%(NLML) can be calculated as
\begin{align} \label{nlml}
    \text{NLML} &= - \frac{1}{2\sigma_0^2}\left(Y^T Y - Y^T \phi^T A^{-1} \phi Y\right) \notag\\
    &\quad - \frac{n - m}{2} \log{\sigma_0^2} - \frac{1}{2} \log{|\Sigma|} - \frac{1}{2} \log{|A|},
\end{align} 
% where $C = (\frac{m}{2} + \frac{n}{2})\log(2\pi)+\frac{1}{2}\log(m)$. 
see \citep{harkonen2023gaussian}. NLML will serve as the loss function for the optimization process in this paper. Here, $Y_i$ is the value corresponding to our prediction  $\phi(x_i)^Tw$,
$\mathbf{X}$ is the matrix of all $n$ data points $x_i$, and 
\begin{align} \label{covmatrixA}
    A = \phi\phi^T + m\sigma_0^2\Sigma^{-1}.
\end{align}
The term $m\sigma_0^2\Sigma^{-1}$ captures the information from the prior distribution, and also makes $A$ positive definite. 
% More detail is shown in the Appendix~\ref{ANLML} with a more general case, i.e., more than one characteristic varieties, 
% \todo[inline]{there is only one characteristic variety. here you talk about decomposing it in $s$ irreducible varieties}
% $w \sim N(0, \frac{1}{ms}\Sigma)$, where $s$ is the number of characteristic varieties. Equation (\ref{nlml}) is the special case of $s=1$. 

During the training process, $\sigma_0^2$, $z_j$'s and $\Sigma$ are learned using a gradient descent-based optimization algorithm with loss function \eqref{nlml}. After the optimal $\hat{\sigma_0^2}$, $\hat{z_j}$'s and $\hat{\Sigma}$ are obtained, the weight $\hat{w}$ is calculated as 
\begin{align*}
    \hat{w} = \hat{A}^{-1}\hat{\phi}Y.
\end{align*}

The problem here is to efficiently calculate the inverse of the covariance matrix $A$. We use Cholesky decomposition $A = LL^T$ to find $A^{-1}$ and the prediction is given by
\begin{align*}
\phi_*^T\hat{w} = \phi_*^T A^{-1} \phi Y= \phi_*^T (L)^{-1} (L^T)^{-1} \phi Y, 
\end{align*}
where $\phi_*$ is the matrix with columns for test data, and $L$ is the lower triangular matrix such that $A^{-1} = (L)^{-1} (L^T)^{-1}$. 



\begin{figure}[b]
\begin{center}
\includegraphics[width=\columnwidth]{figure/highfreq.png}
\caption{Process of Inverse Problem with High Frequency. For high-frequency direct problems, where the wave speed $a$ is given, the process followed the same structure without parameter $a$ in Fig.~\ref{fig:stacked_process}. However, for high-frequency inverse problems, where wave speed $a$ is not given, addressing local minima is a significant challenge. To mitigate this, Instead of training $a$, $z^j$, $\sigma_0^2$ and $\Sigma$ simultaneously, we train the wave speed $a$ first. If the difference between squared $a$ and true wave speed is less than $10^{-6}$, then we proceed to optimize the remaining parameters ($z^j$, $\sigma_0^2$ and $\Sigma$). Otherwise, the process is restarted by randomly selecting new frequency points $z^j$'s to improve initialization.}
\label{fig:invhighfreq}
\end{center}
\end{figure}



\begin{figure*}[h] % 'figure*' spans across both columns
    \centering
    \begin{subfigure}[b]{0.23\textwidth} % 1/4 of text width
        \includegraphics[width=\textwidth]{figure/da3noise.png}
        \caption{}
        \label{fig:imagen1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figure/dy36noise.png}
        \caption{}
        \label{fig:imagen2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figure/ia3noise.png}
        \caption{}
        \label{fig:imagen3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figure/iy36noise.png}
        \caption{}
        \label{fig:imagen4}
    \end{subfigure}
    \caption{These four images show the difference between the prediction and the true solution to the wave equation $u_{tt}=a^2(u_{xx}+u_{yy})$ for unknown wave speed $a = \sqrt{3}$. As data, we considered true solution $u(x,y,t)$ to which we added Gaussian noise. Figure (\ref{fig:image1}) and (\ref{fig:image3}) concern the direct and inverse problem respectively with true solution $u(x,y,t) = \cos(x - \sqrt{3}t)+\cos(y - \sqrt{3}t)$. Figure (\ref{fig:image2}) and (\ref{fig:image4}) concern the direct and inverse problem respectively with true solution $u(x,y,t) = \cos(3(x - \sqrt{3}t))+\cos(6(y - \sqrt{3}t))$. Compared to the noise-free experiments, the errors here are relatively larger, of the order of $10^{-3}$ compared with $10^{-4}$. This was to be expected due to the presence of noise. The errors are sufficiently small to ensure accurate predictions and primarily reflect the added noise rather than inaccuracies in the prediction method.}
    \label{fig:four_images_noise}
\end{figure*}




\subsection{Direct Problem vs. Inverse Problem}
For the direct problem, i.e., given wave speed $a$, we are going to learn the parameters $\hat{\theta} = (\hat{\sigma_0^2}, \hat{z_j}, \hat{\Sigma})$ so that we can reconstruct the wave from the learned frequency. For the inverse problem, in addition to learning these parameters, we learn the wave speed $a$ from the sample points randomly selected from a true solution. The calculated covariance matrix $A$ in \eqref{covmatrixA} acts as a prior that constrains the solution to satisfy the PDEs. 

The results presented in Table~\ref{cpdandi} demonstrate that even the number of data points and frequency points is both increased by 10 times, the performance of the inverse problem remains worse than that of the direct problem. Nevertheless, for the inverse problem in Table~\ref{cpdandi}, the squared wave speed $a^2$ is correctly learned as $3.0002$, closely approximating the true value $a^2 = 3$. The direct problem is solved with remarkable speed, achieving an accuracy of $10^{-8}$ in just 30 seconds. In contrast, the inverse problem requires approximately 15 minutes to reach a similar level of accuracy. The increase in error is due to the error in approximating $a$.

Although the processes for solving the direct and inverse problems differ only by  one  parameter $a$, the results in Table~\ref{cpdandi} indicate challenges in achieving comparable levels of accuracy and computational efficiency for the inverse problem. Further details and specialized techniques for obtaining better results of the inverse problems are presented in the following section.




\begin{table*}[t]
\caption{Error comparison for different true solutions and corresponding equations with Gaussian noise. Compared to results of noise-free experiments, the errors here are relatively larger due to the presence of noise. The parameter $a$ is estimated correctly through optimization. These results indicate that the proposed approach exhibits noise tolerance and maintains high accuracy.}
\label{tab:error_comparison_noise}
\begin{tabular}{l|l|r|r|r|r|r}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{True Solution $u(x,y,t)$}} & \multicolumn{1}{c}{\multirow{2}{*}{Corresponding Equation}} & \multicolumn{2}{c}{Direct}                                  & \multicolumn{2}{c}{Inverse}                                 \\ \cline{3-4} \cline{5-7}
\multicolumn{1}{c}{}                                            & \multicolumn{1}{c}{}                                        & \multicolumn{1}{c}{RMSE} & \multicolumn{1}{c}{MAE} & \multicolumn{1}{c}{RMES} & \multicolumn{1}{c}{MAE}& \multicolumn{1}{c}{$a^2$} \\
\midrule
$\cos(x - \sqrt{3}t) + \cos(y - \sqrt{3}t)$                     & $u_{tt} = 3(u_{xx} + u_{yy})$                         & $9.868\times 10^{-4}$                            & $8\times 10^{-4}$                            & $9.812\times 10^{-4}$                           & $8\times10^{-4}$                         &2.99989 \\ 
$\cos(3(x - \sqrt{3}t)) + \cos(6(y - \sqrt{3}t))$               & $u_{tt} = 3(u_{xx} + u_{yy})$                         & $8.254\times10^{-4}$                            & $9\times10^{-4}$                              & $1.095\times10^{-3}$                             &$8\times10^{-4}$   & 2.99999   \\                      
\bottomrule
\end{tabular}
\end{table*}



\section{Example and Experiments}
In this section, we illustrate the algorithm by testing both direct and inverse problems for the 2$d$-wave equation $u_{tt} = a^2(u_{xx} + u_{yy})$ with different types of true solutions. The Noetherian multiplier of 2$d$-wave equation is 1. The training grid is set as $[-6,6]\times[-6,6]\times[0,12]$ for $x$, $y$ and time $t$ respectively. For the training process, we start by randomly choosing $n = 10,000$ data points $(x_i, y_i, t_i)$ from the grid and $m = 1000$ frequency points $(z^1_j,z_j^2)$ from a standard normal distribution. 

\subsection{2$d$-wave Equations with High Frequency Solutions}
For each data point $\mathbf{x} = (x_i,y_i,t_i)$, $i = 1,2,...,n$ and $m$ frequency points $\mathbf{z}^j = (z_1^j,z_2^j,z_3^j)$, assuming that the solution to the 2$d$-wave is of the form $e^{x_iz_1^j + y_iz_2^j+t_iz_3^j}$, the characteristic variety is given by 
\begin{align*} %\label{z3}
    ({z_3^j})^2 = a^2\left(({z_1^j})^2 + ({z_2^j})^2\right),\quad j = 1,...,m
\end{align*}
% In matrix form, 
% \[
% \mathbf{x} =
% \begin{bmatrix}
% x_1 & y_1 &  t_1 \\
% \vdots & \vdots & \vdots \\
% x_n & y_n &  t_n \\
% \end{bmatrix}_{n \times 3}
% \]
% \[
% \mathbf{z^j} =
% \begin{bmatrix}
% z_1^1 & z_2^1& a^2\left(({z_1^1})^2 + ({z_2^1})^2\right) \\
% \vdots & \vdots\\
% z_1^m & z_2^m&a^2\left(({z_1^m})^2 + ({z_2^m})^2\right) \\
% \end{bmatrix}_{m \times 3}
% \]
and basis matrix $\phi$ is a $4m \times n$ matrix with entries 
\begin{align*}
    \cos{\left(x_iz_1^m + y_iz_2^m \pm t_ia^2(({z_1^1})^2 + ({z_2^1})^2)\right)} \notag\\
    \sin{\left(x_iz_1^m + y_iz_2^m \pm t_ia^2(({z_1^1})^2 - ({z_2^1})^2)\right)}.
\end{align*}

% \[
% \mathbf{\Phi} =
% \begin{bmatrix}
% \cos{\left(x_iz_1^m + y_iz_2^m + t_ia^2(({z_1^1})^2 + ({z_2^1})^2)\right)} \\
% \vdots\\
% \cos{\left(x_iz_1^m + y_iz_2^m - t_ia^2(({z_1^1})^2 - ({z_2^1})^2)\right)}\\
% \vdots\\
% \sin{\left(x_iz_1^m + y_iz_2^m + t_ia^2(({z_1^1})^2 + ({z_2^1})^2)\right)}\\
% \vdots\\
% \sin{\left(x_iz_1^m + y_iz_2^m - t_ia^2(({z_1^1})^2 - ({z_2^1})^2)\right)}\\
% \vdots\\
% \end{bmatrix}_{4m \times n}
% \]



For low-frequency solutions, characterized by fewer oscillations, the optimization process adheres precisely to the steps outlined in Figure~\ref{fig:stacked_process}. In case of high-frequency solutions, the procedure for solving direct problems remains consistent with Figure~\ref{fig:stacked_process}. However, it is not sufficient for the inverse problem because the wave speed $a$ is trapped in an incorrect value during the optimization process. This situation indicates the tendency of inverse problems converging to local minima, thereby affecting the accuracy.

To address this, we adjusted our training approach. Instead of simultaneously optimizing $a$, $z_j$' s, $\Sigma$, and $\sigma_0^2$, we adopted a sequential strategy. Initially, we randomly selected $z_j$' s, then only train $a$ until it closely approximated the true value. Once $a$ is closely estimated to the true value, we proceeded to train $z_j$'s, $\Sigma$ and $\sigma_0^2$. The detailed process is shown in Figure~\ref{fig:invhighfreq}. 
% \todo{This should be described much earlier as our approach. The we have good results with our approach in all examples. And we can make an ablation that choosing bad frequencies does not yield good results by the more naive training approach.}. 



\subsection{Experiments with Different True Solutions}
For one of the true low-frequency solutions 
\begin{align}\label{truea3}
    u(x,y,t) = \cos(x - \sqrt{3}t)+\cos(y - \sqrt{3}t),
\end{align}
which corresponds to a wave speed $a = \sqrt{3}$, highly accurate results are obtained, as presented in Table~\ref{tab:error_comparison}, for both direct and inverse problems, starting from $a = 1$. The root mean square error (RMSE) and mean absolute error (MAE) are on the order $10^{-8}$ for direct problems and of the order $10^{-5}$ for inverse problems. These low errors demonstrate that the predicted solution closely approximates the true solution \eqref{truea3}, highlighting the effectiveness of the approach in capturing wave propagation.

We also explored for a high-frequency solution 
\begin{align}\label{truey36}
    u(x,y,t) = \cos(3(x - \sqrt{3}t))+\cos(6(y - \sqrt{(3}t)).
\end{align}
The direct problem is effectively solved, achieving both RMSE and MAE on the order of $10^{-7}$, as shown in Table~\ref{tab:error_comparison}. A satisfactory result for the inverse problem is obtained when the squared wave speed is initialized at $a^2 = 2$. The results presented in Table~\ref{tab:error_comparison} are derived based on the procedure shown in Figure~\ref{fig:invhighfreq}. As observed in Table~\ref{tab:error_comparison}, the errors for direct problems are approximately on the order $10^{-7}$, whereas for inverse problems, the errors are around $10^{-4}$. It is expected that the inverse problem exhibits larger errors due to one more unknown parameter and sensitivity to initial conditions. Despite this, the method demonstrates robust performance in reconstructing the wave propagation and learning the correct wave speed. 

Even after adding Gaussian noise to the true solutions \eqref{truea3} and \eqref{truey36},  the results remain accurate, as shown in Table~\ref{tab:error_comparison_noise} and Figure \ref{fig:four_images_noise}. Compared to experiments without noise, the difference between the prediction and true solutions increases significantly, approximately 10,000 times for direct problems and 10 times for inverse problems. However, these discrepancies are still small relative to the high accuracy of the method, since the performance in the noiseless experiments was exceptionally precise, underscoring the robustness of the approach even in the presence of noise.

We also tried
\begin{align}
    u(x,y,t) = (x + y - \sqrt{3}t)^2
\end{align}
which is the solution for the wave speed $a = \sqrt{1.5}$. Results are also shown in Table~\ref{tab:error_comparison} with 1000 data points and 100 frequency points. Given any 2$d$-wave equation, normally we need two initial conditions: $u(x,y,0)$ and $\frac{\partial }{\partial t}u(x,y,0)$ together with one boundary condition, i.e., Dirichlet boundary condition $u(x,y,t) = h(x,y,t)$ on $\partial \Omega$. For our algorithm, a single initial condition, $u(x,y,0)$, is sufficient. Selecting data points additionally at $t=1$ enables us to find the unique solution to direct problems.






\section{Conclusion}
This paper presents a computationally efficient approach for solving inverse problems governed by linear partial differential equations (PDEs) using Gaussian Process (GP) regression. By using algebraically informed priors derived from commutative algebra and algebraic analysis, the proposed method ensures both accuracy and interpretability while maintaining computational efficiency. The implementation of priors using the computer algebra software Macaulay2 enables effective handling of complex relationships in PDE solutions.

Through extensive experiments on 2$d$ wave equations, the method demonstrates robust performance in learning wave speed parameters and accurately reconstructing wave propagation even in the presence of  noise. The results show that while the direct problem achieves high precision with minimal computational cost, the proposed approach effectively mitigates the challenges for the inverse problems, i.e., sensitivity to initial conditions and presence of local minima, achieving satisfactory accuracy. 

For future work, one direction involves the application of the proposed method to solve vectorial wave equations.  Practical examples of interest include applications to Maxwell's equations in electromagnetism and the seismic inversion problem; see \cite{symes2009seismic}. Future studies could focus on adapting the GP framework to handle large-scale seismic datasets and improve subsurface structure estimation. 

\bibliography{ref}             
%\begin{thebibliography}{xx}  % you can also add the bibliography by hand

%\bibitem[Able(1956)]{Abl:56}
%B.C. Able.
%\newblock Nucleic acid content of microscope.
%\newblock \emph{Nature}, 135:\penalty0 7--9, 1956.

%\bibitem[Able et~al.(1954)Able, Tagg, and Rush]{AbTaRu:54}
%B.C. Able, R.A. Tagg, and M.~Rush.
%\newblock Enzyme-catalyzed cellular transanimations.
%\newblock In A.F. Round, editor, \emph{Advances in Enzymology}, volume~2, pages
%  125--247. Academic Press, New York, 3rd edition, 1954.

%\bibitem[Keohane(1958)]{Keo:58}
%R.~Keohane.
%\newblock \emph{Power and Interdependence: World Politics in Transitions}.
%\newblock Little, Brown \& Co., Boston, 1958.

%\bibitem[Powers(1985)]{Pow:85}
%T.~Powers.
%\newblock Is there a way out?
%\newblock \emph{Harpers}, pages 35--47, June 1985.

%\bibitem[Soukhanov(1992)]{Heritage:92}
%A.~H. Soukhanov, editor.
%\newblock \emph{{The American Heritage. Dictionary of the American Language}}.
%\newblock Houghton Mifflin Company, 1992.

%\end{thebibliography}

% \appendix
% \section{NLML} \label{ANLML}    % Each appendix must have a short title.
% In this section, we derive the negative log-marginal likelihood (NLML) in (\ref{nlml}).

% Given $f(x) = w^T\phi(x)$, $w \sim N(0, \frac{1}{ms}\Sigma)$, and given $n$ noisy observation $Y = f(x) + \epsilon$, $\epsilon \sim N(0, \sigma_0^2I)$, the marginal likelihood $P(Y|X)$ is obtained by integrating out $w$ from the joint distribution $P(Y|X,w)P(w)$. % $w^T$ is a $1\times mr$ matrix \textcolor{blue}{x is a single point, X is a set of x} 

% Since 
% \begin{align}
%     P(Y|X,w) &= \prod_{i=1}^nP(Y_i|X_i,w) \notag\\
%     &= \frac{1}{(2\pi \sigma_0^2)^{n/2}}\exp{(\frac{-1}{2\sigma_0^2}(Y - \Phi^T w)^T(Y - \Phi^T w))}
% \end{align}
% and
% \begin{align}
%     P(w) = \frac{1}{(2\pi)^{ms/2}|\frac{1}{ms}\Sigma|^{1/2}}\exp{(\frac{-ms}{2}w^T\Sigma^{-1}w)}
% \end{align}
% then the joint distribution is 
% \begin{align} \label{joint}
%     P(Y|X,w)P(w) &\propto \exp{(\frac{-1}{2\sigma_0^2}(Y - \Phi^T w)^T(Y - \Phi^T w))}\notag\\
%     % &\quad \exp{(\frac{-mr}{2}w^T\Sigma^{-1}w)}\notag\\
%     % &= \exp{(\frac{-1}{2\sigma_0^2}Y^TY)} \notag\\
%     % &\quad \exp{\frac{1}{\sigma_0^2}Y^T\Phi^Tw - \frac{1}{2\sigma_0^2}w^T(\Phi\Phi^T + \sigma_0^2mr\Sigma^{-1})w)} \notag\\
%     &= \exp{(\frac{-1}{2\sigma_0^2}Y^TY + \frac{1}{\sigma_0^2}Y^T\Phi^Tw - \frac{1}{2\sigma_0^2}w^TAw)}
% \end{align}
% where $A = \Phi\Phi^T + \sigma_0^2ms\Sigma^{-1})w$.

% Since $Y^TY$ does not depend on $w$, we rewrite the term $\frac{1}{\sigma_0^2}Y^T\Phi^Tw - \frac{1}{2\sigma_0^2}w^TAw)$ into the form $\frac{-1}{2\sigma_0^2}(w - \mu)^TA(w - \mu)$.

% \begin{align}
%     \frac{1}{\sigma_0^2}Y^T\Phi^Tw - \frac{1}{2\sigma_0^2}w^TAw) &= \frac{-1}{2\sigma_0^2}(w^TAw - 2w^TAA^{-1}\Phi Y) \notag\\
%     &= \frac{-1}{2\sigma_0^2}(w - \mu)^TA(w - \mu)\notag\\
%     &\quad +\frac{1}{2\sigma_0^2}Y^T\Phi^TA^{-1}\Phi Y
% \end{align}
% where $\mu = A^{-1}\Phi Y$

% Hence, (\ref{joint}) becomes
% \begin{align}
%     P(Y|X,w)P(w) &\propto \exp{(\frac{-1}{2\sigma_0^2}Y^TY +\frac{1}{2\sigma_0^2}Y^T\Phi^TA^{-1}\Phi Y}\notag\\
%     &\quad + \exp{\frac{-1}{2\sigma_0^2}(w - \mu)^TA(w - \mu)}
% \end{align}

% Since $w$ only appears in the term $\exp{\frac{-1}{2\sigma_0^2}(w - \mu)^TA(w - \mu)}$, which is a Gaussian distribution with mean $\mu$ and covariance matrix $A^{-1}$, the integration of this term is the normalization factor $\frac{1}{\sqrt{|A|}}$.

% Hence, the marginal likelihood $P(Y|X)$ is
% \begin{align}
%     P(Y|X) &= \int P(Y|X,w)P(w)dw \notag\\
%     &= \frac{1}{\sqrt{|A|}}\exp{(\frac{-1}{2\sigma_0^2}Y^TY +\frac{1}{2\sigma_0^2}Y^T\Phi^TA^{-1}\Phi Y)}
% \end{align}
% and  
% \begin{align}
%     \log{P(Y|X)} = \frac{-1}{2}\log{|A|} - \frac{-1}{2\sigma_0^2}Y^TY +\frac{1}{2\sigma_0^2}Y^T\Phi^TA^{-1}\Phi Y
% \end{align}
% Since coefficients are omitted before, they are include to obtain the final form on NLML
% \begin{align}
%     \log{\frac{1}{(2\pi\sigma_0^2)^{n/2}}} &= \frac{-n}{2}\log{2\pi} - \frac{n}{2}\log{\sigma_0^2}
% \end{align}
% \begin{align}
%     \log{\frac{1}{(2\pi)^{ms/2}|\frac{1}{ms}\Sigma|^{1/2}}} &= \frac{-ms}{2}\log{2\pi} -\frac{1}{2}\log{|\Sigma|} + \frac{1}{2}\log{ms}
% \end{align}
% Hence, the NLML is calculated as 
% \begin{align}
%     \text{NLML} &= - \frac{1}{2\sigma_0^2}\left(Y^T Y - Y^T \Phi^T A^{-1} \Phi Y\right) \notag\\
%     &\quad - \frac{n - m}{2} \log{\sigma_0^2} - \frac{1}{2} \log{|\Sigma|} \notag\\
%     &\quad - \frac{1}{2} \log{|A|} + C
% \end{align}
% where $C = (\frac{ms}{2} + \frac{n}{2})\log(2\pi)+\frac{1}{2}\log(ms)$ 

% \section{(MAYBE) Frequency Plots}              % Sections and subsections are supported  
                                                                         % in the appendices.
\end{document}
