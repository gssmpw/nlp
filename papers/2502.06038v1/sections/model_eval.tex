\section{Evaluating on a Single Layer Model}
\label{sec:model_eval}

In this section, we empirically demonstrate our algorithm on a single layer transformer model trained for next token prediction on a standard text corpus.
Specifically, we run \cref{alg:overwhelmCheck2} to calculate a bound on the worst-case deviation and peak-to-peak difference for the model where the domain has an input restriction with free tokens drawn from a single permutation class as in \cref{sec:perm_invar}. When \cref{alg:overwhelmCheck2} outputs $\OverwQ$ it follows from \cref{thm:InpResPermInv} (informally, \cref{thm:informal_overwhelm_perm} in the Introduction) that the output of the model is the same for all permutations.

\subsection{Experimental Setup}
We train a single-layer transformer model with an embedding dimension of 512 and the BERT tokenizer \cite{devlin-etal-2019-bert}.
The model's architecture is outlined in \cref{sec:model}.
We train on the AG News dataset \footnote{see \href{https://pytorch.org/text/stable/_modules/torchtext/datasets/ag_news.html}{PyTorch's documentation}}, using the training split with a batch size of 8, learning rate of 5e-5 using the Adam optimizer, and $20$ epochs.

We examine a few different types of input restrictions
\footnote{
    The first token is always fixed to the ``BOS'' token which connotes the beginning of a string in the BERT tokenizer. For simplicity, we think of the BOS token as part of the fixed tokens.
}:
\begin{itemize}[nosep]
	\item \textbf{Random String:} Randomly sample alpha-numeric tokens (including space and punctuation) to form $\desSet$
	\item \textbf{Random Sentences:} Sample (and concatenate) sentences from the AG News testing set, to form $\desSet$
	\item \textbf{Repeating Tokens:} Form $\desSet$ by repeating the string ``what is it'' (which is $3$ tokens in the BERT tokenizer).
\end{itemize}

In each case, we have a \emph{tunable} number of tokens in $\desSet$\footnote{Larger input restrictions are produced by concatenation.}.
For the permutation class, we consider the following two cases:

\begin{itemize}[nosep]
	\item \textbf{Hamlet}: We use the famous quote from Shakespeare's \emph{Hamlet}:
    % \begin{quote}
    ``To be or not to be, that is the question. Whether it is a nobler in the mind to suffer the slings and arrows''
        % \end{quote}

	\item \textbf{The Old Man and the Sea}: We fix the free tokens to be a snippet from the opening chapter of Hemingway's \emph{The Old Man and the Sea}:
		% \begin{quote}
		 ``The blotches ran well down the sides of his face and his hands had the deep-creased scars from handling heavy fish on the cords.''	
		% \end{quote}
\end{itemize}

Finally, the question mark token ``?'' is used as the query token.

In \cref{fig:smallex}, we show our algorithm in action for the ``Hamlet'' permutation class and repeated fixed string.
In \cref{sec:appOut}, we plot the worst-case deviation in \cref{fig:worst-case} and the peak-to-peak difference in \cref{fig:ptp} for each of the input restrictions and permutation classes.

\begin{figure}
    \centering
\includegraphics[width=\linewidth,keepaspectratio,clip]{figs/plots/w_analysis_single.pdf}
    \caption{
        Worst-case deviation for the ``Hamlet'' permutation string and fixed repeated string.
        The $x$-axis is the number of total tokens and the $y$-axis is the log-scale worst-case deviation.
        We mark cases of provable overwhelming with a red ``x.''
    }
    \label{fig:smallex}
\end{figure}

\subsection*{Observations}
The experimental results for worst-case deviation bounds in \cref{fig:worst-case} and peak-to-peak difference in \cref{fig:ptp} provide interesting insights and questions.
The upper-bound on worst-case deviation trend downwards as the fixed input token size is increased.
But only the ``repeated-string'' setting is monotonically decreasing.
Moreover, all plots exhibit a ``phase transition:'' at around 180 tokens and again around 350 tokens, the upper-bound on worst-case deviation has a sharp drop.
We note that the linear program seems to be the most useful for the ``AG News'' dataset when paired with the \emph{The Old Man and the Sea} free string.
Finally, we note that in both plots, ``not much'' seems to happen prior to 180 tokens.
Specifically, both the peak-to-peak difference and bound on worst-case deviation remain flat for the smaller fixed string.

\subsection{Overwhelming Continues Through Generation}
When a transformer model is \overwQ the generated token immediately after the text is fixed for any free string. However, this does not apriori imply that subsequent tokens when the model is used to repeatedly generate tokens will be fixed. 

To test this we run examples of text generation where we begin with an overwhelming string $s$ and use the model to generate text. We then check if the model remains \overwQ as the generated text is included as part of the fixed string. The results of these tests are plotted in \cref{fig:contOverwhelm} within \cref{sec:appOut}. One would hope the model remains overwhelmed as the fixed string is increased by the addition of the newly generated tokens. This occurs for the Hamlet free string with the repeated and random fixed strings, where as expected the worst-case deviation remains mostly constant throughout token generation. For the longer free string and the AG News fixed strings the worst-case deviation peaks after a few tokens and the model stops to be provably \overwQ.
