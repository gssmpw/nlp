\section{Conclusion and Further Work}
\label{sec:conclusion}
In this work, we introduce the notion of ``overwhelming" transformer models.
We provide algorithms to provably check if a trained one-layer transformer is overwhelmed by some fixed input string $s$ and query token $q$.
% To do this we provide a metric called the $\emph{worst-case deviation}$ to lower-bound the degree of ``over-squashing".
% The bulk of the theoretical work involves bounding this metric throughout the layers of a transformer architecture.
We then empirically run the algorithm on a concrete single-layer model.
We obtain bounds and find examples of natural overwhelming strings for this model in the restricted permutation setting.

This work is a first step in building algorithms to give provable guarantees for practical models.
Improving our existing bounds and extending the algorithms to multi-layer transformers are important problems in this direction.
Moreover, we believe that our approach to proving overwhelming, the use of LP bounds, and worst-case deviation metric will be independently useful for other theoretical studies of transformer based models.

\subsection*{Acknowledgment}
The authors are grateful for helpful discussions and feedback from Jason Gross and Peter Bajcsy.
LS acknowledges funding from the NSF Graduate Research Fellowship.
