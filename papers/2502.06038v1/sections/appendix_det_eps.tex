\section{Overwhelming for $\nctx \to \infty$}
\label{sec:convergence}

In this section, we will consider a specific set $\desSet = \bigtimes_{\nfix} \vecRep$ and $\query = \vecRep$ for some fixed $r \in [\dVocab]$.
In words, we will consider the fixed input to be one repeated token.
We term this a ``repetition restriction.''

Moreover, it will be useful to define the set of all possible inputs under a repetition restriction.
\begin{definition}[Repetition Space]
	\label{def:RepSpace}
	Let $\RepSpace \subset \OneHotSpace^\nctx$ be the set of all possible inputs under a repetition restriction.
	That is, 
	\[
		\RepSpace = \left\{ X \in \R^{\nctx \times \dVocab} \mid X = 
		\begin{bmatrix}
			\vecRep^T \\
			\vecRep^T \\
			\vdots \\
			\vecRep^T \\
			Y \\
			\vecQuery
		\end{bmatrix}, Y \in \OneHotSpace^{\nfree}
		\right\}.
	\]
\end{definition}

For simplicity, we will also not consider positional encodings in this section. 
I.e.\ we remove the use of RoPE in the attention mechanism.
Though as Ref.~\cite{barbero2024transformers} pointed out, rotary positional encodings \cite{su2024roformer} converge to providing zero information as $\nctx \to \infty$.

\begin{theorem}[Asymptotic Convergence to a Fixed Model]
	\label{thm:convergence}
	If $\frac{\nfree}{\nctx} \in o(1)$ as a function of $\nctx$, then the repetition restriction converges to a fixed model if $\peakToPeak(\Model, X)$ is positive for $X = \vece_r$.
\end{theorem}

To prove this theorem, we will need to (a) find a way to compute a $\peakToPeak$ function of the model and input \emph{independent of} $\nctx$ and (b) use the framework of \cref{sec:meta_framework} to individually bound the worst-case deviation for each component of the model.

\subsection*{Computing Peak-to-Peak Difference}
Given that $\peakToPeak$ is computed by evaluating the model on a single input, finding a $\peakToPeak$ value is normally a simple task.
But, in the asymptotic case, we need to ``shortcut'' the computation of $\peakToPeak$ for an $X \in \RepSpace$.
We can simply do this by setting all the free tokens to the repetition token: $X = {\vecRep}^{\nfree}$.

\begin{lemma}[Shortcut for $\peakToPeak$]
	\label{lem:gap_shortcut}
	We can compute $\peakToPeak(\Model, X)$ in time $O(1)$ for $X=\vece_\rep^{\nctx}$ even as $\nctx \rightarrow \infty$.
\end{lemma}
\begin{proof}
	We can note that $\E[X] = \vecDes \cdot \Embed$ and $\Var[X] = \Var[\vecDes \cdot \Embed] = 0$.
	So, we can easily compute blowup, $B$, and shift, $S$.
        Note, for attention head $\vece_\nctx \cdot \desF{f^\attnH}$, the output equals 
        \[
            \vec{p}(X) \cdot (XE \cdot \diag(B) + S) \cdot V
        \]
        for some probability vector $\vec{p}(X)$.
        Because $X = \vecRep^\nctx$, $(XE \cdot \diag(B) + S) \cdot V= \vec{a}^\nctx$ for vector $\vec{a} = (\vecRep \cdot E \cdot \diag(B) + S) V$.
        Thus,
        $$
            \vec{p}(X) \cdot (XE \cdot \diag(B) + S) \cdot V = \vec{a}.
        $$
	We therefore get that the output of $\vece_\nctx \cdot \desF{f^\attnH}$ is fixed to $\vecRep (\Embed \cdot \diag(B) + S) \cdot V$ for all context windows $\nctx$.
	Finally, we can simply compute the value of $\vece_\nctx \cdot \desF{f^\MLP}$ because we know $B$ and $S$ and so just need to compute $\MLP(\vec{e}_r (E \cdot \diag(B) + S))$.
\end{proof}

\subsection*{Bounds on Expectation and Variance}
\begin{lemma}[Bounds on Expectation and Variance]
	\label{lem:bounds}
	Let $X' \in \RepSpace$ and $X = \left(\bigtimes_{i = 1}^{\ndes} \vecDes \right) \times X' \times \vecQuery$. I.e.\ $X$ is an expansion of the input with the first tokens being the repetition token and the last being the query token.
	Then, we can bound the column wise expectation for column $j$, $\mu_j$, and variance, $\Var_j$ of $X$ by
	\begin{align*}
		\frac{1}{\nctx}\left(
			\ndes \cdot \vecDes \Embed \vec{e}_j^T + \nfree \min_i (\vec{e}_i \Embed \vec{e}_j^T) + \vecQuery \Embed \vec{e}_j^T
		\right)
		\leq \mu_j \leq  \\
		\frac{1}{\nctx}\left(
			\ndes \cdot \vecDes \Embed \vec{e}_j^T + \nfree \max_i (\vec{e}_i \Embed \vec{e}_j^T) + \vecQuery \Embed \vec{e}_j^T
		\right).
	\end{align*}
	Let $\mu_j^{\min}$ and $\mu_j^{\max}$ denote the lower and upper bounds.
	Then, we can bound the variance of the $j$-th column of $\vec{x} \Embed$ by
	\begin{align*}
            \Var_j 
			\leq &\frac{1}{\nctx} \cdot \max_{\mu' \in [\mu_j^{\min}, \mu_j^{\max}]} \bigg[
			 (\ndes +1) \cdot  \left( \vecDes \Embed \vec{e}_j^T -  \mu' \right)^2 + 
			 \nfree \cdot \max_{i \in \dVocab} \left( \vec{e}_i \Embed \vec{e}_j^T -  \mu' \right)^2
			\bigg].
	\end{align*}
	We define $\Var_j^{\min}, \Var_j^{\max}$ to be the lower and upper bounds respectively.
\end{lemma}
\begin{proof}
	The bounds on $\mu_j$ follow as we minimize and maximize the contribution of each free row to the expectation of the column of $j$.
	The proof of for variance bounds follows similarly.
	For the lower bound, we note that the variance contribution from each row is at least $0$.
	For the upper bound, we simply maximize the contribution of each row to the variance. 
\end{proof}

Recall the definition of blowup and shift sets from \cref{def:blowup_shift}.
\begin{lemma}[Blowup and Shift Set Bounds]
	\label{lem:blowup_shift}
	Recall that $\gamma$ and $\beta$ are the learned constants for layer normalization.
	Let $\vec{x} \in \InpSpace$.
	Then, we can bound the blowup and shift sets for the $j$-th column of $\vec{x} \Embed$.
	For all $S_j \in \shiftSet_j$ and $B_j \in \blowupSet_j$, we have
	\begin{align*}
		\frac{\gamma}{\sqrt{\Var_j^{\max} + \eps}} \leq B_j \leq \frac{\gamma}{\sqrt{\eps}}
	\end{align*}
	Let $B_j^{\min}$ and $B_j^{\max}$ denote the lower and upper bounds respectively.
	Then,
	\begin{align*}
		B_j^{\min} \mu_j^{\min} + \beta \leq S_j \leq B_j^{\max} \mu_j^{\max} + \beta.
	\end{align*}
\end{lemma}

Now as a corollary of \cref{lem:bounds} and \cref{lem:blowup_shift}, we can show that the minimum and maximums of the blowup and shift sets can be made arbitrarily close together.
\begin{corollary}[Convergence of Blowup and Shift Sets]
	If $\frac{\nfree}{\nctx} \in o(1)$, then for every $\delta_1, \delta_2 \in (0, 1)$, there exists some setting of $\nctx'$ such that for all $\nctx > \nctx'$,
	\[
		B_j^{\max} - B_j^{\min} \leq \delta_1 \quad \text{and} \quad S_j^{\max} - S_j^{\min} \leq \delta_2.
	\]
\end{corollary}
\begin{proof}[Proof]
    The corollary follows from the fact that $\mu_j^{\min}, \mu_j^{\max}$ converge to $\vec{e}_r E \vec{e}_j^T$ as $\nctx \rightarrow \infty$.
    So then, $(\vec{e}_r E \vec{e}_j^T - \mu')^2$ for all $\mu' \in [\mu_j^{\min}, \mu_j^{\max}]$ converges to $0$ and thus the upper bound on $\Var_j$ converges to $0$ as long as $\frac{\nfree}{\nctx}$ converges to $0$.
\end{proof}

Therefore the blowup and shift sets converge, let $B_j'$ and $S_j'$ denote the converged values for $B_j$ and $S_j$ respectively.

Now, we need to show a bound on the difference between the maximum logit for the free tokens and the minimum logit for the repeated tokens in the attention head.

\subsubsection*{Bound on Attention Head Difference}
To bound attention, we will take advantage of the repeated structure as well as the ideas in \cref{lem:min_max_softmax} (bounds on the attention weights).

\begin{corollary}[Sum of Attention Weights]
	\label{cor:sum_attention}
	Let $\logRMin \leq \min_{i \in [\ndes] \cup \{n_ctx\}} \ell_i$ and $\logFMin \leq \min_{j \in (\nfix, \nctx)} \ell_j$.
	Also, let $\logRMax \geq \max_{i \in [\ndes] \cup \{n_ctx\}} \ell_i$ and $\logFMax \geq \max_{i \in (\nfix, \nctx)} \ell_i$. Then,
	\begin{align*}
	\frac{\ndes}{\ndes + 1 + (\nfree) \cdot e^{\logFMax - \logRMin}} 
\leq 
		\sum_{i \in [\ndes] \cup \{\nctx\}} \softmax(\vec{\ell}_i)
	\leq
		\frac{\ndes}{\ndes + 1 + (\nfree) \cdot e^{\logFMin - \logRMax}}
	\end{align*}
	and 
	\begin{align*}
		\frac{\nfree}{\ndes + 1 + (\nfree) \cdot e^{\logFMin - \logRMax}} 
        \leq \sum_{j \in (\nfix, \nctx)} \softmax(\vec{\ell}_j)
        \leq \frac{\nfree}{\ndes + 1 + (\nfree) \cdot e^{\logFMax - \logRMin}}.
	\end{align*}
\end{corollary}

\begin{lemma}[Bound on Difference]
	\label{lem:bound_diff}
	We can bound the difference between the maximum logit for the free tokens and the minimum logit for $\ndes$ repeated tokens
	\begin{align}
		\logFMax - \logRMin &\leq \theta
	\end{align}
	and
	\begin{align}
		\logFMin - \logRMax &\leq \theta
	\end{align}
	where $\theta$ is a small constant.
\end{lemma}
\begin{proof}
	We will prove the first inequality as the second follows analogously.
	Assume that $i \in [\dVocab]$ is the token which maximizes the difference.
	We will relly on the convergence of the blowup and shift sets to show that the difference between the maximum logit for the free tokens and the minimum logit for the repeated tokens in the attention head is bounded.
	
	Connote $\blowupSet - B'$ as the set of blowup values shifted by the converged value and $\shiftSet - S'$ as the set of shift values shifted by the converged value.
	Then $\blowupSet - B'$ and $\shiftSet - S'$ are bounded by $\delta_1, \delta_2$ respectively, we can write the following bound on the difference.
	\begin{align*}
		\logFMax - \logRMin &\leq \max_{\eps \in \blowupSet - B', \; \eps_S \in \shiftSet - S'} (\vec{e}_i - \vecDes) E \cdot \diag(B' + \eps) \\ & \cdot Q K^T (\diag(B' + \eps) \cdot E^T \vecQuery^T + S^T + \eps_S) \\
					   &= \max_{\eps \in \blowupSet - B', \eps_S \in \shiftSet - S'} \\& \sum_{d_1 \in \{\eps, B'\}} \sum_{d_2 \in \{\eps, B'\}} \sum_{d_3 \in \{\eps, S'\}} (\vec{e}_i - \vecDes) E \cdot \diag(d_1) \cdot Q K^T (\diag(d_2) \cdot E^T \vecQuery^T + d_3) \\
					   &\leq \max_{\eps \in \blowupSet - B', \eps_S \in \shiftSet - S'} (\vec{e}_i - \vecDes) E \cdot \diag(B') \cdot Q K^T (\diag(B') \cdot E^T \vecQuery^T + S^T) + \poly(\eps, \eps_S)\\
					   &\leq (\vec{e}_i - \vecDes) E \cdot \diag(B')  \cdot Q K^T (\diag(B') \cdot E^T \vecQuery^T + S^T) + \poly(\delta_1, \delta_2, B', S')
                       \tag{By the bounds on the blowup and shift set}
                       \\
					   &\leq (\vec{e}_i - \vecDes) E \cdot \diag(B')  \cdot Q K^T (\diag(B') \cdot E^T \vecQuery^T + S^T) + \poly(\delta_1, \delta_2)
	\end{align*}
	where the last inequality follows from considering $B'$ and $S'$ as a constant.
	Then, because $\delta_1, \delta_2$ decrease as a function of $\nctx$ and \cref{cor:sum_attention},
	we can bound
	\begin{align*}
		&(\vec{e}_i - \vecDes) E \cdot \diag(B') 
        \cdot 
        Q K^T (\diag(B') \cdot E^T \vecQuery^T + S^T) + \poly(\delta_1, \delta_2) \leq \theta
	\end{align*}
	for a large enough $\nctx$.
\end{proof}


\begin{lemma}[Attention Bound, \cref{lem:att_bound}]
	\label{lem:convgattn}
	For large enough $\nctx$, we get
	\[
		\WD(\desF{f^\attnH}; \RepSpace)_\infty \leq o(1).
	\]
\end{lemma}
\begin{proof}
	\label{proof:convgattn}
	Recall that \cref{lem:att_bound} gives us
	\begin{align*}
		&\WD(\vecNctx \cdot f^{\attnH}_{\ndes \mid r, q} ; \InpSpace \times \blowupSet \shiftSet)_\infty  
						 \leq					   (\ssMaxRB - \ssMinRB) \cdot \max_{B, S}\norm{\vecDes \Embed B + S} + 
											   2 \cdot \ssMaxFB \cdot \max_{B, S}\norm{(\vecDes \Embed B + S) \cdot V}_\infty
	\end{align*}
	Because $\logFMax - \logRMin \leq \theta$ and $\logFMin - \logRMax \leq \theta$,
	we can use \cref{cor:sum_attention} to upper-bound $\ssMaxRB - \ssMinRB$ and $\ssMaxFB$ by
	\begin{align*}
		1 - \frac{\ndes}{\ndes + (\nfree + 1) \cdot \theta}
		= \frac{(\nfree + 1) \cdot \theta}{\ndes + (\nfree + 1) \cdot \theta} 
		\leq \frac{\nfree \cdot \theta}{\ndes} 
		\leq \theta \cdot o(1).
	\end{align*}
	So then
	\[
		\WD(\desF{f^{\attnH}} ; \InpSpace \times \blowupSet \shiftSet)_\infty \leq 
		o(1) 
	\]
	as $\theta \in o(1)$.
\end{proof}

\subsubsection*{Bound on $\fenc$}
Finally, as per \cref{sec:meta_framework}, we need to bound the worst-case deviation of the $\fenc$ function.
Given that $\fenc$ is a linear operation as a function of blowup and shift sets, we can use the convergence of the blowup and shift sets to show that the worst-case deviation of $\fenc$ is bounded.

\begin{lemma}[Bound on $\fenc$]
	\label{lem:conv_fenc_bound}
	For large enough $\nctx$, we have
	\[
		\WD(\vec{e}_\nctx \cdot \desF{\fenc}; \RepSpace)_\infty \leq \poly(\delta_1, \delta_2).
	\],
    where $\delta_1, \delta_2$ are the bounds on the blowup and shift differences.
\end{lemma}
%\begin{proof}
%	The proof follows trivially from the fact that $\vec{e}_{\nctx} \cdot \desF{\fenc} = \vecQuery \cdot E B + S$.
%	Then, we can use the $\delta_1, \delta_2$ bounds on the blowup and shift sets to show that the worst-case deviation of $\vec{e}_\nctx \cdot \desF{\fenc}$ is bounded a polynomial function of $\delta_1, \delta_2$.
%	So, as long as $\peakToPeak(\desF{\Model}, \vecDes^{\nfree})$ is positive, we can always set $\nctx$ to be large enough such that $\WD(\vec{e}_\nctx \cdot \desF{\fenc}; \InpSpace \times \blowupSet \shiftSet)$ is arbitarily small and thus $\WD(\vec{e}_\nctx \cdot \desF{\fenc}; \InpSpace)_2$ is arbitraily small by the lifting monotonicity of worst-case deviation (\cref{lemma:liftMon}).
%\end{proof}

We are now ready to prove \cref{thm:convergence}.
\begin{proof}[Proof of \cref{thm:convergence}]
	Note that we reduced $\WD(\vec{e}_\nctx \cdot \desF{\Model}; \RepSpace)_\infty$ to be upper-bounded by 
	$
	O(1) \cdot \WD(\vec{e}_\nctx \cdot \desF{\fenc}; \RepSpace) + o(1)
	$
	through \cref{lem:mlpbound} and \cref{lem:convgattn}.
	Then, by \cref{lem:conv_fenc_bound}, we have that \cref{thm:convergence} holds as $\WD$ goes to $0$ as $\nctx \rightarrow \infty$.
    So, as long as $\peakToPeak(\desF{\Model}, X)$ is positive for \emph{some} $X \in \RepSpace$, then we converge to ``overwhelming'' by \cref{thm:metathm}.
    Note that $\vece_\rep^\nctx \in \RepSpace$ and by \cref{lem:gap_shortcut}, we can compute a sample of peak-to-peak deviation for all $\nctx$.
\end{proof}




