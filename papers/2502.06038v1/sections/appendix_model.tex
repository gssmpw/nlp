\section{Appendix for Model Details}
\label{sec:appendix_model}
We provide a formal definition of each of the components in the model used in this paper.
\begin{itemize}
	\item $\softmax$ is the softmax function
		\[
			\softmax(\vec{\alpha})[i] = \frac{e^{\vec{\alpha[i]}}}{\sum_{i=1}^{\dVocab} e^{\vec{\alpha}[i]}}
		\]
	\item $\relu$ is the rectified linear unit function
		\[
			\relu(x) = \max(0, x)
		\]
	\item $\LayerNorm$ is the layer normalization function which for matrix $X$ performs the following column wise function for columns of $X$ (i.e.\ $X[:, j]$):
		\[
			\LayerNorm(X[i, j]) = \frac{X[i, j] - \Expec[X[:, j]]}{\sqrt{\Var[X[:, j]] + \eps} } \cdot \gamma + \beta
		\]
		where $\gamma$ and $\beta$ are learned parameters and addition and division are element-wise.
	\item $\MLP$ is a one-layer feed-forward network with ReLU activation. The $\MLP$ is row wise of $X$:
		\[
			\MLP(X[i]) = \relu(X[i, :] \cdot A_{enc} + b_{enc}) \cdot A_{dec}
		\]
		Depending on the model there may be additional bias terms added after the ReLU activation.
		For simplicitily, we will only consider one bias term prior to the ReLU activation though the results of this paper can be easily be extended to two bias terms.
	\item $\RoPE$ \cite{su2024roformer} is the rotary position encoding which applies a rotation to key and query vectors. For input $X$ at row $i$, the rotation is applied as follows: 
		\[
			\RoPE(X)[i, 2j] = \cos(i\theta_j)X[i, {2j}] - \sin(i\theta_j)X[i, 2j + 1]
		\]
		and
		\[
			\RoPE(X)[i, 2j + 1] = \sin(i\theta_j)X[i, {2j}] + \cos(i\theta_j)X[i, 2j + 1]
		\]
		where $\theta_j = 10000^{-2j/\dEmb}$ is the frequency for dimension $j$.
		%\item $\attn$ is the attention mechanism: for matrix $X \in \R^{\nctx \times \dEmb}$, the attention mechanism with RoPE is
		%	\[
		%		\attn(X) = \softmax\left( \frac{(\RoPE(XQ, i))(\RoPE(XK, i))^T}{\sqrt{d}} \right) \cdot XV
		%	\]
		%	where $i$ is the position index for each row of $X$, and the softmax is applied column-wise.
	\item $\attnH$ is an attention head for matrix $X \in \R^{\nctx \times \dEmb}$, an attention head does the following:
		\[
			\attnH(X) = \softmax\left( \frac{\RoPE(X Q) \cdot \RoPE(K^T X^T)}{\sqrt{\dEmb / H}} \right) \cdot X V
		\]
		where the softmax is applied column-wise.
        As in Ref.~\cite{black2022gptneox20bopensourceautoregressivelanguage}, we can re-write the effect of RoPE via a rotation matrix $\Theta_{i, j}$ such that
        \[
            \left(\RoPE(X Q) \cdot \RoPE(K^T X^T)\right)[i, j] = \vece_i \cdot X Q \cdot \Theta_{i, j} \cdot  K^T  X^T \cdot \vece_j^T
        \]
	\item Often, we have a multi-head attention mechanism which is the concatenation of $H$ attention mechanisms along the last dimension:
		\[
			\attn(X) = [\attnH_1(X); \ldots; \attnH_H(X)]
		\]
		where $\attnH_h$ is the $h$-th attention mechanism as outlined
	\item $\Embed \in \R^{\dVocab \times \dEmb}$ is the embedding function which maps a one hot vector in $\R^\dVocab$ to a vector in $\R^{\dEmb}$.
	\item $\Unembed \in \R^{\dEmb \times \dVocab}$ is the unembedding function which maps a vector in $\R^{\dEmb}$ to a one hot vector in $\R^\dVocab$.
\end{itemize}


