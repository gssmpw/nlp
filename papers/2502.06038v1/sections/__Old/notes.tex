
\section{A Simple, Single Attention Head Model}


\[\mathcal{M}(\textbf{t}) = \ell(\textbf{t}) = \sigma^* 
 \left ( (x_{\tExt{query}} E + P_{\tExt{query}}) Q K^T  (P^T + E^T \textbf{x}^T) /\sqrt{d} \right ) \cdot (\textbf{x}E+P)VOU + (x_{\tExt{query}}E+P_{\tExt{query}})U\]


\section{Results for $P=0$}
Here we will prove statements about the case $P=0$ with the idea being that these could later be generalized to non-zero and non-trivial positional embeddings $P$.  Taking $P = 0$ our model reduces to:

\[\mathcal{M}(\textbf{t}) = \ell(\textbf{t}) = \sigma^* 
 \left ( (x_{\tExt{query}} E + P_{\tExt{query}}) Q K^T   E^T \textbf{x}^T /\sqrt{d} \right ) \cdot \textbf{x}EVOU + (x_{\tExt{query}}E+P_{\tExt{query}})U\]


 In fact we could even remove the $P_{\tExt{query}}$ but we choose to keep them in for the purposes of forward compatibility to later sections of the definition of the $\vec{a}$ vectors which we now introduce.  

 \begin{definition}[$\vec{a}$]
     Let $\vec{a} \equiv (x_{\tExt{query}} E + P_{\tExt{query}}) Q K^T   E^T$
 \end{definition}

 Then we have:

\[\mathcal{M}(\textbf{t}) = \ell(\textbf{t}) = \sigma^* 
 \left ( \vec{a} \textbf{x}^T /\sqrt{d} \right ) \cdot \textbf{x}EVOU + (x_{\tExt{query}}E+P_{\tExt{query}})U\]


\[ = \frac{1}{|Z|}\sum_{j=1}^{n_{ctx}}  e^{a_j}\left( EVOU \right)_j  +(x_{\tExt{query}}E+P_{\tExt{query}})U \]

Now defining 

\begin{definition}[$\vec{r_j}$]
    Let $\vec{r_j} \equiv \left( EVOU \right)_j$ be the rows of $EVOU$.
\end{definition}

\begin{definition}[$w_j$]
     Let $\vec{w} = \sigma(\vec{a} \mathbf{x}^T / \sqrt{d})$ and we have that $\sum_j w_j = 1$.
    
\end{definition}

Rewriting, we have:

\[\mathcal{M}(\textbf{t}) = \ell(\textbf{t}) =  \sum_{j=1}^{d_{vocab}}  \Tilde{w}_j \vec{r_j} + (x_{\tExt{query}}E+P_{\tExt{query}})U\]
 
 Where $\Tilde{w}_j$ are the aggregated $w_j$'s collected over repeated input tokens so that there are $d_{vocab}$ different $\Tilde{w}_j$'s as opposed to the $n_{ctx}$ different $w_j$'s.  Nonetheless we still have $\sum_{j=1}^{d_{vocab}}\Tilde{w}_j = 1$
     
\begin{claim}
     Imagine that the input sequence to our model is a prefix of t arbitrary tokens, followed by a fixed sequence of tokens repeated k times.  If the positional embedding of our model is $P=0$, then there exists a linear program which can certify, in time that is polynomial in t, k, and the model dimension d, that the model will persist in repeating the repetitive sequence regardless of which tokens are chosen to fill the t prefix spots.
\end{claim}


\begin{proof}
    Proof Outline:  We want to check that 
\end{proof}

\begin{claim}
    Even though the linear program in Claim 1 does not always return a proof for every value of t and k, it gives a proof of all repeated sequences "asymptotically" in the following sense:  If a fixed sequence is repeated indefinitely by model when $t = 0$ and k is sufficiently large, then, for every $t>0$ there exists a sufficiently large $K_t$ such that the linear program from Claim 1 will return a proof of "robust repetitive loop" for every $k > K_t$.
\end{claim}



\section{Model without fixed $x_{\tExt{query}}$ and with $P \neq 0$}
We propose the following algorithm to find $k$ such that repeating a token 

The model now looks the same, but $x_{\tExt{query}}$ is considered to be a variable rather than a constant as in previous sections. 

\[\mathcal{M}(\textbf{t}) = \ell(\textbf{t}) = \sigma^* 
 \left ( (x_{\tExt{query}} E + P_{\tExt{query}}) Q K^T   E^T \textbf{x}^T /\sqrt{d} \right ) \cdot \textbf{x}EVOU + (x_{\tExt{query}}E+P_{\tExt{query}})U\]

\section{$P \neq 0$}


We now consider the model where $P \neq 0$ and, in particular, as written in \url{https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/}

\[P \equiv \]

Suppose you have an input sequence of length 
 and require the position of the 
 object within this sequence. The positional encoding is given by sine and cosine functions of varying frequencies:

 
 
 

Here:

k: Position of an object in the input sequence, 

d : Dimension of the output embedding space

P(k,j) : Position function for mapping a position 
 in the input sequence to index 
 of the positional matrix

n: User-defined scalar, set to 10,000 by the authors of Attention Is All You Need.

i: Used for mapping to column indices 
, with a single value of 
 maps to both sine and cosine functions

In the above expression, you can see that even positions correspond to a sine function and odd positions correspond to cosine functions.

\subsection{How we can deal with this}
We can manually inspect the contribution of $P$ to the attention head and the $PVOU$ as in Ref.~\cite{gross2024compact}. Then, we can add $\Delta w_{\max}$ as in Algorithm 3 of Ref.~\cite{gross2024compact}. Unfortuantly, its not as straighforward for us as we are not computing the max.
Within our linear program, we may be able to consider a linear program which somehow ``maximizes'' the error contributed by $P$. We do have to consider though what ``error'' means

\subsubsection{P in the attention}
This depends on how we do the $P = 0$ proof. There is a pretty simple way to do this (with $P = 0$ and $P = 1$). 

Starting with $P = 0$, say our repeated token is $\tau$. Then, within the attention matrix, we look at the value of $EQKE[\tau, \tau]$. 

If $P \neq 0$, we look at the most negative contribution of $PQKE$ and subtract that from $EQKE[\tau, \tau]$

\section{Parallel Attention Heads}

\section{Serial Attention Heads (with the original positional encodings)}
So there are a couple of ways of doing things. Right now, I'll go with ``parrallel'' residuals (feed forward and attention added together rather than in serial)

%https://www.overleaf.com/read/bcqzfnsshnvv#c2ba62
%
%Jason Gross
%to
%Everyone
%06:11 PM
%JG
%https://arxiv.org/abs/2103.16080
%
%https://arxiv.org/abs/2310.07923
%
%
%
%
%For a particular model M, forall A, B, S, M(ASB) != parity(S)
%
%For a particular M, forall A, B, exists S, M(ASB) != parity(S)
%
%https://arxiv.org/abs/2405.14838
%
%For a particular M, forall A, B, #(S st M(ASB) == parity(S)) is sufficiently small
%
%
%
%According to Jason:
%Layernorm is substracting the mean (), and then dividing by epsilon (1e-10) plus root mean square.  Typically layernorm is applied to the embedded token vectors (apply layernorm to xE, and then continue with the usual architecture).  (It may be standard to use layernorm on the xE inside the softmax, but not the one multiplied by VOU....no, actually Jason corrected and said that it is standard to use it on the VOU part as well).  But, xE is matrix, not a vector, so how do we apply layernorm to xE?
%
%Jason is getting this information from these documents:
%
%https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html
%
%
%
%
%https://github.com/TransformerLensOrg/TransformerLens/blob/cb5017ad0f30cde0d3ac0b0f863c27fbec964c28/transformer_lens/components/transformer_block.py#L159-L167
%
%see: self.ln1


