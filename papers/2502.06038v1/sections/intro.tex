\section{Introduction}

Decoder-only transformers \cite{vaswani2023attentionneed} have become an enormously popular paradigm in the past few years \cite{geminiteam2024geminifamilyhighlycapable, openai2024gpt4technicalreport, Li_2022}.
%Yet, understanding the real-world performance of transformer models at the level of mathematical proof remains a daunting challenge.  
%Yet, our theoretical understanding of these models remains limited, and even formulating precise mathematical statements with practical relevance is non-trivial.  
%Approaches which seek to provably capture the ability of general transformer models to solve data-defined real-world problems
 %confront the notorious difficulty of analyzing heuristic optimization over a high-dimensional, non-convex space (training), or are challenged to precisely define the model behavior to be proven.
% For this reason, proofs about transformer model behavior are often obtained at the cost of restricting to an unrealistic toy model or making over-simplifying assumptions.
%  In this work we take a different approach which seeks to prove statements directly about the behavior of popular transformer models.  In order to do this we design an algorithm which takes a trained model as input.    
% In this work we take a different approach, precisely defining a particular failure mode of transformers models which seems both relevant and common in practice, and providing an algorithm to prove when this failure mode has been reached (a statement which is infeasible to verify exhaustively).  We ask:
% Most existing proofs of transformer behavior are either too broad to be useful in practice, over-simplifying, or fail to prove concrete statements about the model's output.
% Proofs about transformer model behavior are often obtained at the cost of restricting to an unrealistic toy model or making over-simplifying assumptions. Due to the difficulty of analyzing heuristic optimization over a high-dimensional, non-convex space (training) and precisely defining observed behavior.
Yet, our theoretical understanding of these models remains limited.
Proving mathematically rigorous statements about transformers face challenges due to the high dimensionality and complexity of transformers.
To circumvent these problems, current techniques either require simplifying assumption or very specific restrictions on the model/ dataset. 
%In this work, we try to circumvent the challenges faced by prior works by asking a different question:
In this work we propose a different approach focused toward producing rigorous and operationally relevant guarantees for specific, trained, transformer models.
That is, we ask the question:

%\begin{quote}
	\textit{
	Can we develop an algorithm which can provably bound the behavior of a specific trained transformer model?}
%\end{quote}

With this motivation in mind, we design a class of algorithms which can prove when a particular type of ``\emph{over-squashing}'' phenomenon occurs in a trained transformer model.
Over-squashing is a known phenomenon in graph neural networks (GNNs) in which the representation of distinct nodes becomes arbitrarily close as the number of layers grows \cite{alon2021bottleneckgraphneuralnetworks, barbero2024transformers}.
Recently, Barbero et al.~\cite{barbero2024transformers} have shown that over-squashing occurs in the limit for a transformer model without positional embeddings.

Over-squashing on its own is an important phenomenon.
Yet, it does not directly constrain the model's output behavior.
In this work, we focus on a specific related phenomenon, which we call ``overwhelming''.
A model is ``overwhelmed'' by an input string if the model's output is unchanged by concatenating with \emph{any} new string of tokens of a fixed length.

\begin{definition}[Overwhelm, Overwhelmed, Overwhelming]
    For a given transformer model $\Model$, string of tokens $s$ of length $\nfix$, a fixed final token $q$\footnote{
We find it convenient to fix the final token due to its importance in determining the output logits for the transformer architecture.
We refer to the final token as the ``query token.''}, and integer $\nfree$ we say that $\Model$ is ``overwhelmed" by $s$ if the output of the model evaluated on $s$ plus any additional string $t$ and fixed final token $q$, $\Model(s + t + q)$, is the same regardless of the value of the string $t$ whenever length($t$) $\leq \nfree$.
\end{definition}

In this work, we provide concrete algorithms (\cref{alg:overwhelmCheckDet}, \cref{alg:overwhelmCheck2}) which can produce a proof that a given token sequence overwhelms a given model, as stated in \cref{thm:informal_overwhelm} and \cref{thm:informal_overwhelm_perm} respectively. 
Our algorithms work in two stages: first we upper bound a quantity which we call \emph{worst-case deviation} (denoted by $\WD$), which bounds the extent to which the logit weights output can vary.
Second, we lower-bound a quantity which we call \emph{peak-to-peak difference} which measures the difference in the maximum and second maximum logit weights output by the model.
%how much logit weights output by the model would need to change before the final output of the model would change.  When the worst-case deviation is less than half of the peak-to-peak difference, the model is overwhelmed.  
We thus get our first main result:
%More precisely, our main contributions are Algorithms \ref{alg:overwhelmCheckDet}, and \ref{alg:overwhelmCheck2} together with the following theorems:

\begin{theorem}[Formally restated in \cref{thm:InpRes}]
	\label{thm:informal_overwhelm}
	If Algorithm \ref{alg:overwhelmCheckDet} returns ``Overwhelmed'' when run on a transformer model $\Model$, with a fixed input string of tokens $s$, final token $q$, and context length $n_{ctx}$ then $\Model$ is, provably, overwhelmed by $s$.
\end{theorem}
% \vspace{-1.5em}
% \begin{proof}
%     \cref{thm:informal_overwhelm} follows by combining \cref{thm:metathm} and \cref{thm:upperW} which, together, show that the final \textbf{if} statement in \cref{alg:overwhelmCheckDet} outputs ``Overwhelmed'' only when the model is overwhelmed.
% \end{proof}

Proving ``overwhelming'' for transformer models can have many interesting and useful applications.
For example, discovering overwhelming strings can be used to find examples of model hallucination.
If a fixed string causes the output to disregard even a small set of input tokens, the model will produce the wrong result for highly sensitive functions such as parity and finding bugs in code.
% Imagine a model attempting to compute a highly sensitive function, such as parity or SOMETHING ELSE: then, even if the model is overwhelmed with a relatively long fixed string and short free string, the output 
% For example when performing a sensitive task proving overwhelming of even a single input entry discovers can uncover hallucination. For example, overwhelming a single entry of a string x by a prompt meant to check parity of x. Or similarly a single line of code by a prompt meant to perform error detection.
Additionally, overwhelming strings can be used to ``jailbreak'' models by forcing the model to ignore part of the system prompt.
Discovering these instances can be relevant in performing safety evaluations of models.
Another application would use overwhelming strings to prove no-go results for prompt engineering: no ``prompt'' of a fixed length can equip a model $\mathcal{M}$ to properly compute a specific function, such as parity. Here the prompt will be the free string and the function input the fixed ``overwhelming'' string. 
Finally, we can also use the frequency of overwhelming strings to define a sort of ``model complexity'' for a given context length; the less overwhelming that occurs for a given context window, the more ``powerful'' a model is for that context window.
% The size of context window at which overwhelming strings become significantly more numerous for a given model could be interpreted as a threshold for length and complexity of inputs beyond which the model begins to have difficulty operating successfully. 


Next, we design \cref{alg:overwhelmCheck2}, to achieve performance improvements in a restricted setting in which the $\nfree$ tokens are chosen from the permutation set of a single string (as opposed to arbitrary tokens). This restricted setting is of practical interest because it often occurs with much shorter ``overwhelming'' strings.
Furthermore, the ability of a transformer to distinguish between different permutations of the same string is relevant in many real-world applications.  For example, if an LLM cannot distinguish between the two strings in \cref{fig:perm_example}, then, the model cannot identify the variable $a$ as being defined before it is used.

\begin{figure}[H]
	\centering
	\begin{multicols}{2}
\begin{lstlisting}[language=Python]
# Code snippet
def my_func():
	...
	a = 1
	b = a + 1
	...
	\end{lstlisting}
	\columnbreak
\begin{lstlisting}[language=Python]
# Code snippet
def my_func():
	...
	b = a + 1
	a = 1
	...
\end{lstlisting}
\end{multicols}
\caption{Two snippets of code that are identical except for the order of the assignment statements.}
	\label{fig:perm_example}
\end{figure}

\begin{theorem}[Formally restated in \cref{thm:InpResPermInv}]
	\label{thm:informal_overwhelm_perm}
	If \cref{alg:overwhelmCheck2} returns ``Overwhelmed'' when run on a  transformer model $\Model$, with a fixed input string of tokens $s$, permutation string $x$, query token $q$, and context length $n_{ctx}$ then $\Model$ is, provably, overwhelmed by $s$ when the free string is restricted to be a permutation of $x$.
\end{theorem}
\vspace{-0.5em}
% \begin{proof}
%     The above follows by a direct application of \cref{thm:InpResPermInv} which shows that the final \textbf{if} statement in \cref{alg:overwhelmCheck2} outputs ``Overwhelmed'' only when the model is overwhelmed.
% \end{proof}

% This paragraph LGTM
We focus on single-layer transformers in this paper. Our results can be straight forwardly extended to multi-layer models by approximating  Lipschitz constants \cite{kim2021lipschitzconstantselfattention}, though the bounds obtained for higher depth models are too loose to be useful in practice. New ideas are required to obtain tighter bounds for multi-layer transformers.

Before proving these theorems, we first provide a mathematical description of the model in \cref{sec:model}, as well as a metric of over-squashing called worst-case deviation, in \cref{sec:worst_case_framework}.
In \cref{sec:meta_framework}, we provide the main algorithm of this paper and prove \cref{thm:informal_overwhelm}.
%In \cref{sec:algoCorrectness} we prove the algorithm's correctness by bounding the worst-case deviation of internal layers of a transformer which can be used to prove a variety of properties about a trained transformer model.
In \cref{sec:perm_invar}, we prove \cref{thm:informal_overwhelm_perm} and introduce a linear program which is helpful in bounding extremal values in attention; we believe that this linear program may be of independent interest as well.
In addition, in \cref{sec:convergence}, we prove that, in the asymptotic limit, the model converges to a fixed output model when the fixed string is repeated many times before the free string.

\textbf{Empirical Results:} In \cref{sec:model_eval}, we use our algorithm in practice on a trained, single-layer model.
The algorithm checks for ``overwhelming'' when restricting the free string to an element of one of two permutation classes.
We use fixed strings of varying sizes (up to 500 tokens): our fixed strings come from either: (1) the test set of our training data, (2) randomly sampled tokens, (3) a repeated string.
We also test whether ``overwhelming'' persists across multiple steps of token generation.

\subsubsection*{Related Work}
Gross et al.~\cite{gross2024compact} also consider a concrete instance of a trained transformer model and produce a proof that the model can select the maximum token of a sequence with a lower bound on the probability of success.
Barbero et al.~\cite{barbero2024transformers} prove that over-squashing occurs in the limit for a (multi-layer) model without positional embeddings.
Though similar to our work, their results are not directly comparable to ours as they do not provide a concrete algorithm to check for over-squashing in a trained model.
Refs.~\cite{hahn2020theoretical, hahn2024sensitive} use Lipschitz constants to prove various properties of neural networks and provide an implicit algorithm to check for these properties.
In recent year, there has also been a flurry of impossibility results related to transformers \cite{peng2024limitations, merrill2023parallelism, sanford2024representational}, these works focus on general impossibility for the computational class of the transformer architecture.
% However, to the authors' knowledge, the sole use of Lipschitz constants fails to prove facts about models used in practice.
% Specifically, layer-norm and the attention layer have a Lipschitz constant which \emph{grows in the size of the input} \cite{kim2021lipschitz}.
%We add to their approach 
% Instead of solely using Lipschitz constants though, we use convex programs and an un-normalized variant of the Lipschitz constant alo which we term the \emph{worst-case deviation}.
\iffalse
\section{Introduction}


Since the inception of practically useful neural network architectures, it has remained a widespread and elusive goal of many in the scientific community to develop a predictive theoretical understanding of these objects, especially at the level of proof-based mathematics. 
In this work we exhibit initial positive results from a new approach to proving guarantees about AI models. 
Our approach is a trade-off, focusing on guarantees that are smaller in scope, applying only to limited behaviors of specific models, but, in exchange, offers an algorithm to check when our proof applies to any given model, as well as completely rigorous, operationally relevant guarantees for any model on which our algorithm succeeds.


The majority of theoretical attempts to understand neural network architectures have focused either on results about training  \mnote{broad citations}, or on expressability \mnote{is this the right term?} results \mnote{broad citations}. 
Theoretical attempts to analyze model training face the notorious difficulty of proving any meaningful statement about heuristic descent algorithms on high dimensional, non-convex optimization spaces. 
On the other hand, while theoretical results about expressability can establish rigorous limits on the capabilities of a given architecture, they must account for highly contrived and unnatural parameter settings, and thus often fail to capture model performance in practice. 

In this work we take a different approach to proving results about neural networks, focusing on the enormously popular transformer architecture, specifically the decoder-only GPT style architecture \lev{TODO cite}. 
Rather than attempting mathematical proofs capturing all possible parameter settings of a model, we design an algorithm which produces provable guarantees about a specific \emph{trained transformer model}, reminiscent of \lev{TODO: cite Jason's paper}.
While our algorithm's guarantees are valid only for the transformer model on which it is run, the fact that the guarantees can be produced and checked in an automated manner means that they can potentially be re-established on new AI models just as fast as those models are produced. 

As a concrete example, in this work we use our framework to generate proofs specifically about text inputs that ``overwhelm'' a given trained transformer model, as detailed in Subsection \ref{subsec:Overconcepts}. 
We select this application initially because it is the most concrete and has the simplest proofs, but we believe that the same framework will be useful in the future for establishing more sophisticated provable results, automatically tailored to specific, fixed, AI models. 

\subsection{Overwhelming Inputs - Definition and Results} \label{subsec:Overconcepts}

In this work we focus on a specific phenomenon that we call \emph{overwhelming inputs}.
Specifically, we consider a set of $\nctx$-long inputs to a transformer where $s$ tokens are held constant: e.g.\ the first $s$ tokens are always the same string. 
Then, we will provide an algorithm which checks if the output of a single-layer transformer is the same of the given set
\footnote{
	For simplicity, in this paper, we consider a model which samples its output at $0$-temperature.
	We note that out techniques can generalize to non-zero temperature (simply by considering an $\ell_2$ bound instead of an $\ell_\infty$ bound.
}.

\begin{definition}[Overwhelming Input]
	Given a fixed transformer model M, let $s$ be a string of input tokens, together with specified input positions to M.  We say that $(s, q)$ is an overwhelming input to M if the output of the model M with the input positions specified by $s, q$ populated by the tokens specified by $s$ and $q$, $M(s|*|q)$, is invariant over all possible settings of the inputs $*$ which populate the input positions not populated by $s$ and $q$.    \mnote{this still seems awkwardly worded.  Should optimize.}
\end{definition}

Before constructing a \emph{specific} algorithm to determine if a fixed input string produces a ``frozen'' output, we construct a general framework for proving that a given input string is overwhelming in \cref{sec:meta_framework}.

\begin{theorem}
	If Algorithm \ref{alg:overwhelmCheck} returns ``Frozen'' when run on a transformer model M, with fixed input string of tokens $s$ and context length $n_{ctx}$ then $s$ is an overwhelming string for M.
\end{theorem}

\begin{theorem}
	If Algorithm \ref{alg:freezecheckerLP} returns ``Frozen'' when run on a transformer model M, with fixed input string of tokens $s$ and context length $n_{ctx}$ then then $s$ is an overwhelming string for M.
\end{theorem}

\subsection{Concrete Examples of Overwhelming Phenomenon}

\fi
