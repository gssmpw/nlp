\section{Model}
\label{sec:model}
In this paper, we focus on a standard single-layer transformer model which uses RoPE positional encoding, parallel feed-forward residuals, and layer normalization.
Our model closely follows the GPT-NeoX architecture \cite{black2022gptneox20bopensourceautoregressivelanguage} except that: (1) we do not use an attention mask, and (2) we use a single layer-normalization for the MLP and the attention mechanism instead of two
\footnote{Ref.~\cite{black2022gptneox20bopensourceautoregressivelanguage} intended to initially use a single layer-normalization for the MLP and attention mechanism, but, due to a bug, this was changed in the final implementation.
We thus use the intended architecture in this paper.}.
%Moreover, we will restrict our experiments to single layer transformers for concreteness and clarity. 
%We believe that our results will ultimately extend to transformers with many layers.

We use relatively standard notation for transformer models where $\LayerNorm$ is the layer normalization function, $\Embed$ is the embedding function, $\Unembed$ is the unembedding function, $\attn$ is the attention mechanism,  $\attnH$ is an attention head,
$\MLP$ is the feed-forward network, and $\Iden$ is the identity function.
Concrete details can be found in \cref{sec:appendix_model}.

We also use the notation in Black et al. \cite{black2022gptneox20bopensourceautoregressivelanguage} where we re-write the effect of RoPE via a rotation matrix $\Theta_{i, j}$ such that for $X \in \OneHotSpace^{\nctx}$, we have
\begin{align*}
	\left(\RoPE(X Q) \cdot \RoPE(K^T X^T)\right)[i, j]
	\\= \vece_i \cdot X Q \cdot \Theta_{i, j} \cdot  K^T  X^T \cdot \vece_j^T.
\end{align*}

%In general, when referring to the $\ell$-th layer of the model, we will use the superscript $(\ell)$.
%For example, the MLP of the $\ell$-th layer is $\MLP^{(\ell)}(\vec{x}) =
%\relu(\vec{x} A_{enc}^{(\ell)} + b_{enc}^{(\ell)}) \cdot A_{dec}^{(\ell)}$.

The model we study is formally defined as follows:
\begin{definition}[$1$-Layer Transformer]
	\label{def:one_layer_transformer}
        For a single layer model, $\Model$,
	\[
		\Model = \Unembed \circ (\attn + \MLP + \Iden) \circ \LayerNorm \circ \Embed
	\]	
	where $\attn$ has heads $\attnH_1, \dots, \attnH_H$ for some number of attention heads $H$.
\end{definition}

We also introduce notation related to one-hot vectors:
\begin{definition}[One Hot Space]
	\label{def:one_hot_space}
	Let $\dVocab$ be the size of the vocabulary.
	We define $\OneHotSpace \subset \R^\dVocab$ as the set of one-hot vectors.
	That is, $\OneHotSpace = \{ \vec{e}_i \mid i \in [\dVocab] \}$ where $\vec{e}_i$ is the $i$-th standard basis vector in $\R^{\dVocab}$.
\end{definition}

%
% Multiple layer model::
% \Model^{(k)} =  (\attn^{(k)} + \MLP^{(k)} + \Iden) \circ \LayerNorm^{(k)}
% \Model = \Unembed \vec{e}_{\nctx} ((\Model^{(\ell)} \circ \ldots \circ (\Model^{(1)} \circ \Embed))
%
