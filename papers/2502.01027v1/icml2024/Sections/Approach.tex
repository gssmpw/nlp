\section{Adversarial Attacks on Two-Stage L2D}\label{section:attacks}

\paragraph{Motivation and Setting}  \label{motivation}
The two-stage L2D framework is designed to route queries to the most accurate agents, ensuring optimal decision-making~\cite{mao2023twostage, mao2024regressionmultiexpertdeferral, montreuil2024twostagelearningtodefermultitasklearning}. Despite its effectiveness, we demonstrate that this framework is inherently vulnerable to adversarial attacks that exploit its reliance on the rejector function, a key component responsible for query allocation. Given this critical role, our analysis focuses on adversarial attacks and corresponding defenses targeting the rejector \( r \in \mathcal{R} \), rather than individual agents. This focus is justified because adversarial defenses for specific agents can typically be deployed offline within the two-stage L2D setup. Moreover, evaluating the robustness of individual agents under adversarial conditions simplifies to selecting the most robust agent, whereas ensuring robustness at the system level constitutes a fundamentally different challenge.



\paragraph{Untargeted Attack:} 
In classification, the goal of an untargeted attack is to find a perturbed input \( x^\prime \in B_p(x, \gamma) \) that causes the classifier \( h \in \mathcal{H} \) to misclassify the input \citep{goodfellow2014explaining, akhtar2018threat}. Specifically, for a clean input \( x \in \mathcal{X} \) where the classifier correctly predicts \( h(x) = y \), the attacker aims to identify a perturbed input \( x^\prime \) such that \( h(x^\prime) \neq y \).

In the context of Learning-to-Defer, the attack extends beyond misclassification to compromising the decision allocation mechanism. Given an optimal agent \( j^\ast \in \mathcal{A} \), the attacker aims to find an adversarial input \( x^\prime \in B_p(x, \gamma) \) such that \( r(x^\prime) \neq j^\ast \), thereby forcing deferral to a suboptimal agent \( j \in \mathcal{A} \setminus \{ j^\ast \} \)---incurring a higher loss. This leads to the following untargeted attack formulation:

\begin{restatable}[Untargeted Attack in L2D]{definition}{untargeted}\label{eq:untargeted}
Let \(x^\prime \in B_p(x, \gamma)\) be an adversarial input, where \(B_p(x, \gamma)\) denotes the \(p\)-norm ball of radius \(\gamma\) centered at \(x\). The untargeted attack that maximizes misallocation in L2D is formulated as follows:
\begin{equation*} 
    x^\prime = \argsup_{x^\prime \in B_p(x,\gamma)} \sum^J_{j=0} \tau_j(g(x),m(x),z) \Phi_{01}^u(r, x^\prime, j).
\end{equation*}
\end{restatable}

Definition~(\ref{eq:untargeted}) characterizes an attack in which the adversary maximizes the aggregate discrepancy between the rejector's allocation for the adversarial input \( x^\prime \) and its original allocation, thereby forcing the system to defer to an unintended agent. The adversarial input \( x^\prime \) is designed to maximize these surrogate losses. As a result, the adversarial attack increases the system's overall loss, significantly degrading its performance.  An illustration of this attack is provided in Appendix Figure \ref{fig:untargeted}.



% In certain scenarios, a malicious attacker with additional system information may aim to allocate the decision to a specific agent \( j_t \in \mathcal{A} \). Such a targeted attack leverages knowledge about agent characteristics (e.g., performance, cost, or reliability) to manipulate the decision allocation. The following section formalizes a \textbf{targeted attack} tailored for the Learning-to-Defer framework.



\paragraph{Targeted Attack:} 
Targeted attacks are often more impactful than untargeted ones, as they exploit specific system vulnerabilities to achieve precise adversarial goals \citep{akhtar2018threat, chakraborty2021survey}. For example, in autonomous driving classification, a targeted attack could deliberately misclassify a stop sign as a speed limit sign, potentially leading to hazardous consequences. A targeted attack exploits this asymmetry by forcing the classifier \( h \in \mathcal{H} \) to predict a specific target class \( y_t \in \mathcal{Y} \), potentially leading to harmful consequences.


In the context of L2D, an attacker aims to manipulate the system into assigning a query \( x^\prime \in B_p(x, \gamma) \) to a predetermined expert \( j_t \in \mathcal{A} \) rather than the optimal expert \( j^\ast \in \mathcal{A} \). This targeted attack objective can be formally expressed as follows:

\begin{restatable}[Targeted Attack in L2D]{definition}{targeted}\label{eq:target1}
Let \(x^\prime \in B_p(x, \gamma)\), where \(B_p(x, \gamma)\) denotes the \(p\)-norm ball of radius \(\gamma\) centered at \(x\). The attack targeting the allocation of a query \( x \) to the expert \( j_t \in \mathcal{A} \) is defined as:
\begin{equation*}
    x^\prime = \arginf_{x^\prime \in B_p(x,\gamma)} \tau_{j_t}(g(x), m(x), z) \Phi_{01}^u(r, x^\prime, j_t).
\end{equation*}
\end{restatable}
The adversarial input \( x^\prime \) minimizes the loss associated with the targeted agent \( j_t \), thereby biasing the allocation process towards agent $j_t$ as described in Definition~(\ref{eq:target1}).  For instance, an attacker may have an affiliated partner \( j_t \) among the system’s agents. Suppose the system operates under a pay-per-query model—for example, a specialist doctor in a medical decision-making system or a third-party service provider in an AI-powered platform. By manipulating the allocation mechanism to systematically route more queries toward \( j_t \), the attacker artificially inflates its workload, leading to unjustified financial gains. These gains may benefit both the attacker and the affiliated expert through direct financial compensation, revenue-sharing agreements, or other collusive incentives. An illustration of this attack is provided in Appendix Figure~\ref{fig:targeted}.



% Another scenario involves a malicious attacker affiliated with an agent \( j_t \) in the system. For example, in a medical framework, a compromised doctor could manipulate the rejector to direct a disproportionate number of patient queries to themselves for personal financial gain. Such targeted attacks compromise the integrity of the system by prioritizing profit over accuracy and patient well-being, potentially leading to harmful outcomes.





% While targeted attacks (Definition~\ref{eq:target1}) ensure that queries are directed to a specific agent \( j_t \), they do not necessarily increase overall misallocation within the system. This limitation arises because queries are assigned to \( j_t \) regardless of its correctness. Conversely, untargeted attacks (Definition~\ref{eq:untargeted}) maximize system-wide misallocation but do not target specific agents. To address these limitations, we define an attack that combines aspects of both targeted and untargeted attacks, creating a more disruptive strategy that biases allocation toward \( j_t \) while simultaneously increasing misallocation across the remaining agents.
% \begin{restatable}[Inf-Sup Attack in L2D]{definition}{infsup}\label{eq:infsup}
% Let \(x^\prime \in B_p(x, \gamma)\), where \(B_p(x, \gamma)\) represents the \(p\)-norm ball of radius \(\gamma\) centered at \(x\). The Inf-Sup attack, which balances the tradeoff between targeting a specific expert \( j_t \in \mathcal{A} \) and inducing misallocation among other experts, is defined as:
% \begin{equation}
% \begin{aligned}
%     x^\prime & = \arg \Bigg(\inf_{x^\prime \in B_p(x,\gamma)} \tau_{j_t}(g(x), m(x), z)\Phi_{01}^u(r, x^\prime, j_t) \\
%     & + \sup_{x^\prime \in B_p(x,\gamma)} \sum_{\substack{j=0 \\ j \neq j_t}}^J  \tau_j(g(x), m(x), z)\Phi_{01}^u(r, x^\prime, j)\Bigg),
% \end{aligned}
% \end{equation}
% \end{restatable}

% In Definition~(\ref{eq:infsup}), the first term represents a targeted attack on agent \( j_t \), where the objective is to minimize the loss associated with \( j_t \), thereby biasing the allocation mechanism in its favor. The second term corresponds to an untargeted attack, which maximizes misallocation across the remaining agents \( j \in \mathcal{A} \setminus \{ j_t \} \). 

% Such attacks are especially harmful in critical domains. For instance, in a medical decision-making system, an attacker could manipulate the system to allocate a complex case requiring a specialist's expertise to a less experienced or inappropriate doctor, potentially compromising the patient's safety. Simultaneously, the attacker might redirect queries for benign or non-urgent conditions to highly specialized and costly experts, unnecessarily inflating the system's operational costs and creating resource bottlenecks.




\section{Adversarially Consistent Formulation for Two-Stage Learning-to-Defer}

In this section, we introduce an adversarially consistent formulation of the two-stage L2D framework, ensuring robustness against attacks while preserving optimal query allocation.




\subsection{Novel Two-stage Learning-to-Defer formulation}

\paragraph{Adversarial True Deferral Loss:} 


To defend against the novel attacks introduced in Section~\ref{section:attacks}, we define the worst-case \textit{adversarial true deferral loss}, \( \widetilde{\ell}_{\text{def}}: \mathcal{R} \times \mathcal{G} \times \mathcal{M} \times \mathcal{Z} \to \mathbb{R}^+ \), which quantifies the maximum incurred loss under adversarial perturbations. Specifically, for each \( j \in \mathcal{A} \), an adversarial perturbation \( \delta_j \) is applied, yielding the perturbed input \( x_j' = x + \delta_j \), which lies within an \(\ell_p\)-norm ball of radius \(\gamma\), i.e., \( x_j' \in B_p(x, \gamma) \). We define the \textit{\( j \)-th adversarial true multiclass loss} as  $\widetilde{\ell}_{01}^j(r,x,j) = \sup_{x_j' \in B_p(x, \gamma)} \ell_{01}(r(x_j'), j)$, which captures the worst-case misclassification loss when deferring to agent \( j \) under adversarial conditions. The formal definition of the adversarial true deferral loss is provided in Lemma~\ref{lemma:deferral}.

\begin{restatable}[Adversarial True Deferral Loss]{lemma}{deferral} \label{lemma:deferral}
Let \( x \in \mathcal{X} \) denote the clean input, \( c_j \) the cost associated with agent \( j\in\mc{A} \), and \( \tau_j \) the aggregated cost. The adversarial true deferral loss \( \widetilde{\ell}_{\text{def}} \) is defined as:
    \begin{equation*}
    \begin{aligned}
    \widetilde{\ell}_{\text{def}}(r, g, m, z) & = \sum_{j=0}^J \tau_j(g(x), m(x), z) \widetilde{\ell}_{01}^j(r,x,j)  + (1 - J) \sum_{j=0}^J c_j(g(x), m_j(x), z).
        \end{aligned}
    \end{equation*}
\end{restatable}
See Appendix~\ref{appendix:deferral} for the proof of Lemma~\ref{lemma:deferral}. The attacker’s objective is to compromise the allocation process by identifying perturbations \( \delta_j \) that maximize the loss for each agent \( j \in \mathcal{A} \). Importantly, the costs \( c_j \) and \( \tau_j \) are evaluated based on the clean input \( x \), as the agents' predictions remain unaffected by the perturbations (see Section \ref{motivation}).


Minimizing the \textit{adversarial true deferral loss} in Lemma~\ref{lemma:deferral} is NP-hard \citep{Statistical, bartlett1, Steinwart2007HowTC, Awasthi_Mao_Mohri_Zhong_2022_multi}. Therefore, as in classification problems, we approximate this discontinuous loss using surrogates.


\paragraph{Adversarial Margin Deferral Surrogate Losses:}  
In the formulation of the \textit{adversarial true deferral loss} (Lemma~\ref{lemma:deferral}), discontinuities arise due to the indicator function in the loss definition. To approximate this discontinuity, we build on recent advancements in consistency theory for adversarially robust classification \citep{bao2021calibratedsurrogatelossesadversarially, Awasthi_Mao_Mohri_Zhong_2022_multi, Grounded, mao2023crossentropylossfunctionstheoretical} and propose a continuous, upper-bound surrogate family for the \textit{adversarial true deferral loss}. Specifically, we define the \textit{\(j\)-th adversarial margin surrogate} family $\widetilde{\Phi}^{\rho,u,j}_{01}(r, x, j) = \sup_{x_j' \in B_p(x, \gamma)} \Psi^u \left( \sum_{j' \neq j} \Psi_\rho \big( r(x_j', j') - r(x_j', j) \big) \right)$
where \(\Psi^u\) and \(\Psi_\rho\) are defined in Equation (\ref{eq:family_sup}). Building on this, we derive the \textit{adversarial margin deferral surrogate} losses as:

\begin{restatable}[Adversarial Margin Deferral Surrogate Losses]{lemma}{margindeferral} \label{lemma:deferralmargin}
    Let \( x \in \mathcal{X} \) denote the clean input  and \( \tau_j \) the aggregated cost. The adversarial margin deferral surrogate losses \( \widetilde{\Phi}^{\rho, u}_{\text{def}} \) are then defined as:
    \begin{equation*}
    \begin{aligned}
        \widetilde{\Phi}^{\rho, u}_{\text{def}}(r, g, m, z) & = \sum_{j=0}^J \tau_j(g(x), m(x), z) \widetilde{\Phi}^{\rho,u,j}_{01}(r, x, j).
    \end{aligned}
    \end{equation*}
\end{restatable}
The proof is provided in Appendix~\ref{proof:margindeferral}. One notable limitation of the \textit{adversarial margin deferral surrogate}  family is the non-convexity of the \textit{\(j\)-th adversarial margin surrogate loss} family \( \widetilde{\Phi}^{\rho,u,j}_{01} \), which poses significant challenges for efficient optimization.

\paragraph{Adversarial Smooth Deferral Surrogate Losses:}  
As detailed by \citet{Grounded, mao2023crossentropylossfunctionstheoretical}, the non-convex \emph{adversarial margin surrogate} family can be replaced with a smooth and convex approximation. To this end, we adapt their results and introduce the \emph{smooth adversarial surrogate} family, denoted as \( \widetilde{\Phi}^{\text{smth},u}_{01}: \mathcal{R} \times \mathcal{X} \times \mathcal{A} \to \mathbb{R}^+ \), which approximates the supremum term in Lemma~\ref{lemma:deferral}. Crucially, \( \widetilde{\Phi}^{\text{smth},u}_{01} \) acts as a convex, non-negative upper bound for the \emph{$j$-th adversarial margin surrogate} family, such that \( \widetilde{\Phi}_{01}^{\rho,u,j} \leq \widetilde{\Phi}_{01}^{\text{smth}, u} \). We derive the \emph{smooth adversarial surrogate} losses in Lemma~\ref{lemma:surrogate_class}.
\begin{restatable}[Smooth Adversarial Surrogate Losses]{lemma}{surrogatemulti} \label{lemma:surrogate_class}
Let \( x \in \mathcal{X} \) denote the clean input and hyperparameters $\rho>0$ and $\nu>0$. We define the smooth adversarial  surrogate losses as:
    \begin{equation*}
    \begin{aligned}
        \widetilde{\Phi}_{01}^{\text{smth}, u} (r,x,j) = \Phi_{01}^u& (\frac{r}{\rho},x,j) + \nu \mspace{-20mu} \sup_{x_j^\prime \in B_p(x,\gamma)}\mspace{-20mu}  \| \overline{\Delta}_r(x_j', j) - \overline{\Delta}_r(x, j)\|_2.
        \end{aligned}
    \end{equation*}
\end{restatable}
For completeness, the proof is provided in Appendix~\ref{appendix:smooth}. For \( x\in\mc{X} \), define \( \Delta_r(x, j, j') = r(x, j) - r(x, j') \), and let \( \overline{\Delta}_r(x, j) \) denote the $J$-dimensional vector $\big( \Delta_r(x, j, 0), \ldots, \Delta_r(x, j, j-1), \Delta_r(x, j, j+1), \ldots, \Delta_r(x, j, J) \big)$. The first term, \( \Phi_{01}^u(r / \rho, x, j) \), corresponds to the \textit{multiclass surrogate} losses modulated by the coefficient \( \rho > 0 \). The second term incorporates adversarial evaluations \( x_j^\prime \in B_p(x, \gamma) \) for each agent \( j \in \mathcal{A} \), with a smooth adversarial component scaled by the coefficient \( \nu > 0 \) \citep{Grounded, mao2023crossentropylossfunctionstheoretical}. The coefficients \( (\rho, \nu) \) are typically selected through cross-validation to balance allocation performance and robustness against adversarial perturbations.


Using the \textit{smooth adversarial surrogate} family from Lemma~\ref{lemma:surrogate_class}, we define the \textit{smooth adversarial deferral surrogate} (SAD) family \( \widetilde{\Phi}_{\text{def}}^{\text{smth}, u}: \mathcal{R} \times \mathcal{G} \times \mathcal{M} \times \mathcal{Z} \to \mathbb{R}^+ \), which is convex, non-negative, and serves as an upper bound for \( \widetilde{\ell}_{\text{def}} \) by construction. The formal definition of our novel surrogates is given as:


\begin{restatable}[SAD: \textbf{S}mooth \textbf{A}dversarial \textbf{D}eferral Surrogate Losses]{lemma}{robustsurrogate} \label{lemma:surrogate}
Let \( x \in \mathcal{X} \) denote the clean input and \( \tau_j \) the aggregated cost. Then, the smooth adversarial surrogate family (or SAD)  $\widetilde{\Phi}^{\text{smth}, u}_{\text{def}}$ is defined as:
\begin{equation*}
\widetilde{\Phi}^{\text{smth}, u}_{\text{def}}(r,g,m,z) = \sum_{j=0}^J \tau_j(g(x),m(x),z)  \widetilde{\Phi}_{01}^{\text{smth}, u} (r,x, j).
\end{equation*}
\end{restatable}

% The proposed surrogate loss serves as a proxy for the \textit{adversarial true deferral loss} defined in Lemma~\ref{lemma:deferral}. Adversarial inputs are evaluated across all \( J+1 \) agents using the \textit{adversarial smooth surrogate} \( \widetilde{\Phi}_{01}^{\text{\text{smth}, u}} \), ensuring robustness against stronger attacks.

The proof is provided in Appendix~\ref{proof:surrogate}. While the \textit{smooth adversarial deferral surrogate} family (SAD) provides a smooth and computationally efficient approximation of the \textit{adversarial true deferral loss}, the question of its consistency remains a critical consideration \citep{Statistical, bartlett1}. 


% In the following subsection, we establish that our novel formulation is the first \textit{smooth adversarial deferral surrogate} to achieve consistency in the context of adversarial robustness for Learning-to-Defer.


\subsection{Theoretical Guarantees}
% In the previous subsection, we introduced the \textit{smooth adversarial deferral margin surrogate} (Lemma~\ref{lemma:deferralmargin}) as a convex proxy for the \textit{adversarial true deferral loss} (Lemma~\ref{lemma:deferral}).
To establish the theoretical foundations of SAD, we prove the Bayes-consistency and \((\mathcal{R}, \mathcal{G})\)-consistency of the \textit{adversarial margin deferral surrogate} family $\widetilde{\Phi}^{\rho, u}_{\text{def}}$ (Lemma~\ref{lemma:deferralmargin}). Furthermore, we demonstrate that these guarantees naturally extend to a regularized empirical formulation of SAD, referred to as \name.


\paragraph{\((\mathcal{R}, \mathcal{G})\)-consistency bounds of \(\widetilde{\Phi}^{\rho, u}_{\text{def}}\):}
A key foundational step involves demonstrating the \(\mathcal{R}\)-consistency of the \textit{\(j\)-th adversarial margin surrogate} losses \( \widetilde{\Phi}^{\rho,u,j}_{01} \), under the assumption that \( \mathcal{R} \) is symmetric and that there exists a rejector \( r \in \mathcal{R} \) that is \textit{locally \(\rho\)-consistent}.

\begin{restatable}[Locally \(\rho\)-consistent]{definition}{rhoconsistency}
    A hypothesis set \( \mathcal{R} \) is \textit{locally \(\rho\)-consistent} if, for any \( x \in \mathcal{X} \), there exists a hypothesis \( r \in \mathcal{R} \) such that:
    \begin{equation*}
        \inf_{x' \in B_p(x, \gamma)} |r(x', i) - r(x', j)| \geq \rho,
    \end{equation*}
    where \(\rho > 0\), \(i \neq j \in \mathcal{A}\), and \(x' \in B_p(x, \gamma)\). Additionally, for any \( x' \in B_p(x, \gamma) \), the set \(\{r(x', j) : j \in \mathcal{A}\}\) preserves the same ordering as for \( x \).
\end{restatable}

As shown in \citet{Awasthi_Mao_Mohri_Zhong_2022_multi, mao2023crossentropylossfunctionstheoretical, Grounded}, commonly used hypothesis sets, such as linear models, neural networks, and the set of all measurable functions, are locally \(\rho\)-consistent for some \(\rho > 0\). Consequently, the guarantees established in Lemma~\ref{lemma:rconsistency} are general and broadly applicable across diverse practical settings. The proof of Lemma~\ref{lemma:rconsistency} is deferred to Appendix~\ref{proof:rconsistency}.


\begin{restatable}[$\mathcal{R}$-consistency bounds for \( \widetilde{\Phi}^{\rho,u,j}_{01} \)]{lemma}{rconsistency}\label{lemma:rconsistency} 
Assume \( \mathcal{R} \) is symmetric and locally \( \rho \)-consistent. Then, for the agent set \( \mathcal{A} \), any hypothesis \( r \in \mathcal{R} \), and any distribution \( \mathcal{P} \) with probabilities \( p = (p_0, \cdots, p_J) \in \Delta^{|\mathcal{A}|} \), the following inequality holds:
\begin{equation*}
\begin{aligned}
    & \sum_{j \in \mathcal{A}} p_j \widetilde{\ell}_{01}^j(r,x,j) - \inf_{r \in \mathcal{R}} \sum_{j \in \mathcal{A}} p_j \widetilde{\ell}_{01}^j(r,x,j) \leq \\
    & \Psi^u(1) \Big( \sum_{j \in \mathcal{A}} p_j \widetilde{\Phi}^{\rho,u,j}_{01}(r,x, j) - \inf_{r \in \mathcal{R}} \sum_{j \in \mathcal{A}} p_j \widetilde{\Phi}^{\rho,u,j}_{01}(r,x, j) \Big).
\end{aligned}
\end{equation*}
\end{restatable}
Lemma~\ref{lemma:rconsistency} establishes the consistency of the \emph{\(j\)-th adversarial margin surrogate} family \( \widetilde{\Phi}^{\rho,u,j}_{01} \) for probabilities \( p_j \in \Delta^{|\mathcal{A}|} \), explicitly incorporating adversarial inputs defined for each \( j \in \mathcal{A} \). This result distinguishes our contribution from prior works \citep{mao2023crossentropylossfunctionstheoretical, Grounded, Awasthi_Mao_Mohri_Zhong_2022_multi}, which do not address adversarial inputs at the level of the distribution (dependent on \(j\)). By addressing this limitation, Lemma~\ref{lemma:rconsistency} provides a critical theoretical guarantee, demonstrating that the \emph{\(j\)-th adversarial margin surrogate} family \( \widetilde{\Phi}^{\rho,u,j}_{01} \) aligns with the adversarial loss \( \widetilde{\ell}_{01}^j \) under the specified assumptions.


Building on the foundational result of Lemma~\ref{lemma:rconsistency}, we prove the Bayes and \((\mathcal{R}, \mathcal{G})\)-consistency of the \textit{adversarial margin deferral surrogate} losses. The proof of Theorem~\ref{theo:consistency} is provided in Appendix~\ref{proof:consistency}.


\begin{restatable}[$(\mathcal{R}, \mathcal{G})$-consistency bounds of $\widetilde{\Phi}^{\rho, u}_{\text{def}}$]{theorem}{consistency}
\label{theo:consistency}
Let \( \mathcal{R} \) be symmetric and locally \( \rho \)-consistent. Then, for the agent set \( \mathcal{A} \), any hypothesis \( r \in \mathcal{R} \), and any distribution \( \mathcal{D} \), the following holds for a multi-task model \( g \in \mathcal{G} \):
\begin{equation*}
    \begin{aligned}
        & \mathcal{E}_{\widetilde{\ell}_{\text{def}}}(r, g) - \mathcal{E}_{\widetilde{\ell}_{\text{def}}}^B(\mathcal{R}, \mathcal{G}) + \mathcal{U}_{\widetilde{\ell}_{\text{def}}}(\mathcal{R}, \mathcal{G}) \leq \\
        & \quad \Psi^u(1) \Big( \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}(r) - \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}^\ast(\mathcal{R}) + \mathcal{U}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}(\mathcal{R}) \Big) \\
        & \quad + \mathcal{E}_{c_0}(g) - \mathcal{E}_{c_0}^B(\mathcal{G}) + \mathcal{U}_{c_0}(\mathcal{G}).
    \end{aligned}
\end{equation*}
\end{restatable}

Theorem~\ref{theo:consistency} establishes the consistency of the \textit{adversarial margin deferral surrogate} family \( \widetilde{\Phi}^{\rho, u}_{\text{def}} \), ensuring its alignment with the \textit{true adversarial deferral loss} \( \widetilde{\ell}_{\text{def}} \). The minimizability gaps derived in Theorem~\ref{theo:consistency} vanish  when \(\mathcal{R} = \mathcal{R}_{\text{all}}\) and \(\mathcal{G} = \mathcal{G}_{\text{all}}\) \citep{Steinwart2007HowTC, Awasthi_Mao_Mohri_Zhong_2022_multi}. Under the assumption that these gaps vanish, the following holds:
\begin{equation}
    \begin{aligned}
        & \mathcal{E}_{\widetilde{\ell}_{\text{def}}}(r, g) - \mathcal{E}_{\widetilde{\ell}_{\text{def}}}^B(\mathcal{R}, \mathcal{G}) \leq \mathcal{E}_{c_0}(g) - \mathcal{E}_{c_0}^B(\mathcal{G}) + \Psi^u(1) \Big( \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}(r) - \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}^\ast(\mathcal{R})\Big).
    \end{aligned}
\end{equation}
After offline training of the multi-task model, we assume that the excess \( c_0 \)-risk is bounded as \( \mathcal{E}_{c_0}(g) - \mathcal{E}_{c_0}^B(\mathcal{G}) \leq \epsilon_0 \). Similarly, after training the rejector, the excess risk of the surrogate satisfies \( \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}(r) - \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}^\ast(\mathcal{R}) \leq \epsilon_1 \). Under these conditions, the \(\ell_{\text{def}}\)-excess risk is bounded as \( \mathcal{E}_{\widetilde{\ell}_{\text{def}}}(r, g) - \mathcal{E}_{\widetilde{\ell}_{\text{def}}}^B(\mathcal{R}, \mathcal{G}) \leq \epsilon_0 + \Psi^u(1) \epsilon_1 \), establishing both the Bayes-consistency and \((\mathcal{R}, \mathcal{G})\)-consistency of the surrogate losses \( \widetilde{\Phi}^{\rho, u}_{\text{def}} \).

Building on the theoretical guarantees of the non-convex family \( \widetilde{\Phi}^{\rho, u}_{\text{def}} \), we introduce a \textit{smooth adversarial regularized deferral} (SARD) algorithm. SARD extends the standard SAD framework by incorporating a regularization term that enhances stability and robustness. Despite this modification, SARD preserves the key theoretical guarantees of \( \widetilde{\Phi}^{\rho, u}_{\text{def}} \), ensuring consistency and minimizability under the same conditions \citep{mao2023crossentropylossfunctionstheoretical, Grounded}.

\paragraph{Guarantees for \name:} Using the fact that \( \widetilde{\Phi}^{\text{smth},u}_{01} \geq \widetilde{\Phi}^{\rho,u,j}_{01} \), we establish guarantees for our \textit{smooth adversarial deferral surrogate} family \( \widetilde{\Phi}^{\text{smth}, u}_{\text{def}} \) under the same conditions.
\begin{corollary}[Guarantees for SAD] \label{consistencysmooth}
Assume \( \mathcal{R} \) is symmetric and locally \( \rho \)-consistent. Then, for the agent set \( \mathcal{A} \), any hypothesis \( r \in \mathcal{R} \), and any distribution \( \mathcal{D} \), the following holds for a multi-task model \( g \in \mathcal{G} \):
\begin{equation*}
    \begin{aligned}
        & \mathcal{E}_{\widetilde{\ell}_{\text{def}}}(r, g) - \mathcal{E}_{\widetilde{\ell}_{\text{def}}}^B(\mathcal{R}, \mathcal{G}) + \mathcal{U}_{\widetilde{\ell}_{\text{def}}}(\mathcal{R}, \mathcal{G}) \leq  \\
        & \quad + \Psi^u(1) \Big( \mathcal{E}_{\widetilde{\Phi}^{\text{smth}, u}_{\text{def}}}(r) - \mathcal{E}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}^\ast(\mathcal{R}) + \mathcal{U}_{\widetilde{\Phi}^{\rho, u}_{\text{def}}}(\mathcal{R}) \Big) \\
        & \quad + \mathcal{E}_{c_0}(g) - \mathcal{E}_{c_0}^B(\mathcal{G}) + \mathcal{U}_{c_0}(\mathcal{G}).
    \end{aligned}
\end{equation*}
\end{corollary}

Corollary~\ref{consistencysmooth} establishes that SAD shows similar consistency properties under the given conditions, with the minimizability gap vanishing for \( \mathcal{R} = \mathcal{R}_{\text{all}} \) and \( \mathcal{G} = \mathcal{G}_{\text{all}} \). This motivates the development of an adversarial robustness algorithm based on minimizing a regularized empirical formulation of SAD, referred to as \name{}.

\begin{proposition}[\name: \textbf{S}mooth \textbf{A}dversarial \textbf{R}egularized \textbf{D}eferral Algorithm] \label{radvl2d}
Assume \( \mathcal{R} \) is symmetric and locally \( \rho \)-consistent. For a regularizer \( \Omega \) and hyperparameter \( \eta > 0 \), the regularized empirical risk minimization problem for \name{} is:
\[
\min_{r \in \mathcal{R}} \Bigg[ \frac{1}{K}\sum_{k=1}^K \widetilde{\Phi}^{\text{smth}, u}_{\text{def}}(r, g, m, z_k) + \eta \Omega(r) \Bigg].
\]
\end{proposition}

The pseudo-code for \name{} is provided in Appendix~\ref{appendix:algo}.


% In the next section, we evaluate R-ADVs-L2D on classification, regression, and multi-task settings, demonstrating its effectiveness in achieving adversarial robustness.
