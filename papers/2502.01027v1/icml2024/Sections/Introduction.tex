
\section{Introduction}

Learning-to-Defer (L2D) is a powerful framework that enables decision-making systems to optimally allocate queries among multiple agents, such as AI models, human experts, or other decision-makers \citep{madras2018predict}. In the \emph{two-stage} framework, agents are trained offline to harness their domain-specific expertise, enabling the allocation of each task to the decision-maker with the highest confidence \citep{mao2023twostage, mao2024regressionmultiexpertdeferral, montreuil2024twostagelearningtodefermultitasklearning}. L2D is particularly valuable in high-stakes applications where reliability and performance are critical \citep{mozannar2021consistent, Verma2022LearningTD}. In healthcare, L2D systems integrate an AI diagnostic model with human specialists, delegating routine tasks to AI while deferring edge cases, such as anomalous imaging data, to specialists for nuanced evaluation \citep{Mozannar2023WhoSP}. By dynamically assigning tasks to the most suitable agent, L2D ensures both accuracy and reliability, making it ideal for safety-critical domains.


Robustness in handling critical decisions is, therefore, essential for such systems. However, existing L2D frameworks are typically designed under the assumption of clean, non-adversarial input data, leaving them highly susceptible to adversarial perturbationsâ€”subtle input manipulations that disrupt task allocation and alter decision boundaries. Unlike traditional machine learning systems, where adversarial attacks primarily affect prediction outputs \citep{goodfellow2014explaining, szegedy2014intriguingpropertiesneuralnetworks, Grounded}, L2D systems are susceptible to more sophisticated adversarial threats, including query redirection to less reliable agents or intentional agent overloading. These vulnerabilities severely impact performance, drive up operational costs, and compromise trust.

This paper addresses the critical yet unexplored challenge of adversarial robustness in L2D systems. Inspired by adversarial attacks on classification \citep{goodfellow2014explaining, Madry2017TowardsDL, Gowal2020UncoveringTL}, we introduce two novel attack strategies tailored to \emph{two-stage} L2D: \textit{untargeted attacks}, which disrupt agent allocation, and \textit{targeted attacks}, which redirect queries to specific agents. To counter these attacks, we propose a robust family of surrogate losses based on cross-entropy \citep{mao2023twostage, mao2024regressionmultiexpertdeferral, montreuil2024twostagelearningtodefermultitasklearning}, designed to ensure robustness in classification, regression, and multi-task settings. Building upon advances in consistency theory for adversarial robustness \citep{bao2021calibratedsurrogatelossesadversarially, Awasthi_Mao_Mohri_Zhong_2022_multi, Grounded, mao2023crossentropylossfunctionstheoretical}, we establish both Bayes-consistency and $(\mc{R},\mc{G})$ consistency for our surrogate losses, enabling reliable task allocation even under adversarial scenarios. Our algorithm, \textbf{\name}, leverages these guarantees while preserving convexity.


Our key contributions are:
\begin{enumerate}
    \item We introduce two novel adversarial attack strategies for \emph{two-stage} L2D: \textit{targeted} attacks that redirect queries and \textit{untargeted} attacks that disrupt task allocation.
    \item We propose a robust family of surrogate losses with guarantees of Bayes-consistency and $(\mc{R}, \mc{G})$ consistency across classification, regression, and multi-task settings. Our convex algorithm, \name, leverages these guarantees to achieve robust task allocation.
    \item We empirically demonstrate that our novel attacks effectively exploit vulnerabilities in state-of-the-art \emph{two-stage} L2D approaches, while our algorithm, \name, exhibits strong robustness against the attack across diverse tasks.
\end{enumerate}



This work lays the first theoretical foundation for adversarial robustness in L2D systems.
