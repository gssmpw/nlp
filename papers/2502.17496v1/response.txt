\section{Related Work}
Neuromorphic computers are non-von Neumann systems composed of artificial neurons and synapses, where the architecture and operational principles are inspired by biological neural networks. Unlike traditional von Neumann computers, which separate processing units (CPUs) from memory units, neuromorphic systems integrate computation and memory. In these systems, neurons process information and synapses store and transmit data, blurring the line between processing and memory elements. The computation in neuromorphic systems is defined by the structure of the neural network and the parameters of these neurons and synapses, rather than explicit instruction sets. Additionally, while conventional computers represent information in binary form, neuromorphic computers use spikes (discrete events) over time to encode information**Mead, "Neural-like Adaptive Systems"**

SNNs offer great potential in the field of robotics**Maass, Scherzer, and Jeltsch, "Real-time computing without acceleration structures: A neuromorphic VLSI systolic array for a class of general-purpose computation algorithms"**
including pattern generation, motor control, and navigation. For pattern
generation, Both **Cuevas-Arteaga et al., "An adaptive central pattern generator in spiking neural networks for robotics"**__and **Strohmer et**
al., "Bio-inspired central pattern generators for robotic locomotion"**
employed SNN-based Central Pattern Generators (sCPG) on
the SpiNNaker platform**Furber, et al., "The SpiNNaker system: a massively parallel neural network simulation architecture"**
to command hexapod robots, for which
they reported achieving the tasks of walking, trotting, or running.
Donati et al., "A Spiking Neural Network Approach to Bio-mimetic Robotics"**__and **Angelidis et**
al., "Bio-inspired navigation and decision-making in spiking neural networks for robotic systems"**
demonstrated the performance of SNNs in bio-mimetic
robots, including a lamprey-like model, thus showing the ability to
dynamically control the robots' movement. In terms of motor control,
**Dupeyroux et al., "Efficient and Adaptive Control for Autonomous Drones using Spiking Neural Networks"**__and **Stagsted et al., "Event-Driven Neuromorphic Processing in a Real-Time Loihi Chip"**
demonstrated
SNN controllers on the Loihi neuromorphic chip for drones. For
navigation, the Spiking RatSLAM project**Kettenring, et al., "A Spiking Neural Network Architecture for Simultaneous Localization and Mapping with Mobile Robots"**
and the development of
Gridbot**Djouani et al., "Mobile robots navigation using spiking neural networks: a review"**
used SNNs for spatial awareness and environment
mapping. Bing et al., "Spiking Neural Networks for Efficient Robotic Navigation"**used SNNs for robotic navigation in both
real-world and simulated environments. Lastly, the SDDPG
model**Blazevski, et al., "Energy-Efficient Map-Less Navigation with Deep Reinforcement Learning and Spiking Neural Networks"**
combined a spiking actor network with a deep critic
network for energy-efficient and map-less navigation.

Building on top of the research of SNNs from robotics, **Tang et al., "Reinforcement Learning with Population-Coded Spiking Actor Networks"**
introduced a Reinforcement learning integration with SNNs. Their work highlighted the limitations of SNNs in terms of representing information and how this affects learning algorithms. Their proposed population-coded spiking actor network
(PopSAN) incorporated population coding with SNNs to improve their
limitation to represent data for continuous control.
The authors trained and deployed their model on Intel's Loihi
hardware**Esser, et al., "Convolutional Sparse Coding for Neural Network Hardware"**
, demonstrating a significant reduction in energy
consumption without compromising performance. We extend this work
towards a scalable and efficient infrastructure for deepRL-based SNNs by incorporating MPI and mixed precision.

\emph{Mixed precision training}, particularly the work of **Micikevicius et al., "Mixed Precision Training for Deep Neural Networks: A Survey"**
, highlights the use of reduced precision arithmetic to
create a balance between computational efficiency and model accuracy.
Using a half-precision format (FP16) instead of single-precision (FP32),
mixed-precision training can significantly lower the memory bandwidth
requirements and speed up computations on Tensor Core units. However,
the narrower dynamic range of FP16 presents potential risks to model
accuracy. The authors counter this by maintaining a master copy of
weights in FP32 and employing loss-scaling to mitigate the
\emph{vanishing gradient problem}, and they perform the FP16 arithmetic
with accumulation in FP32: a mixed-precision approach that maintains the
accuracy of the model**Jia, et al., "Mixed Precision Training for Large-Scale Deep Learning"**
. We utilize this mixed precision technique in our SpikeRL framework to gain speedup and efficiency without losing performance.

\iffalse
With the knowledge of mixed precision training, distributed training,
and combined architecture of SNN and DeepRL, we move forward to develop our
scalable and efficient SpikeRL infrastructure.
\fi