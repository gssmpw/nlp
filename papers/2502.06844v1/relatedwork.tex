\section{Related Work}
\textbf{Integer quantization.} In this paper, we focus on integer quantization, where model weights are stored as integers~\citep{Jacob_2018_CVPR,wu2020integer}.
This is one of the most popular quantization schemes because it offers strong performance, high memory savings, as well as hardware efficiency with integer-only arithmetic during inference in certain scenarios~\citep{Jacob_2018_CVPR,awq}.
Earlier work in this regime has explored binary networks that can only represent $\pm 1$, or ternary networks which additionally include the $0$ value~\citep{NIPS2015_3e15cc11,lin2015neural}. However, they only work with small models for simple tasks.
Researchers typically use more bits (e.g., 4--8 bits) in integer quantization to tackle real-world tasks~\cite{krishnamoorthi2018quantizing,NEURIPS2022_c3ba4962,gptq}, but the ultra-low-bit setup remains challenging. 

Low-bit integers are usually unable to cover the range of real-valued weights. Therefore, a scale parameter is introduced to map the low-bit integer range to the original weight range~\cite{zhou2016dorefa,Jacob_2018_CVPR}. Also, it is important to keep the exact 0 value for weights, so a zero-point parameter is also introduced~\cite{Jacob_2018_CVPR}. Such offset and scaling are applied at the granularity of groups (contiguous weights in a matrix). Although the scale and zero-point parameters use additional memory, they drastically improve performance and are commonly used in modern integer quantization studies~\citep{gptq,awq,omniquant}.


One direction of integer quantization is to quantize weights sequentially~\cite{obq,gptq}, inspired by the classic pruning methods~\citep{obs}. The idea is that, given already quantized weights, the rest weights are finetuned to compensate the quantization error before also being quantized. However, the weight updates are computed in a closed form based on second-order gradient information; this is done for each layer separately, which does not consider the dependencies among layers.

An alternative technique in integer quantization is to leverage model invariance, which adjusts the parameters for better quantization performance without impacting the (un-quantized) model's behavior. Previous work has explored various heuristics. For example, \citet{awq} determine the scaling coefficients based on the magnitude of the activations. \citet{lin2024duquant} manually design a zigzag permutation pattern in an attempt to distribute outlier weights throughout the network.
On the other hand, researchers also use gradient-based methods to learn scaling coefficients~\citep{omniquant} and orthogonal transformations~\citep{spinquant}.
However, this requires a straight-through gradient estimation and special treatment for the constraint, as mentioned in Section~\ref{sec:intro}.

Different from previous work, our paper proposes the  \method\ framework that explores permutation, scaling, and rotation invariance for integer quantization. We further propose a discrete search algorithm that can optimize different types of invariance jointly. 

\textbf{Other representations for quantization.} There are also other ways to represent model weights in a low-bit fashion, such as floating-point quantization~\citep{10.1145/103162.103163,micikevicius2018mixed} and vector quantization~\citep{1162229,gong2015compressing}.

Floating-point quantization mostly follows the widely adopted IEEE 754 standard~(\citeyear{4610935}), where values are represented with a sign bit for polarity, exponent bits for magnitude, and mantissa bits for precision.
The exponent bits enable the floating-point representation to cover a wide range of values, which is especially valuable during training~\citep{NEURIPS2018_335d3d1c}.
To save memory, \citet{micikevicius2018mixed} propose mixed-precision training, which has inspired a line of research focusing on designing different bit representations suitable for training~\citep{NEURIPS2018_335d3d1c,NEURIPS2019_65fc9fb4,NEURIPS2020_13b91943}.
Researchers have also explored adaptive methods, which dynamically adjust the floating-point representation~\citep{NIPS2017_a0160709,Liu_2021_ICCV}. However, these methods usually lack hardware support.
In general, floating point is not suitable for the ultra-low-bit setting of our paper, as we cannot afford to allocate any bits for the exponent.

On the other hand, vector quantization compresses model weights by storing a set of representative vectors, also known as a codebook~\citep{gong2015compressing,han2016deepcompression}.
Recent work such as QuiP\#~\citep{quipsharp} and AQLM~\citep{aqlm} show promising results in this direction, but this is beyond the scope of this paper.

\textbf{Weight-only and weight--activation quantization.}
Quantization can be applied to weights only or both weights and activations.
As shown by previous work, quantizing activation is even harder than quantizing weights, because of the emergence of outlier features in large language models~\citep{NEURIPS2022_c3ba4962,smoothquant}. Weight--activation quantization may use hardware integer arithmetic for fast inference, but it is significantly more challenging, with state-of-the-art methods still requiring 4--6 bits~\citep{wei-etal-2023-outlier,yuan2023rptq,omniquant}. Notice that, although our paper focuses on the weight-only setup for ultra-low-bit quantization, it does not use more memory when storing the model because the activation values are temporary variables during inference.