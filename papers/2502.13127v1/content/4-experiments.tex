\section{Experiments}

\input{assets/tables/pai_results}
\input{assets/tables/longpai_results}

\subsection{Experimental Setup}\label{sec:4.1}

\noindent \textbf{Evaluation Benchmarks}.
We evaluate long-context understanding using two practical benchmarks: Loong~\cite{wang2024leave} and $\infty$Bench~\cite{zhang2024bench}. Loong focuses on real-world multi-document question answering and comprises 1,600 test samples across four categories, including Spotlight Locating, Comparison, Clustering, and Chain of Reasoning. These tasks assess distinct capabilities in handling long-context tasks. $\infty$Bench facilitates multilingual evaluation, assessing models on English (En.QA) and Chinese (Zh.QA) question-answering tasks that require long-range dependency and reasoning beyond short-passage retrieval.

\noindent \textbf{Evaluation Metrics}. For evaluation, Loong employs GPT-4-Turbo as a judge, scoring model responses based on accuracy, hallucinations, and completeness on a scale of 0 to 100. Meanwhile, it introduces the Perfect Rate, measuring the proportion of responses achieving a perfect score. In $\infty$Bench, model performance is measured by the F1 score~\cite{zhang2024bench}. 

\noindent \textbf{Base Models}. 
We adopt GPT-4o-mini~\cite{achiam2023gpt} and LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} as our base models. Specifically, GPT-4o-mini serves as the agent of the proposed Preference Agentic Inference (PAI), while LLaMA-3.1-8B is used as the base model for LongPAI, which is fine-tuned on the LongFinanceQA dataset.


\noindent \textbf{Implementation Details}.
To enable training on long sequences (> 250K), we employ several optimization techniques, including flash-attention-2~\cite{dao2023flashattention} and ring-attention~\cite{liu2024ringattention}. Furthermore, we adopt a zigzag sharding approach~\cite{zhu2024ringflash} within ring attention for more effective load distribution across multiple GPUs. This training setup allows us to fine-tune the large language models fully.
Using 8 A100 GPUs, the long-context training is completed in three days for fine-tuning.
In addition, in training the long-context LLMs, we adopt the rotary base scaling approach~\cite{liu2024scaling} and scale up the base value (\ie, rotary base of 1,247,820) to adapt RoPE to a longer context.
For optimization, we use a constant learning rate of 2e-6 for the entire training procedure.
Following the common practice~\cite{fu2024data}, we set the batch size to 16M tokens as mentioned in \cite{dubey2024llama}.
During inference, we set the temperature as zero to eliminate the randomness. We also increase the maximum output tokens to 1,024 since the CoT reasoning requires more output tokens.


\subsection{Main Results}\vspace{-2mm}
In this section, we first assess the quality of our long-context synthetic data (LongFinanceQA) by evaluating the performance of the proposed PAI on the Loong benchmark. Next, we compare the enhanced long-context language model (LongPAI) with its base LLaMA-3.1 model and other state-of-the-art LLMs on the \textit{Finance} subset of Loong.

\noindent \textbf{Data Quality Assessment}.
Reasoning-augmented answers in LongFinanceQA are automatically generated by the PAI framework. To assess the quality of these synthetic answers, we evaluate the annotator, PAI, on the Long benchmark, using GPT-4o-mini as the agent within the PAI, referred to as GPT-4o-mini w/ PAI.
Table~\ref{tab:pai_results} shows that the GPT-4o-mini-based PAI framework significantly enhances the base model’s performance, demonstrating the effectiveness of the PAI.
In particular, compared to the standard GPT-4o-mini model, the overall average score improves by 20.3\%, with substantial gains in key tasks such as spotlight locating (+20.2\%), comparison (+15.7\%), clustering (+29.2\%), and chain of reasoning (+10.2\%).
Although the basic GPT-4o-mini is not the strongest model, GPT-4o-mini w/ PAI outperforms the state-of-the-art closed-source model, Geneni-1.5-pro~\cite{reid2024gemini}, by over 15\%. 
Moreover, the superior performance in the Spotlight Locating task (\ie, single-source QA task) \textit{highlights the quality of intermediate reasoning results generated by the PAI}, as predictions in this task serve as essential reasoning steps for the other three multi-source QA tasks.
Consequently, the strong performance on the Loong demonstrates \textit{the capability of PAI as a reliable annotator, ensuring high-quality synthetic data in long-context scenarios}.


\noindent \textbf{Method Comparisons on Existing Benchmarks}.
To evaluate the effectiveness of supervised CoT reasoning 
on long-context modeling, we measure the performance of the enhanced LongPAI model on two well-known long-context understanding benchmarks, including Loong and $\infty$Bench. 
First, we evaluate the LongPAI on an in-domain benchmark, namely the \textit{Finance} subset of Loong.
Table~\ref{tab:longpai_results} presents that supervised CoT reasoning enables LongPAI to outperform its base model, LLaMA-3.1-8B-Instruct, by 24.6\% in overall results. Furthermore, LongPAI exhibits nearly a 30\% improvement on subsets with longer content (\ie, 200K-250K). Also, LongPAI achieves a competitive performance against existing state-of-the-art language models. Remarkably, LongPAI is comparable to its teacher model, GPT-4o-mini w/ PAI. In some cases, it even surpasses its teacher model. This finding highlights the significance of long-context modeling. Meanwhile, this finding strongly challenges the recent claim that \textit{the long-context problem can be adequately addressed by short language models}~\cite{qian2024long,chen2024long}. In other words, certain long-context problems require long-context modeling, as short language models struggle to analyze and reason effectively over reduced or retrieved information. At the same time, we evaluate the LongPAI on $\infty$Bench. The results in Table~\ref{tab:infbench} show that the LongPAI outperforms its base model even on the out-domain benchmark.


\input{assets/tables/infbench}
\input{assets/tables/cot_ablation}


\subsection{Discussion}
\noindent \textbf{Ablation Study on Supervised CoT Reasoning}. 
To further analyze the impact of supervised CoT reasoning, we fine-tune the base LLaMA-3.1 model on LongFinanceQA while excluding CoT reasoning steps from the augmented answers, resulting in a new model, LongPAI$^{\S}$.
Unlike the original LongPAI, LongPAI$^{\S}$ directly predicts the final answer, skipping intermediate reasoning steps.
Table~\ref{tab:cot_ablation} shows that LongPAI significantly outperforms LongPAI$^{\S}$ over different input lengths. This result strongly supports the hypothesis mentioned in Section~\ref{sec:intro}, which argues that \emph{directly guiding models to generate brief answers without intermediate reasoning steps for long-context modeling will lead to suboptimal training} (\textbf{finding 1}).
Moreover, Table~\ref{tab:cot_ablation} presents several interesting finding.
First, while LongPAI$^{\S}$ performs comparably to LongPAI on short content (10K–50K tokens), its performance declines significantly on longer content, which means \emph{CoT reasoning is necessary for long-context modeling} (\textbf{finding 2}). Furthermore, LongPAI$^{\S}$ performs well in single-source questions (Spotlight Locating) but struggles with multi-source questions (Comparison, Clustering, and Chain of Reasoning) as input length increases, demonstrating that \emph{CoT reasoning benefits complex long-context problem-solving} (\textbf{finding 3}). In sum, all these findings reaffirm the importance of the reasoning capability for long-context modeling.

\noindent \textbf{Comparison of Various Inference Frameworks}. We compare PAI with two relevant inference frameworks: PAI$^-$ and RAG.
PAI$^-$ is a variant of PAI that generates sub-questions directly, rather than first extracting properties and then generating sub-questions.
Table~\ref{tab:inference_comparison} presents that PAI$^-$ generally outperforms the base GPT-4o-mini, except on $\mathtt{Set 1}$. However, there is a gap between PAI$^-$ and PAI, highlighting the superiority of the Property Extraction Agent.
Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} follows a two-step process: it first retrieves the top $K$ chunks relevant to a given query, and then uses retrieved chunks to generate an answer. Following Loong~\cite{wang2024leave}, we adopt the \textit{BGE}~\cite{chen-etal-2024-m3} as the embedding choice and set $K$ to 50, selecting from 5, 10, 30, and 50. Table~\ref{tab:inference_comparison} shows that RAG struggles with long-context problems, a conclusion also reached by \cite{wang2024leave}.


\input{assets/tables/inference_comparison}

\noindent \textbf{Efficiency Analysis on PAI}. The PAI framework relies on multi-step inference, whereas LongPAI achieves results with a single inference step. We adopt the number of input tokens processed on the Loong benchmark (\ie, 1,600 samples) as a metric to compare the efficiency between PAI and LongPAI. In this comparison, PAI processes 3.53B tokens in total, determined by the GPT-4o tokenizer, whereas LongPAI requires only 112M tokens—less than 3\% of PAI's total. Despite PAI delivering the strongest overall performance, LongPAI stands out as the far more efficient approach, demonstrating a dramatic reduction in computational cost.
