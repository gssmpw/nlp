\section{Methodology}
In this section, we first introduce the problem formulation of the QA task enhanced by intermediate CoT reasoning in Section~\ref{sec:3.1}. Then, we present the procedure of LongFinanceQA dataset construction in Section~\ref{sec:3.2}. After that, we describe the fine-tuning details in Section~\ref{sec:3.3}.


\subsection{Problem Formulation} \label{sec:3.1}
Traditional QA tasks require language models to generate outputs $\mathbf{A}$ directly from a given query $\mathbf{Q}$ and the corresponding input content $\mathbf{X}$ by modeling the conditional probability:
\begin{equation}
    p_{\theta}(\mathbf{A}|\mathbf{X},\mathbf{Q}),
\end{equation}
where $\theta$ is the parameters of language models.

Compared to traditional QA tasks, practical long-context QA tasks often require intermediate reasoning steps to analyze multiple pieces of evidence across long documents, and then derive the final answer. Thus, we formalize long-context QA tasks as a joint conditional probability of intermediate reasoning results $\mathbf{R}$ and the final answer $\mathbf{A}$:
\begin{equation}
    p_{\theta}(\mathbf{R}, \mathbf{A}|\mathbf{X},\mathbf{Q}).\label{eq:algo}
\end{equation}
As shown in Eq.(\ref{eq:algo}), this study aims to facilitate long-context understanding by guiding LLMs to first predict intermediate CoT reasoning steps before generating the final answer.
To achieve this, we construct a long-context synthetic dataset that explicitly supervises language models in learning intermediate reasoning. The data construction process can be factorized as follows:
\begin{equation}\small
    p_{\theta}(\mathbf{R}, \mathbf{A}|\mathbf{X},\mathbf{Q}) = p_{\theta}(\mathbf{R}|\mathbf{X},\mathbf{Q}) \cdot p_{\theta}(\mathbf{A}|\mathbf{X},\mathbf{Q},\mathbf{R}),\label{eq:data}
\end{equation}
where $p_{\theta}(\mathbf{R}|\mathbf{X},\mathbf{Q})$ indicates the generation of intermediate reasoning steps $\mathbf{R}$ given a query $\mathbf{Q}$ and input content $\mathbf{X}$, and then $p_{\theta}(\mathbf{A}|\mathbf{X},\mathbf{Q},\mathbf{R})$ is the process of incorporating the query and summarizing reasoning steps to produce the answer $\mathbf{A}$. The data construction follows the principle of Eq.(\ref{eq:data}).

\input{assets/figures/pai}

\subsection{LongFinanceQA Dataset} \label{sec:3.2}\vspace{-1mm}
LongFinanceQA dataset is designed to generate practical long-context QA pairs with reasoning steps to effectively analyze long content. The finance domain was chosen for several reasons. First, annual financial reports are readily accessible and present complex long-context reasoning challenges. Moreover, finance is a data-driven field where accurate insights can drive critical decisions, making advancements in AI particularly valuable for real-world applications. We will introduce the data construction pipeline of LongFinanceQA as follows.

\noindent \textbf{Data Collection}. To begin, we collect bilingual financial annual reports (\ie, English and Chinese) dated before 2022 from open-source official websites, specifically the {SEC-10-K}\footnote{https://www.sec.gov/} and {cninfo}\footnote{http://www.cninfo.com.cn/} platforms. Documents from companies included in the Loong benchmark are then excluded. Next, we filter reports based on a token length range of 20K to 80K, determined using the GPT-4o tokenizer. Moreover, we prioritize companies with consistent annual reports over the years. In the end, 6,911 bilingual financial reports are selected.

\noindent \textbf{Diverse Query Generation}.
We first construct a financial metric pool containing key metrics commonly found in financial reports, such as profit, revenue, and cash flow. Then, we randomly pick several metrics in the metric pool and select a combination of financial reports. Given these financial metrics and the metadata from the selected documents (\eg, company name and year), we generate various long-context questions that require either single- or multi-source evidence. 
Next, we filter out questions whose corresponding combined documents exceed 256K tokens, as this surpasses the maximum token limit of our model. Finally, we obtain 46,457 practical long-context questions.
Please refer to Appendix~\ref{sec:appendix} for more details.

\noindent \textbf{Augmented Answer Generation with CoT Reasoning}.
Given long-context questions and their corresponding documents, our goal is to generate answers with CoT reasoning, following the principle of Eq. (\ref{eq:data}). In particular, we first generate step-by-step reasoning based on the question and documents, then integrate the reasoning steps into a conclusion.
Generally, long-context questions are challenging as they require models to retrieve multiple pieces of evidence scattered throughout long content and then integrate them for a global understanding~\cite{wang2024leave,edge2024local}. Inspired by this phenomenon, we intend to first extract key evidence points that support answering the question, then retrieve relevant information based on these points, and finally aggregate them into the conclusion. We term these supporting evidence points as ``\textit{Properties}''.
Following this methodology, we propose the \textbf{Property-driven Agentic Inference (PAI)} framework, illustrated in Figure~\ref{fig:pai}, containing three steps: (1) property extraction, (2) property-based retrieval, and (3) summarization.

\textbf{Step 1: Property Extraction}. This step focuses on extracting a set of properties $\{\bp_i\}^{N_p}_{i=1}$ from the given query $\mathbf{Q}$, where $N_p$ denotes the number of properties. Each property consists of a metric (a measurable factor being analyzed) and its corresponding subject mentioned in the query $\mathbf{Q}$. For instance, given the query shown in Figure~\ref{fig:pai}, the metric is ``\textit{revenue}'' and subjects are ``<Company A>'s annual reports from different years (\eg, 2018, 2020, and 2022)'', which serve as the sources of information to determine the trend.

\textbf{Step 2: Property-based Retrieval}. After extracting the properties $\{\bp_i\}^{N_p}_{i=1}$ from the given query, this step aims to retrieve relevant information based on these properties. Specifically, each property is first transformed into a sub-query $\bq_i$, which is then matched against relevant content chunks. These chunks are derived from the original long documents $\mathbf{X}$, with each chunk limited to 1,024 tokens. Based on the sub-queries and their corresponding retrieved chunks, we can derive several intermediate findings (\ie, sub-answers).

\textbf{Step 3: Summarization}. In this final step, the original query $\mathbf{Q}$ and all intermediate findings from the second step are integrated to generate a comprehensive conclusion $\mathbf{A}$.

\input{assets/figures/cot_stat}
The PAI serves as a low-cost annotator for the LongFinanceQA dataset. Using PAI, we produce augmented answers with CoT reasoning by combining the intermediate reasoning results from the first two steps with the conclusion.
Figure~\ref{fig:cot_stat} illustrates the token length difference between answers with and without CoT reasoning, showing that reasoning-augmented answers are nearly 200 tokens longer on average.\footnote{The answers are tokenized by the LLaMA-3.1 tokenizer.} Please check out more data statistics in the Appendix~\ref{sec:appendix}.

\vspace{-2mm}
\subsection{Supervised CoT Reasoning} \label{sec:3.3}\vspace{-1mm}
Although PAI can behave like a human in long-context scenarios, it requires human-crafted design and multi-step inference.
To address this limitation, we seek to transfer the long-context reasoning capability of PAI to existing language models, enabling them to analyze long content in a single-step inference.
To this end, we fine-tune a language model on LongFinanceQA to explicitly guide it in learning CoT reasoning. This fine-tuning procedure is termed as \textit{Supervised CoT Reasoning}.
Specifically, we use LLaMA-3.1-8B-Instruct~\cite{dubey2024llama} as the base language model. Following~\cite{fu2024data}, we first extend the context window of LLaMA-3.1 from 128K to 262K through continued pretraining on 1.6B tokens, which consist of packed documents from Slimpajama~\cite{cerebras2023slimpajama}. After extending context length, we fine-tune the extended LLaMA-3.1 model (parameterized by $\theta$) on LongFinanceQA with reasoning-augmented answers.

Through this process, we obtain an enhanced LongPAI model by maximizing the log-likelihood of the predicted answer $\mathbf{Y}$ along with the CoT reasoning $\mathbf{R}$, conditioned on the given long documents $\mathbf{X}$ and the query $\mathbf{Q}$. 
The objective is calculated as:
\begin{equation}
\mathcal{L(\theta)} = \sum_{i=1}^{N_r+N_y}\log p_{\theta}(\mathcal{\mathbf{R}, \mathbf{Y} | \mathbf{X}, \mathbf{Q}}),
\end{equation}
where $N_r$ is the number of reasoning tokens and $N_y$ indicates the answering token, including those in the properties, sub-answers, and final answers. We mask non-answer positions during the fine-tuning, allowing for more efficient learning.
Although we only consider the LLaMA-3.1 model here, LongFinanceQA can also be used to fine-tune other language models. 
Please refer to Section~\ref{sec:4.1} for more implementation details.
