\appendix

\section{Appendix} \label{sec:appendix}


\subsection{Trend of Long-Context LLMs}
In this section, we present the context window sizes of closed-source and open-source LLMs in Figure~\ref{fig:trend}. It indicates the gap between closed-source and open-source LLMs becomes smaller. In particular, the LLMs listed in Figure~\ref{fig:trend} include T5~\cite{raffel2020exploring}, GPT-3~\cite{brown2020language}, Codex~\cite{chen2021evaluating}, T0~\cite{sanh2022multitask}, Anthropic~\cite{priyanshu2024ai}, InstructGPT~\cite{ouyang2022training}, CodeGen~\cite{nijkamp2023codegen}, PaLM~\cite{chowdhery2023palm}, U-PaLM~\cite{tay2022transcending}, Flan-T5~\cite{chung2024scaling}, GPT-3.5 Turbo, LLaMA~\cite{touvron2023llama}, GPT-4~\cite{achiam2023gpt}, Claude 1.3, CodeGen2~\cite{nijkamp2023codegen2}, PaLM2~\cite{anil2023palm}, Claude 2, LLaMA-2~\cite{touvron2023llama}, Qwen~\cite{bai2023qwen}, Kimi-Chat-200K, Yi-34B~\cite{young2024yi}, Falcon 180B~\cite{almazrouei2023falcon}, Gemini Ultra~\cite{team2023gemini}, Mixtral~\cite{jiang2024mixtral}, Qwen1.5~\cite{qwen2024qwen1-5}, Gemini 1.5 Pro, Claude 3.0~\cite{anthropic2024claude3}, Kimi-Chat-2M, GPT-4 Turbo~\cite{achiam2023gpt}, LLaMA-3~\cite{dubey2024llama}, GPT-4o~\cite{hurst2024gpt}, Qwen2~\cite{yang2024qwen2}, GPT-4o-mini~\cite{hurst2024gpt}, Mistral NeMo~\cite{mistral2024mistralnemo}, LLaMA-3.1~\cite{dubey2024llama}, Mistral Large 2~\cite{mistral2024mistrallarge}, o1, Qwen-2.5~\cite{yang2024qwen2-5}, Gemini 2.0~\cite{pichai2024gemini2}, DeepSeek-V3~\cite{liu2024deepseek}, DeepSeek-R1~\cite{guo2025deepseek}, Qwen-2.5-1M~\cite{yang2024qwen2-5}, and o3-mini.



\subsection{Prompts of Property-driven Agentic Inference (PAI)}
In this section, we present the prompts for the three agents in the PAI framework: the property extraction agent, the property-based retrieval agent, and the summarization agent.



\noindent \textbf{Property Extraction Agent}.
To extract properties, we employ the function-calling API of GPT-4o-mini to selectively retrieve the relevant metric and its corresponding subject from a given query. Figure~\ref{fig:pai_fc} illustrates the property extraction process using function calling.
In addition, we incorporate domain-specific examples within the function call to enhance accuracy across different domains. For instance, in the finance domain, we use ``profit'', ``revenue'', and ``debt'' as metric examples, while the ``financial document title'' serves as a subject example. In the legal domain, ``verdict'' represents the metric, and ``legal judgment'' serves as the subject. Similarly, in the academic domain, ``reference'' and ``citation'' are used as metric examples, with ``paper title'' as the subject.


\input{assets/figures/pai_fc}


\input{assets/figures/long-context-trend}


\noindent \textbf{Property-based Retrieval Agent}. After obtaining the metric and its corresponding subject, we generate a sub-question in the format: ``What was the <metric> of the <subject>?''. Then, the long-context input document is divided into a list of 1024-token chunks. Each chunk is evaluated to determine its relevance to the sub-questions, as shown in Figure~\ref{fig:pai_fc} (\textit{bottom}). After that, each sub-question is assigned relevant chunks. Next, we pack these relevant chunks and generate a sub-answer to the corresponding sub-question. Thus, this agent functions similarly to RAG~\cite{lewis2020retrieval}.

\noindent \textbf{Summarization Agent}. Given the original query, the summarization agent summarizes a conclusion based on the sub-answers generated by the property-based retrieval agent.


\input{assets/figures/data_stat}


\input{assets/figures/query_gen}

\subsection{LongFinanceQA Dataset Statistics}
Figure~\ref{fig:data_stat} presents a histogram of input token lengths in the LongFinanceQA dataset, which generally range from 50K to 250K tokens. Most input documents contain approximately 200K tokens. At the same time, we provide the proportion of single-source and multi-source QA pairs in the proposed LongFinanceQA dataset, where single-source QA pairs make up 45.6\%, while multi-source QA pairs account for 54.4\%.

\input{assets/figures/case_studies}

\subsection{Synthetic Question Generation}
Figure~\ref{fig:query_gen} illustrates the procedure of synthetic Question generation. First, We create a financial metric pool with key metrics like profit, revenue, and cash flow (represented as the grey box). Then, we randomly select metrics and metadata of financial reports (\eg, company name and year) to generate long-context questions requiring single- or multi-source evidence (represented as the blue box). In addition, the types of multi-source questions are inspired by Loong~\cite{wang2024leave} and include comparison, clustering, trend analysis, etc.

\subsection{More Empirical Results}


\noindent \textbf{Case Study on Practical Long-Context Problems}.
As shown in Figure~\ref{fig:case_study}, we present two representative long-context QA examples. Both questions are particularly challenging, as they require models to consider multiple pieces of evidence from the input text. Specifically, the case study includes the user query (white box), ground truth (yellow box), predictions from the base LLaMA-3.1-8B-Instruct model~\cite{dubey2024llama} (white box), and predictions from our LongPAI model (green box). In addition, incorrect predictions are highlighted in red, while correct ones are marked in green. The results demonstrate the superiority of the proposed LongPAI model, which benefits from supervised CoT reasoning. Furthermore, Figure~\ref{fig:case_study} illustrates that LongPAI offers significantly greater interpretability than its base model.


% \noindent \textbf{The impact of $K$ in RAG}.
% We present in Figure~, 

% 


% metric pool


\subsection{Full Performances}
Table~\ref{tab:full_longpai_results} presents the full results of the Table~\ref{tab:longpai_results} in the main manuscript. Also, we involve more popular LLMs~\cite{yang2024qwen2-5,guo2025deepseek} into the Table~\ref{tab:full_longpai_results}.

\input{assets/tables/full_longpai_results}