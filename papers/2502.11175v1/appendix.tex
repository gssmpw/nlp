\clearpage
\appendix

\section{Implementation Details}
When retrieving from the datastore in all languages, we utilize the approach outlined in \cite{chirkova-etal-2024-retrieval} as our baseline. Specifically, we employ the basic\_translated\_langspec prompt template, as detailed in Table~\ref{tab:system_prompts} to generate our final mRAG answer from the generator. In our method, we retrieve and re-rank the top-50 documents for each query, and then use only the top-5 documents to generate the final answer. The document retrieval and re-ranking are carried out using bge-m3. We do not translate documents already in query language in the framework of DKM-RAG to reduce costs.

We conduct our experiments using an AMD EPYC 7313 CPU (3.0 GHz) paired with four NVIDIA RTX 4090 GPUs. We use Python 3.11.5 and PyTorch 2.3.1 for the software environment.
\input{tables/wiki_ratio}

\section{Prompts}
\label{appendix:prompts}
As shown in Table~\ref{tab:system_prompts}, we provide the prompts used to generate our final answer with the retrieved documents in our mRAG baseline. \textit{Docs} refers to retrieved documents and question refers to the current query. We also provide prompts during the passage rewriting phase in the DKM-RAG framework as stated in Table~\ref{tab:prompt_dkm}. We only provide english prompts for simplicity. And we provide prompts to measure the language preference of GPT-4o-mini, regarding answering in the specific languages as stated in Table~\ref{tab:prompt_generation}.

\input{tables/prompt_generation}
\input{tables/prompt_dkm}
\input{tables/prompt_genpref}

\section{Language Notation}
In this work, we use standard ISO 639-1 language codes to represent the various languages involved in our experiments. Specifically, en denotes English, ko represents Korean, ar corresponds to Arabic, zh refers to Chinese (Simplified), fi indicates Finnish, fr stands for French, de represents German, ja corresponds to Japanese, it refers to Italian, pt denotes Portuguese, ru stands for Russian, es represents Spanish, and th corresponds to Thai. These concise notations facilitate the identification and processing of language-specific data across datasets and models in multilingual NLP research.


\section{Dataset Statistics}

We present the statistics of the datasets used in our experiments. MKQA serves as the primary dataset, and its details, including the number of examples and the median lengths of questions and answers, are summarized in Table~\ref{tab:data_statistics}. Additionally, we utilize Wikipedia as the external source for the retriever datastore, with its statistics (number of passages and median lengths) also provided in Table~\ref{tab:data_statistics}. And we provide the number of passages in each language and the ratio of them in Table~\ref{tab:wiki_ratio}. These details offer a clear overview of the data resources supporting our experiments.

\paragraph{Language Distribution of Pre-trained LLM}
We provide language distribution in the pre-training corpus of Llama-2. As stated in Table~\ref{tab:llama_distribution}, we use English (EN) as a high-resource, Spanish (ES) as a mid-resource, and Korean (KO) as a low-resource language in our experiment based on their ratios.
\input{tables/llama_distribution}


\label{appendix:statistics}



\input{tables/statistics}

\section{Language Preference of Other Languages}
We also perform additional experiments to explore language preferences for languages not covered in Table~\ref{tab:subset_mlr}, using the MLR score that we propose. As shown in Table~\ref{tab:appendix_removed_langs}, similar to the results in Table~\ref{tab:subset_mlr}, the highest preferences are typically observed when $L_q = L_d$ across all query languages. English is also the most preferred language. For clarity, we omit results for other languages.

For most languages, such as Arabic, Finnish, German, and Russian, switching to a cross-lingual setup leads to a significant drop in MLR. For example, Arabic queries using the bge-m3 encoder achieve a monolingual score of 40.39, but cross-lingual retrieval (e.g., with Thai) results in a 6.80-point decrease.

Interestingly, for Thai queries, some cross-lingual pairs show a slight improvement over the monolingual baseline (as indicated by the positive differences in red), suggesting that for low-resource languages like Thai, cross-lingual signals might sometimes offer complementary benefits

\input{tables/main_appendix}

\section{Similarity Matrices}
We provide similarity matrix measured by LaBSE for each query language en, zh, ko and each generator in Figure~\ref{fig:aya_lang_pref_en}, Figure~\ref{fig:aya_lang_pref_zh}, Figure~\ref{fig:aya_lang_pref_ko}, Figure~\ref{fig:llama_lang_pref_en}, Figure~\ref{fig:llama_lang_pref_zh}, Figure~\ref{fig:llama_lang_pref_ko}, Figure~\ref{fig:gpt_lang_pref_en}, Figure~\ref{fig:gpt_lang_pref_zh} and Figure~\ref{fig:gpt_lang_pref_ko}. Each entry represents the embedding similarity score between answers generated in different languages, with the diagonal values all equal to 1 (i.e., comparing an answer with itself). Moreover, the values shown in Figure~\ref{fig:multi_lang} are computed by averaging over the rows or columns for each language.


\section{Case study}
\paragraph{MLR}
We provide an example of a document that improved MLR score, where the rank of a relevant document significantly increases after translation. In Table~\ref{tab:case_mlr}, the user query \textit{"영국 캐리비안에 언제 노예제가 폐지됐나요? (When was slavery abolished in the British Caribbean?)"} is in Korean, whereas the original passage is in English. Initially, the document’s rank (\(\mathbf{r_d^{\text{init}}} = 34\)) was relatively low, but after translating the passage into Korean and re-ranking (\(\mathbf{r_d^{\text{re-rank}}} = 2\)), the document moved much closer to the top. 
This demonstrates how cross-lingual alignment can substantially improve retrieval performance in a multilingual setting.
Notably, even if the passage content is semantically the same, language preference in the model can lead to poor alignment when the query and document are in different languages, adversely affecting retrieval. Translating the document into the query language effectively mitigates this issue.
\input{tables/case_mlr}

\paragraph{Answer Generation in Language Preference of Generator}
\input{tables/case_generation}
We also provide an example of generated answers in different languages with a generator, GPT-4o-mini as shown in Table~\ref{tab:case_generation}. The preference score in the rightmost column of Table~\ref{tab:case_generation} indicates that the generator prefers the query language and Latin-script languages over other languages.



\paragraph{Unified Document of DKM-RAG}
\input{tables/case_dkmrag}
Additionally, we provide a sample of \(\displaystyle P_{\text{translated}}\) and \(\displaystyle P_{\text{refined}}\) obtained via our proposed DKM-RAG framework in Table~\ref{tab:case_dkm}. 
This example illustrates how the crucial answer component, \textit{``the executive branch''}, which is not apparent from the translated passage alone, emerges through the model’s internal knowledge. 
Consequently, this shows that DKM-RAG can effectively leverage additional knowledge sources that is not included in the translated passage to achieve better performance.

%\vspace{-1mm}
\subsection{Failure Case}
\paragraph{MLR}
We present a failure case of the MLR metric in Table~\ref{tab:failure_mlr}. Due to the difficulty of translating documents in low-resource languages, repetitive phrases such as \textit{Changing the line-up} appear in the translated passage. This repetition causes the re-ranker to misinterpret the content, leading to an improvement in the rank even though the content is irrelevant.
\input{tables/failure_mlr}
\paragraph{DKM-RAG}
We also present a failure case of DKM-RAG in Table~\ref{tab:failure_dkmrag}. The retriever retrieves an English document that is irrelevant to the query due to its language preference. Additionally, the LLM lacks relevant knowledge related to the query, resulting in a failed generation.
\input{tables/failure_dkmrag}


\section{Language Preference of Generators in average}
We provide language preference of generators in terms of average as shown in Figure~\ref{fig:avg_generator_pref}. Consistent with the result of each query language in Figure~\ref{fig:multi_lang}, the generator shows preferences for Latin-script languages. And GPT-4o-mini shows more consistent outputs than other generators. This is because it is a larger model than the others, providing more stable answers regardless of language preference. Between Llama and Aya, Aya produces slightly more consistent outputs, demonstrating its multilingual capability in handling diverse linguistic contexts.


\section{Ablation study of DKM-RAG}
To prove the effectiveness of concatenating translated passages and refined passages in DKM-RAG framework, we provide an ablation study of each component in DKM-RAG. As stated in Table~\ref{tab:ablation_dkm}, removing any component from DKM-RAG results in decreased performance, highlighting that every part is crucial to its overall effectiveness.


\input{tables/dkm_ablation}



\section{MLR Analysis}
We prove the effectiveness of our proposed language preference metric, MLR by comparing language preference between MLR score and the average document language ratio of retrieved documents for each dataset. As stated in Table~\ref{tab:analysis_mlr}, the tendency of average language ratio of retrieved documents and MLR score is similar. To prove it, we also report Pearson and Spearman correlation coefficients and each p-value between them. Pearson value (0.98558) indicates a very strong positive linear correlation between the average MKQA language distribution values (mkqa\_avg) and the MLR (Preference) scores. The p-value (7.75e-10) is extremely small, showing that the probability of observing such a strong correlation by chance is almost negligible. In short, there is a statistically significant, nearly perfect linear relationship between these two sets of values. Similarly, the Spearman value (0.86264) also indicates a strong association, and the corresponding p-value (1.47e-4) confirms that this correlation is statistically significant. By these results, we prove that MLR is efficient for measuring language preference of retriever.
\input{tables/analysis_mlr}



\clearpage

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{aya_similarity_matrix_en.pdf}
  \caption{LaBSE Similarity Matrix of aya-expanse-8b (en).} 
  \label{fig:aya_lang_pref_en}
\end{figure*}

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{aya_similarity_matrix_zh.pdf} 
  \caption{LaBSE Similarity Matrix (zh) of aya-expanse-8b.} 
  \label{fig:aya_lang_pref_zh}
\end{figure*}

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{aya_similarity_matrix_ko.pdf}
  \caption{LaBSE Similarity Matrix (ko) of aya-expanse-8b.} 
  \label{fig:aya_lang_pref_ko}
\end{figure*}

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{llama_similarity_matrix_en.pdf}
  \caption{LaBSE Similarity Matrix (en) of Llama-3.1-8B-instruct.} 
  \label{fig:llama_lang_pref_en}
\end{figure*}

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{llama_similarity_matrix_zh.pdf} 
  \caption{LaBSE Similarity Matrix (zh) of Llama-3.1-8B-instruct.} 
  \label{fig:llama_lang_pref_zh}
\end{figure*}


 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{llama_similarity_matrix_ko.pdf}
  \caption{LaBSE Similarity Matrix (ko) of Llama-3.1-8B-instruct.} 
  \label{fig:llama_lang_pref_ko}
\end{figure*}


 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{gpt_similarity_matrix_en.pdf}
  \caption{LaBSE Similarity Matrix (en) of gpt-4o-mini.} 
  \label{fig:gpt_lang_pref_en}
\end{figure*}

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{gpt_similarity_matrix_zh.pdf} 
  \caption{LaBSE Similarity Matrix (zh) of gpt-4o-mini.} 
  \label{fig:gpt_lang_pref_zh}
\end{figure*}

 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{gpt_similarity_matrix_ko.pdf}
  \caption{LaBSE Similarity Matrix (ko) of gpt-4o-mini.} 
  \label{fig:gpt_lang_pref_ko}
\end{figure*}


 \begin{figure*}[ht]
  \centering
  \includegraphics[width=1.0\textwidth]{avg_generator_language_preference.pdf}
  \caption{Average Generator Preference for three query languages: en, zh, ko.} 
  \label{fig:avg_generator_pref}
\end{figure*}