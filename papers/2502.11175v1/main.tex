% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
%\usepackage[review]{acl}
\usepackage[preprint]{acl}


%\usepackage[table]{xcolor} 
\usepackage{array}
\usepackage{multirow}
\usepackage{adjustbox}

\usepackage{tabularx} 
\usepackage{geometry} 

\usepackage{caption}      
\usepackage{amssymb}
% Standard package includes
\usepackage{booktabs} 
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{comment}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{kotex}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\setcounter{footnote}{0}

\title{Investigating Language Preference of Multilingual RAG Systems}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jeonghyun Park \and Hwanhee Lee\textsuperscript{$\dagger$} \\
    Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea\\
    \texttt{\{tom0365, hwanheelee\}@cau.ac.kr}
}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}






\begin{document}
\maketitle
\footnotetext{\textsuperscript{$\dagger$}Corresponding author.}



\begin{abstract}
Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.
\end{abstract}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{motivation.pdf}
    \caption{Failure cases of multilingual RAG system showing degraded generation ability because of language preference of retriever and generator in mRAG pipeline. $doc_{rel}$ in the Korean (KO) document represents the relevant document to the given query that can be utilized to generate a final answer.}
    %\vspace{-5mm}
    \label{fig:motivation}
\end{figure}

\section{Introduction}
%mRAG
    
Multilingual Retrieval-Augmented Generation (mRAG) extends traditional Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} by leveraging multilingual external sources to generate accurate, contextually and linguistically aware responses. However, mRAG systems face challenges in retrieving relevant information due to linguistic discrepancies between queries and documents~\cite{wu2024limitscrosslingualdensepassage}. Moreover, conflicts among multilingual sources can lead to inconsistencies in the generated responses~\cite{chataigner2024multilingualhallucinationgapslarge}.

Beyond retrieval challenges and source conflicts, language preference is another critical issue in mRAG systems, often leading to inaccurate outputs. 
As illustrated in \textbf{Case 1} of Figure~\ref{fig:motivation}, the retriever may prioritize particular languages—especially high-resource or query-language documents—at the expense of truly relevant information in low-resource language. Consequently, the Large Language Model (LLM) either produces an incorrect answer or deems the query unanswerable due to irrelevant content in the documents. Likewise, in \textbf{Case 2}, even if relevant documents are retrieved, the generator might favor passages in the query language or Latin scripts, ignoring essential evidence in lower-resource languages and resulting in inaccurate outputs. These preferences ultimately limit the effectiveness of mRAG, yielding biased rankings, reduced answer quality, and inconsistencies across languages~\cite{sharma2024fauxpolyglotstudyinformation}.

Prior studies~\cite{10.1145/3626772.3657943, 10.1145/3539813.3545131, sharma2024fauxpolyglotstudyinformation} have investigated this issue by introducing language fairness metrics to assess whether documents from different languages are ranked equitably via statistical equivalence testing, by proposing Language-Preference-Based Re-ranking for multilingual information retrieval, and investigating LLM’s linguistic preference in across-language RAG-based information search setting. However, these approaches primarily focus on a limited set of languages and fail to reflect the true ranking dynamics of documents across languages. 






In this work, we aim to understand language preference phenomena in mRAG systems comprehensively. We focus on the following three key research questions:

\begin{itemize}
    \item \textbf{RQ1 (\S\ref{sec:retriever}):} \textit{Which languages does the retriever prefer?}
    \item \textbf{RQ2 (\S\ref{sec:generator}):} \textit{Which languages does the generator prefer, and how do these preferences correlate with mRAG performance?}
    \item \textbf{RQ3 (\S\ref{sec:dual}):} \textit{How can we mitigate language preference in mRAG?}
\end{itemize}

To address these questions, we present a comprehensive evaluation of language preferences throughout the entire mRAG pipeline across multiple languages. To systematically investigate the language preference problem of multilingual retrievers, we propose MultiLingualRank (MLR), a novel metric that quantifies language preference at the retriever level by measuring the shift in document rankings when non-query-language passages are translated into the query language. Our extensive experiments with diverse language combinations demonstrate that the retriever strongly prefers documents that are in high-resource languages and also share the same language as the query, confirming the presence of significant preference (\S\ref{sec:retriever}).

At the generator level, we evaluate language preference by generating responses in multiple languages for the same query and the same retrieved document set, measuring their semantic similarity. 
Our results show that the generator favors both query languages and Latin script languages, with a relatively modest preference for query languages. This ultimately results in a decline in answer quality.
Moreover, we uncover a nuanced relationship between language preference and overall mRAG performance. We observe that a strong preference for high-resource languages does not always lead to improved mRAG performance (\S\ref{sec:generator}).
This occurs because the retriever may retrieve high-resource but irrelevant documents so that the generator cannot generate accurate answers from them. Therefore, language preference can degrade performance by overlooking lower-resource but relevant documents, thereby causing inconsistencies.


Finally, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that mitigates the language preference of mRAG. DKM-RAG enhances mRAG by combining externally retrieved, translated passages with internally rewritten passages enriched by the model’s knowledge. Empirical results demonstrate that DKM-RAG significantly reduces language preference issues in the generation process, leading to improved performance across a range of linguistic settings (\S\ref{sec:dual}).


\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{mlr.pdf}
    \caption{Overall framework of calculating MLR. For simplicity, we only consider three documents to calculate the MLR score.}
    %\vspace{-3mm}
    \label{fig:mlr}
\end{figure*}


\section{MultiLingualRank}
To evaluate the language preference of a multilingual retriever in the mRAG system, we introduce \textit{MultiLingualRank} (MLR), a novel metric that quantifies how much the ranking of retrieved documents improves when non-query language documents are translated into the query language. As shown in Figure~\ref{fig:mlr}, MLR is computed in three stages: (i) retrieving documents across multiple languages, (ii) translating documents that are not in the query language into query language, and (iii) re-ranking the translated documents to measure rank improvements.

\subsection{Stage 1: Initial Document Retrieval}
For each query $q \in Q$ (where $Q$ is the set of all queries), we retrieve a ranked list of documents $D_q$ from multilingual datastores. Each document $d \in D_q$ is assigned an initial rank $r_d^{\text{init}}$ (with 1 being the highest rank). Let $L_q$ denote the language of the query and $L_d$ the language of document $d$. 

\subsection{Stage 2: Translation of Non-Query Language Documents}
To ensure language consistency when assessing ranking improvements, we extract documents whose language differs from that of the query. Formally, we define:
$$
D_q^{\text{diff}} = \{\, (d,\, r_d^{\text{init}}) \mid d \in D_q,\; L_d \neq L_q \,\}.
$$
Each document in $D_q^{\text{diff}}$ is then translated into the query language $L_q$, resulting in the set:
$$
D_q^{\text{Translated}} = \{\, d \mid d \text{ has been translated into } L_q \,\}.
$$

\subsection{Stage 3: Re-Ranking and MLR Score Computation}
The translated documents in $D_q^{\text{Translated}}$ are re-ranked using retrievers in conjunction with the original query. Let $r_d^{\text{re-rank}}$ denote the new rank of document $d$ after re-ranking. To capture ranking improvements, we compute the rank difference for each document $d$ as:
$$
\Delta r_d = \max\bigl( r_d^{\text{init}} - r_d^{\text{re-rank}},\, 0 \bigr).
$$
A positive value of $\Delta r_d$ indicates that the document has moved up in the ranking. For each query $q$, the total observed improvement is given by:
$$
\Delta r_q = \sum_{d \in D_q^{\text{Translated}}} \Delta r_d.
$$

To normalize this improvement, we first define the maximum possible improvement for each document as:
$$
\Delta r_d^{\max} = r_d^{\text{init}} - 1,
$$
and then compute the total maximum improvement for query $q$:
$$
\Delta r_q^{\max} = \sum_{d \in D_q^{\text{Translated}}} \Delta r_d^{\max}.
$$

The query-specific MLR score is then calculated as:
$$
\text{MLR}_q = 
\begin{cases}
\displaystyle \frac{\Delta r_q}{\Delta r_q^{\max}} \times 100, & \text{if } \Delta r_q^{\max} > 0,\\[6pt]
0, & \text{otherwise}.
\end{cases}
$$
Finally, the overall MultiLingualRank is obtained by averaging the scores over all queries:
$$
\text{MLR} = \frac{1}{\lvert Q \rvert} \sum_{q \in Q} \text{MLR}_q.
$$

%\vspace{-1mm}


\section{General Setup}

\subsection{Dataset}
By following previous study~\cite{chirkova-etal-2024-retrieval}, we use MKQA~\cite{longpre-etal-2021-mkqa} dataset, a multilingual open
domain question answering evaluation set in our experiments. MKQA consists of 10k examples from the Natural Questions (NQ) dataset~\cite{kwiatkowski2019natural}, translated into 25 languages. This dataset is therefore parallel between languages and grounds knowledge primarily in English Wikipedia. In our experiments, we also select a subset of 2.7K samples, overlapping between MKQA and KILT NQ datasets~\footnote{\url{https://huggingface.co/datasets/facebook/kilt_tasks}}, thus recovering relevant documents information from KILT NQ.

\subsection{Models}
\paragraph{Multilingual Retrievers} 
Following previous work~\cite{chirkova-etal-2024-retrieval}, we use a strong and publicly available BGE-m3~\cite{bge-m3} as our multilingual retriever which can encode various languages we consider in our experiments.
Consistent with the retriever, we use BGE-m3~\cite{bge-m3} as a re-ranking encoder for computing MLR. In addition, we use two other Sentence-BERT series re-ranking encoders~\cite{reimers-2019-sentence-bert}, paraphrase-multilingual-MiniLM-L12-v2 and paraphrase-multilingual-mpnet-base-v2 to generalize the experimental results. We abbreviate them as p-mMiniLM and p-mMpNet for better visibility of the table.


%\vspace{-1mm}
\paragraph{Generators}
 We use recently released various strong multilingual LLM, aya-expanse-8b~\cite{dang2024ayaexpansecombiningresearch} that can deal with various languages well. Also, we use strong LLMs, qwen 2.5-7B Instruct~\cite{qwen2.5} and Phi-4 14B~\cite{abdin2024phi4technicalreport}, Llama-3.1-8B-Instruct~\cite{dubey2024llama} as our generators.
                
\subsection{Other Implementation Details}
\paragraph{Translation model}
We utilize a robust translation model for various languages, NLLB-200-distilled-600M~\cite{costa2022no} in our experiments.

%\vspace{-1mm}
\paragraph{Datastore}
Following previous study~\cite{chirkova-etal-2024-retrieval}, we use Wikipedia as our datastore documents collection.  In most of our experiments, we retrieve from two main Wikipedia sources: the KILT version of English Wikipedia\footnote{\url{https://huggingface.co/datasets/facebook/kilt_wikipedia}} and the Wikipedia edition in the user’s native language\footnote{\url{https://huggingface.co/datasets/wikimedia/wikipedia}}.
For detailed statistics, please refer to Appendix~\ref{appendix:statistics}.

\paragraph{Baseline}
We conduct several experiments to measure the language preference of mRAG based on Bergen~\cite{chirkova-etal-2024-retrieval}. Bergen explores the components and adjustments necessary to develop an effective mRAG pipeline, serving as a robust baseline for future research. 

\input{tables/main}


\section{Language Preference of Retrievers}
\label{sec:retriever}

In this section, we examine two factors that may affect the retriever's language preference: (i) the relationship between the query language ($L_q$) and the document language ($L_d$), and (ii) the resource availability of the languages involved.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Effect of the $L_q$ and $L_d$ Relationship}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Experimental Setup}
We evaluate eight language pairs under two scenarios: (1) \textbf{monolingual settings} where $L_q = L_d$, and (2) \textbf{cross-lingual settings} where $L_q \neq L_d$. In each case, our primary metric is \textit{MLR} (MultiLingualRank), computed using three re-ranker encoders (bge-m3, p-mMiniLM, and p-mMpNet). For example, if the query is in English (en) but the target translation is in Korean (ko), we translate all non-English passages into Korean and then measure the rank changes with MLR.

\subsubsection{Results for Monolingual Settings ($L_q = L_d$)}
\paragraph{Strong Preference When the Query and Document Languages Match.}  
As shown in the leftmost column of Table~\ref{tab:subset_mlr}, when the query and document languages are identical, the retriever shows a high preference. This is expected, as direct linguistic alignment avoids the complexities of cross-lingual mapping and translation, thereby yielding stronger preference.

\subsubsection{Results for Cross-Lingual Settings ($L_q \neq L_d$)}
\paragraph{Lower Overall MLR in Cross-Lingual Matching.}  
When $L_q \neq L_d$, the retriever performs cross-lingual matching, which typically results in lower MLR values than in monolingual cases. As indicated by the right-hand columns in Table~\ref{tab:subset_mlr} (highlighted in blue), cross-lingual setups are generally less preferred than their monolingual counterparts—except in cases involving English.



%\vspace{-2mm}
\paragraph{English as a Dominant Target Language.}  
We observe that when the translated document language $L_d$ is English, the retriever exhibits nearly the highest language preference as stated in the English column (en) in Table~\ref{tab:subset_mlr}. In fact, English often outperforms even monolingual configurations, likely due to the abundance of English data in pre-training, which biases the model towards stronger English representations.

%\vspace{-2mm}
\paragraph{Influence of Language Family Similarity.}  
Language family and geographic proximity also play a role. For example, Romance languages (fr, it, pt, es) share extensive lexical and structural similarities, which help maintain a relatively high cross-lingual preference and narrow the performance gap with monolingual setups, as illustrated by the joint $L_q$ and $L_d$ pairs in Table~\ref{tab:subset_mlr}. Similarly, East Asian languages (ko, ja, zh) tend to show moderate declines in cross-lingual scenarios compared to the monolingual baseline, although they still lag behind the highest scores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Impact of Language Resource Availability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Experimental Setup}
We also investigate whether the volume of available language resources affects MLR. We categorize languages into three groups based on their distribution in the pre-training corpus of recent LLMs: high-resource (e.g., English), mid-resource (e.g., Spanish), and low-resource (e.g., Korean). We use the same query set across all setups while systematically varying $L_q$ and $L_d$.



\subsubsection{Results}
\paragraph{Limited Impact of Query Language Resources.}  
The resource level of the query language ($L_q$) has a limited effect on cross-lingual preference. As shown in Table~\ref{tab:subset_mlr}, when $L_q$ is high-resource (e.g., English), strong preference is observed only if $L_d$ also matches a high-resource language. Otherwise, the MLR scores remain similar regardless of whether $L_q$ is high-, mid-, or low-resource.

\paragraph{Document Language Resources Are More Influential.}  
In contrast, the language resource level of the document language ($L_d$) has a pronounced impact on MLR. As shown in Table~\ref{tab:subset_mlr}, documents from high-resource languages consistently achieve the highest preference scores, followed by mid-resource and then low-resource languages. This trend (High > Mid > Low) suggests that extensive pre-training on high-resource languages enables stronger alignment, yielding higher MLR even across diverse query languages. Conversely, low-resource datastores typically produce lower MLR scores unless the query language also corresponds to that low-resource setting.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.3\textwidth]{en_generator_language_preference.pdf}\hfill
  \includegraphics[width=0.3\textwidth]{zh_generator_language_preference.pdf}\hfill
  \includegraphics[width=0.3\textwidth]{ko_generator_language_preference.pdf}
  \caption{Language Preference of the Generators. In each figure, "aya" represents aya-expanse-8B, "llama" represents Llama-3.1-8B-Instruct, and "gpt" represents gpt-4o-mini. The red dotted line indicates the average generator preference.}
  \label{fig:multi_lang}
  %\vspace{-2mm}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Overall, our results indicate that the resource availability of $L_d$ critically influences the language preference of the retriever. These findings lay the groundwork for further investigation into the language dynamics within mRAG systems.


\section{Language Preference of Generators}

\label{sec:generator}
In this section, we explore LLM generators' language preferences in mRAG and their impact on overall performance.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Do LLMs Prefer Certain Languages for Contextual Knowledge?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Experimental Setup}
To assess the generator’s language preference, we measure answer consistency across eight languages: English (en), Korean (ko), Chinese (zh), French (fr), Japanese (ja), Italian (it), Portuguese (pt), and Spanish (es). For a given query, the generator produces responses in each of these languages using the same retrieved document set from multilingual datastores. We then compute the embedding similarity between each pair of generated answers, resulting in an 8×8 similarity matrix. We define the preference for a specific language as the average similarity score of the responses in that language.


We use LaBSE for measuring multilingual semantic similarity \cite{feng2022language}. And we use aya-expanse-8B, Llama-3.1-8B-Instruct, and GPT-4o-mini as our generators. We use language-specific prompts that incorporate the retrieved passages to induce responses in the target language, enabling us to capture the generator's inherent language preference.



\subsubsection{Results}
\paragraph{Strong Preference for Latin Script Languages.}
Figure~\ref{fig:multi_lang} indicates that the generator produces more consistent responses in languages that use Latin scripts (e.g., en, fr, it, pt, es) compared to non-Latin languages (e.g., ko, zh, ja). This suggests that the model benefits from structural advantages in token alignment when processing Latin-based languages. 

\paragraph{Modest Preference for the Query Languages.}
In addition, the generator shows a slight increase in consistency when the output language matches a query language. For instance, Korean (ko) queries yield somewhat more consistent responses when the generator replies in Korean rather than when the query is in English. However, this improvement is marginal, suggesting that the overall preference toward Latin scripts remains influential. Despite the modest gain, the model still demonstrates some capacity to handle multilingual queries effectively, indicating that matching the query language can provide a small but measurable benefit in non-Latin contexts. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Correlation between Language Preference and mRAG Performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Experimental Setup}
To examine how language preference impacts overall mRAG performance, we isolate language effects by providing generators with retrieved passages unified in a single target language—selected from the eight candidates (en, ko, zh, fr, ja, it, pt, es). We retrieve data from multilingual sources, enabling a direct comparison between language preference (measured by MLR, as shown in Table~\ref{tab:subset_mlr}) and performance across three query languages (en, ko, zh).


We evaluate four generators (aya-expanse-8B, Phi-4, Qwen2.5-7B-Instruct, and Llama3.1-8B-Instruct) using character 3-gram recall \cite{chirkova-etal-2024-retrieval} under three configurations. In the first configuration, passages are retrieved from multilingual resources, denoted as \textit{all}. In the second, all retrieved passages are unified into a single target language (single-language document set). Finally, in the third configuration, we employ our proposed DKM-RAG framework (detailed in Section~\ref{sec:dual}) to mitigate language preference. We also compute the average MLR score (across different query languages) for each language to indicate its overall preference.

\input{tables/corr}

\subsubsection{Results and Analysis}
\paragraph{Strong Correlation for English Queries.}
As stated in Table~\ref{tab:resource_comparison_updated}, for queries in English (\( L_q = \text{en} \)), RAG performance shows a strong correlation with language preference. English achieves the best results—likely due to its high-resource availability and the model’s familiarity with it. In this setting, the \textit{all} strategy is particularly effective, as it leverages cross-lingual knowledge fusion. We observe an exception for Japanese (\( \text{ja} \)), where performance is lower despite moderate preference, possibly due to challenges with non-Latin scripts and complex morphology.

\paragraph{Weaker Correlation for Non-English Queries.}
When \( L_q \neq \text{en} \), the relationship between language preference and performance becomes less pronounced. Although the generator generally prefers English passages overall, it achieves optimal performance when it receives retrieved passages that directly match the query language. In these cases, translating all passages into English does not enhance performance; instead, maintaining language consistency between the query and passages yields better results. This finding underscores the importance of linguistic compatibility in mRAG systems.

\paragraph{Optimal mRAG Strategy.}
Based on our experiments, different strategies depending on the query language prove more effective. As stated in Table~\ref{tab:resource_comparison_updated}, for English queries, employing the \textit{all} strategy capitalizes on the high cross-lingual preference for English. In contrast, for non-English queries, translating retrieved passages into the query language \(L_q\) bridges the comprehension gap and ensures better alignment between query intent, passage semantics, and output language. This targeted approach ultimately leads to improved RAG performance by accommodating the specific language dynamics of the generator.



\section{Dual Knowledge Multilingual RAG}
\label{sec:dual}
%\vspace{-1mm}
Translating retrieved documents into the query language benefits mRAG, but it may also reflect retrieval outputs from high-resource languages including irrelevant content. Therefore, leveraging the LLM’s internal knowledge can help filter inaccuracies and enrich the retrieved information with more reliable content. So we rewrite translated passages to refine the relevancy of documents by leveraging LLM's internal information.

Based on this insight, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a framework that leverages both external translated passage and internal knowledge as shown in Figure~\ref{fig:dkm_rag}. First \textbf{(\#1)}, we retrieve documents for a given query from the \textit{all} strategy and re-rank them. Next \textbf{(\#2)}, we obtain external translated passages, \(P_{\text{translated}}\) by translating into the query language. And \textbf{(\#3)}, the rewriter LLM refines each translated passage in the context of the given query to produce refined passages, \(P_{\text{refined}}\). This refining process utilizes a prompt to guide the model in integrating its internal knowledge, removing redundancy, and highlighting relevant information in a coherent and consistent style. For detailed prompts, please refer to Appendix~\ref{appendix:prompts}. Finally \textbf{(\#4)}, We concatenate the two sets to form the final passage set as input to the generator LLM, ensuring that responses are both contextually enriched and linguistically aligned with the query:

%\vspace{-3mm}


\[
P_{\text{final}} = \text{concat}\bigl(P_{\text{translated}}, P_{\text{refined}}\bigr).
\]




\paragraph{Results.} As shown in Table~\ref{tab:resource_comparison_updated}, DKM-RAG outperforms other document-based generator settings. For non-English queries (\(L_{q} \neq \text{en}\)), it leverages translated passages and enriched content to handle linguistic diversity. Even for English queries (\(L_{q} = \text{en}\)), 
it surpasses the \textit{all} baseline, highlighting the importance of integrating translated and refined knowledge.



\section{Related Works}
\subsection{Multilingual RAG}
Researchers explore challenges in mRAG such as problem of cross-lingual dense passage retrieval for low-resource languages~\cite{wu2024limitscrosslingualdensepassage} and research various techniques to address key challenges in mRAG, such as enhancing the performance of language models in low-resource languages~\cite{deshpande2024chain}, resolving low-resource scenarios~\cite{zhang-etal-2024-enhancing-multilingual}, and adapting language models for multilingual reasoning tasks~\cite{yoon-etal-2024-langbridge}. Benchmarks like \textsc{MMTEB}~\cite{enevoldsen2025mmteb} enable systematic evaluation of multilingual retrieval.

Earlier mRAG systems frequently focus on high-resource languages (e.g., English), but a growing body of research aims to make advanced Natural Language Processing (NLP) technology accessible across a wide spectrum of linguistic contexts. Proposed solutions include code-mixed prompts for in-context learning~\cite{shankar-etal-2024-context} and self-distillation from resource-rich to low-resource languages~\cite{zhang-etal-2024-enhancing-multilingual}.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{dkm_rag.pdf}
    \caption{Overall flow of proposed DKM-RAG.}
    %\vspace{-5mm}
    \label{fig:dkm_rag}
\end{figure}

\subsection{Language Preference in mRAG}
Despite significant progress, language preference—a systematic tendency to favor certain languages—remains a critical issue in mRAG systems. This preference arises from imbalances in training data, tokenization mismatches, script differences, and uneven resource availability~\cite{sharma2024fauxpolyglotstudyinformation, wu2024languagesequalinsightsmultilingual}. Studies show that high-resource languages (e.g., English) often overshadow relevant content in lower-resource languages during retrieval~\cite{yang-etal-2024-language-bias, chirkova-etal-2024-retrieval}, leading to suboptimal evidence retrieval~\cite{10.1145/3626772.3657943} and causing inconsistencies or hallucinations in outputs~\cite{chataigner2024multilingualhallucinationgapslarge}. These disparities also raise broader fairness concerns in multilingual NLP, as pre-trained models exhibit group fairness issues across languages~\cite{cabello-piqueras-sogaard-2022-pretrained, ramesh-etal-2023-fairness}.


Researchers propose several methods to counteract language preferences, including language-preference-based re-ranking~\cite{10.1145/3539813.3545131}, evaluate knowledge consistency across languages~\cite{qi-etal-2023-cross}, and specialized datasets designed to detect such imbalances~\cite{li-etal-2024-bordirlines}. However, these approaches often focus on a single mRAG stage or overlook the actual ranking of retrieved documents~\cite{sharma2024fauxpolyglotstudyinformation, 10.1145/3626772.3657943}. We introduce a metric that quantifies language preference in retrieval via ranking differences and propose a simple framework to mitigate these preferences across the entire mRAG pipeline.

\section{Conclusion}

In this work, we investigate language preferences in mRAG systems. We propose a metric that measures the language preference of retrievers by checking the rank difference between the translated passage and the original one. Our experiments reveal that retrievers prefer high-resource and query language but do not always yield better generation performance. We also find that generators often favor the query language or Latin scripts, resulting in inconsistent outputs. To address this, we propose DKM-RAG which integrates translated passages with internal knowledge. Empirical results show that DKM-RAG consistently enhances mRAG performance across diverse languages.



\section*{Limitations}
Our approach involves translating documents to measure rank shifts and unify linguistic representations. This process relies heavily on the quality of the translation model employed. Errors or inaccuracies in translation can distort the original meaning of passages and potentially introduce noise into both the retrieval and generation stages. 

MLR entails translation and re-ranking steps. While this approach offers a principled way to quantify language preference, it also adds latency and computational cost, especially when dealing with large-scale multilingual corpora or real-time systems.

DKM-RAG framework which combines external translated passages and parametric (internal) knowledge, improves performance yet remains relatively straightforward. Future work could explore more sophisticated techniques for merging external and internal knowledge (e.g., trainable fusion mechanisms, dynamic weighting) to further reduce preferences and enhance overall system capabilities.

Lastly, our experiments focus on Wikipedia-based datasets in a specific set of languages, which may not generalize to all linguistic varieties or specialized domains. Future research should examine broader contexts, including low-resource languages not present in widely available corpora or domain-specific retrieval settings, to fully assess how language preferences manifest across diverse real-world scenarios.



\section*{Ethics Statement}
We conduct our experiments using publicly available, multilingual dataset and models that follow recognized research and data-sharing guidelines. These resources are widely utilized in the academic community and are distributed with the intent to minimize harmful biases, inappropriate content, or stereotypes. However, they may still not fully represent the diversity of all languages and cultural contexts. We adhere strictly to the usage protocols and license agreements set forth by the original providers, who have taken steps to ensure compliance with established ethical standards. 





\section*{Acknowledgments}
We would like to thank Byeongjeong Kim for his comments and feedback about our figures. We also thank Gyutae Park for his minor correction of this work. This research was supported by Institute for Information \& Communications Technology Planning \& Evaluation (IITP) through the Korea government (MSIT) under Grant No. 2021-0-01341 (Artificial Intelligence Graduate School Program (Chung-Ang University)).
    

\bibliography{main}
\input{appendix}


\end{document}
