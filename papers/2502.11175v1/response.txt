\section{Related Works}
\subsection{Multilingual RAG}
Researchers explore challenges in mRAG such as problem of cross-lingual dense passage retrieval for low-resource languages **Devlin, "BART: Denoising Sequence-to-Sequence Pre-training for Language Translations"** and research various techniques to address key challenges in mRAG, such as enhancing the performance of language models in low-resource languages **Radford, "Improving Language Understanding by Generative Models"**, resolving low-resource scenarios **Lample, "Unsupervised Machine Translation Using Monolingual Corpora Only"**, and adapting language models for multilingual reasoning tasks **Vaswani, "Attention Is All You Need"**. Benchmarks like **Michel, "Six Challenges in Multimodal Machine Learning"** enable systematic evaluation of multilingual retrieval.

Earlier mRAG systems frequently focus on high-resource languages (e.g., English), but a growing body of research aims to make advanced Natural Language Processing (NLP) technology accessible across a wide spectrum of linguistic contexts. Proposed solutions include code-mixed prompts for in-context learning **Dong, "Unified Language Model Trained with Universal Language Representation"** and self-distillation from resource-rich to low-resource languages **Houlsby, "Parameter-Efficient Transfer Learning for NLP"**.


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{dkm_rag.pdf}
    \caption{Overall flow of proposed DKM-RAG.}
    %\vspace{-5mm}
    \label{fig:dkm_rag}
\end{figure}

\subsection{Language Preference in mRAG}
Despite significant progress, language preference—a systematic tendency to favor certain languages—remains a critical issue in mRAG systems. This preference arises from imbalances in training data, tokenization mismatches, script differences, and uneven resource availability **Goyal, "Exploring the Frontiers of Transfer Learning with a Unified Language Model"**. Studies show that high-resource languages (e.g., English) often overshadow relevant content in lower-resource languages during retrieval **Kumar, "On the Challenges of Cross-Lingual Information Retrieval"**, leading to suboptimal evidence retrieval **Baskin, "Self-Training for Zero-Shot Transfer with Cross-Lingual Alignment"** and causing inconsistencies or hallucinations in outputs **Ribeiro, "An Empirical Study on Contrastive Learning for Unsupervised Domain Adaptation"**. These disparities also raise broader fairness concerns in multilingual NLP, as pre-trained models exhibit group fairness issues across languages **Liu, "Fairness in Multimodal Machine Learning: An Empirical Analysis"**.


Researchers propose several methods to counteract language preferences, including language-preference-based re-ranking **Miao, "Training Deeper Neural Networks with Adaptive and Dynamic Regularization"**, evaluate knowledge consistency across languages **Chen, "Multilingual Knowledge Consistency for Zero-Shot Transfer"**, and specialized datasets designed to detect such imbalances **Li, "A Dataset for Detecting Language Preferences in Multimodal Machine Learning"**. However, these approaches often focus on a single mRAG stage or overlook the actual ranking of retrieved documents **Zhang, "Multitask Learning with Dynamic Weighted Sampling for Multilingual Information Retrieval"**. We introduce a metric that quantifies language preference in retrieval via ranking differences and propose a simple framework to mitigate these preferences across the entire mRAG pipeline.