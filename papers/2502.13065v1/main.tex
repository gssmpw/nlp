\newif\ifllncs\llncsfalse
\newif\ifanon\anontrue




\ifllncs
\documentclass{llncs}
\pagestyle{plain}
\else
\documentclass[11pt,letter]{article}
\fi

\usepackage{breakcites}
% Font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\ifllncs
\else
\usepackage{times}
\usepackage{fullpage}
\fi

\usepackage{mdframed}
\usepackage{amsmath}
\usepackage{relsize}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{braket}
\usepackage{url}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage[mathscr]{euscript}
\allowdisplaybreaks

\usepackage{stmaryrd}

\usepackage{soul}
\usepackage{multirow}
\definecolor{DarkBlue}{RGB}{0,0,150}
\definecolor{NotSoDarkBlue}{RGB}{15,15,210}
\definecolor{DarkRed}{RGB}{150,0,0}
\definecolor{DarkGreen}{RGB}{0,100,0}
\usepackage[pdfstartview=FitH,pdfpagemode=UseNone,colorlinks,linkcolor=DarkRed,filecolor=blue,citecolor=DarkRed,urlcolor=DarkRed,pagebackref,breaklinks]{hyperref}
\usepackage[ruled, algosection]{algorithm2e}
\usepackage{cleveref}


\newcommand{\sk}{\mathsf{sk}}

\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\poly}{\mathsf{poly}}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\spann}{span}
\DeclareMathOperator{\tr}{tr}

\newcommand{\fullcirc}{\boldsymbol{\cdot}}

\def\reconstruct{\mathsf{Reconstruct}}


\newcommand{\norm}[1]{\left\| {#1} \right\|}
\newcommand{\norminf}[1]{\left\| {#1} \right\|_{\infty}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\kett}[1]{\ket{#1}\hspace{-0.8mm}\rangle}
\newcommand{\braa}[1]{\langle\hspace{-0.8mm}\bra{#1}}

\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{maintheorem}{Main Theorem}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}
\newtheorem{fact}{Fact}
\newtheorem*{example}{Example}
\newtheorem*{theorem*}{Theorem}
\numberwithin{theorem}{section}
\numberwithin{conjecture}{section}
% \numberwithin{definition}{section}
\numberwithin{problem}{section}
 \newtheorem{construction}{Construction}
 \newtheorem{game}{Game}
 \newtheorem*{conjecture*}{Conjecture}



 % Protocol
\newcounter{prot}
\newmdtheoremenv[backgroundcolor=gray!10,
                 linewidth=0pt,
                 innerleftmargin=16pt,
                 innerrightmargin=16pt,
                 innertopmargin=6pt,
                 innerbottommargin=6pt,
            splitbottomskip=4pt]{protocol}[prot]{Game}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\edit}[1]{\textcolor{red}{#1}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\bit}{\{0,1\}}
\newcommand\algo{\mathcal}
\newcommand{\Uf}{\mathsf{U}_f}
\newcommand{\reg}[1]{{\color{gray}\mathsf{#1}}}
\newcommand{\linear}{\mathrm{L}}
\newcommand{\state}{\mathrm{S}}
\newcommand{\negl}{\mathsf{negl}}
\newcommand{\ketbra}[2]{\left|#1\right\rangle\!\!\left\langle #2\right|}
\newcommand{\Tr}[1]{\mathrm{Tr}\left[#1 \right]}
\newcommand{\ot}{\otimes}
\newcommand\id{\mathbb{I}}

%\newcommand{\proj}[1]{\ensuremath{|#1\rangle \langle #1|}}



\def\matV{\mathbf{V}}
\def\matG{\mathbf{G}}
\def\matP{\mathbf{P}}
\def\matS{\mathbf{S}}

% Crypto
\newcommand{\KeyGen}{\mathsf{KeyGen}}
\newcommand{\Enc}{\mathsf{Enc}}
\newcommand{\Dec}{\mathsf{Dec}}
\newcommand{\secp}{\lambda}
\newcommand{\StateGen}{\mathsf{StateGen}}
\newcommand{\Invert}{\mathsf{Invert}}

\newcommand{\Type}{\mathsf{Type}}
\newcommand{\BinType}{\mathsf{BinType}}
\newcommand{\SubType}{\mathsf{SubType}}
\newcommand{\USG}{\mathsf{USG}}

% \newcommand{\UE}{\mathsf{UE}}
% \newcommand{\dUE}{\mathsf{dUE}}
\newcommand{\sUE}{\mathsf{sUE}}

\newcommand{\sfU}{\mathsf{U}}

\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\bmu}{\boldsymbol{\mu}}

\newif\ifnotes
%%%%%%%%%%% Author notes %%%%%%%%%%%%%%%%
%\notesfalse
\notestrue
\newcommand{\authnote}[3]{\textcolor{#3}{[{\footnotesize {\bf #1:} { {#2}}}]}}
\newcommand{\vinod}[1]{\ifnotes \authnote{V}{#1}{blue} \fi}
\newcommand{\orz}[1]{\ifnotes \authnote{O}{#1}{purple} \fi}


\title{Improving Algorithmic Efficiency using Cryptography}
\author{Vinod Vaikuntanathan\\MIT \and Or Zamir\\Tel Aviv University}
\date{}


\def\vecy{\mathbf{y}}
\def\vect{\mathbf{t}}
\def\vecx{\mathbf{x}}
\def\Z{\mathbb{Z}}
\def\matA{\mathbf{A}}
\def\lat{\mathcal{L}}
\def\vecz{\mathbf{z}}
\def\vece{\mathbf{e}}
\def\vecs{\mathbf{s}}
\def\matT{\mathbf{T}}
\def\vecc{\mathbf{c}}
\def\vecv{\mathbf{v}}
\def\vecw{\mathbf{w}}
\def\veca{\mathbf{a}}
\def\vecb{\mathbf{b}}
\def\vecd{\mathbf{d}}

\begin{document}
\maketitle


%\newpage
%\thispagestyle{empty}
%\newpage
%\pagenumbering{roman}
%\tableofcontents
%\newpage
%\pagenumbering{arabic}

\def\matM{\mathbf{M}}
\def\matA{\mathbf{A}}
\def\matB{\mathbf{B}}
\def\matE{\mathbf{E}}
\def\bbF{\mathbb{F}}

\begin{abstract}
Cryptographic primitives have been used for various non-cryptographic objectives, such as eliminating or reducing randomness and interaction. We show how to use cryptography to \emph{improve the time complexity} of solving computational problems. Specifically, we show that under standard cryptographic assumptions, we can design algorithms that are asymptotically faster than existing ones while maintaining correctness.

As a concrete demonstration, we construct a distribution of \emph{trapdoored matrices} with the following properties: (a) computationally bounded adversaries cannot distinguish a random matrix from one drawn from this distribution, and (b) given a secret key, we can multiply such a $n\times n$ matrix with any vector in near-linear (in $n$) time. We provide constructions both over finite fields and the reals.
This enables a broad speedup technique: any algorithm relying on a random matrix—such as those using various notions of dimensionality reduction—can replace it with a matrix from our distribution, achieving computational speedups while preserving correctness.
\end{abstract}

\section{Introduction}
Cryptographic techniques have long played a foundational role in securing communication, ensuring privacy, and enabling secure computation. However, beyond their traditional cryptographic applications, such techniques have also been leveraged to achieve non-cryptographic objectives. Notable examples include using cryptographic hardness assumptions or notions inspired by cryptography to reduce randomness in algorithms (e.g.,~\cite{blummicali82,haastad1999pseudorandom,nisan1994hardness,impagliazzo1997p}) or eliminate interaction in protocols (e.g., the celebrated Fiat-Shamir heuristic~\cite{fiat1986prove}). These applications illustrate how computational hardness assumptions can provide algorithmic benefits beyond security.

In this work, we explore a novel use of cryptography: improving the time complexity of solving computational problems. Specifically, we demonstrate that under standard cryptographic assumptions, we can design algorithms that are asymptotically faster than existing ones while ensuring correctness on input instances generated by computationally bounded adversaries. We give a simple demonstration of this idea by leveraging structured randomness derived from cryptographic primitives to accelerate common computational tasks.

To illustrate this approach, we introduce the notion of \emph{trapdoored matrices}, a class of pseudorandom matrices that allow efficient multiplication when a secret key is available. We construct a distribution of such matrices that satisfies the following key properties:
\begin{itemize}
\item\textbf{Indistinguishability:} A computationally bounded adversary cannot distinguish between a truly random matrix and one sampled from our distribution.
\item\textbf{Efficient Multiplication:} Given a secret key, matrix-vector multiplication can be performed in near-linear time.
\end{itemize}
We provide constructions of trapdoored matrices over both finite fields and the reals. Over a finite field, our construction is based on the hardness of the Learning Parity with Noise problem, a common cryptographic assumption. Over the reals, we introduce and motivate a hardness assumption based on Kac's random walks.

As a consequence, any algorithm that relies on a random matrix—such as those used in dimensionality reduction or sketching—can instead use a Trapdoored Matrix, achieving significant speedups due to its efficient multiplication.
Crucially, the cryptographic notion of indistinguishability enables a black-box reduction from the correctness of the modified algorithm to that of the original.
In particular, the reduction is independent of the specific properties of the random matrix required by the algorithm and its proof.

\subsection{Applications of Random Matrices in Algorithm Design}
Random matrices play a fundamental role in modern algorithm design, particularly in settings where dimensionality reduction, efficient approximation, or sketching techniques are required. One of the most well-known applications is the \emph{Johnson-Lindenstrauss (JL) Lemma}, which states that a set of~$n$ points in a possibly high-dimensional Euclidean space can be embedded into a~$\Theta\left(\log n / \varepsilon^2\right)$-dimensional space while approximately preserving pairwise Euclidean distances up to~$\left(1\pm \varepsilon\right)$ multiplicative error. The standard JL transform relies on mapping the set of points through a random projection matrix, often drawn from Gaussian or sub-Gaussian distributions.~\cite{johnson1984extensions}

Beyond the JL lemma, random matrices and the idea of randomized dimensionality reductions are used in various other algorithmic settings, including:
\begin{itemize}
\item
\textbf{Sketching and Streaming:} Randomized sketching techniques, such as AMS, CountSketch, and various other streaming algorithms, enable efficient approximation of high-dimensional data with lower memory and computation costs.~\cite{alon1996space,charikar2002finding}
\item
\textbf{Randomized Singular Value Decomposition (SVD):} Techniques such as those introduced by Halko, Martinsson, and Tropp~\cite{halko2011finding} use random projections to construct a low-rank approximation of a matrix. This is used to efficiently approximate dominant singular values and singular vectors, with applications in machine learning and numerical linear algebra.
\item
\textbf{Subspace Embeddings:} Random matrices can be used to construct oblivious subspace embeddings~\cite{sarlos2006improved,woodruff2014sketching}, which facilitate efficient least squares regression, low-rank approximation, and optimization problems by approximately preserving the geometry of a subspace.
\item
\textbf{Spectral Sparsifiers:} Random sampling techniques, some of which involve random matrices, help construct spectral sparsifiers~\cite{spielman2011spectral}, which approximate the spectral properties of a graph while significantly reducing the number of edges. These sparsifiers are useful in solving linear systems, graph algorithms, and optimization.
\item
\textbf{Error-Correcting Codes (ECCs):} Random matrices play a fundamental role in coding theory, particularly in the construction of random linear codes, where a randomly generated generator matrix is used to encode messages into redundant codewords for error correction. \cite{varshamov1957estimate}
\item
\textbf{Dimensionality Reduction Beyond JL:} The JL lemma is optimal in the sense that~$\Omega\left(\log n / \varepsilon^2\right)$ dimensions are needed to preserve all pair-wise distances for \emph{any} Euclidean set of~$n$ points.~\cite{larsen2017optimality}
On the other hand, if additional assumptions are made regarding the set of points, a reduction to a lower dimension is sometimes possible.
For example, better bounds exist when the set of points is assumed to be of low \emph{intrinsic dimension} (e.g., doubling dimension) or when not all pair-wise distances are required to be preserved. \cite{indyk2007nearest,boutsidis2010random,cohen2015dimensionality,makarychev2019performance,narayanan2021randomized}
\end{itemize}


In all of these applications, random matrices are integral to the algorithm: they are multiplied by vectors or other matrices, often becoming the computational bottleneck. Since a random $m$-by-$n$ matrix is dense and unstructured, even multiplying it by a single vector takes~$\Theta(mn)$ time.

To mitigate this bottleneck, researchers have sought alternative constructions that preserve the key properties of random matrices while allowing for more efficient computation. One prominent example is the extensive study of \emph{Fast JL Transforms}~(e.g., \cite{ailon2009fast,ailon2013almost,bamberger2021optimal,jain2022fast}), which provide fast, low-dimensional embeddings that preserve pairwise distances, similar to traditional Johnson-Lindenstrauss (JL) random projections, but with significantly reduced computational cost---culminating in constructions that run in near-linear time. Another notable example is the development of \emph{error-correcting codes with fast encoding}.

However, these tailored constructions lack the full flexibility of truly random linear transformations. A fast transform designed to preserve pairwise distances may not necessarily maintain other structural properties, such as spectral norms or subspaces, making it unsuitable for the applications it was not specifically designed for.
Our core idea is to replace these randomly sampled matrices with structured pseudorandom matrices---\emph{trapdoored matrices}---that retain the algorithmic utility of fully random matrices while allowing for faster multiplication. This enables a black-box transformation: \emph{any} algorithm that relies on random matrices can be accelerated without requiring new, problem-specific correctness proofs, making our approach broadly applicable across a wide range of applications.

\subsection{A Specific Application: Faster Classification Inference}
We highlight an application where existing fast transforms are not known, demonstrating the versatility of our approach.
It was recently shown that for the task of preserving the \emph{optimal clustering} of a point set, a random projection to a much lower dimension than the JL bound suffices.
Optimal bounds on the dimension were proven when parametrized by the number of clusters~\cite{makarychev2019performance} or by an intrinsic dimension of the point set~\cite{narayanan2021randomized}.
These results rely on intricate arguments beyond standard applications of the JL lemma.

These findings found a compelling application in \emph{Machine Learning}: when training a \emph{classifier} (i.e., a model that distinguishes between different \emph{classes} of inputs), the information often comes from the clustering structure of the data.
Thus, if we apply a transformation that preserves optimal clustering, we can reduce the dimensionality of the training set before running the training algorithm, significantly lowering the computational cost of training and reducing the number of parameters in the model.
At inference time, we would apply the same transformation to new inputs before classification.
Empirical results confirm the viability of this approach:~\cite{narayanan2021randomized} demonstrated that applying a random linear transformation to commonly studied datasets (such as the MNIST handwritten digits dataset and the ISOMAP face image dataset~\cite{tenenbaum2000global}) preserves the separation to classes even after significant dimensionality reduction.

While this method accelerates training, it does not improve the \emph{inference} time - the time required to classify an input - since applying a random linear transformation remains as costly as applying an arbitrary one.
By replacing the random transformation with a \emph{trapdoored matrix}, we not only retain the training-time savings but also achieve \emph{fast inference} by reducing the cost of applying the initial transformation.

\paragraph{Note:} In a concurrent and independent work, Braverman and Newman\footnote{A reference to their work will be added in subsequent versions after both papers will become publicly available.} suggest and use a construction very similar to that of our Section~\ref{sec:lpn}, in a different context motivated by privacy-preserving delegation of computation. 

\section{Definitions and Results}
We provide several constructions of trapdoored matrices over different fields and based on different cryptographic assumptions.
We begin with a rigorous definition of \emph{trapdoored matrices}.

\begin{definition}[Trapdoored Matrices over~$\mathbb{F}^{n \times m}$]
    A distribution of \emph{Trapdoored Matrices} over~$\mathbb{F}^{n \times m}$ is an efficiently samplable distribution~$\mathcal{D}$ over pairs~$(M,C)$ in which the first coordinate is a matrix~$M\in \mathbb{F}^{n \times m}$ and the second coordinate is a \emph{circuit}~$C$, such that for \emph{every} vector~$v\in \mathbb{F}^n$ the circuit computes the matrix-vector multiplication~$C(v)=Mv$.
\end{definition}

To be considered good, we want the distribution to have \emph{efficient multiplication} and \emph{indistinguishability}, which we define next.

\begin{definition}[Efficient Multiplication]
    For a time function~$T:\mathbb{N} \to \mathbb{N}$, we say that a Trapdoored Matrix distribution~$\mathcal{D}$ is~$T$-efficient if for every pair~$(M,C)$ sampled from~$\mathcal{D}$ and every vector~$v\in \mathbb{F}^n$, the circuit~$C$ can be applied to~$v$ in~$T(n)$ time.
\end{definition}

\begin{definition}[Indistinguishability]
    A Trapdoored Matrix distribution~$\mathcal{D}$ is  \emph{indistinguishable from the uniform distribution~$U$} if every polynomial-time algorithm~$\mathcal{A}$ is unable to distinguish whether it is given sample access to uniformly drawn random matrices from~$U$ or to matrices~$M$ drawn using~$\mathcal{D}$, that is
    \[
    \left| \Pr\left[\mathcal{A}(U) = 1\right]
    -
    \Pr\left[\mathcal{A}(\mathcal{D}) = 1\right]
    \right|
    \leq
    \mu(n),
    \]
    where~$\mu(n)$ is a function that vanishes quicker than any inverse polynomial in~$n$.
\end{definition}

The main conceptual contribution of our work is the following simple yet powerful insight: computational indistinguishability ensures that any algorithm using random matrices can instead use matrices drawn from a trapdoored matrix distribution without affecting correctness or success probability. Any observable difference in performance would itself serve as a polynomial-time \emph{distinguisher} between the uniform and trapdoored matrix distributions, contradicting indistinguishability.

Thus, we can replace matrix multiplications within the algorithm with the corresponding trapdoor circuits while preserving correctness, as these circuits produce exactly the same outputs -- just more efficiently.

We emphasize that correctness holds as long as the algorithm’s inputs are not chosen \emph{dependent on the trapdoor circuits}~$C$. However, they may depend on the matrices themselves, and in particular on previous computations involving them.

We also note that while we crucially rely on the full generality of indistinguishability to preserve correctness in a black-box manner, our reliance on hardness assumptions is actually \emph{weaker} than in most works in classical cryptography.
Typically, in cryptographic settings, a computationally bounded adversary is assumed to actively \emph{attempt} to break the assumption. In contrast, in our work, the potential distinguisher is merely a natural algorithm designed for an unrelated, non-cryptographic problem. As a result, it is even less likely that such a distinguisher exists for any of the hardness assumptions we introduce next. 

\subsection{Cryptographic Hardness Assumptions}
We present three (incomparable) hardness assumptions, each of our constructions depends on one of them.
\def\Derr{\mathsf{Bin}}

\paragraph{The Learning Parity with Noise Assumption:} %\orz{Vinod, can you add references?}
We define a version of the learning parity with noise assumption~\cite{DBLP:conf/crypto/BlumFKL93,DBLP:conf/focs/Alekhnovich03,DBLP:conf/tcc/IshaiPS09} generalized over arbitrary finite fields. 
Let $\bbF$ be a finite field and let $\Derr_p$ be an error distribution over $\bbF$ that outputs a random non-zero element of $\bbF$ with probability $p$ and $0$ with probability $1-p$. Define the following distributions: 
\def\dlpn{\mathcal{D}_{\mathsf{lpn}}}
\def\dunif{\mathcal{D}_{\mathsf{unif}}}
\begin{align*}
 & \dlpn := \{ (\veca_i, b_i): \vecs \gets \bbF^k, \veca_i \gets \bbF^k, e_i \gets \Derr_p, b_i = \langle \veca_i, \vecs \rangle + e_i \}_{i\in [n]} \\
 &  \dunif := \{ (\veca_i, b_i): \veca_i \gets \bbF^k, b_i \gets \bbF \}_{i\in [n]}
\end{align*}
The learning parity with noise (LPN) assumption says that the two distributions above are computationally indistinguishable. 

\begin{conjecture}[LPN]\label{conj:lpn}
    The learning parity with noise (LPN) assumption with parameters $k,n=\mathsf{poly}(k),p$, conjectures that for all probabilistic $\mathsf{poly}(k)$-time adversaries $A$, there is a negligible function $\mu:\mathbb{N} \to \mathbb{N}$ such that:
$$ 
\Pr[A(\dlpn) = 1] - \Pr[A(\dunif) = 1] \leq \mu(k)
$$
\end{conjecture}

Note that the security (e.g. the runtime and the distinguishing advantage of $A$) is parameterized by the LPN dimension $k$. The best algorithms known for LPN in dimension $k$ with $2^{O(k/\log k)}$ samples run in time $2^{O(k/\log k)}$~\cite{DBLP:journals/jacm/BlumKW03}; and with 
$\mathsf{poly}(k)$ samples runs in time $2^{O(k/\log \log k)}$~\cite{DBLP:conf/approx/Lyubashevsky05}, so Conjecture~\ref{conj:lpn} is on the safer side. The more aggressive subexponential LPN assumption says the following:


\begin{conjecture}[Subexponential LPN]\label{conj:subexp-lpn}
    The subexponential learning parity with noise (Subexp-LPN) assumption with parameters $k,n,p$, conjectures that there exists a constant $\alpha<1$ such that for $n \leq 2^{k^\alpha}$, for all probabilistic $2^{k^{\alpha}}$-time adversaries $A$:
$$ 
\Pr[A(\dlpn) = 1] - \Pr[A(\dunif) = 1] \leq 2^{-k^\alpha}
$$
\end{conjecture}

\paragraph{The McEliece Assumption Family:} 
Let $\matG \in \bbF^{n\times k}$ be the (fixed, publicly known) generator matrix of an $[n,k,d]$ linear code $\mathcal{C}$. The McEliece assumption~\cite{McEliece1978} w.r.t. $\mathcal{C}$ conjectures that the following two distributions on matrices are computationally indistinguishable:
\def\dmce{\mathcal{D}_{\mathsf{McEliece}}}
\begin{align*}
 & \dmce := \{ \matM = \matP \matG \matS: \matS \gets \bbF^{k\times k}, \matP \gets \mathsf{Perm}(n)\} \\
 &  \dunif := \{ \matM \gets \bbF^{n\times k}\}
\end{align*}
where $\mathsf{Perm}(n)$ is the set of all $n\times n$ permutation matrices. 

For a reader familiar with the McEliece encryption scheme, we remark that the assumption as formulated above is {\em sufficent} for the security of the McEliece cryptosystem (in conjunction with the LPN assumption over $\bbF$), but is not necessary for it.

McEliece formulated the assumption with $\mathcal{C}$ being the Goppa code~\cite{McEliece1978}; this still remains unbroken and is the basis of candidate encryption schemes submitted to the NIST post-quantum cryptography standardization process~\cite{ClassicMcE}. The McEliece assumption w.r.t. ``quasi-cyclic'' codes have been considered in the literature, is the basis of two different NIST encryption candidates~\cite{bike,hqc}. All these choices of codes permit {\em near-linear time encoding}: that is, given a vector $\vecv \in \bbF^k$, one can compute $\matG \vecv \in \bbF^n$ in time $\tilde{O}(n+k)$, as opposed to the trivial $O(nk)$. On the other hand, when $\mathcal{C}$ is either the Reed-Solomon or the Reed-Muller code, the assumption is broken via an attack of Sidelnikov and Shestakov~\cite{SS92}.

Our assumption will be that the McEliece assumption holds w.r.t. Goppa, quasi-cyclic or Hamming quasi-cyclic codes~\cite{ClassicMcE,bike,hqc}. As in the case of the LPN assumption, we can assume either the (conservative) polynomial or the (aggressive) subexponential versions of the assumption, with different results.


\paragraph{Psuedorandomness of Kac's Random Walks:}
Many of the applications of random matrices use Gaussian or sub-Gaussian matrices over the reals, rather than matrices over finite fields. 
Unfortunately, cryptographic hardness assumptions concerning non-periodic real numbers are scarce.
We thus introduce a computational hardness assumption regarding a well-studied mathematical object. We motivate this conjecture in Section~\ref{sec:reals}.

Let~$Q_0 = I_n \in \mathbb{R}^{n \times n}$ be the~$n$-dimensional identity matrix.
For any pair of distinct indices~$i,j\in [n]$ and any angle~$\theta\in [0,2\pi)$ we denote by~$R_{i,j,\theta}$ the rotation of the plane spanned by the basis vectors~$e_i,e_j$ by angle~$\theta$, that is, the sole linear transformation for which
\begin{align*}
    &R_{i,j,\theta}(e_k) = e_k  &\forall k\notin\{i,j\}\\
    &R_{i,j,\theta}(e_i) = \cos \theta \cdot e_i + \sin \theta \cdot e_j \; &\\
    &R_{i,j,\theta}(e_j) = -\sin \theta \cdot e_i + \cos \theta \cdot e_j \;. &
\end{align*}
We define a random walk starting from~$Q_0=I_n$ in which every step consists of a multiplication with a matrix~$R_{i,j,\theta}$ where the indices~$i,j$ and the angle~$\theta$ are drawn uniformly. That is, $i_t,j_t \gets [n]$, $\theta_t \gets [0,2\pi)$ and $Q_{t} = R_{i_t,j_t,\theta_t}$. The resulting matrix is the state~$Q_T$ of the random walk after~$T$ steps.
\begin{conjecture}\label{conj:kac}
    There exists some~$T=n \; \emph{poly}(\log n)$ such that the distribution of~$Q_T$ is computationally indistinguishable from the distribution~$U$ in which every column is an i.i.d. unit vector in~$S^{n-1}$.
\end{conjecture}

\subsection{Our Results}
We first observe that a construction of near-linear-time-efficient trapdoored matrices over \emph{square matrices} actually suffices to get one also for rectangular matrices or for matrix-matrix multiplication (rather than matrix-vector multiplication). 
This is because a rectangular matrix can be simply written as a concatenation of square ones, and matrix-matrix multiplication can be computed column-by-column. 
Furthermore, in some of our constructions we can also construct a near-linear time circuit computing matrix-vector multiplications with the matrix inverse~$M^{-1}$ and not only with the matrix~$M$ itself; this feature might be useful for certain applications.

Our first two constructions give near-linear-time-efficient trapdoored matrices for the uniform distribution of square matrices over a finite field~$\mathbb{F}$. Each of the two constructions is based on a different hardness assumption.
\begin{theorem*}[Section~\ref{sec:lpn}]
  Under the polynomial hardness of the learning parity with noise assumption over a field $\mathbb{F}$, there is a collection of trapdoored matrices for which, given the trapdoor, $n\times n$ matrix-vector multiplication can be performed in time $n^{1+\epsilon}$ for an arbitrarily small constant $\epsilon>0$. Assuming sub-exponential hardness, this can be done in time 
  $n\cdot \mathsf{poly}(\log n)$. 
\end{theorem*}
\begin{theorem*}[Section~\ref{sec:mcel}]
  Under the polynomial hardness of the McEliece assumption over a field $\mathbb{F}$, there is a collection of trapdoored matrices for which, given the trapdoor, $n\times n$ matrix-vector multiplication can be performed in time $n^{1+\epsilon}$ for an arbitrarily small constant $\epsilon>0$. Assuming sub-exponential hardness, this can be done in time $n\cdot \mathsf{poly}(\log n)$. 
\end{theorem*}

Our third construction is for the distribution~$U$ of real-valued matrices in which each column is a random unit vector of~$S^{n-1}$. This is the distribution for which Johnson-Lindenstrauss (JL) like dimension reduction algorithms work. 
\begin{theorem*}[Section~\ref{sec:reals}]
  Under the psuedorandomness of Kac's random walks, there is a collection of trapdoored matrices over the distribution~$U$ for which, given the trapdoor, $n\times n$ matrix-vector multiplication can be performed in time $n\cdot \mathsf{poly}(\log n)$. 
\end{theorem*}

\paragraph{A Bird's Eye View of Our Techniques.}
All our constructions start by identifying special classes of matrices for which matrix-vector multiplication can be performed in near linear time. Such matrices include low-rank matrices, sparse matrices, matrices with Vandermonde-like structure, and a product of a small number of elementary matrices. We then proceed to randomize these special matrices so as to hide their structure, but also preserve near-linear-time computability of matrix-vector products. Our LPN construction employs a combination of sparse and low-rank matrices; our McEliece construction (using Goppa codes) employs the fact that batch polynomial evaluation can be done fast; and our construction of trapdoored matrices over the reals uses Kac's random walk, i.e. products of a small number of elementary matrices.


\section{Trapdoored Matrices from LPN}\label{sec:lpn}

\begin{theorem}
  \label{thm:LPN}
   Under the polynomial hardness of the learning parity with noise assumption over a field $\mathbb{F}$, there is a collection of trapdoored matrices for which, given the trapdoor, $n\times n$ matrix-vector multiplication can be performed in time $n^{1+\epsilon}$ for an arbitrarily small constant $\epsilon>0$. Assuming sub-exponential hardness, this can be done in time 
  $n\cdot \mathsf{poly}(\log n)$.
\end{theorem}

\paragraph{The Base Construction.}
Fix an integer $k$ and a noise parameter $p \in [0,1]$ that we will choose subsequently.
A random matrix $\matM \in \bbF^{n\times m}$ from our distribution $\mathcal{D}_{n,k,p}$ is generated by sampling uniformly random $\matA \gets \bbF^{n\times k}$, $\matB \gets \bbF^{k\times n}$, and a Bernoulli matrix $\matE$ with entries drawn from $\mathsf{Bin}(p)$, and letting $$\matM = \matA \matB + \matE~.$$


The complexity of computing $\matM \vecv$ for an arbitrary $\vecv \in \bbF^n$ is
\begin{equation} \label{eqn:recursion}
T(n) = \frac{2n}{k} \cdot T(k) + O(n+pn^2) 
\end{equation}
since one first cuts $\vecv$ into $n/k$ pieces of length $k$ each, computes $n/k$ matrix-vector multiplications in $k$ dimensions followed by $n/k$ additions of $k$-dimensional vectors to compute $\matB \vecv$, and another $n/k$ matrix-vector multiplications in $k$ dimensions to compute $\matA \matB \vecv$. Computing $\matE \vecv$ takes another $pn^2$ operations.

The distinguishing advantage between $\matM \gets \mathcal{D}_{n,k,p}$ and a uniformly random matrix in $\bbF^{n\times n}$ is
$$ \delta(n) = n\cdot f(k,n,p)$$
where $f(k,n,p)$ is the advantage by which one can break LPN in $k$ dimensions with error parameter $p$ and $n$ samples.

Letting $T(k) = k^2$ and $p(k,n) = (\log k)^c/ k$  for a constant $c>1$ gives us
$$ T(n) = \frac{2n}{k} \cdot k^2 + O\bigg(n + \frac{n^2 (\log k)^c}{k} \bigg) = O\bigg(nk+ \frac{n^2 (\log k)^c}{k} \bigg)$$
Setting $k = \tilde{O}(\sqrt{n})$ gives us
$$T(n) = \tilde{O}(n^{3/2})~.$$
Pseudorandomness of $\matM$ relies on the plausible hardness of LPN in $k$ dimensions with $\tilde{O}(k^2)$ samples and noise rate $p = \frac{(\log k)^c}{k}$, which is conjectured to hold against $\mathsf{poly}(k)$-time adversaries as long as $c > 1$.


\paragraph{The Recursion.} Can we do better? We will run the recursion suggested by equation~(\ref{eqn:recursion}) by picking matrices $\matA$ and $\matB$ to be (trapdoored) pseudorandom matrices as well. That is, we will let the distribution $$\mathcal{D}_{i} := \mathcal{D}^{(i)}_{n_i,n_{i+1},p_{i}}$$ be a distribution over matrices $\matM_i \in \bbF^{n_i\times n_i}$ generated by 
\begin{itemize}
    \item  picking $n_{i+1} \times n_{i+1}$ matrices $\matA_{i,j} \gets \mathcal{D}_{i+1}$, for $j \in [n_{i}/n_{i+1}]$, and letting $$\matA_i := [\matA_{i,1} \sslash \matA_{i,2} \sslash \ldots \sslash \matA_{i,n_{i}/n_{i+1}}] \in \bbF^{n_i\times n_{i+1}}~;$$
    \item picking $n_{i+1} \times n_{i+1}$  matrices $\matB_{i,j} \gets \mathcal{D}_{i+1}$, for $j \in [n_{i}/n_{i+1}]$, and letting $$\matB_i := [\matB_{i,1} || \matB_{i,2} || \ldots || \matB_{i,n_{i}/n_{i+1}}] \in \bbF^{n_{i+1}\times n_{i}}~;$$
    \item letting $\matM_i = \matA_i \matB_i + \matE_i$ where $\matE_i \gets (\mathsf{Bin}(p_i))^{n_i\times n_i}$.
\end{itemize}
We will choose $n_0 = n$, $n_{i+1} = n_i^{1-\epsilon}$ and $p_i = n_{i+1}^{-\delta}$ for constants $\epsilon > 0$ and $0 < \delta < 1$, generalizing the construction above in the natural way. The recursion equation~\ref{eqn:recursion} then tells us that
\begin{align*} 
T(n_i) & = \frac{2n_{i}}{n_{i+1}}  \cdot T(n_{i+1})+ O(n_{i} + pn_i^2) \\
& = 2n_i^{\epsilon} \cdot T(n_{i}^{1-\epsilon}) + O\big(n_i^{2-\delta(1-\epsilon)}\big)
\end{align*}
Running the recursion for $\ell$ steps gives us matrices of dimension $n^{(1-\epsilon)^\ell}$ whereby we apply the trivial matrix-vector multiplication algorithm of complexity $n^{2\cdot (1-\epsilon)^\ell}$. To compute the runtime, observe that the first term in the recursion, after $\ell$ steps becomes 
$$ n^\epsilon \cdot  n^{\epsilon(1-\epsilon)} \cdot n^{\epsilon(1-\epsilon)^2} \cdot \ldots \cdot n^{\epsilon(1-\epsilon)^{\ell-1}} \cdot n^{2\cdot (1-\epsilon)^\ell}  \leq n^{\epsilon \cdot \frac{1}{\epsilon} + 2\cdot (1-\epsilon)^\ell} = n^{1+2\cdot (1-\epsilon)^\ell} $$
where the first inequality is by approximating the geometric sum $\sum_{j=0}^\ell (1-\epsilon)^j$ by $1/\epsilon$. Under the subexponential LPN assumption, we can only go as far as getting $n_\ell = n^{(1-\epsilon)^\ell}$ to be $\mathsf{poly}(\log n)$, which means this sum will be $n\cdot \mathsf{poly}(\log n)$. The second term of the sum becomes 
$$ n^c + n^{\epsilon+(1-\epsilon)\cdot c} + n^{\epsilon + \epsilon\cdot (1-\epsilon) + (1-\epsilon)^2\cdot c} + \ldots + n^{\epsilon\cdot \sum_{j=0}^{\ell-1} (1-\epsilon)^j + (1-\epsilon)^{\ell} c} $$
where $c := 2-\delta\cdot (1-\epsilon) > 1$.
Note the exponents decrease monotonically since $c<1$, and so the sum is dominated by the first term. Setting $\delta \approx 1$ and $\epsilon \approx 0$ gives us $c \approx 1$, and so the sum is $O(n\cdot \mathsf{poly}(\log n))$.

Since the base case is LPN in $\mathsf{poly}(\log n)$ dimensions with an inverse polynomial error rate, under the subexponential assumption, the distinguishing advantage is negligible in $n$. 

If we only assume the polynomial LPN assumption, on the other hand, we will stop the recursion at $n_\ell = n^{\gamma}$ for an arbitrarily small constant $\gamma > 0$. Thus will result in a complexity $T(n)$ that is $n^{1+\gamma}$ for an arbitraily small positive constant $\gamma$.


\begin{corollary}
    \label{cor:matmult}
     Under the sub-exponential hardness of the learning parity with noise assumption over a field $\mathbb{F}$, there is a collection of trapdoored matrices $\matM$ for which, given the trapdoor, $n\times n$ matrix-matrix multiplication with any matrix $\matV$ can be performed in time %$n^{1+\epsilon}$ for an arbitrarily small constant $\epsilon>0$. Assuming sub-exponential hardness, this can be done in time 
  $n^2\cdot \mathsf{poly}(\log n)$. 
\end{corollary}


\section{Trapdoored Matrices from McEliece}\label{sec:mcel}
\begin{theorem}
  \label{thm:McEliece}
  Under the polynomial hardness of the McEliece assumption over a field $\mathbb{F}$, there is a collection of trapdoored matrices for which, given the trapdoor, $n\times n$ matrix-vector multiplication can be performed in time $n^{1+\epsilon}$ for an arbitrarily small constant $\epsilon>0$. Assuming sub-exponential hardness, this can be done in time $n\cdot \mathsf{poly}(\log n)$. 
\end{theorem}


\paragraph{The Construction.} We will describe the base construction that achieves a runtime of $\tilde{O}(n^{3/2})$ here. A recursion of the exact same form as in the LPN setting gives us either $n^{1+\epsilon}$ or $n\cdot \mathsf{poly}(\log n)$, under the polynomial, resp. subexponential McEliece assumption.

Let $\matG$ be the $n\times k$ generator matrix of any near-linear-time encodable code $\mathcal{C}$. That is, given a vector $\vecv \in \bbF^k$, one can compute $\matG \vecv \in \bbF^n$ in time $\tilde{O}(n+k)$. Our trapdoored matrix will be the matrix $\matM = \matP \matG \matS$ where $\matP$ is a random $n\times n$ permutation matrix and $\matS$ is a random $k\times k$ matrix, that comes from the McEliece assumption.

Computing $\matM \vecv$ proceeds by iterative multiplication: first with $\matS$ taking time $O(k^2)$; then with $\matG$ taking time $\tilde{O}(k+n)$ using the linear-time encodability of the code; and finally with $\matP$ taking time $O(n)$. The total runtime is $\tilde{O}(k^2 + n)$, in contrast to the trivial $O(kn)$ time multiplication. 

To construct a square $n\times n$ trapdoored matrix, we stack $n/k$ instances of this construction side by side resulting in a matrix $\matM = [\matM_1 || \ldots || \matM_{n/k}]$ where each $\matM_i \in \bbF^{n\times k}$ is constructed as above. Now, the total runtime of a matrix-vector multiplication is 
$$ O(\bigg( \frac{n}{k} \cdot (k^2+n) + \frac{n}{k} 
 \cdot n\bigg)  = O\bigg(nk + \frac{n^2}{k} \bigg)$$
 where the first term is the time to perform the above algorithm $n/k$ times, and the second is the time to aggregate the answers. Setting $k = \sqrt{n}$ already gives us subquadratic time. A recursion on constructing $\matS$, similar to the LPN case, gives us quasilinear time. %\vinod{Be less sloppy.}

\section{Trapdoored Matrices over the Reals}\label{sec:reals}
We introduce a conjecture on Kac's random walks, yielding a simple construction of trapdoored matrices over the reals.

In his 1954 physics paper, Kac~\cite{kac1956foundations} introduced a random walk on the sphere as a model for a Boltzmann gas (for a summary on the influence of this model on kinetic theory, see~\cite{{mischler2013kac}}). 
This is a Markov chain in which the time is discrete, but the state space is continuous. 
We present it in terms of linear transformations.
Let~$Q_0 = I_n = \mathbb{R}^{n \times n}$ be the~$n$-dimensional identity matrix.
For any pair of distinct indices~$i,j\in [n]$ and any angle~$\theta\in [0,2\pi)$ we denote by~$R_{i,j,\theta}$ the rotation of the plane spanned by the basis vectors~$e_i,e_j$ by angle~$\theta$, that is, the sole linear transformation for which
\begin{align*}
    &R_{i,j,\theta}(e_k) = e_k  &\forall k\notin\{i,j\}\\
    &R_{i,j,\theta}(e_i) = \cos \theta \cdot e_i + \sin \theta \cdot e_j \; &\\
    &R_{i,j,\theta}(e_j) = -\sin \theta \cdot e_i + \cos \theta \cdot e_j \;. &
\end{align*}
We define a random walk starting from~$Q_0=I_n$ in which every step consists of a multiplication with a matrix~$R_{i,j,\theta}$ where the indices~$i,j$ and the angle~$\theta$ are drawn uniformly. That is~$Q_{t} \leftarrow R_{i_t,j_t,\theta_t}$. The resulting matrix is the state~$Q_T$ of the random walk after~$T$ steps.
Essentially, in each step we perform a random rotation in low-dimension. It is thus reasonable to expect that after enough steps we will converge to a random ``rotation'' of the entire space.

Understanding the mixing time of the Kac's random walk over the sphere (that is, considering the evolution of only a single column of~$Q_t$, or equivalently of~$Q_t \cdot e_1$) already required significant mathematical effort. In a sequence of breakthrough papers~\cite{janvresse2001spectral,mischler2013kac,pillai2017kac} it was proven that when~$T=\Theta(n \log n)$ the distribution of the resulting state~$Q_t \cdot e_1$ is already approaching the uniform distribution of~$S^{n-1}$, even in the total variation distance.
This is the best possible mixing time due to a simple lower coupon collector-style lower bound. 

Understanding the mixing time of the entire transformation~$Q_t$ is even more challenging. Although it is easy to show that the chain converges to the uniform distribution in the special orthogonal group~$SO(n)$, only in 2012, Jiang~\cite{jiang2012total} showed that the chain mixes in polynomial time. The bounds have since been improved and it is currently known that the mixing time is between~$\Omega(n^2) \leq T \leq O(n^4 \log n)$.

We pose the following natural conjecture, stating that after~$n\;\poly \log n$ the resulting state~$Q_T$ should already be \emph{computationally indistinguishable} from the uniform distribution over~$SO(n)$. As stated above, after that number of steps any small set of columns is distributed as i.i.d. samples from~$S^{n-1}$, yet the distribution of the entire matrix is still not \emph{statistically} close to uniform. 
We are not aware of previous conjectures or works considering the computational psuedo-randomness of these walks.

\begin{conjecture*}
    There exists some~$T=n \;\poly\log(n)$ such that the distribution of~$Q_T$ is computationally indistinguishable from the distribution~$U\left(SO\left(n\right)\right)$.
\end{conjecture*}

We note that many existing hardness assumptions can also be phrased as running a mixing Markov chain for fewer steps than necessary for statistical convergence and conjecturing that the resulting state is pseudo-random.
Further supporting this conjecture is a recent result~\cite{jain2022fast} in which it was shown that the transformation~$Q_T$ with~$T=\Theta(n \log n)$ suffices for recovering the JL Lemma. This is an intricate unconditional proof that~$Q_T$ preserves all pair-wise distances of a point-set w.h.p., recovering the previous results on Fast JL Transforms~\cite{ailon2013almost}.
We conjecture that the same distribution is in fact \emph{psuedorandom}, and would thus suffice for the far more general applications as well.

Assuming this conjecture, a construction of trapdoored matrices is straightforward: Every step of the Kac's walk affects only two coordinates and can thus be applied in~$O(1)$ time to a vector. 
Furthermore, as each step is easily invertible we are also able to quickly apply the inverse matrix~$Q_T^{-1}$.

\section{Summary and Discussion}
The main contribution of this work is conceptual: we propose speeding up classical algorithms by replacing a truly random object with a pseudorandom one that contains a trapdoor — of a form that allows an equivalent yet more efficient computation. We raise the question of whether this idea can be extended beyond random matrices to other objects.

Another important direction is to further support or refute Conjecture~\ref{conj:kac}, for instance, by showing it is implied by more standard cryptographic assumptions, or by showing that natural attack strategies are bound to fail.

\paragraph{Acknowledgments:}
OZ's research is supported in part by the Israel Science Foundation, Grant No. 1593/24, and in part by the Blavatnik Family foundation. VV's research is supported in part by DARPA under Agreement Number HR00112020023, NSF CNS-2154149, a Simons Investigator
award and a Thornton Family Faculty Research Innovation Fellowship from MIT.

\bibliographystyle{alpha}
\bibliography{refs}

\end{document}
