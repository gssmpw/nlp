\section{Related Work}\label{sec:related}










\subsection{Causally Masked Language Modeling}

\fix{Causal Language Modeling (CLM) is an autoregressive method employed in models such as GPT-4~\citep{achiam2023gpt}, predicting the next token using only prior token information. While effective in applications like text generation~\citep{ li2024pre} and dialogue systems~\citep{hosseini2020simple}, CLM's unidirectional approach is a limitation. 
}
\fix{Masked Language Modeling (MLM), used in models like BERT~\citep{devlin2018bert}, predicts hidden tokens using bidirectional context. Although it processes only about 15\% of tokens during training, limiting some uses, MLM is still broadly applied in biology for representation learning~\citep{chithrananda2020chemberta, lin2023esm}.}
Causally Masked objective improve MLM by providing a type of hybrid of causal and masked language models by enabling full generative modeling while also providing bidirectional context when generating the masked spans. 
However, the existing State-of-the-Art (SOTA) Causally Masked models~\citep{aghajanyan2022cm3} are still limited in enabling the controllable mutation function including conditional expansion and contraction, and the current design of masked tokens leads to misleading interpretations between the masked token and its context. In this work, we address both limitations by redesigning the masked tokens and incorporating a sequence-to-sequence model.

\subsection{Sequence-to-Sequence Modeling }

Seq2Seq, or Sequence-to-Sequence models~\citep{dey2017gate,graves2012long,xue2020mt5,wang2020fairseq, ni2021sentence}, employ an encoder-decoder structure where the encoder interprets the input sequence, and the decoder constructs the output sequence. This method is frequently utilized in tasks such as machine translation~\citep{chen2018best,tiwari2020english,wang2022understanding}, summarization~\citep{prasad2020automatic,shi2021neural}, and question-answering~\citep{tang2018learning,wu2020seq2seq}. Due to their ability to manage complex tasks that require transforming input into output, Seq2Seq models are highly versatile and suitable for a broad spectrum of NLP applications.
Nevertheless, Seq2seq models exhibit limitations in coherence, context understanding, handling variable-length inputs, training efficiency, and capturing bidirectional context when compared to CLMs and MLMs. In this study, we introduce Causally Masked Seq2seq (CMS) modeling, conceptualizing the seq2seq model as a controllable conditional mutation component in biological sequences, and harnessing the strengths of seq2seq models, CLMs, and MLMs.













\subsection{Controllable Generation.}
In the field of computer vision, the introduction of generative adversarial networks~\citep{goodfellow2014generative}  enhanced the quality of image generation. Subsequent research focused on methods to control the generative process and improve the estimation of generative distributions~\citep{kingma2013auto,chen2016infogan,arjovsky2017wasserstein}. In the realm of natural language processing, language models are often developed as conditional models tailored for specific text generation tasks~\citep{brants2007large,sutskever2014sequence,rush2015neural}. Typically, prompts created by models or those written by humans serve merely as a rough starting point for the generated text. This raises questions about how to achieve more explicit control over text generation. Recent advancements in transformer architecture~\citep{vaswani2017attention,radford2019language} and diffusion models~\citep{ho2020denoising} have led to improved control in both text and image generation~\citep{li2019controllable,keskar2019ctrl,raffel2020exploring,li2022diffusion,epstein2023diffusion,zhang2023survey,liang2024controllable}. However, these techniques are not specifically adapted for biological sequences, which may involve unique challenges in nature, such as expansion, reduction, or mutation at specific locations and ranges with desired properties. 
\algname is specifically designed for biological sequences and addresses these challenges by introducing a novel CMS objective.


\subsection{Large Language Models for Drug Optimization}



Large language models have been employed in molecule generation, as evidenced by studies such as MolGPT~\citep{bagal2021molgpt}, C5T5~\citep{rothchild2021c5t5}, and ChemGPT~\citep{frey2023neural}. More recently, ERP~\citep{liu2024erp} has utilized LLMs for drug discovery. In contrast, our work focuses on the drug optimization domain to improve upon existing drugs rather than designing from scratch. In the drug optimization domain, 
DrugImprover~\citep{liu2023drugimprover} starts to effectively define the drug optimization problem by using reinforcement learning with a combination of multiple objectives. Moreover, it integrates Tanimoto similarity~\citep{landrum2016rdkit} as an additional term in the rewards function to ensure that the RL-fine-tuned model generates molecules similar to existing drugs. However, DrugImprover employs an LSTM as the generative model, which has limitations in scalability, capacity, and contextual understanding.
Reinvent 4~\citep{he2021molecular, he2022transformer, loeffler2024reinvent} has made efforts in developing transformer-based generative models and achieving state-of-the-art performance in the Drug Optimization domain with an emphasis on pretraining with simply adopt REINFORCE~\citep{williams1992simple} finetuning. 
Although pretraining aids in producing molecules that resemble those in the training dataset, it naturally limits the scope of exploration because of biases inherent in the training data.
In addition, REINVENT 4 lacks controllability during generation, a crucial aspect for drug optimization. In this study, we tackle controllability issues and surpass the current state-of-the-art, REINVENT 4, in drug optimization benchmarks.





















