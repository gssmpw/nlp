\begin{abstract}

Large Language Models (LLMs) employ three popular training approaches: Masked Language Models (MLM), Causal Language Models (CLM), and Sequence-to-Sequence Models (seq2seq). However, each approach has its strengths and limitations, and faces challenges in addressing specific tasks that require controllable and bidirectional generation, such as drug optimization.
To address this challenge, inspired by the biological processes of growth and evolution, which involve the expansion, shrinking, and mutation of sequences, we introduce \algname. This initiative represents the first effort to combine the advantages of MLM, CLM, and seq2seq into a single unified, controllable GPT framework. It enables the precise management of specific locations and ranges within a sequence, allowing for expansion, reduction, or mutation over chosen or random lengths, while maintaining the integrity of any specified positions or subsequences.
In this work, we designed \algname for drug optimization from the ground up, which included {proposing the Causally Masked Seq2seq (CMS) objective}, developing the training corpus, introducing a novel pre-training approach, and devising a unique generation process.
We demonstrate the effectiveness and controllability of \algname by conducting  experiments on drug optimization tasks for both viral and cancer benchmarks, surpassing competing baselines.

\end{abstract} 









