\section{Preliminaries}
\textbf{LLM.} Let $\tokenSeq=\bracket{\token_1, \token_2, \cdots, \token_n}$ be a sequence of tokens representing an input sentence (prompt), where each $\token_i$ is a token from a vocabulary $\Vocabulary$. Let $\mathbf{Y}=\bracket{y_1,y_2,\cdots, y_T}, y_i\in \mathcal{Y}$ be the output sequence of tokens with vocabulary $\mathcal{Y}$.  $\Vocabulary$ and $\mathcal{Y}$ are potentially different vocabularies. Note that $\mathbf{y}_{<t} = \bracket{y_1, \cdots, y_{t-1}}, \mathbf{y}_{T}:=Y$.
$\horizon$ represents the length of sequence. 
Each training corpus begins with a start token $\bracket{\text{BOS}}$, follows with a sequence of tokens $\mathbf{y}$ where each $y_i$ belongs to $\Vocabulary$, and concludes with a termination action $\bracket{\text{EOS}}$.
Each molecule is depicted using a sequence of tokens $\mathbf{y}$ to assemble a SMILES string, applicable to both incomplete and complete molecular structures.
Let us denote $\circ$ as string concatenation, and let $\mathcal{V}^*$ represent the Kleene closure of $\mathcal{V}$.
The set of training corpus $\corpus$ is defined as: 
    $\corpus := \curlybracket{\text{[BOS]} \circ \mathbf{v} \circ \text{[EOS]}~|~\mathbf{v}\in \mathcal{V}^*}.$

The LLM generator policy $\policy_{\theta}$, which is parameterized by a deep neural network (DNN) with learned weights $\theta$, is defined as a product of probability distributions:
$\policy_{\theta}\paren{\mathbf{y}|\mathbf{x}}=\prod_{t=1}^{|\mathbf{y}|} \policy_{\theta}\paren{y_t|\mathbf{x},\mathbf{y}_{<t}}$, where  $\policy_{\theta}\paren{y_t|\mathbf{x},\mathbf{y}_{<t}}=P\paren{y_{t}|\mathbf{y}_{<t},X}$ is a distribution of next token $y_t$.
The text generation decoding process is designed to select the most probable hypothesis from all possible candidates by addressing the following optimization problem:
$\mathbf{y}^{\star}=\argmax_{\mathbf{y}\in \hypothesis_{\horizon}} \log \policy_{\theta}\paren{\mathbf{y}|\mathbf{x}}. $






\textbf{CLM.}
CLM is a variant of language modeling where the model is trained to estimate the probability of $\token_i$ conditioned on the preceding tokens $\mathbf{\tokenSeq}_{<i}$, where $\mathbf{\tokenSeq}_{<i}= \token_1,\token_2,\cdots, \token_{i-1}$, in typically an autoregressively manner.
The objective of CLM is to maximize the log likelihood of observing the correct next token $\token_i$ given all the previous tokens in the sequence $\tokenSeq_{<i}$, which could be formulated as 
$\max_{\theta}\sum^n_{i=1} \log P\paren{x_i|\mathbf{\tokenSeq}_{<i};\theta}$, where $P\paren{x_i|\mathbf{\tokenSeq}_{<i};\theta}$ is the conditional probability of observing token $\token_i$ given all the preceding tokens $\tokenSeq_{<i}$.
Causal Language Modeling is particularly powerful for generating text, as it conditions on all previous tokens, ensuring that each generated word is based on the full history of the text generated so far. 





\textbf{MLM.}
In MLM, a subset (around 15\%) of the tokens in $\tokenSeq$ is randomly selected and replaced with a special token [MASK].
Let us denote this masked sequence as $\maskedTokens$ and unmasked sequence as $\unmaskedTokens$, $\unmaskedTokens=\curlybracket{x_i},x_i\in \tokenSeq~\text{and}~ x_i\notin \maskedTokens$.
The objective of the MLM is to predict the original tokens of the masked positions based solely on the unmasked context $\unmaskedTokens$, which can be represented as maximizing the likelihood: $\likelihood_{MLM}=\prod_{i\in \maskedTokens} P\paren{\token_i|\unmaskedTokens;\theta}$, where 
$P\paren{\token_i|\unmaskedTokens;\theta}$ represents the conditional probability of observing token $\token_i$ given the context provided by the unmasked tokens in $\unmaskedTokens$.
$\theta$ represents the parameters of the model.
The parameters  $\theta$ of the model 
are optimized to maximize the likelihood of the correct tokens at the masked positions.
During training, the model learns to utilize the surrounding context to predict the masked tokens, which helps it develop a deep understanding of language structure and usage.
MLM has proven effective for pre-training language models that are later fine-tuned for various downstream tasks.













\textbf{Seq2Seq.}
Sequence-to-sequence (seq2seq) modeling is a framework in natural language processing designed to convert sequences from input sequence to output sequence.
Seq2seq models typically consist of two main components: an encoder and a decoder, with model parameter $\theta_{enc}$ and $\theta_{dec}$ respectively. 
The encoder processes the input sequence $\tokenSeq$ to a fixed-dimensional vector representation $\mathbf{c}$ 
to capture the semantic or contextual information. The decoder's objective is to generate the target sequence $\mathbf{Y}$ given the encoded representation $\mathbf{c}$. 
The objective in training seq2seq models is typically to maximize the log likelihood of the correct output sequence $\mathbf{Y}$ given the input sequence $\tokenSeq$ across a dataset of paired sequences:
$\max_{\theta_{enc},\theta_{dec}}\sum_{\paren{\tokenSeq,\mathbf{Y}}}\log P\paren{\mathbf{Y}|\tokenSeq}$, where $\mathbf{P}$ is product of the conditional probabilities of each output token and
$P\paren{\mathbf{Y}|\tokenSeq}=\prod^n_{j=1}P\paren{y_j|\mathbf{Y}_{<j},\mathbf{c};\theta_{dec}}$.
Training involves adjusting both the encoder and decoder parameters to optimize this objective.
Seq2seq models are powerful because they can handle variable-length input and output sequences and are capable of learning complex transformations between different types of sequence data.




\textbf{Limitation.}
The existing models -CLM, MLM, and seq2seq- have limitations in controllable generation, which is especially important for drug optimization tasks that need to preserve specific structures and allow expansion, shrinking, or mutation at specific positions. The current state of the art in drug optimization, REINVENT 4, although it incorporates various similarity metrics in building the training corpus for pre-training the transformer model, still does not yield ideal results due to a lack of controllability in generation. The beneficial structure of the original drug often fails to preserve. In this work, we propose \algname, which effectively addresses above limitations of current GPT models in controllable drug optimization.














