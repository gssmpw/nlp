\section{Related Work}
\label{sec:related}










\subsection{Causally Masked Language Modeling}

\fix{Causal Language Modeling (CLM) is an autoregressive method employed in models such as GPT-4____, predicting the next token using only prior token information. While effective in applications like text generation____ and dialogue systems____, CLM's unidirectional approach is a limitation. 
}
\fix{Masked Language Modeling (MLM), used in models like BERT____, predicts hidden tokens using bidirectional context. Although it processes only about 15\% of tokens during training, limiting some uses, MLM is still broadly applied in biology for representation learning____.}
Causally Masked objective improve MLM by providing a type of hybrid of causal and masked language models by enabling full generative modeling while also providing bidirectional context when generating the masked spans. 
However, the existing State-of-the-Art (SOTA) Causally Masked models____ are still limited in enabling the controllable mutation function including conditional expansion and contraction, and the current design of masked tokens leads to misleading interpretations between the masked token and its context. In this work, we address both limitations by redesigning the masked tokens and incorporating a sequence-to-sequence model.

\subsection{Sequence-to-Sequence Modeling }

Seq2Seq, or Sequence-to-Sequence models____, employ an encoder-decoder structure where the encoder interprets the input sequence, and the decoder constructs the output sequence. This method is frequently utilized in tasks such as machine translation____, summarization____, and question-answering____. Due to their ability to manage complex tasks that require transforming input into output, Seq2Seq models are highly versatile and suitable for a broad spectrum of NLP applications.
Nevertheless, Seq2seq models exhibit limitations in coherence, context understanding, handling variable-length inputs, training efficiency, and capturing bidirectional context when compared to CLMs and MLMs. In this study, we introduce Causally Masked Seq2seq (CMS) modeling, conceptualizing the seq2seq model as a controllable conditional mutation component in biological sequences, and harnessing the strengths of seq2seq models, CLMs, and MLMs.













\subsection{Controllable Generation.}
In the field of computer vision, the introduction of generative adversarial networks____  enhanced the quality of image generation. Subsequent research focused on methods to control the generative process and improve the estimation of generative distributions____. In the realm of natural language processing, language models are often developed as conditional models tailored for specific text generation tasks____. Typically, prompts created by models or those written by humans serve merely as a rough starting point for the generated text. This raises questions about how to achieve more explicit control over text generation. Recent advancements in transformer architecture____ and diffusion models____ have led to improved control in both text and image generation____. However, these techniques are not specifically adapted for biological sequences, which may involve unique challenges in nature, such as expansion, reduction, or mutation at specific locations and ranges with desired properties. 
\algname is specifically designed for biological sequences and addresses these challenges by introducing a novel CMS objective.


\subsection{Large Language Models for Drug Optimization}



Large language models have been employed in molecule generation, as evidenced by studies such as MolGPT____, C5T5____, and ChemGPT____. More recently, ERP____ has utilized LLMs for drug discovery. In contrast, our work focuses on the drug optimization domain to improve upon existing drugs rather than designing from scratch. In the drug optimization domain, 
DrugImprover____ starts to effectively define the drug optimization problem by using reinforcement learning with a combination of multiple objectives. Moreover, it integrates Tanimoto similarity____ as an additional term in the rewards function to ensure that the RL-fine-tuned model generates molecules similar to existing drugs. However, DrugImprover employs an LSTM as the generative model, which has limitations in scalability, capacity, and contextual understanding.
Reinvent 4____ has made efforts in developing transformer-based generative models and achieving state-of-the-art performance in the Drug Optimization domain with an emphasis on pretraining with simply adopt REINFORCE____ finetuning. 
Although pretraining aids in producing molecules that resemble those in the training dataset, it naturally limits the scope of exploration because of biases inherent in the training data.
In addition, REINVENT 4 lacks controllability during generation, a crucial aspect for drug optimization. In this study, we tackle controllability issues and surpass the current state-of-the-art, REINVENT 4, in drug optimization benchmarks.