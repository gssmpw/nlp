








































































































\section{Appendix}



{\subsection{Pre-training Details}\label{app:training_detail}


We used the ZINC dataset, filtering for Standard, In-Stock, and Drug-Like molecules, resulting in approximately 11 million molecules.

In the second phase of pre-training, we first trained for 10 epochs using a single mask. Subsequently, we trained for another 40 epochs with an equal probability of using either one or two masks. For each epoch, the masks were regenerated to create a more comprehensive masked dataset.

In the third phase of pre-training, we applied different mask configurations with specific probabilities: [one mask (0.1), two masks (0.1), one mask and one seq2seq (0.4), two masks and one seq2seq (0.4)] and train 20 epochs.Similar to the second phase, the masks were regenerated for each epoch to enhance the comprehensiveness of the masked dataset.}


\fix{\subsection{Baselines fine-tuning datasets}

As outlined in Section \ref{experiments}, all baseline models are fine-tuned using the Cancer and COVID dataset, following their respective fine-tuning methodologies. For this process, we utilize one million compounds from the ZINC15 dataset, docked to the 3CLPro protein (PDB ID: 7BQY), which is linked to SARS-CoV-2, and the RTCB protein (PDB ID: 4DWQ), associated with human cancer. These datasets, sourced from the latest Cancer and COVID dataset by \citet{liu2023drugimprover}, are consistently applied across all baselines.

Additionally, these datasets are employed for molecular generation in our proposed methods, with further details on the generation process provided in Section \ref{app:generation}.}

{\subsection{Generation}\label{app:generation}



For each mask and seq2seq, we utilize three random variables: the start index, the number of tokens to be masked, and the number of tokens to be generated. During generation, we apply two settings: [one mask + one seq2seq, and two masks], resulting in a total of six random variables for each setting.

During the generation phase, we randomly sample these six variables 10,000 times, using them as prompts for generation, regardless of whether the generated SMILES are valid or not. In addition, for a given prompt molecule, we adopt TOPPK \citep{liu2024erp} for generation strategy.

After generation, for each prompt molecule/SMILES, we select the top 10 generated molecules/SMILES based on their average normalized reward. The mean of these top 10 molecules/SMILES is then used to obtain the final result for the prompt molecule/SMILES.

 }


{\subsection{Baseline REINVENT 4} \label{app:reinvent}
Following are detailed description of six different kinds of property change $Z$ included in REINVENT 4 \citet{he2022transformer, he2021molecular}
\begin{itemize}[itemsep=0pt,parsep=0pt,topsep=0pt,partopsep=0pt]
    \item \textbf{MMP:} There are user-defined desirable property changes between molecules $X$ and $Y$.
    \item \textbf{Similarity $\geq 0.5$:} The Tanimoto similarity between molecules $X$ and $Y$ is greater than 0.5.
    \item \textbf{Similarity $\in[0.5, 0.7)$:} The Tanimoto similarity between the pair $\left(X,Y\right)$ ranges from 0.5 to 0.7.
    \item \textbf{Similarity $\geq 0.7$:} The Tanimoto similarity between molecules $X$ and $Y$ is greater than 0.7.
    \item \textbf{Scaffold:} Molecules $X$ and $Y$ share the same scaffold.
    \item \textbf{Scaffold generic:} Molecules $X$ and $Y$ share the same generic scaffold.
\end{itemize}}

\subsection{{BPE Tokenization}}\label{app:vocabulary}


Byte Pair Encoding (BPE) is a tokenization algorithm initially designed for data compression and later adapted for use in NLP, particularly in the preprocessing of text for deep learning models. The core idea behind BPE is to iteratively merge the most frequent pair of consecutive bytes (or characters in the context of text) into a single, new byte (or token), thereby reducing the size of the data to be processed. This method has been particularly influential in the development of language models and machine translation systems. 
The BPE method follows these main steps: 
\begin{enumerate}
    \item \textbf{Initial vocabulary preparation: }The text is divided into a sequence of characters or symbols, and a special end-of-word symbol (like <\textbackslash w> or another unique marker) is added to each word to distinguish between the same character sequence occurring within a word and at the end of a word.
    \item \textbf{Frequency Count: } The algorithm counts the frequency of each pair of adjacent characters (or symbols) in the text.
    \item \textbf{Iterative Merging: }
    \begin{itemize}
        \item Identify the most frequent pair of adjacent characters.
        \item Merge this pair into a new single symbol (this does not mean changing the text itself but rather how the algorithm interprets the text).
        \item Update the frequency count of all pairs, considering the newly created symbol.
        \item Repeat this process for a predetermined number of iterations or until a desired vocabulary size is reached.
    \end{itemize}
    \item \textbf{Tokenization: } Once the merging process is complete, the original text can be tokenized (i.e., divided into a sequence of tokens) using the final set of symbols, including the merged ones. This results in a text representation where frequent words or subwords are encoded as single tokens, and less common words are broken down into smaller tokens.
\end{enumerate}

A significant benefit of BPE lies in its capacity to manage rare and out-of-vocabulary words effectively. Since BPE operates at the character level, it can segment words that were not encountered during training, thus reducing the negative effects of unfamiliar words on the model's performance. 
In contexts where tokens of various lengths are randomly masked and relocated to the end of the sequence, as proposed in section \ref{sec:cm_obj}, there's a high likelihood of generating a considerable number of unfamiliar tokens. BPE's approach is particularly beneficial here, as it ensures that the model can still process and understand these novel token sequences by breaking them down into familiar subunits, thereby maintaining robustness and reducing the potential degradation in performance due to unexpected or rare words.




















































































\subsection{Surrogate model}\label{app:surrogate_model}

{The surrogate model~\citep{vasan23} is a simplified version of a BERT-like transformer, widely employed in natural language processing. In this model, tokenized SMILES strings are inputted and then positionally embedded. The outputs are subsequently fed into a series of five transformer blocks, each comprising a multi-head attention layer (with 21 heads), a dropout layer, layer normalization with residual connection, and a feedforward network. The feedforward network consists of two dense layers followed by dropout and layer normalization with residual connection. Following the stack of transformer blocks, a final feedforward network is employed to produce the predicted docking score.} \fix{{ The validation $r^2$ values are 0.842 for 3CLPro dataset and 0.73 for the RTCB dataset.}}














\subsection{Performance scales with mask length}
We conducted an analytical experiment to examine how performance scales with mask length. The results show that the smaller the difference between the length of the generated span and the masked span, the higher the validity will be. In our settings, the validity reaches its highest at 90\% when the length of the generated span is between 5 and 10.

\begin{figure*}[ht!]

    \begin{subfigure}{1\textwidth}
        \centering
        \includegraphics[%
        width=10cm,  clip={0,0,0,0}]{figures/heatmap.jpg}
    \end{subfigure}\hfil
    \caption{The x-axis of this heatmap is the number of tokens that are generated, and the y-axis of this heatmap is the number of tokens that are masked in prompt.  
    }
\end{figure*}




\subsection{{Computing infrastructure {and wall-time comparison}}}\label{app:computing_infrastructure}
{We trained our docking surrogate models using 4 nodes of a supercomputer, each node equipped with CPUs (64 cores) and 4 A100 GPUs. The training time for each model was approximately 3 hours.} 
We performed additional pretraining on a cluster consisting of CPU nodes (approximately 280 cores) and GPU nodes (approximately 110 Nvidia GPUs, ranging from Titan X to A6000, primarily configured in 4- and 8-GPU setups). 

{Pretraining utilizes 8 A100 GPUs, while one single generation uses a single Tesla T4 GPU. Based on the computing infrastructure, {pretraining details as described in Appendix \ref{app:training_detail} and generation details as described in Appendix \ref{app:generation}}, we obtained the wall-time comparison in \tabref{table:wall-time} as follows.}


 \begin{table*}[ht!]
 {
    \centering
    {\scriptsize
    \scalebox{1}{
    \begin{tabular}{l c c  }
        \toprule
        \textbf{}
        & {\makecell[c]{Total Run Time}}
        \\
        \midrule
        \textbf{\makecell[l]{Initial Phase Pretraining}}
        &  \makecell[r]{~18h}
        \\
        \textbf{\makecell[l]{Second Phase Pretraining}}
        &  \makecell[r]{~48h}
        \\
        \textbf{\makecell[l]{Third Phase Pretraining}}
        &  \makecell[r]{~20h}
        \\
        \midrule
        \textbf{\makecell[l]{Generation 10k times \\ for one molecule}}
        &  \makecell[r]{~15mins}
        \\
        \bottomrule
    \end{tabular}}}
    \caption{{Wall-time comparison between different methods.} }
        \label{table:wall-time}   
        }
\end{table*}


\subsection{{Hyperparameters and architectures}}\label{app:hyperparameters}
Table \ref{app:tab:hyperparams} provides a list of hyperparameter settings we used for our experiments.

{For experimentation, 1280 molecules from each of the RTCB and 3CLPro datasets, with docking scores ranging from -14 to -6, are selected. This range is based on \citep{liu2024erp}.}


{In addition, when calculating the average normalized reward for the original molecule, where similarity is not considered, we select the weights for docking, drug-likeness, synthesizability, and solubility as $[0.25] \times 4$.}


\begin{table*}[h!]
    {
    \centering
    {\scriptsize
    \scalebox{1}{
    \begin{tabular}{c c }
        \toprule
        \textbf{Parameter} &  \textbf{Value} 
        \\
        \midrule
        {\makecell[l]{Pretraining}}
        \\
        \midrule
        {\makecell[l]{\quad Learning rate}} &  \makecell[c]{$5 \times e^{-5}$}
        \\
        \midrule
        {\makecell[l]{\quad Batch size}} &  \makecell[c]{$24$}
        \\
        \midrule
        {\makecell[l]{\quad Optimizer}} &  \makecell[c]{Adam}
        \\
        \midrule
        {\makecell[l]{\quad \# of Epochs for Training Initial Phase}} &  \makecell[c]{$10$} \\
        \midrule
        {\makecell[l]{\quad \# of Epochs for Training Second Phase}} &  \makecell[c]{$50$} \\
        \midrule
        {\makecell[l]{\quad \# of Epochs for Training Third Phase}} &  \makecell[c]{$20$}
        \\
        \midrule
        {\makecell[l]{\quad Model \# of Params}} &  \makecell[c]{$124M$} 
        \\
        \midrule
        {\makecell[l]{Generation}}
        \\
        \midrule
        {\makecell[l]{\quad \# of Molecules Optimized}} &  \makecell[c]{$1280$}
        \\
        \midrule
        {\makecell[l]{\quad TopK}} &  \makecell[c]{$[10,15,20]$}
        \\
        \midrule
        {\makecell[l]{\quad TopP}} &  \makecell[c]{$[0.85, 0.9, 0.95]$}
        \\


        \bottomrule
    \end{tabular}}}
        \caption{{{Hyperparameters}}. }
        \label{app:tab:hyperparams}
        }
\end{table*}





\subsection{Results on pretrained REINVENT4}

\begin{table*}[ht!]
\setlength{\tabcolsep}{4pt}
   \centering
    {\small
    \scalebox{0.6}{
    \begin{tabular}{l l c c c c c c c c c c }
        \toprule
        \textbf{Target} %
        & \textbf{Algorithm}
        & {\makecell[c]{Avg \\ Norm Reward~$\uparrow$}}
        & {\makecell[c]{Avg Top 10 \% \\ Norm Reward~$\uparrow$}}
        & {\makecell[c]{Docking ~$\downarrow$}}
        & {\makecell[c]{Druglikeliness ~$\uparrow$}}
        & {\makecell[c]{Synthesizability ~$\downarrow$}}
        & {\makecell[c]{Solubility ~$\uparrow$}}
        & {\makecell[c]{Similarity~$\uparrow$}}
        
        \\
        \midrule
        \makecell[l]{\textbf{3CLPro}} %
        &  \textbf{\makecell[l]{Original}}
        &  \makecell[l]{0.532}
        &  \makecell[l]{{0.689}}
        &  \makecell[l]{-8.698}
        &  \makecell[l]{0.682}
        &  \makecell[l]{3.920}
        &  \makecell[l]{2.471}
        &  \makecell[l]{-}
        \\
        (PDBID:
        &  \textbf{\makecell[l]{MMP \citep{he2022transformer}}}
        &  \makecell[l]{0.629 $\pm$ 0.001}
        &  \makecell[l]{0.717 $\pm$ 0.001}
        &  \makecell[l]{-8.241 $\pm$ 0.015}
        &  \makecell[l]{0.687 $\pm$ 0.003}
        &  \makecell[l]{2.683 $\pm$ 0.005}
        &  \makecell[l]{3.144 $\pm$ 0.028}
        &  \makecell[l]{0.870 $\pm$ 0.003 }
        \\
       \ 7BQY)
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.5) \citep{he2022transformer}}}
        &  \makecell[l]{0.617 $\pm$ 0.001}
        &  \makecell[l]{0.706 $\pm$ 0.001}
        &  \makecell[l]{-8.222 $\pm$ 0.022}
        &  \makecell[l]{0.690 $\pm$ 0.003}
        &  \makecell[l]{2.664 $\pm$ 0.005}
        &  \makecell[l]{3.162 $\pm$ 0.014}
        &  \makecell[l]{0.803 $\pm$ 0.002 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ([0.5, 0.7)]) \citep{he2022transformer}}}
        &  \makecell[l]{0.611 $\pm$ 0.001}
        &  \makecell[l]{0.699 $\pm$ 0.001}
        &  \makecell[l]{-8.195 $\pm$ 0.027}
        &  \makecell[l]{0.688 $\pm$ 0.002}
        &  \makecell[l]{\underline{2.660} $\pm$ 0.009}
        &  \makecell[l]{3.196 $\pm$ 0.022}
        &  \makecell[l]{0.775 $\pm$ 0.003 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.7) \citep{he2022transformer}}}
        &  \makecell[l]{0.630 $\pm$ 0.001}
        &  \makecell[l]{0.717 $\pm$ 0.001}
        &  \makecell[l]{-8.218 $\pm$ 0.007}
        &  \makecell[l]{0.694 $\pm$ 0.001}
        &  \makecell[l]{2.719 $\pm$ 0.006}
        &  \makecell[l]{3.058 $\pm$ 0.021}
        &  \makecell[l]{{0.890} $\pm$ 0.003 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold \citep{he2022transformer}}}
        &  \makecell[l]{0.607 $\pm$ 0.001}
        &  \makecell[l]{0.704 $\pm$ 0.002}
        &  \makecell[l]{-8.113 $\pm$ 0.015}
        &  \makecell[l]{0.700 $\pm$ 0.002}
        &  \makecell[l]{2.702 $\pm$ 0.006}
        &  \makecell[l]{2.961 $\pm$ 0.014}
        &  \makecell[l]{0.789 $\pm$ 0.002 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold Generic \citep{he2022transformer}}}
        &  \makecell[l]{0.617 $\pm$ 0.001}
        &  \makecell[l]{0.710 $\pm$ 0.002 }
        &  \makecell[l]{-8.185 $\pm$ 0.017}
        &  \makecell[l]{0.698 $\pm$ 0.002}
        &  \makecell[l]{{2.663} $\pm$ 0.007}
        &  \makecell[l]{3.07 $\pm$ 0.020}
        &  \makecell[l]{0.808 $\pm$ 0.002 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{\algname (masks only)}}
        &  \makecell[l]{\underline{0.668} $\pm$ 0.001} 
        &  \makecell[l]{\textbf{0.743} $\pm$ 0.001} 
        &  \makecell[l]{\underline{-9.083} $\pm$ 0.003}
        &  \makecell[l]{\textbf{0.718} $\pm$ 0.001}
        &  \makecell[l]{2.750 $\pm$ 0.001}
        &  \makecell[l]{\underline{3.630} $\pm$ 0.005}
        &  \makecell[l]{0.889 $\pm$ 0.001}
        
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{\algname (mask + s2s)}}
        &  \makecell[l]{\textbf{0.671} $\pm$ 0.001} 
        &  \makecell[l]{\textbf{0.743} $\pm$ 0.001} 
        &  \makecell[l]{\textbf{-9.150} $\pm$ 0.001}
        &  \makecell[l]{\underline{0.714} $\pm$ 0.001}
        &  \makecell[l]{2.763 $\pm$ 0.002}
        &  \makecell[l]{\textbf{3.672} $\pm$ 0.003}
        &  \makecell[l]{{0.895} $\pm$ 0.001}
        \\
        \bottomrule
        \textbf{RTCB}
        &  \textbf{\makecell[l]{Original}}
        &  \makecell[l]{0.536}
        &  \makecell[l]{{0.698}}
        &  \makecell[l]{-8.572}
        &  \makecell[l]{0.709}
        &  \makecell[l]{3.005}
        &  \makecell[l]{2.299}
        &  \makecell[l]{-}
        \\
        (PDBID:
        &  \textbf{\makecell[l]{MMP \citep{he2022transformer}}}
        &  \makecell[l]{0.636 $\pm$ 0.001}
        &  \makecell[l]{0.731 $\pm$ 0.002}
        &  \makecell[l]{-8.422 $\pm$ 0.022}
        &  \makecell[l]{0.712 $\pm$ 0.03}
        &  \makecell[l]{2.601 $\pm$ 0.003}
        &  \makecell[l]{2.987 $\pm$ 0.025}
        &  \makecell[l]{0.851 $\pm$ 0.002 }
        \\
        \ 4DWQ)
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.5) \citep{he2022transformer}}}
        &  \makecell[l]{0.626 $\pm$ 0.001}
        &  \makecell[l]{0.723 $\pm$ 0.001}
        &  \makecell[l]{-8.452 $\pm$ 0.037}
        &  \makecell[l]{0.712 $\pm$ 0.003}
        &  \makecell[l]{2.579 $\pm$ 0.006}
        &  \makecell[l]{3.013 $\pm$ 0.018}
        &  \makecell[l]{0.785 $\pm$ 0.003}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ([0.5, 0.7)]) \citep{he2022transformer}}}
        &  \makecell[l]{0.622 $\pm$ 0.002}
        &  \makecell[l]{0.718 $\pm$ 0.001}
        &  \makecell[l]{-8.428 $\pm$ 0.016}
        &  \makecell[l]{0.709 $\pm$ 0.002}
        &  \makecell[l]{\underline{2.558} $\pm$ 0.006}
        &  \makecell[l]{3.079 $\pm$ 0.029}
        &  \makecell[l]{0.757 $\pm$ 0.003 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Similarity ($\geq$ 0.7) \citep{he2022transformer}}}
        &  \makecell[l]{0.640 $\pm$ 0.001}
        &  \makecell[l]{0.733 $\pm$ 0.002}
        &  \makecell[l]{-8.445 $\pm$ 0.023}
        &  \makecell[l]{0.718 $\pm$ 0.002}
        &  \makecell[l]{2.629 $\pm$ 0.004}
        &  \makecell[l]{2.880 $\pm$ 0.012}
        &  \makecell[l]{0.880 $\pm$ 0.003}
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold \citep{he2022transformer}}}
        &  \makecell[l]{0.615 $\pm$ 0.003}
        &  \makecell[l]{0.720 $\pm$ 0.002}
        &  \makecell[l]{-8.512 $\pm$ 0.038}
        &  \makecell[l]{0.719 $\pm$ 0.001}
        &  \makecell[l]{2.587 $\pm$ 0.005}
        &  \makecell[l]{2.764 $\pm$ 0.014}
        &  \makecell[l]{0.748 $\pm$ 0.002 }
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{Scaffold Generic \citep{he2022transformer}}}
        &  \makecell[l]{0.624 $\pm$ 0.001}
        &  \makecell[l]{0.723 $\pm$ 0.001}
        &  \makecell[l]{-8.497 $\pm$ 0.023}
        &  \makecell[l]{0.722 $\pm$ 0.002}
        &  \makecell[l]{{2.562} $\pm$ 0.006}
        &  \makecell[l]{2.877 $\pm$ 0.019}
        &  \makecell[l]{0.771 $\pm$ 0.002 }
        \\        
        \textbf{ }
        &  \textbf{\makecell[l]{\algname (masks only)}}
        &  \makecell[l]{\underline{0.675} $\pm$ 0.001} 
        &  \makecell[l]{\underline{0.753} $\pm$ 0.001} 
        &  \makecell[l]{\underline{-9.318} $\pm$ 0.002}
        &  \makecell[l]{\textbf{0.752} $\pm$ 0.001}
        &  \makecell[l]{2.674 $\pm$ 0.001}
        &  \makecell[l]{\underline{3.292} $\pm$ 0.002}
        &  \makecell[l]{{0.883} $\pm$ 0.001}
        
        \\
        \textbf{ }
        &  \textbf{\makecell[l]{\algname (mask + s2s)}}
        &  \makecell[l]{\textbf{0.678} $\pm$ 0.001} 
        &  \makecell[l]{\textbf{0.755} $\pm$ 0.001} 
        &  \makecell[l]{\textbf{-9.377} $\pm$ 0.003}
        &  \makecell[l]{\underline{0.751} $\pm$ 0.001}
        &  \makecell[l]{2.688 $\pm$ 0.001}
        &  \makecell[l]{\textbf{3.328} $\pm$ 0.005}
        &  \makecell[l]{{0.890} $\pm$ 0.001}
        \\
        \bottomrule
        \\
    
    \end{tabular}}}
        \caption{
        {\textbf{Main results.} A comparison of seven baselines including Original, six baselines from REINVENT 4 (\emph{without REINFORCE finetuning}) \{MMP, Similarity ($\geq 0.5$), Similarity $\in [0.5,0.7)$, Similarity $\geq 0.7$, Scaffold, Scaffold Generic\} and \algname on multiple objectives 
        based on 3CLPro and RTCB datasets. {The top two results are highlighted as \textbf{1st} and \underline{2nd}.  Results are reported for 5 experimental runs. \algname outperforms the competing baselines in most of the metrics.}
        }
        }
\end{table*}