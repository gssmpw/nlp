

\section{\algname}

In this section, we propose \algname, a ground-up designed GPT model for molecule optimization. We first introduce the novel Causally Masked Seq2seq (CMS) Objective as the foundation of \algname. Then, we discuss the design of GPT, including designing the training corpus, a pre-training strategy, and a generation process.


\subsection{Causally Masked Seq2seq (CMS) Objective.}\label{sec:cm_obj}

Masked, causal, and seq2seq language modeling each offer unique benefits and limitations. Masked models encode bi-directional contexts but only decode about 15\% of the tokens during training. Causal models, being decoder-only, process every token but are restricted to left-to-right contexts. Seq2seq models are versatile yet often lack bidirectional context and precise generation control. To combine the strengths of MLM, CLM, and seq2seq models and draw inspiration from biological molecule evolution using SMILES representation—which allows for molecular expansion, shrinking, and mutation—we introduce the Causally Masked Seq2seq (CMS) Objective.
The CMS objective enables per-token generation, incorporating optional bidirectional and seq2seq functionality for greater adaptability. It allows for precise control over specific positions and spans within sequences, supporting the expansion, contraction, or mutation of segments while maintaining the integrity of designated areas. The construction of the CMS objective involves the following steps:







\paragraph{Designing the corpus.} Our methodology for developing the CMS objective to a SMILES~\citep{weininger1988smiles} string of length $\len$ begins with the most basic corpus suitable for the CLM objective as
$\corpus=\curlybracket{[BOS],{x_1,\cdots,x_{\horizon}},[EOS]}$.
\paragraph{Blending the MLM objective.} We then
build the MLM objective on top of CLM. It involves a probability $\prob$ to determine the total number of tokens to mask as $\lfloor{\len\cdot \prob}\rfloor$. Let us denote $N\in \mathbb{R}^{+}$ as the number of span of mask in the source document. 
\begin{equation}
{[BOS],x_1,\cdots,
\underbrace{x_{idx_1},\cdots, x_{idx_1+\lfloor \len \cdot \prob \rfloor}}_{<mask\_n:\len\cdot\prob>},\cdots,
x_{\horizon},[EOS]},
\end{equation}
For $N=1$, let us choose a random starting index $idx_1 \sim \bracket{0, \len - \lfloor{\len\cdot \prob}\rfloor - 1}$, and proceed to mask tokens in range $\bracket{idx_1,idx_1 + \lfloor{\len\cdot \prob}\rfloor}$. 
For $N=2$, we divide $\lfloor{\len\cdot \prob}\rfloor$ into two segments, $m1$ and  $m2$, ensuring $m1 + m2 = \lfloor{\len\cdot \prob}\rfloor$ and that each segment's length is uniformly selected from the range $[1,\lfloor{\len\cdot \prob}\rfloor]$.
We then identify a starting point $idx_1$ within $\bracket{0, \len  - \lfloor{\len\cdot \prob}\rfloor - 1}$ for the first mask span and a second starting point $idx_2$ from the range $\bracket{idx_1 + m1 + 1, \len - m2 - 1}$ for the second mask span, ensuring that the two masked segments are non-overlapping and sequentially ordered in the SMILES string. Following the same strategy for selection and masking of these segments, we could reach for any $N$. For the $n_{th}$ span of mask, we replace the span by the token <$mask\_n: \len\cdot\prob$>, where $n$ and $\len\cdot\prob$ represents for the $n_{th}$ masked segment with size hint length $\len\cdot\prob$, which specify the desired length of text to generate for replacing the mask conditioning on tokens length.
Finally, we  reposition the masked spans to the end of the SMILES string, maintaining their sequence order as illustrated in \figref{fig:cm_mol_example_with_sizeHint}. 
In this work, we embed the size hint within the mask token as <$mask\_i:n$> to avoid the ambiguity seen in prior works~\citeauthor{aghajanyan2021size_hint}  that use <$mask\_i$>$n$. This format prevents misinterpretation by models, as numerical values in chemical structures can indicate ring closures or chain lengths.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/causalmaskonly.pdf}
    \caption[A visual representation of causal masked objective on molecule incorporated with size hints]
    {The visual representation of our causal masked objective on a molecule features two mask spans $(n=2)$, each with a specific size hint. The first span, <$mask\_1:2$>, covers two tokens, and the second, <$mask\_2:7$>, covers seven tokens.}
    \label{fig:cm_mol_example_with_sizeHint}
\end{figure}





\paragraph{Blending the seq2seq objective.} 
Finally, we establish CMS objective by applying seq2seq objective on top of MLM and CLM. Initially, we train a GPT model using the MLM and CLM objectives, denoted as $\policy_{\text{CM}}$.
Given a SMILES string, we randomly mask a seq2seq span starting at position $s_1$ and of length $\len$, ensuring it does not overlap with previously masked spans, while regard the remaining tokens as $\mathbf{Z}$. Our goal is to transform this s2s span $\left[x_{s_1}, \cdots, x_{s_1+\len}\right]$ into a target span with desired length $\len^t$. To create this training corpus, we utilize $\policy_{\text{CM}}$ to generate $\len^t$ tokens $\left[m_{1}, \cdots, m_{\len^t}\right]$ with regarded to $\mathbf{Z}$. We then construct the training corpus by mapping the s2s span to the subsequence generated by $\policy_{\text{MLM}}$.
\begin{align}
&x_1,\cdots,x_{s_1-1}, \tuple{\text{mask}\_1:\len^t}, 
\underbrace{x_{s_1+ \len +1 },\cdots, {\color{black}{x_{\horizon}, \tuple{\text{mask}\_1:\len^t}}}}_{\text{Prompt based on the pretrained model in previous step}}
\rightarrow[m_1,\cdots,m_{\len^t}] \nonumber
\end{align}
\begin{align}
&x_1,\cdots,{\tuple{\text{s2s}\_i\_\len^t:x_{s_1},\cdots,x_{s_1+\len}}},
\underbrace{\cdots, {\color{black}x_{\horizon}, {\tuple{\text{s2s}\_i\_\len^t:x_{s_1},\cdots,x_{s_1+\len}}},[m_1,\cdots,m_{\len^t}]}}_{\text{Training corpus for seq2seq objective}} \nonumber
\end{align}
where ${\tuple{\text{s2s}\_i\_\len^t:x_{s_1},\cdots,x_{s_1+\len}}}$ denotes the seq2seq objective conditioned on a specific subsequence $x_{s_1},\cdots,x_{s_1+\len}$ and its bidirectional unmasked tokens. 
The index $i$ indicates the $i$-th span, and $\len^t$ represents the target length of the generated subsequence. 
Unlike conventional sequence-to-sequence models, our work on seq2seq is also conditioned on and benefits from the bidirectional context surrounding the seq2seq span.
This approach allows for the incorporation of task-specific length priors into prompts, resulting in outputs that are more precise and controlled.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/s2s_cm.pdf}
    \caption
    {The visual representation of building the training corpus with both masked and seq2seq spans for seq2seq causal masked objective. }
    \label{fig:s2s_cm}
\end{figure}



























\subsection{The Design of the Controllable GPT.}


\paragraph{Pretraining.}
In this work, 
we propose a novel three-phase training approach to train a GPT model under CMS objective.

\textit{In the initial phase.} Our objective is to train a LLM specifically designed for understanding molecules. This training employs a CLM approach. CLM is an autoregressive technique where the model learns to predict the next token in a sequence based solely on the preceding tokens. This creates a unidirectional context model, which means it only considers past information and ignores any future context when making predictions. For this phase, the model is trained on a dataset comprised of texts about ligands. This dataset enables the model to accurately learn the representation of compounds, including their chemical structures and properties.

\textit{In the second phase.} 
Building on the success of the LLM developed in Phase 1, which demonstrated high accuracy in generating molecular structures, we proceed to refine the model’s training. This phase employs a causally masked objective with multiple mask tokens, each with a size hint, as illustrated in Figure \ref{fig:cm_mol_example_with_sizeHint}.
In this phase, the model, denoted as $\policy_{CM}$, benefits from both Causal Language Modeling (CLM) and Masked Language Modeling (MLM), which enhance CLM's performance by utilizing bidirectional context. $\policy_{CM}$ is capable of generating molecules in a controlled manner, specifying both the target length and the position for expansion.






\textit{In the third phase.} 
Ultimately, we achieve building the GPT under CMS objective by further refining the causally masked model,$\policy_{CM}$, through the integration of a sequence-to-sequence objective. We trained our model, denoted as $\policy_{CMS}$, using the training corpus outlined in \figref{fig:s2s_cm} to refine the causally masked model $\policy_{CM}$ developed in Phase 2.
This advancement aims to enhance the model's controllable generation in terms of both contraction and mutation. It mimics the mutation behavior in biological sequences. Thus, our $\policy_{SCM}$ achieves controllable generation in expansion, contraction, and mutation at specific positions or ranges, in either a random or specified length.



\textit{Loss function.} 
Instead of altering the standard cross-entropy loss \XL{detail}to consider the loss from predicting masked tokens negligible, we treat masked tokens like regular tokens, subject to the usual loss calculations. This method is used because our training data may contain multiple masked tokens, each with size hint information indicating the number of tokens to generate in place of the mask. Thus, it's crucial to accurately predict both the presence of these masked tokens and their corresponding size hints.


\paragraph{Generation Process.} The prompt, output string, and generated SMILES for \algname can be viewed in figure \ref{fig:drugimprovercm_gen_ex1} and figure \ref{fig:drugimprovercm_gen_ex2}. More specifically,
in the process of generating new molecular structures, \algname employs a method that either modifies existing molecules or adds new elements to them without altering the original essential structure, showcasing the flexibility and precision of the model in generating novel molecular designs. This is illustrated through two examples:

\textit{Modifying the Original Molecule:} Initially, two segments of the original molecule's SMILES string are identified and replaced with mask and seq2seq token respectively, which are placeholders indicating where and how long the new segments should be. 
These mask tokens are then processed by the model, which generates new segments in their place. The generated segments, highlighted in green, are manually repositioned to replace the original masked segments, effectively changing the molecule's structure and construct a new molecule. 
This process is depicted in \figref{fig:drugimprovercm_gen_ex1}, where the mask and seq2seq token are shown in red and the newly generated segments in green. 
The caption for \figref{fig:drugimprovercm_gen_ex1} explains this process in detail, emphasizing the manual reintegration of generated tokens.

    \begin{figure}[!ht]
        \centering
    \includegraphics[width=1\linewidth]{figures/modify_original_1.pdf}
        \caption{Modification of an original molecule. This figure illustrates the process of altering a molecule's structure. Key steps include replacing original segments with masked and sequence-to-sequence tokens (highlighted in \textcolor{myred}{red}), generating new molecular segments (in \textcolor{ForestGreen}{green}) by the model, and manually reintegrating these segments into the molecule. }
        \label{fig:drugimprovercm_gen_ex1}
    \end{figure}


\textit{Adding to the Original Molecule Without Modification:} In this scenario, instead of replacing parts of the SMILES string, one mask token and one seq2seq token are inserted at random positions within the string. These tokens serve as prompts for the model to generate new molecular segments that are then manually inserted into the specified positions, expanding the original molecule without altering its existing structure. This approach is visualized in \figref{fig:drugimprovercm_gen_ex2}, with the mask tokens again represented in green and the generated segments in red. The caption for \figref{fig:drugimprovercm_gen_ex2} provides a clear explanation of this additive process.

    \begin{figure}[!ht]
        \centering
        \includegraphics[width=1\linewidth]{figures/modify_original.pdf}
        \caption{Expansion of an original molecule: Mask tokens (in \textcolor{myred}{red}) are inserted into the SMILES string, prompting the generation of new segments (in \textcolor{ForestGreen}{green}). These segments are then manually added to the molecule, showcasing the model's capability to expand molecular structures both creatively and precisely.}
        \label{fig:drugimprovercm_gen_ex2}
    \end{figure}















