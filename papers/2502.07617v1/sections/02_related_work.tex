\section{Related Work}\label{sect:related_work}

\paragraph{Data Scaling.}

The study of scaling laws in large language models (LLMs) has become a critical area of research in NLP. \citet{hestness2017deep} and \citet{kaplan2020scaling} were among the first to systematically explore the relationship among model size, dataset size, and compute, demonstrating predictable power-law scaling of performance. \citet{henighan2020scaling} further emphasized the crucial role of data, showing that substantial performance gains can be achieved by increasing the size and quality of the training dataset, even with fixed model size. DeepMind's Chinchilla~\citep{hoffmann2022training} provided compelling evidence for this data-centric approach, demonstrating that smaller models trained on much larger datasets can achieve comparable or superior performance to larger models, given the same computational budget. This work has shifted the focus of LLM development towards optimizing the scale of  data.

In computer vision, early works, such as ImageNet~\citep{deng2009imagenet}, demonstrated the profound impact of dataset size and diversity on model generalization. Subsequent efforts like JFT-300M~\citep{sun2017revisiting} emphasized the importance of large-scale and high-quality datasets for training state-of-the-art vision models. \citet{zhai2106scaling} further explored scaling behavior in Vision Transformers~\citep{dosovitskiy2020image} using the  JFT-3B dataset, showing that scaling both data and model size simultaneously leads to improved generalization.

The pivotal role of data scaling is equally applicable to vision-language modeling, as highlighted in \citet{cherti2023reproducible}. This has led to a substantial increase in the development of image-text datasets over the last ten years. Early datasets, such as COCO Captions~\citep{chen2015microsoft} and Flickr30k~\citep{young2014image}, were created to enable tasks like image captioning and visual question answering with high-quality annotations. However, their limited size, due to the cost of human annotation, hindered further scaling of the datasets. To address this, Conceptual Captions~\citep{sharma2018conceptual} started to filter image-text pairs from the web based on heuristic rules, leading to millions of image-caption pairs. Going forward along this road, larger image-text datasets have been created from web sources, using increasingly complex filtering techniques~\citep{datacomp,fang2023data,dong2025scalable}. These datasets, ranging from hundreds of millions to several billion image-text pairs, have enabled the training of powerful vision-language models like CLIP~\citep{radford2021learning} and ALIGN~\citep{jia2021scaling}, which have demonstrated impressive performance on a wide range of vision-language tasks. Notably, LAION-5B~\citep{schuhmann2022laion} and WebLI~\citep{chen2022pali} stand out as the largest publicly and privately available image-text datasets, with 5 billion and 10 billion multilingual image-text pairs respectively. 

However, the rapidly growing web contains vastly more data. The impact of scaling to much larger datasets, such as 100 billion samples, remains largely unknown.

\paragraph{Vision-Language Pre-training.}

The field of large vision-language models is advancing quickly, building upon remarkable progress in both computer vision and natural language processing. A prevalent and highly effective strategy is to learn visual representations and language modeling independently, followed by joint pre-training of the vision-language model using high-quality multimodal data.

Since the advent of CLIP~\citep{radford2021learning}, contrastive learning on large, noisy web datasets has become the dominant approach for acquiring powerful visual representations~\citep{chen2020simple}. This weakly supervised paradigm surpasses traditional supervised learning methods~\citep{kolesnikov2020big,steiner2021train}, primarily due to the large scale and high diversity of web data~\citep{jia2021scaling,yuan2021florence,pham2023combined,yu2022coca}. An alternative approach gaining traction involves learning visual features from web data using generative methods~\citep{tschannen2024image,wan2024locca}, which predict paired text for given images. While vision models trained in this manner exhibit superior transferability to generative language models, the high computational cost limits its widespread adoption.

Despite the acquired zero-shot capabilities, which can be directly applied to tasks such as zero-shot classification~\citep{deng2009imagenet} and image-text retrieval~\citep{chen2015microsoft,young2014image}, the strong visual representations learned by contrastively trained models often lead to their utilization as image encoders. This is often leveraged in vision-language tasks by integrating visual tokens with language tokens, enabling LLMs to process multimodal information~\citep{alayrac2022flamingo,chen2022pali,chen2023pali,li2023blip,beyer2024paligemma,liu2024visual}. Following this approach, PaLI-3~\cite{chen2023pali} has demonstrated that vision models trained on large-scale web data outperform those trained on weakly annotated images of a similar scale, which further underscores the importance of the data diversity inherently present in the web corpus.

\paragraph{Inclusive Models.}
Recent studies have highlighted that popular techniques employed to enhance the performance of vision-language models, such as English-language-based filtering, may inadvertently diminish cultural understanding~\cite{goyal2022vision,nguyen2024multilingualdiversityimprovesvisionlanguage,richards2023does,pouget2024no,ananthram2024see}. Hence, we also evaluate cultural diversity in this work, as outlined in~\citet{pouget2024no}, which falls into two categories.

The first category, geo-localization, involves predicting the country or region of origin for an image using few-shot classification.  The second category utilizes zero-shot classification on datasets curated from various geographical regions.  Prominent examples within this category include Dollar Street~\cite{rojas2022dollar}, GeoDE~\cite{ramaswamy2024geode}, and Google Landmarks Dataset v2 (GLDv2)~\cite{weyand2020google}. Dollar Street comprises 38K images depicting household items from 63 countries. GeoDE features 62K manually annotated images collected from diverse geographic locations.  Finally, GLDv2 contains 1,542 images representing 884 landmarks across 84 countries, enabling the assessment of model performance on recognizing culturally important locations. In our evaluations, we employ all three aforementioned datasets. For the zero-shot evaluation on Dollar Street, we adhere to the methodology used in~\citet{rojas2022dollar}, mapping 96 specific topics within the dataset to corresponding ImageNet classes. This mapping results in a curated subset of 21K images, which we utilize for our analysis. These geographically diverse benchmarks, employed collectively, provide a comprehensive framework for evaluating the impact of performance optimization techniques on cultural understanding within vision-language models.

\input{tables/attention_maps_mini}
% \FloatBarrier