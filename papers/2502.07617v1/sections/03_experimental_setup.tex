%\input{tables/main_results_mini}
\input{tables/west}
\input{tables/culture}

\section{Experimental Setup}

\subsection{Pre-training Datasets}
We describe the dataset splits we use in the pre-training.

\paragraph{Raw Datasets.}To assess the performance of vision-language models on large-scale image-text data, we construct a dataset with 100 billion image-text pairs from the web, inspired by the work of \citet{chen2022pali,schuhmann2022laion,zhai2022lit,jia2021scaling}. We refer to this as WebLI-100B, and refer to its subsets with 1 billion and 10 billion examples as 1B and 10B, respectively.  The 1B and 10B datasets are created by randomly sampling 1\% and 10\%, respectively, from the 100 billion dataset. In this work, we apply only essential data filters, such as removing harmful images and personally identifiable information (PII). This approach ensures the dataset remains as multilingual and diverse as possible. We utilize both the alt-text and page title associated with each image as the paired text. To ensure fair evaluations, we remove near-duplicate images across more than 90 common vision-language tasks from our dataset.
% We provide the data card in Appendix~\ref{}.

\paragraph{Quality-filtered Datasets.}To examine the impact of scaling on quality-filtered data, we adopt the common approach of using the CLIP-L/14 model~\citep{radford2021learning} as a filter, retaining a high-quality dataset with 5 billion pairs of images and English alt-text. To further solidify our results, we train a VLM on the web data to classify image-text pairs as aligned or misaligned, and tune its threshold to retrain another filtered dataset of the same size. Unless otherwise noted, we use the language of web pages\footnote{The ``content-language" meta tag in the head of an HTML document.} for multilingual experiments, thereby avoiding potential inaccuracies from language detection on the noisy web text.

\paragraph{Language-rebalanced Datasets.}In the language rebalancing experiments in Section~\ref{sec:lang_rebalance}, we adjust the mixing ratio of the low-resource languages used in the Crossmodal-3600~\citep{thapliyal2022crossmodal} benchmark. These low-resource languages are Bengali (bn), Filipino (fil), Hindi (hi), Hebrew (iw), Maori (mi), Swahili (sw), and Telugu (te)\footnote{Cusco Quechua (quz) is excluded from our experiments because it is not supported by our language detection method.}, ranging from 0.001\% to 0.267\% in our dataset (Appendix~\ref{appendix:lang_distribution}). In model training, we upsample each of them to 1\%, with remaining 93\% comprising of the original data.


\subsection{Contrastive Vision-Language Pretraining}
To study the impact of data scale on model performance, we train SigLIP~\citep{zhai2023sigmoidlosslanguageimage} models using three different dataset sizes: 1 billion, 10 billion and 100 billion. 
We also vary the model size using ViT-B/16, ViT-L/16, and ViT-H/14 architectures for both image and text encoders. During contrastive training, inspired by \citet{zhai2106scaling}, we utilize a large batch size of 32K and an inverse square root learning rate schedule with 200 million warmup and cooldown examples. The learning rate and weight decay are set to 0.001 and 0.0001 respectively. In the preprocessing stage, images are resized to a resolution of 224x224 pixels, and texts are tokenized using the multilingual mt5~\citep{xue2020mt5} tokenizer with a maximum sequence length of 64 tokens.
% More hyperparameters are listed in Appendix~\section{appendix:hyperparameters}

All models are trained on a maximum of 100 billion examples; e.g. a maximum of 100 epochs when using 1B examples. We cool down the models at various training steps where they have seen 3, 7, 10, 17, 26, 33, 49, 66, and 100 billion examples, and evaluate them after the cool-downs. Unless otherwise specified, we report results using the checkpoints where models have been trained on 100 billion examples. All models are compared on a compute-matched regime.
%This means models trained on the 1 billion, 10 billion, and 100 billion scale datasets are trained for 100 epochs, 10 epochs, and 1 epoch, respectively, while utilizing the same computational resources.

\subsection{Evaluations}\label{sect:evals}
The model's capabilities are evaluated across a diverse range of benchmarks, spanning from traditional Western-centric tasks to those measuring inclusivity.

\paragraph{Western-centric.} Our first set of evaluations uses diverse, well-established benchmarks. For zero-shot classification, we employ ImageNet~\citep{deng2009imagenet}, CIFAR-100~\citep{krizhevsky2009learning}, and Oxford-IIIT Pet~\citep{pets} datasets. Additionally, for 10-shot evaluations, we use Caltech-UCSD Birds~\citep{wah2011caltech}, Caltech 101~\citep{li_andreeto_ranzato_perona_2022}, Cars196
~\citep{KrauseStarkDengFei-Fei_3DRR2013}, Colorectal Histology~\citep{kather2016multi}, and Describable Textures Dataset (DTD)~\citep{cimpoi14describing} benchmarks to assess the representation capabilities of vision models. We also conduct zero-shot retrieval evaluations on COCO Captions~\citep{chen2015microsoft} and Flickr30k~\citep{young2014image}, in both image-to-text and text-to-image directions.% While such evaluations are diverse, they are mostly Western-centric.

\paragraph{Cultural Diversity.}
Besides the above metrics, we also incorporate a range of benchmarks aimed at evaluating cultural diversity, following the recommendations in~\cite{pouget2024no}. Specifically, we include zero-shot classification using Dollar Street~\cite{rojas2022dollar}, GeoDE~\cite{ramaswamy2024geode}, and Google Landmarks Dataset v2 (GLDv2)~\cite{weyand2020google}. See Section~\ref{sect:related_work} for a brief description about each dataset. We also include 10-shot geolocalization using Dollar Street and GeoDE. 

\paragraph{Multilinguality.} We evaluate the model's multilinguality using the Crossmodal-3600 dataset~\citep{thapliyal2022crossmodal}, a geographically diverse set of 3600 images with human-generated captions in 36 languages. We assess the model's zero-shot retrieval in both image-to-text and text-to-image directions for each language. In addition to per-language results, we also present average scores for low-resource languages (Bengali, Filipino, Hindi, Hebrew, Maori, Swahili, and Telugu) and high-resource languages (others).

\paragraph{Fairness.}
In addition, we also evaluate the presence of societal biases in the trained model. We report on representation bias (RB) and association bias (AB) between gender and occupation, as defined in~\citet{alabdulmohsin2024clip}. These measure unwanted associations w.r.t. the gender attribute using 1st and 2nd order statistics. Also, we report performance disparity by income in Dollar Street zero-shot accuracy and by region in GeoDE zero-shot accuracy.% Finally, following~\cite{radford2021learning}, we also report the accuracy of the model in recognizing sensitive attributes: perceived gender in FairFace~\cite{karkkainen2021fairface} and perceived race in UTK~\cite{utkface_url}.

\paragraph{Transfer to Generative Models.}
Finally, to assess how well our contrastively trained vision models transfer to generative vision-language tasks, we utilize the compact and versatile PaliGemma model~\citep{beyer2024paligemma}. We initialize PaliGemma's vision component with our contrastively trained models and pretrain it on 50 million seen examples, following its stage-1 recipe at 224x224 resolution. During the pre-training, we explore two common transfer settings: freezing~\citep{liu2024visual,zhu2023minigpt,chen2022pali} and unfreezing~\citep{xiao2024florence,chen2023pali,beyer2024paligemma,steiner2024paligemma2familyversatile} the vision model. We then use PaliGemma's default configuration to finetune on a variety of downstream tasks, covering image captioning, visual question answering, and segmentation, which require the understanding of  semantics, OCR, multilinguality, and remote sensing.