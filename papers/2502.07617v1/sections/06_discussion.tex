\section{Discussion}

\paragraph{Data Filtering.} Data filtering is a common technique used to improve data quality in vision-language pre-training. As demonstrated in Section~\ref{sec:data_filter}, CLIP filter remarkably improves model's performance on the traditional tasks. %Therefore, scaling the data before filtering can readily yield state-of-the-art results on these tasks, as evidenced by the success of JFT-3B~\citep{zhai2106scaling}.
Given the noted impact of filtering on cultural diversity in our experiments, we focus on the impact of scaling raw, unfiltered data, and leave the improvement of data quality at the 100 billion scale for future work.
%The effective data filtering could be explained by the fact that long-tail data, often ranked lower by filters (perhaps due to inherent characteristics), is typically excluded from Western-centric benchmarks. That being said, current vision-language benchmarks are actively encouraging VLMs to learn culturally biased knowledge. This paper addresses this issue, suggesting that using more diverse, larger-scale data or simple data rebalancing can mitigate it. 
We encourage the community to conduct further research into new data filtering techniques that preserve cultural diversity, as well as novel training architectures or methods that improve model inclusivity without requiring additional training data.

\paragraph{Limitations.} The benchmarks used in this paper to evaluate VLM inclusivity are necessarily limited, since inclusivity is a broad societal concept that should be reduced to a handful of metrics. For instance, while we utilize Crossmodal-3600 in a zero-shot setting to assess multilinguality, it only covers 36 languages. %and the results are strongly influenced by the text encoder, which is often discarded when transferring pre-trained vision models to generative VLMs.
%Similar to the challenges faced by \citet{pouget2024no}, we also lack culture-diversity-specific benchmarks during the contrastive VLM pre-training. This issue is also prevalent for generative VLMs, as shown in Section~\ref{sec:transfer}.

% todo: marvl, ocr

% how to affect transfer