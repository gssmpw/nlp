\section{Analysis}

\subsection{Data Quality Filtering}
\label{sec:data_filter}

Raw web data is often too noisy for training effective vision-language models. To address this, a common strategy is to use a data filter model to remove less relevant image-text pairs. In this work, we utilize the CLIP-L/14 model to filter the raw data and retrain 5 billion high-quality English image-text pairs.
For comparison, we also train a classifier model on the raw web data, resulting in a filtered dataset of the same size. Additionally, we sample an English subset of the same size from the raw data to serve as a baseline.
We train ViT-L models on the three datasets and represent the results in Figure~\ref{fig:quality_filter} and Appendix~\ref{appendix:quality_filter}. The CLIP filter excels in Western-centric tasks, consistent with data-centric research showing that effective data filtering enhances model performance~\citep{fang2023data,cao2023less,maini2023t,abbas2023semdedup}. However, all filtered datasets underperform in other tasks, particularly those involving cultural diversity. This illustrates a key drawback of data filtering, that it can inadvertently introduce biases into the filtered dataset, in agreement with prior works~\citep{birhane2021multimodal,pouget2024no,garcia2023uncurated}.

\input{tables/quality_filter}
% \FloatBarrier

\subsection{Language Rebalancing}
\label{sec:lang_rebalance}

The low-resource languages in our raw data collectively represent only 0.5\%, which prevents sufficient model learning of the concepts existing in these languages or areas. To address this, we upsample each low-resource language to a fixed 1\% representation. This rebalancing, visualized in Figure~\ref{fig:lang_rebalance}, improves model performance on the low-resource language benchmark. Accordingly, the performance on the high-resource language slightly decreases, but still remains comparable (also applies to other English-only zero-shot retrieval tasks), which results in an overall improvement on the entire multilingual benchmark. Additionally, we observe a mild improvement in cultural diversity tasks, while other tasks show slightly worse results, potentially due to the reduction in Western-centric examples, as most evaluations are based on the English language. Full evaluation results can be found in Appendix~\ref{appendix:lang_rebalance}.

\subsection{Qualitative Examples}

\label{sec:qualitative_examples}

% To assess how increased training data impacts cultural understanding, we visualize attention maps from ViT models trained on WebLI-1B, 10B, and 100B, focusing on underrepresented, low-resource cultures within standard vision-language datasets. We use culturally relevant Google Image Search queries, refined by Gemini, and included baselines from commonly represented Western cultures.

% Table~\ref{tab:attention_maps_mini} compares ViT-B/16 attention maps across WebLI-1B, 10B, and 100B. While models trained at different scales exhibit broadly similar attention patterns, the 100B model demonstrates significantly more focused attention on semantically relevant regions. For example, in images depicting the "Pohela Boishakh" festival, WebLI-100B captures fine-grained visual elements like traditional decorations, festive clothing worn by participants, and even individual faces within the crowd. Similarly, for the "Igloo" image, the 100B model attends to the structural details of the igloos themselves, differentiating between individual blocks of ice and their arrangement, compared to the more diffuse attention exhibited by WebLI-1B. Furthermore, improvements are not limited to low-resource concepts; WebLI-100B shows subtly enhanced attention to the bison in the Yellowstone image, focusing more precisely on the animals' forms and distinguishing them from the surrounding landscape, compared to the other model. These qualitative results, demonstrating improved focus and granularity with increasing dataset size, bolster our quantitative findings and further highlight the importance of scaling vision-language models with large, diverse datasets for building inclusive visual understanding. More examples are provided in Table~\ref{tab:attention_maps}.

% To assess how increased training data impacts cultural understanding, we visualize attention maps from ViT models trained on WebLI-1B, 10B, and 100B, focusing on underrepresented, low-resource cultures within standard vision-language datasets. %We use culturally relevant Google Image Search queries, refined by Gemini, and included baselines from commonly represented Western cultures. 

We visualize the attention maps from the vision models trained on different scales of data in Table~\ref{tab:attention_maps_mini}. Models trained on larger data tends to have more focused attention on semantically relevant regions.
For example, in the ``Igorot Dance'' image, the 100B-trained model captures finer details, such as intricate patterns on traditional decorations and culturally significant objects.
In the ``Igloo'' image, the 100B-trained model accurately focuses on the igloo' structural details (its dome shape), unlike other models which are distracted by background elements like mountains and ice.
Beyond low-resource concepts, 100B data can also improve performance on common concepts. As shown in the ``Bison" image, models trained on larger datasets more precisely capture the bison, rather than the surrounding landscape.
More visualized examples can be found in Table~\ref{tab:attention_maps}.

% Table~\ref{tab:attention_maps_mini} compares ViT-B/16 attention maps across queries in layer 5. %\footnote{average across all queries @layer #5} 
%across these dataset scales.
% While models trained at different data scales exhibit broadly similar attention patterns, a clear trend emerges: the 100B model demonstrates  more focused attention on semantically relevant regions, exhibiting increased granularity and precision.

