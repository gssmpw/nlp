\section{Conclusion}

% \paragraph{Data filtering.}

% how to affect transfer
% how to avoid introduce bias in filtered data, rebalancing is not perfect

% \paragraph{Limitations.}

% multilinguality: only retrieval, needs others like ocr...

% metric: lack culture diversity/fairness benchmarks, avg not perfect,

% transfer: lack of inclusive benchmarks



% \paragraph{Future Work.}

In this paper, we investigate the impact of scaling image-text data up to 100 billion unique examples, on vision-language pre-training.
We demonstrate that a scale of 100 billion image-text pairs is beneficial for vision-language models in areas beyond traditional Western-centric benchmarks, such as cultural diversity, multilinguality, and reducing performance disparity across subgroups. Hence, this data scale remains fundamentally important for the development of truly inclusive multimodal systems.
We also investigate the impact of applying quality filters, such as those based on CLIP, to large-scale image-text datasets. These filters, though often beneficial for traditional tasks, can negatively impact data diversity by reducing the representation of certain cultural contexts.
Overall, our results highlight the importance of data scale for VLMs. While traditional benchmarks may not benefit significantly from the scaling of noisy, raw web data to 100 billion, this data scale remains crucial for training inclusive vision-language models.

