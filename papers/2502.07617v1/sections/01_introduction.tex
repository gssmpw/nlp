\section{Introduction}
The progress in vision-language models (VLMs) has been intrinsically linked to the availability of large-scale datasets. Larger datasets fuel the development of more powerful models, which are capable of understanding and generating complex relationships between images and text. In turn, such models have pushed boundaries in tasks like zero-shot image classification, image captioning and visual question answering. 

This relationship between data scale and model performance often follows a power law $f(x)=\alpha\, x^{-c}+\varepsilon$, where $f(x)$ is a model performance metric such as its error rate and $x$ is the data size~\citep{mukherjee2003estimating,hestness2017deep,johnson-etal-2018-predicting,rosenfeld2019constructive,kaplan2020scaling,ghorbani2021scaling,alabdulmohsin2022revisiting,bansal2022data,zhai2106scaling}. These ``scaling laws,'' as they came to be known in the literature, have been used, among others, to determine the training data size needed to achieve a specified level of accuracy~\citep{cho2015much,beleites2013sample,figueroa2012predicting} and to optimize the model size~\citep{kaplan2020scaling,hoffmann2022training,alabdulmohsin2024getting}. They have also been justified theoretically using space-partitioning arguments~\citep{bahri2021explaining,hutter2021learning,sharma2022scaling}. Importantly, a power law implies that increasing the amount of training data can yield diminishing, but \emph{still worthwhile}, returns in terms of accuracy and capability. 

Driven by these potential benefits, the field has witnessed a concerted effort towards scaling up the size of vision-language datasets. Early works focused on web curated datasets like Conceptual Captions~\citep{sharma2018conceptual}, which provided millions of image-caption pairs for pre-training~\cite{sharma2018conceptual}. Subsequent work leveraged large-scale web crawling to create even larger datasets. In particular, the Common Crawl project~\cite{commoncrawl2021}---a repository of publicly available web data---became a foundational resource for constructing many of these web-scale datasets. From this foundation emerged datasets like LAION-400M/2B/5B~\cite{schuhmann2022laion}, DataComp~\cite{datacomp}, WebLI~\cite{chen2022pali} and Multimodal C4~\cite{zhu2024multimodal}, pushing the boundaries of dataset size to billions  of image-text pairs, thereby accelerating progress in VLMs. This is similar to how ImageNet~\cite{deng2009imagenet},  JFT-300M~\cite{sun2017revisiting}--a dataset of 300 million images with noisy labels--and its larger variant JFT-3B~\cite{zhai2106scaling} accelerated progress in supervised image pre-training previously.

Despite these advancements, the largest reported datasets to date have plateaued at around 10 billion image-text pairs. This raises the question: \emph{what further benefits are unlocked by pushing the data scale by one order of magnitude to 100 billion unique examples?}

\input{figures/scaling/scaling_curves}


To answer this question, we introduce WebLI-100B, a novel dataset containing 100 billion image-text pairs, representing a \emph{tenfold} increase over the largest reported vision-langauge datasets. To recall, the original WebLI dataset contains 10 billion examples and has been instrumental in training state-of-the-art models like PaliGemma~\cite{beyer2024paligemma,steiner2024paligemma2familyversatile} and SigLIP~\cite{zhai2023sigmoidlosslanguageimage}, and influenced the development of other research directions, such as mitigating social biases~\cite{alabdulmohsin2024clip}, improving cultural diversity~\cite{pouget2024no}, and scaling open-vocabulary object detection~\cite{minderer2024scaling}.
% We scale that dataset tenfold to 100B examples. 

In this work, our primary goal is to provide an empirical investigation to the impact of this data scale on a range of downstream tasks and, importantly, to explore aspects beyond traditional performance metrics. For instance, while our experiments demonstrate that 100 billion scale can lead to tiny improvements on established benchmarks, we reveal its significant impact on less-explored areas, particularly those related to cultural diversity and multilinguality.

For example, when applied to geo-localization tasks based on Dollar Street~\cite{rojas2022dollar}---a metric for evaluating cultural diversity---ViT-L/16 trained on a single epoch of 100 billion data achieves an accuracy of 41.7\%. By contrast, the same model trained on ten epochs of 10 billion data achieves an accuracy of 35.9\% only, despite both models using the same amount of training compute. We attribute these gains, in part, to the dataset's ability to capture a wider range of long-tail cultural concepts that require a substantial data size to become salient. Furthermore, data scaling also enhances the multilinguality of trained models, leading to an improvement in low-resource languages. Figure~\ref{fig:main} summarizes the  improvements in cultural diversity and multilinguality achieved through data scaling.

\paragraph{Statement of Contribution.}Our goal in this paper is to answer the following question: should one invest in scaling up the size of the pretraining dataset to 100 billion examples? We make the following contributions:
\begin{itemize}
    \item We provide an empirical investigation of the potential of pre-training VLMs on a scale of 100 billion unique examples. To the best of our knowledge, studying the impact of this data scale for VLMs has never been conducted before in the literature.  
    \item We demonstrate that a  scale of 100 billion image-text pairs is beneficial for VLMs in areas beyond traditional benchmarks, such as cultural diversity, multilinguality, and reducing performance disparity across subgroups. Hence, this data scale is vital for building truly \emph{inclusive} multimodal systems.
    \item We investigate the impact of applying quality filters that \emph{reduce} the size of the dataset, such as those based on CLIP. While such filters are often employed to improve overall data quality, we find that they can inadvertently reduce the representation of certain cultural contexts, thereby limiting the diversity of the dataset, even when the original dataset contains 100 billion examples.
    % , even when the dataset is scaled to 100 billion examples.  % Xiao: only 5b after filtering, so don't need to emphasize the size here.
    % \item We investigate the impact of language rebalancing. Our findings show that increasing the representation of low-resource languages in training data significantly improves the model's performance on these languages, while having minimal impact on high-resource languages.
\end{itemize}
