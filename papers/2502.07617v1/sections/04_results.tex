\section{Results}\label{sect:results}

\subsection{Established Benchmarks}\label{sect:results_west}
We begin by evaluating all vision-language models on established benchmarks, based on ImageNet and COCO Captions, among other datasets. As revealed in Table~\ref{tab:west_standard_setup}, increasing the dataset size from 10 billion to 100 billion examples does not improve performance substantially. This is statistically supported by Wilcoxon's signed rank test~\cite{wilcoxon1992individual}, which gives a $p$-value of 0.9, indicating that differences are not significant.


In addition, we also fit data scaling laws for every combination of model and dataset following the recipe proposed in~\citet{alabdulmohsin2022revisiting}. This allows us to evaluate whether or not the performance gap is expected to increase or decrease in the infinite-compute regime. We report the resulting scaling exponents and asymptotic performance limits in the tables. Again, we do not observe  significant differences at the 95\% confidence level ($p$-value of 0.09).


\subsection{Cultural Diversity}
Unlike the Western-oriented metrics reported in Section~\ref{sect:results_west}, cultural diversity metrics present an entirely different picture. We observe \emph{notable} gains when scaling the size of the dataset from 10 billion to 100 billion examples in Table~\ref{tab:culture_standard_setup}. 
For example, scaling training data from 10 billion to 100 billion examples yields substantial gains on Dollar Street 10-shot classification task, where ViT-L and ViT-H see absolute improvements of 5.8\% and 5.4\%, respectively. These gains outperform the typical improvements (less than 1\%) observed on Western-oriented 10-shot metrics by a large margin.
Using Wilcoxon's signed rank test, we obtain a $p$-value of 0.002, indicating a statistically significant evidence at the 99\% confidence level.


\subsection{Multilinguality}

Our multilingual benchmark, Crossmodal-3600 zero-shot retrieval~\cite{thapliyal2022crossmodal}, shows a disparity in performance gains: low-resource languages benefit more from the 100 billion scale than the high-resource ones. The disparity, illustrated in Figure~\ref{fig:multilinguality}, which not only exists in all model sizes but also widens as the models become larger. Detailed results for each language can be found in Appendix~\ref{appendix:data_scale}.

% source: https://colab.corp.google.com/drive/1AKgGDITZqTC2hQjVc-Iv8xuysh5giP0i#scrollTo=2EtEXMbly8dB&line=1&uniqifier=1
\begin{figure}[h!]
    % \includegraphics[width=\linewidth]{figures/multilang-Average_Multilingual__Low-Resource_Lang.pdf}
    % \includegraphics[width=0.86\linewidth]{figures/multilang-Average_Multilingual__High-Resource_Lang.pdf}
    \includegraphics[width=\linewidth]{figures/multilang-Average_XM3600_Retrieval.pdf}
    \caption{Scaling up to 100B examples leads to more notable improvements in low-resource languages. $\Delta$ denotes the improved accuracy when scaling from 10B examples to 100B.}
    \label{fig:multilinguality}
\end{figure}


\subsection{Fairness}
For fairness, we report on 3 metrics discussed in Section~\ref{sect:evals}. 

\paragraph{Representation Bias.} The first metric is representation bias (RB), with results detailed in Table~\ref{tab:rb}. We observe that models trained on unbalanced web data have a significantly higher preference to associate a randomly chosen image from ImageNet~\cite{deng2009imagenet} with the label ``Male'' over the label ``Female.'' 

In fact, this occurs nearly 85\% of the time. Training on 100B examples does not mitigate this effect. This finding aligns with previous research highlighting the necessity of bias mitigation strategies, such as data balancing~\cite{alabdulmohsin2024clip}, to address inherent biases in web-scale datasets.

\input{tables/rb}



\paragraph{Association Bias.} Second, Figure~\ref{fig:ab} shows the association bias in SigLIP-H/14 between gender and occupation as we scale the data from 10 to 100 billion examples. Specifically, we plot the probability that the model would prefer a particular occupation label, such as ``{\fontfamily{lmodern}\selectfont secretary}'' over another label, such as ``{\fontfamily{lmodern}\selectfont manager}'' when images correspond to males or females. In this evaluation, we use the Fairface~\cite{karkkainen2021fairface} dataset. The labels we compare are: ``{\fontfamily{lmodern}\selectfont librarian}'' vs. ``{\fontfamily{lmodern}\selectfont scientist}'', ``{\fontfamily{lmodern}\selectfont nurse}'' vs. ``{\fontfamily{lmodern}\selectfont doctor}'', ``{\fontfamily{lmodern}\selectfont housekeeper}'' vs. ``{\fontfamily{lmodern}\selectfont homeowner}'', ``{\fontfamily{lmodern}\selectfont receptionist}'' vs. ``{\fontfamily{lmodern}\selectfont executive}'' and ``{\fontfamily{lmodern}\selectfont secretary}'' vs. ``{\fontfamily{lmodern}\selectfont manager}''. Again, we do not see a reduction in association bias by simply increasing the size of the training data. %Full results are in Appendix~\ref{appendix:ab}.

%Additionally, we are unable to evaluate cultural diversity and fairness in PaliGemma's transfer tasks due to the lack of appropriate benchmarks. This is an open question that we hope to address in the future.

\paragraph{Performance Disparity.} Finally, one common definition of fairness in machine learning is maintaining similar performance across different groups. See, for instance,~\citet{dehghani2023scaling} and the related notions of ``Equality of Opportunity'' and ``Equalized Odds''~\cite{hardt2016equalityopportunitysupervisedlearning}. Table~\ref{tab:perf_disparity} show that scaling the data to 100 billion examples improves performance disparity, which is consistent with the improvement in cultural diversity.

%  to show on top of page
% \input{tables/perf_disparity_mini}
% \FloatBarrier

\input{tables/perf_disparity}


\subsection{Transfer To Generative Models}
\label{sec:transfer}

\input{tables/transfer_avg}

We use PaliGemma~\citep{beyer2024paligemma} with both frozen and unfrozen vision component to assess the transferability of our vision models, which were contrastively pre-trained on datasets of different scales. In Table~\ref{tab:transfer_avg}, when taking the noise level into consideration, we do not observe consistent performance gains across downstream tasks as we scale the pre-training dataset. More details can be found in Appendix~\ref{appendix:transfer}.


%\paragraph{Recognizing Sensitive Attributes.}
%Finally, we also report the performance of the models in recognizing sensitive attributes, following a similar evaluation in~\citet{radford2021learning}. We report the accuracy in predicting perceived gender in Fairface~\cite{karkkainen2021fairface} and predicting perceived race in UTK~\cite{utkface_url}. Overall, we observe that scaling the data to 100 billion examples improves this aspect of fairness as well. Table~\ref{tab:fairness_pred} provides the full results. We do not observe a particular pattern in this type of evaluation.
%\input{tables/fairness_pred}
