\section{Related Work}
\textbf{Distributed optimization}: The set of problems in decentralized decision-making where each agent holds a different  but related piece of information, and makes compatible decisions traces back to**Tsitsiklis et al., "Problems in Decentralized Decision-Making"**. These problems are characterized by local communication and local gradient minimization of a shared (convex) cost function**Bertsekas & Tsitsiklis, "Parallel and Distributed Computation: Numerical Methods"**.The network averaging consensus algorithm for the fixed strongly connected networks**Carpenter et al., "Network Averaging on Fixed Topology Networks: Convergence Rate and Stability"**, and for switching topology networks**Nedic et al., "Achieving Geometric Convergence in Distributed Subgradient Algorithms via Random Projection"**, are essential components for one-hop communication between agents; They enable their agreement on a solution vector to the common decision problem. Large scale convex (supervised) machine learning problems can be within this decentralized  optimization framework, \eg,**Duchi et al., "Distributed Coordinate Descent Method for Learning with Large Data Volumes"**, where agents agree on a solution vector while maintaining the input data private. Gradient tracking**Iain & Johansson, "A Distributed Quadratic Flows Optimization Algorithm," performs consensus both on the solution vector and its local gradients, minimizing an objective that is the average of all local objectives. However, those agents are not reinforcement learning based agents. In reinforcement learning the state changes according to a transition function.

\textbf{Distributed optimization and deep learning}: A recent development is the investigation of distributed optimization in large scale supervised deep learning, \eg,**Kolter et al., "Accelerated Distributed Alternating Direction Method of Multipliers"** develop a image detection algorithm using consensus where each agent has a \textit{i.i.d} sample of a very large dataset. The focus on non-convex problems includes rigorous convergence analysis and the application of momentum based optimizers for better sample efficiency. In the centralized setting, momentum based optimizers, such as the Adam optimizer**Kingma & Ba, "Adam: A Method for Stochastic Optimization"**, boost the performance of deep learning models by incorporating the weight update history. This improvement carries over to the distributed setting, where consensus on model weights is integrated with momentum-based weight updates. For instance, consensus adaptive optimizer presented in**Lian et al., "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Distributed Learning"** surpasses the performance of  centralized optimizers. More recently,**Zhang et al., "Distributed Gradient Tracking under Quantized Communication Constraints" propose gradient tracking Adam optimizer, GTAdam. Their system combines gradient tracking and adaptive momentum, and outperforms  other distributed optimizers. In supervised deep learning, the datasets are static whereas in reinforcement learning the datasets are dynamic. They change in accordance to shifts in agents' policies.

\textbf{Distributed MARL}: A research thread in MARL applies consensus-based mechanisms to the distributed {\em policy evaluation} problem. The problem of estimating the total discounted joint reward under a fixed joint policy under the fully observable setting. Agents perform localized approximations for the state value function, which can either be tabular or approximated using linear function approximation. Under those assumptions, there are methods that guarantee the convergence of the policy evaluation. Examples include:  DTDT**Yuan et al., "DTDT: Decentralized Temporal Difference Learning"**, Decentralized TD(0) with gradient tracking**Zhang et al., "Distributed Gradient Tracking under Quantized Communication Constraints"**,  and**Ito & Zhang, "Decentralized Actor-Critic Methods". Moving to the partially observable setting, consensus-based mechanisms also improve the performance of belief based agents.**Cui et al., "Decentralized Policy Evaluation for Multi-Agent Reinforcement Learning with Linear Function Approximation" propose policy evaluation, with linear function approximation, under the partially observable setting whereby agents use the consensus mechanism to share both policies' parameters and beliefs with closest neighbors.**Yang & Zhang, "Decentralized Actor-Critic Methods for Partially Observable Multi-Agent Systems" proposes learning agents that perform consensus on a tabular belief, and learn polices using interior point methods. In this work, we propose to use non-convex function approximation under partial observability.

\textbf{Coordination graphs} (CGs): is  a solution to coordination problems in games where agents interact with a few neighbors to decide on a local joint action; Agents are nodes and joint actions are edges representing a local coordination dependency. Agents learn the {\em payoff} function for every joint action without exploring the large combinatorial action space. The global payoff functions is the sum of local payoff functions. For solving the CG, **Hegde et al., "Scalable Distributed Optimization for Coordination Graphs"** propose a variable elimination algorithm. However, this approach  can scale exponentially in the number of agents for densely connected graphs**Koppel & Ozdaglar, "Distributed Algorithms for Determining Optimal Joint Actions in Coordination Games"**.**Koppel et al., "A Max-Plus Algorithm for Solving Coordination Graphs with Distributed Communication Constraints" propose the distributed {\em max-plus} algorithm, that out performs  variable elimination for densely connected graphs. However, the underlying CG must be fixed during training, it induces a spanning tree that models the communication channel, and it requires a variable number of message exchanges so that all agents agree on a the best (local) payoff. Moreover, convergence of this message passing scheme is only guaranteed for acyclic CGs. Both variable elimination and max-plus algorithm have been developed to work on the tabular payoff function case. Deep coordination graphs~\cite[DCGs,][]{bohmer_2020} generalize max-plus algorithm to train end-to-end CGs, using parameter sharing between payoff functions and privileged information (global state). DCGs are expressive  enough to represent a rich set of $Q$-functions that factorize, such as VDN and QMIX~\cite[QMIX,][]{rashid_2018}. Particularly, all the methods assume centralized training and a fixed coordination graph topology which is common knowledge throughout training.

\textbf{Networked agents with multi-agent reinforcement learning}:**Khalak et al., "Decentralized Multi-Agent Reinforcement Learning"  develops the first reinforcement learning-based agents with local communication, gradient minimization with asymptotic convergence guarantees using linear function approximation. This method, networked agents, promotes {\em decentralized training} and {\em decentralized execution} paradigm where agents learn locally and are suitable for real world infrastructure and robot teams domains**Zhang et al., "Multi-Agent Reinforcement Learning with Decentralized Training"**. However, there are three limitations to their work: the assumption of full state and action space observability, the slower sample efficiency due to stochastic online learning**Khalak & Zhang, "Decentralized Multi-Agent Reinforcement Learning with Stochastic Online Learning"**, and the risk of convergence to a sub optimal Nash-equilibrium **Hart et al., "Multi-Agent Value Function Approximation"**.**Zhang et al., "Decentralized Multi-Agent Reinforcement Learning with Jointly Observable State" extend networked agents for a mild form of partial observability--jointly observable state, where the state is fully observable taking into account all the of agents' perceptions**Khalak & Zhang, "Decentralized Multi-Agent Reinforcement Learning with Stochastic Online Learning". Their system adapts a well known CTDE algorithm, MADDPG**Lillicrap et al., "Autonomous Experience Replay"**, to the DTDE approach, allowing agents to use consensus iterations on weights to emulate parameter sharing. Our approach extends previous work, by  developing networked agents under partial observability and applying gradient tracking  as means of performing localized weight updates of a shared objective.