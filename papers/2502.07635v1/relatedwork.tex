\section{Related Work}
\textbf{Distributed optimization}: The set of problems in decentralized decision-making where each agent holds a different  but related piece of information, and makes compatible decisions traces back to~\citet{tsitsiklis_1985}. These problems are characterized by local communication and local gradient minimization of a shared (convex) cost function~\citep{nedic_2020}.The network averaging consensus algorithm for the fixed strongly connected networks~\citep{xiao_2003}, and for switching topology networks~\citep{xiao_2007}, are essential components for one-hop communication between agents; They enable their agreement on a solution vector to the common decision problem. Large scale convex (supervised) machine learning problems can be within this decentralized  optimization framework, \eg,~\citep{forero_2010}~\citep{chang_2020}, where agents agree on a solution vector while maintaining the input data private. Gradient tracking~\citet{qu_2018}, performs consensus both on the solution vector and its local gradients, minimizing an objective that is the average of all local objectives. However, those agents are not reinforcement learning based agents. In reinforcement learning the state changes according to a transition function.

\textbf{Distributed optimization and deep learning}: A recent development is the investigation of distributed optimization in large scale supervised deep learning, \eg, ~\citep{jiang_2017} develop a image detection algorithm using consensus where each agent has a \textit{i.i.d} sample of a very large dataset. The focus on non-convex problems includes rigorous convergence analysis and the application of momentum based optimizers for better sample efficiency. In the centralized setting, momentum based optimizers, such as the Adam optimizer~\citet{kingma_2014}, boost the performance of deep learning models by incorporating the weight update history. This improvement carries over to the distributed setting, where consensus on model weights is integrated with momentum-based weight updates. For instance, consensus adaptive optimizer presented in~\citet{nazari_2022} surpasses the performance of  centralized optimizers. More recently,~\citet{carnevale_2023} propose gradient tracking Adam optimizer, GTAdam. Their system combines gradient tracking and adaptive momentum, and outperforms  other distributed optimizers. In supervised deep learning, the datasets are static whereas in reinforcement learning the datasets are dynamic. They change in accordance to shifts in agents' policies.

\textbf{Distributed MARL}: A research thread in MARL applies consensus-based mechanisms to the distributed {\em policy evaluation} problem. The problem of estimating the total discounted joint reward under a fixed joint policy under the fully observable setting. Agents perform localized approximations for the state value function, which can either be tabular or approximated using linear function approximation. Under those assumptions, there are methods that guarantee the convergence of the policy evaluation. Examples include:  DTDT~\citep{wang_2020}, Decentralized TD(0) with gradient tracking~\citep{lin_2022},  and~\citep{sun_2020}. Moving to the partially observable setting, consensus-based mechanisms also improve the performance of belief based agents.~\citet{kayaalp_2023} propose policy evaluation, with linear function approximation, under the partially observable setting whereby agents use the consensus mechanism to share both policies' parameters and beliefs with closest neighbors.~\citet{petric_2023} proposes learning agents that perform consensus on a tabular belief, and learn polices using interior point methods. In this work, we propose to use non-convex function approximation under partially observability.

\textbf{Coordination graphs} (CGs): is  a solution to coordination problems in games where agents interact with a few neighbors to decide on a local joint action; Agents are nodes and joint actions are edges representing a local coordination dependency. Agents learn the {\em payoff} function for every joint action without exploring the large combinatorial action space. The global payoff functions is the sum of local payoff functions. For solving the CG, ~\citet{guestrin_2002} propose a variable elimination algorithm. However, this approach  can scale exponentially in the number of agents for densely connected graphs~\citep{kok_2006}.~\citet{kok_2006} propose the distributed {\em max-plus} algorithm, that out performs  variable elimination for densely connected graphs. However, the underlying CG must be fixed during training, it induces a spanning tree that models the communication channel, and it requires a variable number of message exchanges so that all agents agree on a the best (local) payoff. Moreover, convergence of this message passing scheme is only guaranteed for acyclic CGs. Both variable elimination and max-plus algorithm have been developed to work on the tabular payoff function case. Deep coordination graphs~\cite[DCGs,][]{bohmer_2020} generalize max-plus algorithm to train end-to-end CGs, using parameter sharing between payoff functions and privileged information (global state). DCGs are expressive  enough to represent a rich set of $Q$-functions that factorize, such as VDN and QMIX~\cite[QMIX,][]{rashid_2018}. Particularly, all the methods assume centralized training and a fixed coordination graph topology which is common knowledge throughout training.

\textbf{Networked agents with multi-agent reinforcement learning}:~\citet{zhang_2018}  develops the first reinforcement learning-based agents with local communication, gradient minimization with asymptotic convergence guarantees using linear function approximation. This method, networked agents, promotes {\em decentralized training} and {\em decentralized execution} paradigm where agents learn locally and are suitable for real world infrastructure and robot teams domains~\citep{gronauer_2022}. However, there are three limitations to their work: the assumption of full state and action space observability, the slower sample efficiency due to stochastic online learning~\citep{mnih_2016} and the risk of convergence to a sub optimal Nash-equilibrium \citep{zhang_2021}.~\citet{chen_2022} extend networked agents for a mild form of partial observability--jointly observable state, where the state is fully observable taking into account all the of agents' perceptions~\citep{oliehoek_2016}. Their system adapts a well known CTDE algorithm, MADDPG~\citep{lowe_2017}, to the DTDE approach, allowing agents to use consensus iterations on weights to emulate parameter sharing. Our approach extends previous work, by  developing networked agents under partial observability and applying gradient tracking  as means of performing localized weight updates of a shared objective.