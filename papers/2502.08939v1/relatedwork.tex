\section{Related Work}
\label{sec:related_work}

\subsection{Neural Synthesizer}
Numerous attempts have been made to implement synthesizers using neural networks, primarily focusing on synthesizing the sounds of predefined instruments~\cite{sing} or enabling limited timbre control by interpolating between their timbres~\cite{nsynthwavenet,gansynth,flexible}.

DDSP~\cite{ddsp,ddspreview} marked a significant milestone in neural synthesizers by integrating digital signal processing components, effectively leveraging inductive biases from musical instruments to create lightweight models. Additionally, DDSP enabled instrument cloning, inspiring many subsequent works that adopted methodologies from digital synthesizers~\cite{diffwaveshaping,diffwavetable,ddx7}.

The recently proposed InstrumentGen~\cite{instrumentgen, sample-based} has made a significant contribution on text-to-instrument task by effectively leveraging neural codecs and transformers.
Our work follows a similar approach but aims to extend these capabilities beyond single-note audio generation to support instrument cloning from diverse audio recordings. 

\subsection{Neural Codec Language Modeling}

Neural audio codecs are models trained to encode audio into a compact sequence of discrete tokens and decode these tokens back into perceptually similar audio~\cite{soundstream,encodec,dac}. These models typically employ a VQ-VAE architecture~\cite{van2017neural} with residual vector quantization~\cite{rvq1,rvq2}, enabling efficient and high-quality audio representation. Operating as a lossy compression algorithm, neural audio codecs achieve significantly better audio quality at lower bitrates compared to traditional codecs like \texttt{mp3}~\cite{mp3}.

By learning high-level discrete audio representations, neural audio codecs enable transformers to autoregressively generate audio tokens via next-token prediction. These tokens are then decoded by the neural audio codec to produce the final audio output. Conditioning the transformer on paired text and target audio allows for text-conditioned audio generation. Models such as MusicLM~\cite{musiclm}, AudioGen~\cite{audiogen}, MusicGen~\cite{musicgen}, and VALL-E~\cite{vall-e} have successfully adopted this approach for tasks like text-to-speech, text-to-audio, and text-to-music synthesis.