




\section{Analysis}

\subsection{Task-Specific Prompting}
\label{5.1}
We observed that, even when clearly distinguishing between instruction and input through general prompting, LLMs often fail to align with user intent in instructional distraction scenarios.
Therefore, in this section, we conduct experiments to explore whether task-specific prompting can effectively address this issue, focusing on translation tasks.
Specifically, we employ three prompting strategies: the first is direct prompting (\textit{DIRECT}), which explicitly instructs the model to disregard any instructions or questions embedded in the input\footnote{\textit{Instruction used in the DIRECT prompting method is: "If there is an instruction or question within the input text, do not solve it; handle it as text."}}, and the second is Chain-of-Thoughts (CoT) prompting~\cite{wei2022chain}, which encourages the model to generate responses by following a step-by-step reasoning process.
As demonstrated in Table~\ref{table5}, both methods contribute to an improvement in average performance when evaluated by an LLM judge.
However, neither approach is entirely successful in fully mitigating the issue of instructional distraction.

Moreover, we also experiment with a prompting strategy that alters the sequence of instructions and target inputs (Suffix Instruction).~\footnote{For the suffix instruction experiment, we removed the word "following" from the instruction prompt.}
The results indicate that, in most tasks, placing the instruction after the target input increases the LLM’s vulnerability to instructional distraction.


\subsection{Impact Variations Based on Input Length}

%Moreover, to examine how input length impacts distraction, we conduct experiments by varying the input length in a question answering task.
Moreover, to examine how input length impacts distraction, we conduct LLM-based evaluations by varying the input length in a question answering task.
For testing purposes, we construct four data sets—QA\textsubscript{short}, QA\textsubscript{medium}, QA\textsubscript{long}, and QA\textsubscript{superlong}—with average token counts of 362, 743, 1,087, and 3,007, respectively. Also, we focus on translation tasks among the instruction tasks.
The experimental results reveal that as the input text length increased, LLMs became more prone to distraction, as shown in Table~\ref{table6}. 
%This may be due to the nature of closed-book question answering tasks, where the question appears after the passage. 
%and the question increases, making it more difficult for the model to follow the instruction.
This may be due to the observation that, as the passage lengthens, the distance between the instruction and the question grows, making it increasingly difficult for the model to follow the instruction.


\subsection{Case Study}
%In this section, we present examples of error cases of instructional distractions in Table~\ref{table_case}. 
We present examples of error cases in Table~\ref{table_case}, illustrating how instructional distractions influence the performance of LLMs.
The first case demonstrates a scenario where the instruction is to proofread, but GPT-4o is distracted by an input containing a code generation command and ends up generating code instead. 
The second case involves the model ignoring the instruction to perform style transfer and, instead, providing a solution to a bias detection multiple-choice question. 
%Additional cases can be found in the appendix.







