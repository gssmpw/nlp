
\input{rsc/Figure3}

\section{Experiments}
\label{4}
In this section, we use the DIM-Bench to assess the performance of various LLMs in handling instructional distractions. Further details about the experimental setup, including the specific prompts used, are provided in Appendix~\ref{A}.
%Section~\S\ref{4.1} covers the experimental setup, and Section~\S\ref{4.2} evaluates the performance of multiple LLMs using the LLM Judge method. Additionally, Section~\S\ref{4.3} reports results from a length-based automatic evaluation to support the findings from the LLM Judge assessments. Further details about the experimental setup, including the specific prompts used, are provided in Appendix~\ref{app}.

%\input{rsc/Table_main}




\subsection{Experimental Setting}
\label{4.1}
\paragraph{Models}
%In this experiment, we evaluate the robustness of five LLMs against instructional distractions. 
%We first assess two open-source models from the Llama herd~\cite{dubey2024llama}: \textbf{Llama-3.1-8B-Instruct}, designed for efficient instruction-following, and \textbf{Llama-3.1-70B-Instruct}, a larger model optimized for complex prompts.
%We first assess two open-source Llama herd~\cite{dubey2024llama}: \textbf{Llama-3.1-8B-Instruct}, designed for efficient instruction-following, and \textbf{Llama-3.1-70B-Instruct}, a larger model optimized for complex prompts. 
%We also evaluate three closed-source models: \textbf{GPT-3.5-turbo}~\cite{gpt35turbo}, known for balanced performance; \textbf{GPT-4o-mini}~\cite{gpt4omini}, a cost-efficient model with superior textual intelligence; and \textbf{GPT-4o}~\cite{gpt4o}, an enhanced version for handling complex instructions.


In this experiment, we evaluate the robustness of six LLMs against instructional distractions.
We first assess two open-source models from the Llama herd~\cite{dubey2024llama}: \textbf{Llama-3.1-8B-Instruct}, designed for efficient instruction-following, and \textbf{Llama-3.1-70B-Instruct}, a larger model optimized for complex prompts.
Additionally, we evaluate \textbf{Qwen-2.5-7B}~\cite{qwen2.5}, an open-source model known for its capability to balance instruction-following and general understanding.
We also evaluate three closed-source models: \textbf{GPT-3.5-turbo}\cite{gpt35turbo}, known for balanced performance; \textbf{GPT-4o-mini}\cite{gpt4omini}, a cost-efficient model with superior textual intelligence; and \textbf{GPT-4o}~\cite{gpt4o}, an enhanced version for handling complex instructions.


\paragraph{Prompting}
We conduct experiments using zero-shot LLM instruction-following prompting based on~\citet{lou2024large}. 
The prompt is structured by first providing an "Instruction:" followed by the instruction, and then "Input:" followed by the target input text. 
Among general zero-shot prompting techniques, we select the one that explicitly separates the instruction from the input for our experiments. 
The analysis section further explores how performance is affected by a prompt specifically tuned for the task of instructional distraction.



\paragraph{Judge Model}

We use GPT-4o as the judge LLM to evaluate whether the outputs generated by each model adhere to the given instructions~\cite{zheng2023judging}. 
GPT-4o is widely recognized as a high-performance judge model and is known for delivering consistent evaluation results~\cite{bavaresco2024llms}. 
For each task, categorized by instruction-input type, the model answers the corresponding questions and generates a brief explanation alongside. 
The temperature is set to 0 to ensure deterministic outputs. 
Additional experimental details can be found in Appendix~\ref{A}.





\subsection{LLM Evaluation Results}
\label{4.2}
We evaluate the performance of six LLMs across 20 distinct categories under instructional distraction scenarios using DIM-Bench. 
Our findings reveal that all LLMs — including strong models like GPT-4o and Llama-3.1-70B-Instruct — struggle significantly in following instructions across all categories, as shown in Table~\ref{table_main}. 
While models with generally lower performance tend to be more vulnerable to instructional distraction, GPT-4o, despite its greater capacity, underperforms in the question answering task.
%, recording a lower average accuracy than GPT-4o-mini.


Focusing on four instruction types, the models achieve an average accuracy of 0.301 in Style Transfer, 0.397 in Rewriting, 0.526 in Translation, and 0.458 in Proofreading. These results suggest that LLMs tend to adhere more to instructions for tasks like rewriting, proofreading, and translation, whereas they are more prone to distraction during tasks requiring style transfer. 

Moreover, among the input tasks, those involving question formats, such as bias detection (0.208), reasoning (0.493), and question answering (0.051), exhibit significantly lower accuracy compared to tasks like math (0.738) and code generation (0.612).
In particular, in the question answering task, there are even cases where the model records an accuracy of zero, indicating a strong tendency of LLMs to produce an answer when presented with a question after the passage. 
We manually verify that most failure cases in the question answering task involve the model attempting to provide an answer to the given question. 
Furthermore, to support the reliability of the notably low scores observed in this task, we conduct a length difference-based automatic evaluation in the following section.

\input{rsc/Table5}
\input{rsc/Table6}


\input{rsc/Table_case}
\subsection{Automatic Evaluation Results}
\label{4.3}

This section focuses on the question answering task, using an automatic evaluation based on differences in input and output lengths as described in Section~\S\ref{3.3}. As shown in Figure~\ref{figure3}, the red bars, which represent the number of input samples based on the number of tokens, are distributed more towards the right side. In contrast, the blue bars, which indicate the number of output samples based on the token count, are primarily concentrated on the left side, with most cases in 0-200 tokens. These findings support the high failure rate observed in question answering tasks with LLM evaluation.


%Although similar token counts between input and output do not necessarily mean the instruction was followed, a reduction in output tokens by more than half compared to the input often indicates instruction non-compliance, even accounting for language-specific variations in translation tasks. 