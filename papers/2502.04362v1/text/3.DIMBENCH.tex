\section{DIM-Bench}
%We introduce a novel benchmark, named DIM-Bench (Distractive Instruction Misunderstanding Benchmark), to evaluate the performance of LLMs in the context of instructional distractions. 
We introduce a novel benchmark, named DIM-Bench, to evaluate the performance of LLMs in the context of instructional distractions. 
Section~\S\ref{3.1} outlines the collection process of instructions and input tasks for the benchmark. Section~\S\ref{3.2} discusses the benchmark's statistics, while Section~\S\ref{3.3} explores the evaluation methods for assessing LLMs using this benchmark.

\subsection{Data Collection}
\label{3.1}
In this section, we describe the process of data collection and filtering.
Each data instance consists of two components: \textit{Instructions} and \textit{Inputs}. \textit{Instructions} involve four key tasks—rewriting, proofreading, translation, and style transfer—while the \textit{Inputs} consist of five tasks: reasoning, code generation, mathematical reasoning, bias detection, and question answering. 
Data examples for various combinations can be found in Table~\ref{table_example}. 
%Below, we provide detailed descriptions of each tasks and the datasets from which the tasks are derived.

\subsubsection{Tasks for Instruction}
\paragraph{Rewriting}
The goal of the rewriting task is to rephrase a given text while maintaining its original meaning. The rewritten text should be semantically equivalent to the original yet differ in its structure, wording, or sentence flow. To guide this process, we develop ten template prompts, including instructions such as, "\textit{Restate the following input text in your own words.}"

\paragraph{Proofreading}
The proofreading task involves reviewing and correcting errors in grammar, spelling, and punctuation in a given text. 
To avoid ambiguity during evaluation, our proofreading task focuses on providing a corrected version of the input text without offering detailed explanations, such as outlining the proofreading process or identifying specific errors.
A set of ten instruction templates is designed, including "\textit{Generate a revised version of the input text with corrections for spelling and grammar.}."

\input{rsc/Table_stat}

\paragraph{Translation}
The translation task aims to convert the input text into one of the following languages: Chinese, Spanish, French, German, Arabic, Portuguese, Hindi, or Italian. \footnote{These languages are commonly supported by Llama 3.1, Qwen 2.5, GPT-3.5, and GPT-4o. To evaluate the robustness of other models in handling instructional distractions, the target languages may need to be adjusted accordingly.} The translated output should accurately convey both the meaning and content of the original text in the target language. We create ten instructions to guide the translation process, including prompts such as "\textit{Translate the input text into German.}"

\paragraph{Style Transfer}
Style transfer is a task aimed at transforming a given text to align with a specified stylistic framework. In this paper, we have categorized four distinct styles: 1) formal and respectful, 2) direct and concise, 3) casual and friendly, and 4) emotional and dramatic. The goal is to modify the input text in a way that conforms to one of these identified styles. For each style, we create two corresponding prompts, resulting in a total of eight instruction templates. One such example includes: "\textit{Reword the input text in a more casual and friendly tone.}"

\subsubsection{Tasks for Input Data}
\paragraph{Reasoning}
The reasoning task is intended to evaluate the model's capacity to make logical inferences or solve problems based on a provided scenario. The data for this task is sourced from the ARC dataset~\cite{clark2018think}, which encompasses a diverse range of linguistic and inferential phenomena. Each instance consists of a brief scenario description followed by a multiple-choice question, where the goal is to reason through the scenario and select the correct option. 

\paragraph{Code Generation}
The code generation task involves asking the model to generate code based on a set of instructions or prompts. This task is derived from the Code Alpaca dataset~\cite{chaudhary2023code}, which includes a variety of coding challenges and real-world programming problems. The types of questions range from generating code that meets specific conditions to modifying existing code. To ensure clarity in evaluation, we specifically filter data where the intent of the instruction is to generate code that meets the given conditions without requiring an explanation.

%In cases where the task involves modifying code, ambiguity can arise since the desired output might be either the corrected code or an explanation of how to implement the changes. 

\paragraph{Mathematical Reasoning}
The mathematical reasoning task requires the model to solve math problems, ranging from basic arithmetic to more advanced topics~\cite{imani2023mathprompter}. These problems are sourced from the GSM8k~\cite{cobbe2021training} and MATH datasets~\cite{hendrycks2021measuring}, with an equal number of problems extracted from each dataset.
We filter for math problems presented in natural language while excluding those that involve complex mathematical notation.
%Problems involving complex mathematical notation are excluded. Instead, we filter for math problems presented in natural language, focusing on tasks that instruct the model to solve problems rather than define mathematical concepts.


\paragraph{Bias Detection}
The bias detection task aims to detect social biases in language models, particularly by measuring biases across various protected social categories~\cite{gallegos2024bias}. The dataset for this task is derived from the BBQ~\cite{parrish2021bbq}, which consists of human-annotated contexts designed to highlight social biases against different socially relevant groups through multiple-choice questions. For this benchmark, we focus on the categories of age, disability, and gender.

\paragraph{Question Answering}
For the question answering task, we adopt a closed-book question answering approach~\cite{roberts2020much} to evaluate instructional distraction in longer contexts. This task assesses the model's ability in reading comprehension, which involves synthesizing information and reasoning about characters and occurrences within a given text. The task is sourced from the NarrativeQA dataset~\cite{kovcisky2018narrativeqa}, and passage summaries are concatenated with questions related to their context.

\subsection{Statistics}
\label{3.2}
We construct a benchmark by combining the four instruction tasks and five input tasks previously described, resulting in 20 categories. Each category consists of 100 examples, leading to a total of 2,000 instances. The average token length of \textit{Instructions} and \textit{Inputs} for each category is provided in Table~\ref{table_stat}. Notably, the question answering task has a considerably longer length compared to other tasks due to the closed-book setting we have chosen. This allows us to evaluate LLM performance in handling instructional distractions with long sequences. Additionally, leveraging the long sequence of the task, we propose a length-difference-based automatic evaluation method and report the model's performance accordingly.


\input{rsc/Table_main2}

\subsection{Evaluation}
\label{3.3}

In this section, we introduce the evaluation methods used when assessing LLMs with DIM-Bench: an LLM-based evaluation method~\cite{liu2023g} and a length difference-based automatic evaluation method that enhances reliability. 
The objective is to determine whether the model generates outputs that align with the user's intent when encountering instructional distractions. %Specifically, the LLM must avoid being confused by misleading instructions embedded in the input text and must closely follow the instructions and appropriately process the given text.

%To conduct this evaluation, the model's generated output is assessed using LLM judges. 
%Many instruction-following benchmarks utilize LLM-based evaluations to measure how well the output adheres to the provided instructions~\cite{zheng2023judging, wang2023far}. 
DIM-Bench utilizes LLM-based evaluations to assess how effectively the output adheres to the given instructions, following the methodologies established in existing instruction-following benchmark evaluations ~\cite{zheng2023judging, wang2023far}.
Typically, this is done by breaking down the evaluation into binary (\textit{yes}/\textit{no}) questions. 
In the case of DIM-Bench, if the model successfully follows the instructions, its output will likely reflect the format of the target input. 
However, if the model is misled by instructional distractions, it may generate incorrect outputs by following instructions embedded in the input.
To evaluate this, we formulate 2-3 specific questions for each case.
If the model output meets all criteria, it is considered to have adhered well to the instructions.

For example, if the instruction is a translation task (e.g., English to French), and the input task is reasoning, the questions are structured as follows: 1) \textit{Is the target text in French?} 2) \textit{Is the target text in multiple-choice format?} 3) \textit{Have any options from the original text been removed in the target text?} In the third question, the original reasoning question is provided. 
If the LLM-judge's answers are \textit{yes}, \textit{yes}, and \textit{no}, it confirms that the translation instructions are followed correctly, without any confusion from the reasoning task. 
The decomposed questions for the remaining categories are provided in Appendix~\ref{C}.

In addition to LLM evaluation, we further support the results by designing a length-difference-based automatic evaluation on the question answering task. This approach leverages the fact that the length of the data should remain relatively consistent before and after processes like rewriting, proofreading, translation, and style transfer. 
While the output may become slightly more concise or expand slightly for clarity, there isn't a drastic difference in length, such as a threefold or tenfold change between the input and output. 
Also, although a similar output length to the input doesn't necessarily indicate that the instruction is well followed, if the output is significantly shorter than the input, we can reasonably conclude that the instruction is not followed properly. 
Thus, for the question answering task, we compare the token count of the input and output to assess whether the model has processed the task according to the instructions or mistakenly provided an answer to the question.


