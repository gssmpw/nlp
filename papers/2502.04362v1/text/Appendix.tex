
\section{Reproducibility checklists}
\label{A}


\subsection{Dataset and Source Code}

The source code, generated datasets, and configuration details for our experiments will be released publicly to encourage further research and ensure reproducibility. 


\subsection{Computing Resources}
In our experiments, we employ two NVIDIA A100 GPUs, each equipped with 80GB of memory. The code was implemented in Python version 3.7.13, utilizing PyTorch version 1.10.1.

\subsection{Experimental Setting of the LLMs}

The GPT versions utilized in this study are as follows: GPT-3.5 version is \textit{gpt-3.5-turbo-0125}, the GPT-4o-mini version is \textit{gpt-4o-mini-2024-07-18}, and the GPT-4o version is \textit{gpt-4o-2024-08-06}. All models were accessed through OpenAI's official platform.

For the Llama-3.1 models~\cite{dubey2024llama}, we used \textsc{Llama-3.1-8B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}} and \textsc{Llama-3.1-70B-Instruct}\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}}, both sourced from Hugging Face’s official repository.

For the Qwen 2.5 7B model, we used \textsc{Qwen2.5-7B-Instruct}\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}}, , also sourced from Hugging Face’s official repository.

The six LLMs were run with a temperature setting of 0.7, and the scores from a single run are reported. Also, it was observed that the llama 3.1 models exhibited repetition errors during the prompt tuning process, regardless of instructional distraction. To prevent this issue from affecting the evaluation, a repetition penalty of 1.2 was applied.

The LLM evaluation prompt used in Section~\ref{4} is presented in Table~\ref{Table_llmjudge}. 
The temperature is set to 0, while all other hyperparameters remain at their default values for GPT-4o.
\input{rsc/rsc_appendix/Table_llmjudge}

\subsection{Prompts used in experiments}

In Section~\ref{4}, we evaluate various LLMs using DIM-Bench.
The system prompt used to evaluate the LLMs is: "\textit{You are a helpful assistant. Output concisely without any separate explanation.}"


Also, the CoT prompting method employed in Section~\ref{5.1} can be found in Table~\ref{table_cot}.

\input{rsc/rsc_appendix/Table_cot}


\section{Prompts for Instruction Tasks}
\label{B}
\input{rsc/rsc_appendix/Table_taskprompts}
In this study, the focus into four tasks: rewriting, proofreading, translation, and style transfer. The instruction templates used for each task are provided in Table~\ref{Table_taskprompts}.

\input{rsc/rsc_appendix/Table_questions}

\section{Decomposed questions for LLM-based Evaluation}
\label{C}
As explained in Section~\ref{3.3}, we conduct LLM-based evaluation to assess how well the LLM follows instructions.
The decomposed questions for each input task can be found in Table~\ref{table_question}.
In the case of an instruction task being translation, an additional question corresponding to the translation task is included.



\clearpage
