\section{Conclusion}

In this study, we explore the phenomenon of \textit{instructional distraction} in instruction-following tasks, where the input itself resembles an instruction, potentially confusing the model. 
We categorize various instances of instructional distraction as they occur in real-world scenarios and evaluate the performance of several LLMs when confronted with these distractions. 
We demonstrate that all tested LLMs fail to fully match user intent when encountering instructional distraction, highlighting a critical gap in current LLM capabilities in accurately understanding and processing such inputs. 
%This finding underscores the need for further refinement in LLMs to improve their robustness in more complex, real-world tasks.

