\section{Introduction}

%Large Language Models (LLMs)~\cite{radford2019language,touvron2023llama} have exhibited remarkable performance across a wide range of tasks~\cite{wei2021finetuned}, with instruction following being one of the most critical requirements for their applications~\cite{qin2024infobench}. To better align with user instructions and preferences, LLMs are often further trained through instruction tuning for diverse generative tasks~\cite{zhang2023instruction, peng2023instruction, zhou2024lima}. The ability of LLMs to follow instructions reflects how well they align with human intent, which is why various benchmarks and frameworks have been proposed to evaluate this capability~\cite{mishra2021cross, jiang2023followbench,oh2024instructir}.An instruction-following benchmark typically consists of an instruction that clearly describes the task or goal the model must perform, along with an inputâ€”the actual data or information the model needs to process according to the instruction.Studies have shown that more powerful LLMs tend to adhere better to the constraints provided in the instructions~\cite{jiang2023followbench}.

Large language models (LLMs)~\cite{radford2019language,touvron2023llama} have demonstrated remarkable performance across a wide range of tasks~\cite{wei2021finetuned}, with instruction-following being one of the most critical requirements for their applications~\cite{qin2024infobench}. 
To better align with user instructions and preferences, LLMs are often further trained through instruction tuning for diverse generative tasks~\cite{zhang2023instruction, peng2023instruction, zhou2024lima}. In response to the increasing importance of instruction-following capabilities, several benchmarks have been developed to assess various aspects of this ability~\cite{mishra2021cross, jiang2023followbench,zhou2023instruction,oh2024instructir}. Typically, such benchmarks consist of an instruction that clearly describes the task or goal the model must perform, along with a target input---the actual data or information the model needs to process according to the instruction.


\input{rsc/Figure1}

%However, significant challenge arises when the target input itself resembles an instruction, causing \textit{instructional distraction} and resulting in confusion for the LLM~\cite{wallace2024instruction}.

However, a significant challenge arises when the target input itself resembles an instruction, leading to confusion for the LLM~\cite{wallace2024instruction}. 
We refer to this phenomenon as \textit{instructional distraction}.
Rather than simply processing the target input as data, the model struggles to decide whether to follow the primary instruction or the embedded instruction within the target input, potentially leading to degraded performance or unintended outputs. 
%These misinterpretations not only disrupt alignment with user expectations but also expose a significant limitation in LLM performance.
For instance, consider a scenario where a researcher requires extensive Chinese math data and intends to use an LLM to translate the English math data available. 
In this case, the instruction is to translate, while the input text contains math problems, as shown in Figure~\ref{figure1}.
When tasked with this, the LLM may disregard the translation instruction and attempt to solve the math problems instead, providing solutions in English or Chinese rather than translating the original math problems.

%This challenge arises even when efforts are made to avoid ambiguity by clearly separating the instruction from the input and crafting a prompt intended to align seamlessly with the user's intent. 

%Moreover, we observe that this challenge persists even when deliberate efforts are made to eliminate ambiguity by distinctly separating the instruction from the input, and carefully designing a prompt that is intended to align seamlessly with the user's intent.Such misinterpretations not only disrupt the alignment with user expectations but also reveal a significant limitation in LLM performance, particularly in tasks involving data generation and processing where precise instruction-following is critical. However, despite its importance, there is currently no benchmark that systematically evaluates LLM performance in these \textit{instructional distraction} scenarios.

%Moreover, we observe that this challenge persists even when efforts are made to eliminate ambiguity by distinctly separating the instruction from the target input. 
Moreover, we observe that this challenge persists even when efforts are made to distinctly separate the instruction from the target input to create unambiguous prompts. 
%These misinterpretations not only disrupt alignment with user expectations but also expose a significant limitation in LLM performance.
In addition, tasks involving data generation or processing through LLMs~\cite{guo2024generative, long2024llms,patel2024datadreamer}-where instructional distraction frequently occurs-typically require handling large volumes of data at once, making it impractical to modify each prompt individually.
Furthermore, when substantial post-processing is required after data handling, the associated costs increase significantly, posing a serious issue. However, despite the critical nature of this problem, there is currently no benchmark that systematically evaluates LLM performance in these \textit{instructional distraction} scenarios.


\input{rsc/Table_example}

%This work aims to evaluate the robustness of various LLMs in handling such instructional distraction situations. 
%We introduce a novel benchmark, ID-EVAL (Instructional Distraction Evaluation Benchmark), specifically designed to evaluate the instruction-following capabilities of LLMs in complex scenarios where both the instruction and the input take the form of instructions. 
%This work aims to evaluate the robustness of LLMs in handling instructional distraction scenarios. 
To target this issue, we introduce a novel benchmark, \textbf{DIM-Bench} (\textbf{D}istractive \textbf{I}nstruction \textbf{M}isunderstanding \textbf{Bench}mark), specifically designed to assess the instruction-following capabilities of LLMs in complex situations where both the instruction and the target input take the form of instructions.
To reflect real-world use cases, we focus on tasks commonly used in data generation and processing, such as rewriting, proofreading, translation, and style transfer for instruction tasks.
Meanwhile, the input tasks---which play a deceptive role in this benchmark---include reasoning, code generation, mathematical reasoning, bias detection, and question answering.
By combining tasks across two dimensions, DIM-Bench consists of 20 distinct categories, resulting in a total of 2k instances.

Using DIM-Bench, we evaluate the robustness of six LLMs in these instructional distraction scenarios. 
Our experimental findings are as follows:
(1) Even when provided with explicit prompts, no LLM, including advanced models such as GPT-4o~\cite{gpt4o} and Llama-3.1-70B-Instruct~\cite{dubey2024llama}, demonstrates complete robustness against instructional distractions.
%(2) Among the input tasks, question answering was particularly vulnerable. Notably, weaker models such as GPT-3.5 and LLAMA 3.1 (8B parameters) exhibited a significant failure rate. 
(2) Among the input tasks that serve a deceptive role, LLMs are particularly prone to question answering, as they exhibit a strong inclination to output an answer when confronted with a question in the input text.
(3) We explore three prompting methods to mitigate this issue, including direct prompting to ignore certain instructions in the target input; however, while these methods show partial improvement, none fully resolves the problem.
%These findings highlight a critical limitation in the instruction following capabilities of LLMs in instructional distraction scenarios, suggesting the need for further improvements to enhance their robustness in real-world applications.
These findings highlight a critical limitation in the instruction-following capabilities of LLMs in instructional distraction scenarios, suggesting the need for further improvements to enhance their robustness in accurately interpreting and following the user's intent.

