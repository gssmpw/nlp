\section{Background and Related Work}
Recent studies can be divided into two main groups: affine invariant inverse depth estimation and metric depth estimation. Affine invariant inverse depth aims to estimate the disparity depth between objects. Metric depth estimation focuses on outputting depths in standard units such as meters or millimeters.
\subsection{Affine-Invariant Inverse Depth Estimation}
The learning-based method typically uses large and varied datasets to eliminate bias and reduce the degradation of the model, thus maintaining the robustness of the model. For example, camera-specific intrinsic parameters can cause a mismatch between feature and depth information, which blocks the convergence of the model. Therefore, instead of directly estimating depth in meters, some studies predict the relative depth disparity between objects to avoid the scale and shift offset caused by focal length and distortion. MiDaS\cite{MiDaS} pioneered a zero-shot cross-dataset transfer protocol by training its network on multiple datasets and validating its results across the different datasets. MiDaS also introduced scale and shift-invariant loss, which helps the depth estimation models to converge across different datasets. The following version, MiDaSv3.1\cite{MiDaSv3.1}, uses multiple vision transformer-based encoders \cite{beit,dosovitskiy2020vit,liu2021Swin,liu2021swinv2} and dense prediction transformer decoders \cite{dpt} to provide better performance and runtime options for downstream tasks, with the largest model in this framework improving the quality of depth estimation by over 28\%.

More recently, DepthAnything\cite{depth_anything_v1} provided a new foundation model to achieve a more robust zero-shot depth prediction capability than Midasv3.1 by using the unlabelled images to enrich the train data, thus reducing the generalization error. They used a powerful teacher model to generate pseudo-depth annotations for unlabelled images and frozen weights of DINOv2\cite{oquab2023dinov2} as the encoder. They also introduced a feature alignment loss to preserve the semantic prior from the encoder and maintain the model's ability to understand the unseen scene.

\cite{Marigold} reports that scene understanding is essential for depth estimators to predict depth based on scene content accurately. Therefore, a stable diffusion model that can provide comprehensive and encyclopedic prior knowledge would significantly improve the performance and cross-domain capability of the depth estimator. Hence, \cite{Marigold} introduces a latent diffusion model called Marigold, which is fine-tuned by the denoising U-net with synthetic RGB-D data. This approach achieves advanced performance on various natural images. Furthermore, the network was trained on synthetic data and achieved zero-shot transfer to real data. This makes the method more robust to edge cases wrongly acquired by depth sensors, such as transparent and reflective objects.

Compared to Marigold, the Depth Anything model is more sensitive to reflective surfaces and transparent objects, and the prediction of object edges needs to be smoother. To address this, Depth Anything v2 \cite{depth_anything_v2} provides a more resilient model that is more robust in complex scenes. It achieves finer details, surpassing both the Marigold and DepthAnything. The essential method to solve this problem is data. DepthAnything v2 trained its DINOv2-G-based teacher model only on synthetic images, which can avoid the noise from the sensor depth label, which misleads the result by failing to detect the transparent and reflective surfaces. This study also introduces a method of using real images as intermediate learning targets to eliminate the distribution shift problem for direct training with synthetic images.

In general, recent studies in relative depth estimation achieved significant improvement. The model in this field provides fine-grained details, highly accurate results, and zero-shot capability in unseen scenes. However, most real-world scenes need to precisely predict metric depth in meters, which can directly implement the result to the application.
    
\subsection{Monocular Metric Depth Estimation}
Recent research shows significant differences between indoor and outdoor environments regarding space density and depth boundary. Thus, the distribution is critical in helping us sort it during depth regression. DORN \cite{FuCVPR18-DORN} explores the relationship between depth distribution and regression accuracy. It reports that, in distant scenes, features extracted by a Conv-based neural network contribute less helpful information. Instead, these features can introduce noise into the depth prediction.
To address this, \cite{FuCVPR18-DORN} suggests dividing the depth range in a scene into five subintervals by space-increasing discretization. The model also down-weights the loss for the distant scene to maintain the precise nearby. Hence, the prediction can accurately handle different depth distributions at various distances.

Based on DORN, AdaBins\cite{bhat2021adabins} report that the depth distribution represents massive differences across different images, and apparently, sharp depth discontinuities learned from one image can severely affect the result in others. Therefore, AdaBins denotes the depth subintervals as bins, and the bin widths represent the depth range distribution learned from a transformer-based encoder. Since the distribution changes with the images, the model uses an MLP head to predict the correct bin width based on the prior knowledge learned by the encoder. The bin width then becomes the depth prediction in a few intervals. The final depth prediction can be calculated with the linear combination of the bin center.

LocalBins\cite{bhat2022localbins} reports that AdaBins global adaptive bins heavily rely on the transformer-based architecture and features extracted near the output layer of the model. Otherwise, the network exhibits unstable training divergence and convergence at a local minimum, known as the ``late injection problem". To avoid the drawback of global adaptive bin prediction, LocalBins attempts to estimate a per-pixel bin partition corresponding to the depth distribution in adjacent local pixels. Unlike AdaBins, LocalBins uses the bottleneck and decoder functions of the encoder-decoder network, which can learn better depth representations through distribution monitoring.

ZoeDepth\cite{bhat2023zoedepth} introduces a two-stage framework that further improves the performance of metric depth estimation by extracting prior knowledge from relative depth estimation. The model improves the architecture of MidiasV3.1 by replacing the backbone with $BEiT_{384}$-L\cite{beit} to get better performance on relative depth prediction in the first stage, and training the model with extensive and varied depth data to maintain the generalization of the model. The feature extraction from multiple layers in the encoder is then fed into the backbone and fusion in the Metric Bins module\footnote{Introduce an inverse attractor to adjust the bin center rather than split the bin.} to produce the metric depth prediction. This framework was flexible enough to be modified with the encoder-decoder relative depth estimation approaches. The first stage is compatible with the ViT-encoder structure, thus DepthAnything and DepthAnythingV2 can be used as the first stage relative depth predictor, and the metric depth can be fine-tuned using the Metric Bins module of ZoeDepth.

Although ZoeDepth achieves metric depth estimation through a two-step process, this capability is obtained by fine-tuning the dataset of different scenes, such as NYUv2 or KITTI, which is still strongly related to the camera-specific intrinsic parameters. Unlike two-stage methods, UniDepth\cite{piccinelli2024unidepth} tries to predict metric depth directly from an arbitrary image captured in any scene. They show that decoupling the camera and depth representations is the key to solving this problem. Specifically, they transform the 3D point representation from the orthogonal base Cartesian coordinate (x, y, z) to a pseudo-spherical coordinate with azimuth, elevation, and log-depth ($\theta, \phi, logz$) through a self-promoting camera module based on a transformer structure, thus decoupling the angles of the pinhole camera ray from depth in a natural way. To ensure consistency, they then implement a geometric invariance loss to monitor the camera-prompted depth feature of the same scene captured by different cameras.

Meanwhile, Metric3D\cite{yin2023metric} uses a more straightforward method to resolve the metric ambiguity caused by sensor disparity in different datasets. By empirically studying how sensor settings such as pixel size, focal length, and sensor size affect depth prediction, they found that focal length is the critical factor affecting the result. Therefore, to eliminate the metric ambiguity of the focal length, Metric3D introduces a novel transformation method to transform either the image appearance or the ground truth label into a canonical camera space and supervises the training with a randomly proposed normalization loss, which can not only decouple the scale difference but also emphasize the local geometry and depth distribution from a single image. The subsequent work, Metric3Dv2 \cite{hu2024metric3dv2}, simultaneously predicts the metric depth and the surface normal, as they are highly complementary since the metric depth provides large training datasets and the surface normal contains rich local geometry information that can give geometrical constraints on the metric depth during training.

In summary, previous metric depth estimation methods have significantly improved both the zero-shot capabilities of in-the-wild images and the prediction accuracy. However, most of these studies were based on data from public urban roads or indoor scenes. In addition, some attempted to solve extreme cases, such as reflective surfaces or transparent objects, but none were based on images of primary industry scenes such as vineyards or orchards. The lack of scene-based prior knowledge will eventually degrade the performance of these models in an orchard. Therefore, we propose OrchardDepth to predict the depth of the orchard in a more accurate environment.

    \begin{figure}[htbp]
         \centering
         \begin{subfigure}[t]{0.23\textwidth}
             \centering
             \includegraphics[width=\textwidth]{lidar_points_rviz.png}
             \caption{Combined LiDAR Points}
             \label{fig:rviz lidar points}
         \end{subfigure}
         \hfill
         \begin{subfigure}[t]{0.23\textwidth}
             \centering
             \includegraphics[width=\textwidth]{orchard_1.png}
             \caption{RGB Camera Image}
             \label{fig:orchard scene 1}
         \end{subfigure}
         \hfill
         \begin{subfigure}[t]{0.23\textwidth}
             \centering
             \includegraphics[width=\textwidth]{aligned_orchard_1.png}
             \caption{Projected Points}
             \label{fig:visual points in orchard 1}
         \end{subfigure}
            \caption{ \textbf{Data Preparation} - An illustration of the calibration and projection of different sensors. \textbf{(a)} Combined LiDAR Points acquisition from three LiDAR sensors. \textbf{(b)} Image captured from the center camera. \textbf{(c)} Projected Combined LiDAR points to camera space}
            \label{fig:Data Preparation Demostration}
    \end{figure}
    
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Arichtecture.png}
    \caption{\textbf{Pipeline} - \textit{Top}: The training process with the custom dataset, the depth ground truth is obtained from the combined points from three LiDAR sensors, and the points acquired from LiDAR sensors will be projected to the camera coordinate system and then transformed into the mean camera space. \textit{Bottom}: In the training process with the KITTI dataset, we first generate dense depth with the stereo camera image. Then, we use the left image as the input to predict the depth map and convert it back to the acquisition camera coordinate, calculate the loss of predicted depth between dense depth and KITTI ground truth points, respectively, and then use \(L_{con}\) to supervise the dense-sparse consistency.}
    \label{fig:Model Architecture}
\end{figure*}