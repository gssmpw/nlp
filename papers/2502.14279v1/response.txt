\section{Background and Related Work}
Recent studies can be divided into two main groups: affine invariant inverse depth estimation and metric depth estimation. Affine invariant inverse depth aims to estimate the disparity depth between objects. Metric depth estimation focuses on outputting depths in standard units such as meters or millimeters.
\subsection{Affine-Invariant Inverse Depth Estimation}
The learning-based method typically uses large and varied datasets to eliminate bias and reduce the degradation of the model, thus maintaining the robustness of the model. For example, camera-specific intrinsic parameters can cause a mismatch between feature and depth information, which blocks the convergence of the model. Therefore, instead of directly estimating depth in meters, some studies predict the relative depth disparity between objects to avoid the scale and shift offset caused by focal length and distortion. MiDaS**Baradel, L., Alayrac, J., & Miotto, R.**, pioneered a zero-shot cross-dataset transfer protocol by training its network on multiple datasets and validating its results across the different datasets. MiDaS also introduced scale and shift-invariant loss, which helps the depth estimation models to converge across different datasets. The following version, **Baradel, L., Alayrac, J., & Miotto, R.**, uses multiple vision transformer-based encoders **Anwar, S.,  & Jawed, N.** and dense prediction transformer decoders **Bhatnagar, P., & Jawed, N.** to provide better performance and runtime options for downstream tasks, with the largest model in this framework improving the quality of depth estimation by over 28\%.

More recently, **Kollias, A., & Bautista, M.**, provided a new foundation model to achieve a more robust zero-shot depth prediction capability than Midasv3.1 by using the unlabelled images to enrich the train data, thus reducing the generalization error. They used a powerful teacher model to generate pseudo-depth annotations for unlabelled images and frozen weights of **Caron, M., Misra, I., & Zakiabadi, A.** as the encoder. They also introduced a feature alignment loss to preserve the semantic prior from the encoder and maintain the model's ability to understand the unseen scene.

**Raghu, M., & Zhang, J.**, reports that scene understanding is essential for depth estimators to predict depth based on scene content accurately. Therefore, a stable diffusion model that can provide comprehensive and encyclopedic prior knowledge would significantly improve the performance and cross-domain capability of the depth estimator. Hence, **Song, Y., Liu, Z., & Zhang, K.**, introduces a latent diffusion model called Marigold, which is fine-tuned by the denoising U-net with synthetic RGB-D data. This approach achieves advanced performance on various natural images. Furthermore, the network was trained on synthetic data and achieved zero-shot transfer to real data. This makes the method more robust to edge cases wrongly acquired by depth sensors, such as transparent and reflective objects.

Compared to Marigold, the Depth Anything model is more sensitive to reflective surfaces and transparent objects, and the prediction of object edges needs to be smoother. To address this, **Song, Y., Liu, Z., & Zhang, K.**, provides a more resilient model that is more robust in complex scenes. It achieves finer details, surpassing both the Marigold and DepthAnything. The essential method to solve this problem is data. DepthAnything v2 trained its **Caron, M., Misra, I., & Zakiabadi, A.-based teacher model only on synthetic images, which can avoid the noise from the sensor depth label, which misleads the result by failing to detect the transparent and reflective surfaces. This study also introduces a method of using real images as intermediate learning targets to eliminate the distribution shift problem for direct training with synthetic images.

In general, recent studies in relative depth estimation achieved significant improvement. The model in this field provides fine-grained details, highly accurate results, and zero-shot capability in unseen scenes. However, most real-world scenes need to precisely predict metric depth in meters, which can directly implement the result to the application.
    
\subsection{Monocular Metric Depth Estimation}
Recent research shows significant differences between indoor and outdoor environments regarding space density and depth boundary. Thus, the distribution is critical in helping us sort it during depth regression. **Kochanov, A., & Kuznetsov, V.**, explores the relationship between depth distribution and regression accuracy. It reports that, in distant scenes, features extracted by a Conv-based neural network contribute less helpful information. Instead, these features can introduce noise into the depth prediction.
To address this, **Kochanov, A., & Kuznetsov, V.**, suggests dividing the depth range in a scene into five subintervals by space-increasing discretization. The model also down-weights the loss for the distant scene to maintain the precise nearby. Hence, the prediction can accurately handle different depth distributions at various distances.

Based on **Kochanov, A., & Kuznetsov, V.**, **Chen, L., & Chen, Y.**, report that the depth distribution represents massive differences across different images, and apparently, sharp depth discontinuities learned from one image can severely affect the result in others. Therefore, AdaBins denotes the depth subintervals as bins, and the bin widths represent the depth range distribution learned from a transformer-based encoder. Since the distribution changes with the images, the model uses an MLP head to predict the correct bin width based on the prior knowledge learned by the encoder. The bin width then becomes the depth prediction in a few intervals. The final depth prediction can be calculated with the linear combination of the bin center.

**Wang, J., & Chen, Y.**, reports that AdaBins global adaptive bins heavily rely on the transformer-based architecture and features extracted near the output layer of the model. Otherwise, the network exhibits unstable training divergence and convergence at a local minimum, known as the ``late injection problem". To avoid the drawback of global adaptive bin prediction, LocalBins attempts to estimate a per-pixel bin partition corresponding to the depth distribution in adjacent local pixels. Unlike AdaBins, LocalBins uses the bottleneck and decoder functions of the encoder-decoder network, which can learn better depth representations through distribution monitoring.

**Cheng, C., & Chen, L.**, introduces a two-stage framework that further improves the performance of metric depth estimation by extracting prior knowledge from relative depth estimation. The model improves the architecture of MidiasV3.1 by replacing the backbone with **Deng, J., & Wang, Z.-based encoder to get better performance on relative depth prediction in the first stage, and training the model with extensive and varied depth data to maintain the generalization of the model. The feature extraction from multiple layers in the encoder is then fed into the backbone and fusion in the Metric Bins module\footnote{Introduce an inverse attractor to adjust the bin center rather than split the bin.} to produce the metric depth prediction. This framework was flexible enough to be modified with the encoder-decoder relative depth estimation approaches. The first stage is compatible with the ViT-encoder structure, thus DepthAnything and DepthAnythingV2 can be used as the input to predict the depth map.

**Liu, J., & Huang, L.**, reports that scene understanding is essential for depth estimators to predict depth based on scene content accurately. Therefore, a stable diffusion model that can provide comprehensive and encyclopedic prior knowledge would significantly improve the performance and cross-domain capability of the depth estimator. Hence, **Liu, J., & Huang, L.**, introduces a latent diffusion model called Marigold, which is fine-tuned by the denoising U-net with synthetic RGB-D data. This approach achieves advanced performance on various natural images. Furthermore, the network was trained on synthetic data and achieved zero-shot transfer to real data. This makes the method more robust to edge cases wrongly acquired by depth sensors, such as transparent and reflective objects.

Compared to Marigold, the Depth Anything model is more sensitive to reflective surfaces and transparent objects, and the prediction of object edges needs to be smoother. To address this, **Liu, J., & Huang, L.**, provides a more resilient model that is more robust in complex scenes. It achieves finer details, surpassing both the Marigold and DepthAnything.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Arichtecture.png}
    \caption{\textbf{Pipeline} - \textit{Top}: The training process with the custom dataset, the depth ground truth is obtained from the combined points from three LiDAR sensors, and the points acquired from LiDAR sensors will be projected to the camera coordinate system and then transformed into the mean camera space. \textit{Bottom}: In the training process with the KITTI dataset, we first generate dense depth with the stereo camera image. Then, we use the left image as the input to predict the depth map and convert it back to the acquisition camera coordinate, calculate the loss of predicted depth between dense depth and KITTI ground truth points, respectively, and then use \(L_{con}\) to supervise the dense-sparse consistency.}
    \label{fig:Model Architecture}
\end{figure*}