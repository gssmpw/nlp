

\begin{figure*}[htbp]
    \includegraphics[width=\linewidth]{document/figures/sensing_principle.png}
  \hfill
    \includegraphics[width=\linewidth]{document/figures/sensing_principle_sequential2.png}
  \caption{Acoustic and IMU data over 26 isolated English/ASL alphabet letters and continuously fingerspelled words. Continuous fingerspelling adds complexity due to natural flow and quick transitions between letters, which alter sensor values depending on adjacent letters. }
\Description{Acoustic and IMU data over Isolated 26 English manual alphabets and continuous fingerspelled words: Continuous fingerspelling involves added complexity due to the natural flow and quick transitions between letters, which alter sensor values depending on adjacent letters. }
  \label{fig:sensing_principle}
\end{figure*}



\section{SpellRing}

SpellRing is an AI-powered sensing system designed to recognize continuously fingerspelled words using a single ring. This section elaborates on the challenges of designing such a wearable device and details how we developed an AI-powered ring with intelligent sensing methods to achieve accurate recognition.


\subsection{Challenges}
Recognizing continuous fingerspelling poses several unique challenges that make it significantly more complex than recognizing isolated ASL letters:

\subsubsection{Complexity of Handshape, Movement, and Palm Orientation}
American Sign Language (ASL) fingerspelling involves complex combinations of different handshapes, movements, and palm orientations. This poses challenges for accurate fingerspelling recognition. Some letters, such as `A', `S', `M', `N', and `T' (see Figure \ref{fig:sensing_principle}), appear visually similar, while others like `K' and `P', `G' and `Q', or `H' and `U' share the same handshape while differing in palm orientation. Additionally, certain letters (e.g., `J' and `Z') involve specific hand movements, further complicating the recognition process.


\subsubsection{User-Dependent Customized Transitions}
Continuous fingerspelling introduces an additional layer of complexity due to its natural flow and quick transitions between letters (see Figure \ref{fig:sensing_principle}). These transitions vary significantly depending on letter sequences and individual signing behaviors \cite{keane2016fingerspelling, keane2015segmentation}. For instance, the letter `E' can be signed as either a closed or open form, with the open `E' more commonly used during faster fingerspelling, particularly at the beginning or end of a word.
Fluent signers can fingerspell at speeds of 5--8 letters per second \cite{hanson1982use, quinto2010rates, keane2016fingerspelling}, often blending adjacent letters \cite{hassan2023tap, shi2018american, keane2015segmentation}. This high speed increases both efficiency and user-specific signing behaviors, making the accurate recognition of continuous fingerspelling much more challenging than recognizing isolated ASL letters.


\subsubsection{Form Factor vs. Recognition Accuracy Trade-off}
Designing a wearable device for ASL recognition presents a significant challenge in balancing form factor with user experience and recognition accuracy. Glove-based devices with sensors on all fingers can capture detailed poses but are bulky and impractical for daily use, often hindering dexterity. Wristbands, such as EMG sensor bands, offer better usability but struggle with performance issues due to the need for extensive training data across sessions and muscle variability.
Rings with embedded IMUs are more user-friendly, but reliable recognition often requires multiple rings, which can still compromise simplicity. Capturing complex ASL handshapes and movements with a single ring remains a significant challenge, as it must balance unobtrusive design with the ability to capture detailed and reliable data for recognition.


 \begin{table}[t]

\textcolor{black}{
\caption{\textcolor{black}{Performance over Fingerspelling Speed and Sampling Rate. FPS (Frame Per Second), H (Hearing), CODA (Child of Deaf Adults), G (Gender), \# (Max Letters Per Second)}}
\Description{Performance Analysis: Fingerspelling Experience and Sampling Rate}
\begin{tabular}{c|c|c|c|c|c|c}
\hline
   & Experience    & G & Year & \# & FPS - 87     & FPS - 490    \\ \hline
P1 & Leaner, H & M      & 1    & 2                                                                       & 92.99 (2.01) & 93.32 (1.89) \\ \hline
P2 & Leaner, H & F      & 5    & 4                                                                       & 62.67 (3.49) & 86.86 (4.33) \\ \hline
P3 & CODA, H       & M      & 10   & 5                                                                       & 57.93 (3.61) & 86.03 (3.87) \\ \hline
\end{tabular}
}
\end{table}
\label{fig:performance_pilot}
\subsection{Hardware Prototype Design}
To address these challenges, we developed SpellRing, a single-ring system capable of recognizing fingerspelled words at the word level. Our design incorporates two key sensing modalities: acoustic sensing for handshape and IMU sensors for movement.



\begin{figure*}[t]
  \includegraphics[width=0.8\linewidth]{document/figures/model.png}
  \caption{Fusion Model Framework}
  \Description{Fusion Model Framework}
  \label{fig:model}
\end{figure*}

\subsubsection{Single Ring Approach}

SpellRing is designed specifically for the thumb. While Ring-a-Pose \cite{yu2024ring} showed that rings could potentially be placed on all five fingers to track handshape, thumb placement is ideal for ASL recognition. Placing the sensor on other fingers leads to blockage issues, especially when fingerspelling letters such as `A', `S', `M', `N', `L', and `I', and during transitions between these letters. This blockage makes it difficult both to fingerspell and to capture handshape using acoustic sensing. We chose to position the ring on the thumb to minimize these blockage issues. \textcolor{black}{We further discuss the ring's placement and user experience in Section \ref{future}.}

\subsubsection{Sensing Modalities} \textit{1) Active Acoustic Sensing:} For handshape sensing, we chose to adpot active acoustic sensing on the ring. Only requiring low-power and miniature microphone and speakers, this sensing method has shown promising performance in tracking and understanding various body postures on wearables\cite{yu2024ring,li2022eario,li2024eyeecho,li2024gazetrak,mahmud2023posesonic,li2024sonicid,mahmud2024actsonic,mahmud2024munchsonic,sun2023echonose,lee2024echowrist,zhang2023echospeech,mahmud2024wristsonic,zhang2023hpspeech,parikh2024echoguide}. Similar to Ring-a-Pose\cite{yu2024ring}, the ring acts as a `scanner' by emitting inaudible sound waves (frequency range of 20-24 kHz) to scan hand shapes. These sound waves are reflected and refracted by the fingers and received by the microphone on the ring. The preprocessed reflected acoustic signal patterns vary with different hand shapes, leading to precise estimation of handshape \cite{yu2024ring}. \textcolor{black}{However, our earlier experiments (see Table 2) with users of varying fingerspelling skills—especially in speed—using Ring-a-Pose showed that the system struggled to handle rapid fingerspelling of a participant with 10 years of ASL signing experience with a fingerspelling speed of up to 5 letters per second, resulting in an accuracy of 57.93\% on 1,164 words; our experiment is described in detail in Section \ref{experiment}. To capture sufficient information during fast fingerspelling, we increased the sampling rate by six times based on our hardware capabilities. This adjustment reduced the sensing range from 2.06 m (as with Ring-a-Pose \cite{yu2024ring}) to 34.3 cm, focusing more on finger and hand movements and capturing information every 0.12 seconds to classify letters. These changes led to improved performance, achieving an accuracy of 86.03\%---we used this setup for our full experiment.}



\textit{2) IMU for Hand Movement:} To track hand movement and palm orientation, we utilize a gyroscope from the IMU module \cite{zhang2017fingersound,zhang2017fingorbits}. This allows us to measure changes in rotational velocity (angular velocity) around three axes (x, y, and z). By integrating these measurements over time, we can track changes in hand movement, making it easy to distinguish letters with similar handshapes but different palm orientations.
\begin{figure}[b!]
  \includegraphics[width=\linewidth]{document/figures/prototyping.png}
  \caption{Hardware Prototype: (a) a 3.7V 70mAh LiPo battery, (b) an nRF MCU, (c) a customized Flexible Printed Circuit Board (FPCB) with a microphone and speaker, (d) an IMU sensor board (MPU6050), (e) an ESP32 Feather Board, and (f) a 3D-printed ring case.}
  \Description{Hardware Prototype}
  \label{fig:prototype}
\end{figure}

\subsubsection{Hardware Components}
As shown in Figure \ref{fig:prototype}, the ring incorporates a microphone (TDK-ICS-43434), a speaker (USound UT-P2019), and a customized Flexible Printed Circuit Board (FPCB) \textcolor{black}{(c)} enclosed within a 3D-printed Polylactic Acid (PLA) case \textcolor{black}{(f)}. It also features a microcontroller unit (MCU) \textcolor{black}{(e)}, an SD card for data storage \textcolor{black}{(b)}, and a 3.7V 70mAh LiPo battery \textcolor{black}{(a)}. The ring is powered by the battery, which has a switch for toggling it ON/OFF. Once powered on, the acoustic sensing system initiates and automatically saves data to the SD card until powered off. The IMU sensor board (MPU6050) \textcolor{black}{(d)} includes 6-axis inertial motion sensors (accelerometer and gyroscope), providing three-axis data output at a rate of 150Hz. The IMU is connected to the microcontroller on the wrist via a flexible wire, and the microcontroller transmits the data to an external PC through a flexible USB cable. \textcolor{black}{The acoustic data on the SD card and the IMU data on the PC are then synchronized based on timestamped records.}





% The SpellRing prototype includes a 3.7V 70mAh LiPo battery for powering the acoustic data acquisition, an onboard SD card for storing acoustic data, an ESP32 feather strapped on the wrist for reading IMU data, and an MPU6050 IMU sensor for capturing finger movement. The ESP32 feather reads the IMU data at 150 Hz and transmits it to a PC via serial-over-USB. While our current prototype includes a wrist-mounted component to reduce bulk on the thumb, further miniaturization is possible to eliminate this component in future iterations.


\subsection{Algorithms and Data Processing Pipeline}
SpellRing's software pipeline is designed to process multimodal data from the acoustic and IMU sensors and recognize fingerspelled words accurately. Our approach incorporates sophisticated data processing techniques and a deep learning model optimized for sequence recognition.

\subsubsection{Acoustic Data Processing}

Correlation-based frequency modulated continuous wave (C-FMCW) \cite{wang2018c} is used as the transmitted signal for acoustic sensing. The received signals are processed to calculate an echo profile, following methods specified in prior work \cite{yu2024ring,li2022eario,zhang2023echospeech}. These echo profiles encode temporal and spatial information of reflection and diffraction strengths, representing different handshape patterns. To isolate handshape changes from constant environmental reflections, we calculate the difference between consecutive echo frames, generating differential echo profiles. These profiles serve as the input representation of handshape patterns for our deep learning pipeline.

\subsubsection{IMU Data Processing}
Tri-axial gyroscope data, sampled at 150 Hz, is used to track palm orientation and rotational movement. Before feeding them into the deep learning model, we normalize the x, y, and z values and upsample them to synchronize with the acoustic data. This preprocessing step ensures that we can extract synchronized features from our multimodal deep learning pipeline.

\subsubsection{Deep Learning Pipeline}
Our deep learning pipeline leverages Connectionist Temporal Classification (CTC) \cite{graves2006connectionist,zhang2023echospeech}, a method widely employed in sequence labeling tasks, to recognize fingerspelled words continuously without needing to label or segment each letter. Aa shwon in Figure \ref{fig:model}, the model architecture comprises two main components: an acoustic sensing model and an IMU sensing model.

% \subsubsection{Model Framework}

For the acoustic sensing model, \textcolor{black}{we process differential echo profiles using a convolutional neural network (CNN) with ResNet-18 as the backbone. During pooling steps, we apply one-dimensional average pooling along the temporal axis only, rather than both axes, to preserve sequential information.} The IMU sensing model employs a 2D CNN architecture to process IMU data, as our pilot study demonstrated that this approach slightly outperformed a 1D CNN in terms of CTC loss.

The embeddings generated from both modalities are concatenated and then fed into a fully connected dense layer. This is followed by a dropout layer to prevent overfitting, and finally, a softmax function to produce the output probabilities. This multimodal approach allows our system to effectively combine information from both acoustic and motion sensors, enhancing the accuracy of fingerspelling recognition.

\subsubsection{Data Augmentation and Training Scheme}
To enhance performance and streamline training, we adopted several techniques. \textcolor{black}{To enhance the model's adaptability to varying fingerspelling speeds with a fixed window size, we augment the dataset by merging consecutive fingerspelled words, simply concatenating up to four words.} We also apply random noise during training to prevent overfitting and use random padding to handle variable-length inputs. Our training process involves a two-step approach: first training with data from all participants except one, then retraining with the specific participant's data for leave-one-session-out cross-validation. \textcolor{black}{We note that this two-step approach results in a user-dependent model, using 20 sessions collected from each participant over two to three different days, and the following reported results are based on this setup. User-independent results are further discussed in Section \ref{pretrained model}. }

\subsubsection{Word Correction}
To correct potential errors in the model's character sequence predictions, we compute the Levenshtein distance \cite{Levenshtein1965BinaryCC} between the predicted sequence and each unique word in a reference dictionary. The word with the smallest Levenshtein distance was selected as the corrected word, enhancing the overall accuracy of our system. \textcolor{black}{To align our system evaluation with prior literature, specifically for performance comparison with FingerSpeller \cite{martin2023fingerspeller}—such as multi-ring versus single-ring setups—we adopted their evaluation method by using the MacKenzie-Soukoreff phrase set \cite{mackenzie2003phrase} as the reference dictionary.}
\textcolor{black}{However, since the choice of reference dictionary affects the performance of the auto-correction, we discuss its impact using different dictionary sets in Section \ref{auto_correction}.}



