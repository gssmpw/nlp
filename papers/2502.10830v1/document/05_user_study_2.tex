\section{Phrase-level Recognition}

\subsection{Purpose and Overview}
Building on the insights from our word-level study, our second investigation aimed to evaluate SpellRing's performance for real-time phrase-level recognition. We sought to understand how the system performs in more natural contexts, how users adapt their signing behavior to real-time feedback, and the effectiveness of language models in improving recognition accuracy. It is worth noting that most prior work \cite{paudyal2017dyfav, mummadi2017real} using wearables does not evaluate fingerspelling recognition continuously in real-time. This real-time performance study was a crucial step in assessing SpellRing's potential for practical, everyday use \textcolor{black}{in comprehensive ASL recognition systems.}

\subsection{Participants}
We recruited 11 participants (4 male, 7 female, mean age = 32.0, SD = 3.88) to evaluate our system, consisting of 8 Deaf individuals, 1 ASL interpreter, and 2 hearing ASL learners. The 8 Deaf participants use ASL as their primary language are fluent ASL signers and fingerspellers. The 2 hearing participants had been learning ASL for 1 and 2 years, respectively, contributing to differences in their fingerspelling proficiency. All participants fingerspelled using their right hand as their dominant hand. Detailed information about participants’ proficiency and background is provided in Table \ref{fig:table2}.






\subsection{Iteration on Hardware Prototype Design}
To evaluate our system in a more natural fingerspelling environment, we redesigned the ring prototype to a smaller form factor, enabling real-time evaluation. Our design optimized the device for comfort and ease of use while ensuring it supported continuous and real-time tracking for natural fingerspelling. \textcolor{black}{As shown in Figure \ref{fig:prototype2}, audio data from the FPCB microphone \textcolor{black}{(d)} connected to our custom nRF MCU \textcolor{black}{(b)} and gyro data from the IMU \textcolor{black}{(c)} were synchronized and sent to an off-the-shelf ESP32 S3 microcontroller \footnote{Adafruit QT Py ESP32-S3 WiFi Dev Board with STEMMA QT - 8 MB Flash} \textcolor{black}{(a)}.} This data was then transmitted via the wire to a backend system for processing through our machine learning pipeline, running on a MacBook Pro. The raw predictions were processed through autocorrection and language model pipelines to generate the final output.
\begin{figure}[t]
  \includegraphics[width=\linewidth]{document/figures/new_prototying.png}
  \caption{Prototype for real-time phrase-level evaluation: (a) ESP32 S3 microcontroller, (b) nRF MCU, (c) IMU sensor, (d) FPCB, and (e) a 3D-printed ring-shaped design}
  \Description{Prototype for real-time phrase-level evaluation: (a) ESP32 S3 microcontroller, (b) nRF MCU, (c) IMU sensor, (d) FPCB, and (e) a 3D-printed ring-shaped design}
  \label{fig:prototype2}
\end{figure}


\subsection{Language Model}
With our dataset, we used an N-gram language model to correct fingerspelled words within phrases. Based on the LM model described in \cite{zhu2018typing}, we generated a list of top N words (N = 20) along with their similarity values after autocorrecting a raw predicted word. For each new predicted word in the top N, we applied bigram and trigram probabilities and selected the word set with the highest probability for the final predicted phrase. We then calcuated WER between the ground truth phrases and the final predicted phrases for evaluation. 










\subsection{Dataset and Procedure}

For our phrase-level prediction evaluation, we again utilized the MacKenzie-Soukoreff Phrase Set \cite{mackenzie2003phrase}, as in the first user study. The phrases ranged from 16 to 40 characters in length, consisting of 4 to 8 words each. Our study procedure consisted of two main phases: initial data collection and real-time evaluation.



\subsubsection{Phase 1: Training Data Collection}
We first collected training data from all 11 participants, following a procedure similar to Study 1. Each participant provided two rounds of training data for 1,164 words (2*1164). Our training process involves a two-step
approach: first training with data from all participants except one, then retraining with the specific participant’s data
 for real-time phrase evaluation. Each participant provided the two rounds of training data over two separate days. 
\begin{figure}[t]
  \includegraphics[width=0.8\linewidth]{document/figures/interface_study2.png}
  \caption{User Interface for Real-time Evaluation: The ground truth phrases are displayed in red, and the predicted phrases appear based on each fingerspelled word. Participants receive status updates below, such as `start signing' and `processing.'  }
  \Description{ser Interface for Real-time Evaluation: The ground truth phrases are displayed in red, and the predicted phrases appear based on each fingerspelled word. Participants receive status updates below, such as 'start signing' and 'processing.'  }
  \label{fig:interface_study2}
\end{figure}
\subsubsection{Phase 2: Real-time Evaluation}
\begin{table*}[t]
\caption{Offline evaluation of 1,164 word-level recognition with WPM, Top N and LER, and real-time evaluation of 100 phrase-level recognition with WER and WPM, G: Gender, H: Hearing, DHH: Deaf or Hard of Hearing. Year: Indicates ASL experience (years learning ASL or using it as a primary language)}
\Description{Offline evaluation of 1,164 word-level recognition with WPM, Top N and LER, and real-time evaluation of 100 phrase-level recognition with WER and WPM,G: Gender, H: Hearing, DHH: Deaf or Hard of Hearing. Year: Indicates ASL experience (years learning ASL or using it as a primary language)}
\begin{tabular}{cccc|ccccccc|cc}
\hline
\textbf{}                      & \textbf{}                           & \textbf{}                            & \textbf{}     & \multicolumn{7}{c|}{Offline}                                                                                                                                                                                                                      & \multicolumn{2}{c}{Real-Time}                       \\
\textbf{}                      & \textbf{}                           & \textbf{}                            & \textbf{}     & \multicolumn{7}{c|}{\textbf{Word-level}}                                                                                                                                                                                                          & \multicolumn{2}{c}{\textbf{Phrase-level}}           \\ \hline
\multicolumn{1}{c|}{\textbf{}} & \multicolumn{1}{c|}{\textbf{Types}} & \multicolumn{1}{c|}{\textbf{G}} & \textbf{Year} & \multicolumn{1}{c|}{\textbf{Top 1}} & \multicolumn{1}{c|}{\textbf{Top 2}} & \multicolumn{1}{c|}{\textbf{Top 3}} & \multicolumn{1}{c|}{\textbf{Top 4}} & \multicolumn{1}{c|}{\textbf{Top 5}} & \multicolumn{1}{c|}{\textbf{LER}}   & \textbf{WPM}  & \multicolumn{1}{c|}{\textbf{WER}}   & \textbf{WPM}  \\ \hline
\multicolumn{1}{c|}{\textcolor{black}{Avg.}}       & \multicolumn{1}{c|}{}               & \multicolumn{1}{c|}{}                & \textit{11}   & \multicolumn{1}{c|}{\textit{82.60}} & \multicolumn{1}{c|}{\textit{87.45}} & \multicolumn{1}{c|}{\textit{90.19}} & \multicolumn{1}{c|}{\textit{91.56}} & \multicolumn{1}{c|}{\textit{92.54}} & \multicolumn{1}{c|}{\textit{0.149}} & \textit{39.9} & \multicolumn{1}{c|}{\textit{0.099}} & \textit{20.1} \\ \hline
\multicolumn{1}{c|}{P01}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{M}               & 10            & \multicolumn{1}{c|}{84.87}          & \multicolumn{1}{c|}{89.17}          & \multicolumn{1}{c|}{92.43}          & \multicolumn{1}{c|}{93.64}          & \multicolumn{1}{c|}{94.58}          & \multicolumn{1}{c|}{0.127}          & 32.1          & \multicolumn{1}{c|}{0.041}          & 22.3          \\ \hline
\multicolumn{1}{c|}{P02}       & \multicolumn{1}{c|}{DHH}            & \multicolumn{1}{c|}{F}               & 5             & \multicolumn{1}{c|}{87.21}          & \multicolumn{1}{c|}{90.54}          & \multicolumn{1}{c|}{93.03}          & \multicolumn{1}{c|}{94.84}          & \multicolumn{1}{c|}{95.87}          & \multicolumn{1}{c|}{0.122}          & 39.5          & \multicolumn{1}{c|}{0.124}          & 19.8          \\ \hline
\multicolumn{1}{c|}{P03}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{F}               & 37            & \multicolumn{1}{c|}{77.49}          & \multicolumn{1}{c|}{83.33}          & \multicolumn{1}{c|}{87.03}          & \multicolumn{1}{c|}{88.66}          & \multicolumn{1}{c|}{90.29}          & \multicolumn{1}{c|}{0.192}          & 47.9          & \multicolumn{1}{c|}{0.112}          & 20.3          \\ \hline
\multicolumn{1}{c|}{P04}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{F}               & 10            & \multicolumn{1}{c|}{77.03}          & \multicolumn{1}{c|}{82.79}          & \multicolumn{1}{c|}{85.62}          & \multicolumn{1}{c|}{87.43}          & \multicolumn{1}{c|}{88.38}          & \multicolumn{1}{c|}{0.193}          & 52.5          & \multicolumn{1}{c|}{0.103}          & 19.7          \\ \hline
\multicolumn{1}{c|}{P05}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{M}               & 5             & \multicolumn{1}{c|}{94.40}          & \multicolumn{1}{c|}{97.41}          & \multicolumn{1}{c|}{98.13}          & \multicolumn{1}{c|}{98.71}          & \multicolumn{1}{c|}{98.99}          & \multicolumn{1}{c|}{0.062}          & 34.5          & \multicolumn{1}{c|}{0.061}          & 20.2          \\ \hline
\multicolumn{1}{c|}{P06}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{F}               & 22            & \multicolumn{1}{c|}{67.18}          & \multicolumn{1}{c|}{76.63}          & \multicolumn{1}{c|}{79.96}          & \multicolumn{1}{c|}{82.00}          & \multicolumn{1}{c|}{83.82}          & \multicolumn{1}{c|}{0.234}          & 56.8          & \multicolumn{1}{c|}{0.134}          & 24.9          \\ \hline
\multicolumn{1}{c|}{P07}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{F}               & 5             & \multicolumn{1}{c|}{90.93}          & \multicolumn{1}{c|}{93.76}          & \multicolumn{1}{c|}{95.65}          & \multicolumn{1}{c|}{96.42}          & \multicolumn{1}{c|}{96.85}          & \multicolumn{1}{c|}{0.1}            & 29.4          & \multicolumn{1}{c|}{0.093}          & 21.3          \\ \hline
\multicolumn{1}{c|}{P08}       & \multicolumn{1}{c|}{Deaf}           & \multicolumn{1}{c|}{M}               & 17            & \multicolumn{1}{c|}{65.27}          & \multicolumn{1}{c|}{72.53}          & \multicolumn{1}{c|}{77.21}          & \multicolumn{1}{c|}{80.17}          & \multicolumn{1}{c|}{81.59}          & \multicolumn{1}{c|}{0.26}           & 52.4          & \multicolumn{1}{c|}{0.17}           & 17.9          \\ \hline
\multicolumn{1}{c|}{P09}       & \multicolumn{1}{c|}{Intepreter, H}  & \multicolumn{1}{c|}{F}               & 10            & \multicolumn{1}{c|}{85.94}          & \multicolumn{1}{c|}{89.67}          & \multicolumn{1}{c|}{93.09}          & \multicolumn{1}{c|}{93.37}          & \multicolumn{1}{c|}{94.86}          & \multicolumn{1}{c|}{0.119}          & 30.1          & \multicolumn{1}{c|}{-}              & -             \\ \hline
\multicolumn{1}{c|}{P10}       & \multicolumn{1}{c|}{Learner, H}     & \multicolumn{1}{c|}{M}               & 2             & \multicolumn{1}{c|}{86.41}          & \multicolumn{1}{c|}{91.92}          & \multicolumn{1}{c|}{94.24}          & \multicolumn{1}{c|}{95.53}          & \multicolumn{1}{c|}{95.95}          & \multicolumn{1}{c|}{0.129}          & 40.2          & \multicolumn{1}{c|}{0.052}          & 20.3          \\ \hline
\multicolumn{1}{c|}{P11}       & \multicolumn{1}{c|}{Learner, H}     & \multicolumn{1}{c|}{F}               & 1             & \multicolumn{1}{c|}{91.92}          & \multicolumn{1}{c|}{94.15}          & \multicolumn{1}{c|}{95.70}          & \multicolumn{1}{c|}{96.38}          & \multicolumn{1}{c|}{96.73}          & \multicolumn{1}{c|}{0.098}          & 23.1          & \multicolumn{1}{c|}{0.101}          & 14.1          \\ \hline
\end{tabular}
\end{table*}
\label{fig:table2}
The real-time evaluation was conducted on a third day.  In the real-time evaluation, we randomly selected 200 phrases generated from the 1164 unique words for our study. We used 100 phrases for practice and the remaining 100 phrases for testing. 

We began with practice sessions, where participants were given 100 phrases to familiarize themselves with our interface (Fig. \ref{fig:interface_study2}) and the real-time feedback mechanism. This preparatory step ensured that participants were comfortable with the system before the actual evaluation. For the evaluation, we used the remaining 100 phrases. Participants fingerspelled these phrases over the course of 5 sessions in natural environments such as their homes or quiet rooms. During each evaluation session, participants fingerspelled according to the prompts illustrated in Figure \ref{fig:interface_study2}. 

Participants were first shown the phrase, and instructed to begin fingerspelling each word after pressing the space key. As they fingerspelled, the real-time prediction model provided immediate feedback by displaying the predicted output in green on the interface.  Participants were instructed to proceed to the next word even if they saw a mispredicted word on the screen to ensure that they continued to fingerspell each word as displayed. The language model sometimes corrected a mispredicted word as signers completed more words in each phrase.


After completing each phrase, participants pressed the space key with their non-dominant hand to display the next phrase, allowing them to see and prepare for it. Once ready, they pressed the key to start the phrase and pressed it again upon completion. This action served a dual purpose: it advanced the interface to the next phrase and also marked the start and end times for fingerspelling. This timing information allowed us to calculate fingerspelling speed for each phrase by minimizing perception time, allowing for more precise estimation of participants' fingerspelling speed and any adjustments they made in response to real-time feedback. The duration of each session varied based on the participant's fingerspelling proficiency and typically ranged from 10 to 12 minutes. This variation in session length allowed us to accommodate different signing speeds and ensure that all participants could complete the phrases comfortably.

In total, we collected data on 993 phrases from 10 participants. Due to technical issues, we lost data for seven phrases, and one participant was unable to complete the entire study. Despite these minor setbacks, the collected data provided a robust basis for evaluating our system's performance in real-time, continuous fingerspelling recognition. This two-phase approach allowed us to first train our system on participant-specific data and then evaluate its performance in a realistic, real-time scenario. 







\subsection{Evaluation Metrics}
To evaluate our system, we use Word Error Rate (WER) to report performance. The WER metric ranges from 0 to 1, where 0 indicates that the compared texts are identical, and 1 indicates that they are completely different with no similarity. For example, a WER of 0.10 means there is a 10\% error rate in the compared sentences. WER is based on Levenshtein distance, but it operates at the word level instead of the phoneme (or in this case, letter) level.


\begin{equation}
WER = \frac{{\text{Substitutions} + \text{Deletions} + \text{Insertions}}}{{\text{Total number of words in the reference phrase}}}
\end{equation}








\subsection{Results}
Overall, \theDevice{} recognized 100 phrases with a WER of 0.099\% (0.039\%). While word-level performance achieved an average LER of 0.149\%, phrase-level WER improved with use of a language model. 


\subsubsection{Recognition Performance}
Our results show that fingerspelled words are better recognized within the context of a phrase using a language model (See Table 4). The model showed lower performance for faster signers, such as P06 and P08, with a top-1 accuracy of 67.27\% and 65\%, and WERs of 0.134 and 0.17; this translates to approximately 15\% error on the phrases. Compared to word-level recognition performance, this offers an improvement in recognizing fingerspelled words \textcolor{black}{by applying corrections using a language model at phrase-level recognition.} 



\subsubsection{Signing Speed in Phrase-Level Prediction}

We observed that participants adjusted their signing speed and habits according to the predictions displayed on the user interface, leading to a decrease in words per minute (WPM) for these participants, with WPM averages ranging from 39.87 (data collected in training phase) to 20.09 (data collected in real-time phase) (See Table 3). This decrease accounts for latency, including model processing time, participants' reaction times, and participants' fingerspelling more slowly in response to prediction accuracy.



\subsubsection{Qualitative Analysis}
Participants were asked open-ended survey questions regarding their overall experience with \theDevice{} in terms of performance, form factor, and usability. For performance, 8 out of 10 participants reported that the system performed well with the language model, even when phrases were entirely misclassified. They noted that short fingerspelled words (e.g., "a," "I," "am", "be") were not always recognized accurately but could be corrected by the language model when more context was available. However, participants noted that the system did not always work well at first. In these cases, we observed changes in participant behavior based on predicted results; they tended to fingerspell more slowly and distinctly immediately after they encountered recognition errors. \textcolor{black}{Although our offline evaluation demonstrated that the system works reliably without requiring participants to alter their signing behavior, real-time evaluation revealed that participants adjusted their fingerspelling habits dynamically to accommodate the system. Specifically, they slowed their fingerspelling immediately after observing misrecognized words but returned to their natural habits when the system performed accurately.}



After some practice sessions, some participants adjusted their fingerspelling speed and habits to accommodate the system. P01, P02, P05, and P07 stated that they focused on spelling clearly and distinctly without skipping letters. This directly contrasted their natural fingerspelling behavior, which often involved partially articulated letters and quick, seamless transitions between letters. 
