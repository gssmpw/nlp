\thispagestyle{empty}
\vspace{2cm}
\begin{figure}[ht]
\centering
\includegraphics[scale=0.25]{figures/DSC000672.jpg}
\label{fig:logo}
\end{figure}
\begin{center}

\vspace{1cm}
	\textbf{\Large REGULARIZAÇÃO, APRENDIZAGEM PROFUNDA E INTERDISCIPLINARIDADE EM PROBLEMAS INVERSOS MAL-POSTOS}
	
\vspace{1cm}

\textbf{{\large Perguntas e Respostas}}
	
\vspace{3cm}
	
\begin{center}
\begin{tabular}{ c l }
Roberto Gutierrez Beraldo \orcidlink{0000-0001-6986-3435} & Universidade Federal do ABC  \\ 
 Ricardo Suyama \orcidlink{0000-0002-8398-5268} & Universidade Federal do ABC  \\  
\end{tabular}
\end{center}

\vspace{2cm}
	
{\large \textbf{Book Preprint}}
	
\vspace{\fill}

SANTO ANDRÉ (SÃO PAULO) - BRASIL \\
2025

\end{center}

\newpage
\justify
\thispagestyle{empty}

\section*{\centering PREPRINT DISCLAIMER}

\begin{itemize}
\item \textbf{Sugestões, colaborações, correções e reclamações} são muito bem-vindas e podem ser enviadas para \href{mailto:roberto.gutierrez@ufabc.edu.br}{roberto.gutierrez@ufabc.edu.br} ou para \href{mailto:ricardo.suyama@ufabc.edu.br}{ricardo.suyama@ufabc.edu.br}.
\item Esta é a primeira versão do livro, que ainda não passou por um processo de revisão por pares e editoração formal. Assim, de tempos em tempos ele será atualizado. 
\item A forma escolhida para o livro foi de perguntas e respostas. A maioria das seções foi definida desta maneira, quando adequado. O leitor pode acompanhar o livro do começo ao fim ou ir rapidamente para a pergunta de interesse; 
\item Considerando que há muitas perguntas, a profundidade das respostas varia. O objetivo não era esgotar o assunto, mas sim agrupar elementos e referências relevantes dentro de seu contexto;
\item Existem muitas expressões usualmente utilizadas na língua inglesa. Ao longo do livro, a maioria delas foi traduzida para o português. No entanto, os acrônimos e siglas foram mantidos em inglês, além de incluir uma lista de traduções específicas no material pré-textual. O objetivo é facilitar a busca por tais palavras-chave; 
\item Nas citações, há indicações de páginas, capítulos, apêndices e demais partes da referência, para facilitar a localização do conceito de interesse. No entanto, entre edições ou repositórios, essas indicações podem ser diferentes. No caso de ambiguidades, a página indicada pode ser desconsiderada;
\item As URLs foram verificadas durante a escrita, mas não há como garantir que elas funcionem em momentos posteriores. 
\end{itemize}

\vfill

\newpage
\thispagestyle{empty}
\section*{\centering PREFÁCIO DA PRIMEIRA EDIÇÃO}
A área de pesquisa de problemas inversos tem como objetivo, principalmente, estudar seu comportamento e propor soluções para eles. Um exemplo é a obtenção de informações de um fenômeno a partir apenas de medidas indiretas disponíveis. O desafio é que eles são usualmente mal-postos: podem não ter solução, não ter solução única ou cuja solução é instável a pequenas flutuações nos dados. Com origem na área de matemática aplicada, resolver tais problemas encontra aplicações também nas áreas da física, engenharias e da computação. Nesse contexto, a teoria da inversão de Tikhonov \cite{tikhonov1977solutions} foi um marco, pois trouxe bases teóricas importantes para entendê-los e também desenvolveu o método de regularização como proposta prática de solução. 

Na literatura produzida no Brasil, existem livros sobre problemas inversos e de regularização de modo mais geral \cite{Neto2005}, ou focando em algoritmos específicos para sua solução \cite{Neto2016} e aplicações específicas \cite{2016menin}. Há também diversas dissertações e teses sobre o assunto e livros do IMPA \cite{baumeister2005topics, bleyer2015novel}, mas em língua inglesa. Sendo assim, por que haver mais um livro sobre o assunto?

Existem artigos e livros que são muito completos, mas voltados à matemática aplicada, como \cite{Benning2018, engl1996regularization}, exigindo mais conhecimentos prévios sobre o assunto. Existem também livros que descrevem problemas inversos discretos, partindo principalmente de conceitos de álgebra linear \cite{aster2019parameter, hansen2010discrete, Mueller2012}. Assim, a nossa primeira motivação foi escrever um material sobre problemas inversos discretos que fosse razoavelmente acessível, sob o ponto de vista da língua ou mesmo da notação matemática.

Nos últimos anos, soluções de ponta a ponta de aprendizagem profunda vêm sendo propostas para solução de problemas inversos, contribuindo em várias aplicações \cite{Adler2021, Bai2020, Belthangady2019, Koh2021, Ongie2020, Su2022}. Também estão sendo propostos métodos que unem os paradigmas dos métodos de regularização e de aprendizagem profunda na solução de problemas mal-postos, utilizando as informações do modelo e dos dados em conjunto \cite{Arridge2019}.  Partindo do pressuposto que as informações obtidas nos modelos e nos dados são complementares, a nossa segunda motivação para escrita deste livro foi explorar e introduzir tais métodos integrados para novos leitores, apresentando diversas possibilidades.  

Finalmente, regularização pode ter diferentes significados dependendo do seu contexto  \cite{Chen2002} e interpretações desenvolvidas na aprendizagem profunda \cite{goodfellow2016deep} podem ficar cada vez mais distantes da definição original de Tikhonov. Considerando regularização um conceito polissêmico, a nossa última motivação foi discutir essas interpretações sob o ponto de vista da multidisciplinaridade e da interdisciplinaridade. O livro discute exemplos de ensino de problemas inversos, mostrando algumas iniciativas que colaboram para divulgação da área de forma acessível, discutindo as vantagens que uma abordagem interdisciplinar pode trazer.

  
\newpage
\thispagestyle{empty}
\section*{\centering PREFACE TO THE FIRST EDITION}
The research area of inverse problems aims to study their behavior and propose solutions to them. An example is obtaining information about a phenomenon from indirect measurements available only. The challenge is that these problems are usually ill-posed: they may have no solution, no unique solution or their solution is unstable due to small fluctuations in the data. Originating in the field of applied mathematics, solving such problems also finds applications in the fields of physics, engineering, and computing. In this context, Tikhonov's inversion theory \cite{tikhonov1977solutions} was a milestone, as it provided important theoretical basis for understanding them and also developed the regularization method as a practical solution proposal.

In the literature produced in Brazil, there are books on inverse and regularization problems in a more general way \cite{Neto2005}, or focusing on specific algorithms for their solution \cite{Neto2016} and specific applications \cite{2016menin}. There are also several dissertations and theses on the subject and IMPA books \cite{baumeister2005topics, bleyer2015novel}, but written in English. So why should there be another book on the subject?

Some articles and books are very complete, but focused on applied mathematics, such as \cite{Benning2018, engl1996regularization}, requiring more prior knowledge on the subject. Other books describe discrete inverse problems, mainly based on linear algebra concepts \cite{aster2019parameter, hansen2010discrete, Mueller2012}. Our first motivation was to write material about discrete inverse problems that was reasonably accessible, from the point of view of language or even mathematical notation.

In recent years, end-to-end deep learning solutions have been proposed for solving inverse problems, contributing to various applications \cite{Adler2021, Bai2020, Belthangady2019, Koh2021, Ongie2020, Su2022}. Methods are also being proposed that combine the paradigms of regularization and deep learning methods to solve ill-posed problems, using information from the model and the data together \cite{Arridge2019}.  Based on the assumption that the information obtained from models and data is complementary, our second motivation for writing this book was to explore and introduce integrated methods to new readers, presenting several possibilities.

Finally, regularization can have different meanings depending on its context \cite{Chen2002}, and interpretations developed in deep learning \cite{goodfellow2016deep} can become increasingly distant from Tikhonov's original definition. Considering regularization a polysemic concept, our last motivation was to discuss these interpretations from the point of view of multidisciplinarity and interdisciplinarity. The book discusses examples of teaching inverse problems, showing some initiatives that contribute to making the area accessible and discussing the advantages that results from an interdisciplinary approach.

\newpage
\section*{\centering ORGANIZAÇÃO DO TEXTO}
Os capítulos são descritos a seguir de acordo com a pergunta que eles
visam responder:
\begin{itemize}
\item \textbf{Capítulo \ref{sec:illposed}}: \textit{Quais são os problemas que se quer resolver?} O principal tema do livro é problemas inversos mal-postos. Este capítulo diferencia problemas diretos, inversos, bem-postos e mal-postos;
\item \textbf{Capítulo \ref{sec:deblur_forward}}: \textit{Aplicação: Como ver deblurring como um problema inverso?} Ilustração dos conceitos do Capítulo \ref{sec:illposed} no problema de \textit{deblurring};
\item \textbf{Capítulo \ref{sec:variational}}: \textit{Qual é o método utilizado para tentar resolvê-los?} Este capítulo discute uma forma de resolver problemas mal-postos, o método de regularização de Tikhonov. Sobre ele, são discutidas algumas das principais formas de termos de regularização;
\item \textbf{Capítulo \ref{sec:de_nonblind}}: \textit{Aplicação: Como obter imagens mais nítidas com deblurring?} Ilustração dos conceitos do Capítulo \ref{sec:variational} no problema de \textit{deblurring};
 \item \textbf{Capítulo \ref{sec:integrated}}: \textit{Existem formas de unir abordagens baseadas em modelos e em dados?}  Este capítulo pressupõe que o leitor conheça a visão geral de aprendizado de máquina e aprendizagem profunda. Após uma breve revisão de conceitos, regularização será rediscutida nesse novo contexto, já que ela adquiriu novos significados. Na sequência, o capítulo discute propostas encontradas na literatura que unem o método de regularização e aprendizagem profunda. Ao final, algumas limitações sobre aprendizagem profunda em problemas inversos são revistas, incluindo questões de interpretabilidade e reprodutibilidade;
\item \textbf{Capítulo \ref{sec:polysemy}}: \textit{Quais são algumas das diferentes interpretações de regularização?}  O capítulo descreve dez possíveis interpretações dos autores sobre o que regularização é ou como ela atua na solução. Também foi verificada a ocorrência desses significados em livros técnicos de problemas inversos e aprendizagem de máquina;
\item \textbf{Capítulo \ref{sec:interdisciplinarity}}: \textit{Quais são algumas das implicações de tantas áreas de pesquisa diferentes explorarem o conceito de regularização?} Após trazer diversos exemplos de conceitos abordados por diferentes áreas de pesquisa, discute-se a polissemia de regularização e possíveis caminhos interdisciplinares da área de problemas inversos, destacando algumas implicações dessas características para o ensino. 
\end{itemize}

\newpage
\thispagestyle{empty}
\section*{\centering AGRADECIMENTOS}

\vfill

Agradecemos o Prof. Dr. Fernando S. de Moura, Prof. Dr. Marcelo Zanotello e Dr. Leonardo A. Ferreira, que deram contribuições valiosas durante a escrita deste livro.  

\vfill

\newpage
 \section*{\centering EPÍGRAFE}\label{part0epi}

\vfill

\epigraph{«\textit{The rapidly increasing use of computational technology requires the development of computational algorithms for solving broad classes of problems. But just what do we mean by the ``solution'' of a problem? What requirements must the algorithms for finding a ``solution'' satisfy?}»}{Tikhonov e Arsenin \cite[Pág. 1]{tikhonov1977solutions}}


\vspace{2cm}


\epigraph{«\textit{Sometimes a computing machine does do something rather weird that we hadn’t expected. In principle one could have predicted it, but in practice it’s usually too much trouble. Obviously if one were to predict everything a computer was going to do one might just as well do without it.}»}{Alan Turing \cite{Turing2004}}


\vfill

\newpage
\renewcommand{\listfigurename}{LISTA DE FIGURAS}
\listoffigures
\renewcommand{\listtablename}{LISTA DE TABELAS}
\listoftables
 
\newpage
 \section*{\centering LISTA DE ACRÔNIMOS E SIGLAS}
\begin{flushleft}
\begin{tabular}{ l l }
\textbf{ADAM}&Adaptive Moment Estimation\\                                           \textbf{ADMM}&Alternating Direction Method of Multipliers\\                       \textbf{ANN}&Artificial Neural Network\\                                                     
\textbf{CGLS} & Conjugate Gradient Least Squares \\                                  
\textbf{CNN}&Convolutional Neural Network\\                                             
\textbf{CPU}&Central Processing Unit\\                                                       
\textbf{CS}&Compressed Sensing\\                                                             
\textbf{CT}&Computed Tomography\\                                                         
\textbf{DCT}&Discrete Cosine Transform\\                                                
\textbf{DFT}&Discrete Fourier Transform\\                                            
\textbf{DIP}&Deep Image Prior\\                                                                 
\textbf{DNN}&Deep Neural Network\\                                                         
\textbf{EIT}&Electrical Impedance Tomography\\                                        
\textbf{ERM}&Empirical Risk Minimization\\                                                 
\textbf{FBP}&Filtered Back Projection\\                                                                                                                      
\textbf{FFT}&Fast Fourier Transform\\                                                                                                                      
\textbf{FISTA}&Fast Iterative Soft Thresholding Algorithm\\                          
\textbf{GCV} &  Generalized Cross-Validation \\                                            
\textbf{GMRES} &  Generalized Minimal Residual Method \\   
\textbf{GPU}&Graphics Processing Unit\\                                                       
\textbf{GSVD}& Generalized Singular Value Decomposition\\                         
\textbf{HQS}&Half-Quadratic Splitting\\                                                        
\textbf{IRLS}&Iterative Re-weighted Least Squares\\                                    
\textbf{ISTA}&Iterative Soft Thresholding Algorithm\\                                   
\textbf{LASSO}&Least Absolute Shrinkage and Selection Operator\\              
\textbf{MAP}&Maximum A Posteriori\\                                            
\textbf{MaxEnt}&Maximum Entropy\\                                           
\textbf{MLE}&Maximum Likelihood Estimation\\                                        
\end{tabular}

\begin{tabular}{ l l }
\textbf{MSE}&Mean Squared Error\\                                                           
\textbf{NSR}&Noise-to-Signal Ratio\\
\textbf{OLS}&Ordinary Least Squares\\                                                     
$P^3$&Plug-and-Play Prior\\                                                                      
\textbf{PDE}& Partial Differential Equation\\                                              
\textbf{PSF}&Point Spread Function\\                                                        
\textbf{RAM}&Random Access Memory\\                                                   
\textbf{RED}&Regularization by Denoising\\                                                   
 \textbf{RIP}&Restricted Isometry Property\\                                              
\textbf{SGD}&Stochastic Gradient Descent\\                                              
\textbf{SNR}&Signal-to-Noise Ratio\\     
\textbf{SRM}&Structural Risk Minimization\\                                              
\textbf{STEM} &  Science, Technology, Engineering, and Mathematics\\
\textbf{SVD}&Singular Value Decomposition\\                                            
\textbf{SVM}&Support Vector Machine\\                                                 
\textbf{TSVD}&Truncated Singular Value Decomposition\\                         
\textbf{TV}&Total Variation\\                                                                     
\textbf{UFABC}&Federal University of ABC\\                                                                     
\textbf{VC}& Vapnik-Chervonenkis \\
\end{tabular}
\end{flushleft}


\newpage
 \section*{\centering LISTA DE TERMOS TRADUZIDOS}
\begin{center}
\textbf{Capítulo \ref{sec:illposed}}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Cenário matemático & \textit{Mathematical setting}\\
Declaração do problema & \textit{Problem statement}\\
Estimação de parâmetros & \textit{Parameter estimation} \\
Fotobranqueamento & \textit{Photobleaching} \\
Identificação do sistema & \textit{System identification}\\
 & \quad ou \textit{model identification} \\
Matriz inversa à direita & \textit{Right inverse} \\
Matriz inversa à esquerda & \textit{Left inverse} \\
Problemas bem-postos & \textit{Well-posed problems}\\
 & \quad ou  \textit{Well-defined problems}\\
Problemas diretos & \textit{Forward problems} \\
 & \quad ou  \textit{direct problems} \cite{Bertero2021}\\
Problemas inversos & \textit{Inverse problems} \\
Problemas mal-postos & \textit{Ill-posed problems} \\
 & \quad ou \textit{Improperly posed problems} \\
Solução de norma mínima & \textit{Minimum norm solution} \\
Solução ingênua & \textit{Naive solution} \\
Subamostragem & \textit{Downsampling} \\
Tomografia computadorizada & \textit{Computed tomography} \\
Tomografia por impedância elétrica & \textit{Electrical impedance tomography} \\
\end{tabular}
\end{center}
\end{table}

\begin{center}
\textbf{Capítulo \ref{sec:deblur_forward}}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Borrar & \textit{Blur} (verbo) \\
Borrão & \textit{Blur} (substântivo) \\
Condição de contorno replicada & \textit{Replicate boundary condition} \\
Deconvolução cega & \textit{Blind deblurring}\\
Deconvolução míope & \textit{Myopic deconvolution}\\
 & \quad ou \textit{Semi-blind deconvolution} \\
Filtragem espacial linear & \textit{Linear spatial filtering}\\
Função de espalhamento pontual & \textit{Point spread function}\\
Melhoramento de imagem & \textit{Image enhancement}\\
Nítida & \textit{Sharp}\\
\end{tabular}
\end{center}
\end{table}


\newpage
\begin{center}
\textbf{Capítulo \ref{sec:variational}}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Amostragem comprimida & \textit{Compressed sensing} \\
Contínuas por partes & \textit{Piecewise continuous} \\
Diferenças para trás & \textit{Backward difference} \\
Função de perda & \textit{Loss function} \\
Imagens de treinamento & \textit{Training images} \\
Irregulares, ásperas, rugosas & \textit{Rough} \\
Irregularidade, aspereza, rugosidade & \textit{Roughness} \\
Máxima entropia & \textit{Maximum entropy} \\
Máximo \textit{a posteriori} & \textit{Maximum a posteriori} \\
Mínimos quadrados total & \textit{Total least-squares} \\
Otimização em dois níveis & \textit{Bilevel optimization} \\
Planas & \textit{Flat} \\
\textit{Priors} baseados em amostras & \textit{Sample-based priors} \\
Problema de Tikhonov na forma geral & \textit{General-form Tikhonov problem} \\
Problema de Tikhonov na forma padrão & \textit{Standard-form Tikhonov problem} \\
Qualidade do ajuste & \textit{Goodness of fit} \\
Quase monotônico & \textit{Quasimonotonic} \\
Rede elástica & \textit{Elastic net}\\
Regularização dupla & \textit{Double regularization} \\
Regularização que promove a esparsidade & \textit{Sparsity-promoting regularisation}$^*$\\
Restrição das propriedades físicas & \textit{Physics constrains} \\
Restrição suave & \textit{Soft constraint} \\
Suave & \textit{Smooth} \\
Termo de fidelidade & \textit{Data fidelity}  \\
 & \quad ou \textit{Data misfit} \\
Transformação reversa & \textit{Back-transformation} \\
Validação cruzada generalizada & \textit{Generalized cross-validation} \\
Variação total & \textit{Total variation} \\
\end{tabular}
\end{center}
\end{table}

\vspace{-4mm}

$^*$ Outras expressões incluem: \textit{sparse regularization}, \textit{sparsity-enforcing regularisation}, \textit{sparsity-encouraging penalty terms}, \textit{sparsity regularization} e \textit{sparse recovery algorithms}.

\newpage
\begin{center}
\textbf{Capítulo \ref{sec:de_nonblind}}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Ajuste fino & \textit{Fine tuning} \\
Decomposição em valores singulares & \textit{Generalized singular value} \\
\quad generalizada & \quad \textit{decomposition} \\
Expansão em valores singulares & \textit{Singular value expansion} \\
Gradientes conjugados para  & \textit{Conjugate gradient} \\
\quad mínimos quadrados & \quad \textit{least squares} \\
Método do residual mínimo & \textit{Generalized minimal residual} \\
\quad generalizado & \quad \textit{method} \\
Posto numérico & \textit{Rank} \\
Problemas com deficiência de posto & \textit{Rank-deficient problems} \\
Razão ruído-sinal & \textit{Noise-to-signal ratio} \\
Razão sinal-ruido & \textit{Signal-to-noise ratio} \\
SVD amortecida & \textit{Damped} SVD \\
\end{tabular}
\end{center}
\end{table}


\begin{center}
\textbf{Capítulo \ref{sec:integrated}}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Arquiteturas guiadas pela física & \textit{Physics-guided architectures}\\
Arquiteturas informadas pela física & \textit{Physics-informed architectures}\\
Caixa-preta &  \textit{Black box}\\
\textit{Design} estruturado de rede & \textit{Structured network design}\\
Camadas totalmente conectadas & \textit{Fully conected layers} \\ 
Diferenciação automática & \textit{Automatic differentiation}\\
Dissipação do gradiente & \textit{Vanishing gradient}\\
Extração de características & \textit{Feature extraction}\\
Função de ativação & \textit{Activation function} \\
Microscopia sem lente & \textit{Lens-free microscopy}\\
Minimização do risco empírico & \textit{Empirical risk minimization} \\
\textit{Perceptron} multicamadas & \textit{Multilayer perceptron}\\
Rasa &  \textit{Shallow} \\
Retropropagação & \textit{Backpropagation}\\
Separação de variáveis & \textit{Variable splitting}\\
Sobreparametrizada  & \textit{Overparameterized} \\
Teorema do ``não há almoço grátis'' & \textit{No free lunch theorem})\\
Unidades & \textit{Units} \\
\end{tabular}
\end{center}
\end{table}






\newpage

\begin{center}
\textbf{Capítulo \ref{sec:polysemy}}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Compacidade & \textit{Compactness}\\
Informação \textit{a priori} fraca & \textit{Weak a priori information}\\
Limitantes & \textit{Bounds}\\
Matrizes circulantes de blocos & \textit{Block circulant matrices} \\
\quad com blocos circulantes & \quad  \textit{with circulant blocks} \\
Método dos residuais & \textit{Residual method}\\
Minimização do risco estrutural & \textit{Structural risk minimization}\\
Quase-soluções & \textit{Quasi-solutions}\\
Pesos dos filtros & \textit{Filter weights} \\
Problemas bem-postos vizinhos &  \textit{Neighboring well-posed problems} \\
Regularização variacional & \textit{Variational regularization} \\
Restrição rígida & \textit{Hard constraint} \\
Restrição suave & \textit{Soft constraint} \\
Suavização dos dados & \textit{Data smoothing}\\
\end{tabular}
\end{center}
\end{table}

\vfill

Observação: Não há expressões específicas no Capítulo \ref{sec:interdisciplinarity}. 

\newpage
\begin{center}
\textbf{Apêndices}
\end{center}

\begin{table}[H]
\begin{center}
\begin{tabular}{ l l }
Absolutamente contínuas & \textit{Absolutely continuous}\\
Aprendizagem da transformada & \textit{Transform learning}\\
Aprendizagem de dicionário esparsa & \textit{Sparse dictionary learning}\\
Ajuste de hiperparâmetros & \textit{Hyperparameter tuning} \\
Colocação &  \textit{Collocation}\\
Densidade \textit{a priori} & \textit{Prior density} \\
Decomposição em valores singulares& \textit{Truncated singular value}\\ 
\quad truncada & \quad \textit{decomposition}\\ 
Equalização de canais & \textit{Channel equalization}\\
Erro médio quadrático & \textit{Mean squared error}\\
Espaço gerado &  \textit{span}\\
Estimativa de máxima verossimilhança & \textit{Maximum likelihood estimation}\\
Estimativa de máximo a posteriori &  \textit{Maximum a posteriori estimation}\\
Memória de acesso aleatório & \textit{Random access memory}\\
Métodos diretos & \textit{Direct methods}\\
Mínimos quadrados ordinário & \textit{Ordinary least squares}\\
Mínimos quadrados regularizado & \textit{Regularized output least squares} \\
Não-suave & \textit{Non-smooth}\\
Posto linha completo & \textit{Full row rank} \\
Produto interno & \textit{Inner product} ou \textit{Dot product} \\
Propriedade de isometria restrita & \textit{Restricted isometry property}\\
Rede elástica & \textit{Elastic net}\\
Relaxação convexa & \textit{Convex relaxation}\\
Suave & \textit{Smooth} \\
Transformação por codificação & \textit{Transform coding}\\
Transformada discreta de Fourier & \textit{Discrete Fourier transform}\\
Transformada rápida de Fourier &  \textit{Fast Fourier transform}\\ 
\end{tabular}
\end{center}
\end{table}
 
\newpage
\section*{\centering LISTA DE SÍMBOLOS}


A lista a seguir traz os principais símbolos presentes nos capítulos, exceto apêndices. A notação básica é:
\begin{itemize}
\item Matrizes: $\mathbf{A}$ ou $\bm{\Sigma}$, possuem dimensão $[m \times  n]$, $[m \times  m]$ ou $[n \times  n]$ 
\item Vetores: $\mathbf{a}$ ou $\bm{\sigma}$ possuem dimensão $[m  \times  1]$ ou $[n \times 1]$ 
\item Escalares: $a$ ou $\sigma$  possuem dimensão $[1  \times  1]$ 
\end{itemize}

\begin{multicols}{2}

\subsection*{Capítulo \ref{sec:illposed}}
\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
$\mathcal{A}$ & Operador direto \\
$ f $ &Entrada (parâmetros) \\
$F$  & Espaço dos parâmetros\\
$g$ &Saída (dados)  \\
$G$  & Espaço dos dados\\
$\forall$ & Para todo\\
$\in$ & 	É membro de\\
$\Delta g$ & Variação de $g$  \\
$\mathcal{A}^{-1}$ & Operador inverso \\
$k(t,s)$ &\textit{Kernel} de Hilbert–Schmidt  \\
$h(t-s)$ & Resposta ao impulso  \\ 
$\mathcal{L}\{\cdot\}(s) $ & Transformada de Laplace  \\
$\mathbf{y}$ &Medidas  \\
$\mathbf{x}$ &Parâmetros  \\
$\mathbf{A}$ &Operador direto \\
$i$, $j$ & Índices das matrizes  \\
$A_{i,j}$ & $(i,j)$-ésimo valor de $\mathbf{A}$ \\
$m$ & Nº de linhas da matriz  \\
 $n$ & Nº de colunas da matriz  \\
$\mathbf{a}_n$ &Enésima coluna de $\mathbf{A}$  \\
$x_i$ &i-ésimo valor de $\mathbf{x}$  \\
\end{tabular}

\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
$\approx$ & Aproximadamente  \\
$\vert\vert \cdot \vert \vert_F$ & Norma de Frobenius \\
$\vert\vert \cdot \vert \vert_p$ & Norma $\ell_p$ \\
$p$ & Índice da norma $\ell_p$\\
$\bm{\epsilon}$ &Erros do modelo \\
$\bm{\delta}$ &Erros dos dados \\
$\mathbf{y}_{\delta}$ &Medidas com erros  \\
$\hat{\mathbf{x}}$ &Grandeza estimada \\
$\mathbf{A}^{-1}$ & Matriz inversa  \\
$\mathbf{I}$ &Matriz identidade   \\
$\mathbf{A}^T$ & Matriz transposta  \\
$\mathbf{A}^+_d$ &Matriz pseudoinversa à direita  \\
$\mathbf{A}^+_e$ &Matriz pseudoinversa à esquerda  \\
s.t. &Sujeito a  \\
$\sigma$ &Valores singulares  \\
$\mathbf{u}$ &Vetores singulares à direita \\
$\mathbf{v}$ &Vetores singulares à esquerda  \\ 
$\mathbf{U}, \mathbf{V} $ & Matrizes ortogonais da SVD  \\
$ \mathbf{\Sigma}$& Matriz diagonal de $\sigma$   \\
$\mathbf{A}^*$ & Conjugado transposto  \\
$diag(\cdot)$ & Matriz diagonal \\
$cond(\mathbf{A})$ & Número de condição de $\mathbf{A}$ \\
$\mathcal{O}$ & Grande-O \\
\end{tabular}

\newpage

\subsection*{Capítulo \ref{sec:deblur_forward}}
\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
$\mathbf{Y}$ &Matriz de imagem degradada\\
$\mathbf{X}$ &Matriz de imagem nítida\\
$\mathbf{H}$ &Matriz do \textit{kernel}\\
$\mathbf{N}$ &Matriz de ruído\\
$\mathbf{X} * \mathbf{Y} $ &  Convolução de $\mathbf{X}$ com $\mathbf{Y}$\\
$\mathbf{h}$ &\textit{Kernel} $\mathbf{H}$ vetorizado  \\
$\mu$ & Média \\
$s$ & Desvio padrão\\ 
$s^2$ & Variância\\ 
$\mathbf{\Gamma}$ & Matriz de covariância \\
$\mathcal{N}(\mu,\mathbf{\Gamma})$ &Distribuição gaussiana \\
$\sim$ & É distribuído como  \\
$\mathbf{A}_{sub}$& Matriz de subamostragem\\
\end{tabular}

\vfill

\subsection*{Capítulo \ref{sec:variational}}
\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
$\hat{\mathbf{x}}_{\lambda}$ &Regularização de Tikhonov   \\
$\mathcal{L}(\cdot)$ &Função de perda  \\
$\Omega(\cdot)$ &Termo de regularização  \\
$\lambda$ &Parâmetro de regularização  \\
$\mathcal{M}(\cdot)$ &Funcional  \\
$ \mathbf{A}^+_{\lambda}$ &Matriz de reconstrução \\
$\mathbf{0}$ & Vetor nulo\\
$G(\lambda)$ &GCV: Função\\
$c$& GCV: número de pontos \\
$Tr(\cdot)$ &Traço  \\
$\mathbf{L}$ &Matriz de regularização  \\
$N(\mathbf{A})$ & Núcleo de $\mathbf{A}$ \\ 
$\cap$& Intersecção entre conjuntos \\ 
$\emptyset$& Conjunto vazio \\ 
$\mathbf{L_{1 \hspace{1mm} d1}}$ & $\mathbf{L}$ de primeira derivada \\
$\mathbf{L_{2 \hspace{1mm} d1}}$ & Outra $\mathbf{L}$ de primeira derivada \\
$\mathbf{L}_{d2}$ & Matriz de segunda derivada \\
$\mathbf{H_{d2}}$ & Kernel laplaciano em 2D\\
$\mathbf{X}$ &Matriz de uma imagem\\
$\mathbf{H} * \mathbf{X}$ & Convolução entre $\mathbf{H}$ e $\mathbf{X}$ \\
$\mathbf{x}^*$ &Valor de referência fixo \\
$\mathbf{x}^*_k$ &Valor de referência variável \\
$k$ & Índice da iteração \\
$\mathbf{I}_N$ & Matriz $\mathbf{I}$  $n \times n$ \\
$\mathbf{I}_P$ & Matriz $\mathbf{I}$  $p \times p$\\
$\mathbf{A}^*$ & Hermitiano de $\mathbf{A}$\\
$\vert\vert \mathbf{x} \vert\vert_{p,q}$ & Norma mista \\
$\nabla \mathbf{x}$ &  Gradiente de $\mathbf{x}$\\
\end{tabular}

\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
$\omega_i$ & Máxima entropia: pesos \\ 
$L_{aug}$ &Lagrangiano aumentado \\ 
$\rho$ &ADMM: escalar  \\
$\mathbf{z}$ &ADMM: variável auxiliar  \\
$\mathbf{u}$ &ADMM: variável dual  \\
$ f(\cdot)$    & \textit{Denoiser}\\
$\mathbf{D}$ & Dicionário \\
$\mathbf{s}_D$ & Representação de $\bm{x}$ em $\mathbf{D}$ \\
$\overline{\mathbf{x}}$ & $\mathbf{x}$ na forma padrão\\
$\mathbf{P}$ & Precondicionador \\
$\mathbf{s}_P$ & Representação de $\bm{x}$ em $\mathbf{P}$ \\
$\mathbf{\Gamma}$ & Matriz de covariância \\
$a$ & Atlas anatômico: escalar \\
$\Omega_{\bm{\theta}}(\mathbf{x})$ & $\Omega(\cdot)$ parametrizado por $\bm{\theta}$\\
$\bm{\theta}$ & Parâmetros\\
$\mathbf{x}_t$ & Modelos de treinamento\\
$\hat{\mathbf{x}}_{\bm{\theta}}$ & $\mathbf{x}$ estimada com  $\bm{\theta}$ \\
\end{tabular}

\vfill\null
\columnbreak

\subsection*{Capítulo \ref{sec:integrated}}
\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
  $h_{\bm{\theta}}( \cdot)$ & Estrutura candidata \\
  $\bm{\theta}$ & Parâmetros de $h$\\
 ($\mathbf{x}_t^{i}, \mathbf{y}_t^{i}$) & i-ésimo par de treinamento\\
 $N$ &Número de amostras \\
 $\Sigma_{i=1}^N$ &Somatório de 1 até N  \\
 $\Psi_{\bm{\theta}}(\cdot)$  & Rede neural como uma função\\
 $\mathbf{b}$ & \textit{Bias} \\
$\mathbf{w}$ & Pesos da rede\\
$ f(\cdot)$    & Função de ativação\\
\end{tabular}

\subsection*{Capítulo \ref{sec:polysemy}}

\noindent\begin{tabular}{ m{1.3cm} m{6.2cm}}
$ \mathcal{R}_{\lambda}(\cdot)$ &Operador de regularização  \\
$M$ & Ivanov: subconjunto de $F$ \\
$c$,$c_0$  & Ivanov: limitante superior \\
$\bm{\delta}_1$, $\bm{\delta}_2$ & Limitantes supeiores\\
$\pi(\mathbf{x})$ & Densidade de probabilidade de $\mathbf{x}$ \\ 
$\pi(\mathbf{x})_{priori}$ & Densidade \textit{a priori} de $\mathbf{x}$ \\ 
$\pi(\mathbf{x})_{post}$ & Densidade \textit{a posteriori} de $\mathbf{x}$ \\ 
$\pi(\mathbf{x} | \mathbf{y})$ & Densidade condicional \\ 
$\beta$ & Coeficientes de regressão linear\\
$S$ & SRM: Subconjunto\\
$\subset$ & Pertencente a \\
\end{tabular}

\end{multicols}

\newpage
\renewcommand\contentsname{SUMÁRIO}
\setcounter{tocdepth}{3} 
\tableofcontents