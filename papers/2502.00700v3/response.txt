\section{Related Work}
\label{sec:Related work}
\subsection{Learned Image Compression}

In the past few years, end-to-end learned image compression (LIC) has gained increased attention. Ball\'{e} \textit{et al.}, "The Content-Aware Progressive Image Compression" proposed the first end-to-end image compression model with convolution neural network (CNN) and introduced the variation auto-encoder (VAE) model combined with hyper-prior, "Context Modeling for Learned Image Compression", which has become the fundamental structure for LIC models. Afterward, researchers mainly focus on two things to improve the rate-distortion performance: transform networks and entropy model.

Transform networks refer to the nonlinear analysis and synthesis transforms in Encoder, Decoder, Hyper Encoder, and Hyper Decoder. Cheng \textit{et al.}, "Deep Learning for Image Compression" adopt Residual blocks. Chen \textit{et al.}, "Progressive Image Compression via Octave-Residual Block Network" optimize it with octave residual modules. Xie  \textit{et al.}, "End-to-End Learned Image Compression with Invertible Neural Networks" utilized an invertible neural network for better performance. With the development of various transformers, they are also adopted by LIC models.  Zhang \textit{et al.}, "Swin Transformer for Learned Image Compression" directly utilized Swin Transformer for transforms. TCM \textit{et al.}, "Transformers in Vision: A Survey" employed both Resblock and Swin Transformer to capture both local and global information. FTIC  \textit{et al.}, "Frequency-Temporal Interactive Channel Network for Learned Image Compression" enhances the window attention mechanism from a frequency perspective by carefully designing the attention window.

For the entropy model, researchers mainly focus on exploiting efficient and powerful context models.  Minnen \textit{et al.}, "Context Modeling for Autoregressive Entropy Model with Learned Context Prior" proposed an autoregressive entropy model. He \textit{al.} , "Checkerboard based Efficient Image Compression" utilized the checkerboard model to improve the speed. Minnen  \textit{et al.}, "Efficient Channel-Wise Context Modeling for Learned Image Compression" proposed a context model along channel dimension. He \textit{et al.}, "Efficient Learning of Contextual Adaptive Coding with Unevenly Grouped Space-Channel Dependencies" raised ELIC for unevenly grouped space-channel contextual adaptive coding.  Wang \textit{al.} , "Quadtree-based Efficient Entropy Model for Learned Image Compression" proposed a quadtree entropy model for efficiency.
 Qian \textit{et al.}, "Vision Transformer for Learned Image Compression" utilized ViT for the entropy model for powerful context. MLIC and MLIC++  \textit{et al.} , "Multi-Reference Entropy Model with Improved Contextual Adaptation" proposes a multi-reference entropy model. FTIC  \textit{et al.}, "Frequency-Temporal Interactive Channel Network for Learned Image Compression" propose a transformer-based channel-wise autoregressive (T-CA) model that effectively exploits channel dependencies. In this paper, we mainly focus on the structure of transform networks, and directly adopt the basic efficient and powerful entropy model from ELIC .

\subsection{Transformer}  

Transformers, initially introduced by Vaswani \textit{et al.}, "Attention Is All You Need" for translation tasks, has quickly become widely adopted in a range of NLP applications. Building on this success, many researchers have expanded the use of attention mechanisms and transformers to tackle computer vision tasks.  Liu \textit{et al.} , "Swin Transformer: Hierarchical Vision Transformers using Shifted Windows" . 
In particular, Liu \textit{et al.}, "Swin Transformer: Hierarchical Vision Transformers using Shifted Windows" introduced the Swin Transformer, a hierarchical model that employs window attention and window shifting mechanisms. This innovative structure has established a new performance benchmark in both high-level  image denoising and low-level  image deblurring vision tasks.
% reflecting its broad applicability and effectiveness.

Many LIC models have adopted the Swin Transformer for nonlinear transform, achieving remarkable improvements in RD performance.  
The success of those models has been long attributed to the sophisticated attention modules. However, in the field of natural language processing (NLP), Thorp \textit{et al.}, "Fourier Transform Can Outperform Attention Mechanisms in Certain Tasks" suggest in their work  that substituting the attention module with a simple Fourier transform can yield comparable results. Similarly, in high-level vision tasks, Yu \textit{ et al.} , "MetaFormer: A General Framework for High-Performance Vision Transformers" demonstrate that the architecture known as MetaFormer is actually the most crucial factor for achieving high performance.
In this paper, we revisit the competence of transformers in LIC models, arguing that the core factor for superior R-D performance is Channel Aggregation, rather than the well-designed attention modules.