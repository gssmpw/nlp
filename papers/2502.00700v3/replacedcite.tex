\section{Related Work}
\label{sec:Related work}
\subsection{Learned Image Compression}





In the past few years, end-to-end learned image compression (LIC) has gained increased attention. Ball\'{e} \textit{et al.} ____ proposed the first end-to-end image compression model with convolution neural network (CNN) and introduced the variation auto-encoder (VAE) model combined with hyper-prior ____, which has become the fundamental structure for LIC models. Afterward, researchers mainly focus on two things to improve the rate-distortion performance: transform networks and entropy model ____.

Transform networks refer to the nonlinear analysis and synthesis transforms in Encoder, Decoder, Hyper Encoder, and Hyper Decoder. Cheng \textit{et al.} ____ adopt Residual blocks. Chen \textit{et al.} ____ optimize it with octave residual modules. Xie  \textit{et al.} ____ utilized an invertible neural network for better performance. With the development of various transformers, they are also adopted by LIC models.  ____ directly utilized Swin Transformer for transforms. TCM  ____ employed both Resblock and Swin Transformer to capture both local and global information. FTIC  ____ enhances the window attention mechanism from a frequency perspective by carefully designing the attention window.

For the entropy model, researchers mainly focus on exploiting efficient and powerful context models.  Minnen \textit{et al.} ____ proposed an autoregressive entropy model. He \textit{et al.}  ____ utilized the checkerboard model to improve the speed. Minnen  \textit{et al.} ____ proposed a context model along channel dimension. He \textit{et al.} ____ raised ELIC for unevenly grouped space-channel contextual adaptive coding.  ____ proposed a quadtree entropy model for efficiency.
 Qian \textit{et al.} ____ utilized ViT for the entropy model for powerful context. MLIC and MLIC++  ____ proposes a multi-reference entropy model. FTIC  ____ propose a transformer-based channel-wise autoregressive (T-CA) model that effectively exploits channel dependencies. In this paper, we mainly focus on the structure of transform networks, and directly adopt the basic efficient and powerful entropy model from ELIC ____.

\subsection{Transformer} 










Transformers, initially introduced by ____ for translation tasks, has quickly become widely adopted in a range of NLP applications. Building on this success, many researchers have expanded the use of attention mechanisms and transformers to tackle computer vision tasks. ____. 
In particular, Liu \textit{et al.} ____ introduced the Swin Transformer, a hierarchical model that employs window attention and window shifting mechanisms. This innovative structure has established a new performance benchmark in both high-level ____ and low-level  ____ vision tasks.
% reflecting its broad applicability and effectiveness.

Many LIC models have adopted the Swin Transformer for nonlinear transform, achieving remarkable improvements in RD performance.  
The success of those models has been long attributed to the sophisticated attention modules. However, in the field of natural language processing (NLP), Thorp \textit{et al.} suggest in their work ____ that substituting the attention module with a simple Fourier transform can yield comparable results. Similarly, in high-level vision tasks, Yu \textit{et al.}____ demonstrate that the architecture known as MetaFormer is actually the most crucial factor for achieving high performance.
In this paper, we revisit the competence of transformers in LIC models, arguing that the core factor for superior R-D performance is Channel Aggregation, rather than the well-designed attention modules.