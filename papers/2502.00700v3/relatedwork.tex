\section{Related Work}
\label{sec:Related work}
\subsection{Learned Image Compression}





In the past few years, end-to-end learned image compression (LIC) has gained increased attention. Ball\'{e} \textit{et al.} \cite{balle2016end} proposed the first end-to-end image compression model with convolution neural network (CNN) and introduced the variation auto-encoder (VAE) model combined with hyper-prior \cite{balle2018variational}, which has become the fundamental structure for LIC models. Afterward, researchers mainly focus on two things to improve the rate-distortion performance: transform networks and entropy model \cite{zafari2023frequency, mentzer2022vct, ma2019iwave, lu2022transformer, liu2020unified, gao2021neural, fu2023asymmetric, begaint2020compressai, han2024causal}.

Transform networks refer to the nonlinear analysis and synthesis transforms in Encoder, Decoder, Hyper Encoder, and Hyper Decoder. Cheng \textit{et al.} \cite{cheng2020learned} adopt Residual blocks. Chen \textit{et al.} \cite{chen2022two} optimize it with octave residual modules. Xie  \textit{et al.} \cite{xie2021enhanced} utilized an invertible neural network for better performance. With the development of various transformers, they are also adopted by LIC models.  \cite{zou2022devil, zhu2022transformer} directly utilized Swin Transformer for transforms. TCM  \cite{liu2023learned} employed both Resblock and Swin Transformer to capture both local and global information. FTIC  \cite{li2023frequency} enhances the window attention mechanism from a frequency perspective by carefully designing the attention window.

For the entropy model, researchers mainly focus on exploiting efficient and powerful context models.  Minnen \textit{et al.} \cite{minnen2018joint} proposed an autoregressive entropy model. He \textit{et al.}  \cite{he2021checkerboard} utilized the checkerboard model to improve the speed. Minnen  \textit{et al.} \cite{minnen2020channel} proposed a context model along channel dimension. He \textit{et al.} \cite{he2022elic} raised ELIC for unevenly grouped space-channel contextual adaptive coding.  \cite{li2022hybrid, li2023neural} proposed a quadtree entropy model for efficiency.
 Qian \textit{et al.} \cite{qian2022entroformer} utilized ViT for the entropy model for powerful context. MLIC and MLIC++  \cite{jiang2023mlic, jiang2023mlic++} proposes a multi-reference entropy model. FTIC  \cite{li2023frequency} propose a transformer-based channel-wise autoregressive (T-CA) model that effectively exploits channel dependencies. In this paper, we mainly focus on the structure of transform networks, and directly adopt the basic efficient and powerful entropy model from ELIC \cite{he2022elic}.

\subsection{Transformer} 










Transformers, initially introduced by \cite{vaswani2017attention} for translation tasks, has quickly become widely adopted in a range of NLP applications. Building on this success, many researchers have expanded the use of attention mechanisms and transformers to tackle computer vision tasks. \cite{dosovitskiy2020image, liu2021swin, touvron2021training, zamir2022restormer, chen2022simple}. 
In particular, Liu \textit{et al.} \cite{liu2021swin} introduced the Swin Transformer, a hierarchical model that employs window attention and window shifting mechanisms. This innovative structure has established a new performance benchmark in both high-level \cite{xia2022vision, dong2022cswin, liu2022swin} and low-level  \cite{liang2021swinir, zamir2022restormer, chen2022simple, zhang2022practical} vision tasks.
% reflecting its broad applicability and effectiveness.

Many LIC models have adopted the Swin Transformer for nonlinear transform, achieving remarkable improvements in RD performance.  
The success of those models has been long attributed to the sophisticated attention modules. However, in the field of natural language processing (NLP), Thorp \textit{et al.} suggest in their work \cite{lee2021fnet} that substituting the attention module with a simple Fourier transform can yield comparable results. Similarly, in high-level vision tasks, Yu \textit{et al.}~\cite{yu2022metaformer, yu2023metaformer} demonstrate that the architecture known as MetaFormer is actually the most crucial factor for achieving high performance.
In this paper, we revisit the competence of transformers in LIC models, arguing that the core factor for superior R-D performance is Channel Aggregation, rather than the well-designed attention modules.