
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}


\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage[symbol]{footmisc}
\usepackage{tablefootnote}
\usepackage{amsmath}
\usepackage{placeins}
\usepackage{cleveref}

\title{Yes, Q-learning Helps Offline In-Context RL}
% \title{Actual RL for Scalable In-Context RL}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\iclrfinalcopy
\author{
Denis Tarasov\thanks{Correspondence to: \href{mailto:tarasovd@ethz.ch}{tarasovd@ethz.ch}. Work done by \href{https://dunnolab.ai}{dunnolab.ai}.} \\ AIRI, ETH Zürich \And 
Alexander Nikulin \\ AIRI, MIPT \And 
Ilya Zisman \\ AIRI, Skoltech \And 
Albina Klepach \\ AIRI \And 
Andrei Polubarov \\ AIRI, Skoltech \And 
Nikita Lyubaykin \\ AIRI, Innopolis University \And 
Alexander Derevyagin \\ AIRI, HSE \And 
Igor Kiselev \\ Accenture \And
Vladislav Kurenkov \\ AIRI, Innopolis University
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% All prior studies on scalable offline In-Context Reinforcement Learning (ICRL) propose methods which use supervised training objectives which are known to have limitations in the context of offline RL. In this work, we explore the integration of RL approaches within a scalable offline ICRL framework.
Existing scalable offline In-Context Reinforcement Learning (ICRL) methods have predominantly relied on supervised training objectives, which are known for having limitations in offline RL settings. In this work, we investigate the integration of reinforcement learning (RL) objectives into a scalable offline ICRL framework. 
Through experiments across more than 150 datasets derived from GridWorld and MuJoCo environments, we demonstrate that optimizing RL objectives improves performance by approximately 30\% on average compared to the widely established Algorithm Distillation (AD) baseline across various dataset coverages, structures, expertise levels, and environmental complexities. Our results also reveal that offline RL-based methods, outperform online approahces, which are not specifically designed for offline scenarios. These findings underscore the importance of aligning the learning objectives with RL’s reward-maximization goal and demonstrates that offline RL is a promising direction for applying in ICRL settings.
\end{abstract}

\section{Introduction}
% The advancements in sequence generation models mostly exemplified by Transformer \citep{vaswani2017attention} lead to the appearance of language models which are able to perform novel task out of the training scope. With enough capacity and training data these models are able to complete tasks with a textual description and a few examples given as input without any updates \citep{brown2020language}, and it is called In-Context (IC) learning. This ability is a desirable property for solving meta Reinforcement Learning (RL) tasks \citep{beck2023survey}. In best-case scenario after pretraining on a one set of tasks we want our model to be able to infer a useful behavior when deployed into novel environments. Being able to pretrain model offline is another desirable property as letting the model to run in online environment might be costly or even dangerous (e.g. robotics, self-driving cars or healthcare). 
The advent of sequence generation models, particularly those based on the Transformer architecture \citep{vaswani2017attention}, has revolutionized many fields by enabling models to generalize beyond their training domain. In particular, large language models can perform novel tasks by processing a textual description and a few examples provided as input without any parameter updates, a phenomenon known as in-context (IC) learning \citep{brown2020language}. This capability is highly desirable for solving meta-reinforcement learning tasks \citep{beck2023survey}, where the goal is to produce a model capable of generalizing to unseen tasks. Recently appeared in-context RL \citep{moeini2025survey} aims to produce general meta-RL models with scalable architectures analogous to Large Language Models. However, training such models online is not feasible and may be unsafe. Offline pre-training increases the applicability of ICRL by eliminating potentially costly or dangerous online interactions, as seen in domains such as robotics, autonomous driving, and healthcare. However, current offline ICRL methods face critical limitations.

Established approaches such as Algorithm Distillation (AD) \citep{laskin2022context} and Decision-Pretrained Transformer (DPT) \citep{lee2024supervised}, along with their variants, have shown promise in offline ICRL. However, none of these methods explicitly optimize for the RL objective - maximizitaion of cumulative reward. This oversight poses significant challenges when tackling offline RL tasks, where leveraging RL to achieve optimal behavior is crucial \citep{kumar2022should}. While recent work \citep{grigsby2023amago, elawady2024relic} has explored scalable online ICRL, these methods rely on numerous heuristics for effective performance and remain untested in offline settings, which are inherently more challenging \citep{levine2020offline}. See \autoref{related} for related work.

\begin{figure*}%[!ht]
\centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/overall_discrete.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/overall_cont.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{Mean test NAUC scores across environments averaged over all constructed datasets.}
    \label{fig:overall_avg}
\end{figure*}

Our main goal is to investigate whether methods that optimize RL objective can achieve significantly better results in offline ICRL and whether this improvements are universal across various axis. In particular, we aim to address the following questions: 1) Does explicit optimization of the RL objective improve performance in offline ICRL?
2) How does the effectiveness of this optimization depend on the coverage and quality of offline datasets? 
3) Do we need specialized RL techniques from the offline RL family for effective offline ICRL?
4) How would algorithms behave if we do not have access to learning histories but rather a bunch of data?
5) Does RL better handle mixture of dynamics and out-of-distribution dynamics?
To answer these questions, we conduct an empirical study using more than 150 datasets derived from the widely used GridWorld and MuJoCo \citep{todorov2012mujoco} tasks. We compare several RL-based approaches with Algorithm Distillation, a strong and widely adopted supervised baseline, to evaluate the impact of explicitly optimizing for reward in offline ICRL. To our knowledge, this is the first study to explicitly optimize the RL objective in an offline ICRL setting using a scalable Transformer architecture.
% \section{Preliminaries}
% \subsection{Offline In-Context Reinforcement Learning}
% \textbf{Reinforcement Learning (RL).} 
% RL problem is typically formulated using a Partially Observable Markov Decision Process (POMDP), which is defined by the tuple: $(\mathcal{S}, \mathcal{A}, \mathcal{O}, P, R, \Omega, \gamma)$, where: $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $\mathcal{O}$ is the set of observations,  $P(s' \mid s, a)$ is the transition probability distribution defining the dynamics of the environment, $R(s, a)$ is the reward function, $\Omega(o \mid s')$ is the observation probability distribution, modeling the agent’s partial observability, $\gamma \in [0,1)$ is the discount factor, which determines the importance of future rewards.

% At each timestep, the environment is in some state $s \in \mathcal{S}$, but the agent does not observe it directly. Instead, it receives an observation $o \in \mathcal{O}$ according to the observation distribution $\Omega(o \mid s')$. Based on this observation, the agent selects an action $a \in \mathcal{A}$ according to its policy $\pi(a \mid o)$. The environment then transitions to a new state $s' \sim P(s' \mid s, a)$ and provides a reward $R(s, a)$. The objective of the agent is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward: $J(\pi) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid \pi \right]$, where $r_t = R(s_t, a_t)$.

% \textbf{Offline RL.} 
% Offline RL \citep{levine2020offline}, also known as batch RL, aims to train agents that maximize reward using a pre-collected dataset without any interaction with the environment. This setup is particularly useful in domains where data collection is expensive, dangerous, or otherwise impractical, such as healthcare, autonomous driving, and robotics.

% Formally, offline RL assumes access to a fixed dataset $\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}_{i=1}^{N}$ collected by an unknown behavior policy(-ies) $\pi_\beta$. The goal is to learn an optimal policy $\pi^*$ that maximizes the expected return despite the constraints imposed by the static dataset.

% \textbf{Meta RL.} 
% Meta RL \citep{beck2023survey} aims to train agents that can quickly adapt to new tasks by leveraging prior experience. Unlike standard RL, where agents learn a single policy optimized for a specific task, meta RL focuses on learning a policy that can generalize across a distribution of tasks. 

% Formally, meta RL assumes a distribution over tasks $\mathcal{G}_i \sim p(\mathcal{G})$, where each task $\mathcal{G}_i$ is associated with its own POMDP. The goal is to learn a meta-policy $\pi$ that, given a small amount of experience from a new task $\mathcal{G}_i$, can quickly adapt to maximize rewards in that task.

% \textbf{In-Context RL.} 
% In-Context RL (ICRL) is a recently emerging paradigm that addresses meta-RL challenges by developing agents capable of leveraging their interaction history $c_t$ to adapt to new tasks -- without performing any explicit model updates \citep{moeini2025survey}. This approach is inspired by the in-context learning abilities observed in large language models \citep{brown2020language}, where the model infers and adjusts its behavior based solely on the provided context.

% ICRL agents utilize sequences of past interactions to dynamically infer the underlying task structure and make decisions accordingly. In our work, we focus on the offline ICRL setting. Here, the goal is to train a scalable agent from a pre-collected dataset such that it can adapt to novel environmental instances solely based on its historical interaction context, without any further parameter updates during deployment.

% \subsection{Algorithm Distillation}
% Algorithm Distillation (AD) \citep{laskin2022context} is a scalable offline ICRL method that has become a strong baseline for subsequent works \citep{lee2024supervised, elawady2024relic}. AD aims to distill a policy improvement operator from a dataset
% \[
% \mathcal{D} = \Bigl\{ \bigl( \tau_1^{\mathcal{G}_i}, \dots, \tau_n^{\mathcal{G}_i} \bigr) \sim \mathcal{A}_{\mathcal{G}_i} \mid \mathcal{G}_i \in p(\mathcal{G}) \Bigr\}_{i=1}^{N},
% \]
% where \(\mathcal{A}_{\mathcal{G}_i}\) denotes an RL algorithm trained in environment \(\mathcal{G}_i\) and each trajectory \(\tau_j^{\mathcal{G}_i} = \bigl( o_1^j, a_1^j, r_1^j, \dots, o_T^j, a_T^j, r_T^j \bigr)\) is collected during this training. The ordered sequence of these trajectories is referred to as a learning history.

% The AD model \(M_\theta\) is an autoregressive Transformer \citep{vaswani2017attention} trained with a supervised maximum likelihood objective. It predicts the next action based on a segment of the learning history and the current observation \(o_k^{j+C}\). Formally, the prediction is given by:
% \[
% \hat{a}_k^{j+C} = M_\theta\bigl(o_{T-l}^j, a_{T-l}^j, r_{T-l}^j, \dots, o_T^j, a_T^j, r_T^j,\, o_1^{j+1}, a_1^{j+1}, r_1^{j+1}, \dots, o_{k-1}^{j+C}, a_{k-1}^{j+C}, r_{k-1}^{j+C},\, o_k^{j+C}\bigr).
% \]
% After pretraining with this procedure, the resulting \(M_\theta\) is capable of solving unseen tasks \(\mathcal{G}_i\) entirely in-context, without requiring any additional parameter updates.

\section{Preliminaries}
\subsection{Offline In-Context Reinforcement Learning}
Reinforcement Learning (RL) is commonly formulated as a Partially Observable Markov Decision Process (POMDP) defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{O}, P, R, \Omega, \gamma)$. At each timestep, an agent receives an observation $o \in \mathcal{O}$, selects an action $a \in \mathcal{A}$ according to its policy $\pi(a \mid o)$, and receives a reward $r = R(s, a)$. The goal is to learn an optimal policy $\pi^*$ that maximizes the expected cumulative discounted reward: $J(\pi) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid \pi \right]$, where $r_t = R(s_t, a_t)$. Offline RL \citep{levine2020offline} aims to learn such a policy solely from a fixed dataset $\mathcal{D} = \{(o_i, a_i, r_i, o'_i)\}_{i=1}^{N}$ without further interaction with the environment. In-Context RL (ICRL) \citep{moeini2025survey} aims to enable adaptation to new tasks purely through contextual learning, without explicit parameter updates. Offline ICRL specifically trains models on pre-collected data to infer and adapt to novel tasks at deployment.

\subsection{Algorithm Distillation}
Algorithm Distillation (AD) \citep{laskin2022context} is a scalable offline ICRL method that serves as a strong baseline for subsequent works \citep{lee2024supervised, elawady2024relic}. AD distills a policy improvement operator from a dataset
\[
\mathcal{D} = \Bigl\{ \bigl( \tau_1^{\mathcal{G}_i}, \dots, \tau_n^{\mathcal{G}_i} \bigr) \sim \mathcal{A}_{\mathcal{G}_i} \mid \mathcal{G}_i \in p(\mathcal{G}) \Bigr\}_{i=1}^{N},
\]
where $\mathcal{A}_{\mathcal{G}_i}$ is an RL algorithm trained in environment $\mathcal{G}_i$, and each trajectory $\tau_j^{\mathcal{G}_i} = \bigl( o_1^j, a_1^j, r_1^j, \dots, o_T^j, a_T^j, r_T^j \bigr)$ represents interactions collected during training. The ordered sequence of these trajectories is referred to as a learning history.

AD trains an autoregressive Transformer $M_\theta$ to predict the next action given a segment of learning history and the current observation $o_k^{j+C}$:
\[
\hat{a}_k^{j+C} = M_\theta\bigl(o_{T-l}^j, a_{T-l}^j, r_{T-l}^j, \dots, o_T^j, a_T^j, r_T^j, o_1^{j+1}, \dots, o_{k-1}^{j+C}, a_{k-1}^{j+C}, r_{k-1}^{j+C}, o_k^{j+C}\bigr).
\]
After pretraining, $M_\theta$ can solve unseen tasks in-context without requiring parameter updates.


\section{Methodology}

\subsection{RL Incorporation}
\begin{figure*}%[!ht]
\centering
    \begin{subfigure}[b]{0.8\textwidth}
        \centering
        \centerline{\includegraphics[width=1.0\columnwidth]{imgs/schema_v4.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{Overview of our approach. As the input, our approach takes a sequence of trajectories (without hard requirements on their structure) where each transition is represented with a tuple consisting of previous action, previous reward, previous episode's done flag, current episode timestep and other sequence elements marked by different timestep subscripts ($t$ and $T$) to indicate their potential origin from distinct trajectories. Then using the resulting context embedding $c_t$ to predict both value functions and the policy output $\pi$ (for continuous-action tasks). For continuous-action settings, the Q-heads accept an action value as input, whereas in discrete tasks they simultaneously predict values for all possible actions, following common practice. The V-head is employed only in IQL, while the $\pi$ head is used exclusively for continuous actions. Dashed arrows denote the absence of gradient flow.}
    \label{fig:schema}
\end{figure*}


In this study, we use Algorithm Distillation (AD) \citep{laskin2022context}, a transformer-based \citep{vaswani2017attention} architecture, as our baseline. AD’s objective is to predict the next action given the improving learning history as context, where each step is represented as a tuple (\textit{state}, \textit{previous action}, \textit{previous reward}). In our implementation these tuples are encoded into a single token through concatenation. 

We retain the same Transformer backbone as AD but introduce several modifications to incorporate RL objectives: inspired by \citet{grigsby2023amago}, input tuples are augmented with \textit{previous done} flags to indicate episode termination and current episode \textit{step}; the next-action prediction head is replaced with value-function heads trained using corresponding RL loss functions. For continuous problems, we also add the policy head. The illustration can be found in \autoref{fig:schema}.

We adopt three RL methods for discrete environments. Twin Deep Q Network (DQN) \citep{mnih2013playing}: A simple RL method without offline-specific components.
Conservative Q-Learning (CQL) \citep{kumar2020conservative}: A widely used offline RL approach that incorporates value-function pessimism. And Implicit Q-Learning (IQL) \citep{kostrikov2021offline}: A popular offline RL method based on implicit regularization, known for its strong performance across diverse tasks. Inspired by the adaptation of IQL for the NLP tasks with ILQL \citep{snell2022offline} we add the CQL term to the IQL loss in discrete environments. We also run tests with continuous environments where we adopt TD3 \citep{fujimoto2018addressing} as online baseline, it's minimalist TD3+BC \citep{fujimoto2021minimalist} offline modification and continuous IQL.
During inference, discrete approaches predict actions using the $\arg \max$ operator while continuous use deterministic policy output. We refer to all of the RL methods with IC- (In-Context) prefix, i.e. IC-DQN, IC-CQL, IC-IQL, IC-TD3 and IC-TD3+BC.

\subsection{Environments and Datasets}
For most of our experiments we utilize two environments used by \citet{laskin2022context}: Dark Room (DR) and Dark Key-to-Door (K2D). DR is a discrete Markov Decision Process (MDP), while K2D is a partially observable MDP (POMDP). Both environments involve a 2D grid where the agent can move up, down, left, right, or remain stationary, observing only its current position at each step. These are popular environments that allow us to scale our experiments under the limited computational budget for obtaining trustworthy conclusions. In \Cref{mixture-dynamics} we introduce modified DR environment for a separate set of experiments. We also run tests using popular continuous MuJoCo environments widely used in meta RL research \citep{rakelly2019efficient}: HalfCheetahVel (HCV), AntDir (ANT), HopperParams (HPP) and Walker2DParams (WLP).

In DR, the agent starts at the center of the grid and must navigate to an unknown target goal to receive a reward of 1. In K2D, the agent starts at a random location and must first find a "key" (reward: 1) and then reach a "door" (reward: 1), both of which have unknown locations. In both environments episodes terminate either upon task completion or after a fixed number of steps. In HCV agent must run with a fixed unknown velocity, in ANT agent has to navigate to the unknown point, and in HPP and WLP agent must move as fast as possible avoiding falling but in different instances of environments system parameters (e.g. gravity or masses) are randomized.

For DR we consider 9x9 version with episode length of 20 and 19x19 version with episode length of 100.  For K2D we test 9x9 version with 50 steps per episode and 13x13 version with 100 steps per episode. This allows us to test performance across different levels of complexity. MuJoCo environments are truncated after 200 steps.

For discrete environments, we collect training histories using the Q-learning \citep{watkins1992q} algorithm, varying the number of histories per target goal (1 or 5). For DR 9x9 we form datasets for 70, 40 and 20 train targets, for DR 19x19 we collect histories with 300, 150 and 75 train targets, and for both K2D versions we created datasets with 1000, 500 and 250 train goals. For continuous environments we collect learning histories from 100, 50 and 25 environments instances using Soft Actor-Critic (SAC) algorithm \citep{haarnoja2018soft}.
To analyze the impact of data quality, we partition the trajectory datasets into three expertise levels \texttt{early}, \texttt{mid} and \texttt{late} by dividing original datasets into three equal parts trajectory-wise.

We name datasets using the following convention:
\{\textit{environment name}\}[\{\textit{grid size}\}]-\{\textit{num training targets}\}-\{\textit{num histories per target}\}[-\{\textit{expertise level}\}]. Absence of the expertise level in the name indicates the full (complete) dataset. For example, "DR9-70-5" refers to a complete dataset from the 9x9 DR environment with 70 training targets and 5 histories per target. Additional dataset details are provided in \Cref{app:datasets}.

\subsection{Evaluation}
To assess the performance of trained policies, we roll out each policy over 100 successive episodes for all discrete environments and track performance after 25, 50, and 100 (1, 2, 4) episodes. For continuous environments we roll out over 4 episodes and track performance after 1, 2 and 4 episodes.\footnote{Note that the optimal meta policy should be able to solve all the considered discrete tasks within four episodes and continuous tasks within one or two episodes.} Additionally, we compute the Normalized Area Under the Curve (NAUC)\footnote{The AUC value is divided by the expert policy AUC.} of episode performance. We also report metrics from the rliable library \citep{agarwal2021deep} above the tracked ones for the reliability.

The NAUC provides a single numerical value for comparing agents, as it captures the progression of performance over episodes while being more robust to noise than fixed-episode evaluations. Tracking performance at fixed episodes helps identify convergence rates and potential degradation during rollouts. NAUC is used for selecting the best hyperparameters (details in \Cref{app:details}).

For the DR environments, we evaluate on all target goals that are excluded from the training set. For all other environments, we use a fixed set of 100 random test targets or configurations that are not part of the training data. To ensure robustness, we follow the evaluation protocol from \citet{tarasov2024revisiting}, using different random seeds for hyperparameter search and final evaluation.

\section{Experimental Results}
We begin this section by comparing the overall performance of the selected methods using discrete environments, demonstrating the suitability of the newly introduced NAUC metric for evaluation. Subsequently, we analyze the performance of these methods across critical offline RL dimensions, including data quality and coverage. Further, we investigate the impact of removing the assumption of access to learning histories, which may not always hold in real-world scenarios \citep{zisman2023emergence}. In addition, we test the ability to learn in a mixture of dynamics along with the handling of out-of-distribution (OOD) dynamics and run the experiment in a challenging XLand-Minigrid \citep{nikulin2023xland} environment. In the end, we show that benefits from using RL extend to continuous environments.
\subsection{Overall performance}

\begin{figure*}%[!ht]
\centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/quad_train.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/quad_test.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_train_auc.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
                \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{Top: algorithms performance across tracked metrics averaged over all discrete datasets. Bottom: rliable performance profiles of NAUC. Left: train targets. Right: test targets.}
    \label{fig:overall}
\end{figure*}
For this analysis, we utilized all available discrete datasets. The top graphs in \Cref{fig:overall} show the averaged metrics for both test and train targets. Across the Dark Room (DR) and Dark Key-to-Door (K2D) environments, we observe that the tested methods maintain stable performance throughout rollouts, confirming that NAUC is a reliable metric for comparative analysis.

The bottom plots in \Cref{fig:overall} display performance profiles based on NAUC. From these results, several key observations can be made. First, RL-based approaches consistently outperform Algorithm Distillation (AD) on average.
Second, while all RL methods show similar performance on train targets, there are notable differences on test targets. CQL achieves the best performance on test targets (with a 28.8\% average improvement compared to AD), IQL follows closely behind (23.7\% improvement) and DQN exhibits the weakest performance among the RL methods on average (16.6\% improvement). However, it is worth noting that we did not tune hyperparameters for DQN and reused parameters from CQL, so the DQN's performance has a potential for improvement.

Tabular results for each dataset are provided in \Cref{app:tables}, and additional rliable metrics are included in Appendix \ref{app:plots_overall}. These metrics, including the Interquartile Mean (IQM) and mean values for both NAUC and final scores, statistically validate the superior performance of CQL over other methods.

These findings support our core assumption that optimizing the RL objective is crucial for solving ICRL problems. Moreover, the performance gap between offline RL methods and the online-focused DQN highlights the advantages of offline RL algorithms in this setup. It is important to note that we have very limited hyperparameters tuning of RL approaches compared to AD tuning and potential gains might be even higher with equal tuning budgets (see \Cref{app:details}).
\FloatBarrier

\subsection{Various Coverage}
In this part of the analysis, we explore the impact of dataset coverage, a critical property in the offline RL setup \citep{schweighofer2021dataset}. Coverage is examined along two axes: the number of unique training targets and the number of learning histories per target. While multihistory coverage is essential for AD, it may not be feasible in real-world scenarios. To investigate these aspects, we use the complete datasets across all environments.

\begin{figure*}[ht]
            \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/coverage_DR9_auc_test.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/coverage_DR19_auc_test.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/coverage_K2D9_auc_test.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/coverage_K2D13_auc_test.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{NAUC score comparison between considered approaches for various dataset coverage in terms of number of train targets and histories per target. Averaged over 4 test random seeds. Confidence intervals depict std across seeds.}
    \label{fig:coverage}
\end{figure*}

The NAUC scores for each dataset are shown in \Cref{fig:coverage}. As expected, all methods perform better with increased target coverage and more histories per target. However, the results highlight notable differences in performance across methods. Offline RL approaches outperform AD across most configurations. The exception is the DR9 datasets, where AD slightly surpasses offline RL in scenarios with 20 and 40 targets when using only one history per target. DQN also outperforms AD on average and, surprisingly, shows superior performance over offline RL approaches on DR19 tasks. In the more complex K2D environments, RL-based methods are significantly more robust to the absence of multiple histories per target. For K2D, RL approaches achieve close performance with the five histories per target setup once a sufficient target coverage level is reached. In contrast, AD experiences a notable drop in performance without repeated targets, underscoring its reliance on repetitive learning histories.

The key takeaway is that RL-based approaches are more data-efficient than AD and demonstrate greater tolerance for limited target repetition in learning histories. This robustness makes RL methods more suitable for real-world applications, where complete coverage and extensive learning histories are often unattainable.

\subsection{Various Expertise}
\label{discrete-expertise}
Dataset expertise is another critical factor in offline RL \citep{schweighofer2021dataset}. In this part of the analysis, we evaluate the performance of different methods across discrete datasets of varying expertise levels. The "complete" datasets represent full learning histories, which were originally proposed for AD. The \texttt{mid} datasets include interpolation between low-quality and near-convergence trajectories, resembling truncated versions of the complete datasets. In contrast, \texttt{early} and \texttt{late} datasets reflect real-world scenarios: the former consists of low-quality data that is relatively easy to gather, while the latter comprises near-optimal examples of problem solutions. Detailed statistics for these datasets can be found in \Cref{app:datasets}.

\begin{figure*}[ht]
        \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_early.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_mid.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_late.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_full.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{rliable performance profiles of NAUC for various discrete datasets expertise. Top, from left to right: \texttt{early}, \texttt{mid}, \texttt{late} datasets. Bottom: complete learning histories.}
    \label{fig:expertise}
\end{figure*}
 
\Cref{fig:expertise} shows the test NAUC scores for the various dataset types. AD performs notably poorly on \texttt{early} datasets, failing to produce policies with NAUC scores higher than 0.4. In contrast, all RL approaches achieve significantly higher scores. DQN emerges as the best-performing method in this setup, likely because low-quality datasets require less regularization, allowing the agent to pursue higher rewards. CQL, which is implemented with cross-entropy loss in discrete cases, can be viewed as an interpolation between DQN and AD. Its performance is closer to AD when the regularization coefficient is high, which limits its potential on low-quality datasets. Reducing CQL's pessimism might yield better results than DQN, but we did not test it due to computational constraints.

As anticipated, the \texttt{mid} and complete datasets exhibit similar performance trends, with AD remaining the weakest approach and CQL leading, closely followed by IQL. Surprisingly, AD performs competitively on \texttt{late} datasets with high coverage, despite the lower data diversity in these datasets. In contrast, DQN's performance on \texttt{late} datasets is significantly weaker than the offline RL methods, further underscoring the importance of offline regularization. Additional rliable metrics and final performance scores in Appendix \ref{app:plots_expertise} statistically validate these observations.

In summary, the experiments highlight that RL-based methods outperform AD across datasets of varying expertise levels. However, the results also suggest that the hyperparameters of offline RL methods need to be carefully tuned to match the quality of the available data, an aspect not fully explored in this iteration of the study.

\subsection{No Learning Histories Structure}
\label{no-histories}
\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/order_DR19_auc_test.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/order_K2D13_auc_test.pdf}}
        \label{fig:}
    \end{subfigure}

    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_random.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_sample.pdf}}
        \label{fig:}
    \end{subfigure}
        \caption{NAUC test scores on DR19 and K2D13 datasets with different data structures. Top: performance splitted across environments and dataset coverages. Bottom: rliable performance profiles for random data order (left) and sorted samples (right).}
    \label{fig:no_stories}
\end{figure*}

Algorithm Distillation relies on the availability of progressing learning histories, with multiple behavior policies collecting data for each task. In practice, however, such structured data is rarely available. To address this, we conducted experiments to evaluate how all considered approaches perform when the inherent ordering of learning histories is absent. First, we tested the algorithms using a randomly shuffled dataset, which disrupts the sequential improvement that AD is designed to distill. To counteract this, we also investigated an approach to build some order: after randomly sampling trajectories, we sort them based on their discounted return values. Although this sorting method may be sensitive to the choice of discount factor, we found it to work effectively for some tasks.

For these experiments, we used complete datasets from the DR19 and K2D13 environments with one learning history per target, as this represents a more realistic scenario. 

As illustrated in \Cref{fig:no_stories}, on average, RL approaches outperform AD when the data is randomly ordered, with the sole exception of CQL on the DR19-300-1 dataset. Under random ordering, there is little difference among the RL methods on K2D13, while on DR19 DQN exhibits notable superiority -- a result that is consistent with observations on highly sub-optimal (\texttt{early}) datasets. When the unordered data is sorted by discounted return, offline RL methods consistently outperform both AD and DQN in K2D environment. In the DR19 environment, however, only CQL (which can be seen as an interpolation between AD and DQN) maintains its performance lead, while IQL shows diminished results. Surprisingly, on DR19, both DQN and IQL perform better with randomly ordered data than with sorted samples. We do not yet have a plausible explanation for this phenomenon or why it appears exclusively in the simpler DR19 environment.

In summary, CQL demonstrates the best performance across different environments and dataset coverages without learning histories access. Additional metrics presented in Appendix \ref{app:additional_plots_histories} further support this finding.

\subsection{Mixture of Dynamics}
\label{mixture-dynamics}
\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/grid_janus.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/janus_janus_test_full.pdf}}
        \label{fig:}
    \end{subfigure}
        \caption{Left: Visualization of the Janus test environment. Right: NAUC scores averaged over all Janus datasets, with metrics reported separately for targets located in the first and second zones of the Janus environment. The top values show performance when trained agents are deployed in the standard DR19 environment, while the bottom values reflect performance when deployed in the Janus grid.}
    \label{fig:janus_grid}
\end{figure*}
In this experiment, we investigate how algorithms perform when trained on environments with different dynamics and subsequently deployed into an environment featuring OOD dynamics. To this end, we introduce a modified version of the DR19 environment, called Janus\footnote{Ancient Roman two-faced god of duality.}. In the Janus setup, learning histories are independently collected from two distinct instances of DR19, each governed by a different dynamic function (for example, actions in the second instance may map to inverted directions). Consequently, the training dataset includes examples of behavior under both dynamics, yet no single history contains a mixture of these dynamics. After training the agent on this combined dataset, we deploy it into a grid where the first half exhibits one dynamic and the second half the other, as illustrated in the left graph of \Cref{fig:janus_grid}. The complete datasets are collected using the same configuration as for DR19, with the only modification being that for each learning history, the underlying environment dynamic is uniformly selected at random.

This experimental setup allows us to assess how effectively different approaches learn and generalize across multiple dynamics, as well as how they cope with an environment that blends these dynamics in a single deployment. Given the increased task complexity, we extended the number of rollout episodes to 200. The results, presented in the right plot of \Cref{fig:janus_grid}, indicate that when an agent is trained on both dynamics and deployed into an environment featuring only one of them, its performance remains comparable across dynamics. However, when tested in an environment that combines both dynamics, performance decreases for both, with a more pronounced drop in one of the dynamics. This discrepancy is likely influenced by the asymmetry of the test environment, where the central starting position falls within the second dynamic. Ideally, a robust agent should perform uniformly well under both dynamics.

Across all conditions, RL-based approaches continue to demonstrate superiority over AD. In isolated tests, IQL outperforms AD by approximately 50\%, while DQN and CQL achieve roughly twice the performance of AD. In the Janus environment, RL methods better preserve performance under one dynamic (as evidenced by the slopes of the performance lines) and exhibit slightly improved performance under the other dynamic. It is not surprising that offline RL counterparts do not provide benefits due to the fact that offline algorithms are supposed to avoid the OOD state-action pairs which are unavoidable in the Janus setup and considered offline approaches do not provide guarantees for this case. Tabular scores can be found in Appendix \ref{app:tab_janus}.

\subsection{XLand-Minigrid Evaluation}
\begin{table}[ht]
\centering
\caption{XLand-Minigrid trivial \texttt{tiny} dataset test tasks scores. Statistics are averaged over 4 random seeds.}
\label{table:xland}
\begin{tabular}{l|rrrr}
\toprule
\textbf{Metric} & \textbf{AD} & \textbf{IC-DQN} & \textbf{IC-CQL} & \textbf{IC-IQL}\\
\midrule
NAUC &  0.22 $\pm$ 0.03 & 0.42 $\pm$ 0.03 & 0.40 $\pm$ 0.04 & 0.46 $\pm$ 0.03\\
Last episode mean return & 0.21 $\pm$ 0.02 & 0.43 $\pm$ 0.05 & 0.39 $\pm$ 0.03 & 0.45 $\pm$ 0.05 \\
% Last episode median return & 0.0 $\pm$ 0.0 & 0.33 $\pm$ 0.24 & 0.09 $\pm$ 0.18 & 0.49 $\pm$ 0.22 \\
\bottomrule
\end{tabular}
% \vspace{6pt}
\label{tab:xland-result}
\end{table}

To further validate our approach in more challenging settings, we evaluate our methods on the XLand-Minigrid trivial environment \citep{nikulin2023xland} using datasets provided by \citet{nikulin2024xland}. In order to keep the experiments tractable, we reduce the dataset size by selecting only one learning history per rule set and retaining only the first third of transitions from each history. This subsampling results in a dataset that is just 1\% of the original size, which we refer to as the \texttt{tiny} dataset.

\Cref{tab:xland-result} presents the performance results on this dataset, reporting both the NAUC and the mean return from the last episode. The results show that RL-based approaches significantly outperform AD: NAUC and mean return scores for RL methods are approximately twice as high as those for AD. Among the RL approaches, DQN slightly outperforms CQL, while IQL achieves marginally better performance than DQN.

It is noteworthy that in the original work \citep{nikulin2024xland}, AD achieved a mean performance of approximately 0.4 using 100 times more data and three times more rollout episodes (500 episodes compared to 150 in our experiments). These findings clearly demonstrate that the benefits of explicitly optimizing RL objectives extend to more complex and data-sparse environments.

\subsection{Continuous State and Action Spaces}
\begin{figure*}[ht]
        \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_early_cont.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_mid_cont.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_late_cont.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_full_cont.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{rliable performance profiles of NAUC for various continuous datasets expertise. Top, from left to right: \texttt{early}, \texttt{mid}, \texttt{late} datasets. Bottom: complete learning histories.}
    \label{fig:continuous_expertise}
\end{figure*}

Thus far, our experiments have focused on discrete environments, which offer a controlled setting to analyze the behavior of RL-based ICRL methods. However, many real-world applications -- ranging from robotics and autonomous driving to control tasks -- operate in continuous state and action spaces. In this subsection, we extend our experimental analysis to continuous environments to determine how explicit RL objective optimization performs when faced with the additional challenges posed by infinite state and action spaces. This exploration aims to bridge the gap between our current discrete experiments and the demands of real-world applications, ultimately paving the way for broader adoption and further refinement of offline ICRL methods.

In \Cref{fig:continuous_expertise}, we present performance profiles for AD, the online RL method TD3 \citep{fujimoto2018addressing}, its lightweight offline RL counterpart TD3+BC \citep{fujimoto2021minimalist} and IQL. Consistent with our findings in \Cref{discrete-expertise}, the offline RL approaches (TD3+BC and IQL) significantly outperform AD across most setups, with AD matching performance only on \texttt{late} (near-expert behavior) datasets. IQL performs slightly worse than TD3+BC on average. Notably, a key difference emerges between continuous and discrete environments: the online RL method (TD3) demonstrates lower performance than AD in continuous domains, likely due to the more severe challenges posed by out-of-distribution states and actions. These results underscore that ICRL methods incorporating offline RL components not only achieve better performance but also highlight the critical importance of the offline component in continuous settings. See \Cref{app:additional_plots_metrics} and \Cref{app:tables} for more results and metrics.

% \FloatBarrier
\section{Conclusion and Future Work}
In this work, we have demonstrated that explicitly optimizing RL objectives is highly beneficial for offline ICRL. Our experiments reveal that incorporating RL optimization leads to improved performance across a variety of environments, dataset coverage levels, and dataset expertise and structure conditions. In particular, even under much smaller hyperparameters tunning budget offline RL approaches consistently outperform Algorithm Distillation and are usually more effective than online methods, highlighting the advantages of offline-specific regularizations and methodologies in many ICRL scenarios.

Future work should extend this study by evaluating more complex environments such as NetHack \citep{kuttler2020nethack, kurenkov2024katakomba} or more setups of XLand-MiniGrid \citep{nikulin2023xland, nikulin2024xland}. Additionally, it is essential to explore ICRL in settings with observations, e.g. Meta-World \citep{yu2020meta}, to further validate and generalize our findings. 

Our approach can be further enhanced by incorporating several modifications that have proven effective for Transformer-based RL solutions. For example, integrating adjustments from methods like AMAGO and ReLIC \citep{grigsby2023amago, elawady2024relic}, employing N-gram heads \citep{akyurek2024context, zisman2024n}, or adopting mixture-of-experts (MoE) architectures \citep{shazeer2017outrageously, obando2024mixtures} could all contribute to improved performance. Additionally, replacing the regression loss used for value functions with a classification objective has demonstrated promising results \citep{farebrother2024stop} but should be carefully integrated in offline setup \citep{tarasov2024value}. These potential enhancements underscore the flexibility of our framework and point to exciting avenues for future research.

It is also important to investigate usefulness of RL approaches in creating generalist models which are trained to operate in various environments which was recently done for Algorithm Distillation \citep{polubarov2025vintixactionmodelincontext} or generalization to completely new environments as it was done by \citet{raparthy2023generalization}. 

Another promising direction for future work is to investigate the application of offline In-Context RL methods in an offline-to-online setting \citep{nair2020awac, lee2022offline}, where model weights are updated during rollouts. While this approach does not offer benefits when using the supervised objectives, the explicit RL objectives we optimize have the potential to further improve performance.

Overall, our results underscore the importance of aligning learning objectives with the intrinsic goals of Reinforcement Learning, setting the stage for more robust and efficient offline ICRL methods.
\FloatBarrier

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix

% \section{Additional Experiments}
% \label{app:additional_experiments}

\section{Related Work}
\label{related}
\subsection{Offline Reinforcement Learning}
Offline RL aims to train agents that maximize reward using pre-collected datasets without interacting with the environment. This setup introduces unique challenges, particularly in handling out-of-distribution (OOD) state-action pairs \citep{levine2020offline}. Over the years, this field has witnessed rapid development, with various methods proposed to address these challenges \citep{kumar2020conservative, an2021uncertainty, kostrikov2021offline, fujimoto2021minimalist}. In our study we test widely adopted offline RL baselines for offline ICRL setting in order to demonstrate benefits they bring as reward maximization algorithms. For discrete environments we used Conservative Q-learning (CQL) \citep{kumar2020conservative} and Implicit Q-learning (IQL) \citep{kostrikov2021offline}. Based on findings from \citet{tarasov2024corl}, for continuous environments we used IQL and simple yet effective \citep{tarasov2024revisiting} TD3+BC \citep{fujimoto2021minimalist} approach.

A prominent direction in offline RL involves modeling trajectories with Transformers through supervised learning, as first introduced by Decision Transformer (DT) \citep{chen2021decision}. However, subsequent studies \citep{yamagata2023q, hu2024q, zhuang2024reinformer} demonstrated that supervised approaches, which lack explicit reward maximization, often fail with low-quality datasets or datasets which do not contain problem solving trajectories. They struggle to "stitch" suboptimal trajectories into optimal policies -- a limitation that can be addressed by methods that directly optimize RL objectives. Our intuition tells that in the context of offline ICRL similar issues might arise when reward is not maximized which is confirmed by our experiments.

\subsection{Scalable In-Context Reinforcement Learning}
Algorithm Distillation (AD) \citep{laskin2022context} marked a significant step towards scalable In-Context RL (ICRL) by leveraging Transformer architectures to learn an "improvement" operator. It does so by distilling information from the training histories of single-task agents across various environments. AD assumes access to complete training histories, which may not always be available. In this work we demonstrate that RL-based approaches can levarage datasets more efficiently (especially datasets with low-quality demonstrations) and are able to handle unstructured data better.

Decision-Pretrained Transformer (DPT) \citep{lee2024supervised} introduced another approach, focusing on predicting optimal actions from historical data and a given state. However, this method assumes access to an oracle for optimal action sampling, which is often impractical. RL-based methods that we test in this work do not require access to the oracle.

Neither AD, DPT, nor their follow-up modifications \citep{sinii2023context, schmied2024retrieval, dai2024context, huang2024context, sondistilling, zisman2024n} optimize RL objectives during offline training. This omission can result in suboptimal policies, as these methods essentially adapt supervised learning techniques like DT to the offline ICRL setting, without addressing the fundamental reward maximization goal of RL.

Recent works such as AMAGO \citep{grigsby2023amago} and ReLIC \citep{elawady2024relic} have explored scalable In-Context RL by incorporating off-policy RL techniques. These methods outperform AD and DT in online RL setups but have yet to be tested in offline environments. Offline RL presents distinct challenges—such as the inability to interact with the environment—that make direct application of online approaches less effective \citep{fujimoto2019off, levine2020offline}. This gap underscores the need for offline-specific methods that explicitly optimize RL objectives.  Moreover, AMAGO and ReLIC rely on many implementation details and in this work we demonstrate that solid performance can be achieved without complex modifications.
\section{Additional Experimental Details}
\label{app:details}
All experiments were conducted using NVIDIA H100 GPUs.

\subsection{Implementation Details}
Our implementation of Algorithm Distillation (AD) is based on the Decision Transformer (DT) codebase from \citet{tarasov2024corl}.  In our version, we remove the return-to-go input and merge state, action, and reward into a single token. This tokenization strategy reduces the overall Transformer sequence length, thereby decreasing both computation time and memory usage. When solving XLand-Minigrid we use similar implementation from \citet{nikulin2024xland}.

When adapting AD for Reinforcement Learning, we add value function and policy heads (for continuous problems) on top of the original AD backbone. The value heads heads are implemented as two-layer multilayer perceptrons (MLPs) with a Leaky ReLU activation function between layers. Policy heads are three-layer MLPs and analagous to AMAGO \citep{grigsby2023amago} we do not pass gradients to the Transformer backbone from these heads to improve training stability. There are also standard for RL target value function heads. To provide richer input information, we merge the additional \textit{previous done} flag and step number with the (\textit{state}, \textit{previous action}, \textit{previous reward}) token. In continuous environments Q value heads get the current \textit{action} as additional input. For continuous IQL we also had to add LayerNorm \citep{ba2016layer} into the heads in order to stabilize learning process.

\subsection{Hyperparameters Choice}
For hyperparameter tuning, we use the NAUC metric to select the best model configuration. The tuning is performed on the largest complete dataset, and the best hyperparameters are then applied across other datasets, even though this may result in suboptimal performance, especialy for offline RL approaches.

For AD, we tuned parameters that strongly influence performance, including attention dropout, embedding dropout, and residual dropout (each varied over values $\{0.1, 0.3, 0.5\}$), label smoothing for discrete environments (tested with values $\{0.1, 0.3\}$) and Transformer sequence length (tested over $\{100, 200\}$). This tuning resulted in 54 candidate hyperparameter sets. The best values, along with other general hyperparameter settings, are documented in \Cref{app:hyperparams}.

Due to computational constraints, the hyperparameters identified for AD were reused for the RL approaches. In the case of CQL, we tuned the discount factor $\gamma$ over values $\{0.8, 0.9, 0.95\}$ for XLand-Minigrid and $\{0.7, 0.8, 0.9\}$ for other environments, and adjusted the CQL weight to be within ${0.1, 0.3, 0.5}$ for DR environments, within ${0.3, 0.5, 1.0}$ for Dark K2D environments, within ${0.01, 0.05, 0.1}$ for Janus, and within ${0.01, 0.1, 0.5}$ for XLand-Minigrid. For DQN, we simply adopted the $\gamma$ values found for CQL without additional tuning. Discrete IQL was tuned over a discount factor $\gamma$ with the same configuration as for CQL, an IQL parameter $\tau$ over $\{0.5, 0.7, 0.9\}$, and used a CQL weight of either $0.0$ or the best value found for CQL. For TD3+BC we tuned discount factor over $\{0.9, 0.95, 0.99\}$ and BC weight over $\{0.1, 0.3, 1.0\}$, for TD3 we reuse TD3+BC best discount factor and set BC weight to zero. For continuous IQL, we ran discount factor search over the same values as for TD3+BC, IQL $\tau$ over $\{0.5, 0.7, 0.9\}$ and IQL $\beta$ over $\{1, 3, 10\}$. This tuning yielded 9 candidate configurations for CQL and TD3+BC, 18 for discrete IQL, substantially fewer than those evaluated for AD. For continuous IQL, it resulted in 27 candidates, which is twice less than AD search space.
 
Each approach was trained over a fixed number of epochs to account for varying dataset sizes: 30 epochs for DR9, HPP and WLP, 15 epochs for HCV, 10 epochs for DR19, K2D9 and ANT, 6 epochs for K2D13. We track metrics after each epoch and report the mean value across multiple seeds for the epoch according to the best NAUC value. Notably, we observed that AD exhibits greater instability during training compared to the RL approaches, meaning that our design choices in the experimental protocol tend to favor AD, yet its performance remains inferior.

Preliminary experiments indicated that an important hyperparameter controlling AD subsampling is best set to 4 for DR and continous environments and 8 for K2D. When evaluating on incomplete datasets, we reduced these values to 1 for DR and continuous environments and to 2 for K2D to maintain a consistent number of trajectories. In \Cref{no-histories} the subsample parameter was set to 1, as it does not have any motivation there and would just discard a large number of trajectories.

% See \Cref{app:hyperparams} for all hyperparamters values.
For a complete list of hyperparameter values, please refer to \Cref{app:hyperparams}.

\section{Datasets details}
\label{app:datasets}
In this section, we describe the data collection process and provide detailed statistics for each of the obtained datasets. We do not provide data for Janus datasets as they are very similar to DR19.

\subsection{Data Collection}
\textbf{Discrete Environments.} To construct the complete discrete datasets, we employed a tabular Q-learning algorithm \citep{watkins1992q} with a linearly decayed 
$\epsilon$-greedy exploration strategy. For the K2D environments, which are originally formulated as POMDPs and require memory to solve, we doubled the state space by mapping each grid position to two distinct states: one corresponding to the scenario where the key has not been collected and the other where it has. This transformation effectively converts K2D into a fully observable MDP. The hyperparameter values used for Q-learning across all dataset collections are provided in \Cref{tab:q_learning_params}.
\input{tables/hyperparameters/Q_DR}

\textbf{Continous Environments.} For collecting the learning histories in continuous environments we used SAC implementation from Clean RL \citep{huang2022cleanrl}. 
We kept most of the SAC hyperparameters default and we present the varied subset in \Cref{tab:sac_params}.
\input{tables/hyperparameters/SAC}

Datasets representing various levels of expertise are derived by segmenting the complete learning histories into three equal parts on a trajectory-wise basis.


\FloatBarrier
% \newpage
\subsection{Learning Curves}
The learning curves presented in the following graphs illustrate the average returns for all datasets as a function of the episode number within the learning history. By concatenating the \texttt{early}, \texttt{mid}, and \texttt{late} segments, we obtain the complete dataset curves. We do not provide curves for the HPP and WLP due to the variable amount of episodes for each learning history.
\begin{figure*}[h]
\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR9_20_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR9_40_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR9_70_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR9_20_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR9_40_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR9_70_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Q-learning DR9 learning curves.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR19_75_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR19_150_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR19_300_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR19_75_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR19_150_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_DR19_300_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Q-learning DR19 learning curves.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D9_250_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D9_500_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D9_1000_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D9_250_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D9_500_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D9_1000_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Q-learning K2D9 learning curves.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D13_250_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D13_500_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D13_1000_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D13_250_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D13_500_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_K2D13_1000_5.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Q-learning DR13 learning curves.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_HCV_25_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_HCV_50_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_HCV_100_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{SAC HCV learning curves.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
\centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_ANT_25_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_ANT_50_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/lcs/plots/lc_ANT_100_1.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{SAC ANT learning curves.}
    \label{fig:}
\end{figure*}
\FloatBarrier

\subsection{Datasets Statistics}
\input{tables/datasets/Trivial}
\input{tables/datasets/DR9}
\input{tables/datasets/DR19}
\input{tables/datasets/K2D9}
\input{tables/datasets/K2D13}
\input{tables/datasets/HCV}
\input{tables/datasets/ANT}
\input{tables/datasets/HPP}
\input{tables/datasets/WLP}
\FloatBarrier

\section{Hyperparameters}
\label{app:hyperparams}
\input{tables/hyperparameters/AD}
\input{tables/hyperparameters/AD_pd}
\input{tables/hyperparameters/CQL_pd}
\input{tables/hyperparameters/IQL_pd}
\input{tables/hyperparameters/TD3_pd}
\FloatBarrier

\section{Additional Plots and Metrics}
\label{app:additional_plots_metrics}
\subsection{Overall Performance}
\label{app:plots_overall}
\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_train_auc.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC computed with rliable approach across all available discrete datasets. Top: train targets. Bottom: test targets.}
    \label{fig:rliable_all_auc}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_train_mean_return.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{rliable performance profiles of the 100th episode scores across all available discrete datasets. Left: train targets. Right: test targets.}
    \label{fig:rliable_pp_all_score}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_train_mean_return.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of 100th episode scores computed with rliable approach across all available discrete datasets. Top: train targets. Bottom: test targets.}
    \label{fig:rliable_all_score_auc}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_auc_cont.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_cont.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{rliable metrics of the NAUC across all available continuous datasets. Top: performance profiles. Bottom: median, IQM and mean.}
    \label{fig:}
\end{figure*}

\FloatBarrier
\subsection{Various Expertise Performance}
\label{app:plots_expertise}

\begin{figure*}[ht]
        \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return_early.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return_mid.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return_late.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return_full.pdf}}
        % \caption{}
        \label{fig:}
    \end{subfigure}
    \caption{rliable performance profiles of 100th episode scores for various discrete datasets expertise. Top, from left to right: \texttt{early}, \texttt{mid}, \texttt{late} datasets. Bottom: complete learning histories.}
    \label{fig:}
\end{figure*}


\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_early.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return_early.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC and 100th episode scores computed with rliable approach across
\texttt{early} discrete datasets for test targets. Top: NAUC. Bottom: 100th episode performance.}
    \label{fig:rliable_all_score}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_mid.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return_mid.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC and 100th episode scores computed with rliable approach across
\texttt{mid} discrete datasets for test targets. Top: NAUC. Bottom: 100th episode performance.}
    \label{fig:rliable_all_score_me}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_late.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return_late.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC and 100th episode scores computed with rliable approach across
\texttt{late} discrete datasets for test targets. Top: NAUC. Bottom: 100th episode performance.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_full.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return_full.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC and 100th episode scores computed with rliable approach across
complete discrete datasets for test targets. Top: NAUC. Bottom: 100th episode performance.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_early_cont.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_mid_cont.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_late_cont.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_full_cont.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC computed with rliable approach across
continous datasets for test instances.}
    \label{fig:}
\end{figure*}

\FloatBarrier
\subsection{No Learning Histories Structure}
\label{app:additional_plots_histories}

\begin{figure*}[h]
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return_random.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/perf_profiles_test_mean_return_sample.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{rliable performance profiles of the 100th episode scores on test targets across all DR19 and K2D13 complete datasets without learning histories. Left: random trajectories order. Right: trajectories sorted within random subset.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_random.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_auc_sample.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of NAUC for test targets computed with rliable approach across all DR19 and K2D13 complete datasets without learning histories. Top: random trajectories order. Bottom: trajectories sorted within random subset.}
    \label{fig:}
\end{figure*}

\begin{figure*}[h]
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return_random.pdf}}
        \label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \centerline{\includegraphics[width=\columnwidth]{imgs/metrics_test_mean_return_sample.pdf}}
        \label{fig:}
    \end{subfigure}
    \caption{Median, IQM and mean of the 100th episode scores for test targets computed with rliable approach across all DR19 and K2D13 complete datasets without learning histories. Top: random trajectories order. Bottom: trajectories sorted within random subset.}
    \label{fig:}
\end{figure*}
\FloatBarrier
% \newpage
\section{Tabular Results}
\label{app:tables}
In this section, we provide numerical values of NAUC and final performance (after 100 episodes) for all methods.  Means and stds are computed over 4 random seeds.

\subsection{Discrete Train NAUC}
\input{tables/train_auc_early}
\input{tables/train_auc_mid}
\input{tables/train_auc_late}
\input{tables/train_auc_full}
\FloatBarrier

\subsection{Discrete Test NAUC}
\input{tables/test_auc_early}
\input{tables/test_auc_mid}
\input{tables/test_auc_late}
\input{tables/test_auc_full}
\FloatBarrier

\subsection{Discrete Train Final Scores}
\input{tables/train_mean_return_early}
\input{tables/train_mean_return_mid}
\input{tables/train_mean_return_late}
\input{tables/train_mean_return_full}
\FloatBarrier

\subsection{Discrete Test Final Scores}
\input{tables/test_mean_return_early}
\input{tables/test_mean_return_mid}
\input{tables/test_mean_return_late}
\input{tables/test_mean_return_full}
\FloatBarrier

\subsection{Continuous Test NAUC}
\input{tables/test_auc_early_cont_eval}
\input{tables/test_auc_mid_cont_eval}
\input{tables/test_auc_late_cont_eval}
\input{tables/test_auc_full_cont_eval}
\FloatBarrier

\subsection{Continuous Test 0-shot}
\input{tables/test_mean_return_quarter_early_cont_eval}
\input{tables/test_mean_return_quarter_mid_cont_eval}
\input{tables/test_mean_return_quarter_late_cont_eval}
\input{tables/test_mean_return_quarter_full_cont_eval}
\FloatBarrier


\subsection{Continuous Test Final Scores}
\input{tables/test_mean_return_early_cont_eval}
\input{tables/test_mean_return_mid_cont_eval}
\input{tables/test_mean_return_late_cont_eval}
\input{tables/test_mean_return_full_cont_eval}
\FloatBarrier

\subsection{Janus Test NAUC Tables}
\label{app:tab_janus}
\input{tables/test_auc_full_janus_default}
\input{tables/test_auc_full_janus_inverted}
\input{tables/test_auc_janus_full_janus_default}
\input{tables/test_auc_janus_full_janus_inverted}
\input{tables/test_auc_full_janus}


\FloatBarrier
% \input{tables/test_auc_full_janus_}
% \input{tables/test_auc_full_order_}
 % \input{tables/test_orders}
\end{document}
