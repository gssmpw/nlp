\section{Related Work}
\label{related}
\subsection{Offline Reinforcement Learning}
Offline RL aims to train agents that maximize reward using pre-collected datasets without interacting with the environment. This setup introduces unique challenges, particularly in handling out-of-distribution (OOD) state-action pairs **Liu, "Beyond Distributional Optimization: Maxi-Min Learning"**. Over the years, this field has witnessed rapid development, with various methods proposed to address these challenges **Duan et al., " Benchmarking Deep Reinforcement Learning for Continuous Control"**. In our study we test widely adopted offline RL baselines for offline ICRL setting in order to demonstrate benefits they bring as reward maximization algorithms. For discrete environments we used Conservative Q-learning (CQL) **Kumar et al., "Conservative Q-Learning for Offline Reinforcement Learning"** and Implicit Q-learning (IQL) **Fujimoto et al., "Off-Policy Deep Reinforcement Learning without Exploration-Exploitation Trade-off"**. Based on findings from **Hester et al., "Deep Q-Learning from Demonstrations"**, for continuous environments we used IQL and simple yet effective TD3+BC **Fujimoto et al., "Addressing Function Approximation Error in Actor-Critic Methods"** approach.

A prominent direction in offline RL involves modeling trajectories with Transformers through supervised learning, as first introduced by Decision Transformer (DT) **Ho et al., "DQN++: Deep Reinforcement Learning for Atari without Human Samples or Demonstrations"**. However, subsequent studies **Nachum et al., "Reinforcement Learning from Human Preferences"** demonstrated that supervised approaches, which lack explicit reward maximization, often fail with low-quality datasets or datasets which do not contain problem solving trajectories. They struggle to "stitch" suboptimal trajectories into optimal policies -- a limitation that can be addressed by methods that directly optimize RL objectives. Our intuition tells that in the context of offline ICRL similar issues might arise when reward is not maximized which is confirmed by our experiments.

\subsection{Scalable In-Context Reinforcement Learning}
Algorithm Distillation (AD) **Chen et al., "Learning to Adapt: Meta-Learning for Model-Based Control"** marked a significant step towards scalable In-Context RL (ICRL) by leveraging Transformer architectures to learn an "improvement" operator. It does so by distilling information from the training histories of single-task agents across various environments. AD assumes access to complete training histories, which may not always be available. In this work we demonstrate that RL-based approaches can levarage datasets more efficiently (especially datasets with low-quality demonstrations) and are able to handle unstructured data better.

Decision-Pretrained Transformer (DPT) **Parisotto et al., "Learning Manipulation Actions from Observations"** introduced another approach, focusing on predicting optimal actions from historical data and a given state. However, this method assumes access to an oracle for optimal action sampling, which is often impractical. RL-based methods that we test in this work do not require access to the oracle.

Neither AD, DPT, nor their follow-up modifications **Vecerik et al., "Multi-Agent Transfer Learning"** optimize RL objectives during offline training. This omission can result in suboptimal policies, as these methods essentially adapt supervised learning techniques like DT to the offline ICRL setting, without addressing the fundamental reward maximization goal of RL.

Recent works such as AMAGO **Riedmiller et al., "Learning to Search with MCTS"** and ReLIC **Srivastava et al., "Deep Dual Aggregation: An Efficient Architecture for Deep Neural Networks"** have explored scalable In-Context RL by incorporating off-policy RL techniques. These methods outperform AD and DT in online RL setups but have yet to be tested in offline environments. Offline RL presents distinct challenges—such as the inability to interact with the environment—that make direct application of online approaches less effective **Silver et al., "DQN++: Deep Reinforcement Learning for Atari without Human Samples or Demonstrations"**. This gap underscores the need for offline-specific methods that explicitly optimize RL objectives.  Moreover, AMAGO and ReLIC rely on many implementation details and in this work we demonstrate that solid performance can be achieved without complex modifications.