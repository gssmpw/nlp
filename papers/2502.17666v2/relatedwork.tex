\section{Related Work}
\label{related}
\subsection{Offline Reinforcement Learning}
Offline RL aims to train agents that maximize reward using pre-collected datasets without interacting with the environment. This setup introduces unique challenges, particularly in handling out-of-distribution (OOD) state-action pairs \citep{levine2020offline}. Over the years, this field has witnessed rapid development, with various methods proposed to address these challenges \citep{kumar2020conservative, an2021uncertainty, kostrikov2021offline, fujimoto2021minimalist}. In our study we test widely adopted offline RL baselines for offline ICRL setting in order to demonstrate benefits they bring as reward maximization algorithms. For discrete environments we used Conservative Q-learning (CQL) \citep{kumar2020conservative} and Implicit Q-learning (IQL) \citep{kostrikov2021offline}. Based on findings from \citet{tarasov2024corl}, for continuous environments we used IQL and simple yet effective \citep{tarasov2024revisiting} TD3+BC \citep{fujimoto2021minimalist} approach.

A prominent direction in offline RL involves modeling trajectories with Transformers through supervised learning, as first introduced by Decision Transformer (DT) \citep{chen2021decision}. However, subsequent studies \citep{yamagata2023q, hu2024q, zhuang2024reinformer} demonstrated that supervised approaches, which lack explicit reward maximization, often fail with low-quality datasets or datasets which do not contain problem solving trajectories. They struggle to "stitch" suboptimal trajectories into optimal policies -- a limitation that can be addressed by methods that directly optimize RL objectives. Our intuition tells that in the context of offline ICRL similar issues might arise when reward is not maximized which is confirmed by our experiments.

\subsection{Scalable In-Context Reinforcement Learning}
Algorithm Distillation (AD) \citep{laskin2022context} marked a significant step towards scalable In-Context RL (ICRL) by leveraging Transformer architectures to learn an "improvement" operator. It does so by distilling information from the training histories of single-task agents across various environments. AD assumes access to complete training histories, which may not always be available. In this work we demonstrate that RL-based approaches can levarage datasets more efficiently (especially datasets with low-quality demonstrations) and are able to handle unstructured data better.

Decision-Pretrained Transformer (DPT) \citep{lee2024supervised} introduced another approach, focusing on predicting optimal actions from historical data and a given state. However, this method assumes access to an oracle for optimal action sampling, which is often impractical. RL-based methods that we test in this work do not require access to the oracle.

Neither AD, DPT, nor their follow-up modifications \citep{sinii2023context, schmied2024retrieval, dai2024context, huang2024context, sondistilling, zisman2024n} optimize RL objectives during offline training. This omission can result in suboptimal policies, as these methods essentially adapt supervised learning techniques like DT to the offline ICRL setting, without addressing the fundamental reward maximization goal of RL.

Recent works such as AMAGO \citep{grigsby2023amago} and ReLIC \citep{elawady2024relic} have explored scalable In-Context RL by incorporating off-policy RL techniques. These methods outperform AD and DT in online RL setups but have yet to be tested in offline environments. Offline RL presents distinct challenges—such as the inability to interact with the environment—that make direct application of online approaches less effective \citep{fujimoto2019off, levine2020offline}. This gap underscores the need for offline-specific methods that explicitly optimize RL objectives.  Moreover, AMAGO and ReLIC rely on many implementation details and in this work we demonstrate that solid performance can be achieved without complex modifications.