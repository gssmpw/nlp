\section{Related Work}
\paragraph{CoT prompting} CoT prompting____ has become a pivotal technique for enhancing reasoning capabilities in LLMs by introducing intermediate reasoning steps. Automated approaches like Auto-CoT____, Tree-of-Thoughts____ and Self-play Mutual Reasoning____ explore multiple reasoning paths to expand the search space and improve task accuracy. These methods focus on increasing the reasoning length or expanding the reasoning horizon to handle complex tasks. 
Recent studies have underscored the importance of reasoning granularity and formats in enhancing LLM performance. For instance, ____ identified that longer reasoning steps improve task success for complex problems, while overly concise steps can reduce effectiveness. Tailored reasoning formats____ have demonstrated substantial improvements across tasks. However, these reasoning optimization strategies often comes with significant computational costs____, raising concerns about the trade-off between accuracy and efficiency. 

\paragraph{Knowledge distillation} While direct prompting enables LLMs to perform complex reasoning through CoT, SLMs struggle due to limited capacity____. Knowledge distillation (KD) provides an effective framework for transferring the reasoning capabilities of teachers to SLMs____. A simple yet effective approach is using a teacher-student paradigm, which employs teacher-generated CoT steps to guide SLMs, addressing their limitations and enhancing reasoning-intensive task performance____. 
Despite these advances, a systematic exploration of how to balance reasoning granularity, format, and teaching strategies remains lacking. Addressing these gaps is crucial for optimizing CoT distillation and enabling efficient reasoning in SLMs.