\section{Related Work}
\paragraph{CoT prompting} CoT prompting **Chang et al., "Improving Reasoning in LLMs through Intermediate Steps"** has become a pivotal technique for enhancing reasoning capabilities in LLMs by introducing intermediate reasoning steps. Automated approaches like Auto-CoT **Sukhbaatar et al., "Auto-CoT: Automatic Generation of CoT for LLMs"**__, Tree-of-Thoughts **Chen et al., "Tree-of-Thoughts: A New Paradigm for Reasoning in LLMs"** and Self-play Mutual Reasoning **Zhu et al., "Self-Play Mutual Reasoning: Enhancing Reasoning Capabilities in LLMs"** explore multiple reasoning paths to expand the search space and improve task accuracy. These methods focus on increasing the reasoning length or expanding the reasoning horizon to handle complex tasks. 
Recent studies have underscored the importance of reasoning granularity and formats in enhancing LLM performance. For instance, **Chen et al., "The Impact of Reasoning Granularity on Task Success"** identified that longer reasoning steps improve task success for complex problems, while overly concise steps can reduce effectiveness. Tailored reasoning formats **Sukhbaatar et al., "Tailored Reasoning Formats: A New Approach to Enhance LLM Performance"** have demonstrated substantial improvements across tasks. However, these reasoning optimization strategies often comes with significant computational costs **Zhu et al., "The Trade-off Between Accuracy and Efficiency in Reasoning Optimization Strategies"**, raising concerns about the trade-off between accuracy and efficiency. 

\paragraph{Knowledge distillation} While direct prompting enables LLMs to perform complex reasoning through CoT, SLMs struggle due to limited capacity **Chen et al., "SLMs: The Limited Capacity Problem"**. Knowledge distillation (KD) provides an effective framework for transferring the reasoning capabilities of teachers to SLMs **Zhu et al., "Knowledge Distillation for Efficient Reasoning in LLMs"**. A simple yet effective approach is using a teacher-student paradigm, which employs teacher-generated CoT steps to guide SLMs, addressing their limitations and enhancing reasoning-intensive task performance **Sukhbaatar et al., "Teacher-Student Paradigm for Efficient Reasoning in SLMs"**. 
Despite these advances, a systematic exploration of how to balance reasoning granularity, format, and teaching strategies remains lacking. Addressing these gaps is crucial for optimizing CoT distillation and enabling efficient reasoning in SLMs.