\section{Related Work}
\paragraph{CoT prompting} CoT prompting~\cite{wei2023chainofthoughtpromptingelicitsreasoning} has become a pivotal technique for enhancing reasoning capabilities in LLMs by introducing intermediate reasoning steps. Automated approaches like Auto-CoT~\cite{zhang2023automatic}, Tree-of-Thoughts~\cite{yao2023tree} and Self-play Mutual Reasoning~\cite{qi2024mutualreasoningmakessmaller} explore multiple reasoning paths to expand the search space and improve task accuracy. These methods focus on increasing the reasoning length or expanding the reasoning horizon to handle complex tasks. 
Recent studies have underscored the importance of reasoning granularity and formats in enhancing LLM performance. For instance, \citet{jin-etal-2024-impact} identified that longer reasoning steps improve task success for complex problems, while overly concise steps can reduce effectiveness. Tailored reasoning formats\cite{DBLP:conf/iclr/KhotTFF0CS23, zhou2023leasttomost, deng2024rephraserespondletlarge, xu2024faithfullogicalreasoningsymbolic} have demonstrated substantial improvements across tasks. However, these reasoning optimization strategies often comes with significant computational costs~\cite{nayab2024concisethoughtsimpactoutput}, raising concerns about the trade-off between accuracy and efficiency. 

\paragraph{Knowledge distillation} While direct prompting enables LLMs to perform complex reasoning through CoT, SLMs struggle due to limited capacity~\cite{stolfo-etal-2023-causal}. Knowledge distillation (KD) provides an effective framework for transferring the reasoning capabilities of teachers to SLMs~\cite{xu2024surveyknowledgedistillationlarge}. A simple yet effective approach is using a teacher-student paradigm, which employs teacher-generated CoT steps to guide SLMs, addressing their limitations and enhancing reasoning-intensive task performance~\cite{magister-etal-2023-teaching, ho-etal-2023-large, shridhar-etal-2023-distilling}. 
Despite these advances, a systematic exploration of how to balance reasoning granularity, format, and teaching strategies remains lacking. Addressing these gaps is crucial for optimizing CoT distillation and enabling efficient reasoning in SLMs.