% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage{acl}

% Standard package includes
\usepackage{lipsum}
\usepackage{times}
\usepackage{latexsym}
\usepackage{soul}
\usepackage{url}
%\usepackage[hidelinks]{hyperref}
%\usepackage[small]{caption}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{forest}
\usepackage[switch]{lineno}
\usepackage{pifont}
\usepackage{tabularx}     % 自动分配列宽
\usepackage{array}        % 若需自定义列格式，可用此包
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{enumitem}
\hypersetup{    
    urlcolor=darkblue,
}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

\newcommand{\henry}[1]{\textcolor{red}{\bf\small [#1 --henry]}}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\definecolor{cite_color}{HTML}{114083}

\definecolor{url_color}{RGB}{153, 102, 0}

\definecolor{myMintGreen}{HTML}{CEEBD6}

\definecolor{myMintpink}{HTML}{FDE7E9}


\def\eg{\textit{e.g.,} }
\def\ie{\textit{i.e.,} }


\title{Multi-Agent Autonomous Driving Systems with Large Language Models: \\ A Survey of Recent Advances}



% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


\author{Yaozu Wu\textsuperscript{1,$^*$}, Dongyuan Li\textsuperscript{1,$^*$}, Yankai Chen\textsuperscript{2,3,$^\dag$}, Renhe Jiang\textsuperscript{1,$^\dag$}, \\ \textbf{Henry Peng Zou\textsuperscript{3}}, \textbf{Liancheng Fang\textsuperscript{3}}, \textbf{Zhen Wang}\textsuperscript{1}, \textbf{Philip S. Yu\textsuperscript{3}} 
\\
\\
\textsuperscript{1}The University of Tokyo, 
\textsuperscript{2}Cornell University,
\textsuperscript{3}University of Illinois Chicago
\\
 \small{ 
 {\{yaozuwu279,zhenwangr\}@gmail.com, \{lidy,jiangrh\}@csis.u-tokyo.ac.jp, yankaichen@acm.org, \{pzou3,lfang87,psyu\}@uic.edu
 }
 }
}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\renewcommand{\thefootnote}{}\footnote{$^*$ Equal Contribution. $^\dag$ Corresponding Author.}

\begin{abstract}
\textit{Autonomous Driving Systems (ADSs)} are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety.
Large Language Models (LLMs), known for their exceptional planning and reasoning capabilities, have been integrated into ADSs to assist with driving decision-making.
However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands.
To address these issues, recent advancements in \textit{LLM-based multi-agent ADSs} have focused on improving inter-agent communication and cooperation.
This paper provides a frontier survey of LLM-based multi-agent ADSs.
We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based approaches based on different agent interaction modes.
We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans.
Finally, we summarize key applications, datasets, and challenges in this field to support future research\footnote{{We provide an open-source library for future study via the following link: \url{https://anonymous.4open.science/r/LLM-based_Multi-agent_ADS-3A5C/README.md}}}.
\end{abstract}


\section{Introduction}
\textit{Autonomous driving systems (ADSs)} are redefining driving behaviors, reshaping global transportation networks, and driving a technological revolution~\cite{yurtsever2020survey}.
Traditional ADSs primarily rely on data-driven approaches (as detailed in Appendix~\ref{app:Data-driven}), often focusing on system development while overlooking dynamic interactions with the environment.
To enhance engagement with diverse and complex driving scenarios, agentic roles have been incorporated into ADSs~\cite{durante2024agent} 
using methods such as reinforcement learning~\cite{zhang2024multi} and active learning~\cite{lu2024activead}.
Despite notable progress, these methods struggle with ``long-tail" scenarios, where rare but critical driving situations—such as sudden obstacles—pose significant challenges to model performance.
Furthermore, their ``black-box" nature limits interpretability, making their decisions difficult to trust.

LLM-based single-agent ADSs help overcome the limitations of data-driven methods~\cite{wang2024survey}. 
{Pre-trained on vast, multi-domain datasets, LLMs excel in knowledge transfer and generalization~\cite{achiam2023gpt}, enabling strong performance in traffic scenarios under zero-shot settings, thus addressing the long-tail issue~\cite{yang2023llm4drive}.} 
{Moreover, techniques such as Reinforcement Learning from Human Feedback (RLHF) and Chain-of-Thought (CoT)~\cite{zhao2023survey}, enhance language-based interaction and logical reasoning, allowing LLMs to make human-like, real-time decisions while providing interpretable and trustworthy feedback across various driving conditions.} 
For instance, Drive-Like-a-Human~\cite{fu2024drive} builds a closed-loop system comprising environment, agent, memory, and expert modules.
The agent interacts with the environment, reflects on expert feedback, and ultimately accumulates experience.
DiLu~\cite{wen2023dilu} replaces human experts with a reflection module and integrates an LLM-based reasoning engine to enable continuous decision-making. 
Agent-Driver~\cite{mao2024a} designs a tool library to collect environmental data and uses LLMs' cognitive memory and reasoning to improve planning. 

\begin{figure}[t]
\centering
     \includegraphics[width=
     \columnwidth]{figs/disadvantage.png}
    \caption{Limitations of LLM-based single-agent ADSs. At an intersection without traffic lights, an accident has occurred ahead, causing Veh1 to be stuck. Due to \textbf{limited perception}, Veh1 is unable to assess the situation and cannot proceed. 
    Veh2 intends to go straight, and Veh3 wants to turn left.
    However, due to \textbf{insufficient collaboration}, they are also unable to navigate the intersection efficiently. 
    Furthermore, due to \textbf{high computing demands}, the lightweight agent on Veh1 struggles to handle the complex driving scenario and has to rely on a more powerful cloud-based agent for assistance.}
    \label{fig:disadvantage}
\end{figure}

\begin{figure*}[t]
\centering
     \includegraphics[width=0.95\linewidth]{figs/MAS.pdf}
    \caption{Overview of LLM-based (a) single- and (b) multi-agent ADSs, with key terms and differences highlighted.
    % . We highlight core words for multi-agent.
    }
    \label{fig:MAS}
\end{figure*}
However, as shown in Figure~\ref{fig:disadvantage}, researchers have identified three critical limitations of LLM-based single-agent ADSs in complex traffic environments: 
\ding{182}~\textbf{Limited Perception: } 
LLMs can only respond to sensor inputs and lack predictive and generalization capabilities. As a result, LLM-based single-agent ADSs cannot complement incomplete sensor information and thus miss critical information in driving scenarios, such as pedestrians or vehicles hidden in complex intersection environments~\cite{hu2024collaborative}.
\ding{183}~\textbf{Insufficient Collaboration: } A single LLM-based agent cannot coordinate with other vehicles or infrastructure, leading to suboptimal performance in scenarios requiring multi-agent interactions, such as lane merging or navigating roundabouts~\cite{malik2021collaborative}. 
\ding{184}~\textbf{High Computational Demands: } With billions of parameters in LLMs, these methods demand substantial independent computational resources, making real-time deployment challenging, particularly in resource-limited in-vehicle systems~\cite{cui2024personalizedautonomousdrivinglarge}. 

{To address these limitations, LLM-based multi-agent ADSs enable distinct agents to communicate and collaborate, enhancing safety and performance. 
{First}, LLMs enhance {contextual awareness} by allowing agents to share data, extend their perceptual range, and enhance the detection of occluded objects in complex environments~\cite{hu2024collaborative}. 
{Second}, real-time coordination between LLM-based agents mitigates {insufficient collaboration}, enabling joint decision-making in scenarios such as lane merging and roundabout navigation, ultimately leading to safer and more efficient driving operations~\cite{malik2021collaborative}. 
{Third}, LLMs optimize {computational efficiency} by distributing tasks among agents, reducing individual workloads, and enabling real-time processing in resource-limited systems~\cite{cui2024personalizedautonomousdrivinglarge}.}

As LLM capabilities continue to advance, they are playing an increasingly significant role in ADS as intelligent driving assistants.
Several reviews have focused on two primary aspects: 
\textit{\textbf{i)}} the integration of LLMs into data-driven methods~\cite{yang2023llm4drive,li2023towards} and \textit{\textbf{ii)}} the applications of specific LLM types, such as vision-based~\cite{zhou2024vision} and multimodal-based~\cite{fourati2024xlm,cui2024survey} models in ADSs.
However, no comprehensive survey has systematically examined the emerging field of LLM-based multi-agent ADSs.
This gap motivates us to provide a thorough review that consolidates existing knowledge and offers insights to guide future research and the development of advanced ADSs.

In this study, we present a comprehensive survey of LLM-based multi-agent systems. Specifically, Section~\ref{sec:MAS} introduces the core concepts of LLM-based multi-agent ADSs, including \textit{agent environments and profiles}, \textit{inter-agent interaction mechanisms}, and \textit{agent-human interactions}. 
Section~\ref{sec:agent} provides a structured review of the state-of-the-art in multi-agent ADS, categorizing existing studies into three key interaction types: \textit{multi-vehicle interaction}, \textit{vehicle-infrastructure interaction}, and \textit{vehicle-assistant interaction}.
As agent capabilities continue to grow, human-vehicle co-driving is becoming the dominant autonomous driving paradigm, with human involvement playing an increasingly vital role.
Humans collaborate with agents by providing guidance or supervising their behavior. Therefore, we consider humans as special virtual agents and examine human-agent interactions in Section~\ref{sec:human-vehicle}.
Section~\ref{sec:application} explores various applications, while Section~\ref{sec:dataset} compiles a comprehensive collection of public datasets and open-source resources.
Section~\ref{sec:opportunities} discusses existing challenges and future research directions and Section~\ref{sec:conclusion} concludes the study.

\section{LLM-based Agents for ADS}
\label{sec:MAS}
\subsection{LLM-based Single-agent ADS}
Achieving human-level driving is an ultimate goal of ADS. As shown in Figure~\ref{fig:MAS}(a), the LLM-based single agent retrieves past driving experiences from the memory, integrates them with real-time environmental information for reasoning, and makes driving decisions. Additionally, the driving agent reflects on its decision and updates its memory accordingly, ensuring safe and efficient driving actions. 
However, the complex and dynamic nature of real-world driving scenarios, where interactions with other vehicles significantly impact decision-making, suggests that neglecting these interactions can lead to suboptimal or unsafe driving outcomes.


\subsection{LLM-based Multi-agent ADS}
With interactions among multiple agents, LLM-based multi-agent ADS leverages collective intelligence and specialized skills, with each agent playing a distinct role, communicating and collaborating within the system. This enhances the efficiency and safety of autonomous driving. 
Below, we introduce the LLM-based multi-agent ADS, as shown in Figure~\ref{fig:MAS}(b), and provide a detailed analysis of its three key modules: {Agent Environment and Profile}, {LLM-based Multi-Agent Interaction}, and {LLM-based Agent-Human Interaction}. 

\subsubsection{Agent Environment and Profile}
Similar to the single-agent architecture in Figure~\ref{fig:MAS}(a), multi-agent systems first obtain relevant information from their environments, enabling them to make informed decisions and take appropriate actions.
The environmental conditions define the settings and necessary context for agents in LLM-based multi-agent ADS to operate effectively.
Generally, there are two environment types, \ie {physical environment} and {simulation environment}.

\begin{itemize}[leftmargin=*]
\item  \textbf{\textit{Physical environment}}. It represents the real-world setting where driver agents gather information using various sensors, such as cameras and LiDAR, and interact with other traffic participants.
However, due to the high cost of vehicles and strict regulations on public roads, collecting large amounts of data in real world is impractical. 
\item \textbf{\textit{Simulation environment}}.  As a viable alternative, the simulation environment provides a simulated setting constructed by humans.
It can accurately model specific conditions without incurring the high costs and complexities associated with real-world data collection, allowing agents to freely test actions and strategies across a variety of scenarios~\cite{dosovitskiy2017carla}.
\end{itemize}

In LLM-based multi-agent systems, each agent is assigned distinct roles with specific functions through {profiles}, enabling them to collaborate on complex driving tasks or simulate intricate traffic scenarios.
These profiles are crucial in defining the functionality of the agent, its interaction with the environment, and its collaboration with other agents.
Existing work~\cite{li2024survey} generates agent profiles using three types of methods: {Pre-defined}, {Model-generated}, and {Data-derived}. 
\begin{itemize}[leftmargin=*]
\item \textbf{\textit{Pre-defined methods}}. 
In these cases, system designers explicitly define agent profiles based on prior knowledge and the analysis of complex scenarios~\cite{chen2024edge}.
Each agent has unique attributes and behavior patterns that can be adjusted based on the scenario. 
In driving environments, the objectives of ADS require the collaboration of vehicle agents, infrastructure agents, and drivers. In particular, 
\ding{182}~Vehicle agents denote various types of autonomous vehicles, traveling according to preset routes and traffic rules, while communicating and collaborating with other vehicles and driver agents.
\ding{183}~Infrastructure agents, \eg traffic lights, road condition monitors, and parking facilities, provide real-time traffic information and instructions, influencing the behavior of driver and vehicle agents.

\item \textbf{\textit{Model-generated methods}}. These approaches create agent profiles using advanced LLMs based on the interaction context and the goals that need to be accomplished~\cite{zhou2024algpt}.

\item \textbf{\textit{Data-derived Profile}}. They design agent profiles based on pre-existing datasets~\cite{guo2024large}. 

\end{itemize}


\subsubsection{LLM-based Multi-Agent Interaction}
In LLM-based multi-agent ADS, effective 
information exchange and action coordination between agents are essential to improve collective intelligence and solve complex traffic scenarios. 
Agent interactions are influenced by both the interaction mode and the underlying interaction structure.

\begin{itemize}[leftmargin=*]
\item \noindent \textit{\textbf{The interaction mode}} of LLM-based multi-agent ADS can be classified as: \textit{cooperative}, \textit{competitive}, and \textit{debate} mode. 
\ding{182}~In cooperative mode, agents work together to achieve shared objectives by exchanging information~\cite{DBLP:conf/naacl/ChenZH24,jin2024surrealdriver}.
\ding{183}~In competitive mode, agents strive to accomplish their individual goals and compete with others~\cite{yao2024comal}.
\ding{184}~The Debate mode enables agents to debate with each other, propose their own solutions, criticize the solutions of other agents, and collaboratively identify optimal strategies~\cite{DBLP:conf/emnlp/Liang0JW00Y0T24}.

\item \textit{\textbf{The interaction structure}} delineates the architecture of communication networks within LLM-based multi-agent ADS, including \textit{centralized}, \textit{decentralized}, \textit{hierarchical}, and \textit{shared message pool} structures, as shown in Figure~\ref{fig:structure}.
\begin{figure}[t]
\centering
     \includegraphics[width=\columnwidth]{figs/interaction_m-s.pdf}
    \caption{Different interaction modes and interaction structures.}
    \label{fig:structure}
\end{figure}
Specifically, \ding{182}~the centralized interaction structures defines a central agent or a group of central agents to manage interactions among all agents~\cite{zhou2024algpt}.
\ding{183}~The decentralized interaction structure allows for direct communication between agents, with all agents being equal to each other~\cite{hu2024agentscomerge}.
\ding{184}~Hierarchical structures focus on interactions within a layer or with adjacent layers~\cite{DBLP:conf/coling/OhmerDB22}.
\ding{185}~The shared memory interaction structure maintains a shared message pool, allowing agents to send and extract the necessary information~\cite{jiang2024koma}.
We provide a more detailed introduction to LLM-based multi-agent ADSs based on their interaction structures and modes in Section~\ref{sec:agent}. 
\end{itemize}

\subsubsection{LLM-based Agent-Human Interaction}

Recent studies have shown that human-machine co-driving systems leverage LLMs to improve agent-human interactions, enabling autonomous vehicles to communicate and collaborate seamlessly with human drivers through natural language~\cite{DBLP:conf/emnlp/FengCQLC0W24}.
This capability allows vehicles to better understand and respond to human intent, provide context-aware responses, enhance driving safety and comfort, and offer personalized recommendations based on driver preferences.
Furthermore, humans play a crucial role in guiding and supervising agent behavior, enhancing the agents' capabilities while ensuring safety and compliance with legal standards. 
We explore the role of humans as special virtual agents in LLM-based multi-agent ADS and examine the intricate dynamics of agent-human interactions in Section~\ref{sec:human-vehicle}.

\section{LLM-based Multi-Agent Interaction}
\label{sec:agent}

\begin{figure*}[t]
\centering
\begin{forest}
for tree={
    font=\footnotesize, 
    grow=east, 
    draw,
    rounded corners,
    text width=6em, 
    anchor=center,
    align=center, 
    text centered,
    parent anchor=east,
    child anchor=west,
    edge path={
        \noexpand\path[\forestoption{edge}, line width=0.5pt]
        (!u.parent anchor) -- ++(5pt,0) |- (.child anchor)\forestoption{edge label};
    },
    s sep=1mm, 
    l sep=3mm, 
}
[
\rotatebox{90}{LLM-based Multi-Agent ADS} , fill=blue!20, font=\bfseries, text width=0.7em, anchor=center, align=center, 
    [
    Applications, fill=myMintpink, font=\scriptsize\bfseries, text width=5em
        [
        Collaborative \\ Assistance-Tools, fill=myMintpink, font=\scriptsize, text width=5.5em
            [{ChatSim~\cite{wei2024editable}, HumanSim~\cite{zhou2024humansim}, EC-Drive~\cite{chen2024edge}, \\ CAV-LLM-Driving-Assistant~\cite{tang2024test}, LLM-Edge-Device-Driving~\cite{huang2024efficient}, \\ ALGPT~\cite{zhou2024algpt}}, fill=myMintpink, font=\scriptsize, text width=25.8em]
        ]
        [
        Collaborative \\ Decision-Making, fill=myMintpink, font=\scriptsize, text width=5.5em
            [{LanguageMPC~\cite{sha2023languagempc}, CoMAL~\cite{yao2024comal}, AgentsCoDriver~\cite{hu2024agentscodriver}, \\ AgentsCoMerge~\cite{hu2024agentscomerge}, CoDrivingLLM~\cite{fang2024towards}, KoMA~\cite{jiang2024koma}, \\ SurrealDriver~\cite{jin2024surrealdriver}}, fill=myMintpink, font=\scriptsize, text width=25.8em]
        ]
        [
        Collaborative \\ Perception, fill=myMintpink, font=\scriptsize, text width=5.5em
            [{Complement-Vehicle's-FOV~\cite{dona2024tapping}, V-HOI MLCR~\cite{zhang2024enhancing}}, fill=myMintpink, font=\scriptsize, text width=25.8em]
        ]
    ]
    [
    Human-Agent \\ Interaction, fill=lime!5, font=\scriptsize\bfseries, text width=5em
        [
        Partnership \\ Paradigm, fill=lime!5, font=\scriptsize, text width=5.5em
            [{Drive-as-You-Speak~\cite{cui2024drive}, Reason-and-React~\cite{cui2024receive}, \\ DriVLMe~\cite{huang2024drivlme}, AccidentGPT~\cite{wang2024accidentgpt}, \\ ConnectGPT~\cite{tong2024connectgpt}}, fill=lime!5, font=\scriptsize, text width=25.8em
            ]
        ]    
        [
        Instructor \\ Paradigm, fill=lime!5, font=\scriptsize, text width=5.5em
        [{Co-Pilot~\cite{wang2023chatgpt}, PPE~\cite{ma2024learning}}, , fill=lime!5, font=\scriptsize, text width=25.8em
            ]
        ]
    ]
    [
    Multi-Agent \\ Interaction, fill=blue!5, font=\scriptsize\bfseries, text width=5em
        [
        Vehicle-Assistant \\ Interaction, fill=blue!5, font=\scriptsize, text width=5.5em
            [{ChatSim~\cite{wei2024editable}, ALGPT~\cite{zhou2024algpt}, AD-H~\cite{zhang2024ad}, \\ SurrealDriver~\cite{jin2024surrealdriver}, LDPD~\cite{liu2024language}, V-HOI MLCR~\cite{zhang2024enhancing}}, fill=blue!5, font=\scriptsize, text width=25.8em]
        ]
        [
        Vehicle-Infrastructure \\ Interaction, fill=blue!5, font=\scriptsize, text width=5.5em
            [{CAV-LLM-Driving-Assistant~\cite{tang2024test}, EC-Drive~\cite{chen2024edge}}, fill=blue!5, font=\scriptsize, text width=25.8em]
        ]
        [
        Multi-Vehicle \\ Interaction, fill=blue!5, font=\scriptsize, text width=5.5em
            [{LanguageMPC~\cite{sha2023languagempc}, AgentsCoDriver~\cite{hu2024agentscodriver}, KoMA~\cite{jiang2024koma}, \\AgentsCoMerge~\cite{hu2024agentscomerge}, CoDrivingLLM~\cite{fang2024towards},CoMAL~\cite{yao2024comal}, \\ Complement-Vehicle's-FOV~\cite{dona2024tapping}}, fill=blue!5, font=\scriptsize, text width=25.8em]
        ]
    ]
]
\end{forest}
\caption{A taxonomy of LLM-based Multi-Agent Autonomous Driving Systems.}
\label{fig:taxonomy}
\end{figure*}

Mutual interaction is central to multi-agent ADSs, enabling systems to solve complex problems beyond the capabilities of a single agent. 
Through information exchange and coordinated decision-making, multiple agents effectively complete shared tasks and achieve overarching objectives~\cite{li2024survey}. 
This section reviews recent studies on multi-agent ADSs, emphasizing interactions among vehicles, infrastructures, and assisted agents in driving scenarios. 
As shown in Figure~\ref{fig:taxonomy}, we categorize existing methods into three interaction types: \textit{multi-vehicle interaction}, \textit{vehicle-infrastructure interaction}, and \textit{vehicle-assistant interaction}.


\subsection{Multi-Vehicle Interaction}
Multi-vehicle interactions involve multiple autonomous vehicles powered by LLMs exchanging real-time information, such as locations, speeds, sensor data, and intended trajectories.
By sharing partial observations of the environment or negotiating maneuvers, multiple vehicles overcome the inherent limitations of single-agent ADS, such as restricted perception and lack of collaboration. 
Typically, these interactions operate in a cooperative mode.
LanguageMPC~\cite{sha2023languagempc} employs a centralized structure, where a central agent acts as the ``brain'' of the fleet, providing coordination and control commands to each vehicle agent.
In contrast, other decentralized approaches~\cite{fang2024towards,dona2024tapping} treat all agents equally, allowing direct communication between multiple agents.
For instance, AgentsCoDriver~\cite{hu2024agentscodriver} designs a communication module that generates messages for inter-agent communication when the agent deems it necessary.
AgentsCoMerge~\cite{hu2024agentscomerge} and CoDrivingLLM~\cite{fang2024towards} incorporate agent communication into the reasoning process, facilitating intention sharing and negotiation before decision-making.
Additionally, KoMA~\cite{jiang2024koma} and CoMAL~\cite{yao2024comal} build a shared memory pool, allowing agents to send and retrieve the necessary information to facilitate interaction between agents.

\subsection{Vehicle-Infrastructure Interaction}
The interaction between vehicles and external agents, such as traffic lights, roadside sensors, and LLM-powered control centers, not only helps autonomous vehicles make more intelligent decisions but also alleviates on-board computing requirements.
This enables LLM-based multi-agent ADSs to operate effectively in real-world environments.
EC-Drive~\cite{chen2024edge} proposes an Edge-Cloud collaboration framework with a hierarchical interaction structure.
The edge agent processes real-time sensor data and makes preliminary decisions under normal conditions.
When anomalies are detected or the edge agent generates a low-confidence prediction, the system flags these instances and uploads them to the cloud agent equipped with LLMs. 
The cloud agent then performs detailed reasoning to generate optimized decisions and combines them with the output of the edge agent to update the driving plan.
% Similarly to the architecture of EC-Drive
Following a similar architecture, \citet{tang2024test} uses agents deployed on remote clouds or network edges to assist connected driving agents in handling complex driving decisions.

\subsection{Vehicle-Assistant Interaction}
Beyond the interactions between the primary agents in driving scenarios, 
additional interactions among assisted agents play a crucial role in LLM-based multiagent ADSs.
Both ChatSim~\cite{wei2024editable} and ALGPT~\cite{zhou2024algpt} employ a manager (PM) agent to interpret user instructions and coordinate tasks among other agents. 
ChatSim~\cite{wei2024editable} adopts a centralized structure in which the PM agent decouples an overall demand into specific subtasks and dispatches instructions to other team agents.  
Similarly, the PM agent in ALGPT~\cite{zhou2024algpt} formulates a work plan upon receiving user commands and assembles an agent team with the plan. 
Specifically, agents no longer communicate point-to-point with each other but instead communicate through a shared message pool, greatly improving efficiency. 

Additionally, hierarchical agent architectures further enhance the performance and effectiveness of LLM-based multi-agent ADSs.
AD-H~\cite{zhang2024ad} assigns high-level reasoning tasks to the multimodal LLM-based planner agent while delegating low-level control signal generation to a lightweight controller agent.
These agents interact through mid-level commands generated by the multimodal LLMs.
In LDPD~\cite{liu2024language}, the teacher agent leverages the LLM for complex cooperative decision reasoning and trains smaller student agents via its own decision demonstrations to achieve cooperative decision-making.
SurrealDriver~\cite{jin2024surrealdriver} introduces a CoachAgent to evaluate DriverAgent's driving behavior and provide guidelines for continuous improvement.

Different from the conventional collaborative interaction mode, V-HOI~\cite{zhang2024enhancing} proposes a hybrid interaction mode that blends collaboration with debate.
It establishes various agents across different LLMs to evaluate reasoning logic from different aspects, enabling cross-agent reasoning. 
This process culminates in a debate-style integration of responses from various LLMs, improving predictions for enhanced decision-making.

\section{LLM-based Agent-Human Interaction}
\label{sec:human-vehicle}
Depending on the roles of human assume when interacting with agents, we classify current methods as: \textit{instructor paradigm} and \textit{partnership paradigm}.

\subsection{Instructor Paradigm}
In Figure~\ref{fig:human-agent}, the instructor paradigm involves agents interacting with humans in a conversational manner, where humans act as ``tutors" to offer quantitative and qualitative feedback to improve the agent's decision-making~\cite{li2017dialogue}.
Quantitative feedback typically includes binary evaluations or ratings, while qualitative feedback consists of language suggestions for refinement. 
The agent incorporates these suggestions to adapt and enhance its performance in complex driving scenarios. 
For instance, \citet{wang2023chatgpt} propose ``Expert-Oriented Black-box Tuning", a method where domain experts provide feedback to optimize model performance. 
Similarly, \citet{ma2024learning} present a human-guided learning pipeline that integrates driver feedback to refine agent decision-making.


\subsection{Partnership Paradigm}
In Figure~\ref{fig:human-agent}, the partnership paradigm emphasizes collaboration, where agents and humans interact as equals to accomplish complex driving tasks. 
In this paradigm, agents assist in decision-making by adapting to individual driver preferences and real-time traffic conditions.
For instance, Talk2Drive~\cite{cui2024personalizedautonomousdrivinglarge}, 
DaYS~\cite{cui2024drive} and Receive~\cite{cui2024receive} utilize memory modules to store human-vehicle interactions, enabling a more personalized driving experience based on individual driver preferences, such as overtaking speed and following distance. 
Additionally, infrastructure agents in AccidentGPT~\cite{wang2024accidentgpt} and ConnectGPT~\cite{tong2024connectgpt} connect vehicles to monitor traffic conditions, identify potential hazards, and provide proactive safety warnings, blind spot alerts, and driving suggestions through agent-human interaction.

\begin{figure}[t]
\centering
    \includegraphics[width=1.05\columnwidth]{figs/Human-agent_interaction.pdf}
    \caption{Two modes of agent-human interaction.}
    \label{fig:human-agent}
\end{figure}







\section{Applications}
\label{sec:application}
\subsection{Collaborative Perception}
Despite significant advancements in the perception modules of ADS, LLM-based single-agent ADS continues to face substantial challenges, including constrained sensing ranges and persistent occlusion issues~\cite{han2023collaborative}. 
These two key limitations hinder their comprehensive understanding of the driving environment and can lead to suboptimal decision-making, especially in complex and dynamic traffic scenarios~\cite{hu2024collaborative}.

\cite{dona2024tapping} propose a multi-agent cooperative framework that enhances the ego vehicle's field-of-view (FOV) by integrating 
complementary visual perspectives through inter-vehicle dialogues mediated by onboard LLMs, significantly expanding the ego vehicle's environmental comprehension. 
However, in complex road scenarios, reliance on a single LLM can lead to erroneous interpretations and hallucinatory predictions when processing complex traffic situations. 
To address this limitation, V-HOI MLCR~\cite{zhang2024enhancing} introduces a collaborative debate framework among different LLMs for video-based Human-Object Interaction (HOI) detection tasks.
This framework first implements a Cross-Agent Reasoning scheme, assigning distinct roles to various agents within an LLM to conduct reasoning from multiple perspectives.
Subsequently, a cyclic debate mechanism is employed to evaluate and aggregate responses from multiple agents, culminating in the final outcome.

\subsection{Collaborative Decision-Making}
After obtaining environmental information, the ADS performs three core functions: route planning, trajectory optimization, and real-time decision-making.
In complex traffic scenarios such as roundabout navigation and lane merging, LLM-based multi-agent systems enable coordinated motion planning through three key mechanisms: \ding{182}~real-time intention sharing between agents, \ding{183}~adaptive communication protocols, and \ding{184}~dynamic negotiation frameworks. 
This collaborative architecture allows ADS to precisely coordinate their trajectories, maneuver strategies, and environmental interactions while maintaining operational safety.

LanguageMPC~\cite{sha2023languagempc} uses LLMs to perform scenario analysis and decision-making.
Additionally, it introduces a multi-vehicle control method where distributed LLMs govern individual vehicle operations, while a central LLM facilitates multi-vehicle communication and coordination.
AgentsCoDriver~\cite{hu2024agentscodriver} presents a comprehensive LLM-based multi-vehicle collaborative decision-making framework with life-long learning capabilities, moving the field towards practical applications. 
This framework consists of five parts, as follows: the observation module, cognitive memory module, and reasoning engine support the high-level decision-making process for AD; the communication module enables negotiation and collaboration among vehicles; and the reinforcement reflection module reflects the output and decision-making process. 
Similarly, AgentsCoMerge~\cite{hu2024agentscomerge} combines vision-based and text-based scene understanding to gather essential environmental information and incorporates a hierarchical planning module to allow agents to make informed decisions and effectively plan trajectories. 
Instead of directly interacting with each other, agents in KoMA~\cite{jiang2024koma} analyze and infer the intentions of surrounding vehicles via an interaction module to enhance decision-making.
It also introduces a shared memory module to store successful driving experiences and a ranking-based reflection module to review them.

\subsection{Other Collaborative Assistance-Tools}
The long-term data accumulation in both industry and academia has enabled great success in highway driving and automatic parking~\cite{liu2024survey}. 
However, collecting real-world data remains costly, especially for multi-agents or customized scenarios. 
Additionally, the uncontrollable nature of real scenarios makes it challenging to capture certain corner cases.
To address these issues, many LLM-based studies focus on simulating multi-agent ADS, offering a cost-effective alternative to real-world data collection.
For example, ChatSim~\cite{wei2024editable} provides editable photo-realistic 3D driving scenario simulations via natural language commands and external digital assets. 
The system leverages multiple LLM agents with specialized roles to decompose complex commands into specific editing tasks, introducing novel McNeRF and Mclight methods that generate customized high-quality output.
HumanSim~\cite{zhou2024humansim} integrates LLMs to simulate human-like driving behaviors in multi-agent systems via pre-defined driver characters. 
By employing navigation strategies, HumanSim facilitates behavior-level control of vehicle movements, making it easier to generate corner cases in multi-agent environments.

\begin{table*}[t]
\footnotesize
\centering
\caption{Single-agent and multi-agent autonomous driving datasets. }
\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{llccc}
\toprule
\textbf{Datasets} & \textbf{Dataset Type} & \textbf{Sensor Type}            & \textbf{Tasks}                                                    \\ \hline
KITTI~\cite{geiger2012we}      & Single-agent       & Camera, LiDAR            & 2D/3D detection, tracking, depth estimation                       \\
nuScenes~\cite{caesar2020nuscenes}    & Single-agent      & Cameras, LiDAR, Radars & 3D detection, tracking, trajectory forecasting                    \\
BDD100K~\cite{yu2020bdd100k}   & Single-agent        & Camera                          & Object detection, lane detection, segmentation \\
Waymo~\cite{sun2020scalability}   & Single-agent          & Camera, LiDAR, Radars  & 2D/3D detection, tracking, domain adaptation                      \\ 
BDD-X~\cite{kim2018textual}    & Single-agent         & BDD                             & Object detection, driving scenario captioning                     \\
nuScenes-QA~\cite{qian2024nuscenes}  & Single-agent     & nuScenes                        & 3D detection, tracking, visual QA                                 \\
DriveLM~\cite{sima2025drivelm}      & Single-agent     & nuScenes, Waymo                        & Multi-modal planning, question answering                          \\ 
DAIR-V2X~\cite{yu2022dair}  & Multi-agent       & Camera, LiDAR (multi-vehicle)   & Cooperative perception, tracking                                  \\
TUMTraf-V2X~\cite{zimmer2024tumtraf}     & Multi-agent   & Multi-vehicle camera, LiDAR     & Cooperative perception, multi-agent tracking                      \\
V2V4Real~\cite{xu2023v2v4real}    & Multi-agent      & Multi-vehicle camera, LiDAR     & Cooperative detection, tracking                                   \\
V2XSet~\cite{xu2022v2x}      & Multi-agent       & Multi-vehicle camera, LiDAR     & Multi-agent detection, tracking                                   \\ \bottomrule
\end{tabular}
}
\label{Table-dataset}
\end{table*}

Although many innovative studies have explored the application of LLM-based multi-agent ADS, significant technical challenges remain in deploying LLMs locally on autonomous vehicles due to their huge computational resource requirements~\cite{DBLP:conf/emnlp/0015WCWFWWZY24}.
To address these issues, \citet{tang2024test} apply remote LLMs to provide assistance for connected autonomous vehicles, which communicate between themselves and with LLMs via vehicle-to-everything technologies.
Moreover, this study evaluates LLMs' comprehension of driving theory and skills in a manner akin to human driver tests.
However, remote LLM deployment can introduce inference latency, posing risks in emergency scenarios.
To further improve system efficiency, \citet{chen2024edge} introduce a novel edge-cloud collaborative ADS with drift detection capabilities, using small LLMs on edge devices and GPT-4 on cloud to process motion planning data and complex inference tasks, respectively.

In addition, ALGPT~\cite{zhou2024algpt} uses a muti-agent cooperative framework to enable open-vocabulary and multimodal auto-annotation for autonomous driving.
ALGPT introduces a Standard Operating Procedure that clarifies the role of each agent and shares project documentation, thereby enhancing the effectiveness of multi-agent interactions.
Moreover, ALGPT establishes a specialized knowledge base for each type of agent, using CoT and In-Context Learning~\cite{brown2020language}.



\section{Datasets}
\label{sec:dataset}
We organize the latest state-of-the-art open-source work to foster research of more advanced ADSs.
And we summarize mainstream ADS datasets in Table~\ref{Table-dataset}. More details are listed in Appendix~\ref{app:datasets}.



\section{Challenges and Future Directions}
\label{sec:opportunities}
This section explores key open challenges and potential opportunities for future research. 
\begin{itemize}[leftmargin=*]
    \item \noindent \textit{\textbf{Hallucination Problem.}}
It refers to LLMs generating outputs that are factually incorrect or non-sensical~\cite{huang2023survey}. 
In complex driving scenarios, a single driving agent's hallucinations in an LLM-based multi-agent ADS can be accepted and further propagated by other agents in the network via the inter-agent communication, potentially leading to serious accidents. 
Consequently, detecting and mitigating hallucinations at the individual agent level and managing the flow of information between agents are crucial issues for future research~\cite{fan2024hallucination}.
\item \noindent \textit{\textbf {Multi-Modality Ability.}}
Agents in current multi-agent systems primarily use LLMs for scene understanding and decision making. These methods convert the outputs of perception algorithms into textual representations through manual prompts or interpreters, which are then fed into an LLM to produce decisions.
This approach heavily depends on the performance of the perception algorithm and can lead to loss of environmental information~\cite{gao2023llama}. Therefore, integrating language understanding with the ability to process and fuse multiple data modalities to develop a multimodal multi-agent ADS represents a promising direction for future research.
\item \noindent \textit{\textbf{Scalability Problem.}} 
LLM-based multi-agent ADS can scale up by adding more agents to handle increasingly complex driving scenarios. 
However, more LLM agents increase the demand for computing resources, while their interactions impose strict requirements on communication efficiency, which is critical for real-time decision-making~\cite{huang2024efficient}.
Therefore, under limited computing resources, it is crucial to develop a system architecture that supports distributed computing and efficient communication, as well as agents capable of adapting to various environments and tasks, to optimize multi-agent ADS within resource constraints.
\end{itemize} 


\section{Conclusion}
\label{sec:conclusion}
This paper systematically outlines LLM-based multi-agent ADS and comprehensively reviews the latest research in this field.
Our study first traces the development trajectory of LLM-based multi-agent ADS from single-agent ADS to multi-agent ADS.
Subsequently, we provide a detailed description of the LLM-based multi-agent ADS from the perspectives of agent-environments and profiles, inter-agent interaction mechanisms, and agent-human interactions.
We also systematically classify and introduce existing studies from the perspectives of multi-agent interaction, agent-human interaction, and different applications.
Finally, this paper provides comprehensive public datasets and open source codes, and deeply explores the current challenges and future research directions of LLM-based multi-agent ADS. 
We hope that this review can bring new inspiration and ideas to future research on LLM-based multi-agent ADS.


\section*{Limitations}
\textbf{Emerging Research and Limited Data.} As the field of LLM-based multi-agent ADS is still relatively new, existing research is limited, which may restrict the scope of our classification and analysis. 
\textbf{Some Unverified Work.} Since LLM-based multi-agent ADS is a novel topic, some of the papers summarized in this review are from unreviewed arXiv preprints. As these works have not been formally published, their conclusions may require further investigation to confirm their validity.
\textbf{Limited Discussion on Real-world Applications.} As LLM-based multi-agent ADS is still in the theoretical stage, although many companies have begun deploying practical applications in this area, this review does not cover discussions on real-world deployments due to a lack of up-to-date internal information from these companies.

\bibliography{custom}

\appendix
\newpage
\section{Appendix}
\label{sec:appendix}

\subsection{Data-driven Autonomous Driving System}
\label{app:Data-driven}
Traditional ADS rely on data-driven approaches, which are categorized into modular and end-to-end frameworks~\cite{chen2024end}. 
\textbf{Modular-based systems} break the entire autonomous driving process into separate components, such as \textit{perception module}, \textit{prediction module}, and \textit{planning module}.
Perception modules are responsible for obtaining information about the vehicle's surrounding environment, aiming to identify and locate important traffic elements such as obstacles, pedestrians, and vehicles near the autonomous vehicle, usually including tasks such as object detection~\cite{wang2021detrd} and object occupancy prediction~\cite{tong2023scene}.
Prediction modules estimate the future motions of surrounding traffic participants based on the information provided by the perception module, usually including tasks such as trajectory prediction and motion prediction~\cite{shi2022motion}.
Planning module aims to derive safe and comfortable driving routes and decisions through the results of perception and prediction~\cite{sauer2018conditional}.
Each module is individually developed and integrated into onboard vehicles to achieve safe and efficient autonomous driving functions. 
Although modular methods have achieved remarkable results in many driving scenarios, the stacking design of multiple modules can lead to the loss of key information during transmission and introduce redundant calculations. 
Furthermore, due to the inconsistency in the optimization objectives of each module, the modular-based system may accumulate errors, which can negatively impact the vehicle's overall decision-making performance.
\textbf{End-to-end-based systems} integrate the entire driving process into a single neural network, and then directly optimize the entire driving pipeline from sensor inputs to produce driving actions~\cite{chen2024end}.
However, this approach introduces the “black box” problem, meaning a lack of transparency in the decision-making process, complicating interpretation and validation.

\subsection{LLMs in Autonomous Driving System}
\label{app:single}
As shown in Figure~\ref{fig:single-agent}, \ref{fig:multi-agent}, LLMs, with their powerful open-world cognitive and reasoning capabilities, have shown significant potential in ADSs~\cite{yang2023llm4drive,li2023towards}. 
LC-LLM~\cite{peng2024lc} is an explainable lane change prediction model that leverages LLMs to process driving scenario information as natural language prompts. By incorporating CoT reasoning and supervised finetuning, it not only predicts lane change intentions and trajectories but also provides transparent and reliable explanations for its predictions.
GPT-Driver~\cite{mao2023gpt} regards the motion planning task as a language modeling problem, using a fine-tuned GPT-3.5 model~\cite{ye2023comprehensive} to generate driving trajectories. 
DriveGPT4~\cite{xu2024drivegpt4} introduces an interpretable end-to-end autonomous driving system that uses multimodal LLMs to process multi-frame video inputs and textual queries, enabling vehicle action interpretation and low-level control prediction. By employing a visual instruction tuning dataset and mixfinetuning strategy, it provides a novel approach to directly map sensory inputs to actions, achieving superior performance in autonomous driving tasks.
Driving with LLM~\cite{chen2024driving} integrates vectorized numeric data with pre-trained LLMs to improve context understanding in driving scenarios and enhances the interpretability of driving decisions.

\begin{figure*}[t]
\centering
     \includegraphics[width=0.92\textwidth]{figs/single-agent.pdf}
    \caption{An example of an LLM-based single-agent ADS~\cite{wen2023dilu}.}
    \label{fig:single-agent}
\end{figure*}

\begin{figure*}[t]
\centering
     \includegraphics[width=0.92\textwidth]{figs/multi-agent.pdf}
    \caption{The communication among multiple agents in an LLM-based multi-agent system~\cite{hu2024agentscodriver}}.
    \label{fig:multi-agent}
\end{figure*}

\subsection{Datasets}
\label{app:datasets}


\textbf{Single-agent Autonomous Driving Dataset.} Single-agent datasets are obtained from a single reference agent, which can be the ego vehicle or roadside infrastructure, using various sensors.
Mainstream singel-agent autonomous driving datasets like KITTI~\cite{geiger2012we}, nuScenes~\cite{caesar2020nuscenes}, and Waymo~\cite{sun2020scalability} provide comprehensive multimodal sensor data, enabling researchers to develop and benchmark algorithms for multiple tasks such as object detection, tracking, and segmentation.

In addition to these foundational datasets, newer ones like BDD-X~\cite{kim2018textual}, DriveLM~\cite{sima2025drivelm}, and nuScenes-QA~\cite{qian2024nuscenes} introduce action descriptions, detailed captions, and question-answer pairs that can be used to interact with LLMs.
Combining language information with visual data can enrich semantic and contextual understanding, promote a deeper understanding of driving scenarios, and enhance the safety and interaction capabilities of autonomous vehicles.

\textbf{Multi-agent Autonomous Driving Dataset.}
Beyond single-vehicle view datasets, integrating more viewpoints of traffic elements, such as drivers, vehicles and infrastructures into the data also brings advantages to AD systems. 
Multi-agent autonomous driving datasets, such as DAIR-V2X~\cite{yu2022dair}, V2XSet~\cite{xu2022v2x}, V2V4Real~\cite{xu2023v2v4real}, and TUMTraf-V2X~\cite{zimmer2024tumtraf}  typically include data from multiple vehicles or infrastructure sensors, capturing the interactions and dependencies between different agents and additional knowledge regarding the environments.
These datasets are essential for researching and developing cooperative perception, prediction, and planning strategies that enable vehicles to overcome the limitations of single agent datasets such as limited field of view (FOV) and occlusion.



\end{document}
