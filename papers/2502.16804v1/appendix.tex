\section{Appendix}
\label{sec:appendix}

\subsection{Data-driven Autonomous Driving System}
\label{app:Data-driven}
Traditional ADS rely on data-driven approaches, which are categorized into modular and end-to-end frameworks~\cite{chen2024end}. 
\textbf{Modular-based systems} break the entire autonomous driving process into separate components, such as \textit{perception module}, \textit{prediction module}, and \textit{planning module}.
Perception modules are responsible for obtaining information about the vehicle's surrounding environment, aiming to identify and locate important traffic elements such as obstacles, pedestrians, and vehicles near the autonomous vehicle, usually including tasks such as object detection~\cite{wang2021detrd} and object occupancy prediction~\cite{tong2023scene}.
Prediction modules estimate the future motions of surrounding traffic participants based on the information provided by the perception module, usually including tasks such as trajectory prediction and motion prediction~\cite{shi2022motion}.
Planning module aims to derive safe and comfortable driving routes and decisions through the results of perception and prediction~\cite{sauer2018conditional}.
Each module is individually developed and integrated into onboard vehicles to achieve safe and efficient autonomous driving functions. 
Although modular methods have achieved remarkable results in many driving scenarios, the stacking design of multiple modules can lead to the loss of key information during transmission and introduce redundant calculations. 
Furthermore, due to the inconsistency in the optimization objectives of each module, the modular-based system may accumulate errors, which can negatively impact the vehicle's overall decision-making performance.
\textbf{End-to-end-based systems} integrate the entire driving process into a single neural network, and then directly optimize the entire driving pipeline from sensor inputs to produce driving actions~\cite{chen2024end}.
However, this approach introduces the “black box” problem, meaning a lack of transparency in the decision-making process, complicating interpretation and validation.

\subsection{LLMs in Autonomous Driving System}
\label{app:single}
As shown in Figure~\ref{fig:single-agent}, \ref{fig:multi-agent}, LLMs, with their powerful open-world cognitive and reasoning capabilities, have shown significant potential in ADSs~\cite{yang2023llm4drive,li2023towards}. 
LC-LLM~\cite{peng2024lc} is an explainable lane change prediction model that leverages LLMs to process driving scenario information as natural language prompts. By incorporating CoT reasoning and supervised finetuning, it not only predicts lane change intentions and trajectories but also provides transparent and reliable explanations for its predictions.
GPT-Driver~\cite{mao2023gpt} regards the motion planning task as a language modeling problem, using a fine-tuned GPT-3.5 model~\cite{ye2023comprehensive} to generate driving trajectories. 
DriveGPT4~\cite{xu2024drivegpt4} introduces an interpretable end-to-end autonomous driving system that uses multimodal LLMs to process multi-frame video inputs and textual queries, enabling vehicle action interpretation and low-level control prediction. By employing a visual instruction tuning dataset and mixfinetuning strategy, it provides a novel approach to directly map sensory inputs to actions, achieving superior performance in autonomous driving tasks.
Driving with LLM~\cite{chen2024driving} integrates vectorized numeric data with pre-trained LLMs to improve context understanding in driving scenarios and enhances the interpretability of driving decisions.

\begin{figure*}[t]
\centering
     \includegraphics[width=0.92\textwidth]{figs/single-agent.pdf}
    \caption{An example of an LLM-based single-agent ADS~\cite{wen2023dilu}.}
    \label{fig:single-agent}
\end{figure*}

\begin{figure*}[t]
\centering
     \includegraphics[width=0.92\textwidth]{figs/multi-agent.pdf}
    \caption{The communication among multiple agents in an LLM-based multi-agent system~\cite{hu2024agentscodriver}}.
    \label{fig:multi-agent}
\end{figure*}

\subsection{Datasets}
\label{app:datasets}


\textbf{Single-agent Autonomous Driving Dataset.} Single-agent datasets are obtained from a single reference agent, which can be the ego vehicle or roadside infrastructure, using various sensors.
Mainstream singel-agent autonomous driving datasets like KITTI~\cite{geiger2012we}, nuScenes~\cite{caesar2020nuscenes}, and Waymo~\cite{sun2020scalability} provide comprehensive multimodal sensor data, enabling researchers to develop and benchmark algorithms for multiple tasks such as object detection, tracking, and segmentation.

In addition to these foundational datasets, newer ones like BDD-X~\cite{kim2018textual}, DriveLM~\cite{sima2025drivelm}, and nuScenes-QA~\cite{qian2024nuscenes} introduce action descriptions, detailed captions, and question-answer pairs that can be used to interact with LLMs.
Combining language information with visual data can enrich semantic and contextual understanding, promote a deeper understanding of driving scenarios, and enhance the safety and interaction capabilities of autonomous vehicles.

\textbf{Multi-agent Autonomous Driving Dataset.}
Beyond single-vehicle view datasets, integrating more viewpoints of traffic elements, such as drivers, vehicles and infrastructures into the data also brings advantages to AD systems. 
Multi-agent autonomous driving datasets, such as DAIR-V2X~\cite{yu2022dair}, V2XSet~\cite{xu2022v2x}, V2V4Real~\cite{xu2023v2v4real}, and TUMTraf-V2X~\cite{zimmer2024tumtraf}  typically include data from multiple vehicles or infrastructure sensors, capturing the interactions and dependencies between different agents and additional knowledge regarding the environments.
These datasets are essential for researching and developing cooperative perception, prediction, and planning strategies that enable vehicles to overcome the limitations of single agent datasets such as limited field of view (FOV) and occlusion.

