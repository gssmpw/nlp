\begin{table*}[]
    \centering
    \small
    \scalebox{0.95}{
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{c | c c c| c c |c c}
    \toprule
    Model & LLM Backbone & Vision Encoder & Total Params & Visual Tokens & Output Tokens & Speed (tokens/s) & Total (s)\\
    \midrule
    LLaVA-Phi & Phi2-2.7B&  CLIP ViT-L/14 &3.1B &576 & 256& 26.92& 9.51\\
    MobileVLM-3B & LLaMA-2.7B &  CLIP ViT-L/14&3.1B& 144 & 256 & 35.26 & 7.26  \\
    HoVLE & \multicolumn{2}{c}{32-layer Transformer} &\textbf{2.6B}&768&256&33.03&7.75\\
    \midrule
    Cobra-3.5B & Mamba-2.8B & DINOv2 + SigLIP ViT-SO &3.5B&729&256&99.22&2.58\\
    VisualRWKV-3B & RWKV6-3B &CLIP ViT-L/14 &3.4B&577&256&41.34&6.19\\
    \rowcolor{green!15}
    \name{}-linear & \multicolumn{2}{c}{32-layer Mamba2}&\underline{2.7B}&768&256&\underline{132.43}&\underline{1.93}\\
    \rowcolor{yellow!15}
    \name{}-hybrid & \multicolumn{2}{c}{24-layer Mamba2 + 8-layer Transformer }&\underline{2.7B}&768&256&\textbf{134.77}&\textbf{1.90}\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-1em}
    \caption{\textbf{Inference efficiency comparison under same multimodal prompt and fixed decode length.} We compare with VLMs of the similar parameter scale (3B) across encoder-based, decoder-only, quadratic-complexity, and linear-complexity. The results highlight the speed advantage of \name{}-linear/hybrid. The benchmark recipe directly follows Cobra, and we report the results on the same single NVIDIA RTX 4090 GPU. Note that ``Total Time'' includes the time of both prefilling and decoding, and ``Speed'' = ``Output Tokens'' / ``Total Time''.}
    \vspace{-1em}

    \label{tab:efficiency_comp}
 \end{table*}


