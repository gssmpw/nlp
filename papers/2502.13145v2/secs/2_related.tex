\section{Related Work}
\boldparagraph{Decoder-only VLM.}
The remarkable success of Large Language Models (LLMs) has inspired the research community to extend their capabilities to multi-modal Vision-Language Models (VLMs).
While compositional encoder-based architectures~\cite{llava,internvl,monkey,blip}, leveraging pre-trained foundation vision encoders~\cite{eva, eva02, evaclip, sigclip} and additional connectors, have dominated the field. Recently, a pioneering work Fuyu-8B~\cite{fuyu} demonstrated that a single unified decoder-only Transformer can achieve competitive performance against encoder-based VLMs, offering an appealing alternative due to its architectural simplicity and deployment efficiency.
This breakthrough has sparked researchers' interest in decoder-only VLM. 
SOLO~\cite{solo} proposed a systematic training recipe tailored for decoder-only VLM by adapting pre-trained LLMs to vision-language tasks.
EVE~\cite{eve} advanced this approach by introducing vision-language pre-alignment and auxiliary visual representation supervision during fine-tuning to enhance the performance of decoder-only VLM.
To better preserve the inherited LLM's language capabilities, HoVLE~\cite{hovle} introduces an extra Transformer-based decoder-only holistic embedding module that aligns language and vision modalities before LLM processing multi-modal input tokens.
Despite these advances, existing decoder-only VLMs remain constrained by the quadratic computational complexity of Transformer architectures, resulting in substantial training and deployment costs. In contrast, our proposed \name{} addresses these limitations by converting Transformer layers to linear-complexity Mamba-2 layers through progressive distillation, enabling both pure linear and hybrid architectural variants.


\boldparagraph{Linear-complexity VLM.}
The development of linear-complexity RNN-based LLMs (\eg, Mamba~\cite{mamba}, Mamba-2~\cite{Mamba-2}, RWKV~\cite{rwkv}) has inspired increasing interest in addressing the quadratic complexity limitations of Transformer-based VLMs.
VL-Mamba~\cite{vlmamba} follows the recipe of LLaVA by incorporating a Vision Selective Scan connector with the pre-trained Mamba LLM.
Similarly, Cobra~\cite{cobra} enhances pre-trained Mamba LLM's visual capabilities by integrating DINOv2~\cite{dinov2} and SigLIP~\cite{sigclip} vision encoders.
ML-Mamba~\cite{mlmamba} introduces a Mamba-2 Scan Connector to process visual tokens between the pre-trained vision encoder and the pre-trained Mamba-2 LLM.
Instead of relying on Mamba, VisualRWKV~\cite{visualrwkv} leverages CLIP ViT-L/14~\cite{clip} as the vision encoder and a pre-trained RWKV LLM~\cite{rwkv,rwkv6} with a 2D image scanning mechanism for visual sequence processing.
However, the above works remain constrained by their reliance on pre-trained RNN-based LLMs and vision encoders, following the compositional encoder-based paradigm.
In contrast, our proposed \name{} eliminates the dependency on pre-trained RNN-based LLMs and vision encoders, and enables training a flexible hybrid architecture that interleaves Mamba with Transformer layers with minimal training cost. This capability enables customizable trade-offs between performance and efficiency, making it adaptable to diverse practical applications.

\boldparagraph{Transformer to RNN distillation.}
Instead of training RNN-based LLMs from scratch, recent studies propose to linearize the pre-trained Transformer-based LLMs into RNN-based LLMs through distillation, which can significantly reduce the training cost for building RNN-based LLMs.
\citet{kasai2021finetuning} pioneered this approach by using linear attention and initializing linear attention parameters using pre-trained LLM weights, exploiting the inherent similarities with Transformer's softmax attention.
\citet{zhang2024hedgehog} propose to add loss for matching softmax attention to approximate more closely the base transformer.   
\citet{mercat2024linearizing} advanced the field by replacing softmax attention with a linear RNN kernel coupled with a novel normalization strategy. 
Building upon these foundations, \citet{bick2024transformers}, \citet{wang2024mamba}, and \citet{zhang2024lolcats} developed multi-stage distillation approaches for more effective Transformer to RNN distillation.
Inspired by these advances, we extend this distillation paradigm to VLMs through the proposed novel multi-stage distillation strategy. Our approach first aligns the newly added parameters of the linearized LLM at each layer, followed by layer-wise distillation, and concludes with end-to-end distillation. This progressive pipeline ensures efficient transfer from quadratic knowledge to linear knowledge while maintaining performance.

