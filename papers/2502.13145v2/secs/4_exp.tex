\input{tables/main_results}
\input{tables/ablation_efficiency}
\section{Experiment}
\subsection{Implementation Detail}


\boldparagraph{Training.}
All models are trained using 8 NVIDIA A800 80GB GPUs with BF16 precision and DeepSpeed ZeRO-2~\cite{rajbhandari2020zero,rasley2020deepspeed}. The distillation process utilizes SOLO's~\cite{solo} supervised fine-tuning dataset, comprising 1.7M samples across both language-only and image-text paired instances. We employ the AdamW~\cite{adamw} optimizer with $\beta=(0.9,0.999)$, gradient clipping at 5.0, and a Warmup-Stable-Decay (WSD) scheduler with 10\% warmup and 10\% decay periods. For stages-1 and stage-2 distillation, we use a batch size of 128, train for 20K steps, and set weight decay to 0.05, with learning rates of $1 \times 10^{-3}$ and $5 \times 10^{-4}$ respectively. Stage-3 distillation employs a reduced batch size of 64, continues for 20K steps with weight decay at 0.05, and uses a learning rate of $5 \times 10^{-5}$.



\boldparagraph{Evaluation benchmarks.}
We evaluate our model on 9 diverse public benchmarks, encompassing 6 general VLM benchmarks and 3 visual question answering tasks. 
The general VLM benchmarks include: MME~\cite{mme}, which evaluates visual perception and reasoning through true/false questions; 
MMBench~\cite{mmb}, which assesses model robustness through multiple-choice questions; 
POPE~\cite{pope}, which evaluates object hallucination; 
SEED~\cite{seed}, which gauges open-world multi-modal understanding; 
MMMU~\cite{mmmu}, which scrutinizes models with college-level multi-discipline reasoning tasks;
and MM-Vet~\cite{mmvet}, which evaluates the model on 16 emergent tasks from core visual and linguistic capabilities.
The visual question answering benchmarks comprise: TextVQA~\cite{textvqa}, which evaluates optical character recognition (OCR) capabilities and text-based reasoning; 
ScienceQA~\cite{sqa}, which tests scientific image comprehension; 
and GQA~\cite{gqa}, which assesses real-world visual reasoning and compositional question answering.

For the specific score in the comparison, we report the MME-perception score as the MME score, MMB score is calculated on the MMBench-EN split, and the POPE score is calculated by averaging across its three categories.

\subsection{Main Comparison}
In Tab.~\ref{tab:results_general}, we compare \name{} with previous encoder-based and decoder-only VLMs on 9 popular benchmarks. We highlight the following findings:
\begin{itemize}
\item \name{} only performs distillation as the training recipe, which requires much lower training cost in two aspects: (1) dataset collection--unlike other methods require separate curated datasets for pre-training (PT) and supervised fine-tuning (SFT), our distillation recipe only needs a single SFT dataset; (2) trainable parameters--our method only updates 14.7\% parameters for \name{}-linear and 11.2\% parameters for \name{}-hybrid during training, while other methods require training most of parameters.
\item  \name{}-linear surpasses previous SOTA Transformer-based decoder-only VLM EVE-7B on 6/9 benchmarks (\ie, MME, MMB, POPE, SEED, MM-Vet, ScienceQA), while matching the performance on the left 3 benchmarks with 2$\times$ fewer parameters.
Even compared with encoder-based VLMs (\eg, MobileVLM-3B, LLaVA-phi), \name{}-linear still demonstrates a comparable performance, while the computation complexity is reduced to linear complexity.
\item \name{}-linear matches the performance of recent encoder-based linear-complexity VLMs (VisualRWKV-3B and VL-Mamba-3B), while significantly outperforming them on the ScienceQA benchmark.
\item By interleaving with the Transformer layers, \name{}-hybrid achieves improved performance on all benchmarks over \name{}-linear, significantly narrowing the gap with the teacher Transformer-based  HoVLE and outperforming the linear complexity encoder-based VLMs (VisualRWKV-3B and VL-Mamba-3B).
\end{itemize}

\subsection{Efficiency Analysis}

\boldparagraph{Fixed prompt and fixed decode length.}
We directly follow the benchmark recipe of Cobra in Tab.~\ref{tab:efficiency_comp}, where we prompt the VLM model with the same example image and question ``Describe the image specifically'', and set the number of output tokens to 256.
We record the total time of the VLM model, which includes the image/text prompt prefilling time and the decoding time. The speed (tokens/s) is calculated by the number of output tokens (\ie, 256) divided by the total time.
We compare our method with 3 transformer-based VLMs and 2 linear-complexity encoder-based VLMs of similar parameter scale. All the evaluations are conducted on the same single NVIDIA RTX 4090 GPU.

Thanks to the fixed hidden state rooted in linear-complexity modeling, \name{}-linear/hybrid achieve nearly 4$\times$ faster inference speed than all the Transformer-based VLMs.
\name{}-linear/hybrid also outperforms the linear complexity encoder-based VLMs (Cobra-3.5B and VisualRWKV-3B) by a large margin (about 30 tokens/s and 3$\times$ faster, respectively) due to the simple decoder-only architecture.


\input{tables/ablation_stage}
\input{tables/ablation_init}

\boldparagraph{Increasing context length.}
Long context processing has emerged as a crucial capability in modern VLMs, becoming increasingly important for high-resolution image/video understanding~\cite{videollm, hrvlm} and long chain-of-thought multimodal reasoning~\cite{llavacot,Lightman2023LetsVS,deepseekr1, s1, k1p5}, which often require processing sequences of thousands of tokens.
To showcase the efficiency of the proposed \name{} in this application, we compare our model with Transformer-based HoVLE in the same single NVIDIA RTX 4090 GPU. We report the GPU memory usage and the latency of the model for decoding the next token.

As shown in Fig.~\ref{fig:teaser}, thanks to the efficient implementation of FlashAttention2, HoVLE demonstrates stable and low latency under 4K token length.
As the context token length reaches 8K and beyond, the latency and memory of HoVLE increase linearly with the token length due to the growing Key-Value cache, when the token length reaches 128K, HoVLE squeezes out of the GPU memory and fails to decode.
On the contrary, \name{}-linear exhibits low and stable latency and memory usage with increasing token length, and the inference cost of \name{}-hybrid increases much slower than HoVLE, which can still decode at the 128K token length.
Specifically, at the 103K token length, \name{}-linear demonstrates 20.6$\times$ speedup compared to HoVLE and saves 75.8\% GPU memory, while \name{}-hybrid achieves 13.5$\times$ speedup and saves 60.2\% GPU memory.



















\subsection{Ablation Study}



\input{tables/ablation_hybrid}
\input{tables/ablation_hybrid_strategy}
\boldparagraph{Stage importance.}
As shown in Tab.~\ref{tab:stage}, direct weight transfer from Transformer to Mamba-2 without distillation (Sec.~\ref{sec:init}) lost the multi-modal understanding ability.
By progressively adding the designed distillation stages, the model's performance is increasingly improved.
When comparing ID-7 and ID-8, we can see that the proposed extra parameter distillation stage-1 decouples the optimization and eases the training, leading to a better alignment, with consistent improvements across all metrics 
(48 in MME, 1.7 in POPE, 6.6 in TextVQA, 7.1 in ScienceQA).






\boldparagraph{Parameter initialization.}
In Tab.~\ref{tab:init}, 
We compare with the ``from scratch'' strategy used in Phi-Mamba~\cite{bick2024transformers}, which replace the trained Transformer layer with directly initialized Mamba-2 layer without inheriting the parameters,
and the ``inherit $\boldsymbol{W}_{Q,K,V}$'' strategy used in LoLCATs~\cite{zhang2024lolcats} and Mamba in the LLaMA~\cite{wang2024mamba}, which exploit the similarity and only inherit the parameters of $\boldsymbol{W}_{Q,K,V,O}$ from Transformer to Mamba-2.
The results validate the superiority of our proposed parameter initialization strategy, which should not only inherit the trained parameters but also initialize the extra introduced parameters by mimicking the original attention mechanism.







\boldparagraph{Hybrid architecture.}
The proposed distillation recipe is more flexible than the previous training recipe used in building linear-complexity encoder-based VLM, which requires the trained linear-complexity LLM and can not modify the architecture.
As shown in Tab.~\ref{tab:hybrid}, we can build a hybrid architecture with varied interleaved Transformer layers, enabling the flexible trade-off between performance and efficiency.
By increasing the number of Transformer layers, the performance is gradually improved.
The hybrid architecture with 24 Mamba-2 layers and 8 Transformer layers can achieve comparable performance with a minor decrease compared to the full Transformer model HoVLE.



\boldparagraph{Hybrid strategy.}
In Tab.~\ref{tab:hybrid_strategy}, we explore specific hybrid strategies while fixing the number of interleaved Transformer layers to 8. 
We study 4 interleaving strategies: 
(1) Tail-stacked: stacking all 8 Transformer layers at the top of the network. 
(2) Head-stacked: stacking all 8 Transformer layers at the bottom of the network;
(3) Tail-interleaved: interleaving a Transformer layer at the tail of every 4-layer block; 
(4) Head-interleaved: interleaving a Transformer layer at the head of every 4-layer block; 
The results demonstrate that the Head-interleaved strategy is the most effective, achieving the best performance across all metrics 






