\section{Introduction}
Recent advances in Large Language Models (LLMs)~\cite{gpt3, gpt4, llama, llama2, llama3, qwenllm, mistral, deepseek, phi2} have catalyzed significant research interest in expanding their capabilities beyond text to encompass multimodal understanding, particularly in processing both visual and textual information simultaneously. This expansion has given rise to Multimodal Large Language Models (MLLMs), with Vision Language Models (VLMs) emerging as a prominent subset. Notable examples such as LLaVA~\cite{llava}, BLIP~\cite{blip}, Qwen-VL~\cite{qwenvl}, InternVL~\cite{internvl}, and Monkey~\cite{monkey} have demonstrated remarkable success in enhancing LLMs' visual comprehension capabilities through the integration of pre-trained vision encoders and specialized connectors that bridge the modality gap between vision and language. 

While these encoder-based compositional VLMs have achieved state-of-the-art (SOTA) performance and established themselves as the de-facto paradigm, they face two critical limitations. 
First, processing long contexts becomes prohibitively expensive due to the quadratic computational complexity and linearly growing Key-Value (KV) cache with respect to sequence length. 
This limitation becomes particularly problematic given the increasing demand for long chain-of-thought reasoning~\cite{s1,deepseekr1,k1p5,llavacot} and high-resolution image/video understanding~\cite{videollm,hrvlm,vim,vig,dig}. Second, their heterogeneous architecture heavily relies on pre-trained vision encoders, introducing significant complexity in both training procedures and deployment scenarios~\cite{solo}.

Current research efforts to address these limitations have followed two distinct paths. One approach focuses on developing linear-complexity VLMs by adhering to the conventional encoder-based recipe, which requires both pre-trained vision encoders and pre-trained linear-complexity language models~\cite{visualrwkv,vlmamba}. The alternative approach aims to enhance decoder-only VLMs through increased model scale and expanded training datasets, achieving performance competitive with encoder-based counterparts~\cite{fuyu,eve,hovle,chameleon,emu3}. 



Despite these advances, the development of linear-complexity decoder-only MLLMs remains an understudied yet critical challenge. Addressing this gap holds substantial value for three key reasons: (1) \textit{Unified multimodal understanding}: Such models could seamlessly integrate multimodal reasoning within a single architecture, eliminating the need for heterogeneous, modality-specific frameworks. (2) \textit{Practical efficiency}: Linear-complexity models inherently reduce computational demands during both training and inference, lowering costs and enabling deployment on resource-constrained edge devices. (3) \textit{Untapped Potential}: While recent linear-time models like Mamba-2 demonstrate high text-processing capabilities, their ability to handle multimodal tasks—particularly in cross-modal alignment and reasoning—remains largely unexplored. The research of linear-complexity decoder-only MLLMs could unlock scalable, cost-effective multimodal systems without sacrificing performance.

A straightforward solution is to synergize the recipe of decoder-only VLMs and linear-complexity encoder-based VLMs.
This integration requires a pre-trained linear-complexity LLM and performs image-caption alignment pre-training (PT) and supervised fine-tuning (SFT) using text instructions and image prompts. 
However, this integrated recipe faces two significant challenges:
(1) It demands the curation of different large-scale multimodal datasets for different purposes (\ie, PT and SFT) and requires substantial computational resources. 
(2) The overall performance is inherently limited by the capabilities of pre-trained linear-complexity LLMs, which consistently underperform mainstream SOTA Transformer-based LLMs in language understanding tasks.

In this paper, we propose a novel distillation-based recipe to develop linear-complexity decoder-only VLMs, which requires only moderate academic resources while circumventing the limitations of pre-trained linear-complexity LLMs.
Our method leverages the fundamental similarity between the Transformer attention mechanism and the Mamba-2 state space model (SSM) mechanism. We introduce an initialization scheme that enables direct parameter transfer from Transformer to Mamba-2 layers, effectively converting the attention mechanism into the SSM function while carefully initializing SSM-specific parameters to mimic attention behavior. This approach enables the direct transformation of pre-trained Transformer-based VLMs into linear-complexity Mamba-2-based VLMs without relying on underperforming pre-trained linear-complexity LLMs.
While this parameter inheritance and initialization strategy provides a promising starting point, the transformed Mamba-2-based VLM requires further distillation to recover robust multimodal conversation capabilities. To enhance alignment with the Transformer-based teacher VLM, we develop a three-stage progressive distillation strategy:
(1) Stage-1: we first train the SSM-specific parameters while freezing inherited parameters, and align layer-wise behavior using MSE distillation loss; 
(2) Stage-2: we then optimize complete Mamba-2 layer behavior by enabling the training of inherited Transformer parameters; 
(3) Stage-3: we finally perform complete model alignment using KL-divergence loss on final output logits to recover the teacher model's multimodal understanding capabilities through end-to-end distillation.


The proposed distillation recipe enables two distinct architectural variants: \name{}-linear, which converts all Transformer layers into Mamba-2 layers, achieving full linear complexity, and \name{}-hybrid, which strategically transforms fixed intervals of Transformer layers into Mamba-2 layers. The hybrid design systematically preserves Transformer layers at critical feature hierarchies while leveraging Mamba-2's linear complexity for the majority of computations, striking a balance between efficiency and capability. 
During the final end-to-end distillation stage, we can flexibly adjust the number of interleaved Transformer layers, enabling precise control over the computation-performance trade-off. This architectural flexibility makes our approach highly adaptable to diverse deployment scenarios, allowing optimization for specific computational constraints while maintaining desired performance.



By distilling from the recent Transformer-based decoder-only VLM, HoVLE, we demonstrate that \name{} achieves competitive performance across multiple vision-language benchmarks while significantly improving computational efficiency. Our pure Mamba-2-based linear-complexity variant, \name{}-linear, achieves comparable performance to existing quadratic/linear-complexity VLMs like Mobile-VLM-3B~\cite{mobilevlm}, VisualRWKV-3B~\cite{visualrwkv}, VL-Mamba-3B~\cite{vlmamba}
while eliminating the need for separate vision encoders. \name{}-pure also matches the performance of the previous SOTA Transformer-based decoder-only EVE-7B with 2$\times$ fewer parameters. The hybrid variant, \name-hybrid, significantly improves performance on all benchmarks compared to \name{}-pure, approaching the teacher model HoVLE. Notably, at the context length of 103K tokens, \name{}-linear demonstrates 20.6$\times$ speedup compared to HoVLE and saves 75.8\% GPU memory, while \name{}-hybrid achieves 13.5$\times$ speedup and saves 60.2\% GPU memory. These results and extensive ablation studies validate the effectiveness of our distillation recipe and highlight the potential for practical applications.

Our main contributions can be summarized as follows:
\begin{itemize}
\item We present a novel three-stage progressive distillation recipe for building native multimodal state space models without the reliance on underperforming pre-trained linear-complexity LLMs, enabling effective knowledge transfer from quadratic to linear architectures.
\item With the proposed distillation recipe, we propose the first decoder-only multimodal state space models that include two distinct architectural variants: \name{}-linear with purely linear complexity and \name{}-hybrid offering flexible performance-efficiency trade-offs.
\item Extensive experimental results demonstrate competitive performance with significantly improved computational efficiency across various vision-language tasks, achieving up to 20.6$\times$ speedup and 4.2$\times$ memory reduction for long sequence modeling on NVIDIA 4090 GPU.
\end{itemize}







