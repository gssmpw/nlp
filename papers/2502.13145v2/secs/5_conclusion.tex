\section{Conclusion}
We presented \name{}, a novel framework for building linear-complexity decoder-only VLMs with only moderate academic resources through the proposed distillation recipe, eliminating the need for pre-trained linear-complexity LLMs and vision encoders. Our recipe enables both pure linear and hybrid architectures, achieving competitive performance while significantly reducing computational costs. Experimental results demonstrate that \name{}-linear matches or exceeds the performance of existing linear-complexity and quadratic-complexity VLMs, while \name{}-hybrid further improves performance through flexible efficiency-performance trade-offs with interleaved Transformer layers. At 103K tokens, \name{}-linear achieves up to 20.6Ã— speedup and 75.8\% memory reduction compared to Transformer-based teacher HoVLE, while \name{}-hybrid achieves 13.5$\times$ speedup and saves 60.2\% GPU memory. These results validate the effectiveness of our distillation recipe for building linear-complexity decoder-only VLMs suitable for long-context applications.
