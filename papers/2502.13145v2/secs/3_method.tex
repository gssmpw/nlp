\section{Preliminary}
\label{sec:preliminary}
We firstly give a brief background on quadratic-complexity sequence modeling Transformer and linear-complexity sequence modeling Mamba-2.
Given an input sequence $\mathbf{X} = \left[\boldsymbol{x}_1,\dots, \boldsymbol{x}_T\right]^\top \in \mathbb{R}^{T \times d}$, where $T$ is the sequence length and $d$ is the hidden dimension. The above two sequence modeling layers will compute the output sequence $\mathbf{Y} = \left[\boldsymbol{y}_1,\dots, \boldsymbol{y}_T\right]^\top \in \mathbb{R}^{T \times d}$.

\paragraph{Transformer}
The standard autoregressive Transformer used in LLM employs attention mechanism~\cite{attention} by interacting with all historical positions in the sequence, which is defined as:
\begin{align}
\begin{split}
    \boldsymbol{q}_t, \boldsymbol{k}_t, \boldsymbol{v}_t &= \boldsymbol{x}_t \boldsymbol{W}_Q, \boldsymbol{x}_t \boldsymbol{W}_K, \boldsymbol{x}_t \boldsymbol{W}_V, \\
    \boldsymbol{y}_t &= \frac{\sum_{i=1}^{t}\text{exp}(\boldsymbol{q}_t\boldsymbol{k}_i^\top)\boldsymbol{v}_i}{\sum_{i=1}^{t}\text{exp}(\boldsymbol{q}_t\boldsymbol{k}_i^\top)},
    \label{eq:attn}
\end{split}
\end{align}
where $\boldsymbol{W}_Q, \boldsymbol{W}_K, \boldsymbol{W}_V \in \mathbb{R}^{d \times d}$ are the learnable parameters. The current output token $\boldsymbol{o}_t$ is computed by performing attention over the growing sequence of historical keys $\left\{\boldsymbol{k}_i\right\}_{i=1}^{t}$ and values $\left\{\boldsymbol{v}_i\right\}_{i=1}^{t}$.

\paragraph{Mamba-2}
Instead of interacting with all historical positions, Mamba-2~\cite{Mamba-2} compresses the historical information into a fixed-size matrix-shaped hidden state, which is defined as:
\begin{align}
\begin{split}
    \boldsymbol{q}_t, \boldsymbol{k}_t, \boldsymbol{v}_t &= \boldsymbol{x}_t \boldsymbol{W}_Q, \boldsymbol{x}_t \boldsymbol{W}_K, \boldsymbol{x}_t \boldsymbol{W}_V, \\
    \gamma_t &= \text{exp}\left(-\text{softplus}(\boldsymbol{x}_t\boldsymbol{W}_\gamma)\text{exp}(a)\right), \\
    \mathbf{S}_t &= \gamma_t \mathbf{S}_{t-1} + \boldsymbol{v}_t \boldsymbol{k}_t^\top, \\
    \boldsymbol{y}_t &= \mathbf{S}_t \boldsymbol{q}_t,
    \label{eq:Mamba-2}
\end{split}
\end{align}
where $\boldsymbol{W}_Q, \boldsymbol{W}_K, \boldsymbol{W}_V \in \mathbb{R}^{d \times d}$, $\boldsymbol{W}_\gamma \in \mathbb{R}^{d \times 1}$ and $a \in \mathbb{R}$ are the learnable parameters. $\mathbf{S}_t$ is the fixed-size matrix-shaped hidden state, $\gamma_t$ is the data-dependent gating term to control the information flow by dynamically decaying the historical information $\mathbf{S}_{t-1}$.

\section{Method}
Our method consists of three key components. 
First, we detail the seeding strategy, which carves the Mamba-2 architecture from a pre-trained Transformer by inheriting parameters and carefully initializing the newly introduced SSM-specific parameters in Sec.~\ref{sec:init}. 
Building upon this seeding strategy, we present the proposed progressive distillation pipeline in Sec.~\ref{sec:stage1}, Sec.~\ref{sec:stage2} and Sec.~\ref{sec:stage3} to effectively transfer knowledge from Transformer to Mamba-2. 
With the designed distillation training recipe, we then instantiate two model variants in Sec.~\ref{sec:arch}: \name{}-linear using only Mamba-2 layers, and \name{}-hybrid incorporating interleaved Transformer and Mamba-2 layers.

\subsection{Seeding: Initialize Mamba-2 from Transformer}
\label{sec:init}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/initialize.pdf}
    \caption{
        \textbf{Initialize Mamba-2 from Transformer.} By comparing the mechanism similarity in Sec.~\ref{sec:preliminary}, we directly inherit $\boldsymbol{W}_Q$, $\boldsymbol{W}_K$, $\boldsymbol{W}_V$, $\boldsymbol{W}_O$ parameters (blue) from trained Transformer layer and carefully initialize the extra parameters (orange) including $a$, $\boldsymbol{W}_\gamma$, $\boldsymbol{W}_{\text{conv}}$, and $\boldsymbol{W}_G$ in Mamba-2 to initially mimic the Transformer's behavior, providing a strong foundation for subsequent distillation.
    }
    \vspace{-0.3cm}
    \label{fig:Mamba-2_from_transformer}
\end{figure}

To transfer as much knowledge as possible from quadratic Transformer to linear Mamba-2, we initialize Mamba-2 from Transformer at each layer.
By comparing Eq.~\ref{eq:attn} and Eq.~\ref{eq:Mamba-2}, we can find that Mamba-2 shares the similarity with Transformer, which means we can directly inherit $\boldsymbol{W}_Q, \boldsymbol{W}_K, \boldsymbol{W}_V$ and $\boldsymbol{W}_O$ projection parameters at each layer instead of building from scratch. 
Furthermore, we need to introduce extra parameters $\boldsymbol{W}_\gamma$ and $a$ for state space modeling, replacing the attention mechanism.
For better replacement and ease the training difficulty~\cite{trockman2024mimetic}, we initialize $\boldsymbol{W}_\gamma$ and $a$ to make the gating term $\gamma_t$ close to $1$ at the beginning of training, which means we begin by memorizing all historical information without selectivity.

Beyond the core SSM mechanism, we also introduce extra causal convolution and output gating for enhanced positional awareness and expressiveness.
To eliminate the initial impact of causal convolution, we initialize the weights and biases to make it function as an identity layer (\ie, the output of causal convolution is the same as the input) without affecting the original function of SSM at the beginning of training.

The other parts of the model such as the MLP layers and text and image patch embedding layers are directly inherited from the original Transformer-based VLM and kept as frozen.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/pipeline.pdf}
    \caption{\textbf{Progressive distillation pipeline of our \name{}.} We keep MLP layers, text and image patch embedding layers and freeze them in subsequent distillation training stages. Stage-1: Train the newly-introduced SSM-specific parameters while freezing inherited Transformer parameters in a layer-wise manner. Stage-2: Train all parameters to align Mamba's state representation with Transformer in a layer-wise manner. Stage-3: Train all the Mamba layers of the model to align the end-to-end behavior with the teacher Transformer-based VLM.
    }
    \label{fig:distill_pipeline}
\end{figure*}

\subsection{Stage-1: Layerwise Distillation for the Newly Introduced SSM Parameters}
\label{sec:stage1}

We first perform layerwise distillation for the introduced extra parameters to align the proposed Mamba-2 layer with the original trained Transformer layer.
Specifically, we instantiate the trained Transformer-based VLM as teacher model, and the transferred Mamba-2 VLM model as student model.
The only difference lies in the sequence mixer layer. We feed the multimodal sequence into the teacher model.

To keep the layerwise alignment and diminish the accumulated error of the cascading layers, we input the $i$-th Mamba-2 layer with the output of the $i-1$-th Transformer layer, i.e., $i$-th Mamba-2 layer and $i$-th Transformer layer have the same input. And we align the layerwise behavior by applying the MSE distillation loss between the output of the $i$-th Mamba-2 layer and the output of the $i$-th Transformer layer:
\begin{align}
    \begin{split}
    \phi_{\text{stage1}}^i &= \{a^i, \boldsymbol{W}_{\gamma}^i, \boldsymbol{W}_{\text{conv}}^i, \boldsymbol{W}_{G}^i\}, \\
    \min_{\{\phi_{\text{stage1}}^i\}_{i=1}^{L}} &\sum_{i=1}^{L}\mathcal{L}_{\mathrm{MSE}}(\text{Attn}(\mathbf{X}^{i}), \text{Mamba-2}_{\phi_{\text{stage1}}^i}(\mathbf{X}^{i})),
\end{split}
\end{align}
where $\phi_{\text{stage1}}^i$ is the trainable parameters of the $i$-th Mamba-2 layer, which only includes the introduced extra parameters $a^i, \boldsymbol{W}_{\gamma}^i, \boldsymbol{W}_{\text{conv}}^i, \boldsymbol{W}_{G}^i$.
$\mathbf{X}^{i}$ is the input sequence to the $i$-th Mamba-2 layer and Transformer layer, $\text{Attn}(\mathbf{X}^{i})$ is the output of the $i$-th teacher Transformer layer, $\text{Mamba-2}_{\phi_{\text{stage1}}^i}(\mathbf{X}^{i})$ is the output of the $i$-th student Mamba-2 layer.




\subsection{Stage-2: Layerwise Distillation for the Whole Mamba-2 Parameters}
\label{sec:stage2}
After the stage-1 distillation, we have obtained good initialization of the introduced extra parameters, and we further train all the Mamba-2 parameters to better align the layerwise behavior of the student Mamba-2 with the teacher Transformer.
The only difference between the stage-1 and stage-2 is that we further include the parameters of $\boldsymbol{W}_Q, \boldsymbol{W}_K, \boldsymbol{W}_V$ for optimizing the distillation loss:
\begin{align}
    \begin{split}
    &\phi_{\text{Stage2}}^i = \{a^i, \boldsymbol{W}_{\gamma}^i, \boldsymbol{W}_{\text{conv}}^i, \boldsymbol{W}_{G}^i, \boldsymbol{W}_Q^i, \boldsymbol{W}_K^i, \boldsymbol{W}_V^i\}, \\
    &\min_{\{\phi_{\text{Stage2}}^i\}_{i=1}^{L}} \sum_{i=1}^{L}\mathcal{L}_{\mathrm{MSE}}(\text{Attn}(\mathbf{X}^{i}), \text{Mamba-2}_{\phi_{\text{Stage2}}^i}(\mathbf{X}^{i})),
\end{split}
\end{align}


\subsection{Stage-3: End-to-End Distillation}
\label{sec:stage3}
Beyond the layerwise alignment, the final stage-3 distillation aims to align the end-to-end behavior of the student Mamba-2 with the teacher Transformer.
Specifically, we input the same multi-modal sequence to both the teacher Transformer and the student Mamba-2 without sharing the intermediate output.
For the output of the teacher model and the student model, we apply the word-level KL-Divergence loss, in other words, they are used as soft labels, we enforce the output logits of the student model to be close to the output logits of the teacher model:
\begin{align}
\begin{split}
    &\phi_{\text{Stage-3}} = \{a^i, \boldsymbol{W}_{\gamma}^i, \boldsymbol{W}_{\text{conv}}^i, \boldsymbol{W}_{G}^i, \boldsymbol{W}_Q^i, \boldsymbol{W}_K^i, \boldsymbol{W}_V^i\}_{i=1}^{L}, \\
    &\min_{\phi_{\text{Stage-3}}} \mathcal{L}_{\mathrm{KL}}(\text{Teacher-}(\mathbf{X}^0), \text{Student}_{\phi_{\text{Stage3}}}(\mathbf{X}^{0})),
\end{split}
\end{align}
where $\mathbf{X}^0$ is the same multi-modal input sequence to the teacher model and the student model, $\phi_{\text{Stage3}}$ is the trainable parameters of the student model.
                                                                     


\subsection{Architecture}
\label{sec:arch}
Our \name{} builds upon HoVLE, a decoder-only VLM that consists of 32 Transformer layers. 
For \name{}-linear, we convert each Transformer layer into a Mamba-2 layer while preserving the MLP layers, resulting in a linear-complexity decoder-only VLM. To enhance model expressiveness, we adopt a multi-head design in our Mamba-2 layers by partitioning the SSM into multiple groups and implementing shared queries across groups, consistent with the grouped query attention used in HoVLE.

For \name{}-hybrid, we introduce a systematic layer conversion scheme. Specifically, within every fixed number of consecutive layers, we preserve the first layer as Transformer and convert the remaining layers to Mamba-2. This hybrid scheme maintains the Transformer's modeling capacity at critical feature hierarchies while leveraging Mamba-2's linear complexity for the majority of computation. Such design enables an effective and flexible trade-off between computational efficiency and model capability, suitable for various deployment scenarios with varied requirements. In this paper, we set the interval as 4, building \name{}-hybrid with 8 Transformer layers and 24 Mamba-2 layers in total.
