
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[arxiv]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{float}
\usepackage{color,xcolor}
\usepackage{colortbl}
\usepackage{caption} 
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\boldparagraph}[1]{\vspace{0.05cm}\noindent{\bf #1}}
\newcommand{\name}{mmMamba}
\newcommand{\bc}[1]{{\textcolor{red}{[\textbf{bc:} #1]}}}
\newcommand{\tbd}[1]{{\textcolor{red}{#1}}}
\newcommand{\note}[1]{{\textcolor{blue}{[\textbf{note:} #1]}}}


\def\eg{\emph{e.g.}} \def\Eg{\emph{E.g.}}
\def\ie{\emph{i.e.}} \def\Ie{\emph{I.e.}}
\def\vs{\emph{vs.}}
\def\etc{\emph{etc.}}
\def\etal{\emph{et al.}}
\icmltitlerunning{Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation}

\begin{document}

\twocolumn[
\icmltitle{Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Bencheng Liao}{equal,aia,eic}
\icmlauthor{Hongyuan Tao}{equal,eic}
\icmlauthor{Qian Zhang}{horizon}
\icmlauthor{Tianheng Cheng}{eic}
\icmlauthor{Yingyue Li}{eic}
\icmlauthor{Haoran Yin}{horizon}
\icmlauthor{Wenyu Liu}{eic}
\icmlauthor{Xinggang Wang}{eic}
\end{icmlauthorlist}

\icmlaffiliation{aia}{Institute of Artificial Intelligence, Huazhong University of Science \& Technology}
\icmlaffiliation{eic}{School of EIC, Huazhong University of Science \& Technology}
\icmlaffiliation{horizon}{Horizon Robotics}

\icmlcorrespondingauthor{Xinggang Wang}{xgwang@hust.edu.cn}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
{%

\vspace{-0.5cm}
\begin{center}
    \captionsetup{type=figure}
    \includegraphics[width=0.9\textwidth]{figs/teaser.pdf}
    \vspace{-.1 in}
    \captionof{figure}{
        \textbf{Comprehensive comparison of \name{}.} 
        \textbf{(a)} Our \name{} can build linear-complexity and hybrid decoder-only VLM by distilling the knowledge in Transformer to Mamba-2. 
        \textbf{(b)} By distilling from the quadratic-complexity decoder-only VLM HoVLE,  our \name{}-linear achieves competitive performance against existing linear and quadratic-complexity VLMs with fewer parameters (\eg, 2$\times$ fewer than EVE-7B), while \name{}-hybrid surpasses them across all benchmarks and approaches the teacher model HoVLE's performance.
        \textbf{(c)-(d)} We compare the speed and memory of \name{}-linear and \name{}-hybrid with the teacher model HoVLE on the same single NVIDIA 4090 GPU. \name{}-linear maintains consistently low latency and memory usage, while \name{}-hybrid's resource consumption scales significantly better than HoVLE. At 103K tokens, \name{}-linear demonstrates 20.6$\times$ speedup compared to HoVLE and saves 75.8\% GPU memory, while \name{}-hybrid achieves 13.5$\times$ speedup and saves 60.2\% GPU memory.
    }
    \label{fig:teaser}
\end{center} %
}
]



\printAffiliationsAndNotice{\icmlEqualContribution} %


\begin{abstract}
Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose \name{}, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs.
Distilled from the Transformer-based decoder-only HoVLE, \name{}-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while \name{}-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, \name{}-linear demonstrates 20.6$\times$ speedup and 75.8\% GPU memory reduction compared to HoVLE, while \name{}-hybrid achieves 13.5$\times$ speedup and 60.2\% memory savings.
Code and models are released at \url{https://github.com/hustvl/mmMamba}
\end{abstract}

\input{secs/1_intro}
\input{secs/2_related}
\input{secs/3_method}
\input{secs/4_exp}
\input{secs/5_conclusion}


\bibliography{example_paper}
\bibliographystyle{icml2025}






\end{document}
