\section{Experiments}
In empirical experiments, we follow the original CoT paper~\cite{cot} to evaluate on 3 categories of tasks: arithmetic reasoning, commonsense reasoning, and symbolic reasoning. 
We pick representative tasks where original CoT significantly improves the accuracy over the baseline without reasoning. In particular, we pick GSM8k~\cite{gsm8k} for arithmetic reasoning; date understanding and sports understanding from BIG-bench~\cite{bigbench} for commonsense reasoning; and coin flip tasks introduced in the CoT paper~\cite{cot} for symbolic reasoning. 

\subsection{Experimental Setup}
We compare three different prompt strategies: CoT, CoD, and Standard prompting as a baseline.

\noindent{\bf Standard prompting.} we use standard few-shot prompting~\cite{fewshot}, where the model is given input-output pairs as in-context examples. LLMs are asked to directly return the final answer, without any reasoning or explanation. 

\noindent{\bf Chain-of-Thought.}
We follow the exact few-shot examples provided in the appendix of the CoT paper with the exception of having the final answer after four hashtags ({\small \#\#\#\#}) for a more stable answer extraction. 

\noindent{\bf Chain-of-Draft.} In CoD, we also asked the model to think step by step. However, the model is asked to limit each reasoning step to five words at most. Note that we do not enforce such limitation in any way, it is just a general guideline to promte short reasoning steps. 
For each few-shot example, we also include the Chain of Draft written manually by the authors. 

The complete system prompt for each prompting strategy is shown below.

\begin{center}
\centering
\begin{promptbox}[Standard]
Answer the question directly.
Do not return any preamble, explanation, or reasoning.
\end{promptbox}
\begin{promptbox}[Chain-of-Thought]
Think step by step to answer the following question.
Return the answer at the end of the response after a separator \#\#\#\#.
\end{promptbox}
\begin{promptbox}[Chain-of-Draft]
Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.
Return the answer at the end of the response after a separator \#\#\#\#.
\end{promptbox}
\end{center}

We evaluated each task with two of the most popular flagship models: GPT-4o (gpt-4o-2024-08-06) from OpenAI and Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) from Anthropic. 


\subsection{Arithmetic Reasoning}
We first consider math problems that measure the arithmetic reasoning capabilities of LLMs.  
GSM8k~\cite{gsm8k} has emerged as the benchmark of choice for evaluating arithmetic reasoning in language models, providing a comprehensive dataset of 8,500 diverse grade-school-level mathematical problems. Each problem is paired with a detailed step-by-step solution, emphasizing arithmetic, geometry, algebra, and logical reasoning skills. 

The evaluation results are presented in Table~\ref{tab:gsm8k}. The dataset poses significant challenges for both GPT-4o and Claude 3.5 Sonnet when using standard prompting, yielding accuracies of 53.3\% and 64.6\%, respectively. However, with the application of the CoT, both models surpass 95\% accuracy, albeit at the expense of generating approximately 200 tokens per response. In contrast, CoD achieves an accuracy of 91\% for both models while requiring only about 40 tokens per response, thereby reducing the average output token count by 80\% and cutting the average latency by 76.2\% and 48.4\%, respectively.



\begin{table}[!ht]
\centering
\fontsize{8.5}{8}\selectfont
\begin{tabular}{llrrr}
\toprule
\textbf{Model} & \textbf{Prompt} & \textbf{Accuracy} & \textbf{Token \#} & \textbf{Latency}\\
\midrule
\multirow{4}{*}{GPT-4o} & Standard & 53.3\% & 1.1 & 0.6 s\\
\cmidrule{2-5}
& CoT & 95.4\% & 205.1 & 4.2 s\\
\cmidrule{2-5}
& CoD & 91.1\% & 43.9 & 1.0 s\\
\midrule
\multirow{4}{*}{\shortstack[l]{Claude 3.5 \\Sonnet}} & Standard & 64.6\% & 1.1 & 0.9 s \\
\cmidrule{2-5}
& CoT & 95.8\% & 190.0 & 3.1 s\\
\cmidrule{2-5}
& CoD & 91.4\% & 39.8 & 1.6 s\\
\bottomrule
\end{tabular}
\caption{GSM8K evaluation results.}
\label{tab:gsm8k}
\end{table}

\subsection{Commonsense Reasoning}
We evaluate the tasks of date understanding and sports understanding from BIG-bench to demonstrate the effectiveness of CoD in common sense reasoning. For consistency, we use the same system prompts as those employed in the arithmetic reasoning evaluation.

The evaluation results, presented in Table~\ref{tab:date_understanding}, show that CoD significantly reduces both latency and cost by generating considerably fewer tokens in responses compared to CoT. Additionally, CoD outperforms CoT in accuracy in various cases. 
Notably, chain-of-thought prompting leads to excessively verbose responses for Claude 3.5 Sonnet, especially in the sports understanding task, where CoD reduces the average output tokens from 189.4 to 14.3 â€” a 92.4\% reduction.


\begin{table}[!ht]
\centering
\fontsize{8}{8}\selectfont
\begin{tabular}{llrrr}
\toprule
\textbf{Model} & \textbf{Prompt} & \textbf{Accuracy} & \textbf{Token \#} & \textbf{Latency}\\
\midrule
\multirow{4}{*}{GPT-4o} & Standard & 72.6\% & 5.2 & 0.6 s\\
\cmidrule{2-5}
& CoT & 90.2\% & 75.7 & 1.7 s\\
\cmidrule{2-5}
& CoD & 88.1\% & 30.2 & 1.3 s\\
\midrule
\multirow{4}{*}{\shortstack[l]{Claude 3.5 \\Sonnet}} & Standard & 84.3\% & 5.2 & 1.0 s \\
\cmidrule{2-5}
& CoT & 87.0\% & 172.5 & 3.2 s\\
\cmidrule{2-5}
& CoD & 89.7\% & 31.3 & 1.4 s\\
\bottomrule
\end{tabular}
\caption{Date understanding evaluation results.}
\label{tab:date_understanding}
\end{table}

\begin{table}[!ht]
\centering
\fontsize{8}{8}\selectfont
\begin{tabular}{llrrr}
\toprule
\textbf{Model} & \textbf{Prompt} & \textbf{Accuracy} & \textbf{Token \#} & \textbf{Latency}\\
\midrule
\multirow{4}{*}{GPT-4o} & Standard & 90.0\% & 1.0 & 0.4 s\\
\cmidrule{2-5}
& CoT & 95.9\% & 28.7 & 0.9 s\\
\cmidrule{2-5}
& CoD & 98.3\% & 15.0 & 0.7 s\\
\midrule
\multirow{4}{*}{\shortstack[l]{Claude 3.5 \\Sonnet}} & Standard & 90.6\% & 1.0 & 0.9 s \\
\cmidrule{2-5}
& CoT & 93.2\% & 189.4 & 3.6 s\\
\cmidrule{2-5}
& CoD & 97.3\% & 14.3 & 1.0 s\\
\bottomrule
\end{tabular}
\caption{Sports understanding evaluation results.}
\label{tab:sports_understanding}
\end{table}


\subsection{Symbolic Reasoning}
The original CoT paper~\cite{cot} introduces the task of coin flipping,
where the LLMs are asked to predict which side is up after a sequence of coin flip actions.
Since the exact dataset is not published, we synthesize a test set of 250 examples following the same design.
Specifically, we randomly chose 4 out of the top 1000 first names in the US region according to NameDataset~\cite{name_dataset} and randomly decided to flip the coin or not for each name. 
An example of the evaluation data is shown below. 
%Figure~\ref{fig:coin_flip_example}.

%\begin{figure}[!ht]
%\centering
\begin{center}
\begin{promptbox}
Q: A coin is heads up. Robyn flips the coin. Peggy flips the coin. Grant flips the coin. Vanessa does not flip the coin. Is the coin still heads up?
\newline
A: No.
\end{promptbox}
\end{center}
%\caption{Example evaluation data for coin flip task.}
%\label{fig:coin_flip_example}
%\end{figure}

The evaluation results for GPT-4o and Claude 3.5 Sonnet are shown in Table~\ref{tab:coin_flip}. They achieve 73.2\% and 85.2\% with standard prompting, respectively. However, both models reach a perfect 100\% accuracy with CoT and CoD. Again, CoD demonstrates significant reduction of tokens compared to CoT, from 68\% for GPT-4o to 86\% for Claude 3.5 Sonnet.

\begin{table}[!ht]
\centering
\fontsize{8}{8}\selectfont
\begin{tabular}{llrrr}
\toprule
\textbf{Model} & \textbf{Prompt} & \textbf{Accuracy} & \textbf{Token \#} & \textbf{Latency}\\
\midrule
\multirow{4}{*}{GPT-4o} & Standard & 73.2\% & 1.0 & 0.4 s\\
\cmidrule{2-5}
& CoT & 100.0\% & 52.4 & 1.4 s\\
\cmidrule{2-5}
& CoD & 100.0\% & 16.8 & 0.8 s\\
\midrule
\multirow{4}{*}{\shortstack[l]{Claude 3.5 \\Sonnet}} & Standard & 85.2\% & 1.0 & 1.2 s \\
\cmidrule{2-5}
& CoT & 100.0\% & 135.3 & 3.1 s\\
\cmidrule{2-5}
& CoD & 100.0\% & 18.9 & 1.6 s\\
\bottomrule
\end{tabular}
\caption{Coin flip evaluation results.}
\label{tab:coin_flip}
\end{table}


 

\subsection{Limitaitons of CoD}

\noindent {\bf Inconsistency Without Few-shot Examples}

\noindent We evaluated the performance of CoD under zero-shot setting, where no few-shot examples were provided. The results, presented in Table~\ref{tab:zero_shot}, indicate a significant decline in CoD's effectiveness. Notably, for Claude 3.5 Sonnet, CoD improved performance over direct answering by only 3.6\%. Additionally, the token savings achieved by CoD are less significant compared to few-shot setting.

We hypothesize that this limitation arises due to the scarcity or absence of CoD-style reasoning patterns in the training data of large language models, making it a challenging task to generate concise and insightful ``drafts'' without guidance from few-shot examples. 

\begin{table}[!ht]
\centering
\fontsize{8.5}{8}\selectfont
\begin{tabular}{llrrr}
\toprule
\textbf{Model} & \textbf{Prompt} & \textbf{Accuracy} & \textbf{Token \#} & \textbf{Latency}\\
\midrule
\multirow{4}{*}{GPT-4o} & Standard & 56.9\% & 2.2 & 0.5 s\\
\cmidrule{2-5}
& CoT & 94.8\% & 278.4 & 8.1 s\\
\cmidrule{2-5}
& CoD & 84.4\% & 76.4 & 2.6 s\\
\midrule
\multirow{4}{*}{\shortstack[l]{Claude 3.5 \\Sonnet}} & Standard & 61.9\% & 5.2 & 0.9 s \\
\cmidrule{2-5}
& CoT & 90.4\% & 248.8 & 3.5 s\\
\cmidrule{2-5}
& CoD & 65.5\% & 73.7 & 1.6 s\\
\bottomrule
\end{tabular}
\caption{Zero-shot GSM8K evaluation results.}
\label{tab:zero_shot}
\end{table}

\noindent {\bf Reduced Performance on Small Models}

\noindent We tested CoD on several small language models with fewer than 3B parameters, including Qwen2.5 1.5B/3B instruct~\cite{qwen25}, Llama 3.2 3B instruct~\cite{llama3}, and our in-house Zoom SLM 2.3B model~\cite{zoom-slm}. While CoD effectively reduces the number of tokens required per response and improves accuracy over direct answer, its performance gap compared to CoT is more pronounced in these models. 

Similar to the zero-shot setting, we suspect this is due to the absence of CoD-style data in the training process. We anticipate that fine-tuning these models with additional CoD-formatted data could significantly enhance their reasoning accuracy with CoD.


\begin{table}[!ht]
\centering
\fontsize{8.5}{8}\selectfont
\begin{tabular}{llrr}
\toprule
\textbf{Model} & \textbf{Prompt} & \textbf{Accuracy} & \textbf{Token \#}\\
\midrule
\multirow{4}{*}{Qwen2.5-1.5B-Instruct} & Standard & 5.7\% & 6.6 \\
\cmidrule{2-4}
& CoT & 32.5\% & 141.4 \\
\cmidrule{2-4}
& CoD & 24.2\% & 75.1\\
\midrule
\multirow{4}{*}{Qwen2.5-3B-Instruct} & Standard & 7.2\% & 3.4 \\
\cmidrule{2-4}
& CoT & 59.1\% & 236.4 \\
\cmidrule{2-4}
& CoD & 43.1\% & 41.2\\
\midrule
\multirow{4}{*}{Llama3.2-3B-Instruct} & Standard & 3.9\% & 16.6  \\
\cmidrule{2-4}
& CoT & 70.7\% & 195.3 \\
\cmidrule{2-4}
& CoD & 52.5\% & 98.1\\
\midrule
\multirow{4}{*}{\shortstack[l]{Zoom-SLM-2.3B}} & Standard & 5.9\% & 3.8  \\
\cmidrule{2-4}
& CoT & 77.7\% & 129.0 \\
\cmidrule{2-4}
& CoD & 50.9\% & 55.6 \\
\bottomrule
\end{tabular}
\caption{GSM8K evaluation results on small language models.}
\label{tab:slm}
\end{table}

%CoD style data is not used, or at least very small portion in the training corpus of large language models, making it a challenging task to generate concise and insightful ``draft'' if the model is not strong enough, especially without guidance from fewshot examples. We do expect after fine tuning with CoD style data, we can see a much improved accuracy even on smaller models without fewshot examples. 

