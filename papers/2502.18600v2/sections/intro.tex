

\section{Introduction}
Recent advances in reasoning models such as OpenAI o1~\cite{o1} and DeepSeek R1~\cite{r1} have propelled large language models (LLMs) to unprecedented performance on complex tasks using techniques like Chain of Thought (CoT)~\cite{cot}.
This paradigm encourages models to break down problems into step-by-step explorations, mimicking the structured reasoning process of humans. 
While effective, this approach demands substantially more computational resources at inference time, leading to verbose outputs and higher latency. 
Such verbosity contrasts sharply with how humans typically approach problem-solving: we rely on concise drafts or shorthand notes to capture essential insights without unnecessary elaboration.
%Large Language Models (LLMs) have achieved significant success in solving complex reasoning tasks, often leveraging techniques like Chain of Thought (CoT). This paradigm encourages models to break down problems into step-by-step explanations, mimicking the structured reasoning process of humans. While effective, this approach often results in verbose outputs, requiring substantial computational resources and increasing latency. Such verbosity contrasts sharply with how humans typically approach problem-solving: we rely on concise drafts or shorthand notes to capture essential ideas without unnecessary elaboration.

Motivated by this difference, we propose Chain of Draft (CoD), a novel prompting strategy that aligns more closely with human reasoning by prioritizing efficiency and minimalism. Instead of verbose intermediate steps, Chain of Draft encourages LLMs to generate concise, dense-information outputs at each step. This approach reduces latency and computational costs without sacrifice of accuracy, making LLMs more practical for real-world applications where efficiency is paramount.

\begin{figure}[t]
    \vspace{-1.5em}
    \centering
    \includegraphics[width=0.4\textwidth]{plot.png}
    \caption{Comparison of Claude 3.5 Sonnet's accuracy and token usage across different tasks with three different prompt strategies: direct answer (Standard), Chain of Thought (CoT), and Chain of Draft (CoD). CoD achieves similar accuracy as CoT while using significant fewer tokens.}
    \label{fig:plot}
\end{figure}

The intuition behind Chain of Draft is rooted in how humans externalize thought. When solving complex tasks — whether solving mathematical problems, drafting essays, or coding — we often jot down only the critical pieces of information that help us progress. By emulating this behavior, LLMs can focus on advancing toward solutions without the overhead of verbose reasoning.

To evaluate the effectiveness of Chain of Draft, we conducted experiments across a variety of benchmarks requiring multi-step reasoning, including arithmetic reasoning, common sense reasoning, and symbolic reasoning. Our results demonstrate that this minimalist approach maintains or even improves accuracy compared with standard Chain of Thought, while significantly reducing token usage and latency. %Additionally, we analyze the trade-offs between brevity and interpretability, discussing how Chain of Draft can be fine-tuned to strike the right balance for specific applications.

The contributions of this paper are threefold:
\begin{itemize}
    \item We introduce Chain of Draft, a concise reasoning prompting strategy inspired by human cognitive processes.
    \item We empirically validate that Chain of Draft can achieve significantly reduced latency and cost without sacrificing accuracy.
    \item We discuss the implications of Chain of Draft for LLM design, deployment, and real-world usability.
\end{itemize}

% By reducing computational overhead and aligning LLMs more closely with human-like reasoning, Chain of Draft offers a promising direction for developing faster, smarter, and more resource-efficient AI systems.

% The rest of the paper is organized as follows: ...