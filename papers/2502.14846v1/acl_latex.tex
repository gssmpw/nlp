% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{color}
\usepackage{placeins}
\usepackage{booktabs} 
\usepackage{float}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{adjustbox}
\usepackage{commath}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{caption}
\usepackage{longtable}
\usepackage{array}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{tcolorbox}
\usepackage{diagbox}

\usepackage{algorithm}
\usepackage{algorithmic}

\newcommand{\yy}[1]{\textcolor{blue}{\bf\small [#1 --Yue]}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Scaling Code Guided Synthetic Multimodal Data \\ for Text-Rich Image Understanding}
\title{Scaling Text-Rich Image Understanding \\ via Code-Guided Synthetic Multimodal Data Generation}


\author{Yue Yang$^{*1}$, Ajay Patel$^{*1}$, Matt Deitke$^{2}$, Tanmay Gupta$^{2}$, Luca Weihs$^{2}$, \\ \textbf{Andrew Head$^{1}$,  Mark Yatskar$^{1}$, Chris Callison-Burch$^{1}$, } \\ \textbf{Ranjay Krishna$^{2}$, Aniruddha Kembhavi$^{2}$, Christopher Clark$^{2}$}\\
$^{1}$University of Pennsylvania, $^{2}$Allen Institute for Artificial Intelligence\\
 {\small $^*$ Equal Contribution \hspace{.5cm} \tt{\{yueyang1, ajayp\}@seas.upenn.edu} \hspace{.5cm} \href{https://yueyang1996.github.io/cosyn/}{yueyang1996.github.io/cosyn} } }

\begin{document}
\maketitle
\begin{abstract}
Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). 
However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. 
To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. 
Given input text describing a target domain (e.g., ``nutrition fact labels''), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. 
With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM.
Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. 
Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. 
Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.


\end{abstract}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{images/teaser.pdf}
    \vspace{-.6cm}
    \caption{Given a novel task (e.g., answering questions about nutrition facts), our code-guided generation system can produce targeted synthetic data to enhance the performance of VLMs on that specific task.}
    \label{fig: teaser}
    \vspace{-0.3cm}
\end{figure}

\section{Introduction}
\input{sections/introduction}

\section{Related Work}
\input{sections/related_work}

\section{Problem Formulation}
\input{sections/formulation}

\section{CoSyn System} \label{sec: cosyn}
\input{sections/method}

\section{Experimental Setup}
\input{sections/experiment}

\section{Results}

\input{sections/results}

\section{Conclusion}

In this work, we introduced CoSyn, a framework for generating synthetic data that significantly enhances VLM performance on text-rich image understanding.
Our comprehensive analysis highlights the advantages of synthetic data for domain generalization, data efficiency, and bias mitigation. 
Our work demonstrates that the coding capabilities of text-only LLMs can effectively assist multimodal learning and unleash the potential of vision-language models for real-world applications.

\section*{Limitation}
The effectiveness of synthetic data depends heavily on the quality and diversity of the prompts and rendering pipelines used for data generation. 
For highly specialized or underrepresented domains, generating sufficiently diverse data remains challenging and may require careful prompt engineering or additional customization of rendering tools.
Targeted synthetic data generation may be essential for certain tasks to achieve adequate performance, and ensuring relevance and coverage still requires domain-specific expertise. Synthetic data also may not fully capture the complexity of real-world data in some scenarios. Therefore, improving the diversity and realism of synthetic data to better support models in highly variable or evolving domains is a reasonable avenue for future research.
Finally, our current synthetic data is limited to English and may require further extension for multilingual support.

\section*{Ethical Statement}

To the best of our knowledge, this work presents no significant ethical concerns. We note, however, that the use of synthetic data can propagate biases present in the generation model used. Conversely, synthetic data can also help mitigate biases and expand coverage, as demonstrated in this work, by greatly expanding the domains present in vision-language instruction-tuning training data to yield stronger generalized performance.

\section*{Acknowledgement}
This work was done during Yue Yang's internship at the PRIOR team of Ai2.
This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract \#2022-22072200005, and the Defense Advanced Research Projects Agency's (DARPA) SciFy program (Agreement No. HR00112520300), and gifts from the UPenn ASSET center and Ai2. 
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, DARPA, or the U.S. Government. 
The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes, notwithstanding any copyright annotation therein.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\clearpage
\appendix
% \onecolumn

\input{sections/appendix}
\end{document}
