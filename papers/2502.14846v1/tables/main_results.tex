\begin{table*}[!ht]
    \small
    \centering
    \setlength{\tabcolsep}{4.5pt}
    \begin{tabular}{lcccccccccc}
        \toprule
        \textbf{Model} & \textbf{ChartQA} & \textbf{DocVQA} & \textbf{InfoVQA} & \textbf{TableVQA} & \textbf{AI2D} & \textbf{TextVQA} & \textbf{ScreenQA} & \textbf{Average} \\
        \midrule
        \textcolor{gray}{GPT-4V} & \textcolor{gray}{78.1} & \textcolor{gray}{87.2} & \textcolor{gray}{75.1} & \textcolor{gray}{60.5} & \textcolor{gray}{89.4} & \textcolor{gray}{78.0} & \textcolor{gray}{41.6} & \textcolor{gray}{72.8} \\
        \textcolor{gray}{Gemini 1.5 Flash} & \textcolor{gray}{85.4} & \textcolor{gray}{89.9} & \textcolor{gray}{75.3} & \textcolor{gray}{72.6} & \textcolor{gray}{91.7} & \textcolor{gray}{78.7} & \textcolor{gray}{40.1} & \textcolor{gray}{76.2} \\
        \textcolor{gray}{Claude-3 Opus} & \textcolor{gray}{80.8} & \textcolor{gray}{89.3} & \textcolor{gray}{55.6} & \textcolor{gray}{70.0} & \textcolor{gray}{88.1} & \textcolor{gray}{67.5} & \textcolor{gray}{39.8} & \textcolor{gray}{70.2} \\ \midrule
        PaliGemma-3B$^\dagger$ & 71.4 & 84.8 & 47.8 & 46.4 & 73.3 & 76.5 & 32.2 & 61.8  \\
        BLIP-3-4B$^\dagger$ & 60.0 & 61.4 & 31.5 & 24.3 & 74.2 & 71.0 & 26.2 & 49.8 \\ 
        Cambrian-7B$^\dagger$ & 73.3 & 77.8 & 41.6 & 40.6 & 73.0 & 71.7 & 44.4 & 64.2 \\
        LLaVA-1.5-7B$^\dagger$$^*$ & 17.8 & 28.1 & 25.8 & 33.1 & 55.5 & 58.2 & 17.6 & 33.7 \\
        LLaVA-Next-8B$^\dagger$ & 69.5 & 78.2 & 43.8 & 43.9 & 71.6 & 65.3 & 34.2 &  58.1 \\ 
        LLaVA-OneVision-7B$^\dagger$ & 80.0 & 87.5 & \underline{68.8} & 64.6 & \underline{81.4} & \underline{78.3} & 46.3 & 72.4 \\
        Pixtral-12B & 81.8 & \textbf{90.7} & 50.8 & \textbf{67.0} & 79.0 & 75.7 & 39.4 & 69.2 \\ 
        Llama 3.2 11B & \underline{83.4} & 88.4 & 63.6 & 51.1 & \textbf{91.9} & 73.1 & \textbf{87.7} & \underline{77.0} \\ \midrule
        \cellcolor{gray!10}Ours (7B)$^\dagger$ & \cellcolor{gray!10}\textbf{86.3} & \cellcolor{gray!10}\underline{90.0} & \cellcolor{gray!10}\textbf{70.5} & \cellcolor{gray!10}\underline{65.8} & \cellcolor{gray!10}\textbf{91.9} &  \cellcolor{gray!10}\textbf{82.0} & \cellcolor{gray!10}\underline{80.1} & \cellcolor{gray!10}\textbf{80.9} \\ 
        \cellcolor{gray!10}Ours (7B-zero-shot)$^\dagger$$^*$ & \cellcolor{gray!10}80.8 & \cellcolor{gray!10}82.9 & \cellcolor{gray!10}59.8 & \cellcolor{gray!10}64.9 & \cellcolor{gray!10}83.9 & \cellcolor{gray!10}72.7 & \cellcolor{gray!10}78.1 & \cellcolor{gray!10}74.7  \\
        \bottomrule
    \end{tabular}
    \vspace{-.1cm}
    \caption{\textbf{Results on 7 text-rich benchmarks.} The result of the best-performing open-source model is \textbf{bold}, and the second-best is \underline{underlined}. Models with $^\dagger$ stand for open data and code for multimodal training. Models with $^*$ are zero-shot models, which means the models are not trained on instances from any of the evaluation datasets.}
    \label{tab:model_performance}
    \vspace{-.3cm}
\end{table*}

