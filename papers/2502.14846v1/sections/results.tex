
This section covers (1) the competitive performance of the model trained on our synthetic data (Sec \ref{sec: main_results}), (2) the comprehensive analyses to highlight the benefits of synthetic data (Sec \ref{sec: analysis}), and (3) the effectiveness of synthetic pointing data in improving VLMs for web agent tasks (Sec \ref{sec: pointing}).

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{images/bar_ablation.pdf}
    \vspace{-.8cm}
    \caption{\textbf{Ablation on training data selection.} Aux, Syn, and Eval stand for auxiliary, synthetic, and evaluation datasets, respectively. We report the average score on eight benchmarks. The detailed performance breakdown on each benchmark is in Table \ref{tab:train_data_ablation}.}
    \label{fig: ablation}
    \vspace{-0.3cm}
\end{figure}

\subsection{Main Results} \label{sec: main_results}

Table \ref{tab:model_performance} compares our model’s performance with both open and closed models across seven text-rich benchmarks. 
On average, our 7B model achieves the highest performance, surpassing the second-best model (Llama 3.2 11B) by 3.9\%. 
Notably, our model ranks first in four out of the seven datasets and second in the remaining three.
More surprisingly, our zero-shot model (the last row in Table \ref{tab:model_performance}) outperforms most open and closed models without exposure to any training instances from the evaluation datasets. 
In contrast, these competing models often rely on benchmark training data and are thus not true zero-shot models. 
This result demonstrates that the capabilities learned from our synthetic data can transfer effectively to downstream tasks.



\subsection{Analysis} \label{sec: analysis}
In the following experiments, we quantify the contribution of synthetic data to the benchmark performance by ablating the combinations of fine-tuning datasets. 
Then, we demonstrate that our CoSyn system can efficiently assist VLMs in generalizing to novel tasks. 
Finally, we show that synthetic data can help mitigate the overfitting of biases.
\smallbreak
\noindent \textbf{Synthetic data boosts the performance.} Table \ref{fig: ablation} presents an ablation study on the choices of supervised fine-tuning data. 
In the zero-shot settings, when the model is trained on auxiliary datasets (over 1M training images not directly from the evaluation tasks), it fails to generalize effectively to the evaluation tasks, with a substantial performance gap of 14.1\% below GPT-4V.
However, using only 400K synthetic samples achieves a performance comparable to GPT-4V. 
Our best zero-shot model surpasses GPT-4V when jointly training synthetic and auxiliary data.
Under the supervised settings, training with in-domain data alone yields strong performance. 
However, adding 1M auxiliary samples provides a modest improvement of 1.4\%, while incorporating synthetic data results in a more significant 3.6\% boost. 
These results demonstrate the effectiveness of synthetic data in enhancing VLMs' performance on text-rich tasks.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{images/nutrition.pdf}
    \vspace{-.6cm}
    \caption{\textbf{Zero shot performance on NutritionQA.} The x-axis denotes the number of training examples used for the instruction-tuning stage. The models on the upper left side demonstrate better data efficiency.}
    \label{fig: nutrition}
    \vspace{-0.3cm}
\end{figure}

\smallbreak
\noindent \textbf{Zero-shot Generalization on a Novel Task.} Vision-language models typically rely on in-domain data to perform well on specific tasks. 
When encountering a novel task, such as answering questions about nutrition labels in Figure \ref{fig: teaser}, models without seeing similar examples during training may struggle with this novel task. 
However, our CoSyn system enables controllable data generation. 
Given the task name as input, CoSyn can generate task-specific data to fine-tune the model.

To validate this, we annotated a small 
evaluation dataset called \href{https://huggingface.co/datasets/yyupenn/NutritionQA}{NutritionQA}, which includes 100 examples of questions about photos of nutrition labels. 
Some questions require multi-hop reasoning, as Figure \ref{fig: nurition_qa} illustrates. 
We evaluated GPT-4V and several open-source VLMs on this dataset and report the performance in Figure \ref{fig: nutrition}.
The x-axis in Figure \ref{fig: nutrition} represents the amount of data used during the instruction fine-tuning stage.

Despite being trained on millions of images, we observe that open-source VLMs are not data-efficient and perform poorly on this novel task compared to GPT-4V.
Although many open-source VLMs claim to achieve GPT-4V-level performance, they fall short when tested on new tasks in the wild. 
Without synthetic data, our model (Eval + Aux) achieves results similar to those of open models. 
However, when trained on 400K synthetic samples, our model matches GPT-4V’s performance.

More impressively, we used CoSyn to generate 7K synthetic nutrition label samples and fine-tuned the model using only this 7K data. 
The resulting model outperforms most open-source VLMs on the NutritionQA task. 
These results demonstrate that code-guided synthetic data is an effective and efficient method for adapting VLMs to new domains.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{images/cot_bar.pdf}
    \vspace{-.6cm}
    \caption{\textbf{Ablation of using Chain-of-Thought reasoning.} Short Answer represents prompting model to output the answer as short as possible. $+$ CoT stands for providing Chain-of-Thought reasoning before giving the final answer. Results on all datasets are in Table \ref{tab:cot}.}
    \label{fig: cot}
    \vspace{-0.2cm}
\end{figure}

\smallbreak
\noindent \textbf{Synthetic Data for Chain-of-Thought Reasoning.} Existing text-rich datasets, such as ChartQA \cite{masry-etal-2022-chartqa}, are typically annotated with short answers. 
However, questions like ``Compute the mean of the data in the plot'' require step-by-step mathematical reasoning to arrive at the correct answer. 
Models trained only with short-answer supervision may fail to learn proper plot comprehension, but instead overfitting to annotation biases in these datasets. 
On the contrary, our CoSyn-400K includes explanation text alongside the short answer. 
Each instruction-tuning example consists of a (\textit{question}, \textit{explanation}, \textit{short answer}) triplet, enabling models to learn chain-of-thought (CoT) reasoning.
During fine-tuning, we design two prompt templates for our synthetic data:
\begin{center} 
\begin{tcolorbox}[top=2pt, bottom=2pt, width=\linewidth, boxrule=1pt]
{\small {\fontfamily{put}\selectfont
\textbf{CoT Prompt}: \textbf{<Question>} Provide reasoning steps and then give the short answer.

\textbf{<Explanation>}
Answer: \textbf{<Answer>}
}}
\end{tcolorbox}

\begin{tcolorbox}[top=2pt, bottom=2pt, width=\linewidth, boxrule=1pt]
{\small {\fontfamily{put}\selectfont
\textbf{Short Answer Prompt}: \textbf{<Question>} Answer with as few words as possible. \textbf{<Answer>}
}}
\end{tcolorbox}
\end{center}

Those prompts allow VLMs to switch between the two answering styles and perform CoT reasoning when necessary. Figure \ref{fig: cot} shows that incorporating CoT reasoning improves performance on ChartQA, TableVQA, and NutritionQA, as these datasets contain examples requiring multi-hop reasoning.
However, we observe that adding CoT reasoning reduces performance on DocVQA and InfoVQA. 
We find this decline is caused by answer biases in these benchmarks. 
Specifically, the ground-truth answers favor short responses, often penalizing more detailed and verbal responses. 
For instance, in DocVQA, the ground-truth for an example is ``T-Th'', whereas the model responds with ``Tuesday to Thursday''. 
Although the response is correct, the strict string-matching metric assigns it a zero score.
This highlights key limitations of current multimodal benchmarks, including answering biases and rigid evaluation metrics that fail to capture the full extent of a model’s capabilities.

\input{tables/chartqa}
\smallbreak
\noindent \textbf{Synthetic Data for Mitigating Biases.} Our previous experiments reveal answering biases in multimodal benchmarks, which VLMs trained solely on these datasets often inherit. To further validate this issue, we analyze ChartQA and observe a distribution shift in question types. 
As shown in the pie charts above Table \ref{tab:chartqa}, some ChartQA questions are human-annotated, while others are generated by the language model T5 \citep{raffel2020exploring}, which is heavily influenced by prompt phrasing and limited to a fixed set of question templates.
During training, most questions (73.9\%) in ChartQA are machine-generated, while the test set contains an even distribution of human-annotated and machine-generated questions. Models trained exclusively on ChartQA tend to overfit to T5-generated questions. Table \ref{tab:chartqa} illustrates this issue: PaliGemma \citep{beyer2024paligemma} and ChartPali \citep{carbune2024chart} achieve high accuracy on machine-generated questions but experience a significant performance drop of over 30\% on human-annotated questions.

Similarly, without synthetic data, our model shows a noticeable 21.8\% gap between the two question types. 
However, incorporating synthetic data during training reduces this gap to 14.2\%, improving the model’s ability to answer human-asked questions. This suggests that synthetic data can mitigate overfitting on benchmarks and enhance VLMs' usability in real-world applications.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{images/pointing_new.pdf}
    \vspace{-.6cm}
    \caption{\textbf{The overview of enabling VLMs to point through synthetic data.} (a) We synthesize pointing data by prompting an LLM to generate pointing questions and edit the code to draw the answer points explicitly. (b) We demonstrate that the VLM trained on synthetic pointing data can be generalized to real agentic tasks.}
    \label{fig: pointing}
    \vspace{-.1cm}
\end{figure*}

\subsection{Synthetic Pointing Data} \label{sec: pointing}
Pointing enables vision-language models to answer questions by providing specific points on images. 
This functionality allows models to ground their responses in visual content and interact with environments, which is crucial for developing digital agents.
We find that we can synthesize pointing data using our code-guided generation system.

\smallbreak
\noindent \textbf{Method.}
Since we have access to the source code for all generated images, we can prompt an LLM to modify the code to draw points on the images explicitly. 
As illustrated in Figure \ref{fig: pointing}, we feed the image’s source code as context to the LLM, which generates a pointing question and edits the code to draw points with a predefined color. 
By extracting the pixel values of these points, we can obtain their exact $(x, y)$ coordinates.\footnote{The coordinates of points are normalized to (0, 100) to mitigate the influence of image resolution.} 
We then use this pointing data to train VLMs, enabling them to answer questions by providing point coordinates. 
In total, we generate pointing data for 65K synthetic images. 
Figure \ref{fig: point_example} shows some qualitative examples from our synthetic pointing dataset.

\smallbreak
\noindent \textbf{Setup.}
We evaluate pointing ability on ScreenSpot \cite{cheng2024seeclick}, where the task requires models to provide the correct click location based on a given instruction. ScreenSpot contains screenshots from mobile phones, desktops, and web pages. 
To assess the effectiveness of our synthetic pointing data, we compare it to the model trained on PixMo-point \cite{deitke2024molmo}, which consists of 155K human-annotated images. 
Our best-performing model uses both PixMo-point and synthetic pointing data.
Additionally, we compare against existing methods like CogAgent \cite{hong2024cogagent}, SeeClick \cite{cheng2024seeclick}, and UGround \cite{gou2024navigating}, which is trained on 1.3M screenshots.

\smallbreak
\noindent \textbf{Results.}
Table \ref{tab:click} compares the click accuracy of our models with previous methods.
Using 65K synthetic pointing samples, our model achieves performance comparable to the one trained on 155K human-annotated samples.
When combining synthetic and human data, our model achieves state-of-the-art performance on ScreenSpot, surpassing the recent UGround \cite{gou2024navigating}, which was trained on 1.3M screenshots. These results demonstrate that synthetic pointing data is a data-efficient approach for improving VLM performance on agentic tasks involving click prediction.


\input{tables/click}