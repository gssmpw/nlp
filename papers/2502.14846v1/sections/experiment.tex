\input{tables/main_results}
Our experiments aim to verify the value of our synthetic data in the supervised fine-tuning stage of training vision-language models.
This section introduces the architecture of our model, training strategy, datasets we used, baselines for comparison, and other details on implementation.
\smallbreak
\noindent \textbf{Model Architecture.} We follow the same image preprocessing and architecture as Molmo \cite{deitke2024molmo}, which uses the MLP layer to connect the vision encoder and a pretrained LLM. We choose OpenAIâ€™s CLIP (ViT-L/14 336px) \cite{clip} as the vision backbone and Mistral-7B \cite{jiang2023mistral} as the language model.
\smallbreak
\noindent \textbf{Training Process.} We adopt the same training strategy as Molmo \cite{deitke2024molmo}, which consists of two stages: (1) \textit{Pre-training} on dense captions from \href{https://huggingface.co/datasets/allenai/pixmo-cap}{PixMo-Cap} and (2) \textit{Supervised fine-tuning} on three categories of datasets below:

\begin{itemize}[noitemsep, topsep=0pt, leftmargin=*]
    \item \textbf{Evaluation Datasets.} We evaluate our model on seven text-rich benchmarks, including ChartQA \cite{masry-etal-2022-chartqa}, DocVQA \cite{mathew2021docvqa}, InfographicVQA \cite{mathew2022infographicvqa}, TableVQA-Bench \cite{kim2024tablevqa}, AI2 Diagrams \cite{kembhavi2016diagram}, TextVQA \cite{singh2019towards}, and ScreenQA \cite{baechler2024screenai}. We adopt their official metrics for calculating performance. In total, we have 138K training images from the evaluation datasets.\footnote{TableVQA is an eval-only benchmark (no training split), and we do not use the training split from ScreenQA.}

    \item \textbf{Auxiliary Datasets.} We select additional academic datasets for fine-tuning: VQAv2 \cite{balanced_vqa_v2}, GQA \cite{hudson2019gqa}, OK-VQA \cite{okvqa}, OCR-VQA \cite{mishraICDAR19}, A-OKVQA \cite{schwenk2022okvqa}, ScienceQA \cite{lu2022learn}, TabMWP \cite{lu2023dynamic}, ST-VQA \cite{biten2019scene}, TallyQA \cite{acharya2019tallyqa}, DVQA \cite{kafle2018dvqa}, FigureQA \cite{kahou2017figureqa}, and PlotQA \cite{methani2020plotqa}. The auxiliary datasets contain around 1M training images.

    \item \textbf{Synthetic Datasets.} As introduced in Sec \ref{sec: cosyn} and also shown in Figure \ref{fig: dataset}, our synthetic datasets include 400K text-rich images from 9 categories.
\end{itemize}

\noindent Our best-performing model uses all three categories of datasets above. We also trained a zero-shot model using only auxiliary and synthetic data without any examples from the evaluation datasets, which still exhibits competitive benchmark performance, as shown in the last row of Table \ref{tab:model_performance}.
\smallbreak
\noindent \textbf{Baselines.} We compare recent open-source VLMs with a similar scale (7B), including PaliGemma-3B \cite{beyer2024paligemma}, BLIP-3-4B \cite{xue2024xgen}, Cambrian-7B \cite{tong2024cambrian}, LLaVA-1.5-7B \cite{liu2023llava}, LLaVA-Next-8B \cite{liu2024llavanext}, LLaVA OneVision-7B \cite{li2024llava}, Pixtral-12B \cite{agrawal2024pixtral}, Llama 3.2 V \cite{meta2024llama}.
We also include proprietary models: GPT-4V \cite{gpt4}, Gemini-1.5-Flash \cite{team2024gemini}, and Claude-3 Opus \cite{anthropic2024claude}.
\smallbreak
\noindent \textbf{Implementation Details.} We train our model on TPU v3-128 with a batch size of 32. Our best-performing model is trained for 60K steps, taking about 30 hours. The checkpoints with the highest validation performance are retained for testing.