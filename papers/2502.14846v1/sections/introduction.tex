Instruction-tuned vision-language models (VLMs) have shown strong performance across a range of multimodal tasks \citep{clip,gpt4,liu2023llava}.
However, these tasks typically focus on general image understanding over natural images rather than the specialized reasoning required for text-rich images such as charts, documents, diagrams, signs, labels, and screenshots.
Understanding and reasoning over text-rich images is crucial for many applications, including analyzing scientific literature and figures \cite{asai2024openscholar}, improving accessibility for users with visual impairments \cite{gurari2018vizwiz}, and enabling agentic workflows in real-world environments \cite{OSWorld}.
Effectively interpreting these structured visual formats requires both textual comprehension and spatial reasoning, which current models struggle with due to the limited availability of high-quality, realistic, and diverse vision-language datasets \citep{methani2020plotqa}.

To address these challenges and inspired by the fact that text-rich images are typically rendered from code, we develop \textbf{Co}de Guided \textbf{Syn}thetic data generation system (\textbf{CoSyn}), a flexible framework for generating diverse synthetic text-rich multimodal data for vision-language instruction tuning.
As illustrated in Figure \ref{fig: system}, CoSyn can generate multimodal data for various target domains from a short natural language query, such as \textit{book covers}.
CoSyn leverages text-only LLMs, which excel at code generation, to produce both data and code that render diverse text-rich images using 11 supported rendering tools (e.g., Python, HTML, LaTeX). 
Grounded in the underlying code representation of the images, textual instructions are also generated by the text-only LLM to create vision-language instruction-tuning datasets.

Using this framework, we construct the CoSyn-400K, as shown in Figure \ref{fig: dataset}, a large-scale and diverse synthetic vision-language instruction-tuning dataset tailored for text-rich image understanding.
We comprehensively evaluate the effectiveness of training on CoSyn-generated synthetic data across seven text-rich VQA benchmarks. 
Our model achieves state-of-the-art performance among competitive open-source models and surpasses proprietary models such as GPT-4V and Gemini 1.5.
Notably, training on CoSyn synthetic data enables sample-efficient learning, achieving stronger performance with less data.
In addition, CoSyn can synthesize chain-of-thought (CoT) reasoning data \citep{wei2022chain}, improving performance on tasks requiring multi-hop reasoning. 
A fine-grained analysis of question types in ChartQA \cite{masry-etal-2022-chartqa} reveals that training on CoSyn-400K results in stronger generalization to human-written questions. 
In contrast, models trained solely on existing academic datasets often overfit to biased training data, overperforming on templated or machine-generated questions but struggling with more realistic, human-asked queries.

We then identify a key limitation of open-source VLMs that they struggle to generalize to out-of-domain tasks they were not trained on.
As shown in Figure \ref{fig: teaser}, we introduce NutritionQA, a novel benchmark for understanding photos of nutrition labels, with practical applications like aiding users with visual impairments.
Open-source VLMs perform poorly on this novel task, even after training on millions of images. 
However, by training on CoSyn-400K, our model adapts strongly to this novel domain in a zero-shot setting with significantly less training data.
Remarkably, by generating just 7K in-domain synthetic nutrition label examples using CoSyn for fine-tuning, our model surpasses most open VLMs trained on millions of images. 
This highlights CoSynâ€™s data efficiency and ability to help VLMs adapt to new domains through targeted synthetic data generation.

Finally, beyond the standard VQA task, we use CoSyn to generate synthetic \textit{pointing} training data, which is particularly useful in agentic tasks. 
The pointing data enables VLMs to retrieve coordinates for specific elements in a screenshot given a query like ``Point to the Checkout button'' \citep{deitke2024molmo}.
Our model trained on synthetic pointing data achieves state-of-the-art performance on the ScreenSpot click prediction benchmark \citep{baechler2024screenai}.
Overall, our work demonstrates that synthetic data is a promising solution for advancing vision-language models in understanding text-rich images and unlocking their potential as multimodal digital assistants for real-world applications.