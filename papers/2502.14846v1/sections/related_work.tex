\textbf{Vision Language Models.} \citet{tsimpoukelli2021multimodal} first demonstrate that pre-trained, frozen language models can be extended to process visual inputs.
Previous works fuse vision and language modalities using different strategies, such as cross-attention mechanisms \citep{alayrac2022flamingo} and Q-Former \citep{li2023blip}. 
More recent architectures have converged on using MLP layers to project visual features into the language space \citep{liu2023llava}.
However, these architectures are often imbalanced, with the language backbone substantially larger than the visual encoder. 
As a result, without high-quality image-text data, models may overly rely on language priors, leading to hallucinations in their responses \citep{bai2024hallucination}.
Our work addresses this issue by generating high-quality multimodal data for text-rich images.

\smallbreak
\noindent \textbf{Text-rich Images Understanding.} Chart understanding and text-rich image understanding continue to challenge state-of-the-art models as naturally occurring vision-language data that can support training for understanding text-rich images is still scarce \citep{kahou2017figureqa,kafle2018dvqa,xu2023chartbench,mukhopadhyay-etal-2024-unraveling}. In addition to charts and plots, a number of datasets address other kinds of text-rich images such as documents, infographics, diagrams and figures, and screenshots \citep{figureseer,mathew2021docvqa,mathew2022infographicvqa,baechler2024screenai,roberts2024scifibench} have been made available. Many of these benchmarks are limited in size and scope, diversity of visualization types, and question types, making them suitable for evaluation but not for training data that could lead to generalized performance.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{images/system.pdf}
    \vspace{-.7cm}
    \caption{The overview of our \textbf{Co}de Guided \textbf{Syn}thetic data generation system (\textbf{CoSyn}), which has 20 generation pipelines based on 11 render tools. Given a user query, e.g., ``book cover,'' CoSyn selects the appropriate pipelines and starts with generating diverse topics conditioned on personas, then synthesizes detailed data for code generation. The code renders the image and is also fed as context for an LLM to construct instruction-tuning data.}
    \label{fig: system}
    \vspace{-0.3cm}
\end{figure*}

\smallbreak
\noindent \textbf{Synthetic Data for VLM.} Generating synthetic images with annotations grounded in known source representations has been widely used in domains with limited vision-language data \citep{virtualworldannotation,clevr,simvqa,provision}. 
This approach has been applied to chart and plot VQA typically using a limited small set of chart types and by instantiating handcrafted question templates \citep{kahou2017figureqa,kafle2018dvqa,methani2020plotqa,leafqa++}.
Following this, \citet{scigraphqa} and \citet{chartbasedreasoning} explore using text-only LLMs to generate annotations from tables or text descriptions associated with charts to train VLMs. 
Other recent approaches, similar to our procedure, explore generating data and code to render synthetic charts \cite{chartllama,sbsfigures,chartx}.
Molmo \cite{deitke2024molmo} releases a synthetic text-rich image dataset, \href{https://huggingface.co/datasets/allenai/pixmo-docs}{PixMo-docs}, but smaller in scale and diversity than ours.
These works generate synthetic data that is still highly limited in terms of the diversity of topics, figure types, and rendering pipelines, which is important for generalizing to out-of-distribution tasks. 
In our work, we expand the scope beyond charts to encompass a wider range of diverse text-rich images.