
Given a text query $q$ about an image type, e.g., \textit{flow charts}, our goal is to create a synthetic multimodal dataset $\mathcal{D}_q = \left\{\left(I, T\right)\right\}$, where $I$ is the image, and $T$ is the textual instruction-tuning data (e.g., question-answer pairs). 
$\mathcal{D}_q$ is used to train a VLM to improve its ability to understand images related to $q$. 
The core idea of our approach is using code $C$ as the intermediate representation to bridge the image and text. 
The overall generation process can be decomposed as follows:
\begin{equation*}
    P\left(I, T | q\right) = P_{\text{LM}}\left(C|q\right) \cdot P\left(I|C\right) \cdot P_{\text{LM}}\left(T|C\right)
\end{equation*}
where $P_\text{LM}\left(C|q\right)$ represents prompting a language model to generate code $C$, which is executed to render the image, $P\left(I|C\right)$.
$P_\text{LM}\left(T|C\right)$ uses code $C$ (without the image) as context for an LLM to generate the textual instruction-tuning data.