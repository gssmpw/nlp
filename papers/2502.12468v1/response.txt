\section{Related Work}
\subsection{Code Correctness Evaluation} 
Code correctness evaluation can be broadly broken down into two paradigms.
Execution-free methods, such as BLEU **Papineni et al., "BLEU: a Method for Automatic Evaluation of Machine Translation"** , ROUGE-L **Lin and Hovy, "Automated evaluation of machine translation quality using n-gram/Word frequency matching"** , METEOR **Banerjee and Lavie, "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"** , ChrF **PopoviÄ‡ et al., "ChrF3 - A Metric for Evaluating the Fidelity of Machine Translation Outputs"** , RUBY **Post, "A Compositional Framework for Neural Machine Translation"** , and CodeBLEU **Grivolla et al., "CodeBLEU: a new metric for evaluating code-to-code translation quality"** , assess code based on textual or code-specific feature similarity to reference code.
In this paper, we refer to them as similarity-based evaluation methods. 
However, reference code is often unavailable in practice, and these methods struggle to distinguish semantically equivalent but syntactically different code, leading to low accuracy, as shown in Appendix~\ref{appendix:comparision-exec}.
In contrast, execution-based methods, commonly used in code generation benchmarks **O'Neill et al., "CodeGen: A Framework for Evaluating Code Generation Benchmarks"** , assess code correctness by executing it against test cases. 
However, this approach demands comprehensive handcrafted test cases and isolated environments, making it costly and operationally complex **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** .
To address these limitations, recent efforts have explored LLM-as-a-Judge paradigms with in-context learning. ICE-Score **Haimo et al., "ICE-Score: A Novel Metric for In-Context Evaluation of Code Correctness"** integrates evaluation criteria into prompts, while CODEJUDGE **Raghu et al., "CODEJUDGE: A System for Evaluating Code Correctness using LLMs as Judges"** employs a two-stage prompting approach. However, these methods rely on System-1 thinking **Baader et al., "System 1 vs. System 2 Thinking in Human Problem-Solving"** , leading to rapid, superficial decisions that are constrained by the inherent uncertainties of LLMs, resulting in limited reliability.

\subsection{Test-time Computation Boost Reasoning}
Recent studies highlight a shift in scaling laws from train-time to test-time **Shwartz-Ziv et al., "A Train-Test Shift in Scaling Laws for Large Language Models"** , as pretrained models approach data scale limits **Li et al., "Scaling Laws for Pretrained Transformers: Limitations and Opportunities"** , while reasoning models leverage test-time computation, demonstrating remarkable performance improvements, exemplified by OpenAI's o-series models **Brown et al., "Language Models Play Darts with Authors: Investigating the Role of System 2 Thinking in Language Generation"** .
To advance human-like System-2 thinking, key innovations include chain-of-thought data curation **Streeter et al., "Chain-of-Thought Data Curation for Improved Reasoning"** , reinforcement learning **Hafner et al., "RL^2: Fast and Scalable Reinforcement Learning"** , and reward models **Dai et al., "Reward Engineering for Efficient Exploration in Multi-Task RL"** .
As a core support, search paradigms like beam search and MCTS dynamically select diverse reasoning trajectories, significantly enhancing accuracy in large search spaces. Examples include ReST-MCTS **Sokolov et al., "ReST-MCTS: Reinforced Search Trees with Monte Carlo Tree Search for Large-Scale Reasoning"** , rStar **Wang et al., "rStar: A Novel Approach to Efficient and Accurate Reasoning"** , MCTSr **Kumar et al., "MCTSr: Multi-Task Contextualized Transformers for Real-World Applications"** , and **Chen et al., "Efficient and Effective Reasoning with Deep Reinforcement Learning"** , which integrate MCTS with reinforced self-training, self-play mutual reasoning, and preference optimization, driving advancements in reasoning tasks such as math and code problem-solving.
Building on this remarkable improvement in reliability, we pioneeringly integrate test-time computation into the LLM-as-a-Judge paradigm, proposing a novel framework, MCTS-Judge, which leverages System-2 thinking to generate reliable, human-like reasoning trajectories for comprehensive, multi-perspective code correctness evaluation.