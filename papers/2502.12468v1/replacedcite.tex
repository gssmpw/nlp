\section{Related Work}
\subsection{Code Correctness Evaluation} 
Code correctness evaluation can be broadly broken down into two paradigms.
Execution-free methods, such as BLEU____, ROUGE-L____, METEOR____, ChrF____, RUBY____, and CodeBLEU____, assess code based on textual or code-specific feature similarity to reference code.
In this paper, we refer to them as similarity-based evaluation methods. 
However, reference code is often unavailable in practice, and these methods struggle to distinguish semantically equivalent but syntactically different code, leading to low accuracy, as shown in Appendix~\ref{appendix:comparision-exec}.
In contrast, execution-based methods, commonly used in code generation benchmarks____, assess code correctness by executing it against test cases. 
However, this approach demands comprehensive handcrafted test cases and isolated environments, making it costly and operationally complex____.
To address these limitations, recent efforts have explored LLM-as-a-Judge paradigms with in-context learning. ICE-Score____ integrates evaluation criteria into prompts, while CODEJUDGE____ employs a two-stage prompting approach. However, these methods rely on System-1 thinking____, leading to rapid, superficial decisions that are constrained by the inherent uncertainties of LLMs, resulting in limited reliability.

\subsection{Test-time Computation Boost Reasoning}
Recent studies highlight a shift in scaling laws from train-time to test-time____, as pretrained models approach data scale limits____, while reasoning models leverage test-time computation, demonstrating remarkable performance improvements, exemplified by OpenAI's o-series models____.
To advance human-like System-2 thinking, key innovations include chain-of-thought data curation____, reinforcement learning____, and reward models____.
As a core support, search paradigms like beam search and MCTS dynamically select diverse reasoning trajectories, significantly enhancing accuracy in large search spaces. Examples include ReST-MCTS____, rStar____, MCTSr____, and____, which integrate MCTS with reinforced self-training, self-play mutual reasoning, and preference optimization, driving advancements in reasoning tasks such as math and code problem-solving.
Building on this remarkable improvement in reliability, we pioneeringly integrate test-time computation into the LLM-as-a-Judge paradigm, proposing a novel framework, MCTS-Judge, which leverages System-2 thinking to generate reliable, human-like reasoning trajectories for comprehensive, multi-perspective code correctness evaluation.