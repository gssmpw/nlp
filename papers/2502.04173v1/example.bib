
@INPROCEEDINGS{Moore99,
  AUTHOR =       "R. Moore and J. Lopes",
  TITLE =        "Paper templates",
  BOOKTITLE =    "TEMPLATE'06, 1st International Conference on Template Production",
  YEAR =         "1999",
  publisher =    "SCITEPRESS",
  file = F
}

@BOOK{Smith98,
  AUTHOR =       "J. Smith",
  TITLE =        "The Book",
  PUBLISHER =    "The publishing company",
  YEAR =         "1998",
  address =      "London",
  edition =      "2nd",
  file = F
}


@inproceedings{zhou-etal-2019-bert,
    title = "{BERT}-based Lexical Substitution",
    author = "Zhou, Wangchunshu  and
      Ge, Tao  and
      Xu, Ke  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1328",
    doi = "10.18653/v1/P19-1328",
    pages = "3368--3373"
}

@inproceedings{michalopoulos-etal-2022-lexsubcon,
    title = "{L}ex{S}ub{C}on: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution",
    author = "Michalopoulos, George  and
      McKillop, Ian  and
      Wong, Alexander  and
      Chen, Helen",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.87",
    doi = "10.18653/v1/2022.acl-long.87",
    pages = "1226--1236"
}

@inproceedings{mccarthy-navigli-2007-semeval,
    title = "{S}em{E}val-2007 Task 10: {E}nglish Lexical Substitution Task",
    author = "McCarthy, Diana  and
      Navigli, Roberto",
    booktitle = "Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007)",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S07-1009",
}

@inproceedings{melamud-etal-2015-simple,
    title = "A Simple Word Embedding Model for Lexical Substitution",
    author = "Melamud, Oren  and
      Levy, Omer  and
      Dagan, Ido",
    booktitle = "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-1501",
    doi = "10.3115/v1/W15-1501",
    pages = "1--7",
}

@inproceedings{seneviratne-etal-2022-cilex,
    title = "{CIL}ex: An Investigation of Context Information for Lexical Substitution Methods",
    author = "Seneviratne, Sandaru  and
      Daskalaki, Elena  and
      Lenskiy, Artem  and
      Suominen, Hanna",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.362",
}

@inproceedings{kremer-etal-2014-substitutes,
    title = "What Substitutes Tell Us - Analysis of an {``}All-Words{''} Lexical Substitution Corpus",
    author = "Kremer, Gerhard  and
      Erk, Katrin  and
      Pad{\'o}, Sebastian  and
      Thater, Stefan",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E14-1057",
    doi = "10.3115/v1/E14-1057",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{vstajner2022lexical,
  title={Lexical simplification benchmarks for English, Portuguese, and Spanish},
  author={{\v{S}}tajner, Sanja and Ferr{\'e}s, Daniel and Shardlow, Matthew and North, Kai and Zampieri, Marcos and Saggion, Horacio},
  journal={Frontiers in Artificial Intelligence},
  volume={5},
  pages={991242},
  year={2022},
  publisher={Frontiers},
  doi={10.3389/frai.2022.991242}
}

@article{fu2019paraphrase,
  title={Paraphrase generation with latent bag of words},
  author={Fu, Yao and Feng, Yansong and Cunningham, John P},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/5e2b66750529d8ae895ad2591118466f-Abstract.html}
}

@inproceedings{helbig-etal-2020-challenges,
    title = "Challenges in Emotion Style Transfer: An Exploration with a Lexical Substitution Pipeline",
    author = "Helbig, David  and
      Troiano, Enrica  and
      Klinger, Roman",
    booktitle = "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.socialnlp-1.6",
    doi = "10.18653/v1/2020.socialnlp-1.6",
    pages = "41--50",
    abstract = "We propose the task of emotion style transfer, which is particularly challenging, as emotions (here: anger, disgust, fear, joy, sadness, surprise) are on the fence between content and style. To understand the particular difficulties of this task, we design a transparent emotion style transfer pipeline based on three steps: (1) select the words that are promising to be substituted to change the emotion (with a brute-force approach and selection based on the attention mechanism of an emotion classifier), (2) find sets of words as candidates for substituting the words (based on lexical and distributional semantics), and (3) select the most promising combination of substitutions with an objective function which consists of components for content (based on BERT sentence embeddings), emotion (based on an emotion classifier), and fluency (based on a neural language model). This comparably straight-forward setup enables us to explore the task and understand in what cases lexical substitution can vary the emotional load of texts, how changes in content and style interact and if they are at odds. We further evaluate our pipeline quantitatively in an automated and an annotation study based on Tweets and find, indeed, that simultaneous adjustments of content and emotion are conflicting objectives: as we show in a qualitative analysis motivated by Scherer{'}s emotion component model, this is particularly the case for implicit emotion expressions based on cognitive appraisal or descriptions of bodily reactions.",
}

@inproceedings{agrawal-carpuat-2019-controlling,
    title = "Controlling Text Complexity in Neural Machine Translation",
    author = "Agrawal, Sweta  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1166",
    doi = "10.18653/v1/D19-1166",
    abstract = "This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of news articles available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting dataset makes it possible to train multi-task sequence to sequence models that can translate and simplify text jointly. We show that these multi-task models outperform pipeline approaches that translate and simplify text independently.",
}

@inproceedings{reddy2016obfuscating,
  title={Obfuscating gender in social media writing},
  author={Reddy, Sravana and Knight, Kevin},
  booktitle={Proceedings of the First Workshop on NLP and Computational Social Science},
  pages={17--26},
  year={2016},
  doi={10.18653/v1/W16-5603}
}

@inproceedings{bo-etal-2021-er,
    title = "{ER}-{AE}: Differentially Private Text Generation for Authorship Anonymization",
    author = "Bo, Haohan  and
      Ding, Steven H. H.  and
      Fung, Benjamin C. M.  and
      Iqbal, Farkhund",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.314",
    doi = "10.18653/v1/2021.naacl-main.314",
    pages = "3997--4007",
    abstract = "Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed model on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation.",
}

@inproceedings{Zhou2021DefenseAS,
  title={Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble},
  author={Yi Zhou and Xiaoqing Zheng and Cho-Jui Hsieh and Kai-Wei Chang and Xuanjing Huang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  doi={10.18653/v1/2021.acl-long.426}
}

@inproceedings{Zhang2015CharacterlevelCN,
  title={Character-level Convolutional Networks for Text Classification},
  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  booktitle={NIPS},
  year={2015},
  doi={10.5555/2969239.2969312}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press},
  doi={10.1162/neco.1997.9.8.1735}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
}

@inproceedings{reimers-2019-sentence-bert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils and Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "http://arxiv.org/abs/1908.10084",
    doi={10.18653/v1/D19-1410}
}

@inproceedings{seneviratne2022cilex,
  title={CILex: An Investigation of Context Information for Lexical Substitution Methods},
  author={Seneviratne, Sandaru and Daskalaki, Elena and Lenskiy, Artem and Suominen, Hanna},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={4124--4135},
  year={2022}
}

@inproceedings{lacerra2021alasca,
  title={ALaSca: an Automated approach for Large-Scale Lexical Substitution.},
  author={Lacerra, Caterina and Pasini, Tommaso and Tripodi, Rocco and Navigli, Roberto},
  booktitle={IJCAI},
  pages={3836--3842},
  year={2021},
  doi={10.24963/ijcai.2021/528}
}

@inproceedings{lin-etal-2022-improving,
    title = "Improving Contextual Representation with Gloss Regularized Pre-training",
    author = "Lin, Yu  and
      An, Zhecheng  and
      Wu, Peihao  and
      Ma, Zejun",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.68",
    doi = "10.18653/v1/2022.findings-naacl.68",
    pages = "907--920",
    abstract = "Though achieving impressive results on many NLP tasks, the BERT-like masked language models (MLM) encounter the discrepancy between pre-training and inference. In light of this gap, we investigate the contextual representation of pre-training and inference from the perspective of word probability distribution. We discover that BERT risks neglecting the contextual word similarity in pre-training. To tackle this issue, we propose an auxiliary gloss regularizer module to BERT pre-training (GR-BERT), to enhance word semantic similarity. By predicting masked words and aligning contextual embeddings to corresponding glosses simultaneously, the word similarity can be explicitly modeled. We design two architectures for GR-BERT and evaluate our model in downstream tasks. Experimental results show that the gloss regularizer benefits BERT in word-level and sentence-level semantic representation. The GR-BERT achieves new state-of-the-art in lexical substitution task and greatly promotes BERT sentence representation in both unsupervised and supervised STS tasks.",
}

@inproceedings{hassan2007unt,
    title = "{UNT}: {S}ub{F}inder: Combining Knowledge Sources for Automatic Lexical Substitution",
    author = "Hassan, Samer  and
      Csomai, Andras  and
      Banea, Carmen  and
      Sinha, Ravi  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007)",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S07-1091",
}

@inproceedings{yuret2007ku,
  title={KU: Word sense disambiguation by substitution},
  author={Yuret, Deniz},
  booktitle={Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)},
  pages={207--214},
  year={2007}
}

@inproceedings{mccarthy2002lexical,
  title={Lexical substitution as a task for wsd evaluation},
  author={McCarthy, Diana},
  booktitle={Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions},
  pages={89--115},
  year={2002}
}

@article{mccarthy2009english,
  title={The English lexical substitution task},
  author={McCarthy, Diana and Navigli, Roberto},
  journal={Language resources and evaluation},
  volume={43},
  pages={139--159},
  year={2009},
  publisher={Springer}
}

@inproceedings{erk-pado-2008-structured,
    title = "A Structured Vector Space Model for Word Meaning in Context",
    author = "Erk, Katrin  and
      Pad{\'o}, Sebastian",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    address = "Honolulu, Hawaii",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D08-1094",
    pages = "897--906",
}

@inproceedings{dinu-lapata-2010-measuring,
    title = "Measuring Distributional Similarity in Context",
    author = "Dinu, Georgiana  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D10-1113",
    pages = "1162--1172",
}

@inproceedings{thater2010contextualizing,
  title={Contextualizing semantic representations using syntactically enriched vector models},
  author={Thater, Stefan and F{\"u}rstenau, Hagen and Pinkal, Manfred},
  booktitle={Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  pages={948--957},
  year={2010}
}

@inproceedings{DBLP:journals/corr/abs-1301-3781,
  author       = {Tom{\'{a}}s Mikolov and
                  Kai Chen and
                  Greg Corrado and
                  Jeffrey Dean},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Efficient Estimation of Word Representations in Vector Space},
  booktitle    = {1st International Conference on Learning Representations, {ICLR} 2013,
                  Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year         = {2013},
  url          = {http://arxiv.org/abs/1301.3781},
  timestamp    = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{roller2016pic,
  title={PIC a different word: A simple model for lexical substitution in context},
  author={Roller, Stephen and Erk, Katrin},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year={2016}
}

@inproceedings{melamud-etal-2016-context2vec,
    title = "context2vec: Learning Generic Context Embedding with Bidirectional {LSTM}",
    author = "Melamud, Oren  and
      Goldberger, Jacob  and
      Dagan, Ido",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1006",
    doi = "10.18653/v1/K16-1006",
    pages = "51--61",
}

@article{DBLP:journals/corr/abs-1907-11692,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{lee-etal-2021-swords,
    title = "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality",
    author = "Lee, Mina  and
      Donahue, Chris  and
      Jia, Robin  and
      Iyabor, Alexander  and
      Liang, Percy",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.345",
    doi = "10.18653/v1/2021.naacl-main.345",
}

@inproceedings{qiang-etal-2023-parals,
    title = "{P}ara{LS}: Lexical Substitution via Pretrained Paraphraser",
    author = "Qiang, Jipeng  and
      Liu, Kang  and
      Li, Yun  and
      Yuan, Yunhao  and
      Zhu, Yi",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.206",
    doi = "10.18653/v1/2023.acl-long.206",
    pages = "3731--3746"
}

@article{qiang2020BERTLS,
  title =  {Lexical Simplification with Pretrained Encoders },
  author = {Qiang, Jipeng and 
            Li, Yun and
            Yi, Zhu and
            Yuan, Yunhao and 
            Wu, Xindong},
  journal={Thirty-Fourth AAAI Conference on Artificial Intelligence},
  pages={8649â€“8656},
  year  =  {2020}
}

@article{qiang2023natural,
  title={Natural language watermarking via paraphraser-based lexical substitution},
  author={Qiang, Jipeng and Zhu, Shiyu and Li, Yun and Zhu, Yi and Yuan, Yunhao and Wu, Xindong},
  journal={Artificial Intelligence},
  volume={317},
  pages={103859},
  year={2023},
  publisher={Elsevier}
}

@article{gehrmann2023repairing,
  title={Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text},
  author={Gehrmann, Sebastian and Clark, Elizabeth and Sellam, Thibault},
  journal={Journal of Artificial Intelligence Research},
  volume={77},
  pages={103--166},
  year={2023}
}

@article{chen1998evaluation,
  title={Evaluation metrics for language models},
  author={Chen, Stanley F and Beeferman, Douglas and Rosenfeld, Roni},
  year={1998},
  publisher={Carnegie Mellon University},
  url={https://kilthub.cmu.edu/articles/Evaluation_Metrics_For_Language_Models/6605324/files/12095765.pdf}
}

@misc{Minixhofer_GerPT2_German_large_2020,
author = {Minixhofer, Benjamin},
doi = {10.5281/zenodo.5509984},
month = {12},
title = {{GerPT2: German large and small versions of GPT2}},
url = {https://github.com/bminixhofer/gerpt2},
year = {2020}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf}
}

@inproceedings{meisenbacher-etal-2024-dp,
    title = "{DP}-{MLM}: Differentially Private Text Rewriting Using Masked Language Models",
    author = "Meisenbacher, Stephen  and
      Chevli, Maulik  and
      Vladika, Juraj  and
      Matthes, Florian",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.554/",
    doi = "10.18653/v1/2024.findings-acl.554",
    pages = "9314--9328"
}

@inproceedings{vladika-etal-2022-tum,
    title = "{TUM} sebis at {G}erm{E}val 2022: A Hybrid Model Leveraging {G}aussian Processes and Fine-Tuned {XLM}-{R}o{BERT}a for {G}erman Text Complexity Analysis",
    author = "Vladika, Juraj  and
      Meisenbacher, Stephen  and
      Matthes, Florian",
    booktitle = "Proceedings of the GermEval 2022 Workshop on Text Complexity Assessment of German Text",
    month = sep,
    year = "2022",
    address = "Potsdam, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.germeval-1.9/",
    pages = "51--56",
    abstract = "The task of quantifying the complexity of written language presents an interesting endeavor, particularly in the opportunity that it presents for aiding language learners. In this pursuit, the question of what exactly about natural language contributes to its complexity (or lack thereof) is an interesting point of investigation. We propose a hybrid approach, utilizing shallow models to capture linguistic features, while leveraging a fine-tuned embedding model to encode the semantics of input text. By harmonizing these two methods, we achieve competitive scores in the given metric, and we demonstrate improvements over either singular method. In addition, we uncover the effectiveness of Gaussian processes in the training of shallow models for text complexity analysis."
}