\section{RELATED WORK}
\label{sec:related}
%Considering its linguistic nature focusing on lexical semantics, lexical substitution is one of the older and well-established tasks in the NLP research community. It
%Lexical substitution was formally defined by ____ and later features in a SemEval 2007 competition focusing on English lexical substitution ____. For this competition, a number of early important approaches were developed,
%. They introduced a foundational component of most LS approaches, which is 
%most of them rule-based systems utilizing external knowledge sources and lexicons with synonym sets
%. These approaches were rule-based expert systems that use various heuristics for querying the databases and re-ranking the candidates 
%____. 
%Follow-up work focused on developing more sophisticated scoring functions that could successfully determine the best substitutes, while still maintaining external bases as the main source of replacement candidates.
%Another line of research was in vector-space modeling approaches, which model the distributional sparse-vector representations based on the syntactic context of the sentence ____. With the advent of word embeddings in the form of dense vectors, such as word2vec ____ 
%and GloVe ____, 
%LS approaches started increasingly modeling the target words and surrounding context with vectors in the embedding space and ranking the candidates with vector-similarity metrics ____. 
%Even though these approaches improved the discovery of similar words with no external resources, they rarely took into account preserving the sentence semantics.

Lexical substitution was formally defined by ____.
%and later featured as a shared task in SemEval 2007 ____. 
Early approaches used rule-based heuristics and synonym lookup in word thesauri ____.
%or distributional sparse-vector representations ____. 
With the advent of word embedding methods,
%like word2vec ____, 
LS approaches began representing the target word and its context with dense vectors and ranking the candidates with vector-similarity metrics ____. Most recent approaches utilize pre-trained language models (PLMs). 

Word representations learned by PLMs are highly contextual and finding substitutes that fit the word's surrounding context was significantly improved. Since the prediction of substitutes in this manner can be highly biased towards the target word, ____ apply a dropout mechanism by resetting some dimensions of the target word's embedding and then leverage the model to predict substitutes using this perturbed embedding. Later methods by ____ and ____ combine the PLM word embeddings with a gloss value of target word's synonyms returned by WordNet
%, enhanced with an auxiliary gloss regularizing component from ____,
to guide the vector-space exploration towards similar words. ____ generate substitutes with paraphrase modeling and improved decoding.

Unlike the approaches that use WordNet for embedding perturbation, our approach includes querying WordNet for a rule-based filtering of unsuitable words.
%(like antonyms and related forms of the target word). 
To the best of our knowledge, we are the first to utilize the idea of sentence concatenation for improved LS and the first to do a qualitative analysis of generated substitutes with human evaluation.%, as is common practice in other NLG tasks. 
%Instead of focusing just on benchmark results, we aim to conduct a qualitative investigation into the generated substitutes, in order to assess their usefulness in real-world text rewriting use cases.

% Alasca ____, LexSubCon ____, Cilex ____. %Gloss loss ____