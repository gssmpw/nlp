\section{Introduction}
\label{sec:intro}
% Image-to-Video Generation 

Image-to-video (I2V) generation is a powerful technique that brings still images to life, enabling widespread applications in the fields of content creation, advertising, and art animation. Typically, I2V generation aims to animate a reference image according to user-defined control signals, such as text, object motion, and camera motion, while maintaining the quality and natural dynamics of the generated video. Driven by advances in diffusion-based generative models and extensive web-scale data \cite{bain2021frozen}, recent methods have significantly enhanced the ability to generate temporally coherent and visually compelling videos from limited inputs such as a single image or sparse annotations \cite{guo2023animatediff,chen2023videocrafter1,xing2025dynamicrafter,guo2024i2v,wang2024motionctrl,he2024cameractrl,li2024image,geng2024motion}. However, achieving precise and simultaneous control over multiple video attributes, such as camera motion, object motion, and lighting direction, remains a critical yet challenging objective.



Existing approaches typically address these control attributes independently or in a partially integrated manner. Methods focusing exclusively on camera control, such as CameraCtrl \cite{he2024cameractrl} and ViewCrafter, do not adequately handle detailed object motion or dynamic lighting variations. Similarly, object-motion-centric frameworks, including DragAnything \cite{wu2025draganything}, lack integrated capabilities for precise camera and lighting control. 
For lighting control, techniques like DILightNet \cite{zeng2024dilightnet} and NeuLighting \cite{li2022neulighting} provide lighting control but often lack generalizability due to reliance on specific categories (e.g., human faces) and object-specific annotations. Other methods \cite{li2022neulighting, zhang2021neural, huynh2021new, zhang2021neural, huynh2021new} also primarily target narrow domains, such as human faces, further restricting their applicability to open-domain scenarios.

Simultaneous and fine-grained control of camera motion, object motion, and lighting direction introduces multiple significant challenges: (1) Accurate camera motion control from single-image inputs requires robust extraction of sufficient 3D geometric information. (2) Realistic and detailed object motion control demands effective representation and refinement of sparse motion trajectories without compromising visual fidelity. (3) Dynamic lighting control necessitates integrating illumination adjustments coherently with both camera and object motion to ensure temporal consistency and visual realism.

To overcome these challenges, we propose VidCRAFT3, which integrates three core components. First, the Image2Cloud module leverages DUSt3R~\cite{wang2024dust3r} to reconstruct a 3D point cloud from a single reference image, enabling precise camera motion control by rendering the point cloud along user-defined camera motion trajectory. Second, ObjMotionNet encodes sparse object motion trajectories by extracting multi-scale motion features from Gaussian-smoothed optical flow maps to guide realistic object motion. Third, Spatial Triple-Attention Transformer integrates lighting direction control by integrating lighting embedding with image-text embeddings through parallel cross-attention layers, ensuring consistent illumination effects. 
The capabilities of these modules are shown in Fig. \ref{Figure:Teaser}, showcasing VidCRAFT3's control over camera motion, object motion, and lighting direction.
To address data scarcity, we introduce the VideoLightingDirection (VLD) dataset, providing synthetic yet highly realistic video sequences with accurate per-frame lighting annotations. Additionally, we develop a three-stage training strategy that enables effective learning without joint multi-element annotations.

The main contributions of this paper are:
(1) VidCRAFT3 is the first I2V framework to achieve simultaneous control over camera motion, object motion, and lighting direction through a disentangled architecture combining Image2Cloud, ObjMotionNet, and Spatial Triple-Attention Transformer.
(2) VLD dataset provides synthetic videos accompanied by accurate per-frame lighting direction annotations, effectively addressing the scarcity of annotated real-world datasets.
(3) A three-stage training strategy enables robust multi-element control without requiring real-world videos annotated with camera, object, and lighting signals.
(4) Extensive experiments demonstrate that VidCRAFT3 achieves state-of-the-art performance surpassing existing methods in terms of control precision, visual quality, and generalization capabilities.

