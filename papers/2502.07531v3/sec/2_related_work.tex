\section{Related Work}
\subsection{Image-to-video Generation}
Image-to-video (I2V) generation \cite{guo2023animatediff,chen2023videocrafter1,chen2024videocrafter2,xing2025dynamicrafter,guo2024i2v,guo2024sparsectrl} aims to animate static images into dynamic videos while preserving visual content and introducing realistic motion. 
Recent advances in diffusion models \cite{sohl2015deep,ho2020denoising}  have revolutionized video generation by extending pre-trained Text-to-Image (T2I) models like AnimateDiff \cite{guo2023animatediff} to incorporate temporal dimensions for motion generation.
These methods integrate the input image as a condition, either through CLIP-based \cite{radford2021learning} image embeddings or by concatenating the image with noisy latent. For example, VideoCrafter1 \cite{chen2023videocrafter1}, DynamiCrafter \cite{xing2025dynamicrafter}, and I2V-Adapter \cite{guo2024i2v} use dual cross-attention layers to fuse image embeddings with noisy frames, ensuring spatial-aligned guidance. Similarly, Stable Video Diffusion (SVD) \cite{blattmann2023stable} replaces text embeddings with CLIP image embeddings, maintaining semantic consistency in an image-only manner.
Another line of work, exemplified by SEINE \cite{chen2023seine}, DynamiCrafter \cite{xing2025dynamicrafter} and PixelDance \cite{zeng2024make}, expands the input channels of diffusion models to concatenate the static image with noisy latents, effectively injecting image information into the model. 
However, these methods preserve input image fidelity while generating dynamic videos but often struggle with fine-grained details due to reliance on global conditions. 


\subsection{Motion-controlled Video Generation}
Motion-controlled video generation \cite{wang2024motionctrl,he2024cameractrl,yin2023dragnuwa} focuses on creating high-fidelity videos with user-defined motion dynamics.
Existing methods can be categorized into \textit{camera motion control}, \textit{object motion control}, and \textit{joint motion control}.
For camera motion control \cite{he2024cameractrl,bahmani2024vd3d,kuang2024collaborative,hou2024training,wang2024cpa}, CamI2V \cite{zheng2024cami2v}, CameraCtrl \cite{he2024cameractrl}, and CamCo \cite{xu2024camco} encode camera parameters into Pl√ºcker embeddings for more discriminative motion representation. 
ViewCrafter \cite{yu2024viewcrafter} and I2VControl-Camera \cite{feng2024i2vcontrolcamera} use 3D representations of reference images to render point clouds and generate partial frames.
For object motion control \cite{mou2024revideo,shi2024dragdiffusion,wang2023videocomposer,jain2024peekaboo,ma2024trailblazer,zhang2024tora,zhou2024trackgo,xu2024motion,pandey2024motion,he2024mojito,feng2024i2vcontrol,namekata2024sgi2v,xiao2025trajectory}, existing methods employ diverse strategies to guide object movements in video synthesis. DragNUWA \cite{yin2023dragnuwa}, Image Conductor \cite{li2024image}, DragAnything \cite{wu2025draganything}, and MotionBridge \cite{tanveer2024motionbridge} utilize sparse optical flow to control object motion, while MOFA-Video \cite{niu2025mofa} and Motion-I2V \cite{shi2024motion} predict dense motion fields to control object motion.  
Alternatively, Boximator \cite{wang2024boximator} and Direct-a-Video \cite{yang2024direct} rely on bounding box trajectories to guide object motion. 
LeviTor \cite{wang2024levitor} combines depth and K-means clustering for precise 3D trajectory control.
Current research on Joint Motion Control \cite{yang2024direct,wang2024motionctrl,geng2024motion,chen2025perception}, aiming to simultaneously control camera and object motions, remains limited. MotionCtrl \cite{wang2024motionctrl} designs specialized modules for precise control, while Motion Prompting \cite{geng2024motion} uses motion tracks for flexible guidance. Perception-as-Control \cite{chen2025perception} further enhances control with 3D-aware motion representations.

Unlike existing methods, we propose VidCRAFT3, \textit{the first framework} to achieve simultaneous control over camera motion, object motion, and lighting direction. By combining 3D point cloud rendering, trajectory learning, and Spatial Triple-Attention Transformer, our approach effectively decouples these elements, ensuring temporal consistency and enhanced realism in complex scenes.


\subsection{Lighting-controlled Visual Generation}
Lighting-controlled visual generation aims to manipulate illumination while preserving scene geometry and materials. 
Previous methods primarily focus on portrait lighting \cite{sun2019single,zhou2019deep,rao2024lite2relight,pandey2021total,nestmeyer2020learning,kim2024switchlight,sengupta2018sfsnet,shu2017portrait,he2024diffrelight,ponglertnapakorn2023difareli,zhang2024lumisculpt}, laying the foundation for effective and accurate illumination modeling.
Recent advances in diffusion models have significantly improved the quality and flexibility of lighting control. Methods like DiLightNet \cite{zeng2024dilightnet} and GenLit \cite{bharadwaj2024genlit} achieve fine-grained and realistic relighting through radiance hints and SVD, respectively. Facial relighting methods, including DifFRelight \cite{he2024diffrelight} and DiFaReli \cite{ponglertnapakorn2023difareli}, produce high-quality portrait images. Frameworks like NeuLighting \cite{li2022neulighting} focus on outdoor scenes using unconstrained photo collections, while GSR \cite{poirier2024diffusion} combines diffusion models with neural radiance fields for realistic 3D-aware relighting. IC-Light \cite{zhangscaling} proposes imposing consistent light transport during training.
Extending lighting control to video \cite{zhang2024lumisculpt} introduces challenges such as temporal consistency and dynamic lighting effects. Recent techniques leverage 3D-aware generative models for temporally consistent relighting, as seen in EdgeRelight360 \cite{lin2024edgerelight360} and ReliTalk \cite{qiu2024relitalk}. Neural rendering approaches \cite{cai2024real,zhang2021neural} use datasets like dynamic OLAT for high-quality portrait video relighting, while reflectance field-based methods \cite{huynh2021new} infer lighting from exemplars.
Despite these advancements, most prior works focus on portraits and HDR lighting conditions. In contrast, VidCRAFT3 targets general scenes, enabling interactive adjustment of lighting directions.