\maketitlesupplementary
\appendix


\section{Preliminary}
\noindent \textbf{Image-to-video generation.} 
Given a reference image $I_{\text{ref}}$ and some text description, image-to-video generation aims to generate a sequence of video frames $I = \{I_1, I_2, \ldots, I_F\}$ with $I_1 = I$. In this work, we also allow the user to provide three forms of control signals: camera motion, object motion, and lighting direction. The video frames will be generated according to the text description and all the control signals provided. 
Specifically, the camera motion is represented by the movement of a set of camera extrinsics $E = \{E_1, E_2, \ldots, E_F\}$; the object motion is represented by a few sparse sequences of image pixels $\{s_1^1, s_1^2, s_1^3, \ldots, s_1^{F}\}, \{s_1^2, s_2^2, s_2^3, \ldots, s_2^{F}\}, \ldots$; the lighting direction is simply a unit 3d vector $L \in \boldsymbol{R}^3$.

\noindent \textbf{Video Diffusion model (VDMs)} represent a class of generative models that extend the principles of image diffusion to the domain of video generation. These models operate by defining a forward diffusion process that gradually transforms an initial video sample \(x_0 \sim p_{\text{data}}(x)\) into Gaussian noise \(x_T \sim \mathcal{N}(0, I)\) over \(T\) timesteps. The reverse process, parameterized by a denoising network \(\epsilon_\theta(x_t, t)\), learns to iteratively denoise the noisy latent representation \(x_t\) to recover the original data \(x_0\). The training objective is formulated as:
\[
\min_{\theta} \mathbb{E}_{t, x \sim p_{\text{data}}, \epsilon \sim \mathcal{N}(0, I)} \|\epsilon - \epsilon_\theta(x_t, t)\|^2_2,
\]
where \(\epsilon\) represents the ground truth noise, and \(\theta\) denotes the learnable parameters of the network. Once trained, the model can generate high-quality videos by sampling from a random noise distribution \(x_T\) and applying the learned denoising process iteratively.

To address the computational challenges associated with high-dimensional video data, Latent Diffusion Models (LDMs) are often employed. In this framework, a video \(x \in \mathbb{R}^{F \times 3 \times H \times W}\) is first encoded into a lower-dimensional latent space \(\mathbf{z} = \mathcal{E}(x)\), where \(\mathbf{z} \in \mathbb{R}^{F \times C \times h \times w}\). The diffusion and denoising processes are then performed in this latent space, significantly reducing computational complexity. The denoising process is conditioned on additional inputs \(\mathbf{c}\), such as text prompts or motion control signals, enabling the generation of videos that adhere to specific semantic or temporal constraints. The final video is reconstructed through a decoder \(\hat{x} = \mathcal{D}(\mathbf{z})\), ensuring both spatial and temporal coherence in the generated output.


\section{VLD Dataset Samples}

Fig. \ref{fig:VLD_samples} illustrates representative samples from our proposed \textit{VideoLightingDirection (VLD) Dataset}, a synthetic dataset specifically designed to model complex light-object interactions. The dataset comprises two types of scenes: (a) \textit{Haven} scenes, in which 3D models from Poly Haven are placed centrally within HDR environments; and (b) \textit{BOP} scenes, featuring randomly positioned BOP models in six-plane textured rooms to simulate indirect lighting. Each subset includes video frames captured under two distinct lighting conditions while maintaining a consistent camera trajectory. This design highlights how changes in lighting direction impact object shading, reflections, and overall visual coherence, providing valuable data for training and evaluating models requiring precise lighting direction annotations.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/VLD_samples.pdf}
    % \vspace{-5pt}
    \caption{Illustrations of samples from the proposed \textit{VideoLightingDirection (VLD) Dataset}, featuring synthetic scenes designed to model complex light-object interactions. (a) \textit{Haven} scenes, where 3D models from Poly Haven are placed at the center of HDR environments. (b) \textit{BOP} scenes, with BOP models randomly positioned within six-plane textured rooms to simulate indirect lighting. Each subset includes video frames captured under two distinct lighting conditions, maintaining consistent camera trajectories.
    }   
    \label{fig:VLD_samples}
    % \vspace{-7pt}
\end{figure}


\section{Qualitative Results of Ablation Study}
In this section, we present qualitative results of the ablation studies described in the main text, providing visual insights into how different design choices impact the generated videos.

\noindent\textbf{Training Strategy.}
Fig. \ref{fig:qual_res_ablation_training_strategy} qualitatively compares videos generated using Dense, Sparse, and Dense+Sparse training strategies. Dense training results exhibit detailed object motion but struggle with precise alignment when tested with sparse trajectories. Sparse training achieves better alignment but at the cost of reduced motion detail. The Dense+Sparse approach effectively combines the strengths of both, delivering superior motion coherence and overall visual quality.

\noindent\textbf{Lighting Embedding Integration Strategies.}
Fig. \ref{fig:qual_res_ablation_lighting_embedding} compares the generated videos of different lighting embedding integration strategies. Lighting Cross-Attention demonstrates clear superiority, providing precise control over lighting directions, resulting in more realistic shadows, reflections, and overall lighting effects compared to Text Cross-Attention and Time Embedding.

\noindent\textbf{Representation of Lighting Direction.}
Fig. \ref{fig:qual_res_ablation_lighting_representation} presents a visual comparison between Fourier Embedding and SH Encoding for representing lighting direction. SH Encoding consistently produces more realistic and detailed lighting effects, including enhanced shading and reflections, further supporting the quantitative findings in the main text.

These qualitative analyses further validate our key design decisions, emphasizing their impact on generating precise and realistic image-to-video translations under controlled camera motion, object motion, and lighting direction.


\section{Additional Qualitative Results}

This section provides additional qualitative comparisons illustrating VidCRAFT3's capabilities across multiple control dimensions.

\noindent\textbf{Camera Motion + Lighting Direction Control.}
Fig. \ref{fig:more_relighting_camera} demonstrates VidCRAFT3's effectiveness in simultaneously controlling camera motion and lighting direction, showing clear advantages in lighting realism and trajectory accuracy compared to original sequences.

\noindent\textbf{Camera Motion + Object Motion Control.}
Fig. \ref{fig:more_camera_object2} presents examples of simultaneous control over camera and object motions. VidCRAFT3 accurately reproduces intended object trajectories and maintains specified camera movements, closely matching the ground truth.

\noindent\textbf{Camera Motion Control.}
Fig. \ref{fig:more_camera} compares VidCRAFT3's results with state-of-the-art methods (CameraCtrl, MotionCtrl, CamI2V) and ground-truth sequences, highlighting superior camera trajectory accuracy and visual coherence.

\noindent\textbf{Object Motion Control.}
Fig. \ref{fig:more_object} provides comparisons against Image Conductor and Motion-I2V, showcasing VidCRAFT3's superior capability in generating realistic object trajectories and visually coherent motion across diverse scenarios.

\noindent\textbf{Lighting Direction Control.}
Fig. \ref{fig:more_relighting} further illustrates VidCRAFT3's precise control over lighting direction, achieving accurate shading, shadows, and reflections, consistent with ground-truth sequences.

These additional qualitative evaluations highlight VidCRAFT3's comprehensive and precise control capabilities, confirming its robustness and versatility across different control dimensions.




\section{Limitations}

VidCRAFT3 can generate unstable results for many specific situations, e.g., large human motion, phyisical interactions, significant change of lighting conditions. This is mostly due to the lack of diverse training data and we believe the neural architecture can also be improved to promote a better understanding of physics and 3D spatial relationships of objects. The camera motion control and object motion control datasets contain inaccurate annotations of camera pose and motion trajectories, which sometimes result in blurred generated videos. Currently, VidCRAFT3 only offers control over the lighting direction and therefore can be improved to support full HDR or other more fine-grained representations of the light field. 


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/ablation_training_stratege2.pdf}
    \caption{
    Qualitative comparisons of \textbf{training strategies} (Dense, Sparse, Dense+Sparse) in the ablation study conducted on WebVid-10M. Results clearly illustrate that Dense training struggles with alignment given sparse input trajectories during inference, Sparse training improves alignment but loses motion detail, while Dense+Sparse training effectively combines both strategies, resulting in superior alignment and enhanced motion realism compared to the ground truth (GT).
    }
    \label{fig:qual_res_ablation_training_strategy}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/ablation_lighting_control2.pdf}
    \caption{
    Qualitative comparison of \textbf{lighting embedding integration strategies} (Text Cross-Attn, Time Embedding, Lighting Cross-Attn) in the ablation study on the VLD dataset. Results clearly demonstrate that the Lighting Cross-Attn strategy provides more accurate and realistic lighting control, achieving superior shadows, reflections, and overall visual fidelity compared to Text Cross-Attn and Time Embedding, aligning closely with the ground truth (GT).
    }
    \label{fig:qual_res_ablation_lighting_embedding}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/ablation_lighting_repr2.pdf}
    \caption{
    Qualitative comparison between Fourier Embedding and SH Encoding for \textbf{representing lighting direction} on the VLD dataset. Results demonstrate that SH Encoding consistently produces more realistic lighting effects, achieving better shading accuracy and reflection details compared to Fourier Embedding, aligning closely with the ground truth (GT).
    }
    \label{fig:qual_res_ablation_lighting_representation}
\end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/more_camera_lighting.pdf}
    \caption{Additional qualitative results demonstrating simultaneous control over \textbf{camera motion} and \textbf{lighting direction}. Each pair of rows compares the original video with the corresponding relighting results generated by VidCRAFT3, highlighting the framework’s effectiveness in precisely adjusting lighting direction while preserving accurate camera motion.}
    \label{fig:more_relighting_camera}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/more_camera_motion3.pdf}
    \caption{Additional qualitative results showcasing simultaneous \textbf{camera motion} and \textbf{object motion} control. Each example compares our generated video (Ours) with ground-truth videos (GT), demonstrating VidCRAFT3’s capability to accurately reproduce intended object motion while precisely maintaining the specified camera motion.}
    \label{fig:more_camera_object2}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[height=0.95\textheight]{figures/more_camera3.pdf}
    \caption{Additional qualitative comparisons for \textbf{camera motion} control. Results from VidCRAFT3 (Ours) are compared with state-of-the-art methods (CameraCtrl, MotionCtrl, CamI2V) and ground-truth (GT) videos. These examples demonstrate VidCRAFT3’s superior capability in generating visually coherent and precise camera motion across diverse scenarios.}
    \label{fig:more_camera}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[height=0.95\textheight]{figures/more_motion2.pdf}
    \caption{Additional qualitative comparisons for \textbf{object motion} control. Results generated by VidCRAFT3 (Ours) are compared against state-of-the-art methods (Image Conductor, Motion-I2V) and ground-truth (GT) videos. The examples demonstrate VidCRAFT3's improved capability in accurately reproducing specified object trajectories, achieving more realistic and coherent object motion across diverse scenarios.}
    \label{fig:more_object}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/more_relighting2.pdf}
    \caption{
    Additional qualitative results demonstrating \textbf{lighting direction} control. Our generated videos ("Ours") are compared against ground-truth sequences ("GT"), clearly highlighting VidCRAFT3's capability to accurately reproduce different lighting direction, achieving realistic shading, shadowing, and reflection details across diverse scenes.
    }
    \label{fig:more_relighting}
\end{figure*}





