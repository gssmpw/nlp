\section{Method}
\label{sec:method}

\subsection{Overview}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/framework.pdf}
    \caption{Architecture of \textbf{VidCRAFT3} for controlled image-to-video generation. The model builds on Video Diffusion Model (VDM) and consists of three main components: the \textit{Image2Cloud} reconstructs 3D point cloud from a single reference image and generates point cloud renderings along user-defined camera motion trajectory; the \textit{ObjMotionNet} injects object dynamics into the UNet by encoding sparse trajectories into multi-scale motion features; the \textit{Spatial Triple-Attention Transformer} integrates image, text, and lighting information via parallel cross-attention modules. The model enables I2V generation conditioned on arbitrary combination of camera motion, object motion, and lighting direction.
    }
    \label{fig:framework}
    \vspace{-5pt}
\end{figure*}

We present \textit{VidCRAFT3}, the first framework for high-fidelity Image-to-Video (I2V) generation with disentangled control over camera motion, object motion, and lighting direction. Existing methods struggle to simultaneously manage these elements due to entangled representations and insufficient annotated training data.
To address this challenge, as shown in Fig.~\ref{fig:framework}, our framework integrates three core components: (1) \textit{Image2Cloud}, which reconstructs a 3D point cloud from a single input image and generates point cloud renderings along user-defined camera motion trajectory for precise camera control; (2) \textit{ObjMotionNet}, which encodes sparse object trajectories into multi-scale motion features for realistic object dynamics; and (3) the \textit{Spatial Triple-Attention Transformer}, which integrates lighting direction, image, and text features via parallel cross-attention layers.
To overcome the scarcity of real-world videos with multi-element annotations, we construct three specialized datasets and adopt a three-stage training strategy to progressively optimize the model.

Our model is built upon the open-source I2V model \textit{DynamiCrafter}, which consists of a VAE encoder $\mathcal{E}$ and decoder $\mathcal{D}$ for image compression, a UNet with spatial and temporal layers, and CLIP text and image encoders. 
We employ a dual-stream injection approach where the reference image is encoded by CLIP image encoder and injected into UNet through image cross-attention. Simultaneously, the first frame of the point cloud rendering is replaced with the reference image, which is then encoded by the VAE encoder and concatenated with noise before being fed into the UNet. This design ensures seamless integration of spatial and temporal information for high-quality video generation.



\subsection{Model Architecture}
\label{subsec:architecture}

\noindent \textbf{Camera Motion Control via Point Cloud Rendering.}
To achieve precise camera motion control in I2V, VidCRAFT3 leverages the Image2Cloud module, which reconstructs a high-quality 3D point cloud of the scene from a single reference image. Specifically, we employ DUSt3R, an unconstrained stereo 3D reconstruction model, to generate a 3D point cloud. Given a reference image $I_\text{ref}$, DUSt3R performs monocular or binocular reconstruction via point regression, followed by global alignment to ensure multi-view consistency: $\mathcal{P} = \text{DUSt3R}(I_{\text{ref}})$. The reconstructed point cloud provides explicit 3D geometry, enabling accurate rendering of the scene from arbitrary camera motion trajectory $E = \{E_1, E_2, \ldots, E_F\}$, where $F$ is the length of trajectory. Given a user-defined trajectory $E$, the point cloud rendering at frame $t$ is computed as $R_t = \pi(\mathcal{P}, E_t)$, where $\pi(\cdot)$ is the differentiable rendering function.
However, due to the limitations of point cloud representation and the sparse 3D cues from a single image, the point cloud renderings may exhibit artifacts such as missing regions, occlusions, and geometric distortions.
To address this, VidCRAFT3 integrates point cloud renderings as a input of VDM, which refines the coarse renderings to generate high-quality and temporally consistent video frames. This combination of explicit 3D geometry and generative refinement ensures both accurate camera control and realistic video synthesis.


\noindent \textbf{Object Motion Control through Trajectory Learning.}
Object motion in VidCRAFT3 is controlled through sparse spatial trajectories. For up to $N$ objects in a $F$-frames video, each trajectory is defined as a sequence of 2D pixel coordinates $\mathcal{T} = \{\mathbf{s}_n^f = (x_n^f, y_n^f) \mid n \in [1, N],\, f \in [1, F]\}$, 
where $\mathbf{s}_n^f$ denotes the position of the $n$-th object in frame $f$. To model motion dynamics, we compute inter-frame optical flow vectors between consecutive frames. For each trajectory point $\mathbf{s}_n^f$, the displacement vector $\mathbf{v}_n^f$ is calculated as $\mathbf{v}_n^f = \mathbf{s}_n^{f+1} - \mathbf{s}_n^f = (x_n^{f+1} - x_n^f, \, y_n^{f+1} - y_n^f).$
These sparse motion vectors are then projected into a per-frame optical flow map $\mathcal{V}^f \in \mathbb{R}^{H \times W \times 2}$, where $H$ and $W$ denote spatial dimensions. The mapping is formalized as 
\begin{equation}
\mathcal{V}^f(x, y) = 
\begin{cases} 
\mathbf{v}_n^f & \text{if } (x, y) = (x_n^{f}, y_n^{f}) \text{ for any } n, \\ 
(0, 0) & \text{otherwise},
\end{cases}
\end{equation}
with the first frame's flow initialized as $\mathcal{V}^1(x, y) = (0, 0), \forall (x, y)$. The full spatiotemporal flow tensor $\mathcal{V} \in \mathbb{R}^{F \times H \times W \times 2}$ is subsequently processed through Gaussian smoothing to obtain a dense motion representation $\tilde{\mathcal{V}}$.
The ObjMotionNet is a neural network composed of multiple convolutional layers and downsampling operations, designed to extract multi-scale motion features from $\tilde{\mathcal{V}}$. 
Inspired by T2I-Adapter \cite{mou2024t2i}, ObjMotionNet injects multi-scale motion features exclusively into the UNet Encoder. This balances precise motion control with video quality, as the encoder handles structure while the decoder refines details, ensuring accurate guidance without compromising output quality.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/object_dataset_pipeline2.pdf}
    \caption{Construction pipeline of the Object Motion Control Dataset.}
    \label{fig:object_dataset_pipeline}
    \vspace{-10pt}
\end{figure}

\noindent \textbf{Lighting Direction Control with Spatial Triple-Attention Transformer.}
The lighting direction is represented as a per-frame 3D vector $L=(l_x, l_y, l_z)$, describing the orientation of light source in Cartesian coordinates. To effectively encode this directional information into a high-dimensional feature space, we employ \textit{Spherical Harmonic (SH) Encoding}. SH encoding captures the angular characteristics of the lighting using basis functions up to degree 4, resulting in 16 coefficients. The resulting SH-encoded vector $L_{\text{SH}} \in \mathbb{R}^{16}$ is projected into the feature space of the UNet using a multi-layer perceptron (MLP). $\mathbf{E}_{\text{light}} = \text{MLP}(L_{\text{SH}})$, where $\mathbf{E}_{\text{light}}$ is a lighting embedding aligned with the dimensionality of text embedding.

To incorporate the lighting embedding into the UNet, we propose the \textit{Spatial Triple-Attention Transformer}, which integrates three parallel attention modules: \textit{image cross-attention}, \textit{text cross-attention}, and \textit{lighting cross-attention}.
The \textit{lighting cross-attention} module integrates the encoded lighting embedding $\mathbf{E}_{\text{light}}$ into the UNet. This attention mechanism modulates the spatial features based on the input lighting direction. The operation is defined as 
\begin{equation}
    \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{Q K^\top}{\sqrt{d}}\right) V,
\end{equation}
where $Q$ (Query) comes from the self-attention output of the UNet, $K$ (Key) and $V$ (Value) are derived from $\mathbf{E}_{\text{light}}$.
The outputs of three cross-attention are summed to produce a fused feature representation $\mathbf{O} = \mathbf{O}_{\text{image}} + \mathbf{O}_{\text{text}} + \mathbf{O}_{\text{light}}$
where $\mathbf{O}_{\text{image}}$, $\mathbf{O}_{\text{text}}$, and $\mathbf{O}_{\text{light}}$ are the outputs of the image, text, and lighting cross-attention modules, respectively. 
This novel mechanism ensures the generated videos maintain consistency across lighting, text, and image conditions.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/VLD_pipeline.pdf}
    \caption{Construction pipeline of the VLD Dataset}
    \label{fig:VLD_pipeline}
    \vspace{-10pt}
\end{figure}





\subsection{Dataset Construction}
\label{subsec:dataset}
Due to the lack of datasets with annotations for camera motion trajectory, object motion trajectories, and lighting directions, we construct three specialized datasets. All datasets consist of 25-frame video clips with a spatial resolution of $320 \times 512$ pixels.

\noindent\textbf{Camera Motion Control Dataset.}
We constructe this dataset from RealEstate10K \cite{zhou2018stereo}, creating 62,000 clips with smooth camera transitions. Using DUSt3R, we generated globally aligned point clouds and rendered the first frameâ€™s point cloud along the ground-truth camera trajectory. Since RealEstate10K lacks captions, we uniformly sampled 4 frames per clip and generated descriptions using Qwen2-VL-7B-Instruct \cite{Qwen2VL}. Despite its focus on indoor scenes, the dataset provides diverse camera trajectories for fine-grained motion control.

\noindent\textbf{Object Motion Control Dataset.}
We construct this dataset from WebVid-10M \cite{bain2021frozen}, consisting of 60,000 video clips. The dataset construction pipeline, illustrated in Fig. \ref{fig:object_dataset_pipeline}, consists of the following five steps:
\textbf{(1) Clip Filtering:}
Clips with abrupt scene changes are removed using PySceneDetect, and sequences with temporal intervals (1â€“16 frames) are sampled. To retain only clips with significant motion, optical flow is computed via MemFlow \cite{dong2024memflow}, and the bottom 25\% of clips with low motion scores are filtered out.
\textbf{(2) Dense Trajectory Generation:}
For each filtered clip, CoTrackerV3 \cite{karaev24cotracker3} generates dense trajectories on a $16 \times 16$ grid, capturing pixel-level motion dynamics across frames.
\textbf{(3) Trajectory Filtering by Length:}
Trajectories shorter than the average length are discarded to ensure meaningful object motion. 
\textbf{(4) Sparse Trajectory Sampling:}
From the filtered dense trajectories, 1â€“8 sparse trajectories are sampled with probability proportional to their length.
\textbf{(5) Optical Flow Smoothing:}
Optical flow between adjacent frames is computed to encode motion direction and intensity. Finally, a Gaussian filter is applied to smooth the sparse trajectory matrix, ensuring stable training.


\noindent\textbf{Lighting Direction Control Dataset.}
Since it is challenging to control a camera to follow the same trajectory under varying lighting conditions in real-world scenarios, collecting such datasets is both extremely difficult and costly. To address this, we introduce the \textit{VideoLightingDirection (VLD) Dataset}, a novel synthetic dataset designed for modeling complex light-object interactions. The VLD Dataset comprises 57,600 synthetic videos rendered using Blender, featuring 3,600 realistic scenes, each with 16 videos captured under distinct lighting conditions. As illustrated in Fig. \ref{fig:VLD_pipeline}, the dataset construction pipeline consists of the following four steps:
\textbf{(1) Scene Creation: } 
We designed two scene types: \textit{Haven} scenes and \textit{BOP} scenes. \textit{Haven} scenes places 3D models from Poly Haven at the center of HDR environments, while \textit{BOP} scenes randomly places BOP models in a six-plane textured room to simulate indirect lighting. Models and environments can be randomly combined for diversity. \textit{See supplementary materials for BOP scenes examples.}
\textbf{(2) Camera Trajectory Sampling: }
We sample the starting point of the camera trajectory on a spherical region (radius: 0.7â€“1.3 meters) centered around the 3D models. A smooth trajectory is then randomly generated around the model, ensuring that the camera always facing the center of the model.
\textbf{(3) Lighting Direction Sampling: }
To enhance the lighting effect, we reduce the HDR environment intensity by 40\%. We uniformly sample 16 points on a hemisphere centered on the model, with the base surface normal aligned with the camera's viewing direction. Each sampled point serves as the position of a 2kW spotlight (radius: 1), oriented toward the model's center. The light source position vectors are mapped to all camera coordinate systems and normalized to compute the lighting direction for each view.
\textbf{(4) Rendering and Annotation: }
Using Blender Cycles, we render each scene under 16 different lighting conditions while maintaining a consistent camera trajectory. Each frame is annotated with the corresponding camera pose and lighting direction, providing precise control signals for training. 




\subsection{Training Strategy}
\label{subsec:training_strategy}

To enable simultaneous control over camera motion, object motion, and lighting direction, we adopt a three-stage progressive training strategy, designed to address the absence of datasets annotated with all three control signals.

\noindent \textbf{Stage 1: Camera Motion Control Training.}
The training begins by initializing the model with DynamiCrafter pre-trained weights. The model is then fine-tuned on camera motion control dataset for 40,000 iterations, optimizing the entire UNet to align VDM with camera motion. This stage establishes robust 3D scene understanding by integrating point cloud renderings, ensuring precise global camera movements while maintaining temporal consistency.

\noindent \textbf{Stage 2: Dense Object Trajectories and Lighting Mixed Fine-tuning.}
We combine the Object Motion Control Dataset and VLD Datasets to create a comprehensive dataset annotated with camera motion, object motion, and lighting direction. This hybrid dataset enhances the ability of model to learn joint control over these three conditions. Dense object trajectories are incorporated to provide rich motion details, accelerating model convergence.
To retain the camera control capabilities from stage 1, the temporal layers of the UNet remain frozen, while the spatial layers and newly introduced components, including ObjMotionNet, lighting cross-attention, and the MLP for lighting direction projection, are optimized for 20,000 iterations. This stage enables the model to simultaneously control all three conditions while ensuring global camera alignment.

\noindent \textbf{Stage 3: Sparse Object Trajectories and Lighting Mixed Fine-tuning.}
The same hybrid dataset from Stage 2 is reused, but dense trajectories are replaced with sparse trajectories to simulate real-world user interactions. Fine-tuning follows the same parameters as Stage 2, with 20,000 iterations. This stage forces the model to infer complex motion patterns from limited trajectory data, and the progressive shift from dense to sparse supervision enhances its ability to generalize to practical scenarios. By progressively learning camera motion, object motion, and lighting direction controls across stages while strategically freezing layers to retain prior knowledge, the training strategy ensures that VidCRAFT3 achieves fine-grained, synergistic control over all three elements, even in the absence of fully annotated multi-task training data.













