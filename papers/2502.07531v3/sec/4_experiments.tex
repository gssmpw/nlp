\section{Experiments}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/realestate10k_baseline2.pdf}
    \caption{Qualitative comparisons with SOTA methods on RealEstate10K.}
    \label{fig:qual_realestate10k}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/webvid_baseline2.pdf}
    \caption{Qualitative comparisons with SOTA methods on WebVid-10M.}
    \label{fig:qual_webvid10m}
\end{figure*}



\subsection{Experimental Setup}
\noindent \textbf{Implementation Details.}
Our model builds upon DynamiCrafter, initialized with its pre-trained weights. During training and inference, video clips are processed at $320 \times 512$ resolution with 25 frames. To optimize the model, we employ the Adam optimizer with a learning rate of $1 \times 10^{-5}$ and a batch size of 96. The training is conducted on 8 NVIDIA H100 GPUs. For inference, we adopt the DDIM sampler and classifier-free guidance to enhance generation quality and multimodal consistency. During inference, the model averagely uses 20 GB of GPU memory and takes 42 seconds.


\setlength{\tabcolsep}{4pt}
\begin{table}[t]
\vspace{-5pt}
\scriptsize
\centering
% \renewcommand\arraystretch{0.8}
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & FID$\downarrow$    & FVD$\downarrow$    & CLIPSIM$\uparrow$ & CamMC$\downarrow$ \\
\midrule
CameraCtrl  & 97.99          & 96.11          & 29.41   &  4.19  \\
MotionCtrl  & 103.82         & 188.93         & 30.18            &  4.23           \\
CamI2V          & 98.54 & 85.03 & 30.37            &  4.24           \\
\rowcolor[HTML]{EFEFEF}
\textbf{Ours}   & \textbf{75.62}             &     \textbf{49.77}       &  \textbf{32.32}          &  \textbf{4.07}            \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison with SOTA methods on RealEstate10K.}
\label{tab:result_realestate10k}
\vspace{-5pt}
\end{table}

\noindent \textbf{Evaluation Datasets.}
We evaluate our model on three domain-specific datasets and a generalized test set. 
For camera motion control, we sample 1,000 samples from the RealEstate10K test set. For object motion control, we sample 1,000 samples from the WebVid-10M test set. For lighting direction control, we sample 1,000 samples from the VLD dataset, covering a wide range of lighting directions. To provide a broader evaluation, we create a generalized test set consisting of 100 videos sourced from copyright-free websites like Pixabay and Pexels, as well as videos generated by T2V models. This dataset spans categories such as human activities, animals, vehicles, indoor scenes, artworks, natural landscapes, and AI-generated images.


\noindent \textbf{Evaluation Metrics.}
We evaluate VidCRAFT3 across three dimensions:
(1) Video Quality: FID for assessing visual fidelity, FVD for measuring temporal coherence, and CLIPSIM for detecting semantic alignment;
(2) Motion Control Performance: Based on camera poses estimated by DUSt3R and object trajectories extracted by CoTrackerV3, motion control performance is quantified using CamMC and ObjMC metrics \cite{wang2024motionctrl}, which measure the Euclidean distance between predicted and ground-truth values;
(3) Lighting Control Effectiveness: Evaluated through LPIPS, SSIM, and PSNR, comparing generated frames with ground-truth images to assess perceptual quality  and structural fidelity.



\subsection{Comparisons with State-of-the-Art Methods}

Due to the lack of open-source I2V methods capable of simultaneously controlling camera and object motion, we evaluate VidCRAFT3 separately for \textit{camera motion control} and \textit{object motion control}, comparing it against SOTA methods in each domain.

\setlength{\tabcolsep}{4pt}
\begin{table}
\vspace{-5pt}
\scriptsize
\centering
% \renewcommand\arraystretch{0.8}
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & FID$\downarrow$    & FVD$\downarrow$    & CLIPSIM$\uparrow$ & ObjMC$\downarrow$ \\
\midrule
Image Conductor & 150.26         & 242.01          & 29.69            &  12.96          \\
Motion-I2V      & 128.35         & 171.35          & 30.92            &  3.96           \\
\rowcolor[HTML]{EFEFEF}
\textbf{Ours}            & \textbf{87.12} & \textbf{120.65} & \textbf{32.99}   &  \textbf{3.51} \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison with SOTA methods on WebVid-10M.}
\label{tab:result_webvid10m}
\vspace{-5pt}
\end{table}






\noindent \textbf{Camera Motion Control.}
VidCRAFT3 demonstrates superior performance in camera motion control compared to state-of-the-art (SOTA) methods like CameraCtrl, CamI2V, and MotionCtrl on the RealEstate 10K dataset. As shown in Table \ref{tab:result_realestate10k} and Fig. \ref{fig:qual_realestate10k}, VidCRAFT3 excels in camera motion control, as evidenced by its CamMC score of 4.07, which is lower than CameraCtrl (4.19), CamI2V (4.24), and MotionCtrl (4.23). VidCRAFT3 also achieves better results in other key metrics indicate that VidCRAFT3 not only controls camera motion more accurately but also generates videos with higher visual quality, temporal coherence, and semantic alignment. 
Qualitatively, VidCRAFT3 generates smoother and more realistic camera movements with fewer artifacts, particularly in complex scenes. This success stems from the Image2Cloud module, which enable precise 3D scene reconstruction and fine-grained camera control.


\noindent \textbf{Object Motion Control.}
VidCRAFT3 demonstrates exceptional performance in object motion control, as highlighted in Table \ref{tab:result_webvid10m}. The model achieves an ObjMC score of 3.51, which is lower than Image Conductor (12.96) and Motion-I2V (3.96). This indicates that VidCRAFT3 more accurately aligns the generated object trajectories with the ground truth, resulting in more realistic and faithful object motion. Additionally, VidCRAFT3 outperforms in other key metrics, showcasing superior visual quality, temporal coherence, and semantic alignment.
Qualitatively, as shown in Fig. \ref{fig:qual_webvid10m}, VidCRAFT3 generates more realistic and consistent object movements compared to Image Conductor and Motion-I2V. The model effectively captures the dynamics of object motion, ensuring smooth transitions and natural interactions within the scene. These results highlight VidCRAFT3's robust object motion control, driven by its advanced ObjMotionNet, which effectively captures and controls complex motion patterns. 


\setlength{\tabcolsep}{1pt}
\begin{table}[]
\vspace{-5pt}
\scriptsize
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method}          & Camera Motion Precision$\uparrow$ & Visual Quality$\uparrow$ & Overall Quality$\uparrow$ \\
\midrule
\textbf{CameraCtrl}      & 3.4\%                              & 5.4\%                     & 4.7\%                      \\
\textbf{MotionCtrl}      & 5.7\%                              & 7.3\%                     & 6.5\%                      \\
\textbf{CamI2V}          & 5.6\%                              & 6.8\%                     & 5.2\%                      \\
\rowcolor[HTML]{EFEFEF} 
\textbf{Ours}            & \textbf{85.3\%}                    & \textbf{80.5\%}           & \textbf{83.6\%}            \\
\midrule
                         & Object Motion Precision$\uparrow$          & Visual Quality$\uparrow$          & Overall Quality$\uparrow$          \\
\midrule
\textbf{Image Conductor} & 10.3\%                            & 11.9\%                   & 11.2\%                    \\
\textbf{Motion-I2V}      & 14.8\%                            & 10.9\%                   & 13.2\%                    \\
\rowcolor[HTML]{EFEFEF} 
\textbf{Ours}            & \textbf{74.9\%}                   & \textbf{77.2\%}          & \textbf{75.6\%}    \\
\bottomrule
\end{tabular}
\caption{User study. Results demonstrating our method's superior performance in camera and object motion control compared to baseline approaches across all metrics.}
\label{tab:user_study}
\vspace{-5pt}
\end{table}

\setlength{\tabcolsep}{1pt}
\begin{table}
\vspace{-5pt}
\scriptsize
\centering
\begin{tabular}{lccccccc}
\toprule
\textbf{Method}                & FID$\downarrow$    & FVD$\downarrow$    & CLIPSIM$\uparrow$ & PSNR$\uparrow$  & SSIM$\uparrow$ & LPIPS$\downarrow$ & CamMC$\downarrow$ \\
\midrule
Text Cross-Atten           & 111.08          & 121.95          & 22.77            & 18.14          & 0.72          & 0.13           & 5.31           \\
Time Embed.                & 101.71          & 123.31          & 22.70            & 19.07          & 0.73          & 0.12           & 5.21           \\
\rowcolor[HTML]{EFEFEF}
\textbf{Lighting Cross-Attn} & \textbf{100.83} & \textbf{117.69} & \textbf{23.70}   & \textbf{19.49} & \textbf{0.74} & \textbf{0.11}  & \textbf{5.00} \\
\bottomrule
\end{tabular}
\caption{Ablation of lighting embedding integration strategies on VLD. Text Cross-Atten method concatenates lighting embedding with text embedding and integrates them into the model through text cross-attention. Time Embed method adds lighting embedding to time embedding. The proposed Lighting Cross-Atten method, detailed in Sec. \ref{subsec:architecture}, demonstrates enhanced performance in integrating lighting conditions.}
\label{tab:ablation_lighting_embedding}
\vspace{-5pt}
\end{table}



\noindent\textbf{User Study.}
To evaluate our method, we conducted a user study with 10 participants assessing 200 randomly selected results from our method and five baselines. 
The participants evaluated the results based on four key metrics: Camera Motion Precision, Object Motion Precision, Visual Quality, and Overall Quality.
As shown in Table \ref{tab:user_study}, our method achieved over 80\% in camera motion control and over 74\% in object motion control across all metrics, demonstrating precise and visually appealing control, validating its effectiveness and robustness in real-world applications.



\subsection{Ablation Study}


\noindent\textbf{Training Strategy.}
We compare training with Dense, Sparse, and Dense+Sparse trajectories. As shown in Table \ref{tab:ablation_training_strategy}, Dense training (ObjMC: 4.39) provides rich motion information but struggles during inference due to sparse inputs. Sparse training (ObjMC: 4.05) improves inference alignment but lacks motion detail. The Dense+Sparse approach achieves the best results (ObjMC: 3.51), leveraging dense trajectories for robust learning and fine-tuning with sparse trajectories for better generalization. This hybrid strategy enhances object motion control and video quality, as shown by superior FID (87.12) and FVD (120.65) scores. 

\noindent\textbf{Lighting Embedding Integration Strategies.}
We compare different lighting embedding integration strategies. As shown in Table \ref{tab:ablation_lighting_embedding}, Lighting Cross-Atten achieves the best results, with an PSNR of 19.49, SSIM of 0.74, and LPIPS of 0.11, outperforming other methods. 
These findings confirm the effectiveness of Lighting Cross-Attention for precise lighting direction control.


\noindent\textbf{Representation of Lighting Direction.}
We compare Fourier Embedding \cite{mildenhall2021nerf} and SH Encoding for representation of lighting direction. As shown in Table \ref{tab:ablation_lighting_representation}, SH Encoding outperforms Fourier Embedding, achieving better PSNR (19.49), SSIM (0.74), and LPIPS (0.11). 
These results confirm SH Encoding's effectiveness for precise lighting control in image-to-video generation.
\textit{For more  results, please refer to the supplementary materials.}


\setlength{\tabcolsep}{4pt}
\begin{table}
\vspace{-5pt}
\scriptsize
\centering
% \renewcommand\arraystretch{0.8}
\begin{tabular}{lccccccc}
\toprule
\textbf{Method} & FID$\downarrow$    & FVD$\downarrow$    & CLIPSIM$\uparrow$ &  ObjMC$\downarrow$ \\
\midrule
Dense           & 92.05          & 143.44          & 30.78            &  4.39           \\
Sparse          & 91.54          & 123.15          & 30.93            &  4.05           \\
\rowcolor[HTML]{EFEFEF}
\textbf{Dense+Sparse}    & \textbf{87.12} & \textbf{120.65} & \textbf{32.99}   &  \textbf{3.51} \\
\bottomrule
\end{tabular}
\caption{Ablation of training strategy on WebVid-10M.}
\label{tab:ablation_training_strategy}
\vspace{-5pt}
\end{table}




\setlength{\tabcolsep}{1pt}
\begin{table}
\vspace{-5pt}
\scriptsize
\centering
\begin{tabular}{lccccccc}
\toprule
\textbf{Light Representation} & FID$\downarrow$    & FVD$\downarrow$    & CLIPSIM$\uparrow$ & PSNR$\uparrow$  & SSIM$\uparrow$ & LPIPS$\downarrow$ & CamMC$\downarrow$ \\
\midrule
Fourier Embedding                & 107.40          & 121.89          & 21.71            & 17.48          & 0.70          & 0.14           & 5.03           \\
\rowcolor[HTML]{EFEFEF}
\textbf{SH Encoding}             & \textbf{100.83} & \textbf{117.69} & \textbf{23.70}   & \textbf{19.49} & \textbf{0.74} & \textbf{0.11}  & \textbf{5.00} \\
\bottomrule
\end{tabular}
\caption{Ablation of representation of lighting direction on VLD.}
\label{tab:ablation_lighting_representation}
\vspace{-5pt}
\end{table}
