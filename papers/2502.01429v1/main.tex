\documentclass[9.5pt, journal]{IEEEtai}

\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}

\usepackage{color,array}
\ifCLASSINFOpdf
% \usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../pdf/}{../jpeg/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
% or other class option (dvipsone, dvipdf, if not using dvips). graphicx
% will default to the driver specified in the system graphics.cfg if no
% driver is specified.
% \usepackage[dvips]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../eps/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.eps}
\fi
%\usepackage{subfig}   
%\usepackage{layouts}
%\documentclass[a4paper]{article}
\usepackage{array}
\usepackage[hang]{footmisc}
%\usepackage{subfig}

\usepackage[subrefformat=parens,labelformat=parens,caption=false,font=footnotesize]{subfig}
\captionsetup{belowskip=-2cm}
%\setlength{\belowcaptionskip}{-2cm}

%\usepackage[caption=false,font=footnotesize]{subfig}
%\documentclass[a4paper]{article}
\usepackage{amsmath}
%\usepackage{hyperref}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
	\begin{acronym}
    \acro{4G}{fourth generation}
    \acro{5G}{fifth generation}
    \acro{AoA}{angle of arrival}
   	\acro{AoD}{angle of departure}
   	\acro{AP}{access point}
    \acro{BAI}{best arm identification}
    \acro{BCRLB}{Bayesian CRLB}
    \acro{BS}{base stations}
	\acro{CDF}{cumulative density function}
    \acro{CF}{closed-form}
    \acro{CRLB}{Cramer-Rao lower bound}
    \acro{ECDF}{empirical cumulative distribution function}
    \acro{EI}{Exponential Integral}
    \acro{eMBB}{enhanced mobile broadband}
    \acro{FIM}{Fisher Information Matrix}
    \acro{GoF}{goodness-of-fit}
    \acro{GPS}{global positioning system}
    \acro{GNSS}{global navigation satellite system}
    \acro{HetNets}{heterogeneous networks}
    \acro{LOS}{line of sight}
    \acro{MAB}{multi-armed bandit}
    \acro{MBS}{macro base station}
    \acro{MEC}{mobile-edge computing}
    \acro{MIMO}{multiple input multiple output}
    \acro{mm-wave}{millimeter wave}
    \acro{mMTC}{massive machine-type communications}
    \acro{MS}{mobile station}
    \acro{MVUE}{minimum-variance unbiased estimator}
    \acro{NLOS}{non line-of-sight}
    \acro{OFDM}{orthogonal frequency division multiplexing}
    \acro{PDF}{probability density function}
    \acro{PGF}{probability generating functional}
    \acro{PLCP}{Poisson line Cox process}
    \acro{PLT}{Poisson line tessellation}
    \acro{PLP}{Poisson line process}
    \acro{PPP}{Poisson point process}
    \acro{PV}{Poisson-Voronoi}
    \acro{QoS}{quality of service}
    \acro{RAT}{radio access technique}
    \acro{RL}{reinforcement-learning}
    \acro{RE}{rapid exploration}
    \acro{RSSI}{received signal-strength indicator}
    \acro{BS}{base station}
   	\acro{SINR}{signal to interference plus noise ratio}
    \acro{SNR}{signal to noise ratio}
    \acro{TS}{Thompson Sampling}
    \acro{TS-CD}{TS with change-detection}
    \acro{KS}{Kolmogorov-Smirnov}
    \acro{UCB}{upper confidence bound}
	\acro{ULA}{uniform linear array}
	\acro{UE}{user equipment}
 	\acro{URLLC}{ultra-reliable low-latency communications}
    \acro{V2V}{vehicle-to-vehicle}    
\end{acronym}
\usepackage{caption}
\captionsetup{font=small}
\setlength{\belowcaptionskip}{-10pt}
\captionsetup[figure]{name={Fig.},labelsep=period}
\usepackage{graphicx,wrapfig,lipsum}
%\usepackage{biblatex}
\usepackage{rotating}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{stmaryrd}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}[]
\newtheorem{corollary}{Corollary}[]
\newtheorem{proposition}{Proposition}[]
\newtheorem{lemma}[]{Lemma}
\usepackage{multicol}
\usepackage{xr-hyper}
\newtheorem{remark}{Remark}[]
\newtheorem{assumption}{Assumption}[]
\usepackage{amsmath,amssymb,xcolor,mathtools}
\usepackage{algpseudocode}
\usepackage{scalerel}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{setspace}
\newtheorem{definition}{Definition}
\usepackage[noadjust]{cite}
\usepackage{booktabs}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}}
\newcommand{\cZ}{\mathcal{Z}}

%Blackboard Bold Shorthands
\newcommand{\bE}{\mathbb{E}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

%Probability Shorthands
\def \ind{\mathds{1}}
\newcommand{\norm}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\bet}[2]{\text{Beta}\left(#1,#2 \right)}
\newcommand{\muhat}{\hat{\mu}}


% Statistical Shorthands
\newcommand{\K}{\mathrm{KL}}
\newcommand{\Kb}{\mathrm{K}}

% Misc Math Operations Shorthands

\def \qed {\begin{flushright} $\qquad \Box $
  \end{flushright}}


%Misc Shorthands
\newcommand{\ra}{\rightarrow}
\newcommand{\grad}{\nabla}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\nuhat}{\hat{\nu}}

\usepackage{bbm}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{setspace}
%\usepackage{titlesec}
%\usepackage[titletoc,toc,title]{appendix}
%\doublespacing 
%\raggedbottom
%\usepackage{breqn}
\usetikzlibrary{automata}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage[]{footmisc}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\allowdisplaybreaks

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
%\usepackage{caption}
%\setlength{\belowcaptionskip}{-10pt}
%\captionsetup{aboveskip=-5pt, belowskip = -25pt}
%

\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lfloor}{\rfloor}
% \doublespacing


\usepackage{xcolor}
\usepackage{scalerel}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{svg.path}
\usepackage{tikz,xcolor,hyperref}

% Make Orcid icon
\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{%
	\begin{tikzpicture}
	\draw[lime, fill=lime] (0,0) 
	circle [radius=0.16] 
	node[white] {{\fontfamily{qag}\selectfont \tiny ID}};
	\draw[white, fill=white] (-0.0625,0.095) 
	circle [radius=0.007];
	\end{tikzpicture}
	\hspace{-2mm}
}

\foreach \x in {A, ..., Z}{%
	\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}{\noexpand\orcidicon}}
}

% Define the ORCID iD command for each author separately. Here done for two authors.
\newcommand{\orcidauthorA}{0000-0002-8240-4038}
\newcommand{\orcidauthorB}{0000-0003-2183-1615}
\newcommand{\orcidauthorC}{0000-0003-3685-7201}

\begin{document}

	\title{\fontsize{22.8}{27.6}\selectfont An Algorithm for Fixed Budget Best Arm Identification with Combinatorial Exploration
    }
	\author{Siddhartha Parupudi and Gourab Ghatak \thanks{The authors are with the Department of Electrical Engineering, IIT Delhi, Hauz Khas, India 110016. Email:P.Siddhartha.ee121@ee.iitd.ac.in, gghatak@ee.iitd.ac.in.}}
\setcounter{page}{1}
	\maketitle


		
\begin{abstract}
We consider the best arm identification (BAI) problem in the $K-$armed bandit framework with a modification - the agent is allowed to play a subset of arms at each time slot instead of one arm. Consequently, the agent observes the sample average of the rewards of the arms that constitute the probed subset. Several trade-offs arise here - e.g., sampling a larger number of arms together results in a wider view of the environment, while sampling fewer arms enhances the information about individual reward distributions. Furthermore, grouping a large number of suboptimal arms together albeit reduces the variance of the reward of the group, it may enhance the group mean to make it close to that containing the optimal arm. To solve this problem, we propose an algorithm that constructs $\log_2 K$ groups and performs a likelihood ratio test to detect the presence of the best arm in each of these groups. Then a Hamming decoding procedure determines the unique best arm. We derive an upper bound for the error probability of the proposed algorithm based on a new hardness parameter $H_4$. Finally, we demonstrate cases under which it outperforms the state-of-the-art algorithms for the single play case.
\end{abstract}

%\begin{IEEEImpStatement}
%Our work will find applications in problems involving resource allocation. For example, in wireless communications, a transmitter may distribute its transmit power among multiple available channels or multiple antennas and only get the total received power as feedback. Eventually, the transmitter must identify the best channel to continue data transmission. Similarly, a trader may want to distribute an exploratory capital among different investment instruments while receiving the total profit/loss as a feedback. Here after a fixed budget, the trader has to identify the most profitable investment instrument for an eventual bulk investment. In spite of obvious applications, to the best of our knowledge, this setting has not been studied in literature previously. Our work presents the first steps in this regard and it is envisaged that our study will open new research directions. In particular, a key technical challenge is to derive the lower bound of the error probability which remains an open information theoretic problem.
%\end{IEEEImpStatement}
\begin{IEEEkeywords}
Best arm identification, multi-play bandits.
\end{IEEEkeywords}

\section{Introduction}
A \ac{MAB} framework $\nu_{[K]}$ ($[K] \doteq \{1, 2, \ldots, K\}$), consists of a collection of $K$ arms, where each arm $\nu_a \; (1\leq a\leq K)$ is a probability distribution, typically on but not limited to $\mathbb{R}$ with an expectation $\mu_a$. We denote the sample space of $\nu$ by $\mathcal{N}$. In the single-play setting, at each discrete time slot $t \in \mathbb{Z}^+$, an agent chooses an arm $A_t\in\{1,\dots,K\}$ and receives an independent sample $X_t$ from the corresponding arm $\nu_{A_t}$. We denote by $\bP_\nu$ (resp. $\bE_\nu$) the probability law (resp. expectation) of the process $\{X_t\}$. The \ac{BAI} is a specific problem setting in the \ac{MAB} model, where the agent is required to identify $a^* = \argmax_a \mu_a$. The performance of a policy which recommends the arm $\hat{a}$ in the \ac{BAI} setting is determined by either the probability of error in determining the best arm or in terms of the number of samples it needs for recommendation. This is in contrary to the classical {\it regret} framework, wherein, the agent has to optimize the exploration-exploitation dilemma during the action selection.

Two different variants of the \ac{BAI} problem are studied in literature. In the \emph{fixed-confidence} variant, for a given error probability $\delta$, the agent attempts to minimize the expectation of the number of plays, called sample complexity, $\tau_\delta$, before recommending the best arm. A strategy $\cA(\delta)$ is called \emph{$\delta$-PAC} if, for every choice of $\nu$ in $\mathcal{N}$, $\bP_\nu(\hat{a} = a^*)\geq 1-\delta$. On the contrary, in the \emph{fixed-budget} variant, the number of plays $T \in \N$ is fixed, while the goal is to choose the sampling and recommendation rules so as to minimize the error probability of recommending the best arm $p_T(\nu) := \bP_\nu(\hat{a} \neq a^*)$. In the fixed-budget setting, a family of strategies $\cA(T)$ is called \emph{consistent} if, for every choice of $\nu \in \mathcal{N}$, $p_T(\nu)$ tends to zero when $T$ tends to infinity. In what follows, we adopt the notation $p_{\rm D}^A(T)$ to denote the probability of error of algorithm $A$ in a budget $T$, where the reward distribution is $D$.

\subsection{Related Works}
For the fixed budget setting, Audibert et al.~\cite{audibert2010best} proposed the \texttt{UCB-E} and \texttt{Successive Rejects} (SR) algorithms and proved their optimality up to logarithmic factors. Specifically, the upper bound on the probability of error for their proposed algorithms is
%\begin{align}
    $p(T) \leq \frac{K (K-1)}{2} \exp\left(- \frac{T - K}{\log_2 (K) H_2}\right),$
%\end{align}
where $H_2$ is called the hardness of the problem that depends on the specific instance of $\nu \in \mathcal{N}$ (to be discussed in detail soon). In a remarkable result, Carpentier \& Locatelli~\cite{carpentier2016tight} proved the following lower bound for the probability of error in the fixed-budget setting:
%\begin{align}
       $p(T) \gtrsim \exp\left(-\frac{T}{\log(K) H}\right),$
%\end{align}
where $H$ is another variant of the hardness parameter. This disproved a long-standing assumption—that there must exist an algorithm for this problem whose probability of error is upper bound by $\exp(-T/H)$. This established a key difference between the fixed-confidence and the fixed-budget settings. Following this line of work, Karnin~\cite{karnin2013almost} proposed the \texttt{SEQUENTIAL HALVING} ({SH}) algorithm and proved it to be almost optimal for BAI problems. This is achieved by eliminating half of the surviving arms with the worst estimates in each round. \ac{BAI} problem has also been analyzed using other variants of the \ac{UCB} algorithm (e.g., LUCB of \cite{LUCB}) which are not based on eliminations. Most of the experimental results of these algorithms are present for bounded distributions, that are in fact particular examples of distributions with sub-Gaussian tails. Since then, the fixed budget pure exploration problem has been studied in a wide variety of contexts, e.g., for linear bandits~\cite{jedra2020optimal}, minimax optimality~\cite{yang2022minimax}, spectral bandits~\cite{kocak2020best}, risk-averse bandits~\cite{kagrecha2022statistically}, and bandits with mis-specified linear models~\cite{alieva2021robust}. More recently, large deviation perspective was studied in~\cite{wang2024best}, cost-aware BAI in \cite{qin2025cost} and arm erasures in \cite{reddy2024best}. Thompson sampling has also been employed to form the {\it best challenger} rule to improve the efficiency of \ac{BAI}~\cite{lee2024thompson}. Interesting new directions include combining the regret setting with the BAI setting, e.g., see~\cite{zhang2024fast, qin2024optimizing}. Finally, assuming a distribution of $\nu$ over $\mathcal{N}$, researchers have studied rate-optimal Bayesian regret in \ac{BAI}~\cite{komiyama2024rate}.

\subsection{Motivation and Contributions}
In all of the above works, the agent is restricted to sample a single arm from $\nu_{[K]}$ at each time slot. We propose that in case this assumption is relaxed, i.e., if the agent is allowed to select a subset of arms, it may enhance the efficiency of \ac{BAI}, i.e., the error probability may reduce. Naturally, the case where the selection of a subset of arms leads to the agent having access to individual reward samples, is trivial - the optimal action here is to select all the arms. Hence, we focus on a setting where on the selection of a subset of the arms, the agent receives a single reward that is a function of the individual rewards of the selected arms. In this work, we assume this function to be the sample average of rewards of the selected arms. In particular we make the following contributions:
\begin{itemize}
    \item {\bf Combinatorial Exploration in BAI:} We introduce a novel \ac{BAI} framework where the agent selects subsets of arms instead of individual arms, enabling a trade-off between broader exploration and fine-grained reward estimation, a setting previously unexplored in the literature for non-trivial rewards, e.g., unlike \cite{ghatak2024best} where the rewards of suboptimal arms were constant 0.
    \item {\bf Algorithm and Theoretical Guarantees:}  We propose an innovative arm grouping strategy using Hamming codes, allowing efficient detection of the best arm through likelihood ratio tests and a decoding procedure. Additionally, we introduce a new hardness parameter, $H_4$ and derive an upper bound on the error probability.
    \item {\bf Empirical Analysis:} We conduct numerical experiments to identify the conditions under which our proposed algorithm outperforms state-of-the-art single-play algorithms. We discuss cases where our method under performs, providing insights into its limitations.
\end{itemize}

\subsection{Applications}
Our work will find applications in problems involving resource allocation. For example, in wireless communications, a transmitter may distribute its transmit power among multiple available channels or multiple antennas and only get the total received power as feedback. Eventually, the transmitter must identify the best channel to continue data transmission. Similarly, a trader may want to distribute an exploratory capital among different investment instruments while receiving the total profit/loss as a feedback. Here after a fixed budget, the trader has to identify the most profitable investment instrument for an eventual bulk investment. In spite of obvious applications, to the best of our knowledge, this setting has not been studied in literature previously. Our work presents the first steps in this regard and it is envisaged that our study will open new research directions. In particular, a key technical challenge is to derive the lower bound of the error probability which remains an open information theoretic problem.

\subsection{Organization}
The rest of the paper is organized as follows. In Section II, we introduce the necessary preliminaries, defining key concepts and hardness parameters relevant to our problem. Section III presents our proposed algorithm, detailing the arm grouping strategy, likelihood ratio tests, and the recommendation procedure. In Section IV, we analyze the probability of error for Gaussian rewards, deriving theoretical upper bounds. Section V extends this analysis to bounded reward distributions using Bernstein’s inequality. Section VI provides empirical results, comparing our approach with state-of-the-art algorithms and highlighting scenarios where it outperforms existing methods. Finally, in Section VII, we summarize our contributions and discuss potential future research directions. Additional technical details and proofs are included in the appendices.

\section{Preliminaries}
Let $(\mu_{[1]}, \mu_{[2]}, \dots, \mu_{[K]})$ denote the ordered $K$-tuple of $(\mu_1, \mu_2, \dots, \mu_K)$ sorted in decreasing order. We define the suboptimality gaps as
\begin{align}\label{eqn:suboptimality_gaps}
    \Delta_a &= \mu_{[1]} - \mu_a \ \ \text{for  } a \neq a^* = \argmax \mu_a, \\
    \Delta_{a^*} &= \min_{a \neq a^*} \Delta_a \ \ \ \text{for } a = a^* .
\end{align}
We now define a few relevant {\it hardness terms} below. Each of the hardness terms is a function of the specific $\nu \in \mathcal{N}$, hence we drop this parametrization unless needed.
\begin{align}
  H_1 &\triangleq \sum_{i \in \{1, 2, \dots
    K\}}\frac{1}{\Delta_i^2}, %\label{def:H_1}, \nonumber \\
    &H_2 \triangleq \max_{i \in \{1,2,\dots,K\}}\frac{i}{(\mu_{[1]}-\mu_{[i]})^2}, \nonumber \\
    H_3 &\triangleq  \frac{K}{\Delta_{[1]}^2}, \;\; \text{ and } 
    &H_4 \triangleq  \frac{1}{(\Delta_{[1]} + \Delta_{[K]})^2}.     \nonumber
\end{align}
The term $H_4$ is introduced by us in this paper for the first time, and we will need it to analyze our algorithm. The following inequalities hold true for the hardness terms
\begin{align}
  H_2 \leq H_1 \leq \log(2K) H_2, \nonumber \\
  H_2 \leq H_1 \leq H_3, \text{ and }\quad
  4H_4 \leq H_1 \leq  4KH_4.\nonumber
\end{align}
Let us first review the results for bounded rewards for the existing algorithms.
\subsection{Bounded Rewards}
\begin{lemma}
    For the uniform allocation strategy, where each arm is played an equal number of times and the arm with the highest empirical mean is recommended, the error probability is bounded as
\begin{align}\label{eqn:UE_bounded}
    p_{\rm U}^{\rm{UE}}(T) \leq (K-1)\exp\bigg(\frac{-T}{2H_3}\bigg). 
\end{align}
\end{lemma}
\begin{IEEEproof}
    Please see Appendix~\ref{app:UE}.
\end{IEEEproof}
\begin{lemma}
For rewards in $[0,1]$, the upper bound on the error probability of SR~\cite{Successive-rejects} and SH~\cite{Sequential-halving} are
\begin{align}
    p_{\rm U}^{\mathsf{SR}} \leq \frac{K(K-1)}{2}\exp{\bigg(- \frac{T-K}{\log(K)H_2}\bigg)}, \label{eqn:SR_bounded}\\
    p_{\rm U}^{\mathsf{SH}}(T) \leq 3 \log_2(K) \exp{\bigg(-\frac{T}{8H_2\log_2(K)} \bigg)}. \label{eqn:SH_bounded}
\end{align}
\end{lemma}
\begin{remark}
The SR algorithm is better in terms of lower error probability for large budgets, but the SH algorithm scales better with $K$ for a fixed budget. Furthermore, when $\Delta_{[2]} = \Delta_{[K]}$, i.e., all the suboptimal arms have the same suboptimality gap, $H_1 = H_2 = H_3 = 4KH_4$. In this case, the uniform allocation strategy is moderately better than the other algorithms, however, when $\Delta_{[1]} = \Delta_{[2]}$ is small and all other suboptimality gaps are large, SR and SH are significantly better than the non-adaptive uniform exploration strategy.
\end{remark}

\subsection{Gaussian Rewards}
Although not explicitly reported in literature, in case the reward distribution is Gaussian, the probabilities of error are calculated using the bounds on the Gaussian $Q$-function rather than Hoeffding's inequality, and are presented below. We omit the proofs since they follows in the same manner as the bounded rewards case.
\begin{lemma}
The probability of error for fixed budget BAI for Gaussian rewards with fixed variance $\sigma^2$ for each of the three algorithms, respectively, are
    \begin{align}
    \label{eqn:UE_Gaussian}
    p_{\rm G}^{\mathsf{UE}}(T)\leq &(K-1)\sqrt{\frac{H_3\sigma^2}{\pi T}} \exp\bigg(\frac{-T}{4H_3\sigma^2}\bigg), \\
     p_{\rm G}^{\mathsf{SR}}(T) \leq &\frac{K(K-1)}{2} \sqrt{\frac{H_2\sigma^2\log(K)}{2\pi(T-K)}}\nonumber \\
     &\exp{\bigg(- \frac{T-K}{2H_2\sigma^2\log(K)}\bigg)}, \label{eqn:SR_Gaussian}\\
    p_{\rm G}^{\mathsf{SH}}(T) \leq &3 \log_2(K) \sqrt{\frac{2H_2\sigma^2\log_2(K)}{\pi T}}\nonumber \\
    &\exp{\bigg(-\frac{T}{8H_2\sigma^2\log_2(K)} \bigg)}.\label{eqn:SH_Gaussian}
\end{align}
\end{lemma}
We note the improvement in the leading term of the upper bound for the Gaussian reward setting. We also see that the effective hardness also introduces the variance parameter as a multiplicative term.
These results along with the same for our work are summarized in Table~\ref{table:1} and Table~\ref{table:2}.



\begin{table*}
    \centering
    \begin{tabular}{|c||c|c|} 
     \hline
      %\multirow{}{}
      {Algorithm} 
      & \multicolumn{2}{c|}{Bounded Rewards}  \\ 
      \cline{2-3}
      & Leading term & Exponent term \\ 
     \hline\hline
      Uniform Exploration & $K-1$ & $2H_3$\\ 
     \hline
      Successive Rejects &$K(K-1)/2$ & $H_2\log(K)$ \\
     \hline
      Sequential Halving & $ 3 \log_2(K)$& $8H_2\log_2(K)$\\ 
     \hline
      \ac{RE} (ours) & $\log_2(K)$ & $4KH_4\log_2(K)(1 + \frac{1}{3\sqrt{H_4}})$ \\ [1ex]
     \hline
    \end{tabular}
    \caption{Comparison of upper bounds and gaps for various algorithms for bounded rewards}
    \label{table:1}
\end{table*}

\begin{table*}
    \centering
    \begin{tabular}{|c||c|c|} 
     \hline
      %\multirow{}{}
      {Algorithm} 
      & \multicolumn{2}{c|}{Gaussian Rewards}  \\ [0.5ex]
      \cline{2-3}
      & Leading term & Exponent term \\ [0.5ex]
     \hline\hline
      Uniform Exploration & $(K-1)\sqrt{\frac{H_3\sigma^2}{\pi T}} $ & $4H_3\sigma^2$\\ [1ex]
     \hline
      Successive Rejects &$\frac{K(K-1)}{2} \sqrt{\frac{H_2\sigma^2\log(K)}{2\pi(T-K)}}$ & $2H_2\sigma^2\log(K)$ \\ [1ex]
     \hline
      Sequential Halving & $3 \log_2(K) \sqrt{\frac{2H_2\sigma^2\log_2(K)}{\pi T}}$& $8H_2\sigma^2\log_2(K)$\\ [1ex]
     \hline
      \ac{RE} (ours) & $\log_2(K)\sqrt{\frac{4H_4\sigma^2 K\log_2(K)}{\pi T}}$ & $16KH_4\sigma^2\log_2(K)$ \\ [1ex]
     \hline
    \end{tabular}
    \caption{Comparison of upper bounds and gaps for various algorithms for Gaussian rewards}
    \label{table:2}
\end{table*}

%\subsection{Objective}
In our work, the objective is to improve upon the above algorithms in terms of the upper bounds by relaxing the condition that only one arm can be pulled per slot. Precisely, at each slot $t$, a subset of arms is sampled, i.e., $A_t \subseteq [K]$. The reward received by the agent at time slot $t$ is $\sum_{i \in A_t} X_i(t)$, where $X_i(t) \sim \nu_i$.

\section{\ac{RE} Algorithm}
In this section, we present our algorithm based on grouping the arms based on Hamming codes and performing likelihood-ratio tests to detect the presence of the best arm. 
\begin{algorithm}[ht]
\caption{\ac{RE}}
    \begin{algorithmic}[1]\label{alg:rapid_exploration}
        \State Group the $K$ arms into $\log_2(K)$ groups of size $K/2$ based on Algorithm~\ref{alg:cap}.
        \State Estimate of mean of arm $i$ after $s$ rounds is $\hat{X}_{i,s}$, $1 \leq i \leq K$
        \State Estimate of mean of group $G$ after $s$ rounds is $\hat{\mu}_{G,s}$, $1 \leq j \leq \log_2(K)$
        \State Exploration budget = $T$
        \State Uniform exploration parameter = $\alpha \in [0,1]$ 
        \For{$t = 1 : \alpha T$}
            \State Pull each of the $K$ arms uniformly for $\alpha T/K$ times and observe the rewards.
        \EndFor
         \State Obtain estimates of means of each arm $\hat{X}_{i,\alpha T/K}$ for all arms.
        \State Obtain estimates of means of each group $\hat{\mu}_{G,\alpha T} = \frac{\sum_{i \in G}\hat{X}_{i,\alpha T/K}}{K/2}$ and calculate priors $\pi_{0,G}, \pi_{1,G}$ for each group for the presence of the best arm
        \For{$t = \alpha T : T$}
            \State Pull each of the $\log_2(K)$ groups uniformly and observe the sum of the rewards of the individual arms.
        \EndFor
        \State Perform likelihood ratio test for each group to test whether the best arm is present in that group.
        \State Combine the results from the LRTs and recommend the arm after decoding. The best arm is the one that is common to all the groups in which it is detected.
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Construct Groups}\label{alg:cap}
\begin{algorithmic}[1]
\State {\bf Input:} Arms and $K$.
\State {\bf Initialize:} $G_k = \{\}$, $\forall k = 1, 2, \ldots, \log_2 K$.
\For{$k = 1$ to $\log_2 K$}
    \For{$i = 0$ to $K - 1$}
        \If{\texttt{dec2bin}$(i)$ \texttt{AND} \texttt{onehot}$(k) \neq \texttt{zeros}(1,n)$}
            \State $G_k = G_k \cup i$
        \EndIf
    \EndFor
\EndFor
\State Return $\{G_k, 1\leq k \leq \log_2 K\}$
\end{algorithmic}
\end{algorithm}

\subsection{Arm Grouping and Decoding Strategy}
Our algorithm commences with first creating $\log K$ groups of arms denoted as $G_k, k \in [\log K]$. This is the same grouping strategy employed in \cite{ghatak2023fast} for change detection, whereas here, we employ it to employ likelihood ratio tests discussed later. The $i$-th arm is added to a group $G_k$, if and only if the binary representation of $i-1$ has a "1" in the $k$-th binary place. In other words, arm $i$ is added to $G_k$ if
%\begin{align}
    $\texttt{bin2dec}\left(\texttt{dec2bin}(i-1) \; \texttt{ AND } \; \texttt{onehot}(k) \right) \neq 0,$
    %\nonumber 
%\end{align}
where $\texttt{bin2dec}()$ and $\texttt{dec2bin}()$ are respectively operators that convert binary numbers to decimals and decimal numbers to binary. Additionally, $\texttt{onehot}(k)$ is a binary number with all zeros except 1 at the $k$-th binary position. $\texttt{AND}$ is the bit-wise AND operator. The arm grouping strategy follows similarly to the parity bit generation using Hamming code, which belongs to the family of linear error-correcting codes~\cite{hamming1950error}. It is worth recalling that Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance of three. Thus, the minimum number of parity bits needed to detect and correct one-bit errors is $\log_2 (K)$ for a $K-$ bit sequence. As a consequence of this, the minimum groups that are needed to detect a change in a single arm as well as identify the changed arm is $\log_2 (K)$.

It can be noted that each group consists of $\frac{K}{2}$ arms. Once the groups are formed, we define the mean of the group $G_k$ as the average of the means of the arms constituting the group, i.e., $\mu_{G_k} = \frac{2}{K}\sum_{i \in G_k}\mu_i$.

\subsection{Separability}
A key assumption for the feasibility of the proposed algorithm is the {\it separability} of the arm groupings, i.e., the algorithm is able to differentiate a group which contains the best arm from a group which does not. In case the best arm is not present in a group, the mean of the group is upper bounded by the case that all the $K/2$ arms of that group have the same mean as the second best arm. Thus,
\begin{equation}
   \mu_{G_k} \leq \mu_{[2]} = \mu_{[1]} - \Delta_{[2]}, \quad \forall k \nonumber 
\end{equation}
if $a^* \notin G_k.$ Similarly, in case the best arm is present in a particular group, then the lower bound of the mean of this group is obtained by assuming that the remaining arms of that group consist of the arms with the lowest means. This lower bound is
\begin{equation}
    \mu_{G_k} \geq \frac{\mu_{[1]} + (K/2 - 1)\mu_{[K]}}{K/2},  \quad \forall k  \nonumber
\end{equation}
if $a^* \in G_k$. The separability assumption is formally stated below.
\begin{assumption}
    For the bandit problem to be well separable, we assume
\begin{align}
    %\mu_{[1]} - \bigg(1-\frac{2}{K}\bigg)\Delta_{[K]} \geq \mu_{[1]} - \Delta_{[2]}\\
    \frac{\Delta_{[K]}}{\Delta_{[2]}} \leq \frac{1}{1-2/K}.
\end{align}
The above follows from the condition $\frac{\mu_{[1]} + (K/2 - 1)\mu_{[K]}}{K/2} \geq  \mu_{[1]} - \Delta_{[2]}.$
\end{assumption}

Before the formation of the groups, we perform an initial exploration phase of length $\alpha T$ where each arm is probed for $\alpha T/K$ slots, $0 \leq \alpha \leq 1$. This phase is utilized for constructing initial estimates of the means of the arms which is further employed to engineer the prior probabilities of the hypothesis testing. Once the arms are grouped together, the groups are pulled followed by the composite hypothesis testing as discussed below.


\subsection{Hypothesis Test}
In what follows, we drop the subscript $k$ from $G_k$ since the analysis holds equivalently for all the groups. Let $H_{1,G}$ represent the hypothesis that the best arm is present in group $G$, and $H_{0,G}$ represent the hypothesis that the best arm is absent in group $G$. Mathematically,
\begin{align}
    &H_{1,G} := \mu_{[1]} \in G  \nonumber \\ \nonumber
            &\equiv \hat{\mu}_G \in \Lambda_1 = [\mu_{[1]}- (1-2/K)\Delta_{[K]}, \mu_{[1]} - (1-2/K)\Delta_{[2]}],
\\
    &H_{0,G} := \mu_{[1]} \notin G   \nonumber
            \equiv \hat{\mu}_G \in \Lambda_0 = [\mu_{[1]}-\Delta_{[K]}, \mu_{[1]} - \Delta_{[2]}].
\end{align}
The Bayes' decision rule for identifying the presence/absence of the best arm in the above problem is \cite{DnE_book}
\begin{equation}
   \delta_{G} = \begin{cases}
       0 \\
       \gamma \\
       1
   \end{cases} 
   \frac{\mathbb{P}(\mu_{[1]} \in G | \textit{rewards of batch pulls})}{\mathbb{P}(\mu_{[1]} \notin G | \textit{rewards of batch pulls})} \lesseqgtr \frac{C_{10} - C_{00}}{C_{01} - C_{11}},
\end{equation}
where $C_{ij}$ is the cost of assigning $H_i$ when ground truth is $H_j$. We consider uniform assignment costs $(C_{10} = C_{01} = 1, C_{11} = C_{00} = 0)$, and the above rule simplifies to
\begin{equation}
    \delta_{G} = \begin{cases}
        0 \\
         \gamma \\
        1
        \end{cases}
        \frac{p(R_{G, 1:(1-\alpha)T/\log_2(K)} | \mu_{[1]} \in G)}{p(R_{G,(1:1-\alpha)T/\log_2(K)} | \mu_{[1]} \notin G)} \lesseqgtr \frac{\pi_{0,G}}{\pi_{1,G}},
    \end{equation}
where $R_{G,1:t}$ representing the rewards seen from group $G$ after $t$ pulls. The priors $\pi_{0,G}, \pi_{1,G}$ represent our beliefs on whether the best arm is present or absent in group $G$ based on our initial exploration for $\alpha T$ rounds, which is elaborated in the next sub-section. The function $p(\cdot)$ represents the probability distribution function of the rewards of each group.


\subsection{Initial Exploration and Engineering the Priors}
In order to employ the composite hypothesis test described above, we require the priors $\pi_{0,G}, \pi_{1,G}$ which represent the probabilities that the best arm is absent or present in group $G$. Since this is not present with the agent, it can be engineered with the information gained during the initial uniform exploration stage for $\alpha T$ rounds. Recall that during the initial exploration stage, each arm is pulled $\alpha T/K$ times and we obtain estimates of means of all arms $\hat{X}_{i,\alpha T/K}$ $(1\leq i\leq K)$. Using these estimates, we then obtain the mean estimates of each of the groups $\hat{\mu}_{G, \alpha T}$ $(1\leq j\leq \log_2(K))$ as
\begin{align}
        \hat{\mu}_{G, \alpha T} = \frac{\sum_{i \in G} \hat{X}_{i,\alpha T/K}}{K/2}.
\end{align}
Let the means of the group under hypotheses $H_{0,G}$ and $H_{1,G}$ be $\mu_L$ and $\mu_H$, respectively. Note that $\mu_L$ and $\mu_H$ are random variables whose distribution depends on how the distribution of each arm at each index is decided. Their expected values are given by $\bE_{\nu}[\mu_G | \mu_{[1]} \in G], \bE_{\nu}[\mu_G | \mu_{[1]} \notin G]$. Since the priors sum up to 1, we can view the pair as the parameters of a Bernoulli distribution. In other words, the event that the best arm belongs to this group or not is a Bernoulli random variable with parameters given by the above means. We substitute $\mu_H = \mu_{[1]} - (1-2/K)(\Delta_{min} + \Delta_{max})/2$ and $\mu_L = \mu_{[1]} - (\Delta_{min} + \Delta_{max})/2$. This is because if the arms are arranged randomly among the $K$ positions, and the means of the suboptimal arms are chosen uniformly at random from $[\mu_{[1]} - \Delta_{max}, \mu_{[1]} - \Delta_{min}]$, we can obtain the substituted values of $\mu_H$ and $\mu_L$ %Recall that the Bernoulli distribution is a member of the exponential family with the parameter being the sigmoid of the natural parameter~\cite{GLM}.
Thus, introducing sigmoids into the formulation of the priors is a natural choice~\cite{GLM}. However, a key issue is the asymmetric nature of the domain as the composite hypotheses $H_{1,G}$ and $H_{0,G}$ have supports of different sizes. To handle this asymmetry, we will construct two sigmoids, and normalize them appropriately. We now define $\sigma_{in}(x)$ and $\sigma_{out}(x)$ which represent the priors that the best arm is present or absent in group $G$:
\begin{align}
    \sigma_{in}(x) &= \dfrac{1}{1+\exp(-(x-\mu_H)/|\Lambda_1|)}, \nonumber \\
    \sigma_{out}(x) &= \dfrac{1}{1 + \exp((x-\mu_L)/|\Lambda_0|)}, \nonumber
\end{align}
where $|\Lambda_0|, |\Lambda_1|$ represent the size of the intervals corresponding to the hypotheses $H_{0,G}, H_{1,G}$. We further normalize them to obtain the engineered $\pi_{0,G}, \pi_{1,G}$ as
\begin{equation}
    \pi_{0,G} = \frac{\sigma_{out}(\hat{\mu}_G)}{\sigma_{out}(\hat{\mu}_G) + \sigma_{in}(\hat{\mu}_G)}, \hspace{0.5cm}
    \pi_{1,G} = \frac{\sigma_{in}(\hat{\mu}_G)}{\sigma_{out}(\hat{\mu}_G) + \sigma_{in}(\hat{\mu}_G)}
\end{equation}
This is a good choice for priors as $\sigma_{in}(\cdot)$ is an increasing function of $\hat{\mu}_G$ and $\sigma_{out}(\cdot)$ is a decreasing function of $\hat{\mu}_G$. Also, the arguments are normalized such that the majority of the rise/fall of the sigmoid occurs in their respective hypothesis intervals, i.e., $\Lambda_1$ and $\Lambda_0$, respectively.

\subsection{Recommendation of the Best Arm}
The final recommendation of the best arm follows the results of the hypothesis tests of the individual groups. We employ a Hamming decoding method, wherein, the best arm is the one that belongs to the all groups in which in which it is detected. Thanks to Hamming codes, the binary output of the presence or the absence of the best arm in each group uniquely determines the best arm. An error in determining the best arm occurs when at least one of the group likelihood ratio tests fail. In the next section, we analyze the probability of error of the proposed algorithm. We consider 2 cases, one for Gaussian reward distribution and the other for uniform reward distribution. The total probability of error is given by
\begin{align}
    p_{(\cdot)}^{\rm RE} \triangleq 1 - \prod_{i = 1}^{\log_2 K} (1-\cP_{e,G_1i}), \nonumber
\end{align}
where $\cP_{e,G_1i} = \mathbb{P}\left(\delta_G \geq \frac{\pi_{0,G}}{\pi_{1,G}}, H_{1,G}\right) + \mathbb{P}\left(\delta_G < \frac{\pi_{0,G}}{\pi_{1,G}}, H_{0,G}\right)$. Assume that group $G$ has the highest probability of error, this allows us to bound $p^{\rm RE}_{(\cdot)}$ as
\begin{align}
    p^{\rm RE}_{(\cdot)} \leq 1 - (1-\cP_{e,G})^{\log_2(K)} \leq \log_2(K) \cP_{e,G}. \nonumber 
\end{align}


\section{Probability of Error with Gaussian Rewards}
The distribution of the reward of a group at each time step, conditioned on the presence or absence of the best arm in the group are
%a product as \textcolor{red}{The following expression needs clarity.}
%\begin{equation}\label{eqn:iid1}
%    p(R_{G, 1:(1-\alpha) T/\log_2(K)} | \mu_{[1]} \in G) = \prod_{t = 1}^{(1-\alpha) %T/\log_2(K)} p(R_{G,t} | \mu_{[1]} \in G)
%\end{equation}
% \begin{equation}\label{iid2}
%     p(R_{G, 1:(1-\alpha) T/\log_2(K)} | \mu_{[1]} \notin G) = \prod_{t = 1}^{(1-\alpha) T/\log_2(K)} p(R_{G,t} | \mu_{[1]} \notin G)
% \end{equation}
% For Gaussian bandit models, the conditional distributions above are Gaussian distributed with means $\mu_H, \mu_L$ \footnote{The distribution of $\mu_H$ and $\mu_L$ is discussed in appendix \ref{app:mu_H,mu_L distribution}. Their values are set to the means of their distributions in all the results derived.} respectively and variance $\sigma^2$. This follows from the fact the sum of independent Gaussians is also Gaussian with the means and variances being summed up. Thus,
    \begin{align}
        R_{G,t}|\mu_{[1]} \in G &\sim \cN(\mu_H, \sigma^2/(K/2)) \nonumber \\
         R_{G,t}|\mu_{[1]} \notin G &\sim \cN(\mu_L, \sigma^2/(K/2)) \nonumber
    \end{align}
Let $\overline{R}_{G,t}$ be the mean reward of the group $G$ after $t$ pulls. Since each group is pulled $(1-\alpha)T/\log_2(K)$ times, the conditional distributions of the group means are given by
\begin{align}
    \overline{R}_{G, (1-\alpha)T/\log_2(K)}|\mu_{[1]} &\in G \sim \cN\bigg(\mu_H, \frac{2\sigma^2\log_2(K)}{(1-\alpha)KT}\bigg) \nonumber
\\
    \overline{R}_{G, (1-\alpha)T/\log_2(K)}|\mu_{[1]} &\notin G \sim \cN\bigg(\mu_L, \frac{2\sigma^2\log_2(K)}{(1-\alpha)KT}\bigg) \nonumber
\end{align}
Thus, the likelihood ratio $\mathcal{L} = \frac{p(R_{G, 1:(1-\alpha)T/\log_2(K)} | \mu_{[1]} \in G)}{p(R_{G, 1:(1-\alpha)T/\log_2(K)} | \mu_{[1]} \notin G)}$ is
    \begin{align}
        \mathcal{L} = \nonumber 
        \frac{\exp\bigg(-\frac{(1-\alpha)TK(\overline{R}_{G,(1-\alpha)T/\log_2(K)} - \mu_H)^2}{4 \sigma^2 \log_2(K)}\bigg)}{\exp\bigg(-\frac{(1-\alpha)TK(\overline{R}_{G,(1-\alpha)T/\log_2(K)} - \mu_L)^2}{4 \sigma^2 \log_2(K)}\bigg)}, 
    \end{align}
    and the final decision rule now simplifies to 
     \begin{align}
        \overline{R}_{G,(1-\alpha)T/\log_2(K)} &\lesseqgtr \frac{\mu_H + \mu_L}{2} + \frac{2\sigma^2\ln(\pi_{0,G}/\pi_{1,G})\log_2(K)}{(1-\alpha)TK(\mu_H-\mu_L)} \nonumber \\
        &\triangleq \tau_G \nonumber
    \end{align}

    Now, the algorithm can make a mistake when either a \textit{false alarm} (Type-I error) or a \textit{missed detection} (Type-II error) occurs. A \textit{false alarm} occurs when the ground truth is that the best arm does not belong to the group but the LRT outputs $H_1$, and a \textit{missed detection} occurs when the ground truth is that the best arm is present in the group but the LRT outputs $H_0$. Thus, the probability of error of a single hypothesis test for group $G$ can be written as

    \begin{align}
        &\cP_{e,G} = \mathbb{P}\left(\delta_G \geq \frac{\pi_{0,G}}{\pi_{1,G}}, H_{1,G}\right) + \mathbb{P}\left(\delta_G < \frac{\pi_{0,G}}{\pi_{1,G}}, H_{0,G}\right) \nonumber \\
        &= \Tilde{\pi}_{0,G}\bP_{\nu}\left(\delta_G < \frac{\pi_{0,G}}{\pi_{1,G}} \Large| H_{0,G}\right) + \Tilde{\pi}_{1,G}\bP_{\nu}\left(\delta_G \geq \frac{\pi_{0,G}}{\pi_{1,G}} \Large| H_{1,G}\right). \label{eqn:grp_error_prob}
    \end{align}
Note that $\Tilde{\pi}_{0,G}$ and $\Tilde{\pi}_{1,G}$ are the actual probabilities of the best arm being absent and present in group $G$ and not the estimates $\pi_{0,G}, \pi_{1,G}$ which have been obtained. The \textit{false alarm} and \textit{missed detection} probabilities can be expressed in terms of the tail probability of Gaussian random variables as,
    \begin{align}
        \mathcal{P}_{\rm F} & = \bP_{\nu}\left(\delta_G \geq \frac{\pi_{0,G}}{\pi_{1,G}} \Large| H_{0,G}\right) \nonumber \\
        &= \mathbb{P}\bigg(\overline{R}_G > \tau_G| \overline{R}_G \sim \mathcal{N}\bigg(\mu_L, \frac{2\sigma^2\log_2(K)}{(1-\alpha)TK}\bigg)\bigg) \nonumber \\
        &= \mathbb{P}\bigg(\mathcal{N}(0,1) > (\tau_G - \mu_L)\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}}\bigg) \nonumber \\
        &= Q\bigg((\tau_G - \mu_L)\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}}\bigg)
    \end{align}
    \begin{align}
        \mathcal{P}_{\rm M} = &\bP_{\nu}\left(\delta_G < \frac{\pi_{0,G}}{\pi_{1,G}} \Large| H_{1,G}\right) \nonumber \\
        &= \mathbb{P}\bigg(\overline{R}_G < \tau_G| \overline{R}_G \sim \mathcal{N}\bigg(\mu_H, \frac{2\sigma^2\log_2(K)}{(1-\alpha)TK}\bigg)\bigg) \nonumber \\
        &= \mathbb{P}\bigg(\mathcal{N}(0,1) < (\tau_G - \mu_H)\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}}\bigg) \nonumber \\
        &= Q\bigg((\mu_H - \tau_G)\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}}\bigg)
    \end{align}
    where 
    \begin{equation}
        Q(x) = \frac{1}{\sqrt{2\pi}}\int_{x}^{\infty} e^{-t^2/2} dt 
    \end{equation}
    We now bound the probability of error with the maximum of the \textit{false alarm} and \textit{missed detection} probabilities. Define $\pi_{high,G} = \max\{\pi_{0,G}, \pi_{1,G}\}$ amd $\pi_{low,G} = \min\{\pi_{0,G}, \pi_{1,G}\}$. Thus, 
    \begin{align}
        \cP_{e,G} &\leq \max\{\mathcal{P}_{\rm F} + \mathcal{P}_{\rm M}\} \nonumber \leq  Q\bigg(\frac{\mu_H - \mu_L}{2} \sqrt{\frac{(1-\alpha)TK}{2\sigma^2\log_2(K)}}- \nonumber \\
        &\sqrt{\frac{2\sigma^2\log_2(K)}{(1-\alpha)TK}}\frac{\ln(\pi_{high,G}/\pi_{low,G})}{(\mu_H-\mu_L)}\bigg) \label{eqn:max FA,MD}
    \end{align}
    From \cite{Q_function_bound}, we have the following bounds on $Q(\cdot)$
    \begin{equation}\label{eqn:Qbound}
        \frac{x}{(1+x^2)\sqrt{2\pi}} e^{-x^2/2} < Q(x) < \frac{1}{x\sqrt{2\pi}}e^{-x^2/2}
    \end{equation}
    Therefore, for sufficiently large budget according to (\ref{eqn:budget_constraint}) such that the argument of the $Q(\cdot)$ function is positive, we can then apply (\ref{eqn:Qbound}) to get the result of Theorem \ref{thm:1}
    
    


\begin{theorem}\label{thm:1}
For Gaussian rewards with constant variance $\sigma^2$ and $\alpha = 0$\footnote{$\alpha = 0$ is chosen as it is the minimax value of $\alpha$, i.e., it minimizes the worst case error probability, proved in appendix \ref{app:minimax_alpha}.}, the error probability of Algorithm~\ref{alg:rapid_exploration} is bounded as
    \begin{equation}\label{eqn:RE_Gaussian}
        p^{\rm SR}_{G} \leq \sqrt{\frac{4H_4\sigma^2 K\log^3_2(K)}{\pi T}}\exp\bigg(\frac{-T}{16H_4\sigma^2K\log_2(K)}\bigg)
    \end{equation}
    for a horizon
    \begin{equation}\label{eqn:budget_constraint}
          T > \frac{4\sigma^2\log_2(K)}{K(1-\alpha)(\mu_H-\mu_L)^2} \ln{\frac{\pi_{high,G}}{\pi_{low,G}}}.
    \end{equation}
\end{theorem}
Note that for a bandit model $\nu$ where all the non-optimal arms have the same mean, $H_1 = H_2 = H_3 = 4KH_4$. Thus, plugging this value of $H_4$ into (\ref{eqn:RE_Gaussian}) gives a better result than (\ref{eqn:SR_Gaussian}) and (\ref{eqn:SH_Gaussian}). Thus, for a problem instance with a large number of arms, the \ac{RE} algorithm gives us a lower probability of error than the algorithms designed for non-combinatorial bandits. It also performs better than Combinatorial Successive Accept Reject (CSAR) algorithm of \cite{CPE}. The CSAR algorithm also requires access to a \textit{constrained oracle} to compute the optimal set to sample in each phase of the algorithm.

\section{Probability of Error with Bounded Rewards}
Unlike the previous section, where we used the $Q(\cdot)$ function to bound the error probability, we make use of Bernstein's inequality to bound the probability of error~\cite{giroux1979bernstein}. The group reward observations are i.i.d and the variance is decreased by a factor of $K/2$, which is the group size. 
\begin{align}
    &\bP_{\nu}\left(\delta_G \geq \frac{\pi_{0,G}}{\pi_{1,G}} \Large| H_{0,G}\right) \nonumber \\
    &= \bP\bigg(\overline{R}_{G, T/\log_2(K)} > \frac{\mu_H + \mu_L}{2} | \mu_{[1]} \notin G\bigg) \nonumber \\
    &= \bP\bigg(\overline{R}_{G, T/\log_2(K)} - \mu_L > \frac{\mu_H - \mu_L}{2} | \mu_{[1]} \notin G\bigg) \nonumber\\
    &\leq \exp\bigg(-\frac{T(\mu_H - \mu_L)^2}{8\log_2(K)(\frac{\sigma_{max}^2}{K/2} + \frac{\mu_H - \mu_L}{6})}\bigg) \nonumber\\
    &\leq \exp\bigg(-\frac{T}{8K^2\log_2(K)H_4(\frac{2\sigma_{max}^2}{K} + \frac{1}{6K\sqrt{H_4}})}\bigg) \nonumber\\
    &\leq \exp\bigg(-\frac{T}{8K\log_2(K)H_4(\frac{1}{2} + \frac{1}{6\sqrt{H_4}})}\bigg)
\end{align}
\begin{align}
    &\bP_{\nu}\left(\delta_G < \frac{\pi_{0,G}}{\pi_{1,G}} \Large| H_{1,G}\right) \nonumber \\
    &= \bP\bigg(\overline{R}_{G, T/\log_2(K)} < \frac{\mu_H + \mu_L}{2} | \mu_{[1]} \in G\bigg) \nonumber\\
    &= \bP\bigg(\overline{R}_{G, T/\log_2(K)} - \mu_H < -\frac{\mu_H - \mu_L}{2} | \mu_{[1]} \in G\bigg) \nonumber\\
    &\leq \exp\bigg(-\frac{T(\mu_H - \mu_L)^2}{8\log_2(K)(\frac{\sigma_{max}^2}{K/2} + \frac{\mu_H - \mu_L}{6})}\bigg) \nonumber\\
    &\leq \exp\bigg(-\frac{T}{8K^2\log_2(K)H_4(\frac{2\sigma_{max}^2}{K} + \frac{1}{6K\sqrt{H_4}})}\bigg) \nonumber\\
    &\leq \exp\bigg(-\frac{T}{8K\log_2(K)H_4(\frac{1}{2} + \frac{1}{6\sqrt{H_4}})}\bigg)
    \end{align}
    Similar to (\ref{eqn:max FA,MD}), we get
    \begin{equation}
        \cP_{e,G} \leq \exp\bigg(-\frac{T}{8K\log_2(K)H_4(\frac{1}{2} + \frac{1}{6\sqrt{H_4}})}\bigg)
    \end{equation}

\begin{theorem}\label{thm:2}
For bounded rewards in $[0,1]$ and $\alpha = 0$, the error probability of Algorithm~\ref{alg:rapid_exploration} is bounded as
    \begin{equation}\label{eqn:RE_Bounded}
        p^{\rm RE}_B \leq \log_2(K)\exp\bigg(-\frac{T}{8H_4K\log_2(K)(\frac{1}{2} + \frac{1}{6\sqrt{H_4}})}\bigg)
    \end{equation}
\end{theorem}
Thus, there is a clear advantage in the derived bound in the leading term. However, based on the value of the hardness parameter, experimentally often the \ac{RE} algorithm under performs as discussed next.

\section{Experiments}
\label{app:experiments}
We verify our theoretical findings with numerical experiments. We consider the case where $\Delta_{[2]} = \Delta_{[K]} = \Delta$ due to the assumption on separability, thus, the largest suboptimality gap cannot be too much higher than the smallest suboptimality gap. We consider 2 settings, one with Gaussian rewards and another with Bernoulli rewards. The Gaussian bandits are considered with $\sigma^2 = 0.5$ and the best arm in the Bernoulli bandits has a mean of $\mu_{[1]} = 0.5$. We detail the hardness parameters of the setups considered in Table~\ref{table:hardness}.

\subsection{Error Probability vs. Budget (Figures 2-6)}
These figures illustrate how the probability of error decreases as the budget $T$ increases for different algorithms. Each plot compares our \ac{RE} algorithm against UE, SR and SH, along with their respective theoretical upper bounds (UB). 

{\bf Gaussian Rewards (Figures 2 and 4):} For larger budgets, \ac{RE} consistently achieves a lower probability of error compared to SR and SH, particularly when the hardness parameter $H_4$ is small. However, for small budgets, UE performs slightly better than RE, highlighting that our method benefits more from increased exploration. As the number of arms $K$ increases, RE’s advantage becomes more prominent due to its ability to efficiently group and test arms in a structured manner.


{\bf Bernoulli Rewards (Figures 3 and 5):} Similar to the Gaussian case, \ac{RE} outperforms other methods for large $T$, but for small budgets, UE performs better. The gap between \ac{RE} and UE diminishes as $K$ increases, suggesting that when all suboptimal arms have similar gaps, UE can be a competitive alternative.

\begin{table*}
    \centering
     \begin{tabular}{||c |c |c |c | c | c ||}
     \hline
     Distribution Type & Suboptimality gap & $K = 4$ & $K = 8$ & $K = 16$ & $K = 32$ \\[0.5ex] 
     \hline\hline
     Bernoulli& $\Delta_{[2]} =\Delta_{[K]} = 0.1$& $-$ & $-$&$1.5 \times 10^3$ & $3.1 \times 10^3$\\
    \hline
    Bernoulli& $\Delta_{[2]} =\Delta_{[K]} = 0.05$& $-$ & $-$ &$6 \times 10^3$ & $1.24 \times 10^4$\\
    \hline
    Bernoulli& $\Delta_{[2]} = \Delta_{[K]}(1-2/K) = 0.1$& $250$ & $537.5$ & $1.2719 \times 10^3$ & $-$\\
    \hline
    Bernoulli& $\Delta_{[2]} = \Delta_{[K]}(1-2/K) = 0.05$& $10^3$ & $2.15 \times 10^3$ & $5.0875 \times 10^3$ & $-$\\
    \hline
    Gaussian & $\Delta_{[2]} =\Delta_{[K]} = 0.075$& $-$ & $-$ &$2.67 \times 10^3$ & $5.51 \times 10^3$ \\
    \hline
    Gaussian & $\Delta_{[2]} =\Delta_{[K]} = 0.025$& $-$ & $-$ &$2.4 \times 10^4$ & $4.96 \times 10^4$ \\
    \hline
     Gaussian& $\Delta_{[2]} = \Delta_{[K]}(1-2/K) = 0.075$& $444.44$ & $955.56$ & $2.2611 \times 10^3$ & $-$\\
    \hline
     Gaussian & $\Delta_{[2]} = \Delta_{[K]}(1-2/K) = 0.025$& $4 \times 10^3$ & $8.6 \times 10^3$ & $2.035 \times 10^4$ & $-$\\
    \hline
     \end{tabular}
     \caption{The value of $H_1$ for different setups considered.}
     \label{table:hardness}
\end{table*}


\begin{figure*}
    \centering
    \subfloat[$\Delta = 0.15$, $K = 16$.]{\includegraphics[width=0.3\textwidth]{Gaussian1.pdf}} 
    \hfil
    \subfloat[$\Delta = 0.075$, $K = 16$]{\includegraphics[width=0.3\textwidth]{Gaussian2.pdf}}
    \hfil
    \subfloat[$\Delta = 0.025$, $K = 16$]{\includegraphics[width=0.3\textwidth]{Gaussian3.pdf}}
    \hfil
       \subfloat[$\Delta = 0.15$, $K = 32$]{\includegraphics[width=0.3\textwidth]{Gaussian4.pdf}}
       \hfil
    \subfloat[$\Delta = 0.075$, $K = 32$]{\includegraphics[width=0.3\textwidth]{Gaussian5.pdf}}
    \hfil
    \subfloat[$\Delta = 0.025$, $K = 32$]{\includegraphics[width=0.3\textwidth]{Gaussian6.pdf}}
    \caption{Upper bound and empirical error probability for Gaussian rewards}
    \label{fig:Gaussian1}
\end{figure*}



\begin{figure*}
    \centering
    \subfloat[$\Delta = 0.1, K= 16$]{\includegraphics[width=0.3\textwidth]{Bernoulli1.pdf}} 
    \hfil
    \subfloat[$\Delta = 0.05, K= 16$]{\includegraphics[width=0.3\textwidth]{Bernoulli2.pdf}} 
    \hfil
    \subfloat[$\Delta = 0.01, K= 16$]{\includegraphics[width=0.3\textwidth]{Bernoulli3.pdf}} 
    %\caption{$K = 16$, Bernoulli Rewards}
        \hfil
        \subfloat[$\Delta = 0.1, K= 32$]{\includegraphics[width=0.3\textwidth]{Bernoulli4.pdf}} 
    \hfil
    \subfloat[$\Delta = 0.05, K= 32$]{\includegraphics[width=0.3\textwidth]{Bernoulli5.pdf}} 
    \hfil
    \subfloat[$\Delta = 0.01, K= 32$]{\includegraphics[width=0.3\textwidth]{Bernoulli6.pdf}} 
    \caption{Upper bound and empirical error probability for Bernoulli rewards.}
    \label{fig:Bernoulli1}
\end{figure*}

\begin{figure*}
    \centering
    \subfloat[$\Delta = 0.075, K = 4$]{\includegraphics[width=0.22\textwidth]{Gaussian10.pdf}} 
    \subfloat[$\Delta = 0.025, K = 4$]{\includegraphics[width=0.22\textwidth]{Gaussian11.pdf}}
    \subfloat[$\Delta = 0.075, K = 8$]{\includegraphics[width=0.22\textwidth]{Gaussian12.pdf}} 
    \subfloat[$\Delta = 0.025, K = 8$]{\includegraphics[width=0.22\textwidth]{Gaussian13.pdf}} 
    \caption{Gaussian Rewards}
    \label{fig:Gaussian2}
\end{figure*}


\begin{figure*}
    \centering
    \subfloat[$\Delta = 0.1, K = 4$]{\includegraphics[width=0.22\textwidth]{Bernoulli10.pdf}} 
    \subfloat[$\Delta = 0.05, K = 4$]{\includegraphics[width=0.22\textwidth]{Bernoulli11.pdf}} 
    \subfloat[$\Delta = 0.1, K = 8$]{\includegraphics[width=0.22\textwidth]{Bernoulli12.pdf}} 
    \subfloat[$\Delta = 0.05, K = 8$]{\includegraphics[width=0.22\textwidth]{Bernoulli13.pdf}} 
    \caption{Bernoulli Rewards}
    \label{fig:Bernoilli2}
\end{figure*}


{\bf Error Probability vs. Number of Arms (Figures 6-7):} These figures analyze how the probability of error changes as the number of arms $K$ increases, under different budgets.

{\bf Low Budget (Figures 7a and 8a):} \ac{RE} has a higher error probability compared to UE, indicating that the lack of initial arm-level reward information makes combinatorial exploration less effective when resources are scarce.

{\bf Moderate Budget (Figures 7b and 8b):} \ac{RE} starts to outperform SR and SH, but UE remains competitive. The benefit of structured exploration is more noticeable in this regime.

{\bf Large Budget (Figures 7c and 8c):} \ac{RE} significantly outperforms other algorithms, demonstrating that its structured approach allows for more efficient identification of the best arm in large problem instances.

\subsection{Key Observations and Takeaways}
\begin{itemize}
    \item \ac{RE} performs best in large-budget scenarios where grouping-based exploration can fully leverage structured testing to reduce error probability.
    \item UE remains competitive when suboptimality gaps are similar across arms, as it allocates equal exploration to all arms.
    \item When the budget is small, \ac{RE} struggles because it does not receive sufficient individual arm-level feedback before making group-based inferences.
\end{itemize}
These results highlight the importance of considering both budget constraints and problem hardness.

\begin{figure*}
    \centering
    \subfloat[$T = 300$]{\includegraphics[width=0.3\textwidth]{pekGaussian4.pdf}} 
    \subfloat[$T = 3000$]{\includegraphics[width=0.3\textwidth]{pekGaussian5.pdf}} 
    \subfloat[$T = 30000$]{\includegraphics[width=0.3\textwidth]{pekGaussian6.pdf}} 
    \caption{$\Delta = 0.025$, Gaussian Rewards}
    \label{fig:Gaussian3}
\end{figure*}

\begin{figure*}
    \centering
    \subfloat[$T = 300$]{\includegraphics[width=0.3\textwidth]{pekBernoulli4.pdf}} 
    \subfloat[$T = 3000$]{\includegraphics[width=0.3\textwidth]{pekBernoulli5.pdf}} 
    \subfloat[$T = 30000$]{\includegraphics[width=0.3\textwidth]{pekBernoulli6.pdf}} 
    \caption{$\Delta = 0.05$, Bernoulli Rewards}
    \label{fig:Bernoulli3}
\end{figure*}
    
\section{Conclusions and Future Work}
We proposed an arm grouping strategy for the fixed budget BAI problem in MABs where the agent is allowed to sample multiple arms at a time. In this setup, the agent has to trade-off between sampling a larger number of arms, thereby obtaining a wider view of the environment at the cost of reduced information about per-arm reward distributions, or playing fewer arms that reveal fine-grained reward distributions at the cost of increasing the sample complexity of probing the entire environment. Our algorithm is based on pulling groups of arms formed using binary representation of the arm indices and Hamming codes. For each group the agent performs a likelihood ratio test to determine whether or not the best arm is present in that group. For Gaussian and bounded rewards, we derived the upper bound of the probability of error and discussed the conditions under which it outperforms the state-of-the-art algorithms for the single pull setting.

To the best of our knowledge this is the first work that addresses the BAI problem with combinatorial exploration where the rewards are non-trivial (e.g., not constant). In this paper, we considered a special case of this framework where on sampling multiple arms, the agent receives the sample average of the individual rewards of the arms pulled. Other functions than the sample average of the rewards can be of interest based on the application. Furthermore, deriving the lower bound of the probability of error with combinatorial pulls is indeed an interesting open question that we will address in a future work.

\appendices
\section{Analysis of the Uniform Exploration strategy}
\label{app:UE}
In this section, we give the results for the uniform exploration strategy for the best-arm identification problem in the non-combinatorial setting. Let the means of the $K$ arms be $(\mu_1, \mu_2, \dots, \mu_K)$ and the budget be $T$. We allot equal budget to all the arms and pull each arm $T/K$ times. The algorithm makes a mistake if any of the suboptimal arms has a higher empirical mean after $T/K$ rounds. Therefore,

\begin{align}
    p_{(\cdot)}^{\rm UE} &= \bP_{\nu}(\hat{a}_T \neq a^*) \nonumber\\
        &= \bP_{\nu}(\cup_{i = 1, i \neq a^*}^K \hat{X}_{a^*,T/K} < \hat{X}_{i,T/K}) \nonumber\\
        &\leq \sum_{i=1, i\neq a^*}^K \bP_{\nu}(\hat{X}_{a^*,T/K} < \hat{X}_{i,T/K}) \nonumber\\
        &\leq \sum_{i=2}^K \bP_{\nu}((\hat{X}_{i,T/K} - \mu_{[i]}) - (\hat{X}_{a^*, T/K} - \mu_{[1]}) > \Delta_{[i]}) \nonumber
\end{align}

Thus, for Gaussian bandit models with variance of all arms being $\sigma^2$, we have
\begin{equation}
    (\hat{X}_{i,T/K} - \hat{X}_{a^*,T/K}) \sim \cN(\mu_{[i]}-\mu_{[1]}, \frac{2K\sigma^2}{T}) \nonumber
\end{equation}
\begin{align}\label{eqn:uniform_exploration_bound_Gaussian}
    p_{G}^{\rm UE} &\leq \sum_{i=2}^K \bP_{\nu}\bigg(\cN(0,1) > \Delta_{[i]}\sqrt{\frac{T}{2K\sigma^2}}\bigg) \nonumber\\
    &\leq \sum_{i=2}^K Q\bigg(\Delta_{[i]}\sqrt{\frac{T}{2K\sigma^2}}\bigg) \leq (K-1)Q\bigg(\Delta_{[1]}\sqrt{\frac{T}{2K\sigma^2}}\bigg) \nonumber\\
    &\leq (K-1)\sqrt{\frac{H_3\sigma^2}{\pi T}} \exp\bigg(\frac{-T}{4H_3\sigma^2}\bigg)\nonumber
\end{align}

For bounded bandit models, instead of using $Q$ functions, we make use of Hoeffding's inequality to bound the error probability as,
\begin{align}
    p_{B}^{\rm UE} &\leq \sum_{i=2}^K \bP_{\nu}((\hat{X}_{i,T/K} - \mu_{[i]}) - (\hat{X}_{J^*, T/K} - \mu_{[1]}) > \Delta_{[i]}) \nonumber\\
    &\leq \sum_{i=2}^K \bP_{\nu}\bigg( \sum_{t=1}^{T/K} X_{i,t} - X_{J^*,t}  - \frac{T}{K}\bigg(\mu_{[i]} - \mu_{[1]}\bigg)> \frac{T}{K}\Delta_{[i]}\bigg) \nonumber\\
    &\leq \sum_{i=2}^K \exp\bigg(\frac{-T\Delta_{[i]}^2}{2K}\bigg) \nonumber\\
    &\leq (K-1)\exp\bigg(\frac{-T}{2H_3}\bigg).  \nonumber
\end{align}



\section{Minimax Value of initial exploration parameter $\alpha$}\label{app:minimax_alpha}

The error probability of the hypothesis test of a particular group as given by in (\ref{eqn:grp_error_prob}) is
\begin{align}
    \cP_{e,G} \leq &\Tilde{\pi}_{0,G}Q\Bigg((\tau_G - \mu_L)\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}}\Bigg) + \nonumber \\
    &\Tilde{\pi}_{1,G}Q\Bigg((\mu_H - \tau_G)\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}}\Bigg) \nonumber \\
    &\leq  \Tilde{\pi}_{0,G} Q\Bigg(\frac{\mu_H-\mu_L}{2}\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}} +  \nonumber \\
    & \sqrt{\frac{2\sigma^2 \log_2(K)}{(1-\alpha)TK}}\frac{\ln(\pi_{0,G}/\pi_{1,G})}{\mu_H - \mu_L}\Bigg) \nonumber \\
       & \quad+  \Tilde{\pi}_{1,G} Q\Bigg(\frac{\mu_H-\mu_L}{2}\sqrt{\frac{(1-\alpha)TK}{2\sigma^2 \log_2(K)}} - \nonumber \\
       &\sqrt{\frac{2\sigma^2 \log_2(K)}{(1-\alpha)TK}}\frac{\ln(\pi_{0,G}/\pi_{1,G})}{\mu_H - \mu_L}\Bigg). \nonumber
\end{align}

Thus, if we had a high value of $\alpha$ and either $\pi_{0,G}$ or $\pi_{1,G}$ was high, the error would be low. The worst case is when even after the initial exploration, the estimates are 0.5. This makes the 2nd term in the argument 0, and reduces the 1st term as well. Thus, to maximize the 1st term, we set $\alpha = 0$. Here there is no initial exploration phase. Since there are $K/2$ arms in a group, and the best arm is assigned at random among the $K$ arms, the initial estimates of $\pi_{0,G}, \pi_{1,G}$ is 0.5, which is equivalent to the worst case. Therefore, the value of $\alpha$ which minimizes the maximum error is 0.    Thus,
    \begin{align}
        \cP_{e,G, minimax} &\leq Q\bigg(\frac{\mu_H - \mu_L}{2} \sqrt{\frac{KT}{2\sigma^2\log_2(K)}}\bigg) \nonumber \\
        &\leq Q\bigg(\sqrt{\frac{T}{8H_4\sigma^2K\log_2(K)}}\bigg).\nonumber
    \end{align}

\bibliographystyle{IEEEtran}
\bibliography{refer.bib}



\section{On the distribution of $\mu_H$ and $\mu_L$}\label{app:mu_H,mu_L distribution}

We assume that the best arm is chosen to be one of the $K$ arms at random. 
We now plot the distribution of the conditional means $\mu_H, \mu_L$ of the group with and without the best arm.
\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/distribution.pdf}
    \caption{The conditional distribution of $\mu_H$ and $\mu_L$}
    \label{fig:enter-label}
\end{figure}
The above figure is for $K = 8$ and $\Delta_{[K]} = \frac{K}{K-2}\Delta_{[2]}$. 
For $\Delta_{[2]} = \Delta_{[K]}$, the conditional means are point masses at $\mu_H = \mu_{[1]} - (1-2/K)\Delta$ and $\mu_L = \mu_{[1]} - \Delta$.




\end{document}



% \section*{Appendix A}

