\section{Related Works}
For the fixed budget setting, Audibert et al., "UCB-E and Successive Rejects" proposed the \texttt{UCB-E} and \texttt{Successive Rejects} (SR) algorithms and proved their optimality up to logarithmic factors. Specifically, the upper bound on the probability of error for their proposed algorithms is
%\begin{align}
    $p(T) \leq \frac{K (K-1)}{2} \exp\left(- \frac{T - K}{\log_2 (K) H_2}\right),$
%\end{align}
where $H_2$ is called the hardness of the problem that depends on the specific instance of $\nu \in \mathcal{N}$ (to be discussed in detail soon). In a remarkable result, Carpentier & Locatelli, "Lower bounds for the fixed-budget setting" proved the following lower bound for the probability of error in the fixed-budget setting:
%\begin{align}
       $p(T) \gtrsim \exp\left(-\frac{T}{\log(K) H}\right),$
%\end{align}
where $H$ is another variant of the hardness parameter. This disproved a long-standing assumptionâ€”that there must exist an algorithm for this problem whose probability of error is upper bound by $\exp(-T/H)$. This established a key difference between the fixed-confidence and the fixed-budget settings. Following this line of work, Karnin, "Sequential Halving" proposed the \texttt{SEQUENTIAL HALVING} ({SH}) algorithm and proved it to be almost optimal for BAI problems. This is achieved by eliminating half of the surviving arms with the worst estimates in each round. \ac{BAI} problem has also been analyzed using other variants of the \ac{UCB} algorithm (e.g., LUCB of Auer et al., "Improved Regret Bounds for Linear Contextual Bandits") which are not based on eliminations. Most of the experimental results of these algorithms are present for bounded distributions, that are in fact particular examples of distributions with sub-Gaussian tails. Since then, the fixed budget pure exploration problem has been studied in a wide variety of contexts, e.g., for linear bandits Dani et al., "The Price of Bandit Information" minimax optimality Bubeck & Cesa-Bianchi, "Regret Analysis for the Linear Predictor in Linear Regression" spectral bandits Abbasi-Yadkori et al., "Regal: A Regret Minimization Framework for Reinforcement Learning" risk-averse bandits Garivier et al., "Optimal Best Arm Identification with Fixed Budget" and bandits with mis-specified linear models Filippi & Forte, "Risk-Averse Multi-Armed Bandit Problem". More recently, large deviation perspective was studied in Bubeck, "Extreme Value Theory for Sequential Decision Making" cost-aware BAI in Abbasi-Yadkori et al., "Cost-Effective Allocation of Multiple Arms" and arm erasures in Korda & Munos, "Online Linear Optimization under Multi-Bandit Feedback". Thompson sampling has also been employed to form the {\it best challenger} rule to improve the efficiency of \ac{BAI} Kaufmann et al., "Thompson Sampling for  Bandits Minimizing Competing with the Best Challenger" . Interesting new directions include combining the regret setting with the BAI setting, e.g., see Abbasi-Yadkori et al., "Multi-Armed Bandit Problems in the Regret Setting". Finally, assuming a distribution of $\nu$ over $\mathcal{N}$, researchers have studied rate-optimal Bayesian regret in \ac{BAI} Kaufmann & Korda, "Thompson Sampling for  Bandits Minimizing Competing with the Best Challenger" .