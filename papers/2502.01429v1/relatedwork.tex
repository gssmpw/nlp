\section{Related Works}
For the fixed budget setting, Audibert et al.~\cite{audibert2010best} proposed the \texttt{UCB-E} and \texttt{Successive Rejects} (SR) algorithms and proved their optimality up to logarithmic factors. Specifically, the upper bound on the probability of error for their proposed algorithms is
%\begin{align}
    $p(T) \leq \frac{K (K-1)}{2} \exp\left(- \frac{T - K}{\log_2 (K) H_2}\right),$
%\end{align}
where $H_2$ is called the hardness of the problem that depends on the specific instance of $\nu \in \mathcal{N}$ (to be discussed in detail soon). In a remarkable result, Carpentier \& Locatelli~\cite{carpentier2016tight} proved the following lower bound for the probability of error in the fixed-budget setting:
%\begin{align}
       $p(T) \gtrsim \exp\left(-\frac{T}{\log(K) H}\right),$
%\end{align}
where $H$ is another variant of the hardness parameter. This disproved a long-standing assumptionâ€”that there must exist an algorithm for this problem whose probability of error is upper bound by $\exp(-T/H)$. This established a key difference between the fixed-confidence and the fixed-budget settings. Following this line of work, Karnin~\cite{karnin2013almost} proposed the \texttt{SEQUENTIAL HALVING} ({SH}) algorithm and proved it to be almost optimal for BAI problems. This is achieved by eliminating half of the surviving arms with the worst estimates in each round. \ac{BAI} problem has also been analyzed using other variants of the \ac{UCB} algorithm (e.g., LUCB of \cite{LUCB}) which are not based on eliminations. Most of the experimental results of these algorithms are present for bounded distributions, that are in fact particular examples of distributions with sub-Gaussian tails. Since then, the fixed budget pure exploration problem has been studied in a wide variety of contexts, e.g., for linear bandits~\cite{jedra2020optimal}, minimax optimality~\cite{yang2022minimax}, spectral bandits~\cite{kocak2020best}, risk-averse bandits~\cite{kagrecha2022statistically}, and bandits with mis-specified linear models~\cite{alieva2021robust}. More recently, large deviation perspective was studied in~\cite{wang2024best}, cost-aware BAI in \cite{qin2025cost} and arm erasures in \cite{reddy2024best}. Thompson sampling has also been employed to form the {\it best challenger} rule to improve the efficiency of \ac{BAI}~\cite{lee2024thompson}. Interesting new directions include combining the regret setting with the BAI setting, e.g., see~\cite{zhang2024fast, qin2024optimizing}. Finally, assuming a distribution of $\nu$ over $\mathcal{N}$, researchers have studied rate-optimal Bayesian regret in \ac{BAI}~\cite{komiyama2024rate}.