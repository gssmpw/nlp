\section{Experiments and Evaluation}
\subsection{Datasets}
\subsubsection{Volumetric Datasets}
For quality and performance evaluation, we select 4 large-scale volume datasets with distinct spatial features collected from diverse domains. The Chameleon dataset is CT scan of a chameleon. The Beechnut dataset is a microCT scan of a dried beechnut. The Rayleigh-Taylor dataset is a time step of a density field in a simulation of the mixing transition in Rayleigh-Taylor instability. The Flame dataset is a simulated combustion 3D scalar field. \autoref{tab:datasets} shows the detailed information of the testing datasets. The spacings of all datasets are normalized within the spatial range [-1, 1] with values normalized within the range [0, 1].
\subsubsection{Training Data}
The training dataset used to train the networks is the view and fully rendered image (FR-Image) pairs. First, we randomly select 2000 views around the volume of each volumetric dataset. Second, a FR-Image is rendered using specific volume rendering algorithm with lighting effect. In this work we select commonly used ray casting DVR as the renderer. Third, filter the FR-Image with the sampling pattern to generate the partially rendered image (PR-Image) where only pixels in the sampling pattern are kept. Fourth, apply compaction on PR-Image to generate C-Image. For training the IML Net standalone, the input is C-Image while the output is the predicted C-Image from decoder. For training the IML Net end-to-end with the RecNN, the input is the C-Image while the output is the predicted FR-image from RecNN. For training the IMS Net, the input is the view parameters while the output is the learned IM derived from the trained IML Net. The training and validation partition ratio is 9:1. Each volumetric dataset also generates a testing dataset consisting of 120 views forming an exploratory trajectory to simulate real user exploration. The FR-Image is rendered with a resolution of $512\times512$ using the Visualization Toolkit (VTK)~\cite{865875} with a customized time stamp to measure the rendering time for each pixel. The interpolation is set to trilinear. The sample distance is 0.02. The CPU thread is set as a single thread to faithfully measure the time duration when rendering individual pixel in sequence.

\begin{figure}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/loss_diff_all_short_small.png}
    \caption{IM prediction quality comparison using different configurations of loss functions on Chameleon dataset.}
    \label{fig:diff_loss_image}
\end{figure}

\begin{table}[t]
  \caption{Volumetric datasets used in the experiments.}
  \label{tab:datasets}
  \scriptsize%
	\centering%
  \begin{adjustbox}{width=0.45\textwidth}
      % \begin{tabu}{ m{1cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{1cm} m{1cm} m{1cm} m{1cm} m{2cm}}
      % \begin{tabu}{ c | c c c c c c c c c c c }}
      % \begin{tabu}{ c*{9}{c} | {2}{c} }
      \begin{tabu}{ c c c c }
          \toprule
          Dataset & Resolution & Size & Data Type \\
          \midrule
          Chameleon       & $1024^2\times1088$ & 2.1 GB & uint16 \\
          \midrule
          Beechnut          & $1024^2\times1546$           & 3.0 GB   & uint16 \\
          \midrule
          Rayleigh-Taylor & $1024^3$           & 4.0 GB    & float32 \\
          \midrule
          Flame           & $1408\times1080\times1100$           & 6.23 GB & float32 \\
          \bottomrule
      \end{tabu}
  \end{adjustbox}
\end{table}



\subsection{Experimental Setup}
The experiments are designed to investigate the interactive volume visualization using RecNN. We are going to evaluate both the quality and performance using traditional volume visualization using RecNN and the proposed image synthesis network, IML/S Net + RecNN. The input is a sequence of unseen views and the output are the FR-images. We use PSNR as the main metric to evaluate rendering quality. The computing platform is a desktop featuring an Intel(R) Core(TM) i7-7700K CPU with 8 threads running at 4.20GHz, paired with 16GB of DDR4 DRAM clocked at 3200MHz, and operating on Ubuntu 20.04.4 LTS. 


\subsection{RecNN Selection and Configuration}
We select two state-of-the-art RecNNs used for interactive volume visualization, EnhanceNet and FoVolNet, to showcase how our method can help to further improve their rendering latencies. For the super-resolution RecNN using EnhanceNet, we also use $4\times$ super-resolution setting, the same as the experiment in its paper. Since our evaluation only focuses on one-shot image generation, for fair comparison, we remove the recurrent connections of the FoVolNet and only keep its W-Net structure for static image prediction. Fixed foveated rendering, where the high-resolution region is static and fixed in the center of the screen, is used in the experiment rather than dynamic foveated rendering. The foveal pattern $FP$ is generated by the following methods:
\begin{equation}
    FP_{ij}= 
\begin{cases}
    1,& \text{if } P_{ij} > E_{ij}\\
    0,              & \text{otherwise}
\end{cases}
\end{equation}
where $P_{ij}$ random pattern generated by blue noise which doesn't generate unevenly distributed samples from low-frequency energy spikes in the spatial domain\cite{9903564}. The $E_{ij}$ is a modulation surface to control the size of the foveated area through $\sigma$:
\begin{equation}
E_{ij} = e^{-0.5(f_{x}^{2}+f_{y}^{2})\sigma}
\end{equation}
where $f_x$ and $f_y$ are the center coordinate of the foveal pattern.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includegraphics[trim=10 0 20 0,clip,width=\linewidth]{figures/generator_psnr.png}
        \caption{IMS Net}
        \label{fig:iml_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.485\linewidth}
        \centering 
        \includegraphics[trim=10 0 20 0,clip,width=\linewidth]{figures/end2end_psnr.png}
        \caption{IML/S Net + EnhaneNet}
        \label{fig:ims_loss}
    \end{subfigure}
    \caption{Qualitative comparison of IM prediction and reconstruction using different configurations of loss functions on Chameleon dataset.}
    \label{fig:diff_loss_curve}
\end{figure}

\subsection{Training}
The proposed neural networks are trained using the PyTorch software stack to accelerate the training and inferencing performance on a single NVIDIA RTX A6000 GPU. The Adaptive Moment Estimation (Adam) optimizer is used with an initial learning rate of 0.001 for training both IML and IMS Nets. A validation dataset is selected to detect the overfitting on the training dataset and terminate the network parameters from updating through the early stop mechanism. The patience of the early stop is set as 20 epochs for IML Net and 1000 for IMS Net. The IML Net can be trained standalone or trained end-to-end with RecNN. 

\subsection{Ablation Study}
In this section, we investigate how the configuration of the networks and hyperparameters would influence the model's performance.

\subsubsection{Network Configuration}
\hspace{\parindent}\textbf{Encoder and Decoder of IML:} The encoder of the IML Net is critical to learning an accurate IM for it provides a comprehensive guess of the IM. Its U-Net architecture is capable of distinguishing fine-grained details through its skip connections and recommending candidates of important pixels. Compared with the encoder that serves as an information filter, the decoder serves as an information reconstruction that reconstructs the PR-C-Image to a full C-Image. We investigate how the network configuration of the IML encoder and decoder affects the performance of the proposed rendering pipeline. Since the decoder of the IML Net mirrors the encoder's configuration, we modify both together by adjusting the depth of their U-Nets. We fixed the architecture of IMS Net and only adjusted the IML Net. As the depth of the encoder and decoder increases, its reconstruction quality increases sublinearly (as shown in \cref{fig:depth_iml_psnr_sr} and \cref{fig:depth_iml_psnr_fr}) while the number of parameters of the network increases superlinearly (as shown in \cref{fig:depth_iml_num_para}). While a deeper IML Net can provide better reconstruction quality, its larger size slows down inference, leading to higher rendering latency. We select the depth of both the encoder and decoder as 5 for its balanced performance between rendering quality and inferencing latency.


\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/major_revision/encoder_deep_num_parameters.png}
        \caption{IML network parameter size}
        \label{fig:depth_iml_num_para}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/major_revision/psnr_encoder_deep_super.png}
        \caption{Average PSNR (S-R))}
        \label{fig:depth_iml_psnr_sr}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/major_revision/psnr_encoder_deep_foveated.png}
        \caption{Average PSNR (F-R))}
        \label{fig:depth_iml_psnr_fr}
    \end{subfigure}
    \caption{IML Networks with different depths of the U-Net architecture. Evaluation performed on the Chameleon dataset with a visualization image resolution of $512\times512$. (a) shows the total number of parameters of IML Net. (b) and (c) show the reconstruction quality of super-resolution (S-R) and foveated rendering (F-R) while increasing the percentage of C-Image rendered. }
    \label{fig:depth_iml}
    
\end{figure}

\textbf{Generator of IMS:} The generator of IMS Net takes a novel view and generates the corresponding IM. The accuracy of the generated IM determines the quality of the input to the downstream IML decoder for reconstruction. We evaluate the reconstruction capability of IMS Net by using different numbers of transposed convolutional layers. We fixed the architecture of IML Net and only adjusted the IMS Net. As the number of transposed convolutional layers increases, similarly to adjusting the depth of IML Net, its reconstruction quality increases sublinearly (as shown in \cref{fig:depth_ims_psnr_sr} and \cref{fig:depth_ims_psnr_fr}) while the number of parameters of the network increases superlinearly (as shown in \cref{fig:depth_ims_num_para}). We also observed that adjusting the number of transposed convolutional layers in IMS Net has a greater impact on the reconstruction quality than the depth of the U-Net in IML Net. We select the layer size as 8 for IMS Net to balance rendering quality and inferencing latency.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/major_revision/generator_deep_num_parameters.png}
        \caption{IMS network parameter size}
        \label{fig:depth_ims_num_para}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/major_revision/psnr_generator_deep_super.png}
        \caption{Average PSNR (S-R))}
        \label{fig:depth_ims_psnr_sr}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/major_revision/psnr_generator_deep_foveated.png}
        \caption{Average PSNR (F-R))}
        \label{fig:depth_ims_psnr_fr}
    \end{subfigure}
    \caption{IMS network with different numbers of transpose convolutional layers. Evaluation performed on Chameleon dataset with a visualization image resolution of $512\times512$. (a) shows the total number of parameters of the generator. (b) and (c) show the reconstruction quality of super-resolution (S-R) and foveated rendering (F-R) while increasing the percentage of C-Image rendered.}
    \label{fig:depth_ims}
\end{figure}

\subsubsection{Hyperparameter Tuning}
\hspace{\parindent}\textbf{Mean Value for Rejection Sampling: }
The hyperparameter of the mean value plays an important role in determining the percentage of pixels in C-Image as important pixels. The IML Net trained on a specific mean value, after the rejection sampling, will converge to have the percentage of important pixels of C-Image approximately equal to the $mean\times 10$. For a given reconstruction quality tolerance $\epsilon$, we can find the optimal rendering percentage (ORP) through a binary search on the mean value within range $[0.01, 1]$. The $\epsilon$ is set as 1 dB in our experiment to search for the optimal mean value and ORP. Using the ORP for IML/S Net will give the RecNN a free improvement on the rendering latency without noticeable quality degradation.

% \begin{figure*}[t]
%     \centering 
%     \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/quality_v_all_small.png}
%     \caption{Visual comparison of reconstruction quality. Left column are results using  EnhanceNet and IML/S Net + EnhanceNet. Left column are results using  FoVolNet and IML/S Net + FoVolNet.}
%     \label{fig:super_images_comp}
% \end{figure*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.49\linewidth}
        \centering
        \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/quality_v_small.png}
        \caption{Visual comparison of reconstruction quality using EnhanceNet and IML/S Net + EnhanceNet}
        \label{fig:super_images_comp}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\linewidth}
        \centering 
        \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/quality_v_foveated_small.png}
        \caption{Visual comparison of reconstruction quality using FoVolNet and IML/S Net + FoVolNet}
        \label{fig:foveat_images_comp}
    \end{subfigure}
    \caption{Visual comparison of reconstruction quality.}
    \label{fig:images_comp}
\end{figure*}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/end2end_vs_baseline_psnr.png}
        \caption{PSNR of each view}
        \label{fig:psnr_20p_super}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/end2end_vs_baseline_psnr_wrt2mean.png}
        \caption{Average PSNR}
        \label{fig:psnr_all_super}
    \end{subfigure}
    \caption{Quantitative comparison of reconstruction quality using EnhanceNet and IML/S Net + EnhanceNet on Chameleon dataset. (a) demonstrates the reconstruction quality from each view in the testing dataset using $20\%$ of C-Image pixels rendered. (b) shows the trend of average reconstruction quality while increasing the percentage of C-Image rendered.}
    \label{fig:psnr_qutitative_super}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/end2end_vs_baseline_psnr_foveated.png}
        \caption{PSNR of each view}
        \label{fig:psnr_40p_foveated}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/end2end_vs_baseline_psnr_wrt2mean_foveated.png}
        \caption{Average PSNR}
        \label{fig:psnr_all_foveated}
    \end{subfigure}
    \caption{Quantitative comparison of reconstruction quality between FoVolNet and IML/S Net + FoVolNet on Chameleon dataset. (a) demonstrates the reconstruction quality from each view in the testing dataset using $40\%$ of C-Image pixels rendered. (b) shows the trend of average reconstruction quality while increasing the percentage of C-Image rendered.}
    \label{fig:psnr_qutitative_foveated}
\end{figure}

\textbf{Loss functions: }\label{sec:loss}
For training the IMS Net for the best results, we tried multiple image-based loss functions including Binary Cross Entropy (BCE) loss, Mean Squared Error (MSE) loss, Perceptual Loss (PER) loss, and Adversarial (ADV) loss. We found combining PER and ADV loss gives the best result as shown in \cref{fig:diff_loss_image}. BCE and MSE are not capable of learning an accurate IM. The PER loss is much better in accurately predicting the correct shape of the IM. However, PER loss struggles to predict the correct detail of the IM. Using a loss that combines both the PER (weight = 1) and ADV (weight = 0.01) will give the closest result to the ground truth. The quantitative evaluation on the accuracy of the IM prediction from IMS Net and reconstruction from IML/S Net + EnhanceNet using various loss configurations is listed in \cref{fig:diff_loss_curve}.



% \begin{figure}[t]
%     \centering 
%     \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/quality_v_small.png}
%     \caption{Visual comparison of reconstruction quality using EnhanceNet and IML/S Net + EnhanceNet.}
%     \label{fig:super_images_comp}
% \end{figure}






\subsection{Results}\label{sec:results}
\subsubsection{Quality Evaluation}
We compare the reconstruction quality between using traditional RecNN alone and using our proposed IML/S Net + RecNN. 

\textbf{Super-resolution:}
\cref{fig:super_images_comp} shows the visual comparison between the EnhanceNet and our IML/S Net + EnhanceNet of all 4 volumetric datasets. For each dataset, integrating our networks with EnhanceNet delivers comparable rendering quality to using EnhanceNet alone, but with significantly reduced rendering latency (2 to 4 times faster). In the PR-C-Images, the importance mask is successfully learned to extract a subset of the C-Image as important pixels. From the IM, we can observe that image areas with more dynamic content, like the edges of the object and the boundaries between distinct regions, were assigned more important pixels by our IML Net. This observation demonstrates that our network is capable of distinguishing complex regions from simpler ones and automatically sampling more important pixels in the complex region to achieve optimal reconstruction quality. The zoomed-in view with a smaller background area requires a higher percentage of important pixels to be rendered in order to achieve a quality similar to the zoomed-out view. Complex dataset like Rayleigh-Taylor also needs more important pixels comparing to simple datasets like the Flame. \cref{fig:psnr_qutitative_super} shows the quantitative PSNR metric when rendering all the views in the Chameleon testing dataset. \cref{fig:psnr_20p_super} shows our method gives similar PSNR when only using 20\% of the C-Image pixels rendered. We observe from \cref{fig:psnr_all_super} that as the percentage of important pixels increases, our method infinitely approaches the quality of the standalone RecNN results, and this approximation converges quickly as the percentage of important pixels in the C-Image increases.



\textbf{Foveated Rendering:} 
\cref{fig:foveat_images_comp} shows the visual comparison between the FoVolNet and our IML/S Net + FoVolNet of all 4 volumetric datasets. We can observe similar results from the super-resolution using RecNN. Although the C-Images compacted from foveal pattern does not preserve human-readable features like the ones compacted from downsampling pattern, the IML Net can still find the important pixel from it and the IMS Net can still predict the correct IM successfully. \cref{fig:psnr_40p_foveated} shows our method gives similar PSNR when only using 40\% of the C-Image pixels rendered. A similar convergence in quality can be observed as the percentage of important pixels in the C-Image increases from \cref{fig:psnr_all_foveated} . We can also observe that the convergence in foveated rendering of the same Chameleon testing dataset is slower compared to super-resolution rendering. This is due to the foveal pattern, which limits the sampling to the focal area, restricting the possible locations the IML Net can choose from. 

% \begin{figure}[t]
%     \centering 
%     \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/quality_v_foveated_small.png}
%     \caption{Visual comparison of reconstruction quality using FoVolNet and IML/S Net + FoVolNet.}
%     \label{fig:foveat_images_comp}
% \end{figure}




\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/timing.png}
        \caption{Rendering latency for each view}
        \label{fig:time_super_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.485\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/timing_wrt2mean.png}
        \caption{Average rendering latency}
        \label{fig:time_super_b}
    \end{subfigure}
    \caption{Quantitative comparison of rendering latency between EnhanceNet with and without IML/S Net + EnhanceNet on Chameleon dataset. (a) demonstrates the rendering latency from each view in the testing dataset using $20\%$ of C-Image pixels rendered. (b) shows the trend of average rendering latency while increasing the percentage of C-Image rendered.}
    \label{fig:time_super}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.485\linewidth}
        \centering
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/timing_foveated.png}
        \caption{Rendering latency for each view}
        \label{fig:time_foveated_a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.485\linewidth}
        \centering 
        \includegraphics[trim=10 0 10 0,clip,width=\linewidth]{figures/timing_foveated_wrt2mean.png}
        \caption{Average rendering latency}
        \label{fig:time_foveated_b}
    \end{subfigure}
    \caption{Quantitative comparison of rendering latency between FoVolNet with and without IML/S Net + FoVolNet on Chameleon dataset. (a) demonstrates the rendering latency from each view in the testing dataset using $40\%$ of C-Image pixels rendered. (b) shows the trend of average rendering latency while increasing the percentage of C-Image rendered.}
    \label{fig:time_foveated}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/orp_super_bar.png}
        \caption{Super-resolution RecNN}
        \label{fig:orp_a}
    \end{subfigure}
    % \hfill
    \vskip\baselineskip
    \begin{subfigure}[b]{\linewidth}
        \centering 
        \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/orp_foveated_bar.png}
        \caption{Foveated rendering RecNN}
        \label{fig:orp_b}
    \end{subfigure}
    \caption{Average Rendering latency using ORP. The shaded regions are the time used to inference the network. The solid regions are the time used by the rendering algorithm to compute the pixel values in C-Image.}
    \label{fig:orp}
\end{figure}

\begin{table}[t]
  \caption{Detailed ORP for free rendering latency improvement.}
  \label{tab:orp_table}
  \scriptsize%
	\centering%
  \begin{adjustbox}{width=0.5\textwidth}
      % \begin{tabu}{ m{1cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{1cm} m{1cm} m{1cm} m{1cm} m{2cm}}
      % \begin{tabu}{ c | c c c c c c c c c c c }}
      % \begin{tabu}{ c*{9}{c} | {2}{c} }
      \begin{tabu}{ c | c c | c c | c c  }
      \toprule
      Dataset & \multicolumn{2}{|c|}{ORP} & \multicolumn{2}{|c|}{Avg Rendering Latency (ms)}  & \multicolumn{2}{|c}{Avg Speed-up}  \\
        &
      \multicolumn{2}{|c|}{Sup-Res $\downarrow$  \ \ Foveated $\downarrow$} &
      \multicolumn{2}{|c|}{Sup-Res $\downarrow$  \ \ \ \ \ \ \ \ Foveated $\downarrow$} &
      \multicolumn{2}{|c}{Sup-Res $\uparrow$  \ \ Foveated $\uparrow$}\\
      \midrule
      Chameleon & 20\%  & 40\% & 89.4  & 106.1 & 2.07$\times$ & 1.77$\times$\\
      \midrule
      Beechnut & 18\% & 17\% & 86.0 & 79.1 & 2.02$\times$ & 2.24$\times$\\
      \midrule
      Rayleigh-Taylor & 40\% & 42\% & 142.4 & 151.1 & 1.75$\times$ & 1.49$\times$\\
      \midrule
      Flame & 11\% & 10\% & 56.9 & 54.1 & 2.98$\times$ & 3.2$\times$\\
      \bottomrule
      \end{tabu}
 \end{adjustbox}
\end{table}


\subsubsection{Performance Evaluation}
We compare the overall rendering latency between traditional RecNN rendering 100\% of the C-Image and our proposed IML/S Net together with the RecNN rendering a portion of the C-Image. In this section, we present a detailed analysis of rendering performance using Chameleon dataset as an example. A joint evaluation of both rendering quality and performance across all testing datasets will be provided in the next section.

\textbf{Super-resolution:}
As we discussed before the overall rendering latency is the sum of the time to partially render the input image to RecNN and the RecNN inferencing to reconstruct the FR-Image. \cref{fig:time_super} presents the quantitative measurement of time usage at each stage. We observe that the inferencing time for both methods is minimal, though the overhead introduced by the IML/S Net slightly increases the total inferencing time compared to using the standalone RecNN. Using the IML/S Net significantly reduces the time required to generate the partially rendered image, as fewer pixels undergo the costly rendering computations. \cref{fig:time_super_b} demonstrates the growth of average input latency is close to linear with respect to the percentage of important pixels in C-Image.

\textbf{Foveated Rendering:} 
\cref{fig:time_foveated} presents the quantitative measurement of time usage at each stage for foveated rendering. Our IML/S Net demonstrates similar performance in foveated rendering RecNN as it does in super-resolution RecNN. Since FoVolNet is more complex than EnhanceNet, the inference time for both methods is higher compared to their counterparts in the super-resolution framework. \cref{fig:time_foveated_b} also demonstrates a linear relationship between the percentage of important pixels in C-Image and the average input latency. Considering both quality and performance results on the Chameleon dataset, we can conclude that, as the percentage of important pixels increases, the quality of reconstruction is more sensitive in super-resolution compared to foveated rendering. This suggests that, for the same reconstruction quality, super-resolution benefits more from our IML/S Net in terms of reducing rendering latency compared to foveated rendering. This observation is related to the nature of the distribution of the Chameleon dataset in visualization image space. The contents of interest across views are more evenly distributed across the whole image domain rather than only distributed in the center. As a result, a super-resolution pattern covers such distribution better than foveated rendering.



% \begin{figure}[t]
%     \centering 
%     \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/pixel_ratio.png}
%     \caption{Micro-model storage layout. Knot and Control Point determine the shape and properties of the spline.}
%     \label{fig:micro-model}
% \end{figure}

% \begin{figure}[t]
%     \centering 
%     \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/timing.png}
%     \caption{Micro-model storage layout. Knot and Control Point determine the shape and properties of the spline.}
%     \label{fig:micro-model}
% \end{figure}



\subsubsection{Free Rendering Latency Improvement}
In this section, we further investigate the data-dependent characteristics of various testing datasets on rendering quality and performance using our method. We will also qualitatively evaluate how much rendering latency improvement can be achieved by our method without sacrificing perceivable rendering quality. We find the optimal rendering percentage (ORP) for our IML Net to make IML/S Net reconstruct perspective rendering within $1 dB$ PSNR difference ($\epsilon$) to the rendering from the standalone RecNN. \cref{fig:orp} shows the average rendering latency using ORP for both EnhanceNet and FoVolNet. On average, our method can improve the rendering latency for each volumetric dataset with similar rendering quality. The detailed results are listed in \cref{tab:orp_table}. We can see that for different dataset, ORP varies to achieve close reconstruction quality to the original RecNNs. Rayleigh-Taylor requires the highest ORP to maintain quality due to its dynamic nature across the whole 3D space, which leads to the largest content area in the final rendering compared to other datasets. Flame is simpler and more compact in 3D space than others, resulting in the lowest ORP. Compared to super-resolution, foveated rendering favors datasets where the rendered object is primarily centered in the image, such as Beechnut and Flame, leading to a lower ORP. 






\subsubsection{Feasibility Study}\label{feasibility}
In this section, we assess the practicality and viability of the proposed method. To take advantage of the rendering latency improvement provided by the proposed method, the overhead is the extra training step for the IML and IMS networks. The IMS Net is a simple network and its training dataset (view and IM) is also small, so its training process is relatively quick. Due to the complex structure of the RecNNs and their large training data (full-res images), a significant amount of time is dedicated to the end-to-end training of the IML Net and the downstream RecNN. To enhance the feasibility of our method, the proposed IML Net can be trained independently without connecting to the complicated downstream RecNN. Compared to RecNN, the network structure of IML Net is simpler and its training data is smaller (low-res C-Image). The loss function for training this standalone IML Net is solely based on the compaction image loss. Once the IML Net is trained, its decoder can directly connect to an off-the-shelf pre-trained RecNN to improve the rendering latency. In this way, hours of training time can be saved. The total epochs and approximate training times for training the IML and IMS Net are listed in \cref{tab:training_time}. We can observe that training IML standalone will save a significant amount of training time compared to training IML end-to-end with RecNN. Due to the absence of joint optimization across the entire pipeline, using the independently trained IML Net will result in lower reconstruction quality compared to end-to-end training. However, as shown in \cref{fig:end2end_vs_standalone}, the quality degradation is still acceptable especially when the IML Net is trained using a larger percentage of important pixels on C-Image.

\begin{table}[t]
  \caption{Training time and number of epochs trained for training the IML and IMS Net. End-to-end trains the IML Net together with the downstream RecNN. Standalone only trains the IML Net.}
  \label{tab:training_time}
  \scriptsize%
	\centering%
  \begin{adjustbox}{width=0.5\textwidth}
      % \begin{tabu}{ m{1cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{0.5cm} m{1cm} m{1cm} m{1cm} m{1cm} m{2cm}}
      % \begin{tabu}{ c | c c c c c c c c c c c }}
      % \begin{tabu}{ c*{9}{c} | {2}{c} }
      \begin{tabu}{ c | c c c c | c c }
      \toprule
      Dataset & \multicolumn{4}{|c|}{IML Net (epoches/hours)} & \multicolumn{2}{|c}{IMS Net (epoches/hours)} \\
        &
      \multicolumn{4}{|c|}{EnhanceNet   \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  FoVolNet } &
      \multicolumn{2}{|c}{EnhanceNet   \ \ FoVolNet } \\
        &
      \multicolumn{2}{|c}{End-to-end   \ \ Standalone } &
      \multicolumn{2}{c|}{End-to-end   \ \ Standalone } &
      \multicolumn{2}{|c}{    \ \   } \\
      \midrule
      Chameleon & 83/5 &  92/2.0 & 107/7 & 117/2.5 & 1744/3  &  1687/3 \\
      \midrule
      Beechnut & 95/6 &  102/2.0& 43/3 & 73/1.5& 1632/3 & 1722/3 \\
      \midrule
      Rayleigh-Taylor & 136/8 & 141/2.9 & 161/10 & 180/3.5& 2443/3 & 2901/4 \\
      \midrule
      Flame & 152/9 & 120/2.5 & 255/15 & 163/3.0& 1711/3 & 1804/3 \\
      \bottomrule
      \end{tabu}
 \end{adjustbox}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_super_1.png}
        \caption{Chameleon (S-R)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}  
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_super_2.png}
        \caption{Beechnut (S-R))}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}   
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_super_3.png}
        \caption{Rayleigh-Taylor (S-R))}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}   
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_super_4.png}
        \caption{Flame (S-R))}
    \end{subfigure}
    \vskip\baselineskip
    % \begin{subfigure}[b]{0.485\linewidth}  
    \begin{subfigure}[b]{0.45\linewidth}   
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_foveated_1.png}
        \caption{Chameleon (F-R))}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}   
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_foveated_2.png}
        \caption{Beechnut (F-R))}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.45\linewidth}   
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_foveated_3.png}
        \caption{Rayleigh-Taylor (F-R))}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}   
        \centering 
        \includegraphics[trim=10 0 11 0,clip,width=\linewidth]{figures/end2end_vs_standalone_foveated_4.png}
        \caption{Flame (F-R))}
    \end{subfigure}
    \caption{Reconstruction quality comparison using RecNN, end-to-end trained IML Net + RecNN, and standalone trained IML Net for both super-resolution (S-R) RecNN and foveated Rendering (F-R) RecNN.}
    \label{fig:end2end_vs_standalone}
\end{figure}





% \subsubsection{Learned region analysis}
% network priotize high frequency info area and put less on low frequency area. show the flame imporantance map.


% \subsubsection{Scaling: w.r.t to image resolution and others}
% \subsubsection{Scaling: w.r.t to super-res ratio (4-1, 9-1, 16-1) and foveated radius}
% \subsubsection{Model Precision}
