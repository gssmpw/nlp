\section{Related Work} 
\label{related work}
\subsection{Implicit Neural Representation}
Implicit Neural Representation (INR) refers to a method of representing data, such as images, 3D shapes, or other types of continuous signals, using a neural network. For handling large-scale volume visualization, compression methods utilizing implicit neural representation \cite{lu2021compressive, tang2020deep} have been proposed to reduce network size and optimize I/O-intensive operations. For improving the rendering latency of the visualization system, Wu et al.~\cite{10175377} use hash encoding-based INR to speed up the interactive volume visualization with high reconstruction quality. Yariv et al.~\cite{NEURIPS2021_25e2a30f} improve geometry representation and reconstruction in neural volume rendering by modeling the volume density as an implicit function of the geometry. Weiss et al.~\cite{weiss2022fast} introduce fV-SRN as a novel extension of SRN (Scene Representation Networks) to achieve significantly accelerated reconstruction performance of volume rendering. Despite the use of powerful GPUs, the training time for INR remains significantly long when processing complex, large-scale scientific datasets~\cite{TANG2024103874, 10175377}. Both the proposed IML and IMS Nets were trained as INRs with learned properties of the dataset, view parameters, and the downstream RecNN.

\subsection{Reconstruction Neural Network}

A Reconstruction Neural Network (RecNN) in visualization refers to a type of neural network used to reconstruct or generate detailed representations of data from incomplete, noisy, or compressed input~\cite{10.1007/s10462-022-10147-y, cao2021video}. The goal of RecNN is to recover or restore a high-quality, detailed version of an object or dataset from a simplified or partial form.
The most commonly used type of RecNN is super-resolution neural networks that reconstruct high-res data from low-res data. The sampling pattern of super-resolution RecNN is a downsampling pattern. The EnhanceNet, proposed by Sajjadi et al.~\cite{8237743}, demonstrates high reconstruction quality in generating volume visualization images~\cite{9264699}. Weiss et al.~\cite{8918030} propose a volumetric isosurface rendering with deep learning-based super-resolution. Tang et al.~\cite{TANG2024103874} propose STSR-INR which extends the super-resolution to both spatial and temporal domains. Another important RecNN is foveated rendering RecNN which does reconstruction from partial rendering near the region where the user is focusing the gaze. Together with the techniques in eye-tracking-related research~\cite{sundstedt2022systematic, 8900970}, foveated rendering provides a solution to balance the user experience and computational cost. The sampling pattern of foveated rendering RecNN is a foveal pattern. Kaplanyan et al. propose DeepFovea~\cite{10.1145/3355089.3356557}, the first adversarial neural network (GAN)-based foveated reconstruction method to speed up the rendering frame rate for gaming. Then Bauer et al. propose FoVolNet~\cite{9903564}  to improve the foveated rendering in the volume visualization domain and achieve state-of-the-art rendering latency. In this work, we select two RecNNs, EnhanceNet and FoVolNet, as representations of super-resolution RecNN and foveated rendering RecNN to demonstrate how much rendering latency can be optimized using our method.

\subsection{Image Synthesis}

Deep learning-based image synthesis in volume visualization refers to the use of deep learning techniques to generate or enhance visual representations of 3D volumetric data, such as medical scans (MRI, CT), scientific simulations, or geological models~\cite{lochmann2016real}. Volumetric data can be very large, making real-time rendering computationally expensive. Generative models can synthesize results efficiently by learning the underlying structure of the data, reducing the need for extensive data processing and enabling faster generation of visual results. Berger et al.~\cite{8316963} use a GAN framework to directly generate the volume visualization image from view parameters and transfer functions. He et al.~\cite{8805426} provide a regressor to directly generate visualization images from simulation and visualization parameters for fast parameter space exploration. Jun et al. propose VCNet, a new deep-learning approach for volume completion by synthesizing missing subvolumes. Our IMS Net also utilizes image synthesis techniques to generate an importance mask directly from view parameters. Although training generative models, especially for large-scale or high-dimensional data, requires significant computational resources and large training datasets, the importance mask learned by the IML Net is a simple 2D binary array that can be easily learned by IMS Net. 

