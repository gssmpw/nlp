\section{Methods}
\subsection{Overview}
For a general volume visualization system, the rendering latency (RL) is the time difference between the moment the user makes a data- or view-dependent operation and the time when the visualization image is done rendering. For interactive volume visualization using RecNN, as shown in \cref{fig:pipeline}, the rendering latency ($T_{RL}$) is the sum of two components which are the time of selective rendering (SR) for generating the partially rendered image (PR-Image), $T_{SR}$, and the inferencing time of the RecNN to generate a fully rendered image (FR-Image) from PR-Image, $T_{RecNN}$:

\begin{equation}
T_{RL}=T_{SR} + T_{RecNN}
\end{equation}

\begin{figure}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/pipeline.png}
    \caption{Typical pipeline of interactive volume visualization using RecNN.}
    \label{fig:pipeline}
\end{figure}

$T_{RecNN}$ is determined by the specific RecNN architecture used in the reconstruction pipeline. We define the pixels covered by the sampling pattern as $S_{sp}$. For a sampling pattern with $I$ number of pixels, $T_{SR}$ is the sum of time used to render each pixel in the sampling pattern defined by the specific RecNN. 

\begin{equation}
T_{SR}= \sum_{k = 0}^{I-1}T_{k}
\end{equation}

Once the RecNN is trained, the inferencing time $T_{RecNN}$ can be accelerated by GPU in constant time. The time used to render the PR-Image is the main overhead of the overall rendering latency. The time used to render the $k$th pixel, $T_k$, is determined by the complexity of the chosen rendering technique (ray casting~\cite{795213}, ray tracing~\cite{10.1145/1198555.1198754}, global illumination~\cite{10.1145/2448196.2448205} etc.) and the interpolation method (linear or high-order~\cite{sun2024mfa}). Because of the high computational cost of pixel rendering, decreasing $I$ can significantly lower the rendering latency. However, $I$ is predefined by the sampling pattern which is a fixed hyperparameter for existing RecNNs. In order to further decrease the computational cost, a new set of important pixels $S_{im}$ needs to be selected from $S_{sp}$, and the rendering of $S_{im}$ can losslessly reconstruct the rendering of $S_{sp}$. We call the binary mask that filters out $S_{im}$ from $S_{sp}$ as Importance Mask(IM). For a chosen rendering function $R()$ and reconstruction function $f()$. The optimal $S_{im}$ can be derived from:

\begin{equation}
S_{im} = \arg \min_{\left|R(S_{sp})-f(R(S_{im}))\right|<\epsilon}\left| S_{im}\right|
\end{equation}
where $\epsilon$ is a small predefined interval of error metric between the rendering of $S_{sp}$ and the reconstructed rendering from $S_{im}$. $\left| S_{im}\right|$ is the number of important pixels. Our objective is to select an optimal subset from the sampling pattern with the fewest possible pixels while ensuring that the rendering of this subset can accurately reconstruct the full sampling pattern with minimal reconstruction error. We name the percentage of pixels when using optimal $S_{im}$ as the optimal rendering percentage (ORP).

The volumetric datasets, mostly collected from scientific domains, show strong spatial correlations in their 3D domains. Reflected on the 2D image domain, such spatial correlation can also be observed. This provides opportunities to compress the complex pixel-level rendering into compact representations. Although there are numerous existing works to extract superpixels directly from 2D datasets~\cite{10.1145/3652509}, $S_{im}$ needs to be selected by not only considering the data itself but jointly considering the users' input and downstream RecNN. Inspired by the saliency map~\cite{6248093} demonstrating which parts of the input data (e.g., pixels in an image) contribute most to the predictions of a neural network, we provide a method to automatically learn a selection of pixels that are the most important to reconstructing the rendering of sampling pattern. Instead of calculating the gradient of output with respect to the input, as the saliency map does, we utilize an autoencoder architecture to learn the IM as a latent feature through backpropagation from the training dataset. In this work, we propose two neural networks: Importance Mask Learning Network (IML Net) and Importance Mask Synthesis Network (IMS Net). IML Net will learn the importance mask from training datasets which are visualization images generated from perspective user view-dependent operations. Once the importance masks are learned, they will then be used as training datasets for the IMS Net which will learn the mapping from user's view to the IM. 

\subsection{Differentiable Compaction and Decompaction}
The goal of our work is to derive an optimal $S_{im}$ from $S_{sp}$, however, the shape of the sampling pattern varies for different RecNN. For example, super-resolution RecNN uses a downsampling pattern while the foveated rendering RecNN uses a foveal pattern. The downsampling pattern is evenly distributed across the PR-Image. The foveal pattern generally refers to the region of an image where the eye's fovea (central vision) is focused, which is where human vision is most detailed. In foveated rendering, only the pixels in this high-resolution foveal area are rendered in full detail, while the surrounding areas corresponding to peripheral vision are rendered at a lower resolution. To uniformly process sampling patterns with different shapes, we introduce a compaction layer to compact the pixels in the sampling pattern into a regular 2D matrix. \cref{fig:compaction} show how the PR-Image is selectively rendered according to downsampling and foveal patterns, and how the PR-Image is compacted into a compact 2D matrix. We name this 2d matrix a compact image (C-Image). The compaction collects each pixel within the sampling pattern by following a straightforward Raster scan order on the PR-Image. If the C-Image is not filled up after compaction, zeros are padded to its empty pixels. We can see that the C-Image of super-resolution RecNN is the corresponding low-res version of the FR-Image, while the C-Image of foveated rendering RecNN is more irregular. The compaction layer is a practice of dataset distillation where only informative pixels are gathered for efficient training. Decompaction is the reverse of the compaction. The compaction and decompaction layers are 2D point transformations between PR-Image space and C-Image space. To make the compaction and decompaction layers differentiable, transformation is implemented as a translation matrix multiplying each pixel in one space to map its value to the transformed location of another space. The benefit of incorporating compaction and decompaction layers includes: 1) Reduce the dimension of the redundant training data into a compact subset. 2) Decrease the number of parameters of the IML network. 3) Improve the speed of convergence during training. 4) Improve the speed of inferencing for interactive visualization. 5) Allow the proposed IML and IMS Networks to remain agnostic to diverse sampling patterns within a unified network architecture.

\begin{figure}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/compaction_small.png}
    \caption{Compaction and decompation for super-resolutin RecNN and foveated rendering RecNN.}
    \label{fig:compaction}
\end{figure}

\subsection{Importance Mask Learning Network}
The functions of IML Net include: 1) For each rendered sampling pattern, learn an IM that selects only the important pixels from it to render for the downstream reconstructions. 2) Learn a decoder that reconstructs the full rendering of the sampling pattern (C-Image) from the partially rendered sampling pattern (PR-C-Image) based on the learned IM.

\begin{figure*}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\textwidth]{figures/IMLNet_small.png}
    \caption{Network architecture of IML Net. IML Net works on a compact image (C-Image) domain which has a resolution of $n\times n$. The full-res output has a resolution of $m\times m$ where $m>n$.}
    \label{fig:iml-net}
\end{figure*}

\subsubsection{Network Architecture}
\cref{fig:iml-net} shows the network architecture of IML Net. IML Net is an autoencoder neural network that takes a C-Image after compaction as input and outputs an image as close as the C-Image. The number of pixels and their distribution on C-Image are defined by the specific sampling pattern selected. The importance mask is learned on top of it by optimizing the network parameters through backpropagation during training. Key components of the network are:

\textbf{Encoder and Decoder:}
The first and the last subnetworks of the IML Net are encoder and decoder sharing the same network structure as shown in \cref{fig:endecoder}. We utilize standard U-Net~\cite{10.1007/978-3-319-24574-4_28} to construct the encoder/decoder network. Our IML Net is invariant to the input C-image resolution due to the use of convolutional layers. For the encoder, C\_in is set to 3 for taking RGB C-Image after compaction as input, C\_out is set to 1 for outputting a grayscale image where the value of each pixel correlates to the probability of being an important pixel. The U-Net structure of the encoder can effectively learn such pixel values to find the IM. After the last layer of the encoder, a softplus layer is used to convert the output value into the positive range for the following normalization layer. For the decoder, both C\_in and C\_out are set to 3 for taking the partially rendered C-Image (PR-C-Image) as input and outputting an image close to the input C-Image of the encoder. The decoder of our IML network performs a low-level in-painting to complete the missing pixels in the PR-C-Image to reconstruct the complete C-Image.

\begin{figure}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/endecode.png}
    \caption{Network architecture of the encoder and decoder in IML Net. The input image resolution is $n\times n$.}
    \label{fig:endecoder}
\end{figure}

\textbf{Normalization:} The output of the encoder presents preliminary information about the importance of each pixel, and we name this output Preliminary Importance Mask (PIM). We apply a similar practice proposed by Weiss et al.~\cite{9264699} to normalize the PIM with a hyperparameter, called mean value, which will control how much percentage of C-Image pixels as important pixels. It is worth noticing that our method is trying to solve the problem of interactively generating visualization for novel views from rendered IM while their work~\cite{9264699} provides a differential sampling method that helps the learning of IM. The normalized PIM, can be computed:
\begin{equation}
PIM'_{ij} = l+PIM_{ij}\frac{u - l}{u_{PIM}+\delta}
\end{equation}
where the $i$ and $j$ are pixel indices, $u$ is the prescribed mean value and $l$ is the lower bound on the sample distribution which is set to 0. $u_{PIM}$ is the mean of PIM. $\delta$ is a small constant set as $1e-7$ to avoid zero division. All the above operations ensure that the normalization layer remains differentiable. It is worth noticing that while the normalization step can also be performed using a softmax layer, this approach may result in a loss of control over the proportion of C-Image pixels designated as important through the prescribed mean. Our normalization step enables the hyperparameter tuning to investigate the relationship between the sparsity of important pixels and reconstruction performance.

\textbf{Rejection Sampling:}
The rejection sampling layer is to generate a binary IM from the continuous PIM'. The practice is filtering out the important pixel through a comparison between the pixel value in PIM' and a randomly sampled value. The rationale behind this practice is due to spatial correlation found in the prediction from image generative neural networks. This fact makes the direct learning of the important pixel as a blob of connected pixels which doesn't distinguish the underlying structure. Comparing the prediction with an uncorrelated random pattern can solve this issue and create isolated pixels that reflect the importance of the underlying structure. The random pattern $P$ is a grayscale image with the same dimension as C-Image with its pixel value randomly sampled from a uniform distribution between 0 and 1. We use plastic sampling to generate $P$ for the superior quality of the learned IM compared to other sampling strategies\cite{9264699}. The IM can be retrieved through an independent Bernoulli process through rejection sampling.
\begin{equation}\label{eq:rs}
    IM_{ij}= 
\begin{cases}
    1,& \text{if } PIM'_{ij} > P_{ij}\\
    0,              & \text{otherwise}
\end{cases}
\end{equation}
To make the rejection sampling layer differentiable, the IM can be calculated as:
\begin{equation}
IM_{ij} = Sigmoid(\alpha(PIM'_{ij} - P_{ij})),\;Sigmoid(x)=\frac{1}{1+e^{-x}}
\end{equation}
where $\alpha$ is a positive scalar determining the steepness of the Sigmoid function. The pixel value of each IM is in the range $[0, 1]$. The above equation is only used for training the IML Net. When using IML Net during the inferencing, which will be discussed in \cref{section:inferencing}, only the regular rejection sampling of \cref{eq:rs} is used. In that case, a binary mask using 1 bit per pixel is enough to represent the importance mask.

\textbf{Selective Rendering:}
The selective rendering layer will generate the partially rendered C-Image (PR-C-Image) according to the IM. During training, this step is basically element-wise multiplication between the fully rendered C-Image (FR-C-Image) of the training dataset and IM:
\begin{equation}
PR{\text -}C{\text -}Image = IM\times FR{\text -}C{\text -}Image
\end{equation}
During the inferencing stage, this layer is replaced by actual pixel-level rendering computation defined by the chosen volume rendering algorithm.



\begin{figure}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/IMSNet_small.png}
    \caption{Network architecture of IMS Net.}
    \label{fig:ims-net}
\end{figure}

\subsubsection{Loss Functions}
The IML Net can be trained by itself with a compaction loss function which is an image loss function between the input C-Image and reconstructed C-Image. This practice is suitable for optimizing the off-the-shelf pre-trained RecNNs without going through the long end-to-end training process resulting from adding a complex downstream RecNN. The IML Net can also be trained together with downstream RecNN selected for various sampling patterns. In this case, the loss function is the weighted sum of the previous compacting loss and the image loss between the predicted reconstructed full-resolution image (FR-Image) and the ground truth (GT) image rendered using the baseline rendering algorithm without using RecNN. This end-to-end training will give better reconstruction results by jointly optimizing the parameters from both the IML Net and the downstream RecNN. In our work, we use the Mean Squared Error (MSE) as the image loss function and 0.5 as the weight for both image losses if trained end-to-end.

\subsection{Importance Mask Synthesis Network}
Once the IML Net work is learned, we collect the view that generates the C-Image and the learned IM as input-label pairs to train the IMS Net through supervised learning. The function of IMS Net is to quickly and directly predict the IM from a new set of view parameters.

\subsubsection{Network Architecture}
The focus of this paper is on visualizing scientific data, which is a time-sensitive application. Therefore, we need to keep the main steps of the pipeline, including training and inferencing, finished as fast as possible. Our training includes two steps: training the IML Net and training the IMS Net. We provide a method to speed up the training of IML network, as mentioned in \cref{feasibility}, by training a standalone IML Net without connecting to the downstream RecNN. However, we cannot carry out the same optimization on training the IMS Net. As a result, we have to select a GAN with less training/inferencing complexity. DCGAN~\cite{radford2015unsupervised} provides a better trade-off between the complexity and the reconstruction quality than other GAN architectures like cGAN (Conditional GAN)~\cite{Isola_2017_CVPR} and StyleGAN~\cite{8977347} whose training times are too long for scientific visualization. \cref{fig:ims-net} shows the network architecture of IMS Net and the discriminator used for training. IMS Net is a generator that takes a sparse set of view parameters to generate a 2D binary image as the IM. The IMS Net is trained through the generative adversarial network (GAN). Key components of the network are:

\textbf{Generator:}
The input to the generator is a set of view parameters. We adopted the OpenGL convention to define the view parameters as 9 float numbers:
\begin{equation}
view = (eye\_x/y/z, \; lookAt\_x/y/z, \; up\_x/y/z)
\end{equation}
where the $eye$ is the camera position, $lookAt$ is the focal point where the camera is looking, and $up$ is the "up" direction of the camera. IMS Net is a DCGAN with its detail structure shown in \cref{fig:im_generator}. It consists of several transposed convolutional layers with batch normalization and ReLU activation functions.

\textbf{Discriminator:}
\cref{fig:im_discriminator} shows the detailed structure of the binary classifier as a discriminator. The discriminator consisted of several convolutional layers with batch normalization and Leaky ReLU activation functions.

% \begin{figure}[t]
%     \centering 
%     \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/gan.png}
%     \caption{Network architecture of the IM generator in IMS Net and the IM discriminator.}
%     \label{fig:gendis}
% \end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\linewidth}
        \centering
        \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/gan_generator.png}
        \caption{IM generator}
        \label{fig:im_generator}
    \end{subfigure}
    % \hfill
    \vskip\baselineskip
    \begin{subfigure}[b]{\linewidth}
        \centering 
        \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/gan_discriminator.png}
        \caption{IM discriminator}
        \label{fig:im_discriminator}
    \end{subfigure}
    \caption{Network architecture of the IM generator of IMS Net (a) and the IM discriminator for training the GAN (b).}
    \label{fig:gendis}
\end{figure}


\subsubsection{Loss Functions}
As shown in \cref{fig:ims-net}, we consider two losses to train the IMS Net: image loss on the IM and adversarial loss. The adversarial loss is composed of two parts: the generator loss and the discriminator loss. The discriminator loss (cross-entropy of the binary classifier) measures how well the discriminator can distinguish between real and generated images.
 
\begin{equation}
L_D = -\mathbb{E}{x \sim p{data}(x)}[\log D(x)] - \mathbb{E}_{z \sim p_z(z)}[\log (1 - D(G(z)))]
\end{equation}
where  $D(\mathbf{x})$  is the discriminator’s output for a real image. $D(G(\mathbf{z}))$  is the discriminator’s output for a fake image  $G(\mathbf{z})$. The generator loss measures how well the generator can fool the discriminator.

\begin{equation}
L_G = -\mathbb{E}_{z \sim p_z(z)}[\log D(G(z))]
\end{equation}
The loss function selection and configuration are crucial in training a GAN. Detail results using different loss functions for training IMS Net are discussed in \cref{sec:loss}.

\subsubsection{Training}
Training a generative model to directly generate volume visualization is challenging~\cite{8316963, 10.5555/3157096.3157346, Zhang_2017_ICCV, brock2018large, pmlr-v70-arjovsky17a} for several reasons: 1) It is difficult for GAN to generate high-res images. 2) Training of GAN tends to be unstable because of the mode collapse, vanishing gradients, imbalanced learning rate, and data overlap issues. 3) While generative models can synthesize convincing volume visualization, they may struggle with very fine details or subtle structures. To prevent the aforementioned issues during the training of our generator, we utilize the following practices:
\begin{itemize}[leftmargin=*]
  \item Avoid learning the IM directly from the sparse view parameters but from C-image which presents more structural information. Our experiments show the IM directly learned from view parameters is a binary mask whose important pixels are evenly distributed across the region of the volumetric object. In that case, important pixels couldn't properly capture the locations of informative regions of the rendered image.
  \item Make the generator learn to predict simple patterns. The IM that the IMS Net tries to learn is a simple 2D binary image with only a single channel and finite outputs (-1 or 1).
  \item Utilize batch normalization for both the generator and discriminator to stabilize the training.
  \item Using activation function ReLu for the generator and LeakyRelu(0.2) for the discriminator.
  \item Scale the input binary IM from $\{0, 1\}$ to $\{-1, 1\}$.
  \item Use hyperbolic tangent function (Tanh) at the end of the generator to constrain the output between -1 and 1.
  \item Use adversarial loss together with advanced perceptual loss~\cite{10.1007/978-3-319-46475-6_43} instead of simple BCE or MSE loss. Perceptual loss is a loss function that measures the difference between high-level feature representations of images rather than their pixel-wise difference. It leverages pre-trained deep neural networks (e.g., VGG~\cite{simonyan2014very}) to compare images at a feature level, focusing on perceptual quality and structural similarity. Perceptual loss evaluates the similarity between two images based on the semantic content, such as textures, edges, and patterns, offering superior perceptual quality compared to pixel-wise loss like MSE and MAE.
\end{itemize}

\subsubsection{Inferencing}\label{section:inferencing}
Once the IMS Net is trained, a visualization image synthesis network is constructed by combining three components: the trained IMS Net, the decoder of the trained IML Net, and the RecNN as shown in \cref{fig:imls_net}. We name this network IML/S Net + RecNN. The new network directly generates visualization image from the new view with improved rendering latency than the original RecNN. Detailed quality and timing evaluation of IML/S Net + RecNN is discussed \cref{sec:results}.

\begin{figure}[t]
    \centering 
    \includegraphics[trim=0 0 0 0,clip,width=\linewidth]{figures/IMLSNet_small.png}
    \caption{Network architecture of the proposed visualization image synthesis network, IML/S Net + RecNN. IML/S is the IMS Net connected with decoder of IML Net. Trained IML/S Net will be used for image synthesis to directly predict FR-Image from view parameters.}
    \label{fig:imls_net}
\end{figure}



