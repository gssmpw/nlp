\section{Related work}
\label{Related work}

\subsection{Imbalanced-Small Sample Data (ISSD) HSIC}

Several prior works, including data augmentation, resampling (oversampling and undersampling) **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique"** , deep transfer learning **Pan et al., "Transfer Learning for Visual Categorization"**, and deep few-shot learning **Vinyals et al., "Matching Networks for One Shot Learning"**, have been proposed to address the ISSD HSIC problem. For instance, random oversampling (ROS) and random undersampling (RUS) simply replicate the training samples which leads to data redundancy **Chen et al., "Random Oversampling and Undersampling"** . Synthetic minority oversampling technique (SMOTE) may contribute to better classification performance on conventional classifiers, but this method uses a simple linear combination of one random sample and its nearest neighbors to generate synthetic minority samples, which fails to capture the complex manifold structure and is hard to satisfy the nonlinear spatial-spectral relationship in the HSI data **Chawla et al., "SMOTE: Synthetic Minority Over-sampling Technique"** .

Latent variable models, especially represented by the deep generative models (DGMs), introduce uncertainty into the low-dimensional discriminative latent space during the reconstruction process, possessing the ability to stimulate the complex heterogeneity of HSI data and generate samples for the minority class. Particularly, GANs replace the complexity of resampling by searching for a Nash equilibrium of the generator and discriminator through adversarial training. Specifically, class-informed mixture GANs are developed to ensure the generated samples align with the actual data distribution. For example, Dam et al. proposed a parallel mixture generator spectral 1D-GAN **Dam et al., "Parallel Mixture Generator Spectral 1D-GAN"** to generate class-dependent samples by fully exploiting the spectral feature. Auxiliary classifiers significantly improved the classification performance of imbalanced datasets. Zhu et al. proposed a 3D-GAN **Zhu et al., "3D-GAN: A Novel Deep Learning Architecture for HSIC"** that took both spatial and PCA-reduced spectral features into account to handle 3D patch samples. The discriminator not only distinguished the real data from the synthetic fake data but also leveraged the softmax classifier to produce the classification maps.

The common deep transfer learning methods mainly use some pre-trained models in the natural images field, such as VGG16 and ResNet50 with frozen parameters, to fine-tune the downstream tasks in the remote sensing community. Thanks to these pre-trained models, they can obtain promising results with a small sample size and lower computational burden. Jiao et al. proposed a deep multiscale feature extraction algorithm, which fully took advantage of pre-trained VGG-16, to exploit the spatial structure information of hyperspectral images by performing a full convolution operation, and then realized adaptive spatial-spectral information fusion by employing weighted strategy **Jiao et al., "Deep Multiscale Feature Extraction"** . Wang et al. first built three-spectral band hyperspectral datasets through random band selection (RBS) procedure, letting VGG-16 learn the feature of homogeneous areas with distinguished semantic and geometric properties **Wang et al., "Random Band Selection for HSIC"** . However, extracting features of HSI with models that are pre-trained on natural RGB images requires transforming hundred of bands into triple-band datasets, which will inevitably result in the loss of essential information.

\subsection{Image Synthesis for HSIC}
To estimate the data distribution for more realistic image synthesis and expand the small training data, scholars have developed several methods, which have achieved promising results. The commonly used algorithms include the mixture model (Gaussian Mixture Model, GMM), generative model (GAN, VAE), and DDPM.

Deep learning-based methods, with GAN and VAE being typical representatives, generate new samples of a given distribution. Specifically, this is achieved by training a generator to map random noise from the latent space to the data distribution. Instead of inferring the global distribution, scholars employed conditional generative models, which are designed to condition the output of the generator based on the hyperspectral classes. For instance, in **Audebert et al., "Conditional GAN for HSIC"**, Audebert et al. used conditional GAN to generate an arbitrarily large number of hyperspectral samples that matched the distribution of any dataset. Considering spectral mixing, this method can synthesize any combination of classes by interpolating between vectors in the latent space. Due to the high-dimensional HSI data with complex noise, VAE is more suitable for processing HSI by introducing the Gaussian noise into the encoded results in latent space. In **Xi et al., "DGSSC: Data Augmentation in Latent Variable Space"**, Xi et al. proposed a DGSSC method, which can achieve minority-class data augmentation in the latent variable space to address the problem of imbalanced data. However, the condition was only based on one-hot encoding of hyperspectral classes. Considering the complex situations in the real-world application and the semantic context, it is likely that the one-hot-like encoding in probabilistic form, such as pixel abundance, instead of binary vector, will work better.

Very recently, DDPMs have demonstrated superior performance in image inpainting and synthesis compared to GANs and VAEs. For instance, in **Zhang et al., "R2H-CCD: Real-Time High-Fidelity HSIC Generation"**, Zhang et al. proposed R2H-CCD, an algorithm for HSI generation that leveraged the conditional DDPM to produce high-fidelity HSI from RGB image through cascaded generation. Liu et al. proposed a diverse hyperspectral remote sensing image generation method based on latent diffusion models (HyperLDM) **Liu et al., "HyperLDM: Latent Diffusion Models for HSIC"**, employing a conditional vector quantized generative adversarial network (VQGAN) to compact high-dimensional information for accelerating the diffusion process. Additionally, HyperLDM incorporated abundance maps derived from VQGAN to condition the HSI generation via LDM. By doing so, the model effectively accounted for the spectral mixing of multiple materials within HSI pixels. However, a critical limitation of HyperLDM is its reliance on an accurately calibrated spectral library during the transformation of abundance maps into HSI using the linear mixture model (LMM).

However, the above methods, whether conditional GANs conditional VAEs, or latent conditional DDPMs, still require substantial quantities of paired remote sensing image observations (e.g., RGB image and corresponding HSI). This poses a significant challenge in practical applications, due to the limited availability of such paired datasets arising from satellite revisit period variations and heterogeneous sensor specifications across different observation platforms.

\subsection{Imageâ€“Text Pairing information in HSIC}
Hyperspectral image analysis demands intensive manual labeling, while, language representations are inherently more accessible straightforward for humans to express and acquire. Notably, recent advancements in AI methods, such as contrastive language image pre-training (CLIP) **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** has demonstrated the capability to model the bidirectional relationships between these two modalities (image and text). Moreover, this method can be applied to various applications like image classification, object detection, segmentation, etc. by fine-tuning the pre-trained models on specific downstream tasks.

\subsection{Latent Diffusion Models}

In principle, diffusion models have the capability to formulate conditional distributions in the form of $p(z|c)$. This can be implemented with a conditional denoising autoencoder $\epsilon_{\theta}(z_{t}, t, c)$, which paves the way to control the synthesis process through inputs $c$, such as text **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"**, semantic maps **Long et al., "Convolutional Networks for Image Captioning and Retrieval"**, and other image translation tasks **Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**.