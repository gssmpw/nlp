\renewcommand\thefigure{A\arabic{figure}}
\renewcommand{\theHfigure}{A\arabic{figure}}

\setcounter{figure}{0}
\renewcommand\thetable{A\arabic{table}}
\renewcommand{\theHtable}{A\arabic{table}}
\setcounter{table}{0}
\renewcommand\theequation{A\arabic{equation}}
\renewcommand{\theHequation}{A\arabic{equation}}

\setcounter{equation}{0}
% \pagenumbering{arabic}% resets page counter to 1
% \renewcommand*{\thepage}{A\arabic{page}}
% \setcounter{footnote}{0}

\appendix
\clearpage
\onecolumn


\section{Appendix}
% \tableofcontents % Appendix用の目次を表示

\subsection{Full Related Work}
\label{appendix: related_work_1}
\subsubsection{Interpreting the representations of the brain's neurons.}
Encoding models have long been used in neuroscience to interpret neural representations within the brain~\cite{kay2008identifying, nishimoto2011reconstructing, naselaris2011encoding, huth2012continuous}. These studies used interpretable features, such as low-level visual attributes, or high-level semantic features, such as one-hot encoding of words, for straightforward voxel-wise interpretation.

Recent approaches use features derived from DNNs and have demonstrated higher explanatory power for brain activity than those using simpler, more interpretable features~\cite{gucclu2015deep, schrimpf2021neural, takagi2023high, antonello2024scaling}.  However, the interpretability of these DNN-based encoding models remains challenging, leading to the development of methods that condense the entire set of voxels into a small number of universal and interpretable axes~\cite{huth2016natural, lescroart2019human, nakagi2024unveiling}.

Recent approaches propose data-driven methods to describe the properties of individual brain voxels using natural language~\cite{luo2023brainscuba, singh2023explainingblackboxtext} when analyzing brain representations at a finer, voxel-wise level. BrainSCUBA~\cite{luo2023brainscuba} is an end-to-end method that uses an existing image captioning model, which provides voxel-wise captions of the visual cortex in a data-driven manner. BrainSCUBA projects each voxel‘s encoding weight onto the image feature space via dot-product attention, identifies regions of highest similarity, and then uses a text decoder to generate captions describing the images to which the voxel is most selective. This approach provides a data-driven natural-language description of voxel selectivity without additional training. Similarly, SASC~\cite{singh2023explainingblackboxtext} uses fMRI data collected during speech listening~\cite{lebel2023natural} to identify the short phrases that most strongly activate each voxel. It then uses an LLM to combine these short phrases into a single, data-driven caption describing each voxel’s semantic properties. 

Our proposed method also generates data-driven voxel captions but differs in several ways. First, BrainSCUBA is constrained to pre-existing, end-to-end image captioning models. In contrast, our approach divides the task into (i) identifying an optimal set of images and (ii) converting these images into a caption, allowing us to use any vision model aligned with language and any LLM with advanced language capabilities without requiring specialized fine-tuning. Furthermore, although SASC uses an LLM to create voxel captions, it primarily synthesizes short, low-information phrases (e.g., trigrams), producing only simple keyword-based captions. In contrast, our method summarizes more diverse and informative text and then uses these extracted keywords to compose a complete sentence, capturing a richer range of voxel-level properties.

\subsubsection{Interpreting the Representations of Artificial Neurons in DNNs}
Interpreting artificial neurons is a key challenge in understanding how DNNs process information. We can potentially examine human neural representations at a finer granularity by applying the data-driven and highly accurate interpretation methods developed for artificial neurons to analyze human brain voxels.

Numerous studies have aimed to associate artificial neurons with human-interpretable concepts~\cite{bau2017network, mu2020compositional, oikarinen2022clip, kalibhat2023identifying, bykov2024labeling}. These methods link neurons to textual concepts by comparing neuron output feature maps with outputs from segmentation models. However, these approaches are constrained by predefined concept sets or limited to the dataset’s words and phrases. MILAN~\cite{hernandez2021natural} introduced a generative approach, enabling adaptation to different domains and tasks, but it requires annotated data, which poses challenges for scalable applications.

LLMs permit open-ended descriptions of artificial neurons without additional model training~\cite{singh2023explainingblackboxtext, bai2024describe, wu2024and, hoang2024llm}. Analogous to these methods, our study also leverages LLMs to generate open-ended concepts for \textbf{brain} neurons rather than artificial neurons, seeking flexible and diverse interpretations that do not depend on predefined vocabularies.


\subsection{Implementaion Details}
\subsubsection{Generating Voxel Captions}
\label{appendix:meacap}
In this study, we generate sentence-level captions from keywords by leveraging the ``Sentence Composer'' proposed in the image captioning model, MeaCap~\cite{zeng2024meacap}—referred to in the MeaCap paper as a ``keywords-to-sentence LM.'' In MeaCap, a target image is first matched with similar candidate captions via CLIP-Text embeddings. Each candidate caption is parsed into Subject–Predicate–Object triplets using TextGraphParser~\cite{li2023factual}, which produces keywords.
While TextGraphParser can be used in our approach to extract keywords from candidate captions, it treats each word in the caption individually, limiting its ability to capture higher-level concepts (e.g., ``animal'' as a hypernym of ``dog'' and ``cat'') or to derive keywords that depend on sentence-level understanding (e.g., \textit{interaction between humans and animals}). We overcome these limitations using LLMs (through in-context learning) as our keyword extraction model. We further verify the effectiveness of LLM-based keyword extraction by comparing it against the baseline that uses TextGraphParser (\ref{appendix:ablation}).

After extracting keywords, MeaCap feeds the extracted keywords into CBART~\cite{he2021parallel}, a lexically constrained language model (Sentence Composer). CBART adopts an encoder–decoder structure, where the encoder predicts actions at the token level, such as copying (using the token as-is), substituting (replacing it with a new token), or inserting new tokens. The decoder then selects the token with the highest conditional probability when substitution or insertion is needed and iteratively refines the output until all tokens are predicted to be copies. In MeaCap~\cite{zeng2024meacap}, the decoder’s choices are guided not only by conditional probabilities but also by the summed similarity of the generated caption’s CLIP-Text latent representation with (1) the target image’s CLIP-Vision latent representation and (2) the CLIP-Text latent representations of the top-$N$ most similar external captions. This strategy enables inserting or substituting new words more relevant to the image. Our study replaces the image features used in MeaCap with the voxel-wise encoding weight, substituting and inserting new words based on the encoding weight. By doing so, we generate voxel captions from keywords while preserving the core mechanism of MeaCap’s Sentence Composer.


\subsubsection{Pretrained Checkpoints}
\label{appendix:pretrained_ckpts}
Our experiments use the CLIP ViT-B/32 weights provided on GitHub\footnote{\href{https://github.com/openai/CLIP}{https://github.com/openai/CLIP}} for encoding, caption generation, and image-level caption evaluation. For captioning each voxel‘s optimal image set, we use MLLM model MiniCPM-Llama3-V2.5, available on Hugging Face\footnote{\href{https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5}{https://huggingface.co/openbmb/MiniCPM-Llama3-V-2\_5}}. We extract keywords from the resulting optimal captions using \textit{gpt-4o} (gpt-4o-2024–08–06) via the OpenAI API for the primary analysis and Llama3.1–70B from Hugging Face\footnote{\href{https://huggingface.co/meta-llama/Llama-3.1–70B-Instruct}{https://huggingface.co/meta-llama/Llama-3.1–70B-Instruct}} for the ablation study (\ref{appendix:ablation}). For the sentence-level caption evaluation, we calculate sentence similarity using Sentence-BERT from Hugging Face\footnote{\href{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}}. Moreover, for image-level caption evaluation, we generate voxel images from voxel captions using an image generation model on Hugging Face\footnote{\href{https://huggingface.co/black-forest-labs/FLUX.1-schnell}{https://huggingface.co/black-forest-labs/FLUX.1-schnell}}.

\subsubsection{BrainSCUBA}
\label{appendix:brainscuba}
Because the BrainSCUBA codebase is not publicly available, we implemented it ourselves for this study. In BrainSCUBA, the encoding weights (linear layers) are learned using gradient descent. In our implementation, consistent with our proposed method, we trained the encoding weights using L2-regularized linear regression from the \textit{himalaya} library package\footnote{\href{https://github.com/gallantlab/himalaya}{https://github.com/gallantlab/himalaya}}~\cite{la2022feature}.

Moreover, BrainSCUBA projects each voxel‘s encoding weight into image space using a dataset of 2 million images, combining OpenImages~\cite{OpenImages} and LAION-A v2 (6+ subset)~\cite{schuhmann2022laion}. However, the specific images selected from each dataset are not disclosed. We ensure a fair and dataset-independent comparison by relying solely on the 1.7 million images from OpenImages (the same dataset used by our proposed method, LaVCa). We leverage the training set of the subset that is accompanied by bounding boxes, object segmentations, visual relationships, and localized narratives.

For other hyperparameters, we tested temperature values of 1.0, $1/10$, $1/100$, $1/150$ (the value used in the BrainSCUBA paper), and $1/500$ for the softmax projection (Figure \ref{appendix:scuba_temp}). We used beam search with a beam width of 5 to generate the text decoder’s caption as described in the BrainSCUBA paper.


\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/scuba_temp_all_sub_with_lavca_ver3.pdf}
  \caption{The change in accuracy of BrainSCUBA with respect to temperature. The error bars represent standard error. Moreover, the star markers on the plot indicate the point where the temperature is set to 1/150, as adopted in the original BrainSCUBA paper. The green line represents the average value of LaVCa.
}
  \label{appendix:scuba_temp}
\end{figure*}


\clearpage
\subsection{Ablation Study}
\label{appendix:ablation}
We conduct an ablation study to evaluate how hyperparameter choices affect our method’s accuracy. First, we vary the number of optimal images used for keyword extraction with gpt-4o from 5 to 10, 50, and 100 (Table \ref{appendix:ablation_table}, top) while fixing the number of extracted keywords at 5 (as in the primary analysis). Increasing the number of optimal images up to 50 leads to higher accuracy. This gain likely arises because relying only on top-ranked images may omit useful second- and third-ranked keywords; increasing the number of optimal images enables us to capture a broader range of selective keywords. However, after the number of optimal images reaches 100, the improvement ceases, likely because additional concepts cannot be adequately captured using only five keywords. These observations suggest that increasing the number of concepts could further improve accuracy.

Next, we compare accuracy when the number of extracted keywords from the optimal image set (using gpt-4o) is varied among 1, 5, and 10 (Table \ref{appendix:ablation_table}, middle) while fixing the number of optimal images at 50 as in the primary analysis. The results illustrate that increasing the number of output concepts from 1 to 5 boosts accuracy while pushing this number to 10 reduces accuracy. This decline may be caused by extracting irrelevant or noisy concepts. The improvement with five keywords suggests that voxels encode multiple concepts, but extracting too many can introduce noise. Increasing the number of optimal images, rather than keywords, might be a more effective method of capturing additional helpful concepts.

Finally, we compare three models for keyword extraction from the optimal image set: the large language model gpt-4o, an 8-bit quantized Llama3.1–70B (meta-llama/Llama-3.1–70B-Instruct), and the TextGraphParser \cite{li2023factual} used in MeaCap (Table \ref{appendix:ablation_table}, bottom). For gpt-4o and Llama3.1–70B-8bit, we use 50 optimal images and extracted five keywords, matching the primary analysis. For TextGraphParser, we use MeaCap’s default settings, extracting five image captions and up to four concepts per caption. Experimental results demonstrate that gpt-4o outperforms TextGraphParser, suggesting that an open-ended LLM-based approach to concept extraction is more effective than simply adopting words from captions, as TextGraphParser does. Moreover, comparing gpt-4o with Llama3.1–70B-8bit reveals that a higher-performance LLM can further boost accuracy. These findings imply that, as LLM capabilities continue to advance, so too does our framework’s potential to enhance the interpretability of voxel representations.


\begin{table*}[t]
\centering
\caption{Comparison of sentence-level brain activity prediction accuracy using different hyperparameters. The accuracy represents the mean and standard deviation of brain activity prediction accuracy for the top 5000 voxels in the test data at the text level. The top 5000 voxels were selected based on their prediction accuracy in the training data.}
\vskip 0.15in
\resizebox{\textwidth}{!}{
\begin{tabular}{lp{2.5cm}cccc}
\toprule
Parameter & Setting & subj01 & subj02 & subj05 & subj07 \\
\midrule
\multirow{4}{*}{Num. of images} 
& 5 & 0.2394 ± 0.0681 & 0.2786 ± 0.0730 & 0.2936 ± 0.0830 & 0.2092 ± 0.0730 \\
& 10 & 0.2434 ± 0.0678 & 0.2814 ± 0.0724 & 0.2966 ± 0.0826 & 0.2120 ± 0.0737 \\
& 50 & 0.2459 ± 0.0663 & 0.2874 ± 0.0748 & 0.3055 ± 0.0840 & 0.2182 ± 0.0730 \\
& 100 & 0.2458 ± 0.0672 & 0.2852 ± 0.0739 & 0.3011 ± 0.0834 & 0.2154 ± 0.0721 \\
\midrule
\multirow{3}{*}{Num. of keywords} 
& 1 & 0.2374 ± 0.0660 & 0.2738 ± 0.0734 & 0.2949 ± 0.0849 & 0.2066 ± 0.0719 \\
& 5 & 0.2459 ± 0.0663 & 0.2874 ± 0.0748 & 0.3055 ± 0.0840 & 0.2182 ± 0.0730 \\
& 10 & 0.2410 ± 0.0671 & 0.2790 ± 0.0736 & 0.2956 ± 0.0842 & 0.2116 ± 0.0742 \\
\midrule
\multirow{3}{*}{Extraction model} 
& TextGraphParser & 0.2416 ± 0.0667 & 0.2764 ± 0.0731 & 0.2955 ± 0.0841 & 0.2048 ± 0.0706 \\
& Llama3.1-70B & 0.2375 ± 0.0671 & 0.2812 ± 0.0749 & 0.2984 ± 0.0846 & 0.2140 ± 0.0734 \\
& gpt-4o & 0.2459 ± 0.0663 & 0.2874 ± 0.0748 & 0.3055 ± 0.0840 & 0.2182 ± 0.0730 \\
\bottomrule
\end{tabular}
}
\label{appendix:ablation_table}
\end{table*}

\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/prompt.pdf}
  \caption{The prompt used for summarizing the captions of optimal image groups with an LLM. The text in red represents the captions of the optimal image groups, which vary depending on the target voxel and the number of optimal images used. The blue number specifies the number of concepts, which was varied during the ablation study.
}
  \label{appendix:prompt}
\end{figure*}


\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/voxel_pred_method.pdf}
  \caption{Overview of our evaluation methods. (a) We first obtain text embeddings for both the generated voxel captions (from the language model) and the NSD image captions. We then compute the cosine similarity between the voxel caption embeddings and each NSD caption embedding to derive a rough prediction of voxel activity. Finally, we evaluate text-level prediction performance by calculating Spearman’s rank correlation coefficient between these predicted values and the actual voxel responses. (b) We generate voxel images by visualizing voxel captions with an image generation model and, using the same vision model, compute vision-based embeddings for both the generated voxel images and the NSD image stimuli. As in (a), we compute the cosine similarity between voxel-image embeddings and NSD image embeddings and use Spearman’s rank correlation coefficient to evaluate image-level prediction performance.}
  \label{appendix:voxel_pred_method}
\end{figure*}


\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=0.8\textwidth]{figures/all_sentence_cc_mapping.pdf}
  % \vskip -0.05in
  \caption{Mapping of brain activity prediction accuracy at the sentence level for LaVCa (left) and a comparison of brain activity prediction accuracy at the sentence level between LaVCa and BrainSCUBA (right) onto the flatmap for all subjects. The white outlines indicate Visual Word Form Area (VWFA-1) and Occipital Word Form Area (OWFA), which are ranked among the top two Words-category ROIs based on the mean number of voxels across subjects.
}
  \label{appendix:sentence_cc_flatmap}
\end{figure*}

\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=0.8\textwidth]{figures/all_image_cc_mapping.pdf}
  % \vskip -0.05in
  \caption{Mapping of brain activity prediction accuracy at the image level for LaVCa (left) and a comparison of brain activity prediction accuracy at the image level between LaVCa and BrainSCUBA (right) onto the flatmap for all subjects.
}
  \label{appendix:image_cc_flatmap}
\end{figure*}

\clearpage
\begin{table*}[t]
\centering
\caption{Comparison of sentence-level brain activity prediction performance (all subjects). ``Top-N voxels'' refers to the voxels with top-N prediction performance in the training data. The results demonstrate the mean ± standard deviation of prediction performance in the test data for each subject.}
\vskip 0.15in
\definecolor{lavcabg}{HTML}{B5EAD7} % カスタム色の定義
\begin{tabular}{lcccc}
\toprule
\multicolumn{5}{c}{\textbf{Top1000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & -0.010 ± 0.281 & 0.094 ± 0.312 & 0.145 ± 0.331 & 0.013 ± 0.265 \\
BrainSCUBA & 0.291 ± 0.049 & 0.347 ± 0.062 & 0.378 ± 0.068 & 0.267 ± 0.055 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.338 ± 0.051} & \textbf{0.392 ± 0.057} & \textbf{0.420 ± 0.061} & \textbf{0.320 ± 0.060} \\
\midrule
\multicolumn{5}{c}{\textbf{Top3000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.000 ± 0.228 & 0.059 ± 0.255 & 0.099 ± 0.274 & 0.004 ± 0.205 \\
BrainSCUBA & 0.237 ± 0.057 & 0.284 ± 0.067 & 0.305 ± 0.077 & 0.212 ± 0.061 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.280 ± 0.059} & \textbf{0.325 ± 0.068} & \textbf{0.349 ± 0.075} & \textbf{0.253 ± 0.069} \\
\midrule
\multicolumn{5}{c}{\textbf{Top5000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.007 ± 0.199 & 0.058 ± 0.223 & 0.067 ± 0.243 & 0.009 ± 0.175 \\
BrainSCUBA & 0.207 ± 0.062 & 0.251 ± 0.071 & 0.264 ± 0.084 & 0.182 ± 0.065 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.246 ± 0.066} & \textbf{0.287 ± 0.075} & \textbf{0.306 ± 0.084} & \textbf{0.218 ± 0.073} \\
\midrule
\multicolumn{5}{c}{\textbf{Top10000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.008 ± 0.157 & 0.039 ± 0.178 & 0.051 ± 0.192 & 0.012 ± 0.134 \\
BrainSCUBA & 0.159 ± 0.071 & 0.195 ± 0.081 & 0.199 ± 0.095 & 0.134 ± 0.072 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.191 ± 0.077} & \textbf{0.227 ± 0.086} & \textbf{0.237 ± 0.098} & \textbf{0.163 ± 0.081} \\
\bottomrule
\end{tabular}
\label{appendix:TopN_sentence_acc_comparison}
\end{table*}

\clearpage
\begin{table*}[t]
\centering
\caption{Comparison of image-level brain activity prediction performance (all subjects). "Top-N voxels" refers to the voxels with top-N prediction performance in the training data. The results show the mean ± standard deviation of prediction performance in the test data for each subject.}
\vskip 0.15in
\definecolor{lavcabg}{HTML}{B5EAD7} % カスタム色の定義
\begin{tabular}{lllll}
\toprule
\multicolumn{5}{c}{\textbf{Top1000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.022 ± 0.235 & 0.048 ± 0.254 & 0.104 ± 0.273 & 0.036 ± 0.230 \\
BrainSCUBA & 0.278 ± 0.056 & 0.322 ± 0.052 & 0.357 ± 0.057 & 0.262 ± 0.061 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.314 ± 0.059} & \textbf{0.347 ± 0.053} & \textbf{0.379 ± 0.054} & \textbf{0.289 ± 0.060} \\
\midrule
\multicolumn{5}{c}{\textbf{Top3000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.017 ± 0.187 & 0.059 ± 0.210 & 0.087 ± 0.228 & 0.007 ± 0.174 \\
BrainSCUBA & 0.220 ± 0.062 & 0.262 ± 0.063 & 0.291 ± 0.070 & 0.201 ± 0.067 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.248 ± 0.066} & \textbf{0.286 ± 0.063} & \textbf{0.315 ± 0.068} & \textbf{0.221 ± 0.069} \\
\midrule
\multicolumn{5}{c}{\textbf{Top5000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.017 ± 0.163 & 0.052 ± 0.185 & 0.066 ± 0.204 & 0.009 ± 0.148 \\
BrainSCUBA & 0.188 ± 0.067 & 0.226 ± 0.070 & 0.250 ± 0.078 & 0.169 ± 0.069 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.213 ± 0.072} & \textbf{0.249 ± 0.070} & \textbf{0.273 ± 0.079} & \textbf{0.187 ± 0.073} \\
\midrule
\multicolumn{5}{c}{\textbf{Top10000 voxels}} \\
\midrule
Model & subj01 & subj02 & subj05 & subj07 \\
\midrule
Shuffled & 0.010 ± 0.128 & 0.034 ± 0.145 & 0.049 ± 0.159 & 0.006 ± 0.114 \\
BrainSCUBA & 0.139 ± 0.073 & 0.170 ± 0.081 & 0.188 ± 0.090 & 0.122 ± 0.073 \\
\rowcolor{lavcabg} \textbf{LaVCa (Ours)} & \textbf{0.160 ± 0.078} & \textbf{0.191 ± 0.082} & \textbf{0.208 ± 0.092} & \textbf{0.138 ± 0.077} \\
\bottomrule
\end{tabular}
\label{appendix:TopN_image_acc_comparison}
\end{table*}


\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/xaxis_words.pdf}
  \caption{(a) Relationship between voxel caption prediction performance and word count (all subjects). The color of the plot corresponds to the lineage of each model. The numbers associated with LaVCa indicate the number of optimal images used for summarization, while the numbers associated with Concat represent the number of captions for concatenated optimal images. Error bars indicate the standard error.(b) Comparison of actual voxel captions between Concat-50 and LaVCa-50. Only a portion of the captions is depicted for Concat-50.
}
  \label{appendix:xaxis_words_all_sub}
\end{figure*}

\clearpage
\begin{table*}[t]
\centering
\caption{Evaluation of the diversity of three models. PCs (90\% Var) means the number of principal components required to explain 90\% variance of the text embeddings. For intra-voxel comparisons, the mean ± standard deviation across subjects is presented. The inter-subject average (Average) is presented as the mean ± standard error.}
\vskip 0.15in
\small % フォントサイズを小さくする
\scriptsize
\begin{tabular}{@{}llccccccc@{}}
\toprule
 & & & \multicolumn{3}{c}{Inter-voxel} & \multicolumn{3}{c}{Intra-voxel} \\
\cmidrule(lr){4-6}\cmidrule(lr){7-9}
 & & & \multicolumn{1}{c}{Lexical} & \multicolumn{2}{c}{Semantic} & \multicolumn{2}{c}{Lexical} & \multicolumn{1}{c}{Semantic}\\
\cmidrule(lr){4-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}\cmidrule(lr){9-9}
Subject & Model & Acc. & Vocab. size & Variance & PCs (90\% Var) & Vocab. size & Length & Variance \\ 
\midrule
\multirow{3}{*}{subj01} 
 & BrainSCUBA          &  0.207±0.062 & 3400 & 0.0591 &  99  &  6.21±1.27 &  6.32±1.46 & 0.0163±0.0025 \\
 & Top-1 (Ours)     &  0.202±0.064 & 15384 & 0.0640 & 210 &  9.65±3.59 & 10.0±4.24 & 0.0194±0.0027 \\
 & LaVCa (Ours)        &  0.246±0.066 & 16477 & 0.0639 & 218 & 11.0±2.89 & 11.5±3.19 & 0.0198±0.0025 \\ 
\midrule
\multirow{3}{*}{subj02} 
 & BrainSCUBA          &  0.251±0.071 & 3287 & 0.0591 & 133 &  6.17±1.32 &  6.27±1.51 & 0.0162±0.0026 \\
 & Top-1 (Ours)     &  0.251±0.070 & 14135 & 0.0632 & 206 &  9.99±3.65 & 10.5±4.28 & 0.0195±0.0027 \\
 & LaVCa (Ours)        &  0.287±0.075 & 17242 & 0.0639 & 218 & 11.3±3.64 & 11.8±3.93 & 0.0198±0.0027 \\ 
\midrule
\multirow{3}{*}{subj05} 
 & BrainSCUBA          &  0.263±0.084 & 3043 & 0.0583 & 127 &  6.18±1.37 &  6.26±1.52 & 0.0159±0.0027 \\
 & Top-1 (Ours)     &  0.265±0.081 & 13485 & 0.0631 & 206 &  9.99±3.68 & 10.4±4.34 & 0.0195±0.0028 \\
 & LaVCa (Ours)        &  0.306±0.084 & 17459 & 0.0644 & 218 & 11.8±3.88 & 12.2±4.14 & 0.0199±0.0027 \\ 
\midrule
\multirow{3}{*}{subj07} 
 & BrainSCUBA          &  0.182±0.065 & 3042 & 0.0587 & 131 &  6.23±1.30 &  6.36±1.47 & 0.0163±0.0026 \\
 & Top-1 (Ours)     &  0.179±0.066 & 12831 & 0.0632 & 203 & 10.1±3.51 & 10.6±4.14 & 0.0197±0.0026 \\
 & LaVCa (Ours)        &  0.218±0.073 & 16508 & 0.0646 & 222 & 11.6±3.76 & 12.0±4.02 & 0.0202±0.0026 \\
\midrule
\multirow{3}{*}{Average} 
 & BrainSCUBA          
   & 0.226±0.019 & 3193±90 & 0.0588±0.0002 & 123±7.93 
   & 6.20±0.01 & 6.30±0.02 & 0.0162±0.0001 \\
 & Top-1 (Ours) 
   & 0.224±0.020 & 13959±545 & 0.0634±0.0002 & 206±1.44 
   & 9.93±0.10 & 10.4±0.132 & 0.0195±0.0001 \\
 & LaVCa (Ours) 
   & 0.264±0.020 & 16922±252 & 0.0642±0.0002 & 219±1.00 
   & 11.4±0.175 & 11.9±0.149 & 0.0199±0.0001 \\
\bottomrule
\end{tabular}
\label{table:diversity_analysis}
\end{table*}




\clearpage

\begin{table*}[t]
\centering
\caption{The average prediction accuracy for each subject and the inter-subject average prediction accuracy when captions were shuffled within the ROI (Shuffled) and when they were used as-is (Original). For each subject, the average prediction accuracy ± standard deviation is depicted, while for the inter-subject average, the average prediction accuracy ± standard error is presented.}
\vskip 0.15in
{
\begin{tabular}{@{}cc*{4}{r}@{}}
\toprule
& & \multicolumn{2}{c}{\textbf{Body areas}} & \multicolumn{2}{c}{\textbf{Face areas}} \\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}
& & EBA & FBA-2 & OFA & FFA-1 \\
\midrule
\multirow[c]{2}{*}{subj01} & Shuffled & 0.035±0.147 & 0.014±0.128 & 0.031±0.067 & 0.017±0.113 \\
& \textbf{Original} & \textbf{0.169±0.105} & \textbf{0.124±0.102} & \textbf{0.083±0.069} & \textbf{0.117±0.083} \\
\cline{1-6}
\multirow[c]{2}{*}{subj02} & Shuffled & 0.010±0.144 & 0.026±0.109 & 0.036±0.071 & 0.024±0.097 \\
& \textbf{Original} & \textbf{0.158±0.101} & \textbf{0.128±0.103} & \textbf{0.079±0.066} & \textbf{0.105±0.078} \\
\cline{1-6}
\multirow[c]{2}{*}{subj05} & Shuffled & -0.001±0.148 & 0.007±0.148 & 0.017±0.118 & 0.009±0.116 \\
& \textbf{Original} & \textbf{0.152±0.111} & \textbf{0.149±0.114} & \textbf{0.120±0.100} & \textbf{0.112±0.089} \\
\cline{1-6}
\multirow[c]{2}{*}{subj07} & Shuffled & 0.028±0.135 & 0.025±0.096 & 0.027±0.096 & 0.013±0.100 \\
& \textbf{Original} & \textbf{0.149±0.104} & \textbf{0.099±0.099} & \textbf{0.097±0.099} & \textbf{0.108±0.090} \\
\cline{1-6}
\multirow[c]{2}{*}{Average} & Shuffled & 0.018±0.008 & 0.018±0.005 & 0.028±0.004 & 0.016±0.003 \\
& \textbf{Original} & \textbf{0.157±0.005} & \textbf{0.125±0.010} & \textbf{0.095±0.009} & \textbf{0.111±0.003} \\
\midrule
& & \multicolumn{2}{c}{\textbf{Place areas}} & \multicolumn{2}{c}{\textbf{Word areas}} \\
\cmidrule(lr){3-4}\cmidrule(lr){5-6}
& & OPA & PPA & OWFA & VWFA-1 \\
\midrule
\multirow[c]{2}{*}{subj01} & Shuffled & 0.080±0.108 & 0.105±0.107 & 0.015±0.057 & 0.054±0.114 \\
& \textbf{Original} & \textbf{0.163±0.093} & \textbf{0.172±0.099} & \textbf{0.055±0.048} & \textbf{0.147±0.088} \\
\cline{1-6}
\multirow[c]{2}{*}{subj02} & Shuffled & 0.118±0.139 & 0.178±0.147 & 0.037±0.071 & 0.031±0.135 \\
& \textbf{Original} & \textbf{0.204±0.114} & \textbf{0.243±0.139} & \textbf{0.085±0.066} & \textbf{0.150±0.099} \\
\cline{1-6}
\multirow[c]{2}{*}{subj05} & Shuffled & 0.184±0.140 & 0.217±0.153 & 0.028±0.109 & 0.039±0.148 \\
& \textbf{Original} & \textbf{0.260±0.124} & \textbf{0.275±0.149} & \textbf{0.118±0.105} & \textbf{0.177±0.108} \\
\cline{1-6}
\multirow[c]{2}{*}{subj07} & Shuffled & 0.083±0.119 & 0.105±0.108 & 0.020±0.070 & 0.012±0.159 \\
& \textbf{Original} & \textbf{0.175±0.096} & \textbf{0.163±0.106} & \textbf{0.079±0.069} & \textbf{0.157±0.112} \\
\cline{1-6}
\multirow[c]{2}{*}{Average} & Shuffled & 0.116±0.024 & 0.151±0.028 & 0.025±0.005 & 0.034±0.009 \\
& \textbf{Original} & \textbf{0.200±0.022} & \textbf{0.213±0.027} & \textbf{0.084±0.013} & \textbf{0.158±0.007} \\
\bottomrule
\end{tabular}
}
\end{table*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/PPA_caption_vis.pdf}
    \caption{Interpretation of LaVCa captions in PPA. (a) The UMAP projection of text embeddings from captions onto the flatmap of all participants (top). A word cloud of the top 100 most frequent words (middle), with the colors in the word cloud corresponding to the flatmap. A bar graph of the top 10 most frequent words (bottom).
(b) Visualization of the top 2 captions in terms of accuracy for eight clusters on the flatmap (subj07). The generated images corresponding to the captions are displayed to the left or above the captions. Voxels are connected to their corresponding captions and images with lines. The color of the captions and the frames of the generated images represents the average UMAP color of all voxels in the cluster.}
    \label{results:PPA_caption_vis}
\end{figure*}

\clearpage
\begin{figure*}[ht] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj02_OFA_1.pdf}
  \caption{Comparison of voxel captions and voxel images in the OFA voxels of subj02 (1/2).
}
  \label{appendix:OFA_example_1}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj02_OFA_2.pdf}
  \caption{Comparison of voxel captions and voxel images in the OFA voxels of subj02 (2/2).
}
  \label{appendix:OFA_example_2}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj07_PPA_1.pdf}
  \caption{Comparison of voxel captions and voxel images in the PPA voxels of subj07 (1/2).
}
  \label{appendix:PPA_example_1}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj07_PPA_2.pdf}
  \caption{Comparison of voxel captions and voxel images in the PPA voxels of subj07 (2/2).
}
  \label{appendix:PPA_example_2}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj01-02_OFA_UMAP.pdf}
  \caption{Visualization of OFA captions for subj01 and subj02. For each subject, the captions' UMAP representations were mapped onto a flatmap (top). The top 2 captions of each cluster in the UMAP space were visualized (bottom). The horizontal axis represents UMAP2, and the vertical axis represents UMAP2.
}
  \label{appendix:OFA_caps_umap_subj01-02}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj05-07_OFA_UMAP.pdf}
  \caption{Visualization of OFA captions for subj05 and subj07. The captions’ UMAP representations were mapped onto a flatmap (top) for each subject. The top 2 captions of each cluster in the UMAP space were visualized (bottom). The horizontal axis represents UMAP2, and the vertical axis represents UMAP2.
}
  \label{appendix:OFA_caps_umap_subj05-07}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=\textwidth]{figures/subj01-02_PPA_UMAP.pdf}
  \caption{Visualization of PPA captions for subj01 and subj02. The captions’ UMAP representations were mapped onto a flatmap (top) for each subject. The top 2 captions of each cluster in the UMAP space were visualized (bottom). The horizontal axis represents UMAP2, and the vertical axis represents UMAP2.
}
  \label{appendix:PPA_caps_umap_subj01-02}
\end{figure*}

\clearpage
\begin{figure*}[t] % !b により下部への配置を優先
  \centering
  \includegraphics[width=0.95\textwidth]{figures/subj05-07_PPA_UMAP.pdf}
  \caption{Visualization of PPA captions for subj05 and subj07. The captions’ UMAP representations were mapped onto a flatmap (top) for each subject. The top 2 captions of each cluster in the UMAP space were visualized (bottom). The horizontal axis represents UMAP2, and the vertical axis represents UMAP2.
}
  \label{appendix:PPA_caps_umap_subj05-07}
\end{figure*}

