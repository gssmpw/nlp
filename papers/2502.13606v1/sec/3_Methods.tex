\section{Methods}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/LaVCa_method.pdf}
  \vskip 0.1in % キャプション前のスペース
  % \vspace{0.1in}
  \caption{Architecture of LaVCa. (a) We construct a voxel-wise encoding model for a human subject’s brain activity data (measured using fMRI) while viewing images, using CLIP -Vision latent representations. The encoding weight is obtained through ridge regression. (b) We identify the optimal images for a given voxel by calculating the inner product between the CLIP-Vision latent representations of external image datasets and the voxel’s trained encoding weight, selecting the top-N images (the ``optimal image set'') that produce the highest predicted activation. (c) Next, we use a Multimodal LLM (MLLM) to generate captions for each optimal image set, allowing an LLM to interpret them. (d) Finally, we prompt an LLM to extract keywords from the captions, filter these keywords, and feed them into a ``Sentence Composer,'' producing a concise voxel caption.}
  \label{methods:fig1}
    % \vskip -0.1in % キャプション前のスペース
\end{figure*}

\subsection{fMRI dataset}
This study uses the Natural Scenes Dataset (NSD)~\cite{allen2022massive} following the same experimental conditions as in BrainSCUBA. The NSD consists of data collected over 30 to 40 sessions using a 7 Tesla fMRI scanner, with each participant viewing 10,000 images, repeated three times. We analyze data from the four participants (Subject~01, Subject~02, Subject~05, and Subject~07) who completed all imaging sessions.
The images and captions used in NSD are drawn from MS~COCO and resized to $224 \times 224$ pixels to align with the input requirements of the vision models used. We average the brain activity data for each subject across repeated trials of the same image to improve the signal-to-noise ratio. Up to 9,000 images per subject are used as training data, and the remaining 1,000 images are reserved for testing. We use the preprocessed scans with a resolution of $1.8\,\mathrm{mm}$ provided by NSD for the functional data. We use single-trial beta weights estimated via a generalized linear model (GLM) within ROIs. Moreover, we standardize the response of each voxel to have a mean of zero and a variance of one within each session. We use the ROIs provided by NSD, which include early and higher-level (ventral) visual areas and face, place, body, and word-selective regions.


%-------------------------------------------------------------------------

\subsection{LLM-assisted Visual Cortex Captioning (LaVCa)}
We propose a method, \textbf{LaVCa (LLM-assisted Visual Cortex Captioning)}, to automatically generate data-driven natural language captions that characterize each voxel’s selectivity in the visual cortex. LaVCa consists of four stages (Figure \ref{methods:fig1}): 

\begin{enumerate}
    \item Construct voxel-wise encoding models for each subject while they view natural images.
    \item Identify the optimal image set by finding the top-$N$ images that most strongly activate each voxel (according to the trained encoding models).
    \item Generate captions for these optimal images using a Multimodal LLM (MLLM) for summarization by an LLM in the next step.
    \item Derive concise voxel captions by extracting and filtering keywords from the image captions, then feeding these keywords into a ``Sentence Composer.''
\end{enumerate}


\subsubsection{Encoding Model Construction}
First, we construct voxel-wise encoding models to predict each voxel’s activity in response to natural images (Figure \ref{methods:fig1}a). For ease of comparison with BrainSCUBA, we use the projection layer of CLIP’s vision branch \cite{radford2021learning}, using the same pretrained checkpoint used in BrainSCUBA (\ref{appendix:pretrained_ckpts} for details on pretrained model checkpoints). Hereafter, we refer to CLIP's vision branch as ``CLIP-Vision''. Because the code for BrainSCUBA is not publicly available, we implement it in-house. Both the dataset used for the softmax projection and the training approach for the encoding model differ from those in the original paper (\ref{appendix:brainscuba} for details). We estimate the encoding model weights using L2-regularized linear regression on the NSD training set.

\subsubsection{Exploration of Optimal Image Sets for Voxels}
Next, we identify the optimal image set for each voxel (Figure \ref{methods:fig1}b). We compute the inner product between the voxel’s encoding weight and CLIP-Vision latent representations from a large-scale external dataset (distinct from NSD) to obtain predicted voxel responses for each image. We then select the top-$N$ images that generate the highest predicted activation. This process is equivalent to calculating the predicted responses of each voxel for every image. This study uses approximately 1.7 million images from OpenImages-v6~\cite{OpenImages} (the same dataset used in our in-house BrainSCUBA).

\subsubsection{Captioning Optimal Image Sets with MLLM}
To enable an LLM to interpret each voxel’s optimal image set, we first generate captions for these image sets using an MLLM. We use MiniCPM-V~\cite{yao2024minicpmv} with the prompt \textit{``Describe the image briefly.''} For our accuracy evaluation, we also form a simple baseline by concatenating the top-$N$ captions from the optimal image set. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{figures/compare_sim_inflated.pdf}
  \vskip 0.1in
  \caption{Mapping of brain activity prediction accuracy (subj01). (a) The sentence-level prediction performance is projected onto inflated cortical surfaces
  (top: lateral, medial, and dorsal views) and flattened cortical surfaces (bottom, with the occipital areas at the center) for both hemispheres. Voxels with significant prediction performance are color-coded (all colored voxels $P<0.05$, FDR corrected). The white outlines indicate the ROIs that are among the top two in terms of the total voxel count across subjects for each semantic category—Body (Extra Striate Body Area; EBA, and Fusiform Body Area; FBA-2), Face (Fusiform Face Area; FFA-1, and Occipital Face Area; OFA), and Places (Parahippocampal Place Area; PPA, and Occipital Place Area; OPA). Word areas are shown in Figure \ref{appendix:sentence_cc_flatmap}. (b) A comparison of sentence-level prediction performance between our method, LaVCa, and the existing method, BrainSCUBA on the flattened cortical surface. If only one model exhibits significant prediction performance for a given voxel, the other model's performance for that voxel is set to zero and color-coded accordingly.}
  \label{results:accuracy_mapping}
  % \vskip -0.05in
\end{figure*}

\subsubsection{Generating Voxel Captions}
Finally, we generate interpretable voxel captions from the image captions. First, we use an LLM to extract common keywords across the captions within each voxel’s optimal image set (Figure~\ref{methods:fig1}d). Following the in-context learning prompt approach from~\cite{dunlap2024describing}, we extract multiple keywords from the caption sets using an LLM (\ref{appendix:prompt} for the prompt). We use \textit{gpt-4o} (gpt-4o-2024–08–06 in the OpenAI API) as the LLM. In \ref{appendix:ablation}, we investigate the effects of several hyperparameters on accuracy, including the number of images in the optimal image sets, the number of extracted keywords, and the type of keywords extractor.
To remove irrelevant or noisy keywords, we compute the cosine similarity between each keyword’s embedding from CLIP' text branch (prompted as \textit{``A photo of \{keyword\}.''}) and the encoding weight for that voxel, then apply a softmax threshold to retain only sufficiently relevant keywords. Hereafter, we refer to CLIP's text branch as ``CLIP-Text''.
Next, we transform these filtered keywords into a sentence-level caption using the ``Sentence Composer'' from MeaCap~\cite{zeng2024meacap}, initially designed to generate image captions from keyword sets. 
MeaCap can generate a caption by inputting the target image’s keywords into the Sentence Composer while referencing similarities to the image features.
In this study, we replace image features with encoding weights so that the model composes a coherent sentence from the voxel-specific keywords (for details, see Section~\ref{appendix:meacap}).



\subsection{Caption Evaluation}

\subsubsection{Brain Activity Prediction at Sentence Level}
We predict brain activity based on sentence similarity to assess how accurately voxel captions describe voxel selectivity (Figure \ref{appendix:voxel_pred_method}a). We hypothesize that a voxel caption capturing the true selectivity of a voxel should be more similar to the corresponding caption of an NSD image that strongly activates that voxel and less similar otherwise. Following~\cite{singh2023explainingblackboxtext}, we:
\begin{enumerate}
    \item Use a pretrained Sentence-BERT to compute text embeddings for each voxel caption and each NSD image caption.
    \item Compute the cosine similarity between the voxel caption embedding and each NSD image caption embedding.
    \item Treat this similarity value as the predicted activity for that voxel on that image.
\end{enumerate}
We then calculate Spearman’s rank correlation coefficient ($\rho$) between these predicted values (sentence similarities) and the actual voxel activities. Statistical significance (one-tailed) is determined by comparing the observed correlation to a null distribution of correlations from two independent Gaussian random vectors of the same length, using a threshold of \textit{P}~$<$~0.05, followed by False Discovery Rate (FDR) correction.


\subsubsection{Brain Activity Prediction at Image Level}
Because sentence-based evaluation can be influenced by non-visual linguistic features (e.g., sentence length), we also assess voxel selectivity using \textit{image} similarity (Figure \ref{appendix:voxel_pred_method}b). We use FLUX.1-schnell to create a \textit{voxel image} and then compute vision embeddings (via CLIP-Vision) for both the generated voxel image and each NSD trial image. We obtain an image-level metric of predicted brain activity by comparing these embeddings, focusing purely on visual content.