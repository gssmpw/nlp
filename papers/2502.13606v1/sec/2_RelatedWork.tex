\section{Related Work}

Encoding models have been widely used in neuroscience to interpret neural representations \cite{kay2008identifying, nishimoto2011reconstructing, naselaris2011encoding, huth2012continuous}. These studies typically use interpretable low-level visual features and high-level semantic features, such as one-hot encodings, permitting the interpretation of individual voxels.

Encoding models that rely on features extracted from DNNs can explain brain activity more effectively than those using simpler, more interpretable features~\cite{gucclu2015deep, schrimpf2021neural, takagi2023high, antonello2024scaling}; however, their complexity complicates voxel-level interpretation. Consequently, several methods aggregate all voxels into a limited number of universal and interpretable axes~\cite{huth2016natural, lescroart2019human, nakagi2024unveiling}.

More recently, to enable finer analyses of neural representations at the voxel level, data-driven approaches have been proposed to generate natural-language descriptions of voxel selectivity~\cite{luo2023brainscuba, singh2023explainingblackboxtext}. BrainSCUBA~\cite{luo2023brainscuba}, for example, is an end-to-end method that uses an existing image captioning model to produce voxel-wise captions for the visual cortex. Similarly, SASC~\cite{singh2023explainingblackboxtext} uses an fMRI dataset recorded during speech listening~\cite{lebel2023natural}, uses an LLM to merge multiple short phrases—those with the highest predicted voxel activations—into a single, data-driven caption, thus describing the semantic properties of voxels.

Our proposed method also generates data-driven voxel captions but differs in several ways. First, BrainSCUBA is constrained to pre-existing, end-to-end image captioning models. In contrast, our approach divides the task into (i) identifying an optimal set of images and (ii) converting these images into a caption, allowing us to use any vision model aligned with language and any LLM with advanced language capabilities without requiring specialized fine-tuning. Furthermore, although SASC uses an LLM to create voxel captions, it primarily synthesizes short, low-information phrases (e.g., trigrams), producing only simple keyword-based captions. In contrast, our method summarizes more diverse and informative text and then uses these extracted keywords to compose a complete sentence, capturing a richer range of voxel-level properties.