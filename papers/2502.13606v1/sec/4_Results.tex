\section{Results}

\definecolor{lavca_bg}{HTML}{B5EAD7} % LaVCaの背景色を定義

\begin{table*}[ht]
\centering
\caption{Comparison of brain activity prediction accuracy at the sentence and image levels. For each subject, the mean and standard deviation of accuracy on the test data are displayed for the top 5,000 voxels with the highest accuracy on the train data.}
\vskip 0.15in
\small
{\scriptsize
\begin{tabular}{l@{\hspace{8pt}} cccc@{\hspace{8pt}} c@{\hspace{8pt}} cccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{4}{c}{\textbf{Sentence}} & & \multicolumn{4}{c}{\textbf{Image}} \\
\cline{2-5} \cline{7-10}
& subj01 & subj02 & subj05 & subj07 & & subj01 & subj02 & subj05 & subj07 \\
\hline
Shuffled & 0.007 ± 0.199 & 0.058 ± 0.223 & 0.068 ± 0.243 & 0.009 ± 0.175 & & 0.017 ± 0.163 & 0.052 ± 0.185 & 0.066 ± 0.204 & 0.009 ± 0.149 \\
BrainSCUBA & 0.207 ± 0.062 & 0.251 ± 0.071 & 0.264 ± 0.084 & 0.182 ± 0.065 & & 0.188 ± 0.067 & 0.226 ± 0.070 & 0.250 ± 0.078 & 0.169 ± 0.069 \\
\rowcolor{lavca_bg} \textbf{LaVCa (Ours)} & \textbf{0.246 ± 0.066} & \textbf{0.287 ± 0.075} & \textbf{0.306 ± 0.084} & \textbf{0.218 ± 0.073} & & \textbf{0.213 ± 0.072} & \textbf{0.250 ± 0.070} & \textbf{0.273 ± 0.079} & \textbf{0.187 ± 0.073} \\
\hline
\end{tabular}}
\label{table:prediction_accuracy_combined}
    % \vskip -0.1in
\end{table*}


\subsection{Voxel Activity Prediction}
To determine whether the generated captions accurately describe the properties of the voxels, we evaluate brain activity prediction at both the sentence and image levels. First, we map sentence-level prediction performance across inflated and flattened cortical surfaces (Figure \ref{results:accuracy_mapping}a). These maps illustrate that LaVCa captions significantly predict voxel activity throughout the visual cortex ($P<0.05$, FDR corrected). See Figure \ref{appendix:sentence_cc_flatmap} for the results of all subjects.

Next, we compare LaVCa, the existing method BrainSCUBA, and a shuffled variant (LaVCa captions shuffled across voxels) at both the sentence and image levels, focusing on the top 5,000 voxels with the highest accuracy on the training data (Table \ref{table:prediction_accuracy_combined}). Our proposed method, LaVCa, outperforms BrainSCUBA ($P<0.05$, paired t-test). This finding indicates that generating captions based on multiple keywords extracted from an optimal set of images provides a more accurate explanation of voxel selectivity. Furthermore, the marked drop in accuracy for the shuffled condition compared with the original LaVCa confirms that our evaluation metric efficiently gauges whether captions describe voxel selectivity. Results for the top 1,000, 3,000, and 10,000 voxels appear in Table \ref{appendix:TopN_sentence_acc_comparison} and \ref{appendix:TopN_image_acc_comparison}. After visualizing sentence-level prediction accuracy across the cortex, we find that LaVCa exceeds BrainSCUBA’s performance throughout the visual cortex (Figure \ref{results:accuracy_mapping}b). See Figure \ref{appendix:image_cc_flatmap} for the results of all subjects.

Finally, we examine whether LaVCa can generate concise and interpretable voxel captions without losing critical information in each voxel’s optimal image set. We compare two approaches from the perspective of interpretability by varying the number of optimal images used by LaVCa (Top-$N$) and by simply concatenating the captions of the optimal images (Concat-$N$). Figure~\ref{results:xaxis_num_words} plots prediction accuracy against the average caption length on the horizontal axis, highlighting the trade-off between accuracy and interpretability. Concat-$N$ achieves better accuracy as $N$ increases (up to $N=10$) but at the cost of a much longer caption, which can reduce interpretability. In contrast, LaVCa merges information across the optimal image set into a concise summary, retaining interpretability even as $N$ grows and reaching accuracy comparable to Concat-$N$. Results for all participants are provided in Figure~\ref{appendix:xaxis_words_all_sub}.


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/OFA_caption_vis.pdf}
    \vskip 0.1in
    \caption{Interpretation of LaVCa captions in OFA. (a) The UMAP projection of caption text for all participants, visualized on a flatmap (top). A word cloud of the 100 most frequent words in these captions (middle), colored according to their location in the UMAP space. A bar graph of the top 10 most frequent words (bottom).
(b) Visualization of the top two captions (by accuracy) for eight clusters on the flatmap (subj02). The images generated for each caption appear to the left or above the text. Voxels are connected to their corresponding captions and images by lines. The color of each caption and image border reflects the average UMAP color of all voxels in the cluster.}
    \label{results:OFA_caption_vis}
\end{figure*}

\subsection{Lexical and Semantic Diversity Analysis}
We next assess how effectively LaVCa captions capture both lexical and semantic diversity across voxels, focusing first on \textit{inter-voxel} diversity (Table \ref{table:diversity_analysis}, left). For this quantitative evaluation, we use three metrics: (1) the total vocabulary size (excluding stop-words) across all voxel captions (Lexical); (2) the average variance across each dimension of the CLIP-Text embedding computed on all voxel captions (Semantic); and (3) the number of principal components (PCs) required to capture 90\% of the variance of CLIP-Text embedding across captions in a principal component analysis (PCA; Semantic). 

First, we evaluate the diversity of LaVCa captions compared with the existing method, BrainSCUBA. When averaged across subjects, LaVCa markedly outperforms BrainSCUBA in both lexical (16,922 vs. 3,193 in vocab. size) and semantic (0.0642 vs. 0.0588 in variance of embeddings; 219 vs. 127 in PCs required for 90\% variance explained) diversity. These findings confirm that our open-ended LLM–based approach can produce richer word usage and more meaningful captions across inter-voxel comparisons. 

We evaluate the diversity of LaVCa captions compared with more detailed captions. BrainSCUBA leverages ClipCap \cite{mokady2021clipcap}, a model that produces relatively simple image captions. We use the top-1 captions generated by the MLLM on the optimal image sets (equivalent to the case where $N=1$ in Concat-$N$) to compare the diversity of LaVCa with more detailed captions. When averaged across subjects, Top-$1$ (13,959 vocab. size, 0.0638 avg. variance, 210 PCs) exhibits both a vocabulary range and semantic diversity close to LaVCa. However, LaVCa achieves a higher prediction accuracy (0.264 vs. 0.224), indicating that LaVCa can preserve robust brain activity prediction performance while enhancing the diversity of generated captions.

Next, we evaluate diversity from an \textit{intra-voxel} perspective by comparing captions generated by three models in both lexical and semantic dimensions (Table \ref{table:diversity_analysis}, right). We use three metrics: (1) the vocabulary size of each voxel’s caption (Lexical), (2) the average sentence length in each voxel’s caption (Lexical), and (3) the average variance across all dimensions of Word2Vec embeddings of each caption’s words (excluding stop-words) (Semantic). When averaged across subjects, LaVCa markedly outperforms BrainSCUBA in both lexical (11.4 vs. 6.09 in vocab. size) and semantic (11.9 vs. 6.19 in avg. length; 0.0199 vs. 0.0160 in variance of semantic embeddings) diversity. This improvement suggests that LaVCa more precisely captures the fine-grained intra-voxel characteristics. 

For examples of voxel captions and images from various OFA and PPA voxels—along with their corresponding quantitative metrics—compared across three models (LaVCa, BrainSCUBA, Top-1), see Figures \ref{appendix:OFA_example_1}, \ref{appendix:OFA_example_2}, \ref{appendix:PPA_example_1}, and \ref{appendix:PPA_example_2}.


\begin{table*}[t]
\centering
\caption{Average prediction accuracy with standard error across subjects when captions within each ROI are shuffled (Shuffled) versus used as is (Original).}
\vskip 0.15in
\small
{\scriptsize
\begin{tabular}{c*{8}{c}}
\toprule
& \multicolumn{2}{c}{\textbf{Body areas}} & \multicolumn{2}{c}{\textbf{Face areas}} & \multicolumn{2}{c}{\textbf{Place areas}} & \multicolumn{2}{c}{\textbf{Word areas}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
& EBA & FBA-2 & OFA & FFA-1 & OPA & PPA & OWFA & VWFA-1 \\
\midrule
Shuffled & 0.018±0.008 & 0.018±0.005 & 0.028±0.004 & 0.016±0.003 & 0.116±0.024 & 0.151±0.028 & 0.025±0.005 & 0.034±0.009 \\
\rowcolor{lavca_bg}
\textbf{Original} & \textbf{0.157±0.005} & \textbf{0.125±0.010} & \textbf{0.095±0.009} & \textbf{0.111±0.003} & \textbf{0.200±0.022} & \textbf{0.213±0.027} & \textbf{0.084±0.013} & \textbf{0.158±0.007} \\
\bottomrule
\end{tabular}
}
\label{results:ROI_caption_shuffle}
% \vskip -0.1in
\end{table*}


\subsection{ROI-level Diversity Analysis}
Our results thus far illustrate that LaVCa generates more accurate voxel captions than BrainSCUBA and better reflects inter- and intra-voxel diversity. We now assess how LaVCa captions capture diversity within known ROI, such as the OFA  and PPA. These areas are conventionally associated with specific concepts (e.g., faces or places), \cite{gauthier2000fusiform, haxby2000distributed, epstein1998cortical} yet the extent of diversity within these ROIs remains unclear. We conduct a quantitative evaluation using LaVCa’s captions and generated images to analyze diversity that exists beyond the known selectivity in the ROI (Figures \ref{results:OFA_caption_vis} and \ref{results:PPA_caption_vis}).

\paragraph{Quantitative Assessment.}
We determine how many distinct captions appear in each ROI by comparing the sentence-level prediction accuracy of each ROI when captions are maintained in their original form versus shuffled within the ROI (Figure \ref{results:ROI_caption_shuffle}). For each category (body, face, place, and word area), we select two ROIs with the largest total voxel count across all subjects, resulting in eight ROIs in total. In all ROIs, shuffling reduces prediction accuracy significantly. For example, in the OFA, accuracy drops from 0.0945 (Original) to 0.0280 (Shuffled), a 3.3-fold decrease; in the PPA, accuracy falls from 0.213 (Original) to 0.151 (Shuffled), a 1.4-fold decrease. Thus, even in regions traditionally linked to particular concepts, voxels exhibit a range of distinct selectivities.

\paragraph{Qualitative Assessment.}
We next explore the semantic diversity of LaVCa captions in the OFA by applying UMAP to their CLIP-Text embeddings and visualizing the resulting distributions on a flatmap (Figure~\ref{results:OFA_caption_vis}a, top). Each subject’s OFA illustrates a broad spectrum of UMAP colors, indicating multiple meaningful clusters within an ROI known for face-selective responses. See Figure \ref{results:PPA_caption_vis} for the results of the PPA.

To examine the word-level patterns within ROIs, we construct a word cloud from the top 100 most frequent words and a histogram of the top 10 words (Figure \ref{results:OFA_caption_vis}a, middle and bottom). 
We align the word cloud with the flatmap colors by obtaining the CLIP-Text embeddings for each word using the prompt ``A photo of {word}.'' We then apply the UMAP learned from the CLIP-Text embeddings of the captions to convert the top 100 words into the same UMAP space. While high-frequency words like ``child,'' ``people,'' and ``animal'' align with face or person-related content, the presence of terms such as ``food'' or ``sign'' suggests more diverse encoding.

Finally, to highlight how each caption and its corresponding voxel image relate to specific colors in semantic space, we project them onto a flatmap (Figure \ref{results:OFA_caption_vis}b). We divide the samples into eight clusters by labeling each of the three UMAP dimensions as ``High'' ($\geq2/3$) or ``Low'' ($\leq1/3$). From each cluster, we pick the two voxels with the highest prediction accuracy (or one if only one qualifies, or none if none qualify) and illustrate their captions and generated images.

In OFA, some captions are related to faces (e.g., ``face,'' ``person,'' ``animal''), while particular voxels encoded more fine-grained features such as ``eye,'' ``tongue,'' or ``smiling,'' and other voxels encoded information like ``animal,'' ``bear,'' or ``cardinal.'' Thus, even within this ROI, there appears to be substantial functional differentiation among inter-voxel that extends beyond a generic ``face'' category.
Moreover, we observe \textit{intra-voxel} diversity, where a single caption incorporates multiple ideas (e.g., \textit{``A food packaging features a smiling person and a cartoon character''}), suggesting that individual voxels can simultaneously encode several distinct concepts. 
These findings highlight the fine-grained functional specialization across inter-voxel within the ROI and the diverse nature of intra-voxel encoding beyond singular concepts. 
The results for all participants, visualizing the top two captions for each cluster directly in the UMAP space, can be found in Figures \ref{appendix:OFA_caps_umap_subj01-02}, \ref{appendix:OFA_caps_umap_subj05-07}, \ref{appendix:PPA_caps_umap_subj01-02}, and \ref{appendix:PPA_caps_umap_subj05-07}. 









