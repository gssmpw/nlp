In this section, we present the definition of vertical federated learning (VFL) in Section~\ref{subsec:vfl-def} and provide a comprehensive overview of the VFL pipeline. Distinct from previous surveys \cite{liu2024vertical}, which define VFL through the partitioning of a global dataset, we employ a more generalized definition that accounts for the distribution of real-world data.

\subsection{Definition of VFL}\label{subsec:vfl-def}
Consider a collaboration involving \( C \) parties, each possessing a unique and heterogeneous dataset. The dataset of party \( c \) is defined as \( \mathbf{X}^c = \{\mathbf{K}^c, \mathbf{D}^c\} \), where \(\mathbf{K}^c \) denotes the \textit{keys} and \(\mathbf{D}^c \) denotes the \textit{data} associated with party \( c \). Here, the keys \( \mathbf{K}^c \in \mathbb{R}^{n_c \times k} \) are \( k \)-dimensional features shared across all parties, while the data \( \mathbf{D}^c \in \mathbb{R}^{n_c \times d_c} \) represents features specific to each party \( c \).


A VFL task involves multiple parties collaboratively training a machine learning model on the combined datasets \(\{\mathbf{X}^1, \mathbf{X}^2, \ldots, \mathbf{X}^C\}\) while ensuring the privacy of both keys and data. This survey focuses on the widely studied supervised learning scenario where one party possesses the labels, referred to as the \textit{primary party}. The other collaborating parties are termed \textit{secondary parties}. Without loss of generality, we designate $\mathbf{X}^1$ as the primary party. Formally, the VFL task optimizes the following objective function:
\begin{equation}\label{eq:vfl-obj}
\min_{\theta} \frac{1}{n_1} \sum_{i=1}^{n_1} \mathcal{L}(f(\theta; \mathbf{x}_i^1, \mathbf{X}^2, \ldots, \mathbf{X}^C), y_i),
\end{equation}
where $n_1$ is the number of records in the primary party, $\theta$ denotes the model parameters, $\mathcal{L}$ is the loss function, $f$ represents the model, $\mathbf{x}_i^1$ is the $i$-th record from the primary party, and $y_i$ is the label associated with $\mathbf{x}_i^1$.



\subsection{Pipeline of VFL}
The VFL pipeline consists of two main components: \textit{privacy-preserving record linkage} and \textit{VFL training}. The privacy-preserving record linkage component aligns the keys across different parties while safeguarding their privacy. Leveraging this alignment information, the VFL training component collaboratively trains a model on the combined datasets from all parties in a manner that preserves data privacy.




\paragraph{Privacy-Preserving Record Linkage (PPRL).} Privacy-Preserving Record Linkage (PPRL) encompasses a set of techniques designed to align keys across different parties while ensuring the privacy of these keys, such as private set intersection (PSI) \cite{morales2023private}. Formally, given keys $\{\mathbf{K}^c\}_{c=1}^C$ from $C$ parties, PPRL outputs a row selection function $\phi^c$ for each party $c$ such that, for all $c$, each row of $\phi^c(\mathbf{X}^c) \in \mathbb{R}^{n_1 \times m}$ represents the same data instance. In most studies, the row selection function $\phi^1$ for the primary party is typically a constant function, while $\phi^c$ for secondary parties aligns $\mathbf{X}^c$ with the primary party. In the ideal scenario of precise alignment, $\phi^c$ corresponds to multiplying a row permutation matrix. However, in other cases, $\phi^c$ may output a subset of rows or include duplicate rows. Further details will be discussed in Section~\ref{sec:vfl-keys}.



 





\paragraph{VFL Training.} VFL training refers to the process of collaboratively training a model on aligned datasets without sharing raw data. Formally, building on top of PPRL, VFL training optimizes the following objective function:
\begin{equation}\label{eq:vfl-training-obj}
\min_{\theta} \frac{1}{n_1} \sum_{i=1}^{n_1} \mathcal{L}(f(\theta; \mathbf{x}_i^1, \phi^2(\mathbf{X}^2), \ldots, \phi^C(\mathbf{X}^C)), y_i),
\end{equation}
where $\phi^c$ is the row selection function output by the PPRL component for party $c$. VFL training can be accomplished through various methods. A prevalent approach is split learning, where parties collaborate by exchanging gradients and representations \cite{wang2024unified,nock2021impact}. Alternatively, some methods enable each party to maintain the full set of model parameters and collaborate through boosting strategies \cite{diao2022gal,xian2020assisted}. 





