@inproceedings{abdelnabi2022open,
  title={Open-domain, content-based, multi-modal fact-checking of out-of-context images via online resources},
  author={Abdelnabi, Sahar and Hasan, Rakibul and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14940--14949},
  year={2022}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{chen2024lion,
  title={Lion: Empowering multimodal large language model with dual-level visual knowledge},
  author={Chen, Gongwei and Shen, Leyang and Shao, Rui and Deng, Xiang and Nie, Liqiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26540--26550},
  year={2024}
}

@inproceedings{dai2023instructblip,
  title={InstructBLIP: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
  booktitle={International Conference on Neural Information Processing Systems},
  pages={49250--49267},
  year={2023}
}

@article{gu2024learning,
  title={Learning Domain-Invariant Features for Out-of-Context News Detection},
  author={Gu, Yimeng and Zhang, Mengqi and Castro, Ignacio and Wu, Shu and Tyson, Gareth},
  journal={arXiv preprint arXiv:2406.07430},
  year={2024}
}

@inproceedings{jin2024llava,
  title={LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description},
  author={Jin, Yizhang and Li, Jian and Zhang, Jiangning and Hu, Jianlong and Gan, Zhenye and Tan, Xin and Liu, Yong and Wang, Yabiao and Wang, Chengjie and Ma, Lizhuang},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={11420--11425},
  year={2024}
}

@inproceedings{kuckreja2024geochat,
  title={Geochat: Grounded large vision-language model for remote sensing},
  author={Kuckreja, Kartik and Danish, Muhammad Sohail and Naseer, Muzammal and Das, Abhijit and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={27831--27840},
  year={2024}
}

@inproceedings{liu2024forgery,
  title={Forgery-aware adaptive transformer for generalizable synthetic image detection},
  author={Liu, Huan and Tan, Zichang and Tan, Chuangchuang and Wei, Yunchao and Wang, Jingdong and Zhao, Yao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10770--10780},
  year={2024}
}

@inproceedings{luo2021newsclippings,
  title={NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media},
  author={Luo, Grace and Darrell, Trevor and Rohrbach, Anna},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021)},
  pages={6801--6817},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{papadopoulos2023synthetic,
  title={Synthetic misinformers: Generating and combating multimodal misinformation},
  author={Papadopoulos, Stefanos-Iordanis and Koutlis, Christos and Papadopoulos, Symeon and Petrantonakis, Panagiotis},
  booktitle={Proceedings of the 2nd ACM International Workshop on Multimedia AI against Disinformation},
  pages={36--44},
  year={2023}
}

@inproceedings{qi2024sniffer,
  title={SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection},
  author={Qi, Peng and Yan, Zehong and Hsu, Wynne and Lee, Mong Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13052--13062},
  year={2024}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{wang2024mmidr,
  title={MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation},
  author={Wang, Longzheng and Xu, Xiaohan and Zhang, Lei and Lu, Jiarui and Xu, Yongxiu and Xu, Hongbo and Zhang, Chuang},
  journal={arXiv preprint arXiv:2403.14171},
  year={2024}
}

@article{yang2024gpt4tools,
  title={Gpt4tools: Teaching large language model to use tools via self-instruction},
  author={Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yuan2023support,
  title={Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis-and Disinformation},
  author={Yuan, Xin and Guo, Jie and Qiu, Weidong and Huang, Zheng and Li, Shujun},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023)},
  pages={4268--4280},
  year={2023}
}

@inproceedings{zhang2023aligning,
  title={Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors},
  author={Zhang, Kai and Gutierrez, Bernal Jimenez and Su, Yu},
  booktitle={Proceedings of the 61st Annual Meeting of the Association For Computational Linguistics},
  year={2023}
}

@inproceedings{zhang2023ecenet,
  title={ECENet: Explainable and Context-Enhanced Network for Muti-modal Fact verification},
  author={Zhang, Fanrui and Liu, Jiawei and Zhang, Qiang and Sun, Esther and Xie, Jingyi and Zha, Zheng-Jun},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={1231--1240},
  year={2023}
}

