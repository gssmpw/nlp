\subsection{Task Definition and Background}

Given an authentic image $v$ and its claim $t$ from a dataset $\mathcal{D}$, where the image-claim pair $(v,t)\in\mathcal{D}$ achieves a multitude of evidence $(v_e, t_e)$ gathered by~\cite{abdelnabi2022open}, the task of OOC misinformation detection is to acquire a model $f_{ooc}(v,t,v_e, t_e)$ that makes a prediction $\hat{y}$ regarding the authenticity of the image-claim pair. Each pair is assigned with a semantic label $y\in\{0,1\}$, being either the ``Falsified'' (out-of-context) $(1)$ or ``Pristine'' (not out-of-context) $(0)$.

A typical approach to this issue is to train a classifier that provides a probability $p_{ooc}=p(\hat{y}|v,t,v_e, t_e)$. A more attractive solution taken by Qi \etal~\cite{qi2024sniffer} is to fine-tune the general-domain LVLM $g(v,t,v_e, t_e;\theta)$ on this task-specific dataset, thereby yielding the token $\mu$ of the corresponding semantic label. The symbol $\theta$ represents the learnable parameters of the LVLM model during training.

In the context of LVLMs, existing methods for discerning multimodal OOC misinformation typically default to the use of all the evidence $v_e$, $t_e$ of the image $v$ and claim $t$, but for image-evidence, the long distance between them inevitably prevents the model's discriminatory powers in the sense. Upon investigating the realm of OOC, we summarize the following findings:

\begin{enumerate}[label=\arabic*.]
    \item The fine-tuning model should not require access to all the evidence regarding image-claim pairs.
    \item The LVLMs-based model should require coherent and contextually attuned content, rather than pieces of external information.
    \item Similarly, the model should require both judgment and explanation, because this supervised signal contributes to the model's discriminatory powers.
\end{enumerate}

\subsection{Textual Evidence Reranking and Rewriting}

To rerank the gathered textual evidence of authentic images for acquiring relevant items, the most intuitive solution is to directly depend on the cosine similarity to calculate the relevance between the image $v$ and textual evidence $v_e=\{v^1_e, v^2_e,...,v^k_e\}$, \ie, $\mathrm{argsort}\ sim(\mathbf{z}_v, \mathbf{z}_{v^k_e})$, where $sim(\cdot)$ represents the cosine similarity, $\mathbf{z}_v\in\mathbb{R}^{1\times dim}$ and $\mathbf{z}_{v^k_e}\in\mathbb{R}^{1\times dim}$ denote the $dim$-dimensionality representations of the image and $k$-th evidence, respectively. However, this solution seriously lies in representation quality extracted by chosen backbone encoders (\eg, ViT B/32~\cite{radford2021learning} and ViT L/14~\cite{li2023blip}), and suffers from complex context. This is a sub-optimal option. To overcome this issue, we tend to multimodal understanding and reranking capabilities of LVLMs, and devise a textual evidence reranking strategy.

Textual evidence reranking refers to adopting the LVLM model Qwen2-VL~\cite{wang2024qwen2} to select one most relevant textual evidence $v^r_e$ related to the image. Due to the length extrapolation capability of Qwen2-VL, we incorporate the authentic image and its retrieved textual evidence as the part of the reranking prompt, denoted as $\mathcal{P}_\mathrm{rerank}(v, v_e)$. To finely control the output format, we attach a simple yet effective demonstration to the corresponding prompt. We present this prompt and its example in subfigure (a) of \Cref{fig:3}.

Further, in order to alleviate the discrepancy between the selected textual evidence $v^r_e$ and natural language-based LVLMs for alignment, we design a textual evidence rewriting strategy. Similar to the input prompt of the LVLM model for rerank, we leverage $(v, v^r_e)$ as part of the rewriting prompt, denoted as $\mathcal{P}_\mathrm{rewrite}(v, v^r_e)$, to guide the model for generating coherent and attuned content $\widetilde{v}^r_e$. Such prompt and its example are depicted in subfigure (b) of \Cref{fig:3}.

Additionally, we observe the fact that access to textual evidence of images is not invariably retrieved by search engines. In response to the lack of such textual evidence in the OOC realm, we directly employ the LVLM model to produce image captions as the rewritten content $\widetilde{v}^r_e$.