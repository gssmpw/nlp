\section{Methodology}
\label{sec:Methodology}

In this section, we present the details of the Evidence-Enhanced Large Vision-Language Model (E2LVLM). The proposed E2LVLM endeavors to utilize textual evidence related to authentic images for debunking multimodal OOC misinformation. The framework is illustrated in \Cref{fig:2}.

%---------------------------------------
\begin{figure*}
  \centering
      \includegraphics[width=\linewidth]{fig3.jpg}
    \caption{Prompts and their examples in E2LVLM. (a) Reranking prompt $\mathcal{P}_\mathrm{rerank}$ is to select one most relevant textual evidence related to the authentic image. (b) Rewriting prompt $\mathcal{P}_\mathrm{rewrite}$ is to achieve the coherent and contextually attuned content for alignment. (c) Explanation prompt $\mathcal{P}_\mathrm{Expla.}$ is to generate the compelling rationale to make up support for its assessment. (d) Tuning prompt $\mathcal{P}_\mathrm{OOC}$ is to extend the general-purpose LVLM to the task of multimodal out-of-context misinformation detection.}
    \label{fig:3}
\end{figure*}

\input{3.1_TDB}
% \input{3.2_TER2}
\input{3.3_OOC_MID}
\input{3.4_E2FT}