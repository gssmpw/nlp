\subsection{Evidence-enhanced Fine-tuning}

Regarding the identified challenge in the LVLM model Qwen2-VL~\cite{wang2024qwen2} when it comes to the OOC detection task, we suggest a one-stage multimodal instruction tuning solution extending the general-purpose LVLM, to the news domain for discerning multimodal OOC misinformation.

Our evidence-enhanced fine-tuning strategy is to provide both judgments and explanations for each image-claim pair. Simply put, we introduce both questions and candidate answers~\cite{shao2023prompting} into the tuning prompt $\mathcal{P}_\mathrm{OOC}(v,t,\widetilde{v}^r_e,{t}^r_e)$ that serves as informative inputs of the model. This makes the LVLM model unleash the potential knowledge behind itself~\cite{liu2024fka}, which explicitly analyzes the discrepancy between candidate answers for more accurate decisions. An illustration of the model response is shown in subfigure (d) of \Cref{fig:3}. For an image-claim pair with the ``Falsified'' label, we utilize the image $v$, claim $t$, rewritten textual evidence $\widetilde{v}^r_e$, and reranked visual evidence ${t}^r_e$ to format the input prompt of the LVLM model for response generation. The model provides the response likewise ``$<$Judgment$>$ $<$Explanation$>$'', where $<$Judgment$>$ indicates the symbol (\ie, ``No'') associated with candidate answers, and $<$Explanation$>$ is a coherent sentence serving as the compelling rationale for supporting its assessment.

In order to align the initial training way of the LVLM model, we employ LoRA technology~\cite{hu2022lora} and the next token prediction loss for assessing the modelâ€™s output error. Therefore, the learning objective of the proposed model E2LVLM over the OOC task, which can be described as,
\begin{equation}\label{eq:vcg_5}
    \mathcal{L}_{\mathrm{ooc}} = \sum_{i=1}^{N} -\log P(\epsilon_i \mid v_i,t_i,\widetilde{v_i}^r_e, {t_i}^r_e, \theta_{\mathrm{ooc}}),
\end{equation}
where, $\epsilon_i$ is considered as the generated tokens of both judgment and explanation corresponding to the formatted input $(v_i,t_i,\widetilde{v_i}^r_e,{t_i}^r_e)$, and $\theta_{\mathrm{ooc}}$ refers to the learnable parameters of E2LVLM during training. Besides, $N$ is the size of $\widetilde{\mathcal{D}}$.