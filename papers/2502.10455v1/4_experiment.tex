\section{Experiment}
\label{sec:Experiment}

In accordance with existing multimodal out-of-context misinformation detection methods \cite{abdelnabi2022open, qi2024sniffer}, we conduct a multitude of experiments to demonstrate the effectiveness of the proposed E2LVLM on the public benchmark dataset NewsCLIPpings~\cite{luo2021newsclippings}. Typically, we focus on the six evaluation questions as follows:
\begin{enumerate}[label=\textbf{Q\arabic*:}]
    \item  How does E2LVLM perform in the task of multimodal OOC misinformation detection?
    \item  How does each procedure contribute to the E2LVLM's performance in detection?
    \item  Does E2LVLM provide accurate detections and compelling rationales for their judgments?
    \item  How impact are different sizes of the base LVLM on E2LVLM in detection?
    \item  Can E2LVLM be rapidly deployed at the stage of early detection?
    \item  How does E2LVLM perform on the other dataset?
\end{enumerate}

\subsection{Experimental Setup}

\textbf{Dataset.} We evaluate the efficacy of the proposed E2LVLM on the dataset NewsCLIPpings~\cite{luo2021newsclippings}. This dataset serves as the largest real-world multimodal misinformation detection benchmark. We follow the standard protocol \cite{abdelnabi2022open, yuan2023support, qi2024sniffer}, and report experimental results on the Merged/Balance subset. This subset consists of 71,072 training, 7,024 validation, and 7,264 testing, respectively.

\noindent \textbf{Compared Baselines.} To make a comprehensive performance evaluation, we compare the proposed E2LVLM with a series of representative methods. (1) A line of research focuses on attached classifiers trained from scratch, including SAFE~\cite{massarelli2019safe} and EANN~\cite{wang2018eann}. (2) Another line of research underlines the use of pre-trained models, containing VisualBERT~\cite{li2019visualbert}, CLIP~\cite{radford2021learning}, Neu-Sym detector~\cite{zhang2023detecting}, DT-Transformer~\cite{papadopoulos2023synthetic}, CCN~\cite{abdelnabi2022open}, SEN~\cite{yuan2023support}, and ECENet~\cite{zhang2023ecenet}. (3) Furthermore, in the era of LVLMs, SNIFFER~\cite{qi2024sniffer} is the first attempt to adopt a multimodal large language model for addressing the OOC task. More details of these methods can be provided in their official papers. 

\noindent \textbf{Evaluation Metrics.} We regard the multimodal OOC misinformation detection issue as a binary classification task. Following the standard process~\cite{abdelnabi2022open}, the accuracy over all samples (All), the accuracy over the OOC (Falsified), and not OOC (Pristine) are reported as the metrics during evaluation for a fair comparison.

\noindent \textbf{Implementation Details.} We choose Qwen2-VL-7B~\cite{wang2024qwen2} as the base LVLM, unless otherwise specified. We implement E2LVLM on PyTorch~\cite{paszke2019pytorch} version 2.3.1 with CUDA 12.2, and train it for 2 epochs on 4 NVIDIA GeForce RTX 3090 GPUs with 24G of memory. We adopt FlashAttention - 2~\cite{dao2024flashattention} for efficient training on the visual encoder and large language model. We use a batch size of 8 and a learning rate of $2\times 10^{-4}$. The models are optimized using AdamW~\cite{loshchilov2017decoupled} optimizer with a linear warmup and a cosine learning rate scheduler. Additionally, all experimental results are the average of three runs with no hyper-parameter searching.

% \clearpage
\input{4.1_experiment_q1}
% \clearpage
\input{4.2_experiment_q2}
\input{4.2_experiment_q3}
% \newpage
% \clearpage
% \input{4.2_experiment_q4}
% \newpage
\input{4.2_experiment_q5}

% \input{4.2_experiment_q6}