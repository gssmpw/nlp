\section{Introduction}
\label{sec:intro}

Recent years have witnessed the proliferation and advancement of Deepfake and social media platforms \cite{taigman2014deepface, liu2024forgery, qi2024sniffer, wang2024mmidr, shao2024detecting}, individuals freely present their speeches and opinions in form of image-text pairs. This inevitably accelerates the creation and ``viral'' spread of erroneous information, \ie, fake news, which intentionally deceives or misleads audiences in the real world. For example, during the COVID-19 pandemic~\cite{naeem2020covid}, falsified information conveyed by unaltered images and manipulated claims threats information security (IS) and societal order \cite{bohacek2024making, lin2024detecting}. Therefore, developing automated fact-checking specifically engineered for multimodal out-of-context (OOC) misinformation~\cite{abdelnabi2022open} has become a pressing necessity in the modern era.

% Raw
% \begin{figure}[t]
%   \centering
%    \includegraphics[width=1\linewidth]{sec/fig1.jpg}
%    \caption{Subfigures (a) and (b) show the representation distributions between images and their retrieved/rewritten evidence. In subfigure (a), textual evidence in most cases is away from sampled images, even the most relevant item calculated by cosine similarity. This limitation is mitigated by E2LVLM, as illustrated in subfigure (b). In addition, subfigure (c) provides a quantization comparison of E2LVLM over the OOC on NewsCLIPpings~\cite{luo2021newsclippings}.}
%    \label{fig:1}
% \end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig1.jpg}
   \caption{Subfigures (a) and (b) show the representation distributions between images and their retrieved/rewritten evidence. In addition, subfigure (c) provides a quantization comparison of E2LVLM over the OOC on NewsCLIPpings~\cite{luo2021newsclippings}.}
   \label{fig:1}
\end{figure}

%\footnote{https://programmablesearchengine.google.com}
%\footnote{https://cloud.google.com/vision}
Recent advancements \cite{yuan2023support, zhang2023ecenet, papadopoulos2023red} in the realm of OOC, the task of judging whether an authentic image is rightly or wrongly used in a claim, explore the use of external information retrieved by the custom search engine, such as Google API. Such methods achieve noteworthy improvements in performance. Unlike previous works without injecting external information \cite{aneja2023cosmos, mu2023self, dang2024overview}, where they solely focus on semantic similarity within image-claim pairs, a professional fact-checker commonly takes account of both internal and external information related to image-claim pairs for accurate decisions. A crucial challenge in the OOC task under the retrieved evidence lies in effectively leveraging the potential evidence to make the models identify inconsistencies regarding image-claim pairs.

Some efforts \cite{abdelnabi2022open, qi2024sniffer, tahmasebi2024multimodal} endeavor to introduce the retrieved evidence into attached classifiers, pre-trained models, and LVLMs, \eg, InstructBLIP~\cite{dai2023instructblip}, for the OOC task. This increases architectural complexity for better performance. However, a significant limitation of them is that the retrieved textual evidence doesn't match LVLMs inputs in the sense. As depicted in Figure~\ref{fig:1}, we have observed the following findings:
\begin{itemize}
    \item \textit{Not all textual evidence is effective. Relevant and irrelevant evidence are entangled, making the models challenging to fact verification.}
    \item \textit{A significant discrepancy exists between the retrieved textual evidence and rewritten content. LVLMs suffer from pieces of external information.}
\end{itemize}
This investigation indicates that existing retrieved textual evidence related to authentic images is relevant or irrelevant, which is not effective enough to empower the models with the discriminatory capacity over multimodal OOC misinformation. On the other hand, textual evidence reranking acquires the potentially available clue to support or refute the input image-claim pairs, while the representation of textual evidence is far from that of authentic images, as illustrated in subfigure (a) of \Cref{fig:1}. It is attributed that the retrieved textual evidence may include omit crucial details, redundant content, or fail to align with the input format of LVLMs~\cite{wu2024cotkr}. Therefore, exploring the reranking and rewriting of the retrieved textual evidence about authentic images is crucial to assist LVLMs for both judgment and explanation in the task of OOC.

In this paper, we present an innovative LVLMs-centric framework, dubbed E2LVLM, designed to deeply explore the use of textual evidence of authentic images for checking multimodal OOC misinformation in practical scenarios. Considering a trade-off between computation burden and information capacity~\cite{yin2023survey}, E2LVLM adopts the open-source LVLM such as Qwen2-VL~\cite{wang2024qwen2}, rather than closed-source LVLMs (\eg, GPT-4o~\cite{hurst2024gpt} and Gemini~\cite{team2023gemini}). E2LVLM depends on the vision and language understanding capabilities of Qwen2-VL to acquire informative content for aligning with LVLMs inputs. It first prompts Qwen2-VL to rerank textual evidence, and selects one most relevant textual evidence as external information. Then, E2LVLM rewrites the selected evidence to generate coherent and contextually attuned content, which eliminates the discrepancy between the retrieved textual evidence and model's logic. As shown in subfigure (b) of \Cref{fig:1}, the rewritten content based on the textual evidence reranking is closer to authentic images.

Moreover, to tackle the scarcity of the modelâ€™s explainability regarding multimodal OOC misinformation verification, E2LVLM incorporates the rewritten content into Qwen2-VL to generate explanation annotations for image-claim pairs, apart from ``Pristine'' and ``Falsified'' elements. Based on such supervised signal, E2LVLM attempts to extend one-stage multimodal instruction tuning on Qwen2-VL to the task of OOC. A large body of experiments shows that E2LVLM not only provides accurate decisions, but also yields compelling rationales for their judgments. As shown in subfigure (c) of \Cref{fig:1}, the proposed E2LVLM achieves an accuracy of 90.34\% over the OOC, and outperforms the state-of-the-art (SOTA) method by around 3.44\% accuracy on the benchmark dataset NewsCLIPpings~\cite{luo2021newsclippings}.

In summary, this paper has three-fold contributions as follows:
\begin{itemize}
    \item We investigate the discrepancy of representation distributions between images and their textual evidence, and underline the importance of the effective use of textual evidence related to images for better OOC detection.
    \item Based on this insight, we propose E2LVLM - a novel LVLMs-based framework, for multimodal OOC misinformation detection. Introducing the retrieved textual evidence into LVLMs reranks and rewrites them to acquire informative content for identifying the inconsistencies between image and claim.
    \item We further design a simple yet automated data generation pipeline, to construct a multimodal instruction-following dataset with both judgment and explanation.  Leveraging this dataset, we adopt a one-stage instruction tuning strategy to fine-tune E2LVLM for debunking multimodal OOC misinformation.
    \item We conduct extensive experiments evaluating E2LVLM on the public benchmark dataset NewsCLIPpings, and demonstrate its superiority in terms of both judgment and explanation. E2LVLM outperforms the current SOTA by around 3.44\% accuracy over the OOC.
    
\end{itemize}