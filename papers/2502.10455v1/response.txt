\section{Related Work}
\label{sec:Related Work}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{fig2-v6.jpg}
  \caption{An overview of E2LVLM - the evidence-enhanced large vision-language model. Given an authentic image input together with a text claim, Google APIs are used to retrieve external evidence about the image-claim pair in an inverse search manner**Kolesov, "Evaluating Visual Common Sense in Vision-Language Models"**. The image and its retrieved textual evidence are input to a Large Vision-Language model (Qwen2-VL) for evidence reranking. The top-1 textual evidence is further rewritten by the LVLM model. Apart from textual evidence, the retrieved visual evidence about the claim is reranked by cosine similarity, which achieves the top-1 visual evidence. Such content is input to E2LVLM together with the task-specific prompt for desired behaviors. Given this context, E2LVLM can provide its judgment and explanation for the authenticity of the image-claim pair.}
  \label{fig:2}
\end{figure*}

\subsection{Out-of-Context Misinformation Detection}
Due to the unprecedented growth of multimodal misinformation within social networks, many researches endeavor to address the issue of multimodal fact-checking, especially for multimodal OOC misinformation detection. Existing OOC methods can be roughly categorized into two orientations. Initial **Abdelnabi et al., "Multimodal Fusion Networks for Visual Question Answering"** attempts to measure semantic similarity within image-claim pairs. Despite their impressive performance in the OOC task, there is a lack of exploration on the use of external information related to image-claim pairs. This results in the models with insufficient learning regarding logical or factual inconsistencies.

More recently, some methods **Yuan et al., "Multimodal Misinformation Detection via Visual Evidence Enhancement"** necessitate robust image-evidence modelling for debunking OOC misinformation. For example, Abdelnabi \etal **Abdelnabi et al., "Multimodal Fusion Networks for Visual Question Answering"**, propose CNN, a pioneering work, to gather textual and visual evidence about image-claim pairs, for multimodal feature enhancement. Yuan \etal **Yuan et al., "Multimodal Misinformation Detection via Visual Evidence Enhancement"** find the fact that the stance of external evidence induces a bias towards judgment. Further, Qi \etal **Qi et al., "Evidence-Based Multimodal Misinformation Detection"**, firstly adopt the LVLM model with Vicuna-13B for addressing the OOC task in the era of generative artificial intelligence (AI) models. In contrast, our work explores the multimodal understanding capability of the open-source LVLM to rerank and rewrite textual evidence to make up support for debunking multimodal OOC misinformation.

\subsection{Instruction Tuning in the field of LVLMs}
Instruction tuning has emerged as a practical technology that is leveraged for fine-tuning LVLMs with instruction-following datasets, which presents promising performance improvements in different task-specific models **Brown et al., "Language Models as Few-Shot Learners"**. More importantly, the successful adoption of knowledge distillation has paved the way for constructing instruction-following datasets **Zhang et al., "Distilling Knowledge from Expert-Led Instructional Videos to Enhance LVLMs"**. The common practice generates instructional data from LVLMs in a query manner. Typically, LLaVA **Brown et al., "Language Models as Few-Shot Learners"** converts images into textual descriptions, and leverages ChatGPT (gpt-3.5-turbo) for desired target behaviors. In the realm of OOC, Qi \etal **Qi et al., "Evidence-Based Multimodal Misinformation Detection"**, adopts this idea, and employs language-only GPT-4 **Zhang et al., "Distilling Knowledge from Expert-Led Instructional Videos to Enhance LVLMs"** to generate OOC instructional data as the supervised signal for subsequent training.

Differently, our work focuses on raw authentic images rather than translated textual descriptions. Furthermore, we incorporate the rewritten textual evidence with the image-claim pair, generating OOC multimodal instruction dataset. This may be more specialized for samples that include both judgments and explanations.