\subsection{RQ3: Detection and Beyond}

%---------------------------------
% \begin{figure}[t]
%   \centering
%     \fbox{\rule{0pt}{1.8in} \rule{.8\linewidth}{0pt}}
%    \caption{Reranking performance of E2LVLM.  }
%    \label{fig:4}
% \end{figure}

\begin{figure}[t]
  \centering
   \includegraphics[width=.9\linewidth]{fig4.jpg}
   \caption{Comparison of E2LVLM with various reranking ways.}
   \label{fig:4}
\end{figure}

% \begin{figure}[t]
%   \centering
%    \includegraphics[width=1\linewidth]{sec/fig5.jpg}
%    \caption{Visualization of various data distributions.}
%    \label{fig:5}
% \end{figure}


Considering authentic images in the OOC realm, we further evaluate the effectiveness of the textual evidence reranking and rewriting in E2LVLM for detection and beyond. In subsequent experiments, we uniformly adopt the accuracy over ``All'' samples for comparison, unless otherwise specified.

\noindent \textbf{Reranking Analysis.} Upon analysis experiments above, we have observed a significant performance of E2LVLM in the top-1 textual evidence. To understand the impact of this design, we start an investigation of E2LVLM's predictions, as shown in \Cref{fig:4}. We add other reranking ways for comparison, \ie, cosine similarity and random choice. For $k$, we set the range of it as $\{1,2,3\}$. As shown in these results, we can note that the change trends of accuracy are generally consistent across distinct ways, presenting degradations as the growth of textual evidence. This is caused by the fact that not all textual evidence is effective. This necessitates the textual evidence reranking to prevent the introduction of irrelevant items. Further, these results suggest that E2LVLM lies in the multimodal understanding and reranking capabilities of LVLMs, serving as a professional OOC misinformation detector that incorporates internal and external information for revealing misinformation.

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{fig5.jpg}
   \caption{Visualization of various data distributions.}
   \label{fig:5}
\end{figure}

\noindent \textbf{Explainability Analysis.} To illustrate the E2LVLM's explainability, we conduct a qualitative analysis around data distributions to comprehend the decision-making phase, as shown in \Cref{fig:5}. From the results, the following findings are drawn: 1) As for an image-claim pair with ``Falsified'' in subfigure (a) of \Cref{fig:5}, the authentic image and its rewritten content are far from the falsified claim. Simultaneously, the rewritten content is closer to the authentic image, which supports it and refutes its claim. This makes the proposed E2LVLM provide an accurate decision. 2) As for an image-claim pair with ``Pristine'' in subfigure (b) of \Cref{fig:5}, the representation distributions of the authentic image, rewritten content, and claim are close together. This promotes E2LVLM to accurately discern this sample. These cases indicate that E2LVLM can effectively provide both judgment and explanation for debunking OOC misinformation.

\subsection{RQ4: Discussion of Different LVLMs}




% \begin{table}[t]
% \caption{A multimodal OOC misinformation detectors in distinct settings. Evaluated on NewsCLIPpings~\cite{luo2021newsclippings}. ``\#Params.'' refers to the parameters of Large Language Models in LVLMs.}
% \centering
% % \setlength{\tabcolsep}{4pt} % 调整列间距，减小为4pt
% \renewcommand{\arraystretch}{1.1} % 调整行高（默认为1）
%     \begin{adjustbox}{valign=c,max width=\columnwidth}
%         \begin{tabular}{lc|c|ccc}
%             \toprule
%             Settings & Tuning& \#Params. & \textbf{All} & \textbf{Falsified} & \textbf{Pristine} \\
%             \hline
%             Random & \xmark & -  & 50.3	& 49.2 & 51.5 \\
%             Qwen2-VL-2B & \xmark & 1.5B  & 65.7 & 88.6 & 42.9 \\
%             Qwen2-VL-7B & \xmark & 7.6B  &  79.1 & 75.7  & 82.5  \\
%             Qwen2-VL-72B & \xmark & 72B  & 79.8 & 87.2 & 72.5  \\
%             \hline
%             Qwen2-VL-2B & \cmark & 1.5B  & 81.4	& 80.2 & 82.6 \\
%             \rowcolor{lightgreen} Qwen2-VL-7B & \cmark & 7.6B  & 89.9	& 90.3 &89.4 \\
%             \bottomrule
%         \end{tabular}
%     \end{adjustbox}
% \label{tab:tab_4}
% \end{table}


% \begin{table}[t]
% \caption{A multimodal OOC misinformation detectors with distinct LVLMs. Evaluated on the NewsCLIPpings~\cite{luo2021newsclippings} dataset.}
% \centering
% \small
% % \setlength{\tabcolsep}{0.5pt} % 调整列间距，减小为4pt
% % \renewcommand{\arraystretch}{1.1} % 调整行高（默认为1）
%     \begin{adjustbox}{valign=c,max width=\columnwidth}
%         \begin{tabular}{lc|c|c}
%             \toprule
%             Settings & Tuning & Parameters & \textbf{All} \\
%             \hline
%             Random & \xmark & -  & 50.3	\\
%             Qwen2-VL-2B & \xmark & 1.5B  & 65.7 \\
%             Qwen2-VL-7B & \xmark & 7.6B  &  79.1 \\
%             Qwen2-VL-72B & \xmark & 72B  & 79.8 \\
%             \hline
%             Qwen2-VL-2B & \cmark & 1.5B  & 81.4	\\
%             \rowcolor{lightgreen} Qwen2-VL-7B & \cmark & 7.6B  & \textbf{89.9} \\
%             \bottomrule
%         \end{tabular}
%     \end{adjustbox}
% \label{tab:tab_4}
% \end{table}


% \begin{table}[t]
% \caption{Performance comparison on distinct LVLMs between instructional data construction and model tuning. Evaluated on the NewsCLIPpings~\cite{luo2021newsclippings} dataset.}
% \centering
% \small
% % \setlength{\tabcolsep}{5.5pt} % 调整列间距，减小为4pt
% % \renewcommand{\arraystretch}{1.1} % 调整行高（默认为1）
%     \begin{adjustbox}{valign=c,max width=\columnwidth}
%         \begin{tabular}{lcl|c}
%         \toprule
%          Data Construction &  \ding{223} & Model  Tuning & \textbf{All} \\
%         \hline
%         Qwen2-VL-2B & \ding{223} & Qwen2-VL-2B   & 81.1 \\
%         Qwen2-VL-2B & \ding{223} & Qwen2-VL-7B   & 87.6 \\
%         \hline
%         Qwen2-VL-7B & \ding{223} & Qwen2-VL-2B   & 81.4 \\
%         \rowcolor{lightgreen} Qwen2-VL-7B & \ding{223} & Qwen2-VL-7B   & \textbf{89.9} \\
%         \bottomrule
%         \end{tabular}
%     \end{adjustbox}
% \label{tab:tab_5}
% \end{table}

To analyze the impact of distinct LVLMs on E2LVLM in the OOC detection, we conduct analysis experiments around Qwen2-VL family~\cite{wang2024qwen2} in \Cref{tab:tab_4} and \Cref{tab:tab_5}. As depicted in these results, we achieve the following observations:



% The former is to evaluate the impact of the chosen base LVLM during training, and the latter is to measure the mutual synergy between instructional data construction and model tuning. 

(1) As shown in \Cref{tab:tab_4}, we employ different LVLMs for evidence-enhanced fine-tuning, apart from random guessing. The increasing of model's parameters provides better performance, which is consistent with previous research. Although the LVLM model Qwen2-VL-72B outperforms others (\eg, Qwen2-VL-7B) in a zero-shot scenario, it achieves a performance improvement of 0.7$\%$ with introducing around 10 times parameters. This ignores the compromise between computation burden and detection accuracy. Furthermore, the introduction of instruction tuning extends general-purpose LVLMs to the task of OOC, leading to significant improvements in performance (as shown in the fifth and last rows of this table).

% \newpage
(2) As depicted in \Cref{tab:tab_5}, we use different LVLMs at two stages, \ie, instructional data construction and model tuning. As for the former, we can observe that although LVLMs on a larger scale provide higher performance in detection accuracy, their improvements are finite (\eg, a performance improvement of 2.3$\%$ even in Qwen2-VL-7B). As for the latter, larger-scale LVLMs provide significant improvements in performance, leading to performance improvements of 6.5$\%$ and 8.5$\%$, respectively. This indicates the importance of the adopted LVLM in E2LVLM.