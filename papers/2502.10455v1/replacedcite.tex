\section{Related Work}
\label{sec:Related Work}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{fig2-v6.jpg}
  \caption{An overview of E2LVLM - the evidence-enhanced large vision-language model. Given an authentic image input together with a text claim, Google APIs are used to retrieve external evidence about the image-claim pair in an inverse search manner____. The image and its retrieved textual evidence are input to a Large Vision-Language model (Qwen2-VL) for evidence reranking. The top-1 textual evidence is further rewritten by the LVLM model. Apart from textual evidence, the retrieved visual evidence about the claim is reranked by cosine similarity, which achieves the top-1 visual evidence. Such content is input to E2LVLM together with the task-specific prompt for desired behaviors. Given this context, E2LVLM can provide its judgment and explanation for the authenticity of the image-claim pair.}
  \label{fig:2}
\end{figure*}

\subsection{Out-of-Context Misinformation Detection}
Due to the unprecedented growth of multimodal misinformation within social networks, many researches endeavor to address the issue of multimodal fact-checking, especially for multimodal OOC misinformation detection. Existing OOC methods can be roughly categorized into two orientations. Initial ____ attempts to measure semantic similarity within image-claim pairs. Despite their impressive performance in the OOC task, there is a lack of exploration on the use of external information related to image-claim pairs. This results in the models with insufficient learning regarding logical or factual inconsistencies.

More recently, some methods ____ necessitate robust image-evidence modelling for debunking OOC misinformation. For example, Abdelnabi \etal____ propose CNN, a pioneering work, to gather textual and visual evidence about image-claim pairs, for multimodal feature enhancement. Yuan \etal____ find the fact that the stance of external evidence induces a bias towards judgment. Further, Qi \etal____ firstly adopt the LVLM model with Vicuna-13B____ for addressing the OOC task in the era of generative artificial intelligence (AI) models. In contrast, our work explores the multimodal understanding capability of the open-source LVLM to rerank and rewrite textual evidence to make up support for debunking multimodal OOC misinformation.

\subsection{Instruction Tuning in the field of LVLMs}
Instruction tuning has emerged as a practical technology that is leveraged for fine-tuning LVLMs with instruction-following datasets, which presents promising performance improvements in different task-specific models ____. More importantly, the successful adoption of knowledge distillation has paved the way for constructing instruction-following datasets ____. The common practice generates instructional data from LVLMs in a query manner. Typically, LLaVA____ converts images into textual descriptions, and leverages ChatGPT (gpt-3.5-turbo)____ for desired target behaviors. In the realm of OOC, Qi \etal____ adopts this idea, and employs language-only GPT-4____ to generate OOC instructional data as the supervised signal for subsequent training.

Differently, our work focuses on raw authentic images rather than translated textual descriptions. Furthermore, we incorporate the rewritten textual evidence with the image-claim pair, generating OOC multimodal instruction dataset. This may be more specialized for samples that include both judgments and explanations.