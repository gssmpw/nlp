\section{Related Work}
\label{sec:related_work}

\paragraph{Representation Intervention.}
Representation intervention aims to edit the LLMs' hidden representations at certain layers to guide their behavior \cite{panickssery2023steering,zou2023representation,cao2024personalized, li2024inference, chen2024truth}. In particular, several efforts have been made to steer them toward more truthful generation. ITI \cite{li2024inference} utilizes fine-grained probing accuracy on each layer's attention heads to locate the most ``truthfulness-related" attention heads and improves truthfulness. TruthX \cite{zhang2024truthx} projects the LLM's internal representations into truthful and semantic latent spaces and refines the model within the truthful space, thereby improving its truthfulness. LITO \cite{bayat2024enhanced} aims to improve upon ITI and break the ``one-size-fits-all" intervention solution by sweeping through several intervention intensities to generate candidate responses and trains LSTM to predict which response to select. NL-ITI \cite{hoscilowicz2024nl} adopts MLP to replace the logistics regression in ITI to improve the probing accuracy, which results in a more appropriate choice of attention heads.

\paragraph{Other Approaches to Mitigate Hallucination.} Traditionally, post-training or fine-tuning is the default method for mitigating hallucination issues in LLMs. Typical methods include Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training}, Direct Preference Optimization \cite{rafailov2024direct}, and many other techniques to align LLMs with human values, especially truthfulness \cite{chen2024grath, tian2023fine, hu2024mitigating}. Although these methods have been successful in certain applications, they also exhibit significant shortcomings, such as high computational costs and instability during training \cite{casper2023open}. 
Aside from training-time mitigation and representation intervention, other inference-time approaches have been developed. Contrastive decoding aims to modify the output logits by contrasting strong and weak model outputs \cite{o2023contrastive, zhang2023alleviating, chen2024lower}. \citet{li2022contrastive} attempted to contrast an expert LLM with an amateur LLM to improve fluency and coherence. DoLa \cite{chuang2023dola} contrasted the final layer and early layers to edit output logits, leading to more truthful generation. \citet{kai2024sh2,chen2024context} refined output logits based on key tokens and context sharpness measured by contextual entropy, respectively.