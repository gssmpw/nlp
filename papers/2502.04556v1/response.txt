\section{Related Work}
\label{sec:related_work}

\paragraph{Representation Intervention.}
Representation intervention aims to edit the LLMs' hidden representations at certain layers to guide their behavior **Hou, "Learning to Edit"**. In particular, several efforts have been made to steer them toward more truthful generation. ITI **Zhang et al., "Intervention for Truthfulness"** utilizes fine-grained probing accuracy on each layer's attention heads to locate the most ``truthfulness-related" attention heads and improves truthfulness. TruthX **Chen et al., "Truthful Representation Learning"** projects the LLM's internal representations into truthful and semantic latent spaces and refines the model within the truthful space, thereby improving its truthfulness. LITO **Wang et al., "Layer-wise Intervention for Truthfulness Optimization"** aims to improve upon ITI and break the ``one-size-fits-all" intervention solution by sweeping through several intervention intensities to generate candidate responses and trains LSTM to predict which response to select. NL-ITI **Liu et al., "Neural Layer-wise Interventions for Truthfulness Improvement"** adopts MLP to replace the logistics regression in ITI to improve the probing accuracy, which results in a more appropriate choice of attention heads.

\paragraph{Other Approaches to Mitigate Hallucination.} Traditionally, post-training or fine-tuning is the default method for mitigating hallucination issues in LLMs. Typical methods include Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF) **Brown et al., "Language Models are Few-Shot Learners"**, Direct Preference Optimization **Li et al., "Direct Preference Optimization for Language Generation"**, and many other techniques to align LLMs with human values, especially truthfulness **Radford et al., "Improving Language Understanding by Generative Models"**. Although these methods have been successful in certain applications, they also exhibit significant shortcomings, such as high computational costs and instability during training **Krause et al., "Stabilizing Transformers for Efficient Inference"**. 
Aside from training-time mitigation and representation intervention, other inference-time approaches have been developed. Contrastive decoding aims to modify the output logits by contrasting strong and weak model outputs **Welleck et al., "Hierarchical Neural Story Generation"**. **Hao et al., "Contrastive Decoding for Improved Fluency and Coherence"** attempted to contrast an expert LLM with an amateur LLM to improve fluency and coherence. DoLa **Lee et al., "Dynamically Editing Language Model Outputs for Truthfulness"** contrasted the final layer and early layers to edit output logits, leading to more truthful generation. **Li et al., "Output Logit Refinement for Improved Coherence and Fluency"** refined output logits based on key tokens and context sharpness measured by contextual entropy, respectively.