

% intro -------------------------------------------
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{chen2024grath,
  title={GRATH: Gradual Self-Truthifying for Large Language Models},
  author={Chen, Weixin and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2401.12292},
  year={2024}
}

@article{pal2023med,
  title={Med-halt: Medical domain hallucination test for large language models},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  journal={arXiv preprint arXiv:2307.15343},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{sharma2023towards,
  title={Towards understanding sycophancy in language models},
  author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R and others},
  journal={arXiv preprint arXiv:2310.13548},
  year={2023}
}

@article{hu2024mitigating,
  title={Mitigating Large Language Model Hallucination with Faithful Finetuning},
  author={Hu, Minda and He, Bowei and Wang, Yufei and Li, Liangyou and Ma, Chen and King, Irwin},
  journal={arXiv preprint arXiv:2406.11267},
  year={2024}
}

@article{tian2023fine,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{xiong2023can,
  title={Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms},
  author={Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  journal={arXiv preprint arXiv:2306.13063},
  year={2023}
}

@article{li2024think,
  title={Think twice before assure: Confidence estimation for large language models through reflection on multiple answers},
  author={Li, Moxin and Wang, Wenjie and Feng, Fuli and Zhu, Fengbin and Wang, Qifan and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2403.09972},
  year={2024}
}

@article{tao2024trust,
  title={When to Trust LLMs: Aligning Confidence with Response Quality},
  author={Tao, Shuchang and Yao, Liuyi and Ding, Hanxing and Xie, Yuexiang and Cao, Qi and Sun, Fei and Gao, Jinyang and Shen, Huawei and Ding, Bolin},
  journal={arXiv preprint arXiv:2404.17287},
  year={2024}
}

@article{mundler2023self,
  title={Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation},
  author={M{\"u}ndler, Niels and He, Jingxuan and Jenko, Slobodan and Vechev, Martin},
  journal={arXiv preprint arXiv:2305.15852},
  year={2023}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{shuster2021retrieval,
  title={Retrieval augmentation reduces hallucination in conversation},
  author={Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
  journal={arXiv preprint arXiv:2104.07567},
  year={2021}
}

@article{li2024enhancing,
  title={Enhancing llm factual accuracy with rag to counter hallucinations: A case study on domain-specific queries in private knowledge-bases},
  author={Li, Jiarui and Yuan, Ye and Zhang, Zehua},
  journal={arXiv preprint arXiv:2403.10446},
  year={2024}
}


% halu survey -----------------------------
@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{rawte2023survey,
  title={A survey of hallucination in large foundation models},
  author={Rawte, Vipula and Sheth, Amit and Das, Amitava},
  journal={arXiv preprint arXiv:2309.05922},
  year={2023}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM New York, NY}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}


% flow related -----------------------------
@article{kim2024preference,
  title={Preference Alignment with Flow Matching},
  author={Kim, Minu and Lee, Yongsik and Kang, Sehyeok and Oh, Jihwan and Chong, Song and Yun, Se-Young},
  journal={arXiv preprint arXiv:2405.19806},
  year={2024}
}

@article{liu2022flow,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{lipman2022flow,
  title={Flow matching for generative modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
  journal={arXiv preprint arXiv:2210.02747},
  year={2022}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

% halu baseline ---------------------------

@inproceedings{bayat2024enhanced,
  title={Enhanced language model truthfulness with learnable intervention and uncertainty expression},
  author={Bayat, Farima Fatahi and Liu, Xin and Jagadish, H and Wang, Lu},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={12388--12400},
  year={2024}
}

@article{cao2024personalized,
  title={Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization},
  author={Cao, Yuanpu and Zhang, Tianrong and Cao, Bochuan and Yin, Ziyi and Lin, Lu and Ma, Fenglong and Chen, Jinghui},
  journal={arXiv preprint arXiv:2406.00045},
  year={2024}
}

@article{panickssery2023steering,
  title={Steering llama 2 via contrastive activation addition},
  author={Panickssery, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  journal={arXiv preprint arXiv:2312.06681},
  year={2023}
}

@article{wang2023backdoor,
  title={Backdoor activation attack: Attack large language models using activation steering for safety-alignment},
  author={Wang, Haoran and Shu, Kai},
  journal={arXiv preprint arXiv:2311.09433},
  year={2023}
}

@article{liu2023context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering},
  author={Liu, Sheng and Ye, Haotian and Xing, Lei and Zou, James},
  journal={arXiv preprint arXiv:2311.06668},
  year={2023}
}

@article{li2022contrastive,
  title={Contrastive decoding: Open-ended text generation as optimization},
  author={Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.15097},
  year={2022}
}

@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@article{chuang2023dola,
  title={Dola: Decoding by contrasting layers improves factuality in large language models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  journal={arXiv preprint arXiv:2309.03883},
  year={2023}
}

@article{o2023contrastive,
  title={Contrastive decoding improves reasoning in large language models},
  author={O'Brien, Sean and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.09117},
  year={2023}
}

@article{zhang2023alleviating,
  title={Alleviating hallucinations of large language models through induced hallucinations},
  author={Zhang, Yue and Cui, Leyang and Bi, Wei and Shi, Shuming},
  journal={arXiv preprint arXiv:2312.15710},
  year={2023}
}

@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hoscilowicz2024nl,
  title={NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method},
  author={Hoscilowicz, Jakub and Wiacek, Adam and Chojnacki, Jan and Cieslak, Adam and Michon, Leszek and Urbanevych, Vitalii and Janicki, Artur},
  journal={arXiv preprint arXiv:2403.18680},
  year={2024}
}

@article{zhang2024truthx,
  title={Truthx: Alleviating hallucinations by editing large language models in truthful space},
  author={Zhang, Shaolei and Yu, Tian and Feng, Yang},
  journal={arXiv preprint arXiv:2402.17811},
  year={2024}
}

@inproceedings{chen2024truth,
  title={Truth forest: Toward multi-scale truthfulness in large language models through intervention without tuning},
  author={Chen, Zhongzhi and Sun, Xingwu and Jiao, Xianfeng and Lian, Fengzong and Kang, Zhanhui and Wang, Di and Xu, Chengzhong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={20967--20974},
  year={2024}
}

@article{kai2024sh2,
  title={SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully},
  author={Kai, Jushi and Zhang, Tianhang and Hu, Hai and Lin, Zhouhan},
  journal={arXiv preprint arXiv:2401.05930},
  year={2024}
}

@article{chen2024lower,
  title={Lower Layer Matters: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused},
  author={Chen, Dingwei and Fang, Feiteng and Ni, Shiwen and Liang, Feng and Xu, Ruifeng and Yang, Min and Li, Chengming},
  journal={arXiv preprint arXiv:2408.08769},
  year={2024}
}  % require fine tune


@article{chen2024context,
  title={In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation},
  author={Chen, Shiqi and Xiong, Miao and Liu, Junteng and Wu, Zhengxuan and Xiao, Teng and Gao, Siyang and He, Junxian},
  journal={arXiv preprint arXiv:2403.01548},
  year={2024}
}


% self consistency -------------------------
@article{cheng2024integrative,
  title={Integrative Decoding: Improve Factuality via Implicit Self-consistency},
  author={Cheng, Yi and Liang, Xiao and Gong, Yeyun and Xiao, Wen and Wang, Song and Zhang, Yuji and Hou, Wenjun and Xu, Kaishuai and Liu, Wenge and Li, Wenjie and others},
  journal={arXiv preprint arXiv:2410.01556},
  year={2024}
}


% hidden states ---------------------------
@article{azaria2023internal,
  title={The internal state of an LLM knows when it's lying},
  author={Azaria, Amos and Mitchell, Tom},
  journal={arXiv preprint arXiv:2304.13734},
  year={2023}
}

@inproceedings{ren2022out,
  title={Out-of-distribution detection and selective generation for conditional language models},
  author={Ren, Jie and Luo, Jiaming and Zhao, Yao and Krishna, Kundan and Saleh, Mohammad and Lakshminarayanan, Balaji and Liu, Peter J},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{duan2024llms,
  title={Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States},
  author={Duan, Hanyu and Yang, Yi and Tam, Kar Yan},
  journal={arXiv preprint arXiv:2402.09733},
  year={2024}
}

@article{chen2024inside,
  title={INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection},
  author={Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
  journal={arXiv preprint arXiv:2402.03744},
  year={2024}
}

@article{orgad2024llms,
  title={Llms know more than they show: On the intrinsic representation of llm hallucinations},
  author={Orgad, Hadas and Toker, Michael and Gekhman, Zorik and Reichart, Roi and Szpektor, Idan and Kotek, Hadas and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2410.02707},
  year={2024}
}

@article{ji2024llm,
  title={Llm internal states reveal hallucination risk faced with a query},
  author={Ji, Ziwei and Chen, Delong and Ishii, Etsuko and Cahyawijaya, Samuel and Bang, Yejin and Wilie, Bryan and Fung, Pascale},
  journal={arXiv preprint arXiv:2407.03282},
  year={2024}
}

@article{wang2024adaptive,
  title={Adaptive Activation Steering: A Tuning-Free LLM Truthfulness Improvement Method for Diverse Hallucinations Categories},
  author={Wang, Tianlong and Jiao, Xianfeng and He, Yifan and Chen, Zhongzhi and Zhu, Yinghao and Chu, Xu and Gao, Junyi and Wang, Yasha and Ma, Liantao},
  journal={arXiv preprint arXiv:2406.00034},
  year={2024}
}


% dataset --------------------------------
@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{li2023halueval,
  title={Halueval: A large-scale hallucination evaluation benchmark for large language models},
  author={Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.11747},
  year={2023}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

% extra cite in main body ------------------------
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, M},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{allen2023physics,
  title={Physics of language models: Part 3.1, knowledge storage and extraction},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2309.14316},
  year={2023}
}

@book{euler1845institutionum,
  title={Institutionum calculi integralis},
  author={Euler, Leonhard},
  volume={4},
  year={1845},
  publisher={impensis Academiae imperialis scientiarum}
}

@article{runge1895numerische,
  title={{\"U}ber die numerische Aufl{\"o}sung von Differentialgleichungen},
  author={Runge, Carl},
  journal={Mathematische Annalen},
  volume={46},
  number={2},
  pages={167--178},
  year={1895},
  publisher={Springer}
}

@book{kutta1901beitrag,
  title={Beitrag zur n{\"a}herungsweisen Integration totaler Differentialgleichungen},
  author={Kutta, Wilhelm},
  year={1901},
  publisher={Teubner}
}

@article{sellam2020bleurt,
  title={BLEURT: Learning robust metrics for text generation},
  author={Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P},
  journal={arXiv preprint arXiv:2004.04696},
  year={2020}
}

@article{jin2024exploring,
  title={Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?},
  author={Jin, Mingyu and Yu, Qinkai and Huang, Jingyuan and Zeng, Qingcheng and Wang, Zhenting and Hua, Wenyue and Zhao, Haiyan and Mei, Kai and Meng, Yanda and Ding, Kaize and others},
  journal={arXiv preprint arXiv:2404.07066},
  year={2024}
}

@article{liu2024fantastic,
  title={Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics},
  author={Liu, Zhu and Kong, Cunliang and Liu, Ying and Sun, Maosong},
  journal={arXiv preprint arXiv:2403.01509},
  year={2024}
}

@article{cai2024self,
  title={Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller},
  author={Cai, Min and Zhang, Yuchen and Zhang, Shichang and Yin, Fan and Zhang, Dan and Zou, Difan and Yue, Yisong and Hu, Ziniu},
  journal={arXiv preprint arXiv:2406.02721},
  year={2024}
}

@article{aghajanyan2020intrinsic,
  title={Intrinsic dimensionality explains the effectiveness of language model fine-tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2012.13255},
  year={2020}
}

@inproceedings{manigrasso2024probing,
  title={Probing LLMs for logical reasoning},
  author={Manigrasso, Francesco and Schouten, Stefan and Morra, Lia and Bloem, Peter},
  booktitle={International Conference on Neural-Symbolic Learning and Reasoning},
  pages={257--278},
  year={2024},
  organization={Springer}
}

@inproceedings{zou2024improving,
  title={Improving alignment and robustness with circuit breakers},
  author={Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Kolter, J Zico and Fredrikson, Matt and Hendrycks, Dan},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{du2024haloscope,
  title={Haloscope: Harnessing unlabeled llm generations for hallucination detection},
  author={Du, Xuefeng and Xiao, Chaowei and Li, Yixuan},
  journal={arXiv preprint arXiv:2409.17504},
  year={2024}
}

@misc{burden2010numerical,
  title={Numerical analysis},
  author={Burden, Richard L and Faires, J Douglas},
  year={2010},
  publisher={Brooks Cole}
}