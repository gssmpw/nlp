\section{RELATED WORKS}
\subsection{Collaborative Perception}
    Collaborative perception has raised great interest recently for its potential in autonomous driving tasks. With the emergence of several high-quality datasets, including V2X-Sim \cite{V2X-Sim}, OPV2V \cite{OPV2V} and DAIR-V2X \cite{DAIR-V2X}, etc., a large body of research has been devoted to improving the effectiveness and robustness of multi-agent collaborative perception \cite{CoCa3D,  Where2comm, CORE, HEAL}. For example, Where2comm \cite{Where2comm} proposes a spatial confidence map to pursue a trade-off between communication bandwidth and perception performance. Based on a learning-to-reconstruction formulation, CORE \cite{CORE} presents a conceptually simple but effective model which can offer reasonable and clear supervision to inspire more effective collaboration and eventually promote perception tasks. To address the open heterogeneous problem, HEAL \cite{HEAL} establishes a unified feature space and aligns new agents with an innovative backward alignment. However, these models assume that the vehicle localization is always accurate, which is difficult to achieve in realistic scenarios.

\subsection{Pose Rectification}
    Localization error is a serious challenge encountered by collaborative perception in practical applications, leading to spatial misalignment of features during fusion process. Inaccurate poses caused by sensor accuracy or unknown interference would result in worse collective perception performance. Therefore, many methods attempt to design pose-error rectification module or robust network to alleviate error effects. FPV-RCNN \cite{fpvrcnn} classifies the selected keypoints and use maximum consensus algorithm to find correspondences between agents. In order to handle pose errors, V2X-ViT \cite{v2xvit} present a robust cooperative framework, which consists of multi-agent self-attention and multi-scale window attention. CoAlign \cite{CoAlign} proposes to use agent-object graph optimazation to enhance pose consistency and rectify localization errors. Based on feature-level consensus matching, FeaCo \cite{FeaCo} also devises a pose-error rectification module PRM to align features. In this work, we propose an efficient framework FeaKM to deal with more serious pose inaccuracies.

    \begin{table*}
  \caption{Detection performance AP@0.5 on DAIR-V2X with pose noises.}
  \label{table1}
  \begin{tabular}{c|ccccccccccc}
    \toprule
    Method/Noise & 0.0/0.0 & 0.0/0.2 & 0.4/0.4 & 0.6/0.6 & 0.8/0.8 & 1.0/1.0 & 1.2/1.2 & 1.4/1.4 & 1.6/1.6 & 1.8/1.8 & 2.0/2.0 \\
    \midrule
    MASH \cite{MASH} & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 \\
    F-Cooper \cite{F-Cooper}& 0.740 & 0.723 & 0.703 & 0.693 & 0.676 & 0.672 & 0.670 & 0.662 & 0.659 & 0.658 & 0.650 \\
    V2X-ViT \cite{v2xvit} & 0.713 & 0.706 & 0.693 & 0.680 & 0.670 & 0.664 & 0.654 & 0.651 & 0.643 & 0.638 & 0.637 \\
    CoAlign \cite{CoAlign} & 0.762 & 0.746 & \textbf{0.725} & 0.701 & 0.687 & 0.680 & 0.671 & 0.664 & 0.659 & 0.658 & 0.654 \\
    Ours(4pairs)   & 0.726 & 0.718 & 0.711 & 0.703 & \textbf{0.697} & \textbf{0.697} & \textbf{0.695} & \textbf{0.694} & \textbf{0.694} & \textbf{0.694} & \textbf{0.692} \\
    Ours(8pairs) & \textbf{0.768} & \textbf{0.753} & 0.716 & \textbf{0.705} & 0.690 & 0.686 & 0.678 & 0.673 & 0.670 & 0.667 & 0.668 \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Detection performance AP@0.7 on DAIR-V2X with pose noises.}
  \label{table2}
  \begin{tabular}{c|ccccccccccc}
    \toprule
    Method/Noise & 0.0/0.0 & 0.0/0.2 & 0.4/0.4 & 0.6/0.6 & 0.8/0.8 & 1.0/1.0 & 1.2/1.2 & 1.4/1.4 & 1.6/1.6 & 1.8/1.8 & 2.0/2.0 \\
    \midrule
    MASH \cite{MASH} & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 \\
    F-Cooper \cite{F-Cooper}& 0.579 & 0.572 & 0.563 & 0.555 & 0.553 & 0.546 & 0.546 & 0.542 & 0.540 & 0.534 & 0.533 \\
    V2X-ViT \cite{v2xvit} & 0.544 & 0.539 & 0.536 & 0.531 & 0.526 & 0.523 & 0.522 & 0.520 & 0.516 & 0.512 & 0.510 \\
    CoAlign \cite{CoAlign} & 0.625 & 0.596 & \textbf{0.582} & \textbf{0.575} & 0.570 & 0.567 & 0.564 & 0.559 & 0.558 & 0.557 & 0.555 \\
    Ours(4pairs) & 0.593 & 0.579 & 0.577 & 0.573 & \textbf{0.572} & \textbf{0.572} & \textbf{0.571} & \textbf{0.571} & \textbf{0.571} & \textbf{0.571} & \textbf{0.571} \\
    Ours(8pairs) & \textbf{0.631} & \textbf{0.601} & 0.577 & 0.573 & 0.570 & 0.566 & 0.561 & 0.563 & 0.560 & 0.558 & 0.558 \\
    \bottomrule
  \end{tabular}
\end{table*}