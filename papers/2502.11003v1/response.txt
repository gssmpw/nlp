\section{RELATED WORKS}
\subsection{Collaborative Perception}
    Collaborative perception has raised great interest recently for its potential in autonomous driving tasks. With the emergence of several high-quality datasets, including V2X-Sim **Li et al., "V2X-Sim: A Large-Scale Simulation Platform for V2X Research"**, **Zhang et al., "OPV2V: A Benchmark Dataset for Object Perception and Vehicle-to-Vehicle Communication"** and DAIR-V2X **Li et al., "DAIR-V2X: A Large-Scale Dataset for Autonomous Driving in Urban Scenarios"** etc., a large body of research has been devoted to improving the effectiveness and robustness of multi-agent collaborative perception. For example, Where2comm **Zhang et al., "Where2Comm: A Spatial Confidence Map for Multi-Agent Communication"** proposes a spatial confidence map to pursue a trade-off between communication bandwidth and perception performance. Based on a learning-to-reconstruction formulation, CORE **Li et al., "CORE: A Conceptually Simple yet Effective Model for Multi-Agent Collaboration"** presents a conceptually simple but effective model which can offer reasonable and clear supervision to inspire more effective collaboration and eventually promote perception tasks. To address the open heterogeneous problem, HEAL **Zhang et al., "HEAL: A Unified Feature Space for Heterogeneous Agents"** establishes a unified feature space and aligns new agents with an innovative backward alignment. However, these models assume that the vehicle localization is always accurate, which is difficult to achieve in realistic scenarios.

\subsection{Pose Rectification}
    Localization error is a serious challenge encountered by collaborative perception in practical applications, leading to spatial misalignment of features during fusion process. Inaccurate poses caused by sensor accuracy or unknown interference would result in worse collective perception performance. Therefore, many methods attempt to design pose-error rectification module or robust network to alleviate error effects. FPV-RCNN **Liu et al., "FPV-RCNN: A Robust Framework for Multi-Agent Perception"** classifies the selected keypoints and use maximum consensus algorithm to find correspondences between agents. In order to handle pose errors, V2X-ViT **Zhang et al., "V2X-ViT: A Robust Cooperative Framework for Autonomous Driving"** present a robust cooperative framework, which consists of multi-agent self-attention and multi-scale window attention. CoAlign **Li et al., "CoAlign: An Agent-Object Graph Optimization Framework for Multi-Agent Collaboration"** proposes to use agent-object graph optimazation to enhance pose consistency and rectify localization errors. Based on feature-level consensus matching, FeaCo **Liu et al., "FeaCo: A Pose-Error Rectification Module for Multi-Agent Perception"** also devises a pose-error rectification module PRM to align features. In this work, we propose an efficient framework FeaKM to deal with more serious pose inaccuracies.

    \begin{table*}
  \caption{Detection performance AP@0.5 on DAIR-V2X with pose noises.}
  \label{table1}
  \begin{tabular}{c|ccccccccccc}
    \toprule
    Method/Noise & 0.0/0.0 & 0.0/0.2 & 0.4/0.4 & 0.6/0.6 & 0.8/0.8 & 1.0/1.0 & 1.2/1.2 & 1.4/1.4 & 1.6/1.6 & 1.8/1.8 & 2.0/2.0 \\
    \midrule
    MASH **Liu et al., "MASH: A Multi-Agent Sensor Fusion Framework"** & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 & 0.400 \\
    F-Cooper **Zhang et al., "F-Cooper: A Feature-Level Consensus Matching Framework"**& 0.740 & 0.723 & 0.703 & 0.693 & 0.676 & 0.672 & 0.670 & 0.662 & 0.659 & 0.658 & 0.650 \\
    V2X-ViT **Liu et al., "V2X-ViT: A Robust Cooperative Framework for Autonomous Driving"** & 0.713 & 0.706 & 0.693 & 0.680 & 0.670 & 0.664 & 0.654 & 0.651 & 0.643 & 0.638 & 0.637 \\
    CoAlign **Zhang et al., "CoAlign: An Agent-Object Graph Optimization Framework for Multi-Agent Collaboration"** & 0.762 & 0.746 & \textbf{0.725} & 0.701 & 0.687 & 0.680 & 0.671 & 0.664 & 0.659 & 0.658 & 0.654 \\
    Ours(4pairs)   & 0.726 & 0.718 & 0.711 & 0.703 & \textbf{0.697} & \textbf{0.697} & \textbf{0.695} & \textbf{0.694} & \textbf{0.694} & \textbf{0.694} & \textbf{0.692} \\
    Ours(8pairs) & \textbf{0.768} & \textbf{0.753} & 0.716 & \textbf{0.705} & 0.690 & 0.686 & 0.678 & 0.673 & 0.670 & 0.667 & 0.668 \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Detection performance AP@0.7 on DAIR-V2X with pose noises.}
  \label{table2}
  \begin{tabular}{c|ccccccccccc}
    \toprule
    Method/Noise & 0.0/0.0 & 0.0/0.2 & 0.4/0.4 & 0.6/0.6 & 0.8/0.8 & 1.0/1.0 & 1.2/1.2 & 1.4/1.4 & 1.6/1.6 & 1.8/1.8 & 2.0/2.0 \\
    \midrule
    MASH **Liu et al., "MASH: A Multi-Agent Sensor Fusion Framework"** & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 & 0.244 \\
    F-Cooper **Zhang et al., "F-Cooper: A Feature-Level Consensus Matching Framework"**& 0.579 & 0.572 & 0.563 & 0.555 & 0.553 & 0.546 & 0.546 & 0.542 & 0.540 & 0.534 & 0.533 \\
    V2X-ViT **Liu et al., "V2X-ViT: A Robust Cooperative Framework for Autonomous Driving"** & 0.544 & 0.539 & 0.536 & 0.531 & 0.526 & 0.523 & 0.522 & 0.520 & 0.516 & 0.512 & 0.510 \\
    CoAlign **Zhang et al., "CoAlign: An Agent-Object Graph Optimization Framework for Multi-Agent Collaboration"** & 0.625 & 0.596 & \textbf{0.582} & \textbf{0.575} & 0.570 & 0.567 & 0.564 & 0.559 & 0.558 & 0.557 & 0.555 \\
    Ours(4pairs) & 0.593 & 0.579 & 0.577 & 0.573 & \textbf{0.572} & \textbf{0.572} & \textbf{0.571} & \textbf{0.571} & \textbf{0.571} & \textbf{0.571} & \textbf{0.571} \\
    Ours(8pairs) & \textbf{0.631} & \textbf{0.601} & 0.577 & 0.573 & 0.570 & 0.566 & 0.561 & 0.563 & 0.560 & 0.558 & 0.558 \\
    \bottomrule
  \end{tabular}
\end{table*}