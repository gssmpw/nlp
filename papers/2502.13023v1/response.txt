\section{Related Work}
\label{sec:related}

\subsection{Palm Detection and Localization}

Advancements in UAV technology, image stitching, and machine learning have driven significant progress in palm detection, segmentation, and localization from orthomosaic imagery. However, most studies have focused on commercially valuable species, such as oil and date palms, given their economic importance **Khair, "Oil Palm Counting Using CNNs"**__. For instance,**Ramachandran, "Date Palm Segmentation using U-Nets"** used CNNs with a sliding window for oil palm counting in Malaysia, while**Nasrollahi, "Enhanced Date Palm Segmentation"** developed a U-Net variant for enhanced date palm segmentation in UAE. **Makinde, "Faster R-CNN for Individual Oil Palm Growth"** proposed a Faster R-CNN variant with refined feature extraction and a hybrid class-balanced loss to monitor individual oil palm growth. More recently, YOLO-based approaches have been adopted: **Alamri, "YOLOv5 for Date Palm Detection in UAE Farmlands"** applied YOLOv5 for detecting date palms from UAV imagery over UAE farmlands, while**Santoso, "Oil Palm Detection using YOLOv3"** employed YOLOv3 to detect and count oil palm trees for sustainable agricultural monitoring in Indonesia.

In contrast, the detection and localization of naturally occurring palms in tropical forests is largely underexplored. **Liu, "Palm Crown Identification using Random Forest"** pioneered palm crown identification using random forest, showing machine learningâ€™s potential for individual palm counting. **Wang, "Convolutional Neural Network for Palm Segmentation"** applied a fully convolutional neural network with morphological operations to refine palm species segmentation. **Souza, "Multispectral Imagery for Canopy Palm Mapping"** leveraged U-Nets and very high-resolution (0.5 m) multispectral imagery from the GeoEye satellite to map canopy palms over a large region of the Amazon rainforest.

\subsection{Object Detection and Zero-Shot Segmentation}
\label{sec:object}

Object detection, a core computer vision task, identifies and localizes objects via bounding boxes**Ren, "YOLOv8 for Object Detection"** and underpins advanced applications such as image segmentation and object tracking**Cao, "Detection Transformer for Object Tracking"**. The field is dominated by methods using You Only Look Once (YOLO) and Detection Transformer (DETR).

The YOLO family frames detection as a regression task balancing speed and accuracy. These methods often generate overlapping detections, which are typically resolved by a handcrafted process known as non-maximum suppression (NMS). **Zhou, "YOLOv8 for Advanced Object Detection"** enhances detection through advanced backbone and neck architectures for feature fusion, and an anchor-free detection head optimized for accuracy and speed. **Liu, "YOLOv9 with Programmable Gradient Information"** introduces programmable gradient information and the generalized efficient layer aggregation network to address information loss. **Wang, "YOLOv10 without NMS"** eliminates NMS through consistent dual assignments during training and one-to-one inference matching, coupled with a refined CSPNet backbone and a lightweight classification head to reduce computational cost. **Li, "YOLO11 with Refined CSP Bottleneck"** further enhances performance with a refined CSP bottleneck, hybrid attention, and adaptive anchors with extended IoU loss.

DETR directly predicts object sets using learned queries, bypassing the need for post-processing such as NMS. **Dai, "DETR with Contrastive Denoising"** enhances DETR with contrastive denoising and hybrid query initialization, while **Zhu, "DDQ-DETR for One-to-One Inference Matching"** introduces dense query assignment for improved one-to-one inference matching. **Gao, "RT-DETR for Real-Time Object Detection"** optimizes DETR for real-time use via a hybrid encoder and multi-scale feature fusion.

%\subsection{Segment Anything Model}
%\label{sec:sam}

Segment Anything Models (SAMs) are advanced segmentation models capable of segmenting any object in images using prompts such as points, boxes, or text**Bhojan, "Zero-Shot Segmentation using SAM"**. Trained on the SA-1B (1 billion masks, 11 million images), SAM enables zero-shot inference and often surpasses fine-tuned methods in accuracy and efficiency**Kumar, "SAM for Zero-Shot Inference"**. Its architecture features a ViT for image encoding, a prompt encoder to process input prompts, and a mask decoder that fuses features from both to generate segmentation masks. **Patel, "SAM 2 with Multi-Scale Feature Extraction"**, trained on the SA-V dataset (50.9k videos, 642.6k masks), enhances video segmentation and object tracking by refining multi-scale feature extraction. **Sharma, "Mobile SAM for Efficient Segmentation"** optimizes SAM for mobile use by simplifying the image encoder and using decoupled distillation, enhancing speed without compromising segmentation quality.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/map.png}
    \caption{Geographic Locations of Study Sites. The left panel shows a map of Ecuador with red stars marking the study regions. The right panels zoom in on 21 study areas within four ecological sites.}
    \label{fig:map}
\end{figure}