\section{Related Work}
\label{sec:related}


\subsection{Palm Detection and Localization}

Advancements in UAV technology, image stitching, and machine learning have driven significant progress in palm detection, segmentation, and localization from orthomosaic imagery. However, most studies have focused on commercially valuable species, such as oil and date palms, given their economic importance ____. For instance,________ used CNNs with a sliding window for oil palm counting in Malaysia, while________ developed a U-Net variant for enhanced date palm segmentation in UAE. ________ proposed a Faster R-CNN variant with refined feature extraction and a hybrid class-balanced loss to monitor individual oil palm growth. More recently, YOLO-based approaches have been adopted:________ applied YOLOv5 for detecting date palms from UAV imagery over UAE farmlands, while________ employed YOLOv3 to detect and count oil palm trees for sustainable agricultural monitoring in Indonesia.

In contrast, the detection and localization of naturally occurring palms in tropical forests is largely underexplored. ________ pioneered palm crown identification using random forest, showing machine learningâ€™s potential for individual palm counting. ________ applied a fully convolutional neural network with morphological operations to refine palm species segmentation. ________ leveraged U-Nets and very high-resolution (0.5 m) multispectral imagery from the GeoEye satellite to map canopy palms over a large region of the Amazon rainforest.

\subsection{Object Detection and Zero-Shot Segmentation}
\label{sec:object}

Object detection, a core computer vision task, identifies and localizes objects via bounding boxes____ and underpins advanced applications such as image segmentation and object tracking____. The field is dominated by methods using You Only Look Once (YOLO) and Detection Transformer (DETR). 

The YOLO family frames detection as a regression task balancing speed and accuracy. These methods often generate overlapping detections, which are typically resolved by a handcrafted process known as non-maximum suppression (NMS). YOLOv8____ enhances detection through advanced backbone and neck architectures for feature fusion, and an anchor-free detection head optimized for accuracy and speed. YOLOv9____ introduces programmable gradient information and the generalized efficient layer aggregation network to address information loss. YOLOv10____ eliminates NMS through consistent dual assignments during training and one-to-one inference matching, coupled with a refined CSPNet backbone and a lightweight classification head to reduce computational cost. YOLO11____ further enhances performance with a refined CSP bottleneck, hybrid attention, and adaptive anchors with extended IoU loss.

DETR____ directly predicts object sets using learned queries, bypassing the need for post-processing such as NMS. DINO____ enhances DETR with contrastive denoising and hybrid query initialization, while DDQ-DETR____ introduces dense query assignment for improved one-to-one inference matching. RT-DETR____ optimizes DETR for real-time use via a hybrid encoder and multi-scale feature fusion.

%\subsection{Segment Anything Model}
%\label{sec:sam}

Segment Anything Models (SAMs) are advanced segmentation models capable of segmenting any object in images using prompts such as points, boxes, or text____. Trained on the SA-1B (1 billion masks, 11 million images), SAM enables zero-shot inference and often surpasses fine-tuned methods in accuracy and efficiency____. Its architecture features a ViT for image encoding, a prompt encoder to process input prompts, and a mask decoder that fuses features from both to generate segmentation masks. SAM 2____, trained on the SA-V dataset (50.9k videos, 642.6k masks), enhances video segmentation and object tracking by refining multi-scale feature extraction. Mobile SAM____ optimizes SAM for mobile use by simplifying the image encoder and using decoupled distillation, enhancing speed without compromising segmentation quality.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/map.png}
    \caption{Geographic Locations of Study Sites. The left panel shows a map of Ecuador with red stars marking the study regions. The right panels zoom in on 21 study areas within four ecological sites.}
    \label{fig:map}
\end{figure}