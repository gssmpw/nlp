\section{Related Work}
\label{sec:related}


\subsection{Palm Detection and Localization}

Advancements in UAV technology, image stitching, and machine learning have driven significant progress in palm detection, segmentation, and localization from orthomosaic imagery. However, most studies have focused on commercially valuable species, such as oil and date palms, given their economic importance ~\cite{li2016deep,gibril2021deep,zheng2021growing,jintasuttisak2022deep,putra2023automatic}. For instance,~\citeauthor{li2016deep}~\cite{li2016deep} used CNNs with a sliding window for oil palm counting in Malaysia, while~\citeauthor{gibril2021deep}~\cite{gibril2021deep} developed a U-Net variant for enhanced date palm segmentation in UAE. \citeauthor{zheng2021growing}~\cite{zheng2021growing} proposed a Faster R-CNN variant with refined feature extraction and a hybrid class-balanced loss to monitor individual oil palm growth. More recently, YOLO-based approaches have been adopted:~\citeauthor{jintasuttisak2022deep}~\cite{jintasuttisak2022deep} applied YOLOv5 for detecting date palms from UAV imagery over UAE farmlands, while~\citeauthor{putra2023automatic}~\cite{putra2023automatic} employed YOLOv3 to detect and count oil palm trees for sustainable agricultural monitoring in Indonesia.

In contrast, the detection and localization of naturally occurring palms in tropical forests is largely underexplored. \citeauthor{tagle2019identifying}~\cite{tagle2019identifying} pioneered palm crown identification using random forest, showing machine learningâ€™s potential for individual palm counting. \citeauthor{ferreira2020individual}~\cite{ferreira2020individual} applied a fully convolutional neural network with morphological operations to refine palm species segmentation. \citeauthor{wagner2020regional}~\cite{wagner2020regional} leveraged U-Nets and very high-resolution (0.5 m) multispectral imagery from the GeoEye satellite to map canopy palms over a large region of the Amazon rainforest.

\subsection{Object Detection and Zero-Shot Segmentation}
\label{sec:object}

Object detection, a core computer vision task, identifies and localizes objects via bounding boxes~\cite{zou2023object} and underpins advanced applications such as image segmentation and object tracking~\cite{wang2022sygnet,ma2024rethinking,li2024CPDR}. The field is dominated by methods using You Only Look Once (YOLO) and Detection Transformer (DETR). 

The YOLO family frames detection as a regression task balancing speed and accuracy. These methods often generate overlapping detections, which are typically resolved by a handcrafted process known as non-maximum suppression (NMS). YOLOv8~\cite{yolov8} enhances detection through advanced backbone and neck architectures for feature fusion, and an anchor-free detection head optimized for accuracy and speed. YOLOv9~\cite{yolov9} introduces programmable gradient information and the generalized efficient layer aggregation network to address information loss. YOLOv10~\cite{yolov10} eliminates NMS through consistent dual assignments during training and one-to-one inference matching, coupled with a refined CSPNet backbone and a lightweight classification head to reduce computational cost. YOLO11~\cite{yolo11} further enhances performance with a refined CSP bottleneck, hybrid attention, and adaptive anchors with extended IoU loss.

DETR~\cite{carion2020end} directly predicts object sets using learned queries, bypassing the need for post-processing such as NMS. DINO~\cite{zhang2022dino} enhances DETR with contrastive denoising and hybrid query initialization, while DDQ-DETR~\cite{zhang2023dense} introduces dense query assignment for improved one-to-one inference matching. RT-DETR~\cite{zhao2024detrs} optimizes DETR for real-time use via a hybrid encoder and multi-scale feature fusion.

%\subsection{Segment Anything Model}
%\label{sec:sam}

Segment Anything Models (SAMs) are advanced segmentation models capable of segmenting any object in images using prompts such as points, boxes, or text~\cite{kirillov2023segment,ravi2024sam,mobile_sam}. Trained on the SA-1B (1 billion masks, 11 million images), SAM enables zero-shot inference and often surpasses fine-tuned methods in accuracy and efficiency~\cite{kirillov2023segment}. Its architecture features a ViT for image encoding, a prompt encoder to process input prompts, and a mask decoder that fuses features from both to generate segmentation masks. SAM 2~\cite{ravi2024sam}, trained on the SA-V dataset (50.9k videos, 642.6k masks), enhances video segmentation and object tracking by refining multi-scale feature extraction. Mobile SAM~\cite{mobile_sam} optimizes SAM for mobile use by simplifying the image encoder and using decoupled distillation, enhancing speed without compromising segmentation quality.

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/map.png}
    \caption{Geographic Locations of Study Sites. The left panel shows a map of Ecuador with red stars marking the study regions. The right panels zoom in on 21 study areas within four ecological sites.}
    \label{fig:map}
\end{figure}