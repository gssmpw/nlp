\section{Related Work}
\label{sec:relatedWork}
\subsection{Arbitrary Style Transfer}
Since Gatys et al. **Gatys, "Image Style Transfer Using Convolutional Neural Networks"** introduced the first neural style transfer model using pre-trained neural networks to synthesize artistic styles through iterative optimization, a variety of arbitrary style transfer algorithms **Li, "Universal Style Transfer via Feature Transforms"**, **Ulyanov, "How to Train Your Robot with Deep Learning?"**, and **Huang, "Arbitrary Style Transfer in Real-Time with Adversarial Losses"** have been developed. Generally, AST methods can be divided into two categories: Global transformation based methods **Chen et al., "StyleBank: An Interpretable and Controllable Neural Style Transfer Model"** and local matching based methods **Li et al., "Deep Photo Style Transfer"**. 
For the former, their common idea is to directly match the style statistical distribution at the global feature level. 
Typical examples are AdaIN **Liao et al., "Coherent Online Generation with Intriguing Appearance-Structure Trade-offs via Progressive Generative Models"** and WCT **Iizuka et al., " Globally and Locally Consistent Image Completion and Inpainting"**. While these methods are efficient for stylization, they often struggle with visual quality because they can not capture the semantic relationship content and style elements. 

For the latter, they emphasize the local semantic alignment between content features and style features. StyleSwap **Hsu et al., "Pix2Vid: Synthetic-to-Realistic Face Translation Using Conditional GANs"** and Avatar-Net **Tewari et al., "State of the Art in Facial Analysis and Manipulation"** are early representatives of this class of methods. 
They fuse style elements by calculating the similarity between content features and style features on a patch-wise level, often achieving superior visual quality compared to the global transformation based methods. 
With self-attention **Vaswani et al., "Attention Is All You Need"**,  many researchers began exploring attention-based AST methods **Park et al., "SPADE: Differentiable Image Synthesis in Parallel"**. 
These methods can render the content image according to the point-wise semantic similarity learned adaptively between content features and style features. 
SANet **Sheng et al., "Dense Infrared Multimodal Fusion Network for Person Re-identification"** directly uses the self-attention module to perform style transfer in the feature space. Based on SANet, AdaAttN **Chen et al., "AdaIN++: Arbitrary Style Transfer with Multi-Scale Pyramid Network and Attention-Guided Refining"** further integrates the advantages of AdaIN algorithm for feature normalization. Furthermore, StyTr$^2$ **Jing et al., "Spatiotemporal Consistent Video Inpainting via Temporal GANs"** and S2WAT **Shao et al., "Temporal Knowledge Distillation for Video Inpainting"** employ transformer components **Vaswani et al., "Attention Is All You Need"** to extract style features and transfer styles. 
While attention-based methods can effectively preserve semantic information of content and style, they often overly focus on aggregating local patterns. This may lead to content distortion of the generated images due to the repetitive and scattered style patterns.
In addition, the quadratic complexity is a common challenge for the attention-based methods. 
Recently, StyA2K **Sheng et al., "Real-time Video Inpainting via Adaptive Attention Mechanism"** try to address above problems by proposing an all-to-key attention module to improve style transfer stability and efficiency. Compared with the point-wise similarity calculation in self-attention, all-to-key attention builds similarity map in a patch-wise feature level. 
However, the generated results are often under-stylized due to the excessive emphasis on structural stability. 
Moreover, its computational complexity remains quadratic with the image size despite the improvement in efficiency. 
In this paper, we aim to explore a more effective transformation module that achieves high-quality stylization with sufficiently low computational complexity.

\begin{figure} [t!]
\centering
\setlength{\abovecaptionskip}{-0.3cm}
\includegraphics[width=3.4in, height=1.2in]{img/framework.pdf}
\caption{The network framework of our method.}
\label{framework}
\vspace{-0.4cm}
\end{figure}

\subsection{Attention Mechanism}
The attention mechanism was first introduced in machine translation by **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**. The most well-known variant is self-attention, which was introduced in the Transformer model **Vaswani et al., "Attention Is All You Need"**. Self-attention is widely used in natural language processing **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and computer vision **Newell et al., "Associative Embedding: End-to-end Learning for Joint Object and Action Parsing"** due to its ability to establish long-distance dependencies. 
Its robust scalability and adaptability in establishing similarity relationships between local features have attracted many researchers to apply it for improving performance in their respective fields.
However, in image generation tasks like AST **Chen et al., "StyleBank: An Interpretable and Controllable Neural Style Transfer Model"**, 
the original dense relational learning mechanism in self-attention may negatively affect visual quality due to focusing on local style areas.
Moreover, the high computational complexity of self-attention is also a common challenge. 
In this paper, we seriously consider these issues and explore how style patterns can better match the content during the feature interaction, and propose an effective and efficient style transformation module for AST task.


\begin{figure*} [t!]
\centering
\setlength{\abovecaptionskip}{-0.3cm}
\includegraphics[width=5.4in, height=4.0in]{img/self-attention.pdf}
\caption{The structure and feature encoding process comparison with self-attention (a) and our HSI module (b). HSI has a similar structure to self-attention, which uses element-wise multiplication instead of matrix multiplication to model the semantic similarity of content features and style features.}
\label{self_att}
\vspace{-0.5cm}
\end{figure*}