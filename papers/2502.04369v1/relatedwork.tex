\section{Related Work}
\label{sec:relatedWork}
\subsection{Arbitrary Style Transfer}
Since Gaty et al. \cite{gatys} introduced the first neural style transfer model using pre-trained neural networks to synthesize artistic styles through iterative optimization, a variety of arbitrary style transfer algorithms \cite{artflow,cl1,IEC,styleformer,Avatar-net,deep_reshufffle} have been developed. Generally, AST methods can be divided into two categories: Global transformation based methods \cite{adain,wct,gt_ast1,gt_ast2,efdm} and local matching based methods \cite{Avatar-net,chenqianqi,sanet,aespa-net,stytr2}. 
For the former, their common idea is to directly match the style statistical distribution at the global feature level. 
Typical examples are AdaIN \cite{adain} and WCT \cite{wct}. 
Specifically, AdaIN directly matches the mean and variance between content and style features at the channel level. Similarly, WCT introduces whitening and coloring operations to align the overall style distribution. While these methods are efficient for stylization, they often struggle with visual quality because they can not capture the semantic relationship  content and style elements. 

For the latter, they emphasize the local semantic alignment between content features and style features. StyleSwap \cite{chenqianqi} and Avatar-Net \cite{Avatar-net} are early representatives of this class of methods. 
They fuse style elements by calculating the similarity between content features and style features on a patch-wise level, often achieving superior visual quality compared to the global transformation based methods. 
With self-attention \cite{transformer} gaining popularity across various fields,  many researchers began exploring attention-based AST methods \cite{adaattn,sanet,IEC}. 
These methods can render the content image according to the point-wise semantic similarity learned adaptively between content features and style features. 
SANet \cite{sanet} directly uses the self-attention module to perform style transfer in the feature space. Based on SANet, AdaAttN \cite{adaattn} further integrates the advantages of AdaIN algorithm for feature normalization. Furthermore, StyTr$^2$ \cite{stytr2} and S2WAT \cite{s2wat} employ transformer components \cite{vision_transformer} to extract style features and transfer styles. 
While attention-based methods can effectively preserve semantic information of content and style, they often overly focus on aggregating local patterns. This may lead to content distortion of the generated images due to the repetitive and scattered style patterns.
In addition, the quadratic complexity is a common challenge for the attention-based methods. 
Recently, StyA2K \cite{stya2k} try to address above problems by proposing an all-to-key attention module to improve style transfer stability and efficiency. Compared with the point-wise similarity calculation in self-attention, all-to-key attention builds similarity map in a patch-wise feature level. 
However, the generated results are often under-stylized due to the excessive emphasis on structural stability. 
Moreover, its computational complexity remains quadratic with the image size despite the improvement in efficiency. 
In this paper, we aim to explore a more effective transformation module that achieves high-quality stylization with sufficiently low computational complexity.

\begin{figure} [t!]
\centering
\setlength{\abovecaptionskip}{-0.3cm}
\includegraphics[width=3.4in, height=1.2in]{img/framework.pdf}
\caption{The network framework of our method.}
\label{framework}
\vspace{-0.4cm}
\end{figure}

\subsection{Attention Mechanism}
The attention mechanism was first introduced in machine translation by Bahdanau et al \cite{att2015}. The most well-known variant is self-attention, which was introduced in the Transformer model \cite{transformer}. Self-attention is widely used in natural language processing \cite{nlp_att1,nlp_att2,nlp_att3,nlp_att4} and computer vision \cite{cv_att1,cv_att2,cv_att3,cv_att4} due to its ability to establish long-distance dependencies. 
% Its strong scalability and adaptivity in building the similarity relationships between local features have led many researchers to apply it to improve performance in their research area. 
Its robust scalability and adaptability in establishing similarity relationships between local features have attracted many researchers to apply it for improving performance in their respective fields.
However, in image generation tasks like AST \cite{sanet,adaattn}, 
the original dense relational learning mechanism in self-attention may negatively affect visual quality due to focusing on local style areas.
Moreover, the high computational complexity of self-attention is also a common challenge. 
In this paper, we seriously consider these issues and explore how style patterns can better match the content during the feature interaction, and propose an effective and efficient style transformation module for AST task.


\begin{figure*} [t!]
\centering
\setlength{\abovecaptionskip}{-0.3cm}
\includegraphics[width=5.4in, height=4.0in]{img/self-attention.pdf}
\caption{The structure and feature encoding process comparison with self-attention (a) and our HSI module (b). HSI has a similar structure to self-attention, which uses element-wise multiplication instead of matrix multiplication to model the semantic similarity of content features and style features.}
\label{self_att}
\vspace{-0.5cm}
\end{figure*}