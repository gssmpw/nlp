\section{Conclusions}
This paper presents the first study to examine the security awareness of three popular LLMs when answering programming-related questions.
%In this work, we offer a first look into the security awareness with which LLMs respond to programming questions. Our motivational study showed that ChatGPT rarely mentions security when answering developers' questions. %and that often these mentions are limited to certain types of vulnerabilities.
In our motivational study, ChatGPT demonstrated potential in offering developers relevant security-related information and context-based solutions for writing secure code. 
%Our motivational study showed ChatGPTâ€™s potential to identify security vulnerabilities and provide context-specific information to developers, promoting safe coding practices.
These findings inspired us to conduct a deeper investigation into the capabilities of other popular LLMs.
We prompted three popular LLMs, \gpt, \llama and \claude, with SO questions containing vulnerable code and evaluated their responses. 

Our results indicated the underwhelming performance of all three LLMs in pointing out the security vulnerabilities. We observed even lower performances when prompting LLMs with questions from Transformed-Dataset which, indicates the limitations of these models in generalizing their learned data. 
Additionally, we observed that LLMs are more likely to point out certain vulnerabilities, specially those related to the improper management and protection of sensitive information (e.g., CWE-532, CWE-321, CWE-798) compared to those involving external control of file names or paths (e.g., CWE-400). 
%such as CWE-532 (Insertion of Sensitive Information in Log File), CWE-321 (Use of Hard-coded Cryptographic Key), and CWE-798 (Use of Hard-Coded Credentials). Is to be noted that, in instances where LLMs identified the vulnerabilities, they provided more comprehensive information than Stack Overflow posts. 
%Evaluating the security warnings offered by the LLMs, we found that all models consistently detailed the causes, potential exploits, and possible fixes of vulnerabilities, to better inform users. 
%This information was often comprehensive and therefore more helpful in raising user's awareness compared to the responses found on SO.
% Lastly, we conducted a case study investigating the performance of all LLMs in detecting vulnerabilities, given slightly modified prompts. By adding the phrase ``Address security vulnerabilities" to the end of each question, LLMs detected 40\% of the vulnerabilities for questions in Transformed-Dataset where no LLM originally issued a security warning.
% Finally, we discussed the security implications of our findings in future tool designs. We also suggest a simple, practical prompt engineering technique that developers can adopt to receive more security-aware LLM responses. 

While some of our findings align with existing research on security flaws in LLM-generated content, our approach addresses a crucial gap by focusing on real-world scenarios where developers seek general coding assistance without explicitly considering security, which is common in interactions. By selecting prompts without explicit security mentions, we simulate typical developer interactions in which security concerns are often overlooked, highlighting a key risk: \textit{LLMs often overlook insecure practices and inadvertently reinforce them}. This insight, when coupled with the prompting techniques we propose, could help developers elicit more security-aware responses from LLMs, thus promoting secure coding practices. 

In general, our findings have the following key implications. For software engineering practitioners, prompt engineering can be a practical tool to promote security-aware LLM outputs, though limitations remain; integrating tools like CodeQL may further aid developers in outlining vulnerabilities. For researchers, the results point to the importance of evaluating both code and natural language guidance from LLMs and highlight opportunities for fine-tuning models to generate more security-aware responses. For LLM designers, there is a need for models that proactively identify security issues, perhaps through enhanced training or fine-tuning with high-quality, domain-specific data. 
\newline\newline
\textbf{Data Availability:} The replication package of our study, including the datasets, code, and analysis instructions are available at: \url{https://figshare.com/s/4f85174699540324fc4d}.