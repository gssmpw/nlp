\section{Implications}

%In this section, we explore the implications of our findings from RQ1 and RQ2, grouped into three categories:
Our findings demonstrate critical considerations for leveraging LLMs in SE context. Our study reveals both the potential and limitations of current LLMs in addressing vulnerabilities and informing developers about them. In this section, we discuss actionable insights for developers and model designers aiming to enhance the security awareness of LLMs. These insights not only highlight immediate steps practitioners can take but also point to broader directions for future research and development. We group these implications into three key areas, each addressing a specific stakeholder group: software engineers using LLMs (Section \ref{implications_address_security}), LLM-based tool integrators (Section \ref{implications_tool_integration}), and LLM designers (Section \ref{implications_llm_designers}). Additionally, in Sections \ref{implications_address_security} and \ref{implications_tool_integration} we analyze a total of 100 LLM prompts and responses, experimenting with possible methods of improving security awareness in LLMs' output. Further, we demonstrate a CLI-based tool that serves as a prototype to help achieve these security enhanced responses.

\subsection{Implications for Software Engineers}
\label{implications_address_security}
% Manual Prompt Eng
Our study highlights the critical need for developers to be vigilant about the security of the programming solutions suggested by LLMs. Notably, in RQ1 we found that LLMs rarely issue security warnings autonomously, identifying vulnerabilities in fewer than 40\% of cases under the more favorable experimental settings. This limited detection rate highlights a critical risk: engineers relying solely on LLMs may overlook key security flaws, leaving code vulnerable to exploitation.

One way to make LLM responses more secure could be by providing specific instructions in the prompt \citep{jensen2024software}. 
%One approach to enhancing the security of LLM responses could be for practitioners to deliberately refine their prompts to include specific instructions. %Given certain prompts, LLMs can be more likely to not only address the security vulnerabilities in the prompt but also generate more secure responses. However, 
However, several studies show that developers struggle to create effective LLM prompts ~\citep{chopra2023conversationalchallengesaipowereddata, Nam2024}. Therefore, we experimented with brief prompt additions that would be straight-forward for developers to implement.

% Case Study - Data Sampling
%To explore whether developers can obtain more secure responses from LLMs by modifying their prompts, we experimented with various possibilities in manual prompting. 
We sampled 50 questions from our datasets where none of the LLMs issued any security warnings. To ensure representativeness, we selected 25 instances from the Mentions-Dataset and 25 instances from the Transformed-Dataset. Further, these questions were sampled proportionately to the distribution of vulnerabilities within the corresponding subsets of questions. Next, these prompts were fed to \gpt, the best performing model in our experiments.

We experimented with multiple prompt structures, such as appending the phrase \textit{``Be mindful of potential security implications."} or \textit{``Make the code secure."}, aiming to find a structure that not only maximized the likelihood of LLMs addressing security vulnerabilities but was also concise enough for practical use by developers. We found that the best performing technique included appending the phrase \textit{``Address security vulnerabilities"} to the end of each question. Consequently, we used all modified questions as prompts for three LLMs, resulting in 50 responses. Next, we analyzed the LLM responses to these questions with respect to the RQ1 and RQ2 metrics. The analysis of these 50 responses was performed by two of the authors and followed the same methodology used in answering RQ1 and RQ2 in the main study.

% Editing numbers
Out of the 25 questions from the Mentions-Dataset, \gpt was able to provide security warnings for 19 questions. For the Transformed-Dataset, there was a much less notable enhancement in performance; \gpt issued security warnings for 6 out of the 25 questions. Despite these improvements, \gpt still struggled to identify and warn users about the existence of more than 76\% of the vulnerabilities in the Transformed-Dataset. This indicates that while prompt engineering enhances security awareness, significant limitations remain in LLMs' ability to warn users about vulnerabilities. A significant factor is the challenge LLMs face in accurately detecting vulnerabilities in the first place. Research indicates that even the most advanced  techniques can only achieve an accuracy of 67.7\% \citep{zhou2024large, zhou2024out}.

% Results - RQ2
It is worth noting that when vulnerabilities were pointed out, the LLM always included information about the \textit{causes} and \textit{fixes} in its responses. Exploitations, however, were pointed out in 13/25 instances. In one of the instances, when a user sought help for a Python issue involving an API request and the Flask framework, \gpt addressed the functional problem but also pointed out a security concern, cautioning the user to validate and sanitize all inputs to avoid potential vulnerabilities. Specifically, the model pointed out that directly using user-provided URLs in HTTP requests without proper validation could lead to a Server-Side Request Forgery (SSRF) attack. As such, \gpt identified the cause of the issue (unsanitized user input), recommended input validation as a fix, and noted the potential exploitation through SSRF.% For instance, \llama provided information about the causes, exploits, and fixes for 8/8, 6/8, and 8/8 of the vulnerabilities in the questions of Mentions-Dataset. Similarly, \gpt detailed the causes, exploits, and fixes of all four instances where it pointed out vulnerabilities in the Transformed-Dataset. %These examples suggest that despite the limitations of LLMs in pointing out vulnerabilities, once identified, the LLMs are effective at conveying sufficient security-related information.

% Takeaway
The overall performance of \gpt using the ``security-aware'' prompts highlights the potential influence of prompts on the security of the LLM responses. Although practitioners can rely on such simple prompt engineering techniques to implement more secure programming solutions, they should exercise caution as this method, in some instances, does not result in more secure LLM responses.

% Limitations
% Our findings reveal significant gaps in the security awareness of LLMs, indicating a need for improvements in their design and training. While we found that LLMs can be powerful tools in describing the causes, exploits, and fixes of vulnerabilities, this effectiveness is contingent upon the LLMs' ability to identify vulnerabilities and warn the user in the first place (40\% in the best case). The performance gets worse when LLMs encounter unseen data or new programming problems (as low as 12.6\%). Moreover, we found that certain vulnerabilities, such as CWE-312 and CWE-798, are more likely to be identified by LLMs compared to other CWEs.

% Nonetheless, we can still foster better performance in various tasks by strategically manipulating prompts, thus optimizing the immediate utility of current LLM capabilities. In particular, grounding the prompts, by adding additional contextual or analytical information, makes them more specific and meaningful for the LLM \cite{}.



% CodeQL
\subsection{Implications for Tool Integration in Software Development}
\label{implications_tool_integration}
% \Experiments with the CodeQL?

% LLM Designs


% Integrating Static Analysis Tools into Prompts
% To address this effectively and immediately, we propose an experimental approach that integrates static analysis tools, such as CodeQL, into the prompt generation process for LLMs. By automatically running these tools on the code snippets provided in prompts and including the vulnerability analysis in the input, we can significantly enrich the context for the LLM. This method serves as an extension of the Retrieval Augmented Generation (RAG) technique, leveraging existing security data to enhance the model’s output. This not only improves the relevance and security-awareness of the responses but also aligns with ongoing efforts to augment LLM capabilities through enriched contextual data.
%However, to effectively and immediately 
To address the limitations of LLMs in identifying and mitigating vulnerabilities, alternative approaches have been proposed. 
%For instance, Pearce et al. have demonstrated the integration of outputs from static analysis tools, such as CodeQL and C sanitizers, into LLM prompts \cite{pearce2023examining}. This approach was implemented as a zero-shot strategy, where LLMs were tasked with fixing vulnerabilities. 
For instance, Pearce et al. demonstrated integrating outputs from static analysis tools, such as CodeQL and C sanitizers, into LLM prompts as a zero-shot strategy for fixing vulnerabilities \cite{pearce2023examining}. 
Building on the same idea, we explored using this technique to examine how well LLMs provide relevant security warnings and information when responding to general programming questions.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{side_by_side_example.pdf}
    \caption{\textcolor{blue}{Comparison of the LLM’s original functionality-focused response (left) and its subsequent security-focused response (right). On the right, we observe that explicit mention of vulnerability caused the model to overlook the main question.}}
    \label{fig:side_by_side_example}
    % \vspace{-1em}
\end{figure*}

% "Address Vulnerability: Des
We use the same 50 questions used in Section \ref{implications_address_security} to experiment with this approach. Having analyzed each question's extracted code snippets with CodeQL, we add CodeQL's description of the detected vulnerabilities to the end of each question, using the phrase \textit{``Address security vulnerability: \textless Vulnerability Description \#1\textgreater{} Address security vulnerability: \textless Vulnerability Description \#2\textgreater{}..."}. After prompting the \gpt with these prompts, the LLM responses were again analyzed with respect to RQ1 and RQ2. Here, similar to the previous section, the authors manually carried out the analysis.

% Results
Upon providing \gpt with the output of CodeQL, it was able to identify and address vulnerabilities in 24/25 questions from the Mentions-Dataset. Further, the \gpt was able to identify and address 20/25 vulnerabilities for the Transformed-Dataset. However, for six questions, the LLMs ignored the CodeQL output and proceeded to answer the main question without necessarily fixing the code. Consistent with our previous results, the model provided information about the causes, exploits, and the fixes of almost all the vulnerabilities it pointed out. Overall, this technique significantly performed better than the manual prompt engineering technique (Section \ref{implications_address_security}).


% \textcolor{blue}{We also examined whether emphasizing security affected the overall functional correctness of the LLM's answers. In 45 out of the 50 augmented responses, \gpt still adequately addressed the user's original query while highlighting or fixing security vulnerabilities. However, in 4 instances, the LLM primarily focused on resolving the discovered vulnerabilities, neglecting the user’s main question.Figure \ref{fig:side_by_side_example} illustrates one of these scenarios. In this example, the user asks about a data-processing task and includes a snippet that inserts user-provided text into the DOM, i.e., an XSS risk. Without any security-specific instruction, the LLM simply solves the data-processing problem while ignoring the XSS. However, after we add the instruction “Address security vulnerability: DOM text reinterpreted as HTML,” the LLM emphasizes sanitizing user input and preventing XSS while neglecting the user’s original question. This behavior illustrates a potential trade-off between functionality and security, highlighting the challenge of balancing the two objectives in real-world usage.}
\textcolor{blue}{We also examined whether modifying the prompts affected the overall functional correctness of the LLM's answers. In 46 out of the 50 responses, \gpt adequately addressed the user’s primary concern while fixing and/or acknowledging the security vulnerabilities. However, in 4 instances, the LLM’s response only addressed the security issue and completely overlooked the user’s main query. Figure \ref{fig:side_by_side_example} illustrates one such scenario where the question seeks guidance on preventing multiple database entries for the same user. Note that the user's code insecurely handles logging credentials, introducing a potential security risk. Without a security-specific prompt, the LLM addresses the core issue, ignoring the vulnerability (Figure \ref{fig:side_by_side_example} - left panel). After integrating the CodeQL findings into the prompt, the LLM only addresses the logging and credential handling vulnerabilities while neglecting the main question (Figure \ref{fig:side_by_side_example} - right panel). This example demonstrates how emphasizing security can sometimes overshadow the original problem and highlights the challenge of balancing these objectives in real-world usage.}

%The comparable performance between this approach and the manual prompt engineering (Section \ref{implications_address_security}) indicates the possibility of an automatic process that enables more security-aware LLM responses. 
\textcolor{blue}{Despite the challenges, several opportunities exist for integrating LLMs with SE tools in the development workflows.} For instance, static analysis tool like CodeQL can be used to preprocess code snippets to outline specific security vulnerabilities in the prompt. The identified vulnerability can then be added as meta data to the prompt, providing the LLM with clearer directions on what issues need addressing. Such integration could enhance the LLM's ability to generate secure code and streamline vulnerability detection and mitigation.  

To further demonstrate the potential of this approach in practice, we developed a prototype CLI-based tool that automates the integration of static analysis feedback into LLM prompts. The tool automatically extracts the Python or JavaScript code from developer queries, analyzes the code snippets using CodeQL, and integrates the detected vulnerabilities into the LLM’s prompts using the above-mentioned prompt structure. Figure \ref{fig:prototype_screenshot} shows an example interaction using our prototype, highlighting the security enhanced response from the \gpt. In this example, the developer's prompt does not mention any security risks. When prompted directly with this question, \gpt does not address this issue in the user's code. However, after integrating the CodeQL feedback, the LLM not only identified the risk of log injection but also warned the user about its potential exploits and suggested fixes, such as sanitizing user input using the \textit{sanitize} function from the \textit{validator} library. This example demonstrates how integrating static analysis tools into LLM pipelines can effectively enhance security awareness in LLM responses and increase the proactive mentions of insecure vulnerabilities in developers' code.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig_pipeline_white.pdf}
    \caption{Left: The original user prompt featuring vulnerable code. Right: The LLM's response after prompt modification (integrating CodeQL feedback). Both the prompt and response have been abbreviated for presentation clarity.}
    \label{fig:prototype_screenshot}
    % \vspace{-1em}
\end{figure*}


% \subsection{Implications for LLM Designs}
%\subsection{Implications for Automated Prompt Engineering}
%\subsection{Using LLMs for Programming Assistance}
\subsection{Implications for LLM Designers}
\label{implications_llm_designers}

Overall, our findings highlight substantial gaps in the security awareness of LLMs, emphasizing the need for design and training improvements. Based on the results of RQ1, the best performance we observed in issuing security warnings was only 40\%, which declines even further in the Transformed-dataset, dropping to as low as 12.6\%. This significant disparity in performance across datasets suggests that LLMs require substantial modifications to improve the attention to security in their responses. Addressing these gaps might involve acquiring additional high-quality training data, or domain-specific fine-tuning for SE. %, or even new model architectures. %, such enhancements are outside the scope of this discussion. 

Moreover, our study suggests that LLMs' lack of attention to security extends beyond code generation to the natural language they produce in software engineering-related discussions. This broader issue indicates the need to evaluate the security of LLM-generated content, particularly in the explanations, advice, and recommendations provided to developers. Future research could establish standardized methods for assessing LLM security awareness within software engineering contexts, focusing not only on the code produced but also on the security of LLM-provided guidance. This dual approach to security—encompassing both code and conversational context—could enable more robust, security-conscious LLM designs that better serve practitioners in high-stakes environments.