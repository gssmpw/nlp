
\begin{table*}[]
\centering
\scalebox{1.0}
{
\begin{tabular}{c|c|cc|cccc|c}
\midrule
Methods      & LLM                               & \#Sample & \#Param                            & GQA  & VQA  & VisWiz & SQA  & AVG \\ \midrule
BLIP-2       & Vicuna-13B                        & 129M     &                                    & 41.0 & 42.5 & 19.6   & 61.0 &   41.0  \\
InstructBLIP & Vicuna-13B                        & 130M     &                                    & 49.5 & 50.7 & 33.4   & 63.1 &  49.2   \\
Qwen-VL-Chat & Qwen-7B                           & 1450M    &                                    & 57.5 & 61.5 & 38.9   & 68.2 &  56.5   \\
LLaVA-1.5-7B & Vicuna-1.5-7B                     & 1.2M     &                                    & 62.0 & 58.2 & 50.0   & 66.8 &  59.3   \\
LLaVA-Next   & Vicuna-1.5-13B                    & 1.3M     & \multirow{-5}{*}{\textgreater{}7B} & 65.4 & 67.1 & 60.5   & 73.6 & 66.7    \\ \midrule
LLaVA-3.2-3B & LLaMA-3.2-3B                      & 0.6M     & 3B                                 & 54.7 & 51.2     &   35.8     &    63.4  &  51.3   \\
LLaVA-3.2-1B & LLaMA-3.2-1B                      & 0.6M     & 1B                                 & 48.6 &  49.6    &  32.6      &  63.0    & 48.5    \\
Trans-LLaVA  & Mamba-0.6B & 0.6M     & 0.6B                               &  50.2    &  49.7    &  34.8      & 62.9     & 49.4    \\ \midrule
\end{tabular}}
\caption{
Comparison with state-of-the-art MLLMs on the commonly-used multimodal benchmarks for MLLMs.
\#Sample: Training data sample. 
\#Param: Trainable parameters. 
% Our Trans-LLaVA achieves the result with the fewest model parameters.
}
\label{tab:resulits_llava}
\end{table*}