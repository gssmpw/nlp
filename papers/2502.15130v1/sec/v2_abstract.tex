\begin{abstract}
% %%%% Transformer widly used
% Transformers have been favored in both uni-model and multi-model foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, such as DEIT, CLIP and Stable Diffusion are publicly available. 
% %%%% the transformer's problem
% However, they still face challenges due to the quadratic computational complexity of the attention mechanism, leading to increased computational costs and memory usage. 
% %%%% SSM model appear to solve or alleviate above problem
% % Recent research has introduced the Mamba model architecture, which is based on the State Space Model (SSM) and allows for global awareness with linear complexity. 
% Recent research has introduced subquadratic architectures, such as Mamba, which allows for global awareness with linear complexity. 
%%%%%%%% too redundant above, simplicity
%%%%transformer widly used, problem, subquadratic appear
Transformers have been favored in both uni-modal and multi-modal foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, are publicly available. 
Recent research has introduced subquadratic architectures like Mamba, which enables global awareness with linear complexity.
%%%% scratch training cause resource-cost and time-consuming
Nevertheless, training specialized subquadratic architectures from scratch for certain tasks is both resource-intensive and time-consuming.
%%%%  The main idea is to faciliate the mamba from transformer models
As a motivator, we explore cross-architecture training to transfer the ready knowledge in existing Transformer models to alternative architecture Mamba, termed \myMethodName.
%%%% findings(intro), the main technology(no detail), advantages(data, precision, lack methods to solve architecture-consistency
% This delve into cross-architecture training, specifically the transfer of knowledge from Transformers to Mamba, proposing a training mechanism termed \myMethodName. 
% Our key idea is that both Transformers and Mamba can be seen as applying different forms of mixing matrices over the images. 
Our approach employs a two-stage strategy to expedite training new Mamba models, ensuring effectiveness in across uni-modal and cross-modal tasks.
%% incomplete representation
Concerning architecture disparities, we project the intermediate features into an aligned latent space before transferring knowledge.
% We first match hidden units across different levels and 
% Besides, initialize the Mamba model using part of the non-attention weights from the Transformer. 
On top of that, a Weight Subcloning and 
% Adaptive Forward/Backward distillation method (WSAFB) 
Adaptive Bidirectional distillation method (WSAB)
is introduced for knowledge transfer without limitations on varying layer counts.
% A new adaptive knowledge distillation method is proposed to dynamically transfer knowledge from the Transformer to all layers of Mamba.
%% 感觉可以在Mamba2的基础上（Qkv感觉比较直接）或者结合LLaVA，加强文本与视觉的交互能力？文本与视觉的token拼接后在attention与ssm之间的计算方式是不同的，直接替换，ssm的劣势是啥？如何加强？
For cross-modal learning, we propose a cross-Mamba module that integrates language awareness into Mamba’s visual features, enhancing the cross-modal interaction capabilities of Mamba architecture.  
% To expedite training, we stack Mamba modules and cross-Mamba modules, transferring knowledge from the self-attention and cross-modal modules of pre-trained Transformer models through distillation
%%%% experiments
Despite using less than {$75\%$} of the training data typically required for training from scratch, {\myMethodName} boasts substantially stronger performance across various network architectures and downstream tasks, including image classification, visual question answering, and text-video retrieval. The code will be publicly available.

\end{abstract}