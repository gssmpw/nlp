% \begin{abstract}
% % %%%% Transformer widly used
% % Transformers have been favored in both uni-model and multi-model foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, such as DEIT, CLIP and Stable Diffusion are publicly available. 
% % %%%% the transformer's problem
% % However, they still face challenges due to the quadratic computational complexity of the attention mechanism, leading to increased computational costs and memory usage. 
% % %%%% SSM model appear to solve or alleviate above problem
% % % Recent research has introduced the Mamba model architecture, which is based on the State Space Model (SSM) and allows for global awareness with linear complexity. 
% % Recent research has introduced subquadratic architectures, such as Mamba, which allows for global awareness with linear complexity. 
% %%%%%%%% too redundant above, simplicity
% %%%%transformer widly used, problem, subquadratic appear
% Transformers have been favored in both uni-model and multi-model foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, e.g., LLaVA, CLIP and DEIT, are publicly available. 
% Recent research has introduced subquadratic architectures, such as Mamba, which allows for global awareness with linear complexity compared to the quadratic complexity of attention mechanism.
% %%%% scratch training cause resource-cost and time-consuming
% Nevertheless, training specialized subquadratic architectures from  scratch for certain tasks causes additional resource consumption and time-consuming.
% %%%%  The main idea is to faciliate the mamba from transformer models
% As a motivator, we explore transferring the rich knowledge in existing Transformer models to alternative models such as Mamba.
% %%%% findings(intro), the main technology(no detail), advantages(data, precision, lack methods to solve architecture-consistency
% We delve into cross-architecture training, specifically the transfer of knowledge from Transformers to Mamba, proposing a training mechanism termed \myMethodName. 
% % Our key idea is that both Transformers and Mamba can be seen as applying different forms of mixing matrices over the images. 
% This approach include two-phase to expedite the training of new Mamba models, ensuring effectiveness in both uni-model and cross-model tasks.
% %% incomplete representation
% Specifically, we transform intermediate features to project them into an aligned latent space, thereby addressing architecture disparities.
% % We first match hidden units across different levels and 
% % Besides, initialize the Mamba model using part of the non-attention weights from the Transformer. 
% Besides, a weight subcloning and adaptive Forward/Backward distillation method (WSAFB) is introduced for knowledge transfer without limitations on varying layer counts.
% % A new adaptive knowledge distillation method is proposed to dynamically transfer knowledge from the Transformer to all layers of Mamba.
% %% 感觉可以在Mamba2的基础上（Qkv感觉比较直接）或者结合LLaVA，加强文本与视觉的交互能力？文本与视觉的token拼接后在attention与ssm之间的计算方式是不同的，直接替换，ssm的劣势是啥？如何加强？
% For cross-modal tasks, we introduce a cross-Mamba module that integrates language awareness into Mamba’s visual features to address the issue of Mamba architecture lacking cross-modal interaction capabilities. 
% % To expedite training, we stack Mamba modules and cross-Mamba modules, transferring knowledge from the self-attention and cross-modal modules of pre-trained Transformer models through distillation
% %%%% experiments
% Despite using less than {$75\%$} randomly training data typically used to train models from scratch, {\myMethodName} boasts substantially stronger performance across various network architectures and a wild range of downstream tasks, including image classification, visual question answering, and text-video retrieval. The code will be publicly available.

% \end{abstract}


\begin{abstract}
% %%%% Transformer widly used
% Transformers have been favored in both uni-model and multi-model foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, such as DEIT, CLIP and Stable Diffusion are publicly available. 
% %%%% the transformer's problem
% However, they still face challenges due to the quadratic computational complexity of the attention mechanism, leading to increased computational costs and memory usage. 
% %%%% SSM model appear to solve or alleviate above problem
% % Recent research has introduced the Mamba model architecture, which is based on the State Space Model (SSM) and allows for global awareness with linear complexity. 
% Recent research has introduced subquadratic architectures, such as Mamba, which allows for global awareness with linear complexity. 
%%%%%%%% too redundant above, simplicity
%%%%transformer widly used, problem, subquadratic appear
Transformers have been favored in both uni-model and multi-model foundation models for their flexible scalability in attention modules. Consequently, a number of pre-trained Transformer models, e.g., LLaVA, CLIP and DEIT, are publicly available. 
Recent research has introduced subquadratic architectures, such as Mamba, which allows for global awareness with linear complexity compared to the quadratic complexity of attention mechanism.
Nevertheless, training specialized subquadratic architectures from  scratch for certain tasks causes additional resource consumption and time-consuming.
As a motivator, we explore transferring the rich knowledge in existing Transformer models to alternative models such as Mamba.
We delve into cross-architecture training, specifically the transfer of knowledge from Transformers to Mamba, proposing a training mechanism termed TransMamba. 
This approach include two-phase to expedite the training of new Mamba models, ensuring effectiveness in both uni-model and cross-model tasks.
Specifically, we transform intermediate features to project them into an aligned latent space, thereby addressing architecture disparities.
Besides, a weight subcloning and adaptive Forward/Backward distillation method (WSAFB) is introduced for knowledge transfer without limitations on varying layer counts.
For cross-modal tasks, we introduce a cross-Mamba module that integrates language awareness into Mamba’s visual features to address the issue of Mamba architecture lacking cross-modal interaction capabilities. 
Despite using less than $75\%$ randomly training data typically used to train models from scratch, TransMamba boasts substantially stronger performance across various network architectures and a wild range of downstream tasks, including image classification, visual question answering, and text-video retrieval. The code will be publicly available.

\end{abstract}