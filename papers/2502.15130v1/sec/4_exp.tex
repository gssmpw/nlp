\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setup and Details}\label{sec:esd}
In this section, we briefly introduce the datasets, implemetation details, and evaluation metrics for the corresponding models. More details can be found in the Appendix.
\noindent {\bf Datasets. } 
\textbf{Single model.}
For Image classification, we conduct experiments on a diverse set of 3 benchmarks: CIFAR100~\citep{krizhevsky2009learning}, ImageNet100~\citep{deng2009imagenet}, ImageNet1000~\cite{deng2009imagenet}.
\textbf{Multi model.}
For visual question answering, 0.5M general captioning samples from LLaVA-1.5-pretrain dataset and 0.6M general capationing and conversation samples from LLaVA-1.5-finetune~\citep{liu2024visual}, are used in our paper.
Two video-text datasets, MSR-VTT~\citep{DBLP:conf/cvpr/XuMYR16} and DiDeMo~\citep{DBLP:conf/iccv/HendricksWSSDR17}, are employed in our paper.  
% MSR-VTT~\cite{DBLP:conf/cvpr/XuMYR16} is a large-scale dataset designed for open-domain video captioning, comprising 10,000 video clips across 20 categories. 
% Each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. 
% The DiDeMo~\cite{DBLP:conf/iccv/HendricksWSSDR17} dataset includes training, validation, and test sets containing 8,395, 1,065, and 1,004 videos, respectively.
%%%%最后应该是放不开了


% \textbf{Image Dataset.} For Image classification and text-to-image generation, we conduct experiments on a diverse set of 3 benchmarks: CIFAR100~\citep{krizhevsky2009learning}, ImageNet100~\citep{deng2009imagenet}, ImageNet1000~\cite{deng2009imagenet}, CC3M~\citep{DBLP:conf/cvpr/ChangpinyoSDS21}.  
% CIFAR-100 comprises 100 classes, each containing 600 images, with 500 images allocated for training and 100 for testing. ImageNet-100 consists of 100 randomly selected classes from the larger ImageNet-1000 dataset. 
% \textbf{Video Dataset.} Two video-text datasets, MSR-VTT~\citep{DBLP:conf/cvpr/XuMYR16} and DiDeMo~\citep{DBLP:conf/iccv/HendricksWSSDR17}, are employed in our paper.  
% MSR-VTT~\cite{DBLP:conf/cvpr/XuMYR16} is a large-scale dataset designed for open-domain video captioning, comprising 10,000 video clips across 20 categories. 
% Each video clip is annotated with 20 English sentences by Amazon Mechanical Turks. 
% The DiDeMo~\cite{DBLP:conf/iccv/HendricksWSSDR17} dataset includes training, validation, and test sets containing 8,395, 1,065, and 1,004 videos, respectively.

\input{tab/parameters}
\input{tab/backbones}

\begin{figure*}[t]
\centering
  % \begin{subfigure}{0.32\linewidth}
  %   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  %   \includegraphics[width=1\linewidth]{figs/Imagenet100_results5.pdf}
  %   \caption{ImageNet100 5 steps.}
  %   \label{fig:imagenet100-b0-5}
  % \end{subfigure}
  % \hfill
  \begin{subfigure}{0.32\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{figs/results_acc_imnt_base.pdf}
    \caption{PMamba/accuracy}
    \label{fig:Pmamba acc}
  \end{subfigure}
  \hfill
  % \begin{subfigure}{0.32\linewidth}
  %   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  %   \includegraphics[width=1\linewidth]{figs/Imagenet100_results20.pdf}
  %   \caption{B0 20 steps}
  %   \label{fig:imagenet100-b0-20}
  % \end{subfigure}
  % \hfill
  \begin{subfigure}{0.32\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{figs/results_acc_imnt_base_vmamba.pdf}
    \caption{Vmamba/accuracy}
    \label{fig:Vmamba acc}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\linewidth}
    % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \includegraphics[width=1\linewidth]{figs/results_cifar_loss.pdf}
    \caption{PMamba/loss}
    \label{fig:Pmamba loss}
  \end{subfigure}
  \hfill
  \caption{
  The accuracy and loss under different architectures.
  }
  % Left is evaluated with 5 steps, middle with 10 steps, and right with 20 steps.}
  \label{fig:accuracy}
\end{figure*}

\input{tab/backbones_vqa}

% Four benchmarks are used in the experiments: CIFAR100~\cite{}, ImageNet100~\cite{} (Image classification), (Video retrieval), CC3M~\cite{} (text-to-image syntheses) (see Appendix for more details). 

\noindent {\bf Implementation Details.} \quad
For \textbf{Image Classification}, we build our codebase following VMamba~\citep{DBLP:journals/corr/abs-2401-10166}, PlainMamba~\citep{DBLP:journals/corr/abs-2403-17695} and ViM~\citep{DBLP:journals/corr/abs-2401-09417}. 
Specifially, we train all Mamba models for 300 epochs using AdamW~\citep{DBLP:conf/iclr/LoshchilovH19} optimizer with a learning rete of 5e-04. 
% For VMamba, we set the batch size to 32 while 128 for PMamba-T and PMamba-S, 64 for PMamba-B. Cosine learning rate scheduling is used. Most of these experiments were conducted on NVIDIA 3090 GPUs.
% \input{Styles/tab/parameter}
For \textbf{Visual Question Answering}, we employ the pre-trained CLIP-ViT-L/14 \citep{Radford2021LearningTV} as the vision encoder and a two-layer MLP as the projector. Both teacher and student models leverage the LLaMA-3.2 \citep{dubey2024llama} families to construct their base model. Specifially, the teacher model employs a 3B parameter configuration, while the student model is constructed with 0.6B parameter sizes based on LLaMA-3.2-1B. 
We first train the LLaMA-3B model using the LLaVA training methodology to obtain the LLaVA-LLaMA3.2-3B model as the teacher model. Subsequently, we train our Mamba model using only 0.6 million general caption samples.
We set the batch size is 128, and use Adam optimizer with 2e-5 learning rate. Throughout all states, we train on 16 V100 GPUs for an epoch each.
% For Text-to-Image Generation, the code is followed on PixArt~\citep{DBLP:journals/corr/abs-2310-00426} to employ the T5 large language model as the text encoder for conditional feature extraction with the length of text tokens to 120. To capture the latent features of input images, we employ a pre-trained and frozen VAE from LDM~\citep{DBLP:conf/cvpr/RombachBLEO22}. We train 200 epochs using AdamW optimizer with a weight decay of 0.03 and a constant 2e-5 learning rate. Other settings is similar to PixArt. 
% The pre-trained PixArt servers as the frozen teacher.
% The model is trained on 8 A100 GPUs. 
%For Video Retrieval, We train all Mamba models for 300 epochs using the AdamW optimizer~\cite{} with a learning rate of 5e-4.  Cosine learning rate scheduling is utilized.  For VMamba, we set the batch size to 32, while for ClsMambaP-tiny and ClsMambaP-small, it is set to 128, and for ClsMambaP-base, it is set to 64.  All experiments were conducted on 2 NVIDIA 3090 GPUs. In Table \ref{tab:variants}, we present the number of layers and dimension settings for different Mamba configurations for classification tasks, as well as the model parameter scale.  For PlainMamba, we only modified the number of dimensions from tiny to base.
For \textbf{Video Retrieval}, we train all Mamba models for 5 epochs using the AdamW optimizer~\citep{DBLP:conf/iclr/LoshchilovH19}. The learning rate is set to 1e-4 with a cosine decay schedule. We use the pre-trained CLIP4Clip as the frozen teacher. The batch size is set to 128. The model is trained on 4 A100 GPUs. 


% % \input{Styles/tab/backbones}

% \begin{figure}[t]
%   \centering
%    \begin{minipage}{0.48\linewidth}
%        % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{Styles/figures/results_cifar_t.pdf}
%     \subcaption{CIFAR100}
%     \label{fig:cifar-b0-10}
%    \end{minipage}
%   % \begin{minipage}{0.32\linewidth}
%   %   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%   %   \includegraphics[width=1\linewidth]{Styles/figures/cifar_small.pdf}
%   %   \caption{CIFAR-small}
%   %   \label{fig:cifar-b0-20}
%   % \end{minipage}
%   \begin{minipage}{0.48\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{Styles/figures/results_imnt_t.pdf}
%     \subcaption{ImageNet100}
%     \label{fig:cifar-b50-10}
%   \end{minipage}
%   \hfill
%   \caption{The accuracy under different image classification datasets.}
%   % Left is evaluated with 5 steps, middle with 10 steps, and right with 20 steps.}
%   % These are averaged over 3 runs.}
%   \label{fig:accuracy}
% \end{figure}



\noindent \textbf{Metrics.} For {\textbf{classification task}, we employ the top accuracy as the evaluation metric. 
For {\textbf{visual question answering}, we conduct experiments on GQA\citep{hudson2019gqa}, VizWiz\citep{gurari2018vizwiz}, TextVQA\citep{singh2019towards}, MME\citep{Fu2023MMEAC}.
For {\textbf{text-video retrieval task}, we assess model performance using recall@k and Mean R.

% See more details in Appendix .





\subsection{Main Results}\label{sec:MR}

\noindent {\bf Image Classfication.}  
In Table \ref{tab:variants}, we present the layer configurations, dimensions, and parameter scales for various Mamba setups tailored for classification tasks.  
Unlike the singular PlainMamba configuration outlined in prior works \cite{DBLP:journals/corr/abs-2403-17695}, we expand PlainMamba into three scales: PMamba-T, PMamba-S, and PMamba-B in our new Mamba model.  ViM, VMamba-T and VideoMamba share the same layer configuration as described in  
 \citep{DBLP:journals/corr/abs-2401-09417},
 \citep{DBLP:journals/corr/abs-2401-10166} and \citep{DBLP:journals/corr/abs-2403-06977}. 

We present the experiment results for CIFAR-100, ImageNet-subset and ImageNet1K in Table \ref{tab:results_imageclassification}.  
% The baseline DeiT denotes the experimental outcome achieved through training the model from scratch. 
DeiT-Pretrain represents the fine-tuned model on CIFAR or ImageNet-100 after pre-training on the ImageNet-2012 dataset \cite{DBLP:journals/ijcv/RussakovskyDSKS15}.
From the table, we observe 
% that Mamba-based models demonstrate superior performance with fewer parameters compared to transformer-based models. 
when comparing TransMamba with vanilla Mamba, it surpasses the recently proposed VMamba, ViM and PlainMamba.  
For example, TransMamba-P achieves a 2.83\% higher accuracy than Mamba-P. These results affirm that knowledge from Vision Transformers (ViT) can be effectively transferred to Mamba, thereby enhancing Mamba's subsequent performance.

To validate the efficiency of our TransMamba, we present the loss function convergence graph and accuracy graph during the Mamba training process in Figure \ref{fig:accuracy}.
It is evident from the results that TransMamba exhibits faster convergence rates and improved accuracy, confirming the training efficiency discussed in Section \ref{sec:intro}.  

% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth]{Styles/figures/figure_block.pdf}
% \caption{ xxx.}
% \label{fig:block_fig}
% \end{figure}


\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figs/transpicture.png}

   \caption{Examples of response generated by \myMethodName.}
   \label{fig:transllava visual}
\end{figure*}




% \input{Styles/tab/backbones_video}



% \begin{figure}[t]
%   \centering
%   \begin{subfigure}{0.32\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{Styles/figures/figure_cls.pdf}
%     \caption{B0 10 steps}
%     \label{fig:cifar-b0-10}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.32\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{Styles/figures/figure_cls.pdf}
%     \caption{B0 20 steps}
%     \label{fig:cifar-b0-20}
%   \end{subfigure}
%   \begin{subfigure}{0.32\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{Styles/figures/figure_cls.pdf}
%     \caption{B50 10 steps}
%     \label{fig:cifar-b50-10}
%   \end{subfigure}
%   \hfill
%   \caption{ Incremental Accuracy on CIFAR100. The top-1 accuracy (\%) after learning each task is shown.}
%   % Left is evaluated with 5 steps, middle with 10 steps, and right with 20 steps.}
%   % These are averaged over 3 runs.}
%   \label{fig:cifar}
% \end{figure}



% intialize, trainging details, crossmamba,
\noindent {\bf Visual Question Answering } \quad
In this experiment, we keep the total number of layers unchanged and replace the original Transformer structure with Mamba. Trans-LLaVA achieves a smaller parameter count.
As shown in Table \ref{tab:resulits_llava}, compared to LLaVA-3.2-1B, Trans-LLaVA achieves better performance on 
%%%%结果出来后需要添加上
GQA, VQA, VisWiz
and nearly matches the 3B model. Due to resource constraints, we used a 3B model as the teacher model. Despite this, our model achieved excellent results on multiple metrics.

We present some examples to illustrate the qualitative results of Trans-LLaVA. As shown in Figure \ref{fig:transllava visual}, Trans-LLaVA effectively understands the user's questions and responds accurately.
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.6\textwidth]{figures/results_pixart.pdf}
% \caption{ The training loss on the text-to-image generation task.}
% \label{fig:t2i_loss}
% \end{figure}

\input{tab/backbones_video}
\input{tab/ablation_distill}
\input{tab/abalation_datasize}
% \input{tab/abalation_archi}

\noindent {\bf Video Retrieval } \quad
In Table \ref{tab:results_video}, it can be observed that our proposed method, TransMamba, outperforms Mamba on two datasets in the video domain. For example, on the MSR-VTT dataset, the R1 metric achieves the accuracy of $41.6$, which is $0.5$ points higher than Mamba demonstrating the effectiveness of our TransMamba in video retrieval.


\subsection{Ablation Studies}\label{sec:MR}
In this part, we primarily conduct experiments on classification tasks. More experiments are included in the Appendix.
%%%%%%%% abalation, other architecutre (rwkv,general), other teacher model, disill strategry, data size influence, different weight subcloning strategy， vqa experiments

\noindent\textbf{Distillation strategy:} The traditional logit distillation strategy and feature distillation approach proves advantageous offers limited benefits for Mamba's learning, as shown in Table \ref{tab:aba_distill}}, 
This highlights that WSAB can harness the rich representations acquired at Transformer layers, thereby facilitating more comprehensive knowledge transfer and enhancing the performance of the Mamba model. 


\noindent\textbf{The influence of data size:}
In Table \ref{tab:aba_datasize}, we demonstrate the model accuracy across different data scales. In the PMamba model, using only 50\% of the data is almost sufficient to converge to the best performance, which strongly indicates that the rich representations from the Transformer model significantly shorten the training process of the Mamba model.



% \noindent\textbf{Genralization:} 
% We also applied adaptive distillation to RWKV (without the bidirectional distillation). From the Table \ref{tab:aba_archi}, it can be seen that adaptive distillation is effective in the new RWKV framework. In the Tiny and Small scales, TransRWKV outperforms RWKV, achieving state-of-the-art (SOTA) results.


%%%% 感觉放不开了，其余的放到附录里面
% \input{tab/abalation_teacher} displays different teacher model.

% \input{tab/abalation_datasize} displays the influences of different data size.


% \input{tab/ablation_one} displays the influences of different teacher's feature position.


% Table~\ref{tab:aba_distill} displays all distillation strategies employed in the classification task experiments. 
% The table sequentially illustrates the impacts of the distillation target, distillation ratios, and the distillation layer on classification performance. 
% Here, distillation ratios are defined as $\frac{\alpha}{\beta}$, where $\alpha$ and $\beta$ separately denote the hyper-parameter of loss function $\mathcal{L}_{distill}$ and $\mathcal{L}_{task}$. 
% % Table \ref{tab:ablation} presents all distillation strategies employed in the classification task experiments.  
% % The table sequentially displays the influences of the distillation target, distillation ratios, and the distillation layer on classification performance.  
% % Here, distillation ratios denote the ratio between the distillation loss and the standard classification loss. 

% \noindent\textbf{Distillation target:} The traditional logit distillation strategy offers limited benefits for Mamba's learning, while the feature distillation approach proves advantageous.  
% This highlights that feature distillation can harness the rich representations acquired at intermediate layers, thereby facilitating more comprehensive knowledge transfer and enhancing the performance of the Mamba model. 

% \noindent\textbf{Distillation Ratios:} Increasing the proportion of distillation loss enhances its effectiveness.  
% Remarkably, when the ratio is set to $10:1$, it achieves a state-of-the-art accuracy of $85.9$.  
% This highlights the significance of aligning Mamba with the Transformer distribution, facilitating the extraction of valuable knowledge from the Transformer model's rich features.

% \noindent\textbf{Distillation Layers:} Distilling all blocks in the experiments yields superior results compared to distilling only at the final position.  
% This is because by distilling information from all blocks, the student model can benefit from a richer and more diverse set of features learned by the teacher model.  
% \dx{?????????? do not understand.The table \ref{tab:abation} lists all rectification strategies used in the experiments. It can be observed that the standard logit distillation strategy is not very helpful for Mamba's learning, whereas the Feature distillation strategy is beneficial for Mamba. Additionally, as the proportion of distillation loss increases, its distillation effect improves. It is notable that when the ratio is $10:1$, a SOTA level of $85.9$ is achieved. This indicates that aligning Mamba with the Transformer distribution allows the rich features to enable the Mamba model to learn useful knowledge from the Transformer model. Furthermore, distilling all blocks in the experiments shows better results than distilling only at the final position. }



% \input{Styles/tab/cifar_imagenet}

% \input{tab/backbones}

% \input{Styles/tab/config}

\section{Conclusion}
\label{sec:conlu}
In this work, we conducted comprehensive analyses and experiments to elucidate the following insights on TransMamba models: 
1) The two-stage framework efficiently transfer the knowledge of existing pre-trained Transformer models to SSM-based models.
% Knowledge distillation from pre-trained Transformer models significantly enhances the training efficiency and performance of SSM-based models. 
2) Weight sub-cloning efficiently transfers knowledge from the Transformer to the Mamba architecture. The adaptive bidirectional distillation process assigns weights based on the similarity of different layers, ensuring balanced optimization across layers. Meanwhile, the bidirectional distillation avoids the over/under-optimization issues present in unidirectional distillation.
Besides, cross-Mamba module effectively integrates multi-modal understanding capabilities into SSM, enabling robust performance in vision-language tasks.  
% Based on our thorough analysis and empirical evidence, we made the first attempt to transfer knowledge from pre-trained Transformer models to train a new Mamba and proposed an innovative framework for \textbf{TransMamba}.  
% Our approach does not require extensive architectural modifications or alignment between Transformer and SSM models to achieve efficient knowledge transfer and multi-modal interactions. 
In extensive experiments across various datasets, our method demonstrated superior performance and efficiency.
Furthermore, our algorithm can be seamlessly adapted to other SSM-based models, ensuring broad applicability and impact.

% \noindent \textbf{Limitations:}one limitation lies in the potential challenges of scaling Mamba models to larger datasets and more complex tasks. As the dataset size and task complexity increase, the linear complexity of Mamba's computation may become a bottleneck, leading to longer training times and higher resource requirements.  Additionally, as Mamba's architecture evolves and becomes more intricate, maintaining scalability while ensuring efficient training and inference across diverse applications remains a crucial consideration. 
% % limitation rehearsal-free is not verified.
% {\bf Limitation:} The proposed method is built on the rehearsal strategy. However, the rehearsal-free setting on the model performance is not evaluated.

% {\bf Potential Negative Societal Impacts:} The proposed method requires storing the data samples of previous tasks. It may violate the data privacy regularization and store sensitive data in its memory.