\section{Method}
\label{sec:met}

%%%%%%%%Overall structure, Transmamba, details for different tasks

\begin{figure*}[t]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{figs/main_architecture.png}

   \caption{Overview of the main components of our \myMethodName.}
   \label{fig:main-architecture}
\end{figure*}

The aim of our study is to investigate the feasibility of cross-architecture learning between quadratic and subquadratic architectures. Specifically, we employ Mamba as a case study, aiming to transfer knowledge from pre-trained Transformer models to Mamba models in a more cost-effective, efficient, and robust manner. This section begins with an explanation of the core principles of SSM, followed by an overview of our proposed training approach \myMethodName. We subsequently delve into the strategies for addressing uni-model and cross-model tasks within \myMethodName, providing a detailed description of the pipeline.

% background
\subsection{Preliminary}
\label{subsec:pre}
State Space Models (SSMs) are constructed upon continuous systems that translate a 1D function or sequence, \(x(t) \in \mathbb{R}^L \rightarrow y(t) \in \mathbb{R}^L\), using a hidden state \(h(t) \in \mathbb{R}^N\). Formally, SSMs employ the following ordinary differential equation (ODE) to describe the input data:

\begin{equation}
\begin{aligned}
h'(t) &= \mathbf{A}h(t) + \mathbf{B}x(t), \\
y(t) &= \mathbf{C}h(t),
\end{aligned}
\label{eq:ssm}
\end{equation}
where \(\mathbf{A} \in \mathbb{R}^{N\times N}\) signifies the system's evolution matrix, and \(\mathbf{B} \in \mathbb{R}^{N\times 1}\), \(\mathbf{C} \in \mathbb{R}^{N\times 1}\) represent the projection matrices. This continuous ODE is approximated through discretization in modern SSMs. Mamba is one of the discrete versions of the continuous system, integrating a timescale parameter \(\mathbf{\Delta}\) to transform the continuous parameters \(\mathbf{A}, \mathbf{B}\) into their discrete counterparts \(\overline{\mathbf{A}}, \overline{\mathbf{B}}\).  
The typical method for this transformation involves employing the zero-order hold (ZOH) method, defined as:

\begin{equation}
\begin{aligned}
\overline{\mathbf{A}} &= \exp(\mathbf{\Delta \mathbf{A}}), \\
\overline{\mathbf{B}} &= (\mathbf{\Delta \mathbf{A}})^{-1} (\exp(\mathbf{\Delta \mathbf{A}}) - \mathbf{I}) \cdot \mathbf{\Delta \mathbf{B}}, \\
h_t &= \overline{\mathbf{A}} h_{t-1} + \overline{\mathbf{B}} x_t, \\
y_t &= \mathbf{C}h_t,
\end{aligned}
\label{eq:discrete ssm}
\end{equation}
where the parameters \(\mathbf{B} \in \mathbb{R}^{B\times L \times N}\), \(\mathbf{C} \in \mathbb{R}^{B\times L \times N}\), and \(\mathbf{\Delta} \in \mathbb{R}^{B \times L \times D}\). 
Contrary to conventional models relying on linear time-invariant SSMs, Mamba distinguishes itself by integrating a Selective Scan Mechanism (S6) as its core SSM operator. More precisely, three functions ${S_C}(x)$, ${S_B}(x)$, ${S_\Delta}(x)$ are introduced to associate parameters $\bar B$, $C$, $\Delta$ in Equation \ref{eq:discrete ssm} to the input data $x$. Based on ${S_\Delta}(x)$, $\bar A$ can also be associated with the input data $x$.
When given an input sequence $X: = [{x_1}, \cdots ,{x_N}] \in {R^{N \times D}}$ of $N$ feature vectors, The ouput sequence $Y$ can be denoted as:
% \begin{equation}
% \begin{aligned}
% { \tiny
% Y = \left[ {\begin{array}{*{20}{c}}
% {{{\bar C}_1}}&0& \cdots &0\\
% 0&{{{\bar C}_2}}& \cdots &0\\
%  \vdots & \vdots & \ddots & \vdots \\
% 0&0& \cdots &{{{\bar C}_N}}
% \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
% {{{\bar B}_1}}&0& \cdots &0\\
% {{{\bar A}_2}{{\bar B}_1}}&{{{\bar B}_2}}& \cdots &0\\
%  \vdots & \vdots & \ddots & \vdots \\
% {{{\bar A}_N} \cdots {{\bar A}_2}{{\bar B}_1}}&{{{\bar A}_N} \cdots {{\bar A}_3}{{\bar B}_2}}& \cdots &{{{\bar B}_N}}
% \end{array}} \right]\left[ {\begin{array}{*{20}{c}}
% {{x_1}}\\
% {{x_2}}\\
%  \vdots \\
% {{x_N}}
% \end{array}} \right] }
% \end{aligned}
% \label{eq:output}
% \end{equation}
\begin{equation}
\begin{aligned}
{ \tiny
Y = C\left[ {\begin{array}{*{20}{c}}
{{{\bar B}_1}}&0& \cdots &0\\
{{{\bar A}_2}{{\bar B}_1}}&{{{\bar B}_2}}& \cdots &0\\
 \vdots & \vdots & \ddots & \vdots \\
{{{\bar A}_N} \cdots {{\bar A}_2}{{\bar B}_1}}&{{{\bar A}_N} \cdots {{\bar A}_3}{{\bar B}_2}}& \cdots &{{{\bar B}_N}}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{{x_1}}\\
{{x_2}}\\
 \vdots \\
{{x_N}}
\end{array}} \right] }
\end{aligned}
\label{eq:output}
\end{equation}
which can be expressed as: $Y=C(MX)$. 


\subsection{TransMamba}
%%%%%%%% feature visual --> similar latent space  -->  method overview -->  align, distill, weigth subcloning  --> compatitable other methods  --> analysis
%%%% feature visual
%%%%%%%% state 1: projection, state 2 knowldge distill, state 3 : weight subcloning, summary
%%%% Overall structure
\noindent {\bf Feature Calibration.} \quad
%%%% pre-process 
The first step is to align the latent space of the Transformer with that of Mamba. Considering two latent spaces, $X$ (student), $Y$ (teacher), the aim is to transfer $X$ and $Y$ in a similar spaces, ${X^{'}}$, ${Y^{'}}$. However, the feature dimensions of the two models may differ, with Mamba's dimension potentially being smaller than that of the pre-trained Transformer. We first zero-pad the feature dimension of Mamba model to match the dimensionality of the Transformer model while preserving its underlying structure.
%%%% transformation
A simple yet efficient MLP layer is employed to achieve the alignment of Mamba's dimensions with those of the Transformer.
This process provides a solid foundation for the subsequent knowledge transfer.
% \begin{equation}
% \Upsilon (x) = Rx + b 
% \label{eq:l_t}
% \end{equation}

\noindent {\bf Adaptive Bidirectional Distillation.} \quad
Knowledge distillation is used to transfer the Transformer-based knowledge to the mamba model. 
%%%% introduce model
In our approach, a teacher model $\mathcal{T}$ employing a Transformer architecture, and a student model $\mathcal{S}$ utilizing a Mamba architecture are considered. 
%%%% objective
The objective of TransMamba is to transfer knowledge from pre-trained Transformer models to a novel Mamba-based model.
%%%% simple distill
While inspired by distillation methods, a simplistic approach involves utilizing the task logits or the feature outputs ($F$) of the teacher model $\mathcal{T}$ as supervisory signals to align those from the student model $\mathcal{S}$. However, we observe limited performance with this naive method, as demonstrated in Table \ref{tab:aba_distill}. 
%%%% Adaptive distill
The reason for this is hypothesized to be the differences in the frameworks of the two models. Directly constraining the models through the last layer's features may lead to severe inconsistencies in the intermediate features, which cannot align the entire optimization direction of Mamba with that of the Transformer. 
% We visualize the CKA (Centered Kernel Alignment) plot after aligning the last layer's features and also observe significant differences in the intermediate layer features (Figure \ref{fig:heatmap}). 
Therefore, we choose to perform cosine similarity knowledge distillation across the entire layer and only use the last layer of the teacher network as the supervisory information.
% However, this approach strictly aligns the layers of the teacher and student models, limiting their generalization capabilities. We decided to use only the last layer of the teacher network as the supervisory information for the entire student network. For the intermediate layers, we focused on aligning the optimization direction of the student model with that of the Transformer, rather than specific values. Thus, we used cosine similarity as the optimization function for the intermediate features. 
% \begin{equation}
% {l_{\cos }} = 1 - \frac{{xy}}{{\left\| x \right\|\left\| y \right\|}}
% \label{eq:l_t}
% \end{equation}
% For the last layer, we continued to use the traditional KL divergence loss function.
% \begin{equation}
% {l_{distill}} = KL(x,y)
% \label{eq:l_t}
% \end{equation}
\begin{equation}
{\mathcal{L}_{distill}} = \sum\limits_{i = 1}^N {(1 - \cos (\theta_i ))} = \sum\limits_{i = 1}^N{(1 - \frac{{F_T^{'}F_{S\_i}^{'}}}{{||F_T^{'}||||F_{S\_i}^{'}||}})}
\label{eq:loss_distill}
\end{equation}
%%%% adaptive
However, directly optimizing the intermediate layer features can lead to over-alignment with the teacher network at certain layers and under-alignment at others. To address this, we propose an adaptive optimization method that assigns different weights to the features based on their varying similarities. We reused the previously computed feature similarity values to calculate the total similarity. Then, we assigned a weight coefficient to each layer's features. This ensures a more balanced optimization of feature consistency across all layers.
\begin{equation}
\begin{array}{l}
{\mathcal{L}_\text{AdaptDistill}} = \sum\limits_{i = 1}^{N} {\frac{{\sum\limits_{j = 1}^{N } {\cos {\theta _j}} }}{{{\cos {\theta _i}}}}(1 - \cos ({\theta _i}))}\\
= \sum\limits_{i = 1}^{N } {\frac{{\sum\limits_{j = 1}^{N } {\frac{{F_T^{'}F_{S\_j}^{'}}}{{||F_T^{'}||||F_{S\_j}^{'}||}}} }}{{\frac{{F_T^{'}F_{S\_i}^{'}}}{{||F_T^{'}||||F_{S\_i}^{'}||}}}}(1 - \frac{{F_T^{'}F_{S\_i}^{'}}}{{||F_T^{'}||||F_{S\_i}^{'}||}}} )\\
= \sum\limits_{i = 1}^{N } {\sum\limits_{j = 1}^{N } {\frac{{F_T^{'}F_{S\_j}^{'}}}{{||F_T^{'}||||F_{S\_j}^{'}||}}} \frac{{||F_T^{'}||||F_{S\_i}^{'}|| - F_T^{'}F_{S\_i}^{'}}}{{F_T^{'}F_{S\_i}^{'}}}} 
\end{array} 
\label{eq:loss_layer_distill}
\end{equation}
where $\tau$ is the hyper-parameter, ${SM}$ represents the ${softmax}$. 
% %%%% 省略嘛？代码是下面这么做的，但是不好写，先去掉
% For the last layer, we find the KL divergence is better.
% \begin{equation}
% \begin{array}{l}
% {\mathcal{L}_{adapt\_N}} = \frac{{\sum\limits_{j = 1}^N {\cos {\theta _j}} }}{{{\cos {\theta _N}}}} \mathcal{KL}(SM (F_T^{'}/ \tau ),SM (F_{S\_N}^{'}/ \tau)
% \end{array} 
% \label{eq:loss_l2}
% \end{equation}
% Hence, given any task, the total loss is shown as follows
% \begin{equation}
%  \mathcal{L}_{\text{total}} =  \alpha \mathcal{L}_{\text{task}}+ (1-\alpha) (\mathcal{L}_{\text{layer\_distill}} + \mathcal{L}_{\text{2}}), 
% \label{eq:loss_all}
% \end{equation}
%%%% Forward/backward loss
For image tasks, bidirectional Mamba is necessary to address the issue of Mamba not handling global information effectively. We have simplified the output form of bidirectional Mamba as follows: 
\begin{equation}
{Y_{\text{{forward}}}} = C\left[ {\begin{array}{*{20}{c}}
{{f_{11}}}&0&0\\
{{f_{21}}}&{{f_{22}}}&0\\
{{f_{31}}}&{{f_{32}}}&{{f_{33}}}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{{x_1}}\\
{{x_2}}\\
{{x_3}}
\end{array}} \right]
\label{eq:forward}
\end{equation}
\begin{equation}
{Y_{\text{{backward}}}} = C\left[ {\begin{array}{*{20}{c}}
{{b_{33}}}&0&0\\
{{b_{23}}}&{{b_{22}}}&0\\
{{b_{13}}}&{{b_{12}}}&{{b_{11}}}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{{x_3}}\\
{{x_2}}\\
{{x_1}}
\end{array}} \right]
\label{eq:backward}
\end{equation}
The bi-directional computation transforms the original matrix $M$ from a lower triangular matrix to a dense matrix.
\begin{equation}
Y = C\left[ {\begin{array}{*{20}{c}}
{{f_{11}} + {b_{33}}}&{{b_{12}}}&{{b_{13}}}\\
{{f_{21}}}&{{f_{22}} + {b_{22}}}&{{b_{23}}}\\
{{f_{31}}}&{{f_{32}}}&{{f_{33}} + {b_{33}}}
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
{{x_1}}\\
{{x_2}}\\
{{x_3}}
\end{array}} \right]
\label{eq:last}
\end{equation}
Compared to the standard Transformer form $Y = SV = (SX) V$, where $S = soft\max (Q{K^{\rm T}}/\sqrt D )$. The bi-directional form $Y = C (MX)$ contain duplicates diagonal elements, also shown in VideoMamba \citep{Lu2024VideoMambaProAL}. 
%%%% 存在的问题，扁平的transformer特征去对齐不规则的mamba特征，并不能很好的指导双向mamba架构的学习过程。（得想一下如何描述）
If we use regular Transformer features to align irregular Mamba features (with repeated diagonal elements), it may lead to over/under-optimization of certain matrix features. Therefore, we propose a bidirectional distillation process to avoid this issue.
%%%% 双向蒸馏过程
%% 前向过程
We separate the forward and backward SSM distillation processes. For the forward process, we directly use the aligned output features of the Transformer as the supervision signal.
\begin{equation}
{\mathcal{L}_{\text{{forward}}}} = {\text{AdaptDistill}}(F_T^{'} ,F_{S\_forward}^{'} )
\label{eq:forward loss}
\end{equation}
%%%% 后向过程，需不需要加mask呢？去除重复元素
For the backward process, we reverse the Transformer's features to align with the Mamba matrix features.
\begin{equation}
{\small
{\mathcal{L}_{\text{backward}}} = {\text{AdaptDistill}}({\text{Reverse}}(F_T^{'}) ,F_{S\_backward}^{'} )}
\label{eq:backward loss}
\end{equation}
Hence, given any task, the total loss is shown as follows
\begin{equation}
 \mathcal{L}_{\text{total}} =  \alpha \mathcal{L}_{\text{task}}+ (1-\alpha) (\mathcal{L}_{\text{forward}} + \mathcal{L}_{\text{backward}}), 
\label{eq:loss_all}
\end{equation}





\noindent {\bf Weight Subcloning.} \quad
Typically, models with the same architecture can inherit weights. However, cross-architecture weight inheritance is highly challenging due to structural and dimensional differences. To explore how to fully inherit the knowledge from the Transformer architecture, we propose a weight sub-cloning technique for Mamba-based models. The primary difference between the Transformer and Mamba architectures lies in their attention (attn) and state-space model (SSM) mechanisms. 
% \begin{figure}[t]
%   \centering
% %   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.8\linewidth]{figs/weight subcloning.png}
%    \caption{
%    Weight sucloning.
%    }
%    \label{fig:weight subcloning}
% \end{figure}
%%%% structural difference
For {\bf{structural differences}}, we added an MLP layer to the existing Mamba framework and modified the RMS layer to Layer Norm. We initialized the parameters of all parts except the SSM using Transformer model parameters. 
However, in the LLaVA model, initializing the SSM structure with the existing Mamba model plays a significant role in ensuring stable training.
% We found that fixing these corresponding parameters and combining them with our proposed distillation method allows for effective optimization within each block.
%%%% sub-weight cloning
For {\bf{dimension differences}}, we select the more important parameters based on the significance of neurons can lead to better model initialization. Specifically, after fine-tuning a pre-trained model with a portion of the original data, the model weights that change less compared to those that change more are more critical for the current task. Therefore, we initialize the Mamba model using the parameters whose weights change minimally after the gradient update.
% \begin{equation}
% c_i^t = |n_i^t\frac{{dL(t)}}{{dn_i^t}}|
% \label{eq:loss_gradient}
% \end{equation}





\noindent\textbf{Downstream Tasks.} In our work, two kinds of tasks are included to verify the effectiveness of our \textbf{TransMamba}, including a uni-model task: image classification, and two multi-modal tasks: visual question answering and video retrieval. 

\noindent\textbf{Uni-modal Task:} For the image classification task, we employ three state-of-the-art (SOTA) Mamba architecture models, Vmamba \cite{DBLP:journals/corr/abs-2401-10166} , PlainMamba \cite{DBLP:journals/corr/abs-2403-17695} and VisionMamba
\cite{DBLP:journals/corr/abs-2401-09417} as the student models. 
We illustrate the process of training a Mamba model from scratch using a pre-trained (ImageNet1k/21k) Transformer DeiT model \cite{DBLP:conf/icml/TouvronCDMSJ21}. 
% Both models feature a similar encoder block.  
% Utilizing \textbf{TransMamba}, we directly train them with KL loss to distill image features from the last layer. However, existing Mamba models fall short in achieving semantic alignment across different modalities for multi-modal tasks. 
% Subsequently, we detail the approach to imbue cross-modal interaction capability using the Mamba architecture.

\noindent\textbf{Multi-modal Task:} 
% To endow the Mamba-based model with cross-modal interaction capability, we propose a new multi-modal encoder module termed cross-Mamba.  
% The cross-Mamba block is based on a basic Mamba block but includes an additional module similar to cross-attention, as shown in \dx{Figure \ref{fig:block_fig}}. In the figure, \(\mathcal{Q}\), \(\mathcal{K}\), and \(\mathcal{V}\) operate on the text, image, and image, respectively.  
% To imbue the Mamba-based model with cross-modal interaction capabilities, we introduce a multi-modal module called cross-Mamba. 
%%%% 替换架构导致原有预训练知识被破坏，文本与图像之间对齐程度降低，需要加强文本与图像之间的交互，但是这部分实验结果不知道能不能出来，总不至于结果放到附录里面？
%%%% 
One of the key challenges in using TransMamba for large multimodal models is that the new Mamba architecture can disrupt the parameter distribution of the pre-trained model, leading to inconsistencies between text and image representations. Re-performing pre-training similar to LLaVA is ineffective because the pre-trained knowledge is already compromised. Therefore, in the TransMamba for LLaVA architecture, the interaction between images and text should be enhanced.
%%%% 新架构，增强文本与图像之间的交互,不行啊，已经有一篇类似的工作有类似的东西了，很像，感觉没法强调这个对应关系，太像了，感觉只能强调针对LLaVA做出的改变，
We introduce CrossMamba to address this issue. We first simplify the computational formula of Mamba, $Y=C(MX)$. Compared to standard Transformer form $Y=SV=QKV$. 
% \begin{equation}
% equation1
% % y = C\prod {\bar A} \bar Bx
% \label{eq:crossmamba_eq1}
% \end{equation}
% We compare the simplified formula with Linear Attention and also incorporate the new structures from Mamba2.
% \begin{equation}
% equation 2
% % y = Q\sum K V
% \label{eq:crossmamba_eq2}
% \end{equation}
Then, we simply set $Q = {S_C}(x)$, $K = {S_B}(x)$, $V = x$. Besides, we also set the ${S_\Delta}(x)$ with the similar modality input with $KV$. This way, the CrossMamba can effectively facilitate the interaction between text and images. 
%%%% 训练llava-mamba的处理，重要的初始化，隐藏层的前后置零操作，卷积层的初始化问题
When training large models based on Mamba, gradient divergence appears randomly, as observed in NLP studies in \citep{zuo2024falcon,team2408jamba}. Proper initialization is crucial for ensuring stable training. We initialized parameters outside the SSM using weight subcloning. However, during training, the model exhibited convergence difficulties. To mitigate this, the SSM parameters were initialized using a pre-trained Mamba model from the NLP domain \citep{zuo2024falcon}. Furthermore, we discovered that the initialization of convolutional layers significantly impacts subsequent training. Therefore, these parameters were initialized using a standard normal distribution. These strategies were complemented by the aforementioned distillation methods.
%%%% 图像检索，简单介绍一下
For video retrieval, the SSM parameters were initialized using VideoMamba \citep{DBLP:journals/corr/abs-2403-06977}.


%  Depicted in Figure \ref{fig:block_fig}, this module extends the basic Mamba block with an additional component resembling cross-attention.
% Specifically, we integrate the interaction between textual and image data directly within the internal structure of Mamba.
% Given the verbose nature of textual information, we regard the text data as key ($\mathcal{K}$) and value ($\mathcal{V}$), and utilize the selective scan technique from S6 \cite{DBLP:journals/corr/abs-2312-00752} to identify task-relevant tokens.  
% Concurrently, we introduce the image data as queries ($\mathcal{Q}$), seamlessly injecting it into the textual context. 
% Based on the cross-Mamba module, we stack \(M\) layers using a basic Mamba block and a cross-Mamba block as the fundamental unit to construct the new Mamba network. 

% \xiuwei{
% Inspired by cross-attention mechanisms, we implemented the interaction between text and image information within the internal structure of Mamba. Due to the verbosity of textual information, we input the text data into the key-value pairs. By utlizing the selective scan in S6 \cite{DBLP:journals/corr/abs-2312-00752}, we select task-relevant tokens. Simultaneously, we input the image information into Q (query), injecting the image data into the text, thereby achieving multi-model encoder module within Mamba's internal structure.
% }



% \noindent\textbf{Efficiency Analysis:}  
% % three models？ two tasks?
% Mamba's computation has a linear complexity, which significantly reduces inference time compared to the quadratic complexity of the Transformer. Additionally, by aligning the feature output of the Transformer and Mamba, the output distribution of the Transformer can guide the learning of Mamba's output distribution, allowing Mamba to quickly find the optimal solution distribution. Particularly in the text-to-image generation process, replacing self-attention and cross-attention with Mamba and Cross-Mamba in numerous blocks significantly reduces the model size and training time. More detailed efficiency experiments will be presented in the experimental section.







% \noindent {\bf CrossMamba architecture.} \quad
% %% 感觉可以在Mamba2的基础上（Qkv感觉比较直接）或者结合LLaVA，加强文本与视觉的交互能力？文本与视觉的token拼接后在attention与ssm之间的计算方式是不同的，直接替换，ssm的劣势是啥？如何加强？


