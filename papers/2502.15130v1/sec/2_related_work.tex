\section{Related Work}

% overview
% Deep neural networks have substantially advanced the research in machine visual perception. % CNN and ViT ? why not use CNN?

% transformer
% \noindent {\bf Vision Transformers} (ViTs) are adapted from the NLP community, showcasing a potent perception model for visual tasks and swiftly evolving into one of the most promissing visual foundation models. Early ViT-based models usually require large-scale dataset and appear in a plain configuration. Later, DeiT~\citep{} employs training techniques to address challenges encountered in the optimization process, and subsequent studies tend to incorporate inductive bias of visual perception into network design. For example, the community propose hierarchival ViTs to gradually decrease the feature resolution throughout the backbone. Moreover, other studies propose to harness the advantages of CNNs, such as introducing convolution operations, designing hybrid architectures by combining CNN and ViT modules, etc.
% disadvantage
% \dx{transformer rather than Vit}

\noindent {\bf Transformers}  
% Since the introduction of 
Transformers \citep{DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/cvpr/DongBCZYYCG22,DBLP:conf/iccv/LiuL00W0LG21,cao2021pursuit,cao2023iterative} 
% into natural language processing (NLP) \citep{DBLP:journals/aghcs/WrobelKWPW20,DBLP:journals/mta/KhuranaKKS23,DBLP:journals/corr/abs-2304-02017}, they 
have demonstrated powerful perceptual capabilities for visual tasks and emerged as one of the most promising foundational models for vision. 
\noindent\textbf{Uni-modal Task:} 
Early ViT-based models typically require large-scale datasets \citep{DBLP:conf/iclr/DosovitskiyB0WZ21} for training and have relatively simple architectures. Later, DeiT~\citep{DBLP:conf/icml/TouvronCDMSJ21} employed training techniques to address challenges encountered in the optimization process, and subsequent research tended to incorporate the inductive biases of visual perception into network design. For instance, the community proposed hierarchical ViTs \citep{DBLP:conf/iccv/LiuL00W0LG21,DBLP:conf/cvpr/DongBCZYYCG22,DBLP:conf/nips/DaiLLT21}, gradually reducing the feature resolution of the backbone network. Additionally, other studies proposed leveraging the advantages of Convolutional Neural Networks (CNNs), such as introducing convolution operations \citep{DBLP:conf/nips/DaiLLT21,DBLP:conf/cvpr/VaswaniRSPHS21,chen2023dynamic} or designing hybrid architectures by combining CNN and ViT modules \citep{DBLP:conf/nips/DaiLLT21}. 
% \xiuwei{
\textbf{Multi-modal Task:} CLIP \citep{DBLP:conf/icml/RadfordKHRGASAM21} utilizes multimodal pretraining to redefine classification as a retrieval task, enabling the development of open-domain applications.  
LLaVA \citep{liu2024visual} connects CLIP and large language model for end-to-end fine-tuning on generated visual-linguistic instruction data, with excellent performance on multimodal instruction datasets.%%%%
% PixArt \citep{DBLP:journals/corr/abs-2310-00426}, a transformer-based text-to-image diffusion model, excels in generating high-quality images while reducing training costs and CO$_2$ emissions. 
 However, the attention mechanism \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} demonstrates quadratic complexity concerning image token lengths, leading to substantial computational overhead for downstream dense prediction tasks such as object detection \citep{DBLP:journals/pieee/ZouCSGY23}, semantic segmentation \citep{DBLP:journals/eaai/ThisankeDCSVH23}, among others. This limitation curtails the effectiveness of Transformers.
% % Clip  PixArt  
% CLIP \citep{DBLP:conf/icml/RadfordKHRGASAM21} uses multimodel pretraining to convert classification as a retrieval task that enables the pretrained models to transfer knowledge to various downstream tasks. 
% PixArt \citep{DBLP:journals/corr/abs-2310-00426}, a transformer-based text-to-image diffusion model that excels in generating high-quality images while reducing training costs and CO$_2$ emissions.
% However, the attention mechanism \citep{DBLP:conf/nips/BrownMRSKDNSSAA20} requires quadratic complexity in terms of image sizes, resulting in expensive computational overhead when addressing downstream dense prediction tasks such as object detection \citep{DBLP:journals/pieee/ZouCSGY23}, semantic segmentation \citep{DBLP:journals/eaai/ThisankeDCSVH23}, etc. This limits the capability of Transformers.
% }


% Mamba
% S4-->S6--> Vmamba Plainmamba vision mamba(why not use?)
\noindent {\bf State Space Models} State Space Models (SSMs) \citep{DBLP:journals/corr/abs-2312-00752,DBLP:conf/iclr/FuDSTRR23,DBLP:conf/iclr/GuJTRR23,DBLP:journals/corr/abs-2401-10166,DBLP:journals/corr/abs-2403-17695,DBLP:journals/corr/abs-2401-09417,DBLP:journals/corr/abs-2403-13600,DBLP:journals/corr/abs-2403-06977,DBLP:conf/iclr/GuGR22} have proven highly effective in capturing the dynamics and dependencies of language sequences through state space transformations.  
The structured state-space sequence model (S4) \citep{DBLP:conf/iclr/GuGR22,DBLP:conf/nips/GuG0R22,tang2024muse} is specifically designed to handle long-range dependencies with linear complexity.  
Following the introduction of S4, more related models have been proposed, such as S5 \citep{DBLP:conf/iclr/SmithWL23}, H3 \citep{DBLP:conf/iclr/FuDSTRR23}, and GSS \citep{DBLP:conf/iclr/Mehta0CN23}.  
Mamba stands out by incorporating a data-dependent SSM layer and a selection mechanism known as the parallel scan (S6) \citep{DBLP:journals/corr/abs-2312-00752}. 
 Compared to Transformer-based models, which rely on attention mechanism with quadratic-complexity, Mamba excels at processing long sequences with linear complexity.  
In computer vision, SSM was first applied to pixel-level image classification, while S4 was utilized to manage long-range temporal dependencies in movie clip classification.  
Moreover, Mamba's potential has spurred numerous studies, showcasing its superior performance and higher GPU efficiency over transformers in visual tasks like object detection \citep{DBLP:journals/pieee/ZouCSGY23} and semantic segmentation \citep{DBLP:journals/corr/abs-2401-04722}.  
Distinct from previous works, our TransMamba aims to explore the potential of using knowledge from pre-trained Transformer models to a new model with Mamba architecture in a cross-architecture transferring way.
% (SSMs) are recently proposed models that are introduced into deep learning as state space transforming. 


% distillation
\noindent {\bf Transfer learning}
Several methods~\citep{DBLP:conf/emnlp/FetahuVRM22,DBLP:journals/corr/abs-1909-03508,DBLP:journals/corr/abs-2203-06760,DBLP:conf/accv/LiuCLHDL22,wu2024aligning,hao2024one} have proposed transferring the knowledge of Transformers to convolutional neural network (CNN).
~\citep{DBLP:conf/accv/LiuCLHDL22} introduced the cross attention projector and group-wise linear projector to align the student features with the teacher model in two projected feature spaces.
~\citep{DBLP:journals/corr/abs-2203-06760} introduces a novel approach called Cross-Model knowledge distillation (CMKD) for audio classification, where Convolutional Neural Network (CNN) and Audio Spectrogram Transformer (AST) models are used as teachers for each other to train student models. 
% ~\citep{DBLP:conf/emnlp/FetahuVRM22} discusses the application of Knowledge Distillation to distill multilingual transformer models into simpler convolutional neural network (CNN) models for intent classification in voice assistants.
% ~\citep{DBLP:journals/corr/abs-1909-03508} discusses the use of a model distillation process to train a novel student convolutional neural network architecture from a large teacher language model in the field of natural language processing (NLP).
% ~\citep{wu2024aligning} discusses the frequency-domain properties of image features and the problem of heterogeneous model feature alignment, and propose low-frequency component-based contrast learning(LFCC) framework.
~\citep{hao2024one} discusses discarding information related to the architecture in the feature space to prevent the student from model being interfered with by irrelevant information.
Recently, some work \citep{wang2024mamba,bick2024transformers} in the NLP field has focused on the knowledge transfer process from Transformers to Mamba.
\citep{wang2024mamba} proposes to reuse the linear projection weights of the attention layer of the large transformer model to perform cross-architecture distillation with less GPU resources, achieving performance comparable to that of the large transformer model.
\citep{bick2024transformers} views both Transformers and SSMs as applying different forms of mixing matrices over the token sequences, and propose a progressive distillation strategy to distill the Transformer architecture by matching different degrees of granularity in the SSM.
\citep{lei2024dvmsr} use a simple $l_1$ distillation loss for leveraging the rich representation knowledge of teacher network.
%%%% 这地方该咋写，直接写没有工作做？还是委婉一点说在我们的知识中
To the best of our knowledge, fewer work was explored the knowledge transfer from Transformers to Mamba in the filed of vision and multimodality. The introduction of image information complicates the mamba structure and makes knowledge transfer more challenging. In this paper, we focus on rapid knowledge transfer from Tranformers to Mamba in vision and multimodality.