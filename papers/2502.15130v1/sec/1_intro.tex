\section{Introduction}
\label{sec:intro}



Transformer \citep{DBLP:conf/nips/VaswaniSPUJGKP17} architectures have had a profound impact on the computer vision community \citep{DBLP:conf/cvpr/DongBCZYYCG22,DBLP:conf/iclr/DosovitskiyB0WZ21,DBLP:conf/cvpr/HeZRS16,DBLP:conf/iccv/LiuL00W0LG21,DBLP:journals/corr/SimonyanZ14a}, with their flexible scalability of attention modules  considered crucial to their success.  
Despite their popularity, Transformers encounter obstacles due to the quadratic computational complexity of their attention mechanism \citep{DBLP:conf/nips/BrownMRSKDNSSAA20}, leading to increased computational expenses and memory usage.
% , as demonstrated in Figure \ref{fig:co emossion}.   
Consequently, this results in significant challenges in model optimization and extension, hindering their widespread applications.
% particularly for deployment in edge devices like smartphones.  
% To address these challenges and achieve lightweight models with faster training processes, previous studies have explored model compression techniques \citep{DBLP:journals/corr/abs-1710-09282,DBLP:journals/pieee/DengLHSX20} such as token pruning \citep{DBLP:conf/kdd/KimSTGKHK22} to reduce the computational burden, albeit at the expense of compromising model performance. 
%%%% 
In response to this challenge, recent research introduces the subquadratic architectures, such as Mamba \citep{DBLP:journals/corr/abs-2312-00752,DBLP:conf/iclr/FuDSTRR23,DBLP:conf/iclr/GuJTRR23,DBLP:journals/corr/abs-2401-10166,DBLP:journals/corr/abs-2403-17695,DBLP:journals/corr/abs-2401-09417,DBLP:journals/corr/abs-2403-13600,DBLP:journals/corr/abs-2403-06977}, RWKV\citep{peng2023rwkv,duan2024vision}. 
\xw{Mamba is based on the State Space Model (SSM) \citep{DBLP:conf/iclr/GuGR22}, enables global awareness with linear complexity without comprising performance.}  
% 有点突兀
However, training specialized subquadratic models from scratch for diverse downstream tasks poses a significant computational burden, as shown in Figure \ref{fig:co emossion}, with higher carbon dioxide emissions.
% the Mamba model architecture \citep{DBLP:journals/corr/abs-2312-00752,DBLP:conf/iclr/FuDSTRR23,DBLP:conf/iclr/GuJTRR23,DBLP:journals/corr/abs-2401-10166,DBLP:journals/corr/abs-2403-17695,DBLP:journals/corr/abs-2401-09417,DBLP:journals/corr/abs-2403-13600,DBLP:journals/corr/abs-2403-06977}, which, based on the State Space Model (SSM) \citep{DBLP:conf/iclr/GuGR22}, enables global awareness with linear complexity without comprising performance.  
% However, training specialized Mamba models from scratch for diverse downstream tasks poses a significant computational burden, as shown in Figure \ref{fig:first} (b).  
Fortunately, we observe various Transformer-based pre-trained models, such as  LLaVA \citep{DBLP:conf/icml/RadfordKHRGASAM21},
 CLIP \citep{DBLP:conf/icml/RadfordKHRGASAM21} and
 \xw{DeIT \citep{DBLP:conf/icml/TouvronCDMSJ21}},, are publicly available.  
A natural question arises: 
%%%% Focus on Transfer or facilitate ???? transfer better, facilitate sounds a advantage.
% \textit{Can we utilize the pre-trained Transformer-based models to facilitate the training of subquadratic models such as SSMs?} 
\textit{Can we transfer the knowledge from pre-trained Transformer-based models to subquadratic models, such as Mamba?} 
In this paper, we aim to explore the feasibility of transferring knowledge from these widely available Transformer-based pre-trained models to the subquadratic models, such as Mamba, for more cost-effective and efficient training. 
% cost effitecture: less data, smaller model
% efficient: converge fast, higher accuracy(???)咋说


%%%% 图要简化，其中一部分放llava的东西
\begin{figure*}[t]
  \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figs/CO2.png}
   \caption{
   Comparisons of $CO_2$ emissions and training cost among different methods. Compared to Mamba, TransMamba uses less data and requires shorter training time.
   % Model Adaptation with DRC.
   % Model Adaptation with DRC on new data $D_t$.
   % The network architecture with DRC for model adaptation.
   % Illustration of the network architecture with DRC for model adaptation.
   }
   \label{fig:co emossion}
\end{figure*}


%%%% challenges
Specifically, we address two critical challenges in our research: 1) \textbf{Tackling Cross-Architecture Learning}: This involves adapting knowledge from one architecture framework (pre-trained Transformer models) to another (Mamba models). 
% \xw{As shown in Figure \ref{fig:heatmap}, the features between these two models reside in different latent features spaces. Directly mathching these irrelevant features is less meaningful, highlighting the difficulty in knowledge transfer at intermediate layers. }
% 要不要加呢？三个不同任务的迁移，在这强调单个任务感觉不合适
Besides, we must ensure that the transfered knowledge retains its effectiveness and enhances the performance of the target model without compromising its structural integrity. 
2) \textbf{Equipping SSM-Based Models with Cross-Modal Interaction Capabilities}: This involves developing methods to seamlessly integrate and process information from different modalities, such as text and images, to enhance the versatility and application of SSM-based models in complex tasks.  
We should ensure that SSM-based models can understand and leverage the relationships between various types of data. 

% \begin{figure}[t]
% \centering
%   % \begin{subfigure}{0.32\linewidth}
%   %   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%   %   \includegraphics[width=1\linewidth]{figs/Imagenet100_results5.pdf}
%   %   \caption{ImageNet100 5 steps.}
%   %   \label{fig:imagenet100-b0-5}
%   % \end{subfigure}
%   % \hfill
%   \begin{subfigure}{0.32\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{figs/transformer-transformer.jpg}
%     \caption{DeIT/DeIT}
%     \label{fig:deit/deit}
%   \end{subfigure}
%   \hfill
%   % \begin{subfigure}{0.32\linewidth}
%   %   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%   %   \includegraphics[width=1\linewidth]{figs/Imagenet100_results20.pdf}
%   %   \caption{B0 20 steps}
%   %   \label{fig:imagenet100-b0-20}
%   % \end{subfigure}
%   % \hfill
%   \begin{subfigure}{0.32\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{figs/transformer-transformer.jpg}
%     \caption{Mamba/Mamba}
%     \label{fig:mamba/mamba}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.32\linewidth}
%     % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \includegraphics[width=1\linewidth]{figs/mamba-transformer.jpg}
%     \caption{DeIT/Mamba}
%     \label{fig:deit/mamba}
%   \end{subfigure}
%   \hfill
%   \caption{
%   Similarity heatmap of intermediate features measured by CKA.
%   }
%   % Left is evaluated with 5 steps, middle with 10 steps, and right with 20 steps.}
%   \label{fig:heatmap}
% \end{figure}


Regarding the first challenge, our approach introduce a two-stage strategy to leverage the knowledge distilled from pretrained Transformer models to enhance the training efficiency and performance of SSM-based models. 
% We use zero-padding
% % /pooling 
% to match the dimmension of the SSM-based models without changing its underlying structure. 
% %%%% align
% In \ref{}, it is empirically shown that the spaces mostly differ by an angle-preserving transformation. Moreover, we use a learnable affine transformation to transfer the mismatched representations into an aligned latent space. Therefore, cross-architecture distillation at intermediate layers becomes achievable. 
% We first match the dimmension between transformer and SSM-based models, and then use a simple MLP layer to transfer the mismatched representions into an aligned latent space. 
We first use a simple MLP layer to transfer the mismatched representions into an aligned latent space. 
Therefore, cross-architecture distillation at intermediate layers becomes achievable. 
%%%% knowledge distillation
\xw{To avoid complex designs due to inconsistent layer counts, the last layer of the Transformer model is used to distill all layers of the SSM-based models (one-to-N).}
% The feature from the last transformer layer is distilled to all SSM-based layers.
However, there is an inconsistency correlation between the last Transformer layer and different-level SSM-based layers.
% (also shown in Figure \ref{fig:heatmap}). 这个地方放上的话，感觉最后也得放对齐之后的效果图，但是从实际来看，并没有很齐，因此就不放了？
We use the cosine similarity to evaluate the correlation between layers. And then, adaptive knowledge distillation is proposed to transfer knowledge, with higher similarity features receiving lower weight coefficients, and vice versa. 
%%%% forward/backward distill
To address the inconsistent optimization issues in the dual paths of the bidirectional Mamba for processing image information, we propose a bidirectional (Forward/Backward) distillation strategy 
to optimize different processes separately.
%%%% weigth subcloning
Relying solely on distillation may not effectively facilitate knowledge transfer, especially in large models. Weight reuse is crucial for ensuring the accuracy and efficiency of transferred knowledge.
Model weights of similar architecture can be directly reused for initialization. However, cross-architecture weight reuse is challenging due to structural differences. Inspired by \citep{samragh2023weight}, we initialize SSM-based models from the pretrained Transformer models. One difference between them is similar architecture and cross-architecture (Attention and SSM). 
\xw{We consider the difference in structure and dimension. }
For structural difference, we retrain the weight except for QKV projection layer. 
For dimension difference, partial important weights from the Transformer models are used to match the dimension of the SSM-based models. Weigth subcloning also accelerates the convergence. 
% We remove the weight transfer for QKV projection layer and retrain the weight reuse for the output linear and norm layers. To address the dimensionality mismatch across architectures, we sample partial important weights from the Transformer models to match the dimension of the SSM-based models. Additionally, weigth subcloning also accelerate the convergence.

%%%% Second challenge, whether to write
Moving on to the second challenge, 
the absence of cross-modal interaction capabilities \citep{DBLP:journals/corr/abs-2403-13802,dat2024discrete} in existing SSM-based models poses a significant limitation for various multi-modal applications.  
Inspired by cross-attention mechanisms \citep{DBLP:journals/corr/abs-2310-00426}, we propose a cross-Mamba module to integrate multi-modal understanding capabilities into SSM.  
By stacking the Mamba and cross-Mamba modules, the new Mamba model is empowered to effectively handle vision-language tasks while maintaining efficiency and performance.  

This work primarily selects Mambas as the implementation of SSM-based models. We use Two-phase to transfer knowldge from pretrained Transformers to Mamba and its variant.
Our method is fast and universal as it can boost the training of Mamba and apply to various architectures. In addition, we validate the generalization of the proposed method with thorough experiment settings including image classification, video retrieval, and visual question answering. We claim the following contributions:
\begin{itemize}
    \item  [$ \bullet $] \textbf{Fast and Universal Framework}: We propose a fast and universal two-stage framework that transfers the knowledge of existing pre-trained Transformer models to new SSM-based models, which enhances both training efficiency and subsequent performance at a low cost.
    % \item [$ \bullet $] \textbf{Adaptive Calibration}: To address architectural and learning capacity differences, we use a simple method that calibrate the output distribution of the Mamba to match the counterpart in Transformer models.
    \item  [$ \bullet $] \textbf{Weight Subcloning and Adapative Bidirectional Distillation (WSAB)}: We use the weight subcloning to the cross-architecture transfer that  efficiently leverages the pre-trained knowledge. Additionally, we propose a adaptive Forward/Backward distillation method for mamba architecture and endow Mamba with multi-model interaction capabilities.
    \item  [$ \bullet $] \textbf{Comprehensive Validation}: We validate the proposed method across a wide range of backbone architectures and versatile applications, including visual question answering, video retrieval and image classification.
    
\end{itemize}

































% Please follow the steps outlined below when submitting your manuscript to the IEEE Computer Society Press.
% This style guide now has several important modifications (for example, you are no longer warned against the use of sticky tape to attach your artwork to the paper), so all authors should read this new version.

% %-------------------------------------------------------------------------
% \subsection{Language}

% All manuscripts must be in English.

% \subsection{Dual submission}

% Please refer to the author guidelines on the \confName\ \confYear\ web page for a
% discussion of the policy on dual submissions.

% \subsection{Paper length}
% Papers, excluding the references section, must be no longer than eight pages in length.
% The references section will not be included in the page count, and there is no limit on the length of the references section.
% For example, a paper of eight pages with two pages of references would have a total length of 10 pages.
% {\bf There will be no extra page charges for \confName\ \confYear.}

% Overlength papers will simply not be reviewed.
% This includes papers where the margins and formatting are deemed to have been significantly altered from those laid down by this style guide.
% Note that this \LaTeX\ guide already sets figure captions and references in a smaller font.
% The reason such papers will not be reviewed is that there is no provision for supervised revisions of manuscripts.
% The reviewing process cannot determine the suitability of the paper for presentation in eight pages if it is reviewed in eleven.

% %-------------------------------------------------------------------------
% \subsection{The ruler}
% The \LaTeX\ style defines a printed ruler which should be present in the version submitted for review.
% The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution.
% If you are preparing a document using a non-\LaTeX\ document preparation system, please arrange for an equivalent ruler to appear on the final output pages.
% The presence or absence of the ruler should not change the appearance of any other content on the page.
% The camera-ready copy should not contain a ruler.
% (\LaTeX\ users may use options of \texttt{cvpr.sty} to switch between different versions.)

% Reviewers:
% note that the ruler measurements do not align well with lines in the paper --- this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly.
% Just use fractional references (\eg, this line is $087.5$), although in most cases one would expect that the approximate location will be adequate.


% \subsection{Paper ID}
% Make sure that the Paper ID from the submission system is visible in the version submitted for review (replacing the ``*****'' you see in this document).
% If you are using the \LaTeX\ template, \textbf{make sure to update paper ID in the appropriate place in the tex file}.


% \subsection{Mathematics}

% Please number all of your sections and displayed equations as in these examples:
% \begin{equation}
%   E = m\cdot c^2
%   \label{eq:important}
% \end{equation}
% and
% \begin{equation}
%   v = a\cdot t.
%   \label{eq:also-important}
% \end{equation}
% It is important for readers to be able to refer to any particular equation.
% Just because you did not refer to it in the text does not mean some future reader might not need to refer to it.
% It is cumbersome to have to use circumlocutions like ``the equation second from the top of page 3 column 1''.
% (Note that the ruler will not be present in the final copy, so is not an alternative to equation numbers).
% All authors will benefit from reading Mermin's description of how to write mathematics:
% \url{http://www.pamitc.org/documents/mermin.pdf}.

% \subsection{Blind review}

% Many authors misunderstand the concept of anonymizing for blind review.
% Blind review does not mean that one must remove citations to one's own work---in fact it is often impossible to review a paper unless the previous citations are known and available.

% Blind review means that you do not use the words ``my'' or ``our'' when citing previous work.
% That is all.
% (But see below for tech reports.)

% Saying ``this builds on the work of Lucy Smith [1]'' does not say that you are Lucy Smith;
% it says that you are building on her work.
% If you are Smith and Jones, do not say ``as we show in [7]'', say ``as Smith and Jones show in [7]'' and at the end of the paper, include reference 7 as you would any other cited work.

% An example of a bad paper just asking to be rejected:
% \begin{quote}
% \begin{center}
%     An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of our previous paper [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Removed for blind review
% \end{quote}


% An example of an acceptable paper:
% \begin{quote}
% \begin{center}
%      An analysis of the frobnicatable foo filter.
% \end{center}

%    In this paper we present a performance analysis of the  paper of Smith \etal [1], and show it to be inferior to all previously known methods.
%    Why the previous paper was accepted without this analysis is beyond me.

%    [1] Smith, L and Jones, C. ``The frobnicatable foo filter, a fundamental contribution to human knowledge''. Nature 381(12), 1-213.
% \end{quote}

% If you are making a submission to another conference at the same time, which covers similar or overlapping material, you may need to refer to that submission in order to explain the differences, just as you would if you had previously published related work.
% In such cases, include the anonymized parallel submission~\cite{Authors14} as supplemental material and cite it as
% \begin{quote}
% [1] Authors. ``The frobnicatable foo filter'', F\&G 2014 Submission ID 324, Supplied as supplemental material {\tt fg324.pdf}.
% \end{quote}

% Finally, you may feel you need to tell the reader that more details can be found elsewhere, and refer them to a technical report.
% For conference submissions, the paper must stand on its own, and not {\em require} the reviewer to go to a tech report for further details.
% Thus, you may say in the body of the paper ``further details may be found in~\cite{Authors14b}''.
% Then submit the tech report as supplemental material.
% Again, you may not assume the reviewers will read this material.

% Sometimes your paper is about a problem which you tested using a tool that is widely known to be restricted to a single institution.
% For example, let's say it's 1969, you have solved a key problem on the Apollo lander, and you believe that the 1970 audience would like to hear about your
% solution.
% The work is a development of your celebrated 1968 paper entitled ``Zero-g frobnication: How being the only people in the world with access to the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

% You can handle this paper like any other.
% Do not write ``We show how to improve our previous work [Anonymous, 1968].
% This time we tested the algorithm on a lunar lander [name of lander removed for blind review]''.
% That would be silly, and would immediately identify the authors.
% Instead write the following:
% \begin{quotation}
% \noindent
%    We describe a system for zero-g frobnication.
%    This system is new because it handles the following cases:
%    A, B.  Previous systems [Zeus et al. 1968] did not  handle case B properly.
%    Ours handles it by including a foo term in the bar integral.

%    ...

%    The proposed system was integrated with the Apollo lunar lander, and went all the way to the moon, don't you know.
%    It displayed the following behaviours, which show how well we solved cases A and B: ...
% \end{quotation}
% As you can see, the above text follows standard scientific convention, reads better than the first version, and does not explicitly name you as the authors.
% A reviewer might think it likely that the new paper was written by Zeus \etal, but cannot make any decision based on that guess.
% He or she would have to be sure that no other authors could have been contracted to solve problem B.
% \medskip

% \noindent
% FAQ\medskip\\
% {\bf Q:} Are acknowledgements OK?\\
% {\bf A:} No.  Leave them for the final copy.\medskip\\
% {\bf Q:} How do I cite my results reported in open challenges?
% {\bf A:} To conform with the double-blind review policy, you can report results of other challenge participants together with your results in your paper.
% For your results, however, you should not identify yourself and should not mention your participation in the challenge.
% Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

% \begin{figure}[t]
%   \centering
%   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    %\includegraphics[width=0.8\linewidth]{egfigure.eps}

%    \caption{Example of caption.
%    It is set in Roman so that mathematics (always set in Roman: $B \sin A = A \sin B$) may be included without an ugly clash.}
%    \label{fig:onecol}
% \end{figure}

% \subsection{Miscellaneous}

% \noindent
% Compare the following:\\
% \begin{tabular}{ll}
%  \verb'$conf_a$' &  $conf_a$ \\
%  \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
% \end{tabular}\\
% See The \TeX book, p165.

% The space after \eg, meaning ``for example'', should not be a sentence-ending space.
% So \eg is correct, {\em e.g.} is not.
% The provided \verb'\eg' macro takes care of this.

% When citing a multi-author paper, you may save space by using ``et alia'', shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word).
% If you use the \verb'\etal' macro provided, then you need not worry about double periods when used at the end of a sentence as in Alpher \etal.
% However, use it only when there are three or more authors.
% Thus, the following is correct:
%    ``Frobnication has been trendy lately.
%    It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
%    Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

% This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...'' because reference~\cite{Alpher03} has just two authors.

% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.68\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{An example of a subfigure.}
%     \label{fig:short-a}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.28\linewidth}
%     \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%     \caption{Another example of a subfigure.}
%     \label{fig:short-b}
%   \end{subfigure}
%   \caption{Example of a short caption, which should be centered.}
%   \label{fig:short}
% \end{figure*}
