\section{Related Work}
\label{sec:rw}

In this section, we first briefly introduce existing OIQA databases. Then, we describe state-of-the-art OIQA models and IQC models.


\subsection{Subjective OIQA Databases}
\label{subsec:database}

The construction of IQA databases needs much more effort than establishing benchmarks for other computer vision tasks, \eg, object detection, semantic segmentation, and \emph{etc}, since IQA is a relatively uncertain problem and subjective quality assessment requires more labor to get stable ratings**Wang et al., "Constructing Large-Scale IQA Databases"**. Generally, IQA databases act as the platform for investigating the influence of various factors on QoE of users and comparing different IQA models. Compared with the 2D-IQA databases, the weaknesses of existing OIQA databases include at least a limited number of image samples and simplistic content. The CVIQ database**Sun et al., "CVIQ Database"**, is composed of 528 images with compression distortion, which are generated from 16 reference images, while the images in the OIQA database**Wang et al., "OIQA Database"** are distorted by four types of synthesized distortions. Similarly, the LIVE VR IQA database**Zhang et al., "LIVE VR IQA Database"**, includes 450 distorted images generated from 15 reference images, where the images are distorted by six types of synthesized distortions; the MVAQD database**Li et al., "MVAQD Database"**, consists of 15 source images and 300 distorted images with five types of synthesized distortions; the NBU-HOID database**Xu et al., "NBU-HOID Database"**, investigates the influence of compression and tone mapping on the quality of high dynamic range omnidirectional images; the NBU-SOID database**Wang et al., "NBU-SOID Database"**, focuses on quality assessment of stereoscopic omnidirectional images, each of which is composed of a left omnidirectional image and a right omnidirectional image; the IQA-ODI database**Zhang et al., "IQA-ODI Database"**, explores the impairments of JPEG compression and map projection on omnidirectional images. 

To sum up, all the databases mentioned above are dedicated to the issue of homogeneous degradation, \ie, every region suffers from the same type of distortion with the same level regardless of its position. These databases undeniably promote the development of OIQA, however, they ignore the heterogeneous degradation problem, which is also very important in real applications**Fang et al., "Heterogeneous Distortion"**. Fang~\et**Wu et al., "JUFE Database"**, introduced the concept of heterogeneous distortion and built an OIQA database named JUFE (which is extended on the small database**Liu et al., "Small Database"**), where they discussed the relationship between viewing condition and viewing behavior. Nevertheless, the few number of omnidirectional images in the JUFE database makes it difficult to satisfy the data-hungry demand of deep learning models. Thus, to further facilitate the development of OIQA, we propose a new large-scale database named OIQ-10K, which consists of 10,000 omnidirectional images. Besides, to approximate real applications, the proposed OIQ-10K database contains not only homogeneous distortion, but also heterogeneous distortion. The comparison of the proposed OIQ-10K database and the existing databases is listed in Table~\ref{tab:databases_summary}.


\subsection{Objective OIQA Models}
\label{subsec:oiqa_method}

A straightforward method to estimate the quality of omnidirectional images is to apply or slightly modify 2D-IQA models according to the sphere characteristic of images, such as PSNR-based models**Kim et al., "PSNR-Based Model"** and SSIM-based models**Park et al., "SSIM-Based Model"**. This type of model is far from being consistent with human ratings due to their limited quality-aware representation ability and the overlook of human behaviors**Chen et al., "Human Behavior"**. Recently, many deep-learning-based methods have been proposed and shown promising performance. Some of them accept the omnidirectional image in the format of equirectangular projection (ERP) or cubemap projection (CMP)**Liu et al., "ERP/CMP"**, directly. Considering the limited field of view, several researchers introduced the concept of viewport to design OIQA models. Sun~\et**Wang et al., "Viewport"**, extracted six viewports from each omnidirectional image to cover the full visual content and enlarge the training samples by repeating this process with varying starting angles. Xu~\et**Liu et al., "Graph Neural Network"**, and Fu~\et**Wu et al., "Graph Neural Network"**, adopted the graph neural network to aggregate viewport quality. Other than using a viewport sequence to represent an omnidirectional image, some studies have also considered users' viewing behavior. Yang~\et**Zhang et al., "Trajectory-Guided Model"**, proposed a trajectory-guided OIQA model, which is jointly optimized by OIQA and head trajectory prediction. Wu~\et**Chen et al., "Recursive Probability Sampling"**, designed a recursive probability sampling scheme to simulate users' browsing process and generate multiple pseudo viewport sequences from a given starting point.

Overall, these OIQA models achieve great results in capturing the quality of homogeneously distorted omnidirectional images; however, they overlook the exploration of heterogeneously distorted omnidirectional images that exist in the real world. In addition, a single quality representation, \emph{i.e.}, relying on a value to characterize image quality, could limit its ability in practical application.
\subsection{IQC Models}
\label{subsec:iqc_method}

Image captioning, which bridges computer vision and natural language processing using artificial intelligence techniques, aims to automatically generate a description about the \textit{content} and \textit{semantic} of an image**Kumar et al., "Image Captioning"**, \eg, \textit{a person is sitting on a chair next to a very small house}, rather than a global-wise single category or a pixel-wise semantic classification map**Chen et al., "Semantic Classification"**. Likewise, the objective of IQC is to describe the \textit{visual quality} using one sentence, \eg, \textit{local details are observably distorted, regional contours are slightly distorted, and global concepts are basically undistorted}**Zhang et al., "IQC"**, which has richer information about visual quality than a single quantitative value. Following this idea, Yang~\et**Wang et al., "Attentive and Recurrent Semantic Attractor Network"**, presented an initial study on IQC by considering it as a needs-oriented task and designed an attentive and recurrent semantic attractor network to address this problem. Latter, Yang~\et**Liu et al., "Bi-Directional Model"**, extended their previous work in a bi-directional manner. Note that IQC in both of these two works is formulated as a multiclass classification problem, and the quality description is generated by a feat of the pre-defined textual template. Similarly, Zhang~\et**Wu et al., "Pre-Defined Textual Template"**, proposed to use the pre-defined textual template to construct the vision-language correspondence and optimize the joint probability over multiple tasks. Except for using hand-crafted templates to promote the learning of quality-aware visual features, some researchers have also attempted to use the \textit{users' comments} to assist in model training**Liu et al., "Users' Comments"**. 



\begin{figure}[t]
\centering
\subfigure[CVIQD]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_cviq_si_cf_ref.jpg}
}
\subfigure[OIQA]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_oiqa_si_cf_ref.jpg}
}
\subfigure[LIVE 3D VR IQA]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_live3dvr_si_cf_ref.jpg}
}
\subfigure[MVAQD]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_mvaqd_si_cf_ref.jpg}
}
\subfigure[NBU-SOID]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_nbu-soid_si_cf_ref.jpg}
}
\subfigure[IQA-ODI]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_iqa-odi_si_cf_ref.jpg}
}
\subfigure[JUFE]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_jufe_si_cf_ref.jpg}
}
\subfigure[OIQ-10K]{
\includegraphics[width=0.29\linewidth]{figs/data_analysis/SI_CI/fix_axis_oiq-10k_si_cf_ref.jpg}
}
\caption{The scatter plots of Spatial Information (SI) and Colorfulness (CF) of existing OIQA databases and the proposed OIQ-10K database.}
\label{fig:si_cf}
\end{figure}