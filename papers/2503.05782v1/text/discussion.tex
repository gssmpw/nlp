\section{Discussion and Limitations }
\label{sec:discussion}

Our findings suggest that LLMs show promise in scaling the automatic grading of project proposals; however, the effectiveness of using LLM grades to guide instructional design decisions hinges on whether project proposals and grading criteria contain information that reliably predicts whether a student can benefit from project-based learning. There are several limitations of our work that we now discuss.

First, in an optional feedback text field, human expert raters noted that several project proposals contained vague implementation details and skills, making it challenging for raters to determine whether crowd workers had appropriately paired their skills to computer science careers, tasks, or technologies (inter-TA agreement of 68.61\%, $\kappa=0.26$). To address these concerns, we gave raters a skill classification rubric to quantify the vagueness and irrelevance of skills, achieving moderate agreement among human expert raters (inter-TA agreement of 86.11\%, $\kappa=0.72$). Consistent across both human expert ratings and GPT-4o ratings, students with less computer science knowledge wrote an average of 1 out of 3 acceptable skills, while students with more computer science knowledge wrote an average of 2 of 3 acceptable skills. The effectiveness of project-based learning relies on students' recognizing the necessary skills to develop and maintaining motivation to learn them. While our current work focuses on leveraging LLMs to evaluate issues in project proposals, future iterations of our system may be able to improve students' success in project-based learning by helping beginners gain an awareness of specific and contextually relevant computer science skills.

Second, our study examines project proposals written by crowd workers who were 18 years or older and identified as students (answered yes to ``Are you a student?"). In this preliminary work, our aim was to gather project proposals from students with diverse backgrounds and varying degrees of familiarity with computer science. This approach attempted to capture the potential diversity of students that might enroll in introductory computer science courses at the high school or undergraduate level. The trends observed in the collected proposals can inform the design of future tools to support students in writing project proposals. Future work will investigate whether trends and issues observed in crowd workers' project proposals transfer to classroom assignments. 

Third, while generative AI-based learning technologies like our GPT-4o-based system show promise, they also incur regular costs due to API calls, raising important questions about equity and accessibility in educational contexts. While we use GPT-4o, we should be able to use our LLM-powered grading methodology with any generative large language model. Future work could explore the application of different LLMs for this task, including open-source models.

\section{Acknowledgements}

This work is supported by the Learning Engineering Tools Competition from The Learning Agency. We thank LeRoy Wong, Shereen Tyrrell, Minerva Sharma, Liz Chu, and the reviewers for insightful feedback.