\section{System Evaluation}
\label{sec:evaluation}

Each project proposal was independently evaluated by two human domain experts with experience as head teaching assistant (TA) for undergraduate computer science classes and GPT-4o \citep{hurst2024gpt}. All raters used the same 23-item quality checklist with four subtasks: (1) 10-question quality checklist students had used to self-assess their own work, (2) 3 questions judging the quality of 3 skill descriptions written by students, (3) 9 questions judging the appropriateness of skill-career pairings ($3 \times 3$), and (4) an overall quality judgment question (``I would recommend a student include this project on their resume.”). We did not perform any prompt-engineering on GPT-4o beyond evaluating subtasks and each of the student's three written skill descriptions in a separate prompt to avoid holistic evaluations. We provide the evaluation rubric in Appendix \ref{app:expert_evaluation_rubric} and GPT-4o prompts in Appendix \ref{app:llm-prompts}.

This evaluation protocol captures two main signals of user readiness for PBL. First, by comparing users' self-assessments to expert human evaluations on the same 10-point \textbf{quality checklist} adapted from \cite{WPIRubric} and \cite{lawlor2012smart}, we could determine whether users can accurately report on the quality of their own work. 

Second, by asking human expert raters to classify the \textbf{quality of skill descriptions} (``Good" vs. ``Irrelevant", ``Vague", or ``Not Core Computer Science Skill") of three skills users want to develop by working on their project and the \textbf{appropriateness of how users matched those skills to predefined mentor profiles} (such as ``Data Scientist" or ``Software Developer"), career-specific tasks, and trending technologies sourced from the O*Net Online Database \citep{Onet2024:Online}, we could capture signal on whether users' are capable of identifying perspectives and types of industry knowledge relevant to their project ideas. Users were explicitly instructed to leverage internet resources for assistance if needed, so this metric was intended to measure general reasoning and searching ability rather than ability to recall specific definitions. However, misinterpreting instructions, inability to parse technical language used by O*Net Online Database \citep{Onet2024:Online}, and shoehorning skills to match one of a limited number of options could weaken the strength of this measurement. We sanity check the quality of our grading criteria by comparing scores from users with higher levels of computer science experience against users with less experience. We expect that students with computer science experience should be able to write higher quality project proposals.

Additionally, we asked the human experts raters and GPT-4o to answer a question meant to capture the overall quality of the project proposal (``I would recommend a student include this project on their resume. Yes/No").

We checked the motivational benefits of our project proposal writing activity with a post-activity experience survey.

% --- (START) OLD

% To evaluate the feasibility of using GPT-4o to detect potential issues in students’ project proposals, we compared its ratings against ratings provided by two human domain experts experienced in teaching computer sciences classes using four assessments: (1) a \textbf{skill classification task} to detect whether the skills written by students were good or bad (irrelevant, not computer science, overly vague), (2) a \textbf{skill-paring classification task} to assess the appropriateness of pairing each skill to the selected career, selected tasks, and selected hot technologies (3) \textbf{10-question quality checklist}, and (4) A \textbf{binary choice question to simulate mentorship} (``I would recommend a student include this project on their resume.”)

% We did not perform any prompt-engineering on GPT-4o beyond evaluating each of the student's three written skills in a separate prompt to avoid holistic evaluations. Prompts are given in the appendix.

\textbf{Experimental Procedure.} We recruited 40 participants online via Prolific. Our recruitment call was shown to crowd workers (i) 18 years or older; (ii) located in the USA; (iii) fluent in English; (iv) possess at least a high-school (HS) degree or equivalent; (v) using a desktop device (no mobile or tablets); and (vi) answered yes to ``Are you a student?" on Prolific (see recruitment call in Appendix \ref{app:prolific_recruitment}). 2 participants were filtered out for not completing the activity, and 2 were filtered for failing attention checks. Of the 36 remaining participants, 17 were 18-25 years old, 14 were 26-35 years old, and 5 were over 46 years old. 25 were male, 11 were female. 11 participants reported Computer Science as their field of work/study, and 19 claimed to have computer science experience, i.e, have built a website or interactive application before using ``Programming Language (e.g., Python, JavaScript)", ``Backend Technologies (e.g., Node.js, Django, Flask)", ``Cloud Platforms (e.g., Firebase, AWS)", and/or ``Testing and Deployment (e.g., unit testing libraries, Docker)". Interestingly, some of these users claimed experience in categories like Backend Technologies or Cloud Platforms while simultaneously reporting no experience with Programming Languages. This discrepancy suggests a potential need for more fine-grained and specific questions to accurately capture their technical experience. 

All participants encountered the same set of activities and questions in the same order, and could not go back to previous stages. Participants were told the activity would take 30-45 minutes, but we did not control for time in any phase. The median time to complete the activity was 40 minutes. The recruitment and study process was approved by CMU's Institutional Review Board (research study 2024\_00000405).

\subsection{Results}
As shown in Table \ref{tab:agreement}, human expert raters and GPT-4o showed promising agreement on the relative quality of proposals. As shown in Table~\ref{tab:positive_grades}, human expert raters and GPT-4o tended to give lower quality scores to users with less computer science experience.

\input{tables/agreement}

\input{tables/grade_table}

\textbf{Skill Classification.} Human raters and GPT-4o classifed ~50\% of skills as irrelevant, vague, or not relevant to computer science ($>80\%$, $\kappa>0.6$). Consistent across all three graders, students with less computer science knowledge had an average of 1 out of 3 skills accepted, while students with more computer science knowledge had an average of 2 of 3 skills accepted.


\begin{figure}[t]
\floatconts
  {fig:mean-score-spearman-corr}
  {\caption{Spearman correlation of students' total score on the 10-point quality checklist. Although GPT-4o's scores for the quality checklist task are lower than both teaching assistants and student self-evaluations, GPT-4o's scores preserve the rank order of teaching assistants' scores better than students' self-evaluation scores do.}}
  {\includegraphics[width=0.5\textwidth]{figures/spearman_quality_checklist}}
\end{figure}


\textbf{Appropriateness of Skill Pairing Classification.} The low quality of skills written down by the students might have made the subsequent activity to pair skills with careers, tasks, and technologies more challenging to grade. Human rater agreement was minimal ($68\%, \kappa=0.25$). GPT-4o showed weak agreement with TA 1 ($74\%, \kappa=0.46$) and minimal agreement with TA 2 ($67\%, \kappa=0.20$).

\textbf{Quality Checklist.}  GPT-4o’s scores tended to be lower than either human rater's scores, leading to lower agreement between GPT-4o and humans ($71-75\%, \kappa=0.28$). However, as seen in Figure \ref{fig:mean-score-spearman-corr}, GPT-4o roughly maintained the rank of student project proposal quality relative to each other, shown by GPT-4o’s Spearman correlations with TA 1’s (Spearman=0.70) and TA 2’s (Spearman=0.53). In contrast, students’ self-evaluations had a much weaker correlation with either human expert's ratings (Spearman=0.16, Spearman=0.38).

\textbf{Experience Survey.} As shown in Figure \ref{fig:experience}, the large majority of users enjoyed the process of writing project proposals, with $88.8\%$ of users wanting to use our system in the future to choose skills and technologies to learn more about, and $91.6\%$ of users wanting to use our system in the future to design project ideas that motivate them to learn more.

\begin{figure}[t]
\floatconts
  {fig:experience}
  {\caption{\textbf{Experience Survey, Response Counts.} The majority of users reported high level of excitement, motivation, and wanting to use the activity in the future to choose skills and technologies to learn more about.}}
  {\includegraphics[width=0.9\textwidth]{figures/experience_survey}}
\end{figure}
