
\begin{table}[t]
\centering
\caption{Rater Agreement. We report agreement percentage and Cohenâ€™s $\kappa$ value between the two human expert raters (TA1 and TA2) and GPT-4o with respect to the 4 grading subtasks. As an overall index of agreement, we compute kappa for all rater pairs then report the arithmetic mean of these estimates.
}
\begin{tabular}{lcccc}
\hline 
& TA1 / TA2 & TA1 / GPT-4o & TA2 / GPT-4o  & Avg. $\kappa$ \\
\hline
 Skill Quality Classification & $86.1\%$, $\kappa = 0.72$ & $84.3\%$, $\kappa = 0.68$ & $81.5\%$, $\kappa = 0.63$ & $0.68$ \\
 Skill Pairing Classification & $68.6\%$, $\kappa = 0.26$ & $74.2\%$, $\kappa = 0.46$ & $67.2\%$, $\kappa = 0.20$  & $0.29$ \\
 Quality Checklist& $87.2\%$, $\kappa = 0.49$ & $71.4\%$, $\kappa = 0.28$ & $74.7\%$, $\kappa = 0.28$ & $0.38$ \\
 Recommend for Resume & $80.6\%$, $\kappa = 0.50$ & $75.0\%$, $\kappa = 0.43$ & $77.8\%$, $\kappa = 0.45$ & $0.46$ \\
\hline
\end{tabular}
\label{tab:agreement}
\end{table}