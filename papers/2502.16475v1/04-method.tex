\begin{figure*}[hbt]
  \centering
  \includegraphics[width=\textwidth]{figs/pipeline.pdf} 
  \caption{Overview of the framework.}
  \label{fig:overview}
\end{figure*}

\section{Method}
\subsection{Overview}
Our method, \textsc{Drangen3D}, takes an image as input and generate a 3D object represented by 3D Guassians with multi-view geometric consistency, allowing user interaction of editing the geometry during the process. As illustrated in Fig.~\ref{fig:overview}, we first train an Anchor-Gaussian (Anchor-GS) VAE that encodes complex 3D information into a latent space and decodes it into 3DGS, enabling subsequent generation in the latent space (Sec.~\ref{sec:anchor-vae}).  
%
Then, we propose Seed-Point-Driven Controllable Generation module for 3D generation from a single image. This module starts with the generation of the rough initial geometry represented by a set of sparse surface points, named seed points, where we can apply the editing by deforming the seed points. After that, a mapping module is designed to map the (edited) seed point information to the latent space, which can be decode to 3DGS subsequently (Sec.~\ref{sec:seed-point-driven}). 

% \todo{Note that we do not directly generate anchor points because: computational complex; easy to learn; generated anchor points contains noise that affect the final geoemtry}


% As illustrated in Fig.~\ref{fig:overview}, the framework of \textsc{Dragen3D} mainly consists of two parts. We first adopt an anchor-based approach to obtain 3D Gaussians and propose the Anchor-GS VAE that constructs 3D Gaussians by leveraging points sampled from 3D assets and rendered images.
% %
% Through the anchor-based representation, we can compress both geometric and texture information into a set of anchor latents, which is beneficial for latent generation, as described in Sec.~\ref{sec:anchor-vae}

% Then we generate a set of sparse seed points to control the anchor latents and thus deform the final output 3DGS, designing and utilizing the Seed-Anchor Mapping module, as described in Sec. \ref{sec:seed-point-driven}.

% Overall, combining anchor-based 3DGS VAE and seed-point-driven generation approach gives large flexibility in geometric control and deformation during the 3D generation process.

\subsection{Background}
\paragraph{Gaussian Splatting}
% 3D Gaussian Splatting represents 3D objects explicitly through radiance-based methods, leveraging high-degree shape and color features for multi-view synthesis. Its differentiable volume rendering properties make it suitable for image-to-3D object generation, allowing for the alignment of conditioned image textures. This generation process $G$ can be outlined as follows:
% \begin{equation}
%     G:\{C\}_{x',y'} \rightarrow {\{\mu,o,A,F\}}_p,
% \end{equation}
% where $\{C\}_{x',y'}$ is pixels of the input at $(w, h)$. 
% % \mc{need to use different symbols to describe pixal and gaussian positions}.
% $\mu$, $o$, $A$, and $F$ denote the position, opacity, covariance matrix, and spherical harmonic features of each Gaussian particle $p$, respectively. The discretized splatting rendering renders at $\{C\}_{x',y'}$:
% \begin{equation}
%     \displaystyle\sum_{i \in \mathcal{N}}  \alpha_i \boldsymbol{SH}(r|_{x',y'};F_i)                      \displaystyle\prod_{j=1}^{i-1} (1-\alpha_j )\rightarrow \{C\}_{x',y'},
% \end{equation}
% where $\alpha_i$ is the product of $o_i$ and the projected Gaussian density of where the kernel interacts with the ray in direction $r|_{x',y'}$ from the specific pixel at $(x',y')$, and $\boldsymbol{SH}$ denotes the color calculated with features $F_i$. 
% This is equivalent to computing a depth-and-opacity weighted average color of particles along the ray direction. Consequently, individual Gaussian points possess adequate geometric and chromatic information.
% \subsection{3DGS}
% gaussian splatting将静态场景表示为一组各向异性的3d gaussians，每个像素的颜色通过基于点的alpha混合渲染来获得，从而实现高保真度的实时新视角合成。
Gaussian splatting represents scenes as a collection of anisotropic 3D Gaussians. Each Gaussian primitive $\mathcal{G}_i$ is parameterized by a center $\mu \in \mathbb{R}^3$, opacity $\alpha \in \mathbb{R}$, color $c \in \mathbb{R}^{3(n+1)^2}$ which is represented by n-degree SH coefficients and 3D covariance matrix $\Sigma \in \mathbb{R}^{3 \times 3}$,which can be represented by scaling $s\in \mathbb{R}^3$ and rotation $r\in \mathbb{R}^4$.
% \begin{equation}
%   \mathcal{G}(x) = e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
%   \label{3dgs_define}
% \end{equation}

% 为了保持协方差矩阵的物理意义，它必须是半正定的。因此，将协方差矩阵分解为一个缩放矩阵S和一个旋转矩阵R，其中S和R分别使用一个缩放向量和一个四元数旋转向量来表示。
% To maintain the physical meaning of the covariance matrix, it must be positive semi-definite.Therefore, the covariance matrix $\Sigma$ can be decomposed into a scaling matrix $S$ and a rotation matrix $R$:
% \begin{equation}
%   \Sigma = RSS^T R^T
%   \label{3dgs_decomposed}
% \end{equation}

% 渲染时首先将3D gaussian投影到2D空间。给定视角变换W，可以计算得到2D协方差矩阵，其中J是the Jacobian of the affine approximation of the projective transformation.随后基于深度对覆盖一个像素的高斯进行排序，使用基于点的alpha混合渲染得到像素的颜色。
During rendering, the 3D Gaussian is first projected onto 2D space. Given a view transformation matrix $W$, the 2D covariance matrix $\Sigma'$ can be computed as :
$\Sigma' = JW\Sigma W^T J^T$, where $J$ is the Jacobian of the affine approximation of the projective transformation. Subsequently, the Gaussians covering a pixel are sorted based on depth. The color of the pixel is obtained using point-based alpha blending rendering:
\begin{equation}
  c = \sum_{i=1}^n c_i \alpha_i \prod_{j=1}^{i-1}(1-\alpha_i)
  \label{3dgs_render}
\end{equation}

\paragraph{Rectified Flow Model}
% Recitified flow modle有建立两个分布\pi_0, \pi_1之间mapping的能力，所以很适合我们的任务。给定x_0 ~ \pi_0 和 相对应的x_1 ~ \pi_1, 我们可以通过liner interpolation  得到 x(t) = (1-t) x_0 + t x_1 at timestamp t. And a vector filed v_sita parameterized by a neural network 被用来drive the flow from source distribution \pi_0 to target distribution \pi_1 by minimizing the conditional flow matching objective:
The Rectified Flow Model \cite{liu2022flow, lipman2022flow} has the capability to establish a mapping between two distributions, \( \pi_0 \) and \( \pi_1 \), making it well-suited for our task of mapping seed point latents to anchor latents. Given \( x_0 \sim \pi_0 \) and the corresponding \( x_1 \sim \pi_1 \), we can obtain \( x(t) = (1 - t) x_0 + t x_1 \) at timestamp \( t \in [0,1]\) through linear interpolation. A vector field \( v_{\theta} \) parameterized by a neural network is used to drive the flow from the source distribution \( \pi_0 \) to the target distribution \( \pi_1 \) by minimizing the conditional flow matching objective:
\begin{equation}
    L(\theta) = E_{t,x_0,x_1,y}||v_{\theta}(x_t, t,y) - (x_1 - x_0)||
    \label{eq:flow matching}
\end{equation}
Here, $v_{\theta}(x_t, t, y)$ is the predicted flow at time $t$ for a given point $x_t$, $y$ refers to the image  condition that guides the flow matching.