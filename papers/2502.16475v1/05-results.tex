\section{Experiments}
\subsection{Implementation Details}
\label{sec:implemenet}
\paragraph{Datasets} We use the Multiview Rendering Dataset \cite{qiu2023richdreamer,zuo2024sparse3d} based on Objaverse \cite{objaverse} for training. The dataset includes 260K objects, with 38 views rendered for each object, with a resolution of $512\times512$. To obtain the surface point clouds, we transform the 3D models according to the rendering settings, filter out those that are not aligned with the rendered images, and use Poisson sampling method\cite{poisson} to sample the surface.  We randomly split the final processed data into training and testing sets, with the training dataset consisting of 200K objects.
% We take our in-domain evaluation using the test set from Objaverse, including 2000 objects. To evaluate our model's cross-domain 能力， We 在Google Scanned Objects(GSO) dataset进行评估，which 包含1030个真实的扫描3D model， and we take 32 views renderd for each model in 球面. 
We conduct our in-domain evaluation using the test set from Objaverse, which includes 2,000 objects. To assess our model's cross-domain capabilities, we evaluate it on the Google Scanned Objects (GSO) \cite{downs2022google}dataset, which contains 1,030 real-world scanned 3D models, with 32 views rendered for each model on a spherical surface.

% 我们使用单图作为输入，以所有available的views作为noval views 来评测我们和所有比较方法的单图生成质量. And take the peak-signal-to-noise ratio (PSNR),
% perceptual quality measure LPIPS, structural similarity index (SSIM) 作为evaluation metrics, which is same to previous work\cite{zou2024triplane, chen2025lara}


% 在我们的实现中，anchor latents的fix length是2048，维度是8. the model dim of Anchor-GS VAE是512， with 两个transformer block in encoder and eight transformer block in decoder. For training the Anchor-GS VAE, we set the weights of the losses with $\lambda_s=1, \lambda_l=1, \lambda_c=1, \lambda_e=1 $ and $\lambda_{KL}=0.001$.
%
% For the Seed-Anchor Mapping Module, we use 24 transformer bolck to implement with a model dim 512, 其中 4 bolcks for downsample and 4 blocks for upsample. For the Seed Points Generation, we use 24 transfomrmer blocks to implement with model dim 512. And thanks to the seed points 的sparse 特性, we can directly learn the distribution of seed points 而不需要去train 一个 VAE. 
%
% We train the Anchor-GS VAE use only a subset of our collected datasets, with a batchsize of 128 on 8 40G A100 with 24K steps. For trainging the Seed-Anchor Mapping Module, we use our full collected datasets, with a batchsize 0f 1280 on 64 32G V100 with 20K steps. For training the Seed Points Generation Module, We training on 48 32G V100 with 54K steps.
% In our implementation, the anchor latents have a fixed length of 2048 and a dimension of 8, and the model dimension in our transformer blocks is 512, each transformer block has two attention layers and a feed-forward layer, similar to \cite{zou2024triplane}. The Anchor-GS VAE consists of two transformer blocks in the encoder and eight transformer blocks in the decoder. For training the Anchor-GS VAE, we random select 8 views,  one view as input and all 8 views as the ground truth images for supervision, and we set the loss weights as follows: \(\lambda_s = 1\), \(\lambda_l = 1\), \(\lambda_c = 1\), \(\lambda_e = 1\), and \(\lambda_{KL} = 0.001\).  


\paragraph{Network}
In our implementation, the anchor latents have a fixed length of 2048 and a dimension of 8. The model dimension in our transformer blocks is 512, with each transformer block comprising two attention layers and a feed-forward layer, following the design in \cite{zou2024triplane}. The Anchor-GS VAE consists of two transformer blocks in the encoder and eight transformer blocks in the decoder. 
%
The Seed-Anchor Mapping Module is implemented using 24 transformer blocks, with 4 blocks for downsampling and 4 blocks for upsampling. Similarly, the Seed Points Generation Module is implemented with 24 transformer blocks. Leveraging the sparsity of seed points, we directly learn their distribution without requiring a VAE. The image conditioning in our model is extracted using DINOv2\cite{oquab2023dinov2}.


\paragraph{Training Details}
For training the Anchor-GS VAE, we randomly select 8 views per object, using one view as the input and all 8 views as ground truth images for supervision. The loss weights are set as \(\lambda_s = 1\), \(\lambda_l = 1\), \(\lambda_c = 1\), \(\lambda_e = 1\), and \(\lambda_{KL} = 0.001\). We train the Anchor-GS VAE on a subset of our collected dataset containing approximately 40K objects, using a batch size of 128 on 8 A100 GPUs (40GB) for 24K steps. The Seed-Anchor Mapping Module is trained on the full dataset with a batch size of 1280 on 64 V100 GPUs(32GB) for 20K steps. The Seed Points Generation Module is trained on 48 V100 GPUs (32GB) for 54K steps.  We use the AdamW optimizer with an initial learning rate of \(4 \times 10^{-4}\), which is gradually reduced to zero using cosine annealing during training. The sampling steps for both the Seed-Anchor Mapping Module and the Seed Points Generation Module are set to 50 during inference.
% For training the Anchor-GS VAE, we randomly select 8 views per object, using one view as the input and all 8 views as ground truth images for supervision. The loss weights are set as follows: \(\lambda_s = 1\), \(\lambda_l = 1\), \(\lambda_c = 1\), \(\lambda_e = 1\), and \(\lambda_{KL} = 0.001\).
% We train the Anchor-GS VAE using a subset of our collected dataset around 40K objects with a batch size of 128 on 8 A100 GPUs (40GB) for 24K steps. The Seed-Anchor Mapping Module is trained on the full dataset with a batch size of 1280 on 64 V100 GPUs (32GB) for 20K steps. For the Seed Points Generation Module, we train on 48 V100 GPUs (32GB) for 54K steps. We use the AdamW optimizer with a learning rate of 4e-4, and the learning rate is cosine anneled to zero during training.


\paragraph{Baseline}
% We compared our methods with 之前的SOTA的3D生成模型 in one image input setting. LGM and LaRa use one image as input, then use multi-view diffusion models to get four views of the object, then get corresponding 3DGS from the multiview images in a feed-forard mamner. TriplaneGS first get dense point clouds from the single input image, then use a triplane to 聚合特征 then get the corresponding attributes of 3DGS, achieving SOTA performances.
We compared our method with previous SOTA 3DGS generation models in the single-image input setting. LGM and LaRA rely on 2D multi-view diffusion priors to obtain multi-view images, which are then used to generate the output 3DGS in a feed-forward manner, as described in ~\ref{sec:related-2d-diffusion}. TriplaneGS~\cite{zou2024triplane} does not require a 2D diffusion prior, directly generating 3DGS from a single input image, as outlined in ~\ref{sec:related_3d}. Both of them achieving SOTA performance. For each compared method, we use the official models and provided weights and ensure careful alignment of the camera parameters.

% Both  LGM~\cite{tang2025lgm} and LaRa~\cite{chen2025lara} take one image as input and then use multi-view diffusion models\cite{shi2023mvdream} to generate four views of the object. These multi-view images are subsequently converted into corresponding 3D Gaussian Splatting (3DGS) representations in a feed-forward manner. TriplaneGS~\cite{zou2024triplane}, on the other hand, first generates dense point clouds from the single input image and then aggregates features using a triplane representation to infer the corresponding attributes of 3DGS, also achieving SOTA performance. For each compared method, we use the official models and provided weights and ensure careful alignment of the camera parameters.\todo{remove duplicate description, mention LaRA is designed for four views input}
% compared methods






\subsection{Results of VAE Reconstruction}
In Fig. \ref{fig:vae}, we present the results of our Anchor-GS VAE. Given point clouds and a single image, our Anchor-GS VAE achieves high-quality reconstructions with detailed geometry and textures.



\subsection{Results of 3D Generation }
\label{sec:comparison}
% 1. 首先讲在哪些数据上进行评估。然后逐个分析结果的值，最后说我们的效果达到了SOTA的效果
% 2. 展示可视化的结果，再逐个分析。表明我们的方法相比于没有用diffusion的方法能更好的学习三维物体的分布。
% Table 1展示了我们的方法和previous SOTA methods在Objaverse和GSO上的评测结果。As described in \ref{sec:implemenet},评测在一个dense viewpoint settings下进行，the results are average use all available objects and viewpoints in the testing datasets. The 多视角不一致 in the multiview diffusion model used by LGM 和 LARA 会导致生成几何不一致的3DGS，特别是导致在新视角下的伪影，So 他们会产生相对较低的值在我们的dense viewpoint评估settings下。相比于他们，我们的方法不借助于2D diffusion先验，可以直接从单张图像中得到理想的3D表示。TriplaneGS使用通过tansformer block一次forward facing得到point clouds，但是这种方式往往不能准确学习到3D点云的分布，always failed in 输入图像中没有的区域。与之相比，我们的方法使用diffusion-based methods, 首先学习一个sparse 的seed points, which is easy to learn and can学习到3D的distribution,then mapping from seed points to anchor latents.SO get more 鲁邦的结果。
\paragraph{Metrics} 
Following previous works \cite{zou2024triplane, chen2025lara}, we use peak signal-to-noise ratio (PSNR), perceptual quality measure LPIPS, and structural similarity index (SSIM) as evaluation metrics to assess different aspects of image similarity between the predicted and ground truth. Additionally, we report the time required to infer a single 3DGS. We use a single image as input and evaluate the 3D generation quality using all available views as testing views to compare our method with others, all renderings are performed at a resolution of 512.

Tab. \ref{tab:quantitative comparison} presents the quantitative evaluation results of our method compared to previous SOTA methods on the Objaverse and GSO datasets, along with qualitative results shown in Fig. \ref{fig:image-3d}. The multi-view diffusion model used in LGM tend to produce more diverse but uncontrollable results, and lacks precise camera pose control. As a result, it fails in our dense viewpoints evaluation, achieving PSNR scores of 12.76 and 13.81 on the Objaverse and GSO test sets, respectively.

As shown in Tab. \ref{tab:quantitative comparison}, LGM and LaRa, influenced by the multi-view inconsistency of 2D diffusion models, achieve relatively lower scores in our dense viewpoint evaluation. In contrast, our method achieves the best results across both datasets, with only a slight overhead in inference time.

Fig. \ref{fig:image-3d} presents the first six rows from the Objaverse dataset and the last three rows from the GSO dataset. All methods are compared using the same camera viewpoints. For the Objaverse dataset, the rendering viewpoints are the left and rear views relative to the input viewpoint, while for the GSO dataset, the views are selected to showcase the object as completely as possible. Compared to methods using 2D diffusion priors, such as LGM and LaRa, our method demonstrates better multi-view geometric consistency, while the former tends to generate artifacts or unrealistic results in our displayed views. Compared to TGS, our method learns the 3D object distribution more effectively, resulting in more geometrically consistent multi-view results, such as the sharp feature in the left view in the first knife case.
% Compared to these methods, 我们的方法能取得更multiview geometry consistent的结果. 例如所有的方法都在第一个case中不能正确的表示输入图像中的sharp feature,导致在所有视图中都是近似的结果.

% As described in Sec. \ref{sec:implemenet}, the evaluations are conducted under a dense viewpoint setting, with the results averaged over all available objects and viewpoints in the testing datasets. The multiview diffusion models used by LGM \todo{add lgm fail reason: Image dream can't control viewpoint}and LaRa exhibit inconsistencies across viewpoints, resulting in geometrically inconsistent 3D Gaussian Splatting (3DGS) representations. This inconsistency particularly manifests as artifacts when rendering from novel viewpoints, leading to relatively lower performance under our dense viewpoint evaluation setting. In contrast, our method does not rely on 2D diffusion priors and directly generates a high-quality 3D representation from a single input image. 

% TriplaneGS employs a Transformer-based approach to predict dense point clouds in a single forward pass. However, this approach can face challenges in accurately capturing the 3D distribution of points, particularly in regions not visible in the input image, which may lead to less optimal performance in some cases. In comparison, our method adopts a diffusion-based strategy, first learning a sparse set of seed points. This approach simplifies the learning process, allowing the model to better capture the underlying 3D distribution. The seed points are then mapped to anchor latents, resulting in more robust and consistent outcomes.

\subsection{Editing Results Based on Drag}
% The drag results are presented in Fig. \ref{fig:edit}.
As shown in Fig. \ref{fig:edit}, our method enables Seed-Points-Driven Deformation. Starting with generated seed points from the input image, the sparse nature of the seed points allows for easy editing using 3D tools (e.g., Blender\cite{blender}) with a few drag operations. The edited 3DGS can then be obtained within 2 seconds.
% 如Fig. \ref{fig:edit}.所示，我们的方法可以进行Seed-Points-Driven Deformation. 对于一个generated seed points from input image,
% 由于seed points稀疏的特性，我们可以很方便借助3D编辑工具(blender)使用有限的几次Drag操作对seed points进行编辑，and以2s时间得到编辑后的3DGS.
\subsection{Ablation Study}
% \paragraph{coarse2fine in vae}
% \paragraph{two-stage generation}
% \todo{not finish here}
\paragraph{Seed Points Generation}
% We use the Recitified flow model to learn the generation of seed points with the conditon of single input image. Due to the sparse of seed points, the flow model is easy to learn and 可以很好的学到seed points' distribution. We also 实现this module using transformer block 使用一次feed forward的方法从learnable embeddings 中得到point cloud,like \cite{zou2024triplane}. As shown in Fig. \ref{fig:ablation-seed-gen}, the Feed-forward method failed to learn the distribution of the seed points, 在图片上不可见的区域无法生成理想的结果。
We employ a Rectified Flow model to generate seed points conditioned on a single input image. Owing to the sparsity of the seed points, the flow model is easier to train and effectively learns the distribution of the seed points. However, we also explored an alternative implementation using a transformer-based feed-forward approach, where point clouds are generated from learnable embeddings in a single pass, as in \cite{zou2024triplane}. As demonstrated in Fig. \ref{fig:ablation-seed-gen}, the feed-forward approach struggles to capture the true distribution of seed points and fails to produce satisfactory results in regions not visible in the input image.


\paragraph{Dimension Alignment}
% 为了让Seed-Anchor Mapping Module 的起点和target具有相同的维度，我们将Seed points 通过VAE的encoder based on Eq. \ref{eq:encode_seed_latents}. 这可以保证Mapping的起点的分布和终点的分布更接近，从而降低了Seed-Anchor Mapping Module的学习难度并且避免了Mapping时对image condition的过度依赖. And the alignment bewteen points and image achieved by projection in encoder is vivtal in the Seed-Points-Driven Deformation, as we can change the position of draged seed points while preserve it's correponding projed feats.
To match the dimension of the starting and target points in the Seed-Anchor Mapping Module, we encode the seed points using the Anchor-GS VAE encoder (Eq. \ref{eq:encode_seed_latents}). This process brings their distributions closer, reducing learning difficulty and reliance on image conditions. 
% Additionally, the projection-based alignment between points and the image in the encoder is critical for Seed-Points-Driven Deformation, enabling position adjustments of dragged seed points while preserving their projected features, as shown in Eq.\ref{eq:edit_seed_encode}. 
To validate this method, we conducted experiments by replacing the encoding approach with positional encoding .  When using positional encoding, the Seed-Anchor Mapping overly relied on the image condition, neglecting the contribution of the seed points and failing to enable seed-driven 3DGS deformation, as shown in Fig. \ref{fig:ablation-seed-enc}. 

% When positional encoding is used, the Seed-Anchor Mapping overly relies on the image condition, neglecting the true geometric state of the seed points and failing to achieve seed-driven 3DGS deformation.
% When using positional encoding, the Seed-Anchor Mapping overly relied on the image condition, 忽略了seed points真实的几何状态 and failing to enable seed-driven 3DGS deformation. 
% To validate this method, we conducted experiments by independently testing two variations: replacing the encoding approach with positional encoding and removing the projection of seed points onto the input image during encoding (Fig. \ref{fig:ablation-seed-enc}).  When using positional encoding, the Seed-Anchor Mapping overly relied on the image condition, neglecting the contribution of the seed points and failing to enable seed-driven 3DGS deformation. Separately, without projection-based alignment, the Mapping Module failed when the seed points and the input image were misaligned under the given viewpoint.
% 为了验证这个方法的有效性，我们测试了将基于encoder of anchor-GS VAE的方法换成positional encoding 和 在encode时不采用seed points与输入图像的投影。结果如图所示。在采用positional encoding时，the Seed-Anchor Mapping 将会过度依赖于image condition,导致忽略了从start points本身出发，从而无法实现基于seed points 的3DGS deformation. And lack of the projection-based alignment, the Mapping Module 会在seed points与input image 在给定视角下不一致的情况下fail.


\paragraph{Token Alignment}
We ensure token alignment in Flow Matching by organizing tokens around seed points, followed by  cluster-based partition and repetition. To evaluate its effectiveness, we conducted two ablation experiments, as shown in Tab. \ref{tab:ablation-tokenalign}. In the \textit{No-cluster+No-repetition} setting, we omitted the clustering step, aligning only the corresponding seed and anchor latents while filling unmatched portions with noise. This also prevented cluster-based downsampling in the Flow Model, resulting in doubled memory consumption. In the \textit{No-cluster} setting, we repeated the seed latents to match the number of anchor latents but left them unordered, leading to disorganized token matching. As shown in Tab. \ref{tab:ablation-tokenalign}, on a 40K subset with the same number of training steps, the absence of token alignment significantly degraded Flow Matching performance, resulting in inaccurate correspondences.
% We ensure token alignment in Flow Matching by organizing tokens around seed points, followed by repetition and cluster-based rearrangement. To validate its effectiveness, we conducted two ablation experiments. In the first, we repeated seed latents to match the number of anchor latents but left them unordered, leading to disordered token matching. In the second, we aligned only the corresponding seed and anchor latents, filling the unmatched portions with noise. Without cluster-based rearrangement, downsampling in the Rectified Flow Model became impossible, doubling memory consumption. Tab. \ref{tab:ablation-tokenalign} shows that on a 40K subset, with the same number of training steps, flow matching performance is significantly degraded without token alignment, failing to produce accurate correspondences.

% We ensure token alignment in Flow Matching by organizing tokens around seed points, followed by repetition and cluster-based rearrangement. To validate its effectiveness, we conducted two ablation experiments, as shown in Tab. \ref{tab:ablation-tokenalign}. In the No-cluster, 我们不再进行分cluster，aligned only the corresponding seed and anchor latents, filling the unmatched portions with noise. And we can't do cluster-based downsample in the Flow Model, 这会导致 memory consumption double. In the No-rearrange, we repeated seed latents to match the number of anchor latents but left them unordered, leading to disordered token matching. Tab. \ref{tab:ablation-tokenalign} shows that on a 40K subset, with the same number of training steps, flow matching performance is significantly degraded without token alignment, failing to produce accurate correspondences.


% we repeated seed latents to match the number of anchor latents but left them unordered, leading to disordered token matching. In the second, we aligned only the corresponding seed and anchor latents, filling the unmatched portions with noise. Without cluster-based rearrangement, downsampling in the Rectified Flow Model became impossible, doubling memory consumption. Tab. \ref{tab:ablation-tokenalign} shows that on a 40K subset, with the same number of training steps, flow matching performance is significantly degraded without token alignment, failing to produce accurate correspondences.
% Token alignment 保证了Flow matching 中start point和end point中token数目和对应位置上的语义是匹配的，使用以Seed points为center的方式来组织token，并进行repeat和rearrange。我们设计了消融实验来验证这个模块的必要性和有效性，第一个对照实验是在对seed latets repeat到和anchor latents相同的个数后，我们不对anchor latents进行重排序, so the token之间的匹配关系是杂乱的。第二个对照实验是我们仅仅将对应的seed latents和anchor latents进行对齐，其余无法对齐的部分使用noise进行填充。并且由于没有进行cluster based rearrange，我们无法在Rectified flow Modle中进行downsaple, which 增加了两倍的计算时的显存消耗。在一个40K subset上 经过同样的training steps后，结果如表所示。Without the token alignment, the Flow matching的效果大打折扣，无法得到理想的对应的结果。
% We ensure token alignment in Flow Matching by organizing the tokens around seed points and performing repeating and rearranging operations. This guarantees that the number of tokens and their semantic correspondence between the start and end points are aligned. To validate the necessity and effectiveness of this module, we designed ablation experiments. The first experiment involved repeating the seed latents to match the number of anchor latents but without reordering the anchor latents, resulting in a disordered matching relationship between tokens. The second experiment aligned only the corresponding seed latents and anchor latents, filling the unmatched portions with noise. Without the cluster-based rearrangement, we were unable to downsample in the Rectified Flow Model, which increased the memory consumption during computation by a factor of two. After training on a 40K subset for the same number of steps, the results, shown in the table, indicate that without token alignment, the performance of flow matching is significantly degraded, failing to achieve the desired correspondence.


% Table
\begin{table}%
\caption{ Quantitative evaluation of our method compared to previous work. $\dagger$ achieves relatively lower PSNR values in the evaluation, so we display the results in Sec. \ref{sec:comparison}.}
\label{tab:quantitative comparison}
% \begin{minipage}{\columnwidth}
\resizebox{0.5\textwidth}{!}{
% \begin{center}
\begin{tabular}{llllllll}
  \toprule
  \multirow{2}{*}{Method}  & \multicolumn{3}{c}{Objaverse\cite{objaverse}}   & \multicolumn{3}{c}{GSO\cite{downs2022google}}& \multirow{2}{*}{Time(s)}\\
% \cline{2-4}   \cline{5-7} \cline{8-10} \cline{11-13}
\cmidrule(r){2-4}  \cmidrule(r){5-7} 
   & PSNR$\uparrow$& SSIM$\uparrow$& LPIPS$\downarrow$ & PSNR$\uparrow$& SSIM$\uparrow$& LPIPS $\downarrow$
   \\ \midrule
  LGM$\dagger$\cite{tang2025lgm}     & -&0.836&0.211&-&0.833&0.21&4.82\\
  LaRa\cite{chen2025lara}  & 16.57&0.860&0.174&15.98&9.869&0.162&9.50\\
  TriplaneGS\cite{zou2024triplane}  &18.80 &0.883&0.143&19.84&0.900&0.120&0.70\\
  Ours &20.92&0.896&0.120&20.52&0.904&0.1122&4.71\\
  \bottomrule
\end{tabular}
% \end{center}
}
\end{table}%


\begin{table}%
\caption{Ablation about token alignment}
\label{tab:ablation-tokenalign}
\begin{minipage}{\columnwidth}
\begin{center}
\begin{tabular}{llll}
  \toprule
   & PSNR$\uparrow$& SSIM$\uparrow$& LPIPS$\downarrow$ 
   \\ \midrule
  No-cluster+No-repetition  & 18.84&0.877&0.141\\
  No-cluster     & 19.20 &0.876&0.142\\
  ours-full  &19.94 &0.881&0.134\\
  \bottomrule
\end{tabular}
\end{center}
\bigskip\centering


\end{minipage}
\end{table}%

%figure
\begin{figure}
  \includegraphics[width=\linewidth]{figs/ablation_seed.pdf}
  \caption{Ablation study about different seed points geneartion methods: (a) using our method, (b) using Transformer.}
  \label{fig:ablation-seed-gen}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{figs/ablation_enc.pdf}
  \caption{Without Dimension Alignment, seed-points-driven deformation fails}
  \label{fig:ablation-seed-enc}
\end{figure}


