\section{Conclusions}
% In this paper, we propose Dragen3D, a new framework进行几何一致的单图3D生成，并支持基于Drag的可控编辑。我们构建了一个Anchor-GS VAE, 通过将3D assert 的几何和纹理信息encode到anchor latents,接着decode到3DGS,训练好后将会freeze，支持我们在latents空间中做接下来的生成.我们设计了SEED-POINT-DRIVEN策略，将anchor latents的生成分解为sparse seed points的生成和Seed-Anchor Mapping Module. 这样做可以保证我们得到几何一致的结果 due to seed points的sparse的特性 and easy to learn, and Seed-Anchor Mapping Module可以让我们在对seed points拉拽之后得到不同的Mapping结果，从而进行可控生成with drag operation. 经过在不同数据集上的评测，我们的方法可以达到SOTA的单图生成3DGS质量，with 几何一致的生成结果，and 用户友好的进行细致的drag 编辑.


% In this paper, we present Dragen3D, a novel framework for geometrically consistent single-image 3DGS generation that supports drag-based controllable editing. At the core of our method is the Anchor-GS VAE, which encodes the geometric and texture information of 3D assets into anchor latents and decodes them into 3DGS. Once trained, the Anchor-GS VAE is frozen, facilitating efficient generation within the latent space.

% We propose a Seed-Point-Driven strategy that decomposes the generation of anchor latents into two stages: the generation of sparse seed points and their transformation via the **Seed-Anchor Mapping Module**. This strategy leverages the sparse and easily learnable nature of seed points to ensure geometric consistency. The Seed-Anchor Mapping Module further enables controllable generation by producing varied mappings when the seed points are manipulated, supporting flexible drag-based operations.

% Comprehensive evaluations across multiple datasets demonstrate that our method achieves state-of-the-art 3DGS quality from single images, delivering geometrically consistent results while offering user-friendly and precise drag-based editing capabilities. 

In this paper, we present \textsc{Dragen3D}, a framework for multi-view geometry consistent single-image 3DGS generation with drag-based editing.  
We propose the Anchor-GS VAE, which encodes 3D geometry and texture into anchor latents and decodes them into 3DGS. Combining seed-point generation from single image, user-interacted seed-point editing, and seed-to-anchor-latent mapping, we are able to generate and control the final output 3DGS. Evaluations across multiple datasets demonstrate that \textsc{Dragen3D} achieves state-of-the-art 3DGS quality from single images.
However, our method has room for improvement. First, incorporating 3D appearance editing based on prompts could be an interesting direction to explore, especially when integrated with existing multimodal large models. 
Additionally, the quality and quantity of training data limit our model's capabilities, which can be further improved with more computational resources.


% Once trained, the model is frozen for efficient latent-space generation. Our Seed-Point-Driven strategy divides anchor latent generation into two steps: generating sparse seed points and transforming them via the Seed-Anchor Mapping Module. This approach ensures geometric consistency through the sparsity of seed points and enables flexible drag-based control by adjusting mappings based on seed point manipulation.  Evaluations across multiple datasets demonstrate that Dragen3D achieves state-of-the-art 3DGS quality from single images, providing consistent results with precise and user-friendly drag-based editing.


% \paragraph{limitations:}
% 我们的方法存在一些可以改进的空间，first,目前我们方法的drag-based 编辑主要是做到几何上细致的编辑，加入根据prompt的纹理的编辑可以作为我们future work with与现有的多模态大模型结合. 此外，数据的质量和数量会作为我们模型能力的瓶颈,process 和 filter 更多的现有的3D资产数据将会使我们模型的效果更进一步.
% Our method has room for improvement. First, while the current drag-based editing primarily focuses on detailed geometric adjustments, incorporating texture editing based on prompts could be an interesting direction for future work, especially when integrated with existing multimodal large models. Additionally, the quality and quantity of data serve as a limitation for our model's capabilities. Expanding the collection and processing of high-quality 3D asset data would help address this bottleneck.