% Single-image 3D generation has become a prominent research topic, playing a crucial role in fields such as virtual reality, 3D modeling, and digital content creation. However, existing methods suffer from issues such as a lack of geometric consistency due to their heavy reliance on 2D priors, as well as limited controllability in the generation process. 
% %
% To address these challenges, we propose \textsc{Dragen3D}, a novel approach that achieves high-quality, geometrically controllable 3D generation based on Gaussian Splatting (GS). 
% %
% We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS VAE), a model trained to encode a point cloud and a single image into anchor latents and decode these latents into 3DGS.
% %
% To enable controllable generation, our method employs a seed-point-driven approach to generate anchor latents and control the final geometry: We design a diffusion-based Seed-Anchor Mapping Module to map a set of seed points to the anchor latents, which are then decoded into 3DGS.
% %
% These seed points are generated from the input single image using a diffusion-based method.
% %
% Users can easily drag the seed points to deform the final 3DGS geometry, which is correlated through the anchor latents.
% %
% To the best of our knowledge, we are the first to achieve geometrically controllable 3D Gaussian generation and editing without relying on 2D diffusion priors, delivering comparable 3D generation quality to state-of-the-art methods.

% \JY{
Single-image 3D generation has emerged as a prominent research topic, playing a vital role in virtual reality, 3D modeling, and digital content creation. However, existing methods face challenges such as a lack of multi-view geometric consistency and limited controllability during the generation process, which significantly restrict their usability.
%
To tackle these challenges, we introduce \textsc{Dragen3D}, a novel approach that achieves geometrically consistent and controllable 3D generation leveraging 3D Gaussian Splatting (3DGS).
%
We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GS VAE), which encodes a point cloud and a single image into anchor latents and decode these latents into 3DGS, enabling efficient latent-space generation.
%
To enable multi-view geometry consistent and controllable generation, we propose a Seed-Point-Driven strategy: first generate sparse seed points as a coarse geometry representation, then map them to anchor latents via the Seed-Anchor Mapping Module. Geometric consistency is ensured by the easily learned sparse seed points, and users can intuitively drag the seed points to deform the final 3DGS geometry, with changes propagated through the anchor latents.
%
To the best of our knowledge, we are the first to achieve geometrically controllable 3D Gaussian generation and editing without relying on 2D diffusion priors, delivering comparable 3D generation quality to state-of-the-art methods.
% }