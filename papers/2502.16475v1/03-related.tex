\section{Related Work}
% \alan{better to present the overview of the existing methodology related to our work before diving into details.
% e.g. There are many topics related to controllable 3D generation, including 3D representation, controllable generation and the design of editing scheme. }



% \subsection{Neural rendering and Gaussian Splatting}
% \subsection{基于3D gaussian的3D生成}
% \subsection{3D编辑}
\subsection{Neural Rendering and Gaussian Splatting}
% NeRF 凭借着它的高质量的视图合成能力成为了3D representation 中的热门研究话题. 
% 1. 讲radiance field很好，有很多工作提升
% 2.讲gaussian spaltting很好，做了什么改进，这种方法让什么成为可能
% radiance field 凭借在3D重建和视图合成中的强大的潜力成为3D representation 中的热门研究话题. NeRF作为一个里程碑式的工作，将高质量的视图合成成为可能。and its varients 致力于提升它的渲染质量, 训练和推理速度. Among them, 3D Gaussian Splatting(3DGS)采用point-based Radiance filed, 通过使用3D Gaussian primitives表示场景，通过 anisotropic splatting 和可谓渲染使高质量重建和realtime rendering 成为现实，with some varients致力于提升rendering quality, enhancing geometry，its ablity to 同时表示高质量的几何和纹理，让许多任务和应用有了解决方案，包括3D generation.
Radiance fields have become a popular research topic in 3D representation due to their powerful potential in 3D reconstruction and view synthesis. NeRF\cite{mildenhall2021nerf}, as a milestone work, made high-quality view synthesis possible. Its variants focus on improving rendering quality\cite{barron2021mip, barron2022mip,barron2023zip}, training and inference speed~\cite{muller2022instant,fridovich2022plenoxels,hedman2021snerg,SunSC22}, and generalization ability~\cite{wang2021ibrnet,yu2021pixelnerf,chen2021mvsnerf,johari2022geonerf}. Among them, 3D Gaussian Splatting (3DGS)~\cite{kerbl3Dgaussians} adopts a point-based radiance field, using 3D Gaussian primitives to represent scenes. Through anisotropic splatting and advanced rendering techniques, it enables high-quality reconstruction and real-time rendering. Some variants further enhance rendering quality and geometry~\cite{zhang2024rade,huang20242d, Yu2024GOF,lu2024scaffold,yu2024mip}, offering the ability to represent both high-quality geometry and textures, which provides solutions for various tasks and applications, including 3D generation.
% Novel view synthesis (NVS) has always been a hot topic in the field of computer vision. By using MLP to implicitly represent the scene, Neural Radiance Fields (NeRF) \cite{nerf} achieves realistic rendering. Subsequent works have imporved NeRF to enhance rendering quality \cite{barron2021mip, barron2022mip, wang2023f2}, reduce the number of training views \cite{yang2023freenerf,wang2023sparsenerf,niemeyer2022regnerf,wu2024reconfusion}, lessen dependence on camera poses \cite{lin2021barf,truong2023sparf,chen2023local,bian2023nope}, and improve both training and inference speeds \cite{muller2022instant,fridovich2022plenoxels,  hu2023tri, barron2023zip, hedman2021snerg, reiser2021kilonerf,reiser2023merf,SunSC22, chen2022tensorf}. 
% 为了解决上述问题，3DGS使用各向异性的高斯原语表示场景并引入光栅化进行渲染，提升了速度和渲染质量。一些方法聚焦于减少内存消耗、表面重建提高几何质量、将相机位姿与高斯场联合优化，以及使用扩散模型生成高斯场。
% 3D Gaussian Splatting (3DGS) \cite{kerbl3Dgaussians} employs anisotropic Gaussian primitives to represent scenes and introduces rasterization-based splatting rendering algorithm, enhancing both speed and rendering quality. Some methods focus on various aspects of improving Gaussian field representations, including rendering quality\cite{lu2024scaffold,yu2024mip, ren2024octree,li20243d,zhang2024pixelgs, yang2024spec}, enhancing geometric accuracy\cite{zhang2024rade,huang20242d, Yu2024GOF}, and increasing compression efficiency, \cite{chen2025hac,yang2024spectrally,lu2024scaffold,fan2023lightgaussian}, joint optimization of camera pose and gaussian fields \cite{fan2024instantsplat,Fu_2024_CVPR,schmidt2024noposegs}, as well 3D generation \cite{zou2024triplane, tang2023dreamgaussian,tang2024lgm,LaRa}. 


\subsection{2D Diffusion Priors Based 3D Generation}
\label{sec:related-2d-diffusion}
% 借助于text-to-image 的diffusion model的高质量的生成能力，一些 Multiview diffusion model可以被用来进行视图合成based on text/image and view condition. And 一些方法探索了基于广泛的2D diffusion prior进行3D generation. Some methods use SDS-loss based method to optimizae 3D representation from 2D diffusion prior, inwhich DreamGaussian use 3D Gaussian as representation and optimize in 2 minutes，这些方法时间消耗较大由于逐个场景的优化。Others methods use a feed-forward 方法来进行3D 生成 from 2D priors， LGM通过multiview diffusion model首先生成four views, then infer corresponding 3DGS from them. 这些方法都会受到multiview diffusion model多视图不一致的问题的限制，resulting in 3D不一致的几何和纹理。
% Leveraging the high-quality generation capabilities of text-to-image diffusion models\cite{rombach2022high}, some multi-view diffusion models facilitate view synthesis based on text/image and view conditions, enabling 3D generation using 2D diffusion priors. Some methods optimize 3D representations from these 2D priors using an SDS-loss-based approach or direct optimization from generated images. For instance, DreamGaussian optimizes 3D Gaussians with SDS-loss in just 2 minutes. However, these methods are computationally expensive due to scene-by-scene optimization. Alternatively, some methods adopt a feed-forward or denoising process for 3D generation from 2D priors. For instance, LGM generates four views through a multi-view diffusion model and subsequently infers the corresponding 3DGS. These 2D-prior-based methods are constrained by the inconsistencies of the multiview diffusion model, leading to misaligned 3D geometry and textures. and 由于multi-view images 生成的随机性，无法做到可控的生成.
Leveraging the high-quality generation capabilities of text-to-image diffusion models \cite{rombach2022high,saharia2022photorealistic,betker2023improving}, some multiview diffusion models \cite{liu2023zero,shi2023zero123++,shi2023mvdream,wang2023imagedream,li2023sweetdreamer,long2024wonder3d} enable view synthesis based on text/image and view conditions, facilitating 3D generation from 2D diffusion priors. Some methods optimize 3D representations from these 2D priors using an SDS-loss-based approach \cite{shi2023mvdream,liang2024luciddreamer,poole2022dreamfusion,wang2024prolificdreamer,tang2023dreamgaussian}or direct optimization~\cite{tang2025mvdiffusion++} from generated images. 
% For example, Dreamgaussian\cite{tang2023dreamgaussian} optimizes 3D Gaussians with SDS-loss in just 2 minutes.
However, these methods are computationally expensive due to scene-by-scene optimization. Alternatively, other methods adopt a feed-forward~\cite{xu2024grm, xu2024instantmesh,li2023instant3d,liu2024one,tang2025lgm,chen2025lara} or denoising process~\cite{wang2025crm,liu2024one++,xu2023dmv3d} for 3D generation from 2D priors. For instance, LGM generates four views through a multiview diffusion model and then infers the corresponding 3DGS. These 2D-prior-based methods are constrained by inconsistencies in the multiview diffusion model, leading to misaligned 3D geometry and textures, and due to the stochastic nature of multiview image generation, they lack controlled generation.
%先讲multiview diffusion，再讲把这些用于3D生成以及的问题
%diffusion很好，然后催生出来很好的text-2image, multiview diffusion model. 一些方法借助SDS的方法去做生成，dreamgs取得了不错的效果。 一些方法如LGM，通过feed forwad的方法得到3DGS，and lara 也同样支持从generative 的mv image得到3D，



\subsection{End-to-end 3D Generative Models}
\label{sec:related_3d}
% 一些方法构建large reconstruction model, 直接从单张图像中得到3D表示，而不需要依赖于2D diffusion先验。其中TripalneGaussian首先从单张图像中得到点云表示，再结合triplane融合纹理特征，得到最终的3DGS,    取得了SOTA的单图生3D效果。一些方法使用3D diffusion来实现3D生成。如3DShape2VecSet构建VAE将3D信息encode到latent set，then decode into mesh，and可采用diffusion model进行latent set的生成。一些方法同样探索了基于Gaussian Splatting的diffusion生成方式， 如GaussianCube通过构建规模的structured gaussian，通过3D U-Net based diffusion model to generate Gaussian from noise. These methods能更好的建模到3D data的分布,但不能做到以用户友好的方式进行可控生成和编辑。与之相比Our model同样适用diffusion model来学习3D信息的分布，但无需构造3DGS数据集，并且可以通过drag in 3D space 的方式来来进行可控生成。
Some methods \cite{tochilkin2024triposr,zou2024triplane,hong2023lrm} directly generate 3D representations from a single image without relying on 2D diffusion priors. For example, TriplaneGaussian~\cite{zou2024triplane} creates a point cloud from a single image, combines it with triplane fusion for texture, and produces the final 3DGS, achieving state-of-the-art single-image 3D results. Other approaches~\cite{zhang2024clay,zhang20233dshape2vecset,zhao2024michelangelo,gupta20233dgen,nichol2022point,muller2023diffrf} use 3D diffusion models, like 3DShape2VecSet~\cite{zhang20233dshape2vecset}, which encodes 3D information into a latent set and decodes it into a mesh, with diffusion models generating the latent set. Some approaches~\cite{zhang2024gaussiancube,zhou2024diffgs,he2025gvgen,xiang2024structured} also explore diffusion-based generation with Gaussian Splatting, such as GaussianCube~\cite{zhang2024gaussiancube}, which constructs structured Gaussian representations and uses a 3D U-Net-based diffusion model to generate Gaussians from noise. While these methods model 3D data distribution well, they lack user-friendly control for generation and editing. In contrast, our model leverages a diffusion model to learn 3D information distribution without needing a 3DGS dataset, offering controllable generation through 3D space manipulation.
%讲feed-forwad的生成，如TGS
%讲基于diffusion的，如3dshape2vec, clay, diffgs,gaussiancube, point-e

\subsection{Editing in 3D Generative Models}  
\label{sec: relatd_edit}
% （用prompt或者别的什么形式）edit in 2d space, 在投影到3D时会不一致
% interactive 3D， 需要两个模态之间的转换。需要2D diffusion先验。几何不一致，per-scene optimization
% MVdrag3D, 在3D空间编辑后还是要投影到2D空间
%我们的方法只需要完全在3D空间中，通过drag sparse seed points，得到不同的3DGS表示。
% 为了实现可控的3D生成和编辑，Sketch dream
% Sketch dream ,需要SDS。不是在3D空间中编辑，视角选择用户不友好。需要oprimization
% hyperdreamer, 只能改texture.
% APAP,edit on mesh, need sds
% interactive3D, need sds. 需要把3DGS转成NGP，per scene optimizaiton
% mvdrag3D 需要借助2D diffusion做编辑。需要SDS做refine.
% 为了实现可控的3D生成和编辑， Sketch-dream通过改变草图，使用SDS优化来实现想要的编辑，可以实现生动的编辑效果。However, 可控性is constrained due to 是在2D空间中编辑，在没有被选择的视角下不能得到用户想要的效果。 Interactive3D 直接在3D空间中对3DGS进行编辑，使用SDS优化，and convert the 3DGS representation into instantNGP. MVDrag 将3D空间中的drag操作投影到multiview image中，使用2D diffusion的编辑能力进行编辑，接着通过LGM得到编辑后的3DGS表示，then use SDS refine. 这些在3D空间中进行编辑能以用户友好的方式进行。 However，所有这些方法都需要借助2D generative先验，会导致几何不一致的问题as described in  Sec. 1, and need time-consuming optimization. 与之不同，我们的方法在3D空间中拉拽编辑稀疏的seed point clouds, 不需要2D generative先验，通过our seed-point-driven 策略得到编辑后的3DGS，而无需optimization.
To enable controllable 3D generation and editing, SketchDream~\cite{liu2024sketchdream} allows users to modify the sketch and achieve edits using SDS optimization for vivid results. However, its controllability is limited as user modifications are made in 2D space, which may not produce the desired effect for unselected viewpoints.  Interactive3D~\cite{dong2024interactive3d} directly edits 3DGS in 3D space using SDS optimization and predefined operations, converting the 3DGS representation into InstantNGP~\cite{muller2022instant} with further refine. MVDrag3D~\cite{chen2024mvdrag3d} projects 3D-space drag operations onto multiview images, using 2D diffusion editing capabilities, and infers the edited 3DGS through LGM~\cite{tang2025lgm}, followed by SDS refinement. These methods offer a more user-friendly experience. However, all of these methods rely on 2D generative priors, which may lead to geometric inconsistencies (as discussed in Sec. \ref{sec:related-2d-diffusion}), and require time-consuming optimization. In contrast, our method enables interactive manipulation of sparse seed points in 3D space, applying seed-point-driven deformation to modify the 3DGS without 2D priors or additional optimization, offering a more user-friendly editing experience.
% However, they all rely on 2D generative priors, leading to geometric inconsistencies as discussed in Sec. 1, and require time-consuming optimization. In contrast, our method edits sparse seed point clouds directly in 3D space,  achieving the desired 3DGS edits without the need for optimization.without relying on 2D generative priors,
%先讲一些2D的编辑方法，再讲3D编辑的方法。再说目前没有和我们一样的
