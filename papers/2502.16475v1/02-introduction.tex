\section{Introduction}

% \YH{The field of 3D generation is highly popular at present and enjoys a wide range of applications in research and industry scenarios. However, compared to the traditional 3D modeling process, where artists can directly interact and edit the 3D models, enabling direct editing of 3D models within the 3D generation process is still an area awaiting in-depth study. 
% %This is especially true for the problem of generating 3D models from single-view images. 
% This challenge is markedly accentuated in the context of 3D model generation from single-view images.
% In such cases, for the parts of the generated model that are not visible in the input image, the generation results may have a large stylistic difference from the visible parts or may not even be realistic at all.
% %
% To make the generated results practically usable and to augment the diversity of the generated 3D models, a promising approach is to introduce human interaction into the generation process while improving the geometric consistency of the generated results. However, existing methods struggle to maintain geometric consistency (Sec.~\toref{related}).
% Some studies have explored allowing users to control the final result by modifying the input image as discussed in Sec.~\toref{related}. However, this cannot effectively address the aforementioned problems.

% To align with the creative aspirations and modeling requirements of artists, we hope to find a method that can enable users to directly adjust and control the 3D shape during the generation process.
% %
% To this end, we propose an innovative approach, \textsc{Dragen3D}, utilizing sparse control points for the purpose of manipulating the object shape represented by 3D Gaussians (3DGS) within the 3D generation framework.
% %we propose a method. We attempt to use sparse control points to control the shape of geometric objects. 
% The key point of this approach lies in the integration of the deformation of control points with the resulting generated geometry.
% %
% To accomplish this, we construct a sophisticated transformer-based framework that operates\todo{} within the latent space. We train a Variational Autoencoder (VAE) capable of encoding the complex 3D information of an object into a compact latent space and accurately decoding it back to the 3D domain.
% %Firstly, we build a transformer framework to combine these two kinds of information in the latent space. Specifically, we first train a Variational Autoencoder (VAE) which can encode the 3D information of an object into the latent space and decode it back to 3D. 
% Then, we introduce a module tasked with the generation of 3D control points corresponding to the objects depicted in the input image.
% Furthermore, a mapping module is incorporated to associate the information of control points\todo{control point latent?} with the latent space of the VAE. 
% In this way, when the control points undergo deformation, the corresponding latent codes are updated accordingly, thereby enabling the generation of the final deformed 3D output upon decoding.

% By employing this two-step generation process from control points to the latent space and ultimately to the final geometry, we can initially control the overall shape of the generated geometry with control points, avoiding inconsistencies in the realism of the generated 3D objects compared to the real world as shown in our experiments Fig.~\ref{}.
% }


The field of 3D generation is highly popular at present and enjoys a wide range of applications in research and industry scenarios. However, compared to the traditional 3D modeling process where artists can directly interact and edit high-quality 3D models, achieving high geometric fidelity and direct editing within the 3D generation process is still an area awaiting in-depth study.

This challenge becomes even more pronounced in the context of 3D model generation from single-view images. For parts of the model not visible in the input image, the generated results may exhibit significant stylistic discrepancies from the visible regions, fail to achieve multi-view geometric consistency, or even appear unrealistic.
%
To align with the creative aspirations and modeling requirements of artists, some studies, as discussed in Sec.\ref{sec: relatd_edit}, have explored user control through input image modifications or predefined editing operations, these methods do not effectively address the aforementioned issues. 
To enhance the practical usability and quality of generated 3D models, we aim to develop a method that enables multi-view geometry consistent 3D generation, while allowing users to directly adjust and control the 3D shape during the generation process.
% To enhance the practical usability and quality of generated 3D models, a promising approach is to incorporate human interaction into the generation process. While some studies, as discussed in Sec.
% \ref{sec: relatd_edit}, have explored user control through input image modifications or predefined editing operations, these methods do not effectively address the aforementioned issues.

% To align with the creative aspirations and modeling requirements of artists, we aim to develop a method that enables multi-view geometrically consistent 3D generation, while allowing users to directly adjust and control the 3D shape during the generation process.
%
To this end, we propose an innovative approach, \textsc{Dragen3D}, utilizing sparse seed points for manipulating the object shape represented by 3D Gaussians (3DGS) and enhancing the multi-view geometry consistency within the 3D generation framework.
%
% The key point of this approach lies in ensuring overall geometric consistency through seed points, along with the integration of control point deformation into the resulting generated geometry.
%
To accomplish this, we train a Variational Autoencoder (VAE) that encodes the complex 3D information of an object into a compact latent space and accurately decodes it back into the 3D domain, while also supporting subsequent 3D generation in the latent space.
%Then, we introduce a module tasked with the generation of 3D control points corresponding to the objects depicted in the input image.
% Furthermore, a mapping module is incorporated to associate the information of w33e points with the latent space of the VAE. 
% In this way, when the control points undergo deformation, the corresponding latent codes are updated accordingly, thereby enabling the generation of the final deformed 3D output upon decoding.
%
Then, we introduce a module tasked with generating 3D seed points corresponding to the objects depicted in the input image. This ensures the geometric consistency of the seed points, thanks to the easy learning of their sparse distribution.
%
Furthermore, a mapping module is incorporated to associate the information of seed points with the latent space of the VAE. 

% In this way, the geometric consistency of the seed points ensures the generation of geometrically consistent 3D results. When the seed points undergo deformation, the corresponding latent codes are updated accordingly, enabling the generation of the final deformed 3D output upon decoding.


Our experiments show that \textsc{Dragen3D} produces multi-view geometry consistent 3D results as shown in Fig. \ref{fig:image-3d}. When the seed points undergo deformation, the corresponding latent codes are updated accordingly, enabling the generation of the final deformed 3D output upon decoding, as shown in Fig. \ref{fig:edit}
% In this way, the geometric consistency of the seed points ensures the generation of geometrically consistent 3D results, as shown in \ref{fig:image-3d}. When the seed points undergo deformation, the corresponding latent codes are updated accordingly, enabling the generation of the final deformed 3D output upon decoding, as shown in Fig. \ref{fig:edit}

Our contributions can be summarized as follows:  
\begin{itemize}

    \item We propose the Anchor-GS VAE, which encodes 3D geometry and appearance into anchor latents and decodes them into 3DGS, making it easy to build while enabling efficient latent-space generation. 
    \item We introduce a Seed-Driven Strategy that generates sparse seed points from a single image for geometric consistency and maps them to anchor latents via the Seed-Anchor Mapping Module. 
    \item We design a Seed-Points-Driven Deformation module, enabling user-friendly geometric editing of 3DGS through drag operations on seed points. 
\end{itemize}
We will open-source the implementation of our method and the trained models.


% 3D generation is currently a hot research topic, with methods capable of generating ideal 3D representations from a single input image. This technology has found widespread applications in areas such as 3D modeling, VR, AR, and digital content creation. However, the development of this technology still faces certain limitations. First, some methods rely on 2D diffusion priors, which often struggle to handle multiview consistency when lifting the 2D priors to 3D. Additionally, the lack of controllability in the generated results restricts the further application of generative 3D techniques, as the content often fails to meet specific user needs and lacks the flexibility for editing.


% To achieve 3D generation, some methods rely on extensive 2D diffusion priors, optimizing or using a feed-forward approach from multiview diffusion to obtain 3D representations. While these methods successfully combine 2D priors with 3D representations, the multiview 2D diffusion priors introduce viewpoint inconsistencies that affect the quality of the 3D representation, leading to geometric and texture misalignments. Other methods discard 2D diffusion priors and generate 3D representations directly from image conditions using feed-forward or diffusion techniques, making significant strides in geometric consistency and generation quality. However, these methods neglect the controllability of 3D generation, which is critical for user-defined outcomes, as the inherent variability in generating 3D representations from a single image can lead to unpredictable and undesired results. Some methods have explored controllable 3D generation and editing, but they still rely on 2D diffusion priors, which cause multiview inconsistencies and require time-consuming optimization or post-processing, making them less user-friendly.

% We propose a novel method, \textsc{Dragen3D}, that addresses the aforementioned limitations of the existing methods. We design the Anchor-GS VAE module to compress 3D information into latent space and decode it into 3DGS, eliminating the need for a pre-constructed large 3DGS dataset. To generate and edit the output 3DGS, we propose seed-point-driven anchor latent generation. This module first takes an image as input and generates a set of seed points representing the rough 3D geometry of the object in the input image and in the meantime accepts drag-style editing on the seed points. Then we calculate a seed-anchor mapping via flow matching to convert the 3D information on the generated (and edited) seed points to the anchor latent space for the final 3DGS generation. Our approach resolves geometric inconsistencies and enables intuitive drag-style editing of the generated 3DGS.  

% Our contributions can be summarized as follows:  
% \begin{itemize}
%     % \item We propose Anchor-GS VAE, encode the geometry and texture information into anchor latents then docode into 3DGS, which is easy to train and enable the generation in latent space.
%     % \item We propose Seed-Driven-Strategy to 将latents的generation分解为: 从单图生成sparse seed points， which保证了我们生成的几何一致性, and a Seed-Anchor Mapping Module将seed points map到对应的anchor latents .
%     % \item 我们设计了Seed-Points-Driven Deformation模块，通过对seed points进行drag操作，以用户友好的方式实现对3DGS的可控几何编辑.
%     \item We propose the Anchor-GS VAE, which encodes geometry and texture into anchor latents and decodes them into 3DGS, making it easy to build while enabling efficient latent-space generation. 
%     \item We introduce a Seed-Driven Strategy that generates sparse seed points from a single image for geometric consistency and maps them to anchor latents via the Seed-Anchor Mapping Module. 
%     \item We design a Seed-Points-Driven Deformation module, enabling user-friendly geometric editing of 3DGS through drag operations on seed points. 
% \end{itemize}
% We will open-source the implementation of our method and the trained models.
 


