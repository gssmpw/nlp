\section{Anchor-Based 3DGS VAE}
\label{sec:anchor-vae}
% \subsection{Overview}
% \subsection{Overview}
% 我们采用基于anchor points的方式来表示3D Gaussians， 并构建了Anchor-GS VAE 通过从3D资产中采样到的点和渲染图像来构建3D Gaussians。
% \JY{need a section to describe the relationship between VAE and diffusion?}

% We adopt an anchor-based approach to obtain 3D Gaussians and propose the Anchor-GS VAE, which constructs 3D Gaussians by leveraging points sampled from 3D assets and rendered images. 
% % Unlike previous methods\tocite{}, with our Anchor-GS VAE, there is no need to construct a large-scale 3D Gaussian dataset \YH{as a priori}. Instead, the 
% The VAE can be trained to reconstruct high-quality 3D Gaussians using only sampled point clouds and rendered images \YH{single image?}. Moreover, our anchor-based method interpolates the features for each Gaussian point, offering greater flexibility compared to the regular grid-based feature representation, where features are stored at fixed grid corners \YH{what's the advantage of the flexibility?}. Additionally, the anchor point-based approach aligns more naturally with the point-based rendering technique of 3D Gaussians, which helps \YH{advantage?}.


We adopt an anchor-based approach to obtain 3D Gaussians, where the ``anchor'' refers to anchor points that are surface points capturing the main geometry of the object. 
% and (2) the anchor latents that encodes appearance information from the image on the anchor points.
%
% We propose Anchor-GS Variational Autoencoder(Anchor-GS VAE), which constructs 3D Gaussians by leveraging points sampled from 3D assets and rendered images.
We design and train an Anchor-Gaussian VAE that utilize 
%
% The Anchor-GS VAE utilizes 
Geometry-Texture Encoder $\mathcal{E}$ to encode 
% sampled points and one of the rendered images 
% the surface point cloud, a set of rendered images, and the anchor points 
geometry and appearance information
of a 3D object 
into a set of fixed length latents, called anchor latents $Z$ (Sec. \ref{sec:enc}). Subsequently, the Decoder $\mathcal{D}$  decodes these anchor latents into Gaussian primitives in a coarse-to-fine manner (Sec. \ref{sec:dec}). The encoder and decoder are trained together in an end-to-end manner, with the loss function (Sec. \ref{sec:loss}).

% Through the anchor-based representation, we can compress both geometric and texture information into a set of anchor latents, making it more compact compared to grid-based methods, which is beneficial for latent generation. Additionally, our anchor-based approach offers greater flexibility due to the discrete nature of point clouds, enabling geometric control and deformation, as described in Sec. \ref{sec:seed-point-driven}.

% 借助于我们的Anchor-GS VAE，我们无须构建大规模的3DGS数据集，而仅仅通过采样点云和渲染图像即可训练VAE来重建得到高质量的3DGS。 此外，our anchor-based method 通过插值得到每个Gaussian points的feature，相比于regular grid的特征表示方式更加灵活，which features 在固定的网格顶点上存储。 and  the anchor points based method与3DGS的基于点的渲染方式更加匹配。
% 1. 采用VAE来做gaussian的重建
% 1. 基于anchor的表示的优势 1. 无需构建大规模的高斯数据集 2. 显存占用友好 3. 与高斯的基于point渲染相匹配

\subsection{Geometry-Texture Encoder}
% 1. 讲通过FPS sample组织几何
% 2. 通过点投影组织纹理
% Start with sampled point cloud M and rendered image I, we want to encode the geometry and texture information into a set of Anchor features. 接着Anchor-GS Decoder 将会从这些Anchor features 中decode出Gaussian Primitives to represent the 3D assert we want to reconstruct.
% Starting with the point cloud $\mathbf{X}_\mathcal{M} \in \mathbb{R}^{M\times 3}$ sampled from a 3D object and the rendered image \(I\) of the object from certain viewpoint, we aim to encode the texture and geometry information into a set of anchor features $Z$. Subsequently, the Decoder $\mathcal{D}$ of Anchor-GS VAE would decode these anchor features into Gaussian primitives, which serves as a representation of the 3D asset we seek to reconstruct.

% Similar to \cite{zhang20233dshape2vecset} and \cite{zhang20223dilg},
The Geometry-Texture Encoder encodes the anchor points, the surface point cloud, and a set of rendered images of an object into a latent space.
%
We obtain the anchor points $\mathbf{X}_\mathcal{N}$ of an object by sampling from the surface point cloud $\mathbf{X}_\mathcal{M} \in \mathbb{R}^{M\times 3}$ of a 3D object using Farthest Point Sampling (FPS) method, which is similar to \cite{zhang20233dshape2vecset, zhang20223dilg}.
% Starting with the point cloud $\mathbf{X}_\mathcal{M} \in \mathbb{R}^{M\times 3}$ sampled from a 3D object surface, we use Farthest Point Sampling (FPS) method to sample $N$ anchor points $\mathbf{X}_\mathcal{N}$ from $\mathbf{X}_\mathcal{M}$, which is similar to \cite{zhang20233dshape2vecset, zhang20223dilg}.
% : 
% \begin{equation}
%     \mathbf{X}_\mathcal{N} = \{x_i\}_{i \in \mathcal{N}} = \mathbf{FPS}(\{x_i\}_{i \in M}) 
% \end{equation}
% W
Here \( \mathcal{N} \subsetneqq \mathcal{M} \), represents the index set of point clouds, with default settings of $|\mathcal{N}|=2048$ and $ |\mathcal{M}|=4096$, and \( \mathbf{X}_\mathcal{N} \in \mathbb{R}^{N \times 3} \) denotes the sampled anchor points. 

To encode the appearance information, we then project these anchor points onto the image feature plane $P_I \in \mathbb{R}^{H\times W \times C}$, which is encoded from the rendered image \(I\) of a known viewpoint:
% \begin{equation}
$
    \forall i \in \mathcal{N},\; \; \;  f_{i}=\Psi( \Pi_I(x_i), P_I)
$
% \end{equation}
, where \( \Pi_I(x_i) \) represents the projection of \( x_i\) onto the image plane of \( I\) using the camera parameters of \( I\), and \( \Psi \) denotes bilinear interpolation. 
%
% The poisition encoing of xi and fi can represent the geometry and texture imformation of anchor xi.  And then 我们将这些特征输入到两层的Transformer blocks中， 分别利用point clouds M和 image token extract from input image I 来做cross-attention:
%
The \( f_i \) and positional encoding of \( x_i \)  represents the  texture information and geometric of the \( i \)-th anchor. 

To allow each anchor to capture more global information, we then input these features into two layers of Transformer blocks, which utilize point clouds $\mathbf{X}_\mathcal{M}$ and image tokens extracted from the input image \( I \) to perform cross-attention:
\begin{equation}
\begin{aligned}
% \begin{align}
   & Z^{'} = \mathbf{Transformer}_1(\{(\mathbf{PE}(x_i);f_i) \}_{i \in \mathcal{N}}| \{\mathbf{PE}(x_i)\}_{i \in \mathcal{M}}) \\
& Z = \mathbf{Transformer}_2(Z^{'}| F_I) 
% \end{align}
\end{aligned}
\label{eq: enc}
\end{equation}
% 其中，PE表示 positional encoing, (;)表示在channel维度进行concat, F_I表示由Image Encoder从input image I extract到的image feature token， 这里我们使用的是DINOV2
where $Z$ represents the anchor latents obtained through encoding, \textbf{PE} represents the positional encoding, and (;) denotes concatenation along the channel dimension. $F_I \in \mathbb{R}^{N\times C}$ refers to the image feature tokens extracted by the Image Encoder from the input image $I$, where we use DINOv2\cite{oquab2023dinov2} for the feature extraction.
And $\mathbf{Transformer}(|)$ denotes a Transformer block with cross-attention.
All these encoding processes can be collectively represented by $\mathcal{E}$:
\begin{equation}
    Z = \mathcal{E}(\mathbf{X}_\mathcal{N} \mid \mathbf{X}_\mathcal{M} , I)
\end{equation}
% \JY{
% or:
% \begin{equation}
%     Z = \mathcal{E}(\mathbf{X}_\mathcal{N}  , I)
% \end{equation}
% }
After passing through the encoder $\mathcal{E}$, the anchor feature 
$ Z$ simultaneously encodes both geometric and texture information.
% 接着我们将anchor feature投影到latents维度C,
\label{sec:enc}

\subsection{Decoder}
\label{sec:dec}
% 在Decoder中，我们采用coarse 2 fine的方式来逐步得到Gaussian Primitives. 由于在Encoder中3D的集合信息和纹理信息被gather在了同一组anchor feature Z中， and 我们采用首先decode出一个coarse的几何，接着再由coarse的集合恢复出更精细的几何和相对应的纹理。
% 具体来说，我们将Z通过Transformer block with self-attention:
% \begin{equation}
% Z^L = Transformer(Z)
% \end{equation}
% 在这里Transformer block具有L层,and $\{ Z^j\}_{j=1..L}$表示Transformer每一层block的输出，其中$Z^L$是最终的output. 
In the Decoder, we adopt a coarse-to-fine approach to progressively obtain the Gaussian primitives, which enables higher-quality and more complete geometry. In the Encoder $\mathcal{E}$, both geometry and texture information are consolidated into a set of anchor latents \( Z \), which is first decoded into a coarse geometry and then refined to recover more detailed geometry and corresponding textures.
% We first decode a coarse geometry and then refine it  to recover more detailed geometry and corresponding textures.

Specifically, we apply Transformer with self-attention to \( Z \):
\begin{equation}
    Z^L = \textbf{Transformer}(Z)
\end{equation}
Here, the Transformer block consists of \( L \) layers, and \( \{ Z^j \}_{j=1..L} \) represents the output at $j$-th layer of the Transformer with $Z^L$ being the final output.  We select the output from the \( k \)-th layer ($k=2$ and $L=8$ in default) as \( Z^{\text{coarse}} \), and the output from the last layer as \( Z^{\text{fine}} \). We first pass \( Z^{\text{coarse}} \) through a linear layer to reconstruct the anchors' spatial positions:
\[
\hat{\mathbf{X}}_\mathcal{N}
 = \textbf{Linear}(Z^{\text{coarse}})
\]


The symbol \( \hat{\mathbf{X}}_\mathcal{N} \in \mathbb{R}^{N \times 3} \) represents the reconstructed positions of anchor points, which approximates the coarse geometry. Then, we assign \( m \) ($m=8$ in default) Gaussian points to each anchor point. The positions of these Gaussian points are determined based on the anchor points' positions and a set of offsets derived from \( Z^{\text{fine}} \). For the \( i \)-th anchor point, we have:
\begin{equation}
\begin{aligned}
     \{ O_i^1, \dots, O_i^m \} &= \textbf{Linear}(z_{i}^{\text{fine}}) \\
     \{ \mu_i^1, \dots, \mu_i^m \} &=  \hat{x}_i+ \{ O_i^1, \dots, O_i^m \}
\end{aligned}
\end{equation}
% 在这里$z_{i}^{fine}$ and $\hat{x}_i$ 为第i个anchor point的decode得到的fine feature 和 coarse position， ${O_i^j}_{j=1..m}$为第j个gaussian点相对于anchor position的便宜，and ${\mu_i^j}_{j=1..m}$为最终每个Gaussian point的位置
where \( z_{i}^{\text{fine}} \) is the fine feature of $i$-th anchor and \( \hat{x}_i \) represents the coarse position decoded for the \( i \)-th anchor point. Here, \( \{ O_i^j \}_{j=1..m} \) denotes the offsets of the \( j \)-th Gaussian point relative to the anchor position, and \( \{ \mu_i^j \}_{j=1..m} \) represents the final positions of the Gaussian points. This way, we obtain a set of Gaussian point positions with dimensions \(  \mathbb{R}^{N' \times 3} \), where \( N' = N \times m \), representing the final fine-grained geometry.

For each Gaussian point, we can assign its other attributes by interpolating from its $k$ ($k=8$ in default) nearest anchors in the neighborhood:
\begin{equation}
    z_i = \frac{\sum_{k \in \mathcal{N}(\mu_i)} e^{-d_k}z_{k}^{fine}}{\sum_{k \in \mathcal{N}(\mu_i)}e^{-d_k}}
    \label{interpolate}
\end{equation}
where $\mathcal{N}(\mu_i)$ represents the set of neighboring anchor points of Gaussian point position $\mu_i$, and $d_k$ represents the Euclidean distance from $\mu_i$ to the reconstructed position of $z_k^{fine}$.
Then we can use a linear layer to decode the attributes color $c_i$, opacity $o_i$, scale $scale_i$, and rotation $rot_i$ of a Gaussian primitive $z_i$: $\{ c_i, o_i, scale_i,rot_i\} = \mathbf{Linear}(z_i)$.

% Here, $\{ c_i, o_i, scale_i,rot_i\}$represent the color, opacity, scale, and rotation of the current Gaussian primitive, respectively.

\subsection{Loss Function}
\label{sec:loss}
% 由于我们高效的基于anchor的表示设计，我们的VAE的训练不需要预先构建大规模的3DGS的数据集. 我们使用decoder得到的3DGS的渲染图像$I_{pred}$与$I_{GT}$的渲染损失来监督整个网络：
% \begin{equation}
%     \mathcal{L}_{rgb} = \mathcal{L}_{mse} + \lambda_s\mathcal{L}_{SSIM} + \lambda_l\mathcal{L}_{lpips}
% \end{equation}
% 此外，为了得到更好的几何，我们对decoder 重建得到的anchor points position和Gaussian points position分别与从3D asserts上采样得到的真实点进行3D点云监督:
% \begin{equation}
%     \mathcal{L}_{points} = \lambda_c\mathcal{L}_{cd} + \lambda_e\mathcal{L}_{emd} 
% \end{equation}
% where LCD is the Chamfer Distance (CD) and LEMD is
% the Earth Mover’s Distance (EMD). 再加上对Encoder得到的anchor latents的KL散度regularization，the total loss function can be write as:
% \begin{equation}
%     \mathcal{L} = \mathcal{L}_{rgb} + \mathcal{L}_{points} + \lambda_{kl}  \mathcal{L}_{KL}
% \end{equation}
Thanks to our efficient anchor-based representation design, the training of our VAE does not require pre-constructing a large-scale 3DGS dataset. Instead, we supervise the entire network using the rendering loss between the predicted rendered images and the ground truth images:
\begin{equation}
    \mathcal{L}_{\text{rgb}} = \mathcal{L}_{\text{MSE}} + \lambda_s \mathcal{L}_{\text{SSIM}} + \lambda_l \mathcal{L}_{\text{lpips}}
\end{equation}
where $\mathcal{L}_{\text{MSE}}$represents the pixel-wise Mean Squared Error (MSE) loss, $\mathcal{L}_{\text{SSIM}}$ represents the Structural Similarity Index (SSIM) loss and $\mathcal{L}_{\text{lpips}}$ represents the perceptual loss.
% \YH{where $\lambda_s$ and $\lambda_l$ are ... respectively. (if not discussed here, mentioned it in the result section)}

In addition, to obtain better geometry, we apply 3D point cloud supervision to both the reconstructed anchor point positions and the Gaussian point positions, comparing them with ground-truth points sampled from the 3D assets:
\begin{equation}
    \mathcal{L}_{\text{points}} = \lambda_c \mathcal{L}_{\text{cd}} + \lambda_e \mathcal{L}_{\text{emd}}
\end{equation}

Here, \( \mathcal{L}_{\text{cd}} \) denotes the Chamfer Distance (CD), and \( \mathcal{L}_{\text{emd}} \) represents the Earth Mover's Distance (EMD).

Finally, incorporating KL divergence regularization on the anchor latents produced by the encoder, the total loss function is defined as:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{rgb}} + \mathcal{L}_{\text{points}} + \lambda_{\text{kl}} \mathcal{L}_{\text{KL}}
\end{equation}





