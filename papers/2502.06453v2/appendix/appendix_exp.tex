\section{Additional Experimental Results}
\label{appendix:exp}

\subsection{Categorizing Model Responses Across Problem Variations}
\label{appendix:category}
Recall that for each problem, we have a \SAME modification which can be solved using the same method as the original problem, and a \HARD modification which requires more difficult problem-solving skills. Therefore, there are 8 possible cases regarding the correctness of the model's responses to the three problems. Modulo the fluctuations of the model's correctness among the \SAME variations, we can summarize the model's responses into the following 4 cases:
\begin{itemize}[itemsep=1pt, parsep=1pt, topsep=1pt]
    \item \textbf{Case I}: at least one of the original problem and the \SAME modification is solved \textit{correctly}, and the \HARD modification is also solved \textit{correctly}.
    \item \textbf{Case II}: both the original problem and the \SAME modification are solved \textit{incorrectly}, and the \HARD modification is also solved \textit{incorrectly}.
    \item \textbf{Case III}: both the original problem and the \SAME modification are solved \textit{incorrectly}, but the \HARD modification is solved \textit{correctly}.
    \item \textbf{Case IV}: at least one of the original problem and the \SAME modification is solved \textit{correctly}, but the \HARD modification is solved \textit{incorrectly}.
\end{itemize}
For each of the models, we calculate the percentage of the responses in \cref{tab:cate}. As expected, stronger models have a higher percentage of Case I responses and a lower percentage of Case II responses. Interestingly, the percentages of Case III responses are small (less than 10\%) but non-zero, where the models cannot solve the easier variants but can solve the hard variant correctly. After manual inspection, we found that this is due to the misalignment between the models' capabilities and the annotators' perception of the difficulties of math problems. 


\begin{table*}[t]
\caption{Number and percentage of the models' responses that belong to each of the four categories.}
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lccccccc}
 \toprule
 Model &  Case I &  Case II & Case III & Case IV &\\ \midrule
Gemini-2.0-flash-thinking-exp &  212 (75.99 \%) & 5 (1.79 \%) & 6 (2.15 \%) & 56 (20.07 \%) &  \\ 
o1-preview &  194 (69.53 \%) & 10 (3.58 \%) & 8 (2.87 \%) & 67 (24.01 \%) &  \\ 
o1-mini &  218 (78.14 \%) & 4 (1.43 \%) & 1 (0.36 \%) & 56 (20.07 \%) &  \\ 
\midrule
Gemini-2.0-flash-exp &  176 (63.08 \%) & 11 (3.94 \%) & 11 (3.94 \%) & 81 (29.03 \%) &  \\ 
Gemini-1.5-pro &  145 (51.97 \%) & 28 (10.04 \%) & 13 (4.66 \%) & 93 (33.33 \%) &  \\ 
GPT-4o &  94 (33.69 \%) & 56 (20.07 \%) & 16 (5.73 \%) & 113 (40.50 \%) &  \\ 
GPT-4-turbo &  81 (29.03 \%) & 72 (25.81 \%) & 15 (5.38 \%) & 111 (39.78 \%) &  \\ 
Claude-3.5-Sonnet &  88 (31.54 \%) & 56 (20.07 \%) & 20 (7.17 \%) & 115 (41.22 \%) &  \\ 
Claude-3-Opus &  49 (17.56 \%) & 99 (35.48 \%) & 25 (8.96 \%) & 106 (37.99 \%) &  \\ 
\midrule
Llama-3.1-8B-Instruct &  21 (7.53 \%) & 137 (49.10 \%) & 7 (2.51 \%) & 114 (40.86 \%) &  \\ 
Gemma-2-9b-it &  22 (7.89 \%) & 164 (58.78 \%) & 11 (3.94 \%) & 82 (29.39 \%) &  \\ 
Phi-3.5-mini-instruct &  22 (7.89 \%) & 161 (57.71 \%) & 18 (6.45 \%) & 78 (27.96 \%) &  \\ 
\midrule
Deepseek-math-7b-rl &  25 (8.96 \%) & 138 (49.46 \%) & 13 (4.66 \%) & 103 (36.92 \%) &  \\ 
Qwen2.5-Math-7B-Instruct &  61 (21.86 \%) & 70 (25.09 \%) & 15 (5.38 \%) & 133 (47.67 \%) &  \\ 
Mathstral-7b-v0.1 &  28 (10.04 \%) & 136 (48.75 \%) & 13 (4.66 \%) & 102 (36.56 \%) &  \\ 
NuminaMath-7B-CoT &  39 (13.98 \%) & 118 (42.29 \%) & 9 (3.23 \%) & 113 (40.50 \%) &  \\ 
MetaMath-13B-V1.0 &  6 (2.15 \%) & 199 (71.33 \%) & 10 (3.58 \%) & 64 (22.94 \%) &  \\ 
MAmmoTH2-8B &  9 (3.23 \%) & 201 (72.04 \%) & 12 (4.30 \%) & 57 (20.43 \%) &  \\
\bottomrule
\end{tabular}
}
\label{tab:cate}
\end{table*}


\subsection{Is Mode Collapse a Problem?}
\label{appendix:naive:memorization}

We provide \cref{tab:naive_memorization} to support \cref{sec:naive:memorization}.

\begin{table*}[htbp]
\caption{The number of errors with answers that match the corresponding original answers. The edit distances are normalized by the length of the responses to the original problems.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccc}
 \toprule
\multirow{3}{*}{\textbf{Model}}   & \multicolumn{6}{c}{\textbf{\SAME}} & \multicolumn{6}{c}{\textbf{\HARD}}   \\ 
\cmidrule(r){2-7}  \cmidrule(r){8-13}
&  \multicolumn{3}{c}{Num. Errors} & \multicolumn{3}{c}{Normalized Edit Distance} & \multicolumn{3}{c}{Num. Errors} & \multicolumn{3}{c}{Normalized Edit Distance}  \\
\cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10} \cmidrule(r){11-13}
& $n_{\text{same}}$ & $n_{\text{total}}$ & percentage & min. & avg. & max. & $n_{\text{same}}$ & $n_{\text{total}}$ & percentage & min. & avg. & max. \\
\midrule
 
Gemini-2.0-flash-thinking-exp & 2 & 25 & 8.00 & 0.553 & 0.611 & 0.668  & 10 & 61 & 16.39  &  0.508 & 0.679 & 0.976  \\ 
o1-preview & 1 & 34 & 2.94 & 0.652 & 0.652 & 0.652  & 5 & 77 & 6.49  &  0.729 & 1.07 & 1.89  \\ 
o1-mini & 0 & 14 & 0 & N/A & N/A & N/A  & 9 & 60 & 15.00  &  0.559 & 14.7 & 126.0  \\ 
\midrule
Gemini-2.0-flash-exp & 4 & 48 & 8.33 & 0.644 & 0.82 & 1.09  & 13 & 92 & 14.13  &  0.546 & 1.1 & 1.76  \\ 
Gemini-1.5-pro & 5 & 63 & 7.94 & 0.472 & 0.751 & 1.3  & 11 & 121 & 9.09  &  0.257 & 0.866 & 1.58  \\ 
GPT-4o & 4 & 106 & 3.77 & 0.709 & 0.773 & 0.937  & 14 & 169 & 8.28  &  0.489 & 0.777 & 1.2  \\ 
GPT-4-turbo & 5 & 125 & 4.00 & 0.621 & 0.74 & 0.855  & 17 & 183 & 9.29  &  0.636 & 0.932 & 1.61  \\ 
Claude-3.5-Sonnet & 6 & 116 & 5.17 & 0.509 & 0.729 & 0.83  & 13 & 171 & 7.60  &  0.461 & 0.741 & 1.92  \\ 
Claude-3-Opus & 3 & 162 & 1.85 & 0.355 & 0.485 & 0.614  & 15 & 205 & 7.32  &  0.463 & 0.841 & 1.54  \\ 
\midrule
Llama-3.1-8B-Instruct & 13 & 191 & 6.81 & 0.595 & 0.901 & 1.99  & 18 & 251 & 7.17  &  0.618 & 0.946 & 2.7  \\ 
Gemma-2-9b-it & 3 & 202 & 1.49 & 0.361 & 0.506 & 0.716  & 7 & 246 & 2.85  &  0 & 0.662 & 1.08  \\ 
Phi-3.5-mini-instruct & 8 & 199 & 4.02 & 0.427 & 0.61 & 0.832  & 12 & 239 & 5.02  &  0.289 & 0.754 & 1.69  \\ 
\midrule
Deepseek-math-7b-rl & 9 & 186 & 4.84 & 0.189 & 0.423 & 0.676  & 11 & 241 & 4.56  &  0.121 & 1.5 & 4.24  \\ 
Qwen2.5-Math-7B-Instruct & 6 & 135 & 4.44 & 0.376 & 0.591 & 0.813  & 10 & 203 & 4.93  &  0.273 & 1.01 & 4.91  \\ 
Mathstral-7b-v0.1 & 11 & 178 & 6.18 & 0.0989 & 0.645 & 0.964  & 13 & 238 & 5.46  &  0.105 & 0.586 & 0.984  \\ 
NuminaMath-7B-CoT & 12 & 167 & 7.19 & 0.241 & 0.743 & 1.62  & 14 & 231 & 6.06  &  0.204 & 1.04 & 2.22  \\ 
MetaMath-13B-V1.0 & 13 & 258 & 5.04 & 0.27 & 0.55 & 0.748  & 14 & 263 & 5.32  &  0.509 & 0.982 & 2.83  \\ 
MAmmoTH2-8B & 5 & 229 & 2.18 & 0.00214 & 0.666 & 1.25  & 9 & 258 & 3.49  &  0.708 & 0.822 & 1.04  \\

\bottomrule
\end{tabular}
}
\label{tab:naive_memorization}
\end{table*}
















\clearpage
\subsection{The Effect of In-Context Learning}
\label{appendix:ICL}

In \cref{tab:OIC}, we report the performance of in-context learning (ICL) with the corresponding original (unmodified) problem and solution as the in-context learning example. Furthermore, we decompose the influences on \HARD into the \textbf{ICL effect} and the \textbf{misleading effect} in \cref{tab:icl:breakdown} and visualize the influences for representative models in \cref{fig:icl:full}. Please refer to \cref{sec:icl} for the full discussion.


\begin{table*}[htbp]
\caption{Performance comparisons without and with the original problem and solution as the in-context learning example.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccc}
 \toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{\Original} (0-shot)}   & \multicolumn{2}{c}{\textbf{\SAME}} & \multicolumn{2}{c}{\textbf{\HARD}}   \\ 
\cmidrule(r){3-4}  \cmidrule(r){5-6}
& & zero-shot & ICL w. original & zero-shot & ICL w. original\\ 
\midrule
Gemini-2.0-flash-thinking-exp & 92.47 & 91.04 & 94.62 & 78.14 & 79.21 \\ 
o1-preview & 87.81 & 87.81 & 91.40 & 72.40 & 74.19 \\ 
o1-mini & 94.27 & 94.98 & 94.98 & 78.49 & 78.49 \\ 
\midrule
Gemini-2.0-flash-exp & 88.17 & 82.80 & 89.96 & 67.03 & 67.38 \\ 
Gemini-1.5-pro & 77.78 & 77.42 & 88.17 & 56.63 & 60.57 \\ 
GPT-4o & 67.03 & 62.01 & 77.06 & 39.43 & 43.01 \\ 
GPT-4-turbo & 56.99 & 55.20 & 69.89 & 34.41 & 39.07 \\ 
Claude-3.5-Sonnet & 64.52 & 58.42 & 83.15 & 38.71 & 49.46 \\ 
Claude-3-Opus & 41.94 & 41.94 & 68.10 & 26.52 & 33.33 \\ 
\midrule
Llama-3.1-8B-Instruct & 36.56 & 31.54 & 36.56 & 10.04 & 10.75 \\ 
Gemma-2-9b-it & 27.60 & 27.60 & 42.65 & 11.83 & 14.34 \\ 
Phi-3.5-mini-instruct & 26.16 & 28.67 & 36.92 & 14.34 & 14.34 \\ 
\midrule
Deepseek-math-7b-rl & 37.28 & 33.33 & 45.52 & 13.62 & 15.41 \\ 
Qwen2.5-Math-7B-Instruct & 58.78 & 51.61 & 56.99 & 27.24 & 26.88 \\ 
Mathstral-7b-v0.1 & 36.56 & 36.20 & 48.39 & 14.70 & 16.49 \\ 
NuminaMath-7B-CoT & 43.73 & 40.14 & 47.31 & 17.20 & 17.20 \\ 
MetaMath-13B-V1.0 & 21.15 & 7.53 & 11.11 & 5.73 & 3.58 \\ 
MAmmoTH2-8B & 12.90 & 17.92 & 31.18 & 7.53 & 5.73 \\ 
\bottomrule
\end{tabular}
}
\label{tab:OIC}
\end{table*}





\begin{table*}[htbp]
\caption{Effects of in-context learning (ICL) with original example on \HARD. The percentages of $n(\text{correct} \to \text{wrong})$ are normalized by the number of errors with ICL, while the percentages of $n(\text{wrong} \to \text{correct})$ are by the number of errors without ICL. }
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
 \toprule
Model & num. errors (zero-shot) & num. errors (ICL w. original) & $n(\text{correct} \to \text{wrong})$ & $n(\text{wrong} \to \text{correct})$ \\ 
\midrule
Gemini-2.0-flash-thinking-exp & 61 (21.86 \%) & 58 (20.79 \%) & 17 (29.31 \%) & 20 (32.79 \%) \\ 
o1-preview & 77 (27.60 \%) & 72 (25.81 \%) & 21 (29.17 \%) & 26 (33.77 \%) \\ 
o1-mini & 60 (21.51 \%) & 60 (21.51 \%) & 24 (40.00 \%) & 24 (40.00 \%) \\ 
\midrule
Gemini-2.0-flash-exp & 92 (32.97 \%) & 91 (32.62 \%) & 30 (32.97 \%) & 31 (33.70 \%) \\ 
Gemini-1.5-pro & 121 (43.37 \%) & 110 (39.43 \%) & 27 (24.55 \%) & 38 (31.40 \%) \\ 
GPT-4o & 169 (60.57 \%) & 159 (56.99 \%) & 31 (19.50 \%) & 41 (24.26 \%) \\ 
GPT-4-turbo & 183 (65.59 \%) & 170 (60.93 \%) & 33 (19.41 \%) & 46 (25.14 \%) \\ 
Claude-3.5-Sonnet & 171 (61.29 \%) & 141 (50.54 \%) & 27 (19.15 \%) & 57 (33.33 \%) \\ 
Claude-3-Opus & 205 (73.48 \%) & 186 (66.67 \%) & 35 (18.82 \%) & 54 (26.34 \%) \\ 
\midrule
Llama-3.1-8B-Instruct & 251 (89.96 \%) & 249 (89.25 \%) & 18 (7.23 \%) & 20 (7.97 \%) \\ 
Gemma-2-9b-it & 246 (88.17 \%) & 239 (85.66 \%) & 14 (5.86 \%) & 21 (8.54 \%) \\ 
Phi-3.5-mini-instruct & 239 (85.66 \%) & 239 (85.66 \%) & 17 (7.11 \%) & 17 (7.11 \%) \\ 
\midrule
Deepseek-math-7b-rl & 241 (86.38 \%) & 236 (84.59 \%) & 19 (8.05 \%) & 24 (9.96 \%) \\ 
Qwen2.5-Math-7B-Instruct & 203 (72.76 \%) & 204 (73.12 \%) & 32 (15.69 \%) & 31 (15.27 \%) \\ 
Mathstral-7b-v0.1 & 238 (85.30 \%) & 233 (83.51 \%) & 19 (8.15 \%) & 24 (10.08 \%) \\ 
NuminaMath-7B-CoT & 231 (82.80 \%) & 231 (82.80 \%) & 23 (9.96 \%) & 23 (9.96 \%) \\ 
MetaMath-13B-V1.0 & 263 (94.27 \%) & 269 (96.42 \%) & 11 (4.09 \%) & 5 (1.90 \%) \\ 
MAmmoTH2-8B & 258 (92.47 \%) & 263 (94.27 \%) & 12 (4.56 \%) & 7 (2.71 \%) \\
\bottomrule
\end{tabular}
}
\label{tab:icl:breakdown}
\end{table*}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.33\linewidth]{figures/ICL/icl/Gemini-2.0-flash-thinking.pdf}
    \includegraphics[width=0.32\linewidth]{figures/ICL/icl/o1-mini.pdf}
    \includegraphics[width=0.32\linewidth]{figures/ICL/icl/GPT-4o.pdf}
    \includegraphics[width=0.33\linewidth]{figures/ICL/icl/Claude-3.5-Sonnet.pdf}
    \vspace{5mm}
    \includegraphics[width=0.32\linewidth]{figures/ICL/icl/Llama-3.1-8B-Instruct.pdf}
    \includegraphics[width=0.32\linewidth]{figures/ICL/icl/Gemma-2-9b-it.pdf}
    \includegraphics[width=0.34\linewidth]{figures/ICL/icl/Deepseek-math-7b-rl.pdf}
    \includegraphics[width=0.32\linewidth]{figures/ICL/icl/Qwen2.5-Math-7B-Instruct.pdf}
    
    \caption{The error rates (\%) of the models without and with the original problem and solution as the in-context learning (ICL) example. For \HARD, we decompose the influences of in-context learning into \textbf{ICL effect} (the down arrow $\textcolor{brown}{\boldsymbol{\downarrow}}$), which reduces the error rates, and \textbf{misleading effect} (the up arrow $\textcolor{brown}{\boldsymbol{\uparrow}}$), which increases the error rates.
    }
    \label{fig:icl:full}
\end{figure*}





\clearpage
\subsection{Ablation Study: In-Context Learning with the Original Example v.s. In-Context Learning with a Random Example}

In \cref{tab:oic_sic}, we compare (1) the performance of one-shot in-context learning with the corresponding \textbf{original} unmodified (problem, solution) with (2) the performance of ICL with a \textbf{random} problem and solution chosen from the same category as the query problem. We find that ICL with the \textbf{original} problem and solution consistently outperforms ICL with a \textbf{random} example except for only one case.

\begin{table*}[htbp]
\vspace{-3mm}
\caption{Performance comparisons without and with the original problem and solution as the in-context learning example.}
\centering
\resizebox{0.75\textwidth}{!}{
\begin{tabular}{lcccc}
 \toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{\SAME}} & \multicolumn{2}{c}{\textbf{\HARD}}   \\ 
\cmidrule(r){2-3}  \cmidrule(r){4-5}
& ICL w. original & ICL (random) & ICL w. original  & ICL (random)\\ 
\midrule
o1-mini & \textbf{94.98} & 92.83 & \textbf{78.49} & 75.99 \\ 
\midrule
Gemini-1.5-pro & \textbf{88.17} & 75.99 & \textbf{60.57} & 51.97 \\ 
GPT-4o & \textbf{77.06} & 63.08 & \textbf{43.01} & 37.28 \\ 
GPT-4-turbo & \textbf{69.89} & 57.71 &\textbf{ 39.07} & 32.62 \\ 
Claude-3.5-Sonnet & \textbf{83.15} & 62.37 & \textbf{49.46} & 40.86 \\ 
Claude-3-Opus & \textbf{68.10} & 45.52 & \textbf{33.33} & 23.66 \\ 
\midrule
Llama-3.1-8B-Instruct & \textbf{36.56} & 28.32 & \textbf{10.75} & 6.45 \\ 
Gemma-2-9b-it & \textbf{42.65} & 27.60 & \textbf{14.34} & 12.90 \\ 
Phi-3.5-mini-instruct & \textbf{36.92} & 20.07 & \textbf{14.34} & 10.39 \\ 
\midrule
Deepseek-math-7b-rl & \textbf{45.52} & 34.41 & \textbf{15.41} & 13.26 \\ 
Qwen2.5-Math-7B-Instruct & \textbf{56.99} & 55.20 & \textbf{26.88} & 26.16 \\ 
Mathstral-7b-v0.1 & \textbf{48.39} & 24.37 & \textbf{16.49} & 8.96 \\ 
NuminaMath-7B-CoT & \textbf{47.31 }& 24.73 & \textbf{17.20} & 10.04 \\ 
MetaMath-13B-V1.0 & \textbf{11.11} & 8.60 & 3.58 & \textbf{5.38} \\ 
MAmmoTH2-8B & \textbf{31.18} & 3.94 &\textbf{ 5.73 }& 2.15 \\ 
\bottomrule
\end{tabular}
}
\label{tab:oic_sic}
\end{table*}


\subsection{Inference-time Scaling Behaviors}
\label{sec:inference:scaling}

In this subsection, we investigate the inference-time scaling behaviors of LLMs on our benchmarks. 
We compute the pass@k metric following~\citet{chen2021codex}. Specifically, for each problem, we generate $N$ solutions independently, and compute the pass@k metric via the following formula for each $1\leq k\leq N$:
\[
    \mathrm{pass@k} = \mathbb{E}_{\mathrm{problem}} \left[ 1-\frac{ { N-c \choose k}}{ {N \choose k} } \right], \text{ where } c \text{ is the number of correct answers of the } n \text{ runs}.
\]
We also compute the performance of self-consistency~\citep{wang2022self}, a.k.a., majority voting, where for each $k$, we randomly sample $k$ responses from the $N$ runs and get the majority-voted answer. We report the average and standard deviation among 5 random draws.
We only evaluate three models: o1-mini, Llama-3.1-8B-Instruct, and Qwen2.5-Math-7B-Instruct. For Llama-3.1-8B-Instruct, and Qwen2.5-Math-7B-Instruct, we choose $N=64$, while for o1-mini we set $N=8$. The results are plotted in \cref{fig:inference:scaling}.







\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.32\linewidth]{figures/inference_scaling_meta-llama-Llama-3.1-8B-Instruct.pdf}
    \includegraphics[width=0.32\linewidth]{figures/inference_scaling_Qwen-Qwen2.5-Math-7B-Instruct.pdf}
    \includegraphics[width=0.32\linewidth]{figures/inference_scaling_o1-mini.pdf}
    \caption{The effect of scaling up inference-time compute. We report pass@k and self-consistency (SC) accuracies for different numbers of solutions $k$.
    }
    \label{fig:inference:scaling}
\end{figure*}























 
