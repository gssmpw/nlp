\section{Conclusion}
In this work, we study the \after{generalization} of large language models' math reasoning abilities against hard perturbations of the problems. We modified 279 problems from the level-5 problems of the MATH dataset~\citep{hendrycksmath2021} into \SAME (used for control experiments) and \HARD, via simple perturbations and hard perturbations, respectively. We found performance degradations of all models on \HARD, and many of the errors can be traced to a new form of memorization, where the model memorizes the problem-solving techniques from the training set and blindly applies them without judging whether the modified settings are still suitable. Using the original unmodified problem and solution for in-context learning can deteriorate this issue. We expect the \after{generalization} against hard perturbations to be the next major bottleneck of LLMs' reasoning abilities and urge future work in this direction.

