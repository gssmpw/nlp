\section{Experimental Results}

 

    \begin{table*}[htbp]
    
    \caption{Zero-shot CoT performance of the LLMs (accuracy, \%). \Original refers to the set of 279 unmodified problems.  For the \texttt{train} and \texttt{test} columns, we report the accuracies for problems that \textit{originate} from the \texttt{train} split and \texttt{test} split, respectively. }
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{l>{\columncolor{gray!10}}ccc>{\columncolor{gray!10}}ccc>{\columncolor{gray!10}}ccc}
     \toprule
    \multirow{2}{*}{\textbf{Model}}   & \multicolumn{3}{c}{\textbf{\Original}}  & \multicolumn{3}{c}{\textbf{\SAME}} & \multicolumn{3}{c}{\textbf{\HARD}}   \\ 
    \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10}
    &   All & \texttt{train} & \texttt{test} &  All & \texttt{train} & \texttt{test} & All & \texttt{train} & \texttt{test} \\
    \midrule
    Gemini-2.0-flash-thinking-exp &   92.47 & 92.68 & 92.17  &   91.04 & 87.80 & 95.65  &   78.14 & 77.44 & 79.13  \\ 
    o1-preview &   87.81 & 88.41 & 86.96  &   87.81 & 87.80 & 87.83  &   72.40 & 73.78 & 70.43  \\ 
    o1-mini &   94.27 & 93.90 & 94.78  &   94.98 & 93.29 & 97.39  &   78.49 & 79.27 & 77.39  \\ 
    \midrule
    Gemini-2.0-flash-exp &   88.17 & 87.20 & 89.57  &   82.80 & 81.71 & 84.35  &   67.03 & 68.29 & 65.22  \\ 
    Gemini-1.5-pro &   77.78 & 77.44 & 78.26  &   77.42 & 76.83 & 78.26  &   56.63 & 56.10 & 57.39  \\ 
    GPT-4o &   67.03 & 68.90 & 64.35  &   62.01 & 60.98 & 63.48  &   39.43 & 37.80 & 41.74  \\ 
    GPT-4-turbo &   56.99 & 55.49 & 59.13  &   55.20 & 56.71 & 53.04  &   34.41 & 36.59 & 31.30  \\ 
    Claude-3.5-Sonnet &   64.52 & 62.80 & 66.96  &   58.42 & 57.32 & 60.00  &   38.71 & 38.41 & 39.13  \\ 
    Claude-3-Opus &   41.94 & 39.02 & 46.09  &   41.94 & 39.63 & 45.22  &   26.52 & 25.00 & 28.70  \\ 
    \midrule
    Llama-3.1-8B-Instruct &   36.56 & 45.12 & 24.35  &   31.54 & 35.37 & 26.09  &   10.04 & 10.98 & 8.70  \\ 
    Gemma-2-9b-it &   27.60 & 28.05 & 26.96  &   27.60 & 30.49 & 23.48  &   11.83 & 12.80 & 10.43  \\ 
    Phi-3.5-mini-instruct &   26.16 & 27.44 & 24.35  &   28.67 & 26.83 & 31.30  &   14.34 & 15.24 & 13.04  \\ 
    \midrule
    Deepseek-math-7b-rl &   37.28 & 42.68 & 29.57  &   33.33 & 35.37 & 30.43  &   13.62 & 15.85 & 10.43  \\ 
    Qwen2.5-Math-7B-Instruct &   58.78 & 59.15 & 58.26  &   51.61 & 50.00 & 53.91  &   27.24 & 29.88 & 23.48  \\ 
    Mathstral-7b-v0.1 &   36.56 & 43.29 & 26.96  &   36.20 & 42.07 & 27.83  &   14.70 & 16.46 & 12.17  \\ 
    NuminaMath-7B-CoT &   43.73 & 51.22 & 33.04  &   40.14 & 44.51 & 33.91  &   17.20 & 18.90 & 14.78  \\ 
    MetaMath-13B-V1.0 &   21.15 & 32.32 & 5.22  &   7.53 & 7.32 & 7.83  &   5.73 & 4.88 & 6.96  \\ 
    MAmmoTH2-8B &   12.90 & 11.59 & 14.78  &   17.92 & 17.07 & 19.13  &   7.53 & 10.37 & 3.48  \\ 
    \bottomrule
    \end{tabular}
    }
    
    \label{tab:main}
    \end{table*}


\textbf{Evaluation Setting.} We adopt zero-shot chain-of-thought (CoT)~\citep{wei2022chain, kojima2022large} as the standard evaluation method on our benchmarks. For comparison, we also evaluate the  models on the set of the original 279 problems, referred to as \Original in the following subsections. We do not allow any tool usage including access to a code interpreter, as we find that many problems can be trivially solved by writing a brute-force search program. 

To check whether the generated answer matches the ground-truth answer, we adopt an equivalence checker following~\citet{hendrycksmath2021, shao2024deepseekmath}, which first performs string normalization and then uses sympy package to check the equivalence of two mathematical objects. 




\subsection{Benchmarking the performance of LLMs}
\label{sec:benchmarking}






We consider a wide range of language models including long-CoT models, closed-sourced large models, open-sourced small models, and math-specific models. The version information of the models is deferred to Appendix~\ref{appendix:model}.

In \cref{tab:main}, we report the overall accuracies of the LLMs on Original, \SAME, and \HARD, and also separately calculate the accuracies for problems that originate from the \texttt{train} split and \texttt{test} split.
As expected, for all the models we evaluate, we find that the performance on \HARD is significantly lower than the original problems, which indicates \HARD is more difficult. 

In the meantime, most models also suffer a slight performance drop on \SAME compared to the original problems. We note that the performance drops mainly come from the \texttt{train} split. Generalization errors still exist for the state-of-the-art models even when the test examples follow the exact same reasoning patterns as the training problems.  




For problems that originate from the \texttt{test} split, ideally, both the original problem and its \SAME modification should be equally ``unseen'' to the model. We observe mixed results empirically from \cref{tab:main}: for gemini-2.0-flash-exp, GPT-4-turbo, claude-3.5-sonnet, the performance drops are larger than 5\%, while surprisingly the performance of Phi-3.5-mini-instruct increases. For most of the models we evaluated, the accuracies on \SAME \texttt{test} split are close to the accuracies on the original \texttt{test} split. 
We commend that while \citet{srivastava2024functional} found a \textit{relatively} 58\% to 80\% performance drop between their modified benchmark and the original MATH benchmark among a different set of the models (the best model they tested was GPT-4), we did not observe such huge gaps for the models we evaluate, which is a sign of the progress in the robustness of the newly developed models against simple perturbations.





\textbf{Inference-time Scaling.}  Scaling inference-time computes has been shown to be able to boost the performance of LLMs~\citep{wang2022self, brown2024large, wu2024empirical, cobbe2021training, lightman2023let}. We defer the study of inference-time scaling on our benchmarks to \cref{sec:inference:scaling}. 








\subsection{Failure Mode Analysis}
\label{sec:failure:mode}

\input{case_study/case2}

\input{case_study/case1}

To study \after{the generalization abilities of models against hard perturbations} , we focus on the set of problems where the models fail on the \HARD modification but correctly solve either the original problem or the \SAME modification, which accounts for 20\%-47\% of the total problems. For these problems, one can use the correct solutions to the easier problems as a reference to better determine the failure modes on the hard problems. We defer the discussion on the other cases to Appendix~\ref{appendix:category}.


First, we observe general failure modes when models are exposed to harder problems, including making mistakes in basic numerical computations and algebraic operations, making unjustified claims, missing several cases, and lacking certain math knowledge. These types of errors are more prominent in weaker models. 


Besides general failure modes, when we compare the wrong solution to the \HARD modification with the solutions to the easier versions, we are able to recognize an adequate number of \red{memorization issues}. Specifically, we found that models may \textbf{ignore the modified assumptions and presume that the original assumptions still hold}; see \cref{fig:case:2} for an example. In other cases, the models may \textbf{blindly apply the techniques for the original problems} without first determining whether these techniques are still suitable in the modified setting (the responses in \cref{fig:hard_perturb} are such an example generated by GPT-4o). Interestingly, the models may even \textbf{output the desired outcome of the original problem} (not provided in the context) instead of the modified problem, e.g. \cref{fig:case:1}. This kind of memorization behavior is difficult to capture with most existing type of perturbations in the literature (similar to our \SAME) that does not require different solving strategies.




 
    These issues are often coupled with other types of errors and pervasive among the models we evaluated. 
    For large models, we estimate the percentages of errors caused by memorization to be 40\% for o1-mini and 25\% 
    \begin{wrapfigure}{r}{0.12\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/memo-o1mini.pdf}
        \vspace{4mm}
        \includegraphics[width=\linewidth]{figures/memo-claude3-5-sonnet.pdf} 
        \vspace{-2mm}
        \label{fig:memo_percentage}
    \end{wrapfigure}
    for Claude-3.5-Sonnet, via manual inspections of 20 error cases.
    The general failure modes due to insufficient capabilities are largely reduced for stronger models, making the \red{memorization issues} more prominent. 
    As the capabilities of language models continue to advance, we expect the memorization issues will be the next bottleneck of reasoning models, and we urge more studies on investigating \after{the generalization abilities of reasoning models against hard perturbations.}



 



 




















\subsection{Is Mode Collapse a Problem?}
\label{sec:naive:memorization}

We investigate whether the model makes errors due to \red{mode collapse}, which means the model fails to identify the difference between the perturbed problem and the original problem (seen during its training time) and the model's response \textit{collapses} to the response to the original problem with the identical answer. 


For each model, we report $n_{\text{same}}$, the number of problems where the model's final answer coincides with the ground-truth answer of the corresponding original problem. For those responses, we also compute the edit distance between the full response to the modified problem and the full response to the original problem. The full result is deferred to \cref{tab:naive_memorization} in the appendix. 


We see that this type of failure mode accounts for less than 10\% of the total errors except for three models (gemini-2.0-flash-thinking-exp, o1-mini, and gemini-2.0-flash-exp) on \HARD. After manual inspection, we find that except for only 1 problem pair where gemma-2-9b-it generates the identical answer for the original problem and the modified problem, we do not see collapses of the outputs \textbf{in the superficial text form}. Therefore, we conclude that naive recitation of the \textit{training material} is not the major reason for producing the same answers. \textit{Instead}, the model's responses to the modified problems often collapse to the responses to the original problems \textbf{in more subtle manners}, e.g. ignoring or failing to understand the modified assumptions; see \cref{fig:case:2} for an example.
















\subsection{Does In-context Learning Help or Hurt?}
\label{sec:icl}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.33\linewidth]{figures/ICL_main/Gemini-2.0-flash-thinking.pdf}
    \includegraphics[width=0.32\linewidth]{figures/ICL_main/GPT-4o.pdf}
    \includegraphics[width=0.325\linewidth]{figures/ICL_main/Llama-3.1-8B-Instruct.pdf}
    \caption{The error rates (\%) of the models without and with the original problem and solution as the in-context learning (ICL) example. For \HARD, we decompose the influences of in-context learning into \textbf{ICL effect} (the down arrow $\textcolor{brown}{\boldsymbol{\downarrow}}$), which reduces the error rates, and \textbf{misleading effect} (the up arrow $\textcolor{brown}{\boldsymbol{\uparrow}}$), which increases the error rates. %
    }
    \label{fig:icl}
    \vspace{-2mm}
\end{figure*}



In this subsection, we investigate whether using the corresponding original unmodified problem and solution as the one-shot in-context learning (ICL) example will help with the modified problems in \SAME and \HARD. We visualize the influences of ICL for three models in \cref{fig:icl} and defer the full result to \cref{tab:OIC}.

As expected, using the original (problem, solution) pair as a one-shot in-context demonstration boosts the performance of nearly all the models on \SAME, which should be solvable by simply applying the original solution steps to the modified setting.   

As for the \HARD modifications, there are two factors that need to be considered: (1) \textbf{ICL effect}: the original solutions may supply the model with desired mathematical knowledge that is also helpful for solving the modified problems; (2) \textbf{misleading effect}: on the other hand, as there are subtle differences between the original problems and the \HARD modifications, the models may fail to recognize such differences and be misled by the demonstrated solutions.
Accordingly, in \cref{tab:icl:breakdown} and \cref{fig:icl}, and we calculate and visualize (1) $n_{\text{wrong} \to \text{correct}}$, the number of problems that initially the model fails on \textit{without} the in-context demonstrations but answers correctly \textit{with} the in-context demonstrations, and (2) $n_{\text{correct} \to \text{wrong}}$, the number of problems that initially the model answers correctly \textit{without} demonstrations but fails on \textit{with} demonstrations. 

We observe that many \HARD problems become solvable with the original problems and solutions as demonstrations. The percentages to the number of total errors without demonstrations are larger for closed-sourced large models (24\%-40\%) and smaller for open-sourced small models (2\%-15\%), due to their differences in mathematical capabilities and in-context learning capabilities. \textbf{However}, we also observe many \HARD problems become incorrect with demonstrations, and the percentages are higher for large models (18\%-40\%) than small models (4\%-15\%). 
The misleading effect counteracts the effect of in-context learning, leaving only marginal improvements (less than 5\%) on the \HARD for most models. 


As in-context learning can be viewed as a form of (test-time) training, we hypothesize that any naive fine-tuning technique with a limited distribution of problem settings will hurt the \after{generalization} of the language models against hard perturbations. %









 





