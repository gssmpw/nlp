\section{Related Work}


\textbf{Perturbations to Existing Mathematical Benchmarks.} There is a considerable amount of work focusing on performing perturbations to existing mathematical benchmarks.  
\citet{shi2023large} built GSM-IC from GSM8K~\citep{cobbe2021training} by adding irrelevant context to the problem. GSM-Plus~\citep{li2024gsm} creates 8 types of variations to each of the GSM8K problem and ensure that the perturbed problem is of the same difficulty.
\citet{mirzadeh2024gsm} built GSM-Symbolic that alters the numerical values and entity names via symbolic templates of both the problems and the solution steps. 
Similarly, Functional MATH~\citep{srivastava2024functional} is created from the MATH dataset~\citep{hendrycksmath2021}, and Putnam-AXIOM~\citep{gulati2024putnamaxiom} from the Putnam Mathematical Competition. 


This line of work performed \textbf{simple perturbations} to existing mathematical benchmarks and the perturbed problems can be solved with the same solution steps and the same reasoning pattern as the original ones. In contrast, we performed \textbf{hard perturbations} to curate \HARD, where the original reasoning pattern does not apply.








\textbf{Memorization.} Memorization is a well-studied phenomenon in machine learning~\citep{feldman2020neural, zhang2021understanding, feldman2020does} and has become increasingly prevalent in large language models, due to the growing of the pretraining corpora and the scaling of the model sizes. 
Verbatim memorization, i.e., recitation of the training material, has significant potential consequences ranging from privacy violations~\citep{carlini2022privacy, brown2022does,huang2023privacy} and copyright infringement~\citep{shi2023detecting, karamolegkou2023copyright, wei2024evaluating, chen2024copybench} to training data security risks~\citep{carlini2021extracting, nasr2023scalable}.
Prior work has investigated various factors influencing verbatim memorization, including sequence duplicates~\citep{lee2021deduplicating, hernandez2022scaling}, model size~\citep{tirumala2022memorization}, and sequence position~\citep{biderman2023pythia}. 


In contrast, we investigate the effect of memorization within the mathematical reasoning context. Our methodology falls into the category of \textit{counterfactual tests}~\citep{zhang2023counterfactual, wu2023reasoning, zheng2023large,  xie2024memorization}, where we construct perturbed problems different from the existing ones to test the \after{generalization} of LLMs and examine memorization effects.
Through extensive case studies, we find that LLMs can exhibit subtle forms of memorization \textit{beyond} naive verbatim memorization.





\textbf{Comparison with MATH$^2$~\citep{shah2024ai}.}
\citet{shah2024ai} created MATH$^2$ by combining random pairs of skills extracted from MATH~\citep{hendrycksmath2021} to generate harder problems that require both skills to solve. Their benchmark is mathematically harder, but there are no natural ``original problems'' as references. Therefore, MATH$^2$ is not directly suitable for investigating the memorization effects of language models. In comparison, our \HARD are modified directly from the problems in MATH so that the modified problems require harder skills to solve. \HARD can serve as both a harder math benchmark and a testbed to investigate memorizations of LLMs.
