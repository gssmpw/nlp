

\subfile{../tabs/mainmain}



\section{Experiments}

\subsection{Setup}
\paragraph{Datasets}
We conduct experiments on 10 datasets spanning three distinct domains:
\begin{itemize}[leftmargin=*]
\item \textbf{Geometric Reasoning:} To assess geometry problem-solving capabilities, we utilize the Geometry3K dataset~\cite{geometry3k} and three tasks from IsoBench~\cite{isobench}, including Connectivity, Maxflow, and Isomorphism.
\item \textbf{Mathematical Reasoning:} We evaluate mathematical reasoning performance using the Parity, Convexity, and Winner ID tasks from IsoBench~\cite{isobench}.
\item \textbf{Visual Question Answering (VQA):} To test generalizability, we also employ three challenging VQA benchmarks: $V^*$ Bench~\cite{vstar}, MMVP~\cite{mmvp}, and BLINK~\cite{blink}.
\end{itemize}

More detailed introduction to these datasets can be found in Appendix~\ref{sec:app_dataset}.



\paragraph{Baselines}
CoT~\cite{cot} and Visual Sketchpad~\cite{vsk} are employed in our experiments to implement text-only thought and multi-modal thought, respectively. Both of them are further compared under four inference-time scaling methods: Self-Consistency, Best-of-N, Beam Search, and MCTS. Additionally, the following latest multi-modal reasoning frameworks are also compared in our main experiment, including Multimodal-CoT~\cite{mcot}, DDCoT~\cite{ddcot}, and CCoT~\cite{ccot}. Detailed baseline descriptions are provided in Appendix~\ref{sec:app_baselines}.


\paragraph{Metrics} We evaluate the performance using two metrics: \textbf{Accuracy} and \textbf{Pass@K}.
Accuracy measures whether the highest-ranked answer is correct, while Pass@K represents whether the correct answer can be recalled within $K$ answers.

\paragraph{Implementation Details} 
We employ GPT-4o-mini\footnote{gpt-4o-mini-2024-07-18} as the policy and verifier by default.
% GPT-4o-mini\footnote{gpt-4o-mini-2024-07-18} serves as both our primary policy model and the LVLM in verifier.
For all experiments, we set the temperature to 1.0 during generation.
For Self-Consistency and Best-of-N, the number of samples $N$ is set to 5.
For Beam Search, we set both the beam width $B$ and expansion size $N$ to 4.
For MCTS, we set the maximum expansion per node to 4, the maximum number of iterations to 30, and the exploration constant $w$ in UCB to 1.4.
We adopt the consistency-enhanced verifier by default and set the number of samples $N_v$ to 5.




\subsection{Multi-modal vs. Text-only Thought}
\paragraph{Multi-modal thought shows greater advantages compared with text-only thought.}

Table~\ref{tab:main_x} presents compelling experimental results that highlight the superiority of multi-modal thought over both text-only thought and the latest multi-modal reasoning frameworks, particularly when enhanced by inference-time scaling.

A closer analysis reveals notable performance variations across different domains. In geometric reasoning, multi-modal thought consistently outperforms text-only thought under single-chain reasoning, with the exception of Geometry3K, where it initially lags but surpasses text-only thought when inference-time scaling is applied. In mathematical reasoning, the advantages of multi-modal thought become even more evident. For example, while text-only thought initially excels on Winner ID due to the problemâ€™s rich textual context, multi-modal thought progressively closes the gap under inference-time scaling. The similarly superior performance observed across VQA tasks further underscores the adaptability and effectiveness of multi-modal thought, strengthening its potential for broader applications.


\paragraph{Multi-modal thought exhibits a higher performance upper limit.}


\begin{figure}

    \centering
    \includegraphics[width=\columnwidth]{imgs/oracle_bon_mcts.pdf}
    \caption{Performance comparison between text-only thought and multi-modal thought on the Maxflow dataset under Best-of-N (left) and MCTS (right).}
    \label{fig:oracle}
\end{figure}


To further investigate the potential of text-only and multi-modal thought, we compare their performance upper limits on the Maxflow dataset using Best-of-N and MCTS methods. As shown in Figure~\ref{fig:oracle}, within Best-of-N, accuracy consistently improves as $N$ increases, with multi-modal thought achieving a significantly higher upper limit. This trend is further validated by the MCTS results, which exhibit a similar pattern, reinforcing the superior potential of multi-modal thought.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{imgs/cost.pdf}
  \caption{Performance comparison between text-only thought and multi-modal thought varies with the maximum token consumption on the Maxflow dataset under Self-Consistency.}
  \label{fig:cost}
\end{figure}



\begin{figure}[t]
\centering
  \includegraphics[width=\columnwidth]{imgs/mix.pdf}
  \caption{Performance of Self-Consistency on the Maxflow dataset as the number of samples increases, comparing text-only thought, multi-modal thought, and the hybrid form.}
  \label{fig:mix}
\end{figure}

\paragraph{The effectiveness of multi-modal thought relies on higher token consumption.} 
Given the substantial difference in token cost between processing text and image information in LVLMs, it is crucial to understand the trade-off between performance and token usage for text-only and multi-modal thought. To this end, we conduct experiments on the Maxflow dataset using Self-Consistency to analyze their performance under inference-time scaling with varying token limits per sample. 

Figure~\ref{fig:cost} illustrates the relationship between token limits and performance for both text-only and multi-modal thought. Under strict token constraints, multi-modal thought underperforms compared to text-only thought. This is likely due to the limited number of reasoning steps multi-modal thought can sample when the token budget is severely restricted, preventing the model from effectively processing visual information.

However, as the token limits are relaxed, the performance of multi-modal thought improves rapidly. With a more generous token budget, the model can fully leverage visual information, leading to significant performance gains. This highlights a key trade-off: while multi-modal thought enables more complex reasoning, it requires significantly more tokens to process visual inputs effectively. This suggests that future research should prioritize developing methods for compressing or optimizing the token consumption associated with image processing in LVLMs, enabling more efficient utilization of multi-modal thought.





\paragraph{Multi-modal and text-only thought are complementary.}

While our results demonstrate the overall effectiveness of multi-modal thought, we further examine whether text-only thought retains value in certain scenarios. We hypothesize that the two approaches may be complementary, each leveraging different aspects of the input. To test this, we conduct experiments on the Maxflow dataset using the Self-Consistency method, comparing text-only thought, multi-modal thought, and a hybrid form, i.e., voting from both text-only thought and multi-modal thought.

As presented in Figure~\ref{fig:mix}, the hybrid form outperforms both text-only and multi-modal thought individually, suggesting that text-only thought provides valuable insights that enhance reasoning, even in cases where multi-modal reasoning alone may fall short. This performance boost underscores the complementary nature of the two approaches and highlights the potential of a balanced integration of them for more robust multi-modal reasoning.



\subsection{Ablation Study on the Verifier}


\paragraph{Consistency-enhanced verifier outperforms naive prompting-based alternatives.}

To evaluate the superiority of our proposed consistency-enhanced verifier, we compare its performance against classification-based and regression-based verifiers, as described in~\textsection\ref{verifier}. We conduct this comparison on the Geometry3K, Maxflow, and Isomorphism datasets using MCTS with multi-modal thought. As shown in Figure~\ref{fig:verifier-com}, our verifier consistently outperforms both naive prompting-based approaches across all three datasets, demonstrating its effectiveness in addressing the limitations of existing approaches and providing more reliable guidance for inference-time scaling.




\begin{figure}[t]
    \centering
  \includegraphics[width=\columnwidth]{imgs/verifier-com.pdf}
  \caption{Performance comparison of Classification-Based, Regression-Based, and Consistency-Enhanced Verifiers using MCTS with multi-modal thought across three datasets.}
  \label{fig:verifier-com}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{imgs/error_static.pdf}
  \caption{Error statistics on four inference-time scaling methods with multi-modal thought.}
  \label{fig:error_static}
\end{figure}

\paragraph{Consistency-enhanced verifier performance scales with $N_v$.}

Our consistency-enhanced verifier is designed based on the hypothesis that aggregating multiple verifications enhances robustness and performance. Specifically, we expect accuracy to improve as the number of sampled verifications increases. To validate this, we conduct experiments using MCTS on the Geometry3K, Maxflow, and Isomorphism datasets, systematically varying $N_v$. From the results in Table~\ref{tab:N-verifier}, we observe a clear trend: increasing $N_v$ consistently improves performance. This finding strongly supports our hypothesis, confirming the effectiveness of the design rationale for the verifier.
\subfile{../tabs/n-verifier}




\subsection{Error Analysis}

To gain deeper insights into the behavior of multi-modal thought under different inference-time scaling methods, we conduct a detailed error analysis. We identify four primary sources of error: Wrong Decision, Execution Error, Ineffective Operation, and Invalid Reasoning (detailed explanations are provided in Appendix~\ref{sec:app_error}) and then randomly sample 100 error cases from each method across all datasets, manually classifying the underlying causes.


\paragraph{Tree search-based methods significantly reduce intermediate errors.} \label{subsec:error_analysis}


Figure~\ref{fig:error_static} presents the distribution of error types for various inference-time scaling methods. We can find that sampling-based methods exhibit a predominance of intermediate reasoning errors~(Execution Error, Ineffective Operation, Invalid Reasoning).  Among these, Execution Error is the most prevalent, which mostly occurs in VQA tasks. This is likely due to the difficulty expert models face in accurately operating images, coupled with a lack of error-handling logic in the code.  Consequently, simple sampling methods often continue exploring flawed paths after encountering such errors, leading to the failure. In contrast, tree search-based methods significantly mitigate intermediate reasoning errors.  By incorporating verifiers at each step, these methods strategically select optimal reasoning paths, effectively preventing the exploration of erroneous reasoning steps.


\paragraph{Effective verifiers are crucial for further error reduction.}

The reduction in intermediate errors leads to a relative increase of Wrong Decision, highlighting this error type as the new primary challenge.  Further manual observation reveals that these errors primarily stem from limitations in the verifier. While effective at identifying blatant errors, the verifier struggles with more nuanced or complex reasoning steps.  This underscores the critical importance of improving verifier effectiveness, particularly in discerning subtle errors, to fully unlock the potential of multi-modal thought.


\subsection{Policy Model Impact}

\subfile{../tabs/open-source}


To assess the impact of the underlying policy model on the effectiveness of multi-modal thought, we extended our experiments beyond the previously used GPT-4o-mini to include a representative open-source LVLM Qwen2-VL-72B-Instruct and a more advanced closed-source LVLM GPT-4o\footnote{gpt-4o-2024-08-06}. The experiments are conducted on the Geometry3K, Maxflow, and Connectivity datasets, using Best-of-N as the inference-time scaling method.

\paragraph{Performance of multi-modal thought generalizes across models.}
As shown in Table~\ref{tab:policy}, multi-modal reasoning consistently improves performance across all three evaluated LVLMs, suggesting that the benefits of multi-modal thought are not model-specific but rather a generalizable advantage across different architectures and training paradigms.



\paragraph{The stronger the model, the greater the performance gains.}
An intriguing trend also emerges from the results, i.e., the performance gains from multi-modal thought correlate with the strength of the policy model. Weaker models, such as Qwen2-VL, exhibit modest improvements, whereas the more powerful GPT-4o achieves significantly larger gains. This suggests that a model's capacity to effectively extract and integrate visual information is contingent upon its inherent capabilities.  Stronger models, possessing superior reasoning abilities, are able to leverage multi-modal cues more efficiently, resulting in greater performance improvements under inference-time scaling.