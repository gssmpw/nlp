\section{Preliminaries}

In this section, we firstly establish the formulation for both text-only and multi-modal thought~(\textsection\ref{cot}), and then provide a detailed explanation of two inference-time scaling paradigms~(\textsection\ref{scaling}): sampling-based and tree search-based methods.
In addition, we present a prompting-based verifier equipped with a consistency-enhanced mechanism to effectively guide these inference-time scaling methods~(\textsection\ref{verifier}).

\subsection{Thought Formulation} \label{cot}
Chain-of-Thought (CoT, \citealt{cot}) is a technique that encourages the model to generate intermediate reasoning steps $\mathbf{S}=(\mathbf{s}_1,...,\mathbf{s}_n)$ before reaching the final answer $\mathbf{a}$.
In the context of multi-modal reasoning tasks, given a problem $\mathbf{q}$ and images $\mathbf{I}$, each reasoning step $\mathbf{s}_i$ is sampled from an LVLM $\mathcal{M}$:
\begin{equation}
\label{eq:cot}
    \mathbf{s}_i \sim \mathcal{M}\big(\mathbf{q}, \mathbf{I}, \mathbf{s}_{1:i-1}\big).
\end{equation}
This expansion allows for more computations to be deployed and can unlock the ability to solve more complex problems~\cite{lichain}.

Following the success on language-only reasoning tasks, most previous works~\cite{mcot, ddcot, ccot} study \textit{text-only thought}, where $\mathbf{S}$ is defined as a text string.
In this work, we aim to explore the potential of \textit{multi-modal thought}, where any reasoning step $\mathbf{s}_i$ can blend multi-modal information, e.g., both text and image.

% 这也可以在discussion中讲
However, as most prevalent LVLMs still struggle to generate reliable images, we follow \citet{vsk} to allow the LVLM to generate executable code for visual manipulation instead.
Implementation details can be found in Appendix~\ref{sec:app_vsk}.




\subsection{Inference-Time Scaling} \label{scaling}

\subsubsection{Sampling-based Methods}
These methods scale inference-time computation by generating multiple reasoning chains in parallel, and then determining the most promising answer from them. Here we investigate two widely used techniques, Self-Consistency~\cite{tts2-Self-Consistency} and Best-of-N~\cite{tts-mcts1, tts-mcts3}.

\paragraph{Self-Consistency}
This method leverages the intuition that the correct answer typically can be reached using multiple different ways of thinking~\cite{tts2-Self-Consistency}.
Given $N$ sampled reasoning chains $\mathbf{S}_1,...,\mathbf{S}_N$ generated from the LVLM, it selects the most consistent answer via majority voting over their corresponding answers $\mathbf{a}_1,...,\mathbf{a}_N$:
\begin{equation}
    \mathbf{a}^* = \arg\max_{\mathbf{a} \in \{\mathbf{a}_1,...,\mathbf{a}_N\}} \sum_{i=1}^{N} \mathbb{I}(\mathbf{a}_i = \mathbf{a}),
\end{equation}
where $\mathbb{I}(\cdot)$ denotes the indicator function and $\mathbf{a}^*$ represents the selected answer.


\paragraph{Best-of-N}
This approach utilizes trained verifiers to evaluate the correctness of model generated solutions.
Compared to Self-Consistency based on voting, it can be more efficient when a strong verifier is available.
Formally, given $N$ sampled reasoning chains and a verifier $\mathcal{F}(\cdot)$, we have
\begin{equation}
    \mathbf{S}^*=\max_{\mathbf{S}\in \{\mathbf{S}_1,...,\mathbf{S}_N\} }\mathcal{F}(\mathbf{S}),
\end{equation}
where $\mathbf{S}^*$ is the reasoning chain with the highest verifier score, from which the final answer $\mathbf{a}^*$ is derived.



\subsubsection{Tree Search-based Methods}
Sampling-based methods are easy to implement due to their simplicity.
However, they are inefficient because they require exploring full solution paths, even if a mistake occurs early~\cite{xiang2025towards}.
To address this challenge, tree search-based methods are later explored.
They model the problem-solving as a tree search process, where each node represents a reasoning step.
Here, we investigate the most prevalent Beam Search~\cite{tts-bs1, tts-bs2, tot, tts-bs3} and MCTS~\cite{tts-mcts5,tts-mcts8,  tts-mcts2, tts-mcts4, tts-mcts6, tts-mcts7, yao2024mulberry} algorithms in both language-only and multi-modal tasks.


\paragraph{Beam Search}


Adapted from classical beam search algorithms used in sequence generation tasks (e.g., machine translation~\cite{beamsearch4mt, beamsearch4mt2}), this method dynamically explores and prunes reasoning paths at each step for deductive reasoning.
The algorithm operates with a beam width \( B \) (number of retained reasoning chains per step) and an expansion size \( N \) (new thoughts generated per candidate).
Starting with \( N \) initial candidate thoughts sampled from a question $q$ via Equation~\ref{eq:cot}, a verifier \( \mathcal{F}(\cdot) \) scores all candidates, retaining only the top-\( B \) chains.
Each retained chain is then expanded by sampling \( N \) new thoughts, producing \( B \times N \) candidates, after which the verifier reevaluates and prunes the pool to the top-\( B \) chains.
This cycle of scoring and pruning repeats iteratively until all active chains reach finished states (final answers), at which point the highest-scoring finished chain across all iterations is selected as the final answer \( \mathbf{a}^* \).  








\paragraph{MCTS}



It is a decision-making algorithm that is powerful for solving complex problems, as demonstrated by AlphaGo~\cite{alphago, alphago1}. MCTS operates by iteratively exploring the most promising reasoning paths through four key phases: selection, expansion, evaluation, and backpropagation. 

In the selection phase, the algorithm traverses the tree from the root node, recursively choosing child nodes using the Upper Confidence Bound (UCB) metric~\cite{ucb}. This metric strategically balances the exploitation of high-value paths and the exploration of under-visited paths. The UCB formula is defined as:  
\[
\text{UCB}(\mathbf{s}_i) = \frac{V(\mathbf{s}_i)}{C(\mathbf{s}_i)} + w \times \sqrt{\frac{\ln C(\mathbf{s}_{i-1})}{C(\mathbf{s}_i)}},
\]  
where \(V(\mathbf{s}_i)\) and \( C(\mathbf{s}_i)\) denotes the accumulated value and count of being visited of \(\mathbf{s}_i\). \(w\) is a weighting parameter that adjusts the exploration-exploitation balance.
During expansion, the algorithm extends the selected node by generating a new thought through Equation~\ref{eq:cot}.
The evaluation phase then estimates the value of this newly expanded node using a verifier \(\mathcal{F}(\cdot)\).
Finally, backpropagation updates the values of all ancestors, propagating the evaluation results back to the root.

After multiple iterations, the finished reasoning chain with the highest accumulated value is selected, yielding the optimal final answer \(\mathbf{a}^*\).   



\begin{figure}[htp]
  \includegraphics[width=\columnwidth]{imgs/verifier.pdf}
  \centering
  % \caption{\textbf{Consistency-Enhanced Verifier:} The reasoning chain is formatted as input to the LVLM, which is then instructed to verify it through CoT reasoning. By sampling multiple verification responses, the score is then computed by aggregating the verification results.}
  \caption{Three types of verifiers investigated in this work, where the classification-based verifier outputs sparse binary scores (0 or 1) and the regression-based verifier provides dense but inaccurate scores. We introduce the consistency-enhanced verifier to compute dense and accurate scores by aggregating multiple evaluations sampled from the classification-based verifier.}
  \label{fig:verifier}
\end{figure}


\subsection{Verifiers for Inference-Time Scaling} \label{verifier}

The verifier \(\mathcal{F}\) is pivotal in inference-time scaling, tasked with evaluating the validity of reasoning chains. 
Existing approaches fall into two categories: training-based~\cite{llava-critic, shepherd} and prompting-based~\cite{binary1, binary2, scoring1}.
Training-based approaches hold theoretical promise by optimizing verifiers on corresponding labeled data.
However, acquiring such annotations is challenging, resulting in datasets limited in scale and diversity, thereby restricting the generalizability of trained verifiers.
Thus, this work focuses on prompting-based approaches, capitalizing on the strong instruction-following capabilities of LVLMs to assess reasoning chains.  

Formally, given a reasoning chain \(\mathbf{s}_{1:i}\), the verifier \(\mathcal{F}\) operates as:  
\(\mathbf{r} = \mathcal{M}(\mathbf{P}, \mathbf{s}_{1:i})\),
where \(\mathbf{P}\) represents the verification instruction, and \(\mathbf{r}\) is the evaluation output, which includes a scalar score.
Particularly, we investigate two instruction designs for \(\mathbf{P}\): 
\begin{itemize}[leftmargin=*]
    \item \textbf{Classification-Based}~\cite{binary1, gen-verifier,  binary2}: It prompts the verifier to classify the reasoning chain as ``correct'' or ``incorrect.'' While intuitive, this binary output provides sparse feedback, offering limited granularity to guide searching. 
    \item \textbf{Regression-Based}~\cite{scoring1, llava-critic}: Here, the verifier is prompted to output a continuous score \(\in [0, 1]\) reflecting the quality of chains. However, directly predicting precise scores is error-prone, often yielding high-variance results that degrade performance.\footnote{The prompts used for these two designs are in Appendix~\ref{sec:app_verifier_inst}.}
\end{itemize}

To address these limitations, we introduce the \textbf{Consistency-Enhanced Verifier}, compared with the above two in Figure~\ref{fig:verifier}.
This method aggregates $N_{v}$ independent evaluations using classification-based instructions, computing the proportion of ``correct'' classifications across trials. 
In this way, it not only avoids variance in regression-based outputs but also overcomes the sparsity of single-trail classification, enabling more reliable guidance for inference-time scaling.








