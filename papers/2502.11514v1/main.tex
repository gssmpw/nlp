% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{colortbl}

\usepackage{array} % 引入 array 包
\usepackage{ulem}

\usepackage{subfiles}
\usepackage{amsmath}

\usepackage{booktabs}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{multirow}
\usepackage{graphicx}

\usepackage{pgfplots}
\pgfplotsset{compat=1.18} % 版本可能需要根据你的安装调整
\usepackage{subcaption}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{amssymb}
\usepackage{ stmaryrd }

\usepackage{enumitem}

\usepackage{pgfplots}
\pgfplotsset{width=0.48\linewidth, compat=1.15}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Investigating Inference-time Scaling for Chain of Multi-modal Thought:\\A Preliminary Study}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Yujie Lin\textsuperscript{1}}\thanks{These authors contributed equally.},
 \textbf{Ante Wang\textsuperscript{1}}\footnotemark[1],
 \textbf{Moye Chen\textsuperscript{2}},
 \textbf{Jingyao Liu\textsuperscript{1}},
 \textbf{Hao Liu\textsuperscript{2}},
 \\
 \textbf{Jinsong Su\textsuperscript{1}}\thanks{Corresponding author.} and
 \textbf{Xinyan Xiao\textsuperscript{2}}
\\
\\
 \textsuperscript{1}School of Informatics, Xiamen University, China \\
 \textsuperscript{2}Baidu Inc., Beijing, China
\\
 \small{
   \textbf{Correspondence:} \href{yjlin@stu.xmu.edu.cn}{yjlin@stu.xmu.edu.cn} ~~ \href{jssu@xmu.edu.cn}{jssu@xmu.edu.cn} 
 }
}



\begin{document}
\maketitle
\begin{abstract}

Recently, inference-time scaling of chain-of-thought (CoT) has been demonstrated as a promising approach for addressing multi-modal reasoning tasks.
While existing studies have predominantly centered on text-based thinking, the integration of both visual and textual modalities within the reasoning process remains unexplored.
In this study, we pioneer the exploration of inference-time scaling with multi-modal thought, aiming to bridge this gap.
To provide a comprehensive analysis, we systematically investigate popular sampling-based and tree search-based inference-time scaling methods on 10 challenging tasks spanning various domains.
Besides, we uniformly adopt a consistency-enhanced verifier to ensure effective guidance for both methods across different thought paradigms.
Results show that multi-modal thought promotes better performance against conventional text-only thought, and blending the two types of thought fosters more diverse thinking.
Despite these advantages, multi-modal thoughts necessitate higher token consumption for processing richer visual inputs, which raises concerns in practical applications.
We hope that our findings on the merits and drawbacks of this research line will inspire future works in the field.

\end{abstract}

\subfile{sections/1.Introduction}
\subfile{sections/2.Preliminary}
\subfile{sections/3-4.Experiments}
\subfile{sections/5.Discussion}
% \subfile{sections/6.Related_works}
\subfile{sections/7.Conclusion}



\section*{Limitations}

Due to the inability of current LVLMs to generate fine-grained images directly, we rely on generating image operation codes as a substitute, preventing a fully end-to-end multi-modal thought process. Additionally, we do not explore smaller-scale models, such as 7B, as their limited visual processing, instruction-following, and code generation abilities make them unsuitable for objectively evaluating multi-modal thought under inference-time scaling. Another limitation is the exclusion of training-based methods that require long CoT data, as current LVLMs struggle to generate high-quality long reasoning chains over multi-modal datasets, making data construction a significant challenge. Lastly, our study focuses solely on text and image modalities, leaving the generalizability of our findings to other modalities, such as audio and video, an open question.



\bibliography{custom}

\appendix

\subfile{sections/Appendix}


\end{document}
