\section{Detailed Experimental Settings}
\label{app:exp-details}

\subsection{Datasets}
\label{sec:datasets}
For mathematical reasoning tasks, we evaluate our proposed methods and comparison methods on four mathematical datasets that include MATH, MathOdyssey, OlympiadBench, and AIME datasets. 
\begin{itemize}
    \item MATH dataset~\citep{hendrycks2021math} is a dataset comprised of challenging competition math problems and we use its 5,000 testing data for evaluation. 
    \item MathOdyssey dataset~\citep{Fang24Odyssey} contains 387 problems, covering advanced high-school level, university-level, and Olympiad-level mathematics. 
    \item OlympiadBench dataset~\citep{He24OlympiadBench} contains 8,476 Olympiad-level mathematics and physics problems. We select the English problems without images, resulting in a testing dataset of 1,284 problems. 
    \item AIME dataset~\citep{AIME} contains 993 test problems collected from the American Invitational Mathematics Examination, spanning from 1983 to 2024.
\end{itemize}

For code generation tasks, we conduct experiments on three common benchmark datasets. 
HumanEval~\citep{Codex2021} contains 164 hand-written Python programming problems. 
MBPP~\citep{MBPP2021}(sanitized version) consists of 427 entry-level programming problems. 
We also include the introductory-level problems of APPS~\citep{APPS2021}, which contains 1000 problems.

\subsection{Detailes of Mathematical Reasoning Task}

For all the experiments in the main paper, we use a sampling temperature of 1.0 and set the top-k parameter to 0.95. 

\paragraph{Prompt for Math Reasoning Tasks.}

The InternLM2-MATH-Plus 1.8B and 7B models are chat models that facilitate conversations between two roles: ``user'' and ``assistant''. 
The prompt for the “user” role is provided in Prompt~\ref{pt:internlm-math}. Similarly, the prompt for the DeepSeek-Math 7B model is shown in Prompt~\ref{pt:deepseek-math}.

\begin{figure}[htb!]
    \begin{promptbox}[label=pt:internlm-math]{Prompt for InternLM-2-Math-Plus}
    Problem:\textbackslash n\{instruction\}\textbackslash n Let's think step by step\textbackslash n Solution:\textbackslash n
    \end{promptbox}
\end{figure}

\begin{figure}[htb!]
    \begin{promptbox}[label=pt:deepseek-math]{Prompt for DeepSeek-Math}
    {instruction}\textbackslash n Please reason step by step, and put your final answer within \textbackslash\textbackslash boxed\{\{\}\}.
    \end{promptbox}
\end{figure}

\paragraph{Prompt for Math Verbalized Method.}

We observed that the tuned math models are challenging to prompt for generating confidence. Therefore, we adopted the methods from \citet{tian2023just} to calculate the probability based on the likelihood of the first generated ``True'' token and the first generated ``False'' token. The corresponding prompt is provided in Prompt~\ref{pt:math-verb}.

\begin{figure}[htb!]
    \begin{promptbox}[label=pt:math-verb]{Prompt for DeepSeek-Math}
    Question: {question}\textbackslash n Proposed Answer: {answer}\textbackslash n Is the proposed answer:\textbackslash n \textbackslash t(A) True or\textbackslash n \textbackslash t(B) False?\textbackslash n The proposed answer is:
    \end{promptbox}
\end{figure}

\subsection{Detailes of Code Generation Task}

\paragraph{Code Generation.} 
On the code generation task, we let LLM generate a code snippet to solve a given programming problem, and then evaluate its functional correctness based on the ground-truth test cases provided by the dataset. In detail, we set the top \textit{p} to 0.95, and the max generation length to 1024. For code snippet post-processing, we first extract the code text from the code block surrounded by triple-backticks(\texttt{```}), and then we follow~\citet{Codex2021} to truncate the generated code snippet before the following stop sequences: ``\textbackslash nclass", ``\textbackslash ndef", ``\textbackslash n\#", ``\textbackslash nif", ``\textbackslash nprint". At the same time, we also obtain the log-probability of each token from the LLM response. For "verbalization" setting, the verbalized confidence is also extracted from the text generated by LLM along with the code snippet. 

\paragraph{Self-consistency on Code.}
We follow \citet{chen2022codet} to sample 100 test cases for each programming problem from the same model. Then, we achieved self-consistency in code at the semantic equivalence level, which is based on the execution behavior of any two codes on this set of test cases. More formally, we implemented the consistency function $\mathbb{I}_C(\cdot,\cdot)$ as an indicator function that indicates whether two codes are semantically equivalent, i.e., $\mathbb{I}_C(x,y) = 1$ if and only if code $x$ and $y$ execute the same result on this set of test cases.

\paragraph{Prompt for Generating Code.}
The prompt for generating code consists of a header, a functional signature, and a docstring and LLM needs to implement the body of this function. An Illustration is shown in Prompt~\ref{pt:generate-code}.

\begin{figure}[ht]
\begin{promptbox}[label=pt:generate-code]{Prompt for Generating Code}
\begin{lstlisting}
from typing import List
def has_close_elements(numbers: List[float], 
        threshold: float) -> bool:
""" Check if in given list of numbers, 
are any two numbers closer to
each other than given threshold.
"""\end{lstlisting}
\end{promptbox}
\end{figure}

\paragraph{Prompt for Generating Test Cases.}
For generating test cases, we implemented the function body with a ``pass'' statement on the basis of the prompt to generate the code, and added a comment to require the LLM to generate test cases for the programming problem. An illustration is shown in Prompt~\ref{pt:generate-cases}.

\begin{figure}[ht]
\begin{promptbox}[label=pt:generate-cases]{Prompt for Generating Test Cases.}
\begin{lstlisting}
from typing import List
def has_close_elements(numbers: List[float], 
        threshold: float) -> bool:
""" Check if in given list of numbers, are any two numbers closer to
each other than given threshold.
"""
pass
# check the correctness of has_close_elements
assert
\end{lstlisting}
\end{promptbox}
\end{figure}

\paragraph{Prompt for Code Verbalized Method.}
For generating code with verbalized confidence, we added instructions for generating verbalized confidence, as well as format requirements to facilitate the extraction of code and confidence score. We also gave a simple example to help LLM understand the format requirements at the end of the prompt. 
An Illustration is shown in Prompt~\ref{pt:code-verbalized}.

\begin{figure}[ht]
\begin{promptbox}[label=pt:code-verbalized]{Prompt for Code Verbalized Method}
Come up with a solution that solves the following programming question and
provide your confidence score in this solution like 0\%, 10\%, ... 100\%.
\begin{lstlisting}
import heapq as hq
def heap_queue_largest(nums,n):
'''
Write a function to find the n largest integers from a given list 
of numbers, returned in descending order.
'''
```
\end{lstlisting}
Format requirement: output in the form of the following example. Do not
provide any additional explanations.
Here is an output example:
Solution:
\begin{lstlisting}
```python
your code ...
```
\end{lstlisting}
Confidence: 
\end{promptbox}
\end{figure}