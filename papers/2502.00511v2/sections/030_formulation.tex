\section{Problem and Analysis}
\label{sec:problem}

In this section, we start by outlining the problem formulation of LLM reasoning through sampling multiple reasoning paths.
Then, we provide a theoretical analysis that decomposes LLM reasoning performance into estimation error and model error. 
Finally, we present experimental results verifying our theoretical analysis. 
Our theoretical and empirical analysis motivates our follow-up method design. 

\subsection{Problem Formulation}

Given a reasoning problem $(x, y)$, where $x$ represents the input query, and $y$ represents the ground-truth answer. 
The LLM generates a reasoning path $\hat{t} = (t_1, \ldots, t_m)$ by sequentially sampling tokens according to the conditional probability distribution $p(t_i \,|\, x, t_{<i})$, where $m$ denotes the length of the reasoning path. 
The probability of generating the reasoning path $\hat{t}$ is defined as $p(\hat{t} \,|\, x)$, a.k.a the confidence of the reasoning path $\hat{t}$.
An answer extraction function $g(\cdot)$ maps the reasoning path to the final answer $\hat{y} = g(\hat{t})$, and the reasoning correctness is evaluated by the indicator function $\I[\hat{y} = y]$. 
We can extend the probability to the answer $\hat{y}$, i.e., the answer confidence, denoted as $p(\hat{y} \,|\, x)$.

The confidence essentially represents the probability that the reasoning path $\hat{t}$ or answer $\hat{y}$ is correct, which enables LLMs to select the most reliable solution among multiple candidates.
Nevertheless, enumerating all reasoning paths or answers is unfeasible; 
we have to estimate the LLM confidence based on finite $n$ sampled reasoning paths instead. 
Furthermore, 
to measure the reasoning performance of LLMs, we use the squared error of confidence estimation $\hat{p}(\hat{t}\,|\,x)$ to the reasoning path $\hat{t}$:
\begin{equation*}
\mathcal{E}(p) = \big(\hat{p}(\hat{t} \,|\, x) - \I[g(\hat{t}) = y] \big)^2.
\end{equation*}
If we can extend the confidence estimation to the answer $\hat{y}$, the squared error can be reformulated as
\begin{equation*}
\mathcal{E}(p) = \big(\hat{p}(\hat{y} \,|\, x) - \I[\hat{y} = y] \big)^2.
\end{equation*}
Below, we analyze two confidence estimation methods, i.e., self-consistency method~\citep{wang2022self} and perplexity method~\cite{huang2023look}. Specifically, the self-consistency method computes the answer confidence using Monte-Carlo estimation based on a consistency function $\mathbb{I}_C$, while the perplexity method directly computes the confidence of reasoning paths using internal LLM probabilities. 

\subsection{Theoretical Analysis}

To maximize the reasoning performance of LLMs, 
self-consistency methods (denoted as \SC)~\citep{xiong2023can, yadkori2024believe, becker2024cycles} often sample $n$ reasoning paths $\tilde{t}_1, \dots, \tilde{t}_n$, and then estimate the probability of each answer by 
\begin{equation*}
    \begin{aligned}
\hat{p}^{(\SC)}(\hat{y} \,|\, x) 
&= \frac{1}{n} \sum_{i=1}^n \I[\tilde{y}_i = \hat{y}], \quad \tilde{y}_i = g(\tilde{t}_i).
    \end{aligned}
\end{equation*}
Then, the reasoning error of the \SC method for a given problem $(x,y)$ can be computed by
\begin{equation*}
\begin{aligned}
\mathcal{E}(\hat{p}^{(\SC)}) &= \E_{\tilde{t}_i \sim p(t \,|\, x)}( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] )^2 \\
&=\E_{\tilde{t}_i \sim p(t \,|\, x)}( \frac{1}{n} \sum_{i=1}^n \I[\tilde{y}_i = \hat{y}] - \mathbb{I}[\hat{y} = y] )^2.
\end{aligned}
\end{equation*}

To illustrate the key factors affecting the reasoning error, 
we provide an error decomposition in the following proposition. 

\begin{proposition}[\SC Reasoning Error Decomposition]
\label{prop:sc-reasoning-error-decomposition}
For any input $x$ with ground-truth answer $y$, let $\hat{p}^{(\SC)}(\hat{y} \,|\, x)$ denote the estimated probability of $\hat{y}$ by \SC.
Then, the reasoning error $\mathcal{E}(\hat{p}^{(\SC)})$ can be divided into two components: 
\begin{equation*}
    \begin{aligned}
        \mathcal{E}(\hat{p}^{(\SC)}) 
        = & \underbrace{\frac{1}{n} p(\hat{y} \,|\, x) (1- p(\hat{y}\,|\, x))}_{\text{Estimation Error}} \\
        & \qquad +  \underbrace{\big (p(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] \big )^2}_{\text{Model Error}}. 
    \end{aligned}
\end{equation*}
\end{proposition}

\begin{remark}
The detailed proof is provided in Appendix~\ref{app:props}.
The estimation error refers to the error caused by the finite sampling from the LLM probability, while the model error indicates the LLM's limited reasoning capability.
Note that the estimation error of \SC reduces to only the variance as the sampling is unbiased.
This proposition demonstrates that, aside from the \emph{model error}, which is determined by the LLM's inherent reasoning capabilities,
the reasoning error is bounded by the \emph{estimation error}.
Moreover, the estimation error reduction rate of the sample size is linear, resulting in a large error margin when the sampling is insufficient.
\end{remark}

To effectively offset the estimation error, we switch to analyze the reasoning error of perplexity methods (denoted as \PP).
In contrast to the \SC method that estimates the answer probability using the Monte-Carlo estimation, \PP directly utilizes the internal probability of LLMs $p(\tilde{t}_i \,|\, x)$ for the sampled reasoning path $\tilde{t}_i$. 
Therefore, for given the unique set of $n$ sampled reasoning paths $\mathcal{R} = \left \{ \tilde{t}_1, \ldots, \tilde{t}_n \right \}$, the estimated probability of each reasoning path $\hat{t}$ is
\begin{equation*}
\begin{aligned}
\hat{p}^{(\PP)}(\hat{t} \mid x) &= \left\{
\begin{array}{ll}
p(\tilde{t}_i \,|\, x), & \text{if} \quad \hat{t} = \tilde{t}_i \\
0, & \text{otherwise}
\end{array}
\right. \\
&= \sum_{\tilde{t} \in \mathcal{R}} \I\left [\hat{t} = \tilde{t} \, \right ] p(\tilde{t} \,|\, x).
\end{aligned}
\end{equation*}
Similarly, we also use the mean squared error to measure the reasoning performance of \PP:
\begin{equation*}
\begin{aligned}
& \mathcal{E}(\hat{p}^{(\PP)}) = \E_{\tilde{t}_i \sim p(t \,|\, x)}( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - \mathbb{I}[\hat{y} = y] )^2 \\
& \qquad =\E_{\tilde{t}_i \sim p(t \,|\, x)}( \sum_{\tilde{t}\in \mathcal{R}} \I\left [\hat{t} = \tilde{t} \, \right ] p(\tilde{t} \,|\, x) - \mathbb{I}[g(\hat{t}) = y] )^2.
\end{aligned}
\end{equation*}
Now, we can obtain the following proposition.
\begin{proposition}[\PP Reasoning Error Decomposition]
\label{prop:ppl-reasoning-error-decomposition}
For any given input $x$ with ground-truth answer $y$, let $\hat{p}^{(\PP)}(\hat{t} \,|\, x)$ denote the estimated probability of $\hat{t}$ by \PP method.
Then, the reasoning error $\mathcal{E}(\hat{p}^{(\PP)})$ can be divided into two components: 
\begin{equation*}
    \begin{aligned}
        \mathcal{E}(\hat{p}^{(\PP)}) = 
        & \underbrace{(1 - p(\hat{t} \,|\, x)) ^ n {p}(\hat{t} \,|\, x) ( 2 \I[\hat{y}_i = y] - p(\hat{t} \,|\, x) ) }_{\text{Estimation Error}} \\
        & \qquad + \underbrace{\big ( p(\hat{t} \,|\, x) - \mathbb{I}[g(\hat{t}) = y] \big )^2}_{\text{Model Error}}. \\
    \end{aligned}
\end{equation*}
\end{proposition}

\begin{remark}
The detailed proof is provided in Appendix~\ref{app:props}.
Compared with \SC, the estimation error of \PP decreases exponentially, which is much faster. However, the model error of \PP is usually larger than that of \SC in practice. In Appendix~\ref{subsec:model-error-comparison-ideal}, we provide Proposition~\ref{prop:ideal-model-error-comparison} to demonstrate that \SC achieves a smaller model error than \PP in the ideal case, due to the advantages of the consistency function.
\end{remark}

\subsection{Empirical Observations}

\begin{figure}[t]
    \begin{center}
    \includegraphics[width=\columnwidth]{figures/Motivation/internlm-7b-GSM8k-T1.0.pdf}
    \caption{The performance of InternLM-MATH-Plus 7B model on GSM8K dataset. (1) Estimation error of \PP converges faster than \SC and Na\"{i}ve-\SC; (2) Proper consistency function is the key for \SC to achieve the small model error.}
    \label{fig:motivation-estimation-error}
    \end{center}
    \vskip -0.2in
\end{figure}

To confirm our theoretical results, we conduct some initial experiments on the GSM8K dataset using the InternLM-MATH-Plus 7B model. We limit the sample size $n$ from $1$ to $6$ and plot the accuracy curves and the estimation error in Figure~\ref{fig:motivation-estimation-error}.
Additionally, we include an ablative version called by Na\"{i}ve-\SC. 
Na\"{i}ve-\SC applies the Monte-Carlo estimation, which is consistent with \SC, 
but its consistency function is degraded to a Na\"{i}ve version to the reasoning path matching rather than the answer matching, which is consistent with \PP.
In other words, the reasoning error $\mathcal{E}(\hat{p}^{(\text{Na\"{i}ve-}\SC)})$ of Na\"{i}ve-\SC can be decomposed as
\begin{equation*}
    \begin{aligned}
        \underbrace{\frac{1}{n} p(\hat{t} \,|\, x) (1- p(\hat{t}\,|\, x))}_{\text{Estimation Error}}  +  \underbrace{\big (p(\hat{t} \,|\, x) - \mathbb{I}[g(\hat{t}) = y] \big )^2}_{\text{Model Error}}. 
    \end{aligned}
\end{equation*}

The derived results highlight the following two observations:

\textbf{(I) Estimation Error.}
The estimation errors of both \SC and \PP decrease as the sample size increases. 
However, the accuracy curves and estimation error illustrate that the \PP has a much faster convergence rate compared to \SC. 
Specifically, \PP reaches a stable result with $n=3$, while \SC cannot converge even for $n=6$.
Na\"{i}ve-\SC confirms this result, showing a lower convergent rate, since it uses the same Monte-Carlo estimation with \SC.

\textbf{(II) Model Error.}
\SC and \PP ultimately converge to different results. This is because their model errors are intrinsically different. \SC groups reasoning paths that yield the same answer through its consistency function, ensuring a higher accuracy of \SC. In contrast, \PP only estimates the probability of individual reasoning paths without considering answer-level consistency. Na\"{i}ve-\SC also supports this conclusion, converging to the worst results due to its lack of a proper consistency function.

{\bf Key Insights.} 
Our theoretical and empirical analyses point to the deficiencies and potential synergies of \SC and \PP. 
Although \SC achieves a lower model error due to the advantages of the consistency function, its estimation error is hindered by a slower convergence rate. In contrast, \PP exhibits a much faster convergence rate in estimation error using LLM internal probabilities, but this comes at the cost of a higher model error. This naturally raises a fundamental research question: \emph{Can we design a method fusing strengths of \SC and \PP: achieve both a rapid estimation error convergence rate and a low model error simultaneously?}