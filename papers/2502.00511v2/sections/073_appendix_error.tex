\subsection{Proof of Proposition~\ref{prop:sc-reasoning-error-decomposition} and Proposition~\ref{prop:ppl-reasoning-error-decomposition}} \label{app:props}
\begin{proof}
  (\SC) First, we denote the sampling probability distribution of the LLM as $p(\hat{y} \,|\, x)$, 
  and the confidence function as $\hat{p}^{(\SC)}(\hat{y} \,|\, x) = \frac{1}{n}  \sum_{i=1}^n \mathbb{I}[\tilde{y}_i = \hat{y}_i] $, where $\tilde{y}_1, \dots, \tilde{y}_n$ are sampled on the distribution $p(\hat{y} \,|\, x)$. 
  Apply the error decomposition, we have
  \begin{equation*}
  \begin{aligned}
    \mathcal{E}(\hat{p}^{(\SC)}) & =  \E_{\tilde{y}_i \sim {p}(\tilde{y}_i \,|\, x)} \left[( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \I[\hat{y} = y])^2 \right]  \\
    & =  \E_{\tilde{y}_i \sim{p}(\tilde{y}_i \,|\, x)} \big[(\frac{1}{n}  \sum_{i=1}^n \mathbb{I}[\tilde{y}_i = \hat{y}_i] - \I[\hat{y} = y])^2 \big]  \\
    & = \E_{\tilde{y}_i \sim {p}(\tilde{y}_i \,|\, x)} \big[ ( \frac{1}{n}  \sum_{i=1}^n \mathbb{I}[\tilde{y}_i = \hat{y}_i] - p(\hat{y} \,|\, x) + p(\hat{y} \,|\, x) - \I[\hat{y}_i = y]  )^2 \big] \\
    & = \frac{1}{n} {p}(\tilde{y}_i \,|\, x) (1-{p}(\tilde{y}_i \,|\, x)) + \left( p(\hat{y} \,|\, x) - \I[\hat{y}_i = y] \right )^2.
  \end{aligned}
  \end{equation*}
   (\PP) Another way is to use the confidence function to build the sampling probability distribution, 
    i.e., 
    \begin{equation*}
    \hat{p}^{(\PP)}(\hat{t} \,|\, x) = \left\{
        \begin{array}{ll}
            p(\hat{t} \,|\, x), & \text{if}~ \text{there is}~\tilde{t}_i = \hat{t} \\
            0, & \text{otherwise}
        \end{array}
    \right. 
    = \sum_{i=1}^n \I(\tilde{t}_i = \hat{t})p(\hat{t} \,|\, x).
    \end{equation*}
    Now, we have
    \begin{equation*}
    \begin{aligned}
        & \E_{\tilde{y}_i \sim {p}(\tilde{t}_i \,|\, x)} [ \hat{p}^{(\PP)}(\hat{t} \,|\, x) - p(\hat{t} \,|\, x)]  = -(1 - p(\hat{t} \,|\, x)) ^ n p(\hat{t} \,|\, x) \\
        & \E_{\tilde{y}_i \sim {p}(\tilde{t}_i \,|\, x)} [ (\hat{p}^{(\PP)}(\hat{t} \,|\, x) - p(\hat{t} \,|\, x))^2] = (1 - p(\hat{t} \,|\, x)) ^ n p(\hat{t} \,|\, x)^2 .
    \end{aligned}
    \end{equation*}
    Hence,
    \begin{equation*}
        \begin{aligned}
          \mathcal{E}(\hat{p}^{(\PP)}) &= \E_{\tilde{t}_i \sim p(t \,|\, x)} \big[( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - \I[\hat{y} = y] )^2 \big] \\
	& =\E_{\tilde{t}_i \sim p(t \,|\, x)}\big[(\hat{p}^{(\PP)}(\hat{t} \,|\, x) - p(\hat{t} \,|\, x) + p(\hat{t} \,|\, x) - \I[g(\hat{t}) = y] )^2 \big] \\
	& = - (1 - p(\hat{t} \,|\, x)) ^ n p(\hat{t} \,|\, x)^2 + 2 (1 - p(\hat{t} \,|\, x)) ^ n p(\hat{t} \,|\, x) \I[g(\hat{t}) = y] + (p(\hat{t} \,|\, x) - \I[g(\hat{t}) = y] )^2 \\
	& = (1 - p(\hat{t} \,|\, x)) ^ n p(\hat{t} \,|\, x) (2\I[g(\hat{t}) = y] - p(\hat{t} \,|\, x)) + (p(\hat{t} \,|\, x) - \I[g(\hat{t}) = y] )^2.
        \end{aligned}
        \end{equation*}
Hence, we complete the proof.
\end{proof}

% \label{proof:reasoning-error-decomposition}
% \begin{proof}
% Under Definition~\ref{def:llm-reasoning-error} and the assumptions in Proposition~\ref{prop:reasoning-error-decomposition}, we have
% \begin{equation}
%     \begin{aligned}
%         \mathcal{E}\left ( \btheta \right ) 
%         & = \mathbb{E} \left [ \sum_{i=1}^K \left ( \hat{\psi}_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right )^2 \right ] \\
%         & = \mathbb{E} \left [ \sum_{i=1}^K \left ( \hat{\psi}_{\btheta}(\hat{y}_i \,|\, x) - \psi_{\btheta}(\hat{y}_i \,|\, x) + \psi_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right )^2 \right ] \\
%         & = \mathbb{E} \left [ \sum_{i=1}^K \left ( \hat{\psi}_{\btheta}(\hat{y}_i \,|\, x) - \psi_{\btheta}(\hat{y}_i \,|\, x) \right )^2 \right ]
%          +\mathbb{E} \left [ \sum_{i=1}^K \left ( \psi_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right )^2 \right ] \\
%         & \qquad +2\mathbb{E} \left [ \sum_{i=1}^K \left ( \hat{\psi}_{\btheta}(\hat{y}_i \,|\, x) - \psi_{\btheta}(\hat{y}_i \,|\, x) \right ) \left ( \psi_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right ) \right ] \\
%     \end{aligned}
% \end{equation}
% By the unbiasedness of the sampling strategy, we have $\mathbb{E}[\hat{\psi}_{\btheta}(\hat{y}_i \,|\, x)] = \psi_{\btheta}(\hat{y}_i \,|\, x)$ because the mean of LLM sampled probability distribution equals the ground-truth probability distribution. 
% This implies that the cross term equals zero, as
% \begin{equation}
%     \begin{aligned}
%         &2\mathbb{E} \left [ \sum_{i=1}^K \left ( \hat{\psi}_{\btheta}(\hat{y}_i \,|\, x) - \psi_{\btheta}(\hat{y}_i \,|\, x) \right ) \left ( \psi_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right ) \right ] \\
%         &= 2\sum_{i=1}^K \left ( \mathbb{E}[\hat{\psi}_{\btheta}(\hat{y}_i \,|\, x)] - \psi_{\btheta}(\hat{y}_i \,|\, x) \right ) \left ( \psi_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right ) \\
%         &= 0
%     \end{aligned}
% \end{equation}
% Therefore, we complete the proof as follows.
% \begin{equation}
%     \begin{aligned}
%         \mathcal{E}\left ( \btheta \right ) 
%         = \mathbb{E} \left [ \sum_{i=1}^K \left ( \hat{\psi}_{\btheta}(\hat{y}_i \,|\, x) - \psi_{\btheta}(\hat{y}_i \,|\, x) \right )^2 \right ] 
%         + \mathbb{E} \left [ \sum_{i=1}^K \left ( \psi_{\btheta}(\hat{y}_i \,|\, x) - \mathbb{I}[\hat{y}_i = y] \right )^2 \right ]
%     \end{aligned}
% \end{equation}
% \end{proof}

% \subsection{Proof of Theorem~\ref{thm:error-bound}}

% \begin{proof}
%     Based on the results and assumptions in Proposition~\ref{prop:reasoning-error-decomposition}, we have the decomposition of the reasoning error:
%     \begin{equation}
%         \begin{aligned}
%             \mathcal{E}\left ( p_{\btheta} \right ) 
%             =& \mathbb{E} \left [ \left \| f_{\btheta}(x) - \psi_{\btheta}(x) \right \|^2 \right ] + \mathbb{E} \left [ \left \| \psi_{\btheta}(x) - \y  \right \|^2 \right ]
%         \end{aligned}
%     \end{equation}
%     By the unbiasedness of the sampling, we have $\mathbb{E}[f_{\btheta}(x)] = \psi_{\btheta}(x)$. Therefore:
%     \begin{equation}
%         \mathcal{E}\left ( p_{\btheta} \right ) =
%         \underbrace{\mathbb{E} \left [ \left \| f_{\btheta}(x) - \mathbb{E}[f_{\btheta}(x)] \right \|^2 \right ]}_{\text{Estimation Error (Variance)}}
%         + \underbrace{\mathbb{E} \left [ \left \| \psi_{\btheta}(x) - \y  \right \|^2 \right ]}_{\text{Model Error}}
%     \end{equation}
%     Let $C = \mathbb{E} \left [ \left \| \psi_{\btheta}(x) - \y  \right \|^2 \right ]$ denote the Model Error, which is constant for a given LLM model and reasoning problem. 
    
% \[
% \mathcal{E}\left ( p_{\boldsymbol{\theta}} \right ) \approx \left \| f_{\boldsymbol{\theta}}(\mathbf{x}_i) - \mathbb{E}[f_{\boldsymbol{\theta}}(\mathbf{x})] \right \|^2 + \left \| \psi_{\boldsymbol{\theta}}(\mathbf{x}_i) - \mathbf{y}_i \right \|^2
% \]

%     Consider $N$ independent samples $\{f_{\boldsymbol{\theta}}^{(i)}(x)\}_{i=1}^N$ from the LLM. The expected error can be expressed as:
%     \begin{equation}
%         \begin{aligned}
%             \mathcal{E}(p_{\btheta}) = \lim_{N \to \infty} \frac{1}{N}\sum_{i=1}^N \left\|f_{\boldsymbol{\theta}}^{(i)}(x) - \mathbb{E}[f_{\boldsymbol{\theta}}(x)]\right\|^2 + C
%         \end{aligned}
%     \end{equation}
    
%     Applying Chebyshev's inequality to the sample mean, we have:
%     \begin{equation}
%         \begin{aligned}
%             \Pr\left(\left|\frac{1}{N}\sum_{i=1}^N \left\|f_{\boldsymbol{\theta}}^{(i)}(x) - \mathbb{E}[f_{\boldsymbol{\theta}}(x)]\right\|^2 - \sigma^2\right| \leq \frac{\sigma^2}{\sqrt{N\delta}}\right) \geq 1 - \delta
%         \end{aligned}
%     \end{equation}
    
%     Thus, with probability at least $1-\delta$, the error is bounded by:
%     \begin{equation}
%         \begin{aligned}
%             \mathcal{E}(p_{\btheta}) \leq C + \sigma^2 + \frac{\sigma^2}{\sqrt{N\delta}}
%         \end{aligned}
%     \end{equation}
% \end{proof}



\subsection{Model Error Comparison in Ideal Case}
\label{subsec:model-error-comparison-ideal}

\begin{proposition}[Comparison of Model Errors]
    Consider a setting with infinite sampling of reasoning paths ($n \to \infty$) where each incorrect reasoning path leads to a unique answer, that is, $g(\tilde{t}_i) \neq g(\tilde{t}_j)$ for any $i \neq j$ where $g(\tilde{t}_i) \neq y$ and $g(\tilde{t}_j) \neq y$. The model error of self-consistency ($\mathcal{E}_{\text{Model}}^{(\SC)}$) and perplexity ($\mathcal{E}_{\text{Model}}^{(\PP)}$) satisfy:
    \begin{equation}
        \mathcal{E}_{\text{Model}}^{(\SC)} \leq \mathcal{E}_{\text{Model}}^{(\PP)}
    \end{equation}
    The inequality is strict when the consistency function identifies equivalent correct reasoning paths more than once.
    \label{prop:ideal-model-error-comparison}
\end{proposition}

\begin{remark}
    Although the assumptions in Proposition~\ref{prop:ideal-model-error-comparison} are idealized, this special case demonstrates that the consistency function in self-consistency achieves better model error than the perplexity method. 
    In practice, the perplexity method always gives larger model error compared to the self-consistency method, as it does not leverage the consistency function to analyze the structural properties of specific reasoning problems.
    The proof is presented as follows.
\end{remark}

\begin{proof}
    We first recall the definitions of the model error for self-consistency and perplexity:
    \begin{equation}
        \begin{cases}
            \mathcal{E}_{\text{Model}}^{(\SC)} &= \sum_{\hat{y} \in \{g(\tilde{t}_i) \, | \, i = 1 \ldots n \}} \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2, \\
            \mathcal{E}_{\text{Model}}^{(\PP)} &= \sum_{\hat{t}\in\mathcal{R}} \left ( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - \mathbb{I}[g(\hat{t}) = y] \right )^2.             
        \end{cases}
    \end{equation}
    where $\mathcal{R} = \text{Set}(\tilde{t}_1, \ldots, \tilde{t}_n)$ is the set of reasoning paths sampled from the LLM.
    We can compute the difference between the model error of \SC and \PP as follows:
    \begin{equation}
        \begin{aligned}
            \mathcal{E}_{\text{Model}}^{(\SC)} - \mathcal{E}_{\text{Model}}^{(\PP)}
            &=  \sum_{\hat{y} \in \{g(\tilde{t}_i) \, | \, i = 1 \ldots n \}} \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2 
             %- \sum_{i=1}^n \left ( \hat{p}^{(\PP)}(\tilde{t}_i \,|\, x) - \mathbb{I}[g(\tilde{t}_i) = y] \right )^2 \\
             - \sum_{\hat{t}\in\mathcal{R}} \left ( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - \mathbb{I}[g(\hat{t}) = y] \right )^2 \\            
            &= \sum_{\hat{y} \in \{g(\hat{t}_i) \, | \, i = 1 \ldots n \}} 
            \underbrace{\left [ \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2 - \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \left ( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2 \right ]}_{(A)} \\
        \end{aligned}
    \end{equation}
    Assuming infinite samplings, the unbiasedness of \SC gives us: 
    \begin{equation}
        \begin{aligned}
            \hat{p}^{(\SC)}(\hat{y} \,|\, x) 
            = \frac{1}{n} \sum_{i=1}^n \mathbb{I}[g(\tilde{t}_i) = \hat{y}] 
             = \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \cdot \hat{p}^{(\PP)}(\hat{t} \,|\, x).
        \end{aligned}
    \end{equation}
    For any $\hat{y}$, consider two cases: 
    \begin{enumerate}
        \item[(i)] If $\hat{y} = y$, then $\hat{y}$ is the correct answer. We have 
        \begin{equation}
            \begin{aligned}
                (A) &= \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - 1 \right )^2 
                % - \sum_{i=1}^n \mathbb{I}[g(\tilde{t}_i) = \hat{y}] \left ( \hat{p}^{(\PP)}(\tilde{t}_i \,|\, x) - 1 \right )^2 
                - \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \left ( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - 1 \right )^2 \\
                &= \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x)^2 + 1 - 2 \hat{p}^{(\SC)}(\hat{y} \,|\, x) \right ) 
                % - \sum_{i=1}^n \mathbb{I}[g(\tilde{t}_i) = \hat{y}] \left ( \hat{p}^{(\PP)}(\tilde{t}_i \,|\, x)^2 + 1 - 2 \hat{p}^{(\PP)}(\tilde{t}_i \,|\, x) \right ) \\
                - \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \left ( \hat{p}^{(\PP)}(\hat{t} \,|\, x)^2 + 1 - 2 \hat{p}^{(\PP)}(\hat{t} \,|\, x) \right ) \\
                &= \hat{p}^{(\SC)}(\hat{y} \,|\, x)^2 + 1  
                % - \sum_{i=1}^n \mathbb{I}[g(\tilde{t}) = \hat{y}] \cdot \hat{p}^{(\PP)}(\tilde{t}_i \,|\, x)^2 
                -\sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \cdot \hat{p}^{(\PP)}(\hat{t} \,|\, x)^2
                % - \sum_{i=1}^n \mathbb{I}[g(\tilde{t}) = \hat{y}] \\
                - \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \\
                &\leq \hat{p}^{(\SC)}(\hat{y} \,|\, x)^2 + 1  - \frac{\hat{p}^{(\SC)}(\hat{y} \,|\, x)^2}{\sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}]} 
                % - \sum_{i=1}^n \mathbb{I}[g(\tilde{t}) = \hat{y}] \\
                - \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \\
            \end{aligned}
        \end{equation}
        Let $\hat{p}^{(\SC)}(\hat{y} \,|\, x)^2 = B^2$ and $\sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}]=C$, then
        \begin{equation}
            \begin{aligned}
                (A) &\leq B^2 + 1 - \frac{B^2}{C} - C \\
                &=\frac{1}{C}\left [  B^2C + C - B^2 - C^2 \right ] \\
                &=\frac{1}{C}\left [  (C-B^2)(1-C) \right ] \\
                &\leq 0.
            \end{aligned}
        \end{equation}
        Equality holds if $\sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}]= C = 1$. This indicates that $(A)<0$ if the consistency function is effective at least once, making $\sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}]= C >1$.
        \item[(ii)] If $\hat{y} \neq y$, then $\hat{y}$ is incorrect. Assuming distinct answers for wrong reasoning paths, we have $\sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] = 1$, thus
        \begin{eqnarray}
            \begin{aligned}
                (A) &= \left [ \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2 
                - \sum_{\hat{t} \in \mathcal{R}} \mathbb{I}[g(\hat{t}) = \hat{y}] \left ( \hat{p}^{(\PP)}(\hat{t} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2 \right ] \\
                &= \left [ \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) 
                - \mathbb{I}[\hat{y} = y] \right )^2 - \left ( \hat{p}^{(\SC)}(\hat{y} \,|\, x) - \mathbb{I}[\hat{y} = y] \right )^2 \right ] = 0,
            \end{aligned}
        \end{eqnarray}
        since only one $\hat{t}\in \mathcal{R}$ satisfying that $g(\hat{t})$ equals the incorrect answer $\hat{y}$.
    \end{enumerate}
    Therefore, $\mathcal{E}_{\text{Model}}^{(\PP)} - \mathcal{E}_{\text{Model}}^{(\SC)} \leq 0$, indicating that the model error of self-consistency is always less than or equal to the model error of perplexity under our assumptions. Moreover, if the consistency function is effective at least once, the model error of self-consistency is strictly less than the model error of perplexity.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:thm1}} \label{app:thm1}
\begin{proof}
For given answer $\hat{y}$, the estimated probability of \PC is $\hat{p}(\hat{y} \,|\, x) = \sum_{i=1}^n \I[g(\tilde{t}_i) = \hat{y}] p(\tilde{t}_i \,|\, x)$, allowing the reasoning error of \PC can be computed as follows.
    \begin{equation*}
        \begin{aligned}
          \mathcal{E}^{(\PC)}(\hat{p}) &= \E_{\tilde{t}_i \sim p(t \,|\, x)} \big[( \hat{p}(\hat{y} \,|\, x) - \I[\hat{y} = y] )^2 \big] \\
	& =\E_{\tilde{t}_i \sim p(t \,|\, x)}\big[(\hat{p}(\hat{y} \,|\, x) - p(\hat{y} \,|\, x) + p(\hat{y} \,|\, x) - \I[\hat{y} = y] )^2 \big].
        \end{aligned}
        \end{equation*}
We define $k := |\{\tilde{t} \mid g(\tilde{t}) = \hat{y}\}|$, which means that how many reasoning paths whose answers are $\hat{y}$ can be covered given limited sampling budget. 
Note that we further have $ \E_{\tilde{t} \sim p(t \,|\, x)} [\I[g(\tilde{t}) = \hat{y}] p(\tilde{t} \,|\, x)] = \frac{1}{k} {p}(\hat{y} \,|\, x)$, thus
\begin{equation*}
\begin{aligned}
\E_{\tilde{t}_i \sim p(t \,|\, x)} [\hat{p}(\hat{y} \,|\, x)] &= \E \big[\sum_{i=1}^n \I[g(\tilde{t}_i) = \hat{y}] p(\tilde{t}_i \,|\, x) \big] = \Big(1 - \big(1 - \frac{1}{k} p(\hat{y} \,|\, x)\big)^n \Big) p(\hat{y} \,|\, x) = (1-\alpha^n)p(\hat{y} \,|\, x),
\end{aligned}
\end{equation*}
where $\alpha := \frac{1}{k} p(\hat{y} \,|\, x)$.
Again, we have
\begin{equation*}
\begin{aligned}
& \E_{\tilde{t}_i \sim p(t \,|\, x)}[\hat{p}(\hat{y} \,|\, x) - p(\hat{y} \,|\, x)] = - \big(1 - \alpha\big)^n p(\hat{y} \,|\, x))  \\
& \E_{\tilde{t}_i \sim p(t \,|\, x)}[(\hat{p}(\hat{y} \,|\, x) - p(\hat{y} \,|\, x))^2] = \Big(1 - \big(1 - \alpha\big)^n \Big) \big(1 - \alpha\big)^n p(\hat{y} \,|\, x))^2   
\end{aligned}
\end{equation*}

Hence,
\begin{equation*}
    \begin{aligned}
      \mathcal{E}^{(\PC)}(\hat{p}) &= \E_{\tilde{t}_i \sim p(t \,|\, x)} \big[( \hat{p}(\hat{y} \,|\, x) - \I[\hat{y} = y] )^2 \big] \\
	& =\E_{\tilde{t}_i \sim p(t \,|\, x)}\big[(\hat{p}(\hat{y} \,|\, x) - p(\hat{y} \,|\, x) + p(\hat{y} \,|\, x) - \I[\hat{y} = y] )^2 \big] \\
	&= (1 - \alpha)^{n} p(\hat{y} \,|\, x) \big(2\I[\hat{y}=y] - (1 + (1 - \alpha)^{n}) p(\hat{y} \,|\, x) \big) +  \left( p(\hat{y} \,|\, x) - \I[\hat{y}_i = y] \right )^2,
    \end{aligned}
    \end{equation*}
which completes the proof.
\end{proof}

\subsection{Proof of Theroem~\ref{thm:thm2}} \label{app:thm2}
\begin{proof}
Let us set the pruning threshold by $\tau := p(y \,|\, x)$. Then, we have
\begin{equation*}
    \begin{aligned}
      \mathcal{E}^{(\RPC)}(\hat{p}) 
      = & \underbrace{ \alpha p(\hat{y} \,|\, x) \big(2\I[\hat{y}=y] - (1 + \alpha) p(\hat{y} \,|\, x) \big) \I[(p(\hat{y}) \,|\, x) < \tau]}_{\text{Estimation Error}} \\
        & \qquad + \underbrace{\left( p(\hat{y} \,|\, x) - \I[\hat{y}_i = y] \right )^2  \I[(p(\hat{y}) \,|\, x) < \tau]}_{\text{Model Error}} \\
    \end{aligned}
\end{equation*}
However, we only have an estimation of $p(\hat{y} \,|\, x)$, i.e., $\hat{p}(\hat{y} \,|\, x) = k \E_{\tilde{t} \sim p(t \,|\, x)} [\I[g(\tilde{t}) = \hat{y}] p(\tilde{t} \,|\, x)] \approx \frac{k}{\hat{k}} \sum_{i=1}^k p(\tilde{t}_i \,|\, x)$, where $\tilde{t}_1, \dots, \tilde{t}_{\hat{k}}$ are $\hat{k}$ sampled reasoning paths whose answer is $\hat{y}$.
Therefore, the reasoning error of our approximate version can be computed by
\begin{equation*}
    \begin{aligned}
      \hat{\mathcal{E}}^{(\RPC)}(\hat{p}) 
      = & \underbrace{ \alpha(p) p(\hat{y} \,|\, x) \big(2\I[\hat{y}=y] - (1 + \alpha(p)) p(\hat{y} \,|\, x) \big) \I[\frac{1}{k} \sum_{i=1}^{\hat{k}} p(\tilde{t}_i \,|\, x) < \frac{1}{m}\tau]}_{\text{Estimation Error}} \\
        & \qquad + \underbrace{\left( p(\hat{y} \,|\, x) - \I[\hat{y}_i = y] \right )^2  \I\big[\frac{1}{\hat{k}} \sum_{i=1}^{\hat{k}} p(\tilde{t}_i \,|\, x) < \frac{1}{m}\tau\bigskip
        ]}_{\text{Model Error}} \\
    \end{aligned}
\end{equation*}

Hence, we only need to consider the probability $\frac{1}{\hat{k}} \sum_{i=1}^{\hat{k}} p(\tilde{t}_i \,|\, x) > \frac{1}{m} \tau$.
Using Hoeffding's inequality, we can obtain that
\begin{equation*}
\mathbb{P}\big(\frac{1}{\hat{k}} \sum_{i=1}^{\hat{k}} p(\tilde{t}_i \,|\, x) - \frac{1}{k} p(\hat{y} \,|\, x) \ge \tau \big)  \leq \exp\Big(-\frac{2\hat{k}\gamma^2}{ p(\hat{y} \,|\, x)^2}\Big)
\end{equation*}
We set $\gamma = \tau - \frac{1}{k} p(\hat{y} \,|\, x) = \tau + \alpha - 1$, then 
\begin{equation*}
\mathbb{P}\big(\frac{1}{\hat{k}} \sum_{i=1}^{\hat{k}} p(\tilde{t}_i \,|\, x) \ge \tau \big)  \leq \exp\Big(-{2\hat{k}k^2} (1 - \frac{\tau}{1 - \alpha})^2\Big).
\end{equation*}
Hence, we complete the proof.
\end{proof}


