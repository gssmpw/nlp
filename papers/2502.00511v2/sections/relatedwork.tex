\section{Related Work}

This paper is related to the two research topics, i.e., LLM Reasoning Boosting and LLM Confidence Estimation.

\textbf{LLM Reasoning Boosting.}

Recent research has developed various methods to improve LLM reasoning capabilities. CoT~\citep{kojima22large} proposes the ``Let's think step by step'' prompt to guide LLMs in generating structured solutions. For complex problems, Least-to-Most~\citep{zhou23least} introduces a decomposition strategy that breaks down challenges into manageable sub-problems. Few-shot methods~\citep{wei22cot,fu23complexity,zhang23automatic} leverage carefully selected examples to improve reasoning performance. To enable more comprehensive reasoning, search-based methods~\citep{guan2025rstar} integrate Monte Carlo Tree Search (MCTS). Recent advancements have further enhanced MCTS by incorporating reward models~\citep{zhang2024restmcts, park24ensembling}. To directly optimize reasoning abilities, researchers have explored fine-tuning approaches~\citep{yu24metamath, li24mugglemath, li2024neurosymbolic,ying2024internlmmath} using specialized datasets and reinforcement learning techniques~\citep{shao24deepseekmath,luo25wizardmath}.

While these methods focus on generating accurate reasoning paths, our \RPC can build upon them by utilizing multiple sampling strategies, enabling better reasoning performance.

\textbf{LLM Confidence Estimation.}
The confidence estimation for LLM can be categorized into three types: (1) perplexity confidence, (2) verbalized confidence, and (3) self-consistency confidence. Perplexity confidence~\citep{huang2023look,duan2024shifting} utilizes the geometric mean of LLM prediction probabilities (i.e., perplexity~\citep{chen1998evaluation, blei03LDA}) to evaluate model adherence~\citep{murugadoss2025evaluating} and prompt quality~\citep{yao2024learning}. Verbalized confidence~\citep{kadavath2022language, xiong2023can, tian2023just} directly asks LLMs to express their confidence through various approaches, such as multi-agent deliberation~\citep{yang2024confidence}, multi-step evaluation~\citep{xiong2023can}, top-k ranking~\citep{tian2023just}, few-shot prompting~\citep{liu2023calibrating}, and reflection~\citep{dhuliawala2023chain, zhao2024fact}. Self-consistency confidence~\citep{wang2022self, chen2023universal, cheng2024relic} measures the agreement among multiple generated answers to improve reasoning performance, with recent work~\citep{xiong2023can, yadkori2024believe, becker2024cycles} further developing this approach as a confidence metric. Recent studies recognize its computational issues and propose early stopping~\citep{li24escape} and dynamic sampling methods~\citep{wang24make, wang24dynamic, amad23adaptive} to improve efficiency. 

Our proposed \RPC integrates LLM probability with a self-consistency framework, allowing perplexity and verbalized confidence to be used. \RPC achieves enhanced confidence estimation with fixed reasoning paths, showing a complement to existing self-consistency methods.
