\section{Experiments}
\label{sec:exp}

In this section, we first conduct experiments to answer the following research questions:
\begin{enumerate}[leftmargin=2pt,itemsep=1pt,parsep=0pt,topsep=3pt]
\item[] \textbf{\underline{RQ1}: Efficiency.} How does \RPC reduce the number of samples required to achieve comparable performance through faster convergence?
\item[] \textbf{\underline{RQ2}: Efficacy.} How does \RPC improve reasoning performance compared to existing methods?
\item[] \textbf{\underline{RQ3}: Reliability.} How does \RPC enhance the reliability of confidence estimation compared to existing methods?
\end{enumerate}
Additional discussions are devoted to further demonstrating the effectiveness of \RPC. 
Due to space limitations, supplementary experimental results are included in Appendix~\ref{app:detailed-results}.

\begin{table*}[t]
    \centering
    \caption{Efficiency comparison of \emph{Perplexity Consistency} module (\PC) and \RPC. The table shows the minimum number of samples needed to exceed the best performance of \SC, with reduction rates in bold when sampling is reduced.}
    \label{tab:InternLM2-7B-Reduction}
    % \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|rrrrrrrr}
    \bottomrule
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{\textbf{MATH}} & \multicolumn{2}{c}{\textbf{MathOdyssey}} & \multicolumn{2}{c}{\textbf{OlympiadBench}} & \multicolumn{2}{c}{\textbf{AIME}} \\
    \cmidrule(lr){2-9}  & Accuracy & \#Samplings & Accuracy & \#Samplings & Accuracy & \#Samplings & Accuracy & \#Samplings \\
    \midrule
     Best of \SC & 50.57 & 64   & 28.32 & 112    & 11.07 & 128    & 9.40  & 128       \\
    \midrule
    \PC      & 50.63 & 32      & 28.51 & 112    & 11.07 & 128    & 9.00  & 64        \\
    \rowcolor{gray!20} $\Delta$ & +0.06 & \textbf{-50.0\%} & +0.19 & -0.0\% & 0.00 & -0.0\% & 0.00  & \textbf{-50.0\%}  \\
    \hline
    \RPC     & 51.16 & 32      & 29.31 & 32      & 11.07 & 64      & 9.50  & 48      \\
    \rowcolor{gray!20} $\Delta$ & +0.59 & \textbf{-50.0\%} & +0.99 & \textbf{-71.4\%} & 0.00 & \textbf{-50.0\%} & +0.10 & \textbf{-62.5\%} \\
    \bottomrule
    \toprule
    \end{tabular}}
    \end{sc}
    \end{small}
    \end{center}
    % \vskip -0.1in
\end{table*}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
    \begin{minipage}[t]{\textwidth}
        \centering
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
    \includegraphics[width=\linewidth]{figures/Samplings-wPC/MATH-Accuracy.pdf}
            \vskip -0.3em
            \caption{MATH}
            \label{fig:MATH-Accuracy}
        \end{subfigure}
        \hfill
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figures/Samplings-wPC/Odyssey-Accuracy.pdf}
            \vskip -0.3em
            \caption{MathOdyssey}
            \label{fig:MathOdyssey-Accuracy}
        \end{subfigure}
        \hfill
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figures/Samplings-wPC/OlympiadBench-Accuracy.pdf}
            \vskip -0.3em
            \caption{OlympiadBench}
        \end{subfigure}
        \hfill
        \begin{subfigure}[t]{0.23\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figures/Samplings-wPC/AIME_1983_2024-Accuracy.pdf}
            \vskip -0.3em
            \caption{AIME}
        \end{subfigure}
        \vskip -0.5em
        \caption{The accuracy of the InternLM-2-MATH-Plus 7B model on four math reasoning datasets with different sample sizes $n$. Our proposed \RPC achieves the best performance across all datasets, validating our theoretical analysis. We also show the performance of a single perplexity consistency module (denoted as \PC), which further supports our theoretical findings.}
        \label{fig:InternLM2-7B-Accuracy}
    \end{minipage}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Experimental Setting}

In this section, we briefly introduce the comparison methods, datasets, and details of the implementation. 
The experimental settings can be found in Appendix~\ref{app:exp-details}.

\textbf{Comparison Methods.} 
We compare three types of LLM confidences: perplexity confidence~\citep{wang2022self} (\PP), self-consistency confidence~\citep{chen1998evaluation} (\SC), and verbalized confidence~\citep{tian2023just} (\Verb).
The verbalized confidence is computed based on the probability that the LLM outputs ``True'' versus ``False'' when asked an ``Is-True'' question. For code generation tasks, we extracted verbalized confidence scores from the model's numerical likelihood expressions by prompting the LLM.

\textbf{Datasets.} 
We introduce four popular benchmarks for math reasoning: MATH~\citep{hendrycks2021math}, MathOdyssey~\citep{Fang24Odyssey}, OlympiadBench~\citep{He24OlympiadBench}, and AIME~\citep{AIME}. 
As to code generation tasks, we evaluate each method on three benchmarks, i.e., HumanEval~\citep{Codex2021}, MBPP~\citep{MBPP2021}, and introductory-level problems of APPS~\citep{APPS2021}.
 
\textbf{Implementation Details.} 
For math reasoning tasks, we evaluate the InternLM2-Math-Plus models with 1.8B and 7B parameters~\citep{ying2024internlmmath}, as well as the DeepSeekMath-RL 7B model~\citep{shao24deepseekmath}. The consistency function $\mathbb{I}_C$ is answer comparison. For code generation tasks, we evaluate the Deepseek-Coder 33B model. The consistency function $\mathbb{I}_C$ is constructed based on semantic equivalence~\citep{SemanticEquiv2021} by clustering code based on given test cases. We set the sample size to $n=128$ for the MathOdyssey, OlympiadBench, and AIME datasets and $n=64$ for the MATH dataset by default. Each experiment is repeated 10 times with different random seeds, and the average performance is reported. All experiments were conducted on Linux servers with A800 and H800 GPUs.

\begin{table*}[t]
    \centering
    \caption{Performance Comparison using InternLM-2-MATH-Plus 7B model measured by accuracy and excepted calibration error metrics. The best performance is highlighted in \textbf{bold}. The results show that our \RPC outperforms existing methods in majority of cases.}
    \label{tab:InternLM2-7B-Performance}
    % \vskip -0.5in
    \vspace{-1em}
    \begin{center}
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccccccc|cc}
    \bottomrule
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{2}{c}{\textbf{MATH}} & \multicolumn{2}{c}{\textbf{MathOdyssey}} & \multicolumn{2}{c}{\textbf{OlympiadBench}} & \multicolumn{2}{c}{\textbf{AIME}} & \multicolumn{2}{c}{\textbf{Average}} \\
    \cmidrule(lr){2-11}     & Accuracy($\uparrow$) & ECE($\downarrow$) & Accuracy($\uparrow$) & ECE($\downarrow$) & Accuracy($\uparrow$) & ECE($\downarrow$) & Accuracy($\uparrow$) & ECE($\downarrow$) & Acc.($\uparrow$) & ECE($\downarrow$) \\
    \midrule
    \PP & 46.99 $\pm$ 0.20 & 48.99 $\pm$ 0.19 & 27.35 $\pm$ 1.22 & 67.70 $\pm$ 1.22 & ~~7.27 $\pm$ 0.36 & 86.90 $\pm$ 0.35 & 5.96 $\pm$ 0.48 & 88.98 $\pm$ 0.49 & 21.90  & 73.14 \\
    \Verb & 26.14 $\pm$ 0.25 & 47.46 $\pm$ 0.07 & 10.06 $\pm$ 0.61 & 69.92 $\pm$ 0.88 & ~~3.68 $\pm$ 0.16 & 84.68 $\pm$ 0.25 & 3.17 $\pm$ 0.17 & 86.29 $\pm$ 0.20 & 10.76  & 72.09  \\
    \SC & 50.57 $\pm$ 0.17 & ~~6.71 $\pm$ 0.18 & 28.25 $\pm$ 0.60 & 12.23 $\pm$ 0.54 & 11.07 $\pm$ 0.15 & 20.20 $\pm$ 0.16 & 9.40 $\pm$ 0.21 & \textbf{14.35 $\pm$ 0.23} & 24.82  & 13.37 \\
    \hline
    \rowcolor{gray!20} \RPC & \textbf{51.95 $\pm$ 0.15} & ~~\textbf{6.41 $\pm$ 0.18} & \textbf{31.62 $\pm$ 0.75} & ~~\textbf{9.87 $\pm$ 0.73} & \textbf{11.14 $\pm$ 0.15} & \textbf{18.86 $\pm$ 0.18} & \textbf{9.74 $\pm$ 0.23} & 14.32 $\pm$ 0.21 & \textbf{26.11} & \textbf{12.37} \\
    \bottomrule
    \toprule
    \end{tabular}}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table*}

\begin{figure}[t]
    % \vskip 0.1in
    \begin{center}
        \begin{minipage}[t]{\linewidth}
            \centering
            \begin{subfigure}[t]{0.48\linewidth}
                \centering
                \includegraphics[width=\linewidth]{figures/ECE/Odyssey-intern-1.0-0.95-256-0-NSC-128.pdf}
                \vskip -0.3em
                \caption{\SC}
            \end{subfigure}
            \hfill
            \begin{subfigure}[t]{0.48\linewidth}
                \centering
                \includegraphics[width=\linewidth]{figures/ECE/Odyssey-intern-1.0-0.95-256-0-NDWPC-128.pdf}
                \vskip -0.3em
                \caption{\RPC}
            \end{subfigure}
            \vskip -0.5em
            \caption{The reliability diagrams of \SC and \RPC on MathOdyssey dataset using InternLM-2-MATH-Plus 7B model.}
            \label{fig:InternLM2-7B-Reliability}
        \end{minipage}
    \end{center}
    \vskip -0.3in
\end{figure}

\begin{table}[t]
    \centering
    \caption{Performance Comparison of different models and different parameter scales. The accuracy is reported as the mean and stdev. The best performance is highlighted in \textbf{bold}. The results show that our \RPC outperforms existing methods in major cases.}
    \label{tab:model-performance}
    \vspace{-1em}
    \begin{center}
    \begin{small}
    \begin{sc}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccc}
    \bottomrule
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{4}{c}{\textbf{ InternLM2-Math-Plus 1.8B}} \\
    \cmidrule(lr){2-5}      & MATH & MathOdyssey & OlympiadBench & AIME \\
    \midrule
    \PP & 33.24 $\pm$ 0.24 & \textbf{16.56 $\pm$ 0.88} & 3.08 $\pm$ 0.20 & 1.66 $\pm$ 0.15 \\
    \Verb & ~~7.21 $\pm$ 0.17 & ~~2.81 $\pm$ 0.26 & 0.77 $\pm$ 0.06 & 0.26 $\pm$ 0.05 \\
    \SC & 36.48 $\pm$ 0.15 & 14.52 $\pm$ 0.46 & 5.99 $\pm$ 0.17 & 2.66 $\pm$ 0.20 \\
    \hline
    \rowcolor{gray!20} \RPC & \textbf{37.88 $\pm$ 0.16} & 16.35 $\pm$ 0.61 & \textbf{6.52 $\pm$ 0.24} & \textbf{3.26 $\pm$ 0.24} \\
    \bottomrule
    \toprule
    \multirow{2}{*}{Method} & \multicolumn{4}{c}{\textbf{DeepSeekMath-RL 7B}} \\
    \cmidrule(lr){2-5}      & MATH & MathOdyssey & OlympiadBench & AIME \\
    \midrule
    \PP & 42.51 $\pm$ 0.23 & 22.34 $\pm$ 1.00 & ~~5.90 $\pm$ 0.31 & 3.37 $\pm$ 0.46 \\
    \Verb & 14.29 $\pm$ 0.23 & ~~2.55 $\pm$ 0.24 & ~~2.36 $\pm$ 0.15 & 1.91 $\pm$ 0.12 \\
    \SC & 53.33 $\pm$ 0.09 & 36.68 $\pm$ 0.65 & 11.29 $\pm$ 0.17 & 9.42 $\pm$ 0.23 \\
    \hline
    \rowcolor{gray!20} \RPC & \textbf{53.37 $\pm$ 0.11} & \textbf{37.25 $\pm$ 0.69} & \textbf{11.30 $\pm$ 0.11} & \textbf{9.52 $\pm$ 0.31} \\
    \bottomrule
    \toprule
    \end{tabular}}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}
    
\subsection{Empirical Results}

\textbf{\underline{RQ1}: Efficiency.} How does \RPC reduce the number of samples required to achieve comparable performance through faster convergence?

We evaluate our proposed \RPC against the standard self-consistency method using four mathematical benchmark datasets with the InternLM-2-MATH-Plus 7B model. For the MATH dataset, we set the reasoning path size to 64, while we set the number of reasoning paths to 128 for the other datasets with \SC. We then record the best performance and minimum sampling requirements for \SC. For both \RPC and our \emph{Perplexity Consistency} module (denoted as \PC), we report the minimum number of samples needed to match or exceed the performance of the \SC in \autoref{tab:InternLM2-7B-Reduction}.

The results of \PC indicate improved convergence rates compared to \SC in several cases, while maintaining similar rates in others. These findings support the rapid convergence and degeneration issues of \PC in Theorem~\ref{thm:thm1}. \RPC shows significant efficiency improvements by requiring fewer samples to achieve comparable performance relative to \SC. Moreover, the degeneration issues observed in \PC are effectively addressed in \RPC, validating both the effectiveness of the \emph{Reasoning Pruning} module and our Theorem~\ref{thm:thm2}.


\textbf{\underline{RQ2}: Efficacy.} How does \RPC improve reasoning performance compared to existing methods?

We evaluate the performance of \PC and \RPC in \autoref{fig:InternLM2-7B-Accuracy} across various sample budgets. The results demonstrate that \RPC achieves better performance than both \PP (which relies on internal LLM probabilities) and \SC (which employs Monte Carlo sampling). The detailed accuracy results, including mean and standard deviation in \autoref{tab:InternLM2-7B-Performance} support these findings.

We also analyze the performance of \PC separately. The results indicate that \PC has a faster convergence rate than \SC, which aligns with Theorem~\ref{thm:thm1}. The significant performance gains from \PC to \RPC, as shown in \autoref{fig:MATH-Accuracy} and \autoref{fig:MathOdyssey-Accuracy}, validate the effectiveness of the \emph{Reasoning Pruning} module. This suggests that \emph{Reasoning Pruning} helps reduce model errors when the LLM exhibits good alignment by eliminating incorrect reasoning paths that carry low LLM probability scores.

\begin{figure}[t]
    \vskip 0.1in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{figures/Code/accs.pdf}}
    \caption{Performance on code generation tasks using Deepseek-Coder 33B model. The results show that our \RPC achieves the best performance.}
    \label{fig:code-accuracy}
    \end{center}
    \vskip -0.3in
\end{figure}

\textbf{\underline{RQ3}: Reliability.} How does \RPC enhance the reliability of confidence estimation compared to existing methods?

To evaluate the reliability of confidence estimation, we analyze the ECE results of \RPC and comparison methods in \autoref{tab:InternLM2-7B-Performance}. ECE measures the difference between predicted probabilities and empirical accuracy, as directly computing estimation error using ground-truth probabilities is virtually impractical. The results demonstrate that our \RPC approach achieves lower ECE values and higher accuracy compared to \PP and \SC, indicating more reliable confidence estimation. We visualize this improvement through reliability diagrams comparing \SC and \RPC in \autoref{fig:InternLM2-7B-Reliability} on MathOdyssey, which clearly shows the reduced calibration error of \RPC.

\subsection{Further Discussion}

\textbf{Results on Code Generation Tasks.}
To investigate whether our proposed approaches can generalize to other reasoning tasks, such as code generation tasks, we evaluate \RPC and comparison methods on three code generation benchmarks, as illustrated in \autoref{fig:code-accuracy}. The results show that \RPC achieves the highest accuracy across all datasets, demonstrating its effectiveness in reasoning tasks beyond mathematics. 

\textbf{Results Across Model Scales and Architectures.}
To evaluate the generalization ability of our approaches across different model scales and architectures, we conducted additional experiments using InternLM2-Math-Plus 1.8B and DeepSeek-Math 7B models. The results in \autoref{tab:model-performance} show that \RPC consistently outperforms existing methods, which is consistent with results in \autoref{tab:InternLM2-7B-Performance}.
