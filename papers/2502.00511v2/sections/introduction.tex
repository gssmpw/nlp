\section{Introduction}

Recently, large language models (LLMs) have shown significant progress in various applications such as problem solving~\citep{LewkowyczADDMRS22, li24coc}, planning~\citep{ValmeekamMSK23plan, deng24plan}, and decision making~\citep{Ouyang023decision, SblendorioDCGPC24decision}, demonstrating their reasoning capabilities. 
Since single-shot inference is not always reliable, especially in complex reasoning tasks, one often requires the LLM to produce multiple reasoning paths, facilitating its reasoning performance.

When multiple reasoning paths for a given problem are available, the reasoning performance is determined by the confidence estimation for each result.
To achieve this, perplexity methods~\citep{chen1998evaluation, murugadoss2025evaluating} apply LLMs' internal probability to estimate the confidence of the reasoning path. 
Although the internal probability is quite accurate, the reasoning path confidence is highly insufficient to distinguish each answer, thereby greatly limiting the effectiveness of perplexity methods~\citep{chen24steering}.
In contrast,
self-consistency methods~\citep{wang2022self, chen2023universal} switch to establish the answer confidence using a pre-defined consistency function. 
However, the answer confidence cannot be directly derived from the internal probabilities of LLMs, necessitating the use of Monte-Carlo estimation, which significantly degrades the convergence rate~\citep{amad23adaptive, wang24dynamic, wang24make}.

To better understand the limitations of current methods and to guide the development of an effective and efficient LLM reasoning approach, we formulate the LLM reasoning problem and present a theoretical analysis that decomposes the reasoning error into two components: \emph{Estimation Error} and \emph{Model Error}. 
Self-consistency methods, which rely on the Monte-Carlo estimation, achieve only a linear estimation error reduction rate with respect to the sample size.
The linear convergence rate leads to the method requiring a large sampled budget.
For instance, implementing self-consistency with 64 samples on the MATH dataset using the GPT-4 API costs approximately \$2000~\citep{li24escape}, rendering it extremely expensive for both researchers and organizations.
As to perplexity methods, their estimation error converges exponentially as they use the internal probability of LLMs.
The exponential convergence rate ensures that perplexity methods can work well even in a very limited sample budget, while its final convergent result is far from satisfactory due to the high model error.
A comparison between self-consistency methods and perplexity methods is shown in Figure~\ref{fig:intro-comparison}.
This complementary between estimation error and model error raises a chance to further improve the LLM reasoning performance:
\emph{Can we design a method that achieves both fast estimation error convergence rate and low model error?}
To the best of our knowledge, the efforts in this aspect remain limited.

\begin{figure}[t]
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{figures/intro.pdf}}
    \caption{
        The comparison between self-consistency and perplexity methods. Self-consistency uses Monte-Carlo estimation while perplexity methods directly use LLM prediction probability (i.e., computing probabilities using perplexity of LLM).}
    \label{fig:intro-comparison}
    \end{center}
    \vskip -0.4in
\end{figure}

In this paper, we explore effectively and efficiently integrating the internal LLM probability into the self-consistency framework, allowing us to utilize an accurate probability for rapid estimation error reduction while maintaining low model error.
We name this confidence estimation approach \emph{Perplexity Consistency}.
Our theoretical study illustrates that perplexity consistency provides a tight integration and can indeed achieve the goal.
However, the reduction rate of perplexity consistency estimation error undesirably degenerates to a linear rate when the magnitude of the LLM's internal probability is low. 
To tackle this issue, we further introduce \emph{Reasoning Pruning} to automatically model the probability distribution for each reasoning problem and remove low-probability reasoning paths. 
Combining the perplexity consistency and the reasoning pruning, we propose \emph{\textbf{R}easoning-pruning \textbf{P}erplexity \textbf{C}onsistency} (\RPC).

Our theoretical and experimental results confirm the efficient and effective performance of \RPC. 
Specifically, on four mathematical reasoning datasets, \RPC successfully reduces the sampling budget by at least 50\% while achieving the same reasoning performance as self-consistency. 
Conversely, with an equal sampling budget, \RPC outperforms existing methods by 1.29\% on average.
Additionally, \RPC provides confidence estimates that align better with the ground truth compared to existing methods.

To summarize, the main contributions of the paper are:

(1) We formulate the LLM reasoning problem and offer a theoretical analysis that decomposes LLM reasoning performance into estimation error and model error. This analysis emphasizes the benefits of self-consistency while revealing its limitations when working with limited sampling budgets.

(2) Building on our theoretical framework, we introduce the \RPC, which integrates \emph{Perplexity Consistency} and \emph{Reasoning Pruning}. This approach utilizes precise LLM probabilities and eliminates low-probability reasoning paths to enhance reasoning performance.

(3) Our theoretical analysis shows that \emph{Perplexity Consistency} achieves an exponential error reduction rate in most cases, and \emph{Reasoning Pruning} effectively compensates for the remaining degeneration issues. 

(4) Through extensive experiments conducted on four mathematical reasoning and three code generation tasks, our proposed \RPC delivers promising results of improving both accuracy and confidence consistency. 
