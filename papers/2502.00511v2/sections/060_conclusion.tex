\section{Conclusion} \label{sec:concl}

In this paper, we address a foundational challenge in LLM reasoning: determining the most reliable answer from multiple reasoning paths by measuring LLM confidence. 
We present a theoretical framework to decompose the reasoning error into estimation error and model error, revealing that perplexity methods suffer from substantial model error due to lacking consistency function while self-consistency suffers from high estimation error because of a slow error convergence rate. 
To tackle this limitation, we introduce \emph{\textbf{R}easoning-pruning \textbf{P}erplexity \textbf{C}onsistency} (\RPC), a confidence estimation method with two key components: 
\emph{Perplexity Consistency} utilizes internal LLM probabilities to achieve faster estimation error convergence in major cases. 
\emph{Reasoning Pruning} prunes low-probability reasoning paths to prevent the remaining degeneration cases. 
Our theoretical analysis and extensive experiments demonstrate that \RPC achieves superior error convergence rates and reasoning performance compared to existing methods.

\textbf{Limitations and Future Work.} One limitation of this work is that we have only taken an initial step toward improving confidence estimation for self-consistency. The two components of \RPC can be further enhanced: \emph{Perplexity Consistency} could incorporate additional probability metrics from LLM outputs, while \emph{Reasoning Pruning} could be extended with more sophisticated pruning strategies, such as analyzing the probability distribution of each candidate answer. We believe our initial approach and theoretical analysis can guide future research in this promising direction. 
