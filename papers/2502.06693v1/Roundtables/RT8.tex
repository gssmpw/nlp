\subsection{Bias and Fairness}

\paragraph{Subtopic:} What are the best practices for continuous evaluation of deployed models, given the rise in the popularity of foundation models? Given that resources and expertise required for the requisite local recalibration before model deployment and continuous evaluation of the model one deployed are not available to most health systems, how can we realistically ascertain \textit{first do no harm}? Given how higher education is notoriously slow to change, how do we achieve the AI literacy needed for fail-safe human-AI systems?

\paragraph{Chairs:‌}
\textit{Leo Anthony Celi, Alina Peluso, and Amin Adibi}

\paragraph{Background:}
Philosophers of science have long argued that scientific objectivity and the value-free ideal are myths, particularly in how studies are conceptualized, funded, interpreted, published, and publicized \citep{reiss_scientific_2020}. Even the production of scientific evidence involves a "garden of forking paths" of countless decisions with equally valid alternatives \citep{gelman_garden_2013}, as exemplified by models used to guide the COVID-19 response \citep{harvard_value_2021}. This understanding suggests that it matters who does the research \citep{charpignon_does_2024}. As such, epistemic humility, plurality, and incorporation of diverse viewpoints and lived experiences should be at the heart of the fairness movement in AI.

Recent advances in foundation models have aligned economic and political forces to push for rapid AI deployment, even in traditionally risk-averse fields such as medicine. Although some large health systems in the US have created roles such as Chief Health Data Officer \citep{beecy_chief_2024}, most hospitals in the developed and developing world lack the expertise and resources required for proper AI governance, deployment, and continuous evaluation. A complete overhaul of medical education, clinical validation, regulation, and biomedical innovation appears necessary to equip health systems with the expertise and infrastructure to innovate and improve care, while safeguarding against the risk of harm, particularly to the most vulnerable populations. 

\paragraph{Discussion:}
The roundtable discussion was catalyzed by a recent incident of inappropriate stereotyping in research. Society often emphasizes characteristics such as gender, race and ethnicity in descriptions, even when these factors may not be essential to the discussion. This type of stereotyping reduces the complexity of individuals or groups to simplistic labels. While these traits can be relevant in some contexts, such as discussions of systemic inequality, it is important to assess whether their inclusion is necessary and respectful. One way to avoid such pitfalls for us, the researchers, is by actively seeking out diverse team members who are willing to challenge our assumptions and call out our biases. 

\subsubsection{Embracing Epistemic Humility and Plurality}
Participants noted the speed at which the field is progressing, making it almost impossible for any single person to be an expert in all aspects of AI research. Incorporating social scientists—particularly those with expertise in feminism, race theory, and ethics— and patient views into our teams enriches AI research and ensures that marginalized voices are heard. This diversity of expertise helps shape AI systems that are not only more inclusive but also more reflective of the complexity and nuances of human experience. However, despite the critical importance of interdisciplinary perspectives in AI development, even leading universities may be reluctant to recruit multidisciplinary scientists for faculty positions if they lack traditional technical expertise.

The discussion also emphasized the role of value judgments in research and the importance of incorporating diverse viewpoints from various cultural backgrounds. One participant shared how working with Indigenous researchers revealed alternative frameworks for ensuring research benefits affected communities through relevance, equity of partnership, and data sovereignty. Epistemic humility and plurality require engaging with others in a way that values their ways of knowing, unique identities, and perspectives, while remaining mindful of the social and historical contexts that shape our own views.

\subsubsection{Epistemic Justice and Algorithmic Humility}
Epistemic injustice and oppression constrain scientific progress by excluding vital perspectives and insights \citep{dotson_conceptualizing_2014, fricker_epistemic_2007, kay_epistemic_2024}. When entire groups face marginalization from knowledge production and scientific discourse, we lose insights that could challenge prevailing assumptions and enrich our understanding. To paraphrase an old adage: "If everyone is thinking alike, then no one is really thinking." This is where algorithmic humility — AI designed to offset human arrogance and ignorance - comes in. Algorithmic humility is an approach to AI system design that explicitly acknowledges the boundaries of both human and machine intelligence. Unlike conventional AI approaches, systems built on algorithmic humility principles incorporate mechanisms to actively counteract human overconfidence and uncritical acceptance of automated outputs \citep{kompa_second_2021}. Through this framework, algorithmic humility seeks to develop AI systems that not only address human cognitive biases but also resist becoming unquestioned sources of authority themselves.

\subsubsection{Structural Barriers to Inclusive Research}
Participants identified several systemic barriers to achieving truly inclusive ML research in healthcare. The \textit{publish or perish} culture continues to incentivize quantity over quality and methodology over impact. Conference accessibility remains severely limited by visa restrictions — with wait times for US visa interviews exceeding two years in some countries — and prohibitive travel costs \citep{hutson_canada_2018, owusu-gyamfi_exhausted_2024, thompson_canadian_2024}. Even when researchers from underrepresented groups participate, power dynamics can make it challenging for junior scholars to speak up about bias and discrimination without risking career advancement. The discussion also highlighted how major conferences like NeurIPS, ICML, and ICLR often prioritize the novelty of methodology and larger datasets over real-world impact, perpetuating existing power structures. Extractive AI practices exacerbate barriers to inclusivity by exploiting the resources and labor of the global majority without contributing to the societies that bear the costs of AI development \citep{crawford_atlas_2021, li_making_2023, luccioni_power_2024}.

\subsubsection{Novel Approaches to Measuring Disparities}
While much work on bias and fairness has focused on technical approaches, apparent technical solutions can mask deeper systemic issues. Participants shared examples of how such solutions can fall short. For example, attempting to debias models for multiple protected attributes simultaneously can degrade the model’s performance to the point of clinical irrelevance, achieving equality through universal poor performance. 

Ensuring the safe and effective deployment of AI requires continuous evaluation, transparency, and human oversight, with rigorous testing and active involvement of clinicians in decision-making \citep{adibi2020, Blumenthal2024}. Strong regulation and ongoing model evaluation are key to ensuring AI tools are ethically integrated. Fairness in AI is not about achieving perfect equality but preventing harm to vulnerable populations. 

Participants highlighted innovative methodologies for identifying and measuring healthcare disparities without relying on traditional demographic categories. One innovative approach introduced the concept of \textit{care phenotypes}, patterns in care delivery that reveal systematically underserved patient populations. For instance, research demonstrated that the heaviest intubated patients were not only turned in their beds less frequently than others, but also, inexcusably, received mouth care less often \citep{weishaupt_care_2025}. This approach revealed systematic disparities in care delivery that map to social determinants and potentially accounted for intersectionality, without requiring explicit collection of demographic data. Another example showed how language barriers affected the frequency of blood sugar testing, as staff avoided waking non-English-speaking patients during night shifts when interpreters were not readily available.

\subsubsection{Education and Training Reform}
Healthcare systems often lack the expertise and infrastructure for responsible AI deployment. The roundtable highlighted the need to fundamentally reimagine medical and technical education, with concerning findings that patients sometimes perceive chatbots as more empathetic than human doctors \citep{ayers_comparing_2023}. Participants emphasized the need to reimagine medical education to nurture empathy and incorporate systems thinking and critical analysis of technology. This includes teaching students to question assumptions, understand local context, and consider long-term impacts rather than just immediate clinical outcomes. A systems-level approach is needed to integrate epistemic humility into AI and medical curricula. Empathy and interdisciplinary collaboration should be prioritized to equip students with the tools to address the ethical complexities of AI and its intersection with healthcare. 

\subsubsection{Optimism and Call for Action}
The discussion yielded several concrete recommendations, including expanding conference accessibility through virtual participation options and events in diverse locations and languages, advocating for diverse representation in research teams and paper authorship, developing new frameworks for clinical validation that account for local context and temporal shifts in care patterns, creating dedicated courses on epistemic humility and plurality in computer science, reforming publication incentives to value impact and ethical considerations over methodological novelty, and building research teams that include social scientists and bioethicists and incorporate patient perspectives from project inception.

While acknowledging the magnitude of needed changes, participants expressed optimism about growing awareness and willingness to confront bias in the field. They emphasized that progress requires moving these conversations from the margins to the center, with everyone taking responsibility for change within their sphere of influence. The discussion highlighted how even those early in their careers can make meaningful contributions, citing examples of PhD students choosing to focus on bias and fairness despite institutional pressure to pursue more traditional technical topics. Success will require sustained effort to reimagine education, research, and clinical validation while maintaining focus on benefits to vulnerable populations. 


