\subsection{Causality}

\paragraph{Subtopic:} Many sub-areas in machine learning (e.g., Computer Vision and Natural Language Processing) have greatly benefited from having standardized benchmarks; if causality were to create such a benchmark, what should it look like? And what might be some challenges? Predictive models are widely used in many healthcare applications; how could causality be used to better develop predictive models?

\paragraph{Chairs:}
\textit{Rahul Krishnan, Johnny Xi, Trenton Chang, and Winston Chen}

\paragraph{Background:} 
Causal inference is widely applied in healthcare to derive reliable medical insights and develop robust AI tools to enhance the quality and efficiency of care. We started our discussion by having participants share their expertise in causal inference and what healthcare problems they have tried to solve using causal inference.

Standardized benchmarks have been proven effective in moving subareas of machine learning forward. We further discussed the potential of creating standardized benchmarks for causal inference in healthcare. Participants shared their thoughts on what those benchmarks would look like and what challenges the community must overcome. 

Finally, machine learning has been widely used to develop predictive models in healthcare. We further discussed how causality methods can improve predictive models. What are some considerations when building causally motivated models? 

\paragraph{Discussion:} 

We began by understanding participants' experience with causality. Most participants are familiar with the concept of causal inference, and about half of the participants have hands-on experience working with causal inference methods to estimate the effects of interest.

During the discussion, participants also shared applications in healthcare for which they have used causal methods. Most participants leverage causal inference to understand the heterogeneous effect of specific clinical treatments (e.g., the effect of chemotherapy on lung cancer patients with different genetic mutations). 
Some participants build causal reward models to better optimize for reinforcement learning agents to provide drug dosage recommendations. 

Other applications include incorporating causality into the learning process involving healthcare data and emulating clinical trials with causal models to assess the benefits of drug re-purposing efficiently. 

\subsubsection{Benchmarks and Evaluation}

Creating standard benchmarks has immensely benefited many sub-fields of machine learning. However, such a standard benchmark does not yet exist in causality and healthcare. One of the biggest challenges in creating a standard causality benchmark is that evaluating the estimated causal relationship is not straightforward. This is because ground-truth causal effects are rarely observed. 

Some participants shared their thoughts on what a principled evaluation protocol for causal inference might look like. One possibility is to leverage both observational and interventional (randomized control trial) data to assess the performance of different methods in inferring causal relationships. In this approach, causal effects may be estimated from observational data and subsequently validated on interventional data. 

Another approach for evaluating the causality method is considering the downstream application for which the causal model is built. For example, when causal inference is developed for decision-making, a meaningful approach to evaluate different methods is quantifying the decision-making performance following the effects estimated by each method. 

Interventional data can be used to accurately estimate decision-making performance by leveraging off-policy evaluation (OPE) methods. Other participants also proposed creating benchmarks by selecting applications with which true causal effects can be accurately measured. For example, in many biological and molecular applications, outcomes conditioned on different treatments can be obtained with careful experimental designs. In such cases, researchers can obtain the true causal relationship and use it to validate the performance of different causality methods.
With the recent rise of large language models (LLMs), simulation-based evaluation has gained interest in causality. Instead of estimating the performance of causal methods via OPE methods or selecting applications for which true causal relationships can be experimentally validated, creating high-fidelity simulation can be a good and cheap starting point for evaluating estimated causal insights.

This idea of simulation-based evaluation is common in social sciences such as psychology. In this field, LLMs simulate different human agents and use the simulation results to rank hypotheses and prioritize experimental validation. 
These ideas can inspire the evaluation of causality. 

Some participants also pointed out the importance of having a standardized reporting guideline. Future guidelines should consider reporting the sensitivity of estimation instead of simply reporting the estimated effects. For example, some important information to report include whether certain subpopulations disproportionately contributing to the estimated effect, and how much the estimated effect vary when modifying specific individuals in the cohort.

\subsubsection{Causality and Prediction}

Lastly, our round table discussed how causality could be used to improve predictive models in healthcare. It is important to note that although causality has many desirable properties, it is not always applicable. This is because, in practice, we often observe a trade-off between robustness and performance when building causally-motivated models. Fully causal models are often robust yet suboptimal in terms of performance. Good use of causality should consider the need for specific applications. For example, if certain applications are concerned with having the most performative model instead of leveraging causal relationships, using causal methods might not be necessary. In practice, high-performance predictive models often need to leverage non-causal correlations.

Disentangling causal and correlational representations in the learned predictive model can be a powerful future research direction. Learning such disentangled representations allows practitioners to reuse the causal components across different environments (e.g., hospitals). Then, by augmenting the causal components with environment-specifically learned correlational representations, predictive models' performance can be further enhanced to obtain the best of both the causal and correlational worlds.

Lastly, our participants shared their interests in applying causality of building foundation models in healthcare. Currently, the majority of the foundation models are built purely by leveraging correlations. It is unclear how causality can be foundation models and what some benefits of having causally-motivated foundation models are. This largely unexplored area makes a very intriguing and an impactful future research direction. 