\subsection{Foundation Models and Multimodal AI}
The convergence of multimodal foundation models and healthcare presents a frontier where diverse data streams—from medical imaging and clinical text to biosignals and genomic data—intersect to create comprehensive patient representations. During this roundtable, participants explored critical questions shaping the development of multimodal AI in healthcare:
How does multimodality affect model robustness and feature learning?
What are the privacy and security implications of multimodal healthcare models?
How should demographic and clinical features be incorporated into foundation models?
How do we handle incomplete and misaligned data across modalities?
How do we address causality and confounding in multimodal healthcare models?
What are the trade-offs between general-purpose and healthcare-specific foundation models?
How do we effectively handle temporal data and evaluation in clinical settings?

\paragraph{Chairs:}
\textit{Jason Fries, Max Xu, and Hejie Cui} 

\noindent Special acknowledgment goes to Junyi Gao for helping with taking the notes. 

\paragraph{Background:}
The emergence of multimodal foundation models marks a pivotal advancement in artificial intelligence, particularly for healthcare applications where patient data naturally spans multiple modalities. These models transcend traditional single-modality approaches by simultaneously processing and integrating diverse data types—medical imaging, clinical narratives, structured EHR data, genomic information, and temporal biosignals. This multimodal capability offers unprecedented opportunities to capture the intricate interplay between different aspects of patient health, potentially leading to more comprehensive and nuanced clinical understanding.

Healthcare presents a unique proving ground for multimodal AI, where the synthesis of diverse data types is not just beneficial but often essential for accurate diagnosis and treatment planning. A clinician's assessment typically involves integrating visual information from medical imaging, structured data from lab tests, temporal patterns from vital signs, and contextual information from clinical notes. Multimodal foundation models aim to mirror and augment this holistic approach to patient care.

However, the healthcare domain poses distinct challenges for multimodal AI development. Beyond common hurdles of data privacy and model interpretability, healthcare applications must contend with temporal misalignment between different data streams, varying data quality across modalities, missing modalities in real-world settings, and the need to maintain clinical validity across all data types. This roundtable discussion aimed to explore these intricate dynamics, bringing together experts to examine how multimodal foundation models can be effectively developed and deployed in healthcare while addressing these unique challenges.

\paragraph{Discussion:}
The roundtable discussion explored key challenges and opportunities in developing multimodal foundation models for healthcare applications. Through our discussion, five major themes emerged, spanning technical, clinical, and practical considerations. These themes highlight the complexities of integrating multiple modalities in healthcare AI while maintaining clinical utility and practical feasibility. 

\subsubsection{Data Integration and Robustness}
The first topic highlighted how the integration of multiple data modalities presents both opportunities and challenges for healthcare AI. Participants noted that combining different data types (imaging, text, biosignals) could inherently increase model robustness by reducing reliance on spurious correlations and encouraging more grounded learning. Vision-language models were frequently cited as examples where multimodal integration leads to better generalization. However, this benefit comes with increased complexity in handling data alignment, as healthcare data is often incomplete across modalities. The group emphasized that while more modalities can enrich understanding, they can also limit the available sample size of fully aligned data sets.

\subsubsection{Clinical Context and Data Quality}
A recurring topic was the importance of understanding the clinical context in data collection and model development. Healthcare data collection is inherently biased by clinical decision-making processes – MRI scans are typically only performed for suspected cases, and lab tests are ordered based on clinical necessity. This selection bias poses unique challenges for developing robust multimodal models. The group stressed that understanding these clinical sampling biases is crucial for building trustworthy models. Additionally, the quality of data varies significantly across modalities, with some data types being more standardized (lab values) than others (clinical notes).

\subsubsection{Model Architecture and Specialization}
The discussion revealed insights about model architecture choices and the degree of specialization needed. While general-purpose foundation models can provide quick baselines, they often lack the fine-grained understanding necessary for complex medical tasks. The group noted that even state-of-the-art general models might struggle to outperform simpler, specialized models on specific clinical prediction tasks. This observation led to a consensus that healthcare-specific architectures, particularly those designed to handle multiple modalities effectively, may be necessary for optimal performance.

\subsubsection{Temporal Considerations and Infrastructure}
The handling of temporal data emerged as a critical challenge in multimodal healthcare AI. Healthcare systems generate complex time series data across different modalities, often with obfuscated timestamps and events not recorded in chronological order. The group emphasized the need for robust methods to handle temporal alignment across modalities and the importance of maintaining temporal coherence in model predictions. This challenge is compounded by current healthcare IT infrastructure limitations, which often make it difficult to align and synchronize data from different sources properly.

\subsubsection{Privacy and Security}
The integration of multiple modalities raises significant privacy and security concerns. Each additional modality introduces new potential vulnerabilities and increases the complexity of protecting sensitive patient information. Resource constraints were identified as a major barrier to building specialized healthcare foundation models from scratch. Many participants advocated for a pragmatic approach of fine-tuning pre-trained general models on healthcare-specific tasks, balancing computational costs with performance requirements.

\subsubsection{Future Directions}
The discussion highlighted three critical areas requiring attention for advancing multimodal foundation models in healthcare. First, there is a pressing need for robust methods to handle incomplete multimodal data and temporal alignment, particularly in real-world clinical settings where data streams are often misaligned or partially missing. Second, the development of healthcare-specific benchmarks that reflect real-world clinical conditions was identified as crucial for meaningful evaluation of these models. Third, participants emphasized the importance of improving data infrastructure to better support multimodal data integration while maintaining privacy and security standards. These directions reflect the practical challenges encountered in current implementations and represent concrete areas where focused research efforts could yield significant improvements.