\subsection{Public Datasets and Benchmarks}
\paragraph{Chairs:} \textit{Rahmat Beheshti, John Wu, and Adibvafa Fallahpour}
\paragraph{Background:}

The healthcare AI community faces significant challenges in establishing standardized benchmarks and utilizing public datasets effectively. Despite the availability of several large-scale medical datasets like MIMIC-CXR and MIMIC-IV, researchers encounter substantial difficulties in dataset preprocessing, standardization, and reproducibility of results.
\paragraph{Discussion:}
The roundtable discussion revealed several critical challenges and opportunities in healthcare AI benchmarking, which can be organized into four main themes:

\subsubsection{Dataset Standardization Challenges}
A significant concern raised was the heterogeneity in dataset usage, particularly evident in studies utilizing MIMIC-CXR. Participants highlighted the unbelievable heterogeneity in how researchers split the data, with no clear consensus on task definitions or evaluation metrics. While image-based models often come with standardized preprocessor classes, there is no equivalent standard for EHR data processing. This variance extends beyond simple data splitting to encompass the entire preprocessing pipeline. Of particular concern was the clinical validation of preprocessed variables, where participants noted a significant disconnect between dataset variables and clinical understanding, with estimates suggesting that up to half of the variables might be incorrectly defined or interpreted. The variability extends to fundamental choices such as whether to use radiological findings or impressions and how to define train versus test distributions. The preprocessing pipeline has effectively become an integral part of model design and performance evaluation. The emergence of the MEDS data format represents a promising attempt to standardize EHR data representation and preprocessing, though adoption remains a challenge due to limited incentives for standardization.

\subsubsection{Benchmark Development and Adoption}
The fundamental question of benchmark definition emerged as a critical issue. Before implementing any benchmark system, participants emphasized the need to clearly define the benchmark's objectives and purpose. Current adoption rates of existing benchmarks remain concerningly low, with longitudinal EHR data benchmarks being used by a maximum of nine studies for comparison purposes. The discussion revealed that many researchers focus on developing increasingly complex models rather than conducting comprehensive baseline evaluations. A significant technical challenge emerged regarding the dependency of models on specific preprocessing pipelines, making it difficult to compare models across different preprocessing approaches. Models are often essentially hardcoded for specific types of preprocessing, such as particular sampling rates, making standardization particularly challenging.

\subsubsection{Dataset Availability and Utilization}

The discussion challenged the common perception of limited dataset availability in healthcare AI. Participants noted the existence of substantial public data resources, including approximately 700,000 public ICU patient records and emergency datasets from Iran. However, the real challenge lies not in data availability but in utilization. Researchers face significant barriers including time constraints for data exploration and processing, and limited awareness of available datasets. Even when datasets are publicly available, access can be complicated by payment requirements, non-responsive data holders, and complex licensing agreements. These practical barriers often prevent researchers from exploring alternatives to commonly used datasets like MIMIC. Some groups have successfully merged multiple public ICU datasets, demonstrating that meaningful generalization becomes possible at the scale of seven combined datasets.

\subsubsection{Future Directions and Recommendations}

The path forward requires addressing multiple challenges simultaneously. Participants advocated for the development of multi-population benchmarks that span different healthcare settings, from ICU to standard population care, and diverse geographical locations. The potential of LLMs for dataset harmonization was discussed as a promising avenue for future research. The group emphasized the importance of implementing standardized sub-splits specifically designed to evaluate bias and fairness across different patient populations. Collaborative discussions about cohort selection and feature definition should be conducted openly, potentially through platforms like GitHub, rather than being buried within academic papers.

High-quality benchmark papers should provide comprehensive documentation of the complete pipeline from raw data to predictive modeling, accompanied by well-documented code. The inclusion of completely open demonstration datasets free from licensing restrictions was identified as crucial for enabling code validation and pipeline reproducibility. Papers should report inter-annotator agreement for labels and provide detailed discussions of cohort selection methodology. Participants emphasized the importance of reproducibility and transparency in all aspects of the benchmarking process, including detailed metadata documentation, feasible range specifications, and feature descriptions.

Several critical questions emerged that warrant further investigation. The community must determine the optimal balance between comprehensive feature sets and expert-selected features in ICU datasets. The challenge of incentivizing researchers to adopt updated datasets and reproduce baseline results remains unresolved. The potential role of meta-reviews in standardizing benchmark practices requires further exploration, as does the development of better frameworks for supporting cohort selection discussions in the academic community.

% \subsection{Public Datasets and Benchmarks}
% \paragraph{Chairs:} \textit{John Wu}, \textit{Adibvafa Fallahpour}, \textit{Rahmat Beheshti} % not sure if order matters lol. Alphabetical? 

% \paragraph{Background:}
% The healthcare AI community faces significant challenges in establishing standardized benchmarks and utilizing public datasets effectively. Despite the availability of several large-scale medical datasets like MIMIC-CXR and MIMIC-IV, researchers encounter substantial difficulties in dataset preprocessing, standardization, and reproducibility of results.
% \paragraph{Discussion:}
% The roundtable discussion revealed several critical challenges and opportunities in healthcare AI benchmarking, which can be organized into four main themes:
% \subsubsection{Dataset Standardization Challenges}
% A significant concern raised was the heterogeneity in dataset usage, particularly evident in studies utilizing MIMIC-CXR. Participants highlighted the unbelievable heterogeneity in how researchers split the data, with no clear consensus on task definitions or evaluation metrics. This variance extends beyond simple data splitting to encompass the entire preprocessing pipeline. Of particular concern was the clinical validation of preprocessed variables, where participants noted a significant disconnect between dataset variables and clinical understanding, with estimates suggesting that up to half of the variables might be incorrectly defined or interpreted.
% \subsubsection{Benchmark Development and Adoption}
% The fundamental question of benchmark definition emerged as a critical issue. Before implementing any benchmark system, participants emphasized the need to clearly define the benchmark's objectives and purpose. Current adoption rates of existing benchmarks remain concerningly low, with most benchmarks being used by fewer than ten studies for actual comparison purposes. The discussion revealed that many researchers focus on developing increasingly complex models rather than conducting comprehensive baseline evaluations. A significant technical challenge emerged regarding the dependency of models on specific preprocessing pipelines, making it difficult to compare models across different preprocessing approaches. Models are often essentially hardcoded for specific types of preprocessing, such as particular sampling rates, making standardization particularly challenging.
% \subsubsection{Dataset Availability and Utilization}
% The discussion challenged the common perception of limited dataset availability in healthcare AI. Participants noted the existence of substantial public data resources, including approximately 700,000 public ICU patient records. However, the real challenge lies not in data availability but in utilization. Researchers face significant barriers including time constraints for data exploration and processing, and limited awareness of available datasets. Even when datasets are publicly available, access can be complicated by payment requirements, non-responsive data holders, and complex licensing agreements. These practical barriers often prevent researchers from exploring alternatives to commonly used datasets like MIMIC.
% \subsubsection{Future Directions and Recommendations}
% The path forward requires addressing multiple challenges simultaneously. Participants advocated for the development of multi-population benchmarks that span different healthcare settings, from ICU to standard population care, and diverse geographical locations. A suggestion emerged to integrate at least seven diverse ICU datasets to support meaningful generalization studies. The potential of large language models (LLMs) for dataset harmonization was discussed as a promising avenue for future research. The group emphasized the importance of implementing standardized sub-splits specifically designed to evaluate bias and fairness across different patient populations.
% \paragraph{Quality Indicators for Benchmark Papers}
% High-quality benchmark papers should provide comprehensive documentation of the complete pipeline from raw data to predictive modeling, accompanied by well-documented code. The inclusion of demonstration datasets free from licensing restrictions was identified as crucial for enabling code validation. Papers should report interannotator agreement for labels and provide detailed discussions of cohort selection methodology. Participants emphasized the importance of reproducibility and transparency in all aspects of the benchmarking process.
% \paragraph{Open Questions:}
% Several critical questions emerged that warrant further investigation. The community must determine the optimal balance between comprehensive feature sets and expert-selected features in ICU datasets. The challenge of incentivizing researchers to adopt updated datasets and reproduce baseline results remains unresolved. The potential role of meta-reviews in standardizing benchmark practices requires further exploration, as does the development of better frameworks for supporting cohort selection discussions in the academic community.

% % all the notes that we quickly write down from the discussion
% - Rant: MIMIC-CXR -> papers citing -> unbelievable heterogeneity in how they split the data -> what task isn't even clear -> canonical split vs. (lots of different splits), different metrics
% - Hard to benchmark anything and is hard even on a public dataset 
% - ICU datasets -> MIMIC3, MIMICIV (different versions), Switzerland (hybrid? heart disease dataset?)
% - preprocessing pipeline -> preprocessed dataset -> clinician validation of variables -> half are wrong? dataset variables -> disconnect w/ clinician
% - benchmarking is impossible of private datasets (need exact specified dataset).

% Question? If we want to have ICU dataset, want most comprehensive features? or most expert focused/selected?
% - counter question: We need to define what type of benchmark we want in the first place? Leaderboards?

% - even if we have a benchmark, across any benchmark, cited people fewer than 10 works use it for benchmarking

% - lots of reproducibility works, baselines >> fancy model

% - Preprocessing: Resampling, etc. all matter differently to different people

% - Why can't we just test all models across different types of splits?
% - Problem is the models are built and trained off of specific preprocessing and are essentially hardcoded for a specific type of preprocessing (i.e sampling rates, etc.)
% - Preprocessing (formatting data) is very specific to the model.

% - Very reluctant/hard to convince researchers to revisit newly updated datasets.
% - No incentive to reproduce other baselines.
% - Why don't we see datasets other than MIMIC?
% - the question is misleading? Actually, there's a lot of publicly available data out there. 700k public ICU patient datasets that people could use. 
% - We just don't need it. 
% - Real question: Why don't we use them? 
% - Because people don't have the time to explore and process them. Also, people just don't know.
% - Benchmarks for multiple populations and distributions.
%     ICU vs. Standard Population
%     Different countries?
%     Different hospitals
% How many datasets?
% - need more than 2
% - 7 datasets ICU -> generalization 
% - just need enough datasets may overcome harmonization
% - maybe LLMs use harmonization?
% - Lots of datasets are publicly available -> but not really (payment, no response, etc.)

% - also important to propose sub-splits that ask interesting cohort questions to evaluate bias, fairness, etc. on top of existing standardized benchmarks
% - need more academic discussion surrounding cohort selection
% - what about meta-reviews? 
% - Meds, 

% - what qualities are a good benchmark paper?
% - pipeline -> raw data -> preprocessing -> predictive model, comprehensive
% - good code
% - demo dataset (that isn't privacy specific) without license agreements, etc. exams and other barriers to prevent us to run the actual code.
% - interannotator agreement for labels
% - red flags?
% - green flags? 

% Challenges in benchmarking
% MIMIC-CXR is used heterogeneously (how data is split, findings or impressions, train vs test distribution, different metrics and variously defined). Different variable selection for model training in ICU and tabular datasets. You need a general pipeline to get exact same split and variables to standardize benchmarking. We need leaderboards for benchmarking models on specific test datasets. On longitudinal EHR data benchmarks, maximum 9 works had used that benchmark. Many works are not reproducible especially the training/testing cohort. In machine learning for health there is core need to preprocess data and define tasks because works prefer their own preprocessing pipeline as they see fit. Image models come with a preprocessor class but there is no standard way of processing especially in EHR data. Preprocessing has become part of our model and performance as part of our design choices. MEDS data format tries to standardize the EHR data representation and preprocessing. Missing incentives for standardizing it.

% Limited dataset
% There is shocking amount of public data available but the field only views MIMIC. There are over 700,000 patients available for ICU (mentioned bunch of ICU datasets) across multiple diverse datasets, as well as emergency datasets from Iran. It hasn't been used because each new dataset needs new works to deal with that data which is not appropriate for publishing speed. We need to tackle the barriers in using public datasets. Hospitals won't share their data at scale. There is variety of dataset across populations which causes huge issues in generalization as we need way more data for generalization. Some group merged all public ICU datasets across different datasets and created a unified data and that scale of 7 datasets we see meaningful generalization. Different protocols and resolutions are used in medical video datasets from medical devices but its challenging to merge such huge variety of data. Good amount of time is spent curating cohorts of data to evaluate and there is no suggested subset of test dataset for evaluating fairness. We need to define what variables should be used for down stream tasks (it should be a combination of doctors and machine learning researchers to see what is a reasonable set of features to use). Collaborative discussions are important but this shouldn't buried under papers and rather should be publicly open like in GitHub. We need a paper on dataset that describes the metadata, feasible range, splits, features, etc. so we can control the diversity. MEDS is a simple standard for EHR data that allows you to focus less on data configuration to standardize across tasks.

% Good evals
% Transparency, simplicity, reproducibility. Good code is important too to be able to run. Publicly totally open demo dataset to make sure the pipelines are reproducible. 