\subsection{Integrating AI into Clinical Workflows}

\paragraph{Subtopic:} What level of evidence is required to decide to deploy or retire a model? How do emerging governance structures for AI deployment in hospitals (e.g., Chief AI officer) help these efforts? 

\paragraph{Chairs:} \textit{Adarsh Subbaswamy, Ayush Noori, and Elizabeth Healey}

\paragraph{Background:}
AI has the potential to transform clinical medicine through improved risk prediction, triaging, decision support systems, and helpful administrative tools. Integration of AI models and systems into clinical workflows may improve both outcomes and efficiency in medicine. However, there are many risks and considerations at play when deploying AI into medical systems that must be addressed. For example, AI models must be monitored to determine when they should be deployed and when they should be retired, and few standardized protocols exist for how to manage and monitor these systems over their lifetimes. Furthermore, some models may be more prone to distribution shift, privacy breaches, biases against protected classes, or other risks – therefore, the type of oversight necessary depends on characteristics of individual models. One approach to mitigate these harms include specialized governance structures like the FDA or Chief AI Officers.

This roundtable discussion spanned a breadth of considerations for deployment of AI into clinical workflows, including model considerations, regulatory structures, and practical limitations. The discussion was purposefully designed to bring together unique perspectives from a variety of stakeholders.  

\paragraph{Discussion:}

There was great interest in this topic at ML4H, with over 30 attendees at this roundtable on integrating AI into clinical workflows. Many participants from various backgrounds, including clinicians, industry professionals, and academics, contributed to the discussion. Below, we summarize the discussion by theme. 
 
\subsubsection{Gap Between Research and Deployment}
The conversation started with a discussion on what it takes to bring AI methods to clinical deployment. Many participants pointed out gaps that exist that prevent promising research from making it to a clinical setting. These gaps often center around ways that models are evaluated. Hospital integration of AI models relies on demonstrated clinical utility – a metric that is often measured during research and development of models. Moreover, for certain tools, such as those that include LLMs, there may be disagreement between providers about specific medical decisions.  The variability in human annotation further complicates the evaluation process of certain systems.  How can we measure the performance of decision support tools when human annotators do not agree?
 
The conversation also touched on other deployment considerations, such as distribution shifts.  The effect of distribution shifts should be quantified and studied during the development of models. It also may be prudent to set standards for certain types of deployments for required routine testing to calibrate models. 
Participants also discussed operational challenges with deploying systems in medical systems and cited the importance of including the end-user during the development of systems. Many new tools will require training from medical staff to properly utilize them. Human-computer interaction research on deployment of clinical AI will inevitably be essential to understanding the impact of systems in the clinical workflow, and for understanding how to continually improve these systems based on feedback. 

\subsubsection{Incentives}
The discussion briefly touched on incentives for both researchers and hospitals in this space. In particular, concerns were raised about how to make technology more equitable. Some hospital systems may be better equipped with infrastructure for integration of these systems. When developing AI systems, available infrastructure should be considered such that feasible deployment is not only limited to high-resource hospital institutions.
 
\subsubsection{Monitoring and Surveillance}
Many individuals highlighted the importance of regulatory structures and monitoring protocols specifically for AI deployments. There was agreement that different AI tools require different regulatory processes, and deciding how these tools should be monitored and who should regulate them is an area of importance.  The FDA has been involved in the regulation of biologics and devices. Clinical AI can take a variety of forms with a wide range of safety risks. It is not obvious now what AI models should be defined as medical devices. For high-risk systems, the idea of monthly audit reports was discussed. Most individuals agreed that the type of surveillance will depend on the type of model and specific risk of distribution shifts. 

  
\subsubsection{Looking Ahead Towards Deployable Clinical AI}
Among the diverse group of individuals at the roundtable, many cited the importance of interdisciplinary collaborations including both AI experts and clinicians. The involvement of clinicians in the early stages of projects is paramount to developing methods with the potential to impact clinical care. The group discussed the importance of individuals who work at the intersection of AI and medicine who can bridge the gap to deployment. 

