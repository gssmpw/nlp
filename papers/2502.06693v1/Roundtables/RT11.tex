\subsection{Social AI and Healthcare}

% \paragraph{Subtopic:} 

As artificial intelligence continues to evolve, its applications in healthcare have emerged as one of the most promising avenues for improving patient outcomes. Social AI, which focuses on understanding and interacting with human behavior, emotions, and social contexts, offers transformative potential for both mental and physical health interventions.  During this roundtable participants explored critical questions on the development of Social AI for healthcare:
How can ML-powered models of human social behavior improve the diagnosis and treatment of mental health conditions like depression, anxiety, and post-traumatic stress disorder (PTSD)? How do we combine and balance longitudinal history data and short-term behaviors for the modeling? How can Social AI enable real-time monitoring of patients' emotional and social health, both clinically and through wearable devices? How can Social AI tools adapt to individual patients' social and cultural contexts for equitable and effective care across diverse populations? How can models strike a balance between personalization and generalization for similar patients, benefiting from insights drawn from comparable cases?

\paragraph{Chairs:}
\textit{James M. Rehg, Yuwei Zhang, and Yurui Cao}

\paragraph{Background:} 

Advances in Machine Learning have led to the development of models capable of describing various aspects of human social communication and behavior.
These technological advancements hold significant potential for revolutionizing healthcare by introducing innovative methods for diagnosing and treating mental and developmental conditions, ultimately improving patient outcomes.

In the realm of mental health, AI's ability to model human behavior has been particularly impactful. Emerging tools now enable pre-diagnosis screening and risk assessment, offering insights into an individual’s predisposition to mental illnesses. For example, AI systems can analyze social media interactions to detect early signs of mental health disorders. Additionally, real-time monitoring through wearable devices and smart technologies enables continuous tracking of physiological and behavioral data, facilitating timely interventions while respecting patient autonomy.

Despite the promising applications, the integration of Social AI into healthcare presents challenges. Designing systems that adapt to individual patient needs and preferences is essential for effective care. Yet, personalization must be carefully balanced with concerns about bias and privacy to ensure equitable and secure healthcare delivery. Furthermore, incorporating social determinants of health into AI systems adds complexity, emphasizing the need for multidisciplinary approaches to ensure that these tools are both effective and inclusive.

% This research roundtable focuses on four topics:

% Social AI~\citep{lee2024towards}, Social Communication {\color{red} CITE}

% \begin{itemize}
%     % \item 
%     % % How advances in human behavior modeling can be used to harnessed to diagnose and treat mental health conditions and address developmental disorders
%     % \item 
%     % \item 
%     % \item Human-AI Collaboration in Care Delivery
    
%     % % \item 
%     % % \item
% \end{itemize}



\paragraph{Discussion:}
The discussion delved into AI's capacity to model human behavior, particularly in the areas of mental health diagnosis and treatment, and the management of developmental disorders such as autism and attention-deficit/hyperactivity disorder (ADHD). Participants explored how Social AI facilitates not only real-time emotional and behavioral health monitoring but also personalized interventions that adapt to individual patient needs. Tools such as wearable devices, smart home technologies, and mobile applications were highlighted for their ability to enable timely interventions while safeguarding patient privacy and autonomy. The roundtable also addressed the critical importance of personalization in AI-driven healthcare, emphasizing the balance needed between tailoring systems to individual patients and ensuring fairness, equity, and privacy across diverse populations. Discussions underscored the challenges posed by bias in healthcare data, the necessity of developing context-aware systems capable of addressing disparities, and the ethical responsibility to prioritize data privacy and transparent practices. These conversations highlighted both the immense potential and inherent challenges of integrating Social AI into healthcare, as participants envisioned a future where AI not only enhances our understanding of health but also drives more inclusive, accessible, and secure care.
\subsubsection{Human Behavior Modeling for Mental Health}


The discussion began with an example application focused on understanding textual communications between partners to track the frequency of conflicts.  Approaches brought up for modeling the mental health  in this context include calculating manually defined and statistically derived metadata features, extracting text embeddings using pretrained embedding models, as well as using modern large language models. Traditional ML approaches and LLMs could be used in conjunction to address the overall goal.

One persistent challenge in this area is inferring informative states that cannot be directly measured. While conditions like diabetes offer measurable indicators, mental health presents more difficulty in identifying clear, informative measures. This uncertainty underscores the challenge of determining what can be measured to accurately assess mental health, making it a key issue in the development of Social AI tools for healthcare applications.

\subsubsection{AI and Chatbots for Mental Health Support}

AI models, such as ChatGPT, are increasingly being used informally as therapeutic tools, where individuals engage in conversations to express frustrations and emotional experiences.  These interactions often elicit supportive and empathetic responses from the AI, which may seem overly idealized compared to human responses. While might be perceived as unusual initially, this practice highlights the non-judgmental nature of AI, which may appeal to users seeking a space free from human judgment. This raises questions about the potential role of AI in mental health support, especially in terms of its emotional neutrality and its capacity to provide a comforting, non-critical outlet for users.


While AI models like ChatGPT can provide supportive responses, they face limitations in recognizing and addressing urgent or complex situations. For example, if a patient experiences experiences financial misconduct, a human therapist might offer practical guidance, such as advising legal action. In contrast, AI may respond with general empathy and suggest continuing the conversation, failing to recognize the immediate need for legal intervention or other urgent support. 
This issue is compounded by safety concerns, as research has shown that AI models often struggle to follow instructions, such as avoiding trauma-triggering topics, which could lead to harmful outcomes, particularly in high-stakes mental health contexts.  This raises questions about their reliability and safety in emergency contexts.


\subsubsection{Real-Time and Longitudinal Monitoring through Wearable Devices}

Stress detection is an example application in the field of wearables for mental health. In such studies, participants wear devices that recorded physiological and accelerometry data. An algorithm could be used to predict stress levels, followed by a prompt asking participants to self-report their stress levels, providing a ground truth for validation. This approach reflects current practices in short-term mental health prediction, using wearable signals in conjunction with survey prompts to assess mental health. Incorporating long-term therapist notes into this data could provide valuable insights for further research. For example, the long-term clinical information could be a useful context for short-term predictions.


The discussion also highlighted the scarsity for long-termed data, especially in mental health (as it is not typically included in EHR data), along with the trade-off between eliciting accurate ground truth from the user and avoiding excessive prompting. While it is essential to prompt the user sufficiently to gather reliable data, over-frequent prompts may lead to participant fatigue or reduced engagement. On the other hand, insufficient prompting can result in a substantial amount of missing data, which may undermine the quality and completeness of the dataset. One approach to address this challenge is to utilize large language models for annotating and verifying the data, rather than directly labeling it.

Ethical concerns were also raised about companies mining user data without adequate consent, emphasizing the need for transparent data practices and privacy protection. Additionally, comparisons have been drawn between A/B testing and clinical trials, with A/B testing as a more flexible, real-world approach to experimentation, while clinical trials are held to stricter ethical and regulatory standards due to their focus on human health and well-being. It remains a key question how to balance these ethical considerations and practices when developing Social AI tools for healthcare applications.

\subsubsection{Personalized Healthcare through Social AI}

Personalization is a cornerstone of effective healthcare, and Social AI has the potential to tailor interventions to individual patients by considering their unique social, cultural, and behavioral contexts. Social AI tools can adapt to diverse populations by generating context-aware responses that align with patients' cultural norms and preferences. For instance, personalized health chatbots, such as those trained on individual user data, can provide consistent and long-term support, acting as virtual companions. These systems, designed to adapt to user behavior over time, exemplify the concept of ``personalized digital health companions," which remain relevant and effective for years.

One of the key challenges in personalization is balancing specificity with generalization. While personalized models can offer targeted recommendations by learning from an individual’s history, insights from similar patients can improve predictions and interventions for new users. This requires careful evaluation of the differences between patients and the identification of shared behavioral or contextual patterns. Context-aware models, designed to handle specific nuances of patient groups, are essential to addressing disparities in healthcare.

However, personalization introduces ethical challenges, such as ensuring predictive fairness across different demographic groups. Disparities in healthcare data can lead to biases in model performance, necessitating robust evaluation to guarantee equitable outcomes. Context-aware approaches, combined with rigorous testing across diverse populations, are critical to ensuring that personalized interventions benefit all patients without perpetuating inequities. Balancing privacy concerns with the need for individual data remains another challenge, highlighting the importance of transparent data practices and ethical guidelines in the development of Social AI tools.

