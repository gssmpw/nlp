\subsection{Clinician-AI Interaction}

\paragraph{Subtopic:} What are the primary use cases of AI tools in clinicians’ work? What are challenges, limitations, and concerns of adopting AI tools in clinical workflows? What are the lessons learned from evaluations of Clinician-AI interactions? 

\paragraph{Chairs:}
\textit{Shannon McWeeney, Yuan Pu, and Shannon Zejiang Shen}

\paragraph{Background:} 
The development of many clinical AI tools is predominantly driven by the model creators: the AI researchers identify the underlying machine learning problem and build competent models with good accuracy in such tasks.
However, it is often an afterthought how clinicians can interact and collaborate with these powerful but imperfect AI models in practice, and the efficacy of such AI systems is often evaluated in isolation from clinical workflows ~\citep{10.1145/3290605.3300468}.
In this roundtable discussion, we examine clinician-AI interactions in real-world deployments. 
With a cross-disciplinary audience of around 10 clinicians and 20 computer science/AI researchers, we identify both gaps and opportunities for building effective human-AI collaboration systems in clinical settings.

\paragraph{Discussion:}
Several key themes emerged during the discussion. 

\subsubsection{Use Cases of AI in Clinical Workflow}

A wide range of AI support is developed across different stages of clinical practice, from streamlining operational workflows to supporting complex clinical decisions ~\citep{10.1145/3582430}. 
The participants discussed several imminent AI applications for automating routine tasks like patient check-ins, prescription management, and payment processing. 
Beyond these operational improvements, AI systems have demonstrated promising capabilities in information synthesis, for example generating patient history summaries ~\citep{keszthelyi2023patient} and explainable X-ray reports ~\citep{DEPERLIOGLU2022152}. 
Moreover, recent research shared during the discussion has explored both AI-powered diagnostic predictions ~\citep{10.1001/jama.2023.22295} and LLM-based chatbots that help interpret risk scores and facilitate access to relevant medical guidelines ~\citep{chan2023assessingusabilitygutgptsimulation, 10.1145/3613904.3642024}.

% Such models can interact with clinicians in three distinct modes: 
Different modes of Clinician-AI interactions were discussed: ambient, passive, and proactive. Ambient systems, such as automated scribing tools, work continuously in the background to capture and organize patient-clinician interactions, freeing healthcare providers to focus fully on patient care ~\citep{doi:10.1056/CAT.23.0404}. Passive support comes in the form of on-demand tools like chatbots, which clinicians can consult to help navigate complex medical records and surface critical information for decision-making. The third category, proactive support, represents systems that actively monitor clinical workflows and autonomously surface relevant information and recommendations to clinicians~\citep{jiang2023conceptualizing}, enhancing their work without adding cognitive burden.

% Throughout the discussion, roundtable participants highlighted several use cases of AI in clinician interactions. Two clinician-AI interaction studies were shared, focusing on AI's role in clinical decision support. One study explored the use of AI-generated diagnostic predictions ({\color{red} TODO}: cite Sarah's work), while another examined chatbots powered by large language models (LLMs) that explain risk scores and retrieve relevant medical guidelines ({\color{red} TODO}: cite Yuan's work).  

% Beyond these studies, participants reflected on broader trends in AI applications for healthcare. Operational improvements emerged as some of the most well-received AI applications in hospitals. Examples include tools for patient check-ins, automatic prescription generation, and payment processing, which streamline workflows and enhance efficiency. These applications demonstrate the potential for AI to address routine administrative tasks, allowing clinicians to dedicate more time to direct patient care.

% AI's potential to summarize and interpret clinical data was another key focus. For instance, LLMs have been used to generate patient history summaries in projects at institutions like Massachusetts General Hospital ({\color{red} TODO}: cite???), where clinicians evaluated their accuracy and utility. Participants noted that such tools could reduce the time spent reviewing voluminous medical records, making critical information more accessible for decision-making.

% Participants also discussed the value of AI tools for ambient scribing, which could help reduce documentation burdens during clinical encounters. These tools record and organize interactions between patients and clinicians, producing comprehensive notes while freeing clinicians to focus on their patients. Additionally, the integration of multimodal data, such as bio-signals and imaging, was identified as a promising approach to provide comprehensive patient representations. This capability could improve accessibility to valuable insights across various members of the care team, including nurses and resident physicians.


\subsubsection{Challenges} 
The roundtable discussion explored both the challenges and opportunities in fostering effective clinician-AI interactions. Participants highlighted clinicians' mistrust of AI and their misunderstanding of its applications as significant barriers. They emphasized that addressing these issues will require collaboration and thoughtful design to build trust and enhance the integration of AI into clinical practice.

\paragraph{Socio-technological challenges in practical AI deployment} 
Roundtable participants frequently mentioned that clinicians often show reluctance to adopt AI predictions, even in scenarios where AI demonstrates high accuracy. 
For instance, in a study shared during the session, clinicians disregarded 20\% of AI-generated predictions, even when those predictions were 100\% accurate ~\citep{10.1001/jama.2023.22295}. 
The group suggested that this hesitancy may stem from clinicians' strong preference for relying on their professional judgment developed through extensive training and hands-on experience. 
Resistance to revising decisions based on AI recommendations was further linked to cognitive dissonance. 
As several participants observed, healthcare professionals often experience notable discomfort when faced with contradictory opinions, whether from human peers or AI systems. 
Accepting such recommendations may be perceived as admitting an error, creating tension with a clinician's professional self-image and confidence. 
This psychological barrier can lead to an unconscious dismissal of the AI's inputs, even when the evidence strongly supports them.

\paragraph{Trustworthiness of the model outputs} 

Concerns about accuracy, accountability, and ethics also undermine trust ~\citep{HENGSTLER2016105}. 
Clinicians frequently perceive AI systems as ``black boxes,'' making it difficult to understand and trust their recommendations. 
Ethical issues, such as potential biases in AI models and the privacy of patient data, further complicate this trust dynamic.

For example, systems that use heatmaps (or activation maps) to explain model predictions for medical imaging often fail to present reasoning in an intuitive and easy-to-understand way for clinicians without AI background. 
Clinicians often develop their own interpretations of the outputs, increasing their cognitive burden and reducing confidence in the tools.
Participants also notes that it is very hard to identify the occasional errors in the AI ambient scribing system, thus making it less practically useful.
In such cases, participants emphasized that clinicians worry about who will be held responsible for decisions influenced by AI, particularly in high-stakes scenarios. 

% Roundtable participants frequently mentioned that clinicians often show reluctance to adopt AI predictions, even in scenarios where AI demonstrates high accuracy. For instance, in a study shared during the session, clinicians disregarded 20\% of AI-generated predictions, even when those predictions were 100\% accurate. ({\color{red} TODO}: cite Sarah's work) The group suggested that this hesitancy may stem from clinicians' strong preference for relying on their professional judgment developed through extensive training and hands-on experience. Resistance to revising decisions based on AI recommendations was further linked to cognitive dissonance. As several roundtable participants observed, healthcare professionals often experience notable discomfort when faced with contradictory opinions, whether from human peers or AI systems. Accepting such recommendations may be perceived as admitting an error, creating tension with a clinician’s professional self-image and confidence. This psychological barrier can lead to an unconscious dismissal of the AI’s inputs, even when the evidence strongly supports them. It is worth noting that in real-world implementations, AI systems rarely maintain consistent superiority over clinical experts. These systems often exhibit limitations such as hallucinations and errors that experienced clinicians can readily identify. Participants emphasized that once clinicians encounter such failure cases, their trust in the system significantly diminishes. 

% Concerns about accountability and ethics also undermine trust. Participants emphasized that clinicians worry about who will be held responsible for decisions influenced by AI, particularly in high-stakes scenarios. Ethical issues, such as potential biases in AI models and the privacy of patient data, further complicate this trust dynamic.

% Misunderstanding of AI tools creates additional barriers, often leading to improper use or rejection. Clinicians frequently perceive AI systems as “black boxes,” making it difficult to trust or understand their recommendations. For example, heat maps used in medical imaging are intended to highlight areas of interest, but they often fail to present reasoning in an intuitive way. This leaves clinicians to develop their own interpretations of the outputs, increasing their cognitive burden and reducing confidence in the tools.

\paragraph{Disconnect from real-world case complexities} Participants also pointed out the disconnect between AI models and real-world patient dynamics. 
For example, clinicians expressed concern that real-time factors like a patient’s emotional state are often overlooked and not adequately represented in the training data used to develop AI systems ~\citep{chan2023assessingusabilitygutgptsimulation}. 
More broadly, some participants are not certain about whether synthetic datasets (used in AI deployment) can accurately reflect the variability and complexity of real-world scenarios. 
This gap limits the relevance of AI recommendations in clinical settings and contributes to skepticism. 

\subsubsection{Opportunities} 

\paragraph{Towards holistic evaluation at the model development stage} 
Going beyond only evaluating competence of the models, participants emphasized the need to capture multiple dimensions that reflect real-world user needs in clinical settings. 
For example, when evaluating AI generated X-ray reports, rather than just assessing the ``accuracy'' through lexical overlap between the generation and reference doctor's writing, one should consider metrics that directly address physicians' information needs. 
These include assessments of readability, completeness of critical findings, and the cognitive effort required to locate decision-relevant information.
Through collecting first-hand subjective feedback from clinicians users, the developers can better understand potential challenges and limitations of the AI systems in real-world settings, and iteratively refine the models to better align with clinical workflows.


\paragraph{Rethinking the mechanisms of using AI} 
Participants also suggested a reframing of how AI systems are positioned within clinical use cases. % - not as omnipotent oracles that replace clinical judgment, but as help assistants that can augment and support clinical decision-making.
For example, rather than providing definitive answers, AI systems could be designed to prompt clinicians to engage in deeper analytical thinking (system 2 reasoning) -- they can prompt the clinicians to think step by step and highlight potentially overlooked patterns or suggesting alternative diagnostic considerations.
This approach aligns with how clinicians naturally collaborate with colleagues, where discussion and collective reasoning often lead to better outcomes than isolated decision-making.
The group also mentioned that incorporating AI education into medical training can help future clinicians develop a nuanced understanding of both the capabilities and limitations of AI tools.
