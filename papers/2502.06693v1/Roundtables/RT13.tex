\subsection{Personalization and Heterogeneity in Medicine}

\paragraph{Subtopic:} 
How can machine learning models effectively capture patient heterogeneity while ensuring personalized treatment recommendations?  What are the key challenges in balancing personalization with generalizability of solutions? How can we assess the generalizability/transportability of models that make personalized treatment recommendations? 

\paragraph{Chairs:}
\textit{Mohsen Sadatsafavi, Yuan Xia, and Sazan Mahbub}

\paragraph{Background:}

With the increasing availability of vast medical datasets, machine learning models offer great potential for delivering on the promise of Precision Medicine: enhancing clinical care by tailoring treatments to individual patient characteristics \citep{krishnan2023artificial}.

However, there are significant challenges along the way. Population heterogeneity threatens the transportability of models from one setting to another. Populations also change over time, leading to temporal shifts (aka calibration drift \citep{davis2017calibration}). Change in model performance when transported to a new setting may differ across subgroups, which can remain hidden and can aggravate existing disparities in care. Emerging treatments or changes in guidelines can disrupt predictions. One such disruptive change can be the adoption of the model itself, creating feedback loops between the model's adoption and the metrics used to evaluate its performance. Additionally, treatment assignments in the real world are not random and are influenced by factors such as the perception of disease severity, which are not always captured in the data \citep{kyriacou2016confounding}. Such 'confounding by disease severity', where patients with severe conditions are more likely to receive the treatment, can lead models to mistakenly associate effective treatments with poorer outcomes \citep{xia2024evaluating}.

Randomized controlled trials (RCTs) are the gold standard for evidence on treatment effectiveness in contemporary medicine and often a prerequisite for the approval of new treatments. However, RCTs are typically designed to estimate average treatment effect, while personalization focuses on conditional effect, whose estimation requires a significantly larger sample \citep{curth2024using}. Further, RCTs often have short follow-up periods, restrictive inclusion criteria, and strict study protocols that poorly reflect real-world practice. These limitations challenge many modern machine learning methods that rely on large, representative data.

The desire to create generalizable models might force developers to focus on features that are deemed to be resilient against distribution shifts, striking a balance between local accuracy and transportability \citep{subbaswamy2019preventingfailuresdatasetshift}. A similar trade-off might exist with respect to model explainability. Explainable models are more likely to be accepted by patients and care providers \citep{holzinger2019causability}, and this might incentivize model developers to focus on salient features. 

This round table was attended by individuals with interest in machine learning, causal inference, health policy, epidemiology, and biostatistics. This provided an opportunity to discuss, from diverse perspectives, how generalizability should be defined and measured, the challenges models face in predicting treatment effectiveness, and the trade-offs between making locally accurate predictions versus generalizable ones.

\paragraph{Discussion:}

To have a common ground for discussions, we decided to focus on machine learning  models that generate quantitative predictions about clinical outcomes. An early topic was the difference between predicting outcome risk versus treatment benefit. The former is akin to fitting a conditional mean function. The latter, on the other hand, is a `counter-factual' problem \citep{prosperi2020causal}. The group agreed that typical data that are used to train models are more suitable for predicting risk than predicting treatment benefit, due to the problem of unmeasured confounding (the summary from the Causality round table offers nuanced discussions). We noted that heterogeneity in treatment effect is scale-specific. The absence of treatment-by-feature interaction in one scale (e.g., relative risk) indicates its presence in other scales (e.g., absolute risk difference) \citep{greenland2009interactions}. It was mentioned that a common current practice is to use models to predict risk, to which the average treatment effect (e.g., relative risk reduction) from RCTs is applied to estimate the absolute risk reduction for a given patient \citep{Kent2020}. This approach inevitably ignores treatment-by-feature interactions on the relative scale. Participants noted that advances in causal machine learning based on both observational and RCT data have the potential to address these challenges \citep{feuerriegel2024causal}.

We debated whether certain models are more generalizable than others. Participants agreed that models are different in their generalizability, noting that generalizabiltiy can be formally considered at the development stage \citep{subbaswamy2019preventingfailuresdatasetshift}. However, it was mentioned that generalizability is a feature of both the domain (clinical condition of interest) and the model. For example, models that detect lung cancer on computed tomography images might be more transportable than models that detect individuals at high risk of self-harm based on their conversations with emergency staff. The latter is more likely to be affected by social norms, variations in practice standards, and so on. Participants agreed that clinical expertise in the domain of interest is key in the assessment of model generaliazability. One participant suggested explicit modeling of population heterogeneity for a model via network meta-analysis methods, which enables making probabilistic statements about the performance of the model in a new population \citep{Phillippo2020NMA}.

On the relationship between personalization and generalizability, we discussed that they do not necessarily move in opposite directions (indeed, a perfect oracle will make ultimately accurate and generalizable predictions). However, we acknowledged that different features in the data might vary in their susceptibility to distribution or domain shifts, independently of their predictive power. Participants were asked about the notion that high-dimensional models might be more generalizable, as such models might capture variables that are responsible for differences across populations. A good example of this concept is foundation models \citep{bommasani2021opportunities}. These models are trained on vast amounts of data spanning one or multiple domains, enabling them to learn generalizable priors. As a result, foundation models are expected to exhibit broad applicability, demonstrating the ability to generalize across diverse tasks and settings. However, this was challenged by the counter-argument that such models also have a tendency to fit to local features (or miss local features in a new setting that determine the outcome), leading to significant performance degradation when transported~\citep{kernbach2022foundations, tirumala2022memorization}.

We made a distinction between generalizbility of models in terms of discrimination and calibration. Research in clinical prediction modeling has shown that when models are transported to new populations, their calibration (e.g., change in calibration slope) is often substantially affected, more so than their discrimination (e.g., change in AUC) \citep{van2019calibration, gulati2022generalizability}. This led to the discussion that models can be more generalizable when they are used for ranking (e.g., which three hospitalized COVID-19 patients are at the highest risk of deterioration and should use the three ventilators in the ward?). In contrast, when numerical predictions are communicated (e.g., your risk of heart attacks in the next 10 years), much emphasis should be placed on calibration. 

Towards the end, we discussed different ways models can be evaluated (validated) in a new population. While we universally agreed that models need to be tested before deployment, we also noted that for models that hold promise, waiting for the results of the validation study can itself cause opportunity loss (especially for predictions over long time-horizons, where a validation study might take years). As such, in some circumstances, one might conditionally deploy a model while learning about its performance. Stepped wedge designs were mentioned to be suitable for simultaneous implementation and learning \citep{kappen2018evaluating}. A related point was made on the extent one should fine-tune a model based on available information before a full-on validation study, such as accounting for differences in average outcome risk between training and target populations \citep{sadatsafavi2022marginal}. However, there were concerns about the misuse of such `blanket' corrections. 

Overall, participants highlighted the interdisciplinary expertise required for developing and deploying models that are precise and generalizabile, and at the same time acceptable to patients, providers, and regulators. There was a broad consensus that this is a context-specific task, and any one-size-fits-all recommendation might not itself be generalizable.