\section{Method}
\subsection{Preliminaries}

\paragraph{Central Dogma links islands.}
The \textit{central dogma} of molecular biology describes the flow of genetic information from DNA $\rightarrow$ RNA $\rightarrow$ Protein. DNA is transcribed into RNA, and RNA is subsequently translated into protein. This process ensures that the information encoded in the genome can be expressed to perform diverse biological functions. Let \( \mathcal{D} \subseteq \{\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}\}^*\) be the space of DNA sequences, and let \(\mathcal{R} \subseteq \{\texttt{A}, \texttt{U}, \texttt{C}, \texttt{G}\}^*\) be the space of RNA sequences. For proteins, denote \(\mathcal{P} \subseteq \Sigma^*\), where \(\Sigma\) is the amino acid alphabet (\textit{e.g.}, \(\Sigma = \{ \texttt{Ala}, \texttt{Arg}, \texttt{Asn}, ...\}\)). The central dogma stipulates two fundamental mappings:
\begin{align}
    T_{\mathrm{transcribe}}: \mathcal{D} \to \mathcal{R}, \quad
    T_{\mathrm{translate}}: \mathcal{R} \to \mathcal{P}.
\end{align}
By composition, one obtains \(T_{\mathrm{translate}} \circ T_{\mathrm{transcribe}}: \mathcal{D} \to \mathcal{P}\). In practice, DNA $\rightarrow$ RNA $\rightarrow$ Protein is not a strict one-to-one mapping due to splicing, alternative start sites, etc. However, these transformations offer critical biological correspondences that can link data from different omics sources.
\textit{Why it matters}: By grounding sequences in DNA (\( \mathbf{x} \in \mathcal{D} \)), we naturally align each transcript (\( \mathbf{y} \in \mathcal{R} \)) and its resultant protein (\( \mathbf{z} \in \mathcal{P} \)) within a unified coordinate system. This approach not only enlarges the effective dataset via cross-modality but also preserves mechanistic interpretability, as variations in \(\mathbf{x}\) manifest as changes in \(\mathbf{y}\) and \(\mathbf{z}\).

\paragraph{The bi-directional hybrid model is more efficient.}
DNA has three notable properties:
(1) Length: A chromosome-scale DNA sequence \(\mathbf{x} = (x_1, x_2, \ldots, x_n)\) can reach millions (or more) of base pairs (bp).
(2) Double-Helix Complementarity: Each position \(x_i \in \{\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}\}\) pairs with a complementary base on the opposite strand.
(3) Distributed Regulatory Elements: Important functional sites (\textit{e.g.}, promoters, enhancers, splicing signals) can be located in different regions (CDS or nCDS). In summary,  while Transformer-based architectures exhibit strong representational power, their attention mechanism scales with \(\mathcal{O}(n^2)\) complexity, making it difficult to handle sequences of length \(n\gg 10^4\). Linear or sparse approximations (\textit{e.g.}, Mamba) reduce complexity but often sacrifice representational fidelity for complex biological contexts. Hence, the recent efficient hybrid strategy~\cite{yang2025deltanet,yang2024gateddelta,icml2024chela,ijcai2024longvq} should be considered.
We now formally describe how we unify DNA, RNA, and protein sources into \textit{nucleotides}.

\begin{figure}[b!]
    \vspace{-0.5em}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/fig_packing.pdf}
    \vspace{-2.0em}
    \caption{\textbf{Data Sampling Pipeline}. During pre-training, we sample the DNA sequences and the pair of CDS and amino acids from the DNA dataset and the DNA-AA paring dataset. Since the DNA sequence sampled from the RefSeq is much longer than the CDS, we predefined the max sequence length (\textit{e.g.}, 8k) for each sample. As for the DNA sequence, we employ truncation and padding. As for the CDS and its corresponding amino acids, we apply the packing strategy~\cite{Warner2024ModernBERT} to sample with several CDSs.
    }
    \label{fig:data_packing}
    \vspace{-0.5em}
\end{figure}

\subsection{Data Pipeline for Life-Code}
\paragraph{Unified to nucleotides.} Given an RNA sequence \(\mathbf{y} \in \mathcal{R}\) or a protein sequence \(\mathbf{z} \in \mathcal{P}\), we map them back to DNA-level tokens. Specifically: For RNA, assume we have the corresponding genomic DNA region (\textit{e.g.}, from reference genome or aligned reads) and apply \(\mathbf{y} = T_{\mathrm{transcribe}}(\mathbf{x})\) in reverse to identify the original \(\mathbf{x} \in \mathcal{D}\). For protein, we leverage the standard codon translation table. Each amino acid \(z_j \in \Sigma\) typically maps to one or more codons in \(\{\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}\}^3\). We choose the canonical codon (or the most frequent codon) for each amino acid, yielding a surjective function
    \(g: \Sigma \to \{\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}\}^3.\)
Thus, a protein of length \(L\) becomes a DNA-like sequence of length \(3L\).

\paragraph{Data sampling.}
Following GenBank~\cite{benson2012genbank} curation, we sample from the \texttt{RefSeq} database, truncating long genomic segments and padding shorter ones. For \textit{CDS--Amino Acid} pairs, we extract annotated CDS records and their corresponding protein sequences, then pack multiple short segments into a single training sample with special separators. See Figure~\ref{fig:data_packing} for a schematic illustration.
This approach enables us to efficiently process both longer genomic segments and multiple shorter CDS-AA pairs in a unified framework. By balancing \emph{truncation/padding} for DNA sequences and \emph{packing} for CDS segments, we ensure that each batch contains a diverse mixture of long-range genomic context and coding information, thereby maximizing the model’s exposure to various biological signals.

% \subsection{Tokenizer Training.} 
% After mapping proteins and RNA to DNA-like sequences, all data can be treated as elements of \(\{\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}\}^*\). To reduce the length of the protein-mapped sequences (which can balloon by a factor of 3), we apply a stride-3 convolution to compress each triplet:
% \[
% \mathbf{u}_j = \mathrm{Conv}(\mathbf{x}_{3j+1:3j+3}),
% \]
% where \(\mathbf{u}_j\) is a compressed token embedding. Concretely, if the mapped protein segment has length \(3L\), this yields an embedding sequence of length \(L\). Finally, we pass each token embedding \(\mathbf{u}_j\) through a small MLP:
% \[
% \mathbf{e}_j = \mathrm{MLP}(\mathbf{u}_j),
% \]
% resulting in the final token embedding \(\mathbf{e}_j \in \mathbb{R}^d\).  
% Thus, all data—DNA, RNA, or protein—can be \textit{unified} into a single nucleotide-level embedding space.

\begin{figure}[b!]
    \vspace{-0.5em}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/fig_tokenizer.pdf}
    \vspace{-2.25em}
    \caption{\textbf{Life-Code Tokenizer}. To model the interactions among DNA, RNA, and Amino Acids, our tokenizer takes nucleotide acids (4 words) as the inputs, then takes codons as the latent vocabulary (64 words), and translates them to Amino Acids (20 words) as the outputs, which could be pre-trained by (a) nucleotide masked modeling and (b) CDS to Amino Acid translation.
    }
    \label{fig:tokenizer}
    \vspace{-0.5em}
\end{figure}

\begin{figure*}[t!]
    % \vspace{-0.25em}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/fig_pretraining.pdf}
    \vspace{-2.25em}
    \caption{\textbf{Illustration of Life-Code Encoder pre-training pipeline}. We model the long sequence by the hybrid encoder as a mixture of RNN blocks (DeltaNet) and self-attention blocks. With pre-trained tokenizers and de-tokenizers, we consider three types of pre-training tasks: (a) the nucleotide masked modeling for contextual information, (b) the cDNA-AA translation for the central dogma (with span masks), and (c) the knowledge distillation (KD) with pre-trained protein LM. Note that \red{red lines} indicates the learning objectives.
    }
    \label{fig:lcode_pretraining}
    \vspace{-1.0em}
\end{figure*}

\subsection{Life-Code Tokenizer Training}

After mapping all sequences (DNA, RNA, and protein) to a unified DNA-like form, each sample can be viewed as a string in \(\{\texttt{A}, \texttt{T}, \texttt{C}, \texttt{G}\}^*\). However, to effectively capture both nucleotide-level and codon-level patterns, as shown in Figure~\ref{fig:tokenizer}, our tokenizer incorporates a \textit{two-task} training paradigm: (\emph{i}) masked language modeling for nucleotide reconstruction and (\emph{ii}) protein translation for CDS.

\paragraph{Tokenization with codon-level embeddings.}
Consider a raw DNA segment of length \(3L\). We first apply a 1-d convolution (kernel size \(=3\)) together with an ``unfold'' operation (stride \(=3\)) to transform each consecutive triplet \((x_{3j+1}, x_{3j+2}, x_{3j+3})\) into a codon-like representation:
\[
\mathbf{e}_j \;=\; \mathrm{Conv1d}(\,x_{3j+1:3j+3}\,).
\]
This step effectively condenses triplets---\emph{codons}---into a more compact embedding sequence \(\{\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_L\} \in \mathbb{R}^{L\times 3d}\). We apply a small linear projection or MLP to each \(\mathbf{e}_j \in \mathbb{R}^{d}\), mapping it into the token embedding space.

\paragraph{Dual pre-training.}
\label{sec:tokenizer_tasks}
We jointly pre-train this tokenizer on two objectives:
(1) Nucleotide MLM (Reconstruction).
    For each codon-embedded sequence, we randomly mask a subset of tokens and train the model to reconstruct the original nucleotides. Technically, we design a \textit{DNA De-Tokenizer} that applies a \texttt{Fold} (stride = 3) and an inverse convolution (\texttt{De-Conv1d}) to recover the three-base codon, thereby reconstructing the full-length of DNA sequences. Formally, if \(\widehat{\mathbf{x}}_{3j+1:3j+3}\) is the predicted codon for position \(j\), the loss is:
    \begin{equation}
        \label{eq:loss_MLM}
        \mathcal{L}_{\mathrm{MLM}} \;=\; 
        \sum_{j=1}^L 
        -\log p\bigl(\mathbf{x}_{3j+1:3j+3} \,\big\vert\, \widetilde{\mathbf{x}}\bigr),
    \end{equation}
    where \(\widetilde{\mathbf{x}}\) is the masked sequence. This ensures the tokenizer learns robust nucleotide representations that can reconstruct the original sequence.
(2) CDS-to-Protein Translation.
    For coding regions, each codon corresponds to an amino acid (\(\mathbf{z}_j \in \Sigma\)). We thus include the \textit{Amino Acid Translator} module, a two-layer MLP layer, that looks up the genetic code table and learns to predict \(\mathbf{z}_j\) given \(\mathbf{e}_j\):
    \begin{equation}
        \label{eq:loss_trans}
        \mathcal{L}_{\mathrm{Trans}} \;=\; 
        \sum_{j=1}^L 
        -\log\,p\bigl(\mathbf{z}_j \,\big\vert\, \mathbf{e}_j\bigr).
    \end{equation}
    By training on paired CDS--amino-acid data, the tokenizer acquires biologically grounded representations, effectively bridging codon-level features to protein-level semantics.

\subsection{Bi-Directional Hybrid Model for Long Sequence}
\label{sec:hybrid-model}

We describe the \textbf{Life-Code Encoder}, which takes the tokenized embeddings as input and models DNA sequences efficiently, respecting double-strand complementarity.

\vspace{-0.25em}
\paragraph{Bi-directional input.}
To use the double-helix property as~\citet{icml2024caduceus}, we split the embeddings \(\mathbf{E} = (\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n)\) into two parts in the \textit{feature} dimension:
\[
(\mathbf{E}^{(+)},\ \mathbf{E}^{(-)}) = \mathrm{Split}(\mathbf{E}),
\]
where \(\mathbf{E}^{(+)}\) represents the forward strand embedding, and \(\mathbf{E}^{(-)}\) represents the reverse (complementary) strand. We process these two strands in parallel using the same model weights or parameter-sharing scheme \(f_\theta\):
\begin{align*}
    \mathbf{H}^{(+)} = f_\theta(\mathbf{E}^{(+)}), \quad
    \mathbf{H}^{(-)} = f_\theta(\mathrm{Reverse}(\mathbf{E}^{(-)})).
\end{align*}
Finally, we concatenate or fuse the representations:
\[
\mathbf{H} = [\mathbf{H}^{(+)},\ \mathbf{H}^{(-)}],
\]
yielding a representation that encodes both the forward and reverse complement features, where $\mathbf{H} \in \mathbb{R}^{D}$.

\paragraph{Hybrid Token Mixer.}
Inspired by recent advances in EVO-style architectures~\cite{nguyen2024evo} and Gated Delta Networks~(GDN)~\cite{yang2024gateddelta}, we design the encoder in Life-Code as the linear-attention Gated DeltaNet \emph{mixed} with a small proportion of standard multi-head self-attention (MHSA). Concretely, for every twelve encoder layers, eleven employ the GDN update rule---offering linear or near-linear complexity via kernel-based reordering---while one layer applies MHSA. The GDN layer relies on a \emph{delta update} to maintain memory, reducing the quadratic overhead of naive Transformers. Meanwhile, the \emph{one} full-attention layer provides additional global context, which is crucial for capturing fine-grained long-range dependencies. This \textit{11\,{:}\,1} ratio of GDN to dense layers significantly boosts computational efficiency (compared to a pure \(\mathcal{O}(n^2)\) Transformer) while preserving model expressiveness and long-distance modeling capacity.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{figs/fig_emprical.pdf}
    \vspace{-2.0em}
    \caption{\textbf{Empirical Analysis of Life-Code Tokenizer}.
    \textbf{Left:} \textit{Codon Usage Bias} in Life-Code Tokenizer across four representative species---\textit{E.~coli}, \textit{S.~cerevisiae} (yeast), \textit{D.~melanogaster} (fruit fly), and \textit{H.~sapiens} (human)---illustrating variations in codon frequency (\%) for amino acids. The pastel strips highlight codons belonging to the same amino acid group. 
    \textbf{Right:} \textit{t-SNE visualization} of learned codon embeddings in Life-Code Tokenizer, where codons that translate to the same or biochemically similar amino acids cluster together (\textit{e.g.}, hydrophobic or charged groups), and stop codons (black crosses) form a distinct region. These views jointly demonstrate that Life-Code encodes both species-dependent codon usage preferences and biologically meaningful codon--amino acid relationships.
    }
    \vspace{-1.0em}
    \label{fig:codon}
\end{figure*}

\vspace{-0.25em}
\paragraph{Encoder Pre-training.}
Once the decoder and translator of our tokenizer weights are fixed (see Section~\ref{sec:tokenizer_tasks}), we train the \emph{Life-Code Encoder} (Figure~\ref{fig:lcode_pretraining}) on three tasks: (1) Masked DNA Reconstruction (MLM), (2) CDS-to-Amino-Acid Translation, and (3) Token-Level Knowledge Distillation (KD) from a pretrained Protein LM. The first two objectives match those used to pre-train the tokenizer but are now applied to the encoder output. Specifically, the encoder generates codon embeddings that a \emph{DNA De-tokenizer} can reconstruct (MLM) and that an \emph{Amino Acid Translator} can convert to protein residues (Translation). 

For the \textbf{distillation} step, we replace the previous squared-error formulation with a \emph{token-wise KL loss} on the predicted distributions, which we have empirically found to be more robust (\textit{e.g.}, no negative log issues). Inspired by the DINO framework~\citep{oquab2024dinov2}, we \emph{decode} the encoder’s final protein sequence embeddings to protein structure embeddings a \textit{Protein Decoder} $\mathcal{D}$ to produce a per-token distribution aligned with the teacher PLM like ESM2~\citep{lin2022ESM2}. Formally, let \(P_\theta(\mathcal{D} (\mathbf{h}_j^\mathrm{LifeCode})) \in \mathbb{R}^{D'}\) and \(P_{\theta'}(\mathbf{h}_j^\mathrm{PLM}) \in \mathbb{R}^{D'}\) be the predicted distributions for token \(j\) from our model and the teacher, where the \textit{Decoder} can be a two-layer-MLP with \(D \rightarrow 2D \rightarrow D'\). Then, the \emph{alignment} or KD loss is defined as:
\begin{equation}
    \label{eq:tokenwise_kl}
    \mathcal{L}_\mathrm{KD} \;=\; \sum_{j=1}^{L} 
    -P_{\theta'}\bigl(\mathbf{h}_j^\mathrm{PLM}\bigr)^\top \log P_{\theta}\bigl(\mathcal{D}( \mathbf{h}_j^\mathrm{LifeCode}) \bigr).
\end{equation}
We combine these three losses using a \textit{log-sum} multi-task strategy~\citep{lin2023dualmtl} so that all terms (MLM, Translation, and KD) are treated as entropy-like objectives:
\begin{equation}
    \hspace{-1.0em}
    \label{eq:loss_total}
    \log\bigl(\mathcal{L}_\mathrm{MLM} + \epsilon\bigr) 
    \;+\; \log\bigl(\mathcal{L}_\mathrm{Trans} + \epsilon\bigr)
    \;+\; \log\bigl(\mathcal{L}_\mathrm{KD} + \epsilon\bigr),
\end{equation}
where \(\epsilon\) is a small constant. This adaptive weighting prevents any single task from dominating and ensures stable optimization across various training stages. Notably, we also adopt this strategy when training the tokenizer.


% \paragraph{Encoder Pre-training.}
% Once the tokenizer weights are fixed (see Section~\ref{sec:tokenizer_tasks}), we train the \emph{Life-Code Encoder} (Figure~\ref{fig:lcode_pretraining}) on three tasks: (1) \textbf{Masked DNA Reconstruction (MLM)}, (2) \textbf{CDS-to-Amino-Acid Translation}, and (3) \textbf{Knowledge Distillation} from a pretrained Protein LM. The first two objectives match those used to pre-train the tokenizer but are now applied to the encoder output. Specifically, the encoder generates codon embeddings that a \emph{DNA De-tokenizer} can reconstruct (MLM) and that an \emph{Amino Acid Translator} can convert to protein residues (Translation). Additionally, the encoder’s final protein embeddings are aligned with PLM’s representations, \textit{e.g.}, ESM2-650M~\citep{lin2022ESM2}, via a \emph{distillation} loss to improve the protein-related representation:
% \[
% \mathcal{L}_\mathrm{KD} \;=\; \sum_{j=1}^{L} \|\mathbf{h}_j^\mathrm{(LifeCode)} - \mathbf{h}_j^\mathrm{(PLM)}\|^2.
% \]
% To balance these three tasks, we adopt a \textit{log-weighted} multi-task strategy~\cite{lin2023dualmtl}. Concretely, the total loss:
% \begin{equation}
%     \log\bigl(\mathcal{L}_\mathrm{MLM} + \epsilon\bigr) + \log\bigl(\mathcal{L}_\mathrm{Trans} + \epsilon\bigr) + \log\bigl(\mathcal{L}_\mathrm{KD} + \epsilon\bigr),
%     % \min \bigl[\log\bigl(\mathcal{L}_\mathrm{MLM} + \epsilon\bigr) \;+\; \log\bigl(\mathcal{L}_\mathrm{Trans} + \epsilon\bigr) 
%     % \;+\; \log\bigl(\mathcal{L}_\mathrm{KD} + \epsilon\bigr)\bigr],
%     \label{eq:total}
% \end{equation}
% where \(\mathcal{L}_\mathrm{MLM}, \mathcal{L}_\mathrm{Trans},\) and \(\mathcal{L}_\mathrm{KD}\) denote the reconstruction, translation, and distillation objectives, respectively, and \(\epsilon\) is a small constant (\textit{e.g.}, \(10^{-6}\)) for numerical stability. This \emph{log-sum} formulation adaptively balances learning signals, preventing any single loss from dominating early or late in training. It is worthily to focus that this strategy is also employed to train our tokenizer.

\subsection{Empirical Consequences}
\paragraph{Biological insights}
In Figure~\ref{fig:codon}, our analyses reveal that the Life-Code Tokenizer captures biologically meaningful codon relationships in two complementary ways. First, codon usage bias (left figure) indicates that different species, from \emph{E. coli} to \emph{H. sapiens}, vary markedly in their preference for certain codons—an effect traditionally tied to tRNA abundance and translational efficiency. By visualizing these preferences across amino acids, we observe how each organism clusters around specific high-frequency codons, underscoring the ecological and evolutionary factors shaping codon selection.
Second, the t-SNE plot of the tokenizer’s learned codon embeddings (right figure) demonstrates that codons translating to amino acids with similar biochemical properties (\textit{e.g.}, hydrophobic or charged) are embedded close together, while the stop codons occupy a separate region. This structural arrangement confirms that the model internalizes codon–amino acid mappings, reflecting both frequency patterns and higher-level attributes like polarity or aromaticity. Together, these findings suggest that the Life-Code Tokenizer not only memorizes sequence data but also discerns and encodes intrinsic biological signals—ranging from species-specific usage preferences to fundamental amino acid groupings.

\begin{figure}[b!]
    \vspace{-1em}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/fig_efficiency.pdf}
    \vspace{-1em}
    \caption{%
    \textbf{Training Efficiency on a Single A100 GPU.}
    We measure throughput (K t/s) at four sequence lengths–batch size settings.
    DNABERT-2 (blue) slows significantly at higher lengths and reaches an out-of-memory (OOM) error at \(16\text{K}\times2\).
    Both EVO (red) and Life-Code (green) scale more effectively, with Life-Code consistently achieving the highest throughput.
    }
    \label{fig:efficiency}
\end{figure}

\input{tabs/tab_genomic_benchmark}
\input{tabs/tab_gue}

\vspace{-1em}
\paragraph{Training efficiency}
We evaluated the throughput (thousands of tokens processed per second) of Life-Code, EVO, and DNABERT-2 on a single A100 GPU under increasing sequence lengths (with batch size adjusted accordingly). As shown in Figure~\ref{fig:efficiency}, DNABERT-2 (\blue{blue}) exhibits a steep drop-off at longer sequences and ultimately runs out of memory (OOM) at \(16\text{K}\times2\). In contrast, EVO (\red{red}) and Life-Code (\green{green}) both maintain higher throughput thanks to their hybrid attention mechanisms. Notably, Life-Code achieves the best scalability, retaining above 40K t/s even at \(8\text{K}\times4\). These results confirm that Life-Code’s efficient architecture can accommodate extensive input lengths with minimal degradation in training speed, making it well-suited for large-scale genomic modeling.



