\section{Related Works}
\paragraph{DNA Foundation Models.}
Recent years have seen rapid progress in \textit{DNA} foundation models that leverage Transformer-like architectures. Early attempts such as DNABERT~\cite{ji2021dnabert} treated genomic sequences akin to linguistic tokens, enabling pre-trained contextual representations. Follow-up work DNABERT-2~\cite{zhou2023dnabert2} expanded this paradigm by introducing more efficient training protocols and supporting \textit{multi-species} data. In parallel, The Nucleotide Transformer~\cite{NM2023NucleotideTrans} demonstrated the feasibility of scaling up Transformer architectures for human genomics. Beyond Transformers, kernel-based or hierarchical approaches emergedâ€”\textit{e.g.}, HyenaDNA~\cite{nguyen2024hyenadna}, which reduces the quadratic complexity of attention for extremely long sequences. Meanwhile, alternative designs like Caduceus~\cite{schiff2024caduceus} incorporate selective structural priors for long-range DNA modeling, and MxDNA~\cite{nips2024MXDNA} explores adaptive tokenization schemes that automatically discover suitable patterns for genomic data.
\vspace{-1em}
\paragraph{Protein and RNA Advances.}
On the \textit{protein} side, AlphaFold series~\cite{jumper2021alphafold,jumper2021alphafold2} revolutionized structure prediction, catalyzing a surge in protein-based language models such as ESM-2~\cite{lin2022ESM2}, which refines large-scale protein embeddings. Beyond proteins, RNA modeling also gained traction; for instance, \cite{shen2024rnafm} employed a language-model-based technique for accurate 3D structure prediction. These efforts underscore the trend toward specialized architectures for each biomolecule yet also highlight the desire for integrative multi-omics solutions.
\vspace{-1em}
\paragraph{Multi-Omics Modeling.}
Recent studies aim to bridge the gap between \textit{genomic} and \textit{protein} sequences, pushing beyond single-modality tasks. CD-GPT~\cite{zhu2024CDGPT} explicitly connects DNA, RNA, and proteins through the central dogma, while {BSM~\cite{xiang2024BSM} highlights the potential for small but effective models covering genes and proteins simultaneously. Evo~\cite{nguyen2024evo} similarly integrates molecular and genome-scale data with a generalized sequence-modeling approach. Closely related is LucaOne~\cite{he2024lucaone}, which advocates a unified nucleic acid and protein language for biological representation.
