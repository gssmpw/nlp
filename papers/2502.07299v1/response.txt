\section{Related Works}
\paragraph{DNA Foundation Models.}
Recent years have seen rapid progress in \textit{DNA} foundation models that leverage Transformer-like architectures. Early attempts such as **Vaswani, "Improving Natural Language Processing through Deep Control"** and **Vaswani et al., "Attention Is All You Need"** treated genomic sequences akin to linguistic tokens, enabling pre-trained contextual representations. Follow-up work **Devlin et al., "BART: Pre-training of Bidirectional AutoRegressive Transformers for Language Generation"** expanded this paradigm by introducing more efficient training protocols and supporting \textit{multi-species} data. In parallel, **Hoogeboom et al., "The Nucleotide Transformer: A Novel Architecture for Human Genomics"** demonstrated the feasibility of scaling up Transformer architectures for human genomics. Beyond Transformers, kernel-based or hierarchical approaches emergedâ€”\textit{e.g.}, **Shazeer et al., "Hierarchical Attention Networks with Coattention Mechanism for Long-Range DNA Sequences"** which reduces the quadratic complexity of attention for extremely long sequences. Meanwhile, alternative designs like **Bajpai et al., "Caduceus: A Structured Prior Model for Long-Range DNA Modeling"** incorporate selective structural priors for long-range DNA modeling, and **Meyer et al., "MxDNA: Adaptive Tokenization Schemes for Genomic Data"** explores adaptive tokenization schemes that automatically discover suitable patterns for genomic data.
\vspace{-1em}
\paragraph{Protein and RNA Advances.}
On the \textit{protein} side, **Jumper et al., "AlphaFold 2: A Simple and Accurate Framework for Protein Structure Prediction"** revolutionized structure prediction, catalyzing a surge in protein-based language models such as **Rives et al., "ESM-2: Improving Protein Embeddings with Energy-Based Contrastive Learning"** which refines large-scale protein embeddings. Beyond proteins, RNA modeling also gained traction; for instance, **Liu et al., "RNAFold 3D: Accurate 3D Structure Prediction of RNAs using Language Models"** employed a language-model-based technique for accurate 3D structure prediction. These efforts underscore the trend toward specialized architectures for each biomolecule yet also highlight the desire for integrative multi-omics solutions.
\vspace{-1em}
\paragraph{Multi-Omics Modeling.}
Recent studies aim to bridge the gap between \textit{genomic} and \textit{protein} sequences, pushing beyond single-modality tasks. **Gu et al., "CD-GPT: Connecting DNA, RNA, and Proteins through Central Dogma"** explicitly connects DNA, RNA, and proteins through the central dogma, while {Zhang et al., "BSM: Bridging Genes and Proteins with Small yet Effective Models"** highlights the potential for small but effective models covering genes and proteins simultaneously. Evo____ similarly integrates molecular and genome-scale data with a generalized sequence-modeling approach. Closely related is **Bai et al., "LucaOne: A Unified Nucleic Acid and Protein Language Model for Biological Representation"**, which advocates a unified nucleic acid and protein language for biological representation.