\begin{wraptable}{r}{0.45\linewidth}
% \begin{table}[ht]
    \vspace{-0.75em}
    % \setlength{\tabcolsep}{1.6mm}
    \centering
    \caption{
    Configuration of the network designs and pre-training settings for Life-Code models. GDN blocks denote the Gated DeltaNet block with linear complexity, while Attention blocks denote the self-attention block with FLASH-Attention implementation.
    }
    \vspace{1pt}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l|cc}
    \toprule
% 
Configuration       & Tokenizer          & Encoder         \\ \hline
Embedding dim       & 384                & 1024            \\
Block number        & 1                  & 24              \\
GDN blocks          & 1                  & 22              \\
Attention blocks    & 0                  & 2               \\
Attention heads     & 0                  & 24              \\
Parameters          & 8M                 & 340M            \\ \hline
Optimizer           & \multicolumn{2}{c}{AdamW}            \\
$(\beta_1,\beta_2)$ & \multicolumn{2}{c}{$(0.9,0.98)$}     \\
Training iterations & 100,000            & 1,000,000       \\
Weight decay        & \multicolumn{2}{c}{$1\times 10^{-2}$}  \\
Base learning rate  & $2\times 10^{-4}$  & $1\times 10^{-4}$ \\
Batch size          & 512                & 256             \\
LR scheduler        & \multicolumn{2}{c}{Cosine Annealing} \\
Warmup iterations   & 5000               & 10,000          \\
Gradient clipping   & \multicolumn{2}{c}{1.0}              \\
% 
    \bottomrule
    \end{tabular}
    }
    \label{tab:app_lcode_config}
    \vspace{-0.5em}
% \end{table}
\end{wraptable}
