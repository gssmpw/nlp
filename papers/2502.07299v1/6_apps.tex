% \appendix
% \section{Appendix}
% You may include other additional sections here.

\renewcommand\thefigure{A\arabic{figure}}
\renewcommand\thetable{A\arabic{table}}
\setcounter{table}{0}
\setcounter{figure}{0}

\appendix

\newpage
\onecolumn

\section*{Appendix}
% The appendix is structured as follows:
\begin{itemize}[leftmargin=1.25em]
    \vspace{-0.5em}
    \item In Appendix~\ref{app:implement}, we provide implementation details of pre-training datasets, network architectures, and training schemes of pre-training and fine-tuning stages with hyper-parameter settings.
    \vspace{-0.5em}
    \item In Appendix~\ref{app:additional_res}, we provide detailed descriptions of downstream tasks of biological applications and full comparison results.
\end{itemize}


\begin{figure}[ht]
    % \vspace{-0.5em}
    \centering
    \includegraphics[width=0.65\linewidth]{figs/fig_data_collection.pdf}
    % \vspace{-2.0em}
    \caption{\textbf{Data Collection Pipeline}. We collect the reference sequences from the NCBI RefSeq database as the DNA dataset and collect the cDNA of coding sequences with the corresponding amino acids from the GenBank and UniRef50 databases to build up our DNA-AA paring datasets.
    }
    \label{fig:data_collection}
    \vspace{-1.0em}
\end{figure}


\section{Implementation Details}
\label{app:implement}

\input{tabs/tab_dataset}
\subsection{Pre-training Dataset}
We collect two datasets for pre-training of Life-Code models, \textit{i.e.}, a pure DNA dataset and a DNA-AA pairing dataset, with the collection process shown in Figure~\ref{fig:data_collection}.
As for the DNA dataset, we collect the reference sequences (RefSeq) of multiple species to ensure generalization abilities from the database of the National Center for Biotechnology Information (NCBI) at \url{https://www.ncbi.nlm.nih.gov}, following the Multi-species Genomes\footnote{Multi-species Genomes are originally provided in \url{https://huggingface.co/datasets/InstaDeepAI/multi_species_genomes}, which is further extended by DNABERT-2 in \url{https://github.com/MAGICS-LAB/DNABERT_2}} provided by Nucleotide Transformer~\citep{NM2023NucleotideTrans} and DNABERT2~\citep{iclr2024dnabert2}.
As for the DNA-AA pairing dataset, we collect the cDNA of coding sequences (CDS) and its corresponding Amino Acids (AA) in the GenBank database at \url{https://www.ncbi.nlm.nih.gov/genbank}, which aims to model the transcription and translation processes of the central dogma. We also collect some Amino Acids in the UniRef50 database following LucaOne~\citep{he2024lucaone} and obtain their corresponding cDNA by reverse translation with online tools.
We provide detailed information for used datasets in Table~\ref{tab:app_dataset}.


\subsection{Life-Code Tokenizer}
\label{app:impl_tokenizer}
\paragraph{Vocabulary.}
There are two vocabularies used in Life-Code. The unified vocabulary only uses 4 nucleotides \{A, T/U, C, G\} of nucleic acid with 5 special tokens, including ``[U]"/``[UNK]", ``[PAD]", ``[CLS]", ``[SEP]", and “[MASK]” for unknown nucleotides, padding tokens, the class token, separator tokens, and masking tokens. Meanwhile, the Life-Code can also use the codon vocabulary (\textit{i.e.}, the 3-mer of 4 nucleotides that constructs 64 codon tokens), which could be merged into 20 amino acids of protein (20 uppercase letters excluding ``B", ``J", ``O", ``U", ``X", and ``Z"). It can only be applied when the length of an input sequence is multiples of 3, \textit{i.e.}, the cDNA of amino acids or matured mRNA (CDS). The pre-trained protein language model employs the amino acid vocabulary of ESM-2 \citep{lin2022ESM2}.

\input{tabs/tab_app_config}
\vspace{-0.5em}
\paragraph{Tokenizer Network.}
As shown in Figure~\ref{fig:tokenizer}, with the nucleotide vocabulary (including 4 nucleic acids and 5 special symbols), the \textit{Life-Code tokenizer} contains the following modules: (a) A linear projection from 9-dim to 384-dim implemented by \texttt{nn.Embedding}; (b) A GDN block (Gated DeltaNet) with 384-dim for global contextual modeling with linear computational complexity; (c) A 1-d Convolution with a kernel size of 3 and stride of 1, followed by an \texttt{UnFold} operation to merge every three nucleotide tokens into 768-dim codon embedding. Similarly, we design the \textit{DNA De-Tokenizer} with the symmetrical network as the Life-Code tokenizer: (a) \texttt{Fold} operation with a 1-d Convolution with a kernel size of 3 to unmerge the codon embedding to 384-dim, (b) A linear projection from 384-dim to 9-dim vocabulary to reconstruct the original DNA sequences. We also design the \textit{Amino Acid Translator} as a two-layer MLP that translates the codon embedding to the corresponding Amino Acid sequences.

\vspace{-0.5em}
\paragraph{Pre-training Settings.}
As shown in Figure~\ref{fig:tokenizer}, we pre-train the Life-Code tokenizer with the DNA de-tokenizer and Amino Acid de-tokenizer by AdamW optimizer for 100,000 iterations (randomly sampled datasets) with a basic learning rate of 2e-4 and a batch size of 512, as detailed in Table~\ref{tab:app_lcode_config}. We utilize 8 Nvidia A100-80G GPUs with a per-GPU batch size of 8 and a gradient accumulation time of 4.


\subsection{Life-Code Encoder}
\label{app:impl_encoder}
\paragraph{Encoder Architecture.}
As shown in Table~\ref{tab:app_lcode_config}, the Life-Code encoder has 24 layers in total with the embedding dim of 1024 with the following designs: (1) Mixture of GDN blocks (DeltaNet~\citep{yang2025deltanet}) and multi-head self-attention (MHSA) blocks as a hybrid model, especially every 11 GDN blocks followed by a self-attention block like MiniMax-01~\citep{MiniMax2025MiniMax01}, which could utilize the complementary properties of GDN and MHSA while maintaining efficiency. (2) The model macro design employs pre-norm~\citep{acl2019PreNorm} with RMSNorm~\citep{Zhang2019RMSNorm}, Layer Scale~\citep{iccv2021CaiT}, Rotary Position Embedding (RoPE)~\citep{Su2021RoFormer}, SwiGLU~\citep{Touvron2023LLaMA}, and FlashAttention implementations to facilitate training large-scale models stably with long sequences. (3) During pre-training, we apply the packing strategy~\citep{Warner2024ModernBERT} to build up a long sequence with several CDS, which compromises the gap between different lengths of the reference sequence and the coding sequences, as shown in Figure~\ref{fig:data_packing}.

\vspace{-0.5em}
\paragraph{Pre-training Settings.}
As shown in Figure~\ref{fig:lcode_pretraining}, we further pre-train the Life-Code tokenizer and Encoder with three tasks in Eq.~\ref{eq:loss_total} for 1M steps with the batch size of 256 and the basic learning rate of $1\times 10^{-4}$. We adopt 15\% random masking in BERT for Masked DNA Reconstruction and the 3-mer span masking for CDS-to-Amino-Acid Translation. As for Knowledge Distillation from a pre-trained Protein LM, we adopted ESM2-650M (\textit{esm2\_t33\_650M\_UR50D})~\citep{lin2022ESM2} and a protein decoder with the output dimension of 1280. During the warmup periods, the maximum sequence length is 1024, with a linear warmup of the learning rate for 10 iterations. After that, the maximum sequence length is set to 4k with the learning rate adjusted by the Cosine Annealing scheduler (decay to $1\times 10^{-6}$). We utilize 8 Nvidia A100-80G GPUs with a per-GPU batch size of 2 and a gradient accumulation time of 16.


\subsection{Supervised Fine-tuning}
\label{app:impl_SFT}
In most cases, we apply Supervised Fine-tuning (SFT) to transfer pre-trained models to downstream tasks. Following \citep{nips2024hyenadna, iclr2024dnabert2}, adding the decoder head (\textit{e.g.}, an MLP head) to a specific downstream task, the linear attention (RNN) or self-attention blocks in the pre-trained encoder models are frozen, while Low-Rank Adaptation (LoRA) strategy~\citep{Hu2021LoRA} is employed to parameter-efficiently fine-tuning the models by AdamW optimizer with a batch size of 32. For each task, if the benchmark and models have provided hyper-parameters, we follow the official settings, or we choose the best combinations of the basic learning rate \{1e-5, 5e-5, 1e-4\}, the weight decay \{0, 0.01\}, the LoRA rank \{4, 8, 16, 24, 48\}, the LoRA alpha \{8, 16, 24, 48, 96\}, and the total fine-tuning epoch \{5, 10\} on the validation set following the GUE benchmark and GenBench~\citep{liu2024genbench}. Note that the maximum input length will be determined for different tasks since the sequence lengths of downstream tasks vary widely. We report the averaged results over three runs with the optimal settings.


\input{tabs/tab_app_genomic_benchmark}
\input{tabs/tab_app_gue}

\section{Downstream Task Settings and Extensive Comparison Results}
\label{app:additional_res}
% 
\subsection{DNA Tasks with Genomics Benchmark}
As proposed by \citep{BMC2023genomicbenchmark}, three groups of basic genomic tasks are collected as binary classification with top-1 accuracy in the Genomics Benchmark. As for the enhancer prediction, three datasets are provided for identifying enhancer regions in the mouse or human genome. As for the species classification, two datasets are selected for identifying sequences as either coding (exonic) or intergenic (non-coding) and classifying sequences as originating from humans or worms (C. elegans). As for the regulatory elements classification, three datasets are used for classifying sequences as regulatory regions based on Ensembl annotations, identifying open chromatin regions, or identifying non-TATA promoter regions in the human genome. We utilize the fully reproduced results of various DNA models in GenBench \citep{liu2024genbench}.


\subsection{DNA Tasks with GUE Benchmark}
As proposed by DNABERT2~\citep{iclr2024dnabert2}, the GUE benchmark contains 24 datasets of 7 practical biological genome analysis tasks for 4 different species using Matthews Correlation Coefficient (MCC) as the evaluation metric. To comprehensively evaluate the genome foundation models in modeling variable-length sequences, tasks with input lengths ranging from 70 to 1000 are selected. The following descriptions of the supported tasks are included in the GUE benchmark, where these resources are attached for illustration.

\textbf{Promoter Detection (Human).}\quad
This task identifies human proximal promoter regions essential for transcription initiation. Accurate detection aids in understanding gene regulation and disease mechanisms. The dataset includes TATA and non-TATA promoters, with sequences -249 to +50 bp around the Transition Start Site (TSS) from Eukaryotic Promoter Database (EPDnew) \citep{dreos2013epdpromoter}. Meanwhile, we construct the non-promoter class with equal-sized randomly selected sequences outside of promoter regions but with TATA motif (TATA non-promoters) or randomly substituted sequences (non-TATA, non-promoters). We also combine the TATA and non-TATA datasets to obtain a combined dataset named \textit{all}.

\textbf{Core Promoter Detection (Human).}\quad
This task is similar to the detection of the proximal promoter with a focus on predicting only the core promoter region, the central region closest to the TSS, and the start codon. A much shorter context window (center -34~+35 bp around TSS) is provided, making this a more challenging task than the prediction of the proximal promoter. 

\textbf{Transcription Factor Binding Site Prediction (Human).}\quad
This task predicts human transcription factor binding sites (TFBS), crucial for gene expression regulation. Data from 690 ENCODE ChIP-seq experiments (161 TF binding profiles in 91 cell lines) \citep{mouse2012encyclopedia} are collected via the UCSC genome browser. TFBS sequences are 101-bp regions around peaks, while non-TFBS sequences match in length and GC content. There are 5 datasets selected from a curated subset of 690, excluding trivial or overly challenging tasks.

\textbf{Splice Site Prediction (Human).}\quad
This task predicts splice donor and acceptor sites, the exact locations in the human genome where alternative splicing occurs. This prediction is crucial to understanding protein diversity and the implications of aberrant splicing in genetic disorders. The dataset \citep{BMC2021spliceator} consists of 400-bp-long sequences extracted from Ensembl GRCh38 human reference genome. As suggested by \citet{BioInfo2021dnabert}, existing models can achieve almost perfect performance on the original dataset, containing 10,000 splice donors, acceptors, and non-splice site sequences, which is overly optimistic about detecting non-canonical sites in reality. As such, we reconstruct the dataset by iteratively adding adversarial examples (unseen false positive predictions in the hold-out set) in order to make this task more challenging.  
 
\textbf{Transcription Factor Binding Site Prediction (Mouse).}\quad
This task predicts mouse transcription factor binding sites using mouse ENCODE ChIP-seq data (n=78) \citep{mouse2012ChIPseq} from the UCSC genome browser. Negative examples are created by di-nucleotide shuffling. Five datasets are randomly selected from the 78 datasets using the same process as the human TFBS prediction dataset.

\textbf{Epigenetic Marks Prediction (Yeast).}\quad
This task predicts epigenetic marks in yeast, which influence gene expression without altering DNA sequences. Precise prediction of these marks aids in elucidating the role of epigenetics in yeast. We download the 10 datasets from \url{http://www.jaist.ac.jp/~tran/nucleosome/members.htm} and randomly split each dataset into training, validation, and test sets.

\textbf{Covid Variant Prediction (Virus).}\quad
This task aims to predict the variant type of the SARS\_CoV\_2 virus based on 1000-length genome sequences. We download the genomes from the EpiCoV database \citep{khare202SARS_CoV_2} of the Global Initiative on Sharing Avian Influenza Data (GISAID). We consider 9 types of SARS\_CoV\_2 variants, including \textit{Alpha}, \textit{Beta}, \textit{Delta}, \textit{Eta}, \textit{Gamma}, \textit{Iota}, \textit{Kappa}, \textit{Lambda} and \textit{Zeta}.


\input{tabs/tab_app_splicing}

\subsection{mRNA Splicing Tasks}
% \label{app:rna_splicing}
Following \citep{iclr2024dnabert2, shen2024rnafm}, we evaluate pre-mRNA Splicing Site Prediction as the RNA task, which is a crucial process in eukaryotic gene expression. During splicing, introns are removed from precursor messenger RNAs (pre-mRNAs), and exons are joined together to form mature mRNAs. This process is essential for generating functional
mRNAs that could be translated into proteins. Identifying splice sites—the donor sites at the 5' end of introns and the acceptor sites at the 3' end—is vital for accurately predicting gene structure and location.
Concretely, we regard this task as binary classification of RNA splicing site prediction specifically for acceptor sites and consider two splicing datasets in addition to the \textit{Splicefinder} dataset \citep{wang2019splicefinder} used in the GUE benchmark.

\textbf{Spliceator dataset.}\quad
This dataset~\citep{BMC2021spliceator} consists of ``confirmed" error-free splice-site sequences from a diverse set of 148 eukaryotic organisms, including humans. The gold standard dataset GS 1 is adopted, which contains an equal number of positive and negative samples, and the F1 score is used as the evaluation metric. We chose three independent test datasets containing the samples from 3 different species of humans, fish (Danio rerio), and fruit fly (Drosophila melanogaster).

\textbf{SpliceAI dataset.}\quad
This dataset~\citep{JAGANATHAN2019SpliceAI} also constructs a binary classification dataset similar to Spliceator, which utilizes the GTEx (Genotype-Tissue Expression) project for RNA sequencing data from various human tissues and the GENCODE V24lift37 canonical annotation for gene structure information. SpliceAI also references the ClinVar database to evaluate the clinical significance of predicted splicing variants, which contains information on clinically relevant variants and their associations with diseases. This dataset can be regarded as a long-range evaluation and adopts the top-1 AUC-ROC score as the metric.


\input{tabs/tab_app_overall}

\subsection{Protein and Multi-omic Tasks}
% \label{app:protein_multiomics}
\paragraph{Zero-shot Protein Fitness Prediction.}
Following EVO~\citep{nguyen2024evo} and protein language models~\citep{lin2023esm}, we employ Deep Mutational Scanning (DMS) studies to evaluate the models' abilities for protein tasks, which introduce many mutations to a protein coding sequence and then experimentally measure the effects of these mutations (as fitness scores) on various definitions of fitness~\citep{icml2022Tranception}. EVO obtained (DMS) datasets with bacterial (prokaryote) and human (eukaryote) proteins from ProteinGYM at \url{https://proteingym.org}.
To adapt this task to nucleotide sequences, EVO proposes to use the wild-type coding sequence and nucleotide mutations reported in the original DMS studies \citep{icml2022Tranception, altae2021widespread}. For generative pre-trained models such as EVO, we rely on likelihood-based scores under the same masking scheme, assessing how well the model anticipates mutations.
The model performances of zero-shot function prediction are measured by the strength of Spearman's Rank Correlation Coefficient (SRCC) that correlates the predicted likelihoods with the experimental fitness measurements.
Full results of protein fitness prediction are shown in Table~\ref{tab:protein_dms} and Table~\ref{tab:app_overall}, in which our Life-Code achieves balancing performances with bacterial and human proteins and surpasses NT-2500M and EVO-7B.


\vspace{-0.5em}
\paragraph{ncRNA-Protein Interactions.}
Following LucaOne~\citep{he2024lucaone}, we consider the multi-omics task of ncRNA-protein interactions (ncRPI), which identifies the interaction strengths between non-coding RNAs (\textit{e.g.}, snRNAs, snoRNAs, miRNAs, and lncRNAs) and proteins. Since experimentally identifying ncRPI) is typically expensive and time-consuming, the AI-based ncRPI can be a promising task. LucaOne proposes a binary classification task involving pairs of ncRNA and Amino Acid sequences (20,824 pairs in total) with top-1 accuracy as the metric.

\vspace{-0.5em}
\paragraph{Central Dogma Evaluation.}
To evaluate the modeling of the translation rule in the central dogma, we follow LucaOne~\citep{he2024lucaone} to conduct a binary classification task with top-1 accuracy, which determines whether the DNA sequences and the given proteins are correlated.
LucaOne collects a total of 8,533 accurate DNA-protein pairs from 13 species in the NCBI RefSeq database, with each DNA sequence extending to include an additional 100 nucleotides at both the 5' and 3' contexts, preserving intron sequences within the data. LucaOne generated
double the number of negative samples by implementing substitutions, insertions, and deletions within the DNA sequences or altering amino acids in the protein sequences to ensure the resultant DNA sequences could not be accurately translated into their respective proteins.

