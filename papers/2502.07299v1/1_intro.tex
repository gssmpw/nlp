\section{Introduction}
The advent of large-scale biological datasets has sparked extensive interest in \textit{sequence modeling} across multiple omics domains---ranging from \textit{DNA} and \textit{RNA} to \textit{proteins}~\cite{smith2023largescale, liwang2024protein,icml2024vqdna,liu2024genbench}. These endeavors aim to leverage deep learning to uncover functional elements, predict gene regulation, and accelerate protein engineering. Notably, the successful application of \textit{Transformer}-based architectures in natural language processing (NLP)~\cite{vaswani2017attention, devlin2019bert} has inspired numerous efforts to adapt these methods to \textit{biology}, giving rise to models for variant effect prediction~\cite{zhou2022transformer}, protein structure elucidation~\cite{jumper2021alphafold}, and more.


\input{tabs/tab_overall}
\begin{figure}[t]
    \vspace{-0.25em}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/fig_intro.pdf}
    \vspace{-2.25em}
    \caption{\textbf{Overview of the Central Dogma Data Flow} with biological language models (LM) severally proposed in each omics.
    }
    \label{fig:central_dogma}
    \vspace{-1.0em}
\end{figure}

\begin{figure*}[t!]
    % \vspace{-0.25em}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/fig_framework.pdf}
    \vspace{-2.25em}
    \caption{\textbf{Illustration of Life-Code Framework}, which contains three pipelines. (a) \yellow{\textbf{Data Pre-processing}} for the unified input with the central dogma. (b) \blue{\textbf{Pre-training}} of the Tokenizer and the hybrid Encoder to model the contextual information and the central dogma rules. (c) \purple{\textbf{Transfer Learning}} to multi-omics downstream tasks with parameter-efficient Supervised Fine-tuning (SFT).
    }
    \label{fig:lcode_framework}
    \vspace{-1.0em}
\end{figure*}

Despite the promise of these paradigms of foundation models in computational biology~\cite{lin2023esm,nguyen2024evo,shen2024rnafm}, three major obstacles continue to hamper \textit{multi-omics} modeling:
% 
\textbf{(1) Data Island.} Current approaches often focus on \textit{one molecular modality}, modeling only DNA, RNA, or only protein---leading to siloed representations that cannot exploit the inherent relationships among DNA, RNA, and proteins~\cite{hong2022crossomics}, as shown in Figure~\ref{fig:central_dogma}. As a result, critical cross-omics information (for example, how a genetic variant in DNA might alter RNA splicing and ultimately impact protein function) remains underutilized.
% 
\textbf{(2) Na\"ive ``Copy and Paste''.} While Transformers~\cite{vaswani2017attention, devlin2019bert} have revolutionized NLP, simply applying them without incorporating \textit{biological knowledge} can undermine performance and interpretability~\cite{rives2021biological}. For instance, ignoring codons, coding, and non-coding regions may limit model accuracy and obscure how predictions map to real biological processes.
% 
\textbf{(3) Inefficiency for Long Sequences.} Genomic and transcriptomic sequences can span \textit{thousands to millions} of bases, and mapping proteins back to nucleotide triplets multiplies sequence length~\cite{alquraishi2019proteinnet}. This expansion quickly renders the \(\mathcal{O}(n^2)\) complexity of vanilla Transformers computationally prohibitive, especially for large-scale training on full genomes.

This paper introduces \textbf{Life-Code}, as shown in Figure~\ref{fig:lcode_framework}, a unifying framework that addresses these limitations through \textit{multi-omics unification}, \textit{bio-interpretable designs}, and \textit{long-sequence efficiency}. By grounding our method in the \textit{central dogma} of molecular biology, we align DNA, RNA, and protein sequences under a single representation that captures cross-omics signals. We then augment this representation with a \textit{hybrid} backbone architecture, where key biological structures (\textit{e.g.}, coding sequences, promoters, and regulatory motifs) guide the model to focus computational resources effectively. In particular, we explore specialized attention mechanisms that mitigate the memory bottleneck of na\"ive Transformer models while retaining sufficient global context for \textit{biologically relevant} long-range dependencies.
In summary, this work makes four \textbf{key contributions} to the field of computational biology with deep learning:

\begin{itemize}
    \vspace{-0.5em}
    \item \textbf{Multi-omics Modeling.} 
    We demonstrate a novel \textit{multi-omics} approach by co-representing DNA, RNA, and protein within a \textit{single} model, thus overcoming the traditional data island problem and enabling the transfer of knowledge across modalities.
    
    \vspace{-2pt}
    \item \textbf{Bio-interpretable Design.} 
    Our tokenization and training pipeline explicitly distinguishes \textit{coding (CDS)} and \textit{non-coding (nCDS)} regions, preserving \textit{biological interpretability}. Through tasks like masked language modeling for nCDS and protein translation objectives for CDS, the model learns to capture both regulatory and translational signals.

    \vspace{-2pt}
    \item \textbf{Efficient Long-sequence Computing.}
    We combine structured prior knowledge (\textit{e.g.}, double-helix complementarity, coding frames) with efficient linear attention mechanisms---balancing representational power with reduced complexity for long sequences. This hybrid strategy yields significant computational savings over na\"ive \(\mathcal{O}(n^2)\) Transformer approaches.

    \vspace{-2pt}
    \item \textbf{Comprehensive Validations.}
    We evaluate Life-Code on diverse tasks in DNA, RNA, and protein applications. Empirical results confirm the effectiveness of our unified approach in these three fundamental modalities.
\end{itemize}
