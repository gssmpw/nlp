\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage[font=scriptsize]{caption}
\usepackage[moderate]{savetrees}

\input{preamble}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}



\def\paperID{4374} %
\def\confName{CVPR}
\def\confYear{2025}

\newcommand{\colorRone}{YellowOrange}
\newcommand{\colorRtwo}{blue}
\newcommand{\colorRthree}{ForestGreen}

\newcommand{\Rone}{\textcolor{\colorRone}{\textbf{R1-GJn8}}\xspace}
\newcommand{\Rtwo}{\textcolor{\colorRtwo}{\textbf{R2-sZTo}}\xspace}
\newcommand{\Rthree}{\textcolor{\colorRthree}{\textbf{R3-G1vw}}\xspace}

\input{vincents_commands}
\addeditor{vincent}{VL}{0.0, 0.5, 0.0}
\showeditstrue
\showeditsfalse

\begin{document}

\title{\BN: A Structured Benchmark for Image Matching across Geometric Challenges}  %

\maketitle
\thispagestyle{empty}
\appendix

\noindent We thank the reviewers for their thorough and constructive feedback. We address the main concerns below:

\vspace{3mm}

\noindent \textbf{Dataset Scope and Limitations (\Rone, \Rthree):} 


\noindent We acknowledge the dataset's focus on autonomous driving scenarios, however:

\noindent - The methodology we present is general and can be applied to other domains (e.g., AR, robotics) in future works to build similar structured evaluations. We plan to apply it to other datasets, such as indoor datasets, to create similarly structured benchmarks.

\noindent - As the three reviewers acknowledge, autonomous driving is by itself a crucial application domain for image matching.
    
\noindent - The controlled environment allows us to isolate and study geometric challenges specifically, without confounding factors like extreme weather or time changes for example. 

\noindent - Another advantage of nuScenes is that none of the 14 methods were trained on it, thus providing a fair comparison. \vincentrmk{you also finetune on your dataset, right? If so, you should add .. "when we do not finetune these methods on our dataset."}
    


\vspace{3mm}

\noindent \textbf{Response to \Rone:}

\noindent - \textit{Binning strategy:} Using finer granularity in high-overlap cases (80-100\%) is a great suggestion. We will revise our binning approach and add statistical justification for the boundaries.

\noindent - \textit{Scenes recorded in the same place:}
    Our pair selection process is based purely on geometric criteria, regardless of whether the 85 selected test scenes were recorded in the same location at different times. This approach ensures unbiased sampling across the full range of geometric challenges while maintaining high-quality ground truth poses for individual scene reconstructions. We will clarify this in the paper to avoid any confusion.
    
\noindent - \textit{Multiple Difficulty Thresholds:}
    We agree that adopting the community standards will make our results more comparable. We have added these evaluations by adding a new range (10° / 5m), and the median rank of each method (please see~\cref{tab:extended_results}).
    \begin{table}[h]
        \vspace{-0.2cm}

        \begin{minipage}[c]{0.22\columnwidth}
            \caption{Extended results.
            }
            \label{tab:extended_results}
        \end{minipage}  
        \begin{minipage}[c]{0.8\columnwidth}
            \begin{adjustbox}{width=0.95\columnwidth,center}

                \small
                \centering
                \begin{tabular}{lcccccc}
                    \toprule
                    & \multicolumn{3}{c}{5° / 2m} & \multicolumn{3}{c}{10° / 5m} \\
                    \cmidrule(lr){2-4} \cmidrule(lr){5-7}
                    Method & Avg.R & Med.R & Succ(\%) & Avg.R & Med.R & Succ(\%) \\
                    \midrule
                    \multicolumn{7}{l}{\textit{Detector-based methods}} \\
                    ALIKED+LG & \textbf{5.3} & \textbf{5} & \textbf{36.8} & \textbf{5.1} & \textbf{4} & \textbf{49.4} \\
                    DISK+LG & \underline{5.4} & \textbf{5} & \underline{35.9} & 6.8 & 7 & 45.2 \\
                    SP+LG & 6.1 & \underline{6} & 35.7 & \underline{5.9} & \underline{5} & \underline{47.8} \\
                    SIFT+LG & 7.3 & 7 & 33.1 & 7.5 & 7 & 44.1 \\
                    DeDoDe v2 & 8.6 & 9 & 30.4 & 7.9 & 9 & 41.8 \\
                    XFeat & 13.1 & 14 & 14.2 & 13.0 & 14 & 23.3 \\
                    XFeat* & 12.5 & 13 & 15.1 & 12.9 & 13 & 25.5 \\
                    XFeat+LG & 9.0 & 9 & 30.1 & 9.3 & 9 & 39.8 \\
                    \midrule
                    \multicolumn{7}{l}{\textit{Detector-free methods}} \\
                    LoFTR & 10.8 & 11 & 24.9 & 9.5 & 11 & 37.8 \\
                    ELoFTR & 9.5 & 10 & 26.6 & 8.8 & 9 & 40.3 \\
                    ASpanFormer & 9.8 & 10 & 24.8 & 9.8 & 11 & 35.4 \\
                    RoMa & 2.7 & \underline{3} & 47.3 & \underline{2.9} & 3 & 63.5 \\
                    DUSt3R & \textbf{2.4} & \textbf{2} & \textbf{54.8} & 3.2 & \underline{2} & \underline{72.8} \\
                    MASt3R & \underline{2.5} & \textbf{2} & \underline{53.6} & \textbf{2.4} & \textbf{1} & \textbf{73.5} \\
                    \bottomrule
                \end{tabular}
            \end{adjustbox}
        \end{minipage}
    \vspace{-0.2cm}
    \end{table}

\noindent We also tested finer thresholds (0.25m / 2° and 0.5m / 5°) with similar ranking results. However, since these thresholds are close to the ground truth's noise level, we chose not to report them to avoid potentially misleading future work. %

\vincent{
\noindent - \textit{COLMAP:} We will add details on how we use it. In particular, we consider  intrinsic parameters per camera per scene and a simple pinhole camera model as images are already undistorted, and exhaustive matching using SIFT.
}

\vspace{3mm}

\noindent \textbf{Response to \Rtwo:}

\noindent - \textit{UniDepth usage as GT metric depth}: We agree this needs to be further discussed, especially in the limitations. Before considering using UniDepth as GT metric depth, we did try aligning with COLMAP sparse reconstruction, but it did not make a significant difference. We believe this is due to the fact that UniDepth is surprisingly accurate on nuScenes (please see Tab.~1 in \url{https://arxiv.org/pdf/2410.02073}). However, we agree it is cleaner to align with COLMAP so we will put the alignment back into the pipeline.

\vincent{
\noindent - \textit{Fundamental Matrix:} We re-evaluated the methods using OpenCV findFundamentalMat with MAGSAC++. \cref{tab:ess_vs_fund} reports the aggregated results. The ranking is not affected but all the methods drop significantly compared to estimating the essential matrix, indicating that the pairs are overall quite challenging and require a 5-pt minimal solver. Interestingly, estimating the fundamental matrix solves the "problem" of the high-overlap (80-100\%) bucket, corresponding to small translations. This is not visible in the aggregated results of \cref{tab:ess_vs_fund}, but all the methods now have (close to) 100\%. This should not be surprising, as ORB-SLAM for instance estimates the fundamental matrix rather than the essential matrix for initialization, i.e., when the translation is very small.
}



    \begin{table}[h]
        \vspace{-0.2cm}

        \begin{minipage}[c]{0.15\columnwidth}
            \caption{Essential vs. Fundamental Matrix.}
            \label{tab:ess_vs_fund}

        \end{minipage}
        \begin{minipage}[c]{0.7\columnwidth}
            \begin{adjustbox}{width=0.8\columnwidth,center}
                \small
                \centering
                \begin{tabular}{lcccc}
                    \toprule
                    & \multicolumn{2}{c}{Essential} & \multicolumn{2}{c}{Fundamental} \\
                    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                    Method & Med.R & Succ(\%) & Med.R & Succ(\%) \\
                    \midrule
                    \multicolumn{5}{l}{\textit{Detector-based methods}} \\
                    ALIKED+LG & \textbf{5} & \textbf{36.8} & \underline{6} & \textbf{12.4} \\
                    DISK+LG & \textbf{5} & \underline{35.9} & \textbf{5} & \underline{12.3} \\
                    SP+LG & \underline{6} & 35.7 & 9 & 11.8 \\
                    SIFT+LG & 7 & 33.1 & \underline{6} & 12.0 \\
                    DeDoDe v2 & 9 & 30.4 & 9 & 10.5 \\
                    XFeat & 14 & 14.2 & 11 & 8.4 \\
                    XFeat* & 13 & 15.1 & 12 & 8.8 \\
                    XFeat+LG & 9 & 30.1 & 10 & 11.0 \\
                    \midrule
                    \multicolumn{5}{l}{\textit{Detector-free methods}} \\
                    LoFTR & 11 & 24.9 & 12 & 10.0 \\
                    ELoFTR & 10 & 26.6 & 10 & 11.0 \\
                    ASpanFormer & 10 & 24.8 & 4 & 12.5 \\
                    RoMa & \underline{3} & 47.3 & \underline{3} & 14.7 \\
                    DUSt3R & \textbf{2} & \textbf{54.8} & \textbf{2} & \textbf{16.8} \\
                    MASt3R & \textbf{2} & \underline{53.6} & \textbf{2} & \underline{16.5} \\
                    \bottomrule
                \end{tabular}
            \end{adjustbox}
        \end{minipage}
    \vspace{-0.2cm}

    \end{table}

\noindent - \textit{Additional Baselines:} We agree that including (root)SIFT and ORB would provide valuable context, especially for runtime comparison. We will add these in the camera-ready version.

\vincent{
\noindent - \textit{Limitations:} We will expand our limitations section by discussing the up-is-up constraint and its impact on geometric evaluations.
}


\noindent - \textit{References:} We will cite both the first version of the IMC 2020 for its difficulty grades, as well as the IJCV 2021 paper for its co-visibility metric. That would better position our work in relation to those important contributions.

\vspace{2mm}

\noindent \textbf{Response to \Rthree:}

\vincent{
\noindent We do have findings that surprised us, including the fact that  some detector-based methods are better than early detector-free ones and the high overlap paradox. To provide more insights, we will:
}

\noindent - Add detailed analysis of performance variations across geometric challenges.
    
\noindent - Include practical recommendations for method selection based on specific requirements.

\noindent - Separate detector-free and pointmap-based methods for clearer comparison.



\end{document}
