\section{Related Work}
\label{sec:related}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/pipeline_dataset.png}
    \caption{\textbf{Dense co-visibility map estimation -- }Using normal maps ($\textcolor{myblue}{\m{N_1}}$, $\textcolor{myred}{\m{N_2}}$) and depth maps ($\textcolor{myblue}{\m{D_1}}$, $\textcolor{myred}{\m{D_2}}$) along with relative camera poses ($\m{\textcolor{mygrey}{R}_{\textcolor{myblue}{1}\textcolor{myred}{2}}}$, $\m{\textcolor{mygrey}{t}_{\textcolor{myblue}{1}\textcolor{myred}{2}}}$), we warp depth maps between views to obtain $\textcolor{myblue}{\hat{\m{D}}_1}$ and $\textcolor{myred}{\hat{\m{D}}_2}$. Geometric consistency checks classify pixels as \textcolor{mygreen}{co-visible}, \textcolor{myorange}{occluded}, or \textcolor{mygrey}{outside field-of-view} to obtain the co-visibilty maps $\textcolor{myblue}{\m{C}_{1\rightarrow 2}}$ and $\textcolor{myred}{\m{C}_{2\rightarrow 1}}$ (see~\cref{ssec:covis_gen}). We use UniDepth~\cite{piccinelli2024unidepth} for metric depth estimation and Depth Anything V2~\cite{yang2024depth} for normal map computation.}
    \label{fig:pipeline_dataset}
\end{figure*}

\subsection{Image Matching Benchmarks}
Several benchmarks have been proposed to evaluate image matching methods. HPatches~\cite{balntas2017hpatches} focuses on homography estimation under viewpoint and illumination changes. MegaDepth1500 and ScanNet1500 are two widely used benchmarks initially proposed in~\cite{sarlin2020superglue}. MegaDepth1500 randomly sampled 1,500 image pairs from scenes “Sacre Coeur” and “St. Peter’s Square” of MegaDepth~\cite{li2018megadepth}, discarding pairs with too small or too large overlap. ScanNet1500 randomly sampled 1,500 test pairs from ScanNet~\cite{dai2017scannet} similarly. The KITTI~\cite{geiger2012we}  dataset is also frequently used~\cite{jau2020deep}, where 2,710 image pairs, from consecutive frames, are sampled from the two sequences 09-10.
The Image Matching Challenge 2024~\cite{image-matching-challenge-2024} and its previous occurrences represent a significant advancement in comprehensive evaluation, featuring six distinct categories that cover real-world challenges: from phototourism with varying viewpoints and temporal changes, to aerial-ground matching, repeated structures, natural environments, and challenging scenarios with transparencies and reflections. 

\subsection{Visual Localization Benchmarks}
Aachen Day-Night~\cite{sattler2018benchmarking,zhang2021reference} is a visual localization benchmark addressing outdoor localization in changing conditions. It consists of 4,328 sparsely sampled daytime database images, 824 daytime query images and 98 nighttime query images taken in the same environment.

InLoc~\cite{taira2018inloc} is a visual localization benchmark addressing large scale indoor localization with illumination and long-term changes, as well as repetitive patterns. It consists of 9,972 database images and 329 query images.

The Map-free Relocalization~\cite{arnold2022map} benchmark consists of 655 small places, where each place comes with a reference image. The benchmark features changing conditions and image pairs with low to no visual overlap.

Despite these advances, existing benchmarks often lack controlled geometric variations, making it difficult to systematically analyze method performance across different difficulty levels. Our \BN benchmark addresses this limitation by providing 16.5K test pairs organized according to well-defined geometric criteria to obtain controlled varying levels of difficulty.

