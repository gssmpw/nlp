\section{\BN}
\label{sec:benchmark}
Our \BN benchmark is based on images from the nuScenes dataset~\cite{caesar2020nuscenes}. nuScenes is a large-scale autonomous driving dataset containing 1,000 driving scenes in Boston and Singapore, each 20 seconds long, recorded at 12Hz in various weather conditions and times of day. The dataset provides high-quality synchronized camera images from 6 cameras with complete 360Â° coverage, along with precise camera calibration and pose information. 

For our benchmark, we specifically focus on scenes recorded in good weather conditions to isolate and evaluate geometric difficulties without the confounding effect of adverse weathers. This deliberate choice allows us to systematically analyze how methods perform across different geometric challenges, without the additional complexity of environmental factors.

The creation of \BN consists of four main steps: (1) lifting of nuScenes ground truth 3~DoF camera poses to 6 DoF, (2) generation of depth and surface normal maps for each image in order to (3) compute the co-visibility maps between image pairs, which allow us to (4) systematically organize the image pairs based on geometric criteria.

\subsection{Lifting nuScenes ground truth camera poses}\label{sec:lifting}

While nuScenes provides high-quality metric camera poses, these are limited to 3 DoF within the ground plane, as they are primarily intended for autonomous driving applications. However, for comprehensive camera pose estimation benchmarking, we require full 6~DoF metric camera poses that account for variation in camera height and orientation.
To lift nuScenes ground truth 3 DoF metric poses to 6 DoF metric poses, we carefully process each sequence independently using a two-stage approach:
\\
\\
\noindent \textbf{1. 3D Reconstruction -- }We first perform Structure-from-Motion using COLMAP~\cite{schonberger2016structure}.
This provides us with initial 6-DoF camera pose estimates. However, the translations are not metric (i.e. the scene is reconstructed up to a scale factor) and some camera poses may be erroneous.

\noindent \textbf{2. Pose alignment and filtering -- }To obtain metric translations and filter out erroneous poses, we align the previously estimated 6 DoF poses with the nuScenes ground truth 3 DoF metric poses.
To do so, we use a custom LO-RANSAC~\cite{chum2003locally} approach to estimate a 7 DoF similarity transformation between the projected COLMAP poses and nuScenes ground truth poses.
We set the RANSAC threshold to 1 meter to filter out erroneous poses and ensure high-quality ground truth.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/poses_alignment.pdf}
    \vspace{-0.25cm}
    \caption{\textbf{Camera pose alignment and filtering -- }Visualization of (subsampled) camera trajectories of scene-0266 after aligning COLMAP poses with nuScenes ground truth poses.
    Blue crosses ($\textcolor{blue}{\times}$) indicate inlier poses (alignment error $<$ 1m) that are kept for our benchmark, while red crosses ($\textcolor{red}{\times}$) show outlier poses that are discarded.}
    \label{fig:pose_alignment}

\end{figure}

An example of alignment is shown in~\cref{fig:pose_alignment}.
This alignment with nuScenes' metric ground truth allows us to recover proper metric scale, which is not available from COLMAP reconstructions alone, and consequently maintain the high precision of nuScenes' original poses while adding reliable elevation and orientation information.
The resulting 6~DoF metric poses serve as the foundation for our geometric difficulty criteria and co-visibility map computations.

\subsection{Generation of metric depth and normal maps}\label{sec:depth_generation}
To compute dense co-visibility maps between image pairs, we require accurate depth and normal maps for each image. Recent advances in monocular depth estimation have enable reliable geometric information extraction from single images without expensive ground truth measurements. After evaluating several state-of-the-art models, we found that combining two complementary approaches yield optimal results:

\begin{itemize}
    \item \textbf{Metric Depth Maps -- }We use UniDepth~\cite{piccinelli2024unidepth} for its ability to predict metric depth values. The model provides well-aligned depth predictions that are crucial for consistent cross-view measurements.

    \item \textbf{Surface Normal Maps -- }Normal maps computed from UniDepth~\cite{piccinelli2024unidepth} depth predictions tend to be noisy. Instead, we compute normal maps from Depth Anything V2~\cite{yang2024depth} depth maps. Let us highlight that these depth maps are not metric, thus we align them with Unidepth depth maps before normals computation. This approach produces remarkably sharp normal maps with precise object boundaries and fine geometric details (see \cref{fig:normal_comparison} for a comparison).
\end{itemize}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \fbox{\includegraphics[width=0.9\linewidth]{figures/normals/image_normals.png}}        
        \caption{Input image}
        \label{fig:image_normals}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
        \centering
        \fbox{\includegraphics[width=0.9\linewidth]{figures/normals/unidepth_normals.png}}
        \caption{Normals from UniDepth~\cite{piccinelli2024unidepth}}
        \label{fig:unidepth_normals}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
        \centering
        \fbox{\includegraphics[width=0.9\linewidth]{figures/normals/dav2_normals.png}}
        \caption{Normals from DAv2~\cite{yang2024depth}}
        \label{fig:dav2_normals}
    \end{subfigure}
    \caption{\textbf{Comparison of surface normal maps -- }From left to right: input image (a), normal maps computed from UniDepth's metric depth predictions (b) and from Depth Anything V2 after alignment to UniDepth depth map (c). Note the significantly sharper object boundaries and finer geometric details in Depth Anything V2's prediction, particularly around building edges and depth discontinuities.}
    \label{fig:normal_comparison}
\end{figure*}

\noindent This complementary approach leverages each model's strengths: UniDepth's accuracy and Depth Anything's superior normal predictions. Our experiments show that this combination provides reliable geometric information for computing co-visibility maps between views, as demonstrated in the following section.

\subsection{Generation of Co-visibility maps}
\label{ssec:covis_gen}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/cov_occ_out.pdf}
    \caption{\textbf{Two-view setup -- }Considering two views, a 3D point can either be co-visible ($\textcolor{mygreen}{\bigstar}$), occluded ($\textcolor{myorange}{\bigstar}$), or outside the field of view ($\textcolor{mygrey}{\bigstar}$) in one of the views. For each co-visible 3D point, we compute its distances $\textcolor{myred}{\m{d_1}}$ and $\textcolor{myblue}{\m{d_2}}$ to both camera centers and the angle ${\pmb{\theta}}$ between the two lines of sight.}
    \label{fig:cov_occ_out}
\end{figure}

Given a pair of images $(\m{I}_1, \m{I}_2)$ with known relative camera pose $(\mr_{12}, \vt_{12})$ from~\cref{sec:lifting}, calibration matrices $(\m{K}_1, \m{K}_2)$, depth maps $(\m{D}_1, \m{D}_2)$, and surface normals $(\m{N}_1, \m{N}_2)$ from~\cref{sec:depth_generation}, we generate the co-visibility maps $\m{C}_{1\rightarrow 2}$ and $\m{C}_{2\rightarrow 1}$ (see Fig.~\ref{fig:pipeline_dataset}).
We start by warping the depth map $\m{D}_2$ to predict ${\m{D}}_{1}$ as follows:
 \begin{equation}
        \hat{\m{D}}_{1}(\vp_1) = \left[ 0\, 0\, 1\right] \left( \mr_{12}\m{D}_2(\vp_{1\rightarrow 2})\m{K}_2^{-1}\vp_{1\rightarrow 2} + \vt_{12} \right),
  \end{equation}
where $\vp_1$ is a pixel location (in homogeneous coordinates) in the grid of $\m{I}_1$, $\vp_{1\rightarrow 2} = \m{K}_2\pi\left( \mr_{12}\tr\left(\m{D}_1(\vp_1)\m{K}_1^{-1}\vp_1 - \vt_{12}\right) \right)$ and $\m{D}_2(\vp_{1\rightarrow 2})$ is implemented using bilinear interpolation.

Predicting  ${\m{D}}_{1}$ enables occlusion detection through a relative depth check:
\begin{equation}
        \frac{|\hat{\m{D}}_{1}(\vp_1) - \m{D}_{1}(\vp_1)|}{\m{D}_{1}(\vp_1)} > \tau,
\end{equation}
where we used  $\tau = 5\%$. Let us highlight that if $\vp_{1\rightarrow 2}$ falls outside the image boundaries, then pixel $\vp_1$ is labeled "outside the field of view". 
This occlusion detection is further refined by discarding a pixel $\vp_1$ if its normal does not point towards camera 2:
    \begin{equation}
        \angle \left(\vec{z}, \mr_{12}\tr\m{N}_1(\vp_{1})\right) < 90^{\circ} - \epsilon,
    \end{equation}
where $\vec{z}=\left[ 0\, 0\, 1\right]\tr$ and we used $\epsilon = 5^\circ$.

An example of co-visibility maps $\m{C}_{1\rightarrow 2}$ and $\m{C}_{2\rightarrow 1}$ is shown in Fig.~\ref{fig:pipeline_dataset}. We perform the previous steps in both directions ($\m{I}_1 \rightarrow \m{I}_2$ and $\m{I}_2 \rightarrow \m{I}_1$). Our results surprisingly show that metric monocular depth prediction networks are, from now on, accurate enough to perform cross-view 3D reasoning. We believe this finding opens a path towards camera pose estimation frameworks beyond classical combination of image matching and 3D geometry-based minimal solver, but we leave this as future work.

\subsection{Geometric criteria}
Using the co-visibility maps $\m{C}_{1\rightarrow 2}$ and $\m{C}_{2\rightarrow 1}$ previously computed, we evaluate three complementary criteria to quantify the geometric difficulty of estimating the relative pose between an image pair ($\m{I}_1, \m{I}_2$).
\\
\\
\noindent \textbf{1. Overlap ($\omega$) -- }The ratio of co-visible pixels to total pixels:
    \begin{equation}
        \omega = \frac{|\m{C}_{1\rightarrow2}| + |\m{C}_{2\rightarrow1}|}{|\m{I}_1| + |\m{I}_2|},
    \end{equation}
    where $|\cdot|$ is the cardinal of a set. The overlap is a classical criterion that is often used by image matching methods~\cite{sarlin2020superglue, sun2021loftr, chen2022aspanformer}  to obtain a balanced training set that includes both simple pairs (with large overlaps) and challenging pairs (with small overlaps). However, this criterion alone is somewhat limited, as an image pair with a large pure rotation (i.e. where the translation is null) may result in a small overlap, even though the underlying matching problem is not particularly difficult, since the viewpoint remains unchanged.
    \\

\noindent \textbf{2. Scale Ratio ($\delta$) -- }The median of the ratios of the camera distances to the co-visible 3D points:  
    \begin{equation}
        \delta = {\mathrm{med}} \left\{ 
        \left\{ r_{\vp_1}^{1\rightarrow 2} \right\}_{\vp_1 \in \m{C}_{1\rightarrow 2}} \cup
        \left\{ r_{\vp_2}^{2\rightarrow 1}\right\}_{\vp_2 \in \m{C}_{2\rightarrow 1}}
        \right\},
    \end{equation}
    with $r_{\vp_i}^{i\rightarrow j}=\max \left( \frac{\left\Vert \m{K_i}^{-1}\vp_i \right\Vert }{\left\Vert \m{D}_i(\vp_i)\m{K}_i^{-1}\vp_i - \vt_{ij} \right\Vert },\frac{\left\Vert \m{D}_i(\vp_i)\m{K}_i^{-1}\vp_i - \vt_{ij} \right\Vert }{\left\Vert  \m{K_i}^{-1}\vp_i \right\Vert }\right)$.
    Contrary to the overlap, the scale ratio is independent of the relative rotation between
the two cameras (i.e. rotating camera 1 and camera 2 in~\cref{fig:cov_occ_out} does not affect neither $d_1$ nor $d_2$) and only depends on the 3D geometry of the scene and the relative translation.
\\

\noindent \textbf{3. Viewpoint Angle ($\theta$) -- }The median of the co-visible line-of-sight angles:
    \begin{equation}
        \theta = {\mathrm{med}} \left\{ 
        \left\{ \theta_{\vp_1}^{1\rightarrow 2} \right\}_{\vp_1 \in \m{C}_{1\rightarrow 2}} \cup
        \left\{ \theta_{\vp_2}^{2\rightarrow 1}\right\}_{\vp_2 \in \m{C}_{2\rightarrow 1}}
        \right\},
    \end{equation}
    where $\theta_{\vp_i}^{i\rightarrow j}=\angle \left(\m{K_i}^{-1}\vp_i, \m{D}_i(\vp_i)\m{K}_i^{-1}\vp_i - \vt_{ij}\right)$ represents the angle between the two lines of sight. It is clear that this criterion is also independent of the relative rotation between the two cameras and only depends on the 3D geometry and the relative translation, just like the scale ratio. 
    However, the viewpoint angle and the scale ratio complement each other, as the viewpoint angle is independent of the scale ratio (i.e. changing $d_2$ in~\cref{fig:cov_occ_out} does not affect $\theta$), and vice versa.

The three criteria discussed above complement each other and will be used in the next section to categorize the image pairs from the nuScenes test scenes based on their difficulty level.

\subsection{Benchmark Organization}
Using the three geometric criteria defined above, we can systematically organize image pairs from nuScenes test scenes according to their difficulty level. For each possible pair within a scene, we compute its overlap $\omega$, scale ratio $\delta$, and viewpoint angle $\theta$. Based on the distributions of these values across the entire test set which comprises 4.2M image pairs across 85 successfully reconstructed and filtered scenes, we define meaningful bins for each criterion:\\
\\
\noindent \textbf{Overlap (\%) -- 5 bins:} 5 - 20 - 40 - 60 - 80 - 100\\
\\
\noindent \textbf{Scale ratio -- 4 bins:} 1.0 - 1.5 - 2.5 - 4.0 - 6.0\\
\\
\noindent \textbf{Viewpoint angle (Â°) -- 4 bins:} 0 - 30 - 60 - 120 - 180\\

An example of image pair for each bin is shown in~\cref{fig:teaser}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/boxes_3d.pdf}
    \caption{\textbf{Visualization of our 3D grid organization -- }Each axis represents one of our geometric criteria.}
    \label{fig:boxes_3d}
\end{figure}

While this binning strategy theoretically creates a $5\times4\times4$ grid (80 difficulty levels), not all combinations are physically possible. For instance, image pairs with both very large overlap and small scale ratio rarely exhibit large viewpoint angles, as these geometric conditions are inherently contradictory. We finally obtain 33 valid difficulty levels (see~\cref{fig:boxes_3d}).

To ensure statistical significance while maintaining a manageable dataset size, we populate each valid difficulty level with 500 randomly sampled image pairs. This results in a benchmark of 16.5K pairs, carefully curated to span the full spectrum of geometric challenges encountered in real-world scenarios. As shown in~\cref{fig:ablation_no_samples}, our choice of 500 image pairs per box ensures stable and reliable evaluation metrics, with similar conclusions holding across all evaluated methods.

This structured organization enables systematic evaluation of pose estimation methods across well-defined difficulty levels, from simple cases with large overlap and similar viewpoints to challenging scenarios with minimal overlap and extreme geometric variations.
