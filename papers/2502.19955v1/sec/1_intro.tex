\section{Introduction}
\label{sec:intro}
\vspace{-0.2cm}
Camera pose estimation is a cornerstone of many computer vision applications, including augmented reality~\cite{azuma2001recent}, robotics~\cite{cadena2016past}, 3D reconstruction~\cite{schonberger2016structure,wu2013towards,heinly2015reconstructing}, and autonomous navigation~\cite{taira2018inloc,mur2015orb,mur2017orb}.
Camera pose estimators often rely on state-of-the-art image matching methods~\cite{potje2024xfeat,edstedt2024dedode,lindenberger2023lightglue,zhao2023aliked, sun2021loftr,wang2024efficient,edstedt2024roma,chen2022aspanformer,wang2024dust3r,leroy2024grounding, wang2022matchformer, giang2023topicfm, mao20223dg, tangquadtree, dusmanu2019d2, revaud2019r2d2, edstedt2023dkm, fan2023occ, chen2021learning, yin2024srpose, gleize2023silk, hedlin2024unsupervised, kim2025learning, germainvisual, germain2020s2dnet, germain2021neural, jiang2021cotr, tan2022eco}, which have achieved great performance in challenging scenarios, such as occlusions, limited overlap, and significant viewpoint changes.
The creation of benchmarks~\cite{toft2020long,sattler2018benchmarking,zhang2021reference,taira2018inloc,arnold2022map} significantly contributed to these advancements by allowing a fair comparison between the proposed methods and pushing the development for more performing methods.

In this paper, we propose \BN, a benchmark based on the images from the nuScenes dataset~\cite{caesar2020nuscenes}, specifically designed to provide a more granular understanding of the limitations of current  methods compared to existing benchmarks. Such understanding is critical to identify the weaknesses and to keep improving the performance of camera pose estimation methods.
The nuScenes dataset offers an important diversity, featuring both broad and narrow streets, large and small buildings, as well as vegetation and rivers. Additionally, the images were taken by cameras mounted on a car, oriented in multiple directions. As a result, each scene was captured from numerous viewpoints and at varying distances, making these images an ideal testbed for camera pose estimation.

More exactly, we structured \BN along 3 different types of challenges, as illustrated in~\cref{fig:teaser}. We quantified these challenges in terms of (1) overlap percentage between the two input views, (2) difference of apparent scale between the views, and (3) the difference of view angles: Small overlaps, large scale ratios, and large perspective differences make estimating the camera motion between the images challenging, and can happen alone or simultaneously. Let us highlight that we specifically focus on scenes recorded in good weather conditions to isolate and evaluate geometric difficulties without the confounding effect of adverse weathers.

\noindent We design \BN as follows:
\begin{itemize}
    \item \textbf{Camera registration -- }We carefully estimate the camera poses for the images in nuScenes. The nuScenes dataset already provides the camera poses but only within the ground plane, and we thus used COLMAP~\cite{schonberger2016structure} to obtain full 6 degrees of freedom (DoF) poses. We still use the nuScenes 3~DoF camera poses to ensure the recovered 6~DoF poses are correct. 
    
    \item \textbf{Dense co-visibility maps -- }For each pair of nuScenes images from the same scene, we estimate co-visibility maps, as illustrated in~\cref{fig:pipeline_dataset}. This gives us a fine measure of the co-visible regions between the two images. To do so, we developed a surprisingly simple and efficient method using the camera poses for each image pair and their depth and normal maps as predicted by state-of-the-art monocular depth estimators~\cite{piccinelli2024unidepth,yang2024depth}. 
    
    \item \textbf{Difficulty criteria -- }For each pair, we evaluate our three criteria---overlap, ratio of the distances to the scene, and viewpoint angle difference---to quantify the difficulty of estimating the relative pose of a given image pair. Examples of pairs and their criteria are shown in~\cref{fig:teaser}. We then quantize the range of each criterion into a few bins, which results in a 3D grid of 33 boxes with varying levels of difficulty. Each box is populated with 500 image pairs, for a total of 16.5K test pairs.
    
    \item \textbf{Comprehensive benchmarking -- }\BN allows us to provide an extensive evaluation of 14 methods: SIFT~\cite{lowe2004distinctive}, SuperPoint~\cite{detone2018superpoint}, ALIKED~\cite{zhao2023aliked}, DISK~\cite{tyszkiewicz2020disk}, all using the LightGlue matcher~\cite{lindenberger2023lightglue}, XFeat and its variants XFeat* and XFeat-LighterGlue~\cite{potje2024xfeat}, DeDoDe v2~\cite{edstedt2024dedode}, LoFTR~\cite{sun2021loftr}, ASpanFormer~\cite{chen2022aspanformer}, RoMa~\cite{edstedt2024roma}, Efficient LoFTR~\cite{wang2024efficient}, DUSt3R~\cite{wang2024dust3r} and MASt3R~\cite{leroy2024grounding}.
    Our results show that while most methods accurately estimate the poses under high overlap, similar scale, and small relative viewpoint angle, even the best performing method struggles to correctly estimate the pose for more than 45\% of the image pairs for a threshold of 5Â° (rotation) and 2m (translation). These findings highlight \BN's utility in assessing and comparing different approaches.
\end{itemize}

We believe that \BN will serve as a valuable resource for the computer vision community, encouraging the development of more robust, occlusion-aware, or curriculum learning-based camera pose estimation methods. By providing both a comprehensive dataset and demonstrating the practical benefits of our co-visibility maps, we aim to advance research in this critical area of computer vision.
