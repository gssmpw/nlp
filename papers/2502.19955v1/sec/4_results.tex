\section{Results}
\label{sec:results}

We evaluate 14 image matching methods on our novel \mbox{\BN} benchmark to assess their performance across different geometric challenges. In this section, we first describe our evaluation protocol, then present a comprehensive analysis of both detector-based and detector-free approaches.

\subsection{Evaluation Protocol}
For a fair comparison of all the considered methods, for each image pair in our benchmark, we follow the same evaluation pipeline:

\begin{enumerate}
    \item \textbf{Image Matching -- }We first obtain matches between the two images using each method's default parameters (e.g. number of keypoints, backbone size, input image resolution etc.) and pre-trained weights.

    \item \textbf{Pose Estimation -- }Using these matches, we estimate the essential matrix using MAGSAC++~\cite{barath2020magsac++} from \textsc{OpenCV}, with a threshold of 0.5 pixel. From this essential matrix, we recover the relative rotation and the translation direction between the two views.

    \item \textbf{Scale Recovery -- }To obtain metric translations, we leverage depth predictions from UniDepth~\cite{piccinelli2024unidepth} at matched locations, following the approach in~\cite{leroy2024grounding}. This provides us with a metric scene scale, enabling full 6~DoF pose estimation.
\end{enumerate}

We consider a pose estimation successful when both the rotation error is less than 5째 and the translation error is less than 2m. These thresholds were chosen based on typical requirements in real-world applications such as autonomous navigation and the precision of our ground truth poses, which were obtained through careful COLMAP reconstruction and alignment with nuScenes metric poses (see~\cref{sec:lifting}).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/ablation_no_samples.pdf}
    \caption{\textbf{Impact of sample size on evaluation reliability -- }
    We analyze how the number of image pairs per difficulty level affects the stability of performance metrics for LoFTR~\cite{sun2021loftr}. Left: Percentage of pairs with rotation error $<$ 5째. Right: Percentage of pairs with translation error $<$ 2m. The plots demonstrate that 500 pairs per difficulty level provide stable evaluation metrics, with minimal variance when increasing the sample size further than about 400 pairs. We obtain similar conclusions across all evaluated methods.
    }
    \label{fig:ablation_no_samples}
\end{figure}

\subsection{Learning-based Image Matching Methods}
Before presenting our benchmark results, we first provide a brief overview of recent advances in learning-based image matching methods. Recent years have seen significant advances in learning-based image matching approaches. These can be broadly categorized into detector-based and detector-free methods.

\paragraph{Detector-based methods} build upon traditional keypoint detection and description paradigms. SuperPoint~\cite{detone2018superpoint} pioneered self-supervised interest point detection and description. Recent works like DISK~\cite{tyszkiewicz2020disk}, ALIKED~\cite{zhao2023aliked}, and XFeat~\cite{potje2024xfeat} have further improved efficiency and accuracy. LightGlue~\cite{lindenberger2023lightglue} focuses on accelerating the matching process while maintaining high accuracy. DeDoDe v2~\cite{edstedt2024dedode} specifically addresses the challenges of keypoint detection reliability.

\paragraph{Detector-free methods} take a different approach by directly establishing dense correspondences between images. LoFTR~\cite{sun2021loftr} introduced transformer-based architectures for local feature matching without explicit keypoint detection. Recent works like RoMa~\cite{edstedt2024roma} and Efficient LoFTR~\cite{wang2024efficient} have improved both the efficiency and accuracy of dense matching approaches. DUSt3R~\cite{wang2024dust3r} introduces a paradigm shift by reformulating the matching problem as pointmap regression without requiring camera calibration or pose information, enabling joint optimization of 3D reconstruction and matching. Building upon this, MASt3R~\cite{leroy2024grounding} explicitly grounds the matching process in 3D space and introduces an efficient reciprocal matching scheme that significantly improves both speed and accuracy, particularly for challenging viewpoint changes. These 3D-aware approaches demonstrate substantial improvements over traditional 2D matching methods, especially in scenarios with extreme viewpoint variations.

\subsection{Main Results}
We evaluate the performance of 14 methods on our benchmark, including both detector-based approaches (SIFT~\cite{lowe2004distinctive}, SuperPoint~\cite{detone2018superpoint}, DISK~\cite{tyszkiewicz2020disk}, ALIKED~\cite{zhao2023aliked}, all using LightGlue~\cite{lindenberger2023lightglue}, XFeat~\cite{potje2024xfeat} and its variants, DeDoDe v2~\cite{edstedt2024dedode}) and detector-free approaches (LoFTR~\cite{sun2021loftr}, ELoFTR~\cite{wang2024efficient}, ASpanFormer~\cite{chen2022aspanformer}, RoMa~\cite{edstedt2024roma}, DUSt3R~\cite{wang2024dust3r} and MASt3R~\cite{leroy2024grounding}). For fair runtime comparison, all experiments were conducted on the same NVIDIA RTX 4090 GPU. ~\cref{tab:benchmark_results} presents the overall ranking of these methods, along with their computational efficiency.

\begin{table}[ht]
    \caption{\textbf{Benchmark Results -- }Average ranking across all difficulty levels (lower is better), based on the percentage of successful pose estimations (rotation error $<$ 5째 and translation error $<$ 2m). We also report the overall percentage of successful pose estimations as well as the median runtime per image pair in milliseconds. Best and second-best values are shown in \textbf{bold} and \underline{underlined} respectively.}
    \label{tab:benchmark_results}
    \resizebox{!}{0.47\linewidth}{
        \begin{tabular}{lccc}
            \toprule
            Method & Avg. Rank & Success & Time \\
                   &           &  (\%)   & (ms) \\
            \midrule
            \multicolumn{4}{l}{\textit{Detector-based methods}} \\
            ALIKED+LightGlue~\cite{zhao2023aliked} & \textbf{5.3} & \textbf{36.8} & \underline{45} \\
            DISK+LightGlue~\cite{tyszkiewicz2020disk} & \underline{5.4} & \underline{35.9} & 69 \\
            SP+LightGlue~\cite{detone2018superpoint} & 6.1 & 35.7 & \textbf{43} \\
            SIFT+LightGlue~\cite{lowe2004distinctive} & 7.3 & 33.1 & 194 \\
            DeDoDe v2~\cite{edstedt2024dedode} & 8.6 & 30.4 & 282 \\
            XFeat~\cite{potje2024xfeat} & 13.1 & 14.2 & 54 \\
            XFeat*~\cite{potje2024xfeat} & 12.5 & 15.1 & 82 \\
            XFeat+LighterGlue~\cite{potje2024xfeat} & 9.0 & 30.1 & \textbf{43} \\
            \midrule
            \multicolumn{4}{l}{\textit{Detector-free methods}} \\
            LoFTR~\cite{sun2021loftr} & 10.8 & 24.9 & 185 \\
            ELoFTR~\cite{wang2024efficient} & 9.5 & 26.6 & \underline{124} \\
            ASpanFormer~\cite{chen2022aspanformer} & 9.8 & 24.8 & \textbf{108} \\
            RoMa~\cite{edstedt2024roma} & 2.7 & 47.3 & 614 \\
            DUSt3R~\cite{wang2024dust3r} & \textbf{2.4} & \textbf{54.8} & 257 \\
            MASt3R~\cite{leroy2024grounding} & \underline{2.5} & \underline{53.6} & 173 \\
            \bottomrule
        \end{tabular}
    }
\end{table}
Our benchmark, here aggregated, reveals several key findings:

\begin{enumerate}
    \item \textbf{Detector-free dominance -- }The top three performing methods (DUSt3R, MASt3R and RoMa) are all detector-free approaches, suggesting that direct dense matching is more robust across varying geometric conditions.

    \item \textbf{Speed-accuracy trade-off -- }While detector-free methods achieve better accuracy, they generally require more computation time. Detector-based methods, especially ALIKED or DISK, combined with LightGlue, offer competitive performance with significantly lower runtime (40-70ms vs. 150-600ms).

    \item \textbf{Impact of matching strategies -- }The quite significant performance gap between XFeat variants (with and without LighterGlue) highlights the importance of the matching strategy, even with the same feature detector.

    \item \textbf{Recent advances -- }The newest methods (DUSt3R, MASt3R, RoMa) show substantial improvements over their predecessors (LoFTR, ASpanFormer), demonstrating the rapid progress in the field.
\end{enumerate}

These results demonstrate that while recent detector-free methods achieve the best performance across our benchmark's diverse geometric challenges, detector-based approaches remain competitive, especially when computational efficiency is a priority.
\begin{figure*}[ht]
    \centering
    \begin{subfigure}{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/benchmarks_main/xfeat.pdf}
        \caption{XFeat~\cite{potje2024xfeat}}
        \label{fig:xfeat}
    \end{subfigure}
    \begin{subfigure}{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/benchmarks_main/aliked+lightglue.pdf}      
        \caption{ALIKED+LightGlue~\cite{zhao2023aliked}}
        \label{fig:aliked+lightglue}
    \end{subfigure}
    \begin{subfigure}{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/benchmarks_main/roma.pdf}
        \caption{RoMa~\cite{edstedt2024roma}}
        \label{fig:roma}
    \end{subfigure}
    \begin{subfigure}{0.245\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/benchmarks_main/mast3r.pdf}
        \caption{MASt3R~\cite{leroy2024grounding}}
        \label{fig:mast3r}
    \end{subfigure}
    \caption{\textbf{Performance analysis across geometric criteria -- }Success rate either for R\string@5째 or t\string@2m (bottom-left and top-right of each triangle, respectively), for 4 methods (detector-based and detector-free), when projecting results onto individual geometric criteria (see~\cref{fig:boxes_3d}). For each method, we show three plots corresponding to the projection over overlap (top), scale ratio (middle), and viewpoint angle (bottom).}
    \label{fig:fine_grained}
\end{figure*}

In~\cref{fig:fine_grained}, we show for 4 methods how their performance varies with specific geometric challenges. Results for all other methods are in the Supplementary Material. Overall, detector-free methods show better performance than detector-based methods when dealing with high geometric challenges.
Specifically, DUSt3R and MASt3R show a better balance between speed and performance, allowing for stable results without steep degradation under tougher conditions, as is the case for most of the detector-based methods. Those fine grained findings emphasizes the recent advancements made by newer dense matching methods over traditional keypoint-based approaches. These improvements, especially in challenging scenarios, underline the value of incorporating visibility-aware features and dense matching strategies into camera pose estimation.
Our findings indicate that although most methods provide accurate pose estimates under conditions of high overlap, similar scale, and small relative viewpoint angles, even the top-performing method fails to correctly estimate the pose for over 45\% of image pairs when using thresholds of 5째 (rotation) and 2m (translation). These results underscore the value of \BN in evaluating and comparing various approaches.
