
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{multirow}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}        % special math notations like \mathbb{R}
\usepackage{comment}
\usepackage{algpseudocode}  % pseudo codes
\usepackage{algorithm}      % pseudo codes
\def\red#1{\color[rgb]{0.9, 0.1, 0.1}  \textrm{#1} \color{black}}
\usepackage{listings}
\usepackage{xcolor}

\def\tb#1{\color[rgb]{0, 0, 0}  \textrm{#1} \color{black}}
% \def\tb#1{\color[rgb]{0.7, 0.1, 0.1} \textbf{ \textit{#1} }\color{black}}

\title{A Deep Inverse-Mapping Model \\ for a Flapping Robotic Wing}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Hadar Sharvit$^{1,2,3}$, Raz Karl$^{1,2,3}$ \& Tsevi Beatus$^{1,2,3}$ \thanks{Webpage: \href{https://www.beatus-lab.org}{https://www.beatus-lab.org}} \\
$^1$ School of Computer Science and Engineering\\
$^2$ The Institute of Life Sciences\\
$^3$ Center for Bioengineering \\
The Hebrew University of Jerusalem, Israel 9190401 \\
\texttt{\{hadar.sharvit1, raz.karl ,tsevi.beatus\}@mail.huji.ac.il} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
% The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
% right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
% The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
% line spaces precede the abstract. The abstract must be limited to one
% paragraph.
In systems control, the dynamics of a system are governed by modulating its inputs to achieve a desired outcome. For example, to control the thrust of a quad-copter propeller the controller modulates its rotation rate, relying on a straightforward mapping between the input rotation rate and the resulting thrust. This mapping can be inverted to determine the rotation rate needed to generate a desired thrust. However, in complex systems, such as flapping-wing robots where intricate fluid motions are involved, mapping inputs (wing kinematics) to outcomes (aerodynamic forces) is nontrivial and inverting this mapping for real-time control is computationally impractical. Here, we report a machine-learning solution\footnote{Framework, models, and data are publicly available on \href{https://github.com/Hadar933/AdaptiveSpectrumLayer}{github.com/Hadar933/AdaptiveSpectrumLayer}} for the inverse mapping of a flapping-wing system based on data from an experimental system we have developed. Our model learns the input wing motion required to generate a desired aerodynamic force outcome. We used a sequence-to-sequence model tailored for time-series data and augmented it with a novel adaptive-spectrum layer that implements representation learning in the frequency domain. To train our model, we developed a flapping wing system that simultaneously measures the wing's aerodynamic force and its 3D motion using high-speed cameras. We demonstrate the performance of our system on an additional open-source dataset of a flapping wing in a different flow regime. Results show superior performance compared with more complex state-of-the-art transformer-based models, with 11\% improvement on the test datasets \tb{median loss}. \tb{Moreover, our model shows superior inference time, making it practical for onboard robotic control.}
Our open-source data and framework may improve modeling and real-time control of systems governed by complex dynamics, from biomimetic robots to biomedical devices.
\end{abstract}


\section{Introduction}
In machine learning frameworks that model causal relationships, \textit{e.g.}, for prediction, causes are typically mapped to their effects. 
This forward mapping serves a wide range of applications, for example, predicting the motion of a mechanical system based on the forces acting on it \citep{dearden2005learning}, or in weather forecasting \citep{yu2024climsim}, in which data on previous atmospheric conditions are used to forecast future weather. 
In other cases, though, rather than predicting a system's response to a set of conditions or inputs, it is required to \emph{control} the system by modulating its inputs to achieve a desired outcome.
Hence, systems control would benefit from an \textit{inverse} mapping, which flips the causal relationship by mapping the desired outcome to the input that would have led to this outcome. Such inverse mapping would enable the design of a controller that applies these inferred inputs to achieve desired behaviors. 
If the relationship between causes and their effects can be readily inverted, then an inverse-mapping control approach is useful and simple to implement. For example, in many robotic systems,  a desired mechanical motion (outcome) can be directly mapped to the forces and torques that can generate it (cause) \citep{nguyen2011model}. 

\begin{figure}[b!]
\centering\includegraphics[width=1.0\columnwidth]{figures/fig1_combo_v2a.pdf}
    \caption{\textbf{Forward vs. inverse mapping of a physical system.} $(a)$ In forward mapping, a model $f_\mathrm{fwd}(t)$ predicts system outcomes based on the tracked system dynamics, \textit{e.g.}. Given the history of the wing motion, predicting the current lift force generated by the wing. $(b)$ In inverse mapping, a model $f_\mathrm{inv}(t)$ takes in future/desired system outcomes to infer the inputs that generate them. For the wing, using the future lift force to predict what wing motion created this force. $(c)$ A diagram of a wing driven by a motor, with force and camera sensors. (d) Experimental setup: sample images from the two fast cameras, showing the wing and its markers. (e) The 3D position of the wing in motion. The yellow triangle represents the triangulation of the three markers and colored lines indicate the markers' trajectories. Two black arrows show the cameras' viewpoint.}
    \label{fig:forward-vs-inverse}
\end{figure}


Here, in contrast, we address the case where the forward mapping between inputs and outcomes is nontrivial and difficult to calculate and invert. Examples of such systems include the mapping between pacemaker signals and a desired heart activity \cite{pacemakers-algorithms}, and the mapping from injected insulin dosage to the resulting blood glucose level in a specific patient \citep{insulin-algorithms}. 
An additional noteworthy class of such systems is systems involving complex fluid motion, such as a flapping wing of an insect or a flapping-wing micro-air-vehicle (FW-MAV) \citep{dickinson1999, sane2002aerodynamic, tu2020untethered, Bayiz_Cheng_2021, keennon2012development, ma2013controlled, jafferis2019untethered, karasek2018tailless, coleman2015design, nguyen2019development}, illustrated in Fig. \ref{fig:forward-vs-inverse} and Supplementary Movie 1. 
The fluid dynamics in these systems are highly nonlinear and complex \citep{sane2003aerodynamics, ellington1996leading}: the wing induces intricate vortex structures that determine the aerodynamic forces; during flapping the wing interacts with its own, previously generated, flow field, which introduces complex time dependencies; and, finally, flapping wings often deform due to their elasticity and interaction with the flow, which then effects back on the flow itself, and so on, resulting in a complex fluid-structure interaction \citep{shyy2010recent, nakata2012fluidstructure, miller2009flexible, young2009details}. 
Therefore, the forward mapping from wing motion (cause) to the aerodynamic force (outcome) often requires either using a mechanical, scaled-up flapping-wing analog \citep{dickinson1999, Bayiz_Cheng_2021, whitney_wood_2010, ellington1996leading, muijres_flies_2014, hsu2019speed, melis2024machine}, or numerical solution of the Navier-Stokes flow equation, which is highly computationally intensive and impractical for online system control. Quasi-steady-state approximations of the aerodynamic force are available and relatively simple to invert \citep{dickinson1999, sane2002aerodynamic, whitney_wood_2010, nakata_liu_bomphrey_2015}, however, they might become less accurate on sub-wingbeat resolution and are, hence, typically used for evaluating wingbeat-averaged forces \citep{bomphrey_smart_2017, dickinson1999, brunton2013reduced}. 

Current FW-MAV designs circumvent the complexity of fluid dynamics by using a set of single-axis linear controllers based on insect-inspired control heuristics with manually-tuned parameters \citep{keennon2012development, ma2013controlled, coleman2015design, karasek2018tailless, nguyen2019development,  jafferis2019untethered, tu2020untethered}. Although this simplified approach has made stable flight and maneuvers possible, it might be sub-optimal in allowing these vehicles to exploit their full performance envelope and achieve the remarkable agility and robustness of flying insects and hummingbirds. 
The aerodynamics of a flapping wing is, therefore, an appealing and practical test-bed for modeling the inverse mapping of a complex system. 
In this work, we address this complex inverse-mapping problem using deep-learning tools. To the best of our knowledge, there has been limited exploration of this approach in the existing literature. 

Here, we present a machine-learning model for the inverse mapping of experimental flapping-wing systems that learns the input wing motion required to generate a desired aerodynamic force outcome. We use a sequence-to-sequence model \citep{bahdanau2014neural} that we tailored for time-series data. Our framework employs a bidirectional recurrent neural network (RNN) backbone \citep{schuster1997bidirectional} combined with a time-attention mechanism and an Adaptive Spectrum Layer (ASL), which uses both amplitude and phase information to capture the intricate dependencies between motion and resulting forces in both the time and frequency domains. To generate a dataset of wing kinematics and aerodynamic forces for training and testing our model, we developed a mechanical flapping-wing system. Our system controls wing motion, measures it in 3D using two fast cameras, and simultaneously measures the forces generated by the wing. We trained and tested our model also on an open-source dataset of a flapping wing operating in a different flow regime \citep{Bayiz_Cheng_2021, bayiz2021flapping_dataset}. 
Our approach demonstrates comparable and even superior performance when compared with state-of-the-art transformer-based models \citep{vaswani2023attention}, with 11\% improvement on the test datasets.
The ASL performs representation learning in Fourier space, allowing for the mitigation of noise and amplification of important frequencies, which becomes especially helpful in analyzing periodic systems such as flapping wings.
Learning such inverse-mapping problems may directly improve the control of systems governed by complex dynamics, such as fluid motion. In FW-MAVs, for example, integrating such a trained network into the flight controller would enable efficiently calculating the wing kinematics required for exerting desired forces and torques on the vehicle, thereby exploiting its full performance envelope. We believe that this framework can apply to other complex domains, from robotics to biomedical devices.


\section{Related Work}
\subsection{Forward-mapping modeling}
 
 Several approaches have been used for forward mapping modeling of flapping wing systems, that is, finding the aerodynamic forces resulting from a given wing motion. One direct method is measuring the aerodynamic forces on a scaled-up wing model flapping in a fluid and mimicking the motion of, for example, experimentally measured kinematics of an insect's wing \citep{dickinson1999, Bayiz_Cheng_2021, whitney_wood_2010, ellington1996leading, muijres_flies_2014, hsu2019speed}. With proper scaling of the wing motion and fluid viscosity, the forces measured on the scaled-up model can be rescaled back to the corresponding insect forces.  
 Another method is Computational Fluid Dynamics (CFD), where the Navier-Stokes flow equation is numerically solved on a spatial grid and the aerodynamic forces on the wing are then calculated from the solved flow. While CFD has been instrumental in understanding the fluid dynamics of flapping wings \citep{dickinson2016aerodynamics,  nakata_liu_bomphrey_2015}, insect stability\citep{gao_perturbation_2011, sun2014insect, perl2023lateal}, and complex fluid-structure interactions\cite{young2009details, shyy2010recent, nakata2012fluidstructure, miller2009flexible}, this class of methods is computationally intensive and, hence, impractical for inverse modeling in a real-time flight controller. 
 A dramatic simplification is offered by quasi-steady-state (QS) aerodynamic models, which approximate the aerodynamic force of a wing as a function of its instantaneous motion \cite{dickinson1999, sane2002aerodynamic, weisFogh1973quick, whitney_wood_2010}. For a specific wing geometry, QS models can be calibrated and tuned based on a scaled-up mechanical wing model  \cite{dickinson1999, whitney_wood_2010} or CFD simulations\cite{nakata_liu_bomphrey_2015}. Because these models provide an analytical form of the aerodynamic force, they can, in principle, be inverted and used for real-time control. Yet, QS models neglect complex flow-related features, for example, wing vorticity, wing interaction with its previously generated flow, and fluid-structure interaction, which may be important for utilizing the full capabilities of FW-MAV.  

Deep learning models have been applied for problems in fluid dynamics, such as turbulence and flow control\cite{Ling2016Reynolds, brunton2016discovering, duraisamy2019turbulence}, \tb{and for mitigating windy conditions and structural damage in quad-copter control \cite{oconnell2022neural}. The impressive achievements in quad-copter control do not require significant inverse modeling due to the relatively simple mapping between desired forces and torques and rotor speed. This is markedly different than the complex aerodynamics of flapping wings.}
For a flapping wing, deep learning models enable capturing the complex forward mapping from wing motion to aerodynamic forces without simplifying assumptions. In this approach, a model is trained on a dataset of measured or calculated forces obtained from a scaled-up mechanical model or CFD, respectively, based on a set of predefined wing kinematics. A trained model can potentially predict these forces for given input kinematics and do so much faster than CFD models and potentially more accurately than QS models. The applicability of such a model depends, naturally, on the quality and breadth of the dataset it has been trained on.
In 2021, Bayiz and Cheng introduced a state-space deep learning approach that accurately predicted aerodynamic forces using data from a scaled-up mechanical wing \cite{Bayiz_Cheng_2021}. Their model, trained and tested with 548 diverse wing kinematics, demonstrated the predictability of aerodynamic forces based on a half-wingbeat look-back window of the previous wing kinematics. Here, we invert this dataset to model the system's inverse aerodynamics.

Beyond aerodynamics, forward mapping modeling applies to simulating diverse mechanical systems, such as humanoid robot motion \cite{humanoid_gym_env}. In these simulations, the state of the system encompasses generalized positions and velocities, governed by equations of motion incorporating inertia tensors, external forces, and control inputs. Using a semi-implicit Euler integration scheme enables the iterative calculation of the system's state based on applied actions. %This 

\subsection{Time Series Modeling and Fourier Analysis}

Time-series modeling has been fundamental across various disciplines, including climate modeling \cite{mudelsee2019trend}, biological sciences \cite{stoffer2012special}, and finance \cite{bose2017probabilistic}. Traditional methods such as auto-regressive and exponential smoothing rely on domain expertise \cite{box2015time}, but modern machine learning techniques are increasingly adopted owing to their data-driven nature and scalability \cite{time_series_survey}.
Recent architectures that use FFT (Fast Fourier Transform) include the following: 
AutoFormer features a distinct architecture: the encoder emphasizes modeling the data periodicity, while the decoder includes accumulation structures for trend-cyclical components and stacked auto-correlation mechanisms for periodic components. This mechanism replaces traditional self-attention methods and efficiently computes auto-correlation using FFT \cite{wu2021autoformer}. Similarly, FedFormer introduces low-rank approximated transformation in the frequency domain to expedite attention mechanisms in time series forecasting \cite{zhou2022fedformer}. Adaptive Temporal-Frequency Networks utilize FFT to extract trend and periodic features for improved forecasting accuracy \cite{adapt-temp-freq-net}. StemGNN employs Graph Fourier Transform (GFT) and Fourier transform to capture inter-series correlations and temporal dependencies effectively \cite{cao2021spectral}. Notably, StemGNN automatized the learning of inter-series correlations from data, leveraging spectral representations for prediction.

In Fourier Neural Operator (FNO), a parameterized low-pass filter in Fourier space facilitates the learning of mappings from functional parametric dependencies to solutions, thus enabling the exploration of a broad family of partial differential equations \cite{li2020fourier}. Additionally, the random Fourier method and random Fourier softmax (RF-softmax) technique offer efficient and accurate random sampling, exploiting frequency-space features \cite{rawat2019sampled}.

The adaptive spectrum layer (ASL) reported here uses both the magnitude and phase of the Fourier spectra for representation learning, weighs frequency bins accordingly, and functions as a standalone representation layer. In contrast, other approaches often rely on FFT for efficient computation in Fourier space, as well as overlook the comprehensive information provided by the Fourier transform, or lack a gated weighing mechanism that considers all information from other frequency bins.



\section{Methods}

We developed a flapping wing system (Fig. \ref{fig:forward-vs-inverse}c) in which we measured the wing kinematics using high-speed cameras. The wing degrees of freedom were characterized by $M_K{=}3$ Euler angles (Fig. \ref{fig:angle-notation}): the stroke angle $\phi$, elevation angle $\theta$, and wing pitch angle $\psi$. The aerodynamic forces were measured in sync with the wing kinematics using $M_F{=}4$ vertical force sensors. 
The essence of inverse-mapping modeling in this system lies in learning the relationship between desired output aerodynamic forces and the input wing kinematics that generate them. \tb{Intuitively, the input to the system is a time sequence of desired aerodynamic forces that the wing should generate. The output is the full wing kinematics that, when applied to the wing, would result in the desired forces. The system learns the inverse mapping in two parallel backbones: learning the time-dependent (Seq2Seq) and frequency-dependent (ASL) relationships between the forces. As a result, both the timestamps and frequencies that are relevant for the mapping are learned.}

\subsection{Multi-variate Multi-target Time series Framework}
\textbf{Problem Definition}

We reformat the generalized time series formalism to our inverse-mapping modeling framework of \{desired future outcomes\}$\rightarrow$\{required input kinematics\}. Without loss of generality, we consider wing kinematics (Fig. \ref{fig:angle-notation}) as required system input, and system-measured forces, provided from a set of sensors (Fig \ref{fig:forward-vs-inverse}c) as desired future outcomes. given $N$ distinct time-series events that represent data from $M_F$ (force) sensors $ \left\{\{ F_{1:t_0}^{i,j} \}_{j=1}^{M_F} \right\}_{i=1}^N$ where $ F_{1:t_0}^{i,j}\in\mathbb{R}^{t_0}$ indicates the force values of the $j$'th sensor in the $i$'th dataset at times $1,2,...,t_0$. 
Our goal is to predict the corresponding $N$ distinct wing kinematics $ \left\{\{ K_{1:t_0}^{i,j} \}_{j=1}^{M_k} \right\}_{i=1}^N$ where $ K_{1:t_0}^{i,j}\in\mathbb{R}^{t_0}$, indicates the kinematic values of the $j$'th degree of freedom in the $i$'th event at times $1,2,...,t_0$. To simplify our notation, we will mostly describe the kinematics in terms of three angles ($M_K{=}3$) $\{\phi_{0:t_0}^i, \theta_{0:t_0}^i \psi_{0:t_0}^i\}_{i=1}^N$, where $\phi_t^i,\theta_t^i,\psi_t^i \in \mathbb{R}$ represent the stroke, elevation, and pitch angle at time $t$ in the $i$'th event, respectively (Fig. \ref{fig:angle-notation}). 
Formally, we aim to model the following conditional probability distribution:
\begin{align}
\begin{split}
p\big(\phi_{t=0:t_0}^i, \theta_{t=0:t_0}^i \psi_{t=0:t_0}^i \big| \bigl\{ F_{t_0: t_0+\tau}^{i,j} \bigr\}_{i,j=1}^{M_F}; \Phi\big)
\end{split}
\label{eq: cond distribution}
\end{align}


\begin{figure}[t]
\centering\includegraphics[width=0.7\columnwidth]{figures/angles-notation-large-font.drawio.pdf}
\caption{\textbf{Wing degrees-of-freedom.} The three angles of wing rotation: elevation angle $\theta$ (left, shown in a side view), wing-pitch angle $\psi$ (middle, shown in a front view), and wing-stroke angle $\phi$ (right, shown in a top view). The $\hat{x}$, $\hat{y}$, $\hat{z}$ vectors represent the Cartesian lab frame of reference.}
\label{fig:angle-notation}
\end{figure}
% if needed, instruction for side caption: https://tex.stackexchange.com/questions/29143/caption-on-the-side-of-a-figure

This is the probability of wing kinematics at times $t {\in} [0, t_0]$ given the set of future forces at times $t'{\in}[t_0, t_0+ \tau-1]$ at a fixed future window of size $\tau$ that they generated. These, alongside the learnable parameters $\Phi$, are optimized to minimize loss using an SGD-like process. In practice, we reduce the problem to learning a one-step-ahead prediction model, for any $i {\in} [N]$
\begin{align}
    \begin{split}
    p\big(\phi_t^i, \theta_t^i, \psi_t^i \big| \bigl\{ F_{t: t+\tau}^{i,j} \bigr\}_{j=1}^{M_F}; \Phi\big),
    \end{split}
    \label{eq: one-step cond distribution}
\end{align}
where an optimal model $f$ for which $\phi^i_t,\theta^i_t,\psi^i_t \sim f\left(\bigl\{F^{i,j}_{t:t+\tau}\bigr\}_{j=1}^{M_F}\right)$ is explored, to predict the distribution of wing kinematics at time $t$ given the measured forces at time $t$.

\subsection{Sequence-to-Sequence}
We instantiate our model $f$ as a Seq2Seq model \cite{bahdanau2014neural}, adjusted for time series. More precisely, we utilize an RNN (Recurrent Neural Network) encoder embedding alongside an RNN decoder intertwined with an attention mechanism to predict the next time step given all previous window values (Fig. \ref{fig:Seq2Seq Framework}).
The encoder transforms the input time series data into an embedded representation using an Adaptive Spectrum Layer (see below), followed by a bidirectional GRU (Gater Residual Network). The final hidden states from both forward and backward passes are concatenated and reshaped through a linear layer to serve as the initial state for the decoder.
Crucially, an attention mechanism computes attention weights based on the decoder's current hidden state and all the encoder's outputs. This ensures the decoder focuses on relevant time steps from the encoder when predicting subsequent values. The decoder starts with the last value of the input sequence and, guided by the attention mechanism, iteratively generates predictions for the forecast horizon. 

\paragraph{Adaptive Spectrum Layer.}
\label{paragraph: asl}
Motivated by the potential benefits of frequency domain analysis,  we introduced the Adaptive Spectrum Layer (ASL) layer, shown in Fig. \ref{fig:Seq2Seq Framework}. 
The ASL takes in the raw input signal and applies representation learning in Fourier space. It then combines the new representation and the signal via a skip connection and propagates this output to the intermediate layers of the neural network in which it is encapsulated.

The forward pass begins by applying a real-valued (symmetric) Fourier transform (FFT) on the raw input signal $x$. Then, a low-pass filter is used to retain relevant frequency information below a set frequency. The magnitudes $r_{f_i}$ and phases $\theta_{f_i}$ of each Fourier component (the $i$'th bin) are then concatenated to form a new complex tensor $\hat{x}$, in which every entry consists of the respective magnitude and the phase represented by its sine and cosine values. 
We then use a fully connected layer (FC) with additional non-linearity to derive a new representation from $\hat{x}$.
Subsequently, a dropout layer is applied to the stacked hidden representation $h_i,...,h_{f_N}$, after which a fully connected layer transforms the representation to a bounded weight vector $w_1,...,w_{f_N}$ using a simple gating mechanism. In this vector, every entry corresponds to the weight associated with its respective frequency bin in Fourier space. Lastly, the learned weights are point-wise multiplied with the original complex vector, and an inverse Fourier transform (IFFT) is used to convert the new signal back to the time domain.


\begin{figure}[t]
\centering\includegraphics[width=.95\columnwidth]{figures/Seq2Seq_shorten.drawio.pdf}
\caption{\textbf{System architecture: Seq2Seq with ASL.} The input sequence $x$ is encoded by an adaptive spectrum layer (ASL). ASL conducts representation learning in Fourier space, assigning weights to each frequency bin using the entire complex signal, and then reverting to the time domain via IFFT. A skip connection is added from input to representation. Subsequently, a GRU encoder generates a fixed-size representation. Following this, the attention (fully connected, FC) mechanism utilizes the current decoder hidden state and encoder context vector to compute attention weights $w_t$. The last encoder state is employed instead of the (non-existent) decoder state in the initial iteration. These attention weights adjust the encoder context vector based on the current decoder hidden state. Finally, the resulting weighted tensor passes through a GRU-based decoder to predict the next step $\hat{y}_t$, with $T$ representing the prediction window size.}
    \label{fig:Seq2Seq Framework}
\end{figure}

\section{Datasets}

\subsection{Our Measured Dataset: a Wing Flapping in Air}
\label{subsec:our-dataset}
\paragraph{A Flapping Wing System.}
We developed a flapping wing system (Fig. \ref{fig:forward-vs-inverse}c-e, Supplementary Movie 1) that can move with a predetermined stroke angle profile $\phi(t)$, and with passively-determined elevation $\theta(t)$ and pitch $\psi(t)$ angles. That is, $\theta(t)$ and $\psi(t)$ are outcomes of the dynamic interactions between the wing and the surrounding air, wing inertia, and wing elasticity \cite{beatus2015wing}.
The wing consisted of a $12.7 \mathrm{\mu m}$ thick Mylar sheet, with a leading edge made of a carbon-fiber rod $1 \mathrm{mm}$ in diameter. Wing span was $7 \mathrm{cm}$ and its maximum chord was $3 \mathrm{cm}$. A 3D-printed hinge connected the wind to a brushless DC motor (Maxon ECXSP06M BL KL-A-HP-12V) with a built-in 15:1 gear ratio and an angular position encoder. Thus, the motor was driving the wing directly by following a predetermined stroke angle profile $\phi(t)$ using a designated controller we developed.  

\paragraph{Measurement Setup.}
The measurement setup consisted of two modules: a fast-imaging setup that measured wing kinematics (input), and force sensors that measured the vertical force generated by the wing (outcome). The imaging setup included two high-speed cameras (Phantom v2012, Vision Research) in a stereo configuration (Fig. \ref{fig:forward-vs-inverse}c-e, Supplementary Movie 1) with parallel optical axes. The cameras were mutually calibrated to find their intrinsic and extrinsic matrices. In each experiment, the two cameras recorded the flapping wing and operated in sync at 10,000 frames per second. To track the wing, we attached three white circular markers onto the wing's camera-facing surface and tracked the markers' positions in each camera view using basic segmentation and optical flow\cite{optical_flow}. 
Subsequently, we used the 2D markers' trajectories from both cameras to triangulate the position of the markers in 3D in the lab frame of reference based on the cameras' calibration. Finally, under the verified assumption that the wing maintained a flat shape during its motion, we converted the three 3D marker coordinates into the standard Euler angle description for flapping wings of $\phi(t), \theta(t), \psi(t)$.

The second experimental module -- for force measurement -- consisted of four force sensors (SI-USB4, Interface Inc.) arranged in a symmetric cross configuration, with the motor attached at its center (Fig. \ref{fig:forward-vs-inverse}c-e). The sensors measured in sync at 5,000 samples per second, and their four readouts of vertical forces represent the aerodynamic force output generated by the wing. Treating all readouts as separate signals, rather than, for example, summing them, is relevant for torque calculation and phase-related feature extraction. Finally, the two data streams of the wing angles and forces were synchronized and combined to form a single event in our dataset. 



\paragraph{Data Collection.}
We measured a total of $N{=}153$ events that span different wingbeat frequencies and $\phi(t)$ profiles. This dataset is publicly available.
Each event corresponds to a wing trajectory and force data, $2 {-} 6$ wing-beats, or $0.11 {-} 0.75$ seconds, long.
The range of profiles was obtained by using a parameterization for $\phi(t)$ \cite{berman2007energy} that provides a continuum of profiles from sinusoidal to triangular as a function of a single parameter $K{\in}[0,1]$:
\begin{equation}
    \phi(t) = \frac{1}{2} \Phi \: \frac {\sin^{-1}\big( K \sin(2\pi f t)\big)} {\sin^{-1}K} , 
\end{equation}
where $f$ is the wingbeat frequency and $\Phi$ is the stoke peak-to-peak amplitude. Our dataset included events with $f {\in} [0,20]\mathrm{Hz}$ and $\Phi {\in} [\pi/6,\pi/3] \mathrm{rad}$.  Due to the range of wing speeds in the dataset, the wing's Reynolds number was $1{,}000 {-} 50{,}000$, which covers the flow regimes of medium to large insects, small birds, and FW-MAVs.


The resulting dataset (Table \ref{tab:dataset_summary}, Fig. \ref{fig:prediction_examples}) is formulated as $\mathcal{D} =\left\{ \{F_{1:{t_0}_i}^{i,f}\}_{f=1}^{M_F};\mbox{ } \{K_{1:{t_0}_i}^{i,k} \}_{k=1}^{M_K}  \right\}_{i=1}^N$. For each event $i {=} 1, 2, ..., N$ there are $M_F$ force signals with $t_0$ entries and $M_K$ kinematic signals with $t_0$ entries. We use $M_F {=} 4$ (four force sensors) and $M_K=3$ for stroke, pitch, and elevation angles. The upper scripts $f\in[M_F]$ and $k\in[M_K]$ represents the $f$'th force sensor and $k$'th kinematics respectively. Also, note that ${t_0}_i$ is not the same for all $i$, allowing events of different duration.

\begin{table}[b]
    \centering
    \begin{small}
        \begin{tabular}{llllllr}
            \toprule
            \textbf{Dataset}
                    & \textbf{No. of} & \textbf{Samples per} & \textbf{Input} &\textbf{Output} & \textbf{Total}  & \textbf{Sample}  \\
                    & \textbf{Events} & \textbf{event} & \textbf{dim.} &\textbf{dim.} & \textbf{samples}  &  \textbf{Rate} \\
            \midrule
            Our & 153 & 550--3787 & 4 & 3 & 438,552 & 5,000 Hz \\
            Open Source & 548 & 480 &5 & 3& 263,040 & 25 Hz\\
            \bottomrule
        \end{tabular}
    \end{small}
    \caption{\textbf{Datasets.} The two datasets used in the comparative analysis. ``Our'' dataset is the experimental flapping-wing dataset measured in our system, and ``Open Source'' represents the flapping-wing dataset reported in \cite{bayiz2021flapping_dataset}. Each event represents an individual wing kinematic profile. Event duration is counted in samples. }
    \label{tab:dataset_summary}
\end{table}

\subsection{Open Source Dataset: a Wing Flapping in Viscous Fluid}
\label{subsec:open-source-dataset}
The second dataset we used (Table \ref{tab:dataset_summary}, Fig. \ref{fig:prediction_examples}) has been published by %Bayiz and Cheng 
\cite{Bayiz_Cheng_2021}, who measured the wing kinematics and aerodynamic forces of a plate-like wing flapping in mineral oil. Wing kinematics $\{\phi, \theta, \psi \}$ was controlled by step-motors and the aerodynamic forces were measured by three forces- and two torque-sensors. 
The Reynolds number of the system was  ${\sim}1000$, modeling medium-sized insects. 
This dataset included 548 events with various kinematics, each having 480 samples of synchronized kinematics-force data taken at 25Hz. Bayiz and Cheng used this dataset to develop a deep-learning system that learned the forward-mapping: from wing kinematics to forces and torques. Here, we used this dataset in reverse to learn the inverse mapping.






\section{Results}

We trained an individual model on the two datasets, to infer the wing kinematics that generated a given force time-series. The input signals in the two datasets have different dimensionality, units (force vs. force and torque), and sampling rates. Both datasets were randomly divided such that 75\% of the events were used as a training set, 10\% reserved for validation, and 15\% for testing. 
Throughout the training process, a standard hyper-parameter tuning loop was used to search through hyperparameter space including model parameters (\textit{e.g.}, the number of attention heads, and the size of the hidden representation vector) and input window characteristics (history size, batch size, learning rate, normalization schemes, \textit{etc}.). For further technical details see Appendix \ref{apdx:hyperparam-tuning}. 

Model performance was evaluated on the test set using the Mean Absolute Error (MAE) loss of the predicted wing angles. The MAE for each angle was calculated per event, averaged across the three predicted angles per event, and then averaged across each test set. The resulting model architectures were relatively small, with a few hundred to ${\sim} 200{,}000$ parameters. Hence, training one model took ${\leq} 3 \mathrm{min}$ on a single Nvidia RTX 3090 GPU with 24 GB RAM, and an exhaustive hyperparameter search took ${\sim} 50 \mathrm{hr}$ on the same hardware.  

\begin{figure}[t!]
    \centering\includegraphics[width=0.82\columnwidth]{figures/results.drawio.pdf}
    \caption{\textbf{Prediction examples.} Four pairs of input-output scenarios from our dataset (left, described in \ref{subsec:our-dataset}) and the open source dataset \cite{bayiz2021flapping_dataset} (right, described in \ref{subsec:open-source-dataset}). The upper section displays force/torque inputs representing the desired system outcome. In our dataset, these are $F_1, F_2, F_3, F_4$ as depicted in our experimental setup (see Fig. \ref{fig:forward-vs-inverse}c-e). In the open source dataset, the outcome is represented by a set of three measured forces $F_x, F_y$, and $F_z$, and two measured torques $M_y$, $M_z$. In both experiments, the targets are similar and represented in the lower section as the corresponding true angle labels and predicted angles, generated by our adapted Seq2Seq+ASL model trained to model the inverse mapping. Different system outcomes (top) result from different system dynamics (bottom) in each event. The events shown span various wing kinematics.}
    \label{fig:prediction_examples}
\end{figure}

The performance of our models, with and without ASL, on the two test sets are shown in Table 2 
and Fig. \ref{fig:quantiles_median}, with examples of representative events in Fig. \ref{fig:prediction_examples}. Additionally, these results are compared with the performance of several state-of-the-art models on both datasets (implemented in \cite{zeng2023transformers}): Transformer \cite{vaswani2023attention}, AutoFormer \cite{wu2021autoformer}, FedFormer \cite{zhou2022fedformer}, NLinear \cite{zeng2023transformers}, and a Linear model represented fully connected layer that is applied on the flattened input (see the Appendix for further details).


%\resizebox{!}{\.5\paperheight}{
\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{1.9pt} % Default value: 6pt
    \renewcommand{\arraystretch}{1.} % Default value: 1
    \scalebox{1}{
    \begin{tabular}{c|c|cccccccccccc}
        \toprule
        \multicolumn{2}{c|}{Methods} & Linear & AutoFormer & FedFormer & Seq2Seq+ASL   & Seq2Seq            & Transformer     & NLinear \\
        \midrule
        \multirow{2}{*}{Ours}   & Mean    & 0.2662 & 0.2380 & 0.3049 & \textbf{0.1323} & \underline{0.1471} & 0.1476          & 0.2800 \\
                                & Median  & 0.2643 & 0.2271 & 0.2920 & \textbf{0.1216} & \underline{0.1236} & 0.1430          & 0.2836 \\
        \midrule
        \multirow{2}{*}{OpenSrc} & Mean   & 0.3223 & 0.3626 & 0.3003 & \underline{0.1130}   & 0.1206 & \textbf{0.1123} & 0.3258 \\
                                 & Median & 0.3070 & 0.3379 & 0.2875 & \textbf{0.0908} & 0.1058 & \underline{0.1021}          & 0.3131 \\
        \bottomrule
    \end{tabular}
    \label{table:forecasting-results}
    }
    \caption{\textbf{Results.} Comparison of Mean Absolute Error (MAE) in radians between various models on both Ours and the open source datasets, aggregated across all test events using Mean or Median. Best- and second-best-performing models are highlighted in \textbf{bold} and \underline{underscore}, respectively.}
\end{table*}
%}

\begin{figure}%[htp]
\centering
\includegraphics[width=0.65\columnwidth]{figures/mae_test_loss_comparison_v2.pdf}
\caption{\textbf{Comparison with state-of-the-art models.} The distributions of test loss across seven models for two datasets: Our dataset and the open-source dataset \cite{Bayiz_Cheng_2021}. Inside each box, the horizontal line represents the median MAE, the colored box represents the 2nd and 3rd quartiles, and the whiskers represent the 1st and 4th quartiles. Outliers are indicated by open circles. adapted Seq2Seq+ASL model demonstrates superior performance, particularly evident in its median values, outperforming other models.
\tb{Interestingly, Seq2Seq+ASL has more outliers than the Transformer, which explains the difference between their mean and median metrics.}
}
\label{fig:quantiles_median}
\end{figure}


\paragraph{Inference accuracy.} Our results show that the inverse dynamics problem of a flapping wing system can be well-approximated by deep learning models. First, the data in Table 2 shows that the ASL is consistently improving performance by 11.2\%, 1.7\%, 6.7\% and 16.5\% compared with Seq2Seq alone. The improvement offered by the ASL frequency-space representation layer demonstrates the benefit of using such representations, especially for periodic signals. 
Second, Seq2Seq+ASL is consistently the first- or second-best model within the tested models and metrics. On our dataset with MAE metrics, \tb{Seq2Seq+ASL outperforms the Transformer on $2/3$ of the test set (Wilcoxon signed rank test $p_\textrm{value} {=}0.06$) and} is the best-tested model, improving by 10\% concerning standard Seq2Seq and with 10.4\% compared with Transformer. On the same dataset but using the median metrics, Seq2Seq+ASL, and standard Seq2Seq show equivalently best performance, improving by 13.6\% compared with Transformer. 
On the Open Source dataset Seq2Seq+ASL and Transformer perform equally well under the MAE metric, \tb{(Seq2Seq+ASL outperforms the Transformer in 54\% of the test set, Wilcoxon signed rank test $p_\textrm{value} {=}0.58$)}. Under the median-MAE metric, Seq2Seq+ASL is the best-tested model, improving Transformer by 11\%. In most tested cases, the performance of the Seq2Seq+ASL model suggested here is superior to more sophisticated state-of-the-art models. 

\paragraph{Inference time.} \tb{even though Seq2Seq+ASL has ${\times}4$ more parameters (${\sim}200\textrm{k}$) compared with the Transformer model (${\sim}50\textrm{k}$, Appendix), The inference time of Seq2Seq+ASL is  ${\times}10$ shorter compared with Transformer ($2.00{\pm}0.13 \textrm{ms}$ vs. $19.53 {\pm} 5 \textrm{ms}$). This difference makes Seq2Seq+ASL more practical for integration in onboard flight controllers of FW-MAVs, while the transformer latency is too long with respect to the typical wingbeat period of the existing prototypes. 
The scaling of the inference time with the number of parameters of each of these two models shows that Seq2Seq+ASL inference time is practically constant with the number of parameters, while the Transformer inference time increases significantly with the number of parameters (Appendix)}

\paragraph{Ablation tests.} To characterize the functionality of the ASL, we performed a suite of ablation tests on all of its building blocks. We tested the sine/cosine vs. angular representation, low-pass filter cutoff frequency, ASL gating mechanism, ASL per-frequency layer (combining different frequency bins after FFT or not), and learning a new complex number representation from the FFT vector or not. The full results are given in Supplementary Table 1. Briefly, encoding the phase data as sine/cosine was slightly, but not conclusively better than an angular representation (probably due to the improved encoding of periodicity of sine/cosine). Other attributes, such as the low-pass filter cutoff frequency, were highly significant. Testing three different frequencies: 20, 100, and 210Hz, we see that all of the top-10 models had a cutoff frequency ${\geq}$100Hz.




\section{Conclusion}

We presented an inverse-mapping modeling framework, in which the required system inputs (wing kinematics) are predicted given the desired system outcomes (aerodynamic forces). A specific realization of such a model was explored using an experimental flapping wing system from which we collected a dataset capturing the relationship between wing kinematics and the resulting forces. The task was then formulated as an inverse mapping from the output forces of the system to the wing kinematics that generate them. To model this problem, we proposed a deep learning architecture based on a sequence-to-sequence (Seq2Seq) model with an Adaptive Spectrum Layer (ASL). 
The ASL performs representation learning in the Fourier domain, using both amplitude and phase information. As such, the ASL  captures important frequency patterns, which is particularly beneficial for periodic signals like those in flapping wing systems, and filters out undesired noise. 
On both datasets, which represent two flow regimes, our model demonstrated superior performance compared with other state-of-the-art models, achieving up to ${\sim}$11\% improvement. These results support the current view that Transformer-based models are not necessarily optimal for time-series analysis \cite{zeng2023transformers}. Further, our RNN implementation is expected to be significantly more computationally efficient than Transformer models, which would benefit deployment in onboard systems.

Interestingly, on the current task, our model outperformed FedFormer, which also uses frequency data. This may be explained by the fact that FedFormer is a forecasting model in which the input and output dimensions are identical, unlike in our use case. To apply FedFormer to such cases, one had to change its output dimension, which might have hindered its performance. More crucially, to implement an attention mechanism in linear-time complexity, FedFormer implements random frequency sampling on which it builds its frequency-domain representation. Yet, in our case, because we did not use computationally-expensive attention, we could use a frequency-domain representation on the entire spectrum, without data loss.



\paragraph{Limitations.} First, due to the challenges of developing a fully operational flapping robotic device, our model was demonstrated on bench-top device with a single wing. The model could be incorporated into FW-MAVs by training in a similar bench-top configuration and then including it as a module in the onboard controller that converts force and torque commands into wing actuation. Second, due to the limits of our datasets, our system was not trained to generalize on arbitrary wing forms and Reynolds numbers. Further, the wing-hinge in our system has a single axis, which might limit the repertoire of wing motions, compared, for example, with better-controlled wing models. Yet, this simple actuation may, in fact, fit with current FW-MAV designs, since all of them use a similar single-axis drive, relying on fluid-structure interaction to yield complex insect-like wing kinematics.

In summary, despite these limitations, we believe that the proposed inverse-mapping framework could be seamlessly integrated into a wide range of applications and improve the modeling and control of complex systems, from biomimetic robots to biomedical devices. 

\textit{This work was supported by the Israel Ministry of Science and Technology Grant No. 3-17400.}












% ===================================================================================
%  INSTRUCTIONS     INSTRUCTIONS     INSTRUCTIONS     INSTRUCTIONS     INSTRUCTIONS    
% ===================================================================================

\newpage 


\begin{comment}
    

\section{Submission of conference papers to ICLR 2025}

ICLR requires electronic submissions, processed by
\url{https://openreview.net/}. See ICLR's website for more instructions.

If your paper is ultimately accepted, the statement {\tt
  {\textbackslash}iclrfinalcopy} should be inserted to adjust the
format to the camera ready requirements.

The format for the submissions is a variant of the NeurIPS format.
Please read carefully the instructions below, and follow them
faithfully.

\subsection{Style}

Papers to be submitted to ICLR 2025 must be prepared according to the
instructions presented here.

%% Please note that we have introduced automatic line number generation
%% into the style file for \LaTeXe. This is to help reviewers
%% refer to specific lines of the paper when they make their comments. Please do
%% NOT refer to these line numbers in your paper as they will be removed from the
%% style file for the final version of accepted papers.

Authors are required to use the ICLR \LaTeX{} style files obtainable at the
ICLR website. Please make sure you use the current files and
not previous versions. Tweaking the style files may be grounds for rejection.

\subsection{Retrieval of style files}

The style files for ICLR and other conference information are available online at:
\begin{center}
   \url{http://www.iclr.cc/}
\end{center}
The file \verb+iclr2025_conference.pdf+ contains these
instructions and illustrates the
various formatting requirements your ICLR paper must satisfy.
Submissions must be made using \LaTeX{} and the style files
\verb+iclr2025_conference.sty+ and \verb+iclr2025_conference.bst+ (to be used with \LaTeX{}2e). The file
\verb+iclr2025_conference.tex+ may be used as a ``shell'' for writing your paper. All you
have to do is replace the author, title, abstract, and text of the paper with
your own.

The formatting instructions contained in these style files are summarized in
sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

\section{General formatting instructions}
\label{gen_inst}

The text must be confined within a rectangle 5.5~inches (33~picas) wide and
9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
preferred typeface throughout. Paragraphs are separated by 1/2~line space,
with no indentation.

Paper title is 17~point, in small caps and left-aligned.
All pages should start at 1~inch (6~picas) from the top of the page.

Authors' names are
set in boldface, and each name is placed above its corresponding
address. The lead author's name is to be listed first, and
the co-authors' names are set to follow. Authors sharing the
same address can be on the same line.

Please pay special attention to the instructions in section \ref{others}
regarding figures, tables, acknowledgments, and references.


There will be a strict upper limit of 10 pages for the main text of the initial submission, with unlimited additional pages for citations. 

\section{Headings: first level}
\label{headings}

First level headings are in small caps,
flush left and in point size 12. One line space before the first level
heading and 1/2~line space after the first level heading.

\subsection{Headings: second level}

Second level headings are in small caps,
flush left and in point size 10. One line space before the second level
heading and 1/2~line space after the second level heading.

\subsubsection{Headings: third level}

Third level headings are in small caps,
flush left and in point size 10. One line space before the third level
heading and 1/2~line space after the third level heading.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone, regardless of the formatter being used.

\subsection{Citations within the text}

Citations within the text should be based on the \texttt{natbib} package
and include the authors' last names and year (with the ``et~al.'' construct
for more than two authors). When the authors or the publication are
included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
towards AI~\citep{Bengio+chapter2007}.'').

The corresponding references are to be listed in alphabetical order of
authors, in the \textsc{References} section. As to the format of the
references themselves, any style is acceptable as long as it is used
consistently.

\subsection{Footnotes}

Indicate footnotes with a number\footnote{Sample of the first footnote} in the
text. Place the footnotes at the bottom of the page on which they appear.
Precede the footnote with a horizontal rule of 2~inches
(12~picas).\footnote{Sample of the second footnote}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction; art work should not be
hand-drawn. The figure number and caption always appear after the
figure. Place one line space before the figure caption, and one line
space after the figure. The figure caption is lower case (except for
first word and proper nouns); figures are numbered consecutively.

Make sure the figure caption does not get separated from the figure.
Leave sufficient space to avoid splitting the figure and figure caption.

You may use color figures.
However, it is best for the
figure captions and the paper body to make sense if the paper is printed
either in black/white or in color.
\begin{figure}[ht]
\begin{center}
%\framebox[4.0in]{$\;$}
\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
\end{center}
\caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible. Do not use hand-drawn
tables. The table number and title always appear before the table. See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the table
title, and one line space after the table. The table title must be lower case
(except for first word and proper nouns); tables are numbered consecutively.

\begin{table}[t]
\caption{Sample table title}
\label{sample-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
\\ \hline \\
Dendrite         &Input terminal \\
Axon             &Output terminal \\
Soma             &Cell body (contains cell nucleus) \\
\end{tabular}
\end{center}
\end{table}

\section{Default Notation}

In an attempt to encourage standardized notation, we have included the
notation file from the textbook, \textit{Deep Learning}
\cite{goodfellow2016deep} available at
\url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
is not required and can be disabled by commenting out
\texttt{math\_commands.tex}.


\centerline{\bf Numbers and Arrays}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1in}p{3.25in}}
$\displaystyle a$ & A scalar (integer or real)\\
$\displaystyle \va$ & A vector\\
$\displaystyle \mA$ & A matrix\\
$\displaystyle \tA$ & A tensor\\
$\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
$\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
$\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
$\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
$\displaystyle \ra$ & A scalar random variable\\
$\displaystyle \rva$ & A vector-valued random variable\\
$\displaystyle \rmA$ & A matrix-valued random variable\\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Sets and Graphs}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \sA$ & A set\\
$\displaystyle \R$ & The set of real numbers \\
$\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
$\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
$\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
$\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
$\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
$\displaystyle \gG$ & A graph\\
$\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
\end{tabular}
\vspace{0.25cm}


\centerline{\bf Indexing}
\bgroup
\def\arraystretch{1.5}

\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
$\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
$\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
$\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
$\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
$\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
$\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
$\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
\end{tabular}
\egroup
\vspace{0.25cm}


\centerline{\bf Calculus}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
% NOTE: the [2ex] on the next line adds extra height to that row of the table.
% Without that command, the fraction on the first line is too tall and collides
% with the fraction on the second line.
$\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
$\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
$\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
$\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
$\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
$\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
$\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
$\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
$\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Probability and Information Theory}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
$\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
a variable whose type has not been specified\\
$\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
$\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
$\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
$\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
$\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
$\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
$\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
\end{tabular}
\egroup
\vspace{0.25cm}

\centerline{\bf Functions}
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{p{1.25in}p{3.25in}}
$\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
$\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
  $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
  (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
$\displaystyle \log x$ & Natural logarithm of $x$ \\
$\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
$\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
$\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
$\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
$\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
$\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
\end{tabular}
\egroup
\vspace{0.25cm}



\section{Final instructions}
Do not change any aspects of the formatting parameters in the style files.
In particular, do not modify the width or length of the rectangle the text
should fit into, and do not change font sizes (except perhaps in the
\textsc{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PostScript or PDF files}

Please prepare PostScript or PDF files with paper size ``US Letter'', and
not, for example, ``A4''. The -t
letter option on dvips will produce US Letter files.

Consider directly generating PDF files using \verb+pdflatex+
(especially if you are a MiKTeX user).
PDF figures must be substituted for EPS figures, however.

Otherwise, please generate your PostScript and PDF files with the following commands:
\begin{verbatim}
dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
ps2pdf mypaper.ps mypaper.pdf
\end{verbatim}

\subsection{Margins in LaTeX}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+
from the graphicx package. Always specify the figure width as a multiple of
the line width as in the example below using .eps graphics
\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.eps}
\end{verbatim}
or % Apr 2009 addition
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
for .pdf graphics.
See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

A number of width problems arise when LaTeX cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

\subsubsection*{Author Contributions}
If you'd like to, you may include  a section for author contributions as is done
in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the pasper.

\end{comment}

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage 
\appendix
\section{Appendix}
%You may include other additional sections here.


%Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
%All such materials \textbf{SHOULD be included in the main submission.}


\subsection{Architecture}
\label{apdx:Architecture}

For completeness, the full architecture is provided in Fig. \ref{fig:Seq2Seq Full}
\begin{figure}[htp]
\centering\includegraphics[width=\columnwidth]{figures/Seq2Seq+asl.pdf}
\caption{\textbf{Seq2Seq with ASL: Full Architecture.} Complementary to the high-level description in Fig. \ref{fig:Seq2Seq Framework}, the complete description of the framework is shown. The ASL layer (Purple), is also provided in detail in the top right. In the full architecture, the propagation of the hidden decoder states and the inner workings of the simple attention mechanism are also shown.}
    \label{fig:Seq2Seq Full}
\end{figure}

\newpage 

\subsection{Adaptive Spectrum Layer (ASL)}
\label{apdx:ASL}

We extended the Seq2Seq model with ASL, a layer that performs representation learning of the input signal in the frequency domain, enhancing the model's ability to capture the underlying dynamics (Section \ref{paragraph: asl}), The ASL algorithm is described by the following pseudo-code:

\begin{algorithm}[!htp]
\caption{Adaptive Spectrum Layer (ASL)}\label{alg:asl}
\begin{algorithmic}
\Require Tensor $x$ with shape $[H,F]$ (or batched with shape $[B,H,F]$),
\Require $f_{max}$, the maximal frequency to consider, and $f_s$ - the sampling frequency
\Require \texttt{dropout} rate $p\in(0,1)$
\State $N_f \gets [0,1,2,...,H/2-1,H/2]\mbox{ }/\mbox{ }(H/f_s)$
\State $\hat{x} \gets \texttt{rfft}(x)\left[:,:N_f,:\right]$ \Comment{Real valued Fast Fourier Transform}
\State $\hat{s}\gets\left[|\hat{x}|,\cos{(\angle\hat{x}}),\sin{(\angle\hat{x}})\right]$ \Comment{Stacking phase and magnitude}
\State $H \gets \texttt{ReLU(FullyConnected}(\hat{s})$) \Comment{Hidden representation in Fourier space}
\State $w \gets $\texttt{Sequential(}\\
\begin{itemize}
    \item $H\gets$\texttt{dropout}[p=$p$]$(H)$
    \item $H\gets\texttt{FullyConnected}(H)$
    \item $H\gets H \times \texttt{sigmoid}(H)$
    \item $H\gets\texttt{sigmoid}(H)$
\end{itemize}\\
) \Comment{Gating Mechanism}
\State $\hat{x}\gets \texttt{Padding}(\hat{x} \times w)$ with $H-N_f$ zeros  \Comment{Results in reconstruction of the same shape}
\State $x\gets \texttt{irfft}(\hat{x})$
\end{algorithmic}
\end{algorithm}





\subsection{Hyper-parameter Tuning}
\label{apdx:hyperparam-tuning}

In our framework, we offer the flexibility to explore a larger set of training-related hyperparameters, as specified in Tab. 3 \ref{tab: train-hyperparams}
\begin{table*}[htp]
\begin{center}
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Hyperparameter} & \textbf{Description} \\
        \hline
        \texttt{train\_percent} & Percentage of data for training \\
        \texttt{val\_percent} & Percentage of data for validation \\
        \texttt{feature\_win} & Size of input window for temporal span \\
        \texttt{intersect} & Intersection between feature and target window \\
        \texttt{batch\_size} & Number of samples in each mini-batch \\
        \texttt{model\_args} & Model-specific architecture arguments \\
        \texttt{optimizer\_name} & Choice of PyTorch optimizer \\
        \texttt{criterion\_name} & Loss and regularization criterion \\
        \texttt{patience} & Epochs without validation improvement before stopping \\
        \texttt{patience\_tolerance} & Threshold for validation loss improvement \\
        \texttt{n\_epochs} & Total number of training epochs \\
        \texttt{features\_norm\_method} & Normalization scheme for features \\
        \texttt{targets\_norm\_method} & Normalization scheme for target values \\
        \texttt{features\_global\_normalizer} & Global or per-dataset features normalization \\
        \texttt{targets\_global\_normalizer} & Global or per-dataset target values normalization \\
        \texttt{regularization\_factor} & Regularization factor ($\lambda$) \\
        \hline
    \end{tabular}
\end{center}
\label{tab: train-hyperparams}
\caption{\textbf{Training Hyperparameters.} Various training parameters offered as part of a training process}
\end{table*}

 

These hyperparameters collectively contribute to optimizing our model during the training process. In our experimental setup, we conducted an exhaustive search, particularly focusing on the hyperparameters of the Seq2Seq model, but not only.
The parameters explored during this search included:

\newpage

\begin{itemize}
    \item \textbf{Seq2Seq model args:}
    \begin{itemize}
        \item \texttt{embedding\_size}: Varied from 5 to 50 to assess its impact on feature representation.
        \item \texttt{hidden\_size}: Varied to explore the influence of different hidden dimensions.
        \item \texttt{n\_layers}: Investigated different numbers of RNN layers (1-3 layers).
        \item \texttt{attn\_heads}: Explored different numbers of attention heads (1-10).
    \end{itemize}
    \item \textbf{batch size}: Explored values ranging from a few dozen to a few thousand.
    \item \textbf{feature window size}: Varied (128-512) to assess sensitivity to temporal span.
    \item \textbf{normalizer schemes}: every possible pair of normalization schemes for the features and the targets, namely, every pair in $\{\text{z-score, min-max, none}\}\times \{\text{z-score, min-max, none}\}$
\end{itemize}

With the new Adaptive Spectrum Layer (ASL, sec. \ref{paragraph: asl}), we explored the following hyperparameters:
\begin{itemize}
    \item \texttt{hidden\_size}: determines the size of the magnitude and phase representation learned by the ASL. It is typically set to a value similar in size to the hidden size of the Seq2Seq model, for convenience purposes.
    \item \texttt{dropout\_rate}:  Dropout is applied to the stacked hidden representations in the ASL to prevent overfitting. The dropout rate is a hyperparameter that controls the proportion of units to drop during training. We explored values ranging from 0.05 to 0.15, with 0.1 (10\%) dropout being the most commonly used value.
    \item \texttt{use\_freqs}: controls whether the static frequencies are concatenated to the input features. We initially explored using static frequencies but eventually decided not to use them as they did not change per window, which would introduce a constant bias.
    \item \texttt{cross\_spectral\_density}: represents an additional layer that computes the mean value of the Fast Fourier Transform (FFT) of the cross-correlation between force features. While this approach seemed promising, we ultimately did not use it in our final model.
    \item \texttt{frequency\_threshold}: sets the maximum frequency to consider, acting as a low pass filter. We typically did not surpass 200Hz, and the choice of this threshold should be determined by observing the data and understanding its overall underlying frequencies, which can provide an inductive bias for our system.
    \item \texttt{Complexify}: This boolean hyperparameter controls whether a new complex vector is learned from scratch. If set to true, two designated layers are used to map the hidden representation to new magnitude and phase values. However, after experimentation, we decided not to use this addition as it did not improve our results and introduced unnecessary complexity to the model.
\end{itemize}

Concretely, the best-performing parameters are specified in Tab. \ref{tab: val-hyperparams}
Furthermore, each model we evaluated has gone through similar hyperparameter tuning. Concretely:
\paragraph{Transformer/AutoFormer Model Arguments}
\begin{itemize}
    \item \texttt{d\_model}: Dimension of the model, varied from 2 to 64  to assess the impact on feature representation capacity
    \item \texttt{n\_heads}: Number of attention heads explored from 1 to 16
    \item \texttt{d\_ff}: Dimension of feed-forward network, typically set to 2-64
    \item \texttt{e\_layers, d\_layers}: The number of encoder and decoder layers, varied from 1 to 2
    \item \texttt{moving\_avg}: Kernel size explored in range 4-96 
    \item \texttt{activation}: GeLU
\end{itemize}

\paragraph{FEDFormer Model Arguments}
\begin{itemize}
    \item \texttt{modes}: Selection method set to random
    \item \texttt{version}: Fixed to "fourier" as per model specification
    \item Other arguments: similar to Transformer
\end{itemize}

\paragraph{Informer Model Arguments}
\begin{itemize}
    \item \texttt{factor}: ProbSparse attention factor tested in range [3, 5, 7]
    \item \texttt{distil}: Boolean parameter for encoder distilling mechanism
    \item Other arguments: similar to Transformer
\end{itemize}

\paragraph{Linear/NLinear Model Arguments}
\begin{itemize}
    \item \texttt{individual}: Boolean parameter for feature-specific linear layers
\end{itemize}


All other parameters (e.g batch size, normalization scheme, etc...) are similar to the ones examined for Seq2Seq

\subsection{Scaling Analysis of the Proposed Model}

To further evaluate the scalability of our model, we include two additional plots showcasing its performance with respect to validation loss and inference time as the number of parameters increases.

\begin{itemize}
    \item \textbf{Inference Time Scaling:} The first plot examines the inference time versus model size. The Seq2Seq+ASL model achieves orders-of-magnitude lower inference times compared to the Transformer baseline, highlighting its computational efficiency, crucial when intended to be used on an edge device
    \item \textbf{Validation Loss Scaling:} The second plot demonstrates how the validation mean absolute error (MAE) scales with model size. Notably, the Seq2Seq+ASL model consistently shows improved MAE performance as the number of parameters increases, generally outperforming the Transformer baseline on validation loss across all configurations.
    
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/scaling-laws.drawio.pdf}
    \caption{Scaling Analysis: (Left) Inference Time vs. Model Parameters; (Right) Validation Loss vs. Model Parameters; The Seq2Seq+ASL model demonstrates consistent improvements in MAE and significantly reduced inference time compared to the Transformer baseline.}
    \label{fig:scaling_analysis}
\end{figure}


\begin{table*}[t]
\begin{center}
\begin{tabular}{l|l|l}
\toprule
Seq2Seq +ASL Parameter & Our Dataset & Open Source Dataset \\
\midrule
\texttt{train\_percent} & 0.75 & 0.75 \\
\texttt{val\_percent} & 0.1 & 0.1 \\
\texttt{feature\_win} & 512 & 256 \\
\texttt{target\_win} & 1 & 1 \\
\texttt{intersect} & 1 & 1 \\
\texttt{model\_class\_name} & "Seq2Seq" & "Seq2Seq" \\
\texttt{optimizer\_name} & "Adam" & "Adam" \\
\texttt{criterion\_name} & "L1Loss" & "L1Loss" \\
\texttt{patience} & 10 & 10 \\
\texttt{patience\_tolerance} & 0.005 & 0.005 \\
\texttt{n\_epochs} & 30 & 30 \\
\texttt{seed} & 3407 & 3407 \\
\texttt{features\_norm\_method} & "zscore" & "zscore" \\
\texttt{targets\_norm\_method} & "identity" & "identity" \\
\texttt{features\_global\_normalizer} & true & true \\
\texttt{targets\_global\_normalizer} & true & true \\
\texttt{model\_args\_enc\_embedding\_size} & 10 & 30 \\
\texttt{model\_args\_enc\_hidden\_size} & 110 & 100 \\
\texttt{model\_args\_enc\_num\_layers} & 1 & 1 \\
\texttt{model\_args\_enc\_bidirectional} & false & false \\
\texttt{model\_args\_dec\_output\_size} & 3 & 3 \\
\texttt{model\_args\_use\_asl} & true & true \\
\texttt{model\_args\_concat\_asl} & false & false \\
\texttt{model\_args\_complexify} & false & false \\
\texttt{model\_args\_gate} & true & true \\
\texttt{model\_args\_multidim\_fft} & false & false \\
\texttt{model\_args\_dropout} & 0.1 & 0.1 \\
\texttt{model\_args\_freq\_threshold} & 210 & 200 \\
\texttt{model\_args\_per\_freq\_layer} & true & false \\
\texttt{model\_args\_cross\_spectrum\_density} & false & false \\
\texttt{model\_args\_dec\_hidden\_size} & 110 & 100 \\
\texttt{model\_args\_dec\_embedding\_size} & 10 & 30 \\
\texttt{model\_args\_input\_dim} & [512, 512, 4] & [512, 256, 5] \\
\texttt{batch\_size} & 512 & 512 \\
\bottomrule

\end{tabular}
\caption{\textbf{Hyperparameters of the best models.} The hyper-parameters used to train the Seq2Seq+ASL model on the validation set of each dataset}
\label{tab: val-hyperparams}
\end{center}
\end{table*}

\subsection{Comparison to current state-of-art methods}
We compare the following five models: \textbf{Seq2Seq} is a sequence-to-sequence model introduced by Bahdanau \textit{et al.} (2014) \cite{bahdanau2014neural}, which is widely used for sequence prediction tasks.  \textbf{Transformer}, introduced by Vaswani \textit{et al.} (2017) \cite{vaswani2023attention}, is a powerful architecture based on self-attention mechanisms, showing promising results in various sequence modeling tasks. \textbf{AutoFormer} \cite{wu2021autoformer} and \textbf{FedFormer} \cite{zhou2022fedformer} are recent advancements in transformer models, designed on top of the transformer architecture suggesting auto-correlation layers, etc. However, the input and output dimensions of these models must match, which was not the case in our study (kinematic output size = 3, input forces = 4), requiring an additional linear layer to adjust the dimensions. This adjustment may have affected the performance of these models. \textbf{NLinear}, proposed by Zeng \textit{et al.} (2023) \cite{zeng2023transformers}, is a linear layer that incorporates normalization, aiming to improve the stability and convergence of the model while utilizing nothing but a linear layer and non-linear activation.

\subsection{Data alignment}

\tb{To align the force and angle measurements, we first smoothed the force signal and identified the onset of the force measurement by checking the force difference between consecutive timestamps. Second, we smoothed the 3D trajectory signal from the camera tracking system and identified the onset of the wing motion by checking if the position of each marker on the wing in 3D space exceeded a predefined threshold. Once the start times of the force and wing motion were identified, we aligned the two vectors in time. This procedure was also verified manually for each event.}
 
\subsection{Movie 1}

Movie 1 shows a representative event measure in our experimental system. Similarly to Figs. \ref{fig:forward-vs-inverse}e,d, the movie shows the raw images taken from the two fast cameras, as well as representation of the 3D triangulated positions of the three markers on the wing.


\end{document}
