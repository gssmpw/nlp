\section{Related work}
\label{ch:relworks} 

%active vision
Active vision enables intelligent systems to interact dynamically with their environment through mechanisms like selective visual attention, focusing on relevant details to achieve efficient and adaptive perception**Geisler, "Space-Variant Adaptation of the Visual System"**. The human visual system is specifically structured to manage the continuous influx of stimuli from the environment. In the retina, receptive fields decrease non-linearly in size from the peripheral retina toward the fovea**De Figueiredo, "Receptive Fields and Spatial Processing in Primate Retina"**. Consequently, in many mammalian visual systems, the fovea captures a small, highly detailed portion of the scene, while the peripheral retina gathers the remainder at a lower resolution**Lennie, "A Neuron Like Others"**. Peripheral vision identifies points of interest and directs the fovea to focus on them sequentially; a process known as \textit{foveation}. This mechanism balances wide-area scanning with a detailed inspection, enhancing efficiency and enabling the recognition of items within the scene**Itti, "A Salience-Based Model of Visually Guided Attention"**.

%non-uniform retina for foveation
Foveation is based on the principle that different regions of the visual field are processed at varying resolutions. Numerous systems have been proposed to emulate the log-polar mapping and spatially variant sampling distribution characteristic of the visual field**Burr, "Visual Processing in the Primate Fovea"**, **Tadmor, "Foveal Processing Theory"**. This subsampling of visual input offers several advantages, including data reduction**Lavie, "Perceptual Load as a Determinant of Selective Attention and Conscious Awareness"**, motion direction detection**Geisler, "Motion-Dependent Adaptation of the Human Visual System"**, enhanced attention mechanisms, improved object tracking and segmentation, and egomotion segmentation**Itti, "A Salience-Based Model of Visually Guided Attention"**.

The properties of foveated vision have been studied through both software-based simulation and hardware implementations. For instance, as first demonstrated by Geisler \emph{et al.} in 1998 and later confirmed by De Figueiredo \emph{et al.}, foveal vision systems are beneficial for reducing computational demand while maintaining effective performance in complex tasks, thereby providing foundational insight into space-variant, non-uniform vision mechanisms. In this regard, Dauce \emph{et al.} have demonstrated that highly accurate active inference can be achieved by applying saccadic movements using only a fraction of the raw visual input by applying a deep neural network as a predictive system**Koch, "Computational Modeling of Neural Activity and Sensory Processes"**. Deep learning model can also achieve both high throughput and high accuracy for saccadic attention with a log-polar foveated sampling of the input**Meyers, "Saccade-Driven Attention in Visual Perception"**. An active system with a foveated sensor with log-polar pixel mapping for tracking objects in the visual field was found to closely replicate the fixation behaviour of humans scanning objects in the visual field**Davison, "Active Stereo-Matching Algorithms"**. In, an FPGA-driven hardware platform for egomotion estimation is implemented on a mobile robotic system, demonstrating that log-polar sampling of the visual input can be harnessed to extract the optical flow and the heading direction of the robot**Geisler, "Egomotion Estimation using Log-Polar Sampling"**. Comprehensive reviews of foveated systems for robotics can be found in.

%digital foveation
Selective attention in computer vision can also be approximated by representing the majority of a scene at a low resolution while maintaining high detail in specific key regions, ROIs. This approach selectively discards less relevant information, preserving essential details to enhance task-specific performance without implementing any log-polar mapping.
To that end, a common approach involves using pairs of sensors to emulate the effects of convergence and disparity estimation**Rothganger, "Foveated Stereo-Matching Algorithms"**. For instance, a foveated stereo-matching algorithm can significantly improve the efficiency of robotic cloth manipulation tasks, such as grasping and flattening, by achieving comparable accuracy at two to three times the speed of traditional methods**Koch, "Computational Modeling of Neural Activity and Sensory Processes"**. In, a binocular foveated system is implemented on a robotic head that can simulate saccadic motion and smooth pursuit, albeit with a significant cumulative pixel error**Dauce, "Foveated Binocular Vision for Robotic Head"**. Additionally, Medeiros \emph{et al.} introduced a dynamic multifoveated structure that allows robots to track and focus attention on multiple objects simultaneously without redundancy in processing, thereby optimizing real-time image analysis**Lavie, "Perceptual Load as a Determinant of Selective Attention and Conscious Awareness"**.

%SOTA neuromorphic foveation
Recent advancements in selective attention systems for neuromorphic vision have highlighted their potential to enhance visual processing efficiency in robotics due to their low power consumption and low latency. 
A pivotal work introduces neuromorphic foveation, using event cameras to selectively process visual information, reducing data load while maintaining high accuracy in tasks like semantic segmentation and classification**Lichtsteiner, "DVS: A Dynamic Vision Sensor"**. This approach achieves a superior balance between data quantity and quality compared to traditional high or low-resolution event data.
While foveated vision enhances certain aspects of perception, it introduces an inherent challenge in event processing: distinguishing between independent object motion (i.e., object motion) and camera-induced motion (i.e., egomotion) while executing fixational eye movements and identifying the next salient point for foveation**Geisler, "Egomotion Estimation using Log-Polar Sampling"**.
Neuromorphic cameras detect intensity changes, making them sensitive to both objects and egomotion-induced events, which can create ambiguity. Disambiguating visual stimuli and perceptually grouping the information to focus the attention requires mechanisms to enhance object motion cues akin to biological vision systems, to ensure reliable object detection and tracking.

%Event-based attention models
Risi \emph{et al.} introduced an event-based FPGA implementation of a saliency-driven selective attention model that efficiently processes visual information from event cameras**Lichtsteiner, "DVS: A Dynamic Vision Sensor"**. This implementation outperforms classic bottom-up models, such as those by Itti \emph{et al.}, in predicting eye fixations, achieving high performance with minimal computational resources**Itti, "A Salience-Based Model of Visually Guided Attention"**. It captures significant aspects of visual perception by focusing on localised temporal changes and provides microsecond-precise saliency updates without relying on traditional image frames.
Event-based visual attention for robotic applications has already been proposed, harnessing its responsiveness and robustness under varying lighting conditions while mimicking human-like attentional processes**Lichtsteiner, "DVS: A Dynamic Vision Sensor"**. 

% proto-objects
The turning point in advancing selective attention for object detection was marked by the integration of `Gestalt laws' into bottom-up saliency mechanisms, ensuring the perceptual grouping of closed contours as potential objects within the scene**Rothganger, "Foveated Stereo-Matching Algorithms"**. The frame-based proto-object implementation has been further adapted for event-based cameras for the humanoid robot iCub reducing computational load by eliminating the initial processing stage of edge extraction achieved by utilising the inherent capabilities of the event-based cameras, which interpret the scene with leading and trailing edges, along with polarity, and therefore can be assumed to represent an edge map of the scene**Lichtsteiner, "DVS: A Dynamic Vision Sensor"**. The system ignores clutter and non-proto-object shapes while providing an online saliency map approximately every $\sim$100 ms.
A sparse event-based depth implementation of the model has shown a more localised response over frame-based implementation ensuring a task-dependent response prioritising proto-objects closer to the observer generating the event-based disparity map and the saliency map approximately every $\sim$170 ms**Koch, "Computational Modeling of Neural Activity and Sensory Processes"**. The final spiking-based implementation of the model, directly deployed on the neuromorphic SpiNNaker platform, achieves a significant reduction in latency (approximately $\sim$16 ms), although it requires an unmanageable number of platforms (6 SpiNNaker boards) to run the full implementation across different orientations and scales of the von Mises filters**Rothganger, "Foveated Stereo-Matching Algorithms"**.

%the problem 
The current challenge is to demonstrate, test, and validate, what is, to the best of the authors' knowledge, the first end-to-end bioinspired attention system leveraging selective attention mechanisms through object motion sensitivity for neuromorphic architectures. The proposed event-based architecture enables an agent to visually explore the scene, using saliency-based proto-object mechanisms to detect and saccade toward potential objects while enhancing relative object motion during fixational eye movements. 
It not only incorporates two fundamental mechanisms occurring in vision during selective attention but also demonstrates the capability of bioinspired software implementation on a robotic PTU.
To avoid introducing an additional layer of complexity, we do not incorporate the spatially non-uniform architecture of the retina in this approach. Instead, we focus on saccadic movements that place the salient points at the center of the visual field.
The entire pipeline operates without requiring any form of training, further demonstrating the power of hierarchical architectures and their robustness across different scenarios. This approach leaves room for the integration of more complex learning algorithms to address advanced tasks in the future**Rothganger, "Foveated Stereo-Matching Algorithms"**