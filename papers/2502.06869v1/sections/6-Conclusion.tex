\section{Conclusion and Future Directions}



This survey has reviewed recent advances in the field of XRL, emphasizing a range of techniques - from feature-level, and state-level to dataset-level approaches, and illustrating their roles in adversarial attacks and mitigation, and policy refinement. Evidence across these methods indicates that effective explanations can significantly enhance trust and debugging efficiency in real-world deployments of deep reinforcement learning. Nonetheless, substantial gaps remain to be addressed, which are summarized as follows.



% \noindent\textbf{User-Oriented Explanations. }
\paragraph{User-Oriented Explanations.}
Although existing techniques could highlight critical features/states to illustrate an agent’s decision-making process, these granular depictions can be difficult for non-expert users to interpret. In the case of critical features, users who lack domain knowledge (\eg specific familiarity with a particular game environment) may struggle to grasp the significance of highlighted features and how they influence the agent’s actions. Meanwhile, understanding critical states often demands that users examine multiple visual frames and then manually summarize what these states imply about the agent’s strategy. This process can be cognitively taxing, as it requires piecing together dispersed information and inferring the agent’s underlying rationale without clear contextual guidance.

To address these challenges, future research should therefore prioritize strategy-level or narrative-based explanations, which can provide higher-level rationales that are more accessible to general audiences. In particular, leveraging vision–language models or other multimodal architectures could facilitate the presentation of natural language narratives that encapsulate an agent’s overarching goals, strategies, and reasoning. These narrative formats have the potential to reduce cognitive load, enabling end users to more intuitively comprehend and trust the agent’s behavior.


\paragraph{Developer-Oriented Explanations. }
% \noindent\textbf{Developer-Oriented Explanations. }
In contrast, developers and researchers frequently require detailed insights into an agent’s decision-making process. Mechanistic interpretation methods, such as sparse autoencoders or network dissection, could illuminate hidden representations and policy structures. These more granular approaches enable targeted policy debugging by pinpointing design flaws or overfitting at the architectural level. Crucially, explanations for developers should be \emph{actionable}, which could be compatible with policy refinement workflows to accelerate iterative improvements.

In addition to improving interpretability, explainability tools offer considerable potential for enhancing policy performance. For instance, in game-theoretic contexts, explanations can help identify equilibrium strategies or support robust multi-agent interactions. In hierarchical reinforcement learning, clarifying subtask transitions can streamline learning in sparse-reward or long-horizon tasks. Similarly, in curriculum learning, highlighting critical states through explanation techniques can aid developers in selecting more effective initial conditions. Moving forward, future research should focus on aligning these interpretability and performance objectives by examining how transparent representations of policy decisions can foster robust learning or facilitate agent learning.
