\section{Applications of Explanations}
With the explanation of RL, there can be different applications of it - they can be leveraged both constructively (for policy refinement and debugging) and potentially destructively (for launching adversarial attacks). These applications demonstrate how fidelity and interpretability impact the effectiveness of explanation-based interventions in real-world scenarios.


\begin{table*}[ht]
\centering
\resizebox{2.0\columnwidth}{!}{
\begin{tabular}{lll}
\toprule
\textbf{Category} & \textbf{Subcategory} & \textbf{Citation} \\
\midrule
\multirow{1}{*}{Launching Adversarial Attacks} 
 & Targeted Attack & \citep{lin2020robustness,guo2021edge, cheng2023statemask, wang2024rlhfpoison} \\
\midrule
\multirow{2}{*}{Mitigating Adversarial Attacks} 
 & Blinding Observations & \citep{guo2021edge} \\
 & Shielding Backdoor Triggers & \citep{yuan2024shine} \\
\midrule
\multirow{2}{*}{Policy Refinement} 
 & Human-in-the-Loop Correction & \citep{van2022correct, jiang2024reinforcement} \\
 & Automated Policy Refinement & \citep{guo2021edge, cheng2023statemask, yu2023airs, chengrice,liu2025utilizing} \\
\bottomrule
\end{tabular}
}
\caption{Taxonomy of Explanation-based Interventions in DRL.}
\label{tab:taxonomy}
\end{table*}



\subsection{Launching Adversarial Attacks}
Recent work demonstrates that explanations of a DRL agent's policy can be repurposed to compromise the agent's performance. Recent studies have revealed that explanations of a Deep Reinforcement Learning (DRL) agent's policy can be exploited to compromise the agent's performance. For instance, \citet{lin2020robustness} demonstrated the vulnerability of cooperative Multi-Agent Reinforcement Learning systems to adversarial attacks by introducing perturbations based on feature-level explanations (\ie saliency) to the state space. They proposed a mechanism where an adversary adds perturbations to the observations of a single agent within a team, leading to a significant decrease in overall team performance. 


Besides leveraging feature-level explanations to launch adversarial attacks, researchers also demonstrate that state-level explanations can be utilized to attack DRL agents. EDGE \citep{guo2021edge} proposes a more targeted approach by leveraging explanations to identify critical time steps during an episode. The attacker first collects winning episodes from the victim agent and uses post-hoc explanations to highlight moments where actions strongly contribute to victory. By forcing the agent to take sub-optimal actions at these identified crucial steps, the attack achieves significant performance degradation with minimal intervention.


Subsequent research by \citet{cheng2023statemask} confirms this explanation-driven attack generalizes across different DRL environments, showing that targeting just 10\% of time steps can substantially reduce agent reward. Notably, attacks guided by high-fidelity explanation methods prove more effective than those using lower-fidelity alternatives, highlighting how better interpretability tools can paradoxically increase vulnerability.

In addition to exploiting feature-level and state-level explanations, recent research has explored the use of dataset-level explanations to launch adversarial attacks on LLMs. A study by \citet{wang2024rlhfpoison} investigates the vulnerabilities of reinforcement learning with human feedback. The researchers employ a gradient-based dataset-level explanation method to identify influential data points within the training set. By poisoning a small percentage of critical data, an adversary can significantly manipulate the LLM's behavior, leading to the elicitation of harmful responses. 

This line of work highlights the dual-edged nature of interpretability in RL. While explanations are invaluable for debugging and understanding agent behavior, they can also expose vulnerabilities. By identifying specific moments when an agent's correct actions matter most, adversaries can focus on minimal but high-impact interventions. Consequently, researchers must carefully consider the security implications of providing public or easily accessible explanation systems, especially in safety-critical or competitive domains.

\subsection{Mitigating Adversarial Attacks}

XRL methods not only reveal how adversaries can manipulate agents but can also guide the design of robust policies. By pinpointing which states or actions are most vulnerable, developers can selectively limit or modify the agent's observations and decision pathways at crucial moments, ultimately reducing susceptibility to adversarial inputs.

\textbf{Blinding Observations at Critical Time Steps.}
\citet{guo2021edge} illustrated how explanations of the victim agent's losing episodes in the You-Shall-Not-Pass game~\citep{todorov2012mujoco} uncover the specific times when adversarial actions (\eg, pretending to fall) most effectively mislead the agent. By analyzing contrastive explanations—comparing losing and winning trajectories—it becomes clear that the agent's focus on adversarial cues at certain time steps can trigger suboptimal responses. The authors proposed ``blinding'' the victim agent to these cues precisely at those critical moments. Experimental results show that this explanation-driven defense significantly boosts the victim's win rate, highlighting how identifying the root cause of agent failures can lead to targeted and effective countermeasures.

\textbf{Detecting and Shielding Backdoor Triggers.}
Another line of work focuses on a subtler attack vector: maliciously injected backdoors. \citet{yuan2024shine} introduced \textit{SHINE}, a method to shield a pre-trained agent from both perturbation-based and adversarial-agent attacks in a poisoned environment. SHINE first gathers trajectories and employs a two-stage explanation process to (1) locate states where a backdoor trigger is likely active and (2) isolate the common subset of features critical to the agent's decisions in those states. These features are then treated as the backdoor signature. In the second stage, SHINE retrains the policy to neutralize the trigger's influence while preserving performance in a clean environment. This careful mixture of explanation and policy adjustment provides theoretical guarantees of improved robustness.

These defense mechanisms highlight how explanations serve defensive purposes in adversarial contexts. By precisely identifying \emph{where} and \emph{how} an agent's decision-making is compromised, explanation-guided strategies enable targeted fixes that enhance robustness. This demonstrates that transparency, when properly leveraged, can be a powerful tool for securing DRL agents rather than just exposing their vulnerabilities.

\subsection{Policy Refinement Through Explanations}

To refine the policy of the agents, conventional methods such as continual training~\citep{fickinger2021scalable} often fall short due to a lack of knowledge of the root causes of errors. There are two categories of methods for policy refinement through explanations:
\begin{itemize}
    \item \textbf{Human-in-the-Loop Correction:} Domain experts or non-experts identify suboptimal actions or critical states, providing corrective demonstrations or reward adjustments.
    \item \textbf{Automated Policy Refinement with Explanation:} Explanation techniques automatically identify pivotal states and refine the target agent's policy based on the explanation.
\end{itemize}



For the first category, \cite{van2022correct} proposed to utilize human feedback to correct the agent's failures. 
More specifically, when the agent fails, humans (can be non-experts) are involved to point out how to avoid such a failure (\ie what action should be done instead, and what action should be forbidden). Based on human feedback, the DRL agent gets retrained by taking the human-refined action in those important time steps and finally obtains the corrected policy. 
The downside is that it relies on humans to identify critical steps and craft rules for alternative actions. This can be challenging for a large action space, and the retraining process is ad-hoc and time-consuming. To address the challenges of imperfect corrective actions and extensive human labor, \citet{jiang2024reinforcement} introduced the Iterative learning from Corrective actions and Proxy rewards (ICoPro) framework. In this approach, human labelers provide corrective actions on the agent's trajectories, which are then incorporated into the Q-function using a margin loss to enforce adherence to the labeler's preferences. The agent undergoes iterative training, balancing learning from both proxy rewards and human feedback. Notably, ICoPro integrates pseudo-labels from the target Q-network to reduce human labor and stabilize training. Experimental results in various tasks, including Atari games and autonomous driving scenarios, demonstrate that ICoPro effectively aligns agent behavior with human preferences, even when both proxy rewards and corrective actions are imperfect.

For the second category, \cite{guo2021edge} proposed an explanation-guided policy refinement approach to automatically correct policy errors without relying on explicit human feedback. Their method first identifies losing episodes of the target agent and pinpoints crucial time steps within those episodes using its proposed explanation technique. The authors employ a fixed number of random explorations at the identified critical time steps. Any random actions that transform a losing episode into a win get stored in a look-up table as a remediation policy.
When deployed, the agent consults this table at run-time: if the current state matches one of the stored entries, the agent applies the corresponding remediation action; otherwise, it defaults to its original policy. The success of this policy refinement approach depends heavily on the budget of random exploration and the size of the look-up table. \cite{cheng2023statemask, yu2023airs} further proposed to use DRL explanation methods to identify critical time steps and refine the agent by resetting the environment to the critical states and subsequently resuming training the DRL agents from these critical states. However, this refining strategy can easily lead to overfitting as evidenced in \cite{chengrice} and cannot help the agent escape the local optimal. 
\cite{chengrice} further proposed a novel refining strategy to construct a mixed initial state distribution with both the identified critical states and the default initial states to avoid overfitting and encourage the agent to perform exploration during the refining process. Recently, \citet{liu2025utilizing} proposed a novel framework that leverages explainable reinforcement learning (XRL) to enhance policy refinement. This approach addresses the challenges of DRL agents' lack of transparency and suboptimal performance by providing a two-level explanation of the agents' decision-making processes. The framework identifies mistakes made by the DRL agent and formulates a constrained bi-level optimization problem to learn how to best utilize these explanations for policy improvement. The upper level of the optimization learns how to use high-level explanations to shape the reward function, while the lower level solves a constrained RL problem using low-level explanations. The proposed algorithm theoretically guarantees global optimality and has demonstrated superior performance in MuJoCo experiments compared to state-of-the-art baselines.