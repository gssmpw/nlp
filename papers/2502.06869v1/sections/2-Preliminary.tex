\section{Preliminaries}
\label{sec:preliminaries}
\subsection{Reinforcement Learning Foundations}
Reinforcement Learning (RL) is a subfield of machine learning that focuses on training agents to make sequential decisions by interacting with an environment. The environment is framed as a Markov Decision Process (MDP) \citep{sutton2018reinforcement}, specified by the tuple \((\mathcal{S}, \mathcal{A}, P, \rho, R, \gamma)\):
\begin{itemize}
    \item \(\mathcal{S}\): A set of states representing possible configurations of the environment.
    \item \(\mathcal{A}\): A set of actions available to the agent.
    \item \(P(s' \mid s,a)\): The transition probability function describing how actions lead from one state \(s\) to another state \(s'\).
    \item $\rho$: the distribution of the initial state $s_0$.
    \item \(R(s,a)\): The immediate reward obtained after executing action \(a\) in state \(s\).
    \item \(\gamma \in (0,1)\): A discount factor that balances immediate and future rewards.
\end{itemize}


The goal of RL is to find an optimal policy $\pi(a|s)$: ($\mathcal{S} \rightarrow \mathcal{A}$) which maximizes the agent's long-term reward. Formally, the long-term reward is defined as the \emph{state-value function}
% 
\begin{equation}
\small
V^{\pi}(s) =\sum_{a\in\mathcal{A}} \pi(a|s)\left[R(s,a)+\gamma \sum_{s'\in\mathcal{S}} P(s'|s,a)V^{\pi}(s')\right] \, .
\label{eq:state}
\end{equation}
% 
Accordingly, the \emph{action-value function} $Q^{\pi}(s, a)$ is defined as
% 
\begin{equation}
    \small
    Q^{{\pi}}(s, a) = R(s,a)+\gamma \sum_{s'\in\mathcal{S}} P(s'|s,a) \sum_{a'\in\mathcal{A}}\pi(a'|s')Q^{\pi}(s', a') \, .
    \label{eq:action}
\end{equation}

The \emph{advantage function} $A^{\pi} (s,a)$ is defined as
\begin{equation}
    \small
    A^{{\pi}}(s, a) = Q^{\pi}(s,a)-V^{\pi}(s) \, .
    \label{eq:advantage}
\end{equation}

In reinforcement learning, the state-value function $V^{\pi}(s)$ represents the expected total reward for an agent starting from state $s$. Slightly different from $V^{\pi}(s)$, the action-value function $Q^{\pi}(s, a)$ is the expected total reward for an agent to choose action $a$ while in state $s$. The advantage function measures the expected additional reward for choosing action $a$ over the expected reward of the policy.
The expected total reward of a policy $\pi$ is defined as 
% 
\begin{equation}
    \small
    \eta(\pi)=\mathbb{E}_{s_0, a_0, \ldots}\left[\sum_{t=0}^{\infty} \gamma^t R\left(s_t, a_t\right)\right] \, .
    \label{eq:expected_reward}
\end{equation}
By maximizing the expected total reward, an optimal policy $\pi^{*}$ can be derived, enabling the agent to receive the maximum rewards in the environment.

% There are two primary settings for reinforcement learning: online RL and offline RL. In the online RL setting, the learner possesses interactive access to the environment, enabling the execution of any chosen policy. Conversely, in the offline RL setting, the learner's access is restricted to an ``offline'' dataset acquired through the execution of one or multiple policies within the environment. In this scenario, further interaction with the environment is prohibited.

Reinforcement learning can be categorized into two primary settings based on the agent's ability to interact with the environment: online RL and offline RL. In online RL, the agent has direct, interactive access to the environment and can continuously collect new experiences by executing and updating its policy in real-time. This setting allows for active exploration and immediate policy adaptation. In contrast, offline RL restricts the agent to learn solely from a fixed dataset of previously collected experiences, without any further environment interaction. This dataset typically consists of state-action-reward trajectories collected by one or multiple behavior policies. The offline setting is particularly relevant in scenarios where environment interaction is expensive, risky, or impractical, such as in healthcare, autonomous driving, or industrial control systems.


There are two types of main-stream algorithms, \ie value-based methods and policy-based methods. For value-based methods such as Q-learning algorithm~\citep{watkins1992q}, the agent estimates $Q(s,a)$ and greedily chooses the optimal action. Regarding policy-based methods, the agent directly optimizes its policy based on the reward feedback (\eg Policy Gradient methods~\citep{sutton1999policy}). Classic algorithms have been effective in relatively small or structured environments. However, their performance may degrade in high-dimensional or unstructured domains due to challenges in representation and exploration.

\subsection{Deep Reinforcement Learning Advancements}

To address the limitations of standard RL in complex or high-dimensional settings, Deep Reinforcement Learning (DRL) integrates neural networks as function approximators for policies or value functions. Two prominent approaches for learning deep reinforcement learning policies are Deep Q-Network (DQN)~\citep{mnih2015human} and Proximal Policy Optimization (PPO)~\citep{schulman2017proximal}. We provide a brief overview of the foundational principles underlying each of these algorithms.

\textbf{Deep Q-Network (DQN). }
DQN utilizes a deep neural network to approximate the optimal action-value function (Q function). The network architecture typically processes state inputs $s$ through several layers and outputs Q-values for all possible actions simultaneously. The network is trained by minimizing the temporal difference error between predicted and target Q-values using experience replay and a target network to stabilize training. During execution, the optimal policy is derived by selecting the action with the highest predicted Q-value.

\textbf{Proximal Policy Optimization (PPO).}
Different from DQN, policy gradient methods directly learn a parameterized policy $\pi_{\theta}(a|s) = \mathbb{P}(a|s,\theta)$ to maximize the expected total reward. While these methods offer more direct policy optimization, they often suffer from high variance and sensitivity to learning rates, leading to unstable training. PPO addresses these challenges by introducing a clipped surrogate objective function. It constrains policy updates to prevent excessive changes while optimizing performance. By maintaining proximity between consecutive policies and using advantage estimation, PPO achieves more stable training and better sample efficiency compared to traditional policy gradient methods, making it one of the most widely adopted algorithms in practice.


% By leveraging the representational power of neural networks, these methods have enabled RL to tackle tasks that were previously intractable, including robotic control, complex strategy games, and large-scale resource management. These advancements have demonstrated the potential of DRL to solve real-world problems with high-dimensional inputs and continuous action spaces.

% However, the integration of deep neural networks into RL has led to the explainability challenge. The opacity of neural networks makes it difficult to understand how DRL agents make decisions, which is critical for applications in safety-critical domains such as healthcare, finance, and autonomous systems. Therefore, there is a pressing need for techniques to
% make these agents more explainable and interpretable. For example, a deep neural network may
% be converted to an interpretable format or be made to produce interpretable outputs.

\subsection{Reinforcement Learning for LLMs}

The integration of RL with Large Language Models (LLMs) has emerged as a promising direction for improving the alignment and performance of AI systems. Multiple RL approaches such as PPO, Directed Preference Optimization (DPO)~\citep{rafailov2024direct}, Reward rAnked FineTuning (RAFT)~\citep{dong2023raft} have been used to fine-tune LLMs for specific tasks, such as dialogue generation, summarization, and instruction following. By leveraging reward feedback, RL-based approaches enable LLMs to generate more coherent, contextually appropriate, and user-aligned outputs.

Despite these advancements, the explainability of RL for LLMs remains an open challenge. The complexity of LLMs, combined with the sequential decision-making nature of RL, makes it difficult to interpret how the input data impacts these models to generate outputs. Recent efforts have explored techniques such as data influence functions to enhance the transparency of RL for LLMs. However, there is still a need for more systematic explanation approaches in this domain, particularly for applications involving ethical considerations, bias mitigation, and user trust.


In the subsequent sections, we survey existing methods for providing interpretability in DRL systems as well as LLMs, and discuss how these techniques can be evaluated and applied in practice.

