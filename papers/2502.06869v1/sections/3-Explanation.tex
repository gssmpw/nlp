\section{Explanation Techniques for DRL}
% The growing demand for transparency and accountability in AI systems has spurred significant interest in Explainable Reinforcement Learning (XRL). XRL aims to bridge the gap between the performance of RL agents and their interpretability by developing methods that provide insights into the agent's decision-making process. Existing approaches to XRL can be broadly categorized into three categories: \circled{1} \textbf{Feature-level Explanation Methods}, which focuses on pinpointing the most important feature in the DRL agent's observation; \circled{2} \textbf{State-level Explanation Methods}, which identifies the most critical steps in the RL trajectory; \circled{3} \textbf{Dataset-level Explanation Methods}, which selects the most influential data in RL;  \circled{4} \textbf{Model-level Explanation Methods}, which focuses on the self-explainability of RL policy models. A summary of selected XRL methods is provided in \autoref{tab:xrl_methods}.

\tikzstyle{my-box}=[
 rectangle,
 draw=hidden-draw,
 rounded corners,
 text opacity=1,
 minimum height=1.5em,
 inner sep=2pt,
 align=center,
 fill opacity=.5,
]
\tikzstyle{leaf}=[my-box, 
 minimum height=1.5em,
 fill=hidden-orange!60, 
 text=black, 
 align=left,
 font=\scriptsize,
 inner xsep=2pt,
 inner ysep=4pt,
]

\begin{figure*}[t]
    % \raggedright
    \centering
    \resizebox{1\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                node options={align=center},
                align = center,
                base=left,
                font=\small,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=5.0em,font=\scriptsize}{},
            where level=2{text width=5.6em,font=\scriptsize}{},
            where level=3{text width=6.8em,font=\scriptsize}{},
            [
            DRL Explanation Methods, ver
            [
            Feature-level
            [
            Perturbation-based
                [
               ~\cite{zahavy2016graying,greydanus2018visualizing, atrey2019exploratory}
            %    ~\cite{}
                , leaf, text width=20em
                ]
            ]
            [
            Gradient-based
                [
               ~\cite{wang2016dueling,selvaraju2017grad,sundararajan2017axiomatic}
                , leaf, text width=21em
                ]
            ]
            [
            Attention-based
                [
               ~\cite{mott2019towards,nikulin2019free}
                , leaf, text width=12em
                ]
            ]
            ]
            [
            State-level
                [
            Offline \\ Trajectories
                [
               ~\cite{guo2021edge,yu2023airs,liu2023learning}
                , leaf, text width=16em
                ]
                ]
                [
            Online \\ Interactions
                [
               ~\cite{jacq2022lazymdp,cheng2023statemask,chengrice}
                , leaf, text width=14em
                ]
                ]
            ]
            [
            Dataset-level
                [
            Influence \\ Functions
                [
               ~\cite{koh2017understanding,li2024influence,matelsky2024empirical,ruis2024procedural}
                , leaf, text width=24em
                ]
                ]
                [
            Data Shapley
                [
               ~\cite{ghorbani2019data,wanghelpful, schoch2023srw}
                , leaf, text width=20em
                ]
                ]
                [
            Data Masking
                [
               ~\cite{dong2024promptexp, lin2024data}
                , leaf, text width=11em
                ]
                ]
            ]
            [
            Model-level
                [
            Transparent \\ Architectures
                [
               ~\cite{topin2021iterative,ding2020cdt, demircan2024sparse}
                , leaf, text width=19em
                ]
                ]
                [
            Rule \\ Extraction
                [
               ~\cite{soares2020explaining,likmeta2020combining}
                , leaf, text width=13em
                ]
                ]
            ]
            ]
        \end{forest}
    }
\caption{Taxonomy of DRL Explanation Methods}
\label{fig:drl_explanation}
\end{figure*}

Existing approaches to explaining deep reinforcement learning can be broadly categorized into four categories: \circled{1} \textbf{Feature-level Explanation Methods}, which focuses on pinpointing the most important feature in the DRL agent's observation; \circled{2} \textbf{State-level Explanation Methods}, which identifies the most critical steps in the RL trajectory; \circled{3} \textbf{Dataset-level Explanation Methods}, which selects the most influential data in RL; \circled{4} \textbf{Model-level Explanation Methods}, which focuses on the self-explainability of RL policy models. A summary of selected methods is provided in \autoref{fig:drl_explanation}.

\subsection{Feature-level Explanation Methods}
Feature-level explanation methods aim to identify the most important features in the agent's observation space that influence its decision-making. These methods are particularly useful for understanding how an agent processes visual inputs. 

\citet{zahavy2016graying} approximated the behavior of DRL agents via Semi-Aggregated Markov Decision Processes (SAMDPs) and analyzed the high-level temporal structure of the policy with the more interpretable SAMDPs. However, the explanation from SAMDPs is drawn from t-SNE clusters which could be uninformative for users without machine learning backgrounds. To make the explanation more accessible, \citet{greydanus2018visualizing} proposed a feature-level explanation method to visualize the importance of pixels in Atari game frames by perturbing the input and observing changes in the agent's policy. 

In addition to perturbation-based saliency methods, some researchers also proposed gradient-based saliency methods that use gradients of the agent's policy or value function to pinpoint the most important feature in DRL agent's observation. \citet{wang2016dueling} extend gradient-based saliency maps to deep RL by computing the Jacobian of the output logits with respect to a stack of input images. \citet{joo2019visualization} leveraged Grad-Cam~\citep{selvaraju2017grad} to visualize the important features towards the DRL agent's behavior. \citet{chengrice} mentioned that we can also use integrated gradients~\citep{sundararajan2017axiomatic} to identify the most important features.

Recent advancements in deep reinforcement learning (DRL) also introduce attention-based mechanisms to enhance feature-level explanations of agent behavior. These methods aim to improve interpretability by enabling agents to focus on task-relevant information within their observation space. For instance, \citet{mott2019towards} proposed an attention-augmented agent that employs a soft attention mechanism, allowing the agent to sequentially query its environment and focus on pertinent features during decision-making. This approach not only enhances performance but also provides interpretable attention maps that highlight the areas of the input contributing to the agent's actions. Similarly, \citet{nikulin2019free} introduced a method that integrates an attention module into the agent's architecture, producing saliency maps that visualize the importance of different input regions in the agent's decision process. 

These methods provide insights into the agent's perception of the environment but are often limited to explaining low-level features rather than high-level decision-making processes.



\subsection{State-level Explanation Methods}
State-level explanation methods focus on identifying critical states in the agent's trajectory that significantly impact its performance. These methods are useful for understanding the agent's behavior over time and diagnosing failures. We categorize state-level explanation methods into two categories: (1) Explain through offline trajectories; (2) Explain through online interactions.

For the first category, \cite{guo2021edge} first proposed EDGE that establishes state-reward relationship
by collecting a set of trajectories and then approximating an explanation model offline with the Gaussian Process. Note that, EDGE provides a {\em global explanation} for the policy network. AIRS~\citep{yu2023airs} further introduces a {\em local explanation} method to identify critical time steps for a given trajectory of interest. AIRS pre-collects a set of trajectories and utilizes a deep neural network to estimate the contribution of each state to the final rewards for each trajectory. \cite{liu2023learning} proposed a Deep State Identifier that learns to predict returns from episodes and uses mask-based sensitivity analysis to extract important states. However, the fidelity of these methods is highly related to the quality of the pre-collected trajectories, which limits their ability to measure the importance of ``unseen states''. 

For the second category, \citet{jacq2022lazymdp} presented LazyMDP, which extends the action space with a lazy action and learns to switch between the default action and the lazy action. The states where the policy diverges from the default are further interpreted as non-important states. \cite{cheng2023statemask} proposed StateMask, which online trains a mask network in parallel with the agent's policy network. The mask network learns to ``blind'' the agent's observations at certain time steps (by taking random actions) while minimizing the impact of blinding to the final reward. The time steps when the agent could be blinded are identified as non-critical steps. 

State-level explanations are particularly valuable for debugging and improving RL agents, as they highlight the most influential moments in the agent's decision-making process.

\subsection{Dataset-level Explanation Methods}
Dataset-level explanation methods focus on understanding how specific training examples influence the learned policy of an RL agent. By identifying which data points have the most impact on the policy updates, researchers and practitioners can better diagnose training inefficiencies, detect harmful experiences, and refine data collection strategies. Recent work has highlighted multiple approaches for quantifying this influence:

\paragraph{Influence Functions.} Originally introduced by \citet{koh2017understanding}, influence functions estimate how an upweighting or removal of a single training example impacts model parameters. In RL contexts, these techniques can be adapted to analyze individual experiences in a replay buffer, thereby revealing which transitions most critically shape the agent’s behavior. When incorporating RL with LLMs, \citet{li2024influence,matelsky2024empirical,ruis2024procedural} also investigated the feasibility of leveraging influence functions to identify influential data. However, they found that influence functions show poor performance and the reasons might be (1) inevitable approximation errors when estimating the inverse-Hessian vector products (iHVP) component due to the scale of LLMs, (2) uncertain convergence during fine-tuning, (3) the definition of influential data as changes in model parameters do not necessarily correlate with changes in LLM behavior.

\paragraph{Data Shapley Values.} Shapley values, proposed by \citet{ghorbani2019data}, offer a game-theoretic metric for attributing credit to each data point. By considering all possible subsets of the training set, Data Shapley Values can rank experiences according to their overall contribution to policy performance. However, the original Data Shapley Values are computationally intensive, \citet{wanghelpful} proposed an approximation method FreeShap for instance attribution based on the neural tangent kernel, which makes this method feasible for explaining LLM predictions.

\paragraph{Data Masking.} Recent advances have introduced masking as a way to figure out how specific elements of a training dataset shape an agent’s learning process \citep{dong2024promptexp, lin2024data}. Rather than simply omitting entire experiences, data masking strategically hides or perturbs certain tokens and observes how these modifications affect the LLM's performance. Therefore, researchers can pinpoint the data components most critical to LLM training and can construct a pruned dataset based on the critical data to efficiently train a LLM. 

Dataset-level explanations help researchers and practitioners understand the role of training data in shaping the RL agent's behavior and can guide the design of more efficient and effective training schemes.

\subsection{Model-level Explanation Methods}
Model-level explanation methods focus on the self-explainability of RL policy models, aiming to make the agent's decision-making process inherently interpretable. These methods often involve designing transparent architectures (\eg decision tree~\citep{topin2021iterative, ding2020cdt}) or extracting human-understandable rules~\citep{soares2020explaining, likmeta2020combining} from the agent's policy. 
\cite{demircan2024sparse} utilize sparse auto-encoders within the policy network to provide detailed explanations of LLM's behavior, specifically focusing on how the network approximates Q-learning by revealing the underlying structure and decision-making process of the model.

Model-level explanations are particularly valuable for applications requiring high transparency, such as healthcare and autonomous driving, where understanding the agent's reasoning is critical for trust and safety.