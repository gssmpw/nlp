\section{Introduction}

Deep Reinforcement Learning (DRL) has emerged as a transformative paradigm for solving complex sequential decision-making problems. By enabling autonomous agents to interact with an environment, receive feedback in the form of rewards, and iteratively refine their policies, DRL has demonstrated remarkable success across a diverse range of domains including games (\eg Atari~\citep{mnih2013playing,kaiser2020model}, Go~\citep{silver2018general,silver2017mastering}, and StarCraft II~\citep{vinyals2019grandmaster,vinyals2017starcraft}), robotics~\citep{kalashnikov2018scalable}, communication networks~\citep{feriani2021single}, and finance~\citep{liu2024dynamic}. These successes underscore DRL's capability to surpass traditional rule-based systems, particularly in high-dimensional and dynamically evolving environments.

Despite these advances, a fundamental challenge remains: DRL agents typically rely on deep neural networks, which operate as black-box models, obscuring the rationale behind their decision-making processes. This opacity poses significant barriers to adoption in safety-critical and high-stakes applications, where interpretability is crucial for trust, compliance, and debugging. The lack of transparency in DRL can lead to unreliable decision-making, rendering it unsuitable for domains where explainability is a prerequisite, such as healthcare, autonomous driving, and financial risk assessment.

To address these concerns, the field of Explainable Deep Reinforcement Learning (XRL) has emerged, aiming to develop techniques that enhance the interpretability of DRL policies. XRL seeks to provide insights into an agentâ€™s decision-making process, enabling researchers, practitioners, and end-users to understand, validate, and refine learned policies. By facilitating greater transparency, XRL contributes to the development of safer, more robust, and ethically aligned AI systems.

Furthermore, the increasing integration of Reinforcement Learning (RL) with Large Language Models (LLMs) has placed RL at the forefront of natural language processing (NLP) advancements. Methods such as Reinforcement Learning from Human Feedback (RLHF)~\citep{bai2022training,ouyang2022training} have become essential for aligning LLM outputs with human preferences and ethical guidelines. By treating language generation as a sequential decision-making process, RL-based fine-tuning enables LLMs to optimize for attributes such as factual accuracy, coherence, and user satisfaction, surpassing conventional supervised learning techniques. However, the application of RL in LLM alignment further amplifies the explainability challenge, as the complex interactions between RL updates and neural representations remain poorly understood.

This survey provides a systematic review of explainability methods in DRL, with a particular focus on their integration with LLMs and human-in-the-loop systems. We first introduce fundamental RL concepts and highlight key advances in DRL. We then categorize and analyze existing explanation techniques, encompassing feature-level, state-level, dataset-level, and model-level approaches. Additionally, we discuss methods for evaluating XRL techniques, considering both qualitative and quantitative assessment criteria. Finally, we explore real-world applications of XRL, including policy refinement, adversarial attack mitigation, and emerging challenges in ensuring interpretability in modern AI systems. Through this survey, we aim to provide a comprehensive perspective on the current state of XRL and outline future research directions to advance the development of interpretable and trustworthy DRL models.