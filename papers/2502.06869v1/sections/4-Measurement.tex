\section{Measuring XRL}
Evaluating the quality of explanations in RL requires a multi-faceted approach that captures both user-centered dimensions and objective metrics. This section outlines two broad categories of assessment, \ie qualitative and quantitative.

\subsection{Qualitative Evaluation}
\textbf{Interpretability and Clarity.}
At the heart of XRL is the need for explanations that humans find meaningful and intuitive. Qualitative evaluation often begins with user studies, such as surveys, to gauge how well participants understand the explanation and whether the information provided is perceived as coherent and sufficient for understanding policy decisions. Most researchers provide a visualization of the proposed explanation technique to demonstrate to the participants that the explanation can help them understand the DRL agent's behavior. For feature-level explanations, \citet{greydanus2018visualizing} generated saliency videos to show the feature-level explanations for Atari games and conducted a survey over 31 students at Oregon State University to measure how their visualization helps non-experts with these Atari games. For state-level explanations, \citet{cheng2023statemask} generated game trajectories with a color bar behind each frame to indicate the importance of each state and invited participants to answer a questionnaire to demonstrate their method StateMask could help humans gain a better understanding
of a DRL agent’s behavior.


\noindent\textbf{User-Centered Design Considerations.}
The qualitative evaluation also informs iterative refinement of explanation interfaces. By examining user reactions, researchers and designers can identify which presentation formats (e.g., visual overlays, textual rationales, or example-based justifications) are most effective. This feedback loop, encompassing pilot testing and usability reviews, ensures that explanations remain aligned with the domain’s practical needs and the target audience’s expertise.

\subsection{Quantitative Evaluation}
\textbf{Fidelity and Faithfulness.}
A key quantitative metric is how closely an explanation reflects the true policy or behavior of the RL agent. To evaluate the fidelity of the explanation in RL, researchers commonly use a perturbation-based approach~\citep{guo2021edge,cheng2023statemask}. The researchers remove features/states/data points identified as critical in the explanation and check if such a removal substantially degrades the agent’s performance. A dual form of this approach is to remove the non-critical features/states/data points and the agent's performance is expected to have limited difference. The fidelity score is further measured as the performance difference of the DRL agent before and after perturbing a fixed number of pixels/states/data points. When perturbing the same number of (critical) pixels/states/data points, a higher performance difference indicates a higher fidelity of the explanation method.


\textbf{Downstream Performance Impact.}
XRL systems can also be evaluated on whether their explanations enhance agent performance. For instance, \citet{chengrice} tested their proposed refining method based on the critical steps identified by different explanation methods and compared the agent’s performance after refining to evaluate the quality of these explanation methods.