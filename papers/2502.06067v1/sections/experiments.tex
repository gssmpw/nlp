\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/simulation/linear_regression/two_dim_shift_trig/combined_plot.pdf}
    \caption{Coverages (left) and confidence intervals (right) for our method as well as the 3 other methods that could efficiently run on this data. Only our method achieves the nominal coverage rate.}
    \label{fig:combined-plot-simulation-trig}
\end{figure*}


In simulated and real data experiments, we find that our
method consistently achieves nominal coverage, whereas all the alternatives dramatically fail to do so. We also provide ablation studies to evaluate the effect of varying the Lipschitz constant in both simulated and real settings.

\textbf{Baselines.} We compare to five alternative constructions.

\emph{Ordinary Least Squares (OLS).}
We treat the noise variance as unknown and estimate it as the average squared residual, with a correction for the number of degrees of freedom and a $t$-statistic instead of a $z$-statistic \citep[pp.\ 50--52]{greene2011econometric}.

\emph{Sandwich Estimator.}
The sandwich estimator \citep{huber_behavior_1967, white_heteroskedasticity-consistent_1980,white_1980_usingleastsquares} uses the same point estimate as OLS but a different variance estimate. We take the variance estimate from \citet[Equation 6]{MACKINNON1985305}: $\frac{1}{N-P}e_p^{\transpose}(X^{\transpose}X)^{-1}(X^{\transpose}RX)(X^{\transpose}X)^{-1}e_p$, where $R$ is a diagonal matrix with the squared residuals as entries. 

\emph{Importance Weighted Least Squares (KDEIW).}
As suggested by \citet[Section 9]{shimodaira_improving_2000}, we calculate importance weights via kernel density estimation (KDE). We select the bandwidth parameter with $5$-fold cross validation; see \cref{sec:implementation-of-baselines}. Given the KDE weights, we use the point estimate and confidence interval from weighted least squares.

\emph{Generalized Least Squares (GLS).}
We maximize the likelihood of 
$
    Y \sim \mathcal{N}(\theta^{\transpose} X, \Sigma),
$
with $\Sigma$ specified by an isotropic Mat\'ern 3/2 covariance function and a nugget, to select the parameters of the covariance function and nugget variance. Then we use the restricted spatial-regression framework \citep{hodges2010adding}; since we project the spatially-correlated error term onto the orthogonal complement of the covariates, the point estimate coincides with OLS. % Confidence intervals are inflated due to modeling correlations in the residuals. 

\emph{Gaussian Process Bayesian Credible Intervals (GP BCIs).} We use the model
$
    Y(S) = \theta^{\transpose} X(S) + g(S) + \epsilon, 
$
with $\theta^{\transpose} \sim \mathcal{N}(0, \lambda^2 I_P)$, $g \sim \mathcal{GP}(0, k_{\gamma})$, $k_{\gamma}$ an isotropic Mat\'ern 3/2 kernel function with hyperparameters $\gamma$, and $\epsilon \sim \mathcal{N}(0, \delta^2)$. We select $\{\lambda,\gamma, \delta\}$ by maximum likelihood. We report posterior credible intervals for $\theta_p$.

\textbf{Single Covariate Simulation.} In our first simulation, the source locations are uniform on $\spatialdomain = [-1,1]^2$ (blue points in \cref{fig:two-dim-shift-data} left plots). The target locations are uniform on $[\frac{-1 + \shift}{1+|\shift|}, \frac{1 + \shift}{1+|\shift|}]$, where $\mathrm{shift}$ controls the degree of distribution shift between source and target (orange points in \cref{fig:two-dim-shift-data} left plots). In this experiment, the single covariate $X=\chi(S) = S^{(1)} + S^{(2)}$ (\cref{fig:two-dim-shift-data}, third plot). And the response is $Y = X + \frac{1}{2}((S^{(1)})^2 + (S^{(2)})^2) + \epsilon$, with $\epsilon \sim \mathcal{N}(0,0.1^2)$. \cref{fig:two-dim-shift-data}, fourth plot, shows the conditional expectation of the response given location. We can compute the ground truth parameter in closed form because we have access to the conditional expectation of the response (\cref{eqn:test-set-conditional-ols}). We vary $\shift \in [0, \pm0.2,\pm0.4, \pm0.6, \pm0.8]$ and run 250 seeds for each $\shift$.

% We compare coverages between our method, ordinary least squares, the sandwich estimator, generalized least squares, a Bayesian credible intervals for a GP and importance weighting using kernel density estimation on the spatial locations. 
\Cref{fig:combined-plot-simulation}, left, shows that only our method and the GP consistently achieve nominal coverage. Given correct coverage, narrower (i.e., more precise) confidence intervals are desirable; \cref{fig:combined-plot-simulation}, right, shows that our method yields narrower intervals than the GP. KDEIW comes close to achieving nominal coverage when there is no shift. But under any shift, it is not able to fully debias the estimate, and coverage drops. For large $M$, we expect the sandwich estimator to achieve nominal coverage at $\shift=0$ since it is guaranteed to cover the population under first-order misspecification without distribution shift \citep{huber_behavior_1967,white_1980_usingleastsquares}. But under any of the depicted non-zero shifts, the sandwich, OLS, and GLS achieve zero coverage.

\begin{figure*}
\includegraphics[width=\textwidth]{figures/simulation/linear_regression/combined_plot_lipschitz.pdf}
    \caption{Left: the confidence interval width of our method as a function of shift for each Lipschitz constant $L$. All $L$ yield coverage of $1.0$. Middle and right: the confidence interval width (solid line with dot marker) as a function of the Lipschitz constant for $\shift=0$ (middle) and $\shift=0.8$ (right). The vertical axis is shared across all three plots. The bias contribution to the width (dashed line, x marker) is monotonically increasing in $L$. The randomness contribution (dashed line, square marker) is monotonically decreasing.}\label{fig:combined-plot-simulation-lipschitz}
\end{figure*}

The extreme narrowness of the OLS, sandwich, KDEIW, and GLS intervals (\cref{fig:combined-plot-simulation}, right) suggests that the problem with these methods is exactly their overconfidence. Because these approaches assume that the estimator is unbiased (or can be debiased), and that errors are independent and Gaussian, their intervals contract far too quickly, even with small amounts of data (here, $N=300$). Essentially, these methods' reliance on strong modeling assumptions leads to non-robust coverage.

We note that the confidence intervals and coverages for each method have (approximately) the same values for either $\pm \mathrm{shift}$ in this experiment (\cref{fig:combined-plot-simulation}, right) because the covariate is symmetric around the line $S^{(1)} = S^{(2)}$; see the middle right plot of \cref{fig:two-dim-shift-data} in \cref{app:simulations-additional-details}. So positive and negative $\mathrm{shift}$ values move the source and target locations symmetrically across the $S^{(1)} = S^{(2)}$ line. We do not expect such a symmetry for general covariates and will not see it in our next simulation (\cref{fig:combined-plot-simulation-trig}, right).


\textbf{Simulation with Several Covariates.} Our second simulation generates locations as in the previous experiment. Now we use $N = 10{,}000$ and $M=100$. And we generate 3 covariates, $X^{(1)} = \sin(S^{(1)}) + \cos(S^{(2)}),$ $X^{(2)} = \cos(S^{(1)}) - \sin(S^{(2)})$ and $X^{(3)} = S^{(1)} + S_2$. The response is $Y = X^{(1)}X^{(2)} + \frac{1}{2}((S^{(1)})^2 + (S^{(2)})^2) + \epsilon$ with $\epsilon \sim \mathcal{N}(0, 0.1^2)$. We focus on inference for the first coefficient. Calculation of $\hat{\sigma}_N^2$ scales poorly with $N$ due to needing to solve a quadratic program. So here we instead estimate $\sigma^2$ using the squared error of leave-one-out $1$-nearest neighbor regression fit on the source data. See \cref{sec:scalable-estimation-of-noise-variance} for details.
We compare against the same set of methods except we do not include GLS or GP BCIs since these require further approximations to scale for this $N$.

We again find that our method achieves coverage while the other methods do not (\cref{fig:combined-plot-simulation-trig}, left). In fact, in this experiment, no other method achieves coverage over 30\% across any $\shift$ value (even 0).  As before, we see that competing methods are overconfident, with very small CI widths (\cref{fig:combined-plot-simulation-trig}, right). 

For methods besides our own, coverage levels at $0$ are generally lower in this experiment at $\mathrm{shift}=0$ than in the previous experiment. The difference is that $N$ is much larger here (making confidence intervals narrower and exacerbating overconfidence) while $M$ is the same. The sandwich estimator covers the analogue of $\TestParamOLSp$ where spatial locations are treated as random. So it has good coverage at $\mathrm{shift}=0$ when $M \gg N$, but not when $M \ll N$.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/tree_cover/coverages_by_dim_conservative_west.pdf}
    \caption{Coverages (upper) and confidence interval widths (lower) for our method as well as 5 other methods. Each column represents a parameter in the tree cover experiment. Only our method consistently achieves the nominal coverage.}
    \label{fig:tree-cover-coverage-main-west}
\end{figure*}

\textbf{Effect of Lipschitz Constant on Confidence Intervals in Simulation Experiment.}
In the experiments above, we know the minimum value ($L_0$) for which \cref{assum:lipschitz} holds. Above we fit our method with $L=L_0$ ($L_0=2\sqrt{2}$ and $3\sqrt{2}$, respectively). 
Now we repeat the first simulation but vary $L \in \{0.1, 0.5, 1.0, 2.0, 3.5, 5, 7.5, 10\}$. All of these $L$ values yield coverage of $1.0$ for our method, above the nominal coverage of $0.95$, even though coverage is not guaranteed by our theory for $L \leq 2 \sqrt{2} \approx 2.8$.

We next demonstrate how the confidence interval width reflects a bias-randomness trade-off as $L$ varies. If the noise were known, the confidence interval would be monotonically increasing in $L$. Since it is unknown, only the contribution of the bias to the interval width ($2B$, Step~\ref{step:bias-calc} in \cref{alg:lipschitz_ci}) increases (\cref{fig:combined-plot-simulation-lipschitz}, middle and left, x marker). Conversely, smaller values of $L$ yield larger values for $\hat{\sigma}^2_N$, so the randomness contribution to the interval width ($2c$, Step~\ref{step:estimate-variance} in \cref{alg:lipschitz_ci}, square marker) increases (\cref{fig:combined-plot-simulation-lipschitz}, middle and left). The full confidence interval width, $2B + 2c \Delta(\alpha)$, is non-monotonic in $L$.

%In general we expect that when we extrapolate a large amount, larger values of $L$ will lead to wider confidence intervals, while for smaller levels of extrapolation, the order of confidence intervals for different $L$ is ambiguous (\cref{fig:combined-plot-simulation-lipschitz}, left). 

\textbf{Tree Cover Linear Regression.} We use a linear regression model
$
    Y_{\text{Tree Cover \%}} = \sum_{p \in \mathcal{P}} \theta_p X_p
$
to quantify how tree cover percentage in the contiguous United States (CONUS) in the year 2021 relates to three variables, $\mathcal{P} = \{\text{Aridity\_Index, Elevation, Slope}\}$.
We use the 983 data points from \citet{lu2024quantifying}, who in turn draw on \citep{usfs2023treecover,trabucco2019global,nasadem2020}. We define our target region in the West portion of CONUS as locations with latitude in the range (25, 50) and longitude in the range (-125, -110). Out of all points in this region, we designate $50\%$ --- totaling 133 sites --- as target data. Next, we select the source data by taking a uniform random sample of $20\%$ of the remaining spatial locations, repeated over 250 random seeds to assess coverage performance. Each seed yields 170 source locations. \cref{fig:tree-cover-split-west} illustrates the spatial split between source and target data for a representative seed. We discuss the data and our pre-processing in detail in \cref{app:tree-cover}.

We compare coverages for confidence intervals of the three parameters, $\theta_{\text{Aridity\_Index}}, \theta_{\text{Elevation}}, \text{ and } \theta_{\text{Slope}}$. We discuss how we evaluate coverage in \cref{app:real-data-coverage-computation}. In the top row of \cref{fig:tree-cover-coverage-main-west}, we see that our method is the only one to achieve the $95\%$ nominal coverage for all three parameters. Conversely, for the \textit{Slope} parameter, every other method achieves coverage at most $54\%$.
In the bottom row of \cref{fig:tree-cover-conf-intervals-west}, we again see that alternative methods fail to provide coverage due to their overconfidence (small widths). To see the confidence intervals themselves (rather than just the summary plots here), in \cref{fig:tree-cover-conf-intervals-west}, we depict all methods' constructed CIs across all three parameters for 7 of the 250 seeds. In \cref{app:tree-cover-southeast}, we conduct a similar analysis but with target locations in the Southeast, rather than West, of CONUS. The results align with our discussion here.

\textbf{Choice of Lipschitz Constant in the Tree Cover Experiment.}
For the tree cover experiment, we leverage domain knowledge to set the Lipschitz constant to $L = 0.2$, in units of percent tree cover per kilometer (km). This choice implies that a $1\%$ change in tree cover corresponds to moving $1 / 0.2 = 5$ km. To arrive at this choice, we observe that in certain regions of the U.S., such as the Midwest, tree coverage remains relatively uniform over several kilometers, so smaller Lipschitz constants (e.g., $L = 0.02$, corresponding to a $1\%$ change over 50 km) would be appropriate. However, in other regions --- such as the western U.S., where elevation changes are more pronounced (e.g., the Rockies, California, and the Pacific Northwest) --- tree cover can change sharply over short distances. To account for such variations conservatively, we choose $L = 0.2$. More generally, for real-world applications, we recommend the following strategy: (i) use domain knowledge to select a reasonable Lipschitz constant for the response variable, and (ii) inflate the Lipschitz constant to ensure a conservative estimate (which is more likely to satisfy \cref{assum:lipschitz}).

\textbf{Effect of Lipschitz Constant on Confidence Intervals in Tree Cover Experiment.}
In \cref{fig:tree-cover-multiple-lipschitz}, 
we show the coverage and width of our confidence intervals across three orders of magnitude of Lipschitz constants ($L$ from 0.001 to 1). For $L$ varying between 0.1 and 1 (a single order of magnitude variation around our chosen value of 0.2), we find that coverage is always met except in one case (\textit{Aridity Index} with $L=0.1$), where it is very close to nominal (89\% instead of 95\%). For \textit{Slope} and \textit{Elevation}, coverage is met or very nearly met for all $L$ values. For \textit{Aridity Index}, coverage is low for $L \leq 0.1$. Meanwhile, confidence intervals become noticeably wider for $L > 0.5$ while remaining relatively stable for smaller values. These results support our intuition to err on the side of larger $L$ values to be conservative and maintain coverage.

