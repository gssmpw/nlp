We next provide a confidence interval for $\TestParamOLSp$, the $p$th coefficient of the target-conditional least squares estimand (\cref{eqn:test-set-conditional-ols}). We support its validity with theory (in the present section) and experiments (in \cref{sec:experiments}). To that end, we start by providing an efficiently-computable point estimate. We end by discussing the role and choice of the Lipschitz constant $L$.

\textbf{Lipschitz-Driven Point Estimation.} Since the target covariates are known, the key challenge in estimating \cref{eqn:test-set-conditional-ols} is estimating the unknown quantity $\EE[\Ystar | \Sstar]$.

For our first approximation, recall that, by \cref{{assum:lipschitz}}, $f$ varies smoothly in space. Since the conditional distribution of the responses given the spatial locations is the same function for both target and source data (\cref{{assum:test-train-dgp}}), we can approximate $\EE[\Ystar | \Sstar]$ by a weighted average of $\EE[Y | S]$ values for locations $S$ near $\Sstar$. Concretely, let $\Psi \in \RR^{M \times N}$ be a (non-negative) matrix of weights. If $\Psi$ assigns weight mostly to source locations near each corresponding target location, then by the Lipschitz assumption (\cref{{assum:lipschitz}}), 
\begin{align}
\EE[\Ystar | \Sstar] \approx \Psi \EE[Y | S].
\label{eqn:source_approx}
\end{align}
%
$\EE[Y | S]$ is also unobserved, so we next approximate it by observed values of $Y$ (at each source location in $S$):
\begin{align}
\Psi \EE[Y | S]  \approx \Psi Y.
\label{eqn:data_approx}
\end{align}
%
Together, these two approximations yield the estimator:
\begin{align}\label{eqn:psi-point-estimate-ols}
    \hat{\theta}_p^{\Psi} = e_p^\transpose\left(\Xstart \Xstar \right)^{-1}\Xstart \Psi Y.
\end{align}
In our experiments, we construct $\Psi$ as follows. 
\begin{definition}[Nearest-Neighbor Weight Matrix]\label{def:1nn-psi}
    Define the 1-nearest neighbor weight matrix by
    \begin{align}
        \Psi_{mn} & = \begin{cases}
            1 & S_n = \textrm{closest source location to } \Sstar_m \\
            0 & \text{otherwise}
        \end{cases}
        .
    \end{align}
    We break ties uniformly at random. 
\end{definition}
While this simple construction works well in our present experiments, we discuss the potential benefits of other constructions in \cref{app:psi-choices}.

\begin{algorithm}[t!]
\caption{Lipschitz-Driven CI for $\TestParamOLSp$}
\label{alg:lipschitz_ci}
\begin{algorithmic}[1]
  \INPUT 
    $\{(S_n, X_n, Y_n)\}_{n=1}^N, \{(S_m^\star, X_m^\star)\}_{m=1}^M$, Lipschitz constant $L$, confidence level $1 - \alpha$, $\sigma^2$ (optional) 
  
  \OUTPUT A $(1-\alpha)$-confidence interval $I^\Psi$ for $\TestParamOLSp$
  \STATE

  $\Psi \gets \text{1-NN}(\{S_n\}_{n=1}^N, \{S_m^\star\}_{m=1}^M)$ (\cref{def:1nn-psi})

  \STATE

  $\hat{\theta}_p^\Psi 
      \;\gets\; 
      e_p^\top\bigl(X^{\star\top}X^\star\bigr)^{-1}\,X^{\star\top}\,\Psi\,Y$
    
  \STATE

    $w \gets e_p^\top\bigl(X^{\star\top}X^\star\bigr)^{-1} X^{\star\top}, v^\Psi \gets w\,\Psi$

    \STATE


  $B 
      \gets \!
      \sup_{g \in \lipschitzfns} \!
        \bigl|\,\sum_{m=1}^M w_m\,g(S^\star_m) \!-\! \sum_{n=1}^N v^\Psi_n\,g(S_n)\Bigr|.
    $ \alglinelabel{step:bias-calc}
    Compute with linear program (\cref{app:implementation-wasserstein}).
  \STATE

  If $\sigma^2$ unknown:
  \begin{align} \label{eqn:sigma-estimator}
      \smash{\sigma^2  := \hat{\sigma}^2_{N}
      \;\gets\;
      \inf_{g \in \lipschitzfns} \frac{1}{N}\,\sum_{n=1}^N 
            \bigl(Y_n - g(S_n)\bigr)^{2}}
  \end{align}
    by quadratic program (\cref{app:computation-variance-estimator}). \alglinelabel{step:estimate-variance}
    $\alglinelabel{step:sigma-unknown}$
  \STATE
    $c \gets \sigma\,\|v^\Psi\|_2$


    \STATE
    
    Find $\Delta(\alpha)$ satisfying
    \[
    \Phi\bigl(\Delta(\alpha)\bigr) 
      \;-\; 
      \Phi\Bigl(\tfrac{-2B}{c}-\Delta(\alpha)\Bigr) \;=\; 1 - \alpha,
    \]    
    by root finding algorithm (\cref{app:root-finding}).
    \STATE 
    $%\begin{align*}
        I^\Psi 
      \;\gets\;
      \Bigl[\,
        \hat{\theta}_p^\Psi - B - c\,\Delta(\alpha)
        \;,\;
        \hat{\theta}_p^\Psi + B + c\,\Delta(\alpha)
      \Bigr].
    $\alglinelabel{step:ci}%\end{align*} 
\end{algorithmic}
\end{algorithm}


\textbf{Lipschitz-Driven Confidence Intervals.} We detail how to efficiently compute our proposed confidence interval for $\TestParamOLSp$ in \cref{alg:lipschitz_ci}. We prove its validity in \cref{thm:ci} below. Before stating our result, we establish relevant notation and intuition for how our method works.
%
First, we show the difference between our estimand and the estimator is normally distributed. Toward that goal, we start by writing
\begin{align}
    \label{eqn:wv_intro}
   \TestParamOLSp - \hat{\theta}_p^{\Psi}
    =
    \sum\nolimits_{m=1}^M w_m f(\Sstar_m) - \sum\nolimits_{n=1}^N v^{\Psi}_nY_n,
\end{align}
for $w:= e_p^\transpose\left(\Xstart\Xstar \right)^{-1}\Xstart \in \RR^{M}$ and $v^{\Psi} := w\Psi  \in \RR^{N}$. By \cref{assum:test-train-dgp}, the righthand side of \cref{eqn:wv_intro} can be written 
\begin{align}\label{eqn:bias-variance-decomp}
\underbrace{\sum_{m=1}^M w_m f(\Sstar_m) \!-\!\sum_{n=1}^N \!\!v^{\Psi}_n f(S_n)}_{\text{bias}} - \underbrace{\sum_{n=1}^N v^{\Psi}_n\epsilon_n}_{\text{randomness}}.
\end{align}
%
That is, \cref{{eqn:bias-variance-decomp}} expresses $\TestParamOLSp - \hat{\theta}_p^{\Psi}$ as the sum of (i) a \textit{bias} term due to differing locations between the source and target data and (ii) a a mean-zero Gaussian \textit{randomness} term due to observation noise.

Since the spatial locations are fixed, the bias term is not random and can be written as $b \in \RR$. It follows that
\begin{align}\label{eqn:diff-gaussian}
    \TestParamOLSp  -  \hat{\theta}_p^{\Psi} \sim \mathcal{N}(b, \sigma^2\|v^{\Psi}\|_2^2)
\end{align}
since the variance of the randomness term is the sum of the variances of its (independent) summands.
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\linewidth]{figures/simulation/linear_regression/two_dim_shift/combined_plot.pdf}
    \caption{Coverages (left) and confidence interval widths (right) for our method as well as 5 other methods. Only our method and GP BCIs consistently achieve the nominal coverage rate (95\%) in this example; the GP BCIs line (dashed blue) overlaps with ours, in solid black, for most of the shifts. Of the two methods with correct coverage, our method yields much narrower intervals.}
    \label{fig:combined-plot-simulation}
\end{figure*}

Our strategy from here will be to (1) bound $b$, (2) establish a valid confidence interval using our bound on $b$ while assuming fixed $\sigma^2$, and (3) estimate $\sigma^2$ consistently (as $N\rightarrow\infty$).

To bound the bias $b$, we use \cref{assum:lipschitz} to write 
\begin{align}
   %\!\Big|\!\sum_{m=1}^M &w_m f(\Sstar_m) \!-\!\sum_{n=1}^N v^{\Psi}_n f(S_n)\Big| \nonumber \\ 
    |b| \leq \sup_{g \in \lipschitzfns} \left|\sum_{m=1}^M w_m g(\Sstar_m) -\sum_{n=1}^N \!v^{\Psi}_n g(S_n)\right|,\! \label{eqn:worst-case-bias}
\end{align}
where $\lipschitzfns$ is the space of $L$-Lipschitz functions from $\spatialdomain \to \RR$. In \cref{app:proof-weights}, we show that it is possible to use Kantorovich-Rubinstein duality to restate the righthand side of \cref{eqn:worst-case-bias} as a Wasserstein-1 distance between discrete measures. This alternative formulation is useful since it can be cast as a linear program \citep[Chapter 3]{peyre_computational_2019}; see \cref{app:proof-weights}. Let $B$ denote the output of this linear program.

Given $B \ge |b|$, the following lemma (with proof in \cref{proof:shortest-ci}) allows us to construct a confidence interval for $\TestParamOLSp$ centered on $\hat{\theta}_p^{\Psi}$. We discuss the benefits of this construction over alternative approaches in \cref{app:use-of-shortest-ci}.
\begin{lemma}\label{lem:shortest-ci}
    Let $b \in [-B, B]$, $\tilde{c} > 0$, and $\alpha \in (0,1)$. Then the narrowest $1-\alpha$ confidence interval that is symmetric and valid for all $\mathcal{N}(b, \tilde{c}^2)$ is of the form $[-B-\tilde{c}\Delta, B+\tilde{c}\Delta]$
     where $\Delta$ is the solution of:
    \begin{align} \label{eqn:root-finding-ci}
        \smash{\Phi\left(\Delta\right) - \Phi\left(\frac{-2B}{\tilde{c}}-\Delta\right) = 1 - \alpha}
    \end{align}
    with $\Phi$ the cumulative density function of a standard normal distribution. Also, the $\Delta$ satisfying this inequality is $\Delta \in [\Phi^{-1}(1-\alpha), \Phi^{-1}(1-\frac{\alpha}{2})]$.
\end{lemma}

The resulting confidence interval appears in \cref{alg:lipschitz_ci}. We next establish its validity. So far, we have covered only the known $\sigma^2$ case. We handle the unknown $\sigma^2$ case after the following theorem.
\begin{theorem}
    \label{thm:ci}
    Suppose $(\Sstar_m, \Xstar_m, \Ystar_m)_{m=1}^M$ and $(S_n, X_n, Y_n)_{n=1}^N$ satisfy \cref{assum:cov-fixed-fns,assum:lipschitz,assum:invertibility,assum:test-train-dgp} with known $\sigma^2$. Define the (random) interval $I^{\Psi}$ as in  \cref{alg:lipschitz_ci} using the known value of $\sigma^2$.
    Then with probability at least $1-\alpha$, $\TestParamOLSp \in I^{\Psi}$. That is, $I^{\Psi}$ has coverage (conditional on the test locations) at least $1-\alpha$. 
\end{theorem}

In \cref{app:proof-ci-thm}, we prove validity of our confidence interval for a generic choice of weight matrix $\Psi$. \cref{thm:ci} is an immediate corollary of that result.

\textbf{Consistent Estimation of the Noise Variance $\sigma^2$.}
Generally, the noise variance $\sigma^2$ is unknown, so we will substitute an estimate for $\sigma^2$ in the calculation of the confidence interval $I^{\Psi}$ (Step~\ref{step:sigma-unknown} in \cref{alg:lipschitz_ci}). In \cref{cor:ci-unknown-sigma} below, we show that the resulting confidence interval has asymptotically valid coverage. To that end, we first show that the estimator in \cref{eqn:sigma-estimator} is consistent for $\sigma^2$.
\begin{proposition}\label{prop:noise-variance-consistent}
 Suppose the spatial domain $\spatialdomain = [-A, A]^D$ for some $A>0, D \in \NN$. Take \cref{assum:test-train-dgp,assum:lipschitz}. For any sequence of source spatial locations $(S_n)_{n=1}^\infty$, take $\hat{\sigma}^2_N$ as in \cref{eqn:sigma-estimator}. 
Then $\hat{\sigma}^2_N \to \sigma^2$ in probability as $N \rightarrow \infty$.
\end{proposition}
%
See \cref{app:proof-noise-variance-consistent} for a proof. For intuition, recall that the conditional expectation minimizes expected squared error over all functions. Since the conditional expectation is $L$-Lipschitz (\cref{assum:lipschitz}), we have
\begin{align}
    \sigma^2  
    = \inf_{g \in \lipschitzfns} \EE\left[\frac{1}{N}\sum_{n=1}^N(Y - g(S_n))^2 \Big\vert S\right]. \label{eqn:sigma-sq-inf}
\end{align}
The empirical version of \Cref{eqn:sigma-sq-inf} is \cref{eqn:sigma-estimator},
which should be a good estimate for $\sigma^2$ if $N$ is large. 

When $\spatialdomain$ is a subset of Euclidean space or a subset of the sphere with spherical distance, the minimization in \cref{eqn:sigma-estimator} yields a quadratic program for computing $\hat{\sigma}^2_N$. We provide implementation details in \cref{app:computation-variance-estimator}.

Given \cref{prop:noise-variance-consistent}, it follows from Slutsky's Lemma that the resulting confidence interval is asymptotically valid.
%
\begin{corollary}\label{cor:ci-unknown-sigma}
For $\spatialdomain$ as in \cref{prop:noise-variance-consistent}, with the assumptions and notation of \cref{thm:ci}, but with $\sigma^2$ unknown, the confidence interval $I^{\Psi}$ from \cref{alg:lipschitz_ci} has asymptotic (with fixed $M$ and as $N \to \infty$) coverage at level $1-\alpha$.
\end{corollary}
%
See \cref{app:proof-noise-variance-consistent} for a proof.


\textbf{Choice of the Lipschitz Constant $L$.} The Lipschitz assumption allows us to make inferences about the target data from the source data. The Lipschitz constant subsequently enters our intervals in two principal ways: via $B$ and via $\hat{\sigma}^2_N$. Intuitively, larger values of $L$ (allowing for less smooth responses) lead to our algorithm constructing confidence intervals with larger bounds ($B$) on the bias but smaller estimated residual variance ($\hat{\sigma}^2_N$). We depict this trade-off in a concrete example in \cref{sec:experiments} (\cref{fig:combined-plot-simulation-lipschitz}).

Ultimately the choice of Lipschitz constant must be guided by domain knowledge. We give a concrete example describing our choice of the Lipschitz constant in our real-data experiment on tree cover (\cref{sec:experiments}). In our simulated experiments, we know the minimum value for which \cref{assum:lipschitz} holds; call it $L_0$. So we first choose $L=L_0$. Then we perform ablation studies in both simulated and real data showing that we essentially maintain coverage while varying $L$ over an order of magnitude around our initial choices. We show that further decreasing $L$ can decrease coverage and discuss why it is useful to err on the side of conservatism (i.e., a larger $L$).

%First, via $B$: Larger Lipschitz constants $L$ lead to larger values of $B$ because they make weaker smoothness assumptions. Second, via  $\hat{\sigma}^2_N$: Larger values of $L$ tend to imply smaller values of $\hat{\sigma}^2_N$ since the infimum in \cref{eqn:sigma-sq-inf} allows for less smooth functions. 