\subsection{Derivation of Target-Conditional Ordinary Least Squares}
\label{app:derivation-target-cond-ols}

We derive the OLS (Ordinary Least Squares) estimand for the given optimization problem:
\begin{align}
  \TestParamOLS = \arg\min_{\theta \in \mathbb{R}^P} \mathbb{E}\left[\sum_{m=1}^M (\Ystar_m - \theta^{\top}\Xstar_m)^2 \Big| \Sstar_m\right],  
\end{align}
where $\Ystar_m$ is unobserved, and $\Xstar_m$ is observed and a fixed function of $\Sstar_m$.

First, expand the squared term inside the expectation:
\begin{align}
    (\Ystar_m - \theta^{\top}\Xstar_m)^2 = (\Ystar_m)^2 - 2\Ystar_m \theta^{\top}\Xstar_m + (\theta^{\top}\Xstar_m)^2.  
\end{align}


Substitute this back into the expectation:
\begin{align}
   \mathbb{E}\left[\sum_{m=1}^M (\Ystar_m - \theta^{\top}\Xstar_m)^2 \Big| \Sstar_m\right] = \mathbb{E}\left[\sum_{m=1}^M (\Ystar_m)^2 - 2\Ystar_m \theta^{\top}\Xstar_m + (\theta^{\top}\Xstar_m)^2 \Big| \Sstar_m\right] 
\end{align}

Since the expectation is linear, we can separate the terms:
\begin{align}
  \mathbb{E}\left[\sum_{m=1}^M (\Ystar_m)^2 \Big| \Sstar_m\right] - 2 \mathbb{E}\left[\sum_{m=1}^M \Ystar_m \theta^{\top}\Xstar_m \Big| \Sstar_m\right] + \mathbb{E}\left[\sum_{m=1}^M (\theta^{\top}\Xstar_m)^2 \Big| \Sstar_m\right]  
\end{align}

Now we can simplify each of the terms as follows: 
\begin{itemize}
    \item The first term, $\sum_{m=1}^M \mathbb{E}\left[(\Ystar_m)^2 \Big| \Sstar_m\right]$, does not depend on $\theta$, so it can be treated as a constant with respect to the optimization problem
    \item The second term, $-2 \mathbb{E}\left[\sum_{m=1}^M \Ystar_m \theta^{\top}\Xstar_m \Big| \Sstar_m\right]$, can be rewritten using the linearity of expectation, the fact that \(\theta\) is not random, and \cref{assum:cov-fixed-fns}:
    \begin{align}
       -2 \theta^{\top} \sum_{m=1}^M \Xstar_m \mathbb{E}\left[\Ystar_m  \Big| \Sstar_m\right] 
    \end{align} 
    \item The third term, $\mathbb{E}\left[\sum_{m=1}^M (\theta^{\top}\Xstar_m)^2 \Big| \Sstar_m\right]$, is non-random by \cref{assum:cov-fixed-fns} and can be rewritten as:
    \begin{align}
        \sum_{m=1}^M (\theta^{\top} \Xstar_m)^2 
    \end{align}
\end{itemize}

The optimization problem then becomes
\begin{align}
    \TestParamOLS = \arg\min_{\theta \in \mathbb{R}^P} \left\{ \text{constant} -2 \theta^{\top} \sum_{m=1}^M \Xstar_m \mathbb{E}\left[\Ystar_m  \Big| \Sstar_m\right]  + \sum_{m=1}^M (\theta^{\top} \Xstar_m)^2 \right\}.
\end{align}
And since the constant term does not affect the optimization, we can drop it to get 
\begin{align}
    \TestParamOLS = \arg\min_{\theta \in \mathbb{R}^P} \left\{-2 \theta^{\top} \sum_{m=1}^M \Xstar_m \mathbb{E}\left[\Ystar_m  \Big| \Sstar_m\right]  + \sum_{m=1}^M (\theta^{\top} \Xstar_m)^2 \right\}
\end{align}

To find the minimizer, we take the derivative of the objective function with respect to $\theta$ and set it to zero:
\begin{align}
    \frac{\partial}{\partial \theta} \left\{-2 \theta^{\top} \sum_{m=1}^M \Xstar_m \mathbb{E}\left[\Ystar_m  \Big| \Sstar_m\right]  + \sum_{m=1}^M (\theta^{\top} \Xstar_m)^2 \right\} &= 0 \\
    -2 \sum_{m=1}^M \Xstar_m \mathbb{E}\left[\Ystar_m  \Big| \Sstar_m\right]  + 2 \sum_{m=1}^M \Xstar_m (\Xstar_m)^{\top}  \theta = 0
\end{align}

And by inverting this and using the fact that we assumed that $\Xstart\Xstar$ is invertible 
\begin{align}
    \TestParamOLS = \left( \sum_{m=1}^M \Xstar_m (\Xstar_m)^{\top}  \theta \right)^{-1} \sum_{m=1}^M \Xstar_m \mathbb{E}\left[\Ystar_m  \Big| \Sstar_m\right]
\end{align}

which in matrix form can be written as 

\begin{align}
\TestParamOLS &= (\Xstart\Xstar)^{-1}\Xstart\EE[\Ystar|\Sstar].
\end{align}

as in \cref{eqn:test-set-conditional-ols}.

\subsection{Proof of \cref{thm:ci}}\label{app:proof-ci-thm}
In the main text, we stated our confidence interval for the $1$NN choice of $\Psi$. We now state version for general non-negative matrices with columns summing to $1$.
\begin{theorem}
    \label{thm:ci-general-psi}
    Suppose $(\Sstar_m, \Xstar_m, \Ystar_m)_{m=1}^M$ and $(S_n, X_n, Y_n)_{n=1}^N$ satisfy \cref{assum:cov-fixed-fns,assum:lipschitz,assum:invertibility,assum:test-train-dgp}. 
    Define $w$ as in \cref{alg:lipschitz_ci}. For any $\Psi \in \RR^{M \times N}$, a matrix with non-negative entries with columns summing to $1$, define $v^{\Psi} = w\Psi$. Take $\hat{\theta}_p^{\Psi}$ as in \cref{eqn:psi-point-estimate-ols}. 
    Define $c=\sigma\|v^{\Psi}\|_2$, with $\sigma^2$ the variance of the additive noise from \cref{assum:test-train-dgp}. Define the (random) interval $I^{\Psi}$ as in  \cref{alg:lipschitz_ci} with known $\sigma^2$.
    Then with probability at least $1-\alpha$, $\TestParamOLSp \in I^{\Psi}$. That is, $I^{\Psi}$ has coverage (conditional on the test locations) at least $1-\alpha$. 
\end{theorem}
\begin{proof}
    We being as in the main text. We show the difference between our estimand and estimator is normally distributed. To that end, we decompose the difference between our estimand and estimator into a bias term and a mean-zero noise term.
\begin{align}
    \label{eqn:wv_intro-app}
    \TestParamOLSp - \hat{\theta}_p^{\Psi}
    &=
    \sum_{m=1}^M w_m f(\Sstar_m) - \sum_{n=1}^N v^{\Psi}_nY_n,
\end{align}
for $w:= e_p^\transpose\left(\Xstart\Xstar \right)^{-1}\Xstart \in \RR^{M}$ and $v^{\Psi} := w\Psi  \in \RR^{N}$. By \cref{assum:test-train-dgp}, the righthand side of \cref{eqn:wv_intro} can be written 
\begin{align}\label{eqn:bias-variance-decomp-app}
\underbrace{\sum_{m=1}^M w_m f(\Sstar_m) \!-\!\sum_{n=1}^N \!\!v^{\Psi}_n f(S_n)}_{\text{bias}} - \underbrace{\sum_{n=1}^N v^{\Psi}_n\epsilon_n}_{\text{randomness}}.
\end{align}

Since the spatial locations are fixed, the bias term is not random and can be written as $b \in \RR$. We can calculate the variance directly,
\begin{align}
    \mathbb{V}[\TestParamOLSp  -  \hat{\theta}_p^{\Psi} ] = \mathbb{V}[\sum_{n=1}^N v^{\Psi}_n\epsilon_n] 
     =\sum_{n=1}^N (v^{\Psi}_n)^2 \mathbb{V}[\epsilon_n] 
     = \sigma^2 \|v^{\Psi}\|^2_2.
\end{align}
We used the $\epsilon_n$ are independent and identically distributed with variance $\sigma^2$ (\cref{assum:test-train-dgp}).

It follows that
\begin{align}\label{eqn:diff-gaussian-app}
    \TestParamOLSp  -  \hat{\theta}_p^{\Psi} \sim \mathcal{N}(b, \sigma^2\|v^{\Psi}\|_2^2)
\end{align}

To bound the bias $b$, we use \cref{assum:lipschitz} to write 
\begin{align}
    |b| \leq \sup_{g \in \lipschitzfns} \left|\sum_{m=1}^M w_m g(\Sstar_m) -\sum_{n=1}^N \!v^{\Psi}_n g(S_n)\right|.\! \label{eqn:worst-case-bias-app}
\end{align}

We can therefore apply \cref{lem:shortest-ci} to complete the proof.
\end{proof}
\subsection{Computing an Upper Bound on the Bias of Our Estimand with Wasserstein-$1$ Distance}
\label{app:proof-weights}

First of all observe that if $\sum_{m=1}^M w_m = 1, \sum_{n=1}^N v_n^{\Psi} = 1$, $v_n^{\Psi} \geq 0$ for $1 \leq n \leq N$, and $w_m \geq 0$ for $1 \leq m \leq M$ then the supremum in \cref{eqn:worst-case-bias} would be equal to a Wasserstein-$1$ distance by Kantorovich-Rubinstein duality \citep[Theorem 5.10, Case 5.16]{villani2009optimal}.

Next, consider what happens if $\sum_{m=1}^M w_m - \sum_{n=1}^N v_n^{\Psi} \neq 0$. We show that the right-hand side of \cref{eqn:worst-case-bias} is infinite. Assume by contradiction that there exists a $C> 0$ that upper bounds this supremum. Because $\sum_{m=1}^M w_m - \sum_{n=1}^N v_n^{\Psi} \neq 0$ and all constant functions are $L$-Lipschitz, for any $\gamma > 0$
\begin{align}
\sup_{g \in \lipfns} \left \vert \sum_{m=1}^M w_m g(\Sstar_m) - \sum_{n=1}^N v_n^{\Psi} g(S_n)\right\vert &\geq \left \vert \sum_{m=1}^M w_m \frac{C + \gamma}{\sum_{m=1}^M w_m - \sum_{n=1}^N v_n^{\Psi}} - \sum_{n=1}^N v_n^{\Psi} \frac{C + \gamma}{\sum_{m=1}^M w_m - \sum_{n=1}^N  v_n^{\Psi}}\right\vert \\
& = C + \gamma.
\end{align}
This contradicts the assumption that $C$ is an upper bound on the supremum. Because $C$ was arbitrary, the right hand side of \cref{eqn:worst-case-bias} is infinite, as desired. 

Our assumption that $\Psi$ is a non-negative matrix whose columns sum to 1 avoids this situation. We formalize our upper bound on the bias in the following proposition.

\begin{proposition}
Suppose that $\sum_{n=1}^N  \Psi_{m,n} = 1$ for all $m$. Let $w \in \RR^{M}$ and $v^{\Psi}= (w\Psi)^{\transpose} \in \RR^{N}$.  
\begin{align}
    \sup_{g \in \lipschitzfns} \left|\sum_{m=1}^M w_m g(\Sstar_m) -\sum_{n=1}^N \!v^{\Psi}_n g(S_n)\right| = AL W_1\left(\sum_{m \in I} \frac{w_m}{A} \delta_{\Sstar_m}  + \sum_{n \in I'} \frac{-v_n^{\Psi}}{A} \delta_{S_n} , \sum_{m \in J} \frac{-w_m}{A} \delta_{\Sstar_m}  + \sum_{n \in J'} \frac{v_n^{\Psi}}{A} \delta_{S_n}\right),
\end{align}
where $I = \{ 1 \leq i \leq M : w_i \geq 0\}$, $I' = \{1 \leq i \leq N: v_i^{\Psi} < 0\}$,  $J = \{ 1 \leq j \leq M : w_j < 0\}$ and $J' = \{1 \leq j \leq N : v_j^{\Psi} \geq 0\}$ and $A = \frac{1}{2}\left(\sum_{m=1}^M |w_m| + \sum_{n=1}^N |v_n^{\Psi}|\right)$.
\end{proposition}
\begin{proof}
First, observe that $\sum_{n=1}^N v_n^{\Psi} = \sum_{n=1}^N (w\Psi)_n = \sum_{n=1}^N \sum_{m=1}^M w_m\Psi_{m,n} = \sum_{m=1}^M w_m \sum_{n=1}^N  \Psi_{m,n} = \sum_{m=1}^M w_m$. 

Next, we normalize the weights to sum in absolute value to $2$, and rescale the function class to consist of $1$-Lipschitz function. Define $A = \frac{1}{2}\left(\sum_{m=1}^M |w_m| + \sum_{n=1}^N |v_n^{\Psi}|\right)$. Then
\begin{align}
     \sup_{g \in \lipfns} \Big \vert \sum_{m=1}^M w_m g(\Sstar_m) \! - \! \sum_{n=1}^N v_n^{\Psi} &g(S_n)\Big\vert 
       \!=\! \sup_{g \in \lipfnsone} \left \vert \sum_{m=1}^M w_m Lg(\Sstar_m) \!-\! \sum_{n=1}^N v_n^{\Psi} Lg(S_n)\right\vert \\
     & = AL \sup_{g \in \lipfnsone} \left \vert \sum_{m=1}^M \frac{w_m}{A} g(\Sstar_m) - \sum_{n=1}^N \frac{v_n^{\Psi}}{A} g(S_n)\right\vert 
\end{align}
where we have used that a function is $L$-Lipschitz if and only if it can be written by scaling a $1$-Lipschitz function by $L$.

Define $I = \{ 1 \leq i \leq M : w_i \geq 0\}$, $I' = \{1 \leq i \leq N: v_i^{\Psi} < 0\}$,  $J = \{ 1 \leq j \leq M : w_j < 0\}$ and $J' = \{1 \leq j \leq N : v_j^{\Psi} \geq 0\}$. Then 
\begin{align}
    AL &\sup_{g \in \lipfnsone} \left \vert \sum_{m=1}^M \frac{w_m}{A} g(\Sstar_m) - \sum_{n=1}^N \frac{v_n^{\Psi}}{A} g(S_n)\right\vert  \\ &= AL \sup_{g \in \lipfnsone}\left \vert \sum_{m \in I} \frac{w_m}{A} g(\Sstar_m)  + \sum_{n \in I'} \frac{-v_n^{\Psi}}{A} g(S_n) 
   - \left(\sum_{m \in J} \frac{-w_m}{A} g(\Sstar_m)  + \sum_{n \in J'} \frac{v_n^{\Psi}}{A} g(S_n) \right)\right\vert 
\end{align}
Because $\sum_{n=1}^N v_n^{\Psi}=\sum_{m=1}^M w_m$ and $I$ and $J$ partition the index sets,
\begin{align}
    \sum_{m \in I} w_m + \sum_{n \in I'} -v_n^{\Psi} = \sum_{m \in J} -w_m  + \sum_{n \in J'} v_n^{\Psi}.
\end{align}
And because the set $I, I', J, J'$ sort the indices into positive and negative parts
\begin{align}
    \sum_{m \in I} w_m + \sum_{m \in J} -w_m + \sum_{n \in I'} -v_n^{\Psi} + \sum_{n \in J'} v_n^{\Psi} = \sum_{m=1}^M |w_m| + \sum_{n=1}^N |v_n^{\Psi}|=2A.
\end{align}

Therefore, 
\begin{align}
    \sum_{m \in I} \frac{w_m}{A} \delta_{\Sstar_m}  + \sum_{n \in I'} \frac{-v_n^{\Psi}}{A} \delta_{S_n} \text{\, and \,} \sum_{m \in J} \frac{-w_m}{A} \delta_{\Sstar_m}  + \sum_{n \in J'} \frac{v_n^{\Psi}}{A} \delta_{S_n}
\end{align}
are probability measures. We can apply Kantorovich-Rubinstein duality to write,
\begin{align}
    B \leq AL W_1\left(\sum_{m \in I} \frac{w_m}{A} \delta_{\Sstar_m}  + \sum_{n \in I'} \frac{-v_n^{\Psi}}{A} \delta_{S_n} , \sum_{m \in J} \frac{-w_m}{A} \delta_{\Sstar_m}  + \sum_{n \in J'} \frac{v_n^{\Psi}}{A} \delta_{S_n}\right).
\end{align}
where $W_1$ denotes the 1-Wasserstein distance. 
\end{proof}

We compute the Wasserstein-$1$ distance by linear programming, see discussion in \cref{app:implementation-wasserstein}. Scalable upper bounds could also be computed by exhibiting a coupling between the measures (for example by solving an entropy regularized optimal transport problem). See \citep[Chapters 3 and 4]{peyre_computational_2019} for details on computation of Wasserstein distances.

\subsection{Proof of \cref{lem:shortest-ci}}
\label{proof:shortest-ci}
\begin{proof}[Proof of \cref{lem:shortest-ci}]

We aim to prove that the interval $[-B - \Delta, B + \Delta]$ is the narrowest $1 - \alpha$ confidence interval that is valid for all $b \in [-B, B]$ where $\Delta$ is the solution of \cref{eqn:root-finding-ci}.

\textbf{Ensuring Coverage Probability.} Suppose that $X_b \sim N(b, \tilde{c}^2)$ for $b \in [-B - \Delta, B + \Delta]$. Then,
% \[
% \mathrm{Pr}_b(X \in [-B - \Delta, B + \Delta]) = \Phi\left(\frac{B + \Delta - b}{s}\right) - \Phi\left(\frac{-B - \Delta - b}{s}\right),
% \]
\[
\mathrm{Pr}(X_b \in [-B - \Delta, B + \Delta]) = \Phi\left(\frac{B + \Delta - b}{\tilde{c}}\right) - \Phi\left(\frac{-B - \Delta - b}{\tilde{c}}\right),
\]
To construct a valid confidence interval for any $b \in [-B,B]$, we require that
\[
\mathrm{Pr}(X_b \in [-B - \Delta, B + \Delta]) \geq 1 - \alpha, \quad \forall b \in [-B, B].
\]
This ensures  $1-\alpha$ coverage over all possible values of $b$ in $[-B, B]$.

\textbf{Reduce the problem to Worst-Case Coverage.} To find the narrowest interval, we identify the worst-case value of $b$ that minimizes the coverage probability. Let
\begin{align*}
C(b; \Delta) = \Phi\left(\frac{B + \Delta - b}{\tilde{c}}\right) - \Phi\left(\frac{-B - \Delta - b}{\tilde{c}}\right),
\end{align*}
denote the coverage probability of the interval $[-B-\Delta, B+ \Delta]$ for $X_b\sim \mathcal{N}(b, \tilde{c}^2)$.
In order to ensure the interval is valid for all $b$ coverage, we want to bound below.
\begin{align*}
\inf_{b \in [-B, B]} C(b; \Delta)
\end{align*}

The interval $[-B - \Delta, B + \Delta]$ is symmetric about $0$, and the Probability Density Function for a Gaussian of mean $b$ is symmetric about $b$. Thus, the coverage probabilities at $b = -B$ and $b = B$ are equal. Consequently, it suffices to consider $b \in [0, B]$.

Moreover, observe that $C(b; \Delta)$ is a strictly decreasing function of $b$ on $[0, B]$ since (i) $\Phi\left(\frac{B + \Delta - b}{\tilde{c}}\right)$ decreases as $b$ increases (because $B + \Delta - b$ decreases and $\Phi(z)$ is monotonic) and (ii) $\Phi\left(\frac{-B - \Delta - b}{\tilde{c}}\right)$ also decreases as $b$ increases (because $-B - \Delta - b$ becomes more negative). Thus, $C(b; \Delta)$ is a strictly decreasing function of $b$ on $[0, B]$. The minimum value of $C(b; \Delta)$ occurs at $b = B$.

\textbf{Ensuring coverage in the worst case.}
At the worst-case value $b = B$, the coverage probability is:
\[
C(B; \Delta) = \Phi\left(\frac{\Delta}{\tilde{c}}\right) - \Phi\left(\frac{-2B - \Delta}{\tilde{c}}\right).
\]
To ensure that the interval $[-B - \Delta, B + \Delta]$ achieves at least $1 - \alpha$ coverage for all $b \in [-B, B]$, we solve:
\begin{align}
\Phi\left(\frac{\Delta}{\tilde{c}}\right) - \Phi\left(\frac{-2B - \Delta}{\tilde{c}}\right) = 1 - \alpha. \label{eqn:root-finding-app}
\end{align}
This guarantees that the interval is valid for all $b$ and achieves the desired coverage level.

\textbf{Narrowest interval.} The narrowest interval corresponds to the smallest $\Delta$ that satisfies the \cref{eqn:root-finding-app}. By construction, any smaller $\Delta$ would fail to achieve the required coverage for $b = \pm B$, violating the validity condition.
\end{proof}


\subsection{Proof of \cref{prop:noise-variance-consistent}}\label{app:proof-noise-variance-consistent}

In this section, we prove \cref{prop:noise-variance-consistent}. For simplicity of exposition, we prove the result for $\spatialdomain = [0, 1]^D$. The result generalizes to any spatial domain which is a compact metric space.

\begin{proof}[Proof of \cref{prop:noise-variance-consistent}]
Using \cref{assum:test-train-dgp} and expanding the quadratic form $(\epsilon_n + (f(S_n) - g(S_n))$, we have 
\begin{align}
    \hat{\sigma}^2_N - \sigma^2 = Z_N + \zeta_N,
\end{align}
where $\zeta_N = \inf_{g \in \lipschitzfns} \frac{1}{N}\sum_{n=1}^N (f(S_n) - g(S_n))^2 +\frac{1}{N}\sum_{n=1}^N\epsilon_n (f(S_n) - g(S_n))$ and  $Z_N = \frac{1}{N}\sum_{n=1}^N \epsilon_n^2 - \sigma^2$. Since $Z_N$ is an average of independent and identically distributed variable, and since $\EE[Z_N] = 0$, the law of large numbers (LLN) implies $Z_N \to 0$ in probability. Because $Z_N \to 0$, if $\zeta_N \to 0$ in probability we can conclude by Slutsky's Lemma that $\hat{\sigma}^2_N \to \sigma^2$ in probability. Therefore, the remainder of the proof involves showing $\zeta_N \to 0$ in probability. 

 Define $f_N$ to be the empirically centered version of $f$, that is $f_N = f - \frac{1}{N}\sum_{n=1}^N f(S_n)$. Then since the space of Lipschitz functions is invariant to shifts by constant functions
\begin{align}
    \zeta_N = \inf_{g \in \lipschitzfns} \frac{1}{N}\sum_{n=1}^N (f_N(S_n) - g(S_n))^2 +\frac{1}{N}\sum_{n=1}^N\epsilon_n (f_N(S_n) - g(S_n)).
\end{align}
Define the process,
\begin{align}\label{eqn:emp-process-def}
    \tau_N(g) = \frac{1}{N}\sum_{n=1}^N (f_N(S_n) - g(S_n))^2 +\frac{1}{N}\sum_{n=1}^N\epsilon_n (f_N(S_n) - g(S_n)),
\end{align}
so that $\zeta_N$ is the infimum of $\tau_N$. $\tau_N(f_N) = 0$. Therefore, for $\zeta_N \leq 0$ almost surely. It remains to show that for any $\delta < 0$, $ \lim_{N \to \infty} \mathrm{Pr}(\zeta_N <\delta) \to 0$.

 The essential challenge to showing that for any $\delta < 0$, $ \lim_{N \to \infty} \mathrm{Pr}(\zeta_N <\delta) \to 0$ is the infimum over the space of Lipschitz functions. Our proof has three steps. First, we show that it suffices to consider a subset of the space of Lipschitz functions with bounded infinity norm. Second, we show that this space is compact as a subset of $L^{\infty}$. Third, we show that because the infimum is then over a compact set, it can be well-approximated by a minimum over a finite set of functions. And then a union bound suffices to prove the claim.


 \textbf{Step 1: It's Enough to Consider a Bounded Subset of Lipschitz Functions.}

Because $f_N$ has empirical mean $0$ and is continuous, by the intermediate value theorem it takes on the value $0$ somewhere on $[0,1]^D$. Because $f_N$ is $L$-Lipschitz and defined on a set of diameter $\sqrt{D}$, and $0$ somewhere inside this set, $\|f_N\|_{\infty} \leq L\sqrt{D}$.

Define the set $\overline{\lipschitzfns} = \lipschitzfns \cap B_{\infty}(2L\sqrt{D}+ 2 \sigma^2)$, where $B_{\infty}(r)$ denotes the space of functions uniformly bounded by constant $r$ on $[0,1]^D$.  By subadditivity of measure, for any $\delta < 0$
\begin{align}
    \mathrm{Pr}(\zeta_N < \delta) &\leq \mathrm{Pr}\left(\inf_{g \in \overline{\lipschitzfns}} \tau_N(g) < \delta\right) +  \mathrm{Pr}\left(\inf_{g \in \lipschitzfns\setminus \overline{\lipschitzfns}} \tau_N(g) < \delta\right) \\
    & \leq \mathrm{Pr}\left(\inf_{g \in \overline{\lipschitzfns}} \tau_N(g) < \delta\right) +  \mathrm{Pr}\left(\inf_{g \in \lipschitzfns\setminus \overline{\lipschitzfns}} \tau_N(g) < 0\right).
\end{align}

We first consider the second term in this sum and show it tends to $0$. We apply a crude Cauchy-Schwarz bound to the second term in \cref{eqn:emp-process-def} so that for any $g$, 
\begin{align}
    \tau_N(g) \geq \sqrt{\frac{1}{N}\sum_{n=1}^N (f(S_n) - g(S_n))^2}\left(\sqrt{\frac{1}{N}\sum_{n=1}^N (f(S_n) - g(S_n))^2} - \sqrt{\frac{1}{N}\sum_{n=1}^N\epsilon_n^2}\right).
\end{align}
Therefore, $\tau(g) < 0$ implies
\begin{align}
    \frac{1}{N}\sum_{n=1}^N\epsilon_n^2 \leq \frac{1}{N}\sum_{n=1}^N (f_N(S_n) - g(S_n))^2.
\end{align}
For any $g \in \lipschitzfns\setminus \overline{\lipschitzfns}$ because $g$ takes on a value at least $2L\sqrt{D}+ 2 \sigma^2$ and is $L$-Lipschitz, $g$ is larger than $L\sqrt{D}+ 2 \sigma^2$ over the entire unit cube. And because $\|f_N\| \leq L \sqrt{D}$, $f_N(S_n) - g(S_n) \geq 2 \sigma^2$ for all $n$. Therefore for all $g \in \lipschitzfns\setminus \overline{\lipschitzfns}$ 
\begin{align}
    \frac{1}{N}\sum_{n=1}^N (f_N(S_n) - g(S_n))^2 \geq 2\sigma^2.
\end{align}
We conclude that 
\begin{align}
    \lim_{N \to \infty} \mathrm{Pr}\left(\inf_{g \in \lipschitzfns\setminus \overline{\lipschitzfns}} \tau_N(g) < 0\right) \leq \lim_{N \to \infty} \mathrm{Pr}\left(\frac{1}{N}\sum_{n=1}^N\epsilon_n^2 \geq 2 \sigma^2\right) = 0.
\end{align}
where the final inequality is the law of large numbers.

\textbf{$\overline{\lipschitzfns}$ is a Compact Subset of the Space of Bounded Functions with Sup Norm.}
All that remains to show is that for any $\delta < 0$,
\begin{align}
    \mathrm{Pr}\left(\inf_{g \in \overline{\lipschitzfns}} \tau_N(g) < \delta\right)  \to 0.
\end{align}
The idea (following standard arguments made with empirical processes) is that we can take a cover of $\overline{\lipschitzfns}$ of finite size, such that each element of $\tau_N(g)$ is almost constant over elements of this cover. This essentially lets us approximate the infimum with a minimum over a finite set, up to small error. And once the problem is reduced to a minimum we can apply a union bound and the law of large number. We will formalize this in the next paragraph, but we first show that $\overline{\lipschitzfns}$ is compact.

Because every Lipschitz continuous function is equicontinuous, functions in $\overline{\lipschitzfns}$ are pointwise bounded by $2L\sqrt{D}+ 2 \sigma^2$, and $[0,1]^D$ is compact, we may apply Arzela-Ascoli \citep[Theorem 7.25]{rudin_principles_1976} to conclude that $\overline{\lipschitzfns}$ with the sup norm is sequentially compact. It is therefore compact, as a sequentially compact metric space is compact. 

\textbf{Step 3: Reduction to a Minimum over a Finite Set and a Union Bound.}

For any $\delta' < 0$ and any $g \in \overline{\lipschitzfns}$, 
\begin{align}\label{eqn:lln-tau}
    \lim_{N \to \infty} \mathrm{Pr}(\tau_N(g) < \delta') \leq \lim_{N \to \infty} \mathrm{Pr}\left(\frac{1}{N}\sum_{n=1}^N \epsilon_n g(S_n) < \delta'\right) = 0 
\end{align} 
where the final equality is the law of large numbers. 

Therefore, for any finite set $C \subset \overline{\lipschitzfns}$, 
\begin{align}\label{eqn:finite-lln-tau}
    \lim_{N \to \infty} \mathrm{Pr}(\min_{g \in C} \tau_N(g) < \delta') &\leq \lim_{N \to \infty} \sum_{g \in C} \mathrm{Pr }\left(\frac{1}{N}\sum_{n=1}^N \epsilon_n g(S_n) < \delta'\right) \\
    & = \sum_{g \in C} \lim_{N \to \infty}  \mathrm{Pr}\left(\frac{1}{N}\sum_{n=1}^N \epsilon_n g(S_n) < \delta'\right) = 0.
\end{align} 
We used countable subadditivity in the inequality. 

Now for any $\gamma >0$, there exists a finite set of functions $C_{\gamma} \subset \overline{\lipschitzfns}$ such that for any $g \in\overline{\lipschitzfns}$, there exists a $g' \in C_{\gamma}$ with $\|g - g'\|_{\infty} \leq \gamma$. And since $\tau_N(g)$ is pathwise uniformly (in $N$)  continuous on $\overline{\lipschitzfns}$ equipped with sup norm, 
\begin{align}\label{eqn:reduction-to-finite-case}
    \inf_{g \in \overline{\lipschitzfns}} \tau_N(g) \geq \min_{g \in C_\gamma} \tau_N(g) - \rho(\gamma)
\end{align}
where is a nonnegative function such that $\lim_{\gamma \to 0} \rho(\gamma) = 0$. Therefore, for any $\delta <0$, we can find a $\gamma$ such that $\rho(\gamma) \leq -\frac{\delta}{2}$. For this $\gamma$, applying \cref{eqn:finite-lln-tau} with $\delta' = \frac{\delta}{2}$ allows us to conclude that 
\begin{align}
    \lim_{N \to \infty} \mathrm{Pr}(\inf_{g \in \overline{\lipschitzfns}} \tau_N(g)  \leq \delta) =0.
\end{align}
This is a uniform law of large number for the class of Lipschitz and bounded functions. More quantitative results are likely possible using empirical process theory, see \citet[Chapter 5]{wainwright2019highdim}.
\end{proof}

\paragraph{Proof of Asymptotic Coverage (\cref{cor:ci-unknown-sigma}).}

We now prove \cref{cor:ci-unknown-sigma}. We begin by recalling the definition of an asymptotically valid confidence interval.
\begin{definition}
We say a sequence of (random) intervals $(I_N)_{N=1}^\infty$ has asymptotically valid coverage of $\theta$ at level $(1-\alpha)$ if 
\begin{align}
    \lim_{N \to \infty} \mathrm{Pr}(\theta \in I_N) = 1-\alpha
\end{align}
\end{definition}

\Cref{cor:ci-unknown-sigma} follows from \cref{thm:ci} and \cref{prop:noise-variance-consistent} by the following lemma, which is a special case of Slutsky's lemma.

\begin{lemma}\label{lem:pos-variance-convergence}
Let $\sigma^2 >0$. Suppose that $\TestParamOLSp  -  \hat{\theta}_p^{\Psi} -b_N \sim \mathcal{N}(0, \sigma^2)$ where $(b_N)_{N=1}^N$ is a fixed sequence. Suppose $\hat{\sigma}^2_N$ converges in probability to $\sigma^2$. Then,
\begin{align}
    \frac{1}{\hat{\sigma}^2_N}\left(\TestParamOLSp  -  \hat{\theta}_p^{\Psi} -b_N\right) \to \mathcal{N}(0, 1)
\end{align}
where convergence is in probability.
\end{lemma}
\begin{proof}
    The result is a special case of Slutsky's lemma, using $\hat{\sigma}^2_N \to \sigma^2 >0$.
\end{proof}


 \begin{proof}[Proof of \cref{cor:ci-unknown-sigma}]
     Define $b_N = \sum_{m=1}^M w_m f(\Sstar_m) - \sum_{n=1}^N v_n^{\Psi}f(\Sstar_n)$. By \cref{eqn:diff-gaussian}
     \begin{align}
         (\TestParamOLS -  \hat{\theta}_p^{\Psi} -b_N)\sim \mathcal{N}(0, \sigma^2).
     \end{align}
     If $\sigma^2 = 0$, then $\hat{\sigma}^2_N = 0$ for all $N$, because the conditional expectation is an $L$-Lipschitz function leading to $0$ squared error in the minimization algorithm used to calculate $\hat{\sigma}^2_N$. Therefore, the resulting confidence interval has coverage $(1-\alpha)$ for all $N$ by \cref{thm:ci}. 
     
     For $\sigma^2 > 0 $ we apply \cref{lem:pos-variance-convergence} to conclude that 
     \begin{align}
    \frac{1}{\hat{\sigma}^2_N}\left(\TestParamOLSp  -  \hat{\theta}_p^{\Psi} -b_N\right) \to \mathcal{N}(0, 1)
     \end{align}
     where convergence is in probability. 
     
     Convergence in probability implies convergence in distribution, and therefore convergence of quantiles at continuity points. The Gaussian CDF is continuous. Therefore, the quantile computation in \cref{lem:shortest-ci} using $\hat{\sigma}^2_N$ in place of $\sigma^2$ produces an asymptotically valid confidence interval in the limit as $N \to \infty$. 

 \end{proof}