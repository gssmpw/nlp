In this section, we describe the implementation details of our method. Particularly, this involves computing the upper bound on the bias, computation of $\Delta$ in \cref{lem:shortest-ci} and computation of $\hat{\sigma}^2_N$ from \cref{eqn:sigma-estimator}.

\subsection{Selecting the Matrix $\Psi$.}\label{app:psi-choices}
In the main text we selected $\Psi$ by using the nearest source location to each target location. We now provide additional discussion about alternative choices of $\Psi$ and possible trade-offs. 

A generalization of the $1$-nearest neighbor approach is to consider a $\Psi$ matrix determined by $K$-nearest neighbors.
\begin{definition}[Nearest-Neighbor Weight Matrix]\label{def:knn-psi}
    Define the $K$-nearest neighbor weight matrix by
    \begin{align}
        \Psi_{mn} & = \begin{cases}
            \frac{1}{K} & S_n \in \{ K \text{ \small closest source locations to } \Sstar_m \}\\
            0 & \text{ \small otherwise}
        \end{cases}
        .
    \end{align}
    For definiteness, we assume that, if multiple sources are the same distance from a target, ties are broken uniformly at random. 
\end{definition}

When using a K-nearest neighbor matrix, there is an inherent bias–variance trade-off in choosing K. Increasing K broadens the geographic range of source observations used, hence the bias will increase; however, it also lowers the variance by averaging over more responses.  We do not investigate this trade-off in detail. In the experiments we ran, we found $1$-nearest neighbor to work well. This is consistent with results in the mean estimation literature suggesting that if source and target distributions overlap substantially, the variance term remains manageable, even when using $1$-nearest neighbor approaches \citep{portier2023scalable}.

\subsection{Implementation of the Wasserstein Bound Calculation}\label{app:implementation-wasserstein}

We show in \cref{app:proof-weights} how \cref{eqn:worst-case-bias} reduces to computing a Wasserstein-1 distance between empirical measures. To implement the Wasserstein distance calculation, we rely on the Python Optimal Transport library \citep{flamary2021pot}. For the simulated experiments, we compute the cost matrix using Euclidean distances between spatial locations. For the real-world experiment, we use the Haversine distance to account for Earth’s curvature.

\subsection{Use of Confidence Interval \cref{lem:shortest-ci}}\label{app:use-of-shortest-ci}

In all experiments, we construct confidence intervals as in \cref{lem:shortest-ci}.
A simpler alternative, which guarantees $1 - \alpha$ coverage for all Gaussian distributions 
$\mathcal{N}(b, c^2)$ with $b \in [-B,B]$, is to form two-sided confidence intervals for each 
$b \in [-B,B]$ and then take their union. This produces an interval of the form
\begin{align*}
   [-B \;-\; c \,\Phi^{-1}\!\bigl(1 - \tfrac{\alpha}{2}\bigr)
  \;\;,\;\;
   B \;+\; c \,\Phi^{-1}\!\bigl(1 - \tfrac{\alpha}{2}\bigr)]
\end{align*}
  
However, the confidence intervals from \cref{lem:shortest-ci} are never longer than this
union-based approach, and the root-finding step required to compute them adds negligible overhead.
Consequently, we opt for the intervals in \cref{lem:shortest-ci} in all our experiments.



\subsection{Computation of $\Delta$ for \cref{lem:shortest-ci}}
\label{app:root-finding}
Define $g(\Delta) = \Phi(\Delta) - \Phi(-\frac{2B}{c}-\Delta) - 1 + \alpha$. Our goal is to find a root of $g$ in the interval $[\Phi^{-1}(1-\alpha), \Phi^{-1}(1-\alpha/2)]$. We first show that $g$ is monotonic, that a root exists in this interval, and that the root is unique.

Differentiating, we see $g'(\Delta) = \phi(\Delta) +\phi(-\frac{2B}{c}-\Delta)$, where $\phi$ denotes the Gaussian probability density function. This is strictly positive, and so $g$ is strictly monotone increasing. 

Also,
\begin{align}
    g(\Phi^{-1}(1-\alpha)) = 1- \alpha - \Phi\left(-\frac{2B}{c}-\Phi^{-1}(1-\alpha)\right) - 1 + \alpha < 1-\alpha -1 + \alpha =0.
\end{align}
by non-negativity of the CDF. And, 
\begin{align}
    g(\Phi^{-1}(1-\alpha/2)) &= 1- \alpha/2 - \Phi\left(-\frac{2B}{c}-\Phi^{-1}(1-\alpha/2)\right) - 1 + \alpha \\
    &= \alpha/2 - \left(1-  \Phi\left(\frac{2B}{c}+\Phi^{-1}(1-\alpha/2\right)\right). 
\end{align}
We used symmetry of the Gaussian in the second equality. Then,
\begin{align}
\Phi\left(\frac{2B}{c}+\Phi^{-1}(1-\alpha/2)\right) \geq 1-\alpha/2,
\end{align}
and so $g(\Phi^{-1}(1-\alpha/2)) \leq 0$. We conclude that $g$ has a root in the interval. By strict monotonicity of $g$, this root is unique. 

We use Brent's method \citep{brent_algorithm_1971} as implemented in Scipy \citep{2020SciPy-NMeth} to compute this root numerically.

\subsection{Computation of $\hat{\sigma}^2_N$ via quadratic programming}\label{app:computation-variance-estimator}
To estimate the noise parameter, we need to solve the minimization problem
\begin{align}
      \hat{\sigma}^2_N = \inf_{g \in \lipschitzfns} \frac{1}{N}\sum_{n=1}^N (Y_n - g(S_n))^2.
\end{align}

The first obstacle is that the infimum is taken over an infinite-dimensional space. However, 
by the Kirszbraun theorem \citep{Kirszbraun1934berDZ,Valentine1945}, every $L$-Lipschitz function 
defined on a subset of $\mathbb{R}^D$ or $\mathbb{S}^D$ can be extended to an $L$-Lipschitz function 
on the whole domain. Since our objective function depends only on the values of $g$ at the source 
spatial locations, we only need to enforce the Lipschitz condition between all pairs of source 
spatial locations, not over the entire domain. Enforcing the Lipschitz condition at these $N$ 
source locations amounts to $N(N-1)/2$ linear inequality constraints. 

Particularly, if we define $G = (g(S_1), g(S_2), \dots g(S_n))$, then we have the constraints
\begin{align}
    AG \leq L \mathrm{vec}(\Gamma)
\end{align}
where $L$ is the Lipschitz constant, 
$\Gamma \in \mathbb{R}^{N^2 - N}$ is the matrix of pairwise distances between distinct points in $S$, 
and $A \in \mathbb{R}^{(N^2 - N)\times N}$ is a sparse matrix with exactly one $1$ and one $-1$ in 
distinct rows of each column, representing all such pairs in its rows.

The objective is a symmetric, positive-definite quadratic form since it is a sum of squares. Therefore, the optimization problem is a quadratic program.

In practice, we use the Scipy sparse matrix algebra \citep{2020SciPy-NMeth} and the CLARABEL solver \citep{Clarabel_2024} through the CVXPY optimization interface \citep{diamond2016cvxpy,agrawal2018rewriting} to solve this quadratic program. 

\subsection{Scalable Estimation of $\sigma^2$}\label{sec:scalable-estimation-of-noise-variance}
The quadratic programming approach for estimating $\sigma^2$ outlined in the main text (\cref{eqn:sigma-estimator}) and described in detail in the previous section does not scale well to large numbers of source locations. Therefore, for our synthetic experiment with $N=10{,}000$, we take a different approach. 

For $1 \leq n \leq N$, let $\eta(n) = \arg\min_{n' \neq n} d_{\spatialdomain}(S_n,S_n')$. That is $\eta(n)$ is the index of the nearest point to $S_n$. Define the estimator,
\begin{align}
    \tilde{\sigma}^2_N = \frac{1}{2N}\sum_{n=1}(Y_n - Y_{\eta(n)})^2.
\end{align}
Then as long as $d_{\spatialdomain}(S_n,S_n') \approx 0$, by the Lipschitz assumption, $Y_n - Y_{\eta(n)} \approx \epsilon_n - \epsilon_{\eta(n)}$, and so 
\begin{align}
    \tilde{\sigma}^2_N \approx \frac{1}{2N}\sum_{n=1}^N(\epsilon_n - \epsilon_{\eta(n)})^2 & = \frac{1}{2N}\left(\sum_{n=1}^N\epsilon_n - \sum_{n=1}^N\epsilon_n\epsilon_{\eta(n)} + \sum_{n=1}^N\epsilon_{\eta(n)}^2\right).
\end{align}
Then,
\begin{align}
    \EE[\tilde{\sigma}^2_N ] = \sigma^2.
\end{align}

In general, we expect the estimate to concentrate around its expectation, provided that no single point in the source data is the nearest neighbor of too many other points in the source data.

\subsection{Implementation of Baseline Methods}\label{sec:implementation-of-baselines}
We now describe the details of the implementation of the baseline methods.

\textbf{Ordinary Least Squares.}
We use the ordinary least squares implementation from \texttt{statsmodel} \citep{seabold2010statsmodels}. We use the default implementation, which calculates the variance as the average sum of squared residuals with a degrees-of-freedom correction, as in \cref{eqn:ols-point-estimate-and-ci}. We use a $t$-statistic to compute the corresponding confidence interval, which is again the default in \texttt{statsmodel} \citep{seabold2010statsmodels}.

\textbf{Sandwich Estimator.}
We use the sandwich estimation procedure included in ordinary least squares in \texttt{statsmodels}  \citep{seabold2010statsmodels}. We use the \texttt{HC1} function, which implements the sandwich estimator with the degrees-of-freedom correction from \citet{Hinkley_1977_Jacknifing,MACKINNON1985305}. That is, the variance is estimated as $\frac{1}{N-P}e_p^{\transpose}(X^{\transpose}X)^{-1}(X^{\transpose}RX)(X^{\transpose}X)^{-1}e_p$. We use the default settings in \texttt{statsmodels}, which use a $z$-statistic with the sandwich estimator to compute the corresponding confidence interval.

\textbf{Importance Weighted Least Squares.}
We use the \texttt{scikit-learn} \citep{scikit-learn} implementation of kernel density estimation to estimate the density of test and train point separately. We use a Gaussian kernel (the default) and perform 5-fold cross validation to select the bandwidth parameter, maximizing the average log likelihood of held-out points in each fold. For the simulation experiments, we performed cross-validation to select the bandwidth parameters over the set $\{0.01, 0.025, 0.05, 0.1, 0.25, 0.5\}$. We selected this set of possible bandwidths to span several orders of magnitude from very short bandwidths, to bandwidths on the same order as the entire domain. We select the bandwidths for the source and target density estimation problems separately. For the real data experiments, the bandwidths was selected from the set $\{0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1\}$. This range was selected since the maximum Haversine distance between points in the spatial domain of interest is approximately 1. Once density estimates are obtained, we evaluate the ratio of the density function on the training locations, and use these as weights to perform weighted least squares. Weighted least squares is performed using the default settings in \texttt{statsmodels} \citep{seabold2010statsmodels}.

\textbf{Generalized Least Squares.}
We perform generalized least squares in a two-stage manner. We first approximate the covariance structure with a Gaussian process regression model. Then we use this approximation to fit a generalized least squares model with restricted spatial regression \citep{hodges2010adding}.

More precisely, first we optimize the maximum likelihood of a Gaussian process regression model with a linear mean function depending on the covariates including an intercept and Mat\'ern 3/2 kernel defined on the spatial source locations. We use the \texttt{GPFlow} \citep{GPflow2017} implementation of the likelihood, and L-BFGS \citep{liu_1989_lbfgs} for numerical optimization of the likelihood. The optimization is initialized using the \texttt{GPFlow} default parameters for the mean and covariance functions.

Once the maximum likelihood parameters have been found, we use the found prior covariance function for defining the covariance between datapoints to be used in the generalized least squares routine. We use restricted spatial regression \citep{hodges2010adding}, and so the covariance matrix is defined as $PKP + \lambda^2 I_N$, where $\lambda^2$ is the noise variance selected by maximum likelihood in the GP model, $K$ is the $N \times N$ matrix formed by evaluating the Mat\'ern 3/2 kernel with the maximum likelihood kernel parameters on the source locations, and $P$ is the orthogonal projection onto the orthogonal complement of the covariates (including intercept), i.e.
$
P = I_N - X(X^{\transpose}X)^{-1}X^{\transpose}.
$
This covariance matrix $PKP + \lambda^2 I_N$ is passed to the \texttt{GLS} method in \texttt{statsmodel}, and confidence intervals as well as point estimates are computed using the default settings. 

\textbf{Gaussian Process Bayesian Credible Intervals.}

We first optimize the maximum likelihood of a Gaussian process regression model with Mat\'ern 3/2 kernel defined on the spatial source locations summed with a linear kernel defined on the covariates. This has the same likelihood as having a linear mean function in the covariates with a Gaussian prior over the weights, see \citet[Page 28]{Rasmussen2006Gaussian}. We use the \texttt{GPFlow} \citep{GPflow2017} implementation of the likelihood, and L-BFGS \citep{liu_1989_lbfgs} for numerical optimization of the likelihood. The optimization is initialized using the \texttt{GPFlow} default parameters for the mean and covariance functions. Once we have calculated the maximum likelihood parameters, we compute the posterior credible interval for $\theta$. The posterior over $\theta$ is Gaussian and has the closed form,
\begin{align}
    \theta_{\mathrm{post}} \sim \mathcal{N}(\Sigma_{\mathrm{post}}^{-1}X^{\transpose}\Sigma^{-1}Y, \Sigma_{\mathrm{post}})
\end{align}
where 
\begin{align}
    \Sigma_{\mathrm{post}} = \left(X^{\transpose}\Sigma^{-1} X + \frac{1}{\lambda^2}I_P\right),
\end{align}
where $\lambda^2$ is the prior variance of $\theta$, $\Sigma = K + \delta^2 I_N$, where $\delta^2$ is the variance of the noise, and $K$ is the $N \times N$ kernel matrix formed by evaluating the Mat\'ern 3/2 kernel on the source spatial locations. The posterior for $e_p^{\transpose}\theta_{\mathrm{post}}$ is therefore also Gaussian, 
\begin{align}
    e_p^{\transpose}\theta_{\mathrm{post}} \sim \mathcal{N}(e_p^{\transpose}\Sigma_{\mathrm{post}}^{-1}X^{\transpose}\Sigma^{-1}Y, e_p^{\transpose}\Sigma_{\mathrm{post}}e_p). 
\end{align}
Credible intervals are then computed using $z$-scores together with this mean and variance. 
