In this section, we discuss related work on bias in linear regression in spatial settings, local approaches to regression that often rely on data-borrowing strategies similar to our nearest neighbor approach, and covariate shift.

\paragraph{Bias in Spatial Regression}

Linear models with a Gaussian process random effect --- that is, models of the form
\begin{align}
    Y(S) = \theta^{\transpose} X(S) + g(S) + \epsilon, 
\end{align}
with $g$ a Gaussian process and $\epsilon$ independent and identically distributed noise --- are a classic tool in spatial regression and remain widely used in applications \citep{weber_nnsvg_2023, gramacy2020surrogates, heaton2019case}. Whether to treat the covariates as fixed functions of spatial location or random is a topic of significant debate. In order for the model to be identifiable, it is generally necessary for the covariates to be thought of as random \citep[Proposition 1]{gilbert_consistency_2024}. However, \citet{paciorek_importance_2010} observed it may be more reasonable to think of covariates as fixed (the perspective we also take). If $X$ and $g$ are not independent (in the random case) or close to orthogonal in the fixed case, \citet{paciorek_importance_2010} illustrates that bias is introduced into the estimation of $\theta$. And that the degree of this bias depends on the relative scales over which $X$ and $g$ vary. He gives a closed form for this bias under strong linear-Gaussian assumptions that depend on parameters that would generally be unknown. \Citet{page_estimation_2017} builds on the result of \citet{paciorek_importance_2010}, and shows that bias in predictions made by the spatial model due to confounding may be small, despite biases in parameter estimation. \citet{hodges2010adding} proposed using restricted spatial regression, essentially explaining as much of the variation in the response as possible by the covariates to reduce bias in estimation due to spatially-correlated error. We take this approach in the GLS baseline used in our experiments. \Citet{nobre_effects_2021} extended earlier work about bias due to spatial confounding by considering the case when multiple observations are available at the same spatial location with independent noise. They showed that spatial regression models can still be biased with repeated observations due to confounding. 

In our work, we focus on bias in estimation of the target conditional OLS. Because we do not assume the model is well-specified, there is no `true' value of the parameter to estimate. Instead, our goal is to estimate the `best' (in a least squares sense) linear approximation to the response. We focus on the case when $X$ is a fixed function of location (\cref{assum:cov-fixed-fns}) and make weak non-parametric, smoothness assumptions (\cref{assum:lipschitz}). This contrasts with prior works that generally assume the covariates are random
and make linear-Gaussian assumptions to calculate the bias \citep{paciorek_importance_2010,page_estimation_2017}. Our work shows how to directly incorporate bias into confidence intervals for $\theta$ under our weak assumptions, but does not address issues of (non-)identifiability in the estimation problem.

\citet{gilbert_consistency_2024} showed that despite finite-sample bias in estimation of $\theta$ under spatial confounding, consistent estimation of $\theta$ is possible in the identifiable case when $X$ is random under infill asymptotics. This is essentially orthogonal to our work, as we focus on (finite-sample valid) confidence intervals in the case when $X$ is fixed.


\textbf{Local Versus Global Regression Estimates}

An alternative perspective on regression in spatial settings, referred to as \emph{geographically weighted regression} (GWR), focuses on estimating the association between the response and covariates at each location in space \citep{brunsdon1996geographically,fotheringham_geographically_2002}. For this regression problem to be well-defined, the covariates at each spatial location must be treated as random variables. Like our method, this approach uses nearest neighbors or local smoothing approaches to estimate the local coefficients at each location in space. And --- as in our work --- because of this ``data-borrowing'' from nearby locations, bias is introduced into the estimation of these local regression coefficients. The confidence intervals reported for GWR typically do not account for this bias \citep{yu2020measurement}. While \citet{yu2020measurement} provide a formula for the bias introduced by ``data-borrowing'', their formula relies on the assumption that the model is well-specified and depends on unobservable parameters. 

We focus on the estimation bias in the estimation of global parameters. Although local parameter estimates are often of interest in spatial analyses \citep[Chapter 1]{fotheringham_geographically_2002}, global estimates are also useful as summary statistics for making decisions that impact regions or multiple localities. And we characterize the bias in these estimates under weak, non-parametric assumptions and incorporate it into our confidence intervals. Extensions of our approach to GWR coefficients are a promising direction for future research. However, this extension is not simple, as it involves accounting for randomness in the covariates.

\textbf{Covariate Shift and Importance Weighting.}
Differences between the source locations (with observed responses) and the target locations (at which we want to study the relationship between variables) can be seen as an instance of covariate shift \citep{pmlr-v235-rolf24a, tuia2016domain,weber_nnsvg_2023}. Covariate shift is often dealt with by importance weighting, reweighting each training example to account for differences in the density of the source and target distributions. Prior work in spatial machine learning has generally focused on addressing covariate shift 
in the context of estimating a method's prediction error. However, these approaches can, in principle, be extended to parameter estimation in OLS, mirroring 
the broader covariate shift literature \citep{shimodaira_improving_2000}.
 

While in the main text we focused on density ratio estimation using kernel density estimation \citep{shimodaira_improving_2000}, many other approaches for density ratio estimation can be used \citep{Sugiyama_Suzuki_Kanamori_2012,kanamori2012statistical,gretton2008covariate,que2013inverse,Sugiyama2008kliep}. Importance weighting de-biases the estimator, but can only be used if the distribution of the source data is supported on a region containing the test data. And the confidence intervals obtained by importance weighting with estimated weights do not account for errors in estimation of the density ratio (if this exists). These confidence intervals are therefore not guaranteed to achieve the nominal coverage rate. And they are not applicable in cases where extrapolation is required, as the estimator cannot necessarily be de-biased in these cases. 

In contrast, our approach ensures nominal coverage even when de-biasing the estimate 
via importance weighting is not possible. This advantage comes at the cost 
of an additional regularity assumption (\cref{assum:lipschitz}), as well as often empirically wider confidence intervals.

% \paragraph{Sandwich Estimators.}
% The sandwich estimator \citep{huber_behavior_1967, white_heteroskedasticity-consistent_1980} for linear regression is of the form,
% \begin{align}
%     \hat{\Sigma} = (X^{\transpose}X)^{-1}(X^{\transpose}DX)(X^{\transpose}X)^{-1}
% \end{align}
% where $D$ is a diagonal matrix with $D_{nn} = r_n^2$, and $r_n$ is the $n$th residual. The sandwich estimator is thought of as robust in two senses. First, it can be used in the presence of heteroskedasticity. Second, it correctly estimates the population variance in the presence of first order mis-specification \citep{buja_models_2019, white_1980_usingleastsquares}. This means the sandwich estimator can be used in mis-specified cases when the covariates are independently sampled from the same distribution between the training and test covariates to cover the population parameter for the test distribution. However, the sandwich estimator does not account for biases introduced due to conditioning on the test covariates (or biases due to shift in distribution between the train and test covariates if the population parameter is of interest). And we will show in experiments that even in cases when the training and test covariates, the conditional parameter value (conditioned on the test covariates) may not be covered correctly, even if the population parameter is.

% \paragraph{Importance Weighting.}
% Importance weighting can be used to attempt to debias estimates of parameter of interest in the case when the covariates are treated as random. The idea is that by re-weighting the training examples in the estimating equation proportional to the density ratio between the test and training covariates, one obtains an unbiased estimate of the true parameter. This comes at a cost of increasing the variance of the estimator. In practice, the density ratio is generally not known (and may not exist), but is estimated from data. While both our approach and importance weighting use reweighting, the approximations made are fundamentally different. Importance weighting essentially tries to correct for differences between the testing and training examples by approximating the marginal distribution of the test covariates with a reweighted version of the distribution for the training covariates. We directly use the observed training covariates in our estimator, but then need to use a reweighting approach to approximate the conditional distribution. Our approach allows us to 1.) Obtain covariate-conditional coverage of the parameter; 2.) handle cases where re-weighting isn't sufficient to de-bias the estimator because the inference at the test covariates involves extrapolation. However, we require regularity of the conditional expectation of the response, \cref{assum:lipschitz}, which is not required when for importance weighting to provide unbiased estimates in the idealized case when the density ratio is exactly recovered.



% To discuss the connection to importance weighting, we take the view of random$-X$, with different distributions between the training and test examples. Because the distribution of $X$ is not ancillary in linear regression under mis-specification, $\theta^{\star}_p$ depends on the joint distribution of $(\Xstar, \ystar)~Q$. Under the assumption that $\ystar | \Xstar = y|X$, we can write $Q = \frac{dQ_X}{dP_X} P$, where $P$ is the training distribution, which we do have samples from. Let $T$ be the statistical functional that maps a distribution of $(X,Y)$ to the corresponding optimal linear regression parameter. Then, $T(Q) = T(\frac{dQ_X}{dP_X} P)$. The idea of importance weighting is to approximate $P$ with the empirical distribution of test points, and $\frac{dQ_X}{dP_X}$ with an density ratio estimation technique, and then use these estimates in the formula for computing the linear regression coefficient. Our approach exploits an essentially different factorization of the joint distributions. In particular, we write
% \begin{align}
%     q(X,Y) \approx \sum_{m=1}^M \delta_{\Xstar_m} q(Y| X = \Xstar_m) = \sum_{m=1}^M \delta_{\Xstar_m} p(Y| X = \Xstar_m) 
% \end{align}
% where the first approximation is exact if we are interested in the conditional parameter, and approximate for the population parameter using the empirical test distribution in place of the population test covariate distribution. The remaining difficulty is that we have observed samples from $P_{Y|X}$ at $(X_n)_{n=1}^N$, but want to approximate $ p(Y| X = \Xstar_m)$. But we can make an approximation, 
% \begin{align}
%     p(Y| X = \Xstar_m) \approx \sum_{n=1}^N \psi_{mn} p(Y| X = X_n),
% \end{align}
% where we approximate the conditional distribution at $\Xstar_m$ by a mixture of conditional distributions at the training inputs. We then essentially have a 


% \DRB{??? Discuss propensity scores?}

% \DRB{??? Discuss prior work on mean estimation?}


% Doesn't give conditional coverage. unclear how to tune bandwidth. assumes you can debias by re-weighting, whereas our approach doesn't (e.g.~we get conservative intervals for pure extrapolation). But we need a model for conditional distribution to do this.