%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{enumitem}
\setlist{topsep=0pt, leftmargin=*}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% my package
\usepackage{siunitx}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}
\usepackage[T1]{fontenc}
\usepackage{hyperref} 
\begin{document}

\twocolumn[
\icmltitle{HoP: Homeomorphic Polar Learning for Hard Constrained Optimization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}
% \author{
%     Ke Deng\textsuperscript{1}, 
%     Hanwen Zhang\textsuperscript{1}, 
%     Jin Lu\textsuperscript{2}, 
%     Haijian Sun\textsuperscript{1} \\
%     \textsuperscript{1}School of Electrical \& Computer Engineering, University of Georgia, Athens, USA \\
%     \textsuperscript{2}School of Computing, University of Georgia, Athens, USA \\
%     \texttt{ke.deng@uga.edu, hanwen.zhang@uga.edu, jin.lu@uga.edu, hsun@uga.edu}
% }
\begin{icmlauthorlist}
\icmlauthor{Ke Deng}{equal,yyy}
\icmlauthor{Hanwen Zhang}{equal,yyy}
\icmlauthor{Jin Lu}{xx}
\icmlauthor{Haijian Sun}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{School of Electrical \& Computer Engineering University of Georgia, Athens, USA}
\icmlaffiliation{xx}{School of Computing, University of Georgia, Athens, USA}
\icmlcorrespondingauthor{Ke Deng}{ke.deng@uga.edu}
\icmlcorrespondingauthor{Hanwen Zhang}{hanwen.zhang@uga.edu}
\icmlcorrespondingauthor{Jin Lu}{jin.lu@uga.edu}
\icmlcorrespondingauthor{Haijian Sun}{hsun@uga.edu}
% \icmlkeywords{Machine Learning, ICML}
% % \date{}
% \maketitle

\vskip 0.3in
]
\printAffiliationsAndNotice{\icmlEqualContribution} 

% 复杂非凸优化中，传统优化算法在计算速度上往往低效，而L2O可以作为一个

\begin{abstract}
Constrained optimization demands highly efficient solvers which promotes the development of learn-to-optimize (L2O) approaches. As a data-driven method, L2O leverages neural networks to efficiently produce approximate solutions. However, a significant challenge remains in ensuring both optimality and feasibility of neural networks' output. To tackle this issue, we introduce Homeomorphic Polar Learning (HoP) to solve the star-convex hard-constrained optimization by embedding homeomorphic mapping in neural networks. The bijective structure enables end-to-end training without extra penalty or correction. For performance evaluation, we evaluate HoP's performance across a variety of synthetic optimization tasks and real-world applications in wireless communications. In all cases, HoP achieves solutions closer to the optimum than existing L2O methods while strictly maintaining feasibility.

%we test HoP across a variety of optimization tasks and real-world scenarios in wireless communications. In all cases, HoP achieves solutions closer to the optimum than existing L2O methods while strictly maintaining feasibility.
\end{abstract}
% \footnotetext[1]{These authors contributed equally to this work. Ke Deng focused on the theoretical framework and algorithmic implementation, while Hanwen Zhang worked on experimental design and data analysis.}
\section{Introduction}
\label{Introduction}
% 逻辑：
\begin{figure}[ht]
\vspace{-1em}
\begin{center}
\centerline{\includegraphics[width=0.38\textwidth] {polar_projection_struc.pdf}} 
\caption{HoP is structured as follows: the gray box defines the problem considered in this paper, where the problem variable is expressed as $\mathbf{y}$, where $\mathbf{x}$ denotes problem parameters, and the objective function, respectively. The parameters $\mathbf{x}$ are fed into a NN, which contains a bounded activation function and produces a polar sphere vector comprising the direction vector $\mathbf{v}_{{\theta}}$ and the length scale $\bar{z}_r$. Using a homeomorphic mapping, the polar sphere vector is then transformed into Cartesian coordinates while strictly adhering to the original constraints. The warm-colored space on the right side represents the polar space corresponding to the polar sphere vector, while the green space is the Euclidean space for Cartesian coordinates. The loss function $f_\mathbf{x}(\mathbf{y})$ which can be trained end-to-end without requiring additional penalties or corrections.}
\label{polar_projection_struc}
\end{center}
\vspace{-3em}
\end{figure}
%（1）传统算法求解优化问题，需要大量时间，但是没有很好的解决办法, 因此使用L2O
Optimization takes a fundamental role in numerous scientific and engineering applications \cite{liu2024survey,abido2002optimal,rockafellar2013fundamental}. However, these algorithms often require significant computation time to solve complex problems, particularly when dealing with non-convex or high-dimensional cases. To address these limitations, the paradigm of learn-to-optimize (L2O) has emerged as a promising alternative \cite{hounie2024resilient,ding2024resilient}. As a learning-based scheme, L2O takes optimization parameters as the inputs of neural network (NN) which can efficiently obtain approximate solutions from the outputs. This data-driven approach enhances the speed and reduces the computational cost of achieving satisfactory solutions \cite{donti2021dc3,park2023self}.

%（2）提出L2O

Despite its advantages, L2O faces significant challenges when applied to constrained optimization. A major issue is the lack of guarantees for NN to ensure that solutions strictly remain within the feasible region defined by the constraints \cite{donti2021dc3}. Current works have attempted to address this limitation through various approaches, including supervised learning \cite{zamzam2020learning}, incorporating constrained violations into the loss function \cite{zhang2024constrained, xu2018semantic}, post-correction methods \cite{donti2021dc3}, implicit differentiation \cite{amos2017optnet}, and other techniques \cite{zhong2023neural,misra2022learning}. However, these methods often exhibit limitations in optimal solution searching and hard constraints violation control.

% 给出方法
We propose the homeomorphic polar learning (HoP) to address the non-convex problem with star-convex hard constraints. As stated in Definition \ref{star_convex}, star-convexity is a weaker condition than convexity, which introduces additional challenges in handling hard constraints. HoP is a learning based framework inspired by principles of convexity and topological structure of constraints, designed to ensure both the feasibility and optimality of solutions. As illustrated in Fig.~\ref{polar_projection_struc}, the problem parameters are fed into NN, where the raw outputs are regulated by bounded functions. Subsequently, through the proposed homeomorphic mapping, the polar sphere vectors are mapped to Cartesian coordinates, strictly following to the original constraints. Furthermore, HoP is trained end-to-end with objective function as loss directly, which makes it adaptable to various applications. The key contributions are
\begin{itemize}
    \item \textbf{Novel formulation of hard-constrained optimization via polar coordinates and homeomorphic mapping}. HoP is the first L2O framework based on polar coordinates and homeomorphism to solve hard constrained problem. Our formulation extends the applicability of L2O methods to star-convex constraints, a more complex and less explored constraint type.
    \item \textbf{Reconnection strategies and dynamic learning rate adjustment and  for polar optimization}. To address challenges specific to polar coordinate optimization, such as radial stagnation and angular freezing, we propose a dynamic learning rate adjustment scheme and geometric reconnection strategies. 
    % These include periodic angular mapping and radial domain extension, which ensure smooth transitions and continuity within the optimization process. 
    We provide rigorous theoretical analyses to validate the stability and efficiency of these solutions.
     \item \textbf{Superior experimental performance with zero violation.} 
    % The HoP framework achieves zero violation in experiments, ensuring strict adherence to constraints while delivering superior performance in non-convex optimization problems compared to state-of-the-art (SOTA) neural network-based solvers. 
    Through extensive ablation and comparative experiments, we validate the feasibility and optimality of our approach across a wide range of problems. Results consistently show that HoP outperforms both traditional and learning-based solvers in terms of constraint satisfaction and optimization efficiency.
\end{itemize}






\section{Related Work}
We present related works for constrained optimization problems using L2O. Broadly, researches in this area can be categorized into two distinct directions: soft and hard constrained L2O. 


\subsection{Soft Constrained Optimization with L2O}
% 普通罚项，ALM, PDL, BCD
Soft constrained L2O emphasizes on enhancing computational efficiency on NN inference speed while tolerating a limited rate of constraint violations. Early research in this domain explored the use of supervised learning (SL) to directly solve optimization problems, where the optimal variable $\mathbf{y}^*$ is provided as labels by optimizer \cite{zamzam2020learning,guha2019machine}.  Another prominent direction involves incorporating constraint violations into the objective function by Karush-Kuhn-Tucker conditions \cite{donti2021dc3,zhang2024constrained, xu2018semantic}. In this methods, constraints are reformulated as penalty terms and integrated into the objective function as the loss for self-supervised learning (SSL). Subsequent advancements introduced alternative optimization based learning, where variables and multipliers are alternately optimized through dual NNs \cite{park2023self,kim2023self,nandwani2019primal}. More recent related researches include preventive learning, which incorporates pre-processing in learning to avoid violation \cite{zhao2023ensuring}. Additionally, resilience-based constraint relaxation methods dynamically adjust constraints throughout the learning process to balance feasibility and overall performance \cite{hounie2024resilient,ding2024resilient}.





\subsection{Hard Constrained Optimization with L2O}
% PGD+DU, Effective Set, Implicit differentiation
Hard constrained optimization in L2O prioritizes strict adherence to constraints, even if it results in reduced optimality or slower computation speed. Traditional optimization methods often employ proximal optimization techniques to guarantee feasibility \cite{cristian2023end,min2024hard}. Early methods also used activation functions to enforce basic hard constraints \cite{sun2018learning}. Implicit differentiation became a popular approach for effectively handling equality constraints \cite{amos2017optnet,donti2021dc3,huang2021deepopf}. However, inequality constraints typically require additional correction steps, which can lead to suboptimal solutions \cite{donti2021dc3}. An alternative strategy proposed by \cite{li2023learning} utilized the geometric properties of linear constraints to ensure outputs within the feasible region, although this method is limited to linear constraints. Other studies such as \cite{misra2022learning,guha2019machine} focused on eliminating redundant constraints to improve inference speed instead of solving optimization problem by NNs directly. In certain physical applications, discrete mesh-based approaches restrict feasible solutions to predefined points on a mesh \cite{amos2017input,zhong2023neural,negiar2022learning}. While these methods strictly enforce feasibility, they often lack  flexibility in general scenarios.

\section{Methodology}
This section introduces the problem formulation, the proposed HoP framework, and associated theoretical analyses.
\subsection{Problem Formulation}
% version 3


We consider the following optimization problem:
\begin{flalign}\label{problem_formulaion}
    \min_\mathbf{y} \quad f_\mathbf{x}(\mathbf{y}),
    \quad \text{s.t.} \quad  \mathbf{y} \in \mathcal{Y}_\mathbf{x}, 
\end{flalign}
where $f_\mathbf{x}: \mathbb{R}^n \to \mathbb{R}$ is the objective function, and $\mathcal{Y}_\mathbf{x}$ is defined as star-convex constraint set, both parameterized by $\mathbf{x}$. Star-convexity is a weaker condition than convexity, which exists in $\ell_p$-norm problem and compressed sensing applications \cite{yang2022towards,donoho2011compressed}. Star-convexity is defined as below:
\begin{definition} \label{star_convex}
Let $\mathcal{Y} \subset \mathbb{R}^n$ be a non-empty set. We define $\mathcal{Y}$ as star-convex if $\exists \mathbf{y}_0 \in \mathcal{Y}$ such that $\forall \mathbf{y} \in \mathcal{Y}$, the following holds:
\begin{flalign}
    \bigl\{\, \mathbf{y}_0 + t(\mathbf{y} - \mathbf{y}_0) \mid 0 \leq t \leq 1 \,\bigr\} \subseteq \mathcal{Y}.
\end{flalign}
\end{definition}

% If this condition holds for $\forall\mathbf{y}_0 \in \mathcal{Y}$, $\mathcal{Y}$ becomes convex. Thus, star-convex sets generalize convex sets, accommodating constraint sets that are not globally convex while maintaining a degree of geometric regularity relative to specific points. In Section \ref{sec_hop}, we design HoP by the convexity in star-convex set. 

If this condition holds for $\forall\mathbf{y}_0 \in \mathcal{Y}$, $\mathcal{Y}$ becomes convex. Thus, star-convex sets generalize convex sets, accommodating constraint sets that are not globally convex while maintaining a degree of geometric regularity relative to specific points. In Section \ref{sec_hop}, we design HoP by the convexity in star-convex set.

\subsection{Homeomorphic Polar Learning}\label{sec_hop}

To demonstrate the proposed method, we first introduce the essential idea of HoP: homeomorphic mappings. These mappings leverage the mathematical properties of homeomorphisms, formally defined as follows:
\begin{definition}\label{Homeomorphism_def}
Let $ X = (S_X, \mathcal{T}_X) $ and $ Y = (S_Y, \mathcal{T}_Y) $ be two topological spaces, where: (1) $ S_X $ and $ S_Y $ are point sets;  (2) $ \mathcal{T}_X $ and $ \mathcal{T}_Y $ are topologies on $ S_X $ and $ S_Y $, respectively. Then function $\mathcal{H} \colon X \to Y$ is called a {homeomorphism} if and only if $ \mathcal{H} $ is a bijection and continuous with respect to the topologies $ \mathcal{T}_X $ and $ \mathcal{T}_Y $, while its inverse function $\mathcal{H}^{-1} \colon Y \to X $ exists and is also continuous.
\end{definition}

 
% \paragraph{1-D Constraints Problem: Vanilla Case}
\paragraph{The 1-D Case}

To simplify the mechanism of homeomorphic mappings, we begin with a one-dimensional optimization problem with constraints $a < y < b$, where the feasible region $\mathcal{Y}_{\mathbf{x}}$ is a bounded interval $(a, b)$. The corresponding homeomorphic mapping $\mathcal{H}$ is defined as:
\begin{flalign}\label{1d_case_eq}
\mathcal{H}\colon \quad \hat{y} = a + \mathcal{B}(z) (b-a),
\end{flalign}
% \begin{flalign}
% \mathcal{H} \colon \quad &\hat{y} =  y_0 + v\mathcal{B}(z)  \mathcal{R}(v), \quad v = \{+1, -1\}\\
% &\mathcal{R}(v) = \frac{(1+v)}{2}(b-y_0) +\frac{(1-v)}{2}(y_0-a), \notag
% \end{flalign}
where $z$ is output from NN, $\mathcal{B}(z)$ is a bounded, smooth and monotonic function (e.g., Sigmoid function), mapping $z$ from $\mathbb{R}$ to $(0, 1)$. To guarantee the feasible outputs, we scale the output from $(0,1)$ to $(a,b)$ by Eq. (\ref{1d_case_eq}) which is considered as simple one-to-one homeomorphic mapping defined in Definition \ref{Homeomorphism_def}.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{fig/Polar-Mapping-2d.png}}
\vskip -0.2in
\caption{Illustration of the 2-D HoP principle: The larger green area represents the feasible region $\mathcal{Y}_\mathbf{x}$, while $\mathbf{y}_0\in\mathcal{Y}_\mathbf{x}$. NN in the HoP framework outputs the blue dot as an initial solution within unit circle, the yellow region, centered on $\mathbf{y}_0$ and constrained by a bounded activation function. The blue dot is then scaled along the direction specified by unit vector $\mathbf{v}_{\theta}$ to the red dot $\mathbf{\hat{y}}$. The scaling factor is defined as the ratio given in ${r\mathcal{R}(\mathbf{v}_{{\theta}},\mathcal{Y}_\mathbf{x})}$.}

\label{polar-mapping-2d}
\end{center}
\vspace{-3em}
\end{figure}
The one-dimensional case provides the essence of HoP. In the following higher-dimensional constraint sets, the extended homeomorphism $\mathcal{H}$ is introduced to meet the requirement. The primary reason for introducing homeomorphic mappings is to guarantee that every feasible variable can be bijectively mapped to the NN output. This one-to-one correspondence is critical for maintaining feasibility and ensuring that after homeomorphic mapping the NN outputs consistently satisfy the constraints.
 

% \paragraph{From 1-D to 2-D: Extending the Insight}
\paragraph{Extension to the 2-D Case}
% version 3
Building on the insight from the 1-D case, we extend the idea of bounded function based mappings to the 2-D case while preserving the properties of homeomorphic mappings. 

% $\exists \mathbf{z}=[z_\theta,z_r]^T$ 
As shown in Fig.~\ref{polar-mapping-2d}, the feasible region $\mathcal{Y}_{\mathbf{x}}$ is the green region where $\mathbf{y}_0 \in \mathcal{Y}_{\mathbf{x}}$. We construct a unit circle, the yellow region, by polar coordinate system, with its origin centered at $\mathbf{y}_0$. Generally, $\mathbf{y}_0$ is obtained by solving a convex optimization problem over $\mathcal{Y}_{\mathbf{x}}$, where the objective is either a generic convex function or the Chebyshev center. Therefore, for $\forall \mathbf{\hat{y}} \in \mathcal{Y}_\mathbf{x}$, we have homeomorphic mapping $\mathcal{H}$ given as,
\begin{flalign}\label{2D_hop}
    \mathcal{H}\colon\quad&\mathbf{\hat{y}}=\mathbf{y}_0 + r\mathbf{v}_{{\theta}}\mathcal{R}(\mathbf{v}_{{\theta}},\mathcal{Y}_\mathbf{x})
\end{flalign}
where $r\in(0,1)$ is a scale, $\theta \in(0,2\pi)$ is the angle between x-axis and direction vector, $\mathbf{v}_{{\theta}} $ denotes the unit direction vector, and $\mathcal{R}(\mathbf{v}_{{\theta}},\mathcal{Y}_\mathbf{x})$ defines the distance from $\mathbf{y}_0$ to the boundary of $\mathcal{Y}_\mathbf{x}$ in the direction specified by $\theta$. The $\theta$, $r$, and $\mathbf{v}_{{\theta}} $ are defined as follows:
\begin{flalign}
    \begin{bmatrix}
    \theta\\
    r
    \end{bmatrix} =\begin{bmatrix}
    2\pi&0\\
    0&1
    \end{bmatrix}\begin{bmatrix}
   \mathcal{B}(z_\theta)\\
    \mathcal{B}(z_r)
    \end{bmatrix}, \quad
    \mathbf{v}_{{\theta}} = \begin{bmatrix}
    \cos{\theta}\\
    \sin{\theta}
    \end{bmatrix},
\end{flalign}
where $\mathbf{z} = [z_\theta, z_r]^T$ is raw NN output. It is worth noting that in optimization problems, redundant constraints, which do not affect the feasible region because they are implied by other constraints, can arise and significantly complicate the process of identifying boundary points. To address these challenges, it is critical to ensure that the homeomorphic mapping identifies the boundary points of the feasible region $\mathcal{Y}_\mathbf{x}$ accurately in the presence of redundant constraints. Our proposed method leverages the polar coordinate system to handle this issue effectively. When redundant constraints exist, $\mathcal{R}(\mathbf{v}_{{\theta}},\mathcal{Y}_\mathbf{x})$ finds boundary points by searching the closest intersection in the direction specified by $\theta$, as formalized in Proposition \ref{proposition_redundant}:
\begin{proposition} \label{proposition_redundant}
Let $ C_1, C_2, \dots, C_n $ be sets in the Euclidean space $\mathbb{R}^n$, and let their intersection $ C = \bigcap_{i=1}^N C_i $ be star-convex set. If $ \mathbf{y}_0 \in \operatorname{int}(C) $. For any ray originating from $ \mathbf{y}_0 $, the closest intersection point of the ray with $ C $ belongs to the set $C$ and lies on the boundary of $C$.
\end{proposition}
The proof of Proposition \ref{proposition_redundant} is given in Appendix \ref{proof_redundant}. As a result, the output $\mathbf{\hat{y}}$ computed using Eq. (\ref{2D_hop}) is guaranteed to be feasible for corresponding hard constraints $\mathcal{Y}_\mathbf{x}$. 

% \paragraph{From Bounded to Unbounded: Spherical Mapping} 
\paragraph{Extension to the Semi-Unbounded Case}

Since in the semi-unbounded problem, in which the feasible region extends indefinitely in some directions, making boundary determination challenging or ill-defined at infinity. $\mathcal{R}(\mathbf{v}_{{\theta}},\mathcal{Y}_\mathbf{x})$ given in 2-D scenario is impractical, as the intersection may not exist or could approach infinity. To address this issue, we introduce the following spherical homeomorphism mapping. The process begins with NN's raw outputs $\mathbf{z} = [\mathbf{z}_{{\theta}},{z}_{r}]^T, \mathbf{z}_{{\theta}}\in \mathbb{R}^{d},{z}_{r}\in \mathbb{R}$, where $d=2$ for current 2-D case while the following framework is also applicable to high-dimensional problem. To ensure the outputs are contained within a unit hypersphere, the transformations in Eqs. (\ref{recnection_theta}) and (\ref{recnection_r}) are applied to bound the raw outputs:
\begin{flalign}
    &\mathbf{v}_{{\theta}}  =
    \begin{cases}
    {\mathbf{{z}}_{{\theta}}}/{||\mathbf{{z}}_{{\theta}}||_2}, & \text{if } {z}_{r} \geq 0, \\
    {-\mathbf{{z}}_{{\theta}}}/{||\mathbf{{z}}_{{\theta}}||_2}, & \text{otherwise}.
    \end{cases}\label{recnection_theta}\\
    &\bar{z}_{r}  = \mathcal{B}(|{z}_{r}|),\label{recnection_r}
\end{flalign}
where $\mathbf{v}_{{\theta}}$ is direction vector, $\bar{z}_{r}$ is angle scale. Therefore, the (\ref{recnection_theta}) and (\ref{recnection_r}) can bound the output within unit hyper-sphere and avoid stagnation problem where we provide designing analyses in Section \ref{reconnection_theory}.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{fig/Polar-Mapping-3d.png}}
\vskip -0.1in
\caption{Sketch of the spherical coordinate transformation for semi-unbounded constraints. The 2-D plane (green plane) is elevated to a higher-dimensional system, where the distance $\mathcal{R} (\mathbf{v}_{\theta},\mathcal{Y}_{\mathbf{x}})$ in direction $\mathbf{v}_{\theta}$ from $\mathbf{y}_0$ to boundary, is mapped as the boundary angle $\phi$. Then NN's output ratio $\bar{z}_r$ and $\phi$ is transformed by Eq. (\ref{v_theta_transform}) to $\psi$, the inclination angle of blue ray, in horizontal direction $\mathbf{v}_{\theta}$. Finally, $\mathbf{\hat{y}}$ is the intersection of the blue ray and green plane. Furthermore, points at infinity in the green space correspond to the angle on equator where $\psi = \frac{\pi}{2}$.}
\label{polar-mapping-3d}
\end{center}
\vspace{-3em}
\end{figure}
Based on the polar sphere vector, $[\mathbf{v}_{\theta},\bar{z}_{r}]^T$, we set up polar coordinate system centered at $\mathbf{y}_0$. The 2-D homeomorphic mapping in Eq. (\ref{2D_hop}) can be extended to the HoP in semi-unbounded case and reformulated as follows:
\begin{flalign}\label{ND_hop}
    \mathcal{H}\colon\quad\mathbf{\hat{y}} = \mathbf{y}_0 +  \mathbf{v}_{\theta}\tan(\psi),
\end{flalign}
where  $ \psi$ is a angle defined as:
\begin{flalign} \label{v_theta_transform}
    &\psi = \bar{z}_r\phi,\quad\phi = \tan^{-1}(\mathcal{R}(\mathbf{v}_{\theta},\mathcal{Y}_\mathbf{x}))
\end{flalign}
%这里需要插入一段关于函数H的原理介绍，并且补充一下R的定义，这是一个分段函数
To illustrate the essence of spherical mapping, we use 2-D optimization problem as an example. As shown in Fig.~\ref{polar-mapping-3d}, the original 2-D plane (green plane) is elevated into an additional $z$-axis dimension with unit distance from point O to $\mathbf{y}_0$. Here, $\mathbf{v}_{\theta}$ represents the unit direction vector of green plane. $\phi\in (0,\pi/2)$ represents the maximum angle between z-axis and the green ray extending to the dark green boundary in the direction defined by $\mathbf{v}_{\theta}$. The supremum of $\phi$, denoted as $\text{sup}\phi$, corresponds to the angle between z-axis and an asymptotic direction toward infinity. 

% The blue ray defined by angle $\psi = \bar{z}_r\phi$, $\psi \in (0, \phi)$, with the $z$-axis, is bounded by the ratio $\bar{z}_r$. To prevent numerical instability caused by the divergence of the Jacobian determinant as $\psi \to \pi/2$, a small regularization term $\epsilon > 0$ is introduced, ensuring that $\psi$ remains constrained within $[0, \pi/2 - \epsilon)$. This restriction mitigates the effects of local volume collapse near $\psi = \pi/2$, as discussed in Appendix~\ref{sec:phi_v_mapping}.Consequently, the mapping in Eq.~(\ref{ND_hop}) defines a stable transformation of the angle $\psi$ to $\mathbf{\hat{y}}$ on the green plane. The transformed point $\mathbf{\hat{y}}$ lies closer to $\mathbf{y}_0$ than the boundary point in the direction $\mathbf{v}_{\theta}$, ensuring a robust representation of semi-unbounded regions.

The blue ray defined by angle $\psi = \bar{z}_r\phi$, $\psi\in(0,\phi - \epsilon)$ with z-axis, is bounded by ratio $\bar{z}_r$, where $\epsilon$ is a small positive number to prevent divergence of the Jacobian determinant which is explained in Appendix~\ref{sec:phi_v_mapping}. Then, Eq. (\ref{ND_hop}) maps the angle $\psi$ to $\mathbf{\hat{y}}$ on the green plan which is the intersection of green plane and blue ray. Since $\phi$ is boundary angle while $\psi$ is bounded by $\phi$, the feasibility of intersection point $\mathbf{\hat{y}}$ is ensured.

By transforming the distance to angle, this approach can resolve the challenge of semi-unbounded regions. Building on the semi-unbounded design, HoP can be naturally extended to higher-dimensional and more general scenarios. The pseudocode for the HoP is presented in Algorithm \ref{alg:HoP_final_algorithm}.

%插入一段伪代码来解释算法的输入输出是什么




% In the 2D case, as shown in Figure~\ref{polar-mapping-3d}, we lift the feasible region $ \mathcal{Y} \subset \mathbb{R}^2 $ into a 3D spherical coordinate system by introducing a vertical axis $ z $. The reference point $ \mathbf{y}_0 $ is augmented into three dimensions as $ [\mathbf{y}_0; 0] $, aligning it with the origin of the spherical coordinate system. Each point in the feasible region is parameterized by:
% \begin{itemize}
%     \item The horizontal angle $ \theta \in [0, 2\pi) $, defining the direction in the $ xy $-plane;
%     \item The inclination angle $ \varphi \in [0, \pi/2) $, controlling elevation;
%     \item The radial distance $ r \in [0, 1] $, scaling the point along the ray defined by $ (\theta, \varphi) $.
% \end{itemize}

% Here, the inclination angle $ \varphi $ is restricted to $[0, \pi/2)$, explicitly excluding $\pi/2$ from the output range to preserve the bijectivity required for a homeomorphic mapping. Including $\pi/2$ would result in a discontinuity in the inverse mapping, as $-\infty$ in the input space cannot correspond to a well-defined output point.

% In this spherical representation, the maximum radial distance $ R^*(\theta) $ in the original feasible region is expressed as:
% \begin{flalign}
%     R^*(\theta) = R \tan(\varphi^*),
% \end{flalign}
% where $ R $ is the radius of the spherical system. The final point $ \mathbf{y}^* \in \mathcal{Y} $ is computed as:
% \begin{flalign}
%     \begin{bmatrix}
%     \mathbf{y}^* \\ R
%     \end{bmatrix}
%     = 
%     \begin{bmatrix}
%     \mathbf{y}_0 \\ R
%     \end{bmatrix}
%     + r R \tan(\varphi)
%     \begin{bmatrix} 
%         \cos\theta \\ 
%         \sin\theta \\ 
%         1
%     \end{bmatrix}.
% \end{flalign}

% To ensure feasibility, the parameters are defined as:
% \begin{flalign}
%     \varphi = (\pi/2 - \epsilon_0) \cdot \phi_\varphi(z_\varphi), \quad \theta = 2\pi \cdot \phi_\theta(z_\theta), \quad r = \phi_r(z_r),
% \end{flalign}
% where $ \epsilon_0 > 0 $ is a small tolerance, and $ \phi_\varphi $, $ \phi_\theta $, and $ \phi_r $ are bounded and differentiable functions mapping raw network outputs $ z_\varphi, z_\theta, z_r $ into their respective ranges.

% This construction establishes a structured homeomorphic mapping from the surface of the unit sphere to the feasible region $ \mathcal{Y} $. The transformation elegantly handles semi-unbounded constraints by compressing infinite boundaries onto the bounded surface of the sphere while ensuring bijectivity, continuity, and a well-defined inverse mapping. Conceptually, this shift from volumetric to surface-based mappings avoids numerical instabilities and guarantees that the NN outputs are intrinsically aligned with the feasible region. The proposed method provides a mathematically rigorous and computationally efficient framework for handling both bounded and semi-unbounded constraints in high-dimensional optimization problems.

% \paragraph{Measure Distortion Near $\pi/2$: Jacobian Analysis.}  move to appendix
% The spherical transformation inherently introduces significant measure distortion near the equator of the sphere ($\varphi \to \pi/2$). The distortion is quantified by the Jacobian determinant:
% \begin{flalign}
%     J(\varphi, \theta, r) \propto R^2 \tan(\varphi) \sec^2(\varphi),
% \end{flalign}
% where $R^2$ accounts for the radial scaling, and $\tan(\varphi)\sec^2(\varphi)$ captures the geometric distortion. As $\varphi \to \pi/2$, this term diverges with cubic growth:
% $$
% \tan(\varphi)\sec^2(\varphi) \sim \frac{1}{(\pi/2 - \varphi)^3}.
% $$
% To mitigate this divergence, we enforce two mechanisms: bounding $\varphi$ with a fixed $\epsilon_0$ and incorporating activation functions for smoother control of $\varphi$.

% First, by imposing $\varphi \leq \pi/2 - \epsilon_0$, the distortion term $\tan(\varphi)\sec^2(\varphi)$ is capped at:
% $$
% \tan(\varphi)\sec^2(\varphi) \leq \cot(\epsilon_0)\csc^2(\epsilon_0) := M(\epsilon_0),
% $$
% where $M(\epsilon_0)$ is a finite constant determined by $\epsilon_0$. This ensures that the cubic divergence near $\pi/2$ is avoided entirely.

% Second, we map the raw NN outputs $X \in \mathbb{R}$ to $\varphi$ using bounded activation functions. For $\varphi = \frac{\pi}{2}A(X)$, activation functions such as the sigmoid or tanh provide smooth mappings from $\mathbb{R}$ to $[0, 1)$ and inherently limit the growth of $\varphi$'s derivative. The full Jacobian determinant, incorporating the activation function, becomes:
% $$
% J(\varphi, \theta, r) \propto R^2 \tan(\varphi) \sec^2(\varphi) \frac{\mathrm{d}\varphi}{\mathrm{d}X}.
% $$
% Here, $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$ reflects the bounded derivative of the activation function and plays a critical role in mitigating the distortion.

% For the sigmoid function, $\phi(X) = \frac{1}{1 + e^{-X}}$, the derivative is $\frac{\mathrm{d}\phi}{\mathrm{d}X} = \phi(X)(1 - \phi(X))$, which is maximized at $X = 0$ with a value of $\frac{1}{4}$. Thus, for $\varphi(X) = \frac{\pi}{2}\phi(X)$, we have:
% $$
% \frac{\mathrm{d}\varphi}{\mathrm{d}X} \leq \frac{\pi}{8}.
% $$
% Similarly, for the tanh-based activation function, $\phi(X) = \frac{1}{2}\tanh(X) + 1$, the derivative is $\frac{\mathrm{d}\phi}{\mathrm{d}X} = \frac{1}{2}\mathrm{sech}^2(X)$, which is maximized at $X = 0$ with a value of $\frac{1}{2}$. Therefore, $\frac{\mathrm{d}\varphi}{\mathrm{d}X} \leq \frac{\pi}{4}$. In both cases, $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$ remains bounded, ensuring smooth control over the mapping.

% Combining these results, the Jacobian determinant can be bounded as:
% $$
% |J(\varphi, \theta, r)| \leq R^2 M(\epsilon_0) C_A,
% $$
% where $C_A$ is the maximum derivative of $\varphi$ with respect to $X$. For sigmoid, $C_A = \frac{\pi}{8}$; for tanh, $C_A = \frac{\pi}{4}$. These bounded derivatives, along with the truncation $\epsilon_0$, effectively prevent measure distortion from diverging.

% The effectiveness of different activation functions can be summarized as follows:
% \begin{table}[h]
% \caption{Comparison of Jacobian scaling factors for different activation functions.}
% \label{jacobian-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{tabular}{|l|c|c|}
% \hline
% \textbf{Activation Function} & \textbf{Maximum $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$} & \textbf{Mitigation Level} \\
% \hline
% Sigmoid & $\frac{\pi}{8}$ & Strong \\
% Tanh & $\frac{\pi}{4}$ & Moderate \\
% No Activation & $1$ & None \\
% \hline
% \end{tabular}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% In conclusion, the combination of the $\epsilon_0$ truncation and bounded activation functions ensures that the measure distortion remains finite and well-controlled. While $\epsilon_0$ prevents true singularities, activation functions contribute additional mitigation by limiting the derivative $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$, ensuring stability in both theoretical and computational settings.



% \paragraph{Elegant Integration of Infinity.}
% One of the most compelling aspects of this approach lies in its elegant treatment of semi-unbounded constraints. Points at infinity, which are inherently problematic in traditional coordinate systems, are mapped seamlessly to the equator of the spherical system ($ \varphi = \pi/2 $). This mapping is not merely a technical solution but an intuitively satisfying geometric interpretation: infinity in the original space corresponds naturally to a finite, well-defined location in the spherical framework. 

% This transformation achieves two key objectives:
% \begin{itemize}
%     \item It ensures that every point in the feasible region, including those at infinity, is represented within a bounded domain.
%     \item It preserves the continuity of the mapping, smoothly transitioning points near infinity to the equator, thereby avoiding numerical instabilities.
% \end{itemize}

% \paragraph{Theoretical and Practical Advantages.}
% This method exhibits several theoretical and practical advantages that make it particularly compelling:
% \begin{itemize}
%     \item \textbf{Unified Framework:} By embedding both bounded and semi-unbounded constraints within a spherical system, the method unifies these two cases under a single, consistent representation.
%     \item \textbf{Simplicity and Elegance:} The natural association of infinity with the spherical equator is conceptually simple yet profoundly powerful, demonstrating the versatility of spherical coordinates in handling complex constraints.
%     \item \textbf{Geometric Intuition: }The approach offers a clear geometric interpretation, allowing us to visualize and reason about constraints in a higher-dimensional setting.
%     \item \textbf{Generalizability:} The spherical projection framework extends effortlessly to higher dimensions, accommodating constraints in $ \mathbb{R}^d $ with minimal modification.
% \end{itemize}

% \subsection{Polar Optimization Issue}

% \begin{figure}[ht]
% \begin{center}
% \centerline{\includegraphics[width=0.8\columnwidth]{fig/experiment_polar_visio_small.pdf}}
% \caption{Illustration of optimization trajectories under polar coordinates. The figures demonstrate the impact of learning rate, momentum, and geometric reconnection on the convergence behavior.}
% \label{polar-experiment}
% \end{center}
% \end{figure}

% The optimization process in polar coordinates, while providing a compact representation for many problems, introduces distinct challenges due to the coupling between the radial and angular variables. Figure~\ref{polar-experiment} illustrates these challenges, which we analyze rigorously below.

% \paragraph{Gradient Dynamics in Polar Coordinates.}

% Let the optimization target be a convex and Lipschitz-continuous function $ f: \mathbb{R}^2 \to \mathbb{R} $, with optimization variables $(x, y)$ in Cartesian coordinates. Converting to polar coordinates:
% $$
% x = r \cos\theta,\quad y = r \sin\theta,\quad r \geq 0,\quad \theta \in [0, 2\pi).
% $$
% The gradient in polar coordinates decomposes as:
% $$
% \frac{\partial f}{\partial r} = \cos\theta \frac{\partial f}{\partial x} + \sin\theta \frac{\partial f}{\partial y},\quad
% \frac{\partial f}{\partial \theta} = -r \sin\theta \frac{\partial f}{\partial x} + r \cos\theta \frac{\partial f}{\partial y}.
% $$
% Gradient descent updates the variables as:
% $$
% \begin{cases}
% r_{t+1} = r_t - \eta \left(\cos\theta_t \frac{\partial f}{\partial x} + \sin\theta_t \frac{\partial f}{\partial y}\right), \\
% \theta_{t+1} = \theta_t + \eta r_t \left(\sin\theta_t \frac{\partial f}{\partial x} - \cos\theta_t \frac{\partial f}{\partial y}\right),
% \end{cases}
% $$
% where $\eta > 0$ is the learning rate. 

% \paragraph{Radial Stagnation and Angular Freezing.}

% A critical issue arises due to the non-negativity constraint $ r \geq 0 $, enforced via:
% $$
% r_{t+1} = \max\{0, r_t - \eta \left(\cos\theta_t \frac{\partial f}{\partial x} + \sin\theta_t \frac{\partial f}{\partial y}\right)\}.
% $$
% When the radial update reduces $ r $ to zero, angular updates halt as:
% $$
% \theta_{t+1} = \theta_t + \eta r_{t+1} \left(\sin\theta_t \frac{\partial f}{\partial x} - \cos\theta_t \frac{\partial f}{\partial y}\right).
% $$
% This freezing effect is illustrated in Figure~\ref{polar-experiment}(a), where the optimizer stalls near $ r = 0 $, unable to escape stagnation. Such behavior is especially detrimental when the global optimum lies at an angular position $\theta_t + \pi$, unreachable from the current state.

% \paragraph{Sensitivity to Learning Rate and Momentum.}

% Two major factors exacerbate this stagnation:
% 1. Learning Rate: A large $\eta$ amplifies the risk of radial truncation, as $r_{t+1} = 0$ occurs when:
% $$
% \eta \bigl(\cos\theta_t \frac{\partial f}{\partial x} + \sin\theta_t \frac{\partial f}{\partial y}\bigr) \geq r_t.
% $$
% For Lipschitz gradient norms $\|\nabla f(x, y)\| \leq B$, this implies:
% $$
% \eta < \frac{r_t}{B}.
% $$
% As depicted in Figure~\ref{polar-experiment}(b), excessive $\eta$ leads to overshooting and radial truncation.

% 2. Momentum: Momentum-based optimizers, such as Adam, retain angular inertia, causing oscillatory behavior even near optima. Figure~\ref{polar-experiment}(c) highlights angular oscillations due to momentum, contrasting with the smoother trajectories of Adagrad in Figure~\ref{polar-experiment}(d).

% \paragraph{Geometric Reconnection Techniques.}

% To address stagnation, we propose two reconnection mechanisms:
% 1. Connecting $\theta = 0$ and $\theta = 2\pi$: Introducing periodic mappings (e.g., sine or cosine functions) ensures continuity across angular boundaries.
% 2. Extending $r$ to Negative Values: Allowing $r < 0$ doubles the feasible space. The transformation:
% $$
% r \mapsto |r|,\quad \theta \mapsto \theta + \pi,
% $$
% maintains equivalence between $(r, \theta)$ and $(-r, \theta + \pi)$. Figure~\ref{polar-experiment}(e) shows successful reconnection trajectories using this approach.

% \paragraph{Adaptive Learning Rate for Radial Updates.}

% To mitigate truncation-induced stagnation, we adjust the learning rate dynamically:
% $$
% \eta_t = \alpha r_t,\quad \text{where } \alpha < \frac{1}{B}.
% $$
% This ensures $ r_{t+1} > 0 $, even near $ r = 0 $, preventing radial freezing and preserving angular adjustability.

% Figure~\ref{polar-experiment}(f) demonstrates the impact of the proposed reconnection and adaptive learning strategies. The optimization trajectory avoids radial stagnation and angular freezing, achieving smooth and efficient convergence to the global minimum. By addressing the geometric and dynamic challenges of polar optimization, our approach enhances robustness and efficiency, enabling effective exploration of coordinate-sensitive parameterizations.

% \subsection{Geometric Reconnection Techniques Analysis}\label{reconnection_theory}
\subsection{Resolving Stagnation in Polar Optimization via Reconnection}\label{reconnection_theory}
%\subsection{Resolving Stagnation via Reconnection}\label{reconnection_theory}

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{fig/experiment_polar_visio_small.pdf}}
\vskip -0.2in
\caption{Comparison of optimization trajectories in Cartesian and polar coordinates. The left column demonstrates disconnection issues in polar coordinates, while the right column shows improved behavior with reconnection strategies.}
\label{polar-experiment}
\end{center}
\vskip -0.3in
\end{figure}

 % ${\theta}$ represents the angle between the origin and the current trajectory
In this subsection, we analyze the reason for optimizer stagnation during training procedure of the polar optimization system, and provide two corresponding solutions, including the reconnection strategy given in Eqs. (\ref{recnection_theta}) and (\ref{recnection_r}). 
Firstly, since the use of polar coordinates for optimization brings unique challenges in radial and angular variables updating by simple bounded activation function. To facilitate understanding of the consequence of stagnation, we provide equivalent 2-D convergence sketch in Fig.~\ref{polar-experiment} to demonstrate the phenomenon. As illustrated in two left sub-figures of Fig. ~\ref{polar-experiment}, a critical issue is \textit{radial stagnation and angular freezing}, where the optimizer stagnates near $r = 0$ and unable to adjust the angular variable ${\theta}$. 
% to demonstrate reconnection principal. 

The most significant reason is the non-negativity constraint, $r\geq 0$. When $r$ reaches zero and hold the tendency to be negative, the radial updates are truncated. Meanwhile, the angular updates stagnate completely due to the vanishing gradient caused by $r=0$. Furthermore, this phenomenon is exacerbated when optimizer has large learning rate or momentum. Specifically, this stagnation is clearly observed in left sub-figure of Fig.~\ref{polar-experiment}. The detailed mathematical formulation of this phenomenon and theoretical analysis is provided in Appendix ~\ref{prop:radial_stagnation} and ~\ref{prop:LR_Momentum}.

To mitigate radial stagnation, we provide two alternative solution. The first one is dynamically scaled learning rate for radial updates where adjustable learning rate of $r$ avoid its updating truncation near $r = 0$. This strategy guarantees stability and prevents freezing, as rigorously analyzed in the Appendix ~\ref{prop:Dynamic_LR}.

Another more robust solution is geometric reconnection techniques, shown in bottom right sub-figure in Fig.~\ref{polar-experiment}. When $r < 0$, we expand the angular and radial domains with reconnecting the polar space by
%The technique involves periodic angular mapping, where angular boundaries are treated as continuous by mapping $\theta = 0$ and $\theta = 2\pi$ to the same point. This approach ensures smooth angular transitions without discontinuities to effectively resolve boundary-related stagnation.
%The second technique extends the radial domain by allowing $z_r < 0$, doubling the feasible space with this transformation:
\begin{flalign}
r \mapsto |r|, \quad \theta \mapsto \theta + \pi.
\end{flalign}
Moreover, to guarantee angular is continuous, we output $\theta\in \mathbb{R}$ which connects $\theta=0$ and $\theta=2\pi$ periodically. This reconnection design allows the optimizer to traverse across the regions that were truncated in previous polar space, enabling smoother transition and better exploration of the solution space. 

% By resolving the radial truncation problem, this approach ensures that updates continue even when $r$ approaches zero to prevent stagnation.

Therefore, the geometric reconnection strategies significantly enhance the stability and efficiency of optimization in polar coordinates. As shown in Fig.~\ref{polar-experiment} (bottom right), the proposed methods enable the optimizer to maintain smooth trajectories, effectively avoiding the disconnection issues observed in Fig.~\ref{polar-experiment} (bottom left). As a consequence, we apply similar reconnection design in Eq. (\ref{recnection_theta}) and Eq. (\ref{recnection_r}) to prevent truncation of $\mathbf{z}_{\theta}$ and $z_r$.




% \begin{algorithm}[tb]
%    \caption{HoP}
%    \label{alg:HoP}
% \begin{algorithmic}
%    \STATE {\bfseries Initialization:} NNs parameters, optimizer, dataset, feasible point 
%    \STATE {\bfseries Train Procedure:}
%    % \REPEAT
%    \WHILE{Not converge}
%    \STATE Input train set batch data $\mathbf{x}$ into NNs
%    \STATE output $[{\theta,r}]^T$
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDWHILE
%    % \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}
\begin{algorithm}[tb]
   \caption{HoP: Homeomorphic Polar Learning}
   \label{alg:HoP_final_algorithm}
\begin{algorithmic}
   \STATE Prepare parameters $\mathcal{X}_{\text{train}}$ and $\mathcal{X}_{\text{test}}$, initialize NN, polar center $\mathbf{y}_0$ for every instance in $\mathcal{X}_{\text{train}}$ and $\mathcal{X}_{\text{test}}$.
   \STATE TRAIN:
   \FOR{every epoch}
   \FOR{every batch data $\mathbf{x}$ in $\mathcal{X}_{\text{train}}$}
   \STATE Update $[\mathbf{{z}}_{\theta},{z}_r]^T = \text{NN}(\mathbf{x})$;
   \STATE Update $\mathbf{v}_{\theta}$ by Eq. (\ref{recnection_theta}), ${\bar{z}_r}$ by Eq. (\ref{recnection_r});
   \STATE Output estimated variables $\mathbf{\hat{y}}$ by Eq. (\ref{ND_hop});
   \STATE Update loss function $\mathcal{L}(\mathbf{\hat{y}})$;
   \STATE Backward propagation to update NN parameters.
   \ENDFOR
   \ENDFOR
   \STATE TEST:
   \FOR{every batch data $\mathbf{x}$ in $\mathcal{X}_{\text{test}}$}
   \STATE Update $[\mathbf{{z}}_{\theta},{z}_r]^T  = \text{NN}(\mathbf{x})$;
   \STATE Update $\mathbf{v}_{\theta}$ by Eq. (\ref{recnection_theta}), ${\bar{z}_r}$ by Eq. (\ref{recnection_r});
   \STATE Output estimated variables $\mathbf{\hat{y}}$ by Eq. (\ref{ND_hop});
   \STATE Compute gap between $\mathcal{L}(\mathbf{\hat{y}})$ and $\mathcal{L}(\mathbf{{y}}^*)$;
   \ENDFOR
\end{algorithmic}
% \vskip -0.1in
\end{algorithm}
\vskip -0.1in
\section{Experiments}
% We evaluate proposed method for polygon constrained convex and non-convex problem, star domain constrained non-convex problem, high dimension linear constrained non-convex problem and a real-world optimization problem quality-of-service (QoS)-aware multi-output-
% single-input (MISO) communication system weighted sum rate problem (QoS-MISO WSR) where QoS-MISO WSR is NP-hard problem with quadratic constraints. To show the performance of proposed method, we set the metrics as same as DC3 \cite{donti2021dc3} to evaluate proposed method, included objective function optimality, feasibility of results and computation speed. Moreover, to demonstrate the fairness of the experiment, the ablation experiments and comparison experiments with traditional optimizer, DC3 are given as well. In details, the ablation experiments and comparison experiments setting are defined below:

% To evaluate the proposed method, we conducted experiments on two distinct categories of problems: 


% \paragraph{Synthetic Benchmarks} The synthetic benchmark experiments include the polygon-constrained problem, the $\ell_p$-norm problem, and the high-dimensional problem.  The polygon-constrained problem serves as the primary validation test, while the $\ell_p$-norm problem evaluates the performance of HoP in a star-convex scenario. The high-dimensional problem represents a general optimization setting, allowing us to validate our approach under conditions with redundant and semi-unbounded constraints.
% provides a general optimization setting to validate our approach under conditions with more redundant constraints and semi-unbounded constraints.

% \paragraph{Real-World Application} The quality-of-service-aware multi-input-single-output communication system weighted sum rate (QoS-MISO WSR) problem is a well-known NP-hard problem with non-linear constraints in communication engineering. In most studies on the QoS-MISO WSR problem, researchers commonly employ alternative optimization methods to obtain solutions, which often require significant computational resources. Therefore, it is considered as a real-world benchmark in our experiments.
% {Lu: All are redundant information!!}
% For fair and comprehensive evaluation, we adopt the metrics defined in DC3 \cite{donti2021dc3}, including objective function optimality, feasibility of results, and computation speed. Additionally, ablation studies and comparison experiments with traditional optimizers and DC3 are conducted to validate the proposed method's effectiveness. In details, the ablation experiments and comparison experiments setting are defined below:
% (\textbf{Lu: You have to explain a little about OBJ. VALUE, MAX. CONS, MEAN. CONS, VIOLATION RATE, and TIME  to connect them to the three things you defined here, since you have to interpret the tables}).
In following experiments we evaluate HoP by comparing with other methods as baselines in three aspects, including optimality, feasibility and computation efficiency. Comparative experiments with traditional optimizers, DC3 \cite{donti2021dc3} and other NN-based L2O approaches are conducted to validate HoP's effectiveness. All NNs follow a uniform architecture: a 3-layer multilayer perceptron (MLP) with ReLU activation functions. Since other NN-based methods such as DC3 typically rely on penalty functions to handle constraints, where the penalty is defined as: 
% (\textbf{Lu: I don't think that HoP method has penalty function, so why we adopt a similar penalty approach to handle constraint???})
\begin{flalign}
    \mathcal{P}(\mathbf{\hat{y}}, \mathcal{Y}_\mathbf{x}) = \mathbb{I}(\mathbf{\hat{y}}\notin \mathcal{Y}_\mathbf{x})\cdot\text{dist}(\mathbf{\hat{y}}, \mathcal{Y}_\mathbf{x})
\end{flalign}
where $\mathbb{I}(\hat{\mathbf{y}}\notin\mathcal{Y}_\mathbf{x})$ indicates constraint violations, and $\text{dist}(\mathbf{\hat{y}}, \mathcal{Y}_\mathbf{x})$ quantifies the distance to the constraint set. 

\begin{itemize}
    \item Optimizer: For synthetic benchmarks (Experiments \ref{exp:Synthetic}), the Sequential Least Squares Programming (SLSQP) is used as the solver. For the quality-of-service-aware multi-input-single-output communication system weighted sum rate (QoS-MISO WSR) problem in Experiment \ref{miso_prob}, Splitting Conic Solver (SCS) \cite{diamond2016cvxpy} and fractional programming (FP) \cite{shen2018fractional} is applied for alternative optimization as the solver baseline.
    
    \item NN-SSL (Self-Supervised Learning): The loss function consists individually of the objective function without any penalty term, defined as $\mathcal{L}(\mathbf{\hat{y}}) = f_{\mathbf{x}}(\mathbf{\hat{y}})$.
    
    \item NN-SL: The loss minimizes the mean squared error (MSE) between predictions $\hat{\mathbf{y}}$ and the target labels $\mathbf{y}^*$, expressed as $\mathcal{L}(\mathbf{\hat{y}}) = (\mathbf{\hat{y}} - \mathbf{y}^*)^2$.
    
    \item NN-SL-SC (Soft-Constraint + Supervised Learning): The loss function integrates the MSE and a soft constraint penalty, expressed as $\mathcal{L}(\mathbf{\hat{y}}) = (\mathbf{\hat{y}} - \mathbf{y}^*)^2 + \lambda \mathcal{P}(\mathbf{\hat{y}}), \lambda\geq 0$.
    
    \item NN-SSL-SC (Soft-Constraint + Self-Supervised Learning): The loss incorporates both the objective function and a soft constraint penalty: $\mathcal{L}(\mathbf{\hat{y}}) = f_{\mathbf{x}}(\mathbf{\hat{y}}) + \lambda \mathcal{P}(\mathbf{\hat{y}})$.
    
    \item DC3: This baseline follows \cite{donti2021dc3}, using the soft-constraint loss of NN-SSL-SC with additional gradient post-corrections to improve feasibility.
\end{itemize}
To ensure a fair and comprehensive evaluation, we utilize the same metrics proposed in DC3. Specifically, the metrics in Tables \ref{Polygon-table} -- \ref{miso-table} are defined as follows: (1) Obj. Value represents the objective function value, $f_{\mathbf{x}}(\mathbf{\hat{y}})$, achieved by the solver. (2) Max. Cons and Mean. Cons denote the $\text{max}{\mathcal{P}(\mathbf{\hat{y}})}$ and $\text{mean}({\mathcal{P}(\mathbf{\hat{y}})})$, respectively. (3) Vio. Rate is violation rate which measures the percentage of predicted infeasible solutions. (4) Time reflects the computational efficiency of each method. Moreover, the blue numbers in following tables denote the results from HoP, while the red numbers indicate the worst results or violation for corresponding metrics.
Further experimental setup details are provided in Appendix \ref{exp_setting_details}.

% \subsection{Synthetic Benchmarks: primary validation}
\subsection{Synthetic Benchmarks} \label{exp:Synthetic}
% \textbf{(Lu: What does it mean by the Moderately Non-Convex Function??? Don't use any soft definition which is not mentioned before using it.}

\begin{table*}[ht]
\caption{Experimental results on an 8-sided polygon constraint with varying constraints, using the sinusoidal QP as the objective function. The results demonstrate the performance of different methods in terms of objective value, constraint satisfaction, violation rate, and computation time. }
\label{Polygon-table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrrrr}
\toprule
Method & Obj. Value $\downarrow$ & Max. Cons $\downarrow$ & Mean. Cons $\downarrow$ & Vio. Rate $\downarrow$ & Time / \SI{}{\milli\second} $\downarrow$\\
\midrule
Optimizer& {-29.7252} & 0.0000 & 0.0000 & 0.00\% & {\color{red}0.07104} \\ %0.0710376
HoP & {\color{blue}-29.7170}& {\color{blue}0.0000}& {\color{blue}0.0000} &{\color{blue}0.00\%} & {\color{blue}0.00444}\\ %0.0044443
NN-SSL   & -29.7481 & {\color{red} 0.0334} & {\color{red}0.0029} &{\color{red}18.10\%}&0.00004 \\ %0.0000413
NN-SL   & -29.7247 & {\color{red}0.0153} & {\color{red}0.0001} & {\color{red}7.47\%}&0.00004 \\ %0.0000413
NN-SSL-SC   & {\color{red}-29.5601} & {\color{red}0.0108} & {\color{red}0.0001} & {\color{red}2.94\%}&0.00004\\ % 0.0000413
NN-SL-SC   & -29.7203 & {\color{red}0.0080} & {\color{red}0.0001} & {\color{red}0.35\%}&0.00004 \\ % 0.0000413
DC3    & -29.6893 & 0.0000 & 0.0000 & 0.00\% & 0.01491 \\ %0.0149148
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}

To evaluate Hop, we consider three different synthetic problems: the polygon-constrained problem and the $\ell_p$-norm problem and the high-dimensional semi-unbounded problem. The polygon-constrained problem serves as the primary validation for convergence, the $\ell_p$-norm problem evaluates the capability in addressing star-convex scenarios, and the high-dimensional semi-unbounded problem tests the method’s scalability and effectiveness in handling complex constraints in higher dimensions.
\paragraph{(a) Polygon-Constrained Problem}
As the first benchmark experiment, we choose sinusoidal quadratic programming (QP) as the non-convex objective function with linear constraints. The problem is formulated as follows:
\begin{flalign}\label{QP_sin_Poly}
    \min_{\mathbf{y}} \quad\frac{1}{2}\mathbf{y}^T\mathbf{Q}\mathbf{y} + \mathbf{p}^T\sin(\beta\mathbf{y}),
    \quad \text{s.t.}\quad \mathbf{A}\mathbf{y} \preceq \mathbf{b},
\end{flalign}
where matrix $\mathbf{Q}$ is a positive semi-definite matrix, vector $\mathbf{p}$ is a parameter vector, and scalar $\beta$ controls the frequency of the sinusoidal terms, which introduces non-convexity into the objective function. The constraint are defined by matrix $\mathbf{A}$ and vector $\mathbf{b}$. In this problem, the parameter $\mathbf{x}$ is $\mathbf{b}$, consistent with the setup in \cite{donti2021dc3}. 

% Table \ref{Polygon-table} presents the experimental results for Problem (\ref{QP_sin_Poly}) with an 8-sided polygon constraint. The proposed method, HoP, achieves perfect all constraint satisfaction with $0\%$ violation rate.  This matches the performance of the optimizer and DC3, both of which also satisfies all constraints.  In contrast, the NN-based approaches exhibit significant constraint violations. For instance, NN-SSL shows a Max. Cons of $0.1335$ and Mean. Cons of $0.0463$, with an $83.54\%$ violation rate.  Other NN methods also fail to fully satisfy constraints, with violation rate over $11\%$. Besides, compared to the traditional optimizer, HoP demonstrates a significant advantage in computational complexity given by NN where the processing speed is much faster than SLSQP and DC3.

Table \ref{Polygon-table} presents the results for Problem (\ref{QP_sin_Poly}) under an 8-sided polygon constraint. HoP achieves perfect constraint satisfaction, 0\% violation rate, while significantly outperforming NN-based methods, which exhibit violation rates up to 18.10\%. In terms of computational efficiency, HoP is over 15× faster than the optimizer. Moreover, objective value, -29.7170, given by HoP closely approaches the optimizer's -29.7252 and surpasses DC3's -29.6893. 

These results demonstrate HoP’s ability to enforce hard constraints rigorously, achieve nearer optimal solutions, and operate with superior computational efficiency in simple 2-D optimization problem which validate its effectiveness.

\begin{table*}[ht]
\caption{Experimental results on the $\ell_{0.5}$-norm constraint with $b=1$, using the QP as the objective function. The results demonstrate the performance of different methods in terms of objective value, constraint satisfaction, violation rate, and computation time.}
\label{lp-table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrrrr}
\toprule
Method & Obj. Value $\downarrow$ & Max. Cons $\downarrow$ & Mean. Cons $\downarrow$ & Vio. Rate $\downarrow$ & Time / \SI{}{\milli\second} $\downarrow$\\
\midrule
Optimizer & {-0.4824} & 0.0000 & 0.0000 & 0.00\% & {\color{red}1.49371} \\ %1.4937098
HoP & {\color{blue}-0.3886} & {\color{blue}0.0000}& {\color{blue}0.0000} &{\color{blue}0.00\%} &{\color{blue}0.00946}\\%0.0094566
NN-SSL   & -1.2227 & {\color{red}67.6524} & {\color{red}17.5511} & {\color{red}100.00\%} &0.00886\\%0.0088551
NN-SL   & -0.5285 & {\color{red}10.2304} & {\color{red}2.1575} & {\color{red}99.96\%} &0.00886\\ %0.0088551
NN-SSL-SC   & -0.0679 & {\color{red}0.4356} & {\color{red}0.0007} & {\color{red}0.75\%} &0.00886\\ %0.0088551
NN-SL-SC   & {\color{red}-0.0132} & {\color{red}0.3018} & {\color{red}0.0002} & {\color{red}0.02\%} &0.00886\\ %0.0088551
DC3    & -0.0730 & 0.0000 & 0.0000 & 0.00\% & 0.02295       \\ % 0.0229496
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}
\begin{table*}[t]
\caption{Results on high-dimension problem for $20$ variables with $20$ linear constraints. We use $14,000$ instances for training while $6,000$ samples for testing. The constraints are fixed in this problem while the objective function is different in each instance.}
\label{Table:high_dim}
\vskip -0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrrrr}
\toprule
Method & Obj. Value $\downarrow$ & Max. Cons $\downarrow$ & Mean. Cons $\downarrow$ & Vio. Rate $\downarrow$ & Time / \SI{}{\milli\second} $\downarrow$ \\
\midrule
Optimizer& {-7.6901}& 0.0000& 0.0000 & 0.00\%&{\color{red}4.87273}\\ %4.8727296
Hop &{\color{blue}{-4.7683}}& {\color{blue}0.0000}& {\color{blue}0.0000} & {\color{blue}0.00\%} &{\color{blue}0.09262}\\ %0.0274497
NN-SSL   & 0.2772&{\color{red}0.2079}& {\color{red}0.0115} & {\color{red}100.00\%} &0.04763\\ %0.0172332
NN-SL   &29.5046& 0.0000& 0.0000 & 0.00\% &0.04742\\ %0.0139702
NN-SSL-SC   & 17.3966&{\color{red}0.1758}& {\color{red}0.0001} & {\color{red}22.18\%} &0.05047\\ %0.0143304
NN-SL-SC   & {\color{red}43.9163}& 0.0000& 0.0000&0.00\%&0.04812\\ %0.0146192
DC3    & 8.9205& 0.0000&   0.0000&0.00\%  &  0.08768\\ %0.0265856
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.2in
\end{table*}
\paragraph{(b) $\ell_p$-norm Problem}
The second problem, an $\ell_p$-norm problem, features a QP objective function and star-convex constraints:
\begin{flalign}\label{eq:lp-norm}
\min_{\mathbf{y}}\quad \frac{1}{2} \mathbf{y}^T \mathbf{Q} \mathbf{y} + \mathbf{p}^T \mathbf{y},\quad\text{s.t.}\quad ||\mathbf{y}||_{\ell_{p}}^p \leq {b}
\end{flalign}
where vector $\mathbf{p}$ serves as the input variable $\mathbf{x}$ defined in Eq. (\ref{eq:lp-norm}) to the problem.  Unlike the linear and convex constraints in Section \ref{exp:Synthetic}(a), this problem introduces non-linear, non-convex star-convex constraints, which present a more challenging optimization scenario. 

% While L2O methods remain computationally efficient, achieving significant speedups over traditional optimizers, classical penalty-based methods relying on SL or SSL still fail to guarantee hard constraint satisfaction.
As shown in Table \ref{lp-table}, leveraging on topological equivalence given by HoP's homeomorphic mapping, HoP achieves perfect constraint satisfaction (0\% violation rate) with an objective value of -0.3886, significantly outperforming DC3's -0.0730. In contrast, DC3's gradient post-corrections often introduce bias by diverging from the objective-minimizing direction, leading to suboptimal solutions. Especially in the $\ell_p$ norm optimization, its complex constraint boundary brings more suboptimal corrections to DC3's gradient post-corrections, which highly rely on the gradient of the constraints.

Computationally, HoP is over 150$\times$ faster than the traditional optimizer and matches the speed of NN-based methods. Unlike other basic NN-based approaches which exhibit violative results and inferior objective values, HoP strictly enforces constraints while delivering superior solution quality. These results highlight HoP's effectiveness in handling star-convex constraints.

% Notably, HoP achieves significantly better objective values than DC3 in this non-convex scenario. This may imply that although DC3 ensures constraint satisfaction, its gradient correction steps introduce bias, as the correction direction towards the feasible region often diverges from minimizing the objective function. In contrast, HoP embeds NN outputs directly within the constraint set and uses a homeomorphic mapping, ensuring gradient continuity and topological equivalence. This enables HoP to perform smoother searches for optimal values within the feasible region.

% L2O methods exhibit remarkable computational efficiency, achieving speedups by several orders of magnitude compared to traditional optimizers. While traditional optimizers, such as SciPy, consistently achieve the lowest objective values and strictly satisfy all constraints, they incur significantly higher computational costs, making them less practical for large-scale or real-time applications. 

% Hop demonstrates superior performance among L2O approaches, achieving both high accuracy and perfect constraint satisfaction (\textit{zero violations}). Notably, under the star-convex $\ell_p$-norm constraints problem, our method even surpasses traditional optimizers in objective value, showcasing its robustness and adaptability to extreme scenarios. By contrast, DC3, although effective in ensuring constraint satisfaction, suffers from reduced accuracy due to the gradient correction steps, which limits optimal result. Other NN-based methods often achieve lower objective values at the expense of high violation rates, as they lack integrated mechanisms for hard constraints handling.

% In summary, HoP consistently achieves the best trade-off between objective value, constraint satisfaction, and computational efficiency in these basic benchmarks experiments.




% HoP demonstrates a strong balance between achieving a near-optimal objective value and perfect constraint satisfaction in (\ref{QP_sin_Poly}). Compared to SciPy, HoP offers comparable performance while maintaining the advantage of being a faesible, and optimality approach.  In contrast, NN based methods struggle to consistently satisfy constraints and often exhibit high violation rates.
% ccording to the results given in Table 1. Leveraging on
% homeomorphic mapping, HoP performs no violation with
% theoretical guarantee. By contrast, the other baselines have
% relatively higher violation, except DC3. Especially, all NN
% baselines have violation instances which is infeasible in hard
% constraints scenario, although their objective value exceed
% HoP or DC3. For HoP optimality, we find it has achieved
% −37.4765 closer to the optimum than the −37.1932 from
% DC3. Overall, in this polygon-constrained problem, HoP
% and DC3 share the similar performance and feasibility in
% hard constraints.

% \paragraph{Problem Types:}
% The optimization problems in this study follow the general formulation in Eq.~\eqref{problem_formulaion}. Based on the role of the parameter $\mathbf{x}$, they are categorized into three types. In function-driven input problems, $\mathbf{x}$ appears only in the objective function $f_\mathbf{x}(\mathbf{y})$, while the constraint set $\mathcal{Y}$ remains independent of $\mathbf{x}$. For example, in the quadratic and moderately non-convex functions, $\mathbf{x}$ corresponds to the parameter vector $\mathbf{p}$. In constraint-driven input problems, $\mathbf{x}$ parameterizes the constraint set $\mathcal{Y}_\mathbf{x}$, while the objective function remains independent. A representative case is the polygon constraints, where $\mathbf{x}$ corresponds to $\mathbf{b}$. Finally, in hybrid input problems, $\mathbf{x}$ influences both the objective function and the constraint set. The QoS-MISO problem discussed later exemplifies this category, where $\mathbf{x}$ parameterizes both the weighted sum rate objective and the quadratic constraints.


% \paragraph{Objective Functions:}

% \begin{itemize}
%     \item Quadratic Function:
%     A convex quadratic optimization problem defined as:
%     \begin{flalign}
%     \min_{\mathbf{y}} \frac{1}{2} \mathbf{y}^T \mathbf{Q} \mathbf{y} + \mathbf{p}^T \mathbf{y},
%     \end{flalign}
%     where $\mathbf{Q}$ is a positive semidefinite matrix, and $\mathbf{p}$ is a parameter vector.

%     \item Moderately Non-Convex Function:
%     A non-convex optimization problem incorporating sinusoidal perturbations, expressed as:
%     \begin{flalign}
%     \min_{\mathbf{y}} \frac{1}{2}\mathbf{y}^T\mathbf{Q}\mathbf{y} + \mathbf{p}^T \sin(\alpha \mathbf{y}),
%     \end{flalign}
%     where $\alpha$ controls the frequency of the sinusoidal terms, introducing non-convexity into the objective function.

%     \item Highly Non-Convex Function: Rastrigin Function: 
%     A classical benchmark for non-convex optimization, the Rastrigin function is defined as:
%     \begin{flalign}
%     \min_{\mathbf{y}} f(\mathbf{y}) = 10d + \sum_{i=1}^d \left[ y_i^2 - 10 \cos(2\pi y_i) \right],
%     \end{flalign}
%     where $d$ denotes the dimensionality of $\mathbf{y}$.
% \end{itemize}


% \paragraph{Constraint Types:}

% \begin{itemize}
%     \item Polygon Constraints:
%     The feasible region is a closed polygon defined by:
%     \begin{flalign}
%     \mathbf{A}\mathbf{y} \leq \mathbf{b},
%     \end{flalign}
%     where $\mathbf{A}$ is a matrix and $\mathbf{b}$ is a vector. The number of rows in $\mathbf{A}$ matches the dimensionality of $\mathbf{b}$, ensuring that the constraints describe a polygon with a finite number of edges. This type of constraint serves as a standard benchmark for linear feasible regions.

%     \item Non-Linear Star Convex Constraints:  
%     The feasible region is a star-convex domain defined by:
%     \begin{flalign}
%     \sqrt{y_1^2 + y_2^2} \leq 1 + r_1 \sin(k \arctan2(y_2, y_1)),
%     \end{flalign}
%     where $(y_1, y_2)$ are the coordinates of the point, $k$ determines the number of lobes, and $r_1$ controls the amplitude of the sinusoidal modulation. This formulation describes a non-linear boundary with star-convex properties, allowing for flexible and complex feasible regions.
% \end{itemize}

% The optimization problems considered in this study follow the general formulation in Eq.~\eqref{problem_formulaion}. Based on the role of the parameter $\mathbf{x}$, we categorize problems into three main types:
% \paragraph{Problem Types}
% \begin{itemize}
%     \item Function-Driven Input:
%     In this type, $\mathbf{x}$ is embedded only in the objective function $f_\mathbf{x}(\mathbf{y})$, while the constraint set $\mathcal{Y}$ remains independent of $\mathbf{x}$. For example, in the quadratic and moderately non-convex functions, $\mathbf{x}$ corresponds to the parameter vector $\mathbf{p}$ in the objective function.

%     \item Constraint-Driven Input:
%     Here, $\mathbf{x}$ is embedded in the constraint set $\mathcal{Y}_\mathbf{x}$, defining a parameterized feasible region, while the objective function remains independent of $\mathbf{x}$. For instance, $\mathbf{x}$ corresponds to $\mathbf{b}$ in the polygon constraints.

%     \item Hybrid Input:
%     In this more complex scenario, $\mathbf{x}$ simultaneously influences both the objective function $f_\mathbf{x}(\mathbf{y})$ and the constraint set $\mathcal{Y}_\mathbf{x}$. A representative example is the QoS-MISO problem discussed later, where $\mathbf{x}$ parameterizes both the weighted sum rate objective and the quadratic constraints.
% \end{itemize}

% 1 day


% \begin{table*}[h]
% \caption{Experimental results on a 5-lobed star convex constraint with the Moderately Non-Convex Function as the objective function. The results highlight the robustness of the proposed method under extreme non-linear constraints, demonstrating its ability to achieve superior accuracy compared to other L2O methods and even surpassing traditional optimizers.}

% \label{Star-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lccccr}
% \toprule
% Method & Obj. Value& Max. Cons & Mean. Cons & Violation Rate& Time \\
% \midrule
% Optimizer (SciPy)& -0.2821 & 0.0000 & 0.0000 & 0\% & \\
% \textbf{HoP (Proposed Method)} & \textbf{-0.2928} & \textbf{0.0000}& \textbf{0.0000} &\textbf{0}\% &\\
% NN (self-supervised)   & -0.2851 & 0.6261 & 0.0191 & 9.57\% \\
% NN (supervised)   & -0.2816 & 0.6971 & 0.0197 & 1.19\% \\
% NN (soft-cons+self-supervised)   & -0.2809 & 0.4447 & 0.0080 & 5.37\%\\
% NN (soft-cons+supervised)   & -0.2676 & 0.3800 & 0.0045 & 3.47\% \\
% DC3    & --0.2616 & 0.0000 & 0.0000 & 0\% &        \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}
\vspace{-0.5em}
\paragraph{(c) High-Dimensional Semi-Unbounded Problem}

% \subsection{High-Dimensional Semi-Unbounded Problem}\label{highdim_prob}
\begin{table*}[ht]
\caption{Results on QoS-MISO WSR problem for $U=3$ and $M=4$. The training set has $2,800$ instances while test set has $1,200$ instances. Objective function is MISO WSR, where constraints are QoS and power limitation.}
\label{miso-table}
\vskip -0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lrrrrr}
\toprule
Method & Obj. Value $\uparrow$ & Max. Cons $\downarrow$ & Mean. Cons $\downarrow$ & Vio. Rate $\downarrow$ & Time / \SI{}{\milli\second} $\downarrow$ \\
\midrule
Optimizer& ${1.3791}$& 0.0000&0.0000&0.00\% &{\color{red}11.31842}\\ %11.3184219
HoP & {{\color{blue}1.1393}}& {\color{blue}0.0000}& {\color{blue}0.0000}& {\color{blue}0.00\%}&{\color{blue}0.99895}\\ %0.9989482
NN-SSL   & 20.6766& {\color{red}1738.1122}&{\color{red}395.4313} &{\color{red}100.00\%}& 0.01723\\ %0.0172331
NN-SL   & 3.2477& {\color{red}0.1636}& {\color{red}0.0559}& {\color{red}100.00\%} &0.01984\\ %0.0198408
NN-SSL-SC   & 0.4150& {\color{red}0.2886}& {\color{red}0.0029}& {\color{red}18.25\%}&0.01834\\ %0.0183358
NN-SL-SC   & {\color{red}0.3244}& {\color{red}0.7794}& {\color{red}0.0035} &{\color{red}23.00\%}&0.01864\\ %0.0186376
DC3    & 0.3381& 0.0000&  0.0000&0.00\% &{5.53113}     \\ %14.6001950
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

% In this experiment, we evaluate the proposed method, an optimizer, various NN models, and DC3, focusing on two key aspects: first, assessing the method’s effectiveness in handling semi-unbounded problems; and second, evaluating its adaptability to high-dimensional scenarios. As in the previous experiments, the problem follows the sinusoidal QP formulation introduced earlier (\ref{QP_sin_Poly}). Here, $\mathbf{p}$ represents the input parameter $\mathbf{x}$ for the NN model.


% here, $\beta=30$, and $\mathbf{p}$ represents the input parameter for the NN model.
% The results indicate that the proposed method consistently achieves a minimal gap relative to the optimal results produced by the optimizer, without any violations of the constraints. In contrast, the four different NN models and the DC3 approach fail to ensure compliance with the inequality constraints. Notably, the NN model used self-supervised learning without penalty exhibits a violation rate almost $100\%$ across all test set samples. while the supervised, loss with soft constraints NN perform with much lower violation rate. While the DC3 contributes to a reduction in both the maximum and mean levels of violations for each constraint, they do not completely eliminate the rate of violations. 

This experiment demonstrates the ability of Hop to handle high-dimensional and semi-unbounded problem. As same as the previous experiments, the problem follows the sinusoidal QP formulation introduced earlier in Eq. (\ref{QP_sin_Poly}). Here, $\mathbf{p}$ represents the input parameter $\mathbf{x}$ for the NN model. As shown in Table \ref{Table:high_dim},  HoP consistently satisfies all constraints without any violations, confirming its robustness and feasibility under these challenging conditions. Moreover, HoP achieves the nearly optimal objective value -4.7683 among all methods, approaching to the traditional optimizer -7.6901 while maintaining strict constraint satisfaction. Otherwise, HoP still hold the advantages on computation complexity which is 52$\times$ faster than optimizer. 

Notably, NN-SL and NN-SL-SC also achieve 0 violation rates in this high-dimensional semi-unbounded experiment. This is likely due to the expanded feasible region in such scenarios, where constraints are less tight, and the optimal solution lies away from the boundary. However, it does not imply that SL-based methods can theoretically guarantee hard constraint satisfaction under all conditions. 

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{fig/dim_comp.pdf}}
\caption{Comparison of objective values across methods under different dimensional settings.}
\label{dim_comp}
\end{center}
\vspace{-3em}
\end{figure}

To further evaluate the performance of different methods under higher-dimensional settings, we conduct analyses across 20, 40, 60, and 80 dimensions. As shown in Fig. \ref{dim_comp}, HoP consistently performs near the optimizer in terms of objective value and exhibits a trend better than the optimizer's performance as dimensionality increases. In contrast, the DC3 method struggles to surpass the optimizer's objective value and shows a clear decline in performance as the dimensionality rises. These results highlight the robustness and scalability of HoP, even in high-dimensional scenarios.

\subsection{Application -- QoS-MISO WSR Problem}\label{miso_prob}
In this experiment, we implement HoP to address the QoS-MISO WSR problem. The QoS-MISO WSR problem is a well-known NP-hard problem with non-linear constraints in communication engineering \cite{tang2023energy,niu2021qos}. In most studies on the QoS-MISO WSR problem, researchers commonly employ alternative optimization methods to obtain solutions, which often require significant computational resources. The problem is formulated as follows: 
\begin{subequations}\label{WSR_pro}
\vskip -0.3in
\begin{flalign}
    \max_{\mathbf{w}_k} &\sum_{k=1}^U \alpha_k \log_{2}(1+\text{SINR}_k)\tag{\ref{WSR_pro}}\\
    \text{s.t.}\
    &\log_{2}(1+\text{SINR}_k)\geq \delta_k\label{QoS_cons}\\
    &\sum_{k=1}^U \text{Tr}(\mathbf{w}_k\mathbf{w}_k^H)+\text{P}_\text{c} \leq \text{P}_\text{max}\label{power}
\end{flalign}
\end{subequations}
where $\mathbf{w}_k \in \mathbb{C}^{M}$ is the beamformer, $(.)^H$ represents Hermitian transpose, $\mathbf{h}_k\in \mathbb{C}^{M}$ denotes channel state information, and $\sigma_k^2 \in \mathbb{R}$ is channel noise power.  $\alpha_k\in \mathbb{R}$ denotes priority weight for $U$ users, $\delta_k$ represents the QoS requirements. $\text{P}_\text{max}$ and $\text{P}_\text{c}$ define the system maximum power and circuit power consumption, respectively. The signal-interference-noise-ratio $\text{SINR}_k$ is defined as 
\begin{flalign}
    &\text{SINR}_k = \frac{\mathbf{w}_k^H\mathbf{h}_k\mathbf{h}_k^H\mathbf{w}_k}{\sum_{j\neq k}^U \mathbf{w}_j^H\mathbf{h}_k\mathbf{h}_k^H\mathbf{w}_j+\sigma_k^2}
\end{flalign}
Given that wireless resource allocation requires real-time optimization strategies to effectively manage wireless resources with diverse channel state information, we select $\mathbf{h}_k$ as the input of NN. This selection is crucial as $\mathbf{h}_k$ significantly influences both the constraints and the objective function in Eq. (\ref{WSR_pro}). For a comprehensive understanding of how we apply HoP to the QoS-MISO WSR problem, please refer to Appendix \ref{QOSMISO_polar}.

According to Table \ref{miso-table}, both HoP and DC3 achieve perfect constraint satisfaction, with $0\%$  violation rate. However, the mechanisms of penalty augmentation and post-correction in DC3 create a multi-objective optimization dilemma, fundamentally impeding it to simultaneously guarantee solution feasibility and pursue optimality. This inherent conflict between feasibility and objective minimization manifests as a optimality gap degradation in both of MISO and high-dimensional scenarios, whereas HoP eliminates such competing objectives through homeomorphic mapping. NN-based methods are consistent with previous experiments, struggling with constraint satisfaction. NN-SSL and NN-SL have extremely high Max, Cons and a Mean. Cons, with $100\%$ violation rate, showing that it fails to guarantee feasibility. NN-SSL-SC and NN-SL-SC reduce constraint violations slightly, with violation rate of $18.25\%$ and $23.00\%$, respectively. But they still fall far short of the perfect satisfaction achieved by HoP and DC3. For computation efficiency, HoP is slightly slower than other NN solvers, while it still gains a huge speedup -- 10 times faster than traditional algorithm and 5 times faster than DC3.

The results clearly showcase the superiority of the HoP in balancing objective optimization and constraint satisfaction. While the Optimizer (SCS+FP) achieves the best objective value, HoP obtains a very comparative  result, which outperforms other NN solvers. Moreover, HoP maintains perfect constraint satisfaction, matching the performance of DC3 in violation control. Thus, HoP is a highly effective and reliable method for solving the problem in engineering such as the QoS-MISO WSR problem.


\section{Conclusion}
In this work, we propose HoP, a novel L2O framework for solving hard-constrained optimization problems. The proposed architecture integrates NN predictions with a homeomorphic mapping, which transforms NN's outputs from spherical polar space to Cartesian coordinates, ensuring solution feasibility without extra penalties or post-correction.  Through extensive experiments encompassing both synthetic benchmark tasks and real-world applications, we demonstrate that HoP consistently outperforms existing L2O solvers, achieving superior optimality while maintaining zero constraint violation rates.
% In this work, we proposed HoP, a L2O scheme to solve hard constrained star-convex optimization problem. HoP depends on the outputs from NN and a homeomorphic mapping. The polar coordinates based homeomorphic mapping transform the initial NN's outputs from polar sphere space into Cartesian coordinate to guarantee outputs' feasibility and optimality. we evaluate HoP with numerous synthetic benchmarks task and real-world problem to show HoP perform better than other L2O solver in both of optimality and violation control.
% \section{Impact Statement}
% The HoP framework proposed in this study addresses a critical demand in optimization-driven engineering applications, particularly those requiring real-time computation and rigorous constraint enforcement. This framework demonstrates strong alignment with time-sensitive industrial applications across multiple domains.

% For example, in both wireless communication and radar systems, the necessity for instantaneous beamforming and precise trajectory tracking within stringent power constraints exemplifies the operational challenges that HoP is specifically designed to address.   Similarly, in smart grid optimization and digital signal detection systems, there is a critical demand for efficient transmission strategy making with strict power allocation constraints.   These requirements align fundamentally with the architectural strengths of HoP, making it an ideal solution for such complex optimization tasks.
\bibliography{lib}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Proof of Proposition \ref{proposition_redundant}}\label{proof_redundant}

%如果说有N个普通的凸集，然后所有的凸集都有一个交集，我从这个交集中找到一个内点。能否证明从这个内点射出的任意一条射线，其所有的交点中最近的点是属于这个交集，且是一个边界点

\begin{proof} 
Since $ C $ is star-convex with respect to $ \mathbf{y}_0 $, for any point $ \mathbf{y} \in C $, the line segment connecting $ \mathbf{y}_0 $ and $ \mathbf{y} $ is entirely contained in $ C $. That is, for all $ t \in [0, 1] $, we have:
\begin{flalign}
   (1-t)\mathbf{y}_0 + t\mathbf{y} \in C.
\end{flalign}
Then we consider a ray originating from $ \mathbf{y}_0 $ in the direction of a unit vector $ \mathbf{v}_{\theta} \in \mathbb{R}^n $. The ray can be parameterized as:
\begin{flalign}
   R = \{ \mathbf{y}_0 + t \mathbf{v} : t \geq 0 \},
\end{flalign}
where $ t \geq 0 $ is the parameter along the ray. Since $ \mathbf{y}_0 \in \operatorname{int}(C) $, the ray $ R $ starts inside $ C $. By the Definition \ref{star_convex}, the ray $ R $ must intersect $ C $, and the portion of the ray close to $ \mathbf{y}_0 $ is entirely contained in $ C $. Let $ t^* $ be the supremum of the set of parameters $ t \geq 0 $ such that $ \mathbf{y}_0 + t v \in \operatorname{int}(C) $ which is given as:
\begin{flalign}
   t^* = \sup \{ t \geq 0 : \mathbf{y}_0 + t v \in \operatorname{int}(C) \}.
\end{flalign}
   Since $ \operatorname{int}(C) $ is open and $ \mathbf{y}_0 \in \operatorname{int}(C) $, this set is non-empty and $ t^* $ exists. Define:
\begin{flalign}
   \mathbf{y}_1 = \mathbf{y}_0 + t^* v.
\end{flalign}
By construction, the point $ y_1 $ satisfies the following: (1) For any $ t < t^* $, $ \mathbf{y}_0 + t \mathbf{v} \in \operatorname{int}(C) $; (2) For any $ t > t^* $, $ \mathbf{y}_0 + t \mathbf{v} \notin C $. Therefore, $ \mathbf{y}_1 $ lies on the boundary of $ C $, i.e., $ \mathbf{y}_1 \in \partial C $. Furthermore, since the ray is continuous and $ C $ is closed (as the finite intersection of closed sets), $ \mathbf{y}_1 \in C $.
For any ray originating from $ \mathbf{y}_0 $, the closest intersection point $ \mathbf{y}_1 $ belongs to $ C $ and lies on its boundary $ \partial C $.

% \hfill \qed
\end{proof}

\section{Jacobian Analysis and Measure Distortion}
\label{sec:phi_v_mapping}

\noindent
This section presents a detailed analysis of the transformation
\begin{flalign}
\label{eq:phi_v_mapping}
\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta}) \;=\; \mathbf{y}_0 + \mathbf{v}_{\theta}\tan(\psi),
\end{flalign}
where $\psi \in [0, \tfrac{\pi}{2})$ is a scalar parameter, and $\mathbf{v}_{\theta} \in \mathbb{R}^d$ is a unit vector (i.e., $\|\mathbf{v}_{\theta}\|=1$). In this formulation, the direction $\mathbf{v}_{\theta}$ is treated as a free variable on the unit sphere $S^{d-1}$, while $\psi$ governs the radial displacement via the function $\tan(\psi)$. The mapping in Eq. \eqref{eq:phi_v_mapping} corresponds to the higher-dimensional homeomorphic mapping introduced in Eq.~\eqref{ND_hop}. It provides a diffeomorphic embedding of the parameter space $\mathcal{P}$ into $\mathbb{R}^d$, where $\mathcal{P}$ is defined as:
\begin{flalign}
\mathcal{P} \;=\; \bigl\{(\psi, \mathbf{v}_{\theta}) \mid \psi \in [0, \tfrac{\pi}{2}),\; \mathbf{v}_{\theta}\in S^{d-1}\bigr\}, \notag
\end{flalign}


The analysis below derives the Jacobian determinant of this transformation and discusses the measure distortion that arises as $\psi$ approaches $\tfrac{\pi}{2}$, where $\tan(\psi)$ diverges. These results provide insight into the geometric and numerical properties of the homeomorphic mapping when applied to optimization tasks in semi-unbounded domains.

\subsection{Jacobian Determinant}
To characterize local volume distortion, we compute the Jacobian determinant \cite{spivak2018calculus}. The mapping 
$\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})$ is defined in Eq.~\eqref{eq:phi_v_mapping}. The total derivative 
$\mathrm{D}\mathbf{\hat{y}}(\psi,\mathbf{v}_{\theta})$ is a $d \times d$ matrix whose columns represent the partial derivatives of $\mathbf{\hat{y}}$ with respect to $\psi$ and to the $(d-1)$ degrees of freedom on the unit sphere $S^{d-1}$. Specifically:

\paragraph{(a) Derivative w.r.t.\ $\psi$.} For fixed $\mathbf{v}_{\theta}$,
\begin{flalign}
  \frac{\partial}{\partial \psi}\,\bigl(\tan(\psi)\,\mathbf{v}_{\theta}\bigr)
  \;=\;
  \sec^2(\psi)\,\mathbf{v}_{\theta}.
\end{flalign}
\paragraph{(b) Derivatives w.r.t.\ the sphere parameters $\mathbf{v}_{\theta}$.} 

\begin{lemma}[Tangent Space]
\label{lemma_tangent_space}
Let $\mathbf{v}_{\theta} \in S^{d-1}$ be a unit vector on the sphere in $\mathbb{R}^d$, satisfying $\|\mathbf{v}_{\theta}\|=1$. Then any infinitesimal variation $\mathrm{d}\mathbf{v}_{\theta}$ must lie in the tangent space $T_{\mathbf{v}_{\theta}}S^{d-1}$, given by:
$$
  \mathbf{v}_{\theta} \cdot \frac{\mathrm{d}\mathbf{v}_{\theta}}{\mathrm{d}\theta} = 0
  \quad\Longrightarrow\quad
  \mathrm{d}\mathbf{v}_{\theta} \in T_{\mathbf{v}_{\theta}}S^{d-1}.
$$
The tangent space $T_{\mathbf{v}_{\theta}}S^{d-1}$ is a $(d-1)$-dimensional subspace of $\mathbb{R}^d$.
\end{lemma}

\begin{proof}
The constraint $\|\mathbf{v}_{\theta}\| = 1$ implies $\mathbf{v}^T_{\theta} \cdot \mathbf{v}_{\theta} = 1$. Differentiating this equation with respect to $\theta$ yields:
$$
\frac{\mathrm{d}}{\mathrm{d}\theta} \bigl(\mathbf{v}^T_{\theta} \cdot \mathbf{v}_{\theta}\bigr) = 2\,\mathbf{v}_{\theta} \cdot \frac{\mathrm{d}\mathbf{v}_{\theta}}{\mathrm{d}\theta} = 0.
$$
Thus, any allowed variation $\frac{\mathrm{d}\mathbf{v}_{\theta}}{\mathrm{d}\theta}$ is orthogonal to $\mathbf{v}_{\theta}$, and therefore lies in the tangent space $T_{\mathbf{v}_{\theta}}S^{d-1}$, which is the subspace of $\mathbb{R}^d$ orthogonal to $\mathbf{v}_{\theta}$. 

Since $S^{d-1}$ is a $(d-1)$-dimensional manifold embedded in $\mathbb{R}^d$, its tangent space $T_{\mathbf{v}_{\theta}}S^{d-1}$ is also $(d-1)$-dimensional.
\end{proof}

\begin{lemma}[Linear Independence of $\{\mathbf{v}_{\theta}, \mathbf{w}_1, \dots, \mathbf{w}_{d-1}\}$]
\label{lemma_linear_independence}
Let $\mathbf{v}_{\theta} \in S^{d-1}$ be a unit vector, and let $\{\mathbf{w}_1, \dots, \mathbf{w}_{d-1}\}$ be an orthonormal basis of $T_{\mathbf{v}_{\theta}}S^{d-1}$, satisfying:
$$
 \langle \mathbf{v}_{\theta}, \mathbf{w}_i \rangle= 0, \quad \langle \mathbf{w}_i, \mathbf{w}_j \rangle=
  \begin{cases}
    1, & \text{if } i = j, \\
    0, & \text{if } i \neq j,
  \end{cases}
  \quad
  i,j = 1,\dots,d-1.
$$
Then the set $\{\mathbf{v}_{\theta}, \mathbf{w}_1, \dots, \mathbf{w}_{d-1}\}$ is linearly independent in $\mathbb{R}^d$. Specifically, any linear combination
$$
  a\,\mathbf{v}_{\theta} + \sum_{i=1}^{d-1} b_i\,\mathbf{w}_i = \mathbf{0}
$$
implies $a = 0$ and $b_i = 0$ for all $i$.
\end{lemma}

\begin{proof}
Assume the linear dependence:
$$
  a\,\mathbf{v}_{\theta} + \sum_{i=1}^{d-1} b_i\,\mathbf{w}_i = \mathbf{0}.
$$
Taking the inner product with $\mathbf{v}_{\theta}$, we obtain:
$$
  a\,\langle\mathbf{v}_{\theta}, \mathbf{v}_{\theta}\rangle + \sum_{i=1}^{d-1} b_i\,\langle\mathbf{w}_i, \mathbf{v}_{\theta}\rangle = a\,\|\mathbf{v}_{\theta}\|^2 = a = 0,
$$
since $\|\mathbf{v}_{\theta}\| = 1$ and $\mathbf{v}_{\theta} \cdot \mathbf{w}_i = 0$. Next, taking the inner product with $\mathbf{w}_j$, we have:
$$
  \sum_{i=1}^{d-1} b_i\,\langle\mathbf{w}_i, \mathbf{w}_j\rangle = b_j,
$$
because $\langle\mathbf{w}_i, \mathbf{w}_j\rangle = 0$ for $i \neq j$ and $\langle\mathbf{w}_j , \mathbf{w}_j\rangle = 1$. Hence, $b_j = 0$. 

Since $j$ is arbitrary, we conclude that $b_i = 0$ for all $i$, and thus $a = 0$. This proves that the set $\{\mathbf{v}_{\theta}, \mathbf{w}_1, \dots, \mathbf{w}_{d-1}\}$ is linearly independent.
\end{proof}

%\textbf{Partial derivatives on the sphere.} 
% Given the mapping $\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta}) = \tan(\psi)\,\mathbf{v}_{\theta}$, each small variation along the unit sphere can be represented by moving $\mathbf{v}_{\theta}$ in the direction of some $\mathbf{w}_i$. Hence, for $i=1,\dots,d-1$,
% $$
%   \frac{\partial \mathbf{\hat{y}}}{\partial v_{\theta,i}} = \tan(\psi)\,\mathbf{w}_i.
% $$
% This indicates that $\mathbf{\hat{y}}$ increases linearly in the directions $\mathbf{w}_i$, with a scaling factor $\tan(\psi)$.

% Since $\mathbf{v}_{\theta}\in S^{d-1}$ must remain on the unit sphere, any infinitesimal change lies in the tangent space $T_{\mathbf{v}_{\theta}}S^{d-1}$, satisfying $\mathbf{v}_{\theta}\cdot \mathrm{d}\mathbf{v}_{\theta}=0$. We select an orthonormal basis $\{\mathbf{w}_1,\dots,\mathbf{w}_{d-1}\}$ for this $(d-1)$-dimensional space. Consequently, each partial derivative with respect to $\mathbf{v}_{\theta}$ naturally appears in the direction of $\mathbf{w}_i$, yielding $\frac{\partial \mathbf{\hat{y}}}{\partial v_{\theta,i}}=\tan(\psi)\,\mathbf{w}_i$.

Given the mapping $\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta}) = \tan(\psi)\,\mathbf{v}_{\theta}$, any infinitesimal change of $\mathbf{v}_{\theta}$ on the unit sphere $S^{d-1}$ lies in the tangent space $T_{\mathbf{v}_{\theta}}S^{d-1}$ (see Lemma~\ref{lemma_tangent_space}), ensuring $\mathbf{v}_{\theta}\cdot \mathrm{d}\mathbf{v}_{\theta}=0$. We introduce an orthonormal basis $\{\mathbf{w}_1,\dots,\mathbf{w}_{d-1}\}$ for this $(d-1)$-dimensional space. Consequently, each partial derivative with respect to $\mathbf{v}_{\theta}$ appears in the direction of some $\mathbf{w}_i$, yielding
$$
  \frac{\partial \mathbf{\hat{y}}}{\partial v_{\theta,i}} 
  \;=\; 
  \tan(\psi)\,\mathbf{w}_i,
  \quad
  i=1,\dots,d-1.
$$
Hence, $\mathbf{\hat{y}}$ increases linearly in each $\mathbf{w}_i$-direction, with a scaling factor $\tan(\psi)$.

% \end{enumerate}

Collecting \emph{all} these derivatives, The Jacobian matrix of the transformation is given by:
$$
\mathrm{D}\mathbf{\hat{y}}(\Phi, \mathbf{v}_{\theta}) =
\begin{bmatrix}
\sec^2(\psi)\,v_{\theta,1} & \tan(\psi)\,w_{1,1} & \dots & \tan(\psi)\,w_{d-1,1} \\
\sec^2(\psi)\,v_{\theta,2} & \tan(\psi)\,w_{1,2} & \dots & \tan(\psi)\,w_{d-1,2} \\
\vdots & \vdots & \ddots & \vdots \\
\sec^2(\psi)\,v_{\theta,d} & \tan(\psi)\,w_{1,d} & \dots & \tan(\psi)\,w_{d-1,d} \\
\end{bmatrix}.
$$
Here, the first column corresponds to $\frac{\partial \mathbf{x}}{\partial \Phi}$, and the subsequent $(d-1)$ columns represent $\frac{\partial \mathbf{x}}{\partial v_i}$ for each tangent direction $\mathbf{w}_i$.

Since $\{\mathbf{v}_{\theta}, \mathbf{w}_1, \dots, \mathbf{w}_{d-1}\}$ is an orthonormal set (as established in Lemma~\ref{lemma_linear_independence}, with $\langle\mathbf{v}_{\theta},\mathbf{w}_i\rangle=0$, $\|\mathbf{v}_{\theta}\|=1$, and $\|\mathbf{w}_i\|=1$), the determinant of $\mathrm{D}\mathbf{\hat{y}}(\psi,\mathbf{v}_{\theta})$ equals the product of the column norms:
$$
\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi,\mathbf{v}_{\theta})\bigr)
\;=\;
\bigl\|\sec^2(\psi)\,\mathbf{v}_{\theta}\bigr\| 
\,\times\, 
\prod_{i=1}^{d-1}
\bigl\|\tan(\psi)\,\mathbf{w}_i\bigr\|.
$$
Noting that $\|\mathbf{v}_{\theta}\|=1$ and $\|\mathbf{w}_i\|=1$, the norms are
$$
\bigl\|\sec^2(\psi)\,\mathbf{v}_{\theta}\bigr\| \;=\; \sec^2(\psi),
\quad
\bigl\|\tan(\psi)\,\mathbf{w}_i\bigr\|
\;=\;
\tan(\psi).
$$
Hence, the determinant simplifies to
$$
\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\Phi,\mathbf{v}_{\theta})\bigr) 
\;=\;
\bigl(\sec^2(\psi)\bigr)
\,\times\,
\bigl(\tan(\psi)\bigr)^{d-1}
\;=\;
\tan(\psi)^{\,d-1}\,\sec^2(\psi).
$$


\subsection{Measure Distortion Near $\psi\to \pi/2$}
% As $\psi$ approaches $\tfrac{\pi}{2}$, we have $\tan(\psi)\to +\infty$ and $\sec^2(\psi)\to +\infty$. In particular, setting $\epsilon = \tfrac{\pi}{2} - \psi$ implies
% $$
%  \tan(\psi)\;\approx\;\frac{1}{\epsilon},
%  \quad
%  \sec^2(\psi)\;\approx\;\frac{1}{\epsilon^2},
% $$
% which yields
% $$
%  \tan(\psi)^{\,d-1}\,\sec^2(\psi)
%  \;\approx\;
%  \frac{1}{\epsilon^{\,d-1}}\;\times\;\frac{1}{\epsilon^2}
%  \;=\;
%  \frac{1}{\epsilon^{\,d+1}}.
% $$

\begin{theorem}[Regularization of Jacobian Divergence]
Let $\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta}) = \tan(\psi)\,\mathbf{v}_{\theta}$ be the homeomorphic mapping defined in HoP, where $\psi \in (0, \tfrac{\pi}{2})$ and $\mathbf{v}_{\theta} \in S^{d-1}$. The Jacobian determinant of $\mathbf{\hat{y}}$ is given by:
$$
\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})\bigr) = \tan(\psi)^{\,d-1}\,\sec^2(\psi).
$$
As $\psi \to \tfrac{\pi}{2}$, the Jacobian determinant diverges as:
$$
\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})\bigr) \sim \frac{1}{\epsilon^{\,d+1}},
$$
where $\epsilon = \tfrac{\pi}{2} - \psi$. To address this divergence, HoP introduces $\epsilon > 0$ as a regularization parameter, ensuring that:
\begin{enumerate}
\item The mapping $\mathbf{\hat{y}}$ remains well-defined and smooth for all $\psi \in (-\tfrac{\pi}{2}, \tfrac{\pi}{2} - \epsilon]$.
\item The Jacobian determinant is bounded by:
$$
\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})\bigr) \leq C \cdot \frac{1}{\epsilon^{\,d+1}},
$$
where $C$ is a constant depending only on $d$.
\end{enumerate}
This regularization prevents unbounded volume distortion and ensures numerical stability in optimization and sampling procedures.
\end{theorem}

\begin{proof}
The divergence follows from the asymptotic behavior of $\tan(\psi)$ and $\sec(\psi)$ as $\psi \to \tfrac{\pi}{2}$:
$$
\tan(\psi) \sim \frac{1}{\epsilon} \quad \text{and} \quad \sec^2(\psi) \sim \frac{1}{\epsilon^2}.
$$
Substituting these into the Jacobian determinant yields:
$$
\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})\bigr) = \tan(\psi)^{\,d-1}\,\sec^2(\psi) \sim \frac{1}{\epsilon^{\,d-1}} \cdot \frac{1}{\epsilon^2} = \frac{1}{\epsilon^{\,d+1}}.
$$
By restricting $\psi$ to $\psi \leq \tfrac{\pi}{2} - \epsilon$, the Jacobian determinant remains bounded, completing the proof.
\end{proof}

Thus, the Jacobian determinant diverges polynomially in $\tfrac{1}{\epsilon}$, reflecting a severe measure distortion in the limit $\psi\to \tfrac{\pi}{2}$. In practical terms, small parameter increments around $\psi \approx \tfrac{\pi}{2}$ map to disproportionately large volume elements in $\mathcal{Y}_\mathbf{x}$, leading to potential numerical instability if one attempts to sample or optimize directly over $\psi$ without truncation.

Although the Jacobian determinant $\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})\bigr)$ diverges as $\psi \to \tfrac{\pi}{2}$, practical considerations ensure numerical stability. Empirical knowledge or problem-specific constraints often imply an upper bound $\psi_{\text{max}} < \tfrac{\pi}{2}$, allowing the selection of $\epsilon = \tfrac{\pi}{2} - \psi_{\text{max}}$. Additionally, finite precision in computing hardware inherently limits the representable range, preventing true divergence in practice.


% Although the Jacobian determinant $\det\bigl(\mathrm{D}\mathbf{\hat{y}}(\psi, \mathbf{v}_{\theta})\bigr)$ diverges as $\psi \to \tfrac{\pi}{2}$, practical considerations allow us to determine a suitable $\epsilon > 0$ to ensure numerical stability. Specifically:

% \begin{itemize}
%     \item Empirical Upper Bound: By leveraging domain knowledge and empirical testing, one can estimate a loose upper bound on the range of feasible solutions. For instance, if the problem's physical or geometric constraints suggest that $\psi$ is unlikely to exceed a certain value $\psi_{\text{max}} < \tfrac{\pi}{2}$, then $\epsilon$ can be chosen as $\epsilon = \tfrac{\pi}{2} - \psi_{\text{max}}$.

%     \item Hardware Limitations: Modern computing hardware (e.g., 32-bit or 64-bit floating-point systems) imposes finite limits on the representable range of values. For example, in 64-bit floating-point arithmetic, the maximum representable value is approximately $1.8 \times 10^{308}$. This effectively bounds $\tan(\psi)$ and $\sec^2(\psi)$, even if $\psi$ approaches $\tfrac{\pi}{2}$.
% \end{itemize}

% \section{Jacobian Analysis and Mitigation Techniques}

% The spherical transformation inherently introduces significant measure distortion near the equator of the sphere ($\Phi \to \pi/2$). The distortion is quantified by the Jacobian determinant:
% \begin{align}
%     J(\Phi, \theta, r) &\propto R^2 \tan(\psi) \sec^2(\psi),
% \end{align}
% where $R^2$ accounts for the radial scaling, and $\tan(\psi)\sec^2(\psi)$ captures the geometric distortion. As $\varphi \to \pi/2$, this term diverges with cubic growth:
% \begin{align}
%     \tan(\psi)\sec^2(\psi) &\sim \frac{1}{(\pi/2 - \Phi)^3}.
% \end{align}

% To mitigate this divergence, we enforce two mechanisms: bounding $\Phi$ with a fixed $\epsilon_0$ and incorporating activation functions for smoother control of $\Phi$.

% First, by imposing $\Phi \leq \pi/2 - \epsilon_0$, the distortion term $\tan(\psi)\sec^2(\psi)$ is capped at:
% \begin{align}
%     \tan(\psi)\sec^2(\psi) &\leq \cot(\epsilon_0)\csc^2(\epsilon_0) := M(\epsilon_0),
% \end{align}
% where $M(\epsilon_0)$ is a finite constant determined by $\epsilon_0$. This ensures that the cubic divergence near $\pi/2$ is avoided entirely.

% Second, we map the raw NN outputs $X \in \mathbb{R}$ to $\varphi$ using bounded activation functions. For $\varphi = \frac{\pi}{2}A(X)$, activation functions such as the sigmoid or tanh provide smooth mappings from $\mathbb{R}$ to $[0, 1)$ and inherently limit the growth of $\varphi$'s derivative. The full Jacobian determinant, incorporating the activation function, becomes:
% \begin{align}
%     J(\varphi, \theta, r) &\propto R^2 \tan(\varphi) \sec^2(\varphi) \frac{\mathrm{d}\varphi}{\mathrm{d}X}.
% \end{align}

% Here, $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$ reflects the bounded derivative of the activation function and plays a critical role in mitigating the distortion.

% \begin{table*}[t]
% \caption{Comparison of Jacobian scaling factors for different activation functions.}
% \label{jacobian-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lccr}
% \toprule
% Activation Function & Maximum $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$ & Mitigation Level \\
% \midrule
% Sigmoid & $\frac{\pi}{8}$ & Strong \\
% Tanh & $\frac{\pi}{4}$ & Moderate \\
% No Activation & $1$ & None \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}


% In conclusion, the combination of the $\epsilon_0$ truncation and bounded activation functions ensures that the measure distortion remains finite and well-controlled. While $\epsilon_0$ prevents true singularities, activation functions contribute additional mitigation by limiting the derivative $\frac{\mathrm{d}\varphi}{\mathrm{d}X}$, ensuring stability in both theoretical and computational settings.

\section{Challenges of Stagnation in Polar Coordinate Optimization}\label{appendix:pco_analysis}
\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{fig/polar-experiment.pdf}}
\caption{Illustration of optimization trajectories under polar coordinates. The figures demonstrate the impact of learning rate, momentum, and geometric reconnection on the convergence behavior.}
\label{polar-experiment-full}
\end{center}
\end{figure}

Optimization in polar coordinates introduces distinct challenges due to the coupling between radial and angular variables. As illustrated in Fig.~\ref{polar-experiment}, these challenges manifest as radial stagnation, angular freezing, and oscillatory behavior under large learning rates or momentum-based updates. Below, we formally state and address these issues.

\begin{remark} Radial Stagnation and Angular Freezing.\label{prop:radial_stagnation}
In polar coordinates, the non-negativity constraint $ r \geq 0 $ introduces radial stagnation and angular freezing when $ r = 0 $. Specifically, the radial update halts as:
\begin{flalign}
r_{t+1} = \max\{0, r_t - \eta (\cos\theta_t \frac{\partial f}{\partial x} + \sin\theta_t \frac{\partial f}{\partial y})\}.
\end{flalign}
When $ r_{t+1} = 0 $, angular updates are frozen:
\begin{flalign}
\theta_{t+1} = \theta_t + \eta r_{t+1} (\sin\theta_t \frac{\partial f}{\partial x} - \cos\theta_t \frac{\partial f}{\partial y}) = \theta_t.
\end{flalign}
This prevents the optimizer from escaping local regions and exploring global optima, particularly when the solution lies at $ \theta_t + \pi $.
\end{remark}
\begin{remark}Impact of Learning Rate and Momentum.\label{prop:LR_Momentum}
Large learning rates and momentum exacerbate radial stagnation and angular freezing. For a Lipschitz-continuous gradient $\|\nabla f\| \leq B$, radial truncation occurs when:
\begin{flalign}
\eta > \frac{r_t}{B}.
\end{flalign}
Momentum-based optimizers introduce oscillatory behavior near optima, as angular updates retain past gradient contributions, leading to overshooting or divergence.
\end{remark}
\begin{remark}Dynamic Learning Rate Strategy.\label{prop:Dynamic_LR}
To mitigate radial stagnation, dynamically scaling the learning rate as:
\begin{flalign}
\eta_t = \alpha r_t, \quad \text{where } \alpha < \frac{1}{B},
\end{flalign}
ensures $ r_{t+1} > 0 $. This prevents radial truncation by reducing step sizes near $ r = 0 $, while maintaining convergence stability. 
\end{remark}
Fig.~\ref{polar-experiment-full} (right) illustrates the effectiveness of geometric reconnection and adaptive learning strategies. The combined approach avoids radial stagnation, preserves gradient continuity, and facilitates efficient exploration of the optimization landscape.

%For detailed derivations of remark 1, 2, and 3, along with additional experimental validation, refer to the Appendix.

% \section{Supplementary Experiments}

% \subsection{Rastrigin Function Optimization with Arbitrary Polygon Constraints}

% \begin{table*}[t]
% \caption{}
% \label{Rastrigin-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lccccr}
% \toprule
% Method & Obj. Value& Max. Cons & Mean. Cons & Violation Rate& Time \\
% \midrule
% Optimizer (SciPy)& 20.7516& 0.0000&0.0000&0\% \\
% \textbf{HoP (Proposed Method)} & $\mathbf{23.2972}$& $\mathbf{0.0000}$& $\mathbf{0.0000}$& $\mathbf{0.00}$\%\\
% NN (self-supervised)   & 1.3942& 0.7192&0.5498 &100\%& \\
% NN (supervised)   & 35.4912& 0.0005& 0.0000& 0.01\% \\
% NN (soft-cons+self-supervised)   & 22.1460& 0.0190& 0.0006& 8.12\%\\
% NN (soft-cons+supervised)   & 35.9026& 0.0026& 0.0002 &4.56\%\\
% DC3    & 29.1321& 0.0000&  0.0000&0.00\%       \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}

\section{Experimental Data generation and Parameter Setup}\label{exp_setting_details}
% 讲如何设计实验，采集的数据，以及对应的参数设置
% \subsection{Experiment \ref{polycons_prob}}
In this section, the hyper-parameters of pervious experiments are provided. We split the section into two subsections to introduce the setting in Experiment \ref{exp:Synthetic} and Experiment \ref{miso_prob}, respectively. 

All experiments were conducted on a system with the following specifications:  

\begin{itemize}
    \item Host: Lambda Vector 1
    \item OS: Ubuntu 22.04.5 LTS x86\_64  
    \item CPU: AMD Ryzen Threadripper PRO 5955WX (32 cores) @ 4.000GHz  
    \item GPU: Single NVIDIA RTX A6000 
\end{itemize}

To simulate full parallelization, we report the total computation time divided by the number of test instances. Note that our implementations are not tightly optimized, and all timing comparisons should be interpreted as approximate.

\subsection{Synthetic Benchmarks \ref{exp:Synthetic}}

\subsubsection{(a) Polygon-Constrained Problem}

% 目标函数 如\ref{QP_sin_Poly}所示，在实验过程中，Q为随机生成的半正定矩阵，实验维度为2维，然后 b 为随机生成，通过一个“check”函数，把不为polygon约束的b剔除，直至生成满数据20000个样本，数据集划分则是0.7的训练集，0.3的测试集；对了那个方程里还有两个参数，p 在这里取向量 30, 30 beta 取 1；然后A是固定生成的；对与center point \mathbf{y}_0 我们通过求解 chebshev 中心点获得

The objective function for this experiment is defined as in Eq. (\ref{QP_sin_Poly}). The problem is set in a 2-dimensional space, with the matrix $\mathbf{Q}$ randomly generated as a positive semi-definite matrix to ensure convex quadratic terms. Specifically, $\mathbf{Q}$ is created using the following procedure:
\begin{flalign}\label{semi-definite}
  \mathbf{Q} = \mathbf{A}^T \mathbf{A} + \mathbf{I},
\end{flalign}
where $\mathbf{A}$ is a randomly generated $2 \times 2$ matrix drawn from a standard Gaussian distribution, and $\mathbf{I}$ is the identity matrix to guarantee positive semi-definiteness.

The vector $\mathbf{b}$ is generated by sampling from a uniform distribution $\mathcal{U}(0, 2)$ and is checked against the polygon constraints using a validation function. Specifically, $\mathbf{b}$ is accepted only if it satisfies the closure conditions of the polygon constraints. Samples violating these constraints are discarded, ensuring all 20,000 instances in the dataset strictly satisfy the feasibility conditions.

The dataset is divided into a training set (70\%) and a test set (30\%). The parameters in the objective function are set as follows: the vector $\mathbf{p}$ is fixed to $[30, 30]$, and the sinusoidal term's parameters $\mathbf{q}$ are randomly generated for each instance, also drawn from $\mathcal{U}(0, 2)$. The constraint matrix $\mathbf{A}$ is precomputed and fixed for all experiments.

The center point $\mathbf{y}_0$ of the polygon is computed as the Chebyshev center of the feasible region. The Chebyshev center is obtained by solving a secondary optimization problem that maximizes the radius of the largest inscribed circle within the polygon constraints, ensuring that $\mathbf{y}_0$ lies strictly inside the feasible region.


\subsubsection{(b) $\ell_p$-norm Problem}

%  目标函数 如 \label{eq:lp-norm} Q 同上，为随机生成的半正定矩阵，\mathbf{p}为一个随机生成向量，然后 p 为范数里的p取的0.5，然后 b 取的常数1 ，20000个样本还是7：3 中心点 y_0 取 0，0即可 大概就这些吧，我暂时没想到还要写什么

The objective function for this experiment is defined in Eq. (\ref{eq:lp-norm}). The matrix $\mathbf{Q}$ is randomly generated as a positive semi-definite matrix, ensuring convex quadratic terms, while the vector $\mathbf{p}$ is sampled from a standard normal distribution $\mathcal{N}(0, 1)$. The $\ell_p$-norm constraint uses $p = 0.5$, and the parameter $b$ is fixed to 1. The dataset consists of 20,000 samples, split into a training set ($70\%$) and a test set ($30\%$). The center point $\mathbf{y}_0$ is set to $[0, 0]$.

The positive semi-definite matrix $\mathbf{Q}$ is generated as described in Eq. (\ref{semi-definite}).

The dataset is constructed by generating feasible samples that satisfy the $\ell_p$-norm constraint. Instances violating the constraint are discarded, ensuring that the entire dataset strictly adheres to the defined feasibility conditions.

\subsubsection{High-Dimensional Semi-Unbounded Problem}

% \subsection{}
The objective function for high-dimensional semi-unbounded problem is given in Eq. (\ref{QP_sin_Poly}), where $\beta=30$, and $\mathbf{p}\sim\mathcal{N}$ is randomly sampled from the Gaussian Distribution $(-10,\mathbf{I})$. The parameters in the constraints are fixed, where $\mathbf{A}$ is randomly drawn from $\mathcal{N}(0,\mathbf{I})$ and $\mathbf{b}$ is drawn randomly as a positive number.  Since in this experiment constraints are fixed for a specific dataset, without lose of generality we define $\mathbf{b}=\mathbf{1}$. Moreover, the number of linear constraints is $d$, which is the dimension of $\mathbf{\hat{y}}$. Thus arbitrary semi-unbounded constraints are obtained while we have $\mathbf{0}$ as $\mathbf{y}_0$. The dataset is divided into a training set ($70\%$) and a test set ($30\%$). For training part, the learn rate is $10^{-4}$, the optimizer is Adam, and the training configuration includes $500$ epochs with a batch size of $2,048$. For $20$-dimensional problem, the dataset contains $20,000$ instance; 40, 60, 80 dimensions problems have $40,000$, $60,000$, $80,000$ instances, respectively. The MLP used for both all baseline methods and HoP has 512 neurons with ReLU activation functions.

\subsection{QoS-MISO WSR Problem}\label{QOSMISO_polar}
In this subsection the data preparation and how we apply HoP on QoS-MISO WSR problem are demonstrated. To generate the MISO simulation data, we apply algorithm given in Appendix \ref{FP_MISO_data_pre}. The user priority weight $\alpha_k\sim\mathcal{U}(0,1)$ with uniformization by $\alpha_k/(\sum_{k=1}^U \alpha_k)$. Channel state information is $\mathbf{h}_k$, followed circularly symmetric complex Gaussian (CSCG) distribution where the real part and imagine part follows $\mathcal{CN}(0,1)$. pathloss $=10$ and $\sigma^2=0.01$, $\delta_k\sim\mathcal{U}(0,1/3)$. $\text{P}_\text{max}=33$ dbm, $\text{P}_\text{c}=30$ dbm. For the training part, the learn rate is $10^{-2}$, the optimizer is Adam, and the training configuration includes $500$ epochs with a batch size of $64$. The dataset has $4000$ instances which is divided into a training set ($70\%$) and a test set ($30\%$).
Then, in the following part, we introduce how to formulate this multi-variables problem as a single variable problem which satisfies the NN's outputs format. The problem given in Eq. (\ref{WSR_pro}) can be reformulated as:
\begin{subequations}\label{miso_app}
\begin{flalign}
    \max_{\mathbf{w}_k} &\sum_{k=1}^U \alpha_k \log_{2}(1+\text{SINR}_k)\tag{\ref{miso_app}}\\
    \text{s.t.}\
    &\frac{\mathbf{w}_k^H\mathbf{h}_k\mathbf{h}_k^H\mathbf{w}_k}{\sum_{j\neq k}^U \mathbf{w}_j^H\mathbf{h}_k\mathbf{h}_k^H\mathbf{w}_j+\sigma_k^2}\geq \omega_k\label{sinr_cons}\\
    &\sum_{k=1}^U \text{P}_k \leq \text{P}_\text{max}-\text{P}_\text{c}
\end{flalign}
\end{subequations}
where $ \omega_k = 2^{\delta_k}-1$. Then, the constraint in Eq. (\ref{sinr_cons}) is identical to:
\begin{flalign}
    {\mathbf{w}_k^H\mathbf{h}_k\mathbf{h}_k^H\mathbf{w}_k}-\omega_k{\sum_{j\neq k}^U \mathbf{w}_j^H\mathbf{h}_k\mathbf{h}_k^H\mathbf{w}_j}\geq \omega_k\sigma_k^2\label{sinr_expand}
\end{flalign}
In this problem, for the convenience, $\mathbf{w}_k$ and $\mathbf{h}_k$ are reformulated as real vector and matrix by splicing:
\begin{flalign}\label{real_imag_reformH}
    &\Tilde{\mathbf{H}}_k = 
    \begin{bmatrix} 
    \Re{(\mathbf{h}_k\mathbf{h}_k^H)}&-\Im{(\mathbf{h}_k\mathbf{h}_k^H)}\\
    \Im{(\mathbf{h}_k\mathbf{h}_k^H)}&\Re{(\mathbf{h}_k\mathbf{h}_k^H)}
    \end{bmatrix} \\
    &\Tilde{\mathbf{w}}_k=\begin{bmatrix} 
    \Re{(\mathbf{w}_k)}\\\Im{(\mathbf{w}_k)}\end{bmatrix}\label{real_imag_reformw}
\end{flalign}
Therefore, Eq. (\ref{sinr_expand}) is represented by Eq. (\ref{real_imag_reformH}) and Eq. (\ref{real_imag_reformw}):
\begin{flalign}
    \Tilde{\mathbf{w}}_k^T\Tilde{\mathbf{H}}_k\Tilde{\mathbf{w}}_k - \omega_k\sum_{j\neq k}^U \Tilde{\mathbf{w}}_j^T\Tilde{\mathbf{H}}_k\Tilde{\mathbf{w}}_j\geq \omega_k\sigma_k^2\label{real_sinr}
\end{flalign}
Splicing all $\mathbf{\Tilde{w}}_k$ for each user, then Eq. (\ref{real_sinr}) is rewritten as:
\begin{flalign}
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_k\bar{\mathbf{w}} \geq \omega_k\sigma_k^2
\end{flalign}
where $\bar{\mathbf{w}}$, $\bar{\mathbf{H}}_k$ and $f_j(\omega_k)$ are defined as:
\begin{flalign}
    &\bar{\mathbf{w}}= [\Tilde{\mathbf{w}}_1^T,\Tilde{\mathbf{w}}_2^T,...,\Tilde{\mathbf{w}}_U^T]^T\\
    &\bar{\mathbf{H}}_k = \begin{bmatrix}
        f_1(\omega_k)\Tilde{\mathbf{H}}_k&\mathbf{0}&...&\mathbf{0}\\
        \mathbf{0}&f_2(\omega_k)\Tilde{\mathbf{H}}_k&...&\mathbf{0}\\
        \vdots&\vdots&\ddots&\vdots
        \\
        \mathbf{0}&\mathbf{0}&...&f_U(\omega_k)\Tilde{\mathbf{H}}_k
    \end{bmatrix}\\
    &f_j(\omega_k)
\begin{cases}
1, & {j = k}, \\
-\omega_k, & {j \neq k}.
\end{cases}
\end{flalign}
Based on above transformation, we apply the same operation on other constraints, then we have
\begin{flalign}
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_1\bar{\mathbf{w}} &\geq \omega_1\sigma_1^2\notag\\
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_2\bar{\mathbf{w}} &\geq \omega_2\sigma_2^2\notag\\
    &\vdots\notag\\
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_U\bar{\mathbf{w}} &\geq \omega_k\sigma_U^2\label{block_qos}\\
    \bar{\mathbf{w}}^T\mathbf{I}\bar{\mathbf{w}}&\leq \text{P}_\text{max}-\text{P}_\text{c}
\end{flalign}
Here, the original problem can be reorganized as an equivalent problem as follows:
\begin{subequations}\label{final_miso}
\begin{flalign}
    \max_{\mathbf{\bar{w}}_k} \sum_{k=1}^U \alpha_k \log_{2}&(1+\text{SINR}_k)\tag{\ref{final_miso}}\\
    \text{s.t.}\
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_1\bar{\mathbf{w}} &\geq \omega_1\sigma_1^2\notag\\
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_2\bar{\mathbf{w}} &\geq \omega_2\sigma_2^2\notag\\
    &\vdots\notag\\
    \bar{\mathbf{w}}^T\bar{\mathbf{H}}_U\bar{\mathbf{w}} &\geq \omega_U\sigma_U^2\\
    \bar{\mathbf{w}}^T\mathbf{I}\bar{\mathbf{w}}&\leq \text{P}_\text{max}-\text{P}_\text{c}
\end{flalign}
\end{subequations}
% \section{Dataset Preparation for Experiment \ref{polycons_prob}, \ref{star_prob} and \ref{highdim_prob}}
As consequence, $\bar{\mathbf{w}}$ is the estimated variable $\mathbf{\hat{y}}$ by HoP, where the NN's inputs, $\mathbf{x}$ are flatten $\Tilde{\mathbf{H}}_k$.
\section{FP for QoS-MISO WSR Experiment}\label{FP_MISO_data_pre}
We use the FP method to solve the original QoS-MISO WSR problem to compute the reference optimal beamformers results. According to \cite{shen2018fractional}, we reformulate the original problem as
\begin{subequations}\label{fp_prob}
\begin{flalign}
    \max_{\mathbf{W}_k} &\sum_{k=1}^U \alpha_k \log_{2}\big(1+2s_k\sqrt{\mathbf{h}_k^H\mathbf{W}_k\mathbf{h}_k}- s_k^2(\sigma_k^2+\sum_{j\neq k}^U(\mathbf{h}_k^H\mathbf{W}_j\mathbf{h}_k)\big) \tag{\ref{fp_prob}}\\
    \text{s.t.}\
    &\log_{2}(1+\frac{\mathbf{h}_k^H\mathbf{W}_k\mathbf{h}_k}{\sigma_k^2+\sum_{j\neq k}^U(\mathbf{h}_k^H\mathbf{W}_j\mathbf{h}_k)})\geq \delta_k\\
    &\sum_{k=1}^U \text{Tr}(\mathbf{W}_k) \leq \text{P}_\text{max}-\text{P}_\text{c}
\end{flalign}
\end{subequations}
where $s_k$ is auxiliary variable can be obtained by:
\begin{flalign}
    s_k = \frac{\sqrt{\mathbf{h}_k^H\mathbf{W}_k\mathbf{h}_k}}{\sigma_k^2+\sum_{j\neq k}^U\mathbf{h}_k^H\mathbf{W}_j\mathbf{h}_k}\label{z_k_update}
\end{flalign}
Note that the variables are applied with semidefinite relaxation such that $\mathbf{W}_k =\mathbf{w}_k\mathbf{w}_k^H $, $\text{rank}(\mathbf{W}_k) = 1$. Then, $\mathbf{w}_k$ is obtained by applying singular value decomposition (SVD) on $\mathbf{W}_k$. Therefore, the FP algorithm can solve QoS-MISO WSR problem by Algorithm \ref{alg:fp_qos_MISOWSR_data}.


\begin{algorithm}[tb]
   \caption{FP for QoS-MISO WSR optimization}
   \label{alg:fp_qos_MISOWSR_data}
\begin{algorithmic}
   \STATE {\bfseries Input:} Initialization parameters. Set counter $j=1 $ and convergence precision $\varphi_p$,
   % \WHILE{}
   \REPEAT
   \STATE Update $s_k$ by (\ref{z_k_update});
   \STATE Solve (\ref{fp_prob}) by cvxpy;
   \STATE Solve $\mathbf{w}_k$ by SVD;
   \UNTIL{$|\text{WSR}_{j+1}-\text{WSR}_{j}| \leq \varphi_p$}
\end{algorithmic}
\end{algorithm}





% new things


\end{document}

