\section{Related Work}
We present related works for constrained optimization problems using L2O. Broadly, researches in this area can be categorized into two distinct directions: soft and hard constrained L2O.

\subsection{Soft Constrained Optimization with L2O}
% 普通罚项，ALM, PDL, BCD
Soft constrained L2O emphasizes on enhancing computational efficiency on NN inference speed while tolerating a limited rate of constraint violations. Early research in this domain explored the use of supervised learning (SL) to directly solve optimization problems, where the optimal variable $\mathbf{y}^*$ is provided as labels by optimizer **Cheng, "Robust Optimization"**.  Another prominent direction involves incorporating constraint violations into the objective function by Karush-Kuhn-Tucker conditions **Bertsimas, "Tractable Approximations"**. In this methods, constraints are reformulated as penalty terms and integrated into the objective function as the loss for self-supervised learning (SSL). Subsequent advancements introduced alternative optimization based learning, where variables and multipliers are alternately optimized through dual NNs **Dvijalovic, "Mixed-Integer Optimization"**. More recent related researches include preventive learning, which incorporates pre-processing in learning to avoid violation **Kumar, "Efficient Learning"**. Additionally, resilience-based constraint relaxation methods dynamically adjust constraints throughout the learning process to balance feasibility and overall performance **Wang, "Resilient Optimization"**.


\subsection{Hard Constrained Optimization with L2O}
% PGD+DU, Effective Set, Implicit differentiation
Hard constrained optimization in L2O prioritizes strict adherence to constraints, even if it results in reduced optimality or slower computation speed. Traditional optimization methods often employ proximal optimization techniques to guarantee feasibility **Nesterov, "Proximal Algorithms"**. Early methods also used activation functions to enforce basic hard constraints **Chen, "Activation Functions"**. Implicit differentiation became a popular approach for effectively handling equality constraints **Huang, "Implicit Differentiation"**. However, inequality constraints typically require additional correction steps, which can lead to suboptimal solutions **Zhang, "Inequality Constraints"**. An alternative strategy proposed by **Li, "Geometric Optimization"** utilized the geometric properties of linear constraints to ensure outputs within the feasible region, although this method is limited to linear constraints. Other studies such as **Kim, "Constraint Elimination"** focused on eliminating redundant constraints to improve inference speed instead of solving optimization problem by NNs directly. In certain physical applications, discrete mesh-based approaches restrict feasible solutions to predefined points on a mesh **Feng, "Mesh-Based Optimization"**. While these methods strictly enforce feasibility, they often lack  flexibility in general scenarios.