\section{Related Work}
We present related works for constrained optimization problems using L2O. Broadly, researches in this area can be categorized into two distinct directions: soft and hard constrained L2O. 


\subsection{Soft Constrained Optimization with L2O}
% 普通罚项，ALM, PDL, BCD
Soft constrained L2O emphasizes on enhancing computational efficiency on NN inference speed while tolerating a limited rate of constraint violations. Early research in this domain explored the use of supervised learning (SL) to directly solve optimization problems, where the optimal variable $\mathbf{y}^*$ is provided as labels by optimizer \cite{zamzam2020learning,guha2019machine}.  Another prominent direction involves incorporating constraint violations into the objective function by Karush-Kuhn-Tucker conditions \cite{donti2021dc3,zhang2024constrained, xu2018semantic}. In this methods, constraints are reformulated as penalty terms and integrated into the objective function as the loss for self-supervised learning (SSL). Subsequent advancements introduced alternative optimization based learning, where variables and multipliers are alternately optimized through dual NNs \cite{park2023self,kim2023self,nandwani2019primal}. More recent related researches include preventive learning, which incorporates pre-processing in learning to avoid violation \cite{zhao2023ensuring}. Additionally, resilience-based constraint relaxation methods dynamically adjust constraints throughout the learning process to balance feasibility and overall performance \cite{hounie2024resilient,ding2024resilient}.





\subsection{Hard Constrained Optimization with L2O}
% PGD+DU, Effective Set, Implicit differentiation
Hard constrained optimization in L2O prioritizes strict adherence to constraints, even if it results in reduced optimality or slower computation speed. Traditional optimization methods often employ proximal optimization techniques to guarantee feasibility \cite{cristian2023end,min2024hard}. Early methods also used activation functions to enforce basic hard constraints \cite{sun2018learning}. Implicit differentiation became a popular approach for effectively handling equality constraints \cite{amos2017optnet,donti2021dc3,huang2021deepopf}. However, inequality constraints typically require additional correction steps, which can lead to suboptimal solutions \cite{donti2021dc3}. An alternative strategy proposed by \cite{li2023learning} utilized the geometric properties of linear constraints to ensure outputs within the feasible region, although this method is limited to linear constraints. Other studies such as \cite{misra2022learning,guha2019machine} focused on eliminating redundant constraints to improve inference speed instead of solving optimization problem by NNs directly. In certain physical applications, discrete mesh-based approaches restrict feasible solutions to predefined points on a mesh \cite{amos2017input,zhong2023neural,negiar2022learning}. While these methods strictly enforce feasibility, they often lack  flexibility in general scenarios.