\section*{Appendix}

\section{Proof of \autoref{p:coverage-bound}}\label{a:theory}

As stated in \autoref{p:coverage-bound}, let $P^{c}_{t+H}$ and $P^c_{\psi}$ indicate the probability distributions associated with the true data distribution at $t+H$ and with the learned quantile function $\gQ_{\psi}$, respectively. It follows from the definition of total variation distance $\,{TV}(P,Q) := \sup_{B}|P(B)-Q(B)|,$ that for any event $B$
\begin{align}
    &|P^{c}_{t+H}(B)-P^c_{\psi}(B)|\,\le\,{{TV}}(P^c_{\psi},P^{c}_{t+H}) \quad \Longrightarrow \\
    &P^{c}_{t+H}(B)\,\ge\,P^c_{\psi}(B)\,-\,{{TV}}(P^c_{\psi},P^c_{t+H}).\label{eq:tv-bound}
\end{align}
For $ \hat C^{\alpha}_t$ in both \autoref{eq:cp-interval} and \autoref{eq:cp-interval-optimized} we have by construction that
\begin{equation}\label{eq:proof-quantiles}
    G^c_{\psi}(\mX_{t+H} \in \hat C^{\alpha}_t) = 1 - \alpha.
\end{equation}
Then, putting together \autoref{eq:tv-bound} and \autoref{eq:proof-quantiles} we obtain
\begin{align*}
    P^c_{t+H}\left(\mX_{t+H} \in \hat C^{\alpha}_t (\widehat \mX_{t+h})\right) \geq 1 - \alpha - {TV}\left(P^c_{\psi}, P^c_{t+H}\right).
\end{align*}
Hence, the proof is complete.

\section{Hardware and software platforms}\label{a:exp}

Benchmarks have been developed with Python~\cite{rossum2009python} and the following open-source libraries:
\begin{itemize}
    \item Numpy~\cite{harris2020array};
    \item PyTorch~\cite{paske2019pytorch};
    \item PyTorch Lightning~\cite{Falcon_PyTorch_Lightning_2019};
    \item PyTorch Geometric~\cite{fey2019fast};
    \item Torch Spatiotemporal~\cite{Cini_Torch_Spatiotemporal_2022}.
\end{itemize}

Experiments were conducted on a server equipped with AMD EPYC 7513 CPUs and NVIDIA RTX A5000 GPUs. The code for reproducing the computational experiments will be open-sourced upon publication. For the \gls{hopcpt}, \gls{scpi}, \gls{nexcp}, and \gls{seqcp} baselines we use the open-source implementation made available by \citet{auer2023conformal}\footnote{\url{https://github.com/ml-jku/HopCPT}}. 

\section{Datasets}\label{a:datasets}

We considered several datasets from different application domains, real-world scenarios and a simulated controlled environment. For the real world datasets we followed the pre-processing steps adopted by \citet{cini2023taming}.

\paragraph{Air Quality Monitoring} The \textbf{\gls{air}} \cite{zheng2015forecasting} dataset consists of 8,760 hourly measurements of pollutant PM2.5 from 437 monitoring stations in China. We use window of $W=24$ time steps and predict the $3$-time-step-ahead observations. For the \gls{stgnn} base model, we build an adjacency matrix using a thresholded Gaussian kernel computed from pairwise geographic distances~\cite{shuman2013emerging}.

\paragraph{Traffic Forecasting} We considered the \textbf{\gls{la}} \cite{li2018diffusion} traffic forecasting dataset, consisting of 34,272 timesteps of measurements from 207 loop detectors sampled at 5-minute intervals in the Los Angeles County Highways. We use window of $12$ time steps and predict the $12$-time-step-ahead observations. For the \gls{stgnn} base model, we followed previous works~\cite{wu2019graph} and build an adjacency matrix using a thresholded Gaussian kernel applied to geographic distances.

\paragraph{Electric Load Forecasting} We selected the \textbf{\gls{cer}} dataset \cite{cer2016cer, cini2022filling}, comprising 25,728 timesteps of energy consumption readings aggregated at 30-minute intervals from 485 smart meters monitoring small and medium-sized enterprises. We use window of $48$ time steps and predict the $5$-time-step-ahead observations. For the \gls{stgnn} base model, we built the adjacency matrix by extracting a 10-nearest neighbor graph from week-wise correntropy similarities between time series, following previous work~\cite{cini2022filling}.

\paragraph{GPVAR} For the \gls{gpvar} dataset, we generate synthetic data with 40,000 timesteps over an undirected network of 60 nodes connected in a community graph structure by following the system model in \autoref{eq:gpvar}~\cite{zambon2022aztest}. The parameters of the spatiotemporal process are set as
\begin{equation}
    \Theta = \begin{bmatrix} 2.5 & -2.0 & -0.5 \\ 1.0 & 3.0 & 0.0 \end{bmatrix}, \quad a=b=0.5.
\end{equation}
We used an input window of $5$ time steps to predict the next observation. For the \gls{stgnn} base model, we used the same community graph structure as the adjacency matrix.

\paragraph{Base models} We trained three base models~(point predictors) for each dataset: a \gls{rnn} with \gls{gru} cells (1 layer with hidden size 32), a decoder-only Transformer (hidden size 32, feed-forward size 64, 2 attention heads, 3 layers, dropout 0.1), and a \gls{stgnn} following the template in \autoref{sec:stgnn} (hidden size 32, node embedding size 16, 1 layer \gls{gru}, 2 message-passing layers). All models were trained by minimizing the MAE loss using the Adam optimizer for 200 epochs with batch size 32, using 40\% of the data for training, 40\% for calibration, and 20\% for testing. Input features were standardized across the spatial and temporal dimensions. For the \gls{stgnn}, we used the pre-defined graph structure provided with each dataset.

\section{Additional details on \gls{method} implementation}\label{a:corel}

As discussed in \autoref{sec:method} and \autoref{sec:experiments}, we implemented \gls{method} as 
\gls{tts} model with a single-layer \gls{gru} followed by $2$ message passing layers. As a readout, we used an \gls{mlp} mapping the learned representations to prediction of $39$ equally spaced quantiles. 

The graph learning module was implemented as described in \autoref{sec:structure-learning} by parametrizing $\mPhi$ with a matrix of $N\times N$ learnable parameters. To allow for sampling less neighbors than the specified neighborhood size $K$, we followed previous works~\cite{cini2023sparse} and modified the sampling procedure by introducing a set of dummy nodes then discarded from the sampled graph before message passing. We use dummy nodes in the \gls{gpvar} experiment only. To allow for sparse message-passing operations we use a straight-through gradient estimator~\cite{bengio2013estimating} and backpropagate gradient only through the sampled edges for each node plus $10\%$ of the remaining ones. In the \gls{gpvar} experiment we simply propagate gradients w.r.t. the entire adjacency matrix. 

Besides residuals, we use as additional covariates datetime encodings whenever available plus the value of the target time series w.r.t.\ the time steps in the input window.


\section{Evaluation metrics}
For a prediction interval $\widehat{C}_i^\alpha = [\hat{x}_i + \hat{q}_i^{\alpha/2}, \hat{x}_i + \hat{q}_i^{1-\alpha/2}]$ and true value $x_i$, evaluation is conducted for a desired confidence level $\alpha$ using three key metrics. 

\paragraph{Coverage gap} $\Delta\text{Cov}_i$ measures the difference between the achieved coverage and the target coverage $1-\alpha$, and is given by
\begin{equation}
    \text{$\Delta$Cov}_i = \mathbbm{1}(x_i \in \widehat{C}_i^\alpha) - (1-\alpha),
\end{equation}
where $\mathbbm{1}$ denotes the indicator function.

\paragraph{Prediction interval width} Quantifies the width of the prediction intervals, and is given by
\begin{equation}
    \text{PI-Width}_i = \hat{q}_i^{1-\alpha/2} - \hat{q}_i^{\alpha/2}.
\end{equation}

\paragraph{Winkler Score} Combines interval width with a penalty for predictions that fail to capture the true value, with misses penalized by a factor of $\frac{2}{\alpha}$. The Winkler score is given by
\begin{equation}
    W_i = \begin{cases}
        (\hat{q}^{1-\alpha/2}_i - \hat{q}^{\alpha/2}_i) + \frac{2}{\alpha}(\hat{q}^{\alpha/2}_i - x_i) & \text{if } x_i < \hat{q}^{\alpha/2}_i, \\
        (\hat{q}^{1-\alpha/2}_i - \hat{q}^{\alpha/2}_i) & \text{if } \hat{q}^{\alpha/2}_i \leq x_i \leq \hat{q}^{1-\alpha/2}_i, \\
        (\hat{q}^{1-\alpha/2}_i - \hat{q}^{\alpha/2}_i) + \frac{2}{\alpha}(x_i - \hat{q}^{1-\alpha/2}_i) & \text{if } x_i > \hat{q}^{1-\alpha/2}_i.
    \end{cases}
\end{equation}
All the metrics are then averaged over all nodes and time steps within the specified set.

\section{Hyperparameters and empirical setup}

Hyperparameters were tuned separately for each combination of base predictor and dataset on a validation set.

\paragraph{\gls{method}} For \gls{method} we tuned the number of neurons in the \gls{stgnn} with a small grid search on $10\%$ of the calibration data. For the experiments on real-word data, the model was training for a maximum of $100$ epochs the calibration set. Each epoch consisted of a maximum of $50$ mini-batches of size $64$. We used the Adam optimizer~\cite{kingma2014adam} with an initial learning rate of $0.003$ and reduced by $75\%$ every $20$ epochs. We used a fixed number of $K=20$ neighbors for the grapn learning module. We used the same hyperparameters and model selection procedure for \gls{cornn}.

\paragraph{HopCPT }  For \gls{hopcpt} we followed the model selection procudere described in~\cite{auer2023conformal}. 
The model was trained for $3000$ epochs using a batch size of $4$ time series. We adopted the paper's AdamW optimizer configuration with standard parameters ($\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 0.01$) and tuned the model by running the same hyperparameter configurations searched by \cite{auer2023conformal}. All the remaining hyperparameters were set accordingly to the original paper. 

\paragraph{SeqCP } \gls{seqcp}, similarly to \cite{xu2023conformal}, employs a sliding window approach to conformal prediction, assigning equal weights to observations within the most recent $K$ time steps and zero weights to older observations. The window size $K$ was treated as a hyperparameter and tuned over the values $\{$200, 150, 125, 100, 75, 50, 25, 10$\}$. 


\paragraph{NexCP} NexCP implements conformal prediction using exponentially decaying weights controlled by a parameter $\rho$. Rather than using a fixed window of historical observations, it assigns weights that decay exponentially with time, giving more recent observations higher importance. The decay parameter $\rho$ was tuned over the values $\{$0.999, 0.995, 0.993, 0.99, 0.98, 0.95, 0.90$\}$.

\paragraph{SCPI} As already mentioned, we used the \gls{scpi}~\cite{xu2023sequential} implementation provided by \citet{auer2023conformal} and followed an analogous protocol for training the mode. In particular, \gls{scpi} was run using a fixed window length of $100$ time steps for all experiments, corresponding to the longest setting in the original paper. The computational demands of SPCI were substantial, as separate quantile random forests had to be trained for each combination of node and target coverage level, making extensive hyperparameter tuning impractical. To further manage the computational costs, we trained each SPCI model only once on the calibration data, rather than implementing the time-adaptive approach where models are re-trained as new observations become available.

\section{Optimization of the \gls{pi} width}\label{a:beta}

Here, we show how the width of the \gls{pi} can be reduced by searching for an appropriate quantile offset $\beta$.
Specifically, we perform the optimization described in \autoref{eq:cp-interval-optimized}, which identifies pairs of quantiles yielding the same coverage $1-\alpha$ while achieving the smallest interval width. 
We summarize our results in \autoref{t:beta_optim}. 
As we can see, the procedure consistently finds intervals with a smaller width while retaining approximately the same coverage, whether using \gls{method} or the \gls{cornn} variant.

\input{tables/beta-optim}
