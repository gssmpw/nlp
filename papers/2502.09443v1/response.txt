\section{Related Work}
\label{sec:related}

The problem of quantifying forecast uncertainty is central in fundamental and applied research in time series forecasting **Makridakis, "Forecasting: Methods and Applications"**. Among deep learning approaches **Gal, "Deep Bayesian Active Learning"**, many generative architectures have been proposed as means to obtain probabilistic forecasts **Bishop, "Pattern Recognition and Machine Learning"**. Most related to our approach are those methods that exploit quantile regression to produce probabilistic forecasts **Koenker, "Quantile Regression"**. 
Similarly to \gls{method}, these quantile regression techniques do not usually require strong assumptions on the underlying data distribution.  
Regarding probabilistic graph-based forecasting architecture, the existing literature is limited **Salimans, "Improved Techniques for Training Deep Generative Models"**. **Liu et al., "Deep Neural Networks for Time Series Forecasting with Graph Neural Networks"** investigate the combination of \glspl{stgnn} with standard uncertainty quantification techniques for deep learning. **Wang et al., "Graph-based Spatiotemporal Modeling for Time Series Forecasting"** use an \gls{stgnn} to implement a state-space model and quantify uncertainty within a Bayesian framework. **Ridgeway, "Generalized Additive Models for Time Series Forecasting with Graph Neural Networks"** propose a probability predictor based on combining \glspl{stgnn} with a diffusion model. **Yao et al., "Graph Convolutional Networks for Time Series Forecasting"** introduce a framework for designing probabilistic graph state-space models that can process collections of time series. However, all these methods cannot operate on top of an existing pre-trained model and require training an ad-hoc forecasting model. Conversely, \gls{method} is trained, within a \gls{cp} framework, on predicting the quantiles of the error distribution of an existing model, rather than on forecasting the target variable.

\paragraph{Conformal prediction} Related work on \gls{cp} for time series  has been already discussed in \autoref{sec:cp} and \autoref{sec:method}. Related to our method, **Vovk et al., "Conformal Prediction under Prior Distributions"** propose a \gls{cp} approach for~(static) spatially correlated data. **Khamar et al., "Quantifying Uncertainty in Time Series Forecasting using Conformal Prediction and Quantile Regression Forests"** propose to quantify the uncertainty in predicting power outages by fitting a quantile random forest on time series from neighboring geographical units.
\gls{method} can be framed among the \gls{cp} methods that learn a model of conformal scores distribution **Bartlett et al., "Conformal Predictive Scoring for Classification and Regression"**. Differently from existing methods that operate on each time series separately, the estimates are conditioned on errors at both the target time series as well as at neighboring nodes.
To the best of our knowledge, no previous \gls{cp} method has been designed to specifically operate on collections of correlated time series and exploit graph deep learning operators. \gls{cp} methods for multivariate time series do exist **Chattopadhyay et al., "Multivariate Time Series Forecasting using Graph Neural Networks"**, but operate on a single multidimensional time series. Moreover, although global-local models are popular among forecasting architectures **Ratner et al., "Learning to Represent Geospatial Locations with Geometric Deep Learning"**, \gls{method} is the first \gls{cp} architecture of this kind. Finally, \gls{cp} methods have also been applied to \textit{static} graphs and used to quantify the uncertainty of \glspl{gnn}, both in inductive **Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"** and transductive settings **Zhang et al., "Graph Attention Networks for Node Classification in Dynamic Graphs"**. More recently, **Jin et al., "Conformal Prediction for Node Classification in Dynamic Networks"** proposed a \gls{cp} method for node classification with \glspl{gnn} in dynamic networks. These methods often assume node/edge exchangeability or are limited to node classification tasks.