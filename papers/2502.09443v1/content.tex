\section{Introduction}\label{sec:intro}

Many recent advancements in deep learning methods for time series forecasting rely on learning from large collections of~(related) time series~\cite{benidis2022deep, liang2024foundation}. In many application domains, such time series are characterized by a rich spatiotemporal dependency structure that can be exploited by introducing inductive biases in the forecasting architecture~\cite{cini2023graphdeep}, to steer the learning procedure toward the most plausible models. Accounting for the existing dependencies, usually represented as a graph, allows the resulting models to obtain accurate predictions with a reduced sample complexity~\cite{jin2023survey, cini2023graphdeep}. Besides the accuracy of the point estimates, the \textit{reliability} of the forecasts is a critical aspect of the problem and a key element to enable effective decision-making in many applications~\cite{makridakis1996forecasting, petropoulos2022forecasting}.
Uncertainty quantification methods~\cite{smith2024uncertainty, vovk2005algorithmic} can improve reliability by providing confidence intervals on the forecasting error magnitude, allowing for making more informed decisions~\cite{hyndman2018forecasting}. This is particularly true for risk-sensitive applications such as healthcare~\cite{makridakis2019forecasting} and load forecasting~\cite{gasparin2022deep}. In this context, inter-series~(spatiotemporal) dynamics offer both a challenge and an opportunity. Indeed, while these dependencies can lead to wide \glspl{pi} if overlooked, they may also provide additional knowledge to reduce uncertainty~\cite{zambon2022aztest}.

Existing probabilistic forecasting frameworks often rely on strong distributional assumptions and major modifications of the base point predictor~\cite{benidis2022deep, salinas2020deepar}. As such, they cannot be used to quantify uncertainty given a pre-trained forecasting model. 
In such a setting, \gls{cp}~\cite{vovk2005algorithmic, angelopoulos2023conformal} methods are particularly appealing. \Gls{cp} is an uncertainty quantification framework that estimates confidence intervals with marginal coverage guarantees from observed prediction residuals. 
One of the main assumptions of standard \gls{cp} methods is that of exchangeability between the data used to estimate the confidence intervals and the test data points, i.e., the assumption that their joint probability distribution is invariant to the ordering of the associated sequence of random variables~\cite{angelopoulos2024theoretical}. Although this assumption does not usually hold when operating on time series~\cite{barber2023conformal}, several methods have successfully adapted \gls{cp} to estimate forecast uncertainty~\cite{stankeviciute2021conformal, xu2023conformal, xu2023sequential, jensen2022ensemble, auer2023conformal}. Nevertheless, existing \gls{cp} approaches operate on each (possibly multivariate) time series independently and cannot account for dependencies among correlated time series. 

In this paper, we propose \gls{method}, a novel \gls{cp} approach leveraging graph representations and \gls{gdl} for quantifying uncertainty in correlated time series forecasting. In our framework, a \gls{stgnn}~\citep{jin2023survey, cini2023graphdeep} is trained on a calibration set to approximate the quantile function of the distribution of prediction residuals. Relationships among time series, assumed to be \textit{sparse}, are learned end-to-end from the observed residuals owing to a graph structure learning module integrated into the processing. Our approach estimates the error quantile function for each time series at each time step, by conditioning the shared uncertainty quantification model on past observations at neighboring nodes~(as defined by the learned graph structure). {Finally, an adaptive component is added to handle potential non-stationarities by relying on a small set of parameters specific to each time series.} Our approach can be applied to the residuals generated by \textit{any} point forecasting model, even those that completely disregard potential relationships among the input time series.

Our main novel contributions can be summarized as follows.
\begin{itemize}
    \item The first application of \gls{gdl} to \gls{cp} for time series;
    \item A novel, sound, and effective \gls{cp} method able to quantify uncertainty from observations across a collection of correlated time series;
    \item A family of graph-based architectures to estimate uncertainty that share most of the learnable parameters among the processed time series, {while including node-level parameters that dynamically adapt to changes in each target sequence}.
\end{itemize}
Empirical results show that \gls{method} achieves state-of-the-art performance compared to existing \gls{cp} approaches for time series in several datasets and under different scenarios.

\section{Preliminaries}\label{sec:preliminaries}

\subsection{Problem formulation}\label{sec:forecasting} 

Consider a collection of $N$ \textit{sparsely} correlated time series. Denote by $\vx^i_t \in \sR$ the scalar target variable associated with the $i$-th time series at time step $t$; $\mX_t \in \sR^{N\times 1}$ indicates the $N$ stacked target variables w.r.t.\ the entire time series collection. $\mX_{t:t+T}$ indicates the sequence within time interval $[t, t+T)$; conversely, with the shorthand $\mX_{<t}$ refers to observations up to time step $t$~(excluded). Time series are assumed to be \newterm{homogenous}, i.e., all the variables~(observables) describe the same physical quantity~(e.g., temperature or energy consumption). Analogously, $\mU_t \in \sR^{N \times d_u}$ indicates the $d_u$-dimensional exogenous covariates associated with each time series. 
We assume that the $i$-th time series is generated by a stochastic \textit{time-invariant} process such as
\begin{equation}
    \vx_t^i \sim p\left(\vx_{t}^i | \mX_{<t}, \mU_{< t}\right).
\end{equation}
Let us hypothesize the existence of a \textit{sparse} predictive causality \textit{\`a la Granger}~\citep{granger1969investigating}, i.e., 
we assume that the values of a single time series are related to the values of a~(\textit{small}) subset of other time series in the collection. The extension of the framework to collections of multivariate time series is straightforward, but we focus on the univariate case to simplify the presentations. {The problem of dealing with non-stationary processes will be discussed in \autoref{sec:adaptive-inference}}.

\paragraph{Forecasting} We are interested in a model that makes point forecasts by predicting the unknown $H$-steps-ahead~($H\geq 0$) observation $\mX_{t+H}$ given a window $W \geq 1$ of past observations $\mX_{t-W:t}$ and the associated exogenous variables $\mU_{t-W:t}$ as
\begin{equation}
    \widehat \mX_{t+H} = \gF_\vtheta(\mX_{t-W:t}, \mU_{t-W:t}).
\end{equation}
$\gF_\theta$ denotes a generic parametric model family, i.e., a simple \gls{rnn} for univariate time series.
Given a trained model, our objective is to build a confidence interval around predictions $\widehat \mX_{t+H}$. Note that the following easily extends to multi-step predictions $\widehat \mX_{t:t+H}$, but we focus on forecasting the single time step $H$ to simplify the presentation and discussion.


\paragraph{Uncertainty quantification} Our objective is to estimate \glspl{pi}, $ C^\alpha_{i,t}(\widehat \mX_{t+H})$, such that
\begin{equation}\label{eq:marginal_coverage}
    P\left(\vx^i_{t} \in C^\alpha_{i,t}\left(\widehat \mX_{t+H} \right)\right) \geq 1- \alpha,
\end{equation}
where $\alpha$ is the desired confidence level. If the interval satisfies \autoref{eq:marginal_coverage}, we say that the \gls{pi} achieves marginal coverage $1-\alpha$. Similarly, we say that the \gls{pi} provides conditional coverage if
\begin{equation}\label{eq:conditional_coverage}
    P\left(\vx^i_{t} \in C^\alpha_{i,t}\left(\widehat \mX_{t+H} \right) \Big| \mX_{<t}, \mU_{<t} \right) \geq 1- \alpha.
\end{equation}
Conditional coverage provides stronger guarantees and it is often harder to achieve~\cite{angelopoulos2024theoretical}. In the following, we will omit the dependence of the interval on the forecasts and simply write $C^\alpha_{i,t}$. Among uncertainty quantification methods, we are interested in post-hoc approaches that can build confidence intervals for any given pre-trained point predictor $\gF_\theta$ without requiring any modification of the base forecasting architecture.

\subsection{Conformal prediction}\label{sec:cp}

As anticipated in \autoref{sec:intro}, standard \Gls{cp} methods~\cite{vovk2005algorithmic, angelopoulos2023conformal} are a class of distribution-free uncertainty quantification techniques that build \glspl{pi} from empirical quantiles of \textit{conformal scores}. In the forecasting setting, we consider as conformal scores the prediction residuals,
\begin{equation}\label{eq:residual}
    \vr^i_t = \vx^i_t - \hat \vx^i_t,
\end{equation}
and use $\mR_t$ to denote residuals w.r.t.\ the entire time series collection. Under appropriate assumptions, \gls{cp} methods can build valid and informative \glspl{pi}~\cite{angelopoulos2023conformal, barber2023conformal}. \Gls{scp}~\cite{vovk2005algorithmic} is arguably the most common approach and exploits scores computed on a \textit{calibration set} that is disjoint from the training data (i.e., a post-hoc approach). 

As mentioned, most standard \gls{cp} methods rely on the assumption that calibration and test data are \textit{exchangeable} which allows the procedure to treat them symmetrically and obtain valid~(marginal) coverage guarantees~\cite{angelopoulos2024theoretical}. Since this assumption does not hold when dealing with time series data, there have been several recent results extending the \gls{cp} framework beyond exchangeability~\cite{tibshirani2019conformal,stankeviciute2021conformal, gibbs2021adaptive, xu2023conformal}. In particular, \citet{barber2023conformal} showed that approximate coverage can be achieved by reweighting the residuals to account for the lack of exchangeability between calibration and test set. \citet{auer2023conformal} learn such a reweighting scheme through an attention-based architecture. Differently, \citet{xu2023sequential} introduce \gls{scpi}, a method based on fitting a quantile random forest~\cite{meinshausen2006quantile} on the most recent prediction residuals at each time step. Similar to \gls{scpi}, our approach relies on quantile regression to build \glspl{pi} but differently from existing methods, it exploits observations in arbitrary sets of time series by relying on \gls{gdl} operators. 

\subsection{Quantile regression}\label{sec:quantile-reg}

Quantile regression~\cite{koenker2001quantile} is an established statistical framework that consists of learning a model of the quantile function~(the inverse c.d.f.) of a target distribution from observations. In particular, given $\vy \sim p(\vy | \vx)$ and observations $(x_1, y_1), \dots, (x_N, y_N)$, a standard approach to estimate the $\alpha$-quantile is to train a model by minimizing the so-called pinball loss
\begin{equation}\label{eq:pinball}
    \ell^\alpha(\hat q^\alpha(x), y) = 
    \begin{cases}
        (1-\alpha)(\hat q^\alpha(x) - y), \, &\hat q^\alpha(x) \geq y\\
        \alpha(y - \hat q^\alpha(x)), \, &\hat q^\alpha(x) < y
    \end{cases}
\end{equation}
where $\hat q^\alpha(x)$ is the estimate of the $\alpha$-quantile w.r.t.\ $x$. 

\paragraph{Quantile networks} Quantile regression has been incorporated in several probabilistic forecasting architectures~\cite{benidis2022deep}. The simplest approach consists of using a multi-output network to predict a set of quantiles of interest and interpolate among them to approximate the entire quantile function~\cite{wen2017multi}. More complex approaches rely on, e.g.,  splines~\cite{gasthaus2019probabilistic}. Conversely, \glspl{iqn}~\cite{dabney2018implicit, ostrovski2018autoregressive, gouttes2021probabilistic} approximate the quantile function by being trained to minimize the loss in \autoref{eq:pinball} given the quantile level $\alpha$ as input and sampling a random $\alpha$ for each sample in a mini-batch. 

\input{imgs/figure_1}

\subsection{Graph deep learning for time series forecasting}\label{sec:stgnn}

\Glspl{gnn}~\cite{bacciu2020gentle, bronstein2021geometric} process graph-structured data by incorporating the graph topology as an inductive bias, e.g., by relying on message-passing layers~\cite{gilmer2017neural}. \Glspl{stgnn}~\cite{jin2023survey, cini2023graphdeep} leverage message-passing layers within sequence modeling architectures to process spatiotemporal data and collections of time series where dependencies are represented as a (possibly dynamic) graph. We consider as reference architectures \gls{tts} models~\cite{ gao2021equivalence, cini2023graphdeep} where each time series in the collection is processed independently from the others by a temporal encoder whose output is then fed into a stack of \gls{gnn} layers. In particular, we adopt the following template architecture:
\begin{align}
    \vh^{i,0}_t &= \textsc{SeqEnc}\left(\vx^i_{t-W:t}, \vu^i_{t-W:t}\right),\\
    \mH_t^{l+1} &= \gnn_l(\mH_t^l,\mA),\label{eq:mp}, \quad l=0,\dots,L-1 \\
    \hat \vy_{t} &= \textsc{Readout}\left(\vh_t^{L}\right),
\end{align}
where $\mA \in \sR^{N\times N}$ is the graph adjacency matrix and $\hat \vy_{t}$ a generic node-level prediction associated with the problem at hand. $\textsc{SeqEnc}({}\cdot{})$ and $\gnn_l({}\cdot{})$ denote, respectively, any sequence modeling architecture, e.g., an \gls{rnn}, and any \gls{gnn} layer, e.g., based on message-passing. Representations can then be mapped into predictions $\widehat \mY_{t}$ by using any readout block, e.g., a \gls{mlp}. \Glspl{stgnn} have been used as forecasting architecture~($\mY_t = \mX_{t+H}$) with great success. In the following, we will exploit this framework as a backbone for estimating the residual quantile distribution. We refer to \autoref{sec:related} and \citet{jin2023survey} for more discussion on the application of \glspl{stgnn} in the context of time series analysis.

\section{Conformal relational prediction}\label{sec:method}

Our objective is to build \glspl{pi} by exploiting relational dependencies across the residuals of the target time series. We model the dependencies as edges of a graph and learn them under the assumption that the relational structure is \textit{sparse}, which reduces the computational costs and act as an inductive bias on the structure learning architecture. By relying on such representation, we can leverage \gls{gdl} methods for time series to process the data. In particular, we train a \gls{stgnn} on the residuals of the calibration set to predict the quantiles of the error distribution. Conditioning the prediction on the recent history of related time series allows for taking the dependency structure of the data into account when estimating uncertainty: a key aspect in applying conformal prediction to non-exchangeable data~\citep{barber2023conformal}. Compared to existing methods~\cite{xu2023sequential, auer2023conformal} that only capture temporal dependencies, our approach allows for modeling spatiotemporal dependencies among different time series. 
\autoref{sec:conformal-procedure} presents the details of the proposed conformal inference procedure by assuming that the relational structure at each time step is defined by an adjacency matrix $\mA \in \sR^{N\times N}$~\autorefp{sec:conformal-procedure}. We then show how to learn the graph structure directly from data and make the model adaptive in \autoref{sec:structure-learning} and \autoref{sec:adaptive-inference}, respectively. Finally, we discuss the theoretical properties of the approach in \autoref{sec:discussion}. \autoref{fig:corel} shows an overview of the architecture.

\subsection{Relational Quantile Predictor}\label{sec:conformal-procedure}

Consider a standard \gls{scp} setup, where the training data are split into training and calibration sets. For the moment, we disregard possible nonstationarities in the data considering the problem setup introduced in \autoref{sec:forecasting} and encode spatial dependencies in the adjacency matrix $\mA \in \sR^{N\times N}$. While the training set is used to fit the point predictor $\gF_\theta$, we use the prediction residuals in the calibration set~($\gR^{cal}$) to learn the quantile function of the error distribution at each time step. 

We implement the quantile regressor as a hybrid global-local \gls{stgnn}, which mixes global (shared) parameters with local, target-specific components~\cite{smyl2020hybrid}. Sharing most learnable parameters across all time series reduces sample complexity, while local parameters allow for processing each series differently. Specifically, we keep all processing blocks shared and associate a learnable node embedding $\vv^i \in \sR^{d_v}$ with each time series~\cite{cini2023taming}.
More specifically, our model is a quantile network (see \autoref{sec:quantile-reg}) composed of the following processing layers:
\begingroup
\allowdisplaybreaks
\begin{align}
    \vh^{i,0}_{t} &= \textsc{Enc}\left(\vr^i_{t-1},\vv^i\right),\label{eq:enc}\\
    \mZ_{t} &= \textsc{STGNN}\Big(\mH^{0}_{
    \leq t}, \mA\Big),\label{eq:stmp-block}\\
    \hat \vq^{\alpha,i}_{t+H} &= \textsc{QDec}\left(\alpha, \vz_{t}^{i}, \vv^i\right)\label{eq:dec},
\end{align}
\endgroup
where $\vr^i_{t-1}$ are prediction residuals~(\autoref{eq:residual}) and  $\hat \vq^{\alpha,i}_{t+H}$ is the predicted $\alpha$-quantile at time step $t+H$ for the $i$-th time series. $\textsc{Enc}({}\cdot{})$ denotes any encoding layer,e.g., a linear transformation or an \gls{mlp}. 
For the \gls{stgnn} block, several designs are possible~(e.g., see \citealt{jin2023survey}); the one we follow is the template in \autoref{sec:stgnn}.
$\textsc{QDec}({}\cdot{})$ is a readout mapping the representations at each node to the prediction of the quantile of specified level $\alpha$.
We refer to the family of quantile networks defined in \autorefseq{eq:enc}{eq:dec} as \glspl{relqn} and use the notation
\begin{equation}\label{eq:quantile-estimator}
    \widehat \mQ^\alpha_t = \gQ_{\psi}\left(\alpha, \mV; \mR_{t-W:t}, \mA\right),
\end{equation}
where $\gQ_{\psi}$ indicates the shared~(global) part of the network and $\widehat \mQ^\alpha_t \in \sR^N$ denotes the predicted $\alpha$-quantiles at time step $t$ w.r.t.\ the full time series collection. Note that the framework can easily accommodate further inputs at the encoding block~(e.g., we can condition the regression on $\mX_{<t}$ and $\mU_{<t}$). 
The model is trained by, at each time step in the calibration set, the pinball loss~(\autoref{eq:pinball}) w.r.t.\ the full-time series collection. 
Through the message-passing layers, the residuals of each time series contribute to estimating the quantiles of the error distribution at neighboring nodes. In practice, we restrict the input of the regressor to the most recent observations rather than considering the full sequence~(the window length here can also be different from the one used by the point predictor).

\paragraph{Building the confidence intervals} Given the trained quantile network $\gQ_\psi$, we build the \glspl{pi} for each target~(test) time step as
\begin{equation}\label{eq:cp-interval}
    \widehat C^\alpha_{t} = \left[\widehat \mX_{t+H} + \widehat{\mQ}^{\alpha/2}_t, \widehat \mX_{t+H} + \widehat{\mQ}^{1-\alpha/2}_t\right],
\end{equation}
or
\begin{align}
    \hat \beta &= \argmin_\beta \Big|\widehat{\mQ}^{1-\alpha/2 + \beta}_t - \widehat{\mQ}^{\alpha/2 + \beta}_t\big|\\
    \widehat C^\alpha_{t} &= \left[\widehat \mX_{t+H} + \widehat{\mQ}^{\alpha/2 + \hat \beta}_t, \widehat \mX_{t+H} + \widehat{\mQ}^{1-\alpha/2 + \hat \beta}_t\right],\label{eq:cp-interval-optimized}
\end{align}
where  $\widehat C^\alpha_{t}$ indicates the estimated \gls{pi}. While both \autoref{eq:cp-interval} and \autoref{eq:cp-interval-optimized} correspond to the same confidence level, the \gls{pi} in \autoref{eq:cp-interval-optimized} can be narrower, at the expense of the additional computation needed to obtain $\hat \beta$~\cite{xu2023conformal}. In practice, one can choose between the two approaches given computational constraints and the difference in performance observed on a validation set.  Note that $\widehat C^\alpha_{i,t}$ can provide only \textit{approximate} coverage, as it is subject to approximation errors of the true quantile function. \autoref{sec:discussion} will discuss this aspect in detail. 
{A potential drawback of \gls{method} is that residuals cannot be assumed exchangeable in most practical scenarios. The error distribution can be \textit{non-stationary}, making it difficult to obtain any coverage guarantee. To mitigate the problem, \autoref{sec:adaptive-inference} discusses an efficient and scalable approach to make the framework adaptive by updating local components of the architecture over time.} Finally, it is worth noting that the use of relational components in \gls{method} relies on the actual presence of the associated dependencies in the data. 
In practical applications, the presence of spatiotemporal correlations in the residuals can be verified through ad-hoc statistical tests ~\cite{zambon2022aztest, zambon2023where}, whose outcome can support the adoption of \gls{method}. 

\subsection{Learning the Relational Structure}\label{sec:structure-learning}

Assuming the dependency structure across time series to be unknown, we integrate a graph learning module into the architecture to derive the operational graph topology directly from the residuals. To do so, we adopt a probabilistic structure learning framework~\cite{niculae2023discrete, cini2023sparse, manenti2024learning}. In particular, we associate each edge with a score $\phi^{ij}$ and learn a distribution over $K$-NN graphs parametrized by the matrix $\mPhi \in \sR^{N\times N}$~\cite{cini2023sparse, kazi2022differentiable}. Notably, we consider graphs obtained by sampling, for each $i$-th node, $K$ elements \textit{without replacement} from the categorical distribution 
\begin{align}\label{eq:graph-sampler}
    \mPhi &= \gE_{\bm{\xi}}\left(\mR_{<t}, \mV, \dots\right)\\
    M_i &= \text{Categorical}\left(\frac{\exp\{\phi^{ik}\}}{\sum_{j=1}^N\exp\{\phi^{ij}\}};k \in \{1, \dots, N\}\right),
\end{align}
where $\gE_{\bm{\xi}}({}\cdot{})$ is a generic trainable encoder with parameters $\bm{\xi}$. In practice, sampling can be done efficiently by exploiting the $\text{GumbelTopK}$ trick~\cite{kool2019stochastic} and scores $\mPhi$ can be parametrized directly as $\mPhi = \bm{\xi}$. 

\paragraph{End-to-end learning} To propagate gradients through the sampling, we rely on the continuous relations introduced by ~\citet{xie2019reparameterizable} paired with a straight-through gradient estimator~\cite{bengio2013estimating} to obtain discrete samples. Optionally, we sparsify the gradients by backpropagating only through a random subset of the zero entries of $\mA$~({more details are provided in \autoref{a:corel}}). 

\subsection{Theoretical analysis and further discussion}\label{sec:discussion}

We start the discussion by providing an intuitive bound on the approximate coverage provided by \gls{method}. 

\begin{proposition}\label{p:coverage-bound} Let $P^{c}_{t+H}(\mX_{t+H}) = p_{t+H}(\mX_{t+H} \mid \mX_{<t}, \mU_{<t})$ and $P^c_{\psi}(\mX_{t+H}) = p_{\psi}(\mX_{t+H} \mid \mX_{<t},\mU_{<t})$ be the true conditional data-generating distribution at the test point $t+H$ and the probability distribution associated with the learned quantile function $\gQ_{\psi}$, respectively. Then
\begin{align*}
    P^c_{t+H}\left(\mX_{t+H} \in \hat C^{\alpha}_t (\widehat \mX_{t+h})\right) \geq 1 - \alpha - {TV}\left(P^c_{\psi}, P^c_{t+H}\right)%\label{eq:coverage-bound}
\end{align*}
where ${TV}({}\cdot{})$ denotes the total variation function.
\end{proposition}
The proof relies on the properties of the total variation of probability measures and can be found in \autoref{a:theory}. Here, differently from the problem settings introduced in \autoref{sec:forecasting}, we do not assume the process to be time-invariant. \autoref{p:coverage-bound} links the \textit{conditional} coverage gap to the approximation error in estimating the quantile function of the residuals. By making assumptions on the expressivity of the quantile regressor in \autoref{eq:quantile-estimator} and on the stationarity process~(e.g., by assuming a strongly mixing process), we can expect the total variation between the learned and true distribution to shrink asymptotically as the size of the calibration set increases. Moreover, in this case, monitoring the coverage gap on a validation set offers an estimate of the actual miscoverage on test data. Conversely, if we expect the process to be non-stationary, $\gQ_\psi$ has to be updated over time to keep the coverage gap contained. Within this context, the next section discusses a simple and sample-efficient approach to make \gls{method} adaptive. Finally, the bound provided in \autoref{p:coverage-bound} shares similarities with the one in \cite{barber2023conformal}, which bounds the miscoverage gap for \gls{cp} from weighted empirical quantiles. \autoref{p:coverage-bound} can be seen as an analogous result that holds when estimates obtained from empirical quantiles are replaced with a parametric function approximator. 


\subsection{Adaptativity}\label{sec:adaptive-inference}

The \gls{relqn} model introduced in \autoref{sec:conformal-procedure} can yield arbitrarily large coverage gaps in the presence of distribution shifts from the calibration set, where the model is trained.
Adopting a re-training approach such as in \citet{xu2023sequential} would be impractical due to the higher sample complexity entailed by the deep learning approach that we adopt. 
Therefore, to mitigate this issue while keeping the computational complexity under control, we update only the local components of the model over time, i.e., the learnable node embeddings $\mV$~\cite{cini2023taming}. This allows for keeping most of the parameters of the model fixed and fine-tuning only a small number of weights for each node. Empirically, we show that this procedure can effectively improve the quality of the uncertainty estimates.




\section{Related Work}\label{sec:related}

The problem of quantifying forecast uncertainty is central in fundamental and applied research in time series forecasting~\cite{hyndman2018forecasting, petropoulos2022forecasting}. Among deep learning approaches~\cite{benidis2022deep}, many generative architectures have been proposed as means to obtain probabilistic forecasts~\cite{salinas2020deepar,rangapuram2018deep, debezenac2020normalizing, rasul2021autoregressive}. Most related to our approach are those methods that exploit quantile regression to produce probabilistic forecasts~\cite{wen2017multi, gasthaus2019probabilistic, kan2022multivariate, gouttes2021probabilistic}. 
Similarly to \gls{method}, these quantile regression techniques do not usually require strong assumptions on the underlying data distribution.  
Regarding probabilistic graph-based forecasting architecture, the existing literature is limited~\cite{jin2023survey, cini2023graphdeep}. \citet{wu2021quantifying} investigate the combination of \glspl{stgnn} with standard uncertainty quantification techniques for deep learning. \citet{pal2021rnn} use an \gls{stgnn} to implement a state-space model and quantify uncertainty within a Bayesian framework. \citet{wen2023diffstg} propose a probability predictor based on combining \glspl{stgnn} with a diffusion model~\cite{ho2020denoising}. \citet{zambon2023graph} introduce a framework for designing probabilistic graph state-space models that can process collections of time series. However, all these methods cannot operate on top of an existing pre-trained model and require training an ad-hoc forecasting model. Conversely, \gls{method} is trained, within a \gls{cp} framework, on predicting the quantiles of the error distribution of an existing model, rather than on forecasting the target variable.

\paragraph{Conformal prediction} Related work on \gls{cp} for time series  has been already discussed in \autoref{sec:cp} and \autoref{sec:method}. Related to our method, \citet{mao2024valid} propose a \gls{cp} approach for~(static) spatially correlated data. \citet{jiang2024spatio} propose to quantify the uncertainty in predicting power outages by fitting a quantile random forest~\cite{meinshausen2006quantile} on time series from neighboring geographical units.
\gls{method} can be framed among the \gls{cp} methods that learn a model of conformal scores distribution~\citep{xu2023sequential, lee2024conformal}. Differently from existing methods that operate on each time series separately, the estimates are conditioned on errors at both the target time series as well as at neighboring nodes.
To the best of our knowledge, no previous \gls{cp} method has been designed to specifically operate on collections of correlated time series and exploit graph deep learning operators. \gls{cp} methods for multivariate time series do exist~\cite{xu2024conformal, sun2024copula}, but operate on a single multidimensional time series. Moreover, although global-local models are popular among forecasting architectures~\cite{smyl2020hybrid, benidis2022deep}, \gls{method} is the first \gls{cp} architecture of this kind. Finally, \gls{cp} methods have also been applied to \textit{static} graphs and used to quantify the uncertainty of \glspl{gnn}, both in inductive \cite{zargarbashi2023conformal} and transductive settings \cite{huang2024uncertainty}. More recently, \citet{davis2024valid} proposed a \gls{cp} method for node classification with \glspl{gnn} in dynamic networks. These methods often assume node/edge exchangeability~\cite{zargarbashi2023conformal, huang2024uncertainty} or are limited to node classification tasks~\cite{clarkson2023distribution}. 


\input{tables/tab-exp}

\section{Experiments}\label{sec:experiments}



We validate \gls{method} across three experimental settings. In the first one~(\autoref{sec:benchmarks}), we compare it against state-of-the-art \gls{cp} methods operating on the residuals produced by different forecasting models. Then, we analyze \gls{method} in a controlled environment~(synthetic dataset). Finally, we assess the effectiveness of the procedure described in \autoref{sec:adaptive-inference} in adaptively improving the \glspl{pi}.
We implement \textbf{\gls{method}} as an \gls{rnn} followed by two message-passing layers. To approximate the quantile function, we train the model by minimizing the pinball loss over a discrete set of quantiles, similarly to \citet{wen2017multi}. \glspl{pi} are constructed as in \autoref{eq:cp-interval}; \autoref{a:beta} shows results for the alternative construction in \autoref{eq:cp-interval-optimized}. To learn the graph, we directly parametrize the score matrix $\mPhi$ by associating a learnable parameter with each of its entries.
We use as metrics: 1)~the difference between the specified confidence level $1-\alpha$ and the observed coverage on the test set~(\textit{$\Delta$Cov}); 2)~the width of the \gls{pi}~(\textit{\gls{pi}-Width}); 3)~the Winkler score~\cite{winkler1972decision}, which is computed as the width of the \gls{pi} plus penalty for each observation outside of the predicted interval proportional to the actual error~(\textit{Winkler}). More details and results are provided in the appendix.


\subsection{Time series forecasting benchmarks}\label{sec:benchmarks}\label{s:benchmark}

We consider the following datasets, each coming from a different application domain: 
\textbf{\gls{la}} from the traffic forecasting literature~\cite{li2018diffusion}; a collection of air quality measurements from different Chinese cities~(\textbf{\gls{air}})~\cite{zheng2015forecasting}; a collection of energy consumption profiles acquired from smart meters within the CER smart metering project~(\textbf{\gls{cer}})~\cite{cer2016cer, cini2022filling}. 
We follow the preprocessing steps of previous works~\cite{li2018diffusion, wu2019graph, cini2023taming} and adopt $40\%/40\%/20\%$ splits for training, calibration, and testing, respectively. 
For each dataset, we first train $3$ different baseline models: a simple \textbf{\gls{rnn}} with \gls{gru} cells~\cite{cho2014properties}, a decoder-only \textbf{Transformer}~\cite{vaswani2017attention}, a simple \gls{tts} \textbf{\gls{stgnn}} by following the template in~\autoref{sec:stgnn}.
The latter uses a pre-defined graph that models the dependencies across the time series.
After training, we evaluate each baseline on the calibration set and save the associated residuals, which are then used as input to the different \gls{cp} methods. More details on the datasets and base models are provided in \autoref{a:datasets}.

\paragraph{Baselines} We compared \gls{method} against the following methods: 1) \textbf{\gls{scp}}, the standard split \gls{cp}; 2) \textbf{\gls{seqcp}}, where, analogously to \citet{xu2023conformal}, 
we compute empirical quantiles using only the most recent $K$ residuals at each time step; 3) \textbf{\gls{nexcp}}~\cite{barber2023conformal}, which computes empirical quantiles by assigning exponentially decaying weights to past residuals; 4) \textbf{\gls{scpi}}~\cite{xu2023sequential}, which estimates the residuals' quantile function from the last few steps of each time series with a random forest; 5) \textbf{\gls{hopcpt}} which reweights past residuals by learning attention scores with a Modern Hopfield
Network~\cite{ramsauer2021hopfield}. We include in our comparison a model called \textbf{\gls{cornn}}, where we use the same architecture as \gls{method} but remove the message-passing layers. 
Except for \gls{hopcpt} which uses a custom procedure~\cite{auer2023conformal}, model selection is performed on a validation set by optimizing the Winkler score. 

\input{tables/table-adaptation}

\paragraph{Results} \autoref{tab:exp1} reports the results across the datasets and the base prediction models. The first observation is that \gls{method} outperforms the competitors in terms of Winkler score in almost all cases. 
We observed a few exceptions only when the baseline is itself an \gls{stgnn}, as it is already expected to take care of modeling spatiotemporal dependencies. 
However, note that the \gls{stgnn} base model has access to a pre-defined graph, which is not always available in practical applications. 
In terms of coverage, \gls{method} achieves good results, with the exception of some cases in the \gls{cer} dataset. However, we note that our model selection prioritized the Winkler score which emphasizes the width of the prediction bands besides the coverage. \gls{cornn}, the simplified version of our approach, obtains good overall performance and is competitive against the state-of-the-art but, despite providing good coverage, it is outperformed by \gls{method} in most scenarios in terms of Winkler score. 
Among the competitors, \gls{hopcpt} provides competitive results in \gls{la} and \gls{cer} but with a larger coverage gap. Except for \gls{seqcp}, the other baselines obtain good coverage in most settings at the expense of drastically wider \glspl{pi}. 

\input{tables/table-gpvar}

\subsection{Controlled environment}\label{sec:gpvar-exp}

We evaluated the behavior of \gls{method} in a controlled environment by simulating a diffusion process on a graph. In particular, the experiment relied on the \gls{gpvar} benchmark introduced by \citet{zambon2022aztest}, with a setup analogous to \cite{cini2023taming}.
Data were generated recursively from the auto-regressive polynomial graph filter:
\begin{align}\label{eq:gpvar}
    \mH_t &= \sum_{l=1}^L\sum_{q=1}^{Q} \Theta_{q,l}\mA^{l-1}\mX_{t-q},\notag\\
    \mX_{t+1} &= \va \odot \text{tanh}\left(\mH_t\right) + \vb\odot\text{tanh}\left(\mX_{t-1}\right) + \eta_t,
\end{align}
%
where parameters $\boldsymbol{\Theta}\in\sR^{Q \times L}$, $\va\in\sR^N$, $\vb\in\sR^N$ are kept fixed across nodes and $\eta_t \sim \mathcal{N}(\boldsymbol{0}, \sigma^2\sI)$. We ran the simulation on a graph with $60$ nodes and with a topology analogous to previous works~\cite{cini2023taming}; more details are provided in \autoref{a:datasets}. 
We use both a \gls{rnn} and \gls{stgnn} as base point predictors. As shown by \citet{zambon2022aztest}, \glspl{stgnn} can obtain a forecasting accuracy near the theoretical optimum in this dataset, which results in i.i.d.\ residuals. As such, we would expect standard \gls{scp} to be sufficient in this scenario. The objective of this experiment is to show that \gls{method} is effectively able to capture and leverage existing spatiotemporal dependencies.

\paragraph{Results} We compare \gls{method} against standard \gls{scp} and the \gls{cornn} variant. Moreover, we also compare performance against \gls{method} with access to the true graph used to generate the data. Results are shown in \autoref{t:gpvar}. When using an \gls{rnn} as a point predictor~(\textbf{\gls{gpvar}-\gls{rnn}}) \gls{method} significantly outperforms both standard \gls{scp} and \gls{cornn}. 
Furthermore, \gls{method} achieves results that closely match those obtained with direct access to the ground truth graph, showing the effectiveness of the proposed architecture in capturing latent relational dependencies. Finally, results obtained by using an \gls{stgnn} as baseline~(\textbf{\gls{gpvar}-\gls{stgnn}}) show~(as expected) that the standard \gls{scp} is sufficient when the point-predictor captures all the relevant dependencies. 

\subsection{Adaptability}\label{sec:adpatation-exp}

In this experiment, we evaluate how effectively the adaptation technique proposed in \autoref{sec:adaptive-inference} provides accurate \glspl{pi} over time. We focused on the \gls{cer} dataset where the calibration set does not cover a full year, which likely introduces a shift at test time given the seasonality of energy consumption. This behavior is also confirmed by the results in \autoref{s:benchmark}.

We used \gls{method} with a fixed hyperparameter configuration across scenarios and trained on the calibration set.
We then split the test set in $K=6$ folds and then iteratively evaluated and fine-tuned the model on each fold. Results, reported in \autoref{t:adaptation}, show that the proposed adaptation schemes improve performance by reducing the coverage gap and improving the accuracy of the \gls{pi}. 

\section{Conclusion}\label{sec:conclusion}

In this paper, we introduced \acrfull{method}, a novel \gls{cp} method for correlated time series. \gls{method} exploits graph-based neural operators to implement an uncertainty quantification architecture that can operate on top of any pre-trained point predictor. Furthermore, our approach does not require the relational structure to be known in advance.  Results show that the proposed method compares favorably against the state-of-the-art in several relevant scenarios. 

\paragraph{Future works} We believe that \gls{method} constitutes an important step toward effective spatiotemporal \gls{cp} methods. There are several directions for future works to explore, such as applying the framework to heterogenous time series. Future work should also further focus on mitigating issues coming from learning in non-stationary environments. 

