\begin{table}[t]
    \centering
    \caption{\change{Summary Recommendations for Future Cross-Country Smartphone-Based Data Collection.}}
    \label{tab:recommendations}
    \begin{tabular}{p{0.02\linewidth} p{0.85\linewidth}}
        \toprule
        %\textbf{Areas} & \textbf{Recommendations} \\
        %\midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#1: Core Protocol Design}}} \\
        & 
        Develop a standardized core protocol that allows flexible local adaptations. Consult with local experts early to validate instruments, and pilot-test before
        large-scale deployment. \\

        \arrayrulecolor{Gray}
        \midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#2: Cultural Sensitivity}}} \\
        & 
        Involve local stakeholders to ensure cultural norms are respected.
        Conduct small-scale pilots to refine sensitive questions, and leverage cultural experts for precise translations.  \\

        \arrayrulecolor{Gray}
        \midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#3: Privacy and Ethics}}} \\
        & 
        Create a modular privacy framework compliant with international (e.g., GDPR) and local regulations. Use adaptable templates and regularly consult legal experts to monitor changes.\\

        \arrayrulecolor{Gray}
        \midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#4: Data Anonymization}}} \\
        & 
        Adopt multi-level anonymization (e.g., personal identifiers, location data) to balance privacy with data utility. Offer multiple data versions (e.g., “RoundDown” GPS vs. “POI” GPS) to accommodate different re-identification risks.\\

        \arrayrulecolor{Gray}
        \midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#5: Incentive Strategies}}} \\
        & 
        Tailor incentives to local economic conditions and cultural preferences. Collaborate with local partners to choose monetary vs. non-monetary rewards for maximum engagement.\\

        \arrayrulecolor{Gray}
        \midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#6: Technological Adaptation}}} \\
        & 
        Design or select survey tools that can operate both online and offline. Plan for restricted services (e.g., Google) and ensure solutions work in diverse mobile ecosystems.\\

        \arrayrulecolor{Gray}
        \midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#7: Participant Engagement}}} \\
        & 
        Maintain transparent communication through notifications, reminders, and helpdesks. Offer support channels that address technical or procedural issues quickly without biasing responses.\\
        \arrayrulecolor{black}
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Data Collection -- Lessons Learned and Recommendations for the Future}\label{subsec:lessons_design}

\change{In Section 3, we detailed our core design decisions for gathering cross-country \dataset dataset. Building on those design considerations, this section focuses on the broader lessons learned during implementation, compares our experiences with other local data collection practices, and discusses how to adapt these insights for future studies. We consolidate all recommendations into Table~\ref{tab:recommendations}.}

\change{Collecting smartphone data across diverse cultural and regulatory contexts offered an invaluable opportunity to observe how different countries and research teams navigate ethical, logistical, and cultural considerations. Our collaboration revealed several common threads shared with local researchers as follows. First, \textit{Cultural Adaptation:} while our study required adjustments in survey content, incentive structures, and communication styles, local researchers often faced similar challenges. For instance, local studies had to modify language and question framing to ensure sensitivity to cultural or religious norms. Our experiences echo these practices but highlight the importance of designing a standardized core protocol that can be flexibly adapted without sacrificing data comparability. Second, \textit{Privacy and Regulatory Compliance:} many local research groups reported ongoing adaptations to meet international guidelines (e.g., GDPR) while conforming to national regulations. Our approach—having a core GDPR-compliant framework and then tailoring it to meet local legal expectations—aligned with similar multi-country studies in the literature. This comparison highlights the benefits of centralized documentation that can be adjusted to local requirements. Third, \textit{Technological Infrastructure and Offline Solutions:} the need to accommodate varying degrees of internet connectivity and restricted services was not unique to our study; many local researchers rely on hybrid models (online/offline) for data gathering. Our development of an “offline” version of the \texttt{iLog} app is congruent with other best practices in low-connectivity settings, such as storing data locally and syncing at scheduled intervals.}

%\change{\subsubsection{Key Areas for Improvement in Study Design} Based on these reflections, we identify three key areas that future cross-country projects should emphasize: \textit{Enhanced Community Engagement:} Early dialogue with local communities and stakeholders could further align research goals with local needs, improving response rates and data quality; \textit{Continual Regulatory Monitoring:} Given that privacy regulations evolve, maintaining an adaptable legal and ethical framework can prevent delays and ensure compliance in multi-year studies; \textit{Context-Specific Scalability:} As studies scale to more countries, technology solutions (apps, dashboards, etc.) must remain flexible to rapidly integrate new languages, offline capabilities, and participant support systems.}


\change{Hence, Table~\ref{tab:recommendations} brings together our main recommendations and the current reflections. Each item links directly to a stage of the study—from protocol design to data anonymization—and highlights how researchers can adapt these suggestions to their own contexts. By integrating our field experiences, we observed that successful cross-country data collection depends on a core set of adaptable protocols. We hope these consolidated recommendations will guide future multi-country studies in designing, implementing, and scaling smartphone-based data collection responsibly and effectively.}



\subsection{Dataset analysis -- Lessons Learned and Recommendations for the Future}\label{subsec:lessons_analysis}

The analysis of \dataset  across various studies has revealed critical insights into the influence of local context, country-specific nuances, and multi-modal data streams on the effectiveness of mobile sensing models.
%\cite{meegahapola2023generalization, meegahapola2024m3bat, assi2023complex, mader2024learning, kammoun2023understanding, bouton2022your}. A summary of these studies is provided in Table~\ref{tab:prev_studies_summary}. 
Presented below are key lessons learned from these studies, each accompanied by a recommendation for future research. 

\begin{table}[t]
\centering
\small
\caption{\change{A summary of previous studies that utilized the \dataset dataset is presented in the table. The acronyms used are as follows: Target/s Inferred refers to the primary aspect that was focused in the particular study; CS indicates that country-specific models were trained; MC denotes that multi-country models were trained by combining data from multiple countries; CA refers to the examination of a country-agnostic setting, focusing on the generalization performance of models across countries; and PER indicated that model personalization was examined; SL refers to supervised learning; FT/RT refers to transfer learning with fine-tuning or re-training for classic machine learning models to achieve personalization; and UDA refers to unsupervised domain adaptation.}}\label{tab:prev_studies_summary}
\begin{tabular}{llllllll}
\toprule
 & \textbf{Venue and Year} & \textbf{Target/s Inferred}                                         & \textbf{CS}  & \textbf{MC}   & \textbf{CA} & \textbf{PER} & \textbf{Methods} \\
\midrule
Bouton–Bessac et al. \cite{bouton2022your} & PervasiveHealth '22 & Complex Daily Activities              & $x$ & $\checkmark$  & $x$  & $\checkmark$ & SL, FT/RT \\
Meegahapola et al. \cite{meegahapola2023generalization} & IMWUT '23 & Mood              & $\checkmark$ & $\checkmark$  & $\checkmark$  & $\checkmark$ & SL, FT/RT \\
Assi \& Meegahapola et al. \cite{assi2023complex} & CHI '23 & Complex Daily Activities  & $\checkmark$ & $\checkmark$  & $\checkmark$  & $\checkmark$  & SL, FT/RT \\
Kammoun et al. \cite{kammoun2023understanding} & ICMI '23 & Social Context when Eating  & $\checkmark$ & $\checkmark$  & $x$      & $\checkmark$  & SL, FT/RT \\
Girardini et al. \cite{girardini2023adaptation} & EPJ DataScience '23 & Daily routines and Covid       & $\checkmark$ & $x$  & $x$  & $\checkmark$  & SL, FT/RT \\
Meegahapola et al. \cite{meegahapola2024m3bat} & IMWUT '24 & Mood, Social Context       & $\checkmark$ & $x$      & $\checkmark$  & $x$   & SL, UDA   \\
Mader \& Meegahapola et al. \cite{mader2024learning} & CHI '24 & Social Context         & $\checkmark$ & $\checkmark$  & $\checkmark$  & $\checkmark$  & SL, FT/RT \\

\bottomrule
\end{tabular}
\end{table}

\subsubsection{Country-specific Models over Multi-country Models}


Studies consistently demonstrate that models trained on country-specific data outperform those trained on multi-country datasets. For tasks such as mood inference \cite{meegahapola2023generalization} and complex daily activity recognition \cite{assi2023complex}, country-specific models achieved higher accuracy, with AUROC scores ranging from 0.76 to 0.98 when using partially personalized, hybrid models. Conversely, generic models not adapted to specific countries often struggled when applied to new regions, failing to capture unique data distributions that reflect localized contexts. This gap emphasizes the critical importance of country-specific patterns and nuances in daily routines and sensor data, which vary widely and directly influence data interpretation. These findings are in line with the findings of Khawaja et al. \cite{khwaja2019modeling}. 

Moreover, these insights shed light on limitations faced by previous multi-country sensing studies, such as those conducted by Servia-Rodriguez et al. \cite{servia2017mobile}. By pooling data from multiple countries, these models averaged out region-specific patterns, reducing predictive power and capacity to capture unique, localized behaviors. For example, in the study by Meegahapola et al. \cite{meegahapola2023generalization}, location and movement data were highly predictive of mood in certain countries but held minimal relevance in others, underscoring regional differences in daily routines and smartphone usage. This diversity indicates that a one-size-fits-all approach overlooks essential cultural and contextual factors crucial for accurate behavior inference, reinforcing the need for adaptable, localized models.

Computer vision models provide a valuable contrast here. Convolutional neural networks (CNNs) and vision transformers excel at capturing features in images—textures, edges, and spatial hierarchies—regardless of geographic origin. These models generalize effectively across multi-country visual datasets by learning local and global image features, excelling when regional content differences are less impactful on performance. However, smartphone time-series data captures complex, region-specific behavioral nuances that current time-series models struggle to interpret consistently across geographies. Developing models that can capture local nuances in behavioral data while generalizing across regions remains crucial for advancing global smartphone sensing applications.


\subsubsection{Value of Hybrid Model Personalization}

\change{Typically, models developed for various prediction tasks are tested on a different population to evaluate their performance, reflecting real-world scenarios. These are referred to as population-level models. 
%\cite{bangamuarachchi2022sensing}
However, prior work \cite{zhang2024reproducible, meegahapola2023generalization, assi2023complex} in ubiquitous computing has shown that such models often underperform, especially for highly subjective or complex inferences. Therefore, the population-level model should either be fine-tuned or re-trained for the target individual using a small portion of that individual's data (known as hybrid models), or, if sufficient data are available and it is computationally feasible, a model can be trained solely on the target individual's data (fully personalized models).
%\cite{bangamuarachchi2022sensing}
} Research using \dataset dataset has shown that even modest personalization using hybrid models enhances performance in behavioral inference tasks such as mood detection \cite{meegahapola2023generalization}, complex activity recognition \cite{assi2023complex}, and social context inference \cite{mader2024learning}. This aligns with findings from other datasets, such as those studied by Zhang et al. \cite{zhang2024reproducible}, which further validate the value of hybrid models. Hybrid models, which combine general population data with user-specific features, improve accuracy by balancing broad patterns with individual nuances. Fully personalized models can yield the highest accuracy but require extensive user-specific data, making them resource-intensive and less scalable. %\cite{bangamuarachchi2022sensing}.
Hybrid models offer a practical compromise, delivering strong performance with fewer data demands per user, better accommodating individual behavioral variations.

For example, in mood detection, hybrid models incorporating individual app usage and movement data can capture unique variations, improving accuracy compared to purely generic models. This hybrid approach demonstrates that while general models detect broader trends, they often miss critical individual subtleties that partially personalized models can capture, providing a practical strategy for improving model relevance and precision.



\subsubsection{Advancing Techniques in Multimodal Integration for Mobile Sensing}

Integrating diverse data streams—such as activity, location, Bluetooth, app usage, device use, and WiFi— enhances behavioral inference by capturing a broader spectrum of contextual and activity patterns. Existing research using \dataset has largely relied on foundational multimodal techniques, often without leveraging advanced integration methods tailored to the high-dimensional, diverse data sources typical in mobile sensing. A recent study applying an unsupervised domain adaptation model, M3BAT \cite{meegahapola2024m3bat}, explored multimodal settings, demonstrating that domain adaptation can help mitigate distributional shifts across modalities, achieving up to a 12\% improvement in AUC on specific classification tasks. However, much remains to be done to develop models capable of handling the complexities of multimodal data in mobile sensing, particularly when accounting for regional, cultural, and individual behaviors.

Smartphone sensor data is shaped by regional routines, cultural practices, and individual habits, requiring models to adapt dynamically to context-dependent patterns across data streams. Temporal transformers and adversarial training approaches tailored to multimodal data offer promising directions, though these are not yet widely applied in mobile sensing. Future research could benefit from multimodal architectures that use independent branches for each sensor modality, allowing each data type to be processed in context while adapting to shifting behavioral patterns. The next generation of multimodal models in mobile sensing should aim to improve upon initial efforts by adopting advanced architectures that facilitate robust, context-aware fusion of diverse data sources. Such models would enable richer, more generalized applications, supporting domains like health monitoring, behavior analysis, and activity recognition across diverse populations.


\subsubsection{The Challenge of Distribution Shift}

In cross-country smartphone sensing, distribution shifts in data impact the performance of models trained in one region when applied in another. Recent studies highlight that sensor data distributions vary widely between countries, introducing substantial challenges for model generalization. For instance, Meegahapola et al. \cite{meegahapola2024m3bat} observed high Cohen’s-d values, up to 1.0, across various sensing modalities such as location and step count. These large effect sizes reveal pronounced shifts in data distributions that hinder a model's ability to transfer effectively across geographic contexts. For example, GPS patterns that indicate social activities in one country may reflect commuting in another, necessitating region-specific adaptations to accurately interpret similar data.

While substantial progress has been made in addressing distribution shifts in computer vision and natural language processing \cite{wang2018deep, ge2023domain}, domain adaptation in multimodal mobile sensing is still an emerging field \cite{chang2020systematic, meegahapola2024m3bat, wu2023udama}. As mentioned earlier, the M3BAT framework by Meegahapola et al. employed a multi-branch adversarial training approach to tackle distribution shifts in multimodal sensor data, achieving performance gains of up to 12\% in AUC for classification tasks and a 0.13 MAE reduction in regression tasks. By creating separate branches for features with varying distribution shifts, M3BAT tailored adaptation strategies to each modality, enhancing model robustness. Moreover, Wu et al. \cite{wu2023udama} also applied domain adaptation for multimodal data, however, without considering the multimodality. Xu et al. \cite{xu2023globem} used a domain generalization algorithm, again without considering the multimodality. However, all the above methods are still nascent, addressing only a subset of multimodal data shifts. %In comparison, computer vision models, such as CNNs and Vision Transformers, excel at capturing universal features in images—such as edges and textures—that allow for robust generalization across diverse image datasets. This generalization benefits from image data's more universal features, unlike the highly localized behavioral and contextual nuances in time-series smartphone data. For smartphone sensing, further research into specialized adaptation techniques is essential to manage the nuanced regional variations in multimodal time-series data.

%htbp
\begin{table}[t]
    \centering
    \caption{Summary Recommendations for Future Research Based on the Dataset Analysis.}
    \label{tab:analysis_recommendations}
    \begin{tabular}{p{0.02\linewidth} p{0.85\linewidth}}
        \toprule
        %\textbf{} & \textbf{} \\
        %\midrule

        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#8: Country-Specific vs.\ Multi-Country Models}}} \\
        
        & 
        Country-specific or regional models could be better for initial deployment in diverse regions, as compared to multi-country models. Tailoring models to capture unique behavioral patterns and cultural nuances in countries enhances accuracy without full personalization. \\

        \arrayrulecolor{Gray}
        \midrule
        
        %\textbf{Hybrid Model Personalization} 
        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#9: Hybrid Model Personalization}}} \\
        
        & 
        Adopt a hybrid strategy for model personalization by combining general population data with user-specific inputs to capture both local and individual behavioral nuances. This approach achieves a balance between broad generalization and user-specific relevance, enhancing model performance while managing data demands effectively. \\

        \arrayrulecolor{Gray}
        \midrule
        
        %\textbf{Advanced Multimodal Integration}
        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#10: Advanced Multimodal Integration}}} \\
        
        & 
        Develop advanced multimodal integration techniques for behavioral models in mobile sensing. Multi-branch architectures may provide an effective way forward by facilitating context-aware fusion of diverse data sources. \\

        \arrayrulecolor{Gray}
        \midrule
        
        %\textbf{Addressing Distribution Shifts}
        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#11: Addressing Distribution Shifts}}} \\
        & 
        Implement domain adaptation techniques that account for cross-country
distribution shifts in mobile sensing. Consider using multi-branch adversarial training to tailor model responses to region-specific patterns, improving generalization across diverse data distributions. \\

        \arrayrulecolor{Gray}
        \midrule

        %\textbf{Label Quality}
        \multicolumn{2}{l}{\textbf{\textsc{Recommendation \#12: Label Quality}}} \\
        & 
        Prioritize objective labels for tasks where label quality critically impacts model performance. For subjective data, explore advanced preprocessing techniques such as outlier regularization, attention mechanisms, and weighted averaging to reduce noise, thereby improving label consistency and model accuracy. \\

        \arrayrulecolor{black}
        \bottomrule
    \end{tabular}
\end{table}


\subsubsection{Effect of Label Quality on Model Performance}

The quality of labels has been shown to profoundly influence model performance after transer learning, especially in mobile sensing applications where labels can vary widely in terms of subjectivity \cite{zeni2019fixing,bontempelli2020learning,wu2023udama}. A study using \dataset \cite{meegahapola2024m3bat} revealed that models trained on subjective labels, such as self-reported mood, tend to exhibit lower consistency and reliability compared to models trained on more objective labels, such as social context. Although both labels are silver standard \footnote{\change{In machine learning, the term “silver-standard” refers to labels or annotations that, due to uncertainty, bias, or other sources of noise, offer lower accuracy or reliability than “gold-standard” labels. Despite these limitations, silver-standard labels remain valuable and are commonly employed in inference tasks. They may be derived from a variety of less rigorous methods, including automated algorithms, self-reports, surrogate measures, or indirect observations. \cite{wu2023udama, meegahapola2024m3bat, dy2023domain}}} because they are self-reported, social context is less subjective and mood is more subjective, leading to various behaviors for models. This difference suggests that, where possible, researchers should aim to use objective and gold standard labels \footnote{\change{In machine learning, “gold-standard” refers to the most accurate and reliable ground truth labels or annotations. They are often derived from highly trusted methods—such as expert manual annotations, precise measurements, or comprehensive and well-established criteria. However, producing these gold-standard labels can be challenging, time-consuming, and costly, as it typically requires specialized expertise, rigorous data collection, and meticulous validation \cite{wu2023udama, meegahapola2024m3bat, dy2023domain}.}} to enhance model reliability and accuracy. However, for tasks that inherently rely on subjective input—like mood, depression, stress or perceived energy levels---it is crucial to develop preprocessing methods that can transform subjective data into more standardized, reliable labels. Techniques such as regularizing outliers, employing weighted averages for conflicting reports, or integrating machine learning methods like attention mechanisms to focus on higher confidence data points can help mitigate the impact of noise in subjective labels.

By integrating these lessons with advanced modeling techniques, researchers can enhance the robustness and scalability of mobile sensing applications in diverse cultural and geographic contexts. These recommendations aim to guide future work toward more accurate, context-aware models, ultimately supporting broader real-world adoption in areas such as health monitoring, behavior prediction, and social context inference.




\subsection{Limitations}\label{sec:limits}

Although we observed a high response rate for questionnaires at many pilot sites, participation in the intensive longitudinal survey varied between countries. For example, \UNITN and \NUM had exceptionally high participation numbers, while \AMRITA faced a notably high dropout rate, with some cases reporting completion rates below 40\%. Despite achieving strong results in certain areas of data collection during the challenges of the COVID-19 pandemic, we have identified several critical reflections on the study's progress to better contextualize the disparities observed in the collected data.

First, although the methodology included a local adaptation phase, communication challenges arose during the design and preparation stages. These issues likely stemmed from the involvement of experts outside the social sciences and/or ubiquitous computing, which may have disadvantaged those less familiar with questionnaire design or intensive longitudinal surveys \cite{helm2023diversity}. To foster a more inclusive and diversity-aware approach in future data collections involving interdisciplinary teams, it would be beneficial not only to share and adapt data collection materials but also to provide methodological guidance that informs design strategies across fields, as mentioned in \cref{subsec:lessons_design}. 

Second, it is evident that the recruitment strategy yielded varying results across pilot sites. The decision to use a uniform approach—sending an email invitation to all students at each institute—was based on methodological considerations regarding sampling strategies, data standardization, and comparability. However, cultural differences and distinct socialization processes led to divergent response rates. For example, while some sites recorded over 5,000 responses to the invitation questionnaire, others received very few. This suggests that future communication campaigns should better account for cultural diversity by utilizing a range of channels and enriching promotional materials, possibly incorporating videos, interviews, or participant testimonials to engage broader audiences.

Third, although sensor data collection remained consistent across countries, participants outside of \AMRITA submitted a high volume of sensor data meeting state-of-the-art standards (see \cite{assi2023complex}). Nevertheless, high dropout rates persisted throughout data collection, a common issue in intensive longitudinal surveys. Various factors contribute to this problem, many of which relate to incentives (discussed further below) and respondent burden. For instance, delays in providing support (especially if app-related issues arose) and communication challenges likely impacted participants’ continued engagement. Although break options were designed to alleviate respondent burden (outlined in Section \ref{subsec:ils}), these were not always utilized, perhaps due to expectations for plug-and-play apps or an overwhelming amount of materials, including privacy documentation. To address this, a single repository or website where participants could easily access all relevant information, supported by tutorials and videos, would likely improve usability and engagement, which is to be explored in future work. 

Fourth, disparities in the proposed incentives appear to have influenced data collection success in some contexts. This finding aligns with Singer’s \cite{singer2013use} assertion that fixed monetary compensation generally outperforms gifts or prize draws in motivating participation unless no incentive is provided. Therefore, it is advisable to offer fixed monetary incentives or a set of equivalent alternatives, such as university credits combined with vouchers for students. Additionally, exploring incentives such as digital badges, personalized messages, or gamification elements may be worth considering to sustain long-term engagement.

Finally, the COVID-19 pandemic presented an unforeseen circumstance that invalidated some initial design choices, complicating efforts to ensure ecological validity. In response, we adjusted the survey as needed and conducted a follow-up study at the end of 2021 to examine the pandemic's impact on student’s lives through the lens of smartphone sensors, providing the possibility of comparative insights for post-pandemic contexts in future studies, even though the later dataset is not released as part of \dataset. This second will be released soon, hopefully within 2024. \cite{girardini2023adaptation} studies the effects of Covid-19 by comparing a subset of DiversityOne, limited to UniTN, with a dataset collected earlier on, in 1918, on the same population.


















%\paragraph{Students routines before and during COVID-19 pandemic}\cite{girardini2023adaptation} One population group that had to adapt and change their behaviour during the COVID-19 pandemic is students. While previous studies have extensively investigated the impact of the pandemic on their psychological well-being and academic performance, limited attention has been given to their activity routines. In this work, we analyze students’ behavioural changes by examining qualitative and quantitative differences in their daily routines between two distinct periods (2018 and 2020). Using an Experience Sampling Method (ESM) that captures multimodal self-reported data on students’ activity, locations and sociality, we apply Non-Negative Matrix Factorization (NMF) to extract meaningful behavioural components, and quantify the variations in behaviour between students in 2018 and 2020. Surprisingly, despite the presence of COVID-19 restrictions, we find minimal changes in the activities performed by students, and the diversity of activities also remains largely unaffected. Leveraging the richness of the data at our disposal, we discover that activities adaptation to the pandemic primarily occurred in the location and sociality dimensions.

%We built the LifeSnaps Dataset with the goal to serve multiple-purpose scientific research. In this section, we discuss indicative use cases for the data (see Fig. 8), and hope that researchers and practitioners will devise further uses for various aspects of the data.Among others, the LifeSnaps dataset includes data emerging from diverse vital signals, such as heart rate, temperature, and oxygen saturation. Such signals can be of use to medical researchers and practitioners for gen- eral health monitoring, but also to signal processing experts due to their fine granularity. When it comes to coarse granularity data, the dataset includes a plethora of physical activities, sleep sessions, and other data emerging from behaviors related to physical well-being. Such data can be exploited by researchers and practitioners within the sleep research and sports sciences domain to study how human behavior affects overall physical well-being. On top of that, the LifeSnaps dataset is a rich source of mood and affect data, both measured and self-reported, that have the potential to empower research in the domains of stress monitoring and prediction, and overall mental well-being. Additionally, the diverse modalities of the dataset allow for exploring the correlation between objectively measured and self-reported mental and physical health data, while the psychological and behav- ioral scales distributed can facilitate research in the behavioral sciences domain. On a different note, incentive schemes, such as badges, and their effect on user behavior, user compliance, and engagement with certain system features could be of great interest to human-computer interaction experts. Finally, handling such sensitive data can fuel discussions and efforts towards privacy preservation research and data valorization.