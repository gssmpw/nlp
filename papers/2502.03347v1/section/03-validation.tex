Our dataset provides a rich and diverse array of information, making it possible to extract valuable insights into students' behavior across different pilot sites. Below, we present key descriptive statistics\footnote{\change{The statistics are compatible with those reported in previous studies analyzing the dataset, as they applied data filtering.}} and focus on crucial dimensions that highlight study participation and the diversity observed in the students' everyday lives.

\subsection{Overall Study Engagement}

\begin{table}[tb]
    \centering
    \caption{\label{tab:participants} \change{Number of participants for each of the three questionnaires and data collection. ``iLog signed'' and ``iLog data'' columns show the total number of students who logged into the iLog app and those who actively contributed with data, respectively. Additionally, demographic information of the participants involved in the iLog data collection is reported.}}
    \input{tables/country_stats}
\end{table}

Study participation is described according to the protocol outlined in \cref{subsec:protocol}.
As shown in \cref{tab:participants}, during the invitation phase, 18,628 students responded to the initial questionnaire, with peak participation at the \UNITN, \AMRITA, and \NUM pilot sites, each registering over 3,000 responses. Most sites saw more than 1,000 students participate, except for \IPICYT and \AAU. The lower participation at these sites is likely due to the recruitment strategies used at \IPICYT and the communication channels adopted at \AAU. In the first case, participants may not have encouraged their acquaintances to take part in the survey, as in the usual snowball sampling procedures. In contrast, in the second case, students might not have been used to checking their emails or visiting the institutional website, so they had not received the communications.

After the invitation phase, up to 350 students from each site were randomly selected for participation in the iLog data collection. This selection process considered factors such as gender and the students' enrolled departments to ensure a representative sample. This approach aimed to address potential biases in the data that might arise from differences in lesson schedules across departments, which could influence students' daily behavior.

Among the invited participants, \nilogusers students installed the iLog app. \cref{tab:participants} details the number of students who signed into the app and those who actively provided data. The number of participants varies across universities, with the highest participation rates observed at \NUM and \UNITN and the lowest at \AAU and \IPICYT. These differences are also evident in the second and third questionnaires, distributed exclusively to previously selected participants. The uneven participation across pilot sites could be due to several factors, such as smartphone compatibility with the iLog app, individual inclinations and preferences, and variations in the research protocols and engagement methods (detailed in \cref{sec:limits}). However, the dataset remains consistent and suitable for high-quality analysis, as shown in \cref{sec:discussion}, presenting a unique opportunity for methodological exploration and analysis. These analyses can examine participant profiles in relation to survey participation, as well as response rates and response times (as outlined below), offering valuable insights for optimizing future data collection.

Having introduced participant engagement at various stages, \cref{fig:val:td:answers} and \cref{fig:val:dropout} illustrate the response rates and sensor data provision during the intensive longitudinal survey, as discussed in the following sections.

\begin{figure}[t]
\begin{minipage}[c]{0.47\linewidth}
\includegraphics[width=\textwidth]{figures/validation/td/td_all_v3_2x.png}
 \caption{Distribution of participants based on the number of daily diaries completed. Each dot represents a participant.}
 \Description{Box plot showing the participants distribution for each university based on the number of answered time diaries. The mean value is around 1200 in \JLU, 1000 in \AAU, 100 in \AMRITA, 1100 in \UNITN, 1500 in \IPICYT, 800 in \NUM, 400 in \uc and 500 in \LSE.}
 \label{fig:val:td:answers}
\end{minipage}
\hfill
\begin{minipage}[c]{0.47\linewidth}
\includegraphics[width=\linewidth]{figures/validation/sensors/dropoutv2_2x.png}
\Description{Line plot showing for each university the decreasing percentage of users providing data over one month of experiment. The x-axis reports time, and the y-axis the percentage.}
\caption{Percentage of participants at each pilot site providing sensor data for each day.}
\label{fig:val:dropout}
\end{minipage}%
\end{figure}

\subsection{Response Rate in Time Diaries} \label{subsec:dailyresponses}

\cref{fig:val:td:answers} shows the number of responses to the daily questions outlined in \cref{sec:td}. We expected approximately 1400 responses per participant throughout the study, including morning and evening questions, time diaries, and snack questions. In all pilots (except \AMRITA), 40\% of participants provided an average of 20 responses daily, equating to roughly ten hours of annotated sensor data each day. Notably, the daily response rate is significantly higher at \UNITN and \NUM, where over 150 and 130 participants, respectively, provided more than 20 responses per day. \JLU and \IPICYT also had high participation rates; however, the latter's increased response rate was partly due to the study configuration (see \cref{subsec:ils}), which included more questions than other pilot sites.

Variations in response rates can be attributed to several factors, including adaptations to the survey protocol mentioned above, as well as technical issues and participant characteristics. Some participants did not receive all notifications correctly due to smartphone settings, connectivity issues, or server management. While these technical problems affected only a small fraction of the data, they may have impacted participant engagement.

Furthermore, the data collection process still required the participants to be considerably careful and organized, despite measures such as break options and maintaining active notifications for 12 hours (see \cref{subsec:ils}). Personal factors, such as interest and curiosity (like the appeal of tracking one’s activities to increase self-awareness), as well as tendencies toward procrastination or stress, may have influenced response consistency.

Ultimately, each student’s consistency of participation mainly varied due to dropout effects. While some students ceased involvement after a few days, others maintained consistent engagement throughout the data collection period.

\begin{figure}[hbt]
    \includegraphics[width=0.95\textwidth]{figures/validation/td/userPercentagesv7_2x.png}
    \Description{One horizontal barplot for each sensor, each bar representing the percentage of participants with data in one university.}
    \caption{Average percentage of participants that provided sensor data for each site.}
    \label{fig:sensors:participantstats}
\end{figure}

\subsection{Sensor Data Provision}

Sensor data collection adhered to state-of-the-art methods, ensuring that studies based on this dataset can achieve high replicability. We also present details on dropout rates in \cref{fig:val:dropout}, along with the percentage of participants who provided data compared to the total participants with at least one recorded event, shown in \cref{fig:sensors:participantstats}\footnote{A comprehensive description of the sensors and their collection frequency is available in \cref{tab:sensor-list,tab:sensor}.}.

\cref{fig:val:dropout} illustrates the percentage of participants providing sensor data over time with respect to the total number of participants who contributed data. The most notable decrease occurs at \AMRITA, where participation drops from 80\% on the first day of data collection to less than 25\% by the final day. On average, the dropout rate across sites is approximately 35\%. The initial increase observed at \JLU and \IPICYT results from participants who joined the data collection after the start date.

For each sensor, \cref{fig:sensors:participantstats} shows the percentage of participants who contributed in relation to all participants with at least one recorded event (see also the “iLog data” column in \cref{tab:participants}). A lower percentage does not necessarily indicate low data quality. For instance, the number of participants reporting data from on-change sensors, such as airplane mode or music playback, is influenced by the actual usage of these smartphone features. Sensors collected at fixed intervals, like the accelerometer and application usage, tend to have a higher number of participants with data. In contrast, sensors like pressure capture data from a smaller subset of participants.

\noindent
In addition to dropouts, variations in sensor data provision may be attributed to application or device issues and the lack of support for specific hardware sensors on some devices. Participants could also disable any sensor directly from the app at any time for privacy considerations. For instance, a participant could disable GPS tracking if visiting a location they preferred to keep private (e.g., a place of worship or a hospital). Additionally, some sensors may have been disabled to conserve battery life. Together, these factors contribute to the observed instances of missing data. Hence, although this privacy feature resulted in some missing data, we believe this design significantly contributed to attracting a higher number of participants who felt comfortable providing data over an entire month.


\subsection{Diversity in Everyday Life}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/validation/td/td_answer_hour_sleepingv2_2x.png}
    \caption{Sleeping}
    \label{fig:val:td:disthours:sleeping}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/validation/td/td_answer_hour_eatingv2_2x.png}
    \caption{Eating}
    \label{fig:val:td:disthours:eating}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/validation/td/td_answer_hour_studyv2_2x.png}
    \caption{Studying}
    \label{fig:val:td:disthours:studying}
    \end{subfigure}
    \Description{Line plot for each university showing the percentage of participants reporting the sleeping, eating and studying activity over the day. The x-axis is time and the y-axis the percentage.}
    \caption{Proportion of time diary reports during different hours of the day.}
    \label{fig:val:td:disthours}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/validation/td/td_food_rankingv2_2x.png}
        \caption{Foods}
        \label{fig:val:td:food:food}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/validation/td/snack_rankingv2_2x.png}
        \caption{Snacks}
        \label{fig:val:td:food:snack}
    \end{subfigure}
    \Description{The two plots show the ranking position of each food and snack in each university. X-axis represents universities, y-axis the ranking position.}
    \caption{Ranking comparison of the three most consumed foods and snacks.}
    \label{fig:val:td:food}
\end{figure}

%In this section, we illustrate the diversity of students' everyday lives across various dimensions, as collected through daily diaries and sensor data.

To highlight behavioral variations, we analyze the distribution of primary daily activities—sleeping, eating, and studying—across different pilot sites. For instance, \cref{fig:val:td:disthours} displays these differences. Notably, \cref{fig:val:td:disthours:sleeping} shows distinct sleeping patterns between midnight and 7:00 AM, with many students from \JLU resting in the afternoon, despite similar waking times to other pilot sites. Comparing these patterns with study hours (\cref{fig:val:td:disthours:studying}) reveals that study-related annotations account for over 30\% of activities at 8:00 PM in \JLU, compared to just 7\% in \IPICYT. It is also worth noting that sleep-related annotations were primarily captured using an app feature that allowed participants to define sleep time windows for several hours before returning to regular self-reporting in the morning.

The timing of eating activities also varies among universities. As shown in Figure \ref{fig:val:td:disthours:eating}, \AMRITA reports eating spikes around 8:00 AM and 10:00 PM, while \NUM peaks around 11:00 AM and \UC around 6:00 PM. Differences in eating behaviors are further highlighted in Figure \ref{fig:val:td:food}, which ranks the top three foods and drinks reported at each site. For example, water is the most consumed drink at \UNITN, while it ranks only 16th at \NUM and \JLU (refer to Figure \ref{fig:val:td:food:food}). Interestingly, water consistently appears among the top-reported drinks during snack breaks across all sites, as illustrated in Figure \ref{fig:val:td:food:snack}.

Mood reporting adds an intriguing layer to this study, allowing researchers to investigate how mood ratings vary in similar contexts across different universities (what was captured is, actually, a facet of mood called Valence). This has been discussed in detail by Meegahapola et al. \cite{meegahapola2023generalization}, who used this dataset for one of their studies). Figure \ref{fig:mood_location:hourdist} illustrates changes in positive mood over the day. While trends appear similar across locations, absolute percentages vary, with \UC reporting the lowest mood levels and \AMRITA the highest. Participants’ personal rating criteria also influence these differences. Furthermore, mood differences are noticeable depending on location: distinct distributions of mood ratings are evident when participants are at home (e.g., apartments, gardens, or relatives' homes) compared to university spaces (e.g., classrooms and libraries), as shown in \cref{fig:mood_location:home} and \cref{fig:mood_location:uni}.

Research questions regarding phone usage and time spent on activities such as internet browsing, chatting, or gaming can be examined by analyzing application usage. \cref{tab:application} ranks applications based on the number of participants using them. As expected, social networking and messaging apps are the most common across universities, with \JLU students, for instance, favoring local applications such as WeChat.

This section offers only an initial glimpse into the potential analyses and research questions that can be pursued by combining and reshaping the data. A more comprehensive discussion can be found in \cref{sec:discussion}.

\begin{table}
\centering
\small
\caption{Ranking of each institution's five most used mobile applications.}\label{tab:application}
\begin{tabular}{llllll}
\toprule
 & \textbf{1st} & \textbf{2nd} & \textbf{3rd} & \textbf{4th} & \textbf{5th} \\
\midrule
\JLU & WeChat & QQ - Tencent & Taobao & Bilibili & Google Messages \\
\AAU & Youtube & Facebook Messenger & Spotify & Google Chrome & Google Maps \\
\AMRITA & Android settings & Whatsapp & Google Chrome & Android settings & Youtube \\
\UNITN & Whatsapp & Google Gmail & Youtube & Google Chrome & Android settings \\
\IPICYT & Facebook & Whatsapp & Google quick search & Google Chrome & Android settings \\
\NUM & Facebook Messenger & Facebook & Youtube & Android settings & Google Play Store \\
\UC & Google Chrome & Whatsapp & Instagram & Android settings & Youtube \\
\LSE & Whatsapp & Google Chrome & Android settings & Google Play Store & Google Gmail \\
\bottomrule
\end{tabular}
\end{table}

\iffalse
\begin{table}[tb]
    % from Matteo's thesis
    \small
    \caption{Average percentage of participants that provided sensors data for each country. In red, $\le 35\%$.}
    \label{tab:sensor:stats}
    \input{tables/sensor_stats}
\end{table}
\fi


\begin{figure}
    \centering
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\linewidth]{figures/validation/td/td_answer_hour_moodv2_2x.png}
        \Description{Line plot for each university. The X-axis is the percentage of participants reporting positive mood, and the y-axis is the hours of the day.}
        \caption{Percentage of positive mood reports over the day for each site.}
        \label{fig:mood_location:hourdist}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\linewidth]{figures/validation/td/td_mood_distribution_country_homev2_2x.png}
        \Description{Stacked bar plot reporting the overall mood distribution when the participant is at home, for each university.}
        \caption{Mood distribution when the participants are at home.}
        \label{fig:mood_location:home}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \includegraphics[width=\linewidth]{figures/validation/td/td_mood_distribution_country_univ2_2x.png}
        \Description{Stacked bar plot reporting the overall mood distribution when the participant is at the university, for each university.}
        \caption{Mood distribution when the participants are at the university.}
        \label{fig:mood_location:uni}
    \end{subfigure}
    \caption{Mood ratings distribution.}
    \label{fig:mood_location}
\end{figure}
