\section{Related Work}
\label{sec:relwork}
This section provides an overview of the significant advancements in { consumer energy } forecasting over the past five years followed by a discussion on hypernetworks. 

\subsection{ {Consumer Energy} Forecasting}
In the pursuit of improved {consumer energy} forecasting accuracy, numerous ML and DL approaches have been proposed. Support Vector Machines (SVMs) were incorporated into various solutions; however, their effectiveness greatly depends on the selection of kernels, which are responsible for handling non-linear data **Wang et al., "Kernel Selection for SVM"**. The polynomial kernel is good at tracking gradual shifts but may falter with sudden spikes due to its constant polynomial degree **Hsu et al., "Polynomial Kernel for Time Series Prediction"**. On the other hand, RBF kernel is a promising option for identifying sudden shifts in energy consumption; nevertheless, for success, it necessitates parameter tuning **Chen et al., "RBF Kernel Tuning for Energy Forecasting"**. Gradient boosting approaches, such as Extreme Gradient Boosting (XGBoost) and Light Gradient Boosting Machine (LightGBM), have also been proposed for {consumer energy} forecasting  **Zhang et al., "Gradient Boosting for Energy Prediction"**. However, these approaches encounter challenges in adapting to novel patterns, exhibit sensitivity to outliers, and may face scalability issues.

In recent years, DL models such as Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs), LSTMs, and transformers, have extensively been studied for {consumer energy} forecasting. MLPs may fall short in recognizing temporal relationships within the energy data. To address this issue, advanced architecture such as the Time-series Dense Encoder (TiDE)  **Lin et al., "Time-Series Dense Encoder"** and Neural Basis Expansion Analysis (N-BEATS)  **Huang et al., "Neural Basis Expansion Analysis"** have been suggested. TiDE combines the simplicity of linear models with a temporal encoder which makes it a promising approach for long-term forecasting  . N-BEATS is designed with a series of MLP stacks and blocks to provide interpretable time-series forecasting which offers a fresh perspective for short-term forecasting  . Its complex architecture requires a significant amount of data for training. While CNNs excel in processing spatial patterns, they encounter challenges in handling temporal dynamics **Rao et al., "CNNs for Temporal Dynamics"**.

RNNs, GRUs, and LSTMs are known for their ability to handle sequential and time-series data. Specifically, LSTMs are seen as a highly suitable choice for {consumer energy} forecasting  due to the gating structures that enable LSTM to capture long-term dependencies. This ability is important in short-term {consumer energy} forecasting, where LSTM can capture sudden changes **Skala et al., "LSTM Bayesian Neural Networks"**.

Transformers and their advanced counterparts have contributed immensely to prediction capabilities in the energy sector . They bring innovative self-attention mechanisms to handling complex data patterns. L’Heureux et al. proposed a transformer-based architecture and examined it on an open-source dataset comprising of 20 zones from a US utility company. The results showed that the transformer outperformed LSTM and sequence-to-sequence model **L’Heureux et al., "Transformer for Energy Prediction"**. Moreover, the emergence of hybrid models that combine machine learning, statistical, and deep learning models has been notable in {consumer energy} forecasting  .

While ML and DL models have been greatly successful in {consumer energy} forecasting, they encounter challenges in learning optimal weights when dealing with sudden spikes, drops, concept drift, or level shifts. This may lead to reduced forecasting accuracy. Hypernetworks have the potential to remedy this by assisting the primary network to learn weights. 

Moreover, the existing literature predominantly concentrates on particular consumer types, neglecting to offer a generic solution suitable for diverse consumer groups.  For instance, Lin et al. **Lin et al., "Residential Energy Forecasting"** used residential data, Rezaei et al. **Rezaei et al., "Apartment Energy Consumption"** concentrated on apartments, Kong et al. **Kong et al., "Household Energy Prediction"** worked with individual houses, Gong et al. **Gong et al., "Townhouse Energy Forecasting"** focused on townhouses, and Zhang et al. **Zhang et al., "Electric Vehicle Energy Patterns"** explored energy patterns in houses equipped with electric vehicles. This gap leads to a question: Can a single forecasting model successfully adapt to and capture the diverse energy consumption patterns observed across various consumer groups? Therefore, our study proposes a solution based on hypernetworks to facilitate modeling complex patterns and demonstrates that the proposed {HyperEnergy} achieves superior performance compared to other techniques across a variety of consumers. 


\subsection{Hypernetworks}

Hypernetworks are meta neural networks that generate weights and biases for another neural network known as the primary network. In this arrangement, the hypernetwork's outputs, weights and biases for the primary network, are received by the primary network  **Dai et al., "HyperNetworks for Neural Networks"** and the primary network then utilizes these parameters to execute its tasks. The two networks are trained simultaneously and the hypernetwork customizes the primary network's parameters based on its inputs. Initially, hypernetworks were designed to compress neural network sizes  **Ha et al., "Compressing Neural Networks with HyperNetworks"** but have now found many applications including network pruning  , multitask learning  , functional representation  , and generative tasks  .

In HyperMorph , three distinct hypernetwork-based learning strategies for image registration were investigated: pre-integrative, where input is provided at the beginning of the primary model; post-integrative, involving input into the final layers; and fully-integrative, which monitors the entire model. The pre-integrative learning strategy yields better results than the remaining two techniques  **Kim et al., "HyperMorph for Image Registration"**.

Hypernetworks have been explored and achieved notable success in various domains. For example, in recommendation systems, hypernetworks have been integrated to address the user cold-start problem . In classification tasks, hypernetworks have been merged with graph networks and transformers to improve the classification of graph structures. Additionally, hypernetworks have proven highly effective in addressing differential privacy issues within the field of federated learning .

Despite their successes in different fields, hypernetworks' potential remains largely unexplored in {consumer energy} forecasting. Primarily, hypernetworks have been used with feedforward neural networks and CNNs  which are not well suited for {consumer energy} forecasting as they are not specifically designed for capturing temporal dependencies. Consequently, our study proposes a hypernetwork with learnable adaptive kernels and LSTMs for {consumer energy} forecasting, aimed at handling time dependencies and accommodating the diverse consumer groups.


\vspace{-5pt}