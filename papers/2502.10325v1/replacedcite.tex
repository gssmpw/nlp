\section{Related Work}
\vspace{-1em}

\textbf{Fine-tuning agents.} 
Most of the work on LLM agents rely on prompting LLMs, e.g. ReAct____, Reflexion____, AdaPlanner____. However, prompting alone is insufficient to correct errors encountered at test-time____. A simple way to improve LLMs is to fine-tune on successful trajectories generated manually or via a prompted LLM____. However, manually collecting demonstrations of reason and actions is challenging and hard to scale. 

Recent work \texttt{LEAP} has looked at leveraging privileged AI feedback____ to design critics that distill the information into student agents, showing strong performance in text-based games, web navigation and interactive coding. However, the privileged correction in \texttt{LEAP} can be unrealizable for the agent, leading to poor success rates. Hence, we look at training agents directly using RL to maximize the outcome reward. 

Finally, ARCHER____ proposes a very similar framework to train LLM agents using hierarchical RL. The Q-value is trained using temporal difference, while the policy is trained using REINFORCE. However, the results are limited to small models (GPT2). We simplify the framework so it connects with existing RLHF pipelines, do RL with Llama 3B models, propose novel algorithms like \inverseprm, and provide practical recipes like using reset distribution and reward shaping to improve efficiency. 

\textbf{Process Reward Models.} 
PRMs have mostly been looked at in the context of multi-stage math reasoning problems____, where they were trained in human annotation data to provide fine-grained supervision____. Recent works look at automatically computing PRMs as Q value estimates____. PRMs have been used to train generators____ and used for test-time scaling with beam search____, heuristic search____ or tree search____. 

There are interesting similarities and differences between PRMs used for math reasoning and the agent setting we look at here. Many works____ report small gains from optimizing PRMs rather than the outcome reward. In contrast, we see pretty strong gains with PRMs, where outcome reward is infeasible given long-horizons and limited access to the external environment. Some works have noted the reward-hacking / value-estimation issues with PRMs that we also analyze in Sec.~\ref{sec:agent_prm:exp}. To counter such issues, recent works____ propose reward shaping PRMs using reference policies, which we also explore in Sec.~\ref{sec:challenge:prm}.

% SCORE

% Rewarding progress

\vspace{-1em}