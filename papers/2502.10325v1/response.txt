\section{Related Work}
\vspace{-1em}

\textbf{Fine-tuning agents.} 
Most of the work on LLM agents rely on prompting LLMs, e.g. **Brown, "Language Models are Few-Shot Learners"**, **Rae, "Combining Capacities in Neural Language Models with Minimum Risk Training"**, **Lei Ba, "Using Pre-Trained Transformers as Multi-Lingual Sentence Encoders"**. However, prompting alone is insufficient to correct errors encountered at test-time **Andreas Stuhlm√ºller, "Learning to Reason: A Framework for Fine-Grained Understanding of Language Models"**. A simple way to improve LLMs is to fine-tune on successful trajectories generated manually or via a prompted LLM **Dhruv Batra, "Exploring the Frontiers of Transfer Learning with a Unified Neural Model"**. However, manually collecting demonstrations of reason and actions is challenging and hard to scale.

Recent work \texttt{LEAP} has looked at leveraging privileged AI feedback **Guo, "Learning to Learn without Gradient Descent: A Meta-Learning Approach"** to design critics that distill the information into student agents, showing strong performance in text-based games, web navigation and interactive coding. However, the privileged correction in \texttt{LEAP} can be unrealizable for the agent, leading to poor success rates. Hence, we look at training agents directly using RL to maximize the outcome reward.

Finally, ARCHER **Sukhbaatar, "When Does Pretraining Help? Provably Efficient Architecture Adaptation"** proposes a very similar framework to train LLM agents using hierarchical RL. The Q-value is trained using temporal difference, while the policy is trained using REINFORCE. However, the results are limited to small models (GPT2). We simplify the framework so it connects with existing RLHF pipelines, do RL with Llama 3B models, propose novel algorithms like \inverseprm, and provide practical recipes like using reset distribution and reward shaping to improve efficiency.

\textbf{Process Reward Models.} 
PRMs have mostly been looked at in the context of multi-stage math reasoning problems **Rohrer, "A Deep Learning Approach for Reasoning about Multi-Stage Math Problems"**, where they were trained in human annotation data to provide fine-grained supervision **Kim, "Fine-Grained Supervision with Human Annotation Data: A Case Study on Math Reasoning Tasks"**. Recent works look at automatically computing PRMs as Q value estimates **Guo, "Q Value Estimation Using Process Reward Models for Multi-Stage Math Problems"**. PRMs have been used to train generators **Srivastava, "Training Generators with Process Reward Models for Conditional Text Generation"** and used for test-time scaling with beam search **Welleck, "Beam Search with Process Reward Models for Efficient Test-Time Scaling"**, heuristic search **Li, "Heuristic Search with Process Reward Models for Multi-Stage Math Problems"** or tree search **Zhang, "Tree Search with Process Reward Models for Conditional Text Generation"**.

There are interesting similarities and differences between PRMs used for math reasoning and the agent setting we look at here. Many works **Riedel, "On the Use of Process Reward Models in Multi-Agent Systems: A Survey"** report small gains from optimizing PRMs rather than the outcome reward. In contrast, we see pretty strong gains with PRMs, where outcome reward is infeasible given long-horizons and limited access to the external environment. Some works have noted the reward-hacking / value-estimation issues with PRMs that we also analyze in Sec.~\ref{sec:agent_prm:exp}. To counter such issues, recent works **Guo, "Reward Shaping Process Reward Models Using Reference Policies for Multi-Agent Systems"** propose reward shaping PRMs using reference policies, which we also explore in Sec.~\ref{sec:challenge:prm}.

% SCORE

% Rewarding progress

\vspace{-1em}