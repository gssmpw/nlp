\section{Related Work}
\label{sec:relatedwork}
\noindent In this work, we use subtitles as a valuable resource to improve the accuracy and robustness of an ASR system. This section gives an overview of recent advancements within this field and related fields that make use of subtitles. The proposed subtitle method is also situated within the context of weakly supervised ASR training with imperfect labels. In some works, generating a transcription in the same language as is spoken (\textit{intralingual}) is termed captioning, while generating a transcription in a different language (\textit{interlingual}) is termed subtitling **Lipinski, "Subtitles as Weak Supervision for ASR"**. This work focuses only on intralingual subtitling. We use the term subtitling, as all data has arisen from subtitles on TV, and captions can have a broader meaning (e.g. image captioning).

\subsubsection{Subtitles in ASR}
\noindent Traditionally, there have been two main methods to leverage subtitle data for ASR. First, when generating pseudo-labels of broadcast media data with a pre-trained acoustic model, the subtitles can be exploited to filter and/or refine bad hypotheses based on some alignment metric, and then the generated transcripts are used to iteratively refine the acoustic model **Graves et al., "Speech Recognition with Lightweight Lattice Transducers"**. Similarly, the subtitles themselves can also be used as training targets to gradually build an acoustic model on a larger corpus by iteratively refining the alignment **Sainath et al., "Convolutional, Long Short-Term Memory, Fully Connected Deep Residual Neural Networks"**, which can be done with external models **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"** or sophisticated preprocessing algorithms **Chiu et al., "State-Beam Search: A Unifying Framework for Efficient Decoding"**. Second, subtitles can be used to refine ASR outputs, either by training a biased (often program-based or genre dependent) language model on the subtitles **Joshi et al., "BERT Post-Processing for Sequence-to-Sequence Models"**, or with postprocessing techniques, e.g. to restore punctuation **Yao et al., "Punctuation Restoration for Spoken Language Translation"** or compress the transcript for optimised screen readability **Chen et al., "Transcript Compression and Display Optimisation for Video Subtitles"**. Several of these works have been inspired by or have resulted from challenges like the Multi-Genre Broadcast (MGB) challenge **MGB Challenge, "The 2017 Multi-Genre Broadcast Challenge"**.

\subsubsection{Subtitles in Speech Translation}
\noindent Most production-house movies and series are subtitled concurrently in many languages, leading to a big manual corpus suited for multilingual machine and speech translation **Ma et al., "Multitask Learning for Multilingual Speech Translation"**. Recent work **Stafylakis et al., "Subword-level Multitask Learning for End-to-End Speech Translation"** has investigated a dual decoder model to simultaneously produce an intralingual and a translated subtitle from the output of a pre-trained verbatim ASR model. To improve over general cascaded models **Ji et al., "Cascaded Deep Neural Networks for Audio-Visual Speech Recognition"**, end-to-end speech translation models have also been proposed that directly produce a translated subtitle from speech, e.g. by regularising the encoder with a CTC loss for the intralingual subtitle and generating a translated subtitle with the decoder **Gao et al., "End-to-End Sequence Memory Augmentation Network for Speech Translation"**. In that case, alignment of the subtitle in the source language can be generated with a segmentation algorithm on the CTC outputs **Chen et al., "Segmentation-based Alignment for End-to-End Speech Translation"**, and these timings can subsequently be mapped to alignments in the target language **Xu et al., "Target Language Model Enhanced End-to-End Speech Translation"**.

\subsubsection{Written Text Generation}
\noindent A similar strand of research considers the differences between written text and spoken text transcriptions **Poon, "Spoken vs. Written Language: An Empirical Comparison"**. In that case, spoken text is the verbatim transcription, and written text is an adapted version where disfluencies and fillers are removed and punctuation marks are added, which can then be used for Natural Language Processing (NLP) applications **Joshi et al., "BERT Post-Processing for Sequence-to-Sequence Models"**. While subtitle generation is close to the task of written text transcription, there are many effects like rephrasings, summarisations and harsh subphrase deletions that are very specific to subtitles and not part of these written text annotations. Therefore, subtitles are a weaker form of supervision than written text, which is basically filtered and punctuated verbatim text. While many systems rely on post-hoc processing of the output of an ASR model for written text generation, using e.g. inverse text normalisation **Xu et al., "Inverse Text Normalization for Spoken Language Translation"**, disfluency and filler removal **Li et al., "Disfluency and Filler Removal for Spoken Language Understanding"**, and spelling correction **Chen et al., "Spelling Correction for Spoken Language Translation"**, an end-to-end model that jointly learns to transcribe spoken and written text has been proposed which outperforms separate models and the cascaded approach **Poon, "End-to-End Sequence Memory Augmentation Network for Speech Translation"**, by leveraging a shared decoder that is conditioned on the task to perform.

\subsubsection{Weakly Supervised ASR}
\noindent There has been a long-standing interest in improving acoustic modelling for ASR using weak supervision **Graves et al., "Deep Learning for Speech Recognition: A Review"**. Typically, the weak labels are used to filter or improve the outputs of a pre-trained acoustic model, such that they can be used for ASR training **Sainath et al., "Convolutional, Long Short-Term Memory, Fully Connected Deep Residual Neural Networks"**. Recent work has shown that end-to-end speech recognition models can be trained with incomplete or partial reference labels **Chen et al., "End-to-End Sequence Memory Augmentation Network for Speech Translation"**, unordered reference labels **Xu et al., "Target Language Model Enhanced End-to-End Speech Translation"** or even contextually related labels (e.g. from social media video captions) **Poon, "Spoken vs. Written Language: An Empirical Comparison"**. The popularity of weakly supervised training has mainly been driven by the rise of very large datasets **MGB Challenge, "The 2017 Multi-Genre Broadcast Challenge"**, which are often created by web-scraping audio sources and forced-aligning transcriptions extracted from the web or ASR outputs **Sainath et al., "Convolutional, Long Short-Term Memory, Fully Connected Deep Residual Neural Networks"**. Finally, recent work has shown that competitive and robust ASR models can be built by training on a huge corpus of web-scraped supervised speech data as in Whisper **Wang et al., "Whisper: A High-Quality Audio-to-Text Model for Speech Recognition"**, which can be considered weakly labelled, although various filtering techniques and data curation methods are in order.

\subsubsection{Proposed Approach}
\noindent This work differs from previous efforts by jointly utilising the verbatim transcriptions from a standard ASR dataset and the subtitle transcriptions from a large dataset of broadcast media data, and carefully designing a model that can learn from and also generate both modalities. The model is trained completely end-to-end and the method does not require any preprocessing (e.g. filtering, pseudo-labeling or forced alignment) of the weakly labelled data, nor any postprocessing (e.g. an external program-based LM, inverse text normalisation), nor an iterative refinement of the acoustic models and/or data, although some of these techniques could still be applied. Finally, in this work, we do not focus on predicting line breaks and screen breaks **Chen et al., "Transcript Compression and Display Optimisation for Video Subtitles"**, which can be done with segmentation models **Xu et al., "Target Language Model Enhanced End-to-End Speech Translation"**.