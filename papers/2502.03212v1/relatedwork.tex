\section{Related Work}
\label{sec:relatedwork}
\noindent In this work, we use subtitles as a valuable resource to improve the accuracy and robustness of an ASR system. This section gives an overview of recent advancements within this field and related fields that make use of subtitles. The proposed subtitle method is also situated within the context of weakly supervised ASR training with imperfect labels. In some works, generating a transcription in the same language as is spoken (\textit{intralingual}) is termed captioning, while generating a transcription in a different language (\textit{interlingual}) is termed subtitling \cite{xu-etal-2022-joint}. This work focuses only on intralingual subtitling. We use the term subtitling, as all data has arisen from subtitles on TV, and captions can have a broader meaning (e.g. image captioning).

\subsubsection{Subtitles in ASR}
\noindent Traditionally, there have been two main methods to leverage subtitle data for ASR. First, when generating pseudo-labels of broadcast media data with a pre-trained acoustic model, the subtitles can be exploited to filter and/or refine bad hypotheses based on some alignment metric, and then the generated transcripts are used to iteratively refine the acoustic model \cite{lamel_2002, lanchantin16_interspeech, BangIEICE}. Similarly, the subtitles themselves can also be used as training targets to gradually build an acoustic model on a larger corpus by iteratively refining the alignment \cite{Ando2021ConstructionOA, reazonspeech}, which can be done with external models \cite{bell15_ASRU, Saz2018, JHU_kaldi} or sophisticated preprocessing algorithms \cite{BangIEICE}. Second, subtitles can be used to refine ASR outputs, either by training a biased (often program-based or genre dependent) language model on the subtitles \cite{lamel_2002, BangIEICE, vishwa2015}, or with postprocessing techniques, e.g. to restore punctuation \cite{Guerreiro21eswa, geislinger-etal-2022-improved, Milde2021_1109} or compress the transcript for optimised screen readability \cite{liu-etal-2020-adapting}. Several of these works have been inspired by or have resulted from challenges like the Multi-Genre Broadcast (MGB) challenge \cite{MGB1, MGB2, iberspeech}.

\subsubsection{Subtitles in Speech Translation}
\noindent Most production-house movies and series are subtitled concurrently in many languages, leading to a big manual corpus suited for multilingual machine and speech translation \cite{MustCinema}. Recent work \cite{xu-etal-2022-joint} has investigated a dual decoder model to simultaneously produce an intralingual and a translated subtitle from the output of a pre-trained verbatim ASR model. To improve over general cascaded models \cite{che2017}, end-to-end speech translation models have also been proposed that directly produce a translated subtitle from speech, e.g. by regularising the encoder with a CTC loss for the intralingual subtitle and generating a translated subtitle with the decoder \cite{papi-etal-2023-direct-speech}. In that case, alignment of the subtitle in the source language can be generated with a segmentation algorithm on the CTC outputs \cite{kurzinger2020}, and these timings can subsequently be mapped to alignments in the target language \cite{papi-etal-2023-direct-speech}.

\subsubsection{Written Text Generation}
\noindent A similar strand of research considers the differences between written text and spoken text transcriptions \cite{ihori-etal-2020-parallel, liao23, ihori23_interspeech}. In that case, spoken text is the verbatim transcription, and written text is an adapted version where disfluencies and fillers are removed and punctuation marks are added, which can then be used for Natural Language Processing (NLP) applications \cite{nozaki22_interspeech, futami23_icassp}. While subtitle generation is close to the task of written text transcription, there are many effects like rephrasings, summarisations and harsh subphrase deletions that are very specific to subtitles and not part of these written text annotations. Therefore, subtitles are a weaker form of supervision than written text, which is basically filtered and punctuated verbatim text. While many systems rely on post-hoc processing of the output of an ASR model for written text generation, using e.g. inverse text normalisation \cite{sunkara21_icassp}, disfluency and filler removal \cite{wang-etal-2022-adaptive}, and spelling correction \cite{guo19_icassp}, an end-to-end model that jointly learns to transcribe spoken and written text has been proposed which outperforms separate models and the cascaded approach \cite{ihori23_interspeech}, by leveraging a shared decoder that is conditioned on the task to perform.

\subsubsection{Weakly Supervised ASR}
\noindent There has been a long-standing interest in improving acoustic modelling for ASR using weak supervision \cite{li2017, li2019, lamel_2002}. Typically, the weak labels are used to filter or improve the outputs of a pre-trained acoustic model, such that they can be used for ASR training \cite{lamel_2002}. Recent work has shown that end-to-end speech recognition models can be trained with incomplete or partial reference labels \cite{pratap2022_neurips}, unordered reference labels \cite{pratap2022_icassp} or even contextually related labels (e.g. from social media video captions) \cite{sing2020_icassp}. The popularity of weakly supervised training has mainly been driven by the rise of very large datasets \cite{chen21o_interspeech}, which are often created by web-scraping audio sources and forced-aligning transcriptions extracted from the web or ASR outputs \cite{wenetspeech, galvez2021peoples, reazonspeech}. Finally, recent work has shown that competitive and robust ASR models can be built by training on a huge corpus of web-scraped supervised speech data as in Whisper \cite{whisper}, which can be considered weakly labelled, although various filtering techniques and data curation methods are in order.

\subsubsection{Proposed Approach}
\noindent This work differs from previous efforts by jointly utilising the verbatim transcriptions from a standard ASR dataset and the subtitle transcriptions from a large dataset of broadcast media data, and carefully designing a model that can learn from and also generate both modalities. The model is trained completely end-to-end and the method does not require any preprocessing (e.g. filtering, pseudo-labeling or forced alignment) of the weakly labelled data, nor any postprocessing (e.g. an external program-based LM, inverse text normalisation), nor an iterative refinement of the acoustic models and/or data, although some of these techniques could still be applied. Finally, in this work, we do not focus on predicting line breaks and screen breaks \cite{wilken-etal-2022-suber}, which can be done with segmentation models \cite{karakanta-etal-2022-evaluating}.