\section{Related Work}
\label{sec:related work}
\textbf{\noindent{Normalization Techniques in Transformers.}}\
Normalization is essential for stabilizing deep Transformer training \citep{wang2024world, wang2022anchor}, with Layer Normalization (LN) \citep{ba2016layer,wang2022anchor} being the standard. Pre-Norm \citep{xiong2020layer} improves stability but often reduces expressivity, while Post-Norm \citep{vaswani2017attention} enhances generative performance but is prone to gradient explosion in deep networks. Approaches like DeepNorm \citep{wang2024deepnet} and Sandwich-LN \citep{ding2021cogview} aim to address these challenges by balancing stability and expressivity. Our method, Scale-Distribution Decoupling (SDD), builds on these efforts by explicitly disentangling the scale and distribution of the weight matrix, preserving stability while enhancing expressivity and optimizing training.

\textbf{\noindent{Mixture of Experts and Large-Scale Model Training.}}\
The adoption of Mixture of Experts (MoE) architectures \citep{shazeer2017outrageously,fedus2022switch} has allowed for more efficient computation by activating subsets of parameters per forward pass. However, MoE introduces instability in expert selection and training divergence. OLMoE \citep{muennighoff2024olmoeopenmixtureofexpertslanguage} and architectures like Switch Transformers \citep{fedus2022switch} mitigate these issues with improved routing and load balancing. SDD complements these approaches by enhancing convergence and robustness, ensuring MoE models remain stable even under varying hyperparameter settings.

\textbf{\noindent{Scaling and Stability in Large Language Models.}}\
Training stability becomes more difficult as Transformer depth increases, with gradient-related issues like vanishing or exploding gradients. Techniques such as T-Fixup \citep{huang2020improving} and GradNorm \citep{chen2018gradnorm} focus on balancing gradient magnitudes, while Megatron-Init \citep{shoeybi2019megatron} improves initialization. However, these methods primarily address stability from a weight-scaling perspective, rather than tackling optimization dynamics directly. SDD addresses these challenges by improving depth scalability and maintaining stable feature representations across layers, reducing redundancy, and mitigating feature collapse. These advantages make SDD a robust solution for training large-scale Transformers.

By addressing both stability and expressivity, SDD offers a scalable and efficient solution that enhances training stability while preserving the model's capacity to capture complex patterns. This decoupling of scale and distribution ensures robust optimization, enabling effective training of modern Transformer architectures, even in deep or high-dimensional networks, while maintaining model performance.