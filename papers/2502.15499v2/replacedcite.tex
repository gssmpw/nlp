\section{Related Work}
\label{sec:related work}
\textbf{\noindent{Normalization Techniques in Transformers.}}\
Normalization is essential for stabilizing deep Transformer training ____, with Layer Normalization (LN) ____ being the standard. Pre-Norm ____ improves stability but often reduces expressivity, while Post-Norm ____ enhances generative performance but is prone to gradient explosion in deep networks. Approaches like DeepNorm ____ and Sandwich-LN ____ aim to address these challenges by balancing stability and expressivity. Our method, Scale-Distribution Decoupling (SDD), builds on these efforts by explicitly disentangling the scale and distribution of the weight matrix, preserving stability while enhancing expressivity and optimizing training.

\textbf{\noindent{Mixture of Experts and Large-Scale Model Training.}}\
The adoption of Mixture of Experts (MoE) architectures ____ has allowed for more efficient computation by activating subsets of parameters per forward pass. However, MoE introduces instability in expert selection and training divergence. OLMoE ____ and architectures like Switch Transformers ____ mitigate these issues with improved routing and load balancing. SDD complements these approaches by enhancing convergence and robustness, ensuring MoE models remain stable even under varying hyperparameter settings.

\textbf{\noindent{Scaling and Stability in Large Language Models.}}\
Training stability becomes more difficult as Transformer depth increases, with gradient-related issues like vanishing or exploding gradients. Techniques such as T-Fixup ____ and GradNorm ____ focus on balancing gradient magnitudes, while Megatron-Init ____ improves initialization. However, these methods primarily address stability from a weight-scaling perspective, rather than tackling optimization dynamics directly. SDD addresses these challenges by improving depth scalability and maintaining stable feature representations across layers, reducing redundancy, and mitigating feature collapse. These advantages make SDD a robust solution for training large-scale Transformers.

By addressing both stability and expressivity, SDD offers a scalable and efficient solution that enhances training stability while preserving the model's capacity to capture complex patterns. This decoupling of scale and distribution ensures robust optimization, enabling effective training of modern Transformer architectures, even in deep or high-dimensional networks, while maintaining model performance.