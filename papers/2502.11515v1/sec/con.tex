\section{Conclusion}



We present SayAnything, an end-to-end video diffusion framework for audio-driven lip synchronization. Our method achieves natural lip movements with consistent teeth rendering and larger motion dynamics while maintaining identity preservation. Through a unified condition fusion scheme, SayAnything effectively balances audio-visual conditions and generates high-quality results without requiring additional supervision signals. The efficiency, innovation, and broad applicability of SayAnything make it promising for practical applications in lip synchronization.

% \begin{figure}[!htp]
%     \centering
%     \includegraphics[width=0.5\textwidth]{icml2024/figures/compare3.pdf}
%     \caption{Generated results within 5s. The first row demonstrates the original input video, and the second row is the video from which we extracted the audio as input, the video can be regarded as the target lip movements. Rows 3 - 7 display the lip-synced videos. It  demonstrates that our method can produce clearer and more consistent teeth as well as more flexible lip movements, while other methods are more influenced by the original video.}
%     \label{fig:compare-teeth}
% \end{figure}



\section*{Impact Statement}

The proposed SayAnything for audio-driven lip synchronization has significant potential applications across multiple domains. It can enhance digital content creation by enabling high-quality dubbing in different languages, facilitating international content distribution, and cross-cultural communication. The technology can improve virtual communication experiences through more natural and expressive virtual avatars in video conferencing and online education platforms. In the entertainment industry, our method can streamline the production of animated content and virtual characters by providing realistic lip movements synchronized with audio, benefiting film production, gaming, and digital media creation. Our experiments demonstrate that the model generalizes well across different scenarios, from actual human subjects to virtual characters and animated styles, enabling broad applications without domain-specific fine-tuning.

Potential Negative Social Impact: We acknowledge that this technology could potentially be misused for creating deceptive content, such as deepfake videos with manipulated speech. To address these concerns, we recommend: (1) implementing robust watermarking techniques to trace the origin of generated contents; (2) developing detection methods to identify AI-generated videos; (3) establishing clear guidelines and ethical frameworks for the deployment of such technologies. We emphasize the importance of collaborative efforts between researchers, industry practitioners, and policymakers to ensure the responsible development and application of audio-driven video synthesis technologies.