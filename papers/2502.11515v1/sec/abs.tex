% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[width=\textwidth]{icml2024/figures/intro.pdf}
%     \caption{SayAnything performs audio-driven lip synchronization through video editing, demonstrating zero-shot generalization to in-the-wild and various style domains without fine-tuning. Our fusion scheme eliminates the dependency on additional supervision signals like SyncNet for lip synchronization. More video results are available in the supplementary materials.}
%     \label{fig:teaser}
% \end{figure*}

\begin{abstract}
Recent advances in diffusion models have led to significant progress in audio-driven lip synchronization. However, existing methods typically rely on constrained audio-visual alignment priors or multi-stage learning of intermediate representations to force lip motion synthesis. This leads to complex training pipelines and limited motion naturalness. In this paper, we present SayAnything, a conditional video diffusion framework that directly synthesizes lip movements from audio input while preserving speaker identity. Specifically, we propose three specialized modules, including an identity preservation module, an audio guidance module, and an editing control module. Our novel design effectively balances different condition signals in the latent space, enabling precise control over appearance, motion, and region-specific generation without requiring additional supervision signals or intermediate representations. Extensive experiments demonstrate that SayAnything generates highly realistic videos with improved lip-teeth coherence, enabling unseen characters to \textbf{say anything} while effectively generalizing to animated characters.
\end{abstract}
\vspace{-1cm}