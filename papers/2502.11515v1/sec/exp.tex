% \begin{table*}[!ht]
% \caption{Quantitative comparison on HDTF~\citep{zhang2021flow} and AVASpeech~\citep{chaudhuri2018ava}. 
% Arrows indicate whether higher ($\uparrow$) or lower ($\downarrow$) values are better. 
% Best results are in \textbf{bold}. Notably, all baseline methods incorporate SyncNet~\citep{chung2017out} as lip expert supervision and LPIPS~\citep{zhang2018unreasonable} as an additional training loss, where the ground-truth (GT) score is computed using SyncNet on real videos. SayAnything outperforms these methods across most metrics without requiring such expert supervision.}
% \label{tab:comparison}
% \vskip 0.15in
% \centering
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{l c c c c c c c c c c c c}
% \toprule
%  & \multicolumn{6}{c}{\textbf{HDTF}} & \multicolumn{6}{c}{\textbf{AVASpeech}} \\
% \cmidrule(lr){2-7}\cmidrule(lr){8-13}
% \textbf{Method} & 
% \textbf{FID}$\downarrow$ & 
% \textbf{FVD}$\downarrow$ & 
% \textbf{SSIM}$\uparrow$ & 
% \textbf{PSNR}$\uparrow$ & 
% \textbf{LPIPS}$\downarrow$ & 
% \textbf{Sync\_c}$\uparrow$ &
% \textbf{FID}$\downarrow$ & 
% \textbf{FVD}$\downarrow$ & 
% \textbf{SSIM}$\uparrow$ & 
% \textbf{PSNR}$\uparrow$ & 
% \textbf{LPIPS}$\downarrow$ & 
% \textbf{Sync\_c}$\uparrow$ \\
% \midrule
% Diff2lip~\citep{mukhopadhyay2024diff2lip}   
% & 39.11 & 647.42 & 0.5664 & 14.36 & 0.2336 & 3.55
% & 54.80 & 578.05 & 0.5153 & 12.14 & 0.2649 & 2.09 \\
% VideoRetalking~\citep{cheng2022videoretalking} 
% & 15.48 & 328.33 & 0.0865 & 24.70 & 0.0528 & 7.60
% & 18.93 & 354.27 & 0.8632 & 25.35 & 0.0550 & 7.98 \\
% MuseTalk~\citep{zhang2024musetalk}       
% & 8.42  & 209.79 & 0.8871 & 27.99 & 0.0297 & 6.39
% & 15.76 & 331.71 & 0.8800 & 26.32 & 0.0414 & 6.46 \\
% LatentSync~\citep{li2024latentsync}     
% & 8.40  & 200.83 & 0.8870 & 27.67 & 0.0391 & \textbf{9.29}
% & 13.91 & 234.95 & 0.8836 & 27.09 & 0.0451 & \textbf{8.31} \\
% Ours           
% & \textbf{5.68} & \textbf{197.71} & \textbf{0.9347} & \textbf{29.04} & \textbf{0.0239} & 7.23 
% & \textbf{11.31} & \textbf{222.61} & \textbf{0.8881} & \textbf{27.12} & \textbf{0.0332} & 7.06 \\
% \midrule
% GT             
% & --    & --     & --     & --    & --     & 8.58 
% & --    & --     & --     & --    & --     & 7.84 \\
% \bottomrule
% \end{tabular}
% }%
% \vskip -0.1in
% \end{table*}

\section{Experiments}
\subsection{Experimental Settings}

\paragraph{Datasets.}
We utilize four public datasets for training: AVASpeech~\citep{chaudhuri2018ava}, HDTF~\citep{zhang2021flow}, VFHQ~\citep{wang2022vfhq}, and MultiTalk~\citep{sung2024multitalk}. AVASpeech (45 hours) was originally designed for speech activity detection and includes a portion of noisy audio segments. HDTF comprises 362 high-definition (HD) videos at resolutions mostly between 720p and 1080p. VFHQ contains over 16,000 high-fidelity video clips from diverse interview scenarios. Finally, MultiTalk is a multilingual dataset for 3D talking head generation; it spans 20 languages and 423 hours of video content. After filtering out videos with low face resolution, incomplete head regions, and audio-visual misalignment, we obtain approximately 600 hours of curated training data. For evaluation, we randomly sample 30 videos from the test sets of HDTF and AVASpeech datasets.

\paragraph{Implementation Details.}
We train our model on 8 NVIDIA H800 GPUs with a batch size of 16. The model is initialized with SVD weights~\citep{blattmann2023stable} and trained for 200k steps using AdamW optimizer~\citep{loshchilov2017decoupled} with a fixed learning rate of 6e-5. For training, video clips are processed at 25 fps with a resolution of 320×320 pixels, and each sequence consists of 16 frames. Audio inputs are resampled to 16 kHz. The reference frame for each sequence is randomly sampled from the corresponding complete video.

\paragraph{Evaluation Metrics.}
Our evaluation framework consists of three key aspects: (1) Visual fidelity: We employ FID~\citep{heusel2017gans} to evaluate the quality of generated frames, particularly focusing on identity preservation and visual details. SSIM and PSNR provide complementary measurements of reconstruction accuracy, while LPIPS~\citep{zhang2018unreasonable} captures perceptual similarities. (2) Temporal coherence: We adopt FVD~\citep{unterthiner2018towards} to assess video-level quality and motion consistency. (3) Audio-visual synchronization: The SyncNet confidence score~\citep{chung2017out} quantifies the accuracy of lip movements relative to audio input.



\subsection{Comparisons}
\begin{figure*}[!htp]
    \centering
 \subfloat[]{
 \includegraphics[width=0.525\textwidth]{figures/compare_v2.pdf}
 \label{fig:compare1}
 }
  \subfloat[]{
 \includegraphics[width=0.46\textwidth]{figures/compare2_v3.pdf}
 \label{fig:compare2_v2}
 }
    \caption{Qualitative comparisons with SOTA diffusion-based lip-sync methods~\citep{mukhopadhyay2024diff2lip,zhang2024musetalk,li2024latentsync,cheng2022videoretalking}. The first row demonstrates the original input video, and the second row is the video from which we extracted the audio as input, the video can be regarded as the target lip movements. Rows 3 - 7 display the lip-synced videos.  (a) Two cases in the cross-sex and ID generation setting.  (b) Two cases in the animate settings. Our method can generate more federal visual features like driven animators while others tend to generate fake features which are more realistic.
}
    \label{fig:compare}
\end{figure*}

\paragraph{Qualitative and Quantitative Comparison.}
We compare our method with state-of-the-art diffusion-based approaches that provide both inference code and pre-trained weights. Specifically, we consider: (1) LatentSync~\citep{li2024latentsync}, which utilizes an optimized SyncNet for additional supervision; (2) Diff2lip~\citep{mukhopadhyay2024diff2lip}, which directly generates the lower half of frames with adversarial and sync losses; (3) MuseTalk~\citep{zhang2024musetalk}, which builds upon the stable diffusion framework; and (4) VideoRetalking~\citep{cheng2022videoretalking}, which employs a three-stage network architecture.

As shown in \cref{tab:comparison}, our method demonstrates consistent improvements across most metrics on both datasets. While our method shows lower Sync-c scores~\citep{chung2017out}, this metric is computed by the original SyncNet model. Notably, all four baseline methods incorporate SyncNet as an additional supervision signal during training, with LatentSync employing an optimized version that produces scores exceeding ground truth values. Our results in visual quality, identity preservation, temporal consistency and competitive lip synchronization demonstrate the superiority of SayAnything.

\begin{figure}[!htp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/compare3.pdf}
    \caption{Qualitative comparison of lip motion dynamics and tooth rendering. Our method demonstrates clearer and more consistent teeth as well as more flexible lip movements.}
    \label{fig:compare-teeth}
    % \vspace{-1cm}
\end{figure}

As illustrated in \Cref{fig:compare}, existing methods exhibit various limitations: Diff2Lip~\citep{mukhopadhyay2024diff2lip} generates corrupted facial appearance, LatentSync~\citep{li2024latentsync} produces incoherent lip movements, MuseTalk~\citep{zhang2024musetalk} struggles with identity preservation, and VideoRetalking~\citep{cheng2022videoretalking} shows limited temporal consistency. Influenced by SyncNet's visual priors, these methods not only transform animated characters' lip shapes into life-like ones, compromising identity preservation, but also tend to generate conservative lip movements. In contrast, SayAnything maintains consistent identity preservation while generating more dynamic yet natural lip movements with high-quality teeth rendering and stable temporal coherence. Previous studies~\citep{jiang2024loopy,yaman2023plug} and comparisons of different methods~\citep{mukhopadhyay2024diff2lip,zhang2024musetalk,li2024latentsync,cheng2022videoretalking} in \cref{fig:compare-teeth} further demonstrate this phenomenon: SyncNet evaluation is unstable and favors conservative lip movements. Although our method shows significant improvements in visual metrics and lip motion dynamics, such more dynamic lip movements adversely affect the quantitative evaluation of lip synchronization.

\begin{table}[!htp]
\vspace{-0.5cm}
\caption{User preference rates (\%) across different scenarios. 
Users select one preferred method per test case. 
User study results across four video template categories 
Users upload their audio for testing. 
Results from 500 users show that SayAnything achieves the highest preference rates across all scenarios.}
\label{tab:userstudy}
\vskip 0.15in
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
Method & Virtual & Cartoon & Life & News \\
\midrule
Diff2Lip~\citep{mukhopadhyay2024diff2lip} & 0.2 & 0.4 & 5.2 & 3.4 \\
VideoRetalking~\citep{cheng2022videoretalking} & 1.4 & 1.2 & 7.4 & 6.0 \\
MuseTalk~\citep{zhang2024musetalk}  & 2.2 & 2.2 & 9.4 & 11.4 \\
LatentSync~\citep{li2024latentsync} & 19.4 & 21.4 & 30.2 & 29.4 \\
SayAnything & \textbf{76.8} & \textbf{74.8} & \textbf{48.8} & \textbf{49.8} \\
\bottomrule
\end{tabular}%
}
% \vspace{-0.5cm}
\vskip -0.1in
\end{table}

\begin{table*}[t]
\caption{Ablation studies to validate the effectiveness of SayAnything modules. Video Fusion refers to encoding masked video through ID-Guider while processing reference images following stable video diffusion, concatenated with noise latent. VAE Feature indicates pre-encoding reference image through VAE. w/o Mask represents using larger fixed masks for videos rather than adaptive masking. w/o Cfg denotes the removal of the condition masking strategy during training.}
\label{tab:ablation}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c}
\toprule
 & \multicolumn{6}{c}{\textbf{HDTF}} & \multicolumn{6}{c}{\textbf{AVASpeech}} \\
\cmidrule(lr){2-7}\cmidrule(lr){8-13}
\textbf{Method} &
\textbf{FID}\(\downarrow\) & 
\textbf{FVD}\(\downarrow\) & 
\textbf{SSIM}\(\uparrow\) & 
\textbf{PSNR}\(\uparrow\) & 
\textbf{LPIPS}\(\downarrow\) & 
\textbf{Sync-c}\(\uparrow\) &
\textbf{FID}\(\downarrow\) & 
\textbf{FVD}\(\downarrow\) & 
\textbf{SSIM}\(\uparrow\) & 
\textbf{PSNR}\(\uparrow\) & 
\textbf{LPIPS}\(\downarrow\) & 
\textbf{Sync-c}\(\uparrow\) \\
\midrule
Video Fusion &
20.14 & 247.73 & 0.8589 & 23.52 & 0.0504 & 6.09 &
20.32 & 280.65 & 0.8680 & 24.63 & 0.0497 & 6.09 \\
VAE Feature &
20.13 & 244.42 & 0.8569 & 23.20 & 0.0516 & 5.88 &
21.24 & 324.96 & 0.8642 & 23.81 & 0.0518 & 5.56 \\
w/o Mask &
16.19 & 299.44 & 0.8676 & 24.50 & 0.0483 & 6.54 &
17.80 & 299.85 & 0.8749 & 24.93 & 0.0497 & 6.30 \\
w/o Cfg &
14.58 & 255.29 & 0.8628 & 23.72 & 0.0510 & 5.14 &
18.65 & 257.97 & 0.8635 & 24.06 & 0.0523 & 5.69 \\
SayAnything &
\textbf{5.68} & \textbf{197.71} & \textbf{0.9347} & \textbf{29.04} & \textbf{0.0239} & \textbf{7.23} &
\textbf{11.31} & \textbf{222.61} & \textbf{0.8881} & \textbf{27.12} & \textbf{0.0332} & \textbf{7.06} \\
\bottomrule

\end{tabular}%
}
% \vspace{-0.5cm}
\end{table*}
\paragraph{User Study.}

We further evaluate SayAnything through a comprehensive user study. We provide 40 video templates that can be categorized into four types with broad application scenarios: virtual characters generated by AI models, cartoon characters rendered in styles similar to Disney animation films, news anchors in studio settings, and lifestyle scenarios featuring dynamic backgrounds and more significant head movements. Users can test with their own audio inputs, comparing results from all methods simultaneously and selecting the most preferred one. As shown in \cref{tab:userstudy}, SayAnything achieves the highest preference rates across all scenarios, demonstrating its versatility and superiority in handling diverse visual styles.



\subsection{Ablation Studies}

This section presents a comprehensive ablation analysis of SayAnything’s multi-modal fusion scheme and its key components. In earlier experiments, we attempted to use the ID-Guider module to encode the masked video, then replace the original first-frame image in stable video diffusion with the reference image. This seemed a natural approach, but we observed that the reference image ended up dominating the output. For instance, if the reference image had a wide-open mouth, the resulting video could no longer perform a closed-mouth action and instead remained open-mouthed throughout, which severely impairs lip synchronization accuracy. We believe this arises because the reference image gets replicated to align with the noise latent dimension, which originally helps lock in the first frame in the original model, but in our approach, it suppresses any subsequent mouth movement due to the spatial influence of the reference image.

Likewise, using a VAE to compress the reference image into latent space and then input it to the ID-Guider produced a similar effect because it allowed the identity information to align with the denoising UNet from the earliest stage of training. Furthermore, applying a larger, fixed mask hampered consistent identity in the generated video and caused colour shifts between masked and unmasked regions, degrading visual quality. This finding suggests that our adaptive masking strategy effectively balances the editing region control with the audio-driven lip movements. Moreover, our conditional masking strategy further boosts the overall visual quality and stabilizes lip motions in the output video. \cref{fig:ablastudy} illustrates these phenomena and how our method improves them. We also evaluated these different training configurations on the HDTF~\citep{zhang2021flow} and AVASpeech~\citep{chaudhuri2018ava} datasets; as shown in \cref{tab:ablation}, our final approach’s key components demonstrate both strong effectiveness and well-grounded design in the fusion scheme.



\subsection{Generalization Capability}

Notably, SayAnything generalizes to out-of-domain video inputs without any additional fine-tuning. We validate this capability through our user study. As shown in \cref{fig:gen}, our method demonstrates zero-shot generalization to diverse animation styles while maintaining natural lip movements and tooth consistency. Additional visual results are provided in the supplementary materials.




\begin{figure}[!ht]
    % \vspace{-0.4cm}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/ablation2.pdf}
    
    \caption{Ablation studies of our components in SayAnything. Video Fusion and VAE Feature significantly enhance reference image influence, limiting the range of lip movements. Larger fixed masks lead to colour shifts in masked regions and unnatural lip motions. Removing the condition masking strategy reduces visual quality. Zoom in for generated details.}
        
    \label{fig:ablastudy}
    % \vspace{-0.5cm}
\end{figure}

\begin{figure}[!htp]
    \centering
    \setlength{\abovecaptionskip}{0.cm}
    \setlength{\belowcaptionskip}{0.cm}
    \includegraphics[width=0.45\textwidth]{figures/gene.pdf}
    \caption{Visualization of pixar and virtual character videos generated by SayAnything.}
    \label{fig:gen}
    % \vspace{-0.3cm}
\end{figure}

% \begin{figure*}[!htp]
%     \centering
%  \subfloat[]{
%  \includegraphics[width=0.525\textwidth]{icml2024/figures/compare.pdf}
%  \label{fig:compare1}
%  }
%   \subfloat[]{
%  \includegraphics[width=0.46\textwidth]{icml2024/figures/compare2_v2.pdf}
%  \label{fig:compare2_v2}
%  }
%     \caption{Qualitative comparisons with SOTA Diffusion-based lip-sync methods. The first row demonstrates the original input video, and the second row is the video from which we extracted the audio as input, the video can be regarded as the target lip movements. Rows 3 - 7 display the lip-synced videos.  ((a)) Two cases in the cross-sex and ID generation setting.  ((b)) Two cases in the cross-ID generation setting.  
% }
%     \label{fig:compare}
% \end{figure*}
% \begin{figure}[!htp]
%     \centering
%     \includegraphics[width=0.5\textwidth]{icml2024/figures/compare3.pdf}
%     \caption{Generated results within 5s. The first row demonstrates the original input video, and the second row is the video from which we extracted the audio as input, the video can be regarded as the target lip movements. Rows 3 - 7 display the lip-synced videos. It  demonstrates that our method can produce clearer and more consistent teeth as well as more flexible lip movements, while other methods are more influenced by the original video.}
%     \label{fig:compare-teeth}
% \end{figure}

% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{icml2024/figures/compare3.pdf}
%     \caption{Generated results within 5s. The first row demonstrates the original input video, and the second row is the video from which we extracted the audio as input, the video can be regarded as the target lip movements. Rows 3 - 7 display the lip-synced videos. It  demonstrates that our method can produce clearer and more consistent teeth as well as more flexible lip movements, while other methods are more influenced by the original video.}
%     \label{fig:compare-teeth}
% \end{figure*}

% \begin{figure*}[!htp]
%     \centering
%     \includegraphics[width=0.9\textwidth]{icml2024/figures/ablation2.pdf}
%     \caption{Ablation Stydy: Generated results within 5s. The first row demonstrates the video from which we extracted the audio as input, the video can be regarded as the target lip movements. Rows 2 - 6 display the lip-synced videos. xxxx: explain the method }
%     \label{fig:ablastudy}
% \end{figure*}

