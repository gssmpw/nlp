\clearpage
\appendix

\section{Dataset Construction}

We collect and process four public datasets, AVASpeech \citep{chaudhuri2018ava}, HDTF ~\citep{zhang2021flow}, MultiTalk \citep{sung2024multitalk}, and VFHQ \citep{wang2022vfhq}. to build our training corpus. 
Among these, \emph{AVASpeech} is the largest and includes some noisy audio segments. 
\emph{HDTF} and \emph{VFHQ} contain predominantly high-definition (HD) video, thus providing detailed visual information suitable for our task. 
In the following, we outline our data filtering and pre-processing steps:
\vspace{-0.3cm}
\paragraph{Face Detection and Resolution Check.}
We employ YOLOv5 to detect faces in each video, retaining only the largest bounding box per frame. 
Any video whose face region never exceeds $228\times228$ pixels is discarded, removing clips with insufficient facial detail.
\vspace{-0.3cm}
\paragraph{Quality Filtering.}
Following VFHQ, we adopt HyperIQA~\citep{9156687} to remove videos exhibiting low clarity. 
Since most videos in our dataset share relatively consistent scenes, we additionally analyze frame-to-frame movements of the bounding box to exclude clips with excessive jitter or head motion. 
If the bounding box displacement across frames surpasses a threshold, we segment the video around those points to ensure each resulting clip is temporally coherent with a stable face region. 
Furthermore, we discard clips shorter than 2 seconds, which also effectively removes most multi-face scenes.
\vspace{-0.3cm}
\paragraph{Adaptive Mask Allowance.}
Due to our adaptive masking strategy (see Section~3.2 of the main text), we can tolerate up to eight consecutive frames without a detected face. 
In such cases, the mask is derived from contextual smoothing. 
Consequently, we adopt a lenient criterion for filtering side-face or partially occluded segments, aiming to maximize data utilization.
\vspace{-0.3cm}
\paragraph{Audio-Visual Alignment.}
Lastly, we remove instances where audio is clearly misaligned with the person on screen (e.g., voice from an off-screen speaker or background music instead of speech). 
Any clip with evidence of audio-video asynchrony or background-only audio is excluded to guarantee consistent lip and voice matching.

After applying all these steps, we obtain a curated dataset in which the speakerâ€™s face is clearly visible and well-aligned with the corresponding audio. This final corpus spans a diverse range of speaking styles and resolutions, satisfying the requirements of our audio-driven lip synchronization framework.

\vspace{-0.3cm}
\section{Inference Procedure}
At test time, \emph{SayAnything} takes as input an audio clip and a video clip, which often differ in duration. We standardize to the \emph{audio} length: 
\begin{itemize}
    \item If the audio is shorter than the video, we simply truncate the video to match the audio length.
    \vspace{-0.3cm}
    \item If the audio is longer than the video, we concatenate the video in both forward and reverse orders, applying a smoothing function around the junction points to mitigate abrupt transitions.
\end{itemize}
\vspace{-0.3cm}
Once the video duration is matched to the audio, we detect and crop the face region in each frame. The bounding box is expanded by a certain proportion to avoid tight cropping, and its coordinates are smoothed across frames to prevent large jitter. We randomly select a reference image from the video frames to serve as the identity condition.

To generate the final lip-synced output, we adopt a \emph{segmented inference} approach, setting an overlap of four frames between adjacent segments. The guidance scale is set to 3.0, and we use 15 denoising steps, with the corresponding conditional CFG strategy as in training. After inference, we locate the face region in both the original and the generated videos. We dilate these bounding boxes slightly, then take their union as the final region into which the generated face is composited. This ensures a seamless integration of the lips and facial region in the output video.


\section{Limitations}

Despite several efficiency optimizations, our method's inference speed is still constrained by the diffusion process. As noted before, we trade off speed and quality by using 15 denoising steps, taking roughly 7 seconds on an RTX 4090 GPU to process 1 second of 25\,fps video. Moreover, \emph{SayAnything} has certain input limitations: for animated characters, it struggles with extended silent segments where the lips should remain static. Additionally, when faces are heavily occluded, the method often produces visually inconsistent results.

\section{Discussion on Motivation and Future Work}

We discuss potential research and engineering improvements as follows.
First, to further expand real-world applications, our method can be distilled into latent consistency models (LCMs)~\citep{luo2023latent}, substantially boosting inference efficiency.
Second, lip synchronization can exhibit a potential gap between audio signals and emotional expressiveness; by extending the region of interest from the lips to the entire head or even the full body, one can incorporate additional posture-based features that convey richer emotional cues.
Such directions are promising for enhancing both the performance and expressiveness of audio-driven generation.

\section{Addtional Visualizations}

In this section, we present additional generation results of SayAnything across various scenarios in \Cref{fig:add1}.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/addv1.pdf}
    \caption{Lip synchronization results for different animated characters driven by the same audio segment. Our method demonstrates consistent lip motion patterns across various animation styles while preserving each character's unique visual characteristics.}
    \label{fig:add1}
\end{figure*}










