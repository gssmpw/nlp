\begin{figure*}[!ht]
    \centering
    \vspace{5mm}
    \includegraphics[width=0.9\textwidth]{figures/icml_overview.pdf}
    \caption{(a) Overview of SayAnything architecture for lip synchronization. The denoising UNet takes noisy latents as input, concatenated with video latents obtained from masked video through VAE encoding. The reference image is processed by ID-Guider to produce multi-scale ID features, which are injected as residual signals into the denoising UNet. Audio features from Whisper are fused through cross-attention layers in the denoising process. (b) A typical UNet block, consisting of ResNet block, Self Attention, Audio Cross Attention, and Temporal Attention.}
    \label{fig:overview}
\end{figure*}

\section{Related Work}
\subsection{Video Diffusion Models}
% Diffusion generative models~\citep{sohl2015deep, song2019generative, ho2020denoising, song2021scorebased, ruiz2024lane} have emerged as a leading approach for vision generation tasks.
% Recent advancements have also been witnessed in diffusion model-based video generative methods~\cite{videoworldsimulators2024,polyak2024movie,opensora2024, chen2024videocrafter2,blattmann2023stable, jta+22,jwc+22,arh+23,yca+23,xyg+24,rma+23, zhou2023magicvideoefficientvideogeneration}. For example, \cite{jta+22} present \textbf{VDM}, an early diffusion-based video generation model. \textbf{ImagenVideo}~\cite{jwc+22} use a cascaded diffusion process (\cite{ho2022cascaded} that can generate high-resolution videos in their model. \textbf{MagicVideo}~\cite{zhou2023magicvideoefficientvideogeneration} adapts the Latent Diffusion Models (\cite{rombach2022high} architecture for video generation tasks which takes place in a low-dimensional latent embedding space defined by a pre-trained variational auto-encoder (VAE).
% The text-to-video \textbf{SORA}~\cite{videoworldsimulators2024} model has been able to generate high-quality videos up to a minute long conditional on a user's prompt. 
% \textbf{Movie Gen}~\cite{polyak2024movie} from Meta offers a suite of foundation models capable of generating high-quality, high-resolution videos in various aspect ratios with synchronized audio.
% For open-source models, recent foundational video generation models include \textbf{OpenSora} \cite{opensora2024}, \textbf{VideoCrafter-2} \cite{chen2024videocrafter2} and \textbf{Stable Video Diffusion} \cite{blattmann2023stable}. 

Diffusion generative models have recently gained prominence as a leading approach for vision generation tasks, with notable contributions from several studies~\citep{sohl2015deep, song2019generative, ho2020denoising, song2021scorebased, ruiz2024lane}. In the realm of video generation, diffusion models have also seen significant advancements, with various methods being proposed~\cite{videoworldsimulators2024, polyak2024movie, opensora2024, chen2024videocrafter2, blattmann2023stable, jta+22, jwc+22, arh+23, yca+23, xyg+24, rma+23, zhou2023magicvideoefficientvideogeneration}. For instance, the early diffusion-based video generation model VDM was introduced by~\cite{jta+22}. ImagenVideo~\cite{jwc+22} employs a cascaded diffusion process~\cite{ho2022cascaded} to generate high-resolution videos. MagicVideo~\cite{zhou2023magicvideoefficientvideogeneration} leverages the architecture of Latent Diffusion Models~\cite{rombach2022high} for video generation, utilizing a low-dimensional latent embedding space defined by a pre-trained variational auto-encoder (VAE). The text-to-video model SORA~\cite{videoworldsimulators2024} is capable of generating high-quality videos up to a minute long based on user prompts. Movie Gen~\cite{polyak2024movie}, developed by Meta, offers a suite of foundation models that can produce high-quality, high-resolution videos with synchronized audio in various aspect ratios. In the open-source domain, recent foundational video generation models include OpenSora~\cite{opensora2024}, VideoCrafter-2~\cite{chen2024videocrafter2}, and Stable Video Diffusion~\cite{blattmann2023stable}.

\subsection{Video Editing}
%% cite the work 
% As the field of diffusion-based video generation continues to expand, editing videos from text inputs also sparked considerable research interest.
% Tune-A-Video~\cite{wgw+23} pioneers the concept of one-shot tuning in video editing. It transforms the spatial self-attention layers of a T2I StableDiffusion into the sparse-casual attention layers.
% While effective, methods like Tune-A-Video entail high fine-tuning costs. 
% % These constraints drive subsequent enhancements. 
% SimDA \cite{zqh+23} aims to extend T2I StableDiffusion to T2V by parameter efficient fine-tuning. It optimizes an adapter comprising two learnable fully connected (FC) layers.
% Fairy~\cite{bcx+23} is designed for parallelized frames sampling and editing keyframes in a single forward pass with T2I diffusion models~\cite{bhe23}.
% Some works focus on other conditions to edit the video.
% Gen-1~\cite{pjp+23} concatenates depth maps~\cite{RanftlLHSK22} to effectively offering sequential depth guidance but is computational costly for finetuning all parameters.
% MagicEdit~\cite{jhj+23} freezes the pre-trained parameters of the T2I model and fine-tunes the new temporal layers on video data, similar to AnimateDiff~\cite{yca+23}. Subsequently, it directly integrates the ControlNet~\cite{zra23} trained for conditional image generation. 
% Ground-A-Video~\cite{hj23} utilizes both depth maps and bounding boxes to offer spatial control to target regions through an inflated ControlNet~\cite{zra23}.
% VideoComposer~\cite{WangYZCWZSZZ23} extends the types of controllable conditions and provides simultaneous control with multiple conditions, including sketch images~\cite{0002LYH00P021}, depth maps~\cite{RanftlLHSK22}, and motion vectors~\cite{VadakitalDLTLR22}.

As diffusion-based video generation continues to advance, the ability to edit videos using text inputs has become a focal point of research. One notable contribution is Tune-A-Video~\cite{wgw+23}, which introduces the idea of one-shot video editing tuning. This method converts the spatial self-attention layers of a text-to-image (T2I) StableDiffusion model into sparse-casual attention layers. However, approaches like Tune-A-Video come with high fine-tuning costs.
To address this, SimDA~\cite{zqh+23} proposes a more parameter-efficient fine-tuning method to extend T2I StableDiffusion to text-to-video (T2V) generation. It achieves this by optimizing an adapter composed of two learnable fully connected (FC) layers. Another approach, Fairy~\cite{bcx+23}, focuses on parallelized frame sampling and keyframe editing using T2I diffusion models~\cite{bhe23} in a single forward pass.
Other works explore different conditions for video editing. For example, Gen-1~\cite{pjp+23} incorporates depth maps~\cite{RanftlLHSK22} to provide sequential depth guidance, although this method is computationally expensive due to the need to fine-tune all parameters. MagicEdit~\cite{jhj+23} freezes the pre-trained parameters of a T2I model and fine-tunes new temporal layers on video data, similar to AnimateDiff~\cite{yca+23}. It then integrates a ControlNet~\cite{zra23} trained for conditional image generation.
Ground-A-Video~\cite{hj23} leverages both depth maps and bounding boxes to provide spatial control over target regions using an inflated ControlNet~\cite{zra23}. Finally, VideoComposer~\cite{WangYZCWZSZZ23} expands the range of controllable conditions and allows simultaneous control with multiple inputs, including sketch images~\cite{0002LYH00P021}, depth maps~\cite{RanftlLHSK22}, and motion vectors~\cite{VadakitalDLTLR22}.
\vspace{-0.2cm}

\subsection{Audio-Driven Talking Head Generation}
Recently, GAN-based methods~\cite{prajwal2020lip, zhang2023dinet, zhong2023identity, wang2023lipformer, tan2024style2talker, tan2025edtalk, yang2024ladtalk, zhang2024musetalk, cheng2022videoretalking} have emerged as an essential research area in talking head generation, but they struggle to represent detailed texture making unremarkable video performance. On the other hand, with the recent rise of diffusion model in the AIGC community, numerous works~\cite{xu2024hallo, xu2024vasa, tian2025emo, chen2024echomimic, zhu2024infp} have been proposed to synthesize talking head video via sound and single portrait conditions and achieved remarkable performance in lip synchronization and visual effect. However, single portrait animation has fewer application scenarios than video dubbing. To this end, Diff2lip~\cite{mukhopadhyay2024diff2lip}  and LatentSync~\cite{li2024latentsync} are proposed, which both introduce a lip expert~\cite{prajwal2020lip} to offer strong audio-visual alignment in pixel space. While this expert benefits from the improvement of lip synchronization, the life-like prior that it contains hinders the variety of input video types. To this end, we propose SayAnything, which directly drives lip synchronization by leveraging the general visual priors from stable video diffusion to enable talking head generation across various styles.