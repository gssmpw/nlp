

\section{Introduction}

Audio-driven lip synchronization aims to generate synchronized lip movements in videos based on input audio while preserving the speaker's identity and appearance. This task has significant applications in video dubbing, virtual avatars, and live-streaming platforms.

In the field of lip synchronization, GAN-based approaches~\citep{guan2023stylesync,su2024audio} remain the dominant paradigm. However, these methods face two major limitations. First, they lack stability and diversity in motion generation, resulting in unnatural visual effects. Second, the inherent training instability and mode collapse issues make them difficult to scale to diverse datasets~\citep{heusel2017gans}, limiting their practical applications.


With the success of diffusion models~\citep{ho2020denoising,song2020score} in image and video generation~\citep{blattmann2023stable,rombach2022high}, audio-driven facial animation has emerged as a promising research direction. Recent work leveraging diffusion models for lip synchronization encounters a fundamental challenge where models favour visual conditions over audio signals during generation. Existing approaches~\citep{mukhopadhyay2024diff2lip,li2024latentsync,shen2023difftalk,zhong2024high} force models to learn audio-visual correspondence through additional supervision from Syncnet~\citep{chung2017out} as lip experts~\citep{mukhopadhyay2024diff2lip,li2024latentsync} or intermediate representations like landmarks~\citep{shen2023difftalk,zhong2024high}. While this mitigates the challenge, the strong dependency on visual priors from lip experts and intermediate representations significantly constrains model generalizability and motion dynamics.

To address these limitations, we present SayAnything, an end-to-end framework that directly establishes audio-visual correlations in the latent space without requiring additional supervision signals or intermediate representations. Specifically, we design a multi-modal condition fusion scheme, incorporating three specialized modules: 1) For reference images, we design an identity preservation module with an efficient encoder ID-Guider that extracts identity features while controlling their influence on lip motion styles, 2) For videos, we propose an editing control module with an adaptive masking strategy that regulates the lip editing region and eliminates the dependency of lip movements on surrounding pixel motion patterns, and 3) For audio inputs, we develop an audio guidance module that enhances the control of weakly-correlated audio features over motion generation. As shown in \cref{fig:teaser}, our method demonstrates effective zero-shot generalization to both real-world and animated characters. To the best of our knowledge, this is the first work to leverage Stable Video Diffusion (SVD)~\citep{blattmann2023stable} for lip synchronization.

Our main contributions can be summarized as follows:
\begin{itemize}
   \item We propose an end-to-end video diffusion framework for audio-driven lip synchronization that eliminates the need for additional supervision signals or intermediate representations.
   
   \item We design a novel multi-modal condition fusion scheme that effectively balances audio-driven control and portrait preservation, incorporating identity preservation, audio guidance, and editing control modules.
   
   \item Both extensive experiments and user studies demonstrate that our approach achieves superior performance in terms of generation quality, temporal consistency, and lip motion flexibility compared to existing methods. Furthermore, our framework shows strong generalization capability to in-the-wild videos and animated characters.
\end{itemize}
