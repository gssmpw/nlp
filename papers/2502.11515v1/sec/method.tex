

\section{Method}
In this section, we will introduce our method, SayAnything. First, we will provide an overview of the framework and the preliminaries of SVD~\citep{blattmann2023stable}. Then, we will detail our multi-modal condition fusion scheme.


\subsection{Overview}

\paragraph{Stable Video Diffusion.} SVD is a high-quality and commonly used image-to-video generation model. Given a reference image $I$, SVD will generate a video frame sequence $x = \{x_0, x_1, ..., x_{N-1}\}$ of length N, starting with $I$. The sampling of SVD is conducted using a latent denoising diffusion process. At each denoising step, a conditional 3D UNet $\Phi_\theta$ is used to iteratively denoise this sequence:
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
z^0 = \Phi_\theta(z_t, t, I)
\setlength{\belowdisplayskip}{3pt}
\end{equation}
where $z_t$ is the latent representation of $x_t$ and $z^0$ is the prediction of $z_0$. There are two conditional injection paths for the reference image $I$: (1) It is embedded into tokens by the CLIP~\citep{radford2021learning} image encoder and injected into the diffusion model through a cross-attention mechanism; (2) It is encoded into a latent representation by the VAE~\citep{kingma2013auto} encoder of the latent diffusion model~\citep{rombach2022high}, and concatenated with the latent of each frame in channel dimension.
SVD follows the EDM-preconditioning~\citep{karras2022elucidating} framework, which parameterizes the learnable denoiser 
\(\Phi_\theta\) as
\begin{small}
\begin{align}
\setlength{\abovedisplayskip}{0pt}
& \Phi_\theta\bigl(z_t,\,t,\,I;\sigma\bigr) 
= c_{skip}(\sigma)\,z_t \\
\nonumber
&\quad +\, c_{out}(\sigma)\,
F_\theta\Bigl(
  c_{in}(\sigma)\,z_t,\,
  t,\,
  I;\,
  c_{noise}(\sigma)
\Bigr).
\label{eq:edm_param}
\setlength{\belowdisplayskip}{0pt}
\end{align}
\end{small}
where \(\sigma\) is the noise level, and \(F_\theta\) is the network to be trained. 
\(c_{skip},\, c_{out},\, c_{in},\, \text{and}\; c_{noise}\) are preconditioning hyper-parameters.
\noindent
\(\Phi_\theta\) is trained via a denoising score matching (DSM)~\citep{song2019generative} objective:
\begin{equation}
\setlength{\abovedisplayskip}{3pt}
\mathbb{E}_{\substack{z_0,\,t\\n\,\sim\,\mathcal{N}(0,\sigma^2)}}
\Bigl[
  \lambda_\sigma
  \,\bigl\|
    \Phi_\theta\bigl(z_0 + n,\, t,\, I\bigr)
    \;-\;
    z_0
  \bigr\|_2^2
\Bigr].
\label{eq:edm_dsm}
\setlength{\belowdisplayskip}{3pt}
\end{equation}
\paragraph{SayAnything.}
Given a reference video $V_m$ and an audio $a$, our objective is to edit the lip motion of the person in $V_m$ so it aligns with $a$. 
To achieve this, three key elements are involved:
(1)~identity preservation via a reference image $\mathcal{I}_r$,
(2)~driving the spatial configuration of the lips based on the audio $a$,
and 
(3)~maintaining overall visual coherence by specifying an editable region in the video through masking.

To ensure broad generalization, our method inherits as many structures and parameters as possible from a pretrained stable video diffusion model, fully exploiting its robust visual priors. We design a unified, efficient multi-modal fusion scheme to balance these three conditioning signalsâ€”comprising an efficient identity preservation module, an 
editing control module for region edits, and an attention-based audio guidance module.

Specifically, we adapt SVD's denoiser $\Phi_{\theta}$ to fuse $\mathcal{I}_r$, $a$, and $V_m$ in a unified manner.
Below is our conditional denoiser, which replaces the original single-condition path:
\begin{small}
\begin{align}
\setlength{\abovedisplayskip}{0pt}
&\Phi_\theta\bigl(z_t,\,t,\,\mathcal{I}_r,\,a,\,V_m;\;\sigma\bigr)
~=~
c_{\mathrm{skip}}(\sigma)\,z_t 
\nonumber\\[-3pt]
&\quad~+\;
c_{\mathrm{out}}(\sigma)\;
F_\theta\Bigl(
    c_{\mathrm{in}}(\sigma)\,z_t,\;
    t,\;
    \mathcal{I}_r,\;
    a,\;
    V_m;\;
    c_{\mathrm{noise}}(\sigma)
\Bigr).
\setlength{\belowdisplayskip}{0pt}
\label{eq:ours_denoiser}
\end{align}
\end{small}
By jointly injecting the identity signal $\mathcal{I}_r$, the audio $a$, and the masked-video prior $V_m$, we avoid the need for multi-stage training or extra intermediate representations. Moreover, we eliminate the need for adversarial losses or lip expert supervision by directly adopting the DSM objective. 
For each training iteration, we sample a clean latent $z_0$ and noise $n \sim \mathcal{N}(0,\sigma^2)$, then set $z_t = z_0 + n$. 
Our loss is:
\begin{small}
\begin{align}
\setlength{\abovedisplayskip}{0pt}
\mathcal{L}
=
\mathbb{E}_{\substack{z_0,\,t\\n\,\sim\,\mathcal{N}(0,\sigma^2)}}
\Bigl[
  \lambda_{\sigma}\,
  \bigl\|\,
    \Phi_\theta\bigl(z_t,\,t,\,\mathcal{I}_r,\,a,\,V_m;\,\sigma\bigr)
    % \;-\;
    \!\! - \!\!
    z_0
  \bigr\|_2^2
\Bigr].
\label{eq:ours_dsm}
\setlength{\belowdisplayskip}{0pt}
\end{align}
\end{small}

\cref{fig:overview} illustrates the overall pipeline of our method.
Building on insights from our experiments regarding the relative strength of each condition, 
we first present in \S3.2 our editing control module, 
then introduce the identity preservation module in \S3.3, 
followed by the audio-driven module in \S3.4.
Finally, we discuss several key training details and hyper-parameter choices.
\begin{figure}[!htp]
\setlength{\abovecaptionskip}{0.cm}
\setlength{\belowcaptionskip}{0.cm}
\centering
\includegraphics[width=0.95\linewidth]{figures/mask.pdf}
\caption{Our adaptive masking strategy first determines the initial mask through detected landmarks, then obtains the final mask through expansion and smoothing, effectively preventing motion leakage.}
\label{fig:mask}
\end{figure}
% \vspace{-1cm}

\subsection{Editing Control}

The masked video sequence provides the strongest guidance signal in our framework, as the model only needs to synthesize the masked regions while preserving the rest. Previous studies \citep{mukhopadhyay2024diff2lip, yaman2023plug} have shown that lip motions often fail to align with input audio without SyncNet supervision, and we also observed this issue in our early experiments. Even with the lip region masked, initial experiments reveal that generated results maintain high similarity with the reference video regardless of the input audio, indicating the audio condition fails to guide lip motion synthesis effectively.

By comparing different masking strategies, we find that this motion leakage effect originates from the temporal context in the masked video sequence. With a low masking ratio, the model learns to infer lip movements from visible facial muscle patterns instead of audio features, as facial movements are highly correlated - visible facial regions can effectively indicate the movements of the masked lip region. Although this is not our desired optimization direction, it helps model convergence in the early training stages.

% 
While increasing the mask coverage might be a solution, this approach alone is not optimal as it may remove essential facial features. To address this trade-off, we design an adaptive masking strategy that tracks head position across different head and lip poses. We opt for rectangular masks to avoid potential guidance from mask shapes, ensuring lip-surrounding pixels are masked and eliminating the influence of lower facial muscle motion patterns on lip movements. As shown in \cref{fig:mask}, the mask is generated using facial landmarks, with additional padding to ensure coverage of potential motion areas. To prevent motion leakage and maintain temporal stability, we apply a temporal smoothing process to the mask coordinates:
% \vspace{-0.8cm} 
\begin{equation}
% \setlength{\abovedisplayskip}{3pt}
\begin{split}
x_t^{\prime} &= \alpha x_t + (1-\alpha)x_{t+1} \\
y_t^{\prime} &= \alpha y_t + (1-\alpha)y_{t+1}
\end{split}
% \setlength{\belowdisplayskip}{0pt}
\end{equation}
where $\alpha=0.75$ provides a balance between temporal consistency and local accuracy. This smoothing process enhances mask movement stability and further prevents the model from inferring lip movements based on landmark motion patterns.

For implementation, we encode the masked video through the VAE encoder to obtain its latent representation. We then concatenate these latents with the noise latents along the channel dimension, forming an 8-channel input tensor. This design replaces the original first-frame concatenation in SVD while maintaining the same channel dimensionality, thereby providing rich temporal guidance for generation.
\subsection{Identity Preservation}

Existing methods \citep{chang2023magicdance,jiang2024loopy} typically employ ReferenceNet \citep{hu2024animate} for reference image encoding because it shares the same architecture with the denoising UNet and can be initialized with identical weights, enabling rapid condition-backbone alignment. However, directly adopting a stable video diffusion architecture would introduce substantial parameter redundancy.

Through experiments, we find that introducing complex architectures or large numbers of parameters not only increases computational overhead but also leads to excessive visual conditions. During our experiments, we discovered a spatial dependency issue: a strong identity condition influences the lip state in the reference image to dominate generation results.

To address computational efficiency and spatial dependency issues, we propose ID-Guider, an efficient encoding module comprising convolution layers. Before feeding the reference image into ID-Guider, we first concatenate the lip mask as an indicator channel with the reference image, then process it through a simple pure convolutional downsampler with channel dimensions $[32, 64, 128, 64]$ to align the input with the denoising UNet. Furthermore, compared to ReferenceNet, we remove computationally intensive modules such as 3D convolutions, retaining only 2D ResBlock modules with channel dimensions and layer counts consistent with stable video diffusion to ensure proper residual integration into the denoising UNet. We also eliminate upsample layers. Moreover, since we remove the timestep-related modules, ID-Guider does not need to recalculate following the denoising steps during inference. As a result, ID-Guider maintains only 98M parameters, reducing the parameter count by over 90\%. This significantly improves computational efficiency and reduces the influence of visual information from the reference image while maintaining strong identity preservation capabilities.

\subsection{Audio Guidance}
Audio signals, while relatively weak compared to visual conditions \citep{chen2024echomimic,tian2025emo}, provide essential features for driving lip movements. We employ Whisper \citep{radford2023robust} as our audio feature extractor for its robust audio representation capabilities. Following common practice in audio-driven synthesis, we align each video frame with a window of surrounding audio features $(x_{t-k}, ..., x_t, ..., x_{t+k})$ to capture temporal context, where $x_t$ denotes the audio feature at timestep $t$, and $k$ determines the temporal context range. For boundary frames, we apply zero-padding without additional processing.

To strengthen the influence of audio signals, we incorporate audio cross-attention into both the downsample and upsample modules of our denoising UNet. Specifically, the noisy latents act as queries, while the audio features serve as keys and values. Since these audio features already encode a contextual window, we focus on computing spatial attention that guides the lip regionâ€™s spatial distribution during generation.

\begin{table*}[!ht]
\caption{Quantitative comparison on HDTF~\citep{zhang2021flow} and AVASpeech~\citep{chaudhuri2018ava}. 
Arrows indicate whether higher ($\uparrow$) or lower ($\downarrow$) values are better. 
The best results are in \textbf{bold}. Notably, all baseline methods incorporate SyncNet~\citep{chung2017out} as lip expert supervision and LPIPS~\citep{zhang2018unreasonable} as an additional training loss, where the ground-truth (GT) score is computed using SyncNet on real videos. SayAnything outperforms these methods across most metrics without requiring such expert supervision.}
\label{tab:comparison}
\vskip 0.15in
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c}
\toprule
 & \multicolumn{6}{c}{\textbf{HDTF}} & \multicolumn{6}{c}{\textbf{AVASpeech}} \\
\cmidrule(lr){2-7}\cmidrule(lr){8-13}
\textbf{Method} & 
\textbf{FID}$\downarrow$ & 
\textbf{FVD}$\downarrow$ & 
\textbf{SSIM}$\uparrow$ & 
\textbf{PSNR}$\uparrow$ & 
\textbf{LPIPS}$\downarrow$ & 
\textbf{Sync-c}$\uparrow$ &
\textbf{FID}$\downarrow$ & 
\textbf{FVD}$\downarrow$ & 
\textbf{SSIM}$\uparrow$ & 
\textbf{PSNR}$\uparrow$ & 
\textbf{LPIPS}$\downarrow$ & 
\textbf{Sync-c}$\uparrow$ \\
\midrule
Diff2lip~\citep{mukhopadhyay2024diff2lip}   
& 39.11 & 647.42 & 0.5664 & 14.36 & 0.2336 & 3.55
& 54.80 & 578.05 & 0.5153 & 12.14 & 0.2649 & 2.09 \\
VideoRetalking~\citep{cheng2022videoretalking} 
& 15.48 & 328.33 & 0.0865 & 24.70 & 0.0528 & 7.60
& 18.93 & 354.27 & 0.8632 & 25.35 & 0.0550 & 7.98 \\
MuseTalk~\citep{zhang2024musetalk}       
& 8.42  & 209.79 & 0.8871 & 27.99 & 0.0297 & 6.39
& 15.76 & 331.71 & 0.8800 & 26.32 & 0.0414 & 6.46 \\
LatentSync~\citep{li2024latentsync}     
& 8.40  & 200.83 & 0.8870 & 27.67 & 0.0391 & \textbf{9.29}
& 13.91 & 234.95 & 0.8836 & 27.09 & 0.0451 & \textbf{8.31} \\
SayAnything           
& \textbf{5.68} & \textbf{197.71} & \textbf{0.9347} & \textbf{29.04} & \textbf{0.0239} & 7.23 
& \textbf{11.31} & \textbf{222.61} & \textbf{0.8881} & \textbf{27.12} & \textbf{0.0332} & 7.06 \\
\midrule
GT             
& --    & --     & --     & --    & --     & 8.58 
& --    & --     & --     & --    & --     & 7.84 \\
\bottomrule
\end{tabular}
}%
\vskip -0.1in
\end{table*}

\subsection{Training Strategy}

In our framework, multiple conditions are involved, including audio $a$, reference image $\mathcal{I}_r$, and masked video sequence $V_m$. Due to the inherently weaker correlation with lip movements, audio signals can be easily overwhelmed by visual features during training, leading to insufficient audio-driven control. Based on this observation, we adopt distinct masking strategies for the conditions during the training process. First, $a$ has a 5\% probability of being masked to zero, while $\mathcal{I}_r$ has a 15\% probability, and the masking of $a$ always triggers the masking of $\mathcal{I}_r$ to ensure audio-only generation scenarios. Second, all masking operations are applied to the fused features in the latent space by setting them to zero. The masked video sequence $V_m$ remains a fixed input condition without masking.

