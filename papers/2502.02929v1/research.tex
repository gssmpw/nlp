\documentclass[nonacm,manuscript]{acmart}
\PassOptionsToPackage{prologue,dvipsnames}{xcolor}
% note: Overleaf includes acmart.cls for you so you don't even need that
% it even includes the ACM-ReferenceFormat.* files, but I edited the .bst file to not warn about missing publisher/address for inproceedings

%\setcopyright{acmcopyright}
%\copyrightyear{2022}
%\acmYear{2022}
%\acmDOI{10.1145/1122445.1122456}

%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage[dvipsnames]{xcolor}
\usepackage{booktabs} % use booktabs instead of ugly regular tables
\usepackage{graphicx} % more figure options
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{subcaption}
\RequirePackage[l2tabu, orthodox]{nag} % checks for common LaTeX errors
\usepackage{microtype} % better typesetting
\usepackage[utf8]{inputenc} % lenient to utf-8 characters like smart quotes
%\usepackage{refcheck} % warns about unreferenced figures/tables that have labels. remove it before submitting
\usepackage[subtle]{savetrees} % denser formatting, you can comment this out
\usepackage{csquotes}
\usepackage{array}
\usepackage{multirow}
\usepackage{gensymb}
\usepackage{hyperref}


\interfootnotelinepenalty=10000 % split footnotes are ugly
\tolerance=400 % reduce how often words stick out into columns at the expense of word spacing

\graphicspath{{figures/}} % put all your figures in this folder
\newcommand{\note}[1]{{\color{blue}{{#1}}}}
\begin{document}

\newcommand{\nb}[1]{{\color{red}{$\Rightarrow$ ~\textbf{{#1}}}}} %nota bene

\newcommand{\cp}[1]{{\color{blue}{\enquote{{#1}}}}} %copy paste

\title[]{AudioMiXR: Spatial Audio Object Manipulation \hfill \\  with 6DoF for Sound Design in Augmented Reality}


%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% -- FIRST LINE (two equal-contribution authors) --
\author{\textbf{Brandon Woodard}}
\authornote{Both authors contributed equally to this research.\\ 
Video demos and additional information on this work can be found at \textcolor{blue}{\url{https://audiomixr.github.io/}}.
}
\affiliation{%
  \institution{Brown University}
  \country{United States}
}
\email{brandon\_woodard@brown.edu}

\author{\textbf{Margarita Geleta}}
\authornotemark[1] 
\affiliation{%
  \institution{UC Berkeley}
  \country{United States}
}
\email{geleta@berkeley.edu}

\author{\textbf{Joseph J. LaViola Jr.}}
\affiliation{%
  \institution{University of Central Florida}
  \country{United States}
}

\author{\textbf{Andrea Fanelli}}
\affiliation{%
  \institution{Dolby Laboratories}
  \country{United States}
}
%\email{youremail@example.com}


\author{\textbf{Rhonda Wilson}}
\affiliation{%
  \institution{Dolby Laboratories}
  \country{United States}
}

\renewcommand{\shortauthors}{\footnotesize{Woodard, et al.}}

\begin{abstract}

 We present AudioMiXR, an augmented reality (AR) interface intended to assess how users manipulate virtual audio objects situated in their physical space using six degrees of freedom (6DoF) deployed on a head-mounted display (Apple Vision Pro) for 3D sound design. Existing tools for 3D sound design are typically constrained to desktop displays, which may limit spatial awareness of mixing within the execution environment. Utilizing an XR HMD to create soundscapes may provide a real-time test environment for 3D sound design, as modern HMDs can provide precise spatial localization assisted by cross-modal interactions. However,  there is no research on design guidelines specific to sound design with six degrees of freedom (6DoF) in XR. To provide a first step toward identifying design-related research directions in this space, we conducted an exploratory study where we recruited 27 participants, consisting of expert and non-expert sound designers. The goal was to assess design lessons that can be used to inform future research venues in 3D sound design. We ran a within-subjects study where users designed both a music and cinematic soundscapes. After thematically analyzing participant data, we constructed two design lessons: 1. Proprioception for AR Sound Design, and 2. Balancing Audio-Visual Modalities in AR GUIs. Additionally, we provide application domains that can benefit most from 6DoF sound design based on our results.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
    <concept>   <concept_id>10003120.10003121.10003124.10010392</concept_id>
       <concept_desc>Human-centered computing~Mixed / augmented reality</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003128.10010869</concept_id>
       <concept_desc>Human-centered computing~Auditory feedback</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10003120.10003121.10003128.10011755</concept_id>
       <concept_desc>Human-centered computing~Gestural input</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Mixed / augmented reality}
\ccsdesc[500]{Human-centered computing~Auditory feedback}
\ccsdesc[500]{Human-centered computing~Gestural input}

\keywords{Augmented Reality, Spatial Audio, Sound Design, Interactive Technologies}


\begin{teaserfigure}
\includegraphics[width=\textwidth]{figures/paper/placeholder.png}
\caption{(Left) A schematic illustrating a user surrounded by virtual audio objects in 3D space. The arrows represent the virtual audio object panning initiated by user's gesture; (Right) Real-world images of AudioMiXR users employing directed free-hand manipulation for music and cinematic sound design, respectively. }
\end{teaserfigure}

\maketitle

\section{INTRODUCTION}
 
Traditional approaches to spatial audio, such as channel-based surround sound systems, have been gradually supplanted by the more versatile and immersive object-based audio format \cite{sinclair2020principles, mathew:hal-01517188}. This format facilitates the placement of each sound source as independent audio objects within a 3D space. Although existing tools for spatial audio panning within audio mixing frameworks provide more precise and flexible spatialization, they remain confined to 2D user interfaces \cite{marrington2017composing, misha2023}. This limitation constrains the creative freedom needed for immersive mixes and spatialized audio editing, and the transition towards the object-based audio paradigm has introduced implementation complexities that remain unresolved.
%% Addressing the challenge of creating immersive mixes
One such challenge lies in the sequential nature of 3D panning. Often, movement along the $z$-axis is constrained after the initial adjustments in the $x$-$y$ plane, limiting the creative flexibility and the natural coherence of spatial audio compositions. %On top of that, panning over time adds an additional fourth dimension. 
%% Addressing that the learnability for spatial audio panners is steep
Moreover, the complexity of user interfaces for spatial audio panners can result in a steep learning curve. Further, binaural rendering can complicate audio object localization within mixing environments due to the translation between 3D user interfaces (3DUI) rendered on desktop displays and real-world applications \cite{jens1996binaural}.
%% Addressing that it is difficult to localize sound objects when mixing binaurally
 
Currently, most commercial Digital Audio Workstations (DAWs) and sound design software still rely on planar controls to manipulate the placement and motion of 3D audio objects \cite{marrington2017composing}. 
%\nb{Maybe mention the cognitive load of mapping spatial audio in a unit-less panning cube}
This setup forces audio engineers and producers to mentally map 3D sound onto 2D screens, often resulting in significant cognitive load \cite{farnell2010designing}. Extended reality (XR) -- comprising both augmented reality (AR) and virtual reality (VR) -- offers an opportunity to overcome these limitations, especially when six degrees of freedom (6DoF) is employed. Within an 6DoF XR interface, users can move freely in real or virtual space (translation and rotation) rather than just relying on purely rotational or \enquote{point-and-click} interactions. Such physical freedom has been shown to facilitate more accurate cognitive map building \cite{walking2011, Pastor2024}, and can enable direct manipulation  of virtual audio objects situated within the user's environment. 

%By superimposing these objects onto the real world, we posit that augmenting traditional sound design workflows with AR interfaces can increase the spatial awareness of audio objects, provide a greater sense of control over spatial audio, and enable a better understanding of the localization of each 3D audio object. In other words, an AR-based approach can place audio objects exactly where they are perceptually \enquote{heard}, allowing users to visually and physically localize, arrange, and understand their interplay without the abstraction of a 3D panner on a 2D display. 

Motivated by the challenges identified through conversations with professional audio engineers -- including the difficulty of creating immersive mixes, localizing binaurally rendered sound sources, and mastering steep learning curves for 3D panners -- we developed AudioMiXR, an AR-based user interface for sound design and audio mixing, designed for AR-capable head-mounted displays (HMDs), offering free-hand gestural interactions with visually rendered 3D audio objects. While its current implementation focuses on AR HMDs, it can be adapted to VR contexts. By leveraging how the human perceptual system understands spatial relationships, AudioMiXR aims to make immersive sound design and audio mixing more intuitive and reduce the cognitive load of 3D panning.%, and?


This premise leads to our research questions, which seek to quantify and characterize the benefits of spatial audio interaction techniques %/panning 
in AR:
\begin{enumerate}[label=RQ\arabic*.]%,align=left]
    \item How should we design an AR interface that leverages 6DoF for sound design? %%This is straightforward, but its worth noting that we must emphasize the practicality and meaningfulness as being a goal because we don't want AudioMiXR to sound 'gimmicky' -- The design lesons must actually lead to something helpful to the mixing experience. Although sometimes gimmicks can be helpful for user enjoyment.
    \item What application areas will benefit most from an AR sound design interface with 6DoF? %%This last research question will be answered through our observations of the cinematic and music mixing scenes. It will also be answered through our question for the expert mixers where they describe how AudioMiXR might be integrated in their DAW workflows. 
\end{enumerate}



\section{RELATED WORK}

%% Proposed section order:  
% 1. 3D Sound design (in-general, e.g., DAWs, binaural rendering, spatial audio, 3DoF vs. 6DoF)
% 2. Audio Augmentation in AR
% 3. Authoring Sound in XR environments 
We present an overview of relevant concepts and prior work in 3D sound design and rendering, spatial localization in XR and the utilization of expanded degrees of freedom, a brief outline of sonified experiences in XR, and XR tools for sound design and audio mixing.

\subsection{3D Sound Design and Rendering}

Modern audio production is frequently carried out within DAWs -- software environments for music creation \cite{marrington2017composing}. Popular DAWs such as Cubase, Nuendo, Logic Pro, Ableton, and Pro Tools provide visual interfaces for manipulating %two primary data types: MIDI (musical note) information and 
digital audio, often adopting a skeumorphic design to replicate the look and feel of legacy hardware (e.g., mixers, drum machines, or synthesizers). While this approach helps users transition from traditional studio setups by mimicking familiar controls and interaction paradigms, it can constrain the interfaces to 2D representations. Furthermore, most DAWs rely on a timeline-based structure which encourages the sequential arrangement of audio objects \cite{marrington2017composing}. This temporal editing view does not take into account the \enquote{spatial} aspect of audio. Under the \emph{object-based audio} paradigm, each sound source (or \enquote{object}) is treated as an independent entity accompanied by metadata describing its spatial attributes -- namely, the location of \emph{spatial audio} in a 3D space \cite{presence1997, begault19943}. For spatial editing, DAWs offer 3D panners, which are tools to control the position of each audio object in a 3D space \cite{mathew:hal-01517188, apple2024logicpro}. A prime example is the Dolby Atmos Panner available in Pro Tools. 

There are several approaches to spatial audio rendering, and we can highlight two in the context of object-based audio workflows \cite{sinclair2020principles}. The first is \emph{loudspeaker stereophony}, which consists in placing multiple speakers around the listener to create a surround sound experience commonly used in cinemas, home theaters, and gaming. A limitation of loudspeaker-based setups is that the listener must typically remain in an optimal \enquote{sweet spot} to fully appreciate the intended spatial effect, and phantom sound sources (i.e., perceptual illusions of sound sources) are restricted to positions between the speakers. The second approach, thanks to which many of audio-augmented XR applications are possible, is \emph{binaural rendering}. This method reproduces 3D audio using only two channels, simulating how sound is perceived by the human auditory system \cite{jens1996binaural, mauro2013binaural}. By convolving audio signals with head-related transfer functions (HRTFs), which model how sound interacts with anthropometric features, binaural rendering can simulate the localization, timbre, and externalization of sound sources, creating a realistic perception of spatial objects in a 3D environment.
However, this technique can suffer from perceptual errors, such as front-back or up-down confusions and \enquote{in-head localization} effects \cite{yang2022audio}. 

Once a spatial mix is finalized, the sequence of objects is sent to the renderer -- for example, the Dolby Atmos Renderer (potentially configured with personalized HRTFs). The renderer interprets the scene metadata to produce the final output for various configurations, whether loudspeaker arrays (e.g., 5.1.4, 7.1.4, 22.1) or headphone-based binaural playback. 

\subsection{Spatial Localization with Expanded Degrees of Freedom in XR}
%%Include audio localization

XR spans both augmented reality (AR) and virtual reality (VR), combining the physical world with a digital twin world in an interactable environment \cite{milgram1995augmented}. A central factor in XR experiences is the notion of \emph{degrees of freedom} (DoF), which refers to the number of independent ways users can move within these virtual or augmented environments, and consists of translational and rotational components. Per convention, 3DoF restricts head tracking to the three rotational axes (pitch, yaw, roll), allowing users to look around but not physically move in space. A more immersive XR experience comes from adding to the equation the translational component (right/left, up/down, forward/back), resulting in 6DoF movement, which enables a more immersive and naturalistic exploration and navigation. An essential component for a user's navigation in an XR environment is building the cognitive map -- an internal representation of locations within a world-reference frame. Prior work has shown that 6DoF movement develops a more accurate cognitive map compared to 3DoF or purely joystick-based navigation \cite{walking2011, Pastor2024}. When users can physically walk around, rather than just pivot in place or point a controller, they gain a richer \enquote{movement fidelity} of real-world locomotion. 

Beyond navigation, 6DoF also permits precise placement of virtual objects. Instead of pointing a device (e.g., controller) or hand gesture in 3DoF to interact with graphical user interfaces (GUIs) situated spatially around them, with 6DoF, users can physically move to the target location and place those virtual directly \cite{kari2023scene}. This approach magnifies the realism of XR soundscapes in 6DoF audio experiences for storytelling or entertainment. 
For instance, many 360$\degree$ cinematic experiences are in 3DoF and allow the user to look around the scene as if they were also co-located with the actors. Video games involving 6DoF interactions facilitate realism due to the envelopment of a user's entire body as an input device to interact with the content, encouraging direct engagement with XR content \cite{belonging2024}.  

Spatial audio is central to audio augmentation, such that virtual sounds are perceived as emanating from specific locations in 3D space \cite{Ruminski2015}. Humans rely on multiple sensory modalities when they engage with their environment \cite{new_ears_2024, metatla2016tap, kari2023scene}, and the auditory sense remains highly significant for localization even when visual cues is limited, sometimes replacing visual information altogether (e.g., \enquote{watching} television from another room) \cite{correa2023spatial, nonspeech1994}. Research indicates that the use of spatial audio encourages users to adopt a more active role in spatial navigation, leading to even more accurate cognitive maps \cite{Clemenson2021}, while simultaneously reinforcing the sense of presence in XR environments \cite{Kern2020-nh}. As reported in \cite{new_ears_2024}, audio interaction techniques should present directional and distance cues in contextually meaningful ways. Finally, an enhanced feeling of immersion often requires not just \enquote{looking and hearing around}, but also \enquote{moving around} in 6DoF to achieve the full benefits of spatial localization \cite{farnell2010designing}.


\subsection{Sound Experiences in XR}
%%At the end of 
%-- XR authoring tools for XR experiences (prior works have been primarily focused on this; this is where we are different) -- We are focused on the affordances of XR can have for 3D/spatial sound design in general with the inclusion of 3dof/6dof 
%Tools for creating music and soundscapes in AR and VR are primarily centered on composition with virtual instruments. 
Sound experiences in XR focus on interactions that leverage multimodal GUIs afforded by AR or VR head-mounted displays (HMDs), providing audio experiences that are more expressive and facilitate engagement for users. A notable example is \emph{Spatial Orchestra}, an AR interface, presented by \citet{kim2024spatial} which allowed users to walk into \enquote{bubbles} in a fixed position and co-located near each other emitting musical notes, that can only be heard once they were fully inside the bubble. This provided users with an audio experience where they could interact with music with their body without having to play a traditional instrument; although in order for the interface to work, users had to remain within a confined virtual space that was 3.3 m by 3.3 m, thus limiting their translational movements. This work is one example of audiovisual experiences that have taken advantage of the spatial cues to visualize and interact with audio using methods to render 3D visualizations of sound that are reported in the literature to form stronger connections with the audio content engaging multiple senses of users due to the multimodal feedback \cite{bilbow2022evaluating,kim2024spatial,agrawal2019defining}. Other forms of sound experiences in XR include VR or AR music-based video games like \emph{Beat Saber} where users wield a saber to hit oncoming blocks containing parts of the song's beat \cite{beatSaber2024}. In this game users are able to stand up and use their body's orientation in a fixed location to interact with the game. The visual-aural feedback provided in music-based video games provide an engaging experience where musical notes tend to take a physical form engaging users in a multisensory experience. These works consist of different combinations of haptic feedback, visual representations, 3DoF, or physical interactions.  However, these approaches are experiential and do not allow for precise control of audio to compose soundscapes intended for 3D audio, for example, music, cinematic scenes, or video games. To illustrate, \cite{murphy2010spatial} and \cite{sinclair2020principles} discussed the use of spatial sound in VR for creating immersive virtual environments with a focus on computer games.

Research in audio augmented reality (AAR) has leveraged the physical environment to augment auditory feedback rendered to the user targeted \emph{navigation and location-awareness}, as spatialized \enquote{audio beacons}, verbal instructions, or guides \cite{zoo_guide_2007, audio_stickies_2013}, studies have underscored its potential for diverse domains, including  \emph{presentation and display}, particularly in museums or archaeological sites that use audio augmentations to deliver contextual information about cultural artifacts \cite{Hatala2005, archeological2012}, or increase engagement with visitors at art exhibitions through spatialized sonic artwork like \emph{Sonic Sculpture} \cite{martin2020sonic}. 

\subsection{XR tools for Sound Design and Audio Mixing}
 A subset of design tools in XR utilize the flexibility of a spatial interface to interact with floating GUIs comprising of drag-and-drop UIs that can be manipulated with controllers. A notable commercial example is \emph{DearVR}, which uses virtual knobs and faders with a similar appearance of a music production studio \cite{dearvr2024}. Several research works in sound design \cite{jiang2023spatializing,bargum2023spatial} have utilized a VR headset to recreate a virtual mixing environment, similar to the panning tools in DAWs, where users can control audio objects positioned at a distance with a controller and raycast to select the objects while remaining stationary.% while they enabling them to mix in a virtual world . 
 While all of these tools utilize some spatial aspects applied to sound design, they rely on a stationary user position and none of them exploit 6DoF direct manipulation, in which users can physically move around the environment to locate and place virtual audio objects. 
By contrast, AudioMiXR integrates 6DoF direct free-hand manipulation into the audio mixing workflow, allowing the user to traverse a real or virtual environment, directly grab virtual audio objects, and reposition them wherever they see fit -- this way, enabling true \emph{hands-on} editing of spatial sound as if the users were \enquote{inside the panner}, in ways conventional DAW panners or existing VR mixing tools cannot. 

% localization of sound and visual elements for object placement that can be facilitated with 6DoF. %%%% Can specify theirs is not 6DoF
%--- ADD THAT WE DO DIRECT INTERACTIONS FOR MIXING WHERE THE MIXING TOOLS SPECIFICALLY DO NOT
%Another example is LeMo which leverages a virtual MIDI-based tool in VR similar to MIDI controllers used by producers and DJs and they found their interface to enable collaboration for multiple users and demonstrate strategies for communication among users \cite{men2018lemo}
%These interfaces and others in sound design research for XR design interfaces that do not fully depend on the users position, translation, and orientation to assist in creating the soundscape or mix.


%Studies have shown that the integration of auditory events is essential for a holistic AR experience \cite{yang2022audio} -- spatial audio cues can improve depth perception in AR environments, increase the sense of presence and task performance, as well as the overall immersion and realism of the AR environments  \cite{billinghurst1998, zhou2004, sodnik2006}.

%Although initial efforts in audio augmentation targeted \emph{navigation and location-awareness}, as spatialized \enquote{audio beacons}, verbal instructions, or guides \cite{zoo_guide_2007, audio_stickies_2013}, recent studies have underscored its potential for diverse domains, including  \emph{presentation and display}, particularly in museums or archaeological sites that use audio augmentations to deliver contextual information about cultural artifacts \cite{Hatala2005, archeological2012}, or increase engagement with visitors at art exhibitions through spatialized sonic artwork like \emph{Sonic Sculpture} \cite{martin2020sonic}. \cite{murphy2010spatial, sinclair2020principles} discussed the use of spatial sound in VR for creating immersive virtual environments with a focus on computer games.Beyond gaming, audio-visual augmentations have been embedded in a variety of prototypes for \emph{entertainment and recreation}, such as the AR/DJ system \cite{stampfl2003ardj}, which introduced
% visualizes sound sources in a virtual 3D model of the dance floor.real-time 3D sound placement in a club setting by allowing the DJ to visualize a virtual 3D model of the dance floor, %offering DJs the ability to place 3D audio objects anywhere in the room. 
\begin{comment}
    \begin{itemize}
    \item \cite{su2024sonifyar} presents an augmented reality authoring tool that generates a sound library depending on the context of the scene (e.g., wooden table, virtual object materials) for authoring sound in augmented reality. They have a good example in their experiment where an augmented reality virtual robot is situated on top of a table and the recommended sounds to assign to this virtual element resemble metal walking on a wooden table. 
    \begin{itemize}
        \item "For example, imagine sliding an AR tea cup across a real-world surface such as a wood table. SonifyAR observes this user action (a slide gesture), the action source (the user), and the action target (a virtual ceramic teacup), infers scene information such as the surface material (a wood table), and uses a custom AI backend to recommend, retrieve, generate, or sound-style transfer sound effects."
        \item 
    \end{itemize}
    \item \cite{wu2024scene} Describes an automated sonification method based on data from 3D sensors (LiDAR) to map appropriate sound to the 3D environment based on the dimensions of the physical objects captured by the sensors. 
    \item The state of the art for sound authoring or sonification methods for 3D or AR environments primarily relies on automated processes that do not consider human perception -- This is what separates AudioMiXR from existing methods. AudioMiXR is intended to be a flexible UI approach where users leverage free-hand manipulation to mimic real world actions of placing objects and AudioMiXR relies on the human perceptual system in order to place objects 'accurately' (subjective). People share common perceptual systems which implies that a more 'direct' approach towards sound design in augmented reality or 3D environments may allow for more realistic spatial audio renderings that are representative of how we perceive sound in real life. 
    \item \cite{yang2019audio} Spatial audio authoring tool using ArUco markers.
   
\end{itemize}
\end{comment}





\section{DESIGN CONSIDERATIONS}

To inform the early design of AudioMiXR, we conducted semi-structured formative interviews with four professional audio mixers % at Dolby Laboratories 
(E1, E2, E3, E4), and followed sound design principles \cite{farnell2010designing} and AR design guidelines \cite{ar_design_heuristics_2017} to ensure AudioMiXR aligns with both established spatial audio mixing conventions and ergonomic AR design. We identify experts as people who use DAW software at a professional capacity and recruited from an industry-leading company specializing in audio technology. The insights from the interviews are listed next.

\subsection{Expert formative interview}
For our expert interviews, we adapted the structure of the formative interview method by \cite{adobe2020q}. Each one revolved around a set of 12 questions designed to uncover established spatial audio workflows, technical challenges, and preferences. These were used to design AudioMiXR by maximizing the prospective benefits of an AR interface. Refer to the appendix \ref{formative-interview} for the complete list of questions.

\subsubsection{Mixing Workflow}
We began by asking experts to describe their general processes for mixing and panning spatial audio, as well as the software tools they typically use. When describing, experts emphasized how they use audio object placement in 3D to support the narrative of sonified experiences and how the visual-aural feedback is key to create a compelling experience. E2 noted that if a visual on-screen object moves, the sound should follow its motion to create a convincing illusion that they are \enquote{\emph{in sync}} -- an observation that highlights the importance of perceptual alignment between visual cues and audio sources. This notion reflects the AR design heuristic of \emph{alignment between physical-virtual worlds} \cite{ar_design_heuristics_2017}, ensuring that the listener's perceptual system \enquote{\emph{automatically accepts}} the audio-visual synchronization.
%Expert E2 highlighted how they use audio objects placed in 3D to support the story of  sonified experiences and how the visual-aural feedback is key to create a compelling experience: \emph{\enquote{an audio object... The advantage of that [...] you can put it wherever the sound is supposedly emanating from [...] So, it helps create the illusion. [...] It's in sync with that visual thing that I'm seeing. When it moves, the sound moves [...] that means that sound is coming from that thing. Your brain buys it. You just automatically accept that}}. This emphasis on matching visual cues with corresponding sounds reflects the need for perceptual alignment.   %--> Can reference AR principles
Experts stressed that the overall focus of a spatial mix is to create a \emph{compelling} environment -- realistic or stylized -- that meets the goal of \enquote{\emph{telling a story or supporting}} a particular user experience. In other words, the spatial mix must reflect the intended function, whether it is cinematic storytelling, gaming, or sound meditation, mirroring the AR design heuristic of \emph{fitting with the user environment and the task} \cite{ar_design_heuristics_2017}.
%The focus of a spatial mix is generally to create a compelling environment and the approach specifics are highly dependent on the goal; as E2 said: \enquote{\emph{the low level goal is to create a compelling, hopefully interesting environment that helps tell the story or support the gameplay or support whatever it is you're trying to do}}.  

\subsubsection{Spatialization}
%The discussion then shifted to mixing specifics, such as spatialization strategies. 
When asked about specific spatialization strategies, E1, E2, E3 agreed that it is ultimately \enquote{\emph{done to taste}}, although certain heuristics commonly apply. 
%Our experts agreed that again it comes down to the goal, what kind of sonified experience one tries to convey to the listener, and the audio mixer's taste. E1, E2, E3 agree that there are some heuristics and templates, but dependent on the application: 
For instance, low-frequency elements (like bass or kick drums) often remain front and center, while higher-frequency elements might be placed in overhead channels. These practicalities resonate with how the human perceptual system associates certain frequency bands with location \cite{karla2010vision, gibson1997art}, %and closely relates to the sound design principle of \emph{layering} \cite{farnell2010designing}: 
such that the audio object arrangement feels coherent to the listener's mental model of the environment. In other words, it should be familiar enough so that the listener's expectations are met (i.e, \enquote{\emph{the waves should be there, the birds over there}}).% putting the sounds out there to create a very compelling environment.
%\emph{\enquote{it's all done to taste. In general, I tend to put backing vocals in the Left wide or Right wide (LwRw) or sides} -- E1}, \emph{\enquote{I consider the frequency range of the elements and where in space they make sense i.e. high frequency sounds might be placed as height objects. [...] Often, a kick drum or low-frequency instrument will stay front and center, as will vocals.  But other instruments have more freedom to be placed and panned in the mix, according to taste.} -- E2},  \emph{\enquote{if you're just doing music only kind of mix, then you're gonna stay, stay with some kind of the normal conventions, like [...] the bass and bass and kick turn, usually dead center. [...] so there's kind of some some templates, or some kind of traditional ways you do things like. [...] It comes down to like, is there picture involved? [...] Or is it, is there no picture and you just trying to create a lush environment, is it some kind of a relaxation or a health and wellness kind of meditation thing?} -- E2}.
%Experts noted that the intended environment that they are trying to create, no matter how real or fantastic, has to be recognizable to the listener, in other words, familiar enough so that expectations are met (i.e, \enquote{\emph{the waves should be there, the birds over there}}) putting the sounds out there to create a very compelling environment. This practicality resonates with the AR principle of fitting with the user environment and the task. 


\subsubsection{Challenges of Spatial Audio}
The experts identified localization issues while mixing -- particularly front/back confusion in binaural rendering -- as a frequent challenge.
%We inquired about the challenges or difficulties they encounter when working with spatial audio. One of our experts noted how challenging it is to localize audio objects in binaural rendering, referencing the front/back confusion phenomenon.
They noted that point-source audio objects can fail to convey an effective sense of immersion if the sounds remain narrowly localized or singular. 
%Our other experts expressed their frustrations with creating effective immersive mixes with point sources. 
Both E1 and E3 mentioned how changing the size or duplicating audio objects %could mitigate this problem: 
to occupy more of the soundscape could address this issue by providing multiple directional cues or a broader \enquote{\emph{spread}}.
%\emph{\enquote{sometimes using size (referred to as \emph{spread}) or doubling parts and adding different effects helps} -- E1}, 
%\emph{\enquote{it could be a point source, or [...] you can incorporate the whole space. If you make it the whole space, now that kind of defeats some of the purposes, because now it's like everywhere, right? As opposed to, I'm trying to create this 3D spatial audio environment, and I want it to be right there, and I want the water to be over there, and I want something else to be something else to be over there} -- E2},
%E2 also emphasized how changing size and duplicating audio objects can help create interesting spatial audio effects: \emph{\enquote{maybe, instead of a cool nature thing, you were trying to do Something scary. It was a dark kind of industrial space that was edgy [...] kind of a scary sound, and you wanted it to be a big part, but you only had, like one one element. SO you could take that [...] and you take its size and you make it bigger so it incorporates that whole size of that whole side of the room. And maybe, depending on the tools you have, you take it and duplicate it, maybe change its pitch just a little bit, just maybe a few cents. And now those two things would beat on each other if they crossed [...] Maybe that's cool, or maybe you're like, No, I don't like that. You physically create the size of them so that they, they don't overlap in the in the system}}. 
Such references to \emph{spread}, \emph{duplication}, and \emph{overlapping elements} yet again reflect the sound design principle of \emph{layering} \cite{farnell2010designing}, where combining or stacking multiple sounds adds depth and complexity to the mix. It also connects with the AR principle of \emph{form communicates function} \cite{ar_design_heuristics_2017}, since manipulating the visual appearance of an audio object maps directly to certain auditory attribute changes, creating affordances.
%Adapting audio object sizes or duplicating them also ties to the idea that the form communicates function -- essential idea in AR --> affordances.


\subsubsection{Collaborative Design}
Because spatial audio often involves interdisciplinary collaboration, we also inquired about the social and collaborative aspects of mixing. 
%Expert audio mixers like to gather feedback from listeners and adjust their mixes iteratively, although for the actual mixing, E2 noted that typically work happens offsite (remote work). In the reality of these days, collaborators often exchange binaural versions and provide notes after listening with headphones. 
In current workflows, experts typically exchange binaural mixes remotely, receiving asynchronous feedback from listeners. An AR environment could potentially streamline this iterative process by allowing co-located or remote collaborators to view and hear each other's manipulations in real-time. A scenario where collaborators can be in the same room with an AR mixing interface synced with loudspeakers in the physical space or while one `operator' mixes was discussed.

\subsubsection{Future Directions}
We also asked participants about notable successes in their spatial audio practices, and future directions that might enhance their workflows. General directions pointed toward improving the UIs to be more user-friendly and intuitive and, interestingly, E3 hinted at how a more physical approach could make mixing easier and more accessible: \emph{\enquote{certainly using body movement to create and automate panning.  You could almost \enquote{dance the mix}.  This also opens up accessibility} -- E3}. This notion of using one's body movement and gestures directly touches on the applicability of AR in an audio mixing task, encouraging gestures that feel natural rather than fatiguing, referencing the AR design heuristic of \emph{fitting with user's physical abilities} \cite{ar_design_heuristics_2017}.

\subsubsection{Evaluation of a Mix}
Finally, experts reflected on how they typically evaluate the quality of a spatial audio mix, describing metrics for success. For instance, they consider whether it \enquote{\emph{sounds more interesting than [in] stereo}} or whether it has \enquote{\emph{a good sense of height, width, depth, externalization, and timbre}}. Although these mostly rely on subjective measures of audio mixer's taste, E2 noted: \emph{\enquote{the one thing that can be truly measured is the loudness}}. Generally measured in LUFS units for audio mixing applications, \emph{loudness} serves as an objective metric for ensuring that the final mix meets the delivery standards across different platforms. %--> maybe link to Minimize distraction and overload AR design principle, unsure.

%% Should add some final thoughts?
\subsection{Key Takeways}
Experts in our formative interviews revealed several core considerations for spatial audio mixing in AR which we took into account for designing AudioMiXR. Foremost, we clarified why AudioMiXR could be better than traditional 2D audio panning interfaces. Its advantage lies in the natural conversion of virtual audio objects in a 3D space, which makes it easier for creators to mold compelling experiences for diverse applications. Experts stressed that ensuring audio objects are synchronized with their corresponding visuals is crucial for immersion. Moreover, experts highlighted the benefits of a more physical, gesture-driven interface. Following on their feedback, we made sure to design AudioMiXR with real-time aural-visual feedback and to provide free-hand spatial manipulation within our interface. However, at the same time, localization challenges remain an obstacle. We leverage the audiovisual sensory component of our AR interface since humans are better at that than in unisensory localization scenarios \cite{odegaard2015biases, crossmodal2016chi}. 

Our experts also recognized the collaborative nature of spatial audio production. An AR approach could support more synchronous or alternative forms of collaboration -- allowing multiple users to see and hear each other's changes in real time. In all, the interviews point to the promise of AR-based mixing for bridging the mapping gap between the virtual spatial mix and the actual physical implementation and delivery. Taken together, these insights form the backbone of AudioMiXR. We present its detailed system design in the following section.

%In developing AudioMiXR, we were guided by sound design principles -- (layering, space, and rhythms/timing?). \emph{Layering} involves combining multiple sounds to achieve a sense of depth and complexity within the mix, whereas \emph{space} refers to the use of reverb to create 3D acoustical depth. \emph{Rhythm/timing} ensures that all sonic elements align harmonically and temporally, for coherence in the arrangement. Additionally, preliminary research and interviews with audio professionals clarified why AudioMiXR could be better than traditional 2D audio panning interfaces. 


%The questions listed in the template below served as a starting point for the interviews, though the conversation was encouraged to evolve and flow naturally.

\section{SYSTEM}

This section describes the AudioMiXR system, an XR user interface for free-hand manipulation of spatial audio. We detail the system architecture, and provide an overview of user interaction mechanisms and the standard workflow.

\subsection{System Architecture}
To enable free-hand spatial audio manipulation, AudioMiXR relies on several hardware and software components.
%Figures \ref{fig:unity-ui} and \ref{fig:side-by-side} illustrate key components of AudioMiXR: hardware sensors,  software modules, and user gestures intersect to enable free-hand spatial audio manipulation. 
On the hardware side, the system runs on an XR-capable HMD which provides a built-in device camera and depth sensors to capture the user's hand gestures and movements, producing 3D coordinates for real-time gesture recognition. Additionally, the inertial measurement unit (IMU) within the HMD tracks the position and orientation relative to a defined origin. This ensures that the system consistently aligns the user's real world head movement with the superimposed virtual environment.

On the software side, AudioMiXR is comprised of a suite of modules. First, an \emph{image tracking and gesture recognition} module processes the depth-camera feed and applies HMD's native computer vision algorithms to interpret how the user's hands move and which gestures they perform. Next, a \emph{3D mapping} module translates these hand positions into virtual coordinates, accurately tracking and registering the hand positions within a 3D coordinate space. A \emph{gesture I/O} module then assigns a class of a specific gesture to input and output functions in order to interact with the visual 3D audio objects.  To render and update these virtual audio objects, AudioMiXR has a \emph{3D simulation} module -- whenever the system detects a relevant user gesture, the 3D simulation module handles how the the visual 3D objects should respond to the users input (e.g., the user moves their hand up and down along the vertical axis then the audio object vertically translates in accordance to the hand movement and positioned at the extent of their fingertips). The \emph{communication module} updates the 3D hand coordinates and visual audio object positions for a set interval while the application is running (e.g. updates audio object position once every 30 frames via USB or wireless protocol). 
The HMD's spatial operating system, processor, and memory provide the necessary computational resources to execute computer vision algorithms, maintain stable 3D rendering, and track hand gestures while simultaneously updating audio objects.

\subsection{Deployment Target}
Although both AR and VR share common goals of augmenting user environments, they differ in key respects. 
VR often disconnects users visually from their surroundings, risking collisions or accidents while they are interacting with the digital twin world without having cues on their real environment \cite{interactions_vr_2024, vrtoer2023}. Even when using VR safety measures, such as boundaries (e.g., Meta Quest Guardian System) users may still break them, and safety comes at the expense of immersion \cite{interactions_vr_2024}. VR immersion can also induce \emph{cybersickness} -- a visual-vestibular conflict from motion in a virtual world -- which may intensify if, for instance, users rely on teleportation for locomotion \cite{new_ears_2024} or omnidirectional treadmills for infinite-walking \cite{treadmill_2024}, which even after proper training do not provide firm locomotion, but rather an awkward gait \cite{walking2011}. Ensuring perceptual-motor fidelity between a user and their digital-twin avatar can mitigate these negative effects by \enquote{virtual body ownership} \cite{hands2016, 10.1371/journal.pone.0010564}. 

Conversely, AR situates virtual content onto physical environments, allowing users to interact with virtual elements as though they were truly present in their surroundings \cite{sodnik2006, Ruminski2015, yang2022audio}, often avoiding some of VR's drawbacks, as users retain awareness of their physical environment \cite{Steffen03072019, interactions_vr_2024}. %Early research in both AR and VR commonly focused on focused mostly on visual augmentations \cite{dam2024taxonomy, yang2019mattern}, yet subsequent works made it evident that any human sense can be augmented. In particular, the augmentation of the physical world with \emph{virtual soundscapes} through spatially coherent auditory cues has existed for decades\cite{aar1993}. 
Given these considerations, our user study focused on the AR setting, although future extensions could address VR scenarios, since AudioMiXR runs on headset capable of supporting both AR and VR modes. While the immersive 6DoF capabilities in AR and VR are comparable in principle, VR introduces additional aforementioned caveats that might require further design adaptations.


\subsection{User Interaction and GUI}

In AudioMiXR, we utilize rendered computer graphics to depict 3D audio object 
manipulations (e.g., position in space or distance from the listener). The visual representation of each sound objects is modified through a free-hand pinch gesture used for selecting and holding objects.
AudioMiXR uses simple, direct gestures to give the sensation of physically touching and/or moving virtual audio objects in the air. As shown in Figure \ref{fig:side-by-side}, users can \enquote{pinch} a virtual audio object to grab it, \enquote{push} it further away, or \enquote{pull} it closer -- even if they remain standing at a distance. 

\begin{figure}[ht]
    \centering
    \begin{subfigure}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/paper/audiomixr1.png}
        %\caption{unit less cube}
        %\label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/paper/audiomixr2.png}
        %\caption{user interactions}
        %\label{fig:sub2}
    \end{subfigure}
    \caption{(Left-to-Right) AudioMiXR enables users to reposition audio objects within their reach envelope through direct gestural interactions, giving the impression that their hands are physically in contact with the objects. Users can push objects beyond their reach envelope by targeting the object with a pinch gesture and performing a pushing motion. If an object is out of reach and the user prefers not to physically walk towards it, they can align their pinch gesture with the object in their line of sight and \enquote{pull} it back into their reach envelope.}
    \label{fig:side-by-side}
\end{figure}

\subsection{User Workflow}
Once calibration is complete, the user can start the session by booting the XR interface, which initializes the \emph{gesture recognition}, \emph{hand tracking}, and \emph{body tracking} modules. Once in the environment, the user can freely grab any virtual audio object by pinching it (Figure \ref{fig:user-worklow-example}) and reposition it within or beyond their physical reach. If the target object lies out of each, the user may \enquote{pull} it back by targeting it with the gaze, performing the pinch gesture, and making a dragging motion. Changes in position immediately translate to perceptible alterations in audio attributes.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/6_lut.png}
    \caption{A user interacting with virtual audio objects in AR using direct free-hand manipulation. }
    \label{fig:user-worklow-example}
\end{figure}



\subsection{Implementation}
We developed AudioMiXR using the Unity game engine (runtime version 2022.3.30f1) in C\#, deploying it on an Apple Vision Pro (AVP) HMD, on its native spatial operating system visionOS 1.0. We used Unity for its robust developer community and comprehensive documentation and support for rendering and interaction. %However, the AVP-specific documentation and developer discussions are currently paywalled and exclusive to certain licensing tiers. 
We used a Unity Professional license to gain access to the necessary AVP deployment modules and visionOS-specific libraries, and an Apple Developer license to sign, build, and deploy AudioMiXR to the AVP HMD.   % According to https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7333561 Unity3D is one of the best documented game engines out there

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/mockup_full_4.png}
\caption{(A) Hierarchy in the Unity3D engine showing all the game objects; (B) A user featuring the headset with a tracked pose driver enabling 6DoF movement and gesture-based interaction with a virtual audio object; (C) Configuration settings for Unity Engine's spatial tracking driver; (D) Settings of the audio source element attached to the virtual audio object. }
\label{fig:unity-ui}
\end{figure}

\subsubsection{Hardware and tracking.} The AVP provides 6DoF tracking through its built-in IMU sensors and LiDAR-based environment mapping, allowing real-time position (right/left, up/down, forward/back) and orientation (pitch, yaw, roll) updates. As illustrated in Figure \ref{fig:unity-ui}.C, we leverage Unity's \emph{Tracked Pose Driver} component on the HMD camera to enable positional and rotational tracking, configured to update sample tracking input right before the rendering step %\emph{Before Render} 
for minimal latency. Our \emph{XR origin} in Unity contains the audio listener -- a component that captures the incoming sound -- and references the HMD's head pose using the XR Interaction toolkit, while an additional \emph{AR session} object manages environment tracking (position and rotation) via ARKit, provided by the Unity AR Foundation library. To support free-hand spatial audio manipulation, we incorporate the AVP's integrated 3D touch input using the PolySpatial engine, allowing users to \enquote{pinch} virtual objects and reposition them in 3D space when recognized by the AVP's cameras (Figure \ref{fig:unity-ui}.B) . %hen the user performs a pinch gesture recognized by the AVP's cameras (Figure \ref{fig:unity-ui}.B), they can grab and reposition any virtual audio object.
For audio playback, users can opt for AVP's native near-ear headphones or Bluetooth-enabled headphones. 

% UnityEngine Enhanced Touch
% Apple VisioOS XR PLugin
% Apple ARKit XR Plugin
% AR Foundation
% PolySpatial
% PolySpatial visionOS
% PolySpatial XR
% rendered with RealityKit. Materials and shaders are translated to this new environment. PolySPatial takes care of this translation, materials, mesh renderers

%Input types: look and tap (input colliders) or direct touch, hands and head pose tracking gives precise information about each hand joint and the viewer's head position relative to global tracking origin. Low-level hand data is provided via Unity's Hands package and head pose is provided through the input system. 
%AR data from ARKit (plane detection, world mesh, image markers) in Unity AR Foundation.
%XR Interaction toolkit (XRI) --> abstraction from input to 3D interactions within app 

%Licenses:
%Unity Professional license to deploy app to AVP
%Apple Developer license to deploy app to AVP

\subsubsection{Audio objects and interaction.} Each audio object in our scene (Figure \ref{fig:unity-ui}.A) is represented as a spherical \emph{GameObject} Unity's fundamental object type, with the following added components: a Unity \emph{audio source} component (Figure \ref{fig:unity-ui}.D) configured for 3D spatialization with logarithmic rolloff for distance-based attenuation; a custom \emph{opacity modulation script} that adjusts visual transparency relative to the distance from the \emph{XR origin} (see left subfigure in Figure \ref{fig:AR-panner-reverb-settings}); a custom \emph{pulse-transferring script} employing a Fast Fourier Transform (FFT) with a Blackman window to sample the audio signal in real time. We segment the FFT result into frequency bands and dynamically scale each sphere based on the amplitude of the selected band (e.g., high-frequency kicks cause the sphere to pulsate, providing immediate visual feedback). 
Our scenes feature multiple audio objects representing distinct sound samples. To accomodate large spatial layouts, we set Unity's \emph{volume camera} configuration to \enquote{unbounded}, giving users freedom to walk around and place objects anywhere in their physical environment. Unity's PolySpatial engine also takes care of converting Unity's materials and mesh renderers into AVP's native rendering.
%Unbounded volume displays in a full space with no dimensions and Unity units mapped to real-world units 


\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/audiomixr4_full_3.png}
    % Try figures/paper/audiomixr4_full_log.png too (without second reverb)
    % Or figures/paper/audiomixr4_full_2.png (without log scale)
    \caption{(Left) Schematic representation of the AR panner. The spheres represent virtual audio objects, and the color saturation represents the level decay based on distance from the audio listener located at the HMD to the audio spheres. The $X$-$Y$ axes quantify the amount of room response in the binaural rendering setup; (Right) The plot displays the 3D sound settings in the game engine. The virtual audio object level displays a logarithmic decay based on distance from the audio listener. For the reverb setting, we experimented with a uniform, full-strength reverberation (\enquote{Reverb setting 1}, our default setting for the user study) and a monotonically increasing reverb mode that adds more ambience as the distance from the listener grows (\enquote{Reverb setting 2}).}
    \label{fig:AR-panner-reverb-settings}
\end{figure}

%\nb{Describe binaural rendering in Unity}.

\subsubsection{Binaural rendering and reverb.} Unity's native audio engine allows binaural rendering by applying 3D sound settings (Figure \ref{fig:unity-ui}.D) that simulate how audio signals reach each ear from different directions and distances. While additional proprietary plugins exist for HRTF modeling, AudioMiXR relies on Unity's built-in audio spatializer.
%, where the HRTF filtering is based on a modified version of the (MIT Media lab's) KEMAR data set. KEMAR --> head/torso simulator for acoustic measurements 
For that reason, we manually deactivated AVP's native spatializer. We also experimented with two reverb schemes for the audio objects. The first, \emph{constant reverb}, %set to full reverb effect with no tapering off -- it stays the same strength for the entire distance range, as result, the entire space feels like one large, uniformly reverberant area, which we used as our default option during user studies. 
applies a uniform reverberation effect at full strength across all distances between the virtual audio object and the audio listener, creating the sensation of a large, uniformly reverberant area. This setting served as the default option during our user studies. As an alternative, we used the \emph{distance-based reverb} mode in our supplementary demos, which starts with no reverb at close range and increases logarithmically with distance. This approach provides an effect of ambiance and space as objects move farther away from the listener. The right subfigure in Figure \ref{fig:AR-panner-reverb-settings} illustrates both the logarithmic distance-based attenuation and the corresponding reverberation curves.

\section{METHOD}

To evaluate AudioMiXR, we conducted a formal user study  designed to uncover usability issues. % this paragraph sounds kinda dry


\subsection{Formal user study}
We recruited 27 participants via snowball sampling at Dolby Laboratories, which is typically sufficient to reach saturation of recurring themes in participant responses \cite{guest2006many,iftikhar2023together}. The sample included 17 male and 10 female participants, ranging in age from 22 to 59 ($\mu=31.22, \sigma=8.70$). Participants have diverse audio mixing  and AR backgrounds, which we used to categorize them into DAW \emph{experts} or \emph{non-experts}. Specifically, those who self-reported as \emph{unexperienced}, \emph{beginners}, or \emph{intermediate level} DAW users were considered \enquote{non-experts}, while those who self-identified as \emph{advanced} and \emph{expert} were considered \enquote{experts}, resulting in 18 non-experts and 9 experts.
%, and 23 AR non-experts and 4 AR experts.
This study aimed to quantitatively measure workload, observe user interaction patterns, and collect qualitative feedback on AudioMiXR's usability and potential professional applications. We used the AVP without optical inserts; %(used for vision correction);
consequently, we collected participants' glasses prescriptions and diopters, if available (for control). Of the 27 participants, 10 had perfect vision and 17 had different mild eye conditions (e.g., mild nearsightedness or astigmatism) %ranging from mild, mid,  strong, and/or astigmatism, and one participant reported having amblyopia and harmonic anomalous retinal convergence, 
but felt comfortable taking part in the experiment without vision correction.

%Eye conditions: https://www.nei.nih.gov/learn-about-eye-health/eye-conditions-and-diseases

%10 had perfect vision (1 wore contact lenses for correction), 6 self-reported mild nearsightedness (left eye $\mu=-0.88, \sigma=0.25$, right eye $\mu=-0.44, \sigma=0.52$), 2 had strong nearsightedness (diopters recorded for one participant only: left eye $-6.75$, right eye $-6.5$), 4 self-reported mild farsightedness (left eye $\mu=+0.38, \sigma=0.18$, right eye $\mu=-1.12, \sigma=0.88$), 24 reported no astigmatism, 4 had astigmatism (left eye $\mu=-0.83, \sigma=1.38$, right eye $\mu=-0.66, \sigma=0.76$), and 1 participant reported having amblyopia and harmonic anomalous retinal convergence, which affect their 3D vision and depth perception. 

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/audiomixr3_v2.png}
    \caption{(Left) Schematic representation of the room where the user study was run, the dimensions of the room were $7.62$m in width and $6.09$m in depth. The origin was marked right at the center of the room with a white cross on the floor; (Right); Snapshots of users interacting with AudioMiXR during the user study--a variety of interesting user behaviors were capturing, including but not limited to: crouching, attempt to manipulate audio objects with two hands at once, placing audio objects over the head, etc.; (Bottom) Panoramic view of the room where the user study was run.}
    \label{fig:experiment-setup-room}
\end{figure}


\subsubsection{Setup.} The study took place in a room measuring $7.62 \times 6.09$m (width $\times$ depth), as illustrated in Figure \ref{fig:experiment-setup-room} (left). Participants wore an AVP HMD and stood at the origin point marked with a cross on the floor. We positioned an additional camera near one wall of the room to capture each participant's movement, while the AVP simultaneously recorded the first-person view of virtual audio object manipulation.

\subsubsection{Procedure.} We ran a within-subjects study where each user attempted the instructed directed tasks in 6DoF. Each session began with a brief \emph{calibration phase} ($\sim$2 minutes), in which participants aligned hand-tracking and eye-tracking for the AVP, followed by a \emph{training phase} ($\sim$3 minutes) during which the participants were able to practice essential gestures -- pinching to grab and reposition virtual audio objects, walking around them (clockwise/counter-clockwise), and moving objects closer or farther away to observe changes in audio attributes. After the training session, participants were instructed to complete tasks in two distinct \emph{scenes}, each containing five unique audio objects. We choose five as a lower bound of \emph{Miller's rule} \cite{miller1956magical} -- which defines $7\pm2$ as the number of objects an average human can hold in short-term memory -- to leverage the attention span of our participants \cite{farnell2010designing}. In the \emph{music mixing scene} (Figure \ref{audio-mixing-scene-example-1}), users placed and arranged objects labeled \enquote{\emph{Synth}}, \enquote{\emph{Bass}}, \enquote{\emph{Kick}}, \enquote{\emph{Filtered Key}}, and \enquote{\emph{Drum}} to construct what they feel is the \emph{most} immersive listening experience. In the \emph{cinematic scene} (Figure \ref{fig:environment-scene-example-1}), they positioned \enquote{\emph{Bird}} chirps, \enquote{\emph{Frog}} croaks, \enquote{\emph{River}} stream, \enquote{\emph{Breeze}}, and \enquote{\emph{Campfire}} sounds to simulate an outdoor camping environment. We randomized the order of these two scenes for each participant, and each scene had a 5-minute limit (or ended earlier if participants were satisfied of their mixing output). Throughout the sessions, participants were free to navigate the room, placing virtual objects anywhere in the physical room (from floor-level to near the ceiling) to get the sonic effect they envisioned. We recorded the positions of virtual audio objects and participant's head movements. External and internal HMD cameras captured a full account of each session for subsequent analysis. 

\begin{figure}[ht!]
    \centering
    
    \begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/7_lut.png}
    %\caption{Caption}
    %\label{fig:enter-label}
    \end{subfigure}
    \\
    \vspace{1mm}
    \begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/9_lut.png}
    \end{subfigure}
    \caption{Users interacting with virtual audio objects in the music mixing scene. For illustration purposes, note that the users are using Meta Quest Pro. Since AudioMiXR has been developed in Unity3D, it can be deployed on both platforms, AVP and Meta Quest Pro.}
    \label{audio-mixing-scene-example-1}
\end{figure}


%5 audio objects 

\subsubsection{Measures.} Upon completing each scene, participants filled out the NASA-TLX \cite{nasatlx1988} to report on mental, physical, and temporal demands, as well as perceived effort and frustration (refer to the full list of questions in appendix \ref{nasa-tlx}). We used a 7-point Likert scale, where 1-3 corresponded to lower workload, 4 was neutral, and 5-7 indicated higher workload. Participants also completed a \emph{demographic questionnaire} (see appendix \ref{demographic-questions}) and a \emph{short-answer survey} (see appendix \ref{short-answer-survey}) describing any difficulties they found, suggestions for improvements, what they enjoyed the most, and -- if they were experienced DAW professionals -- potential applications of AudioMiXR in their own workflows. We also collected placement and movement logs (trajectories), identifying any patterns in how they arranged and organized virtual audio objects.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/3_lut.png}
    \caption{A user building a cinematic experience in the outdoor camping scene. Observe how the users crouches to place the \enquote{\emph{River}} and \enquote{\emph{Frog}} audio objects on the floor, and positions the \enquote{\emph{Bird}} audio object close to the ceiling.}
    \label{fig:environment-scene-example-1}
\end{figure}

\subsection{Data Analysis Approach}
Two authors conducted a thematic analysis to identify themes in participant responses in order inform design lessons for mixing with 6DoF. Open coding was done independently and followed by axial coding to form larger code groups. Lastly, the two authors formed overarching themes specific to the interactive, aural, and visual elements with 6DoF. 
%Experimenters conduct semi-structured interviews with the following qualitative questionnaire based on the AR Design Heuristics presented by \citet{endsley2017augmented} and tailored to fit our research questions. The first and third authors carried out the thematic analysis by initially performing an open coding procedure followed by axial coding independently. Afterward, they reconciled differences in the final code groups and formed themes. 

\section{RESULTS}
This section begins by presenting the quantitative results of the study, including NASA-TLX workload measures, statistical tests, and spatial visualizations of virtual audio object placement, which collectively offer objective insights about AR sound design patterns. We then present the qualitative results, summarizing various sentiments, codes, and themes on using virtual audio objects for sound design in AR. Users shared their impressions in response to our survey questions on audio object manipulation for the cinematic and music mix tasks.

\subsection{Cognitive Demands}% 

First we examined the normality assumptions with the Shapiro-Wilkinson test, which indicated that only the dimensions of Frustration, Efficiency, Satisfaction for DAW expert users followed a normal distribution; but the remaining dimensions and groups violated the normality assumption (all $p<0.05$). Consequently, we opted for the non-parametric Mann-Whitney U to assess the statistical significance of trends. A comparison of NASA-TLX scores between DAW expert and non-expert users, as depicted in Figure \ref{fig:nasa-tlx-experts}, revealed no statistically significant differences across all workload dimensions based on the Mann-Whitney U tests (all $p$-values $>0.05$), suggesting that AudioMiXR imposes a similar perceived workload on users irrespective of their prior audio mixing experience and is thus suitable even for novice users. 
Overall, participants reported low levels of mental demand ($\mu=2.30, \sigma=1.20$), physical demand ($\mu=2.22, \sigma=1.25$), and temporal demand ($\mu=1.85, \sigma=1.13$) when manipulating virtual audio objects in AR, along with low frustration levels ($\mu=2.04, \sigma=1.22$). These factors help explaining the high satisfaction scores ($\mu=5.89, \sigma=0.93$) with the system usability. Participants perceived a moderate level of effort ($\mu=2.48, \sigma=1.28$) required to complete the tasks, and the performance dimension received neutral ratings ($\mu=2.70, \sigma=1.44$), reflecting on the success of positioning and manipulating virtual audio objects accurately without excessive difficulty. Additionally, the ratings on the efficiency of completing the tasks are consistently high ($\mu=5.11, \sigma=1.45$), highlighting that participants were able to complete tasks without overhead. 

% We also tested the NASA-TLX workload dimensions for significance by comparing AR experience. Mann-Whitney U tests revealed no significant differences ($p>0.05$) across all dimensions except the temporal demand ($U=11.0$, $p = 0.0093$), where AR expert users had significantly higher time pressure ($\mu=3.25, \sigma=0.95$) than non-expert users ($\mu=1.61, \sigma=0.99$).
%% Possibly counterintuitive results: may reflect the experts' desire to spend more time to refine their mix more precisely, they felt more urgency.
% This is also shown in the application runtime: the average elapsed time in minutes for the cinematic scene for experts and non-experts is $4.37$ and $3.67$, respectively; for the music mix scene it is similar: $4.62$ and $4.03$.



\begin{figure}[ht!]
    \centering
\includegraphics[width=1.0\linewidth]{figures/paper/nasa_tlx_experts.png}
    \caption{Comparison of mean Likert scores on NASA-TLX assessment profiled by DAW expertise level (with \emph{unexperienced}, \emph{beginners}, and \emph{intermediate level} users labeled as \enquote{non-experts}, and \emph{advanced} and \emph{expert} level users classified as \enquote{experts}).}
    \label{fig:nasa-tlx-experts}
\end{figure}

\subsection{Coordinates Visualization}\label{heatmaps}

We analyzed the HMD logs from each user trial to aggregate the 3D coordinates of all virtual audio objects within each audio scene. This aggregation involved combining all logged timestamps across users and averaging the $x$,$y$,$z$ coordinates of each virtual audio object, thereby obtaining a mean 3D position for each object in each scene. Next, we binned the positions of each virtual audio object into two 2D histograms: one for the $X$-$Z$ plane (top-floor view) and another for the $X$-$Y$ plane (side view), and applied a Gaussian filter to the counts to produce smoother heat map visualizations of the average positions of each audio object class (Figures \ref{fig:heatmap-environment-scene} and \ref{fig:heatmap-audio-mixing-scene}). The color intensity within each heat map indicates the regions where a virtual audio object is more likely to be placed on average, across our trials. We observed significantly different spatial patterns between the two scenes, the cinematic and the music mixing scene, which are described below. We also computed the distances between pairs of virtual audio objects within each scene and estimated the densities of histograms of distances using a Gaussian kernel (Figure \ref{fig:kdeplots}).
% The strongest proximity relationships are those between overlapping subjects, but just grouping objects into a single area can 
These densities allowed us to compare the proximity relationships between pairs of virtual audio objects and correlate that with their semantics. 


\begin{figure}[ht!]
\centering
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xz_positions_Environment_1.png}
\end{subfigure}
\hfill      
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xz_positions_Environment_2.png}
\end{subfigure}
\\
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xy_positions_Environment_1.png}
\end{subfigure} 
\hfill
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xy_positions_Environment_2.png}
\end{subfigure}
    \caption{Heat maps of the mean audio object 3D positions in the environment scene (first row: $X$-$Z$ plane, second row: $X$-$Y$ plane).}
    \label{fig:heatmap-environment-scene}
\end{figure}

In examining the heat maps for the cinematic scene, as illustrated in Figure \ref{fig:heatmap-environment-scene}, the \enquote{\emph{Breeze}} sound is consistently positioned at the origin in $X$-$Z$ plane, corresponding to the location where the user initiates the application, and approximately at the headset height in the $X$-$Y$ plane. Because  \enquote{\emph{Breeze}} is an ambient sound, participants naturally tended to place it near their headset to emulate its pervasive presence. In contrast, the \enquote{\emph{Bird}} sound, characterized as a point-source sound, is generally positioned above the headset in the $X$-$Y$ plane. For instance, during the experiment a participant expressed they felt \emph{the birds should be placed up high like there are flying or in a tree}. The opposite occurs with the \enquote{\emph{River}} audio object, expected to emanate from the ground and, thus, heard from below. Consequently, in the $X$-$Y$ plane, it is typically situated on the floor. The \enquote{\emph{Frog}} audio object exhibits greater variance across both planes, likely because, as an animal, it could virtually be placed anywhere on the $X$-$Z$ and still be coherent with the scene. Nonetheless, it exhibited a considerable overlap with the \enquote{\emph{River}} heat map, indicating a tendency for participants to place the frog close to the river, reflecting nature-related biases. This is a possible manifestation of the Gestalt principle of similarity \cite{farnell2010designing, shelvock2016gestalt}, where the strongest proximity between subjects relates to those that have overlapping semantic associations, and led our participants to arrange those virtual audio objects together. This is also observed in the density estimates in Figure \ref{fig:kdeplots}, where the closest virtual audio object to \enquote{\emph{River}} is \enquote{\emph{Frog}}, and vice versa. Additionally, the \enquote{\emph{Frog}} audio object is generally placed below the headset height, similar to the \enquote{\emph{Campfire}} audio object. The \enquote{\emph{Campfire}} is typically positioned at the origin, close to the user. This pattern may stem from participants viewing the campfire as the central point of the camping setup, thus leaving it at the origin. Alternatively, it might be perceived as an ambient sound with a larger spread, justifying its proximity to the user.

\begin{figure}
\centering
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xz_positions_Mix_synth_kick.png}
\end{subfigure}
\hfill      
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xz_positions_Mix_key_bass.png}
\end{subfigure}
\hfill      
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xz_positions_Mix_drum.png}
\end{subfigure}
\\
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xy_positions_Mix_synth_kick.png}
\end{subfigure}
\hfill      
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xy_positions_Mix_key_bass.png}
\end{subfigure}
\hfill      
\begin{subfigure}{0.32\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/v2_heatmap_AO_mean_xy_positions_Mix_drum.png}
\end{subfigure}
    \caption{Heat maps of the mean audio object 3D positions in the audio mixing scene (first row: X-Z plane, second row: X-Y plane).}
    \label{fig:heatmap-audio-mixing-scene}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/gestalt_distances_3.png}
    \caption{Density estimates using a Gaussian kernel over histograms of distances between pairs of virtual audio objects.}
    \label{fig:kdeplots}
\end{figure}

In the music mixing scene (Figure \ref{fig:heatmap-audio-mixing-scene}), the heat maps present considerably more overlap and are primarily more concentrated around the origin (notice the range of the $X$-$Z$ axes), extending roughly to the user's reach envelope. 
In standard spatial audio panning the loudness of spatial audio objects is kept constant, only the room response is modified. 
% https://www.head-fi.org/threads/the-science-of-soundstage.959791/
In those scenarios, expert audio mixers \cite{gibson1997art, dewey2024} have noted that a common practice is to place high frequencies (called \enquote{treble} bands in sound design) higher and low frequencies (also known as \enquote{bass} bands) lower in the spatial field, reinforced by psychophysics research on congruence between pitch and visual vertical positioning \cite{bernstein1971, karla2010vision, crossmodal2016chi}.
However, we have not observed such a pattern with audio objects with stronger high-frequency content (e.g. the \enquote{\emph{Drum}}) versus audio objects with richer low-frequency content (such as the \enquote{\emph{Filtered Key}}), likely because loudness was not kept constant. In a 6DoF application, raising or lowering a virtual audio object spatially substantially increases its Euclidean distance from the headset, effectively causing larger loudness attenuation than if the object was kept at the HMD level. That might explain why we observe that most of the music mix audio objects are placed approximately at the HMD level, producing notable overlap among different objects (Figure \ref{fig:kdeplots}).

\subsection{Variance Analysis} \label{variance-analysis}

% The Levene test is less sensitive than the Bartlett test to departures from normality.
To assess the homoscedasticity of final audio object placement between music and cinematic mixes, 
%  A significant p-value below the  = 0.05 level indicates that the data samples being tested are unlikely to have come from populations with equal variances
% If the result of the p-value is less than the significance level, then the sample variances are equal.
we employed Levene's test. Specifically, we examined the distributions of $x$,$y$,$z$ spatial components of the final audio object placement, as well as the overall distances of audio objects from the origin (see Figure \ref{fig:tests-3d-gaussian}). %For both, coordinate-wise and distance-wise test,  --> talk about why both
Our analysis revealed $p$-values below the significance level of $\alpha=0.05$ for all tested components ($x$,$y$,$z$, and distance-based: $F_X=36.94$, $p<0.05$; $F_Y=30.52$, $p<0.05$; $F_Z=4.93$, $p<0.05$; $F_d=13.90$, $p<0.05$), indicating that the variances between the music and cinematic mixes are significantly different. 

To visualize and further understand these differences, we represented the experimental room setup with the final placements of virtual audio objects across all users and scenes in Figure \ref{fig:tests-3d-gaussian}. We fitted a multivariate mixture-of-Gaussian (MoG) model using the expectation-maximization algorithm for density estimation, choosing one component per mix to capture the distinct spatial distributions of music and cinematic mixes. Each component was assigned its own covariance matrix. The resulting 3D Gaussians ellipsoids (see Figure \ref{fig:tests-3d-gaussian}) illustrate the spatial dispersion of final audio object placements. The estimated means for the cinematic scenes ($\hat{\mu}_X=0.071$, $\hat{\mu}_Y=1.247$, $\hat{\mu}_Z=0.242$) and mix scenes ($\hat{\mu}_X=-0.064$, $\hat{\mu}_Y=1.324$, $\hat{\mu}_Z=0.209$) indicate slight positional shifts between the two mix types, while the variances ($\hat{\sigma}_X^2=2.654$, $\hat{\sigma}_Y^2=0.766$, $\hat{\sigma}_Z^2=1.610$ for cinematic and $\hat{\sigma}_X^2=1.016$, $\hat{\sigma}_Y^2=0.384$, $\hat{\sigma}_Z^2=1.156$ for music) reveal that audio object placement in cinematic mixes exhibit greater variability, particularly along the $X$ and $Z$ axes (width and depth dimensions of the room), corroborating the results from Levene's test. 

\begin{figure}[ht!]
\centering
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/3d_cinematic_vs_mix.png}
\end{subfigure}
\hfill      
\begin{subfigure}{0.49\linewidth}
\includegraphics[width=1.0\linewidth]{figures/paper/variance_comparison_boxplots_final.png}
\end{subfigure}
\caption{(Left) A room representation showing the final placement of audio objects across all experiments and users, color-coded by mix. A Gaussian mixture with two components (one for each mix) is fitted to the data. The cross at the origin represents the initial position, while the vertical dashed line represents the average user height; (Right) Boxplots illustrating the distribution of the final placement of audio objects along the $X$, $Y$, $Z$ axes, as well as the distribution of distances from the final audio object positions to the origin.}
\label{fig:tests-3d-gaussian}
\end{figure}

\subsection{Qualitative Analysis}
This section outlines the findings from our thematic analysis, where we organized the codes into three themes: Embodied Mixing, Mapping Sound Attributes
to AR GUIs, and Prototyping with
Spatial Interface. These themes underscore the overarching insights gathered from participant responses to provide context for our analysis and design lessons. 
\begin{table}[ht!] %% multirow package
    \caption{Final Codebook of Identified Themes, Corresponding Codes, and Exemplar Quotes}
    \centering
    \begin{tabular}{>{\raggedright}p{4cm}|>{\raggedright}p{4cm}|>{\raggedright\arraybackslash}p{6cm}}
        \toprule
        \textbf{Theme} & \textbf{Codes} & \textbf{Exemplar Quote} \\ \midrule
        \multirow{4}{4cm}{1. Embodied Mixing \textbf{(N = 20, 74\%)}} 
            & 6DoF Audio Presence \textbf{(N = 14, 52\%)} & 
                P19: \enquote{\emph{I liked being able to hear the sound coming from different directions as I moved it around, it is cool that instead of just hearing the sound around you you can interact with the sound }} \\ \cline{2-3}
            & 6DoF Audio Perceptibility \textbf{(N = 8, 30\%)} & 
                P6: \enquote{\emph{I chucked my head into a sphere, so I could hear just what was going on in there}} \\\cline{2-3}
            & Visual-Aural Responsiveness \textbf{(N = 7, 26\%)} & 
                P22: \enquote{\emph{There is a natural intuition
                on how to manipulate the objects. The immediate
                acoustic feedback made it easy to converge
                on manipulations that were most effective}}  \\ 
        \midrule
        \multirow{2}{4cm}{2. Mapping Sound Attributes to AR GUIs \textbf{(N = 18, 67\%)}} 
            & Audio Attribute Level Control \textbf{(N = 15, 56\%)} & 
                P24: \enquote{\emph{For
                example close distances always made the object
                much louder than I wanted. Id like to specify
                volume across different distances more precisely}} \\ \cline{2-3}
            & Object Appearance \textbf{(N = 8, 30\%)} & 
                P23: \enquote{\emph{I think the size of audio object should be associated with spread}} \\ \cline{2-3}                & Modularity \textbf{(N = 4, 15\%)} & 
                P25: \enquote{\emph{ I could see this integrated as a plugin in my DAW while looping / experimenting with multitrack placement.}}\\ \midrule
        \multirow{2}{4cm}{3. Prototyping with Spatial Placement \textbf{(N = 12, 44\%)}} 
            & Vertical Positioning \textbf{(N = 7, 26\%)} & 
                P17: \enquote{\emph{ Make it easier to move objects far away. Make it clear to look up if user is trying to move an object overhead}} \\ \cline{2-3}
                   & Spatial Exploration \textbf{(N = 6, 22\%)} & 
                P25: \enquote{\emph{Also Experimented with dragging objects while moving  Moving sound around while walking throughout the space, also finding different mix levels within a room to compare by walking through it.}} \\ \cline{2-3}
                & Shaping soundscape \textbf{(N = 5, 19\%)} & 
                P6: \enquote{\emph{The wind should be everywhere, so how would I do I turn a wind to an object that isnt spatial, its like a skybox in a computer game, so I put it above myself}} 
                \\ 
         \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Theme 1: Embodied Mixing}

We first present codes describing participant responses (N = 20) related to their bodies' influence on their mixing experience. Overall, respondents expressed that the combination of their body movements and the feedback they received contributed to their experiential design process and supported their mix. 

\paragraph{6DoF Audio Presence}
About half of the participants (N=14; 8 Non-Expert, 6 Expert) shared positive sentiments highlighting the effects of 6DoF, which adapted to their position and orientation-induced feelings of \emph{presence} when designing their soundscape. We adopt the two-dimensional conceptualization of \emph{presence} suggested by Weber et al. and define it as, \enquote{as the allocation of
attentional resources to the mediated world and the sensation of perceptually being surrounded by the virtual environment or the user's judgment about the degree of realism} \cite{weber2021get}. Participants found the directional audio feedback to feel \enquote{\emph{real-time}} in response to their position and orientation. Additionally, some participants described enveloping themselves in their mix by navigating freely through the responsive soundscape or surrounding themselves with their audio objects: 
\begin{quote}
    \emph{\enquote{Being able to place the different parts of the environment where you wanted was super cool. In the camping environment, I closed my eyes and it felt like I was actually camping. Being able to put specific tracks/channels above and behind me in the music mixing environment felt like I was truly immersed in the song.} -- P21 (expert) }
\end{quote}

Participants noted that when they placed objects above or behind themselves, the 6DoF spatial audio accurately translated those physical positions into perceivable directional cues resulting in an \enquote{\emph{immersive}} experience. This aligns with research linking mediated experiences involving body movement in physical spaces to an enhanced sense of presence \cite{presence1997} -- specifically, when individuals engage more of their physical selves, the experience often feels more real. 
%Directionality combined with the fluidity of movement seemed to be sensitive attributes in users' overall experience, and if not implemented appropriately or if they experienced a system error, it could disrupt their interactions. 
Some respondents delved deeper into why they were particularly intrigued by the sound's directionality, noting how it interacted with and changed based on their physical movements.: 
\begin{quote}
    \emph{\enquote{I liked being able to hear the sound coming from different directions as I moved it around, it is cool that instead of just hearing the sound around you can interact with the sound.} -- P19 (Non-Expert)}
\end{quote}
The interface may have added a level of interactivity for experts and non-experts, which appeared to be unique from their everyday audio experiences, possibly contributing to feelings of envelopment. The dynamic audio changing due to their movements can be attributed to how the spatial audio responds to their body with 6DoF, which allows for more precise directionality and includes translation (e.g., audio attributes changing as a user walks away from origin) versus 3DoF interfaces which, in the context of audio mixing applications, generally support only rotational components.


\begin{comment}
    The only negative sentiment is related to the 6DoF audio rendering is related to 

\begin{quote}
    \emph{So, I didn't want to always change my gaze to look at the source I  needed to move. I wanted to keep my gaze forward so I could listen to the intended mix as I changed the object position or distance. Also, it was pretty choppy when manipulating an object while walking- P22}
\end{quote}
\end{comment}


\paragraph{6DoF Audio Perceptibility} 
Some participants (N = 8; 5 Non-Expert, 3 Expert) shared 
specific strategies leveraging spatial audio with 6DoF, free-hand manipulation, and head orientation enabling strategies of perceving and isolating sound emitted from audio objects. Generally, increased users' perceptibility was due to the physical actions coupled with binaural rendering, amplifying interaural level differences (ILD). ILD is a phenomenon where the head occludes acoustics depending on the direction, altering the amplitudes' distribution to either ear \cite{farnell2010designing,birchfield2005acoustic}. For instance, during the experiments, people often moved the relative position between their ears and the audio objects to hear different perspectives of their mix: 
\begin{quote}
    \emph{\enquote{I really enjoyed the head tracking aspect. I felt that being able to bring an object close and then tilt my head from side to side made it easier to get a sense of the timbre of the sound. Also, being able to isolate objects in space was helpful.} -- P22 (Non-Expert)} 
\end{quote}
Head orientation plays a key role in sound localization, and actions like turning the head toward or away from the source assist in determining its physical location\cite{farnell2010designing,birchfield2005acoustic}. Therefore, head movement may have felt advantageous to users when assessing the quality of their mix.
Another technique worth noting was expressed by an expert during the experiment where they state,
\begin{quote}
    \emph{\enquote{I tried putting my head inside the audio sphere to isolate the sound.} -- P21 (Expert)}
\end{quote} 
P21 defined their strategy as inserting their head inside a virtual audio object, thereby attenuating all other sound sources within the scene. The sound isolation likely happened because the audio listener (attached to the AVP's position) was dominated by the audio source of the object where P21 inserted their head due to the distance attenuation in Unity's 3D sound settings. In the sound settings, other audio sources attenuate according to a logarithmic distance scale. As a result, sources that are in close proximity, or in this case near the same position, will dominate the audio output. All user responses in the coded group shared similar statements about the ease of perceptibility, except for one, who noted that the stark differences in audio could cause some confusion when mixing:  
\begin{quote}
    \emph{\enquote{ I found some inconsistencies with how I wanted the scene to sound in my head when looking at the objects versus how the sound actually rendered out. Moving around an object (walking in a circle around a ball) made me question the placement of other objects in relation to it because the changes in level were semi-stark.} -- P27} 
\end{quote}
P27 describes the disorientation they experienced due to conflicting expectations of where they observed their placement versus the sound they heard. The misalignment was exacerbated by walking in a circle, which likely caused the 6DoF audio to pan too sensitively in the user's headphones, hindering their ability to perceive the audio sources coherently and thus affecting their expected mix. To address this sentiment, it may be worth offering users greater flexibility over the sensitivity of the spatial audio panning.


\paragraph{Visual-Aural Responsiveness} 
Another subset of participants (N = 7; 4 Non-Expert, 3 Expert) found the alignment of the position of the audio objects' visual appearance corresponding to changes in the audio directionality, specifically when they would manipulate objects with free-hand gestures, to be both useful and enjoyable. Users remarked the immediate audio and visual feedback in response to their free-hand manipulation as \emph{intuitive} and assisted them in mixing, allowing for easy self-correction of placements:
\begin{quote}
\emph{\enquote{I really enjoyed the lack of latency when moving objects around the space. It really felt like the objects responded to my gestures in real-time, and their movements were also accurately reflected by the audio.} -- P23}
\end{quote}
P23's response explicitly highlights the sense of control in feeling convinced the audio objects reacted visually and aurally to their physical manipulations within a time window, making the physical-virtual interaction believable. Congruent audio-visual feedback has been shown to result in better performance and higher engagement \cite{crossmodal2016chi, o2024sound}. The blending of senses, also known as \enquote{synesthesia}, has been previously linked to excitability and arousal \cite{Terhune2011}, a fact that film, games and VR entertainment creators have been exploiting -- by integrating cross-modal stimuli, they evoke a sensation of presence that creates exciting experiences \cite{presence1997, o2024sound}, and if spent an extended period of time in the immersive experience, virtual artifacts start feeling like physical \cite{finnegan2016distance}. One of our participants expressed a synesthetic experience: 
\begin{quote}
\emph{\enquote{I quite like how close I can be to the audio objects (it almost feels like there's a haptic feedback from being close to certain audio objects)} -- P16}
\end{quote}
Another participant echoes this sentiment and also expressed how the immediate visual-aural feedback assisted their mixing process: 
\begin{quote}
\emph{\enquote{The immediate acoustic feedback made it easy to converge on manipulations that were most effective.} -- P22 (Expert)} 
\end{quote}
Overall, the immediate visual-aural feedback and alignment were complimentary in providing interactions that allow users to feel in control, thereby making their mixing process feel like a \emph{direct} result of their intended placements. The interrelationship of the visual and aural attributes of the interface captured in participant responses highlights the need for proper alignment and immediacy of feedback; otherwise, the mixing experience may be disrupted. Additionally, multimodal synchronization is a key sound design principle necessary to reduce cognitive load when, for example, viewing and listening to cinematic experiences or when developing audio authoring tools \cite{farnell2010designing}.


\subsubsection{Theme 2: Mapping Sound to AR GUIs}
Next, we outline the codes centered on controlling sound attributes in an AR interface. Responses (N = 18; 10 Non-Expert, 8 Expert) in this category generally consisted of the trade-offs of using depth to modify audio attributes (e.g., loudness or reverberation), instead of varying alternative visual properties and physical interactions of the audio objects to have better UI affordances and enable more granular control.

\paragraph{Audio Attribute Level Control}
The \emph{Audio Attribute Control} centers on challenges that arose when using the \emph{push-pull} action to vary the loudness parameter coupled with suggestions to modify alternative audio attributes to enable more fine-tuned control over their soundscapes. In this category, users (N = 15; 9 Non-Expert, 6 Expert) noted difficulties in depth perception when pushing objects out to control the loudness parameter of the audio objects:
\begin{quote}
\emph{\enquote{Depending on where I was standing in the room, I wished there was some guidance on how far I could push out an object on each $XYZ$ plane before they became inaudible/not part of the mix. } -- P27} 
\end{quote}
P27 highlights an issue with recognizing when an audio object becomes \enquote{inaudible}. This sentiment is aligned with how multimodal cues are often explored for additional guidance through an interface, and how depth alone may not offer enough control over audio attributes for some users \cite{chen2018investigating,hashky2024multi}. In this particular case, the user expresses uncertainty when audio objects will become inaudible, and their response suggests the inclusion of additional visual cues, like implementing a visual boundary representing the \emph{inaudible zone} that persists through the duration of the application to identify the distance before an object is silenced quickly. 

%Previous research has shown that visual boundaries can provide better sensitivity in spatial localization \cite{boundaries2012}

Participant responses in this category also expressed mostly negative sentiments regarding the \enquote{loudness} parameter control implemented in the interface. Users generally felt pushing objects \emph{too far away} made it harder to align their hands with the perceivably smaller objects due to the camera's perspective view in Unity. P2 describes the difficulty of aligning their hands with the virtual audio object that has been pushed far away from them:
\begin{quote}
\emph{\enquote{At one point, I had moved some objects really far away from me, and I was struggling to bring them back. I had to physically walk over to the objects to bring them back.} -- P2 (Non-Expert)}.     
\end{quote} 

Moreover, determining the sensitivity using the forward axis for loudness combined with the challenges in perspective projection presented difficulties in varying the audio attribute. P16 states:
\begin{quote}
\emph{\enquote{It would be nice to have a way to tweak the sensitivity concerning distance. It took some trial and error to figure out how far I needed to move the object to get the level of loudness I wanted.} -- P16}
\end{quote}
Other participants echoed this sentiment and typically suggested different features like modifying the sensitivity of the \enquote{\emph{push-pull motion}} or alternative methods for level control that do not rely on distance.

\paragraph{Object Appearance}
In contrast to the \emph{Audio Attribute Level Control}, the \emph{Audio Object Appearance} categorizes sentiments related to participants' (N = 8; 4 Non-Expert, 4 Expert) preconceived associations with mentally mapping visual elements to audio attributes (e.g., loudness, reverb, or spread). Most of the respondents in this category described preferences of \emph{changing the size of the sphere to increase/decrease loudness} or other audio attributes not implemented in the interface. Additionally, some responses related to the trade-offs of using translucence to represent the loudness:
\begin{quote}
    \emph{\enquote{I think the size of the audio object should be associated with spread} -- P27 (Expert)}
\end{quote}

Their mixing experience with DAWs may influence P27's sentiment because the spread is typically associated with the size of audio objects in object-based spatial audio metadata, such as Dolby Atmos \cite{dolby2018atmosrenderer}. In contrast, a response from P22 (Expert) associated audio object size with \emph{draping} and felt translucence could represent \emph{direct-to-reverberant ratio}. The other two experts in this category also stated that size and translucence make sense when representing loudness. While there were mixed sentiments on the preferred visual elements for audio objects, all participants in this category suggested varying visual associations with different audio attributes, indicating that there may be associations from their mixing experience or, in the case of non-experts, there are natural associations with visual properties of the spheres and audio attributes:

\begin{quote}
\emph{\enquote{I wasn't sure if I could change the magnitude of the \enquote{orb} or if I need to bring it closer to increase the volume} -- P15 (Non-Expert)}    
\end{quote}
 P15 expresses confusion regarding the visible size of the \enquote{orb} being associated with loudness. The visual parameter, \emph{size} being mistakenly associated with \emph{volume} or loudness was also expressed by P23 and it may be due to the natural associations humans have between audio and visual representations, often referred to as audiovisual perception \cite{malpica2020crossmodal, crossmodal2016chi, karla2010vision, gibson1997art}. This could also contribute to a GUI design strategy focused on how to represent sound visually in the most effective way for the intended task. 

\paragraph{Modularity}
 The AudioMiXR interface was seen by four experts (N = 8; 4 Non-Expert, 4 Expert) as a component for a potential hybrid system combined with an existing DAW for audio mixing with more spatial awareness. The four experts expressed that designing within the execution environment could fit nicely in-game audio design or XR workflows and foresee the \emph{AudioMiXR} interface integrated into a hybrid system where they have the flexibility to switch between their DAWs on a desktop and into a headset when they require more spatial awareness.  For example, P27 states: 
 \begin{quote}
 \emph{
 \enquote{Think this fits nicely within game audio design workflows and specific audio mixing workflows where the output is specifically for AR/XR especially at the ideation phase of the project... DAWs are notoriously cumbersome to use unless you are experienced with hot keys and shortcuts so this was very refreshing to be able to make large changes with little physical exertion.}
 -- 27 (Expert)}
 \end{quote}
 Respondents thought of \emph{AudioMiXR} as a complement to their existing workflows where they can overcome some of the drawbacks of DAWs but felt they would still want access to their traditional mixing tools. The flexibility of control underscores an optimal medium where all audio attributes may not necessarily need to be mapped to the AR interface and instead can leverage its features when required.  

\subsubsection{Theme 3: Prototyping with Spatial Placement}
Lastly, we present responses related to the potential affordances and drawbacks spatially placing objects has on prototyping mixes. Participants (N = 12; 5 Non-Expert, 7 Expert) expressed suggestions, challenges, and benefits the existing interface has on prototyping their soundscapes. 

 \paragraph{Vertical Positioning}
\textit{Vertical positioning} refers to sentiments centered on audio object placement along the vertical axis affected participants' (N = 7; 5 Non-Expert, 2 Expert) (i.e., placing an object above or below their line of sight). P5 and P20 both expressed the features was \emph{playing with height} while the rest of the respondents voiced difficulties with placement along the vertical axes: 
\begin{quote}
\emph{\enquote{Placing object overhead was a bit awkward with the limited field of view. You can also feel the mass of the headset when doing that} --  P9 (Non-Expert) }   
\end{quote}
Placement of objects overhead or below the HMD is a unique feature supported by AudioMiXR. Still, the limited FOV forced users to engage in excessive upward head rotation so that the cameras on the headset could capture their hand gestures, thereby disrupting their placements. The difficulties with the vertical placement due to the FOV of the headset may imply that there should be alternatives to object placement. For example, a function that maps subtle movements along the vertical axis may be a potential circumvention to take advantage of the vertical placement unique to an audio mixing interface with 6DoF.

\paragraph{Spatial Exploration}

Respondents expressed how the autonomy and \emph{freedom} provided by the AudioMiXR interface encouraged exploration and experimentation of their arrangements while designing their mix (N = 6; 4 Non-Expert, 2 Expert). Responses center on free-hand movement and being able to use \emph{all} of the surrounding space to manipulate objects: 
\begin{quote}
    \emph{\enquote{It allowed me to be creative and place the objects as far or close as I wanted. I felt that it understood what I wanted it to do most of the times.} -- P8 (Non-Expert) }
\end{quote}

P8's sentiment represents the code group in expressing how the interface feels unrestricted to their movements. P12 describes how the spatial placement enabled them to \emph{remix} audio objects easily and try different combinations of loudness and spatial configurations, thus supporting their creativity. As metaphorized in \cite{shelvock2016gestalt}, a mix is a \enquote{sonic portrait} and its essence can be conveyed in a myriad of ways. This flexibility may be worth considering for creative workflows within AR Sound Design. Another interesting connection is related to the quantitative results on virtual audio object spread reported in Subsection \ref{variance-analysis}, where a significant difference in spread and, thus, spatial exploration between the cinematic and music mix scenes can be observed, which underscores that the amount of spatial exploration is task-dependent. 

\paragraph{Shaping of Soundscape}
During the experiment, a few participants (N = 5; 3 Non-Expert, 2 Expert) described different strategies of grouping or arranging multiple audio objects, as well as how these objects worked together to create a cohesive soundscape. For instance, P2 would have preferred the ability to \enquote{tie objects} to their location instead of the \enquote{physical environment} in order for the arrangement of objects in the scene to be relative to the headset which is more aligned with an audio experience with 3DoF. Further, the rest of the participants echoed different types of arrangement functionalities centered on different types of grouping of objects to make the design of their soundscapes more flexible. P27 captures these sentiments well by suggesting moving objects together to simplify their mixing process: 
\begin{quote}
\emph{\enquote{Grouping objects -- link multiple balls together and moving as a unit.} -- P27 (Expert)}
\end{quote}
P27 describes a feature that may embody the concept of a cohesive soundscape better than simply treating each audio object as separate entities at all times. It is possible P27's experience with DAWs inspired their suggestion as modern DAWs like Logic Pro support users to arrange audio channel groups \cite{apple2024logicpro}, which is a known technique for professional audio mixers to design complex soundscapes \cite{farnell2010designing, gibson1997art}. This would exhibit the Gestalt principles of proximity and common fate \cite{farnell2010designing}, where similar sounds -- where similarity may be measured in terms of rhythm, timbre, or semantics -- would have the tendency to be arranged and moved together. This is something that has been reflected in our quantitative data (Subsection \ref{heatmaps}) as well, where one can clearly see several instances of semantically-related audio objects to be clustered together in space.


\begin{comment}
    \nb{
\begin{enumerate}
    \item must define 'autonomy' when theme #1 is first introduced, there is still ambiguity in the term among the HCI community
    \item This paper seems to provide more insight on autonomy and agency in the hci community (its a literature review): https://dl.acm.org/doi/full/10.1145/3544548.3580651
    \item from the aforementioned paper "Material and Experiential. Papers in our corpus also addressed material and experiential aspects of agency and autonomy. We use material here to refer to both the material expression of autonomy and agency (e.g., as Coyle et al. note:  the fact of controlling an action as distinct from the immediate sense or experience of having done so [37, p. 2026].),"
    \item Basically, our interface provides both the immediate experience as well as immediate actions as described in theme 1
    \item I wrote these down to see the difference, it feels like translation is how you implement or realize a principle in AR, while control is how you can enforce the principle in AR -- Rita
    \item A number of papers indicated that the sense-of-agency in quite minimal interactions could be manipulated, by manipulating sense-of-ownership [89], haptic feedback [152], or the timing of events [89, 90]. \item Several other papers drew on Self-Determination Theory, which (as discussed above) emphasises that sense of autonomy is supported by outcomes congruent with values [135, 139], raising the possibility that it may be manipulated without material change in the users independence, range of choice, or infuence on events [44]. However as noted above, in practice in all these cases, material and experiential aspects of agency or autonomy did not diverge. 
    \item basically what I'm trying to say here is that there are aspects of 6DoF that inherently allow the user to feel free, in general. With that freedom, users are able to have more intrinsic motivation which leads to the enjoyment of the activity in itself and not for an external reward.
    \item Proprioception: perception or awareness of the position and movement of the body.
\end{enumerate}
}
\end{comment}

%\clearpage
%\newpage
\section{DISCUSSION}
In this section, we form design lessons derived by combining themes based on participant responses, analyzing our quantitative data, and sharing observations during the experiment while supporting these design lessons with existing literature in audio augmentations in AR, and authoring in XR. Afterward, we describe the applications of mixing that may benefit most from a 6DoF AR mixing interface with considerations of the qualitative and quantitative data. 


%\clearpage
%\newpage
\subsection{Design lesson -- Proprioception for AR Sound Design} %(Themes 1\&3)


Most users shared positive sentiments related to their experience when \enquote{\textit{freely walking around through their soundscapes}}, \enquote{\textit{hearing spatial audio from all directions change}} according to their movements, \enquote{\textit{immediate audio feedback}} in response to their actions and changes in the audio object positions, and \enquote{\textit{being able to explore and be creative}} which may inform how sound design principles should be translated into an AR setup. Theme 1 highlights how users' bodies influencing the mix made them feel actively engaged in the directed tasks where they noticed their movements and actions directly affected the soundscapes they designed. Their fascination with their body's influence on the soundscape was primarily attributed to immediate UI response to participants' actions, position, and orientation, which are inherent to the 6DoF interactions in the interface and can facilitate sound design principles like sound attribute perception, layering and spatial awareness \cite{farnell2010designing,gibson1997art}. For instance, the dynamic visual-aural feedback in response to participants' movements seemed to allow for feelings of self-agency and control of the UI throughout the mixing tasks, as expressed in P23's statement: \enquote{\textit{It really felt like the objects responded to my gestures in real-time, and their movements were also accurately reflected by the audio}} in regards the immediate feedback in response to their physical actions.  Similar reactions align well with existing literature on perceiving ILDs and assist with audio localization, which can support the spatial audio mixing process \cite{birchfield2005acoustic,farnell2010designing,francart2007perception}. A 6DoF interface may be able to leverage proprioception in assisting the user in understanding the spatial relationships between their body, actions, audio objects, and the physical space around them.

Moreover, designing dynamic sound design interfaces in AR requires a use case that effectively leverages the space of the virtual environment. In Figure \ref{fig:tests-3d-gaussian}, we demonstrate the spatial distribution of all audio objects across all participants in the music mix and cinematic mix tasks -- participants generally used significantly more of the physical space to place audio objects for the cinematic blend. Additionally, we have provided a visualization of heat maps (Figures \ref{fig:heatmap-environment-scene} and \ref{fig:heatmap-audio-mixing-scene}) demonstrating where participants typically placed audio objects in the cinematic scene and uncovered that their placements aligned with real-world expectations of where participants could imagine the audio source (e.g., bird) would be concerning their physical location. The quantitative data demonstrates a possible benefit of using an AR design interface for mixing tasks that have familiar contexts in the real world, suggesting that this may be a reasonable focus for AR sound design UIs with 6DoF interactions. Further, our quantitative findings align well with some of our expert responses where they suggest using the AudioMiXR interface for soundscapes intended for \textit{experimenting with object positions in VR experiences} and \textit{video game design workflows}, which both use cases that involve a `perceivably' physical virtual environment where 3D position and orientation of audio objects would be necessary. 

\subsection{Design Lesson: Balancing Audio-Visual Modalities in AR GUI}
A number of feature requests emerged from our user study that reflect established psychophysical principles related to spatial cues, crossmodal congruences, and audiovisual localization -- all of which play a significant role in how participants want to experience sound design in AR. By addressing these features, AudioMiXR can better align with human perception, ultimately enhancing presence, immersion.

Several participants expressed alternatives for parameter control that align with familiar associations between visual effects and audio. Participants desired to control volume by \enquote{\textit{making objects larger or smaller}} or the \enquote{\textit{level of translucence}} of the audio objects, which are common associations present in traditional DAWs or music production tools \cite{gibson1997art}. Further, associations of visual and audio attributes are commonly researched in audiovisual perception \cite{crossmodal2016chi}. They may be coupled with AR design frameworks to design AR sound design interfaces sensitive to humans' preconceived associations with audio attributes and their visual representations. 

% Including extending the set of audio attributes to manipulate, such as reverb cues, equalization, spread --> to give even more control over the sound rendering.
Participants consistently expressed the desire to have independent control of an audio object's loudness, as opposed to just moving it closer or farther away, 
as reflected by the following responses of P24: \enquote{\emph{I'd like to have control over object size and overall object volume at different distances [...] I'd like to specify volume across different distances more precisely}} or P27 \enquote{\emph{Expanding and reducing objects to represent volume change rather than just using distance to control levels}}. 
reflecting the strong cross-modal tendency to associate larger visuals with louder sound and viceversa \cite{farnell2010designing,gibson1997art}. Users also noted the challenge of distance-loudness calibration, (P24 \enquote{\emph{For example close distances always made the object much louder than I wanted}} or P16 \enquote{\emph{It would be nice to have a way to tweak the sensitivity with respect to distance. It took some trail and error to figure out how far I actually need to move the object to get the level of loudness I want}})
e.g., moving an object only a short distance sometimes resulted in an unexpectedly large change in loudness. These observations highlight how individual expectations of sound intensity and distance often vary due to distance compression biases \cite{farnell2010designing, finnegan2016distance}, where the weight of the HMD itself can be a contributing factor to this bias \cite{hmd2009}. Although the mutisensory integration of both visual and auditory stimuli has been proven to gauge better distance localization due to gain re-balancing between senses \cite{odegaard2015biases, crossmodal2016chi}, it has been also observed that in scenarios of level/size discrepancy, the audio component takes precedence over visuals \cite{crossmodal2016chi}, up to the extent that spatial audio, in particular, can overpower visual perception \cite{correa2023spatial, nonspeech1994}. Therefore, when dealing with spatial audio one has to be careful to not overdo with audio. Several participants noted the benefit of having \enquote{visual boundaries} (i.e. weighting on the visual component) in order to have some guidance on level control. 
(P27 \enquote{\emph{Knowing the boundaries of when an object became visible/invisible aka audible/inaudible. Depending on where I was standing in the room, I was wishing there were some guidance on how far I could push out an object on each $XYZ$ plane before they became inaudible/not part of the mix}}). 
Previous research has shown that visual boundaries can provide better sensitivity in spatial localization \cite{boundaries2012}. Additionally, our participants re-iteratively requested a quick way to enable or silence specific virtual audio objects so that they can isolate sounds or compare before-and-after placement. This can be linked to selective attention mechanisms \cite{farnell2010designing}. In particular, the \enquote{cocktail party effect} describes this selective attention phenomenon, where listeners can concentrate on one channel in a noisy environment and dynamically switch attention as needed \cite{nonspeech1994, shelvock2016gestalt}. In a mixing context, muting or soloing audio objects leverages \emph{figure-ground organization} -- also rooted in Gestalt principles -- allowing foreground sounds to be distinguished from background layers. 
% --> maybe link to selective attention mechanisms (i.e., focus on one cue without interference)
This is where the quantitative results of spread difference between scenes start making sense: in the cinematic the spatial spread of audio objects is significantly larger than in the music mixing scene, and this can be attributed to the fact that participants had a bias towards considering cinematic audio objects as more ambient-related and tried to magnify the spread each point-source audio object by extending the sonic perimeter of their mix. In contrast, in the music mixing scene, most of the audio objects were considered as \enquote{salient} and were required to be loud enough in the mix to contribute its timbral and rhythmic component, which lead to narrower spread, mostly at the hand's reach.

Beyond loudness, participants requested control over other audio attributes, such as reverberation levels or pitch. Even though loudness is considered a primary cue for distance estimation \cite{finnegan2016distance}, studies report that reverberation and timbral qualities (e.g., the frequency spectrum) are some other cues that greatly influence distance perception. Participants expected to manipulate these cues in tandem with loudness, similarly, modifying the physical placement of virtual markers for a coherent and holistic mixing experience. There is a lot of previous work relating size of visual markers to pitch \cite{karla2010vision, farnell2010designing, nonspeech1994}, vertical positioning to pitch \cite{bernstein1971, crossmodal2016chi, karla2010vision}, even marker color to pitch \cite{o2024sound, karla2010vision}, some of which converged into heuristics adopted in practice by audio mixing practitioners \cite{gibson1997art, dewey2024}. 

 %Participants also foresaw another way of increasing the spread of a single audio object, 
Another recurring feature request was replicating and grouping virtual audio objects. One participant described how they wanted to duplicate a single virtual audio object to occupy various positions, increasing its spread. 
%which is by means of its replication (placing it more than once in the mix). 
Others recommended grouping virtual audio objects and moving them as a unit or \enquote{layer}, distilling core design principles approaches to layering and soundscape arrangement \cite{farnell2010designing}. Grouping resonates again with Gestalt principles, where similar audio sources -- especially, those sharing timbral or rhythmic traits, are naturally clustered together, evidenced by the similarity and common-fate principles \cite{farnell2010designing, shelvock2016gestalt}. It is of extreme interest how our participants positioned virtual audio objects in the space, forming semantic patterns between pairs or groups of sounds, as it can be observed in the heat map analysis (Subsection \ref{heatmaps}). This is especially evident in the cinematic scene, where the task was to create a compelling sonic environment which would resemble a real scene.
By supporting object duplication, grouping, and spatial layering, AudioMiXR can help organizing granular virtual audio objects into a coherent and sensible whole more effectively. 



\subsection{Limitations and Future Work}

AudioMiXR may have the potential for a wide range of sound design tasks by virtue of its object-based workflow in an AR interface with 6DoF. In this section we outline some of the limitations of our experiment and describe future scenarios where our interface can be applied to. %While our current, the system design is equally adaptable to VR and we discuss this next, along with an outline of key avenues for expanding AudioMiXR's functionality and addressing its current limitations. 
 %By default, the HMD's over-ear headphones suffice for audio referencing, but for complete sound isolation users can opt for noise-canceling Bluetooth headphones. 

\subsubsection{Experiment Limitations}
The HMD's over-ear headphones suffice for audio referencing, but for complete sound isolation users can opt for noise-canceling Bluetooth headphones. Our experiment relied on synthesized spatial audio based on generic HRTFs. We invite future studies to couple AudioMiXR with personalized HRTFs, which can further enhance the localization accuracy of the virtual audio objects. Furthermore, AudioMiXR's design is not restricted to binaural rendering; it could be extended to loudspeaker stereophony, allowing users to design mixes that target specific speaker arrays. In the future, we would like to implement the interface for a loudspeaker scenario to assess if new design lessons may emerge with the differences in loudspeaker stereophony and binaural rendering. 

Additionally, we could have used a wider range of audio objects and mixing scenarios to identify nuanced insights that may be specific to other scenarios. We only used cinematic and music mixing scenes due to the backgrounds of the professional mixers we interviewed, although one expert did also have mixing experience for video games and meditative sound experiences, so these scenarios may also be worth considering for future studies.

Further, the experiment used AR and not VR, so it is worth exploring the interactions in a VR interface. We discuss our VR implementation in the following section.

\subsubsection{Extension to VR}

Beyond AR, extending AudioMiXR into VR offers immersive sound design in fully virtual environments. 
Deploying the same 6DoF interaction model in VR would let users build and experience soundscapes in virtual worlds -- useful for VR cinematic productions, interactive media, and game audio production. However, VR introduces unique challenges, such as cybersickness and occlusion of the real environment, calling for careful design of user feedback and locomotion loops. See Figure \ref{fig:vr-mixing} where we test AudioMiXR within AVP's \emph{Environments} application.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/5_lut.png}
    \caption{Audio mixing in virtual environments. This example illustrates a demo of AudioMiXR running on top of AVP's native \emph{Environments} application, displaying the virtual Yosemite environment.}
    \label{fig:vr-mixing}
\end{figure}

\subsubsection{Multi-user collaboration}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/1_lut.png}
    \caption{Synchronous multi-user collaborative mixing}
    \label{fig:collaborative-mixing}
\end{figure}

Although our prototype focuses on single-user, shared XR mixing session is a very attractive direction of research. Enabling synchronous multi-user collaboration in 3D environments, supporting interactions in both remote and co-located spaces, would allow multiple users wearing headsets to co-edit an audio scene in real time. In co-located setups, where users would share the same space, upon AudioMiXR initialization, users would be able to freely navigate in the \emph{XR origin} position within the space and reset the scene to return to the origin. The reset would transform virtual audio objects present in the scene relative to their reset position. Resetting the scene would require verbal communication to ensure all users agree and maintain spatial awareness, as changes would affect everyone sharing the workspace (Figure \ref{fig:collaborative-mixing}). Conversely, in remote collaborations across different physical locations, resetting the workspace would adjust all scene assets, including peer users, relative to the new position. To increase co-presence and spatial awareness in remote AR mixing sessions, visual cues would be used. Each user would be represented by a full-body avatar within the shared workspace, facilitating embodiment and identification. During user-object interactions, such as selection and manipulation, the system would display the users hands and highlight the manipulated object. Simultaneous selection and manipulation of the same 3D audio object by multiple users would be prevented to maintain interaction integrity.



\subsubsection{Immersive music production.} 
With AudioMiXR, audio mixers and producers can mix spatial audio directly \enquote{inside the panner}. As illustrated in Figure \ref{fig:applications-1}, users can arrange spatial audio \emph{anywhere} -- not only in traditional studios, but also in living rooms, concert halls, or even outdoor venues. 

We foresee AudioMiXR as a tool for musicians and producers to be used jointly with DAWs by serving as an 6DoF panner for accurate positioning of spatial audio. In live performances (Figure \ref{fig:applications-3}) --  musicians or DJs could experiment with spatial placement of audio elements while on stage, adjusting soundscapes in real time to match the dynamics of the crowd. 

\begin{figure}[ht!]
    \centering
\begin{subfigure}{1.0\linewidth}    \includegraphics[width=1.0\linewidth]{figures/paper/2_lut.png}
    %\caption{Caption}
    %\label{fig:enter-label}
\end{subfigure}\\
\vspace{1mm}
\begin{subfigure}{1.0\linewidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/4_lut.png}
\end{subfigure}
    \caption{Snapshots of AudioMiXR users in different environments--AudioMiXR allows for audio mixing in any physical environment.}
    \label{fig:applications-1}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/10.png}
    \caption{AudioMiXR being used during a DJ set, illustrating how multiple spatial audio objects can be visually placed and manipulated above the turntables and out into the audience. By leveraging 6DoF interaction, DJs can \enquote{grab}, rearrange, and adjust each virtual audio object in real time.}
    \label{fig:applications-3}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/paper/8_lut.png}
    \caption{AudioMiXR facilitates mixing immersive cinematic experiences--a user is positioning virtual audio objects in a cinema according to how it should sound for the public.}
    \label{fig:applications-2}
\end{figure}

\subsubsection{Cinematic and post-production sound.} Another potential application of AudioMiXR is targeted towards audio post-production teams. They can use AudioMiXR to place sound effects or dialogues in a cinema, theater or set. This would eliminate \enquote{guesswork} since they can \enquote{walk around} to verify how spatial cues would sounds from different listening positions in an auditorium or film set. This opens up a whole new world for interactive film experiences since producers can design spatial audio cues directly in a physical space, refining the placement of the audio objects for maximum emotional impact (Figure \ref{fig:applications-2}). 



\subsubsection{Sonified exhibitions, art, and aesthetic entertainment.} In galleries, museums, or live-installation art, AudioMiXR allows curators to add layers of music, sound effects, or voice-overs over exhibits. This application can heighten visitor engagement and sense of immersion by blending visual and auditory stimuli.






\begin{comment}
\subsection{Technical Challenges / Limitations}
\begin{itemize}
\item Audio visualization would be helpful for debugging because we cannot see audio 
Solution: Build and test a lot -- if permitted more time in the internship we would have implemented audio visualizations
\item Device tracking was difficult to implement due to conflicts with different XR libraries in our project 
Solution: Explicitly implemented compatible device tracking library with the AVP
\item Binaural Rendering: Unity builds the project, but once the project moves over to XCode, AVP and Unity renderings collide
Solution: Add code to bypass AVP binaural rendering.
\end{itemize}
\end{comment}





\section{CONCLUSION}
%\note{What did you do?
%What did you find?

%What are the broader implications of your work?}


In this work, we presented AudioMiXR, to the best of the author's knowledge, the first 6DoF AR interface for sound design, enabling direct free-hand manipulation of spatial audio. Therefore, no guidelines for sound design in XR currently exist. We developed two design lessons: 1. Proprioception for AR Sound Design and 2. Balancing Audio Visual Modalities in AR GUI. The first design lesson underscores how utilizing the body when mixing audio has benefits supporting perceptibility and self-localization concerning the physical environment and the manipulated audio objects. Further, our results of participants' final placements imply that mixing scenes with more semantic meaning could leverage the autonomy 6DoF provides due to the real-world context of the audio objects used. For instance, participants generally placed river sounds in a lower position along the vertical axis versus their bird sound placements, which they situated higher. 

The second design lesson highlights the need to balance the audio-visual modalities necessary for a cohesive mixing experience and to facilitate appropriate visuals to represent sound more effectively. Specifically, our interpretations derived from user responses indicate that people have natural preconceived associations of visual and aural elements (e.g., associating translucence of an audio object with loudness). Moreover, a mismatch between the visual and aural modalities may disrupt the mixing experience by not meeting user expectations. Thus, visual-aural associations should be studied when designing a 6DoF sound design user interface.  

Our findings suggest that embracing 6DoF in XR audio mixing has distinct features that enhance the user experience for 3D audio mixing in contexts where the involvement of the body or free range of motion is beneficial. Specifically, we highlight contexts in cinema, interactive media, XR applications, music, and video games as areas where audio mixing with 6DoF in AR-VR may benefit. We hope that our interface, experiment, and results can be a first step in defining further research trajectories centered on these areas.


\section{Acknowledgments}
%We wish to acknowledge the inspiration drawn from
We would like to thank our participants in the user study for their valuable time and feedback. Lastly, we extend our gratitude to music, whose diverse genres continually inspired this work.


%Thank anybody else you'd like: anyone who didn't contribute enough to be a co-author, people who gave good advice, someone who helped set up a piece of equipment, maybe even participants.}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\newpage
\appendix
\section{Survey for professional audio mixers}\label{formative-interview}
Questions for professional audio mixers:
\begin{enumerate}[label=(\arabic*)]
\item Can you describe your general process when mixing audio tracks with spatial audio objects?
\item What tools or software do you typically use for spatial audio localization, mixing, and panning? How do these tools support your workflow?
\item How do you approach spatialization of audio objects within a 3D environment? What are your considerations when positioning audio objects spatially?

\item Can you discuss any specific techniques or strategies you employ to achieve optimal spatialization and panning effects in your projects?
\item In your experience, what are the main challenges or difficulties you encounter when working with spatial audio? How do you typically address these challenges?

\item How often do you collaborate with others (e.g., other sound engineers, content creators) during the spatial audio production process? Can you describe how collaboration influences your workflow and decision-making?

\item Have you encountered any notable successes or breakthroughs in your approach to spatial audio that you'd like to share? How did these impact your projects or clients?

\item Looking ahead, what advancements or improvements do you envision in spatial audio technology or tools that would benefit your work?

\item Can you describe a recent project where spatial audio played a critical role? What were the specific challenges and successes in that project?
\item How do you evaluate the effectiveness of spatial audio mixes? Are there specific criteria or metrics you use to assess quality?
\item What audio attributes do you think can benefit from spatialized interaction? (eg. in a audio panner to manipulate, volume, reverb, equalization etc.)
\item Finally, based on your experience, what advice would you give to someone new to spatial audio mixing and localization?
\end{enumerate}

\section{Demographic Questions}\label{demographic-questions}
\begin{enumerate}[label=(\arabic*)]
\item Age
\item Sex (M/F)
\item Glasses prescription
\item Experience with DAWs (1 = no experience, 2 = beginner, 3 = intermediate, 4 = advanced, 5 = expert)
\item Experience with Augmented reality prior to the experiment 1-5 (1 = no experience, 2 = beginner, 3 = intermediate, 4 = advanced, 5 = expert)
\end{enumerate}

\section{NASA TLX Questions for both non-experts and experts}\label{nasa-tlx}


\begin{enumerate}[label=(\arabic*)]
\item \textbf{Mental Demand}: Using a scale from 1 to 7, where 1 means very low workload and 7 means very high workload, please rate the mental workload involved in manipulating virtual audio objects in the XR system. How mentally demanding was the task?  (1 = very low, 7 = very high)

\item \textbf{Physical Demand}: How physically demanding was it to interact with virtual audio objects using free-hand gestures? How physically demanding was the task? (1 = very low, 7 = very high)

\item \textbf{Temporal Demand}: How hurried or rushed was the pace of the task? (1 = very low, 7 = very high)

\item \textbf{Performance}: How accurately were you able to position and manipulate virtual audio objects using free-hand gestures in the XR environment? How successful were you in accomplishing the task? (1 = perfect, 7 = failure)

\item \textbf{Effort}: How hard did you have to work to accomplish your level of performance? (1 = very low, 7 = very high)

\item \textbf{Frustration}: To what extent did you experience frustration or confusion while using the XR system to manipulate audio objects? How insecure, discouraged, irritated, stressed, and annoyed were you? (1 = very low, 7 = very high)

%% OTHER QUESTIONS
\item \textbf{Efficiency}: Rate the efficiency of completing tasks (e.g., moving, resizing, rotating) with virtual audio objects in the XR system. (1 = very low, 7 = very high)

\item \textbf{Overall Satisfaction}: On a scale from 1 to 7, how satisfied are you with the overall usability of the XR system for manipulating audio objects? (1 = very low, 7 = very high)
\end{enumerate}

\section{Short-answer survey}\label{short-answer-survey}
Questions for non-experts:
\begin{enumerate}[label=(\arabic*)]
\item How intuitive was the process of navigating and interacting with virtual audio objects using free-hand manipulation?
\item Did you encounter any difficulties or frustrations while attempting to manipulate the virtual audio objects? Please describe.
\item Were there any features or functionalities that were unclear or difficult to understand during your interaction with the system?
\item How comfortable did you feel using the XR system to manipulate audio objects in your physical environment?
\item What aspects of the system did you find most useful or enjoyable? Why?
\item Were there any specific improvements or additional features you would suggest to enhance the usability of the system?
\item (Experts only) Where can you see this interface being applied to your audio mixing workflow?
\end{enumerate}

\end{document}