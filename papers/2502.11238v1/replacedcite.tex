\section{Related Work}
The problem of learning optimal policies in average-reward MDPs (AMDPs) is studied in ____. We start with the recent work ____, which was the first to obtain the optimal span-based sample complexity but required prior knowledge of $\tspannorm{h^\star}$ to do so. All earlier work also required knowledge of problem-dependent complexity parameters. More recent work, which we discuss below, has studied the setting without such prior knowledge; see Table~\ref{table:AMDPs} for a summary.

____ and ____ show that it is generally impossible to obtain a multiplicative estimate of $\tspannorm{h^\star}$ with $\text{poly}(SA\tspannorm{h^\star})$ samples. See Appendix \ref{sec:gammahat_hstar} for discussion of the relationship between our algorithms and estimating $\tspannorm{h^\star}$. 
By estimating the MDP's diameter $D$, which upper bounds $\spannorm{h^\star} $ but can be arbitrarily larger ____, the work in ____ removes the need for prior knowledge within the algorithm of ____ but obtains a complexity involving $D$ rather than $\tspannorm{h^\star}$. The Q-learning-based algorithm in ____ uses increasing discount factors and does not require prior knowledge. Their complexity bound however depends on the largest mixing time of all policies, $\tmix$, which  satisfies $3\tmix \geq \tspannorm{h^\star}$ and can be infinite or arbitrarily larger than $\tspannorm{h^\star}$ ____.
(See Appendix~\ref{sec:complexity_params} for definitions of $D$ and $\tmix$.) 

____ and ____ study, respectively, approaches based on stochastic saddle-point optimization and the average-reward plug-in method, both obtaining bounds involving the bias spans of certain policies output by the algorithm. These spans are not generally controlled by $\tspannorm{h^\star}$; in particular, ____ present an example where this is the case and show that the average-reward plug-in approach cannot achieve the optimal $SA \tspannorm{h^\star}/\varepsilon^2$ complexity. ____ also analyzes a DMDP-reduction algorithm that uses a (relatively small) effective horizon independent of $\tspannorm{h^\star}$, achieving a suboptimal complexity with $\tspannorm{h^\star}^2$ dependence.

Also related to the present work are papers studying the sample complexity of the model-based/plug-in approach for discounted MDPs  ____.
We also note that ____ recently developed an algorithm for the \emph{online} setting achieving a $\tspannorm{h^\star}$-based regret bound without requiring prior knowledge. This result does not imply any sample complexity bounds in our setting, because there is no general regret-to-PAC conversion for average-reward MDPs ____, and even if this were possible, their result appears to require $\Omega(S^{40}A^{20}\tspannorm{h^\star}^{10})$ interaction steps before achieving the optimal regret, which would imply a massive ``burn-in'' cost in our setting.


%\setlength{\textfloatsep}{15pt plus 1.0pt minus 2.0pt}
\begin{table}[t]
{
\renewcommand{\arraystretch}{2} 
\centering
\begin{tabular}{p{0.22\textwidth}ccc}
\toprule
Algorithm & Sample Complexity & Reference & \parbox[c]{1.6cm}{Prior\\Knowledge} \\ 
\midrule
%
\multicolumn{1}{m{0.22\textwidth}}{DMDP Reduction} & $SA\frac{\spannorm{h^\star}+1}{\varepsilon^2}$ & ____ & Yes  \\ 
\midrule
%
\multicolumn{1}{m{0.22\textwidth}}{Diameter Estimation + DMDP Reduction} & $SA\frac{D}{\varepsilon^2} + S^2 A D^2 $ & ____ & No  \\ 
%
\multicolumn{1}{m{0.22\textwidth}}{Dynamic Horizon Q-Learning} & $SA\frac{\tmix^8}{\varepsilon^8} $ & ____ & No  \\ 
%
\multicolumn{1}{m{0.22\textwidth}}{Stochastic Saddle-Point Optimization} & $S^2A^2\frac{\spannorm{h^{\pihat}}^4}{\varepsilon^2} $ & ____ & No  \\
%
\multirow{1}{0.22\textwidth}[-.35cm]{Plug-in Approach with Anchoring and Reward Perturbation} &  $SA \frac{\min \{D, \, \tmix\}}{\varepsilon^2}$ \vspace{.1cm} & \multirow{2}{*}{____} & \multirow{2}{*}{No} \\ 
%
 &  $SA\frac{\spannorm{h^\star} + \min\big\{\bigspannorm{\hhatanc^\star}, \spannorm{\hanc^{\pihat}} \big\}}{\varepsilon^2} \vspace{.1cm} $  &  &  \\ 
%
\multicolumn{1}{m{0.21\textwidth}}{$\sqrt{n}$-Horizon DMDP Reduction}~  & $SA\frac{\spannorm{h^\star}^2 + 1 }{\varepsilon^2} $ & ____ & No \\ 
\midrule
%
\multicolumn{1}{m{0.22\textwidth}}{DMDP Reduction + Horizon Calibration} & $SA\frac{\spannorm{h^\star}+1}{\varepsilon^2}$ & Our Theorems \ref{thm:n_based_alg} and \ref{thm:eps_based_alg} & No  \\ 
%
\multicolumn{1}{m{0.22\textwidth}}{Span Penalization} & $SA\,\inf_{\pi : ~\rho^\pi \text{ constant}} \cig\{ \frac{\spannorm{h^\pi}}{(\rho^\pi - \rho^\star + \varepsilon)^2} \cig\}$ & Our Theorem \ref{thm:span_regularization_performance} & No  \\ 
\bottomrule
\end{tabular}

    \caption{\textbf{Algorithms and sample complexity bounds for average reward MDPs} for finding an $\varepsilon$-optimal policy under a generative model (up to $\log$ factors). See Appendix \ref{sec:complexity_params} for the definitions of the complexity parameters $D,\tmix, \tspannorm{h^{\pihat}}, \tspannorm{\hhatanc^\star}, \tspannorm{\hanc^{\pihat}}$ used in prior work. Note that the diameter $D$ and uniform mixing time $ 3 \tmix $ are both upper bounds of $\tspannorm{h^\star}$ and can be arbitrarily larger than $\tspannorm{h^\star}$. 
    The parameters $\tspannorm{h^{\pihat}}, \tspannorm{\hhatanc^\star}, \tspannorm{\hanc^{\pihat}}$ are not generally controlled by $\tspannorm{h^\star}$.
    The guarantee for our Span Penalization algorithm involves the infimum over all policies $\pi$ with constant (state-independent) gain $\rho^\pi$; see Theorem \ref{thm:span_regularization_performance} for an equivalent guarantee in terms of the dataset size.
    }
    
\label{table:AMDPs}
}
\end{table}