\begin{figure*}[htbp]  
    \centering
    \includegraphics[width=0.8\textwidth]{figures/05_UI_screenshot.png} % Changed \linewidth to \textwidth
    \caption{The main interface of AIdeation includes (a) the Ideas Overview Panel, displaying all brainstormed design ideas as images with titles based on user input. Users can select an idea to view in (b) the Idea Detail Panel, which provides detailed information on the selected idea. (b1) The left sidebar lists key elements extracted from the idea, categorized into six groups as keywords. Users can select a keyword to view related search results in (b2). (b3) The right panel allows users to refine the idea by combining it with references or by instruction. (b4) Below the current idea, its origin is shown; in this case, the idea was generated by combining "Idea 4" and a colorful sofa.}
    \Description{The figure presents the AIdeation interface, divided into two main sections. (a) The Ideas Overview Panel displays multiple AI-generated design ideas as images with titles. Users can select an idea (a1) to explore further. (b) The Idea Detail Panel provides more information on the selected idea. (b1) The left sidebar lists extracted key elements, categorized into six keyword groups. Clicking a keyword retrieves related search results (b2), such as antique telephones. (b3) The right panel allows users to refine the idea by combining it with references or modifying it by instruction. (b4) Below the current idea, its origin is shown, demonstrating that this idea was created by merging "Idea 4" with a weathered vintage sofa. The interface supports iterative ideation by integrating brainstorming, reference searching, and refinement.}
    \label{fig:ui}
\end{figure*}

\section{SYSTEM \& IMPLEMENTATION}
We propose AIdeation, a system that integrates multiple generative models to enhance concept designers' early ideation phase. Unlike existing tools, AIdeation combines the strengths of traditional and AI-driven approaches, streamlining the process by unifying research, brainstorming, and design idea refinement into a cohesive, iterative workflow.

\subsection{System Components}
AIdeation's key design aligns closely with the system’s design goals: Breadth exploration through brainstorming, Depth exploration via Research, and Flexible iterative exploration through Refining Design Ideas. To illustrate how AIdeation supports the early ideation phase for concept designers, we present a real-world design task from one of our users, who was tasked with creating game environments for a horror game set in traditional Taiwanese apartments.

\subsubsection{Brainstorming: Supporting breadth exploration}
After receiving the design specification, the designer inputs the instruction into AIdeation: "Design a living room scene for a horror game set in an old Taiwanese apartment." AIdeation generates 8 distinct design ideas, each featuring various elements accurately aligned with the specification. In our design, the ideas are described across six key design elements: Theme, Contents, Art Style, Lighting and Atmosphere, Color Palette, and Shot Angle. These categories are derived from observations in our formative study. We use "Theme" to represent the main reference, as designers normally base their primary searches on the central theme of the design task. "Content" covers detailed references for objects and elements within the scene. The other categories were selected based on the references most frequently used by concept designers. Composition was excluded due to current AI limitations. These categories were later reviewed by the same art directors mentioned in Section 4.

% The ideas are described across 6 categories of design elements: Theme, Art Style, Contents, Lighting and Atmosphere, Color Palette, and Shot Angle. These categories are based on our formative study observations, reflecting how designers collect and prioritize specific design elements in their references.


The ideas are presented as generated images in an ideas overview panel (Figure \ref{fig:ui}-a), offering a clear visual summary of each design and its key components, which can serve as potential hero references. This approach directly addresses challenges identified in the formative study, enabling designers to efficiently grasp the design topic while exploring a diverse range of visuals that align with the design specification and can be incorporated into their creative process.

\subsubsection{Research: Supporting depth exploration}
After selecting a design idea of interest (Figure \ref{fig:ui}-a1), the designer is directed to the idea detail panel (Figure \ref{fig:ui}-b), which provides in-depth information about the chosen idea. The left-side information bar displays key elements of the generated image extracted as keywords (Figure \ref{fig:ui}-b1), organized into 6 categories corresponding to the design idea description. In the "Content" category, elements are further divided into subcategories like "Central Focus" and "Background" due to the volume of information. This structure helps the designer clearly understand the composition of the design and easily identify specific elements in the generated image.

The idea detail panel also allows users to explore supporting references by clicking on relevant keywords. When a keyword is selected, corresponding search results are displayed in the same panel (Figure \ref{fig:ui}-b2), giving access to additional information and detailed references. Combined with diverse outputs from brainstorming, these features provide users with a broader array of ideas and information, facilitating deeper exploration and a more comprehensive understanding of the design topic and generated concepts.


\subsubsection{Refining idea: Supporting flexible iterative exploration}
Following this, AIdeation allows designers to refine the selected design idea using the detailed information provided through a flexible iterative approach—either by combining it with additional references or refining it through specific instructions (Figure \ref{fig:ui}-b3). These options enable users to either expand their exploration or narrow and focus their design scope, depending on their creative needs. 

After identifying a reference of interest based on the selected keyword (Figure \ref{fig:ui}-b2), the designer can combine it with the current idea to generate 5 new design variations. Figure \ref{fig:ui}-b illustrates the result of combining a previous design idea with a selected reference (Figure \ref{fig:ui}-b4). AIdeation adjusts the original design, such as transforming the style of the sofa to match the selected reference, demonstrating how the design scope can be refined. Conversely, if the reference is less related to the original elements, the new design will be more diverse, offering additional creative possibilities.

For the "refine by instruction" feature, once the designer identifies specific elements in the current design, they can use natural language to instruct AIdeation on what to change. These refinements can be based on AIdeation's provided information or the designer’s creative vision. The system then generates 5 new designs that incorporate the user’s instructions, maintaining the essence of the original idea while introducing diversity.

\subsubsection{Next ideation cycle for exploration}
AIdeation enables users to begin the next brainstorming cycle seamlessly based on the current design idea (Figure \ref{fig:ui}-b3, "Explore More"). This feature meets the need for designers to create related tasks based on an existing environment, as noted by participants in the formative study. For example, the designer could input “\textit{design a kitchen based on this}” using the design idea from Figure \ref{fig:ui}-b, efficiently expanding on the current concept.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/06_Technical_pipeline.png}
    \caption{Technical pipeline of AIdeation: (a) The user’s input image is captioned by a vision model and processed by Idea Generation GPT, which integrates instructions and creative score to generate design ideas description. This idea is then converted into keywords, and DALL-E 3 generates an image with the idea description. (b) User-selected keywords initiate a Bing Image Search, returning a set of relevant images. When the user wants to refine the idea, (c) a selected reference is captioned by a vision model and processed by Combine Reference GPT, merging it with the original idea to create modified designs based on the creative score. (d) In contrast, AIdeation also supports refining ideas by instruction. The original idea and user instructions are processed by Refine by Instruction GPT, along with the creative score, to generate additional refined ideas.}
    \Description{The figure illustrates the AIdeation technical pipeline, consisting of four stages. (a) Brainstorming begins with a user-provided input and image, which is captioned by a vision model and processed by Idea Generation GPT to create a design idea. Keywords are extracted, and DALL·E 3 generates a corresponding image. (b) Research allows users to select keywords, triggering a Bing Image Search that retrieves relevant images for further exploration. (c) Refining by Combining Reference integrates a selected reference with the original idea using Combine Reference GPT, producing a modified design with an updated description and newly generated image. (d) Refining by Instruction enables users to modify the idea by providing textual input, which is processed by Refine by Instruction GPT to create an adjusted design. Each step incorporates a creative score to guide idea development iteratively.}
    \label{fig:technical}
\end{figure*}


\subsection{Technical Implementation}
\subsubsection{Brainstorming and research}
AIdeation accepts both textual instructions and image inputs. As shown in Figure \ref{fig:technical}-a, when an image is provided, it is first processed by the GPT-4o Vision model, which generates an image caption. Then, we use Idea Generation GPT—a specially prompted LLM (GPT-4o-2024-05-13 as the base model)—designed for generating environment concept design descriptions (see prompt in Appendix \ref{AppendixC}). The instruction, image caption (if applicable), and creative score are then passed into the LLM. Based on the user’s input, the model generates multiple design ideas in parallel. Each idea was assigned a creative score, ranging from 0 to 1, to reflect the diversity of the outputs. A higher creative score prompts Idea Generation GPT to produce more innovative design descriptions. The output format is detailed in Section 5.1.1.

Each generated idea is processed in two ways: 1) important information is extracted as keywords using a prompted Keyword Extraction GPT (see Appendix \ref{AppendixD}) and displayed in the idea detail panel, and 2) the idea is input into an image generation model to create an image, which is shown in the ideas overview panel. For this work, we used DALL-E 3 as the image generation model due to its ability to interpret natural language prompts, understand complex instructions, and generate corresponding images \footnote{Dall-E, https://openai.com/index/dall-e-3/}. The images are produced at a resolution of 1792x1024, suitable for environment concept design. The entire generation process takes approximately 30 seconds, or around 40 seconds on average when an image is included as input.

In the Idea Detail Panel, when a keyword is selected from the left-side information bar, it is sent to the backend, where the Bing Image Search API \footnote{Bing Image Search API, https://www.microsoft.com/en-us/bing/apis/bing-image-search-api} retrieves a batch of 50 images (Figure \ref{fig:technical}-b). Scrolling to the end of the page triggers an additional batch of images.

\subsubsection{Refining idea and next ideation cycle for exploration}
When a user selects a reference to combine with the current design idea, the reference is processed through the GPT-4o Vision model to generate image captions (Figure \ref{fig:technical}-c). The selected keyword, image caption, creative score (following the same distribution as brainstorming), and the current design description are then input into the Combine Reference GPT (see Appendix \ref{AppendixE}). This GPT modifies the design description by incorporating details from the reference image based on the selected keyword. For example, in Figure \ref{fig:ui}-b4, the keyword "Weathered Vintage Sofa" updates the corresponding section of the original design idea with the sofa's style from the reference image. If the reference is less related to the current design elements, the GPT will make broader adjustments, incorporating the reference while modifying other parts of the description. The level of modification is influenced by the creative score—higher scores result in more significant changes and diverse combinations, offering both control and variety. 

Figure \ref{fig:technical}-d illustrates the technical process of the "Refine by Instruction" feature. Like the brainstorming and reference combination processes, the prompted "Refine by Instruction GPT" (see Appendix \ref{AppendixF}) uses the user’s instruction, creative score, and current design description as inputs. The GPT adjusts the design based on the instruction, with the creative score determining the extent of changes and creativity. The modified design descriptions follow the same format, allowing for later keyword extraction and image generation, just like in the brainstorming process. Both Idea Refinement process takes a similar amount of time as Brainstorming.

For the Next Ideation Cycle for Exploration, the process follows the same structure as the brainstorming phase, with the key difference being that image captioning is replaced by the current design idea description.
