\clearpage
\setcounter{page}{1}
\maketitlesupplementary

\appendix

\section{Datasets}
\label{app:datasets}


\paragraph{SurroundOcc-nuScenes \citep{wei2023surroundocc} and Occ3D-nuScenes \citep{tian2023occ3d}} are derived from the nuScenes dataset \cite{caesar2020nuscenes}. nuScenes provides 1000 scenes of surround view driving scenes in Boston and Singapore split in three sets train/val/test of size 700/150/150 scenes. Each comprises 20 seconds long and is fully annotated at 2Hz using one ground truth from 5 radars, 6 cameras at resolution $900 \times 1600$ pixels, one LiDAR, and one IMU.
From the LiDAR annotations, SurroundOcc-nuScenes \cite{wei2023surroundocc} derived a 3D grid of shape $[200,200,16]$ with a range in $[-50,50] \times [-50,50] \times [-5,3]$ meters at a spatial resolution of $[0.5,0.5,0.5]$ meters, annotated with 17 different classes, 1 representing empty and 16 for the semantics. 
The Occ3d-nuScenes \cite{tian2023occ3d} dataset has a lower voxel size of 0.4 meters in all directions while keeping the same voxel grid shape with a range of $[-40,40]\times [-40,40]\times [-1,5.4]$ meters. It contains 18 classes: 1 representing empty, 16 for the semantics, and 1 for others.
\paragraph{SSCBench-Kitti360 \cite{li2024sscbench}} is derived from the Kitti360 dataset \cite{Liao2022PAMI}. Kitti360 consists of over 320k images shot by 2 front cameras at a resolution $376 \times 1408$ pixels and two fisheye cameras in surburban areas covering a driving distance of 73.7km. Only one camera is used in the 3D occupancy task.
SSCBench-Kitti360 \cite{li2024sscbench} annotates for each sequence a voxel grid of shape $[256,256,32]$ with a range in $[0,51.2] \times [-25.6,25.6] \times [-2,4.4]$ meters at a voxel resolution of 0.2 in all directions.
{The provided voxel grid is annotated with 19 classes: one is used to designate empty voxels, and the 18 other are used for the semantic classes.}

\section{Models and implementation details}
\label{app:models}

We integrate our rendering module and associated loss into three different models: \textbf{SurroundOcc} \cite{wei2023surroundocc} (multi-scale voxel-based approach), \textbf{TPVFormer} \cite{huang2023tpv} (triplane-based approach), and \textbf{Symphonies} \cite{jiang2024symphonies} (voxel-with-instance query-based approach).
Each model is retrained using the same training setting, following the optimization parameters from SurroundOcc. No extensive hyperparameter searches are conducted on the learning rate; the goal is to demonstrate that the loss can be integrated at minimal cost into existing pipelines. All models are trained for 20 epochs on 4 A100 or H100 GPUs with a batch size of 1, using an AdamW optimizer with a learning rate of $2e^{-4}$ and a weight decay of $0.01$.
For each combination of models and datasets, we evaluate existing checkpoints if provided; otherwise, we report the scores from previous papers when available or we re-train the models. Note that we used the official checkpoint for Symphonies \cite{jiang2024symphonies} while noticing there is a discrepancy in IoU / mIoU between the reported value in the paper and the actual one of the official checkpoint, as explained in their GitHub issue \footnote{https://github.com/hustvl/Symphonies/issues/5}.


\section{Computational cost}

{Our module introduces a computation overhead for each rendering it performs. For a given input scene, we generate two views (one `cam' and one `bev'), and for each view, we render both the predictions and the ground truth, resulting in a total of four renderings per iteration.
}

{
As reported in \cref{tab:gpu_memory_comparison}, training with \method{} incurs a modest increase in memory and computation time (~10\%), even with high-resolution renderings. This overhead can be further reduced by pre-selecting camera locations, allowing annotation renderings (the two ground-truth renderings) to be pre-processed in advance. Additionally, lower rendering resolutions can be used if needed.
}

{While \method{} introduces a small per-iteration cost, it actually accelerates learning. A key observation is that models using \method{} reach the same performance level as their baseline counterpart ~17\% faster. Overall, despite a minor increase in computational overhead, \method{} ultimately reduces the total training time required to achieve comparable or superior performance.}



\input{tables/computational_cost}

\section{Ablations}
\subsection{Gaussian scaling}

An important parameter in our rendering process is the fixed size of the Gaussians representing voxels. To study its impact, we train a TPVFormer \cite{huang2023tpv} model on Occ3d-nuScenes \cite{tian2023occ3d}, varying the Gaussian scale for both ground-truth and predicted renderings. We train models using only the 2D rendering losses (\autoref{eq:rendering_losses}), excluding the usual 3D voxel losses to isolate the effect of scale on rendering metrics.

\input{fig/impact_of_scale_fig}

Our results, shown in \autoref{fig:impact_of_scale}, highlight the importance of the Gaussian scale. If the Gaussians are too large, only a few will cover the image, and the loss will be backpropagated mainly from the nearest ones. If they are too small, gaps appear between voxels, leading to sparse activations and a model that renders mostly the empty class, yielding poor metrics.

Theoretically, the optimal size should correlate with the voxel size. For Occ3d-nuScenes and SurroundOcc-nuScenes, the optimal scale is $s = 0.25$ and $s = 0.20$, while for SSCBench-KITTI360, it is $s = 0.1$. This aligns with our intuition: a voxel should be represented by a spherical Gaussian with a standard deviation such that $2s = c$, where $c$ is the voxel side.

Qualitatively, \autoref{fig:impact_of_scale} shows the effect of scale on rendering, confirming the need for a balanced Gaussian size to avoid either sparse activation or excessive concentration on nearby elements.


\input{fig/visu_scale_render}

\subsection{Loss balance}

We {investigate the importance of the balance} $\lambda$ between the 2D loss and the 3D loss. If the {weight of the} 2D loss is too high, there is a risk of optimizing the image rendering at the expense of voxel predictions. Conversely, if the 2D loss is too low, its contribution to the learning process may be {overlooked}. To analyze this, we vary the {contribution $\lambda$ of the 2D loss} and study the impact on the final metrics, {as reported in} \autoref{fig:lambda_impact}. Based on the training results of TPVFormer \cite{huang2023tpv} on a subset of Occ3D-nuScenes \cite{tian2023occ3d}, we set the weight to $\lambda=15$.
\input{tables/lambda_loss}

\subsection{Cameras}
\label{app:ablation:camera}

{
For a given input, we can position as many cameras as needed to render multiple views. In this experiment, we explore using multiple cameras by selecting from the six available views.
While, in theory, more cameras could provide more accurate gradients, we observe in practice that it does not significantly impact the final results (\autoref{fig:num_cams_impact}).
Since additional cameras introduce computational overhead, we opt to render from a single camera per iteration, changing its position across batches according to the strategy defined in \autoref{sec:methods:camera}.
}


\input{tables/number_of_cameras}

\section{Scores detailed per class}
\label{app:results}

The following tables give the detailed IoU and mIoU scores of the models studied for each dataset. \autoref{tab:Occ3d-nuScenes} concerns the Occ3D-nuScenes dataset \cite{tian2023occ3d}, \autoref{tab:SSCBench-KITTI360} the SSCBench-KITTI360 dataset \cite{li2024sscbench} and  \autoref{tab:SurroundOcc-Nuscenes} the SurroundOcc-nuScenes dataset \cite{wei2023surroundocc}. 

\section{Qualitative results}
\label{app:qualitative}
In \autoref{fig:qualitative_0} and \autoref{fig:qualitative_1}, we respectively present qualitative results on randomly selected scenes from SurroundOcc-nuScenes \cite{wei2023surroundocc} and Occ3d-nuScenes datasets \cite{tian2023occ3d}. We also provide complete gifs in our github.


\input{tables/surr_results}
\input{tables/occ3D_results}
\input{tables/kitti360_results}

\include{fig/qualitative}
