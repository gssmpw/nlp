\section{Related work}
\label{sec:rw}

\subsection{Learning 3D semantic geometry from cameras}



Reconstructing unified 3D scenes from multi-camera systems must address three interconnected challenges: multi-view cameras create perspective limitations requiring occlusion reasoning, lifting 2D image features to metric 3D space demands geometric and semantic 2D-to-3D transfer, and merging multi-view inputs into volumetric representations like voxels incurs cubic memory growth \citep{xu2023survey3Docc}.
Modern methods address these challenges through structured intermediate representations.



Discrete methods rely on regular 3D grids to stay as close as possible to the desired voxelic output.
Bird’s-eye-view (BeV) projections \cite{tian2024mambaocc, li2024viewformer} significantly reduce memory cost but introduce a substantial bias in the intermediate representation due to height compression.
Instead, tri-plane features \cite{huang2023tpv} and tensor decompositions \cite{zhao2024lowrankocc} map any 3D coordinate onto distinct planes or vectors performing interpolation to get distinct features. As opposed to BeV approaches, they do not suffer from height compression while still operating on a compressed representation.
Octrees \cite{lu2023octreeocc} preserves a full multi-scale 3D representation, coarse in uniform areas and fine where details are needed, resulting in a fast and memory-efficient representation~\cite{wei2023surroundocc}.
Continuous architectures have emerged, using a set of Gaussians to represent the scene \cite{huang2024gaussianformer2, huang2024gaussian} which {are then} discretized to obtain the voxelized output map. Furthermore, to better capture the overall instance semantics and scene context, query-based methods have been introduced \cite{jiang2024symphonies}. They facilitate interactions between image and volume features using sparse instance queries.





\begin{figure*}[t]
\centering
    \includegraphics[width=1.0\linewidth]{fig/pipeline_5.pdf}
    \caption{\textbf{Overview of \method{}.}
    Our module enforces 3D-2D consistency via differentiable Gaussian rendering. First, both predicted and ground-truth voxel grids are `gaussianized' by converting each voxel into a simple spherical Gaussian: the center $\mu$ is fixed at the voxel center, {the scale $s$ is a simple fixed scaling of the original voxel dimensions}, and features are directly transferred {as the semantic `color' $c$} --- with only the opacity $o$ learned when voxel features are available. Next, virtual cameras are positioned in the scene (a fixed bird’s-eye view and a dynamic, arbitrarily placed camera as described in \autoref{sec:methods:camera}). The resulting 3D Gaussians are then projected into 2D using Gaussian splatting (\autoref{sec:methods:gaussian_rendering}), producing both semantic and depth renderings. These rendered views are compared against their ground-truth counterparts using an L1 penalty, ensuring enhanced spatial coherence and geometric consistency (\autoref{sec:methods:l2d_details}).
    }
    \label{fig:pipeline}
\end{figure*}


Complementary approaches have been proposed to enhance 3D occupancy predictions beyond architectural choices.
Temporal aggregation uses past frames to resolve occlusions and refine geometric details \cite{li2024viewformer, ye2024cvtoccc, li2024hierarchical, silva2024uniftri, pan2024renderocc}, with extensions to 4D forecasting for dynamic scenes \cite{khurana2023point, yang2025drivingoccupancy, wang2024occsora, ma2024cotr}.
Self-supervised methods reduces dependency on 3D annotations by generating pseudo-labels from monocular depth and segmentation \cite{huang2024selfocc, gan2024gaussianocc}, but suffer from scale miscalibrations and label mismatches between 2D and 3D labels.
Lastly, supervised rendering with lidar reprojections \cite{pan2024renderocc,sun2024gsrender} has been introduced to provide accurate distances. While more precise than pseudo-annotations, lidar reprojections face challenges such as signal sparsity, occlusions, and misalignment between the lidar and cameras.
{In particular, \cite{pan2024renderocc,sun2024gsrender} require temporal supervision from adjacent frames to compensate for lidar sparsity and occlusions. In contrast, our method achieves accurate supervision without these constraints, allowing more flexible and efficient 3D occupancy learning across different architectures.
}

\input{fig/camera_location}

Despite architectural variations, all methods ultimately produce voxel grids for supervision. This common 
output space lets \method{} enhance any 3D occupancy model with a simple rendering loss applied to the voxelic output.






\subsection{Differentiable rendering of 3D representations}

Supervising 3D predictions through 2D projections requires efficient differentiable rendering methods.
While traditional differentiable rendering methods handle 3D modalities like point clouds and meshes \cite{kato2018meshrenderer, miu2018paparazzi}, recent approaches focus on neural rendering and Gaussian-based methods.

NeRF-based methods \cite{mildenhall2020NeRF} perform volume rendering by predicting a volumetric density and applying ray-based integration. RenderOcc \cite{pan2024renderocc} constructs a NeRF-style volume representation supervised by semantic LiDAR projections. It leverages lidar-derived 2D labels (depth and semantics) for 3D supervision but inherits lidar's sparsity.
SelfOcc \cite{huang2024selfocc} uses signed distance fields for occupancy prediction and applies differentiable volume rendering to synthesize depth and semantic views. Self-supervision is enforced using multi-view consistency from video sequences.
However, NeRF-based rendering is computationally expensive, especially with high ray-sampling resolutions. Additionally, its reliance on image quality and occlusion mitigation necessitates auxiliary supervision from temporal frames \citep{sun2024gsrender,pan2024renderocc}.

Gaussian splatting provides an efficient alternative to NeRF by representing 3D scenes as a set of Gaussians \citep{kerbl3Dgaussians}. In 3D semantic occupancy prediction, GaussianOcc \cite{gan2024gaussianocc} projects a voxel-based Gaussian representation camera, using adjacent views for self-supervision, with optional 2D segmentation to refine the predicted occupancy.
{GSRender~\cite{sun2024gsrender} uses lidar reprojection and temporal consistency to help the learning process.}
GaussTR \cite{jiang2024gausstr} extends Gaussian representation learning with foundation models (e.g., CLIP, Grounded-SAM) to enable open-vocabulary occupancy prediction. Unlike other self-supervised methods, it does not use temporal supervision but relies on heavy pre-trained vision models.
A key limitation of these approaches is their tight coupling of Gaussian representations with the model architecture, imposing a fixed representation that limits flexibility. 
In contrast, our approach introduces a Gaussian rendering loss applicable to any model, without requiring the underlying 3D representation to be Gaussian.



All aforementioned methods are restricted to rendering from recorded camera perspectives, as they require RGB images to estimate pseudo-labels \citep{huang2024selfocc,gan2024gaussianocc,jiang2024gausstr} or rely on semantic LiDAR reprojections \citep{pan2024renderocc,sun2024gsrender}.
{In contrast, as \method{} renders both the ground truth and the predictions, it is not constrained by fixed camera perspectives or temporal supervision, enabling rendering from any viewpoint.}
Moreover, \method{} does not enforce a specific 3D feature representation, {and operates at prediction-level}, making it adaptable to diverse architectures.








