\begin{abstract}
Understanding the 3D geometry and semantics of driving scenes is critical for safe autonomous driving.
{Recent advances in 3D occupancy prediction have improved scene representation but often suffer from spatial inconsistencies, leading to floating artifacts and poor surface localization. Existing voxel-wise losses (e.g., cross-entropy) fail to enforce geometric coherence.}
In this paper, we propose \method{}, a module that improves 3D occupancy learning by enforcing projective consistency. 
{Our key idea is to project both predicted and ground-truth 3D occupancy into 2D camera views, where we apply supervision.}
{Our method penalizes 3D configurations that produce inconsistent 2D projections, thereby enforcing a more coherent 3D structure.}
{To achieve this efficiently, we leverage differentiable rendering with Gaussian splatting.}
\method{} seamlessly integrates with existing architectures while maintaining efficiency and requiring no inference-time modifications.
Extensive evaluations on {multiple} benchmarks (SurroundOcc-nuScenes, Occ3D-nuScenes, SSCBench-KITTI360) demonstrate that \method{} significantly improves geometric fidelity across various 3D occupancy models (TPVFormer, SurroundOcc, Symphonies), achieving state-of-the-art results, particularly on surface-sensitive metrics.
The code is open-sourced at \url{https://github.com/valeoai/GaussRender}.
\end{abstract}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/teasing.pdf}
    \caption{\textbf{Comparison of rendered 3D predictions.} Standard 3D Occupancy models trained on per-voxel losses result in physically implausible predictions (e.g., floating voxels, poorly localized surfaces, highlighted with \textcolor{orange}{orange} ellipses) that maintain high 3D IoU but fail to produce consistent predictions. \method{} enforces multi-view consistency, eliminating artifacts through learning projective constraints.}
    \label{fig:teaser_quali}
\end{figure}
