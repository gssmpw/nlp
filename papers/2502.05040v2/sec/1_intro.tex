





\section{Introduction}

Understanding the 3D geometry and semantics of driving scenes from multiple cameras is both a fundamental challenge and a critical requirement for autonomous driving. This problem is central to perception tasks such as object detection \cite{li2022bevformer, liu2022petr, li2023bevdepth, lin2022sparse4d, jiang2024far3d, zou2024unim}, agent forecasting \cite{kim2021lapred, yuan2021agentformer, gu2023vip3d, feng2024unitraj, shi2024mtrpp, shi2022motion, xu2024towardsmotion, xu2024ppt}, and scene segmentation \cite{chambon2024pointbev, gui2024fiptr, hu2021fiery, hu2022stp3, bartoccioni2022lara, harley2022simple, sirkogalouchenko2024occfeat}.
3D occupancy prediction \cite{vobecky2023pop3d, huang2023tpv, wei2023surroundocc, jiang2024symphonies, li2023fbocc3d} has emerged as a task to evaluate how well models capture the spatial structure and semantics of a scene.

The challenge in 3D occupancy prediction lies in achieving geometrically coherent reasoning from multi-view images.
Existing methods \citep{wei2023surroundocc,huang2023tpv,huang2024gaussianformer2,li2023voxformer} typically optimize per-voxel predictions using standard 3D losses, such as cross-entropy, Dice \cite{sudre2017generalised}, or Lov√°sz \cite{berman2018lovasz}. However, these losses treat all voxels equally and fail to enforce spatial consistency between neighboring voxels, leading to fragmented or floating occupancy artifacts, {as {noticed in prior works \citep{liu2024sparseocc} and} illustrated in \autoref{fig:teaser_quali}}. 
{Such artifacts (floating voxels, disjoint surfaces, and misaligned boundaries) often have a low impact on voxel-based segmentation losses, which give the same weight to all voxels.}
The consequences extend beyond mere visual artifacts: poor surface localization and unrealistic floating objects may significantly undermine downstream tasks such as free-space estimation or motion planning.
{The underlying issue is that conventional supervision methods lack mechanisms to penalize unrealistic spatial arrangements, a deficiency that becomes especially apparent when projecting 3D artifacts into 2D views.}
{This observation motivates our key insight: integrating projective consistency into the training objective encourages the model to learn consistent and physically plausible geometries.}

We {introduce} \method{}, a {module} that bridges the gap through differentiable rendering of 3D occupancy predictions.
Our key idea is to enforce spatial consistency {during training,} by projecting both predicted and ground-truth voxels into 2D camera views using Gaussian splatting \cite{kerbl3Dgaussians}.
{These projections serve as a supervision signal, allowing us to penalize 3D configurations that produce poor 2D projections, thereby enforcing a more coherent and geometrically plausible 3D structure.}
{To achieve this, we apply two complementary supervision signals:}
{(1) a semantic rendering loss that enforces local semantic coherence, }
and (2) a depth rendering loss that penalizes occlusion-disrupting artifacts.
These rendering-based losses are applied alongside standard 3D supervision, such as cross-entropy, ensuring compatibility with existing occupancy learning frameworks.
{\method{} enables rendering from arbitrary viewpoints.}
Its flexibility helps reduce occlusions, 
for instance, by leveraging elevated viewpoints 
that are {less affected by horizontal obstructions from 
ground objects.}

We validate our approach on multiple datasets, including SurroundOcc-nuScenes \cite{wei2023surroundocc}, Occ3D-nuScenes \cite{tian2023occ3d}, and SSCBench-KITTI360 \cite{li2024sscbench}. Our experiments demonstrate that \method{} significantly improves geometric fidelity across diverse architectures --- ranging from multi-scale voxel-based models (SurroundOcc \cite{wei2023surroundocc}), tri-plane models (TPVFormer \cite{huang2023tpv}) to hybrid query-voxel models (Symphonies \cite{jiang2024symphonies}).
Across all settings and datasets, \method{} consistently achieves state-of-the-art performance on classical metrics such as IoU and mIoU. Moreover, when evaluated using surface- and artifact-sensitive metrics, {such as RayIoU~\cite{liu2024sparseocc}},
the improvements are even more pronounced. This is because the projective constraints enforced by our method promote surface continuity --- ensuring that neighboring voxels agree when rendered from any viewpoint, thereby eliminating floating artifacts and discontinuities.
Crucially, our approach is plug-and-play, integrating seamlessly with existing 3D occupancy frameworks without requiring any architectural modifications. Thanks to our efficient Gaussian rendering proxy, \method{} incurs minimal computational and memory overhead compared to NeRF-based alternatives \cite{huang2024selfocc, pan2024renderocc}.
Moreover, it introduces only a few hyperparameters, which are only loosely sensitive, ensuring straightforward integration into any pipeline.

The key contributions include:
\begin{itemize}
    \item {A rendering-based {module} that enforces semantic and geometric consistency in 3D occupancy prediction, eliminating spatial artifacts through projective constraints,}
    \item {A fast and memory-efficient loss implementation-based on Gaussian splatting} without architectural modifications during training and having no impact during inference,
    \item {A viewpoint selection strategy} that amplifies supervision signals in geometrically complex regions.
    \item State-of-the-art results on three standard benchmarks, with particular gains in surface-sensitive metrics.
\end{itemize}





