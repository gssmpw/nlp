\section{Experiments}
\label{sec:exp}

We evaluate \method{} on several models and datasets to demonstrate its versatility. We present experimental details in \autoref{sec:exp:details}, compare our results against state-of-the-art models and datasets in \autoref{sec:exp:sota}, show how \method{} enhances 3D semantic occupancy predictions from multiple views in \autoref{sec:exp:finer-details}, and present ablations in \autoref{sec:exp:ablations}.

\subsection{Data and models}
\label{sec:exp:details}

\paragraph{Data.}
The trainings and evaluations are conducted on three datasets: SurroundOcc-nuScenes \cite{wei2023surroundocc}, Occ3d-nuScenes \cite{tian2023occ3d}, and SSCBench-Kitti360 \cite{li2024sscbench}.
We briefly outline below the specific characteristics of each dataset. More details can be found in \autoref{app:datasets}.

\textbf{SurroundOcc-nuScenes} is derived from the nuScenes dataset \cite{caesar2020nuscenes}, acquired in Boston and Singapore. It aggregates the lidar annotations of nuScenes to create 3D semantic occupancy grids of range 
{$[\text{-}50,50] \! \times \! [\text{-}50,50] \! \times \! [\text{-}5,3]$ meters with 50cm voxel resolution}, 
with labels corresponding to 17 lidar semantic segmentation classes.
This dataset takes into account both visible and occluded voxels. The occluded voxels are obtained by accumulating lidar data over the frames of the whole sequence consequently introducing temporal artifacts over dynamic objects. 

\textbf{Occ3D-nuScenes} is also based on {the} nuScenes dataset. It contains 18 semantic classes and has 
{a 40cm voxel grid of range $[\text{-}40,40] \! \times \! [\text{-}40,40] \! \times \! [\text{-}1,5.4]$ meters.}
One major difference with SurroundOcc-nuScenes, is that Occ3D only evaluates the voxels visible from the cameras at the current time frame. Thus, it focuses on the geometric and semantic understanding of the visible objects, rather than extrapolating to occluded regions, leading to a simpler task.

\textbf{SSCBench-Kitti360} \cite{li2024sscbench} is derived from the Kitti360 dataset \cite{Liao2022PAMI}, acquired in Germany. 
{It contains 19 semantic classes and has a 20cm voxel grid of range $[0,51.2] \! \times \! [-25.6,25.6] \! \times \! [-2,4.4]$ meters, resulting in a very precise semantic of urban scenes.}
{It evaluates both visible and occluded voxels.}


\paragraph{Models and training details.}
We {integrate} \method{} into three different models that use different intermediate representations: \textbf{SurroundOcc} \cite{wei2023surroundocc} (multi-scale voxel-based approach), \textbf{TPVFormer} \cite{huang2023tpv} (tri-plane-based approach), and \textbf{Symphonies} \cite{jiang2024symphonies} (voxel-with-instance query-based approach).
By doing so we validate the claim that our proposed approach is compatible with any type of architecture.
For each combination of models and datasets, we follow the same procedure. {By default, we evaluate the original model checkpoints when available; otherwise, we report scores from previous papers if provided or re-train the models for the target dataset}. Each model uses the same training settings, following the optimization parameters of ~\cite{wei2023surroundocc}. 
{
Camera strategy parameters (elevation and translation are detailed in~\autoref{sec:exp:ablations}).
}
More {technical} details can be found in \autoref{app:models}.

\subsection{3D semantic occupancy results} %
\label{sec:exp:sota}


\input{tables/unified_benchmark}

We evaluate \method{} across multiple models \citep{wei2023surroundocc,huang2023tpv,jiang2024symphonies} and datasets \citep{wei2023surroundocc,tian2023occ3d,li2024sscbench} for 3D semantic occupancy prediction. Our method consistently improves performance without {relying on} other sensors such as the lidar used in certain methods \cite{pan2024renderocc, sun2024gsrender}. Results are summarized in \autoref{tab:unified_benchmark} and detailed scores by class can be found in \autoref{app:results}.

\textbf{SurroundOcc-nuScenes \citep{wei2023surroundocc}.}
On {this dataset}, considering visible and occluded voxels, \method{} brings significant gains to all tested models. As shown in \autoref{tab:unified_benchmark}, TPVFormer \cite{huang2023tpv} and SurroundOcc \cite{wei2023surroundocc} reach the top two ranks. They achieve higher IoU (+1.2\% and +1.1\%) and mIoU (+3.8\% and +0.5\%) compared to their original implementations.
Remarkably, \method{} enables these models to surpass {more} recent approaches like GaussianFormerV2 \cite{huang2024gaussianformer2} in both IoU and mIoU, proving that older architectures can achieve state-of-the-art results when {their training is enhanced with \method{}.}

\input{tables/rayiou}

\textbf{Occ3D-nuScenes.}
On Occ3D-nuScenes \cite{tian2023occ3d}, filtering unobserved voxels, \method{} leads to the top two results: TPVFormer with \method{} achieves 30.48 mIoU (+2.65), ranking first, and SurroundOcc with \method{} reaches 30.38 mIoU (+1.17), ranking second.
Notably, our approach outperforms RenderOcc \cite{pan2024renderocc} and GSRender \cite{sun2024gsrender} without requiring lidar inputs for loss computation. This demonstrates {that methods augmented with \method{}} can learn strong 3D representations using only cameras. 

\textbf{SSCBench-Kitti360.}
On the challenging SSCBench-Kitti360 \cite{Liao2022PAMI} dataset, integrating \method{} to SurroundOcc \cite{wei2023surroundocc} and Symphonies \cite{jiang2024symphonies} {boosts} IoU (+0.11\% and +0.68\%) and mIoU (+0.26\% and +0.29\%).
While absolute gains appear smaller here, models in this benchmark achieve tightly clustered performance due to the smaller voxel size making the segmentation task more difficult; even modest improvements are difficult to achieve.

Our results show that \method{} consistently enhances various architectures leading to state-of-the-art results, reaches top results without requiring projected lidar annotations, and remains effective across different dataset scales and annotation densities. This demonstrates the significant advantages of \method{} for 3D semantic occupancy learning.

\subsection{Finer-grained multi-view metric analysis}
\label{sec:exp:finer-details}


\input{tables/image_depth_bev_metrics}

\paragraph{RayIoU.}
{Classical 3D occupancy metrics, such as voxel-wise IoU, treat all voxels equally, often failing to capture inconsistencies in object surfaces and depth localization. This can lead to misleading evaluations, as models may artificially inflate scores by predicting thick or duplicated surfaces \citep{liu2024sparseocc} rather than accurately reconstructing scene geometry.}
{To address this, we use RayIoU \cite{liu2024sparseocc}, a metric designed to assess 3D occupancy predictions in a depth-aware manner. Instead of evaluating individual voxels, RayIoU casts rays through the predicted 3D volume and determines correctness based on the first occupied voxel the ray intersects. A prediction is considered correct if both the class label and depth fall within a given tolerance. This approach mitigates issues with voxel-level IoU, ensuring that models are rewarded for precise surface localization rather than over-segmentation.}


Using \method{} consistently improves RayIoU across tested architectures, as reported in \autoref{tab:rayiou_metrics}, highlighting its ability to enhance spatial consistency.
{Notably, models enhanced with \method{} achieve state-of-the-art performances with a single frame, outperforming prior works by a significant margin.} %


\paragraph{BeV metrics.}

Additionally, we evaluate some Bird’s-Eye-View (BeV) metrics --- critical for {downstream} motion forecasting and planning --- measuring spatial accuracy {on the horizontal plane} using IoU$^\text{BeV}$ and mIoU$^\text{BeV}$ that capture different aspects of spatial understanding.

{
For this study, we compute the orthographic BeV image for both the prediction and the ground truth, following the rendering procedure of \autoref{sec:methods:gaussian_rendering} for ground truth.
The class assigned to the pixel $p$ is the one corresponding to the maximal value of $C_p$.
Then, we compute IoU (binary occupancy, full vs empty) and mIoU (semantic occupancy) by comparing the two images.
}


Our analysis is presented in \autoref{tab:bev_metrics}.
We observe that the use of \method{} enhances {both} metrics simultaneously, with systematic gains associated with different combinations of datasets and models. 
Both TPVFormer and SurroundOcc show significant improvements across all datasets and evaluations.
This evaluation highlights that the use of \method{} not only improves 3D occupancy predictions but also enhances consistency with BeV and 2D sensor observations.

\subsection{Ablation studies}

{We conduct our ablations} on {fixed} subsets of the datasets{, each representing 20\% of the training and validation sets.} 

\label{sec:exp:ablations}



\input{tables/camera_voxels_strategy}

\subsubsection{Impact of supervising with virtual viewpoints}

A key advantage of \method{} is that {3D occupancy can be rendered} from arbitrary viewpoints. To explore the impact of different virtual camera placements, we evaluate several positioning strategies described below:

\begin{itemize}
    \item \textbf{Sensor Strategy:} Cameras are placed at the original positions and orientations from the dataset.
    \item \textbf{Random Strategy:} Cameras are randomly positioned within the scene by applying a pitch and yaw variation of $\pm$10° and a forward-backward shift from  the half of the maximum range.
    \item \textbf{Elevated Strategy:} Cameras are lifted by 2 meters while tilted 20° downward. This provides a wider field of view and reduces voxel occlusion.
    \item {\textbf{Elevated + Random Strategy:} Cameras are elevated by 2 meters, {tilted 20° downward,} and randomly moved in the scene around the vehicle to half of the maximum range. This allows capturing voxels that are not visible from static sensor positions.}
    \item \textbf{Stereo Strategy:} We select one sensor camera and position another one close to the sensor to mimic a stereo behavior.
\end{itemize}

\autoref{tab:camera_voxel_strategies} quantifies the impact of these strategies on the final 3D predictions. The choice of camera placement should be tailored to the specific task.
If the goal is to predict only visible voxels, as in Occ3D-nuScenes \cite{tian2023occ3d}, using sensor-aligned viewpoints yields the best results. However, if the objective is to also infer occluded regions, as in SurroundOcc-nuScenes \cite{wei2023surroundocc}, using sensor-aligned viewpoints leads to low results but placing virtual cameras to maximize the visibility of these areas provides better supervision. The random strategy further highlights the importance of careful camera positioning: placing cameras arbitrarily often leads to poor 3D metrics, as they mainly observe empty space.









\subsubsection{Loss increment}
Finally, we analyze the contribution of each individual loss component to the final metrics by gradually introducing each term. The results, in \autoref{tab:loss_components_study}, show that each successive addition of a loss component leads to a gradual improvement in performance, justifying the inclusion of each term.



\input{tables/loss_increment}


