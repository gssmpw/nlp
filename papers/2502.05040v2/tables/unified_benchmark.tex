        
        



        

        

        




\begin{table}
    \centering
    \small
    \resizebox{\columnwidth}{!}
    {
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{@{}l c c c c c@{}}
        \toprule
              & \multicolumn{2}{c}{\textbf{Surround-}}  & \textbf{Occ3D} & \multicolumn{2}{c}{\textbf{SSCBench}}\\
              & \multicolumn{2}{c}{\textbf{Occ nusc.} \citep{wei2023surroundocc}}  & \textbf{nusc} \citep{tian2023occ3d} & \multicolumn{2}{c}{\textbf{KITTI360} \citep{li2024sscbench}}\\
              \textit{Evaluation split} & \multicolumn{2}{c}{\textit{val.}}  & \textit{val.} & \multicolumn{2}{c}{\textit{test}}\\
        Visible voxels only & \multicolumn{2}{c}{\ding{55}} & \ding{51} & \multicolumn{2}{c}{\ding{55}} \\
              \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6}
        Model &  IoU &  mIoU & mIoU & IoU &  mIoU\\
        \midrule
        BEVDet \citep{huang2021bevdet} & - & - & 19.38 & - & - \\
        BEVStereo \citep{li2023bevstereo} & - & - & 24.51 & - & - \\
        RenderOcc \citep{pan2024renderocc} & - & - & 26.11 & - & - \\
        CTF-Occ \citep{tian2023occ3d} & - & - & 28.53 & - & - \\
        GSRender \citep{sun2024gsrender} & - & - & 29.56 & - & - \\
        TPVFormer-lidar \citep{huang2023tpv} & 11.51 & 11.66 & -  & - & - \\
        MonoScene \cite{cao2022monoscene} & 23.96 & 7.31 & 6.06 & 37.87 & 12.31 \\
        Atlas \cite{murez2020atlas} & 28.66 & 15.00 & -  & - & - \\
        GaussianFormer \citep{huang2024gaussian} & 29.83 & {19.10} &  - & - & - \\
        BEVFormer \cite{li2022bevformer} & 30.50 & 16.75 & 26.88 & - & - \\
        GaussianFormerv2 \citep{huang2024gaussianformer2} & 30.56 & 20.02 & - & - & - \\
        VoxFormer \citep{li2023voxformer} & - & - & - & 38.76 & 11.91 \\
        OccFormer \citep{zhang2023occformer} & {31.39} & {19.03} & 21.93 & 40.27 & 13.81\\
        \midrule
        TPVFormer \citep{huang2023tpv} & {30.86} & 17.10 & 27.83 & - & - \\

        \rowcolor{Apricot!20!}
        \quad w/ \method{} & 32.05 & \textbf{20.85} & \textbf{30.48} &  - & - \\

        & \textcolor{ForestGreen}{+1.19} & \textcolor{ForestGreen}{+3.75} & \textcolor{ForestGreen}{+2.65} & - & - \\

        \midrule

        SurroundOcc \cite{wei2023surroundocc} & 31.49 & 20.30 & 29.21 & 38.51 & 13.08 \\

        \rowcolor{Apricot!20!}
        \quad w/ \method{} & \textbf{32.61} &  20.82 &  30.38 &  38.62 & 13.34 \\
        & \textcolor{ForestGreen}{+1.12} & \textcolor{ForestGreen}{+0.52} & \textcolor{ForestGreen}{+1.17} &  \textcolor{ForestGreen}{+0.11} & \textcolor{ForestGreen}{+0.26} \\

        \midrule
        Symphonies \citep{jiang2024symphonies} & - & - & - & 43.40 & 17.82 \\
        \rowcolor{Apricot!20!}
        \quad w/ \method{} & - & - & - & \textbf{44.08} & \textbf{18.11} \\
         & - & - & - & \textcolor{ForestGreen}{+0.68} & \textcolor{ForestGreen}{+0.29} \\
        \bottomrule
    \end{tabular}}
    \caption{\textbf{Performance Comparison on Multiple 3D Occupancy Benchmarks.} We report IoU ($\uparrow$) and mIoU ($\uparrow$) metrics on \textbf{SurroundOcc-nuScenes} \citep{wei2023surroundocc}, \textbf{Occ3D-nuScenes} \citep{tian2023occ3d}, and \textbf{SSCBench-KITTI360} \citep{li2024sscbench}. The best results are highlighted in bold. Our module, \method{}, consistently improves performance when integrated with standard models, achieving state-of-the-art results across all benchmarks. Performance gains introduced by \method{} are shown in green.}
    \label{tab:unified_benchmark}
\end{table}



        

        
        


