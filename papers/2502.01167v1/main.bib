@article{ManipLearnRev,
author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
title = {A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {30},
numpages = {82},
keywords = {MDPs, robots, learning, manipulation, review}
}

@article{LearningPlanning,
author = {Arora Ankuj et al.},
year = {2018},
month = {11},
pages = {},
title = {A Review of Learning Planning Action Models},
volume = {33},
journal = {The Knowledge Engineering Review},
doi = {10.1017/S0269888918000188}
}

@article{TrendsInTAMP,
author = {Guo Huihui et al.},
title = {Recent Trends in Task and Motion Planning for Robotics: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583136},
doi = {10.1145/3583136},
abstract = {Autonomous robots are increasingly served in real-world unstructured human environments with complex long-horizon tasks, such as restaurant serving and office delivery. Task and motion planning (TAMP) is a recent research method in Artificial Intelligence Planning for these applications. TAMP integrates high-level abstract reasoning with the low-level geometric feasibility check and thus is more comprehensive than traditional task planning methods. While regular TAMP approaches are challenged by different types of uncertainties and the generalization of various applications when implemented in real-world scenarios. This article systematically reviews the most relevant approaches to TAMP and classifies them according to their features and emphasis; it categorizes the challenges and presents online TAMP and machine learning-based TAMP approaches for addressing them.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {289},
numpages = {36},
keywords = {Task and motion planning, learning for planning, online planning}
}

@inproceedings{TPVQA,
title = {Grounding Classical Task Planners via Vision-Language Models},
author = {Zhang, Xiaohan and Ding, Yan and Amiri, Saeid and Yang, Hao and Kaminski, Andy and Esselink, Chad and Zhang, Shiqi},
year = {2023},
month = {04},
pages = {},
booktitle = {arXiv preprint arXiv:2304.08587}
}

@article{PDDL,
author = {Ghallab Malik et al.},
year = {1998},
month = {08},
pages = {},
title = {PDDL - The Planning Domain Definition Language},
journal = "Techincal Report"
}

@article{SAS1,
  title={Planning in polynomial time: the SAS‐PUBS class},
  author={Christer B{\"a}ckstr{\"o}m and Inger Klein},
  journal={Computational Intelligence},
  year={1991},
  volume={7},
  url={https://api.semanticscholar.org/CorpusID:6578652}
}

@article{CLASSICAL1,
author = {Kaelbling, Leslie and Pasula, Hanna and Zettlemoyer, Luke},
year = {2011},
month = {10},
pages = {},
title = {Learning Symbolic Models of Stochastic Domains},
volume = {29},
journal = {Journal of Artificial Intelligence Research},
doi = {10.1613/jair.2113}
}

@inproceedings{CLASSICAL2,
author="Rodrigues Christophe et al.",editor="Muggleton, Stephen H.
and Tamaddoni-Nezhad, Alireza
and Lisi, Francesca A.",
title="Active Learning of Relational Action Models",
booktitle="Inductive Logic Programming",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="302--316",
abstract="We consider an agent which learns a relational action model in order to be able to predict the effects of his actions. The model consists of a set of STRIPS-like rules, i.e. rules predicting what has changed in the current state when applying a given action as far as a set of preconditions is satisfied by the current state. Here several rules can be associated to a given action, therefore allowing to model conditional effects. Learning is online, as examples result from actions performed by the agent, and incremental, as the current action model is revised each time it is contradicted by unexpected effects resulting from his actions. The form of the model allows using it as an input of standard planners.",
isbn="978-3-642-31951-8"
}

@article{CLASSICAL3,
author = {Cresswell, Stephen and Mccluskey, T. and West, Margaret},
year = {2013},
month = {06},
pages = {},
title = {Acquiring planning domain models using LOCM},
volume = {28},
journal = {The Knowledge Engineering Review},
doi = {10.1017/S0269888912000422}
}

@inproceedings{CLASSICAL4,
  title={Learning STRIPS Action Models with Classical Planning},
  author={Diego Aineto and Sergio Jim{\'e}nez and Eva Onaind{\'i}a},
  booktitle={International Conference on Automated Planning and Scheduling},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49405691}
}

@article{CLASSICAL5,
  title={Learning Symbolic Operators for Task and Motion Planning},
  author={Tom Silver et al.},
  journal={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2021},
  pages={3182-3189},
  url={https://api.semanticscholar.org/CorpusID:232076417}
}

@article{Groundnig,
    title={Grounding Predicates through Actions},
    author={Toki Migimatsu and Jeannette Bohg},
    journal={IEEE International Conference on Robotics and Automation (ICRA)},
    year={2022},
}

@article{VLMsurvey,
author={Chen, Fei-Long et al.},
title={VLP: A Survey on Vision-language Pre-training},
journal={Machine Intelligence Research},
year={2023},
month={Feb},
day={01},
volume={20},
number={1},
pages={38-56},
abstract={In the past few years, the emergence of pre-training models has brought uni-modal fields such as computer vision (CV) and natural language processing (NLP) to a new era. Substantial works have shown that they are beneficial for downstream uni-modal tasks and avoid training a new model from scratch. So can such pre-trained models be applied to multi-modal tasks? Researchers have explored this problem and made significant progress. This paper surveys recent advances and new frontiers in vision-language pre-training (VLP), including image-text and video-text pre-training. To give readers a better overall grasp of VLP, we first review its recent advances in five aspects: feature extraction, model architecture, pre-training objectives, pre-training datasets, and downstream tasks. Then, we summarize the specific VLP models in detail. Finally, we discuss the new frontiers in VLP. To the best of our knowledge, this is the first survey focused on VLP. We hope that this survey can shed light on future research in the VLP field.},
issn={2731-5398},
doi={10.1007/s11633-022-1369-5},
url={https://doi.org/10.1007/s11633-022-1369-5}
}


@InProceedings{InnerMonologe,
  title = 	 {Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author =       {{Huang et al.}, Wenlong},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {1769--1782},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/huang23c/huang23c.pdf},
  url = 	 {https://proceedings.mlr.press/v205/huang23c.html},
  abstract = 	 {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.}
}

@article{SuccessVQA,
  title={Vision-Language Models as Success Detectors},
  author={Yuqing Du et al.},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.07280},
  url={https://api.semanticscholar.org/CorpusID:257496810}
}

@inproceedings{RT2,
    title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
    author={{A. Brohan et al.}},
    booktitle={arXiv preprint arXiv:2307.15818},
    year={2023}
}

@inproceedings{PALME,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={{Driess et al.}, Danny},
    booktitle={arXiv preprint arXiv:2303.03378},
    year={2023}
}

@inproceedings{Lu2019ViLBERTPT,
  title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
  author={Jiasen Lu et al.},
  booktitle={Neural Information Processing Systems},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:199453025}
}

@InProceedings{Doveh_2023_CVPR,
    author    = {Doveh, Sivan et al.},
    title     = {Teaching Structured Vision \& Language Concepts to Vision \& Language Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {2657-2668}
}


@inproceedings{ADAMW,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:53592270}
}

@article{RT1,
  title={RT-1: Robotics transformer for real-world control at scale},
  author={{Brohan et al.}},
  journal={Robotics: Science and Systems XIX},
  year={2023},
  url={https://doi.org/10.15607/rss.2023.xix.025}
}


@inproceedings{BridgeData,
  title={BridgeData V2: A Dataset for Robot Learning at Scale},
  author={{Walke et al.}, Homer},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2023}
}

@article{RoboSet,
  author  = {Bharadhwaj, Homanga and Vakil, Jay and Sharma, Mohit and Gupta, Abhinav and Tulsiani, Shubham and Kumar, Vikash},
  title   = {RoboAgent: Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking},
  journal = {arxiv},
  year    = {2023}}

@inproceedings{AllenNLP,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  et al.",
    editor = "Park, Eunjeong L.  and
      Hagiwara, Masato  and
      Milajevs, Dmitrijs  and
      Tan, Liling",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2501",
    doi = "10.18653/v1/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

@inproceedings{
ViT,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={{Alexey Dosovitskiy et al.}},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@misc{Dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={{Oquab et al.}, Maxime},
  journal={arXiv:2304.07193},
  year={2023}
}


@InProceedings{CLIP,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {{Radford et al.}, Alec},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@article{flamingo,
  title={OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models},
  author={{Anas Awadalla et al.}},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@inproceedings{Alayrac2022FlamingoAV,
 author = {{Alayrac et al.}, Jean-Baptiste},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo et al.},
 pages = {23716--23736},
 publisher = {Curran Associates, Inc.},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 volume = {35},
 year = {2022}
}

@article{OON,
    title={{Long-Horizon Planning and Execution with Functional Object-Oriented Networks}},
    author={Paulius, David and Agostini, Alejandro and Lee, Dongheui},    
    journal={IEEE Robotics and Automation Letters}, 
    year={2023},
    volume={8},
    number={8},
    pages={4513-4520},
    doi={10.1109/LRA.2023.3285510}
}


@article{CONT1,
title = {Receding Horizon Task and Motion Planning in Changing Environments},
journal = {Robotics and Autonomous Systems},
volume = {145},
pages = {103863},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103863},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021001482},
author = {Nicola Castaman and Enrico Pagello and Emanuele Menegatti and Alberto Pretto},
keywords = {Task and Motion Planning, Robot manipulation, Non-static Environments},
abstract = {Complex manipulation tasks require careful integration of symbolic reasoning and motion planning. This problem, commonly referred to as Task and Motion Planning (TAMP), is even more challenging if the workspace is non-static, e.g. due to human interventions and perceived with noisy non-ideal sensors. This work proposes an online approximated TAMP method that combines a geometric reasoning module and a motion planner with a standard task planner in a receding horizon fashion. Our approach iteratively solves a reduced planning problem over a receding window of a limited number of future actions during the implementation of the actions. Thus, only the first action of the horizon is actually scheduled at each iteration, then the window is moved forward, and the problem is solved again. This procedure allows to naturally take into account potential changes in the scene while ensuring good runtime performance. We validate our approach within extensive experiments in a simulated environment. We showed that our approach is able to deal with unexpected changes in the environment while ensuring comparable performance with respect to other recent TAMP approaches in solving traditional static benchmarks. We release with this paper the open-source implementation of our method.}
}

@article{CONT2,
  title={Representing Robot Task Plans as Robust Logical-Dynamical Systems},
  author={Chris Paxton and Nathan D. Ratliff and Clemens Eppner and Dieter Fox},
  journal={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2019},
  pages={5588-5595},
  url={https://api.semanticscholar.org/CorpusID:199453145}
}

@article{CONT3,
author = {Pezzato, Corrado et al.},
title = {Active Inference and Behavior Trees for Reactive Action Planning and Execution in Robotics},
year = {2023},
issue_date = {April 2023},
publisher = {IEEE Press},
volume = {39},
number = {2},
issn = {1552-3098},
url = {https://doi.org/10.1109/TRO.2022.3226144},
doi = {10.1109/TRO.2022.3226144},
abstract = {In this article, we propose a hybrid combination of active inference and behavior trees (BTs) for reactive action planning and execution in dynamic environments, showing how robotic tasks can be formulated as a free-energy minimization problem. The proposed approach allows handling partially observable initial states and improves the robustness of classical BTs against unexpected contingencies while at the same time reducing the number of nodes in a tree. In this work, we specify the nominal behavior offline, through BTs. However, in contrast to previous approaches, we introduce a new type of leaf node to specify the desired state to be achieved rather than an action to execute. The decision of which action to execute to reach the desired state is performed online through active inference. This results in continual online planning and hierarchical deliberation. By doing so, an agent can follow a predefined offline plan while still keeping the ability to locally adapt and take autonomous decisions at runtime, respecting safety constraints. We provide proof of convergence and robustness analysis, and we validate our method in two different mobile manipulators performing similar tasks, both in a simulated and real retail environment. The results showed improved runtime adaptability with a fraction of the hand-coded nodes compared to classical BTs.},
journal = {Trans. Rob.},
month = {apr},
pages = {1050–1069},
numpages = {20}
}

@inproceedings{ANOMALY1,
  title={Multi-level task learning based on intention and constraint inference for autonomous robotic manipulation},
  author={Willibald, Christoph and Lee, Dongheui},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={7688--7695},
  year={2022},
  organization={IEEE}
}

@ARTICLE{Eiband2019,
  author={Eiband, Thomas and Saveriano, Matteo and Lee, Dongheui},
  journal={IEEE Robotics and Automation Letters}, 
  title={Intuitive Programming of Conditional Tasks by Demonstration of Multiple Solutions}, 
  year={2019},
  volume={4},
  number={4},
  pages={4483-4490},
  keywords={Task analysis;Robot sensing systems;Monitoring;Force;Time series analysis;Switches;Learning from demonstration;failure detection and recovery;learning and adaptive systems},
  doi={10.1109/LRA.2019.2935381}}

@inproceedings{ANOMALY3,
  title={Data-Driven Online Decision Making for Autonomous Manipulation},
  author={Daniel Kappler et al.},
  booktitle={Robotics: Science and Systems},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:14713141}
}

@article{STRIPS,
  title={STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving},
  author={Richard Fikes and Nils J. Nilsson},
  journal={Artif. Intell.},
  year={1971},
  volume={2},
  pages={189-208},
  url={https://api.semanticscholar.org/CorpusID:8623866}
}

@misc{instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai et al.},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{TAMPSurvey,
author = {Guo Huihui et al.},
title = {Recent Trends in Task and Motion Planning for Robotics: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583136},
doi = {10.1145/3583136},
abstract = {Autonomous robots are increasingly served in real-world unstructured human environments with complex long-horizon tasks, such as restaurant serving and office delivery. Task and motion planning (TAMP) is a recent research method in Artificial Intelligence Planning for these applications. TAMP integrates high-level abstract reasoning with the low-level geometric feasibility check and thus is more comprehensive than traditional task planning methods. While regular TAMP approaches are challenged by different types of uncertainties and the generalization of various applications when implemented in real-world scenarios. This article systematically reviews the most relevant approaches to TAMP and classifies them according to their features and emphasis; it categorizes the challenges and presents online TAMP and machine learning-based TAMP approaches for addressing them.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {289},
numpages = {36},
keywords = {Task and motion planning, online planning, learning for planning}
}

@article{BT,
title = {A survey of Behavior Trees in robotics and AI},
journal = {Robotics and Autonomous Systems},
volume = {154},
pages = {104096},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104096},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000513},
author = {Matteo Iovino et al.},
keywords = {Behavior Trees, Robotics, Artificial Intelligence, Learning Behavior Trees},
abstract = {Behavior Trees (BTs) were invented as a tool to enable modular AI in computer games, but have received an increasing amount of attention in the robotics community in the last decade. With rising demands on agent AI complexity, game programmers found that the Finite State Machines (FSM) that they used scaled poorly and were difficult to extend, adapt and reuse. In BTs, the state transition logic is not dispersed across the individual states, but organized in a hierarchical tree structure, with the states as leaves. This has a significant effect on modularity, which in turn simplifies both synthesis and analysis by humans and algorithms alike. These advantages are needed not only in game AI design, but also in robotics, as is evident from the research being done. In this paper we present a comprehensive survey of the topic of BTs in Artificial Intelligence and Robotic applications. The existing literature is described and categorized based on methods, application areas and contributions, and the paper is concluded with a list of open research challenges.}
}

@article{DomainGAP,
title = {Deep visual domain adaptation: A survey},
journal = {Neurocomputing},
volume = {312},
pages = {135-153},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.05.083},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218306684},
author = {Mei Wang and Weihong Deng},
keywords = {Deep domain adaptation, Deep networks, Transfer learning, Computer vision applications},
abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.}
}

@misc{RTX,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {Open X-Embodiment Collaboration},
howpublished  = {\url{https://arxiv.org/abs/2310.08864}},
year = {2023},
}

@InProceedings{SayCan,
  title = 	 {Do As I Can, Not As I Say: Grounding Language in Robotic Affordances},
  author =       {Ichter Brian et al.},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {287--318},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/ichter23a/ichter23a.pdf},
  url = 	 {https://proceedings.mlr.press/v205/ichter23a.html},
  abstract = 	 {Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so  that  the  language model  provides  high-level  knowledge about the procedures for performing complex and temporally extended instructions,  while  value  functions  associated  with  these  skills  provide  the  grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project’s website, video, and open source can be found at say-can.github.io.}
}

@InProceedings{SayPlan,
  title = 	 {SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning},
  author =       {Rana Krishan et al.},
  booktitle = 	 {Proceedings of The 7th Conference on Robot Learning},
  pages = 	 {23--72},
  year = 	 {2023},
  editor = 	 {Tan, Jie and Toussaint, Marc and Darvish, Kourosh},
  volume = 	 {229},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v229/rana23a/rana23a.pdf},
  url = 	 {https://proceedings.mlr.press/v229/rana23a.html},
  abstract = 	 {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a "semantic search" for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an "iterative replanning" pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors and 36 rooms with 140 assets and objects and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute. We provide real robot video demonstrations on our project page https://sayplan.github.io.}
}

@article{VQA20,
  title={VQA: Visual Question Answering},
  author={Aishwarya Agrawal and Jiasen Lu and Stanislaw Antol and Margaret Mitchell and C. Lawrence Zitnick and Devi Parikh and Dhruv Batra},
  journal={International Journal of Computer Vision},
  year={2015},
  volume={123},
  pages={4 - 31},
  url={https://api.semanticscholar.org/CorpusID:3180429}
}

@INPROCEEDINGS{FinoNet,
  author={Inceoglu, Arda and Aksoy, Eren Erdal and Cihan Ak, Abdullah and Sariel, Sanem},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={FINO-Net: A Deep Multimodal Sensor Fusion Framework for Manipulation Failure Detection}, 
  year={2021},
  volume={},
  number={},
  pages={6841-6847},
  keywords={Deep learning;Codes;Multimodal sensors;Robot sensing systems;Safety;Monitoring;Intelligent robots},
  doi={10.1109/IROS51168.2021.9636455}}

@inproceedings{willibald2020collaborative,
  title={Collaborative programming of conditional robot tasks},
  author={Willibald, Christoph and Eiband, Thomas and Lee, Dongheui},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5402--5409},
  year={2020},
  organization={IEEE}
}

@article{eiband2023collaborative,
  title={Collaborative programming of robotic task decisions and recovery behaviors},
  author={Eiband, Thomas and Willibald, Christoph and Tannert, Isabel and Weber, Bernhard and Lee, Dongheui},
  journal={Autonomous Robots},
  volume={47},
  number={2},
  pages={229--247},
  year={2023},
  publisher={Springer}
}

@inproceedings{Chernova2007,
  title={Confidence-based policy learning from demonstration using gaussian mixture models},
  author={Chernova, Sonia and Veloso, Manuela},
  booktitle={Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems},
  pages={1--8},
  year={2007}
}

@InProceedings{Maeda2017,
  author    = {Maeda, Guilherme and Ewerton, Marco and Osa, Takayuki and Busch, Baptiste and Peters, Jan},
  title     = {Active incremental learning of robot movement primitives},
  booktitle = { Conference on Robot Learning (CoRL)},
  year      = {2017},
  pages     = {37-46},
}

@inproceedings{romeres2019anomaly,
  title={Anomaly detection for insertion tasks in robotic assembly using Gaussian process models},
  author={Romeres, Diego and Jha, Devesh K and Yerazunis, William and Nikovski, Daniel and Dau, Hoang Anh},
  booktitle={2019 18th European Control Conference (ECC)},
  pages={1017--1022},
  year={2019},
  organization={IEEE}
}

@inproceedings{azzalini2020hmms,
  title={HMMs for anomaly detection in autonomous robots},
  author={Azzalini, Davide and Castellini, Alberto and Luperto, Matteo and Farinelli, Alessandro and Amigoni, Francesco},
  booktitle={Int. Conf. on Autonomous Agents and MultiAgent Systems},
  pages={105--113},
  year={2020}
}

@article{park2019multimodal,
  title={Multimodal anomaly detection for assistive robots},
  author={Park, Daehyung and Kim, Hokeun and Kemp, Charles C},
  journal={Autonomous Robots},
  volume={43(3)},
  pages={611--629},
  year={2019},
  publisher={Springer}
}

@INPROCEEDINGS{AnomalyDetSlip,
  author={Yoo, Youngjae and Lee, Chung-Yeon and Zhang, Byoung-Tak},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip Perception of Mobile Manipulation Robots}, 
  year={2021},
  volume={},
  number={},
  pages={11443-11449},
  keywords={Visualization;Service robots;Vision sensors;Robot sensing systems;Reliability;Mobile robots;Synchronization},
  doi={10.1109/ICRA48506.2021.9561586}}

@ARTICLE{FaluireClassification1,
  author={Inceoglu, Arda and Aksoy, Eren Erdal and Sariel, Sanem},
  journal={IEEE Robotics and Automation Letters}, 
  title={Multimodal Detection and Classification of Robot Manipulation Failures}, 
  year={2024},
  volume={9},
  number={2},
  pages={1396-1403},
  keywords={Robot sensing systems;Robots;Task analysis;Monitoring;Hidden Markov models;Collision avoidance;Real-time systems;Deep learning methods;data sets for robot learning;failure detection and recovery;sensor fusion},
  doi={10.1109/LRA.2023.3346270}}

@article{Altan2022CLUEAIAC,
  title={CLUE-AI: A Convolutional Three-Stream Anomaly Identification Framework for Robot Manipulation},
  author={Dogan Altan and Sanem Sariel},
  journal={IEEE Access},
  year={2022},
  volume={11},
  pages={48347-48357},
  url={https://api.semanticscholar.org/CorpusID:247476170}
}

@article{anomalydetroboticssurv,
author = {Hsu, Kai-Chieh and Hu, Haimin and Fisac, Jaime},
year = {2024},
month = {02},
pages = {},
title = {The Safety Filter: A Unified View of Safety-Critical Control in Autonomous Systems},
volume = {7},
journal = {Annual Review of Control, Robotics, and Autonomous Systems},
doi = {10.1146/annurev-control-071723-102940}
}

@inproceedings{liu2023modelbased,
      title={Model-Based Runtime Monitoring with Interactive Imitation Learning},
      author={Huihan Liu and Shivin Dass and Roberto Martín-Martín and Yuke Zhu},
      booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
      year={2024}
    }


@INPROCEEDINGS{Pato, 
    AUTHOR    = {Shivin Dass AND Karl Pertsch AND Hejia Zhang AND Youngwoon Lee AND Joseph J Lim AND Stefanos Nikolaidis}, 
    TITLE     = {{PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2023}, 
    ADDRESS   = {Daegu, Republic of Korea}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2023.XIX.013} 
} 

@INPROCEEDINGS{Asking,
  author={Gokmen, Cem and Ho, Daniel and Khansari, Mohi},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Asking for Help: Failure Prediction in Behavioral Cloning through Value Approximation}, 
  year={2023},
  volume={},
  number={},
  pages={5821-5828},
  keywords={Representation learning;Costs;Automation;Cloning;Estimation;Closed box;Predictive models;robotics;imitation learning;failure detection;policy evaluation},
  doi={10.1109/ICRA48891.2023.10161004}}

@inproceedings{
liu2023reflect,
title={{REFLECT}: Summarizing Robot Experiences for Failure Explanation and Correction},
author={Zeyi Liu and Arpit Bahety and Shuran Song},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=8yTS_nAILxt}
}

@MISC{chatgpt,
  title        = "{ChatGPT (May 16 version )}",
  abstract     = "ChatGPT helps you get answers, find inspiration and be more
                  productive. It is free to use and easy to try. Just ask and
                  ChatGPT can help with writing, learning, brainstorming and
                  more.",
  author={OpenAI},
  howpublished = "\url{https://chat.openai.com/chat}",
}

@article{ruan2011regularized,
  title={Regularized parameter estimation in high-dimensional Gaussian mixture models},
  author={Ruan, Lingyan and Yuan, Ming and Zou, Hui},
  journal={Neural computation},
  volume={23},
  number={6},
  pages={1605--1622},
  year={2011},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}