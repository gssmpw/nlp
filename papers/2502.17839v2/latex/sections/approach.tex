\section{Approach: Combining Step-Back Reasoning With Pragmatic Retrieval}
\label{sec:approach}
Conceptually, our approach is a simple plug-and-play extension that emphasizes important information in any standard RAG setup (as shown in Figure \ref{fig:SayLessRAG-diagram}).
In this paper, we apply our extension to a collection of documents  retrieved by a dense passage retriever (DPR) \cite{izacard2021contriever}.\footnote{We use the same KB collection of documents as Self-RAG \cite{asai2024selfrag} and CRAG \cite{yan2024corrective}.} 
We adapt the unsupervised iterative sentence retriever proposed by \newcite{yadav2020unsupervisedalignmentbasediterativeevidence} to identify important sentences in the documents retrieved by RAG with DPR, as follows:
\textbf{(1)} Given a query and associated passages retrieved by DPR, the query is first conjoined with a more abstract \textit{step-back} version of itself created by a \textit{step-back LLM}~\cite{zheng2024stepbackevokingreasoning}. \textbf{(2)} In the first sentence retrieval iteration, this conjoined query is used to retrieve a set of relevant evidence sentences from the corresponding passages (see Eqs. 1 and 2). \textbf{(3)} In the next iteration(s), the query is reformulated to focus on \textit{missing information}, i.e., query keywords not covered by the current set of retrieved evidence sentences (see Eq. 3) and the process repeats until all question phrases are covered. As such, this strategy implements Grice's maxims of relation (because the evidence sentences are relevant to the question), quantity, and manner (because we identify as many sentences as needed to cover the question and no more).

By aggregating sets of retrieved evidence sentences across iterations, this retrieval strategy allows constructing \textit{chains} of evidence sentences for a given query, which can extend dynamically until a parameter-free termination criteria is reached. Further, by varying the first evidence sentence in the top $N$\footnote{In our experiments, we set $N=3$.} retrieved evidences, we can trivially extend this retriever to extract \textit{parallel evidence chains}, each of varying lengths, to create a more diverse set of evidence sentences that support the query. 

Lastly, we condition the generation of the Question Answering (QA) LLMs on the retrieved evidences, highlighted with special \textit{evidence tokens}, embedded in their original DPR contexts, in order (see Table~\ref{tab:example} for an example). 
We describe each of these stages in more detail below.

\subsection{Step-Back Query Expansion}
\label{sec:step-back}
In this work, we employ \textit{Step-Back Prompting} \cite{zheng2024stepbackevokingreasoning}, a simple technique to integrate LLM driven reasoning into the retrieval process. A step-back prompt elicits from the LLM an abstract, higher-level question derived from the original query, encouraging higher-level reasoning about the problem. For example, a step-back version of the query: ``As bank president, Alex Sink eliminated thousands of Florida jobs while taking over \$8 million in salary and bonuses. True or False?'' could be: ``What were the actions taken by Alex Sink as bank president?''. We hypothesize that step-back queries, representing a more generalized query formulation, when utilized as initialization seeds for the iterative retrieval (refer to Figure \ref{fig:SayLessRAG-diagram}), will generate a more diverse yet still relevant set of candidate evidence sentences. For multiple-choice questions (MCQs), we generate step-back answer choices for each option, combining them with the step-back query to guide retrieval. This approach introduces an additional dimension of parallelism in constructing evidence chains for MCQs. The step-back prompts used to elicit multi-hop reasoning follow the Knowledge QA template from \newcite{zheng2024stepbackevokingreasoning} (refer to appendix \ref{sec:appendix3} for prompts and Table \ref{tab:stepback-examples} for examples of step-back questions).
\subsection{Parallel Iterative Evidence Retrieval}
Computing an alignment score between queries and documents is a critical step in any retrieval system. Keeping in mind the Gricean maxim's of \textit{quality} and \textit{relation} (Section \ref{sec:intro}), which emphasize relevance and factual grounding, we leverage a principle similar to ``late interaction''
\citep{khattab2020colbertefficienteffectivepassage,santhanam2022colbertv2effectiveefficientretrieval}, where evidences are selected based on token-level similarities between queries and KB passages. We align query tokens with tokens from each sentence in the KB passages to construct evidence sentences, by selecting the most maximally similar token from the KB passage based on cosine similarity scores over dense embeddings\footnote{While \newcite{yadav2020unsupervisedalignmentbasediterativeevidence} align tokens based on similarity over GloVe embeddings, we use sentence transformer embeddings: \url{https://huggingface.co/jinaai/jina-embeddings-v2-base-en}} (Equation~\ref{eq:alignment_score}). 
\begin{equation}
\label{eq:alignment_score}
s(Q, P_j) = \sum_{i=1}^{|Q|} align(q_i, P_j)
\end{equation}

\begin{equation}
\label{eq:align_function}
align(q_i, P_j) = \max_{k=1}^{|P_j|} cosSim(q_i, p_k)
\end{equation}
where $q_i$ and $p_k$ are the $i^{th}$ and $k^{th}$ terms of the query $(Q)$ and evidence sentence $(P_j)$ respectively.

Query reformulation is driven by remainder terms, defined as the set of query terms which have not yet been covered by the set of evidence sentences which were retrieved in the first $i$ iterations of the multi-hop retriever (Equation~\ref{eq:remainder_terms}): 
\begin{equation}
\label{eq:remainder_terms}
Q_r(i) = t(Q) - \bigcup_{s_k \in S_i} t(s_k)
\end{equation}
where $t(Q)$ represents the unique set of query terms, $t(s_k)$ represents the unique terms of the $k^{th}$ evidence sentence in set $S_i$, which is the set of evidences retrieved in the $i^{th}$ iteration of the retrieval process.

The notion of coverage here is based on soft matching alignment: a query term is considered to be included in the set of evidence terms if its cosine similarity with a evidence term is greater than $M$\footnote{In this work, we set $M=0.98$.}. Note that the goal of query reformulation is to maximize the coverage of the query keywords by the retrieved chain of evidences, which aligns with the notion of the maxim of \textit{quantity} (Section \ref{sec:intro}).

Ambiguous queries are mitigated by dynamically expanding the current query with terms from all previously retrieved evidence sentences if the number of uncovered terms in the query falls below $T$,\footnote{In this work, we set $T=4$.} which also satisfies the last of Grice's maxims (maxim of \textit{manner}).