% \vspace{-2mm}
\section{Analysis}
\textbf{When Does Pragmatics Help?}
Our error analysis indicates that leveraging pragmatics is effective when answering the query requires connecting facts along a causal path to deduce the answer (as shown in the example of Good Evidence in Table \ref{tab:evidence_quality_examples}, appendix \ref{sec:appendix1}). 

We also observe that highlighted evidence often functions as implicit few-shot exemplars, facilitating analogical reasoning. For instance, given the question ``In the design process, what is an example of a trade-off?'', our method highlights two analogous scenarios: a career decision (``\$50,000 salary worker sacrificing income to pursue medical training with the goal of increasing their future income after becoming a doctor'') and a biological principle (``beneficial trait changes linked to detrimental ones''). We hypothesize that such examples stimulate the model's in-context learning capabilities, possibly explaining the observed 10\% relative improvement in OLMo-1B's performance on ARC-C.

However, our method exhibits a few limitations in specific scenarios (refer to Table \ref{tab:low_quality_evidences}, appendix \ref{sec:appendix2}). First, it fails to highlight relevant evidences for queries which require arithmetic manipulation or comparison of physical quantities, as these tasks depend more on mathematical reasoning than factual knowledge. Second, it struggles with complex linguistic phenomena, particularly negation patterns. For example, consider the question: ``Which human activities would have a positive effect on the natural environment?'' Most retrieved passages focus on negative environmental impacts, reflecting their prevalence in real world corpora. The task here requires identifying contrary evidence from the long tail of the distribution, but our unsupervised retrieval heuristics do not account for such semantic inversions. \\
Lastly, we find that for factoid QA tasks like PopQA, evidence highlighting can slightly degrade performance compared to DPR, likely because these tasks rely more on the model's parametric knowledge. For instance, PopQA queries like ``What is Antonio Álvarez Alonso's occupation?'' often retrieve ambiguous contexts with multiple roles (e.g., Spanish retired footballer, Spanish paracanoeist, Spanish pianist and composer), offering insufficient signals for disambiguation. In such scenarios, our method may either highlight all potential evidences or arbitrarily select one, confusing the model and potentially leading to incorrect answers.

\textbf{Time Complexity of Retrieval}
The computational complexity of our retrieval method can be decomposed into two main components: First, for every query, we make one call to a step-back LLM for query expansion (i.e., creating an abstract step-back version of the query, refer to section \ref{sec:step-back}). Second, for evidence selection and highlighting \cite{yadav2020unsupervisedalignmentbasediterativeevidence}, given $S$ sentences retrieved by DPR, we select a subset of $K$ evidence sentences  from $S$ passage sentences. In each hop of the iterative retriever, one evidence sentence is chosen from $S$. The number of hops is upper bounded by the hyperparameter $K$ (where we set $K \leq 6$). 
Thus the cost of this step is $O(K \times S)$ (constant). Since we allow the retriever to extract $N$ parallel evidence chains by varying the top-scoring evidence (see section \ref{sec:approach}), the total cost of parallel evidence retrieval is $O(N \times K \times S)$ (constant). Evidence highlighting requires a linear scan of the $S$ passage sentences with complexity $O(S)$ (constant). Therefore, the total computational complexity is: $\text{Cost}_{\text{total}} = \text{Cost}({\text{LLM}_{\text{stepback}}}) + O(n)$, where $n$ represents the number of tokens in the retrieved passages $S$. We note two important considerations: (a) the base retrieval cost is inherent to any RAG system and thus unavoidable, and (b) our method introduces minimal computational overhead compared to alternative reasoning-enhanced QA approaches such as STaR \cite{zelikman2022starbootstrappingreasoningreasoning}.

\textbf{Is keeping full DPR context necessary?}
We conduct an experiment to assess the impact of removing versus retaining the surrounding context of highlighted evidence sentences on QA performance. As shown in Table \ref{tab:highlighted_justifications}, across both ARC-C and PubHealth datasets and three different LLMs, we find that providing only the highlighted evidence sentences—without their surrounding context—can significantly degrade QA performance compared to retaining the full context while highlighting the evidence within.

\textbf{How does the quality of the retrieved passages impact our method}? To assess the relationship between initial retrieval quality and our method's effectiveness, we conduct comparative experiments using the sparse retrieval method BM25 \cite{Robertson2009ThePR} in place of DPR. For each query, we retrieve the top-20 passages using BM25, then apply our iterative retrieval approach with step-back reasoning (Section \ref{sec:approach}) to identify and highlight key evidence sentences within these contexts. As shown in Table \ref{tab:bm25_results}, retrieval quality significantly influences our method's performance. We observe substantial improvements across multiple models and datasets: Llama-2-7B achieves a 16.74\% gain on ARC-Challenge, Alpaca-7B shows up to a 45.90\% improvement on PubHealth, while Llama-7B and Qwen2.5-3B demonstrate gains of up to 44.16\% and 9.73\% on PopQA, respectively, relative to their baseline BM25 performance. However, the efficacy of our method when applied to BM25-retrieved passages is inconsistent, with several models also demonstrating performance deterioration compared to both baseline BM25 and the ``\textit{No Retrieval}'' setting. We hypothesize that this is because of two reasons: (a) 
% BM25, which aims to maximize lexical overlap, retrieves passages that contain necessary but insufficient information for answering the query. 
BM25's lexical overlap-based retrieval mechanism yields passages containing necessary but insufficient information for query resolution.
For instance, on ARC-Challenge (refer to Table \ref{tab:bm25_results}), Alpaca-7B improves by 16\% when using BM25-retrieved passages as context, but subsequent evidence highlighting on top of these passages diminishes this gain. (b) Evidence highlighting more effectively grounds the LLM in the retrieved context, potentially overriding useful parametric knowledge. This effect is particularly pronounced with Qwen-2.5 3B on PubHealth, where the model significantly degrades by 51.3\% when provided with BM25 retrieved passages as contexts relative to ``\textit{No Retrieval}'',  and the application of evidence highlighting over these contexts further reduces performance by 1.6\%. This suggests that while evidence highlighting effectively directs model attention in high-quality passages, it creates a bias that may be counterproductive when retrieved passages are of lower quality. \footnote{We do not imply that BM25-retrieved passages are always lower quality than those retrieved by DPR; rather, in this specific case, the DPR \textit{Contriever} has been finetuned on web-domain data \cite{bajaj2018msmarcohumangenerated} similar to our evaluation datasets, making it a more effective retrieval method. We acknowledge that BM25 can be more robust than DPR out-of-domain.}
In such instances, our method may constrain the model to prioritize highlighted information over potentially superior parametric knowledge (which the model may have acquired through test data appearing in its pre-training corpora). These results suggest that our approach is more complementary to DPR and similar neural retrieval methods than to lexical matching approaches like BM25.

\textbf{Evaluating Quality of Highlighted Evidence}
We conduct a human evaluation to assess the quality of evidence highlighting across 40 questions, evenly distributed between the ARC-Challenge and PubHealth datasets (20 questions from each). For each question, we use a three-point rating scale to evaluate the corresponding highlighted evidence: \textbf{0 (bad)}, \textbf{0.5 (medium)} and \textbf{1 (good)}. Overall, 60\% to 70\%  of highlighted evidences were rated at least ``medium'' by the human evaluator across both datasets. See Appendix \ref{sec:appendix1} for the evaluation criteria used and examples of `good', `medium' and `bad' evidence sentences.

\textbf{Understanding the Impact of Top-\textit{k} Retrieval}
We analyze the effect of varying DPR's top-$k$ retrieved contexts on Qwen2.5-3B's performance in each of our three settings: vanilla DPR, DPR with evidence highlighting, DPR with evidence highlighting and step-back reasoning. Our results (figure \ref{fig:top-k}) indicate that larger $k$ values generally improve performance on each dataset. On ARC-C, we see a ``Goldilocks zone'' with step-back reasoning: increasing context ($k>10$) degrades performance. On PopQA, both evidence highlighting methods outperform vanilla DPR for all $k$ values tested. On PubHealth, we observe that evidence highlighting methods can significantly outperform DPR for smaller values of $k$ ($<10$).