@inproceedings{booch2021thinking,
  title={Thinking fast and slow in AI},
  author={Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jonathan and Linck, Nick and Loreggia, Andreas and Murgesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={15042--15046},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{deng2024cognidual,
  title={CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks},
  author={Deng, Yongxin and Qiu, Xihe and Tan, Xiaoyu and Qu, Chao and Pan, Jing and Cheng, Yuan and Xu, Yinghui and Chu, Wei},
  journal={arXiv preprint arXiv:2409.03381},
  year={2024}
}

@inproceedings{echterhoff2024cognitive,
  title={Cognitive bias in decision-making with LLMs},
  author={Echterhoff, Jessica and Liu, Yao and Alessa, Abeer and McAuley, Julian and He, Zexue},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={12640--12653},
  year={2024}
}

@article{furniturewala2024thinking,
  title={Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models},
  author={Furniturewala, Shaz and Jandial, Surgan and Java, Abhinav and Banerjee, Pragyan and Shahid, Simra and Bhatia, Sumit and Jaidka, Kokil},
  journal={arXiv preprint arXiv:2405.10431},
  year={2024}
}

@article{hagendorff2023human,
  title={Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT},
  author={Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
  journal={Nature Computational Science},
  volume={3},
  number={10},
  pages={833--838},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{kamruzzaman2024prompting,
  title={Prompting techniques for reducing social bias in llms through system 1 and system 2 cognitive processes},
  author={Kamruzzaman, Mahammed and Kim, Gene Louis},
  journal={arXiv preprint arXiv:2404.17218},
  year={2024}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{liu2022neural,
  title={A neural-symbolic approach to natural language understanding},
  author={Liu, Zhixuan and Wang, Zihao and Lin, Yuan and Li, Hang},
  journal={arXiv preprint arXiv:2203.10557},
  year={2022}
}

@article{magister2022teaching,
  title={Teaching small language models to reason},
  author={Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
  journal={arXiv preprint arXiv:2212.08410},
  year={2022}
}

@article{mondorf2024beyond,
  title={Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models--A Survey},
  author={Mondorf, Philipp and Plank, Barbara},
  journal={arXiv preprint arXiv:2404.01869},
  year={2024}
}

@article{pan2024dynathink,
  title={DynaThink: Fast or slow? A dynamic decision-making framework for large language models},
  author={Pan, Jiabao and Zhang, Yan and Zhang, Chen and Liu, Zuozhu and Wang, Hongwei and Li, Haizhou},
  journal={arXiv preprint arXiv:2407.01009},
  year={2024}
}

@inproceedings{parmar2024logicbench,
  title={LogicBench: Towards systematic evaluation of logical reasoning ability of large language models},
  author={Parmar, Mihir and Patel, Nisarg and Varshney, Neeraj and Nakamura, Mutsumi and Luo, Man and Mashetty, Santosh and Mitra, Arindam and Baral, Chitta},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13679--13707},
  year={2024}
}

@article{sourati2024arn,
  title={Arn: Analogical reasoning on narratives},
  author={Sourati, Zhivar and Ilievski, Filip and Sommerauer, Pia and Jiang, Yifan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1063--1086},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@inproceedings{valmeekam2022large,
  title={Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{wang2024chain,
  title={Chain-of-thought reasoning without prompting},
  author={Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.10200},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{weston2023system,
  title={System 2 Attention (is something you might need too)},
  author={Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2311.11829},
  year={2023}
}

@article{yang2024llm2,
  title={LLM2: Let Large Language Models Harness System 2 Reasoning},
  author={Yang, Cheng and Shi, Chufan and Li, Siheng and Shui, Bo and Yang, Yujiu and Lam, Wai},
  journal={arXiv preprint arXiv:2412.20372},
  year={2024}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yu2024distilling,
  title={Distilling system 2 into system 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2407.06023},
  year={2024}
}

@inproceedings{zeng2024mr,
  title={Mr-ben: A meta-reasoning benchmark for evaluating system-2 thinking in llms},
  author={Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{zhou2024self,
  title={Self-discover: Large language models self-compose reasoning structures},
  author={Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
  journal={arXiv preprint arXiv:2402.03620},
  year={2024}
}

