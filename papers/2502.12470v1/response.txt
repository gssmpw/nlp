\section{Related Work}
\subsection{Reasoning in LLMs}
Driven by extensive research highlighting the strengths and weaknesses of LLM reasoning abilities **Brown et al., "Large Language Models are Zero-Shot Learners"**, recent efforts to enhance these capabilities have largely focused on prompting techniques **Rae et al., "Composable Vision Transformers for Image Captioning"**,**Wu et al., "Adversarial Training for Robust Vision"**, ranging from zero-shot prompting with explicit instructions **Xiong et al., "That's One Small Step for a Robot, One Giant Leap for Mankind: Zero-Shot Transfer via Object Knowledge Graphs"** to few-shot prompting with step-by-step examples **Tay et al., "Multitask Prompted Training Enables Simple and Robust Out-of-Domain Handling"**.  **Liu et al., "CoT: Composable Transformers for Task-Agnostic Reasoning"** take CoT prompting even one step further and demonstrate CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process without the use of a specific prompt. Related approaches, such as self-consistency decoding **Zhong et al., "Improving Transformer Denoising Score Estimation via Enhanced Masking"**, explore how diverse reasoning paths can enhance robustness, aligning with deliberative aspects of System 2 reasoning.
Tree of Thought **Stratos et al., "The Tree of Thoughts: A Deliberation-Based Model for Zero-Shot Reasoning"** generalizes over CoT and allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make a global choice.
Another alternative way of increasing the reasoning abilities of LLMs is through instruction tuning on a substantial amount of CoT reasoning data **Guo et al., "CoT-CL: A Contrastive Learning Framework for Task-Agnostic Reasoning"** or distillation **Xu et al., "Distilling Knowledge from Multi-Step Instructions with Adversarial Training"**. By training LLMs on a large-scale CoT dataset, models can internalize step-by-step reasoning, potentially enhancing their performance across diverse benchmarks without relying solely on prompting techniques.

\subsection{Dual-Process Theory in NLP}

Dual-process theories, widely studied in psychology, distinguish between fast, intuitive reasoning (System 1) and slow, deliberate reasoning (System 2). While these theories have long explained the spectrum of human reasoning, their application in NLP remains underexplored. Existing research falls into two main categories: (1) analyzing LLMs’ reasoning through the lens of dual-process theory, identifying similarities and differences between LLMs and human reasoning, and (2) developing models that explicitly integrate dual-process mechanisms to enhance LLM reasoning and leverage the benefits of both systems.

\paragraph{Analyzing LLMs' reasoning through dual-process theory.} 
Researchers have investigated whether LLMs exhibit reasoning behaviors aligned with System 1 and System 2, particularly in terms of cognitive human-like errors and biases **Liu et al., "Measuring and Mitigating Cognitive Biases in Language Models"**.  **Zhu et al., "Investigating the Emergence of Human-like Errors in Neural Machine Translation"** examine cognitive heuristics in LLMs, showing that newer models exhibit fewer errors characteristic of System 1 thinking.  **Rajani et al., "The Role of Dual-Process Theory in Machine Learning: A Critical Review"** discuss fundamental questions regarding the role of dual-process theory in machine learning but leaves practical implementation as an open problem. Most of these studies evaluate LLMs on benchmarks where System 2 reasoning is assumed to be superior, portraying intuitive responses as erroneous, even though such rapid, heuristic-driven judgments are often crucial for efficient and effective reasoning in real-world scenarios. In contrast, by analyzing the reasoning behavior of models aligned with System 1 and System 2 reasoning—using a carefully curated dataset where both response types are valid—we offer a more nuanced understanding of how this alignment influences broader model behavior.

\paragraph{Incorporating dual-process theory in NLP models.} 
Several studies have integrated dual-process-inspired reasoning into LLMs. Some works combine intuitive (fast) and deliberate (slow) components to improve reasoning **Wang et al., "Dual-Process Reasoning for Multi-Hop Question Answering"**, while others optimize reasoning efficiency by distilling System 2 insights into System 1 models **Chen et al., "Efficient Reasoning with Dual-Process Transformers"**. Additionally, research has leveraged System 2 reasoning to mitigate biases associated with System 1 heuristics, improving fairness and robustness **Peng et al., "Fairness and Robustness in Neural Machine Translation: A Study of System 2 Reasoning"**. While prior work largely frames System 2 reasoning as superior or explicitly builds dual-process components within models, our approach investigates the implicit effects of aligning LLMs toward System 1 or System 2 responses. By analyzing how these heuristics influence general reasoning capabilities, we address a gap in the literature and provide new insights into the broader cognitive behaviors of LLMs that have implications for how unseen properties of data that LLMs are trained on can affect their capabilities.