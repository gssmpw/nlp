\section{Related Work}
\subsection{Reasoning in LLMs}
Driven by extensive research highlighting the strengths and weaknesses of LLM reasoning abilities \citep[e.g., ][]{huang2022towards,mondorf2024beyond,valmeekam2022large,parmar2024logicbench,sourati2024arn}, recent efforts to enhance these capabilities have largely focused on prompting techniques \citep{brown2020language}, ranging from zero-shot prompting with explicit instructions \citep{kojima2022large, wang-etal-2023-plan, zhou2024self} to few-shot prompting with step-by-step examples \citep{wei2022chain}. \citet{wang2024chain} take CoT prompting even one step further and demonstrate CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process without the use of a specific prompt. Related approaches, such as self-consistency decoding \cite{wang2022self}, explore how diverse reasoning paths can enhance robustness, aligning with deliberative aspects of System 2 reasoning.
Tree of Thought \citep[ToT;][]{yao2024tree} generalizes over CoT and allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make a global choice.
Another alternative way of increasing the reasoning abilities of LLMs is through instruction tuning on a substantial amount of CoT reasoning data \cite{chung2024scaling, huang2022large} or distillation \cite{magister2022teaching}. By training LLMs on a large-scale CoT dataset, models can internalize step-by-step reasoning, potentially enhancing their performance across diverse benchmarks without relying solely on prompting techniques. 

\subsection{Dual-Process Theory in NLP}

Dual-process theories, widely studied in psychology, distinguish between fast, intuitive reasoning (System 1) and slow, deliberate reasoning (System 2). While these theories have long explained the spectrum of human reasoning, their application in NLP remains underexplored. Existing research falls into two main categories: (1) analyzing LLMs’ reasoning through the lens of dual-process theory, identifying similarities and differences between LLMs and human reasoning, and (2) developing models that explicitly integrate dual-process mechanisms to enhance LLM reasoning and leverage the benefits of both systems.

\paragraph{Analyzing LLMs' reasoning through dual-process theory.} 
Researchers have investigated whether LLMs exhibit reasoning behaviors aligned with System 1 and System 2, particularly in terms of cognitive human-like errors and biases \citep{hagendorff2023human,booch2021thinking,pan2024dynathink,echterhoff2024cognitive,zeng2024mr}. \citet{hagendorff2023human} examine cognitive heuristics in LLMs, showing that newer models exhibit fewer errors characteristic of System 1 thinking. \citet{booch2021thinking} discuss fundamental questions regarding the role of dual-process theory in machine learning but leaves practical implementation as an open problem. Most of these studies evaluate LLMs on benchmarks where System 2 reasoning is assumed to be superior, portraying intuitive responses as erroneous, even though such rapid, heuristic-driven judgments are often crucial for efficient and effective reasoning in real-world scenarios. In contrast, by analyzing the reasoning behavior of models aligned with System 1 and System 2 reasoning—using a carefully curated dataset where both response types are valid—we offer a more nuanced understanding of how this alignment influences broader model behavior.

\paragraph{Incorporating dual-process theory in NLP models.} 
Several studies have integrated dual-process-inspired reasoning into LLMs. Some works combine intuitive (fast) and deliberate (slow) components to improve reasoning \citep{he-etal-2024-planning, liu2022neural, hua-zhang-2022-system, pan2024dynathink}, while others optimize reasoning efficiency by distilling System 2 insights into System 1 models \citep{yang2024llm2, deng2024cognidual, yu2024distilling}. Additionally, research has leveraged System 2 reasoning to mitigate biases associated with System 1 heuristics, improving fairness and robustness \citep{furniturewala2024thinking, kamruzzaman2024prompting, weston2023system}. While prior work largely frames System 2 reasoning as superior or explicitly builds dual-process components within models, our approach investigates the implicit effects of aligning LLMs toward System 1 or System 2 responses. By analyzing how these heuristics influence general reasoning capabilities, we address a gap in the literature and provide new insights into the broader cognitive behaviors of LLMs that have implications for how unseen properties of data that LLMs are trained on can affect their capabilities.