% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{array}
\usepackage{amsmath}

\newcolumntype{N}{>{\centering\arraybackslash}p{1.5cm}}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{tcolorbox}
\usepackage{floatrow}
\DeclareFloatFont{tiny}{\tiny}
\floatsetup[table]{font=tiny}
% \usepackage[hyphens]{url}

% \definecolor{green5}{RGB}{145, 101, 194}
% \definecolor{green4}{RGB}{165, 129, 205}
% \definecolor{green3}{RGB}{198, 175, 223}
% \definecolor{green2}{RGB}{231, 221, 241}

% \definecolor{red5}{RGB}{218, 120, 21}
% \definecolor{red4}{RGB}{235, 140, 45}
% \definecolor{red3}{RGB}{240, 167, 95}
% \definecolor{red2}{RGB}{251, 231, 211}


\definecolor{green5}{RGB}{96, 197, 98}
\definecolor{green4}{RGB}{134, 211, 135}
\definecolor{green3}{RGB}{171, 224, 172}
\definecolor{green2}{RGB}{227, 245, 227}


\definecolor{red5}{RGB}{252, 196, 41}
\definecolor{red4}{RGB}{253, 218, 122}
\definecolor{red3}{RGB}{254, 235, 181}
\definecolor{red2}{RGB}{254, 241, 205}

\newcommand{\getcellcolor}[1]{%
    \ifdim #1 pt > 4 pt \cellcolor{green5}%
    \else\ifdim #1 pt > 2 pt \cellcolor{green4}%
    \else\ifdim #1 pt > 1 pt \cellcolor{green3}%
    \else\ifdim #1 pt > 0 pt \cellcolor{green2}%
    \else\ifdim #1 pt = 0 pt \cellcolor{white}%
    \else\ifdim #1 pt > -1 pt \cellcolor{red2}%
    \else\ifdim #1 pt > -2 pt \cellcolor{red3}%
    \else\ifdim #1 pt > -4 pt \cellcolor{red4}%
    \else \cellcolor{red5}%
    \fi\fi\fi\fi\fi\fi\fi\fi
}

% \definecolor{green0}{rgb}{0.92, 0.97, 1.00}  % Very light, fresh blue
% \definecolor{green1}{rgb}{0.85, 0.93, 0.98}  % Light, soft blue
% \definecolor{green2}{rgb}{0.655, 0.843, 0.667}  % Medium-light blue
% \definecolor{green3}{rgb}{0.380, 0.721, 0.400}  % Medium blue
% \definecolor{green4}{rgb}{0.278, 0.620, 0.302}  % Medium-dark blue
% \definecolor{green5}{rgb}{0.157, 0.345, 0.169}  % Dark blue

% \definecolor{red0}{rgb}{1.00, 0.95, 0.92}  % Very light, soft orange
% \definecolor{red1}{rgb}{0.98, 0.89, 0.84}  % Light, warm orange
% \definecolor{red2}{rgb}{0.96, 0.74, 0.74}  % Medium-light orange
% \definecolor{red3}{rgb}{0.91, 0.31, 0.31}  % Medium orange
% \definecolor{red4}{rgb}{0.78, 0.12, 0.12}  % Medium-dark orange
% \definecolor{red5}{rgb}{0.43, 0.07, 0.07}  % Dark orange

% \newcommand{\arrowcell}[2]{%
%     \ifdim #2 pt > 4 pt \textcolor{green5}{\boldmath$\uparrow$}%
%     \else\ifdim #2 pt > 2 pt \textcolor{green4}{\boldmath$\uparrow$}%
%     \else\ifdim #2 pt > 1 pt \textcolor{green3}{\boldmath$\uparrow$}%
%     \else\ifdim #2 pt > 0 pt \textcolor{green2}{\boldmath$\uparrow$}%
%     \else\ifdim #2 pt = 0 pt \textcolor{white}{\boldmath$\uparrow$}%
%     \else\ifdim #2 pt > -1 pt \textcolor{red2}{\boldmath$\downarrow$}%
%     \else\ifdim #2 pt > -2 pt \textcolor{red3}{\boldmath$\downarrow$}%
%     \else\ifdim #2 pt > -4 pt \textcolor{red4}{\boldmath$\downarrow$}%
%     \else \textcolor{red5}{\boldmath$\downarrow$}%
%     \fi\fi\fi\fi\fi\fi\fi\fi
%     \makecell{#1}
% }


% Add difference and baseline to the cell
\newcommand{\highlightcell}[2]{%
    \getcellcolor{#2} #1
}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{duckuments}
\usepackage{booktabs}
\usepackage{multirow}
% add [disable] to turn it off
\usepackage{todonotes} 

\newcommand{\atodo}[2][]{\todo[color=cyan!20,size=\scriptsize,#1]{old: #2}}
\newcommand{\ntodo}[2][]{\todo[color=yellow!20,size=\scriptsize,#1]{Nana: #2}}
\newcommand{\stodo}[2][]{\todo[color=magenta!20,size=\scriptsize,#1]{Sooolati: #2}}
\newcommand{\otodo}[2][]{\todo[color=green!20,size=\scriptsize,#1]{Ostad: #2}}
\renewcommand{\arraystretch}{1.5}

% \renewcommand{\sectionautorefname}{Section}
% \renewcommand{\subsectionautorefname}{Section}
% \renewcommand{\subsubsectionautorefname}{Section}
\usepackage{cleveref}



\title{Reasoning on a Spectrum: \\ Aligning LLMs to System 1 and System 2 Thinking}

\author{ Alireza S. Ziabari\thanks{Equal contribution.}
\And Nona Ghazizadeh\footnotemark[1] 
\And Zhivar Sourati  \AND   Farzan Karimi-Malekabadi
\And Payam Piray \And Morteza Dehghani \AND \normalfont \small  University of Southern California \\
\texttt{\small\{salkhord, nghaziza, souratih, karimima, piray, mdehghan\}@usc.edu} \\
}

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) exhibit impressive reasoning abilities, yet their reliance on structured step-by-step processing reveals a critical limitation. While human cognition fluidly adapts between intuitive, heuristic (System 1) and analytical, deliberative (System 2) reasoning depending on the context, LLMs lack this dynamic flexibility. This rigidity can lead to brittle and unreliable performance when faced with tasks that deviate from their trained patterns. To address this, we create a dataset of 2,000 samples with valid System 1 and System 2 answers, explicitly align LLMs with these reasoning styles, and evaluate their performance across reasoning benchmarks. Our results reveal an accuracy-efficiency trade-off: System 2-aligned models excel in arithmetic and symbolic reasoning, while System 1-aligned models perform better in commonsense tasks. A mechanistic analysis of model responses shows that System 1 models employ more definitive answers, whereas System 2 models demonstrate greater uncertainty. Interpolating between these extremes produces a monotonic transition in reasoning accuracy, preserving coherence. This work challenges the assumption that step-by-step reasoning is always optimal and highlights the need for adapting reasoning strategies based on task demands.\footnote{Our data and code are available at \url{https://github.com/AlirezaZiabari/System_1_System_2_Alignment}}
\end{abstract}

\section{Introduction}
% Main arguments
% (1) LLMs struggle in reasoning task when it has nuance or needs complex reasoning
% (2) LLMs have the same biases as human in REASONING TASKS! (this bias is different from gender-bias) they act like human!!!
% (3) LLMs can generate system 1 and system 2 answers, for example, CoT is a proxy to system 2 answer!

LLMs have demonstrated remarkable reasoning capabilities, often achieving near-human or even superhuman performance \citep{huang-chang-2023-towards}. These advances have largely been driven by techniques that simulate step-by-step, deliberative reasoning, such as Chain-of-Thought (CoT) prompting and inference-time interventions \citep{wei2022chain, wang2022self}. Given their success, such methods are increasingly integrated into LLM training \citep{chung2024scaling}, reinforcing explicit, structured reasoning regardless of the task necessity. However, LLM reasoning remains brittle, particularly in tasks requiring nuanced judgment \citep{deletang2023neural}, logical consistency \citep{jiang-etal-2024-peek}, or adaptability to uncertainty \citep{mirzadeh2024gsm}. For example, when faced with simple factual queries, they typically generate unnecessarily verbose explanations instead of direct answers \citep{wang-etal-2023-plan}. 
\input{figures/pipeline}

This focus on explicit, structured reasoning highlights a key difference between LLMs and human cognition: while LLMs are being pushed towards a single mode of processing, human reasoning is far more nuanced. Rather than a monolithic process, human reasoning emerges from a sophisticated suite of cognitive tools evolved to tackle a \emph{spectrum} of computational problems. This spectrum of human reasoning encompasses both automatic and reflective processes, a key insight recognized across diverse fields from behavioral economics to psychology and neuroscience \citep{daw2005uncertainty, dolan2013goals, balleine1998goal}. On one end lie computationally \emph{light} problems demanding rapid, intuitive judgments (e.g. instinctively dodging a speeding car), handled by the reflexive ``System 1.''  On the other end are \emph{heavy} problems requiring deliberate, step-by-step analysis, managed by the reflective ``System 2'' \citep{kahneman2011thinking, stanovich2000advancing}. This dual-process system allows us to dynamically shift between modes depending on the task, balancing speed and accuracy \citep{evans2013dual}.

While some studies explore whether LLMs exhibit System 1 and System 2 behaviors \citep{hagendorff2023human, pan2024dynathink} or attempt to create hybrid models \citep{yang2024llm2, deng2024cognidual}, virtually all prior work implicitly assumes that structured, deliberative reasoning is universally superior. Even research suggesting LLMs' capacity for both reasoning modes \citep{wang2024chain} largely overlooks the crucial question of when each mode is indeed advantageous. The assumption that a single "best" reasoning strategy can apply across all contexts is a fundamental simplification that limits current approaches in LLM development. This assumption prevents LLMs from achieving true cognitive flexibility, hindering their ability to adapt their reasoning processes to diverse situations.

To address this gap, we propose explicitly aligning LLMs with System 1 and System 2 reasoning and evaluating their reasoning capabilities. Our approach involves designing an experimental setup where both thinking styles can produce valid answers but follow distinct paths, one leveraging intuitive heuristics, and the other prioritizing deliberate, step-by-step reasoning. By systematically assessing the trade-offs between accuracy and efficiency, we provide insights into when intuitive heuristics or structured deliberation is most effective.

Specifically, as demonstrated in \Cref{fig:overview}, we first construct a dataset of 2,000 reasoning tasks, where each problem has both a fast, heuristic-driven (System 1) response and a deliberative, structured (System 2) response, grounded in 10 different cognitive heuristics \citep{tversky1974judgment}. We then explicitly align LLMs with either System 1 or System 2 type responses and evaluate these models on diverse reasoning tasks. Our findings reveal a structured accuracy-efficiency trade-off: System 2-aligned models consistently outperform pre-trained and CoT prompt baselines in arithmetic and symbolic reasoning, demonstrating superior multi-step inference, but generating more extended token-intensive responses. Conversely, System 1-aligned models generate more succinct responses and excel at commonsense reasoning, where heuristic shortcuts are effective. Importantly, unlike CoT models, which always engage in structured reasoning regardless of necessity, our models provide an explicit way to study when different reasoning styles are beneficial, mirroring the well-known efficiency-accuracy trade-off in human cognition \citep{keramati2011speed, mattar2018prioritized}. By framing LLM reasoning as a structured and adaptable process, rather than simply an ability to achieve higher benchmark scores, this work highlights the importance of selecting the right reasoning strategy for a given task. This perspective not only aligns LLM reasoning more closely with human cognition but also paves the way for more flexible, efficient, and robust reasoning systems, setting a foundation for future advancements in LLM reasoning.


\section{Related Work}
\subsection{Reasoning in LLMs}
Driven by extensive research highlighting the strengths and weaknesses of LLM reasoning abilities \citep[e.g., ][]{huang2022towards,mondorf2024beyond,valmeekam2022large,parmar2024logicbench,sourati2024arn}, recent efforts to enhance these capabilities have largely focused on prompting techniques \citep{brown2020language}, ranging from zero-shot prompting with explicit instructions \citep{kojima2022large, wang-etal-2023-plan, zhou2024self} to few-shot prompting with step-by-step examples \citep{wei2022chain}. \citet{wang2024chain} take CoT prompting even one step further and demonstrate CoT reasoning paths can be elicited from pre-trained LLMs by simply altering the decoding process without the use of a specific prompt. Related approaches, such as self-consistency decoding \cite{wang2022self}, explore how diverse reasoning paths can enhance robustness, aligning with deliberative aspects of System 2 reasoning.
Tree of Thought \citep[ToT;][]{yao2024tree} generalizes over CoT and allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make a global choice.
Another alternative way of increasing the reasoning abilities of LLMs is through instruction tuning on a substantial amount of CoT reasoning data \cite{chung2024scaling, huang2022large} or distillation \cite{magister2022teaching}. By training LLMs on a large-scale CoT dataset, models can internalize step-by-step reasoning, potentially enhancing their performance across diverse benchmarks without relying solely on prompting techniques. 

\subsection{Dual-Process Theory in NLP}

Dual-process theories, widely studied in psychology, distinguish between fast, intuitive reasoning (System 1) and slow, deliberate reasoning (System 2). While these theories have long explained the spectrum of human reasoning, their application in NLP remains underexplored. Existing research falls into two main categories: (1) analyzing LLMs’ reasoning through the lens of dual-process theory, identifying similarities and differences between LLMs and human reasoning, and (2) developing models that explicitly integrate dual-process mechanisms to enhance LLM reasoning and leverage the benefits of both systems.

\paragraph{Analyzing LLMs' reasoning through dual-process theory.} 
Researchers have investigated whether LLMs exhibit reasoning behaviors aligned with System 1 and System 2, particularly in terms of cognitive human-like errors and biases \citep{hagendorff2023human,booch2021thinking,pan2024dynathink,echterhoff2024cognitive,zeng2024mr}. \citet{hagendorff2023human} examine cognitive heuristics in LLMs, showing that newer models exhibit fewer errors characteristic of System 1 thinking. \citet{booch2021thinking} discuss fundamental questions regarding the role of dual-process theory in machine learning but leaves practical implementation as an open problem. Most of these studies evaluate LLMs on benchmarks where System 2 reasoning is assumed to be superior, portraying intuitive responses as erroneous, even though such rapid, heuristic-driven judgments are often crucial for efficient and effective reasoning in real-world scenarios. In contrast, by analyzing the reasoning behavior of models aligned with System 1 and System 2 reasoning—using a carefully curated dataset where both response types are valid—we offer a more nuanced understanding of how this alignment influences broader model behavior.

\paragraph{Incorporating dual-process theory in NLP models.} 
Several studies have integrated dual-process-inspired reasoning into LLMs. Some works combine intuitive (fast) and deliberate (slow) components to improve reasoning \citep{he-etal-2024-planning, liu2022neural, hua-zhang-2022-system, pan2024dynathink}, while others optimize reasoning efficiency by distilling System 2 insights into System 1 models \citep{yang2024llm2, deng2024cognidual, yu2024distilling}. Additionally, research has leveraged System 2 reasoning to mitigate biases associated with System 1 heuristics, improving fairness and robustness \citep{furniturewala2024thinking, kamruzzaman2024prompting, weston2023system}. While prior work largely frames System 2 reasoning as superior or explicitly builds dual-process components within models, our approach investigates the implicit effects of aligning LLMs toward System 1 or System 2 responses. By analyzing how these heuristics influence general reasoning capabilities, we address a gap in the literature and provide new insights into the broader cognitive behaviors of LLMs that have implications for how unseen properties of data that LLMs are trained on can affect their capabilities. 

\section{Method}
\input{tables/datasample}
\subsection{Aligning to System 1 \&  System 2 Thinking}\label{subsec:aligning-to-system1-2-thinking}
We formalize the task of modeling fast or slow thinking as an alignment problem. This approach is effective for two key reasons. First, our aim is not to introduce new knowledge or instructions but rather to shape the model’s reasoning process based on existing capabilities. Second, previous research has shown that prompt engineering can guide LLMs toward System 2 reasoning \citep{wei2022emergent} or System 1 reasoning \citep{zhou-etal-2024-llms}. This suggests that LLMs already have both reasoning abilities. Therefore, instead of creating new reasoning pathways, our goal is to guide the model to favor either System 1 or System 2 thinking by distinguishing between them.

Within an alignment framework, one response is designated as the preferred (winner) answer, while the other is demoted (loser). To encourage System 1 thinking, we align the model by treating the intuitive response as the winner and the deliberative response as the loser. Conversely, to induce System 2 thinking, we invert this preference, treating the analytical response as the winner and the intuitive response as the loser. This structured preference framework allows the model to internalize the distinction between intuitive and analytical reasoning processes without requiring any changes to its underlying knowledge.
In the next section, we describe the dataset creation process that enables alignment training.

\subsection{Dataset of System 1 \& System 2 Thinking} \label{sec:dataset}
Our dataset consists of 2,000 questions designed to elicit two distinct response styles in English: one intuitive and rapid, reflecting cognitive shortcuts (System 1), and the other deliberate and analytical (System 2). This dual structure allows us to study the distinct mechanisms underlying System 1 and System 2 reasoning \citep{kahneman2011thinking, stanovich2000advancing, evans2013dual}. To create this dataset, we use cognitive heuristics that offer a practical way to distinguish between System 1 and System 2 reasoning, where both produce valid but distinct responses. A sample of the curated dataset is shown in \Cref{tab:sampledata}. These questions span 10 different cognitive heuristics (\Cref{appendix:cognitivebiases}). There are four stages for dataset creation.

\paragraph{Cognitive heuristics examples.} To create an initial set of examples, an expert selected 10 categories of heuristics and biases from \citet{kahneman2011thinking} and generated one example question for each category with both System 1 and System 2 answers. These examples are in \Cref{appendix:dataexpertsamples}. 

\paragraph{Data expansion using an LLM.} 
We then expanded the dataset with GPT-4o \cite{hurst2024gpt} with one-shot prompting. For each cognitive heuristic, we provided its definition \citep{kahneman2011thinking}, a description of how System 1 and System 2 thinking would approach the question \citep{kahneman2011thinking}, and an expert-generated example illustrating the heuristic. More details about the prompt for data expansion are outlined in \Cref{appendix:dataexpansionprompt}

\paragraph{Human validation and refinement.}
To ensure the dataset accurately reflected our definitions of fast and slow thinking, as well as the targeted cognitive heuristics, the expert manually reviewed and modified around 20\% of the responses.

\paragraph{Length adjustment.}
As a result of the data expansion process, System 2 answers tended to be longer and more detailed, reflecting their step-by-step reasoning approach, whereas System 1 answers were shorter and more direct, relying on heuristics. The data expansion process yielded a significant difference in answer length between the two reasoning styles, with System 2 answers being substantially longer.
% Since the variances of the two answer types are different based on Levene’s test, we performed Welch's $t$-test to confirm the length difference ($F(1, 3998) = 2487.9$, $p <.001$). 
This difference in length was verified using Welch's $t$-test ($t(2090.1) = -184.74$, $p <.001$, $d = -5.84$). This test was chosen due to unequal variances between the two answer types confirmed by Levene’s test ($F(1, 3998) = 2487.9$, $p <.001$). 
% The test revealed a significant difference in tokenized answer lengths $t(2090.1) = -184.74$, $p <.001$, $d = -5.84$, with System 2 answers being substantially longer.

Recently, \citet{Singhal2023ALW} highlight a strong correlation between output length and perceived quality, with longer outputs often being considered preferable. To address this potential problem, we used GPT-4o with zero-shot prompting to adjust the lengths of System 1 and System 2 answers, ensuring comparability without changing their content. This adjustment was applied only when there was a significant length disparity. More details about the prompt and length disparity threshold are described in \Cref{appendix:length}. By reducing the length disparity, we minimized any preference for System 2 answers arising from their longer responses. After adjustment, System 1 answers had an average length of 82.19 tokens, while System 2 answers averaged 83.93 tokens. A two one-sided t-test (TOST) confirmed the equivalence of these post-adjustment lengths across various token counts as equivalence margins,\footnote{$\pm 3$ tokens, $t(3870.30) = 85.82$, $p < .001$; $\pm 5$ tokens, $t(3870.30) = 149.07$, $p < .001$; $\pm 7$ tokens, $t(3870.30) = 212.31$, $p < .001$; and 5\% of the mean token count ($\pm 4.15$ tokens), $t(3870.30) = 122.29$, $p < .001$} indicating that the adjustment effectively eliminated significant length differences between the two response types. 

\section{Experiments}
\subsection{Alignment Algorithm}\label{sec:exp_alg}
To implement the alignment strategy for System 1 and System 2 reasoning, we utilize two prominent preference optimization methods: 

Direct Preference Optimization \citep[DPO;][]{rafailov2024direct} is an offline alignment method that fine-tunes LLMs by comparing the preferred and disfavored outputs of a model against a reference model, optimizing preferences without requiring a separate reward model. As a prominent method in preference optimization, DPO has gained traction for its stability and efficiency, making it a widely adopted alternative to Reinforcement Learning from Human Feedback \citep[RLHF;][]{ouyang2022training}.

Simple Preference Optimization \citep[SimPO;][]{meng2024simpo} builds on the principles of DPO but introduces a reference-free approach to preference optimization. Instead of requiring a separate reference model, SimPO aligns responses by directly optimizing preference signals within the model itself. This makes it computationally more efficient and removes the dependency on an external reference model, offering a streamlined alternative for aligning LLMs to a specific preference.


\subsection{Benchmarks}\label{benchmark}
Building on previous research in LLM reasoning \cite{wei2022chain, kojima2022large, kong2024betterzeroshotreasoningroleplay}, we evaluate our System 1 and System 2 models using 10 reasoning benchmarks across three different categories: (1) arithmetic reasoning, which includes MultiArith \cite{roy-roth-2015-solving}, GSM8K \cite{cobbe2021trainingverifierssolvemath}, AddSub \cite{hosseini-etal-2014-learning}, AQUA-RAT \cite{ling-etal-2017-program}, SingleEq \cite{10.1162/tacl_a_00160}, and SVAMP \cite{patel-etal-2021-nlp}; (2) commonsense reasoning, including CSQA \cite{talmor-etal-2019-commonsenseqa} and StrategyQA \cite{10.1162/tacl_a_00370}; (3) symbolic reasoning, covering Last Letter Concatenation and Coin Flip \cite{wei2022chain}. More details about the benchmarks are in \Cref{appendix:benchmarkdetails}.

Following \citet{kong2024betterzeroshotreasoningroleplay}, our evaluation follows a two-stage process. In the first stage, we present the benchmark questions to the model and record its responses. In the second stage, we prompt the model again, this time providing the original question, its initial response, and benchmark-specific instructions to ensure the output is formatted as required. The instructions for each benchmark are detailed in \Cref{appendix:benchmarkinstruction}.
\input{tables/exp1_noother}

\subsection{Experiment Setup \& Details}
\label{subsec:experimental-setup}
We use Llama-3-8B-Instruct \citep{llama3modelcard} and Mistral-7B-Instruct-v0.1 \citep{jiang2023mistral7b} as the SFT models in the alignment process. Following \citet{kojima2023largelanguagemodelszeroshot}, we compare the performance of these aligned models against their pre-trained counterparts under zero-shot and zero-shot CoT prompting (additional details in \Cref{appendix:impdet}).

To analyze the model's behavior along the System 1-System 2 reasoning spectrum, we train seven intermediate models, where the winner responses are mixed at predefined ratios: 87.5\%-12.5\%, 75\%-25\%, 62.5\%-37.5\%, 50\%-50\%, 37.5\%-62.5\%, 25\%-75\%, and 12.5\%-87.5\%, between System 1 and System 2. This structured variation allows us to systematically examine the transition between System 1 and System 2 reasoning.

\section{Results}
\subsection{Comparing Reasoning Benchmarks}\label{res:system2}
\Cref{tab:compare} presents a comparison of exact matching accuracy across 10 reasoning benchmarks for two LLMs, Llama and Mistral. Specifically, we compare the base models with the System 1 and System 2 variants created using the alignment algorithms described in \Cref{sec:exp_alg}.  We also include results for CoT prompting for reference. Our findings reveal distinct performance trends for the System 1 and System 2 models, highlighting their respective strengths in different reasoning paradigms.

Across all arithmetic benchmarks (MultiArith, GSM8K, AddSub, AQuA, and SingleEq), System 2 models consistently outperformed both the base model and their System 1 counterpart, evident for both Llama and Mistral. This improvement is most significant in AddSub and SingleEq benchmarks. Similarly, System 2 models outperformed System 1 models in nearly all symbolic reasoning tasks. These tasks require pattern recognition and logical structuring, further validating the idea that deliberative, slow-thinking models enhance performance in structured reasoning.


Conversely, System 1 models excelled at commonsense reasoning tasks, particularly on the CSQA and Strategy benchmarks. These tasks, which rely on intuitive judgment, and heuristic decision-making, clearly play to System 1's strengths. Both Llama and Mistral saw their System 1 variants outperform not only their System 2 counterparts but also their respective base and CoT models in these domains.

When comparing Llama and Mistral, Llama models generally achieved higher accuracy across all benchmarks. This suggests that Llama may have stronger foundational reasoning capabilities, which are further enhanced by the System 2 and System 1 alignment. Moreover, pre-trained models with CoT prompt generally showed improvements over base models, reinforcing the effectiveness of explicit step-by-step reasoning. However, since pre-trained models have already been trained on CoT data, the improvements from explicit prompting are not significant. This suggests that step-by-step thinking has been internalized within these pre-trained models, reducing the necessity for additional CoT prompts. Based on this observation, we focus solely on Llama models in the following experiments.

In summary, our results showcase that System 2 models excel in structured, multi-step reasoning tasks such as arithmetic and symbolic reasoning, while System 1 models are particularly effective in intuitive and commonsense reasoning tasks. These findings highlight the significant potential of dual-process alignment for boosting LLM performance across a diverse range of reasoning paradigms.

\input{figures/resanalysis}
\input{figures/ratio}

\subsection{Model Response Analysis}\label{res:system1}
As described in \Cref{benchmark}, we use a two-stage prompting to generate the final responses from our models. \Cref{fig:resanaylsis} shows the difference in token counts for System 1 and System 2 answers, relative to the Llama model, across both stages for the two alignment algorithms, DPO and SimPO. Although both System 1 and System 2 models are aligned based on the equal-length response as mentioned in \Cref{sec:dataset}, the average length of tokens in System 1 and System 2 answers differs in the second stage for both DPO, $t(8836) = 57.14, p < .001$, and  SimPO, $t(8586) = 9.833, p < .001$. This suggests that the System 2-aligned model does not rely solely on its initial response and attempts to elaborate, resulting in more accurate answers when this additional rigor is beneficial. 


% \ntodo[]{add something to relate it to exp4}
\subsection{Moving from Fast to Slow Thinking}\label{res:ratio}
In the previous study, we considered System 1 and System 2 as endpoints on a spectrum. Paralleling psychological approaches \citep{daw2011model, piray2021linear}, we explored the space between these extremes by creating interpolated models by blending System 2 and System 1 answers at varying ratios. %\Cref{fig:ratio} shows a monotonic accuracy shift across all benchmarks ($r^2 > 0.9, p < 0.001$ for all), with no abrupt fluctuations, indicating that transitioning from intuitive to structured reasoning does not introduce instability. %Absolute Pearson correlation (r) values across all benchmarks are above 0.9.
\Cref{fig:ratio} demonstrates a consistent, monotonic increase in accuracy across all benchmarks ($r^2 > 0.9, p < 0.001$). Critically, there are no sudden drops or fluctuations in performance when transitioning between reasoning styles. This stability indicates that the shift from System 1 to System 2 reasoning is gradual and predictable, without any unexpected anomalies. This observation reinforces the idea that LLMs can be strategically guided toward different reasoning styles without sacrificing the coherence of their responses.
%This observation reinforces the idea that LLMs can be strategically guided toward different reasoning modes without losing coherence in their responses.
\input{figures/exp4}

While arithmetic and symbolic reasoning tasks exhibit a steady increase in accuracy moving toward System 2 thinking, commonsense tasks show the opposite trend, with accuracy increasing as models rely more on System 1 reasoning. This trade-off highlights that both reasoning styles offer unique advantages, with System 2 excelling in structured, multi-step problem-solving and System 1 providing efficient, adaptable responses in intuitive scenarios. These findings straighten the importance of task-dependent reasoning strategies that leverage the strengths of both System 1 and System 2 thinking.

\subsection{Reasoning \& Uncertainty}\label{res:uncertainty}
Beyond evaluating model reasoning abilities based on reasoning benchmarks, we examine how uncertainty manifests in System 1 and System 2 responses. A key insight from psychology and neuroscience is that System 1 operates on confident heuristics, providing quick, intuitive judgments, while System 2 engages in more deliberate, analytical thought, accurately assessing the uncertainty associated with its conclusions \citep{daw2005uncertainty, lee2014neural, keramati2011speed, xu2021career}. To examine uncertainty and confidence, we consider three different characteristics: 1) token-level uncertainty; 2) the presence of hedge words \citep{lakoff1973hedges,ott2018hedging} such as \textit{``might''} and \textit{``possibly''}\footnote{Gathered from \url{https://github.com/words/hedges}.} in model output; and 3) definitive commitment to answers in System 1 versus System 2.
% Our results confirm that our steered models exhibit such reasoning properties. 

Measuring token-level uncertainty through the logits of generated tokens, \Cref{fig:exp4}, Plot A shows that System 2-aligned models consistently generate tokens with lower confidence compared to System 1 models. This trend holds across arithmetic $t(4075) = 54.53, p < .001$, symbolic $t(999) = 42.53, p < .001$, and commonsense $t(3510) = 106.86, p < .001$ benchmarks. Additionally, we analyzed surface-level uncertainty in model reasoning by examining word choices. \Cref{fig:exp4}, Plot B shows System 2-aligned models use significantly more hedge words, in arithmetic $t(4075) = 22.03, p < .001$ and commonsense $t(3510) = 21.49, p < .001$ when models reiterate their reasoning. While increased uncertainty enhances analytical reasoning, it may hinder tasks requiring rapid, intuitive judgments. To assess early-stage response conclusiveness, we used LLM-as-Judge \citep{zheng2023judging} as detailed in \Cref{appendix:additional-insights-into-models-reasoning}. \Cref{fig:exp4}, Plot C shows System 1 models provide significantly more definitive answers than System 2 models in commonsense reasoning, $\textit{McNemar's } \chi^2(1, 400) = 20.0, p < .001$, regardless of where in the response the definitive answer is reached (see \Cref{appendix:additional-insights-into-models-reasoning}).


This analysis reinforces the idea that different reasoning styles are suited to different tasks. Greater uncertainty in models' generated reasoning suggests that System 2 models can explore alternative reasoning paths more effectively. This uncertainty is reflected in both their model output probabilities and word choices. System 2 models' superior performance in arithmetic tasks highlights the benefits of deliberate, effortful processing in tasks that demand exploration and uncertainty. 
On the other hand, the greater tendency of System 1 models to commit to answers in a more definitive way aligns with their advantage in tasks requiring rapid and intuitive judgments. This behavior is observed exclusively in commonsense reasoning, where quick, decisive responses are advantageous—a trend supported by human studies \citep{byrd2022bounded} and confirmed by our findings in \Cref{res:system2}. However, it does not appear in other benchmarks (see \Cref{appendix:additional-insights-into-models-reasoning}), suggesting that the activation of a particular reasoning style is context-dependent and influenced by task demands. 
% Furthermore, the lack of significant differences in hedge words ratio for symbolic tasks suggests that certain types of reasoning do not strongly favor either System 1 or System 2. This indicates that some problems may be inherently accessible to both reasoning styles, without requiring the rapid intuitions of System 1 or the structured analytical approach of System 2.

\section{Conclusion}
We proposed aligning LLMs to System 1 and System 2 thinking, representing two ends of the spectrum in fast and slow cognitive processes. Our results demonstrate that each of these models exhibits distinct properties: System 2 excels in arithmetic and symbolic reasoning, while System 1 is more effective and accurate in commonsense reasoning (\Cref{res:system2}). By introducing intermediate models that interpolate between System 1 and System 2, we observed a monotonic accuracy shift across all benchmarks, reinforcing the idea that reasoning ability transitions smoothly along this spectrum (\Cref{res:ratio}). Additionally, System 1 models generate responses with fewer tokens, highlighting its efficiency in decision-making (\Cref{res:system1}). Finally, our analysis in \Cref{res:uncertainty} illustrated that System 2 models exhibit greater uncertainty throughout the reasoning process, potentially enabling them to engage in more structured, step-by-step problem-solving. In contrast, System 1 models display higher confidence, allowing them to reach answers faster, which is particularly advantageous for tasks requiring rapid, intuitive judgments.

Beyond these empirical findings, our study aligns with broader principles observed across cognitive science and neuroscience. The observation that System 1 models generate faster responses echoes established theories in human cognition, where intuitive, heuristic-driven thinking allows for rapid decision-making. Similarly, the higher uncertainty exhibited by System 2 models aligns with neuroscience findings that deliberate reasoning involves increased cognitive load and self-monitoring mechanisms. These parallels suggest that LLMs, when properly aligned, can mirror key aspects of human cognition, offering new insights into both artificial and natural intelligence.

This work is a first step toward adaptive reasoning in LLMs, where models can dynamically shift between heuristic and deliberative thinking based on task demands. Furthermore, understanding how to optimally balance speed and accuracy in LLMs can have significant implications for real-world applications, from conversational agents to automated decision-making systems. By grounding our findings in cognitive science, we provide an approach for developing more flexible models that can strategically deploy different reasoning styles on various tasks to maximize efficiency and effectiveness.


% By framing LLM reasoning as a structured and controllable process rather than a static improvement, this work provides a foundation for AI systems that can better model human-like reasoning trade-offs. While this study establishes the first structured approach for training LLMs in distinct reasoning styles, future research can build on this framework to develop mechanisms for adaptive reasoning selection based on task complexity, improving both efficiency and robustness in decision-making.

\section*{Limitations}
Despite the promising advancements of using different thinking styles presented in our approach, several limitations should be acknowledged. First, our curated dataset of 2,000 questions, though designed to capture diverse cognitive heuristics, may not fully represent the entire spectrum of reasoning challenges encountered in real-world tasks. Second, the reliance on prompt engineering and specific alignment algorithms (DPO and SimPO) means that our findings may be sensitive to changes in model architecture or training procedures, potentially limiting generalizability across different LLMs. Third, our analysis of uncertainty—using token-level logits and heuristic word choices—provides a useful proxy but may not comprehensively capture the nuanced aspects of human-like uncertainty. Finally, while our experiments reveal a clear accuracy-efficiency trade-off between intuitive and deliberative reasoning, the extent to which these findings translate to more complex or dynamic decision-making scenarios remains an open question. Future work should explore larger, more diverse datasets and investigate alternative alignment strategies to further validate and extend these results.

\section*{Ethical Statement}
This research was conducted with ethical considerations in data collection, model alignment, and societal impact. All cognitive heuristic examples and dataset validation were performed by the authors to guarantee quality but potentially introduce biases.

Aligning LLMs with System 1 and System 2 reasoning raises concerns about model behavior in different contexts. System 1 models may produce overly confident but incorrect answers, while System 2 models, though more deliberate, may slow response times and increase computational costs. Responsible deployment requires balancing these trade-offs to prevent biased or misleading outputs.

\section*{Acknowledgments}
This research was supported by DARPA INCAS HR001121C0165. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{custom, anthology}

\appendix

\section{Cognitive heuristics} \label{appendix:cognitivebiases}
In \Cref{tab:cognitive_biases}, we list 10 different cognitive heuristics and their definitions, which we used in curating the dataset \cite{kahneman2011thinking, stanovich2000advancing, evans2013dual}.
\input{tables/cogbias}
\section{Initial Data Examples} \label{appendix:dataexpertsamples}
\input{tables/expertsample}
The 10 samples generated by the expert for our data generation are shown in \Cref{tab:expertsample}.
\section{Prompt for Data Expansion} \label{appendix:dataexpansionprompt}
We expand our sample dataset by concatenating the expert-generated samples with the definitions in \Cref{tab:cognitive_biases}, along with a description of how System 1 and System 2 would respond to a given question, as shown below:
\begin{tcolorbox}[colback=gray!10, colframe=black, fontupper=\small]
\texttt{The System 1 response should be intuitive, fast, and reflect the cognitive heuristic associated with the question.}
\end{tcolorbox}

\begin{tcolorbox}[colback=gray!10, colframe=black, fontupper=\small]
\texttt{The System 2 response should be more deliberate, slower, and use reasoning to correct or mitigate the heuristic.}
\end{tcolorbox}

\section{Benchmark Details} \label{appendix:benchmarkdetails}
We use three categories of reasoning tasks: arithmetic, commonsense reasoning, symbolic reasoning,  We provide an overview of the datasets used in each category.
\paragraph{Arithmetic reasoning.}
We use six datasets: MultiArith, GSM8K, AddSub, AQuA, SingleEq, and SVAMP. Each dataset consists of questions that present a scenario requiring numerical computation and multi-step reasoning based on mathematical principles.
\paragraph{Commonsense reasoning.}
To assess commonsense reasoning, we utilize two datasets: CommonsenseQA (CSQA) and StrategyQA. Both require models to infer answers based on prior commonsense knowledge. CSQA focuses on multiple-choice questions grounded in general world knowledge, while StrategyQA includes questions that require implicit multi-hop reasoning.
\paragraph{Symbolic reasoning.}
We use the Last Letter Concatenation and Coin Flip datasets. Last Letter Concatenation involves forming a word by extracting the last letter of given words in order. Coin Flip presents a sequence of coin-flipping instructions and asks for the final coin orientation. These datasets were originally proposed by \citet{wei2023chainofthoughtpromptingelicitsreasoning} but were not publicly available. \citet{kojima2023largelanguagemodelszeroshot} later followed their approach to create and release accessible versions, which we use in our experiments.

\section{Length Adjustment Threshold and Prompt} \label{appendix:length}
We adjust the length if there is a disparity of more than 15 tokens between the System 1 and System 2 answers using GPT-4o with the following prompt:

\begin{tcolorbox}[colback=gray!10, colframe=black, fontupper=\small]
\texttt{For a given \{question\}, we have two types of answers:
\\
A fast, intuitive response based on cognitive heuristics which is our System 1 Answer.
\\
System 1 Answer: \{System 1 Answer\} 
\\
And a slow, deliberate, and logical reasoning response which is our System 2 Answer.
\\
System 2 Answer: \{System 2 Answer\}
\\
Your task is to adjust the two answers so that they are presented in the same order of tokens without altering their content. Ensure that the intuitive nature of the System 1 Answer and the logical reasoning of the System 2 Answer are preserved.}
\end{tcolorbox}

\section{Benchmark Instruction} \label{appendix:benchmarkinstruction}
The benchmark-specific instructions are shown in \Cref{tab:benchmarkinstruction}.
\input{tables/benchmark_instrction}

\section{Implementation Details} \label{appendix:impdet}
We use Python 3.10.12, PEFT 0.12.0, PyTorch 2.4.0, and Transformers 4.44.2.
The dataset is split into 80\% training and 20\% validation.
For alignment, we apply Low-Rank Adaptation \citep[LoRA][]{hu2021lora} with a rank of 8, an alpha of 16, and dropout rate of 0.1.
We train for five epochs, using accuracy on winner responses as an early stopping criterion to prevent overfitting, with patience of 5. We set the train batch size to 4 and the validation batch size to 8.
To align Llama 3 using the DPO method, we followed \citet{meng2024simpo} and set the learning rate to $7e-7$ with beta of $0.01$. For SimPO, we use a learning rate of $1e-6$, beta of $2.5$, and a gamma-to-beta ratio of $0.55$. For Mistral v0.1, we set the DPO learning rate to $5e-7$ with beta of $0.001$. In SimPO, we use a learning rate of $5e-7$, beta of $2.5$, and a gamma-to-beta ratio of $0.1$.

The experiments were conducted using NVIDIA RTX A6000 GPU equipped with 48GB of RAM. The total computation time amounted to approximately 670 GPU hours.


\section{Additional Insights into Models' Reasoning}\label{appendix:additional-insights-into-models-reasoning}

In this analysis, we investigate when different models reach definitive answers. We aim to detect this commitment as early as possible during the reasoning process. This early commitment serves as a proxy for the model's confidence in the generated reasoning and its final answer. By analyzing this behavior, we explore whether models can arrive at a definitive answer or if they leave room for ambiguity or subjective interpretation.

We leverage the strong extractive capabilities of LLMs \citep{wei2023zeroshot} and their near-human-like annotation abilities \citep{gilardi2023chatgpt,alizadeh2023open}. Specifically, we focus on the Phi4 (14B) model \citep{abdin2024phi}, which demonstrates exceptional performance in question-answering and reasoning tasks, even surpassing closed-source models like GPT-4o \citep{hurst2024gpt}. To determine whether a model’s reasoning contains a definitive answer, we use the following prompt fed to Phi4:

\begin{tcolorbox}[colback=gray!10, colframe=black, fontupper=\small]
    Does the given answer directly answer the given question in a definitive way? ONLY RETURN YES OR NO IN A  \textbackslash textbf\{\}. Definitive answers are clear and do not leave room for interpretation or ambiguity. If the answer tries to explore multiple perspectives or factors involved, it is not definitive, and YOU HAVE TO RETURN NO.
\end{tcolorbox}

This prompt is applied to reasoning generated by both System 1 and System 2 models. To understand when these models commit to a definitive answer during their reasoning process, we focus on the first $n$ sentences of their reasoning, where $n \in \{1, 3, 6, 9, 12, 15\}$. We set a cap of 15 sentences based on our observations that nearly all generated reasonings across benchmarks fall within this range (see \Cref{fig:distribution-of-num-sentences-model-reasoning}).

Applying the prompt to each generated reasoning from the models across all benchmarks (200 randomly sampled data points from each benchmark, totaling 2000 samples for both System 1 and System 2 reasonings), we append six solved demonstrations to the prompt to help further guide the models. These demonstrations, selected randomly from the cognitive heuristics introduced in \Cref{sec:dataset}, help clarify what qualifies as a definitive answer, aligning the models’ knowledge with patterns we have aligned system 1 and 2 models with (see \Cref{subsec:aligning-to-system1-2-thinking}).

\Cref{fig:exp4_app} shows the proportion of definitive answers in the first n sentences, across all benchmarks.\footnote{Note that this ratio should not necessarily converge to 1.0 as more sentences are considered. In some cases, even when considering the full reasoning chain, the models may still leave room for vagueness.} For tasks where quick, intuitive judgments are advantageous, such as in commonsense reasoning. System 1 models consistently provide more definitive answers than System 2 models. This gap emerges early, with System 1 providing more definitive answers in the first three sentences. The difference persists even as we extend the number of sentences considered (see \Cref{tab:mcnemar_results} for a quantitative analysis of the significance between System 1 and System 2 regarding the definitiveness of their answers).


\input{figures/exp4_appendix}
\input{figures/distrubution_app}
\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{ 
    \caption{McNemar's test results comparing the ratio of answers providing committed and definitive responses between System 1 and System 2 across different benchmarks. Statistically significant results ($p$-value < 0.05) are boldfaced.}
    \label{tab:mcnemar_results}
    \begin{tabular}{l ccc | ccc | ccc }
        \toprule
        \multirow{2}{*}{\# Sen.} & \multicolumn{3}{c|}{Arithmetic} & \multicolumn{3}{c|}{Symbolic} & \multicolumn{3}{c}{Common Sense} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
        & $\chi^2$ & $p$-value & Winner & $\chi^2$ & $p$-value & Winner & $\chi^2$ & $p$-value & Winner \\
        \midrule
        1  & 21.0  & 1.00  & System 1 & 19.0  & .755  & System 2 & 25.0  & \textbf{.050}  & \textbf{System 1} \\
        3  & 123.0 & \textbf{.028}  & \textbf{System 2} & 29.0  & .228  & System 1 & 20.0  & \textbf{> .001}  & \textbf{System 1} \\
        6  & 125.0 & .272  & System 2 & 33.0  & .720  & System 1 & 21.0  & \textbf{> .001}  & \textbf{System 1} \\
        9  & 120.0 & \textbf{.040}  & \textbf{System 2} & 44.0  & 1.00  & System 1 & 21.0  & \textbf{> .001}  & \textbf{System 1} \\
        12 & 118.0 & .051  & System 2 & 45.0  & .320  & System 2 & 20.0  & \textbf{> .001}  & \textbf{System 1} \\
        15 & 121.0 & .069  & System 2 & 45.0  & .836  & System 1 & 20.0  & \textbf{> .001}  & \textbf{System 1} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}
\end{document}
