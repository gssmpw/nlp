@article{nassiri2023transformer,
  title={Transformer models used for text-based question answering systems},
  author={Nassiri, Khalid and Akhloufi, Moulay},
  journal={Applied Intelligence},
  volume={53},
  number={9},
  pages={10602--10635},
  year={2023},
  publisher={Springer}
}

@inproceedings{zeng2024mr,
  title={Mr-ben: A meta-reasoning benchmark for evaluating system-2 thinking in llms},
  author={Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and others},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{weston2023system,
  title={System 2 Attention (is something you might need too)},
  author={Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2311.11829},
  year={2023}
}

@article{kamruzzaman2024prompting,
  title={Prompting techniques for reducing social bias in llms through system 1 and system 2 cognitive processes},
  author={Kamruzzaman, Mahammed and Kim, Gene Louis},
  journal={arXiv preprint arXiv:2404.17218},
  year={2024}
}

@article{yu2024distilling,
  title={Distilling system 2 into system 1},
  author={Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia},
  journal={arXiv preprint arXiv:2407.06023},
  year={2024}
}

@inproceedings{echterhoff2024cognitive,
  title={Cognitive bias in decision-making with LLMs},
  author={Echterhoff, Jessica and Liu, Yao and Alessa, Abeer and McAuley, Julian and He, Zexue},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={12640--12653},
  year={2024}
}

@article{lakoff1973hedges,
  title={Hedges: A study in meaning criteria and the logic of fuzzy concepts},
  author={Lakoff, George},
  journal={Journal of philosophical logic},
  volume={2},
  number={4},
  pages={458--508},
  year={1973},
  publisher={Springer}
}

@misc{wei2023zeroshot,
      title={Zero-Shot Information Extraction via Chatting with ChatGPT}, 
      author={Xiang Wei and Xingyu Cui and Ning Cheng and Xiaobin Wang and Xin Zhang and Shen Huang and Pengjun Xie and Jinan Xu and Yufeng Chen and Meishan Zhang and Yong Jiang and Wenjuan Han},
      year={2023},
      eprint={2302.10205v1},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{byrd2022bounded,
  title={Bounded reflectivism and epistemic identity},
  author={Byrd, Nick},
  journal={Metaphilosophy},
  volume={53},
  number={1},
  pages={53--69},
  year={2022},
  publisher={Wiley Online Library}
}

@article{abdin2024phi,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}

@article{gilardi2023chatgpt,
  title={ChatGPT outperforms crowd workers for text-annotation tasks},
  author={Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={30},
  pages={e2305016120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{alizadeh2023open,
  title={Open-source large language models outperform crowd workers and approach ChatGPT in text-annotation tasks},
  author={Alizadeh, Meysam and Kubli, Ma{\"e}l and Samei, Zeynab and Dehghani, Shirin and Bermeo, Juan Diego and Korobeynikova, Maria and Gilardi, Fabrizio},
  journal={arXiv preprint arXiv:2307.02179},
  volume={42},
  year={2023},
  publisher={Technical Report}
}

@article{ott2018hedging,
  title={Hedging, weasel words, and truthiness in scientific writing},
  author={Ott, Douglas E},
  journal={JSLS: Journal of the Society of Laparoendoscopic Surgeons},
  volume={22},
  number={4},
  year={2018},
  publisher={Society of Laparoscopic \& Robotic Surgeons}
}

@article{xu2021career,
  title={Career decision-making from a dual-process perspective: Looking back, looking forward},
  author={Xu, Hui},
  journal={Journal of Vocational Behavior},
  volume={126},
  pages={103556},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{valmeekam2022large,
  title={Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}

@article{mondorf2024beyond,
  title={Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models--A Survey},
  author={Mondorf, Philipp and Plank, Barbara},
  journal={arXiv preprint arXiv:2404.01869},
  year={2024}
}

@inproceedings{parmar2024logicbench,
  title={LogicBench: Towards systematic evaluation of logical reasoning ability of large language models},
  author={Parmar, Mihir and Patel, Nisarg and Varshney, Neeraj and Nakamura, Mutsumi and Luo, Man and Mashetty, Santosh and Mitra, Arindam and Baral, Chitta},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13679--13707},
  year={2024}
}

@article{sourati2024arn,
  title={Arn: Analogical reasoning on narratives},
  author={Sourati, Zhivar and Ilievski, Filip and Sommerauer, Pia and Jiang, Yifan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={1063--1086},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{furniturewala2024thinking,
  title={Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models},
  author={Furniturewala, Shaz and Jandial, Surgan and Java, Abhinav and Banerjee, Pragyan and Shahid, Simra and Bhatia, Sumit and Jaidka, Kokil},
  journal={arXiv preprint arXiv:2405.10431},
  year={2024}
}

@article{liu2022neural,
  title={A neural-symbolic approach to natural language understanding},
  author={Liu, Zhixuan and Wang, Zihao and Lin, Yuan and Li, Hang},
  journal={arXiv preprint arXiv:2203.10557},
  year={2022}
}

@article{pan2024dynathink,
  title={DynaThink: Fast or slow? A dynamic decision-making framework for large language models},
  author={Pan, Jiabao and Zhang, Yan and Zhang, Chen and Liu, Zuozhu and Wang, Hongwei and Li, Haizhou},
  journal={arXiv preprint arXiv:2407.01009},
  year={2024}
}

@article{hagendorff2023human,
  title={Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT},
  author={Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
  journal={Nature Computational Science},
  volume={3},
  number={10},
  pages={833--838},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@inproceedings{booch2021thinking,
  title={Thinking fast and slow in AI},
  author={Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jonathan and Linck, Nick and Loreggia, Andreas and Murgesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={15042--15046},
  year={2021}
}

@article{hagendorff2023thinking,
  title={Thinking fast and slow in large language models},
  author={Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
  journal={arXiv preprint arXiv:2212.05206},
  year={2023}
}

@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{kahneman2013perspective,
  title={A perspective on judgment and choice: Mapping bounded rationality},
  author={Kahneman, Daniel},
  journal={Progress in Psychological Science around the World. Volume 1 Neural, Cognitive and Developmental Issues.},
  pages={1--47},
  year={2013},
  publisher={Psychology Press}
}

@article{wason1974dual,
  title={Dual processes in reasoning?},
  author={Wason, Peter C and Evans, J St BT},
  journal={Cognition},
  volume={3},
  number={2},
  pages={141--154},
  year={1974},
  publisher={Elsevier}
}

@article{gan2024reasoning,
  title={Reasoning robustness of llms to adversarial typographical errors},
  author={Gan, Esther and Zhao, Yiran and Cheng, Liying and Mao, Yancan and Goyal, Anirudh and Kawaguchi, Kenji and Kan, Min-Yen and Shieh, Michael},
  journal={arXiv preprint arXiv:2411.05345},
  year={2024}
}

@article{yax2024studying,
  title={Studying and improving reasoning in humans and machines},
  author={Yax, Nicolas and Anll{\'o}, Hernan and Palminteri, Stefano},
  journal={Communications Psychology},
  volume={2},
  number={1},
  pages={51},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{webb2023emergent,
  title={Emergent analogical reasoning in large language models},
  author={Webb, Taylor and Holyoak, Keith J and Lu, Hongjing},
  journal={Nature Human Behaviour},
  volume={7},
  number={9},
  pages={1526--1541},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{rogers2023qa,
  title={Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension},
  author={Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
  journal={ACM Computing Surveys},
  volume={55},
  number={10},
  pages={1--45},
  year={2023},
  publisher={ACM New York, NY}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2024chain,
  title={Chain-of-thought reasoning without prompting},
  author={Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.10200},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhou2024self,
  title={Self-discover: Large language models self-compose reasoning structures},
  author={Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
  journal={arXiv preprint arXiv:2402.03620},
  year={2024}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{Singhal2023ALW,
  title={A Long Way to Go: Investigating Length Correlations in RLHF},
  author={Prasann Singhal and Tanya Goyal and Jiacheng Xu and Greg Durrett},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03716},
  url={https://api.semanticscholar.org/CorpusID:263672200}
}

@book{kahneman2011thinking,
  abstract = {In this work the author, a recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology that challenged the rational model of judgment and decision making, has brought together his many years of research and thinking in one book. He explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. He exposes the extraordinary capabilities, and also the faults and biases, of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. He reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives, and how we can use different techniques to guard against the mental glitches that often get us into trouble. This author's work has transformed cognitive psychology and launched the new fields of behavioral economics and happiness studies. In this book, he takes us on a tour of the mind and explains the two systems that drive the way we think and the way we make choices.},
  added-at = {2013-01-10T15:41:11.000+0100},
  address = {New York},
  author = {Kahneman, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2f322864169411fd5914f3fa5488e288c/schmidt2},
  description = {Thinking, Fast and Slow: Amazon.de: Daniel Kahneman: Englische BÃ¼cher},
  interhash = {a1400a299a00de009ec8eda73e6289af},
  intrahash = {f322864169411fd5914f3fa5488e288c},
  isbn = {9780374275631 0374275637},
  keywords = {bib books psychology thinking toread},
  publisher = {Farrar, Straus and Giroux},
  refid = {706020998},
  timestamp = {2013-01-10T15:41:11.000+0100},
  title = {Thinking, fast and slow},
  url = {https://www.amazon.de/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie=UTF8&colid=151193SNGKJT9&coliid=I3OCESLZCVDFL7},
  year = 2011
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@article{yang2024llm2,
  title={LLM2: Let Large Language Models Harness System 2 Reasoning},
  author={Yang, Cheng and Shi, Chufan and Li, Siheng and Shui, Bo and Yang, Yujiu and Lam, Wai},
  journal={arXiv preprint arXiv:2412.20372},
  year={2024}
}

@article{deng2024cognidual,
  title={CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks},
  author={Deng, Yongxin and Qiu, Xihe and Tan, Xiaoyu and Qu, Chao and Pan, Jing and Cheng, Yuan and Xu, Yinghui and Chu, Wei},
  journal={arXiv preprint arXiv:2409.03381},
  year={2024}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{jin2024comprehensive,
  title={A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods},
  author={Jin, Hanlei and Zhang, Yang and Meng, Dan and Wang, Jun and Tan, Jinghua},
  journal={arXiv preprint arXiv:2403.02901},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734},
  year={2024}
}

@misc{kong2024betterzeroshotreasoningroleplay,
      title={Better Zero-Shot Reasoning with Role-Play Prompting}, 
      author={Aobo Kong and Shiwan Zhao and Hao Chen and Qicheng Li and Yong Qin and Ruiqi Sun and Xin Zhou and Enzhi Wang and Xiaohang Dong},
      year={2024},
      eprint={2308.07702},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.07702}, 
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@article{10.1162/tacl_a_00160,
    author = {Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena
                            Dumas},
    title = {Parsing Algebraic Word Problems into Equations},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {3},
    pages = {585-597},
    year = {2015},
    month = {12},
    abstract = {This paper formalizes the problem of solving multi-sentence algebraic word
                    problems as that of generating and scoring equation trees. We use integer linear
                    programming to generate equation trees and score their likelihood by learning
                    local and global discriminative models. These models are trained on a small set
                    of word problems and their answers, without any manual annotation, in order to
                    choose the equation that best matches the problem text. We refer to the overall
                    system as Alges.We compare Alges with previous work and show that it covers the full
                    gamut of arithmetic operations whereas Hosseini et al. (2014) only handle
                    addition and subtraction. In addition, Alges overcomes the brittleness
                    of the Kushman et al. (2014) approach on single-equation problems, yielding a
                    15\% to 50\% reduction in error.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00160},
    url = {https://doi.org/10.1162/tacl\_a\_00160},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00160/1566814/tacl\_a\_00160.pdf},
}

@article{10.1162/tacl_a_00370,
    author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
    title = {Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {346-361},
    year = {2021},
    month = {04},
    abstract = {A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87\%) on this task, while our best baseline reaches an accuracy of âˆ¼ 66\%.},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00370},
    url = {https://doi.org/10.1162/tacl\_a\_00370},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00370/1924104/tacl\_a\_00370.pdf},
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{huang2022large,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}

@article{magister2022teaching,
  title={Teaching small language models to reason},
  author={Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
  journal={arXiv preprint arXiv:2212.08410},
  year={2022}
}

@article{mirzadeh2024gsm,
  title={Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2410.05229},
  year={2024}
}

@inproceedings{deletang2023neural,
  author       = {Gr{\'{e}}goire Del{\'{e}}tang and
                  Anian Ruoss and
                  Jordi Grau{-}Moya and
                  Tim Genewein and
                  Li Kevin Wenliang and
                  Elliot Catt and
                  Chris Cundy and
                  Marcus Hutter and
                  Shane Legg and
                  Joel Veness and
                  Pedro A. Ortega},
  title        = {Neural Networks and the Chomsky Hierarchy},
  booktitle    = {11th International Conference on Learning Representations},
  year         = {2023},
}

@article{radhakrishnan2023question,
  title={Question decomposition improves the faithfulness of model-generated reasoning},
  author={Radhakrishnan, Ansh and Nguyen, Karina and Chen, Anna and Chen, Carol and Denison, Carson and Hernandez, Danny and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and others},
  journal={arXiv preprint arXiv:2307.11768},
  year={2023}
}

@article{llama3modelcard,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}


@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and LÃ©lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and TimothÃ©e Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}


@article{stanovich2000advancing,
  title={Advancing the rationality debate},
  author={Stanovich, Keith E and West, Richard F},
  journal={Behavioral and brain sciences},
  volume={23},
  number={5},
  pages={701--717},
  year={2000},
  publisher={Cambridge University Press}
}

@article{evans2013dual,
  title={Dual-process theories of higher cognition: Advancing the debate},
  author={Evans, Jonathan St BT and Stanovich, Keith E},
  journal={Perspectives on psychological science},
  volume={8},
  number={3},
  pages={223--241},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}


@article{tversky1974judgment,
  title={Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty.},
  author={Tversky, Amos and Kahneman, Daniel},
  journal={science},
  volume={185},
  number={4157},
  pages={1124--1131},
  year={1974},
  publisher={American association for the advancement of science}
}

@misc{kojima2023largelanguagemodelszeroshot,
      title={Large Language Models are Zero-Shot Reasoners}, 
      author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
      year={2023},
      eprint={2205.11916},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.11916}, 
}

@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@article{keramati2011speed,
  title={Speed/accuracy trade-off between the habitual and the goal-directed processes},
  author={Keramati, Mehdi and Dezfouli, Amir and Piray, Payam},
  journal={PLoS computational biology},
  volume={7},
  number={5},
  pages={e1002055},
  year={2011},
  publisher={Public Library of Science San Francisco, USA}
}

@article{mattar2018prioritized,
  title={Prioritized memory access explains planning and hippocampal replay},
  author={Mattar, Marcelo G and Daw, Nathaniel D},
  journal={Nature neuroscience},
  volume={21},
  number={11},
  pages={1609--1617},
  year={2018},
  publisher={Nature Publishing Group US New York}
}

@article{daw2005uncertainty,
  title={Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control},
  author={Daw, Nathaniel D and Niv, Yael and Dayan, Peter},
  journal={Nature neuroscience},
  volume={8},
  number={12},
  pages={1704--1711},
  year={2005},
  publisher={Nature Publishing Group US New York}
}

@article{lee2014neural,
  title={Neural computations underlying arbitration between model-based and model-free learning},
  author={Lee, Sang Wan and Shimojo, Shinsuke and Oâ€™doherty, John P},
  journal={Neuron},
  volume={81},
  number={3},
  pages={687--699},
  year={2014},
  publisher={Elsevier}
}

@article{dolan2013goals,
  title={Goals and habits in the brain},
  author={Dolan, Ray J and Dayan, Peter},
  journal={Neuron},
  volume={80},
  number={2},
  pages={312--325},
  year={2013},
  publisher={Elsevier}
}

@article{balleine1998goal,
  title={Goal-directed instrumental action: contingency and incentive learning and their cortical substrates},
  author={Balleine, Bernard W and Dickinson, Anthony},
  journal={Neuropharmacology},
  volume={37},
  number={4-5},
  pages={407--419},
  year={1998},
  publisher={Elsevier}
}

@article{piray2021linear,
  title={Linear reinforcement learning in planning, grid fields, and cognitive control},
  author={Piray, Payam and Daw, Nathaniel D},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={4942},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{daw2011model,
  title={Model-based influences on humans' choices and striatal prediction errors},
  author={Daw, Nathaniel D and Gershman, Samuel J and Seymour, Ben and Dayan, Peter and Dolan, Raymond J},
  journal={Neuron},
  volume={69},
  number={6},
  pages={1204--1215},
  year={2011},
  publisher={Elsevier}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}