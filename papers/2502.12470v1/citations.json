[
  {
    "index": 0,
    "papers": [
      {
        "key": "huang2022towards",
        "author": "Huang, Jie and Chang, Kevin Chen-Chuan",
        "title": "Towards reasoning in large language models: A survey"
      },
      {
        "key": "mondorf2024beyond",
        "author": "Mondorf, Philipp and Plank, Barbara",
        "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models--A Survey"
      },
      {
        "key": "valmeekam2022large",
        "author": "Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao",
        "title": "Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)"
      },
      {
        "key": "parmar2024logicbench",
        "author": "Parmar, Mihir and Patel, Nisarg and Varshney, Neeraj and Nakamura, Mutsumi and Luo, Man and Mashetty, Santosh and Mitra, Arindam and Baral, Chitta",
        "title": "LogicBench: Towards systematic evaluation of logical reasoning ability of large language models"
      },
      {
        "key": "sourati2024arn",
        "author": "Sourati, Zhivar and Ilievski, Filip and Sommerauer, Pia and Jiang, Yifan",
        "title": "Arn: Analogical reasoning on narratives"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      },
      {
        "key": "wang-etal-2023-plan",
        "author": "Wang, Lei  and\nXu, Wanyu  and\nLan, Yihuai  and\nHu, Zhiqiang  and\nLan, Yunshi  and\nLee, Roy Ka-Wei  and\nLim, Ee-Peng",
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models"
      },
      {
        "key": "zhou2024self",
        "author": "Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven",
        "title": "Self-discover: Large language models self-compose reasoning structures"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2024chain",
        "author": "Wang, Xuezhi and Zhou, Denny",
        "title": "Chain-of-thought reasoning without prompting"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2022self",
        "author": "Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny",
        "title": "Self-consistency improves chain of thought reasoning in language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yao2024tree",
        "author": "Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik",
        "title": "Tree of thoughts: Deliberate problem solving with large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chung2024scaling",
        "author": "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others",
        "title": "Scaling instruction-finetuned language models"
      },
      {
        "key": "huang2022large",
        "author": "Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei",
        "title": "Large language models can self-improve"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "magister2022teaching",
        "author": "Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei",
        "title": "Teaching small language models to reason"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hagendorff2023human",
        "author": "Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal",
        "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT"
      },
      {
        "key": "booch2021thinking",
        "author": "Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jonathan and Linck, Nick and Loreggia, Andreas and Murgesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and others",
        "title": "Thinking fast and slow in AI"
      },
      {
        "key": "pan2024dynathink",
        "author": "Pan, Jiabao and Zhang, Yan and Zhang, Chen and Liu, Zuozhu and Wang, Hongwei and Li, Haizhou",
        "title": "DynaThink: Fast or slow? A dynamic decision-making framework for large language models"
      },
      {
        "key": "echterhoff2024cognitive",
        "author": "Echterhoff, Jessica and Liu, Yao and Alessa, Abeer and McAuley, Julian and He, Zexue",
        "title": "Cognitive bias in decision-making with LLMs"
      },
      {
        "key": "zeng2024mr",
        "author": "Zeng, Zhongshen and Liu, Yinhong and Wan, Yingjia and Li, Jingyao and Chen, Pengguang and Dai, Jianbo and Yao, Yuxuan and Xu, Rongwu and Qi, Zehan and Zhao, Wanru and others",
        "title": "Mr-ben: A meta-reasoning benchmark for evaluating system-2 thinking in llms"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hagendorff2023human",
        "author": "Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal",
        "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "booch2021thinking",
        "author": "Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jonathan and Linck, Nick and Loreggia, Andreas and Murgesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and others",
        "title": "Thinking fast and slow in AI"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "he-etal-2024-planning",
        "author": "He, Tao  and\nLiao, Lizi  and\nCao, Yixin  and\nLiu, Yuanxing  and\nLiu, Ming  and\nChen, Zerui  and\nQin, Bing",
        "title": "Planning Like Human: A Dual-process Framework for Dialogue Planning"
      },
      {
        "key": "liu2022neural",
        "author": "Liu, Zhixuan and Wang, Zihao and Lin, Yuan and Li, Hang",
        "title": "A neural-symbolic approach to natural language understanding"
      },
      {
        "key": "hua-zhang-2022-system",
        "author": "Hua, Wenyue  and\nZhang, Yongfeng",
        "title": "System 1 + System 2 = Better World: Neural-Symbolic Chain of Logic Reasoning"
      },
      {
        "key": "pan2024dynathink",
        "author": "Pan, Jiabao and Zhang, Yan and Zhang, Chen and Liu, Zuozhu and Wang, Hongwei and Li, Haizhou",
        "title": "DynaThink: Fast or slow? A dynamic decision-making framework for large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yang2024llm2",
        "author": "Yang, Cheng and Shi, Chufan and Li, Siheng and Shui, Bo and Yang, Yujiu and Lam, Wai",
        "title": "LLM2: Let Large Language Models Harness System 2 Reasoning"
      },
      {
        "key": "deng2024cognidual",
        "author": "Deng, Yongxin and Qiu, Xihe and Tan, Xiaoyu and Qu, Chao and Pan, Jing and Cheng, Yuan and Xu, Yinghui and Chu, Wei",
        "title": "CogniDual Framework: Self-Training Large Language Models within a Dual-System Theoretical Framework for Improving Cognitive Tasks"
      },
      {
        "key": "yu2024distilling",
        "author": "Yu, Ping and Xu, Jing and Weston, Jason and Kulikov, Ilia",
        "title": "Distilling system 2 into system 1"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "furniturewala2024thinking",
        "author": "Furniturewala, Shaz and Jandial, Surgan and Java, Abhinav and Banerjee, Pragyan and Shahid, Simra and Bhatia, Sumit and Jaidka, Kokil",
        "title": "Thinking Fair and Slow: On the Efficacy of Structured Prompts for Debiasing Language Models"
      },
      {
        "key": "kamruzzaman2024prompting",
        "author": "Kamruzzaman, Mahammed and Kim, Gene Louis",
        "title": "Prompting techniques for reducing social bias in llms through system 1 and system 2 cognitive processes"
      },
      {
        "key": "weston2023system",
        "author": "Weston, Jason and Sukhbaatar, Sainbayar",
        "title": "System 2 Attention (is something you might need too)"
      }
    ]
  }
]