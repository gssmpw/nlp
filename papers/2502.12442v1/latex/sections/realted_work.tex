\section{Related Work} 
\label{sec:RelatedWork}
\paragraph{Retrieval-Augmented Generation}
Retrieval-augmented generation significantly improves large language models by incorporating a retrieval module that fetches relevant information from external knowledge sources \citep{EaE, REALM, FID, survey, yu2025mramg}. Retrieval models have evolved from early sparse retrievers, such as TF-IDF \citep{TF-IDF} and BM25 \citep{BM25}, which rely on word statistics and inverted indices, to dense retrievers \citep{2020RAG} that utilize neural representations for semantic matching. Advanced methods, such as Self-RAG \citep{Self-RAG} and FLARE~\citep{FLARE} which determine the necessity and timing of retrieval, represent significant developments. However, the knowledge index remains logically unstructured, with each round of search considering only lexical or semantic similarity.
% , and RAG-end2end \citep{RAG-end2end} that jointly trains the retriever and generator, 

%In RAG frameworks, the retrieval module plays a pivotal role as it directly determines the relevance and quality of the retrieved information. Retrieval methods are typically divided into sparse and dense retrieval. Sparse methods like TF-IDF and BM25 rely on keyword matching and inverted indices but often fail to capture deeper semantic relations. In contrast, Dense retrieval, encodes documents and queries into dense vector representations , enabling the capture of semantic meanings beyond surface-level keywords, which is crucial for complex question answering tasks.
%Dense retrievers encode text into vector representations, with relevance scores between a document $c$ and a query $q$ commonly computed using similarity measures  such as cosine similarity in the embedding space.
%A good embedding is essential for an effective dense retriever, as it allows semantically similar content to be closer within the vector space.

%Specifically, RETRO~\citep{RETRO} employs attention mechanisms to integrate retrieved documents into the model’s reasoning process, aligning question representations with relevant content to generate accurate answers. MultiHop-RAG extracts and combines information from multiple documents, constructing a unified context that equips the generator to respond accurately to complex queries.

%RAPTOR~\citep{RAPTOR} enhances RAG by focusing on chunk optimization, utilizing iterative embedding, clustering, and summarization to refine the chunking process.
%Rangan~\citep{rangan}  leverages the Quantized Influence Measure to assess statistical biases between a query and its reference, using this analysis primarily to rerank retrieval results based on data subset similarity. FLARE~\citep{FLARE}  dynamically determines the necessity and timing of retrieval based on probabilistic assessments during the generation process.

% Dense retrieval version of naive RAG stores the vector representation of text chunks as the index and retrieves through similarity ranking\cite{lewis_retrieval-augmented_2021}. Subsequent improvements over logical relation among chunks include the use of tree structure\cite{sarthi_raptor_2024,chen_walking_2023,fatehkia2024traglessonsllmtrenches} or graph structure\cite{guo_lightrag_2024,rappazzo_gem-rag_2024,liang_empowering_2024,10.1609/aaai.v38i17.29889,zhang_graph_2024} for knowledge index, so as to connect different chunks and improve the performance of retrieval.


\paragraph{Tree\&Graph-structured RAG}
Tree and graph are both effective structures for modeling logical relations.
% Tree-structured RAG build RAG systems with Tree-structured knowledge index.
RAPTOR~\citep{RAPTOR} recursively embeds, clusters, and summarizes passages, constructing a tree with differing levels of summarization from the bottom up.
MemWalker~\citep{chen_walking_2023} treats the LLM as an interactive agent walking on the tree of summarization. SiReRAG~\citep{zhang2024sireragindexingsimilarrelated} explicitly considers both similar and related information by constructing both similarity tree and relatedness tree. 
PG-RAG~\citep{PG-RAG} prompts LLMs to organize document knowledge into mindmaps, and unifies them for multiple documents. 
GNN-RAG~\citep{GNN-RAG} reasons over dense KG subgraphs with learned GNNs to retrieve answer candidates.
For query-focused summarization, GraphRAG~\citep{GraphRAG} builds a hierarchical graph index with knowledge graph construction and recursive summarization. 
Despite advancements, tree-structured RAG only focuses on the hierarchical logic within a single document; graph-structured RAG is costly, time-consuming, and returns triplets instead of plain text. 
In contrast, HopRAG offers a more lightweight and downstream task friendly alternative, with flexible logical modeling, cross-document organization, efficient construction and updating.
% EtD~\citep{EtD} features a graph neural network (GNN) to traverse a knowledge graph hop by hop to discover more relevant knowledge, thus enhancing LLM generation quality. 
% GNN-RAG~\citep{GNN-RAG} learns to reason over graphs using GNNs, and retrieves answer candidates for a given question.
% G-RAG~\citep{G-RAG} proposes to rerank documents by learning graph representation on abstract meaning representation graphs, while GNN-Ret~\citep{GNN-Ret} refines semantic distances between documents and queries by modeling relations among related passages. 
% ToG~\citep{ToG} and KGP~\citep{KGP} treat LLMs as agents to traverse and reason over knowledge graphs in an iterative way, while RoG~\citep{RoG} first generates plans for retrieval and then conducts reasoning. 
% G-Retriever~\citep{G-retriever} transforms retrieved knowledge subgraphs into graph embeddings by training a graph encoder and textualizes subgraphs to serve as inputs of LLMs. 






% Besides, its usage of predefined relations for entity linking and sub-graph matching\cite{liang_empowering_2024} makes it less suitable for query that requires comprehensively connecting multiple entities for reasoning and generalization\cite{li_graph_2024}. Furthermore, textualizing or fine-tuning\cite{he_g-retriever_2024} might be needed to enhance the graph understanding ability of LLM. 


% We observe that imperfect retrieval augmentation is widespread even with adept real-world search engine (such as Google Search with Web as corpus) – roughly 70\% retrieved passages do not directly contain true answers, leading to the impeded performance of LLM with RAG augmentation.\footnote{Note that some passages may contain information indirectly relevant to the answer, but may unintentionally mislead or distract LLMs.}

% Sparse retrievers \citep{TF-IDF,BM25}, focus on lexical similarity and offer advantages in terms of efficiency and interpretability. Dense retrievers \citep{BGE,e52024multilingual,sturua2024jina,wang_qaencoder_2024}, focus on semantic similarity and enable more precise retrieval by mapping queries and documents into semantic embeddings.


