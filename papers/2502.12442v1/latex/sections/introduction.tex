\section{Introduction}
\label{sec:introduction}
\label{introduction}

\begin{quote}
    \textit{``Everyone and everything is six or fewer steps away, by way of introduction, from any other person in the world.''} 
\begin{flushright}
\qquad\;\;\, --- Six Degrees of Separation
\end{flushright}
\end{quote}
Retrieval-augmented generation (RAG) has become the standard approach for large language models (LLMs) to tackle knowledge-intensive tasks \citep{guu2020retrieval,lewis2020retrieval, izacard2022few, min-etal-2023-nonparametric, ram2023context,liang2025saferag}. Not only can it effectively address the inherent knowledge limitations and hallucination issues \cite{zhang_sirens_2023}, but it can also enable easy interpretability and provenance tracking \citep{akyurek-etal-2022-towards}. Especially, the efficacy of RAG hinges on its retrieval module for identifying relevant documents from a vast corpus.


Currently, there are two mainstream types of retrievers: sparse retrievers \citep{TF-IDF,BM25} and dense retrievers \citep{BGE,e52024multilingual,sturua2024jina,wang_qaencoder_2024}, which focus on lexical similarity and semantic similarity respectively, and are often combined for better retrieval performance \citep{sawarkar2024blended}.
Despite advancements, the ultimate goal of information retrieval extends beyond lexical and semantic similarity, striving instead for \textit{logical relevance}. Due to the lack of logic-aware mechanism, the imperfect retrieval remains prominent \citep{wang2024astute, shao2024scaling, dai2024unifying, su2024bright, su2024brightrealisticchallengingbenchmark}. For precision, the retrieval system may return lexically and semantically similar but indirectly relevant passages; regarding recall, it may fail to retrieve all the necessary passages for the user query. 

Both cases eventually lead to inaccurate or incomplete LLM responses \citep{chen2024benchmarking,xiang2024certifiably,zou2024poisonedrag}, especially for multi-hop or multi-document QA tasks requiring multiple relevant passages for the final answer.  
In contrast, the reasoning capability of generative models is rapidly advancing, with notable examples such as openai-o1 \citep{jaech2024openai} and deepseek-r1 \citep{guo2025deepseek}. Therefore, a natural research question arises: \textit{"Is it possible to introduce reasoning capability into the retrieval module for more advanced RAG systems?"} 

\begin{figure*}[ht]
\centering
\subfigure[Precision, recall and F1 score]{\label{chunk precision and recall}\includegraphics[width=1\columnwidth]{latex/figures/precision_recall.png}}
\subfigure[Proportions of passages on relevance]
{\label{relevance}\includegraphics[width=1\columnwidth]{latex/figures/combined_relevant.png}}
\caption{(a) Precision, recall and F1 score of BGE dense retrievers on MuSiQue, 2WikiMultiHopQA and HotpotQA with different $top_k$ parameters, revealing the severe imperfect retrieval phenomenon. The highest recall reaches saturation at 0.45 in our settings.
(b) We categorize retrieved passages into \textbf{relevant}, \textbf{indirectly relevant} and \textbf{irrelevant} according to the logical relevance to the query. The relevant passages are exactly the supporting facts, and indirectly relevant passages can hop to the supporting facts via HopRAG while irrelevant passages cannot. A large proportion of retrieved passages are indirectly relevant. 
}\label{motivation_figure}
\end{figure*}


From a logical structure perspective, existing RAG systems can be mainly categorized into three types: 
\textbf{Non-structured RAG} simply adopts sparse or dense retrievers. The retrieval is only based on keyword matching or semantic vector similarity, but fails to capture the logical relations between user queries and passages.
\textbf{Tree-structured RAG} \citep{sarthi_raptor_2024, chen_walking_2023, fatehkia2024traglessonsllmtrenches} focuses on the hierarchical logic of passages within a single document, but ignores relations beyond the hierarchical structure or across documents. Additionally, it introduces redundant information across different levels.
\textbf{Graph-structured RAG} \citep{soman2024biomedicalknowledgegraphoptimizedprompt,kang2023knowledgegraphaugmentedlanguagemodels,edge_local_2024,guo_lightrag_2024} models logical relations in the most ideal form by constructing knowledge graphs (KGs) to represent documents, where entities are vertices and their relations are edges. However, the reliance on predefined schemas limits the flexible expressive capability \citep{li_graph_2024}; constructing and updating knowledge graphs is challenging and prone to errors or omissions \citep {edge_local_2024}; the triplet format of knowledge necessitates extra textualization or fine-tuning to improve LLMs' understanding \citep{he_g-retriever_2024}.


\begin{figure}[t]
  \includegraphics[width=\columnwidth]{latex/figures/fig2.pdf}
  \caption{Demonstration of hopping between passages. For the user query, BGE dense retriever can only return one of the three supporting facts within $top_k$ budget. However, lexically or semantically similar passages complement each other.
  Hopping between passages, by questions as pathways, improves the retrieval accuracy and completeness.}
  \label{fig:demo}
\end{figure}

\paragraph{Motivation} As reported by \citep{wang2024astute}, even with advanced real-world search engines, roughly 70\% retrieved passages do not directly contain true answers in their settings. We confirm the severity of imperfect retrieval in terms of both precision and recall, as illustrated in Figure \ref{motivation_figure}(a). Inspired by the small-world theory \citep{kleinberg2000small} or six degrees of separation \citep{guare2016six}, we propose that, \textit{although lexically and semantically similar passages could be indirectly relevant or even distracting, they can serve as helpful starting points to reach truly relevant ones}. As shown in Figure \ref{motivation_figure}(b), considering a graph composed of passages with logical relations as edges, a large proportion of retrieved passages fall within several hops of the ground truths. 

Based on these observations, we propose \textbf{HopRAG}, an innovative graph-structured RAG system. At indexing phase, we construct a graph-structured knowledge index with passages as vertices and logic relations as directed edges. Specifically, the passages are connected by pseudo-queries generated by \textit{query simulation} and \textit{edge merging} operations. For example, as demonstrated in Figure \ref{fig:demo}, the pseudo-query "What does the frog do?" connects the raiser passage and the solver passage, as the pivot for logical hops. During retrieval, we employ reasoning-augmented graph traversal, following a three-step paradigm of retrieval, reasoning, and pruning. This process searches for truly relevant passages within the multi-hop neighborhood of indirectly relevant passages, guided by both the index structure and LLM reasoning.

\paragraph{Contributions}
Our contributions are as follows:
\begin{itemize}
\item We reveal the severe imperfect retrieval phenomenon for multi-hop QA tasks. The results quantify that currently over 60\% of retrieved passages are indirectly relevant or irrelevant. To turn "trash" into "treasure", we further employ indirectly relevant passages as stepping stones to reach truly relevant ones.
\item We propose HopRAG, a novel RAG system with logic-aware retrieval mechanism.
As lexically or semantically similar passages complement each other, HopRAG connects the raiser and solver passages with pseudo-queries. Beyond similarity-based retrieval, it reasons and prunes along the queries during retrieval. It also features flexible logical modeling, cross-document organization, efficient construction and updating.
\item Extensive experiments confirm the effectiveness of HopRAG. The retrieve-reason-prune mechanism achieves over 76.78\% higher answer metric and 65.07\% higher retrieval F1 score compared to conventional RAG approaches. Several ablation studies provide more valuable insights.
\end{itemize}







% , whereas the parametric knowledge of LLMs is opaque and difficult to trace back to its source \citep{akyurek-etal-2022-towards}.
% These limitations highlight the urgent need for an innovative RAG system with more flexible logical modeling, cross-document organization, efficient construction and updating, and convenient knowledge format.

 