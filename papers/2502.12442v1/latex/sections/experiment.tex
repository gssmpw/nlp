\section{Experiments}


\begin{table*}[htb]
    \centering
\resizebox{0.7\textwidth}{!}{
      \begin{tabular}{ccccccccc}
        \toprule
        & \multicolumn{2}{c}{MuSiQue}& \multicolumn{2}{c}{2Wiki}& \multicolumn{2}{c}{HotpotQA}& \multicolumn{2}{c}{Average}\\
        
        Method& EM & F1  & EM & F1  & EM & F1   & EM &F1   \\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        BM25
& 5.80 & 11.00 
 & 27.00 & 31.55  & 33.40 & 44.30 
  & 22.07 &28.95  \\
        BGE
& 11.80 & 18.60  & 27.90 & 30.80  & 38.40 & 50.56   & 26.03 &33.32  \\
 HippoRAG
& 32.60 & 43.78  & \textbf{66.40}& \textbf{74.01}& 59.90 &74.29 
  & 52.97 &64.03 
 \\
 RAPTOR
& 35.30 & 47.47  & 54.90 & 61.20  & 58.10 &72.48 
  & 49.43 &60.38 
 \\
 SiReRAG& \underline{38.90}& \underline{52.08} & 60.40 & 68.20  & \textbf{62.50}&\underline{77.36}  & \underline{53.93}&\underline{65.88} \\ 
        \midrule
        HopRAG&  \textbf{39.10}&  \textbf{53.00} &  \underline{61.60}&  \underline{68.93}&  \underline{61.30}&  \textbf{78.34}  & \textbf{54.00}&\textbf{66.76} \\
        \bottomrule
    \end{tabular}
}
    \caption{We test our HopRAG against a series of baselines on multiple datasets using GPT-4o and GPT-3.5-turbo as the inference model with top 20 passages. We report the QA performance metrics EM and F1 score with GPT-3.5-turbo here and GPT-4o in Table \ref{tab:QA-performance-4o}, where the best score is in \textbf{bold} and the second best is \underline{underlined}.}
    \label{tab:QA-performance-3.5}
\end{table*}

\begin{table*}[htb]
    \centering
    \resizebox{0.7\textwidth}{!}{
    \begin{tabular}{ccccccccc}
        \toprule
        & \multicolumn{2}{c}{MuSiQue} & \multicolumn{2}{c}{2Wiki} & \multicolumn{2}{c}{HotpotQA} & \multicolumn{2}{c}{Average}\\
        
        Method& EM & F1 & EM & F1 & EM & F1  & EM &F1  \\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        BM25& 13.80 & 21.50 
& 40.30 & 44.83 
& 41.20 & 53.23 
 & 31.77 &39.85 \\
        BGE& 20.80 & 30.10 & 40.10 & 44.96 & 47.60 & 60.36  & 36.17 &45.14 \\
 GraphRAG & 12.10 & 20.22 & 22.50 & 27.49 & 31.70 &42.74 
 & 22.10 &30.15 
\\
 RAPTOR & 36.40 & 49.09 & 53.80 & 61.45 & 58.00 &73.08  & 49.40 &61.21 
\\
 SiReRAG& \underline{40.50}& \underline{53.08}& \underline{59.60}& \underline{67.94}& \underline{61.70}&\textbf{76.48} & \underline{53.93}&\underline{65.83}\\ 
        \midrule
        HopRAG&  \textbf{42.20}&  \textbf{54.90}&  \textbf{61.10}&  \textbf{68.26}&  \textbf{62.00}&  \underline{76.06} & \textbf{55.10}&\textbf{66.40}\\
        \bottomrule
    \end{tabular}
    }
    \caption{We report the QA performance metrics EM and F1 score with GPT-4o and top 20 passages here,  where the best score is in \textbf{bold} and the second best is \underline{underlined}.}
    \label{tab:QA-performance-4o}
\end{table*}

%
\label{sec:experiment}
\subsection{Experimental Setups}
\paragraph{Datasets}
We collect several multiple-hop QA datasets to evaluate the performance of HopRAG. We use HotpotQA dataset \citep{yang2018hotpotqadatasetdiverseexplainable}, 2WikiMultiHopQA dataset \citep{ho2020constructingmultihopqadataset} and MuSiQue dataset  \citep{trivedi2022musiquemultihopquestionssinglehop}. Following the same procedure as \citep{zhang2024sireragindexingsimilarrelated}, we obtain 1000 questions from each validation set of these three datasets. 
% These datasets measure the systematic ability to conduct multi-hop among passages during retrieval and reason over their logic to answer fact-based questions. 
See Appendix \ref{dataset} for details.
\paragraph{Baselines}
We compare HopRAG with a variety of baselines: (1) unstructured RAG - sparse retriever BM25 \citep{robertson_probabilistic_2009} 
 (2) unstructured RAG - dense retriever BGE \citep{BGE,karpukhin2020densepassageretrievalopendomain} (3) tree-structured RAG - RAPTOR \citep{sarthi_raptor_2024} (4) tree-structured RAG - SiReRAG \citep{zhang2024sireragindexingsimilarrelated} (5) graph-structured RAG - GraphRAG \citep{edge_local_2024} with the local search function (6) graph-structured RAG - HippoRAG \citep{guti√©rrez2025hipporagneurobiologicallyinspiredlongterm}. For structured RAG baselines, we follow the same setting as previous work \citep{zhang2024sireragindexingsimilarrelated}. 

\paragraph{Metrics}
We adopt exact match (EM) and F1 scores on answers to measure the QA performance of different methods, which focus on the accuracy between a generated answer and the corresponding ground truth. We do not use retrieval metrics to compare all the methods because some baseline methods like SiReRAG \citep{zhang2024sireragindexingsimilarrelated} and RAPTOR \citep{sarthi_raptor_2024} create new candidates (e.g., summary nodes) in the retrieval pool and it would be unfair to use retrieval metrics to compare the methods. But we report both the answer metrics and the retrieval metrics in the ablation study when comparing the effects of different hyperparameters and reasoning models on HopRAG. See Appendix \ref{metric} for more metric details.
\paragraph{Settings}
We adopt the BGE embedding model for semantic vectors at 768 dimensions. To avoid the loss of semantic information caused by chunking at a fixed size, we adopt the same chunking methods utilized in the original datasets respectively. 
%at the sentence level using periods as delimiters and later turn these passages into nodes. 
GPT-4o-mini serves as both the model generating in-coming and out-coming questions when constructing the graph index, and the reasoning model for graph traversal. We use two reader models GPT-4o and GPT-3.5-turbo to generate the response given the context with the fixed size of 20 retrieval candidates and at most $n_{hop}=4$. See Appendix \ref{setup} for more setting details. 
\subsection{Main Results}
\begin{table*}[htb]
\centering
\setlength{\tabcolsep}{3pt}
\resizebox{0.9\textwidth}{!}{
      \begin{tabular}{ccccccccccccc}
    \toprule
 & \multicolumn{3}{c}{MuSiQue}& \multicolumn{3}{c}{2Wiki}& \multicolumn{3}{c}{HotpotQA} & \multicolumn{3}{c}{Average}\\
 & \multicolumn{2}{c}{Answer}&Retrieval& \multicolumn{2}{c}{Answer}&Retrieval& \multicolumn{2}{c}{Answer}&Retrieval & \multicolumn{2}{c}{Answer}&Retrieval \\
        
        $top_k$& EM& F1 & F1 & EM& F1 & F1 
& EM& F1 &F1 
 & EM& F1 &F1 
\\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        2& 32.50 & 46.31 & \textbf{37.83}&  47.84 & 53.91 & \textbf{36.77}& 52.05 & 67.78 &\textbf{50.23} 
 &  44.13 & 56.00 &\textbf{41.61}\\
        4& 36.50 & 49.53 & \underline{35.02}& 54.51 & 59.35 & \underline{33.22}& 55.64 & 71.10 &\underline{46.45} 
 & 48.88 & 59.99 &\underline{38.23}\\
 8& \underline{38.50}& 50.81 & 26.36 
& 56.08 & 61.81 & 23.90 & 58.21 & 75.05 &34.14  
 & 50.93 & 62.56 &28.13 \\
 12& 37.50 & \underline{51.47}& 
20.38 
& 57.65 & 64.33 & 18.54 & \underline{59.49}& 75.54 &26.34 
 & 51.55 & 63.78 &21.75 \\
 16& 37.50 & 51.44 & 16.47 
&  
\underline{60.00}& \underline{67.52}& 15.02 &  
\underline{59.49}& \underline{76.45}&21.75 
 & \underline{52.33}& \underline{65.14}&17.75 \\ 
        20&  \textbf{39.10}&  \textbf{53.00}&  13.89 & \textbf{61.60}& \textbf{68.93}& 12.51 & \textbf{61.30}& \textbf{78.34}&18.48   & \textbf{54.00}& \textbf{66.76}&14.96 \\
        \bottomrule
    \end{tabular}
}
    \caption{We test the robustness w.r.t hyperparameter $top_k$ on HopRAG using GPT-3.5-turbo on multiple datasets. We vary $top_k$ from 2 to 20 and report both the answer and retrieval metrics, where the best score is in \textbf{bold} and the second best is \underline{underlined}.}
    \label{tab:Ablation-3.5}
\end{table*}

\begin{table*}[htb]
\setlength{\tabcolsep}{2pt}
\centering
\resizebox{0.9\textwidth}{!}{
      \begin{tabular}{ccccccccc}
    \toprule
 &\multicolumn{2}{c}{MuSiQue}&\multicolumn{2}{c}{2Wiki}&\multicolumn{2}{c}{HotpotQA}&\multicolumn{2}{c}{Average}\\
        
        $n_{hop}$& Retrieval F1&LLM Cost& Retrieval F1&LLM Cost&Retrieval F1&LLM Cost&Retrieval F1&LLM Cost\\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        1&  8.78 &\textbf{20.00}&  8.68 &\textbf{19.86}& 6.78 &\textbf{19.91}& 8.08 &\textbf{19.92}\\
        2&  11.86 &\underline{30.32}&  11.42 &\underline{31.52}& 15.13 &\underline{29.39}& 12.80 &\underline{30.41}\\
 3&  \underline{12.67}&37.28 &  \underline{11.97}&37.15 
& \underline{16.76}&33.35& \underline{13.80}&35.93 
\\ 
        4&  \textbf{13.89}&40.32 & \textbf{12.51}&40.12 &\textbf{18.48}&35.14&\textbf{14.96}&38.53 \\
        \bottomrule
    \end{tabular}
    }
    \caption{We test the effect of hyperparameter $n_{hop}$ on HopRAG using GPT-3.5-turbo on multiple datasets with top 20 passages. We vary $n_{hop}$ from 1 to 4 and report both the answer and retrieval metrics in Table \ref{tab:nhopall}, and report the retrieval metrics here. For retrieval metrics, we calculate the retrieval F1 score and also the average number of calling LLM during traversal to measure the cost (the lower, the better). The best score is in \textbf{bold} and the second best is \underline{underlined}.}
    \label{tab:Ablation-3.5-hop}
\end{table*}

The main results are presented in Table \ref{tab:QA-performance-3.5} and \ref{tab:QA-performance-4o}. We observe that almost in all the settings our method gives the best performance, with exceptions on HotpotQA when compared against SiReRAG and 2WikiMultiHopQA against HippoRAG. Overall, HopRAG achieves approximately 76.78\% higher than dense retriever (BGE), 9.94\% higher than RAPTOR,  3.08\% higher than HippoRAG,  1.11\% higher than SiReRAG. This illustrates the strengths of HopRAG in capturing both textual similarity and logical relations for handling multi-hop QA. 

Specifically,  unstructured RAG baselines sparse retriever (BM25) and dense retriever (BGE) yield unsatisfactory results since they rely solely on similarity. Since GraphRAG only considers relevance among entities instead of similarity for graph search, and RAPTOR mainly focuses on the hierarchical logical relations among passages but cannot capture other kinds of relevance, both of them are more suitable for query-focused summarization but they are not the most competitive method for multi-hop QA tasks, as also reported in \citep{zhang2024sireragindexingsimilarrelated}. 


\begin{table*}[htb]
\setlength{\tabcolsep}{3pt}
\centering
\resizebox{0.9\textwidth}{!}{
      \begin{tabular}{ccccccccccccc}
    \toprule
 & \multicolumn{3}{c}{MuSiQue}& \multicolumn{3}{c}{2Wiki}& \multicolumn{3}{c}{HotpotQA} & \multicolumn{3}{c}{Average}\\
 & \multicolumn{2}{c}{Answer}&Retrieval& \multicolumn{2}{c}{Answer}&Retrieval& \multicolumn{2}{c}{Answer}&Retrieval & \multicolumn{2}{c}{Answer}&Retrieval \\
        
        Method& EM& F1 & F1 & EM& F1 & F1 
& EM& F1 &F1 
 & EM& F1 &F1 
\\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        BM25& 5.80& 11.00 
& 5.79& 27.00 & 31.55 & \underline{9.25}& 33.40& 44.30&8.75&  22.07 & 28.95 &7.93 \\
        BGE& 
11.80& 18.60& \underline{8.76}& 27.90 & 30.80 & 7.60 & 38.40& 50.56&11.10& 26.03 & 33.32 &9.16 \\
 \thead{HopRAG \\ (non-LLM)}& 
\underline{19.00}& \underline{27.68}& 8.27& \underline{42.20}& \underline{46.72}& 8.09 & \underline{46.92}& \underline{61.17}&\underline{11.73}& \underline{36.04}& \underline{45.19}&\underline{9.36}\\ 
        \thead{HopRAG\\(GPT-4o-mini)}&    \textbf{39.10}&  \textbf{53.00}&  \textbf{13.89}& \textbf{61.60}& \textbf{68.93}& \textbf{12.51}& \textbf{61.30}& \textbf{78.34}&\textbf{18.48}& \textbf{54.00}& \textbf{66.76}&\textbf{14.96 }\\
        \bottomrule
    \end{tabular}
}
    \caption{We conduct an ablation study on whether to use a reasoning model (GPT-4o-mini) during traversal with GPT-3.5-turbo as the inference model and top 20 passages. We compare 4 scenarios including sparse retriever (BM25), dense retriever (BGE), HopRAG (non-LLM) and HopRAG (GPT-4o-mini) and report both the answer and the retrieval metrics, where the best score is in \textbf{bold} and the second best is \underline{underlined}.}
    \label{tab:nonllm}
\end{table*}

In terms of HippoRAG, it prioritizes relevance signals such as vertices with the most edges and does not explicitly model similarity in its design while our design HopRAG directly integrates similarity with logical relations when constructing edges. Although HopRAG only outperforms SiReRAG by a small margin in the scenario with top 20 candidate passages, our general graph structure does not introduce additional summary and proposition aggregate nodes and can facilitate efficient multi hops for faster retrieval compared with SiReRAG. In the discussion, we will show that HopRAG can achieve competitive results with smaller context length. Besides quantitative scores, we also demonstrate a case study in Appendix \ref{case study}.
\subsection{Ablations and Discussion}
\label{ablation}
To confirm the robustness of HopRAG and provide more insights, we vary hyperparameters $top_k$, $n_{hop}$ and conduct ablation studies on whether to use a reasoning model during traversal.

\paragraph{Effects of $top_k$} To show our efficiency in faster hopping from indirectly relevant passages to truly relevant ones, we test the robustness by evaluating both the QA and retrieval performance on GPT-3.5-turbo with smaller $top_k$, as is shown in Table \ref{tab:Ablation-3.5}. From the results we find that even with top 12 candidates, the QA performance of HopRAG is still comparable with HippoRAG or RAPTOR with 20 candidates, which emphasizes the advantage of our graph traversal design on efficient hopping to retrieve more information within a limited context length. Meanwhile, we also observe that as $top_k$ and context length increase, the retrieval F1 score gradually decreases due to the inclusion of excessive redundant information. Conversely, the answer quality generally improves, attributed to GPT-3.5-turbo's strong capability in processing and reasoning over extended contexts, with only one exception in the MuSiQue dataset where the answer Exact Match (EM) score is slightly higher with top 8 candidates compared to top 12. This suggests that excessive redundancy in the context can, to some extent, hinder the LLM's ability to identify truly useful information during QA inference. 

\paragraph{Effects of $n_{hop}$} To assess the effect of the hyperparameter $n_{hop}$ in reasoning-augmented graph traversal, we varied $n_{hop}$ from 1 to 4 and evaluated the corresponding retrieval performance and cost of graph traversal, which is represented by the total number of calling LLM for each vertex in the queue in each round of hop. The results shown in Table \ref{tab:Ablation-3.5-hop} indicate that as $n_{hop}$ increases, retrieval performance tends to improve, as more vertices are covered during traversal for reasoning and pruning. However, the expense of calling LLM also increases with $n_{hop}$, creating a trade-off between performance and cost. To find the ideal $n_{hop}$, we notice that as $n_{hop}$ increases, the number of new vertices that require LLM reasoning in the queue in each hop decays quickly, as different vertices may hop to the same important vertex, making the actual queue length of each hop less than $top_k$. Specifically, the average queue length in the fourth hop is only 2.60 while in the fifth hop it is only 1.23 so there is no absolute need for one more hop. Based on these findings, we believe that 4 is the ideal $n_{hop}$ for the three datasets, as is what we set for $n_{hop}$ in Table \ref{tab:QA-performance-3.5} and \ref{tab:QA-performance-4o}. We also evaluate the answer performance as $n_{hop}$ varies and show the overall results in the appendix \ref{nhop_ablation}.

\paragraph{Ablation on Reasoning Model} 
In order to generalize our method to the scenario with no budget of calling LLM during retrieval, we provide a graph traversal version based purely on logical relations and textual similarity. Specifically, we replace the Reasoning phase in Algorithm \ref{ragt} with similarity matching where we directly choose the most similar edge ${e_{j,k}}$   with the query $q$ %from $ \{e_{j,k}|e_{j,k}=({q_{k,n}^{-}},{k_{k,n}^{-}\cup k_{j,m}^{+}},{v_{k,n}^{-}}),\langle v_j,e_{j,k},v_k\rangle \in \mathcal{E},v_j \in C_{queue},v_k \in \mathcal{V}\}$  
and hop to $v_k$ to extend $C_{queue}$. The results are shown in Table \ref{tab:nonllm}. From the results we observe that even without using the reasoning ability of LLM in the graph traversal, HopRAG can achieve 45.84\% higher than BM25 and 25.43\% higher than dense retriever (BGE), which shows that our graph-structured index can fully capture the textual similarity and logical relations among different vertices to assist in logic-aware retrieval. But as is discussed in Section \ref{introduction}, high similarity doesn't necessarily mean logical relevance so the introduction of reasoning ability from LLM is still irreplaceable and it can achieve about 45.78\% higher average score than non-LLM version. Non-LLM graph traversal is suitable for scenarios that require efficiency, while reasoning-augmented graph traversal is designed for QA tasks that require high accuracy. 

