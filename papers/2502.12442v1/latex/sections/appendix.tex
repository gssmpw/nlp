\appendix
\section{Appendix}
\label{sec:appendix}
\localtableofcontents
\subsection{Symbols}
The symbols and their corresponding meanings are listed in Table \ref{tab:symbol}.
\begin{table*}[ht]
\setlength{\tabcolsep}{3pt}
\centering
\resizebox{0.8\textwidth}{!}{
    \begin{tabular}{cc|cc}
        \toprule
        symbol& meaning & symbol& meaning \\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        $P$& passage corpus& $Q_i^+$& set of out-coming quesitions for $p_i$
\\
        $p$& passage & 
$q_{i,j}^+$& the j-th out-coming quesition for $p_i$\\
 $q$& query
& $Q_i^-$& set of in-coming quesitions for $p_i$
\\
  $C$&retrieval context& $q_{i,j}^-$& the j-th in-coming quesition for $p_i$\\
 $\mathcal{P}(\cdot|\cdot)$&LLM distribution& 
$K_i^+$& set of keywords for $Q_i^+$ 
\\
 $\mathcal{O}$&response for $q$& $k_{i,j}^+$& keywords for $q_{i,j}^+$\\
 $G$&graph&  

$K_i^-$& set of keywords for $Q_i^-$ 
\\
  $\mathcal{V}$&set of vertices& $k_{i,j}^-$& keywords for $q_{i,j}^-$\\
  $v$&vertex&  
$V_i^+$& set of embeddings for $Q_i^+$ 
\\
 $\mathcal{E}$& set of directed edges& $v_{i,j}^+$& embedding for $q_{i,j}^+$\\
 $e_{i,j}$& directed edge from $v_i$ to $v_j$& $V_i^-$& set of embeddings for $Q_i^-$ 
\\
 
$k_q$& keywords for $q$& $v_{i,j}^-$& embedding for $q_{i,j}^-$\\
 $v_q$& embedding for $q$& $R^+_i$& set of out-coming triplets\\
 $C'$& counter of vertices during traversal& $r_{i,j}^+$&out-coming
 triplet for $q_{i,j}^+$\\
 $C_{queue}$& queue of vertices during traversal& $R^-_i$&set of in-coming triplets\\
 $H$& helpfulness metric& $r_{i,j}^-$&in-coming triplet for $q_{i,j}^-$\\
 
$top_k$& context budget& $n_{hop}$& number of hop\\
\bottomrule
    \end{tabular}
}
    \caption{Table of symbols and meanings.}
    \label{tab:symbol}
\end{table*}
\subsection{Datasets}
\label{dataset}
Table \ref{tab:dataset_statistics} shows the basic statistics of our datasets with their corresponding passage pool. Compared with knowledge graph, our graph-structured index is less dense and more efficient to construct. Since we put each passage text in the vertex, we can use fewer vertices to cover all the passages, which lowers the space complexity of the database. The average number of directed edges for each vertex is only 5.87, which lowers the time complexity for graph traversal. 
% Requires: \usepackage{booktabs}
\begin{table*}[ht]
    \centering
    \setlength{\tabcolsep}{2pt}
    \begin{tabular}{cccccccc}
        \toprule
        dataset& number& docs &supporting facts& vertices& edges&  
avg text 
 length
& avg edge number\\
        \midrule
        MuSiQue & 1000 &   19990 &2800 &  13086 &  81348 
&  489.52 &  6.22 
\\
        2Wiki &  1000 &   10000 
&2388 &  23360 &  167068 
&  116.07 &  7.15 
\\
        HotpotQA & 1000 &   9942 
&2458 &  40534 &  171946 
&  132.44 &  4.24 
\\
        Average & 1000 &   13311 &2549 &  25660 &  140121 &  246.01 &  5.87 \\
        \bottomrule
    \end{tabular}
    \caption{Dataset Statistics. We report the basic statistics of the graph structure on different datasets and demonstrate that our efficient graph structure is traversal-friendly.}
    \label{tab:dataset_statistics}
\end{table*}
\subsection{Metrics}
\label{metric}

In our experiment, we mainly report the answer exact match (EM) and F1 score to compare all the methods.

     \paragraph{Exact Match (EM)} 
    The Exact Match (EM) metric measures the percentage of predictions that match any one of the ground truth answers exactly. It is defined as:
    \[
    EM = \frac{|\{ p \mid p = g \} |}{| P |}
    \]
    where \( p \) denotes a predicted answer, \( g \) denotes the corresponding ground truth answer, and \( P \) is the set of all the predictions.

     \paragraph{F1 Score}
    The F1 score is the harmonic mean of precision and recall, which measures the average overlap between the prediction and ground truth answer. Precision \( P \) and recall \( R \) are defined as:
    \[
    \text{P} = \frac{| A \cap \hat{A} |}{| \hat{A} |}, \quad \text{R} = \frac{| A \cap \hat{A} |}{| A |}
    \]
    where \( | A \cap \hat{A} | \) refers to the number of matching tokens between the prediction \( \hat{A} \) and the ground truth \( A \), and \( |\hat{A}| \), \( |A| \) denote the number of tokens in the predicted and ground truth answers, respectively.

    The F1 score is then computed as:
    \[
    F1 = \frac{2 \cdot \text{P} \cdot \text{R}}{\text{P} + \text{R}}
    \]

In our ablation study, we also report the retrieval F1 score to test the sensitivity of HopRAG, which is calculated as follows.

The Precision (P) and Recall (R) for retrieval are computed as:
\[
P = \frac{| \text{Ret} \cap \text{Rel} |}{| \text{Ret} |}, \quad R = \frac{| \text{Ret} \cap \text{Rel} |}{| \text{Rel} |}
\]
where \( \text{Ret} \) represents the set of passages retrieved during retrieval, and \( \text{Rel} \) denotes the set of relevant passages that support the ground truth answer.

The Retrieval F1 score is then calculated as the harmonic mean of precision and recall:
\[
F_{1_{\text{retrieval}}} = \frac{2 \cdot P \cdot R}{P + R}
\]
\subsection{Settings}  
\label{setup}
To avoid semantic loss by chunking the documents at a fixed size, we chunk each document in a way corresponding to the supporting facts of each dataset. Specifically, we chunk each document in HotpotQA and 2WikiMultiHopQA by sentence since the smallest unit of these two datasets' supporting facts is a sentence.
To get embedding representation for each chunk, we use bge-base model. To extract keywords, we use the part-of-speech tagging function of Python package PaddleNLP to extract and filter entities.
In our method, we use the Neo4j graph database to store vertices and build edges. When building edges we employ prompt engineering technique to instruct the LLM to generate an appropriate number of questions for each vertex to cover its information, with a minimum requirement of at least 2 in-coming questions and 4 out-coming questions. To prevent the graph structure from becoming overly complex and dense, we retain only $O(n\cdot\log(n))$ edges, where $n$ is the number of vertices. We use GPT-4o-mini for reasoning-augmented graph traversal, GPT-4o and GPT-3.5-turbo for inference with 2048 max tokens and 0.1 temperature. 
\subsection{Prompts}
\label{prompt}
The prompt used for generating in-coming questions is shown in Figure \ref{fig:incoming}.  The prompt used for generating out-coming questions is shown in Figure \ref{fig:outcoming}. %The prompt for generating a title for each document is shown in Figure \ref{fig:titleprompt}. 
The prompt for reasoning-augmented graph traversal is shown in Figure \ref{fig:ragtprompt}. 
\begin{figure*}[t]
\centering
  \includegraphics[width=0.75\linewidth]{latex/figures/incoming.jpg}
  \caption{Prompt for generating in-coming questions.}
  \label{fig:incoming}
\end{figure*}
\begin{figure*}[t]
\centering
  \includegraphics[width=0.75\linewidth]{latex/figures/outcoming.jpg}
  \caption{Prompt for generating out-coming questions.}
  \label{fig:outcoming}
\end{figure*}

\begin{figure*}[ht]
\centering
\subfigure[Demonstration of our graph structure.]{\label{graph demo}\includegraphics[width=1\columnwidth]{latex/figures/graph_demo.jpg}}
\subfigure[Demonstration of reasoning-augmented graph traversal.]
{\label{donnie demo}\includegraphics[width=1\columnwidth]{latex/figures/donnie_demo.jpg}}
\caption{Demonstration of the traversal in the graph structure}\label{demo1}
\end{figure*}

%\begin{figure*}[t]
%\centering
  %\includegraphics[width=0.75\linewidth]{latex/figures/title_.jpg}
  %\caption{Prompt for generating title.}
 % \label{fig:titleprompt}
%\end{figure*}

\begin{figure*}[t]
\centering
  \includegraphics[width=0.75\linewidth]{latex/figures/traversal.jpg}
  \caption{Prompt for reasoning-augmented graph traversal.}
  \label{fig:ragtprompt}
\end{figure*}

\begin{figure*}[t]
\centering
  \includegraphics[width=0.75\linewidth]{latex/figures/vertex_edge.jpg}
  \caption{Demonstration of one edge between two vertices.}
  \label{fig:vertex_edge}
\end{figure*}
\subsection{Case Study}
\label{case study}
We demonstrate the graph structure in Figure \ref{graph demo}, one example edge with two vertices in Figure \ref{fig:vertex_edge}. Using the query "Donnie Smith who plays as a left back for New England Revolution belongs to what league featuring 22 teams?" as an example we conduct a qualitative analysis. For this multi-hop question (correct answer: Major League Soccer), the HotpotQA corpus contains three relevant sentences: (1) "Donald W. \textbackslash{}"Donnie\textbackslash{}" Smith (born December 7, 1990 in Detroit, Michigan) is an American soccer player who plays as a left back for New England Revolution in Major League Soccer. "; (2) "Major League Soccer (MLS) is a men's professional soccer league, sanctioned by U.S. Soccer, that represents the sport's highest level in both the United States and Canada. " and (3) "The league comprises 22 teams in the U.S. and 3 in Canada." 

With dense retriever (BGE), we can easily retrieve the first sentence but the last two facts can't be retrieved in the context even with a $top_k$ of 30. However, in our graph-structured index, these three vertices are logically connected, as is shown in Figure \ref{donnie demo}. During the traversal, LLM starts from the semantically similar but indirectly relevant vertices and can reach all the supporting facts within only a maximum of 3 hops.

\subsection{Discussion results on $n_{hop}$}
\label{nhop_ablation}
In the discussion we notice that as the hyper-parameter $n_{hop}$ varies from 1 to 4, the answer and retrieval performance both increase, along with the retrieval cost of calling LLM during traversal. Since the average queue length in the fifth hop is only as small as 1.23, we believe 4 is the ideal $n_{hop}$. The overall results are shown in Table \ref{tab:nhopall}.

\begin{table*}[hb]
\setlength{\tabcolsep}{3pt}
\small
\centering
      \begin{tabular}{cccclccclccclcccl}
    \toprule
 & \multicolumn{4}{c}{MuSiQue}& \multicolumn{4}{c}{2Wiki}& \multicolumn{4}{c}{HotpotQA}& \multicolumn{4}{c}{Average}\\
 & \multicolumn{2}{c}{Answer}&\multicolumn{2}{c}{Retrieval}& \multicolumn{2}{c}{Answer}&\multicolumn{2}{c}{Retrieval}& \multicolumn{2}{c}{Answer}&\multicolumn{2}{c}{Retrieval}& \multicolumn{2}{c}{Answer}&\multicolumn{2}{c}{Retrieval}\\
        
        $n{hop}$& EM& F1 & F1  &Cost& EM& F1 & F1 
 &Cost& EM& F1 &F1 
  &Cost& EM& F1 &F1 
 &Cost\\
        \midrule
        % \multicolumn{9}{c}{GPT-3.5-Turbo} \\ \midrule
        1& 21.50 & 30.77 & 8.78 &\textbf{20.00}& 48.60 & 52.44 & 8.68 &\textbf{19.86}& 47.95 & 62.92 &6.78 &\textbf{19.91}&  39.35 & 48.71 &8.08 &\textbf{19.92}\\
        2& 
32.00 & 43.75 & 11.86 &\underline{30.32}& \underline{54.90}& \underline{60.37}& 11.42 &\underline{31.52}& 55.90 & 71.26 &15.13 &\underline{29.39}& 47.60 & 58.46 &12.80 &\underline{30.41}\\
 3& 
\underline{32.50}& \underline{44.50}& \underline{12.67}&37.28 & 52.90& 59.16& \underline{11.97}&37.15 & \underline{57.44}& \underline{73.86}&\underline{16.76}&33.35 & \underline{47.61}& \underline{59.17}&\underline{13.80}&35.93 \\ 
        4&    \textbf{39.10}&  \textbf{53.00}&  \textbf{13.89}&40.32& \textbf{61.60}& \textbf{68.93}& \textbf{12.51}&40.12& \textbf{61.30}& \textbf{78.34}&\textbf{18.48}&35.14& \textbf{54.00}& \textbf{66.76}&\textbf{14.96}&38.53 \\
        \bottomrule
    \end{tabular}
    \caption{    
   We test the effect of hyperparameter $n_{hop}$ on HopRAG using GPT-3.5-turbo on multiple datasets
with top 20 passages. We vary $n_{hop}$ from 1 to 4 and report both the answer and retrieval metrics here. For answer metrics, we report the answer EM and F1 score; For retrieval metrics, we report the F1 score and average number of calling
LLM during traversal to measure the cost (the lower, the better). The best score is in \textbf{bold} and the second best is
\underline{underlined}. 
    }
    \label{tab:nhopall}
\end{table*}