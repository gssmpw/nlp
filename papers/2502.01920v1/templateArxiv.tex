\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder

% add by me
\usepackage{amsfonts, bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
%\fancyhead[LO]{Running Title for Header}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Anomaly Detection via Autoencoder Composite Features and NCE
%%%% Cite as
%%%% Update your official citation here when published 
% \thanks{\textit{\underline{Citation}}: 
% \textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
  Yalin Liao \\
  Department of Electrical and Computer Engineering \\
  University of Delaware \\
  Newark, Delaware\\
  \texttt{yalin@udel.edu} \\
  %% examples of more authors
   \And
  Austin J. Brockmeier\\
  Department of Electrical and Computer Engineering\\
  Department of Computer and Information Sciences\\
  University of Delaware \\
  Newark, Delaware\\
  \texttt{ajbrock@udel.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\begin{document}
\maketitle


\begin{abstract}
Unsupervised anomaly detection is a challenging task. Autoencoders (AEs) or generative models are often employed to model the data distribution of normal inputs and subsequently identify anomalous, out-of-distribution inputs by high reconstruction error or low likelihood, respectively. However, AEs may generalize and achieve small reconstruction errors on abnormal inputs. We propose a decoupled training approach for anomaly detection that both an AE and a likelihood model trained with noise contrastive estimation (NCE).  After training the AE, NCE estimates a probability density function, to serve as the anomaly score, on the joint space of the AE’s latent representation combined with features of the reconstruction quality. To further reduce the false negative rate in NCE we systematically varying the reconstruction features to augment the training and optimize the contrastive Gaussian noise distribution. Experimental assessments on multiple benchmark datasets demonstrate that the proposed approach matches the performance of prevalent state-of-the-art anomaly detection algorithms.
\end{abstract}


% keywords can be removed
% \keywords{First keyword \and Second keyword \and More}


\section{Introduction}
The goal of anomaly detection is to identify observations that considerably deviate from the typical distribution \cite{chandola2009anomaly}. 
In recent years, anomaly detection has achieved significant success in various domains, 
such as cybersecurity \cite{xin2018machine,malaiya2019empirical}, 
medical care \cite{gugulothu2018sparse,naud2020manifolds,shvetsova2021anomaly}, 
industrial monitoring \cite{atha2018evaluation,borghesi2019anomaly,sipple2020interpretable}. To detect anomalies, various machine learning and statistical methods have been proposed or applied, including principal component analysis  (PCA) \cite{huang2006network}, one-class support vector machines  \cite{scholkopf1999support}, kernel density estimation (KDE) \cite{parzen1962estimation}, and isolation forests  \cite{liu2008isolation}. However, these classical methods rely on already having a meaningful feature representation, and their  efficacy is diminished on complicated data such as images, which require processing to extract meaningful patterns. %A motivating example is flagging causes of cancer from multi-gigapixel whole-side images in histopathology \cite{faust2018visualizing}.

Along with the overall rise of deep learning, neural network-based anomaly detectors are often used for image-related applications \cite{seebock2016identifying, ruff2018deep, sabokrou2018deep}. Autoencoders (AEs), often with convolutional architectures, are trained on the `normal' data and widely applied for anomaly detection in one of two distinct cases. In the first case, the reconstruction error of an instance serves as the anomaly score \cite{sakurada2014anomaly, xia2015learning, chen2018autoencoder}. In the second case, the AE's learned latent representation of the data in the bottleneck layer are treated as features, and subsequently, a machine learning or statistical approach is employed to detect anomalies based on this learned representation \cite{andrews2016detecting,  sabokrou2018deep, xu2015learning}. 


In contrast to the prevailing majority of prior studies, which solely utilize either latent representation or reconstruction error as features, our approach incorporates both types of features for anomaly detection. The latent representation at the bottleneck layer is concatenated with reconstruction error metrics for the AE's output as additional features. Specifically, we train a constrained AE exclusively on normal images. Subsequently, a composite feature vector is formed by concatenating the low-dimensional feature and the reconstruction feature. To derive the anomaly score, we use noise contrastive estimation (NCE)~\cite{gutmann2010noise,gutmann2012noise} to estimate a log-likelihood function in terms of this composite feature, which will serve as the anamoly score.  

The composite feature enhances the robustness of our method, and we propose techniques to adjust the AE to be better suited for the subsequent NCE, which trains a network to distinguish the latent representation of typical input from Gaussian noise. Firstly, the architecture of the AE is designed such that first and second moments of the latent representation better match a standard Gaussian. Specifically, the batch normalization is introduced is introduced to ensure a zero-mean and unit-variance latent representation. Additionally, a covariance loss is introduced to encourage diagonal covariance, mitigating a singular covariance matrix. This will objectively encourage the development of statistically uncorrelated latent feature, making the composite feature better suited for NCE. 

In the second step, the NCE is enhanced through systematic data augmentation of the reconstruction features. We introduce additional normal instances with artificial reconstruction features when training the estimation network to ensure that the marginal density function for low reconstruction errors is no less than the noise distribution. This decreases the probability of predicting abnormal points as normal points. 

Experimental results on multiple benchmark datasets demonstrate that the proposed approach matches the performance of prevalent state-of-the-art anomaly detection algorithms. An ablation study demonstrates the contribution of the proposed additions to improve the anomaly detection performance. Finally, we demonstrate the generality of the two-step composite approach by substituting the AE with a pretrained network representation followed by PCA, where the principal components and the PCA reconstruction error form the composite features.


\section{Related Work}
Various strategies for anomaly detection are explored by approximating the density function of normal instances \cite{abati2019latent}, where anomalies are identified by their low modeling probabilities. A straightforward approach involves using statistical models, such as Gaussian distribution \cite{jain1988algorithms} and Gaussian mixture model (GMM) \cite{bishop1994novelty}, to fit the training dataset and valuate the log-likelihood of a test point as its anomaly score. For modeling complex distributions, non-parametric density estimators, like Kernel Density Estimation (KDE) \cite{parzen1962estimation} and histogram estimators, have been developed. KDE stands out as the most commonly employed classic density estimator partially because it has theoretical advantages over histograms \cite{devroye1985nonparametric} and addresses practical challenges related to fitting and parameter selection in GMM \cite{fruhwirth2006finite}. KDE, equipped with a more recent adaptation capable of handling varying levels of outliers in the training data \cite{kim2012robust,vandermeulen2013consistency}, has remained a popular approach for anomaly detection. 

Although KDE and GMM perform reasonably well in low-dimensional scenarios, both suffer from the curse of dimensionality \cite{ruff2021unifying}. Additionally, while these classic approaches for anomaly detection work well when they can exploit meaningful feature representations, for domains such as images, directly applying these methods yields poor performance. Instead, density estimation or parametric modeling can be applied to the latent learning representations of AEs \cite{andrews2016detecting,  sabokrou2018deep, xu2015learning} as is common in prior work. This is supported by the fact that the true effective dimensionality is significantly smaller than the image dimensionality \cite{ruff2021unifying}. 

%\cite{zong2018deep} noted that anomalous inputs may share some critical features with normal inputs and be well reconstructed. To address this,

Almost all prior work on using AE for anomaly detection have relied on either scores derived from latent features or from reconstruction error. One prior work the Autoencoding Gaussian mixture model (DAGMM) \cite{zong2018deep} also integrates latent and reconstruction features for anomaly detection,  wherein an AE and a GMM are jointly optimized for their parameters. Like DAGMM, we incorporate both latent features and reconstruction errors for anomaly detection. The key difference in our approach is that we adopt noise contrast estimation as non-parametric machine learning based approach for density estimation, which allows us to sidestep the challenges associated with forming a GMM, including specifying the number of mixture components in the DAGMM.

Alternatives to AEs include, deep generative model techniques that enable modeling more complicated `normal' data to enhance anomaly detection. While deep energy-based models~\cite{zhai2016deep} have been used, their reliance on Markov chain Monte Carlo (MCMC) sampling creates computationally expensive training. Alternatively, autoregressive models \cite{salimans2017pixelcnn++} and flow-based generative models \cite{kingma2018glow} have been used to detect outliers 
via direct likelihood estimation. However, these approaches tend to assign high likelihood scores to anomalies as reported in recent literature \cite{choi2018waic,ren2019likelihood,yoon2024energy}.

Variational Autoencoders (VAEs) can approximate the distribution of normal data via Monte Carlo sampling from the prior, thereby making them effective tools for anomaly detection.  However, experiments in previous work \cite{nalisnick2018deep,xu2018unsupervised} have demonstrated that utilizing the reconstruction probability \cite{an2015variational} as an alternative can lead to improved performance.

Finally, Generative Adversarial Network (GAN) \cite{goodfellow2014generative} provide an implicit model of data distribution have been refined for application in anomaly detection \cite{di2019survey}. Most GAN-based approaches, such as AnoGAN \cite{schlegl2017unsupervised} and EGBAD \cite{zenati2018efficient}, assume that after training the generator can produce normal points from the latent space better than anomalies, and naturally the discriminator, trained to distinguish  between the generated data and the input data, could work as the anomaly measure.  However, the optimization of GAN-based methods is challenged by the failure to converge during training and mode collapse \cite{metz2016unrolled}.  

\section{Proposed Method}
As overviewed in Figure~\ref{overall method}, we present a two-step methodology for anomaly detection. First, we employ a decoupled Autoencoder (AE) to construct a composite feature. Subsequently, we utilize a network trained with noise contrastive estimation (NCE) to estimate the negative log-likelihood function based on the composite feature, which serves as the scoring function, with higher values indicating anomalies and lower values signifying normality.

\begin{figure}[htb] % picture
\centering
\includegraphics[width=0.85\columnwidth]{ad_method.pdf}
\caption{Proposed method for anomaly detection.}
\label{overall method}
\end{figure}

\subsection{Method Introduction}
For input data $\bm x\sim p_{d_0}$ in the data space $\mathbb{R}^{d_0}$ distributed according to $p_{d_0}$, we propose to estimate a score function $S:\mathbb{R}^{d_0}\rightarrow \mathbb{R}$ to predict anomalies. Ideally, $S(\bm x)$ could approximate the negative log-likelihood function, but since probability density functions often do not exist in $\mathbb{R}^{d_0}$, especially for image datasets, we estimate a score function $S_C: \mathbb{R}^{d+2}\rightarrow \mathbb{R}$ as the negative log-likelihood of the composite feature $\bm z = C(\bm x)$ to score possible anomalies for input data $\bm x$ via $S(\bm x)=S_C(C(\bm x))$. The distribution of composite features is $p_d = C_\sharp p_{d_0}$, which is obtained as a pushforward measure through the composite feature function $C$. (To simplify notation, distributions will be denoted by their probability density or mass functions for discrete random variables.) The composite feature $\bm z\in\mathbb{R}^{d+2}$ includes latent feature $\bm z_l \in\mathbb{R}^d$ and reconstruction quality features $\bm z_r\in\mathbb{R}^2$ (error and cosine dissimilarity) from a pre-trained AE with encoder $E_{\phi_1}(\bm x)$ and decoder $D_{\phi_2}(\bm z_l)$, or another pretrained network combined with PCA.  

The score function $S_C$ is derived through noise contrastive estimation (NCE)~\cite{gutmann2010noise,gutmann2012noise}. In this process, an estimation network $T_{\bm\theta}(\bm u)$ is trained with supervision to predict whether the network's input $\bm u$ is from the composite feature distribution $p_d$, such that $\bm u =\bm z = C(\bm x) \sim p_d$, or from a specified noise distribution $p_n$,  such that $\bm u = \bm v\sim p_n$. %Let $$\bm u$ is either $\bm z$ ($\bm z\sim p_d$) or $\bm v$  ($\bm v\sim p_n$).} 
After training, the optimized estimation network $T_{\bm\theta^*}(\bm z)$ approximates the log density ratio $\ln\frac{p_d(\bm z)}{p_n(\bm z)}$ plus a constant, which provides an approximation of the negative log-likelihood $-\ln {p_d(\bm z)}$, since $p_n$ is known. $S_C$ is this approximation, such that a high $S(\bm x)=S_C(C(\bm x))$ suggests the data point $\bm x$ is likely to be abnormal, while a low value indicates normality. 

\subsection{Autoencoder Network Design and Training}
The AE is designed to provide a compressed space of latent features, which can be used for anomaly detection as shown in previous work \cite{sakurada2014anomaly, sabokrou2018deep}, and accurate reconstructions of normal data, while ideally providing poor reconstructions of anomalies. However, with vanilla training the latent feature distribution could create a degenerate distribution, i.e., the latent representations could contract to lie in a strict subspace of the whole latent space \cite{ozsoy2022self}, creating a singular covariance matrix. Non-degeneracy of the learned representations is necessary for the subsequent NCE as the contrastive noise is assumed to follow a multivariate Gaussian, and a degenerate distribution that lies in a subspace makes the problem ill-posed. To avoid the collapsed representation, we implement batch normalization directly before the latent space and penalize correlation among latent features in terms of the squared values of off-diagonal elements in the covariance matrix. In summary, we propose to learn structured representations by training a constrained AE to jointly minimize the reconstruction error and a covariance loss term that encourages the components of the latent feature to be statistically uncorrelated. 

The loss function that guides training of the compression networks encoder and decoder parameters, $\bm\phi_1$ and $\bm\phi_2$, respectively, is $\mathcal{L}(\bm\phi_1,\bm\phi_2)=\mathcal{L}_{error}(\bm\phi_1,\bm\phi_2)+\lambda\mathcal{L}_{cov}(\bm\phi_1)$ with  trade-off hyperparameter $\lambda$ between the two losses
\begin{align}\label{ae_loss}
\mathcal{L}_{error}(\bm\phi_1,\bm\phi_2)&=\mathbb{E}_{\bm{x}\sim p_{d_0}}
\left[\|\bm{x}-D_{\bm\phi_2}(E_{\bm\phi_1}(\bm{x}))\|^2\right],\\
\mathcal{L}_{cov}(\bm\phi_1)&=\frac{1}{d(d-1)}\left\Vert
\mathrm{off}(\bm\Sigma_{E_{\bm\phi_1}(\bm{x})})\right\Vert_F^2,
\end{align}
where $\mathcal{L}_{error}(\bm\phi_1,\bm\phi_2)$ is the mean squared error of the reconstruction, $\mathcal{L}_{cov}(\bm\phi_1)$ is the mean of the squared off-diagonal elements in the covariance matrix $\bm\Sigma_{E_{\bm\phi_1}(\bm{x})}$ of the latent representation $\bm{z}_l=E_{\bm\phi_1}(\bm{x})$, $\mathrm{off}(\bm\Sigma)=\bm\Sigma-\bm\Sigma  \odot \bm{I}_{d}$, $\odot$ is the element-wise product,  and $\bm{I}_{d}$ is the identity matrix. 

The primary goal of incorporating the covariance loss $\mathcal{L}_{cov}(\bm\phi_1)$ into the AE's loss is to maintain the non-singularity of $\bm\Sigma_{E_{\bm\phi_1}(\bm{x})}$. In NCE, the noise distribution typically follows a Gaussian, with its mean and covariance derived from the training dataset. When the covariance matrix $\bm\Sigma_{E_{\bm\phi_1}(\bm{x})}$ is singular degenerate, it corresponds to a degenerate distribution and lacks a density. Furthermore, if the covariance matrix is ill-conditioned it causes numerical issues during the computation of the covariance matrix’s inverse. Consequently, the goal is to simply choose the smallest $\lambda$ that yields a well-conditioned covariance.  

Additionally, we adopt a decoupled training strategy to mitigate the impact of the covariance loss on the AE's ability to reconstruct input images. Specifically, the encoder is updated only during the first stage of training and is subsequently frozen in the second stage while the decoder is further trained. This decoupled training method also facilitates the learning of a higher-quality latent feature \cite{hu2024complexity,loaiza2024deep}. 

Given a data point $\bm{x}\in\mathbb{R}^{d_0}$, its composite feature $\bm z\in\mathbb{R}^{d+2}$
is formulated by concatenating the latent feature $\bm{z}_l=E_{\bm\phi_1}(\bm{x})\in\mathbb{R}^d$ and the construction feature $\bm z_r=(z_e(\bm{x}, z_c(\bm{x}))\in\mathbb{R}^2$. Specifically,
\begin{align*}
\bm z=(\bm{z}_l,\bm{z}_r)=\left (E_{\bm\phi_1}(\bm{x}), z_e(\bm{x}), z_c(\bm{x}) \right )= C(\bm x) 
\end{align*}
where $z_e(\bm{x})=\frac{\|\bm{x}-\bm{x}'\|^2}{d_o}$ is squared error of the reconstruction $\bm x'=D_{\bm\phi_2}(E_{\phi_1}(\bm x))$ and $z_c(\bm{x})=\frac{1}{2}\left(1-\frac{\bm x^T\bm x'}{\|\bm x\|\|\bm x'\|}\right)$ is a cosine dissimilarity. 

As an alternative to using an AE, pretrained models can be employed to extract feature embeddings or latent representations. In line with the methodologies of \cite{bergmann2019mvtec, reiss2021panda, han2022adbench}, we utilize ResNet-18 \cite{he2016deep}, pretrained on ImageNet \cite{deng2009imagenet}, to extract meaningful embedding after the last average pool layer. Given that the latent representation corresponding to features extracted by  ResNet-18 are high-dimensional and the covariance matrix may be ill-conditioned, we further apply principal component analysis (PCA) to compress these features. Similar to the approach used with decoupled autoencoders, we concatenate the latent features and the reconstructed features of the PCA to form the composite features.

\subsection{Noise-Contrastive Estimation (NCE)} 
We adopt noise-contrastive estimation (NCE) 
\cite{gutmann2010noise,gutmann2012noise} to train a neural network to produce an estimate of the probability density function $p_d$ of the composite features $\bm{z}$ for nomral data. The fundamental concept behind NCE is to model an unnormalized density function by contrasting it with an auxiliary noise distribution, which is intentionally designed to be tractable for both evaluation and sampling purposes. Given the data distribution $p_d$ and the noise distribution $p_n$,  we define the conditional distributions of $\bm{u}$, which is either data or noise, as
\begin{equation*}
p_{\bm u|y}(\bm{u}|y)=\begin{cases}p_d(\bm{u}),& y=1\\p_n(\bm{u}),& y=0\end{cases},
\end{equation*}
where $y\in\{0,1\}$.
Then, the model distribution $p^{\bm\theta}_{\bm z}$ is indirectly fit to the data distribution $p_d$ using the maximum likelihood  estimate of $p^{\bm\theta}_{y|\bm u}$ as $
\max_{\bm\theta}  \mathbb{E}[\ln p^{\bm\theta}_{y|\bm u}(y|\bm{u})]$, or, equivalently, 
\begin{align}\label{general_nce2}
\max_{\bm\theta} \mathbb{E}_{\bm{z}\sim p_d}[\ln p^{\bm\theta}_{y|\bm z}(1|\bm{z})]+\nu \mathbb{E}_{\bm{v}\sim p_n}[\ln p^{\bm\theta}_{y|\bm z}(0|\bm{v})], 
\end{align}
where $\nu$ denotes $\frac{\mathrm{Pr}(y=0)}{\mathrm{Pr}(y=1)}$. 
The posterior probability $p^{\bm\theta}_{y|\bm u}$ is  modeling using logistic regression
\begin{align*}
p^{\bm\theta}_{y|\bm u}(y=1|\bm{u})&=\frac{p^{\bm\theta}_{\bm z}(\bm{u})}{p^{\bm\theta}_{\bm z}(\bm{u})+\nu p_n(\bm{u})}\\
&=\sigma(\ln p^{\bm\theta}_{\bm z}(\bm{u})-\ln\nu p_n(\bm{u})),
\end{align*}
where $\sigma(x)=\frac{1}{1+e^{-x}}$ is the sigmoid function. 
The log-odds $\ln p^{\bm\theta}_{\bm z}(\bm{u})-\ln\nu p_n(\bm{u})$ can be modeled by a neural network
\begin{align}\label{network_model}
T_{\bm\theta}(\bm{u}):=\ln p^{\bm\theta}_{\bm z}(\bm{u})-\ln\nu p_n(\bm{u}).
\end{align}
By substituting 
$p^{\bm\theta}_{y|\bm u}(1|\bm{u})=\sigma\left(T_{\bm\theta}(\bm{u})\right)$ and 
$p^{\bm\theta}_{y|\bm u}(0|\bm{u})=1-\sigma\left(T_{\bm\theta}(\bm{u})\right)$
into \eqref{general_nce2},
we obtain the loss function $\mathcal{L}_{\mathrm{NCE}}(\bm\theta)$
\begin{align}\label{nce}
-\underset{{\bm{z}\sim p_d}}{\mathbb{E}}\left[\ln\sigma\left(T_{\bm\theta}(\bm{z})\right)\right]-\nu\underset{\bm{v}\sim p_n}{\mathbb{E}}\left[\ln(1-\sigma\left(T_{\bm\theta}(\bm{v}))\right)\right].
\end{align}
Substituting $\bm\theta^*$, the minimizer of
$\mathcal{L}_{\mathrm{NCE}}(\bm\theta)$, into \eqref{network_model}  and rearranging the terms yields
\begin{align}\label{density_estimator_}
\ln p^{\bm\theta^*}_{\bm z}(\bm z) = T_{\bm\theta^*}(\bm z)+\ln\nu p_n(\bm z) = -S_C(\bm z),
\end{align}
where $S_C$ is the anomaly score on the composite features. When the model is sufficiently powerful, the optimal model $p^{\bm\theta^*}_{y|\bm{u}}$ will match $p_{y|\bm u}$, implying that $p^{\bm\theta^*}_{\bm z} \equiv p_d$  and
\begin{align}\label{match}
 S_C(\bm z) \equiv - \ln p_d(\bm z). 
\end{align}


\subsection{Adapting NCE for Anomaly Detection}
Selecting an appropriate noise distribution $p_n(\bm{z})$ is crucial for the success of NCE.  As discussed in \cite{gutmann2010noise}, NCE performs optimally when the noise distribution $p_n$ closely resembles the composite feature distribution $p_d$. Following this principle, we iteratively optimize the noise distribution during NCE training.

\paragraph{Optimizing Noise Distribution}
In NCE, the noise distribution $p_n$ is often chosen to be Gaussian $\mathcal{N}(\hat{\bm\mu}_{\bm z},\hat{\bm\Sigma}_{\bm z})$, where $\hat{\bm\mu}_{\bm z}$ and $\hat{\bm\Sigma}_{\bm z}$ are the sample mean and variance derived from the training dataset, respectively.\footnote{For large training datasets, these two estimators are not feasible because the entire dataset cannot be processed by the compression network simultaneously. To address this limitation, we can iteratively estimate the mean and covariance of the sample in batches as detailed in Appendix \ref{iterative_mean}.}  We create a refined the noise distribution for NCE through the parametrization $p_{n\bm{K}}=\mathcal{N}(\hat{\bm\mu}_{\bm z},\bm{K}^T\hat{\bm\Sigma}_{\bm z}\bm{K})$, where $\bm{K}$ represents a parameter matrix. Subsequently, $\bm{K}$ is adjusted to maximize the NCE loss.

To make it feasible to optimize $\bm{K}$ through backpropagation, we first draw a sample $\bm{z}\sim\mathcal{N}(\hat{\bm\mu}_{\bm z}, \hat{\bm\Sigma}_{\bm z})$ and then use the affine function $L_{\bm K}(\bm z)=\bm{K}(\bm{z}-\hat{\bm\mu}_{\bm z})+\hat{\bm\mu}_{\bm z}$ to draw from the intended Gaussian $\mathcal{N}(\hat{\bm\mu}_{\bm z}, \bm{K}^T\hat{\bm\Sigma}\bm{K})$, which is the well-known reparameterization trick. Note that the affine transformation $L_{\bm K}$ only alters the covariance matrix under the assumption that the sample mean is reliable. 

The naive maximization of the NCE loss in terms of $\bm{K}$ is equivalent to minimizing $\mathbb{E}_{\bm{u}\sim p_n}[\ln(1-\sigma(T_{\bm\theta}(L_{\bm K}(\bm u))))]$,
since the first term in \eqref{nce} does not depend on noise. While it `confuses' $T_{\bm\theta}$, it does not guarantee the noise distribution is better matched. Instead, following similar work for GAN training \cite{che2016mode}, the first term in the NCE loss can be incorporated into the optimization  as
\begin{equation}\label{adnce}
\begin{aligned}
\min_{\bm{K}}\quad &\mathbb{E}_{\bm{u}\sim p_d}\left[\ln\sigma\left(T_{\bm\theta}\left(L_{\bm K}\left(\textbf{sg}\left(L_{\bm K}^{-1}\left(\bm{u}\right)\right)\right)\right)\right)\right]\\
& +\nu\mathbb{E}_{\bm{u}\sim p_n}\left[\ln\left(1-\sigma\left(T_{\bm\theta}\left(L_{\bm K}\left(\bm{u}\right)\right)\right)\right)\right],
\end{aligned}
\end{equation}
where $L^{-1}_{\bm K}(\bm z)=\bm{K}^{-1}(\bm{z}-\hat{\bm\mu}_{\bm z})+\hat{\bm\mu}_{\bm z}$ and $\textbf{sg}(\cdot)$ is the stop gradient operation. 

\paragraph{Augmenting Reconstruction Features for Normal Data}
Well-chosen data augmentation techniques generally enhance model performance. However, in anomaly detection augmentations that preserve normality require domain knowledge. We propose to augment normal data by adjusting the reconstruction features alone, without modifying the input or latent representations. We achieve this by generating additional normal points by replacing reconstruction features with artificially lower values while maintaining the latent representations. The goal is to bias the estimation network such that artificially low reconstruction features, which may be observed by chance in points the noise distribution, are deemed normal. 

Specifically, we create artificial normal points by defining $\bm z = (\bm z_l, z_e', z_c')$, where $\bm z_l$ is the latent feature of normal data, and $z_e'\sim p_{t_1}$ and $z_c'\sim p_{t_2}$ are independently drawn from the truncated normal distributions. Next, we explain how the parameters of the truncated normal distributions $p_{t_1}$ and $p_{t_2}$ are defined to ensure that each marginals of the augmented data distribution have a density that is higher than the noise distribution over low reconstruction errors/dissimilarities. 

Given that the reconstruction feature exhibits a skewed unimodal form, we assume it follows a log-normal distribution, $\ln z\sim\mathcal{N}(\mu, \sigma)$, where $z\sim p_t$ is either $z_e\sim p_{t_1}$ or $z_c\sim p_{t_2}$. Then, its density function is
$p_0(z)=\frac{1}{z\sigma\sqrt{2\pi}}\exp\left[-\frac{(\ln z-\mu)^2}{2\sigma^2}\right]$.
Its mode $m_z$ can be computed using its mean $\mu_z$ and variance $\sigma_z$ as
\begin{align}\label{mode}
m_z=\mu_z\bigg(\frac{\sigma_z^2}{\mu_z^2}+1\bigg)^{-\frac{3}{2}},
\end{align}
as shown in Appendix \ref{mode_dev}. We estimate the distribution mean $\mu_z$, distribution variance $\sigma_z$, and distribution mode $m_z$ using the training dataset and \eqref{mode}. These estimates are then used to define the truncated normal distribution $p_t$ defined on $[0,m_z]$.
With an equal mixture of normal and artificial normal points, the augmented density function becomes
\begin{align}
p_m(\bm{z})=\frac{1}{2}p_d(\bm{z})+\frac{1}{2}p_l(\bm{z}_l)p_{t_1}(z_e)p_{t_2}(z_c),
\end{align}
where $p_l$ represents the marginal distribution of $p_d$ with respect to the latent feature $\bm{z}_l$, and $p_{t_1}$ and $p_{t_2}$ are the truncated normal distributions for the reconstruction features $z_e$ and $z_c$, respectively.  $p_m$ is substituted for $p_d$ in \eqref{nce} during the optimization of the estimation network. The following proposition (proof in Appendix~\ref{proposition_proof}) provides a quantitative justification for the data augmentation strategy.
\begin{proposition}\label{prop}
The density of the marginal distribution of the reconstruction feature in $p_m$ is no less than the density of the corresponding marginal distribution of the noise distribution $p_n$ over the interval $[0,m_z]$.
\end{proposition}

\subsection{Implementation}
Taking into account the parameterized noise distribution, we adopt an alternating optimization strategy with the batch loss for the estimation network $T_{\bm\theta}$ as
\begin{equation}\label{correct_nce}
-\frac{1}{M}\sum_{i=1}^M\ln\sigma(T_{\bm\theta}(\bm{z}_i))-\frac{\nu}{N}\sum_{i=1}^N\ln(1-\sigma(T_{\bm\theta}(L_{\bm K}(\bm{v}_i)))),
\end{equation}
where $\nu=\frac{N}{M}$ is the noise-sample ratio, $\bm{z}_1,\cdots,\bm{z}_M$ is sampled from the augmented data $\bm z_i \sim p_m$ and $\bm{v}_1,\cdots, \bm{v}_N$ is sampled from the noise distribution $\bm v_i \sim p_n$. The batch loss for the parameter matrix is then
 \begin{align}\notag
&-\frac{1}{M}\sum_{i=1}^M\ln\sigma(T_{\bm\theta}( L_{\bm K}(\textbf{sg}(L_{\bm K}^{-1}\left(\bm{z}_i\right)))))\\\label{addnce}
&\quad -\frac{\nu}{N}\sum_{i=1}^N\ln(1-\sigma(T_{\bm\theta}(L_{\bm K}(\bm{v}_i)))).
 \end{align}
Furthermore, we constrain $\bm{K}$ to be a diagonal matrix with diagonal elements equal to or greater than $1$. This constraint enhances training stability (noise variance can only grow) and facilitates a more efficient computation of the inverse of the affine transformation $L_{\bm K}$. In practice, the constraint is enforced via softplus $K_{jj}=1+\log(1+\exp(\psi_j)),\quad  \bm{\psi}\in\mathbb{R}^d$. 
AdamW is used for both sets of parameters $\bm{\theta}$ and $\bm{\psi}$. 

\section{Experiments}
In this section, we utilize benchmark datasets to empirically assess the effectiveness of our proposed method in unsupervised anomaly detection tasks.
\subsection{Datasets and Evaluation Metric}
\paragraph{Datasets.}\textbf{MNIST} \cite{lecun2010mnist} is a grayscale image dataset with $10$ classes containing digits from $0$ to $9$. It consists of $60,000$ training images and $10,000$ test images, each $28\times28$ pixels. \textbf{MNIST-C} \cite{mu2019mnist} is a comprehensive suite of 15 corruptions applied to the MNIST test set (along with the original set), for benchmarking out-of-distribution robustness in computer vision. \textbf{CIFAR-10} \cite{krizhevsky2010cifar} is a color image dataset with $10$ classes. It includes $50,000$ training images and $10,000$ test images, each $32\times32$ pixels. 

\paragraph{Training Dataset.} Following prior work \cite{abati2019latent}, we use the labeled image datasets to create \textbf{unimodal} anomaly datasets where one class is normal and the rest as anomalies. Only normal data in the training set is seen during training and model selection. The whole test dataset is employed at testing. For \textbf{multimodal} datasets, two or more classes are considered normal. Again, data from these classes in the training set is used for training and the entire test set is for testing. 
Finally, for MNIST-C we adopt the settings from \cite{lee2023semi}: the entire MNIST training dataset is used for model training, while the MNIST-C \cite{mu2019mnist} dataset is utilized for testing, such that the original MNIST images are considered normal and corrupted images in MNIST-C are deemed abnormal.

\paragraph{Evaluation Metric.} Anomaly detection performance is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC), as is common in prior work \cite{abati2019latent, ruff2018deep}. We perform each experiment five times and report the mean and the standard deviation of the AUROC.

\subsection{Ablation Study}
We conduct an ablation study to evaluate the contributions of the individual
components of our proposed method designated as follows:
\begin{itemize}
\item CANCE: NCE on the composite feature while augmenting with artificial reconstruction features \textbf{(proposed)}
\item CNCE: NCE on the composite features
\item LatNCE: NCE on the AE's latent features 
\item Error: AE's error as the anomaly score
\end{itemize}

\begin{table}[tbh]
\caption{Ablation study on unimodal training dataset}
\label{ablation-unimode}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{cccccc}
\hline
Data &Error & LatNCE & CNCE & CANCE\\
\hline
\multicolumn{3}{l}{MNIST}\\
$0$ &$99.4\pm0.1$ & $85.0\pm3.5$ & $99.5\pm0.0$ & $99.6\pm0.0$ \\
$1$ &$99.9\pm0.0$ & $98.4\pm0.3$ & $99.8\pm0.0$ & $99.8\pm0.0$ \\
$2$ &$90.3\pm1.5$ & $81.9\pm6.5$ & $96.6\pm0.7$ & $97.1\pm0.5$ \\
$3$ &$92.6\pm0.6$ & $81.2\pm1.2$ & $95.6\pm0.4$ & $96.7\pm0.3$ \\
$4$ &$95.5\pm1.3$ & $75.0\pm3.8$ & $95.8\pm0.3$ & $96.9\pm0.4$ \\
$5$ &$95.1\pm1.5$ & $76.1\pm4.8$ & $96.1\pm0.4$ & $97.2\pm0.4$ \\
$6$ &$98.9\pm0.3$ & $88.2\pm4.3$ & $99.2\pm0.0$ & $99.4\pm0.0$ \\
$7$ &$96.1\pm0.6$ & $89.1\pm2.1$ & $96.9\pm0.6$ & $97.5\pm0.3$ \\
$8$ &$85.8\pm0.4$ & $80.6\pm1.3$ & $94.6\pm0.5$ & $95.6\pm0.3$ \\
$9$ &$97.0\pm0.3$ & $86.0\pm1.5$ & $96.2\pm0.2$ & $97.1\pm0.1$ \\
Avg &$95.1$ & $84.1$ & $97.0$ & $97.7$ \\
\hline
\multicolumn{3}{l}{CIFAR-10}\\
$0$ &$57.8\pm2.3$ & $63.4\pm2.0$ & $63.4\pm2.3$ & $63.8\pm2.3$ \\
$1$ &$33.8\pm1.5$ & $63.4\pm1.2$ & $63.4\pm0.7$ & $63.6\pm0.8$ \\
$2$ &$65.0\pm0.4$ & $59.5\pm1.8$ & $59.9\pm1.8$ & $60.0\pm1.7$ \\
$3$ &$54.6\pm0.5$ & $62.3\pm1.9$ & $62.2\pm2.0$ & $62.3\pm2.1$ \\
$4$ &$71.0\pm0.8$ & $68.9\pm1.6$ & $69.3\pm1.7$ & $69.4\pm1.5$ \\
$5$ &$54.6\pm0.8$ & $61.1\pm1.1$ & $61.5\pm1.2$ & $61.6\pm1.1$ \\
$6$ &$55.2\pm2.9$ & $73.0\pm2.4$ & $73.0\pm2.4$ & $73.0\pm2.3$ \\
$7$ &$44.7\pm0.7$ & $61.1\pm1.1$ & $61.3\pm1.1$ & $61.2\pm1.0$ \\
$8$ &$67.8\pm0.8$ & $71.9\pm2.1$ & $72.4\pm2.1$ & $72.3\pm2.3$ \\
$9$ &$36.4\pm1.1$ & $66.3\pm1.6$ & $66.9\pm1.7$ & $66.5\pm1.5$ \\
Avg &$54.1$ & $65.1$ & $65.3$ & $65.4$ \\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}

The results for the unimodal cases of MNIST and CIFAR-10 datasets are presented in Table~\ref{ablation-unimode}. In almost all cases, CNCE achieves higher AUROC values than Error (with similar performance on MNIST and CNCE providing superior performance on CIFAR-10), which highlights the importance of latent features. Moreover, CNCE consistently outperforms LatNCE, with superior performance on MNIST, and very similar performance on the CIFAR-10 dataset. Nonetheless, CNCE has significantly higher mean performance across the 10 classes at a significance threshold of $0.01$ for a one-sided Wilcoxon signed-rank test (p-value of $0.00488$). Finally, CANCE performs slightly better than CNCE (equal or better mean performance on 17 of the 20 datasets).

\begin{table}[htb]
\caption{Ablation study on the multimodal training dataset}
\label{ablation-multimodal}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{ccccc}
\hline
 Data &Error &LatNCE &CNCE &CANCE\\
\hline
\multicolumn{5}{l}{MNIST }\\
$0,1$ &$99.4\pm0.1$ & $93.8\pm1.1$ & $99.5\pm0.1$ & $99.6\pm0.1$\\
$0,8$ &$87.8\pm0.7$ & $79.8\pm1.3$ & $95.0\pm0.5$ & $95.5\pm0.4$\\
$1,8$ &$96.7\pm0.7$ & $89.3\pm2.3$ & $97.4\pm0.3$ & $97.8\pm0.2$\\
Avg &$94.6$ & $87.6$ & $97.3$ & $97.6$\\
\hline
\multicolumn{5}{l}{CIFAR-10}\\
$0,1$ &$44.4\pm1.3$ & $53.9\pm0.9$ & $54.5\pm1.3$ & $54.8\pm1.1$\\
$0,8$ &$65.7\pm1.7$ & $64.4\pm5.2$ & $64.6\pm5.7$ & $64.7\pm5.7$\\
$1,8$ &$48.9\pm0.7$ & $63.3\pm1.8$ & $63.4\pm1.7$ & $63.1\pm1.5$\\
Avg &$53.0$ & $60.5$ & $60.8$ & $60.9$ \\
\hline
\multicolumn{3}{l}{MNIST-C}\\
&$89.7\pm0.5$ & $78.4\pm1.4$ & $91.3\pm0.4$ & $92.2\pm0.4$ \\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}

The AUROC values from the ablation study on multimodal datasets are reported in Table \ref{ablation-multimodal}, where the last row corresponds to training the model on the entire MNIST training dataset and evaluating it on the MNIST-C dataset. CANCE consistently achieves the best or nearly best performance, demonstrating the contribution of each of its components. 

\subsection{Results on Unimodal MNIST and CIFAR-10}
We consider the following baseline methods based on the fact that they are similar to CANCE in that they use probability models and/or reconstruction models with minimal data pre-processing:
\begin{itemize}
\item KDE: Kernel Density Estimator after PCA-whitening;
\item VAE: variational autoencoder~\cite{kingma2013auto}, Evidence Lower Bound (ELBO) is anomaly score;
\item Pix-CNN \cite{van2016conditional} uses density modeling by  autoregression in the image space;
\item LSA: Latent Space Autoregression~\cite{abati2019latent};
\item DAGMM \cite{zong2018deep} uses composite features using latent representation and reconstruction feature with  density estimation performed by jointly training an AE and Gaussian mixture model.
\end{itemize}
Except for the last two methods, AUROC values on MNIST and CIFAR-10 are extracted from previous literature \cite{abati2019latent}. Since DAGMM is not evaluated on MNIST and CIFAR-10 in \cite{zong2018deep}, we use the same architecture as our method. In all cases, the model is trained for $400$ epochs with a fixed learning rate of $10^{-4}$, and the number of Gaussians within the model is set to $4$. The best model is saved when the lowest validation loss is achieved. However, on CIFAR-10, we have encountered issues with degenerated covariance matrices in the DAGMM. Therefore, we have changed the latent dimension from $64$ to $16$. In the process of implementing DAGMM, we find that DAGMM does not train stably if the latent dimension or the number of Gaussians within the model is not properly set up. We also perform an additional comparison with DAGMM using the same dataset and neural network. The results and analysis are provided in Appendix \ref{dagmm-exp}.


\begin{table}[hbt]
\caption{AUCROC [\%] for baseline methods, Pix-CNN (PC) and DAG (DAGMM), compared to CANCE mean and std. value across $5$ independent runs. }
\label{comparision}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{cccccc|cc}
\hline
 Data&KDE &VAE &PC  &LSA &DAG  &CANCE\\
\hline
\multicolumn{3}{l}{MNIST}\\
$0$ &$88.5$  &$99.8$ & $53.1$ &$99.3$ & $53.6$ & $99.6\pm0.0$\\
$1$ &$99.6$  &$99.9$ & $99.5$ &$99.9$ & $51.5$ & $99.8\pm0.0$\\
$2$ &$71.0$  &$96.2$ & $47.6$ &$95.9$ & $53.5$ & $97.1\pm0.5$\\
$3$ &$69.3$  &$94.7$ & $51.7$ &$96.6$ & $49.7$ & $96.7\pm0.3$\\
$4$ &$84.4$  &$96.5$ & $73.9$ &$95.6$ & $52.7$ & $96.9\pm0.4$\\
$5$ &$77.6$  &$96.3$ & $54.2$ &$96.4$ & $54.3$ & $97.2\pm0.4$\\
$6$ &$86.1$  &$99.5$ & $59.2$ &$99.4$ & $55.2$ & $99.4\pm0.0$\\
$7$ &$88.4$  &$97.4$ & $78.9$ &$98.0$ & $53.8$ & $97.5\pm0.3$\\
$8$ &$66.9$  &$90.5$ & $34.0$ &$95.3$ & $54.8$ & $95.6\pm0.3$\\
$9$ &$82.5$  &$97.8$ & $66.2$ &$98.1$ & $51.8$ & $97.1\pm0.1$\\
Avg &$81.4$  &$96.9$ & $61.8$ &$97.5$ & $53.1$ & $97.7$\\
\hline
\multicolumn{3}{l}{CIFAR-10}\\
$0$ &$65.8$ &$68.8$ & $78.8$ &$73.5$ & $47.5$ & $63.8\pm2.3$\\
$1$ &$52.0$ &$40.3$ & $42.8$ &$58.0$ & $47.2$ & $63.6\pm0.8$\\
$2$ &$65.7$ &$67.9$ & $61.7$ &$69.0$ & $46.1$ & $60.0\pm1.7$\\
$3$ &$49.7$ &$52.8$ & $57.4$ &$54.2$ & $47.3$ & $62.3\pm2.1$\\
$4$ &$72.7$ &$74.8$ & $51.1$ &$76.1$ & $48.5$ & $69.4\pm1.5$\\
$5$ &$49.6$ &$51.9$ & $57.1$ &$54.6$ & $48.9$ & $61.6\pm1.1$\\
$6$ &$75.8$ &$69.5$ & $42.2$ &$75.1$ & $47.8$ & $73.0\pm2.3$\\
$7$ &$56.4$ &$50.0$ & $45.4$ &$53.5$ & $47.6$ & $61.2\pm1.0$\\
$8$ &$68.0$ &$70.0$ & $71.5$ &$71.7$ & $48.4$ & $72.3\pm2.3$\\
$9$ &$54.0$ &$39.8$ & $42.6$ &$54.8$ & $48.1$ & $66.5\pm1.5$\\
Avg &$61.0$ &$58.6$ & $55.1$ &$64.1$ & $47.7$ & $65.4$\\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}

Table \ref{comparision} details the AUROC performance of each method. As shown in Table \ref{comparision}, our proposal outperforms all baselines tested across both datasets. All methods except DAGMM and Pix-CNN perform favorably on MNIST. DAGMM completely fails because it is not designed for image dataset, as noted in other work \cite{hojjati2023dasvdd}. Pix-CNN struggles to model distributions, which partly supports our previous argument that the true effective dimensionality is significantly smaller than the image dimensionality, and thus data density functions may not exist in image space. Notably, the deep probability models, including VAE, LSA, and CANCE, achieve better performance than KDE on MNIST, but CIFAR-10 presents a much greater challenge due to the higher diversity of classes and the complex backgrounds in which the class objects are depicted. Although more general data augmentation has proven effective for improving model performance on this dataset \cite{golan2018deep}, it is beyond the scope of this paper, as our model and other baselines do not incorporate it. 

\subsection{Results on Multimodal MNIST-C}
For the MNIST-C dataset, we compare CANCE to  SVDD \cite{tax2004support}, Deep SVDD \cite{ruff2018deep}, and Deep SAD \cite{ruff2019deep}, which were previously reported in Table 3 of \cite{lee2023semi} and DROCC \cite{goyal2020drocc}. For CANCE, we use the same network structure and hyperparameters as in the unimodal case, except for increasing the latent dimension from $6$ to $10$ to accommodate the more complex training dataset consisting of 10 class/`modes'.  We also execute DROCC, a state-of-the-art method, $30$ times as the baseline. DROCC trains a robust classifier by adaptively generating negative samples via adversarially ascending the classifier loss. The output value of the classifier is then used as the anomaly score. For the network architecture, we adapt the published version used by DROCC for CIFAR-10, with the following modifications: changing the input image channel from $3$ to $1$ and adjusting the latent dimension from $128$ to $32$. The testing results are shown in Table \ref{mnist-c}. CANCE and DROCC are comparable and outperform other methods. 

\begin{table}[ht]
\caption{Anomaly detection on MNIST-C, AUROC over $30$ runs}
\label{mnist-c}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{lllll}
\hline
SVDD &DSVDD &DSAD &DROCC &CANCE\\
 \hline
$67.6$ & $82.8$ & $84.0$ &$92.3\pm1.9$ & $92.2\pm0.4$\\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}


\subsection{ResNet-18 as Feature Extractor}
To demonstrate the generality of our method, we also tested it using a pretrained ResNet-18, followed by PCA, to prepare the composite feature and perform density estimation as previously conducted. Similarly, we summarize all AUROC values in Table \ref{features_ResNet-18_mnist_cifar10}. 
\begin{table}[htb]
\caption{Ablation study on features extracted by ResNet-18 on MNIST and CIFAR-10}
\label{features_ResNet-18_mnist_cifar10}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{ccccc}
\hline
Data &Error  & LatNCE & CNCE & CANCE\\
\hline
\multicolumn{3}{l}{MNIST}\\
0 & $98.1\pm0.0$ & $78.4\pm0.9$ & $96.7\pm0.6$ & $98.6\pm0.0$ \\
1 & $99.8\pm0.0$ & $98.6\pm0.1$ & $99.6\pm0.0$ & $99.7\pm0.0$ \\
2 & $88.3\pm0.0$ & $74.4\pm0.7$ & $83.8\pm1.4$ & $91.0\pm0.1$ \\
3 & $95.1\pm0.0$ & $74.1\pm0.5$ & $92.5\pm0.3$ & $94.7\pm0.2$ \\
4 & $97.3\pm0.0$ & $84.4\pm0.5$ & $96.3\pm0.3$ & $97.3\pm0.2$ \\
5 & $92.2\pm0.0$ & $60.8\pm0.2$ & $87.7\pm0.5$ & $91.5\pm0.4$ \\
6 & $94.8\pm0.0$ & $72.7\pm0.8$ & $93.5\pm0.2$ & $94.6\pm0.2$ \\
7 & $96.8\pm0.0$ & $85.5\pm1.2$ & $96.3\pm0.3$ & $96.8\pm0.3$ \\
8 & $91.0\pm0.0$ & $75.4\pm1.1$ & $85.8\pm1.5$ & $92.1\pm0.5$ \\
9 & $92.0\pm0.0$ & $65.5\pm0.6$ & $89.1\pm0.4$ & $93.3\pm0.2$ \\
Avg & $94.5$ & $77.0$ & $92$ & $95.0$ \\
\hline
\multicolumn{3}{l}{CIFAR-10}\\
0 & $88.5\pm0.1$ & $75.5\pm0.8$ & $80.9\pm1.6$ & $86.5\pm0.4$ \\
1 & $94.6\pm0.0$ & $90.9\pm0.3$ & $93.5\pm0.6$ & $95.3\pm0.1$ \\
2 & $77.0\pm0.1$ & $60.0\pm0.2$ & $63.9\pm3.1$ & $75.1\pm0.3$ \\
3 & $80.3\pm0.1$ & $71.3\pm1.6$ & $75.1\pm1.7$ & $78.8\pm0.7$ \\
4 & $90.2\pm0.0$ & $81.2\pm0.8$ & $85.8\pm1.0$ & $89.2\pm0.3$ \\
5 & $84.1\pm0.0$ & $68.6\pm1.2$ & $76.9\pm2.7$ & $87.0\pm0.4$ \\
6 & $90.4\pm0.1$ & $80.5\pm0.7$ & $85.7\pm0.9$ & $88.9\pm0.9$ \\
7 & $86.5\pm0.1$ & $76.0\pm0.5$ & $85.4\pm2.7$ & $91.1\pm0.4$ \\
8 & $91.7\pm0.1$ & $83.1\pm0.6$ & $88.7\pm0.3$ & $92.4\pm0.3$ \\
9 & $94.6\pm0.0$ & $88.7\pm0.9$ & $93.4\pm0.8$ & $95.8\pm0.1$ \\
Avg & $87.8$ & $77.6$ & $82.9$ & $88.0$ \\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}
As before, CNCE consistently outperforms LatNCE, highlighting the effectiveness of including reconstruction error in density estimation. Additionally, augmenting indeed helps detect anomalies, as evidenced by the gap between CANCE and CNCE. However, unlike in Table \ref{ablation-unimode}, CANCE only shows comparable performance with Error on average. There are two potential reasons for this: first, the latent feature extracted by PCA is simple and thus less useful compared to AE; second, the density estimator induced by NCE does not capture data distribution well enough for anomaly detection. 
\begin{table}[htb]
\caption{AUROC[\%] on CIFAR-10 for baseline methods. Nearest neighbor (NN) baseline uses either original space or like CANCE the ResNet-18 features.}
\label{features_ResNet-18}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{cccccc}
\hline
Data & DSVDD & NN & DROCC & CANCE\\
\hline
0 & $61.7\pm4.1$ & $69.0\mid80.0$ & $81.7\pm0.2$ & $86.5\pm0.4$ \\
1 & $65.9\pm2.1$ & $44.2\mid90.5$ & $76.7\pm1.0$ & $95.3\pm0.1$ \\
2 & $50.8\pm0.8$ & $68.3\mid64.7$ & $66.7\pm1.0$ & $75.1\pm0.3$ \\
3 & $59.1\pm1.4$ & $51.3\mid71.5$ & $67.1\pm1.5$ & $78.8\pm0.7$ \\
4 & $60.9\pm1.1$ & $76.7\mid83.8$ & $73.6\pm2.0$ & $89.2\pm0.3$ \\
5 & $65.7\pm2.5$ & $50.0\mid70.0$ & $74.4\pm2.0$ & $87.0\pm0.4$ \\
6 & $67.7\pm2.6$ & $72.4\mid83.0$ & $74.4\pm0.9$ & $88.9\pm0.9$ \\
7 & $67.3\pm0.9$ & $51.3\mid76.7$ & $74.3\pm0.2$ & $91.1\pm0.4$ \\
8 & $75.9\pm1.2$ & $69.0\mid82.8$ & $80.0\pm1.7$ & $92.4\pm0.3$ \\
9 & $73.1\pm1.2$ & $43.3\mid87.5$ & $76.2\pm0.7$ & $95.8\pm0.1$ \\
Avg &$64.8$  &$59.5\mid79.1$  & $74.2$ & $88.0$ \\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}

Finally, we compare compare versus baselines in Table~\ref{features_ResNet-18}. CANCE on top of ResNet-18 has much higher performance than an AE space, outperforming other methods by a wide margin and achieving state-of-the-art performance. We see an improved performance using ResNet-18 features for the nearest neighbor (NN) baseline too, which was shown to be the second best method to DROCC~\cite{goyal2020drocc}. Admittedly, we did not train a DROCC network on top of ResNet-18 representation.

\subsection{Validation on Tabular Data}
We also validate our CANCE on two tabular dataset: Abalone and Thyroid. Consistent with previous research \cite{zong2018deep,goyal2020drocc}, we utilize the F1-score to compare the methods and adhere to their guidelines in preparing the dataset.
% drocc 
\begin{table}[htb]
\caption{Anomaly detection on tabular dataset.}
\label{drocc}
%\vskip 0.15in
\begin{center}
% \begin{small}
\begin{tabular}{lcc}
\hline
 Method &Abalone &Thyroid \\
 \hline
DAGMM & $0.20\pm0.03$  & $0.49\pm0.04$\\
DeepSVDD & $0.62\pm0.01$  & $0.73\pm0.00$ \\
GOAD  & $0.61\pm0.02$ &  $0.72\pm0.01$ \\
DROCC & $0.68\pm0.02$ &  $0.78\pm0.03$ \\
CANCE &$0.79\pm0.06$  & $0.73\pm0.02$ \\
\hline
\end{tabular}
% \end{small}
\end{center}
\end{table}
CANCE outperforms the previous methods by a wide margin on Abalone, but is worse than DROCC and comparable to DeepSVDD and GOAD \cite{bergman2020classification} on Thyroid. 


\section{Conclusion}
In this work, we propose an innovative two-stage approach for detecting anomalies within an unsupervised learning framework. Our approach, in contrast to other complex deep probability models, is relatively straightforward. We train a constrained AE to capture low-dimensional features and construct a classifier on top of it trained to distinguish Gaussian noise from normal data. Experimental evaluations on multiple benchmark datasets demonstrate that our proposed approach matches the performance of leading state-of-the-art anomaly detection algorithms.


\section*{Acknowledgments}
The research at the University of Delaware was sponsored by the Department of the Navy, Office of Naval Research, under ONR award numbers N00014-21-1-2300 and N00014-24-1-2259. This research was supported in part through the use of Information Technologies (IT) resources at the University of Delaware, specifically the high-performance computing resources.

%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  

\newpage
\appendix
\section{Mean and Variance Derivation}\label{iterative_mean}
By the definition of the sample mean, we have
\begin{align}\label{update_mean}
\hat{\bm\mu}_{\bm z}^{(t+1)}&=\frac{1}{n_t+n_b}\sum_{i=1}^{n_t+n_b}\bm{z}_i\nonumber\\
&=\frac{1}{n_t+n_b}\left(\sum_{i=1}^{n_t}\bm{z}_i+\sum_{i=n_t+1}^{n_t+n_b}\bm{z}_i\right)\nonumber\\
&=\frac{n_t\hat{\bm\mu}_{\bm z}^{(t)}+n_b\hat{\bm\mu}_{\bm z}^{(b)}}{n_t+n_b}
\end{align}
By definition, the sample covariance matrix is as follows:
\begin{align*}
\hat{\bm\Sigma}_{\bm z}^{(t+1)}&=\frac{1}{n_t+n_b}\sum_{i=1}^{n_t+n_b}
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=\frac{1}{n_t+n_b}\left[\sum_{i=1}^{n_t}
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)+\sum_{i=n_t+1}^{n_t+n_b}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\right]
\end{align*}
Consider expanding the first sum term as follows,
\begin{align*}
\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)&=\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}+
\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}+\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)^T
\left(\bm {z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)\quad+\sum_{i=1}^{n_t}
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&+\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\sum_{i=1}^{n_t}\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)\\
\end{align*}
By calculation, we see
\begin{align*}
\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)&=\left[\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)^T\right]\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=\left[\sum_{i=1}^{n_t}\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right]^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=\left[\sum_{i=1}^{n_t}\bm{z}_i-\sum_{i=1}^{n_t}\hat{\bm\mu}_{\bm z}^{(t)}\right]^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=[n_t\hat{\bm\mu}_{\bm z}^{(t)}-n_t\hat{\bm\mu}_{\bm z}^{(t)}]^T\left(\hat{\bm\mu}_{\bm z}^{(t)}
-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=\bm0
\end{align*}
Similarly,
\begin{align*}
\sum_{i=1}^{n_t}\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm{z}}^{(t)}\right)=\bm0.
\end{align*}
Thus, 
\begin{align*}
&\quad\sum_{i=1}^{n_t}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=n_t\hat{\bm\Sigma}_{\bm z}^{(t)}+\sum_{i=1}^{n_t}
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=n_t\hat{\bm\Sigma}_{\bm z}^{(t)}+n_t\left(\hat{\bm\mu}_{\bm z}^{(t)}
-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=n_t\hat{\bm\Sigma}_{\bm z}^{(t)}+n_t\left(\hat{\bm\mu}_{\bm z}^{(t)}-
\frac{n_t\hat{\bm\mu}_{\bm z}^{(t)}+n_b\hat{\bm\mu}_{\bm z}^{(b)}}{n_t+n_b}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(t)}-\frac{n_t\hat{\bm\mu}_{\bm z}^{(t)}+n_b\hat{\bm\mu}_{\bm z}^{(b)}}{n_t+n_b}\right)\\
&=n_t\hat{\bm\Sigma}_{\bm z}^{(t)}+\frac{n_tn_b^2}{(n_t+n_b)^2}\left(\hat{\bm\mu}_{\bm z}^{(t)}
-\hat{\bm\mu}_{\bm z}^{(b)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(b)}\right)
\end{align*}
Applying the same techniques to the second sum term, we obtain
\begin{align*}
\sum_{i=n_t+1}^{n_t+n_b}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)&=\sum_{i=n_t+1}^{n_t+n_b}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}+\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}+\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=\sum_{i=n_t+1}^{n_t+n_b}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}\right)^T
\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}\right)+\sum_{i=n_t+1}^{n_t+n_b}\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&\quad+\sum_{i=n_t+1}^{n_t+n_b}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)+\sum_{i=n_t+1}^{n_t+n_b}\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(t)}\right)\\
&=\sum_{i=n_t+1}^{n_t+n_b}\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}\right)^T\left(\bm{z}_i-\hat{\bm\mu}_{\bm z}^{(b)}\right)+\sum_{i=n_t+1}^{n_t+n_b}\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=n_b\hat{\bm\Sigma}_{\bm z}^{(b)}+n_b\left(\hat{\bm\mu}_{\bm z}^{(b)}
-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t+1)}\right)\\
&=n_b\hat{\bm\Sigma}_{\bm z}^{(b)}+n_b\left(\hat{\bm\mu}_{\bm z}^{(b)}
-\frac{n_t\hat{\bm\mu}_{\bm z}^{(t)}+n_b\hat{\bm\mu}_{\bm z}^{(b)}}{n_t+n_b}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(b)}-\frac{n_t\hat{\bm\mu}_{\bm z}^{(t)}+n_b\hat{\bm\mu}_{\bm z}^{(b)}}{n_t+n_b}\right)\\
&=n_b\hat{\bm\Sigma}_{\bm z}^{(b)}+\frac{n_t^2n_b}{(n_t+n_b)^2}\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t)}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t)}\right)\\
\end{align*}
Therefore, 
\begin{align}\label{update_cov}
\hat{\bm\Sigma}_{\bm z}^{(t+1)}&=\frac{1}{n_t+n_b}\left[n_t\hat{\bm\Sigma}_{\bm z}^{(t)}+\frac{n_tn_b^2}{(n_t+n_b)^2}\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(b)}\right)^T
\left(\hat{\bm\mu}_z^{(t)}-\hat{\bm\mu}_{\bm z}^{(b)}\right)+n_b\hat{\bm\Sigma}_{\bm z}^{(b)}
+\frac{n_t^2n_b}{(n_t+n_b)^2}\left(\hat{\bm\mu}_{\bm z}^{(b)}
-\hat{\bm\mu}_{\bm z}^{(t)}\right)^T\left(\hat{\bm\mu}_{\bm z}^{(b)}-\hat{\bm\mu}_{\bm z}^{(t)}\right)\right]\nonumber\\
&=\frac{n_t}{n_t+n_b}\hat{\bm\Sigma}_{\bm z}^{(t)}+\frac{n_b}{n_t+n_b}\hat{\bm\Sigma}_{\bm z}^{b}
+\frac{n_tn_b}{(n_t+n_b)^2}\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(b)}\right)^T
\left(\hat{\bm\mu}_{\bm z}^{(t)}-\hat{\bm\mu}_{\bm z}^{(b)}\right)
\end{align}
% In this work, equation (\ref{update_mean}) and (\ref{update_cov}) are used to estimate the sample mean and covariance matrix, respectively.

\section{Mode}\label{mode_dev}
Note that the mean, variance, and mode of a log normal distribution are given by the following formulas:
\begin{align*}
\mu_z=e^{\mu+\frac{1}{2}\sigma^2}, \sigma_z^2=\left(e^{\sigma^2}-1\right)e^{2\mu+\sigma^2},
m_z = e^{\mu-\sigma^2} 
\end{align*}
Then the mode $m_z$ can be expressed by the mean and variance,
\begin{align*}
m_z=e^{\mu+\frac{1}{2}\sigma^2-\frac{3}{2}\sigma^2}
=\mu_z\bigg(\frac{\sigma_z^2}{\mu_z^2}+1\bigg)^{-\frac{3}{2}}
\end{align*}
as
\begin{align*}
&\frac{\sigma_{z}^{2}}{\mu_{z}^{2}}=
\frac{\left(e^{\sigma^{2}}-1\right)e^{2\mu+\sigma^{2}}}{\left(e^{\mu+\frac{1}{2}\sigma^{2}}\right)^{2}}
=\frac{\left(e^{\sigma^{2}}-1\right)e^{2\mu+\sigma^{2}}}{e^{2\mu+\sigma^{2}}}
=e^{\sigma^{2}}-1\\&\Rightarrow e^{\sigma^{2}}
=\frac{\sigma_{z}^{2}}{\mu_{z}^{2}}+1\Rightarrow e^{-\frac{3}{2}\sigma^{2}}
=\left(\frac{\sigma_{z}^{2}}{\mu_{z}^{2}}+1\right)^{-\frac{3}{2}}
\end{align*}


\section{Proof of Proposition}\label{proposition_proof}
Note that the marginal distribution of the reconstruction feature in $p_m$ is
$\frac{1}{2}p_0(z)+\frac{1}{2}p_t(z)$. For any $z\in[0,m_z]$, we have
\begin{align*}
\frac{1}{2}p_0(z)+\frac{1}{2}p_t(z)&=\frac{1}{2}p_0(z)+\frac{1}{2}\cdot\frac{\frac{1}{\sqrt{2\pi\sigma_z^2}}
e^{-\frac{(z-m_z)^2}{2\sigma_z^2}}}{\Phi(\frac{m_z-m_z}{\sigma_z})
-\Phi(\frac{0-m_z}{\sigma_z})}\\
&=\frac{1}{2}p_0(z)+\frac{1}{2}\cdot\frac{\frac{1}{\sqrt{2\pi\sigma_z^2}}
e^{-\frac{(z-m_z)^2}{2\sigma_z^2}}}{\frac{1}{2}
-\Phi(\frac{0-m_z}{\sigma_z})}\\
&\geq\frac{1}{2}p_0(z)+\frac{1}{2}\cdot\frac{\frac{1}{\sqrt{2\pi\sigma_z^2}}
e^{-\frac{(z-m_z)^2}{2\sigma_z^2}}}{\frac{1}{2}}\\
&=\frac{1}{2}p_0(z)+\frac{1}{\sqrt{2\pi\sigma_z^2}}
e^{-\frac{(z-m_z)^2}{2\sigma_z^2}}\\
&\geq\frac{1}{2}p_0(z)+\frac{1}{\sqrt{2\pi\sigma_z^2}}
e^{-\frac{(z-\mu_z)^2}{2\sigma_z^2}}\\
&\geq\frac{1}{\sqrt{2\pi\sigma_z^2}}
e^{-\frac{(z-\mu_z)^2}{2\sigma_z^2}}\\
&=p_n(z)
\end{align*}
where the second inequality is according to the fact $m_z\leq\mu_z$.

\section{DAGMM}\label{dagmm-exp}
We conduct an extra comparison between CANCE and DAGMM \cite{zong2018deep} since both methods employ latent and reconstruction feature for anomaly detection. In this experiment, we use the same neural networks and training strategy as in DAGMM except three modifications: 1) DAGMM is only trained over $10$ epochs instead of $200$ epochs; 2) the reconstruction error is $\frac{\|\bm x-\bm x'\|^2}{d_0}$ rather than the relative Euclidean distance $\frac{\|\bm x-\bm x'\|}{\|\bm x\|}$; and the cosine dissimilarity $\frac{1}{2}\left(1-\frac{\bm x^T\bm x'}{\|\bm x\|\|\bm x'\|}\right)$ is utilized rather than the cosine similarity $\frac{\bm x^T\bm x'}{\|\bm x\|\|\bm x'\|}$. We observe that the training loss of DAGMM converges in less than $10$ epochs, making it unnecessary to train the model for $200$ epochs. Since the reconstruction error is used as an anomaly score instead of the relative Euclidean distance in AE, it is more reasonable to include the reconstruction error as a component of the composite feature. The reconstruction error is scaled by the data dimension $d_0$ to ensure its value remains small. Additionally, we choose cosine dissimilarity over cosine similarity because it is non-negative, similar to the reconstruction error.

We independently run the experiment $20$ times on the dataset KDDCUP99\footnote{Refer to the DAGMM paper for the implementation details.}, as done in DAGMM, and summarize the average precision, recall, and $F_1$ score in Table \ref{dagmm-nce}. The values for DAGMM-0 are extracted from Table 2 in the DAGMM paper. The results for DAGMM-1 were obtained by training DAGMM with three modifications as mentioned earlier. The last row, DAGMM-CANCE, involves training a DAGMM first for feature learning and then using our method, CANCE, for density estimation. Clearly, DAGMM-1 achieves better performance than DAGMM-0. Furthermore, DAGMM-CANCE achieves comparable results to DAGMM-1. We argue that separately optimizing the compression network and estimation network will not degrade the method’s performance, provided they are well designed and optimized. 

\begin{table}[t]
\caption{Anomaly detection results on contaminated training data from KDDCUP99}
\label{dagmm-nce}
%\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{cccc}
\hline
Method & Precision & Recall & $F_1$\\
\hline
DAGMM-0 & $93.0$ & $94.4$ & $93.7$  \\
DAGMM-1 & $97.7\pm0.3$  & $96.9\pm0.6$  & $97.3\pm0.5$ \\
DAGMM-CANCE & $97.6\pm0.3$  & $96.9\pm0.6$ & $97.3\pm0.5$ \\ 
\hline
\end{tabular}
\end{small}
\end{center}
\end{table}


\end{document}
