\section{Related Work}
Various strategies for anomaly detection are explored by approximating the density function of normal instances \cite{abati2019latent}, where anomalies are identified by their low modeling probabilities. A straightforward approach involves using statistical models, such as Gaussian distribution \cite{jain1988algorithms} and Gaussian mixture model (GMM) \cite{bishop1994novelty}, to fit the training dataset and valuate the log-likelihood of a test point as its anomaly score. For modeling complex distributions, non-parametric density estimators, like Kernel Density Estimation (KDE) \cite{parzen1962estimation} and histogram estimators, have been developed. KDE stands out as the most commonly employed classic density estimator partially because it has theoretical advantages over histograms \cite{devroye1985nonparametric} and addresses practical challenges related to fitting and parameter selection in GMM \cite{fruhwirth2006finite}. KDE, equipped with a more recent adaptation capable of handling varying levels of outliers in the training data \cite{kim2012robust,vandermeulen2013consistency}, has remained a popular approach for anomaly detection. 

Although KDE and GMM perform reasonably well in low-dimensional scenarios, both suffer from the curse of dimensionality \cite{ruff2021unifying}. Additionally, while these classic approaches for anomaly detection work well when they can exploit meaningful feature representations, for domains such as images, directly applying these methods yields poor performance. Instead, density estimation or parametric modeling can be applied to the latent learning representations of AEs \cite{andrews2016detecting,  sabokrou2018deep, xu2015learning} as is common in prior work. This is supported by the fact that the true effective dimensionality is significantly smaller than the image dimensionality \cite{ruff2021unifying}. 

%\cite{zong2018deep} noted that anomalous inputs may share some critical features with normal inputs and be well reconstructed. To address this,

Almost all prior work on using AE for anomaly detection have relied on either scores derived from latent features or from reconstruction error. One prior work the Autoencoding Gaussian mixture model (DAGMM) \cite{zong2018deep} also integrates latent and reconstruction features for anomaly detection,  wherein an AE and a GMM are jointly optimized for their parameters. Like DAGMM, we incorporate both latent features and reconstruction errors for anomaly detection. The key difference in our approach is that we adopt noise contrast estimation as non-parametric machine learning based approach for density estimation, which allows us to sidestep the challenges associated with forming a GMM, including specifying the number of mixture components in the DAGMM.

Alternatives to AEs include, deep generative model techniques that enable modeling more complicated `normal' data to enhance anomaly detection. While deep energy-based models~\cite{zhai2016deep} have been used, their reliance on Markov chain Monte Carlo (MCMC) sampling creates computationally expensive training. Alternatively, autoregressive models \cite{salimans2017pixelcnn++} and flow-based generative models \cite{kingma2018glow} have been used to detect outliers 
via direct likelihood estimation. However, these approaches tend to assign high likelihood scores to anomalies as reported in recent literature \cite{choi2018waic,ren2019likelihood,yoon2024energy}.

Variational Autoencoders (VAEs) can approximate the distribution of normal data via Monte Carlo sampling from the prior, thereby making them effective tools for anomaly detection.  However, experiments in previous work \cite{nalisnick2018deep,xu2018unsupervised} have demonstrated that utilizing the reconstruction probability \cite{an2015variational} as an alternative can lead to improved performance.

Finally, Generative Adversarial Network (GAN) \cite{goodfellow2014generative} provide an implicit model of data distribution have been refined for application in anomaly detection \cite{di2019survey}. Most GAN-based approaches, such as AnoGAN \cite{schlegl2017unsupervised} and EGBAD \cite{zenati2018efficient}, assume that after training the generator can produce normal points from the latent space better than anomalies, and naturally the discriminator, trained to distinguish  between the generated data and the input data, could work as the anomaly measure.  However, the optimization of GAN-based methods is challenged by the failure to converge during training and mode collapse \cite{metz2016unrolled}.