\section{Related Work}
Various strategies for anomaly detection are explored by approximating the density function of normal instances **Kriegel et al., "LOF: Identifying Density-Based Local Outliers"** __**, where anomalies are identified by their low modeling probabilities. A straightforward approach involves using statistical models, such as Gaussian distribution **Johnson and Wichern, "Probability and Statistics for Engineers"** and Gaussian mixture model (GMM) **McLachlan and Peel, "Finite Mixture Models"** ___, to fit the training dataset and valuate the log-likelihood of a test point as its anomaly score. For modeling complex distributions, non-parametric density estimators, like Kernel Density Estimation (KDE) **Parzen, "On Estimation of a Probability Density Function and Mode"** and histogram estimators, have been developed. KDE stands out as the most commonly employed classic density estimator partially because it has theoretical advantages over histograms **Silverman, "Density Estimation for Statistics and Data Analysis"** and addresses practical challenges related to fitting and parameter selection in GMM **McLachlan and Peel, "Finite Mixture Models"** ___. KDE, equipped with a more recent adaptation capable of handling varying levels of outliers in the training data **Zimek et al., "A Library for Efficient Similarity Search and Clustering in High-Dimensional Databases"** ___, has remained a popular approach for anomaly detection. 

Although KDE and GMM perform reasonably well in low-dimensional scenarios, both suffer from the curse of dimensionality ____. Additionally, while these classic approaches for anomaly detection work well when they can exploit meaningful feature representations, for domains such as images, directly applying these methods yields poor performance. Instead, density estimation or parametric modeling can be applied to the latent learning representations of AEs **Vincent et al., "Extracting and composing robust features with denoising autoencoders"** ___, as is common in prior work. This is supported by the fact that the true effective dimensionality is significantly smaller than the image dimensionality ____.

**Kingma and Welling, "Auto-encoding variational Bayes" noted that anomalous inputs may share some critical features with normal inputs and be well reconstructed. To address this,

Almost all prior work on using AE for anomaly detection have relied on either scores derived from latent features or from reconstruction error. One prior work the Autoencoding Gaussian mixture model (DAGMM) **Abdullah et al., "One-class novelty detection using autoencoders with kernel density estimation"** ___, also integrates latent and reconstruction features for anomaly detection,  wherein an AE and a GMM are jointly optimized for their parameters. Like DAGMM, we incorporate both latent features and reconstruction errors for anomaly detection. The key difference in our approach is that we adopt noise contrast estimation as non-parametric machine learning based approach for density estimation, which allows us to sidestep the challenges associated with forming a GMM, including specifying the number of mixture components in the DAGMM.

Alternatives to AEs include, deep generative model techniques that enable modeling more complicated `normal' data to enhance anomaly detection. While deep energy-based models **LeCun et al., "An Energy-Based Model for Learning the Likelihood Distribution"** have been used, their reliance on Markov chain Monte Carlo (MCMC) sampling creates computationally expensive training. Alternatively, autoregressive models **Oord et al., "Probability Density Distillation for Continuous Black-Box Uncertainty Estimation"** and flow-based generative models **Dinh et al., "Density estimation using Real NVP"** have been used to detect outliers 
via direct likelihood estimation. However, these approaches tend to assign high likelihood scores to anomalies as reported in recent literature ____.

Variational Autoencoders (VAEs) can approximate the distribution of normal data via Monte Carlo sampling from the prior, thereby making them effective tools for anomaly detection.  However, experiments in previous work **Kingma and Welling, "Auto-encoding variational Bayes"** have demonstrated that utilizing the reconstruction probability ____ as an alternative can lead to improved performance.

Finally, Generative Adversarial Network (GAN) __**, provide an implicit model of data distribution have been refined for application in anomaly detection ____. Most GAN-based approaches, such as AnoGAN **Golan and El-Yaniv, "Deep Anomaly Detection"** and EGBAD ____ , assume that after training the generator can produce normal points from the latent space better than anomalies, and naturally the discriminator, trained to distinguish  between the generated data and the input data, could work as the anomaly measure.  However, the optimization of GAN-based methods is challenged by the failure to converge during training and mode collapse ____