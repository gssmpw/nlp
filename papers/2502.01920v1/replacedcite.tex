\section{Related Work}
Various strategies for anomaly detection are explored by approximating the density function of normal instances ____, where anomalies are identified by their low modeling probabilities. A straightforward approach involves using statistical models, such as Gaussian distribution ____ and Gaussian mixture model (GMM) ____, to fit the training dataset and valuate the log-likelihood of a test point as its anomaly score. For modeling complex distributions, non-parametric density estimators, like Kernel Density Estimation (KDE) ____ and histogram estimators, have been developed. KDE stands out as the most commonly employed classic density estimator partially because it has theoretical advantages over histograms ____ and addresses practical challenges related to fitting and parameter selection in GMM ____. KDE, equipped with a more recent adaptation capable of handling varying levels of outliers in the training data ____, has remained a popular approach for anomaly detection. 

Although KDE and GMM perform reasonably well in low-dimensional scenarios, both suffer from the curse of dimensionality ____. Additionally, while these classic approaches for anomaly detection work well when they can exploit meaningful feature representations, for domains such as images, directly applying these methods yields poor performance. Instead, density estimation or parametric modeling can be applied to the latent learning representations of AEs ____ as is common in prior work. This is supported by the fact that the true effective dimensionality is significantly smaller than the image dimensionality ____. 

%____ noted that anomalous inputs may share some critical features with normal inputs and be well reconstructed. To address this,

Almost all prior work on using AE for anomaly detection have relied on either scores derived from latent features or from reconstruction error. One prior work the Autoencoding Gaussian mixture model (DAGMM) ____ also integrates latent and reconstruction features for anomaly detection,  wherein an AE and a GMM are jointly optimized for their parameters. Like DAGMM, we incorporate both latent features and reconstruction errors for anomaly detection. The key difference in our approach is that we adopt noise contrast estimation as non-parametric machine learning based approach for density estimation, which allows us to sidestep the challenges associated with forming a GMM, including specifying the number of mixture components in the DAGMM.

Alternatives to AEs include, deep generative model techniques that enable modeling more complicated `normal' data to enhance anomaly detection. While deep energy-based models____ have been used, their reliance on Markov chain Monte Carlo (MCMC) sampling creates computationally expensive training. Alternatively, autoregressive models ____ and flow-based generative models ____ have been used to detect outliers 
via direct likelihood estimation. However, these approaches tend to assign high likelihood scores to anomalies as reported in recent literature ____.

Variational Autoencoders (VAEs) can approximate the distribution of normal data via Monte Carlo sampling from the prior, thereby making them effective tools for anomaly detection.  However, experiments in previous work ____ have demonstrated that utilizing the reconstruction probability ____ as an alternative can lead to improved performance.

Finally, Generative Adversarial Network (GAN) ____ provide an implicit model of data distribution have been refined for application in anomaly detection ____. Most GAN-based approaches, such as AnoGAN ____ and EGBAD ____, assume that after training the generator can produce normal points from the latent space better than anomalies, and naturally the discriminator, trained to distinguish  between the generated data and the input data, could work as the anomaly measure.  However, the optimization of GAN-based methods is challenged by the failure to converge during training and mode collapse ____.