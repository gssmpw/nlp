\section{Datasets and Experimental Details}\label{ap-a}

\subsection{Computing Environment} Our implementation is based on PyG \cite{fey2019fast}. The experiments are conducted on a single workstation with 8 RTX 3090 GPUs.

\subsection{Datasets}\label{ap-a2} Table \ref{tab:dataset} presents a summary of the statistics and characteristics of the datasets.

\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
    \item \textbf{GNN Benchmark} \cite{dwivedi2023benchmarking}\textbf{.}  \textbf{ZINC} contains molecular graphs with node features representing atoms and edge features representing bonds The task is to regress the constrained solubility (logP) of the molecule. \textbf{MNIST} and \textbf{CIFAR10} are adapted from image classification datasets, where each image is represented as an 8-nearest-neighbor graph of SLIC superpixels, with nodes representing superpixels and edges representing spatial relationships. The 10-class classification tasks follow the original image classification tasks. \textbf{PATTERN} and \textbf{CLUSTER} are synthetic datasets sampled from the Stochastic Block Model (SBM) for inductive node classification, with tasks involving sub-graph pattern recognition and cluster ID inference. For all datasets, we adhere to the respective training protocols and standard evaluation splits \cite{dwivedi2023benchmarking}.
    \item \textbf{Long-Range Graph Benchmark (LRGB)} \cite{dwivedi2022long,freitas2021large}\textbf{.} \textbf{Peptides-func} and \textbf{Peptides-struct} are atomic graphs of peptides from SATPdb, with tasks of multi-label graph classification into 10 peptide functional classes and graph regression for 11 3D structural properties, respectively. \textbf{PascalVOC-SP} and \textbf{COCO-SP} are node classification datasets derived from the Pascal VOC and MS COCO images by SLIC superpixelization, where each superpixel node belongs to a particular object class. We did not use PCQM-Contact in \cite{dwivedi2022long} as its download link was no longer valid. \textbf{MalNet-Tiny} \cite{freitas2021large} is a subset of MalNet with 5,000 function call graphs (FCGs) from Android APKs, where the task is to predict software type based on structure alone. For each dataset, we follow standard training protocols and splits \cite{dwivedi2022long,freitas2021large}.
    \item \textbf{Open Graph Benchmark (OGB)} \cite{hu2020open}\textbf{.} We also consider a collection of larger-scale datasets from OGB, containing graphs in the range of hundreds of thousands to millions: \textbf{ogbg-molhiv} and \textbf{ogbg-molpcba} are molecular property prediction datasets from MoleculeNet. ogbg-molhiv involves binary classification of HIV inhibition, while ogbg-molpcba predicts results of 128 bioassays in a multi-task setting. \textbf{ogbg-ppa} contains protein-protein association networks, where nodes represent proteins and edges encode normalized associations between them; the task is to classify the origin of the network among 37 taxonomic groups. \textbf{ogbg-code2} consists of abstract syntax trees (ASTs) from Python source code, with the task of predicting the first 5 subtokens of the function’s name. We maintain all the OGB standard evaluation settings \cite{hu2020open}.
\end{itemize}

\begin{table*}[h]
\vspace{-0.1 in}
	\centering
        % \footnotesize
         \caption{Overview of the datasets used for graph-level tasks \cite{dwivedi2023benchmarking,dwivedi2022long,hu2020open,freitas2021large}.}
         \resizebox{0.95\linewidth}{!}{
	\begin{tabular}{lcccccccc}
		\toprule
		{Dataset} & {\# graphs} & {Avg. \# nodes} & {Avg. \# edges} & {\# node/edge feats} & {Prediction level} & {Prediction task} & {Metric}\\
		% \cmidrule{2-3,4-5,6-7}
		\midrule %
        ZINC & 12,000 & 23.2 & 24.9 &28/1 & graph & regression & MAE \\
        MNIST & 70,000 & 70.6 & 564.5 &3/1 & graph & 10-class classif. & Accuracy \\
        CIFAR10 & 60,000 & 117.6 & 941.1 &5/1& graph & 10-class classif. & Accuracy \\
        PATTERN & 14,000 & 118.9 & 3,039.3 &3/1&  inductive node & binary classif. & Accuracy \\
        CLUSTER & 12,000 & 117.2 & 2,150.9 &7/1& inductive node & 6-class classif. & Accuracy \\
        \midrule %
        Peptides-func & 15,535 & 150.9 & 307.3 &9/3 & graph & 10-task classif. & Avg. Precision \\
        Peptides-struct & 15,535 & 150.9 & 307.3 &9/3& graph & 11-task regression & MAE \\
        PascalVOC-SP& 11,355& 479.4& 2,710.5& 14/2& inductive node& 21-class classif.& F1 score \\
        COCO-SP& 123,286& 476.9& 2,693.7& 14/2& inductive node& 81-class classif.& F1 score \\
        MalNet-Tiny& 5,000& 1,410.3& 2,859.9& 5/1& graph& 5-class classif.& Accuracy \\
        \midrule %
       ogbg-molhiv &41,127& 25.5& 27.5& 9/3& graph& binary classif.& AUROC \\
        ogbg-molpcba &437,929& 26.0 &28.1 &9/3 &graph &128-task classif. &Avg. Precision \\
        ogbg-ppa &158,100& 243.4 &2,266.1 &1/7 &graph &37-task classif. &Accuracy \\
        ogbg-code2& 452,741& 125.2 &124.2 &2/2 &graph &5 token sequence &F1 score \\
        \bottomrule
	\end{tabular}}
	\label{tab:dataset}
\end{table*}

\subsection{Hyperparameters and Reproducibility} 

Please note that we mainly follow the experiment settings of GraphGPS \cite{rampavsek2022recipe,tonshoff2023did}. For the hyperparameter selections of classic GNNs, in addition to what we have covered, we list other settings in Tables~\ref{tab:gcn-parameter1}, \ref{tab:gcn-parameter2}, \ref{tab:gin-parameter1}, \ref{tab:gin-parameter2}, \ref{tab:gatedgcn-parameter1}, \ref{tab:gatedgcn-parameter2}. Further details regarding hyperparameters can be found in our code.

In all experiments, we use the validation set to select the best hyperparameters. \textbf{GNN$^+$} denotes enhanced implementation of the GNN model. 
%We denote the tuned GNN model by \textbf{GNN$^+$}. 

Our code is available under the MIT License.


\begin{table*}[h]
	\centering
        % \scriptsize
        \small
         \caption{Hyperparameter settings of GCN$^+$ on benchmarks from \cite{dwivedi2023benchmarking}.}
          % \resizebox{0.7\linewidth}{!}{
	\begin{tabular}{lccccc}
		\toprule
		{Hyperparameter} & {ZINC} & {MNIST} & {CIFAR10} & {PATTERN} & {CLUSTER}\\
		% \cmidrule{2-3,4-5,6-7}
		\midrule
            \# GNN Layers & 12 & 6 & 5 & 12 & 12 \\
            Edge Feature Module & True & True & True & True & False \\
            Normalization & BN & BN & BN & BN & BN \\
            Dropout & 0.0 & 0.15 & 0.05 & 0.05 & 0.1 \\
            Residual Connections & True & True & True & True & True \\
            FFN & True & True & True & True & True \\
            PE & RWSE-32 & False & False & RWSE-32 & RWSE-20 \\
            Hidden Dim & 64 & 60 & 65 & 90 & 90 \\
            Graph Pooling & add & mean & mean & – & – \\
		\midrule
            Batch Size & 32 & 16 & 16 & 32 & 16 \\
            Learning Rate & 0.001 & 0.0005 & 0.001 & 0.001 & 0.001 \\
            \# Epochs & 2000 & 200 & 200 & 200  & 100 \\
            \# Warmup Epochs & 50 & 5 & 5 & 5 & 5 \\
            Weight Decay & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 \\
		\midrule
            \# Parameters & 260,177 & 112,570 & 114,345 & 517,219 & 516,674 \\
            Time (epoch) & 7.6s & 60.1s & 40.2s & 19.5s & 29.7s \\
        \bottomrule
\end{tabular}
% }
	\label{tab:gcn-parameter1}
\end{table*}

\begin{table*}[h]
	\centering
        % \scriptsize
         \caption{Hyperparameter settings of GCN$^+$ on LRGB and OGB datasets.}
         \resizebox{\linewidth}{!}{
	\begin{tabular}{lccccccccc}
	\toprule
	{Hyperparameter} & Peptides-func & Peptides-struct & PascalVOC-SP & COCO-SP & MalNet-Tiny & ogbg-molhiv & ogbg-molpcba & ogbg-ppa & ogbg-code2 \\
	% \cmidrule{2-3,4-5,6-7}
	\midrule
    \# GNN Layers & 3 & 5 & 14 & 18 & 8 & 4 & 10 & 4 & 4 \\
    Edge Feature Module & True & False & True & True & True & True & True & True & True \\
    Normalization & BN & BN & BN & BN & BN & BN & BN & BN & BN \\
    Dropout & 0.2 & 0.2 & 0.1 & 0.05 & 0.0 & 0.1 & 0.2 & 0.2 & 0.2 \\
    Residual Connections & False & False & True & True & True & False & False & True & True \\
    FFN & False & False & True & True & True & True & True & True & True \\
    PE & RWSE-32 & RWSE-32 & False & False & False & RWSE-20 & RWSE-16 & False & False \\
    Hidden Dim & 275 & 255 & 85 & 70 & 110 & 256 & 512 & 512 & 512 \\
    Graph Pooling & mean & mean & – & – & max & mean & mean & mean & mean \\
	\midrule
    Batch Size & 16 & 32 & 50 & 50 & 16 & 32 & 512 & 32 & 32 \\
    Learning Rate & 0.001 & 0.001 & 0.001 & 0.001 & 0.0005 & 0.0001 & 0.0005 & 0.0003 & 0.0001 \\
    \# Epochs & 300 & 300 & 200 & 300 & 150 & 100 & 100 & 400 & 30 \\
    \# Warmup Epochs & 5 & 5 & 10 & 10 & 10 & 5 & 5 & 10 & 2 \\
    Weight Decay & 0.0 & 0.0 & 0.0 & 0.0 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-6 \\
	\midrule
    \# Parameters & 507,351 & 506,127 & 520,986 & 460,611 & 494,235 & 1,407,641 & 13,316,700 & 5,549,605 & 23,291,826 \\
    Time (epoch) & 6.9s & 6.6s & 12.5s & 162.5s & 6.6s & 16.3s & 91.4s & 178.2s & 476.3s \\
	% \vspace{-0.2 in}
    \bottomrule
\end{tabular}}

	\label{tab:gcn-parameter2}
\end{table*}


\clearpage

\begin{table*}[h]
	\centering
        \small
         \caption{Hyperparameter settings of GIN$^+$ on benchmarks from \cite{dwivedi2023benchmarking}.}
	\begin{tabular}{lccccc}
		\toprule
		{Hyperparameter} & {ZINC} & {MNIST} & {CIFAR10} & {PATTERN} & {CLUSTER}\\
		% \cmidrule{2-3,4-5,6-7}
		\midrule
            \# GNN Layers & 12 & 5 & 5 & 8 & 10 \\
            Edge Feature Module & True & True & True & True & True \\
            Normalization & BN & BN & BN & BN & BN \\
            Dropout & 0.0 & 0.1 & 0.05 & 0.05 & 0.05 \\
            Residual Connections & True & True & True & True & True \\
            FFN & True & True & True & True & True \\
            PE & RWSE-20 & False & False & RWSE-32 & RWSE-20 \\
            Hidden Dim & 80 & 60 & 60 & 100 & 90 \\
            Graph Pooling & sum & mean & mean & – & – \\
		\midrule
            Batch Size & 32 & 16 & 16 & 32 & 16 \\
            Learning Rate & 0.001 & 0.001 & 0.001 & 0.001 & 0.0005 \\
            \# Epochs & 2000 & 200 & 200 & 200  & 100 \\
            \# Warmup Epochs & 50 & 5 & 5 & 5 & 5 \\
            Weight Decay & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 \\
		\midrule
            \# Parameters & 477,241 & 118,990 & 115,450 & 511,829 & 497,594 \\
            Time (epoch) & 9.4s & 56.8s & 46.3s & 18.5s & 20.5s \\
        \bottomrule
	\end{tabular}
	\label{tab:gin-parameter1}
\end{table*}

\begin{table*}[h]
	\centering
        % \scriptsize
         \caption{Hyperparameter settings of GIN$^+$ on LRGB and OGB datasets.}
         \resizebox{\linewidth}{!}{
	\begin{tabular}{lccccccccc}
	\toprule
	{Hyperparameter} & Peptides-func & Peptides-struct & PascalVOC-SP & COCO-SP & MalNet-Tiny & ogbg-molhiv & ogbg-molpcba & ogbg-ppa & ogbg-code2 \\
	% \cmidrule{2-3,4-5,6-7}
	\midrule
    \# GNN Layers & 3 & 5 & 16 & 16 & 5 & 3 & 16 & 5 & 4 \\
    Edge Feature Module & True & True & True & True & True & True & True & True & True \\
    Normalization & BN & BN & BN & BN & BN & BN & BN & BN & BN \\
    Dropout & 0.2 & 0.2 & 0.1 & 0.0 & 0.0 & 0.0 & 0.3 & 0.15 & 0.1 \\
    Residual Connections & True & True & True & True & True & True & True & False & True \\
    FFN & False & False & True & True & True & False & True & True & True \\
    PE & RWSE-32 & RWSE-32 & RWSE-32 & False & False & RWSE-20 & RWSE-16 & False & False \\
    Hidden Dim & 240 & 200 & 70 & 70 & 130 & 256 & 300 & 512 & 512 \\
    Graph Pooling & mean & mean & – & – & max & mean & mean & mean & mean \\
	\midrule
    Batch Size & 16 & 32 & 50 & 50 & 16 & 32 & 512 & 32 & 32 \\
    Learning Rate & 0.0005 & 0.001 & 0.001 & 0.001 & 0.0005 & 0.0001 & 0.0005 & 0.0003 & 0.0001 \\
    \# Epochs & 300 & 250 & 200 & 300 & 150 & 100 & 100 & 300 & 30 \\
    \# Warmup Epochs & 5 & 5 & 10 & 10 & 10 & 5 & 5 & 10 & 2 \\
    Weight Decay & 0.0 & 0.0 & 0.0 & 0.0 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-6 \\
	\midrule
    \# Parameters & 506,126 & 518,127 & 486,039 & 487,491 & 514,545 & 481,433 & 8,774,720 & 8,173,605 & 24,338,354 \\
    Time (epoch) & 7.4s & 6.1s & 14.8s & 169.2s & 5.9s & 10.9s & 89.2s & 213.9s & 489.8s \\
	% \vspace{-0.2 in}
    \bottomrule
\end{tabular}}
	\label{tab:gin-parameter2}
\end{table*}

\clearpage

\begin{table*}[h]
	\centering
        \small
         \caption{Hyperparameter settings of GatedGCN$^+$ on benchmarks from \cite{dwivedi2023benchmarking}.}
	\begin{tabular}{lccccc}
		\toprule
		{Hyperparameter} & {ZINC} & {MNIST} & {CIFAR10} & {PATTERN} & {CLUSTER}\\
		% \cmidrule{2-3,4-5,6-7}
		\midrule
            \# GNN Layers & 9 & 10 & 10 & 12 & 16 \\
            Edge Feature Module & True & True & True & True & True \\
            Normalization & BN & BN & BN & BN & BN \\
            Dropout & 0.05 & 0.05 & 0.15 & 0.2 & 0.2 \\
            Residual Connections & True & True & True & True & True \\
            FFN & True & True & True & True & True \\
            PE & RWSE-20 & False & False & RWSE-32 & RWSE-20 \\
            Hidden Dim & 70 & 35 & 35 & 64 & 56 \\
            Graph Pooling & sum & mean & mean & – & – \\
		\midrule
            Batch Size & 32 & 16 & 16 & 32 & 16 \\
            Learning Rate & 0.001 & 0.001 & 0.001 & 0.0005 & 0.0005 \\
            \# Epochs & 2000 & 200 & 200 & 200  & 100 \\
            \# Warmup Epochs & 50 & 5 & 5 & 5 & 5 \\
            Weight Decay & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-5 \\
		\midrule
            \# Parameters & 413,355 & 118,940 & 116,490 & 466,001 & 474,574 \\
            Time (epoch) & 10.5s & 137.9s & 115.0s & 32.6s & 34.1s \\
        \bottomrule
	\end{tabular}



	\label{tab:gatedgcn-parameter1}
\end{table*}

\begin{table*}[h]
	\centering
        % \scriptsize
         \caption{Hyperparameter settings of GatedGCN$^+$ on LRGB and OGB datasets.}
         \resizebox{\linewidth}{!}{
	\begin{tabular}{lccccccccc}
	\toprule
	{Hyperparameter} & Peptides-func & Peptides-struct & PascalVOC-SP & COCO-SP & MalNet-Tiny & ogbg-molhiv & ogbg-molpcba & ogbg-ppa & ogbg-code2 \\
	% \cmidrule{2-3,4-5,6-7}
	\midrule
    \# GNN Layers & 5 & 4 & 12 & 20 & 6 & 3 & 10 & 4 & 5 \\
    Edge Feature Module & True & True & True & True & True & True & True & True & True \\
    Normalization & BN & BN & BN & BN & BN & BN & BN & BN & BN \\
    Dropout & 0.05 & 0.2 & 0.15 & 0.05 & 0.0 & 0.0 & 0.2 & 0.15 & 0.2 \\
    Residual Connections & False & True & True & True & True & True & True & True & True \\
    FFN & False & False & False & True & True & False & True & False & True \\
    PE & RWSE-32 & RWSE-32 & RWSE-32 & False & False & RWSE-20 & RWSE-16 & False & False \\
    Hidden Dim & 135 & 145 & 95 & 52 & 100 & 256 & 256 & 512 & 512 \\
    Graph Pooling & mean & mean & – & – & max & mean & mean & mean & mean \\
	\midrule
    Batch Size & 16 & 32 & 32 & 50 & 16 & 32 & 512 & 32 & 32 \\
    Learning Rate & 0.0005 & 0.001 & 0.001 & 0.001 & 0.0005 & 0.0001 & 0.0005 & 0.0003 & 0.0001 \\
    \# Epochs & 300 & 300 & 200 & 300 & 150 & 100 & 100 & 300 & 30 \\
    \# Warmup Epochs & 5 & 5 & 10 & 10 & 10 & 5 & 5 & 10 & 2 \\
    Weight Decay & 0.0 & 0.0 & 0.0 & 0.0 & 1e-5 & 1e-5 & 1e-5 & 1e-5 & 1e-6 \\
	\midrule
    \# Parameters & 521,141 & 492,897 & 559,094 &  508,589 & 550,905 & 1,076,633 & 6,016,860 & 5,547,557 & 29,865,906 \\
    Time (epoch) & 17.3s & 8.0s & 21.3s & 208.8s & 8.9s & 15.1s & 85.1s & 479.8s & 640.1s \\
	% \vspace{-0.2 in}
    \bottomrule
\end{tabular}}

	\label{tab:gatedgcn-parameter2}
\end{table*}

