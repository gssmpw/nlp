% \vspace{-0.05 in}
%\section{Experimental Setup for Graph-level Tasks}

\section{Assessment: Experimental Setup}

% \textbf{Datasets.} Table \ref{tab:dataset} presents a summary of the statistics and characteristics of the datasets. We used multiple widely adopted graph-level datasets in our experiments, including ZINC, MNIST, CIFAR10, PATTERN, and CLUSTER from the GNN Benchmark \cite{dwivedi2023benchmarking}; Peptides-func, Peptides-struct, PascalVOC-SP, COCO-SP, and MalNet-Tiny from LRGB \cite{dwivedi2022long,freitas2021large}; and ogbg-molhiv, ogbg-molpcba, ogbg-ppa, and ogbg-code2 from OGB \cite{hu2020open}. These datasets cover a broad range of tasks, such as molecular property prediction, image-based graph classification, and function call graph analysis. All datasets follow their respective standard evaluation protocols (including splits and metrics); for more details, please refer to the Appendix.

% \textbf{Datasets.} Table \ref{tab:dataset} summarizes the statistics and characteristics of the datasets. 

\textbf{Datasets, Table \ref{tab:dataset-s}}. We use widely adopted graph-level datasets in our experiments, including \textbf{ZINC}, \textbf{MNIST}, \textbf{CIFAR10}, \textbf{PATTERN}, and \textbf{CLUSTER} from the GNN Benchmark \cite{dwivedi2023benchmarking}; \textbf{Peptides-func}, \textbf{Peptides-struct}, \textbf{PascalVOC-SP}, \textbf{COCO-SP}, and \textbf{MalNet-Tiny} from Long-Range Graph Benchmark (LRGB) \cite{dwivedi2022long,freitas2021large}; and \textbf{ogbg-molhiv}, \textbf{ogbg-molpcba}, \textbf{ogbg-ppa}, and \textbf{ogbg-code2} from Open Graph Benchmark (OGB) \cite{hu2020open}. 
% These datasets span diverse tasks, including molecular property prediction, image-based graph classification, and function call graph analysis. 
We follow their respective standard evaluation protocols including the splits and metrics. For further details, refer to the Appendix~\ref{ap-a2}.

% \vspace{-0.05 in}
% \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt] %[leftmargin=*] % ,noitemsep,topsep=0pt
% 	\item Can \textbf{HDSE improve upon existing graph transformers}, and how does the choice of \textbf{coarsening algorithm} affect performance? (\textbf{Sec. \ref{graph-level}})
% \end{itemize}



\begin{table*}[t]
\footnotesize
	\centering
        \vspace{-0.1 in}
         \caption{Test performance on five datasets from
Long-Range Graph Benchmarks (LRGB) \cite{dwivedi2022long, freitas2021large}. \quad 
% Shown is the mean $\pm$ s.d. of 5 runs with different random seeds. 
$^+$ denotes the enhanced version, while the baseline results were obtained from their respective original papers. \# Param $\sim$ 500K for all. }
        % \vspace{-0.1 in}
	\resizebox{0.95\linewidth}{!}{
	\begin{tabular}{l|lllll}
		\toprule
		&Peptides-func& Peptides-struct& PascalVOC-SP& COCO-SP& MalNet-Tiny\\
        \# graphs&  15,535 & 15,535 &  11,355 & 123,286 & 5,000\\
        Avg. \# nodes&  150.9 & 150.9&  479.4 &476.9&   1,410.3\\
        Avg. \# edges&   307.3 & 307.3& 2,710.5 &2,693.7&   2,859.9\\
		% \cmidrule{2-3,4-5,6-7}
		Metric& Avg. Precision $\uparrow$ & MAE $\downarrow$ & F1 score $\uparrow$ & F1 score $\uparrow$ & Accuracy $\uparrow$\\
		
        \midrule %
        GT (\citeyear{dwivedi2020generalization}) & 0.6326{\tiny{ $\pm$ 0.0126}} & 0.2529{\tiny{ $\pm$ 0.0016}} & 0.2694{\tiny{ $\pm$ 0.0098}} & 0.2618{\tiny{ $\pm$ 0.0031}} &  –  \\
        SAN (\citeyear{kreuzer2021rethinking}) & 0.6439{\tiny{ $\pm$ 0.0075}} &0.2545{\tiny{ $\pm$ 0.0012}} &  0.3230{\tiny{ $\pm$ 0.0039}} & 0.2592{\tiny{ $\pm$ 0.0158}} &  –  \\
        GraphGPS (\citeyear{rampavsek2022recipe}) & 0.6535{\tiny{ $\pm$ 0.0041}} &0.2500{\tiny{ $\pm$ 0.0005}} &0.3748{\tiny{ $\pm$ 0.0109}} &0.3412{\tiny{ $\pm$ 0.0044}} & 0.9350{\tiny{ $\pm$ 0.0041}} \\
        GraphGPS (\citeyear{tonshoff2023did}) & 0.6534{\tiny{ $\pm$ 0.0091}} & 0.2509{\tiny{ $\pm$ 0.0014}} & \textbf{\textcolor{tealblue!90}{0.4440{\tiny{ $\pm$ 0.0065}}}} & \textbf{\textcolor{tealblue!90}{0.3884{\tiny{ $\pm$ 0.0055}}}} & 0.9350{\tiny{ $\pm$ 0.0041}} \\
        NAGphormer  (\citeyear{chen2023nagphormer}) &–&–& 0.4006{\tiny{ $\pm$ 0.0061}}& 0.3458{\tiny{ $\pm$ 0.0070}} &– \\
         DIFFormer  (\citeyear{wu2023difformer}) &–&–& 0.3988{\tiny{ $\pm$ 0.0045}}& 0.3620{\tiny{ $\pm$ 0.0012}} &– \\
        MGT (\citeyear{ngo2023multiresolution}) & 0.6817{\tiny{ $\pm$ 0.0064}}  & 0.2453{\tiny{ $\pm$ 0.0025}} &– &– &– \\
        DRew (\citeyear{gutteridge2023drew}) & \textbf{\textcolor{tealblue!90}{0.7150{\tiny{ $\pm$ 0.0044}}}} & 0.2536{\tiny{ $\pm$ 0.0015}} & 0.3314{\tiny{ $\pm$ 0.0024}} &– &–  \\
        Graph ViT/MLP-Mixer (\citeyear{he2023generalization}) &0.6970{\tiny{ $\pm$ 0.0080}} & 0.2449{\tiny{ $\pm$ 0.0016}} &– &– &–\\
        Exphormer (\citeyear{shirzad2023exphormer}) & 0.6258{\tiny{ $\pm$ 0.0092}} &0.2512{\tiny{ $\pm$ 0.0025}} &0.3446{\tiny{ $\pm$ 0.0064}} &0.3430{\tiny{ $\pm$ 0.0108}}& \textbf{\textcolor{darkorange!90}{0.9402{\tiny{ $\pm$ 0.0021}}}}\\
        GRIT (\citeyear{ma2023graph}) & 0.6988{\tiny{ $\pm$ 0.0082}} &0.2460{\tiny{ $\pm$ 0.0012}} &– &– &–  \\
        Subgraphormer (\citeyear{bar2024subgraphormer})  & 0.6415{\tiny{ $\pm$ 0.0052}} & 0.2475{\tiny{ $\pm$ 0.0007}} &– &– &–\\
        GRED (\citeyear{ding2024recurrent}) & \textbf{\textcolor{darkorange!90}{0.7133{\tiny{ $\pm$ 0.0011}}}} & 0.2455{\tiny{ $\pm$ 0.0013}} &– &– &–\\
        GEAET (\citeyear{liang2024graph}) & 0.6485{\tiny{ $\pm$ 0.0035}} & 0.2547{\tiny{ $\pm$ 0.0009}}&   0.3933{\tiny{ $\pm$ 0.0027}}& 0.3219{\tiny{ $\pm$ 0.0052}} & – \\
        TIGT (\citeyear{choi2024topology}) & 0.6679{\tiny{ $\pm$ 0.0074}} &0.2485{\tiny{ $\pm$ 0.0015}}&  – &  – &  – \\
        GECO (\citeyear{sancak2024scalable})& 0.6975{\tiny{ $\pm$ 0.0025}} &0.2464{\tiny{ $\pm$ 0.0009}} &0.4210{\tiny{ $\pm$ 0.0080}} &0.3320{\tiny{ $\pm$ 0.0032}}& – \\
        GPNN (\citeyear{lin2024understanding})& 0.6955{\tiny{ $\pm$ 0.0057}} &0.2454{\tiny{ $\pm$ 0.0003}} &  – &  – &  –  \\
        Graph-Mamba (\citeyear{wang2024graph}) & 0.6739{\tiny{ $\pm$ 0.0087}} &0.2478{\tiny{ $\pm$ 0.0016}}& 0.4191{\tiny{ $\pm$ 0.0126}} &\textbf{\textcolor{customcyan}{0.3960{\tiny{ $\pm$ 0.0175}}}} &0.9340{\tiny{ $\pm$  0.0027}} \\
        GSSC (\citeyear{huang2024can}) & 0.7081{\tiny{ $\pm$ 0.0062}}& 0.2459{\tiny{ $\pm$ 0.0020}}& \textbf{\textcolor{customcyan}{0.4561{\tiny{ $\pm$ 0.0039}}}} & – &\textbf{\textcolor{tealblue!90}{0.9406{\tiny{ $\pm$ 0.0064}}}} \\
         \midrule %
        GCN & 0.6860{\tiny{ $\pm$ 0.0050}} &0.2460{\tiny{ $\pm$ 0.0007}} &0.2078{\tiny{ $\pm$ 0.0031}} &0.1338{\tiny{ $\pm$ 0.0007}} &   0.8100{\tiny{ $\pm$ 0.0081}} \\
        \rowcolor{gray!20}
       \textbf{GCN$^+$} & \textbf{\textcolor{customcyan}{0.7261{\tiny{ $\pm$ 0.0067}}}} \textbf{5.9\%$\uparrow$}& \textbf{\textcolor{customcyan}{0.2421{\tiny{ $\pm$ 0.0016}}}} \textbf{1.6\%$\downarrow$} & 0.3357{\tiny{ $\pm$ 0.0087}} \textbf{62.0\%$\uparrow$} & 0.2733{\tiny{ $\pm$ 0.0041}} \textbf{104.9\%$\uparrow$} & 0.9354{\tiny{ $\pm$ 0.0045}} \textbf{15.5\%$\uparrow$} \\ 
       \midrule %
        GIN &  0.6621{\tiny{ $\pm$ 0.0067}}& 0.2473{\tiny{ $\pm$ 0.0017}}&  0.2718{\tiny{ $\pm$ 0.0054}}& 0.2125{\tiny{ $\pm$ 0.0009}}&  0.8898{\tiny{ $\pm$ 0.0055}}  \\ 
        \rowcolor{gray!20}
         \textbf{GIN$^+$} & 0.7059{\tiny{ $\pm$ 0.0089}} \textbf{6.6\%$\uparrow$} & \textbf{\textcolor{tealblue!90}{0.2429{\tiny{ $\pm$ 0.0019}}}} \textbf{1.8\%$\downarrow$} & 0.3189{\tiny{ $\pm$ 0.0105}} \textbf{17.3\%$\uparrow$} & 0.2483{\tiny{ $\pm$ 0.0046}} \textbf{16.9\%$\uparrow$} & 0.9325{\tiny{ $\pm$ 0.0040}} \textbf{4.8\%$\uparrow$} \\  
        \midrule %
        GatedGCN & 0.6765{\tiny{ $\pm$ 0.0047}} &0.2477{\tiny{ $\pm$ 0.0009}} & 0.3880{\tiny{ $\pm$ 0.0040}} &0.2922{\tiny{ $\pm$ 0.0018}}& 0.9223{\tiny{ $\pm$ 0.0065}}  \\
        \rowcolor{gray!20}
        \textbf{GatedGCN$^+$} & 0.7006{\tiny{ $\pm$ 0.0033}} \textbf{3.6\%$\uparrow$} & \textbf{\textcolor{darkorange!90}{0.2431{\tiny{ $\pm$ 0.0020}}}} \textbf{1.9\%$\downarrow$} & \textbf{\textcolor{darkorange!90}{0.4263{\tiny{ $\pm$ 0.0057}}}} \textbf{9.9\%$\uparrow$} & \textbf{\textcolor{darkorange!90}{0.3802{\tiny{ $\pm$ 0.0015}}}} \textbf{30.1\%$\uparrow$} & \textbf{\textcolor{customcyan}{0.9460{\tiny{ $\pm$ 0.0057}}}} \textbf{2.6\%$\uparrow$} \\ 
        \midrule %
        Time (epoch) of GraphGPS & 6s & 6s & 17s & 213s & 46s \\
        \rowcolor{gray!20}
        Time (epoch) of  \textbf{GCN$^+$} & 6s & 6s & \textbf{12s} & \textbf{162s} & \textbf{6s}  \\
        % & 16.3s & 91.4s & 178.2s & 476.3s
        \bottomrule
	\end{tabular}}
    % \vspace{-0.1 in}
	\label{tab:tab3}
\end{table*}

\begin{table*}[t]
\footnotesize
	\centering
        % \vspace{-0.1 in}
         \caption{Test performance in four benchmarks from 
        Open Graph Benchmark (OGB) \cite{hu2020open}. 
        % Shown is the mean $\pm$ s.d. of 5 runs with different random seeds. 
        $^+$ denotes the enhanced version, while the baseline results were obtained from their respective original papers. $^{\dagger}$ indicates the use of additional pretraining datasets, included here for reference only and excluded from ranking. }
        % \vspace{-0.1 in}
         \resizebox{0.9\linewidth}{!}{
	\begin{tabular}{l|llll}
		\toprule
		& ogbg-molhiv & ogbg-molpcba & ogbg-ppa & ogbg-code2 \\
        \# graphs&  41,127 &  437,929 &   158,100 &  452,741 \\
        Avg. \# nodes&  25.5 &  26.0&   243.4 &125.2\\
        Avg. \# edges&   27.5 & 28.1& 2,266.1 &124.2\\
		% \cmidrule{2-3,4-5,6-7}
		Metric & AUROC $\uparrow$ & Avg. Precision $\uparrow$ & Accuracy $\uparrow$ & F1 score $\uparrow$ \\
        \midrule %
        GT (\citeyear{dwivedi2020generalization}) &  – &  – &  0.6454{\tiny{ $\pm$ 0.0033}} & 0.1670{\tiny{ $\pm$ 0.0015}}   \\
        GraphTrans (\citeyear{wu2021representing}) &  – &0.2761{\tiny{ $\pm$ 0.0029}}& –& 0.1830{\tiny{ $\pm$ 0.0024}}   \\
        SAN (\citeyear{kreuzer2021rethinking})& 0.7785{\tiny{ $\pm$ 0.2470}} &0.2765{\tiny{ $\pm$ 0.0042}} &– &–   \\
        Graphormer (pre-trained) (\citeyear{ying2021transformers}) & 0.8051{\tiny{ $\pm$ 0.0053}}$^{\dagger}$ &  – &  – &  –  \\
        SAT (\citeyear{chen2022structure})& – &  – & 0.7522{\tiny{ $\pm$ 0.0056}} & \textbf{\textcolor{customcyan}{0.1937{\tiny{ $\pm$ 0.0028}}}}   \\
        EGT (pre-trained)  (\citeyear{hussain2022global}) & 0.8060{\tiny{ $\pm$ 0.0065}}$^{\dagger}$ &  0.2961{\tiny{ $\pm$ 0.0024}}$^{\dagger}$ &  – &  –   \\
        GraphGPS (\citeyear{rampavsek2022recipe}) &  0.7880{\tiny{ $\pm$ 0.0101}} &0.2907{\tiny{ $\pm$ 0.0028}} &0.8015{\tiny{ $\pm$ 0.0033}} &0.1894{\tiny{ $\pm$ 0.0024}}  \\
        Specformer (\citeyear{bo2023specformer}) & 0.7889{\tiny{ $\pm$ 0.0124}} & \textbf{\textcolor{tealblue!90}{0.2972{\tiny{ $\pm$ 0.0023}}}} &  – &  –   \\
        Graph ViT/MLP-Mixer (\citeyear{he2023generalization}) &0.7997{\tiny{ $\pm$ 0.0102}} & – &  – &  –   \\
        Exphormer (\citeyear{shirzad2023exphormer}) &0.7834{\tiny{ $\pm$ 0.0044}}  & 0.2849{\tiny{ $\pm$ 0.0025}} &  – &  –   \\
        GRIT (\citeyear{ma2023graph}) & 0.7835{\tiny{ $\pm$ 0.0054}} &  0.2362{\tiny{ $\pm$ 0.0020}} &  – &  –   \\
         Subgraphormer (\citeyear{bar2024subgraphormer}) & \textbf{\textcolor{tealblue!90}{0.8038{\tiny{ $\pm$ 0.0192}}}} &  – &  –&  –  \\
         GECO (\citeyear{sancak2024scalable})& 0.7980{\tiny{ $\pm$ 0.0200}} &\textbf{\textcolor{darkorange!90}{0.2961{\tiny{ $\pm$ 0.0008}}}} &0.7982{\tiny{ $\pm$ 0.0042}} &\textbf{\textcolor{tealblue!90}{0.1915{\tiny{ $\pm$ 0.0020}}}} \\
         GSSC (\citeyear{huang2024can}) & \textbf{\textcolor{darkorange!90}{0.8035{\tiny{ $\pm$ 0.0142}}}} &  – &  –&  –  \\
        \midrule %
        GCN & 0.7606{\tiny{ $\pm$ 0.0097}} & 0.2020{\tiny{ $\pm$ 0.0024}} & 0.6839{\tiny{ $\pm$ 0.0084}} & 0.1507{\tiny{ $\pm$ 0.0018}}   \\
        \rowcolor{gray!20}
        \textbf{GCN$^+$} & 0.8012{\tiny{ $\pm$ 0.0124}} \textbf{5.4\%$\uparrow$} & 0.2721{\tiny{ $\pm$ 0.0046}} \textbf{34.7\%$\uparrow$} & \textbf{\textcolor{darkorange!90}{0.8077{\tiny{ $\pm$ 0.0041}}}} \textbf{18.1\%$\uparrow$} & 0.1787{\tiny{ $\pm$ 0.0026}} \textbf{18.6\%$\uparrow$} \\  
        \midrule %
        GIN & 0.7835{\tiny{ $\pm$ 0.0125}} & 0.2266{\tiny{ $\pm$ 0.0028}} & 0.6892{\tiny{ $\pm$ 0.0100}} & 0.1495{\tiny{ $\pm$ 0.0023}}   \\  
        \rowcolor{gray!20}
        \textbf{GIN$^+$} & 0.7928{\tiny{ $\pm$ 0.0099}} \textbf{1.2\%$\uparrow$} & 0.2703{\tiny{ $\pm$ 0.0024}} \textbf{19.3\%$\uparrow$} & \textbf{\textcolor{tealblue!90}{0.8107{\tiny{ $\pm$ 0.0053}}}} \textbf{17.7\%$\uparrow$} & 0.1803{\tiny{ $\pm$ 0.0019}} \textbf{20.6\%$\uparrow$} \\  
        \midrule %
        GatedGCN & 0.7687{\tiny{ $\pm$ 0.0136}} & 0.2670{\tiny{ $\pm$ 0.0020}} & 0.7531{\tiny{ $\pm$ 0.0083}} & 0.1606{\tiny{ $\pm$ 0.0015}}   \\  
        \rowcolor{gray!20}
        \textbf{GatedGCN$^+$} & \textbf{\textcolor{customcyan}{0.8040{\tiny{ $\pm$ 0.0164}}}} \textbf{4.6\%$\uparrow$} & \textbf{\textcolor{customcyan}{0.2981{\tiny{ $\pm$ 0.0024}}}} \textbf{11.6\%$\uparrow$} & \textbf{\textcolor{customcyan}{0.8258{\tiny{ $\pm$ 0.0055}}}} \textbf{9.7\%$\uparrow$} & \textbf{\textcolor{darkorange!90}{0.1896{\tiny{ $\pm$ 0.0024}}}} \textbf{18.1\%$\uparrow$} \\ 
        \midrule %
        Time (epoch/s) of GraphGPS & 96s & 196s & 276s& 1919s \\
        \rowcolor{gray!20}
        Time (epoch/s) of  \textbf{GCN$^+$} & \textbf{16s} & \textbf{91s} & \textbf{178s} & \textbf{476s}  \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.05 in}
	\label{tab:tab4}
\end{table*}

\begin{table}[!h]
    \centering
    \vspace{-0.05 in}
    \caption{Ablation study on GNN Benchmark \cite{dwivedi2023benchmarking} (\%). - indicates that the corresponding hyperparameter is not used in GNN$^+$, as it empirically leads to inferior performance. }
    % The baseline results are primarily taken from \cite{deng2024polynormer}, with the remaining obtained from their respective original papers. 
    % The format is average score $\pm$ standard deviation.
    % \vspace{-0.1 in}
    \setlength\tabcolsep{3pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccccccc}
        \toprule
            & {ZINC} & {MNIST} & {CIFAR10} & {PATTERN} & {CLUSTER}\\
            Metric & MAE $\downarrow$ & Accuracy $\uparrow$ & Accuracy $\uparrow$ & Accuracy $\uparrow$ & Accuracy $\uparrow$\\
        \midrule %
        \textbf{GCN$^+$} & \textbf{0.076}{\tiny{ $\pm$ 0.009}} & \textbf{98.382}{\tiny{ $\pm$ 0.095}} & \textbf{69.824}{\tiny{ $\pm$ 0.413}} & \textbf{87.021}{\tiny{ $\pm$ 0.095}} & \textbf{77.109}{\tiny{ $\pm$ 0.872}} \\
        (-) Edge. & 0.135{\tiny{ $\pm$ 0.004}} & 98.153{\tiny{ $\pm$ 0.042}} & 68.256{\tiny{ $\pm$ 0.357}} & 86.854{\tiny{ $\pm$ 0.054}} & – \\ 
        (-) Norm & 0.107{\tiny{ $\pm$ 0.011}} & 97.886{\tiny{ $\pm$ 0.066}} & 60.765{\tiny{ $\pm$ 0.829}} & 52.769{\tiny{ $\pm$ 0.874}} & 16.563{\tiny{ $\pm$ 0.134 }} \\ 
        (-) Dropout & –  & 97.897{\tiny{ $\pm$ 0.071}} & 65.693{\tiny{ $\pm$ 0.461}} & 86.764{\tiny{ $\pm$ 0.045}} & 74.926{\tiny{ $\pm$ 0.469}} \\ 
        (-) RC & 0.159{\tiny{ $\pm$ 0.016}} & 95.929{\tiny{ $\pm$ 0.169}} & 58.186{\tiny{ $\pm$ 0.295}} & 86.059{\tiny{ $\pm$ 0.274}} & 16.508{\tiny{ $\pm$ 0.615}} \\ 
        (-) FFN & 0.132{\tiny{ $\pm$ 0.021}} & 97.174{\tiny{ $\pm$ 0.063}} & 63.573{\tiny{ $\pm$ 0.346}} & 86.746{\tiny{ $\pm$ 0.088}} & 72.606{\tiny{ $\pm$ 1.243}} \\ 
        (-) PE & 0.127{\tiny{ $\pm$ 0.010}} & – & – & 85.597{\tiny{ $\pm$ 0.241}} & 75.568{\tiny{ $\pm$ 1.147}} \\ 
        \midrule %
         \textbf{GIN$^+$} & \textbf{0.065}{\tiny{ $\pm$ 0.004}} & \textbf{98.285}{\tiny{ $\pm$ 0.103}} & \textbf{69.592}{\tiny{ $\pm$ 0.287}} & \textbf{86.842}{\tiny{ $\pm$ 0.048}} & \textbf{74.794}{\tiny{ $\pm$ 0.213 }} \\  
         (-) Edge. & 0.122{\tiny{ $\pm$ 0.009}} & 97.655{\tiny{ $\pm$ 0.075}} & 68.196{\tiny{ $\pm$ 0.107}} & 86.714{\tiny{ $\pm$ 0.036}} & 65.895{\tiny{ $\pm$ 3.425 }} \\ 
        (-) Norm & 0.096{\tiny{ $\pm$ 0.006}} & 97.695{\tiny{ $\pm$ 0.065}} & 64.918{\tiny{ $\pm$ 0.059}} & 86.815{\tiny{ $\pm$ 0.855}} & 72.119{\tiny{ $\pm$ 0.359 }} \\ 
         (-) Dropout & – & 98.214{\tiny{ $\pm$ 0.064}} & 66.638{\tiny{ $\pm$ 0.873}} & 86.836{\tiny{ $\pm$ 0.053}} & 73.316{\tiny{ $\pm$ 0.355 }} \\ 
         (-) RC & 0.137{\tiny{ $\pm$ 0.031}} & 97.675{\tiny{ $\pm$ 0.175}} & 64.910{\tiny{ $\pm$ 0.102}} & 86.645{\tiny{ $\pm$ 0.125}} & 16.800{\tiny{ $\pm$ 0.088 }} \\ 
        (-) FFN & 0.104{\tiny{ $\pm$ 0.003}} & 11.350{\tiny{ $\pm$ 0.008}} & 60.582{\tiny{ $\pm$ 0.395}} & 58.511{\tiny{ $\pm$ 0.016}} & 62.175{\tiny{ $\pm$ 2.895 }} \\ 
        (-) PE & 0.123{\tiny{ $\pm$ 0.014}} & – & – & 86.592{\tiny{ $\pm$ 0.049}} & 73.925{\tiny{ $\pm$ 0.165 }} \\  
        \midrule %
        \textbf{GatedGCN$^+$} & \textbf{0.077}{\tiny{ $\pm$ 0.005}} & \textbf{98.712}{\tiny{ $\pm$ 0.137}} & \textbf{77.218}{\tiny{ $\pm$ 0.381}} & \textbf{87.029}{\tiny{ $\pm$ 0.037}} & \textbf{79.128}{\tiny{ $\pm$ 0.235 }} \\ 
        (-) Edge. & 0.119{\tiny{ $\pm$ 0.001}} & 98.085{\tiny{ $\pm$ 0.045}} & 72.128{\tiny{ $\pm$ 0.275}} & 86.879{\tiny{ $\pm$ 0.017}} & 76.075{\tiny{ $\pm$ 0.845 }} \\
        (-) Norm & 0.088{\tiny{ $\pm$ 0.003}} & 98.275{\tiny{ $\pm$ 0.045}} & 71.995{\tiny{ $\pm$ 0.445}} & 86.942{\tiny{ $\pm$ 0.023}} & 78.495{\tiny{ $\pm$ 0.155 }} \\
        (-) Dropout & 0.089{\tiny{ $\pm$ 0.003}} & 98.225{\tiny{ $\pm$ 0.095}} & 70.383{\tiny{ $\pm$ 0.429}} & 86.802{\tiny{ $\pm$ 0.034}} & 77.597{\tiny{ $\pm$ 0.126 }} \\
        (-) RC & 0.106{\tiny{ $\pm$ 0.002}} & 98.442{\tiny{ $\pm$ 0.067}} & 75.149{\tiny{ $\pm$ 0.155}} & 86.845{\tiny{ $\pm$ 0.025}} & 16.670{\tiny{ $\pm$ 0.307 }} \\
        (-) FFN & 0.098{\tiny{ $\pm$ 0.005}} & 98.438{\tiny{ $\pm$ 0.151}} & 76.243{\tiny{ $\pm$ 0.131}} & 86.935{\tiny{ $\pm$ 0.025}} & 78.975{\tiny{ $\pm$ 0.145 }} \\
        (-) PE & 0.174{\tiny{ $\pm$ 0.009}} & – & – & 85.595{\tiny{ $\pm$ 0.065}} & 77.515{\tiny{ $\pm$ 0.265 }} \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:ab1}
\vspace{-0.10 in}
\end{table}

% \textbf{Datasets.} Table \ref{tab:dataset} presents a summary of the statistics and characteristics of the datasets.

% \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
%     \item \textbf{GNN Benchmark} \cite{dwivedi2023benchmarking}\textbf{.}  \textbf{ZINC} contains molecular graphs with node features representing atoms and edge features representing bonds The task is to regress the constrained solubility (logP) of the molecule. \textbf{MNIST} and \textbf{CIFAR10} are adapted from image classification datasets, where each image is represented as an 8-nearest-neighbor graph of SLIC superpixels, with nodes representing superpixels and edges representing spatial relationships. The 10-class classification tasks follow the original image classification tasks. \textbf{PATTERN} and \textbf{CLUSTER} are synthetic datasets sampled from the Stochastic Block Model (SBM) for inductive node classification, with tasks involving sub-graph pattern recognition and cluster ID inference. For all datasets, we adhere to the respective training protocols and standard evaluation splits \cite{dwivedi2023benchmarking}.
%     \item \textbf{Long-Range Graph Benchmark (LRGB)} \cite{dwivedi2022long,freitas2021large}\textbf{.} \textbf{Peptides-func} and \textbf{Peptides-struct} are atomic graphs of peptides from SATPdb, with tasks of multi-label graph classification into 10 peptide functional classes and graph regression for 11 3D structural properties, respectively. \textbf{PascalVOC-SP} and \textbf{COCO-SP} are node classification datasets derived from the Pascal VOC and MS COCO images by SLIC superpixelization, where each superpixel node belongs to a particular object class. We did not use PCQM-Contact in \cite{dwivedi2022long} as its download link was no longer valid. \textbf{MalNet-Tiny} \cite{freitas2021large} is a subset of MalNet with 5,000 function call graphs (FCGs) from Android APKs, where the task is to predict software type based on structure alone. For each dataset, we follow standard training protocols and splits \cite{dwivedi2022long,freitas2021large}.
%     \item \textbf{Open Graph Benchmark (OGB)} \cite{hu2020open}\textbf{.} We also consider a collection of larger-scale datasets from OGB, containing graphs in the range of hundreds of thousands to millions: \textbf{ogbg-molhiv} and \textbf{ogbg-molpcba} are molecular property prediction datasets from MoleculeNet. ogbg-molhiv involves binary classification of HIV inhibition, while ogbg-molpcba predicts results of 128 bioassays in a multi-task setting. \textbf{ogbg-ppa} contains protein-protein association networks, where nodes represent proteins and edges encode normalized associations between them; the task is to classify the origin of the network among 37 taxonomic groups. \textbf{ogbg-code2} consists of abstract syntax trees (ASTs) from Python source code, with the task of predicting the first 5 subtokens of the function’s name. We maintain all the OGB standard evaluation settings \cite{hu2020open}.
% \end{itemize}
% % \vspace{-0.1 in}


\textbf{Baselines.} 
Our main focus lies on classic GNNs: \textbf{GCN} \cite{kipf2017semisupervised}, \textbf{GIN} \cite{xu2018powerful,hu2019strategies}, \textbf{GatedGCN} \cite{bresson2017residual}, the SOTA GTs: GT (\citeyear{dwivedi2020generalization}), GraphTrans (\citeyear{wu2021representing}), SAN (\citeyear{kreuzer2021rethinking}), Graphormer (\citeyear{ying2021transformers}), SAT (\citeyear{chen2022structure}), EGT (\citeyear{hussain2022global}), GraphGPS (\citeyear{rampavsek2022recipe, tonshoff2023did}), GRPE (\citeyear{park2022grpe}), Graphormer-URPE (\citeyear{luo2022your}), Graphormer-GD (\citeyear{zhang2023rethinking}), Specformer (\citeyear{bo2023specformer}), LGI-GT (\citeyear{yinlgi}), GPTrans-Nano (\citeyear{chen2023graph}), Graph ViT/MLP-Mixer (\citeyear{he2023generalization}), NAGphormer  (\citeyear{chen2023nagphormer}), DIFFormer  (\citeyear{wu2023difformer}), MGT (\citeyear{ngo2023multiresolution}), DRew (\citeyear{gutteridge2023drew}), Exphormer (\citeyear{shirzad2023exphormer}), GRIT (\citeyear{ma2023graph}), GRED (\citeyear{ding2024recurrent}), GEAET (\citeyear{liang2024graph}), Subgraphormer (\citeyear{bar2024subgraphormer}), TIGT (\citeyear{choi2024topology}), GECO (\citeyear{sancak2024scalable}), GPNN (\citeyear{lin2024understanding}), Cluster-GT (\citeyear{huang2024clusterwise}), and the SOTA graph state space models (GSSMs): GMN (\citeyear{behrouz2024graph}), Graph-Mamba (\citeyear{wang2024graph}), GSSC (\citeyear{huang2024can}).
Furthermore, various other GTs exist in related surveys \cite{hoang2024survey,shehzad2024graph,muller2023attending}, empirically shown to be inferior to the GTs we compared against for graph-level tasks. We report the performance results of baselines primarily from \cite{rampavsek2022recipe,tonshoff2023did}, with the remaining obtained from their respective original papers or official
leaderboards whenever possible, as those results are obtained by well-tuned models.


\textbf{Hyperparameter Configurations.} We conduct hyperparameter tuning on 3 classic GNNs, consistent with the hyperparameter search space of GraphGPS \cite{rampavsek2022recipe, tonshoff2023did}. Specifically, we utilize the AdamW optimizer \cite{loshchilov2017decoupled} with a learning rate from $\{0.0001, 0.0005, 0.001\}$ and an epoch limit of 2000. As discussed in Section~\ref{sec3}, we focus on whether to use the edge feature module, normalization (BN), residual connections, FFN, PE (RWSE), and dropout rates from $\{0.05, 0.1, 0.15, 0.2, 0.3\}$, the number of layers from 3 to 20. 
Considering the large number of hyperparameters and datasets, we do not perform an exhaustive search. Additionally, \emph{we retrain baseline GTs using the same hyperparameter search space and training environments as the classic GNNs. Since the retrained results did not surpass those in their original papers, we present the results from those sources}. \textbf{GNN$^+$} denotes the enhanced version. We report mean scores and standard deviations after 5 independent runs with different random seeds. Detailed hyperparameters are provided in Appendix~\ref{ap-a}.


%\section{Empirical Findings}
\vspace{-0.05 in}
\section{Assessment: Results and Findings}


%\subsection{Overall Performance of Classic GNNs in Graph-level Tasks}

\subsection{Overall Performance}

% In this section, we evaluate the performance of 3 classic GNNs under the enhanced implementation across 14 well-known graph-level datasets.

We evaluate the performance of the enhanced versions of 3 classic GNNs across 14 well-known graph-level datasets.

% Our results demonstrate significant performance improvements in GCN, GIN, and GatedGCN, achieving state-of-the-art (SOTA) performance across all datasets. 
% Below, we summarize the key findings.


\begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, arc=1pt, left=3pt, right=3pt, top=2pt, bottom=2pt]
{
% Our implementation has significantly improved the previously reported results of classic GNNs, often outperforming SOTA graph transformers in quality and efficiency. It has achieved the best results across GNN Benchmark, LRGB, and OGB datasets, 
% including those that require handling distant information within the graph. This indicates that message passing remains highly effective for learning graph representations, challenging the assumption that it suffers from over-smoothing, over-squashing, and is mainly suited for local structural information.
The enhanced versions of classic GNNs achieved state-of-the-art performance, ranking in the \textbf{top three across 14 datasets}, including \textbf{first place in 8 of them}, while also demonstrating \textbf{superior efficiency}. This suggests that the GNN$^+$ framework effectively harnesses the potential of classic GNNs for graph-level tasks and successfully mitigates their inherent limitations.
%challenge the assumption that GNNs suffer from over-smoothing, over-squashing, and limitations in capturing long-range dependencies.
}
\end{tcolorbox}

\textbf{GNN Benchmark, Table \ref{tab:tab2}.}
We observe that our GNN$^+$ implementation substantially enhances the performance of classic GNNs, with the most significant improvements on ZINC, PATTERN, and CLUSTER. On MNIST and CIFAR, GatedGCN$^+$ outperforms SOTA models such as GEAET and GRED, securing top rankings.

\textbf{Long-Range Graph Benchmark (LRGB), Table \ref{tab:tab3}.} 
The results reveal that classic GNNs can achieve strong performance across LRGB datasets. Specifically, GCN$^+$ excels on the Peptides-func and Peptides-struct datasets. On the other hand, GatedGCN$^+$ achieves the highest accuracy on MalNet-Tiny. Furthermore, on PascalVOC-SP and COCO-SP, GatedGCN$^+$ significantly improves performance, securing the third-best model ranking overall. These results highlight the potential of classic GNNs in capturing long-range interactions in graph-level tasks.

\textbf{Open Graph Benchmark (OGB), Table \ref{tab:tab4}.}
Finally, we test our method on four OGB datasets. As shown in Table \ref{tab:tab4}, GatedGCN$^+$ consistently ranks among the top three models and achieves top performance on three out of the four datasets. On ogbg-ppa, GatedGCN$^+$ shows an improvement of approximately 9\%, ranking first on the OGB leaderboard. On ogbg-molhiv and ogbg-molpcba, GatedGCN$^+$ even matches the performance of Graphormer and EGT pre-trained on other datasets. Additionally, on ogbg-code2, GatedGCN$^+$ secures the third-highest performance, underscoring the potential of GNNs for large-scale OGB datasets.

% \subsection{Influence of Hyperparameters on Performance}\label{ablationsec}

\subsection{Ablation Study}\label{ablationsec}

 \begin{table*}[t]
    \centering
    % \vspace{-0.1 in}
    \caption{Ablation study on LRGB and OGB datasets. - indicates that the corresponding hyperparameter is not used in GNN$^+$, as it empirically leads to inferior performance. }
    % The baseline results are primarily taken from \cite{deng2024polynormer}, with the remaining obtained from their respective original papers. 
    % The format is average score $\pm$ standard deviation.
    % \vspace{-0.1 in}
    \setlength\tabcolsep{3pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccccccccc}
        \toprule
            &Peptides-func& Peptides-struct& PascalVOC-SP& COCO-SP& MalNet-Tiny & ogbg-molhiv & ogbg-molpcba & ogbg-ppa & ogbg-code2 \\
            Metric  & Avg. Precision $\uparrow$ & MAE $\downarrow$ & F1 score $\uparrow$ & F1 score $\uparrow$ & Accuracy $\uparrow$ & AUROC $\uparrow$ & Avg. Precision $\uparrow$ & Accuracy $\uparrow$ & F1 score $\uparrow$ \\
        \midrule %
        \textbf{GCN$^+$} &  \textbf{0.7261}{\tiny{ $\pm$ 0.0067}} & \textbf{0.2421}{\tiny{ $\pm$ 0.0016}} & \textbf{0.3357}{\tiny{ $\pm$ 0.0087}} & \textbf{0.2733}{\tiny{ $\pm$ 0.0041}} & \textbf{0.9354}{\tiny{ $\pm$ 0.0045}} &  \textbf{0.8012}{\tiny{ $\pm$ 0.0124}} & \textbf{0.2721}{\tiny{ $\pm$ 0.0046}} & \textbf{0.8077}{\tiny{ $\pm$ 0.0041}} & \textbf{0.1787}{\tiny{ $\pm$ 0.0026}}  \\ 
        (-) Edge. & 0.7191{\tiny{ $\pm$ 0.0036}} & – & 0.2942{\tiny{ $\pm$ 0.0043}} & 0.2219{\tiny{ $\pm$ 0.0060}} & 0.9292{\tiny{ $\pm$ 0.0034}} & 0.7714{\tiny{ $\pm$ 0.0204}} & 0.2628{\tiny{ $\pm$ 0.0019}} & 0.2994{\tiny{ $\pm$ 0.0062}} & 0.1785{\tiny{ $\pm$ 0.0033}} \\ 
        (-) Norm & 0.7107{\tiny{ $\pm$ 0.0027}} & 0.2509{\tiny{ $\pm$ 0.0026}} & 0.1802{\tiny{ $\pm$ 0.0111}} & 0.2332{\tiny{ $\pm$ 0.0079}} & 0.9236{\tiny{ $\pm$ 0.0054}} & 0.7753{\tiny{ $\pm$ 0.0049}} & 0.2528{\tiny{ $\pm$ 0.0016}} & 0.6705{\tiny{ $\pm$ 0.0104}} & 0.1679{\tiny{ $\pm$ 0.0027}} \\ 
        (-) Dropout &  0.6748{\tiny{ $\pm$ 0.0055}} & 0.2549{\tiny{ $\pm$ 0.0025}} & 0.3072{\tiny{ $\pm$ 0.0069}} & 0.2601{\tiny{ $\pm$ 0.0046}} & –  &  0.7431{\tiny{ $\pm$ 0.0185}} & 0.2405{\tiny{ $\pm$ 0.0047}} & 0.7893{\tiny{ $\pm$ 0.0052}} & 0.1641{\tiny{ $\pm$ 0.0043}}  \\ 
        (-) RC & – & – & 0.2734{\tiny{ $\pm$ 0.0036}} & 0.1948{\tiny{ $\pm$ 0.0096}} & 0.8916{\tiny{ $\pm$ 0.0048}} & – & – & 0.7520{\tiny{ $\pm$ 0.0157}} & 0.1785{\tiny{ $\pm$ 0.0029}} \\ 
        (-) FFN & – & – & 0.2786{\tiny{ $\pm$ 0.0068}} & 0.2314{\tiny{ $\pm$ 0.0073}} & 0.9118{\tiny{ $\pm$ 0.0078}} & 0.7432{\tiny{ $\pm$ 0.0052}} & 0.2621{\tiny{ $\pm$ 0.0019}} & 0.7672{\tiny{ $\pm$ 0.0071}} & 0.1594{\tiny{ $\pm$ 0.0020}} \\ 
        (-) PE & 0.7069{\tiny{ $\pm$ 0.0093}} & 0.2447{\tiny{ $\pm$ 0.0015}} & – & – & – & 0.7593{\tiny{ $\pm$ 0.0051}} & 0.2667{\tiny{ $\pm$ 0.0034}} &  – & – \\ 
        \midrule %
         \textbf{GIN$^+$} & \textbf{0.7059}{\tiny{ $\pm$ 0.0089}} & \textbf{0.2429}{\tiny{ $\pm$ 0.0019}} & \textbf{0.3189}{\tiny{ $\pm$ 0.0105}} & \textbf{0.2483}{\tiny{ $\pm$ 0.0046}} & \textbf{0.9325}{\tiny{ $\pm$ 0.0040}} & \textbf{0.7928}{\tiny{ $\pm$ 0.0099 }} & \textbf{0.2703}{\tiny{ $\pm$ 0.0024}} & \textbf{0.8107}{\tiny{ $\pm$ 0.0053}} & \textbf{0.1803}{\tiny{ $\pm$ 0.0019}}  \\
         (-) Edge. & 0.7033{\tiny{ $\pm$ 0.0015}} & 0.2442{\tiny{ $\pm$ 0.0028}} & 0.2956{\tiny{ $\pm$ 0.0047}} & 0.2259{\tiny{ $\pm$ 0.0053}} & 0.9286{\tiny{ $\pm$ 0.0049}} & 0.7597{\tiny{ $\pm$ 0.0103 }} & 0.2702{\tiny{ $\pm$ 0.0021}} & 0.2789{\tiny{ $\pm$ 0.0031}} & 0.1752{\tiny{ $\pm$ 0.0020}}  \\
        (-) Norm & 0.6934{\tiny{ $\pm$ 0.0077}} & 0.2444{\tiny{ $\pm$ 0.0015}} & 0.2707{\tiny{ $\pm$ 0.0037}} & 0.2244{\tiny{ $\pm$ 0.0063}} & 0.9322{\tiny{ $\pm$ 0.0025}} & 0.7874{\tiny{ $\pm$ 0.0114 }} & 0.2556{\tiny{ $\pm$ 0.0026}} & 0.6484{\tiny{ $\pm$ 0.0246}} & 0.1722{\tiny{ $\pm$ 0.0034}}  \\
         (-) Dropout & 0.6384{\tiny{ $\pm$ 0.0094}} & 0.2531{\tiny{ $\pm$ 0.0030}} & 0.3153{\tiny{ $\pm$ 0.0113}} & – & – & – & 0.2545{\tiny{ $\pm$ 0.0068}} & 0.7673{\tiny{ $\pm$ 0.0059}} & 0.1730{\tiny{ $\pm$ 0.0018}}  \\
         (-) RC & 0.6975{\tiny{ $\pm$ 0.0038}} & 0.2527{\tiny{ $\pm$ 0.0015}} & 0.2350{\tiny{ $\pm$ 0.0044}} & 0.1741{\tiny{ $\pm$ 0.0085}} & 0.9150{\tiny{ $\pm$ 0.0047}} & 0.7733{\tiny{ $\pm$ 0.0122 }} & 0.1454{\tiny{ $\pm$ 0.0061}} & – & 0.1617{\tiny{ $\pm$ 0.0026}}  \\
        (-) FFN & – & – & 0.2393{\tiny{ $\pm$ 0.0049}} & 0.1599{\tiny{ $\pm$ 0.0081}} & 0.8944{\tiny{ $\pm$ 0.0074}} & – & 0.2534{\tiny{ $\pm$ 0.0033}} & 0.6676{\tiny{ $\pm$ 0.0039}} & 0.1491{\tiny{ $\pm$ 0.0016}}  \\
        (-) PE & 0.6855{\tiny{ $\pm$ 0.0027}} & 0.2455{\tiny{ $\pm$ 0.0019}} & 0.3141{\tiny{ $\pm$ 0.0031}} & – & – & 0.7791{\tiny{ $\pm$ 0.0268 }} & 0.2601{\tiny{ $\pm$ 0.0023}} & – & –  \\
        \midrule %
        \textbf{GatedGCN$^+$} & \textbf{0.7006}{\tiny{ $\pm$ 0.0033}} & \textbf{0.2431}{\tiny{ $\pm$ 0.0020}} & \textbf{0.4263}{\tiny{ $\pm$ 0.0057}} & \textbf{0.3802}{\tiny{ $\pm$ 0.0015}} & \textbf{0.9460}{\tiny{ $\pm$ 0.0057}} & \textbf{0.8040}{\tiny{ $\pm$ 0.0164}} & \textbf{0.2981}{\tiny{ $\pm$ 0.0024}} & \textbf{0.8258}{\tiny{ $\pm$ 0.0055}} & \textbf{0.1896}{\tiny{ $\pm$ 0.0024}}  \\ 
        (-) Edge. & 0.6882{\tiny{ $\pm$ 0.0028}} & 0.2466{\tiny{ $\pm$ 0.0018}} & 0.3764{\tiny{ $\pm$ 0.0117}} & 0.3172{\tiny{ $\pm$ 0.0109}} & 0.9372{\tiny{ $\pm$ 0.0062}} & 0.7831{\tiny{ $\pm$ 0.0157}} & 0.2951{\tiny{ $\pm$ 0.0028}} & 0.0948{\tiny{ $\pm$ 0.0000}} & 0.1891{\tiny{ $\pm$ 0.0021}} \\ 
        (-) Norm & 0.6733{\tiny{ $\pm$ 0.0026}} & 0.2474{\tiny{ $\pm$ 0.0015}} & 0.3628{\tiny{ $\pm$ 0.0043}} & 0.3527{\tiny{ $\pm$ 0.0051}} & 0.9326{\tiny{ $\pm$ 0.0056}} & 0.7879{\tiny{ $\pm$ 0.0178}} & 0.2748{\tiny{ $\pm$ 0.0012}} & 0.6864{\tiny{ $\pm$ 0.0165}} & 0.1743{\tiny{ $\pm$ 0.0026}} \\ 
        (-) Dropout & 0.6695{\tiny{ $\pm$ 0.0101}} & 0.2508{\tiny{ $\pm$ 0.0014}} & 0.3389{\tiny{ $\pm$ 0.0066}} & 0.3393{\tiny{ $\pm$ 0.0051}} & – & – & 0.2582{\tiny{ $\pm$ 0.0036}} & 0.8088{\tiny{ $\pm$ 0.0062}} & 0.1724{\tiny{ $\pm$ 0.0027}}  \\ 
        (-) RC & – & 0.2498{\tiny{ $\pm$ 0.0034}} & 0.4075{\tiny{ $\pm$ 0.0052}} & 0.3475{\tiny{ $\pm$ 0.0064}} & 0.9402{\tiny{ $\pm$ 0.0054}} & 0.7833{\tiny{ $\pm$ 0.0177}} & 0.2897{\tiny{ $\pm$ 0.0016}} & 0.8099{\tiny{ $\pm$ 0.0053}} & 0.1844{\tiny{ $\pm$ 0.0025}} \\ 
        (-) FFN & – & – & – & 0.3508{\tiny{ $\pm$ 0.0049}} & 0.9364{\tiny{ $\pm$ 0.0059}} & – & 0.2875{\tiny{ $\pm$ 0.0022}} & – & 0.1718{\tiny{ $\pm$ 0.0024}} \\ 
        (-) PE & 0.6729{\tiny{ $\pm$ 0.0084}} & 0.2461{\tiny{ $\pm$ 0.0025}} & 0.4052{\tiny{ $\pm$ 0.0031}} & – & – & 0.7771{\tiny{ $\pm$ 0.0057}} & 0.2813{\tiny{ $\pm$ 0.0022}} & – & – \\ 
        \bottomrule
    \end{tabular}
    }
    \label{tab:ab2}
% \vspace{-0.05 in}
\end{table*}

To examine the unique contributions of different technique used in GNN$^+$, we conduct a series of ablation analysis by selectively removing elements such as edge feature module (Edge.), normalization (Norm), dropout, residual connections (RC), FFN, PE from GCN$^+$, GIN$^+$, and GatedGCN$^+$. The effect of these ablations is assessed across GNN Benchmark (see Table~\ref{tab:ab1}), LRGB, and OGB (see Table~\ref{tab:ab2}) datasets. 
% Our findings, which we detail below, indicate that the ablation of single components affects model accuracy in distinct ways.

\begin{tcolorbox}[colback=gray!10, colframe=black, boxrule=1pt, arc=1pt, left=3pt, right=3pt, top=2pt, bottom=2pt]
{
% Each module in GNN$^+$ is indispensable, yet its role varies across different datasets.
Our ablation study demonstrates that each module incorporated in GNN$^+$—including edge feature integration, normalization, dropout, residual connections, FFN, and PE—is \textbf{indispensable}; the removal of any single component results in a degradation of overall performance.
}
\end{tcolorbox}

\textbf{Observation 1: The integration of edge features is particularly effective in molecular and image superpixel datasets, where these features carry critical information.} 

In molecular graphs such as ZINC and ogbg-molhiv, edge features represent chemical bond information, which is essential for molecular properties. Removing this module leads to a significant performance drop. In protein networks ogbg-ppa, edges represent normalized associations between proteins. Removing the edge feature module results in a substantial accuracy decline, ranging from 0.5083 to 0.7310 for classic GNNs. Similarly, in image superpixel datasets like CIFAR-10, PascalVOC-SP, and COCO-SP, edge features encode spatial relationships between superpixels, which are crucial for maintaining image coherence. However, in code graphs such as ogbg-code2 and MalNet-Tiny, where edges represent call types, edge features are less relevant to the prediction tasks, and their removal has minimal impact.

\textbf{Observation 2: 
%Batch normalization generally plays a more significant role for larger-scale datasets, while its effect is less pronounced on smaller-scale datasets
Normalization tends to have a greater impact on larger-scale datasets, whereas its impact is less significant on smaller datasets.} 

For large-scale datasets such as CIFAR 10, COCO-SP, and the OGB datasets, removing normalization leads to significant performance drops. Specifically, on ogbg-ppa, which has 158,100 graphs, ablating normalization results in an accuracy drop of around 15\% for three classic GNNs. This result is consistent with \citet{luo2024classic}, who found that normalization is more important for GNNs in node classification on large graphs. In such datasets, where node feature distributions are more complex, normalizing node embeddings is essential for stabilizing the training process. 


\textbf{Observation 3: Dropout proves advantageous for most datasets, with a very low dropout rate being sufficient and optimal}. 

Our analysis highlights the crucial role of dropout in maintaining the performance of classic GNNs
on GNN Benchmark and LRGB and large-scale OGB datasets, with its ablation causing significant declines—for instance,  an 8.8\% relative decrease for GatedGCN$^+$ on CIFAR-10 and a 20.4\% relative decrease on PascalVOC-SP. This trend continues in large-scale OGB datasets, where removing dropout results in a 5–13\% performance drop across 3 classic GNNs on ogbg-molpcba. Notably, 97\% of the optimal dropout rates are $\le$ 0.2, and 64\% are $\le$ 0.1, indicating that a very low dropout rate is both sufficient and optimal for graph-level tasks. Interestingly, this finding for graph-level tasks contrasts with \citet{luo2024classic}’s observations for node-level tasks, where a higher dropout rate is typically required.

% It is crucial to emphasize that a small, yet non-negligible, dropout plays a significant role.

% \textbf{Observation 4: Residual connection is almost consistently essential for graph-level tasks, except in shallow GNNs for small graphs}. Removing residual connections generally leads to significant performance drops across datasets. The only exceptions are found in the peptide datasets. Although similar in the number of nodes to CLUSTER and PATTERN, peptide datasets involve GNNs with only 3-5 layers, while the others employ deeper networks with over 10 layers. For shallow networks in small graphs, residual connections may not be as beneficial and can even hurt performance by disrupting feature flow. In contrast, deeper networks in larger graphs may rely on residual connections to maintain gradient flow and stabilize training.

\textbf{Observation 4: Residual connections are generally essential, except in shallow GNNs applied to small graphs.} 

Removing residual connections generally leads to significant performance drops across datasets, with the only exceptions being found in the peptide datasets. Although similar in the number of nodes to CLUSTER and PATTERN, peptide datasets involve GNNs with only 3-5 layers, while the others use deeper networks with over 10 layers. For shallow networks in small graphs, residual connections may not be as beneficial and can even hurt performance by disrupting feature flow. In contrast, deeper networks in larger graphs rely on residual connections to maintain gradient flow and enable stable, reliable long-range information exchange.

\textbf{Observation 5: FFN is crucial for GIN$^+$ and GCN$^+$, greatly impacting their performance across datasets.}

Ablating FFN leads to substantial performance declines for GIN$^+$ and GCN$^+$ across almost all datasets, highlighting its essential role in graph-level tasks. Notably, on MNIST, removing FNN leads to an 88\% relative accuracy drop for GIN$^+$. This is likely because the architectures of GIN$^+$ and GCN$^+$ rely heavily on FFN for learning complex node feature representations. In contrast, GatedGCN$^+$ uses gating mechanisms to adaptively adjust the importance of neighboring nodes’ information, reducing the need for additional feature transformations. 
The only exceptions are observed in the peptides datasets, where FFN is not used in all three models.
This may be due to the shallow GNN architecture, where complex feature transformations are less necessary.

\textbf{Observation 6: PE is particularly effective for small-scale datasets, but negligible for large-scale datasets.} 

Removing PE significantly reduces performance for classic GNNs on small-scale datasets like ZINC, PATTERN, CLUSTER, Peptides-func, and ogbg-molhiv, which only contain 10,000-40,000 graphs. By contrast, on large-scale datasets like ogbg-code2, ogbg-molpcba, ogbg-ppa, and COCO-SP (over 100,000 graphs), the impact of PE is less pronounced. This may be because smaller datasets rely more on PE to capture graph structure, whereas larger datasets benefit from the abundance of data, reducing the need for PE.

% \textbf{Observation 2: Dropout is consistently found to be essential for node classification.}

% \textbf{Observation 3: Residual connections can significantly boost performance on specific datasets, exhibiting a more pronounced effect on heterophilous graphs than on homophilous graphs.}