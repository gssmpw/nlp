\section{Classic GNNs for Graph-level Tasks}
% \vspace{-0.05 in}
\label{sec:pre}

Define a graph as \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, \boldsymbol{X}, \boldsymbol{E}) \), where \(\mathcal{V} \) is the set of nodes, and \( \mathcal{E} \subseteq \mathcal{V} \times \mathcal{V} \) is the set of edges. The node feature matrix is \( \boldsymbol{X} \in \mathbb{R}^{|\mathcal{V}| \times d_\mathcal{V}} \),  where \( |\mathcal{V}| \) is the number of nodes, and \( d_\mathcal{V} \) is the dimension of the node features. The edge feature matrix is \( \boldsymbol{E} \in \mathbb{R}^{|\mathcal{E}| \times d_\mathcal{E}} \),  where \( |\mathcal{E}| \) is the number of edges and \( d_\mathcal{E} \) is the dimension of the edge features. Let \( \boldsymbol{A} \in \mathbb{R}^{|\mathcal{V}| \times |\mathcal{V}|} \) denote the adjacency matrix of $\mathcal{G}$.

%Each row in \( \boldsymbol{X} \) corresponds to the feature vector of a node,. 
%The feature vector of a node \( v \) is denoted by \( \boldsymbol{x}_v \in \mathbb{R}^d \). 
%The connectivity of the graph is also represented by the adjacency matrix \( A \in \mathbb{R}^{|V| \times |V|} \). 


\textbf{Message-passing Graph Neural Networks (GNNs)} compute node representations $\boldsymbol{h}_{v}^{l}$ at each layer $l$ via a message-passing mechanism, defined by \citet{gilmer2017neural}: 
\begin{equation}	\boldsymbol{h}_{v}^{l}=\text{UPDATE}^{l}\left( \boldsymbol{h}_{v}^{l -1},\text{AGG}^{l}\left( \left\{ \boldsymbol{h}_{u}^{l-1}\mid u\in \mathcal{N}\left( v \right) \right\} \right) \right),
 % \nonumber
 \label{eq1}
\end{equation} 
where \(\mathcal{N}(v)\) represents the neighboring nodes adjacent to \(v\), \(\text{AGG}^{l}\) is the message aggregation function, and $\text{UPDATE}^{l}$ is the update function.
% and $\mathrm{R}$ are readout functions.
Initially, each node \(v\) is assigned a feature vector \(\boldsymbol{h}_{v}^{0} = \boldsymbol{x}_v \in \mathbb{R}^d\).
The function $\text{AGG}^{l}$ is then used to aggregate information from the neighbors of $v$ to update its representation. 
The output of the last layer $L$, i.e., \(\text{GNN}(v, \boldsymbol{A}, \boldsymbol{X}) = \boldsymbol{h}_{v}^{L}\), is the representation of $v$ produced by the GNN. In this work, we focus on three classic GNNs: GCN \cite{kipf2017semisupervised}, GIN \cite{xu2018powerful}, and GatedGCN \cite{bresson2017residual}, which differ in their approach to learning the node representation $\boldsymbol{h}_{v}^{l}$. 

\textbf{Graph Convolutional Networks (GCN)}~\cite{kipf2017semisupervised}, the vanilla GCN model, is formulated as:
\begin{equation}
\boldsymbol{h}_v^l = \sigma(\sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} \boldsymbol{h}_{u}^{l-1}\boldsymbol{W}^l),
\label{eq2}
\end{equation} 
where
$\hat{d}_v = 1 + \sum_{u \in \mathcal{N}(v)} 1$, \(\sum_{u \in \mathcal{N}(v)} 1\) denotes the degree of node \(v\), $\boldsymbol{W}^l$ is the trainable weight matrix in layer $l$, and \(\sigma\) is the activation function, e.g., ReLU(·) = \(\max(0, \text{·})\). 

\textbf{Graph Isomorphism Networks (GIN)} \cite{xu2018powerful} %utilizes node feature information to generate node embeddings through a different approach:
learn node representations through a different approach:
\begin{equation}
\boldsymbol{h}_v^l =  \text{MLP}^l((1 + \epsilon) \cdot \boldsymbol{h}_{v}^{l-1} + \sum_{u \in\mathcal{N}(v)} \boldsymbol{h}_u^{l-1}),
\end{equation}
where $\epsilon$ is a constant, typicallyset to 0, and $ \text{MLP}^l$ denotes a multi-layer perceptron, which usually consists of 2 layers.

\textbf{Residual Gated Graph Convolutional Networks (GatedGCN)}~\cite{bresson2017residual}
enhance traditional graph convolutions by incorporating gating mechanisms, improving adaptability and expressiveness:
\begin{equation}
\boldsymbol{h}_v^l = \boldsymbol{h}_v^{l-1}\boldsymbol{W}_1^l + \sum_{u \in \mathcal{N}(v)} \boldsymbol{\eta}_{v,u} \odot \boldsymbol{h}_u^{l-1}\boldsymbol{W}_2^l,
\label{eq3}
\end{equation}
where \(\boldsymbol{\eta}_{v,u} = \sigma(\boldsymbol{h}_v^{l-1} \boldsymbol{W}_3^l + \boldsymbol{h}_u^{l-1} \boldsymbol{W}_4^l)\) is the gating function, and \(\sigma\) denotes the sigmoid activation function. This gating function determines how much each neighboring node contributes to updating the representation of the current node. The matrices \(\boldsymbol{W}_1^l\), \(\boldsymbol{W}_2^l\), \(\boldsymbol{W}_3^l\), \(\boldsymbol{W}_4^l\) are trainable weight matrices specific to the layer $l$. 
% This structure allows selective information aggregation based on the relevance and connectivity of neighboring nodes, enhancing the model's ability to capture complex patterns in graph data.

\textbf{Graph-level tasks} treat the entire graph, rather than individual nodes or edges, as the fundamental unit for dataset composition, splitting, and training. Formally, given a labeled graph dataset $\Gamma = \{(\mathcal{G}_i, \boldsymbol{y}_i)\}_{i=1}^n$, each graph $\mathcal{G}_i$ is associated with a label vector $\boldsymbol{y}_i$, representing either categorical labels for classification or continuous values for regression. Next, the dataset $\Gamma$ is typically split into training, validation, and test sets, denoted as $\Gamma = \Gamma_\text{train} \cup \Gamma_\text{val} \cup \Gamma_\text{test}$. 

%Graph-level tasks involve inductive prediction tasks on nodes, edges, or entire graphs, 

Graph-level tasks encompass inductive prediction tasks that operate on entire graphs, as well as on individual nodes or edges~\cite{dwivedi2022long}, with each corresponding to a distinct label vector $\boldsymbol{y}_i$. Each type of task requires a tailored graph readout function $\mathrm{R}$, which aggregates the output representations to compute the readout result, expressed as:
% which aggregates the output representations $\boldsymbol{h}_v^L$ of node $v$ in $\mathcal{V}_i$ to compute the readout result. The general form of the readout operation is expressed as:
\begin{equation}
\boldsymbol{h}^\text{readout}_i = \mathrm{R}\left( \left\{ \boldsymbol{h}_v^L : v \in \mathcal{V}_i \right\} \right),
\label{readout}
\end{equation}
where $\mathcal{V}_i$ represents the set of nodes in the graph $\mathcal{G}_i$. For example, for \emph{graph prediction tasks}, which aim to make predictions about the entire graph, the readout function $\mathrm{R}$ often operates as a global mean pooling function. 

Finally, for any graph $\mathcal{G}_i$, the readout result is passed through a prediction head \( g(\text{·}) \) to obtain the predicted label \( \hat{\boldsymbol{y}}_i = g(\boldsymbol{h}^\text{readout}_i) \). The training objective is to minimize the total loss \( L(\boldsymbol{\theta}) = \sum_{\mathcal{G}_i \in \Gamma_{\text{train}}} \ell(\hat{\boldsymbol{y}}_i, \boldsymbol{y}_i) \) w.r.t. all graphs in the training set $\Gamma_{\text{train}}$, where \(\boldsymbol{y}_i\) represents the ground-truth label of $\mathcal{G}_i$ and \(\boldsymbol{\theta}\) denotes the trainable GNN parameters. 

% This pooling aggregates node-level representations into a single vector, calculated as:
% \begin{equation}
% \boldsymbol{h}_\text{readout} = \frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \boldsymbol{h}_v^L.
% \end{equation}

% This readout step ensures that information from all nodes is effectively summarized to represent the entire graph, enabling the prediction of global properties.


% \textbf{Various Graph Downstream Tasks.} Classification tasks in the graph domain can be categorized into: (1) node-level tasks, which involve classifying individual nodes within the graph. (2) link-level tasks, which focus on reasoning about the connections between pairs of nodes. (3) graph-level tasks, which aim to make predictions about the entire graph. In the context of link-level tasks, the readout function \(\mathrm{R}\) can be a Hadamard product of the target node pair \((u, v)\) representations, formulated as \(h_\text{readout} = h_v^L \odot h_u^L\) \citep{kipf2016variational}. For graph-level tasks, \(\mathrm{R}\) can be a global mean pooling of all node vectors in graph \(G\), expressed as $h_\text{readout} = \frac{1}{|V|} \sum_{v \in V} h_v^L$. For node-level tasks, \(\mathrm{R}\) may simply be an identity mapping.