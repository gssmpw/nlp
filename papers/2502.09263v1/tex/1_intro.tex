\vspace{-0.2 in}
\section{Introduction}

% Graph-level tasks has emerged as a significant field within the graph learning community due to their broad applicability across various domains \cite{zhang2018end,dwivedi2023benchmarking,ju2024comprehensive}, 

Graph machine learning addresses both graph-level tasks and node-level tasks, as illustrated in Figure~\ref{fig:graphlevel}. 
% These tasks differ fundamentally in their choice of the basic unit for data splitting, training, and prediction,
These tasks fundamentally differ in their choice of the basic unit for dataset composition, splitting, and training,
with graph-level tasks focusing on the entire graph, while node-level tasks focus on individual nodes. Graph-level tasks \cite{dwivedi2023benchmarking,hu2020open,luo2023impact,luo2023improving} often involve the classification of relatively small molecular graphs in chemistry \cite{morris2020tudataset} or the prediction of protein properties in biology \cite{dwivedi2022long}. In contrast, node-level tasks typically involve large social networks \cite{tang2009social} or citation networks \cite{yang2016revisiting}, where the primary goal is node classification. This distinction in the fundamental unit of dataset leads to differences in methodologies, training strategies, and application domains.

% Graph-level tasks, as illustrated in Figure~\ref{fig:graphlevel}, consider the entire graph, rather than individual nodes or edges, as the fundamental unit for data splitting, training, and prediction. These tasks encompass a wide range of applications \cite{zhang2018end,dwivedi2023benchmarking,ju2024comprehensive}, including molecular property prediction in chemistry \cite{duvenaud2015convolutional,gilmer2017neural,morris2020tudataset}, protein classification in biology \cite{hu2020open,dwivedi2022long}, and source code recognition in software engineering \cite{allamanis2018survey,freitas2021large}.

% Graph-level tasks treat the entire graph, rather than individual nodes or edges, as the fundamental unit for splitting, training and predictions,
% including molecular property prediction in chemistry \cite{duvenaud2015convolutional,gilmer2017neural,morris2020tudataset}, protein classification in biology \cite{hu2020open,dwivedi2022long}, and source code recognition in software engineering \cite{allamanis2018survey,freitas2021large}. 

Message-passing Graph Neural Networks (GNNs) \cite{gilmer2017neural}, which iteratively aggregate information from local neighborhoods to learn node representations, have become the predominant approach for both graph-level and node-level tasks \cite{niepert2016learning,kipf2017semisupervised,velivckovic2018graph,xu2018powerful,bresson2017residual,wu2020comprehensive}. Despite their widespread success, GNNs exhibit several inherent limitations, including restricted expressiveness \cite{xu2018powerful,morris2019weisfeiler}, over-smoothing \cite{li2018deeper,chen2020measuring}, over-squashing \cite{alon2020bottleneck}, and a limited capacity to capture long-range dependencies \cite{dwivedi2022long}.

\begin{figure}[t]   \center{\includegraphics[width=7.5cm]  {graph-level-task.jpg}}   \vspace{-0.05 in} \caption{\label{1} 
Differences between graph-level and node-level tasks.} \vspace{-0.15 in}  \label{fig:graphlevel} \end{figure}

A prevalent perspective is that Graph Transformers (GTs) \cite{muller2023attending,min2022transformer,hoang2024survey}, as an alternative to GNNs, leverage global attention mechanisms that enable each node to attend to all others \cite{yun2019graph,dwivedi2020generalization}, effectively modeling long-range interactions and addressing issues such as over-smoothing, over-squashing, and limited expressiveness \cite{kreuzer2021rethinking,ying2021transformers,zhang2023rethinking,luo2023transformers,luo2024transformers}. %While recent advances in state-of-the-art (SOTA) GTs have shown promising results, their quadratic complexity constrains scalability in large-scale, real-world settings \cite{behrouz2024graph,sancak2024scalable,ding2024recurrent}. 
% Moreover, GTs often preprocess complex graph structures into fixed-length node embeddings, inherently treating graphs as collections of nodes and risking the loss of essential local structural information \cite{chen2024learninglongrangedependencies}. 
However, the quadratic complexity of global attention mechanisms limits the scalability of GTs in large-scale, real-world applications \cite{behrouz2024graph,sancak2024scalable,ding2024recurrent}.
Moreover, it has been noted that many state-of-the-art GTs \cite{chen2022structure,rampavsek2022recipe,shirzad2023exphormer,ma2023graph} still rely—either explicitly or implicitly—on the message passing mechanism of GNNs to learn local node representations, thereby enhancing performance.

%which are integrated via global attention for a more comprehensive representation. 

% This reliance raises the question of whether the potential of GNNs has been underestimated.

% Moreover, GTs often require preprocessing complex graph structures into fixed-length vector representations for each node, inherently treating graphs as sets of nodes and potentially losing crucial local structural information \cite{chen2024learninglongrangedependencies}.

% In light of these challenges, we observe that many recent state-of-the-art (SOTA) GTs for graph-level tasks \cite{chen2022structure,rampavsek2022recipe,shirzad2023exphormer,ma2023graph} still rely—whether explicitly or implicitly—on message passing to learn local node representations, which are then integrated with global attention mechanisms to form a more comprehensive representation. This prompts the question of whether the potential of message-passing GNNs for graph-level tasks has been previously underestimated.

% At the same time, several recent GNN variants, which entirely omit global self-attention and rely solely on message passing, have achieved comparable performance to GTs in some graph-level tasks. 

% Recently, \citet{luo2024classic} have already demonstrated that, with identical hyperparameter tuning, classic GNNs can outperform SOTA GTs on most node-level tasks. By contrast, graph-level performance remains relatively unexplored: \citet{tonshoff2023did} only preliminarily found that, by adjusting dropout and a few other hyperparameters, classic GNNs can surpass GTs on certain datasets heavily reliant on long-range interactions. These findings raise an important question: \emph{Have we been underestimating the potential of GNNs for graph-level tasks all along?}

Recent studies~\cite{luo2024classic,luo2025node,luo2025beyond} have shown that, contrary to common belief, classic GNNs such as GCN~\cite{kipf2017semisupervised}, GAT~\cite{velivckovic2018graph}, and GraphSAGE~\cite{hamilton2017inductive} can achieve performance comparable to, or even exceeding, that of state-of-the-art GTs for node-level tasks. However, a similar conclusion has not yet been established for graph-level tasks. While \citet{tonshoff2023did} conducted pioneering research demonstrating that tuning a few hyperparameters can significantly enhance the performance of classic GNNs, their results indicate that these models still do not match the overall performance of GTs. Furthermore, their investigation is limited to the Long-Range Graph Benchmark (LRGB)~\cite{dwivedi2022long}. This raises an important question: \emph{``Can classic GNNs also excel in graph-level tasks?''}

%While \citet{tonshoff2023did} conducted pioneering research indicating that by adjusting dropout rates and a few other hyperparameters, classic GNNs can outperform GTs on the Long-Range Graph Benchmark (LRGB)~\cite{dwivedi2022long}, their investigation is limited to datasets that rely on long-range interactions. This raises an important question: \emph{``Can classic GNNs also excel in graph-level tasks?''}

To thoroughly investigate this question, we introduce GNN$^+$, an enhanced GNN framework that incorporates established techniques into the message-passing mechanism, to effectively address graph-level tasks. As illustrated in Fig.~\ref{fig:architecture}, GNN$^+$ integrates six widely used techniques: the incorporation of edge features~\cite{gilmer2017neural}, normalization~\cite{ioffe2015batch}, dropout~\cite{srivastava2014dropout}, residual connections~\cite{he2016deep}, feed-forward networks (FFN)~\cite{vaswani2017attention}, and positional encoding~\cite{vaswani2017attention}. Each technique serves as a hyperparameter that can be tuned to optimize performance.

%In this work, we introduce GNN$^+$, an enhanced GNN architecture designed to provide a comprehensive assessment of GNN baselines.
% GNN$^+$ incorporates widely used neural network components, including edge feature modules \cite{gilmer2017neural}, normalization \cite{ioffe2015batch}, dropout \cite{srivastava2014dropout}, residual connections \cite{he2016deep}, fully connected feed-forward networks (FFNs) and positional encodings \cite{vaswani2017attention}. 


% By systematically evaluating 3 classic GNNs—GCN \cite{kipf2017semisupervised}, GIN \cite{xu2018powerful}, and GatedGCN \cite{bresson2017residual}—across 14 well-known graph-level datasets using GNN$^+$, we demonstrate that these models match or even outperform SOTA GTs by ranking in the top three on all datasets, including first place on 8 of them, while also exhibiting superior efficiency. These results challenge the prevailing assumptions that GNNs are constrained by issues such as over-smoothing, over-squashing, and difficulties in capturing long-range dependencies, suggesting that the true potential of GNNs for graph-level applications has indeed been previously underestimated.


We systematically evaluate 3 classic GNNs—GCN~\cite{kipf2017semisupervised}, GIN~\cite{xu2018powerful}, and GatedGCN~\cite{bresson2017residual}—enhanced by the GNN$^+$ framework across 14 well-known graph-level datasets from GNN Benchmark \cite{dwivedi2023benchmarking}, LRGB \cite{dwivedi2022long}, and OGB \cite{hu2020open}. The results demonstrate that the enhanced versions of classic GNNs match or even outperform state-of-the-art (SOTA) GTs, achieving rankings in the \textbf{top three}, including \textbf{first place in eight datasets}, while exhibiting superior efficiency. These findings provide a \emph{positive answer} to the previously posed question, suggesting that the true potential of GNNs for graph-level applications has been previously underestimated, and the GNN$^+$ framework effectively unlocks this potential while addressing their inherent limitations. 
Our ablation study also highlights the importance of each technique used in GNN$^+$ and offers valuable insights for future research.

% Our key findings from the empirical study are as follows:
% \begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
% \item
% With proper tuning, classic GNNs achieve highly competitive performance on graph-level tasks across the GNN Benchmark, LRGB, and large-scale OGB datasets. Notably, classic GNNs outperform SOTA GTs by ranking in the top three across all 14 datasets and achieving the top rank on 8 out of 14 datasets. This suggests that the previously claimed superiority of GTs over GNNs may have been overstated, possibly due to suboptimal hyperparameter configurations in GNN evaluations.
% \vspace{0.05 in}
% \item
% Our ablation studies provide valuable insights into GNN hyperparameters:
% (1) normalization plays a more significant role on larger-scale datasets;
% (2) dropout benefits most datasets, with a subtle dropout rate proving sufficient and optimal;
% (3) residual connections are consistently essential for graph-level tasks; (4) FFNs are especially important for simpler models like GCN in graph-level tasks; (5) positional encodings are more critical for small-scale datasets than for large-scale ones.
% \end{itemize}

% \subsection{Related Work}

% Our work is closely related to previous graph benchmarking efforts \cite{dwivedi2022long, dwivedi2023benchmarking, hu2020open, tonshoff2023did,grotschla2024benchmarking}, which underscore the importance of rigorous evaluation of graph learning architectures. However, these studies have been constrained in both scope and comprehensiveness, primarily due to the limited number and diversity of datasets used, as well as an incomplete examination of GNN hyperparameters.
