%\section{Key Hyperparameters for Training GNNs} \label{sec3}

% \section{SGA: A Simple GNN Architecture} \label{sec3}

\section{GNN$^{+}$: Enhancing Classic GNNs for Graph-level Tasks} \label{sec3}

\begin{figure}[t]   \center{\includegraphics[width=7.5cm]  {gnn_2.jpg}}  \vspace{-0.1 in}  \caption{\label{1} 
The architecture of GNN$^{+}$.}  \vspace{-0.1 in}
\label{fig:architecture} \end{figure}

We propose an enhancement to classic GNNs for graph-level tasks by incorporating six popular techniques: edge feature integration, normalization, dropout, residual connections, feed-forward networks (FFN), and positional encoding. The enhanced framework, GNN$^{+}$, is illustrated in Figure~\ref{fig:architecture}.

% In practice, researchers often incorporate commonly used components, such as dropout, into GNNs to enhance model performance. However, these additions are typically scattered and inconsistently applied across different studies. To systematically assess the performance of GNN baselines, we introduce SGA, a simple GNN architecture. As shown in Figure~\ref{fig:architecture}, SGA integrates several key components (hyperparameters) essential for training GNNs—such as an edge feature module, normalization, dropout, residual connections, a fully connected feed-forward network, and positional encodings—into the message-passing mechanism.

% In this section, we introduce SGA, a simple GNN architecture designed as new GNN baselines. As shown in Figure~\ref{fig:architecture}, SGA incorporates several key components (hyperparameters) essential for training classic GNNs, including the edge feature module, normalization layers, dropout, residual connections, a fully connected feed-forward network module, and positional encodings. These hyperparameters are commonly employed across various neural network architectures to enhance model performance.

% In this section, we provide an overview of the key hyperparameters for training GNNs, as illustrated in Figure~\ref{fig:architecture}. These include the edge feature module, normalization, dropout, residual connections, fully connected feed-forward network module, positional encodings, and network depth. Such hyperparameters are commonly employed across various neural network architectures to enhance model performance.

\subsection{Edge Feature Integration}

% With the advent of GraphGPS \cite{rampavsek2022recipe} and subsequent GTs, their local modules encode real edge features into the node-level hidden representations. However, these works often do not add edge features to baseline GNNs, leading to inconsistent comparisons between their models and classic GNNs. In fact, the concept of incorporating edge features was already established in general GNN frameworks \cite{gilmer2017neural, hu2019strategies}, which directly integrate edge features into the message-passing phase to enhance the propagation of information between nodes. To address this, we treat the edge feature module as a tunable hyperparameter, 
% The concept of incorporating edge features was already established in general GNN frameworks \cite{gilmer2017neural, hu2019strategies}, which directly integrate edge features into the message-passing phase to enhance the propagation of information between nodes. Building on these principles, GraphGPS \cite{rampavsek2022recipe} and subsequent GTs also encode real edge features in their local modules to enrich node representations. To systematically examine this capability, we treat the edge feature module as a tunable hyperparameter, 
% taking GCN (eq.~\ref{eq2}) as an example:

Edge features were initially incorporated into some GNN frameworks \cite{gilmer2017neural, hu2019strategies} by directly integrating them into the message-passing process to enhance information propagation between nodes. Following this practice, GraphGPS \cite{rampavsek2022recipe} and subsequent GTs encode edge features within their local modules to enrich node representations. 
%To systematically examine this capability, we treat the edge feature module as a tunable hyperparameter, 

Taking GCN (Eq.~\ref{eq2}) as an example, the edge features are integrated into the massage-passing process as follows:
\begin{equation}
\boldsymbol{h}_v^l = \sigma(\sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} \boldsymbol{h}_{u}^{l-1}\boldsymbol{W}^l+\boldsymbol{e}_{uv}\boldsymbol{W}^l_e),
\end{equation} 
where $\boldsymbol{W}^l_e$ is the trainable weight matrix in layer $l$, and $\boldsymbol{e}_{uv}$ is the feature vector of the edge between $u$ and $v$.

\subsection{Normalization}

Normalization techniques play a critical role in stabilizing the training of GNNs by mitigating the effects of \emph{covariate shift}, where the distribution of node embeddings changes across layers during training. By normalizing node embeddings at each layer, the training process becomes more stable, enabling the use of higher learning rates and achieving faster convergence \cite{cai2021graphnorm}.

% One widely adopted method is Batch Normalization (BN) \cite{ioffe2015batch}, which can be applied to the output of each layer \emph{before} the activation function \(\sigma(\cdot)\):
Batch Normalization (BN) \cite{ioffe2015batch} and Layer Normalization (LN) \cite{ba2016layer} are widely used techniques, typically applied to the output of each layer \emph{before} the activation function \(\sigma(\cdot)\). Here, we use BN:
\begin{equation}
\boldsymbol{h}_v^l = \sigma(\text{BN}(\sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} \boldsymbol{h}_{u}^{l-1}\boldsymbol{W}^l + \boldsymbol{e}_{uv}\boldsymbol{W}^l_e)).
\end{equation}
\vspace{-0.2 in}
\subsection{Dropout}

Dropout~\cite{srivastava2014dropout}, a technique widely used in convolutional neural networks (CNNs) to address overfitting by reducing co-adaptation among hidden neurons~\cite{hinton2012improving,yosinski2014transferable}, has also been found to be effective in addressing similar issues in GNNs~\cite{shu2022understanding}, where the co-adaptation effects propagate and accumulate via message passing among different nodes. Typically, dropout is applied to the embeddings \emph{after} activation:
\begin{align}
\nonumber \boldsymbol{h}_v^l = \text{Dropout}(\sigma(\text{BN}(\sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} \boldsymbol{h}_{u}^{l-1}\boldsymbol{W}^l \\+ 
\boldsymbol{e}_{uv}\boldsymbol{W}^l_e))).
\end{align} 
\vspace{-0.35 in}
\subsection{Residual Connection}
Residual connections~\cite{he2016deep} significantly enhance CNN performance by directly connecting the input of a layer to its output, thus alleviating the problem of vanishing gradient. They were first adopted by the vanilla GCN~\cite{kipf2017semisupervised} and has since been incorporated into subsequent works such as GatedGCN~\cite{bresson2017residual} and DeepGCNs~\cite{li2019deepgcns}. Formally, residual connections can be integrated into GNNs as follows:
\vspace{-0.05 in}
\begin{align}
\nonumber \boldsymbol{h}_v^l =  \text{Dropout}(\sigma(\text{BN}(  \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} \boldsymbol{h}_{u}^{l-1}\boldsymbol{W}^l \\ + 
\boldsymbol{e}_{uv}\boldsymbol{W}^l_e))) + \boldsymbol{h}_{v}^{l-1}.
\end{align} 
While deeper networks, such as deep CNNs~\cite{he2016deep,huang2017densely}, are capable of extract more complex features, GNNs encounter challenges like over-smoothing~\cite{li2018deeper}, where deeper models lead to indistinguishable node representations. Consequently, most GNNs are shallow, typically with 2 to 5 layers. However, by incorporating residual connections, we show that deeper GNNs, ranging from 3 to 20 layers, can achieve strong performance.
 
\subsection{Feed-Forward Network} 
GTs incorporate a feed-forward network (FFN) as a crucial component within each of their layers. The FFN enhances the model's ability to perform complex feature transformations and introduces non-linearity, thereby increasing the network's expressive power. Inspired by this, we propose appending a fully-connected FFN at the end of each layer of GNNs, defined as:
\begin{align}
\text{FFN}(\boldsymbol{h}) = \text{BN}(\sigma(\boldsymbol{h}\boldsymbol{W}_{\text{FFN}_1}^l)\boldsymbol{W}_{\text{FFN}_2}^l + \boldsymbol{h}),
\end{align}
where
$\boldsymbol{W}_{\text{FFN}_1}^l$ and $\boldsymbol{W}_{\text{FFN}_2}^l$ are the trainable weight matrices of the FFN at the $l$-th GNN layer. The node embeddings output by the FFN are then computed as:
\vspace{-0.05 in}
\begin{align}
\nonumber &\boldsymbol{h}_v^l =  \text{FFN}(\text{Dropout}(\sigma(\text{BN}(  \sum_{u \in \mathcal{N}(v) \cup \{v\}} \frac{1}{\sqrt{\hat{d}_u \hat{d}_v}} \boldsymbol{h}_{u}^{l-1}\boldsymbol{W}^l \\  &  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + \boldsymbol{e}_{uv}\boldsymbol{W}^l_e))) + \boldsymbol{h}_{v}^{l-1}). 
% &\text{with}~~\text{FFN}(\boldsymbol{h}) = \text{BN}(\sigma(\boldsymbol{h}\boldsymbol{W}_{\text{FFN}_1}^l)\boldsymbol{W}_{\text{FFN}_2}^l + \boldsymbol{h}),
\end{align} 
% where
% $\boldsymbol{W}_{\text{FFN}_1}^l$ and $\boldsymbol{W}_{\text{FFN}_2}^l$ are the trainable weight matrices in layer $l$. 
% Multiple such layers can be stacked together to ultimately provide node-level representations for the graph.
\vspace{-0.3 in}
\subsection{Positional Encoding} 

Positional encoding (PE) was introduced in the Transformer model \cite{vaswani2017attention} to represent the positions of tokens within a sequence for language modeling. In GTs, PE is used to incorporate graph positional or structural information. The encodings are typically added or concatenated to the input node features $\boldsymbol{x}_v$ before being fed into the GTs. Various PE methods have been proposed, such as Laplacian Positional Encoding (LapPE) \cite{dwivedi2020generalization,kreuzer2021rethinking}, Weisfeiler-Lehman Positional Encoding (WLPE) \cite{zhang2020graph}, Random Walk Structural Encoding (RWSE) \cite{li2020distance,dwivedi2021graph,rampavsek2022recipe}, Learnable Structural and Positional Encodings (LSPE) \cite{dwivedi2021graph}, and Relative Random Walk Probabilities (RRWP) \cite{ma2023graph}. 
%In practice, PE can also improve the performance of classic GNNs. 
Following the practice, we use RWSE, one of the most efficient PE methods, to improve the performance of GNNs as follows:
\begin{equation}
\boldsymbol{x}_v = [\boldsymbol{x}_v \| \boldsymbol{x}^\text{RWSE}_v] \boldsymbol{W}_\text{PE},
\end{equation}
where \([\cdot \| \cdot]\) denotes concatenation, \(\boldsymbol{x}^\text{RWSE}_v\) represents the RWSE of node \(v\), and \(\boldsymbol{W}_\text{PE}\) is the trainable weight matrix.

\begin{table}[t]
\vspace{-0.1 in}
	\centering
         \caption{Overview of the datasets used for graph-level tasks.}
         \resizebox{\linewidth}{!}{
	\begin{tabular}{lcccc}
		\toprule
		{Dataset} & {\# graphs} & {Avg. \# nodes} & {Avg. \# edges} & {Task Type} \\
		\midrule
        ZINC & 12,000 & 23.2 & 24.9 & Graph regression \\
        MNIST & 70,000 & 70.6 & 564.5 & Graph classification \\
        CIFAR10 & 60,000 & 117.6 & 941.1 & Graph classification \\
        PATTERN & 14,000 & 118.9 & 3,039.3 & Inductive node cls. \\
        CLUSTER & 12,000 & 117.2 & 2,150.9 & Inductive node cls. \\
        \midrule
        Peptides-func & 15,535 & 150.9 & 307.3 & Graph classification \\
        Peptides-struct & 15,535 & 150.9 & 307.3 & Graph regression \\
        PascalVOC-SP & 11,355 & 479.4 & 2,710.5 & Inductive node cls. \\
        COCO-SP & 123,286 & 476.9 & 2,693.7 & Inductive node cls. \\
        MalNet-Tiny & 5,000 & 1,410.3 & 2,859.9 & Graph classification \\
        \midrule
        ogbg-molhiv & 41,127 & 25.5 & 27.5 & Graph classification \\
        ogbg-molpcba & 437,929 & 26.0 & 28.1 & Graph classification \\
        ogbg-ppa & 158,100 & 243.4 & 2,266.1 & Graph classification \\
        ogbg-code2 & 452,741 & 125.2 & 124.2 & Graph classification \\
        \bottomrule
	\end{tabular}}
	\label{tab:dataset-s}
    \vspace{-0.1 in}
\end{table}
\begin{table*}[t]
\footnotesize
	\centering
        \vspace{-0.05 in}
         \caption{Test performance on five benchmarks from \cite{dwivedi2023benchmarking} (\%). Shown is the mean $\pm$ s.d. of 5 runs with different random seeds. $^+$ denotes the enhanced version, while the baseline results were obtained from their respective original papers. \# Param $\sim$ 500K for ZINC, PATTERN, and CLUSTER, and $\sim$ 100K for MNIST and CIFAR10.
         The top \textbf{\textcolor{customcyan}{$\mathbf{1^{st}}$}}, \textbf{\textcolor{tealblue!90}{$\mathbf{2^{nd}}$}} and \textbf{\textcolor{darkorange!90}{$\mathbf{3^{rd}}$}} results are highlighted.}
        % \vspace{-0.1 in}
         \resizebox{0.95\linewidth}{!}{
	\begin{tabular}{l|lllll}
		\toprule
		 & {ZINC} & {MNIST} & {CIFAR10} & {PATTERN} & {CLUSTER}\\
         \# graphs&  12,000 & 70,000 & 60,000 & 14,000 & 12,000\\
        Avg. \# nodes&  23.2&  70.6&  117.6&  118.9&  117.2\\
        Avg. \# edges&  24.9&  564.5&  941.1&  3039.3&  2150.9\\
		Metric & MAE $\downarrow$ & Accuracy $\uparrow$ & Accuracy $\uparrow$ & Accuracy $\uparrow$ & Accuracy $\uparrow$\\
        \midrule %
        GT (\citeyear{dwivedi2020generalization}) & 0.226{\tiny{ $\pm$ 0.014}} & 90.831{\tiny{ $\pm$ 0.161}} & 59.753{\tiny{ $\pm$ 0.293}} & 84.808{\tiny{ $\pm$ 0.068}} & 73.169{\tiny{ $\pm$ 0.622}} \\
        SAN (\citeyear{kreuzer2021rethinking}) & 0.139{\tiny{ $\pm$ 0.006}} & – & – & 86.581{\tiny{ $\pm$ 0.037}} & 76.691{\tiny{ $\pm$ 0.650}} \\
        Graphormer (\citeyear{ying2021transformers}) & 0.122{\tiny{ $\pm$ 0.006}} & – & – & – & – \\
        SAT (\citeyear{chen2022structure}) & 0.094{\tiny{ $\pm$ 0.008}} & – & – & 86.848{\tiny{ $\pm$ 0.037}} & 77.856{\tiny{ $\pm$ 0.104}} \\
        EGT (\citeyear{hussain2022global}) & 0.108{\tiny{ $\pm$ 0.009}} & 98.173{\tiny{ $\pm$ 0.087}} & 68.702{\tiny{ $\pm$ 0.409}} & 86.821{\tiny{ $\pm$ 0.020}} & \textbf{\textcolor{tealblue!90}{79.232{\tiny{ $\pm$ 0.348}}}} \\
        GraphGPS (\citeyear{rampavsek2022recipe}) & 0.070{\tiny{ $\pm$ 0.004}} & 98.051{\tiny{ $\pm$ 0.126}} & 72.298{\tiny{ $\pm$ 0.356}} & 86.685{\tiny{ $\pm$ 0.059}} & 78.016{\tiny{ $\pm$ 0.180}} \\
        GRPE (\citeyear{park2022grpe}) & 0.094{\tiny{ $\pm$ 0.002}} & – & – & 87.020{\tiny{ $\pm$ 0.042}} & – \\
        Graphormer-URPE (\citeyear{luo2022your}) & 0.086{\tiny{ $\pm$ 0.007}} & – & – & – & – \\
        Graphormer-GD (\citeyear{zhang2023rethinking}) & 0.081{\tiny{ $\pm$ 0.009}} & – & – & – & – \\
        Specformer (\citeyear{bo2023specformer}) & 0.066{\tiny{ $\pm$ 0.003}} & – & – & – & – \\
        LGI-GT (\citeyear{yinlgi}) & – & – & – & 86.930{\tiny{ $\pm$ 0.040}} & – \\
        GPTrans-Nano (\citeyear{chen2023graph}) & – & – & – & 86.731{\tiny{ $\pm$ 0.085}} & – \\
        Graph ViT/MLP-Mixer (\citeyear{he2023generalization}) & 0.073{\tiny{ $\pm$ 0.001}} & \textbf{\textcolor{darkorange!90}{98.460{\tiny{ $\pm$ 0.090}}}} & 73.960{\tiny{ $\pm$ 0.330}} & – & – \\
        Exphormer (\citeyear{shirzad2023exphormer}) & – & 98.414{\tiny{ $\pm$ 0.038}} & 74.754{\tiny{ $\pm$ 0.194}} & 86.734{\tiny{ $\pm$ 0.008}} & – \\
        GRIT (\citeyear{ma2023graph}) & \textbf{\textcolor{tealblue!90}{0.059{\tiny{ $\pm$ 0.002}}}} & 98.108{\tiny{ $\pm$ 0.111}} & 76.468{\tiny{ $\pm$ 0.881}} & \textbf{\textcolor{customcyan}{87.196{\tiny{ $\pm$ 0.076}}}} & \textbf{\textcolor{customcyan}{80.026{\tiny{ $\pm$ 0.277}}}} \\
        GRED (\citeyear{ding2024recurrent}) & 0.077{\tiny{ $\pm$ 0.002}} & 98.383{\tiny{ $\pm$ 0.012}} & \textbf{\textcolor{tealblue!90}{76.853{\tiny{ $\pm$ 0.185}}}} & 86.759{\tiny{ $\pm$ 0.020}} & 78.495{\tiny{ $\pm$ 0.103}} \\
        GEAET (\citeyear{liang2024graph}) & – & \textbf{\textcolor{tealblue!90}{98.513{\tiny{ $\pm$ 0.086}}}} & \textbf{\textcolor{darkorange!90}{76.634{\tiny{ $\pm$ 0.427}}}} & 86.993{\tiny{ $\pm$ 0.026}} & – \\
        TIGT (\citeyear{choi2024topology}) & \textbf{\textcolor{customcyan}{0.057{\tiny{ $\pm$ 0.002}}}} & 98.231{\tiny{ $\pm$ 0.132}} & 73.963{\tiny{ $\pm$ 0.361}} & 86.681{\tiny{ $\pm$ 0.062}} & 78.025{\tiny{ $\pm$ 0.223}} \\
        Cluster-GT (\citeyear{huang2024clusterwise}) & 0.071{\tiny{ $\pm$ 0.004}} & – & – & – & – \\
        GMN (\citeyear{behrouz2024graph}) & – & 98.391{\tiny{ $\pm$ 0.182}} & 74.560{\tiny{ $\pm$ 0.381}} & \textbf{\textcolor{tealblue!90}{87.090{\tiny{ $\pm$ 1.260}}}} & – \\
        Graph-Mamba (\citeyear{wang2024graph}) & – & 98.420{\tiny{ $\pm$ 0.080}} & 73.700{\tiny{ $\pm$ 0.340}} & 86.710{\tiny{ $\pm$ 0.050}} & 76.800{\tiny{ $\pm$ 0.360}} \\
         \midrule %
        GCN & 0.367{\tiny{ $\pm$ 0.011}} & 90.705{\tiny{ $\pm$ 0.218}} & 55.710{\tiny{ $\pm$ 0.381}} & 71.892{\tiny{ $\pm$ 0.334}} & 68.498{\tiny{ $\pm$ 0.976}} \\
        \rowcolor{gray!20}
        \textbf{GCN$^+$} & 0.076{\tiny{ $\pm$ 0.009}} \textbf{79.3\%$\downarrow$} & 98.382{\tiny{ $\pm$ 0.095}} \textbf{8.5\%$\uparrow$} & 69.824{\tiny{ $\pm$ 0.413}} \textbf{25.4\%$\uparrow$} & 87.021{\tiny{ $\pm$ 0.095}} \textbf{21.1\%$\uparrow$} & 77.109{\tiny{ $\pm$ 0.872}} \textbf{12.6\%$\uparrow$} \\ 
        \midrule %
        GIN & 0.526{\tiny{ $\pm$ 0.051}} & 96.485{\tiny{ $\pm$ 0.252}} & 55.255{\tiny{ $\pm$ 1.527}} & 85.387{\tiny{ $\pm$ 0.136}} & 64.716{\tiny{ $\pm$ 1.553}} \\ 
        \rowcolor{gray!20}
         \textbf{GIN$^+$} & \textbf{\textcolor{darkorange!90}{0.065{\tiny{ $\pm$ 0.004}}}} \textbf{87.6\%$\downarrow$} & 98.285{\tiny{ $\pm$ 0.103}} \textbf{1.9\%$\uparrow$} & 69.592{\tiny{ $\pm$ 0.287}} \textbf{25.9\%$\uparrow$} & 86.842{\tiny{ $\pm$ 0.048}} \textbf{1.7\%$\uparrow$} & 74.794{\tiny{ $\pm$ 0.213}} \textbf{15.6\%$\uparrow$} \\  
        \midrule %
        GatedGCN & 0.282{\tiny{ $\pm$ 0.015}} & 97.340{\tiny{ $\pm$ 0.143}} & 67.312{\tiny{ $\pm$ 0.311}} & 85.568{\tiny{ $\pm$ 0.088}} & 73.840{\tiny{ $\pm$ 0.326}} \\
        \rowcolor{gray!20}
        \textbf{GatedGCN$^+$} & 0.077{\tiny{ $\pm$ 0.005}} \textbf{72.7\%$\downarrow$} & \textbf{\textcolor{customcyan}{98.712{\tiny{ $\pm$ 0.137}}}} \textbf{1.4\%$\uparrow$} & \textbf{\textcolor{customcyan}{77.218{\tiny{ $\pm$ 0.381}}}} \textbf{14.7\%$\uparrow$} & \textbf{\textcolor{darkorange!90}{87.029{\tiny{ $\pm$ 0.037}}}} \textbf{1.7\%$\uparrow$} & \textbf{\textcolor{darkorange!90}{79.128{\tiny{ $\pm$ 0.235}}}} \textbf{7.1\%$\uparrow$} \\  
        \midrule %
        Time (epoch) of GraphGPS &  21s & 76s & 64s & 32s & 86s \\
        \rowcolor{gray!20}
        Time (epoch) of  \textbf{GCN$^+$} & \textbf{7s} & \textbf{60s} & \textbf{40s} & \textbf{19s} & \textbf{29s} \\
        \bottomrule
	\end{tabular}}
    \vspace{-0.05 in}
	\label{tab:tab2}
\end{table*}

% \subsection{Network Depth} 
% Deeper network architectures, such as deep CNNs~\cite{he2016deep,huang2017densely}, are capable of extracting more complex, high-level features from data, potentially leading to better performance on various prediction tasks. However, GNNs face unique challenges with depth, such as over-smoothing~\cite{li2018deeper}, where node representations become indistinguishable with increased network depth. Consequently, in practice, most GNNs adopt a shallow architecture, typically consisting of 2 to 5 layers. 
% However, with the inclusion of residual connections, our findings demonstrate that significantly deeper GNN architectures, ranging from 3 to 20 layers, can achieve promising performance.

% While previous research, such as DeepGCN \cite{li2019deepgcns} and DeeperGCN \cite{li2020deepergcn}, advocates the use of deep GNNs with up to 56 and 112 layers, our findings indicate that comparable performance can be achieved with significantly shallower GNN architectures, typically ranging from 3 to 16 layers.