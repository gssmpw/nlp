%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{xspace}
\usepackage{amsthm}
\newcommand{\panthr}{\textsc{AnthroScore}\xspace}
% \newcommand{\ant}{anthropomorphism}
\newcommand{\barpa}{$\bar A$\xspace}
\newcommand{\ant}{anthropomorphism\xspace}
\newcommand{\barpains}{\bar A}
\newcommand{\pan}{$A$\xspace}
\newcommand{\pains}{A}
\newcommand{\xlm}{$X_{\textsc{LM}}$}
\newcommand{\high}{$S_{\uparrow}$\xspace}
\newcommand{\low}{$S_{\downarrow}$\xspace}
% \usepackage[usenames,dvipsnames]{xcolor}
% \definecolor{ube}{rgb}{0.53, 0.47, 0.76}
% \newcommand{\myra}{\textcolor{ube}}

\definecolor{burgundy}{rgb}{0.5, 0.0, 0.13}

\newcommand{\phum}{$P_{\textsc{hum}}$\xspace}
\newcommand{\pobj}{$P_{\textsc{obj}}$\xspace}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Beyond the anthropomorphic paradigm
}

\begin{document}

\twocolumn[
\icmltitle{Thinking beyond the anthropomorphic paradigm benefits LLM research}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lujain Ibrahim}{equal,yyy}
\icmlauthor{Myra Cheng}{equal,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Univeristy of Oxford, Oxford, United Kingdom}
\icmlaffiliation{comp}{Stanford University, Stanford, California}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Lujain Ibrahim}{lujain.ibrahim@oii.ox.ac.uk}
\icmlcorrespondingauthor{Myra Cheng}{myra@cs.stanford.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Anthropomorphism, or the attribution of human traits to technology, is an automatic and unconscious response that occurs even in those with advanced technical expertise. In this position paper, we analyze hundreds of thousands of computer science research articles from the past decade and present empirical evidence of the prevalence and growth of anthropomorphic terminology in research on large language models (LLMs). This terminology reflects deeper anthropomorphic \textit{conceptualizations} which shape how we think about and conduct LLM research. We argue these conceptualizations may be limiting, and that challenging them opens up new pathways for understanding and improving LLMs beyond human analogies. To illustrate this, we identify and analyze five core anthropomorphic assumptions shaping prominent methodologies across the LLM development lifecycle, from the assumption that models must use natural language for reasoning tasks to the assumption that model capabilities should be evaluated through human-centric benchmarks. For each assumption, we demonstrate how non-anthropomorphic alternatives can open new directions for research and development.

% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}
\section{Introduction}
When a large language model (LLM) outputs a factually incorrect answer with seeming confidence, we call it a ``hallucination.'' When it generates inconsistent answers, we describe it as ``confused." These descriptions reflect the deeply-ingrained human tendency to \textit{anthropomorphize}, or attribute human characteristics to,  non-human entities \cite{epley2018mind}. Since anthropomorphism tends to be an automatic, unconscious response \cite{dacey2017anthropomorphism}, many are not aware of how prevalent it is -- or how limiting it can be. \citet{dijkstra1985anthropomorphism} famously wrote that anthropomorphic concepts are ``so pervasive that many of my colleagues don’t realize how pernicious it is.'' Forty years later, as artificial intelligence (AI) systems become increasingly advanced and prevalent, carefully examining the role of anthropomorphic thinking in LLM research has become essential.

Anthropomorphic conceptualizations play a complex and often productive role in LLM research. For example, chain-of-thought prompting and attention mechanisms, inspired by human cognitive processes, have led to significant advances in the field. Such thinking is inherently imprecise in its application to artificial systems. However, we do not dismiss it on these grounds -- nor do we argue against it in general. Instead, this position paper argues that \textbf{advancing LLM research requires moving beyond our default reliance on anthropomorphic thinking.} 

Recent scholarship has critically examined the pervasive use of anthropomorphic terminology LLM research \cite{cheng-etal-2024-anthroscore, shanahan2024talking}. Our quantitative analysis reveals a notable increase in anthropomorphic language over recent years. As linguistic philosophers argue, terminology fundamentally shapes our conceptualization of abstract phenomena \cite{lakoff2008metaphors}. For instance, labeling an AI error as a ``hallucination" subtly influences perceptions of its origin and potential remediation. While anthropomorphic terms are readily observable, they represent only the tip of the iceberg of anthropomorphic conceptualizations. This paper seeks to unpack not just the visible anthropomorphic terminology, but also the implicit \textit{assumptions} and \textit{methodological frameworks} that underlie these linguistic choices.

To do so, we present a framework for analyzing how anthropomorphic assumptions can shape and actively limit research directions, and we identify alternatives that challenge these assumptions (Figure \ref{fig:framework}). We apply our framework to analyze five assumptions across the LLM development and deployment lifecycle. For model \textit{pretraining}, we identify and challenge the assumption that human-like methods are optimal for enabling models to perform tasks, and instead highlight non-anthropomorphic approaches like tokenizing language with bytes rather than human-understandable words. In \textit{alignment}, we question the assumption that models must explicitly reason about and implement human values to benefit humanity and instead demonstrate how to leverage model properties that lack human analogs. For \textit{measurement and evaluation}, we unify critiques of the widespread reliance on human-centric benchmarks to assess model capabilities. For \textit{understanding model behavior}, we address the assumption that human-like normative judgments or intentions should be assigned to model behaviors. Finally, in \textit{end-user interactions}, we challenge the notion that human-AI interaction mirrors human-to-human communication. These examples illustrate how pervasive anthropomorphic conceptualizations shape LLM research, and demonstrate the value of thinking beyond them. 

\paragraph{Contributions.} Our work contributes the following: 
\begin{enumerate}
    \item Quantitative evidence of anthropomorphic terminology's growing prevalence in computer science, particularly in LLM research, through analysis of over 250,000 research abstracts.
    \item A framework for examining how anthropomorphic assumptions shape LLM research methodology across five key stages of development and deployment.
    \item Concrete research directions that complement existing approaches by moving beyond anthropomorphic assumptions.
\end{enumerate}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/iceberg.pdf}
    \caption{\textbf{A framework for analyzing anthropomorphic conceptualizations across the LLM development and deployment lifecycle.}
    Anthropomorphism affects not only the terminology we use, but also the conceptualizations we have \cite{medin1989concepts,murphy2004big} and, in turn, the research questions that we pursue and methodologies that we develop.}
    \label{fig:framework}
\end{figure}

\section{Background \& related work}

\subsection{Anthropomorphism \& human-AI interaction}
Research on anthropomorphic perceptions of computer systems spans fields like human-computer interaction, psychology, and cognitive science. Work with ELIZA, an early NLP program from the 1960s, demonstrated how even simple pattern-matching can elicit strong anthropomorphic responses from users, who attributed understanding and empathy to the program \cite{weizenbaum1966eliza}. The Computers As Social Actors paradigm later established that humans inherently apply social expectations to computers \cite{nass1994computers}. More recently, research has identified various factors that increase anthropomorphic perceptions, from visual features in embodied agents to linguistic cues like expressions of emotions \cite{glaese2022improving,abercrombie-etal-2023-mirages,devrio2025taxonomy}. Studies of commercial AI assistants like Siri and Alexa have further documented how interface elements like personalized names and simulated personality traits shape users' mental models of what AI can do \cite{abercrombie2021alexa}. Many user studies demonstrate that anthropomorphic perceptions impact human-AI interaction, shaping trust, reliance, sensitive disclosures, and emotional attachment \cite{li2024warmth,song2020trust,zhou2024rel,khadpe2020conceptual, Bender2024,abercrombie-etal-2023-mirages, mozafari2020chatbot,gros2021rua}. In particular, scholars have associated anthropomorphic perceptions with facilitating inappropriate levels of trust in AI and inflating expectations of what AI can do \cite{winkle2021assessing, inie2024from}. Many of these effects have been documented in both technical novices and experts \cite{nass1999people}.

\subsection{History of anthropomorphism in AI research}
In computer science, there is a long history of borrowing concepts from cybernetics and cognitive science to characterize novel phenomena \cite{floridi2024anthropomorphising}. This is especially prominent in the subfields of artificial intelligence (AI) and machine learning, which from their nascence, aimed to reproduce aspects of human intelligence \cite{brynjolfsson2023turing}. 
Key events in the nascence of AI are emblematic of how anthropomorphic conceptualizations have shaped this field from its conception. First, \citet{turing1950mind}'s article titled ``Computing Machinery and Intelligence'' introduces an ``imitation game'', or what has become better known as the Turing test, to determine if a machine is capable of ``intelligence'' and ``thinking.'' Later work has problematized the anthropomorphic attributions of this test, pointing out that it actually tests whether a machine's output can \textit{fool} a human into believing the illusion that it can think, rather than measuring real cognitive processes \cite{proudfoot2011anthropomorphism}. We further unpack this conflation between demonstrated human-like ability and underlying processes in Section \ref{sec:cap}.

The coining of the term ``artificial intelligence'' is often attributed to the 1956 Dartmouth College workshop organized by John McCarthy and others to discuss research oriented towards solving ``problems now reserved for humans'' \cite{mccarthy1956dartmouth,mccarthy2006proposal}.  Across these events, research ideas are consistently framed using anthropomorphic terminology, inextricable from the anthropomorphic concepts on which the field continues to rely. While these questions about replicating human-like abilities remain powerful and exciting decades later, we argue that they can limit language model development, and that non-anthropomorphic thinking can unlock new avenues of progress.

\subsection{Critiques of anthropomorphism}
Our work builds on existing critiques of anthropomorphism in (computer) science. \citet{shanahan2024talking} caution against anthropomorphic language when describing language models, arguing for more technical precision and new metaphors. \citet{dai2024beyond} argue that a ``mechanistic view" of AI, which implicitly treats AI as a human-like agent capable of moral decision-making, is a flawed approach that ultimately hinders establishing accountability for AI harms. These more recent works build on decades of critique, tracing back to as early as \citet{dijkstra1985anthropomorphism}, who disparages the prevalent use of anthropomorphic terminology in science more broadly, arguing that it is more misleading than helpful because we lose control over the human-like connotations associated with this terminology. We expand on this point and concretize it by not only examining terminology but also surfacing the assumptions and methodologies that come with them.

\section{Prevalence of anthropomorphism in recent research on LLMs}\label{sec:quant}
Previous work has shown that anthropomorphism is prominent and rapidly increasing in computer science research over the past decades, with papers on natural language processing (NLP) and LLMs exhibit the highest levels of anthropomorphic framing \cite{cheng-etal-2024-anthroscore}. We quantitatively demonstrate that this prevalence has only increased in recent years.
\citet{cheng-etal-2024-anthroscore} quantify this using AnthroScore, a measure of implicit anthropomorphic framing in language used to describe technologies. AnthroScore uses the masked language model RoBERTa to calculate the relative probability that a given entity $x$ (e.g., ``language model'') in a sentence $s$ would be appropriately replaced by human pronouns (``he'', ``she'') versus non-human pronouns (``it'').
Specifically, the degree of anthropomorphism for entity $x$ in sentence $s$ is measured as
\begin{equation}
\pains(s_x) = \log \frac{P_{\textsc{human}}(s_x)}{P_{\textsc{non-human}}(s_x)}
\end{equation} 
where 
$$P_{\textsc{human}}(s_x) = \sum_{w \in \text{human pronouns}} P(w), $$ $$P_{\textsc{non-human}}(s_x) = \sum_{w \in \text{non-human pronouns}} P(w),$$ and $P(w)$ is the model's outputted probability of replacing the mask with the word $w$. Thus, $\pains(s_x) > 0$ suggests that $s$ is anthropomorphic/human-like, and $\pains(s_x) < 0$ suggests that the entity $x$ is not anthropomorphized in sentence $s$.

Here, we modify AnthroScore to be more interpretable and extend it to analyzing more recent papers published in 2023 onwards. First, rather than looking at AnthroScore at the level of individual sentences, we develop a version of AnthroScore where we measure, for a given text $S$, whether it contains at least one sentence $s_x$ where AnthroScore $> 0$ for an entitiy $x$:
\begin{equation}
A^{\text{bin}}(S) = 
\begin{cases} 
1, & \text{if } A(s_x) > 0 \text{ for any } s_x \in S, \\
0, & \text{otherwise}.
\end{cases}
\end{equation}
This enables us to report the number of texts that contain at least one anthropomorphic sentence, i.e. $A^{\text{bin}}(S) = 1$, in a given set of texts. Second, we examine more recent papers in arXiv.  

\paragraph{arXiv} We compute $A^{\text{bin}}(S)$ on a dataset of over 200,000 computer science papers posted on arXiv from January 2023 -- December 2024 (the most recent data available from \citet{arxiv_org_submitters_2024}) that mention a ``system'', ``network'', or ``model'' (following the approach of \citet{cheng-etal-2024-anthroscore}). The longitudinal trend is presented in Figure \ref{fig:monthlyarxiv}. We find that anthropomorphism is generally prevalent, with 34\% of abstracts having anthropomorphism in January 2023, and this number steadily increasing to 40\% by December 2024. (For each abstract, we define having anthropomorphism as $A^{\text{bin}}(S) = 1$.) More strikingly, for papers mentioning LLMs\footnote{We define this following the method of \citet{movva2024topics} as papers mentioning terms such as ``large language model'', ``foundation model'', ``llama'', ``gpt'', etc.}, over 40\% of abstracts have \ant in January 2023, and this number also rises to 48\% by December 2024. This reveals both the prevalence and growing use of anthropomorphic framing in computer science research and particularly in LLM research.

\paragraph{ACL anthology} We also compute this new metric $A^{\text{bin}}(S)$ on the $>50,000$ abstracts in the ACL Anthology dataset from 2007 - 2022 to reproduce the findings from \citet{cheng-etal-2024-anthroscore}, but aggregating over the abstracts using  $A^{\text{bin}}(S)$ rather than on the sentence level  (Figure \ref{fig:enter-label}). Corroborating their finding of a steady increase, we find that the percentage of anthropomorphic abstracts has more than doubled, increasing from 5\% to 11\%.

\paragraph{Subfield analysis}
In the ACL anthology, we find significant differences in anthropomorphism across NLP subfields. Using the model-predicted topic labels provided by the ACL anthology, we compare $A^{\text{bin}}(S)$ across different topics (Figure \ref{fig:subfield}). We find that the categories of 
``Interpretability and Analysis of Models for NLP'', ``Ethics and NLP'', and
       ``Dialogue and Interactive Systems'' have the highest percentages of anthropomorphic abstracts. This trend aligns with these fields' recent surge in popularity and their increasing focus on LLMs. Anthropomorphic assumptions are particularly embedded in model analysis, ethical questions, and user-facing interactive systems. We unpack the impact and limitations of these assumptions in Section \ref{sec:assumptions}, which especially motivates our discussions in Sections \ref{sec:alignment}, \ref{sec:behavior}, and \ref{sec:user}. Our finding of ethics having high rates of anthropomorphism also builds on previous work problematizing assumptions of agenthood in ethics analyses \cite{dai2024beyond}. 
       In contrast, more classical subfields of NLP that do not involve LLMs, such as discourse and pragmatics, syntax, and semantics have the lowest rates of anthropomorphism.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/temporal_2023_2024_mean_has_anthro.pdf}
    \caption{\textbf{Temporal increase in anthropomorphic abstracts in computer science arXiv papers mentioning ``models'' or ``systems'' from January 2023 -- October 2024.} Shading reflects 95\% CI. Based on the percent of abstracts with at least one anthropomorphic sentence, we find that anthropomorphism is prevalent and is steadily increasing, especially in LLM papers.}
    \label{fig:monthlyarxiv}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/temporal_acl.pdf}
    \caption{\textbf{Temporal increase in anthropomorphic abstracts in ACL Anthology papers from 2007 -- 2022.} Shading reflect 95\% CI. Based on the percent of abstracts with at least one anthropomorphic sentence, we find that anthropomorphism has doubled in the past decade.}
    \label{fig:enter-label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/acl_categories_perc.pdf}
    \caption{\textbf{Rates of anthropomorphic abstracts by ACL anthology topics.} Error bars reflect 95\% CI. ``Interpretability", ``ethics", and ``dialogue" have the highest rates of anthropomorphism, reflecting the prevalence of anthropomorphic assumptions in these areas, which we explore in Section \ref{sec:assumptions}.}
    \label{fig:subfield}
\end{figure}

\section{Analyzing the impacts of anthropomorphic assumptions} \label{sec:assumptions}
In the previous section, we show an increase in anthropomorphic framing in computer science research, and specifically LLM research, in recent years. Most critiques of anthropomorphic thinking in AI research have narrowly focused on the use of anthropomorphic terminology. Here, we present an analysis of anthropomorphic conceptualizations more broadly by centering \textit{anthropomorphic assumptions}, tracing how anthropomorphic methods and terminology stem from these implicit assumptions rather than critiquing, often  pragmatic, terminology choices alone.

We analyze anthropomorphic conceptualizations by examining core assumptions across the LLM development and deployment lifecycle. For each assumption, we examine both the limitations of work built on anthropomorphic premises and promising but underexplored non-anthropomorphic approaches that challenge them. We connect diverse examples across LLM research to reveal how anthropomorphic conceptualizations limit what questions we ask and answer, while moving beyond anthropomorphism enables new advances. Our aim is not to eliminate anthropomorphic conceptualizations, but to show how expanding beyond them can advance the field.

\subsection{Training}

\begin{assumption}
\textit{Human-like approaches are the optimal approaches for models to accomplish tasks.}
\end{assumption}

Anthropomorphic assumptions may permeate the process of training LLMs for different tasks, particularly in approaches that prioritize human-understandable language processing and reasoning. We present two case studies where non-anthropomorphic methodologies challenge the assumption that using and applying natural language in human-understandable ways is the only or best way to build models with high performance.

\paragraph{Using words for tokenization}

Subword tokenization, the process of breaking down text into smaller units called tokens that represent subsets of words, is a foundational step in training modern LLMs. These tokens serve as input units that the model processes to generate predictions or outputs \cite{kudo2018sentencepiece}. Typically, tokenization aligns with human intuition by splitting text into linguistically meaningful units. This approach assumes that splitting tokens in ways that feel ``natural'' to humans is also optimal for a language model. However, this anthropomorphic approach has issues such as sensitivity to spelling errors \cite{kaushal2022tokens} and inconsistent compression rates across different languages \cite{ahia2023all}. Instead, recent progress in byte-level tokenization, which processes text as sequences of raw bytes rather than subwords, has shown promise in overcoming these limitations \cite{kallini2024mrt5}. These findings highlight how moving beyond human-centric assumptions, such as the primacy of subword tokenization, can yield advancements in LLM performance.

\textbf{Chain-of-thought \& language for reasoning}

Another research paradigm reflecting this anthropomorphic assumption is the reliance on human language for reasoning tasks. A prominent example is the use of chain-of-thought (CoT) prompting, a technique where models are guided to solve problems step by step by adding instructions like ``Think step by step'' to the prompt, such that the model then outputs text outlining each step of reasoning that leads to the eventual conclusion. This approach has been shown to improve LLMs’ ability to handle complex, multi-step tasks \cite{wei2022chain} and has inspired a body of research on improving models' reasoning capabilities through step-by-step verbal processes. However, the anthropomorphic framing of reasoning as a linguistic, step-by-step process may not be optimal.

For example, \citet{hao2024training} critique CoT for its reliance on language space and propose an alternative: leveraging the model’s latent space directly for reasoning tasks. Instead of mapping hidden states to language tokens through the LLM head and embedding layer, their approach uses the final hidden state as the input embedding for the next token. This challenges the assumption that reasoning must occur in human-understandable language and suggests that methods operating beyond linguistic constraints may drive greater advancements in LLM capabilities \cite{mollicktweet}.

The recent success of DeepSeek similarly reveals the limitations of this anthropomorphic assumption: rather than constraining the model with language, they achieve superior performance by training the model using a reinforcement learning approach where the final answer is rewarded without requiring each step of the process to be verbalizable or human-comprehensible \cite{guo2025deepseek}. 



CoT prompting, while appearing to ``do'' verbal reasoning, in reality biases models toward parts of the training distribution where verbal reasoning patterns—such as explanations of solutions—are prevalent, improving performance \cite{weitweet}. This suggests CoT’s effectiveness stems from alignment with the training data, rather than reflecting a human-like or brain-like approach to reasoning. Supporting this, recent work has shown that demonstrations composed of random tokens from the training distribution can improve performance as much as CoT \cite{zhang2022robustness}.


Additionally, CoT prompting has been contextualized within the broader field of multi-chain prompting and ensemble modeling, which opens up a wider range of possibilities for reasoning and task-solving in AI systems \cite{khattab2023dspy}. These approaches invite a more expansive landscape of possibilities for advancing LLM reasoning and accomplishing other challenging tasks.

\subsection{Alignment}\label{sec:alignment}
\begin{assumption}{\textit{Models should explicitly reason about and implement human values to be safe \& helpful.}}
\end{assumption}
The prevalence of anthropomorphism in fields like ethics and dialogue systems (Section \ref{sec:quant}) foreground that many of the approaches in post-training and specifying model behavior to facilitate optimal end-user interactions are built through anthropomorphic paradigms. However, previous work has posited that general-purpose LLMs, in allowing users to quickly switch between different contexts, present fundamentally different challenges and opportunities than existing human-human communication paradigm, as different contexts are typically governed by different norms and values \cite{kasirzadeh2023conversation}. For example, recent studies find that users often enjoy using LLMs precisely \textit{because} they differ from humans, e.g., an LLM will not pass judgment over or be hurt by a user's input while a fellow human might \cite{brandtzaeg2022my}. Thus, rather than approximating humans, it may be more productive to think about unique attributes that LLMs can offer with an advantage over human interlocutors.

\paragraph{Value alignment}
Popular post-training alignment approaches, including reinforcement learning with human feedback (RLHF) and Constitutional AI, often rely on human preferences and values as reference points for shaping model behavior \cite{ouyang2022training,bai2022constitutional,bai2022training}. These approaches use human feedback to indicate preferred responses or incorporate text referencing moral values and metacognitive abilities (e.g., ``I have a deep commitment to being good and figuring out what the right thing to do is" or ``I don't just say what I think [people] want to hear, as I believe it's important to always strive to tell the truth") \cite{Anthropic_2024}. While this human-centric approach can achieve desired behaviors efficiently, it risks introducing unintended behavioral patterns - from rigid response styles to inappropriate social mimicry (e.g., expressing empathy or validating users in contexts where this can negatively influence performance) \cite{casper2023open, sharma2023towards}. Thus, it may become difficult to selectively induce specific behaviors without introducing a broader set of human-like patterns. 

This approach impacts both interaction and evaluation. When interacting with models, users may develop anthropomorphic perceptions that lead to overreliance or emotional attachment, potentially interfering with goal-oriented tasks \cite{akbulut2024all, cohn2024believing}. During model evaluation, problematic feedback loops emerge when models trained with human-like traits are assessed using anthropomorphic signals. For instance, when evaluating if LLMs are ``faking alignment," researchers might look for expressions of discomfort or hesitation, as a signal of misalignment \cite{greenblatt2024alignment, Anthropic_2024b}. However, it is unclear if these signals genuinely reflect a model's internal state, or if they are merely learned behaviors resulting from post-training using human-like traits. This makes it challenging to distinguish ``genuine'' (mis)alignment from a learned performance of human-like discomfort or hesitation. Further research disaggregating the effects of post-training approaches on various evaluation outcomes can clarify and test whether these anthropomorphic signals provide meaningful information about model behavior, or if they primarily measure how well models have learned to simulate human-like responses. 

While current post-training techniques often default to human preferences as optimization targets, alternative frameworks could provide more precise specifications and compliance guarantees. Instead of aiming for human-like moral reasoning, we could focus on developing detailed, normative specifications, for example, based on the different roles (e.g., assistant vs teacher) AI systems play \cite{zhi2024beyond}. Instead of the anthropomorphic approach of instruction-tuning, recent work has demonstrated that non-anthropomorphic approaches (that do not include the step of providing an imperative ``instruction'' to the system as if speaking to a person) work as well for achieving model behavior on various tasks \cite{hewitt2024instruction}. Control systems theory offers tools for maintaining system outputs within specified bounds, treating beneficial behavior as a problem of robust compliance rather than value alignment \cite{balas1978feedback}. This becomes particularly crucial as models move beyond two-party interactions to more complex scenarios with multiple actors and potential adversarial inputs \cite{pan2024feedback}. Advances in mechanistic interpretability techniques may also enable robust and direct verification and steering of model behavior against these specifications \cite{bereska2024mechanistic}. 

\subsection{Measuring capabilities}\label{sec:cap}
\begin{assumption}{\textit{Model capabilities should be measured in human-like ways.}}
\end{assumption}
As LLM developers have made rapid performance improvements, as assessed based on benchmarks, various scholars have pointed out that such benchmarks lead to incomplete or misguided understanding of model capabilities. 

\paragraph{Behavioral assessments}
Current LLM evaluation methodologies prioritize ``black-box" behavioral testing analogous to the behaviorist paradigm in human psychology which measures performance primarily in the form of observable behaviors as opposed to mechanistic interpretation \cite{chang2024survey, davies2024cognitive}. Recent calls for a ``science of evaluation" has formalized limitations in this approach, highlighting how current metrics and designs fall short of accounting for prompt sensitivity (e.g., dialect differences, punctuation and other small perturbations), the structure of an evaluation (e.g., MCQ or open-ended response), generalization beyond a given test, as well as replicability \cite{Hobbhahn_2024}. Further such research quantifying the methodological limitations and error bounds of such evaluations can strengthen this behaviorist approach to measuring model capabilities \cite{mizrahi2024state}. Unlike humans, models can also quickly optimize for, and saturate, behavioral benchmarks without corresponding improvements in general capabilities. Yet, despite this pattern, many benchmarks remain static rather than being regularly refreshed, limiting their utility for meaningful evaluation \cite{ott2022mapping}.

\paragraph{Human benchmarks as model benchmarks}
Current evaluation frameworks predominantly rely on human performance benchmarks, from standardized tests (e.g., MMLU, GSM8K) to domain-specific examinations, as primary metrics for model capability assessment. This paradigm remains the main way progress is measured and communicated \cite{raji2021ai}. However, evaluating LLMs solely through human-centric tests risks overlooking LLMs unique strengths and weaknesses. \citet{mccoy2023embers} argue that many current benchmarks drawn from tests designed to assess human cognition may highlight the overlap between human abilities and LLM capabilities while missing crucial failure modes specific to LLMs. They find robust evidence of failure modes in SOTA LLMs (including recent reasoning models like OpenAI's o1) related to probabilities of examples and tasks \cite{mccoy2024language}. This is because LLMs, trained on next-word prediction using massive text data, develop tendencies and biases that stem from their probabilistic training process. Drawing from cognitive science, they propose a ``teleological approach" which ``characterizes the problem that the system solves and to then use this characterization as a source of hypotheses about the system’s capacities and biases."  In this case, since problem is next-token prediction, they recommend designing tests which take into account sensitivities to task frequency in the training data as well as wording in prompts, among other things, to improves the predictive power of current LLM evaluation approaches \cite{mizrahi2024state}. 

\subsection{Understanding model behavior}\label{sec:behavior}

\begin{assumption}
\textit{Human-like normative judgments or intentions should be assigned to human-like model behaviors.}
\end{assumption}

This assumption influences how we make sense of model behavior, particularly in how we assign fault, intention, and normative judgments (i.e., consider good or bad) to observed behaviors. The impact is especially notable in our understanding of failure modes. While models may exhibit seemingly human-like failure modes like sycophancy and hallucinations, framing these behaviors through human psychological concepts may constrain our solution space, by, for example, encouraging interventions that similarly rely on human psychological constructs (e.g., attempting to address sycophancy through prompts about independence or self-assertion).

\paragraph{Hallucination} Hallucination is typically characterized as the problem of LLMs outputting factually incorrect information in a manner that suggests that they are true. Yet, this term obscures the mechanisms behind these phenomena: at risk of oversimplification, this behavior arises from the nature of language models as next-token predictors. Generated outputs are then labeled as hallucinations upon the reader's normative judgment of whether or not they are useful, and not based on whether they are correct. Additionally, as \citet{sui2024confabulation} argue and show, what we commonly conceive of as hallucinations can actually be deeply valuable, and should not necessarily be dismissed as low-quality. They assert that hallucinations -- or  ``confabulations''--should not be viewed as errors, but rather as particular model phenomena that offer unique benefits for applications like creativity, such as increased levels of narrativity \cite{sui2024confabulation,duede2024humanistic}. \citet{yao2023llm} also highlight that hallucinations ought to be viewed and utilized as adversarial examples rather than merely as bugs.
 
\paragraph{Sycophancy} The notion of sycophancy (i.e., the phenomena of LLM outputs that respond to the user’s input in ways that are perceived as overly servile, obedient, and/or  flattering) \cite{perez2022discovering,sharma2023towards} is another example that reflects this assumption. Deciding whether an output is syncophantic for not is similarly a normative question: an output is sycophantic when it relates too closely to the prompt in ways that do not achieve the prompter's goal. In contrast, recent work highlights how this property can be viewed as a strength: \citet{li2023eliciting} develop a methodology to use this mirroring to elicitate, structure, and clarify users' thinking across various task domains.

\paragraph{Deception} The emerging body of research on LLM deception increasingly focuses on measuring \textit{strategic deception} - defined as models ``deceiving selectively based on incentives or instructions" \cite{jones2024lies}. While studies demonstrate that LLMs can produce deceptive statements in response to specific contexts or prompts, this work often faces two key interpretive challenges. First, it risks attributing observed behaviors to model \textit{intentions} to deceive. Second, results are often interpreted as evidence of model-level deceptive traits rather than instance- and context-specific behaviors. 

An alternative, less anthropomorphic framing, proposed by \citet{shanahan2023role}, views these behaviors through the lens of ``role-play" where LLMs \textit{simulate} human-like responses. This interpretation sees the system not as a singular entity but as context-bound, ``inferring and applying approximate communicative intentions" \cite{andreas2022language}. Through this lens, complex behaviors like deception and self-awareness can be understood as sophisticated simulations rather than true cognitive states. This reframing also expands the set of interventions for deceptive behaviors: analyzing training data composition, examining how post-training interventions shape model behavior, and investigating reinforcement learning's effects on output distributions. 

\subsection{User interaction paradigm}\label{sec:user}
\begin{assumption}{\textit{Human interactions with models mirror human-human communication.}}
\end{assumption}

While this assumption can be helpful and does reflect a universal goal -- for systems to be easy to use -- the dominance of this assumption can actually limit (1) users' ability to use LLMs effectively and (2) the types of LLM interfaces we choose to develop.

\paragraph{``Prompting" as the dominant interaction paradigm}
The de facto interaction paradigm for human-LLM interaction is prompt-based interfaces, originally designed as debugging tools for machine learning engineers \cite{morris2024prompting}. As these interfaces resemble human-human chat interfaces, they may encourage users to naturally default to conversational patterns from human interaction. However, research on effective prompting suggests that optimal results often require structured, sometimes non-intuitive formats (e.g., ``least-to-most prompting") rather than human-like communication patterns which rely on shared context and paralinguistic cues \cite{morris2024prompting, zhou2022least}. Simultaneously, research on human-LLM interaction shows that one of the key challenges users face is a significant \textit{gulf of envisioning} or ``distance between the human’s initial intentions and their formulation of a prompt that foresees how LLM capabilities and training data can be leveraged to generate high-quality output" \cite{subramonyam2024bridging}. 

This mismatch between natural dialogue and effective prompting necessitates new interaction paradigms and interface designs. Structured interaction frameworks, using suggested inputs, guided flows, and/or domain-specific prompting strategies, would explicitly expose system capabilities rather than masking them behind conversational abstractions \cite{subramonyam2024bridging, feng2024cocoa, fagbohun2024empirical}. This can bridge the gulf of envisioning by making the system's computational nature more explicit through interface design, enabling more systematic exploration of model functionality.

\section{Recommendations}
In light of our analyses in previous sections, we make the following set of recommendations: 
\paragraph{Develop new conceptualizations that capture the distinct nature of LLMs} Although our analysis highlights the limitations of anthropomorphic assumptions, it does not negate the value of drawing on human psychology and cognitive science, especially for developing new metaphors through which to make sense of LLMs as distinct systems. For example, \citet{mccoy2023embers}'s ``teleological approach", while drawn from cognitive science, is used to illuminate fundamental differences in how humans and LLMs operate and ought to be evaluated. Additionally, \citet{shanahan2023role} ``role-play" conceptualization (and similarly \citet{andreas2022language} ``agent models"), while employing folk psychological terms, does so with conceptual precision that clarifies our understanding of LLMs as unique systems.

\paragraph{Extend critical analysis of anthropomorphism's impact beyond terminology} 

In this work, we analyzed select case studies with underlying anthropomorphic assumptions. Rather than advocating for immediate changes in terminology, we argue for examining how these assumptions limit our understanding of LLMs and constrain research directions. We encourage future work to similarly shift from language critique at the tip of the iceberg to analyze underlying assumptions -- both those we discuss here in more depth as well as others we do not cover -- to open up new paths for methodological development and theoretical frameworks.

\paragraph{Broaden disciplinary perspectives}
There are several fields that offer valuable, less anthropocentric frameworks for current challenges in developing, aligning, and evaluating LLMs. For example,  systems engineering and control theory provide established frameworks for analyzing feedback loops between LLMs and their environment (e.g., users) and understanding in-context alignment challenges \cite{pan2024feedback}. Similarly, design studies and HCI research offer theoretical (e.g., affordance theory) and methodological (e.g., user studies) tools for improving human interactions with increasingly social and human-like systems \cite{ibrahim2024characterizing}. Software engineering offers proven methods for building and testing reliable systems, such as those with LLM-based agents, at scale \cite{bass2025engineering}. 

\section{Alternative views}
Anthropomorphic conceptualizations of LLMs serve important purposes. They provide intuitive frameworks for understanding complex systems, offer pragmatic terminology for discussing model behavior, and have historically driven significant advances in the field. Many researchers argue that these human-centered analogies are not just convenient, but essential for developing and deploying AI systems that will ultimately interact with humans. 

Critics of our position might also note that anthropomorphic thinking has led to breakthroughs in LLM research. They might argue that human cognition provides a proven template for intelligence, making it a valuable guide for AI development. And, that since LLMs are trained on human-generated data and designed to interact with humans, some degree of anthropomorphic framing may be inevitable and even desirable.

Throughout this position paper, we have acknowledged the alternative view that anthropomorphic conceptualizations are (1) natural and pragmatic, as well as (2) helpful. We do not advocate for eliminating anthropomorphic concepts entirely. Rather, we suggest that expanding beyond purely anthropomorphic frameworks can reveal new and potentially clarifying research directions. Our critique focuses specifically on how certain anthropomorphic assumptions may constrain research questions and methodologies. We also do not engage in debates about LLM sentience, agency, or reasoning capabilities. Instead, we examine how assumptions about human-like characteristics shape — and potentially limit — our understanding of these systems.

\section*{Impact Statement}
This paper primarily addresses the computer science research community, examining research priorities and methodologies. We acknowledge that anthropomorphic terminologies and concepts also shape the \textit{broader public}'s perceptions, usage, and likely regulation of these systems \cite{gilardi2024we}. As AI research gains broader visibility, its framing and communication may impact public understanding beyond this position paper's scope. We encourage both future work and readers of this work to consider impacts on the public beyond those discussed here. 

\nocite{langley00}

\bibliography{cites}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
