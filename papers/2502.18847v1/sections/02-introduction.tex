Real-world applications ranging from predicting sales in e-commerce to diagnosing diseases in healthcare rely on tabular data. These datasets are oftentimes a mix of numerical, categorical, and text values, presenting a unique challenge for machine learning models. 
Traditional approaches~\cite{breiman2001random, chen2016xgboost, prokhorenkova2018catboost} as well as some early Deep Learning (DL) models~\cite{yoon2020vime, arik2021tabnet, gorishniy2021revisiting, hollmann2022tabpfn} convert textual data into numerical encodings modeling only structural features from an input table, leading to loss of semantic information.
Recent trends in tabular DL indicate an increase in approaches attempting modality switch from tabular to image~\cite{DeepInsight, wang2019supertml}, text~\cite{tabllm, arik2021tabnet}, or graph~\cite{ignnet, tabgnn}, modeling \textit{either} semantic or structural relationships. These transformations aim to exploit the strengths of established models in vision, language, and graph domains to enhance the representation learning of tabular data. 
Unfortunately, modeling a single type of relationship through uni-modal transformation limits the ability of DL models in this domain to perform well on heterogeneous datasets. In addition, DL models are often prone to overfitting, especially on datasets with high dimensionality or limited samples. Thus, such models frequently struggle to outperform simple linear and tree based models. This discrepancy highlights a fundamental challenge: \textit{effectively integrating the diverse types of information within tabular data}, while preserving the rich semantic and structural nuances.

\input{figures/title_figure}

We bridge this gap by introducing \textbf{\tabglm} (\textbf{Tab}ular \textbf{G}raph \textbf{L}anguage \textbf{M}odel), a novel multi-modal architecture designed to effectively capture \textbf{both structural and semantic information in tabular data}. This is achieved by transforming each row of a tabular dataset into a graph and serialized text, and encoding it using a graph neural network (GNN) and a pretrained text encoder, respectively (Figure \ref{fig:title_figure}). 
Transforming a record into a graph encodes relationships between columns, thus modeling structure, while transforming it into text embeddings captures semantic information.
The joint semi-supervised learning strategy in \tabglm, namely \textsc{MuCosa} (detailed in Section \ref{sec:joint_learner}), aligns the learned representations from both the graph and text encoders while adapting to downstream tasks.
This alignment enhances the quality of the learnt representations by leveraging complementary information from both modalities, while acting as a regularization strategy to prevent overfitting.

To the best of our knowledge, we are the \textit{first to introduce a multi-modal learning framework for tabular data}, with the following principal contributions -
\begin{itemize}
    \item We introduce a \textbf{multi-modal method} that transforms each row of a tabular dataset into a graph and serialized text, capturing both structural and semantic features.\looseness-1
    \item Our joint loss (\textsc{MuCosa}) assists with information fusion from the 2 modalities, while acting as a regularization mechanism to mitigate overfitting.
    \item \tabglm's~targeted use of frozen and trainable components achieves a \textbf{significantly lower (by over 80\%) parameter count} compared to State-of-the-Art (SoTA) uni-modal DL approaches.\looseness-1
    \item Extensive experiments and ablation studies validate the effectiveness of \tabglm\ and its key components. We \textbf{demonstrate an absolute improvement in AUC-ROC scores up to 5.56\%, compared to SoTA models across 25 benchmark datasets} detailed in Section \ref{sec:experiments} of the main paper.\looseness-1
\end{itemize}