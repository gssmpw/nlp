Datasets in real-world are oftentimes heterogeneous consisting of both numerical and textual features. To this end, we introduce Tabular Graph Language Model (\tabglm) depicted in Figure \ref{fig:tabglm_overview}, which tackles the aforementioned challenge by preserving both structural and semantic features enumerated in tabular datasets.

\subsection{Problem Definition}
\label{sec:problem_definition}
Given a tabular dataset $\mathcal{T} \in \mathbb{R}^{n \times m}$ represented as a matrix of $n$ records each with $m$ feature columns and a label $y_i$, where $i \in |\mathcal{T}|$, we are tasked to predict the probability of a newly introduced record $x$ in the test dataset to be one among the target classes $y_i$.
To achieve this goal we train a feature representation learner $h(\mathcal{T}_i; \theta)$, which learns feature representations $g_i$ from samples (rows) $\mathcal{T}_i$ in a training dataset $\mathcal{T}_{train}$, where $i \in [1, n]$ and total model parameters $\theta$. The learned representations $g_i, \forall i \in [1,n]$ are then passed to a predictor $Clf(g_i)$ for downstream classification tasks. 
The performance of the model $h(x_i, \theta)$ on unseen records in $\mathcal{T}_{test}$ largely depends on the quality of learned representations $g$.
Particularly, in this paper we tackle the challenges associated with tables $\mathcal{T}$ containing heterogeneous column types (both numeric and textual) through a multi-modal architecture as discussed in Section \ref{sec:tabglm}.

\subsection{\tabglm: Tabular Graph Language Model}
\label{sec:tabglm}
\tabglm\ introduces a multi-modal architecture as shown in Figure \ref{fig:tabglm_overview}, which encodes each record in an input table into a graph (learning structural features) and serialized text (learning semantic features).
As highlighted in Section \ref{sec:related_work}, recent approaches employ uni-modal transformations, encoding either structural or semantic information. 
A uni-modal model, therefore, lacks the advantages provided by auxiliary modalities (promoting learning of only specific types of features). The multi-modal architecture of \tabglm\ addresses this gap and demonstrates improvements in downstream tasks by combining two complimentary modalities in a single unified architecture. 

We simultaneously transform each record $\mathcal{T}_i \in \mathcal{T}$ into a fully-connected graph $G_i$ and natural language (serialized text) $T_i^{ser}$. 
\tabglm\ then encodes $G_i$ and $T_i^{ser}$ using a graph encoder $E_{graph}$ and a text encoder $E_{text}$, producing feature vectors $g_i^{graph}$ and $g_i^{text}$ respectively.
Finally, we combine the encoded feature vectors, $g_i^{graph}$ and $g_i^{text}$ using a Multi-Modal Consistency Learner (MuCosa) that minimizes the feature separation between complimentary modalities (unsupervised) while adapting to downstream tasks (supervised).
We detail the aforementioned components in our \tabglm\ architecture below, which can be decomposed into the text pipeline and the graph pipeline.

\input{figures/data_processing}

\noindent \textbf{Text Pipeline } The text encoder ($E_{text}$) encodes each record $\mathcal{T}_i \in\mathcal{T}$ into an embedding $g_i^{text}$ with the goal of preserving the semantic information in the cell values. 
We achieve this by first transforming each row in $\mathcal{T}$ to serialized natural text $T^{ser}$ as shown in Figure \ref{fig:data_preprocessing}. 
Drawing inspiration from the authors in \citet{tabllm}, we adopt the simple \texttt{text} serialization which is inexpensive while being representative. An example of this is also depicted in Figure \ref{fig:data_preprocessing} where each row in the table in represented as a templated~\cite{tabllm, tapas} text (serialization). 
The serialized output is tokenized to convert the natural language input (each row in the input table) into tensors $T^{ser}$ before passing to the text encoder $E_{text}$. The text encoder produces the text embedding $g_i^{text}$ as shown in Equation \ref{eq:text_encoding}, where $g_i^{text} \in \mathbb{R}^d$.
Following the recent success of LLMs in tabular question answering~\cite{tapex, tapas}, \tabglm\ adopts the best performing pretrained text encoders in TAPAS~\cite{tapas} and TAPEX~\cite{tapex}, trained on a large number of records. 
The choice of the text encoder presents a trade-off between performance and computational complexity, which is elucidated through the ablation study in Section \ref{sec:ablations}.
The parameters $\theta_{text}$ of the text encoder $E_{text}$ are kept frozen during the training process and used to produce an instance-level (row) embedding \textbf{$g_i^{text}$, encoding context aware features from each record}.
\begin{align}
\begin{split}
g_i^{text} = E_{text}(\texttt{tokenize}(\mathcal{T}_i^{ser}); \theta_{text})
\end{split}
\label{eq:text_encoding}
\end{align}

\noindent \textbf{Graph Pipeline } The Graph Encoder ($E_{graph}$) takes as input a fully connected graph $G(v, e)$ to learn embeddings $g_i^{graph}$ corresponding to each row $\mathcal{T}_i \in \mathcal{T}$. The goal of $E_{graph}$ is to encode the latent structural relationships between columns in the underlying table $\mathcal{T}$.
Following the authors in \citet{ignnet}, we first transform each record $\mathcal{T}_i \in \mathcal{T}$ into a graph representation $G_i(v, e)$ where each node in $v$ encodes the value associated with a feature column in $\mathcal{T}$ and each edge in $e$ represents the relationship between feature columns (as edge weight). 
During implementation, a set of edge weights $W$ re-weights each edge in $G_i$ while an adjacency matrix $A$ encodes the structure of $G_i$.
A known limitation of SoTA tabular graph learning approaches~\cite{ignnet, huang22table2graph} is the ability to encode categorical features. As depicted in Figure \ref{fig:data_preprocessing}, \tabglm\ converts columns with categorical features to numerical encodings as a preprocessing step before passing them as input to $E_{graph}$ which is parameterized by $\theta_{graph}$ as shown in Equation \ref{eq:graph_encoding}.\looseness-1
\begin{align}
\begin{split}
g_i^{graph} = \texttt{proj}( E_{graph}( G_i(v, e); \theta_{graph} ) )    
\end{split}
\label{eq:graph_encoding}
\end{align}

\tabglm\ learns a latent representation $q_i$ for each row in $\mathcal{T}$ by adopting the popular Graph Neural Network (GNN) in \citet{xu2019gnn, ignnet} which employs a sequence of \textit{message passing}~\cite{xu2019gnn} layers in its architecture to learn node level features. These node level features are further aggregated using \textit{read-out layers} to learn an embedding for the input graph $G_i$. 
Several iterations of message passing during model training allows \textbf{$E_{graph}$ to learn the structure of the table $\mathcal{T}$ by modeling the relationships between features columns} (represented as nodes $v$).
The output latent representation $q_i$ is projected to a lower dimensional space using a projection layer \texttt{proj} to produce graph embeddings $g_i^{graph} = \texttt{proj}(q_i)$ as depicted in Figure \ref{fig:tabglm_overview} and expressed in Equation \ref{eq:graph_encoding}. This layer projects the graph embedding in the same dimensional space as the text embedding.\looseness-1

During the \textit{training phase $E_{graph}$ is trained from scratch in an end to end fashion while $E_{text}$ remains frozen}, with the total parameter count of the feature extractor $h$ (composed of $E_{graph}$ and $E_{text}$) being $\theta = \theta_{graph} + \theta_{text}$. This design choice is based on the assumption that the text encoder in TAPAS / TAPEX has learnt generalizable representations from a large volume of records (26.9 billion in \citet{tapas})  it was pretrained on.ng \textit{inference, \tabglm\ omits the forward pass through the text encoder $E_{text}$}, relying solely on the embeddings learnt from $E_{graph}$, \textbf{significantly boosting inference speeds}.
Further, we show through ablation experiments in Section \ref{sec:ablations} that the proposed \textbf{\tabglm\ architecture uses only 336M parameters which is over 80\% lower than SoTA approach TabLLM}~\cite{tabllm}.\looseness-1

\subsection{\textsc{MuCosa}: \textbf{Mu}lti-Modal \textbf{Co}n\textbf{s}istency Le\textbf{a}rner}
\label{sec:joint_learner}
The training of \tabglm\ proceeds in a single stage with a joint sem-supervised learning approach, \textsc{MuCosa}.
As discussed in Section \ref{sec:tabglm}, the representations learnt from both $E_{graph}$ and $E_{text}$ encode orthogonal concepts with the former encoding structure and the latter encoding semantic information.
To combine the learnings from both $g_i^{graph}$ and $g_i^{text}$ we minimize the consistency between the two modalities through a consistency loss, $L_{\text{consistency}}$ as shown in Equation \ref{eq:consistency_loss}. $L_{\text{consistency}}$ aligns the text embeddings $g_i^{text}$ with the graph embeddings $g_i^{graph}$ corresponding to each row in $T_{train}$ and vice versa, in a label free fashion.\looseness-1
\begin{align}
\begin{split}
L_{\text{consistency}} = -\frac{1}{2n} \sum_{i=1}^{n} \Biggl[ \log \frac{\exp\left(\frac{\hat{g}_i^{\text{text}} \cdot (\overline{\hat{g}_i^{\text{graph}}})^T}{\tau}\right)}{\sum_{j=1}^{n} \exp\left(\frac{\hat{g}_i^{\text{text}} \cdot (\overline{\hat{g}_j^{\text{graph}}})^T}{\tau}\right)} \\
+ \log \frac{\exp\left(\frac{\hat{g}_i^{\text{graph}} \cdot (\overline{\hat{g}_i^{\text{text}}})^T}{\tau}\right)}{\sum_{j=1}^{n} \exp\left(\frac{\hat{g}_i^{\text{graph}} \cdot (\overline{\hat{g}_j^{\text{text}}})^T}{\tau}\right)} \Biggr] \\
\end{split}
\label{eq:consistency_loss}
\end{align}

Here, $\hat{g}_i^{\text{text}} = \frac{g_i^{\text{text}}}{\|g_i^{\text{text}}\|_2}$ and $\hat{g}_i^{\text{graph}} = \frac{g_i^{\text{graph}}}{\|g_i^{\text{graph}}\|_2}$ represents the normalized form of the graph and text embeddings, $\tau$ (set to 0.1 following \citet{chen2020simple}) denotes the temperature term and $\overline{\hat{g}_i^{\text{text}}}$, $\overline{\hat{g}_i^{\text{graph}}}$ indicates explicitly that gradients are not propagated for those terms.
Additionally, we minimize a supervised loss $L_{\text{supervised}}$ between the ground truth $y_i$ and the predicted logits from a classifier head $\hat{y}_i = Clf(g_i^{graph})$ as shown in Figure \ref{fig:tabglm_overview}. Note, that the classifier head consumes only the graph embeddings to mimic the inference setting.
The supervised loss can be represented as Equation \ref{eq:supervised}.\looseness-1

\begin{align}
    L_{\text{supervised}} = \frac{1}{n} \sum_{i = 1}^{n} H(y_i, \hat{y}_i)
\label{eq:supervised}
\end{align}

Finally, \tabglm\ introduces joint objective, \textsc{MuCosa} ($L$) as shown in Equation \ref{eq:joint_loss} which combines both $L_{\text{supervised}}$ and $L_{\text{consistency}}$ as a weighted sum with the hyper-parameter $\lambda$ controlling the contribution of each component to the total loss $L$.\looseness-1
\begin{align}
L = (1 - \lambda) L_{\text{supervised}} + \lambda L_{\text{consistency}}
\label{eq:joint_loss}
\end{align}