\noindent \textbf{Traditional Tabular Machine Learning} 
Historically, the realm of tabular data modeling over the past decade has been largely dominated by conventional machine learning methods~\cite{shwartz2022tabular}. 
Models such as Gradient Boosting~\cite{bentejac2021comparative}, ExtraTrees~\cite{geurts2006extremely}, and Random Forests~\cite{breiman2001random} have been pivotal in learning intricate data patterns and enhancing robustness against overfitting. 
Notable techniques like XGBoost \cite{chen2016xgboost} and LightGBM \cite{ke2017lightgbm} stand out for their efficiency, optimization techniques, and scalability, making them go-to options in various applications. 
Logistic regression \cite{hosmer2013applied} has been particularly applied to binary classification tasks due to its simplicity and interpretability.
Specialized algorithms like CatBoost~\cite{prokhorenkova2018catboost}, designed to handle categorical features seamlessly, have gained prominence. 
This diverse set of models contributes to a versatile toolbox, addressing the intricacies of tabular data modeling with distinct strengths and adaptability \cite{grinsztajn2022tree}. These traditional models provide a solid foundation for tabular data analysis, balancing interpretability, efficiency, and performance essential for real-world applications.
Despite their effectiveness, these models are often limited by their reliance on handcrafted feature engineering and their inability to leverage the representation learning capabilities inherent in deep learning models.\looseness-1

\input{figures/overview}

\noindent \textbf{Transformers for tabular data} 
Following the popularity of Transformer architectures in vision and language, several methods~\cite{hollmann2022tabpfn, arik2021tabnet, zhu2023xtab} have adapted transformers for learning from tabular datasets. 
For instance, FT-Transformer~\cite{gorishniy2021revisiting} showed superior performance in tabular classification and regression tasks by separating numerical and categorical features. 
Additionally, Saint~\cite{somepalli2021saint} introduced row-wise attention, capturing inter-sample interactions, Fastformer~\cite{wu2021fastformer} suggested the use of additive attention which is lightweight with linear complexity, while TransTab~\cite{wang2022transtab} incorporated transfer learning in tabular tasks, all using transformers as backbones. Recent advancements have specifically tailored the transformer architecture to address challenges in data imputation and cross-table learning, incorporating modifications to the attention mechanism and embedding layers~\cite{badaro2023transformers}.\looseness-1

\noindent \textbf{Self-supervised pretraining} 
Furthermore, the emergence of self-supervised pretraining in the tabular domains has paved the way for novel approaches to feature extraction and representation learning, reducing the reliance on labeled data \cite{liu2021self}. Specifically, drawing inspiration from the success of pretraining in vision and language, previous studies have delved into tabular self-supervised learning \cite{yoon2020vime, ucar2021subtab, somepalli2021saint, bahri2021scarf, majmundar2022met, rubachev2022revisiting, wang2022transtab}. 
Authors in~\cite{yoon2020vime, ucar2021subtab} introduced an auto-encoder framework with a pretext task focused on reconstructing missing elements in a table while \cite{bahri2021scarf} utilized contrastive learning~\cite{chen2020simple} as pretraining objective for improving generalizability of trained architectures in tabular tasks. 
Additionally, \cite{rubachev2022revisiting, wang2022transtab} created a target-aware objective by incorporating label columns of tabular tasks in pretraining. 
Although these innovations have largely improved performance over traditional machine learning approaches, these models have been shown to particularly underperform in the presence of heterogeneous feature columns~\cite{tabllm}.\looseness-1

\noindent \textbf{Modality switch for Tabular Deep Learning} Recent research has explored the conversion of tabular data into orthogonal modalities, such as text, image, and graph. 
TabLLM \cite{tabllm} converted tabular data to text for few-shot classification using large language models. Although it can suffer from context loss and inefficiency when handling high-dimensional data, TabLLM successfully captures the semantic information encapsulated within columns in a table.
SuperTML \cite{wang2019supertml} introduced a method to transform tabular data into a super ensemble of image-based data points, enabling the use of convolutional neural networks for tabular tasks. DeepInsight \cite{DeepInsight} proposed projecting tabular data into an image space using t-SNE, enabling the application of image classification models to tabular data. Even though this technique effectively captures underlying feature correlations, the reliance on a single-image representation and t-SNE's specific distance metric limits its ability to capture diverse and multi-faceted relationships inherent in complex tabular datasets. 
Table2Graph~\cite{huang22table2graph} transforms tabular data into a unified weighted graph and IGNNet \cite{ignnet} transforms each record into a fully-connected graph, allowing the application of graph neural networks (GNNs) for tabular data learning. 
Additionally, GCondNet~\cite{margeloiu2023gcondnet} transforms each column into a graph while CARTE~\cite{kim2024carte} mines entities in tables to learn from entity-centric graphs.
Furthermore, models like Graph foundation models \cite{galkin2024towards, zhang2024gnn} and \cite{sun2023gpt} highlight the efficacy of GNNs in capturing relational structures within tabular data.  
HyTrel \cite{chen2023hytrel} enhances tabular data representation by integrating hypergraph structures, which can capture high-order relationships among features, but the complexity of hypergraph construction and the increased computational cost are significant challenges.
Despite their innovative approach, these methods often face scalability issues with large datasets and are sensitive to the graph construction method. Additionally, even though graphs can capture the structural relationships among features in a table, they cannot capture the semantic information of the categorical and text columns, as well as the column headers. This information can provide valuable insight, which is especially valuable when learning from small datasets. \looseness-1

\noindent \textbf{Multi-Modal Learning}
Multi-modal learning integrates data from multiple sources, such as text, image, video, and audio to enhance machine learning models' performance. A pivotal model in this domain is CLIP \cite{radford2021learning}, which aligns text and image representations using contrastive learning, enabling effective zero-shot learning and image-text retrieval. Other significant advancements include~\cite{hegde2023clip3d}, which adapts CLIP to 3D recognition tasks through prompt tuning for language grounding, as well as \cite{Chen_2023}, which introduces cross-modal knowledge distillation, and \cite{ramesh2021zero}, which introduces zero-shot text-to-image generation.\looseness-1

An important lesson from existing literature is that multi-modal models are capable of generalizing to downstream tasks by capturing complementary information from multiple modalities. For instance, they extract complex spatial patterns from images, semantic meaning from text, and structural relationships from graphs.  
We capitalize on this property to design a multi-modal model for tabular machine learning that combines the richness of graph and text modalities into a unified embedding space to improve performance on downstream ML tasks.
To the best of our knowledge, we are the first to introduce multi-modal learning for tabular datasets using a single table as input across several classification based downstream tasks.\looseness-1