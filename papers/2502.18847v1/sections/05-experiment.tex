We conduct our experiments on a wide variety of tabular datasets (refer Section \ref{sec:datasets}) with varying levels of data heterogeneity across a variety of downstream classification tasks (refer Section \ref{sec:tabglm}).

\input{tables/01-benchmark_Sota_methods}

\subsection{Datasets}
\label{sec:datasets}
To demonstrate the effectiveness of \tabglm\ in the presence of heterogeneous feature columns, as discussed in Section \ref{sec:tabglm}, we conduct our experiments on 25 datasets encompassing both binary and multi-class classification tasks, curated from popular papers TabLLM~\cite{tabllm}, TabPFN~\cite{hollmann2022tabpfn}, and large scale datasets in OpenML~\cite{openml2017}.
Following the principal goal of \tabglm, we consider heterogeneous datasets that encapsulate both numerical and textual columns like \textbf{Bank} ($\sim$45k records with 7 numerical and 9 categorical columns), \textbf{Creditg} (1k rows with 7 numerical and 13 categorical columns), \textbf{Heart} (918 rows with 6 numerical and 5 categorical columns) and \textbf{Income} ($\sim$48k rows with 4 numerical and 8 categorical columns), as shown in TabLLM. In addition, we use 12 datasets from OpenML, containing at least 1 numerical and 1 categorical column, including \textbf{balance-scale} (5 numerical and 1 categorical), \textbf{tic-tac-toe} (10 numerical and 10 categorical), \textbf{dress-sales} (13 numerical and 12 categorical) etc with more details in appendix. 
We also include datasets containing only numerical columns like \textbf{blood} (4 numerical columns), \textbf{calhousing} (8 numerical columns), \textbf{coil2000} (86 numerical columns) etc. alongside datasets containing only categorical columns like \textbf{car}, from both OpenML and TabPFN. 
We adopted datasets of varying sizes, with number of rows ranging from 500 (in \textbf{dress-sales}) to 45,211 (in \textbf{bank}) to demonstrate the applicability of our method to real-world large tabular datasets.
Note that the multi-modal architecture in \tabglm\ involves a LLM encoder~\cite{tapas, tapex} that is limited by the number of input tokens, which is 512 (from TAPAS) in our case.
More details on each dataset experimented upon in Table \ref{tab:sota_perf_contrast} is discussed in the supplementary material.\looseness-1

\subsection{Experimental Setup}
We conduct our experiments on datasets discussed in Section \ref{sec:datasets} and report the average performance (AUC-ROC scores) of each model across the same 5 random seeds (kept constant across datasets) in Section \ref{sec:results}. 
For all numerical and heterogeneous datasets, numerical columns are normalized using min-max\footnote{Scikit learn package: \url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html}} normalization while the categorical (text) columns are converted into One-Hot encodings (refer ablation in supplementary material) to create a numeric dataset for graph transformation.
For the text transformation, each record in the table is converted to serialized text following the tokenizer in TAPAS~\cite{tapas}. We chose TAPAS based on ablation experiments on the choice of LLMs in Section \ref{sec:ablations}. 
For datasets that contain only categorical columns, our \tabglm\ method uses only the text pipeline, utilizing only the semantic information present in such datasets.
Models for all datasets are trained on a fixed set of hyperparameters with an initial learning rate of $1e^{-4}$, batch size of 256 and weighting the consistency loss at 20\% ($\lambda = 0.2$). 
All experiments are conducted on 4 NVIDIA V100 GPUs with additional details on the experiment setup in the Appendix and code released at \url{https://github.com/amajee11us/TabGLM}.\looseness-1

\input{tables/02-benchmark_dl_methods}

\subsection{Results}
\label{sec:results}
At first, we compare the performance of \tabglm\ with traditional linear and tree based Machine Learning models like CatBoost~\cite{prokhorenkova2018catboost}, XGBoost~\cite{chen2016xgboost}, Gradient Boosting (GB) \cite{ke2017lightgbm}, Random Forest (RF) \cite{breiman2001random} and Logistic Regression (LR).
Our results in Table \ref{tab:sota_perf_contrast} show that \tabglm\ demonstrates significant increase in AUROC of 4.77\% over LR, 2.51\% over RF etc. outperforming such techniques across 25 downstream tabular classification tasks.
However, for simple datasets with lower number of feature columns like \textbf{kr-vs-kp}, \textbf{pc3} etc., tree based models (CatBoost) continue to show dominance in performance.\looseness-1

Secondly, we compare the performance of \tabglm\ with SoTA tabular DL models like FT-Transformer~\cite{gorishniy2021revisiting}, TabTransformer~\cite{huang2020tabtransformer} and NODE~\cite{popov2019neural}. \tabglm\ consistently outperforms tabular DL models like FT-Transformer by 5.56\%, TabTransformer by 3.64\% and NODE by 1.26\% respectively.
Finally, we compare the performance of \tabglm\ against SoTA uni-modal DL architectures like IGNNet (table-to-graph) and TabLLM (table-to-text) on 9 datasets in the benchmark introduced in TabLLM~\citet{tabllm}. We observe that \tabglm\ outperforms TabLLM by 1.35\% and IGNNet by 7.96\% respectively on the benchmark datasets in \cite{tabllm}, summarized in Table \ref{tab:dl_model_benchmark}.
The above results indicate a strong generalization of the proposed \tabglm\ architecture to a variety of downstream tasks, establishing \tabglm\ as a strong choice for Tabular Deep Learning under feature heterogeneity.\looseness-1

\subsection{Ablation Study}
\label{sec:ablations}

\input{tables/04-ablation_choice_of_llm}

\noindent \textbf{Multi-Modal vs. Uni-Modal training:}
The core contribution of \tabglm\ lies in its multi-modal architecture for tabular representation learning. To evaluate its components, we decompose it into two uni-modal architectures: \textit{Graph only} (using only the graph encoder $E_{\text{graph}}$) and \textit{Text only} (using only the text encoder $E_{\text{text}}$), based on the choice of the feature extractor during both training and inference. 
Their performance is compared against the complete multi-modal \tabglm\ training recipe, with results summarized in Table \ref{tab:tabglm_components}. 
The \textit{Graph only} pipeline employs the GNN from \cite{ignnet}, while the \textit{Text only} pipeline uses the BART-based TAPAS~\cite{tapas} encoder. 
Both pipelines use the same classifier head (Section \ref{sec:tabglm}) for downstream tasks. In \textbf{Text only}, the encoder is frozen, and only the classifier head is trained, whereas in \textit{Graph only}, both the encoder and classifier head are trained, to ensure fair comparison with \tabglm, where the text encoder remains frozen during training. 
Experiments on three representative datasets—\textbf{pc3} (numerical), \textbf{bank} (balanced numerical and categorical), and \textbf{creditg} (categorical-heavy)—show that \tabglm's multi-modal design consistently outperforms its uni-modal variants, underscoring the value of modality fusion for learning from heterogeneous tables.\looseness-1

\input{tables/03-ablation_modalities}

\noindent \textbf{Choice of LLM architecture for Text Transformation:}
The choice of the pretrained LLM architecture plays a crucial role in improving the model performance of \tabglm. 
While larger LLMs like \cite{sun2023gpt, tabllm, tapex} ($\geq$7 billion parameters) can encode superior semantic features in complex text, it also adds a significant computational overhead. Additionally, their benefits may be negligible when dealing with simpler semantic content.
To address this trade off, we conducted an ablation experiment by varying the architecture of the text encoder ($E_{text}$) across three popular LLM models - TAPAS~\cite{tapas}, TAPEX~\cite{tapex} and TabLLM~\cite{tabllm}.
For all three settings we adopt the complete multi-modal training strategy, modifying only the text encoder $E_{text}$. 
The results from this experiment, shown in Table \ref{tab:choice_of_llm}, highlight that TAPAS~\footnote{We adopt the TAPAS-base model from \url{https://huggingface.co/google/tapas-base}}, a smaller parameter count, BERT~\cite{devlin2018bert} based text encoder, outperforms other larger models like TAPEX~\cite{tapex}. We thus adopt this architecture for the text transformation pipeline in \tabglm.\looseness-1
