\section{Notations}
\label{app:notations}
In this section we pen down explanations for the symbols used in various equations and mathematical formualations in \tabglm.

\begin{table*}[t]
      \caption{Collection of notations used in \tabglm.}
      \centering
      \begin{tabular}{ c | c }
            \toprule
           \textbf{Symbol}  & \textbf{Description} \\
            \midrule
            $\mathcal{T}$ & Tabular dataset where $\mathcal{T} \in \mathbb{R}^{m \times n}$ with $|\mathcal{T}| = n$. \\
            $\mathcal{T}_{train}$ & Training dataset where $\mathcal{T}_{train} \subseteq \mathcal{T}$. \\
            $\mathcal{T}_{test}$ & Testing dataset where $\mathcal{T}_{test} \subseteq \mathcal{T}$. \\
            $n$ & Total number of rows in the dataset $\mathcal{T}$. \\
            $m$ & Total number of features (columns) in the dataset $\mathcal{T}$. \\ 
            $h(x, \theta)$ & Multi-Modal Neural Network used as feature extractor which accepts as $x$ (one row) as input. \\
            $Clf(.,.)$ & Multi-Layer Perceptron as classifier or Regressor. \\
            $\theta$ & Total Parameters of the feature extractor $h$ including both text and graph encoders. \\
            $\theta_{graph}$ & Parameters of the graph feature extractor (GNN in \cite{ignnet}). \\
            $\theta_{text}$ & Parameters of the text encoder (TAPAS~\cite{tapas}). \\
            $E_{graph}$ & Graph Encoder. Represents a Message-Passing based graph neural network~\cite{xu2019gnn}. \\
            $E_{text}$ & Text Encoder. Represented through a Large Language Model (LLM). \\
            $q_i$ & Intermediate representation of each row $\mathcal{T}_i$ represented as grpah $G_i$. \\
            $g_i^{graph}$ & Graph embedding corresponding to each row of the input table. \\
            $g_i^{text}$ & Text embedding corresponding to each row of the input table. \\
            $T_i^{ser}$ & Serialized text representation of each row of the input table. \\
            \texttt{proj} & MLP projection layer to bridge the gap between text and graph domains. \\ \bottomrule
      \end{tabular}\\
      \label{tab:notations}
      
\end{table*}

\section{Additional Setup Details}
\label{app:setup}
In this section we detail out additional setup details with details of hyper-parameters and outline of the software framework in \tabglm. 
At first, we detail out the Data-Preprocessing pipeline in \tabglm\ explained in Section 3.2 of the main paper.
The multi-modal pipeline in \tabglm\ transforms the input table $\mathcal{T}$ into graph and text representation. 
To minimize the sample level consistency in \tabglm, \textbf{each row in $\mathcal{T}$ is simultaneously converted to a serialized text $T_i^{ser}$ and a graph $G_i$}. 
The text serialization process is largely dependent on the choice of the tokenizer. In \tabglm\ we experiment on two different choices - TAPEX~\citep{tapex} and TAPAS~\citep{tapas} and the best performing architecture is chosen for downstream evaluation tasks (in our case this is TAPAS). Thus, we follow the serialization for TAPAS as shown in Figure 3 of the main paper.
The challenge lies in the graph transformation where existing methods like IGNNet~\citep{ignnet}, Table2Graph~\citep{huang22table2graph} etc. have assumed the input data to be numeric during the graph generation phase.
To overcome this challenge we convert the textual (categorical) columns into one-hot encodings, rendering them as numeric which are then joined to the existing numeric columns to form the final table for graph generation. Although, one-hot encoding adds additional columns to the input table $\mathcal{T}$, contrasting against other encoding techniques popular in Tabular DL literature like LabelEncoder or OrdinalEncoder from Scikit-learn, One-hot encoding produced the best downstream performance on several datasets. Thus we adopted On-hot encoding in our design of \tabglm. We ablate on the choice of encoders (LabelEncoder vs. One-Hot Encoder) in Table \ref{tab:choice_of_encoder} and conclude that One-Hot Encoding produces the best overall performance in \tabglm\ across 4 diverse datasets.

\begin{table}[h]
\centering
\caption{\textbf{Contrasting Encoding Strategies} for transforming heterogeneous tables to numeric ones for graph generation. Results from all methods are averaged over five seeds.}
    \begin{tabular}{l|cc}
    \toprule
    \multirow{3}{*}{ \textbf{Dataset} } & \multicolumn{2}{c}{ \textbf{Encoding Techniques} } \\ \cline{2 - 3}
                               & \textbf{\tabglm} & \textbf{\tabglm} \\
                               & (LabelEncoder) & (One-Hot Encoder)\\
    \midrule \midrule
        blood        & 78.29 & \textbf{78.49}  \\
        calhousing   & 95.34 & \textbf{95.47} \\
        coil2000     & 73.68          & \textbf{74.17} \\
        diabetes     & 83.14 & \textbf{83.70} \\ \midrule
        \textbf{Average} & 82.61 & \textbf{82.95} \\ \hline
    \end{tabular}
\label{tab:choice_of_encoder}
\end{table}

Since, our experiments consisted of several benchmark datasets spanning over multiple types of downstream tasks in classification and regression, it is challenging to tune the hyper-parameters for each and every dataset. We thus adopt a fixed set of hyper-parameters across every experiment in \tabglm. We adopt a fixed batch size of 256 for all benchmark datasets, with an initial learning rate of 1e-4 and weight the consistency loss in \textsc{MuCosa} at 0.2 which provides a higher weightage to the supervised loss allowing the model to rapidly adapt to downstream tasks. 
Additionally, we also introduce early stopping with a maximum number of training epochs set at 240. This paves the way for a fairer comparison across methods and datasets in \tabglm. Finally, since the performance of \tabglm\ varies with machine seed values we report the performance of each method across datasets on 5 fixed seeds - 5, 108, 180, 234 and 250. A particular seed value is set at the start of each experiment.
With the aim for introducing a framework for experimenting on tabular datasets we encapsulate the codebase for the aforementioned experiments with additional plug and play features for integrating new datasets and benchmarks, on \url{https://anonymous.4open.science/r/TabGLM/}.

\subsection{Dataset Details}
\label{sec:dataset_details}
As already summarized in Section 4.1 in the main paper, we conduct our benchmark experiments on 25 datasets spanning binary classification, multi-class classification and regression tasks. 
Given below are brief descriptions of each dataset describing the heterogeneity of the feature columns. A summary of the statistics corresponding to each dataset is also demonstrated in Table \ref{tab:dataset_details}. 

\begin{enumerate}
    \item \textbf{bank}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/bank+marketing}}: The Bank Marketing dataset contains 7 numerical and 9 categorical columns, with 45,211 rows. It captures data related to a direct marketing campaign of a Portuguese bank, used to predict whether a client will subscribe to a term deposit.


    \item \textbf{blood}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Blood+Transfusion+Service+Center}}: This dataset includes 4 numerical columns and 748 rows, capturing features of blood donors used to predict whether a donor gave blood in a recent period.

    \item \textbf{calhousing}\footnote{\url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html}}: The California Housing dataset, consisting of 8 numerical columns and 20,640 rows, provides median house prices for California districts, commonly used for regression tasks. However, following the dataset specifications in TabLLM~\citep{tabllm} benchmark we convert this dataset to a binary classification setting with the positive label being assigned to properties with price $\geq$ median price.

    \item \textbf{car}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Car+Evaluation}}: The Car Evaluation dataset contains 6 categorical columns and 1,728 rows, assessing the quality of a car based on several criteria, used for multi-class classification.

    \item \textbf{coil2000}\footnote{\url{https://www.openml.org/d/384}}: This dataset comprises 86 numerical columns and 9,822 rows, containing information about clients of an insurance company, used to predict whether a client will be interested in a product.

    \item \textbf{creditg}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)}}: The German Credit dataset includes 7 numerical and 13 categorical columns, with 1,000 rows. It is used to predict credit risk, making it a staple for binary classification tasks.

    \item \textbf{diabetes}\footnote{\url{https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database}}: The Pima Indians Diabetes dataset contains 8 numerical columns and 768 rows, capturing health parameters used to predict the onset of diabetes, widely used for binary classification.

    \item \textbf{heart}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/heart+Disease}}: This dataset includes 6 numerical and 5 categorical columns, with 918 rows. It is used to predict heart disease, a common binary classification task.

    % \item  \textbf{jungle}\footnote{https://archive.ics.uci.edu/ml/datasets/Covertype}: The Covertype dataset, primarily composed of 6 numerical columns and 44,819 rows, is used in various classification tasks, including forest cover type prediction.

    \item \textbf{kr-vs-kp}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/kr-vs-kp}}: The King-Rook vs. King-Pawn dataset contains 37 numerical columns and 3,196 rows, with features extracted from chess endgames, used to classify game outcomes.

    \item \textbf{mfeat-fourier}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Multiple+Features}}: This dataset provides 77 numerical columns and 2,000 rows, consisting of Fourier coefficients of digitized images of handwritten digits, used for multi-class classification.

    \item \textbf{pc3}\footnote{\url{http://promise.site.uottawa.ca/SERepository/datasets-page.html}}: The PC3 dataset contains 38 numerical columns and 1,563 rows, from software defect prediction, used to classify software modules as defective or not.

    \item \textbf{income}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/adult}}: The Adult dataset includes 4 numerical and 8 categorical columns, with 32,561 rows. It is often used for income prediction based on census data, making it a common binary classification task.

    \item \textbf{texture}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Multiple+Features}}: This dataset consists of 40 numerical and 1 categorical column, with 5,500 rows. It captures texture images, commonly used in image classification tasks.

    \item \textbf{balance-scale}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Balance+Scale}}: The Balance Scale dataset simulates the balance state of a scale based on the weight and distance from the center, with 5 numerical and 1 categorical column, totaling 625 rows, used for multi-class classification.

    \item \textbf{mfeat-karhunen}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Multiple+Features}}: This dataset contains 65 numerical and 1 categorical column, with 2,000 rows, capturing Karhunen-Loeve coefficients from digitized images of handwritten digits, used for multi-class classification.

    \item \textbf{mfeat-morphological}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Multiple+Features}}: This dataset is derived from morphological features of handwritten digits, containing 7 numerical and 1 categorical column, with 2,000 rows, used for multi-class classification tasks.

    \item \textbf{mfeat-zernike}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Multiple+Features}}: This dataset provides 48 numerical and 1 categorical column, with 2,000 rows, consisting of Zernike moments of digitized images of handwritten digits, used for multi-class classification tasks.

    \item \textbf{cmc}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Contraceptive+Method+Choice}}: The Contraceptive Method Choice dataset includes 10 numerical and 8 categorical columns, with 1,473 rows. It is used to predict contraceptive method choices among women, making it a common multi-class classification task.

    \item \textbf{tic-tac-toe}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame}}: This dataset represents the states of a Tic-Tac-Toe game with 10 numerical and 10 categorical columns, totaling 690 rows, used to predict win/loss outcomes in a binary classification task.

    \item \textbf{vehicle}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes)}}: This dataset contains 19 numerical and 1 categorical column, with 846 rows, used to classify different types of vehicles based on their silhouette, a common multi-class classification task.

    \item \textbf{eucalyptus}\footnote{\url{https://openml.org/d/188}}: The Eucalyptus dataset includes 20 numerical and 6 categorical columns, with 736 rows. It is used for the classification of eucalyptus species, making it a multi-class classification task.

    \item \textbf{analcatdata\_author}\footnote{\url{https://www.openml.org/d/183}}: The dataset includes 5 numerical and 5 categorical columns, with 797 rows, used for predicting the gender of an author based on bibliometric data, typically for binary classification.

    \item \textbf{MiceProtein}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression}}: This dataset consists of 82 numerical and 5 categorical columns, with 1,080 rows. It captures protein expression levels across multiple protein types, used for multi-class classification tasks.

    \item \textbf{steel-plates-fault}\footnote{\url{https://archive.ics.uci.edu/ml/datasets/Steel+Plates+Faults}}: This dataset contains 28 numerical and 1 categorical column, with 1,941 rows, capturing features related to faults in steel plates, used for multi-class classification of fault types.

    \item \textbf{dress-sales}\footnote{\url{https://www.openml.org/search?type=data&status=active&id=23381}}: This dataset includes 13 numerical and 12 categorical columns, with 500 rows, containing sales data for different dress designs, used for binary classification tasks as used in OpenML.

\end{enumerate}

\begin{table}[h]
    \centering
    \caption{Adaptation of \tabglm\ towards categorical datasets by employing only the text pipeline during model training. The results are averaged over 5 seeds on the \textbf{car} dataset.}
    \begin{tabular}{c|c}
         \toprule
         \textbf{Design Strategy in \tabglm} & AUC-ROC \\ \midrule
         \tabglm\ Full (multi-modal)    & 98.37  \\
         \tabglm\ text-only (uni-modal) & \textbf{99.40}  \\
         \bottomrule
    \end{tabular}    
    \label{tab:text_only}
\end{table}


\begin{table*}[ht]
    \centering
    \caption{\textbf{Detailed Statistics of Datasets in \tabglm\ benchmark.}}
    \label{tab:dataset_details}
    \resizebox{0.8\textwidth}{!}{
        \begin{tabular}{l|c|ccc}
        \toprule
        \textbf{Dataset} & \textbf{Type} & \textbf{Numerical Columns} & \textbf{Categorical Columns} & \textbf{No. of Rows} \\
        \midrule \midrule
        bank             & Binary        & 7   & 9  & 45,211 \\
        blood            & Binary        & 4   & 0  & 748 \\
        calhousing       & Binary\footnote{As in TabLLM~\citep{tabllm}}    & 8   & 0  & 20,640 \\
        car              & Multi-class   & 0   & 6  & 1,728 \\
        coil2000         & Binary        & 86  & 0  & 9,822 \\
        creditg          & Binary        & 7   & 13 & 1,000 \\
        diabetes         & Binary        & 8   & 0  & 768 \\
        heart            & Binary        & 6   & 5  & 918 \\
        % jungle           & Binary        & 6   & 0  & 44,819 \\
        kr-vs-kp         & Binary        & 37  & 0  & 3,196 \\
        mfeat-fourier    & Multi-class   & 77  & 0  & 2,000 \\
        pc3              & Binary        & 38  & 0  & 1,563 \\
        income           & Binary        & 4   & 8  & 32,561 \\
        texture          & Multi-class   & 40  & 1  & 5,500 \\
        balance-scale    & Multi-class   & 5   & 1  & 625 \\
        mfeat-karhunen   & Multi-class   & 65  & 1  & 2,000 \\
        mfeat-morphological & Multi-class & 7   & 1  & 2,000 \\
        mfeat-zernike    & Multi-class   & 48  & 1  & 2,000 \\
        cmc              & Multi-class   & 10  & 8  & 1,473 \\
        tic-tac-toe      & Binary        & 10  & 10 & 690 \\
        vehicle          & Multi-class   & 19  & 1  & 846 \\
        eucalyptus       & Multi-class   & 20  & 6  & 736 \\
        analcatdata\_author & Binary      & 5   & 5  & 797 \\
        MiceProtein      & Multi-class   & 82  & 5  & 1,080 \\
        steel-plates-fault & Multi-class & 28  & 1  & 1,941 \\
        dress-sales      & Binary        & 1  & 12 & 500 \\
        \bottomrule
        \end{tabular}
    }
\end{table*}

\subsection{Additional Explanations to Results in Table 1}
Following the discussion in Section 4.4 we show in Table 1, the performance of our approach \tabglm\ on 25 diverse benchmarks spanning varied dataset sizes and degree of heterogeneity (number of numerical vs. categorical columns in the table). 
For each dataset we run each method for 5 distinct seed values and report the average AUC-ROC scores in Table 1 with standard deviations provided in Table \ref{tab:sota_perf_contrast_with_std}. 
On an average we show that \tabglm\ outperforms both traditional ML models and newer Tabular DL models on most downstream tasks. In this section, we provide a more granular analysis of our results.
At first, we observe that \tabglm\ consistently outperforms existing approaches in datasets with at least 1 heterogeneous feature column demonstrating its capability to learn discriminative feature representations from heterogeneous datasets. 
Secondly, we observe that \tabglm\ consistently outperforms recent DL based approaches like NODE~\citep{popov2019neural}, TabTransformer~\citep{huang2020tabtransformer} and unimodal approaches like IGNNet and TabLLM~\citep{tabllm} by significant margins. This is particularly due to the fact that DL based approaches model either spatial, semantic or structural features completely missing the importance of modelling the benefits of auxiliary modalities. 
Thirdly, we observe that for numeric tables like \textbf{blood}, \textbf{calhousing} graph based models like IGNNet provide significant boost in performance either comparable or greater than that of \tabglm\ (refer Table 3). On the contrary for categorical tables (containing only categorical columns), IGNNet significantly underperforms over \tabglm\ highlighting the importance of multi-modal learning to address data heterogeneity.

\begin{table*}[ht]
\centering
\scriptsize
\caption{\textbf{Comparison of performance (AUCROC) including Standard Deviations (Std) of existing approaches in tabular Machine Learning against \tabglm}. Our proposed method \tabglm\ achieves significant performance gains across 25 classification datasets. The best performing model is highlighted in bold, while the second best is italicized.}
\label{tab:sota_perf_contrast_with_std}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|rl|rl|rl|rl|rl|rl|rl|rl|rl}
\toprule
& \multicolumn{2}{c}{\textbf{TabGLM (ours)}} & \multicolumn{2}{c}{\textbf{CatBoost}} & \multicolumn{2}{c}{\textbf{GB}} & \multicolumn{2}{c}{\textbf{LR}} & \multicolumn{2}{c}{\textbf{RF}} & \multicolumn{2}{c}{\textbf{XGBoost}} & \multicolumn{2}{c}{\textbf{TabTransformer}} & \multicolumn{2}{c}{\textbf{FT-Transformer}} & \multicolumn{2}{c}{\textbf{NODE}}\\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
\cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
\cmidrule(lr){16-17} \cmidrule(lr){18-19}
Dataset & AUC-ROC & Std & AUC-ROC & Std & AUC-ROC & Std & AUC-ROC & Std
        & AUC-ROC & Std & AUC-ROC & Std & AUC-ROC & Std & AUC-ROC & Std
        & AUC-ROC & Std \\
\midrule
\midrule
bank                 & 92.07 & $\pm$0.10 & 93.51 & $\pm$0.03 & 92.36 & $\pm$0.01 & 86.76 & $\pm$0.11 & 92.46 & $\pm$0.01 & 92.84 & $\pm$0.03 & 90.05 & $\pm$0.06 & 92.07 & $\pm$0.06 & 92.67 & $\pm$0.01 \\
blood                & 78.48 & $\pm$0.04 & 74.94 & $\pm$0.01 & 72.24 & $\pm$0.01 & 76.76 & $\pm$0.02 & 70.77 & $\pm$0.01 & 69.51 & $\pm$0.01 & 74.26 & $\pm$0.04 & 74.98 & $\pm$0.10 & 76.21 & $\pm$0.06 \\
calhousing           & 95.47 & $\pm$0.03 & 93.55 & $\pm$0.03 & 92.47 & $\pm$0.03 & 90.84 & $\pm$0.02 & 93.45 & $\pm$0.02 & 81.99 & $\pm$0.01 & 83.13 & $\pm$0.13 & 93.62 & $\pm$0.07 & 93.84 & $\pm$0.05 \\
car                  & 99.40 & $\pm$0.04 & 99.97 & $\pm$0.02 & 99.83 & $\pm$0.02 & 78.46 & $\pm$1.03 & 99.41 & $\pm$0.15 & 99.92 & $\pm$0.08 & 98.57 & $\pm$0.11 & 98.51 & $\pm$0.20 & 99.64 & $\pm$0.27 \\
coil2000             & 74.17 & $\pm$0.40 & 73.97 & $\pm$1.20 & 74.66 & $\pm$0.60 & 73.22 & $\pm$1.10 & 69.43 & $\pm$1.21 & 71.19 & $\pm$1.00 & 71.64 & $\pm$1.00 & 65.59 & $\pm$0.34 & 73.09 & $\pm$0.12 \\
creditg              & 79.32 & $\pm$0.10 & 80.54 & $\pm$0.06 & 78.36 & $\pm$0.03 & 75.21 & $\pm$0.10 & 79.76 & $\pm$0.06 & 76.81 & $\pm$0.02 & 79.40 & $\pm$0.10 & 56.60 & $\pm$0.16 & 79.83 & $\pm$0.08 \\
diabetes             & 83.70 & $\pm$0.03 & 82.55 & $\pm$0.03 & 82.34 & $\pm$0.02 & 82.89 & $\pm$0.02 & 81.65 & $\pm$0.03 & 79.17 & $\pm$0.03 & 82.72 & $\pm$0.07 & 82.34 & $\pm$0.08 & 82.18 & $\pm$0.02 \\
heart                & 93.29 & $\pm$0.01 & 92.61 & $\pm$0.01 & 92.00 & $\pm$0.01 & 90.74 & $\pm$0.02 & 91.92 & $\pm$0.01 & 91.16 & $\pm$0.01 & 92.16 & $\pm$0.02 & 91.81 & $\pm$0.02 & 92.61 & $\pm$0.01 \\
kr-vs-kp             & 99.43 & $\pm$0.15 & 99.95 & $\pm$0.50 & 99.77 & $\pm$0.20 & 99.15 & $\pm$1.46 & 99.86 & $\pm$0.35 & 99.95 & $\pm$0.30 & 99.30 & $\pm$0.71 & 86.79 & $\pm$0.88 & 99.41 & $\pm$0.97 \\
mfeat-fourier        & 99.94 & $\pm$1.10 & 99.97 & $\pm$1.42 & 99.62 & $\pm$1.70 & 100.00 & $\pm$1.50 & 99.99 & $\pm$1.05 & 99.70 & $\pm$1.05 & 99.99 & $\pm$1.75 & 99.92 & $\pm$1.74 & 100.00 & $\pm$1.06 \\
pc3                  & 82.82 & $\pm$1.24 & 82.48 & $\pm$0.65 & 80.80 & $\pm$2.71 & 79.44 & $\pm$1.79 & 80.89 & $\pm$3.37 & 77.76 & $\pm$3.32 & 79.02 & $\pm$4.20 & 76.57 & $\pm$3.42 & 81.00 & $\pm$0.45 \\
income               & 92.59 & $\pm$0.13 & 92.44 & $\pm$0.04 & 91.75 & $\pm$0.03 & 79.03 & $\pm$0.09 & 89.19 & $\pm$0.02 & 92.35 & $\pm$0.10 & 89.63 & $\pm$0.08 & 70.57 & $\pm$0.10 & 90.30 & $\pm$0.05 \\
texture              & 100.00 & $\pm$0.44 & 99.98 & $\pm$0.27 & 99.93 & $\pm$0.30 & 99.87 & $\pm$0.53 & 99.94 & $\pm$0.26 & 99.96 & $\pm$0.27 & 99.98 & $\pm$1.16 & 99.94 & $\pm$1.07 & 99.94 & $\pm$0.70 \\
balance-scale        & 99.10 & $\pm$0.03 & 92.35 & $\pm$0.02 & 98.37 & $\pm$0.00 & 93.11 & $\pm$0.02 & 84.89 & $\pm$0.03 & 98.99 & $\pm$0.00 & 91.60 & $\pm$0.21 & 91.03 & $\pm$0.18 & 94.41 & $\pm$0.01 \\
mfeat-karhunen       & 99.88 & $\pm$0.56 & 99.86 & $\pm$0.44 & 99.79 & $\pm$0.47 & 99.52 & $\pm$1.43 & 99.71 & $\pm$0.40 & 98.69 & $\pm$0.32 & 99.56 & $\pm$1.22 & 98.85 & $\pm$1.19 & 99.88 & $\pm$0.38 \\
mfeat-morphological  & 96.99 & $\pm$0.37 & 96.20 & $\pm$0.42 & 96.01 & $\pm$0.30 & 95.74 & $\pm$1.25 & 95.53 & $\pm$0.31 & 96.12 & $\pm$0.30 & 95.75 & $\pm$1.01 & 96.33 & $\pm$1.01 & 96.34 & $\pm$0.33 \\
mfeat-zernike        & 98.09 & $\pm$0.42 & 97.59 & $\pm$0.35 & 97.16 & $\pm$0.30 & 97.74 & $\pm$0.51 & 96.72 & $\pm$0.30 & 97.35 & $\pm$0.34 & 98.02 & $\pm$0.65 & 97.76 & $\pm$0.68 & 97.49 & $\pm$0.53 \\
cmc                  & 74.45 & $\pm$1.22 & 72.56 & $\pm$1.26 & 72.89 & $\pm$1.62 & 70.41 & $\pm$1.44 & 70.52 & $\pm$1.10 & 73.00 & $\pm$1.73 & 69.96 & $\pm$1.66 & 71.56 & $\pm$1.64 & 73.88 & $\pm$1.10 \\
tic-tac-toe          & 99.85 & $\pm$0.01 & 99.92 & $\pm$0.01 & 99.81 & $\pm$0.00 & 72.00 & $\pm$0.06 & 96.12 & $\pm$0.01 & 99.98 & $\pm$0.01 & 70.90 & $\pm$0.07 & 72.76 & $\pm$0.09 & 98.82 & $\pm$0.03 \\
vehicle              & 94.50 & $\pm$0.04 & 93.02 & $\pm$0.08 & 92.33 & $\pm$1.07 & 88.79 & $\pm$0.11 & 93.23 & $\pm$0.07 & 92.84 & $\pm$0.06 & 93.19 & $\pm$0.18 & 90.50 & $\pm$0.21 & 91.61 & $\pm$0.13 \\
eucalyptus           & 91.95 & $\pm$0.55 & 88.59 & $\pm$0.73 & 89.31 & $\pm$0.54 & 87.45 & $\pm$1.30 & 90.11 & $\pm$0.82 & 90.04 & $\pm$1.12 & 88.27 & $\pm$1.43 & 89.98 & $\pm$1.71 & 89.70 & $\pm$1.04 \\
analcatdata\_author  & 58.96 & $\pm$0.30 & 55.89 & $\pm$0.41 & 54.61 & $\pm$0.37 & 53.56 & $\pm$0.55 & 53.20 & $\pm$0.40 & 57.43 & $\pm$0.35 & 53.63 & $\pm$0.50 & 53.94 & $\pm$0.62 & 55.50 & $\pm$0.36 \\
MiceProtein          & 99.98 & $\pm$0.01 & 99.99 & $\pm$0.00 & 99.97 & $\pm$0.02 & 99.51 & $\pm$0.02 & 99.85 & $\pm$0.00 & 99.98 & $\pm$0.01 & 99.91 & $\pm$0.04 & 99.41 & $\pm$0.10 & 99.97 & $\pm$0.01 \\
steel-plates-fault   & 94.52 & $\pm$0.05 & 96.51 & $\pm$0.06 & 96.26 & $\pm$0.04 & 91.35 & $\pm$0.04 & 91.71 & $\pm$0.06 & 96.56 & $\pm$0.03 & 91.91 & $\pm$0.12 & 91.92 & $\pm$0.10 & 94.45 & $\pm$0.06 \\
dress-sales          & 57.89 & $\pm$0.58 & 56.96 & $\pm$0.56 & 55.93 & $\pm$0.47 & 55.94 & $\pm$1.29 & 53.72 & $\pm$0.55 & 57.23 & $\pm$0.56 & 53.38 & $\pm$1.25 & 54.41 & $\pm$1.17 & 52.62 & $\pm$0.49 \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{Handling of Textual Only tables}
An important challenge in multi-modal tabular deep learning arises when the input dataset contains only categorical columns as features.
This renders the graph transformation pipeline ineffective in modelling structure of the underlying table. 
\tabglm\ addresses this by only adopting the text transformation pipeline (pretrained encoder in TAPAS alongside the classifier layers) in such situations significantly boosting performance.
This strategy is also adopted if there exists columns in the table with unique entries greater than 60\% of the number of rows in the table. A comparison between the full architecture of \tabglm\ and the text-only (adopting only the text transformation pipeline) architecture in \tabglm\ for the \textbf{car} dataset (only dataset in the benchmark with text only columns) has been given in Table \ref{tab:text_only}.

\section{Limitations}
\label{app:limitations}
Although \tabglm\ demonstrates significant developments over SoTA approaches demonstrating feature heterogeneity, but it also demonstrates some limitations. At first, the maximum number of columns that an input table can have is capped by the token limit of the underlying LLM requiring us to adopt larger networks with large memory and compute footprints. Secondly, the design of the graph transformation pipeline uses the column indexes and the statistical relationships between columns to learn the structure of the graph but does not consider the column name during node creation. These limitations of \tabglm\ would be handled in future research.
