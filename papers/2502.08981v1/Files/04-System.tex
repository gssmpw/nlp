
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\linewidth]{Figures/SystemInSitu_CameraReady.pdf}
    \caption{\Insitu user interface of \SystemName. \textsf{(A)} Users can scan the environment to obtain a \textit{Coarse Mesh} of the surroundings; \textsf{(B)} Users can tap \textsf{Capture} to take a \textit{3D Snapshot}; \textsf{(C)} Users can tap \textsf{Capture} or \textsf{Annotate} to access feature sub-menus from the main menu and tap and hold on the screen to spawn a \textit{3D Cursor}; \textsf{(D)} Users can create drawings projected onto surfaces with \textit{Surface Draw}; \textsf{(E)} Users can create a trajectory or a 3D drawing with \textit{Air Draw} by moving their smartphone.}
    \label{fig:insitu-system-overview}
    %
\end{figure*}

\section{\SystemName System}\label{sec:system}
Based on our formative study, we developed \SystemName, a collaborative authoring system for outdoor AR experiences, designed to facilitate real-time interaction between \exsitu developers and \insitu collaborators. 
The system enables \exsitu creators, who typically design and develop the experience within Unity~\cite{unity} asynchronously, to synchronously collaborate with \insitu users who experience the AR content directly in the field. By integrating real-time communication, contextual reference tools, and spatial data capture, \SystemName aims to reduce the need for repeated on-site visits during the iterative design of site-specific AR experiences. While \SystemName supports multiple \exsitu and \insitu users, in this section and our user study, we focus on a two-user scenario in which a single \exsitu user collaborates with one \insitu partner.

\SystemName primarily utilizes two frameworks: the \textit{Niantic SDK for Unity}~\cite{lightship-ardk-niantic}, identified in the formative study as the most widely used outdoor AR development kit, and \textit{Ubiq}~\cite{fristonUbiqSystemBuild2021}, an open-source library for collaborative mixed reality. The \textit{Niantic SDK for Unity} provides VPS localization, depth estimation, meshing, and occlusion handling, whereas \textit{Ubiq} facilitates real-time networking, avatar representation, and peer-to-peer communication, including WebRTC-based audio and video streaming.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Figures/SystemExSitu_CameraReady.pdf}
    \caption{\Exsitu user interface of \SystemName. \textsf{(A)} All objects under \texttt{NetworkedScene} are automatically synchronized between \exsitu and \insitu users; \textsf{(B)} \textit{3D Snapshots} captured by the \insitu user; \textsf{(C)} The \locMesh of \locA; \textsf{(D)} \textit{Coarse Mesh} captured by the \insitu user; \textsf{(E)} Live feed of the \insitu user's screen, including AR content; \textsf{(F)} The \textit{3D Cursor} of the \exsitu user, projected into world space; \textsf{(G)} Close-up of the \textit{3D Cursor} of the \exsitu user as seen in the scene view; \textsf{(H)} List of annotations and spatial captures, persistently saved in the scene for later review; \textsf{(I)} Sample assets that can be added to the scene at runtime.}
    \label{fig:exsitu-system-overview}
    %
\end{figure*}

\subsection{System Requirements}\label{sec:system:requirements}
Based on challenges reported by participants in our formative study (\cref{sec:formative-study}) and previous work, we identified five requirements for the development of \SystemName.

\paragraph{\normalfont\textbf{R1: Reduce the development and testing iteration loop caused by frequent on-site visits}} Our formative study revealed that participants required frequent on-site visits to gather real-world context and feedback. \SystemName aims to reduce the travel burden by allowing \exsitu users to collaborate with \insitu partners synchronously, enabling adjustments to be made remotely based on immediate feedback and live validation.

\paragraph{\normalfont\textbf{R2: Support incorporation of additional or updated real-world spatial information.}} Interviewees highlighted the limitations of static meshes, which are often outdated or incomplete, leading to alignment issues. \SystemName aims to address this by enabling \insitu users to capture and relay real-time updates about the site, such as changes in the environment.

\paragraph{\normalfont\textbf{R3: Facilitate the capture of real-world context beyond spatial data.}} Participants emphasized the importance of capturing additional real-world context, including lighting conditions, pedestrian flow, and safety concerns.
\SystemName incorporates tools that enable \insitu users to capture this contextual information through annotations, real-time audio, snapshots, and a live video feed.

\paragraph{\normalfont\textbf{R4: Enable live persistent scene adjustments and additions during testing and prototyping.}} Participants described having to return to their workplace to make adjustments after \insitu testing as challenging. To mitigate this, \SystemName supports live modifications during testing sessions, allowing developers to make immediate adjustments based on \insitu feedback.

\paragraph{\normalfont\textbf{R5: Integrate captured data into existing development workflows.}} Participants reported difficulties in incorporating \insitu feedback into the virtual scene representations within their development tools, particularly in Unity. \SystemName addresses this by integrating captured spatial data and annotations directly into Unity and anchored to the \locMesh, enabling developers to incorporate feedback without manual import procedures.

\subsection{Scene View and Localization}
\SystemName enables both users to view the same AR content from their respective familiar perspectives. \Insitu users can interact with the AR experience and a \SystemName UI layer within their real-world environment via a smartphone (\cref{fig:insitu-system-overview}), whereas \exsitu users observe the scene remotely through Unity's scene editor (\cref{fig:exsitu-system-overview}).

At launch, \insitu users complete a step-by-step VPS-based localization process to align the AR experience with their surroundings. The system continuously localizes, prompting the \insitu user to confirm alignment by overlaying a semi-transparent pre-captured \locMesh onto the real world. The \insitu user can adjust its transparency to verify overlap before finalizing localization. Once confirmed, the full scene, including the \insitu user's avatar, loads into the \exsitu user's scene view. The \exsitu user can similarly adjust the \locMesh transparency to better view AR elements, spatial captures, and annotations throughout the collaboration process (\cref{fig:exsitu-system-overview}J). If needed, localization can be restarted via the settings panel to correct for drift that may occur during collaboration.

\subsection{Real-Time In-Situ View Streaming and Audio Communication}

\SystemName supports real-time peer-to-peer video and audio communication to facilitate collaboration between users. A peer-to-peer audio channel enables verbal communication. Additionally, a live feed of the \insitu user's screen is streamed to the \exsitu user, who can view this feed in a floating panel within the Unity scene view (\cref{fig:exsitu-system-overview}E). This feed presents both the real-world camera view and the rendered AR elements, enabling the \exsitu user to understand the interaction between AR content and the physical environment.

\Exsitu users can also capture screenshots of the \insitu user's screen, which are saved and displayed in the Unity scene and anchored to the \locMesh (\cref{fig:exsitu-system-overview}K). These captures align with the \insitu user's perspective and can mark specific moments, errors, or areas of interest for later review.

\subsection{Spatial References with 3D Cursors}

To support spatial referencing during the collaborative process, \SystemName incorporates networked \textit{3D Cursors} that each user can control. These cursors are projected into shared world space based on screen inputs. \Insitu users can tap and hold anywhere on their screen to spawn and control a cursor in the 3D environment, while \exsitu users can hover over the live \insitu view panel within Unity to achieve the same. Cursors appear as semi-transparent spheres with axis indicators, aiding spatial referencing. Green represents \insitu cursors, while blue represents \exsitu cursors (see \cref{fig:exsitu-system-overview}F).

Both users can create persistent cursor markers: \exsitu users by clicking within the Unity editor and \insitu users by double-tapping on their screen. This functionality supports alignment tasks and spatial referencing during collaboration.

\subsection{Auto-Networked Objects and Scene Updates}

A core feature of \SystemName is the automatic synchronization of all scene changes (\cref{fig:exsitu-system-overview}A). Any modifications made by the \exsitu user in Unity---such as adjusting the position, rotation, or properties of AR objects---are immediately reflected in the \insitu user's view. This functionality extends Ubiq's networking capabilities to transmit network messages whenever changes in object transforms, materials, or user-controllable serializable script parameters are detected. \SystemName persistently saves all modifications within the Unity scene file, enabling continued iteration and version control after the collaborative session concludes.

\subsection{Spatial Context Capture with Coarse Meshes and 3D Snapshots}
\Insitu users have two tools for capturing additional spatial context to supplement the \exsitu user's representation of the real world: \textit{3D Snapshots}, which are point clouds based on world-projected RGB-D frames, and \textit{Coarse Meshes}, which are untextured 3D meshes.

The \textit{3D Snapshot} feature utilizes the depth submodule of \textit{Niantic SDK} to capture a depth image alongside the current camera frame from a LiDAR-enabled iPhone model. However, we note that \textit{Niantic SDK} also supports 3D data capture on other devices and can extend beyond the LiDAR sensor's range. Following this, the obtained depth data is combined with RGB information to generate a point cloud, which is transmitted to the \exsitu user's Unity scene. There, it is processed and visualized as a colored world-space point cloud anchored to the \locMesh. For \insitu users, the point cloud appears in white and is semi-transparent to provide feedback without distracting from the overall experience. This feature is particularly useful for capturing smaller objects or areas requiring finer detail, such as user interfaces or situated virtual objects that demand precise positioning. 

For larger-scale spatial data capture, \SystemName provides a coarse 3D mesh capturing feature, extending the \textit{Niantic SDK's} meshing submodule. Unlike \textit{3D Snapshots}, the \textit{Coarse Mesh} feature captures the environment's geometry without color information. This method is well-suited for representing larger structures, such as buildings, or open areas, like streets, where surface texture may be less relevant. Mesh updates are streamed to the \exsitu user's Unity editor in real time, with network message coalescing applied to reduce network load. The meshing process automatically terminates after 15 seconds to prevent excessive network usage but can be restarted at any time. The colored mesh material represents surface normals to enhance shape visualization.

Both \textit{3D Snapshots} and \textit{Coarse Meshes} are persistently saved within the Unity scene for future reference, organized into separate timestamped objects for each collaborative session (\cref{fig:exsitu-system-overview}H).

\subsection{Annotation of the Real World with Surface Draw and Air Draw}

\SystemName also enables \insitu users to provide contextual feedback through annotations. The system supports two types of annotations: \textit{Surface Draw} and \textit{Air Draw}. \textit{Surface Draw} projects the \insitu user's sketches onto real-world surfaces using depth estimates, while \textit{Air Draw} allows users to draw freely in 3D space by moving their smartphone through the environment.

Annotations are color-coded and can be tagged with predefined labels, such as \textit{hazard} or \textit{user flow}, or with custom labels which are automatically assigned distinct colors for easy identification. All annotations are streamed to the \exsitu user's Unity editor in real time, where they appear as world-space objects and are saved for future iterations. Like spatial captures, annotations are persistently stored in the Unity scene, allowing contextual information to be revisited and informing subsequent development steps (\cref{fig:exsitu-system-overview}H).
