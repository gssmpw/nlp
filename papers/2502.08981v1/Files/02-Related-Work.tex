\section{Related Work}
This section discusses related work on AR authoring tools, focusing on \insitu and \exsitu methods, collaborative workflows, and techniques for integrating dynamic environmental context.

\subsection{Leveraging Real-World Context in AR Experience Authoring}
AR authoring tools span a wide spectrum of fidelity levels, supporting different development stages and expertise levels. As categorized by \citet{nebelingTroubleAugmentedReality2018}, these range from low-fidelity prototyping tools that require little to no technical expertise~\cite{adobeAero,nebelingProtoARRapidPhysicalDigital2018,rajaramPaperTrailImmersive2022} to high-fidelity authoring tools, such as Unity~\cite{unity} and Unreal Engine~\cite{unreal}, which are commonly used for developing production-ready applications.

Despite their utility, existing AR authoring tools have challenges that limit their effectiveness. Several surveys and studies have identified key issues, including difficulties in spatial navigation, adapting to changing environmental contexts, and integrating context-specific considerations into AR experiences~\cite{kraussCurrentPracticesChallenges2021, ashtariCreatingAugmentedVirtual2020, kraussElementsXRPrototyping2022, nebelingXRToolsWhere2022}. A central limitation, as noted by \citet{nebelingXRToolsWhere2022}, is the insufficient incorporation of real-world context during authoring, which can disrupt user experience and immersion. To incorporate this context, authoring tools generally adopt one of two approaches: \textit{\insitu} authoring, which supports real-time creation in the target environment, or \textit{\exsitu} authoring, which relies on proxy representations or recordings of the target environment.

\subsubsection{\Insitu authoring}
Tools that incorporate \insitu interfaces, such as the mobile applications of Adobe Aero~\cite{adobeAero}, Reality Composer~\cite{realityComposer}, and Unity Mars~\cite{unityMars} and other prior work~\cite{zollmannComprehensibleInteractiveVisualizations2012,langlotzSketchingWorldSitu2012}, build on the \textit{WYXIWYG (What You Experience Is What You Get)} paradigm introduced by \citet{leeImmersiveAuthoringWhat2005}, which emphasizes the benefits of allowing users to experience and verify authored content \insitu[ ], such as the ability for immediate testing and adjustment. 

In a user study, \citet{leeImmersiveAuthoringOfTangible2009} found that their \insitu system allowed for efficient and precise arrangement of virtual content in real-world settings. However, they noted a key limitation: abstract tasks, such as programming behaviors, were better supported by traditional \exsitu desktop environments. \citet{langlotzSketchingWorldSitu2012} developed a mobile AR authoring system with a focus on enabling spontaneous content creation in unprepared environments, including outdoor settings. While highlighting the low barriers to entry \insitu authoring provides, they also emphasized the importance of integrating it with \exsitu desktop-based interfaces for refining the content. \citet{vargasgonzalezComparisonDesktopAugmented2019} compared \insitu AR and \exsitu desktop authoring tools in scenario-based training contexts and similarly found that while both approaches offered comparable usability and task completion times, desktop tools were perceived as more efficient, particularly for tasks requiring global context.

\subsubsection{\Exsitu authoring}
\Exsitu authoring tools, such as \textit{DART}~\cite{macintyreDARTToolkitRapid2004}, \textit{ScalAR}~\cite{qianScalARAuthoringSemantically2022}, \textit{DistanciAR}~\cite{wangDistanciARAuthoringSiteSpecific2021}, and \textit{Corsican Twin}~\cite{prouzeauCorsicanTwinAuthoring2020}, offer broader authoring capabilities than \insitu tools by providing 3D scene editors, scripting environments, flexible testing workflows, and asset integration. These tools represent environmental context using various forms of pre-captured data, including 3D models~\cite{prouzeauCorsicanTwinAuthoring2020, qianScalARAuthoringSemantically2022, cavalloCAVEARVRAuthoring2019, lightship-ardk-niantic, googleGeospatialCreator}, video~\cite{macintyreDARTToolkitRapid2004, leivaRapidoPrototypingInteractive2021}, sensor data~\cite{macintyreDARTToolkitRapid2004}, or $360\degree$ footage~\cite{nebelingProtoARRapidPhysicalDigital2018}. For example, \textit{DART}~\cite{macintyreDARTToolkitRapid2004} facilitates recording and playback for authoring by synchronizing real-time video and sensor data capture, while \textit{DistanciAR}~\cite{wangDistanciARAuthoringSiteSpecific2021} supports \insitu 3D reconstruction to capture environmental context and enables \exsitu authoring and testing through different model visualization modes. However, a key limitation of these approaches is their reliance on static environmental representations. These models capture only a fixed moment in time, falling short in accounting for dynamic changes. As a result, authoring errors and spatial misalignment may occur.

Recent \exsitu authoring tools for outdoor AR experiences, such as \textit{Niantic Remote Content Authoring}~\cite{lightship-ardk-niantic} and \textit{Google Geospatial Creator}~\cite{googleGeospatialCreator}, adopt similar approaches by providing 3D models of large outdoor sites. However, outdoor environments present even greater challenges due to a higher degree of dynamic change. Unlike indoor spaces, outdoor settings are continuously affected by shifting lighting, weather, seasons, and moving elements like people and vehicles. These dynamic factors significantly impact the accuracy and relevance of pre-captured data, making static environmental representations particularly problematic for authoring outdoor AR content. While \insitu testing can help mitigate some challenges, it is often constrained by logistical factors such as time, cost, and accessibility. Moreover, \insitu authoring of outdoor AR experiences is complicated by the scale and complexity of outdoor spaces~\cite{imottesjoIterativePrototypingUrban2020, numanOutdoorCollaborativeMixed2023, caoMobileAugmentedReality2023}, making \insitu adjustments more difficult to execute.

Overall, our analysis indicates that current AR authoring tools remain limited in bridging the gap between \exsitu flexibility and \insitu contextual awareness. \Exsitu tools rely on static proxies that fail to capture the dynamic nature of outdoor spaces, while \insitu tools, despite offering real-time context, lack the expressiveness and flexibility required for comprehensive authoring, as we found in our formative study (\cref{sec:formative-study}). This disconnect forces authors to switch between \exsitu and \insitu development, leading to inefficiencies and challenges in maintaining contextual accuracy. To address this gap, we propose \SystemName, a system that leverages the flexibility of \exsitu authoring while incorporating real-time environmental awareness from \insitu users. By enabling a synchronous collaborative workflow between \exsitu and \insitu roles, our approach supports contextually aware authoring without compromising the richness and advanced capabilities of \exsitu tools.

\subsection{Collaborative Approaches to AR Experience Authoring}
Given the complex and multidisciplinary nature of AR experience development, collaboration is integral to the authoring process, as noted by \citet{kraussCurrentPracticesChallenges2021}. Specifically, AR application development typically involves multiple roles, such as interaction designers, content creators, and developers, each contributing distinct expertise, as well as non-technical collaborators like clients or end-users, who can help shape the design and content~\cite{kraussElementsXRPrototyping2022}. Inspired by role-based collaboration, \citet{nebelingXRDirectorRoleBasedCollaborative2020} introduced \textit{XRDirector}, a system that enables users to manipulate virtual objects from different subjective viewpoints in AR and VR. Designed for indoor immersive storytelling, \textit{XRDirector} enabled multiple contributors to author experiences from perspectives aligned with their respective roles.

Beyond role-based collaboration, prior work has also examined different modes of collaboration when authoring AR experiences. \citet{guoBlocksCollaborativePersistent2019} explored synchronous and asynchronous collaboration in AR authoring through their system, \textit{Blocks}. Their findings indicated that synchronous collaboration fostered higher engagement, particularly when users collaboratively created shared AR structures in real time. Conversely, asynchronous collaboration, while offering greater flexibility, introduced challenges related to maintaining awareness of others' contributions.

While prior systems demonstrate different approaches to collaborative authoring, they do not adequately address the challenges of site-specific AR development, particularly in outdoor environments where \exsitu authors must consider \insitu perspectives. In designing \SystemName, we sought to support this workflow by facilitating collaboration between \exsitu developers and \insitu collaborators without requiring specialized technical skills beyond familiarity with handheld AR. Our system leverages subjective views, similar to \textit{XRDirector}, but applies them to gathering \insitu context and feedback in outdoor, site-specific authoring. Additionally, we compare synchronous and asynchronous workflows, as examined in \textit{Blocks}, including specific hypotheses on engagement and task load (\cref{sec:user-study}).

\subsection{Environmental Context Awareness in Collaborative Mixed Reality}
Effective integration of real-world context is crucial for collaborative AR authoring, particularly when bridging \exsitu and \insitu roles. 
While previous work has explored a wide range of methods to represent remote environments, most focused on indoor settings or static outdoor spaces, leaving the dynamic nature of outdoor environments largely unaddressed.

Early systems such as \textit{MARS}~\cite{hollererExploringMARSDeveloping1999} and \textit{Tinmith-Metro}~\cite{piekarskiTinmithMetroNewOutdoor2001} enabled interaction with virtual models of physical spaces in both indoor and outdoor contexts. However, they lacked visual fidelity, real-time updates, and dynamic environmental data, which limited their effectiveness for site-specific AR authoring.

To enhance environmental awareness, more recent approaches utilize higher-fidelity representations such as $360\degree$-video for immersive panoramic views~\cite{kasaharaJackInHeadImmersive2015,speicher360AnywhereMobileAdhoc2018,teo360DropsMixedReality2019}. For example, \textit{360Anywhere}~\cite{speicher360AnywhereMobileAdhoc2018} supports multi-user collaboration with gaze awareness and annotation tools. While offering comprehensive visual coverage, these methods often lack depth information and do not fully capture the spatial complexity of outdoor environments.

Volumetric representations, including static photogrammetry-based 3D reconstructions and fused RGB-D streams, offer richer spatial representations. Photogrammetry-based methods generate detailed 3D models from images~\cite{gaoRealtimeVisualRepresentations2018,numanExploringUserBehaviour2022, tianUsingVirtualReplicas2023}, though their creation is time-consuming and not suitable for capturing dynamic changes. Systems employing fused RGB-D data, such as \textit{Remixed Reality}~\cite{lindlbauerRemixedRealityManipulating2018} and \textit{TransceiVR}~\cite{thoravikumaravelTransceiVRBridgingAsymmetrical2020}, integrate real-time color and depth information for interactive and dynamic 3D scene representations. However, these approaches commonly rely on specific sensors, system configurations, and controlled settings, which can limit their application in outdoor environments.

Combining multiple spatial representation techniques has proven effective in enhancing environmental awareness in collaborative settings~\cite{teo360DropsMixedReality2019,thoravikumaravelLokiFacilitatingRemote2019,youngMobileportationNomadicTelepresence2020,sakashitaSharedNeRFLeveragingPhotorealistic2024,tianUsingVirtualReplicas2023,reynoldsPopMetaverseMultiUserMultiTasking2024,luRevivingEustonArch2023}. For example, \citet{thoravikumaravelLokiFacilitatingRemote2019} introduced \textit{Loki}, integrating live video feeds, 3D models, and spatial annotations for remote instruction in indoor controlled environments. Focused on representing outdoor environments, \citet{duGeolleryMixedReality2019} projected \textit{Google Street View} imagery onto building geometries derived from \textit{OpenStreetMap} to place and view geotagged information in \textit{Geollery}. While effective within their respective contexts, these systems generally do not account for the challenges posed by highly dynamic, large-scale outdoor environments.

Building on prior work, we aim to address these challenges by enhancing environmental awareness in outdoor environments through \SystemName. We integrate multiple forms of spatial capture methods to supplement pre-captured 3D meshes of outdoor locations used for authoring site-specific AR content. Specifically, \SystemName enables \insitu users to capture single-frame RGB-D data for detailed short-range capture and coarse 3D meshes for broader geometric context. This approach provides \exsitu authors with up-to-date, targeted, real-world spatial context to incorporate into their authoring process.

While several individual components of our proposed system---such as RGB-D capture, mesh reconstruction, and annotation tools---have been explored in prior research, their combined application in highly dynamic, large, and diverse outdoor environments within an AR authoring context represents a novel exploration. By integrating these spatial capture methods into a collaborative workflow, \SystemName aims to bridge the gap between \exsitu and \insitu roles, enhancing environmental awareness and supporting effective real-time collaboration in developing site-specific AR experiences.
