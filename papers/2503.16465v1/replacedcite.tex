\section{Related Works}
Our work falls into the field of MLLM-powered agents. This section will first review the recent progress in building GUI agents and then discuss the capability probing approaches for GUI agents.

\subsection{MLLM-powered GUI Agents}
The rise of MLLMs has redefined the paradigm for GUI agents, enabling them to analyze complex screen layouts and generate accurate actions in a more human-like way____. Importantly, this paradigm is a non-intrusive manner without reliance on complex, platform-specific scripts or predefined workflows. Notable examples across different platforms include SeeAct____ and WebRL____ for web navigation, AppAgent____, Auto-UI____, and CoCoAgent____ for mobile interactions, and ScreenAgent____ for Windows OS applications. This paper investigates the over-execution of MLLM-powered GUI agents on mobile devices.

Early efforts to build GUI agents rely on the availability of commercial MLLMs. These agents can be built through prompt learning based on $\mathtt{GPT}$-$\mathtt{4o}$ or Gemini-Pro Vision, e.g., AppAgent____ and Mobile-Agent____. However, practitioners are concerned about the costs associated with API requests and the delays in inference on mobile devices. Recent studies have focused on fine-tuning to optimize foundation models. On the one hand, they work on performing fine-grained visual understanding____, model scaling laws____, multimodal information integration____, and GUI grounding enhancements____ in the pre-training phase. On the other hand, researchers fine-tune the foundation model on GUI-specific datasets to enhance action orientation____, planning decision____, perception enhancement____, and reasoning____. Moreover, a framework based on reinforcement learning (RL) designed specifically for the GUI agents can further enhance robustness____.

% However, due to challenges related to practicality and reliability, existing GUI automation encounter performance bottlenecks in complex scenarios (Figure~\ref{fig2}), such as interference from the external environment and ambiguous instructions. 
Despite the progress, existing GUI agents encounter performance bottlenecks in complex scenarios (Figure~\ref{fig2}), such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks.
____ proposed Meta-GUI that leverages precise guidance through task-oriented dialogue. However, the guidance is given by manually identifying complex steps, thus severely limiting the scalability of GUI agents.

\subsection{Capability Probing for GUI Agent}
GUI agent-oriented capability probing is critical for real-world applications____. Generally, the capability of GUI agents can be probed by releasing benchmark datasets. Examples like UIBert____, SeeClick____, and OS-Copilot____, which investigate the problem of grounding understanding to UI elements on a screen. Besides, large-scale, diverse, and high-quality trajectory datasets can identify challenges of action prediction in terms of effectiveness (e.g., PixelHelp____, Meta-GUI____, and AndroidWorld____), task complexity (e.g., Mobile-Bench____ and GUI Odyssey____), and data-scaling (e.g., AITW____ and AndroidControl____). After identifying the capability bottleneck of GUI agents, the introduction of specific strategies (e.g., planning lists____, action chains____, and supplementary data) further enhance the environment perception. However, most benchmark datasets rely on crowdsourcing and human annotation.  

Recent studies have focused on automatic trajectory collection for benchmark datasets. For example, ____ introduces a two-stage RL framework that explores successful trajectories during optimization. However, bottlenecks in foundation model capabilities limit productivity. ____ further proposed OS-Genesis, which back-generates instructions through UI element traversal and ensures the generated high-quality trajectory based on a reward model. However, environment emulators (e.g., Android Studio Emulator____) do not reflect real-world scenarios.  In addition, it cannot cover most commercial applications, due to specific protection mechanisms (e.g., RedNote). Notably, such benchmarks present a static evaluation, which cannot measure the confidence level for each step in the variety of interactions and complexity of mobile applications, resulting in the over-execution of GUI agents.