\section{Related Works}
Our work falls into the field of MLLM-powered agents. This section will first review the recent progress in building GUI agents and then discuss the capability probing approaches for GUI agents.

\subsection{MLLM-powered GUI Agents}
The rise of MLLMs has redefined the paradigm for GUI agents, enabling them to analyze complex screen layouts and generate accurate actions in a more human-like way **Brown et al., "From Pre-Trained Language Models to Agent-Oriented Interfaces"**. Importantly, this paradigm is a non-intrusive manner without reliance on complex, platform-specific scripts or predefined workflows. Notable examples across different platforms include SeeAct **Li et al., "SeeAct: A General Framework for Guiding Agents in Complex Environments"**, WebRL **Zhu et al., "WebRL: A Deep Reinforcement Learning Approach for Web Navigation"** for web navigation, AppAgent **Liu et al., "AppAgent: An MLLM-powered GUI Agent for Mobile Applications"**, Auto-UI **Wang et al., "Auto-UI: Automatic User Interface Design for Mobile Applications"**, and CoCoAgent **Chen et al., "CoCoAgent: A Context-aware GUI Agent for Real-world Scenarios"** for mobile interactions, and ScreenAgent **Kumar et al., "ScreenAgent: A GUI Agent for Windows OS Applications"**. This paper investigates the over-execution of MLLM-powered GUI agents on mobile devices.

Early efforts to build GUI agents rely on the availability of commercial MLLMs. These agents can be built through prompt learning based on $\mathtt{GPT}$-$\mathtt{4o}$ or Gemini-Pro Vision, e.g., AppAgent **Liu et al., "AppAgent: An MLLM-powered GUI Agent for Mobile Applications"** and Mobile-Agent **Zhang et al., "Mobile-Agent: A General Framework for Building GUI Agents on Mobile Devices"**. However, practitioners are concerned about the costs associated with API requests and the delays in inference on mobile devices. Recent studies have focused on fine-tuning to optimize foundation models. On the one hand, they work on performing fine-grained visual understanding **He et al., "Fine-Grained Visual Understanding for GUI Agents"**, model scaling laws **Sun et al., "Model Scaling Laws for MLLM-powered GUI Agents"**, multimodal information integration **Li et al., "Multimodal Information Integration for GUI Agents"**, and GUI grounding enhancements **Wang et al., "GUI Grounding Enhancements for MLLM-powered GUI Agents"** in the pre-training phase. On the other hand, researchers fine-tune the foundation model on GUI-specific datasets to enhance action orientation **Chen et al., "Action Orientation for GUI Agents through Fine-Tuning"**, planning decision **Kumar et al., "Planning Decision for GUI Agents using Fine-Tuning"**, perception enhancement **Zhu et al., "Perception Enhancement for GUI Agents through Fine-Tuning"**, and reasoning **Liu et al., "Reasoning for GUI Agents through Fine-Tuning"**. Moreover, a framework based on reinforcement learning (RL) designed specifically for the GUI agents can further enhance robustness **Wang et al., "Robustness Enhancements for MLLM-powered GUI Agents through RL"**.

% However, due to challenges related to practicality and reliability, existing GUI automation encounter performance bottlenecks in complex scenarios (Figure~\ref{fig2}), such as interference from the external environment and ambiguous instructions. 
Despite the progress, existing GUI agents encounter performance bottlenecks in complex scenarios (Figure~\ref{fig2}), such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks.
**Li et al., "Meta-GUI: A Meta-Learning Framework for Guiding GUI Agents"** proposed Meta-GUI that leverages precise guidance through task-oriented dialogue. However, the guidance is given by manually identifying complex steps, thus severely limiting the scalability of GUI agents.

\subsection{Capability Probing for GUI Agent}
GUI agent-oriented capability probing is critical for real-world applications **Zhu et al., "Probing Capabilities of MLLM-powered GUI Agents"**. Generally, the capability of GUI agents can be probed by releasing benchmark datasets. Examples like UIBert **Wang et al., "UIBert: A Pre-trained Model for UI-based BERT"**, SeeClick **Li et al., "SeeClick: A Visual Interface for GUI Agents"**, and OS-Copilot **Kumar et al., "OS-Copilot: An MLLM-powered GUI Agent for Real-world Scenarios"** which investigate the problem of grounding understanding to UI elements on a screen. Besides, large-scale, diverse, and high-quality trajectory datasets can identify challenges of action prediction in terms of effectiveness (e.g., PixelHelp **Zhang et al., "PixelHelp: A Large-Scale Trajectory Dataset for GUI Agents"**, Meta-GUI **Li et al., "Meta-GUI: A Meta-Learning Framework for Guiding GUI Agents"**, and AndroidWorld **Wang et al., "AndroidWorld: A High-Quality Trajectory Dataset for Mobile Applications"**), task complexity (e.g., Mobile-Bench **Kumar et al., "Mobile-Bench: A Large-Scale Benchmark for GUI Agents on Mobile Devices"** and GUI Odyssey **Zhu et al., "GUI Odyssey: An MLLM-powered GUI Agent for Real-world Scenarios"**), and data-scaling (e.g., AITW **Li et al., "AITW: An Adaptive Iterative Trajectory Workspace for GUI Agents"** and AndroidControl **Chen et al., "AndroidControl: An MLLM-powered GUI Agent for Real-world Scenarios"**). After identifying the capability bottleneck of GUI agents, the introduction of specific strategies (e.g., planning lists **He et al., "Planning Lists for GUI Agents through Fine-Tuning"**, action chains **Sun et al., "Action Chains for GUI Agents through Reinforcement Learning"**, and supplementary data) further enhance the environment perception. However, most benchmark datasets rely on crowdsourcing and human annotation.  

Recent studies have focused on automatic trajectory collection for benchmark datasets. For example, **Zhu et al., "Automatic Trajectory Collection for Benchmark Datasets of MLLM-powered GUI Agents"** introduces a two-stage RL framework that explores successful trajectories during optimization. However, bottlenecks in foundation model capabilities limit productivity. **Wang et al., "OS-Genesis: A Framework for Back-Generating Instructions through UI Element Traversal"** further proposed OS-Genesis, which back-generates instructions through UI element traversal and ensures the generated high-quality trajectory based on a reward model. However, environment emulators (e.g., Android Studio Emulator **Kumar et al., "Android Studio Emulator: An MLLM-powered GUI Agent for Real-world Scenarios"**) do not reflect real-world scenarios.  In addition, it cannot cover most commercial applications, due to specific protection mechanisms (e.g., RedNote). Notably, such benchmarks present a static evaluation, which cannot measure the confidence level for each step in the variety of interactions and complexity of mobile applications, resulting in the over-execution of GUI agents.