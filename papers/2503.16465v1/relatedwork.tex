\section{Related Works}
Our work falls into the field of MLLM-powered agents. This section will first review the recent progress in building GUI agents and then discuss the capability probing approaches for GUI agents.

\subsection{MLLM-powered GUI Agents}
The rise of MLLMs has redefined the paradigm for GUI agents, enabling them to analyze complex screen layouts and generate accurate actions in a more human-like way~\cite{zhang2024large}. Importantly, this paradigm is a non-intrusive manner without reliance on complex, platform-specific scripts or predefined workflows. Notable examples across different platforms include SeeAct~\cite{zhenggpt} and WebRL~\cite{qi2024webrl} for web navigation, AppAgent~\cite{zhang2023appagent}, Auto-UI~\cite{zhang2023you}, and CoCoAgent~\cite{ma2024comprehensive} for mobile interactions, and ScreenAgent~\cite{niu2024screenagent} for Windows OS applications. This paper investigates the over-execution of MLLM-powered GUI agents on mobile devices.

Early efforts to build GUI agents rely on the availability of commercial MLLMs. These agents can be built through prompt learning based on $\mathtt{GPT}$-$\mathtt{4o}$ or Gemini-Pro Vision, e.g., AppAgent~\cite{zhang2023appagent} and Mobile-Agent~\cite{wang2024mobile}. However, practitioners are concerned about the costs associated with API requests and the delays in inference on mobile devices. Recent studies have focused on fine-tuning to optimize foundation models. On the one hand, they work on performing fine-grained visual understanding~\cite{bai2023qwen}, model scaling laws~\cite{chen2024internvl}, multimodal information integration~\cite{hong2024cogagent}, and GUI grounding enhancements~\cite{wu2024atlas, qin2025ui} in the pre-training phase. On the other hand, researchers fine-tune the foundation model on GUI-specific datasets to enhance action orientation~\cite{wu2024mobilevlm}, planning decision~\cite{zhang2024dynamic}, perception enhancement~\cite{ma2024comprehensive}, and reasoning~\cite{zhang2023you, zhang2024android}. Moreover, a framework based on reinforcement learning (RL) designed specifically for the GUI agents can further enhance robustness~\cite{zhoudigirl, liu2024autoglm, wang2024distrl}.

% However, due to challenges related to practicality and reliability, existing GUI automation encounter performance bottlenecks in complex scenarios (Figure~\ref{fig2}), such as interference from the external environment and ambiguous instructions. 
Despite the progress, existing GUI agents encounter performance bottlenecks in complex scenarios (Figure~\ref{fig2}), such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks.
\citet{sun2022meta} proposed Meta-GUI that leverages precise guidance through task-oriented dialogue. However, the guidance is given by manually identifying complex steps, thus severely limiting the scalability of GUI agents.

\subsection{Capability Probing for GUI Agent}
GUI agent-oriented capability probing is critical for real-world applications~\cite{deka2017rico}. Generally, the capability of GUI agents can be probed by releasing benchmark datasets. Examples like UIBert~\cite{bai2021uibert}, SeeClick~\cite{cheng2024seeclick}, and OS-Copilot~\cite{wu2024atlas}, which investigate the problem of grounding understanding to UI elements on a screen. Besides, large-scale, diverse, and high-quality trajectory datasets can identify challenges of action prediction in terms of effectiveness (e.g., PixelHelp~\cite{li2020mapping}, Meta-GUI~\cite{sun2022meta}, and AndroidWorld~\cite{rawles2024androidinthewild}), task complexity (e.g., Mobile-Bench~\cite{deng2024mobile} and GUI Odyssey~\cite{lu2024gui}), and data-scaling (e.g., AITW~\cite{rawles2024androidinthewild} and AndroidControl~\cite{li2024effects}). After identifying the capability bottleneck of GUI agents, the introduction of specific strategies (e.g., planning lists~\cite{zhang2024dynamic}, action chains~\cite{zhang2023you, zhang2024android}, and supplementary data) further enhance the environment perception. However, most benchmark datasets rely on crowdsourcing and human annotation.  

Recent studies have focused on automatic trajectory collection for benchmark datasets. For example, \citet{zhoudigirl} introduces a two-stage RL framework that explores successful trajectories during optimization. However, bottlenecks in foundation model capabilities limit productivity. \citet{sun2024genesis} further proposed OS-Genesis, which back-generates instructions through UI element traversal and ensures the generated high-quality trajectory based on a reward model. However, environment emulators (e.g., Android Studio Emulator~\cite{deka2017rico}) do not reflect real-world scenarios.  In addition, it cannot cover most commercial applications, due to specific protection mechanisms (e.g., RedNote). Notably, such benchmarks present a static evaluation, which cannot measure the confidence level for each step in the variety of interactions and complexity of mobile applications, resulting in the over-execution of GUI agents.