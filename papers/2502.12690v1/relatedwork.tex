\section{Related Work}
\label{sec:related_work}
The research presented in this paper intersects multiple fields.
First, it is based on the idea of using a computer algorithm to optimize \gls{nn} architectures, commonly known as \gls{nas}.
Second, it considers the deployment of resource-intensive \gls{nn} models on severely resource-constrained embedded devices, a domain known as TinyML.
This has so far been achieved either by handcrafting manual \gls{nn} architectures or via Hardware Aware \glspl{nas}.
Third, it intersects the growing trend of Data-Centric \gls{ai}.

\subsection{Neural Architecture Search}
\Gls{nas} is a widely explored approach for designing high-performance \gls{nn} architectures in traditional computational scales~\cite{zoph2017neural, zoph2018learning, ying2019bench, brock2018smash}.
Early \gls{nas} systems relied on evolutionary algorithms~\cite{real2017large} and reinforcement learning~\cite{zoph2017neural, zoph2018learning} to explore discrete search spaces of potential \gls{nn} architectures.

Recently, supernet-based methods, which train a single large model and extract high-quality subarchitectures from it, have gained popularity as a way to reduce search times~\cite{brock2018smash}.
\Gls{dnas} has also emerged as a promising technique, where a discrete \gls{nn} search space is turned continuous through proxy scalar values, enabling fast searches through gradient-based optimization methods~\cite{liu2018darts}.

\subsection{TinyML Neural Architecture Search}
The application of \gls{nas} has been central in creating models that balance performance and efficiency to run on TinyML hardware since the very beginning of the field.
SpArSe~\cite{fedorov2019sparse}, one of the first works to show that \glspl{cnn} can be deployed on TinyML hardware, uses bayesian-based \gls{nas} to find suitable \gls{cnn} architectures.

More recent works on TinyML \gls{nas} include MCUNet~\cite{lin2020mcunet} and Micronets~\cite{banbury2021micronets}, used to create the MCUNet and Micronet family of models, which are among the most used in TinyML.

The MCUNet \gls{nas} consists of two steps.
During the first step, a search space is created where most models fit tightly into the resource constraints of a TinyML device.
In the second step, a one-shot \gls{nas} is performed on the search space, after which an evolutionary search is used to sample the best sub-network.
The MCUNet \gls{nas} arguably includes data in its search space as the adapted search space for TinyML is created by, among others, reducing the input resolution of the search space~\cite{lin2020mcunet}.

The authors of the Micronets paper use \gls{dnas}~\cite{liu2018darts} based on either a MobileNetv2~\cite{sandler2018mobilenetv2} or a DS-CNN(L)~\cite{zhang2017hello} backbone to design TinyML models for Person Detection, Keyword Spotting, and Anomaly Detection.

Other works on \gls{nas} for TinyML include CNAS, which optimizes \gls{nn} for hardware device operator constraints~\cite{gambella2022cnas} and \textmu{}NAS, which proposes a \gls{nas} with a highly granular search space~\cite{liberis2021munas}.

\subsection{Manually Designed TinyML Architectures}
While \gls{nas} dominates TinyML model development, several works propose manually designed architectures.
For instance, CoopNet is a manually designed model that utilizes heterogeneous quantization and binarization to produce an efficient model architecture~\cite{mocerino2019coopnet}.
Another example is the DS-CNN model, which uses depthwise separable convolution layers to achieve superior performance in keyword spotting applications within TinyML resource constraints~\cite{zhang2017hello}.

\subsection{Data-Centric Artificial Intelligence}
An up-and-coming research area that has traditionally received little attention is Data-Centric \gls{ai}.
Where many research papers propose techniques for optimizing \gls{ml} models, much fewer papers consider ways to improve \gls{ml} through data-centric innovations~\cite{mazumder2023dataperf}.
Techniques such as weak supervision for fast dataset labeling~\cite{ratner2017snorkel}, confident learning for automated label error spotting and correction~\cite{northcutt2021confident}, and active learning where only the most important data samples are labeled~\cite{settles2009active} are key contributions to this domain.

\subsection{Data Aware Neural Architecture Search}
Researchers recently introduced Data Aware \gls{nas}~\cite{njor2023data}, which incorporates data configurations into the search space of a Hardware Aware \gls{nas}.
Using evolutionary algorithms, they jointly optimized data configurations and \gls{cnn} models within a layer-based search space.
Their findings demonstrates that Data Aware \gls{nas} can significantly reduce resource consumption in TinyML systems.

Building on this foundation, this present study conducts a comprehensive comparison between Data Aware- and traditional Hardware-Aware \gls{nas} through experiments on a more complex dataset.
This comparison would not be possible using the prior Data Aware \gls{nas} implementation due to speed limitations.
To overcome this barrier, we contribute a new Data Aware \gls{nas} implementation, built on state-of-the-art \gls{nas} techniques.