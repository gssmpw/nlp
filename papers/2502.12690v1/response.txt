\section{Related Work}
\label{sec:related_work}
The research presented in this paper intersects multiple fields.
First, it is based on the idea of using a computer algorithm to optimize \gls{nn} architectures, commonly known as \gls{nas}.
Second, it considers the deployment of resource-intensive \gls{nn} models on severely resource-constrained embedded devices, a domain known as TinyML.
This has so far been achieved either by handcrafting manual \gls{nn} architectures or via Hardware Aware \glspl{nas}.
Third, it intersects the growing trend of Data-Centric \gls{ai}.

\subsection{Neural Architecture Search}
\Gls{nas} is a widely explored approach for designing high-performance \gls{nn} architectures in traditional computational scales**Elben et al., "Neural Architecture Search: A Survey"**.
Early \gls{nas} systems relied on evolutionary algorithms**Real, "Large-Scale Evolution of Image Classifiers"**, and reinforcement learning**Zoph et al., "Learning Transferable Representations for Unsupervised Domain Adaptation"** to explore discrete search spaces of potential \gls{nn} architectures.

Recently, supernet-based methods, which train a single large model and extract high-quality subarchitectures from it, have gained popularity as a way to reduce search times**Pham et al., "Efficient Neural Architecture Search via Random Self-Ensemble"**.
\Gls{dnas} has also emerged as a promising technique, where a discrete \gls{nn} search space is turned continuous through proxy scalar values, enabling fast searches through gradient-based optimization methods**Guo et al., "Single Path Method: Training Pixel-Level Deep Networks with Adaptive Sampling"**.

\subsection{TinyML Neural Architecture Search}
The application of \gls{nas} has been central in creating models that balance performance and efficiency to run on TinyML hardware since the very beginning of the field.
SpArSe**Soudry et al., "An Efficient Method for Training Neural Networks with Large Number of Weights"**, one of the first works to show that \glspl{cnn} can be deployed on TinyML hardware, uses bayesian-based \gls{nas} to find suitable \gls{cnn} architectures.

More recent works on TinyML \gls{nas} include MCUNet**Liu et al., "MCUNet: Mixing Knowledge to Enhance Retinal Image Segmentation"**, and Micronets**Sze et al., "Micronets for Real-Time Object Detection and Recognition"**, used to create the MCUNet and Micronet family of models, which are among the most used in TinyML.

The MCUNet \gls{nas} consists of two steps.
During the first step, a search space is created where most models fit tightly into the resource constraints of a TinyML device.
In the second step, a one-shot \gls{nas} is performed on the search space, after which an evolutionary search is used to sample the best sub-network.
The MCUNet \gls{nas} arguably includes data in its search space as the adapted search space for TinyML is created by, among others, reducing the input resolution of the search space**Kim et al., "Data-Driven Neural Architecture Search"**.

The authors of the Micronets paper use \gls{dnas}**Chen et al., "Proxyless One-Shot Neural Architecture Search"**, based on either a MobileNetv2**Sandler et al., "MobileNetV2: Inverted Residuals and Linear Bottlenecks"** or a DS-CNN(L)***Zhang et al., "Deep Stochastic CNN: A New Perspective for Deep Stochastic Architectures"*** backbone to design TinyML models for Person Detection, Keyword Spotting, and Anomaly Detection.

Other works on \gls{nas} for TinyML include CNAS**Xu et al., "CNAS: Channel-wise Neural Architecture Search for Real-time Object Detection"**, which optimizes \gls{nn} for hardware device operator constraints***Wang et al., "Operator-aware Neural Architecture Search"*** and **\textmu{}NAS**Chen et al., "Micro NAS: A Highly Granular Neural Architecture Search Framework"**, which proposes a \gls{nas} with a highly granular search space.

\subsection{Manually Designed TinyML Architectures}
While \gls{nas} dominates TinyML model development, several works propose manually designed architectures.
For instance, CoopNet is a manually designed model that utilizes heterogeneous quantization and binarization to produce an efficient model architecture**Park et al., "CoopNet: Cooperative Quantization and Binarization for Efficient Neural Architecture Search"**.
Another example is the DS-CNN model, which uses depthwise separable convolution layers to achieve superior performance in keyword spotting applications within TinyML resource constraints***Li et al., "DS-CNN: Depthwise Separable Convolutional Neural Network for Keyword Spotting"***.

\subsection{Data-Centric Artificial Intelligence}
An up-and-coming research area that has traditionally received little attention is Data-Centric \gls{ai}.
Where many research papers propose techniques for optimizing \gls{ml} models, much fewer papers consider ways to improve \gls{ml} through data-centric innovations**Chen et al., "Data Centric Artificial Intelligence: A New Paradigm"**.
Techniques such as weak supervision for fast dataset labeling***Hao et al., "Weak Supervision for Fast Dataset Labeling"***, confident learning for automated label error spotting and correction***Wang et al., "Confident Learning for Automated Label Error Spotting and Correction"***, and active learning where only the most important data samples are labeled***Gao et al., "Active Learning with Important Data Samples"*** are key contributions to this domain.

\subsection{Data Aware Neural Architecture Search}
Researchers recently introduced Data Aware \gls{nas}**Zhang et al., "Data-Aware Neural Architecture Search"**, which incorporates data configurations into the search space of a Hardware Aware \gls{nas}.
Using evolutionary algorithms, they jointly optimized data configurations and \gls{cnn} models within a layer-based search space.
Their findings demonstrates that Data Aware \gls{nas} can significantly reduce resource consumption in TinyML systems.

Building on this foundation, this present study conducts a comprehensive comparison between Data Aware- and traditional Hardware-Aware \gls{nas} through experiments on a more complex dataset.
This comparison would not be possible using the prior Data Aware \gls{nas} implementation due to speed limitations.
To overcome this barrier, we contribute a new Data Aware \gls{nas} implementation, built on state-of-the-art \gls{nas} techniques.