\section{Related work}
Adversarially robust PAC-learning was formulated to study an empirical phenomenon, first encountered in image classification: state of the art models were vulnerable to adversarial attacks, namely imperceptible perturbations of an input image that led the otherwise highly accurate model to erroneously change its output~\citep{SzegedyZSBEGF13}. Adversarial robustness has since developed into a fertile area of research in the last decade. 
However, developing sound and efficient practical methods as well as obtaining a theoretical understanding of the problem remain challenging.

From a theoretical perspective, the sample complexity of adversarial PAC learning~\citep{feige2015learning, MontasserHS19} has been widely investigated~\citep{feige2015learning, attias2018improved, ashtiani2020black,montasser2021adversarially}. However, the best known upper bounds~\citep{MontasserHS19, montasser2022adversarially} are based on rather involved and impractical learning methods, namely on intricate compression schemes~\citep{moran2016sample} or one-inclusion-graphs.
Also, the known sample complexity bounds are exponential in VC-dimension of the hypothesis class. 
Variations of the problem such as semi-supervised learning~\citep{ashtiani2020black, attias2022characterization} and learning real-valued functions~\citep{attias2023adversarially} have also been studied and similar upper bounds have been derived. It can be easily shown that VC-dimension does not provide a lower bound, since any class with infinite VC-dimension is trivially learnable when
$\U(x)$ is the entire domain $X$. A dimension characterizing adversarially robust PAC-learning was obtained in~\citep{montasser2022adversarially}, but the dimension is based on a global variant of the one-inclusion graph~\citep{haussler1994predicting} 
with potentially infinite vertices and edges. A simpler characterization remains elusive.

A major drawback of the standard PAC framing of adversarial robustness is the fixation on one perturbation type, which realistically cannot be known by the learner. Various alternatives have been investigated, such as robustness that is adaptive to the underlying distribution~\citep{BhattacharjeeC21adaptive} or robustness with respect to a large collection of perturbation sets~\citep{montasser2021adversarially,lechner2024adversarially}. For the latter approach, it was shown that under structural assumptions on the class or perturbation types (such as a linear ordering by inclusion) and access to additional oracles (a perfect attack oracle provides witness points to adversarial vulnerability) adversarially robust learning is still possible.


Adversarial learning with tolerance was introduced in~\cite{ashtiani2023adversarially} and further studied by~\cite{bhattacharjee2023robust, raman2024proper}. A related notion (corresponding to the special case of tolerance parameter $\gamma = 1$ in our terminology) was considered in additional studies~\citep{montasser2021transductive,blum2022boosting}. Another relaxation of the adversarial problem was studied in~\cite{robey2022probabilistically, raman2024proper} where the requirement is robustness to the majority of perturbations in a perturbation set (as opposed to all perturbations in the set).