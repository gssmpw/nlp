\section{Experiment}
\label{sec:experiment}
In this section, we elaborate on the effectiveness of our \textbf{Da}ta- and \textbf{M}odel-\textbf{a}ware Direct Preference Optimization (DAMA). Specifically, we first introduce the details of our experimental settings. Next, we illustrate the ablation studies, and finally, we compare the results with the state-of-the-art methods over various benchmarks.

\subsection{Experimental Settings}
In this section, we describe the experimental settings.
\newline
\noindent \textbf{Backbone}: We employ the LLaVA-1.5 7B and 13B for performance comparison \cite{LLaVA}.
\newline
\noindent \textbf{Dataset}: Our focus is not on the preference data construction, thus we directly utilize the released dataset by \cite{RLAIF-V}, which contains 22k preference data totally.

\noindent \textbf{Baselines.}
In this work, we compare against state-of-the-art baselines across various categories:
\newline
(1) {Hallucination-specific baselines.} In this category, we mainly compare with VCD \cite{VCD}, Less-is-more \cite{Less_Is_More}, OPERA \cite{OPERA}, and CCA-LLaVA \cite{CCA-LLaVA}.
\newline 
(2) {Preference Optimization-based baselines.} In this category, we mainly compare with HA-DPO \cite{HA-DPO}, POVID \cite{POVID}, LLaVA-RLHF \cite{LLaVA-RLHF}, RLHF-V \cite{RLHF-V}, RLAIF-V \cite{RLAIF-V}, AMP-MEG \cite{AMP-MEG}, CSR \cite{CSR}, V-DPO \cite{V-dpo}, and TPO \cite{TPO}.
\newline
(3) {Commercial baseline.} We include GPT-4V as a strong reference to evaluate the performance gap between the open-source and commercial models.

\noindent \textbf{Benchmarks.}
We conduct experiments on five benchmarks, including three hallucination benchmarks reflecting trustworthiness, and two general benchmarks:
\newline
(1) {Object HalBench} is employed to evaluate object hallucination by detailed descriptions of the image content. We report the response-level and mentioned-level \textbf{non}-hallucination rates to evaluate its capability to reduce hallucination \cite{object_hallucination_benchmark}.
\newline
(2) {AMBER} is a multi-dimensional hallucination benchmark, which contains more than 15k samples. We report the Accuracy and F1 metric by its discriminative component \cite{AMBER}.
\newline
(3) {MMHal-Bench} assesses response-level hallucination rate and informativeness by GPT-4 compare model outputs with human responses and object labels \cite{LLaVA-RLHF}.
\newline
(4) {LLaVA Bench} consists of 24 images and 60 questions including conversation, detailed description, and complex reasoning ability \cite{LLaVA}.
\newline
(5) {MM-Vet} is designed to evaluate six integrated competencies, including OCR, recognition, knowledge, language generation,  spatial awareness, and math \cite{MM-vet}.

\noindent \textbf{Implementation Details.}
For both LLaVA-1.5 7B and 13B models, we employ full parameter-tuning over the preference dataset with four epochs. Specifically, for reproducibility, we adopt the same hyperparameters as provided in the official LLaVA GitHub repository \footnote{https://github.com/haotian-liu/LLaVA}. The batch size $N$ is set to $16$, the selected size $K$ is set to $12$, and the penalty hyperparameter $\beta$ is set to $0.1$ by following \cite{DPO, RLAIF-V}. All experiments are conducted with four A100 80GB GPUs, and four epochs of fine-tuning cost seven hours for both backbones.

\subsection{Ablation Studies}
In this section, we evaluate the effects of different components of our DAMA. To this end, we utilize the LLaVA-1.5 7B model as the backbone. For clear illustration, we report both the response and the mentioned-level non-hallucination rate on the Object Hallucination benchmark, where the non-hallucination rate is defined as $100\% -$ hallucination rate.
The following are detailed illustrations.

\noindent \textbf{Influences of different components.}
The performance of various components of DAMA is reported in Table~\ref{tab:ab1-components}, where ``DPO'' refers to Direct Preference Optimization \cite{DPO}, ``MDPO'' represents our Model-aware Preference Optimization, ``D$^{2}$PO'' denotes Data-aware Preference Optimization, and ``DAMA'' corresponds to our combined strategy. The experimental results demonstrate: (1) All strategies significantly outperform the baseline method (DPO), especially our final ``DAMA'', achieving more than $10\%$ response level performance gains, highlighting the effectiveness of our method; (2) Compared with ``MDPO'', the performance gain of ``D$^{2}$PO'' is relatively modest, suggesting that the quality of the preference data is already high, which further validates the efficacy of the preference data construction strategy \cite{RLAIF-V}.


\begin{table}[t]
\caption{Experimental results of different components of DAMA.}
\label{tab:ab1-components}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l | c c }
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Object HalBench}} \\
& Response ($\uparrow$) & Mention ($\uparrow$) \\
\midrule
LLaVA-1.5-7B & 47.75 & 73.08 \\
+DPO & 78.29 & 89.48  \\
\midrule
+D$^{2}$PO & \addvalue{82.54}{4.25}{6.5} & \addvalue{90.64}{1.16}{6.5} \\
+MDPO & \addvalue{88.00}{9.71}{6.5} & \addvalue{93.74}{4.26}{6.5}\\
\rowcolor{lightgray} +\textbf{DAMA} & \addvalue{\textbf{90.87}}{12.58}{8} & \addvalue{\textbf{95.33}}{5.85}{6.5} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\noindent \textbf{Influences of probability transformation in data-aware preference optimization.} Table~\ref{tab:ab3-clip_prob} summarizes the performance of different response inconsistency estimation strategies, where  ``CLIP Scores'' denotes that we directly estimate $\delta$ based on $\mathbf{C}_{w}$ and $\mathbf{C}_{l}$, with $\delta = \sum_{j=1}^{p} \mathbf{C}_{w,j} / \sum_{k=1}^{q}\mathbf{C}_{l,k}$, and ``CLIP Probs'' represents our strategy, which transforms $\mathbf{C}_{w}$ and $\mathbf{C}_{l}$ into probabilities. The results indicate that: firstly, both strategies improve the performance, underscoring the effectiveness of integrating data inconsistency into the optimization process, which allows the model to better handle varying levels of data hardness, thereby improving overall robustness; furthermore, transforming $\mathbf{C}{w}$ and $\mathbf{C}_{l}$ into probabilities yields a larger performance gain, as probabilities smooth the estimation of $\delta$, mitigating noise from large gaps between $\mathbf{C}{w}$ and $\mathbf{C}_{l}$, and preventing the influences by the outliers.
% combination
\begin{table}[t]
\caption{Experimental results of different preference inconsistency construction strategies.}
\label{tab:ab3-clip_prob}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l | c c }
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Object HalBench}} \\

& Response ($\uparrow$) & Mention ($\uparrow$) \\
\midrule
DPO & 78.29 & 89.48  \\
\midrule
+CLIP Scores & \addvalue{80.65}{2.36}{6.5} & \addvalue{89.62}{0.14}{6.5} \\
\rowcolor{lightgray} \textbf{+CLIP Probs} & \addvalue{\textbf{82.54}}{4.25}{6.5} & \addvalue{\textbf{90.64}}{1.16}{6.5}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\noindent \textbf{Effects of the data filtering in model-aware preference optimization.} Table \ref{tab:ab2-data_filter} presents the performance of different data filtering strategies, where ``No Filter'' denotes that we directly utilize the mean gaps to estimate the model state without filtering, ``Bottom'' shows that we remove the $N-K$ samples with the largest distances in the batch, ``Top'' is filtering the $N-K$ samples with the smallest distances, ``Bottom \& Top'' refers to our filtering strategy, which filters both extremes based on the squared distances. Specifically, we can observe that: firstly, filtering solely from the bottom or top leads to performance degradation, indicating that such data introduces bias in estimating the model state. Moreover, filtering only the bottom samples results in significant performance drops due to overfitting on the top-ranked data, which misguides the estimation of model responsiveness to focus excessively on potentially less representative instances. Furthermore, filtering both bottom and top samples yields performance improvements, demonstrating the effectiveness of our proposed strategy, as it balances the influence of extreme data points.

\begin{table}[t]
\caption{Experimental results of different filtering strategies.}
\label{tab:ab2-data_filter}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l | c c }
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Object HalBench}} \\

& Response ($\uparrow$) & Mention ($\uparrow$) \\
\midrule
No Filter & 86.66 & 92.62 \\
\midrule
Bottom & \minusvalue{76.36}{10.30}{7.4} & \minusvalue{87.20}{5.42}{6} \\
Top & \minusvalue{86.34}{0.32}{6} & \minusvalue{92.48}{0.14}{6} \\
\rowcolor{lightgray} \textbf{Bottom \& Top} & \addvalue{\textbf{88.00}}{1.34}{6.5} & \addvalue{\textbf{93.74}}{1.12}{6.5} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
% over-fitting


\begin{table*}[t]
\caption{Performance comparisons with state-of-the-art methods on different benchmarks. 
We report non-hallucination rates in different levels including response level (Non-Rsp.) and mentioned-level (Non-Men.) for Object HalBench \cite{object_hallucination_benchmark}. Hall. refers to the Hallucination Rate for MMHal Bench \cite{LLaVA-RLHF}.
The best results of all methods are indicated in \textbf{bold}, and the second best results are \uline{underlined}.
The compared results are sourced from \cite{RLAIF-V, TPO}, and the reported results of LLaVA-1.5, DPO, and DAMA are evaluated using GPT-4-turbo-2024-04-09.}
\label{tab:comparison_SOTA}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\scalebox{0.86}{
\begin{tabular}{l | c | c c | c c | c c | c | c }
\toprule
\multirow{3}{*}{\textbf{Method}} & \multirow{3}{*}{\textbf{Size}} & \multicolumn{2}{c|}{\hspace{-1mm}\textbf{Object}} & \multicolumn{2}{c|}{\multirow{2}{*}{\hspace{-3mm}\textbf{AMBER}}} & \multicolumn{2}{c|}{\textbf{MMHal-}} & {\textbf{LLaVA}} & \multirow{3}{*}{\hspace{-2mm}\textbf{MM-Vet}($\uparrow$)} \\

& & \multicolumn{2}{c|}{\hspace{-1mm}\textbf{HalBench}} & & & \multicolumn{2}{c|}{\textbf{Bench}} & {\textbf{Bench}($\uparrow$)} & \\

\cline{3-8} & & Non-Rsp.($\uparrow$) & Non-Men.($\uparrow$) & Acc($\uparrow$) & F1($\uparrow$) & Scores($\uparrow$) & Hall.($\downarrow$) & & \\

\midrule
\multicolumn{10}{l}{\multirow{1}*{\textbf{Method (Hallucination-specific)}}}\\
% \multicolumn{10}{l}{} \\
\midrule
VCD~\conf{CVPR'24} & 7B & 51.2 & 75.7 & 71.8 & 74.9 & 2.12 & 54.2 & 61.6 & - \\
Less-is-more~\conf{ACL'24} & 7B & 59.7 & 82.2 & 72.4 & 75.8 & 2.33 & 50.0 & - & - \\
OPERA~\conf{CVPR'24} & 7B & 54.9 & 77.7 & 75.2 & 78.3 & 2.15 & 54.2 & 61.3 & - \\
CCA-LLaVA~\conf{NIPS'24} & 7B & 53.3 & 76.2 & 77.7 & 81.9 & 1.92 & 61.5 & 64.3 & - \\
\midrule
\multicolumn{10}{l}{\multirow{1}*{\textbf{Method (Preference optimization)}}}\\
% \multicolumn{10}{l}{} \\
\midrule
HA-DPO~\conf{arXiv'23} & 7B & 60.1 & 80.1 & 75.2 & 79.9 & 1.98 & 60.4 & - & - \\
POVID~\conf{arXiv'24} & 7B & 51.9 & 75.6 & 82.9 & 87.4 & 2.08 & 56.2 & 68.2 & 31.7 \\
RLHF-V~\conf{CVPR'24} & 7B & - & - & 74.8 & 78.5 & 2.02 & 60.4 & 68.0 & 32.3 \\
RLAIF-V~\conf{arXiv'24} & 7B & \uline{89.5} & \uline{94.8} & 76.8 & 84.5 & \uline{2.95} & \textbf{32.3} & - & - \\
CSR~\conf{NIPS'24} & 7B & - & - & 73.2 & 76.1 & 2.05 & 60.4 & 68.9 & 31.0 \\
%%%%% 原文的llava bench和mm-vet：
% CSR~\conf{NIPS'24} & 7B &  &  & 73.2 & 76.1 & 2.05 & 60.4 & 71.1 & 33.9 \\
V-DPO~\conf{EMNLP'24} & 7B & - & - & - & 81.6 & 2.16 & 56.0 & - & - \\
TPO~\conf{arXiv'24} & 7B & - & - & 79.3 & 85.0 & 2.47 & 51.0 & 70.2 & 33.0 \\
% Silkie~\conf{EMNLP'24} & 10B & 72.9 & 86.6 & 82.2 & 87.6 & 3.19 & 32.3 & - & \textbf{49.9} \\
TPO~\conf{arXiv'24} & 13B & - & - & \uline{83.9} & \uline{88.0} & 2.72 & 45.8 & 72.8 & 36.2 \\
LLaVA-RLHF~\conf{ACL'24} & 13B & 61.9 & 81.1 & 79.7 & 83.9 & 2.02 & 62.5 & \textbf{95.6} & - \\
RLHF-V~\conf{CVPR'24} & 13B & 87.8 & 92.5 & 72.6 & 75.0 & 2.45 & 51.0 & \uline{76.7} & \textbf{38.5} \\
AMP-MEG~\conf{NIPS'24} & 13B & 68.3 & 79.4 & 79.5 & 84.6 & \textbf{3.08} & \uline{36.5} & - & - \\
\midrule
LLaVA-1.5 & 7B & 47.8 & 71.2 & 73.9 & 77.7 & 1.95 & 63.5 & 62.3 & 31.6 \\
\hspace{1mm} + DPO & 7B & 78.3 & 89.5  & 75.5 & 79.2 & 2.15 & 57.0 & 64.6 & 30.4\\
\hspace{1mm} + DAMA & 7B & \textbf{90.9} & \textbf{95.3}  & 83.3 & {87.0} & {2.76} & {41.0} & 68.0 & 32.8 \\
\rowcolor{lightgray} \hspace{1mm} Improvements (\%) & & \textcolor{blue}{\textbf{+16.1\%}} & \textcolor{blue}{\textbf{+6.5\%}}  & \textcolor{blue}{\textbf{+10.3\%}} & \textcolor{blue}{\textbf{+9.8\%}} & \textcolor{blue}{\textbf{+28.4\%}} & \textcolor{blue}{\textbf{+28.1\%}} & \textcolor{blue}{\textbf{+5.3\%}} & \textcolor{blue}{\textbf{+7.9\%}} \\
\midrule
LLaVA-1.5 & 13B & 50.0 & 76.4 & 71.2 & 73.0 & 2.36 & 56.0 & 66.1 & 36.1 \\
\hspace{1mm} + DPO & 13B & 84.5 & 92.4  & 78.5 & 83.5 & 2.59 & 48.0 & 74.0 & 35.4 \\
\hspace{1mm} + DAMA & 13B & 89.1 & 94.4  & \textbf{84.3} & \textbf{88.1} & {2.89} & 43.0 & 75.1 & \uline{36.4} \\
\rowcolor{lightgray} \hspace{1mm} Improvements (\%) & & \textcolor{blue}{\textbf{+5.1 \%}}  & \textcolor{blue}{\textbf{+2.1 \%}} & \textcolor{blue}{\textbf{+ 5.8\%}} & \textcolor{blue}{\textbf{+ 4.6\%}} & \textcolor{blue}{\textbf{+15.4\%}} & \textcolor{blue}{\textbf{+10.4\%}} & \textcolor{blue}{\textbf{+1.4 \%}} & \textcolor{blue}{\textbf{+2.8 \%}} \\
\midrule
\textbf{GPT-4V} & - & 86.4 & 92.7 & 83.4 & 87.4 & 3.49 & 28.1 & 98.0 & 67.7 \\
\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\noindent \textbf{Effects of the combination strategy.}
Figure~\ref{fig:exp-ab-response} and Figure ~\ref{fig:exp-ab-mention} illustrate the performance of different combination strategies from the Response and Mention, respectively. ``Multiplication'' refers to combining both effects using a weighted sum strategy, where $\alpha = (1-\rho) \cdot \alpha_{M} + \rho \cdot \alpha_{\mathrm{D}}^{\mathcal{B}}$, with $\rho$ ranging from $0.0$ to $1.0$, where $\rho = 0.0$ is the MDPO, and $\rho = 1.0$ refers to the ``D$^{2}$PO''. We can observe that multiplication outperforms the weighted sum by a significant margin. This is because the weighted sum balances the two components without effectively merging their influences, whereas multiplication amplifies their contributions, allowing for more robust and superior performance gains.

% combination
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/ab-resonses.pdf}}
\caption{Experimental results of the combination strategies with the response-level non-hallucination rates.}
\label{fig:exp-ab-response}
\end{center}
\vskip -0.2in
\end{figure}

% combination
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/ab-mention.pdf}}
\caption{Experimental results of the combination strategies with the mentioned-level non-hallucination rates.}
\label{fig:exp-ab-mention}
\end{center}
\vskip -0.2in
\end{figure}

%合并两个图
% \begin{figure}[t]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{Figures/合并-上下.pdf}}
% \caption{Experimental results of the combination strategies with the response-level and mentioned-level non-hallucination rates.}
% \label{fig:exp-ab-合并}
% \end{center}
% \vskip -0.2in
% \end{figure}

\subsection{Comparison with state-of-the-art methods}
In this subsection, we compare our method with state-of-the-art methods under three trustworthy benchmarks:  Object HalBench, AMBER, and MMHal-Bench, and two general benchmarks: LLaVA-Bench and MM-Vet. We compare our method against baselines from various aspects, including hallucination-specific baselines, preference optimization-based baselines, and the commercial baseline GPT-4V. Our DAMA is adapted to both the LLaVA-1.5 7B and 13B models. The experimental results, along with results of vanilla DPO and the improvements achieved by DAMA over DPO, are listed in Table \ref{tab:comparison_SOTA}. 

From the experimental results, we can observe that: (1) Compared to DPO, our DAMA achieves substantial performance improvements over all compared benchmarks. It's noteworthy that on MMHal-Bench, it achieves more than 10\% improvements for the 13B model, and more than 28\% gain for 7B models, which is significant; (2) DAMA achieves new state-of-the-art over various benchmarks. Our DAMA-7B reduces the response-level and mentioned-level hallucination on the generative Object HalBench by 90.9\% and 95.3\%. Moreover, DAMA-13B achieves 84.3\% Accuracy and 88.1\* F1 score on the discriminative AMBER benchmark. These attained results surpass those of GPT-4V, fully demonstrating the effectiveness of DAMA.