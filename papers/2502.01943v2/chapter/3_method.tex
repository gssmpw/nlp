\section{Approach}
\label{sec:approach}
In this section, we describe our DAMA in detail. Specifically, we first illustrate our data-aware preference optimization, then we describe our model-aware preference optimization, and finally, we show our combination strategies for robust preference optimization. Our approach algorithm is listed in Algorithm. \ref{algorithm}.

\subsection{Data-aware Preference Optimization}
\label{subsec:Data-anchored Preference Optimization}
An overview of our data-aware preference optimization is shown in Figure~\ref{fig:pipleine-vlm}. Given a preference instance from the dataset $\mathcal{D}$ as $\{ (\mathcal{I}, x, y_{w}, y_{l})\} \sim \mathcal{D}$, where $\mathcal{I}$, $x$, $y_{w}$, $y_{l}$ denotes the image, question, preferred response, and rejected response, respectively, it firstly splits the responses into simple and self-contained sub-sentences. Next, it calculates the image-text similarity scores between the sub-sentences and the image by the CLIP classifier. Then, it combines the scores of each response and compares the difference between the preferred and rejected responses as the data hardness. This hardness is embedded into the preference optimization process by modifying the $\beta$ in Equ~\eqref{equ:dpo}.
The following are detailed descriptions.

We employ the CLIP classifier $\mathbf{\Gamma}_{\mathrm{CLIP}}$ \cite{CLIP}, to calculate similarity scores.
For each preference instance, we aim to effectively capture the similarity between the responses $(y_{w}, y_{l})$ with the given image $\mathcal{I}$, while alleviating the 77 token length constraints in CLIP. To achieve this, we decompose the complex responses, which contain various objects and relations, into simple and self-contained sub-sentences. 
Concretely, we prompt the open-source large language model, such as LLaMA-3 \cite{LLaMA3}, to split $(y_{w}, y_{l})$ into sub-sentences $\mathbf{S}_{w} = \{ \mathbf{S}_{w,j} | j = 1,2, \dots p\}$ and $\mathbf{S}_{l} = \{ \mathbf{S}_{l,k} | k = 1,2, \dots q\}$, where $p$ and $q$ denotes the number of sub-sentences for $\mathbf{S}_{w}$ and $\mathbf{S}_{l}$.

Subsequently, we employ the CLIP classifier $\mathbf{\Gamma}_{\mathrm{CLIP}}$ to calculate the similarity score between the given image $\mathcal{I}$ and the sub-sentences $\mathbf{S}_{w}, \mathbf{S}_{l}$ as:
\begin{equation}
\label{equ:subsentence-score}
\begin{aligned}
\mathbf{C}_{w} &= \left[ \mathbf{\Gamma}_{\mathrm{CLIP}}(\mathcal{I}, \mathbf{S}_{w,j}) \right]_{j=1}^{p}, \\
\mathbf{C}_{l} &= \left[ \mathbf{\Gamma}_{\mathrm{CLIP}}(\mathcal{I}, \mathbf{S}_{l,k}) \right]_{k=1}^{q},
\end{aligned}
\end{equation}
where $\mathbf{C}_{w} \in \mathbb{R}^{p}$ and $\mathbf{C}_{l} \in \mathbb{R}^{q}$ represents the corresponding similarity scores of $\mathbf{S}_{w}$ and $ \mathbf{S}_{l}$, respectively. 
To effectively quantify the difference between preferred response $\mathbf{C}_{w}$ and rejected response $\mathbf{C}_{l}$ for each instance, 
we normalize the corresponding score by the softmax probabilities as :
\begin{equation}
\label{equ:subsentence-prob}
\left[ \begin{array}{c}
\mathbf{P}_{w} \\
\mathbf{P}_{l}
\end{array} \right] = 
\mathbf{Softmax}   \left (  \left[ \begin{array}{c}
    \mathbf{C}_{w} \\
    \mathbf{C}_{l}
    \end{array} \right]  \right ) ,
\end{equation}
$\mathbf{P}_{w} \in \mathbb{R}^{p}$ and $\mathbf{P}_{l} \in \mathbb{R}^{q}$ represents the probabilites. 
The difference between the preferred and rejected probabilities demonstrates the data hardness. A large difference implies that the preference data is ``easy-to-distinguish'', where the rejected response includes more elements that are not present in the image, conversely, a small difference suggests that the preference data is ``hard-to-distinguish'', and the rejected response exhibits minimal hallucination. 
Then we define the hardness based on the probabilities difference as:
\begin{align}
    \label{equ:difference}
        \delta = & \sum_{j=1}^{p} \mathbf{P}_{w,j} - \sum_{k=1}^{q} \mathbf{P}_{l,k}, \\
% \end{equation}
% \begin{equation}
    \label{equ:alpha_v}
    % \alpha_{\mathrm{D}} &= \frac{\sigma(\delta)}{\sigma(\bar{\delta})},
    & \alpha_{\mathrm{D}} = \sigma(\delta) / \sigma(\bar{\delta}),
\end{align}
where $\alpha_{\mathrm{D}}$ denotes the data hardness, $\delta$ measures the difference between $\mathbf{P}_{w}$ and $\mathbf{P}_{l}$, and $\bar{\delta}$ denotes the mean difference across the dataset. The Sigmoid function $\sigma(\cdot)$ is employed to transform the response divergence $\mathbf{P}_{w}$ and the mean $\bar{\delta}$ into the range $(0, 1)$ for convenient comparison.

Finally, we adapt $\beta$ of Equ~\eqref{equ:dpo} to incorporate the hardness into the optimization procedure, and each preference instance corresponding to a specific $\beta$ as:
\begin{equation}
\label{equ:update beta data}
    \beta_{\mathrm{D}} = \beta \cdot \alpha_{\mathrm{D}}.
\end{equation}
This adjustment allows the model to optimize based on the data hardness, further enhancing its adaptability to the data.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/pipleine-llm-v5.pdf}}
\caption{Overview of our model-aware preference optimization. Given $N$ preference instances: (1) we first calculate the reward gap of each instance using the implicit reward model; (2) To ensure stable modeling, we filter out the outliers (\textit{i.e.} the instance with excessively high or low gaps) and then estimate the average gap; (3) To enable the model to be aware of its current responsiveness, we integrate such estimation into the preference optimization process by modifying $\beta$ in Equ~\eqref{equ:dpo}.}
\label{fig:pipleine-llm-v2}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Model-aware Preference Optimization}
\label{subsec:Model-aware Preference Optimization}
An overview of our model-aware preference optimization is shown in Figure~\ref{fig:pipleine-llm-v2}. 
Given a batch of preference instances from dataset $\mathcal{D}$, as $\mathcal{B} = \{ (\mathcal{I}_{i}, x_{i}, y_{w,i}, y_{l,i}) | i = 1,2,\dots,N \} \sim \mathcal{D}$, 
% where $\mathcal{I}_{i}$, $x_{i}$, $y_{w,i}$, $y_{l,i}$ denotes the image, question, preferred response, and rejected response, 
it firstly calculates the reward gaps between the preferred $y_{w,i}$ and rejected $y_{l,i}$, and then filters out the outliers (\textit{i.e.} the instance with excessively high or low gaps) for stable estimation.
Such estimations are embedded into the preference optimization process by integrating into $\beta$ in Equ~\eqref{equ:dpo}, enabling the model to be aware of its current responsiveness. 
Details of model-aware preference optimization are as follows.

We employ current implicit reward gaps between the preferred and rejected responses of the given $\mathcal{B}$ instances to measure the current model's responsiveness. Specifically, the reward gap $\mathcal{R}$ for the $i$-th instance in $\mathcal{B}$ is formalized as:
\begin{equation}
\label{equ:hardness}
\resizebox{.91\hsize}{!}{
    \(\mathcal{R}_i =\left [  \beta \log \frac{{\pi}_{\bm{\theta}}(y_{w,i}|\mathcal{I}_i,x_i)}{{\pi}_{\text{ref}}(y_{w,i}|\mathcal{I}_i,x_i)}
    -\beta \log \frac{{\pi}_{\bm{\theta}}(y_{l,i}|\mathcal{I}_i,x_i)}{{\pi}_{\text{ref}}(y_{l,i}|\mathcal{I}_i,x_i)} \right ],\)
}
\end{equation}
where ${\pi}_{\bm{\theta}}$ and ${\pi}_{\text{ref}}$ represent the optimizing model and reference model, respectively. 
We then normalize the reward gaps using the estimated mean as follows:
\begin{equation}
\label{equ:batch_hard}
    \bar{{\mathcal{R}}_i} = \mathcal{R}_i / \bar{\mathcal{R}},
\end{equation}
where $\bar{\mathcal{R}}$ represents the estimated average reward gap, and $\bar{{\mathcal{R}}_i}$ is the normalized one for the $i$-th instance.

However, the estimation remains sensitive to outliers despite normalization, especially in the full fine-tuning scenario, where the batch size is relatively small.
To mitigate this issue, we filter out instances with exceptionally high or low gaps using a mask vector $\mathcal{M} \in \mathbb{R}^{N}$, defined as:
\begin{equation}
\label{equ:filter hardness}
    \bm{M}_{i} =
    \left\{
    \begin{aligned}
    &1, & (\bar{\mathcal{R}}_{i} - \bar{\mathcal{R}})^{2} \le  \tau, \\ 
    &0, & (\bar{\mathcal{R}}_{i} - \bar{\mathcal{R}})^{2}  >   \tau,
    \end{aligned}
    \right. 
\end{equation}
where $(\bar{\mathcal{R}_{i}} - \bar{\mathcal{R}})^{2}$ implies the squared distances from the mean, and $\tau$ represents the sorted $K$-th distance.
With the filtering, current responsiveness of ${\pi}_{\bm{\theta}}$ can be formalized as:
\begin{align}
\label{equ:average hardness}
    \bar{\mathcal{R}}_{\mathcal{B}} & = \frac{1}{N-K} \sum_{i=1}^{N}  \mathcal{M}_{i} \times \bar{\mathcal{R}}_{i}, \\
        \label{equ:alpha_M}
    \alpha_{\text{M}} & = \sigma(\bar{\mathcal{R}}_{\mathcal{B}}) / \sigma(\bar{\mathcal{R}}).
\end{align}

$\sigma(\cdot)$ is the Sigmoid function, which transforms both the filtered gaps $\bar{\mathcal{R}}_{\mathcal{B}}$ and the estimated mean $\bar{\mathcal{R}}$ into the range $(0, 1)$ for convenient comparison, and $\alpha_{M}$ refers to the estimated model responsiveness.

We then integrate $\alpha_{M}$ into the optimization procedure by modifying $\beta$ of Equ~\eqref{equ:dpo} as:
\begin{equation}
\label{equ:update beta reward}
    \beta_{\text{M}} = \beta \cdot \alpha_{\text{M}}.
\end{equation}
By utilizing $\beta_{M}$ to optimize $\pi_{\theta}$ with the filtered batch $\mathcal{B} \cdot \mathbf{M}$, the model can effectively adapt to its current responsiveness to the preference data.

Finally, we update the estimated mean $\bar{\mathcal{R}}$ using a moving average after optimization over batch $B$ as follows:
\begin{equation}
\label{equ:update mean}
    \bar{\mathcal{R}} \bm{\leftarrow} \gamma \cdot  \bar{\mathcal{R}} + (1- \gamma) \cdot \bar{\mathcal{R}}_{\mathcal{B}},
\end{equation}
the momentum $\gamma$ is set to $0.9$, and $\bar{\mathcal{H}}$ is initialized to $0$. 


\subsection{Combining for Preference Optimization}
\label{subsec:combination}
In this section, we present the combination of both data- and model-aware strategies for robust preference optimization.
Specifically, given a batch of preference instances $\mathcal{B} = \{ (\mathcal{I}_{i}, x_{i}, y_{w, i}, y_{l, i}) | i = 1,2,\dots, N \} \sim \mathcal{D}$, where $\mathcal{I}_{i}$, $x_{i}$, $y_{w, i}$, $y_{l, i}$ denotes the image, question, preferred response, and rejected response, respectively,
we first compute data hardness offline based on our data-aware preference optimization, and then obtain the corresponding instance-wise hardness by Equ~\eqref{equ:alpha_v} as $\alpha_{\mathrm{D}}^{\mathcal{B}} \in \mathbb{R}^{N}$. 
Next, we calculate the reward gaps using Equ~\ref{equ:alpha_M} as $\alpha_{\text{M}}$.
To facilitate the optimization process to be both data- and model-aware, we propose an element-wise combination strategy. Specifically, we combine $\alpha_{\mathrm{D}}^{\mathcal{B}}$ and $\alpha_{\text{M}}$ as:
\begin{equation}
    \label{equ:alpha}
    \alpha = \alpha_{\mathrm{D}}^{\mathcal{B}} \cdot \alpha_{\text{M}},
\end{equation}
where $\alpha \in \mathbb{R}^{N}$ represents the combined factor. 
Subsequently, we adjust $\beta$ of Equ~\eqref{equ:dpo} to incorporate both components into optimization procedure as:
\begin{equation}
    \label{equ:update beta all}
        \beta_{\text{C}} = \beta \cdot \alpha,
    \end{equation}
where $\beta_{\text{C}} \in \mathbb{R}^{N}$, and each preference instance in $\mathcal{B}$ corresponds to a specific $\beta$. 
Finally, our combination strategy can be achieved by employing the $\beta_{\text{C}}$ to optimize $\pi_{\bm{\theta}}$ with the filtered batch $\mathcal{B} \cdot \bm{M}$, where $\bm{M}$ is obtained by Equ~\eqref{equ:filter hardness}.
Thus, the optimization process can become more adaptive, allowing the model to refine its preferences based on both pre-computed data hardness and real-time model responsiveness, further enhancing the robustness.

\begin{algorithm}[t]
   \caption{Algorithm of DAMA.}
   \label{algorithm}
\begin{algorithmic}
   \STATE {\bfseries Input:} Preference dataset $\mathcal{D}$, hyper-parameter $\beta$, SFT model $\pi_{\text{SFT}}$, CLIP classifier $\Gamma_{\text{CLIP}}$.
   \STATE {\bfseries Output}: The optimized model $\pi_{\bm{\theta}}$.
   \STATE Initialize model $\pi_{\bm{\theta}}$ and reference model $\pi_{\text{ref}}$ as $\pi_{\text{SFT}}$.
   \FOR{$\{ (\mathcal{I}, x, y_{w}, y_{l})\}$ in $\mathcal{D}$}
        \STATE 
        $\mathbf{S}_{w} \gets \text{LLM}\{y_w\}$, $\mathbf{S}_{l} \gets \text{LLM}\{y_l\}$;
        \STATE obtains $\delta$ with $\{\mathcal{I}, \mathbf{S}_{w}\}$, $\{\mathcal{I}, \mathbf{S}_{l}\}$; \COMMENT{Equ~\eqref{equ:subsentence-score} $\to$ \eqref{equ:difference}};
        \STATE $\alpha_{\mathrm{D}} \gets \sigma(\delta) / \sigma(\bar{\delta})$; \COMMENT{Equ~\eqref{equ:alpha_v}};
    \ENDFOR
   \REPEAT
   \FOR{$\mathcal{B} = \{ (\mathcal{I}_{i}, x_{i}, y_{w,i}, y_{l,i})\}_{i=1}^{N} \sim \mathcal{D}$}
        \STATE obtain $\mathcal{R}_{i}$ with $y_{w,i}$ and $y_{l,i}$; \COMMENT{Equ~\eqref{equ:hardness}};
        \STATE obtain $\bar{\mathcal{R}}_{\mathcal{B}}$ with $\mathcal{R}_{i}$;
        \COMMENT{Equ~\eqref{equ:batch_hard} $\to$ \eqref{equ:average hardness}};
        \STATE $\alpha_{M}  \gets \sigma(\bar{\mathcal{R}}_{\mathcal{B}}) / \sigma(\bar{\mathcal{R}})$; \COMMENT{Equ~\eqref{equ:alpha_M}};
        \STATE $\alpha \gets \alpha_{\mathrm{D}}^{\mathcal{B}} \cdot \alpha_{M}$, where $ \alpha_{\mathrm{D}}^{\mathcal{B}} = \{ {\alpha_{\mathrm{D,i}}}\}_{i=1}^{N}$;
        \COMMENT{Equ~\eqref{equ:alpha}}; 
        \STATE $\beta_{\mathrm{C}} \gets \beta \cdot \alpha$; \COMMENT{Equ~\eqref{equ:update beta all}}; 
        \STATE Compute loss {w.r.t.} $\beta_{\mathrm{C}}, \pi_{\bm{\theta}}$; \COMMENT{Equ~\eqref{equ:dpo}};
        \STATE Compute the gradient and update the model $\pi_{\bm{\theta}}$.
        \STATE $\bar{\mathcal{R}} \bm{\leftarrow} \gamma \cdot  \bar{\mathcal{R}} + (1- \gamma) \cdot \bar{\mathcal{R}}_{\mathcal{B}}$; \COMMENT{Equ~\eqref{equ:update mean}};
    \ENDFOR
   \UNTIL{The optimization is converged.}
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[t]
%    \caption{Algorithm of DAMA.}
%    \label{algorithm}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} Preference dataset $\mathcal{D}$, hyper-parameter $\beta$, SFT model $\pi_{\mathrm{SFT}}$, CLIP classifier $\Gamma_{\mathrm{CLIP}}$.
%    \STATE {\bfseries Output}: The optimized model $\pi_{\bm{\theta}}$.
%    \STATE Initialize model $\pi_{\bm{\theta}}$ and reference model $\pi_{\mathrm{ref}}$ as $\pi_{\mathrm{SFT}}$.
   % \STATE Compute individual response difference $\delta$ with the CLIP classifier $\Gamma_{\mathrm{CLIP}}$ on $\mathcal{D}$.
   % \STATE Compute the data hardness for each instance on $\mathcal{D}$ as $\alpha_{\mathrm{D}}=\sigma(\delta) / \sigma(\bar{\delta})$.
   % \REPEAT
   % \STATE Sample a batch of preference samples $\mathcal{B} = \{ (\mathcal{I}_{i}, x_{i}, y_{w,i}, y_{l,i}) | i = 1,2,\dots,N \}$ combing with the data hardnesses $\alpha_{\mathrm{D}}^{\mathcal{B}} = \{\alpha_{\mathrm{D}}^{i}  | i = 1,2,\dots,N \}$.
   % \STATE Compute reward gap for each $\mathcal{R}_{i}$ by Equ~\eqref{equ:hardness}.
   % \STATE Normalize $\mathcal{R}$ with the estimated mean $\bar{\mathcal{R}}$.
   % \STATE Filter the $K$ outliers using the mask $\mathcal{M}$ in Equ~\eqref{equ:filter hardness}.
   % \STATE Compute the masked average reward gaps for the batch $\bar{\mathcal{R}}_{\mathcal{B}}$ using Equ~\eqref{equ:average hardness}.
   % \STATE Compute the model responsiveness as $\alpha_{M} = \sigma(\bar{\mathcal{R}}_{\mathcal{B}}) / \sigma(\bar{\mathcal{R}})$.
   % \STATE Compute the combined factor as $\alpha = \alpha_{\mathrm{D}}  \cdot \alpha_{M}$.
   % \STATE Compute the combined $\beta$ as $\beta_{\mathrm{S}} = \beta \cdot \alpha$.
   % \STATE Update the estimated mean $\bar{\mathcal{R}} \bm{\leftarrow} \gamma \cdot  \bar{\mathcal{R}} + (1- \gamma) \cdot \bar{\mathcal{R}}_{\mathcal{B}}$.
   % \STATE Compute the loss $\mathcal{L}_{\mathrm{dpo}}$ over the filtered batch instances $\mathcal{B} \cdot \mathcal{M}$ with $\beta_{\mathrm{S}}$ by Equ~\eqref{equ:dpo}.
   % \STATE Compute the gradient and update the model $\pi_{\bm{\theta}}$.
   % \UNTIL{The optimization is converged.}
% \end{algorithmic}
% \end{algorithm}