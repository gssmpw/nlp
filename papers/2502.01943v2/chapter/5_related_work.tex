\section{Related Work}
\label{sec:related_work}

In this section, we overview the related background. Specifically, we first briefly summarize recent methods for aligning with human preferences, and then, we discuss existing hallucination mitigation methods for multi-modal large language models. Finally, we enumerate the differences between ours and related methods.

\subsection{Alignment with Human Preference}
To align the model outputs with human preferences, preference optimization methods have garnered significant attention \cite{rlhf, DPO, SimPO}. Concretely, RLHF (\cite{rlhf}) first introduces alignment through reward modeling, which trains a parametric reward model on preference data and subsequently optimizes the preference model using PPO \cite{PPO}. 
However, obtaining an effective reward model is challenging. To address this,  DPO \cite{DPO} is proposed to simplify the reward modeling process with an implicit reward function, allowing for direct optimization of preference data. Subsequent works like \cite{SimPO} and \cite{KTO} further simplify this process by removing the reference model and introducing binary feedback, respectively.

Current alignment works in MLLMs focus on two aspects:
(1) Collecting high-quality preference data. For example, existing works \cite{RLHF-V, RLAIF-V, VLFeedback} construct high-quality preference data to advance the research of MLLM alignment. For example, the works in \cite{RLHF-V, LLaVA-RLHF} introduce human-based preference construction strategies, RLAIF-V \cite{RLAIF-V} utilizes the open-source models (\textit{e.g.} LLaVA-NeXT \cite{LLaVA-Next}) to construct preference data, and the works in \cite{VLFeedback} and \cite{MAVIS} employ closed-source models like GPT-4V for preference annotation.
(2) Emphasizing the focus on visual detail. For instance, the works in \cite{V-dpo} and \cite{mDPO} construct rejected samples by destroying the image, the works like TPO \cite{TPO} and FiSAO \cite{FiSAO} identify and emphasize key tokens from the preference responses to attend to visual detail.

\subsection{Hallucination mitigation in MLLMs}
Hallucination, As a key indicator of trustworthiness, refers to that the MLLM outputs are not aligned with the image content \cite{hallucination-survey}. Beyond the preference optimization-based methods, current works to mitigate hallucination can be summarized into the following three aspects: 
(1) Data filtering. Works in LRV-Instruction \cite{LRV-Instruction} and Hallucidoctor \cite{Hallucidoctor} identify that noises within the instruction tuning data serve as a cause of hallucination, and introduce fine-grained data cleaning and curation strategies to mitigate hallucination.
(2) Enhanced visual representation. Works like \cite{Vcoder, MMVP} suggest that insufficient visual cues are the fundamental cause of hallucination, and they incorporate more intricate visual features to enrich the visual representations.
(3) Inference-time enhancement. Works like VCD \cite{VCD} and MARINE \cite{CFG} introduce visual contrastive decoding mechanisms to help enhance the model's focus on visual details by contrastively sampling from original and visually distorted distribution.

Compared with existing methods, which focus on data curation and introduce fine-grained regularizations, our proposed method aims to optimize adaptively based on the specific model responsiveness and the hardness of the data. 
We seek to design an improved optimization strategy that can effectively leverage the current model and the preference data, enabling the model to adaptively respond to the preference data, and thereby enhancing the alignment performance.