\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{Figures/pipeline-vlm-v4.pdf}}
\caption{Overview of our data-aware preference optimization. For each preference instance: (1) We first break the preferred and rejected response into sub-sentences by prompting a large language model (LLM); 
(2) Next, we estimate the similarity scores between each sub-sentence and the given image using the CLIP classifier, and then calculate the differences between the preferred and rejected response as the hardness of the data; 
(3) Finally, we incorporate the estimated hardness into the preference optimization process by modifying $\beta$ in Equ~\eqref{equ:dpo}, allowing the model to adjust based on the data hardness.}
\label{fig:pipleine-vlm}
\end{center}
\vskip -0.2in
\end{figure*}


\section{Preliminary}
\label{sec:preliminary}
In this section, we briefly review the MLLM preference learning procedure, which starts by sampling pairwise preference data with a supervised fine-turned (SFT) model, and then optimizes on such preference data. Specifically, we categorize this process into the following aspects:

\noindent \textbf{Supervised Fine-Tuning (SFT).}
Preference learning of an MLLM $\bm{\pi}$ begins with an SFT model $\bm{\pi}_{\text{SFT}}$. Concretely, the SFT process fine-tunes the pre-trained MLLM model with millions of multi-modal question-answer pairs to align LLM with multi-modal tasks. 
After this process, we construct preference data by sampling pair-wise preference responses from $\bm{\pi}_{\mathrm{SFT}}$, formalized as $(y_w, y_l) \sim \bm{\pi}_{\mathrm{SFT}}(y|x,\mathcal{I})$, where $(\mathcal{I}$ denotes the image and $x$ is the prompt question. 
Meanwhile, $(y_w, y_l)$ are labeled as preferred and less preferred responses by humans, formalized as $(y_w \succ  y_l | \mathcal{I}, x)$.

\noindent \textbf{RLHF with Reward Models.}
Given pair-wise preference data $(y_w, y_l) \sim \bm{\pi}_{\mathrm{SFT}}(y|x,\mathcal{I})$, the preference learning process can be described in 2 stages: reward modeling and preference optimization. 
Specifically, the reward model $r_{\bm{\theta}}(y|\mathcal{I}, x)$ is defined to rank the model responses by learning to distinguish $y_w$ from $y_l$, and the preference optimization aims to distill the preference knowledge into MLLM. 
To learn a reward model, pioneering work \cite{rlhf} employs the Bradley-Terry model \cite{BT_model} to model the pair-wise preference distribution as:
\begin{equation}
\resizebox{.9\hsize}{!}{
\begin{math}
\begin{aligned}
    \mathrm{P}(y_w \succ  y_l|\mathcal{I}, x) & =  \sigma(r^{*}(y_w|\mathcal{I}, x)- (r^{*}(y_l|\mathcal{I}, x)) \\
     & = \frac{\mathrm{exp}(r^{*}(y_w|\mathcal{I}, x))}{\mathrm{exp}(r^{*}(y_w|\mathcal{I}, x))+\mathrm{exp}(r^{*}(y_l|\mathcal{I}, x))}.
\end{aligned}
\end{math}
}
\end{equation}

Thus, the learning process can be achieved by minimizing the negative log-likelihood $-\mathrm{logP}(y_w \succ y_l|\mathcal{I}, x)$ over the preference data with the parametrized reward model $r_{\bm{\phi}}(y_w|\mathcal{I}, x)$ initialized as $\bm{\pi}_{\mathrm{SFT}}$ with a simple linear layer to produce reward prediction. 
With the well-optimized reward model $r_{\phi}^{*}(y|\mathcal{I}, x)$, prior work \cite{rlhf} proposes to employ policy optimization algorithms in RL such as PPO \cite{PPO} to maximize the learned reward with KL-penalty, which can be formalized as:
\begin{equation}
\label{equ:ppo}
\begin{aligned}
    \underset{\bm{\pi}_{\theta}}{\text{max}} & \  \mathbf{E}_{(\mathcal{I},x) \sim \mathcal{D}, y \sim \bm{\pi}_{\theta}(\cdot|\mathcal{I}, x)} [r_{\phi}^{*}(y|\mathcal{I}, x)] \\
    & -\beta \mathbb{D}_{\mathbf{KL}}[\bm{\pi}_{\theta}(y|\mathcal{I},x)||\bm{\pi}_{\text{ref}}(y|\mathcal{I},x)], 
\end{aligned}
\end{equation}
where the fixed reference model $\bm{\pi}_{\text{ref}}$ is parameterized as $\bm{\pi}_{\text{SFT}}$, and the hyper-parameter $\beta$ controls the deviation of $\bm{\pi}_{\theta}$ from $\bm{\pi}_{\text{ref}}$ during the optimization process.

\noindent \textbf{Direct Preference Optimization (DPO).}
To relieve the high computational complexity of reward training in RLHF, DPO \cite{DPO} is proposed, which provides a simple way to directly optimize $\bm{\pi}_{\theta}$ with the pair-wise preference data, without parametrized reward model. Specifically, the DPO loss can be described as:
\begin{equation}
\label{equ:dpo}
\begin{aligned}
    \mathcal{L}_{\mathrm{dpo}} = - \bm{\mathrm{E}}_{(\mathcal{I},x, y_{w}, y_{l})} [ {\log \sigma}( & \beta \log \frac{{\pi}_{\bm{\theta}}(y_{w}|\mathcal{I},x)}{{\pi}_{\mathrm{ref}}(y_{w}|\mathcal{I},x)} \\
    - & \beta \log \frac{{\pi}_{\bm{\theta}}(y_{l}|\mathcal{I},x)}{{\pi}_{\mathrm{ref}}(y_{l}|\mathcal{I},x)}) ].
\end{aligned}
\end{equation}