\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{Figures/Figure-model-v12.pdf}}
\caption{ (1) Preference data (Prompt, Image, Preferred response $y_w$, Rejected response $y_l$) with different hardness: ``easy-to-distinguish'' data denotes a large Image-Text sim score gap between $y_l$ and $y_w$; ``hard-to-distinguish'' data indicates a low score gap between $y_l$ and $y_w$.
(2) Implicit reward across the optimization stage: the reward gap for ``easy-to-distinguish" data enhances significantly during optimization, while for ``hard-to-distinguish" data, the gap remains low.}
\label{fig:figure-1}
\end{center}
\vskip -0.2in
\end{figure}

% 总结：
% MLLM -> Hallucination -> DPO
Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated remarkable visual understanding capabilities on the basis of large language models \cite{LLaVA, InternVL, QwenVL}. 
However, despite their effectiveness, the hallucination issue --- generating outputs inconsistent with the image content and human preference --- limits their broader applicability \cite{VCD, LRV-Instruction, RLHF-V}. 
To address this, direct preference optimization (DPO) \cite{DPO} has been adapted into MLLM alignment \cite{RLAIF-V, mDPO, CLIP-DPO}, achieving encouraging performance with moderate computational costs.

DPO methods \cite{LLaVA-RLHF, RLAIF-V} collect preference data consisting of an image, a prompt, and two responses ($y_w, y_l$). The preferred response $y_w$ is better aligned with the visual content (large Image-Text similarity score), while the rejected response $y_l$ contains more hallucinated content (small Image-Text similarity score). 
They prioritize preferred responses $y_w$ over the rejected ones $y_l$ using DPO with a hyperparameter $\beta$, which balances retaining the reference model $\pi_\text{ref}$ and incorporating new preferences into the updated model $\pi_{\bm{\theta}}$ \cite{TPO, V-dpo}.

However, our analysis reveals that current methods exhibit imbalanced responsiveness in handling the data with varying hardness during the optimization process, resulting in suboptimal performance.
As illustrated in Figure \ref{fig:figure-1}, for ``easy-to-distinguish" data (large Image-Text similarity score gap between preferred $y_w$ and rejected $y_l$), the reward gap amplifies during training, indicating stronger alignment.
Conversely, for ``hard-to-distinguish'' data (small Image-Text similarity score gap), the reward gap stagnates, suggesting limited capability to distinguish $y_w$ from $y_l$. 
This implies that current methods, which employ optimization strategies using a static $\beta$ across data with varying hardness \cite{LLaVA-RLHF, RLAIF-V}, could fail to capture the learning dynamics inherent in multimodal preference data.

To address this imbalanced responsiveness issue, we propose \textbf{Da}ta- and \textbf{M}odel-\textbf{a}ware direct preference optimization (\textbf{DAMA}), which dynamically adapts $\beta$ to both data hardness and model's responsiveness, enabling adaptively adjust model's learning behavior based on the inherent preference dynamics and model's real-time responses. 
Specifically, we propose two novel mechanisms:

\textbf{Data-aware Preference Optimization. (Section \ref{subsec:Data-anchored Preference Optimization})}: 
    % It incorporates data hardness into preference optimization to dynamically adjust model's learning behavior based on the inherent dynamics in preference data.
    We quantify data hardness via CLIP-based image-text similarity scores \cite{CLIP}, decomposing responses into sub-sentences for granularity-aware estimation.
    Then we normalize and transform the scores into probabilities to enable effective hardness estimation.
    By dynamically scaling $\beta$ inversely with hardness, we enforce stronger regularization on ``easy-to-distinguish'' samples (large $\beta$) while relaxing constraints for ``hard-to-distinguish'' ones (small $\beta$), preventing overfitting and underfitting, respectively.

\textbf{Model-aware Preference Optimization. (Section \ref{subsec:Model-aware Preference Optimization})}: 
    % It integrates model's real-time responses into preference optimization to adaptively adjust model's learning behavior with     
    % Our model-aware strategy incorporates the model's responsiveness to the data into the optimization process to adaptively adjust the learning behavior based on the model's responsiveness at different stages. 
    We estimate the model's responsiveness through the reward gaps between the preferred and rejected responses. Moreover, to improve the robustness of the estimation, we normalize the reward gaps and filter out outliers. 
    Similarly to Section \ref{subsec:Data-anchored Preference Optimization}, we incorporate the estimated responsiveness into the optimization process by dynamically modifying $\beta$ according to it, enabling the model to effectively optimize according to its current responsiveness.

By combining these strategies via element-wise multiplication, DAMA enables real-time adaptation to both data hardness and model responsiveness, demonstrating strong alignment performance across various evaluation benchmarks.
Our contributions are summarized as follows:
\begin{itemize}[leftmargin=*]
    \item We introduce a data-aware preference optimization strategy to integrate the data hardness into the preference optimization process, enabling the model to adaptively optimize based on the data hardness.
    \item We propose a model-aware preference optimization strategy to incorporate the real-time model response into the preference optimization process, facilitating the model to effectively optimize based on its current responsiveness.
    \item By combining the data- and model-aware strategies, we adaptively refine the optimization process, achieving significant alignment performance, as demonstrated by extensive empirical evaluations.
\end{itemize}

% % 总结：
% % MLLM -> Hallucination -> DPO
% Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated remarkable visual understanding capabilities by incorporating visual knowledge into large language models \cite{LLaVA, InternVL, QwenVL}. 
% However, despite their effectiveness, the hallucination issue --- where the MLLM generates outputs that are inconsistent with the image content --- and misalignment with human preference \cite{RLAIF-V, RLHF-V} limit their broader applicability \cite{VCD, LRV-Instruction}. 
% To address this, direct preference optimization (DPO) \cite{DPO} has been adapted into MLLM alignment \cite{RLAIF-V, mDPO, CLIP-DPO}, achieving improved performance with moderate computational costs.

% % 总结
% % DPO在干啥：background knowledge
% To achieve this, DPO methods \cite{LLaVA-RLHF, RLAIF-V} collect preference data consisting of an image, a prompt, and two responses ($y_w, y_l$). The preferred response $y_w$ is better aligned with the image and question, while the rejected response $y_l$ contains more hallucinated content. 
% Moreover, they prioritize preferred responses $y_w$ over the rejected ones $y_l$ by using DPO, with the trade-off between retaining the reference $\pi_\text{ref}$ and incorporating new preferences $\pi_{\bm{\theta}}$ governed by a hyperparameter $\beta$ that constrain the optimization process \cite{TPO, V-dpo}.

% However, our analysis reveals that current methods exhibit imbalanced responsiveness in handling data with varying hardness during optimization, resulting in suboptimal performance.
% As illustrated in Figure \ref{fig:figure-1}, for ``Easy-to-distinguish" data (large Image-Text similarity score gap between preferred $y_w$ and rejected $y_l$), the reward gap amplifies during training, indicating effective alignment.
% Conversely, for ``Hard-to-distinguish'' data (small Image-Text similarity score gap), the reward gap stagnate, suggesting limited capability to distinguish $y_w$ from $y_l$. 
% This implies that current methods, which based on uniform optimization strategies with a static $\beta$ across data of varying hardness \cite{RLHF-V, RLAIF-V}, fail to capture varied learning dynamics inherent in multimodal preference data.
% % This imbalance implies that current methods, which employ a uniform optimization strategy across data of varying hardness \cite{RLHF-V, RLAIF-V, LLaVA-RLHF}, fail to capture the varied learning dynamics inherent in multimodal preference data.

% To address this imbalanced responsiveness issue, we propose \textbf{D}ata-anchored and \textbf{M}odel-\textbf{a}ware Direct \textbf{P}reference \textbf{O}ptimization (DMaPO), which dynamically adapts $\beta$ to both data hardness and model responsiveness at different training stages. 
% Specifically, it achieves this through two key mechanisms:
% % By integrating data hardness and the model’s evolving responsiveness at different training stages into the optimization process, our approach enables dynamic adaptation to varying levels of data hardness. 

% \textbf{Data-anchored Preference Optimization. (Section \ref{subsec:Data-anchored Preference Optimization})}: 
%     Our data-anchored strategy incorporates data hardness into the preference optimization process to adaptively adjust the model's learning behavior based on varying levels of data hardness. 
%     Specifically, we quantify data hardness via CLIP-based image-text alignment scores \cite{CLIP}, decomposing responses into sub-sentences for granularity-aware estimation.
%     By dynamically scaling $\beta$ inversely with hardness, we enforce stronger regularization on ``Easy-to-distinguish'' samples (large $\beta$) while relaxing constraints for ``Hard-to-distinguish'' ones (small $\beta$), preventing overfitting and underfitting, respectively.
%     % To better capture the nuances of the data, we normalize and transform the scores into probabilities for effective estimation.
%     % Inspired by prior works \cite{rlhf, DPO, beta-DPO}, we integrate this hardness estimation into the optimization process as an anchor by dynamically adjusting the hyperparameter $\beta$.
%     % Specifically, a large $\beta$ is assigned to ``Easy-to-distinguish'' data, where the optimization process imposes stronger constraints to prevent overfitting. Conversely, a small $\beta$ is used for ``Hard-to-distinguish'' data, allowing the model to flexibly adapt to preferences. 

% \textbf{Model-aware Preference Optimization. (Section \ref{subsec:Model-aware Preference Optimization})}: 
%     Our model-aware strategy incorporates the model's responsiveness to the data into the preference optimization process to adaptively adjust the learning behavior based on the model's responsiveness at different stages. 
%     Specifically, we estimate the model's responsiveness through the reward gaps between the preferred and rejected responses. Furthermore, to improve the robustness of the estimation, we normalize the reward gaps and filter out outliers. 
%     Similarly to Section \ref{subsec:Data-anchored Preference Optimization}, we incorporate the estimated responsiveness into the optimization process by dynamically modifying $\beta$ according to the estimated responsiveness, enabling the model to effectively optimize based on its current responsiveness to the preference data.

% % Moreover, we further enhance preference optimization by combining data-anchored and model-aware strategy with element-wise multiplication, adaptively refining the optimization process with real-time model responsiveness and the data hardness, improving the alignment performance. 
% By combining these strategies via element-wise multiplication, DMaPO enables real-time adaptation to both data hardness and model responsiveness.
% To sum up, we have the following contributions:
% \begin{itemize}[leftmargin=*]
%     \item We introduce a data-anchored preference optimization strategy to integrate the data hardness into the preference optimization process, enabling the model to adaptively optimize based on the preference data.
%     \item We propose a model-aware preference optimization strategy to incorporate the model responsiveness into the preference optimization process, facilitating the model to effectively optimize based on its current responsiveness.
%     \item  By combining the model-aware and data-anchored strategies, we adaptively refine the optimization process, achieving significant alignment performance, as demonstrated by extensive empirical evaluations.
% \end{itemize}

% As shown in Figure \ref{fig:figure-1}, for ``Easy-to-distinct'' data (large Image-Text similarity score gap between $y_w$ and $y_l$), the reward differences between $y_w$ and $y_l$ increase from ``Early Stage'' to ``Later Stage'', indicating that the model aligns better on the preferred response $y_w$ with the rejected $y_l$. 
% Conversely, for the ``Hard-to-distinct'' data (small Image-Text similarity score gap between $y_w$ and $y_l$), the reward differences between $y_w$ and $y_l$ remain low and stable from ``Early Stage'' to ``Later Stage'', suggesting that the model struggles to distinguish $y_w$ from $y_l$ in optimization process.
% This indicates that while the model's responsiveness to ``Easy-to-distinct'' data is enhanced, its ability to handle ``Hard-to-distinct'' data remains limited throughout the optimization process.
% Therefore, we propose that data with varying levels of hardness require tailored adjustments to effectively utilize model's responsiveness. 


% 总结
% 当前的问题：数据有差异 -> 模型基于这些数据优化产生问题：问题：过拟合到easy的样例上，丧失了一定的generalization的能力 -> model responsiveness 不好
% However, revisiting the hallucination issue, where MLLM generates inconsistent output with image content, we observe that current methods show limited capacity in handling data with varying hardness during optimization, resulting in a suboptimal performance. 
% As shown in Figure \ref{fig:figure-1}, for ``Easy-to-distinct'' data (large Image-Text similarity score gap between $y_w$ and $y_l$), the reward differences between $y_w$ and $y_l$ increase from ``Early Stage'' to ``Later Stage'', indicating that the model aligns better on the preferred response $y_w$ with the rejected $y_l$. 
% Conversely, for the ``Hard-to-distinct'' data (small Image-Text similarity score gap between $y_w$ and $y_l$), the reward differences between $y_w$ and $y_l$ remain low and stable from ``Early Stage'' to ``Later Stage'', suggesting that the model struggles to distinguish $y_w$ from $y_l$ in optimization process.
% This indicates that while the model's responsiveness to ``Easy-to-distinct'' data is enhanced, its ability to handle ``Hard-to-distinct'' data remains limited throughout the optimization process.
% Therefore, we propose that data with varying levels of hardness require tailored adjustments to effectively utilize model's responsiveness. 

% Therefore, we propose that data with varying levels of hardness require tailored adjustments to effectively utilize model's responsiveness to the data. Specifically, ``Hard-to-distinct'' data demand finer adjustments to enhance the model’s capacity to differentiate, while ``Easy-to-distinct'' data benefit from stricter regularization to prevent overfitting.
% Therefore, we argue that data with varying hardness requires fine-grained adjustment, leveraging model capacity. Specifically, "Hard-to-distinct" data need more nuanced adjustments to enhance model differentiation capabilities, while "Easy-to-distinct" data should be optimized with more constraints to avoid overfitting.

% To address the above issue, we propose an adaptive optimization approach, namely Model-aware and Data-anchored Direct Preference Optimization, which distills both the data hardness and the model's capacity to fit the data into the optimization process, enabling the model to better respond to varying data hardness, and it consists of the following strategies:

% To address this limitation, we propose a Model-aware and Data-anchored Direct Preference Optimization. By integrating data hardness and the model’s evolving responsiveness at different training stages into the optimization process, it enables dynamic adaptation to varying levels of data hardness. Specifically, it achieves this through two key mechanisms:

% 总结：
% 我们的做法：数据侧 -> 把数据的差异喂到优化过程
% 模型测 -> 把模型的capacity蒸馏到优化过程
% \textbf{Data-anchored Preference Optimization. (Section \ref{subsec:Data-anchored Preference Optimization})}: 
%     Our data-anchored strategy incorporates data hardness into the preference optimization process to adaptively adjust the model's learning behavior based on varying levels of data hardness. 
%     Specifically, we quantify data hardness using the CLIP score \cite{CLIP}, which measures the alignment between images and text.
%     To better capture the nuances of the data, we decompose the responses into simple and self-contained sub-sentences, which are further normalized and transformed into probabilities to enable effective estimation.
%     Inspired by prior works \cite{rlhf, DPO, beta-DPO}, we integrate this hardness estimation into the optimization process as an anchor by dynamically adjusting the hyperparameter $\beta$.
%     A large $\beta$ is assigned to ``Easy-to-distinct'' data, where the optimization process imposes stronger constraints to prevent overfitting. Conversely, a small $\beta$ is used for ``Hard-to-distinct'' data, allowing the model to flexibly adapt to preferences. 
%     Therefore, our approach enables the dynamically adjustment the optimization process based on the data hardness.
    
    % By anchoring the optimization process with the data hardness, our approach enables the model to dynamically adjust to varying levels of data hardness.
    % Data-anchored strategy is achieved by incorporating the data hardness into preference optimization process. Specifically, we utilize the CLIP classifier \cite{CLIP} to estimate the image-text similarity, and decompose the responses into simple and self-contained sub-sentences for better capturing.
    
    % These are further normalized and transformed into probabilities to enable a fair and effective estimation.
    % Additionally, inspired by prior works\cite{rlhf, DPO, beta-DPO}, we integrate such estimation into the optimization process as the anchor by adjusting $\beta$ according to the data hardness. A large $\beta$ indicates that the data is ``Easy-to-distinct'', and greater constraints is required to the optimization to avoid overfitting, while small $\beta$ implies a ``Hard-to-distinct'' data, and fewer constraints should be applied to better adapt to the preference data. Anchoring optimization with the hardness estimation enables the model to adaptively respond to the data.

% 没找到合适的词
% model capacity: model’s capacity to fit the data，动态的
% \textbf{Model-aware Preference Optimization. (Section \ref{subsec:Model-aware Preference Optimization})}: 
%     Our model-aware strategy incorporates the model's responsiveness to the data into the preference optimization process to adaptively adjust the learning behavior based on the model's responsiveness at different stages. 
%     Specifically, we estimate the model's responsiveness by calculating the reward discrepancies between the preferred and rejected responses. Furthermore, to improve the robustness of the estimation, we normalize the reward discrepancies and filter out outliers. 
%     Similarly to Section \ref{subsec:Data-anchored Preference Optimization}, we incorporate the estimated responsiveness into the optimization process by dynamically modifying $\beta$ according to the estimated responsiveness, enabling the model to effectively optimize based on its current responsiveness to the preference data.
    
    % A large $\beta$ implies that the model has sufficient confidence, requiring greater constraints to avoid overfitting, and a small $\beta$ indicates that the model has limited capability, requiring fewer constraints to enhance its responsiveness to the preference data.
    
    % Model-aware strategy is achieved by integrating the model capability into the preference optimization process.
    % Specifically, we estimate the model capacity by calculating the reward discrepancies between the preferred and rejected responses, and then normalizing and filtering outliers, which helps to improve the robustness of the estimation.
    % Moreover, similar to Section \ref{subsec:Data-anchored Preference Optimization}, 
    % we incorporate the estimated model's capability into the optimization process by dynamically modifying $\beta$ according to the estimated capability, with a large $\beta$ implies that the model has sufficient confidence, requiring greater constraints to avoid overfitting, and a small $\beta$ indicates that the model has limited capability, requiring fewer constraints to enhance its responsiveness to the preference data.
    % This enables the model to be aware of and effectively optimize based on its current capability.

% 最后:
% 通过细粒度的整合，得到最终的模型
% Moreover, we further enhance preference optimization by combining model-aware and data-anchored strategy with element-wise multiplication, adaptively refining the optimization process with real-time model responsiveness and the data hardness, improving the alignment performance. 
% To sum up, we have the following contributions:
% \begin{itemize}[leftmargin=*]
%     \item We introduce a data-anchored preference optimization strategy to integrate the data hardness into the preference optimization process, enabling the model to adaptively optimize based on the preference data.
%     \item We propose a model-aware preference optimization strategy to incorporate the model responsiveness into the preference optimization process, facilitating the model to effectively optimize based on its current responsiveness.
%     \item  By combining the model-aware and data-anchored strategies, we adaptively refine the optimization process, achieving significant alignment performance, as demonstrated by extensive empirical evaluations.
% \end{itemize}

% rely on the static optimization procedure (\textit{c.f.} Figure~\ref{fig:figure-1}(2)), and have the following two key limitations:
% (a) \textit{Neglecting the implicit differences in preference data.}
% Preference data exhibit implicit differences between preferred and rejected responses, quantifiable via image-text correspondence. Specifically, for the response pair with greater difference, where the rejected response contains more hallucinated content misaligned with the given image, it becomes easier to distinguish between the preferred and rejected responses. In contrast, for pair with small difference, where the distinction is more challenging. Incorporating these differences into the optimization process can enhance adaptability to the data (\textit{cf.} Figure~\ref{fig:figure-1}(4)).
% Moreover, (b) \textit{Neglecting the model's state.} The model's response to the data varies based on its state, which should be incorporated into the optimization for a more adaptive adjustment. Specifically, if the model demonstrates sufficient confidence for the data, the distinction is easier; Conversely, if the model's capacity to handle the data is limited, the distinction becomes more challenging. The MLLM model should be aware of its current state to strengthen its responsiveness to the data. (\textit{c.f.} Figure~\ref{fig:figure-1}(3)). 

% Alignment-based methods \cite{LLaVA-RLHF, RLHF-V, RLAIF-V} have been proposed to align MLLM outputs with human preferences, leading to significant performance gains. Specifically, the preference data for alignment consist of an image with a prompt question and two responses, where the preferred response is more aligned with the image and the question, while the rejected response includes more hallucinated content (\textit{c.f.} Figure~\ref{fig:figure-1}(1)). By designing optimization strategies \cite{FiSAO, TPO, V-dpo}, they bring the model output closer to the preferred response and farther from the rejected one.

% Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated remarkable visual understanding capabilities by incorporating perceived visual knowledge into powerful large language models \cite{LLaVA, InternVL, QwenVL}. 
% Despite their effectiveness, the hallucination issue, where the MLLM outputs are incorrect with the image content, which deviates from human preference \cite{RLAIF-V, RLHF-V}, limits their broader applicability \cite{VCD, LRV-Instruction}. 

% To address this, Alignment-based methods \cite{LLaVA-RLHF, RLHF-V, RLAIF-V} have been proposed to align MLLM outputs with human preferences, leading to significant performance gains. Specifically, existing methods explore two key aspects:
% (1) Investigating improved preference data collection with priors to better represent human preferences \cite{V-dpo, LLaVA-RLHF, CLIP-DPO, mDPO} (\textit{c.f.} Figure~\ref{fig:figure-1}(1, a)),
% and (2) Figure~\ref{fig:figure-1}(1, b): Delving into fine-grained regularization strategies to focus more on visual details \cite{RLHF-V, FiSAO, TPO}, thus further mitigating the risk of generating unreliable or hallucinated responses (\textit{c.f.} Figure~\ref{fig:figure-1}(1, b)).

% 图文匹配的语义分布的差异
% However, revisiting the hallucination issue, where MLLM generates inconsistent output with image content, we observe that current methods rely on the static optimization procedure (\textit{c.f.} Figure~\ref{fig:figure-1}(2)), and have the following two key limitations:
% (a) \textit{Neglecting the implicit differences in preference data.}
% Preference data exhibit implicit differences between preferred and rejected responses, quantifiable via image-text correspondence. Specifically, for the response pair with greater difference, where the rejected response contains more hallucinated content misaligned with the given image, it becomes easier to distinguish between the preferred and rejected responses. In contrast, for pair with small difference, where the distinction is more challenging. Incorporating these differences into the optimization process can enhance adaptability to the data (\textit{cf.} Figure~\ref{fig:figure-1}(4)).
% Moreover, (b) \textit{Neglecting the model's state.} The model's response to the data varies based on its state, which should be incorporated into the optimization for a more adaptive adjustment. Specifically, if the model demonstrates sufficient confidence for the data, the distinction is easier; Conversely, if the model's capacity to handle the data is limited, the distinction becomes more challenging. The MLLM model should be aware of its current state to strengthen its responsiveness to the data. (\textit{c.f.} Figure~\ref{fig:figure-1}(3)). 

% Moreover, we argue that current methods neglect the variation of model's capability to distinguish $y_w$ and $y_l$ at different optimization stage. The model's response to the data varies throughout the optimization process (\textit{c.f.} the reward at various stages in Figure~\ref{fig:figure-1}). In the early stages, the model maintains a relatively balanced performance, but it overfits to the "easy-to-distinguish" data in the later stages. This overfitting results in a reduced capacity to distinguish "hard-to-distinguish" data, thus compromising its generalization capability on more challenging cases. 

% background knowledge
% What is alignment? Example: Image Text 在第二段
% 另外intro中缺少例子：mllm中的alignment是啥？mllm中的preferred和less-preferred responses是啥，是图片里的东西，还是啥
% figure 1 修改，加一个 example
% 不接地气，直接讲 optimization 有些奇怪