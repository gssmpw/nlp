\begin{abstract}
    % This paper addresses an open problem posed in \citet{ji2020directional} by establishing a theoretical framework for deep, near-homogeneous networks trained under exponential loss using gradient flow or large stepsize gradient descent. We show that once a certain initial condition is satisfied, the normalized margin increases nearly monotonically, and the network weights converge in direction. Under additional regularity conditions, the limiting direction satisfies some Karush–Kuhn–Tucker (KKT) optimality conditions. We also demonstrate that widely used architectures, such as ResNet, are near-homogeneous under our framework. Finally, we present a toy example illustrating how the initial condition can be met, providing a concrete grounding for our theoretical results.
We establish the asymptotic implicit bias of gradient descent (GD) for generic non-homogeneous deep networks under exponential loss. Specifically, we characterize three key properties of GD iterates starting from a sufficiently small empirical risk, where the threshold is determined by a measure of the network's non-homogeneity. First, we show that a normalized margin induced by the GD iterates increases nearly monotonically. Second, we prove that while the norm of the GD iterates diverges to infinity, the iterates themselves converge in direction. Finally, we establish that this directional limit satisfies the Karush–Kuhn–Tucker (KKT) conditions of a margin maximization problem. Prior works on implicit bias have focused exclusively on homogeneous networks; in contrast, our results apply to a broad class of non-homogeneous networks satisfying a mild near-homogeneity condition. In particular, our results apply to networks with residual connections and non-homogeneous activation functions, thereby resolving an open problem posed by \citet{ji2020directional}.
\end{abstract}

\section{Introduction} \label{sec:intro}

Deep networks often have an enormous amount of parameters and are theoretically capable of \emph{overfitting} the training data. 
However, in practice, deep networks trained via \emph{gradient descent} (GD) or its variants often generalize well. 
This is commonly attributed to the \emph{implicit bias} of GD, in which GD finds a certain solution that prevents overfitting \citep{zhang2021understanding,neyshabur2017pac,bartlett2021deep}.
Understanding the implicit bias of GD is one of the central topics in deep learning theory. 


The implicit bias of GD is relatively well-understood when the network is \emph{homogeneous} \citep[see][and references therein]{soudry2018implicit,ji2018risk,lyu2020gradient,ji2020directional,wu2023implicit}. 
For linear networks trained on linearly separable data, GD diverges in norm while converging in direction to the maximum margin solution \citep{soudry2018implicit,ji2018risk,wu2023implicit}.
Similar results have been established for generic homogeneous networks that include a class of deep networks, assuming that the network at initialization can separate the training data. 
Specifically, \citet{lyu2020gradient} showed that the normalized margin induced by GD increases nearly monotonically and that the limiting direction of a subsequence of the GD iterates satisfying the Karush-Kuhn-Tucker (KKT) condition of a margin maximization problem.
Moreover, if the network is definable in an o-minimal structure (see \Cref{sec:prelim}, which is satisfied for most networks), \citet{ji2020directional} showed that \emph{gradient flow} (GF, that is, GD with infinitesimal stepsizes) converges in direction and that the limiting direction aligns with the direction of the gradient.

However, the implicit bias of GD remains largely unknown when the network is \emph{non-homogeneous}, an arguably more common case in deep learning (there are a few exceptions, which will be discussed later in \Cref{sec:related}). 
For instance, networks with residual connections or non-homogeneous activation functions are inherently non-homogeneous. 
As posted as an open problem by \citet{ji2020directional}, it is unclear whether the implicit bias results developed for homogeneous networks can be extended to non-homogeneous networks.


\paragraph{Contributions.} 
In this work, we establish the implicit bias of GD for a broad set of non-homogeneous but definable networks under exponential loss. 
Starting from the simpler case of \emph{gradient flow} (GF), we identify two natural conditions of \emph{near-homogeneity} and \emph{strong separability}, respectively.  
The former condition requires the homogeneous error to grow slower than the output of the network, and the latter condition requires GF to attain a sufficiently small empirical risk depending on the homogeneous error of the network. 
Under these two conditions, we prove the following implicit bias of GF for non-homogeneous networks:
\begin{enumerate}[leftmargin=*]
\item GF induces a normalized margin that increases nearly monotonically. 
\item GF converges in its direction while diverging in its norm. 
\item The limiting direction of GF satisfies the KKT conditions of a margin maximization problem.
\end{enumerate}
% These results extend the existing implicit bias results for homogeneous networks \citep{lyu2020gradient,ji2020directional} to a generic class of non-homogeneous networks. 
% We next briefly discuss the applicability of the two conditions.

Our near-homogeneity condition covers many commonly used deep networks. In particular, it applies to networks with residual connections and nearly homogeneous activation functions. In addition, we provide structural rules for computing the near-homogeneity order of a network based on that of each layer in the network. 

Our strong separability condition is a generalization of the separability condition used in the prior homogeneous analysis \citep{lyu2020gradient,ji2020directional}. In particular, it reduces to the separability condition when the network is homogeneous.
Later in \Cref{sec:example}, we demonstrate that this condition is satisfiable by any non-degenerate near-homogeneous network. 
Moreover, we show that GF reaches this strong separability condition from zero initialization for training a two-layer network with residual connections.

Finally, we extend the above results from GF to GD with an arbitrarily large stepsize under additional technical conditions. 
Altogether, we extend the implicit bias of GD from homogeneous cases \citep{lyu2020gradient,ji2020directional} to non-homogeneous cases, thereby addressing the open problem posted by \citet{ji2020directional}.

% \JW{add a few words/pointers about technical innovations.}

% In this work, we bridge this gap by introducing the concept of near-homogeneity (\Cref{def:nearhomo}), which allows us to analyze the implicit bias of GD across a broad class of non-homogeneous models. 
% We establish the following results for learning general near-homogeneous models with GF and arbitrary-stepsize GD under exponential loss. 



% \begin{enumerate}[leftmargin=*]
% \item \textbf{Margin Improvement.} For a near-homogeneous network $f$ trained by GD or GF, we show that a normalized margin increases nearly monotonically if the model attains a sufficiently small initial empirical risk depending on a specific measure of the network’s non-homogeneity (\Cref{thm: Margin improving and convergence} and \Cref{thm: Margin improving and convergence-gd}). Further, we derive convergence rates for the empirical risk and parameter norm along the GD or GF path. Our findings generalize the margin improvement result for homogeneous models in \citet{lyu2020gradient}.

% \item \textbf{Convergence to KKT Direction.} Furthermore, if the network $f$ is definable in some o-minimal structure, we show that its parameters learned by GD or GF converge directionally (\Cref{thm: directional convergence} and \Cref{thm: directional convergence-gd}). 
% % extending the directional convergence of homogeneous models in \citet{ji2020directional}.
% %\item \textbf{Convergence to KKT direction.} 
% Under the additional assumption that $\nabla f$ is near-homogeneous, we show that the limiting direction is a KKT point of an optimization problem that maximizes the margin of the network's homogeneization on the training data (\Cref{thm: KKT convergence} and \Cref{thm: KKT convergence-gd}). Our results extend the KKT convergence for homogeneous models \citep{ji2020directional,lyu2020gradient} to non-homogeneous settings. 
% \end{enumerate}


\paragraph{Notation.} 
For two positive-valued functions $f(x)$ and $g(x)$, we write  $f(x)\lesssim g(x)$ (or $f(x)\gtrsim g(x)$)
if $f(x) \le cg(x)$ (or $f(x) \ge cg(x)$) for some constant 
$c \in (0, +\infty)$. 
We write 
$f(x) \eqsim g(x) $ if $f(x) \lesssim g(x) \lesssim f(x)$.
We use $\Rbb_{\ge 0}[x]$ to denote the set of all univariate polynomials with non-negative real coefficients.
% \SM{Consider to use $\Rbb_{\ge 0}[\theta]$; later the polynomials are functions of $\theta$} 
For a polynomial $\homop$, we use $\deg \homop$ to denote its degree.
We use $\nabla$ to denote the gradient of a scalar function or the Jacobian of a vector-valued function. We use $\|\cdot \|$ to denote the $\ell^2$-norm of a vector or the operator norm of a matrix. We use $[n]$ to denote the set $\{1,2,\ldots,n\}$. 
We use $\text{conv} \mathcal{A}$ to denote the convex hull of a set $\mathcal{A}$ in Euclidean space. 
Throughout the paper, we define $\phi(x) := \log (1 / (nx))$ 
% \SM{Is this $\log(1/ (nx))$? } 
and $\Phi(x) := \log \phi(x) - 2 / \phi(x)$, where $n$ is the number of samples. 
% \ML{Not yet clear what $n$ is going to mean and that it will be fixed}

\subsection{Related Works} \label{sec:related} 
We discuss related papers in the remainder of this section.

\paragraph{Homogeneous Networks.}
% Most previous studies on implicit bias have focused on homogeneous models, including logistic regression and deep homogeneous networks.
We first review prior implicit bias results for deep homogeneous networks. 
% For instance, \citet{soudry2018implicit} showed that GD for logistic regression converges to the max-margin direction. \citet{wu2023implicit} proved that large stepsize GD for logistic regression also has implicit bias. 
In this case, \citet{lyu2020gradient} showed that GD induces a nearly increasing normalized margin, and the direction of a subsequence of GD iterates converges, the limit of which can be characterized by the KKT conditions of a margin maximization problem. 
Part of these results are generalized to \emph{steepest descent} by  \citet{tsilivis2024flavors}.
Using the notion of definability, \citet{ji2020directional} further showed the directional convergence and alignment for gradient flow. 
In the special case of two-layer homogeneous networks, \citet{chizat2020implicit} characterized the limiting direction of (Wasserstein) gradient flow in the large-width limit.
For a two-layer Leaky ReLU network with symmetric data, \citet{lyu2021gradient} showed that gradient flow eventually converges to a linear classifier. 
Different from these works, we aim to establish implicit bias of GD for non-homogeneous deep networks. 
% However, all these studies assume homogeneous architectures, which excludes many practical network architectures. In contrast, our near-homogeneous framework accommodates many network architectures, including residual connections and non-homogeneous activations.

\paragraph{Non-homogeneous Networks.}
Before our work, there are a few papers that extend the implicit bias results from homogeneous cases to certain special non-homogeneous cases \citep{nacson2019lexicographic,chatterji2021does,kunin2023asymmetric,cai2024large}.
The works by \citet{nacson2019lexicographic,kunin2023asymmetric} considered a special non-homogeneous network, which is homogeneous when viewed as a function of a subset of the trainable parameters while other parameters are fixed. The homogeneity orders for different subsets of parameters might be different. However, their results cannot cover networks with many commonly used non-homogeneous activation functions.
% that can be decomposed into the sum of multiple homogeneous networks (with possibly different homogeneous orders). 
% while we consider more general non-homogeneous models and the optimization path. 
The work by \citet{chatterji2021does} showed the margin improvement for GD with small stepsizes for MLPs with a special type of near-homogeneous activation functions. As a consequence, their results do not allow networks that use general non-homogeneous activation functions or residual connections. 
In comparison, we handle a large class of non-homogeneous networks satisfying a natural definition of near-homogeneity, covering far more commonly used deep networks.
% —an instance of near-homogeneous models—and showed the normalized margin will increase under GD. However, they require the homogeneous error of the model to be sufficiently small relative to the initial. In contrast, we allow it to be as large as a polynomial in the parameter norm. Moreover, we also characterize the limit of parameter directions.
% \citet{le2022training}
% analyzed non-homogeneous networks restricted to a few nonlinear layers, which again is a special case of near-homogeneous models.
% The work by \citet{kunin2023asymmetric} proposed a notion of quasi-homogeneous architectures, also a subset of near-homogeneous models, but assumed the limiting parameter direction exists. 

% By contrast, we prove the convergence of parameter directions in a more general setting.
The work by \citet{cai2024large} is most relevant to us, in which they proved the margin improvement of GD for near-$1$-homogeneous networks (see their Assumption 1 and our \Cref{def:nearhomo} in \Cref{sec:prelim}).
Our work can be viewed as an extension of theirs by handling near-$M$-homogeneous networks for general $M \ge 1$, as well as proving that GF and large-stepsize GD converge in direction to the KKT point of a margin maximization problem. 

% directly motivated our work by showing that for near-1-homogeneous models, the normalized margin increases under gradient descent. Our approach, however, goes further by accommodating deeper non-homogeneous networks and characterizing their limiting parameter directions.

% \paragraph{Other Related Works.}\JW{please revise}
% The work by \citet{davis2020stochastic} introduced the tools of \emph{definability} for studying the convergence of the stochastic \emph{subgradient} method on tame functions, which could be non-homogeneous. 
% Their tools are later used by \citet{ji2020directional} for establishing the directional convergence of gradient flow for homogeneous networks. 
% Although our focus on studying implicit bias is quite different from \citet{davis2020stochastic}, their tools are fundamental for us to deal with subgradients. 
% which includes the definable functions in an o-minimal structure. Hence, their results can be applied to our near-homogeneous models. Although we also leverage the o-minimal structure to study the parameter limit, we focus on the implicit bias under a realizable setting in which the parameter norm diverges, in contrast to their assumption that the parameters remain bounded.

% In what follows, we make a detailed discussion about ppaers that directly motivate our work \citet{lyu2021gradient,ji2020directional,cai2024large}.

% \paragraph{Comparison with \citet{lyu2020gradient}.}

% \paragraph{Comparison with \citet{ji2020directional}.}

% \paragraph{Comparison with \citet{cai2024large}.}

\section{Preliminaries} \label{sec:prelim}

% In this section, we first introduce the notions of Clarke subdifferential and o-minimal structure. Based on these definitions, we describe the data set, the gradient flow, and the near-homogeneous assumptions. 
In this section, we set up the problem and introduce basic mathematical tools used in our analysis.


\paragraph{Locally Lipschitz Functions and Clarke Subdifferential.} 
For a function $f: D \to \Rbb$ defined on an open set $D$, we say $f$ is \emph{locally Lipschitz} if, for every $x\in D$, there exists a neighborhood $U$ of $x$ such that $f|_U$ is Lipschitz continuous.  
By Rademacher's theorem, a locally Lipschitz function is differentiable almost everywhere \citep{borwein2000convex}. 
The {\it Clarke subdifferential} of a locally Lipschitz function $f$ at $x\in D$ is defined as 
\[
\begin{aligned}
\partial f(x) \coloneqq \text{conv} \bigg\{ 
 \lim_{i\to \infty} \nabla f(x_i) : x= \lim_{i\to \infty} x_i, 
\text{where}\ x_i \in D \ \text{and}\ \nabla f(x_i) \ \text{exists}
  \bigg\},
\end{aligned}
\]
which is nonempty, convex, and compact \citep{clarke1975generalized}. 
In particular, if $f$ is continuously differentiable at $x$, then $\partial f(x) = \{\nabla f(x)\}$. 
Elements of $\partial f(x)$ are called {\it subgradients}. 
% In our setting, we only consider $\partial_{\param} f(\param;\xB)$, the Clarke subdifferential of the network $f$ with respect to $\param$.

% \SM{If $f$ is a function of $(\theta, x)$, does local-Lipschitz requirement apply to both $(\theta, x)$? }


\paragraph{Gradient Flow.} 
Let $(\xB_i, y_i)_{i=1}^n$ be a binary classification dataset, where $\xB_i \in \Rbb^d$ and $y_i \in \{\pm 1\}$ for all $i\in [n]$.  We denote a network by $f(\param; \cdot) : \Rbb^d \to \Rbb$, where $\param \in \Rbb^D$ are the trainable parameters.   
% We make the following assumption on the network $f$.
Thoughout the paper, we assume $f(\param;\xB_i)$ is locally Lipschitz with respect to $\param$ for every $i \in [n]$.
We focus on the empirical risk under the exponential loss defined as 
\begin{equation}
\label{eq:loss}
        \Loss \big( \param \big) \coloneqq \frac{1}{n} \sum_{i=1}^n \ell \big( y_i f(\param; \xB_i) \big),\quad  \ell(x) := e^{-x}.
        % =  \frac{1}{n} \sum_{i=1}^n \ell \big( \bar f_i(\param) \big),
\end{equation}
A curve $z$ from an interval $I$ to some Euclidean space $\Rbb^m$ is called an {\it arc} if it is absolutely continuous on any compact subinterval of $I$. 
% An arc is almost everywhere differentiable. 
Clearly, the composition of an arc and a locally Lipschitz function is still an arc. 
 % \JW{Follwing (cite a papaer)}, 
Following \citet{lyu2020gradient,ji2020directional},
we define \emph{gradient flow} as an arc from $[0, + \infty)$ to $\Rbb^D$ that satisfies
\begin{equation}
    \label{eq:GF}
    \tag{GF}
\frac{\mathrm{d} \param_t}{ \mathrm{d} t} \in - \partial \Loss \big( \param_t\big), \quad \text{for almost every}\ t\ge 0. 
\end{equation}


\paragraph{Homogeneity and Near-Homogeneity.} 
% We also impose the following near-homogeneity assumption for our network $f$, extending the homogeneity conditions used in \citet{lyu2020gradient,ji2020directional}. 
Let $M\ge 1$ be an integer. 
Recall that a locally Lipschitz network $f(\param; \xB)$ is \emph{$M$-homogeneous} \citep{lyu2020gradient,ji2020directional} if for every $\xB\in(\xB_i)_{i=1}^n$,
% Recall that \Cref{eq:homogeneous} is equivalent to 
\begin{equation}\label{eq:homogeneous}
\text{for all $a>0$ and $\param\in\Rbb^D$},\ f(a \param; \xB) = a^M f(\param; \xB).
\end{equation}
% \JW{can someone double-check this?}
One can verify that the above is equivalent to:
for every $\xB\in(\xB_i)_{i=1}^n$, $\thetaB\in\Rbb^D$, and $\boldsymbol{h} \in  \partial_{\param} f(\param ;\xB)$,
\begin{equation}
\label{eq:homo-2}
\langle \boldsymbol{h}, \param \rangle - M  f(\param ;\xB) = 0.
\end{equation}
We generalize this definition by introducing the following near-homogeneous conditions. 
\begin{definition}[Near-$M$-homogeneity]
    \label{def:nearhomo}
Let $M\ge 1$ be an integer. A network $f(\param; \xB)$ is called \emph{near-$M$-homogeneous}, if there exist $\homop, \homoq \in \Rbb_{\ge 0} [x]$ with $\deg \homop, \deg \homoq \le M$ such that the following holds for every $\xB\in(\xB_i)_{i=1}^n$, $\thetaB\in\Rbb^D$, and $\boldsymbol{h} \in  \partial_{\param} f(\param ;\xB)$:
% \begin{itemize}
\begin{assumpenum}
\item 
% \textbf{$M$-Near homogeneity.}
% \label{asp:nearhomo:near-homo}
$|\langle \boldsymbol{h}, \param \rangle - M  f(\param ;\xB)| \le \homop^\prime (\|\param\|)$;
    
\item 
% \textbf{$M$-bounded gradient.}  
% \label{asp:nearhomo:bound-grad}
$\| \boldsymbol{h} \| \le \homoq^\prime (\|\param\|)$;

\item 
% \textbf{$M$-bounded value.} 
% \label{asp:nearhomo:bound-val}
$|f(\param;\xB)| \le \homoq(\|\param\|)$.  
\end{assumpenum}
\end{definition}

We make a few remarks on \Cref{def:nearhomo}. 
% \JW{add a discussion of why we measure homo-error by the second definition.}
First, our near-homogeneity condition is modified from \Cref{eq:homo-2} instead of \Cref{eq:homogeneous}. In this way, our near-homogeneity condition implicitly puts regularity conditions on the subgradient of the network, which will be useful in our analysis.  

Second, every $M$-homogeneous (see \Cref{eq:homogeneous}) network for $M\ge 1$ is also near-$M$-homogeneous. 
% \JW{revise}1
We see this by setting $\homop(x)=0$ and $\homoq(x) = C (x^M+1)$ for a sufficiently large constant $C>1$ in \Cref{def:nearhomo}. 

Third, a near-$M$-homogeneous network is also near-$(M+1)$-homogeneous according to \Cref{def:nearhomo}. Thus when we say a network is near-$M$-homogeneous, the degree $M$ should be interpreted as the minimum degree $M$ such that \Cref{def:nearhomo} is satisfied. 
We will see that this interpretation is necessary when we introduce the strong separability condition in \Cref{sec:margin-direct}.
% so it is natural to consider the minimal degree $M$. 
% In fact, this assumption --- together with \Cref{asp:initial-cond-gf} on the initial empirical risk --- uniquely determines the near-homogeneous degree $M$ of $f$, as shown in \Cref{lem: Sanity check for gradient flow}.

Finally, we point out that many commonly used deep networks are near-homogeneous but not homogeneous. Examples include networks using residual connections or non-homogeneous activation functions.
% compared to the homogeneous assumption, a wider class of practical neural networks, including residual networks, satisfy the near-homogeneous assumption. 
This will be further elaborated in \Cref{sec:near-homo-nn}. 


% Recall that the exponential map is locally Lipschitz and definable. 
% For simplicity, we denote $\bar f_i(\param) = y_i f(\param; \xB_i)$.
% and $\ell(x) = e^{-x}$.  The exponential loss is locally Lipschitz and definable.


\paragraph{O-minimal Structure and Definable Functions.}
% \ML{Functions?}
% O-minimal structure is a collection $S= (S_n)_{n=1}^\infty $, where $S_n$ is the subset of $\Rbb^n$ that includes all algebraic sets. 
The o-minimal structure and definable functions 
% \ML{?} 
were introduced to deep learning theory by \citet{davis2020stochastic,ji2020directional} for studying the convergence and implicit bias of subgradient methods. 
% \JW{someone check? is there an earlier ref for DL} 
We briefly review these notions below.

An o-minimal structure is a collection $\Scal = (\Scal_n)_{n=1}^\infty$, where each $\Scal_n$ is a collection of subsets of $\Rbb^n$ that satisfies the following properties. First, $\Scal_n$ contains all algebraic sets in $\Rbb^n$, i.e., zero sets of real-coefficient polynomials on $\Rbb^n$. Second, $\Scal_n$ is closed under finite union, finite intersection, complement, Cartesian product, and projection. Third, $\Scal_1$ consists of finite unions of open intervals and points in $\Rbb^1$. A set $A \subset \Rbb^n$ is {\it definable} if $A \in \Scal_n$. A function $f: D \to \Rbb^m$ with $D \subset \Rbb^n$ is {\it definable} if its graph is in $\Scal_{n+m}$. See \Cref{sec:o-minimal} for a more detailed introduction.
% \SM{It seems that there is a maximal o-minimal structure. Can we just define "definable" is with respect to the maximal o-minimal structure?} 

By the definition of o-minimal structure, the set of definable functions is closed under algebraic operations, composition, inversion, and taking maxima or minima. 
The work by \citet{wilkie1996model} established the existence of an o-minimal structure in which polynomials and the exponential function are definable. 
By the closure property, commonly used mappings in deep learning are all definable with respect to this o-minimal structure, including fully connected layers, convolutional layers, ReLU activation, max-pooling layers, residual connections, and cross-entropy loss. We refer the readers to \citet{ji2020directional} for more discussion. 


% \SM{Are they definable with respect to the o-minimal structure by \citet{wilkie1996model}, but there exists another o-minimal structure on which they are definable?} 

Throughout this paper, we assume that 
% \begin{assumption} [Definable network]
% \JW{move this to preliminary since it's used everywhere?}
% \label{asp:definable}
there exists an o-minimal structure such that $t\mapsto\exp(t)$ and the network $\param\mapsto f(\param; \xB_i)$, $i=1,\dots,n$, are all definable. 
% \SM{Instead of existence, can we just define "definable" to be with respect to the maximal o-minimal structure?}
    % For each $\xB\in (\xB_i)_{i\in[n]}$, the network $f(\param; \xB)$ as a function of $\param$ is definable in some o-minimal structure, including the exponential function.\JW{not clear} 
% \end{assumption}
% For simplicity, we assume throughout this paper that all models and network architectures are definable in this o-minimal structure. 
This assumption, together with the local Lipschitzness, allows us to apply the chain rule (see \Cref{lem:chain-rule-clark} in \Cref{sec:o-minimal}) to GF defined by subgradients. 
% enabling us to analyze important quantities during the trajectory. 
Moreover, this assumption allows us to leverage the desingularizing function (see \Cref{def: designularizing function} in \Cref{sec:proof:direct}, and \citet{ji2020directional}) to show the directional convergence. 
% By contrast, \citet{lyu2020gradient} did not adopt this assumption; they had to assume the applicability of the chain rule and they failed to prove directional convergence.
% Moreover, they could not show directional convergence, whereas we prove it here in \Cref{thm: directional convergence,thm: directional convergence-gd}.



% \JW{make this a global assumptions. This allows two benefits: (1) chain rule, (2) directional convergence by desingular function etcc.. Disucss (1) is an assumption in Lyu and Li...}

% \paragraph{Notations.} 
% % To simplify the presentation, we use the following notations. 
% We use $\nabla$ as a shorthand of $\nabla_{\thetaB}$, that is, gradient operator with respect to $\thetaB$. 
% We define $\LinkFun(x):= \log 1/(nx)$ for $x>0$. 
% We use $f_i(\param)$ to denote $f(\param; \xB_i)$, $\bar f_i(\param)$ to denote $y_i f_i(\param)$, and $\bar f_{\min}(\param)$ to denote $\min_{i\in[n]} \bar f_i(\param)$. 
% Similarly, for functions with a lower index, such as $\homoPredictor$, we use $\bar f_{M,i}(\param )$ to denote $y_i \homoPredictor(\param ;\xB_i)$, and $\bar f_{M,\min}(\param)$ to denote $\min_{i\in [n]} \bar f_{M,i}(\param )$.


\section{Implicit Bias of Gradient Flow} \label{sec:margin-direct}
In this section, we establish the implicit bias of gradient flow for near-homogeneous networks. 
% In this part, we show the margin improvement of GF. 
Our first assumption is that the network is near-$M$-homogeneous (see \Cref{def:nearhomo}). 
% Now we introduce two near-homogeneous assumptions for dealing with gradient flow. \SM{We now introduce two technical assumptions essential for our main theorem on the risk convergence of gradient flow.}
\begin{assumption}[Near-homogeneous network]
\label{asp:nearhomo}
Let $M\ge 1$.
Assume that the network $f$ is near-$M$-homogeneous with polynomials $\homop(\cdot)$ and $\homoq(\cdot)$. 
\end{assumption}

% \JW{revise}
% Note that we require $M\ge 2$ in the above assumption. This is because we need to assume $\grad f$ is near-$(M-1)$-homogeneous (see \Cref{asp:strongerhomo}) in proving the KKT convergence. When $M=1$, this assumption on $\grad f$ implies that the network $f$ must be a piecewise linear function, which is less interesting and requires separate treatment. The nontrivial near-$1$-homogeneous case is solved by \citet{cai2024large}, where they made some other assumptions on the smoothness of the network instead of \Cref{asp:strongerhomo}. We leave it as future work to unify our results. 

Under \Cref{asp:nearhomo}, let
$\homop(x) := \sum_{i=0}^M a_i x^i$. The following function is handy for our presentation: 
% \JW{unify two cases}
% \JW{check for $M=1$.}

\begin{comment}
\begin{equation}
\label{eq:def-pa}
\homop_a (x) := 
\begin{cases}
\sum_{i=1}^{M-1} \frac{(i+1)a_{i+1}}{M-i} x^{i} + \frac{a_1}{M-1}, &\text{ if }  M\ge 2 ;\\
a_1 + 1, &\text{ if } M=1 .
\end{cases}
\end{equation}
\end{comment}

\begin{equation}\label{eq:def-pa}
\homop_a (x) := \sum_{i=1}^{M-1} \frac{(i+1)a_{i+1}}{M-i} x^{i} + \frac{a_1}{M-1/2}.
\end{equation}


\paragraph{Normalized and Modified Margins.}
% For a near-$M$-homogeneous network $f(\param; \cdot)$, 
Under \Cref{asp:nearhomo}, we define the \emph{normalized margin} \citep{lyu2020gradient} as 
\begin{equation}
\label{eq: normalized margin}
    \Normalmargin(\param) \coloneqq  \min_{i \in [n]}\frac{ y_i f(\param;\xB_i)}{\paramNorm^M}.
\end{equation}
% \SM{Now the assumption is a mixture of assumption and definition. Consider to pull out the definition of $\homop_a$ from the assumption. }
The normalized margin is hard to analyze directly due to the hard minimum in its definition. Instead, we analyze a \emph{modified margin}, which increases monotonically and approximates the normalized margin well. Specifically, the \emph{modified margin} is defined as
% introduce the following {\it modified margin}. We will show the modified margin improves and serves as a good approximation of 
% \SM{Just looking at the equation below, it is not immediate to see how modified margin (the numerator) is related to the margin. Consider to add a sentence to explain that they are close to each other which will be explained later.}
\begin{equation}
\label{eq: modified margin}
        \GFmargin(\param) \coloneqq \frac{\phi\big(\Loss(\param)\big) - \homop_a(\paramNorm)}{\paramNorm^M},
\end{equation}
where $\phi(x) := \log\big(1/(nx)\big)$ and $\homop_a$ is given by \Cref{eq:def-pa}. 

% When the margin $\min_{i\in [n]} y_i f(\param;\xB_i)$ is large. Under exponential loss, 
In the definition of the modified margin, the $\phi(\Loss(\param))$ term produces a soft minimum of $(y_i f(\param; \xB_i))_{i=1}^n$, which approximates the hard margin $\min_{i\in [n]} y_i f(\param;\xB_i)$. This idea appears in prior analysis for homogeneous networks \citep{lyu2020gradient,ji2020directional}. 
The offset term $\homop_a(\paramNorm)$ is our innovation, which controls the homogeneous error when the network is non-homogeneous. 
Note that as $\|\param\|$ grows, the offset term $\homop_a(\paramNorm)$ grows slower than the main term $\phi(\Loss(\param))$ according to \Cref{def:nearhomo}. Therefore our modified margin is a good approximation of the normalized margin. 
% Further since $\deg (\homop_a) < M$, the term $\homop_a (\| \param \|)$ is dominated by $\min_{i\in [n]} y_i f(\param;\xB_i)$. This explains why $\GFmargin(\param)$ is a good approximation to $\Normalmargin(\param)$ for large margins.
% \JW{check $M=1$}
%\KZ{The definition of $\homop_a$ has changed.}
% Given the two margins, the following theorem gives sufficient initial bound condition for the normalized margin of the near-homogeneous models to be nearly increasing, and further characterizes the convergence rate of the loss $\Loss(\param_t)$ and the parameter norm $\| \param_t \|$. The proof of \Cref{thm: Margin improving and convergence} is deferred to \Cref{sec:proof:margin}. 

% We introduce the following initial condition on the loss: 
% The following strong separability condition is crucial to our analysis.
Our second assumption ensures GF can reach a state in which the network strongly separates the data.
\begin{assumption}[Strong separability condition]
\label{asp:initial-cond-gf}
Let $f(\param;\xB)$ be a network satisfying \Cref{asp:nearhomo}.
% with $(M,\homop, \homoq)$, 
Assume that there exists a time $s>0$ such that $\param_s$ given by \Cref{eq:GF} satisfies 
\begin{equation}
\label{eq: initial-cond-gf}
    \Loss(\param_s) < 
    % \begin{dcases}
        e^{-\homop_a(\|\param_s\|)}/n,
    % \end{dcases}
\end{equation}
where $\homop_a$ is defined in \Cref{eq:def-pa} and $n$ is the number of samples.
\end{assumption}

Our \Cref{asp:initial-cond-gf} is a natural extension of the separability condition (that is, \(\Loss(\param_s) < 1/n\) for some $s$) used in the analysis for homogeneous networks \citep{lyu2020gradient,ji2020directional}. Specifically, for homogeneous networks, we can set $\homop=0$, in which $\homop_a=0$ by its definition in \Cref{eq:def-pa}. Then \Cref{asp:initial-cond-gf} reduces to the separability condition.% \(\Loss(\param_s) < 1/n\).

For non-homogeneous networks, \Cref{asp:initial-cond-gf} requires GF to attain an empirical risk that is sufficiently small compared to a function of the homogeneous error. Note that this condition can be satisfied by any non-degenerate near-homogeneous network that is able to separate the training data, which will be discussed further in \Cref{sec:example}.

% the extra factor of $e^{-\homop_a(\|\param_s\|)}$ in  allows us to control 
\Cref{asp:nearhomo,asp:initial-cond-gf} together force the ``leading term'' in the network to be exactly $M$-homogeneous. Therefore, the near-homogeneity order in \Cref{def:nearhomo} must be understood as the minimum possible one. 
This is precisely explained in the following lemma, whose proof is deferred to \Cref{sec:proof: sanity check gf}.

%────────────────────────────────────────
\begin{lemma}
[Near-homogeneity order]
\label{lem: Sanity check for gradient flow}
Let $f$ be such that
\[
    f(\param; \xB) = \sum_{i=0}^\infty f^{(i)}(\param;\xB),
\]
where $f^{(i)}(\param;\xB)$ is $i$-homogeneous with respect to $\param$. 
If $f$ satisfies \Cref{asp:nearhomo,asp:initial-cond-gf}, 
% with $(M, \homop, \homoq)$, 
then for every $j\in[n]$, we must have 
\begin{align*}
    f^{(i)}(\cdot;\xB_j) \begin{cases}
        = 0, & \text{if } i>M, \\
        \ne 0, & \text{if } i=M.
    \end{cases}
\end{align*}
% $f^{(i)}(\param;\xB)\equiv 0$ for all $i>M$ and $f^{(M)}(\param;\xB) \not \equiv 0$. 
Furthermore, we have $f^{(M)}(\param_s; \xB_j) >0$ for all $j\in [n]$. 
% \SM{$i$ is used for two meanings. Use $j$ instead of $i$ in $f^{(i)}$. }
% \JW{$\xB$ should be $\xB_i$?}
\end{lemma}
%────────────────────────────────────────


% Now we can state our main result for the gradient flow. This theorem characterizes the convergence rate of the loss and the parameter norm, and the margin improvement of the gradient flow. The proof is deferred to \Cref{sec:proof:margin}.
% The following theorem establishes that the normalized margin $\Normalmargin (\param_t)$ is nearly increasing, and further characterizes the convergence rate of the loss $\Loss(\param_t)$ and the parameter norm $\| \param_t \|$.
\paragraph{Margin Improvement.}
We are ready to present our first main theorem on the margin improvement of GF.
The proof is deferred to \Cref{sec:proof:margin}. 
%────────────────────────────────────────
\begin{theorem}
[Risk convergence and margin improvement]
\label{thm: Margin improving and convergence}
Suppose that \Cref{asp:nearhomo,asp:initial-cond-gf} hold. 
For $(\param_t)_{t>s}$ given by  \Cref{eq:GF}, we have:
% Let $f(\param;\xB)$ be a locally Lipschitz network satisfying \Cref{asp:nearhomo,asp:initial-cond-gf,asp:definable}. 
% We have the following results for the \Cref{eq:GF} regime: 
\begin{itemize}[leftmargin=*]
\item %(Convergence rates) 
For all $t>s$, the risk and the parameter norm satisfy
% \SM{Consider to spell out what parameters $\Theta$ depends on.}
\begin{equation*}
     \Loss(\param_t) < e^{-\homop_a(\paramNormt)}/n.
\end{equation*}
Furthermore, %as $t \to \infty$, 
we have
\begin{align*}
\Loss(\param_t) \eqsim  \frac{1}{t(\log t)^{2-2/M}},\quad 
\paramNormt \eqsim  (\log t)^{\frac{1}{M}} ,
\end{align*}
% \begin{itemize}
% \item $\Loss(\param_t) < e^{-\homop_a(\paramNormt)}/n$
% \item $\Loss(\param_t) \eqsim  \frac{1}{t(\log t)^{2-2/M}} $.
% \item $\paramNormt \eqsim  (\log t)^{\frac{1}{M}} $,
% % \item $\Loss(\param_t) = \Theta \Big( \frac{1}{t(\log t)^{2-2/M}} \Big)$.
% % \item $\paramNormt = \Theta \big( (\log t)^{\frac{1}{M}} \big)$.
% \end{itemize}
% Note the hidden constants
where $\eqsim$ hides constants that depend on $M$, $\GFmargin(\param_s)$, and coefficients of $\homoq$.
\item %(Margin improvement) 
The modified margin $\GFmargin(\param_t)$ is positive, increasing, and upper bounded. Moreover,  
$\GFmargin(\param_t)$ is an $\epsilon_t$-multiplicative approximation of $\Normalmargin(\param_t)$, that is, 
\[
\GFmargin(\param_t) \le \Normalmargin(\param_t) \le \big(1+\epsilon_t\big) \cdot \GFmargin(\param_t),\quad  \text{for all}\ t>s, 
\]
where 
\[\epsilon_t :=  \frac{\log n + \homop_a(\paramNormt)}{\LinkFun(\Loss(\param_t))-\homop_a(\paramNormt)}  =\Ocal \big((\log t)^{-1/M}\big)\to 0.\] 
\end{itemize}
\end{theorem}
%────────────────────────────────────────

This result generalizes Theorem 4.1 in \citet{lyu2020gradient} from homogeneous networks to near-$M$-homogeneous networks for $M\ge 1$.
In particular, we recover their results when the network is $M$-homogeneous, in which we set $\homop_a(x) = 0$. When the network is non-homogeneous, our \Cref{asp:initial-cond-gf} is stronger than the separability condition in \citet{lyu2020gradient}. This is one of the key conditions that enables our analysis for non-homogeneous networks.
% Compared to their work, we are able to handle a broader class of models, while imposing a stronger condition on the initial risk. 



\paragraph{Directional Convergence.}
% In this part, we show the directional convergence of GF. 
% We use $\tilde{\param}_t \coloneqq \param_t / \|\param_t\|$ to denote the direction of GF path. 
% The theorem below establishes directional convergence of gradient flow for near-$M$-homogeneous and definable network $f$. 
Our next theorem establishes the directional convergence of GF for non-homogeneous networks. The proof is deferred to \Cref{sec:proof:direct}.

\begin{theorem}[Directional convergence]
\label{thm: directional convergence}
% Let $f(\param;\xB)$ be a locally Lipschitz network satisfying \Cref{asp:nearhomo,asp:definable,asp:initial-cond-gf}.
Under the setting of \Cref{thm: Margin improving and convergence}, let $\tilde{\param}_t \coloneqq \param_t / \|\param_t\|$ be the direction of  \Cref{eq:GF}. Then the curve swept by $\tilde{\param}_t$ has finite length. Therefore, 
% \SM{Does a curve having finite length imply the limit exists?}
the directional limit $\param_* \coloneqq \lim_{t\to \infty} \tilde{\param}_t$ exists.
% and thus $\tilde{\param}_t$ converges. 
\end{theorem}


% \JW{update}
Our \Cref{thm: directional convergence} extends Theorem 3.1 in \citet{ji2020directional} from $M$-homogeneous networks to near-$M$-homogeneous networks for $M\ge 1$. 
% \JW{check..}
Our \Cref{asp:nearhomo,asp:initial-cond-gf} allow the application of 
tools (specifically the desingularizing functions) from \citet{ji2020directional} for showing the direction convergence in the non-homogeneous cases. 
% To handle non-homogeneity, we utilize the chain rules for definable functions and invoke a desingularizing function for the trajectory; see \Cref{sec:proof:direct} for more details.
% \KZ{Does this mean their assumptions are stronger than ours or weaker?}

% Meanwhile, \citet{ji2020directional} also established gradient alignment under more regularity assumptions; investigating this property for near-homogeneous models remains for future work.

\paragraph{KKT Convergence.}
% Having established directional convergence, 
% The preceding theorem establishes 
Provided with the directional convergence of GF,  
our next step is to characterize the limiting direction.
To this end, we need to understand the asymptotic behavior of the near-homogeneous network $f$ as $\|\param\| \to \infty$. 
Since $f$ is near-$M$-homogeneous, one can expect that $f$ is close to an $M$-homogeneous function for large $\param$. Motivated by this, we define the \emph{homogenization} of $f$ as 
% When the network is $M$-homogeneous, \JW{Theorem XX in} \citet{lyu2020gradient} shows that a subsequence of GF convergences in direction to the KKT point of the following margin maximization problem.
% % . Since $\homoPredictor$ is $M$-homogeneous, we can consider the optimization problem: 
% \begin{equation*}
% % \label{eq: KKT}
%     \mbox{minimize} \, \| \param\|^2, \quad \text{s.t.} \, \min_{i \in [n]} y_i f(\param;\xB_i) \ge 1. 
% \end{equation*}
% The above problem is well-defined when $f$ is $M$-homogeneous (recall the definition in \Cref{eq:homogeneous}). \JW{discuss more?}
% % There, the margin maximization problem is well defined thanks to the 

% However, due to non-homogeneity, the corresponding optimization problem cannot be defined directly via the network $f$ \JW{why?}. Nevertheless, we can consider $\homoPredictor$, the homogenization of our near-$M$-homogeneous network:
\begin{equation}
\label{eq: homogenized f}
\homoPredictor (\param; \xB) \coloneqq \lim_{r \to +\infty} \frac{f(r \param; \xB)}{r^M}.
\end{equation}
The well-definedness, continuity, and differentiability of $\homoPredictor$ are guaranteed by the near-homogeneity of $f$ (see \Cref{thm:homogenization} in \Cref{sec:example}). 
% We defer the proof of these properties to \Cref{sec:homogeneization}. \ML{Missing some transition here?}
% Given $\homoPredictor$, 
We will show that the limiting direction of GF satisfies the KKT conditions of the following margin maximization problem:
\begin{equation}
\label{eq: KKT}
    \mbox{minimize} \, \| \param\|^2, \quad \text{s.t.} \, \min_{i \in [n]} y_i \homoPredictor(\param;\xB_i) \ge 1. \tag{P}
\end{equation}
This generalizes the margin maximization problem in \citep{lyu2020gradient} from homogeneous to non-homogeneous cases.
It is worth noting that when $f$ is already homogeneous, \Cref{eq: KKT} reduces to the same optimization problem as that in \citep{lyu2020gradient}.

To ensure the limiting direction satisfies the KKT conditions of \Cref{eq: KKT}, we need to compare the gradients of $f$ and $\homoPredictor$. This comparison requires an additional regularity assumption.

\begin{comment}
\begin{assumption}[Near-homogeneous gradient]
\label{asp:strongerhomo}
Assume that the network $f(\param;\xB)$ is differentiable with respect to $\param$ for $\xB \in (\xB_i)_{i=1}^n$. \JW{check $M=1$ case?}
Moreover, assume that $M\ge 2$ and each component of its gradient, $[\nabla f(\param; \xB)]_j$ for $j=1,\dots,D$, 
is near-$(M-1)$-homogeneous with polynomials $\homor(\cdot)$ and $\homos(\cdot)$.
\end{assumption}
\end{comment}

\begin{assumption}[Weak-homogeneous gradient]
\label{asp:strongerhomo}
Assume that the network $f(\param;\xB)$ is continuously differentiable with respect to $\param$ for $\xB \in (\xB_i)_{i=1}^n$ and that $\lim_{r \to \infty} {\nabla f (r \param; \xB)}/{r^{M-1}}$ exists for all $\param$ and $\xB \in (\xB_i)_{i=1}^n$. Assume that the limit
% and that $\grad f (\param; \xB) $
% $\grad f (\param; \xB) := \grad_{\param} f (\param; \xB)$ 
% uniformly converges to its $(M-1)$-homogenization $(\nabla f)_{\homo} (\param; \xB)$ as $\| \param \| \to \infty$. Namely, for all $\xB \in (\xB_i)_{i=1}^n$ and $\param \in \Rbb^d$,
\begin{equation*}
    (\nabla f)_{\homo} (\param; \xB) := \lim_{r \to \infty} \frac{\nabla f (r \param; \xB)}{r^{M-1}}
\end{equation*}
satisfies 
\begin{equation*}
    \left\vert \nabla f(\param; \xB) - (\nabla f)_\homo (\param; \xB) \right\vert \le \homor \left( \| \param \| \right),
\end{equation*}
where $\homor: \Rbb_{\ge 0} \to \Rbb_{\ge 0}$ is a function such that
\begin{equation*}
    \lim_{x \to + \infty} \frac{\homor(x)}{x^{M - 1}} = 0.
\end{equation*}
\end{assumption}

% \begin{assumption}[Weak-homogeneous gradient]
% \label{asp:strongerhomo}
% Assume that the network $f(\param;\xB)$ is continuously differentiable with respect to $\param$ for $\xB \in (\xB_i)_{i=1}^n$, and that $\grad f (\param; \xB) $
% % $\grad f (\param; \xB) := \grad_{\param} f (\param; \xB)$ 
% uniformly converges to its $(M-1)$-homogenization $(\nabla f)_{\homo} (\param; \xB)$ as $\| \param \| \to \infty$. Namely, for all $\xB \in (\xB_i)_{i=1}^n$ and $\param \in \Rbb^d$,
% \begin{equation}
%     (\nabla f)_{\homo} (\param; \xB) = \lim_{r \to \infty} \frac{\nabla f (r \param; \xB)}{r^{M-1}}
% \end{equation}
% exists, and
% \begin{equation}
%     \left\vert \nabla f(\param; \xB) - (\nabla f)_\homo (\param; \xB) \right\vert \le \homor \left( \| \param \| \right),
% \end{equation}
% where $\homor: \Rbb_{\ge 0} \to \Rbb_{\ge 0}$ is a function satisfying
% \begin{equation}
%     \lim_{x \to + \infty} \frac{\homor(x)}{x^{M - 1}} = 0.
% \end{equation}
% \end{assumption}

% We can show that for any differentiable $f$, if $\nabla f$ satisfies \Cref{asp:strongerhomo}, then $f$ satisfies \Cref{asp:nearhomo}. 
% \KZ{Do we still need this lemma C.3?} 

It is worth noting that for $M \ge 2$, \Cref{asp:nearhomo,asp:strongerhomo} are satisfied as long as $\nabla_{\param} f(\param;\xB)$ is component-wise near-$(M-1)$-homogeneous. See \Cref{lem:near-homo gradients lead to near-homo functions} for a precise statement. In comparison, our \Cref{asp:strongerhomo} is less restrictive and allows for $M=1$.
% By \Cref{lem:near-homo gradients lead to near-homo functions}, \Cref{asp:strongerhomo} implies \Cref{asp:nearhomo}.
% \JW{can we elaborate a bit?}
% To be consistent with our preceding results, when $f$ satisfies \Cref{asp:strongerhomo}, we still use $\homop(\cdot)$ and $\homoq(\cdot)$ to denote the polynomials with which $f$ satisfies \Cref{asp:nearhomo}. 
% We will still use $\homop(\cdot)$ and $\homoq(\cdot)$ to denote the polynomials for the near-homogeneity of $f$, when $\nabla f$ satisfies \Cref{asp:strongerhomo}. 
% \SM{Is this "without loss of generality" necessary? If we assume Assumption 1 and 4, it is already clear what are $\homop$ and $\homoq$. Change "Without loss of generality" to "As a reminder"?}
% \JW{how are $r$ and $s$ related to the polynomials for the strong separability assumptions? }
The next theorem shows that the limiting direction of GF satisfies the KKT conditions of \eqref{eq: KKT}, with its proof included in \Cref{sec:proof:KKT}. 

\begin{theorem}[KKT convergence]
\label{thm: KKT convergence} 
% Let $f(\param;\xB)$ be a locally Lipschitz network satisfying \Cref{asp:definable,asp:initial-cond-gf,asp:strongerhomo}.
Under the same setting of \Cref{thm: directional convergence}, and additionally assume \Cref{asp:strongerhomo} hold.
% then $\param_t$ given by \Cref{eq:GF} and 
% then 
% $$
% \hB_M(\param_t) \coloneqq  \frac{1}{n}\sum_{i=1}^n e^{-y_i f(\param_t;\xB_i)} y_i\nabla \homoPredictor(\param_t;\xB_i)
% $$
% also converges in direction to $\param_*$. 
% Furthermore, 
Then the rescaled limiting direction %$\param_*$
% \begin{equation}
% \label{eq:limit-tilde-param}
%     \param_* \coloneqq \lim_{t\to \infty} \tilde{\param}_t,
% \end{equation}
% satisfies that 
$$
\param_* / \big(\min_{i \in [n]} y_i f(\param_* ;\xB_i)\big)^{1/M}
$$ 
satisfies the KKT conditions of \Cref{eq: KKT}, where $\param_*$ is the directional limit in \Cref{thm: directional convergence}.
\end{theorem}

This theorem is a generalization of 
% Theorem 4.1 in \citep{ji2021characterizing} and 
Theorem 4.4 in \citet{lyu2020gradient} from homogeneous networks to non-homogeneous networks. 
% \JW{say ours need $M\ge 2$ (check?) and they only show subsequence convergence etc.}
% Note that when the model is homogeneous, $\hB_M$ will be reduced to $-\nabla L(\param_t)$. 
% , we strengthen their conclusion by establishing that $\tilde{\param}_t$ converges to a unique limit that satisfies the KKT conditions. 
% Note that \Cref{thm: KKT convergence} also shows a version of alignment between the GF path and the ``gradient''. 
% This is because $\hB_M$ can be viewed as a version of the ``gradient'' of the empirical risk, where we replace 
Note that our margin maximization problem \Cref{eq: KKT} is defined for the homogenization of the non-homogeneous network. 
% Besides, $\hB_M(\param_t)$ generalizes $-\nabla \Loss(\param_t)$ by replacing 
% the network $ f$ with its homogenization $ \homoPredictor$.
% Both of them are 
This is because, asymptotically, every near-homogeneous network $f$ can be approximated by its homogenization $f_{\homo}$ (see \Cref{thm:homogenization} in \Cref{sec:example}). It is worth noting that \citet{ji2020directional} also established the asymptotic alignment between parameters and gradients, which we leave as future work.

% \JW{check if we have grad align?}

% ultimately behave closely to their homogenized networks.
% Recall that Theorem 4.4 in \citep{lyu2020gradient}
% shows that a subsequence of ${\param}_t$ converges in direction, with the limiting direction satisfying the KKT conditions of \Cref{eq: KKT} when $f$ is $M$-homogeneous.

% For the direction alignment, $\hB_M(\param_t)$ generalizes $-\nabla \Loss(\param_t)$ by replacing $\nabla f(\param;\xB_i)$ in each term with $\nabla \homoPredictor(\param;\xB_i)$. 

% \JW{add alignment here?}

We have presented our results on the implicit bias for non-homogeneous networks. In the next two sections, we discuss the satisfiability of \Cref{asp:nearhomo,asp:initial-cond-gf}, respectively.

% \KZ{Should we explicitly write down the KKT conditions here?}

% After presenting our main results, one may naturally question which models fulfill the near-homogeneity assumption. In \Cref{sec:near-homo-nn}, we establish a systematic framework demonstrating that this assumption is not vacuous and that most deep learning architectures indeed satisfy it.

% \SM{Talk about the flow: The following two sections describe how assumptions are satisfied. }


\section{Near-Homogeneity Condition} \label{sec:near-homo-nn}

In this section, we verify that a large class of building blocks used in deep learning are near-homogeneous, and, by a composition rule, networks constructed using these blocks are also near-homogeneous. 
% We will also establish a structural rule for computing the near-homogeneous
% near-homogeneous deep networks can be constructed by composing near-homogeneous building blocks, and hence many commonly used neural networks satisfy the near-homogeneity condition. 

% We regard networks as a composition of block mappings $s(\param;\xB)$,
We denote a block by $s(\param;\xB)$, where $\param$ are the trainable parameters in this block and $\xB$ is the input (and the output of the preceding block). 
We use $s_{\param}(\xB)$ as a shorthand for $s(\param;\xB)$. 
Then, a network is defined as
\begin{equation}\label{eq:network}
    f(\param;\xB) := s^1_{\param_1} \circ s^2_{\param_2} \circ \cdots \circ s^L_{\param_L}(\xB),
\end{equation}
where $\param = (\param_i)_{i=1}^L$ and $s^i$ is the $i$-th block. Here and in sequel, we assume all the blocks are locally Lipschitz and definable with respect to some o-minimal structure.
% \SM{The logic in the following paragraph requires modification. } 

To deal with the compositional structure of networks, we need to introduce the following generalized definition of near-homogeneity, which takes both trainable parameters and input into account.
% However, compared to \Cref{def:nearhomo}, additional conditions are required for the block mappings. This is because $\xB$ in $s^{i}(\param_i;\xB)$ cannot be treated as fixed; it also depends on the inputs from previous blocks. Therefore, we introduce \Cref{def:dual-homo} to address this dependence.

% \KZ{What about near-$(M_1, M_2)$-homogeneous?}
%────────────────────────────────────────
\begin{definition}
[Near-$(M, N)$-homogeneity]
\label{def:dual-homo}
% Given a definable and locally Lipschitz function $s(\param; \xB)$, we assume that there exists $M_1, M_2\in \Zbb_+$ 
Let $M, N$ be two non-negative integers such that $M + N \ge 1$.
A function $s(\param; \xB)$ is called \emph{near-$(M, N)$-homogeneous},
if there exist $\homop_s, \homoq_s,\homor_s, \homot_s \in \Rbb_{\ge 0} [x]$ with $\deg \homop_s, \deg\homoq_s \le M$ and $ \deg \homor_s, \deg \homot_s \le N $ such that the following holds
for every $\xB\in(\xB_i)_{i=1}^n$, $\thetaB$, and $(\hB_{\param}, \hB_{\xB}) \in  \partial s(\param ;\xB)$:
% \SM{Should this be $f$ or $s$?} 
% the followings hold:
\begin{assumpenum}
    \item %[(B1)]  \textbf{$(M_1,M_2)$-Near homogeneity.} 
    % For all $\param$ and $\xB$, 
  \(
     | \la\hB_{\param}, \param \ra  - M s(\param;\xB) | \le \homop_s^\prime (\|\param\|) \homor_s(\|\xB\|) \),   \(  
     | \la \hB_{\xB}, \xB \ra - N  s(\param;\xB) | \le \homop_s (\|\param\|) \homor_s^\prime(\|\xB\|);
  \)
  
  %\KZ{Actually I think we should use Jacobian instead of gradient, but we can change this later}

  \item %[(B2)] \textbf{$(M_1, M_2)$-bounded gradient.}
  %For all $\param$ and $\xB$, 
  % \begin{align*}
    \(\| \hB_{\param} \| \le \homoq_s^\prime (\|\param\|) \homot_s(\|\xB\|)\), 
    \(\| \hB_{\xB} \| \le \homoq_s (\|\param\|) \homot_s^\prime(\|\xB\|);\)
 % \end{align*}
 %\KZ{Operator norm?}
 \item %[(B3)] \textbf{$(M_1, M_2)$-bounded value.} 
%  \label{asp:dual:bound-val}
 %For all $\param$ and $\xB$,
    \(
        \|s(\param;\xB)\| \le \homoq_s (\|\param\|) \homot_s(\|\xB\|).
    \) 
\end{assumpenum}
A block $s(\param;\xB) : \Rbb^{d_1} \times \Rbb^{d_2} \to \Rbb^{d_3} $ is called near-$(M, N)$-homogeneous if all of its components,
$ s(\param;\xB)_{i}$ for $i \in [d_3]$, 
% $\big( s(\param;\xB) \big)_{i\in [d_3]}$ 
are near-$(M,N)$-homogeneous with the same polynomials $\homop_s, \homoq_s,\homor_s, \homot_s$.
\end{definition}
%────────────────────────────────────────

From \Cref{def:dual-homo}, it is easily seen that a near-$(0, N)$-homogeneous block $s(\param; \xB)$ must be independent of $\param$ and near-$N$-homogeneous in $\xB$, and vice versa.
Below are a few examples of near-homogeneous blocks used in practice.
%────────────────────────────────────────
\begin{example}
\label{eg:Examples of dual homogeneous blocks}
The following blocks are near-homogeneous:
\begin{assumpenum}
    % \item []
    \item For $\param = (A, b)$, the linear mapping $s(\param;\xB) = A\xB + b$ is near-$(1,1)$-homogeneous.
    \item Let $M\ge 1$ be an integer. Then for $\param = (A, b)$, the perceptron layer $s(\param; \xB) = \phi^M(A\xB + b)$ is near-$(M, M)$-homogeneous, where the activation function $\phi$ is one of the following: ReLU, Softplus, GELU, Swish, SiLU, and Leaky ReLU.
    \item Max pooling layer, average pooling layer, convolution layer, and residual connection are near-$(0,1)$-homogeneous. %\KZ{Should we add the definition of near-$(0, M)$-homogeneity to Definition 2?}
    \item The SwiGLU activation \citep{shazeer2020glu} is near-$(2,2)$-homogeneous.
    \item The linear self-attention \citep{zhang2024trained} is near-$(2,3)$-homogeneous and the ReLU attention \citep{wortsman2023replacing} is near-$(4,3)$-homogeneous.
\end{assumpenum}
\end{example}

The following lemma suggests that near-homogeneity is preserved under functional composition and tensor product. Note the residual connection mentioned in \Cref{eg:Examples of dual homogeneous blocks} is not a specific block mapping, but a way to enhance an existing block by adding the input from the previous block to the output of this block. Part C of \Cref{prop: Composition of block mappings} can thus be applied to compute the near-homogeneous order of networks with residual connections.
% Note that the near-homogeneity of residual connection with another block is a special case of part C of the above example.  
The proof of  \Cref{prop: Composition of block mappings} is deferred to \Cref{sec:proof:42}. 
%────────────────────────────────────────
\begin{lemma}
[Composition and multiplication rules]
\label{prop: Composition of block mappings}
Let the blocks $s^1(\param_1;\xB): \Rbb^{d_1} \times \Rbb^{d_2} \to \Rbb^{d_3}$ and $s^2(\param_2;\xB): \Rbb^{d_4} \times \Rbb^{d_5} \to \Rbb^{d_6}$ be near-$(M_1, M_2)$-homogeneous and near-$(M_3, M_4)$-homogeneous, respectively.
% Given  a definable and locally Lipschitz near-$(M_1, M_2)$-homogeneous block mapping $s^1(\param_1;\xB): \Rbb^{d_1} \times \Rbb^{d_2} \to \Rbb^{d_3}$, a definable, locally Lipschitz, and $(M_3, M_4)$-dual-homogeneous block mapping $s^2(\param_2;\xB): \Rbb^{d_4} \times \Rbb^{d_5} \to \Rbb^{d_6}$, and a definable and locally Lipschitz 
% Let $f(\param_3; \xB)$ be near-$M_5$-homogeneous as a function of $\param_3$. 
Then, the followings hold:
\begin{assumpenum}
\item Let $d_2 = d_6$, then $s^1_{\param_1} \circ s^2_{\param_2}(\xB)$ is near-$(M_1 + M_2M_3, M_2M_4)$-homogeneous. 
\item The tensor product $ s^1(\param_1;\xB) \otimes s^2(\param_2;\xB)$ is %definable, locally Lipschitz,  and 
near-$(M_1 + M_3, M_2 + M_4)$-homogeneous. 
\item Let $(d_1, d_2, d_3) = (d_4, d_5, d_6)$, then $ s^1(\param_1;\xB) + s^2(\param_2;\xB)$ is
% definable, locally Lipschitz,  and 
near-$(\max(M_1, M_3), \max(M_2, M_4))$-homogeneous.
% \SM{Could $s^1$ be a matrix and $s^2$ be a vector, so that $s$ is a vector? }
\item Let $T: \Rbb^{d_3} \to \Rbb^{d_6}$ be a linear mapping, then $T(s^1(\param_1;\xB))$ is 
% definable, locally Lipschitz, and 
near-$(M_1, M_2)$-homogeneous. 
\item Let $f: \Rbb^{d_4} \times \Rbb^{d_5} \to \Rbb^{d_2}$ be near-$M_5$-homogeneous as a function of $\param_3$, then
$s^1_{\param_1} \circ f(\param_2;\xB)$ is near-$(M_1+ M_2 M_5)$-homogeneous. 
\end{assumpenum}
\end{lemma}
%────────────────────────────────────────
%For simplicity, we skip the dimension of the input and output for $s^1, s^2$, and $f$ in the composition. We assume that the input and output dimensions are compatible here. 
The following corollary shows the near-homogeneity of a network composed of near-homogeneous blocks. The proof is deferred to \Cref{sec:proof-43}. 
% Once we can add, compose, and multiply the individual block mappings, we can characterize the near homogeneity of general deep networks.

\begin{corollary}
[Near-homogeneous networks]
\label{cor: Near homogeneity order of networks}
Consider a network defined as \Cref{eq:network}.
% Assume that a network $f(\param;\xB)$ of depth $L$ is composed of: 
% \[
%     f(\param;\xB) = s^1_{\param_1} \circ s^2_{\param_2} \circ \cdots \circ s^L_{\param_L}(\xB),
% \]
If the block $s^i(\param_i;\xB) $ is near-$(M_1^i, M_2^i)$-homogeneous for $i\in [L]$, then the network $f$ is near-$(M_1, M_2)$-homogeneous with
% Then, the network is definable and near-homogeneous with parameter: 
\begin{equation}
\label{eq:composition_network_order}
M_1 := \sum_{j=1}^{L} M_1^j \cdot \prod_{i=1}^{j-1} M_2^i, \quad M_2 := \prod_{j=1}^{L} M_2^j.
\end{equation}
In particular, $f$ is near-$M_1$-homogeneous as a function of $\param$ for any fixed $\xB$.
\end{corollary}

% We only need some basic block mappings to construct a near-homogeneous network. In fact, most commonly used architectures in deep learning are dual-homogeneous blocks. 
Based on \Cref{eg:Examples of dual homogeneous blocks}, \Cref{prop: Composition of block mappings} and \Cref{cor: Near homogeneity order of networks}, we can show that a broad class of commonly used networks are near-homogeneous, including the following examples:

%────────────────────────────────────────

%────────────────────────────────────────
\begin{example}
% [Near homogeneous network]
\label{eg: Near homogeneous networks}
% Examples of near homogeneous networks:
The following networks are near-homogeneous:
\begin{assumpenum}
    % \item An $L$-layer MLP with ReLU activation is near-$L$-homogeneous. The same holds when ReLU is replaced by other activation functions in \Cref{eg: Activation functions} \JW{implied by the next example}.
    \item An $L$-layer MLP with $k$-th power of ReLU activation is near-$(k^L-1)/(k-1)$-homogeneous, or near-$L$-homogeneous when $k=1$. The same holds when ReLU is replaced by other activation functions in \Cref{eg:Examples of dual homogeneous blocks} (B).
    \item VGG-$L$ \citep{simonyan2015very} is near-$L$-homogeneous where $L\in\{11, 13, 16, 19\}$. 
    \item Without batch normalization, ResNet-$L$ \citep{he2016deep} is near-$L$-homogeneous where $L \in \{18,34,50,101,152\}$, and DenseNet-$L$ \citep{huang2017densely} is near-$L$-homogeneous where $L \in \{121, 169, 201, 264\}$.
\end{assumpenum}
\end{example}

To conclude this section, we comment that  normalization layers and softmax map violate our near-homogeneity definitions. Intuitively, these blocks should be ``near-$0$-homogeneous'' as their outputs are bounded. However, our near-homogeneity definitions are only non-trivial for $M\ge 1$. 
As a consequence, the softmax attention architecture also violates our definitions of near-homogeneity, thus requiring a different treatment.
% We leave this as a future 
We believe that our notion of near-homogeneity can be generalized to include those components, which we leave as future work.
% into dual-homogeneous block mappings remains challenging because these operations constrain the input to a finite range, which is incompatible with the properties of near-homogeneous functions. Consequently, the standard attention mechanism \citep{vaswani2017attention}, which relies on softmax, is not dual-homogeneous. 
% We leave the investigation of near-homogeneity in attention mechanisms and normalizations as future work.

% \WC{add examples for networks}
% \SM{Is there any operation that is not near-homogeneous? Worth mentioning if there is or there is not. }



\section{Strong Separability Condition} \label{sec:example}

In this section, we discuss the satisfiability of \Cref{asp:initial-cond-gf}. 
% We first show that, for any non-degenerate near-homogeneous network, 
% it can satisfy the strong separability condition as long as it can satisfy the separability condition in \citep{lyu2020gradient}.
We first give an intuitive explanation of why \Cref{asp:initial-cond-gf} should be expected to hold.
Note that \Cref{asp:initial-cond-gf} is equivalent to 
\[
\Loss(\param_s) < e^{-\homop_a(\|\param_s\|)}/n\ \Leftrightarrow \ 
% \frac{\phi\big(\Loss(\param_t)\big)}{\|\param_t\|^{\deg \homop_a}} > c,
% \log \frac{1}{ \Loss(\param_s)}
\frac{\log\big(1/(n\Loss(\param_s))\big)}{\homop_a(\|\param_s\|)} > 1.
\]
Recall that $\deg \homop_a \le M-1$ by \Cref{asp:nearhomo}. Thus \Cref{asp:initial-cond-gf} can be understood as requiring GF to induce a ``lower-order'' smoothed margin which is at least $1$.
If the normalized margin \Cref{eq: normalized margin} is asymptotically positive (as $\|\param_t\|$ grows), then the ``lower-order'' smoothed margin 
% ${\log\big(1/\Loss(\param_t)\big)}/{\homop_a(\|\param_t\|)}$ 
must diverge, hence \Cref{asp:initial-cond-gf} must hold at some point. 

% is almost equivalent to having a margin that dominates a polynomial of degree lower than $M$: 
% Recall that $\deg \homop_a \le M-1$ from \Cref{asp:nearhomo}. 
% where $c$ is some constant that depends on $\homop_a$. In an over-parameterized setting, the dataset can be
% separated and the margin will be positive and increasing.
% As $\|\param_t\| \to \infty$, 
% \[
% \frac{\phi\big(\Loss(\param_t)\big)}{\|\param_t\|^{\deg \homop_a}} \ge \GFmargin( \param_t) \|\param_t\|^{M-\deg \homop_a} \to \infty.  
% \]

% This observation offers an intuition for how the initial condition can be satisfied. We further elaborate on this intuition as follows:
% \begin{itemize}
%     \item \Cref{sec:homogeneization}: We present a sufficient condition (\Cref{cor:Initial condition via homogeneization}) for the existence of a parameter $\param$ satisfying the initial condition. 
%     \item \Cref{sec:toy-eg}: We offer a concrete example (\Cref{thm: Near-two homogeneous example achieves init bound}) illustrating how a gradient flow can fulfill this requirement.
% \end{itemize}

In what follows, we establish a sufficient condition under which a non-homogeneous network satisfies \Cref{asp:initial-cond-gf}.
To this end, we establish below several important properties of the \emph{homogenization} of a non-homogeneous network.
% In \Cref{sec:toy-eg}, We also provide a concrete example illustrating how a gradient flow can fulfill this requirement (\Cref{thm: Near-two homogeneous example achieves init bound}).
% It is worth noting that when the model is homogeneous, $\homop_a(x) = 0$ and our result recovers Theorem 4.1 in \citet{lyu2020gradient}. 


\paragraph{Homogenization.} %\label{sec:homogeneization}
The idea of homogenization appears in \Cref{sec:margin-direct} for constructing the margin maximization problem, where its KKT conditions characterize the limiting direction of GF. Our next theorem rigorously controls the approximation error between a non-homogeneous network and its homogenization.
% This subsection is devoted to the homogenization of the near-homogeneous networks. The main idea is to construct a homogenized network that approximates the original network. It's worth noting that the homogenized network plays an important role in the KKT conditions \Cref{eq: KKT}. We have the following theorem for the homogenization of the near-homogeneous networks. 

\begin{theorem}[Homogenization]
\label{thm:homogenization}
Suppose that $f$ satisfies \Cref{asp:nearhomo}.
% be alocally Lipschitz, and near-$M$-homogeneous model with  $\homop(x) \in \Rbb_+ [x]$, $\deg \homop \le M$. 
Then for every $\xB\in(\xB_i)_{i=1}^n$, the homogenization of $f(\param; \xB)$:
\begin{equation*}
    \homoPredictor (\param; \xB) := \lim_{r \to +\infty} \frac{f(r \param; \xB)}{r^M}
\end{equation*}
exists and is well-defined.
Moreover, as a function of $\param$, $\homoPredictor (\param; \xB)$ is continuous, differentiable almost everywhere, and $M$-homogeneous. 
% And the error between $f$ and $\homoPredictor$ is bounded by: 
We also have
\[
\text{for every } \param,\quad 
    \left\vert f(\param; \xB) - \homoPredictor (\param; \xB) \right\vert \le \homop_a( \left\| \param \right\| ),
\]
where $\homop_a$ is given by \Cref{eq:def-pa}.

If, in addition, $f(\param;\xB)$ satisfies \Cref{asp:strongerhomo},
% is differentiable and $\nabla f(\param;\xB)$ is near-homogeneous (see \Cref{asp:strongerhomo}), 
then $\homoPredictor(\param;\xB)$ is continuously differentiable for every nonzero $\param$, and that $(\grad f)_\homo (\param; \xB) = \grad \homoPredictor (\param; \xB)$. % on $\Rbb^d /\{0\}$. 
\end{theorem}
\Cref{thm:homogenization} ensures the well-definedness, continuity, and differentiability of $\homoPredictor$.
Consequently, the following theorem guarantees that the strong separability condition can be satisfied by a near-homogeneous network, as long as its homogenization satisfies the (weak) separability condition of \cite{lyu2020gradient}.

%────────────────────────────────────────
\begin{theorem}
[A sufficient condition]
\label{cor:Initial condition via homogeneization}
Suppose that $f$ satisfies \Cref{asp:nearhomo}. Then $f$ admits a homogenization, denoted by $\homoPredictor$.
Assume that $\homoPredictor$ satisfies the weak separability condition, that is, 
\[
\sum_{i} \ell\big(- y_i \homoPredictor(\param^\prime ;\xB_i) \big) < 1\   \text{for some}\ \ \param^\prime.
\]
% there exists $\param^\prime $ such that $y_i \homoPredictor(\param^\prime ;\xB_i)>0$ for all $i \in [n]$. 
Then there exists a constant $c>0$ such that $f$ with $\param_s := c \param^\prime $ satisfies  \Cref{asp:initial-cond-gf}.
% \ML{\Cref{asp:initial-cond-gd} hasn't appeared yet}
\end{theorem}
%────────────────────────────────────────

%  (or \Cref{asp:initial-cond-gd}, the strong separability condition for GD, which will be discussed in \Cref{sec:margin-direct-gd}).

% Here, \Cref{asp:initial-cond-gd} refers to an alternative initial condition for gradient descent (GD), which will be introduced in \Cref{sec:margin-direct-gd}.

% \section{Homogenization of Networks}

% \JW{The following two theorems are unclear. I suggest moving them to the appendix.}
To further elaborate the idea of homogenization, we provide the following compositional rule. 
% \JW{@yuhang check the statement of this thm}
% \begin{theorem}\label{thm:block_homogenization}
\begin{theorem}[Homogenization of networks]\label{thm:block_compo_homogenization}
Consider a network given by \Cref{eq:network}, where each block $s^i (\param_i; \xB)$ is near-$(M_1^i, M_2^i)$-homogeneous (see \Cref{def:dual-homo}) for $i=1,\dots,L$.
Then each block $s^i (\param_i; \xB)$ admits a well-defined 
% For each $i \in [L]$, denote by $s_{\homo, \param_i}^i (\cdot) = s_M^i (\param_i; \cdot)$ the 
homogenization 
% of $s_{\param_i}^i (\cdot) = s^i (\param_i; \cdot)$, namely
\begin{equation*}
s_{\homo}^i (\param_i; \xB) := \, \lim_{r_1, r_2 \to \infty} \frac{s^i (r_1 \param_i; r_2 \xB)}{r_1^{M_1^i} r_2^{M_2^i}}.
\end{equation*}
Moreover, the homogenization network \Cref{eq:network} is well-defined and satisfies 
\begin{equation*}%\label{eq:homo_decomp_f}
\homoPredictor (\param; \xB) = s_{\homo, \param_1}^1 \circ s_{\homo, \param_2}^2 \circ \cdots \circ s_{\homo, \param_L}^L (\xB).
\end{equation*}
\end{theorem}

% \section{Homogenization of Networks}

% \JW{The following two theorems are unclear. I suggest moving them to the appendix.}
% \begin{theorem}\label{thm:block_homogenization}
%     Let $s(\param; \xB)$ satisfy Assumption (B1) with parameters $(M_1, M_2)$. Then, the limit \JW{@yuhang, what's B1 and B2?}
%     \begin{equation*}
%         s_M (\param; \xB) = \lim_{r_1, r_2 \to \infty} \frac{s(r_1 \param; r_2 \xB)}{r_1^{M_1} r_2^{M_2}}
%     \end{equation*}
%     exists for all $(\param, \xB)$, and is $M_1$-homogeneous in $\param$ and $M_2$-homogeneous in $\xB$. Additionally, if $s(\param; \xB)$ satisfies (B2), then $s_M (\param; \xB)$ is continuous and almost everywhere continuous in $(\param, \xB)$.
% \end{theorem}

% We next establish that the composition of blocked functions
% \begin{equation}
%     f(\param;\xB) = s^1_{\param_1} \circ s^2_{\param_2} \circ \cdots \circ s^L_{\param_L}(\xB),
% \end{equation}
% admits a homogenization determined by the homogenizations of the block components $\{ s_{\param_i}^i \}_{i=1}^{L}$. 

% % First, note that using the same proof technique as that of \Cref{prop: Composition of block mappings} and induction, we can prove the following:
% % \begin{proposition}\label{prop:homo_param_composition}
% % Let $\{ s_{\param_i}^i (\cdot) = s^i (\param_i; \cdot) \}_{i=1}^{L}$ be a sequence of block functions such that for each $i \in [L]$, $s^i (\param_i; \xB)$ satisfies Assumption (B1) and (B2) with parameters $(M_1^i, M_2^i)$. Then, $f(\param;\xB) = s^1_{\param_1} \circ s^2_{\param_2} \circ \cdots \circ s^L_{\param_L} (\xB)$ satisfies Assumption (B1) and (B2) with parameters $(M_1, M_2)$, where
% % \begin{equation}\label{eq:homo_degree_f}
% % M_1 = \sum_{j=1}^{L} M_1^j \cdot \prod_{i=1}^{j-1} M_2^i, \quad M_2 = \prod_{j=1}^{L} M_2^j.
% % \end{equation}
% % \end{proposition}

% Denote $\param = (\param_1, \cdots, \param_L)$ as the collection of all parameters. Applying \Cref{thm:block_homogenization}, we know that the function
% \begin{equation}
% \homoPredictor (\param; \xB) = \, \lim_{r_1, r_2 \to \infty} \frac{f(r_1 \param; r_2 \xB)}{r_1^{M_1} r_2^{M_2}}
% \end{equation}
% is continuous, almost everywhere differentiable, and $(M_1, M_2)$-homogeneous in $(\param, \xB)$, where $M_1$ and $M_2$ are given in \Cref{eq:composition_network_order}. The theorem below establishes a closed-form expression for $\homoPredictor (\param; \xB)$.


% \begin{theorem}\label{thm:block_compo_homogenization}
% For each $i \in [L]$, denote by $s_{\homo, \param_i}^i (\cdot) = s_M^i (\param_i; \cdot)$ the homogenization of $s_{\param_i}^i (\cdot) = s^i (\param_i; \cdot)$, namely
% \begin{equation}
% s_M^i (\param_i; \xB) = \, \lim_{r_1, r_2 \to \infty} \frac{s^i (r_1 \param_i; r_2 \xB)}{r_1^{M_1^i} r_2^{M_2^i}}.
% \end{equation}
% Then, we have
% \begin{equation}\label{eq:homo_decomp_f}
% \homoPredictor (\param; \xB) = s_{\homo, \param_1}^1 \circ s_{\homo, \param_2}^2 \circ \cdots \circ s_{\homo, \param_L}^L (\xB).
% \end{equation}
% \end{theorem}



\paragraph{A Two-Layer Network.}% \label{sec:toy-eg}
We next provide a two-layer network example where GF with zero initialization can provably reach a point that satisfies \Cref{asp:initial-cond-gf}. The two-layer network is defined as 
% Motivated by \citet{lyu2021gradient}, we show a near 2-homogeneous example can reach the initial bound in \Cref{thm: Margin improving and convergence}. We consider the following model which mimics the ResNet architecture:
\begin{equation}
\label{eq: toy-resnet}
    f(\param;\xB) := \wB_1^\top \xB + \wB_2^\top \xB + a_1 \varphi( \wB_1^\top \xB) - a_2 \varphi( - \wB_2^\top \xB),
\end{equation}
where $\param := (\wB_1, \wB_2, a_1, a_2)$ are the trainable parameters, and  $\varphi$ is the leaky ReLU activation function, 
% \SM{notation $\varphi$ has been used before to be $\log(1/nx)$}
\[
\varphi(x) := \max\{x, \alpha_L x\},\quad 0< \alpha_L<1.
    % \varphi(x) = \begin{cases}
    %     x, & \text{ if } x\ge 0, \\
    %     \alpha_L x, & \text{ if } x < 0.
    % \end{cases}
\]
Our example \Cref{eq: toy-resnet} is motived by the two-layer network considered in \citet{lyu2021gradient}. However, their network is homogeneous, while ours is non-homogeneous (but near-$2$-homogeneous) due to the residual connections.
% As for the initialization, we assume that $\wB_{1,0} = \wB_{2,0} = \boldsymbol 0$ 
% \SM{use $\boldsymbol 0$ instead of $\boldsymbol 0$} 
% and $a_{1,0} = a_{2,0} = 0$. 
Similar to \citet{lyu2021gradient}, we consider a symmetric and linearly separable dataset.
\begin{assumption}[Dataset conditions] 
\label{asp: symmetric}
Assume that the dataset $(\xB_i, y_i)_{i=1}^n$ satisfies $\max_{i}\|\xB_i\|\le 1$ and $\min_{i} y_i\xB_i^\top \wB_* \ge \gamma$ for some unit vector $\wB_*$ and margin $\gamma>0$; moreover, 
% The dataset is symmetric, i.e.,
$n$ is even and $(y_{i+n/2},\xB_{i+n/2}) = -(y_i,\xB_i) $ for $i=1,\ldots ,n/2$. 
% All $\xB_i$ are in the unit ball ,i.e., $\|\xB_i\|\le 1$ for all $i \in [n]$. 
% The dataset is linear separable, i.e., there exists a $\gamma>0$ and $\wB_* \in \Rbb^d$, $\|\wB_*\|=1$ such that $y_i\xB_i^\top \wB_* \ge \gamma$ for all $i$.
\end{assumption}
%  We can check that the homogeneous error of this model is: 
% % \SM{Remind readers that $\xB$ is in norm ball. }
% \[
%     | \langle \nabla_{\param} f(\param;\xB), \param \rangle - 2f(\param;\xB)| = |  \wB_1^\top \xB + \wB_2^\top \xB| \le \sqrt{2} \| \param\|.  
% \]
% Hence, we have the corresponding $\homop(x) = \frac{\sqrt{2} }{2}x^2, \homop_a(x) = \sqrt{2} x $. In \Cref{thm: Margin improving and convergence-gd}, we assume $f$ is twice differentiable, while ReLU is not. Neverthless, we can show that $\wB_1^\top \xB$ and $-\wB_2^\top \xB$ will stay positive, when the loss is small. This enables $f$ to satisfy \Cref{asp:strongerhomo}. We are going to verify the following initial condition: 
% \SM{Why alternative? }
% \begin{equation}
% \label{eq: toy init bound}
%         \Loss (\param_t) < e^{-\homop_a(\|\param_t\|)}/n \Leftrightarrow \frac{\log 1/\Loss (\param_t)}{\sqrt{2} \|\param_t\| } > \log n.
% \end{equation}
% \KZ{In fact, we can show that $\Loss (\param_t) < e^{-\homop_a(\|\param_t\|)}/N$ for any $N > 0$, so that Assumption 6 is also satisfied?}

% We will consider the gradient flow dynamics \Cref{eq:GF} of this model.
Note that the network \Cref{eq: toy-resnet} is not Lipschitz continuous, thus \Cref{eq:GF} is not well-defined. Instead, we study the small stepsize limit of gradient descent for \Cref{eq: toy-resnet}, which is well-defined under our assumptions.
In the next theorem, we establish that the small-stepsize limit of gradient descent with zero initialization for \Cref{eq: toy-resnet} can satisfy \Cref{asp:initial-cond-gf}.
% the uniqueness or existence of the solution to \Cref{eq:GF} is not guaranteed. But with the symmetric assumption and the initial condition, we can show that there exists a good symmetric solution, i.e., $\wB_{1,t} = \wB_{2,t}, a_{1,t} = a_{2,t}$, which is also the 
% % \SM{the small stepsize limit} 
% small stepsize limit of the gradient descent dynamics \Cref{eq:GD}.

% \begin{lemma}
% [Symmetry of the parameters]
% \label{lem:symmetry-toy}
% We assume the dataset satisfies \Cref{asp: symmetric}, and the model is \eqref{eq: toy-resnet} with initial condition  $\wB_{1,0} = \wB_{2,0} = \boldsymbol 0$ and $a_{1,0} = a_{2,0} = 0$. Then, $\wB_{1,t} = \wB_{2,t}= \wB_t$ and $a_{1,t} = a_{2,t} = a_t$ is a solution to the gradient flow dynamics \eqref{eq:GF}, where $\wB_t$ and $a_t$ satifies the following ODE:  
% % \SM{Notation $f_i(\param_t)$ not defined. Possibly collide with $\homoPredictor(\param; \xB)$}
% \begin{equation}
% \label{eq: dynamic of w and a}
% \begin{cases}
% \dot \wB_t = \frac{1}{n} \sum_{i=1}^n e^{-y_i f(\param_t;\xB_i)} (1+ c_L a)y_i \xB_i\\ 
% \dot a_t = \frac{1}{n} \sum_{i=1}^n e^{-y_i f(\param_t;\xB_i)}  c_L y_i \xB_i^\top \wB_t
% \end{cases}
% \end{equation}
% initialized at $(\wB_0, a_0) = (\boldsymbol{0}, 0)$, with $c_L \coloneqq (1+\alpha_L)/2$. Furthermore, $(\wB_t, a_t)$ is the limit of the GD dynamics \eqref{eq:GD} with respect to \Cref{eq: toy-resnet}, as the step size goes to $0$: 
% \[
% (\wB_t, a_t) = \, \lim_{\eta \to 0} ( \wB_{\eta t}, a_{\eta t} ). 
% \]
% \end{lemma}

% Therefore, we do not need to worry about the existence and the uniqueness of the solution. The next theorem shows the solution can satisfy the initial bound in \Cref{thm: Margin improving and convergence}. The proof is deferred to \Cref{sec:proof-eg}.

%────────────────────────────────────────
\begin{theorem}
[A two-layer network example]
\label{thm: Near-two homogeneous example achieves init bound}
Consider the network $f$ given by \eqref{eq: toy-resnet} and a dataset satisfying \Cref{asp: symmetric}.
Let 
$(\thetaB_t)_{t\ge 0}$ be the limit of GD iterates with initialization $\thetaB_0=\boldsymbol 0$ as the stepsize tends to zero. 
Then there exists $s>0$ such that $f$ with $\thetaB_s$ satisfies \Cref{asp:initial-cond-gf}.
% and the predict in with initial condition  $\wB_{1,0} = \wB_{2,0} = \boldsymbol 0$ and $a_{1,0} = a_{2,0} = 0$, there exists $T$ such that the initial bound 
% \Cref{eq: toy init bound}
\end{theorem}
%────────────────────────────────────────

\section{Implicit Bias of Gradient Descent} \label{sec:margin-direct-gd}
In this section, we extend our previous results for GF to gradient descent (GD), which is defined as 
% we analyze the dynamics of GD: 
\begin{equation}
\label{eq:GD}
    \tag{GD}
    \param_{t+1} := \param_t - \eta \nabla \Loss (\param_t),
\end{equation}
where $\eta > 0$ is a fixed stepsize.
% To handle the discretization error brought by GD, we need to assume the network satisfies the stronger near-homogeneous condition \Cref{asp:strongerhomo}. \JW{is this correct?}
% Moreover, we need a stronger strong separability condition. 
Similarly, we will assume the network $f$ is near-$M$-homogeneous with $\homop$ and $\homoq$ (see \Cref{asp:nearhomo}).
% Recall that \Cref{asp:strongerhomo} implies \Cref{asp:nearhomo} with $\homop$ and $\homoq$.
For GD, the definition of $\homop_a$ needs to be slightly modified. Let $\homop(x) :=\sum_{i=0}^M a_i x^i$. For $M\ge 2$, we 
% Similar to the case of GF, we assume $M \ge 2$. However, we need a modified version of $\homop_a$ and a stronger initial condition for GD compared to GF. 
define
\begin{equation}
\label{eq:def-pa-gd}
\homop_a (x) := 
\begin{dcases}
\sum_{i=1}^{M-1} \frac{(i+1)a_{i+1}}{M-i} x^{i} + \frac{a_1}{M-1/2}, & \text{if } x \ge 1, \\
\sum_{i=2}^{M-1} \frac{(i+1)a_{i+1}}{M-i} x^{i}  + \frac{2 a_2}{M - 1} \frac{x^2 + 1}{2} + \frac{a_1}{M-1/2}, & \text{if } 0 \le x < 1.
\end{dcases}
\end{equation}
For $M=1$, we define $\homop_a(x) \coloneqq a_1/(M-1/2)$. 
% \KZ{Find a better way to write this definition later}
% \JW{$M=1$?}
Compared to \Cref{eq:def-pa}, \Cref{eq:def-pa-gd} replaces the linear term $x$ with a quadratic polynomial $(x^2 + 1)/2$. This is purely for circumventing a technical issue when $\| \param \|$ is small (see the proof of \Cref{lem:gradient_hessian_bound} in \Cref{sec:proof:gd:prelim}). 

To handle the discretization error introduced by GD, we need a stronger version of the strong separability condition. 
\begin{assumption}
[Strong separability condition for GD]
\label{asp:initial-cond-gd}
Let $f(\param;\xB)$ be a network satisfying \Cref{asp:nearhomo}.
Assume that $f$ is twicely differentiable for $\param$ and % with $(M,\homop, \homoq)$. 
that for some constant $A > 0$, we have
\begin{equation*}
    \left\| \nabla_{\param}^2 f(\param; \xB) \right\| \le
    \begin{dcases}
        A \left( \| \param \|^{M - 2} + 1 \right), & \text{if $M \ge 2$} \\
        A, & \text{if $M = 1$} \\
    \end{dcases} 
\end{equation*}
for every $\xB \in (\xB_i)_{i=1}^n$. 
% \SM{For $\xB$ in dataset or all $\xB$?}. 
For $\homop_a$ given by \cref{eq:def-pa-gd},
assume that $\deg \homop_a \ge 1$ if $M \ge 2$, and that there exists a time $s>0$ such that
\begin{equation*}%\label{eq:init-cond-gd}
    \Loss(\param_s) < \min \left\{ \frac{1}{n e^2}, \ \frac{1}{B \eta} \right\} e^{-\homop_a(\|\param_s\|)},
\end{equation*}
where $B$ is a constant depending only on $(M, \homop, \homoq)$ and $A$. 
% \SM{Is $B$ a particular constant, or it is enough that there exists a constant?}. 
\end{assumption}
Note that in \Cref{asp:initial-cond-gd}, the stepsize $\eta$ can be arbitrarily large as long as the empirical risk $\Loss(\param_s)$ is of the order of $\Ocal(1/\eta)$.
We note that \Cref{cor:Initial condition via homogeneization} can also be adapted to guarantee the satisfiability of \Cref{asp:initial-cond-gd}.

% Analogous to \Cref{thm: Margin improving and convergence}, we will establish convergence results for GD, in terms of the following modified margin:
We consider the following modified margin for GD:
\begin{equation}\label{eq:modified_margin_GD}
    \GDmargin(\param) \coloneqq \frac{\exp \left( \Phi \left( e^{\homop_a (\| \param \|)} \Loss(\param) \right) \right)}{\|\param\|^M},
\end{equation}
where $\Phi(x) := \log (\log \frac{1}{nx}) + \frac{2}{\log (nx)}$. 

In the following two theorems, we extend our results for GF to GD. The proofs are included in \Cref{sec:margin_improve_gd}.
% We summarize our main findings on margin improvement and convergence rates in the following theorem, whose proof is deferred to \Cref{sec:margin_improve_gd,sec:converge_rate_gd}.
% \SM{Maybe just spell out the particular form of $\Phi$ instead of writing it in terms of $\phi$, since $\phi$ is also defined a long way ago. }.
\begin{theorem}
    [Risk convergence and margin improvement for GD]
    \label{thm: Margin improving and convergence-gd} 
    Suppose that \Cref{asp:nearhomo,asp:initial-cond-gd} hold. For $(\param_t)_{t\ge 0}$ given by \Cref{eq:GD} with any stepsize $\eta>0$, we have:
    \begin{itemize}[leftmargin=*]
        \item For all $t > s$, the risk and parameter norm satisfy
        \begin{equation*}
            \Loss (\param_t) < \min \left\{ \frac{1}{ne^2}, \frac{1}{B \eta} \right\} \cdot  e^{-\homop_a(\|\param_t \|)}.
        \end{equation*}
        Furthermore, as $t \to \infty$, we have
        \begin{align*}
            \Loss (\param_t) \eqsim \frac{1}{\eta t(\log \eta t)^{2-2/M}}, \quad
            \| \param_t \| \eqsim (\log \eta t)^{\frac{1}{M}},
        \end{align*}
        where $\eqsim$ hides constants that depend on $M$, $\GDmargin(\param_s)$, and coefficients of $\homoq$, but not $\eta$.
        
        \item The modified margin $\GDmargin(\param_t)$ in \Cref{eq:modified_margin_GD} is increasing and bounded. 
        Moreover, $\GDmargin(\param_t)$ is an $\epsilon_t$-multiplicative approximation of $\gamma(\param_t)$, that is, for all $t > s$,
            \begin{equation*}%\label{eq:margin_approximation_gd}\JW{let's not number equations if not used in the main paper}
                \GDmargin\big(\param_t\big) \le \gamma\big(\param_t\big) \le \big(1+\epsilon_t\big) \GDmargin\big(\param_t\big), 
            \end{equation*}
            where $\epsilon_t \to 0$ as $t \to \infty$. 
        % \end{itemize}
    \end{itemize}
\end{theorem}
% The above theorem generalizes \Cref{thm: Margin improving and convergence} to GD with arbitrary step size. In fact, our analysis even allows for a time-dependent learning rate $\eta_t$ satisfying certain growth conditions.

% Note that Theorem 2.2 in \citet{cai2024large} established the margin improvement and the convergence rates for the near-1-homogeneous case. Our assumption $M\ge 2$ arises from a different theoretical framework, which enables the treatment of more general $M$. 
% % \JW{Add: Comapred to Theorem XX in \citep{cai2024large} for near-$1$-homogeneous cases....} 

% Similar to GF,  we establish that the GD iterates converges to a limiting direction $\param_*$. Further, if $\nabla_{\param} f$ is also near homogeneous, the $\param_*$ is a KKT point of the optimization problem~\Cref{eq: KKT}. We present our main results on convergence to KKT direction below, and prove them in \Cref{sec:direct_converge_gd,sec:KKT_convergence_gd}.

\begin{theorem}[Directional and KKT convergence for GD]
\label{thm: directional convergence-gd}
Under \Cref{asp:nearhomo,asp:initial-cond-gd}, the same results in \Cref{thm: directional convergence} hold for \Cref{eq:GD} with any stepsize $\eta>0$. %\JW{do we need \Cref{asp:strongerhomo} here?}
% we have the curve swept by $\tilde{\param}_t \coloneqq \param_t / \| \param_t \|$ has finite length. As a consequence, $\tilde{\param}_t$ converges to a limiting direction $\param_*$. 
% \end{theorem}
Under \Cref{asp:strongerhomo,asp:initial-cond-gd}, the same results in \Cref{thm: KKT convergence} hold for \Cref{eq:GD} with any stepsize $\eta>0$.
% \begin{theorem}[Convergence to KKT direction for GD]
% \label{thm: KKT convergence-gd} 
% Under \Cref{asp:strongerhomo,asp:initial-cond-gd}, the $\param_t$ and $\hB_M$ converge to the same direction, where
% $$
% \hB_M \coloneqq  \sum_{i=1}^n e^{-y_i f(\param_t;\xB_i)} y_i\nabla \homoPredictor(\param_t;\xB_i). 
% $$
% Furthermore, the directional limit $\param_*$
% % \begin{equation}
% % \label{eq:limit-tilde-param}
% %     \param_* \coloneqq \lim_{t\to \infty} \tilde{\param}_t,
% % \end{equation}
% satisfies that 
% $$
% \param_* / [\min_{i \in [n]} y_i \homoPredictor(\param_* ;\xB_i)]^{1/M}
% $$ 
% is a KKT point of \Cref{eq: KKT}.
\end{theorem}

% \section{Near-Homogeneous Networks and Homogenization} \label{sec:near-homo}
\section{Proof Overview}\label{sec:proof_overview}
In the previous sections, we present several main results on the implicit bias of GF/GD for near-homogeneous networks. Below we briefly describe the approaches we use to prove these results, with actual proofs deferred to appendices.

\paragraph{Margin Improvement and Convergence Rates.}
We first sketch the proof of \Cref{thm: Margin improving and convergence} (GF), and then highlight the major technical innovations in the proof of \Cref{thm: Margin improving and convergence-gd} (GD), as compared to the case of GF. The key ingredient for proving \Cref{thm: Margin improving and convergence} is the following lemma, which establishes the monotonicity of the modified margin under the strong separability condition.

\begin{lemma}[Restatement of \Cref{thm:gamma-a-increase}]\label{lem:restate_gamma_gf_increase}
    Denote $\Loss_t = \Loss (\param_t)$ and $\rho_t = \| \param_t \|$. Under \Cref{asp:nearhomo,asp:initial-cond-gf}, we have  
    $\Loss_t < e^{-\homop_a(\rho_t)}/n$ for all $t\ge s$, and
    \begin{equation}\label{eq:restate_gamma_gf_lowerbd}
        \frac{\mathrm{d} \log \GFmargin \big(\param_t\big)}{\mathrm{d} t} > \frac{\| \bar \partial \Loss_t \|^2 \rho_t^2 - \langle  \bar \partial \Loss_t , \param_t \rangle^2  }{\rho_t^2\Loss_t \big (\LinkFun(\Loss_t) - \homop_a(\rho_t)\big)} \ge 0.
    \end{equation}
\end{lemma}

The proof of \Cref{lem:restate_gamma_gf_increase} is similar to the proof of \citet[Lemma 5.1]{lyu2020gradient}. Since $\GFmargin \big(\param_t\big)$ only depends on $\Loss_t$ and $\rho_t$, its growth rate can be attributed to two quantities: $\mathrm{d} \Loss_t / \mathrm{d} t$ and $\mathrm{d} \rho_t / \mathrm{d} t$. We use the same argument as that in \citet[Proof sketch of Lemma 5.1]{lyu2020gradient} to estimate each of these two quantities, which finally leads to the lower bound \eqref{eq:restate_gamma_gf_lowerbd} on $\mathrm{d} \GFmargin ( \param_t ) / \mathrm{d} t$. Further since
\begin{equation*}
    \LinkFun(\Loss_t) - \homop_a(\rho_t) > 0 \Longleftrightarrow \Loss_t < \frac{1}{n} e^{- \homop_a (\rho_t)},
\end{equation*}
we know that the strong separability condition is necessary for the lower bound \eqref{eq:restate_gamma_gf_lowerbd} at $t = s$, and can be established for $t \ge s$ using continuous induction.

From this lower bound, the ``margin improvement" part of \Cref{thm: Margin improving and convergence} directly follows. For the ``convergence rates" part, we use the monotonicity of modified margin to upper and lower bound $- \mathrm{d} \Loss_t / \mathrm{d} t$, thus establishing convergence rates for $\Loss_t$. The claim for $\rho_t$ can be proved similarly.

For the proof of \Cref{thm: Margin improving and convergence-gd}, we need to establish a lower bound on $\log \GDmargin ( \param_{t+1} ) - \log \GDmargin ( \param_{t} )$ similar to \Cref{eq:restate_gamma_gf_lowerbd}. Due to the discrete nature of GD, the Hessian of $\log \GDmargin$ should also be taken into account when dealing with $\log \GDmargin ( \param_{t} )$ as a function of $t$. To address this challenge, we analyze a modified loss $\ModifiedLoss (\param) = \exp(\homop_a(\| \param \|)) \Loss(\param)$ that is closely related to $\log \GDmargin$. We establish tight upper bounds on the Hessian of $\ModifiedLoss (\param_t)$ in terms of $\Loss_t$ and $\rho_t$, thus leading to a tight lower bound on $\log \GDmargin ( \param_{t+1} ) - \log \GDmargin ( \param_{t} )$. Notably, our lower bound allows for arbitrarily large step size $\eta$, as long as $\ModifiedLoss (\param_s) = O(1 / \eta)$, which is guaranteed by \Cref{asp:initial-cond-gd}. The proof of other parts in  \Cref{thm: Margin improving and convergence-gd} is completely analogous to the case of GF.

\paragraph{Convergence to the KKT Direction.}
The proofs of \Cref{thm: directional convergence,thm: directional convergence-gd} largely rely on the techniques developed in \citet{ji2020directional}. As before, we will first sketch the proof for GF, and then explain how to adapt it to establish KKT convergence for GD. For GF, define the arc length swept by the direction of $\param_t$:
\begin{equation*}
	\zeta_t = \, \int_{s}^{t} \left\| \frac{\mathrm{d} \tilde{\param}_u}{\mathrm{d} u} \right\| \mathrm{d} u.
\end{equation*}
Similar to \citet{ji2020directional}, we construct a desingularizing function $\Psi$ that controls the growth rate of $\zeta_t$ using that of $\Psi (\gamma_* - \GFmargin (\param_t))$, where $\gamma_* \coloneqq \lim_{t \to \infty} \GFmargin (\param_t)$:
\begin{lemma}[Restatement of \Cref{lem: Existence of desingularizing function}]
\label{lem: restate of Existence of desingularizing function}
There exist $R>0, \nu>0$ and a definable desingularizing function $\Psi$ on $[0, \nu)$, such that for a.e. large enough $t$ with $\left\|\param_t\right\|>R$ and $\GFmargin(\param_t)>\gamma_*-\nu$, it holds that
\begin{equation}\label{eq:desingularizing_inequality}
	\frac{\mathrm{d} \zeta_t}{\mathrm{~d} t} \leq-c \frac{\mathrm{d} \Psi\left(\gamma_*-\GFmargin(\param_t)\right)}{\mathrm{d} t}
\end{equation}
for some constant $c>0$.
\end{lemma}
The proof of the above lemma is similar to that of \citet[Lemma 3.1]{ji2020directional}. Integrating both sides of \Cref{eq:desingularizing_inequality}, we deduce that $\lim_{t \to \infty} \zeta_t < \infty$. Therefore, $\tilde{\param}_t$ must converge to some $\param_*$. This establishes directional convergence of GF path.

To go further and show that $\param_*$ satisfies the KKT conditions \Cref{eq: KKT}, the key ingredient is to show the asymptotic alignment between $\param_t$ and $\hB_M(\param_t)$ for subsequence of $t$, i.e., 
\[
\lim_{t_m\to \infty}\beta(t_m) = \lim_{t_m\to \infty} \frac{\langle\param_{t_m}, \hB_M(\param_{t_m}) \rangle}{ \|\param_{t_m}\| \cdot \|\hB_M(\param_{t_m})\|} = 1,
\]
where we define
\begin{equation*}
    \hB_M(\param_t) \coloneqq  \frac{1}{n}\sum_{i=1}^n e^{-y_i f(\param_t;\xB_i)} y_i\nabla \homoPredictor(\param_t;\xB_i)
\end{equation*}
as a proxy of $\grad f (\param_t; \xB)$.
% Note that this also shows $\hB_M(\param_t)$ converges to the direction of $\param_*$ as $t \to \infty$.
We establish the following bound by looking further into the first inequality in \cref{eq:restate_gamma_gf_lowerbd}. 
\begin{lemma}[Restatement of \Cref{cor: beta bound}] For any $t_2>t_1$ large enough, there exists $t_*\in (t_1,t_2)$ such that 
\[
    \frac{1 - p_1(t_*)}{\big(\beta(t_*) + p_2(t_*) \big)^2}-1 \le \frac{1}{M} \cdot \frac{\log \GFmargin(\param_{t_2}) - \log\GFmargin(\param_{t_1})}{\log \|\param_{t_2}\| - \log \|\param_{t_1}\|}, 
\]
where $p_1(t), p_2(t) \to 0$ as $t\to \infty$. 
\end{lemma}
As $\GFmargin (\param_t)$ converges and $\|\param_t\|$ diverges, the right-hand side of the above inequality converges to $0$. Hence, there must exist a subsequence $\{ \beta(t_m) \}$ converging to $1$. This shows that $\param_t$ and $\hB_M(\param_t)$ asymptotically aligns along this subsequence, and consequently verifies that $\param_*$ is a KKT point.
% This indicates $\beta(t)$ also converges to $1$. 

To establish directional convergence for GD, we need to show that the discrete arc length swept by $\tilde{\param}_t$ is finite, i.e., 
\begin{equation}\label{eq:gd_sketch_finite_arc}
    \sum_{t=s}^{\infty} \left\| \tilde{\param}_{t+1} - \tilde{\param}_t \right\| < \infty.
\end{equation}
Similar to the proof of GF, we construct a desingularizing function $\Psi$, and show that there exists a constant $c > 0$, such that for all large enough $t$,
\begin{equation}\label{eq:gd_sketch_desingular}
     \left\| \tilde{\param}_{t+1} - \tilde{\param}_t \right\| \leq c \left( \Psi \left(\gamma_*-\GDmargin (\param_t) \right) - \Psi \left(\gamma_*-\GDmargin (\param_{t+1}) \right) \right),
\end{equation}
where $\GDmargin$ is the modified margin for GD and $\gamma_* = \lim_{t \to \infty} \GDmargin (\param_{t})$. Summing up both sides of \Cref{eq:gd_sketch_desingular} immediately leads to \Cref{eq:gd_sketch_finite_arc}, thus proving directional convergence of $\param_t$ along the GD path. The proof of KKT convergence is completely similar to the case of GF.

\section{Conclusion} \label{sec:conclusion}
We show the implicit bias of gradient descent (GD) for training generic non-homogeneous deep networks under exponential loss.
We show that the normalized margin induced by GD increases nearly monotonically.
Moreover, GD converges in direction, with the limiting direction satisfying the KKT conditions of a margin maximization problem.
Our results rely on a near-homogeneity condition and a strong separability condition, both of which are natural generalizations of the conditions used in prior implicit bias analysis for homogeneous networks. 
In particular, our results apply to networks with residual connections and non-homogeneous activation functions.

% Looking ahead, several promising directions remain to be studied. First, it would be valuable to prove that GD on near-homogeneous models can meet the initial conditions outlined in \Cref{asp:initial-cond-gf} and \Cref{asp:initial-cond-gd}. Another direction is to further weaken the near-homogeneous condition to capture the attention mechanism and transformer architectures. Finally, looking into the limit of the parameter direction could shed light on the generalization behavior of neural networks. 

% \ML{clarify if this last sentence is about transformers?}

\section*{Acknowledgements}
We thank Fabian Pedregosa for his helpful comments.
We gratefully acknowledge the support of the NSF for FODSI through grant DMS-2023505, of the Founder's Postdoctoral Fellowship in Statistics at Columbia University, of the Sloan Fellowship by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research’s Applied Mathematics Competitive Portfolios program under Contract No. AC02-05CH11231, of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and \#814639, DMS-2210827, CCF-2315725, and of the ONR through MURI award N000142112431 and N00014-24-S-B001.