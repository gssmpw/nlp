
The point cloud-based XR teleoperation system enables immersive teleoperation and data collection by allowing users to interact with a real-world robotic setup through an Extended Reality (XR) interface. Unlike conventional video-streaming methods, this approach provides spatial awareness and depth perception by rendering real-time 3D reconstructions of the environment in XR. The system consists of an ORBBEC Femto Bolt depth camera positioned in front of the robot to capture RGB and depth data in real time. A QR code on the robot’s end effector serves as a reference marker for pose estimation, aligning point clouds with the robot’s frame. A C++-based point cloud processing pipeline converts raw data into XYZ-RGB point clouds at 30Hz, generating approximately 2 million points per frame. To optimize computational efficiency, GPU-accelerated voxel grid downsampling is applied before transmission. The processed point clouds are sent to a main process that handles transformation, cropping, and communication with the XR system. A ZeroMQ (ZMQ) messaging protocol is used to enable efficient, low-latency communication between system components, ensuring real-time transmission of point clouds and control commands. A Meta Quest 3 headset renders the processed point clouds in real time, enabling users to visualize and interact with the scene in a fully immersive 3D environment.

\subsubsection{Camera-to-Robot Base Transformation}

\paragraph{Pose Estimation and Homogeneous Transformation}
To ensure accurate spatial alignment, the system transforms the camera’s coordinate frame into the robot’s base coordinate frame using a homogeneous transformation matrix, computed as follows:

\begin{enumerate}
    \item The QR code on the robot’s end effector provides a fixed reference point for tracking position and orientation in the camera's space.
    \item Using Forward Kinematics (FK), the end effector’s pose relative to the robot base is determined.
    \item The camera-to-robot base transformation is derived as:
    \begin{equation}
        T_{\mathrm{Robot Base}}^{\mathrm{Camera}} = T_{\mathrm{Robot Base}}^{\mathrm{End Effector}} \cdot T_{\mathrm{End Effector}}^{\mathrm{Camera}}
    \end{equation}
    where:
    \begin{itemize}
        \item $T_{\mathrm{Robot Base}}^{\mathrm{End Effector}}$ is obtained from the robot’s FK.
        \item $T_{\mathrm{End Effector}}^{\mathrm{Camera}}$ is estimated using the QR code tracking system.
    \end{itemize}
\end{enumerate}

This ensures that the captured point clouds align precisely with the robot’s coordinate frame, eliminating drift and inconsistencies in XR visualization.

\subsubsection{Point Cloud Processing Pipeline}
\label{appendix:Point Cloud Processing Pipeline}
\paragraph{Raw Data Acquisition}
The ORBBEC Femto Bolt camera captures:
\begin{itemize}
    \item Depth images (encoded as a depth map),
    \item RGB images (color information),
    \item Camera intrinsic parameters (for depth-to-3D conversion).
\end{itemize}

\paragraph{Conversion to XYZ-RGB Point Cloud}
Using the camera intrinsics, each depth pixel is converted into 3D world coordinates $(X, Y, Z)$ using:
\begin{equation}
    X = (u - c_x) \frac{Z}{f_x}, \quad Y = (v - c_y) \frac{Z}{f_y}, \quad Z = D(u,v)
\end{equation}
where:
\begin{itemize}
    \item $(u, v)$ are pixel coordinates,
    \item $(c_x, c_y)$ are the camera's principal point offsets,
    \item $(f_x, f_y)$ are focal lengths,
    \item $D(u,v)$ is the depth value at pixel $(u, v)$.
\end{itemize}
Each $(X, Y, Z)$ point is assigned an $(R, G, B)$ value from the color image, forming the XYZ-RGB point cloud.

\paragraph{Cropping and Filtering}
To reduce noise and retain only relevant portions of the workspace, the system:
\begin{itemize}
    \item Crops unnecessary regions using bounding box constraints,
    \item Applies statistical outlier removal to eliminate noise points.
\end{itemize}

\paragraph{Voxel Grid Downsampling (GPU-Accelerated)}
Since the raw point cloud consists of approximately 2 million points per frame, direct transmission is computationally expensive. To optimize efficiency, voxel grid downsampling is applied, which:
\begin{itemize}
    \item Divides the workspace into 3D voxels,
    \item Averages all points within each voxel to generate a single representative point,
    \item Reduces the point cloud size while preserving structural details.
\end{itemize}

\subsubsection{XR Visualization and Teleoperation}

\paragraph{Real-Time Communication with XR System}
The processed point cloud is transmitted to a main process, which then relays the data to the Meta Quest 3 headset using ZeroMQ. ZeroMQ ensures efficient, asynchronous, and low-latency messaging between the processing node and the XR system, allowing:  
\begin{itemize}  
    \item Real-time rendering of the scene in XR,  
    \item Full 3D immersion with accurate depth perception,  
    \item Reliable transmission of point cloud data and control commands.  
\end{itemize}  

\paragraph{Leader-Follower Teleoperation Framework}
The teleoperation system consists of:
\begin{itemize}
    \item A leader robot controlled by a human wearing the Meta Quest 3 headset,
    \item A follower robot that replicates the leader’s movements, including gripper actions.
\end{itemize}

A networked control architecture ensures:
\begin{itemize}
    \item Low-latency synchronization between the leader and follower,
    \item Seamless remote manipulation, removing the need for physical human presence.
\end{itemize}

\subsubsection{Calibration for XR-Robot Alignment}
To ensure that XR visualization aligns precisely with real-world objects, the system undergoes a calibration process consisting of:
\begin{enumerate}
    \item Robot workspace alignment: The follower robot’s workspace is adjusted to match the leader’s XR-rendered environment,
    \item Point cloud adjustment: The camera-to-robot transformation is fine-tuned,
    \item User feedback correction: Users verify object positions in XR and real-world views.
\end{enumerate}

\subsubsection{Future Extensions: Multi-Camera and ICP-Based Fusion}
Future implementations will:
\begin{itemize}
    \item Incorporate multiple cameras for wider coverage,
    \item Perform point cloud fusion using Iterative Closest Point (ICP) for improved spatial accuracy,
    \item Enhance depth perception by reconstructing high-fidelity 3D representations of the workspace.
\end{itemize}

