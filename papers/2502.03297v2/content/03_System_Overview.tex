\section{System Overview}


This section introduces the features and technical details of IRIS.
An overview of its paradigm is shown in Figure \ref{fig:system_overview}.

\input{image/system_overview}

\subsection{IRIS}

% In this section,
% We separate the physical simulation and rendering into a simulation PC and MR headsets
% ScenePublisher

\subsubsection{Node Communication Protocol}
% The software architecture of IRIS utilized master node and node,
% while simulation PC is the master node and headsets as well as other devices are XR node.
% The communication between them is based on socket and ZMQ.
% Inspired by ROS, a communication protocol is created for multiple clients and transmitting data in different kinds of format.
% To make all the XR node find the master node,
% master node broadcasts UDP messages in a fixed rate to broadcast port in the network,
% then all the XR nodes will get the server ip information including ZMQ socket address and port from the master node so that XR node will start ZMQ connection to the master node.
% After that, they can communicate in request-response and publish-subscribe patterns by ZMQ.
% When the master node is offline, the XR headsets will keep listen to the discover port and wait for next master node.
% This flexible and extendable communication framework supports multiple nodes, auto-reconnection and auto-recovering from disconnection.

% IRIS operates across a simulation PC and multiple XR headsets, it requires a robust and reliable connection between them, and they need to identify each other and connect.
% In IRIS, All the simulation and XR headsets are running in the same subnet, and the XR headsets are connected by WIFI router, they should use the IP address with the same subnet.
% Therefore, IRIS expand the function of ROS from communicating in a local host to local network with different host. introduces a custom communication protocol designed to support multiple nodes across different devices and enable data transmission using request-response and publish-subscribe patterns.
% This communication protocol adopts a master-node and multi-node architecture, where the simulation PC serves as the master node and the XR headsets, along with other devices, function as XR nodes.
% Communication between these nodes is implemented using sockets and ZMQ.
% To ensure that all XR nodes can discover the master node,
% the master node broadcasts UDP messages to a fixed broadcast port on the network in a rate of five Hz.
% XR nodes receive these messages and extract the master node's details, including the ZMQ socket address and port, enabling them to establish a stable ZMQ connection with the master node.
% Once connected, communication between nodes seamlessly follows the request-response and publish-subscribe patterns supported by ZMQ.
% When the master node is offline, XR nodes continue listening the discovery port, ready to reconnect to a new master node as soon as it the master node relaunches.
% This robust communication framework supports multiple nodes, automatic reconnection, and seamless recovery from disconnections, making it highly reliable and well-suited for dynamic, multi-device XR systems.

The IRIS system operates across simulation and/or sensor-processing computers, multiple XR headsets, and other monitoring and control programs, requiring a robust and reliable network connection between them.
All devices are part of the same subnet, with all the devices connected via Wi-Fi or cable.
Inspired by the Robot Operating System (ROS \cite{quigley2009ros}), 
However, instead of using a single host IRIS leverages a local network across multiple hosts, 
while using both Request-Response and Publish-Subscribe patterns for data transmission.
This protocol follows a master-node architecture with multiple nodes, where the simulation PC serves as the master node, and the XR headsets and other devices act as XR nodes.
Communication is achieved through a combination of UDP sockets and ZeroMQ (ZMQ).

\input{image/protocol}

To ensure node discovery, the master node broadcasts UDP messages at 5 Hz to a fixed broadcast port on the network.
When a new XR node is launched, 
it listens on the broadcast port to receive a broadcast message from the master node. 
%It extracts the master node's details, 
%including its ZMQ socket address and port, to establish a stable ZMQ connection.
Upon receiving the broadcast message, the XR node extracts the master node's details, including its ZMQ socket address and port, to establish a reliable ZMQ connection.
If the master node goes offline, XR nodes continue listening on the discovery port, allowing automatic reconnection when the master node relaunches.
This protocol (Fig. \ref{fig:protocol}) achieves \textbf{Cross-User} ability of IRIS, ensures reliable communication, automatic reconnection, and smooth recovery from disconnections,
making it ideal for dynamic multi-device XR systems.


\subsubsection{Unified Scene Specification}
% To visualize a scene with arbitrary objects in headsets from the simulation,
% the headsets need to receive the scene model from the simulation and rebuild them.
% The XR application running in the headsets is developed by C\# and Unity,
% and simulation is running by Python or C++.
% From related works, they only support specific robots and assets,
% since they need to create some identical predefined models in a model set for the XR application,
% and they don't support robots or objects which is not in the models set.
% This mechanics highly restrict the flexibility and reusability of their system.
% To fully support all the objects, robots, and assets from simulation,
% we defined a unified scene model format and directly read the scene from simulation.
% Then this information will be sent to the headset by the node communication protocol for rebuilding an identical scene in our XR application.

To visualize a scene with arbitrary objects, the XR headsets need to receive the scene model from the simulation and reconstruct it.
However, the XR application and the simulation run on different devices and use different software architectures (the XR application is developed with C\# and Unity, while simulators might be built in Python or C++).
This makes it impractical to directly transfer the scene from the simulation to the XR environment.
To address this issue, existing solutions rely on predefined models in the XR application for specific robots and assets, requiring a static set of models to be maintained within the application.
This approach restricts flexibility and reusability, preventing the support of robots or objects not included in the model set.
To address this issue, IRIS introduces a novel unified scene specification, which is generated by parsing the scene directly from the simulation.
This specification is subsequently transmitted to the headsets using the node communication protocol, enabling the XR application to accurately and dynamically recreate the scene in real time.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{image/xinkai_tree.pdf}
    \caption{
The hierarchical structure of the Scene Specification begins with a root SimObject, which contains all objects in the scene. Each SimObject has a name, a list of child SimObjects, and a list of visuals. Each visual represents a geometric element attached to the object.
Within each geometric element, materials define properties such as color and texture, while meshes determine the shape. The scene's raw data includes the byte streams of meshes and textures. Since these streams are extensive, they are sent to XR headsets sequentially after the initial scene specification is transmitted.
}
    \label{fig:scene_specification}
\end{figure}

% The unified scene specification includes environment setting (e.g., lighting) and all the objects as well as their geometry elements, meshes, materials and textures attached to them.
% IRIS provides a Python Library named ScenePublisher for parsering a simulation scene to a scenespevification.
% In the scene specification, geometry element is the shape of object (e.g., Cube, sphere, capsule, cylinder, or mesh).
% The mesh contains the vertex list, faces list, normals list, and texture coordinate list.
% material is the color and eemissionColor, reflectance or texture attached to them.
% Texture is an image which repesetnt texture.
% The objects is stacked by the kinematic tree and they will be parsed into json format by the ScenePublisher.
% Since the mesh and texuture contain too much data, and they will be sent to clients at beginning because it is rather big.
% So the visual and material which have mesh and texture will hold an hash code, and wait the request from XR node to retrieve it later.

The unified scene specification (shown in Fig. \ref{fig:scene_specification}) includes all objects with their geometry, meshes, materials, and textures. 
IRIS provides a Python library called \textit{ScenePublisher} to parse a simulation scene into this scene specification.
In the specification, 
geometry defines the object's shape (e.g., cube, sphere, capsule, cylinder, or mesh).
The mesh contains vertex lists, face lists, normals, and texture coordinates. 
Materials define surface properties, including color, emission color, reflectance, and attached textures, 
and the texture is an image representing the object's surface appearance.
Objects are organized using a kinematic tree structure and are serialized into JSON by the \textit{ScenePublisher}.

Since meshes and textures contain large amounts of data, 
geometry and materials store only their hash code to reduce the transmission load. 
The XR application rebuilds the scene upon receiving the kinematic tree and then requests the meshes and textures from the simulation server in byte format.
In some cases, textures can be quite large (e.g., the textures for the RoboCasa \cite{nasiriany2024robocasa} scene exceed 700 MB). To ensure the scene loads within an acceptable time for users, we compress the textures. This compression reduces the loading time to a few seconds.
Afterwards, the \textit{ScenePublisher} continually acquires simulation states and forwards them to the XR headset. This way the positions and rotations of all the objects are updated at a fixed frequency.

The scene specification enables IRIS to support a wide range of robots and objects in simulation, facilitating both \textbf{Cross-Scene} and \textbf{Cross-Embodiment} capabilities.


\subsubsection{Extendable and Flexible Framework Support}
The unified scene specification is a general definition that does not rely on any specific simulator, providing an extensible mechanism for scene loading and updating. IRIS can be easily adapted to various simulation engines and frameworks by implementing a new simulation parser to generate the unified specification from the simulation scene and a new publisher to update the states of scene.

Currently, IRIS supports scene parsers for MuJoCo, IsaacSim, CoppeliaSim, and Genesis, with the potential to be extended to other simulation engines as desired.
IRIS has been tested in some MuJoCo-based benchmarks including \textbf{Meta World} \cite{yu2020meta}, \textbf{LIBERO} \cite{liu2024libero}, \textbf{RoboCasa} \cite{nasiriany2024robocasa}, \textbf{robosuite} \cite{zhu2020robosuite}, \textbf{Fancy Gym} \cite{fancy_gym}, and CoppeliaSim-based benchmark like 
\textbf{PyRep} \cite{james2019pyrep},
\textbf{Colosseum} \cite{pumacay2024colosseum}.
This demonstrates that IRIS can be easily adapted to various benchmarks and simulators, highlighting its \textbf{Cross-Simulator} capability.


IRIS provides a user-friendly API.
For each environment or framework,
a single line of code suffices to visualize and update the simulation in the XR headset.
Here is a short example of how to use it in the MuJoCo Simulation, where the important line is marked in bold: 
\begin{lstlisting}[language=Python]
# import scenepub
from scenepub.sim.mj_publisher import MujocoPublisher
# define the mujoco environment
model = mujoco.MjModel.from_xml_path(xml_path)
data = mujoco.MjData(model)
# define the ScenePublisher for mujoco
(*@\textbf{publisher = MujocoPublisher(model, data, host)}  @*)
# run simulation
while True:
    # run simulation step logic
    pass
\end{lstlisting}

The MujocoPublisher instance only needs to access the model and data from the MuJoCo simulation. It then creates a separate thread to run the communication protocol, automatically connecting and communicating with all available XR headsets.
IRIS provides various Simulation Publishers for different environments, all with a consistent and easy-to-use interface. This mechanism ensures a seamless experience, making the system very user-friendly.
% \input{table/sim_framework_support_table}

\subsubsection{Real Scene Loading}
\label{sec:real_world_teleop}
% The loading and updating of real scene is similar to the simulation.
% IRIS use cameras and calibrate them for merging multiple point clouds into one point cloud.
% Then the point cloud will be cropped and down-sampled,
% each point will be sent to headsets and IRIS use particle system in Unity to visualize and update the point cloud.

% We merge and down-sample the point clouds from multiple cameras by first applying extrinsic transformations to each point cloud. 
% Then, we map the XYZ coordinates of the both pointclouds to voxel-grid indices.
% Next, we blend the colors of all points within a voxel and compute the voxel's centroid XYZ coordinates, returning both as output.
% This entire process uses Thrust \cite{bell2012thrust} and runs on the GPU to achieve low latency.

% The loading and updating of real-world scenes in IRIS follows a process similar to that used for simulation scenes, making the feature of Cross-Reality.
The loading and updating of real-world scenes in IRIS follow a process similar to that of simulation scenes, demonstrating its Cross-Reality capability.
It processes point clouds from one or multiple RGB-D cameras, which are extrinsically calibrated to a fiducial marker in the scene. A point cloud processor applies the extrinsic transformation to each point cloud before merging them. The merged point cloud is then cropped and downsampled using a voxel-grid filter.
This filter maps all 3D points to voxel-grid indices, blends the colors of points within the same voxel, and calculates the voxels centroid.
The output is a reduced point cloud that retains both color and position data.
To ensure low latency, this process runs on the GPU using Thrust \cite{bell2012thrust}.
Finally, the processed point cloud is transmitted to the headsets, where IRIS uses a particle system in Unity to visualize and dynamically update the scene in real-time.


\subsubsection{Interaction Data Collection}
\label{sec:interaction_data_collection}
IRIS provides user-friendly access to interaction data through the \textit{ScenePublisher}, which supports both Meta Quest 3 and HoloLens 2. 
Users only need to create a new device instance and assign a name to the connection.
The \textit{ScenePublisher} then waits automatically for the device to launch.
Once the device is launched, the instance receives messages from the XR headset, enabling users to access various types of interaction data through different methods.
This process makes it straightforward to retrieve data such as hand tracking, motion controller inputs, and other relevant interaction information.
The interaction data accessible through the ScenePublisher is listed in Tab. \ref{tab:feature-comparison},
and the example code can be found in the Appendix.
\ref{app:interaction_data}





\input{table/interaction_data}



\subsubsection{Spatial Anchor}
\label{sec:spatial_anchor}
% \textcolor{red}{rewrite this part}

% Spatial anchor serves as a reference in a 3D environment to accurately position virtual objects within physical space. 
% It allows virtual objects to maintain their position, orientation, and alignment even as users move around or leave and return to the environment.
% IRIS use QR code or motion controller (only for Meta Quest 3) as spatial anchor
% Alignment between the virtual environment and the physical world is achieved by using a trackable QR code.
% The QR code serves as the coordinate frame of the world for the virtual environment,
% and users can provide the offset.
% This method could be used for align all the virtual scene from multiple headsets and makes them look like they are sharing the same scene in the real world.
% facilitates seamless alignment of virtual and physical elements, 
% This method could also be used for synchronizing a virtual robot with its real-world counterpart in the Kinesthetic Teaching interface and multiple-collector view alignment.
% The implementation of QR tracking varies in different headsets,
% and this function is supported by all the headsets in IRIS.

A spatial anchor serves as a reference point in a 3D environment to accurately position virtual objects within physical space.
It enables virtual objects to maintain their position, orientation, and alignment, even as users move around or leave and return to the environment.
IRIS utilizes either QR codes or motion controllers (currently only implemented for the Meta Quest 3) as spatial anchors.
This approach allows for the alignment of augmented scenes across multiple headsets in the real world, making it appear as though all users are sharing the same scene.
Additionally, this method can synchronize a virtual robot with its real-world counterpart in the {Kinesthetic Teaching} (\ref{sec:kt}) interface and facilitate multiple-collector view alignment.
Fig. \ref{fig:alignment} shows the alignment between real robot and virtual robot.

\input{image/alignment/alignment}

\subsubsection{Headset Compatibility}
% \textcolor{red}{rewrite this part}

% The basic hardware of IRIS is one PC for running the simulation and XR headsets.
% The whole architecture is extendable so that other hardware is also easy to integrate into IRIS if they use IRIS communication protocol.
% To project the scene to the MR devices,
% a local WIFI network is utilized to build communication between the simulation PC, the headset, and other devices.
% The local network has a shorter communication delay compared to the remote network or the cloud.
% Currently, we tested our system on HoloLens 2 \cite{hololens2_lolambean_2023} and Meta Quest 3.


IRIS implements an XR application using Unity, featuring essential capabilities such as node communication protocols, scene construction based on scene specifications, and synchronization with remote simulations.
The application can be directly deployed to other headset platforms using the Unity deployment pipeline, showcasing IRIS's \textbf{Cross-Platform} capability.
For input data handling (reading and sending), the application requires the use of platform-specific APIs, making it impractical to rely on a generalized framework.
This approach separates the visualization and interaction components, minimizing the effort needed to transfer IRIS codebase to new platforms.


\subsection{Intuitive Robot Control Interface}
\label{sec:intuitive_robot_interface}
% \textcolor{red}{should have some images of each interface}

In data collection tasks, robot control interfaces are used to operate the robot in both simulated and real-world environments.
Based on research in teleoperation and robot data collection \cite{jiang2024comprehensive}, Kinesthetic Teaching and Motion Controllers have been identified as the most intuitive and effective control interfaces.
Hence, we ensured that IRIS supports these two methods.
Thanks to IRIS's flexible framework, it is possible to easily customize and implement additional alternative control interfaces,
such as hand tracking, gloves, smartphones, or motion tracking systems.
This adaptability enables tailored solutions to meet specific requirements, enhancing both the usability and versatility of the system for various applications.
Fig. \ref{fig:interface} shows how these two interfaces work in IRIS. The implementation of these interfaces is outlined below.

\input{image/interface/interface}

\subsubsection{Kinesthetic Teaching}
\label{sec:kt}
% Kinesthetic teaching is an intuitive interface controlling robots by physically moving the real robot.
% The real robot transmit joints position and velocity to the controlled robot in the real time,
% and the controlled robot could be real robot or virtual robot.
% In both real world and simulation data collection,
% the key of Kinesthetic Teaching is the alignment of the real robot with the corresponding virtual robot in the XR headsets,
% so that usrs feel an immersive control. 
% By using spatial anchor, the robot objects in the XR and points of cloud could be perfectly match to the real robot.

Kinesthetic teaching is an intuitive interface that allows users to control robots by physically moving the real robot \cite{wrede2013user, sukkar2023guided, jiang2024comprehensive}.
The real robot transmits joint positions and velocities in real time to the controlled robot, which can be either a virtual or another physical robot.
In both real-world and simulation data collection,
the key aspect of kinesthetic teaching is ensuring alignment between the real robot and its virtual counterpart in the XR headsets.
By utilizing \textit{Spatial Anchors} (\ref{sec:spatial_anchor}), the virtual robot in the XR headsets can be perfectly aligned with the real robot, which provide users with an intuitive and immersive experience.

% In addition, our Kinesthetic Teaching interface provides force feedback for users,
% which means that users feel the resistance force from real robot when controlled robots contacts with objects,
% this feature also applied to real world data and virtual data collection.


\subsubsection{Motion Controller}

% Motion controllers are widely used in robot data collection.
% \textcolor{red}{some papers}
% They offer stable tracking and a flexible approach for various applications.
% While some XR headsets, such as the HoloLens 2, do not natively support motion controllers, this limitation can be addressed by integrating third-party motion controllers compatible with platforms like SteamVR. \textcolor{red}{citation}
% Motion controllers use inverse kinematics to control robots in Cartesian space. The movement of the robot’s end effector is dictated by the motion controller's trigger. 
% IRIS supports retrieve motion controller data from Meta Quest 3.
% For more complex scenario, users could also design their own controller based on the our IRIS.
% From our usage, motion controllers are only suitable for two-gripper end effectors, and it is challenging to use them with robotic hands to perform complex movements.

Motion controller Interfaces are commonly used in robot data collection and teleoperation \cite{pettinger2020reducing, lin2022comparison}, providing stable tracking and flexibility for various applications. 
Although some XR headsets, such as HoloLens 2, do not natively support motion controllers, this limitation can be resolved by integrating third-party controllers compatible with platforms such as SteamVR \cite{steampoweredSteamVR}.
Motion controllers utilize inverse kinematics to control robots in Cartesian space, with the movement of the robot’s end effector controlled by the controller's trigger. 
IRIS supports retrieval of motion controller data from devices like the Meta Quest 3. 
For more complex scenarios, users can design custom controllers using IRIS' flexible framework.
% Based on our experience, motion controllers are well-suited for two-gripper end effectors, 
% but are less effective for controlling robotic hands in tasks requiring complex movements.


% \subsubsection{Hand Tracking}

% Hand tracking is an intuitive method for controlling robots, especially humanoid robots or robotic arms with hand-like end effectors.
% The inside-out hand tracking (HT) interface uses the cameras of XR headsets to track hand movements and recognize gestures, 
% which is widely supported across different types of XR devices.
% This method is particularly useful for data collection tasks, as it closely mimics robotic hand movements, providing a natural and immersive control experience.
% \textcolor{red}{not sure about this part}
% IRIS provides an easy-to-use Python API (\ref{sec:interaction_data_collection}) to access hand tracking data. 
% Although this interface has only been tested with two-finger end effectors rather than humanoid robots. 
% users can easily leverage IRIS to develop custom controllers for their own robotic systems.
% However, hand tracking has some limitations.
% It requires the user's hands to stay within the headset's field of view, and tracking can be disrupted by occlusions, poor lighting, or fast hand movements, reducing its reliability for precise control.


% \textcolor{red}{should be a table of interfaces here}

\subsection{Affiliated Monitor Tools}
The extensibility of IRIS opens up numerous possibilities for creating new applications. IRIS includes a web-based monitoring tool for managing all XR headsets. This tool allows users to easily start and stop alignment processes, as well as rename devices.
Additionally, the tool supports real-time scene visualization using \textit{three.js} \cite{threejsThreejsDocs}, using the unified scene specification. An example screenshot is shown in Fig. \ref{fig:webapp}, with further technical details provided in the appendix. \textcolor{red}{}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{image/webapp.png}
    \caption{
The IRIS Dashboard, accessible via a web interface, 
allows the control and monitoring of all connected nodes. 
The scene streamed by IRIS is rendered on the right-hand side. 
The left panel displays the connected XR devices, 
providing an interface through which users can control all services made available by each device.
    }
    \label{fig:webapp}
\end{figure}



