\section{System Application}

\subsection{Robot Data Collection for Benchmarks}

Through its flexible framework design, IRIS supports four simulators and various robot manipulation benchmarks.
Based on the interfaces introduced in Sec. \ref{sec:interaction_data_collection},
IRIS already provides example controllers for various benchmarks and frameworks, such as robosuite, LIBERO, RoboCasa, mink, Metaworld, Fancy Gym, and so on.
% Since all the objects in the simulation including robots and other objects are considered as one item, so naturally IRIS supports visualize any type of robot in the XR headsets from the simulation without any configuration.
Robots from all the frameworks can be controlled by Motion Controller or Kinesthetic Teaching.
Fig. \ref{fig:front_page} shows the robots controlled by IRIS including Franka Panda, Aloha 2, Barrett Wam Arm, UR5e, iiwa14, Boston Dynamics Spot, Unitree H1, and more.


% \textcolor{red}{picture with all kinds of robot control}

% \input{image/aloha}


% \subsection{Deformable Object Manipulation}

% As far as we know, no existing work has explored the manipulation of deformable objects using XR technologies.
% This is a significant gap in the field, as deformable object manipulation presents unique challenges compared to rigid object manipulation, including the need to handle continuously changing shapes, states, and dynamics in real time.
% Deformable objects, such as fabrics, ropes, or soft materials, are more complex to simulate and interact with due to their high degrees of freedom and nonlinear behaviors.

% IRIS supports deformable object manipulation by dynamically updating the mesh state in real time.
% This capability enables users to interact intuitively with deformable objects in a simulated environment,
% making it possible to train and test robotic algorithms for tasks that involve soft and flexible materials.


% To validate our framework, we conducted experiments in Isaac Sim, a state-of-the-art physics simulation environment designed for robotics applications.
% The specific task we designed for testing is folding a jacket, a representative challenge in deformable object manipulation. Jacket folding is a highly dynamic task that requires precise control and adaptability to continuously changing object shapes.
% \textcolor{red}{Qihao:}

% \subsection{Bimanual Robot Manipulation}

% % \textcolor{red}{rewrite this part}

% The collection of bimanual robot data is challenging for previous work.
% However, it is quite simple for IRIS for its compatiblity.


% Humanoid robots are inherently complex and challenging to implement and utilize effectively in the real world.
% These challenges arise from various factors, including the high cost of hardware, difficulties in maintaining precision and reliability, and the requirement for sophisticated infrastructure.
% Real-world robot training often demands extensive physical space, a controlled environment, and advanced sensors and actuators, all of which contribute to significant logistical and financial burdens.
% Furthermore, safety concerns and the potential for physical damage to robots during training add additional layers of complexity, particularly in dynamic or unstructured environments.

% Given these barriers, the use of simulation environments becomes not only a practical but also a highly effective alternative. Extended Reality (XR) technologies, in particular, have opened new doors for creating immersive and interactive virtual environments that bridge the gap between human users and robots.
% With XR, users can access and control virtual robots without the need for expensive hardware, allowing for a more flexible and cost-efficient approach to train, test and deploy their algorithm.

% In this paper, we explore using hand tracking to control humanoid robot as an intuitive and accessible interface for interacting with and testing virtual robots in simulated environments.
% Specifically, we utilize hand-tracking to evaluate robot performance across various datasets and tasks, demonstrating the feasibility and practicality of this approach in a simulated context. By leveraging the power of XR and simulation, we aim to democratize access to robotic training and lower the barriers to innovation in robotics.

\subsection{Collaborative Manipulation}

\input{image/collaborative_collection/collaborative_collection}

% Collaborative manipulation is a critical technology for advancing human-robot systems. It involves enabling multiple users or robots to work together seamlessly on shared tasks, leveraging their unique strengths to achieve goals that would be difficult or impossible to accomplish individually. 
% One of the major challenges in collaborative manipulation lies in ensuring smooth communication and synchronization between human participants, robots, and the virtual environment.
% To address this, our system employs a flexible communication framework that allows for easy integration of new devices and interfaces. Specifically, our system supports the addition of new XR headsets with minimal effort, enabling more users to join the collaborative environment dynamically. This flexibility makes it possible to scale up or adapt the system as needed, facilitating diverse use cases and applications.


% When a new XR application is started, the first step is to establish communication with the simulation master node. The master node serves as the central coordinator for all XR nodes and is responsible for managing the shared virtual environment. Upon connection, the master node sends the current scene data to the new XR node, ensuring that the new application has a synchronized starting point with the existing XR nodes.
% However, upon initialization, the newly added XR scene is not automatically aligned with the scenes of the existing XR nodes. To resolve this, the new XR node must locate a spatial anchor, which serves as the foundational reference point for aligning all XR nodes within the shared environment. 
% The spatial anchor ensures that virtual objects, scenes, and interactions are consistently positioned across devices, regardless of variations in the physical setup or device configurations.
% By aligning its virtual environment with the spatial anchor, the new XR node can seamlessly integrate into the shared scene, providing users with a unified and immersive experience.
% This process is critical for maintaining coherence and synchronization in multi-user XR environments, especially in applications like collaborative manipulation tasks. The spatial anchor acts as the cornerstone for all XR nodes, enabling accurate interaction and coordination among participants and ensuring that the virtual environment behaves as a single, unified system.


% By combining these technologies, our system enhances the user experience in collaborative manipulation tasks, making interactions more natural, intuitive, and efficient. This work demonstrates how XR can bridge the gap between humans and robots in shared environments, fostering better teamwork and paving the way for more advanced and scalable human-robot systems.


Collaborative manipulation, where multiple users provide demonstrations simultaneously and/or interactively, plays a crucial role in the advancement of human-robot systems \cite{tung2021learning}.
Collecting demonstration data for such tasks has been a significant challenge as it requires smooth communication and synchronization between human participants, robots, and the virtual environment.
Previous work on manipulation involving multiple humans often facilitates collaboration through multiple screens  \cite{Qin2023AnyTeleopAG}, 
which lacks the immersive experience provided by XR.
Moreover, typically only one person is in control of the demonstrations while others are limited to observing or monitoring \cite{szczurek2023multimodal}.
By leveraging the node communication protocol,
IRIS allows for easy integration of new devices and interfaces for controlling multiple robots in a shared scene.
Specifically, IRIS supports the addition of new XR headsets with minimal effort,
allowing more users to dynamically join the collaborative environment.
% This flexibility allows for scaling or adapting the system as needed, facilitating diverse use cases and applications.
% A handover task was chosen as an example, as illustrated in Fig. \textcolor{red}{image}.
% In this scenario, two operators each control two Aloha 2 robotic arms to pass a red board between them.
Fig. \ref{fig:collaborative_collection} shows collaborative manipulation for handover task.
% \textcolor{red}{more pictures!!!!!}

\subsection{Interaction with Robot Policies in Simulation}
Simulation has long been an essential tool for training robot agents, especially for reinforcement learning (RL) policies.
Its advantages -- such as safety, scalability, and cost-efficiency -- make it widely used in RL training. 
Previous works on interactive learning have utilized interfaces such as keyboards \cite{mandlekar2018roboturk}, 3D mouse \cite{luo2024precise, liu2024libero} or smartphone \cite{mandlekar2023human}. However, these methods are insufficient for tasks that require multiple viewpoints or complex motions.
% However, collecting human-robot interaction data for these agents remains challenging, as accurately modeling human behavior in simulation is inherently complex.
% IRIS offers a novel solution by bridging the gap between simulation and reality.
% By projecting the simulation into the real world, IRIS enables a real human operator and a virtual RL agent to share the same workspace.
% This immersive interaction framework overcomes the limitations of simulation-only setups. 
% It provides more flexibility in data collection with different robot models and scenes, facilitating human-robot interactive data collection for cross-embodiment and cross-scene tasks.
IRIS delivers an immersive experience, providing the appearance of "stepping into" the simulation.
Its interaction API (\ref{sec:interaction_data_collection}) allows users to effortlessly move and manipulate objects within the simulation, mirroring real-world interactions.
This capability enables IRIS to serve as a powerful tool for highly interactive tasks, such as competitive sports, where real-time responsiveness and precise control are essential.

\input{image/interact_with_policy/interact_with_policy}

To demonstrate the capabilities of IRIS,
we designed an experiment in which a participant played table tennis against an episodic RL agent trained with \textit{Deep Black-Box Reinforcement Learning} (BBRL) \cite{otto2023deep} exclusively in simulation. 
The environment was adapted from Fancy Gym \cite{fancy_gym}.
Table tennis is a challenging task that requires dynamic perspective shifts and precise racket control.
Using a motion controller and XR headset, the participant learned to successfully return the ball the RL agent served within a few minutes.
This success highlights the potential of IRIS to facilitate high-quality interactive data collection, which can further enhance the training and performance of RL agents.

% table tennis

\subsection{Real World Teleoperation and Data Collection}
\label{sec:real_world_teleop}

Real robot data plays a crucial role in conducting real-world experiments. To minimize physical obstruction from humans, tele-operation using XR (Extended Reality) is commonly employed for collecting such data.
Some approaches, such as \cite{openteach}, utilize cameras to stream videos to XR headsets. However, this method lacks depth perception. Other works, like \cite{opentelevision}, employ movable platforms to track the movement of users' heads, allowing for active scene observation by adjusting the viewing perspective. While effective, this approach requires significant effort and resources to install and maintain the necessary hardware.

To overcome these limitations, we integrate a point cloud-based XR tele-operation system into our tele-operation framework, ensuring both immersion and interactivity for data collection. An ORBBEC Femto Bolt depth camera captures real-time depth and RGB data, and a QR code on the robot’s end effector enables camera-to-robot base transformation, aligning point clouds with the robot’s frame. A GPU-accelerated C++ pipeline processes (see Appendix~\ref{appendix:Point Cloud Processing Pipeline}) point clouds at 30Hz (\textasciitilde 2 million points per frame), applying voxel grid downsampling to reduce computational load and latency while maintaining real-time performance. Before downsampling, unnecessary regions are cropped, retaining only relevant portions of the workspace. The processed point clouds are transmitted to a main process, which forwards them to a Meta Quest 3 headset for real-time visualization and interaction.

\input{image/Real_robot/real_robot_img}

Our teleoperation framework features two Franka Emika Panda \cite{Franka_Emika_Robot} robots: a leader controlled by a human in zero-torque mode and a follower mirroring it's movements, including gripper actions. This setup enables seamless remote manipulation while eliminating the need for human presence in the physical workspace. In future implementations, additional cameras can be incorporated, and point cloud fusion via Iterative Closest Point (ICP) will ensure precise alignment across multiple views. The system architecture, including point cloud processing, XR integration, and teleoperation, is illustrated in Figure~\ref{fig:real_robot_point_cloud_setup}. By integrating real-time point cloud visualization with XR-based teleoperation, our system enhances efficiency, accuracy, and scalability, enabling obstruction-free data collection for human-robot interaction studies.



