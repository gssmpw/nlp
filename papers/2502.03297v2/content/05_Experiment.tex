\section{Experiment}

This section evaluates IRIS's capability to create demonstrations, focusing on the efficiency and intuitiveness of data collection.
It is followed by a performance profile analysis.

\subsection{Data Collection Evaluation in Simulation}

% IRIS is promising to collect demonstration in the simualtion,
% To verify the data collection efficiency of IRIS,
% a study was conducted to evaluate it.
% LIBERO was selected and 4 tasks is picked which represent four different movement.
% which includes
% \textit{close the microwave},
% \textit{turn off the stove},
% \textit{pick up the book in the middle and place it on the cabinet shelf},
% and
% \textit{turn on the stove and put the frying pan on it}.


% To make a fair comparison, we use the task from LIBERO and the data rather than designing other task.
% The baselines are two control interfaces from LIBERO (keyboards and 3D mouse),
% From the prior study from \cite{jiang2024comprehensive} \textcolor{red}{find another one support research} and our try,
% the hand tracking is not stable comparing to motion controller. so we choose KT and MC to conduct the user study.

% There are 8 participants in this user study, we evaluate each interface by using objective metrics ans subjective metrics.
% The objective metrics includes success rate and time consumed in each tasks,
% and the subjective metrics are conducted by questionnaires includes 
% \textit{Experience}, \textit{Usefulness}, \textit{Intuitiveness}, and \textit{Efficiency}.
% Every participant will use each interface to collect demonstration five times for each task,
% they will give a score in these four dimensions from 1 to 7.

% \input{image/user_study_task/user_studt_task_fig}
To assess the data collection efficiency of IRIS, a user study was conducted by using tasks from the LIBERO benchmark \cite{liu2024libero}, which represents diverse types of movements.
Four representative tasks (see Figure~\ref{fig:images_grid}) were selected in the dimension of translation, rotation, and compound movement:
\begin{itemize}
\item[-] Task 1: \textit{close the microwave}
\item[-] Task 2: \textit{turn off the stove}
\item[-] Task 3: \textit{pick up the book in the middle and place it on the cabinet shelf}
\item[-] Task 4: \textit{turn on the stove and put the frying pan on it}.
\end{itemize}

\input{image/user_study_task/user_studt_task_fig}

To ensure a fair comparison, the tasks and data were taken directly from LIBERO \cite{liu2024libero} without any modification.
The baselines for this study were two standard control interfaces provided by LIBERO: the Keyboard (KB) and the 3D Mouse (3M).
Based on findings from prior research \cite{jiang2024comprehensive},
hand tracking was found to be less stable than motion controllers. Therefore, we selected Kinesthetic Teaching and Motion Controller as the interfaces for the user study.

The study involved eight participants who evaluated the efficiency and intuitiveness of each interface for collecting demonstrations using both objective and subjective metrics.
The objective metrics included the success rate and the average time taken per task.
To ensure successful demonstrations, 
participants executed tasks at a very slow pace, which diluted efficiency measurements (as all interfaces appeared efficient when tasks were performed slowly), which biased participants against later interfaces \cite{jiang2024comprehensive}. 
To mitigate these biases and ensure high-quality data collection, a time limit per task was introduced,
and the time limits for four tasks are 20s, 20s, 30s, and 40s, which are quite enough for finishing the task.
The subjective metrics were assessed through a questionnaire evaluating four dimensions: \textit{Experience}, \textit{Usefulness}, \textit{Intuitiveness}, and \textit{Efficiency}.
Each participant performed each task five times using all interfaces. After finishing using one interface, they provided ratings on the subjective dimensions using a 7-point Likert scale.

\input{table/success_rate_table}
\input{image/objective_study}
For the objective metrics, Table \ref{tab:success_rate} presents the success rates for each interface across the tasks. 
A trial was considered unsuccessful if the participant failed to complete the task or exceeded the time limit. The time limits were set based on task difficulty: 20 seconds for Task 1 and Task 2, 30 seconds for Task 3, and 40 seconds for Task 4.
The data shows a success rate of over $90\%$ across all four tasks when using the KT and MC interfaces from IRIS. In contrast, the Keyboard and 3D Mouse methods from LIBERO often resulted in failures. For example, the 3D Mouse interface achieved only a $37.5\%$ success rate on task 3.
Figure \ref{fig:objective_study} shows the average time consumed for each task across four interfaces. The KT and MC interfaces consistently demonstrate lower task completion times, indicating higher efficiency. In contrast, the Keyboard and 3D Mouse interfaces show significantly higher completion times, particularly for Task 3 and Task 4, where the 3D Mouse method approaches or exceeds the task's time limit. These results align with the observed lower success rates for these interfaces, highlighting their inefficiency in time-critical tasks.


\input{image/subjective_study}

Figure \ref{fig:subjective_study} compares subjective scores for four interfaces based on usefulness, experience, intuitiveness, and efficiency.
The KT and MC interfaces consistently receive high scores across all criteria, indicating positive user perception and ease of use. 
In contrast, the Keyboard and 3D Mouse interfaces receive significantly lower ratings, particularly in intuitiveness and efficiency, reflecting the participants' difficulties in using these methods to control the robot.

The results of this user study show that IRIS received higher scores than the baseline interfaces across both objective and subjective metrics.
This indicates that the system offers a more intuitive and efficient approach for data collection.

\subsection{Performance Analysis}

The performance analysis focuses on two key aspects: network latency and headset FPS (frames per second).

Since IRIS uses asynchronous bidirectional data transfer, network latency is low, averaging around 20-30 ms. Transmission bandwidth depends on Wi-Fi capacity. Even for large scenes from RoboCasa, which contain over 200 MB of compressed assets, it takes no more than 5 seconds to transfer and generate a full scene with more than 300 objects. For real robot teleoperation, the system handles point cloud data efficiently, achieving a transmission speed of 10,000 points at 60 Hz—exceeding the camera’s frame rate.

The FPS performance depends on the hardware. On the Meta Quest 3, a scene with one robot runs at approximately 70 FPS, while on HoloLens 2, it runs around 40 FPS. As the scene size increases, FPS gradually decreases. With around 200 objects, the Meta Quest 3 headsets struggle to keep up with head movement, causing virtual objects to lag or become stuck. However, performance is additionally influenced by the complexity of the meshes and textures, as these require significant computational resources from the XR headset.

In our experiments by using Meta Quest 3, IRIS successfully handled all benchmark scenarios listed in the paper, except for some scenes from RoboCasa \cite{nasiriany2024robocasa}. These scenes have over 700 MB of assets in one single instance, which is closed to the Meta Quest 3 RAM limit. Nonetheless, IRIS was able to manage most of scenes from RoboCasa without any significant performance issues.
For real robot data collection, the optimal point cloud size is around 10,000 points, which achieves a balance between point cloud quality and FPS, maintaining a frame rate of approximately 40 FPS.


% \textcolor{red}{should be a table of performance here}


