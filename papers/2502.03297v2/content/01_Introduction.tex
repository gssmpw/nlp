\section{Introduction}

% \textcolor{red}{Still on working}

% \textcolor{red}{add label for each section}


Robot learning relies on diverse and high-quality data to learn complex behaviors \cite{aldaco2024aloha, wang2024dexcap}.
Recent studies highlight that models trained on datasets with greater complexity and variation in the domain tend to generalize more effectively across broader scenarios \cite{mann2020language, radford2021learning, gao2024efficient}.
% However, creating such diverse datasets in the real world presents significant challenges.
% Modifying physical environments and adjusting robot hardware settings require considerable time, effort, and financial resources.
% In contrast, simulation environments offer a flexible and efficient alternative.
% Simulations allow for the creation and modification of digital environments with a wide range of object shapes, weights, materials, lighting, textures, friction coefficients, and so on to incorporate domain randomization,
% which helps improve the robustness of models when deployed in real-world conditions.
% These environments can be easily adjusted and reset, enabling faster iterations and data collection.
% Additionally, simulations provide the ability to consistently reproduce scenarios, which is essential for benchmarking and model evaluation.
% Another advantage of simulations is their flexibility in sensor integration. Sensors such as cameras, LiDARs, and tactile sensors can be added or repositioned without the physical limitations present in real-world setups. Simulations also eliminate the risk of damaging expensive hardware during edge-case experiments, making them an ideal platform for testing rare or dangerous scenarios that are impractical to explore in real life.
By leveraging immersive perspectives and interactions, Extended Reality\footnote{Extended Reality is an umbrella term to refer to Augmented Reality, Mixed Reality, and Virtual Reality \cite{wikipediaExtendedReality}}
(XR)
is a promising candidate for efficient and intuitive large scale data collection \cite{jiang2024comprehensive, arcade}
% With the demand for collecting data, XR provides a promising approach for humans to teach robots by offering users an immersive experience.
in simulation \cite{jiang2024comprehensive, arcade, dexhub-park} and real-world scenarios \cite{openteach, opentelevision}.
However, reusing and reproducing current XR approaches for robot data collection for new settings and scenarios is complicated and requires significant effort.
% are difficult to reuse and reproduce system makes it hard to reuse and reproduce in another data collection pipeline.
This bottleneck arises from three main limitations of current XR data collection and interaction frameworks: \textit{asset limitation}, \textit{simulator limitation}, and \textit{device limitation}.
% \textcolor{red}{ASSIGN THESE CITATION PROPERLY:}
% \textcolor{red}{list them by time order???}
% of collecting data by using XR have three main limitations.
Current approaches suffering from \textit{asset limitation} \cite{arclfd, jiang2024comprehensive, arcade, george2025openvr, vicarios}
% Firstly, recent works \cite{jiang2024comprehensive, arcade, dexhub-park}
can only use predefined robot models and task scenes. Configuring new tasks requires significant effort, since each new object or model must be specifically integrated into the XR application.
% and it takes too much effort to configure new tasks in their systems since they cannot spawn arbitrary models in the XR application.
The vast majority of application are developed for specific simulators or real-world scenarios. This \textit{simulator limitation} \cite{mosbach2022accelerating, lipton2017baxter, dexhub-park, arcade}
% Secondly, existing systems are limited to a single simulation platform or real-world scenarios.
significantly reduces reusability and makes adaptation to new simulation platforms challenging.
Additionally, most current XR frameworks are designed for a specific version of a single XR headset, leading to a \textit{device limitation} 
\cite{lipton2017baxter, armada, openteach, meng2023virtual}.
% and there is no work working on the extendability of transferring to a new headsets as far as we know.
To the best of our knowledge, no existing work has explored the extensibility or transferability of their framework to different headsets.
These limitations hamper reproducibility and broader contributions of XR based data collection and interaction to the research community.
% as each research group typically has its own data collection pipeline.
% In addition to these main limitations, existing XR systems are not well suited for managing multiple robot systems,
% as they are often designed for single-operator use.

In addition to these main limitations, existing XR systems are often designed for single-operator use, prohibiting collaborative data collection.
At the same time, controlling multiple robots at once can be very difficult for a single operator,
making data collection in multi-robot scenarios particularly challenging \cite{orun2019effect}.
Although there are some works using collaborative data collection in the context of tele-operation \cite{tung2021learning, Qin2023AnyTeleopAG},
there is no XR-based data collection system supporting collaborative data collection.
This limitation highlights the need for more advanced XR solutions that can better support multi-robot and multi-user scenarios.
% \textcolor{red}{more papers about collaborative data collection}

To address all of these issues, we propose \textbf{IRIS},
an \textbf{I}mmersive \textbf{R}obot \textbf{I}nteraction \textbf{S}ystem.
This general system supports various simulators, benchmarks and real-world scenarios.
It is easily extensible to new simulators and XR headsets.
IRIS achieves generalization across six dimensions:
% \begin{itemize}
%     \item \textit{Cross-scene} : diverse object models;
%     \item \textit{Cross-embodiment}: diverse robot models;
%     \item \textit{Cross-simulator}: 
%     \item \textit{Cross-reality}: fd
%     \item \textit{Cross-platform}: fd
%     \item \textit{Cross-users}: fd
% \end{itemize}
\textbf{Cross-Scene}, \textbf{Cross-Embodiment}, \textbf{Cross-Simulator}, \textbf{Cross-Reality}, \textbf{Cross-Platform}, and \textbf{Cross-User}.

\textbf{Cross-Scene} and \textbf{Cross-Embodiment} allow the system to handle arbitrary objects and robots in the simulation,
eliminating restrictions about predefined models in XR applications.
IRIS achieves these generalizations by introducing a unified scene specification, representing all objects,
including robots, as data structures with meshes, materials, and textures.
The unified scene specification is transmitted to the XR application to create and visualize an identical scene.
By treating robots as standard objects, the system simplifies XR integration,
allowing researchers to work with various robots without special robot-specific configurations.
\textbf{Cross-Simulator} ensures compatibility with various simulation engines.
IRIS simplifies adaptation by parsing simulated scenes into the unified scene specification, eliminating the need for XR application modifications when switching simulators.
New simulators can be integrated by creating a parser to convert their scenes into the unified format.
This flexibility is demonstrated by IRISâ€™ support for Mujoco \cite{todorov2012mujoco}, IsaacSim \cite{mittal2023orbit}, CoppeliaSim \cite{coppeliaSim}, and even the recent Genesis \cite{Genesis} simulator.
\textbf{Cross-Reality} enables the system to function seamlessly in both virtual simulations and real-world applications.
IRIS enables real-world data collection through camera-based point cloud visualization.
\textbf{Cross-Platform} allows for compatibility across various XR devices.
Since XR device APIs differ significantly, making a single codebase impractical, IRIS XR application decouples its modules to maximize code reuse.
This application, developed by Unity \cite{unity3dUnityManual}, separates scene visualization and interaction, allowing developers to integrate new headsets by reusing the visualization code and only implementing input handling for hand, head, and motion controller tracking.
IRIS provides an implementation of the XR application in the Unity framework, allowing for a straightforward deployment to any device that supports Unity. 
So far, IRIS was successfully deployed to the Meta Quest 3 and HoloLens 2.
Finally, the \textbf{Cross-User} ability allows multiple users to interact within a shared scene.
IRIS achieves this ability by introducing a protocol to establish the communication between multiple XR headsets and the simulation or real-world scenarios.
Additionally, IRIS leverages spatial anchors to support the alignment of virtual scenes from all deployed XR headsets.
% To make an seamless user experience for robot learning data collection,
% IRIS also tested in three different robot control interface
% Furthermore, to demonstrate the extensibility of our approach, we have implemented a robot-world pipeline for real robot data collection, ensuring that the system can be used in both simulated and real-world environments.
The Immersive Robot Interaction System makes the following contributions\\
\textbf{(1) A unified scene specification} that is compatible with multiple robot simulators. It enables various XR headsets to visualize and interact with simulated objects and robots, providing an immersive experience while ensuring straightforward reusability and reproducibility.\\
\textbf{(2) A collaborative data collection framework} designed for XR environments. The framework facilitates enhanced robot data acquisition.\\
\textbf{(3) A user study} demonstrating that IRIS significantly improves data collection efficiency and intuitiveness compared to the LIBERO baseline.

\input{table/vr_work_comparison_table}