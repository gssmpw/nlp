\section{Related work}
\label{section:related}
Gradient-based optimization with large stepsize has attracted significant attention recently.
Specifically, non-monotonic behaviors of loss functions____ and the sharpness adaptivity to loss landscapes____ have been observed empirically.
____ argued that the sharpness tends to initially increases until the classical stable regime breaks down, and hovers on this boundary, termed as the edge of stability.
This observation mainly sparks two questions: why the loss landscape hovers on EoS, and why converging.
Answering either question must go beyond the classical optimization theory under the stable regime.

On why the loss landscape hovers on EoS, let us make a brief review, though it is not a central focus of this paper: ____ is a seminal work to empirically investigate the homogeneity of loss functions contributes to maintain EoS.
Later, ____ showed that normalized GD (represented by scale-invariant losses) adaptively leads their intrinsic stepsize toward sharpness reduction.
____ and ____ attribute the sharpness fluctuation to the non-negligible third-order Taylor remainder of the loss landscape.

We rather focus on why GD converges with arbitrary stepsize.
In this line, previous studies show convergence based on specific models such as multi-scale loss function____, quadratic functions____, matrix factorization____, a scalar multiplicative model____, a sparse coding model____, and linear logistic regression____.
Among them, we advocate the logistic regression setup proposed by ____ because it is relevant to implicit bias of GD____, and moreover, the follow-up work by ____ corroborates the benefit of large stepsize in GD convergence rate.
Our work is provoked by ____, questioning what structure in a loss function yields GD convergence.
Indeed, we do observe in \cref{figure:pilot} that loss functions without the self-bounding property~\eqref{equation:self_bounding_property} can make GD converge, though the self-bounding property seems essential to calm EoS down to the stable phase____ as well as to establish the max-margin directional convergence____. 
A similar question to ours is raised by ____, who argues that the stable convergence of large-stepsize logistic regression might be an artifact due to the functional form of the logistic loss---%
eventually ____ asserts that large-stepsize logistic regression behaves like the classical perceptron.
To this end, we show in \cref{theorem:gd} that arbitrary-stepsize GD can converge under a wide range of losses even without the self-bounding property \eqref{equation:self_bounding_property},
and moreover, occasionally yielding a better rate than the classical stable convergence rate.
We discuss it more in \cref{section:discussion}.
Lastly, we note that ____ attempts to extend the separable logistic regression setup to the non-separable one; yet, we still do not have satisfactory results beyond the one-dimensional case.
Due to its intricateness, we follow the linearly separable case for now.