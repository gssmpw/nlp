\section{Related work}
\label{section:related}
Gradient-based optimization with large stepsize has attracted significant attention recently.
Specifically, non-monotonic behaviors of loss functions~\citep{Xing2018} and the sharpness adaptivity to loss landscapes~\citep{Lewkowycz2020,Cohen2021ICLR} have been observed empirically.
\citet{Cohen2021ICLR} argued that the sharpness tends to initially increases until the classical stable regime breaks down, and hovers on this boundary, termed as the edge of stability.
This observation mainly sparks two questions: why the loss landscape hovers on EoS, and why converging.
Answering either question must go beyond the classical optimization theory under the stable regime.

On why the loss landscape hovers on EoS, let us make a brief review, though it is not a central focus of this paper: \citet{Ahn2022ICML} is a seminal work to empirically investigate the homogeneity of loss functions contributes to maintain EoS.
Later, \citet{Lyu2022NeurIPS} showed that normalized GD (represented by scale-invariant losses) adaptively leads their intrinsic stepsize toward sharpness reduction.
\citet{Ma2022} and \citet{Damian2023ICLR} attribute the sharpness fluctuation to the non-negligible third-order Taylor remainder of the loss landscape.

We rather focus on why GD converges with arbitrary stepsize.
In this line, previous studies show convergence based on specific models such as multi-scale loss function~\citep{Kong2020NeurIPS}, quadratic functions~\citep{Arora2022ICML}, matrix factorization~\citep{Wang2022ICLR,Chen2023ICML}, a scalar multiplicative model~\citep{Zhu2023ICLR,Kreisler2023ICML}, a sparse coding model~\citep{Ahn2023NeurIPS}, and linear logistic regression~\citep{Wu2023NeurIPS}.
Among them, we advocate the logistic regression setup proposed by \citet{Wu2023NeurIPS} because it is relevant to implicit bias of GD~\citep{Soudry2018,Ji2019COLT,Ravi2024NeurIPS}, and moreover, the follow-up work by \citet{Wu2024COLT} corroborates the benefit of large stepsize in GD convergence rate.
Our work is provoked by \citet{Wu2024COLT}, questioning what structure in a loss function yields GD convergence.
Indeed, we do observe in \cref{figure:pilot} that loss functions without the self-bounding property~\eqref{equation:self_bounding_property} can make GD converge, though the self-bounding property seems essential to calm EoS down to the stable phase~\citep{Wu2024COLT} as well as to establish the max-margin directional convergence~\citep{Ji2019COLT,Ravi2024NeurIPS}. 
A similar question to ours is raised by \citet{Tyurin2024}, who argues that the stable convergence of large-stepsize logistic regression might be an artifact due to the functional form of the logistic loss---%
eventually \citet{Tyurin2024} asserts that large-stepsize logistic regression behaves like the classical perceptron.
To this end, we show in \cref{theorem:gd} that arbitrary-stepsize GD can converge under a wide range of losses even without the self-bounding property \eqref{equation:self_bounding_property},
and moreover, occasionally yielding a better rate than the classical stable convergence rate.
We discuss it more in \cref{section:discussion}.
Lastly, we note that \citet{Meng2024} attempts to extend the separable logistic regression setup to the non-separable one; yet, we still do not have satisfactory results beyond the one-dimensional case.
Due to its intricateness, we follow the linearly separable case for now.