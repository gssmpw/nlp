@incollection{beukeboom2017linguistic,
  title={Linguistic bias},
  author={Beukeboom, Camiel J and Burgers, CF},
  booktitle={Oxford Encyclopedia of Communication},
  pages={1--19},
  year={2017},
  publisher={Oxford University Press}
}
@article{fortuna2018survey,
  title={A survey on automatic detection of hate speech in text},
  author={Fortuna, Paula and Nunes, S{\'e}rgio},
  journal={ACM Computing Surveys (CSUR)},
  volume={51},
  number={4},
  pages={1--30},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@misc{EU_Trustworthy_AI,
  title={Ethics Guidelines for Trustworthy AI},
  author={{European Commission}},
  year={2019},
  url={https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai},
  note={Accessed: [Insert date of access here]}
}.

@inproceedings{dixon2018measuring,
  title={Measuring and mitigating unintended bias in text classification},
  author={Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={67--73},
  year={2018}
}
@incollection{maass1999linguistic,
  title={Linguistic intergroup bias: Stereotype perpetuation through language},
  author={Maass, Anne},
  booktitle={Advances in experimental social psychology},
  volume={31},
  pages={79--121},
  year={1999},
  publisher={Elsevier}
}
@article{nosek2002math,
  title={Math= male, me= female, therefore math$\ne$ me.},
  author={Nosek, Brian A and Banaji, Mahzarin R and Greenwald, Anthony G},
  journal={Journal of personality and social psychology},
  volume={83},
  number={1},
  pages={44},
  year={2002},
  publisher={American Psychological Association}
}
@article{gallegos2024bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  pages={1--79},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@article{garrido2021survey,
  title={A survey on bias in deep {NLP}},
  author={Garrido-Mu{\~n}oz, Ismael and Montejo-R{\'a}ez, Arturo and Mart{\'\i}nez-Santiago, Fernando and Ure{\~n}a-L{\'o}pez, L Alfonso},
  journal={Applied Sciences},
  volume={11},
  number={7},
  pages={3184},
  year={2021},
  publisher={MDPI}
}
@inproceedings{blodgett2020language,
  title={Language (Technology) is Power: A Critical Survey of “Bias” in {NLP}},
  author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5454--5476},
  year={2020}
}
@book{modest2018woorden,
  title={{Woorden Doen Ertoe: een Complete Gids voor Woordkeuze Binnen de Culturele Sector}},
  author={Modest, Wayne and Schoonderwoerd, Stijn},
  year={2018},
  publisher={Tropenmuseum, Afrikamuseum, Museum Volkenkunde}
}
@misc{samuel2022waarden,
  title={Waarden {voor een} Nieuwe Taal},
  year={2021},
  author={Samuel, Mounir},
  howpublished  = {Code Diversiteit \& Inclusie},
  note          = {Available at \url{https://codedi.nl/wp-content/uploads/2021/03/WAARDEN_VOOR_TAAL_DIGI_DEF.pdf}},
}
@inproceedings{nangia2020crows,
  title={{CrowS-Pairs}: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1953--1967},
  year={2020}
}
@inproceedings{caselli2021dalc,
  title={{DALC}: the Dutch abusive language corpus},
  author={Caselli, Tommaso and Schelhaas, Arjan and Weultjes, Marieke and Leistra, Folkert and Van Der Veen, Hylke and Timmerman, Gerben and Nissim, Malvina},
  booktitle={Proceedings of the 5th Workshop on Online Abuse and Harm},
  pages={54--66},
  year={2021},
  organization={Association for Computational Linguistics (ACL)}
}
@misc{devries2019bertje,
	title = {{BERTje}: {A} {Dutch} {BERT} {Model}},
	shorttitle = {{BERTje}},
	author = {de Vries, Wietse  and  van Cranenburgh, Andreas  and  Bisazza, Arianna  and  Caselli, Tommaso  and  Noord, Gertjan van  and  Nissim, Malvina},
	year = {2019},
	month = dec,
	howpublished = {arXiv:1912.09582},
	url = {http://arxiv.org/abs/1912.09582},
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@inproceedings{delobelle-etal-2020-robbert,
    title = "{R}ob{BERT}: a {D}utch {R}o{BERT}a-based {L}anguage {M}odel",
    author = "Delobelle, Pieter  and
      Winters, Thomas  and
      Berendt, Bettina",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.292",
    doi = "10.18653/v1/2020.findings-emnlp.292",
    pages = "3255--3265",
    abstract = "Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model{'}s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.",
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}
@inproceedings{caselli2021hatebert,
  title={HateBERT: Retraining BERT for Abusive Language Detection in English},
  author={Caselli, Tommaso and Basile, Valerio and Mitrovi{\'c}, Jelena and Granitzer, Michael},
  booktitle={Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)},
  pages={17--25},
  year={2021}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese computational linguistics: 18th China national conference, CCL 2019, Kunming, China, October 18--20, 2019, proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer}
}
@article{fleiss1973equivalence,
  title={The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability},
  author={Fleiss, Joseph L and Cohen, Jacob},
  journal={Educational and psychological measurement},
  volume={33},
  number={3},
  pages={613--619},
  year={1973},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}
@inproceedings{neveol2022french,
  title={French CrowS-pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English},
  author={N{\'e}v{\'e}ol, Aur{\'e}lie and Dupont, Yoann and Bezan{\c{c}}on, Julien and Fort, Kar{\"e}n},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8521--8531},
  year={2022}
}
@inproceedings{nadeem2021stereoset,
  title={StereoSet: Measuring stereotypical bias in pretrained language models},
  author={Nadeem, Moin and Bethke, Anna and Reddy, Siva},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5356--5371},
  year={2021}
}
@inproceedings{zakizadeh-etal-2023-difair,
    title = "{D}i{F}air: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias",
    author = "Zakizadeh, Mahdi  and
      Miandoab, Kaveh  and
      Pilehvar, Mohammad",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.127",
    doi = "10.18653/v1/2023.findings-emnlp.127",
    pages = "1897--1914",
    abstract = "Numerous debiasing techniques have been proposed to mitigate the gender bias that is prevalent in pretrained language models. These are often evaluated on datasets that check the extent to which the model is gender-neutral in its predictions. Importantly, this evaluation protocol overlooks the possible adverse impact of bias mitigation on useful gender knowledge. To fill this gap, we propose **DiFair**, a manually curated dataset based on masked language modeling objectives. **DiFair** allows us to introduce a unified metric, *gender invariance score*, that not only quantifies a model{'}s biased behavior, but also checks if useful gender knowledge is preserved. We use **DiFair** as a benchmark for a number of widely-used pretained language models and debiasing techniques. Experimental results corroborate previous findings on the existing gender biases, while also demonstrating that although debiasing techniques ameliorate the issue of gender bias, this improvement usually comes at the price of lowering useful gender knowledge of the model.",
}
@article{webster-etal-2018-mind,
    title = "Mind the {GAP}: A Balanced Corpus of Gendered Ambiguous Pronouns",
    author = "Webster, Kellie  and
      Recasens, Marta  and
      Axelrod, Vera  and
      Baldridge, Jason",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1042",
    doi = "10.1162/tacl_a_00240",
    pages = "605--617",
    abstract = "Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun{--}name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9{\%} F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.",
}
@article{hilte2023haters,
  title={Who are the haters? A corpus-based demographic analysis of authors of hate speech},
  author={Hilte, Lisa and Markov, Ilia and Ljube{\v{s}}i{\'c}, Nikola and Fi{\v{s}}er, Darja and Daelemans, Walter},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  pages={986890},
  year={2023},
  publisher={Frontiers Media SA}
}
@inproceedings{smith-etal-2022-im,
    title = "{``}{I}{'}m sorry to hear that{''}: Finding New Biases in Language Models with a Holistic Descriptor Dataset",
    author = "Smith, Eric Michael  and
      Hall, Melissa  and
      Kambadur, Melanie  and
      Presani, Eleonora  and
      Williams, Adina",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.625",
    doi = "10.18653/v1/2022.emnlp-main.625",
    pages = "9180--9211",
    abstract = "As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.",
}
@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@inproceedings{hengst2024conformal,
  title={Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition},
  author={den Hengst, Floris and Wolter, Ralf and Altmeyer, Patrick and Kaygan, Arda},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2024},
  pages={2412--2432},
  year={2024}
}
@inproceedings{angelopoulosconformal,
  title={Conformal Risk Control},
  author={Angelopoulos, Anastasios Nikolas and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@inproceedings{stankiewicz2021bias,
  title={Bias detection in Wikipedia articles. A study on Polish and English Datasets.},
  author={Stankiewicz, Weronika and Baraniak, Katarzyna and Sydow, Marcin},
  booktitle={IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
  pages={589--594},
  year={2021}
}
@article{macavaney2019hate,
  title={Hate speech detection: Challenges and solutions},
  author={MacAvaney, Sean and Yao, Hao-Ren and Yang, Eugene and Russell, Katina and Goharian, Nazli and Frieder, Ophir},
  journal={PloS one},
  volume={14},
  number={8},
  pages={e0221152},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}
@inproceedings{recasens-etal-2013-linguistic,
    title = "Linguistic Models for Analyzing and Detecting Biased Language",
    author = "Recasens, Marta  and
      Danescu-Niculescu-Mizil, Cristian  and
      Jurafsky, Dan",
    editor = "Schuetze, Hinrich  and
      Fung, Pascale  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1162",
    pages = "1650--1659",
}
@article{spinde2021automated,
  title={Automated identification of bias inducing words in news articles using linguistic and context-oriented features},
  author={Spinde, Timo and Rudnitckaia, Lada and Mitrovi{\'c}, Jelena and Hamborg, Felix and Granitzer, Michael and Gipp, Bela and Donnay, Karsten},
  journal={Information Processing \& Management},
  volume={58},
  number={3},
  pages={102505},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{hube2018detecting,
  title={Detecting biased statements in wikipedia},
  author={Hube, Christoph and Fetahu, Besnik},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1779--1786},
  year={2018}
}
@inproceedings{kuang2016semantic,
  title={Semantic and context-aware linguistic model for bias detection},
  author={Kuang, Sicong and Davison, Brian D},
  booktitle={Proc. of the Natural Language Processing meets Journalism IJCAI-16 Workshop},
  pages={57--62},
  year={2016}
}
@inproceedings{zhang-etal-2020-demographics,
    title = "Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting",
    author = "Zhang, Guanhua  and
      Bai, Bing  and
      Zhang, Junqi  and
      Bai, Kun  and
      Zhu, Conghui  and
      Zhao, Tiejun",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.380",
    doi = "10.18653/v1/2020.acl-main.380",
    pages = "4134--4145",
    abstract = "With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., {``}gay{''}, {``}black{''}) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like {``}She makes me happy to be gay{''} as abusive simply because of the word {``}gay.{''} In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models{'} generalization ability.",
}
@inproceedings{bartl-etal-2020-unmasking,
    title = "Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias",
    author = "Bartl, Marion  and
      Nissim, Malvina  and
      Gatt, Albert",
    editor = "Costa-juss{\`a}, Marta R.  and
      Hardmeier, Christian  and
      Radford, Will  and
      Webster, Kellie",
    booktitle = "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.gebnlp-1.1",
    pages = "1--16",
    abstract = "Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.",
}