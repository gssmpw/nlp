@inproceedings{bartl-etal-2020-unmasking,
    title = "Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias",
    author = "Bartl, Marion  and
      Nissim, Malvina  and
      Gatt, Albert",
    editor = "Costa-juss{\`a}, Marta R.  and
      Hardmeier, Christian  and
      Radford, Will  and
      Webster, Kellie",
    booktitle = "Proceedings of the Second Workshop on Gender Bias in Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.gebnlp-1.1",
    pages = "1--16",
    abstract = "Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models.",
}

@inproceedings{hube2018detecting,
  title={Detecting biased statements in wikipedia},
  author={Hube, Christoph and Fetahu, Besnik},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1779--1786},
  year={2018}
}

@inproceedings{kuang2016semantic,
  title={Semantic and context-aware linguistic model for bias detection},
  author={Kuang, Sicong and Davison, Brian D},
  booktitle={Proc. of the Natural Language Processing meets Journalism IJCAI-16 Workshop},
  pages={57--62},
  year={2016}
}

@inproceedings{recasens-etal-2013-linguistic,
    title = "Linguistic Models for Analyzing and Detecting Biased Language",
    author = "Recasens, Marta  and
      Danescu-Niculescu-Mizil, Cristian  and
      Jurafsky, Dan",
    editor = "Schuetze, Hinrich  and
      Fung, Pascale  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1162",
    pages = "1650--1659",
}

@article{spinde2021automated,
  title={Automated identification of bias inducing words in news articles using linguistic and context-oriented features},
  author={Spinde, Timo and Rudnitckaia, Lada and Mitrovi{\'c}, Jelena and Hamborg, Felix and Granitzer, Michael and Gipp, Bela and Donnay, Karsten},
  journal={Information Processing \& Management},
  volume={58},
  number={3},
  pages={102505},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{zhang-etal-2020-demographics,
    title = "Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with Instance Weighting",
    author = "Zhang, Guanhua  and
      Bai, Bing  and
      Zhang, Junqi  and
      Bai, Kun  and
      Zhu, Conghui  and
      Zhao, Tiejun",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.380",
    doi = "10.18653/v1/2020.acl-main.380",
    pages = "4134--4145",
    abstract = "With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets. For example, texts containing some demographic identity-terms (e.g., {``}gay{''}, {``}black{''}) are more likely to be abusive in existing abusive language detection datasets. As a result, models trained with these datasets may consider sentences like {``}She makes me happy to be gay{''} as abusive simply because of the word {``}gay.{''} In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution. Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms. Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models{'} generalization ability.",
}

