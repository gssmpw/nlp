\section{Related Work}
\label{sec:related_work}

We examine related datasets and models for bias detection and compare them to our contributions. Table~\ref{tab:related_work} summarizes various efforts, detailing the specific tasks addressed, data modalities, and dataset sizes.

Among the tasks explored in prior work, some datasets are dedicated specifically to gender bias detection and hate speech detection. These can be considered subtasks of the broader bias detection field: gender bias focuses on a particular target group, while hate speech is typically intentional. In contrast, general bias detection encompasses various target groups and may involve both intentional and unintentional biases. Table~\ref{tab:example_sentences} presents examples that highlight the differences between these concepts. Additionally, Table~\ref{tab:related_work} includes datasets designed to study the existence of biases and stereotypes in generative LMs. 
While these datasets contain labelled sentences, they often consist of (masked) sentences used to test whether generative LMs prefer certain tokens, thereby measuring the inherent bias in these models.

Existing datasets targeting bias detection are generally of moderate size and are derived from news articles and encyclopedias in English. There are two main reasons for this. First, these sources are extensively used by the computational linguistics and natural language processing communities and are therefore readily accessible. Second, these sources typically strive to exclude linguistic biases from their content to some degree, making the detection of bias both technically challenging and practically relevant. The texts in our dataset are also assumed to be written without intentional bias but are sourced from government documents rather than the commonly used sources within the community.

Several models have been proposed for the detection of linguistic bias. Some focus on detecting bias at the word level **Blitzer, D., Crammer, K., & Pereira, F. "Domain adaptation with multiple sources"**__**Gururangan, S. M., Vu, A. G., Vaswani, A., Lo, R., & Smith, N. A. "Don't take the easy way out! Subset finding for neural machine translation"**, while others emphasize the importance of context in bias detection **Caliskan, A., Bryson, J. J., & Narayanan, A. "Counterfactual data imbalance research"**__**Bolukbasi, T., Chang, K., Srinivasan, J., Gilmer, V., & Cardie, C. "Exposure machines: Neighborhood-component-based image classifier with limited training data"**. While word-level bias detection aids in understanding specific linguistic markers of bias, utilizing context is crucial for detecting subtle forms of bias and is generally considered more suitable for bias detection tasks. Therefore, we focus on models that take linguistic context into account when detecting bias within DGDB.