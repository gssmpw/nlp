% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
%
\usepackage{amssymb}
\usepackage{utfsym}
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
% \usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmic}%
% \usepackage{algorithmicx}%
% \usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage[utf8]{inputenc}
\usepackage{dutchcal}
\usepackage{tabularx} 
% \usepackage[table]{xcolor}
\usepackage{array}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage[misc]{ifsym}
\pagenumbering{gobble}
\graphicspath{ {image/} }
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}
\renewcommand{\algorithmicensure}{ \textbf{Output:}}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{}
% \institute{}
\author{Xuqi Mao\inst{1,3} \and
Zhenying He\inst{1,3} \and
X. Sean Wang\inst{1,2,3}\textsuperscript{(\Letter)}}
% %
% \authorrunning{F. Author et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
\institute{School of Computer Science, Fudan University, Shanghai, China \and
School of Software, Fudan University, Shanghai, China \and
Shanghai Key Laboratory of Data Science, Shanghai, China\\
\email{\{xqmao17,zhenying,xywangCS\}@fudan.edu.cn}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Graph neural networks (GNNs) have emerged as the state of the art for a variety of graph-related tasks and have been widely used in Heterogeneous Graphs (HetGs), where meta-paths help encode specific semantics between various node types. Despite the revolutionary representation capabilities of existing heterogeneous GNNs (HGNNs) due to their focus on improving the effectiveness of heterogeneity capturing, the huge training costs hinder their practical deployment in real-world scenarios that frequently require handling ad-hoc queries with user-defined meta-paths. To address this, we propose FHGE, a Fast Heterogeneous Graph Embedding designed for efficient, retraining-free generation of meta-path-guided graph embeddings. The key design of the proposed framework is two-fold: segmentation and reconstruction modules. It employs Meta-Path Units (MPUs) to segment the graph into local and global components, enabling swift integration of node embeddings from relevant MPUs during reconstruction and allowing quick adaptation to specific meta-paths. In addition, a dual attention mechanism is applied to enhance semantics capturing. Extensive experiments across diverse datasets demonstrate the effectiveness and efficiency of FHGE in generating meta-path-guided graph embeddings and downstream tasks, such as link prediction and node classification, highlighting its significant advantages for real-time graph analysis in ad-hoc queries.

\keywords{heterogeneous graph \and graph neural network \and ad-hoc meta-path.}
\end{abstract}
%
%
%
\section{Introduction}
Numerous real-world datasets can be modeled as Heterogeneous Graphs (HetGs), with diverse objects and the relations represented as various types of nodes and edges, respectively. Examples expand across various domains such as social networks \cite{gamba2024exit,muppasani2024expressive,avery2024effect}, citation networks \cite{jin2023heterformer,jia2023enhancing,mao2024hetfs}, traffic networks \cite{jin2023transferable}, recommendation systems \cite{agrawal2024no,liu2023generative,yang2024fine}, natural language processing \cite{Christmann23Explainable,MaYLMC24HetGPT,WU23Heterogeneous}, knowledge graphs \cite{zang2023commonsense,liu2023knowledge}, and biology information networks \cite{zhong2023knowledge}. Fig.~\ref{fig:example_of_hetg}(a) presents a movie graph example of a HetG, illustrating actors, movies, directors, and the interconnected relationships between them. The diversity of types and the richness of properties within HetGs give rise to a multitude of varied and intricate semantics and have led to a significant amount of related work, particularly in the domains of traditional graph representation learning and heterogeneous graph neural networks (HGNNs) \cite{Yu23KGTrust,Shi2022hgnn,yang23HGNAS}. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{example.eps}
\caption{Example of HetG. (a) depicts a movie network with heterogeneous nodes and relations, while (b) outlines its meta-paths. For instance, the path ``a1-m1-a2'' in (a) indicates that actor a1 and a2 co-starred in the movie m1, represented as the meta-path``AMA'' in (b).} \label{fig:example_of_hetg}
\end{figure}

The complex semantics of heterogeneous graphs often lead users to express preferences for specific relations. These preferences can be explicitly specified using meta-paths, i.e., sequences of node types and edge types defining composite relations between objects with examples in Fig.~\ref{fig:example_of_hetg}(b). Given their specificity, users often prefer node embeddings tailored to particular semantics. For example, consider ``Terminator 2'', a fictional action film directed by James Cameron and starring Arnold Schwarzenegger and Linda Hamilton. We analyzed the top 5 movies similar to it under different semantics. Table~\ref{tab:movies_under_meta-paths} showcases the results under distinct semantics: (a) movies with similar actors, using the meta-path ``MAM'', (b) movies with similar directors, using the meta-path ``MDM'', and (c) movies sharing all available similar information under ``meta-path-free'' semantics. The diverse outcomes for each scenario underscore the need to process ad-hoc queries with user-defined meta-paths.
 % These ad-hoc meta-paths, tailored to specific querying needs, are not predefined but are instead designed to capture user search intentions more precisely, thereby meeting user expectations more effectively.
% A special case is what is called a meta-path-free scenario, where all available information between the two movies is taken into account without restricting to specific meta-paths. The last column of Table~\ref{tab:movies_under_meta-paths} gives the movies that are most similar to “Roman Holiday” under meta-path-free semantics. 

\begin{table}[htbp]
\centering
% \small
% \tabcolsep=0.08cm
% \resizebox{0.6\linewidth}{!}{%
\caption{Top 5 similar movies for ``Terminator 2'' under three different scenarios using FHGE.}\label{tab:movies_under_meta-paths}
\begin{tabular}{clll}
\toprule
Rank  & meta-path: MAM   & meta-path: MDM   & meta-path-free \\
\midrule
1     & Terminator & True Lies & Terminator \\
2     & True Lies & Aliens & Terminator 3 \\
3     & Total Recall & The Abyss & True Lies \\
4     & End of Days & Titanic & The Abyss \\
5     & Jingle All the Way & Ghosts of the Abyss & Titanic \\
\bottomrule
\end{tabular}%
% }
\end{table}

% The superior performance of HINs in characterizing complex information has spurred a surge of research dedicated to developing specialized graph mining techniques. 

% They either capture semantics based on pre-defined meta-paths or overlook different semantics to generate meta-path-free graph embedding.

Over the past decade, significant progress has been made in mining information from graphs from traditional representation learning approaches \cite{perozzi2014deepwalk,grover2016node2vec,dong2017metapath2vec} to methods utilizing deep neural networks, including GNNs \cite{fan2019metapath,yan2021relation,zhang23pagelink,zhu23AutoAC,SHAN24KPI-HGNN,MaYLMC24HetGPT} and GCNs \cite{kipf2016semi,liu2023rhgn}. Inspired by the Transformer \cite{vaswani2017attention}, GAT \cite{velickovic2017graph} integrates the attention to aggregate node-level information in homogeneous networks, while HAN \cite{wang2019heterogeneous} introduces a two-level attention mechanism for node and semantic information in heterogeneous networks. MAGNN \cite{fu2020magnn}, MHGNN \cite{liang2022meta} and R-HGNN \cite{yu23RHGNN} proposed meta-path-based models to learn meta-path-based node embeddings. HetGNN \cite{zhang2019heterogeneous} and MEGNN \cite{chang2022megnn} take a meta-path-free approach to consider both structural and content information for each node jointly. HGT \cite{hu2020heterogeneous} incorporates information from high-order neighbors of different types through messages passing across ``soft'' meta-paths. MHGCN \cite{fu2023multiplex} captures local and global information by modeling the multiplex structures with depth and breadth behavior pattern aggregation. SeHGNN \cite{Yang23Simple} simplifies structural information capture by precomputing neighbor aggregation and incorporating a transformer-based semantic fusion module. HAGNN \cite{zhu2023hagnn} integrates meta-path-based intra-type aggregation and meta-path-free inter-type aggregation to generate the final embeddings.

% In this paper, we explore the task of generating graph embeddings for ad-hoc meta-paths, referred to as meta-path-guided heterogeneous graph embedding. Significant progress has been made in representing HetGs effectively, notably in capturing graph data, differentiating different semantics, and generating precise node representations. 
% FHGE fills this gap by enabling user-defined ad-hoc meta-paths to guide the embedding process in heterogeneous graphs.

% While existing methods have partially addressed heterogeneous graph embedding, none efficiently support embeddings based on user-specified ad-hoc meta-paths. 

While existing methods have shown effectiveness in predefined meta-path and meta-path-free scenarios, they encounter significant challenges when generating meta-path-guided heterogeneous graph embeddings: (a) Meta-path-based HGNNs \cite{fu2020magnn,wang2019heterogeneous} generate accurate graph embeddings only when pre-defined meta-paths align the user-given meta-paths. Otherwise, the resulting embeddings will be suboptimal, and retraining the embeddings can be both time-consuming and practically challenging. (b) Meta-path-free HGNNs \cite{zhang2019heterogeneous,hu2020heterogeneous,li2023metapath,schlichtkrull2018modeling} either treat all meta-paths uniformly or assign weights to them using mechanisms like attention. However, these approaches may incorporate meta-paths irrelevant to the ad-hoc query, potentially leading to embeddings that do not align with the user-defined meta-paths. Fig.~\ref{fig:metric_time_memory} compares the time and memory overhead of various methods. This underscores the need for a method capable of meta-path-guided heterogeneous graph embedding.
% However, existing methods encounter limitations when generating graph embedding for ad-hoc meta-paths. In this paper, we address this problem, which we refer to as the task of meta-path-guided heterogeneous graph embedding. In the following, we elaborate on the inherent limitations of applying existing methods to the task of meta-path-guided heterogeneous graph embedding:
 
\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{image/metric_time_memory.eps}
\caption{Comparison of time and memory consumption for HGNNs on ``LastFM'' and ``DBLP''. The area of the circles indicates the memory consumption of each model, while the time costs are recorded on a log scale.} \label{fig:metric_time_memory}
\end{figure}

To fill the gap outlined above, this paper proposed FHGE, a Fast Heterogeneous Graph Embedding designed for ad-hoc meta-paths. FHGE consists of two core modules: segmentation and reconstruction modules. It starts by segmenting graph information into local and global components using a Meta-Path Unit (MPU), then employs a random walk with a restart within the MPU to capture essential local structures. During reconstruction, FHGE quickly integrates node embeddings from relevant MPUs by reutilizing local embeddings, enabling swift adaptation to ad-hoc meta-paths. Specifically, it applies type-specific linear transformations to project heterogeneous content of nodes, potentially of unequal dimensions for different feature types, into the same latent vector space. A graph neural network architecture is then used to aggregate heterogeneous node information from both homogeneous and mixed-type nodes within the MPU. To enhance information capturing, a dual attention mechanism, one for intra-MPU and another for inter-MPU, is employed to mine local and global semantics. The aggregated features are further combined with attention based on ad-hoc meta-paths following two strategies, enabling FHGE to generate meta-path-guided graph embeddings that effectively leverage the structural, node, and semantic information.

To summarize, this work makes the following main contributions:
\begin{itemize}
\item To our best knowledge, FHGE is the first meta-path-guided heterogeneous network for fast heterogeneous graph embedding under ad-hoc meta-paths.
\item It introduces the MPU to separate graph information into local and global components. It allows the preservation of partial semantics on the heterogeneous graph, enabling its repetitive utilization.
\item A dual attention mechanism is applied to enhance graph information reconstruction, and two kinds of integration strategies are employed across various MPUs based on ad-hoc meta-paths.
\item Extensive experiments are conducted to demonstrate the accuracy and \\ prompt responsiveness of meta-path-guided graph embedding of FHGE.
\end{itemize}



\section{Preliminary}\label{sec:pre}

In this section, we give an overview of heterogeneous graphs. 

% In this section, we give an overview of graph neural networks and heterogeneous graphs. 

% \subsection{Graph Neural Networks}

% Table 2 presents and elucidates the notations employed in this manuscript.

% \begin{table}[h]
% \centering
% \caption{\centering Notation Table.}\label{notationTable}
% % \scriptsize
% \begin{tabular}{cm{0.35\textwidth}}
% \toprule
% % \makebox[2cm][c]{Notation} & {Description} \\
% Notation & \makecell[c]{Description}\\
% \hline
% $G$ & the data graph\\
% $a$, $b$ & the node in data graph\\
% $e$ & the edge in data graph\\
% $V$, $E$ & the node set, the edge set of the data graph\\
% $A$, $R$ & the node type, the edge type of the data graph\\
% $\mathcal{A}$, $\mathcal{R}$ & the node type set, the edge type set of the data graph\\
% $\theta$, $\kappa$ & the type mapping function of the node and edge\\
% $\psi, \phi$ & a meta-path unit and meta-path\\
% $l$ & the length of meta-path\\
% $h, h^{\prime}$ & the initial node feature, the projected node feature \\
% $M$ & the MPU-specific transformation matrix \\
% $a, q$ & node-level attention vector for MPU, MPU-level attention vector \\
% $e(a), w(a)$ & the importance of MPU-specific node intra-MPU, the importance of MPU to MPU-specific node $a$  intra-MPU\\
% $\alpha, \beta$ & weight of MPU-specific node intra-MPU, weight of MPU to MPU-specific node $a$ intra-MPU \\
% $N$ & MPU-based neighbors \\
% $Zi(a)$ & embedding of MPU-specific node $a$ intra-MPU \\
% $Zw(a)$ & embedding of MPU-specific node $a$ inter-MPU \\
% $Z^{\phi}(a)$ & final node embedding of node $a$ for meta-path $\phi$\\
% \bottomrule
% \end{tabular}
% \end{table}

% \subsection{Heterogeneous Graphs}

\noindent\textbf{Definition 1} Heterogeneous Graph. A heterogeneous graph is a graph $G(V, E)$, where each node $v \in V$ is mapped to a specific node type $\theta(v) \in \mathcal{A}$ by $\theta: V \rightarrow \mathcal{A}$ and each edge $e \in E$ is mapped to a specific relation type $\kappa(e) \in \mathcal{R}$ by $\kappa: E \rightarrow \mathcal{R}$. In a heterogeneous graph, the number of distinct node types $|\mathcal{A}|$ is greater than 1, or the number of distinct relation types $|\mathcal{R}|$  is greater than 1. 
	
\noindent\textbf{Definition 2} Meta-path. A meta-path $\phi$ is a type sequence of nodes and edges defining a composite relation. It is typically represented by the notation of $A_1 \stackrel {R_1} {\longrightarrow} A_2 \stackrel {R_2} { \longrightarrow} \cdots \stackrel {R_l} {\longrightarrow} A_{l+1}$, where $A_1$ denotes the starting node type, $A_{l+1}$ denotes the ending node type, and each intermediate relation $R_i$ denotes a connection between node type $A_i$ and $A_{i+1}$. 
% The length $l$ of a meta-path $\phi$ corresponds to the edge count along it. To simplify notation, we can use node types along the meta-path to specify a meta-path if there is no more than one edge type connecting the same pair of node types. For example, the co-author relation in ``DBLP'', denoted by a length-2 meta path $A\stackrel{writing}{\longrightarrow}P\stackrel{written-by}{\longrightarrow}A$, can be represented in a shorter form $APA$ with no ambiguity.

\section{Methodology}

% Existing methods either pre-define specific meta-paths and compute their weights to leverage semantic information or weigh all information in the graph to utilize semantics. These methods prove inadequate in meta-path-guided graph embedding due to their cumbersome and impractical pre-defining processes. 
In this section, we describe FHGE, which is designed for meta-path-guided graph embedding. FHGE is composed of two modules: segmentation and reconstruction module. Fig.~\ref{fig:architecture} illustrates the architecture of FHGE.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{architecture.eps}
\caption{(a) The overall architecture of FHGE: it first segments graph information into local and global components, samples and groups heterogeneous neighbors to balance the graph, and then reutilizes local information during the reconstruction process to facilitate meta-path-guided node embedding; (b) sampling and grouping process; (c) dual attention mechanism.} \label{fig:architecture}
\end{figure*}
% (1) It segments graph information into local and global components, then samples and groups heterogeneous neighbors within MPUs to balance the graph. (2) Local information is reutilized during the reconstruction process to facilitate the meta-path-guided node embedding. (3) A dual attention mechanism is employed to enhance semantic information aggregating.
% three key components: structure information encoding, node information encoding, and semantics information encoding. 

\subsection{Segmentation Module}

\subsubsection{Structure Information Extaction}

To efficiently generate meta-path-guided node embeddings, FHGE introduces a Meta-Path Unit (MPU) to separate graph information into local and global components. By preserving partial information across the entire heterogeneous graph, the MPU ensures repetitive utilization of node embedding within it, thereby enhancing the overall efficiency of the model. Next, we give the definition of MPU.

\noindent\textbf{Definition 3} Meta-path unit (MPU). An MPU $\psi$ is defined as the minimal segmentable unit of meta-path $\phi$, with a length of 1, represented as $\psi = (A_1A_2)$, $A_1$, $A_2 \in \mathcal{A}$ in the HetG. It serves as the smallest unit to compose a meta-path, capturing the fundamental relationships between two node types connected by an edge.

% Correspondingly, we give the definition of meta-graph unit:

% \noindent\textbf{Definition 4} Meta-graph unit (MGU). An MGU $G_u^\psi$ is a projection of $G$ following MPU $\psi$, represented as $ (\{V_1, V_2\}, E_1), V_1, V_2 \in V, E_1 \in E$. In this projection, only one kind of edge exists, meaning each node in $V_1$ has only one-hop ending neighbors in $V_2$.

Before discussing the heterogeneous aggregation process, we first address the issue of unequal data distribution, which can negatively impact the embedding of ``hub'' nodes and inadequately represent ``cold-start'' nodes. To mitigate this, FHGE begins with MPU-based sampling using random walk with restart (RWR) in two steps: (a) Sampling Fixed-Length RWR. Beginning with a node $a \in V^\psi$, the random walker either moves to an adjacent node or returns to the starting node with probability $p$ until a fixed number of nodes for each type are collected. (b) Grouping Neighbors by Type. For each node type $A \in \mathcal{A}$, the top $k_A$ neighbors most frequently encountered during the RWR in MPU $\psi$ are selected as $A$-type neighbors, based on the intuition that frequently co-occurring nodes are more similar. This process enables FHGE to effectively capture and utilize the rich local information around each node, enhancing the quality of node embeddings. 

\subsection{Reconstruction Module}

\subsubsection{Node Information Transformation}
% , even when their dimensions are the same. For instance, while $n_i$-dimension intensity histogram vectors of images and $n_j$-dimensional bag-of-words vectors of texts might share the same dimensionality but occupy separate feature spaces, complicating direct integration.
In heterogeneous graphs with node attributes, feature vectors for different content types of a node $a$ often have unequal dimensions and belong to distinct feature spaces. As a result, managing feature vectors of diverse dimensions within a unified framework presents significant challenges. To address this, a two-step transformation is used to capture node information.

\paragraph{Type-Specific Transformation} 

For each content type of a node, features are projected into a unified feature space via a type-specific neural network $f$. For the $n$-th content of node $a$, we have:
\begin{equation}
H_{an}=f_n\big(C_{an}\big)
\end{equation}
where $C_{an}$ is the original feature vector of $n$-th content for node $a$ and $H_{an}$ is the projected latent vector, $f_n$ is the type-specific transformation function, which can be pre-trained using different techniques for various content types. The node information of node $a$ can then be represented as:
\begin{equation}
H_a=\left\{H_{an}, n \in |C_a|\right\}
\end{equation}
where $H_a$ is the collection of latent feature vectors of node $a$, and $|C_a|$ denotes the amount of content feature of node $a$. 

% , streamlining the subsequent aggregation process across nodes
% , which is adept at maintaining long-term dependencies and capturing information over extended distances

\paragraph{Node Information Aggregation} 

After transforming the content for each node, all node features are standardized to the same dimension. We utilize Bidirectional LSTM, as described by \cite{hamilton2017inductive}, to aggregate the diverse set of unordered features of the node. The feature embedding process is as follows:
\begin{equation}
\hat{H}_a = \frac{\sum_{n\in H_a} \left[\overrightarrow{LSTM}\left (H_{an}\right) \bigoplus \overleftarrow{LSTM}\left (H_{an}\right)\right]}{|H_a|}
\end{equation}
where $\bigoplus$ denotes concatenation. This method enables the integration of features from neighbors of the same type, ensuring comprehensive representation in the embeddings. This operation aligns the projected features of all nodes to a uniform dimension, facilitating the subsequent processing of nodes of arbitrary types. 
% The LSTM is formulated as:
% \begin{equation}
% \begin{aligned}
% &i_n = \sigma \left(\mathcal{W}_i \odot [H_{n}(a), h_{n-1}] + b_i\right) \\
% &f_n = \sigma \left(\mathcal{W}_f \odot [H_{n}(a)), h_{n-1}] + b_f\right) \\
% &\hat{c}_n = \tanh\left(\mathcal{W}_c \odot [H_{n}(a), h_{n-1}] + b_c\right) \\
% &c_n = f_n \ast c_{n-1} + i_n \ast \hat{c_n} \\
% &o_n = \sigma \left(\mathcal{W}_o \odot [H_{n}(a)), h_{n-1}] + o_t\right) \\
% &h_n = o_n \ast \tanh (c_n) 
% \end{aligned}
% \end{equation}
% where $h_n$ represents the output hidden state of $n$-th feature, $\ast$ denotes Hadamard product, $W$ and $b$ are learned weights and biases for different gates, $i_n$, $f_n$, and $o_n$ correspond to input gate vector, forget gate vector, and output gate vector, respectively. 

% Following this step, diverse node features of the same type within the MPU are aggregated into a consistent vector space.
% , such as Par2Vec \cite{le2014distributed} for text and CNNs \cite{long2015fully} for images. 


% Next, we aggregated neighbors to capture ``deep'' feature interactions and encoded them with a projected latent vector, creating a fixed-size embedding to enhance expressive capabilities:

\subsubsection{Semantics Information Aggregation}

To efficiently aggregate heterogeneous neighbor embeddings for each node, we developed an MPU-based aggregation network with two main components: (1) intra-MPU aggregation and (2) inter-MPU aggregation.
% The intra-MPU aggregation can be performed offline and re-post-training, while the inter-MPU aggregation enables the online integration of node embeddings based on user-specified meta-paths, allowing for real-time handling of ad-hoc meta-paths.

\paragraph{Intra-MPU Aggregation}

Given an MPU $\psi$, the intra-MPU aggregation captures local information inherent in the target node, its MPU-based neighbors, and the contextual relationships in between. The aggregation of fixed-size neighbor sets corresponding to each node type for each node $a$ is achieved using LSTM, formulated as follows:
\begin{equation}
Q_a = \frac{\sum_{b \in N_a} \left[\overrightarrow{LSTM}\left(\hat{H}_b\right) \bigoplus \overleftarrow{LSTM}\left(\hat{H}_b\right)\right]}{|N_a|}
\end{equation}
where $N_a$ denotes the sampled neighbor set of $a$, $b$ is one of the neighbors of $a$ within MPU $\psi$. Subsequently, we apply a graph attention layer \cite{velickovic2017graph} to compute a weighted sum of the nodes in each MPU $\psi$ related to the target node $a$. The attention mechanism is structured as follows:
\begin{equation}
% \begin{split}
\begin{aligned}
% &e_{a,b} = LeakyReLU\left(u^T \cdot [\hat{Q}_a || \hat{Q}_b]\right) \\
% &\alpha_{a,b} = softmax\left(e_{a,b}\right) = \frac{exp\left(e_{a,b}\right)}{\sum_{s\in N_a}exp\left(e_{a,s}\right)} \\
&\alpha_{a,b} = \frac{exp{\left(LeakyReLU\left(u^T[{Q}_a \bigoplus {Q}_b]\right)\right)}}{\sum_{s\in N_a}exp{\left(LeakyReLU\left(u^T[{Q}_a \bigoplus {Q}_s]\right)\right)}}\\
&Zi_a = \sum\nolimits_{b \in N_a}\alpha_{a,b} \odot {Q}_b
\end{aligned}
% \end{split}
\end{equation}
where $\alpha_{a,b}$ is the normalized weight of node $b$ to node $a$, $u^T$ denotes the node-level attention vector, and $LeakyReLU$ refers to the leaky version of a Rectified Linear Unit, $Zi_a$ is the node embedding of $a$ within MPU $\psi$, $\odot$ denotes Hadamard product.

% The underlying concept is that different nodes in each MPU $\psi$ contribute to the representation of the target node in varying degrees. This is accomplished by learning a normalized importance weight $\alpha_{a,b}$ for each node $b \in V$ in the MPU $\psi$ with respect to the target node $a$, followed by a weighted sum within the MPU $\psi$

\paragraph{Inter-MPU Aggregation}
%  A Straightforward approach for semantic composition is to calculate the element-wise mean of all the node vectors. 
% \begin{equation}
% Zw_a = \frac{\sum\nolimits_{i\in |\psi|} Zi_a}{|\psi|}
% \end{equation}
% To enhance this process,
After aggregating the local information within each MPU, the next step is inter-MPU aggregation, which involves combining the semantic information across all MPUs. First, we employ an attention mechanism to assign varying weights $\beta^{\psi}$ to different MPUs:
\begin{equation}
\begin{aligned}
% &w_a = LeakyReLU\left(q^T \cdot Zi_a\right) \\
% &\beta_a = softmax\left(w_a\right) = \frac{exp\left(w_a\right)}{\sum\nolimits_{i\in|\psi|}exp\left(w_{ai}\right)} \\
&\beta^{\psi} = \frac{exp\left(LeakyReLU\left(q^T \odot Zi_a\right)\right)}{\sum\nolimits_{i\in|\psi|}exp\left(LeakyReLU\left(q^T \odot Zi_{ai}\right)\right)}\\
\end{aligned}
\end{equation}
where $\beta_{\psi}$ is the normalized weight of MPU $\psi$ over all MPUs, $q^T$ represents the parameterized attention vector. Clearly, a higher value of $\beta_a$ indicates a greater importance of MPU $\psi$. Subsequently, with node embeddings on the MPU, FHGE can generate graph embeddings for any user-defined meta-path. The integration of node embeddings can be divided into two scenarios: cascaded integration and cumulative integration, as illustrated in fig.~\ref{fig:integration}.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.65\textwidth]{image/integration.eps}
    \caption{Two types of embedding integration based on user-defined meta-paths.} \label{fig:integration}
\end{figure}

Cascaded Integration. In a movie graph context, for example, exploring actors starring in movies directed by the same director requires integrating ``AM'' and ``MD'' MPUs for ``deeper'' interactions. We implement cascaded integration to capture this specific semantics for meta-path $\phi$ = ($A_1A_2 \cdots A_{l\text{-}1} \\ A_lA_{{\text{-}}l+1} \cdots A_{\text{-}2}A_{\text{-}1}$):
\begin{equation}
% Z^{\phi}_a = \prod\limits_{k\in \{1, \cdots, l\text{-}1, \text{-}l+1, \cdots, \text{-}1\}}Zw^{k}_{a_k}
\begin{aligned}
&Zw_a^{\psi} = \left\|\beta^{\psi}\right\|_1 \odot Zi_a\\
&Z^{\phi}_a = Zw^{1}_{a} \odot Zw^{2}_{a} \cdots \odot Zw^{{l\text{-}1}}_{a} \\
\end{aligned}
% \alpha_a^{\psi_k} \cdot \beta_{\psi_k}
\end{equation}
where ${l\text{-}1}$ indicates the length of the meta-path $\phi$, representing the depth of interactions.

Cumulative integration. Here, we accumulate node embeddings from different meta-paths. For instance, one might be interested in actors who have collaborated as well as actors starring in movies directed by the same director, involving two meta-paths, namely ``AMA'' and ``AMDMA''. For such cases, we use cumulative integration to integrate ``broader'' semantics:
\begin{equation}
% Z^{\phi}_a = \sum\limits_{j\in|\phi|}\prod\limits_{k\in {1, \cdots, l-1, \text{-}l+1, \cdots, \text{-}1}}Zw^{jk}_{a_{jk}}
Z^{\phi}_a = \frac{\sum_{j\in|\phi|}Zw^{{j1}}_{a_{j1}} \odot Zw^{{j2}}_{a_{j2}} \cdots \odot Zw^{{jl}}_{a_{j{(l\text{-}1)}}}}{|\phi|}
\end{equation}
where $|\phi|$ is the number of meta-paths the user is interested in, and $a_{j{(l\text{-}1)}}$ represents the node at depth $l\text{-}1$ in the $j$-th meta-path. With these techniques, FHGE can generate aligned node embeddings for any user-defined meta-paths.
% \subsubsection{Composite Fusion}

% In complex real-world scenarios, user-specified meta-paths often involve a combination of the above two cases. We provide composite fusion under such circumstances.:
% \begin{equation}
% Z(a)^{\phi} = \sum\limits_{j\in|\psi|}\prod\limits_{k\in {1, \cdots, l-1, \text{-}l+1, \cdots, \text{-}1}}Zw(a_{jk})
% % \alpha_a^{\psi_k} \cdot \beta_{\psi_k}
% \end{equation}
% where $a_{jk}$ represents the node at depth $k$ in the $j$-th meta-path.

\subsection{Training}

We optimize the model weights by minimizing the cross-entropy loss function using negative sampling \cite{mikolov2013distributed}:
\begin{equation}
\mathcal{L} = \sum\limits_{\langle a, b, b^{\prime}\rangle \in \tau} \log \sigma \Big(Zi_a \cdot Zi_b\Big) - \log \sigma \Big(\text{-}Zi_a \cdot Zi_{b^{\prime}}\Big)
\end{equation}
where $\sigma(\cdot)$ represents the sigmoid function, $\tau$ denotes the set of triples $\langle a, b, b^{\prime}\rangle$ collected by walk sampling based on MPU $\psi$ in the heterogeneous graph. 
% We employ a sampling strategy inspired by \cite{zhang2019heterogeneous}, where for each node $a$, we collect node $b$ within a distance of $\tau$ to $a$, and for each $b$, we sample a negative node $b^{\prime}$ with the same node type of $b$.

% Subsequently, we update the model parameters using the Adam optimizer and iterate through the training process until reaching a specified number of iterations. The embeddings generated are then integrated based on specific meta-paths from the MPU. Leveraging the acquired model parameters, we can generate graph embedding for diverse graph mining tasks. 

\section{Experiments} \label{sec:experiment}

In this section, we conduct extensive experiments to study the following research questions: \textbf{RQ1}: How does FHGE compare to SOTA methods in terms of effectiveness and efficiency for meta-path-guided graph embedding? \textbf{RQ2}: How does FHGE perform relative to SOTA methods across various downstream graph mining tasks such as link prediction and node classification? \textbf{RQ3}: What impact do different components, such as global and local semantics information, have on the performance of FHGE? \textbf{RQ4}: How do different hyper-parameters affect the performance of FHGE?

\subsection{Experimental Setup}

We describe the details of datasets and methods, how to set the ground truth and parameters, and how to assess the effectiveness of each method in our experiments.

\subsubsection{Datasets}

We use seven real-world datasets, i.e., Academic, IMDB, Amazon, LastFM, PubMed, DBLP, and ACM, which are widely used for benchmarking \cite{lv2021we}. The main statistics of the seven datasets are summarized in Table 3.
% Each dataset is divided into training, validation, and testing sets with ratios of 81/9/10\%, respectively, after allocating 10\% of the training set for validation when needed.

\begin{table}[htbp]
\centering
% \tabcolsep=0.02cm
    % \small
\caption{\centering Statistics of datasets.}\label{tab:statisticOfDatasets}
    \begin{tabular}{cccccccc}
    \toprule
    & Academic & IMDB & Amazon & LastFM & PubMed & DBLP & ACM \\
    \midrule
    Node & 72258 & 28751 & 10099 & 20612 & 63109 & 26128 & 10942 \\
    Edge & 296367 & 47355 & 148659 & 141521 & 244986 & 239566 & 547872 \\
    \bottomrule
    \end{tabular}%
\end{table}

\subsubsection{Baselines}

We compare FHGE against the following baselines: (1) SeHGNN \cite{Yang23Simple}: an HGNN adopts a light-weight mean aggregator to reduce complexity by removing overused neighbor attention and avoiding repeated neighbor aggregation. (2) HGT \cite{hu2020heterogeneous}: an HGNN that extends the transformer architecture to the graph-structured data, utilizing the self-attention mechanism to capture heterogeneous graph characteristics. (3) MHGNN \cite{li2023metapath}: a meta-path-based HGNN capturing both structural and content-related information from HetGs. (4) HetGNN \cite{zhang2019heterogeneous}: an HGNN using attention mechanisms to aggregate both structural and content information from heterogeneous neighbors. (5) MAGNN \cite{fu2020magnn}: an HGNN capturing information at various levels by incorporating node attributes, intermediate semantic nodes, and multiple meta-paths. (6) HAN \cite{wang2019heterogeneous}: an HGNN firstly employing a two-layer attention mechanism to capture weights at both the node and semantic levels in HetGs. (7) RGCN \cite{schlichtkrull2018modeling}: an extension of GCN for relational (multiple edge types) graphs that combines graph convolutions tailored to specific edge types.

% \item MAGNN \cite{fu2020magnn} is a heterogeneous GNN designed to capture information from various levels by incorporating node attributes, intermediate semantic nodes, and multiple meta-paths. This enables MAGNN to leverage both network topology and content-related information for effective representation learning in HINs.
% \item HAN \cite{wang2019heterogeneous} is a heterogeneous model that proposes an attention mechanism to address the heterogeneity inherent in nodes and edges. Specifically, it employs two layers of attention structures to capture weights at both the node and semantic levels intricately.
% \item Node2Vec \cite{grover2016node2vec} is an enhanced homogeneous model derived from DeepWalk. While DeepWalk utilizes a depth-first traversal algorithm with the ability to revisit visited nodes to generate node sequences, Node2Vec adopts a novel random walk strategy. It combines breadth-first and depth-first traversals by introducing parameters to balance local and global similarity.
% \item Metapath2Vec \cite{dong2017metapath2vec} is a heterogeneous model that generates random walk sequences based on pre-defined meta-paths to construct heterogeneous neighbors for each node.

\subsubsection{Evaluation Metrics}

To assess the effectiveness of FHGE, we employ two widely-used evaluation metrics, Recall@K and NDCG@K, focusing on the recommendation results from meta-path-guided node embeddings with $K$ set to 20 by default. To evaluate embedding efficiency, we track the CPU time required. To gauge the performance in downstream applications, we conduct meta-path-free experiments. Specifically, we use ``AUC'' and ``MRR'' for link prediction tasks and ``Micro-F1'' and ``Macro-F'' for node classification tasks.
% For each randomly selected center node, We sample one positive node and one negative node to form a triple list. This triple list is then used as input to the loss function for cross-entropy calculation. We use logistic regression to estimate the link probability between two node embeddings, $Z_a^\phi$, and $Z_b^\phi$, as $p_{ab} = \sigma(Z_a^\phi \cdot Z_b^\phi)$ with the sigmoid activate function. The performance of the embedding models in link prediction is evaluated using AUC and MRR.

% Classification tasks are divided into single-label and multi-label types. For single-label classification, we use softmax activation combined with cross-entropy loss. And for multi-label classification, we apply sigmoid activation and binary cross-entropy loss. The embeddings of labeled nodes (authors in ``DBLP'', movies in ``IMDB'', and papers in ``ACM'') generated by each model are fed into a logistic regression classifier. 

\subsubsection{Implementation Details}

The whole method is implemented on PyTorch. We initialize the trainable parameters with Xavier initialization and optimize loos with Adam. As for hyper-parameters, we decide the important hyper-parameters by grid search and keep them the same in all datasets. For example, the size of embeddings is 128, the learning rate is 0.01. We configure the random walk parameters with a window size of 5, a walk length of 30, 10 walks per node, and 5 negative samples. Following the previous work, we also use early stopping to terminate training with a patient of 20 epochs, and the maximum training epochs is 200. For the benchmark methods, We utilize the code provided by a uniform benchmark \cite{lv2021we}. 

% For the proposed FHGE, we randomly initialize parameters and optimize the model with Adam. The semantic-level attention vector $q^T$ is set to a dimension of 128, and the learning rate is set to 0.01. We configure the random walk parameters with a window size of 5, a walk length of 30, 10 walks per node, and 5 negative samples.



% adopt the experimental parameters for HetGNN from the original paper: set the window size to 5, walk length to 30, walks per node to 10, and the number of negative samples to 5, the dropout rate to 0.5, the dimension of the attention vector set to 128, utilizing the Adam optimizer with a learning rate of 0.005, and a weight decay (L2 penalty) of 0.001, trained for 50 epochs.

% All experiments are performed on a Linux machine equipped with an Intel(R) Xeon(R) Silver 4208 CPU running at 2.1GHz and 128GB of memory.

\subsection{Meta-Path-Guided Graph Embedding(RQ1)}

As mentioned earlier, various meta-paths convey varied semantic meanings, leading to diverse graph embedding that influences query results. We focus on evaluating the effectiveness and efficiency of the method for meta-path-guided graph embedding.

\subsubsection{Effectiveness}

% \begin{table*}[ht]
% \centering
% \caption{\centering Case study on query ``Jiawei Han'' under different scenarios on ``Academic'' dataset.}
% % \scriptsize
% % \footnotesize
% % \renewcommand\arraystretch{1.2}
% % \resizebox{0.6\textwidth}{!}{
% \begin{tabular}{ccccc}
% \toprule
% \multicolumn{1}{l}{Rank} & APA   & APVPA   & APTPA   & meta-path-free \\
% \midrule
% 1     & Quanquan Gu & Philip S. Yu & Xifeng Yan & Yizhou Sun \\
% 2     & Yizhou Sun & Christos Faloutsos & Hong Cheng & Xifeng Yan \\
% 3     & Dong Xin & Divesh Srivastava & James Cheng & Philip S. Yu \\
% 4     & Deng Cai & Wei Wang & Ada Wai-Chee Fu & Xiao Yu \\
% 5     & Chi Wang & Yongxin Tong & Jia Wang & Christos Faloutsos \\
% \bottomrule
% \end{tabular}%
% % }
% \label{tab:addlabel}%
% \end{table*}%

Given that MHGNN generates distinct node embeddings for each meta-path and excels in downstream applications such as link prediction \ref{sec:app}, we validate our meta-path-guided embedding performance using the recommendation results of MHGNN as the benchmark. Table~\ref{tab:recommendation} shows that the FHGE model demonstrates superior performance in both datasets, confirming its effectiveness in feature extraction and reconstruction. MAGNN and SeHGNN also show strong performances in specific categories, especially in handling complex structural data. The performance of RGCN suggests that simple aggregation of graph information may not be enough for analysis.

% Table generated by Excel2LaTeX from sheet 'recommendation'
\begin{table}[htbp]
% \tabcolsep=0.06cm
% \small
\centering
% \caption{Add caption}
\caption{\centering recommendation result using meta-path-guided node embeddings. r@20 refers to recall@20 and n@20 refers to ndcg@20. Vacant positions (``-'') are due to a lack of node embeddings based on that method. The best results are highlighted in bold.}\label{tab:recommendation}
\begin{tabular}{lcccccccc}
\toprule
      & \multicolumn{4}{c}{LastFM}    & \multicolumn{4}{c}{IMDB} \\
\midrule
      & \multicolumn{2}{c}{UAU} & \multicolumn{2}{c}{UATAU} & \multicolumn{2}{c}{MAM} & \multicolumn{2}{c}{MGM} \\
\midrule
      & \multicolumn{1}{l}{r@20} & \multicolumn{1}{l}{n@20} & \multicolumn{1}{l}{r@20} & \multicolumn{1}{l}{n@20} & \multicolumn{1}{l}{r@20} & \multicolumn{1}{l}{n@20} & \multicolumn{1}{l}{r@20} & \multicolumn{1}{l}{n@20} \\
\midrule
RGCN  & 12.41 & 9.39  & 7.97  & 6.09  & 63.58 & 54.17 & 86.21 & 74.63 \\
MAGNN & 13.66 & 12.05 & 9.81  & 9.32  & 65.44 & 56.35 & 87.73 & 73.21 \\
SeHGNN & \multicolumn{1}{l}{-} & \multicolumn{1}{l}{-} & \multicolumn{1}{l}{-} & \multicolumn{1}{l}{-} & 68.52 & 56.89 & \textbf{88.65} & \textbf{74.79} \\
FHGE  & \textbf{15.32} & \textbf{14.68} & \textbf{13.99} & \textbf{12.24} & \textbf{69.33} & \textbf{57.2}  & 88.19 & 73.24 \\
\bottomrule
\end{tabular}%
\end{table}%

\subsubsection{Efficiency}

To accommodate ad-hoc meta-paths defined by users, it's crucial that methods can quickly generate meta-path-guided graph embeddings. Fig.~\ref{fig:time_cost_compare} depicts the efficiency performance of these methods based on various scenarios. In the meta-path-free scenario on the ACM dataset, HetGNN and MHGNN take 3523.5s and 2693.7s, respectively, while RGCN, HAN, and HGT take 32.76s, 15.92s, and 135.57s, respectively. Conversely, FHGE achieved a time cost of just 1.2s, 26.6 times faster than the HAN on the ACM dataset and 39.8 times faster than the MAGNN method on the IMDB dataset. The comparison of the time cost of node embedding based on specific meta-paths gives a similar conclusion. This rapid performance is reaffirmed in Fig. ~\ref{fig:metric_time_memory}, which compares the time cost of node embedding for specific meta-paths across different datasets. The exceptional speed of FHGE in meta-path-guided node embedding makes it capable of user query with ad-hoc meta-paths.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{image/time_cost_compare.eps}
    \caption{The time cost comparison on different meta-paths for the ``Academic'' and ``IMDB'' datasets. The time costs are presented on a log scale. (Blank bars indicate that the method is unable to generate meta-path-guided graph embeddings.)} \label{fig:time_cost_compare}
\end{figure}

% Such delays are almost intolerable for real-time searches.
% . This highlights the significant speed improvement of FHGE,

% FHGE’s exceptional speed stems from its design that segments graph information using Meta-Path Units (MPUs). It preprocesses key computations related to local MPU information and inter-MPU attention mechanisms offline. During live queries, FHGE quickly integrates this data based on ad-hoc meta-paths to generate the required node embeddings, significantly accelerating the process by avoiding real-time computational delays.

% In contrast, other algorithms either lack the functionality to create node embeddings based on specific meta-paths or require lengthy, on-the-fly computations using graph data, resulting in slower performance. FHGE’s strategy of pre-processing much of the needed data enables it to quickly synthesize results as required, offering a substantial speed advantage over conventional methods. This efficiency makes FHGE a potent tool for graph-based data analysis, capable of performing complex queries in a fraction of the time traditionally needed.

\subsection{Applications (RQ2)}\label{sec:app}

To validate the practicality of HetFS, we compare it with SOTA methods on various applications.

\subsubsection{Link Prediction}\label{subsec:lp}
% FHGE outperforms all baselines, particularly excelling in citation link prediction. 
Table~\ref{tab:lp} presents the performance metrics of all models, with the superior results highlighted in bold. FHGE demonstrates competitive performance compared to SOTA methods. Despite not spending extensive time generating graph embedding online, FHGE still achieves strong results in link prediction. This effectiveness is due to its systematic integration of structural, node, and semantic information, which adeptly captures the complexities of graph data. Additionally, the dual attention mechanism employed by FHGE effectively discerns subtle semantics, preserving the intrinsic relationships between nodes.

% Table generated by Excel2LaTeX from sheet 'link_prediction'
\begin{table}[htbp]
\centering
% \tabcolsep=0.15cm
\caption{\centering Link prediction task. Vacant positions (``-'') are due to a lack of similarities based on that method. Highlighted are the top \underline{\textbf{first}}, \textbf{second}, \underline{third}.}\label{tab:lp}
\begin{tabular}{ccccccc}
\toprule
      & \multicolumn{2}{c}{Amazon} & \multicolumn{2}{c}{LastFM} & \multicolumn{2}{c}{PubMed} \\
\midrule
      & AUC   & MRR   & AUC   & MRR   & AUC   & MRR \\
\midrule
RGCN  & 85.02 & 93.76 & 81.91 & \textbf{96.67} & 71.79 & \underline{\textbf{97.37}} \\
MAGNN & -     & -     & 81.06 & 87.33 & -     & - \\
HetGNN & \textbf{95.53} & \underline{\textbf{97.86}} & 87.73 & 96.05 & 89.45 & 94.21 \\
MHGNN & \textbf{95.53} & \textbf{97.85} & \textbf{87.93} & \underline{96.56} & \underline{89.89} & 94.57 \\
HGT   & 95.3  & 97.79 & \underline{\textbf{90.22}} & 96.37 & \underline{\textbf{90.43}} & \textbf{96.98} \\
FHGNN & \underline{\textbf{95.54}} & \textbf{97.85} & \textbf{89.26} & \underline{\textbf{97.31}} & \textbf{90.38} & \underline{96.65} \\
\bottomrule
\end{tabular}%
\end{table}%


\subsubsection{Node Classification}

Table~\ref{tab:nc} reports the Micro-F1 and Macro-F1 scores for each model, with the best results highlighted in bold. As we can see: (1) Most models demonstrate excellent performance in single-label classification, achieving high Macro-F1 and Micro-F1 scores (over 0.80). This is expected given the distinct nature of labeled nodes. (2) MAGNN and FHGE demonstrate high consistency and effectiveness across both datasets. This is because their architectures are carefully designed to capture information from different relations. Consequently, they are well-suited for diverse data scenarios.

% (2) All methods show significantly lower performance in multi-label classification tasks, which is attributable to the higher complexity of these tasks. Multi-label classifications are not necessarily independent. The implicit correlation between labels makes simple binary classification approaches less accurate

% For node clustering and classification tasks, we follow the methodology outlined in \cite{zhang2019heterogeneous}. We categorize authors in the ``Academic'' dataset into four distinct research domains: Data Mining (DM), Computer Vision (CV), Natural Language Processing (NLP), and Database (DB). To achieve this, we select the top three venues for each domain. Each author is assigned a label corresponding to the domain with the majority of their publications (authors without papers in these venues are excluded from the evaluation). The node information is derived from the complete dataset.

% For the node clustering task, we assumed that authors sharing similar research interests were likely to be active in similar domains. Consequently, we classified authors based on the conference categories where they exhibited the highest participation. We fed the learned node embeddings into a k-means clustering algorithm. The clustering performance was evaluated using metrics such as normalized mutual information (NMI) and adjusted rand index (ARI).

% For the multi-label classification task, the acquired node embeddings served as input for a logistic regression classifier. Notably, the train/test ratio was set at 1:9. Evaluation metrics encompass both Micro-F1 and Macro-F1.

% (3) The superior classification results of FHGE can be attributed to its consideration of authors who co-participate in conferences as highly similar. Since we also classify authors based on their most frequently attended conferences in the node clustering task, FHGE aligns well with this clustering objective.

% \begin{table}[htbp]
% \centering
% \tabcolsep=0.15cm
% \begin{tabular}{ccccccc}
% \toprule
%       & \multicolumn{2}{c}{DBLP} & \multicolumn{2}{c}{IMDB} & \multicolumn{2}{c}{ACM} \\
% \midrule
%       & Ma & Mi & Ma & Mi & Ma & Mi \\
% \midrule
% RGCN  & 89.88 & 90.49 & 51.44 & 56.74 & 83.14 & 82.91 \\
% HAN   & \underline{\textbf{93.4}}  & \textbf{93.84} & \underline{\textbf{60.46}} & \underline{\textbf{65.68}} & 86.78 & 86.87 \\
% MAGNN & \underline{92.56} & \underline{93.24} & \textbf{57.72} & \textbf{63.93} & \underline{\textbf{92.05}} & \underline{\textbf{91.97}} \\
% HetGNN & 92.35 & 92.88 & 50.29 & 53.28 & 88.76 & 88.62 \\
% MHGNN & \textbf{92.64} & 92.95 & 51.11 & 53.07 & \underline{90.83}  & \underline{90.77} \\
% HGT   & 91.11 & 92.04 & 41.09 & 54.24 & 88.82 & 88.91 \\
% FHGE & 92.05 & \underline{\textbf{93.97}} & \underline{54.24} & \underline{57.13} & \textbf{90.86}  & \textbf{90.79} \\
% % FHGE &       &       &       &       &       &  \\
% \bottomrule
% \end{tabular}%
% \caption{\centering Node classification task. ``Ma'' refers to to Macro-F1 and ``Mi'' refers to to Micro-F1. Highlighted are the top \underline{\textbf{first}}, \textbf{second}, \underline{third}.}\label{tab:nc}
% \end{table}

\begin{table}[htbp]
\centering
% \tabcolsep=0.15cm
\caption{\centering Node classification task. Highlighted are the top \underline{\textbf{first}}, \textbf{second}, \underline{third}.}\label{tab:nc}
\begin{tabular}{ccccc}
\toprule
      & \multicolumn{2}{c}{DBLP} & \multicolumn{2}{c}{ACM} \\
\midrule
      & Macro-F1 & Micro-F1 & Macro-F1 & Micro-F1 \\
\midrule
RGCN  & 89.88 & 90.49 & 83.14 & 82.91 \\
HAN   & \underline{\textbf{93.4}}  & \textbf{93.84} & 86.78 & 86.87 \\
MAGNN & \underline{92.56} & \underline{93.24} & \underline{\textbf{92.05}} & \underline{\textbf{91.97}} \\
HetGNN & 92.35 & 92.88 & 88.76 & 88.62 \\
MHGNN & \textbf{92.64} & 92.95 & \underline{90.83}  & \underline{90.77} \\
HGT   & 91.11 & 92.04 & 88.82 & 88.91 \\
FHGE & 92.05 & \underline{\textbf{93.97}} & \textbf{90.86}  & \textbf{90.79} \\
\bottomrule
\end{tabular}%
\end{table}

% Table generated by Excel2LaTeX from sheet 'node_classification'


\subsection{Ablation Study (RQ3)}

FHGE introduces a dual attention mechanism to capture local and global semantics by intra-MPU and inter-MPU aggregation, respectively. To validate these attentions for FHGE, we conduct ablation studies to evaluate the performance of two model variants that eliminate the intra-MPU and inter-MPU attention, respectively. Table~\ref{tab:ablation_study} shows that both intra-MPU attention and inter-MPU attention consistently improve overall performance. However, intra-MPU attention contributes to a smaller improvement across various tasks, potentially due to the presence of excessive irrelevant information, particularly relational data, within the graph, which may degrade overall performance effectiveness.

\begin{table}[htbp]
\centering
% \fontsize{12}{12}\selectfont
% \tabcolsep=0.1cm
% \scriptsize
% \small
% \resizebox{0.48\textwidth}{!}{
\caption{\centering Ablation study for FHGE.}\label{tab:ablation_study}
\begin{tabular}{ccccccc}
\toprule
Task & \multicolumn{2}{c}{Link Prediction} & \multicolumn{2}{c}{Node Classification} \\
\midrule
Dataset & \multicolumn{2}{c}{LastFM} & \multicolumn{2}{c}{DBLP} \\
\midrule
Metric & AUC & MRR & Macro-F1 & Micro-F1 \\
\midrule
+intra-MPU attention & 85.88 & 94.49 & 83.14 & 84.91 \\
+inter-MPU attention & 88.56 & 94.24 & 86.78 & 88.87 \\
% w.o. intra-MPU attention & 88.56 & 94.24 & 86.78 & 88.87 \\
% w.o. inter-MPU attention & 85.88 & 94.49 & 83.14 & 84.91 \\
FHGE & 89.26  & 97.31 & 92.05 & 93.97 \\
\bottomrule
\end{tabular}%
% }
\end{table}

\subsection{Hyper-parameter Sensitivity (RQ4)}

The hyper-parameters play important roles in FHGE. To provide in-depth knowledge of how they determine the generations of graph embedding, we investigate the sensitivity of different hyper-parameters in FHGE, including the amount of iterations and the embedding dimensions.

\subsubsection{Amount of Iterations}

We analyze the convergence properties of our FHGE on various datasets. The results in Fig.~\ref{fig:iteration_lp} demonstrate that FHGE converges quickly and reaches stable performance after 50 iterations, with additional iterations not significantly enhancing its performance. This may be attributed to the increased perceptual field during the precomputation phase, which allows for more comprehensive information capture. The ability to quickly converge makes FHGE particularly well suited for generating ad hoc meta-path-guided node embeddings.

% As most pertinent information is captured during precomputation, online integration does not contribute substantially to further data assimilation. 

 % The ability to converge during preprocessing and to quickly achieve relatively high performance during online integration makes FHGE particularly well suited for generating ad hoc meta-path-guided node embeddings.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{image/iteration_lp.eps}
    \caption{Impact of iteration amount on link prediction tasks.} \label{fig:iteration_lp}
\end{figure}

% We analyze the online convergence properties of our FHGE on various datasets. The results in Fig.~\ref{fig:iteration_lp_online} show that additional iterations do not significantly enhance FHGE's performance. 

% \begin{figure}[htbp]
%     \centering
%         \includegraphics[width=\textwidth]{image/iteration_lp_online.png}
%     \caption{Impact of iteration amount during inter-MPU aggregation on link prediction tasks.} \label{fig:iteration_lp_online}
% \end{figure}

\subsubsection{Amount of Embedding Dimensions}

We then test the effect of the embedding dimension. Fig.~\ref{fig:dimension} illustrates the performance of FHGE when altering the embedding dimension from the default setting 128. We can see that as the embedding dimension grows, the performance rises first, then holds on within a large range of embedding dimensions, and even drops when the embedding dimension is too large. This is because the more embedding dimensions, the better representations can be learned. However, a too-large dimension may introduce additional redundancies and cause overfitting to the model.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{image/dimension.eps}
    \caption{Impact of dimension amount on link prediction tasks.} \label{fig:dimension}
\end{figure}

% \subsubsection{Amount of attention head}

% In this section, we employ a pooling approach to assess the effectiveness of meta-path-free cases. The pooling algorithm, commonly used in the domain of information retrieval, serves as an evaluation method to gauge the quality of document retrieval results. Typically, this method is applied to evaluate the top-K document ranking, where the system returns the K most pertinent documents relying on a user query. In the pooling algorithm, evaluators annotate each document in the retrieval results, with the annotation result being either binary (relevant/irrelevant) or a rating (e.g., on a scale of 1-5). The annotations from multiple evaluators are then aggregated to calculate the average score for each document. This allows for the ranking of the documents, with those ranked higher being considered more relevant by more evaluators.

% The underlying concept of pooling is as follows. Let's consider evaluating $l$ similarity measures $A_1, A_2, \cdot, A_l$, where each measure retrieves k results most similar to the given query. To begin, the top-k results are merged into a pool, eliminating any duplicates. Next, we present the pool of results to domain experts for evaluation. Taking into account the experts’ feedback, we select out top-k results, which will be used as the reference to access the top-k outcomes returned by $A_1, A_2, \cdot, A_l$. Specifically, we handpicked 14 widely recognized movies and presented 126 related movies as a reference. After crafting a survey, we invited 400 experts to participate and obtained 389 responses. Following a rigorous screening process, we identified 230 high-caliber responses to serve as the benchmark for statistical analysis. Next, we calculate the frequency of each movie and rank them. The frequency is considered as the true similarity score, and the order is ranking. We eventually reserve 13 movies to form the input query set, and the selected frequency and the ranking of 87 movies related to these 13 movies are regarded as the ground truth.

% With the ground truth in hand, we carry out a series of experiments on IMDB to assess the performance of all the methods using the two metrics.

% A similar property that HNSim performs outstandingly is shown in Fig.12(b), Fig12.(c), and Fig.12(d), among which the last two graphs analyze the results on global datasets. Note that wPPR even briefly outperforms HNSim in top-1 and top-2 results on IMDB-C and IMDB-D, but the mAP after convergence is still lower than HNSim. The difference between local datasets and global datasets suggests that increasing the dataset size has little impact on mAP.

 % There is a detail that even though the initial mAP and the converged mAP of SimRank-2 and SimRank-3 are the same, the top-2 and top-3 mAPs of SimRank-2 are higher than those of SimRank-3. This suggests that more iteration only sometimes leads to more similar results. In other words, the similarity between two items may exist in a relatively local relationship without considering too much complexity.

% \begin{figure}[h]
%     \centering
%         \includegraphics[width=\textwidth]{images/mapKMethodDataSetOnUserStudy14.png}
%     \caption{The mAP with varying K on IMDB} \label{fig:mapKMethodDataSetOnUserStudy14}
% \end{figure}

% The experiments conducted in this study demonstrated the practical effectiveness and efficiency of FHGE in mining graph data. Ad-hoc meta-path experiments revealed that the graph embedding generated with different meta-paths focused on different content aspects, showcasing the versatility of FHGE. Not only does FHGE delve deeply into graph data to provide graph embedding with insight, but it also generates graph embedding quickly for ad-hoc meta-paths. The downstream applications further demonstrated the real-world value of FHGE, showing that it can deliver results comparable to SOTA methods while ensuring rapid responses in on-the-fly queries.

% \section{Related Work}

% Over the past decade, numerous research on mining information from graphs have shifted from traditional representation learning approaches \cite{perozzi2014deepwalk,grover2016node2vec,dong2017metapath2vec} to methods utilizing deep neural networks, including GNNs \cite{fan2019metapath,yan2021relation,zhang23pagelink,zhu23AutoAC,SHAN24KPI-HGNN,MaYLMC24HetGPT} and GCNs \cite{kipf2016semi,liu2023rhgn}. Inspired by the Transformer \cite{vaswani2017attention}, GAT \cite{velickovic2017graph} integrates the attention to aggregate node-level information in homogeneous networks, while HAN \cite{wang2019heterogeneous} introduces a two-level attention mechanism for node and semantic information in heterogeneous networks. MAGNN \cite{fu2020magnn}, MHGNN \cite{liang2022meta} and R-HGNN \cite{yu23RHGNN} proposed meta-path-based models to learn meta-path-based node embeddings. HetGNN \cite{zhang2019heterogeneous} and MEGNN \cite{chang2022megnn} take a meta-path-free approach to consider both structural and content information for each node jointly. HGT \cite{hu2020heterogeneous} incorporates information from high-order neighbors of different types through messages passing across ``soft'' meta-paths. MHGCN \cite{fu2023multiplex} captures local and global information by modeling the multiplex structures with depth and breadth behavior pattern aggregation. SeHGNN \cite{Yang23Simple} simplifies structural information capture by precomputing neighbor aggregation and incorporating a transformer-based semantic fusion module. HAGNN \cite{zhu2023hagnn} integrates meta-path-based intra-type aggregation and meta-path-free inter-type aggregation to generate the final embeddings. While existing methods have partially addressed heterogeneous graph embedding, none efficiently support embeddings based on user-specified ad-hoc meta-paths. FHGE fills this gap by enabling user-defined ad-hoc meta-paths to guide the embedding process in heterogeneous graphs.
% , providing a more flexible and customizable solution.

\section{Discussion and Conclusion}

In this study, we present FHGE, a fast heterogeneous graph embedding method designed for ad-hoc queries, which incorporates graph information segmentation and reconstruction. Specifically, FHGE employs MPU to segment graph information into local and global components, then integrates node embeddings from relevant MPUs during reconstruction, while reusing local information to quickly adapt to specific meta-paths. During reconstruction, a dual attention mechanism is employed to enhance semantics capturing. Extensive experiments confirm its effectiveness and efficiency in similarity search and various downstream applications. Future research could focus on enhancing multi-label node embedding, extending FHGE to temporal link prediction, and boosting performance using advanced language models.

\begin{credits}
\subsubsection{\ackname} This work was mainly supported by the National Natural Science Foundation of China (NSFC No. 61732004).

\end{credits}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\bibliographystyle{splncs04}
\bibliography{FHGE_D}
% \begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}
\end{document}
