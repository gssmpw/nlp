\section{Background}
\label{sec:background}


This section discusses sensor-based applications, critiques existing approaches, and formalises the entropy metrics underpinning our analysis.

\subsection{Mobile Sensor-based Security Applications}

One major security application of mobile sensors is proximity detection, particularly for mitigating man-in-the-middle and relay attacks on mobile devices~\cite{halevi2012secure,mehrnezhad2015tap,gurulian2018good,markantonakis2024using,shrestha2014drone}. Mehrnezhad et al.\cite{mehrnezhad2015tap} introduced a technique that uses accelerometer readings to verify that an NFC payment instrument and terminal are physically tapped together. The authors posit that \emph{``physical tapping causes random but correlated vibrations at both devices, which are hard to forge (or reproduce)''} (p.1,~\cite{mehrnezhad2015tap}). The work reports an equal error rate (EER) of 17.65\% using a machine learning-based approach. Gurulian et al.~\cite{gurulian2018good} also explored the use of shared vibration patterns between users using unique vibration patterns generated by one device. Shrestha et al.~\cite{shrestha2014drone} explore four environmental modalities, including temperature and humidity, for proximity detection. In this work, single sensors yields 0.733--0.881 F1-score, while combining multiple increases the overall performance to 0.913--0.957. Mobile sensors have also found utility in tackling the longstanding problem of continuous authentication~\cite{patel2016continuous,mekruksavanich2021deep,hayashi2013casa,riva2012progressive,shi2011senguard,micallef2015aren,miettinen2014conxsense,li2013unobservable}, where sensor data is used to authenticate users passively without explicit interaction (with up to 99\% accuracy claimed in some work~\cite{mekruksavanich2021deep}). Sensors have also been used to underpin the security of novel device pairing schemes, whether as a primary~\cite{pan2018universense} or second line of authentication~\cite{mayrhofer2009shake}. These schemes generally rely on detecting similar motion patterns between two devices using joint accelerometer and gyroscope sensor measurements in order to provide evidence of co-location.% While such schemes assume that the motion patterns are highly unpredictable, they lack best-practice entropy analyses using, leaving open the question of how resistant they are to adversarial attacks.

The notion of `hardness' is typically inferred through model evaluation metrics. A general approach follows one whereby sensor data is collected from $N$ users from which various features are extracted in the time or frequency domain (e.g.\ cross-correlation, spectral energy, Hamming, Euclidean and mean-absolute distances)~\cite{markantonakis2024using,mehrnezhad2015tap,shrestha2018sensor,shrestha2014drone}. Features are then classified using simple threshold-based or supervised classification models, e.g.\ Support Vector Machines (SVM) and Random Forests. Calculating false positive (FPR) and negative (FNR) rates~\cite{mayrhofer2009shake}, precision and recall~\cite{riva2012progressive}, EERs~\cite{gurulian2018good,mehrnezhad2015tap}, Receiver-Operator Curves (ROC)~\cite{li2013unobservable}, and accuracy~\cite{li2013unobservable} are used to evaluate the model with respect to  distinguishing between legitimate and illegitimate samples. A system is deemed to be effective if the model can discriminate between such samples with low error. (User studies have also been employed to evaluate the effectiveness of sensor-based authentication systems~\cite{hayashi2013casa,mayrhofer2009shake}.) 

Some proposals have attempted to assess the entropy of sensor signals within the context of a security mechanism, but this represents a minority of work in the literature. T2Pair by Li et al.~\cite{li2020t2pair}, a zero-interaction pairing protocol, is found to have 32.3--38.5 bits of Shannon entropy; a refinement by Wu et al.~\cite{wu2024t2pair} reported 51--54 bits. Even in the best cases, this is low relative to modern cryptographic standards.\footnote{See AIS 20/31~\cite{bsi2024ais31}: \texttt{FCS\_RNG.1} specifies $\geq$240 and $\geq$250 bits of min- and Shannon entropy respectively for the effective internal state of a random number generator.} We also point to work that has questioned the utility of mobile sensors in time-critical domains. Markantonakis et al.~\cite{markantonakis2024using}, building on Gurulian et al.~\cite{gurulian2017effectiveness} and Shepherd et al.~\cite{shepherd2017applicability}, presented a reproducibility study of mobile sensors when deployed under a 500ms time constraint for NFC-based transactions as specified by the EMV payment protocol. Here, 0.179--0.246 EER was reported depending on the given sensor combination. Sensors were thus deemed unsuitable for proximity and relay attack detection without posing usability and security issues in practice. 

\subsection{Sensor Entropy Analyses}


In earlier work, Voris et al.~\cite{voris2011accelerometers} investigated accelerometers as true random number generators (TRNGs) on a WISP RFID tag and Nokia N97 phone. The authors find that min-entropy---defined in \S\ref{sec:defs}---is proportional to the motion applied to the device, with stationary movement having the lowest min-entropy. Intrinsic noise from the sensor's circuitry and seismic noise, and the sampling rate of its analog-to-digital converter (ADC), are considered significant influences on entropy generation. Min-entropy values of 3.1--11.4 bits were measured depending on the movement of the accelerometer. Lv et al.~\cite{lv2020analysis} analysed three mobile sensors on an undisclosed Xiaomi Redmi smartphone: a triaxial accelerometer, gyroscope, and magnetometer. Min-entropy values of 0.593--5.876 are reported, depending on the modality and the entropy estimation method.  Krhovj\'{a}k et al.~\cite{krhovjak2007sources} examined the entropy of image and audio data collected from mobile phone cameras and microphones respectively. Using Nokia N73 and E-Ten X500
and M700 phones, Shannon entropies of 2.9 (microphone) and 2.408--5.376 (camera) are reported, with min-entropy of 0.5 (microphone) and 0.754--3.928 (camera). Hennebert et al.~\cite{hennebert2013entropy} presented an analysis of  10 sensors on two wireless sensor monitors: a TI eZ430-RF2500 and a Zolertia Z1. A single-sensor analysis is presented, yielding min-entropy values of 0--7.85; motion sensors, e.g.\ accelerometer and vibration sensors, produced the highest entropy.


Sensors have also been suggested as entropy sources in low-cost RNG designs for mobile devices. Suciu et al.~\cite{suciu2011unpredictable} proposed using a phone's GPS module along with its accelerometer, gyroscope and orientation sensors. Using data from an HTC Google Nexus One, the approach passes the tests established in the NIST SP 800-22~\cite{rukhin2001nist} suite, but no precise entropy values were presented.  Wallace et al.~\cite{wallace2016toward} explored an RNG design using the accelerometer, gyroscope, microphone, WiFi, GPS and camera data as randomness sources. Results are presented from a non-standard entropy evaluation using 37 Android devices. Sensors within existing work serve as inherently \emph{opportunistic} entropy sources, i.e.\ in contrast with dedicated TRNGs using ring oscillators, Johnson-Nyquist thermal noise, and quantum phenomena (e.g.\ see \cite{hurley2020quantum}). Mobile sensors depend heavily on user behaviour and environmental, which could result in biases and correlations that are absent in controlled entropy sources. Existing sensor-based mechanisms largely overlook these dynamics, relying principally on heuristic or model evaluation metrics~\cite{riva2012progressive,li2013unobservable}. Such approaches do not account well for skewed distributions and other biases in the underlying data that affect predictability.  Contrast this with typical measures used in the area of randomness testing and authentication~\cite{mai2017guessability,uellenbeck2013quantifying,hurley2020quantum,turan2018recommendation,bsi2024ais31}. For instance, NIST SP800-90B~\cite{turan2018recommendation} and AIS 20/31~\cite{bsi2024ais31} recommend the use of min-entropy to estimate `worst-case' unpredictability of a given source. Our study bridges the gap by systematically evaluating entropy across modalities and datasets.


\subsection{Definitions}
\label{sec:defs}
We use the following definitions and notation throughout this work.



\begin{definition}[R\'{e}nyi Entropy]
Let \(X\) be a discrete random variable taking values in a set \(\mathcal{X}\) with probability mass function \(p(x)\). 
The \emph{R\'{e}nyi entropy} of order \(\alpha\) (\(\alpha > 0\), \(\alpha \neq 1\)) is defined as
\begin{equation}
H_{\alpha}(X)
\,=\, \frac{1}{1 - \alpha} \,\log\!\Biggl(\sum_{x \in \mathcal{X}} p(x)^{\alpha}\Biggr).
\label{eq:renyi_entropy_general}
\end{equation}
\end{definition}

We draw attention to four special cases of $\alpha$ that are widely used in the literature. Firstly, the \emph{Hartley (Max) Entropy}, given in Eq.~\ref{eq:renyi_H0}, is the  logarithm of the number of possible outcomes that have non-zero probability; it serves effectively as an upper bound.
\begin{equation}
H_{0}(X) \equiv \lim_{\alpha \to 0}H_\alpha(X)
\,=\, \log \Bigl|\bigl\{\,x \in \mathcal{X}: p(x) > 0 \bigr\}\Bigr|.
\label{eq:renyi_H0}
\end{equation}

Second is the \emph{Shannon Entropy} (Eq.~\ref{eq:renyi_H1}), which corresponds to the classical definition of entropy in information theory, and is the limit of \(H_{\alpha}\) as \(\alpha \to 1\).
 
\begin{equation}
H_{1}(X) \equiv \lim_{\alpha \to 1}H_\alpha(X)
\,=\, - \sum_{x \in \mathcal{X}} p(x)\,\log p(x).
\label{eq:renyi_H1}
\end{equation}

Another case is the \emph{Collision Entropy} (\(\alpha = 2\)), quantifying the probability of ``collisions'' of multiple draws from \(X\). This is given in Eq.~\ref{eq:renyi_H2}.
\begin{equation}
H_{2}(X)
\,=\, -\log \Bigl(\sum_{x \in \mathcal{X}} p(x)^{2}\Bigr).
\label{eq:renyi_H2}
\end{equation}

$H_1$ provides an average-case measure of uncertainty. It takes into account the entire distribution of outcomes; however, an adversary may only need to guess the most likely event to gain an advantage. Min-entropy is thus used as a conservative, worse-case metric, accounting for the least favorable distribution of outcomes. This is the  \emph{Min-Entropy} (Eq.~\ref{eq:renyi_Hinf}), i.e.\ the value in the limit \(\alpha \to \infty\).
\begin{equation}
H_{\infty}(X) \equiv \lim_{\alpha \to \infty}H_\alpha(X)
\,=\, -\log \Bigl(\max_{x \in \mathcal{X}} p(x)\Bigr).
\label{eq:renyi_Hinf}
\end{equation}

Note that $H_\alpha$ is a non-increasing function of $\alpha$, i.e.\ $H_{\infty}(X) \leq H_2 (X) \leq H_1(X)\leq H_0(X)$. $H_{\infty}(X)$ focuses on the single most likely outcome, providing a strictly tighter (and generally minimum) bound on uncertainty. We observe that min-entropy ensures that even the most skewed probability distributions still meet the required security guarantees; indeed, it is a recommended method for assessing entropy sources in NIST SP800-90B~\cite{turan2018recommendation} and AIS 20/31~\cite{bsi2024ais31}. We also rely on joint entropy for assessing the entropy of multiple random variables.

\begin{definition}[Joint R\'{e}nyi Entropy]
Let \(X_1, X_2, \dots, X_n\) be discrete random variables that jointly take values in
\(\mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_n\),
with joint probability mass function
\(p(x_1, x_2, \dots, x_n)\).
The \emph{R\'{e}nyi entropy} of order \(\alpha\) (\(\alpha > 0, \alpha \neq 1\)) for these \(n\) variables is defined as the following, where the sum is taken over all \((x_1,\dots,x_n)\) in
\(\mathcal{X}_1 \times \mathcal{X}_2 \times \cdots \times \mathcal{X}_n\):
\begin{equation}
H_{\alpha}(X_1, X_2, \dots, X_n)
\,=\, \frac{1}{1 - \alpha}\,\log\Biggl(
  \sum_{(x_1,\dots,x_n)} p(x_1, \dots, x_n)^{\alpha}
\Biggr)
\label{eq:multivariate_renyi_entropy}
\end{equation}


\end{definition}

In the limit \(\alpha \to 1\), then \(H_{\alpha}(X_1, X_2, \dots, X_n)\) converges to the classical joint Shannon entropy of these \(n\) variables. To support the later discussion on Chow-Liu trees---our approach to joint entropy estimation---we also define the Kullback-Leibler divergence.

\begin{definition}[Kullback-Leibler (KL) Divergence]\label{def:kl} Given a true probability distribution \( P(x) \) of a random variable and an approximate or reference distribution \( Q(x) \), the KL divergence is defined as follows:
\begin{equation}
    D_{KL}(P\;||\; Q) = \sum_{x \in \mathcal{X}}P(x)\log\Big(\frac{P(x)}{Q(x)}\Big)
\end{equation}
\end{definition}

