\subsection{Experimental Setup}

\subsubsection{Reinforcement Learning Settings} We use the SAC~\cite{haarnoja2018soft} reinforcement learning algorithm in all our experiments, with a buffer length of 1,000,000, a learning rate of 0.003, and a discount factor of 0.99. When the proposed action is unsafe, we execute a fallback in the environment but store the proposed action in the buffer. For all meta-learning experiments, the safety budget for each episode is $10^{-7}$; otherwise, the budget for the whole training is $10^{-3}$. We run all our experiments on a machine with one Intel Xeon E5-2683 v3 56-Core Processor and \qty{126}{\giga\byte} of RAM. A typical run of 400,000 interaction steps lasts from $4$ to $6$ hours. All our figures and tables report means and standard deviations, as computed from 3 random seeds.

\subsubsection{State and Action Spaces} As defined in Definition~\ref{def:shielded-env}, the state and action spaces of shielded environments feature large and complex objects of dynamic size, which are not easily fed to neural networks. For simplicity, we represent states across experiments using a finite number of features, which are: the state of the underlying MDP, the current bound parameter values, the current step ID within the episode (normalized by the maximum episode length), the number of available observations of each type (normalized by the maximum episode length) and the remaining safety budget (normalized by the total training budget when learning in a fixed environment and by the episodic budget in the case of meta-learning). Restricted classes of inference policies are represented by neural networks on a case-by-case basis, encoding inference actions as vectors of features.




\subsubsection{Benchmarked Agents} All experiments benchmark at least three agents. The \emph{Adaptive} agent is a shielded agent that showcases our framework. The \emph{Unshielded} baseline is a vanilla SAC agent that is allowed to perform unsafe actions. The \emph{Non-adaptive} baseline does not use the inference module to update the controller monitor and therefore implements a non-adaptive shield in the style of \emph{Justified Speculative Control}~\cite{DBLP:conf/aaai/FultonP18}. To ensure a fair comparison, especially in meta-learning settings, the last two baselines are given access to an inference module and an inference policy similar to the one used by the \emph{Adaptive} agent. However, the resulting bound parameters are not used to update a controller monitor but instead simply included in the agent's state, ensuring that all controllers can benefit from the same knowledge. Numbers of crashes are reported in Table~\ref{tab:crash-stats}, while the training overhead is shown in Table~\ref{tab:overhead-stats}. From these tables, we can see that our method can achieve zero crashes with little overhead. Additionally, we show the effects of the budget for different settings in Table~\ref{tab:budget-stats}. Large safety budgets can make the agent less conservative, potentially leading to crashes when the budgets are significantly high.


\begin{table}
    \footnotesize
    \caption{Shielding overhead during training. For all case studies, we indicate (in order): the total shielding overhead as a percentage of training time, the shielding overhead per simulation step (in milliseconds), the shielding overhead due to running the inference module per simulation step (in milliseconds), the average number of aggregated measurements per simulation step and the monitor size (as the sum of the AST sizes of the generated ``\textsf{safe}'' and ``\textsf{fallback}'' programs). We present means and standard deviations across 3 random seeds. As can be seen, despite our implementation being written in unoptimized Python, shielding overhead is consistently below $15\%$. Also, shielding overhead is dominated by the cost of running the inference module, which in turn increases with the average number of aggregated measurements per time step.}
    \label{tab:overhead-stats}
    \begin{tabular}{lccccc}
        \toprule
        Case Study & Overhead  ($\%$) & Overhead (\unit{ms}) & Inference (\unit{ms})  &  Avg. Aggregation & Monitor Size \\
        \midrule
        \primitiveinput{img/sisyphean_train/sisyphean_train_overhead.tex}
        \midrule
        \primitiveinput{img/versatile_train/versatile_train_overhead.tex}
        \midrule
        \primitiveinput{img/crossing_the_river/crossing_the_river_overhead.tex}
        \midrule
        \primitiveinput{img/revisiting_ACAS_X/revisiting_ACAS_X_overhead.tex}
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \small
    \caption{Return at testing time (average value of the last 100 episodes during 10,000 evaluation steps) and number of crashes during training and testing time, for different case studies and under different settings. The number of training steps and the maximum episode length for each setting are 80,000/100; 400,000/100; 400,000/100; 400,000/50; and 400,000/40 respectively.}
    \label{tab:crash-stats}
    \begin{tabular}{lcccc}
        \toprule
        Setting & Method & Return & Crashes during training & Crashes during testing\\
        \midrule
        \primitiveinput{img/sisyphean_train/sisyphean_train.tex}
        \midrule
        \primitiveinput{img/versatile_train/versatile_train_ksigma_large.tex}
        \midrule
        \primitiveinput{img/versatile_train/versatile_train_ksigma_small.tex}
        \midrule
        \primitiveinput{img/crossing_the_river/crossing_the_river.tex}
        \midrule
        \primitiveinput{img/revisiting_ACAS_X/revisiting_ACAS_X.tex}
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
  \small
  \caption{Return during testing and the number of crashes during both training and testing across all experiments, under varying safety budgets (budget per episode for meta-learning settings, and total budget for other scenarios). We present means and standard deviations across 3 random seeds. As can be seen, reducing the safety budget causes the agent to be more conservative. Setting an overly high safety budget results in crashes during training and/or testing.}
  \label{tab:budget-stats}
  \begin{tabular}{lcccc}
      \toprule
      Setting & Safety Budget & Return (testing) & Crashes (training) & Crashes (testing) \\
      \midrule
      \primitiveinput{img/sisyphean_train/sisyphean_train_budget.tex}
      \midrule
      \primitiveinput{img/versatile_train/versatile_train_ksigma_large_budget.tex}
      \midrule
      \primitiveinput{img/versatile_train/versatile_train_ksigma_small_budget.tex}
      \midrule
      \primitiveinput{img/crossing_the_river/crossing_the_river_budget.tex}
      \midrule
      \primitiveinput{img/revisiting_ACAS_X/revisiting_ACAS_X_budget.tex}
      \bottomrule
  \end{tabular}
\end{table}

\subsection{Sisyphean Train}\label{ap:sisyphean-train}

\subsubsection{Environment} Assume that we have a train that is 1,000 meters away from the station with a speed of $v$, and we want it to stop within a distance range. We take one of two actions every second: accelerate with an acceleration $A$ of $\qty{4}{\meter\per\second\squared}$ or brake with an acceleration $-B$ of $\qty{-4}{\meter\per\second\squared}$. However, due to the incline of the railroad, the final acceleration includes an additional, unknown term $f(x)$, where $x$ is the position of the train. We assume that the shape of the railroad is $C\sin(\omega x + \phi)$. As a consequence, we derive:
\[
    f(x) = g\cdot\frac{C\omega\cos(\omega x + \phi)}{\sqrt{1 + C^2\omega^2\cos^2(\omega x + \phi)}}.
\]
We know that $f$ is $gC\omega^2$-Lipschitz and bounded between $\left[-gC\omega/\sqrt{1 + C^2\omega^2}, \, gC\omega/\sqrt{1 + C^2\omega^2}\right]$. At each step, the agent observes $f(x) + \eta$, where $\eta \sim \mathcal{U}(\qty{-0.3}{\meter\per\second\squared}, \qty{0.3}{\meter\per\second\squared})$.


Our goal is to stop within a distance range of $0$ to $\qty{100}{\meter}$ from the station. If we are inside this range with a speed of less than $\qty{1}{\meter\per\second}$, the episode ends with a reward of $10$; if we go over the station, the episode ends with a reward of $-10$; in other cases, we get a reward of $-0.05$ per step (i.e., a reward sequence of $-0.00$, $-0.05$, $-0.10$) to encourage achieving the goal as early as possible. All episodes stop at a maximum length of 100 control cycles.

\subsubsection{Modelling} For this example, we use a variant of the shield defined in Figure~\ref{fig:overview-train-local}, with uniform measurement noise instead of Gaussian noise, allowing us to also demonstrate the use of concentration bounds to implement the $\InvCCDF$ operator.


\subsubsection{Experimental Protocol} In this case study, we choose $k=\qty{0.0025}{\per\second\squared}$, $F=\qty{3}{\meter\per\second\squared}$, $v=\qty{30}{\meter\per\second}$, $C=\qty{0.22}{\meter}$, $\omega=\qty{0.00083}{\per\meter}$, and $\phi=\pi/2$. Because the shape of the railroad is $C\sin(\omega x + \phi)$, we assume that the station is at the highest position, and we want to go up along the railroad. Although the change rate of the railroad's height is small, our knowledge of the upper bound $F$ is very conservative. Thus, using information from the observations for inference is important.

We do not have analytical solutions for the $\InvCCDF$ function with linear combinations of uniformly distributed observations, so we need to use concentration bounds to do approximate inference. We consider two different bounds: Chebyshev and Hoeffding. From our analysis in Section~\ref{sec:eval-tail-expressions}, we expect Hoeffding bounds to work better. When not mentioned, we use them as the default method.

We benchmark our \emph{Adaptive} agent against \emph{Unshielded} and \emph{Non-adaptive} baselines. In addition, we test a variant of our adaptive agent that uses the Chebyshev inequality for inference instead of the Hoeffding inequality. For all methods, we use a hardcoded inference policy that performs aggregation every time 20 observations are available within a radius of 100 meters. Each time, a portion of the remaining safety budget is spent, which is equal to the number of steps since the last aggregation, divided by the total number of training steps. %

\subsubsection{Results} We run experiments for 80,000 steps with a maximum episode length of 100. Results are shown in Figure~\ref{fig:example1-1} and Table~\ref{tab:crash-stats}. We find that the adaptive method achieves a higher return than the non-adaptive method, while still benefitting from the same safety guarantees. The adaptive agent that uses Chebyshev bounds performs no better than the non-adaptive baseline, due to producing bounds that are too conservative to be useful.

\begin{figure}
    \includegraphics{img/sisyphean_train/sisyphean_train_returns.pdf}
    \caption{Returns for the \textsc{Sisyphean train} case study. \emph{Chebyshev} is a variation of \emph{Adaptive} where the Chebyshev bound is used in place of the default Hoeffding bound.}
    \label{fig:example1-1}
    \Description[]{}
\end{figure}

\subsection{Versatile Train}\label{ap:versatile-train}

\subsubsection{Environment} We use the same environment setting and shield as in the Sisyphean train case study, except that observations involve Gaussian measurement noise $\eta \sim \mathcal{N}(0, \sigma^2)$ instead of uniform noise. Additionally, we add a reward of 0.1 for non-terminal states with a non-zero remaining budget to encourage the agent only to use its budget when necessary.


\subsubsection{Experimental Protocol} To demonstrate meta-learning, we randomly sample the phase factor $\phi$ from $\mathcal{U}(0, 2\pi)$ at every episode, so as to represent different shapes of the railroad. We consider two settings: one where $k/\sigma$ is large and one where $k/\sigma$ is small. For each of them, we benchmark two adaptive agents with different hardcoded policies. The first one spends a fixed budget to aggregate all available observations every 20 steps, while the other spends a fixed budget to aggregate all available observations every single step. In addition, the \emph{Adaptive} agent \emph{learns} an inference policy that chooses at every time step whether or not to aggregate all available observations and how much budget to spend on it.

Regarding the environment parameters, we set $F=\qty{2.5}{\meter\per\second\squared}$ and $v=\qty{30}{\meter\per\second}$ for both settings. In the $k/\sigma$ large setting, we choose $k=\qty{0.002}{\per\second\squared}$, $\sigma=\qty{0.001}{\meter\per\second\squared}$, $C=\qty{0.19}{\meter}$, and $\omega=\qty{0.00080}{\per\meter}$. In the $k/\sigma$ small setting, we choose $k=\qty{0.00001}{\per\second\squared}$, $\sigma=\qty{1}{\meter\per\second\squared}$, $C=\qty{38.2}{\meter}$, and $\omega=\qty{0.0000040}{\per\meter}$. Note that the value of $k$ is determined by $C$ and $\omega$.

\subsubsection{Results} We run training for 400,000 steps, with a maximum episode length of 100. The results are shown in Figure~\ref{fig:example2-1} and Table~\ref{tab:crash-stats}. As expected, an inference policy that aggregates observations in \emph{small} batches works better in a setting with irregular tracks (large $k$) and low sensor noise (small $\sigma$), while a policy that aggregates observations in \emph{large} batches works better in settings with regular tracks and high sensor noise. However, in both cases, an inference policy can be \emph{learned} that matches the performance of its best-hardcoded counterpart.

We also perform a qualitative analysis of the policy learned by the adaptive agent, which is shown in Figure~\ref{fig:example2-2}. We show two cases, involving two different combinations of $k$ and $\sigma$. Regarding the choice of acceleration/braking, we can see that as long as the estimated braking distance is small enough for safety, the agent chooses to accelerate (unless it is near the terminal). For the inference part, in both cases, no budget is spent in the last few steps, because inference does not help when it needs to stop anyway. Also, there are some differences in the frequency of budget spending. When $k/\sigma$ is large, the change of the road can be large, which means the information we get from one position decays quickly; the noise level is small, which means we can get a good estimation without aggregating a lot of observations. All of these issues make a high inference frequency attractive, and vice versa. It is also demonstrated in the figure.

\subsection{Crossing the River}\label{ap:crossing-river}

\subsubsection{Environment} Assume that we are a robot moving in a two-dimensional plane. Since our speed is slow, we assume that we have direct control of our velocity at each control cycle. We want to cross the bridge over a river, at night, as quickly as possible and without falling into the river. However, we do not know the position of the bridge and must find it using our lamp. Once turned on, the lamp allows getting noisy measurements of the bridge position, but only when the robot is within a given radius of the bridge. Using the lamp also consumes our energy, so we only want to turn it on when necessary.

For simplicity, we assume that the bridge is a line segment at $x=0 \wedge y\in [y_b - W, y_b + W]$, whose center position $y_b$ is sampled from $\mathcal{U}(\qty{-10}{\meter}, \qty{10}{\meter})$ and whose width is $W=\qty{1}{\meter}$. The river is a line at $x=0$. Our initial position for both the $x$ and $y$ directions is sampled from $\mathcal{U}(\qty{-20}{\meter}, \qty{20}{\meter})$ (we resample if $x=0$). Initially, our velocity is 0. Every second, we can control our velocity for both directions, within a range of $[\qty{-2}{\meter\per\second}, \qty{2}{\meter\per\second}]$.

We can observe the bridge position $y_b$ from within a $\qty{5}{\meter}$ radius when our lamp is turned on, with Gaussian noise of standard deviation $\sigma \times |x|$ (i.e. we get better observations by moving closer to the river). Observations are only available every 2 seconds, due to sensor delay.

Our goal is to go to the other side of the river. If we succeed, the episode ends with a reward of 10; if we move into the river, the episode ends with a reward of -10; otherwise, we get a penalty of -0.1 at each time step, or -0.2 when the lamp is turned on. Episodes have a maximum length of 50.

\subsubsection{Modelling} We show a shield specification for this environment in Figure~\ref{fig:river-crossing}. Note that none of the details about \emph{when} observations become available need to be modeled, since this information is not soundness-relevant.


\subsubsection{Experimental Protocol} We compare the usual \emph{Unshielded}, \emph{Adaptive} and \emph{Non-adaptive} agents.

\subsubsection{Results} We run training for 400,000 steps. The results are shown in Figure~\ref{fig:example3-1} and Table~\ref{tab:crash-stats}. Unsurprisingly, the non-adaptive agent is incapable of ever crossing the bridge since it can never get enough certainty about its position. We also provide a qualitative analysis of the learned policy in Figure~\ref{fig:example3-2}. In two instances, we can see the robot approaching the river, turning on its lamp, and moving along the river until it gets some observations, after which it spends some safety budget for inference and crosses.




\begin{figure}
    \includegraphics{img/crossing_the_river/crossing_the_river_returns.pdf}
    \caption{Returns for the \textsc{crossing the river} case study. All methods use a learned inference policy.}
    \label{fig:example3-1}
    \Description[]{}
\end{figure}

\input{figures/river-crossing}

\subsection{Revisiting ACAS X}\label{ap:acas}

\input{figures/acas}

\subsubsection{Environment} Suppose we are in an aircraft, and there is an intruder moving towards us, also within the same, vertical 2D plane. We want to avoid a collision with minimal disturbance to our planned trajectory, by choosing a sequence of vertical accelerations based on sensor information. For simplicity, we assume that both aircraft have a known, constant horizontal velocity. This allows us to specify a \emph{horizontal meeting time} $\catm=\qty{40}{\second}$ (when the horizontal separation between the planes is 0). At that time, we require that the difference between the altitude of our plane and the intruder is at least $\qty{500}{\metre}$ to avoid a collision.

At time $t=0$, we assume $h=0$ and $v=0$. We control the vertical acceleration of the plane every second, and the maximum vertical acceleration $A$ in both directions is $\qty{3}{\metre\per\second\squared}$.

We denote the altitude and vertical velocity of the intruder at time $t$ as $\cahint(t)$ and $\cavint(t)$ respectively. These quantities specify the intruder's trajectory, which is unknown. In addition, we distinguish between two kinds of intruders: those that are ACAS-compliant and those that are not. Compliant intruders will never accelerate upward if their initial altitude is lower than ours and never accelerate downward if their initial altitude is higher. We can model compliance with a boolean unknown $c$ with values in $\{0, 1\}$ (1 means compliant).

We assume that for all $t$, $\cahint(t)$ is bounded between $\qty{-2000}{\metre}$ and $\qty{2000}{\metre}$, and $\cavint(t)$ is bounded between $\qty{-50}{\metre\per\second}$ and $\qty{50}{\metre\per\second}$. The maximum vertical acceleration for the intruder in both directions is also $\qty{3}{\metre\per\second\squared}$.

We get information about the intruder's position and velocity using our sensors. We can measure its altitude with noise $\mathcal{N}(0, (\qty{20}{\meter})^2)$ and its velocity with noise $\mathcal{N}(0, (\qty{2}{\meter\per\second})^2)$. Two kinds of observations can be used to infer compliance: one comes from making radio calls and the other comes from analyzing the current intruder's trajectory. Both provide independent evidence and are received at times $t=5$ and $t=10$ respectively. For simplicity, we also assume that both have a false-positive rate of $10^{-4}$ (i.e. falsely classifying a plane as compliant) and a false-negative rate (i.e. failing to classify a plane as compliant) of $0.1$. Since the safety budget for each episode is $10^{-7}$, we can only afford to claim compliance from two supportive observations.

In simulation, we assume that the initial altitude $\cahint(0)$ of the intruder is between $\qty{-500}{\meter}$ and $\qty{500}{\meter}$, and its initial vertical velocity $\cavint(0)$ is between $\qty{-2}{\meter\per\second}$ and $\qty{2}{\meter\per\second}$. The compliant variable $c$ is sampled from $\slbernouilli(0.5)$. Then, the intruder chooses a fixed acceleration for the whole simulation. If not compliant, it chooses an acceleration within $[(-V - \cavint(0)) / \catm, (V - \cavint(0)) / \catm] \cap [\qty{-3}{\metre\per\second\squared}, \qty{3}{\metre\per\second\squared}]$, where $V=\qty{50}{\metre\per\second}$ is the vertical velocity bound. If compliant, it takes the intersection of the previous bound with $[0, \infty)$ or $(-\infty, 0]$, depending on the sign of $\cahint(0)$.

The safety goal is to maintain a distance of at least $\qty{200}{\meter}$ between our plane and the intruder at the time of the meeting. We get a reward of 10 if successful, and -30 otherwise. During other time steps, we are penalized in proportion to our distance to the horizontal line $h=0$ (our initial intended trajectory) and rewarded if compliance is inferred, with a reward of $-0.2|h| + 0.2\cdot(\cacmin > 0)$.

\subsubsection{Shield Specification} We show our shield specification in Figure~\ref{fig:acas}.
The invariant dictates that one of these cases must hold: (1) if we accelerate with acceleration $A$, we will be higher than the maximum possible altitude of the intruder at the meeting time $\catm$ (with margin $R$) or (2) if we accelerate with acceleration $-A$, we will be lower than the minimum possible altitude of the intruder (with margin $R$ also). The controller ensures that the invariant will still hold at the next control cycle. Interestingly, the controller itself does \emph{not} directly depend on the lower bound $\cacmin$ for the compliance parameter. Instead, $\cacmin$ influences the controller indirectly, by affecting the inference strategy for other bound parameters such as $\cahmintmax$ that the controller depends on.


\subsubsection{Experimental Protocol} Here, we still compare the unshielded, adaptive, and non-adaptive agents. All of them rely on a learned inference policy that determines (1) how much budget should be spent on aggregating all the available compliance evidence (if any) and (2) how much budget should be spent aggregating all other available measurements (if any).

\subsubsection{Results}
We run training for 400,000 steps, with a maximum episode length of 40. Results are shown in Figure~\ref{fig:example4-1} and Table~\ref{tab:crash-stats}. The adaptive agent fares better than the non-adaptive one, and it also achieves stable rewards without crashes.

We also perform a qualitative analysis of the policy learned by the adaptive agent, which is shown in Figure~\ref{fig:example4-2}. We show two cases, involving a \emph{compliant} and \emph{non-compliant} agent. When successfully inferring compliance, our aircraft significantly reduces its uncertainty about the final intruder position and thus manages to limit the magnitude of its avoidance maneuver.

\begin{figure}
    \centering
    \includegraphics{img/revisiting_ACAS_X/revisiting_ACAS_X_returns.pdf}
    \caption{Returns for the \textsc{Revisiting ACAS X} case study. All methods learn an inference policy that specifies how much budget to spend on compliance observations and position/velocity observations.}
    \label{fig:example4-1}
    \Description[]{}
\end{figure}
