\documentclass[acmsmall,screen,nonacm]{acmart}

\usepackage{amsmath}
\usepackage{paralist}
\usepackage{enumitem} %
\usepackage{mathtools} %
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{siunitx} %
\DeclareSIUnit{\rad}{rad}
\sisetup{input-digits = 0123456789\pi} %
\usepackage{algorithm} %
\usepackage[noend]{algpseudocode} %
\usepackage{proof} %
\usepackage{accents} %
\usepackage{makecell} %
\usepackage{forloop} %
\usepackage{ifthen} %
\usepackage{lipsum} %
\usepackage{xspace} %
\usepackage{listings} %
\usepackage{makecell}
\usepackage{color}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{acmlicensed}
\acmDOI{10.1145/3720450}
\acmYear{2025}
\acmJournal{PACMPL}
\acmVolume{9}
\acmNumber{OOPSLA1}
\acmArticle{109}
\acmMonth{4}
\received{2024-10-16}
\received[accepted]{2025-02-18}



\newcommand{\EquationsFigureSize}{}

\input{macros/math}
\input{macros/logic}
\input{macros/symbols}
\input{macros/notations}
\input{macros/shield-language}
\input{macros/shortcuts}
\input{macros/case-studies}
\input{macros/algorithmic-match.tex}





\begin{document}

\title{Adaptive Shielding via Parametric Safety Proofs}

\author{Yao Feng}
\authornote{Yao Feng contributed a majority of the case studies and experiments.}
\orcid{0000-0002-8213-5181}
\affiliation{%
  \institution{Tsinghua University}
  \city{Beijing}
  \country{China}
}
\email{y-feng23@mails.tsinghua.edu.cn}

\author{Jun Zhu}
\orcid{0000-0002-6254-2388}
\affiliation{%
  \institution{Tsinghua University}
  \city{Beijing}
  \country{China}
}
\email{dcszj@mail.tsinghua.edu.cn}

\author{Andr√© Platzer}
\orcid{0000-0001-7238-5710}
\affiliation{%
  \institution{Karlsruhe Institute of Technology (KIT)}
  \city{Karlsruhe}
  \country{Germany}
}
\email{platzer@kit.edu}

\author{Jonathan Laurent}
\authornote{Jonathan Laurent contributed a majority of the theoretical framework and writing.}
\orcid{0000-0002-8477-1560}
\affiliation{%
  \institution{KIT}
  \city{Karlsruhe}
  \country{Germany}
}
\affiliation{%
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{jonathan.laurent@kit.edu}

\renewcommand{\shortauthors}{Feng, Zhu, Platzer and Laurent}

\begin{abstract}
  A major challenge to deploying cyber-physical systems with learning-enabled controllers is to ensure their safety, especially in the face of changing environments that necessitate runtime knowledge acquisition. Model-checking and automated reasoning have been successfully used for shielding, i.e., to monitor untrusted controllers and override potentially unsafe decisions, but only at the cost of hard tradeoffs in terms of expressivity, safety, adaptivity, precision and runtime efficiency. We propose a programming-language framework that allows experts to statically specify \emph{adaptive shields} for learning-enabled agents, which enforce a safe control envelope that gets more permissive as knowledge is gathered at runtime. A shield specification provides a safety model that is parametric in the current agent's knowledge. In addition, a nondeterministic inference strategy can be specified using a dedicated domain-specific language, enforcing that such knowledge parameters are inferred at runtime in a statistically-sound way. By leveraging language design and theorem proving, our proposed framework empowers experts to design adaptive shields with an unprecedented level of modeling flexibility, while providing rigorous, end-to-end probabilistic safety guarantees.
\end{abstract}

\begin{CCSXML}
  <ccs2012>
     <concept>
         <concept_id>10010520.10010553</concept_id>
         <concept_desc>Computer systems organization~Embedded and cyber-physical systems</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10011007.10010940.10010992.10010998</concept_id>
         <concept_desc>Software and its engineering~Formal methods</concept_desc>
         <concept_significance>500</concept_significance>
         </concept>
     <concept>
         <concept_id>10010147.10010257.10010258.10010261</concept_id>
         <concept_desc>Computing methodologies~Reinforcement learning</concept_desc>
         <concept_significance>300</concept_significance>
         </concept>
   </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Computer systems organization~Embedded and cyber-physical systems}
\ccsdesc[500]{Software and its engineering~Formal methods}
\ccsdesc[300]{Computing methodologies~Reinforcement learning}

\keywords{Safe Reinforcement Learning, Programming Languages, Differential Dynamic Logic, Statistical Inference, Hybrid Systems}


\maketitle

\section{Introduction}

Learning-based methods such as reinforcement learning have shown great promise in the fields of autonomous driving~\cite{DBLP:journals/corr/Shalev-ShwartzS16a,DBLP:conf/eccv/LiangWYX18} and robot control~\cite{DBLP:journals/jirs/RovedaMFABTP20,DBLP:conf/icra/HesterQS12,DBLP:conf/icra/SmartK02}. However, their deployment in the real-world has been held back by reliability and safety concerns. The use of formal methods has been suggested to guarantee the safety of learning-enabled systems, both \emph{after} and \emph{during} training~\cite{DBLP:conf/aaai/FultonP18,DBLP:conf/tacas/FultonP19,DBLP:conf/atal/Elsayed-AlyBAET21,DBLP:conf/hybrid/HuntFMHDS21}. Many different approaches have been proposed to do so, which share as a foundation the idea of \emph{sandboxing} or \emph{shielding}~\cite{DBLP:journals/fmsd/MitschP16,DBLP:conf/aaai/AlshiekhBEKNT18,DBLP:conf/aaai/FultonP18,DBLP:conf/icra/ThummA22}. In this framework, the intended actions of a learning-enabled agent are monitored at runtime and overridden by appropriate fallbacks whenever they cannot be proved safe with respect to a model of the environment.

Most existing approaches leverage automated techniques for proving safety such as reachability analysis~\cite{DBLP:conf/icra/ThummA22,DBLP:conf/cdc/KollerBT018}, LTL model-checking~\cite{DBLP:conf/aaai/AlshiekhBEKNT18,DBLP:conf/isola/KonighoferL0B20} or Hamilton-Jacobi solving~\cite{DBLP:journals/tac/FisacAZKGT19}. Such techniques can be applied \emph{offline} to precompute numerical, table-based control envelopes that indicate what actions are safe in every possible state of the system. Alternatively, they can be used at runtime to analyze the safety of different actions with respect to the current state. Such \emph{online} methods mostly offer only bounded-horizon guarantees and must typically make aggressive tradeoffs to regain efficiency at the cost of precision or generality. However, an advantage of online methods is that they allow the model to be updated at runtime as the agent gathers information about its environment. Indeed, in many situations, a fully-specified model of the environment is not known at design-time and using a necessarily conservative static model may lead to overly cautious behavior. In principle, adaptivity can also be offered by offline methods, by precomputing control envelopes for \emph{every} possible model that may be considered at runtime. However, doing so further compounds the scalability challenges of model-checking. Finally, existing approaches to adaptive shielding, whether \emph{online} or \emph{offline}, are either offering limited expressivity to encode model uncertainty (e.g. \emph{bounded disturbance terms}~\cite{althoff2014online} or \emph{finite model families}~\cite{DBLP:conf/tacas/FultonP19}) or failing to provide rigorous end-to-end guarantees under assumptions that can be easily validated (e.g. methods based on learning Gaussian processes~\cite{DBLP:conf/aaai/ChengOMB19,DBLP:conf/eucc/BerkenkampS15,DBLP:conf/nips/BerkenkampTS017,DBLP:journals/tac/FisacAZKGT19}).

This work introduces a programming-language framework for designing \emph{adaptive} safety shields, leveraging a minimal yet crucial amount of human insight to target a very general class of model families, while shifting the full burden of safety analysis offline. In this framework, control envelopes are described by nondeterministic controllers, whose safety is established using differential dynamic logic~\cite{DBLP:journals/jar/Platzer08,DBLP:journals/jar/Platzer17}. Environment models can be parametrized by unknown function symbols. Those function symbols are subject to parametric bounds, whose parameters are accessible to the controller and can be \emph{instantiated and refined at runtime}. Updating these parameters in a way that is sound and efficient is a great challenge in itself. We address this challenge by offering experts a domain-specific language to express nondeterministic inference strategies that are \emph{sound by construction}, and whose nondeterminism is resolved by an \emph{inference policy} that can be programmed or learned in the same way that the \emph{control policy} is.

We illustrate our framework on four case studies, demonstrating its safety and its ability to handle advanced uncertainty models that are beyond the reach of existing methods. We also showcase the possibility of having agents learn how to manage their own safety budget by learning inference policies, which -- to the best of our knowledge -- has no equivalent in the literature.

\section{Overview}


\newcommand{\JSCModel}{\mathKeyword{Model}}

In this section, we motivate and illustrate our framework on a series of closely-related examples.

\subsection{Extracting Shields from Verified Nondeterministic Controllers}

Our framework builds on the idea of extracting runtime controller monitors from provably safe, nondeterministic controllers~\cite{DBLP:journals/fmsd/MitschP16,DBLP:conf/aaai/FultonP18}. The safety of such nondeterministic controllers is established using \emph{differential dynamic logic} ($\dL$)~\cite{DBLP:journals/jar/Platzer08,DBLP:journals/jar/Platzer17}, which is designed specifically to verify hybrid systems with both discrete and continuous dynamics. In $\dL$, we use hybrid programs (HPs) to model both controllers and the differential equations of the physical environments they interact with. A summary of the syntax and semantics of hybrid programs can be found in Table~\ref{tab:hps}. Proving the safety of a cyber-physical system using $\dL$ typically comes down to proving a modal formula of the form $\Init \!\limplyI\! \lboxI{(\Ctrl \seqI \Plant)^*}{\Safe}$, where $\Init$ and $\Safe$ are logical formulas while $\Ctrl$ and $\Plant$ are hybrid programs. This means that under a certain initial condition ($\Init$), no matter how often we execute the discrete-time controller ($\Ctrl$) and let the continuous-time system evolve to the next control cycle $(\Plant)$, the safety condition ($\Safe$) is satisfied.

\input{figures/hp-table}

For example, the $\dL$ model described in Figure~\ref{fig:overview-etcs} designates a nondeterministic train controller that must stop by the \emph{end of movement authority} $e$ located somewhere ahead of the train, as assigned by the train network scheduler~\cite{DBLP:conf/hybrid/PlatzerQ08}. Variable $x$ models the position of the train on its tracks. At every control cycle, the driver can choose between braking with acceleration $-B < 0$ or accelerating with acceleration $A > 0$. It then gives control to the environment until the next control cycle, which happens in at most $T$ seconds. However, the option to accelerate is only acceptable when a sufficient distance remains to accelerate and then brake safely. The safety of this nondeterministic train controller can be established by proving the validity of Formula~\ref{eq:example-model-def}, using the rules and axioms of $\dL$. This can be done interactively in a proof assistant such as KeYmaera X~\cite{DBLP:conf/cade/FultonMQVP15}. In particular, doing so requires finding a loop invariant $\Inv$ that holds initially, implies the safety property and is preserved by any run of hybrid program $\Ctrl \seqI \Plant$. Here, one can take $\Inv \equiv \Init$.

\input{figures/overview-etcs}

Crucially, one can extract a runtime monitor from the nondeterministic controller shown in Equation~\ref{eq:example-model-ctrl} and use it as a shield for a learning-enabled agent~\cite{DBLP:conf/aaai/FultonP18}. For example, one could use reinforcement learning to learn a deterministic train controller that optimizes for speed, energy efficiency and passenger comfort, while guaranteeing safety by overriding acceleration whenever the distance to the end-of-movement authority is insufficient (i.e. the guard in Equation~\ref{eq:example-model-ctrl} is violated).



\subsection{Adding Adaptivity via Parametric Bounds}

As illustrated in the previous section, previous work~\cite{DBLP:journals/fmsd/MitschP16,DBLP:conf/aaai/FultonP18} has shown how to extract shields from nondeterministic controllers whose safety is established using differential dynamic logic. However, doing so requires an accurate model of every safety-relevant aspect of the system. In cases where only a conservative model is available, the shield may force the system into an overly cautious behavior. One contribution of this work is to allow the design of \emph{adaptive} shields, whose behavior is refined at runtime as the agent gathers more information about its environment. To do so, we allow environment models to be parameterized by unknown quantities and control envelopes to be parameterized by bounds on these same quantities.

An example of such an adaptive shield is described in Figure~\ref{fig:overview-train-global}. It is specified using our proposed \emph{shield specification language}, which we define rigorously in Sections~\ref{sec:adaptive-shielding}~and~\ref{sec:inference-strategy-language}. This example is a variation of the train control system from Figure~\ref{fig:overview-etcs} with a continuous action space: instead of making a binary choice between \emph{accelerating} and \emph{braking}, the agent must select a \emph{commanded acceleration} $u \in [-B, A]$, which is expressed in $\dL$ using a nondeterministic assignment $u \dlassign *$ followed by a test $?(-B \!\le\! u \!\le\! A)$. In addition, the relationship between the commanded acceleration $u$ and the \emph{actual acceleration} $a$ of the train is governed by an unknown linear function: $a = \TrainThetaSlope u + \TrainThetaBias$ where $\TrainThetaSlope > 0$ and $\TrainThetaBias$ are unknown real parameters. At runtime, the actual acceleration of the train is measured repeatedly, allowing the agent to compute increasingly precise estimates of the systematic disturbance $\TrainThetaSlope$ and $\TrainThetaBias$ and thus allowing the shield to be increasingly permissive.

The shield specification in Figure~\ref{fig:overview-train-global} consists of eleven sections, each of them introduced by a distinct keyword. \textsc{constant} introduces real quantities that are known by the agent at runtime such as the maximum commanded acceleration $A$. \textsc{unknown} introduces quantities that are \emph{not} known and must be estimated at runtime, namely $\TrainThetaSlope$ and $\TrainThetaBias$. \textsc{assume} gathers global assumptions about constant and unknown symbols such as $A > 0$ and $\TrainThetaSlope > 0$. Such assumptions could be encoded as system invariants instead (see \textsc{invariant}) but separating them from state-dependent invariants leads to greater conceptual clarity and more concise proof obligations (global assumptions are preserved by definition). The \textsc{controller}, \textsc{plant}, \textsc{safe} and \textsc{invariant} sections define a $\dL$ model similar in shape to the one already studied in Figure~\ref{fig:overview-etcs}. However, the plant model can feature unknown symbols, which it does here via the $v' = \TrainThetaSlope u + \TrainThetaBias$ differential equation. Also the controller can depend on \emph{bound parameters} that constrain these unknowns and that are introduced in the \textsc{bound} section. Here, we introduce a lower bound $\TrainThetaSlopeMin$ on $\TrainThetaSlope$, an upper bound $\TrainThetaSlopeMax$ on $\TrainThetaSlope$, and an upper bound $\TrainThetaBiasMax$ on $\TrainThetaBias$. These parameters are instantiated and refined at runtime as the agent is gathering knowledge, using statistical inference. 
The controller is similar to Equation~\ref{eq:example-model-ctrl}, except that it offers a continuous action space and conservatively assumes a maximum achievable braking rate of $\TrainThetaBrakingRate$. Thus, the control envelope defined by the nondeterministic controller gets increasingly permissive as tighter bounds are obtained on $\TrainThetaSlope$ and $\TrainThetaBias$.

The invariant can depend on bound parameters but only monotonically, in the sense that it can only be made more permissive by a tightening of the bounds. This is an intuitive requirement: acquiring knowledge must never transition an agent from a state considered safe to a state considered unsafe. Here, this requirement clearly holds since the maximum guaranteed braking rate of $\TrainThetaBrakingRate$ can only get larger as $\TrainThetaSlopeMin$ and $\TrainThetaBiasMax$ tighten. Finally, \textsc{noise} and \textsc{observe} specify the kind of information that may become available at runtime to compute and refine such bounds. Here, we are assuming that at every control cycle, an estimate of the \emph{actual} acceleration of the system \emph{may} be measured, with some Gaussian noise of standard deviation $\sigma$. No guarantee is offered on \emph{when} and \emph{under what conditions} such an observation becomes available. $\textsc{infer}$ defines a family of provably-sound strategies for updating $\TrainThetaSlopeMin, \TrainThetaSlopeMax$ and $\TrainThetaBiasMax$ based on such observations, as we discuss shortly in Section~\ref{sec:overview-inference}.
Before we do so though, we introduce a last variant of our train control shield that illustrates how \emph{functional} unknowns can be handled, using the concept of a \emph{local bound}.





\input{figures/overview-train-global}

\subsection{Handling Functional Unknowns with Local Bounds}\label{sec:overview-train-local}

In our previous example from Figure~\ref{fig:overview-train-global}, two real-valued unknowns are estimated at runtime. However, our framework can handle a more powerful and flexible form of model-uncertainty in the form of \emph{functional unknowns}. Figure~\ref{fig:overview-train-local} provides an example, considering a train that evolves on tracks of unknown, varying slope. Variable $x$ denotes the train position on an arc-length parametrization of the tracks. A train at position $x$ is subject to an additional acceleration term of $-g\cdot\sin(\theta_x)$ where $g \approx \qty{9.81}{\metre\per\s\squared}$ is Earth's gravitational acceleration and $\theta_x$ the track angle at coordinate $x$. We model this influence by defining the train's kinematics as $v' = a + f(x)$, with $a$ the acceleration commanded by the controller and $f$ an unknown function of $x$. In addition, $f$ is assumed to be $k$-Lipschitz, globally lower-bounded by $-A$ (ensuring that the train will always move forward when instructed to accelerate at rate $A$) and upper-bounded by a known constant $F$. One could simply use this global bound to implement a conservative shield. Our challenge is to do better by estimating superior, local estimates of $f$ based on runtime observations.

\input{figures/overview-train-local}

We do so by introducing the concept of a \emph{local bound}. Figure~\ref{fig:overview-train-local} defines such a  bound, namely $\TrainFxMax \ge f(x)$. As opposed to the \emph{global} bounds used in the previous section, this bound involves quantity $f(x)$ that is state-dependent. The guarantee that comes with such a definition is that before every control cycle, the inference module must provide a value $\TrainFxMax$ that is an upper bound on the value of $f(x)$, as evaluated in the current state. The value of $\TrainFxMax$ can be used in the controller. However, $\TrainFxMax$ \emph{cannot} be mentioned in the invariant since $\TrainFxMax \ge f(x)$ may not hold anymore as the plant executes and the train moves further on the tracks. A local bound is only guaranteed to be valid at discrete points in time, after each run of the inference module and right before the controller executes. Thus, we also maintain an upper bound \emph{y} on $f(x)$ that holds \emph{throughout} the plant and can therefore be used in the invariant. Variable $y$ is updated before the controller executes with the value of $\TrainFxMax$ whenever the latter provides a tighter bound. Then, it is evolved via the differential equation $y'=kv$, degrading the precision of our bound at a rate proportional to the Lipschitz constant of $f$ to ensure its preservation by the plant ($\dif f/\dif t \le \dif f / \dif x \cdot \dif x / \dif t \le k \cdot v$).

We can now derive our invariant, from which the controller's acceleration guard follows. Given a particular state and provided a bound $y \ge f(x)$, we wonder whether the train can be kept safe indefinitely from the current state by fully engaging the brakes. Given a constant, {effective} braking rate of $b$ and starting with speed $v$, it can be shown that the distance needed for the train to brake to a full stop is $\bdist{v}{b} \equiv v^2/2b$. %
Thus, a sufficient condition for the train to be safe indefinitely is $x + \bdist{v}{B-F} \le e$, since we can guarantee an effective braking rate of at least $B-F$ using the global bound $F$ on $f$. However, a stronger guarantee may result from combining the local bound $y \ge f(x)$ and the fact that $f$ is $k$-Lipschitz. Indeed, since we already know from our naive estimate that the train can stop within distance $\bdist{v}{B-F}$, we also know that the value of $f$ along this trajectory must be upper-bounded by $y + k \cdot \bdist{v}{B-F}$. This gives us an effective braking rate along the stopping trajectory of $B - (y + k \cdot \bdist{v}{B-F})$. In turn, a new stopping distance can be computed from this estimate, yielding the invariant shown in Figure~\ref{fig:overview-train-local}. The controller's acceleration guard follows from the invariant, as it simply ensures that the invariant will still hold after executing the plant for time $T$.

So far, we have seen how adaptive shields can be extracted from nondeterministic, parametric controllers. Our next step is to show how inference modules can also be synthesized that are guaranteed to instantiate such parameters from runtime observations in a statistically sound way.


\subsection{Inferring Statistically-Sound Bound Parameters}\label{sec:overview-inference}


In this section, we delve into our proposed \emph{inference strategy language}, which is used to specify nondeterministic inference strategies for instantiating bound parameters at runtime. Inference strategies are introduced by the \textsc{infer} keyword. We consider the strategy from Figure~\ref{fig:overview-train-local} as an example (the strategy from Figure~\ref{fig:overview-train-global} is analyzed in Appendix~\ref{ap:train-global-inf-strategy}). An inference strategy consists of a sequence of \emph{inference assignments}, where each assignment computes a value for a particular bound parameter and updates this parameter whenever the new value is tighter than the old one. Our language supports three forms of assignments, which are all represented in the strategy from Figure~\ref{fig:overview-train-local}. Each assignment yields a proof obligation that establishes its soundness.

The first assignment $\TrainFxMax \slassign F$ indicates that the global bound $F$ can always be used as a local bound on $f(x)$. Its soundness is justified by the validity of the following proof obligation, which is automatically generated by our framework and to be proved by the user: \[\cdots \land(\lforall{x}  (-A \le f(x) \le F)) \land \cdots \limply f(x) \le F.\] Here, the right-hand-side of the implication is simply the definition of $\TrainFxMax$, where $\TrainFxMax$ has been substituted into the assignment's right-hand-side. The left-hand-side consists of the global assumptions, from which only the relevant ones are shown here. The second assignment $\TrainFxMax \slassign \slbest i: \TrainFxMax_i + k|x - x_i|$ indicates that any previously established local bound $\TrainFxMax_i$ induces a new bound $\TrainFxMax_i + k|x-x_i|$ for the current state. The associated proof obligation is: \[  (\lforall x \lforall y |f(x) - f(y)| \le k |x - y|) \land (f(x_i) \le \TrainFxMax_i) \limply f(x) \le \TrainFxMax_i + k|x - x_i|, \] where all irrelevant assumptions have been removed for clarity. To a first approximation, the semantics of this assignment is to compute one such bound for every element in the agent's history and assign the tightest one to $\TrainFxMax$ if it beats its current value. The third assignment $\TrainFxMax \slassign \slaggregate i: \omega_i + k|x - x_i| \sland \eta_i$ is most interesting and the one that performs statistical inference from observations. To understand it better, let us start from the associated proof obligation:
\begin{equation}\label{eq:train-local-aggregate-obligation}
(\lforall x \lforall y |f(x) - f(y)| \le k |x - y|) \land (\omega_i = f(x_i) - \eta_i) \limply (f(x) \le \omega_i + k|x-x_i| + \eta_i).
\end{equation}
The validity of this obligation establishes the inequality $f(x) \le \omega_i + k|x-x_i| + \eta_i$ for every triple $(x_i, \omega_i, \eta_i)$ in the agent's history. This does not directly give us a usable bound since $\omega_i$ and $\eta_i$ are random variables, only the first one of which is observed. However, given a tolerance-level of $\Eps$, we can use the standard tail bound on Gaussians to establish a concrete upper-bound on $f(x)$ that holds with probability at least $1 - \Eps$. Here, we get the upper-bound $b_i \equiv \omega_i + k|x-x_i| + \sigma \cdot z_\Eps$, where  $z_\Eps \equiv \sqrt{2} \cdot \ErfInv(1 - 2\Eps)$ and $\ErfInv$ is the inverse of the \emph{error function}~\cite{bertsekas2008introduction}. Indeed:
\[ \Prob\{f(x) > b_i \} \,\le\, \Prob\{ \omega_i + k|x-x_i| + \eta_i > b_i \} \,=\, \Prob \{ \eta_i > \sigma \cdot z_\Eps \} \,=\, \Eps. \]
The first inequality above is a consequence of Equation~\ref{eq:train-local-aggregate-obligation}, the middle equality holds by substituting the definition of $b_i$, and the last equality holds by definition of the error function.

\newcommand{\OverviewAggBound}{b_{\lambda, \Eps}}

Such a probabilistic bound may be acceptable in cases where measurement noise is small. However, when $\sigma$ is large, it may be unacceptably conservative. Fortunately, we can get tighter bounds by aggregating multiple independent observations together. More precisely, consider some nonnegative coefficients $\lambda_i$ that sum up to one. Then, one can show that the following is a bound on $f(x)$ with probability at least $1 - \Eps$:
\begin{equation}\label{eq:overview-train-local-aggr}
  \OverviewAggBound \ \equiv \ \sum_i \lambda_i \cdot (\omega_i + k|x-x_i|) \ + \ \sqrt{\sum_i \lambda_i^2} \cdot \sigma \cdot z_\Eps.
\end{equation}
The proof is similar to the one for Equation~\ref{eq:train-local-aggregate-obligation}. We have $f(x) > b \limplyI \omega_i + k|x-x_i| + \eta_i > b$ for all $i$ and $b$. Thus, taking a convex combination, we have $f(x) > b \limplyI \sum_i \lambda_i (\omega_i + k|x-x_i| + \eta_i) > b$ for all $b$. As a consequence:
\begin{equation}\label{eq:overview-train-local-aggr-proof}
  \Prob\{f(x) > \OverviewAggBound \} \ \le\  \Prob\,\biggl\{  \,\sum_i \lambda_i (\omega_i + k|x-x_i| + \eta_i) > \OverviewAggBound \, \biggr\} \ = \  \Prob\,\biggl\{ \,\sum_i \lambda_i\eta_i > \sigma' z_\Eps\, \biggr\} \,=\, \Eps,
\end{equation}
where $\sigma' \!\equiv\! \sigma \sqrt{\sum_i \lambda_i^2}$ is the standard deviation of $\sum_i \lambda_i \eta_i$, which is a zero-mean Gaussian provided that the choice of $\lambda$ is uninformed by -- and thus \emph{independent} from -- the values of $\eta_i$ and $\omega_i$.

The bound obtained in Equation~\ref{eq:overview-train-local-aggr} is the sum of two terms. Minimizing the first term requires putting all the weight on the \emph{closest} measurement, where $k|x-x_i|$ is smallest. On the other hand, minimizing the second term requires spreading the weights uniformly between measurements, resulting in a value of $(\sigma \cdot z_\Eps) / \sqrt{n}$, where $n$ is the number of considered measurements. Finding the right tradeoff can be subtle and situation-dependent. Similarly, the value chosen for $\Eps$ at every control cycle must be substracted from a global \emph{safety budget}, whose proper management constitutes an important challenge. Fortunately, our framework does not force such choices on the shield designers. Rather, it enables the specification of nondeterministic inference strategies, where the choice of $\Eps$ and $\lambda$ is determined at each control cycle by a non-soundness-critical \emph{inference policy} that can be learned similarly to the \emph{control policy}.

The \textsc{aggregate} construct from our inference strategy language allows generalizing the reasoning above to a large class of bounds and distributions. Provided a symbolic bound that can be expressed as the sum of an \emph{observable} component and of a \emph{noise} component (syntactically separated using the \textsc{and} keyword), \textsc{aggregate} computes a weighted average of multiples instances of such bounds, using probabilistic tail inequalities to handle the aggregated noise term. More generally, a strategy written in our proposed inference languages can be compiled into one proof obligation per inference assignment, along with an inference module that integrates with an inference policy at runtime. %

\subsection{System Overview}

\input{figures/overview-diagram}

Figure~\ref{fig:overview-diagram} provides a diagrammatic summary of our proposed framework for designing adaptive shields. In this framework, human experts are offered a domain-specific language for specifying shields in the form of parametric $\dL$ models coupled with nondeterministic inference strategies. A specification written in this language can be automatically compiled into a shield and into a series of proof obligations that are sufficient in establishing its soundness. All obligations are $\dL$ formulas, which can be discharged automatically in many cases but also proved interactively in a proof assistant such as KeYmaera X~\cite{DBLP:conf/cade/FultonMQVP15}. Obligations include the following:
\begin{itemize}
  \item The proposed invariant must be preserved when running the controller and the plant in sequence, assuming that all assumptions and bounds hold initially.
  \item The proposed invariant and the global bounds must imply the postcondition.
  \item The nondeterministic controller must be \emph{total}, in the sense that at least one action should be available in any state where the invariant and bounds are true.
  \item All inference assignments must be sound.
\end{itemize}
In the particular case where no functional unknowns are used and all differential equations admit an analytical solution expressible as a multivariate polynomial, all generated proof obligations are decidable. In general, manual proof assistance may be required for certain obligations. Notably, $\dL$ has proved effective at reasoning about \emph{unsolvable} differential equations~\cite{platzer2018differential,kabra2022verified}.

The generated shield consists of three components. The first one is an \emph{inference module} that is extracted from the expert-defined, nondeterministic inference strategy. The other two are a \emph{controller monitor} and a \emph{fallback policy}, both of which are extracted from the nondeterministic $\dL$ controller. At runtime, an untrusted agent interacts with the physical world under the protection of the shield, which overrides any proposed action that is not validated by the controller monitor using its fallback policy. The inference module is tasked to instantiate the model's bound parameters, which the controller monitor and the fallback policy depend on. The inference module is guided by the agent's inference policy, which provides hints on how the safety budget should be spent and how observations should be weighted when building aggregates. The safety budget is initialized with the system's tolerated probability of failure, which is typically a small value such as $10^{-6}$. In cases where not enough budget remains to honor the inference policy's recommendation, the inference module skips the associated inference assignment.

Importantly, the inference policy must \emph{not} depend on the value of the observations processed by the inference module and must only base its decisions on the \emph{availability} of these observations and on features of their associated states. Intuitively, this restriction prevents the agent from $p$-hacking its way to unsafety~\cite{significant}. Indeed, it would be unsound for the agent to make several measurements of the same quantity and then discard all measurements but the most favorable one. Another requirement enforced by the inference module is that the same observation cannot be reused across control cycles. Indeed, allowing the reuse of observations across control cycles enables an indirect form of cherry-picking since the next state after each control cycle may be influenced by the observations made during this cycle and thus carry some amount of information about it.


\section{Background}

\subsection{Differential Dynamic Logic}\label{sec:dL-semantics}

\input{figures/dl-syntax-semantics.tex}

Differential dynamic logic ($\dL$)~\cite{DBLP:journals/jar/Platzer08,DBLP:journals/jar/Platzer17} is designed specifically to verify hybrid systems with both discrete and continuous dynamics.
The syntax and semantics of $\dL$ is summarized in Figure~\ref{fig:dl-syntax-semantics}. A \emph{state} is defined as a mapping from variable names to real values. An \emph{interpretation} is defined as a mapping from function symbols to real functions of proper arity (possibly zero). The semantics of a \emph{hybrid program} $\alpha$ can be defined by induction on $\alpha$. For $I$ an interpretation, $\sem{\alpha} (I)$ denotes the set of all pairs of states $(s_1, s_2)$ such that $s_2$ is reachable from $s_1$ by executing $\alpha$. We provide an intuitive summary of such semantics in Table~\ref{tab:hps} but formal definitions can be found in the literature~\cite{DBLP:journals/jar/Platzer17}. The semantics $\sem{P}$ of a \emph{formula} $P$ is defined as the set of all $(I, s)$ pairs such that $P$ is true in state $s$ and under interpretation $I$. It follows naturally from first-order logic. In addition, $[\alpha]P$ is true if and only if $P$ is true after all runs of the hybrid program $\alpha$ and $\langle \alpha \rangle P$ is true if and only if there exists a run of the hybrid program $\alpha$ after which $P$ is true. Atomic formulas consist in comparisons of \emph{terms}. The semantics $\sem{\theta}$ of a term $\theta$ maps a pair of an interpretation and a state to a real value. A formula $P$ is said to be \emph{valid} (written $\Fvalid P$) if it is true in all states and interpretations.

\subsection{Notations for Valuations}

Given a set of variables $V$, a \emph{valuation} of $V$ is defined as a function $\rho : V \to \Reals$ that maps each variable in $V$ to a value. Moreover, a \emph{partial valuation} of $V$ is defined as a {partial} function $\rho': V \parto \Reals$ that attributes values to a subset $\dom \rho'$ of variables from $V$ (note the special arrow symbol for partial functions). We denote $\EmptyValuation$ an empty valuation. Moreover, if $v$ and $v'$ are valuations of two disjoint subsets of variables $V$ and $V'$, we write $v \CatValuation v'$ the valuation of $V \cup V'$ induced by $v$ and $v'$.

\subsection{Reinforcement Learning}

\newcommand{\RLState}{{S}}
\newcommand{\RLAction}{{A}}

Reinforcement learning (RL) is about learning to make decisions in an environment in such a way to maximize rewards. Environments are typically represented using Markov Decision Processes (MDPs)~\cite{sutton2018reinforcement}. An MDP is defined by a tuple $\tuple{\RLState, \RLAction, P, R, \gamma}$ where $\RLState$ is the state space, $\RLAction$ is the action space, and $P: \RLState \times \RLAction \to \Distrs{\RLState}$ is the transition function, which maps any state-action pair to a probability distribution over new states. $R: \RLState \times \RLAction \times \RLState \to \Reals$ is the reward function, which describes the reward associated with a specific state-action pair and a specific next state. Finally, $\gamma\in (0, 1)$ is the factor by which future rewards are discounted at each time step. The goal of RL is to find an optimal policy $\pi:\RLState \to \Distrs{\RLAction}$ for the agent to maximize the expected discounted sum of future rewards: $\mathbb{E}_\pi\left[\sum_{i=0}^{\infty}\gamma^iR(s_i, a_i)\right]$. In the most common \emph{model-free} RL setting, $P$ and $R$ are unknown and the agent accesses samples of these by directly interacting with the environment.

\subsection{Safe Reinforcement Learning via Shielding}
\label{sec:safe-rl-via-jsc}

This work builds on the Justified Speculative Control framework (JSC)~\cite{DBLP:conf/aaai/FultonP18}, which leverages models written in $\dL$ to shield reinforcement learning agents by only allowing provably-safe control actions. JSC crucially relies on the ability to extract monitors~\cite{DBLP:journals/fmsd/MitschP16} from nondeterministic $\dL$ controllers. To define those monitors~\cite{DBLP:journals/fmsd/MitschP16}, we need to bridge $\dL$ models with reinforcement learning environments. Thus, consider a $\dL$ model of the kind defined in Equation~\ref{eq:example-model-def}. Such a model induces a state space $\ProgState \equiv \FV(\JSCModel) \to \Reals$, where a state maps each free variable occuring in the model to a real value. In addition, the controller $\Ctrl$ induces an action space that can be defined inductively:
\begin{gather*}
    \CtrlActionSem{\alpha \cup \beta} \!\equiv\! \CtrlActionSem{\alpha} \uplus \CtrlActionSem{\beta}, \ \
    \CtrlActionSem{\alpha \seqI \beta} \!\equiv\! \CtrlActionSem{\alpha} \times \CtrlActionSem{\beta}, \ \
    \CtrlActionSem{x \dlassign *} \!\equiv\! \Reals, \ \
    \CtrlActionSem{x \dlassign e} \!\equiv\! \CtrlActionSem{?Q} \!\equiv\! \UnitSet.
\end{gather*}
In this definition, $X \uplus Y \equiv \{(\SumLeft, x) \!:\! x \!\in\! X\} \cup \{(\SumRight, y) \!:\! y \!\in\! Y\}$ denotes the \emph{disjoint union} of sets $X$ and $Y$ and $\UnitSet \equiv \{\Unit\}$ denotes a set with a single element. Intuitively, an action records a sequence of choices that uniquely characterizes a run of the controller.
For example, the discrete controller defined in Equation~\ref{eq:example-model-ctrl} has an action space $\CtrlActionSem{\Ctrl} = \UnitSet \uplus (\UnitSet \times \UnitSet)$, which is isomorphic to a 2-element set containing a \textit{braking} and an \textit{accelerating} action.
The continuous controller used in Figure~\ref{fig:overview-train-global} has an action space $\CtrlActionSem{\Ctrl} = \Reals \times \UnitSet$.
For any state $s \in \ProgState$ and action $a \in \CtrlActionSem{\Ctrl}$, we write $\CtrlExeSem{\Ctrl}(s, a)$ for the state that results from executing $\Ctrl$ while performing the sequence of choices encoded in $a$. A formal definition is provided in Appendix~\ref{ap:defining-action-ctrl}. From the structure of a nondeterministic controller, one can extract a \emph{controller monitor} that takes as an input a state and an action and returns a Boolean value indicating whether or not the action is permissible.


\begin{lemma}[Existence of a controller monitor and fallback policy] \label{lem:ctrl-monitor} Let $\Ctrl$ be a nondeterministic controller in the form of a $\dL$ hybrid program that is free of loops, modalities, differential equations, quantifiers and function symbols of nonzero arity. Then, there exists a \emph{computable} function $\CtrlMonitorSem{\Ctrl} : (\FV(\Ctrl) \toI \Reals) \times \CtrlActionSem{\Ctrl} \to \{\True, \False\}$ that we call \emph{controller monitor} and such that for all state $s$ and action $a$, $\CtrlMonitorSem{\Ctrl}(s, a) = \True$ if and only if $(s, \,\CtrlExeSem{\Ctrl}(s, a)) \in \sem{\Ctrl}(\EmptyValuation)$. In addition, given any formula $\Inv$ such that $\Fvalid \Inv \limplyI \ldiamondI{\Ctrl}{\,\True}$ (i.e. there exists an outgoing controller transition for all states in $\Inv$), there exists a computable function $\CtrlFallbackSem{\Ctrl} : (\FV(\Ctrl) \toI \Reals) \to \CtrlActionSem{\Ctrl}$ that we call \emph{fallback policy} and such that for all state $s$ such that $(\EmptyValuation, s) \in \sem{\Inv}$, $(s, \,\CtrlExeSem{\Ctrl}(s, \,\CtrlFallbackSem{\Ctrl}(s)) \in \sem{\Ctrl}(\EmptyValuation)$.
\end{lemma}


In our train example from Figure~\ref{fig:overview-train-global} (with a \emph{continuous} control input), the action space $\CtrlActionSem{\Ctrl}$ is isomorphic to $\mathbb{R}$. An action is a commanded acceleration $u$ and the runtime monitor $\CtrlMonitorSem{\Ctrl}$ simply evaluates the two tests featured in the controller definition for the selected value of $u$: it checks that both formula $(-B \!\le\! u \!\le\! A)$ and formula \( (x + vT + (\TrainThetaAcc)T^2/2 + (v + (\TrainThetaAcc)T)^2/2(\TrainThetaBrakingRate) \le e) \) evaluate to \emph{true} in the current state and for the current values of bound parameters $\TrainThetaSlopeMin$, $\TrainThetaSlopeMax$ and $\TrainThetaBiasMax$.
In our train example from Figure~\ref{fig:overview-train-local} (with a \emph{binary} control input), the action space is isomorphic to $\{\textsf{brake}, \textsf{accelerate}\}$. The braking action is always allowed. To determine whether or not to allow acceleration, the monitor evaluates the test from the associated branch of the nondeterministic controller from Figure~\ref{fig:overview-train-local}. In general, controller monitors can be derived from shield specifications via a simple syntactic transformation. We formalize this transformation and provide a proof of Lemma~\ref{lem:ctrl-monitor} in Appendix~\ref{ap:ctrl-monitor}. The KeYmaera X toolchain features a tool called ModelPlex~\cite{DBLP:journals/fmsd/MitschP16} for automatically deriving executable controller monitors from $\dL$ models.

The existence of a fallback policy is guaranteed by the assumption $\Fvalid \Inv \!\!\limply\!\! \ldiamondI{\Ctrl}{\,\True}$, which indicates that there exists \emph{at least} one way to resolve the nondeterministic choices in $\Ctrl$ in such a way to pass all tests (i.e. not trigger any monitor alert). The corresponding action can be computed systematically by solving a decidable SMT problem, hence the existence of a decidable fallback policy (Appendix~\ref{ap:generating-fallbacks}). However, for the sake of monitoring efficiency, tools like ModelPlex require the user to specify an \emph{explicit} fallback policy on a case-by-case basis. Doing so is never a problem in practice since proving $\Fvalid \Inv \!\!\limply\!\! \ldiamondI{\Ctrl}{\,\True}$ requires characterizing such a policy anyway. In particular, an explicit fallback policy can be automatically extracted from a constructive proof of this formula~\cite{bohrer2020constructive}. In our train examples, a possible fallback policy is to brake with maximal force, which corresponds to action $-B$ in the continuous case and action ``\textsf{brake}'' in the discrete case.





\section{Adaptive Shielding}\label{sec:adaptive-shielding}

\subsection{Shield Specifications}\label{sec:shield-spec}

We define a language to specify adaptive shields via parametric safety models. Models can assume a set of parametric bounds on a number of unknown quantities. The derived runtime controller is parametric in these same bounds, which are maintained and improved at runtime by a derived inference module.  Examples of \emph{shield specifications} are available in Figures~\ref{fig:overview-train-global}~and~\ref{fig:overview-train-local}. Formally, a \emph{shield specification} is defined by a tuple of the following elements:

\makeMathKeywordText
\begin{description}
  \item[$\Const$:] a set of \emph{constants}, which are $\dL$ symbols of arity zero whose value is known at runtime. %
  \item[$\Unknown$:] a set of $unknowns$, which are $\dL$ symbols of fixed arity, whose value is \emph{not} known at runtime and can be \emph{estimated} by the inference module using observations.
  \item[$\Assum$:] a set of \emph{assumptions}, which are $\dL$ formulas representing global assumptions about the constants and unknowns. Assumptions can feature symbols but no free variables. When clear from the context, we also write $\Assum$ for the \emph{conjunction} of all assumptions.
  \item[$\StateVar$:] a set of \emph{state variables}, which are $\dL$ variables used to represent model state. In our concrete syntax, the set of state variables is inferred as the set of all mentioned free variables that are not parameters, observation variables or noise variables (see below).
  \item[$\Param, \Dir, \Bound$:] a set of \emph{parametric bounds}. $\Param$ is a set of $\dL$ variables called \emph{bound parameters} (or \emph{parameters} for short). Each parameter $p$ is mapped to a $\dL$ formula $\Bound_p$ that is either \emph{monotone} or \emph{anti-monotone} with respect to $p$, depending on \emph{bound direction} $\Dir_p \in \{ \UpperAnnot, \LowerAnnot \}$. In our concrete syntax, the bound direction can be omitted whenever easily inferrable. For $\theta$ a $\dL$ term, we write $\BoundWith{p}{\theta}$ for the result of substituting $\theta$ for $p$ in $\Bound_p$. If $\Dir_p = \UpperAnnot$, we call $\Bound_p$ an \emph{upper-bound} and we mandate that it is monotone, meaning that $\Fvalid \forall x y \; (x  \le y \limplyI \BoundWith{p}{x} \limplyI \BoundWith{p}{y})$. If $\Dir_p = \LowerAnnot$, we call $\Bound_p$ a \emph{lower-bound} and we mandate that it is anti-monotone: $\Fvalid \forall x y \; (x \le y \limplyI \BoundWith{p}{y} \limplyI \BoundWith{p}{x})$. $\Bound_p$ can feature constants, unknowns and state variables. It cannot feature any parameter other than $p$. It is said to be \emph{local} if it contains a free occurence of a state variable. Otherwise, it is said to be \emph{global}. The associated parameter $p$ is also called \emph{local} or \emph{global} accordingly. We write $\LocalParam$ the set of \emph{local} parameters and $\GlobalParam$ the set of \emph{global} parameters. We write $\GlobalBound \equiv \bigwedge_{p \in \GlobalParam} \Bound_p$ for the conjunction of {all} global bounds. Finally, when clear from the context, we use the notational shortcut $\Bound \equiv \bigwedge_{p \in \Param} \Bound_p$ for the conjunction of all bounds. For $b \in \Param \parto \Reals$, we also write $\BoundConj{b} \equiv \bigwedge_{p \in \dom b} \BoundWith{p}{b(p)}$. %
  \item[$\Ctrl$:] a \emph{controller}, in the form of a (nondeterministic) $\dL$ hybrid program that is free of loops, modalities, differential equations and quantifiers. The controller can involve constants, parameters and state variables but \emph{no unknowns}. Only state variables can be modified. For $c \in \Const \toI \Reals$ and $b \in \Param \toI \Reals$, we write $\CtrlWith{c, b}$ for the result of substituting all constants and parameters in $\Ctrl$ by their value according to $c$ and $b$. Thus, $\CtrlWith{c, b}$ has no symbols and its free variables are all in $\StateVar$. For any $(c, b)$ pair, Lemma~\ref{lem:ctrl-monitor} guarantees that a controller monitor and a fallback policy can be extracted from $\CtrlWith{c, b}$.
  \item[$\Plant$:] a $\dL$ hybrid program that represents the \emph{physical environment}. As opposed to the controller, it can feature unknowns but no parameters. Only state variables can be modified.
  \item[$\Safe$:] a $\dL$ formula that represents the \emph{safety constraint} under which the system must operate. It can involve constants, unknowns and state variables but no parameters, since the safety condition cannot change over time as knowledge is gathered.
  \item[$\Inv$:] an \emph{invariant}, in the form of a $\dL$ formula. The invariant can feature constants, state variables and unknowns but only \emph{global} parameters. Local parameters are not allowed in the invariant. The following proof obligations are generated that involve the invariant:%
    \begin{enumerate}
      \item \label{obl:inv-implies-post} The invariant must imply the postcondition:
        $\Fvalid \Assum \wedge \GlobalBound \wedge \Inv \limplyI \Safe$.
      \item \label{obl:inv-preserved} The invariant must be preserved:
          $\Fvalid \Assum \wedge \Bound \wedge \Inv \limplyI \lbox{\Ctrl \seqI \Plant}{\Inv}$.
      \item \label{obl:ctrl-total} There always exists a safe controller action: $\Fvalid \Assum \wedge \Bound \wedge \Inv \limplyI \ldiamond{\Ctrl}{\True}$.
      \item \label{obl:inv-monotone} The invariant must be \emph{monotone} with respect to upper-bound parameters and \emph{anti-monotone} with respect to lower-bound parameters.
    \end{enumerate}
  \item[$\NoiseVar, \Noise$:] a set of \emph{noise variable declarations}. $\NoiseVar$ is a set of $\dL$ variables that can be used in the definition of observations and that model samples from \emph{mutually independent} random variables. Each  {noise variable} $\eta \in \NoiseVar$ is mapped to an expression $\Noise_\eta$ that denotes a probability distribution. $\NoiseVar_\eta$ can be either $\slnormal(\theta_1, \theta_2)$ for a normal distribution, $\sluniform(\theta_1, \theta_2)$ for a uniform distribution or $\slbernouilli(\theta_1)$ for a Bernouilli distribution. In all cases, $\theta_1$ and $\theta_2$ are $\dL$ terms that can feature constants and state variables. We write $\sem{\Noise} : (\Const \toI \Reals) \to \NoiseVar \to \Distr(\Reals)$ for the semantic denotation of $\Noise$.
  \item[$\ObsVar, \Obs$:] a set of \emph{observations}. $\ObsVar$ is a set of $\dL$ variables called \emph{observation variables}. Each observation variable $\omega$ is mapped to a defining $\dL$ term $\Obs_\omega$ that may feature constants, unknowns, state variables and noise variables (but no bound parameter).
 \item[$\Inference$:] an \emph{inference strategy}, which defines an inference module that issues concrete values for bound parameters at runtime, using an external \emph{inference policy} for guidance.
\end{description}
\makeMathKeywordMath
The inference strategy can be defined in a custom, domain-specific language that is explained in details in Section~\ref{sec:inference-strategy-language}. Before we dive into this language though, we characterize the abstract requirements that an inference module must fulfill in Section~\ref{sec:symbolic-inference-bounds}. The soundness of our proposed inference strategy language can then be established against these requirements.

\subsection{Symbolic Bound Instantiations}\label{sec:symbolic-inference-bounds}

An inference strategy $\Inference$ induces an action space $\InfActionSem{\Inference}$ for the agent's inference policy, which is defined rigorously in Section~\ref{sec:inference-strategy-language}. In particular, inference actions associated with a single $\textsc{aggregate}$ assignment are $(\Eps, \lambda)$ pairs, as seen in Section~\ref{sec:overview-inference}. An inference strategy denotes a function $\sem{\Inference} : \InfActionSem{\Inference} \to \List{\Param \times \SymbBoundExpr \times \EpsType}$ that maps an inference action to a list of {symbolic inference assignments}. A \emph{symbolic inference assignment} consists of a triple $(p, e, \Eps)$ of a bound parameter $p$, a \emph{symbolic bound instantiation} $e$ and a failure probability $\Eps$.

To illustrate the concept of a \emph{symbolic bound instantiation}, consider the following example statement from Section~\ref{sec:overview-inference}: ``$\TrainFxMax \slassign \slaggregate i: \omega_i + k|x - x_i| \sland \eta_i$''. Provided a $(\Eps, \lambda)$ pair and a history of states and observations, the right-hand-side of this statement can be evaluated into a concrete, numerical value proposal for $\TrainFxMax$. This is done in two stages, the first one of which being the computation of a \emph{symbolic bound instantiation} $e$ for $\TrainFxMax$. For example, if $\Eps=10^{-8}$, $\lambda_2 = 0.3$, $\lambda_5 = 0.7$ and $\lambda_i = 0$ for all $i \notin \{2, 4\}$, we obtain the following symbolic bound instantiation:
\begin{equation}\label{eq:symb-inst-example}
  e \,\equiv\, 0.3 \times (\omega_2 + k|x-x_2|) + 0.7 \times (\omega_5 + k|x-x_5|) + \InvCCDFwith{\eta_2,\eta_5 \sim \slnormal(0, \,\sigma^2)}(0.3 \eta_2+0.7\eta_5, 10^{-8}).
\end{equation}
In the expression above, the $\InvCCDF$ construct provides a symbolic representation of the \emph{inverse tail function} of a probability distribution, also called its \emph{inverse complementary cumulative distribution function} -- hence the notation. Given a random expression and a tolerance $\Eps$, $\InvCCDF$ returns the best upper-bound obtainable on this expression that is true with probability $1-\Eps$ at least. For example, we have $\InvCCDFwith{X \sim \sluniform(0, 1)}(X + 1, \Eps) = 2 - \Eps$ for all $\Eps \in [0, 1]$ since $\Prob_{X \sim \sluniform(0, 1)}\{X + 1 > 2 - \Eps\} = \Eps$. Details on how to compute or approximate the $\InvCCDF$ operator numerically are available in Appendix~\ref{sec:eval-tail-expressions}.

Provided some values for the constant symbols, for the current state and for past states and observations from the agent's history, the symbolic bound instantiation from Equation~\ref{eq:symb-inst-example} can be in turn evaluated into a real number. There are two reasons for performing such staging. First, it makes it very clear that $\sem{\Inference}$ has no access to the actual observation values when building $e$, which is critical for soundness (as previously discussed in Section~\ref{sec:overview-inference}). Second, this pushes the orthogonal challenge of evaluating $\InvCCDF$ outside of the core inference machinery.

\subsubsection{Formal syntax and semantics}

A sketch of a formal syntax and semantics for symbolic bound instantiations is provided in Figure~\ref{fig:tail-expressions}. A symbolic bound instantiation $e$ can be a $\dL$ term $\theta$, the sum of two other symbolic bound instantiations, an application of the $\InvCCDF$ operator or a \emph{guarded expression} $(e \WhenSymbBE P)$, where $P$ is a quantifier-free, modality-free $\dL$ formula. Importantly, the evaluation of a symbolic bound instantiation can fail and return a special value $\OptNone$. It does so when attempting to access the value of an undefined variable or symbol, or when evaluating $(e \WhenSymbBE P)$ while $P$ evaluates to $\False$. Support for $\OptNone$ is important since our framework does not mandate observations to be available at each cycle, making some observation variables possibly unassigned. In addition, ``$\WhenSymbBE$'' is necessary for our inference strategy language to support conditionally sound bounds, which we use in the example from Figure~\ref{fig:overview-train-global} and discuss further in Section~\ref{sec:inference-strategy-language}.

\subsubsection{Indexed variables}

Finally, symbolic bound instantiations can contain special $\dL$ variables, whose name contains an integer index (i.e. $\omega_2$). Semantically, there is nothing special about such variables and we can just regard them as normal variables following a naming convention. The only thing that matters is that for any $x \in V$ and $i \in \Nats$, $x_i \notin V$ where $V \equiv \StateVar \cup \Param \cup \ObsVar \cup \NoiseVar$. We also introduce some notations to ease the manipulation of indexed variables. For $v$ a partial valuation over $V$ and $i \in \Nats$, we write $\TagWithIndex{v}{i} \equiv \{x_i \mapsto r : v(x) = r \}$ the same valuation in which all domain variables are tagged with index $i$. For example, if $x, y \in V$ and $v = \{x \mapsto 0, y \mapsto 1\}$, then $\TagWithIndex{v}{2} = \{x_2 \mapsto 0, y_2 \mapsto 1\}$. In addition, for $P$ a formula with variables in $V$, we write $\TagWithIndex{P}{i}$ the formula that results from $P$ after tagging every free variable with index $i$. Finally, we also allow the use of symbolic names as indices. For example, in the inference assignment ``$\slaggregate i: \omega_i + k|x - x_i| \sland \eta_i$'', $\omega_i$ is a normal $\dL$ variable whose name follows a particular convention. We introduce a special syntactic transformation for ``instantiating'' symbolic indices in formulas. For example, if $P \equiv x_i + x_j > 0$, we write $\FSubst{P}{i,j}{2,3} \equiv x_2 + x_3 > 0$.

\input{figures/tail-expressions.tex}

\subsection{Soundness of Symbolic Inference Assignments}

As mentioned in the previous section, an inference strategy denotes a function $\sem{\Inference} : \InfActionSem{\Inference} \to \List{\Param \times \SymbBoundExpr \times \EpsType}$ that maps an inference action to a list of \emph{symbolic inference assignments}. Our proposed inference strategy language is \emph{sound} in the sense that any strategy expressed with it produces \emph{sound} assignments by construction. In turn, a symbolic inference assignment $(p, e, \Eps)$ is \emph{sound} if $e$ evaluates to a ``correct'' instantiation of $p$ with probability at least $1-\Eps$. To make this definition more precise, we need a couple of preliminary definitions:
\begin{itemize}
  \item A \emph{state} is defined as a valuation of the state variables: $\ProgState \equiv \StateVar \to \Reals$.
  \item A \emph{bound instantiation} is a partial valuation of the parameters: $\Inst \equiv \Param \parto \Reals$.
  \item An \emph{observation availability set} is a subset of observation variables indicating what observation were made at one point in time: $\ObsAvail \equiv \Subsets{\ObsVar}$.
\end{itemize}
We can now define the notion of \emph{soundness} for symbolic inference assignments. Intuitively, $(p, e, \Eps)$ is \emph{sound} if and only if for any possible history of states, observations and bound instantiations and for any intepretation of the model unknowns, updating an instantiation $b$ by assigning the value of $e$ to $p$ preserves the truth of $\BoundConj{b}$ with probability at least $1-\Eps$. The challenge in formalizing this intuition is to precisely characterize what counts as a \emph{possible} history. We do so in Definition~\ref{def:sound-inf-assignment}, building a valuation $v$ that stands for an arbitrary, consistent history.
\begin{definition}[Sound inference assignment]\label{def:sound-inf-assignment}
  A symbolic inference assignment $(p, \BSymb, \Eps)$ is said to be \emph{sound} if and only if for any $c \in \Const \toI \Reals$, $u \in \Unknown \toI \Funs$, $n \in \Nats$, $(s_1 \dots s_{n}) \in \ProgState^{n}$, $(z_1 \dots z_{n}) \in \ObsAvail^{n}$ and $(b_1 \dots b_{n}) \in \Inst^{n}$ such that $(c \CatValuation u, s_i) \in \sem{\Assum \land \Inv \land \BoundConj{b_i}}$ for all $i \le n$, we have:
  \[\Prob\left\{ (c \CatValuation u, s_{n}) \notin \sem{\BoundConj{\UpdateMap{b_n}{p}{r}}} \right\} \le \Eps \]
   where
    $r = \sem{e}(c, v)$,
    $v = \bigl(s_n \CatValuation b_n \bigr) \cup \bigl(\BigCatValuation_{1 \le i \le n} \TagWithIndex{s_i}{i} \CatValuation \TagWithIndex{b_i}{i} \CatValuation \TagWithIndex{o_i}{i}\bigr)$,
    $o_i = \{\omega \mapsto \sem{\Obs_\omega}(c \CatValuation u, s_i \CatValuation \eta_i) : \omega \in \Param \cap z_i \}$ for all $1 \le i \le n$,
    and $\eta_1 \cdots \eta_{n} \sim \sem{\Noise}(c)$ i.i.d.
\end{definition}



\subsection{Compatible Environments}

A shield acts as a buffer between an untrusted agent and a \emph{compatible} environment. By \emph{compatible}, we mean that the environment must conform to the assumptions made by the shield specification, without which no safety guarantee can be provided. The following definition formalizes this notion. %

\begin{definition}[Compatible environments]\label{def:compat-env}
  Consider a shield specification $\tuple{\Const, \dots, \Inference}$. A \emph{compatible environment} is a tuple $\tuple{\tuple{S, A, P, R, \gamma}, \StateMapping, \ActionMapping, \RevActionMapping, c, u, \ObsAvailFun, \ObsMeasurementFun}$ where:
  \begin{itemize}
    \item $\tuple{S, A, P, R, \gamma}$ is a Markov Decision Process.
    \item $\StateMapping: S \to \ProgState$ is a \emph{state mapping} function that maps environment states to shield states.
    \item $\ActionMapping: A \to \CtrlActionSem{\Ctrl}$ is a surjective \emph{action mapping}.
    \item $\RevActionMapping : \CtrlActionSem{\Ctrl} \to A$ is a reverse action mapping such that $\ActionMapping \circ \RevActionMapping = \IdFun$.
    \item $c : \Const \to \Reals$ is an interpretation of the shield's constant symbols.
    \item $u : \Unknown \to \Funs$ is an interpretation of the shield's unknown symbols.
    \item $\alpha : S \to \Subsets{\ObsVar}$ is an \emph{observation availability function}.
    \item $\mu : S \to \Distrs{\ObsVar \to \Reals}$ is an \emph{observation measurement function}.
  \end{itemize}
  In addition, the following should hold:
  \begin{enumerate}
    \item \label{item:correct-model} The shield's plant correctly models the environment's transition function: for all $s, s' \in S$, $a \in A$ and $b \in \Inst$, assuming that
    \begin{inparaenum}[\it (1)]
      \item $s' \in \Support{P(s, a)}$ (i.e $(s, a, s')$ is a valid transition),
      \item $(c \CatValuation u, \StateMapping(s) \CatValuation b) \in \sem{\Assum \land \Bound \land \Inv}$, and
      \item $(\StateMapping(s) \CatValuation b,  s'' \CatValuation b) \in \sem{\Ctrl}(c)$ where $s'' \equiv \CtrlExeSem{\CtrlWith{c, b}}(\StateMapping(s), \ActionMapping(a))$,
    \end{inparaenum} then we have $(s'', \StateMapping(s')) \in \sem{\Plant}(c \CatValuation u)$.
    \item \label{item:correct-observation} \label{} The observation measurement function behaves consistently with $\Noise$ and $\Obs$: for all $s \in S$ and $o \in \ObsVar \toI \Reals$, then $\Prob_{\eta \sim \sem{\Noise}(c)}\{ \sem{\Obs}(c \CatValuation u, \StateMapping(s) \CatValuation \eta) = o\} = \ObsMeasurementFun(s)(o)$.
  \end{enumerate}
\end{definition}

A compatible environment extends a standard RL environment with a formal mapping between this environment and a shield specification's underlying model. It also defines some ground-truth values for all unknown symbols, along with two observation functions for acquiring knowledge about those. We choose to have two separate functions, where $\alpha$ indicates what observations are available in any given state and $\mu$ performs an actual measurement of those observations. Such a separation is convenient since the inference policy is allowed to access information about observation availability but \emph{cannot} access actual observation values.

\subsection{Shielded Environments and Main Safety Theorem}\label{sec:shielded-env}

We define our main shielding algorithm by describing how a shield acts as a \emph{wrapper} around compatible environments, resulting into \emph{shielded environments} that are amenable to RL themselves and in which no unsafe state is reachable by construction.

\input{figures/shield-algo}

\begin{definition}[Shielded environment]\label{def:shielded-env}
Consider a shield specification $\tuple{\Const, \dots, \Inference}$ and a compatible environment $\tuple{\tuple{S, A, P, R, \gamma}, \dots, \ObsMeasurementFun}$ (see Definition~\ref{def:compat-env}). We define the \emph{induced shielded environment} as an MDP $\tuple{\hat S, \hat A, \hat P, \hat R, \gamma}$ where:
\begin{itemize}
  \item A \emph{state} $\hat s = (s, h, \GlobBounds, \EpsRem)$ consists of a state $s$ from the original environment, a history $h$, an instantiation of global parameters $\GlobBounds$ and a remaining safety budget $\EpsRem \ge 0$. In turn, a history is a list of $(s', z, \LocBounds)$ triples where $s'$ is a state, $z$ is an observation availability set and $\LocBounds$ is a local parameter instantiation. Formally, $\ShieldedState \equiv S \times \Hist \times (\GlobalParam \to \Reals) \times [0, 1]$ and $\Hist \equiv \List{S \times \Subsets{\ObsVar} \times (\LocalParam \to \Reals)}$.
  \item An \emph{action} $\hat a = (\CtrlAc, \InfAc)$ is defined as a pair of a \emph{control action} and of an \emph{inference action}. Formally, $\ShieldedAction \equiv \CtrlActionSem{\Ctrl} \times \InfActionSem{\Inference}$.
  \item The \emph{transition function} $P : \hat S \times \hat A \to \Distrs{\hat S}$ is defined as in Algorithm~\ref{alg:shield}.
  \item The \emph{reward function} is lifted from the original MDP: if $\hat s = (s, h, \GlobBounds, \EpsRem)$, $\hat a = (\CtrlAc, \InfAc)$ and $\hat s' = (s', h', \GlobBounds', \EpsRem')$, then $\hat R(\hat s, \hat a, \hat s') = R(s, \CtrlAc, s')$.
\end{itemize}
\end{definition}

Note that in our definition, a state of the shielded MDP does not contain any information about past observation values. Keeping such knowledge from the agent is crucial for soundness since the agent could otherwise engage in cherry-picking. Instead, the history component of a state only indicates which past observations are available and not used already.

Algorithm~\ref{alg:shield} rigorously defines the process of sampling a transition from a shielded environment, indirectly defining the behavior of our proposed adaptive shield (see Figure~\ref{fig:overview-diagram} for a synthetic view). Lines~\ref{line:inf-start}~to~\ref{line:inf-end} describe the execution of the inference module, which computes valuations $\GlobBounds$ and $\LocBounds$ for global and local bound parameters respectively while updating the history $h$ and the remaining safety budget $\EpsRem$. Lines~\ref{line:mon-start}~to~\ref{line:mon-end} instantiate the controller monitor and fallback policy using $\GlobBounds$ and $\LocBounds$. If the action proposed by the agent is rejected by the controller monitor, it is overriden via the fallback policy. Finally, line~\ref{line:perform-underlying}, executes the resulting action in the real environment.

Note that we rely on a formalization trick where Algorithm~\ref{alg:shield} performs observation measurements lazily (Line~\ref{line:lazy-measurement}), sometimes on past states, rather than performing them eagerly and storing them. Although physically unrealistic, this formalization enables us to stay within the standard MDP framework that is dominant in RL. Concrete implementations would of course perform observations eagerly and ``secretely'' cache them, offering the same interface. An alternative choice would have been to define our shielding algorithm using framework of Partially-Observed Markov Decision Processes (POMDPs)~\cite{kochenderfer2015decision}. However, doing so would complicate our proofs and definitions for little gain. Finally, instantiating the controller monitor (Line~\ref{line:mon-start}) requires \emph{every} global and local parameter to be mapped to a real value. This trivially holds for global parameters, by definition of a shielded environment state (Definition~\ref{def:shielded-env}). However, this does not obviously hold for \emph{local} parameters, hence the assertion on Line~\ref{line:assert-local}. It is the inference strategy itself that must guarantee the validity of this assertion, which it does by statically enforcing that every local parameter is provided a \emph{default} value that is independent from observations or other local parameters. In the example from Figure~\ref{fig:overview-train-local}, the first assignment $\TrainFxMax \slassign F$ serves this role. We can now state our main safety theorem, for which a proof sketch is provided in Appendix~\ref{ap:main-theorem-proof}.


\begin{theorem}[Safety of shielded environments]\label{thm:main-safety}
  Consider a shield specification $\tuple{\Const, \dots}$ and a compatible environment $\tuple{\tuple{S, \dots}, \dots, \ObsMeasurementFun}$ (see Definition~\ref{def:compat-env}). Let $\hat E$ be the resulting shielded MDP (see Definition~\ref{def:shielded-env}). Then, given a safety budget $\Eps \in [0, 1]$, an initial global bound instantiation $b \in \GlobalParam \toI \Reals$ and an initial state $s \in S$ such that $(c \CatValuation u, \StateMapping(s) \CatValuation b) \in \sem{\Assum \land \Bound \land \Inv}$, any trace in $\hat E$ starting with state $\hat s_0 = (s, \EmptyList, b, \Eps)$ is \emph{safe} with probability at least $1 - \Eps$, meaning that $(c \CatValuation u, \StateMapping(s_t)) \in \sem{\Safe}$ for all state $\hat s_t = (s_t, \,\dots)$ in the trace.
\end{theorem}

\subsection{Learning in a Shielded Environment}\label{sec:learning-in-shielded-env}

Adaptive shields such as defined in Section~\ref{sec:shielded-env} can be used to protect \emph{any} agent from acting unsafely in its environment, whether or not it is capable of learning and regardless of how it is implemented. This is emphasized by our choice of formalizing shields as \emph{environment wrappers}, which are agent-agnostic by construction. Still, for the sake of concreteness, it is useful to illustrate the use of Algorithm~\ref{alg:shield} and Theorem~\ref{thm:main-safety} in two typical reinforcement-learning scenarios:

\begin{description}
  \item[Learning in a fixed environment.] \label{learning:fixed} A shielded agent is placed in a fixed, \emph{compatible} environment. It ignores the value of \emph{unknowns} but those are kept constant across training. A total safety budget is defined for the inference module, which must be small enough to make crashes unlikely (as per Theorem~\ref{thm:main-safety}). Initially, the parameter bounds provided by the inference module are very loose and so the shield's action monitor is equally conservative. However, at each control cycle, the inference module spends some safety budget to improve those bounds, thereby allowing the agent to take increasingly aggressive exploratory moves. When the safety budget is fully spent, the inference module is not allowed to update parameter bounds anymore. However, the agent is still guaranteed to remain safe indefinitely, and in particular as it finishes to optimize its control policy. Section~\ref{sec:experiments} demonstrates this setting by showcasing a train that learns to navigate on a \emph{fixed} circuit.
  
  \item[Meta-learning across a family of environments.] \label{learning:meta} In many practical scenarios, autonomous cyber-physical systems must be capable of \emph{quickly} adapting to changing environments without training a dedicated policy from scratch. It is thus useful to train an agent across a \emph{family} of environments so that it can generalize across them. Such a \emph{meta-learning} setting integrates particularly well with our shielding framework. Indeed, we can train a shielded agent through a series of episodes, each of which takes place in a possibly different environment that is nonetheless compatible with the agent's shield (for possibly different values of the unknowns). At the start of each episode, the inference module is reinitialized and a fixed, \emph{per-episode} safety budget is granted. In addition to the state information, the agent is given access to the current bound parameters and the remaining safety budget at all times. It must learn to gather suitable information about its environment and adapt its actions accordingly \emph{within each episode}. In particular, this allows a form of \emph{active sensing}, where the agent learns to act in such a way to receive useful information from the inference module. This also allows optimizing the behavior of the inference module itself by learning \emph{inference policies}, as we discuss in Section~\ref{sec:inference-strategy-language}. In the meta-learning setting, Theorem~\ref{thm:main-safety} is applied at \emph{each} episode and so a global safety bound can be obtained by multiplying the \emph{per-episode} safety budget by the number of training episodes. Section~\ref{sec:experiments} explores this setting in three different case studies, including one with a train controller that must learn to quickly adapt to different circuits with different slope profiles.
\end{description}



\section{The Inference Strategy Language}\label{sec:inference-strategy-language}


We define the syntax and semantics of our \emph{inference strategy language} in Figure~\ref{fig:inf-lang-syntax-semantics}. An inference strategy is defined as a sequence of inference assignments. Each kind of assignment defines its own action space and the action space for a whole strategy is the product of the action spaces of individual assignments.  Three kinds of assignments are supported.
Any assignment can be attached a \textsc{when}-clause that restricts its applicability, as illustrated in Figure~\ref{fig:overview-train-global} and Appendix~\ref{ap:train-global-inf-strategy}.
A direct assignment ``$p \slassign \theta$\,'' does not require any input from the inference policy and executing it costs no budget. It yields a single symbolic bound instantiation, which is $\theta$ itself. An assignment of the form ``$p \slassign \slbest i_1, \cdots, i_n : \theta$\,'' expects as an input a list of $n$-tuples of history indices and yields one bound instantiation $\FSubst{\theta}{i_1, \cdots, i_n\,}{\,j_1, \cdots, j_n}$ for each tuple $(j_1, \cdots, j_n)$. An alternate design would expect no input and consider \emph{every} possible combination of indices through the current history. However, the number of such combinations may grow intractably large, which explains our current pragmatic choice. %

\paragraph{Semantics of \textsc{aggregate}}
An assignment of the form ``$p \slassign \slaggregate i_1, \cdots, i_n : \theta_1 \sland \theta_2$'' takes as an input an $(\Eps, D)$ pair, where $\Eps$ indicates how much safety budget should be spent and $D$ is a finite distribution over all $n$-tuples of history indices. More precisely, $D$ is a sequence of $(\lambda, j)$ pairs where $j$ indexes a combination of measurements from history and $\lambda$ is a positive weight attached to it ($\sum_{\lambda, j \in D} \lambda = 1$). It yields a single bound instantiation that performs a weighted average over all instances of the \emph{observable bound component} $\theta_1$, while using the $\InvCCDF$ operator to handle the \emph{noise component} $\theta_2$. As a requirement, $\theta_1$ can contain observation variables but no noise variables, while $\theta_2$ can contain noise variables but no observation variables. The semantics of \textsc{aggregate} generalizes our reasoning from Section~\ref{sec:overview-inference} (see Equation~\ref{eq:overview-train-local-aggr-proof}). Indeed, let us assume that
$\BoundWith{p}{\FSubst{(\theta_1 + \theta_2)}{i}{j}}$ is true for any instantiation $j \equiv (j_1, \dots, j_n)$ of $i \equiv (i_1, \dots, i_n)$, as guaranteed by the generated proof obligation. Also, let us assume without loss of generality that $p$ is an \emph{upper}-bound. Since $\Bound_p$ is monotone in $p$ and thus convex, we know that $\BoundWith{p}{b^*}$ is true where
$ b^* \equiv \sum_{\lambda, j \in D} \lambda \FSubst{(\theta_1 + \theta_2)}{i}{j}. $
However, $b^*$ is not known at runtime since $\theta_2$ features noise variables that are not observed directly. Instead, our semantics yields instantiation $b \equiv \sum_{\lambda,j \in D} \lambda  \FSubst{\theta_1}{i}{j} + \delta$, where $\delta \equiv \InvCCDF(\sum_{\lambda, j \in D} \lambda \FSubst{\theta_2}{i}{j},\,\Eps).$
Since $\Bound_p$ is monotone, we have $b \ge b^* \limplyI \BoundWith{p}{b}.$ Contraposing, we get $\neg \BoundWith{p}{b} \limplyI b^* > b$. We can then establish the soundness of $(p, b, \Eps)$ with the exact same reasoning that was demonstrated in Equation~\ref{eq:overview-train-local-aggr-proof}:
\begin{equation*}
  \Prob(\neg \BoundWith{p}{b}) \,\le\, \Prob(b^* > b)
    \,=\, \Prob \Bigl( \,\sum_{i,\lambda \in D} \lambda \cdot \FSubst{\theta_2}{i}{j} \,>\, \delta\, \Bigl)
    \,\le\, \Eps.
\end{equation*}
Details on how to evaluate or approximate the $\InvCCDF$ operator are available in Appendix~\ref{sec:eval-tail-expressions}. In the particular case where $\theta_2$ is a linear combinations of Gaussian variables (with possibly state-dependent coefficients), closed-form solutions exist that involve the $\ErfInv$ function. Concentration inequalities (e.g. Chebyshev, Hoeffding, or Chernoff bounds) can be used to approximate it conservatively in the general case~\cite{bertsekas2008introduction}.

\input{figures/inference-syntax-semantics}

\paragraph{Proof obligations and soundness}
An inference strategy $\Inference$ induces a proof obligation in the form of a $\dL$ formula, $\ProofSem{\Inference}$, which is defined inductively in Figure~\ref{fig:inf-lang-obligations}. Provided that this obligation is valid, our language guarantees the soundness of all resulting inferences. This is expressed in Theorem~\ref{thm:inf-soundness}, for which a proof is provided in Appendix~\ref{ap:inf-lang-soundness-proof}.


\input{figures/inference-obligations}

\begin{theorem}[Soundness of the inference strategy language]\label{thm:inf-soundness}
  Let $\tuple{\Const,\dots,\Inference}$ a shield specification. If the formula $\ProofSem{\Inference}$ is valid, then $\sem{\Inference}(a)$ is a list of \emph{sound} inference assignments for any inference action $a \in \InfActionSem{\Inference}$.
\end{theorem}


\paragraph{Learning inference policies} A crucial feature of our inference strategy language is its nondeterminism: an inference strategy does not fully specify the behavior of the inference module and key decisions about what measurements to aggregate, how much budget to spend and when are left to a separate \emph{inference policy}. As illustrated in Section~\ref{sec:overview-inference}, such a policy must make subtle tradeoffs. However, it is not soundness-critical and amenable to learning. Learning an inference policy is possible within the \emph{meta-learning} setting described in Section~\ref{sec:learning-in-shielded-env}. A \emph{per-episode} safety-budget is fixed and the agent must learn a way to best manage this budget within an episode, in a way that generalizes across a family of environments and allows maximizing returns. Section~\ref{sec:experiments} illustrates this idea in three different case studies.

\section{Experimental Evaluation}\label{sec:experiments}

We evaluate our framework on a series of case studies and demonstrate the claims:

\begin{enumerate}[label=C\arabic*]
  \item \label{claim:modelling-flexibility} Our framework allows expressing diverse types of model uncertainty in a unified way.
  \item \label{claim:shield-safe} Shielded agents always remain safe.
  \item \label{claim:adaptive-shield-efficient} Adaptive shields allow more aggressive control strategies than non-adaptive shields.
  \item \label{claim:inference-learned} Inference policies can be learned that generalize across settings.
  \item \label{claim:control-info-learned} Control policies can be learned that actively seek information as a prerequisite for success.
\end{enumerate}

All case studies are summarized in Table~\ref{tab:experiments}. Each case study trains a shielded reinforcement learning agent, either in a fixed environment or using the meta-learning setting described in Section~\ref{sec:learning-in-shielded-env}. We demonstrate the qualitative claim~\ref{claim:modelling-flexibility} by having each case study illustrate distinct modelling concepts, all of which are listed in Table~\ref{tab:experiments}. Claims~\ref{claim:shield-safe} and \ref{claim:adaptive-shield-efficient} are evaluated on all case studies. As shown in Table~\ref{tab:crash-stats-short} and validating Theorem~\ref{thm:main-safety}, shielded agents never encounter crashes. Unshielded agents reach unsafe states during training, but also \emph{after} training in all but one environment. Furthermore, disabling the inference module leads to inferior policies in all examples. Claims~\ref{claim:inference-learned}~and~\ref{claim:control-info-learned} are relevant in the \emph{meta-learning} setting, which is explored in three case studies whose results we summarize below. Full details on our experimental protocol, results, and on the shield used for each case study are available in Appendix~\ref{ap:experiments}.

\paragraph{Versatile Train.} This case study uses the shield from Figure~\ref{fig:overview-train-local}, which is inspired by published models of train control systems~\cite{platzer2009european,kabra2022verified}, with added support for runtime track slope estimation. Our key results are summarized in Figures~\ref{fig:example2-1}~and~\ref{fig:example2-2}. We show how different inference policies work best for different kinds of train tracks and noise models. A policy that aggregates observations in \emph{small} batches works better in a setting with irregular tracks (large $k$) and low sensor noise (small $\sigma$), while a policy that aggregates observations in \emph{large} batches works better in settings with regular tracks and high sensor noise. Importantly, inference policies can be learned that are at least competitive with the best hardcoded solution in both cases. 

\paragraph{Crossing the River.} This case study illustrates the concept of \emph{active sensing} and demonstrates claim~\ref{claim:control-info-learned} in a minimal setting. The goal is for a robot equipped with a lamp to cross a bridge at night. The robot must find the position of the bridge, which translates into a model where the unknown is in the {safety property} instead of the plant. The robot only observes the bridge when it gets close enough and its lamp is on. In addition to an inference policy, it must learn a control policy that seeks knowledge \emph{explicitly}. We illustrate this learned control policy in Figure~\ref{fig:example3-2}.

\paragraph{Revisiting ACASX} This case study revisits a classic of cyberphysical-system verification: the next-generation Airborne Collision Avoidance System (ACAS X)~\cite{jeannin2015formally}. However, rather than assuming a \emph{known} intruder trajectory or a \emph{random-walk} intruder model~\cite{kochenderfer2012next}, we propose a new uncertainty model where intruders are either ACAS-compliant (in which case they are also actively trying to avoid collision) or not (in which case they must be treated as adversarial). The agent must attempt to infer the compliance of the intruder so as to avoid drastic maneuvers whenever possible. We illustrate the effectiveness of our learned inference policy in Figure~\ref{fig:example4-2}.


\input{figures/experiments-table}


\begin{table}
  \small
  \caption{Return at testing time (average value of the last 100 episodes during 10,000 evaluation steps) and number of crashes during training and testing time, for different case studies and under different settings. The number of training steps and the maximum episode length for each setting are 80,000/100; 400,000/100; 400,000/100; 400,000/50; and 400,000/40 respectively. Agents are given a total safety budget of $10^{-3}$ in \emph{fixed-environment} settings (\textsc{Sisyphean Train}) and $10^{-7}$ per episode in \emph{meta-learning} settings (all others).}
  \label{tab:crash-stats-short}
  \begin{tabular}{lcccc}
      \toprule
      Setting & Method & Return & Crashes during training & Crashes during testing\\
      \midrule
      \primitiveinput{img/sisyphean_train/sisyphean_train_short.tex}
      \midrule
      \primitiveinput{img/versatile_train/versatile_train_ksigma_large_short.tex}
      \midrule
      \primitiveinput{img/versatile_train/versatile_train_ksigma_small_short.tex}
      \midrule
      \primitiveinput{img/crossing_the_river/crossing_the_river_short.tex}
      \midrule
      \primitiveinput{img/revisiting_ACAS_X/revisiting_ACAS_X_short.tex}
      \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
  \vspace{0.2cm}
  \includegraphics{img/versatile_train/versatile_train_ksigma_large_returns.pdf}
  \hskip 15pt
  \includegraphics{img/versatile_train/versatile_train_ksigma_small_returns.pdf}
  ~\\
  \includegraphics[trim=0 0 0 150,clip]{img/versatile_train/versatile_train_returns_legend.pdf}
  \caption{Returns for the \textsc{versatile train} case study, for two different combinations of $k$ and $\sigma$ (and averaged across three random seeds, with standard deviations represented as shaded areas around the mean). The train gets rewarded for reaching the station as quickly as possible (a fixed negative reward is issued at every step before the train reaches destination) and penalized for (unsafely) overshooting its target. Training returns are shown for an unshielded agent (red), an agent using our proposed adaptive shield (green), and a nonadaptive variant where the inference module is deactivated (orange). In addition, the \emph{Aggregate-$i$} agent is a variant of the \emph{Adaptive} agent that uses a hardcoded inference policy that spends a fixed budget to aggregate all available observations every $i$ steps. Different hardcoded inference policies perform best in different settings but the learned inference policy manages to match the best one in both cases.}
  \label{fig:example2-1}
  \Description[]{}
\end{figure}

\begin{figure}
  \includegraphics{img/versatile_train/policy_ksigma_large_1.pdf}
  \hskip 15pt
  \includegraphics{img/versatile_train/policy_ksigma_small_2.pdf}
  \caption{A visualization of the learned control and inference policies for the \textsc{versatile train} case study, on two random tracks and for two different combinations of $k$ and $\sigma$. We show the position of the train over time using dots. The train needs to stop before $x=0$ (bold black line). At each time step, a vertical segment indicates the estimated braking distance assuming that the train decides to accelerate for this time step. The segment is drawn in red if the train effectively decides to accelerate and in blue if it decides to brake. A green circle is shown whenever the inference module aggregates existing measurements to improve the local slope estimate. The resulting reduction in the shield's estimated braking distance is plotted as a green segment. In particular, we observe as expected that the learned inference policy aggregates measurements in bigger batches when $k/\sigma$ is small.}
  \label{fig:example2-2}
  \Description[]{}
\end{figure}

\begin{figure}
  \includegraphics{img/crossing_the_river/policy_1.pdf}
  \hskip 15pt
  \includegraphics{img/crossing_the_river/policy_2.pdf}
  \caption{Visualization of the learned policy for the \textsc{crossing the river} case study. The river is drawn as a vertical blue line at $x=0$ and the bridge is drawn in brown. The range within which the agent can observe the bridge is plotted as a bold, black dashed circle. The agent's position is marked by a point for each control cycle, which is blue if the lamp is off and red if it is on. A green circle around the current position indicates some safety budget being spent. Regardless of its starting position, the agent learns to go near the river, activate the lamp when it gets close enough and then move along the river until it observes the bridge.}
  \label{fig:example3-2}
  \Description[]{}
\end{figure}

\begin{figure}
  \includegraphics{img/revisiting_ACAS_X/policy_1.pdf}
  \hskip 15pt
  \includegraphics{img/revisiting_ACAS_X/policy_0.pdf}
  \caption{Learned policy for the \textsc{Revisiting ACAS X} case study, in two different scenarios. The scenario on the right features an ACAS-compliant intruder while the scenario on the left involves a non-compliant intruder. In both cases, the ownship's trajectory is shown on the left of the plot while the intruder's trajectory is shown on the right. The estimated upper and lower bounds for the intruder's final altitude at meeting time is shown for each time step as a dashed line. Red dots on the ownship's trajectory indicate the observation of information that is relevant to estimating the intruder's compliance. Green dots indicate that the intruder has been identified as compliant. When this happens, a less aggressive avoidance maneuver can be executed.}
  \label{fig:example4-2}
  \Description[]{}
\end{figure}

\section{Discussion}\label{sec:discussion}

\paragraph{Runtime overhead} Our framework pushes most of the burden of safety analysis offline via static proof obligations. Thus, it benefits from a small and predictable runtime overhead. This overhead results from running the inference module, monitoring the proposed control actions and executing the fallback policy when needed. The complexity of running a cycle of the inference module is linear in the size of the specification and in the size of the inference action being considered (e.g. the number of measurements being aggregated), assuming that the $\InvCCDF$ operator can be evaluated in linear time (which is true in particular in the Gaussian case or when using concentration inequalities, see Appendix~\ref{sec:eval-tail-expressions}). Running the controller monitor is inexpensive since it consists in evaluating a formula whose size does not exceed the size of the shield itself (see Appendix~\ref{ap:proof-obligations}). Finally, assuming that an explicit fallback policy is provided (see discussion in Section~\ref{sec:safe-rl-via-jsc}), the cost of computing fallback actions is small and predictable. In our experiments, the total overhead of shielding during training never exceeds 15\% (see Table~\ref{tab:overhead-stats}).

\paragraph{On the complexity of designing adaptive shields} There is an inherent complexity to the engineering task of designing adaptive monitors for rich, realistic model families, which typically necessitates the exploitation of domain-specific insights. Our framework allows designers to focus on this essential complexity, while eliminating the accidental complexity of enforcing end-to-end soundness guarantees that encompass statistical inference. For example, while our paper shows how tricky and error-prone statistical reasoning can be (e.g. reusing measurements is sound within an inference cycle but not across cycles), our framework exposes abstractions that fully protect users from such considerations and generates $\dL$ proof obligations that require no probabilistic reasoning.
Still, specifying formal models of hybrid systems and proving properties about them using interactive theorem proving is a nontrivial skill to acquire. Doing so is eased by the increasing availability of automation in tools such as KeYmaera X~\cite{DBLP:conf/cade/FultonMQVP15,DBLP:journals/fmsd/SogokonMTCP22}. We show the proof obligations generated by our tool for all case studies in Appendix~\ref{ap:proof-obligations}. Out of 32 obligations, 27 can be discharged fully automatically or with trivial human guidance (such as providing a single term for instantiating an existential variable). The others require more effort but can be proved in under an hour by an experienced KeYmaera X user. We provide an informal proof of the most challenging one (the proof of the train shield's invariant from Figure~\ref{fig:overview-train-local}) in Appendix~\ref{ap:train-invariant-proof}.


\section{Related Work}


\emph{Safe reinforcement learning} is widely studied~\cite{DBLP:journals/corr/abs-2205-10330,DBLP:journals/jmlr/GarciaF15,brunke2022safe}, including several approaches without formal verification. Several policy-search algorithms have been proposed that take into account safety constraints specified separately from the reward signal, but do not offer any guarantee that those constraints will not be violated during training or deployment~\cite{DBLP:conf/icml/AchiamHTA17,DBLP:conf/aaai/YangSTS21,DBLP:conf/iclr/TesslerMM19}. In contrast, this work is part of a family of approaches that are often called \emph{sandboxing} or \emph{shielding-based}~\cite{DBLP:conf/aaai/AlshiekhBEKNT18,DBLP:conf/aaai/FultonP18,DBLP:conf/icra/ThummA22}, and where the intended actions of an RL agent are monitored at runtime and overridden by appropriate fallbacks whenever they cannot be proved safe with respect to a model of the environment.

Virtually all existing approaches to shielding aim for full-automation of the shield computation, imposing hard tradeoffs in terms of safety, adaptivity, precision, expressivity and scalability. In circumstances where they run fast enough to be used online, methods based on reachability analysis~\cite{DBLP:conf/icra/ThummA22,DBLP:conf/cdc/KollerBT018,ivanov2019verisig} are naturally amenable to a form of adaptivity. However, they only offer bounded-horizon guarantees and precision comes at the expense of runtime efficiency. Methods based on LTL model-checking~\cite{DBLP:conf/aaai/AlshiekhBEKNT18,DBLP:conf/isola/KonighoferL0B20} or finite-MDP solving~\cite{pranger2021adaptive} can handle infinite time-horizons but require discretization of hybrid dynamics, often at a significant cost in terms of precision and safety. They also tend to scale poorly with increasing dimensionality. Methods based on Hamilton-Jacobi solving~\cite{DBLP:journals/tac/FisacAZKGT19} face similar challenges. In contrast, our proposed framework allows leveraging human ingenuity by extracting shields from nondeterministic, symbolic controllers that are proved safe using interactive theorem proving. It builds on the \emph{Justified Speculative Control} (JSC) framework~\cite{DBLP:conf/aaai/FultonP18}, which it generalizes in a crucial way to support adaptivity.

Our framework offers a uniquely expressive language for modelling environment uncertainty via arbitrarily constrained function symbols. For example, reachability analysis typically uses {bounded disturbance terms}~\cite{althoff2014online} to model environment uncertainty, which is sufficient to model the example from~Figure~\ref{fig:overview-train-global} but not those from Figures~\ref{fig:overview-train-local}~and~\ref{fig:acas}. To the best of our knowledge, these last two examples cannot be accommodated by any pre-existing approach. Another standard way of representing model uncertainty for the purpose of adaptive shielding is to use \emph{Gaussian processes} to model an unknown, state-dependent term added to the system's dynamics~\cite{DBLP:conf/aaai/ChengOMB19,DBLP:conf/eucc/BerkenkampS15,DBLP:conf/nips/BerkenkampTS017,DBLP:journals/tac/FisacAZKGT19}. This approach offers a different form of modelling flexibility, where assumptions about functional unknowns are implicitly encoded into prior kernels rather than hard logical constraints. However, this also makes the resulting safety guarantees harder to interpret and fundamentally dependent on assumptions that are nearly impossible to validate experimentally.

Another approach has been proposed to extend the JSC framework with a form of adaptivity, in which an agent starts with a finite set of plausible models and then progressively discards those that are found inconsistent with observations~\cite{DBLP:conf/tacas/FultonP19}. Our framework handles a more general form of parametric model uncertainty and additionally supports noisy observations and statistical reasoning. Finally, our idea of having experts define nondeterministic inference strategies that are sound by construction and refined via learning -- thereby making shielded agents in charge of their own safety budget -- has, to the best of our knowledge, no equivalent in the literature.

\section{Conclusion}

Our framework gives experts full access to the power of differential dynamic logic to build adaptive shields for hybrid systems. Its unique flexibility raises the equally unique challenge of performing statistical inference soundly and efficiently. We tackle this challenge with a mix of language design and learning, introducing the concepts of an \emph{inference strategy} and of an \emph{inference policy} respectively. Future work may explore the integration of reachability analysis and model-checking within our framework, allowing hybrid combinations of symbolic and numerical, offline and online proving.






\section*{Data-Availability Statement}
An artifact that contains our framework implementation, instructions to replicate all experiments, and formal proofs for all case studies is available at \url{https://doi.org/10.5281/zenodo.14916164}.

\begin{acks}
  This work was supported by the NSFC (Nos. 62276149, 92370124, 62350080, 92248303, U2341228, 62061136001), BNRist (BNR2022RC01006), Tsinghua Institute for Guo Qiang, and the High Performance Computing Center, Tsinghua University. J. Zhu was also supported by the XPlorer Prize. Finally, this work was supported by the Alexander von Humboldt Professorship program. We thank the anonymous reviewers for their helpful feedback.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


\appendix

\section{Experimental Case Studies}\label{ap:experiments}

We implemented a compiler for our shield specification language, which is used in all our experiments and made available as part of our artifact.

\input{experiments}

\input{obligations}


\section{Deriving controller monitors and fallback policies}\label{ap:ctrl-monitor}

\subsection{Deriving Control Action Spaces}
\label{ap:defining-action-ctrl}
We formally demonstrate how nondeterministic $\dL$ controllers induce an action space $\CtrlActionSem{\Ctrl}$ and define $\CtrlExeSem{\Ctrl}(s, a)$ in Figure~\ref{fig:ctrl-fun-sem}. To provide a more advanced example that uses those definitions, let us consider the following controller:
\[
\Ctrl \ \equiv \ (x := * \,;\, y := * \,;\, ?x \ge y) \,\cup\, (x := 0 \,;\, ((y := * \,;\, ?y \ge 0) \,\cup\, y := -1)).
\]

Then, we have $\CtrlActionSem{\Ctrl} \equiv (\Reals \times \Reals \times \UnitSet) \uplus (\UnitSet \times ((\Reals \times \UnitSet) \uplus \UnitSet)) \!\IsomorphTo \Reals^2 \uplus \Reals \uplus \UnitSet$. Moreover, consider action $a = \SumRightInj{(\Unit, \SumLeftInj{(8, \Unit)})}$, which encodes the path: ``take the right branch, do the assignment, take the left branch, pick value $8$ for $y$ and do the test''. Then, we have: \(\CtrlExeSem{\Ctrl}((3, 7), a) = (0, 8). \)

\begin{figure}
  \begin{minipage}{0.4\textwidth}
  \begin{align*}
      \CtrlActionSem{\alpha \cup \beta} &\ \equiv \ \CtrlActionSem{\alpha} \uplus \CtrlActionSem{\beta} \\
      \CtrlActionSem{\alpha ; \beta} &\ \equiv \ \CtrlActionSem{\alpha} \times \CtrlActionSem{\beta} \\
      \CtrlActionSem{x := *} &\ \equiv \ \Reals \\
      \CtrlActionSem{x := e} &\ \equiv \ \UnitSet \\
      \CtrlActionSem{?Q} &\ \equiv \ \UnitSet \\
  \end{align*}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
  \begin{align*}
      \CtrlExeSem{\alpha \cup \beta}(s, \SumLeftInj{a}) & \ \equiv \ \CtrlExeSem{\alpha}(s, a) \\
      \CtrlExeSem{\alpha \cup \beta}(s, \SumRightInj{a}) & \ \equiv \ \CtrlExeSem{\beta}(s, a) \\
      \CtrlExeSem{\alpha ; \beta}(s, (a_1, a_2)) & \ \equiv \ \CtrlExeSem{\beta}(\CtrlExeSem{\alpha}(s, a_1), a_2) \\
      \CtrlExeSem{x \dlassign *}(s, v) & \ \equiv \ \UpdateMap{s}{x}{v} \\
      \CtrlExeSem{x \dlassign e}(s, \Unit) & \ \equiv \ \UpdateMap{s}{x}{\sem{e}(\EmptyValuation, s)} \\
      \CtrlExeSem{?Q}(s, \Unit) & \ \equiv \ s
  \end{align*}
  \end{minipage}
  \caption{A formal definition of $\CtrlActionSem{\Ctrl}$ and $\CtrlExeSem{\Ctrl}$, as introduced in Section~\ref{sec:safe-rl-via-jsc}. Note that these definitions do not feature loops and differential equations, as these are not allowed to occur in controllers.}
  \Description[]{}
  \label{fig:ctrl-fun-sem}
\end{figure}

\subsection{Deriving Controller Monitors}
\label{ap:extracting-cm}
Figure~\ref{fig:deriving-cm} provides more details on how controller monitors can be automatically extracted from nondeterministic controllers. For pedagogical reasons, we take a slightly different approach here than is used in the original ModelPlex paper~\cite{DBLP:journals/fmsd/MitschP16}, but both approaches are similar in spirit. Note that both $\CtrlMonitorSem{\alpha}$ and $\CtrlExeSem{\alpha}$ are computable whenever $\alpha$ is a hybrid program that is free of function symbols, loops, differential equations, modalities, and quantifiers.

\begin{figure}
  \begin{align*}
  \CtrlMonitorSem{\alpha \cup \beta}(s, \SumLeftInj{a}) & \ \equiv \ \CtrlMonitorSem{\alpha}(s, a) \\
  \CtrlMonitorSem{\alpha \cup \beta}(s, \SumRightInj{a}) & \ \equiv \ \CtrlMonitorSem{\beta}(s, a) \\
  \CtrlMonitorSem{\alpha \seqI \beta}(s, (a_1, a_2)) & \ \equiv \ \BoolAnd{\CtrlMonitorSem{\alpha}(s, a_1)}{\CtrlMonitorSem{\beta}(\CtrlExeSem{\alpha}(s), a_2)} \\
  \CtrlMonitorSem{x \dlassign *}(s, \Unit) & \ \equiv \ \BoolTrue \\
  \CtrlMonitorSem{x \dlassign e}(s, \Unit) & \ \equiv \ \BoolTrue \\
  \CtrlMonitorSem{?Q}(s, \Unit) & \ \equiv \ (\EmptyValuation, s) \in \sem{Q}
  \end{align*}
  \caption{Extracting a controller monitor from a nondeterministic $\dL$ controller.}
  \Description{The decomposition of controller monitors.}
  \label{fig:deriving-cm}
\end{figure}

\subsection{Existence of a Computable Fallback Policy}\label{ap:generating-fallbacks}
Our shielding algorithm requires a fallback policy $\CtrlFallbackSem{\Ctrl}$ to pick actions whenever the desired action is rejected by the controller monitor. An explicit fallback policy is typically provided by users on a case-by-case basis (see discussion in Section~\ref{sec:safe-rl-via-jsc}), but fallback actions can also be generated systematically and automatically with the help of an SMT solver. We demonstrate this point by studying an example with all features of the general case. Indeed, consider the following controller:
\[
  \alpha \ \equiv \ ((x \dlassign * \,;\, y \dlassign v \cdot x) \,\cup\, (y \dlassign w)) \seq ?(y \ge 1).
\]
Here, the action space is $\CtrlActionSem{\alpha} \equiv ((\Reals \times \UnitSet) \uplus \UnitSet) \times \UnitSet$. In order to find an action, one can consider the following modified controller where a vector of variables $\VarsVec{u}$ is used to encode nondeterministic choices:
\[
  \alpha_{\VarsVec{u}} \ \equiv \ ((?u_1 = 0 \seq x \dlassign u_2 \seq y \dlassign v \cdot x) \,\cup\, (?u_1=1 \seq y \dlassign w)) \seq\, ?(y \ge 1).
\]
In the general case, any occurrence of $x \dlassign *$ can be replaced by $x  \dlassign u$ with $u$ a fresh variable, and any occurrence of $\alpha \cup \beta$ can be replaced by $(?(u=0) \seq \alpha) \cup (?(u=1) \seq \beta)$ with $u$ a fresh variable also. Then, one can consider the formula $F_{\VarsVec{u}} \equiv \ldiamond{\alpha_{\VarsVec{u}}}{\text{true}}$. Using the axioms of $\dL$, we can rewrite $F_\VarsVec{u}$ into an equivalent, quantifier-free real arithmetic formula $G_{\VarsVec{u}}$~\cite{DBLP:conf/hybrid/Platzer07}. In our example, we get:
\[
  G_{u_1, u_2} \equiv (u_1 = 0 \land v \cdot u_2 \ge 1) \lor (u_1 = 1 \land w \ge 1).
\]
Finally, given a particular state $s$, one can replace any variable not in $\VarsVec{u}$ by its value in $s$ and then generate an action by computing a model of the resulting formula using an SMT solver. In this particular case, assuming that $s(v) = 2$ and $s(w) = 0$, we seek models of the following formula:
\[
  (u_1 = 0 \land 2 \cdot u_2 \ge 1) \lor (u_1 = 1 \land 0 \ge 1).
\]
A valuation for $\VarsVec{u}$ can be naturally mapped into an action. For example, the model $\{u_1 \mapsto 0, u_2 \mapsto 4 \}$ of the formula above is mapped to action $(\SumLeftInj{(4, \Unit)}, \Unit)$, which can be interpreted as ``take the left branch and choose $x := 4$''.




\section{Details on the Inference Strategy Language}\label{ap:inf-details}

\subsection{Analysis of the Strategy from Figure~\ref{fig:overview-train-global}}\label{ap:train-global-inf-strategy}
In this section, we analyze the inference strategy of the shield defined in Figure~\ref{fig:overview-train-global}. The first line is:
\[ \TrainThetaSlopeMin, \TrainThetaSlopeMax \slassign \slaggregate i, j: (\omega_j - \omega_i)/(u_j - u_i) \sland (\eta_j - \eta_i)/(u_j - u_i) \slwhen u_j > u_i. \]
First, note that $\TrainThetaSlopeMin, \TrainThetaSlopeMax \slassign e$ is simply a syntactic sugar for $\TrainThetaSlopeMin \slassign e \seq \TrainThetaSlopeMax \slassign e$, as defined in Figure~\ref{fig:inf-lang-syntax-semantics}. Let us thus focus on the second assignment. The corresponding proof obligation is:
\[ (\omega_i = \TrainThetaSlope u_i + \TrainThetaBias - \eta_i) \land (\omega_j = \TrainThetaSlope u_j + \TrainThetaBias - \eta_j) \land (u_j > u_i) \limplyI \TrainThetaSlope \le (\omega_j - \omega_i)/(u_j - u_i) + (\eta_j - \eta_i)/(u_j - u_i),  \]
where irrelevant assumptions have been elided. As can be seen, the guard specified by the $\textsc{when}$-clause appears as a crucial assumption. During evaluation, the inference statement fails when a positive weight is put on a pair of steps where $u_j > u_i$ does not hold (and thus $\OptNone$ is returned). The second inference assignment from Figure~\ref{fig:overview-train-global} is:
\[ \TrainThetaBiasMax \slassign \slaggregate i: \omega_i - \TrainThetaSlopeMax u_i \sland \eta_i \slwhen u_i \le 0. \]
Here, the associated proof obligation is:
\[ (\omega_i = \TrainThetaSlope u_i + \TrainThetaBias - \eta_i) \land (\TrainThetaSlopeMax \ge \TrainThetaSlope) \land (u_i \le \omega_i - \TrainThetaSlopeMax u_i + \eta_i) \limply (\TrainThetaBias \le 0). \]
Note how the definition of $\TrainThetaSlopeMax$ (i.e. $\TrainThetaSlopeMax \ge \TrainThetaSlope$) is included as an assumption since it appears in the \textsc{aggregate} expression. This behavior is formalized in the definition of $\VarDef{\cdot}$ in Figure~\ref{fig:inf-lang-obligations}.


\subsection{Evaluating Probabilistic Tail Bounds}\label{sec:eval-tail-expressions}

In this Appendix, we provide more details on how the $\InvCCDF$ operator used by the inference strategy language can be implemented. Note however that optimizing this operator in the general case is orthogonal to our main contributions and goes beyond the scope of this paper.

\paragraph{Gaussian case.} When the noise component of an \textsc{aggregate} statement is a linear combination of Gaussian variables, the resulting $\InvCCDF$ expression can be evaluated optimally. Note that the coefficients and Gaussian hyperparameters can be state-dependent since all state variables can be substituted by their concrete values before $\InvCCDF$ is evaluated. The following properties make the handling of Gaussian noise particularly straightforward:
\begin{itemize}
  \item If $X_i \sim \slnormal(\mu_i, \sigma_i^2)$ are independent and $\lambda_i \in \Reals$, then $\sum \lambda_i X_i \sim \slnormal\bigl(\sum_i \lambda_i X_i, \, \sum_i \lambda_i^2 \sigma_i^2\bigr)$.
  \item $\Prob_{X \sim \slnormal(\mu, \sigma^2)} \{ X > \delta_\Eps \} = \Eps$ where $\delta_\Eps \equiv \mu + \sqrt{2} \cdot \sigma \cdot \ErfInv(1 - 2\Eps)$ and $\ErfInv$ is the inverse of the error function defined by $\Erf(z) \equiv (2/\sqrt{\pi}) \int_0^z e^{-t^2}dt$ for $z \in \Reals$~\cite{bertsekas2008introduction}.
\end{itemize}

\paragraph{Discrete case} Discrete distributions that are expressed using a finite combination of Bernouilli variables can be generally handled by computing the associated, finite probability table. Also:
\begin{itemize}
  \item If $X \sim \slbernouilli(p)$, then $\InvCCDF(X, \Eps) = 0$ if $\Eps \ge p$ and $1$ otherwise.
  \item If $X_i \sim \slbernouilli(p_i)$ i.i.d, then $\InvCCDF((1/n)\sum_{i=1}^n X_i, \Eps) = 0$ if and only of $\Eps \ge \Prob(\forall i \ X_i = 0) = 1 - \prod_i(1-p_i)$.
\end{itemize}

\paragraph{Use of concentration inequalities} Concentration inequalities (e.g. Chebyshev, Cantelli, Hoeffding, or Chernoff bounds) can be used to implement the $\InvCCDF$ operator for arbitrary distributions, using simple features such as their mean, variance, support or moment-generating function~\cite{bertsekas2008introduction}. Such features benefit from strong compositional properties\footnote{For example, $\Expectation(X + Y) = \Expectation(X) + \Expectation(Y)$ and $\Variance(X + Y) = \Variance(X) + \Variance(Y)$ for $X$ and $Y$ independent random variables.}, making them easy to estimate in a large variety of cases. The simplest concentration bound is the Chebyshev inequality. It states that for any distribution $X$ with mean $\mu$ and variance $\sigma^2$ and for any $k > 0$, we have: \[\Prob(|X-\mu| > k \sigma) \le \frac{1}{k^2}. \]
We can therefore define $\InvCCDF(X, \Eps) = \mu + \sigma / \sqrt{\Eps}$. In particular, in a typical case where we have independent variables $X_1,\dots,X_n$ with mean $0$ and variance $\sigma^2$ along with some convex coefficients $\lambda_1,\dots,\lambda_n$,
this yields $\InvCCDF\bigl(\sum_i \lambda_i X_i, \, \Eps\bigr) = \SqrtI{\sum_i \lambda_i^2} \cdot \sigma \cdot \bigl(1/\sqrt{\Eps}\bigr)$.

The Hoeffding inequality often provides tighter bounds when aggregating independent noise variables with bounded support. It states that if $X_1, \dots, X_n$ are independent and that $a_i \le X_i \le b_i$ and $\Expectation(X_i) = \mu_i$ for all $i$, then the following holds for all $t > 0$:
\[ \Prob((X_1+\dots+X_n) - (\mu_1 + \dots + \mu_n) > t) \le \exp\biggl(-\frac{2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\biggr). \] In the particular case where $X_1, \dots, X_n$ are independent with mean $0$, $a \le X_i \le b$ for all $i$ and $\lambda_1,\dots\lambda_n$ are convex coefficients, the following holds for all $t>0$: \[\Prob\biggl(\sum_i\lambda_iX_i > t\biggr) \le \exp\biggl(-\frac{2t^2}{(b-a)^2 \cdot \sum_i \lambda_i^2}\biggr).\] Thus, in this case, we can define $\InvCCDF\bigl(\sum_i \lambda_i X_i, \, \Eps\bigr) = \SqrtI{\sum_i \lambda_i^2} \cdot (b-a) \cdot \ln \bigl(1/\sqrt{\Eps}\bigr)$. This bound is exponentially better than the Chebyshev bound in terms of $\Eps$, as it features a factor of $\ln \bigl(1/\sqrt{\Eps}\bigr)$ instead of $1/\sqrt{\Eps}$. However, both bounds improve at the same rate of $1/\sqrt{n}$ as $n$ gets larger, assuming a uniform distribution $\lambda_i=1/n$ of the convex coefficients.




\section{Proofs}

\subsection{Main Safety Theorem}\label{ap:main-theorem-proof}

This section provides a proof sketch for Theorem~\ref{thm:main-safety}. Throughout this section, we fix a shield specification $\tuple{\Const, \dots}$, a compatible environment $\tuple{\tuple{S, \dots}, \dots, c, u, \ObsAvailFun, \ObsMeasurementFun}$ and the associated shielded MDP $\hat E = \tuple{\hat S, \hat A, \hat P,\dots}$. Our proof uses the notions of a \emph{consistent history} and of a \emph{consistent shielded state}.

\begin{definition}[Consistent history]
  A history $h \in \Hist$ is said to be \emph{consistent} if for all $(s, z, \LocBounds) \in h$, we have $(c \CatValuation u, \StateMapping(s)) \in \sem{\BoundConj{\LocBounds}}$ and $z \subseteq \ObsAvailFun(s)$.
\end{definition}

\begin{definition}[Consistent shielded state]
  A state $\hat s=(s,h,\GlobBounds,\EpsRem) \in \hat S$ is said to be \emph{consistent} if and only if  $h$ is consistent, $\EpsRem \ge 0$ and $(c \CatValuation u, \StateMapping(s) \CatValuation \GlobBounds) \in \sem{\Assum \land \Bound \land \Inv}$.
\end{definition}

Theorem~\ref{thm:main-safety} follows from the following lemma, using the fact that the system's invariant implies the safety condition (Definition~\ref{sec:shield-spec}, item~(\ref{obl:inv-implies-post})).

\begin{lemma}[Consistency Preservation]
 For any consistent state $\hat s=(\dots,\EpsRem)$ and action $a \in \hat A$, the next state $\hat s' = (\dots,\EpsRem') \sim \hat P(\hat s, \hat a)$ is also consistent with probability at least $1 - (\EpsRem - \EpsRem')$.
\end{lemma}
\begin{proof}
We show how the consistency of $(s, h, \GlobBounds, \EpsRem)$ is preserved throughout Algorithm~\ref{alg:shield} with suitable likelihood. From Line~\ref{line:algo-start} to Line~\ref{line:start-exec-inference} (excluded), only the observation availability component of history $h$ is changed, in a way that preserves consistency. From the soundness of inference (Theorem~\ref{thm:inf-soundness}), the soundness of the measurement function (Definition~\ref{def:compat-env}, item~(\ref{item:correct-observation})) and the monotonicity of the invariant (Section~\ref{sec:shield-spec}, item~(\ref{obl:inv-monotone})), each iteration of the loop starting at Line~\ref{line:start-exec-inference} preserves the consistency of $(s, \, h \cdot (s, \ObsAvailFun(s), \{p \mapsto v(p) : p \in \LocalParam\}), \, \{p \mapsto v(p) : p \in \GlobalParam\}, \EpsRem)$, barring an event of probability bounded by the decrease in $\EpsRem$. This ensures that consistency still holds before executing Line~\ref{line:mon-start}, with suitable probability. From Lemma~\ref{lem:ctrl-monitor} and controller totality (Section~\ref{sec:shield-spec}, item~(\ref{obl:ctrl-total})), a fallback is always guaranteed to exist on Line~\ref{sec:shield-spec}. Finally, from the model's accuracy (Definition~\ref{def:compat-env}, item~(\ref{item:correct-model})) and the invariant preservation property (Section~\ref{sec:shield-spec}), Line~\ref{line:perform-underlying} preserves consistency.
\end{proof}

\subsection{Soundness of the Inference Strategy Language}\label{ap:inf-lang-soundness-proof}

We provide a proof of Theorem~\ref{thm:inf-soundness} below.

\newcommand{\latestEnv}{w}
\newcommand{\historyEnv}{h}

\begin{proof}
  Let $\tuple{\Const,\dots,\Inference}$ a shield specification and let us assume that $\Fvalid \ProofSem{\Inference}$. Let $a \in \InfActionSem{\Inference}$ an inference action. We need to show that $\sem{\Inference}(a)$ is a list of \emph{sound} inference assignments. By the semantics of strategy composition, it is enough to consider an arbitrary assignment $p \slassign e$ in the strategy and show that it produces a sound symbolic inference assignment. Thus, let $a \in \InfActionSem{p \slassign e}$ and $(p, e', \Eps) = \sem{\Inference}(a)$. We must show that $(p, e', \Eps)$ is sound.

  To do so, by Definition~\ref{def:sound-inf-assignment}, we must fix an arbitrary history and show that updating the latest bound instantiation with $e'$ preserves the truth of $\Bound_p$. Thus, let us consider:
  \begin{itemize}
    \item some symbol valuations $c \in \Const \toI \Reals$ and $u \in \Unknown \toI \Funs$,
    \item a history size $n \in \Nats$,
    \item a state history $(s_1 \dots s_{n}) \in \ProgState^{n}$,
    \item an observation availability history $(z_1 \dots z_{n}) \in \ObsAvail^{n}$, and
    \item a bound instantiation history $(b_1 \dots b_{n}) \in \Inst^{n}$.
  \end{itemize}
  Let us also define:
  \begin{itemize}
    \item some sampled noise variables $\eta_1 \cdots \eta_{n} \sim \sem{\Noise}(c)$,
    \item corresponding observations $o_i = \{\omega \mapsto \sem{\Obs_\omega}(c \CatValuation u, s_i \CatValuation \eta_i) : \omega \in \Param \cap z_i \}$,
    \item the valuation $v = \bigl(s_n \CatValuation b_n \bigr) \cup \bigl(\BigCatValuation_{1 \le i \le n} \TagWithIndex{s_i}{i} \CatValuation \TagWithIndex{b_i}{i} \CatValuation \TagWithIndex{o_i}{i}\bigr)$ summarizing the history and
    \item the value $r = \sem{e'}(c, v)$ of the concrete bound obtained using $v$.
  \end{itemize}
  In addition, let us assume that $(c \CatValuation u, s_i) \in \sem{\Assum \land \Inv \land \BoundConj{b_i}}$ for all $i$. We must show: \[\Prob\left\{ (c \CatValuation u, s_{n}) \notin \sem{\BoundConj{\UpdateMap{b_n}{p}{r}}} \right\} \le \Eps.\]
  Let us abbreviate $\latestEnv = (c \CatValuation u, s_n \CatValuation b_n)$ the data about the current state and $\historyEnv = (c \CatValuation u, v)$ the full historical data. It is enough to show that $\Prob\{ \latestEnv \notin \sem{\BoundWith{p}{r}} \} \le \Eps$. We do so by case disjunction on $e$.
  \paragraph{Case $e = (\theta \slwhen G)$} In this case, the semantics of the inference strategy language gives us $e' = (\theta \WhenSymbBE G)$, and $\Eps = 0$. Let us assume that $\historyEnv \in \sem{G}$. We must show that $\latestEnv \in \sem{\BoundWith{p}{r}}$. By validity of the generated proof obligation, we have $\latestEnv \in \sem{A \limplyI G \limplyI \BoundWith{p}{\theta}}$ where $A \equiv \Assum \wedge (\bigwedge \VarDef{\FV(e)})$. Since $h \in \sem{G}$, we also have $\latestEnv \in \sem{G}$. In addition, we have $\latestEnv \in \sem{A}$ since $(c \CatValuation u, s_i) \in \sem{\Assum \land \Inv \land \BoundConj{b_i}}$ for all $i$. Thus, we get $\latestEnv \in \sem{\BoundWith{p}{\theta}}$. We conclude by observing that $r = \sem{\theta}(\latestEnv)$.
  \paragraph{Case $e = (\slbest i_1, \dots, i_n : \theta  \slwhen G)$} This case is very similar to the previous one.
  \paragraph{Case $e = (\slaggregate i_1, \dots, i_n: \theta_1 \sland \theta_2  \slwhen G)$} By the semantics of the inference strategy language, we have $e' = (\theta_1' + \theta_2' \WhenSymbBE G')$, where $G' = \bigwedge_{\lambda, j \in D} \FSubst{G}{i}{j}$, $\theta_1' \equiv \sum_{\lambda, j \in D} {\lambda \FSubst{\theta_1}{i}{j}}$ and $\theta_2' = \InvCCDF\bigl(\sum_{\lambda, j \in D} {\lambda \FSubst{\theta_2}{i}{j}}, \Eps \bigl)$. Let us assume that $\historyEnv \in \sem{G'}$. We must show that $\Prob(\latestEnv \notin \sem{\BoundWith{p}{r}}) \le \Eps$. By validity of the generated proof obligation and by alpha-renaming, we have: \[h \in \sem{\FSubst{A}{i}{j} \limplyI \FSubst{G}{i}{j} \limplyI \BoundWith{p}{\FSubst{(\theta_1+\theta_2)}{i}{j}}}\]
  for all $\lambda,j \in D$. In addition, since $\historyEnv \in \sem{G'}$, then $\historyEnv \in \sem{\FSubst{G}{i}{j}}$ for all $j$. Finally, $\historyEnv \in \sem{\FSubst{A}{i}{j}}$ for all $j$ by our \emph{history consistency} assumption. As a consequence, we have $h \in \sem{\BoundWith{p}{\FSubst{(\theta_1+\theta_2)}{i}{j}}}$ for all $j$ and thus $h \in \sem{\BoundWith{p}{b^*}}$ where $b^* \equiv \sum_{\lambda, j \in D} \lambda \FSubst{(\theta_1 + \theta_2)}{i}{j}$ since $\Bound_p$ is monotone in $p$ and thus convex (assuming without loss of generality that $p$ is an \emph{upper}-bound). Also, by monotonicity of $p$ again, we have $\historyEnv \in \sem{b \ge b^* \limplyI \BoundWith{p}{b}}$. Therefore, we deduce:
  \[ \Prob(\historyEnv \in \sem{\neg\BoundWith{p}{\theta_1'+\theta_2'}}) \,\le\, \Prob(h \in \sem{b^* > \theta_1' + \theta_2'}) \,=\, \Prob\Bigl(h \in \bigsem{\sum_{\lambda, j \in D} {\lambda \FSubst{\theta_2}{i}{j}} > \theta_2'}\Bigr) \,\le\, \Eps \] where the last inequality results from the definitions of $(\eta_i)_i$ and $(\omega_i)_i$ and from the definition of the $\InvCCDF$ operator.
  We established $\Prob(\historyEnv \in \sem{\neg\BoundWith{p}{\theta_1'+\theta_2'}}) \le \Eps$. In addition, since $\sem{\theta_1' + \theta_2'}(\historyEnv) = r$, we also have $\Prob(\historyEnv \in \sem{\neg\BoundWith{p}{r}}) \le \Eps$. Finally, since $\latestEnv \subseteq \historyEnv$ and $\BoundWith{p}{r}$ does not contain any indexed variable, we have $\Prob(\latestEnv \in \sem{\neg\BoundWith{p}{r}}) \le \Eps$, which concludes the proof.
\end{proof}

\end{document}
