\section{Unsupervised Setting} \label{sec:unlabeled}


\subsection{Approximate Volume Optimality}

Suppose $Y_1,\cdots,Y_n,Y_{n+1}$ are independently drawn from a distribution $P$ on $\mathbb{R}$. The goal is to predict $Y_{n+1}$ based on the first $n$ samples $Y_1,\cdots,Y_n$. To be specific, we would like to construct a data-dependent set $\widehat{C}=\widehat{C}(Y_1,\cdots,Y_n)$ such that 
\begin{equation}
\mathbb{P}(Y_{n+1}\in \widehat{C})\geq 1-\alpha. \label{eq:un-cov}
\end{equation}
Among all data-dependent set that satisfies (\ref{eq:un-cov}), our goal is to find the one with the smallest volume, quantified by the Lebesgue measure $\vol(\widehat{C})=\lambda(\widehat{C})$.
When the distribution $P$ is known, one can directly minimize $\lambda(C)$, subject to $P(C)\geq 1-\alpha$ without even using the data. In particular, when $P\ll \lambda$, an optimal solution is given by the density level set
$$C_{\rm opt}=\left\{\frac{dP}{d\lambda}>t\right\}\cup D,$$
for some $t>0$ and $D$ is some subset of $\{dP/d\lambda =t\}$.

In general, $P$ may not be absolutely continuous and the density need not exist. Nonetheless, we can still define the optimal volume by
$$\opt(P,1-\alpha)=\inf\{\vol(C):P(C)\geq 1-\alpha\}.$$
Note that without any assumption on $P$, the above optimization problem may not have a unique solution. Moreover, it is  possible that the infimum cannot be achieved by any measurable set. Therefore, a natural relaxation is to consider approximate volume optimality. For some $\epsilon\in (0,\alpha)$, a prediction set $\widehat{C}$ is called $\epsilon$-optimal if
\begin{equation}
\vol(\widehat{C})\leq \opt(P,1-\alpha+\epsilon), \label{eq:un-vol}
\end{equation}
either in expectation or with high probability.

The notion of volume optimality defined by (\ref{eq:un-vol}) is quite different from those considered in the literature. A popular quantity that has already been studied is the volume of set difference $\vol\left(\widehat{C}\Delta C_{\rm opt}\right)$ \citep{Lei2013DistributionFreePS,izbicki2020flexible,chernozhukov2021distributional}. However, this much stronger notion requires that the optimal solution $C_{\rm opt}$ must not only exist but also be unique. Usually additional assumptions need to be imposed in the neighborhood of the boundary of $C_{\rm opt}$ in order that the set difference vanishes in the large sample limit. In comparison, the definition (\ref{eq:un-vol}) only requires the volume to be controlled, which can be achieved even if $\widehat{C}$ is not close to $C_{\rm opt}$, or when $C_{\rm opt}$ does not even exist. Indeed, from a practical point of view, any set with coverage and volume control would serve the purpose of valid prediction. Insisting the closeness to a questionable target $C_{\rm opt}$ comes at the cost of unnecessary assumptions on the data generating process.

Another notion considered in the literature is close to our formulation (\ref{eq:un-vol}). Instead of relaxing the coverage probability level from $1-\alpha$ to $1-\alpha+\epsilon$, one can consider the following approximate volume optimality,
\begin{equation}
\vol(\widehat{C})\leq \opt(P,1-\alpha) + \epsilon. \label{eq:additive}
\end{equation}
Results of interval length optimality in the sense of (\ref{eq:additive}) have been studied by \citep{chernozhukov2021distributional,kiyani2024length}. However, the $\epsilon$ in (\ref{eq:additive}) is usually proportional to the scale of the distribution $P$, or may depend on $P$ in some other ways. In comparison, the $\epsilon$ in (\ref{eq:un-vol}) has the unit of probability, and as we will show later, can be made independent of the distribution $P$, which leads to more natural and cleaner theoretical results with fewer assumptions.


\subsection{Impossibility of Distribution-Free Volume Optimality}

It is known that conformal prediction achieves the coverage property (\ref{eq:un-cov}) in a distribution-free sense, meaning that (\ref{eq:un-cov}) holds uniformly for all distributions $P$. One naturally hopes that the approximate volume optimality (\ref{eq:un-vol}) can also be established in a distribution-free way. Perhaps not surprisingly, this goal is too ambitious. The theorem below rigorously proves the impossibility of the task.

\begin{theorem}\label{thm:impossibility}
Consider observations $Y_1$, $Y_2$, $\dots$, $Y_n$, $Y_{n+1}$ sampled $i.i.d.$ from a distribution $P$ on $\R$. 
Suppose $\widehat{C}=\widehat{C}(Y_1,\cdots,Y_n)$ satisfies $\mathbb{P}(Y_{n+1}\in \widehat{C})\geq 1-\alpha$
for all distribution $P$. 
Then, for any $\epsilon \in (0,\alpha)$, there exists some distribution $P$ on $\R$, such that the expected volume of the prediction set is at least
$$\E\vol(\widehat{C})\geq \opt(P,1-\alpha+\epsilon).$$
\end{theorem}

The above impossibility result can be regarded as a consequence of a nonparametric testing lower bound. Consider the following hypothesis testing problem,
\begin{eqnarray*}
H_0:&& P=P_0 \\
H_1:&& P\in\left\{P:\TV(P,P_0)>1-\delta\right\}.
\end{eqnarray*}
It is well known that a testing procedure with both vanishing Type-1 and Type-2 errors does not exist without further constraining the alternative hypothesis, even when $\delta$ is arbitrarily close to $0$ \citep{lecam1960necessary,barron1989uniformly}. In the setting of distribution-free inference with simultaneous coverage and volume guarantees, the coverage property involves the measure $P^{n+1}$, while the expected volume is defined by another measure $P^n\otimes\lambda$. When restricting the support of $P$ to the unit interval $[0,1]$, $\lambda$ becomes the uniform probability, and thus both $P^{n+1}$ and $P^n\otimes\lambda$ are probability distributions. It turns out achieving approximate volume optimality is related to hypothesis testing between $P^{n+1}$ and $P^n\otimes\lambda$ with total variation separation.






\subsection{Distribution-Free Restricted Volume Optimality}

The impossibility result implies a volume lower bound $\opt(P,1-\alpha+\epsilon)$, where the coverage level $1-\alpha+\epsilon$ can be arbitrarily close to $1$. This means that, at least in the worst case, the volume cannot be smaller than that of the support of $P$.

To avoid this triviality, in this section, we consider a weaker notion of volume optimality by only considering prediction sets that are unions of $k$ intervals. We use $\mathcal{C}_k$ to denote the collection of all sets that are unions of $k$ intervals. The restricted optimal volume with respect to the class $\mathcal{C}_k$ is defined by
\begin{equation}
% \opt_k(P,1-\alpha) = \inf\left\{\vol(C):P(C)\geq 1-\alpha, C \in \calC_k \right\}.\label{eq:res-opt}
\opt_k(P, 1-\alpha) = \inf_{C \in \mathcal{C}_k} \left\{\vol(C) : P(C) \geq 1-\alpha \right\}. \label{eq:res-opt}
\end{equation}

\begin{remark}\label{rem:noloss:smooth}
We remark that we are still in a distribution-free setting, since no assumption is imposed on $P$. Instead, the restriction only constrains the shape of the prediction set. 
From a practical point of view, it is reasonable to require that
$\widehat{C}\in \mathcal{C}_k$,
since a more complicated prediction set would be hard to interpret. Moreover, as long as $P$ admits a density function with at most $k$ modes, the two notions match,
$$\opt_k(P,1-\alpha)=\opt(P,1-\alpha).$$
More generally, it can be shown that
$$\opt_k(P,1-\alpha)\leq\opt(P,1-\alpha+\epsilon),$$
for some $\epsilon\in (0,\alpha)$, whenever $P$ can be approximated by a distribution with at most $k$ modes. This, in particular, includes the situation where the density of $P$ can be well estimated by a kernel density estimator. A rigorous statement will be given in Appendix \ref{sec:DPvsKDE}. 
% \anote{put above in remark environment that can be referenced?}
% %\label{rem:noloss:smooth}
\end{remark}

Given the observations $Y_1,\cdots,Y_n$, we define the empirical distribution $\mathbb{P}_n=\frac{1}{n}\sum_{i=1}^n\delta_{Y_i}$. To achieve restricted volume optimality, one can use
\begin{equation}
\widehat{C}=\argmin_{C \in \calC_k}\left\{\vol(C): \mathbb{P}_n(C)\geq 1-\alpha \right\}. \label{eq:erm}
\end{equation}
According to its definition, the prediction set (\ref{eq:erm}) satisfies both $\mathbb{P}_n(\widehat{C})\geq 1-\alpha$ and $\vol(\widehat{C})=\opt_k(\mathbb{P}_n,1-\alpha)$. The coverage and volume guarantees under $P$ can be obtained via
\begin{equation}
\sup_{C \in \calC_k}|\mathbb{P}_n(C)-P(C)| = O_P\left(\sqrt{{\rm VC}(\calC)/n}\right),\label{eq:un-con}
\end{equation}
with ${\rm VC}(\calC)=O(k)$. Therefore, approximate optimality can be achieved by (\ref{eq:erm}) whenever (\ref{eq:un-con}) holds.

A naive exhaustive search to find (\ref{eq:erm}) requires exponential computational time. We show that an efficient dynamic programming algorithm (Algorithm \ref{alg:dp}) can solve (\ref{eq:erm}) approximately with some additional slack $\gamma$, which determines the computational complexity. Theoretical guarantees of Algorithm \ref{alg:dp} are given in the following proposition.
%The dynamic programming table $DP(i,j,l)$ stores the minimum volume of $i$ intervals that cover $l \gamma n$ points in $Y_{(1)}, \dots, Y_{(j)}$ and the right endpoint of the rightmost interval is at $Y_{(j)}$, where $Y_{(1)}, \dots, Y_{(n)}$ are training data points $Y_1,\dots, Y_n$ sorted in non-decreasing order.
%For each state in DP table, we enumerate all possible left endpoint of the rightmost interval and the right endpoint of the previous interval (if it exists). 
%Then, we use the standard backtrack approach on the DP table to find the prediction set $\widehat C_{DP}$.

\input{algorithm/dp}

\begin{proposition}\label{thm:DP}
For any $\gamma\in(0,\alpha)$, Algorithm \ref{alg:dp} computes a prediction set $\widehat{C}_{\rm DP}\in \calC_k$ by dynamic programming with time complexity $O(n^3k/\gamma)$ such that 
    \begin{enumerate}
        \item $\mathbb{P}_n(\widehat{C}_{\rm DP})\geq 1-\alpha$;
        \item $\vol(\widehat{C}_{\rm DP})\leq \opt_k(\mathbb{P}_n,1-\alpha+\gamma)$.
    \end{enumerate}
\end{proposition}

Together with (\ref{eq:un-con}), the coverage and volume guarantees of the dynamic programming can also be generalized from $\mathbb{P}_n$ to $P$.

\subsection{Conformalizing Dynamic Programming}\label{sec:cdp}

Having understood the generalization ability of dynamic programming, we are ready to conformalize the procedure to achieve a finite-sample coverage property. For simplicity, we will adopt the framework of split conformal prediction, though in principle full conformal prediction can also be applied here.

In the split conformal predicition framework, the data set is split into two halves. The first half is used to compute a conformity score, and the second half determines the quantile level. For convenience of notation, let us assume, from now on, that the sample size is $2n$. The split conformal procedure is outlined below.
\begin{enumerate}
\item Compute a score function $q(\cdot)$ using $Y_1,\cdots,Y_n$.
\item Evaluate $q(Y_{n+1}),\cdots, q(Y_{2n})$, and order them as $q_1\leq \cdots\leq q_n$.
\item Output the prediction set
\begin{equation}
\widehat{C}=\left\{y: q(y)\geq q_{\lfloor(n+1)\alpha\rfloor}\right\}. \label{eq:pred-set-un}
\end{equation}
\end{enumerate}
By the exchangeability of $Y_1,\cdots,Y_{2n},Y_{2n+1}$, the prediction set $\widehat{C}$ satisfies
$$\mathbb{P}\left(Y_{2n+1}\in \widehat{C}\right)\geq 1-\alpha,$$
where the above probability is over the randomness of $(Y_1,\cdots,Y_n)$, that of $(Y_{n+1},\cdots,Y_{2n})$, and that of $Y_{2n+1}$.

To conformalize the dynamic programming that approximately computes (\ref{eq:erm}), we will first compute a nested system $S_1\subset\cdots\subset S_m\subset \mathbb{R}$ using the data $Y_1,\cdots,Y_n$. The nested system is required to satisfy the following assumption.

\begin{assumption}\label{as:ne-un}
The sets $S_1\subset\cdots\subset S_m\subset\mathbb{R}$ are measurable with respect to the $\sigma$-field generated by $Y_1,\cdots,Y_n$. Moreover, for some positive integer $k$, some $\alpha\in(0,1)$ and some $\delta,\gamma$ such that $3\delta+\gamma+n^{-1}\leq \alpha$, we have
\begin{enumerate}
\item $\mathbb{P}_n(S_j)=\frac{j}{m}$ and $S_j\in\calC_k$ for all $j\in[m]$.
\item There exists some $j^*\in[m]$, such that\\ $\mathbb{P}_n(S_{j^*})\geq 1-\alpha+n^{-1}+3\delta$ and \\$\vol(S_{j^*})\leq \opt_k(\mathbb{P}_n,1-\alpha+\frac{1}{n}+3\delta+\gamma)$.
\end{enumerate}
Here, $\mathbb{P}_n$ denotes the empirical distribution $\frac{1}{n}\sum_{i=1}^n\delta_{Y_i}$ of the first half of the data.
\end{assumption}
To construct a nested system $\{S_j\}_{j\in[m]}$ that satisfies the above assumption, one only needs to make sure that there exists one subset $S_{j^*}$ in the system that is computed by the dynamic programming (Algorithm \ref{alg:dp}) with confidence level $1-\alpha+n^{-1}+3\delta$ and slack parameter $\gamma$. The rest of the sets in the system can be constructed just to satisfy $\mathbb{P}_n(S_j)=\frac{j}{m}$. In Section \ref{sec:con-nest-m}, we will present a greedy expansion/contraction algorithm that satisfies Assumption \ref{as:ne-un}.

%Though the exact way of construction will not affect the theory, it does have an impact on the practical performance. More discussion on the impact and a recommendation of a specific construction will be given in Appendix \ref{sec:con-nest}.  In particular, we can interpret previous methods as constructing these nested sets in a way that is not volume-aware.  Our construction, on the other hand, takes the volume of the nested sets into account, leading to improved volume efficiency in practice.  
%\vnote{Just added two sentences here about the nested sets.  I think that we should be selling this, since it actually provides a huge improvement over DCP-QR*, but phrasing can be changed.}
With a nested system $\{S_j\}_{j\in[m]}$ satisfying Assumption \ref{as:ne-un}, we can define the conformity score as
\begin{equation}
q(y) = \sum_{j=1}^m\mathbb{I}\{y\in S_j\}. \label{eq:score-un}
\end{equation}
The equivalence between nested system and conformity score was advocated by \cite{gupta2022nested}.
Intuitively, $q(y)$ quantifies the depth of the location $y$. A higher score implies that $y$ is covered by more sets in the nested system, and thus the location should be more likely to be included in the prediction set. Applying the standard split conformal framework, our prediction set based on conformalized dynamic programming is defined by (\ref{eq:pred-set-un}) with the conformity score (\ref{eq:score-un}).

\begin{theorem}\label{thm:unsupervised}
Consider i.i.d. observations $Y_1,\cdots,Y_{2n},Y_{2n+1}$ generated by some distribution $P$ on $\mathbb{R}$. Let $\widehat{C}_{\rm CP-DP}$ be the split conformal prediction set defined by the score (\ref{eq:score-un}) based on a nested system $\{S_j\}_{j\in[m]}$ satisfying Assumption \ref{as:ne-un}. Suppose the parameter $\delta$ in Assumption \ref{as:ne-un} satisfies $\delta\gg \sqrt{\frac{k+\log n}{n}}$. Then the following properties hold.
\begin{enumerate}
\item Coverage: $\mathbb{P}\left(Y_{2n+1}\in\widehat{C}_{\rm CP-DP}\right)\geq 1-\alpha$.
\item Restricted volume optimality: \\$\vol(\widehat{C}_{\rm CP-DP})\leq \opt_k\left(P,1-\alpha+\frac{1}{n}+4\delta+\gamma\right)$ with probability at least $1-2\delta$.
\end{enumerate}
\end{theorem}

We emphasize that Theorem \ref{thm:unsupervised} guarantees both distribution-free coverage and distribution-free volume optimality properties. In practice, $k$ is usually chosen to be a constant for prediction interpretability. By setting $\gamma=O\left(\sqrt{\frac{\log n}{n}}\right)$, the volume sub-optimality is at most $\frac{1}{n}+4\delta+\gamma=O\left(\sqrt{\frac{\log n}{n}}\right)$.


\iffalse
\begin{remark}
A careful reader may notice that the restricted volume optimality is established with high probability. It will be also interesting to obtain a bound for the expected volume $\mathbb{E}\vol(\widehat{C}_{\rm CP-DP})$. In fact, a careful scrutiny of the proof leads to the tail bound
$$\mathbb{P}\left(\vol(\widehat{C}_{\rm CP-DP})> \opt_k\left(P,1-\alpha+\gamma+O\left(\sqrt{\frac{k+t}{n}}\right)\right)\right)\leq 2e^{-t},$$
for all $t>0$. Then, expected volume can be bounded by integrating out the tail probability
$$\mathbb{E}\vol(\widehat{C}_{\rm CP-DP})=\int_0^{\infty}\mathbb{P}\left(\vol(\widehat{C}_{\rm CP-DP})>x\right)dx.$$
Under some weak regularity assumption on $P$, the above integral will lead to $(1+\epsilon')\opt_k(P,1-\alpha+\epsilon)$ for some $\epsilon,\epsilon'>0$, which then can further be bounded by $\opt_k(P,1-\alpha+\epsilon'')$ for some $\epsilon''$. Such a strategy certainly covers distribution classes such as Gaussian mixtures, but in general it should be done case by case.
\end{remark}
\fi

\iffalse
\begin{theorem}\label{thm:unsupervised}
    Given $n$ independent observations $Y_1, \dots, Y_n$ drawn from $P$ and parameters $\alpha, \gamma > 0$, the dynamic programming outputs a prediction set $\widehat{C}(Y_1,\cdots,Y_n) = S_1 \cup \dots \cup S_k$ which is the union of $k$ intervals such that for $Y_{n+1}$ drawn from $P$,
    %$$P(\widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha,$$
    $$\Pr(Y_{n+1}\in \widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha,$$
    and with high probability,
    $$\vol(\widehat{C}(Y_1,\cdots,Y_n))\leq \opt_k\left(P,1-\alpha+\gamma+\delta_{n,k}\right),$$
    where $\delta_{n,k} = O(\sqrt{k/n} + \sqrt{\log n/n})$.
\end{theorem}

We now show the construction of the prediction set using dynamic programming. First, we consider a set of $n$ data points $X \subset \R$. We aim to find $k$ intervals to cover at least $1-\alpha$ fraction of points in $X$. 
Let $\opt_k(X, 1-\alpha) = \inf \{\vol(C): C \in \calC_k, |C \cap X| \geq (1-\alpha)n \}$ be the optimal coverage volume of $k$ intervals that cover $1-\alpha$ fraction of $X$. We show that the dynamic programming procedure finds $k$ intervals that cover $1-\alpha$ fraction of points and have a near-optimal volume. 



We next show how to use this dynamic programming to construct the prediction set that satisfies the coverage condition and the volume optimality in Theorem~\ref{thm:unsupervised}.

\begin{proof}[Proof of Theorem~\ref{thm:unsupervised}]
Let $P_n$ be the empirical distribution of the given data points $Y_1, \dots, Y_n$. 
Note that the VC dimension of the class of $k$ intervals $\calC_k$ is $2k$. 
By the Talagrand's inequality, we have with probability at least $1- 1/\mathrm{poly}(n)$,
$$\sup_{C\in\mathcal{C}_k}|P_n(C)-P(C)| \leq \delta_{n,k},$$
where $\delta_{n,k} = O(\sqrt{k/n} + \sqrt{\log n/n})$.
 
%For example, one could take $\delta_{n,k}\asymp \frac{k}{\sqrt{n}}$, or perhaps it can be improved to $\sqrt{\frac{k}{n}}$. 
By Lemma~\ref{thm:DP}, the dynamic programming with parameters $\alpha, \gamma$, and $k$ outputs $k$ intervals that satisfies %$\widehat{C}(Y_1,\cdots,Y_n)$ satisfying
$$P_n(\widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha,$$
and
$$\vol(\widehat{C}(Y_1,\cdots,Y_n)) \leq \opt_k(P_n,1-\alpha+\gamma).$$
Therefore, we have with probability at least $1-1/\mathrm{poly}(n)$,
$$P(\widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha-\delta_{n,k},$$
and 
\begin{eqnarray*}
\vol(\widehat{C}(Y_1,\cdots,Y_n)) &\leq& \opt_k(P_n,1-\alpha+\gamma) \\
&=& \inf\{\vol(C):P_n(C)\geq 1-\alpha+\gamma, C\in\mathcal{C}_k\} \\
&\leq& \inf\{\vol(C):P(C)\geq 1-\alpha+\gamma+\delta_{n,k}, C\in\mathcal{C}_k\} \\
&=& \opt_k(P,1-\alpha+\gamma+\delta_{n,k}).
\end{eqnarray*}

Finally, we adjust the parameters of the dynamic programming to be $\alpha' = \alpha - \delta_{n,k} - 1/\mathrm{poly}(n)$. Then, we have the prediction set outputs by the adjusted dynamic programming satisfies
$$\Pr(Y_{n+1}\in \widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha,$$
and with probability at least $1-1/\mathrm{poly}(n)$,
$$\vol(\widehat{C}(Y_1,\cdots,Y_n))\leq \opt_k(P,1-\alpha+\gamma+\delta'_{n,k}),$$
where $\delta'_{n,k} = 2\delta_{n,k} + 1/\mathrm{poly}(n).$
\end{proof}
\fi

% In fact, one could adjust the parameter of the dynamic programming, so that with high probability, we have
% $$P(\widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha,$$
% and
% $$\vol(\widehat{C}(Y_1,\cdots,Y_n))\leq \opt_k(P,1-\alpha+\gamma+2\delta_{n,k}),$$
% with high probability. One could integrate out the probability tail to get in-expectation bounds (this requires additional details to be checked).

