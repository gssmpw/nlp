\section{KDE Optimality Implies DP Optimality}\label{sec:DPvsKDE}

Suppose a distribution $P$ on $\mathbb{R}$ admits a density function $p$. The kernel density estimator depending on $k$ i.i.d. samples $Z_1,\cdots,Z_k$ is defined by
\begin{equation}
p_k(y)=\frac{1}{k\rho}\sum_{j=1}^kK\left(\frac{y-Z_j}{\rho}\right),\label{eq:KDE-k}
\end{equation}
where $K(\cdot)$ is a standard Gaussian kernel and $\rho$ is a bandwidth parameter. The conformal prediction method by \cite{Lei2013DistributionFreePS} is based on the idea that the level set of $p_k$ is close to that of $p$ as long as $p_k$ is close to $p$. In this section, we will show that as long as $p_k$ is close to $p$, the dynamic programming also finds a prediction set whose volume is nearly optimal compared with the level set of $p$. This implies that DP always requires no stronger assumption to achieve volume optimality.

The existence of a KDE close to $p$ can be even weakened into the following assumption.
\begin{assumption}\label{as:kde}
For any positive integer $k$, there exists some $\epsilon_k>0$ and some Gaussian mixture $P_k=\sum_{j=1}^k w_jN(\mu_j,\sigma_j^2)$ such that $\TV(P_k,P)\leq\epsilon_k$.
\end{assumption}
In particular, the KDE (\ref{eq:KDE-k}) based on $k$ samples is a special case of the Gaussian mixture, given that Gaussian kernel is used. Though the characterization of the closeness between $p_k$ and $p$ is through $\ell_{\infty}$ norm by \cite{Lei2013DistributionFreePS}, similar error bounds also apply to the $\ell_1$ norm, which is the total variation distance. For example, suppose $P$ has bounded support and the HÃ¶lder smoothness is $\beta\in (0,2]$. Then, one can take $\epsilon_k=\widetilde{\Theta}\left(k^{-\frac{\beta}{2\beta+1}}\right)$ with an appropriate choice of the bandwidth, where $\widetilde{\Theta}$ hides some logarithmic factor of $k$.

\begin{theorem}\label{thm:DPvsKDE}
Consider i.i.d. observations $Y_1,\cdots,Y_n,Y_{n+1}$ generated by some distribution $P$ on $\mathbb{R}$ that satisfies Assumption \ref{as:kde}. For any $\alpha,\delta,\gamma\in(0,1)$ such that $\delta\gg\sqrt{\frac{k+\log n}{n}}$ and $\gamma+2\delta+2\epsilon_k<\alpha$, let $\widehat{C}_{\rm DP}\in\calC_k$ be the output of Algorithm \ref{alg:dp} with coverage level $1-\alpha+\delta$ and slack $\gamma$. Then, with probability at least $1-\delta$, we have
\begin{enumerate}
\item $\mathbb{P}\left(Y_{n+1}\in \widehat{C}_{\rm DP}\mid Y_1,\cdots,Y_n\right)\geq 1-\alpha$;
\item $\vol(\widehat{C}_{\rm DP})\leq \opt(P,1-\alpha+\gamma+2\delta+2\epsilon_k)$.
\end{enumerate}
\end{theorem}

The result of Theorem \ref{thm:DPvsKDE} can also be conformalized as in Section \ref{sec:cdp}, so that the restricted volume optimality $\opt_k(P,\cdot)$ in Theorem \ref{thm:unsupervised} can be strengthened to $\opt(P,\cdot)$ without restriction whenever $P$ satisfies Assumption \ref{as:kde}, which, in particular, includes the situation where the density of $P$ can be well estimated by KDE.

The volume sub-optimality given by Theorem \ref{thm:DPvsKDE} is $\gamma+2\delta+2\epsilon_k$. When the distribution $P$ is $\beta$-smooth, the sub-optimality is of order $\widetilde{\Theta}\left(\sqrt{\frac{k}{n}}+k^{-\frac{\beta}{2\beta+1}}\right)$ by taking $\epsilon_k=\widetilde{\Theta}\left(k^{-\frac{\beta}{2\beta+1}}\right)$, $\delta=\widetilde{\Theta}\left(\sqrt{\frac{k}{n}}\right)$, and $\gamma$ sufficiently small. Thus, optimizing this bound over $k$ leads to the rate $\widetilde{\Theta}\left(n^{-\frac{\beta}{4\beta+1}}\right)$. In comparison, the KDE achieves a faster rate $\widetilde{\Theta}\left(n^{-\frac{\beta}{2\beta+1}}\right)$ \citep{Lei2013DistributionFreePS} for smooth densities. This is actually a technical artifact by specializing Assumption \ref{as:kde} to the KDE (\ref{eq:KDE-k}). In fact, when the density of $P$ is $\beta$-smooth, it is well known that Assumption \ref{as:kde} is satisfied with a better $\epsilon_k=\widetilde{\Theta}(k^{-\beta})$ \citep{ghosal2007posterior,kruijer2010adaptive}, which then leads to the volume sub-optimality $\widetilde{\Theta}\left(\sqrt{\frac{k}{n}}+k^{-\beta}\right)$ that leads to the near optimal rate $\widetilde{\Theta}\left(n^{-\frac{\beta}{2\beta+1}}\right)$ with $k=\widetilde{\Theta}(n^{\frac{1}{2\beta+1}})$.

\subsection{Proof of Theorem \ref{thm:DPvsKDE}}

We first state a lemma that shows that a level set of a Gaussian mixture with $k$ components must belong to the class $\calC_k$.
\begin{lemma}\label{lem:gmmode}
For a Gaussian mixture $P_k=\sum_{j=1}^kw_jN(\mu_j,\sigma_j^2)$ and any $\alpha\in(0,1)$,
$$\opt_k(P_k,1-\alpha)=\opt(P_k,1-\alpha).$$
\end{lemma}
The proof of the lemma will be given in Appendix~\ref{sec:proof_of_lemmas}.
Now we are ready to state the proof of Theorem \ref{thm:DPvsKDE}.
\begin{proof}[Proof of Theorem \ref{thm:DPvsKDE}]
By Proposition \ref{thm:DP}, we know that $\widehat{C}_{\rm DP}$ satisfies $\mathbb{P}_n(\widehat{C}_{\rm DP})\geq 1-\alpha+\delta$ and $\vol(\widehat{C}_{\rm DP})\leq \opt_k(\mathbb{P}_n,1-\alpha+\delta+\gamma)$. The condition on $\delta$ implies that $\sup_{C \in \calC_k}|\mathbb{P}_n(C)-P(C)|\leq\delta$ with probability at least $1-\delta$ \citep{devroye2001combinatorial}. Therefore, the coverage probability is
$$\mathbb{P}\left(Y_{n+1}\in \widehat{C}_{\rm DP}\mid Y_1,\cdots,Y_n\right)\geq \mathbb{P}_n(\widehat{C}_{\rm DP})-\delta \geq 1-\alpha,$$
and the volume can be bounded by
\begin{eqnarray*}
\vol(\widehat{C}_{\rm DP}) &\leq & \opt_k(\mathbb{P}_n,1-\alpha+\delta+\gamma) \\
&\leq& \opt_k(P,1-\alpha+2\delta+\gamma) \\
&\leq& \opt_k(P_k,1-\alpha+2\delta+\gamma+\epsilon_k) \\
&=& \opt(P_k,1-\alpha+2\delta+\gamma+\epsilon_k) \\
&\leq& \opt(P,1-\alpha+2\delta+\gamma+2\epsilon_k),
\end{eqnarray*}
where the identity above is by Lemma \ref{lem:gmmode}.
\end{proof}


\section{Additional Proofs}\label{sec:proofs}


\subsection{Proof of Theorem \ref{thm:impossibility}}

The proof relies on the following technical lemma, whose proof will be given in Appendix~\ref{sec:proof_of_lemmas}.

\begin{lemma}\label{lem:tv}
For any $\delta,\epsilon>0$ and any integer $n>0$, there exists some distribution $\Pi$ supported on
$$\mathcal{P}_{\epsilon}=\left\{P:\text{supp}(P)\subset[0,1],\TV(P,\lambda)\geq 1-\epsilon\right\},$$
such that $\TV\left(\lambda^n,\int P^n \mathrm{d}\Pi\right)\leq \delta$. 
\end{lemma}

Now we are ready to state the proof of Theorem \ref{thm:impossibility}.
\begin{proof}[Proof of Theorem \ref{thm:impossibility}]
By Lemma \ref{lem:tv}, there exist $\Pi_{n,\delta}$ and $\Pi_{n+1,\delta}$ supported on $\mathcal{P}_{\delta}$, such that $\TV\left(\lambda^n, \int P^n \mathrm{d}\Pi_{n,\delta}\right)\leq\delta$ and $\TV\left(\lambda^{n+1}, \int P^{n+1} \mathrm{d}\Pi_{n+1,\delta}\right)\leq\delta$. 

Since $P^{n+1}(Y_{n+1}\in \widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha$ for all $P$, we have
$$\int P^{n+1}(Y_{n+1}\in \widehat{C}(Y_1,\cdots,Y_n))\mathrm{d}\Pi_{n+1,\delta}\geq 1-\alpha.$$
By $\TV\left(\lambda^{n+1}, \int P^{n+1} \mathrm{d}\Pi_{n+1,\delta}\right)\leq\delta$, we have
\begin{align*}
    \E_{Y_1,\dots,Y_n \sim \lambda^n}(\lambda(\widehat{C}(Y_1,\cdots,Y_n)))
    =\lambda^{n+1}(Y_{n+1}\in \widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha-\delta.
\end{align*}
% $$\E_{Y_1,\dots,Y_n \sim \lambda^n}(\lambda(\widehat{C}(Y_1,\cdots,Y_n)))=\lambda^{n+1}(Y_{n+1}\in \widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha-\delta.$$
By $\TV\left(\lambda^n, \int P^n \mathrm{d}\Pi_{n,\delta}\right)\leq\delta$, we have
$$\int \E_{Y_1,\dots,Y_n \sim P^n}\lambda(\widehat{C}(Y_1,\cdots,Y_n)) \mathrm{d}\Pi_{n,\delta}\geq 1-\alpha-2\delta.$$
Then, there must exists some $P\in\text{supp}(\Pi_{n,\delta})\subset\mathcal{P}_{\delta}$, such that
$$\E_{Y_1,\dots,Y_n \sim P^n}\lambda(\widehat{C}(Y_1,\cdots,Y_n))\geq 1-\alpha-2\delta.$$
The fact that $P\in \mathcal{P}_{\delta}$ implies $\TV(P,\lambda)\geq 1-\delta$. 
By the definition of total variation, there exists some set $B$ such that $P(B)-\lambda(B)\geq 1-\delta$, which implies $P(B)\geq 1-\delta$ and $\lambda(B)\leq\delta$. Therefore, $\opt(P,1-\delta)\leq \delta$. 

We finally have for any $\varepsilon \in (0,\alpha)$, the expected volume of prediction set is
\begin{eqnarray*}
\E_{Y_1,\dots,Y_n \sim P^n}\vol(\widehat{C}(Y_1,\cdots,Y_n)) 
&\geq& 1-\alpha-2\delta \\
&\geq& \delta \\
&\geq& \opt(P,1-\delta) \\
&\geq& \opt(P,1-\alpha + \varepsilon),
\end{eqnarray*}
as long as $\delta$ is sufficiently small so that $\delta<\min\{(1-\alpha)/3, \alpha-\epsilon\}$. The proof is complete.
\end{proof}


\subsection{Proof of Theorem \ref{thm:unsupervised}}

Theorem \ref{thm:unsupervised} is a special case of Theorem \ref{thm:supervised} in the setting where $F(\cdot\mid x)$ does not depend on $x$ and $\widehat{F}(\cdot\mid x)$ is defined as the empirical CDF of $Y_1,\cdots,Y_n$. Then, Assumption \ref{ass:F} is automatically satisfied by a standard VC dimension bound \citep{devroye2001combinatorial}. 


\subsection{Proof of Theorem \ref{thm:supervised}}

We will prove the three properties of Theorem \ref{thm:supervised} separately. We note that the marginal coverage property holds without Assumptions \ref{as:ne-su} and \ref{ass:F}. It is a standard consequence of applying the split conformal framework, but we still include a proof here for completeness.

\begin{proof}[Proof of Theorem \ref{thm:supervised} (marginal coverage)]
By the construction of $\widehat{C}_{\rm DCP-DP}(X_{2n+1})$, we have
\begin{align*}
    \Pr\left(Y_{2n+1} \in \widehat{C}_{\rm DCP-DP}(X_{2n+1})\right) = \Pr \left(q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}\right).
\end{align*}
Since the conformity score $q$ is constructed from $\widehat{F}(\cdot \mid \cdot)$, it is independent from the second half of the data. 
Thus, $q(Y_{2n+1},X_{2n+1})$ is exchangeable with $q(Y_{n+1},X_{n+1}),\cdots, q(Y_{2n},X_{2n})$, which implies the desired conclusion by the definition of $q_{\lfloor(n+1)\alpha\rfloor}$.
\end{proof}

Next, we establish the conditional coverage property. We need the following property of the conformity score that is computed based on a nested system.
\begin{lemma}\label{lem:nest}
For any $j\in[m+1]$, $y\in S_j(x)$ if and only if $q(y,x)\geq m-j+1$, where the set $S_{m+1}(x)$ is defined as $\mathbb{R}$.
\end{lemma}


The proof of the lemma will be given in Appendix~\ref{sec:proof_of_lemmas}.

\begin{proof}[Proof of Theorem \ref{thm:supervised} (approximate conditional coverage)]
We first note that Assumption \ref{ass:F} implies
\begin{equation}
\mathbb{E}\|\widehat{F}(\cdot \mid X_{2n+1})-F(\cdot \mid X_{2n+1})\|_{k,\infty}\leq 2\delta.\label{eq:f-expectation}
\end{equation}
We use $\mathcal{F}_{2n}$ to denote the $\sigma$-field generated by the random variables $(X_1,Y_1),\cdots, (X_{2n},Y_{2n})$. Let $\mathbb{E}_{X_{2n+1}}$ and $\mathbb{E}_{\mathcal{F}_{2n}}$ be the expectation operators under the marginal distributions of $X_{2n+1}$, and of $(X_1,Y_1),\cdots, (X_{2n},Y_{2n})$, respectively. Then, we have
\begin{align*}
\mathbb{P}\left(q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}\right) = \E_{\mathcal{F}_{2n}}\E_{X_{2n+1}}\mathbb{P}\left(q(Y_{2n+1},X_{2n+1}) 
\geq q_{\lfloor(n+1)\alpha\rfloor}| X_{2n+1},\mathcal{F}_{2n}\right). 
\end{align*}
By Lemma \ref{lem:nest}, $q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}$ is equivalent to $Y_{2n+1}\in S_{\widehat{j}}(X_{2n+1})$ for some $\widehat{j}$ measurable with respect to $\mathcal{F}_{2n}$. Then, we have
\begin{align*}
\E_{\mathcal{F}_{2n}}\E_{X_{2n+1}}\mathbb{P}\left(q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}|X_{2n+1},\mathcal{F}_{2n}\right) &=\mathbb{E}_{\mathcal{F}_{2n}}\mathbb{E}_{X_{2n+1}}\mathbb{P}\left(Y_{2n+1}\in S_{\widehat{j}}(X_{2n+1})|X_{2n+1},\mathcal{F}_{2n}\right) \\
&=\mathbb{E}_{\mathcal{F}_{2n}}\mathbb{E}_{X_{2n+1}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d} F(y\mid X_{2n+1}). 
\end{align*}
Since $S_{\widehat{j}}(X_{2n+1}) \in \calC_k$, by (\ref{eq:f-expectation}), we have 
\begin{align*}
\mathbb{E}_{\mathcal{F}_{2n}}\mathbb{E}_{X_{2n+1}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d} F(y\mid X_{2n+1}) 
\leq& \mathbb{E}_{\mathcal{F}_{2n}}\mathbb{E}_{X_{2n+1}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d}\widehat{F}(y \mid X_{2n+1}) \\
&+ \mathbb{E}\|\widehat{F}(\cdot \mid X_{2n+1})-F(\cdot \mid X_{2n+1})\|_{k,\infty} \\
\leq& \mathbb{E}_{\mathcal{F}_{2n}}\mathbb{E}_{X_{2n+1}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d}\widehat{F}(y \mid X_{2n+1}) + 2\delta.
\end{align*}
By Assumption \ref{as:ne-su}, we have $\int_{S_{\widehat{j}}(X_{2n+1})}\mathrm{d}\widehat{F}(y \mid X_{2n+1})=\frac{\widehat{j}}{m}$, which is independent of $X_{2n+1}$, since $\widehat{j}$ measurable with respect to $\mathcal{F}_{2n}$. Thus, we have 
\begin{align*}
\mathbb{E}_{\mathcal{F}_{2n}}\mathbb{E}_{X_{2n+1}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d}\widehat{F}(y \mid X_{2n+1}) + 2\delta 
=\mathbb{E}_{\mathcal{F}_{2n}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d}\widehat{F}(y \mid X_{2n+1}) + 2\delta.
\end{align*}
By Assumption~\ref{ass:F}, we have with probability at least $1-\delta$,
\begin{align*}
\mathbb{E}_{\mathcal{F}_{2n}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d}\widehat{F}(y \mid X_{2n+1}) + 2\delta 
\leq& \mathbb{E}_{\mathcal{F}_{2n}}\int_{S_{\widehat{j}}(X_{2n+1})} \mathrm{d}F(y \mid X_{2n+1}) +3\delta \\
=& \mathbb{P}\left(q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor} \mid X_{2n+1}\right) + 3\delta.
\end{align*}
Therefore, with probability at least $1-\delta$, the approximate conditional coverage holds
\begin{align*}
\mathbb{P}\left(q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor} \mid X_{2n+1}\right) 
\geq \mathbb{P}\left(q(Y_{2n+1},X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}\right) - 3\delta \geq 1-\alpha -3\delta.
\end{align*}
\end{proof}

Finally, we prove the last property on volume optimality.
\begin{proof}[Proof of Theorem \ref{thm:supervised} (conditional restricted volume optimality)]
By Assumption \ref{as:ne-su}, there exists some $j^*\in[m]$, such that
$$\mathbb{E}\int_{S_{j^*}(X_{2n+1})} \mathrm{d}\widehat{F}(y \mid X_{2n+1})\geq 1-\alpha+\frac{1}{n}+3\delta.$$
Assumption \ref{ass:F} implies that 
\begin{eqnarray*}
\mathbb{P}\left(Y_{2n+1}\in S_{j^*}(X_{2n+1})\right) 
&=& \mathbb{E}\int_{S_{j^*}(X_{2n+1})} \mathrm{d}F(y\mid X_{2n+1}) \\
&\geq& \mathbb{E}\int_{S_{j^*}(X_{2n+1})} \mathrm{d}\widehat{F}(y\mid X_{2n+1})\\
&& - \mathbb{E}\|\widehat{F}(\cdot \mid X_{2n+1})-F(\cdot\mid X_{2n+1})\|_{k,\infty}\\
&\geq& 1-\alpha+\frac{1}{n}+\delta.
\end{eqnarray*}
By Hoeffding's inequality and the condition on $\delta$, we have
$$\frac{1}{n}\sum_{i=n+1}^{2n}\mathbb{I}\{Y_i\in S_{j^*}(X_i)\}\geq \mathbb{P}\left(Y_{2n+1}\in S_{j^*}(X_{2n+1})\right)-\delta,$$
with probability at least $1-\delta$.
Combining the two inequalities above and Lemma \ref{lem:nest}, we get
$$\frac{1}{n}\sum_{i=n+1}^{2n}\mathbb{I}\{q(Y_i,X_i)\geq m-j^*+1\}\geq 1-\alpha+\frac{1}{n},$$
with probability at least $1-\delta$.
This immediately implies $q_{\lfloor n\alpha\rfloor}=q_{\lfloor n(\alpha-n^{-1})+1\rfloor}\geq m-j^*+1$ by the definition of order statistics. Therefore, the volume of the prediction set $\widehat{C}_{\rm DCP-DP}(X_{2n+1})$ is at most
\begin{align*}
&\vol\left(\left\{y\in\mathbb{R}: q(y,X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}\right\}\right) \\
\leq& \vol\left(\left\{y\in\mathbb{R}: q(y,X_{2n+1})\geq q_{\lfloor n\alpha\rfloor}\right\}\right) \\
\leq& \vol\left(\left\{y\in\mathbb{R}: q(y,X_{2n+1})\geq m-j^*+1\right\}\right) \\
=& \vol(S_{j^*}(X_{2n+1})),
\end{align*}
where the last identiy is by Lemma \ref{lem:nest}.
The volume of $S_{j^*}(X_{2n+1})$ can be controlled by Assumption \ref{as:ne-su},
\begin{align*}
    \vol(S_{j^*}(X_{2n+1})) \leq& \opt_k\left(\widehat{F}(\cdot\mid X_{2n+1}),1-\alpha+\frac{1}{n}+3\delta+\gamma\right) \\
\leq& \opt_k\left(F(\cdot\mid X_{2n+1}),1-\alpha+\frac{1}{n}+4\delta+\gamma\right),
\end{align*}
where the last inequality, which holds with probability at least $1-\delta$, is by Assumption~\ref{ass:F}. Combining the inequalities above with union bound, we get the conclusion.
% \begin{eqnarray}
% \nonumber && \Vol\left(\left\{y\in\mathbb{R}: q(y,X_{n+1})\geq q_{((n+1)\alpha)}\right\}\right) \\
% \nonumber &\leq& \Vol\left(\left\{y\in\mathbb{R}: q(y,X_{n+1})\geq q_{(n\alpha)}\right\}\right) \\
% \nonumber &\leq& \Vol\left(\left\{y\in\mathbb{R}: q(y,X_{n+1})\geq m-j^*+1\right\}\right) \\
% \nonumber &=& \Vol(S_{j^*}(X_{n+1})) \\
% \label{eq:use-dp-v} &\leq& \text{Opt}_k\left(\hat{F}(\cdot|X_{n+1}),1-\alpha+\frac{1}{n}+3\delta+\gamma\right) \\
% \nonumber &\leq& \text{Opt}_k\left(F(\cdot|X_{n+1}),1-\alpha+\frac{1}{n}+4\delta+\gamma\right),
% \end{eqnarray}
% where we have used (\ref{eq:dp-v}) in (\ref{eq:use-dp-v}).
\end{proof}



\subsection{Proofs of Proposition \ref{thm:DP}, Lemma \ref{lem:gmmode}, Lemma \ref{lem:tv} and Lemma \ref{lem:nest}} \label{sec:proof_of_lemmas}

\textbf{Dynamic Programming Algorithm.} 
The dynamic programming table $DP(i,j,l)$ stores the minimum volume of $i$ intervals that collectively cover $l \gamma n$ points from the sorted training data $Y_{(1)}, \dots, Y_{(j)}$, where the right endpoint of the rightmost interval is fixed at $Y_{(j)}$. Here, $Y_{(1)}, \dots, Y_{(n)}$ are the training data points $Y_1,\dots, Y_n$ sorted in non-decreasing order.
For each state in the DP table, we iterate over all possible left endpoints of the rightmost interval, as well as the right endpoint of the preceding interval (if it exists). This allows us to systematically compute the optimal solution for each state by the following formula:
\begin{align*}
&\text{If $i = 1$,} \quad DP(i,j,l) = Y_{(j)} - Y_{(j-\lceil l\gamma n \rceil +1)},\\
&\text{If $i > 1$,} \quad DP(i,j,l) = \min_{i-1\leq j''< j' \leq j}\{Y_{(j)} - Y_{(j')} + DP(i-1, j'', l')\},
\end{align*}
where $l' = l - \lfloor (j - j' + 1)/(\gamma n) \rfloor$.
Finally, we find the minimum volume solution among all entries $DP(k,j,\lceil(1-\alpha)/ \gamma\rceil)$ for all $1\leq j\leq n$.
Then, we use the standard backtrack approach on the DP table to find the prediction set $\widehat C_{DP}$.

\begin{proof}[Proof of Proposition \ref{thm:DP}.]
    Without loss of generality, we assume that $1/\gamma$ is an integer, otherwise, we can decrease $\gamma$ to make this hold.
    For any $i \in [k], j \in [n], l \in [ 1/\gamma ]$, we use the dynamic programming table entry $DP(i,j,l)$ to store the minimum volume of $i$ intervals that cover $ \lceil l\cdot \gamma n \rceil$ points in $Y_{(1)},\dots, Y_{(j)}$ and the right endpoint of the rightmost interval is at $Y_{(j)}$. 
    If there is no feasible solution for this subproblem, we set $DP(i,j,l) = \infty$. 
    % Then, for each $DP(i,j,l)$, we enumerate all possible left endpoints $X_{(j')}$ of the rightmost interval for $i \leq j' \leq j$. 
    % For each $j'$, we search all possible right endpoints $X_{(j'')}$ of the subproblem with $i-1$ intervals for $i-1 \leq j'' < j'$. 
    % We consider all subproblems $DP(i-1, j'', l')$ where $l' = l - \lfloor (j-j'+1)/(\gamma n) \rfloor$. 
    % If all subproblems have no feasible solution, then we set $DP(i,j,l) = \infty$. 
    % Otherwise, we find the minimum volume solution among all possible configurations and store it in $DP(i,j,l)$. 
    This dynamic programming is shown in Algorithm~\ref{alg:dp}.
    This dynamic programming runs in time $O(n^3k /\gamma)$.

    We then find the solution with the minimum volume among all subproblems $DP(k,j, \lceil (1-\alpha)/\gamma \rceil )$ for $1\leq j \leq n$. It is easy to see that there exists a feasible solution. Let $\widehat{C}_{\rm DP} \in \calC_k$ be a union of $k$ intervals in this solution. This solution covers at least $ \lceil (1-\alpha)/\gamma \cdot (n\gamma) \rceil  = \lceil (1-\alpha) n \rceil$ points in $X_1,\dots, X_n$. Thus, we have 
    \begin{align*}
        \mathbb{P}_n(\widehat{C}_{\rm DP})\geq 1-\alpha.
    \end{align*}
    If the restricted optimal volume $\opt_k(\mathbb{P}_n, ((1-\alpha)/\gamma+1) \cdot (n\gamma)/n )$ is smaller than the volume of $\widehat{C}_{\rm DP}$, then this solution cannot have the minimum volume among all subproblems $DP(k,j, \lceil (1-\alpha)/\gamma \rceil )$ for $1\leq j \leq n$. Thus, the volume of this solution must satisfy
    \begin{align*}
        \vol(\widehat{C}_{\rm DP}) \leq& \opt_k(\mathbb{P}_n, ((1-\alpha)/\gamma+1) \cdot (n\gamma) / n) \\
        =& \opt_k(\mathbb{P}_n,1-\alpha + \gamma).
    \end{align*}
    The proof is thus complete.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:gmmode}]
By \cite{carreira2003number}, there are $k' \le k$ local maxima for the density function $p_k$. We will use $k'$ intervals and define the rest of the intervals to be empty. Suppose $u_1 \le u_2 \le  \dots \le u_{k'} \in \R$ are the local maxima of $p_k$. The density $p_k(y)$ is differentiable, and its local minima and local maxima have to alternate. Hence there are exactly $k'-1$ local minima, denoted by $\ell_1, \ell_2, \dots, \ell_{k'-1}$ with $\ell_j \in [u_{j}, u_{j+1}]$ for all $ j \in \{1,2,\dots, k'-1\}$. (Note that there are no local minima less than $u_1$ or greater than $u_{k'}$ since $p_k(y) \to 0$ as $y \to \pm \infty$). For notational convenience let $\ell_0 = -\infty, \ell_{k'}=\infty$. Let $C^* \subset \R$  satisfies $P_k(C^*)\geq 1-\alpha$ and $\vol(C^*)=\opt(P_k,1-\alpha)$.

We now show that there exist $I_1,\cdots,I_{k'}\in\calC_1$ such that $u_j\in I_j\subset [\ell_{j-1},\ell_j]$ for all $j\in[k']$, $P_k(\cup_{j=1}^{k'}I_j)\geq 1-\alpha$, and $\vol(\cup_{j=1}^{k'}I_j)\leq \vol(C^*)$. This would imply the desired conclusion. Consider $S_j = S^* \cap [\ell_{j-1}, \ell_j]$ for all $j \in [k']$. Next we observe for all $j \in [k']$, $p_k$ is monotonically increasing in the interval $[\ell_{j-1}, u_{j}]$ and is monotonically decreasing in the intervals $[u_j, \ell_j]$, with a local maximum at $u_j$. Hence if $S_j$ comprises multiple disjoint intervals within $[\ell_{j-1}, \ell_{j}]$, we can pick one interval with the same volume within $[\ell_{j-1}, \ell_j]$ that also includes $u_j$ and covers at least as much probability mass. This establishes the property of $\cup_{j=1}^{k'}I_j$, and hence the lemma.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:tv}]
First, we construct a family of distributions supported on subsets of $[0,1]$. Consider an integer $m$. We partition the interval $[0,1]$ into $m$ intervals with length $1/m$ each and define the subinterval $A_j=\left[\frac{j-1}{m},\frac{j}{m}\right)$ for $j \in [m]$. 
We next define a family of distributions supported on these subintervals. 
For any vector $Z\in\{0,1\}^m$, let $A_Z=\bigcup_{j:Z_j=1}A_j$ denote the union of intervals corresponding to the indices where $Z_j = 1$. Then, we define the density function $$p_Z(y)=\frac{\mathbb{I}\{y\in A_Z\}}{\frac{1}{m}\sum_{j=1}^mZ_j},$$
where $\mathbb{I}$ is the indicator function.
Let $P_Z$ be the corresponding distribution.
% For any vector $Z\in\{0,1\}^m$, define the density function
% $$p_Z(y)=\frac{\sum_{j=1}^mZ_j\mathbb{I}\{x\in A_j\}}{\frac{1}{m}\sum_{j=1}^mZ_j},$$
% where $\mathbb{I}$ is the indicator function.
 
We then construct the weight distribution $\Pi$. 
Given any $\varepsilon > 0$, we now provide a restricted set of vectors $Z \in \{0,1\}^m$ such that the distribution $P_Z$ has at least $1-\varepsilon$ total variation distance to the uniform distribution $\lambda$. We pick a parameter $\beta \in (0,1)$ depending on $\varepsilon$ and $m$. Then, we define a set of vectors $Z$ with $A_Z$ covering approximately $\beta$ fraction of $[0,1]$, 
$$\mathcal{Z}=\left\{Z\in\{0,1\}^m:\left|\frac{1}{m}\sum_{j=1}^mZ_j-\beta\right|\leq\left(\frac{\beta}{m}\right)^{1/3}\right\}.$$
For any $Z\in\mathcal{Z}$, we have the total variation distance 
$$\TV(P_Z,\lambda) \geq \lambda(A_Z^c)=1-\frac{1}{m}\sum_{j=1}^mZ_j\geq 1-\beta-\left(\frac{\beta}{m}\right)^{1/3}.$$ 
Therefore, as long as $\beta+\left(\frac{\beta}{m}\right)^{1/3}\leq\epsilon$, we have $P_Z\in\mathcal{P}_{\epsilon}$ for all $Z\in\mathcal{Z}$.
We construct the weight distribution $\Pi$ supported on the $\{P_Z: Z \in \calZ\}$. Let $\tilde{\Pi}=\otimes_{j=1}^m\text{Bernoulli}(\beta)$ be the product distribution on $\{0,1\}^m$ such that each coordinate is $1$ with probability $\beta$. Then, we define $\Pi$ to be the distribution $\tilde{\Pi}$ conditioning on $\mathcal{Z}$, $\Pi(Z)=\frac{\tilde{\Pi}(Z\cap\mathcal{Z})}{\tilde{\Pi}(\mathcal{Z})}$. 


We bound $\TV(\lambda^n,\int P_Z^n d\Pi(Z))$ by chi-squared divergence,
\begin{align*}
    \frac{1}{2}\TV\left(\lambda^n,\int P_Z^n \mathrm{d}\Pi(Z)\right)^2 \leq \int_{[0,1]^n} \left(\int_Z p_Z^n \mathrm{d}\Pi(Z)\right)^2 \mathrm{d}x -1.
\end{align*}
Since $\int P_Z^n d\Pi(Z)$ is a mixture of product distributions over $\{P_Z^n: Z \in \calZ\}$ with weights $\Pi$, we can expand the first term in the right-hand side as
\begin{align*}
    \int_{[0,1]^n} \left(\int_Z p_Z^n \mathrm{d}\Pi(Z)\right)^2 \mathrm{d}x
= \mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \Pi}\left(\int p_Z(y)p_{Z'}(y) \mathrm{d}x\right)^n.
\end{align*}
By taking the density $p_Z(y) = \frac{\mathbb{I}\{x\in A_Z\}}{\frac{1}{m}\sum_{j=1}^mZ_j}$ into the equation, we have
\begin{align*}
    \mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \Pi}\left(\int p_Z(y)p_{Z'}(y) \mathrm{d}x\right)^n
= \mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \Pi}\left(\frac{1}{\frac{1}{m}\sum_{j=1}^mZ_j}\cdot \frac{1}{\frac{1}{m}\sum_{j=1}^mZ'_j}\cdot \frac{1}{m}\sum_{j=1}^mZ_jZ_j'\right)^n.
\end{align*}
According to the construction of $\calZ$, for any vector $Z \in \calZ$, we have $\frac{1}{m}\sum_{j=1}^mZ_j \geq \beta - \left(\frac{\beta}{m}\right)^{1/3}$. Since $\tilde\Pi$ is the product of Bernoulli distribution with probability $\beta$, by the Chernoff bound, we have $\tilde\Pi(Z \in \calZ) \geq 1 - (\beta/m)^{1/3}$. Thus, we have
\begin{eqnarray*}
&&\mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \Pi}\left(\int p_Z(y)p_{Z'}(y) \mathrm{d}x\right)^n\\
&\leq& \left(\beta-\left(\frac{\beta}{m}\right)^{1/3}\right)^{-2n}\mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \Pi}\left(\frac{1}{m}\sum_{j=1}^mZ_jZ_j'\right)^n \\
&=& \left(\beta-\left(\frac{\beta}{m}\right)^{1/3}\right)^{-2n}\frac{\mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \tilde{\Pi}}\left(\frac{1}{m}\sum_{j=1}^mZ_jZ_j'\right)^n\mathbb{I}\{Z\in\mathcal{Z}\}\mathbb{I}\{Z'\in\mathcal{Z}\}}{\tilde{\Pi}(Z\in \mathcal{Z})^2} \\
&\leq& \left(\beta-\left(\frac{\beta}{m}\right)^{1/3}\right)^{-2n}\left(1-\left(\frac{\beta}{m}\right)^{1/3}\right)^{-2}\mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \tilde{\Pi}}\left(\frac{1}{m}\sum_{j=1}^mZ_jZ_j'\right)^n.
\end{eqnarray*}
The last term on the right-hand side can be bounded by
\begin{eqnarray*}
 \mathbb{E}_{Z,Z'\stackrel{iid}{\sim} \tilde{\Pi}}\left(\frac{1}{m}\sum_{j=1}^mZ_jZ_j'\right)^n 
&\leq& \left(\beta^2 + \left(\frac{\beta^2}{m}\right)^{1/3}\right)^n + \tilde{\Pi}\left(\frac{1}{m}\sum_{j=1}^mZ_jZ_j'>\beta^2 + \left(\frac{\beta^2}{m}\right)^{1/3}\right) \\
&\leq& \left(\beta^2 + \left(\frac{\beta^2}{m}\right)^{1/3}\right)^n + \left(\frac{\beta^2}{m}\right)^{1/3},
\end{eqnarray*}
where the first inequality is using $\frac{1}{m}\sum_{j=1}^mZ_jZ_j' \leq 1$ for $\frac{1}{m}\sum_{j=1}^mZ_jZ_j' > \beta^2 + \left(\frac{\beta^2}{m}\right)^{1/3}$ and the second inequality is from the Chernoff bound on the sum of $m$ Bernoulli variables with probability $\beta^2$.
Combining all bounds above, we have
$$\frac{1}{2}\TV\left(\lambda^n,\int P_Z^n \mathrm{d}\Pi(Z)\right)^2\leq \frac{\left(\beta^2 + \left(\frac{\beta^2}{m}\right)^{1/3}\right)^n + \left(\frac{\beta^2}{m}\right)^{1/3}}{\left(\beta-\left(\frac{\beta}{m}\right)^{1/3}\right)^{2n}\left(1-\left(\frac{\beta}{m}\right)^{1/3}\right)^{2}}-1.$$
It is clear that the above bound tends to zero when $m\rightarrow\infty$. Therefore, for any $\delta > 0$, we have
 $\TV\left(\lambda^n,\int P_Z^n \mathrm{d}\Pi(Z)\right)\leq\delta$ for a sufficiently large $m$.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:nest}]
The result is a direct consequence of the nested property of $\{S_j(x)\}_{j\in[m]}$.
\end{proof}




