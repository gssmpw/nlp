%
%
% \anote{Just collecting papers and organizing them. Need to work on it. And can be moved as needed.}
\section{Related Work} \label{sec:related}

% \anote{Needs work. But may need to put in appendix.}
% \cnote{agree to put in the appendix}

\iffalse
Starting with the foundational work of \citet{gammerman1998learning}, conformal prediction has provided flexible distribution-free methods that work under just an exchangeability assumption on the joint distribution of samples, and construct prediction sets that satisfy the necessary coverage requirement. Over the past several years, there have been several different methods that build on the original conformal prediction method and differ in the form of coverage guarantee \citep[see e.g.,][for a detailed review of prior work]{Vovkbook, angelopoulos2023survey}. 

\paragraph{Different forms of coverage} As described earlier, the classic coverage requirement is also called {\em marginal coverage} guarantee, since it is on average across the test samples. %, the prediction sets will contain the true value with the desired probability. 
A stronger notion called {\em conditional coverage} requires the prediction set to have the same coverage guarantee conditional on the specific feature vector $X_{n+1} \in \mathcal{X}$. It is well known that the conditional coverage property cannot be achieved without additional assumptions on $P$ \citep{vovk2012conditional,lei2014distribution,foygel2021limits}. An intermediate notion of coverage is {\em group coverage} where there are $k$ groups, and there is a marginal coverage requirement of $1-\alpha$ for the distribution condition on each of the groups~\citep{angelopoulos2023survey, jung2022BatchMC}. A more flexible framework is through covariate shifts, where the density of the covariates can be reweighted according to a function $w(x)$ from a family of functions $\mathcal{W}$~\citep{Gibbs2023conformal}. Through appropriate choices of the family $\mathcal{W}$ one can capture the different coverage notions, develop algorithms for certain classes of covariate shifts~\citep[see e.g.,][]{angelopoulos2023survey, Gibbs2023conformal, kiyani2024length}. 

To overcome the challenges with conditional coverage, other approaches have also been proposed in the past few years. One line of work assumes black-box access to the CDF of the conditional distribution $Y|X$ or quantiles of this distribution ~\citep{romano2019conformalized, chernozhukov2021distributional}. The developed methods provide conditional coverage guarantees that depend on how accurate these estimates of the the conditional distribution are. %This has an appealing similarity with the existing philosophy in conformal prediction, where we obtain guarantees when since we assume black-box access to the conditional distribution   
Our work fits into this framework developed in \citep{chernozhukov2021distributional}, and provides volume optimality guarantees in this setting.  


%In marginal coverage, for a sample $x$, the goal is to output a prediction set $y$ such that $\Pr_{x \sim \mathcal{D}}[]$

%Surveys ~\cite{Vovkbook,angelopoulos2023survey}. 

%Marginal coverage \cite{Vovkbook}; Group coverage \cite{jung2022BatchMC}. \citet{Gibbs2023conformal}

%
%\paragraph{Conditional Coverage}
%\citet{romano2019conformalized}
%\citet{chernozhukov2021distributional}

%Potentially \citet{xie2024boosted} 
%Mention how all of these have mostly focussed on coverage, and empirically evaluate volume/ size of the prediction set. 

% Volume Optimality
\paragraph{Volume Optimality} 
The volume of the prediction set in conformal prediction is also sometimes referred to as `efficiency' has been stated as an important consideration in many prior works \citep[see e.g.,][]{shafer2008tutorial, angelopoulos2023survey}. However, most works that we are aware of do not give theoretical guarantees of volume optimality, and mainly reason about volume control through empirical evaluations. As mentioned in the introduction, notable exceptions include the work of \citet{Lei2013DistributionFreePS} in the unlabeled setting that uses a kernel density estimator, and the work of \citet{Sadinle2016LeastAS} in the set value setting.  The works that are most relevant to our work are \citet{Lei2013DistributionFreePS} for the unlabeled setting, and the very recent work of \citet*{izbicki2020flexible,izbicki2022cd} and \citet{kiyani2024length} in the regression setting. We now compare our work to \cite{Lei2013DistributionFreePS}, \citet{izbicki2020flexible,izbicki2022cd} and \cite{kiyani2024length}.  

\fi

\paragraph{Comparison to \citet{Lei2013DistributionFreePS}} 
The influential work of \citet{Lei2013DistributionFreePS}  gave the first theoretical guarantees of volume control or optimality to the best of our knowledge. 
In fact \citet{Lei2013DistributionFreePS} and subsequent follow-up works including \citet{Sadinle2016LeastAS, chernozhukov2021distributional} with theoretical guarantees on volume control study a stricter quantity that corresponds to the volume of set difference $\vol\left(\widehat{C}\Delta C_{\rm opt}\right)$ \citep{Lei2013DistributionFreePS,Sadinle2016LeastAS, chernozhukov2021distributional}. However, this much stronger notion requires that the optimal solution $C_{\rm opt}$ must not only exist but also be unique. Usually additional assumptions need to be imposed in the neighborhood of the boundary of $C_{\rm opt}$ in order that the set difference vanishes in the large sample limit. 
Specifically, the work of \citep{Lei2013DistributionFreePS} assumes that the density is smooth, and in addition is strictly increasing or decreasing significantly. 
In comparison, our notion of volume optimality only requires the volume to be controlled, which can be achieved even if $\widehat{C}$ is not close to $C_{\rm opt}$, or when $C_{\rm opt}$ does not even exist. 
Moreover, we do not need to make any assumptions on the smoothness of the density. In fact, the density may not even exist, and can have discrete point masses or $\delta$ functions as shown in the experiments. Indeed, from a practical point of view, any set with coverage and volume control would serve the purpose of valid prediction. Insisting the closeness to a questionable target $C_{\rm opt}$ comes at the cost of unnecessary assumptions on the data generating process.

\paragraph{Comparison to \citet{izbicki2020flexible,izbicki2022cd}}

The work of \citet{izbicki2020flexible,izbicki2022cd} provided conformal prediction methods that can produce a union of intervals in a supervised setting. Specifically, their methods, CD-split and HPD-split, are designed to leverage level sets of an estimated conditional density function. CD-split achieves local and marginal validity by partitioning the feature space adaptively but does not guarantee conditional coverage in general. 
In contrast, HPD-split simplifies tuning by using a conformity score based on the cumulative distribution function of the conditional density. Under certain assumptions of density estimation accuracy and the uniqueness of the optimal solution, HPD-split achieves asymptotic conditional coverage and converges to the highest predictive density
set which is the smallest volume set with the desired coverage.
In comparison, our method outputs a union of intervals with the smallest length from a direct estimator of the conditional CDF, which only requires the accuracy of conditional CDF estimation. Estimating the conditional CDF is statistically simpler than estimating the conditional density, which usually requires additional smoothness or regularity conditions.


\paragraph{Comparison to \citet{kiyani2024length}} In very recent independent work, \citet{kiyani2024length} considered a min-max approach for conformal prediction in the covariate shift setting with a view towards length optimality of their intervals. They proposed a new method based on minimax optimization to optimize the average volume of prediction sets in the context of covariate shift, which generalizes the marginal or group-conditional coverage setting. 
Their method uses a given (predefined) conformity score, and optimizes the choice of the thresholds $h(X)$ for different covariates $X \in \mathcal{X}$ %by selecting the threshold depending on the covariate value 
to minimize the average prediction interval length, while maintaining the marginal or group-conditional coverage. Under certain assumptions that the conformity score is consistent with a volume optimal prediction set, they show that solving their minimax optimization will give a volume-optimal solution. However the problem of finding the best threshold function $h(X)$ is a non-convex problem that may be computational inefficient in theory; but they use SGD to find a good heuristic solution in practice. This work is incomparable to this paper in multiple ways. While \citet{kiyani2024length} considers the covariate shift setting with a specific focus on marginal coverage and group coverage, we focus more on the unlabeled setting, and the conditional coverage setting of \citet{chernozhukov2021distributional}. 
In contrast to their method that uses an off-the-shelf conformity score (and optimizes the thresholds $h(X)$), our method introduces a new conformity score function based on dynamic programming to find volume-optimal unions of intervals. This also suggests that our methods and the methods of \citet{kiyani2024length} may potentially be complementary. Finally, by restricting the prediction sets to unions of $k$ intervals, we got theoretical guarantees of volume optimality and get polynomial time algorithms based on dynamic programming to achieve them. Hence, while both their work and our work try to address the important consideration of volume optimality, they are incomparable in terms of the setting, the results and the techniques.

\paragraph{Other Related work}

In the non-conformal setting, the work of \citep{scott2006learning} studied the problem of finding minimal volume sets from a certain set family given samples drawn i.i.d from a distribution, with at least $1-\alpha$ fraction of probability mass. However this work mostly focused on statistical efficiency, and did not consider the conformal inference setting. 
In the past few years, there has been an explosion of literature in conformal inference that develops new conformal methods for various settings \citep[see e.g., ][and references therein]{barber2021predictive, stutz2022learning,Kumar2023ConformalPW, barber2023beyond, xie2024boosted}. To the best of our knowledge these works do not provide theoretical guarantees on volume optimality. 