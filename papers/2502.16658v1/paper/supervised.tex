\section{Supervised Setting}\label{sec:labeled}


\subsection{Problem Setting}

In this section, we consider conformal prediction with labeled data. Suppose data points $(X_1,Y_1)$,$\cdots$, $(X_{2n},Y_{2n})$,$(X_{2n+1},Y_{2n+1})$ are $i.i.d.$ drawn from a distribution $P$ on $\calX \times \calY$ with $\calY = \R$. Using the first $2n$ samples, our goal is to compute a prediction set $\widehat{C}(x)$ for each $x \in \calX$. We will study the following properties for the prediction set.
\begin{enumerate}
\item \textit{Marginal Coverage:}
$$
\mathbb{P}\left(Y_{2n+1} \in \widehat{C}(X_{2n+1})\right)\geq 1-\alpha,
$$
where the probability $\mathbb{P}$ is jointly over all $2n+1$ pairs of observations.
\item \textit{Conditional Coverage:}
\begin{equation}
\mathbb{P}\left(Y_{2n+1}\in \widehat{C}(X_{2n+1}) \mid X_{2n+1}\right)\geq 1-\alpha, \label{eq:con-co}
\end{equation}
with high probability.
\end{enumerate}
It is well known that the conditional coverage property cannot be achieved without additional assumptions on $P$ \citep{vovk2012conditional,lei2014distribution,foygel2021limits}. Therefore, some form of relaxation of (\ref{eq:con-co}) is necessary.

In addition to the coverage properties listed above, we will also extend the notion of restricted volume optimality (\ref{eq:res-opt}) from the unsupervised setting to the supervised setting. Define the conditional CDF by
$$F(y\mid x)=\mathbb{P}\left(Y_{2n+1}\leq y \mid X_{2n+1}=x\right).$$
The conditional restricted optimal volume is given by
\begin{eqnarray}
\nonumber && \opt_k\left(F(\cdot\mid x),1-\alpha\right) \\
\nonumber  &=& \inf\left\{\vol(C): \int_C\mathrm{d} F(\cdot\mid x)\geq 1-\alpha, C\in\mathcal{C}_k\right\}.
\end{eqnarray}
With this definition, we can list the following volume requirement.
\begin{enumerate}
  \setcounter{enumi}{2}
  \item \textit{Conditional Restricted Volume Optimality:}
\begin{equation}
\hspace{-2.5em}
\vol(\widehat{C}(X_{2n+1}))\leq \opt_k\left(F(\cdot|X_{2n+1}),1-\alpha+\epsilon\right), \label{eq:con-vo}
\end{equation}
with high probability, for some $\epsilon\in(0,\alpha)$.
\end{enumerate}
Similar to the conditional coverage property (\ref{eq:con-co}), the conditional restricted volume optimality (\ref{eq:con-vo}) is only required for a typical value of the design point. We will show that based on an extension of distributional conformal prediction \citep{chernozhukov2021distributional}, these two properties can be achieved under the same assumption.


\subsection{Distributional Conformal Prediction}
%According to the above result, one does not need to use conformal prediction to achieve coverage and restricted volume optimality. However, a conformalizing step is necessary in a supervised setting to guarantee marginal coverage even under model misspecification. 
Conformal prediction based on estimating the conditional CDF has been considered independently by \cite{izbicki2020flexible,chernozhukov2021distributional}. 
We will briefly review the version by~\citet{chernozhukov2021distributional}, and then extend it to serve our purpose. Suppose $\widehat{F}(y\mid x)$ is an estimator of the conditional CDF, which is computed from the first half of the data $(X_1,Y_1),\cdots,(X_n,Y_n)$.
The prediction set proposed by~\citet{chernozhukov2021distributional} is
$$\widehat{C}_{\rm DCP}(X_{2n+1})=\left\{y\in\mathbb{R}:\left|\widehat{F}(y\mid X_{2n+1})-\frac{1}{2}\right|\leq \widehat{t}\right\},$$
where $\widehat{t}$ is an appropriate quantile of $$\left\{\left|\widehat{F}(Y_{n+1}\mid X_{n+1})-\frac{1}{2}\right|,\cdots,\left|\widehat{F}(Y_{2n}\mid X_{2n})-\frac{1}{2}\right|\right\}.$$
Since $\widehat{C}_{\rm DCP}(X_{2n+1})$ is in the form of split conformal prediction, the marginal coverage property is automatically satisfied. When $\widehat{F}(y\mid x)$ is close to $F(y\mid x)$ in some appropriate sense, it was proved by \cite{chernozhukov2021distributional} that asymptotic conditional coverage also holds. However, in general, $\widehat{C}_{\rm DCP}(X_{2n+1})$ is not optimal in terms of its volume. A modification was also proposed in \cite{chernozhukov2021distributional} to achieve volume optimality within the class of intervals. Though not explicitly stated in \cite{chernozhukov2021distributional}, we believe that the DCP procedure essentially achieves (\ref{eq:con-vo}) for $k=1$. Our goal is to achieve the conditional restricted volume optimality for a general $k$ by combining the ideas of DCP and dynamic programming (DP).

\subsection{DCP meets DP}\label{sec:DCP-DP}

To achieve (\ref{eq:con-vo}) for a general $k$, we will modify the DCP procedure by considering a different conformity score that generalizes (\ref{eq:score-un}) to the supervised setting. Recall that $\widehat{F}(y\mid x)$ is an estimator of the conditional CDF, and it is computed from the first half of the data $(X_1,Y_1),\cdots,(X_n,Y_n)$. Our first step is to construct a nested system for each $x\in\calX$. To be specific, for each $x\in\calX$, we will construct a collection of sets $\{S_j(x)\}_{j\in[m]}$ based on the function $\widehat{F}(\cdot\mid x)$. The requirement of the nested system is summarized as the following assumption.

\begin{assumption}\label{as:ne-su}
The sets $S_1(x)\subset\cdots\subset S_m(x)\subset\mathbb{R}$ are measurable with respect to the $\sigma$-field generated by $\widehat{F}(\cdot\mid x)$. Moreover, for some positive integer $k$, some $\alpha\in(0,1)$ and some $\delta,\gamma$ such that $3\delta+\gamma+n^{-1}\leq \alpha$, we have
\begin{enumerate}
\item $\int_{S_j(x)}\mathrm{d} \widehat{F}(\cdot\mid x)=\frac{j}{m}$ and $S_j\in\calC_k$ for all $j\in[m]$.
\item There exists some $j^*\in[m]$, such that $\int_{S_{j^*}(x)}\mathrm{d} \widehat{F}(\cdot\mid x)\geq 1-\alpha+n^{-1}+3\delta$ and $\vol(S_{j^*})\leq \opt_k(\widehat{F}(\cdot\mid x),1-\alpha+\frac{1}{n}+3\delta+\gamma)$.
\end{enumerate}
\end{assumption}
The construction of nested systems satisfying Assumption \ref{as:ne-su} is similar to that in the unsupervised setting. That is, one can apply dynamic programming (Algorithm \ref{alg:dp}) and obtain $S_{j^*}(x)$, and the rest of the sets can be constructed via the greedy expansion/contraction procedure described in Section \ref{sec:con-nest-m} to satisfy $\int_{S_j(x)}\mathrm{d} \widehat{F}(\cdot\mid x)=\frac{j}{m}$.  The main difference here is that Algorithm \ref{alg:dp} is directly applied to the data in the unsupervised setting, while we only have access to $\widehat{F}(\cdot\mid x)$ in the supervised setting. This issue can be easily addressed by computing quantiles $Y_1(x),\cdots,Y_L(x)$ from $\widehat{F}(\cdot\mid x)$ on a grid, and then apply Algorithm \ref{alg:dp} with $Y_1(x),\cdots,Y_L(x)$ as input. Indeed, since the distance between $\widetilde{F}(\cdot\mid x)$ and $\widehat{F}(\cdot\mid x)$ can be controlled by the size of the grid with $\widetilde{F}(y\mid x)=\frac{1}{L}\sum_{l=1}^L\mathbb{I}\{Y_l(x)\leq y\}$, Assumption \ref{ass:F}, which will be stated later in Section \ref{sec:theory}, is also satisfied by $\widetilde{F}(y\mid x)$ (with a slightly larger value of $\delta$) by triangle inequality.

The computational cost of constructing $\{S_j(x)\}_{j\in[m]}$ for a single $x\in\calX$ is $O(L^3k/\gamma)$. Note that there is no need to repeat the construction for each individual $x\in\calX$. Since the split conformal framework only requires evaluating the conformity score at $(X_{n+1},Y_{n+1}),\cdots,(X_{2n},Y_{2n}),(X_{2n+1},y)$, it is sufficient to compute $\{S_j(X_i)\}_{j\in[m]}$ for $i=n+1,\cdots,2n+1$, which leads to the total computational cost $O(nL^3k/\gamma)$.

With nested systems satisfying Assumption \ref{as:ne-su}, the conformity score in the supervised setting is defined as
\begin{equation*}
q(y,x) = \sum_{j=1}^m\mathbb{I}\{y\in S_j(x)\}.
\end{equation*}
Let $q_1\leq \cdots\leq q_n$ be the order statistics computed from the set
\begin{equation*}
\left\{q(X_{n+1},Y_{n+1}),\cdots, q(X_{2n},Y_{2n})\right\}.
\end{equation*}
The prediction set for $Y_{2n+1}$ is constructed as
\begin{equation*}
\widehat{C}_{\rm DCP-DP}(X_{2n+1}) = \left\{y: q(y,X_{2n+1})\geq q_{\lfloor(n+1)\alpha\rfloor}\right\}. \label{eq:dcp-dp}
\end{equation*}

\subsection{Theoretical Guarantees}\label{sec:theory}

We will show in this section that $\widehat{C}_{\rm DCP-DP}(X_{2n+1})$ satisfies marginal coverage. Moreover, when $\widehat{F}(y\mid x)$ is close to $F(y\mid x)$ in some appropriate sense, it also satisfies approximate conditional coverage and conditional restricted volume optimality. Given two CDFs $\widehat{F}$ and $F$, we define the $(k,\infty)$ norm of the difference by
$$\|\widehat{F}-F\|_{k,\infty}=\sup_{C\in\calC_k}\left|\int_C\mathrm{d}\widehat{F}-\int_C\mathrm{d}F\right|.$$

\begin{assumption}\label{ass:F}
The estimated conditional CDF $\widehat{F}(y \mid x)$ satisfies 
$$\mathbb{P}\left(\|\widehat{F}(\cdot \mid X_{2n+1})-F(\cdot \mid X_{2n+1})\|_{k,\infty}\leq\delta\right)\geq 1-\delta,$$
where $\delta$ takes the same value as the one in Assumption \ref{as:ne-su}.
\end{assumption}

The theoretical properties of $\widehat{C}_{\rm DCP-DP}(X_{2n+1})$ are given by the theorem below.

\begin{theorem}\label{thm:supervised}
Consider i.i.d. observations $(X_1,Y_1)$, $\dots$, $(X_{2n},Y_{2n})$, $(X_{2n+1},Y_{2n+1})$ generated by some distribution $P$ on $\calX \times \R$. The conformal prediction set $\widehat{C}_{\rm DCP-DP}(X_{2n+1})$ is computed from nested systems $\{S_j(\cdot)\}_{j\in[m]}$ and $\widehat{F}(\cdot\mid\cdot)$ satisfying Assumption \ref{as:ne-su} and Assumption \ref{ass:F}. Suppose the parameter $\delta$ in the two assumptions satisfies $\delta^2\geq \frac{\log(2\sqrt{n})}{2n}$. Then the following properties hold. 
    \begin{enumerate}
        \item Marginal coverage, 
        $$\mathbb{P}\left(Y_{2n+1} \in \widehat{C}_{\rm DCP-DP}(X_{2n+1})\right)\geq 1-\alpha.$$
        \item Approximate conditional coverage,
        $$\mathbb{P}\left(Y_{2n+1}\in \widehat{C}_{\rm DCP-DP}(X_{2n+1})| X_{2n+1}\right)\!\geq\! 1-\alpha-3\delta,$$
        with probability at least $1-\delta$.
        \item Conditional restricted volume optimality,
        \begin{align*}
        &\vol\left(\widehat{C}_{\rm DCP-DP}(X_{2n+1})\right) \\
        \leq& \opt_k\left(F(\cdot\mid X_{2n+1}),1-\alpha+\frac{1}{n}+4\delta+\gamma\right),
        \end{align*}
        with probability at least $1-2\delta$.
    \end{enumerate} 
\end{theorem}

Theorem \ref{thm:supervised} can be regarded as a generalization of Theorem \ref{thm:unsupervised}. Indeed, when $F(\cdot\mid x)$ does not depend on $x$ and $\widehat{F}(\cdot\mid x)$ is defined as the empirical CDF of $Y_1,\cdots,Y_n$, Theorem \ref{thm:supervised} recovers Theorem \ref{thm:unsupervised}. Moreover, since the volume optimality is over all sets that are unions of $k$ intervals, it also covers the length optimality of intervals considered by \cite{chernozhukov2021distributional}. The case $k\geq 2$ will be important if the conditional density of $Y$ given $X$ has multiple modes; Gaussian mixture is a leading example.




\iffalse
To close this section, we discuss the role of $m$. It seems that the theoretical results do not depend on $m$. This is actually not true. From (\ref{eq:eq-mass}) and (\ref{eq:dp-c}), we know that the set $S_{j^*}$ computed by dynamic programming is indexed by $j^*$ satisfying
$$\frac{j^*}{m}\geq 1-\alpha+\frac{1}{n}+3\delta.$$
Moreover, in order that the volume guarantee (\ref{eq:dp-v}) holds, the dynamic programming also requires the above inequality becomes an approximate identity, which leads to the condition $m\gg \delta^{-1}$. Therefore, we could set the default choice as $m=\sqrt{n}$.

Our theory suggests that only $S_{j^*}$ needs to be computed by the dynamic programming. The rest of the nested system does not matter. This is true if the knowledge of $\delta$ is available. Then, with $m$ sufficiently large, we can set $j^*\approx m\left(1-\alpha+\frac{1}{n}+3\delta\right)$. In practice, however, one needs to guess $\delta$ according to some conservative estimate of the error of $\hat{F}(\cdot|\cdot)$. On the other hand, suppose we know that $\delta\in [\delta_{\min},\delta_{\max}]$. As long as we have volume guarantee for all $S_j$ with $j\in\left[m\left(1-\alpha+\frac{1}{n}+3\delta_{\min}\right),m\left(1-\alpha+\frac{1}{n}+3\delta_{\max}\right)\right]$, the algorithm would automatically adapt to the correct $\delta$ in the range. In other words, volume guarantees for more than one set in the system would alleviate the need of knowing $\delta$.
\fi