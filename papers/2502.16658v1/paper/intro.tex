\section{Introduction}

%\anote{Short Para on conformal prediction.}
Conformal inference has emerged as a powerful black-box method for quantifying uncertainty in model predictions, providing confidence sets or prediction sets that contain the true value with a specified probability%, typically $1-\alpha$
~\citep{gammerman1998learning,Vovkbook}. Consider a prediction problem where $\mathcal{X}$ is the covariate space (feature space), and $\mathcal{Y}$ is the label space.  Given a dataset of $n$ labeled samples $(X_1, Y_1), \dots, (X_n, Y_n) \in \mathcal{X} \times \mathcal{Y}$, a conformal prediction algorithm uses these $n$ samples (often called calibration samples) to construct for a test $X_{n+1} \in \mathcal{X}$ with (unknown) true value $Y_{n+1} \in \mathcal{Y}$, a prediction set that we will denote by $\widehat{C}(X_{n+1}) \subset \mathcal{Y}$,\footnote{It may be more accurate to use $\widehat{C}(X_1, Y_1, \dots, X_n,Y_n, X_{n+1})$ instead of $\widehat{C}(X_{n+1})$ to reflect that $\widehat{C}$ is a function of the calibration samples and $X_{n+1}$.} 
satisfying the {\em coverage} requirement for some desired parameter $\alpha \in (0,1)$:
\begin{equation}\label{eq:intro:coverage}
 \mathbb{P}\left(Y_{n+1} \in \widehat{C}(X_{n+1})\right)\geq 1-\alpha.
\end{equation}

 Here the probability $\mathbb{P}$ refers to the joint distribution over all $n+1$ pairs of observations $(X_1, Y_1), \dots, (X_n, Y_n), (X_{n+1}, Y_{n+1})$ including the test sample $(X_{n+1}, Y_{n+1})$. 
 %\footnote{The samples $(X_1,Y_1), \dots, (X_n, Y_n)$ are often called calibration samples, and $(X_{n+1}, Y_{n+1})$ is the test sample.}.
 Unlike traditional approaches, conformal inference is distribution-free, relying only on the assumption of exchangeability of the joint distribution $\mathbb{P}$  over the $(n+1)$ samples. %$(X_1, Y_1), \dots, (X_n, Y_n), (X_{n+1}, Y_{n+1})$ between calibration and test samples.
%\anote{Can we mention: In this paper, we will focus on $\mathcal{X}=\mathbb{R}^d$ and $\mathcal{Y}=\mathbb{R}$ or something like this?}

\iffalse
Over the past several years, there has been an explosion of works that build on the original conformal prediction method and differ in the form of coverage guarantee. The coverage requirement \eqref{eq:intro:coverage} is referred to as a {\em marginal coverage} guarantee, since it is on average across the test samples. %, the prediction sets will contain the true value with the desired probability. 
A stronger notion called {\em conditional coverage} requires the prediction set to have the same coverage guarantee conditional on the specific feature vector $X_{n+1} \in \mathcal{X}$:
 \begin{equation}\label{eq:intro:co-co}
 \mathbb{P}\left(Y_{n+1}\in \widehat{C}(X_{n+1}) \mid X_{n+1}\right)\geq 1-\alpha. 
 \end{equation}
% with high probability.
% \end{enumerate}
It is well known that the conditional coverage property cannot be achieved without additional assumptions on $P$ \citep{vovk2012conditional,lei2014distribution,foygel2021limits}. 
Over the past few years, several approaches have also been proposed to overcome this, including additional assumptions on the conditional distribution ~\citep{romano2019conformalized, chernozhukov2021distributional} and intermediate notions of coverage like group coverage and coverage under certain classes of covariate shifts~\citep[see e.g.,][and the related work section in appendix]{angelopoulos2023survey, jung2022BatchMC, Gibbs2023conformal}. 

With these rapid advancements, conformal inference  %holds strong promise as an uncertainty quantification tool 
holds much promise as a tool for making informed and reliable decisions using machine learning models across a wide range of applications in  healthcare, finance, and even in complex neural networks ~\citep[see e.g.,][for a recent survey]{angelopoulos2023survey}. %\anote{more citations here.} 
Yet there remain fundamental questions in conformal prediction that are poorly understood even in simple settings. 
\fi


While most conformal methods provide guarantees on coverage, they do not provide any control on the size or volume of the prediction sets; in fact, the trivial choice of $\widehat{C}(X_{n+1})=\mathcal{Y}$ also satisfies the coverage requirement. Consequently, the size of these sets is often validated empirically, without formal guarantees. 
This raises the important question of {\em volume optimality}, which is the focus of this paper:\\

\vspace{-15pt}
\noindent {\bf Question:} {\em Given calibration samples $(X_1, Y_1),\dots,$ $(X_n, Y_n)$ drawn i.i.d. from a distribution $P$, can we find among all data-dependent sets $\widehat{C} \subset \mathcal{Y}$ satisfying the desired coverage requirement for $(X_{n+1}, Y_{n+1}) \sim P$, the one with the smallest volume, as quantified by the Lebesgue measure
$\vol(\widehat{C})=\lambda(\widehat{C})?$}

The volume of the prediction set in conformal prediction is also sometimes referred to as `efficiency' has been stated as an important consideration in many prior works \citep[see e.g.,][]{shafer2008tutorial, angelopoulos2023survey}. However, most works that we are aware of do not give theoretical guarantees of volume optimality, and mainly reason about volume control through empirical evaluations.

There are few works that provide guarantees of volume optimality.\footnote{Some works also guarantee that the coverage is not much more than $1-\alpha$, e.g., $\mathbb{P}\left(Y_{n+1}\in \widehat{C}(X_{n+1}) \mid X_{n+1}\right) \le  1-\alpha + o(1)$ to argue that the prediction set is not too big. However, smallness according to the measure $\mathbb{P}$ does not necessarily reflect a small volume (or Lebesgue measure) for the set. %(e.g., from any prediction set with coverage $1-\alpha+\beta$, one can remove an arbitrary region, potentially a tiny volume region of mass $\approx \beta$). 
} Notable exceptions include \citet{Lei2013DistributionFreePS} in the unsupervised setting,  \citet{Sadinle2016LeastAS} in the set value setting, and recent works of \citet*{izbicki2020flexible,izbicki2022cd} and \citet{kiyani2024length} in the regression setting. As summarized by \cite{angelopoulos2024theoretical}, a sufficient condition that leads to volume optimality of conformal prediction is consistent estimation of the conditional density function of $Y$ given $X$. This is essentially the strategy adopted by previous work \citep{Lei2013DistributionFreePS,izbicki2020flexible,izbicki2022cd}. In comparison, our method, by incorporating a framework of \cite{chernozhukov2021distributional}, builds on the estimation of the conditional CDF via a new conformity score computed by dynamic programming, and thus also works in settings where good conditional density estimation is impossible or density does not even exist.

% One notable example is the work of \citet{lei2014distribution} that gives guarantees for a stronger notion of volume optimality, but under strong assumptions about smoothness and growth of the density function -- under these assumptions, level sets of a good kernel density estimate (KDE) are nearly volume optimal. (Please refer to Appendix~\ref{sec:related} for other related work and comparisons.) %~\citep{kiyani2024length}.

%This limitation raises important questions about volume optimalityâ€”whether it is possible to design methods that consistently generate small prediction sets while maintaining valid coverage

\subsection{Our Results}

\noindent {\bf An Impossibility Result.} We first prove an impossibility result in a one-dimensional setting where any distribution-free method that satisfies the coverage requirement can only find a trivial solution whose volume is sub-optimal. See Theorem~\ref{thm:impossibility} for a formal statement. This result provides an explanation for the lack of such volume-optimality guarantees in the conformal prediction literature, and also motivates our new notion of volume-optimality that we introduce in this work. 

%\anote{Yet we don't understand basic things in simple setup. How existing methods do not give guarantees for size/ volume. They empirically validate size of the prediction set.  } \anote{Also called efficiency sometimes, and footnote about how it is not the same as getting probability = $1-\alpha$.}
%\anote{Volume optimality and its impossibility.}

\paragraph{Structured Prediction Sets and Restricted Volume Optimality.}

%We consider a collection of sets $\mathcal{C} \subset 2^{\mathcal{Y}}$, of bounded VC-dimension. The set family $\mathcal{C}$ comprises all the candidate  sets for the prediction task. For regression tasks when $\mathcal{Y}=\mathbb{R}$, a natural family that we study is $\mathcal{C}_k$ which is the collection of all unions of $k$ intervals; intervals are easy to interpret, and prediction intervals are often used in most applications~\citep{angelopoulos2023survey,barber2021predictive}). %We require that the VC-dimension of the family $\mathcal{C}$ is bounded, by say $d$.   


Motivated by the impossibility result, our goal is to find a prediction set $\widehat{C} \in \mathcal{C}\subset 2^{\mathcal{Y}}$ whose volume is competitive with the optimum volume of any set in the family $\mathcal{C}$ as given by
\begin{equation} \label{eq:intro:optC}
% \opt_k(P,1-\alpha) = \inf\left\{\vol(C):P(C)\geq 1-\alpha, C \in \calC_k \right\}.\label{eq:res-opt}
\opt_{\mathcal{C}}(P, 1-\alpha) = \inf_{C \in \mathcal{C}} \left\{\vol(C) : P(C) \geq 1-\alpha \right\}. 
\end{equation}
As long as $\mathcal{C}$ has bounded VC-dimension, for any distribution $P$ we can obtain good empirical estimates of the probability measure of any set $C \in \mathcal{C}$ via a standard uniform concentration inequality, which allows us to overcome the impossibility result in Theorem~\ref{thm:impossibility}.
%This focus on the family of prediction sets differs from the predominant approach of using conformity score functions in existing conformal approaches. 
In the rest of the paper, we focus on the setting when $\mathcal{Y}=\mathbb{R}$ and $\mathcal{C}= \mathcal{C}_k$ which is the collection of unions of $k$ intervals.

%We remark that when the density of $P$ has at most $k$ modes or when it can be well approximated by a kernel density estimator, then this becomes equivalent to the (unrestricted) notion of volume optimality (see Remark~\ref{rem:noloss:smooth} and Appendix~\ref{sec:DPvsKDE}). 

% \anote{Our restricted notion of optimality}
% \anote{Quick line about why VC-dimension comes in.}

% \anote{Instantiation with Unions of $k$-Intervals}

% \anote{Why the change of perspective helps. How it implies unrestricted volume optimality in nice settings.}

% \anote{Conformity score.}
% \anote{How it is not lossy in nice/smooth settings.}

%\anote{Short paragraph on main contribution.}
\paragraph{Conformalized Dynamic Programming.}

Equipped with our new notion of volume optimality, we propose a new conformity score based on dynamic programming, the proposed method is shown to not only achieve approximate conditional coverage as in \citep{chernozhukov2021distributional} and \citep{romano2019conformalized}, but also conditional volume optimality with respect to unions of $k$ intervals, as long as a reasonable estimator of the conditional CDF is available. Our method of learning a predictive set via CDF can be regarded an extension of the framework of \cite{izbicki2020flexible,chernozhukov2021distributional}.

\iffalse

Equipped with our new notion of volume optimality, we design new conformal prediction methods that output unions of intervals as prediction sets with theoretical volume-optimality and coverage guarantees, and empirical evaluations that demonstrate the advantage of our methods in many settings. These positive results can be summarized as follows:

\vspace{-10pt}
\begin{itemize}
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\item {\em Unlabeled setting:} The simplest conformal prediction setting is with unlabeled data from $\mathcal{Y}$ (the setting with no feature space $\mathcal{X}$) that is explored in \citet*{Lei2013DistributionFreePS}. %Given samples from $P$ the goal is to identify a $\widehat{C} \in \mathcal{C}_k$ that contains at least $1-\alpha$ of the probability mass.  
We design a polynomial time algorithm based on dynamic programming (DP) that is distribution-free and finds a union of $k$-intervals with the desired coverage property and is nearly volume-optimal with respect to unions of $k$-intervals. This is described in detail in Section~\ref{sec:unlabeled}. 
\item {\em Labeled data and conditional coverage:} We can also extend these ideas to get {\em conditional coverage} guarantees in the supervised setting by adopting the framework of distributional conformal prediction \citep{izbicki2020flexible,chernozhukov2021distributional}. While conditional coverage guarantees are impossible in general (even without volume optimality considerations), the setting of \citet{izbicki2020flexible,chernozhukov2021distributional} assumes black-box access to an estimate of the conditional distribution $Y | X$. Combined with the new conformity score based on dynamic programming, the proposed method is shown to not only achieve approximate conditional coverage as in \citep{chernozhukov2021distributional} and \citep{romano2019conformalized}, but also conditional volume optimality with respect to unions of $k$ intervals, as long as a reasonable estimator of the conditional CDF is available. Details about the setting, the algorithm and theoretical guarantees are given in Section~\ref{sec:labeled}. 
\end{itemize}
\fi

% \item {\em Experimental results:} We complement our theoretical guarantees with an evaluation of our methods for both the unlabeled setting of Section~\ref{sec:unlabeled} and the conditional coverage setting of Section~\ref{sec:labeled}. The algorithms in Section~\ref{sec:unlabeled} are compared against state-of-the-art methods based on kernel density estimation due to \citet{Lei2013DistributionFreePS} and evaluated on many different kinds distributions, including simple smooth unimodal distributions, heterogenous distributions like mixtures of Gaussians, and more non-smooth distributions with point masses like sums of ReLU activations. 
% % \anote{Insert a figure for unlabed experiments here with a good caption}
% The algorithms for conditional coverage setting are compared against conformalized quantile regression (CQR)  \citep{romano2019conformalized} and distributional conformal prediction methods (DCP-QR and DCP-QR*) of \citet{chernozhukov2021distributional} against benchmark simulated datasets in \citet{romano2019conformalized, xie2024boosted}. As shown in Figures~\ref{fig:intro:unlabeled} and \ref{fig:intro:labeled}, our methods are at least as good, and often outperform previous methods significantly. See Appendix~\ref{sec:numerical} for details. 
%\end{itemize}


% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}[b]{0.48\columnwidth}
%         \includegraphics[width=\textwidth]{figures/mix_gaussian3_DP_k3.png}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.48\columnwidth}
%         \includegraphics[width=\textwidth]{figures/mix_gaussian3_KDE_bandwidth0.5.eps}
%     \end{subfigure}
%     \caption{Conformal prediction sets on the mixture of Gaussians data from $P = \frac{1}{3}N(-6,0.0001)+\frac{1}{3}N(0,1)+\frac{1}{3}N(8,0.25)$. The left plot shows the histogram of the dataset and the prediction set given by our proposed conformalized DP with $k=3$ intervals. (The first interval is at $[-6.03,-5.97]$.) The right plot shows the kernel density estimation with bandwidth $\rho=0.5$ and the prediction set by conformalized KDE. The coverage probability is $80\%$. The volume of the prediction sets by conformalized DP and conformalized KDE are $3.1438$ (left) and $4.4775$ (right). The theoretically optimal volume is $3.0178$.}
%     \label{fig:intro:unlabeled}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \begin{subfigure}{0.48\columnwidth}
%         \includegraphics[width=\textwidth]{figures/dcp-qrstar-synthetic.png}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\columnwidth}
%         \includegraphics[width=\textwidth]{figures/conformalizedDP-k5-synthetic.png}
%     \end{subfigure}
%     \caption{Results in the conditional coverage setting on a synthetic data from \cite{romano2019conformalized} for target coverage 0.7.  The left plot shows the output of DCP-QR*, the state of the art method by \cite{chernozhukov2021distributional}, which outputs prediction sets with average volume 1.29.  The right plot shows the output of our method for \(k = 5\) intervals, which achieves a significantly improved average volume of 0.45. }
%     \label{fig:intro:labeled}
% \end{figure}

% %\anote{Move it to the end? Needs to be rephrased.}

% These experiments together demonstrate that our methods give a clear advantage over competitive procedures under these different conditions. More crucially, our methods also come with theoretical guarantees for both coverage and volume-optimality with respect to unions of $k$ intervals. 

% Finally, we remark that our focus on the family of structured prediction sets conceptually differs from the predominant approach of designing new conformity score functions in existing approaches. %\anote{Should we add: We remark that a nested subcollection of prediction sets also naturally defines a simple conformity score function.} %The prediction sets that are output e.g., unions of intervals, are often more interpretable.
% Apart from being more interpretable, the shift in focus to structured prediction sets, which is in the same spirite of \cite{gupta2022nested}, is what allows us to overcome any sample efficiency concers, to obtain {\em distribution-free volume-optimality}, through {computationally efficient algorithms}. 


% \paragraph{Unsupervised setting}
% \anote{Theory}

% \anote{Experiments}

% \paragraph{Supervised setting}
% \anote{Theory}
% \anote{Experiments}

%\anote{Some notes: We should emphasize the change of perspective in looking at families of confidence sets. As I'm looking through the literature, I don't think this is standard in conformal prediction. } 
%\anote{E.g., in existing literature, the conformity score seems important, and algorithms are typically phrased in terms of it (with the nested confidence sets being the level sets of this function). It seems less natural to reason about this by restricting the complexity of the conformity score function.}


%\input{paper/related}

%\cnote{may need to say what the appendix includes here?}


\subsection{Paper Organization}

We will start with the unsupervised setting with label-only data in Section \ref{sec:unlabeled}. The extension of the theory and algorithm to the supervised setting is given in Section \ref{sec:labeled}. The numerical comparisons between our proposed methodology and existing methods in the literature are presented in Section \ref{sec:numerical_experiments}. All technical proofs and additional numerical experiments will be presented in the appendix.