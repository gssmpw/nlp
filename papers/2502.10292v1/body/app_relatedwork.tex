\section{Related Work}\label{app:related_work}


\paragraph{Oracle-Efficiency in online learning.}
    In order to better develop algorithms for online learning, and motivated by the success of optimization in more classical paradigms,  \cite{kalai2005efficient} introduced the notion of oracle-efficient online learning with the \emph{Follow the Perturbed Leader} family of algorithms.  This approach to online learning serves to stabilize the most na{\"i}ve approach to prediction, i.e. finding the function  that performs best in hindsight and playing this, in order to ensure good performance against adversarial data; subsequent work within this framework \citep{dudik2020oracle,syrgkanis2016efficient,wang2022adaptive,agarwal2019learning,suggala2020online} has significantly generalized the application of these techniques.  One particularly relevant work is that of \citet{hutter2004prediction}, which made a connection between small loss bounds and a stability condition closely resembling differential privacy; this connection was  then made explicit in \cite{abernethy2019online}.  While oracle-efficiency allows for the devlopment of a powerful suite of algorithms,
    unfortunately, \cite{hazan2016computational} showed that in the worst-case, oracle efficiency cannot be achieved for general function classes and adversarial data. 
    In order to circumvent this, a recent line of work has focused on structured instances such as smoothed online learning \citep{haghtalab2020smoothed,oracle-efficient,haghtalab2022smoothed,block2022smoothed,DBLP:journals/corr/abs-2303-04845,DBLP:conf/colt/BlockP23,block2023oracle,pmlr-v247-block24a,block2024smoothed} and has constructed oracle-efficient algorithms for a variety of settings.
    Our work has a conceptually similar goal of circumventing the worst-case hardness of oracle efficiency by focusing on a structured class of functions, $\rho$-separated functions, and constructing oracle-efficient algorithms for online learning.  Unlike these other works, we primarily focus on $\Lstar$ bounds as opposed to minimax regret.  Recently, \citet{wang2022adaptive} also studied $\Lstar$ bounds in the oracle-efficient setting, and we compare our results to theirs in \Cref{sec:main}.


\paragraph{Oracle-Efficiency in differentially private learning.} 
    As in online learning, the using the framework of oracle efficiency to develop practical algorithms for differentially private learning has been an active area of research \citep{DBLP:conf/focs/Neel0W19,Neel0VW20,vietri2020new, gaboardi2014dual,nikolov2013geometry,block2024oracle} and the connections between online learning and differential privacy have been extensively explored \citep{AlonLMM19,bun2020equivalence,abernethy2019online}.  Of particular relevance to our work are the works of \citet{Neel0VW20,block2024oracle}; the former develops oracle-efficient, differentially private learners for function classes with a small separator set (which our results generalize), while the latter uses a similar algorithmic technique as ours to develop oracle-efficient algorithms that makes use of some public data, building upon work in semi-private learning \cite{beimel2014learning,BassilyMA19}


