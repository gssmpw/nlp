In this section we sketch the proofs of our main results above.  We begin with the technical result underlying our main theorems, which establishes a stability property for Gaussian processes.  Then, in \Cref{ssec:online-learning-proofs}, we prove \Cref{thm:lstar_online} using this stability result.  

\subsection{An Improved Gaussian Process Stability Bound}\label{ssec:gp-stability}
We now describe the key Gaussian process stability bound on which our results rely.  This bound is a stronger form of the bounds found in \citet{block2022smoothed,block2024oracle}, which conclude that approximate minimizers of a Gaussian process are, in some sense, close to the true minimizer with reasonable probability. We now formally define a Gaussian process. 
\begin{definition}[Gaussian Process]
    Let $T$ be an index set. 
    A collection of random variables $ \{\omega(t) \}_{t \in T} $ is a Gaussian process if for all finite subsets $T' \subseteq T$, the random vector $\{ \omega(t)\}_{t \in T'}$ has a multivariate Gaussian distribution.
    The function $ m : T \to \mathbb{R} $ given by $ m(t) = \ee [\omega(t)] $ is called the mean function of the Gaussian process and 
    the function $ K : T \times T \to \mathbb{R} $ given by $ K(t, t') = \ee [(\omega(t) - m(t))(\omega(t') - m(t'))] $ is called the covariance kernel of the Gaussian process.
    
\end{definition}
We recall that a Gaussian process on an index set $T$ can be fully characterized by its mean function $m : T \to \rr$ and covariance kernel $K: T \times T \to \rr$, and define a Gaussian process now.  The covariance kernel in particular can be interpreted as a measure of distance between points in $T$, with two points $s,t \in T$ being considered close if $K(s, t) \approx K(t,t)$. 
In particular, define the distance between two points $s,t \in T$ as $\norm{s-t} = \sqrt{K(s,s) + K(t,t) - 2K(s,t)}$.
We use the norm notation though $T$ need not be a vector space; this is standard notation and can be justified using standard representation theorems for Gaussian processes.  

The results of \citet{block2022smoothed,block2024oracle} involve \emph{approximate minimizers} of a Gaussian process $\Omega: T \to \rr$ with mean function $m$ and covariance kernel $\eta^2 \cdot K$ for some variance parameter $\eta > 0$, where $t \in T$ is an approximate minimizer if $\Omega(t) \leq \Omega(\tstar) + \tau$ for some tolerance $\tau > 0$ for $\tstar = \argmin \Omega(s)$.  In particular, those results say that assuming the covariance kernel is well conditioned in the sense that $\sup_t K(t,t) / \inf_t K(t,t)$ is not too large, then
\begin{align}\label{eq:gp_stability_old}
    \pp\left( \exists t \in T \text{ s.t. } \Omega(t) \leq \Omega(\tstar) + \tau \text{ and } \nicefrac{K(s,\tstar)}{K(\tstar, \tstar)} \leq 1 - \rho^2 \right) 
    \lesssim \frac{\tau \cdot \ee\left[ \sup_{t \in T} \Omega(t) - m(t) \right]}{\rho^2 \eta^2}.
\end{align}
The upper bound on the probability that there exist approximate minimizers of $\Omega$ far from the true minimizer scales naturally with the tolerance $\tau$ and distance $\rho$ as well as the size of $T$, as measured by the Gaussian complexity, suggesting that the larger an index set is, the more likely it is that far away approximate minimizers exist.  Moreover, as the scale of the noise, parameterized by $\eta$, increases, the probability of far away approximate minimizers decreases, reflecting the fact that the true minimizer becomes less dependent on the mean function $m$.

For learning theory applications, we can instantiate \eqref{eq:gp_stability_old} in the case where $T = \cF$ is a function class, $\omega$ is a centred Gaussian process with covariance kernel $K(f, f') = \inprod{f}{f'}_m$, the empirical inner product on a dataset $\left\{ Z_1, \dots, Z_m \right\}$, and $\Omega(f) = m(f) + \eta \cdot \omega(f)$ for some mean function $m$.  Given an alternative mean function $m'$ such that $\sup_f \abs{m(f) - m'(f)} \leq \tau$, we can define the offset process $\Omega'(f) = m'(f) + \eta \cdot \omega(f)$ and let $\fstarp = \argmin \Omega'(f)$.  Applying \eqref{eq:gp_stability_old} and some rearranging then implies that with reasonable probability,
\begin{align}\label{eq:gp_stability_old_cor}
    \|\fstar - \fstarp\|_m^2 \lesssim \frac{\tau \cdot \ee\left[ \sup_f \omega(f)\right]}{\eta},
\end{align}
i.e., minimizers of Gaussian processes with similar mean functions are close in $\norm{\cdot}_m$.  While powerful, unfortunately \eqref{eq:gp_stability_old} and its corollary \eqref{eq:gp_stability_old_cor} do not yield sufficiently strong guarantees to establish small loss bounds for online learning, nor does it immediately yield a differentially private algorithm.  Instead, as explained in the sequel, a stronger guarantee is necessary to establish these results. \looseness-1
\begin{lemma}\label{lem:gp_stability_cor}
    Let $\cF: \cX \to \rr$ be a function class and $\mu$ a measure on $\cF$ with $\omega: \cF \to \rr$ the canonical Gaussian process associated to this space; further assume that $\mu$ is a $\rho$-separating measure on $\cF$ in the sense that $\norm{f -f '}_{L^2(\mu)} \geq \rho$ for all $f \neq f' \in \cF$.  
    Let $m,m': \cF \to \rr$ be two mean functions satisfying $\sup_{f \in \cF} \abs{m(f) - m'(f)} \leq \tau$.  If $\fstar = \argmin_f \Omega(f)$, where $\Omega(f) = m(f) + \eta \cdot \omega(f)$ and $\fstarp, \Omega'$ are defined similarly then as long as 
    \begin{align}\label{eq:gp_dp_bound_eta}
        \eta \geq \frac{32\tau}{ \rho^2} \left( \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sqrt{2\log\left( \frac 2\delta \right)} \right),
    \end{align}
    it holds for any $f$ that
    \begin{align}\label{eq:gp_dp_bound}
        \pp\left( \fstar = f \right) \leq \left( 1 + \frac{32 \tau}{\eta \rho^2} \left( \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sqrt{2 \log\left( \frac 2\delta \right)} \right) \right) \cdot \pp\left( \fstarp = f \right) + \delta.
    \end{align}
\end{lemma}
In contradistinction to \eqref{eq:gp_stability_old}, \Cref{lem:gp_stability_cor} requires an additional assumption of \emph{separation} on the function class $\cF$, which cannot be dropped in general because its application to private learning, which is only possible with finite Littlestone dimension \citep{AlonLMM19}.  What is gained from this reduction in generality, however, is the fact that the guarantee on stability is significantly stronger than that of \eqref{eq:gp_stability_old_cor} in that it provides fine-grained control on the \emph{distributional} stability of the approximate minimizers, which is essential for our applications.  Furthermore, we eliminate the assumption of well-conditionedness on the covariance kernel, which substantially increases the generality of application of this result, in particular by allowing functions with arbitrarily small norm.

While we defer a full proof of \Cref{lem:gp_stability_cor} to \Cref{app:gp_stability}, we sketch the argument here.  The proof begins in the same way as that of \citet{block2022smoothed,block2024oracle}, by introducing the `bad' event $\cE(\rho, \tau)$ corresponding to the existence of a $\tau$-approximate minimizer existing at lesat distance $\rho$ from the true minimizer.  In this first step we additionally assume that the covariance kernel $K$ is well-conditioned in the sense that $1 \geq K(f,f) \geq \kappa^2 > 0$ for some $\kappa$, a requrement we later drop.  As in those earlier bounds, we then fix an arbitrary $y \in \rr$ and condition on the event that $\fstar = t$ and $\Omega(\fstar) = y$; unlike those earlier works, however, we also require conditioning on the event $\Phi_\delta$, occuring with probability at least $1 - \delta$, that the supremum of $\omega$ is not too much larger than its expectation, the reason for which we explain below.  As $\Phi_\delta$ occurs with high probability, we may condition on this event without losing more than a constant in the final bound; similarly, we will ignore the `good' event $\cE^c(\rho, \tau)$, as the resulting probability of this event will appear on the right hand side of \eqref{eq:gp_dp_bound}.  The proof then uses a fundamental fact of Gaussian Processes: the covariance kernel of a pinned Gaussian Process does not depend on the value at which this process is pinned.  After careful analysis using several more properties specific to Gaussian Processes, we conclude
\begin{align}
    \pp\left( \fstar = f \text{ and } \cE(\rho, \tau) \text{ and } \Phi_\delta \right) \lesssim \frac{\tau}{\eta \kappa^2 \rho^2} \cdot \ee\left[ \omega(\fstar) \bbI[\Phi_\delta] | \tstar = t \right] \cdot \pp\left( \tstar = t \right).
\end{align}
It is here that relying on $\Phi_\delta$ is essential, as under $\Phi_\delta$, it holds that $\omega(\fstar)$ is not so large, even conditioned on the possibly low probability event that $\fstar = f$.  A stronger version of \eqref{eq:gp_stability_old} follows after rearranging terms.  In order to remove the dependence on $\kappa$, we introduce an auxiliary process $\Omegatil$ by adding additional points $\wt{Z}_i$ such that $f(\wt{Z}_i) = 1$ for all $f$.  Adding enough of these points ensures that the auxiliary process is well-conditioned and careful comparisons of $\Omegatil$ to $\Omega$ as well as a sharp understanding of the relationship between the induced covariance kernel and the norm $\norm{\cdot}$ on $\cF$ allows us to conclude the desired result.  Note that the introduction of this auxiliary process is a subtle point as one \emph{cannot use this approach to remove the $\kappa$ parameter from } \eqref{eq:gp_stability_old} because the resulting separation between points as measured by correlation is hurt by the indroduction of these auxiliary points; in order for this approach to work, we carefully relate the norm induced on $\cF$ by $\Omegatil$ to that by $\Omega$, which is significantly better behaved than the correlation.  With the main technical step thus proved, we now apply this result to online learning.  








\subsection{Online Learning Proofs}\label{ssec:online-learning-proofs}
While a full proof of \Cref{thm:lstar_online} can be found in \Cref{app:online_learning}, we sketch the main argument here and present the full algorithm. 
The proof proceeds by applying \Cref{lem:gp_stability_cor} to the well-known `Be-the-Leader' lemma from \citet{kalai2005efficient,cesa2006prediction}, which states that `Follow-the-Leader' style algorithms have regret that can be bounded by the stability of the predictions.  In particular, the lemma concludes that if $f_t$ is chosen as in \eqref{eq:ftpl_gaussian}, then
\begin{align}\label{eq:btl_body}
    \ee\left[ \reg_T \right] \leq 2 \eta \cdot  \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sum_{t = 1}^{T} \ee\left[ \ell_t(f_t) - \ell_t(f_{t+1}) \right].
\end{align}
Here, the first term measures the `bias,' i.e., how large the perturbation is, while the second term measures the stability of the predictions.  For Lipschitz $f$, it suffices to control the stability of predictions measured by the $L^2$ norm of the perturbation, as was done for the case of smoothed data in \citet{block2022smoothed}.  In order to realize $\Lstar$ bounds, however, \citet{hutter2004prediction} used a stronger notion of stability akin to differential privacy, which was then further explored in \citet{wang2022adaptive}.  This stronger notion of stability is precisely what is controlled by \Cref{lem:gp_stability_cor}; thus, we apply that result 
with $\delta = \nicefrac{1}{T \cdot \abs{\cF}}$ and, bounding the Gaussian complexity by $\sqrt{\log(\abs{\cF})}$ \citep{boucheron2013concentration}, we sum over $f \in \cF$ and see that for sufficiently large $\eta$,
\begin{align}
    \ee\left[ \ell_t(f_t) - \ell_t(f_{t+1}) \right] \lesssim  \frac{1}{T} + \frac{B \sqrt{\log(T \abs{\cF})} }{\eta \rho^2} \cdot \ee\left[ \ell_t(f_{t+1}) \right].
\end{align}
Plugging the preceding display into \eqref{eq:btl_body}, we observe that the regret is bounded by
\begin{align}
    \ee\left[ \reg_T \right] \lesssim 1 + \eta \cdot \sqrt{\log(\abs{\cF})} +  \frac{B \sqrt{\log(T \abs{\cF})} }{\eta \rho^2} \cdot \sum_{t= 1}^T \ee\left[ \ell_t(f_{t+1}) \right].
\end{align}
The result concludes by controlling the summation in the last term by $\Lstar$, which is another consequence of the Be-the-Leader Lemma, and balancing $\eta$ subject to \eqref{eq:gp_dp_bound_eta}.


\iftoggle{colt}{}{
\input{body/app_privacy.tex}
}

































    










   














