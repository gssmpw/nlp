

\iftoggle{colt}{
\section{Proof of \Cref{thm:main_privacy}}
}{
    \subsection{Differential Privacy Proofs}
}
\label{sec:proof_main_privacy}








    
    In this section, we apply that Gaussian stability bound in order to improve oracle efficient private algorithms. 
    Recall that the \Cref{alg:erm_perturbed} outputs a hypothesis $\hat{f}$ that is a minimizer of the empirical risk with a Gaussian perturbation term added. 
    Thus, the output of the algorithm can be seen as corresponding to the minimizer of a Gaussian process with the mean function given by the loss on the input data and since we are using a set of auxiliary points drawn from a separating distribution, it follows that the conditions of \Cref{lem:gp_stability_cor} are satisfied.
    This instantiated in the present context gives us the following privacy guarantee.
    
    \begin{lemma}[Privacy of Perturbed ERM]\label{lem:privacy_perturbed}
        Let $\delta, \eta > 0$.  
        Suppose that $\mu$ is a $\rho$ separating measure for $\mathcal{F}$.
        The output of the \Cref{alg:erm_perturbed} is $(\epsilon, \delta)$-differentially private for 
        \begin{align}
              \epsilon \;=\; \frac{8}{\eta\, \rho^2} \left(  \mathcal{G}( \mathcal{F})  + \sqrt{2\log\Bigl(\frac{2}{\delta}\Bigr)} \right).
        \end{align} 
    \end{lemma}
    
    \begin{proof}
        In order to prove the privacy of the algorithm, we need to verify that the conditions of \Cref{lem:gp_stability_cor}.
    Fix the data set $S = \left\{ (x_1 , y_1) , \dots , (x_n , y_n) \right\} $ 
    Let the Gaussian process corresponding to the hypothesis class $\mathcal{F} $  be $\Omega(f) =  \sum_i \ell(f(x_i) , y_i ) +    \frac{1}{ \sqrt{m}} \sum_{i=1}^{m} \xi_i f(z_i)$.
    Note that the mean function $m_S(f) =  \sum_i \ell(f(x_i) , y_i )$ and the covariance kernel $K(f,f') = \frac{1}{m} \sum_{i=1}^{m} f(z_i) f'(z_i)$.

    
    Note that we assumed that $\mu$ is a $\rho$ separating measure for $\mathcal{F}$.
    From \Cref{lem:separating_measure_sample}, we know that a sample of size $m = (d + \log(100/ \delta)) \rho^{-2}$ also induces a $ \rho / 2 $ separating measure with probability at least $1 - \delta/100$. 
    This implies that the Gaussian process $\omega(f)$ is $\rho/2$ separated with respect to the norm induced by the bilinear form $K$.
    When the dataset is changed from \(S\) to a neighboring dataset \(S'\), the change in the mean function \(m_S(f)\) is bounded by
    \[
    \sup_{f\in\mathcal{F}  } \bigl| m_S(f) - m_{S'}(f) \bigr| \le 1.
    \]
    We apply this to \Cref{lem:gp_stability_cor} with $ \tau = 1 $, and setting 
    \[
    \eta =  \frac{8} {\rho^2 \epsilon } \left( \mathbb{E}\Bigl[\sup_{f\in\mathcal{F}} \omega(f)\Bigr] + \sqrt{\log\Bigl(\frac{2}{\delta}\Bigr)} \right),
    \]
    then for any \(f\in \mathcal{F}\) the selection probabilities satisfy
    \[
    \mathbb{P}\Bigl( \hat{f}(S) = f \Bigr) \;\le\; \left(1 + \frac{8\tau}{\eta\,  \rho^2} \left( \mathbb{E}\Bigl[\sup_{f\in\mathcal{F}} \omega(f)\Bigr] + \sqrt{2\log\Bigl(\frac{2}{\delta}\Bigr)} \right) \right) \mathbb{P}\Bigl( \hat{f}(S') = f \Bigr) + \delta.
    \]

    Simplifying the above expression using the identity $1+ x \leq e^x $, we get for any measurable set \(O\) that 
    \[
    \mathbb{P}\Bigl( \hat{f}(S) \in O \Bigr) \;\le\; e^{\epsilon} \, \mathbb{P}\Bigl( \hat{f}(S') \in O \Bigr) + \delta,
    \]
   as required. 
    \end{proof}


    It remains to be shown that the output of the algorithm \cref{alg:erm_perturbed} is still accurate on the test distribution. 
    In order to reason about this, the key quantity to control is the size of the perturbation term. 
    Fortunately, since we are using Gaussian perturbations, this is naturally bounded in terms of the noise level $\eta$ and the Gaussian complexity of the function class which in turn bounds the generalization error of the ERM procedure even if the Gaussian perturbation were not present.
    
    
    \begin{lemma}[Accuracy of Perturbed ERM]\label{lem:accuracy_perturbed}
        Let $\beta, \eta > 0$.  
        The output of the \Cref{alg:erm_perturbed} satisfies $(\alpha, \beta)$ accuracy as long as the number of samples $n$ satisfies  
        \begin{align}
            n \geq \max\left\{ \frac{\mathcal{G}( \mathcal{F} )^2 + \log(1/\beta)}{\alpha^2}  , \eta  \frac{\mathcal{G}( \mathcal{F} ) + \sqrt{ \log(1/\beta) }}{\alpha}    \right\} .
        \end{align}  
    \end{lemma}
    \begin{proof}
        Note that from standard uniform convergence bound in terms of Gaussian complexity, we have that with probability at least $1 - \beta $, that for all $f \in \mathcal{F}$,
        \begin{align}
            \left| \frac{1}{n} \sum_{i=1}^{n} \ell(f(x_i) , y_i ) - \mathbb{E} \ell(f(x) , y ) \right| \leq \alpha
        \end{align} 
        as long as $ n \geq (\mathcal{G}( \mathcal{F} )^2 + \log(1/\beta)) / \alpha^2 $.
        Thus, for the optimizer $\hat{f}$  from \Cref{alg:erm_perturbed}, we have that 
        \begin{align}
            \mathbb{E} \ell( \hat{f}(x) , y ) \leq \min_{f \in \mathcal{F}}  \mathbb{E} \ell( \hat{f}(x) , y ) + \sup_{f \in \mathcal{F}  } \frac{\eta}{ n \sqrt{m} } \sum_{i=1}^{m} \xi_i f(z_i) + \alpha
        \end{align}  
        From Gaussian concentration for Lipschitz functions \citep[Section 10.5]{boucheron2013concentration}, we have that with probability at least $1 - \beta $, the Gaussian complexity term is bounded by
        \begin{align}
            \mathbb{E} \sup_{f \in \mathcal{F}  } \frac{\eta}{ n \sqrt{m} } \sum_{i=1}^{m} \xi_i f(z_i) + \frac{\eta}{n} \sqrt{\log(1/\beta)} . 
        \end{align}
        Note that this is the same as the Gaussian complexity of the function class $\mathcal{F}$.
    \end{proof}

    Plugging this in (and simplifying) with the choice of $\eta$ from  the privacy guarantee from \Cref{lem:privacy_perturbed}, we get the following result we get that 
    \begin{align}
        n \geq \max \left\{ \frac{\mathcal{G}( \mathcal{F} )^2 + \log(1/\beta) }{\alpha^2} ,  \frac{\mathcal{G}( \mathcal{F} )^2 + \sqrt{ \log(1/\beta) \log(1/\delta) }   } { \alpha \epsilon \rho^2 }    \right\}
    \end{align}
    as required.





    



