
We now recall prerequisite notions, define the main problems of study, and introduce the fundamental assumption underlying our work: $\rho$-separation.  


\subsection{Notions from Learning Theory}\label{ssec:prelims_learning}
Recall that the fundamental premise of learning theory is to assume the learner has access to some function class $\cF: \cX \to \cY$ mapping contexts $\cX$ to labels $\cY$, where we always consider $\cY \subset [-1,1]$ in this work.  Given a loss function $\ell: \cY \times \cY \to \rr_{\geq 0}$, the goal of the learner is to produce labels with minimal loss relative to the benchmark function class $\cF$.  A standard formal instantiation of this idea is the PAC-learning framework \cite{vapnik1964class,valiant1984theory}. 

\begin{definition}[PAC Learning]\label{def:pac}
    Given n independent and identically distributed data points $(X_i, Y_i)$ and a loss function $\ell$, an algorithm $\fhat$ depending on these data is said to $(\alpha, \beta)$-learn if for independent $(X, Y)$ it holds that $\pp\left( \ell(\fhat(X), Y) \geq \alpha + \inf_{f \in \cF} \ell(f(X), Y) \right) \leq \beta$.
    The number of samples required to $(\alpha, \beta)$-learn is referred to as the sample complexity.
\end{definition}

 
There are a wide variety of complexity measures of the function class, that dictate the sample  considered. 
In this work, we will work with the Gaussian complexity. 
\begin{definition}[Gaussian Complexity]
    Let \(\mathcal{F}\) be a class of functions from \(\mathcal{X}\) to \(\mathbb{R}\). For a fixed sample \(S = \{x_1, x_2, \ldots, x_n\} \subseteq \mathcal{X}\), the \emph{empirical Gaussian complexity} of \(\mathcal{F}\) is defined as \iftoggle{colt}{$\mathcal{G}_S(\mathcal{F}) = \mathbb{E}_{\xi}\left[ \sup_{f \in \mathcal{F}} \frac{1}{ \sqrt{n}} \sum_{i=1}^n \xi_i\, f(x_i) \right]$,}{
\begin{align}
    \mathcal{G}_S(\mathcal{F}) = \mathbb{E}_{\xi}\left[ \sup_{f \in \mathcal{F}} \frac{1}{ \sqrt{n}} \sum_{i=1}^n \xi_i\, f(x_i) \right],
\end{align}
    }
    where \(\gamma_1, \ldots, \gamma_n \sim \mathcal{N}(0,1)\) are independent standard Gaussians. 
\end{definition}
The Gaussian complexity of a function class is related to a number of natural notions of function class size.  
For example, if $\cF$ is finite, then $\cG_S(\cF) \lesssim \sqrt{\log(\abs{\cF})}$ \citep[Theorem 2.5]{boucheron2013concentration}. 
Furthermore, the Gaussian complexity is bounded by the square root of the \emph{VC dimension} \citep[Theorem 6]{bartlett2002rademacher}, which is the canonical measure of complexity in binary classification and is defined as follows \citep{shai}. 

\begin{definition}[VC Dimension] 
    Let $\mathcal{F}$ be a class of functions from a domain $\mathcal{X}$ to $\{0, 1\}$. 
    We say that $\mathcal{F}$ shatters a set $X \subseteq \mathcal{X}$ if for all $Y \subseteq X$, there exists a function $f \in \mathcal{F}$ such that $f(x) = 1$ if $x \in Y$ and $f(x) = 0$ otherwise.
    The VC dimension of $\mathcal{F}$ is the size of the largest set $X \subseteq \mathcal{X}$ such that there exists a function $f \in \mathcal{F}$ that shatters $X$.
\end{definition}

\paragraph{Online Learning.}
In contradistinction to PAC learning where data are iid and come all at once, in the online learning framework, a learner interacts with an adversarial environment over a sequence of rounds. At round \(t\), the adversary selects a context $x_t$ and a label $y_t$, while the leaner selects a hypothesis \(f_t\) from  some \(\mathcal{F}\); the context and label are then revealed to the learner, who suffers loss $\ell_t(f_t) = \ell(f(x_t), y_t)$, with the goal of minimizing \emph{regret} over $T$ rounds, defined to be
\begin{align}\label{eq:regret_def}
    \reg_T = \sum_{t=1}^T \ell_t(f_t) - \inf_{f \in \mathcal{F}} \sum_{t=1}^T \ell_t(f).
\end{align}
Rates in online learning have been classically established in a number of settings \citep{ben2009agnostic,rakhlin2011online,rakhlin2015sequential} and the minimax rate for simple function classes typically scales like $O(\sqrt{T})$ along with some complexity parameter of the class $\cF$.  Unfortunately, fully adversarial online learning is known to be strictly harder than PAC learning and its difficulty is characterized by the \emph{Littlestone dimension} \citep{littlestone1988learning,ben2009agnostic} as opposed to the Gaussian complexity or VC dimension above.   On the other hand, in some settings the minimax $\sqrt{T}$ regret scaling may be overly pessimistic.  For example, if the best hypothesis suffers a small loss $\Lstar = \inf_f \sum_{t = 1}^T \ell_t(f)$, e.g. in the realizable setting where $\Lstar=0$, we may hope to get algorithms whose regret scales only \emph{logarithmically} in $T$ and polynomially in $\Lstar$, which is a vast improvement over the minimax framework.  One of the main contributions of this work is to introduce an \emph{efficient} algorithm capable of achieving this rate whenever $\cF$ satisfies a separation condition discussed in the sequel.  We now discuss the other primary learning setting considered here.

\paragraph{Differential Privacy.}  While online learning makes success more difficult by removing assumptions on the data, in differential privacy, we place additional requirements on the algorithm.  
Motivated by data-privacy concerns, we consider the following notion of privacy \citep{DworkMNS06}. 
\begin{definition}[Differential Privacy]
    An algorithm $\mathcal{A}$ is said to satisfy $(\epsilon, \delta)$ (approximate) differential privacy  if for all datasets $S_1, S_2$ differing in a single element, and all measurable sets $A$ in the output space, we have
    \begin{align}
        \pp\left(\mathcal{A}(S_1) \in A\right) \leq e^\epsilon \pp\left(\mathcal{A}(S_2) \in A\right) + \delta.
    \end{align}
    If $\delta = 0$, we say that $\mathcal{A}$ satisfies $\epsilon$ (pure) differential privacy.
\end{definition}
The goal of private learning is to design an algorithm that is both an $(\alpha, \beta)$-learner and is $(\epsilon, \delta)$-differentially private with the number of samples scaling as $\poly\left( 1/\alpha, 1/\epsilon, \log(1/\beta), \log(1/\delta) \right)$. 
Recent work has demonstrated that differentially private learning is intimately connected with online learning in that the ability to succeed in either framework is characterized not by the Littlestone dimension mentioned above \citep{AlonLMM19} and $\Lstar$ bounds in online learning arise from private online algorithms \citep{hutter2004prediction,abernethy2019online,wang2022adaptive}.

\paragraph{Oracle Efficiency.}
    In both the online and private learning frameworks, due to practical resource constraints, we are interested in computationally efficient algorithms; unfortunately, for most cases of interest, learning is computationally intractable even for simple concept class such as halfspaces (see \citep{tiegel2022hardness} and references therein).
    To circumvent pessimistic fact and  develop theory that incorporate the success of practical heuristics, we consider the notion of oracle efficiency \citep{kalai2005efficient}.  In aprticular, we suppose that the learner has access to an \emph{Empirical Risk Minimization Oracle} for the class $\cF$ such that, given loss functions $\ell_1, \dots, \ell_k$ and data points $(x_1, y_1), \dots, (x_k, y_k)$ can efficiently return the minimizer of the cumulative loss:
    \begin{align}
        \fhat \in \argmin_{f \in \cF} \sum_{ i = 1}^k \ell_i(f(x_i), y_i).
    \end{align}
    The time complexity of a given algorithm is the number of oracle calls the algorithm makes multiplied by the number of inputs to each oracle call.  Our algorithms consider the special case where $k = n + m$, the $\ell_1 = \cdots = \ell_n = \ell$ and for $n < i \leq n + m$, $\ell_i(f(x_i), y_i) = \xi_i f(x_i)$ for $\xi_i$ a  gaussian. \looseness-1


\paragraph{Additional Notation.}  We denote by $\norm{\cdot}_m$ the empirical $L^2$ norm on $m$ points, assumed to be given and $\norm{\cdot}_1$, $\norm{\cdot}_2$ the (unnomralized) $\ell^1$ and Euclidean norms on $\rr^m$.  We use $f = \bigO(g)$ to say that $f/g$ is bounded and $f = \bigOtil(g) $ to mean that $f/g$ is bounded by polylogarithmic factors of the inputs to $f,g$; we use $f \lesssim g$ if $f = \bigO(g)$.















\subsection{Key Assumption: $\rho$-Separation}


    In this section, we will introduce the key assumption underlying our work: $\rho$-separation.

    \begin{definition}\label{def:rho_separating}
        Let $\cF: \cX \to \cY$ be a function class.  We call a measure $\mu$ on $\cX$ a $\rho$\emph{-separating measure} for $\cF$ if for all $f, g \in \cF$ such that $f \neq g$, we have $\norm{f-g}_{L^2(\mu)} \geq \rho$.  We abuse notation and say a set of points $\left\{ z_1, \dots, z_m \right\} \subset \cX$ is $\rho$-separating if the empirical measure is $\rho$-separating.
    \end{definition}
    In essence, $\rho$-separation ensures that no two functions in $\cF$ are too close together, which allows boosting stability in $L_2$ into differential privacy guarantees.  To gain intuition, note that $\rho$-separation is implied for finite function classes by a natural spectral property of the function class.
    \begin{proposition}\label{prop:singular_value}
        Let $\cF: \cX \to \rr$ be a finite function class, let $z_1, \dots, z_m \in \cX$ be points, and consider the matrix $A \in \rr^{m \times \abs{\cF}}$ whose columns are $(f(z_i))$.  Letting $\sigma$ be the minimal singular value of $A$, we have that $\cF$ is $\rho$-separated by the empirical measure on $\cX$ with $\rho = \sigma \sqrt{2/m}$.
    \end{proposition}
    We defer a proof of this result to \Cref{sec:separator_proofs}, which is a simple consequence of the definition of a singular value. 
    Such spectral properties of function classes are appear naturally in signals processing \citep{vetterli2014foundations}, wavelet theory \citep{stephane1999wavelet}, frame theory, and harmonic analysis \citep{benedetto2020harmonic}. 
    Learning with respect such function classes corresponds to the natural problem of recovering the large coefficient in a signal decomposition. 
    As a concrete example, if the function class corresponds to the Fourier basis (which it is easy to see satisfies our separation assumption), the learning problem corresponds to recovering the large Fourier coefficients of a signal. \looseness-1
    
    \ascomment{@Abhishek discuss with applications}


    Furthermore, $\rho$-separation can be seen as a generalization of the notion of a separator set \cite{goldman1993exact}, which has previously been used in the context of oracle efficient learning \citep{dudik2020oracle,DBLP:conf/focs/Neel0W19,block2024oracle}. 

    \begin{definition}[Separator Set]\label{def:separator_set}
        Let $\cF: \cX \to \left\{ \pm 1 \right\}$ be a function class.
        We call a set $S \subseteq \cX$ a separator set for $\cF$ if for all $f , g \in \cF$ such that $f \neq g$, there exists $x \in S$ such that $f(x) \neq g(x)$.
    \end{definition}
    In the special case of binary classification, if $S$ is a separator set then it yields a $\rho$-separating measure with $\rho = 1/\sqrt{\abs{S}}$.  Furthermore, in this special case, while the existence of a separating measure itself does not directly bound the complexity of a function class, under the additional assumption that the function class is learnable,
    we have that $\rho$-separation implies finiteness as well as .
    Further, we can show that the existence of $ \rho $-separating measure implies the existence of a separator set.
    \begin{proposition}\label{lem:separating_measure_sample}
        Let $\cF$ be a learnable, binary-valued function class and suppose there exists some measure $\mu$ that $\rho$-separates $\cF$.  Then $\cF$ is finite and has a separating set of size polynomial in $1/\rho$.
    \end{proposition}
    We defer a proof of this result to \Cref{sec:separator_proofs}, and note that it shows that, at least in the binary-valued case, $\rho$-separation and the existence of a small separator set are qualitatively equivalent, although we will see that quantitatively they can lead to different rates.  
    We emphasize that \Cref{lem:separating_measure_sample} implies that if $\cF$ is $\rho$-separated and PAC-learnable, then it is both online- and privately-learnable by finiteness in an information theoretic sense, without regard to computation time.
    The primary focus of our paper is demonstrating that this learning can be achieved (oracle-)efficiently.

    





 









    






    





        






























