


In this section, we present our main results. 
First, we will look at the small loss regret bounds for oracle efficient algorithms. 
We present nearly optimal regret bounds under the assumption that the function class is well-separated, unifying and improving on prior work.   Second, we will provide an oracle-efficient algorithm capable of achieving differentially private learning under separation that matches the sample complexity of a non-private learning algorithm, up to the standard $\epsilon^{-1}$ blow up due to privacy, improving upon the results of \cite{block2024oracle}.



\subsection{An Oracle-Efficient Algorithm with a Small-Loss Bound }\label{ssec:small-loss-bounds}

\iftoggle{colt}{
\begin{algorithm}[t]
    \caption{Follow the Perturbed Leader with Gaussian Perturbation}
    \label{alg:ftpl}
    \SetKwInput{KwInput}{Input}                %
    \SetKwInput{KwOutput}{Output}              %
    \DontPrintSemicolon
    \KwInput{Function class $\cF: \cX \to \cY$, horizon $T$, loss function $\ell$, $\rho$-separating set $\cZ = \left\{ Z_1, \dots, Z_m \right\}$, noise level $\eta > 0$, $L_0(f) = 0$}

    \For{$1 \leq t \leq T$}{
        Draw $\xi_1, \dots, \xi_m \sim \cN(0, 1)$ and let $\omega_t(f) = m^{-1/2} \cdot \sum_{i = 1} \xi_i f(Z_i)$. \\
        Set $f_t \gets \argmin_{f \in \cF} L_{t-1}(f) + \eta \cdot \omega_t(f)$. \\
        Observe $x_t$, play $\yhat_t = f_t(x_t)$, observe $y_t$ and incur $\ell(\yhat_t, y_t)$.
        Update $L_t(f) = L_{t-1}(f) + \ell(f(x_t), y_t)$ for all $f \in \cF$.
    }
\end{algorithm}
}
{
    \begin{algorithm}[t]
        \vspace{2pt} \hrule \vspace{4pt}  %
        \caption{Follow the Perturbed Leader with Gaussian Perturbation}
        \label{alg:ftpl}
        \SetKwInput{KwInput}{Input}                %
        \SetKwInput{KwResult}{Output}              %
        \DontPrintSemicolon

    
        \KwInput{Function class $\mathcal{F}: \mathcal{X} \to \mathcal{Y}$, 
                 horizon $T$, 
                 loss function $\ell$, 
                 $\rho$-separating set $\mathcal{Z} = \left\{ Z_1, \dots, Z_m \right\}$, 
                 noise level $\eta > 0$, 
                 initial loss $L_0(f) = 0$}
    
        \For{$1 \leq t \leq T$}{
            \tcp{Step 1: Sample Gaussian Perturbation}  
            Draw $\xi_1, \dots, \xi_m \sim \mathcal{N}(0, 1)$\;
            Compute perturbation: $\omega_t(f) = m^{-1/2} \sum_{i = 1}^{m} \xi_i f(Z_i)$\;
    
            \tcp{Step 2: Select Predictor}  
            Set $f_t \gets \arg\min_{f \in \mathcal{F}} L_{t-1}(f) + \eta \cdot \omega_t(f)$\;
    
            \tcp{Step 3: Play and Update Loss}  
            Observe $x_t$, play $\hat{y}_t = f_t(x_t)$, and observe $y_t$\;
            Incur loss $\ell(\hat{y}_t, y_t)$\;
            Update loss: $L_t(f) = L_{t-1}(f) + \ell(f(x_t), y_t)$ for all $f \in \mathcal{F}$\;
        }
    
        \vspace{4pt} \hrule \vspace{2pt}  %
    \end{algorithm}
    
}


We now present our first main result, a new, oracle-efficient algorithm for online learning capable of achieving a adaptive regret for bounded, separated function classes $\cF$.  Our algorithm is an instantiation of Follow the Perturbed Leader \citep{kalai2005efficient} and appears (with differently tuned parameters) in \citet{block2022smoothed,block2024oracle}.

\begin{theorem}\label{thm:lstar_online}
    Let $\cF: \cX \to [-1,1]$ be a function class and let $Z_1, \dots, Z_m \in \cX$ such that $\cF$ is $\rho$-separated by the empirical measure on the $Z_i$.  Let $\ell$ be a loss function bounded in $[-B,B]$ and suppose the learner plays according to \Cref{alg:ftpl}, i.e. at each time $t$, plays
    \begin{align}\label{eq:ftpl_gaussian}
        f_t \in \argmin L_{t-1}(f) + \eta \cdot \omega_t(f), \qquad \text{where} \qquad \omega_t(f) = \frac 1{\sqrt m} \cdot \sum_{i = 1}^{m} \xi_i f(Z_i),
    \end{align}
    with $\xi_i$ standard Gaussian random variables, $L_{t}$ the cumulative loss until time $t$, and $\eta > 0$ a regularization parameter.  For some choice of $\eta$, it holds that
    \begin{align}\label{eq:ftpl_gaussian_regret}
        \ee\left[ \reg_T \right] \leq 1 +   \frac{16 \sqrt{B \log\left( 2 T \abs{\cF} \right)\Lstar}}{\rho} + \frac{64B \log(2 T \abs{\cF})}{\rho^2}   .
    \end{align}
\end{theorem}
Note that in the regime where the separation $\rho$ is constant, an example of which we furnish in the sequel, we attain regret $\bigOtil\left( \sqrt{\log(\abs{\cF}) \cdot \Lstar} + \log(\abs{\cF})   \right)$, which is optimal up to logarithmic factors of $T$ \citep{cesa2006prediction}.  Furthermore,  we require only a single oracle call per round with a dataset of size $\bigO(T + m)$.  In the worst case, when no function in $\cF$ performs well in cumulative loss, $\Lstar \approx T$ and we recover a regret scaling as $\bigOtil\left( \sqrt{T \log(\abs{\cF})} / \rho \right)$, which is optimal for constant $\rho$; thus, if we can choose $m$ to be small, we achieve a new oracle-efficient regret bound.

As above, the downside of \Cref{thm:lstar_online} is that we require $\cF$ to be $\rho$-separated, which limits the generality of the bound, although we now highlight several key ways in which this separation can be achieved.  The first way is to construct new points $Z_1, \dots, Z_m$ that explicitly separate the function class $\cF$.  As we are assuming that $\cF$ is finite, we can simply introduce points $Z_1, \dots, Z_{\abs{\cF}}$ such that $f'(Z_f) = \bbI\left[ f = f' \right]$ for all $f, f' \in \cF$.  The downside of this approach is twofold: first, this requires modifying the assumption on the oracle, but, more importantly, it requres feeding in $\bigO(\abs{\cF})$ points to the oracle per round, precluding any hope of efficiency in the typical regime where we expect $\cF$ to be exponentially large.  This polynomial in $\abs{\cF}$ dependence is expected due to the computational lower bound of \citet{hazan2016computational}.  By significantly strengthening the oracle, we can avoid this issue, as we can back $\abs{\cF}$ points into a sphere of dimension $\log(\abs{\cF})$ with constant separation; while this solves the oracle-efficiency issues, it is not at all clear how to instantiate the oracle in practice, even heuristically.
A less general, but more optimistic, way to construct points $Z_1, \dots, Z_m$ separating $\cF$ is to assume sample access to some measure $\mu$ on $\cX$ that $\rho$-separates $\cF$.  In this case, standard uniform concentration bounds imply that we can take $m = \bigO\left( \log(\abs{\cF})  / \rho^2\right)$ (cf. \Cref{lem:separator_set_2}), which implies that achieving average regret $\epsilon$ is possible with $\poly\left( \log(\abs{\cF}) , \rho^{-1}, \epsilon^{-1}\right)$ time. \looseness=-1

A third instance in which $\rho$-separation holds is in the presence of a \emph{small separator set} (cf. \Cref{def:separator_set}) \citep{goldman1993exact,syrgkanis2016efficient}.  Because the existence of a separator set of size $m$ implies that $\cF$ is $\rho$-separated  with $\rho = m^{-1/2}$,  and $\Lstar \leq T$, we see that \eqref{eq:ftpl_gaussian} achieves expected regret $\bigOtil\left( m \log(\abs{\cF}) + \sqrt{m T \log (\abs{\cF})} \right)$, which shaves a $m^{1/4}$ factor from the dominant term in the bound of \citet[Theorem 2]{syrgkanis2016efficient}.  A related condition, termed $\delta$-admissability, is introduced in \citet{dudik2020oracle}, where they use this condition to achieve an oracle-efficient regret bound using a different variant of FTPL; we remark that their condition implies that $\cF$ is $\rho$-separated for a set of size $m$ with $\rho = \delta / \sqrt{m}$ and thus, with the same number of calls to the oracle with the same size set of inputs, our regret becomes (using the $\delta$ from \citet{dudik2020oracle}) $\bigOtil\left( m \log\left( \abs{\cF} \right) / \delta^2 + \sqrt{m T} / \delta \right)$, shaving a factor of $\sqrt{m}$ from that of \citet[Theorem 2.5]{dudik2020oracle}.  We emphasize that neither the result of \citet{syrgkanis2016efficient} nor that of \citet{dudik2020oracle} can adapt to the small loss setting where $\Lstar = o(T)$, but we see that even outside of this regime, \Cref{thm:lstar_online} improves upon the state of the art.

The key reason for the improvement of \Cref{thm:lstar_online} is that the Gaussian perturbation we choose in \eqref{eq:ftpl_gaussian} is naturally adapted to $\ell_2$ geometry as opposed to the $\ell_1$ geometry to which the Laplacian perturbations of many alternative instantiations of FTPL are adapted.  Thus, unlike these other perturbations, there is no explicity cost in \emph{regret} that \Cref{thm:lstar_online} pays for adding more points $Z_i$; the only reason to not increase $m$ is \emph{computational}, i.e., we do not wish to increase $m$ solely in order to avoid the additional time required to feed more points into the oracle at each round. 


\paragraph{Comparison to \citet{wang2022adaptive}.} 
While the aforementioned results are an interesting consequence of \Cref{thm:lstar_online}, the purpose of the algorithm is to consider regimes where $\Lstar$ is small and we can see improved regret.  Such bounds for FTPL were first introduced in \citet{hutter2004prediction} for general classes, but the efficiency of their algorithm scales linearly in $\abs{\cF}$, which is not acceptable in the standard setting where $\cF$ is exponentially large.  We thus compare \Cref{thm:lstar_online} to the approach of \citet{wang2022adaptive}, which presents an efficient instantiation of FTPL capable of achieving $\Lstar$ bounds whenever there exists what  a $\gamma$-\emph{approximable} matrix for $\cF$, defined as follows.
\begin{definition}[Definition 2 from \citet{wang2022adaptive}]\label{def:approximable}
    Given a function class $\cF: \cX \to [-1,1]$, a matrix $\Gamma \in \rr^{\abs{\cF} \times m}$ is said to be $\gamma$-\emph{approximable} with respect to $\cF$ if for all $f \in \cF$ and $(x,y) \in \cX \times \cY$, there is some $s \in \rr^m$ such that $\norm{s}_1 \leq \gamma$ and for all $f' \in \cF$, it holds that $\inprod{\Gamma_f - \Gamma_{f'} }{s} \geq \ell(f(x), y) - \ell(f'(x), y)$.
\end{definition}
Using this definition, \citet[Theorem 1]{wang2022adaptive} shows that given a $\gamma$-approximable matrix and an ERM oracle, one can achieve
\begin{align}\label{eq:wang_regret}
    \ee\left[ \reg_T \right] \lesssim 1 + \left( \log(\abs{\cF}) + \sqrt{m \log(\abs{\cF})} + \gamma \right) \cdot \sqrt{\Lstar} + \gamma^2 + \gamma \left( \log(\abs{\cF}) + \sqrt{m \log\left( \abs {\cF} \right)} \right),
\end{align}
with $T$ calls to the oracle and each call having an input size of $\bigO\left( T + m \right)$.  As the oracle complexity of this algorithm is the same as that of \Cref{thm:lstar_online} over $T$ rounds, we must only compare the resulting regret, i.e., \eqref{eq:ftpl_gaussian_regret} to \eqref{eq:wang_regret}.  
A key point to note is that even as $\gamma \to 0 $, the above regret bound has as its dominant term $ \sqrt{ \Lstar m  \log \abs{ \mathcal{F} }  } $ which can be large even for natural classes with $ \rho = O(1) $.   

\begin{proposition} \label{prop:hadamard_gap}
    For any $m$, there exists a class  $\cF$ with $\abs{ \mathcal{F} } = 2^m$ on a domain of size $m$ with $\rho$-separating measure $\mu$ such that \Cref{alg:ftpl} has regret $\bigOtil\left( \sqrt{ \Lstar \cdot m } \right)$, while the algorithm of \citet{wang2022adaptive} has regret $\bigOtil\left( m \sqrt{ \Lstar } \right)$ with the same runtime.
    
    In addition, there is a class $\mathcal{F}$ with $\abs{ \mathcal{F} }$ on a domain of size $2^m$ such that \Cref{alg:ftpl} has regret $\bigOtil\left( \sqrt{ \Lstar m } \right)$,  the algorithm of \citet{wang2022adaptive} has regret $\bigOtil\left(  \sqrt{ \Lstar m 2^m } \right)$ with the same runtime.
\end{proposition}
    The classes presented here are modifications of the Fourier transform matrix. 
    The reason to present these two results separately (instead of just the exponential gap) is to present the gap in the common regime where the function class size is much larger than that of the domain. This result shows that the algorithm of \citet{wang2022adaptive} suffers because even as $\gamma$ tends to 0, the regret bound still depends on $m$. 
    On the other hand, we note that through duality, $\gamma$-approximability implies $\rho$-separation, although at the cost of an additional factor of $\sqrt{m}$.
We formally present this in the following lemma and defer the proof to \Cref{sec:approximability}. 

\begin{corollary}\label{cor:gamma_approx}
    Suppose that $\cF$ has a $\gamma$-approximable matrix $\Gamma \in \rr^{\abs{\cF} \times m}$.  Then running the Gaussian FTPL algorithm in \eqref{eq:ftpl_gaussian} achieves regret
    \begin{align}
        \ee\left[ \reg_T \right] \lesssim 1 + \gamma \cdot \sqrt{m \log\left( T \abs{\cF} \right) \Lstar} + m \gamma^2 \log\left( T \abs{\cF} \right).
    \end{align}
\end{corollary}



Comparing \Cref{cor:gamma_approx} to \eqref{eq:wang_regret}, we see that on the dominant term, up to logarithmic factors, we have replaced a term scaling like $\bigOtil\left( \sqrt{\Lstar} \cdot \left( \gamma + \sqrt{m \log\left( \abs{\cF} \right)} \right) \right)$ with one scaling like $\bigO\left( \gamma \cdot \sqrt{m \log\left( \abs{\cF} \right) \Lstar} \right)$, which amounts to a $\sqrt{m}$ reduction in efficiency.  


Summarizing the above discussion, we see that while $\gamma$-approximability implies $\rho$-separation at the cost of a polynomial blowup in the regret, there are natural examples of function classes that have large separation of which the algorithm of \citet{wang2022adaptive} does not fully take advantage, leading to large (even exponential) suboptimality with respect to \Cref{alg:ftpl}.









































\subsection{Privacy}\label{ssec:privacy}


    In this section, we show that an adaptation of \Cref{alg:ftpl}, whose pseudocode can be found in \Cref{alg:erm_perturbed}, is an oracle-efficient, differentially private learner.  Indeed, the following result shows that under constant separation, our algorithm achieves optimal rates in these condtions.
\iftoggle{colt}{
    \begin{algorithm2e}[t]
        \hrulefill
        \caption{Perturbed ERM with Gaussian Perturbation}
        \label{alg:erm_perturbed}
        \SetKwInput{KwInput}{Input}                %
        \SetKwInput{KwOutput}{Output}              %
        \DontPrintSemicolon
        \KwInput{Dataset $S = \{ (x_1, y_1), (x_2,y_2) \dots, (x_n,y_n) \}$, separating distribution $\mu$, auxiliary sample size $m$, loss function $\ell(w,x)$, hypothesis class $\mathcal{F}$, noise level $\eta$}
        Draw $m$ independent samples $Z = \{z_1, z_2, \dots, z_m\} \sim \mu^m$\;
        Draw $m$ independent samples $\omega = \{ \omega_1, \omega_2, \dots, \omega_m\} \sim \mathcal{N} ( 0 , I) $\;
        Compute     $\hat{f} \gets \arg\min_{f \in \mathcal{F}}  \sum_{i=1}^{n} \ell(f(x_i),y_i) + \frac{ \eta }{ \sqrt{m}} \sum_{i=1}^{m} \omega_i f(z_i)$ \;
        \Return $\hat{f}$ \\
        \hrulefill
    \end{algorithm2e}
}{
    \begin{algorithm}[t]
        \vspace{2pt} \hrule \vspace{4pt}  %
        \caption{Perturbed ERM with Gaussian Perturbation}
        \label{alg:erm_perturbed}
        \SetKwInput{KwInput}{Input}                %
        \SetKwInput{KwResult}{Output}              %
        \DontPrintSemicolon
    
        \KwInput{Dataset $S = \{ (x_1, y_1), \dots, (x_n,y_n) \}$, 
                 separating distribution $\mu$, 
                 auxiliary sample size $m$, 
                 loss function $\ell(w,x)$, 
                 hypothesis class $\mathcal{F}$, 
                 noise level $\eta$}
    
        \KwResult{Trained function $\hat{f}$}
    
        \tcp{Step 1: Draw Samples}  
        Draw $m$ independent samples $Z = \{z_1, z_2, \dots, z_m\} \sim \mu^m$\;
    
        \tcp{Step 2: Sample Gaussian Perturbation}  
        Draw $m$ independent samples $\omega = \{ \omega_1, \dots, \omega_m\} \sim \mathcal{N} ( 0 , I)$\;
    
        \tcp{Step 3: Compute Perturbed ERM}  
        Compute $\hat{f} \gets \arg\min_{f \in \mathcal{F}}  
        \sum_{i=1}^{n} \ell(f(x_i),y_i) + \frac{\eta}{\sqrt{m}} \sum_{i=1}^{m} \omega_i f(z_i)$\;
    
        \Return $\hat{f}$\;
        \vspace{4pt} \hrule \vspace{2pt}  %
    \end{algorithm}
}
    
    

    
    \begin{theorem}
        \label{thm:main_privacy}
        Let $\cF: \cX \to [-1,1]$ be a function class and $\ell$ be a Lipschitz loss function bounded by 1. 
        Further, assume that the points $Z_1, \dots, Z_m \in \cX$ are a $\rho$-separating set for $\cF$.  Then \Cref{alg:erm_perturbed} is $(\epsilon, \delta)$-differentially private if $\eta \gtrsim \nicefrac{\left( \cG(\cF) + \sqrt{\log(1/\delta)} \right)}{\rho^2\epsilon}$.
        Furthermore, for $\epsilon \leq 1$, \Cref{alg:erm_perturbed} $(\alpha, \beta)$-learns $\cF$ if 
        \begin{align}
            n \geq \max\left\{  \frac{ \mathcal{G} ( \mathcal{F}   )^2 + \log(1/\beta) }{  \alpha^2} ,  \frac{ \mathcal{G} ( \mathcal{F}   )^2 + \sqrt{ \log(1/\delta) \log(1/\beta) } }{ \alpha \epsilon \rho^2  }    \right\}
        \end{align}
        
        
        
    \end{theorem}
    As we described in \Cref{sec:prelims}, for $\cF$ with VC dimension at most $d$, it holds that $\cG(\cF) \lesssim \sqrt{d} $ and thus the above guarantee recovers the optimal sample complexity of learning \citep{shai} with the typical $\epsilon^{-1}$ blow up due to the privacy requirement.
    Thus, in the presence of the separating condition, we achieve, with a differentially private algorithm, the optimal sample complexity of a \emph{non-private} algorithm, in an oracle-efficient manner.
    Note that such a guarantee is not possible without the separation condition since the sample complexity of private learning is bounded in terms of the Littlestone dimension of the class \citep{AlonLMM19}. 

    
    The work of \citet{Neel0VW20} initiated the study of oracle-efficient differentially private algorithms with a focus on binary-valued function classes $\cF$ with separating sets of size $m$.  Their algorithm, related to our \Cref{alg:erm_perturbed}, achieves a sample complexity bound of $n \gtrsim \nicefrac{d}{\alpha^2} + \nicefrac{m^{3/2}}{\alpha \epsilon}$, where $d$ is the VC dimension of $\cF$.  
    For direct comparison, we state a corollary of our result 

    \begin{corollary}
        Suppose that $\cF$ has a separator set of size $m$.  Then, \Cref{alg:erm_perturbed} achieves $(\epsilon, \delta)$-differential privacy with sample complexity $ n \geq \max\left\{ \nicefrac{d}{\alpha^2} , \nicefrac{m d}{\alpha^2 \epsilon} \right\}  $.
    \end{corollary}
    
    Though\footnote{We remark that \citet{Neel0VW20} also presents an algorithm that is capable of achieving \emph{pure} differential privacy with $\delta = 0$ with a dependence of $m^2$ instead of $m^{3/2}$.}  these guarantees are in general incomparable as at worst $d \leq m$, typically we have $d \ll m$ which can be lead to an improvement in sample complexity.   On the other hand, examples such as \Cref{prop:hadamard_gap} demonstrate that in favorable cases, \Cref{alg:erm_perturbed} can have sample complexity independent of $m$.  

    We also compare our results to those of \citet{block2023oracle}, who studied the related setting of oracle-efficient private learning with access to unlabelled public data (from a closely related distribution).
    They study a variant of \Cref{alg:erm_perturbed} (with an additional output perturbation step) and show that it is $(\epsilon, \delta)$-differentially private for general $\cF$, but the sample complexity that is achieve are large polynomial functions of the accuracy parameter $\alpha$ and the privacy parameters $\epsilon$ and thus significantly worse than the rates appearing in \Cref{thm:main_privacy}.   In addition to the quantitative improvements, while less general, we note that the $\rho$ separation condition does \emph{not} have to do with the test distribution at all and thus certifying the separation condition can be seen as a preprocessing step that can amortized for many future learning tasks with the same hypothesis class.



    















