\section{Introduction}



\abcomment{TODO:
\begin{enumerate}
    \item Work out lower bound to separate earlier result from ours.  Check normalization to see if this is true.
    \item Work out actual regret bound that we get using the stability lemma.
    \item Work out regret bound we get if we instead use offset-gaussian complexity.
    \item Do privacy stuff.
    \item Think of other applications?
\end{enumerate}}

\abcomment{fill in}





\section{Main Proof}

In this section we state and prove the key lemma.

\begin{theorem}\label{thm:main}
    Let $T$ be a set and let $m: T \to \rr$ be a mean function and $K: T \times T \to \rr$ a covariance kernel such that $0 < \kappa \leq K(t, t) \leq 1$ for all $t \in T$.  Suppose $m$ is continuous and separable with respect to the metric on $T$ induced by $K$ and $\omega$ is a Gaussian Process on $T$ with covariance $K$ and let
    \begin{align}
        \Omega(t) = m(t) + \eta \cdot \omega(t)
    \end{align}
    denote an offset Gaussian process.  Letting $\tstar = \argmin \Omega(t)$ and 
    \begin{align}
        \cE(\rho, \tau) = \left\{ \text{there exists } s \in T, \text{ s.t. } \frac{K(s,\tstar)}{K(\tstar, \tstar)} \leq 1 - \rho^2 \text{ and } \Omega(s) \leq \Omega(\tstar) + \tau \right\},
    \end{align}
    we have for any $t$ that
    \begin{align}
        \pp\left( \tstar = t \right) \leq \left(1 + \frac{2 \tau }{ \eta \kappa^2  \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right)\right) \cdot \pp\left( \tstar = t \text{ and } \cE(\rho, \tau)^c \right) + \delta,
    \end{align}
    whenever
    \begin{align}
        \eta \geq \frac{2\tau}{\kappa^2 \rho^2} \left( \ee\left[ \sup_{t \in T} \omega(t) \right] + \sqrt{\log\left( \frac 2\delta \right)} \right).
    \end{align}
\end{theorem}
\begin{proof}
    We modify the proof of \citet[Theorem 5]{block2024oracle} which in turn is an improvement on \citet{block2022smoothed}.  First, for $\delta > 0$, let
    \begin{align}
        \Phi_\delta = \left\{ \sup_{t \in T} \omega(t) \leq  \ee\left[ \sup_{t \in T} \omega(t) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right\}.
    \end{align}

    \ascomment{In this normalization, the second term has a $\max K(t,t)$ right?}\abcomment{yes}
    
    By standard Gaussian tailbounds (see, e.g., \citet[Theorem 8.5.5]{vershynin2018high}), it holds that $\pp(\Phi_\delta) \geq 1 - \delta$.  Now, a union bound implies that
    \begin{align}\label{eq:proof1}
        \pp\left( \tstar = t \right) &\leq \pp\left( \tstar = t \text{ and } \cE(\rho, \tau)^c \right) + \pp\left( \tstar = t \text{ and } \cE(\rho, \tau) \text{ and } \Phi_\delta \right) + \delta.
    \end{align}
    We focus on the middle term and show that
    \begin{align}\label{eq:proof2}
        \pp\left( \tstar = t \text{ and } \cE(\rho, \tau) \text{ and } \Phi_\delta \right) \leq \frac{2\tau}{\eta \kappa^2 \rho^2} \left( \ee\left[ \sup_{t \in T} \omega(t) \right] + \sqrt{\log\left( \frac 1\delta \right)} \right) \cdot \pp\left( \tstar = t \right).
    \end{align}
    Given \eqref{eq:proof2}, rearranging \eqref{eq:proof1} and observing that $(1-x)^{-1} \leq 1 + 2x$ for $0 \leq x \leq 1$ concludes the proof of the theorem.

    To establish \eqref{eq:proof2}, we follow \citet{block2024oracle} and introduce for $y \in \rr$ and $s, t \in T$:
    \begin{align}\label{eq:proof3}
        m_{t,y}(s) = m(s) + \frac{K(s,t)}{K(t,t)}(y - m(t)), \qquad a(s) = \frac{\tau}{\rho^2} \cdot \frac{K(s,t)}{K(t,t)}, \qquad \text{and} \qquad b(s) = \frac{\tau}{\rho^2} - a(s).
    \end{align}
    It is immediate that if $K(s,t) \leq (1 - \rho^2) K(t,t)$, then $b(s) \geq \tau$; moreover, $b(s) \geq 0$ for all $s$ by Cauchy-Schwarz and $m_{t, y + \tau/\rho^2}(s) = m_{t,y}(s) + a(s)$ pointwise.  The utility of introducing $m_{t,y}$ is that the distribution of $\Omega(s)$ conditioned on $\Omega(t) = y$ is also a Gaussian process with mean $m_{t,y}$ and covariance $K_t$, independent of $y$.  Thus, we have for all fixed $t \in T$ and $y \in \rr$
    \begin{align}
        \pp&\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) \geq y + \tau | \Omega(t) = y \right) \\
        &\geq \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) - b(s) \geq y | \Omega(t) = y \right) \\
        &=  \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) - b(s) - a(s) + a(s) \geq y | \Omega(t) = y \right) \\
        &=  \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s)  + a(s) \geq y + \frac{\tau}{\rho^2} | \Omega(t) = y \right) \\
        &= \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) \geq y + \frac{\tau}{\rho^2} | \Omega(t) = y + \frac{\tau}{\rho^2} \right).
    \end{align}
    Letting
    \begin{align}
        q_t(y) = (2 \pi K(t,t,)^{-1/2}) \cdot \exp\left( -\frac{(y - m(t))^2}{2 \eta^2 K(t,t)} \right)
    \end{align}
    denote the density of $\Omega(t)$, we have
    \begin{align}
        \pp&\left( \tstar = t \text{ and } \cE(\rho, \tau) \text{ and } \Phi_\delta \right) \\
        &= \pp\left( \tstar = t  \text{ and } \Phi_\delta \right)  - \int_{-\infty}^\infty \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) \geq y + \tau | \Omega(t) = y \right)q_t(y) d y \\
        &\leq \pp\left( \tstar = t  \text{ and } \Phi_\delta \right)  - \int_{-\infty}^\infty \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) \geq y + \frac{\tau}{\rho^2} | \Omega(t) = y + \frac{\tau}{\rho^2} \right) q_t(y)d y \\
        &= \int_{-\infty}^\infty \left( q_t(y) - q_t\left( y - \frac{\tau}{\rho^2} \right) \right) \pp\left( \Phi_\delta, \, \tstar=t \text{, and } \inf_{K(s,t) \leq (1- \rho^2) K(t,t)} \Omega(s) \geq y  | \Omega(t) = y  \right) d y \\
        &= \int_{-\infty}^\infty \left( q_t(y) - q_t\left( y - \frac{\tau}{\rho^2} \right) \right) \pp\left( \Phi_\delta \text{ and } \tstar=t  | \Omega(t) = y  \right) d y.
    \end{align}
    Noting now that
    \begin{align}
        q_t(y) - q_t\left( y - \frac{\tau}{\rho^2} \right) \leq \frac{\tau q_t(y)}{ \eta^2 \kappa^2  \rho^2} (y - m(t))
    \end{align}
    by the proof of \citet[Theorem 5]{block2024oracle}, we have that
    \begin{align}
        \pp\left( \tstar = t \text{ and } \cE(\rho, \tau) \text{ and } \Phi_\delta \right) &\leq \frac{\tau }{ \eta^2 \kappa^2  \rho^2} \int_{-\infty}^\infty (y - m(t)) \pp\left( \Phi_\delta \text{ and } \tstar=t  | \Omega(t) = y  \right) q_t(y) d y \\
        &= \frac{\tau }{ \eta^2 \kappa^2  \rho^2} \cdot  \ee\left[ \eta  \cdot \omega(t) \cdot \bbI\left[ \Phi_\delta \text{ and } \tstar = t \right] \right] \\
        &= \frac{\tau }{ \eta \kappa^2  \rho^2} \cdot \ee\left[ \omega(\tstar) \bbI[\Phi_\delta] | \tstar = t \right] \cdot \pp\left( \tstar = t \right).
    \end{align}
    Now, noting that 
    \begin{align}
        \ee\left[ \omega(\tstar) \bbI[\Phi_\delta] | \tstar = t \right]  \leq \ee\left[ \sup_{t \in T} \omega(t) \right] + \sqrt{\log\left( \frac 2\delta \right)}
    \end{align}
    by definition, we conclude the proof.
\end{proof}

\begin{theorem}
    Let $\cF: \cX \to [-1,1]$ denote a function class and let $\ell$ denote a loss function bounded in $[-1,1]$.  Suppose that there is some measure $\mu$ for which $\norm{f - f'}_{L^2(\mu)} \geq \rho$ for all $f \neq f' \in \cF$ and $\norm{f}_{L^2(\mu)} \geq \kappa$ for all $f$.  Letting
    \begin{align}
        f_t \in \argmin L_{t-1}(f) + \eta \cdot \omega(f)
    \end{align}
    where $\omega$ is a centred Gaussian process on $(\cF, \norm{\cdot}_{L^2(\mu)})$ and
    \begin{align}
        L_t(f) = \sum_{s = 1}^t \ell_s(f),
    \end{align}
    we have that for some choice of $\eta$, it holds that
    \begin{align}
        \ee\left[ \reg_T \right] \leq 1 + \frac{4\sqrt{\log(2T)}}{\kappa^2 \rho^2} \cdot \ee\left[ \sup_{f \in \cF} \omega(f) \right]^2 + \frac{\ee\left[ \sup_{f \in \cF} \omega(f) \right] }{\kappa \rho} (\log(2T))^{1/4} \cdot \sqrt{\Lstar}.
    \end{align}
\end{theorem}
\begin{proof}
    From the Be-the-Leader lemma \abcomment{cite} and \Cref{thm:main}, we have for sufficiently large $\eta$,
    \begin{align}
        \ee\left[ \reg_T \right] &\leq 2 \eta \cdot \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sum_{t = 1}^T \ee\left[\ell_t(f_t) - \ell_t(f_{t+1})\right] \\
        &\leq 2 \eta \cdot \ee\left[ \sup_{f \in \cF} \omega(f) \right] +\frac{2 \tau }{ \eta \kappa^2  \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right) \cdot \sum_{t = 1}^T \ee\left[ \ell_t(f_{t+1}) \right] + \delta T \\
        &\leq 3 \eta \cdot \ee\left[ \sup_{f \in \cF} \omega(f) \right] +\frac{2 \tau }{ \eta \kappa^2  \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right) \cdot \Lstar + \delta T.
    \end{align}
    Letting
    \begin{align}
        \eta &= \frac{2\tau}{\kappa^2 \rho^2} \left( \ee\left[ \sup_{t \in T} \omega(t) \right] + \sqrt{\log\left( \frac 2\delta \right)} \right) \\
        &\quad+ \sqrt{6 \cdot \ee\left[ \sup_{f \in \cF} \omega(f) \right] \cdot \frac{\tau }{ \eta \kappa^2  \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right) \cdot \Lstar}
    \end{align}
    and setting $\tau = 1$ and $\delta = 1/T$ concludes the proof.



    Follows from follow-the-leader and the previous theorem. \abcomment{fill in rigorously}
\end{proof}
As an instantiation of the above, suppose that $\cF$ has VC dimension $d$ and a separator set of size $m$.  Then the regret becomes
\begin{align}
    \ee\left[ \reg_T \right] \lesssim \sqrt{\log(T)} \cdot d m + \sqrt{d m \log(T) \cdot \Lstar}.
\end{align}
While nice, this rate depends on the product $dm$ which can be quite large.  If we replace $\omega$ with the offset Gaussian process $\inprod{\gamma}{f} - \alpha \cdot \norm{f}^2$, then we can get a rate of
\begin{align}
    \ee\left[ \reg_T \right] \lesssim \polylog(T) \left( d^2\log^2(m) + d \log(m) \cdot \sqrt{\Lstar} \right),
\end{align}
which is almost tight up to the additional polynomial factor in $d$.






Note that our separation condition is a weaker assumption then that of $\gamma$-approximability from \citet{wang2022adaptive}.  Recall their definition (translated into our language):
\begin{definition}[Definition 2 from \citet{wang2022adaptive}]\label{def:strong_separation}
    Let $\cF$ be finite of size $K$ and let $z_1, \dots, z_m$ be points.  We say that the set of points $z_1, \dots, z_m$ is $\gamma$-approximable if for all $f \in \cF$ and all $(x,y) \in \cX \times \cY$, there is some $s \in \rr^m$ with $\norm{s}_1 \leq \gamma$ such that for all $f' \in \cF$ it holds that
    \begin{align}
        \sum_{i = 1}^m s_i (f(z_i) - f'(z_i)) \geq \ell(f(x), y) - \ell(f'(x), y).
    \end{align}
\end{definition}
By writing our separation condition with respect to a dual norm, we get
\begin{align}
    \inf_{f \neq f'} \sup_{\norm{s}_2 \leq 1}  \inprod{s}{f - f'} \geq  \rho.
\end{align}
Pretending that the conditions for the minimax theorem hold, if we swap the infimum and supremum, we get
\begin{align}
    \inf_{f \in \cF} \sup_{\norm{s}_2 \leq 1} \inf_{f' \neq f} \inprod{s}{f - f'} \geq  \rho.
\end{align}
If we strengthen the condition on the set over which we take the supremum to be over the $\ell^1$ ball as opposed to the Euclidean ball, we arrive at
\begin{align}
    \inf_{f \in \cF} \max_{\norm{s}_1 \leq 1} \inf_{\substack{x \in \cX \\ f'(x) \neq f(x)}} \inprod{s}{f|_{\cZ} - f'|_{\cZ}} \geq  \rho,
\end{align}
which is a weakening of the condition in \Cref{def:strong_separation} when $\ell$ is the indicator loss, where we now allow the $s$ to depend on the $x,y$ pair.

More formally, note that if $\Gamma$ is $\gamma$-approximable, then by Cauchy-Schwarz, we see that for all $x,y$ it holds that $\gamma \norm{f- f'} \geq \ell(f(x), y) - \ell(f'(x), y)$ and thus $\rho$-separation on the empirical measure on the $m$ points holds for
\begin{align}
    \rho \leq \frac{\sup_{\substack{f,f' \in \cF \\ (x,y)}} \ell(f(x), y) - \ell(f'(x), y)}{\gamma}.
\end{align}
\abcomment{Come up with lower bound demonstrating that we are strictly more general?  Maybe $\cF$ projected onto $z_1, \dots, z_m$ is a rotated cube with vertices in $\left\{ \pm m^{-1/2} \right\}^m$?}




\abcomment{
    Remarks:
    \begin{enumerate}
        \item Examples include when $\cF$ as a separator set of size $m$, we can take $\rho = 1/m$.
        \item This also applies to real-valued function classes, unlike previous separator set based algorithms.
        \item Holds for all finite $\cF$.  Does not contradict Hazan-Koren because the separation can be polynomial in $\cF$.
        \item Can be adapted to give an $\Lstar$ plus $T^{2/3}$ bound for smoothed functions by applying surprise lemma \citep{block2024performance}.  This is worse than previous bounds.
        \item Can we show this is strictly more general than assumption in \citet{wang2022adaptive}? Maybe not by minimax theorem (rewrite norm as dual and swap min over $f,f'$ with max over $s$)?  Emphasize that packing framing leads to simpler way to think about separation and naturally extends to $\mu$ that are not finitely supported. 
        \item Also improves oracle-efficient differential privacy is achievable for well-separated function classes.
        \item Use offset Gaussian complexity instead of Gaussian complexity and get tighter bounds.
    \end{enumerate}
}

\abcomment{Use JL/probabilistic method to find points that are mostly separated for the Hadamard matrix function class.}
