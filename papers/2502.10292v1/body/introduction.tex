
\abcomment{
    TODOs:
    \begin{enumerate}
        \item Move algs to top of page and put them in main results section.
        \item Write abstract.
        \item Polish body.
        \item Add related work
        \item @Abhishek Add example of separated class that we may care about (signals processing/tight frames/spectral condition/etc?)
    \end{enumerate}
}

In order to satisfy modern resource constraints it is important to develop computationally efficient algorithms for learning.  Unfortunately, many function classes of interest are computationally hard to learn, which prompts the search for algorithms that make efficient use of computational primitives, or oracle, that have heuristically motivated and practical implementation in settings of interest.  To wit, a common such primitive in learning is the notion of an Empirical Risk Minimization (ERM) Oracle, which grants one the ability to efficiently find a function that minimizes the empirical loss of some loss function on a given data set.  This oracle is motivated both by it sufficiency for classical learning \citep[Chapter 4]{shai} and its practical instantiation in a variety of settings of interest, such as deep neural networks \citep{lecun2015deep}.  
While a single call to an ERM oracle on independent data suffices for classical learning, there are many alternative settings where the extent to which this oracle can aid learning is less well understood; two examples of such are online and differentially private learning.

In online learning, a learner receives adversarially generated data points one in a series of $T$ rounds and, at each time step $t$, makes a prediction; the learner attempts to minimize his regret to the best function in a given class $\cF$ viewed in hindsight \citep{cesa2006prediction}.  
In addition to being an important learning setting in its own right, online learning algorithms are an important primitive in a number of other applications including sequential decision-making \citep{Lattimore_Szepesv√°ri_2020}, algorithm design \citep{MWU}, learning in games \citep{cesa2006prediction}, and fairness \citep{haghtalab2023unifying}. 
While online learning is separated from classical learning by removing assumptions on the data, differentially private learning \citep{DworkMNS06} allows independent data but forces the algorithm to satisfy strong stability constraints that ensure that release of a trained model does not leak private information about the data on which it is trained.  Due to the increasing deployment of machine learning models, often trained on sensitive customer data, privacy has seen wide application in many fields \citep{dwork2014algorithmic} and has even been adopted by the US Census \citep{abowdcensus}.  While a priori these two settings may seem quite different, in fact they are intimately related because successful algorithms in both settings require some degree of \emph{stability}.  Indeed, a formal example of this connection is the fact that the combinatorial notion of complexity known as the \emph{Littlestone dimension} \citep{littlestone1988learning,ben2009agnostic} characterizes learnability in both settings \citep{AlonLMM19,bun2020equivalence}.

Another way in which online and differentially private learning are connected is in the study of small loss (or $\Lstar$) bounds in online learning \citep[Section 2.4]{cesa2006prediction}.
Typically, one expects regret for online learning to scale like $\bigO(\sqrt{T})$ in the worst case.  Often, however, one may expect $\cF$ to be chosen so that the $f \in \cF$ performing best in hindsight has total loss $\Lstar = o(T)$; in such cases, one may hope for algorithms  whose regret scales with $\Lstar$ instead of with $T$, automatically adapting to the beneficial problem instance.  Prior work \citep{hutter2004prediction,abernethy2019online,wang2022adaptive} has identified differential privacy as an essential ingredient to proving $\Lstar$ bounds for online learning for a family of algorithms conducive to the application of an ERM oracle, but the generality in which one can efficiently achieve such guarantees are unknown.  \looseness-1

In this work, we identify a new condition on function classes, $\rho$\emph{-separation} (\Cref{def:rho_separating}), that suffices to ensure oracle-efficient online and differentially private learning.  Our condition unifies and generalizes several earlier approaches including small separator sets \cite{dudik2020oracle,syrgkanis2016efficient} and $\gamma$-approximability \citep{wang2022adaptive}.  The notion of $\rho$-separation requires that functions in $\cF$ are sufficiently distinguishable in the sense that distinct functions are not too close to each other in $L^2(\mu)$ for some measure $\mu$; intuitively, this separation helps `boost' stability in an $L^2$ sense to differential privacy.  Using $\rho$-separation, we demonstrate the following:
\begin{itemize}
    \item We present an oracle-efficient algorithm for online learning (\Cref{thm:lstar_online}) that attains a small-loss ($\Lstar$) bounds whenever the function class is $\rho$-separated.  We compare our regret to that of alternative oracle-efficient algorithms and find that our approach generalizes and, in many settings, improves upon existing methods.
    \item We show that a variant of the algorithm used for online learning is capable of differentially private PAC-learning with optimal rates whenever $\cF$ is $\rho$-separated, improving on existing results in many cases.
\end{itemize}
In the course of our analysis, we prove a new stability bound for minimizers of Gaussian Processes (\Cref{lem:gp_stability_cor}) that stengthens prior such bounds both in the precise notion of stability controlled and the generality of the result itself.  In \Cref{sec:prelims} we state a number of prerequisite notions from learning theory as well as provide a formal definition of $\rho$-separation.  In \Cref{sec:main}, we provide rigorous statements of our main results as well as the oracle-efficient algorithms achieving the stated bounds; we conclude in \Cref{sec:gp_stability} by providing the main ideas behind our proofs.  We defer an extended discussion of related work to \Cref{app:related_work}.













































