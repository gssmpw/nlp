
In this section, we prove \Cref{thm:lstar_online}.  
We rely on the `Be-The-Leader' Lemma introduced in \citet{kalai2005efficient} and discussed in \citet{cesa2006prediction}.  In particular, we invoke the following version of the lemma:
\begin{lemma}[Lemma 31 from \citet{block2022smoothed}]\label{lem:btl}
    Let $x_t, y_t$ be a possibly adaptively chosen sequence of contexts and labels, let $\ell$ be a loss function, denote $\ell_t(f) = \ell(f(x_t), y_t)$, and let 
    \begin{align}
        L_t(f) = \sum_{s = 1}^t \ell_t(f).
    \end{align}
    Let $R_t: \cF \to \rr$ be a sequence of identically distributed functionals on $\cF$ and let
    \begin{align}
        f_t \in \argmin_{f \in \cF} L_{t-1}(f) + R_t(f).
    \end{align}
    Then it holds for any learner playing $f_t$ at time $t$ that
    \begin{align}
        \ee\left[ \reg_T \right] \leq \ee\left[ \sup_{f \in \cF} R_1(f) \right] - \ee\left[ \inf_{f \in \cF} R_1(f) \right] + \sum_{t = 1}^T \ee\left[ \ell_t(f_t) - \ell_t(f_{t+1}) \right].
    \end{align}
    Furthermore,
    \begin{align}
        \ee\left[ \sum_{t = 1}^T \ell_t(f_{t+1}) \right] \leq \Lstar + \ee\left[ \sup_{f \in \cF} R_1(f) \right] - \ee\left[ \inf_{f \in \cF} R_1(f) \right]
    \end{align}
\end{lemma}
\Cref{lem:btl} allows us to control the regret of FTPL style algorithms by bounding two terms: a `bias' term measuring the size of the perturbation and a `variance' term that controls the stability of the predictions enforced by the perturbation itself.  In our case, we invoke the lemma with $
\omega_t$ from \eqref{eq:ftpl_gaussian} as the perturbation and observe that, because $\omega_t(f)$ is symmetric, we can collaps the difference between expected supremum and infimum into a single term.  We now apply \Cref{lem:gp_stability_cor} to control the stability term.
\begin{proof}[\pfref{thm:lstar_online}]
    By \Cref{lem:btl}, it holds that
    \begin{align}
        \ee\left[ \reg_T \right] &\leq 2 \eta \cdot \ee\left[ \sup_{f \in \cF} \omega_1(\cF) \right] + \sum_{t = 1}^T \ee\left[ \ell_t(f_t) - \ell_t(f_{t+1}) \right] \\
        &= 2 \eta \cdot \ee\left[ \sup_{f \in \cF} \omega_1(\cF) \right] + \sum_{t = 1}^T \sum_{f \in \cF} \ell_t(f)  \left( \pp(f_t = f) - \pp(f_{t+1} = f) \right). \label{eq:online_learning_proof1}
    \end{align}
    For a fixed $t$ and fixed $f$, note that because $\abs{L_{t-1}(f) - L_t(f)} \leq B$ for all $f$, we may apply \Cref{thm:separated_privacy}
    \begin{align}
        \pp(f_t = f) - \pp(f_{t+1} = f) &\leq  \delta + \left(1 + \frac{64 B }{ \eta   \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega_t(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right)\right) \cdot \pp\left( f_{t+1} = f \right) - \pp(f_{t+1} = f) \\
        &= \delta + \frac{64 B }{ \eta  \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega_t(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right) \cdot \pp\left( f_{t+1} = f \right),
    \end{align}
    as long as
    \begin{align}\label{eq:online_learning_proof_etalb}
        \eta \geq \frac{64 B }{   \rho^2}  \cdot\left( \ee\left[ \sup_{f \in \cF} \omega_t(f) \right] + \sqrt{\log\left( \frac 2\delta \right)}\right)
    \end{align}
    Setting $\delta = \nicefrac{1}{T \abs{\cF}}$, we see that
    \begin{align}\label{eq:online_learning_proof2}
        \pp(f_t = f) - \pp(f_{t+1} = f) \leq \frac{1}{T \abs{\cF}} + \frac{64 B}{\eta\rho^2} \cdot\sqrt{\log\left( 2 T |\cF| \right)} \cdot \pp\left( f_{t+1} = f \right)
    \end{align}
    Plugging \eqref{eq:online_learning_proof2} into \eqref{eq:online_learning_proof1} we get
    \begin{align}
        \ee\left[ \reg_T \right] \leq 2 \eta \cdot \sqrt{\log \abs{\cF}} + 1 + \frac{64 B}{\eta  \rho^2} \cdot\sqrt{\log\left( 2 T |\cF| \right)} \cdot \left( \Lstar + 2 \cdot \ee\left[ \sup_{f \in \cF} \omega_1(f) \right] \right)
    \end{align}
    whenever $\eta$ satisfies \eqref{eq:online_learning_proof_etalb}.  Now, if
    \begin{align}
        \frac{2 \cdot \sqrt{B (\Lstar + \log\left( \abs{\cF} \right))}}{\rho} \geq \frac{64 B }{   \rho^2}  \cdot\sqrt{\log\left(  2T \abs{\cF} \right)},
    \end{align}
    then we may set $\eta$ to the left-hand side above; otherwise, we minimize $\eta$ subject to \eqref{eq:online_learning_proof_etalb} to get the desired result.
\end{proof}


