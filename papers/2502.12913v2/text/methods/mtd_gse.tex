\input{figures/fig_mtd_comparison_format}


\paragraph{Group-Shared Exponents Integer (GSE-INT).} 

Inspired with block FP~\cite{zhang2022fast}, we propose the Group-Shared Exponents Integer (GSE) format as an alternative to FP formats for matrix multiplication in both forward-propagation and back-propagation. 
This format is also used for storing activations required by back-propagation to reduce memory consumption. 
% The GSE format enables lower bit usage while maintaining training accuracy. 
As illustrated in Fig.~\ref{fig:comparison_format}, GSE introduces the following key modifications compared to traditional floating-point formats:
(1) To leverage the locality of tensor values, we share the exponent across a group of N numbers. That is, all N numbers within the group use the same exponent.
(2) The number of bits used for the shared exponent is fixed at 5.
(3) The implicit leading 1 in floating-point representations is removed and replaced with a standard integer representation.
The numerical representation in GSE is:
% \vspace{-2mm}
\begin{equation*}
% \small
\begin{aligned}
    x = (-1)^{s} \cdot 2^{e} \cdot m
% \vspace{-6mm}
\end{aligned}
\end{equation*}
where $s$ is sign, $e$ is the exponent value (For simplicity, we omit the exponent bias), $m$ is the mantissa value.
The GSE format is memory efficient through sharing exponent bits. Memory for FP is $N(E+M+1)$ and memory for GSE is $N(M+1)+E$. As the group size N increases, the memory savings grow proportionally, while the overhead of the shared exponent is negligible.




\paragraph{Matrix Multiplication using GSE.}

Consider two vectors, \( \mathbf{A} \) and \( \mathbf{B} \), both represented using the GSE format and having a length of \( N \). The dot product of the two vectors can be computed as:
% \vspace{-2mm}
\begin{equation*}
\small
\begin{aligned}
y &= 2^{e_A + e_B} \underbrace{\sum_{i=1}^N (-1)^{s_A \oplus s_B} m_{A,i} m_{B,i}}_{\mathclap{\text{standard integer multiply-accumulate}}},
\label{eq:GSE_dotprod}
% \vspace{-2mm}
\end{aligned}
\end{equation*}
% 
where \( m_{A,i} \) and \( m_{B,i} \) are the integer mantissas of the \( i \)-th elements of the vectors. The computation involves a standard integer multiply-accumulate (MAC) operation, followed by scaling with the combined exponent \( 2^{e_A + e_B} \).

The dot product operation can be extended to large-scale matrix multiplication. For two matrices \( \mathbf{X} \) and \( \mathbf{Y} \), we partition the data into groups of size \( N \). Specifically, rows of \( \mathbf{X} \) are grouped along their elements, with each group sharing a single exponent, and columns of \( \mathbf{Y} \) are grouped similarly. This grouping strategy simplifies hardware implementation and makes the GSE format a practical and efficient choice for large-scale matrix operations.

% \vspace{-3mm}
\paragraph{Transform from FP to GSE.} 

The transformation from FP representation to GSE format is efficient due to the design of GSE. First, within a group of \( N \) FP numbers, identify the largest exponent \( e_{\text{max}} \) among them. Then, double \( e_{\text{max}} \) to account for the shared exponent of the group. For each FP value in the group, its mantissa is adjusted by adding the implicit leading bit (if applicable) and then right-shifting the value based on the difference between its original exponent and \( e_{\text{max}} \). This process ensures that all values are aligned to the shared exponent, leading the storage and computation efficiency while preserving precision.
