% \begin{figure*}[t]
%   \centering
%    \includegraphics[width=\linewidth]{figures/FQFTV2.pdf}
%    % \vspace{-2mm}
%    \caption{The FQFT framework and dataflow of proposed GSQ-Tuning.}
%    \label{fig:FQFT}
% \vspace{-4mm}
% \end{figure*}

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{figures/forward_backward.pdf}
   % \vspace{-2mm}
   \caption{Dataflow of GSQ-Tuning. The weight is NF4 in full-rank branch and is FP32 in low-rank branch.}
   \label{fig:GSQ_tuning}
\vspace{-4mm}
\end{figure}

\subsection{Fully Quantized Fine-tuning}
\label{sec: FQFT}

As illustrated in Fig.~\ref{fig:GSQ_tuning}, our GSQ-Tuning framework introduces a hardware-efficient quantization pipeline. Compared to QLoRA, we fully quantize weights, activations, and gradients to low-bit integers. While QLoRA primarily focuses on 4-bit quantization of frozen base model weights (NF4) while keeping adapters in high precision (BF16), our approach achieves superior computational and memory efficiency. Building on the \textit{quantize-compute-dequantize} (QCD) paradigm for low-precision matrix multiplication (MM)~\citep{jetfire}, the QCD approach operates in three stages: (1) \textit{Quantization}: Convert high-precision inputs matrices (e.g., BF16) to low-precision (e.g., GSE-INT6) using a quantizer $Q(\cdot)$; (2) \textit{Computation in low-precision MM}: Perform low-precision MM to produce an intermediate output (e.g., GSE-INT6); and (3) \textit{Dequantization}: Convert output back to high-precision using a dequantizer $Q^{-1}(\cdot)$. 
% \vspace{-7mm}
\paragraph{Forward Propagation.} The forward propagation for a linear layer is calculated as follows:
\begin{equation*}
\small
\begin{aligned}
\label{forward}
    \mathbf{Y^{BF16}} & = \underbrace{Q^{-1}\left(Q(\mathbf{X^{BF16}}) Q\left(\textit{DQ}(\mathbf{W^{NF4}})\right)^T \right)}_{\text{frozen base model}} \\
    & \hfill + \underbrace{Q^{-1}\left(Q(\mathbf{X^{BF16}})Q(\mathbf{A^{BF16}})^TQ(\mathbf{B^{BF16}})^T \right)}_{\text{trainable adapter}}
\end{aligned}
\end{equation*}
% \vspace{-7mm}
\paragraph{Backward Propagation.}
Gradients are computed directly on quantized tensors using back propagation and chain rule:
% \vspace{-5mm}
\begin{equation*}
\small
\begin{aligned}
\vspace{-2mm}
\label{back_a}
\frac{\partial \mathcal{L}}{\partial \mathbf{A}} = Q^{-1}(Q(\mathbf{B})^TQ\left( \frac{\partial \mathcal{L}}{\partial \mathbf{Y}} \right)^T Q(\mathbf{X})) 
\end{aligned}
\end{equation*}

\vspace{-3mm}

\begin{equation*}
\small
\begin{aligned}
\vspace{-2mm}
\label{back_b}
\frac{\partial \mathcal{L}}{\partial \mathbf{B}} = Q^{-1}(Q\left(\frac{\partial \mathcal{L}}{\partial \mathbf{Y}}\right)^T Q(\mathbf{X})Q(\mathbf{A})^T) \\
\end{aligned}
\end{equation*}

\vspace{-3mm}

\begin{equation*}
\small
\begin{aligned}
\vspace{-2mm}
\label{back_x}
 \frac{\partial \mathcal{L}}{\partial \mathbf{X}} &= Q^{-1}(Q\left(\frac{\partial \mathcal{L}}{\partial \mathbf{Y}}\right)\left( Q(\mathbf{W}) + Q(\mathbf{B}) Q(\mathbf{A}) \right))
\end{aligned}
\end{equation*}
% \vspace{-6mm}
