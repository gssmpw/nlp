\subsection{Preliminaries}
\label{sec: background}

\noindent\textbf{Low-rank Adaptation.} LoRA~\cite{hu2021lora} is a milestone method that injects trainable low-rank adapters into linear layers, allowing efficient fine-tuning while keeping the original parameters unchanged. Specifically, a LoRA linear layer is parameterized by a non-trainable weight matrix $\mathbf{W}\in\mathbb{R}^{oc\times ic}$, along with trainable components $\mathbf{A} \in\mathbb{R}^{r\times ic}$ and $\mathbf{B} \in\mathbb{R}^{oc\times r}$, where $r$ is a small integer. The input $\mathbf{X} \in \mathbb{R}^{b \times ic}$ and output $\mathbf{Y} \in \mathbb{R}^{b \times oc}$ correspond to a linear layer with $oc\times ic$ processing a batch of size $b$. Building on LoRA, QLoRA integrates it with 4-bit NormalFloat (NF4) quantization and Double Quantization (DQ)techniques, enabling the fine-tuning of a 65B parameter model on a single 48GB GPU with minimal performance loss. In this paper, due to the memory constraint of on-device PEFT, we adopt QLoRA to quantize the weights of LLMs. The formulation is:
% \vspace{-3mm}
\begin{equation*}
\small
\begin{aligned}
\vspace{-5mm}
    \label{eq:peft-qlora}
    \mathbf{Y = X \textit{DQ}(W^{NF4})^T + X A^T B^T}
\vspace{-5mm}
\end{aligned}
\end{equation*}
where we omit the transpose for similarity, $NF4$ means the 4 bit NormalFloat (NF) data type and and $DQ$ is \textit{Double Quantization} operation to map weights from NF4 to BF16 in QLoRA. The low-rank components $\mathbf{A}$ and $\mathbf{B}$ and input $\mathbf{X}$ remain in BF16 during fine-tuning process. However, QLoRA still does not suit on-device PEFT scenarios because the low-rank term $X A^T B^T$ remains in BF16, whereas most on-device hardware only supports integer operations. This limitation motivates our development of a new LoRA method that relies exclusively on integer operations.

\noindent\textbf{Quantization.} Quantization~\citep{Jacob_2018_CVPR} is a crucial technique that maps a floating-point number to a discrete interval using integer values. Given a floating-point (FP) tensor $\mathbf{x}$ (such as weights, activations or gradients),
the $b$-bits formulation is:
% it can be uniformly quantized to $b$-bits as follows:
% \begin{equation*}
\begin{equation*}
\small
\begin{aligned}
\label{eq:quant}
    \mathbf{\mathbf{x}_{int}} = Q(\mathbf{x}) & = \mathrm{clamp}\left(\left\lfloor \frac{\mathbf{x}}{s} \right\rceil + z, 0, 2^b - 1\right) 
    % \\
    % \hat{\mathbf{x}} & = s(\mathbf{x_{int}} - z)
\vspace{-2mm}
\end{aligned}
\end{equation*}
where $s = \frac{\max(|\mathbf{x}|)}{2^{b-1}-1}$ is the scaling factor, $z$ denotes zero points, $\lfloor \cdot \rceil$ refers to the round-to-nearest, and the function $\mathrm{clamp}(\cdot)$ clips values outside the integer range $\left[q_{\min}, q_{\max} \right]$ respectively. $\left[q_{\min}, q_{\max} \right]$ is the quantization range determined by the bit width $b$, where $q_{\min}=-sz$ and $q_{\max}=-s(2^b-1-z)$. 
\noindent\textbf{Fully Quantized Training (FQT).} FQT involves quantizing all tensors—weights, activations, and gradients—needed for computation-intensive operations (like matrix multiplication) during both forward and backward propagation~\cite{wang2018training,yang2020training,zhu2020towards}. When the network's weights, activations, and gradients are each quantized to 8 bits, this is referred to as W8A8G8 quantization. Notably, FQT is different from Quantization-Aware Training (QAT), we also discuss the difference in Sec.~\ref{qat_fqt}.