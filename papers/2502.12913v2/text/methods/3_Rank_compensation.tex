\subsection{Pareto Frontier for Quantization Bits and Low-rank.}
\label{sec:Pareto}


\paragraph{Co-optimization Principle for Model Bits and Rank.} The memory footprint and FLOPs during fine-tuning exhibit strong dependence on both quantization bit-width and LoRA rank $\mathcal{O}(b \cdot r)$ scaling. Excessive values in either dimension impose prohibitive computational burdens: (1) \textit{ Memory}:$\mathrm{Mem} \propto b \cdot r$ (adapter parameter storage); (2) \textit{ Compute}: $\mathrm{Flops} \propto r \cdot d^2$ (for hidden dimension $d$). This necessitates joint optimization of $(b,r)$ to guide the accuracy-efficiency trade-off space effectively. Pure bit-width reduction sacrifices model capacity, while unrestrained rank scaling inflates computation costs disproportionately.

The effectiveness of GSQ-Tuning hinges on how quantization bit-width interacts with the dimensions of low-rank adapters. To inform real-world deployments, we systematically analyze this interplay by constructing a Pareto frontier that illustrates the balance between model memory consumption during fine-tuning and accuracy across various bits-rank settings. We hope our findings not only highlight optimal configurations, but also offer practical guidelines for practitioners to tailor solutions to specific hardware constraints.

\paragraph{Pareto Frontier Analysis.} Based on our GSQ-Tuning, we construct a Pareto frontier by plotting model memory during fine-tuning against validation accuracy across different \textit{bits-rank} configuration. As shown in Fig.~\ref{fig:pareto}, the frontier reveals three distinct optimization regimes (1) \textit{High-Bit Low-Rank Regime (8-bit, r=64):} Reaches 65.60 Acc with suboptimal efficiency. 0.50 Acc gain from $r=16$ to $64$ indicates high-bit quantization inherently limits error magnitude, requiring less rank compensation. (2) \textit{Mid-Bit Balanced Regime (6-bit, r=128):} Delivers 65.58 Acc with moderate resources. 0.71 Acc gain from $r=16$ to $128$ shows diminishing returns beyond this point (only 0.32 Acc gain from $r=128$ to $512$). (3) \textit{Low-Bit High-Rank Regime (5-bit, r=512):}  Achieves 64.88 Acc with minimal memory footprint. 0.91 Acc gain from $r=16$ to $512$ demonstrates that aggressive rank scaling can compensate for severe quantization errors. We also provide the Pareto frontier and detailed results for other models in appendix. Extra extensive results yield similar guidance.

\begin{figure}[t]
  \centering
   \includegraphics[width=\linewidth]{figures/pareto_curveV5.pdf}
   % \vspace{-5mm}
   \caption{Pareto curve of accuracy-memory trade-offs. Compared to FP16, our GSQ-Tuning can reduce 1.85$\times$ memory usage while having the comparable accuracy. Detailed results are in Tab.\ref{tab:llama2-7b}}
    \label{fig:pareto}
    % \vspace{-5mm}
\end{figure}