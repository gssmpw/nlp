\section{Related Work}
\label{related_works}
\vspace{-2mm}

\paragraph{Parameter-Efficient Fine-Tuning (PEFT).} PEFT reduces memory and computational costs by introducing a small set of trainable parameters while keeping the pretrained model frozen. Approaches including soft prompt tuning \citep{wang2023multitask}, partial fine-tuning \citep{fu2023effectiveness}, and low-rank adaptation \citep{hu2021lora}. Among these, LoRA stands out as a seminal work, injecting trainable low-rank matrices into linear layers to enable efficient fine-tuning without modifying the base model weights. QLoRA extends this with 4-bit NF4 quantization and Double Quantization, supporting 65B model fine-tuning on a single 48GB GPU with minimal performance degradation. Despite recent studies further extending LoRA from the perspective of quantization-aware fine-tuning \citep{xu2023qa, liloftq} to improve efficiency, stability, and performance \citep{hu2023llmadapters,liu2024dora, zhao2024galore,hayou2024lora+,meng2024pissa}, these methods still maintain compute-intensive forward and backward propagations at high bit-widths during fine-tuning. This results in a substantial computational burden when targeting edge AI accelerators.


\paragraph{Quantization.} Much excellent works ~\cite{yuan2023rptq,shang2023pbllm,yuan2023asvd,yue2024wkvquant, frantar2022gptq, xiao2022smoothquant,hu2024illm} use the quantization techniques to accelerate the inference of LLMs. For instance, GPTQ~\cite{frantar2022gptq} quantizes weights to 3-4 bit with slight accuracy drop based on approximate second-order information. AWQ~\cite{lin2024awq}, SmoothQuant~\cite{xiao2022smoothquant}, and OmniQuant~\cite{shaoomniquant} explore the scheme of smoothing by detecting the importance of different activation channels. Recent works (e.g., Quarot~\cite{ashkboos2024quarot}, SpinQuant~\cite{liu2024spinquant}, OSTQuant~\cite{hu2025ostquant}) further suppress outliers by utilizing computation-invariant rotation transformation. However, above methods mainly focus on the post-training optimization while overlooking the overhead during training. Interestingly, several studies~\cite{banner2018scalable,drumond2018training,adelman2018faster,wu2018training,langroudi2019deep,langroudi2019cheetah,yang2020training,zhu2020towards,xi2023training} have made much efforts to improve the efficiency and optimization of the training process, notably through \textbf{Fully Quantized Training (FQT)}. FQT involves quantizing all tensors—weights, activations, and gradients—needed for computation-intensive operations (like matrix multiplication) during both forward and backward propagation~\cite{wang2018training,yang2020training,zhu2020towards}. Particularly during the backward propagation, quantizing gradients remains challenging due to the wide dynamic range. Recent works have focused on integer-based quantization for smaller-scale models; for instance, SwitchBack~\citep{switchback} quantizes partial matrix multiplications with INT8, but it is limited to vision models with up to 1B parameters. Jetfire~\citep{jetfire} proposes a 2D block-wise quantization approach that maintains accuracy and achieves significant memory savings (1.4-1.5x) when training in INT8. However, these methods have yet to scale to LLMs. Additionally, some efforts have attempted to alleviate the training burden using low-bit floating-point representations; for example, LM-FP8~\citep{peng2023fp8} trains LLMs from scratch using FP8, achieving performance comparable to BF16. Nonetheless, above methods have not yet been explored to for low-bit integer fine-tuning tasks for LLMs. Notably, FQT is different from Quantization-Aware Training (QAT), we also discuss the difference in Sec.~\ref{qat_fqt}.
