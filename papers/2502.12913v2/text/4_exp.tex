\section{Experiments}
\label{sec:exp}

\input{text/tables/main_0shot}

\noindent\textbf{Foundation Models and Evaluation Metrics.} We apply our method to the entire LLaMA family, including LLaMA-2 (7B/13B/70B)\citep{touvron2023llama2}, and LLaMA-3 (3B-8B). We evaluate the fine-tuning models on up to 9 zero-shot tasks using the \texttt{lm-evaluation-harness} (version 0.4.7)\citep{eval-harness}, including BoolQ\citep{clark2019boolq}, HellaSwag~\citep{zellers2019hellaswag}, LAMBADA (OpenAI)\citep{radford2019language}, OpenBookQA\citep{OpenBookQA2018}, PIQA~\citep{bisk2020piqa}, SIQA~\citep{sap2019socialiqa}, WinoGrande~\citep{sakaguchi2019winogrande}, ARC-Easy, and ARC-Challenge~\citep{clark2018think}. The fine-tuning dataset follows Alpaca~\citep{alpaca}, with 52K instruction data from text-davinci-003. 

\noindent\textbf{Training Details.} We employ the whole fine-tuning process based on LLaMA-Factory~\citep{zheng2024llamafactory}. 
We implement \methodname{} in \texttt{PyTorch} using models from \texttt{Hugging Face}. We freeze the parameters of the linear modules and update a smaller (low-rank) set of parameters during the fine-tuning. The group size of our \methodname{} is $32$. We fine-tune models using the 8-bits AdamW optimizer~\citep{dettmers8} in bfloat16 precision. We choose the constant learning rate schedule and set the learning rate to be $1\times10^{-5}$ for all models. In all cases, we tune the hyper-parameters on the base BF16 tasks, and re-use the same values for low-precision training. We always perform single-epoch experiments using a linear learning rate warm-up of $100$ steps. The batch size and sequence length are fixed at $16$ and $2048$. The number of fine-tuning steps is 3.24K for Alpaca. 

\noindent\textbf{Hardware Synthesis.} We implemented the hardware in Verilog RTL and synthesized it using Synopsys Design Compiler with a 7nm technology library to estimate the process engine's area, latency, and power consumption~\citep{clark2016asap7}. The hardware operates at 1 GHz and has a capability of 50 TOPS. The memory subsystem is not considered in our settings and analyses.

\subsection{Overall Results}\label{subsec_overall_res}
\paragraph{GSQ-Tuning Results on LLaMA Family.} Here, we compare the fine-tuning performance across LLaMA family (3B$~$70B) against QLoRA. As shown in Tab.~\ref{tab:compare_gsq}, GSQ-Tuning achieves comparable or better zero-shot accuracy across different LLaMA model scales (7B-70B) under a fully low-bit quantization fine-tuing setting. With 8-bit quantization precision (W8A8G8), GSQ-Tuning matches or exceeds QLoRA's performance on 83\% of tasks, despite using 50\% fewer bits for activations and gradients. Even at aggressive 5-bit quantization (W5A5G5), the GSQ-Tuning maintains 98.6\% of QLoRA's average accuracy while reducing 1.85$\times$ memory footprint. These results confirm GSQ-Tuning's effectiveness for resource-constrained edge deployment. Besides, we present comprehensive results of our GSQ-Tuning across various LlaMA models in Sec.~\ref{sec:detailed_results}. This includes LlaMA2-7B (Tab.\ref{tab:llama2-7b}), LlaMA2-13B (Tab.\ref{tab:llama2-13b}), LlaMA2-70B (Tab.\ref{tab:llama2-70b}), LlaMA3-3B (Tab.\ref{tab:llama3-3b}), and LlaMA3-8B (Tab.\ref{tab:llama3-8b}). The findings consistently highlight the effectiveness and efficiency of GSQ-Tuning.

\input{text/tables/main_fp8_rank32}

\input{text/tables/tbl_mm}
\input{text/tables/general_dataset}

\paragraph{Comparison with FP8.} Here, we compare the designed GSE data format with FP8 in fully quantized fine-tuning framework. As shown in Tab.~\ref{tab:copare_fp8_r32}, the results demonstrate that the designed GSE implemented in our GSQ-Tuning method achieves superior fine-tuning performance compared to FP8 while significantly reducing computation efficiency. Even under 5-bit settings, GSQ-Tuning maintains fine-tuning performance on par with FP8, further validating its effectiveness. Additionally, to mitigate the impact of rank variations, we report the fine-tuning results using 64-rank setting in Tab.~\ref{tab:copare_fp8_r64} of the appendix. Extensive experiments consistently support the advantages of our approach.
% \input{text/tables/main_mmlue}

\input{text/tables/exp_setting}
\paragraph{Hardware Efficiency Analysis.} Table~\ref{tab:hardware_consumption} compares the chip area and power consumption of different formats through hardware synthesis. GSE-INT format demonstrates significant advantages over FP: (1) Area Efficiency: GSE-INT6 process engine requires only 0.47mm$^2$, 10.7$\times$ smaller than FP8(E4M3). This stems from simplified integer arithmetic logic and group-wise exponent sharing that eliminates complex alignment. (2) Power Superiority: At comparable bit-widths, GSE-INT6 consumes 0.76W (the 23.52\% of FP8's 3.23W).
% (4.25$\times$ reduction).

\vspace{-3mm}
\subsection{Generalization Results}\label{subsec_generalization}
% \noindent\textbf{Generalization of GSQ-Tuning for VLM (LLaVA)}
\paragraph{Generalization of GSQ-Tuning for Vision-Language Model (LLaVA).}
Model used is LLaVA-v1.5-7B~\citep{liu2024improved} with Vicuna-7B-v1.5~\citep{zheng2023judging} as language model and CLIP ViT-L-336px~\citep{radford2021learning} as vision tower, connected by a 2-layer MLP. Instruction dataset and other settings for finetuning follow the LLaVA official repository, LLaVA-Instruction~\citep{liu2024visual} and the improved one~\citep{liu2024improved}.
% \paragraph{Analysis.} 
Tab.~\ref{tab:exp_mm} shows performance drop of the vanilla quantization of 4-bits/64-rank QLoRA, especially, referring to the TextVQA evaluation. Finetuning with GSE shows comparable performance compared to that with BF16. BF16 is of E8M7 while GSE is of E5M7, demonstrating the redundancy of the dynamic range \textit{w.r.t.} exponents is at least 3-bits much. Moreover, the memory cost of GSE is about a half of BF16.

\paragraph{Generalization of GSQ-Tuning on Other Fine-tune Dataset.} Here, we also select Commonsense170K (CS170K)~\citep{hu2023llmadapters} to evaluate the generalization ability of GSQ-Tuning across different fine-tuing dataset. CS170K is a dataset constructed from the training sets of BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, and OBQA with pre-defined templates, comprising 170K commonsense reasoning samples. As shown in Tab.~\ref{tab:cs170k}, on larger fine-tuning datasets, our GSQ-Tuning also demonstrates comparable or even superior accuracy compared to QLoRA, while being more computationally efficient. 


\vspace{-3mm}
%\paragraph{Group Size Analysis.} 
\subsection{Ablation Study.}\label{ablation}
\paragraph{Group Size Analysis.} As shown in Tab.~\ref{tab:ablation_group}, our GSQ-Tuning with 32 groups achieves optimal accuracy-efficiency balance in 6-bit configurations (W6A6G6). The 32-group setting yields significantly higher average accuracy (65.39) compared to 64-group (64.72) and 128-group (64.27) variants, while maintaining comparable memory efficiency (70.96 vs 69.22/69.53). This sweet spot emerges from the tension between quantization bit width and hardware deployment - smaller groups better capture value distributions but increase computation overhead, while larger groups sacrifice adaptation granularity. We therefore adopt group=32 as the default configuration.
\input{text/tables/ablation_group}




\input{text/tables/ablation_rank}
\paragraph{Low Rank Size Analysis.} As shown in Tab.~\ref{tab:ablation_rank}, the results demonstrates the accuracy-efficiency trade-off across different LoRA ranks in 6-bit configurations (W6A6G6). While larger ranks consistently improve performance (64.87 $\rightarrow$ 66.31 average accuracy for rank 16 $\rightarrow$ 512), the marginal gains diminish significantly beyond rank 64 (+0.38 from 64â†’128 vs +0.73 from 16 $\rightarrow$ 64). This aligns with the spectral properties of weight matrices, where most task-relevant information is captured by the dominant singular vectors. We observe that rank=64 provides an optimal balance, achieving 98.6\% of the maximum accuracy while using 50\% less memory than rank=512 configurations.



