\section{Introduction}
Recent advances in Large Language Models (LLMs) have delivered impressive results in a variety of natural language tasks~\citep{touvron2023llama,touvron2023llama2,liu2023gpt}.
LLMs are typically trained in several stages, including large-scale pretraining followed by one or more fine-tuning phases~\cite{dubey2024llama,liu2024deepseek}. 
LLM fine-tuning approaches like supervised fine-tuning (SFT)~\citep{zhang2023instruction}, usually employ curated, high-quality corpora for refining the model with a standard language modeling~\cite{chiang2023vicuna}.

Despite their effectiveness, most LLM fine-tuning approaches require powerful cloud servers or GPUs equipped with large memory capacities. 
This poses two significant challenges in real-world settings: (1) uploading sensitive data to remote servers poses a fundamental privacy risk, and (2) in many practical scenarios, models must be deployed on resource-constrained edge devices—such as mobile processors or embedded AI accelerators—where memory and power budgets are tightly limited.  
\textit{Such constraints become critical in LLM's personalized applications, where data cannot be shared with the cloud and model updates must remain local to ensure privacy. Meeting these challenges thus necessitates on-device adaptation methods capable of preserving data privacy and functioning within the limited memory and compute budgets of edge hardware.}

Parameter-Efficient Fine-Tuning (PEFT)~\cite{han2024parameter} techniques such as LoRA~\cite{hu2021lora} and QLoRA~\cite{dettmers2023qlora} alleviate part of this burden by reducing trainable parameters to around 1\% of the original model. Unfortunately, they remain reliant on floating-point operations for both forward and backward passes, which clashes with edge-device constraints in three ways. 
First, during fine-tuning, weights, activation and gradients must be stored and updated in high-precision floating-point. It introduces additional overhead or even makes the LLM fine-tuning impractical on edge devices. Second, floating-point representations incur high memory overhead (e.g., FP16 doubles the memory cost compared to INT8; for a 7B-parameter model, this can surpass 20GB memory during fine-tuning process, presenting substantial challenges for mobile processors). Last but not least, commercial edge AI accelerators (e.g., Qualcomm Hexagon~\cite{QUALCOMM2024}) typically get peak throughput only on integers, leaving up to 84\% of compute units idle under FP16 training.

Therefore, eliminating floating-point arithmetic for fine-tuning would have a substantial impact on software, hardware, and application design for efficient on-device LLM adaptation~\citep{ARM2020,kim2021bert}. 
While previous studies on integer quantization~\citep{jacob2018quantization,kim2021bert,xiao2022smoothquant, yuan2023rptq} verify the feasibility of inference, they do not extend to gradient quantization, which is required for effective fine-tuning of LLMs at the edge.

In this paper, we propose a new framework for resource-efficient on-device LLM fine-tuning, termed \textbf{GSQ-Tuning}. Central to our method is the \emph{Group-Shared Exponents Integer} format, a novel quantization strategy that replaces floating-point with a specialized integer-based representation. 
We integrate this with parameter-efficient LoRA-like modules to enable fully on-device fine-tuning without incurring large memory and computation costs. 
We further examine this design through a Pareto frontier analysis, which demonstrates how various bits-rank settings impact the trade-off between fine-tuning memory costs and accuracy. Extensive experiments across models of varying scales, different fine-tuning datasets, and diverse tasks have demonstrated the effectiveness and generalizability.
We highlight our main contributions as follows:
\begin{itemize}
    \item  \textbf{Group-Shared Exponents Integer Quantization:} We introduce a quantization strategy that shares exponents among groups, thereby reducing the storage and computation overhead while still representing model parameters in integer format. Combined with LoRA-like adapters, our method supports fine-tuning under tight memory constraints. 
    \item \textbf{Integer Forward and Backward Computations:} By extending integer quantization pipelines beyond inference to include gradients, both forward and backward passes remain hardware-friendly and efficiently utilize integer-focused edge accelerators. 
    \item \textbf{Pareto Frontier for Quantization Bits and Low-rank:} We demonstrates how various bits-rank settings impact the trade-off between fine-tuning memory costs and accuracy through a Pareto frontier analysis. We empirically show that our approach achieves accuracy on par with FP16-based fine-tuning while dramatically lowering both 1.85$\times$ memory footprint. Furthermore, compared with FP8, at comparable performance levels, our method (GSE-INT5) reduces the power consumption of MAC unit by $\sim$ 5 $\times$ and decreases chip area by $\sim$ 11 $\times$ comparing to the origin.
\end{itemize}