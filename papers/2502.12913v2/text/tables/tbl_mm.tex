\begin{table*}[t]
    \tablestyle{1pt}{1.3}
    \centering
    \caption{Cross-modal task evaluation on LLaVA-v1.5-7B: the {default} setting is QLoRA on 4-bits/64-rank without finetuning. Shared exponents shows robustness to LLM's finetuning, referring to the comparison with BF16.}
    % \vspace{-2mm}
    \begin{tabular}{lcccccccccc}
    \noalign{\vspace{0.3em}}
    \toprule
    \noalign{\vspace{0.1em}}
    \multirow{2}{*}{\textbf{Settings}}
    &
    \multirow{2}{*}{LLMs branch} 
    &
    \multirow{2}{*}{low-rank branch}
    &
    \multicolumn{3}{c}{\textbf{POPE-random}}
    &
    \multicolumn{3}{c}{\textbf{POPE-adversarial}}
    &
    \multirow{2}{*}{\textbf{TextVQA}}
    &
    \multirow{2}{*}{\textbf{MMBench}}
    \\
    
     \cline{4-9}
    &
    &
    &
    accuracy
    &
    precision
    &
    F1-score
    & 
    accuracy
    &
    precision
    &
    F1-score
    & 
    \\
    \shline
    LLaVA-v1.5-7B 
    &
    4-16-16
    &
    w/o
    &
    {84.19}
    &
    {84.08}
    &
    {84.80}
    &
    {74.40}
    &
    {69.96}
    &
    {76.96}
    &
    {~~6.51}
    &
    {55.45}
    \\
    w/ QLoRA
    &4-16-16
    &
    16-16-16
    &
    87.47
    &
    92.51
    &
    86.68
    &
    83.87
    &
    85.52
    &
    83.48
    &
    47.68
    &
    67.08
    \\
    % \noalign{\vspace{0.1em}}
    \hdashline[0.8pt/1pt]
    % \noalign{\vspace{0.1em}}
    \multirow{2}{*}{w/ GSQ-Tuning}
    &
    4-8-8
    &
    4-8-8
    &
    {87.84}
    &
    {96.21}
    &
    {87.08}
    &
    \default{\textbf{84.03}}
    &
    \default{\textbf{87.40}}
    &
    \default{\textbf{83.28}}
    &
    {45.19}
    &
    {69.75}
    \\
    
    &
    4-6-6
    &
    4-6-6
    &
    \default{\textbf{88.08}}
    &
    \default{\textbf{96.08}}
    &
    \default{\textbf{87.39}}
    &
    83.43
    &
    85.80
    &
    82.87
    &
    \default{\textbf{49.13}}
    &
    \default{\textbf{70.19}}
    \\
    \hline
    \end{tabular}
    \label{tab:exp_mm}
    \vspace{-4mm}
\end{table*}