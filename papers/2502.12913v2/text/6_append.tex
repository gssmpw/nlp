\appendix

\section{Appendix}
\label{sec:appendix}
\subsection{Differences with Quantization-aware training (QAT): }
\label{qat_fqt}
Quantization-aware training (QAT)~\cite{choi2018pact,Zhang_2018_ECCV,zhou2017incremental,jacob2018quantization,dong2019hawq,dong2019hawqv2,shen2019q,zafrir2019q8bert,shen2020QBERT,tang2022mkq,zhang2020ternarybert,bai2020binarybert,foret2020sharpness,wang2022squat} is an \emph{inference acceleration} technique which trains networks with quantizers inserted in the forward propagation graph, so the trained network can perform efficiently during inference. 
QAT can compress activation/weights to extremely low precision (e.g. 1-2 bits). 
It is tempting to think that directly applying a quantizer for QAT to FQT can lead to similar low activation/weights bit-width. However, even only quantizing the forward propagation for FQT is much more challenging than QAT because:  \raisebox{-0.5pt}{\ding[1.1]{182\relax}} QAT requires a converged full-precision model as initialization~\cite{esser2019learned} and/or as a teacher model for knowledge distillation~\cite{bai2020binarybert}; \raisebox{-0.5pt}{\ding[1.1]{184\relax}} QAT may approximate the discrete quantizer with continuous functions during training~\cite{gong2019differentiable}, which cannot be implemented with integer arithmetic. Due to these challenges, it is still an open problem to do FQT with low-bit activations/weights. 

\input{text/tables/main_0shot_llama2-7b}
\input{text/tables/main_0shot_llama2-13b}
\input{text/tables/main_0shot_llama2-70b}
\input{text/tables/main_0shot_llama3-3b}
\input{text/tables/main_0shot_llama3-8b}
\subsection{Detailed results on different rank setting:}
\label{sec:detailed_results}
Here, we also report the results of our GSQ-Tuning on different LlaMA model, including LlaMA2-7B (Tab.\ref{tab:llama2-7b}), LlaMA2-13B (Tab.\ref{tab:llama2-13b}), LlaMA2-70B(Tab.\ref{tab:llama2-70b}), LlaMA3-3B(Tab.\ref{tab:llama3-3b}), and LlaMA3-8B(Tab.\ref{tab:llama3-8b}). The results consistently demonstrated the effectiveness and efficiency of GSQ-Tuning.

% \input{text/tables/main_0shot_rank512}

\subsection{Comparison with FP8 with 64 rank}
Here, we compare the designed GSE data format with FP8 in fully quantized fine-tuning framework with 32 rank setting. As shown in Tab.~\ref{tab:copare_fp8_r64}, the results still demonstrate that the designed GSE implemented in our GSQ-Tuning method achieves superior fine-tuning performance compared to FP8 while significantly reducing computation efficiency. Even under 5-bit settings, GSQ-Tuning maintains fine-tuning performance on par with FP8, validating its effectiveness.
\input{text/tables/main_fp8_rank64}