\section{Conclusion} 
\label{sec:conclusion}
\vspace{-2mm}
In this paper, we propose GSQ-Tuning, a resource-efficient framework that addresses the critical challenges of floating-point dependency, privacy risks, and hardware incompatibility in on-device LLM fine-tuning. By integrating Group-Shared Exponents Integer (GSE) quantization with parameter-efficient adaptation, our method achieves three key advancements:(1) Full Integer Pipeline: Eliminates floating-point operations across both forward and backward passes, reducing 1.85$\times$ memory footprint compared to FP16 while maintaining comparable accuracy. (2) Hardware-Optimized Design: The GSE format reduces metadata overhead via group-wise exponent sharing, enabling 5-8bit integer representations. Combined with LoRA-like adapters, this achieves 5 $\times$ lower power consumption and 11$\times$ smaller chip area compared to FP8 at equivalent accuracy levels. (3) Practical Deployment Guidance: A Pareto frontier analysis guides optimal bit-rank configurations for diverse edge constraints. These innovations establish GSQ-Tuning as a foundational step toward democratizing LLM adaptation for resource-constrained environments. This breakthrough makes private, on-device LLM adaptation practical for sensitive applications. Future work will explore sub-4bit quantization to further push the boundaries of edge AI.

