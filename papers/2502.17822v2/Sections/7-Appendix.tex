\clearpage

\section{Appendix}

\subsection{Related Task Comparison}
\label{sec: related task comparison}

We could categorize related tasks into four categories based on input/output modality: Textual-Modal Summarization of Videos (TMSV), Visual-Modal Summarization of Videos (VMSV), \textbf{Bi}modal \textbf{S}emantic \textbf{S}ummarization of \textbf{V}ideos (BiSSV), and Bimodal Summarization of Multimodal Data (BSMD). The former two tasks are primarily concerned with unimodal summarization, while BiSSV and BSMD focus on multimodal summarization. Figure \ref{fig: task comparison} illustrates a detailed task comparison.

\begin{figure*}[b]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/Task_Comparison.pdf}
    \vspace{-8pt}
    \caption{Comparison between BiSSV and related tasks. DVC: Dense Video Captioning. BSMD: Bimodal Summarization of Multimodal Data. BiSSV: Bimodal Semantic Summarization of Videos.}
    \label{fig: task comparison}
\end{figure*}

\textbf{Can unimodal summarization datasets be modified for BiSSV?} One closely related task in unimodal summarization is Dense Video Captioning (DVC)~\cite{krishna2017dense}, which provides text description and grounded segments for the events within the video. DVC could be divided into two sub-tasks: event localization and event captioning~\cite{wang2021end,zhang2022unifying}. The primary divergence between BiSSV and DVC lies in their objectives. The goal of BiSSV is to generate a TM-Summary as a global overview of the entire video instead of chronological captions, and the VM-Summary is a collection of highlighted moments instead of localized events. This difference in focus implies that simply concatenating dense captions from DVC may not produce a suitable TM-Summary, as in the case of the first video-to-video\&text dataset VideoXum~\cite{lin2023videoxum}, which has TM-Summaries that are, on average, 49.9 words in length.

\textbf{Can BSMD datasets be modified for BiSSV?} Recently, the task of multimodal summarization with multimodal output~\cite{zhu2018msmo}, a typical task of BSMD, is proposed to generate a bimodal summary for multimodal inputs. Some BSMD approaches use video and corresponding textual metadata (document~\cite{li2020vmsmo,fu2020multi, tang2023tldw} or transcript~\cite{fu2020multi,he2023align,qiu2023multisum}) as input, yielding a bimodal summary comprising text descriptions and keyframes~\cite{li2020vmsmo,fu2020multi,tang2023tldw,qiu2023multisum} or key segments~\cite{he2023align}. We summarize BSMD datasets with video input in Table \ref{tab: BSMD} for comparison. 

In summary, we discover that BSMD can be more accurately characterized as a combination of unimodal summarization tasks, with a primary focus on information extraction, e.g. direct selection of the most informative sentences from the source text~\cite{he2023align} or detailed description of transcripts~\cite{qiu2023multisum}. Given the time-consuming nature of acquiring auxiliary information in real-world scenarios, BSMD and BiSSV serve distinct application purposes. BSMD is particularly well-suited for video-contained documents, such as news articles, while BiSSV is better suited for various web applications like video browsing, retrieval and recommendation. Furthermore, it's worth mentioning that the majority of visual outputs of BSMD datasets consist of keyframes~\cite{li2020vmsmo,fu2020multi,tang2023tldw,qiu2023multisum}, which lacks smoothness for online browsing compared to short videos.

\begin{table*}[b]
\centering

% \small
\begin{tabular}{lccccc}

\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{\textbf{\textit{Input}}}    & \multicolumn{2}{c}{\textbf{\textit{Output}}} & \multirow{2}{*}{\textbf{Source}} \\
\cmidrule(r){2-3} \cmidrule(r){4-5}
                  & \textbf{Vision} & \textbf{Language}            & \textbf{Vision}       & \textbf{Language}    &                         \\
\midrule
VMSMO~\cite{li2020vmsmo}             & video  & document            & keyframe     & sentence    & news                    \\
MM-AVS~\cite{fu2020multi}            & video  & document/transcript & keyframe    & sentence    & news                    \\
XMSMO~\cite{tang2023tldw}             & video  & document            & keyframe     & sentence    & news                    \\
BLiSS~\cite{he2023align}             & video  & transcript          & segment     & paragraph   & livestream              \\
MultiSum~\cite{qiu2023multisum}          & video  & transcript          & keyframe    & paragraph   & YouTube                 \\
BIDS (ours)             & video  & $\times$            & segment    & sentence    & YouTube  \\
\bottomrule
\end{tabular}
\label{tab: BSMD}
\caption{Comparison between BSMD datasets and BIDS.}
\end{table*}



\subsection{Pseudo-Code for VM-Summary Extraction}
\label{sec: pseudo code}

We provide a pseudo-code for VM-Summary extraction in Algorighm \ref{algorithm: VM extraction}.

\begin{algorithm*}
\caption{VM-Summary Extraction}
\LinesNotNumbered 
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{2s-clips $C_{2s}$, score sequence for 2s-clips $S_{2s}=\{s_i\}$, VM-Summary duration L}
\Output{selected VM-Summary segments $Segment_{VM}$}

$Segments_{all}$ = Merge($C_{2s}$, condition=($s_i==s_i+1$)) \\
$S_{seg}$ = Merge\_Score($S_{2s}$, condition=($s_i==s_i+1$)) \\
For score in Rank($S_{seg}$):\\
\Indp{ 
    $Segment_{score}$ = Get\_Segment($Segments_{all}$, $s_i==score$)\\
    if Duration($Segment_{score}$) + Duration$(Segment_{VM})$ $< L$: \\
    \Indp {
        $Segment_{score}\rightarrow Segment_{VM}$\\
    }
    \Indm
    else:\\
    \Indp{
        For seg in $Segment_{score}$:\\
        \Indp{
            scaled\_length = Duration($seg$) / Duration($Segment_{score}$)\\
            $s_{left}, s_{right}$ = Get\_Score(Left\_Segment(seg), Right\_Segment(seg))\\
            if $s_{left} > score$ and $s_{right} > score$:\\
                \Indp{
                    $Segment_{left}$ = [Get\_Left\_Boundary(seg), Get\_Left\_Boundary(seg)+$scaled\_length/2$] \\
                    $Segment_{right}$ = [Get\_Right\_Boundary(seg)-$scaled\_length/2$, Get\_Right\_Boundary(seg)] \\
                    $Segment_{left}, Segment_{right}\rightarrow Segment_{VM}$ \\
                }
            \Indm
            else if $s_{left} > score$: \\
                \Indp{
                    $Segment_{left}$ = [Get\_Left\_Boundary(seg), Get\_Left\_Boundary(seg)+$scaled\_length$] \\
                    $Segment_{left}\rightarrow Segment_{VM}$\\
                }
            \Indm
            else if $s_{right} > score$: \\
                \Indp{
                    $Segment_{right}$ = [Get\_Right\_Boundary(seg)-$scaled\_length$, Get\_Right\_Boundary(seg)] \\
                    $Segment_{right}\rightarrow Segment_{VM}$\\
                }
            \Indm
            else:\\
                \Indp{
                    $Segment_{mid}$ = [Get\_Mid(seg)-$scaled\_length/2$, Get\_Mid(seg)+$scaled\_length/2$] \\
                    $Segment_{mid}\rightarrow Segment_{VM}$\\
                }
            \Indm
        }
        \Indm
        break\\
    }
    \Indm
}
\Indm
return $Segment_{VM}$
\label{algorithm: VM extraction}
\end{algorithm*}

\subsection{Textual-Modal Summarization Warm-up}
\label{sec: warm up}

\begin{table*}[b]
% \small
\begin{tabular}{lcccccc}

\toprule
     & \textbf{CIDEr}          & $\boldsymbol{\tau}$         & $\mathbf{NDCG_{VM}@15\%}$ & $\mathbf{NDCG_{VM}@all}$ & $\mathbf{NDCG_{TM}@15\%}$ & $\mathbf{NDCG_{TM}@all}$ \\
\midrule
N=0  & \underline{38.23}    & 16.14          & 65.99           & 84.74          & 57.09           & \textbf{81.47} \\
N=10 & 36.65          & 16.96          & 66.33           & 84.89          & \textbf{57.39}  & \underline{81.40}    \\
N=20 & 36.91          & \textbf{18.18} & \textbf{67.01}  & \textbf{85.09} & 56.63           & 81.29          \\
N=30 & \textbf{40.87} & 17.58          & \underline{66.58}     & \underline{84.94}    & 56.55           & 81.30          \\
N=40 & 37.52          & \underline{17.61}    & 66.35           & 84.93          & \underline{57.24}     & 81.35          \\
N=50 & 37.74          & 15.70          & 65.20           & 84.45          & 56.88           & 81.35  \\       
\bottomrule
\end{tabular}
\caption{ Comparison of different epochs N for textual-modal summarization warm-up.
}
\label{tab: warm up}
\end{table*}

Results of different epochs for textual-modal summarization warm-up are presented in Table \ref{tab: warm up}, all chosen by CIDEr~\cite{vedantam2015cider}. The model's capability in visual-modal summarization demonstrates an initial improvement followed by a decline as the number of warm-up epochs increases. It is important to note that though the precise number of epochs required for textual-modal summarization warm-up may vary across different machines, a closer alignment between visual-modal and textual-modal summarization consistently yields superior results. Inadequate training or overfitting for each sub-task can lead to a decline in overall performance.

The results presented in Table \ref{tab: warm up} also reveal an issue with the ranking-based optimization objective~\cite{Pobrotyn2021NeuralNDCG}, as discussed in Section \ref{sec: multi-stage}. This issue arises when the model takes shortcuts by assigning extremely low scores to insignificant features. For instance, when considering models trained with N=10 and 20 or 30 warm-up epochs, we observe a consistent increase in $NDCG_{VM}$. However, there is a decline in textual-modal summarization performance, indicating that partially absent features may not be sufficient to generate a global TM-Summary. We address exploring the integration of saliency learning and visual modeling as a promising direction for future research.


\subsection{Human Evaluation Details}
\label{sec: human eval}

We conduct human evaluation under two settings. To evaluate how bimodal summaries could contribute to Satisfaction (Satis) and Informativeness (Inform) in comparison with unimodal summaries, we randomly select 30 sets of videos and ask participants to score VM-Summary only, TM-Summary only, and bimodal summaries on a scale ranging from 1 to 5. For both metrics, a rating of 1 indicates very dissatisfactory while 5 indicates very satisfactory. For informativeness, a rating of 1 indicates the video content was not summarized adequately while 5 indicates that the video content was perfectly summarized. We present different forms of summaries to participants after shuffling. Figure \ref{fig: human eval} (a) shows an example set of summaries.

Besides automatic evaluation metrics, we also conduct a comparative evaluation between UBiSS and concatenated unimodal summarization baselines. We randomly sample 20 sets of videos. Different summaries generated by UBiSS trained with NeuralNDCG/MSE, PGL-Swin, and Swin-PGL are presented to participants in random order. The participants are asked to rate the accuracy of VM- and TM-Summary, based on how they could capture the highlights (VM-Summary) or present a global overview (TM-Summary), along with the consistency of bimodal summaries. Figure \ref{fig: human eval} (b) offers an illustrative example.

The participants are college students with an educational background in computer science. The average age of the participants is 22, and the gender ratio is 7:4 (males: females). According to DataReportal (due April 2023)\footnote{https://datareportal.com/essential-youtube-stats}, most YouTube users are between 25 and 34, and the gender ratio is approximately 1.195. Therefore, participants' distribution is similar to real-world users' distribution.

\begin{figure*}[b]
    \centering
    \includegraphics[width=0.95\linewidth]{Images/Human_Evaluation.pdf}
    \vspace{-8pt}
    \caption{Example cases to be evaluated by participants.}
    \label{fig: human eval}
\end{figure*}

