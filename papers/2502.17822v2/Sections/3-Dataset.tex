% \section{BIDS Dataset}
% \label{sec: dataset}

% % 数据集概括
% As it is very costly to build a bimodal summarization dataset from scratch, we, therefore, leverage the QVHighlights dataset~\cite{lei2021detecting} to construct a \textbf{B}imodal V\textbf{ID}eo \textbf{S}ummarization dataset (\textbf{BIDS}) to support the investigation of the BiSSV task. The constructed BIDS dataset finally contains 8130 videos with corresponding ground-truth Visual-Modal (VM) and Textual-Modal (TM) Summaries and saliency scores annotated for each 2-second clip, indicating its significance. Following the restrictions of traditional video summarization~\cite{gygli2014creating}, we ensure that the length of the VM-Summary does not exceed 15\% of the original video's duration. We describe the data processing and analysis in detail in the following subsections.

% \subsection{Data Processing}
% \label{sec: datapro}

% % 数据处理概括
% We aim to build a bimodal video summarization dataset with triplet data samples (video, TM-Summary, VM-summary), where the TM-Summary is a concise text description, and the VM-summary contains highlighted segments within the video. Firstly, we merge text-related segments from the original videos to guarantee that the TM-Summary accurately captures the main content of the video. 
% Secondly, we design a ranking-based extraction algorithm to preserve the most salient visual content as VM-Summary. 
% Lastly, we perform data cleaning to remove unsuitable videos that lack a clear focus for summarization. The overview of the BIDS building process is illustrated in Figure \ref{fig: data_construct}.

% % 步骤
% % 合并标注和视频
% \noindent \textbf{Data merging.} QVHighlights~\cite{lei2021detecting} is a video dataset that supports query-based moment retrieval and highlight detection, with annotations of natural language query, segments relevant to the query, and saliency scores for each 2s-clip within the segments. Taking the query as the TM-Summary, we merge the relevant segments chronologically as original videos in our dataset. In this way, we obtain a (video, TM-Summary) pair, for which we subsequently extract the VM-Summary. 

% % 提取 VM-Summary
% \noindent \textbf{VM-Summary extraction.} We utilize the annotated 2s-saliency scores for VM-Summary extraction. 
% Unlike the Knapsack algorithm utilized by previous video summarization datasets~\cite{song2015tvsum,gygli2014creating}, our extraction algorithm retains salient visual content within long segments and avoids favoring short segments. An illustration of this algorithm is presented in Figure ~\ref{fig: data_construct}. 
% We also provide a  pseudo-code in Appendix \ref{sec: pseudo code}.

% (a) \textit{Ranking}. We first merge adjacent 2s-clips with the same saliency scores into segments. Then, we rank all the candidate segments according to their saliency scores. The candidate segments are subsequently selected for VM-Summary in descending order. To comply with the length limit of VM-Summary (15\% video duration in our case), we may need to scale some candidate segments.

% (b) \textit{Scaling}. As the candidate segments vary in length, the purpose of scaling is to preserve informative parts within segments while guaranteeing conciseness. Specifically, candidate segments with the same score will be appended to the VM-Summary if it does not surpass the length constraint. Otherwise, these segments are proportionally scaled. 
% We assume that the parts closer to higher-scored segments usually contain more valuable information.
% Therefore, if the segment has higher-scored neighbors, adjacent parts closer to those neighbors are preserved (colored in \textcolor{red}{red} and \textcolor{myyellow}{yellow}, indicating two and one higher-scored neighbors, respectively); otherwise, its central part is preserved (colored in \textcolor{mygreen}{green}). The scaled segments are appended to the VM-Summary, and the segments with lower ranks are all rejected.

% % 数据清洗
% \noindent \textbf{Data cleaning.} Finally, we remove segments shorter than 2 seconds and videos with VM-Summary occupying less than 5\% of the video duration since they lack clear focal points for summarization. Finally, of 8,172 videos, only 42 (0.51\%) videos are removed.

% \subsection{Data Analysis}
% \label{sec: dataana}

% % 传统数据处理方式的缺陷
% Traditional video summarization datasets use the Knapsack algorithm to generate VM-Summary~\cite{gygli2014creating,song2015tvsum}. 
% However, Otami M et al.~\cite{otani2019rethinking} point out that their segmentation-selection pipeline favors short segments since selecting long segments costs more. 
% However, long and visually consistent segments can also contain informative moments. For example, when watching a video of \textit{someone playing basketball}, most of the visual content is similar, but we can still identify key moments, such as \textit{shooting}.
% Inspired by humans' ability to distinguish important moments in long videos, we choose to scale the candidate segments instead of rejecting them entirely. As a result, our VM-Summary shows a stronger correlation between the saliency scores and the selected segments.  

% % 相关系数比较
% We use Spearman's correlation coefficient $\rho$~\cite{zwillinger1999crc} to validate the effectiveness of our VM-Summary extraction algorithm. A higher coefficient between the saliency scores $S$ and the frame-level selection sequence $F$ (1 for the frame being selected into the VM-Summary and 0 for otherwise) indicates more salient content is preserved, which is the goal of summarization. 
% As presented in Table ~\ref{tab: dataset comparison}, BIDS has the highest Spearman's $\rho$ compared to traditional datasets. Moreover, Spearman's $\rho$ between $S$ and $F$ (generated by annotators) surpasses the $\rho$ between $S$ and GT-$F$ (obtained by applying Knapsack algorithm over the annotated saliency scores) in SumMe~\cite{gygli2014creating}, which further demonstrates that Knapsack algorithm can not effectively preserve salient parts within long segments. 

% % 统计数据
% After removing invalid and duplicate videos, BIDS contains 8130 videos, with 5854/650/1626 videos for training/validation/test set. 
% We ensure that the original videos between different sets do not overlap to avoid data leakage.  
% The data statistics of BIDS are presented in Table \ref{tab: dataset statistics}. As presented in Figure \ref{fig: distribution}, our algorithm is able to generate VM-summaries within a strict length constraint, with the majority occupying 14-15\% of the video's duration. Furthermore, the segments in a VM-Summary are evenly distributed throughout the corresponding video.

% \begin{table}[t]
%     \centering
%     \small  
%     \caption{Comparison with traditional video summarization datasets.
%     $\rho$: Average Spearman's correlation coefficient. 
%     Sig.: Significance (p < 0.05). 
%     $S$: Saliency score.
%     $F$: Frame-level sequence indicating each frame is selected (1) or not selected (0) into the VM-Summary. 
%     GT-$F$: the $F$ is calculated by averaging human annotated scores for each video in SumMe~\cite{gygli2014creating} and TVSum~\cite{song2015tvsum}.
%     dp: the $F$ is obtained by the Knapsack algorithm.
%     } 
    
%     \vspace{-8pt}
%     \begin{tabular}{ccccc}
%     \toprule
%     \textbf{Dataset}                & \textbf{Set of Variables}          & $\boldsymbol{\rho}$ & \textbf{Sig.}  & \textbf{\# of Videos}    \\
%     \midrule
%     \multirow{2}{*}{SumMe~\cite{gygli2014creating}} &($S$, GT-$F_{dp}$) & 0.34                               & \checkmark & \multirow{2}{*}{25} \\
%                            &($S$, $F$)        &  \underline{0.44}                        & \checkmark &                     \\
%                            \midrule
%     \multirow{2}{*}{TVSum~\cite{song2015tvsum}} &($S$, GT-$F_{dp}$)& 0.31                               & \checkmark & \multirow{2}{*}{50} \\
%                            &($S$, $F_{dp}$)    & 0.24                               & $\times$   &                     \\
%                            \midrule
%     BIDS(ours)           &($S$, GT-$F$)     & \textbf{0.52}                      & \checkmark & \textbf{8130}     \\ 
%     \bottomrule
%     \end{tabular}
%     \label{tab: dataset comparison}
% \end{table}


% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{Images/Distribution.pdf}
%     \vspace{-6pt}
%     \caption{(a) Distribution of duration ratio between VM-Summary and original video; (b) Distribution of temporal positions of the segments selected into the VM-Summary in the original video.
%     }
%     \label{fig: distribution}
% \end{figure}

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{Images/Framework.pdf}
%     \vspace{-8pt}
%     \caption{Model Architecture of UBiSS. 
%     }
%     \label{fig: framework}
% \end{figure*}

% \begin{table*}[t]
%     \centering
%      \small
%     \captionsetup{skip=10pt}
%     % \renewcommand{\arraystretch}{1.2}
%     \caption{Statistics of BIDS. 
%     {VM: Visual-Modal Summary. TM: Textual-Modal Summary.}
%     \vspace{-8pt}
%     }
%     \begin{tabular}{ccccccc}
%            \toprule
%            & \textbf{Avg. Video Len(s)} & \textbf{Total Video Len(h)} & \textbf{Avg. VM Len(s)} & \textbf{Avg. VM proportion(\%)} & \textbf{Avg. TM Len(word)} & \textbf{\# of Videos} \\
%            \midrule
%             Training   & 43.55           & 70.82             & 6.05         & 14.07               & 10.52            & 5854             \\
%             Validation & 40.05           & 7.23              & 5.57         & 14.07               & 10.41            & 650              \\
%             Test       & 44.83           & 20.25             & 6.19         & 14.12               & 10.42            & 1626             \\
%             All        & 43.53           & 98.3              & 6.04         & 14.08               & 10.49            & 8130      \\
%             \bottomrule
%     \end{tabular}
%     \label{tab: dataset statistics}
% \end{table*}
