\section{Related Work}
\label{sec: related work}

\subsection{3D Object Detection}

3D object detection in autonomous driving has seen significant advancements in recent years, with methods primarily categorized based on sensor modalities: \textbf{camera-based}, \textbf{point cloud-based}, and \textbf{multi-modality-based} approaches \cite{3dodsurvey2022}. Current research trends encompass various techniques within these categories, including monocular and multi-camera systems, Bird's Eye View (BEV) representations, and pseudo-LIDAR technologies for camera-based methods. Point cloud-based approaches have explored voxel-based, point-voxel hybrid, BEV, 4D radar, spatiotemporal, and data augmentation techniques. Multi-modal methods focus on deep fusion and cross-modal interaction strategies.

\noindent
\textbf{LIDAR-only Detectors.}
Point-cloud processing methods in LIDAR-based systems can be broadly classified into three categories. The first category focuses on learning objective-based approaches, such as anchor-based \cite{UTsemi2022} and anchor-free \cite{Afdet2020} methods. The second category emphasizes data representation-based approaches, including point-based, grid-based, point-voxel hybrid, range-based, and 4D radar methods. The third category includes auxiliary methods like data augmentation \cite{Qdataaug2020}, spatio-temporal sequence analysis, and pseudo-labeling techniques. While LIDAR-based object detection excels in speed and accuracy, the inherent sparsity of point clouds can lead to missed detections, particularly for small and distant objects. To address these limitations, multi-modal fusion approaches have emerged as a promising solution, leveraging the complementary strengths of camera and LIDAR modalities.

\noindent
\textbf{Multi Modal Fusion.}
% Multi-modal approaches primarily focus on fusion and cross-modality interaction. Common fusion paradigms include camera-radar, camera-LIDAR \cite{li2024fast,li2023poly}, camera-4D radar, LIDAR-4D radar, and LIDAR-radar combinations. For camera-LIDAR fusion, there are three primary strategies. The first strategy is early fusion, which combines raw sensor data before feature extraction \cite{lang2019pointpillars,PointNet2018}. The second strategy is deep fusion, which integrates features at multiple levels of the network \cite{li2024fast,li2023poly}. The third strategy is late fusion, where high-level features or detection results are merged \cite{CLOCs2020,Fast-CLOCs2022}. The integration of diverse sensor modalities, including image, point cloud, millimeter-wave radar, depth, and 4D radar data, has become a focal point in enhancing detection accuracy. We present a novel camera-radar feature fusion approach that synergistically leverages the complementary strengths of image and point cloud data. By incorporating advanced data augmentation techniques, our method achieves state-of-the-art accuracy and robustness in 3D object detection, particularly excelling in challenging scenarios and adverse weather conditions. The resulting high-fidelity detections serve as input for subsequent 3D MOT, significantly enhancing the stability and efficiency of the tracking pipeline.


Multi-modal approaches encompass various fusion paradigms, including camera-radar, camera-LIDAR \cite{li2024fast,li2023poly}, camera-4D radar, LIDAR-4D radar, and LIDAR-radar combinations. Camera-LIDAR fusion strategies can be categorized into three primary types: (1) early fusion, which combines raw sensor data prior to feature extraction \cite{lang2019pointpillars,PointNet2018}; (2) deep fusion, integrating features at multiple network levels \cite{li2024fast,li2023poly}; and (3) late fusion, merging high-level features or detection results \cite{CLOCs2020,Fast-CLOCs2022}. The integration of diverse sensor modalities (e.g., image, point cloud, millimeter-wave radar, depth, and 4D radar data) has become crucial for enhancing detection accuracy. We propose a novel camera-radar feature fusion approach that synergistically leverages the complementary strengths of image and point cloud data. By incorporating advanced data augmentation techniques, our method achieves state-of-the-art accuracy and robustness in 3D object detection, particularly in challenging scenarios and adverse weather conditions.


% \noindent
% \textbf{Data augmentation.}
% Data augmentation, a critical technique in computer vision, involves transforming training data to generate new samples, thereby enhancing dataset diversity and robustness. This process is particularly crucial in 3D point cloud detection, where data scarcity often poses challenges. By applying methods such as rotation, translation, scaling, and flipping, data augmentation mitigates overfitting and improves model generalization, especially for sparse point cloud data. Recent LIDAR research has extensively explored data augmentation, with notable contributions including PointAugmentation~\cite{Pointaugmenting2021}, PA-AUG~\cite{Part-aware2021}, SE-SSD~\cite{SE-SSD2021}, ProposalContrast~\cite{Proposalcontrast2022}, HSSDA~\cite{HSSDA2023}, and PG-RCNN~\cite{PGRCNN2023}. Our work primarily focuses on various enhancement techniques within the 3D object detection domain, with particular emphasis on flip and rotate operations, which have demonstrated significant efficacy.

\subsection{3D Multiple Object Tracking}

3D MOT is a critical component in autonomous driving perception systems, with increasing real-world applications. Despite its relatively recent emergence, the field has seen significant advances in the past decade. AB3DMOT~\cite{weng20203d} pioneered the extension of the \textbf{Tracking-by-Detection (TBD)} framework in 3D space, establishing an effective baseline. Subsequent work has focused on improving tracking accuracy by addressing challenges such as occlusion, data association, and long-term tracking in dynamic environments. The TBD framework features a well-established pipeline, dividing 3D MOT into four components: pre-processing, estimation, association, and life-cycle.

Poly-MOT and Fast-Poly are efficient 3D MOT methods based on TBD framework. Despite their status as advanced 3D MOT methods, these approaches continue to face significant challenges. These include poor tracking performance under adverse weather conditions, difficulties in handling crowded scenes and small objects, as well as errors in trajectory management. In response to these limitations, our proposed Easy-poly framework specifically addresses these issues by introducing a series of innovative solutions.

\textbf{Pre-processing.}
While prior work~\cite{pang2022simpletrack, li2023poly, li2023camo} has focused primarily on tracking algorithms, the object detection component has received less attention. In contrast, Easy-Poly emphasizes multi-modal data augmentation for K3D detection and incorporates optimized Non-Maximum Suppression (NMS) threshold parameters. These enhancements significantly improve small object detection and crowded scene handling capabilities.

\textbf{Estimation.}
This module utilizes filters (Linear Kalman Filter~\cite{kim2021eagermot, li2023camo, pang2022simpletrack, PC3T}, Extended Kalman Filter~\cite{li2023poly}, Point Filter~\cite{benbarka2021score, yin2021center}, etc.), along with motion models, to perform two functions:
(1) Predict the alive tracklets to achieve temporal alignment and association with the detection. (2) Update the matched tracklets with the corresponding observations, preparing prior information for downstream. To enhance the AMOTA score, Easy-poly optimizes the BICYCLE model parameters through grid search or Bayesian optimization. In the motion module, we integrate object detector confidence as a weighting factor in Linear and Extended Kalman Filters, which is DMM, improving 3D object tracking accuracy and robustness.

\textbf{Association.}
As the core of the system, this module establishes tracklet-observation similarity and resolves matching correspondence. Geometry-based (IoU~\cite{weng20203d, wang2022deepfusionmot}, GIoU~\cite{li2023camo, li2023poly, pang2022simpletrack}, Euclidean~\cite{benbarka2021score, PC3T, kim2021eagermot}, NN distance~\cite{ding20233dmotformer, sadjadpour2023shasta, zaech2022learnable, gwak2022minkowski}, etc.) and appearance-based are the commonly used affinity metrics. Despite advancements in association techniques like Fast-Poly, addressing uncertainties and ambiguities in data association remains challenging, particularly in complex scenarios involving occlusions, missed detections, and false positives. Easy-Poly proposes to leverage DTO algorithm to solve this dilemma.

 \textbf{Life-cycle.}
This module initializes, terminates, and merges tracklets based on the count-based strategy~\cite{kim2021eagermot, li2023camo, li2023poly, pang2022simpletrack, wang2022deepfusionmot, weng20203d} or the confidence-based~\cite{benbarka2021score, li2023poly, pang2022simpletrack, PC3T} strategy. To mitigate issues such as premature tracking termination, we propose extending the tracking duration and dynamically adjusting the threshold. This approach enhances the robustness of our tracking framework, ensuring persistent target tracking across extended sequences.







% Recent advancements in computer vision and machine learning have been made in areas such as person retrieval \cite{shen2023pbsl}, vehicle re-identification \cite{shen2023triplet}, and long-term generation of talking faces \cite{shen2025long}. In particular, the development of pose-guided generation frameworks and customizable virtual dressing models has shown promising results \cite{shen2024imagpose,shen2024imagdressing}. Furthermore, new techniques for boosting consistency in story visualization and advancing image synthesis have contributed significantly to the state of the art \cite{shen2023advancing,shen2024boosting}.
% Some methods leverage deep learning techniques, such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, to capture temporal dependencies and improve object association across frames. Recent approaches have also incorporated sensor fusion, combining 3D point clouds and image data, to further enhance tracking performance. 

