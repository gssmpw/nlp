\section{Methodology}
\label{sec: method}


\begin{table}
\caption{Comparison of Existing 3D object detection methods Applied to the nuScenes Val Set, except for line 2 using LIDAR only, all other methods are based on multi-modal.}
\label{tab:mAP}
    % \renewcommand{\arraystretch}{0.7}
    \setlength{\tabcolsep}{0.6mm}
  \begin{tabular}{ccclccl}
    \toprule
    \textbf{Method} & \textbf{Detector} & \textbf{ Augmentation} & \textbf{mAP} & \textbf{NDS} \\
    \midrule
     FocalsConv~\cite{chen2022focal} & CenterPoint~\cite{yin2021center} &  \checkmark & 63.86 & 69.41 \\
      FocalsConv~\cite{chen2022focal} & LargeKernel3D~\cite{chen2022scaling} & $\times$  & 63.30 & 69.10 \\
    \midrule
     % Ours & CenterPoint & 2D+3D & No & 63.15 & 68.88 \\
     \textbf{Ours} & CenterPoint~\cite{yin2021center} &  \checkmark & 
        \textcolor{blue}{64.89} & 
        \textcolor{blue}{70.13} \\
       % \textbf{70.28} \\
     % \midrule
     % Ours & LargeKernel3D & 2D+3D & No & 63.56 & 69.40 \\
     % Ours & LargeKernel3D & 2D+3D & \colorbox{red}{Yes}  & 
     \textbf{Ours} & LargeKernel3D~\cite{chen2022scaling} & \checkmark  & 
        \textcolor{red}{64.96} & 
        \textcolor{red}{70.28} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Augmented Proposal Generator}

 \begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Images/Framework.pdf}
    \vspace{-8pt}
    \caption
    {
       The pipeline of our Easy-Poly method. Real-time improvements to the baseline~\cite{li2024fast} are highlighted in distinct colors.
        \textbf{\textcolor[RGB]{255, 182, 193}{Pink}} denotes \textcolor{black}{the Optimization of Fast-Poly.}
        \textbf{\textcolor[RGB]{139, 58, 98}{Purple}} denotes \textcolor{black}{the new functional modules.}
     }
     \Description{}
    \label{fig: frameworkpoly}
\end{figure*}


% \section{Optimized FocalsConv Framework for Multi-modal Object Detection}
We propose an augmented proposal generator based on FocalsConv to generate more accurate 3D proposals. It involves upgrades to \textbf{CenterPoint} and \textbf{LargeKernel3D}, incorporating advanced data augmentation techniques and refined SpConv convolution methods. To tackle real-world challenges, we implement robust optimization and exception handling mechanisms for erroneous and empty frames. This leads to significant improvements in indexing efficiency during training, evaluation, and tracking phases, and substantial enhancements in object detection performance metrics, such as mAP and NDS (please see the Table~\ref{tab:mAP}).

Our optimized FocalsConv framework enhances the CenterPoint and LargeKernel3D methods through multi-modal data augmentation. A comprehensive analysis of augmentation techniques, including \textbf{"db\_sample"}, \textbf{"flip"}, \textbf{"rotate"}, \textbf{"rescale"}, and \textbf{"translate"}, reveals that a combination of \textbf{double flip} and \textbf{rotation} significantly boosts detection accuracy, especially in crowded scenes and for small object detection. The convolution process in FocalsConv is mainly implemented via the Det3D backbone network. We integrate a multi-modal LargeKernel3D module into the existing LiDAR-based LargeKernel3D framework and introduce a novel VoxelSpecific module, derived from VoxelLocal, for efficient voxelization. In the post-processing stage, FocalsConv employs rotated NMS followed by multi-class NMS. Comparative experiments with circular NMS show that PointPillars benefits the most from this approach, while LargeKernel3D achieves optimal performance with rotated NMS, and CenterPoint shows intermediate results. Table~\ref{tab:mAP} presents the performance differences between the LargeKernel3D and CenterPoint methods in both the original FocalsConv model and our optimized version.
% We propose a comprehensive optimization framework for FocaleConv, encompassing enhancements to \textbf{CenterPoint} and \textbf{LargeKernel3D}, with a primary focus on advancing multi-modal object detection. Our approach integrates sophisticated data augmentation techniques and refined spconv convolution methods. To address challenges in real-world scenarios, we implement robust optimization and exception handling mechanisms for erroneous and empty frames. This strategy significantly improves indexing efficiency during training, evaluation, and tracking phases, resulting in substantial enhancements to object detection performance metrics, notably mAP and NDS (see Table~\ref{tab:mAP}). The incorporation of advanced data augmentation techniques serves a dual purpose: mitigating overfitting risks and bolstering the model's generalization capabilities through strategic expansion of the training dataset.


% We present an optimized FocalsConv framework, enhancing CenterPoint and LargeKernel3D methods with a focus on multi-modal data augmentation. Our comprehensive analysis of augmentation techniques, including \textbf{"db\_sample"}, \textbf{"flip"}, \textbf{"rotate"}, \textbf{"rescale"}, and \textbf{"translate"}, reveals that a combination of \textbf{double flip} and \textbf{rotation} significantly improves detection accuracy, particularly in crowded scenes and for small object detection. The convolution process in FocalsConv is primarily implemented through the Det3D backbone network. We integrate a multi-modal LargeKernel3D module into the existing LiDAR-based LargeKernel3D framework. Additionally, we introduce a novel VoxelSpecific module, derived from VoxelLocal, for efficient voxelization. In the post-processing stage, FocalsConv employs rotated NMS followed by multi-class NMS. Comparative experiments with circular NMS demonstrate that PointPillars benefits most from this approach, while LargeKernel3D achieves optimal performance with rotated NMS, and CenterPoint shows intermediate results. Table~\ref{tab:mAP} illustrates the performance disparities between LargeKernel3D and CenterPoint methods in both the original FocalsConv model and our optimized version.

\subsection{Easy-Poly Framework}
In this section, we introduce the details of the Easy-Poly, that processing the output from the augmented proposal generator to produce the results of 3D MOT. Easy-Poly's pipeline is structured into five key stages: \textbf{Pre-processing}, \textbf{Association}, \textbf{Matching}, \textbf{Estimation}, and \textbf{Life-cycle}. The process unfolds as follows:
\begin{itemize}
\item \textbf{Pre-processing and Prediction:} For each frame, Easy-Poly employs parallel computing processes to filter 3D detections and predict existing trajectories. SF and NMS filters are applied to the detections, while specialized filters predict the motion (time-variable), score, and time-invariant states of trajectories.
\item \textbf{Association and Matching:} Cost matrices are constructed for two-stage association. Voxel mask and geometry-based metric accelerate NMS and matching computations. The DTO  algorithm~\cite{kuhn1955hungarian} is then applied to determine matched pairs, unmatched detections, and unmatched tracklets.
\item \textbf{State Update:} To optimize matrix dimensionality, time-invariant and time-variable states in matched tracklets are updated using optimized lightweight filter and Extended Kalman Filter (EKF), respectively. Scores are refined using a confidence-count mixed life-cycle approach. We propose novel DMM module, enhancing the system's robustness to varying environmental conditions.
\item \textbf{Initialization and Termination:} Unmatched detections are initialized as new active tracklets. FP agents in are identified through soft-termination, considering max-age and online average refined scores.
\item \textbf{Frame Advancement:} The remaining tracklets are forwarded to downstream tasks and prepared for subsequent frame tracking.
\end{itemize}

% Our Easy-Poly mainly optimizes and adjusts the past work of Fast Poly, see in Figure~\ref{fig: frameworkpoly}. For example, in the preprocessing phase, we format the first frame of the result file generated by FocalsConv+ as the input data of the tracking part. Fine-tune the score filter and voxel mask to optimize the NMS process. Existing 3D detectors generate numerous low-confidence bounding boxes to ensure high recall, but applying these detections directly to update trajectories can result in severe ID switches. To tackle this issue, raw detection objects must be preprocessed to reduce false-positive matches. Before NMS, we apply SF to remove detection objects of lower confidence scores. We apply NMS to remove bboxes with high similarity, improving precision without significant loss of recall. Directly applying NMS to objects would lead to substantial computational overhead. SF can efficiently remove apparent false-positive detections, improving the inference speed of the algorithm.





Our Easy-Poly framework significantly enhances the previous Fast-Poly work, introducing numerous optimizations and innovations, as illustrated in Figure~\ref{fig: frameworkpoly}.
In the pre-processing stage, we utilize the output from our augmented proposal generator, formatting the initial frame as input for the tracking module. We fine-tuned the score filter and voxel mask to enhance performance. A two-stage optimization strategy is implemented: first, we apply \textbf{Score Filtering (SF)} to eliminate low-confidence detections, effectively reducing false positives. Subsequently, \textbf{Non-Maximum Suppression (NMS)} is applied to remove highly similar bounding boxes, improving precision without significantly compromising recall. We introduce a DTO data association algorithm to address uncertainty and ambiguity in the data association phase, and implement a DMM module to enhance tracking accuracy and robustness during the life-cycle phase.

\subsection{Dynamic Track-Oriented Data Association}

In the data association stage, we introduce the \textbf{DTO} algorithm, a novel technique complementing traditional \textbf{Hungarian}, \textbf{Greedy}, and \textbf{Mutual Nearest Neighbor (MNN)} approaches. DTO, an optimized variant of Multiple Hypothesis Tracking (MHT), addresses object-tracking assignment uncertainties by maintaining and updating multiple association hypotheses between detections and tracks over time. This approach enables robust handling of complex scenarios as new observations become available.

%  The algorithm operates as follows:

% \begin{itemize}
% \item \textbf{Initialization:} In the first time step, each detection is considered a new track, and a hypothesis is created for each detection.
% \item \textbf{Prediction:} For each existing track, the algorithm predicts the expected location of the object in the next time step based on the object's previous state and a motion model.
% \item \textbf{Association:} The algorithm computes the cost matrix of associating each detection with each existing track. This cost can be based on various factors, such as the distance between the predicted track location and the detection, the appearance similarity, or other object-specific features.
% \item \textbf{Generation of hypotheses:} The algorithm generates new hypotheses by considering all possible combinations of associating detections with tracks. This includes hypotheses where a detection is associated with an existing track, a new track is created for a detection, or a track remains unassociated (missed detection).
% \item \textbf{Hypothesis Pruning:} The number of hypotheses can grow exponentially over time, so the algorithm needs to prune the hypothesis tree to keep the problem tractable. This is typically done by removing hypotheses with low probabilities or scores.
% \item \textbf{Hypothesis Update:} For each remaining hypothesis, the algorithm updates the state of the associated tracks based on the new detections. This may involve updating the track's position, velocity, and other state variables.
% \item \textbf{Repeat:} The process repeats from step 2 for the next time step, using the updated track states and the new set of detections.
% \end{itemize}
 

The DTO data association algorithm can be summarized by the following mathematical formula in our method:

\begin{equation}
    \mathbf{m}_{\mathrm{det}}, \mathbf{m}_{\mathrm{tra}}, \mathbf{u}_{\mathrm{det}}, \mathbf{u}_{\mathrm{tra}} = \mathrm{DTO}(\mathbf{C}, \boldsymbol{\tau})
    \label{eq:DTO}
\end{equation}

$\mathbf{C} \in \mathbb{R}^{N_{\mathrm{cls}} \times N_{\mathrm{det}} \times N_{\mathrm{tra}}}$ is the cost matrix between classes, detections, and tracks. If the cost matrix has a size of $N_{\mathrm{det}} \times N_{\mathrm{tra}}$, then $\mathbf{C} \in \mathbb{R}^{N_{\mathrm{det}} \times N_{\mathrm{tra}}}$. $\boldsymbol{\tau} \in \mathbb{R}^{N_{\mathrm{cls}}}$ is the matching threshold for each class.  $\mathbf{m}_{\mathrm{det}} \in \mathbb{N}^{|\mathbf{m}_{\mathrm{det}}|}$ is the list of matched detection indices. $\mathbf{m}_{\mathrm{tra}} \in \mathbb{N}^{|\mathbf{m}_{\mathrm{tra}}|}$ is the list of matched track indices.  $\mathbf{u}_{\mathrm{det}} \in \mathbb{N}^{|\mathbf{u}_{\mathrm{det}}|}$ is the list of unmatched detection indices. $\mathbf{u}_{\mathrm{tra}} \in \mathbb{N}^{|\mathbf{u}_{\mathrm{tra}}|}$ is the list of unmatched track indices. Equation \eqref{eq:DTO} describes the main function of the DTO algorithm, which is to take the cost matrix $\mathbf{C}$ and the matching thresholds $\boldsymbol{\tau}$ as inputs, and output the lists of matched detection and track indices $\mathbf{m}_{\mathrm{det}}$ and $\mathbf{m}_{\mathrm{tra}}$, as well as the lists of unmatched detection and track indices $\mathbf{u}_{\mathrm{det}}$ and $\mathbf{u}_{\mathrm{tra}}$.


Specifically, the DTO algorithm first solves the optimal matching between detections and tracks for each class $c \in [1, N_{\mathrm{cls}}]$ using the Hungarian algorithm on the cost matrix $\mathbf{C}[c, :, :]$, and then filters out the matches that do not satisfy the threshold $\tau_c$. Finally, it combines the matching results from all classes to obtain the final lists of matched and unmatched detections and tracks. In summary, Equation \eqref{eq:DTO} describes the inputs, outputs, and internal processing of the DTO algorithm, which is the mathematical expression of this algorithm.


% \begin{equation}
% \mathbf{m}{\mathrm{det}}, \mathbf{m}{\mathrm{tra}}, \mathbf{u}{\mathrm{det}}, \mathbf{u}{\mathrm{tra}} = \mathrm{DTO}(\mathbf{C}, \boldsymbol{\tau})
% \label{eq:DTO}
% \end{equation}
% Among them, the implementation of the function is as follows:
% For each category, find the optimal match between the detection box of the category and the tracking target:
% \begin {align*}
% \mathbf {m}{\mathrm{det}}^{(c)}, \mathbf{m}{\mathrm {tra}}^{(c)} &= \mathrm {solve_hungarian}(\mathbf {C}[c, :, :], \tau_c)
% \end {align*}
% Among them, the function uses the Hungarian algorithm to solve the optimal match and filters out matches that do not meet the conditions based on a threshold.
% Merge the matching results of all categories:

% \begin{align*}
% \mathbf{m}{\mathrm{det}} &= \bigcup{c=1}^{N_{\mathrm{cls}}} \mathbf{m}{\mathrm{det}}^{(c)} \
% \mathbf{m}{\mathrm{tra}} &= \bigcup_{c=1}^{N_{\mathrm{cls}}} \mathbf{m}_{\mathrm{tra}}^{(c)} \
% \end{align*}

% Calculate the index of unmatched detection boxes and tracking targets:
% \begin {align*}
% \mathbf {u}{\mathrm{det}} &= \mathbb{N}^{N{\mathrm{det}}} \setminus \mathbf{m}{\mathrm{det}} \
% \mathbf{u}{\mathrm{tra}} &= \mathbb{N}^{N_{\mathrm{tra}}} \setminus \mathbf{m}_{\mathrm{tra}}
% \end{align*}

% In summary, the mathematical formula of the DTO algorithm can be expressed as:
% \begin{equation}
% \mathbf{m}{\mathrm{det}}, \mathbf{m}{\mathrm{tra}}, \mathbf{u}{\mathrm{det}}, \mathbf{u}{\mathrm{tra}} = \mathrm{DTO}(\mathbf{C}, \boldsymbol{\tau})
% \label{eq:DTO}
% \end{equation}

 
% Among them, \mathbf{m}{\mathrm{det}}, \mathbf{m}{\mathrm{tra}} and \mathbf{u}{\mathrm{det}}, \mathbf{u}{\mathrm{tra}}, respectively, represent the index lists of matched detection boxes and tracking targets, and respectively represent the index lists of unmatched detection boxes and tracking targets.

The key advantage of the DTO algorithm is its ability to handle uncertainty and ambiguity in the data association problem. By maintaining multiple hypotheses, the algorithm can explore different possible associations and delay the decision-making process until more information becomes available. This can lead to more robust and accurate tracking performance, especially in challenging scenarios with occlusions, missed detections, or false alarms.

% Extensive experimental evaluations were conducted to compare the performance of the \textbf{DTO}, \textbf{Hungarian}, \textbf{Greedy}, and \textbf{MNN} algorithms. The results conclusively demonstrate that the Hungarian and DTO algorithms outperform their counterparts in the given application context. The evaluation provides a detailed comparison of the strengths and weaknesses of each algorithm, highlighting their adaptability and accuracy within the specific use case.

\subsection{Dynamic Motion Modeling} \subsubsection{Confidence Weighted Kalman Filter}

In our motion model, we introduce the confidence score from the object detector as a weighting factor in the update steps of both Linear Kalman Filter and Extended Kalman Filter. This approach assigns higher weights to detections with greater confidence during state and covariance matrix updates. Consequently, this enhancement improves the accuracy (MOTA and AMOTA) and robustness of 3D object tracking. The modified update equation for the state estimate can be expressed as:

\begin{equation}
\hat{x}k = \hat{x}{k|k-1} + w_k K_k(z_k - H_k\hat{x}_{k|k-1})
\label{eq:XK}
\end{equation}

where $w_k$ is the confidence score of the detection at time step $k$, and $K_k$ is the Kalman gain.

Similarly, the covariance update is adjusted to:

\begin{equation}
P_k = (I - w_kK_kH_k)P_{k|k-1}(I - w_kK_kH_k)^T + w_k^2K_kR_kK_k^T
\label{eq:PK}
\end{equation}

This weighted approach effectively incorporates the reliability of detections into the filtering process, leading to more accurate and robust 3D object tracking performance.

\subsubsection{Adaptive Noise Covariances}

We use \textbf{Kalman Filter} for the motion model, we have introduced new dynamic adjustments to the noise covariance, including dynamic adjustment of measurement noise covariance and dynamic adjustment of process noise covariance. These are applied to both \textbf{Linear Kalman Filter} and \textbf{Extend Kalman Filter}.
The dynamic adjustment of measurement noise covariance is based on the magnitude of the measurement residual to dynamically adjust the measurement noise covariance R. When the measurement residual is small, the value of R is reduced; when the measurement residual is large, the value of R is increased. The specific formula is as follows:
% \[
% R_{new} = \begin{cases}
% 0.9 \cdot R, & \text{if } \|res\| < 1.0 \\
% 1.1 \cdot R, & \text{if } \|res\| > 5.0 \\
% R, & \text{otherwise}
% \end{cases}
% \]

\begin{equation}
R_{new} = \begin{cases}
0.9 \cdot R, & \text{if } \|res\| < 1.0 \\
1.1 \cdot R, & \text{if } \|res\| > 5.0 \\
R, & \text{otherwise}
\end{cases}
\label{eq:R}
\end{equation}

\(R_{new}\) is the adjusted measurement noise covariance, \(R\) is the original measurement noise covariance, \(\|res\|\) represents the Euclidean norm (L2 norm) of the measurement residual, \(\|res\| = \sqrt{\sum_{i=1}^n res_i^2}\), where \(res_i\) is the i-th component of the residual vector. This formula expresses the following logic:
If the norm of the residual is less than 1.0, R is reduced by 10\%. If the norm of the residual is greater than 5.0, R is increased by 10\%. If the norm of the residual is between 1.0 and 5.0, R remains unchanged. This dynamic adjustment strategy aims to adjust the behavior of the Kalman filter based on the reliability of the measurements. When the measurement residual is small, it increases the trust in the measurement (decreases R); when the measurement residual is large, it decreases the trust in the measurement (increases R).

The dynamic adjustment of process noise covariance is based on the current state to dynamically adjust the process noise covariance Q. For example, adjusting the value of Q based on the magnitude of velocity. Its formula is as follows:


% \[
% Q_{new} = \begin{cases}
% 0.9 \cdot Q, & \text{if } \|\mathbf{v}\| < 1.0 \\
% 1.1 \cdot Q, & \text{if } \|\mathbf{v}\| > 10.0 \\
% Q, & \text{otherwise}
% \end{cases}
% \]

\begin{equation}
Q_{new} = \begin{cases}
0.9 \cdot Q, & \text{if } \|\mathbf{v}\| < 1.0 \\
1.1 \cdot Q, & \text{if } \|\mathbf{v}\| > 10.0 \\
Q, & \text{otherwise}
\end{cases}
\label{eq:Q}
\end{equation}

The \(Q_{new}\) is the noise covariance, \(Q\) is the original process noise covariance, \(\mathbf{v} = [state_1, state_2]\) represents the first two components of the state vector (assumed to be velocity components), \(\|\mathbf{v}\| = \sqrt{state_1^2 + state_2^2}\) is the Euclidean norm (L2 norm) of the velocity vector.
This formula expresses the following logic: If the norm of the velocity is less than 1.0, Q is reduced by 10\%. If the norm of the velocity is greater than 10.0, Q is increased by 10\%. If the norm of the velocity is between 1.0 and 10.0, Q remains unchanged.

This dynamic adjustment strategy aims to adjust the Kalman filter's process model based on the current state (in this case, velocity). When the velocity is low, it decreases the process noise (reduces Q), indicating higher confidence in the system dynamics; when the velocity is high, it increases the process noise (increases Q), indicating lower confidence in the system dynamics.

Autonomous vehicles encounter diverse environmental conditions during operation, such as urban streets, highways, and adverse weather. Dynamic adjustment of R and Q enables the Kalman filter to adapt to these varying conditions, enhancing tracking accuracy by increasing measurement uncertainty in rainy conditions and decreasing it in clear weather. Sensor performance in autonomous vehicles may fluctuate due to factors like temperature, vibration, or partial occlusion. Dynamic R adjustment compensates for these variations. Optimal noise parameters may differ for near and far target tracking, with Q allowing more precise tracking at close range and greater uncertainty at longer distances.


\subsection{Life-cycle Adjustment}

Threshold adjustment significantly impacts object detection and tracking performance. The \textbf{Intersection over Union (IoU)} threshold determines the trade-off between precision and recall. A higher threshold ensures more accurate detections but may miss objects with slightly lower IoU. Conversely, a lower threshold increases sensitivity, potentially detecting more objects at the risk of false positives. Through extensive experimentation, we iteratively optimize this threshold to achieve a balance between detection accuracy and false alarm rate, thereby enhancing the overall system performance in autonomous driving scenarios.

In the life-cycle management stage, compared to the baseline Fast-Poly, Easy-Poly improve in terms of the motion model. We adjusted the wheelbase ratio and rear tire ratio parameters, finding the optimal values through grid search or Bayesian optimization, changing them from the original 0.8 and 0.5 to the latest 0.6 and 0.3. Through these adjustments, tracking performance and robustness have been improved. In additional, we proposed increasing the maximum age parameter to 20 for all detection and tracking categories. This modification significantly extends object tracking duration while maintaining high tracking accuracy and computational efficiency. Consequently, we observed a substantial reduction in frame loss during life-cycle management, resulting in more complete and robust tracking trajectories for each object of interest.



% \subsection{Conclusion}

% Our Easy-Poly framework demonstrates comprehensive optimizations across all stages, from pre-processing to life-cycle management. Key innovations include enhanced pre-processing techniques, a novel DTO data association algorithm, and an advanced DMM incorporating both \textbf{Adaptive Noise Covariances} and \textbf{Confidence
% Weight}. Rigorous experimental evaluations, including comparative studies and ablation analyses, validate the efficacy and robustness of Easy-Poly across diverse scenarios, underscoring its meticulousness and comprehensiveness.


