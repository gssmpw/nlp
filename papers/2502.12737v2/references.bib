@inproceedings{das_case-based_2021,
	title = {Case-based {Reasoning} for {Natural} {Language} {Queries} over {Knowledge} {Bases}},
	booktitle = {EMNLP},
	author = {Das, Rajarshi and Zaheer, Manzil and Thai, Dung and Godbole, Ameya and Perez, Ethan and Lee, Jay Yoon and Tan, Lizhen and Polymenakos, Lazaros and McCallum, Andrew},
	year = {2021},
	pages = {9594--9611}
}

@inproceedings{wu_survey_2019,
  author = {Wu, Peiyun and Zhang, Xiaowang and Feng, Zhiyong},
  title = {A {{survey}} of {{question answering}} over {{knowledge base}}},
  booktitle = {CCKS},
  year = {2019},
  pages = {86--97}
}


@inproceedings{zhang_subgraph_2022,
	title = {Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering},
	booktitle = {ACL},
	author = {Zhang, Jing and Zhang, Xiaokang and Yu, Jifan and Tang, Jian and Tang, Jie and Li, Cuiping and Chen, Hong},
	year = {2022},
	pages = {5773--5784}
}

@inproceedings{auerDBpediaNucleusWeb2007,
  title = {{{DBpedia}}: A Nucleus for a Web of Open Data},
  author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  year = {2007},
  pages = {722--735},
  booktitle = {The Semantic Web}
}



@misc{gabrilovich_facc1_2013,
  author = {Gabrilovich, Evgeniy and Ringgaard, Michael and Subramanya, Amarnag},
  title = {{FACC1: Freebase annotation of clueweb corpora, version 1(release date 2013-06-26,  format version 1, correction level 0)}},
  year = {2013}
}

@inproceedings{lan_query_2020,
	title = {Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases},
	booktitle = {ACL},
	author = {Lan, Yunshi and Jiang, Jing},
	year = {2020},
	pages = {969--974}
}

@inproceedings{ding_enhancing_2024,
author = {Ding, Wentao and Li, Jinmao and Luo, Liangchuan and Qu, Yuzhong},
title = {Enhancing Complex Question Answering over Knowledge Graphs through Evidence Pattern Retrieval},
year = {2024},
booktitle = {WWW},
pages = {2106--2115}
}

@inproceedings{lin_knowledge-injected_2024,
	title = {A Knowledge-Injected Curriculum Pretraining Framework for Question Answering},
	booktitle = {WWW},
	author = {Lin, Xin and Su, Tianhuang and Huang, Zhenya and Xue, Shangzi and Liu, Haifeng and Chen, Enhong},
	year = {2024},
	pages = {1986--1997}
}

@inproceedings{dong_question_2015,
	title = {Question Answering over {Freebase} with Multi-Column Convolutional Neural Networks},
	booktitle = {ACL},
	author = {Dong, Li and Wei, Furu and Zhou, Ming and Xu, Ke},
	year = {2015},
	pages = {260--269}
}

@inproceedings{liu_knowledge_2023,
	title = {Knowledge Graph Question Answering with Ambiguous Query},
	booktitle = {WWW},
	author = {Liu, Lihui and Chen, Yuzhong and Das, Mahashweta and Yang, Hao and Tong, Hanghang},
	year = {2023},
	pages = {2477--2486},
}

@inproceedings{jiang_unikgqa_2023,
	title = {{UniKGQA}: {Unified} Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph},
	author = {Jiang, Jinhao and Zhou, Kun and Zhao, Wayne Xin and Wen, Ji-Rong},
        booktitle = {ICLR},
	year = {2023},
}

@inproceedings{zhou_commonsense_2018,
	title = {Commonsense knowledge aware conversation generation with graph attention},
	booktitle = {IJCAI} ,
	author = {Zhou, Hao and Young, Tom and Huang, Minlie and Zhao, Haizhou and Xu, Jingfang and Zhu, Xiaoyan},
	year = {2018},
	pages = {4623--4629},
}

@article{guo_survey_2022,
	title = {A Survey on Knowledge Graph-Based Recommender Systems},
	author = {Guo, Qingyu and Zhuang, Fuzhen and Qin, Chuan and Zhu, Hengshu and Xie, Xing and Xiong, Hui and He, Qing},
    journal= {IEEE Transactions on Knowledge and Data Engineering},
	year = {2022},
	pages = {3549--3568},
volume={34},
number={8}
}

@inproceedings{jalota_lauren_2021,
	title = {{LAUREN} - {Knowledge} Graph Summarization for Question Answering},
	booktitle = {ICSC},
	author = {Jalota, Rricha and Vollmers, Daniel and Moussallem, Diego and Ngomo, Axel-Cyrille Ngonga},
	year = {2021},
	pages = {221--226},
}


@misc{bordes_large-scale_2015,
	title = {Large-scale Simple Question Answering with Memory Networks},
	publisher = {arXiv},
	author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
	month = jun,
	year = {2015},
	note = {arXiv:1506.02075},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}
@inproceedings{wu_seq2seq-based_2020,
	title = {A {Seq2seq}-Based Approach to Question Answering over Knowledge Bases},
	booktitle = {Joint International Semantic Technology Conference},
	author = {Wu, Linjuan and Wu, Peiyun and Zhang, Xiaowang},
	year = {2019},
	pages = {170--181}
}

@misc{guo_towards_2019,
	title = {Towards {Complex} {Text}-to-{SQL} in {Cross}-{Domain} {Database} with {Intermediate} {Representation}},
	url = {http://arxiv.org/abs/1905.08205},
	abstract = {We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. IRNet aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) and the implementation details in SQL; 2) the challenge in predicting columns caused by the large number of out-of-domain words. Instead of end-to-end synthesizing a SQL query, IRNet decomposes the synthesis process into three phases. In the first phase, IRNet performs a schema linking over a question and a database schema. Then, IRNet adopts a grammar-based neural model to synthesize a SemQL query which is an intermediate representation that we design to bridge NL and SQL. Finally, IRNet deterministically infers a SQL query from the synthesized SemQL query with domain knowledge. On the challenging Text-to-SQL benchmark Spider, IRNet achieves 46.7\% accuracy, obtaining 19.5\% absolute improvement over previous state-of-the-art approaches. At the time of writing, IRNet achieves the first position on the Spider leaderboard.},
	urldate = {2024-10-10},
	publisher = {arXiv},
	author = {Guo, Jiaqi and Zhan, Zecheng and Gao, Yan and Xiao, Yan and Lou, Jian-Guang and Liu, Ting and Zhang, Dongmei},
	month = may,
	year = {2019},
	note = {arXiv:1905.08205},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{wang_rat-sql_2021,
	title = {{RAT}-{SQL}: {Relation}-{Aware} {Schema} {Encoding} and {Linking} for {Text}-to-{SQL} {Parsers}},
	author = {Wang, Bailin and Shin, Richard and Liu, Xiaodong and Polozov, Oleksandr and Richardson, Matthew},
	booktitle = {ACL},
	year = {2020},
    pages = {7567--7578}
}


@misc{liang_lambda_2013,
	title = {Lambda {Dependency}-{Based} {Compositional} {Semantics}},
	url = {http://arxiv.org/abs/1309.4408},
	abstract = {This short note presents a new formal language, lambda dependency-based compositional semantics (lambda DCS) for representing logical forms in semantic parsing. By eliminating variables and making existential quantification implicit, lambda DCS logical forms are generally more compact than those in lambda calculus.},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Liang, Percy},
	month = sep,
	year = {2013},
	note = {arXiv:1309.4408},
	keywords = {Computer Science - Artificial Intelligence},
}

@inproceedings{francis_cypher_2018,
	title = {Cypher: {An} {Evolving} {Query} {Language} for {Property} {Graphs}},
	booktitle = {SIGMOD},
	author = {Francis, Nadime and Green, Alastair and Guagliardo, Paolo and Libkin, Leonid and Lindaaker, Tobias and Marsault, Victor and Plantikow, Stefan and Rydberg, Mats and Selmer, Petra and Taylor, Andrés},
	year = {2018},
	pages = {1433--1445}
}

@inproceedings{hu_lora_2021,
	title = {{LoRA}: {Low}-Rank Adaptation of Large Language Models},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year = {2022},
	booktitle = {ICLR}
}

@inproceedings{li_efficient_2020,
	title = {Efficient One-Pass End-to-End Entity Linking for Questions},
	author = {Li, Belinda Z. and Min, Sewon and Iyer, Srinivasan and Mehdad, Yashar and Yih, Wen-tau},
    booktitle = {ACL},
	year = {2020},
    pages = {6433–6441}
}

@inproceedings{chen_retrack_2021,
	address = {Online},
	title = {{ReTraCk}: {A} {Flexible} and {Efficient} {Framework} for {Knowledge} {Base} {Question} {Answering}},
	shorttitle = {{ReTraCk}},
	url = {https://aclanthology.org/2021.acl-demo.39},
	doi = {10.18653/v1/2021.acl-demo.39},
	abstract = {We present Retriever-Transducer-Checker (ReTraCk), a neural semantic parsing framework for large scale knowledge base question answering (KBQA). ReTraCk is designed as a modular framework to maintain high flexibility. It includes a retriever to retrieve relevant KB items efficiently, a transducer to generate logical form with syntax correctness guarantees and a checker to improve transduction procedure. ReTraCk is ranked at top1 overall performance on the GrailQA leaderboard and obtains highly competitive performance on the typical WebQuestionsSP benchmark. Our system can interact with users timely, demonstrating the efficiency of the proposed framework.},
	language = {en-US},
	urldate = {2024-03-15},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Shuang and Liu, Qian and Yu, Zhiwei and Lin, Chin-Yew and Lou, Jian-Guang and Jiang, Feng},
	editor = {Ji, Heng and Park, Jong C. and Xia, Rui},
	month = aug,
	year = {2021},
	note = {},
	keywords = {/prior},
	pages = {325--336},
}

@article{omar_universal_2023,
	title = {A {Universal} {Question}-{Answering} {Platform} for {Knowledge} {Graphs}},
	volume = {1},
	issn = {2836-6573},
	url = {https://dl.acm.org/doi/10.1145/3588911},
	doi = {10.1145/3588911},
	abstract = {Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs. In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-sequence model. We also develop a just-in-time linker that maps at query time the abstract representation to a SPARQL query for a specific KG, using only the publicly accessible APIs and the existing indices of the RDF store, without requiring any pre-processing. Our experiments with several real KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin the state-of-the-art in terms of quality of answers and processing time, especially for arbitrary KGs, unseen during the training.},
	language = {en},
	number = {1},
	urldate = {2024-02-01},
	journal = {Proceedings of the ACM on Management of Data},
	author = {Omar, Reham and Dhall, Ishika and Kalnis, Panos and Mansour, Essam},
	month = may,
	year = {2023},
	note = {},
	keywords = {\#LQAD1, \#QALD-9},
	pages = {1--25},
}

@article{vrandecic_wikidata_2014,
	title = {Wikidata: A Free Collaborative Knowledgebase},
	volume = {57},
	number = {10},
	journal = {Communications of the ACM},
	author = {Vrandečić, Denny and Krötzsch, Markus},
	year = {2014},
	pages = {78--85}
}

@inproceedings{banerjee_semantic_2023,
	address = {Cham},
	title = {Semantic {Parsing} for {Knowledge} {Graph} {Question} {Answering} with {Large} {Language} {Models}},
	isbn = {978-3-031-43458-7},
	doi = {10.1007/978-3-031-43458-7_42},
	abstract = {This thesis explores the topic of Knowledge Graph Question Answering with a special emphasis on semantic parsing approaches, incorporating pre-trained text-to-text language models. We use the text generation ability of these models to convert natural language questions to logical forms. We test whether correct logical forms are being generated, and if not, how to mitigate the failure cases. As a second step, we try to make the same models generate additional information to aid the process of grounding of the logical forms to entities, relations and literals in the Knowledge Graph. In experiments conducted so far, we see encouraging results on both generation of base logical forms, and grounding them to the KG elements. At the same time, we discover failure cases prompting directions in future work (The author considers himself a ‘middle-stage’ Ph.D. candidate).},
	language = {en},
	booktitle = {The {Semantic} {Web}: {ESWC} 2023 {Satellite} {Events}},
	publisher = {Springer Nature Switzerland},
	author = {Banerjee, Debayan},
	editor = {Pesquita, Catia and Skaf-Molli, Hala and Efthymiou, Vasilis and Kirrane, Sabrina and Ngonga, Axel and Collarana, Diego and Cerqueira, Renato and Alam, Mehwish and Trojahn, Cassia and Hertling, Sven},
	year = {2023},
	keywords = {/unread},
	pages = {234--243},
}

@article{rogers_qa_2023,
	title = {{QA} {Dataset} {Explosion}: {A} {Taxonomy} of {NLP} {Resources} for {Question} {Answering} and {Reading} {Comprehension}},
	volume = {55},
	issn = {0360-0300},
	shorttitle = {{QA} {Dataset} {Explosion}},
	url = {https://dl.acm.org/doi/10.1145/3560260},
	doi = {10.1145/3560260},
	abstract = {Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with more than 80 new datasets appearing in the past 2 years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of “skills” that question answering/reading comprehension systems are supposed to acquire and propose a new taxonomy. The supplementary materials survey the current multilingual resources and monolingual resources for languages other than English, and we discuss the implications of overfocusing on English. The study is aimed at both practitioners looking for pointers to the wealth of existing data and at researchers working on new resources.},
	language = {en-US},
	number = {10},
	urldate = {2024-03-13},
	journal = {ACM Computing Surveys},
	author = {Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
	year = {2023},
	keywords = {\#Survey, /unread, Reading comprehension, natural language understanding},
	pages = {197:1--197:45},
}

@misc{cao_pay_2023,
	title = {Pay {More} {Attention} to {Relation} {Exploration} for {Knowledge} {Base} {Question} {Answering}},
	url = {http://arxiv.org/abs/2305.02118},
	doi = {10.48550/arXiv.2305.02118},
	abstract = {Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the knowledge base to enhance entity representation and introduce additional supervision. We explore guidance from relations in three aspects, including (1) distinguishing similar entities by employing a variational graph auto-encoder to learn relation importance; (2) exploring extra supervision by predicting relation distributions as soft labels with a multi-task scheme; (3) designing a relation-guided re-ranking algorithm for post-processing. Experimental results on two benchmark datasets demonstrate the effectiveness and superiority of our framework, improving the F1 score by 5.7\% from 40.5 to 46.3 on CWQ and 5.8\% from 62.8 to 68.5 on WebQSP, better or on par with state-of-the-art methods.},
	language = {en-US},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Cao, Yong and Li, Xianzhi and Liu, Huiwen and Dai, Wen and Chen, Shuai and Wang, Bin and Chen, Min and Hershcovich, Daniel},
	month = may,
	year = {2023},
	note = {arXiv:2305.02118 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, unread},
}

@inproceedings{banerjee_modern_2022,
	title = {Modern {Baselines} for {SPARQL} {Semantic} {Parsing}},
	url = {http://arxiv.org/abs/2204.12793},
	doi = {10.1145/3477495.3531841},
	abstract = {In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.},
	language = {en-US},
	urldate = {2024-03-01},
	booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	author = {Banerjee, Debayan and Nair, Pranav Ajit and Kaur, Jivat Neet and Usbeck, Ricardo and Biemann, Chris},
	month = jul,
	year = {2022},
	note = {arXiv:2204.12793 [cs]},
	keywords = {\#LQAD2, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	pages = {2260--2265},
}

@article{yin_neural_2021,
	title = {Neural Machine Translating from Natural Language to {SPARQL}},
	volume = {117},
	journal = {Future Generation Computer Systems},
	author = {Yin, Xiaoyu and Gromann, Dagmar and Rudolph, Sebastian},
	year = {2021},
	pages = {510--519}
}

@inproceedings{jiang_knowledge_2022,
	address = {New York, NY, USA},
	series = {{SIGIR} '22},
	title = {Knowledge {Graph} {Question} {Answering} {Datasets} and {Their} {Generalizability}: {Are} {They} {Enough} for {Future} {Research}?},
	isbn = {978-1-4503-8732-3},
	shorttitle = {Knowledge {Graph} {Question} {Answering} {Datasets} and {Their} {Generalizability}},
	url = {https://dl.acm.org/doi/10.1145/3477495.3531751},
	doi = {10.1145/3477495.3531751},
	abstract = {Existing approaches on Question Answering over Knowledge Graphs (KGQA) have weak generalizability. That is often due to the standard i.i.d. assumption on the underlying dataset. Recently, three levels of generalization for KGQA were defined, namely i.i.d., compositional, zero-shot. We analyze 25 well-known KGQA datasets for 5 different Knowledge Graphs (KGs). We show that according to this definition many existing and online available KGQA datasets are either not suited to train a generalizable KGQA system or that the datasets are based on discontinued and out-dated KGs. Generating new datasets is a costly process and, thus, is not an alternative to smaller research groups and companies. In this work, we propose a mitigation method for re-splitting available KGQA datasets to enable their applicability to evaluate generalization, without any cost and manual effort. We test our hypothesis on three KGQA datasets, i.e., LC-QuAD, LC-QuAD 2.0 and QALD-9). Experiments on re-splitted KGQA datasets demonstrate its effectiveness towards generalizability. The code and a unified way to access 18 available datasets is online at https://github.com/semantic-systems/KGQA-datasets as well as https://github.com/semantic-systems/KGQA-datasets-generalization.},
	language = {en-US},
	urldate = {2024-03-09},
	booktitle = {Proceedings of the 45th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Jiang, Longquan and Usbeck, Ricardo},
	year = {2022},
	keywords = {\#Survey, /unread, benchmark, evaluation, generalizability, generalization, kgqa, question answering},
	pages = {3209--3218},
}

@inproceedings{cao_kqa_2022,
	address = {Dublin, Ireland},
	title = {{KQA} {Pro}: {A} {Dataset} with {Explicit} {Compositional} {Programs} for {Complex} {Question} {Answering} over {Knowledge} {Base}},
	shorttitle = {{KQA} {Pro}},
	url = {https://aclanthology.org/2022.acl-long.422},
	doi = {10.18653/v1/2022.acl-long.422},
	abstract = {Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro\_Baselines.},
	language = {en-US},
	urldate = {2024-02-19},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	keywords = {\#KQAPro},
	pages = {6101--6119},
}

@inproceedings{sakor_falcon_2020,
	address = {New York, NY, USA},
	series = {{CIKM} '20},
	title = {Falcon 2.0: {An} {Entity} and {Relation} {Linking} {Tool} over {Wikidata}},
	isbn = {978-1-4503-6859-9},
	shorttitle = {Falcon 2.0},
	url = {https://dl.acm.org/doi/10.1145/3340531.3412777},
	doi = {10.1145/3340531.3412777},
	abstract = {The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/.},
	language = {en-US},
	urldate = {2024-03-07},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Information} \& {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Sakor, Ahmad and Singh, Kuldeep and Patel, Anery and Vidal, Maria-Esther},
	year = {2020},
	keywords = {\#LQAD2, background knowledge, dbpedia, english morphology, entity linking, nlp, relation linking, wikidata},
	pages = {3141--3148},
}

@inproceedings{li_few-shot_2023,
	title = {Few-shot In-context Learning for Knowledge Base Question Answering},
	author = {Li, Tianle and Ma, Xueguang and Zhuang, Alex and Gu, Yu and Su, Yu and Chen, Wenhu},
    booktitle = {ACL},
	year = {2023},
    pages = {6966--6980}
}

@inproceedings{gu_knowledge_2022,
	title = {Knowledge Base Question  Answering: {A} Semantic Parsing Perspective},
	booktitle = {AKBC},
	author = {Gu, Yu and Pahuja, Vardaan and Cheng, Gong and Su, Yu},
	year = {2022}
}

@article{qi_enhancing_2024,
	title = {Enhancing {SPARQL} {Query} {Generation} for {Knowledge} {Base} {Question} {Answering} {Systems} by {Learning} to {Correct} {Triplets}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/4/1521},
	doi = {10.3390/app14041521},
	abstract = {Generating SPARQL queries from natural language questions is challenging in Knowledge Base Question Answering (KBQA) systems. The current state-of-the-art models heavily rely on fine-tuning pretrained models such as T5. However, these methods still encounter critical issues such as triple-flip errors (e.g., (subject, relation, object) is predicted as (object, relation, subject)). To address this limitation, we introduce TSET (Triplet Structure Enhanced T5), a model with a novel pretraining stage positioned between the initial T5 pretraining and the fine-tuning for the Text-to-SPARQL task. In this intermediary stage, we introduce a new objective called Triplet Structure Correction (TSC) to train the model on a SPARQL corpus derived from Wikidata. This objective aims to deepen the model’s understanding of the order of triplets. After this specialized pretraining, the model undergoes fine-tuning for SPARQL query generation, augmenting its query-generation capabilities. We also propose a method named “semantic transformation” to fortify the model’s grasp of SPARQL syntax and semantics without compromising the pre-trained weights of T5. Experimental results demonstrate that our proposed TSET outperforms existing methods on three well-established KBQA datasets: LC-QuAD 2.0, QALD-9 plus, and QALD-10, establishing a new state-of-the-art performance (95.0\% F1 and 93.1\% QM on LC-QuAD 2.0, 75.85\% F1 and 61.76\% QM on QALD-9 plus, 51.37\% F1 and 40.05\% QM on QALD-10).},
	language = {en},
	number = {4},
	urldate = {2024-03-06},
	journal = {Applied Sciences},
	author = {Qi, Jiexing and Su, Chang and Guo, Zhixin and Wu, Lyuwen and Shen, Zanwei and Fu, Luoyi and Wang, Xinbing and Zhou, Chenghu},
	month = jan,
	year = {2024},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\#LQAD2, /prior, /unread, Knowledge Base Question Answering, Text-to-SPARQL, Triplet Structure, further pretraining, semantic parsing},
	pages = {1521},
}

@inproceedings{avila_experiments_2024,
	title = {Experiments with text-to-{SPARQL} based on {ChatGPT}},
	url = {https://ieeexplore.ieee.org/abstract/document/10475614},
	doi = {10.1109/ICSC59802.2024.00050},
	abstract = {Currently, large language models (LLMs) are the state of the art for pre-trained language models. LLMs have been applied to many tasks, including question and answering over Knowledge Graphs (KGs) and text-to-SPARQL, that is, the translation of Natural Language questions to SPARQL queries. With such motivation, this paper first describes preliminary experiments to evaluate the ability of ChatGPT to answer NL questions over KGs. Based on these experiments, the paper introduces Auto-KGQAGPT, an autonomous domain-independent framework based on LLMs for text-to-SPARQL. The framework selects fragments of the KG, which the LLM uses to translate the user’s NL question to a SPARQL query on the KG. Finally, the paper describes preliminary experiments with Auto-KGQAGPT with ChatGPT that indicate that the framework substantially reduced the number of tokens passed to ChatGPT without sacrificing performance.},
	language = {en-US},
	urldate = {2024-04-02},
	booktitle = {2024 {IEEE} 18th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	author = {Avila, Caio Viktor S. and Vidal, Vânia M.P. and Franco, Wellington and Casanova, Marco A.},
	month = feb,
	year = {2024},
	note = {ISSN: 2472-9671},
	keywords = {/unread, Benchmark testing, ChatGPT, Chatbots, Iterative methods, Knowledge Graph, Knowledge graphs, LLM, Natural languages, Semantics, Training, text-to-SPARQL},
	pages = {277--284},
}

@article{lan_complex_2023,
	title = {Complex Knowledge Base Question Answering: {A} Survey},
	volume = {35},
	number = {11},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Lan, Yunshi and He, Gaole and Jiang, Jinhao and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
	year = {2023},
	pages = {11196--11215}
}

@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of Deep Bidirectional Transformers for Language Understanding},
	booktitle = {NAACL-HLT},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2019},
	pages = {4171--4186}
}

@inproceedings{luo_chatkbqa_2024,
	title = {{ChatKBQA}: {A} Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models},
	author = {Luo, Haoran and E, Haihong and Tang, Zichen and Peng, Shiyao and Guo, Yikai and Zhang, Wentai and Ma, Chenghao and Dong, Guanting and Song, Meina and Lin, Wei},
    booktitle = {Findings of the ACL},
    year = {2024},
    pages = {2039--2056}
}

@article{niu_bridging_2023,
	title = {Bridging the {Gap} between {Synthetic} and {Natural} {Questions} via {Sentence} {Decomposition} for {Semantic} {Parsing}},
	volume = {11},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00552/115893/Bridging-the-Gap-between-Synthetic-and-Natural},
	doi = {10.1162/tacl_a_00552},
	abstract = {Semantic parsing maps natural language questions into logical forms, which can be executed against a knowledge base for answers. In real-world applications, the performance of a parser is often limited by the lack of training data. To facilitate zero-shot learning, data synthesis has been widely studied to automatically generate paired questions and logical forms. However, data synthesis methods can hardly cover the diverse structures in natural languages, leading to a large gap in sentence structure between synthetic and natural questions. In this paper, we propose a decomposition-based method to unify the sentence structures of questions, which benefits the generalization to natural questions. Experiments demonstrate that our method significantly improves the semantic parser trained on synthetic data (+7.9\% on KQA and +8.9\% on ComplexWebQuestions in terms of exact match accuracy). Extensive analysis demonstrates that our method can better generalize to natural questions with novel text expressions compared with baselines. Besides semantic parsing, our idea potentially benefits other semantic understanding tasks by mitigating the distracting structure features. To illustrate this, we extend our method to the task of sentence embedding learning, and observe substantial improvements on sentence retrieval (+13.1\% for Hit@1).},
	language = {en},
	urldate = {2024-02-24},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Niu, Yilin and Huang, Fei and Liu, Wei and Cui, Jianwei and Wang, Bin and Huang, Minlie},
	month = may,
	year = {2023},
	keywords = {\#CWQ, \#KQAPro, /prior, /unread},
	pages = {367--383},
}

@article{song_advancements_2023,
	title = {Advancements in {Complex} {Knowledge} {Graph} {Question} {Answering}: {A} {Survey}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	shorttitle = {Advancements in {Complex} {Knowledge} {Graph} {Question} {Answering}},
	url = {https://www.mdpi.com/2079-9292/12/21/4395},
	doi = {10.3390/electronics12214395},
	abstract = {Complex Question Answering over Knowledge Graph (C-KGQA) seeks to solve complex questions using knowledge graphs. Currently, KGQA systems achieve great success in answering simple questions, while complex questions still present challenging issues. As a result, an increasing number of novel methods have been proposed to remedy this challenge. In this survey, we proposed two mainstream categories of methods for C-KGQA, which are divided according to their use for knowledge graph representation and construction, namely, graph metric (GM)-Based Methods and graph neural network (GNN)-based methods. Additionally, we also acknowledge the influence of ChatGPT, which has prompted further research into utilizing knowledge graphs as a knowledge source to assist in answering complex questions. We also introduced methods based on pre-trained models and knowledge graph joint reasoning. Furthermore, we have compiled research achievements from the past three years to make it easier for researchers with similar interests to obtain state-of-the-art research. Finally, we discussed the resources and evaluation methods for tackling C-KGQA tasks and summarized several research prospects in this field.},
	language = {en},
	number = {21},
	urldate = {2024-02-20},
	journal = {Electronics},
	author = {Song, Yiqing and Li, Wenfa and Dai, Guiren and Shang, Xinna},
	month = jan,
	year = {2023},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {\#Survey, complex question, knowledge graph question answering, question answering, survey},
	pages = {4395},
}

@inproceedings{gu_beyond_2021,
	title = {Beyond {I}.{I}.{D}.: {Three} Levels of Generalization for Question Answering on Knowledge Bases},
	shorttitle = {Beyond {I}.{I}.{D}.},
	booktitle = {WWW},
	author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
	year = {2021},
	pages = {3477--3488}
}

@article{hu_empirical_2023,
	title = {An empirical study of pre-trained language models in simple knowledge graph question answering},
	volume = {26},
	issn = {1573-1413},
	url = {https://doi.org/10.1007/s11280-023-01166-y},
	doi = {10.1007/s11280-023-01166-y},
	abstract = {Large-scale pre-trained language models (PLMs) such as BERT have recently achieved great success and become a milestone in natural language processing (NLP). It is now the consensus of the NLP community to adopt PLMs as the backbone for downstream tasks. In recent works on knowledge graph question answering (KGQA), BERT or its variants have become necessary in their KGQA models. However, there is still a lack of comprehensive research and comparison of the performance of different PLMs in KGQA. To this end, we summarize two basic KGQA frameworks based on PLMs without additional neural network modules to compare the performance of nine PLMs in terms of accuracy and efficiency. In addition, we present three benchmarks for larger-scale KGs based on the popular SimpleQuestions benchmark to investigate the scalability of PLMs. We carefully analyze the results of all PLMs-based KGQA basic frameworks on these benchmarks and two other popular datasets, WebQuestionSP and FreebaseQA, and find that knowledge distillation techniques and knowledge enhancement methods in PLMs are promising for KGQA. Furthermore, we test ChatGPT (https://chat.openai.com/), which has drawn a great deal of attention in the NLP community, demonstrating its impressive capabilities and limitations in zero-shot KGQA. We have released the code and benchmarks to promote the use of PLMs on KGQA (https://github.com/aannonymouuss/PLMs-in-Practical-KBQA).},
	language = {en},
	number = {5},
	urldate = {2024-02-19},
	journal = {World Wide Web},
	author = {Hu, Nan and Wu, Yike and Qi, Guilin and Min, Dehai and Chen, Jiaoyan and Pan, Jeff Z. and Ali, Zafar},
	month = sep,
	year = {2023},
	keywords = {\#SQ, \#Survey, \#WQSP, Accuracy and efficiency, Knowledge graph question answering, Pretrained language models, Scalability},
	pages = {2855--2886},
}

@inproceedings{pang_survey_2022,
	title = {A {survey} on {information} {retrieval} {method} for {knowledge} {graph} {complex} {question} {answering}},
	booktitle = {CAC},
	author = {Pang, Junbiao and Zhang, Yongheng and Deng, Jiaxin and Zhu, Xiaoqing},
	year = {2022},
	pages = {1059--1064}
}

@inproceedings{li_flexkbqa_2024,
	title = {{FlexKBQA}: {A} Flexible {LLM}-Powered Framework for Few-Shot Knowledge Base Question Answering},
	author = {Li, Zhenyu and Fan, Sunqi and Gu, Yu and Li, Xiuxing and Duan, Zhichao and Dong, Bowen and Liu, Ning and Wang, Jianyong},
        booktitle = {AAAI},
	year = {2024},
        pages = {18608--18616}
}

@article{borroto_sparql-qa-v2_2023,
	title = {{SPARQL}-{QA}-v2 system for {Knowledge} {Base} {Question} {Answering}},
	volume = {229},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423008850},
	doi = {10.1016/j.eswa.2023.120383},
	abstract = {Accessing the large volumes of information available in public knowledge bases might be complicated for those users unfamiliar with formal languages, such as the SPARQL query language and the ontology definition languages. This issue can be overcome by providing systems able to answer questions posed in natural language on a knowledge base, a task that is called Knowledge Base Question Answering (KBQA) in the literature. More in detail, many KBQA systems aim at translating automatically questions into the corresponding SPARQL queries to be executed over the knowledge base to get the answers. Effective state-of-the-art KBQA systems are based on neural-machine translation but easily fail to recognize words that are Out Of the Vocabulary (OOV) of the training set. This is a serious issue while querying large ontologies where the list of entities is huge and easily evolves over time. In this paper, we present the SPARQL-QA-v2 system that combines in an innovative way Named Entity Linking, Named Entity Recognition, and Neural Machine Translation for addressing the problem of generating SPARQL queries from questions posed in natural language. We demonstrate empirically that SPARQL-QA-v2 is effective and resilient to OOV words and delivers state-of-the-art performance in well-known datasets for question answering over DBpedia and Wikidata knowledge bases.},
	language = {en},
	urldate = {2024-03-12},
	journal = {Expert Systems with Applications},
	author = {Borroto, Manuel A. and Ricca, Francesco},
	month = nov,
	year = {2023},
	keywords = {/unread},
	pages = {120383},
}

@misc{cao_program_2022,
	title = {Program {Transfer} for {Answering} {Complex} {Questions} over {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/2110.05743},
	doi = {10.48550/arXiv.2110.05743},
	abstract = {Program induction for answering complex questions over knowledge bases (KBs) aims to decompose a question into a multi-step program, whose execution against the KB produces the final answer. Learning to induce programs relies on a large number of parallel question-program pairs for the given KB. However, for most KBs, the gold program annotations are usually lacking, making learning difficult. In this paper, we propose the approach of program transfer, which aims to leverage the valuable program annotations on the rich-resourced KBs as external supervision signals to aid program induction for the low-resourced KBs that lack program annotations. For program transfer, we design a novel two-stage parsing framework with an efficient ontology-guided pruning strategy. First, a sketch parser translates the question into a high-level program sketch, which is the composition of functions. Second, given the question and sketch, an argument parser searches the detailed arguments from the KB for functions. During the searching, we incorporate the KB ontology to prune the search space. The experiments on ComplexWebQuestions and WebQuestionSP show that our method outperforms SOTA methods significantly, demonstrating the effectiveness of program transfer and our framework. Our codes and datasets can be obtained from https://github.com/THU-KEG/ProgramTransfer.},
	language = {en-US},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Cao, Shulin and Shi, Jiaxin and Yao, Zijun and Lv, Xin and Yu, Jifan and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Xiao, Jinghui},
	month = mar,
	year = {2022},
	note = {arXiv:2110.05743 [cs]},
	keywords = {\#WQSP, /prior, Computer Science - Artificial Intelligence, unread},
}

@inproceedings{wang_no_2024,
	title = {No Need for Large-Scale Search: {Exploring} Large Language Models in Complex Knowledge Base Question Answering},
	booktitle = {{LREC}-{COLING}},
	author = {Wang, Shouhui and Qin, Biao},
	year = {2024},
	pages = {12288--12299}
}

@inproceedings{gu_arcaneqa_2022,
	title = {{ArcaneQA}: {Dynamic} Program Induction and Contextualized Encoding for Knowledge Base Question Answering},
	author = {Gu, Yu and Su, Yu},
        booktitle = {COLING},
	year = {2022},
        pages = {1718--1731}
}

@inproceedings{shu_tiara_2022,
	title = {{TIARA}: {Multi}-grained Retrieval for Robust Question Answering over Large Knowledge Base},
	booktitle = {EMNLP},
	author = {Shu, Yiheng and Yu, Zhiwei and Li, Yuhan and Karlsson, Börje and Ma, Tingting and Qu, Yuzhong and Lin, Chin-Yew},
	year = {2022},
	pages = {8108--8121}
}

@inproceedings{wu_scalable_2020,
	address = {Online},
	title = {Scalable {Zero}-shot {Entity} {Linking} with {Dense} {Entity} {Retrieval}},
	url = {https://aclanthology.org/2020.emnlp-main.519},
	doi = {10.18653/v1/2020.emnlp-main.519},
	abstract = {This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK.},
	language = {en-US},
	urldate = {2024-06-17},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {6397--6407},
}

@inproceedings{ye_rng-kbqa_2022,
	title = {{RnG}-{KBQA}: {Generation} Augmented Iterative Ranking for Knowledge Base Question Answering},
	author = {Ye, Xi and Yavuz, Semih and Hashimoto, Kazuma and Zhou, Yingbo and Xiong, Caiming},
	booktitle = {ACL},
	year = {2022},
    pages = {6032--6043}
}

@inproceedings{gu_dont_2023,
	title = {Don't Generate, Discriminate: {A} Proposal for Grounding Language Models to Real-World Environments},
	booktitle = {ACL},
	author = {Gu, Yu and Deng, Xiang and Su, Yu},
	year = {2023},
	pages = {4928--4949}
}

@inproceedings{yu_decaf_2023,
	title = {{DecAF}: {Joint} Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases},
	author = {Yu, Donghan and Zhang, Sheng and Ng, Patrick and Zhu, Henghui and Li, Alexander Hanbo and Wang, Jun and Hu, Yiqun and Wang, William and Wang, Zhiguo and Xiang, Bing},
        booktitle = {ICLR},
	year = {2023}
}

@inproceedings{zhang_fc-kbqa_2023,
	title = {{FC}-{KBQA}: {A} Fine-to-Coarse Composition Framework for Knowledge Base Question Answering},
	author = {Zhang, Lingxi and Zhang, Jing and Wang, Yanling and Cao, Shulin and Huang, Xinmei and Li, Cuiping and Chen, Hong and Li, Juanzi},
	booktitle = {ACL},
	year = {2023},
        pages = {1002--1017}
}

@inproceedings{yih_value_2016,
	title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
	booktitle = {ACL},
	author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
	year = {2016},
	pages = {201--206}
}

@misc{nie_graphq_2022,
	title = {{GraphQ} {IR}: {Unifying} the {Semantic} {Parsing} of {Graph} {Query} {Languages} with {One} {Intermediate} {Representation}},
	shorttitle = {{GraphQ} {IR}},
	url = {http://arxiv.org/abs/2205.12078},
	abstract = {Subject to the huge semantic gap between natural and formal languages, neural semantic parsing is typically bottlenecked by its complexity of dealing with both input semantics and output syntax. Recent works have proposed several forms of supplementary supervision but none is generalized across multiple formal languages. This paper proposes a unified intermediate representation (IR) for graph query languages, named GraphQ IR. It has a natural-language-like expression that bridges the semantic gap and formally defined syntax that maintains the graph structure. Therefore, a neural semantic parser can more precisely convert user queries into GraphQ IR, which can be later losslessly compiled into various downstream graph query languages. Extensive experiments on several benchmarks including KQA Pro, Overnight, GrailQA, and MetaQA-Cypher under standard i.i.d., out-of-distribution, and low-resource settings validate GraphQ IR's superiority over the previous state-of-the-arts with a maximum 11\% accuracy improvement.},
	language = {en-US},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {Nie, Lunyiu and Cao, Shulin and Shi, Jiaxin and Sun, Jiuding and Tian, Qi and Hou, Lei and Li, Juanzi and Zhai, Jidong},
	month = nov,
	year = {2022},
	note = {arXiv:2205.12078 [cs]},
	keywords = {\#KQAPro, /prior, Computer Science - Computation and Language, Computer Science - Databases},
}

@misc{liu_-context_2024,
	title = {An {In}-{Context} {Schema} {Understanding} {Method} for {Knowledge} {Base} {Question} {Answering}},
	url = {http://arxiv.org/abs/2310.14174},
	abstract = {The Knowledge Base Question Answering (KBQA) task aims to answer natural language questions based on a given knowledge base. Recently, Large Language Models (LLMs) have shown strong capabilities in language understanding and can be used to solve this task. In doing so, a major challenge for LLMs is to overcome the immensity and heterogeneity of knowledge base schemas.Existing methods bypass this challenge by initially employing LLMs to generate drafts of logic forms without schema-specific details.Then, an extra module is used to inject schema information to these drafts.In contrast, in this paper, we propose a simple In-Context Schema Understanding (ICSU) method that enables LLMs to directly understand schemas by leveraging in-context learning. Specifically, ICSU provides schema information to LLMs using schema-related annotated examples. We investigate three example retrieval strategies based on raw questions, anonymized questions, and generated SPARQL queries. Experimental results show that ICSU demonstrates competitive performance compared to baseline methods on both the KQA Pro and WebQSP datasets.},
	language = {en-US},
	urldate = {2024-03-13},
	publisher = {arXiv},
	author = {Liu, Yantao and Li, Zixuan and Jin, Xiaolong and Guo, Yucan and Bai, Long and Guan, Saiping and Guo, Jiafeng and Cheng, Xueqi},
	month = feb,
	year = {2024},
	note = {arXiv:2310.14174 [cs]},
	keywords = {\#KQAPro, \#WQSP, /unread, Computer Science - Computation and Language},
}

@inproceedings{faldu_retinaqa_2024,
	title = {{RetinaQA} : {A} Knowledge Base Question Answering Model Robust to both Answerable and Unanswerable Questions},
	author = {Faldu, Prayushi and Bhattacharya, Indrajit and Mausam},
        booktitle = {ACL},
	year = {2024},
        pages = {6643--6656}
}

@misc{tan_can_2023,
	title = {Can {ChatGPT} {Replace} {Traditional} {KBQA} {Models}? {An} {In}-depth {Analysis} of the {Question} {Answering} {Performance} of the {GPT} {LLM} {Family}},
	shorttitle = {Can {ChatGPT} {Replace} {Traditional} {KBQA} {Models}?},
	url = {http://arxiv.org/abs/2303.07992},
	abstract = {ChatGPT is a powerful large language model (LLM) that covers knowledge resources such as Wikipedia and supports natural language question answering using its own knowledge. Therefore, there is growing interest in exploring whether ChatGPT can replace traditional knowledge-based question answering (KBQA) models. Although there have been some works analyzing the question answering performance of ChatGPT, there is still a lack of large-scale, comprehensive testing of various types of complex questions to analyze the limitations of the model. In this paper, we present a framework that follows the black-box testing specifications of CheckList proposed by Ribeiro et. al. We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex question answering datasets, which include six English datasets and two multilingual datasets. The total number of test cases is approximately 190,000. In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5 to identify commonalities between the GPT family and other LLMs. The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git},
	language = {en-US},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Tan, Yiming and Min, Dehai and Li, Yu and Li, Wenbo and Hu, Nan and Chen, Yongrui and Qi, Guilin},
	month = sep,
	year = {2023},
	note = {arXiv:2303.07992 [cs]},
	keywords = {\#LQAD2, Computer Science - Computation and Language},
}

@inproceedings{talmor_web_2018,
	title = {The {Web} as a {Knowledge}-{Base} for {Answering} {Complex} {Questions}},
	booktitle = {NAACL-HLT},
	author = {Talmor, Alon and Berant, Jonathan},
	year = {2018},
	pages = {641--651}
}

@inproceedings{hong_knowledge--sql_2024,
	address = {Bangkok, Thailand and virtual meeting},
	title = {Knowledge-to-{SQL}: {Enhancing} {SQL} {Generation} with {Data} {Expert} {LLM}},
	shorttitle = {Knowledge-to-{SQL}},
	url = {https://aclanthology.org/2024.findings-acl.653},
	doi = {10.18653/v1/2024.findings-acl.653},
	abstract = {Generating accurate SQL queries for user questions (text-to-SQL) has been a long-standing challenge since it requires a deep understanding of both the user's question and the corresponding database schema in order to retrieve the desired content accurately. Existing methods rely on the comprehensive capability of large language models (LLMs) to generate the SQL. However, some necessary knowledge is not explicitly included in the database schema and user question or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient questions may be inaccurate, negatively influencing the text-to-SQL models' performance and robustness. To address this challenge, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all text-to-SQL models. Specifically, we introduce the detailed implementation of DELLM regarding table reading and the basic fine-tuning process. We further propose a Preference Learning via Database Feedback (PLDBF) strategy, refining the DELLM to generate more helpful knowledge for LLMs. Extensive experiments verify that DELLM can enhance the state-of-the-art approaches for text-to-SQL tasks. The corresponding code of DELLM is released for further research.},
	language = {en-US},
	urldate = {2024-09-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics} {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Hong, Zijin and Yuan, Zheng and Chen, Hao and Zhang, Qinggang and Huang, Feiran and Huang, Xiao},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {10997--11008},
}

@inproceedings{nie_code-style_2024,
	title = {Code-{Style} {In}-{Context} {Learning} for {Knowledge}-{Based} {Question} {Answering}},
	
	booktitle = {AAAI},
	author = {Nie, Zhijie and Zhang, Richong and Wang, Zhongyuan and Liu, Xudong},
	year = {2024},
	pages = {18833--18841}
}

@misc{rangel_sparql_2024,
	title = {{SPARQL} {Generation}: an analysis on fine-tuning {OpenLLaMA} for {Question} {Answering} over a {Life} {Science} {Knowledge} {Graph}},
	shorttitle = {{SPARQL} {Generation}},
	url = {http://arxiv.org/abs/2402.04627},
	abstract = {The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs. In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic "clues" in the queries, such as meaningful variable names and inline comments. Finally, we evaluate our approach over the real-world Bgee gene expression knowledge graph and we show that semantic clues can improve model performance by up to 33\% compared to a baseline with random variable names and no comments included.},
	language = {en-US},
	urldate = {2024-02-23},
	publisher = {arXiv},
	author = {Rangel, Julio C. and de Farias, Tarcisio Mendes and Sima, Ana Claudia and Kobayashi, Norio},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04627 [cs]},
	keywords = {\#KQAPro, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Information Retrieval},
}

@inproceedings{xu_fine-tuned_2023,
	title = {Fine-tuned {LLMs} {Know} {More}, {Hallucinate} {Less} with {Few}-{Shot} {Sequence}-to-{Sequence} {Semantic} {Parsing} over {Wikidata}},
	url = {https://openreview.net/forum?id=fEuslEGN0j},
	abstract = {While large language models (LLMs) can answer many questions correctly, they can also hallucinate and give wrong answers. Wikidata, with its over 12 billion facts, can be used to ground LLMs to improve their factuality. This paper presents WikiWebQuestions, a high-quality question answering benchmark for Wikidata. Ported over from WebQuestions for Freebase, it consists of real-world data with SPARQL annotation. This paper presents a few-shot sequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the unique domain and property names instead of their IDs. We train the parser to use either the results from an entity linker or mentions in the query. We fine-tune LLaMA by adding the few-shot training data to that used to fine-tune Alpaca. Our experimental results demonstrate the effectiveness of this methodology, establishing a strong baseline of 76\% and 65\% answer accuracy in the dev and test sets of WikiWebQuestions, respectively. By pairing our semantic parser with GPT-3, we combine verifiable results with qualified GPT-3 guesses to provide useful answers to 96\% of the questions in dev. We also show that our method outperforms the state-of-the-art for the QALD-7 Wikidata dataset by 3.6\% in F1 score.},
	language = {en},
	urldate = {2024-03-23},
	author = {Xu, Silei and Liu, Shicheng and Culhane, Theo and Pertseva, Elizaveta and Wu, Meng-Hsi and Semnani, Sina and Lam, Monica},
	month = dec,
	year = {2023},
	keywords = {/unread, unread},
}

@inproceedings{hudson_nsm_2019,
    author = {Hudson, Drew A. and Manning, Christopher D.},
    title =  {Learning by Abstraction: The Neural State Machine},
    booktitle = {NeurIPS},
    year = {2019},
    page = {5903--5916}
}


@inproceedings{hu_logical_2022,
	title = {Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases},
	booktitle = {COLING},
	author = {Hu, Xixin and Wu, Xuan and Shu, Yiheng and Qu, Yuzhong},
	year = {2022},
	pages = {1687--1696}
}

@misc{xiong_interactive-kbqa_2024,
	title = {Interactive-{KBQA}: {Multi}-{Turn} {Interactions} for {Knowledge} {Base} {Question} {Answering} with {Large} {Language} {Models}},
	shorttitle = {Interactive-{KBQA}},
	url = {http://arxiv.org/abs/2402.15131},
	abstract = {This study explores the realm of knowledge base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. However, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.},
	language = {en},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Xiong, Guanming and Bao, Junwei and Zhao, Wen},
	month = jul,
	year = {2024},
	note = {arXiv:2402.15131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7},
}

@inproceedings{shu_data_2024,
	title = {Distribution Shifts Are Bottlenecks: Extensive Evaluation for Grounding Language Models to Knowledge Bases},
	booktitle = {EACL},
	author = {Shu, Yiheng and Yu, Zhiwei},
	year = {2024},
	pages = {71--88}
}

@inproceedings{bollacker_freebase_2008,
	title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
	booktitle = {SIGMOD},
	author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
	year = {2008},
	pages = {1247--1250}
}



@article{raffel_exploring_2023,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
year = {2020},
volume = {21},
number = {1},
journal = {The Journal of Machine Learning Research},
pages = {5485--5551}
}

@misc{noauthor_association_nodate,
	title = {Association for {Computational} {Linguistics} ({ACL}) conference},
	url = {https://www.overleaf.com/project/66f3a51555247979b70f76fb},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2024-09-29},
}

@inproceedings{yih_value_2016-1,
	address = {Berlin, Germany},
	title = {The {Value} of {Semantic} {Parse} {Labeling} for {Knowledge} {Base} {Question} {Answering}},
	url = {https://aclanthology.org/P16-2033},
	doi = {10.18653/v1/P16-2033},
	urldate = {2024-09-29},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
	editor = {Erk, Katrin and Smith, Noah A.},
	month = aug,
	year = {2016},
	pages = {201--206},
}

@misc{chen_evaluating_2021,
	title = {Evaluating {Large} {Language} {Models} {Trained} on {Code}},
	url = {http://arxiv.org/abs/2107.03374},
	doi = {10.48550/arXiv.2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	language = {en-US},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03374 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{li_incremental_2021,
	title = {Incremental {Knowledge} {Based} {Question} {Answering}},
	url = {http://arxiv.org/abs/2101.06938},
	abstract = {In the past years, Knowledge-Based Question Answering (KBQA), which aims to answer natural language questions using facts in a knowledge base, has been well developed. Existing approaches often assume a static knowledge base. However, the knowledge is evolving over time in the real world. If we directly apply a fine-tuning strategy on an evolving knowledge base, it will suffer from a serious catastrophic forgetting problem. In this paper, we propose a new incremental KBQA learning framework that can progressively expand learning capacity as humans do. Specifically, it comprises a margin-distilled loss and a collaborative exemplar selection method, to overcome the catastrophic forgetting problem by taking advantage of knowledge distillation. We reorganize the SimpleQuestion dataset to evaluate the proposed incremental learning solution to KBQA. The comprehensive experiments demonstrate its effectiveness and efficiency when working with the evolving knowledge base.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Li, Yongqi and Li, Wenjie and Nie, Liqiang},
	month = jan,
	year = {2021},
	note = {arXiv:2101.06938 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_self-improvement_2024,
	title = {Self-{Improvement} {Programming} for {Temporal} {Knowledge} {Graph} {Question} {Answering}},
	url = {http://arxiv.org/abs/2404.01720},
	doi = {10.48550/arXiv.2404.01720},
	abstract = {Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric.},
	language = {en-US},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Chen, Zhuo and Zhang, Zhao and Li, Zixuan and Wang, Fei and Zeng, Yutao and Jin, Xiaolong and Xu, Yongjun},
	month = apr,
	year = {2024},
	note = {arXiv:2404.01720 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{du_contrastive_2023,
	title = {A contrastive framework for enhancing {Knowledge} {Graph} {Question} {Answering}: {Alleviating} exposure bias},
	volume = {280},
	issn = {09507051},
	shorttitle = {A contrastive framework for enhancing {Knowledge} {Graph} {Question} {Answering}},
	language = {en},
	urldate = {2024-03-13},
	journal = {Knowledge-Based Systems},
	author = {Du, Huifang and Zhang, Xixie and Wang, Meng and Chen, Yunwen and Ji, Daqi and Ma, Jun and Wang, Haofen},
	month = nov,
	year = {2023},
	keywords = {\#KQAPro},
	pages = {110996},
}


@article{touvron_llama_2023,
  title={{LLaMA}: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-09-29},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@incollection{wang_llm-based_2023,
	address = {Singapore},
	title = {{LLM}-{Based} {SPARQL} {Generation} with {Selected} {Schema} from {Large} {Scale} {Knowledge} {Base}},
	volume = {1923},
	isbn = {978-981-9972-23-4 978-981-9972-24-1},
	url = {https://link.springer.com/10.1007/978-981-99-7224-1_24},
	abstract = {Knowledge base question answering (KBQA) aims to answer natural language questions using structured knowledge bases. Common approaches include semantic parsing-based approaches and retrieval-based approaches. However, both approaches have some limitations. Retrieval-based methods struggle with complex reasoning requirements. Semantic parsing approaches have a complex reasoning process and cannot tolerate errors in earlier steps when generating the ﬁnal logical form. In this paper, we proposed a large language model (LLM)based SPARQL generation model, which accepts multiple candidate entities and relations as inputs, reducing the reliance on mention extraction and entity linking performance, and we found an entity combination strategy based on mentions, which can produce multiple SPARQL queries for a single question to boost the chances of ﬁnding the correct answer. Finally, our model achieves state-of-the-art performance in the CCKS2023 CKBQA competition, F1 score is 75.63\%.},
	language = {en},
	urldate = {2024-02-18},
	booktitle = {Knowledge {Graph} and {Semantic} {Computing}: {Knowledge} {Graph} {Empowers} {Artificial} {General} {Intelligence}},
	publisher = {Springer Nature Singapore},
	author = {Yang, Shuangtao and Teng, Mao and Dong, Xiaozheng and Bo, Fu},
	editor = {Wang, Haofen and Han, Xianpei and Liu, Ming and Cheng, Gong and Liu, Yongbin and Zhang, Ningyu},
	year = {2023},
	doi = {10.1007/978-981-99-7224-1_24},
	note = {Series Title: Communications in Computer and Information Science},
	keywords = {\#KQAPro},
	pages = {304--316},
}

@misc{noauthor_few-shot_nodate,
	title = {Few-shot {Logical} {Form} {Generation} for {KGQA} via {Pseudo} {Label} {Pre}-training and {Knowledge} {Distillation}},
}

@misc{patidar_i_2023,
	title = {Do {I} have the {Knowledge} to {Answer}? {Investigating} {Answerability} of {Knowledge} {Base} {Questions}},
	shorttitle = {Do {I} have the {Knowledge} to {Answer}?},
	url = {http://arxiv.org/abs/2212.10189},
	abstract = {When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability.},
	language = {en},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Patidar, Mayur and Faldu, Prayushi and Singh, Avinash and Vig, Lovekesh and Bhattacharya, Indrajit and Mausam},
	month = jun,
	year = {2023},
	note = {arXiv:2212.10189 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{gong_diffuseq_2023,
	title = {{DiffuSeq}: {Sequence} to {Sequence} {Text} {Generation} with {Diffusion} {Models}},
	shorttitle = {{DiffuSeq}},
	url = {http://arxiv.org/abs/2210.08933},
	abstract = {Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at {\textbackslash}url\{https://github.com/Shark-NLP/DiffuSeq\}},
	language = {en},
	urldate = {2024-08-21},
	publisher = {arXiv},
	author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},
	month = feb,
	year = {2023},
	note = {arXiv:2210.08933 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zou_chinese_2021,
	title = {A {Chinese} {Multi}-type {Complex} {Questions} {Answering} {Dataset} over {Wikidata}},
	url = {http://arxiv.org/abs/2111.06086},
	doi = {10.48550/arXiv.2111.06086},
	abstract = {Complex Knowledge Base Question Answering is a popular area of research in the past decade. Recent public datasets have led to encouraging results in this field, but are mostly limited to English and only involve a small number of question types and relations, hindering research in more realistic settings and in languages other than English. In addition, few state-of-the-art KBQA models are trained on Wikidata, one of the most popular real-world knowledge bases. We propose CLC-QuAD, the first large scale complex Chinese semantic parsing dataset over Wikidata to address these challenges. Together with the dataset, we present a text-to-SPARQL baseline model, which can effectively answer multi-type complex questions, such as factual questions, dual intent questions, boolean questions, and counting questions, with Wikidata as the background knowledge. We finally analyze the performance of SOTA KBQA models on this dataset and identify the challenges facing Chinese KBQA.},
	language = {en-US},
	urldate = {2024-02-28},
	publisher = {arXiv},
	author = {Zou, Jianyun and Yang, Min and Zhang, Lichao and Xu, Yechen and Pan, Qifan and Jiang, Fengqing and Qin, Ran and Wang, Shushu and He, Yifan and Huang, Songfang and Zhao, Zhou},
	month = nov,
	year = {2021},
	note = {arXiv:2111.06086 [cs]},
	keywords = {\#LQAD2, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases},
}

@article{wu_ar-diffusion_2023,
	title = {{AR}-{Diffusion}: {Auto}-{Regressive} {Diffusion} {Model} for {Text} {Generation}},
	volume = {36},
	shorttitle = {{AR}-{Diffusion}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/7d866abba506e5a56335e4644ebe18f9-Abstract-Conference.html},
	language = {en},
	urldate = {2024-08-21},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wu, Tong and Fan, Zhihao and Liu, Xiao and Zheng, Hai-Tao and Gong, Yeyun and Shen, Yelong and Jiao, Jian and Li, Juntao and Wei, Zhongyu and Guo, Jian and Duan, Nan and Chen, Weizhu},
	month = dec,
	year = {2023},
	pages = {39957--39974},
}

@inproceedings{ayoola_refined_2022,
	address = {Hybrid: Seattle, Washington + Online},
	title = {{ReFinED}: {An} {Efficient} {Zero}-shot-capable {Approach} to {End}-to-{End} {Entity} {Linking}},
	shorttitle = {{ReFinED}},
	url = {https://aclanthology.org/2022.naacl-industry.24},
	doi = {10.18653/v1/2022.naacl-industry.24},
	abstract = {We introduce ReFinED, an efficient end-to-end entity linking model which uses fine-grained entity types and entity descriptions to perform linking. The model performs mention detection, fine-grained entity typing, and entity disambiguation for all mentions within a document in a single forward pass, making it more than 60 times faster than competitive existing approaches. ReFinED also surpasses state-of-the-art performance on standard entity linking datasets by an average of 3.7 F1. The model is capable of generalising to large-scale knowledge bases such as Wikidata (which has 15 times more entities than Wikipedia) and of zero-shot entity linking. The combination of speed, accuracy and scale makes ReFinED an effective and cost-efficient system for extracting entities from web-scale datasets, for which the model has been successfully deployed.},
	urldate = {2024-08-16},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Track}},
	publisher = {Association for Computational Linguistics},
	author = {Ayoola, Tom and Tyagi, Shubhi and Fisher, Joseph and Christodoulopoulos, Christos and Pierleoni, Andrea},
	editor = {Loukina, Anastassia and Gangadharaiah, Rashmi and Min, Bonan},
	month = jul,
	year = {2022},
	pages = {209--220},
}

@misc{noauthor_addon_nodate,
	title = {Addon {Item}},
}

@inproceedings{lan_survey_2021,
	title = {A Survey on Complex Knowledge Base Question Answering: {Methods}, Challenges and Solutions},
	author = {Lan, Yunshi and He, Gaole and Jiang, Jinhao and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
        booktitle = {IJCAI},
	year = {2021},
        pages = {4483--4491}
}

@inproceedings{lukovnikov_pretrained_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pretrained {Transformers} for {Simple} {Question} {Answering} over {Knowledge} {Graphs}},
	isbn = {978-3-030-30793-6},
	doi = {10.1007/978-3-030-30793-6_27},
	abstract = {Answering simple questions over knowledge graphs is a well-studied problem in question answering. Previous approaches for this task built on recurrent and convolutional neural network based architectures that use pretrained word embeddings. It was recently shown that finetuning pretrained transformer networks (e.g. BERT) can outperform previous approaches on various natural language processing tasks. In this work, we investigate how well BERT performs on SimpleQuestions and provide an evaluation of both BERT and BiLSTM-based models in limited-data scenarios.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2019},
	publisher = {Springer International Publishing},
	author = {Lukovnikov, Denis and Fischer, Asja and Lehmann, Jens},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	year = {2019},
	keywords = {\#SQ},
	pages = {470--486},
}

@article{zhang_survey_2023,
	title = {A survey on complex factual question answering},
	volume = {4},
	issn = {26666510},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666651022000249},
	doi = {10.1016/j.aiopen.2022.12.003},
	abstract = {Answering complex factual questions has drawn a lot of attention. Researchers leverage various data sources to support complex QA, such as unstructured texts, structured knowledge graphs and relational databases, semi-structured web tables, or even hybrid data sources. However, although the ideas behind these approaches show similarity to some extent, there is not yet a consistent strategy to deal with various data sources. In this survey, we carefully examine how complex factual question answering has evolved across various data sources. We list the similarities among these approaches and group them into the analysis–extend–reason framework, despite the various question types and data sources that they focus on. We also address future directions for difficult factual question answering as well as the relevant benchmarks.},
	language = {en},
	urldate = {2024-04-01},
	journal = {AI Open},
	author = {Zhang, Lingxi and Zhang, Jing and Ke, Xirui and Li, Haoyang and Huang, Xinmei and Shao, Zhonghui and Cao, Shulin and Lv, Xin},
	year = {2023},
	keywords = {\#Survey},
	pages = {1--12},
}

@article{pan_unifying_2024,
	title = {Unifying {Large} {Language} {Models} and {Knowledge} {Graphs}: {A} {Roadmap}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Unifying {Large} {Language} {Models} and {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/2306.08302},
	doi = {10.1109/TKDE.2024.3352100},
	abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
	language = {en-US},
	urldate = {2024-02-02},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
	year = {2024},
	note = {arXiv:2306.08302 [cs]},
	keywords = {\#Survey, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {1--20},
}

@article{wang_novel_2024,
	title = {A {Novel} {Joint} {Training} {Model} for {Knowledge} {Base} {Question} {Answering}},
	volume = {32},
	issn = {2329-9290, 2329-9304},
	url = {https://ieeexplore.ieee.org/document/10328715/},
	doi = {10.1109/TASLP.2023.3336526},
	abstract = {In knowledge base question answering (KBQA) systems, relation detection and entity recognition are two core components. However, since the relation detection in KBQA contains thousands of relations and this task always becomes a zero-shot learning task due to the relations in some test samples while they have not appeared in training data, relation detection is more difﬁcult than entity recognition. In addition, previous studies only considered these two tasks separately and did not take full advantage of their correlation. This article proposes a novel relation and entity joint extraction framework, named Gated-Attention-based Joint Training Model (Ga-JTM), to integrate relation detection and entity recognition. In addition, to train the two models simultaneously, a knowledge-driven gated unit based on a multihead attention mechanism is designed. It combines the knowledge graph embeddings and the current context semantic information to process relation detection and entity recognition tasks, respectively. The experiments are conducted on a single-relation dataset (SimpleQuestions) and a multiple-relation dataset (WebQSP), and the experimental results demonstrate that our Ga-JTM is superior to the state-of-the-art (SOTA) performance of relation detection and can improve the performance of entity recognition and entity linking. Finally, these improvements contribute to the SOTA performance in our KBQA system.},
	language = {en},
	urldate = {2024-03-12},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Wang, Shouhui and Qin, Biao},
	year = {2024},
	keywords = {\#SQ, \#WQSP},
	pages = {666--679},
}

@misc{jiang_structgpt_2023,
	title = {{StructGPT}: {A} {General} {Framework} for {Large} {Language} {Model} to {Reason} over {Structured} {Data}},
	shorttitle = {{StructGPT}},
	url = {http://arxiv.org/abs/2305.09645},
	abstract = {In this paper, we study how to improve the zero-shot reasoning ability of large language models{\textasciitilde}(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an {\textbackslash}emph\{Iterative Reading-then-Reasoning{\textasciitilde}(IRR)\} approach for solving question answering tasks based on structured data, called {\textbackslash}textbf\{StructGPT\}. In our approach, we construct the specialized function to collect relevant evidence from structured data ({\textbackslash}ie {\textbackslash}emph\{reading\}), and let LLMs concentrate the reasoning task based on the collected information ({\textbackslash}ie {\textbackslash}emph\{reasoning\}). Specially, we propose an {\textbackslash}emph\{invoking-linearization-generation\} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/RUCAIBox/StructGPT\}.},
	language = {en-US},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
	month = oct,
	year = {2023},
	note = {arXiv:2305.09645 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{rony_sgpt_2022,
	title = {{SGPT}: {A} {Generative} {Approach} for {SPARQL} {Query} {Generation} {From} {Natural} {Language} {Questions}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {{SGPT}},
	url = {https://ieeexplore.ieee.org/document/9815253/},
	doi = {10.1109/ACCESS.2022.3188714},
	abstract = {SPARQL query generation from natural language questions is complex because it requires an understanding of both the question and underlying knowledge graph (KG) patterns. Most SPARQL query generation approaches are template-based, tailored to a speciﬁc knowledge graph and require pipelines with multiple steps, including entity and relation linking. Template-based approaches are also difﬁcult to adapt for new KGs and require manual efforts from domain experts to construct query templates. To overcome this hurdle, we propose a new approach, dubbed SGPT, that combines the beneﬁts of end-to-end and modular systems and leverages recent advances in large-scale language models. Speciﬁcally, we devise a novel embedding technique that can encode linguistic features from the question which enables the system to learn complex question patterns. In addition, we propose training techniques that allow the system to implicitly employ the graph-speciﬁc information (i.e., entities and relations) into the language model’s parameters and generate SPARQL queries accurately. Finally, we introduce a strategy to adapt standard automatic metrics for evaluating SPARQL query generation. A comprehensive evaluation demonstrates the effectiveness of SGPT over state-of-the-art methods across several benchmark datasets.},
	language = {en},
	urldate = {2024-03-01},
	journal = {IEEE Access},
	author = {Rony, Md Rashad Al Hasan and Kumar, Uttam and Teucher, Roman and Kovriguina, Liubov and Lehmann, Jens},
	year = {2022},
	note = {rate: 4},
	keywords = {\#LQAD2},
	pages = {70712--70723},
}

@inproceedings{li_efficient_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {Question} {Answering} {Based} on {Language} {Models} and {Knowledge} {Graphs}},
	isbn = {978-3-031-44216-2},
	doi = {10.1007/978-3-031-44216-2_28},
	abstract = {Knowledge graph question answering (Q \&A) aims to answer questions through a knowledge base (KB). When using a knowledge base as a data source for multihop Q \&A, knowledge graph Q \&A needs to obtain relevant entities, their relationships and the correct answer, but often the correct answer cannot be obtained through the reasoning path because of absent relationships. Currently, using pre-trained language models (PLM) and knowledge graphs (KG) has a good effect on complex problems. However, challenging problems remain; the relationships between problems and candidate entities need to be better represented, and joint reasoning must be performed in the relationship graph based on problems and entities. To solve these problems, we expand the relational graph by adding tail entities to the list of preselected entities through reverse relations and then add the processed problems and entities to the problem subgraph. To perform inference on a relational graph, we design an attention-based neural network module. To calculate the loss of the model’s inference process nodes, we use a modified Euclidean distance function as the loss function. To evaluate our model, we conducted experiments on the WebQSP and CWQ datasets, and the model obtained state-of-the-art results in both the KB-full and KB-half settings.},
	language = {en},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Li, Fengying and Huang, Hongfei and Dong, Rongsheng},
	editor = {Iliadis, Lazaros and Papaleonidas, Antonios and Angelov, Plamen and Jayne, Chrisina},
	year = {2023},
	keywords = {\#CWQ, \#WQSP, /unread, Knowledge graphs, Neural networks, Question answering},
	pages = {340--351},
}

@inproceedings{yu_compleqa_2023,
	address = {Singapore},
	title = {{CompleQA}: {Benchmarking} the {Impacts} of {Knowledge} {Graph} {Completion} {Methods} on {Question} {Answering}},
	shorttitle = {{CompleQA}},
	url = {https://aclanthology.org/2023.findings-emnlp.849},
	doi = {10.18653/v1/2023.findings-emnlp.849},
	abstract = {How much success in Knowledge Graph Completion (KGC) would translate into the performance enhancement in downstream tasks is an important question that has not been studied in depth. In this paper, we introduce a novel benchmark, namely CompleQA, to comprehensively assess the influence of representative KGC methods on Knowledge Graph Question Answering (KGQA), one of the most important downstream applications. This benchmark includes a knowledge graph with 3 million triplets across 5 distinct domains, coupled with over 5000 question-answering pairs and a completion dataset that is well-aligned with these questions. Our evaluation of four well-known KGC methods in combination with two state-of-the-art KGQA systems shows that effective KGC can significantly mitigate the impact of knowledge graph incompleteness on question-answering performance. Surprisingly, we also find that the best-performing KGC method(s) does not necessarily lead to the best QA results, underscoring the need to consider downstream applications when doing KGC.},
	language = {en-US},
	urldate = {2024-03-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Yu, Donghan and Gu, Yu and Xiong, Chenyan and Yang, Yiming},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {/unread},
	pages = {12748--12755},
}

@article{huang_question_2023,
	title = {Question {Decomposition} {Tree} for {Answering} {Complex} {Questions} over {Knowledge} {Bases}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26519},
	doi = {10.1609/aaai.v37i11.26519},
	abstract = {Knowledge base question answering (KBQA) has attracted a lot of interest in recent years, especially for complex questions which require multiple facts to answer. Question decomposition is a promising way to answer complex questions. Existing decomposition methods split the question into sub-questions according to a single compositionality type, which is not sufficient for questions involving multiple compositionality types. In this paper, we propose Question Decomposition Tree (QDT) to represent the structure of complex questions. Inspired by recent advances in natural language generation (NLG), we present a two-staged method called Clue-Decipher to generate QDT. It can leverage the strong ability of NLG model and simultaneously preserve the original questions. To verify that QDT can enhance KBQA task, we design a decomposition-based KBQA system called QDTQA. Extensive experiments show that QDTQA outperforms previous state-of-the-art methods on ComplexWebQuestions dataset. Besides, our decomposition method improves an existing KBQA system by 12\% and sets a new state-of-the-art on LC-QuAD 1.0.},
	language = {en},
	number = {11},
	urldate = {2024-04-12},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Huang, Xiang and Cheng, Sitao and Shu, Yiheng and Bao, Yuheng and Qu, Yuzhong},
	month = jun,
	year = {2023},
	note = {Number: 11},
	keywords = {Chunking \& Parsing, SNLP: Syntax -- Tagging},
	pages = {12924--12932},
}

@misc{diallo_comprehensive_2024,
	title = {A {Comprehensive} {Evaluation} of {Neural} {SPARQL} {Query} {Generation} from {Natural} {Language} {Questions}},
	url = {http://arxiv.org/abs/2304.07772},
	doi = {10.48550/arXiv.2304.07772},
	abstract = {In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed significant growth. Incorporating the copy mechanism with traditional encoder-decoder architectures and using pre-trained encoder-decoders and large language models have set new performance benchmarks. This paper presents various experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained language models (PLMs), non-pre-trained language models (NPLMs), and large language models (LLMs), highlighting the impact of question annotation and the copy mechanism and testing various fine-tuning methods using LLMs. In particular, we provide a systematic error analysis of the models and test their generalization ability. Our study demonstrates that the copy mechanism yields significant performance enhancements for most PLMs and NPLMs. Annotating the data is pivotal to generating correct URIs, with the "tag-within" strategy emerging as the most effective approach. Additionally, our findings reveal that the primary source of errors stems from incorrect URIs in SPARQL queries that are sometimes replaced with hallucinated URIs when using base models. This does not happen using the copy mechanism, but it sometimes leads to selecting wrong URIs among candidates. Finally, the performance of the tested LLMs fell short of achieving the desired outcomes.},
	language = {en-US},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Diallo, Papa Abdou Karim Karou and Reyd, Samuel and Zouaq, Amal},
	month = jan,
	year = {2024},
	note = {arXiv:2304.07772 [cs]},
	keywords = {\#LQAD1, \#LQAD2, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{do_compact_2019,
	title = {Compact {Trilinear} {Interaction} for {Visual} {Question} {Answering}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Do_Compact_Trilinear_Interaction_for_Visual_Question_Answering_ICCV_2019_paper.html},
	urldate = {2024-04-25},
	author = {Do, Tuong and Do, Thanh-Toan and Tran, Huy and Tjiputra, Erman and Tran, Quang D.},
	year = {2019},
	pages = {392--401},
}

@misc{dai_counter-intuitive_2024,
	title = {Counter-intuitive: {Large} {Language} {Models} {Can} {Better} {Understand} {Knowledge} {Graphs} {Than} {We} {Thought}},
	shorttitle = {Counter-intuitive},
	url = {http://arxiv.org/abs/2402.11541},
	abstract = {Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension of KG. Contrary to our initial expectations, our analysis revealed that LLMs effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language (NL) textual prompts. This counter-intuitive finding provides substantial insights for future research on LLMs' comprehension of structured knowledge.},
	language = {en-US},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Dai, Xinbang and Hua, Yuncheng and Wu, Tongtong and Sheng, Yang and Qi, Guilin},
	month = feb,
	year = {2024},
	note = {arXiv:2402.11541 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.4, I.2.7},
}

@misc{hirigoyen_copy_2022,
	title = {A {Copy} {Mechanism} for {Handling} {Knowledge} {Base} {Elements} in {SPARQL} {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/2211.10271},
	doi = {10.48550/arXiv.2211.10271},
	abstract = {Neural Machine Translation (NMT) models from English to SPARQL are a promising development for SPARQL query generation. However, current architectures are unable to integrate the knowledge base (KB) schema and handle questions on knowledge resources, classes, and properties unseen during training, rendering them unusable outside the scope of topics covered in the training set. Inspired by the performance gains in natural language processing tasks, we propose to integrate a copy mechanism for neural SPARQL query generation as a way to tackle this issue. We illustrate our proposal by adding a copy layer and a dynamic knowledge base vocabulary to two Seq2Seq architectures (CNNs and Transformers). This layer makes the models copy KB elements directly from the questions, instead of generating them. We evaluate our approach on state-of-the-art datasets, including datasets referencing unknown KB elements and measure the accuracy of the copy-augmented architectures. Our results show a considerable increase in performance on all datasets compared to non-copy architectures.},
	language = {en-US},
	urldate = {2024-03-09},
	publisher = {arXiv},
	author = {Hirigoyen, Rose and Zouaq, Amal and Reyd, Samuel},
	month = nov,
	year = {2022},
	note = {arXiv:2211.10271 [cs]},
	keywords = {\#LQAD1, \#LQAD2, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{gou_knowledge_2021,
	title = {Knowledge {Distillation}: {A} {Survey}},
	volume = {129},
	issn = {1573-1405},
	shorttitle = {Knowledge {Distillation}},
	url = {https://doi.org/10.1007/s11263-021-01453-z},
	doi = {10.1007/s11263-021-01453-z},
	abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher–student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
	language = {en},
	number = {6},
	urldate = {2024-04-25},
	journal = {International Journal of Computer Vision},
	author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J. and Tao, Dacheng},
	month = jun,
	year = {2021},
	keywords = {Deep neural networks, Knowledge distillation, Knowledge transfer, Model compression, Teacher–student architecture},
	pages = {1789--1819},
}

@article{mitchell_ned_2018,
author = {Mitchell, T. and Cohen, W. and Hruschka, E. and Talukdar, P. and Yang, B. and Betteridge, J. and Carlson, A. and Dalvi, B. and Gardner, M. and Kisiel, B. and Krishnamurthy, J. and Lao, N. and Mazaitis, K. and Mohamed, T. and Nakashole, N. and Platanios, E. and Ritter, A. and Samadi, M. and Settles, B. and Wang, R. and Wijaya, D. and Gupta, A. and Chen, X. and Saparov, A. and Greaves, M. and Welling, J.},
title = {Never-ending learning},
year = {2018},
volume = {61},
number = {5},
journal = {Communications of the ACM},
pages = {103--115},
}

@inproceedings{lan_topic_unit_2019,
  author = {Lan, Yunshi and Wang, Shuohang and Jiang, Jing},
  title = {Knowledge {{Base Question Answering}} with {{Topic Units}}},
  booktitle = {IJCAI},
  pages = {5046--5052}
}

@article{guo_deepseek_2025,
  title={{DeepSeek-R1}: Incentivizing reasoning capability in {LLMs} via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}


@inproceedings{he_improving_2021,
	title = {Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals},
	booktitle = {WSDM},
	author = {He, Gaole and Lan, Yunshi and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
	year = {2021},
	pages = {553--561}
}

@article{liang_querying_2021,
	title = {Querying Knowledge Graphs in Natural Language},
	volume = {8},
	number = {1},
	journal = {Journal of Big Data},
	author = {Liang, Shiqi and Stockinger, Kurt and De Farias, Tarcisio Mendes and Anisimova, Maria and Gil, Manuel},
	year = {2021},
	pages = {3:1--3:23},
}

@inproceedings{huang_unseen_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Unseen {Entity} {Handling} in {Complex} {Question} {Answering} over {Knowledge} {Base} via {Language} {Generation}},
	url = {https://aclanthology.org/2021.findings-emnlp.50},
	doi = {10.18653/v1/2021.findings-emnlp.50},
	abstract = {Complex question answering over knowledge base remains as a challenging task because it involves reasoning over multiple pieces of information, including intermediate entities/relations and other constraints. Previous methods simplify the SPARQL query of a question into such forms as a list or a graph, missing such constraints as “filter” and “order\_by”, and present models specialized for generating those simplified forms from a given question. We instead introduce a novel approach that directly generates an executable SPARQL query without simplification, addressing the issue of generating unseen entities. We adapt large scale pre-trained encoder-decoder models and show that our method significantly outperforms the previous methods and also that our method has higher interpretability and computational efficiency than the previous methods.},
	language = {en-US},
	urldate = {2024-03-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Xin and Kim, Jung-Jae and Zou, Bowei},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	keywords = {/unread},
	pages = {547--557},
}

@article{hogan_knowledge_2022,
	title = {Knowledge {Graphs}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3447772},
	doi = {10.1145/3447772},
	abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
	language = {en},
	number = {4},
	urldate = {2024-04-20},
	journal = {ACM Computing Surveys},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D’amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, José Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = may,
	year = {2022},
	pages = {1--37},
}

@incollection{fensel_introduction_2020,
	address = {Cham},
	title = {Introduction: {What} {Is} a {Knowledge} {Graph}?},
	isbn = {978-3-030-37439-6},
	shorttitle = {Introduction},
	url = {https://doi.org/10.1007/978-3-030-37439-6_1},
	abstract = {Since its inception by Google, Knowledge Graph has become a term that is recently ubiquitously used yet does not have a well-established definition. This section attempts to derive a definition for Knowledge Graphs by compiling existing definitions made in the literature and considering the distinctive characteristics of previous efforts for tackling the data integrationData integration challenge we are facing today. Our attempt to make a conceptual definition is complemented with an empirical survey of existing Knowledge Graphs. This section lays the foundation for the remainder of the book, as it provides a common understanding on certain concepts and motivation to build Knowledge Graphs in the first place.},
	language = {en},
	urldate = {2024-04-21},
	booktitle = {Knowledge {Graphs}: {Methodology}, {Tools} and {Selected} {Use} {Cases}},
	publisher = {Springer International Publishing},
	author = {Fensel, Dieter and Şimşek, Umutcan and Angele, Kevin and Huaman, Elwin and Kärle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, Jürgen and Wahler, Alexander},
	editor = {Fensel, Dieter and Şimşek, Umutcan and Angele, Kevin and Huaman, Elwin and Kärle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, Jürgen and Wahler, Alexander},
	year = {2020},
	doi = {10.1007/978-3-030-37439-6_1},
	pages = {1--10},
}

@article{wang_richpedia_2020,
	title = {Richpedia: {A} {Large}-{Scale}, {Comprehensive} {Multi}-{Modal} {Knowledge} {Graph}},
	volume = {22},
	issn = {2214-5796},
	url = {https://www.sciencedirect.com/science/article/pii/S2214579620300277},
	doi = {https://doi.org/10.1016/j.bdr.2020.100159},
	abstract = {Large-scale knowledge graphs such as Wikidata and DBpedia have become a powerful asset for semantic search and question answering. However, most of the knowledge graph construction works focus on organizing and discovering textual knowledge in a structured representation, while paying little attention to the proliferation of visual resources on the Web. To consolidate this recent trend, in this paper, we present Richpedia, aiming to provide a comprehensive multi-modal knowledge graph by distributing sufficient and diverse images to textual entities in Wikidata. We also set Resource Description Framework links (visual semantic relations) between image entities based on the hyperlinks and descriptions in Wikipedia. The Richpedia resource is accessible on the Web via a faceted query endpoint, which provides a pathway for knowledge graph and computer vision tasks, such as link prediction and visual relation detection.},
	journal = {Big Data Research},
	author = {Wang, Meng and Wang, Haofen and Qi, Guilin and Zheng, Qiushuo},
	year = {2020},
	keywords = {Knowledge graph, Multi-modal, Ontology, Wikidata},
	pages = {100159},
}

@inproceedings{reyd_assessing_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Assessing the {Generalization} {Capabilities} of {Neural} {Machine} {Translation} {Models} for {SPARQL} {Query} {Generation}},
	isbn = {978-3-031-47240-4},
	doi = {10.1007/978-3-031-47240-4_26},
	abstract = {Recent studies in the field of Neural Machine Translation for SPARQL query generation have shown rapidly rising performance. State-of-the-art models have reached almost perfect query generation for simple datasets. However, such progress raises the question of the ability of these models to generalize and deal with unseen question-query structures and entities. In this work, we propose copy-enhanced pre-trained models with question annotation and test the ability of several models to handle unknown question-query structures and URIs. To do so, we split two popular datasets based on unknown URIs and question-query structures. Our results show that the copy mechanism effectively allows non-pre-trained models to deal with unknown URIs, and that it also improves the results of some pre-trained models. Our results also show that, when exposed to unknown question-query structures on a simple dataset, pre-trained models significantly outperform non-pre-trained models, but both non-pre-trained and pre-trained models have a considerable drop in performance on a harder dataset. However, the copy mechanism significantly boosts the results of non-pre-trained models on all settings and of the BART pre-trained model, except for the template split on LC-QuAD 2.0 dataset.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Reyd, Samuel and Zouaq, Amal},
	editor = {Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villalón, María and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi},
	year = {2023},
	keywords = {\#LQAD1, \#LQAD2, Generalization, Out-of-vocabulary problem, SPARQL query generation, Unknown URIs},
	pages = {484--501},
}

@misc{agarwal_-policy_2024,
	title = {On-{Policy} {Distillation} of {Language} {Models}: {Learning} from {Self}-{Generated} {Mistakes}},
	shorttitle = {On-{Policy} {Distillation} of {Language} {Models}},
	url = {http://arxiv.org/abs/2306.13649},
	language = {en-US},
	urldate = {2024-03-25},
	publisher = {arXiv},
	author = {Agarwal, Rishabh and Vieillard, Nino and Zhou, Yongchao and Stanczyk, Piotr and Ramos, Sabela and Geist, Matthieu and Bachem, Olivier},
	month = jan,
	year = {2024},
	note = {arXiv:2306.13649 [cs]},
	keywords = {/prior, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kim_sequence-level_2016,
	title = {Sequence-{Level} {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/1606.07947},
	doi = {10.48550/arXiv.1606.07947},
	abstract = {Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search. Applying weight pruning on top of knowledge distillation results in a student model that has 13 times fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.},
	language = {en-US},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Kim, Yoon and Rush, Alexander M.},
	month = sep,
	year = {2016},
	note = {arXiv:1606.07947 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{zhang_survey_2023-1,
	title = {A survey on complex factual question answering},
	volume = {4},
	issn = {2666-6510},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651022000249},
	doi = {10.1016/j.aiopen.2022.12.003},
	abstract = {Answering complex factual questions has drawn a lot of attention. Researchers leverage various data sources to support complex QA, such as unstructured texts, structured knowledge graphs and relational databases, semi-structured web tables, or even hybrid data sources. However, although the ideas behind these approaches show similarity to some extent, there is not yet a consistent strategy to deal with various data sources. In this survey, we carefully examine how complex factual question answering has evolved across various data sources. We list the similarities among these approaches and group them into the analysis–extend–reason framework, despite the various question types and data sources that they focus on. We also address future directions for difficult factual question answering as well as the relevant benchmarks.},
	urldate = {2024-04-01},
	journal = {AI Open},
	author = {Zhang, Lingxi and Zhang, Jing and Ke, Xirui and Li, Haoyang and Huang, Xinmei and Shao, Zhonghui and Cao, Shulin and Lv, Xin},
	month = jan,
	year = {2023},
	keywords = {Complex question, Document-based question answering, Factual question, Knowledge base question answering, Multi-source question answering, Question answering, Table question answering, Text2SQL},
	pages = {1--12},
}

@misc{yang_knowledge_2018,
	title = {Knowledge {Distillation} in {Generations}: {More} {Tolerant} {Teachers} {Educate} {Better} {Students}},
	shorttitle = {Knowledge {Distillation} in {Generations}},
	url = {http://arxiv.org/abs/1805.05551},
	doi = {10.48550/arXiv.1805.05551},
	abstract = {We focus on the problem of training a deep neural network in generations. The flowchart is that, in order to optimize the target network (student), another network (teacher) with the same architecture is first trained, and used to provide part of supervision signals in the next stage. While this strategy leads to a higher accuracy, many aspects (e.g., why teacher-student optimization helps) still need further explorations. This paper studies this problem from a perspective of controlling the strictness in training the teacher network. Existing approaches mostly used a hard distribution (e.g., one-hot vectors) in training, leading to a strict teacher which itself has a high accuracy, but we argue that the teacher needs to be more tolerant, although this often implies a lower accuracy. The implementation is very easy, with merely an extra loss term added to the teacher network, facilitating a few secondary classes to emerge and complement to the primary class. Consequently, the teacher provides a milder supervision signal (a less peaked distribution), and makes it possible for the student to learn from inter-class similarity and potentially lower the risk of over-fitting. Experiments are performed on standard image classification tasks (CIFAR100 and ILSVRC2012). Although the teacher network behaves less powerful, the students show a persistent ability growth and eventually achieve higher classification accuracies than other competitors. Model ensemble and transfer feature extraction also verify the effectiveness of our approach.},
	language = {en-US},
	urldate = {2024-03-27},
	publisher = {arXiv},
	author = {Yang, Chenglin and Xie, Lingxi and Qiao, Siyuan and Yuille, Alan},
	month = sep,
	year = {2018},
	note = {arXiv:1805.05551 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{lin_autoregressive_2020,
	title = {Autoregressive {Knowledge} {Distillation} through {Imitation} {Learning}},
	url = {http://arxiv.org/abs/2009.07253},
	doi = {10.48550/arXiv.2009.07253},
	abstract = {The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Lin, Alexander and Wohlwend, Jeremy and Chen, Howard and Lei, Tao},
	month = oct,
	year = {2020},
	note = {arXiv:2009.07253 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@incollection{ghidini_lc-quad_2019,
	address = {Cham},
	title = {{LC}-{QuAD} 2.0: {A} {Large} {Dataset} for {Complex} {Question} {Answering} over {Wikidata} and {DBpedia}},
	volume = {11779},
	isbn = {978-3-030-30795-0 978-3-030-30796-7},
	shorttitle = {{LC}-{QuAD} 2.0},
	url = {https://link.springer.com/10.1007/978-3-030-30796-7_5},
	abstract = {Providing machines with the capability of exploring knowledge graphs and answering natural language questions has been an active area of research over the past decade. In this direction translating natural language questions to formal queries has been one of the key approaches. To advance the research area, several datasets like WebQuestions, QALD and LCQuAD have been published in the past. The biggest data set available for complex questions (LCQuAD) over knowledge graphs contains ﬁve thousand questions. We now provide LC-QuAD 2.0 (Large-Scale Complex Question Answering Dataset) with 30,000 questions, their paraphrases and their corresponding SPARQL queries. LC-QuAD 2.0 is compatible with both Wikidata and DBpedia 2018 knowledge graphs. In this article, we explain how the dataset was created and the variety of questions available with examples. We further provide a statistical analysis of the dataset.},
	language = {en},
	urldate = {2024-02-28},
	booktitle = {The {Semantic} {Web} – {ISWC} 2019},
	publisher = {Springer International Publishing},
	author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	year = {2019},
	doi = {10.1007/978-3-030-30796-7_5},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {\#LQAD2},
	pages = {69--78},
}

@misc{li_dq-bart_2022,
	title = {{DQ}-{BART}: {Efficient} {Sequence}-to-{Sequence} {Model} via {Joint} {Distillation} and {Quantization}},
	shorttitle = {{DQ}-{BART}},
	url = {http://arxiv.org/abs/2203.11239},
	doi = {10.48550/arXiv.2203.11239},
	abstract = {Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Li, Zheng and Wang, Zijian and Tan, Ming and Nallapati, Ramesh and Bhatia, Parminder and Arnold, Andrew and Xiang, Bing and Roth, Dan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.11239 [cs]},
	keywords = {/prior, Computer Science - Computation and Language},
}

@article{kang_knowledge-augmented_2023,
	title = {Knowledge-{Augmented} {Reasoning} {Distillation} for {Small} {Language} {Models} in {Knowledge}-{Intensive} {Tasks}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/97faedc90260eae5c400f92d5831c3d7-Abstract-Conference.html},
	language = {zh-CN},
	urldate = {2024-03-25},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kang, Minki and Lee, Seanie and Baek, Jinheon and Kawaguchi, Kenji and Hwang, Sung Ju},
	month = dec,
	year = {2023},
	keywords = {/unread},
	pages = {48573--48602},
}

@misc{bustamante_sparql_2024,
	title = {{SPARQL} {Generation} with {Entity} {Pre}-trained {GPT} for {KG} {Question} {Answering}},
	url = {http://arxiv.org/abs/2402.00969},
	doi = {10.48550/arXiv.2402.00969},
	abstract = {Knowledge Graphs popularity has been rapidly growing in last years. All that knowledge is available for people to query it through the many online databases on the internet. Though, it would be a great achievement if non-programmer users could access whatever information they want to know. There has been a lot of effort oriented to solve this task using natural language processing tools and creativity encouragement by way of many challenges. Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them. We managed to isolate which property of the task can be the most difficult to solve at few or zero-shot and we proposed pre-training on all entities (under CWA) to improve the performance. We obtained a 62.703\% accuracy of exact SPARQL matches on testing at 3-shots, a F1 of 0.809 on the entity linking challenge and a F1 of 0.009 on the question answering challenge.},
	urldate = {2024-03-11},
	publisher = {arXiv},
	author = {Bustamante, Diego and Takeda, Hideaki},
	month = feb,
	year = {2024},
	note = {arXiv:2402.00969 [cs]},
	keywords = {/unread, 68P20, 68T50, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Information Retrieval, H.2.3, H.3.3, I.2.7},
}

@inproceedings{li_bekbqa_2023,
	address = {Urumqi, China},
	title = {{BEKBQA}: {Improving} {Knowledge} {Base} {Question} {Answering} with a {Bidirectional} {Entity} {Scoring} {Model}},
	isbn = {9798350324303},
	shorttitle = {{BEKBQA}},
	url = {https://ieeexplore.ieee.org/document/10348352/},
	doi = {10.1109/PRML59573.2023.10348352},
	abstract = {Knowledge base multi-hop question answering must perform hop-by-hop reasoning on long paths to obtain answer entities. However, due to the lack of datasets with correct path annotations, the model’s training can only rely on the results of the last hop answer for updating. A more innovative approach is to apply the knowledge distillation method to the questionanswering task, and the complex teacher model learns the entity distribution of each hop as the intermediate supervision signal of the student model. However, the structure of this dual model needs to be simplified. At the same time, in the backward reasoning process from the candidate answer entity to the topic entity, this method only uses the last hop result of the forward as the initialization, which does not guarantee that the backward reasoning process can conform to the semantic information of the question sentence. To address these challenges, we design a structurally simple model capable of constraining the intermediate reasoning process through forward and backward bidirectional reasoning. Moreover, we updated the initialization method of backward reasoning to make it more reasonable. At the same time, the main functions of the model are entirely based on the attention mechanism so that the results of each hop can be visualized and the reasoning process is transparent.},
	language = {en},
	urldate = {2024-03-22},
	booktitle = {2023 {IEEE} 4th {International} {Conference} on {Pattern} {Recognition} and {Machine} {Learning} ({PRML})},
	publisher = {IEEE},
	author = {Li, Jiaming and He, Liang and Ma, Hanhan and Wang, Shaolei and Yi, Sheng and Hu, Weiwei},
	month = aug,
	year = {2023},
	pages = {475--482},
}

@article{pliukhin_improving_nodate,
	title = {Improving {Subgraph} {Extraction} {Algorithms} for {One}-{Shot} {SPARQL} {Query} {Generation} with {Large} {Language} {Models}},
	abstract = {Question answering over scholarly knowledge graphs involves many challenges: complex graph patterns, long-tail distributed data, revision and evolution of the scholarly ontologies, and knowledge graphs incompleteness due to constant research dynamics. In this work, we present an LLM-based approach for SPARQL query generation over Open Research Knowledge Graph (ORKG) for the ISWC SciQA Challenge. Our approach proposes a couple of improvements to the recently published SPARQLGEN approach, that performs one-shot SPARQL query generation by augmenting Large Language Models (LLMs) with the relevant context within a single prompt. Similar to SPARQLGEN, we include heterogeneous data sources in the SPARQL generation prompt: a question itself, an RDF subgraph required to answer the question, and an example of a correct SPARQL query. In the current work, we focused on designing subgraph extraction algorithms, that are close to real-life scenarios of generative KGQA, and replaced the random choice of example question-query pair with similarity scoring.},
	language = {en},
	author = {Pliukhin, Dmitrii and Radyush, Daniil and Kovriguina, Liubov and Mouromtsev, Dmitry},
	keywords = {/prior, /unread},
}

@misc{huang_markqa_2023,
	title = {{MarkQA}: {A} large scale {KBQA} dataset with numerical reasoning},
	shorttitle = {{MarkQA}},
	url = {http://arxiv.org/abs/2310.15517},
	abstract = {While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA and propose a new task, NR-KBQA, which necessitates the ability to perform both multi-hop reasoning and numerical reasoning. We design a logic form in Python format called PyQL to represent the reasoning process of numerical reasoning questions. To facilitate the development of NR-KBQA, we present a large dataset called MarkQA, which is automatically constructed from a small set of seeds. Each question in MarkQA is equipped with its corresponding SPARQL query, alongside the step-by-step reasoning process in the QDMR format and PyQL program. Experimental results of some state-of-the-art QA methods on the MarkQA show that complex numerical reasoning in KBQA faces great challenges.},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Huang, Xiang and Cheng, Sitao and Bao, Yuheng and Huang, Shanshan and Qu, Yuzhong},
	month = dec,
	year = {2023},
	note = {arXiv:2310.15517 [cs]},
	keywords = {\#MarkQA, Computer Science - Computation and Language},
}

@article{chen_comprehensive_2023,
	title = {A {Comprehensive} {Empirical} {Study} of {Bias} {Mitigation} {Methods} for {Machine} {Learning} {Classifiers}},
	volume = {32},
	issn = {1049-331X, 1557-7392},
	url = {https://dl.acm.org/doi/10.1145/3583561},
	doi = {10.1145/3583561},
	abstract = {Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53\% of the studied scenarios (ranging between 42\%∼66\% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46\% of all the scenarios (ranging between 24\%∼59\% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25\% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30\% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s).},
	language = {en},
	number = {4},
	urldate = {2023-09-07},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Chen, Zhenpeng and Zhang, Jie M. and Sarro, Federica and Harman, Mark},
	month = oct,
	year = {2023},
	pages = {1--30},
}

@misc{purkayastha_knowledge_2021,
	title = {Knowledge {Graph} {Question} {Answering} via {SPARQL} {Silhouette} {Generation}},
	url = {http://arxiv.org/abs/2109.09475},
	abstract = {Knowledge Graph Question Answering (KGQA) has become a prominent area in natural language processing due to the emergence of large-scale Knowledge Graphs (KGs). Recently Neural Machine Translation based approaches are gaining momentum that translates natural language queries to structured query languages thereby solving the KGQA task. However, most of these methods struggle with out-of-vocabulary words where test entities and relations are not seen during training time. In this work, we propose a modular two-stage neural architecture to solve the KGQA task. The first stage generates a sketch of the target SPARQL called SPARQL silhouette for the input question. This comprises of (1) Noise simulator to facilitate out-of-vocabulary words and to reduce vocabulary size (2) seq2seq model for text to SPARQL silhouette generation. The second stage is a Neural Graph Search Module. SPARQL silhouette generated in the first stage is distilled in the second stage by substituting precise relation in the predicted structure. We simulate ideal and realistic scenarios by designing a noise simulator. Experimental results show that the quality of generated SPARQL silhouette in the first stage is outstanding for the ideal scenarios but for realistic scenarios (i.e. noisy linker), the quality of the resulting SPARQL silhouette drops drastically. However, our neural graph search module recovers it considerably. We show that our method can achieve reasonable performance improving the state-of-art by a margin of 3.72\% F1 for the LC-QuAD-1 dataset. We believe, our proposed approach is novel and will lead to dynamic KGQA solutions that are suited for practical applications.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Purkayastha, Sukannya and Dana, Saswati and Garg, Dinesh and Khandelwal, Dinesh and Bhargav, G. P. Shrivatsa},
	month = sep,
	year = {2021},
	note = {arXiv:2109.09475 [cs]},
	keywords = {\#LQAD1, \#QALD-9, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
}

@misc{berk_convex_2017,
	title = {A {Convex} {Framework} for {Fair} {Regression}},
	url = {http://arxiv.org/abs/1706.02409},
	abstract = {We introduce a ﬂexible family of fairness regularizers for (linear and logistic) regression problems. These regularizers all enjoy convexity, permitting fast optimization, and they span the range from notions of group fairness to strong individual fairness. By varying the weight on the fairness regularizer, we can compute the eﬃcient frontier of the accuracy-fairness trade-oﬀ on any given dataset, and we measure the severity of this trade-oﬀ via a numerical quantity we call the Price of Fairness (PoF). The centerpiece of our results is an extensive comparative study of the PoF across six diﬀerent datasets in which fairness is a primary consideration.},
	language = {en},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Berk, Richard and Heidari, Hoda and Jabbari, Shahin and Joseph, Matthew and Kearns, Michael and Morgenstern, Jamie and Neel, Seth and Roth, Aaron},
	month = jun,
	year = {2017},
	note = {arXiv:1706.02409 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mehrabi_debiasing_2019,
	address = {Vancouver British Columbia Canada},
	title = {Debiasing community detection: the importance of lowly connected nodes},
	isbn = {978-1-4503-6868-1},
	shorttitle = {Debiasing community detection},
	url = {https://dl.acm.org/doi/10.1145/3341161.3342915},
	doi = {10.1145/3341161.3342915},
	language = {en},
	urldate = {2023-10-24},
	booktitle = {Proceedings of the 2019 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining}},
	publisher = {ACM},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Peng, Nanyun and Galstyan, Aram},
	month = aug,
	year = {2019},
	pages = {509--512},
}

@inproceedings{zhang_achieving_2017,
	address = {Halifax NS Canada},
	title = {Achieving {Non}-{Discrimination} in {Data} {Release}},
	isbn = {978-1-4503-4887-4},
	url = {https://dl.acm.org/doi/10.1145/3097983.3098167},
	doi = {10.1145/3097983.3098167},
	language = {en},
	urldate = {2023-10-24},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Zhang, Lu and Wu, Yongkai and Wu, Xintao},
	month = aug,
	year = {2017},
	pages = {1335--1344},
}

@article{mehrabi_survey_2021,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3457607},
	doi = {10.1145/3457607},
	abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	number = {6},
	urldate = {2023-09-06},
	journal = {ACM Computing Surveys},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	year = {2021},
	keywords = {Fairness and bias in artificial intelligence, deep learning, machine learning, natural language processing, representation learning},
	pages = {115:1--115:35},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Machines learn what people know implicitly
            
              AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan
              et al.
              now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.
            
            
              Science
              , this issue p.
              183
              ; see also p.
              133
            
          , 
            Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.
          , 
            Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	language = {en},
	number = {6334},
	urldate = {2023-08-11},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pages = {183--186},
}

@inproceedings{brunet_understanding_2019,
	title = {Understanding the {Origins} of {Bias} in {Word} {Embeddings}},
	url = {https://proceedings.mlr.press/v97/brunet19a.html},
	abstract = {Popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems can amplify stereotypes in important contexts. Although some methods have been developed to mitigate this problem, how word embedding biases arise during training is poorly understood. In this work we develop a technique to address this question. Given a word embedding, our method reveals how perturbing the training corpus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikipedia and New York Times corpora, and find it to be very accurate.},
	language = {en},
	urldate = {2023-09-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson, Ashton and Zemel, Richard},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {803--811},
}

@misc{bolukbasi_quantifying_2016,
	title = {Quantifying and {Reducing} {Stereotypes} in {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1606.06121},
	abstract = {Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes. In this paper, we initiate the study of gender stereotypes in \{{\textbackslash}em word embedding\}, a popular framework to represent text data. As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding. We developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding. We evaluated our algorithm on several metrics. While we focus on male/female stereotypes, our framework may be applicable to other types of embedding biases.},
	language = {en},
	urldate = {2023-08-20},
	publisher = {arXiv},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jun,
	year = {2016},
	note = {arXiv:1606.06121 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{alelyani_detection_2021,
	title = {Detection and {Evaluation} of {Machine} {Learning} {Bias}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/14/6271},
	doi = {10.3390/app11146271},
	abstract = {Machine learning models are built using training data, which is collected from human experience and is prone to bias. Humans demonstrate a cognitive bias in their thinking and behavior, which is ultimately reflected in the collected data. From Amazon’s hiring system, which was built using ten years of human hiring experience, to a judicial system that was trained using human judging practices, these systems all include some element of bias. The best machine learning models are said to mimic humans’ cognitive ability, and thus such models are also inclined towards bias. However, detecting and evaluating bias is a very important step for better explainable models. In this work, we aim to explain bias in learning models in relation to humans’ cognitive bias and propose a wrapper technique to detect and evaluate bias in machine learning models using an openly accessible dataset from UCI Machine Learning Repository. In the deployed dataset, the potentially biased attributes (PBAs) are gender and race. This study introduces the concept of alternation functions to swap the values of PBAs, and evaluates the impact on prediction using KL divergence. Results demonstrate females and Asians to be associated with low wages, placing some open research questions for the research community to ponder over.},
	language = {en},
	number = {14},
	urldate = {2023-09-03},
	journal = {Applied Sciences},
	author = {Alelyani, Salem},
	month = jan,
	year = {2021},
	note = {Number: 14
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {KL divergence, bias detection, bias evaluation, cognitive bias, explainable models, machine learning bias},
	pages = {6271},
}

@inproceedings{swinger_what_2019,
	address = {Honolulu HI USA},
	title = {What are the {Biases} in {My} {Word} {Embedding}?},
	isbn = {978-1-4503-6324-2},
	url = {https://dl.acm.org/doi/10.1145/3306618.3314270},
	doi = {10.1145/3306618.3314270},
	abstract = {This paper presents an algorithm for enumerating biases in word embeddings. The algorithm exposes a large number of offensive associations related to sensitive features such as race and gender on publicly available embeddings. These biases are concerning in light of the widespread use of word embeddings. The associations are identified by geometric patterns in word embeddings that run parallel between people’s names and common lower-case tokens. The algorithm is highly unsupervised: it does not even require the sensitive features to be pre-specified. This is desirable because: (a) many forms of discrimination–such as racial discrimination–are linked to social constructs that may vary depending on the context, rather than to categories with fixed definitions; and (b) it makes it easier to identify biases against intersectional groups, which depend on combinations of sensitive features. The inputs to our algorithm are a list of target tokens, e.g. names, and a word embedding. It outputs a number of Word Embedding Association Tests (WEATs) that capture various biases present in the data. We illustrate the utility of our approach on publicly available word embeddings and lists of names, and evaluate its output using crowdsourcing. We also show how removing names may not remove potential proxy bias.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Swinger, Nathaniel and De-Arteaga, Maria and Heffernan IV, Neil Thomas and Leiserson, Mark DM and Kalai, Adam Tauman},
	month = jan,
	year = {2019},
	pages = {305--311},
}

@inproceedings{sun_mitigating_2019,
	address = {Florence, Italy},
	title = {Mitigating {Gender} {Bias} in {Natural} {Language} {Processing}: {Literature} {Review}},
	shorttitle = {Mitigating {Gender} {Bias} in {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/P19-1159},
	doi = {10.18653/v1/P19-1159},
	abstract = {As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artiﬁcial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.},
	language = {en},
	urldate = {2023-08-20},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Tony and Gaut, Andrew and Tang, Shirlyn and Huang, Yuxin and ElSherief, Mai and Zhao, Jieyu and Mirza, Diba and Belding, Elizabeth and Chang, Kai-Wei and Wang, William Yang},
	year = {2019},
	pages = {1630--1640},
}

@inproceedings{hovy_tagging_2015,
	address = {Beijing, China},
	title = {Tagging {Performance} {Correlates} with {Author} {Age}},
	url = {http://aclweb.org/anthology/P15-2079},
	doi = {10.3115/v1/P15-2079},
	abstract = {Many NLP tools for English and German are based on manually annotated articles from the Wall Street Journal and Frankfurter Rundschau. The average readers of these two newspapers are middle-aged (55 and 47 years old, respectively), and the annotated articles are more than 20 years old by now. This leads us to speculate whether tools induced from these resources (such as part-of-speech taggers) put older language users at an advantage. We show that this is actually the case in both languages, and that the cause goes beyond simple vocabulary differences. In our experiments, we control for gender and region.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hovy, Dirk and Søgaard, Anders},
	year = {2015},
	pages = {483--488},
}

@inproceedings{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	volume = {29},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female.  Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	urldate = {2023-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
	year = {2016},
}

@misc{zhao_gender_2018,
	title = {Gender {Bias} in {Coreference} {Resolution}: {Evaluation} and {Debiasing} {Methods}},
	shorttitle = {Gender {Bias} in {Coreference} {Resolution}},
	url = {http://arxiv.org/abs/1804.06876},
	abstract = {We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without signiﬁcantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are avialable at http://winobias.org.},
	language = {en},
	urldate = {2023-08-11},
	publisher = {arXiv},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = apr,
	year = {2018},
	note = {arXiv:1804.06876 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{basumallik_towards_2005,
	address = {Cambridge Massachusetts},
	title = {Towards automatic translation of {OpenMP} to {MPI}},
	isbn = {978-1-59593-167-2},
	url = {https://dl.acm.org/doi/10.1145/1088149.1088174},
	doi = {10.1145/1088149.1088174},
	language = {en},
	urldate = {2023-10-21},
	booktitle = {Proceedings of the 19th annual international conference on {Supercomputing}},
	publisher = {ACM},
	author = {Basumallik, Ayon and Eigenmann, Rudolf},
	month = jun,
	year = {2005},
	pages = {189--198},
}

@article{pagano_bias_2023,
	title = {Bias and {Unfairness} in {Machine} {Learning} {Models}: {A} {Systematic} {Review} on {Datasets}, {Tools}, {Fairness} {Metrics}, and {Identification} and {Mitigation} {Methods}},
	volume = {7},
	issn = {2504-2289},
	shorttitle = {Bias and {Unfairness} in {Machine} {Learning} {Models}},
	url = {https://www.mdpi.com/2504-2289/7/1/15},
	doi = {10.3390/bdcc7010015},
	abstract = {One of the difﬁculties of artiﬁcial intelligence is to ensure that model decisions are fair and free of bias. In research, datasets, metrics, techniques, and tools are applied to detect and mitigate algorithmic unfairness and bias. This study examines the current knowledge on bias and unfairness in machine learning models. The systematic review followed the PRISMA guidelines and is registered on OSF plataform. The search was carried out between 2021 and early 2022 in the Scopus, IEEE Xplore, Web of Science, and Google Scholar knowledge bases and found 128 articles published between 2017 and 2022, of which 45 were chosen based on search string optimization and inclusion and exclusion criteria. We discovered that the majority of retrieved works focus on bias and unfairness identiﬁcation and mitigation techniques, offering tools, statistical approaches, important metrics, and datasets typically used for bias experiments. In terms of the primary forms of bias, data, algorithm, and user interaction were addressed in connection to the preprocessing, in-processing, and postprocessing mitigation methods. The use of Equalized Odds, Opportunity Equality, and Demographic Parity as primary fairness metrics emphasizes the crucial role of sensitive attributes in mitigating bias. The 25 datasets chosen span a wide range of areas, including criminal justice image enhancement, ﬁnance, education, product pricing, and health, with the majority including sensitive attributes. In terms of tools, Aequitas is the most often referenced, yet many of the tools were not employed in empirical experiments. A limitation of current research is the lack of multiclass and multimetric studies, which are found in just a few works and constrain the investigation to binary-focused method. Furthermore, the results indicate that different fairness metrics do not present uniform results for a given use case, and that more research with varied model architectures is necessary to standardize which ones are more appropriate for a given context. We also observed that all research addressed the transparency of the algorithm, or its capacity to explain how decisions are taken.},
	language = {en},
	number = {1},
	urldate = {2023-09-02},
	journal = {Big Data and Cognitive Computing},
	author = {Pagano, Tiago P. and Loureiro, Rafael B. and Lisboa, Fernanda V. N. and Peixoto, Rodrigo M. and Guimarães, Guilherme A. S. and Cruz, Gustavo O. R. and Araujo, Maira M. and Santos, Lucas L. and Cruz, Marco A. S. and Oliveira, Ewerton L. S. and Winkler, Ingrid and Nascimento, Erick G. S.},
	month = jan,
	year = {2023},
	pages = {15},
}

@misc{nadeem_stereoset_2020,
	title = {{StereoSet}: {Measuring} stereotypical bias in pretrained language models},
	shorttitle = {{StereoSet}},
	url = {http://arxiv.org/abs/2004.09456},
	abstract = {A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or Asians are bad drivers. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real world data, they are known to capture stereotypical biases. In order to assess the adverse effects of these models, it is important to quantify the bias captured in them. Existing literature on quantifying bias evaluates pretrained language models on a small set of artificially constructed bias-assessing sentences. We present StereoSet, a large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion. We evaluate popular models like BERT, GPT-2, RoBERTa, and XLNet on our dataset and show that these models exhibit strong stereotypical biases. We also present a leaderboard with a hidden test set to track the bias of future language models at https://stereoset.mit.edu},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Nadeem, Moin and Bethke, Anna and Reddy, Siva},
	month = apr,
	year = {2020},
	note = {arXiv:2004.09456 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{mitchell_algorithmic_2021,
	title = {Algorithmic {Fairness}: {Choices}, {Assumptions}, and {Definitions}},
	volume = {8},
	shorttitle = {Algorithmic {Fairness}},
	url = {https://doi.org/10.1146/annurev-statistics-042720-125902},
	doi = {10.1146/annurev-statistics-042720-125902},
	abstract = {A recent wave of research has attempted to define fairness quantitatively. In particular, this work has explored what fairness might mean in the context of decisions based on the predictions of statistical and machine learning models. The rapid growth of this new field has led to wildly inconsistent motivations, terminology, and notation, presenting a serious challenge for cataloging and comparing definitions. This article attempts to bring much-needed order. First, we explicate the various choices and assumptions made—often implicitly—to justify the use of prediction-based decision-making. Next, we show how such choices and assumptions can raise fairness concerns and we present a notationally consistent catalog of fairness definitions from the literature. In doing so, we offer a concise reference for thinking through the choices, assumptions, and fairness considerations of prediction-based decision-making.},
	number = {1},
	urldate = {2023-09-03},
	journal = {Annual Review of Statistics and Its Application},
	author = {Mitchell, Shira and Potash, Eric and Barocas, Solon and D'Amour, Alexander and Lum, Kristian},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-statistics-042720-125902},
	pages = {141--163},
}

@article{turner_lee_detecting_2018,
	title = {Detecting racial bias in algorithms and machine learning},
	volume = {16},
	issn = {1477-996X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/JICES-06-2018-0056/full/html},
	doi = {10.1108/JICES-06-2018-0056},
	abstract = {Purpose – The online economy has not resolved the issue of racial bias in its applications. While algorithms are procedures that facilitate automated decision-making, or a sequence of unambiguous instructions, bias is a byproduct of these computations, bringing harm to historically disadvantaged populations. This paper argues that algorithmic biases explicitly and implicitly harm racial groups and lead to forms of discrimination. Relying upon sociological and technical research, the paper offers commentary on the need for more workplace diversity within high-tech industries and public policies that can detect or reduce the likelihood of racial bias in algorithmic design and execution.},
	language = {en},
	number = {3},
	urldate = {2023-08-20},
	journal = {Journal of Information, Communication and Ethics in Society},
	author = {Turner Lee, Nicol},
	month = aug,
	year = {2018},
	pages = {252--260},
}

@article{gaonkar_ethical_2020,
	title = {Ethical {Issues} {Arising} {Due} to {Bias} in {Training} {A}.{I}. {Algorithms} in {Healthcare} and {Data} {Sharing} as a {Potential} {Solution}},
	volume = {1},
	issn = {26901633, 26901625},
	url = {https://www.aiethicsjournal.org/10-47289-aiej20200916},
	doi = {10.47289/AIEJ20200916},
	abstract = {Machine learning algorithms have been shown to be capable of diagnosing cancer, Alzheimer’s disease and even selecting treatment options. However, the majority of machine learning systems implemented in the healthcare setting tend to be based on the supervised machine learning paradigm. These systems tend to rely on previously collected data annotated by medical personnel from specific populations. This leads to ‘learnt’ machine learning models that lack generalizability. In other words, the machine’s predictions are not as accurate for certain populations and can disagree with recommendations of medical experts who did not annotate the data used to train these models. With each human-decided aspect of building supervised machine learning models, human bias is introduced into the machine’s decision-making. This human bias is the source of numerous ethical concerns. In this article, we describe and discuss three challenges to generalizability which affect real world deployment of machine learning systems in clinical practice. First, there is bias which occurs due to the characteristics of the population from which data was collected. Second, the bias which occurs due to the prejudice of the expert annotator involved. And third, the bias by the timing of when A.I. processes start training themselves. We also discuss the future implications of these biases. More importantly, we describe how responsible data sharing can help mitigate the effects of these biases – and allow for the development of novel algorithms which may be able to train in an unbiased manner. We discuss environmental and regulatory hurdles which hinder the sharing of data in medicine – and discuss possible updates to current regulations that may enable ethical data sharing for machine learning. With these updates in mind, we also discuss emerging algorithmic frameworks being used to create medical machine learning systems, which can eventually learn to be free from population- and expert-induced bias. These models can then truly be deployed to clinics worldwide, making medicine both cheaper and more accessible for the world at large.},
	language = {en},
	number = {1},
	urldate = {2023-09-02},
	journal = {AI Ethics Journal},
	author = {Gaonkar, Bilwaj and Kim, Kirstin and Macyszyn, Luke},
	month = sep,
	year = {2020},
	pages = {1--9},
}

@inproceedings{zhang_mitigating_2018,
	address = {New Orleans LA USA},
	title = {Mitigating {Unwanted} {Biases} with {Adversarial} {Learning}},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278779},
	doi = {10.1145/3278721.3278779},
	language = {en},
	urldate = {2023-09-07},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Zhang, Brian Hu and Lemoine, Blake and Mitchell, Margaret},
	month = dec,
	year = {2018},
	pages = {335--340},
}

@inproceedings{li_towards_2018,
	title = {Towards {Robust} and {Privacy}-preserving {Text} {Representations}},
	author = {Li, Yitong and Baldwin, Timothy and Cohn, Trevor},
        booktitle = {ACL},
	year = {2018},
        pages = {25--30}
}

@article{gao_comp90044_nodate,
	title = {{COMP90044} {Research} {Methods} {Assignment} 1: {Literature} {Review}},
	language = {en},
	author = {Gao, Shengxiang},
}

@inproceedings{razavian_cnn_2014,
	address = {Columbus, OH, USA},
	title = {{CNN} {Features} {Off}-the-{Shelf}: {An} {Astounding} {Baseline} for {Recognition}},
	isbn = {978-1-4799-4308-1},
	shorttitle = {{CNN} {Features} {Off}-the-{Shelf}},
	url = {https://ieeexplore.ieee.org/document/6910029},
	doi = {10.1109/CVPRW.2014.131},
	abstract = {Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classiﬁcation on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classiﬁcation, scene recognition, ﬁne grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classiﬁcation tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classiﬁer (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modiﬁed using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.},
	language = {en},
	urldate = {2023-10-15},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
	month = jun,
	year = {2014},
	pages = {512--519},
}

@article{neumann_image_2006,
	title = {Image retrieval and perceptual similarity},
	volume = {3},
	issn = {1544-3558, 1544-3965},
	url = {https://dl.acm.org/doi/10.1145/1119766.1119769},
	doi = {10.1145/1119766.1119769},
	abstract = {Simple, low-level visual features are extensively used for content-based image retrieval. Our goal was to evaluate an image-indexing system based on some of the known properties of the early stages of human vision. We quantitatively measured the relationship between the similarity order induced by the indexes and perceived similarity. In contrast to previous evaluation approaches, we objectively measured similarity both for the few best-matching images and also for relatively distinct images. The results show that, to a large degree, the rank orders induced by the indexes predict the perceived similarity between images. The highest index concordance employing a single index was obtained using the chromaticity histogram. Combining different information sources substantially improved the correspondence with the observers. We conclude that image-indexing systems can provide useful measures for perceptual image similarity. The methods presented here can be used to evaluate and compare different image-retrieval systems.},
	language = {en},
	number = {1},
	urldate = {2023-10-14},
	journal = {ACM Transactions on Applied Perception},
	author = {Neumann, Dirk and Gegenfurtner, Karl R.},
	month = jan,
	year = {2006},
	pages = {31--47},
}

@article{sinha_perceptually_2011,
	title = {A {Perceptually} {Based} {Comparison} of {Image} {Similarity} {Metrics}},
	volume = {40},
	issn = {0301-0066, 1468-4233},
	url = {http://journals.sagepub.com/doi/10.1068/p7063},
	doi = {10.1068/p7063},
	abstract = {The assessment of how well one image matches another forms a critical component both of models of human visual processing and of many image analysis systems. Two of the most commonly used norms for quantifying image similarity are L1 and L2, which are specific instances of the Minkowski metric. However, there is often not a principled reason for selecting one norm over the other. One way to address this problem is by examining whether one metric, better than the other, captures the perceptual notion of image similarity. This can be used to derive inferences regarding similarity criteria the human visual system uses, as well as to evaluate and design metrics for use in image-analysis applications. With this goal, we examined perceptual preferences for images retrieved on the basis of the L1 versus the L2 norm. These images were either small fragments without recognizable content, or larger patterns with recognizable content created by vector quantization. In both conditions the participants showed a small but consistent preference for images matched with the L1 metric. These results suggest that, in the domain of natural images of the kind we have used, the L1 metric may better capture human notions of image similarity.},
	language = {en},
	number = {11},
	urldate = {2023-10-14},
	journal = {Perception},
	author = {Sinha, Pawan and Russell, Richard},
	month = nov,
	year = {2011},
	pages = {1269--1281},
}

@misc{risser-maroix_learning_2022,
	title = {Learning an {Adaptation} {Function} to {Assess} {Image} {Visual} {Similarities}},
	url = {http://arxiv.org/abs/2206.01417},
	abstract = {Human perception is routinely assessing the similarity between images, both for decision making and creative thinking. But the underlying cognitive process is not really well understood yet, hence difﬁcult to be mimicked by computer vision systems. State-of-the-art approaches using deep architectures are often based on the comparison of images described as feature vectors learned for image categorization task. As a consequence, such features are powerful to compare semantically related images but not really efﬁcient to compare images visually similar but semantically unrelated. Inspired by previous works on neural features adaptation to psycho-cognitive representations, we focus here on the speciﬁc task of learning visual image similarities when analogy matters. We propose to compare different supervised, semi-supervised and self-supervised networks, pre-trained on distinct scales and contents datasets (such as ImageNet-21k, ImageNet-1K or VGGFace2) to conclude which model may be the best to approximate the visual cortex and learn only an adaptation function corresponding to the approximation of the the primate IT cortex through the metric learning framework. Our experiments conducted on the Totally Looks Like image dataset highlight the interest of our method, by increasing the retrieval scores of the best model @1 by 2.25×. This research work was recently accepted for publication at the ICIP 2021 international conference [1]. In this new article, we expand on this previous work by using and comparing new pre-trained feature extractors on other datasets.},
	language = {en},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Risser-Maroix, Olivier and Marzouki, Amine and Djeghim, Hala and Kurtz, Camille and Lomenie, Nicolas},
	month = jun,
	year = {2022},
	note = {arXiv:2206.01417 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{rosenfeld_totally_2018,
	title = {Totally {Looks} {Like} - {How} {Humans} {Compare}, {Compared} to {Machines}},
	url = {http://arxiv.org/abs/1803.01485},
	abstract = {Perceptual judgment of image similarity by humans relies on rich internal representations ranging from low-level features to high-level concepts, scene properties and even cultural associations. However, existing methods and datasets attempting to explain perceived similarity use stimuli which arguably do not cover the full breadth of factors that aﬀect human similarity judgments, even those geared toward this goal. We introduce a new dataset dubbed Totally-Looks-Like (TLL) after a popular entertainment website, which contains images paired by humans as being visually similar. The dataset contains 6016 image-pairs from the wild, shedding light upon a rich and diverse set of criteria employed by human beings. We conduct experiments to try to reproduce the pairings via features extracted from state-of-the-art deep convolutional neural networks, as well as additional human experiments to verify the consistency of the collected data. Though we create conditions to artiﬁcially make the matching task increasingly easier, we show that machine-extracted representations perform very poorly in terms of reproducing the matching selected by humans. We discuss and analyze these results, suggesting future directions for improvement of learned image representations.},
	language = {en},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Rosenfeld, Amir and Solbach, Markus D. and Tsotsos, John K.},
	month = oct,
	year = {2018},
	note = {arXiv:1803.01485 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{may_measuring_2019,
	title = {On {Measuring} {Social} {Biases} in {Sentence} {Encoders}},
	url = {http://arxiv.org/abs/1903.10561},
	abstract = {The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test's assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.},
	urldate = {2023-09-07},
	publisher = {arXiv},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	month = mar,
	year = {2019},
	note = {arXiv:1903.10561 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
	urldate = {2023-09-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
}

@article{sun_evolution_2020,
	title = {Evolution and impact of bias in human and machine learning algorithm interaction},
	volume = {15},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0235502},
	doi = {10.1371/journal.pone.0235502},
	language = {en},
	number = {8},
	urldate = {2023-09-07},
	journal = {PLOS ONE},
	author = {Sun, Wenlong and Nasraoui, Olfa and Shafto, Patrick},
	editor = {Xin, Baogui},
	month = aug,
	year = {2020},
	pages = {e0235502},
}

@inproceedings{jiang_identifying_2020,
	title = {Identifying and {Correcting} {Label} {Bias} in {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v108/jiang20a.html},
	abstract = {Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.},
	language = {en},
	urldate = {2023-09-03},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Jiang, Heinrich and Nachum, Ofir},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {702--712},
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
}

@incollection{el_naqa_what_2015,
	address = {Cham},
	title = {What {Is} {Machine} {Learning}?},
	isbn = {978-3-319-18305-3},
	url = {https://doi.org/10.1007/978-3-319-18305-3_1},
	booktitle = {Machine {Learning} in {Radiation} {Oncology}: {Theory} and {Applications}},
	publisher = {Springer International Publishing},
	author = {El Naqa, Issam and Murphy, Martin J.},
	editor = {El Naqa, Issam and Li, Ruijiang and Murphy, Martin J.},
	year = {2015},
	doi = {10.1007/978-3-319-18305-3_1},
	keywords = {Cancer, Human-machine interaction, Machine learning, Radiotherapy},
	pages = {3--11},
}

@inproceedings{narayanan_venkit_towards_2023,
	address = {Montr{\textbackslash}'\{e\}al QC Canada},
	title = {Towards a {Holistic} {Approach}: {Understanding} {Sociodemographic} {Biases} in {NLP} {Models} using an {Interdisciplinary} {Lens}},
	isbn = {9798400702310},
	shorttitle = {Towards a {Holistic} {Approach}},
	url = {https://dl.acm.org/doi/10.1145/3600211.3604754},
	doi = {10.1145/3600211.3604754},
	abstract = {The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches.},
	language = {en},
	urldate = {2023-09-03},
	booktitle = {Proceedings of the 2023 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Narayanan Venkit, Pranav},
	month = aug,
	year = {2023},
	pages = {1004--1005},
}

@article{wuest_machine_2016,
	title = {Machine learning in manufacturing: advantages, challenges, and applications},
	volume = {4},
	issn = {2169-3277},
	shorttitle = {Machine learning in manufacturing},
	url = {https://www.tandfonline.com/doi/full/10.1080/21693277.2016.1192517},
	doi = {10.1080/21693277.2016.1192517},
	abstract = {The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.},
	language = {en},
	number = {1},
	urldate = {2023-09-02},
	journal = {Production \& Manufacturing Research},
	author = {Wuest, Thorsten and Weimer, Daniel and Irgens, Christopher and Thoben, Klaus-Dieter},
	month = jan,
	year = {2016},
	pages = {23--45},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	abstract = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
	language = {en},
	number = {6245},
	urldate = {2023-09-02},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	pages = {255--260},
}

@inproceedings{leavy_gender_2018,
	address = {Gothenburg Sweden},
	title = {Gender bias in artificial intelligence: the need for diversity and gender theory in machine learning},
	isbn = {978-1-4503-5738-8},
	shorttitle = {Gender bias in artificial intelligence},
	url = {https://dl.acm.org/doi/10.1145/3195570.3195580},
	doi = {10.1145/3195570.3195580},
	abstract = {Artificial intelligence is increasingly influencing the opinions and behavior of people in everyday life. However, the over-representation of men in the design of these technologies could quietly undo decades of advances in gender equality. Over centuries, humans developed critical theory to inform decisions and avoid basing them solely on personal experience. However, machine intelligence learns primarily from observing data that it is presented with. While a machine’s ability to process large volumes of data may address this in part, if that data is laden with stereotypical concepts of gender, the resulting application of the technology will perpetuate this bias. While some recent studies sought to remove bias from learned algorithms they largely ignore decades of research on how gender ideology is embedded in language. Awareness of this research and incorporating it into approaches to machine learning from text would help prevent the generation of biased algorithms. Leading thinkers in the emerging field addressing bias in artificial intelligence are also primarily female, suggesting that those who are potentially affected by bias are more likely to see, understand and attempt to resolve it. Gender balance in machine learning is therefore crucial to prevent algorithms from perpetuating gender ideologies that disadvantage women.},
	language = {en},
	urldate = {2023-08-20},
	booktitle = {Proceedings of the 1st {International} {Workshop} on {Gender} {Equality} in {Software} {Engineering}},
	publisher = {ACM},
	author = {Leavy, Susan},
	month = may,
	year = {2018},
	pages = {14--16},
}

@inproceedings{carlini_towards_2017,
	address = {San Jose, CA, USA},
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	isbn = {978-1-5090-5533-3},
	url = {http://ieeexplore.ieee.org/document/7958570/},
	doi = {10.1109/SP.2017.49},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {2017 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Carlini, Nicholas and Wagner, David},
	month = may,
	year = {2017},
	pages = {39--57},
}

@article{biggio_wild_2018,
	title = {Wild patterns: {Ten} years after the rise of adversarial machine learning},
	volume = {84},
	issn = {00313203},
	shorttitle = {Wild patterns},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320318302565},
	doi = {10.1016/j.patcog.2018.07.023},
	language = {en},
	urldate = {2023-08-11},
	journal = {Pattern Recognition},
	author = {Biggio, Battista and Roli, Fabio},
	month = dec,
	year = {2018},
	pages = {317--331},
}

@article{mitzenmacher_model_nodate,
	title = {A {Model} for {Learned} {Bloom} {Filters} and {Optimizing} by {Sandwiching}},
	abstract = {Recent work has suggested enhancing Bloom ﬁlters by using a pre-ﬁlter, based on applying machine learning to determine a function that models the data set the Bloom ﬁlter is meant to represent. Here we model such learned Bloom ﬁlters, with the following outcomes: (1) we clarify what guarantees can and cannot be associated with such a structure; (2) we show how to estimate what size the learning function must obtain in order to obtain improved performance; (3) we provide a simple method, sandwiching, for optimizing learned Bloom ﬁlters; and (4) we propose a design and analysis approach for a learned Bloomier ﬁlter, based on our modeling approach.},
	language = {en},
	author = {Mitzenmacher, Michael},
}

@article{ma_self-driving_nodate,
	title = {Self-{Driving} {Database}  {Management} {Systems}: {Forecasting}, {Modeling}, and {Planning}},
	language = {en},
	author = {Ma, Lin},
}

@inproceedings{kraska_case_2018,
	address = {Houston TX USA},
	title = {The {Case} for {Learned} {Index} {Structures}},
	isbn = {978-1-4503-4703-7},
	url = {https://dl.acm.org/doi/10.1145/3183713.3196909},
	doi = {10.1145/3183713.3196909},
	abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
	language = {en},
	urldate = {2023-08-11},
	booktitle = {Proceedings of the 2018 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
	month = may,
	year = {2018},
	pages = {489--504},
}

@article{kraska_sagedb_nodate,
	title = {{SageDB}: {A} {Learned} {Database} {System}},
	abstract = {Modern data processing systems are designed to be general purpose, in that they can handle a wide variety of diﬀerent schemas, data types, and data distributions, and aim to provide eﬃcient access to that data via the use of optimizers and cost models. This general purpose nature results in systems that do not take advantage of the characteristics of the particular application and data of the user. With SageDB we present a vision towards a new type of a data processing system, one which highly specializes to an application through code synthesis and machine learning. By modeling the data distribution, workload, and hardware, SageDB learns the structure of the data and optimal access methods and query plans. These learned models are deeply embedded, through code synthesis, in essentially every component of the database. As such, SageDB presents radical departure from the way database systems are currently developed, raising a host of new problems in databases, machine learning and programming systems.},
	language = {en},
	author = {Kraska, Tim and Alizadeh, Mohammad and Beutel, Alex and Chi, Ed H and Ding, Jialin and Kristo, Ani and Leclerc, Guillaume and Madden, Samuel and Mao, Hongzi and Nathan, Vikram},
}