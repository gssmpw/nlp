\section{Related Work}
\label{sec:literature}

\paragraph{Knowledge Base Question Answering}

Most KBQA 
%(a.k.a. knowledge graph question answering \citep{Yao2014}, KGQA) %\footnote{This problem is also referred to as knowledge graph question answering____. We use KBQA to refer to both problems, since most KBs are organized in a graph.}  
solutions use {information retrieval-based} (IR-based) or {semantic parsing-based} (SP-based) methods \citep{Liao2018}. IR-based methods construct a question-specific subgraph starting from the retrieved entities (i.e., the \emph{topic entities}). They then reason over  the  subgraph to derive the answer. SP-based methods focus on transforming input questions into logical forms, which are then executed to retrieve answers \citep{Dong2017}. %Compared to IR-based methods, SP-based methods can produce a more interpretable reasoning process through converting the natural language questions into executable logical forms. Moreover, 
SOTA solutions are mostly SP-based, as detailed next.
%on popular benchmarks (e.g., GrailQA \citep{Jansen2020}, WebQuestionsSP \citep{WebQuestionsSP} 

%JHL1: does SP-based = generation-based approach? does IR-based = ranking-based (in the intro)? it seems like these things are all the same, but we have two different terms; let be more consistent. Also, it doesn't look like IR/ranking based method is all that important to us, so let's drop that discussion in the intro and focus on contrasting our method to generation-based/SP-based models


% SP-based methods can be further categorized into ranking-based and generation-based methods \citep{Chen2019}. 

% \jz{Two sentences to explain ranking-based and generation-based methods, rep.} \sxfix{Ranking-based methods perform path traversal and ranking in the KB, starting from the retrieved entities \citep{Gao2020}. Generation-based methods directly transform the input question into the target logical form using a Seq2Seq model \citep{Zhang2019}.}

\paragraph{KBQA under I.I.D. Settings}

%Benefiting from the powerful natural language understanding and logical form generation capabilities of Large Language Models (LLMs), 
Recent KBQA studies under I.I.D. settings fine-tune LLMs to map input questions to rough KB elements and generate approximate logical form drafts \citep{Huang2020}. %, exploiting LLMs' semantic capabilities to understand input natural language questions. 
The approximate (i.e., inaccurate or ambiguous) KB elements are then aligned to exact KB elements through a subsequent retrieval stage. These solutions often fail over test questions that refer to KB elements unseen during training. While we also use LLMs for logical form generation, we ground the generation with retrieved relations, entities, and  schema contexts, thus addressing the non-I.I.D. issue. 
%Our model first retrieves KB elements through schema-guided retrieval, and then uses the retrieved KB elements along with their schema context to guide the generation of logical forms. This approach enables better generalization to questions containing unseen knowledge base elements, while also enhancing performance under the i.i.d. setting.


\paragraph{KBQA under Non-I.I.D. Settings}

Studies considering non-I.I.D. settings can be largely classified into \emph{ranking-based} and \emph{generation-based} methods. 

Ranking-based methods start from retrieved entities, traverse the KB, and construct the target logical form by ranking the traversed paths.  
%Ranking-based methods reduce the search space based on the KB structure and the retrieved entities. 
\citep{Wang2020a} enumerate and rank all possible logical forms within two hops  of retrieved entities, while \citep{Xu2019} incrementally expand and rank paths from retrieved entities. % to obtain the target logical form. %They then obtain the candidate logical form that best matches the question by ranking. %They evaluated both supervised fine-tuning LMs (e.g., T5 \citep{T5}, BERT \citep{BERT}) and few-shot in-context learning LLMs (e.g., Codex \citep{Codex}) as the partial logical form discriminators. 

Generation-based methods transform an input question into a logical form using a Seq2Seq model (e.g., T5 \citep{T5}).
They often use additional contexts beyond the question to augment the input of the Seq2Seq model and enhance its generalizability. For example, 
\citep{Wang2020b} use  top-5 candidate logical forms enumerated from retrieved entities as the additional context. 
\citep{Zhang2020} further use top-ranked relations, \emph{disambiguated entities}, and classes (retrieved \emph{separately}) as the additional context. 
\citep{Li2019} use connected pairs of retrieved KB elements. 

Our \model\ is generation-based. We use schema contexts (relations and classes) from retrieved relations and entities, rather than separate class retrieval (as in \citep{Zhang2020}), which could introduce noise. We also defer entity disambiguation to the logical form generation stage, thus avoiding error propagation induced by premature entity disambiguation without considering the generation context, as done in existing works outlined below. 

%employ an additional middle-grained component that converts the retrieved KB elements into connected pairs of KB elements. A Seq2Seq model then transforms the concatenation of the question, the retrieved KB element pairs, and logical form sketches generated based on the question into the target logical form. 

%The methods above retrieve KB elements to serve as additional contexts to enable the models to generalize in non-I.I.D. settings. 
%Existing ranking-based and generation-based methods limit the subsequent logical form search space through KB element retrieval, thereby improving generalization abilities. 

 \paragraph{KBQA Entity Retrieval}%\label{subsec:related_work_er} 
%\jz{The discussions above have focused on logical form generation of existing KBQA models.} 
KBQA entity retrieval typically has three steps: {entity mention detection}, {candidate entity retrieval}, and {entity disambiguation}. BERT \citep{BERT}-based named entity recognition  is widely used for entity mention detection from input questions. %____. %TIARA \citep{TIARA} treats entity mention detection as a span classification task. It scores question spans of varying lengths with BERT and takes the ones with top scores as entity mentions. 
%\ssf{tuning the threshold of detecting a candidate mention in order to improve coverage} \jz{tuning the threshold of detecting a candidate mention in order to improve coverage [Not sure how it works]}. 
To retrieve KB entities corresponding to entity mentions, the FACC1 dataset \citep{FACC1} is commonly used, which contains over 10 billion surface forms (with popularity scores) of Freebase entities. 
\citep{Li2020} use the popularity scores for entity disambiguation, while 
\citep{Zhang2019a} and 
\citep{Tang2020} adopt a BERT reranker. %after pruning by popularity. 

%A key issue here is that NER systems may fail to identify  entity mentions precisely, which in turn fail entity retrieval afterwards. 
%Our \model\ model addresses this issue with the help of relation-augmented logical form sketches generation, which enable detecting entity mentions (and hence the entities) more accurately.